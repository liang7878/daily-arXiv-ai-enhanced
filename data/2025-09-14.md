<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 13]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 3]
- [stat.CO](#stat.CO) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.CR](#cs.CR) [Total: 7]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.CY](#cs.CY) [Total: 7]
- [math.CO](#math.CO) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.RO](#cs.RO) [Total: 5]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [econ.GN](#econ.GN) [Total: 3]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [econ.TH](#econ.TH) [Total: 1]
- [cs.CL](#cs.CL) [Total: 15]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [cs.CV](#cs.CV) [Total: 22]
- [math.NA](#math.NA) [Total: 2]
- [q-fin.MF](#q-fin.MF) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: 本文将贝叶斯定理扩展到区间类型 - 2 版本，开发 IT2 贝叶斯定理并提出编码算法。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯推理假设精确输入值不适合现实应用，实际中专家提供的多是区间范围估计。

Method: 开发 IT2 版本的贝叶斯定理以避免输入 IT2 MFs 潜在不一致，提出将专家提供的区间编码为 IT2 模糊隶属函数的算法。

Result: 得到 IT2 版本的贝叶斯定理和编码算法。

Conclusion: 将贝叶斯定理扩展到 IT2 版本，算法推广并扩展了之前相关问题的研究。

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


### [2] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: 提出用NLP和多模态大模型将游戏设计文档转化为Unity游戏原型的框架，结合微调模型和自定义包，评估表现优。


<details>
  <summary>Details</summary>
Motivation: 解决AI辅助游戏开发中从游戏设计到实现的转换问题，提高开发效率。

Method: 引入端到端系统解析GDD，提取规范并合成C#代码，结合微调的LLaMA - 3模型和自定义Unity集成包。

Result: 相比基线模型有显著改进，微调模型在多项指标上表现更优，生成模板高度符合GDD规范。

Conclusion: 该系统有效填补AI辅助游戏开发的关键空白，使大模型成为简化从设计到实现流程的有价值工具。

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [3] [Global Constraint LLM Agents for Text-to-Model Translation](https://arxiv.org/abs/2509.08970)
*Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: 介绍用多LLM代理框架将自然语言描述的优化或满足问题转化为MiniZinc模型，实验表现优于基线方法并规划未来工作。


<details>
  <summary>Details</summary>
Motivation: 自然语言描述的优化或满足问题转化为MiniZinc模型需逻辑推理和约束编程专业知识，具有挑战性。

Method: 采用代理方法，多个专门的LLM代理按全局约束类型分解建模任务，最后由组装代理整合代码成完整模型。

Result: 用多个LLM进行的初始实验表现优于单步提示和思维链提示等基线方法。

Conclusion: 提出未来工作的全面路线图，指出潜在改进方向。

Abstract: Natural language descriptions of optimization or satisfaction problems are
challenging to translate into correct MiniZinc models, as this process demands
both logical reasoning and constraint programming expertise. We introduce a
framework that addresses this challenge with an agentic approach: multiple
specialized large language model (LLM) agents decompose the modeling task by
global constraint type. Each agent is dedicated to detecting and generating
code for a specific class of global constraint, while a final assembler agent
integrates these constraint snippets into a complete MiniZinc model. By
dividing the problem into smaller, well-defined sub-tasks, each LLM handles a
simpler reasoning challenge, potentially reducing overall complexity. We
conduct initial experiments with several LLMs and show better performance
against baselines such as one-shot prompting and chain-of-thought prompting.
Finally, we outline a comprehensive roadmap for future work, highlighting
potential enhancements and directions for improvement.

</details>


### [4] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: 由于生成式 AI 模型对合成数据依赖增加，会导致模型崩溃。本文识别出模型对自生成数据过度自信是崩溃主因，提出 TCE 损失函数，理论和实证验证该方法可缓解崩溃，还能跨模态泛化。


<details>
  <summary>Details</summary>
Motivation: 生成式 AI 模型依赖合成数据会导致模型崩溃，现有缓解策略有限。

Method: 识别模型对自生成数据过度自信为崩溃主因，提出 TCE 损失函数，构建与缓解模型崩溃相关的模型无关框架。

Result: TCE 显著延迟递归训练中的模型崩溃，能将崩溃前模型保真区间延长超 2.3 倍，方法可跨模态泛化。

Conclusion: 损失函数设计是合成数据时代保持生成式模型质量的简单有力工具。

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


### [5] [Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games](https://arxiv.org/abs/2509.09071)
*Crystal Qian,Kehang Zhu,John Horton,Benjamin S. Manning,Vivian Tsai,James Wexler,Nithum Thain*

Main category: cs.AI

TL;DR: 本文对比人类、大语言模型和贝叶斯代理在动态谈判场景中的表现，发现性能相当可能掩盖过程和一致性的根本差异。


<details>
  <summary>Details</summary>
Motivation: 随着协调任务向自主代理转移，需评估代理在动态多智能体环境中的谈判过程，且不同代理有不同优势。

Method: 在动态谈判场景中对人类、大语言模型（GPT - 4o、Gemini 1.5 Pro）和贝叶斯代理进行相同条件下的直接比较，捕捉结果和行为动态。

Result: 贝叶斯代理通过激进优化获取最高盈余但常遭交易拒绝；人类和大语言模型总体盈余相似，但行为不同，大语言模型倾向保守、让步交易，人类更具战略性、冒险性和公平导向。

Conclusion: 性能相当这一常见评估基准可能掩盖过程和一致性的根本差异，这对现实协调任务的实际部署至关重要。

Abstract: Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

</details>


### [6] [Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using Local and Global Explanations](https://arxiv.org/abs/2509.08989)
*Carina Newen,Daniel Bodemer,Sonja Glantz,Emmanuel Müller,Magdalena Wischnewski,Lenka Schnaubert*

Main category: cs.AI

TL;DR: 本文聚焦可解释AI中常被忽略的不确定性和全局解释，选取特定算法测试校准信任能力及用户满意度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 构建可解释AI方案的通用指南，聚焦常被忽略的不确定性和全局解释。

Method: 选择能同时涵盖不确定性、鲁棒性和全局可解释AI等概念的算法，测试其校准信任的能力，并检验复杂但直观的算法能否提高用户满意度和可解释性。

Result: 未提及

Conclusion: 未提及

Abstract: Explainable AI has become a common term in the literature, scrutinized by
computer scientists and statisticians and highlighted by psychological or
philosophical researchers. One major effort many researchers tackle is
constructing general guidelines for XAI schemes, which we derived from our
study. While some areas of XAI are well studied, we focus on uncertainty
explanations and consider global explanations, which are often left out. We
chose an algorithm that covers various concepts simultaneously, such as
uncertainty, robustness, and global XAI, and tested its ability to calibrate
trust. We then checked whether an algorithm that aims to provide more of an
intuitive visual understanding, despite being complicated to understand, can
provide higher user satisfaction and human interpretability.

</details>


### [7] [Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users](https://arxiv.org/abs/2509.09066)
*Haowei Yang,Yushang Zhao,Sitao Min,Bo Su,Chao Yao,Wei Xu*

Main category: cs.AI

TL;DR: 提出上下文条件提示公式解决推荐系统冷启动问题，实验证明优化提示可提升模型指标，提示适配是解决冷启动的途径之一。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中冷启动用户因历史行为信息受限导致推荐效果不佳的问题。

Method: 引入上下文条件提示公式P(u, Ds) -> Rˆ，结合基于Transformer的自回归大语言模型，采用token级对齐和嵌入空间正则化。

Result: 最优示例注入和指令结构能显著提高低数据设置下模型的precision@k和NDCG分数，及时组合不仅是语法上的，还能控制注意力和译码器行为。

Conclusion: 基于提示的适配可作为解决基于大语言模型管道中冷启动推荐问题的方法之一。

Abstract: The cold-start user issue further compromises the effectiveness of
recommender systems in limiting access to the historical behavioral
information. It is an effective pipeline to optimize instructional prompts on a
few-shot large language model (LLM) used in recommender tasks. We introduce a
context-conditioned prompt formulation method P(u,\ Ds)\ \rightarrow\
R\widehat, where u is a cold-start user profile, Ds is a curated support set,
and R\widehat is the predicted ranked list of items. Based on systematic
experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2,
GPT-4), we provide empirical evidence that optimal exemplar injection and
instruction structuring can significantly improve the precision@k and NDCG
scores of such models in low-data settings. The pipeline uses token-level
alignments and embedding space regularization with a greater semantic fidelity.
Our findings not only show that timely composition is not merely syntactic but
also functional as it is in direct control of attention scales and decoder
conduct through inference. This paper shows that prompt-based adaptation may be
considered one of the ways to address cold-start recommendation issues in
LLM-based pipelines.

</details>


### [8] [Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning](https://arxiv.org/abs/2509.09127)
*Khashayar Namdar,Pin-Chien Wang,Tushar Raju,Steven Zheng,Fiona Li,Safwat Tahmin Khan*

Main category: cs.AI

TL;DR: 本文提出开发机器学习管道的方法，用于识别高风险银行客户，在竞赛数据集上取得AUROC为0.961的结果并获竞赛第二名。


<details>
  <summary>Details</summary>
Motivation: 反洗钱是金融机构优先事项，机器学习在此有高潜力，要在竞赛中识别高风险银行客户。

Method: 采用16步设计和统计分析，将数据存入SQLite数据库，开发基于SQL的特征工程算法，连接预训练模型并使其可推理，提供可解释人工智能模块。

Result: 管道的接收器操作特征曲线下平均面积（AUROC）为0.961，标准差为0.005，在竞赛中获第二名。

Conclusion: 所提出的开发机器学习管道的方法有效，能较好识别高风险银行客户。

Abstract: Anti-money laundering (AML) actions and measurements are among the priorities
of financial institutions, for which machine learning (ML) has shown to have a
high potential. In this paper, we propose a comprehensive and systematic
approach for developing ML pipelines to identify high-risk bank clients in a
dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for
Management and Innovation (IMI) Big Data and Artificial Intelligence
Competition. The dataset included 195,789 customer IDs, and we employed a
16-step design and statistical analysis to ensure the final pipeline was
robust. We also framed the data in a SQLite database, developed SQL-based
feature engineering algorithms, connected our pre-trained model to the
database, and made it inference-ready, and provided explainable artificial
intelligence (XAI) modules to derive feature importance. Our pipeline achieved
a mean area under the receiver operating characteristic curve (AUROC) of 0.961
with a standard deviation (SD) of 0.005. The proposed pipeline achieved second
place in the competition.

</details>


### [9] [Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective](https://arxiv.org/abs/2509.09154)
*Bui Duc Manh,Soumyaratna Debnath,Zetong Zhang,Shriram Damodaran,Arvind Kumar,Yueyi Zhang,Lu Mi,Erik Cambria,Lin Wang*

Main category: cs.AI

TL;DR: 现有代理AI空间推理能力有限，本文提出基于神经科学原理的计算框架，分析现有方法、评估基准和数据集，探索应用领域并指出研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有代理AI空间推理能力受限，多为符号和顺序处理，而人类空间智能更灵活，需缩小差距以提升与3D物理世界的交互能力。

Method: 研究计算神经科学中的空间神经模型，引入基于神经科学原理的计算框架，分析现有方法，评估基准和数据集。

Result: 形成跨虚拟和物理环境的代理空间推理能力视角，找出阻碍神经科学空间推理模块发展的关键差距。

Conclusion: 强调可在动态或非结构化环境中推广空间推理的研究方向，为研究界提供神经科学视角和结构化路径。

Abstract: Recent advances in agentic AI have led to systems capable of autonomous task
execution and language-based reasoning, yet their spatial reasoning abilities
remain limited and underexplored, largely constrained to symbolic and
sequential processing. In contrast, human spatial intelligence, rooted in
integrated multisensory perception, spatial memory, and cognitive maps, enables
flexible, context-aware decision-making in unstructured environments.
Therefore, bridging this gap is critical for advancing Agentic Spatial
Intelligence toward better interaction with the physical 3D world. To this end,
we first start from scrutinizing the spatial neural models as studied in
computational neuroscience, and accordingly introduce a novel computational
framework grounded in neuroscience principles. This framework maps core
biological functions to six essential computation modules: bio-inspired
multimodal sensing, multi-sensory integration, egocentric-allocentric
conversion, an artificial cognitive map, spatial memory, and spatial reasoning.
Together, these modules form a perspective landscape for agentic spatial
reasoning capability across both virtual and physical environments. On top, we
conduct a framework-guided analysis of recent methods, evaluating their
relevance to each module and identifying critical gaps that hinder the
development of more neuroscience-grounded spatial reasoning modules. We further
examine emerging benchmarks and datasets and explore potential application
domains ranging from virtual to embodied systems, such as robotics. Finally, we
outline potential research directions, emphasizing the promising roadmap that
can generalize spatial reasoning across dynamic or unstructured environments.
We hope this work will benefit the research community with a
neuroscience-grounded perspective and a structured pathway. Our project page
can be found at Github.

</details>


### [10] [ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting](https://arxiv.org/abs/2509.09210)
*Xing Gao,Zherui Huang,Weiyao Lin,Xiao Sun*

Main category: cs.AI

TL;DR: 提出ProgD策略解决多智能体交互预测中交互演变性被忽视问题，在相关基准测试获最优表现。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体交互预测方法忽略交互的演变性，需改进。

Method: 提出ProgD策略，借助动态异质图进行场景建模，设计渐进式场景建模、分解架构处理时空依赖，采用多尺度解码程序。

Result: ProgD在INTERACTION和Argoverse 2基准测试中达到最优性能，排名第一。

Conclusion: ProgD策略有效解决了现有方法的局限性，能更好地进行多智能体未来运动预测。

Abstract: Accurate motion prediction of surrounding agents is crucial for the safe
planning of autonomous vehicles. Recent advancements have extended prediction
techniques from individual agents to joint predictions of multiple interacting
agents, with various strategies to address complex interactions within future
motions of agents. However, these methods overlook the evolving nature of these
interactions. To address this limitation, we propose a novel progressive
multi-scale decoding strategy, termed ProgD, with the help of dynamic
heterogeneous graph-based scenario modeling. In particular, to explicitly and
comprehensively capture the evolving social interactions in future scenarios,
given their inherent uncertainty, we design a progressive modeling of scenarios
with dynamic heterogeneous graphs. With the unfolding of such dynamic
heterogeneous graphs, a factorized architecture is designed to process the
spatio-temporal dependencies within future scenarios and progressively
eliminate uncertainty in future motions of multiple agents. Furthermore, a
multi-scale decoding procedure is incorporated to improve on the future
scenario modeling and consistent prediction of agents' future motion. The
proposed ProgD achieves state-of-the-art performance on the INTERACTION
multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2
multi-world forecasting benchmark.

</details>


### [11] [Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions](https://arxiv.org/abs/2509.09215)
*Qinnan Hu,Yuntao Wang,Yuan Gao,Zhou Su,Linkang Du*

Main category: cs.AI

TL;DR: 本文提出区块链赋能的分层架构用于监管代理协作，设计三个关键模块，为大规模代理生态系统监管机制奠定基础并探讨未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型赋能的自主代理虽有诸多机遇，但不可预测行为和异构能力带来治理与问责挑战。

Method: 提出包含代理层、区块链数据层和监管应用层的分层架构，设计行为追踪仲裁、动态声誉评估和恶意行为预测三个关键模块。

Result: 为大规模代理生态系统中可信赖、有韧性和可扩展的监管机制建立了系统基础。

Conclusion: 讨论了多智能体系统中区块链赋能监管框架的未来研究方向。

Abstract: Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.

</details>


### [12] [Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search](https://arxiv.org/abs/2509.09245)
*Shuocheng Li,Yihao Liu,Silin Du,Wenxuan Zeng,Zhe Xu,Mengyu Zhou,Yeye He,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 提出可扩展管道提取数据科学任务与解决方案构建NbQA数据集，用Jupiter框架增强多步推理，实验显示模型在任务解决上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多步推理和工具使用上存在困难，限制了其在复杂数据分析任务中的有效性。

Method: 提出可扩展管道从Jupyter笔记本提取任务与解决方案构建NbQA数据集；提出Jupiter框架将数据分析建模为搜索问题，用蒙特卡罗树搜索生成解决方案轨迹用于价值模型学习。

Result: Qwen2.5 - 7B和14B - Instruct模型在NbQA上分别解决77.82%和86.38%的InfiAgent - DABench任务，匹配或超越GPT - 4o和先进代理框架。

Conclusion: 该方法能提升模型在多步推理任务中的泛化能力和工具使用推理能力。

Abstract: Large language models (LLMs) have shown great promise in automating data
science workflows, but existing models still struggle with multi-step reasoning
and tool use, which limits their effectiveness on complex data analysis tasks.
To address this, we propose a scalable pipeline that extracts high-quality,
tool-based data analysis tasks and their executable multi-step solutions from
real-world Jupyter notebooks and associated data files. Using this pipeline, we
introduce NbQA, a large-scale dataset of standardized task-solution pairs that
reflect authentic tool-use patterns in practical data science scenarios. To
further enhance multi-step reasoning, we present Jupiter, a framework that
formulates data analysis as a search problem and applies Monte Carlo Tree
Search (MCTS) to generate diverse solution trajectories for value model
learning. During inference, Jupiter combines the value model and node visit
counts to efficiently collect executable multi-step plans with minimal search
steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on
NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,
respectively-matching or surpassing GPT-4o and advanced agent frameworks.
Further evaluations demonstrate improved generalization and stronger tool-use
reasoning across diverse multi-step reasoning tasks.

</details>


### [13] [Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs](https://arxiv.org/abs/2509.09272)
*Vaibhav Chaudhary,Neha Soni,Narotam Singh,Amita Kapoor*

Main category: cs.AI

TL;DR: 文章对三种构建知识图谱三元组并与大语言模型集成用于问答的方法进行技术比较研究，实验表明OpenIE三元组覆盖最全面，GraphRAG推理能力最强，最后讨论各方法优缺点并给出未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在处理复杂长文本主题和整体理解时有局限，需要对文本和上下文进行更深入分析，因此研究构建知识图谱三元组并与大语言模型集成用于问答的方法。

Method: 对spaCy、Stanford CoreNLP - OpenIE和GraphRAG三种利用开源技术构建知识图谱三元组并与大语言模型集成用于问答的方法进行全面技术比较研究，分析其能力、发展状况及对基于大语言模型问答性能的影响。

Result: OpenIE提供的三元组覆盖最全面，GraphRAG在三者中展现出更优的推理能力。

Conclusion: 讨论了每种方法的优缺点，并对改进基于知识图谱的问答给出未来方向的见解。

Abstract: Knowledge graphs, a powerful tool for structuring information through
relational triplets, have recently become the new front-runner in enhancing
question-answering systems. While traditional Retrieval Augmented Generation
(RAG) approaches are proficient in fact-based and local context-based
extraction from concise texts, they encounter limitations when addressing the
thematic and holistic understanding of complex, extensive texts, requiring a
deeper analysis of both text and context. This paper presents a comprehensive
technical comparative study of three different methodologies for constructing
knowledge graph triplets and integrating them with Large Language Models (LLMs)
for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all
leveraging open source technologies. We evaluate the effectiveness,
feasibility, and adaptability of these methods by analyzing their capabilities,
state of development, and their impact on the performance of LLM-based question
answering. Experimental results indicate that while OpenIE provides the most
comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning
abilities among the three. We conclude with a discussion on the strengths and
limitations of each method and provide insights into future directions for
improving knowledge graph-based question answering.

</details>


### [14] [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
*Bingning Huang,Tu Nguyen,Matthieu Zimmer*

Main category: cs.AI

TL;DR: 本文探讨用MCTS轨迹改进基于偏好的强化学习中的策略优化，提出分阶段GRPO训练范式，分析了相关奖励信号，指出问题并给出解决方案。


<details>
  <summary>Details</summary>
Motivation: 受MCTS在大语言模型推理中生成高质量中间轨迹的启发，探索将MCTS轨迹用于基于偏好的强化学习中改进策略优化。

Method: 提出分阶段GRPO训练范式，利用部分揭示的MCTS滚动生成补全，引入树状结构进行优势估计。

Result: 结构化优势估计可稳定更新并反映推理质量，但存在优势饱和和奖励信号崩溃问题。

Conclusion: 提出启发式和统计解决方案，讨论了在分阶段或树状奖励结构下学习的开放性挑战。

Abstract: Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.

</details>


### [15] [LightAgent: Production-level Open-source Agentic AI Framework](https://arxiv.org/abs/2509.09292)
*Weige Cai,Tong Zhu,Jinyi Niu,Ruiqi Hu,Lingyao Li,Tenglong Wang,Xiaowu Dai,Weining Shen,Liwen Zhang*

Main category: cs.AI

TL;DR: 提出轻量级且强大的代理框架LightAgent，解决现有框架在灵活性和简单性之间的权衡问题，已开源。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型发展下，多智能体系统在设计通用、健壮和高效的代理部署平台方面存在挑战。

Method: 提出LightAgent框架，集成记忆、工具和思维树等核心功能，保持轻量级结构，且作为开源方案与主流聊天平台集成。

Result: 成功提出LightAgent框架并开源。

Conclusion: LightAgent能有效解决现有框架的权衡问题，方便开发者构建自学习代理。

Abstract: With the rapid advancement of large language models (LLMs), Multi-agent
Systems (MAS) have achieved significant progress in various application
scenarios. However, substantial challenges remain in designing versatile,
robust, and efficient platforms for agent deployment. To address these
limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic
framework, effectively resolving the trade-off between flexibility and
simplicity found in existing frameworks. LightAgent integrates core
functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while
maintaining an extremely lightweight structure. As a fully open-source
solution, it seamlessly integrates with mainstream chat platforms, enabling
developers to easily build self-learning agents. We have released LightAgent at
\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}

</details>


### [16] [Explaining Tournament Solutions with Minimal Supports](https://arxiv.org/abs/2509.09312)
*Clément Contet,Umberto Grandi,Jérôme Mengin*

Main category: cs.AI

TL;DR: 研究为不同锦标赛规则下候选人成为获胜者提供认证解释的问题，确定最小支持集，给出部分规则的多项式时间算法并展示其用于生成解释。


<details>
  <summary>Details</summary>
Motivation: 为不同锦标赛规则下候选人成为获胜者提供认证解释。

Method: 确定最小支持集（即候选人在其中必赢的最小子锦标赛），针对常见锦标赛规则确定最小支持集大小，设计算法。

Result: 除加权未覆盖集问题是NP完全问题外，为其他规则给出多项式时间算法计算最小支持集。

Conclusion: 最小支持集可用于产生紧凑、认证且直观的解释。

Abstract: Tournaments are widely used models to represent pairwise dominance between
candidates, alternatives, or teams. We study the problem of providing certified
explanations for why a candidate appears among the winners under various
tournament rules. To this end, we identify minimal supports, minimal
sub-tournaments in which the candidate is guaranteed to win regardless of how
the rest of the tournament is completed (that is, the candidate is a necessary
winner of the sub-tournament). This notion corresponds to an abductive
explanation for the question,"Why does the winner win the tournament", a
central concept in formal explainable AI. We focus on common tournament
solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule,
the maximin rule, and the weighted uncovered set. For each rule we determine
the size of the smallest minimal supports, and we present polynomial-time
algorithms to compute them for all but the weighted uncovered set, for which
the problem is NP-complete. Finally, we show how minimal supports can serve to
produce compact, certified, and intuitive explanations.

</details>


### [17] [Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance](https://arxiv.org/abs/2509.09314)
*Thuy Ngoc Nguyen,Anita Williams Woolley,Cleotilde Gonzalez*

Main category: cs.AI

TL;DR: 本文研究受限交流下空间协调三维度对团队表现的影响，分析34个四人团队数据，发现空间专业化正向预测表现，自适应空间接近度呈倒U关系，结果对训练和AI辅助有启示。


<details>
  <summary>Details</summary>
Motivation: 现有文献多关注同地点同步团队或知识工作协调，而很多团队需在无视觉线索和大量交流下协调物理空间行动，故研究空间协调维度对团队表现的影响。

Method: 以协作在线搜索救援任务为研究场景，限制明确交流，通过指标衡量团队空间接近度、分布模式和运动对齐，分析34个四人团队数据。

Result: 空间专业化正向预测表现，自适应空间接近度呈边际倒U型关系，指标的时间动态能区分高低绩效团队。

Conclusion: 研究为基于角色的团队隐式空间协调提供见解，强调平衡自适应策略的重要性，对训练和AI辅助团队支持系统有意义。

Abstract: Coordinated teamwork is essential in fast-paced decision-making environments
that require dynamic adaptation, often without an opportunity for explicit
communication. Although implicit coordination has been extensively considered
in the existing literature, the majority of work has focused on co-located,
synchronous teamwork (such as sports teams) or, in distributed teams, primarily
on coordination of knowledge work. However, many teams (firefighters, military,
law enforcement, emergency response) must coordinate their movements in
physical space without the benefit of visual cues or extensive explicit
communication. This paper investigates how three dimensions of spatial
coordination, namely exploration diversity, movement specialization, and
adaptive spatial proximity, influence team performance in a collaborative
online search and rescue task where explicit communication is restricted and
team members rely on movement patterns to infer others' intentions and
coordinate actions. Our metrics capture the relational aspects of teamwork by
measuring spatial proximity, distribution patterns, and alignment of movements
within shared environments. We analyze data from 34 four-person teams (136
participants) assigned to specialized roles in a search and rescue task.
Results show that spatial specialization positively predicts performance, while
adaptive spatial proximity exhibits a marginal inverted U-shaped relationship,
suggesting moderate levels of adaptation are optimal. Furthermore, the temporal
dynamics of these metrics differentiate high- from low-performing teams over
time. These findings provide insights into implicit spatial coordination in
role-based teamwork and highlight the importance of balanced adaptive
strategies, with implications for training and AI-assisted team support
systems.

</details>


### [18] [Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization](https://arxiv.org/abs/2509.09321)
*Hangyi Jia,Yuxi Qian,Hanwen Tong,Xinhui Wu,Lin Chen,Feng Wei*

Main category: cs.AI

TL;DR: 现有基准无法全面评估基于大语言模型的端到端机器学习代理能力，本文提出TAM Bench基准，有三项创新，构建不同规模子集，Lite版可用于日常测试和对比研究。


<details>
  <summary>Details</summary>
Motivation: 现有基准在任务覆盖、领域多样性、难度建模和评估严谨性方面存在局限，无法全面评估基于大语言模型的端到端机器学习代理能力。

Method: 提出TAM Bench基准，包括基于浏览器自动化和大语言模型的任务获取系统、基于排行榜的难度建模机制、多维度评估框架，构建不同规模的基准子集。

Result: 构建了Lite、Medium和Full三个不同规模的基准子集，Lite版有18个任务，各模态和难度级别覆盖均衡。

Conclusion: TAM Bench是一个多样化、现实且结构化的基准，可用于评估基于大语言模型的端到端机器学习任务代理，Lite版可作为日常基准测试和比较研究的实用测试平台。

Abstract: Recent advances in large language models (LLMs) have enabled the emergence of
general-purpose agents for automating end-to-end machine learning (ML)
workflows, including data analysis, feature engineering, model training, and
competition solving. However, existing benchmarks remain limited in task
coverage, domain diversity, difficulty modeling, and evaluation rigor, failing
to capture the full capabilities of such agents in realistic settings. We
present TAM Bench, a diverse, realistic, and structured benchmark for
evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three
key innovations: (1) A browser automation and LLM-based task acquisition system
that automatically collects and structures ML challenges from platforms such as
Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities
(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty
modeling mechanism that estimates task complexity using participant counts and
score dispersion, enabling scalable and objective task calibration; (3) A
multi-dimensional evaluation framework incorporating performance, format
compliance, constraint adherence, and task generalization. Based on 150 curated
AutoML tasks, we construct three benchmark subsets of different sizes -- Lite,
Medium, and Full -- designed for varying evaluation scenarios. The Lite
version, with 18 tasks and balanced coverage across modalities and difficulty
levels, serves as a practical testbed for daily benchmarking and comparative
studies.

</details>


### [19] [Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning](https://arxiv.org/abs/2509.09356)
*Abdel Hakim Drid,Vincenzo Suriani,Daniele Nardi,Abderrezzak Debilou*

Main category: cs.AI

TL;DR: 提出用于资源高效语义探索的DRL架构，结合VLM常识和课程学习策略，实验显示提升对象发现率等，为机器人自主探索提供新方法。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法难以平衡高效探索和语义理解，在语义探索中常需人工干预，需解决此挑战。

Method: 提出新颖DRL架构，通过分层奖励函数集成VLM常识，将VLM查询建模为专用动作，结合课程学习策略。

Result: 代理显著提高对象发现率，学会有效导航到语义丰富区域，掌握何时获取外部环境信息的策略。

Conclusion: 研究提供了在自主代理中嵌入常识语义推理的实用可扩展方法，为机器人全智能自引导探索提供新途径。

Abstract: Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.

</details>


### [20] [TORSO: Template-Oriented Reasoning Towards General Tasks](https://arxiv.org/abs/2509.09448)
*Minhyuk Kim,Seungyoon Lee,Heuiseok Lim*

Main category: cs.AI

TL;DR: 介绍了一种名为TORSO的方法，能让大语言模型在无需手工制作少样本示例的情况下利用内部推理能力生成响应，实验显示其在多种基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有使用少样本提示生成响应的方法依赖示例，限制模型推理能力，且构建特定任务提示成本高、易导致任务间不一致。

Method: 引入Template - Oriented Reasoning (TORSO)，促使模型利用内部推理能力生成不同任务的合适响应。

Result: TORSO在多种大语言模型基准测试中凭借合理的推理取得了良好表现。

Conclusion: TORSO是一种有效的方法，可让大语言模型在不同任务中利用自身推理能力生成响应，无需依赖少样本示例。

Abstract: The approaches that guide Large Language Models (LLMs) to emulate human
reasoning during response generation have emerged as an effective method for
enabling them to solve complex problems in a step-by-step manner, thereby
achieving superior performance. However, most existing approaches using
few-shot prompts to generate responses heavily depend on the provided examples,
limiting the utilization of the model's inherent reasoning capabilities.
Moreover, constructing task-specific few-shot prompts is often costly and may
lead to inconsistencies across different tasks. In this work, we introduce
Template-Oriented Reasoning (TORSO), which elicits the model to utilize
internal reasoning abilities to generate proper responses across various tasks
without the need for manually crafted few-shot examples. Our experimental
results demonstrate that TORSO achieves strong performance on diverse LLMs
benchmarks with reasonable rationales.

</details>


### [21] [Inteligencia Artificial jurídica y el desafío de la veracidad: análisis de alucinaciones, optimización de RAG y principios para una integración responsable](https://arxiv.org/abs/2509.09467)
*Alex Dantart*

Main category: cs.AI

TL;DR: 分析大语言模型应用于法律领域时的“幻觉”问题，探讨RAG策略局限性并提出优化，强调人类监督，主张采用“咨询式”AI范式。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型应用于法律领域时出现“幻觉”（虚假信息）的挑战。

Method: 分析“幻觉”的原因、表现，研究RAG缓解策略有效性，探索伦理和监管影响。

Result: 发现RAG策略有局限性，需整体优化，人类监督不可替代。

Conclusion: 解决方案是采用“咨询式”AI范式，增强而非取代专业判断。

Abstract: This technical report analyzes the challenge of "hallucinations" (false
information) in LLMs applied to law. It examines their causes, manifestations,
and the effectiveness of the RAG mitigation strategy, highlighting its
limitations and proposing holistic optimizations. The paper explores the
ethical and regulatory implications, emphasizing human oversight as an
irreplaceable role. It concludes that the solution lies not in incrementally
improving generative models, but in adopting a "consultative" AI paradigm that
prioritizes veracity and traceability, acting as a tool to amplify, not
replace, professional judgment.
  --
  Este informe t\'ecnico analiza el desaf\'io de las "alucinaciones"
(informaci\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,
manifestaciones y la efectividad de la estrategia de mitigaci\'on RAG,
exponiendo sus limitaciones y proponiendo optimizaciones hol\'isticas. Se
exploran las implicaciones \'eticas y regulatorias, enfatizando la
supervisi\'on humana como un rol insustituible. El documento concluye que la
soluci\'on no reside en mejorar incrementalmente los modelos generativos, sino
en adoptar un paradigma de IA "consultiva" que priorice la veracidad y la
trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el
juicio profesional.

</details>


### [22] [SEDM: Scalable Self-Evolving Distributed Memory for Agents](https://arxiv.org/abs/2509.09498)
*Haoran Xu,Jiacong Hu,Ke Zhang,Lei Yu,Yuxin Tang,Xinyuan Song,Yiqun Duan,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: 提出SEDM框架解决多智能体系统内存管理难题，评估显示其能提升推理准确性、减少开销，是可扩展可持续的内存机制。


<details>
  <summary>Details</summary>
Motivation: 长期多智能体系统产生大量轨迹和交互，现有内存管理方法存在噪声积累、内存膨胀和跨领域泛化受限等问题。

Method: 提出SEDM框架，集成基于可重现回放的可验证写入许可、根据经验效用动态排序和合并条目的自调度内存控制器，以及支持跨异构任务迁移的跨领域知识扩散。

Result: 在基准数据集上评估表明，SEDM比强内存基线提高推理准确性、减少令牌开销，且能利用事实验证提炼的知识增强多跳推理。

Conclusion: SEDM是用于开放式多智能体协作的可扩展和可持续的内存机制。

Abstract: Long-term multi-agent systems inevitably generate vast amounts of
trajectories and historical interactions, which makes efficient memory
management essential for both performance and scalability. Existing methods
typically depend on vector retrieval and hierarchical storage, yet they are
prone to noise accumulation, uncontrolled memory expansion, and limited
generalization across domains. To address these challenges, we present SEDM,
Self-Evolving Distributed Memory, a verifiable and adaptive framework that
transforms memory from a passive repository into an active, self-optimizing
component. SEDM integrates verifiable write admission based on reproducible
replay, a self-scheduling memory controller that dynamically ranks and
consolidates entries according to empirical utility, and cross-domain knowledge
diffusion that abstracts reusable insights to support transfer across
heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM
improves reasoning accuracy while reducing token overhead compared with strong
memory baselines, and further enables knowledge distilled from fact
verification to enhance multi-hop reasoning. The results highlight SEDM as a
scalable and sustainable memory mechanism for open-ended multi-agent
collaboration. The code will be released in the later stage of this project.

</details>


### [23] [Compositional Concept Generalization with Variational Quantum Circuits](https://arxiv.org/abs/2509.09541)
*Hala Hawashin,Mina Abbaszadeh,Nicholas Joseph,Beth Pearson,Martha Lewis,Mehrnoosh sadrzadeh*

Main category: cs.AI

TL;DR: 研究量子模型在需要组合泛化的图像字幕任务中的表现，用不同图像编码技术训练变分量子电路，取得一定成果。


<details>
  <summary>Details</summary>
Motivation: 当前AI工具如视觉语言模型缺乏组合泛化能力，先前基于组合张量的句子语义研究结果不佳，推测量子模型训练效率提升可改善任务表现。

Method: 在希尔伯特空间解释组合张量模型的表示，训练变分量子电路学习表示，使用多热编码（MHE）和角度/幅度编码两种图像编码技术。

Result: 使用有噪声的MHE编码取得概念验证的良好结果，在CLIP图像向量上表现不一，但仍优于经典组合模型。

Conclusion: 量子模型在需要组合泛化的图像字幕任务中有一定潜力，可改善组合泛化能力。

Abstract: Compositional generalization is a key facet of human cognition, but lacking
in current AI tools such as vision-language models. Previous work examined
whether a compositional tensor-based sentence semantics can overcome the
challenge, but led to negative results. We conjecture that the increased
training efficiency of quantum models will improve performance in these tasks.
We interpret the representations of compositional tensor-based models in
Hilbert spaces and train Variational Quantum Circuits to learn these
representations on an image captioning task requiring compositional
generalization. We used two image encoding techniques: a multi-hot encoding
(MHE) on binary image vectors and an angle/amplitude encoding on image vectors
taken from the vision-language model CLIP. We achieve good proof-of-concept
results using noisy MHE encodings. Performance on CLIP image vectors was more
mixed, but still outperformed classical compositional models.

</details>


### [24] [Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution](https://arxiv.org/abs/2509.09560)
*Shulai Zhang,Ao Xu,Quan Chen,Han Zhao,Weihao Cui,Ningxin Zheng,Haibin Lin,Xin Liu,Minyi Guo*

Main category: cs.AI

TL;DR: 提出Auras推理框架优化具身AI代理推理频率，能提高吞吐量且保证准确性。


<details>
  <summary>Details</summary>
Motivation: 传统顺序计算模式在实现具身AI系统现实应用所需的“思考”频率上有显著局限。

Method: 提出Auras框架，将感知和生成模块分离并提供可控的流水线并行处理，建立公共上下文以解决数据陈旧问题。

Result: Auras平均提高吞吐量2.54倍，达到原准确率的102.7%。

Conclusion: Auras能克服顺序计算的限制，提供高吞吐量。

Abstract: Embodied AI systems operate in dynamic environments, requiring seamless
integration of perception and generation modules to process high-frequency
input and output demands. Traditional sequential computation patterns, while
effective in ensuring accuracy, face significant limitations in achieving the
necessary "thinking" frequency for real-world applications. In this work, we
present Auras, an algorithm-system co-designed inference framework to optimize
the inference frequency of embodied AI agents. Auras disaggregates the
perception and generation and provides controlled pipeline parallelism for them
to achieve high and stable throughput. Faced with the data staleness problem
that appears when the parallelism is increased, Auras establishes a public
context for perception and generation to share, thereby promising the accuracy
of embodied agents. Experimental results show that Auras improves throughput by
2.54x on average while achieving 102.7% of the original accuracy, demonstrating
its efficacy in overcoming the constraints of sequential computation and
providing high throughput.

</details>


### [25] [The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs](https://arxiv.org/abs/2509.09677)
*Akshit Sinha,Arvindh Arun,Shashwat Goel,Steffen Staab,Jonas Geiping*

Main category: cs.AI

TL;DR: 研究大语言模型扩展是否收益递减，发现执行能力关键，大模型执行优势明显，还研究自调节效应并对前沿思维模型进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型持续扩展是否收益递减，解决其能解决复杂推理问题但长简单任务易失败的矛盾。

Method: 观察单步准确率边际增益对任务长度的影响，隔离执行能力，研究不同规模模型执行情况及自调节效应，对前沿思维模型进行基准测试。

Result: 大模型在执行长任务时表现更好，模型每步准确率随步数增加而下降，存在自调节效应，思维模型无自调节且单轮执行任务更长。

Conclusion: 关注执行能力可调和相关争议，凸显扩展模型规模和顺序测试时间计算对长任务的巨大益处。

Abstract: Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [26] [Symmetries in stochastic homogenization and acclimatizations for the RVE method](https://arxiv.org/abs/2509.08977)
*Binh Huy Nguyen,Matti Schneider*

Main category: cs.CE

TL;DR: 研究随机微结构对称性对热导率有效张量及其波动的影响，提出后处理中强制对称性的策略并验证其效果。


<details>
  <summary>Details</summary>
Motivation: 解决RVE方法中单元类型对称性可能破坏整体对称性、影响有效属性近似的问题。

Method: 在RVE方法框架下建立有效张量及其波动的不变性条件，引入后处理强制对称性的技术，进行大规模FFT均质化模拟。

Result: 合适的投影可实现无偏方差减少策略，准确强制预期对称性；研究了估计有效电导率及其波动的对称结构，验证了对称投影技术对工业规模纤维增强复合材料微结构的有效性。

Conclusion: 对称投影技术能解决RVE方法中对称性问题，实现无偏方差减少并强制预期对称性。

Abstract: We investigate the implications of a given symmetry of a random
microstructure on the obtained effective tensor and its fluctuation in the
context of thermal conductivity, and study strategies for enforcing these
symmetries in postprocessing via orthogonal projectors. Within the framework of
the representative volume element (RVE) method, we establish the invariance
conditions for the effective tensor and its fluctuation under different
symmetry groups of the microstructure. Interestingly, the symmetry of the
considered cell type in the RVE method may break the ensemble symmetry and
compromise the approximation of the effective properties. To rectify this
issue, we introduce dedicated techniques which permit to enforce the expected
symmetries in postprocessing and study the implications on the bounds for the
effective properties as well as the total, the random and the systematic
errors. We provide theoretical arguments that suitable projections lead to
unbiased variance-reduction strategies which furthermore enforce the expected
symmetries exactly. Through large-scale FFT-based homogenization simulations,
we study the symmetry structure of the estimated effective conductivities and
their fluctuations. Moreover, we demonstrate the power of the
symmetry-projection techniques for fiber-reinforced composite microstructures
of industrial scale.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [27] [Koza and Koza-Hub for born-interoperable knowledge graph generation using KGX](https://arxiv.org/abs/2509.09096)
*Daniel R Korn,Patrick Golden,Aaron Odell,Katherina Cortes,Shilpa Sundar,Kevin Schaper,Sarah Gehrke,Corey Cox,Harry Caufield,Justin Reese,Evan Morris,Christopher J Mungall,Melissa Haendel*

Main category: cs.DB

TL;DR: 本文介绍Koza和Koza - Hub解决生物医学知识图谱构建中冗余劳动问题。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学知识图谱构建方法存在大量冗余劳动，源于缺乏数据标准和‘知识图谱就绪’数据。

Method: 使用KGX标准，引入Python软件包Koza和Koza - Hub，将知识图谱摄取转化为一组基本操作，通过YAML文件配置并遵循选定数据模式。

Result: 无明确提及结果。

Conclusion: 提出的方法可简化将原始生物医学信息摄取到KGX格式的过程。

Abstract: Knowledge graph construction has become an essential domain for the future of
biomedical research. But current approaches demand a high amount of redundant
labor. These redundancies are the result of the lack of data standards and
"knowledge-graph ready" data from sources. Using the KGX standard, we aim to
solve these issues. Herein we introduce Koza and the Koza-Hub, a Python
software package which streamlines ingesting raw biomedical information into
the KGX format, and an associated set of conversion processes for thirty gold
standard biomedical data sources. Our approach is to turn knowledge graph
ingests into a set of primitive operations, provide configuration through YAML
files, and enforce compliance with the chosen data schema.

</details>


### [28] [Let's Simply Count: Quantifying Distributional Similarity Between Activities in Event Data](https://arxiv.org/abs/2509.09440)
*Henrik Kirchmann,Stephan A. Fahrenkrog-Petersen,Xixi Lu,Matthias Weidlich*

Main category: cs.DB

TL;DR: 现有基于神经网络的活动分布相似性方法有计算成本高和表征难解释的问题，本文提出基于计数的嵌入方法并通过基准框架测试，证明其高效有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经网络的活动分布相似性方法存在计算成本高和表征可解释性有限的问题，需更简单的建模方法。

Method: 引入基于计数的嵌入方法避免复杂训练过程，并提供全面的基准测试框架。

Result: 实验对比表明，基于计数的嵌入方法为事件数据中活动的分布相似性提供了高效有效的基础。

Conclusion: 基于计数的嵌入方法在活动分布相似性建模上是简单且有效的。

Abstract: To obtain insights from event data, advanced process mining methods assess
the similarity of activities to incorporate their semantic relations into the
analysis. Here, distributional similarity that captures similarity from
activity co-occurrences is commonly employed. However, existing work for
distributional similarity in process mining adopt neural network-based
approaches as developed for natural language processing, e.g., word2vec and
autoencoders. While these approaches have been shown to be effective, their
downsides are high computational costs and limited interpretability of the
learned representations.
  In this work, we argue for simplicity in the modeling of distributional
similarity of activities. We introduce count-based embeddings that avoid a
complex training process and offer a direct interpretable representation. To
underpin our call for simple embeddings, we contribute a comprehensive
benchmarking framework, which includes means to assess the intrinsic quality of
embeddings, their performance in downstream applications, and their
computational efficiency. In experiments that compare against the state of the
art, we demonstrate that count-based embeddings provide a highly effective and
efficient basis for distributional similarity between activities in event data.

</details>


### [29] [Database Views as Explanations for Relational Deep Learning](https://arxiv.org/abs/2509.09482)
*Agapi Rissaki,Ilias Fountalis,Wolfgang Gatterbauer,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 本文提出解释关系数据库上机器学习模型的框架，以视图定义解释模型预测，在RelBench上评估表明解释有用且生成高效。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的关系数据库模型复杂，难以用人类可理解的方式解释其如何利用数据进行预测。

Method: 提出新颖框架，通过适配经典确定性概念建立全局溯因解释，可调节确定性与简洁性权衡及控制粒度；开发启发式算法，提出模型无关和针对异质图神经网络的技术。

Result: 在RelBench上的广泛实证研究表明，提出的解释有用，且生成效率高。

Conclusion: 所提框架能有效解释关系数据库上的机器学习模型，生成解释具有实用性和高效性。

Abstract: In recent years, there has been significant progress in the development of
deep learning models over relational databases, including architectures based
on heterogeneous graph neural networks (hetero-GNNs) and heterogeneous graph
transformers. In effect, such architectures state how the database records and
links (e.g., foreign-key references) translate into a large, complex numerical
expression, involving numerous learnable parameters. This complexity makes it
hard to explain, in human-understandable terms, how a model uses the available
data to arrive at a given prediction. We present a novel framework for
explaining machine-learning models over relational databases, where
explanations are view definitions that highlight focused parts of the database
that mostly contribute to the model's prediction. We establish such global
abductive explanations by adapting the classic notion of determinacy by Nash,
Segoufin, and Vianu (2010). In addition to tuning the tradeoff between
determinacy and conciseness, the framework allows controlling the level of
granularity by adopting different fragments of view definitions, such as ones
highlighting whole columns, foreign keys between tables, relevant groups of
tuples, and so on. We investigate the realization of the framework in the case
of hetero-GNNs. We develop heuristic algorithms that avoid the exhaustive
search over the space of all databases. We propose techniques that are
model-agnostic, and others that are tailored to hetero-GNNs via the notion of
learnable masking. Our approach is evaluated through an extensive empirical
study on the RelBench collection, covering a variety of domains and different
record-level tasks. The results demonstrate the usefulness of the proposed
explanations, as well as the efficiency of their generation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [30] [A Comparative Analysis of Identifier Schemes: UUIDv4, UUIDv7, and ULID for Distributed Systems](https://arxiv.org/abs/2509.08969)
*Nima Karimian Kakolaki*

Main category: cs.DC

TL;DR: 本文分析分布式标识符演变，对比传统自增键与UUIDv4、UUIDv7和ULIDs，实验表明ULIDs性能最优，相关代码和数据公开。


<details>
  <summary>Details</summary>
Motivation: 分布式系统需要健壮、可扩展的标识符方案来确保数据唯一性和高效索引，因此分析分布式标识符演变并对比不同方案。

Method: 结合碰撞概率的数学计算与模拟分布式环境中生成速度和网络传输开销的实证实验。

Result: ULIDs显著优于UUIDv4和UUIDv7，减少83.7%网络开销，提高97.32%生成速度，碰撞风险比UUIDv7低98.42%。

Conclusion: ULIDs是高性能分布式系统的最优选择，适用于可扩展应用，相关资源公开便于复现和拓展研究。

Abstract: Distributed systems require robust, scalable identifier schemes to ensure
data uniqueness and efficient indexing across multiple nodes. This paper
presents a comprehensive analysis of the evolution of distributed identifiers,
comparing traditional auto-increment keys with UUIDv4, UUIDv7, and ULIDs. We
combine mathematical calculation of collision probabilities with empirical
experiments measuring generation speed and network transmission overhead in a
simulated distributed environment. Results demonstrate that ULIDs significantly
outperform UUIDv4 and UUIDv7, reducing network overhead by 83.7% and increasing
generation speed by 97.32%. statistical analysis further shows ULIDs offer a
98.42% lower collision risk compared to UUIDv7, while maintaining negligible
collision probabilities even at high generation rates. These findings highlight
ULIDs as an optimal choice for high-performance distributed systems, providing
efficient, time-ordered, and lexicographically sortable identifiers suitable
for scalable applications. All source code, datasets, and analysis scripts
utilized in this research are publicly available in our dedicated repository at
https://github.com/nimakarimiank/uids-comparison. This repository contains
comprehensive documentation of the experimental setup, including configuration
files for the distributed environment, producer and consumer implementations,
and message broker integration. Additionally, it provides the data scripts and
datasets. Researchers and practitioners are encouraged to explore the
repository for full reproducibility of the experiments and to facilitate
further investigation or extension of the presented work.

</details>


### [31] [Optimizing the Variant Calling Pipeline Execution on Human Genomes Using GPU-Enabled Machines](https://arxiv.org/abs/2509.09058)
*Ajay Kumar,Praveen Rao,Peter Sanders*

Main category: cs.DC

TL;DR: 本文提出基于机器学习的方法优化人类基因组变异检测流程执行，在GPU机器上减少总执行时间，评估显示比贪心和动态方法有加速效果。


<details>
  <summary>Details</summary>
Motivation: 变异检测计算密集，为在GPU机器上高效执行人类基因组变异检测流程，减少总执行时间。

Method: 提出基于机器学习的方法，包括用ML预测变异检测流程各阶段执行时间，借鉴柔性作业车间调度问题生成最优执行计划，并跨机器同步执行。

Result: 能有效利用ML根据序列特征预测执行时间，相比贪心方法平均加速2倍，相比动态方法平均加速1.6倍。

Conclusion: 基于机器学习的优化方法能有效减少人类基因组变异检测流程的总执行时间。

Abstract: Variant calling is the first step in analyzing a human genome and aims to
detect variants in an individual's genome compared to a reference genome. Due
to the computationally-intensive nature of variant calling, genomic data are
increasingly processed in cloud environments as large amounts of compute and
storage resources can be acquired with the pay-as-you-go pricing model. In this
paper, we address the problem of efficiently executing a variant calling
pipeline for a workload of human genomes on graphics processing unit
(GPU)-enabled machines. We propose a novel machine learning (ML)-based approach
for optimizing the workload execution to minimize the total execution time. Our
approach encompasses two key techniques: The first technique employs ML to
predict the execution times of different stages in a variant calling pipeline
based on the characteristics of a genome sequence. Using the predicted times,
the second technique generates optimal execution plans for the machines by
drawing inspiration from the flexible job shop scheduling problem. The plans
are executed via careful synchronization across different machines. We
evaluated our approach on a workload of publicly available genome sequences
using a testbed with different types of GPU hardware. We observed that our
approach was effective in predicting the execution times of variant calling
pipeline stages using ML on features such as sequence size, read quality,
percentage of duplicate reads, and average read length. In addition, our
approach achieved 2X speedup (on an average) over a greedy approach that also
used ML for predicting the execution times on the tested workload of sequences.
Finally, our approach achieved 1.6X speedup (on an average) over a dynamic
approach that executed the workload based on availability of resources without
using any ML-based time predictions.

</details>


### [32] [Coherence-Aware Task Graph Modeling for Realistic Application](https://arxiv.org/abs/2509.09094)
*Guochu Xiong,Xiangzhong Luo,Weichen Liu*

Main category: cs.DC

TL;DR: 随着多核系统发展，缓存一致性对系统性能至关重要，现有任务图建模方法有局限，本文提出CoTAM框架，实验表明其优于隐式方法，体现了将缓存一致性纳入任务图建模的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基于预定义任务图的系统级设计策略对缺乏显式图、具有动态数据依赖行为的真实应用效果不佳，且现有任务图建模方法存在无法生成显式图、通用性不足、忽略一致性交互等问题。

Method: 提出CoTAM框架，通过将一致性影响从整体执行中分离、用学习的加权方案量化其影响、推断任务间依赖来构建反映运行时行为的统一任务图。

Result: 广泛实验显示CoTAM优于隐式方法，缩小了动态工作负载行为与现有设计之间的差距。

Conclusion: 将缓存一致性纳入任务图建模对准确和可推广的系统级分析很重要。

Abstract: As multicore systems continue to scale, cache coherence has emerged as a
critical determinant of system performance, with coherence behavior and task
execution closely intertwined, reshaping inter-task dependencies. Task graph
modeling provides a structured way to capture such dependencies and serves as
the foundation for many system-level design strategies. However, these
strategies typically rely on predefined task graphs, while many real-world
applications lack explicit graphs and exhibit dynamic, data-dependent behavior,
limiting the effectiveness of static approaches. To address this, several task
graph modeling methods for realistic workloads have been developed. Yet, they
either rely on implicit techniques that use application-specific features
without producing explicit graphs, or they generate graphs tailored to fixed
scheduling models, which limits generality. More importantly, they often
overlook coherence interactions, creating a gap between design assumptions and
actual runtime behavior. To overcome these limitations, we propose CoTAM, a
Coherence-Aware Task Graph Modeling framework for realistic workloads that
constructs a unified task graph reflecting runtime behavior. CoTAM analyzes the
impact of coherence by decoupling its effects from overall execution,
quantifies its influence through a learned weighting scheme, and infers
inter-task dependencies for coherence-aware graph generation. Extensive
experiments show that CoTAM outperforms implicit methods, bridging the gap
between dynamic workload behavior and existing designs while demonstrating the
importance of incorporating cache coherence into task graph modeling for
accurate and generalizable system-level analysis.

</details>


### [33] [WebAssembly and Unikernels: A Comparative Study for Serverless at the Edge](https://arxiv.org/abs/2509.09400)
*Valerio Besozzi,Enrico Fiasco,Marco Danelutto,Patrizio Dazzi*

Main category: cs.DC

TL;DR: 本文对比WebAssembly和基于unikernel的MicroVMs用于无服务器工作负载，展示Limes运行时并评估，得出不同技术在冷启动和执行性能上的表现。


<details>
  <summary>Details</summary>
Motivation: 边缘无服务器计算需要轻量级执行环境以减少冷启动延迟，尤其是在紧急边缘计算中。

Method: 提出基于Wasmtime构建的WebAssembly运行时Limes，并与基于Firecracker的SPARE环境进行对比评估。

Result: WebAssembly对轻量级函数冷启动时间短，但处理复杂工作负载不佳；Firecracker冷启动时间长但稳定，且在I/O密集型任务上执行性能更好。

Conclusion: 不同的无服务器执行环境在冷启动和执行性能方面各有优劣，应根据具体工作负载选择。

Abstract: Serverless computing at the edge requires lightweight execution environments
to minimize cold start latency, especially in Urgent Edge Computing (UEC). This
paper compares WebAssembly and unikernel-based MicroVMs for serverless
workloads. We present Limes, a WebAssembly runtime built on Wasmtime, and
evaluate it against the Firecracker-based environment used in SPARE. Results
show that WebAssembly offers lower cold start times for lightweight functions
but suffers with complex workloads, while Firecracker provides higher, but
stable, cold starts and better execution performance, particularly for
I/O-heavy tasks.

</details>


### [34] [Barycentric Coded Distributed Computing with Flexible Recovery Threshold for Collaborative Mobile Edge Computing](https://arxiv.org/abs/2509.09435)
*Houming Qiu,Kun Zhu,Dusit Niyato,Nguyen Cong Luong,Changyan Yi,Chen Dai*

Main category: cs.DC

TL;DR: 本文针对现有编码分布式计算（CDC）方案在协同移动边缘计算（MEC）中应对掉队节点的局限性，提出基于重心有理插值的近似 CDC 方案，实验表明该方案在等待时间和近似精度上优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有 CDC 方案在 MEC 系统中存在无法灵活解码和数值不稳定等局限，影响系统性能。

Method: 提出基于重心有理插值的近似 CDC 方案，该方案可利用任意返回结果解码、支持多字段计算、编码/解码函数无极点，还集成了基于 BRI 的梯度编码算法。

Result: 实验结果显示，该方案在等待时间和近似精度上优于现有 CDC 方案。

Conclusion: 所提出的近似 CDC 方案能有效解决现有 CDC 方案的局限性，提升 MEC 系统性能。

Abstract: Collaborative mobile edge computing (MEC) has emerged as a promising paradigm
to enable low-capability edge nodes to cooperatively execute
computation-intensive tasks. However, straggling edge nodes (stragglers)
significantly degrade the performance of MEC systems by prolonging computation
latency. While coded distributed computing (CDC) as an effective technique is
widely adopted to mitigate straggler effects, existing CDC schemes exhibit two
critical limitations: (i) They cannot successfully decode the final result
unless the number of received results reaches a fixed recovery threshold, which
seriously restricts their flexibility; (ii) They suffer from inherent poles in
their encoding/decoding functions, leading to decoding inaccuracies and
numerical instability in the computational results. To address these
limitations, this paper proposes an approximated CDC scheme based on
barycentric rational interpolation. The proposed CDC scheme offers several
outstanding advantages. Firstly, it can decode the final result leveraging any
returned results from workers. Secondly, it supports computations over both
finite and real fields while ensuring numerical stability. Thirdly, its
encoding/decoding functions are free of poles, which not only enhances
approximation accuracy but also achieves flexible accuracy tuning. Fourthly, it
integrates a novel BRI-based gradient coding algorithm accelerating the
training process while providing robustness against stragglers. Finally,
experimental results reveal that the proposed scheme is superior to existing
CDC schemes in both waiting time and approximate accuracy.

</details>


### [35] [Weaker Assumptions for Asymmetric Trust](https://arxiv.org/abs/2509.09493)
*Ignacio Amores-Sesar,Christian Cachin,Juan Villacis*

Main category: cs.DC

TL;DR: 本文指出在非对称信任分布式系统中，现有解决问题的假设过于严格，提出新方法解决可靠广播和共识问题，且方法可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有解决非对称信任分布式系统中基本问题的方法假设过于严格，消除了非对称信任的好处。

Method: 提出新方法来刻画非对称问题，并基于此给出可靠广播和共识算法，所需假设比之前的解决方案弱。

Result: 得到了可靠广播和共识算法，且方法具有通用性。

Conclusion: 新方法能在较弱假设下解决非对称信任系统中的问题，且可扩展到其他核心问题。

Abstract: In distributed systems with asymmetric trust, each participant is free to
make its own trust assumptions about others, captured by an asymmetric quorum
system. This contrasts with ordinary, symmetric quorum systems and threshold
models, where trust assumptions are uniformly shared among participants.
Fundamental problems like reliable broadcast and consensus are unsolvable in
the asymmetric model if quorum systems satisfy only the classical properties of
consistency and availability. Existing approaches overcome this by introducing
stronger assumptions. We show that some of these assumptions are overly
restrictive, so much so that they effectively eliminate the benefits of
asymmetric trust. To address this, we propose a new approach to characterize
asymmetric problems and, building upon it, present algorithms for reliable
broadcast and consensus that require weaker assumptions than previous
solutions. Our methods are general and can be extended to other core problems
in systems with asymmetric trust.

</details>


### [36] [TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes](https://arxiv.org/abs/2509.09525)
*Jialiang Huang,Teng Ma,Zheng Liu,Sixing Lin,Kang Chen,Jinlei Jiang,Xia Liao,Yingdi Shan,Yongwei Wu,Ning Zhang,Mengting Lu,Tao Ma,Haifeng Gong,Mingxing Zhang*

Main category: cs.DC

TL;DR: 论文指出无服务器计算对LLM代理存在开销瓶颈，提出TrEnv平台，可降低延迟和内存使用。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算的基础设施开销成为LLM代理等新兴工作负载的瓶颈，运行成本高，需要更高效、高密度的无服务器平台。

Method: 提出TrEnv平台，支持容器和基于VM的环境，通过可重复使用的沙箱和内存模板减少启动延迟和内存使用，还利用浏览器共享和页面缓存绕过机制减少基于VM的代理工作负载的开销。

Result: TrEnv在基于容器的设置中，P99延迟最多降低7倍，内存使用减少48%；与E2B等先进系统相比，基于VM的代理P99延迟最多降低58%，内存节省61%。

Conclusion: TrEnv平台能有效降低LLM代理在无服务器计算中的延迟和内存使用，提高效率。

Abstract: Serverless computing provides dynamic scalability, but its infrastructure
overhead becomes a bottleneck for emerging workloads such as LLM agents, which
exhibit unpredictable invocation patterns and variable resource demands. Our
analysis shows that for these agents, the cost of running on serverless
platforms can reach up to 70% of the cost of LLM API calls. This finding
motivates the need for a more efficient, high-density serverless platform. We
present TrEnv, a co-designed serverless platform that supports both container-
and VM-based environments, optimized for the unique demands of LLM agents.
TrEnv reduces startup latency and memory usage through repurposable sandboxes
and memory templates, which enable fast reuse and restoration of execution
environments. To further reduce overhead in VM-based agent workloads, TrEnv
leverages browser sharing and a page cache bypassing mechanism. Evaluations
show that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in
container-based settings, and achieves up to 58% lower P99 latency and 61%
memory savings for VM-based agents compared to state-of-the-art systems like
E2B.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [37] [Improved Approximation Guarantees and Hardness Results for MNL-Driven Product Ranking](https://arxiv.org/abs/2509.09180)
*Danny Segev,Gidi Steinberg*

Main category: cs.DS

TL;DR: 本文针对市场份额排名问题提出多项式时间近似方案，证明该问题强NP难。


<details>
  <summary>Details</summary>
Motivation: 解决Derakhshan等人（2022）提出的市场份额排名问题中的计算问题。

Method: 利用新技术和分析思路，结合修正前人见解，设计近似方案，引入黑箱归约。

Result: 得到优雅且易实现的准多项式时间近似方案。

Conclusion: 市场份额排名问题是强NP难的。

Abstract: In this paper, we address open computational questions regarding the market
share ranking problem, recently introduced by Derakhshan et al. (2022). Their
modelling framework incorporates the extremely popular Multinomial Logit (MNL)
choice model, along with a novel search-based consider-then-choose paradigm. In
a nutshell, the authors devised a Pandora's-Box-type search model, where
different customer segments sequentially screen through a ranked list of
products, one position after the other, forming their consideration set by
including all products viewed up until terminating their inspection procedure.
Subsequently, a purchasing decision out of this set is made based on a joint
MNL choice model.
  Our main contribution consists in devising a polynomial-time approximation
scheme for the market share ranking problem, utilizing fresh technical
developments and analytical ideas, in conjunction with revising the original
insights of Derakhshan et al. (2022). Along the way, we introduce a black-box
reduction, mapping general instances of the market share ranking problem into
``bounded ratio'' instances, showing that this result directly leads to an
elegant and easily-implementable quasi-PTAS. Finally, to provide a complete
computational characterization, we prove that the market share ranking problem
is strongly $\mathrm{NP}$-hard.

</details>


### [38] [Additive Approximation Schemes for Low-Dimensional Embeddings](https://arxiv.org/abs/2509.09652)
*Prashanti Anderson,Ainesh Bakshi,Samuel B. Hopkins*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the task of fitting low-dimensional embeddings to
high-dimensional data. In particular, we study the $k$-Euclidean Metric
Violation problem ($\textsf{$k$-EMV}$), where the input is $D \in
\mathbb{R}^{\binom{n}{2}}_{\geq 0}$ and the goal is to find the closest vector
$X \in \mathbb{M}_{k}$, where $\mathbb{M}_k \subset
\mathbb{R}^{\binom{n}{2}}_{\geq 0}$ is the set of all $k$-dimensional Euclidean
metrics on $n$ points, and closeness is formulated as the following
optimization problem, where $\| \cdot \|$ is the entry-wise $\ell_2$ norm: \[
  \textsf{OPT}_{\textrm{EMV}} = \min_{X \in \mathbb{M}_{k} } \Vert D - X
\Vert_2^2\,.\] Cayton and Dasgupta [CD'06] showed that this problem is NP-Hard,
even when $k=1$. Dhamdhere [Dha'04] obtained a $O(\log(n))$-approximation for
$\textsf{$1$-EMV}$ and leaves finding a PTAS for it as an open question
(reiterated recently by Lee [Lee'25]). Although $\textsf{$k$-EMV}$ has been
studied in the statistics community for over 70 years, under the name
"multi-dimensional scaling", there are no known efficient approximation
algorithms for $k > 1$, to the best of our knowledge.
  We provide the first polynomial-time additive approximation scheme for
$\textsf{$k$-EMV}$. In particular, we obtain an embedding with objective value
$\textsf{OPT}_{\textrm{EMV}} + \varepsilon \Vert D\Vert_2^2$ in $(n\cdot
B)^{\mathsf{poly}(k, \varepsilon^{-1})}$ time, where each entry in $D$ can be
represented by $B$ bits. We believe our algorithm is a crucial first step
towards obtaining a PTAS for $\textsf{$k$-EMV}$. Our key technical contribution
is a new analysis of correlation rounding for Sherali-Adams / Sum-of-Squares
relaxations, tailored to low-dimensional embeddings. We also show that our
techniques allow us to obtain additive approximation schemes for two related
problems: a weighted variant of $\textsf{$k$-EMV}$ and $\ell_p$ low-rank
approximation for $p>2$.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [39] [Toward a Multi-Echelon Cyber Warfare Theory: A Meta-Game-Theoretic Paradigm for Defense and Dominance](https://arxiv.org/abs/2509.08976)
*Ya-Ting Yang,Quanyan Zhu*

Main category: cs.GT

TL;DR: 本文提出用博弈论结合现代AI技术，为网络战提供统一框架以设计和优化策略，并以RedCyber为例说明，最后给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 网络战需整合防御和进攻技术形成连贯策略，以往研究侧重孤立战术或零散技术，缺乏整体理解，因此需要新方法。

Method: 运用博弈论建模攻击者 - 防御者交互，结合现代AI技术，设计和优化网络战多层次策略。

Result: 以RedCyber为例，展示博弈论方法能捕捉网络作战的相互依赖性。

Conclusion: 提出未来在弹性、跨层级规划和AI在网络战中角色演变等方面的研究方向。

Abstract: Cyber warfare has become a central element of modern conflict, especially
within multi-domain operations. As both a distinct and critical domain, cyber
warfare requires integrating defensive and offensive technologies into coherent
strategies. While prior research has emphasized isolated tactics or fragmented
technologies, a holistic understanding is essential for effective resource
deployment and risk mitigation. Game theory offers a unifying framework for
this purpose. It not only models attacker-defender interactions but also
provides quantitative tools for equilibrium analysis, risk assessment, and
strategic reasoning. Integrated with modern AI techniques, game-theoretic
models enable the design and optimization of strategies across multiple levels
of cyber warfare, from policy and strategy to operations, tactics, and
technical implementations. These models capture the paradoxical logic of
conflict, where more resources do not always translate into greater advantage,
and where nonlinear dynamics govern outcomes. To illustrate the approach, this
chapter examines RedCyber, a synthetic cyber conflict, demonstrating how
game-theoretic methods capture the interdependencies of cyber operations. The
chapter concludes with directions for future research on resilience,
cros-echelon planning, and the evolving role of AI in cyber warfare.

</details>


### [40] [Persuasion Gains and Losses from Peer Communication](https://arxiv.org/abs/2509.09099)
*Toygar T. Kerman,Anastas P. Tenev,Konstantin Zabarnyi*

Main category: cs.GT

TL;DR: 研究贝叶斯说服场景下发送者效用与接收者间通信的关系，指出网络扩展不一定单调影响发送者收益，某些网络扩展可使发送者效用达上限，部分网络结构修改会排除有益扩展可能。


<details>
  <summary>Details</summary>
Motivation: 探究发送者在贝叶斯说服场景下，其预期效用如何随接收者间通信增加而变化。

Method: 研究位于通信网络上的同质二元行动接收者，发送者向其透露部分状态信息，分析不同网络结构下的情况。

Result: 对于一般网络族，扩展网络可使发送者严格受益，发送者说服收益与网络密度非单调关系；许多网络扩展能使发送者预期效用达上限；部分网络结构稍作修改会排除有益扩展可能。

Conclusion: 不应假定更多通信必然带来更好的集体结果。

Abstract: We study a Bayesian persuasion setting in which a sender wants to persuade a
critical mass of receivers by revealing partial information about the state to
them. The homogeneous binary-action receivers are located on a communication
network, and each observes the private messages sent to them and their
immediate neighbors. We examine how the sender's expected utility varies with
increased communication among receivers. We show that for general families of
networks, extending the network can strictly benefit the sender. Thus, the
sender's gain from persuasion is not monotonic in network density. Moreover,
many network extensions can achieve the upper bound on the sender's expected
utility among all networks, which corresponds to the payoff in an empty
network. This is the case in networks reflecting a clear informational
hierarchy (e.g., in global corporations), as well as in decentralized networks
in which information originates from multiple sources (e.g., influencers in
social media). Finally, we show that a slight modification to the structure of
some of these networks precludes the possibility of such beneficial extensions.
Overall, our results caution against presuming that more communication
necessarily leads to better collective outcomes.

</details>


### [41] [Mechanism Design with Outliers and Predictions](https://arxiv.org/abs/2509.09561)
*Argyrios Deligkas,Eduard Eiben,Sophie Klumper,Guido Schäfer,Artem Tsikiridis*

Main category: cs.GT

TL;DR: 本文研究含异常值的机制设计，以直线上的设施选址为例，推导确定性策略证明机制在功利主义和平均主义社会成本目标下的边界，发现丢弃异常值未必提高效率。


<details>
  <summary>Details</summary>
Motivation: 研究当部分参与者有极端或非典型偏好时，可从社会成本目标中剔除z个参与者的机制设计。

Method: 针对功利主义和平均主义社会成本目标，推导确定性策略证明机制的边界，还建立了期望诚实的随机机制的下界。

Result: 当z ≥ n/2时，无策略证明机制能对任一目标实现有界近似；对于平均主义成本，选择第(z + 1)阶统计量是策略证明且2 - 近似的；对于功利主义成本，策略证明机制无法有效利用异常值，近似保证随异常值数量增加而变差。

Conclusion: 丢弃异常值不一定能提高效率，不同成本目标下机制设计有不同特点，有预测时可在一致性和鲁棒性间实现最佳权衡。

Abstract: We initiate the study of mechanism design with outliers, where the designer
can discard $z$ agents from the social cost objective. This setting is
particularly relevant when some agents exhibit extreme or atypical preferences.
As a natural case study, we consider facility location on the line: $n$
strategic agents report their preferred locations, and a mechanism places a
facility to minimize a social cost function. In our setting, the $z$ agents
farthest from the chosen facility are excluded from the social cost. While it
may seem intuitive that discarding outliers improves efficiency, our results
reveal that the opposite can hold.
  We derive tight bounds for deterministic strategyproof mechanisms under the
two most-studied objectives: utilitarian and egalitarian social cost. Our
results offer a comprehensive view of the impact of outliers. We first show
that when $z \ge n/2$, no strategyproof mechanism can achieve a bounded
approximation for either objective. For egalitarian cost, selecting the $(z +
1)$-th order statistic is strategyproof and 2-approximate. In fact, we show
that this is best possible by providing a matching lower bound. Notably, this
lower bound of 2 persists even when the mechanism has access to a prediction of
the optimal location, in stark contrast to the setting without outliers. For
utilitarian cost, we show that strategyproof mechanisms cannot effectively
exploit outliers, leading to the counterintuitive outcome that approximation
guarantees worsen as the number of outliers increases. However, in this case,
access to a prediction allows us to design a strategyproof mechanism achieving
the best possible trade-off between consistency and robustness. Finally, we
also establish lower bounds for randomized mechanisms that are truthful in
expectation.

</details>


### [42] [Maximizing social welfare among EF1 allocations at the presence of two types of agents](https://arxiv.org/abs/2509.09641)
*Jiaxuan Ma,Yong Chen,Guangting Chen,Mingyang Gong,Guohui Lin,An Zhang*

Main category: cs.GT

TL;DR: 研究两种效用函数下不可分物品公平分配以最大化功利主义社会福利，给出不同情况下近似算法并改进前人结果。


<details>
  <summary>Details</summary>
Motivation: 在公平标准为至多忌妒一件物品且只有两种效用函数时，提高不可分物品分配最大化功利主义社会福利的近似比。

Method: 设计不同情况下的近似算法，如两种效用函数归一化时的2 - 近似算法、n = 3时不同情况的近似算法。

Result: 在两种效用函数归一化时将近似比从16√n 提高到2；n = 3时，归一化时得到5/3 - 近似算法，未归一化时得到2 - 近似算法。

Conclusion: 在该特殊情况下，常数比近似算法证实了之前被证明为APX - 难问题的APX - 完备性，且给出了n = 3时不同情况的最优近似算法。

Abstract: We study the fair allocation of indivisible items to $n$ agents to maximize
the utilitarian social welfare, where the fairness criterion is envy-free up to
one item and there are only two different utility functions shared by the
agents. We present a $2$-approximation algorithm when the two utility functions
are normalized, improving the previous best ratio of $16 \sqrt{n}$ shown for
general normalized utility functions; thus this constant ratio approximation
algorithm confirms the APX-completeness in this special case previously shown
APX-hard. When there are only three agents, i.e., $n = 3$, the previous best
ratio is $3$ shown for general utility functions, and we present an improved
and tight $\frac 53$-approximation algorithm when the two utility functions are
normalized, and a best possible and tight $2$-approximation algorithm when the
two utility functions are unnormalized.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [43] [Generative Engine Optimization: How to Dominate AI Search](https://arxiv.org/abs/2509.08919)
*Mahe Chen,Xiaoxuan Wang,Kaiwen Chen,Nick Koudas*

Main category: cs.IR

TL;DR: 随着生成式AI搜索引擎兴起，传统SEO面临挑战，本文对比AI搜索和传统搜索，揭示AI搜索特点并制定GEO战略。


<details>
  <summary>Details</summary>
Motivation: 生成式AI搜索引擎改变信息检索方式，挑战传统SEO，需新范式GEO。

Method: 通过多领域、多语言、多查询表述的大规模对照实验，对比AI搜索和谷歌传统搜索获取信息的差异。

Result: AI搜索更偏向第三方权威来源，不同AI搜索服务在领域多样性、新鲜度等方面有显著差异。

Conclusion: 制定GEO战略议程，为从业者提供行动指南，为新搜索环境提供基础分析和战略框架。

Abstract: The rapid adoption of generative AI-powered search engines like ChatGPT,
Perplexity, and Gemini is fundamentally reshaping information retrieval, moving
from traditional ranked lists to synthesized, citation-backed answers. This
shift challenges established Search Engine Optimization (SEO) practices and
necessitates a new paradigm, which we term Generative Engine Optimization
(GEO).
  This paper presents a comprehensive comparative analysis of AI Search and
traditional web search (Google). Through a series of large-scale, controlled
experiments across multiple verticals, languages, and query paraphrases, we
quantify critical differences in how these systems source information. Our key
findings reveal that AI Search exhibit a systematic and overwhelming bias
towards Earned media (third-party, authoritative sources) over Brand-owned and
Social content, a stark contrast to Google's more balanced mix. We further
demonstrate that AI Search services differ significantly from each other in
their domain diversity, freshness, cross-language stability, and sensitivity to
phrasing.
  Based on these empirical results, we formulate a strategic GEO agenda. We
provide actionable guidance for practitioners, emphasizing the critical need
to: (1) engineer content for machine scannability and justification, (2)
dominate earned media to build AI-perceived authority, (3) adopt
engine-specific and language-aware strategies, and (4) overcome the inherent
"big brand bias" for niche players. Our work provides the foundational
empirical analysis and a strategic framework for achieving visibility in the
new generative search landscape.

</details>


### [44] [Envy-Free but Still Unfair: Envy-Freeness Up To One Item (EF-1) in Personalized Recommendation](https://arxiv.org/abs/2509.09037)
*Amanda Aird,Ben Armstrong,Nicholas Mattei,Robin Burke*

Main category: cs.IR

TL;DR: 本文概述嫉妒无关性及其在经济学和推荐系统中的应用，说明嫉妒不适用于个性化场景衡量公平性。


<details>
  <summary>Details</summary>
Motivation: 介绍嫉妒无关性概念在不同领域的应用情况，并探讨其在个性化场景衡量公平性的适用性。

Method: 综述已有文献概念并进行分析

Result: 指出嫉妒不适合在个性化发挥作用的场景中衡量公平性。

Conclusion: 嫉妒无关性虽在多领域有应用，但在个性化场景衡量公平性存在问题。

Abstract: Envy-freeness and the relaxation to Envy-freeness up to one item (EF-1) have
been used as fairness concepts in the economics, game theory, and social choice
literatures since the 1960s, and have recently gained popularity within the
recommendation systems communities. In this short position paper we will give
an overview of envy-freeness and its use in economics and recommendation
systems; and illustrate why envy is not appropriate to measure fairness for use
in settings where personalization plays a role.

</details>


### [45] [Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation](https://arxiv.org/abs/2509.09114)
*Kelin Ren,Chan-Yang Ju,Dong-Ho Lee*

Main category: cs.IR

TL;DR: 提出MambaRec框架解决多模态推荐系统现有方法问题，实验显示其在融合质量、泛化性和效率上优于现有方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐系统方法存在难以建模细粒度跨模态关联和缺乏全局分布一致性的问题。

Method: 提出MambaRec框架，引入DREAM模块进行局部特征对齐，应用MMD和对比损失函数进行全局分布正则化，采用降维策略提高可扩展性。

Result: 在真实电商数据集上的实验表明，MambaRec在融合质量、泛化性和效率上优于现有方法。

Conclusion: MambaRec能有效解决现有多模态推荐系统的问题，有更好的性能。

Abstract: Multimodal recommendation systems are increasingly becoming foundational
technologies for e-commerce and content platforms, enabling personalized
services by jointly modeling users' historical behaviors and the multimodal
features of items (e.g., visual and textual). However, most existing methods
rely on either static fusion strategies or graph-based local interaction
modeling, facing two critical limitations: (1) insufficient ability to model
fine-grained cross-modal associations, leading to suboptimal fusion quality;
and (2) a lack of global distribution-level consistency, causing
representational bias. To address these, we propose MambaRec, a novel framework
that integrates local feature alignment and global distribution regularization
via attention-guided learning. At its core, we introduce the Dilated Refinement
Attention Module (DREAM), which uses multi-scale dilated convolutions with
channel-wise and spatial attention to align fine-grained semantic patterns
between visual and textual modalities. This module captures hierarchical
relationships and context-aware associations, improving cross-modal semantic
modeling. Additionally, we apply Maximum Mean Discrepancy (MMD) and contrastive
loss functions to constrain global modality alignment, enhancing semantic
consistency. This dual regularization reduces mode-specific deviations and
boosts robustness. To improve scalability, MambaRec employs a dimensionality
reduction strategy to lower the computational cost of high-dimensional
multimodal features. Extensive experiments on real-world e-commerce datasets
show that MambaRec outperforms existing methods in fusion quality,
generalization, and efficiency. Our code has been made publicly available at
https://github.com/rkl71/MambaRec.

</details>


### [46] [CESRec: Constructing Pseudo Interactions for Sequential Recommendation via Conversational Feedback](https://arxiv.org/abs/2509.09342)
*Yifan Wang,Shen Gao,Jiabao Fang,Rui Yan,Billy Chiu,Shuo Shang*

Main category: cs.IR

TL;DR: 提出CESRec框架结合SRS与CRS优势，通过语义伪交互构建和双对齐离群项掩码技术，实验证明其能提升SRS性能。


<details>
  <summary>Details</summary>
Motivation: 现有SRS方法难捕捉实时偏好，CRS忽略历史行为，需结合两者优势。

Method: 提出CESRec框架，引入语义伪交互构建更新历史交互序列，提出双对齐离群项掩码减少离群项影响。

Result: CESRec通过提升强SRS模型达到了当前最优性能。

Conclusion: CESRec在将对话反馈集成到SRS中是有效的。

Abstract: Sequential Recommendation Systems (SRS) have become essential in many
real-world applications. However, existing SRS methods often rely on
collaborative filtering signals and fail to capture real-time user preferences,
while Conversational Recommendation Systems (CRS) excel at eliciting immediate
interests through natural language interactions but neglect historical
behavior. To bridge this gap, we propose CESRec, a novel framework that
integrates the long-term preference modeling of SRS with the real-time
preference elicitation of CRS. We introduce semantic-based pseudo interaction
construction, which dynamically updates users'historical interaction sequences
by analyzing conversational feedback, generating a pseudo-interaction sequence
that seamlessly combines long-term and real-time preferences. Additionally, we
reduce the impact of outliers in historical items that deviate from users'core
preferences by proposing dual alignment outlier items masking, which identifies
and masks such items using semantic-collaborative aligned representations.
Extensive experiments demonstrate that CESRec achieves state-of-the-art
performance by boosting strong SRS models, validating its effectiveness in
integrating conversational feedback into SRS.

</details>


### [47] [We're Still Doing It (All) Wrong: Recommender Systems, Fifteen Years Later](https://arxiv.org/abs/2509.09414)
*Alan Said,Maria Soledad Pera,Michael D. Ekstrand*

Main category: cs.IR

TL;DR: 本文重提Amatriain对推荐系统研究的批判，指出问题仍存在，强调需要根本性重构研究议程。


<details>
  <summary>Details</summary>
Motivation: Amatriain曾批判推荐系统研究存在问题，如今问题仍以更隐蔽或系统的形式存在，需要重新审视。

Method: 借鉴可重复性、评估方法、环境影响和参与式设计等方面的研究，展示领域复杂性超越自省的情况，介绍社区主导的变革举措。

Result: 领域复杂性超过了自省，有社区主导的变革举措在尝试转变范式。

Conclusion: 有意义的变革不仅需要新指标和更好的工具，还需从根本上重构推荐系统研究的目标、服务对象以及知识的产生和验证方式，呼吁基于认知谦逊、人类影响和可持续实践的研究议程。

Abstract: In 2011, Xavier Amatriain sounded the alarm: recommender systems research was
"doing it all wrong" [1]. His critique, rooted in statistical misinterpretation
and methodological shortcuts, remains as relevant today as it was then. But
rather than correcting course, we added new layers of sophistication on top of
the same broken foundations. This paper revisits Amatriain's diagnosis and
argues that many of the conceptual, epistemological, and infrastructural
failures he identified still persist, in more subtle or systemic forms. Drawing
on recent work in reproducibility, evaluation methodology, environmental
impact, and participatory design, we showcase how the field's accelerating
complexity has outpaced its introspection. We highlight ongoing community-led
initiatives that attempt to shift the paradigm, including workshops, evaluation
frameworks, and calls for value-sensitive and participatory research. At the
same time, we contend that meaningful change will require not only new metrics
or better tooling, but a fundamental reframing of what recommender systems
research is for, who it serves, and how knowledge is produced and validated.
Our call is not just for technical reform, but for a recommender systems
research agenda grounded in epistemic humility, human impact, and sustainable
practice.

</details>


### [48] [Boosting Data Utilization for Multilingual Dense Retrieval](https://arxiv.org/abs/2509.09459)
*Chao Huang,Fengran Mo,Yufeng Chen,Changhao Guan,Zhenrui Yue,Xinyu Wang,Jinan Xu,Kaiyu Huang*

Main category: cs.IR

TL;DR: 提出提升多语言密集检索数据利用率的方法，在MIRACL基准测试中优于多个基线。


<details>
  <summary>Details</summary>
Motivation: 多语言密集检索需在共享向量空间对齐不同语言表示，现有对比学习方法依赖负样本质量和小批量数据有效性。

Method: 通过获取高质量难负样本和有效小批量数据来提升多语言密集检索的数据利用率。

Result: 在包含16种语言的MIRACL多语言检索基准测试中，该方法优于多个现有强基线。

Conclusion: 所提出的提升数据利用率的方法对于多语言密集检索是有效的。

Abstract: Multilingual dense retrieval aims to retrieve relevant documents across
different languages based on a unified retriever model. The challenge lies in
aligning representations of different languages in a shared vector space. The
common practice is to fine-tune the dense retriever via contrastive learning,
whose effectiveness highly relies on the quality of the negative sample and the
efficacy of mini-batch data. Different from the existing studies that focus on
developing sophisticated model architecture, we propose a method to boost data
utilization for multilingual dense retrieval by obtaining high-quality hard
negative samples and effective mini-batch data. The extensive experimental
results on a multilingual retrieval benchmark, MIRACL, with 16 languages
demonstrate the effectiveness of our method by outperforming several existing
strong baselines.

</details>


### [49] [AskDoc -- Identifying Hidden Healthcare Disparities](https://arxiv.org/abs/2509.09622)
*Shashank Gupta*

Main category: cs.IR

TL;DR: 研究通过Reddit社区AskDoc分析在线医生咨询服务，发现部分帖子无回复，用户多为20 - 39岁白人男性，医患参与有差异。


<details>
  <summary>Details</summary>
Motivation: 了解互联网平台在线医生咨询服务的医疗建议，研究平台是否反映不同人群医疗障碍和偏见。

Method: 下载AskDoc 2020年1月至2022年5月的数据，用正则表达式识别帖子中自我报告的人口统计信息，进行统计分析。

Result: 一半帖子无回复，至少90%的人透露性别和年龄，80%的人不透露种族，社区以20 - 39岁白人男性为主，不同人口特征用户参与有差异，医生参与度低于发帖者。

Conclusion: 社交媒体可拉近医患距离，但目前医生参与度较低。

Abstract: The objective of this study is to understand the online Ask the Doctor
services medical advice on internet platforms via AskDoc, a Reddit community
that serves as a public AtD platform and study if platforms mirror existing
hurdles and partiality in healthcare across various demographic groups. We
downloaded data from January 2020 to May 2022 from AskDoc -- a subreddit, and
created regular expressions to identify self-reported demographics (Gender,
Race, and Age) from the posts, and performed statistical analysis to understand
the interaction between peers and physicians with the posters. Half of the
posts did not receive comments from peers or physicians. At least 90% of the
people disclose their gender and age, and 80% of the people do not disclose
their race. It was observed that the subreddit is dominated by adult (age group
20-39) white males. Some disparities were observed in the engagement between
the users and the posters with certain demographics. Beyond the confines of
clinics and hospitals, social media could bring patients and providers closer
together, however, as observed, current physicians participation is low
compared to posters.

</details>


### [50] [Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations](https://arxiv.org/abs/2509.09651)
*Zakaria El Kassimi,Fares Fourati,Mohamed-Slim Alouini*

Main category: cs.IR

TL;DR: 研究无线电法规领域问答，提出特定RAG管道和评估集，定义检索指标，结果显示该方法提升检索和生成准确率，代码等公开。


<details>
  <summary>Details</summary>
Motivation: 解决无线电法规这一法律敏感且高风险领域的问答问题。

Method: 提出电信特定的RAG管道，构建该领域首个选择题评估集，定义领域特定检索指标。

Result: 检索器准确率约97%，该方法提升所有测试模型生成准确率，如使GPT - 4o相对提升近12%。

Conclusion: 精心定位的基础方法为法规问答提供简单有效的特定领域解决方案。

Abstract: We study question answering in the domain of radio regulations, a legally
sensitive and high-stakes area. We propose a telecom-specific
Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,
the first multiple-choice evaluation set for this domain, constructed from
authoritative sources using automated filtering and human validation. To assess
retrieval quality, we define a domain-specific retrieval metric, under which
our retriever achieves approximately 97% accuracy. Beyond retrieval, our
approach consistently improves generation accuracy across all tested models. In
particular, while naively inserting documents without structured retrieval
yields only marginal gains for GPT-4o (less than 1%), applying our pipeline
results in nearly a 12% relative improvement. These findings demonstrate that
carefully targeted grounding provides a simple yet strong baseline and an
effective domain-specific solution for regulatory question answering. All code
and evaluation scripts, along with our derived question-answer dataset, are
available at https://github.com/Zakaria010/Radio-RAG.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [51] [Uncertainty Estimation using Variance-Gated Distributions](https://arxiv.org/abs/2509.08846)
*H. Martin Gillis,Isaac Xu,Thomas Trappenberg*

Main category: cs.LG

TL;DR: 提出基于信噪比的不确定性估计和分解框架，引入方差门控度量并讨论委员会机器多样性崩溃。


<details>
  <summary>Details</summary>
Motivation: 神经网络逐样本不确定性量化评估对高风险应用决策至关重要，但现有加法分解方法遭质疑。

Method: 提出基于不同模型预测类别概率分布信噪比的不确定性估计和分解框架，引入方差门控度量。

Result: 引入方差门控度量并用于讨论委员会机器多样性崩溃情况。

Conclusion: 所提框架和度量有助于神经网络不确定性的估计和分解。

Abstract: Evaluation of per-sample uncertainty quantification from neural networks is
essential for decision-making involving high-risk applications. A common
approach is to use the predictive distribution from Bayesian or approximation
models and decompose the corresponding predictive uncertainty into epistemic
(model-related) and aleatoric (data-related) components. However, additive
decomposition has recently been questioned. In this work, we propose an
intuitive framework for uncertainty estimation and decomposition based on the
signal-to-noise ratio of class probability distributions across different model
predictions. We introduce a variance-gated measure that scales predictions by a
confidence factor derived from ensembles. We use this measure to discuss the
existence of a collapse in the diversity of committee machines.

</details>


### [52] [Instance-Optimal Matrix Multiplicative Weight Update and Its Quantum Applications](https://arxiv.org/abs/2509.08911)
*Weiyuan Gong,Tongyang Li,Xinzhao Wang,Zhiyu Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Matrix Multiplicative Weight Update (MMWU) is a seminal online learning
algorithm with numerous applications. Applied to the matrix version of the
Learning from Expert Advice (LEA) problem on the $d$-dimensional spectraplex,
it is well known that MMWU achieves the minimax-optimal regret bound of
$O(\sqrt{T\log d})$, where $T$ is the time horizon. In this paper, we present
an improved algorithm achieving the instance-optimal regret bound of
$O(\sqrt{T\cdot S(X||d^{-1}I_d)})$, where $X$ is the comparator in the regret,
$I_d$ is the identity matrix, and $S(\cdot||\cdot)$ denotes the quantum
relative entropy. Furthermore, our algorithm has the same computational
complexity as MMWU, indicating that the improvement in the regret bound is
``free''.
  Technically, we first develop a general potential-based framework for matrix
LEA, with MMWU being its special case induced by the standard exponential
potential. Then, the crux of our analysis is a new ``one-sided'' Jensen's trace
inequality built on a Laplace transform technique, which allows the application
of general potential functions beyond exponential to matrix LEA. Our algorithm
is finally induced by an optimal potential function from the vector LEA
problem, based on the imaginary error function.
  Complementing the above, we provide a memory lower bound for matrix LEA, and
explore the applications of our algorithm in quantum learning theory. We show
that it outperforms the state of the art for learning quantum states corrupted
by depolarization noise, random quantum states, and Gibbs states. In addition,
applying our algorithm to linearized convex losses enables predicting nonlinear
quantum properties, such as purity, quantum virtual cooling, and R\'{e}nyi-$2$
correlation.

</details>


### [53] [Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates](https://arxiv.org/abs/2509.08933)
*Sreejeet Maity,Aritra Mitra*

Main category: cs.LG

TL;DR: 提出抗对抗性干扰的Q学习算法变体，分析有限时间收敛率并给出信息论下界，还提出无需先验知识的算法变体，为异步Q学习提供首个有限时间鲁棒性保证。


<details>
  <summary>Details</summary>
Motivation: 奖励信号受对抗性干扰会严重降低经典Q学习算法性能，需解决该挑战。

Method: 提出新的Q学习算法变体，在异步采样模型下分析其收敛率，推导信息论下界，利用改进的Azuma - Hoeffding不等式分析无先验知识的算法变体。

Result: 算法在对抗性干扰下有限时间收敛率与非对抗情况匹配（有与受损样本比例成正比的附加项），证明附加项不可避免。

Conclusion: 为异步Q学习提供了首个有限时间鲁棒性保证，填补了鲁棒强化学习的重要空白。

Abstract: We consider the problem of learning the optimal policy in a discounted,
infinite-horizon reinforcement learning (RL) setting where the reward signal is
subject to adversarial corruption. Such corruption, which may arise from
extreme noise, sensor faults, or malicious attacks, can severely degrade the
performance of classical algorithms such as Q-learning. To address this
challenge, we propose a new provably robust variant of the Q-learning algorithm
that operates effectively even when a fraction of the observed rewards are
arbitrarily perturbed by an adversary. Under the asynchronous sampling model
with time-correlated data, we establish that despite adversarial corruption,
the finite-time convergence rate of our algorithm matches that of existing
results for the non-adversarial case, up to an additive term proportional to
the fraction of corrupted samples. Moreover, we derive an information-theoretic
lower bound revealing that the additive corruption term in our upper bounds is
unavoidable.
  Next, we propose a variant of our algorithm that requires no prior knowledge
of the statistics of the true reward distributions. The analysis of this
setting is particularly challenging and is enabled by carefully exploiting a
refined Azuma-Hoeffding inequality for almost-martingales, a technical tool
that might be of independent interest. Collectively, our contributions provide
the first finite-time robustness guarantees for asynchronous Q-learning,
bridging a significant gap in robust RL.

</details>


### [54] [Group Distributionally Robust Machine Learning under Group Level Distributional Uncertainty](https://arxiv.org/abs/2509.08942)
*Xenia Konti,Yi Shen,Zifan Wang,Karl Henrik Johansson,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: 提出基于Wasserstein的分布鲁棒优化框架，解决多源异质数据中机器学习模型性能问题并验证有效性。


<details>
  <summary>Details</summary>
Motivation: 标准机器学习方法在多源异质数据中学习到虚假关联，影响非典型或代表性不足群体性能，且现有方法假设数据分布可准确估计，在复杂环境不适用。

Method: 提出基于Wasserstein的分布鲁棒优化框架，开发梯度下降 - 上升算法求解DRO问题。

Result: 得到算法收敛结果，在真实数据上验证了方法有效性。

Conclusion: 所提框架能解决多源异质数据中分布不确定性问题，提高最差群体性能。

Abstract: The performance of machine learning (ML) models critically depends on the
quality and representativeness of the training data. In applications with
multiple heterogeneous data generating sources, standard ML methods often learn
spurious correlations that perform well on average but degrade performance for
atypical or underrepresented groups. Prior work addresses this issue by
optimizing the worst-group performance. However, these approaches typically
assume that the underlying data distributions for each group can be accurately
estimated using the training data, a condition that is frequently violated in
noisy, non-stationary, and evolving environments. In this work, we propose a
novel framework that relies on Wasserstein-based distributionally robust
optimization (DRO) to account for the distributional uncertainty within each
group, while simultaneously preserving the objective of improving the
worst-group performance. We develop a gradient descent-ascent algorithm to
solve the proposed DRO problem and provide convergence results. Finally, we
validate the effectiveness of our method on real-world data.

</details>


### [55] [FoundationalECGNet: A Lightweight Foundational Model for ECG-based Multitask Cardiac Analysis](https://arxiv.org/abs/2509.08961)
*Md. Sajeebul Islam Sk.,Md Jobayer,Md Mehedi Hasan Shawon,Md. Golam Raibul Alam*

Main category: cs.LG

TL;DR: 提出FoundationalECGNet用于自动心电图分类，在多数据集表现优异，有临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，当前心电图分析方法受噪声、类别不平衡和数据集异质性限制。

Method: 提出FoundationalECGNet框架，集成小波变换、注意力模块、图注意力网络和时间序列变压器，先区分正常和异常信号，再对异常信号分类。

Result: 在多数据集上，正常与异常分类F1分数达99%，多类疾病检测表现优异，还能提供风险等级估计。

Conclusion: FoundationalECGNet是可扩展、可解释、可泛化的心电图分析解决方案，有望提高诊断精度和患者预后。

Abstract: Cardiovascular diseases (CVDs) remain a leading cause of mortality worldwide,
underscoring the importance of accurate and scalable diagnostic systems.
Electrocardiogram (ECG) analysis is central to detecting cardiac abnormalities,
yet challenges such as noise, class imbalance, and dataset heterogeneity limit
current methods. To address these issues, we propose FoundationalECGNet, a
foundational framework for automated ECG classification. The model integrates a
dual-stage denoising by Morlet and Daubechies wavelets transformation,
Convolutional Block Attention Module (CBAM), Graph Attention Networks (GAT),
and Time Series Transformers (TST) to jointly capture spatial and temporal
dependencies in multi-channel ECG signals. FoundationalECGNet first
distinguishes between Normal and Abnormal ECG signals, and then classifies the
Abnormal signals into one of five cardiac conditions: Arrhythmias, Conduction
Disorders, Myocardial Infarction, QT Abnormalities, or Hypertrophy. Across
multiple datasets, the model achieves a 99% F1-score for Normal vs. Abnormal
classification and shows state-of-the-art performance in multi-class disease
detection, including a 99% F1-score for Conduction Disorders and Hypertrophy,
as well as a 98.9% F1-score for Arrhythmias. Additionally, the model provides
risk level estimations to facilitate clinical decision-making. In conclusion,
FoundationalECGNet represents a scalable, interpretable, and generalizable
solution for automated ECG analysis, with the potential to improve diagnostic
precision and patient outcomes in healthcare settings. We'll share the code
after acceptance.

</details>


### [56] [Value bounds and Convergence Analysis for Averages of LRP attributions](https://arxiv.org/abs/2509.08963)
*Alexander Binder,Nastaran Takmil-Homayouni,Urun Dogan*

Main category: cs.LG

TL;DR: 本文通过将LRP型归因方法表示为修改后的梯度矩阵乘积，分析其数值特性，推导奇异值和归因图值的界，得到控制归因经验均值收敛的常数，揭示LRP - beta的特性。


<details>
  <summary>Details</summary>
Motivation: 分析LRP型归因方法的数值特性，了解归因值分布。

Method: 将LRP型归因方法表示为修改后的梯度矩阵乘积，类比雅可比矩阵乘法，推导奇异值和分量界。

Result: 得到控制归因经验均值收敛到期望的乘法常数，发现LRP - beta的常数与权重范数无关。

Conclusion: 研究结果对非几何数据增强和Smoothgrad型归因方法有重要意义，LRP - beta与基于梯度的方法和LRP - epsilon有显著区别。

Abstract: We analyze numerical properties of Layer-wise relevance propagation
(LRP)-type attribution methods by representing them as a product of modified
gradient matrices. This representation creates an analogy to matrix
multiplications of Jacobi-matrices which arise from the chain rule of
differentiation. In order to shed light on the distribution of attribution
values, we derive upper bounds for singular values. Furthermore we derive
component-wise bounds for attribution map values. As a main result, we apply
these component-wise bounds to obtain multiplicative constants. These constants
govern the convergence of empirical means of attributions to expectations of
attribution maps. This finding has important implications for scenarios where
multiple non-geometric data augmentations are applied to individual test
samples, as well as for Smoothgrad-type attribution methods. In particular, our
analysis reveals that the constants for LRP-beta remain independent of weight
norms, a significant distinction from both gradient-based methods and
LRP-epsilon.

</details>


### [57] [Green Federated Learning via Carbon-Aware Client and Time Slot Scheduling](https://arxiv.org/abs/2509.08980)
*Daniel Richards Arputharaj,Charlotte Rodriguez,Angelo Rodio,Giovanni Neglia*

Main category: cs.LG

TL;DR: 本文研究联邦学习中通过碳感知客户端选择和训练调度减少碳排放，构建了碳感知调度器，实验表明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大规模机器学习模型训练会产生大量碳排放，联邦学习可利用碳强度的区域和时间差异，因此研究如何在联邦学习中减少排放。

Method: 先量化利用空闲时间的碳感知调度策略的减排量，再研究该调度的性能权衡，最后构建集成空闲时间、α - 公平碳分配和全局微调阶段的碳感知调度器。

Result: 在真实世界碳强度数据上的实验显示，调度器在各种碳预算下都能实现更高的模型精度，在严格碳约束下优势明显。

Conclusion: 所构建的碳感知调度器优于不考虑空闲时间的基线方法，能在减少碳排放的同时提高模型精度。

Abstract: Training large-scale machine learning models incurs substantial carbon
emissions. Federated Learning (FL), by distributing computation across
geographically dispersed clients, offers a natural framework to leverage
regional and temporal variations in Carbon Intensity (CI). This paper
investigates how to reduce emissions in FL through carbon-aware client
selection and training scheduling. We first quantify the emission savings of a
carbon-aware scheduling policy that leverages slack time -- permitting a modest
extension of the training duration so that clients can defer local training
rounds to lower-carbon periods. We then examine the performance trade-offs of
such scheduling which stem from statistical heterogeneity among clients,
selection bias in participation, and temporal correlation in model updates. To
leverage these trade-offs, we construct a carbon-aware scheduler that
integrates slack time, $\alpha$-fair carbon allocation, and a global
fine-tuning phase. Experiments on real-world CI data show that our scheduler
outperforms slack-agnostic baselines, achieving higher model accuracy across a
wide range of carbon budgets, with especially strong gains under tight carbon
constraints.

</details>


### [58] [STRIDE: Scalable and Interpretable XAI via Subset-Free Functional Decomposition](https://arxiv.org/abs/2509.09070)
*Chaeyun Ko*

Main category: cs.LG

TL;DR: 提出STRIDE框架，通过RKHS中的正交功能分解缓解XAI框架的两个问题，在表格数据上有速度提升和高保真度。


<details>
  <summary>Details</summary>
Motivation: 解决大多数可解释AI框架推理成本高和表达力低的问题。

Method: 将解释构建为RKHS中无子集枚举的正交功能分解，通过递归核中心程序的分析投影方案计算功能组件。

Result: 在公共表格基准上速度提升0.6 - 9.7倍，中位数约3.0倍，保持高保真度（R^2在0.81 - 0.999 ）和较高排名一致性。

Conclusion: STRIDE以结构化功能视角补充标量归因方法，可进行新诊断。

Abstract: Most explainable AI (XAI) frameworks face two practical limitations: the
exponential cost of reasoning over feature subsets and the reduced
expressiveness of summarizing effects as single scalar values. We present
STRIDE, a scalable framework that aims to mitigate both issues by framing
explanation as a subset-enumeration-free, orthogonal functional decomposition
in a Reproducing Kernel Hilbert Space (RKHS). Rather than focusing only on
scalar attributions, STRIDE computes functional components f_S(x_S) via an
analytical projection scheme based on a recursive kernel-centering procedure,
avoiding explicit subset enumeration. In the tabular setups we study, the
approach is model-agnostic, provides both local and global views, and is
supported by theoretical results on orthogonality and L^2 convergence under
stated assumptions. On public tabular benchmarks in our environment, we
observed speedups ranging from 0.6 times (slower than TreeSHAP on a small
dataset) to 9.7 times (California), with a median approximate 3.0 times across
10 datasets, while maintaining high fidelity (R^2 between 0.81 and 0.999) and
substantial rank agreement on most datasets. Overall, STRIDE complements scalar
attribution methods by offering a structured functional perspective, enabling
novel diagnostics like 'component surgery' to quantitatively measure the impact
of specific interactions within our experimental scope.

</details>


### [59] [Active Learning and Explainable AI for Multi-Objective Optimization of Spin Coated Polymers](https://arxiv.org/abs/2509.08988)
*Brendan Young,Brendan Alvey,Andreas Werbrouck,Will Murphy,James Keller,Mattias J. Young,Matthew Maschmann*

Main category: cs.LG

TL;DR: 提出集成主动Pareto前沿学习算法与可视化和可解释AI技术的框架优化聚合物旋涂薄膜加工参数，实验证明方法有效且解释性好。


<details>
  <summary>Details</summary>
Motivation: 旋涂聚合物薄膜实现特定机械性能是多目标优化问题，需有效优化加工参数的方法。

Method: 将主动Pareto前沿学习算法PyePAL与可视化和可解释AI技术结合，用高斯过程模型预测目标值，UMAP可视化，结合模糊语言总结。

Result: 能有效识别有前景的聚合物设计，可视化和语言解释便于专家分析和知识发现。

Conclusion: 所提方法可高效优化聚合物薄膜加工参数，可视化和语言解释增强了结果的可解释性。

Abstract: Spin coating polymer thin films to achieve specific mechanical properties is
inherently a multi-objective optimization problem. We present a framework that
integrates an active Pareto front learning algorithm (PyePAL) with
visualization and explainable AI techniques to optimize processing parameters.
PyePAL uses Gaussian process models to predict objective values (hardness and
elasticity) from the design variables (spin speed, dilution, and polymer
mixture), guiding the adaptive selection of samples toward promising regions of
the design space. To enable interpretable insights into the high-dimensional
design space, we utilize UMAP (Uniform Manifold Approximation and Projection)
for two-dimensional visualization of the Pareto front exploration.
Additionally, we incorporate fuzzy linguistic summaries, which translate the
learned relationships between process parameters and performance objectives
into linguistic statements, thus enhancing the explainability and understanding
of the optimization results. Experimental results demonstrate that our method
efficiently identifies promising polymer designs, while the visual and
linguistic explanations facilitate expert-driven analysis and knowledge
discovery.

</details>


### [60] [ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning](https://arxiv.org/abs/2509.09534)
*Sena Ergisi,Luis Maßny,Rawad Bitar*

Main category: cs.LG

TL;DR: 提出抗拜占庭的联邦学习算法ProDiGy，通过联合双评分系统评估客户端梯度，实验表明其在多种场景优于现有防御机制，尤其在非IID数据分布时表现良好。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受对抗攻击，特别是在数据异构情况下，需要更有效的防御算法。

Method: 提出ProDiGy算法，使用基于梯度接近度和相异性的联合双评分系统评估客户端梯度。

Result: 通过大量数值实验，ProDiGy在各种场景下优于现有防御机制，在非IID数据分布时能保持强防御能力和模型准确性。

Conclusion: 双视角方法有效，能促进诚实客户端间的自然相似性，并检测可疑的一致性以发现攻击。

Abstract: Federated Learning (FL) emerged as a widely studied paradigm for distributed
learning. Despite its many advantages, FL remains vulnerable to adversarial
attacks, especially under data heterogeneity. We propose a new Byzantine-robust
FL algorithm called ProDiGy. The key novelty lies in evaluating the client
gradients using a joint dual scoring system based on the gradients' proximity
and dissimilarity. We demonstrate through extensive numerical experiments that
ProDiGy outperforms existing defenses in various scenarios. In particular, when
the clients' data do not follow an IID distribution, while other defense
mechanisms fail, ProDiGy maintains strong defense capabilities and model
accuracy. These findings highlight the effectiveness of a dual perspective
approach that promotes natural similarity among honest clients while detecting
suspicious uniformity as a potential indicator of an attack.

</details>


### [61] [Fast attention mechanisms: a tale of parallelism](https://arxiv.org/abs/2509.09001)
*Jingwen Liu,Hantao Yu,Clayton Sanford,Alexandr Andoni,Daniel Hsu*

Main category: cs.LG

TL;DR: 引入次二次时间复杂度的近似最近邻注意力机制ANNA，证明其保留标准注意力表达能力且能解决关键推理任务，还可模拟常数深度低秩transformers。


<details>
  <summary>Details</summary>
Motivation: Transformer时间复杂度为二次，限制可扩展性，需高效注意力机制。

Method: 引入近似最近邻注意力机制ANNA。

Result: ANNA - transformers保留标准注意力表达能力，能解决关键推理任务，常数深度ANNA - transformers可模拟常数深度低秩transformers。

Conclusion: ANNA为一大类高效注意力近似提供统一推理方式。

Abstract: Transformers have the representational capacity to simulate Massively
Parallel Computation (MPC) algorithms, but they suffer from quadratic time
complexity, which severely limits their scalability. We introduce an efficient
attention mechanism called Approximate Nearest Neighbor Attention (ANNA) with
sub-quadratic time complexity. We prove that ANNA-transformers (1) retain the
expressive power previously established for standard attention in terms of
matching the capabilities of MPC algorithms, and (2) can solve key reasoning
tasks such as Match2 and $k$-hop with near-optimal depth. Using the MPC
framework, we further prove that constant-depth ANNA-transformers can simulate
constant-depth low-rank transformers, thereby providing a unified way to reason
about a broad class of efficient attention approximations.

</details>


### [62] [Constructing a Question-Answering Simulator through the Distillation of LLMs](https://arxiv.org/abs/2509.09226)
*Haipeng Liu,Ting Long,Jing Fu*

Main category: cs.LG

TL;DR: 本文提出LLM Distillation based Simulator (LDSim)方法，从大语言模型中提取知识和推理能力来提升问答模拟性能，实验表明该方法在模拟和知识追踪任务上效果好。


<details>
  <summary>Details</summary>
Motivation: 现有预测学生回答正确性的问答模拟器方法中，无大语言模型方法性能欠佳，基于大语言模型的方法推理慢且显存消耗高，需要更好的方法提升性能。

Method: 提出LDSim方法，从大语言模型中蒸馏领域知识和推理能力以辅助预测。

Result: LDSim在模拟任务和知识追踪任务上取得了良好结果。

Conclusion: LDSim是一种有效的提升问答模拟性能的方法。

Abstract: The question-answering (QA) simulator is a model that mimics real student
learning behaviors and predicts their correctness of their responses to
questions. QA simulators enable educational recommender systems (ERS) to
collect large amounts of training data without interacting with real students,
thereby preventing harmful recommendations made by an undertrained ERS from
undermining actual student learning. Given the QA history, there are two
categories of solutions to predict the correctness, conducting the simulation:
(1) LLM-free methods, which apply a traditional sequential model to transfer
the QA history into a vector representation first, and make predictions based
on the representation; (2) LLM-based methods, which leverage the domain
knowledge and reasoning capability of LLM to enhence the prediction. LLM-free
methods offer fast inference but generally yield suboptimal performance. In
contrast, most LLM-based methods achieve better results, but at the cost of
slower inference speed and higher GPU memory consumption. In this paper, we
propose a method named LLM Distillation based Simulator (LDSim), which distills
domain knowledge and reasoning capability from an LLM to better assist
prediction, thereby improving simulation performance. Extensive experiments
demonstrate that our LDSim achieves strong results on both the simulation task
and the knowledge tracing (KT) task. Our code is publicly available at
https://anonymous.4open.science/r/LDSim-05A9.

</details>


### [63] [Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison](https://arxiv.org/abs/2509.09009)
*Marianna Nezhurina,Taishi Nakamura,Timur Carstensen,Niccolò Ajroldi,Ville Komulainen,David Salinas,Jenia Jitsev*

Main category: cs.LG

TL;DR: 介绍open - sci - ref系列密集变压器模型，在多个规模和数据集上训练，建立参考点，对比不同数据集表现，发布相关内容促进研究。


<details>
  <summary>Details</summary>
Motivation: 为研究人员提供跨规模和数据集评估替代训练方法合理性和质量的参考。

Method: 在8个近期开放参考数据集上，对不同参数和token规模的open - sci - ref模型进行训练，并在各种标准化基准上评估。

Result: 训练运行集建立参考点，对比显示在NemoTron - CC HQ上训练表现最佳，其次是DCLM - baseline和FineWeb - Edu。

Conclusion: 建立的参考基线可通过缩放趋势比较训练程序，发布的内容有助于简化重现、标准化比较和推动未来研究。

Abstract: We introduce open-sci-ref, a family of dense transformer models trained as
research baselines across multiple model (0.13B to 1.7B parameters) and token
scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on
various standardized benchmarks, our training runs set establishes reference
points that enable researchers to assess the sanity and quality of alternative
training approaches across scales and datasets. Intermediate checkpoints allow
comparison and studying of the training dynamics. The established reference
baselines allow training procedures to be compared through their scaling
trends, aligning them on a common compute axis. Comparison of open reference
datasets reveals that training on NemoTron-CC HQ consistently outperforms other
reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to
intermediate training checkpoints, the release includes logs, code, and
downstream evaluations to simplify reproduction, standardize comparison, and
facilitate future research.

</details>


### [64] [Deep Context-Conditioned Anomaly Detection for Tabular Data](https://arxiv.org/abs/2509.09030)
*Spencer King,Zhilu Zhang,Ruofan Yu,Baris Coskun,Wei Ding,Qian Cui*

Main category: cs.LG

TL;DR: 提出针对表格数据集的上下文条件异常检测框架，实验证明优于现有方法，强调上下文对异常检测的重要性。


<details>
  <summary>Details</summary>
Motivation: 无监督异常检测在处理含异构上下文的大规模表格数据时，依赖单一全局分布会忽略上下文细微差别，降低检测性能。

Method: 自动识别上下文特征，使用简单深度自编码器对条件数据分布进行建模。

Result: 在多个表格基准数据集上的大量实验表明，该方法优于现有方法。

Conclusion: 上下文在准确区分异常和正常实例中非常重要。

Abstract: Anomaly detection is critical in domains such as cybersecurity and finance,
especially when working with large-scale tabular data. Yet, unsupervised
anomaly detection -- where no labeled anomalies are available -- remains a
significant challenge. Although various deep learning methods have been
proposed to model a dataset's joint distribution, real-world tabular data often
contain heterogeneous contexts (e.g., different users), making globally rare
events normal under certain contexts. Consequently, relying on a single global
distribution can overlook these contextual nuances, degrading detection
performance. In this paper, we present a context-conditional anomaly detection
framework tailored for tabular datasets. Our approach automatically identifies
context features and models the conditional data distribution using a simple
deep autoencoder. Extensive experiments on multiple tabular benchmark datasets
demonstrate that our method outperforms state-of-the-art approaches,
underscoring the importance of context in accurately distinguishing anomalous
from normal instances.

</details>


### [65] [MoWE : A Mixture of Weather Experts](https://arxiv.org/abs/2509.09052)
*Dibyajyoti Chakraborty,Romit Maulik,Peter Harrington,Dallas Foster,Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 文章提出MoWE方法结合现有模型输出做天气预测，训练资源需求低且预测RMSE更低，是高效可扩展策略。


<details>
  <summary>Details</summary>
Motivation: 数据驱动天气模型近年进展停滞，需新方法突破局限。

Method: 采用基于Vision Transformer的门控网络，根据预测提前时间动态加权多个专家模型输出。

Result: 在2天预测中RMSE比最佳AI天气模型低10%，远超单个专家模型和简单平均。

Conclusion: 该方法是计算高效且可扩展的策略，能推动数据驱动天气预测发展。

Abstract: Data-driven weather models have recently achieved state-of-the-art
performance, yet progress has plateaued in recent years. This paper introduces
a Mixture of Experts (MoWE) approach as a novel paradigm to overcome these
limitations, not by creating a new forecaster, but by optimally combining the
outputs of existing models. The MoWE model is trained with significantly lower
computational resources than the individual experts. Our model employs a Vision
Transformer-based gating network that dynamically learns to weight the
contributions of multiple "expert" models at each grid point, conditioned on
forecast lead time. This approach creates a synthesized deterministic forecast
that is more accurate than any individual component in terms of Root Mean
Squared Error (RMSE). Our results demonstrate the effectiveness of this method,
achieving up to a 10% lower RMSE than the best-performing AI weather model on a
2-day forecast horizon, significantly outperforming individual experts as well
as a simple average across experts. This work presents a computationally
efficient and scalable strategy to push the state of the art in data-driven
weather prediction by making the most out of leading high-quality forecast
models.

</details>


### [66] [A Scoping Review of Machine Learning Applications in Power System Protection and Disturbance Management](https://arxiv.org/abs/2509.09053)
*Julian Oelhaf,Georg Kordowich,Mehran Pashaei,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 本文对机器学习在电力系统保护和扰动管理中的应用进行了范围综述，指出当前研究缺乏标准化，提出解决措施并指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 可再生和分布式能源整合重塑现代电力系统，挑战传统保护方案，需研究机器学习在电力系统保护中的应用。

Method: 遵循PRISMA for Scoping Reviews框架，综合分析100多篇文献。

Result: 机器学习模型在模拟数据集上准确率高，但在现实条件下性能验证不足，现有文献缺乏标准化，影响结果可比性和普遍性。

Conclusion: 引入面向机器学习的分类法，倡导标准化报告实践，未来研究应优先考虑公共基准数据集、现实验证方法和先进机器学习架构。

Abstract: The integration of renewable and distributed energy resources reshapes modern
power systems, challenging conventional protection schemes. This scoping review
synthesizes recent literature on machine learning (ML) applications in power
system protection and disturbance management, following the PRISMA for Scoping
Reviews framework. Based on over 100 publications, three key objectives are
addressed: (i) assessing the scope of ML research in protection tasks; (ii)
evaluating ML performance across diverse operational scenarios; and (iii)
identifying methods suitable for evolving grid conditions. ML models often
demonstrate high accuracy on simulated datasets; however, their performance
under real-world conditions remains insufficiently validated. The existing
literature is fragmented, with inconsistencies in methodological rigor, dataset
quality, and evaluation metrics. This lack of standardization hampers the
comparability of results and limits the generalizability of findings. To
address these challenges, this review introduces a ML-oriented taxonomy for
protection tasks, resolves key terminological inconsistencies, and advocates
for standardized reporting practices. It further provides guidelines for
comprehensive dataset documentation, methodological transparency, and
consistent evaluation protocols, aiming to improve reproducibility and enhance
the practical relevance of research outcomes. Critical gaps remain, including
the scarcity of real-world validation, insufficient robustness testing, and
limited consideration of deployment feasibility. Future research should
prioritize public benchmark datasets, realistic validation methods, and
advanced ML architectures. These steps are essential to move ML-based
protection from theoretical promise to practical deployment in increasingly
dynamic and decentralized power systems.

</details>


### [67] ["A 6 or a 9?": Ensemble Learning Through the Multiplicity of Performant Models and Explanations](https://arxiv.org/abs/2509.09073)
*Gianlucca Zuin,Adriano Veloso*

Main category: cs.LG

TL;DR: 论文提出Rashomon Ensemble方法，从多样的高性能解决方案中选模型提升泛化能力，在数据集验证有效，对企业应用有好处。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中模型泛化性选择难题，应对Rashomon Effect现象。

Method: 提出Rashomon Ensemble方法，基于模型性能和解释分组，构建最大化多样性且保持预测准确性的集成。

Result: 在开放和专有协作真实数据集验证，Rashomon ratio大的场景AUROC提升0.20+，对企业应用有实际好处。

Conclusion: 该方法具有鲁棒性、实用性和有效性。

Abstract: Creating models from past observations and ensuring their effectiveness on
new data is the essence of machine learning. However, selecting models that
generalize well remains a challenging task. Related to this topic, the Rashomon
Effect refers to cases where multiple models perform similarly well for a given
learning problem. This often occurs in real-world scenarios, like the
manufacturing process or medical diagnosis, where diverse patterns in data lead
to multiple high-performing solutions. We propose the Rashomon Ensemble, a
method that strategically selects models from these diverse high-performing
solutions to improve generalization. By grouping models based on both their
performance and explanations, we construct ensembles that maximize diversity
while maintaining predictive accuracy. This selection ensures that each model
covers a distinct region of the solution space, making the ensemble more robust
to distribution shifts and variations in unseen data. We validate our approach
on both open and proprietary collaborative real-world datasets, demonstrating
up to 0.20+ AUROC improvements in scenarios where the Rashomon ratio is large.
Additionally, we demonstrate tangible benefits for businesses in various
real-world applications, highlighting the robustness, practicality, and
effectiveness of our approach.

</details>


### [68] [An entropy formula for the Deep Linear Network](https://arxiv.org/abs/2509.09088)
*Govind Menon,Tianmin Yu*

Main category: cs.LG

TL;DR: 研究深度线性网络（DLN）的黎曼几何，为学习过程的热力学描述奠定基础。


<details>
  <summary>Details</summary>
Motivation: 为深度线性网络学习过程的热力学描述提供基础。

Method: 利用群作用分析过参数化，使用从参数空间到可观测量空间的黎曼浸没，用群轨道对参数空间中平衡流形的叶状结构定义和计算玻尔兹曼熵，用雅可比矩阵理论构造平衡流形切空间的正交基。

Result: 展示了文献[2]中定义的可观测量空间上的黎曼几何可通过平衡流形的黎曼浸没得到。

Conclusion: 通过一系列方法研究DLN的黎曼几何，为其学习过程的热力学描述提供支持。

Abstract: We study the Riemannian geometry of the Deep Linear Network (DLN) as a
foundation for a thermodynamic description of the learning process. The main
tools are the use of group actions to analyze overparametrization and the use
of Riemannian submersion from the space of parameters to the space of
observables. The foliation of the balanced manifold in the parameter space by
group orbits is used to define and compute a Boltzmann entropy. We also show
that the Riemannian geometry on the space of observables defined in [2] is
obtained by Riemannian submersion of the balanced manifold. The main technical
step is an explicit construction of an orthonormal basis for the tangent space
of the balanced manifold using the theory of Jacobi matrices.

</details>


### [69] [Sensitivity-LoRA: Low-Load Sensitivity-Based Fine-Tuning for Large Language Models](https://arxiv.org/abs/2509.09119)
*Hao Zhang,Bo Huang,Zhenjia Li,Xi Xiao,Hui Yi Leong,Zumeng Zhang,Xinwei Long,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: 提出Sensitivity - LoRA方法解决LoRA在LLM微调中秩分配问题，实验证明其有效性、效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型（LLM）适配到特定任务有挑战，现有低秩适配（LoRA）方法存在秩分配局限，现有解决技术计算效率低、复杂且不稳定。

Method: 提出Sensitivity - LoRA方法，基于权重矩阵的全局和局部敏感性动态分配秩，利用损失函数的二阶导数有效捕捉权重敏感性。

Result: 实验结果表明Sensitivity - LoRA在不同任务和基准测试中具有强大的有效性、效率和稳定性。

Conclusion: Sensitivity - LoRA能有效解决LoRA的局限，在LLM微调中表现良好。

Abstract: Large Language Models (LLMs) have transformed both everyday life and
scientific research. However, adapting LLMs from general-purpose models to
specialized tasks remains challenging, particularly in resource-constrained
environments. Low-Rank Adaptation (LoRA), a prominent method within
Parameter-Efficient Fine-Tuning (PEFT), has emerged as a promising approach to
LLMs by approximating model weight updates using low-rank decomposition.
However, LoRA is limited by its uniform rank ( r ) allocation to each
incremental matrix, and existing rank allocation techniques aimed at addressing
this issue remain computationally inefficient, complex, and unstable, hindering
practical applications. To address these limitations, we propose
Sensitivity-LoRA, an efficient fine-tuning method that dynamically allocates
ranks to weight matrices based on both their global and local sensitivities. It
leverages the second-order derivatives (Hessian Matrix) of the loss function to
effectively capture weight sensitivity, enabling optimal rank allocation with
minimal computational overhead. Our experimental results have demonstrated
robust effectiveness, efficiency and stability of Sensitivity-LoRA across
diverse tasks and benchmarks.

</details>


### [70] [Learning What Matters: Causal Time Series Modeling for Arctic Sea Ice Prediction](https://arxiv.org/abs/2509.09128)
*Emam Hossain,Md Osman Gani*

Main category: cs.LG

TL;DR: 提出因果感知深度学习框架，结合MVGC和PCMCI+进行因果特征选择，用北极海冰范围数据验证，提升预测准确性和可解释性，适用于其他领域。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习和深度学习模型基于相关性学习，难以区分因果关系，限制了鲁棒性、可解释性和泛化能力。

Method: 引入结合MVGC和PCMCI+的因果感知深度学习框架，在混合神经架构中进行因果特征选择。

Result: 结合因果输入提高了不同预测时长下的预测准确性和可解释性。

Conclusion: 该框架适用于其他动态高维领域，推动了因果预测建模的理论基础和实际应用。

Abstract: Conventional machine learning and deep learning models typically rely on
correlation-based learning, which often fails to distinguish genuine causal
relationships from spurious associations, limiting their robustness,
interpretability, and ability to generalize. To overcome these limitations, we
introduce a causality-aware deep learning framework that integrates
Multivariate Granger Causality (MVGC) and PCMCI+ for causal feature selection
within a hybrid neural architecture. Leveraging 43 years (1979-2021) of Arctic
Sea Ice Extent (SIE) data and associated ocean-atmospheric variables at daily
and monthly resolutions, the proposed method identifies causally influential
predictors, prioritizes direct causes of SIE dynamics, reduces unnecessary
features, and enhances computational efficiency. Experimental results show that
incorporating causal inputs leads to improved prediction accuracy and
interpretability across varying lead times. While demonstrated on Arctic SIE
forecasting, the framework is broadly applicable to other dynamic,
high-dimensional domains, offering a scalable approach that advances both the
theoretical foundations and practical performance of causality-informed
predictive modeling.

</details>


### [71] [Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.09135)
*Xuefeng Wang,Lei Zhang,Henglin Pu,Ahmed H. Qureshi,Husheng Li*

Main category: cs.LG

TL;DR: 现有强化学习方法处理复杂动态系统有困难，连续时间RL（CTRL）有局限，本文提出CT - MARL框架，用PINNs近似HJB值函数，引入VGI模块，实验表明该方法优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有CTRL方法主要局限于单智能体领域，传统HJB方程求解方法有维数灾难，多智能体环境中准确近似集中式值函数困难，影响策略训练稳定性。

Method: 提出CT - MARL框架，使用物理信息神经网络（PINNs）近似HJB值函数，引入值梯度迭代（VGI）模块，沿轨迹迭代细化值梯度。

Result: 在多智能体粒子环境（MPE）和多智能体MuJoCo等连续时间基准测试中，该方法始终优于现有连续时间RL基线，能处理复杂多智能体动态。

Conclusion: 所提出的CT - MARL框架有效，可用于复杂多智能体动态系统，比现有连续时间RL方法表现更好。

Abstract: Existing reinforcement learning (RL) methods struggle with complex dynamical
systems that demand interactions at high frequencies or irregular time
intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by
replacing discrete-time Bellman recursion with differential value functions
defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation.
While CTRL has shown promise, its applications have been largely limited to the
single-agent domain. This limitation stems from two key challenges: (i)
conventional solution methods for HJB equations suffer from the curse of
dimensionality (CoD), making them intractable in high-dimensional systems; and
(ii) even with HJB-based learning approaches, accurately approximating
centralized value functions in multi-agent settings remains difficult, which in
turn destabilizes policy training. In this paper, we propose a CT-MARL
framework that uses physics-informed neural networks (PINNs) to approximate
HJB-based value functions at scale. To ensure the value is consistent with its
differential structure, we align value learning with value-gradient learning by
introducing a Value Gradient Iteration (VGI) module that iteratively refines
value gradients along trajectories. This improves gradient fidelity, in turn
yielding more accurate values and stronger policy learning. We evaluate our
method using continuous-time variants of standard benchmarks, including
multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results
demonstrate that our approach consistently outperforms existing continuous-time
RL baselines and scales to complex multi-agent dynamics.

</details>


### [72] [Peering Partner Recommendation for ISPs using Machine Learning](https://arxiv.org/abs/2509.09146)
*Md Ibrahim Ibne Alam,Ankur Senapati,Anindo Mahmood,Murat Yuksel,Koushik Kar*

Main category: cs.LG

TL;DR: 本文探索利用公开数据构建机器学习模型预测ISP是否应建立对等关系，树基模型表现最佳，XGBoost准确率达98%，有望实现自动选择对等伙伴。


<details>
  <summary>Details</summary>
Motivation: 对等连接过程冗长复杂，自动化对等伙伴选择可提高全球互联网生态系统的效率。

Method: 从公开数据库收集ISP数据，评估树基、神经网络和基于Transformer的三类机器学习模型预测对等关系的性能。

Result: 树基模型在实验中准确性和效率最高，用公开数据训练的XGBoost模型预测对等伙伴准确率达98%，且对时间、空间变化和数据缺失有很强的适应性。

Conclusion: ISP可采用该方法实现对等伙伴选择过程的完全自动化，构建更高效优化的互联网生态系统。

Abstract: Internet service providers (ISPs) need to connect with other ISPs to provide
global connectivity services to their users. To ensure global connectivity,
ISPs can either use transit service(s) or establish direct peering
relationships between themselves via Internet exchange points (IXPs). Peering
offers more room for ISP-specific optimizations and is preferred, but it often
involves a lengthy and complex process. Automating peering partner selection
can enhance efficiency in the global Internet ecosystem. We explore the use of
publicly available data on ISPs to develop a machine learning (ML) model that
can predict whether an ISP pair should peer or not. At first, we explore public
databases, e.g., PeeringDB, CAIDA, etc., to gather data on ISPs. Then, we
evaluate the performance of three broad types of ML models for predicting
peering relationships: tree-based, neural network-based, and transformer-based.
Among these, we observe that tree-based models achieve the highest accuracy and
efficiency in our experiments. The XGBoost model trained with publicly
available data showed promising performance, with a 98% accuracy rate in
predicting peering partners. In addition, the model demonstrated great
resilience to variations in time, space, and missing data. We envision that
ISPs can adopt our method to fully automate the peering partner selection
process, thus transitioning to a more efficient and optimized Internet
ecosystem.

</details>


### [73] [HISPASpoof: A New Dataset For Spanish Speech Forensics](https://arxiv.org/abs/2509.09155)
*Maria Risques,Kratika Bhagtani,Amit Kumar Singh Yadav,Edward J. Delp*

Main category: cs.LG

TL;DR: 引入首个大规模西班牙语合成语音检测与溯源数据集HISPASpoof，评估多种方法，显示其对西班牙语语音检测有提升。


<details>
  <summary>Details</summary>
Motivation: 现有合成语音检测研究中西班牙语数据不足，需填补该领域空白。

Method: 创建包含六种口音真实语音及六种零样本TTS系统生成的合成语音的HISPASpoof数据集，评估五种代表性方法。

Result: 在英语上训练的检测器无法泛化到西班牙语，在HISPASpoof上训练可显著提升检测效果，同时评估了合成语音溯源性能。

Conclusion: HISPASpoof为完善西班牙语语音取证提供了关键基准。

Abstract: Zero-shot Voice Cloning (VC) and Text-to-Speech (TTS) methods have advanced
rapidly, enabling the generation of highly realistic synthetic speech and
raising serious concerns about their misuse. While numerous detectors have been
developed for English and Chinese, Spanish-spoken by over 600 million people
worldwide-remains underrepresented in speech forensics. To address this gap, we
introduce HISPASpoof, the first large-scale Spanish dataset designed for
synthetic speech detection and attribution. It includes real speech from public
corpora across six accents and synthetic speech generated with six zero-shot
TTS systems. We evaluate five representative methods, showing that detectors
trained on English fail to generalize to Spanish, while training on HISPASpoof
substantially improves detection. We also evaluate synthetic speech attribution
performance on HISPASpoof, i.e., identifying the generation method of synthetic
speech. HISPASpoof thus provides a critical benchmark for advancing reliable
and inclusive speech forensics in Spanish.

</details>


### [74] [Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication](https://arxiv.org/abs/2509.09168)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出预训练视觉变压器自适应令牌合并免训练框架，降低推理时间和传输资源使用，实验证明性能优。


<details>
  <summary>Details</summary>
Motivation: 大规模变压器模型计算需求大，阻碍其在资源受限6G网络的实际部署。

Method: 将每层合并比例选择构建为多目标优化问题，用基于高斯过程的贝叶斯优化构建帕累托最优配置前沿。

Result: 方法优于其他基线，减少浮点运算，在不同信噪比下保持竞争力；自适应策略有效。

Conclusion: 为未来边缘智能系统中基于变压器的语义通信部署提供可扩展高效方法。

Abstract: Large-scale transformer models have emerged as a powerful tool for semantic
communication systems, enabling edge devices to extract rich representations
for robust inference across noisy wireless channels. However, their substantial
computational demands remain a major barrier to practical deployment in
resource-constrained 6G networks. In this paper, we present a training-free
framework for adaptive token merging in pretrained vision transformers to
jointly reduce inference time and transmission resource usage. We formulate the
selection of per-layer merging proportions as a multi-objective optimization
problem to balance accuracy and computational cost. We employ Gaussian
process-based Bayesian optimization to construct a Pareto frontier of optimal
configurations, enabling flexible runtime adaptation to dynamic application
requirements and channel conditions. Extensive experiments demonstrate that our
method consistently outperforms other baselines and achieves significant
reductions in floating-point operations while maintaining competitive accuracy
across a wide range of signal-to-noise ratio (SNR) conditions. Additional
results highlight the effectiveness of adaptive policies that adjust merging
aggressiveness in response to channel quality, providing a practical mechanism
to trade off latency and semantic fidelity on demand. These findings establish
a scalable and efficient approach for deploying transformer-based semantic
communication in future edge intelligence systems.

</details>


### [75] [Quantum Machine Learning, Quantitative Trading, Reinforcement Learning, Deep Learning](https://arxiv.org/abs/2509.09176)
*Jun-Hao Chen,Yu-Chien Huang,Yun-Cheng Tsai,Samuel Yen-Chi Chen*

Main category: cs.LG

TL;DR: 结合量子长短期记忆网络与量子异步优势演员 - 评论家算法实现美元/新台币交易代理，表现优于部分货币ETF，显示混合模型在外汇交易有竞争力。


<details>
  <summary>Details</summary>
Motivation: 探索量子启发神经网络与深度强化学习融合在金融交易的应用，实现更优的外汇交易策略。

Method: 将量子长短期记忆网络（QLSTM）用于短期趋势预测，与量子异步优势演员 - 评论家（QA3C）算法结合，实现交易代理，详细设计状态、奖励函数并进行多核训练。

Result: 仅做多的交易代理在约5年实现11.87%的回报，最大回撤0.92%，优于多个货币ETF。

Conclusion: 混合模型在外汇交易中有有竞争力的表现，QLSTM适用于低利润、严控风险的交易，存在经典量子模拟和策略简化的局限。

Abstract: The convergence of quantum-inspired neural networks and deep reinforcement
learning offers a promising avenue for financial trading. We implemented a
trading agent for USD/TWD by integrating Quantum Long Short-Term Memory (QLSTM)
for short-term trend prediction with Quantum Asynchronous Advantage
Actor-Critic (QA3C), a quantum-enhanced variant of the classical A3C. Trained
on data from 2000-01-01 to 2025-04-30 (80\% training, 20\% testing), the
long-only agent achieves 11.87\% return over around 5 years with 0.92\% max
drawdown, outperforming several currency ETFs. We detail state design (QLSTM
features and indicators), reward function for trend-following/risk control, and
multi-core training. Results show hybrid models yield competitive FX trading
performance. Implications include QLSTM's effectiveness for small-profit trades
with tight risk and future enhancements. Key hyperparameters: QLSTM sequence
length$=$4, QA3C workers$=$8. Limitations: classical quantum simulation and
simplified strategy. \footnote{The views expressed in this article are those of
the authors and do not represent the views of Wells Fargo. This article is for
informational purposes only. Nothing contained in this article should be
construed as investment advice. Wells Fargo makes no express or implied
warranties and expressly disclaims all legal, tax, and accounting implications
related to this article.

</details>


### [76] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [77] [Breaking the Statistical Similarity Trap in Extreme Convection Detection](https://arxiv.org/abs/2509.09195)
*Md Tanveer Hossain Munim*

Main category: cs.LG

TL;DR: 现有深度学习气象模型评估指标存在‘统计相似性陷阱’，文章引入DART框架应对挑战，有多项发现并验证其有效性，为极端天气预警AI提供途径。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习气象模型评估指标的‘统计相似性陷阱’问题，实现精准极端对流检测。

Method: 引入DART框架，采用双解码器架构、显式背景/极端分解、物理激励过采样和特定任务损失函数。

Result: 验证‘统计相似性陷阱’，发现‘IVT悖论’，证明架构必要性，通过实际案例验证，DART在极端对流检测上表现优于基线。

Conclusion: DART框架设计专业，可精确校准，训练快，能融入现有气象工作流程，为极端天气预警的可靠AI指明方向。

Abstract: Current evaluation metrics for deep learning weather models create a
"Statistical Similarity Trap", rewarding blurry predictions while missing rare,
high-impact events. We provide quantitative evidence of this trap, showing
sophisticated baselines achieve 97.9% correlation yet 0.00 CSI for dangerous
convection detection. We introduce DART (Dual Architecture for Regression
Tasks), a framework addressing the challenge of transforming coarse atmospheric
forecasts into high-resolution satellite brightness temperature fields
optimized for extreme convection detection (below 220 K). DART employs
dual-decoder architecture with explicit background/extreme decomposition,
physically motivated oversampling, and task-specific loss functions. We present
four key findings: (1) empirical validation of the Statistical Similarity Trap
across multiple sophisticated baselines; (2) the "IVT Paradox", removing
Integrated Water Vapor Transport, widely regarded as essential for atmospheric
river analysis, improves extreme convection detection by 270%; (3)
architectural necessity demonstrated through operational flexibility (DART
achieves CSI = 0.273 with bias = 2.52 vs. 6.72 for baselines at equivalent
CSI), and (4) real-world validation with the August 2023 Chittagong flooding
disaster as a case study. To our knowledge, this is the first work to
systematically address this hybrid conversion-segmentation-downscaling task,
with no direct prior benchmarks identified in existing literature. Our
validation against diverse statistical and deep learning baselines sufficiently
demonstrates DART's specialized design. The framework enables precise
operational calibration through beta-tuning, trains in under 10 minutes on
standard hardware, and integrates seamlessly with existing meteorological
workflows, demonstrating a pathway toward trustworthy AI for extreme weather
preparedness.

</details>


### [78] [Incentivizing Safer Actions in Policy Optimization for Constrained Reinforcement Learning](https://arxiv.org/abs/2509.09208)
*Somnath Hazra,Pallab Dasgupta,Soumyajit Dey*

Main category: cs.LG

TL;DR: 提出增量惩罚近端策略优化算法IP3O解决连续控制中约束强化学习的训练稳定性问题，经实验验证其有效性并给出理论保证。


<details>
  <summary>Details</summary>
Motivation: 连续控制中约束强化学习在平衡奖励最大化和约束满足时存在挑战，策略优化方法在约束边界附近不稳定，训练性能不佳。

Method: 引入自适应激励机制，提出IP3O算法，对违反约束施加递增惩罚以稳定训练。

Result: 在基准环境上的实验表明IP3O比现有安全强化学习算法更有效。

Conclusion: IP3O算法能有效解决连续控制中约束强化学习的训练稳定性问题，且有理论上的最坏情况误差界保证。

Abstract: Constrained Reinforcement Learning (RL) aims to maximize the return while
adhering to predefined constraint limits, which represent domain-specific
safety requirements. In continuous control settings, where learning agents
govern system actions, balancing the trade-off between reward maximization and
constraint satisfaction remains a significant challenge. Policy optimization
methods often exhibit instability near constraint boundaries, resulting in
suboptimal training performance. To address this issue, we introduce a novel
approach that integrates an adaptive incentive mechanism in addition to the
reward structure to stay within the constraint bound before approaching the
constraint boundary. Building on this insight, we propose Incrementally
Penalized Proximal Policy Optimization (IP3O), a practical algorithm that
enforces a progressively increasing penalty to stabilize training dynamics.
Through empirical evaluation on benchmark environments, we demonstrate the
efficacy of IP3O compared to the performance of state-of-the-art Safe RL
algorithms. Furthermore, we provide theoretical guarantees by deriving a bound
on the worst-case error of the optimality achieved by our algorithm.

</details>


### [79] [Identifying Key Features for Establishing Sustainable Agro-Tourism Centre: A Data Driven Approach](https://arxiv.org/abs/2509.09214)
*Alka Gadakh,Vidya Kumbhar,Sonal Khosla,Kumar Karunendra*

Main category: cs.LG

TL;DR: 本文研究农业旅游增长策略，通过两阶段研究确定重要指标，应用机器学习模型，结果显示LASSO方法下LR模型分类准确率高。


<details>
  <summary>Details</summary>
Motivation: 农业旅游是促进农村发展的经济模式，作为热门领域需详细研究其增长策略。

Method: 分两阶段研究，先通过文献综述确定指标，再用机器学习模型（LASSO结合LR、DT、RF、XGBOOST）进行特征选择。

Result: LASSO方法下，70 - 30%训练测试数据中LR模型分类准确率达98%，RF为95%；80 - 20%数据中LR准确率99%，DT和XGBoost为97%。

Conclusion: 未明确提及结论内容，但结果表明LASSO结合LR等模型对农业旅游增长指标的选择有较好效果。

Abstract: Agro-tourism serves as a strategic economic model designed to facilitate
rural development by diversifying income streams for local communities like
farmers while promoting the conservation of indigenous cultural heritage and
traditional agricultural practices. As a very booming subdomain of tourism,
there is a need to study the strategies for the growth of Agro-tourism in
detail. The current study has identified the important indicators for the
growth and enhancement of agro-tourism. The study is conducted in two phases:
identification of the important indicators through a comprehensive literature
review and in the second phase state-of-the-art techniques were used to
identify the important indicators for the growth of agro-tourism. The
indicators are also called features synonymously, the machine learning models
for feature selection were applied and it was observed that the Least Absolute
Shrinkage and Selection Operator (LASSO) method combined with, the machine
Learning Classifiers such as Logistic Regression (LR), Decision Trees (DT),
Random Forest (RF) Tree, and Extreme Gradient Boosting (XGBOOST) models were
used to suggest the growth of the agro-tourism. The results show that with the
LASSO method, LR model gives the highest classification accuracy of 98% in
70-30% train-test data followed by RF with 95% accuracy. Similarly, in the
80-20% train-test data LR maintains the highest accuracy at 99%, while DT and
XGBoost follow with 97% accuracy.

</details>


### [80] [Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement](https://arxiv.org/abs/2509.09219)
*Jakob Nyberg,Pontus Johnson*

Main category: cs.LG

TL;DR: 提出Vejde框架，结合数据抽象、图神经网络和强化学习处理复杂状态决策问题，测试显示其策略泛化能力好。


<details>
  <summary>Details</summary>
Motivation: 解决具有丰富结构状态的决策问题，如对象类和关系，需要有能处理不同规模和结构问题的方法。

Method: 将MDP状态表示为实体事实数据库，转换为二分图，通过神经消息传递映射到潜在状态，使用监督和强化学习训练策略，分离训练和测试集。

Result: Vejde策略平均能泛化到测试实例且分数无显著损失，归纳代理在未见测试实例上的分数接近特定实例的MLP代理。

Conclusion: Vejde框架有效，其策略具有较好的泛化能力。

Abstract: We present and evaluate Vejde; a framework which combines data abstraction,
graph neural networks and reinforcement learning to produce inductive policy
functions for decision problems with richly structured states, such as object
classes and relations. MDP states are represented as data bases of facts about
entities, and Vejde converts each state to a bipartite graph, which is mapped
to latent states through neural message passing. The factored representation of
both states and actions allows Vejde agents to handle problems of varying size
and structure. We tested Vejde agents on eight problem domains defined in RDDL,
with ten problem instances each, where policies were trained using both
supervised and reinforcement learning. To test policy generalization, we
separate problem instances in two sets, one for training and the other solely
for testing. Test results on unseen instances for the Vejde agents were
compared to MLP agents trained on each problem instance, as well as the online
planning algorithm Prost. Our results show that Vejde policies in average
generalize to the test instances without a significant loss in score.
Additionally, the inductive agents received scores on unseen test instances
that on average were close to the instance-specific MLP agents.

</details>


### [81] [Unsupervised Multi-Attention Meta Transformer for Rotating Machinery Fault Diagnosis](https://arxiv.org/abs/2509.09251)
*Hanyang Wang,Yuxuan Yang,Hongjun Wang,Lihui Wang*

Main category: cs.LG

TL;DR: 提出MMT - FD方法用于少样本无监督旋转机械故障诊断，在少量标注数据下实现99%诊断准确率且泛化性强


<details>
  <summary>Details</summary>
Motivation: 解决实际工程中故障样本有限和预测模型缺乏泛化性的问题

Method: 提出MMT - FD框架，集成时频域编码器和元学习泛化模型，通过时频域随机增强生成状态表示，再进行分类和泛化训练，最后用少量标注数据微调

Result: 在轴承故障数据集和转子试验台数据实验中，MMT - FD模型用1%标注样本数据实现99%故障诊断准确率

Conclusion: MMT - FD模型具有高效性和强泛化能力，适用于不同类型机械设备的故障诊断

Abstract: The intelligent fault diagnosis of rotating mechanical equipment usually
requires a large amount of labeled sample data. However, in practical
industrial applications, acquiring enough data is both challenging and
expensive in terms of time and cost. Moreover, different types of rotating
mechanical equipment with different unique mechanical properties, require
separate training of diagnostic models for each case. To address the challenges
of limited fault samples and the lack of generalizability in prediction models
for practical engineering applications, we propose a Multi-Attention Meta
Transformer method for few-shot unsupervised rotating machinery fault diagnosis
(MMT-FD). This framework extracts potential fault representations from
unlabeled data and demonstrates strong generalization capabilities, making it
suitable for diagnosing faults across various types of mechanical equipment.
The MMT-FD framework integrates a time-frequency domain encoder and a
meta-learning generalization model. The time-frequency domain encoder predicts
status representations generated through random augmentations in the
time-frequency domain. These enhanced data are then fed into a meta-learning
network for classification and generalization training, followed by fine-tuning
using a limited amount of labeled data. The model is iteratively optimized
using a small number of contrastive learning iterations, resulting in high
efficiency. To validate the framework, we conducted experiments on a bearing
fault dataset and rotor test bench data. The results demonstrate that the
MMT-FD model achieves 99\% fault diagnosis accuracy with only 1\% of labeled
sample data, exhibiting robust generalization capabilities.

</details>


### [82] [Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents](https://arxiv.org/abs/2509.09265)
*Jiawei Wang,Jiacai Liu,Yuqian Fu,Yingru Li,Xintao Wang,Yuan Lin,Yu Yue,Lin Zhang,Yang Wang,Ke Wang*

Main category: cs.LG

TL;DR: 论文指出大语言模型在长周期任务中学习动态存在问题，提出熵调制策略梯度（EMPG）框架解决问题，实验显示其性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在长周期任务中因稀疏结果奖励难以对中间步骤进行信用分配的问题，以及学习动态中策略梯度幅度与熵耦合导致的更新效率和稳定性问题。

Method: 提出EMPG框架，基于逐步不确定性和最终任务结果重新校准学习信号，放大正确动作更新、惩罚错误动作、减弱不确定步骤更新，并引入未来清晰度奖励项。

Result: 在WebShop、ALFWorld和Deep Search三个挑战性任务实验中，EMPG取得显著性能提升，远超强策略梯度基线。

Conclusion: EMPG框架有效解决了大语言模型在长周期任务中的学习问题，提升了任务表现。

Abstract: In long-horizon tasks, recent agents based on Large Language Models (LLMs)
face a significant challenge that sparse, outcome-based rewards make it
difficult to assign credit to intermediate steps. Previous methods mainly focus
on creating dense reward signals to guide learning, either through traditional
reinforcement learning techniques like inverse reinforcement learning or by
using Process Reward Models for step-by-step feedback. In this paper, we
identify a fundamental problem in the learning dynamics of LLMs: the magnitude
of policy gradients is inherently coupled with the entropy, which leads to
inefficient small updates for confident correct actions and potentially
destabilizes large updates for uncertain ones. To resolve this, we propose
Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the
learning signal based on step-wise uncertainty and the final task outcome. EMPG
amplifies updates for confident correct actions, penalizes confident errors,
and attenuates updates from uncertain steps to stabilize exploration. We
further introduce a bonus term for future clarity that encourages agents to
find more predictable solution paths. Through comprehensive experiments on
three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we
demonstrate that EMPG achieves substantial performance gains and significantly
outperforms strong policy gradient baselines. Project page is at
https://empgseed-seed.github.io/

</details>


### [83] [Data Driven Discovery of Emergent Dynamics in Reaction Diffusion Systems from Sparse and Noisy Observations](https://arxiv.org/abs/2509.09278)
*Saumitra Dwivedi,Ricardo da Silva Torres,Ibrahim A. Hameed,Gunnar Tufte,Anniken Susanne T. Karlsen*

Main category: cs.LG

TL;DR: 本文尝试通过学习软人工生命模型解决无物理先验知识时反应 - 扩散系统的系统识别问题，验证DRSALife模型适用性，实验表明学习模型预测准确性高且鲁棒性好。


<details>
  <summary>Details</summary>
Motivation: 解决在无底层物理先验知识情况下，反应 - 扩散系统发现过程中的系统识别挑战。

Method: 从观察数据中学习软人工生命模型（如基于代理和元胞自动机模型），运用DRSALife概念框架学习规则集，还研究了噪声和稀疏数据集的影响。

Result: 学习的模型能以74%的准确率预测涌现动力学，在高斯噪声和时间稀疏性下表现出较强鲁棒性，成功识别底层偏微分方程的结构和参数。

Conclusion: DRSALife模型适用于从观察数据中学习准确表示反应 - 扩散系统涌现动力学的软人工生命规则集，在无物理先验知识情况下有效。

Abstract: Data-driven discovery of emergent dynamics is gaining popularity,
particularly in the context of reaction-diffusion systems. These systems are
widely studied across various fields, including neuroscience, ecology,
epidemiology, and several other subject areas that deal with emergent dynamics.
A current challenge in the discovery process relates to system identification
when there is no prior knowledge of the underlying physics. We attempt to
address this challenge by learning Soft Artificial Life (Soft ALife) models,
such as Agent-based and Cellular Automata (CA) models, from observed data for
reaction-diffusion systems. In this paper, we present findings on the
applicability of a conceptual framework, the Data-driven Rulesets for Soft
Artificial Life (DRSALife) model, to learn Soft ALife rulesets that accurately
represent emergent dynamics in a reaction-diffusion system from observed data.
This model has demonstrated promising results for Elementary CA Rule 30, Game
of Life, and Vicsek Flocking problems in recent work. To our knowledge, this is
one of the few studies that explore machine-based Soft ALife ruleset learning
and system identification for reaction-diffusion dynamics without any prior
knowledge of the underlying physics. Moreover, we provide comprehensive
findings from experiments investigating the potential effects of using noisy
and sparse observed datasets on learning emergent dynamics. Additionally, we
successfully identify the structure and parameters of the underlying partial
differential equations (PDEs) representing these dynamics. Experimental results
demonstrate that the learned models are able to predict the emergent dynamics
with good accuracy (74%) and exhibit quite robust performance when subjected to
Gaussian noise and temporal sparsity.

</details>


### [84] [MoSE: Unveiling Structural Patterns in Graphs via Mixture of Subgraph Experts](https://arxiv.org/abs/2509.09337)
*Junda Ye,Zhongbao Zhang,Li Sun,Siqiang Luo*

Main category: cs.LG

TL;DR: 本文提出MoSE框架用于基于子图的表示学习，能在不同图任务中灵活捕获子图模式，理论证明其比SWL更强大，实验显示优于基线模型且有可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有GNN依赖局部成对消息传递，结构表达能力不足，集成随机游走核的方法适用于图级任务，固定核配置缺乏灵活性。

Method: 提出MoSE框架，通过匿名游走提取信息子图，根据结构语义将其动态路由到专门的专家模块。

Result: 理论上MoSE比SWL更强大，实验中MoSE优于竞争基线模型，且能可视化展示学习到的子图专家。

Conclusion: MoSE框架能在不同图任务中实现灵活且有表现力的子图表示学习，具有更好的灵活性和可解释性。

Abstract: While graph neural networks (GNNs) have achieved great success in learning
from graph-structured data, their reliance on local, pairwise message passing
restricts their ability to capture complex, high-order subgraph patterns.
leading to insufficient structural expressiveness. Recent efforts have
attempted to enhance structural expressiveness by integrating random walk
kernels into GNNs. However, these methods are inherently designed for
graph-level tasks, which limits their applicability to other downstream tasks
such as node classification. Moreover, their fixed kernel configurations hinder
the model's flexibility in capturing diverse subgraph structures. To address
these limitations, this paper proposes a novel Mixture of Subgraph Experts
(MoSE) framework for flexible and expressive subgraph-based representation
learning across diverse graph tasks. Specifically, MoSE extracts informative
subgraphs via anonymous walks and dynamically routes them to specialized
experts based on structural semantics, enabling the model to capture diverse
subgraph patterns with improved flexibility and interpretability. We further
provide a theoretical analysis of MoSE's expressivity within the Subgraph
Weisfeiler-Lehman (SWL) Test, proving that it is more powerful than SWL.
Extensive experiments, together with visualizations of learned subgraph
experts, demonstrate that MoSE not only outperforms competitive baselines but
also provides interpretable insights into structural patterns learned by the
model.

</details>


### [85] [Robust Non-Linear Correlations via Polynomial Regression](https://arxiv.org/abs/2509.09380)
*Luca Giuliani,Michele Lombardi*

Main category: cs.LG

TL;DR: 本文提出一种基于用户可配置多项式核的HGR计算新方法，比之前方法更稳健，还通过实验验证其在约束机器学习框架中的适用性。


<details>
  <summary>Details</summary>
Motivation: 现有可微估计HGR的算法存在不可计算性导致的偏差 - 方差权衡问题，影响方法在现实场景中的鲁棒性。

Method: 引入依赖用户可配置多项式核的计算方法。

Result: 新方法在鲁棒性和确定性方面有显著优势，计算能得到可作为损失正则化器的有洞察力的次梯度。

Conclusion: 新方法是现实应用中更可靠的选择，在约束机器学习框架中具有适用性。

Abstract: The Hirschfeld-Gebelein-R\'enyi (HGR) correlation coefficient is an extension
of Pearson's correlation that is not limited to linear correlations, with
potential applications in algorithmic fairness, scientific analysis, and causal
discovery. Recently, novel algorithms to estimate HGR in a differentiable
manner have been proposed to facilitate its use as a loss regularizer in
constrained machine learning applications. However, the inherent
uncomputability of HGR requires a bias-variance trade-off, which can possibly
compromise the robustness of the proposed methods, hence raising technical
concerns if applied in real-world scenarios. We introduce a novel computational
approach for HGR that relies on user-configurable polynomial kernels, offering
greater robustness compared to previous methods and featuring a faster yet
almost equally effective restriction. Our approach provides significant
advantages in terms of robustness and determinism, making it a more reliable
option for real-world applications. Moreover, we present a brief experimental
analysis to validate the applicability of our approach within a constrained
machine learning framework, showing that its computation yields an insightful
subgradient that can serve as a loss regularizer.

</details>


### [86] [MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization](https://arxiv.org/abs/2509.09387)
*Mohammed Tiouti,Mohamed Bal-Ghaoui*

Main category: cs.LG

TL;DR: 提出MetaLLMiX零样本超参数优化框架，结合多种技术推荐最优超参和预训练模型，实验表明其性能优且大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 深度学习中有效模型和超参数选择有挑战，现有基于大语言模型的方法依赖试错和昂贵API，解释性和泛化性有限。

Method: 提出MetaLLMiX框架，结合元学习、可解释AI和高效大语言模型推理，利用历史实验结果和SHAP解释推荐超参数，采用大语言模型作为评判器控制输出。

Result: 在八个医学影像数据集上，MetaLLMiX性能优于传统超参数优化方法，本地部署优于基于API的方法，多项指标表现出色。

Conclusion: MetaLLMiX是有效的零样本超参数优化框架，能在降低计算成本的同时保持较好的性能。

Abstract: Effective model and hyperparameter selection remains a major challenge in
deep learning, often requiring extensive expertise and computation. While
AutoML and large language models (LLMs) promise automation, current LLM-based
approaches rely on trial and error and expensive APIs, which provide limited
interpretability and generalizability. We propose MetaLLMiX, a zero-shot
hyperparameter optimization framework combining meta-learning, explainable AI,
and efficient LLM reasoning. By leveraging historical experiment outcomes with
SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained
models without additional trials. We further employ an LLM-as-judge evaluation
to control output format, accuracy, and completeness. Experiments on eight
medical imaging datasets using nine open-source lightweight LLMs show that
MetaLLMiX achieves competitive or superior performance to traditional HPO
methods while drastically reducing computational cost. Our local deployment
outperforms prior API-based approaches, achieving optimal results on 5 of 8
tasks, response time reductions of 99.6-99.9%, and the fastest training times
on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of
best-performing baselines.

</details>


### [87] [LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations](https://arxiv.org/abs/2509.09396)
*Harry Mayne,Ryan Othniel Kearns,Yushi Yang,Andrew M. Bean,Eoin Delaney,Chris Russell,Adam Mahdi*

Main category: cs.LG

TL;DR: 研究大语言模型生成反事实解释的有效性和最小性，发现存在有效性 - 最小性权衡，SCEs可能是无效或误导性的解释工具。


<details>
  <summary>Details</summary>
Motivation: 为使语言模型能与人类有效协作，需其用自然语言解释决策，研究自生成反事实解释（SCEs）。

Method: 评估大语言模型生成的SCEs是否有效和最小，在不同模型、数据集和评估设置下进行研究。

Result: 大语言模型生成的SCEs通常有效但不最小，要求生成最小反事实时编辑过小无法改变预测，存在有效性 - 最小性权衡。

Conclusion: SCEs最多是无效的可解释性工具，最坏会误导模型行为，高风险场景部署大语言模型需考虑不可靠自解释的影响。

Abstract: To collaborate effectively with humans, language models must be able to
explain their decisions in natural language. We study a specific type of
self-explanation: self-generated counterfactual explanations (SCEs), where a
model explains its prediction by modifying the input such that it would have
predicted a different outcome. We evaluate whether LLMs can produce SCEs that
are valid, achieving the intended outcome, and minimal, modifying the input no
more than necessary. When asked to generate counterfactuals, we find that LLMs
typically produce SCEs that are valid, but far from minimal, offering little
insight into their decision-making behaviour. Worryingly, when asked to
generate minimal counterfactuals, LLMs typically make excessively small edits
that fail to change predictions. The observed validity-minimality trade-off is
consistent across several LLMs, datasets, and evaluation settings. Our findings
suggest that SCEs are, at best, an ineffective explainability tool and, at
worst, can provide misleading insights into model behaviour. Proposals to
deploy LLMs in high-stakes settings must consider the impact of unreliable
self-explanations on downstream decision-making. Our code is available at
https://github.com/HarryMayne/SCEs.

</details>


### [88] [Kriging prior Regression: A Case for Kriging-Based Spatial Features with TabPFN in Soil Mapping](https://arxiv.org/abs/2509.09408)
*Jonas Schmidinger,Viacheslav Barkov,Sebastian Vogel,Martin Atzmueller,Gerard B M Heuvelink*

Main category: cs.LG

TL;DR: 提出“克里金先验回归”（KpR）混合框架，结合机器学习与地统计学，用TabPFN模型评估，结果显示KpR与TabPFN比其他方法更优，是精准农业数字土壤制图的强大框架。


<details>
  <summary>Details</summary>
Motivation: 机器学习和地统计学是预测和空间映射土壤属性的不同框架，希望结合二者优势提出新方法。

Method: 提出KpR框架，通过普通克里金的“空间滞后”特征丰富机器学习的空间上下文，使用TabPFN模型在六个实地数据集上评估KpR的点预测和概率预测性能。

Result: KpR与TabPFN比其他空间技术和非空间机器学习算法有更可靠的不确定性估计和更准确的预测，平均R2比无空间上下文的机器学习算法提高约30%。

Conclusion: KpR与TabPFN是精准农业数字土壤制图非常稳健且通用的建模框架。

Abstract: Machine learning and geostatistics are two fundamentally different frameworks
for predicting and spatially mapping soil properties. Geostatistics leverages
the spatial structure of soil properties, while machine learning captures the
relationship between available environmental features and soil properties. We
propose a hybrid framework that enriches ML with spatial context through
engineering of 'spatial lag' features from ordinary kriging. We call this
approach 'kriging prior regression' (KpR), as it follows the inverse logic of
regression kriging. To evaluate this approach, we assessed both the point and
probabilistic prediction performance of KpR, using the TabPFN model across six
fieldscale datasets from LimeSoDa. These datasets included soil organic carbon,
clay content, and pH, along with features derived from remote sensing and
in-situ proximal soil sensing. KpR with TabPFN demonstrated reliable
uncertainty estimates and more accurate predictions in comparison to several
other spatial techniques (e.g., regression/residual kriging with TabPFN), as
well as to established non-spatial machine learning algorithms (e.g., random
forest). Most notably, it significantly improved the average R2 by around 30%
compared to machine learning algorithms without spatial context. This
improvement was due to the strong prediction performance of the TabPFN
algorithm itself and the complementary spatial information provided by KpR
features. TabPFN is particularly effective for prediction tasks with small
sample sizes, common in precision agriculture, whereas KpR can compensate for
weak relationships between sensing features and soil properties when proximal
soil sensing data are limited. Hence, we conclude that KpR with TabPFN is a
very robust and versatile modelling framework for digital soil mapping in
precision agriculture.

</details>


### [89] [Fused Lasso Improves Accuracy of Co-occurrence Network Inference in Grouped Samples](https://arxiv.org/abs/2509.09413)
*Daniel Agyapong,Briana H. Beatty,Peter G. Kennedy,Toby D. Hocking*

Main category: cs.LG

TL;DR: 研究针对现有微生物群落共现网络推理算法不足，提出SAC框架和fuser算法，结果显示fuser在同质环境预测表现与现有算法相当，跨环境场景降低测试误差。


<details>
  <summary>Details</summary>
Motivation: 现有算法多分析单一环境样本微生物关联，未考虑不同生态条件下微生物群落关联变化，研究旨在解决此局限。

Method: 分析多地点和时间点公开微生物丰度数据，用SAC框架评估算法预测微生物关联性能，提出fuser算法。

Result: fuser在同质环境中与glmnet等现有算法预测性能相当，在跨环境场景比基线算法显著降低测试误差。

Conclusion: fuser算法在微生物群落网络推理中具有一定优势，尤其在跨环境场景表现良好。

Abstract: Co-occurrence network inference algorithms have significantly advanced our
understanding of microbiome communities. However, these algorithms typically
analyze microbial associations within samples collected from a single
environmental niche, often capturing only static snapshots rather than dynamic
microbial processes. Previous studies have commonly grouped samples from
different environmental niches together without fully considering how microbial
communities adapt their associations when faced with varying ecological
conditions. Our study addresses this limitation by explicitly investigating
both spatial and temporal dynamics of microbial communities. We analyzed
publicly available microbiome abundance data across multiple locations and time
points, to evaluate algorithm performance in predicting microbial associations
using our proposed Same-All Cross-validation (SAC) framework. SAC evaluates
algorithms in two distinct scenarios: training and testing within the same
environmental niche (Same), and training and testing on combined data from
multiple environmental niches (All). To overcome the limitations of
conventional algorithms, we propose fuser, an algorithm that, while not
entirely new in machine learning, is novel for microbiome community network
inference. It retains subsample-specific signals while simultaneously sharing
relevant information across environments during training. Unlike standard
approaches that infer a single generalized network from combined data, fuser
generates distinct, environment-specific predictive networks. Our results
demonstrate that fuser achieves comparable predictive performance to existing
algorithms such as glmnet when evaluated within homogeneous environments
(Same), and notably reduces test error compared to baseline algorithms in
cross-environment (All) scenarios.

</details>


### [90] [Composable Score-based Graph Diffusion Model for Multi-Conditional Molecular Generation](https://arxiv.org/abs/2509.09451)
*Anjie Qiao,Zhen Wang,Chuan Chen,DeFu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 提出可组合分数图扩散模型CSGD用于可控分子图生成，在多数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有图扩散模型在多条件设置下效果有限，依赖联合条件或连续松弛会影响保真度。

Method: 提出CSGD模型，通过具体分数将分数匹配扩展到离散图，引入Composable Guidance和Probability Calibration技术。

Result: 在四个分子数据集上取得SOTA性能，可控性比先前方法平均提高15.3%，保持高有效性和分布保真度。

Conclusion: 基于分数的建模对离散图生成有实际优势，具备灵活的多属性分子设计能力。

Abstract: Controllable molecular graph generation is essential for material and drug
discovery, where generated molecules must satisfy diverse property constraints.
While recent advances in graph diffusion models have improved generation
quality, their effectiveness in multi-conditional settings remains limited due
to reliance on joint conditioning or continuous relaxations that compromise
fidelity. To address these limitations, we propose Composable Score-based Graph
Diffusion model (CSGD), the first model that extends score matching to discrete
graphs via concrete scores, enabling flexible and principled manipulation of
conditional guidance. Building on this foundation, we introduce two score-based
techniques: Composable Guidance (CoG), which allows fine-grained control over
arbitrary subsets of conditions during sampling, and Probability Calibration
(PC), which adjusts estimated transition probabilities to mitigate train-test
mismatches. Empirical results on four molecular datasets show that CSGD
achieves state-of-the-art performance, with a 15.3% average improvement in
controllability over prior methods, while maintaining high validity and
distributional fidelity. Our findings highlight the practical advantages of
score-based modeling for discrete graph generation and its capacity for
flexible, multi-property molecular design.

</details>


### [91] [AquaCast: Urban Water Dynamics Forecasting with Precipitation-Informed Multi-Input Transformer](https://arxiv.org/abs/2509.09458)
*Golnoosh Abdollahinejad,Saleh Baghersalimi,Denisa-Andreea Constantinescu,Sergey Shevchik,David Atienza*

Main category: cs.LG

TL;DR: 本文提出AquaCast深度学习模型预测城市水动力学，在真实和合成数据集上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 应对城市水动力学预测挑战。

Method: 开发多输入多输出深度学习模型AquaCast，融合外生变量，通过嵌入层处理外生输入。

Result: 在LausanneCity数据集上表现达到先进水平，加入外生变量和预报报告性能提升，在三个大规模合成数据集上也表现出色。

Conclusion: 模型始终优于现有基线，在真实和合成数据集上能进行稳健准确的预测。

Abstract: This work addresses the challenge of forecasting urban water dynamics by
developing a multi-input, multi-output deep learning model that incorporates
both endogenous variables (e.g., water height or discharge) and exogenous
factors (e.g., precipitation history and forecast reports). Unlike conventional
forecasting, the proposed model, AquaCast, captures both inter-variable and
temporal dependencies across all inputs, while focusing forecast solely on
endogenous variables. Exogenous inputs are fused via an embedding layer,
eliminating the need to forecast them and enabling the model to attend to their
short-term influences more effectively. We evaluate our approach on the
LausanneCity dataset, which includes measurements from four urban drainage
sensors, and demonstrate state-of-the-art performance when using only
endogenous variables. Performance also improves with the inclusion of exogenous
variables and forecast reports. To assess generalization and scalability, we
additionally test the model on three large-scale synthesized datasets,
generated from MeteoSwiss records, the Lorenz Attractors model, and the Random
Fields model, each representing a different level of temporal complexity across
100 nodes. The results confirm that our model consistently outperforms existing
baselines and maintains a robust and accurate forecast across both real and
synthetic datasets.

</details>


### [92] [AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings](https://arxiv.org/abs/2509.09470)
*Om Vishesh,Harshad Khadilkar,Deepak Akkil*

Main category: cs.LG

TL;DR: 提出自动系统，用AI代理识别论文并执行RPA，验证效果好，展示了任务导向AI代理潜力。


<details>
  <summary>Details</summary>
Motivation: 解决学术文献增长快，学术发现手动工作耗时的问题。

Method: 构建从数据发现到直接行动的自动化系统，用AI代理'Agent - E'识别特定地理区域论文并执行RPA。

Result: 在586篇论文上验证，召回率100%，准确率99.4%。

Conclusion: 任务导向AI代理不仅能过滤信息，还能参与并加速学术社区工作流程。

Abstract: Keeping pace with the rapid growth of academia literature presents a
significant challenge for researchers, funding bodies, and academic societies.
To address the time-consuming manual effort required for scholarly discovery,
we present a novel, fully automated system that transitions from data discovery
to direct action. Our pipeline demonstrates how a specialized AI agent,
'Agent-E', can be tasked with identifying papers from specific geographic
regions within conference proceedings and then executing a Robotic Process
Automation (RPA) to complete a predefined action, such as submitting a
nomination form. We validated our system on 586 papers from five different
conferences, where it successfully identified every target paper with a recall
of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the
potential of task-oriented AI agents to not only filter information but also to
actively participate in and accelerate the workflows of the academic community.

</details>


### [93] [CountTRuCoLa: Rule Confidence Learning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2509.09474)
*Julia Gastinger,Christian Meilicke,Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: 提出基于时间规则的可解释方法进行时间知识图谱预测，性能超多个模型且预测可解释


<details>
  <summary>Details</summary>
Motivation: 受使用循环事实的强基线工作启发，解决时间知识图谱预测任务

Method: 学习四种简单类型规则，使用考虑近期性和频率的置信函数

Result: 在九个数据集上评估，性能匹配或超越八个最先进模型和两个基线

Conclusion: 该方法能在时间知识图谱预测中提供完全可解释的预测

Abstract: We address the task of temporal knowledge graph (TKG) forecasting by
introducing a fully explainable method based on temporal rules. Motivated by
recent work proposing a strong baseline using recurrent facts, our approach
learns four simple types of rules with a confidence function that considers
both recency and frequency. Evaluated on nine datasets, our method matches or
surpasses the performance of eight state-of-the-art models and two baselines,
while providing fully interpretable predictions.

</details>


### [94] [Balancing Utility and Privacy: Dynamically Private SGD with Random Projection](https://arxiv.org/abs/2509.09485)
*Zhanhong Jiang,Md Zahid Hasan,Nastaran Saadati,Aditya Balu,Chao Liu,Soumik Sarkar*

Main category: cs.LG

TL;DR: 文章提出D2P2 - SGD优化器，结合动态差分隐私和随机投影与SGD，有次线性收敛率，实验显示能提升准确率并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 现有随机优化方法存在隐私泄露问题，DPSGD有静态噪声机制影响模型性能，且模型参数增加使高效学习更具挑战。

Method: 引入D2P2 - SGD优化器，结合动态差分隐私与自动梯度裁剪、随机投影与SGD，动态调整模型效用和隐私的权衡。

Result: D2P2 - SGD在不同目标函数下有可证明的次线性收敛率，匹配最优可用率；理论分析表明动态差分隐私以隐私为代价带来更好效用，随机投影使模型学习更高效。

Conclusion: 大量实验表明D2P2 - SGD能显著提高准确率并保持隐私。

Abstract: Stochastic optimization is a pivotal enabler in modern machine learning,
producing effective models for various tasks. However, several existing works
have shown that model parameters and gradient information are susceptible to
privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy
concerns, its static noise mechanism impacts the error bounds for model
performance. Additionally, with the exponential increase in model parameters,
efficient learning of these models using stochastic optimizers has become more
challenging. To address these concerns, we introduce the Dynamically
Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we
combine two important ideas: (i) dynamic differential privacy (DDP) with
automatic gradient clipping and (ii) random projection with SGD, allowing
dynamic adjustment of the tradeoff between utility and privacy of the model. It
exhibits provably sub-linear convergence rates across different objective
functions, matching the best available rate. The theoretical analysis further
suggests that DDP leads to better utility at the cost of privacy, while random
projection enables more efficient model learning. Extensive experiments across
diverse datasets show that D2P2-SGD remarkably enhances accuracy while
maintaining privacy. Our code is available here.

</details>


### [95] [PIPES: A Meta-dataset of Machine Learning Pipelines](https://arxiv.org/abs/2509.09512)
*Cynthia Moreira Maia,Lucas B. V. de Amorim,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: 为降低机器学习算法选择问题的计算成本，分析OpenML局限性后提出PIPES，包含多管道实验结果，可跨多样管道和数据集分析，有扩展潜力。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习算法选择问题中评估算法性能计算成本高的问题，且OpenML存在管道多样性不足等局限。

Method: 提出PIPES，将9408个管道应用于300个数据集进行实验，并存储详细结果。

Result: 得到涵盖管道块、训练测试时间、预测、性能和错误信息等的综合实验结果。

Conclusion: PIPES能让研究者跨多样管道和数据集分析，有扩展潜力，支持元学习社区。

Abstract: Solutions to the Algorithm Selection Problem (ASP) in machine learning face
the challenge of high computational costs associated with evaluating various
algorithms' performances on a given dataset. To mitigate this cost, the
meta-learning field can leverage previously executed experiments shared in
online repositories such as OpenML. OpenML provides an extensive collection of
machine learning experiments. However, an analysis of OpenML's records reveals
limitations. It lacks diversity in pipelines, specifically when exploring data
preprocessing steps/blocks, such as scaling or imputation, resulting in limited
representation. Its experiments are often focused on a few popular techniques
within each pipeline block, leading to an imbalanced sample. To overcome the
observed limitations of OpenML, we propose PIPES, a collection of experiments
involving multiple pipelines designed to represent all combinations of the
selected sets of techniques, aiming at diversity and completeness. PIPES stores
the results of experiments performed applying 9,408 pipelines to 300 datasets.
It includes detailed information on the pipeline blocks, training and testing
times, predictions, performances, and the eventual error messages. This
comprehensive collection of results allows researchers to perform analyses
across diverse and representative pipelines and datasets. PIPES also offers
potential for expansion, as additional data and experiments can be incorporated
to support the meta-learning community further. The data, code, supplementary
material, and all experiments can be found at
https://github.com/cynthiamaia/PIPES.git.

</details>


### [96] [Cough Classification using Few-Shot Learning](https://arxiv.org/abs/2509.09515)
*Yoga Disha Sendhil Kumar,Manas V Shetty,Sudip Vhaduri*

Main category: cs.LG

TL;DR: 研究少样本学习用于呼吸音分类，尤其是基于咳嗽检测疾病的有效性，模型有竞争力，显示少样本学习在医疗诊断可行。


<details>
  <summary>Details</summary>
Motivation: 探究少样本学习在呼吸音分类中能否用更少样本达到传统深度学习性能，比较多分类和二分类模型表现。

Method: 利用原型网络和咳嗽声频谱图表示，评估少样本学习模型，对比多分类和二分类模型。

Result: 少样本学习模型有竞争力，多分类74.87%准确率，二分类超70%，流感最易区分，健康最难，两类模型无显著性能差异。

Conclusion: 少样本学习在医疗诊断尤其是缺乏大标注数据集时可行。

Abstract: This paper investigates the effectiveness of few-shot learning for
respiratory sound classification, focusing on coughbased detection of COVID-19,
Flu, and healthy conditions. We leverage Prototypical Networks with spectrogram
representations of cough sounds to address the challenge of limited labeled
data. Our study evaluates whether few-shot learning can enable models to
achieve performance comparable to traditional deep learning approaches while
using significantly fewer training samples. Additionally, we compare
multi-class and binary classification models to assess whether multi-class
models can perform comparably to their binary counterparts. Experimental
findings show that few-shot learning models can achieve competitive accuracy.
Our model attains 74.87% accuracy in multi-class classification with only 15
support examples per class, while binary classification achieves over 70%
accuracy across all class pairs. Class-wise analysis reveals Flu as the most
distinguishable class, and Healthy as the most challenging. Statistical tests
(paired t-test p = 0.149, Wilcoxon p = 0.125) indicate no significant
performance difference between binary and multiclass models, supporting the
viability of multi-class classification in this setting. These results
highlight the feasibility of applying few-shot learning in medical diagnostics,
particularly when large labeled datasets are unavailable.

</details>


### [97] [Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication](https://arxiv.org/abs/2509.09597)
*Maysam Behmanesh,Erkan Turan,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: 提出新图对齐框架，结合双通编码器和几何感知功能图模块，实验显示其优于现有无监督方法，且能泛化到视觉 - 语言领域。


<details>
  <summary>Details</summary>
Motivation: 现有无监督图对齐方法存在节点区分度下降和潜在空间不对齐的问题，导致节点对应不可靠。

Method: 提出结合低通和高通光谱滤波器的双通编码器生成嵌入，引入几何感知功能图模块学习图嵌入间的双射和等距变换。

Result: 在图基准测试中始终优于现有无监督对齐基线，在视觉 - 语言基准测试中能有效泛化。

Conclusion: 所提框架增强了节点区分度和潜在空间几何一致性，具有良好性能和泛化能力。

Abstract: Graph alignment-the problem of identifying corresponding nodes across
multiple graphs-is fundamental to numerous applications. Most existing
unsupervised methods embed node features into latent representations to enable
cross-graph comparison without ground-truth correspondences. However, these
methods suffer from two critical limitations: the degradation of node
distinctiveness due to oversmoothing in GNN-based embeddings, and the
misalignment of latent spaces across graphs caused by structural noise, feature
heterogeneity, and training instability, ultimately leading to unreliable node
correspondences. We propose a novel graph alignment framework that
simultaneously enhances node distinctiveness and enforces geometric consistency
across latent spaces. Our approach introduces a dual-pass encoder that combines
low-pass and high-pass spectral filters to generate embeddings that are both
structure-aware and highly discriminative. To address latent space
misalignment, we incorporate a geometry-aware functional map module that learns
bijective and isometric transformations between graph embeddings, ensuring
consistent geometric relationships across different representations. Extensive
experiments on graph benchmarks demonstrate that our method consistently
outperforms existing unsupervised alignment baselines, exhibiting superior
robustness to structural inconsistencies and challenging alignment scenarios.
Additionally, comprehensive evaluation on vision-language benchmarks using
diverse pretrained models shows that our framework effectively generalizes
beyond graph domains, enabling unsupervised alignment of vision and language
representations.

</details>


### [98] [Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics](https://arxiv.org/abs/2509.09599)
*Ira J. S. Shokar,Rich R. Kerswell,Peter H. Haynes*

Main category: cs.LG

TL;DR: 提出基于PDE参数值的深度学习模拟器，经预训练和微调实现跨参数泛化，在两个方程上验证其能力，有计算加速和不确定性量化优势。


<details>
  <summary>Details</summary>
Motivation: 为随机和混沌时空系统开发一种能基于PDE参数值进行泛化的深度学习模拟器。

Method: 先在单一参数域预训练模型，再在小而多样的数据集上微调；引入局部注意力机制。

Result: 在Kuramoto - Sivashinsky方程和随机强迫beta平面湍流上展示了模型在插值参数值上捕捉现象的能力；模拟器比传统数值积分有显著计算加速。

Conclusion: 该模拟器能有效探索参数空间，其概率变体可进行不确定性量化和罕见事件统计研究。

Abstract: We present a deep learning emulator for stochastic and chaotic
spatio-temporal systems, explicitly conditioned on the parameter values of the
underlying partial differential equations (PDEs). Our approach involves
pre-training the model on a single parameter domain, followed by fine-tuning on
a smaller, yet diverse dataset, enabling generalisation across a broad range of
parameter values. By incorporating local attention mechanisms, the network is
capable of handling varying domain sizes and resolutions. This enables
computationally efficient pre-training on smaller domains while requiring only
a small additional dataset to learn how to generalise to larger domain sizes.
We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky
equation and stochastically-forced beta-plane turbulence, showcasing its
ability to capture phenomena at interpolated parameter values. The emulator
provides significant computational speed-ups over conventional numerical
integration, facilitating efficient exploration of parameter space, while a
probabilistic variant of the emulator provides uncertainty quantification,
allowing for the statistical study of rare events.

</details>


### [99] [ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance](https://arxiv.org/abs/2509.09611)
*Haolan Zheng,Yanlai Chen,Jiequn Han,Yue Yu*

Main category: cs.LG

TL;DR: 提出Reduced Basis Neural Operator (ReBaNO)算法解决含多不同输入的PDEs，性能优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 解决一组含多个不同输入的偏微分方程（PDEs），并改善现有算子学习算法的泛化差距和离散化不变性问题。

Method: 受简化基方法和生成式预训练物理信息神经网络启发，用严格贪婪算法离线自适应构建网络结构，通过特定任务激活函数进行知识蒸馏。

Result: 数值结果表明，与PCA - Net、DeepONet、FNO和CNO等先进算子学习算法相比，ReBaNO在消除/缩小泛化差距和实现严格离散化不变性方面表现更优。

Conclusion: ReBaNO是一种有效的数据精简算子学习算法，在解决含多个不同输入的PDEs方面具有显著优势。

Abstract: We propose a novel data-lean operator learning algorithm, the Reduced Basis
Neural Operator (ReBaNO), to solve a group of PDEs with multiple distinct
inputs. Inspired by the Reduced Basis Method and the recently introduced
Generative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a
mathematically rigorous greedy algorithm to build its network structure offline
adaptively from the ground up. Knowledge distillation via task-specific
activation function allows ReBaNO to have a compact architecture requiring
minimal computational cost online while embedding physics. In comparison to
state-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO,
and CNO, numerical results demonstrate that ReBaNO significantly outperforms
them in terms of eliminating/shrinking the generalization gap for both in- and
out-of-distribution tests and being the only operator learning algorithm
achieving strict discretization invariance.

</details>


### [100] [Explaining Concept Drift through the Evolution of Group Counterfactuals](https://arxiv.org/abs/2509.09616)
*Ignacy Stępka,Jerzy Stefanowski*

Main category: cs.LG

TL;DR: 本文提出通过分析基于组的反事实解释（GCEs）的时间演变来解释概念漂移的新方法，在三层框架中操作分析，能更全面诊断漂移并区分不同根源。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在动态环境中受概念漂移影响，检测漂移已有研究，但解释模型决策逻辑变化的方式和原因仍是重大挑战。

Method: 引入分析GCEs时间演变的方法，跟踪漂移前后GCEs的聚类质心及其相关反事实行动向量的变化，在三层框架中结合数据层、模型层和解释层的见解进行分析。

Result: 实现了对概念漂移更全面的诊断，能够区分不同的根源，如空间数据转移与概念重新标记。

Conclusion: 所提出的方法通过分析GCEs和三层框架，为解释概念漂移提供了有效途径，有助于更深入理解模型决策逻辑的变化。

Abstract: Machine learning models in dynamic environments often suffer from concept
drift, where changes in the data distribution degrade performance. While
detecting this drift is a well-studied topic, explaining how and why the
model's decision-making logic changes still remains a significant challenge. In
this paper, we introduce a novel methodology to explain concept drift by
analyzing the temporal evolution of group-based counterfactual explanations
(GCEs). Our approach tracks shifts in the GCEs' cluster centroids and their
associated counterfactual action vectors before and after a drift. These
evolving GCEs act as an interpretable proxy, revealing structural changes in
the model's decision boundary and its underlying rationale. We operationalize
this analysis within a three-layer framework that synergistically combines
insights from the data layer (distributional shifts), the model layer
(prediction disagreement), and our proposed explanation layer. We show that
such holistic view allows for a more comprehensive diagnosis of drift, making
it possible to distinguish between different root causes, such as a spatial
data shift versus a re-labeling of concepts.

</details>


### [101] [Functional Groups are All you Need for Chemically Interpretable Molecular Property Prediction](https://arxiv.org/abs/2509.09619)
*Roshan Balaji,Joe Bobby,Nirav Pravinbhai Bhatt*

Main category: cs.LG

TL;DR: 提出基于官能团概念的FGR框架进行分子表示，在33个基准数据集上表现优异且具化学可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型用于分子属性预测时缺乏可解释性，阻碍化学家采用。

Method: 引入FGR框架，整合既定化学知识的官能团和从大型分子语料库挖掘的官能团，利用预训练将分子编码到低维潜在空间，并可纳入基于2D结构的描述符。

Result: FGR框架在33个基准数据集上达到了先进水平。

Conclusion: 该工作为分子发现开发高性能、化学可解释的深度学习模型迈出重要一步。

Abstract: Molecular property prediction using deep learning (DL) models has accelerated
drug and materials discovery, but the resulting DL models often lack
interpretability, hindering their adoption by chemists. This work proposes
developing molecule representations using the concept of Functional Groups (FG)
in chemistry. We introduce the Functional Group Representation (FGR) framework,
a novel approach to encoding molecules based on their fundamental chemical
substructures. Our method integrates two types of functional groups: those
curated from established chemical knowledge (FG), and those mined from a large
molecular corpus using sequential pattern mining (MFG). The resulting FGR
framework encodes molecules into a lower-dimensional latent space by leveraging
pre-training on a large dataset of unlabeled molecules. Furthermore, the
proposed framework allows the inclusion of 2D structure-based descriptors of
molecules. We demonstrate that the FGR framework achieves state-of-the-art
performance on a diverse range of 33 benchmark datasets spanning physical
chemistry, biophysics, quantum mechanics, biological activity, and
pharmacokinetics while enabling chemical interpretability. Crucially, the
model's representations are intrinsically aligned with established chemical
principles, allowing chemists to directly link predicted properties to specific
functional groups and facilitating novel insights into structure-property
relationships. Our work presents a significant step toward developing
high-performing, chemically interpretable DL models for molecular discovery.

</details>


### [102] [Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management](https://arxiv.org/abs/2509.09655)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: 介绍FG - FARL离线RL程序，通过 Medicaid 项目数据评估，其在实现相近价值时提升公平性指标。


<details>
  <summary>Details</summary>
Motivation: 在平衡不同受保护子组公平目标（覆盖或伤害）的同时减少伤害。

Method: 引入FG - FARL离线RL程序，用 Medicaid 项目去识别化的纵向轨迹数据进行评估，报告带置信区间的离策略价值估计和带p值的子组差异分析。

Result: FG - FARL与基线实现相近价值，同时改善了公平性指标。

Conclusion: FG - FARL为更安全和更公平的决策支持提供了可行路径。

Abstract: We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning
(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds
to reduce harm while equalizing a chosen fairness target (coverage or harm)
across protected subgroups. Using de-identified longitudinal trajectories from
a Medicaid population health management program, we evaluate FG-FARL against
behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global
conformal safety baseline). We report off-policy value estimates with bootstrap
95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL
achieves comparable value to baselines while improving fairness metrics,
demonstrating a practical path to safer and more equitable decision support.

</details>


### [103] [ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms](https://arxiv.org/abs/2509.09679)
*Bingxin Xu,Zhen Dong,Oussama Elachqar,Yuzhang Shang*

Main category: cs.LG

TL;DR: 大语言模型内存占用大，极端2位量化因激活值离群点有性能损失，现有旋转方法固定，本文提出ButterflyQuant，用可学习蝶形变换，在LLaMA - 2 - 7B上2位量化表现优于QuaRot。


<details>
  <summary>Details</summary>
Motivation: 现有基于旋转的量化方法使用固定变换，不能适应特定权重分布，不同Transformer层有不同离群点模式，需层自适应旋转。

Method: 提出ButterflyQuant，用可学习蝶形变换替代Hadamard旋转，引入后变换激活值的均匀性正则化，只需128个校准样本学习，在单GPU上几分钟收敛。

Result: 在LLaMA - 2 - 7B的2位量化中，ButterflyQuant困惑度为15.4，QuaRot为22.1。

Conclusion: ButterflyQuant在解决大语言模型极端量化性能损失问题上有效，优于现有固定旋转量化方法。

Abstract: Large language models require massive memory footprints, severely limiting
deployment on consumer hardware. Quantization reduces memory through lower
numerical precision, but extreme 2-bit quantization suffers from catastrophic
performance loss due to outliers in activations. Rotation-based methods such as
QuIP and QuaRot apply orthogonal transforms to eliminate outliers before
quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} =
(\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these
methods use fixed transforms--Hadamard matrices achieving optimal worst-case
coherence $\mu = 1/\sqrt{n}$--that cannot adapt to specific weight
distributions. We identify that different transformer layers exhibit distinct
outlier patterns, motivating layer-adaptive rotations rather than
one-size-fits-all approaches. We propose ButterflyQuant, which replaces
Hadamard rotations with learnable butterfly transforms parameterized by
continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$
entries that are non-differentiable and prohibit gradient-based learning,
butterfly transforms' continuous parameterization enables smooth optimization
while guaranteeing orthogonality by construction. This orthogonal constraint
ensures theoretical guarantees in outlier suppression while achieving $O(n \log
n)$ computational complexity with only $\frac{n \log n}{2}$ learnable
parameters. We further introduce a uniformity regularization on
post-transformation activations to promote smoother distributions amenable to
quantization. Learning requires only 128 calibration samples and converges in
minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit
quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [104] [Time-Fair Benchmarking for Metaheuristics: A Restart-Fair Protocol for Fixed-Time Comparisons](https://arxiv.org/abs/2509.08986)
*Junbo Jacob Lian*

Main category: cs.NE

TL;DR: 论文指出应使用时钟时间而非仅函数评估次数来公平比较元启发式算法，提出固定时间、重启公平的基准测试协议，倡导使用基于时间的性能指标，并引入清单规范报告实践。


<details>
  <summary>Details</summary>
Motivation: 现有元启发式算法常以函数评估次数宣称性能优越，但隐藏额外计算负担，需更公平的比较方式。

Method: 制定固定时间、重启公平的基准测试协议，采用基于时间的性能指标，引入清单规范报告。

Result: 未提及具体结果。

Conclusion: 该方法能促进对元启发式算法更可信且实用的评估。

Abstract: Numerous purportedly improved metaheuristics claim superior performance based
on equivalent function evaluations (FEs), yet often conceal additional
computational burdens in more intensive iterations, preprocessing stages, or
hyperparameter tuning. This paper posits that wall-clock time, rather than
solely FEs, should serve as the principal budgetary constraint for equitable
comparisons. We formalize a fixed-time, restart-fair benchmarking protocol
wherein each algorithm is allotted an identical wall-clock time budget per
problem instance, permitting unrestricted utilization of restarts, early
termination criteria, and internal adaptive mechanisms. We advocate for the
adoption of anytime performance curves, expected running time (ERT) metrics,
and performance profiles that employ time as the cost measure, all aimed at
predefined targets. Furthermore, we introduce a concise, reproducible checklist
to standardize reporting practices and mitigate undisclosed computational
overheads. This approach fosters more credible and practically relevant
evaluations of metaheuristic algorithms.

</details>


### [105] [A modified RIME algorithm with covariance learning and diversity enhancement for numerical optimization](https://arxiv.org/abs/2509.09529)
*Shangqing Shi,Luoxiao Zhang,Yuchen Yin,Xiong Yang,Hoileong Lee*

Main category: cs.NE

TL;DR: 本文提出改进的MRIME - CD算法解决RIME算法不足，经多测试集验证，该算法在解精度、收敛速度和稳定性上有优势。


<details>
  <summary>Details</summary>
Motivation: RIME算法在优化过程中种群多样性快速丧失，易陷入局部最优，开发和探索能力不平衡，需改进。

Method: 提出MRIME - CD算法，采用协方差学习策略、平均引导策略和随机协方差学习策略来提升优化能力，并在CEC2017和CEC2022测试集上验证，用Friedman、Wilcoxon秩和与Kruskal Wallis检验分析结果。

Result: MRIME - CD能有效提升基本RIME算法性能，在解精度、收敛速度和稳定性上有明显优势。

Conclusion: MRIME - CD算法可有效解决RIME算法的缺点，是一种更优的算法。

Abstract: Metaheuristics are widely applied for their ability to provide more efficient
solutions. The RIME algorithm is a recently proposed physical-based
metaheuristic algorithm with certain advantages. However, it suffers from rapid
loss of population diversity during optimization and is prone to fall into
local optima, leading to unbalanced exploitation and exploration. To address
the shortcomings of RIME, this paper proposes a modified RIME with covariance
learning and diversity enhancement (MRIME-CD). The algorithm applies three
strategies to improve the optimization capability. First, a covariance learning
strategy is introduced in the soft-rime search stage to increase the population
diversity and balance the over-exploitation ability of RIME through the
bootstrapping effect of dominant populations. Second, in order to moderate the
tendency of RIME population to approach the optimal individual in the early
search stage, an average bootstrapping strategy is introduced into the
hard-rime puncture mechanism, which guides the population search through the
weighted position of the dominant populations, thus enhancing the global search
ability of RIME in the early stage. Finally, a new stagnation indicator is
proposed, and a stochastic covariance learning strategy is used to update the
stagnant individuals in the population when the algorithm gets stagnant, thus
enhancing the ability to jump out of the local optimal solution. The proposed
MRIME-CD algorithm is subjected to a series of validations on the CEC2017 test
set, the CEC2022 test set, and the experimental results are analyzed using the
Friedman test, the Wilcoxon rank sum test, and the Kruskal Wallis test. The
results show that MRIME-CD can effectively improve the performance of basic
RIME and has obvious superiorities in terms of solution accuracy, convergence
speed and stability.

</details>


### [106] [An improved educational competition optimizer with multi-covariance learning operators for global optimization problems](https://arxiv.org/abs/2509.09552)
*Baoqi Zhao,Xiong Yang,Hoileong Lee,Bowen Dong*

Main category: cs.NE

TL;DR: 本文提出基于多协方差学习算子的增强型教育竞争优化器（IECO - MCO），通过基准函数测试和统计分析验证其在解决复杂优化问题上优于基本ECO和其他算法，还验证了其在约束优化问题中的实用性。


<details>
  <summary>Details</summary>
Motivation: 基本教育竞争优化器（ECO）存在开发与探索不平衡问题，易陷入局部最优，在解决复杂优化问题时效果有限。

Method: 引入三种不同的协方差学习算子改进ECO，利用CEC 2017和CEC 2022测试套件的基准函数评估性能，并与多种算法对比，进行Friedman、Kruskal - Wallis和Wilcoxon秩和检验等统计分析，还求解约束优化问题验证实用性。

Result: IECO - MCO在收敛速度、稳定性和避免局部最优能力上超越基本ECO和其他竞争算法，在CEC2017和CEC2022测试套件上平均排名分别为2.213（改进算法为2.488）。

Conclusion: IECO - MCO在解决复杂优化问题上表现出色，在现实场景中具有鲁棒性和实际有效性。

Abstract: The educational competition optimizer is a recently introduced metaheuristic
algorithm inspired by human behavior, originating from the dynamics of
educational competition within society. Nonetheless, ECO faces constraints due
to an imbalance between exploitation and exploration, rendering it susceptible
to local optima and demonstrating restricted effectiveness in addressing
complex optimization problems. To address these limitations, this study
presents an enhanced educational competition optimizer (IECO-MCO) utilizing
multi-covariance learning operators. In IECO, three distinct covariance
learning operators are introduced to improve the performance of ECO. Each
operator effectively balances exploitation and exploration while preventing
premature convergence of the population. The effectiveness of IECO is assessed
through benchmark functions derived from the CEC 2017 and CEC 2022 test suites,
and its performance is compared with various basic and improved algorithms
across different categories. The results demonstrate that IECO-MCO surpasses
the basic ECO and other competing algorithms in convergence speed, stability,
and the capability to avoid local optima. Furthermore, statistical analyses,
including the Friedman test, Kruskal-Wallis test, and Wilcoxon rank-sum test,
are conducted to validate the superiority of IECO-MCO over the compared
algorithms. Compared with the basic algorithm (improved algorithm), IECO-MCO
achieved an average ranking of 2.213 (2.488) on the CE2017 and CEC2022 test
suites. Additionally, the practical applicability of the proposed IECO-MCO
algorithm is verified by solving constrained optimization problems. The
experimental outcomes demonstrate the superior performance of IECO-MCO in
tackling intricate optimization problems, underscoring its robustness and
practical effectiveness in real-world scenarios.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [107] [HD-MoE: Hybrid and Dynamic Parallelism for Mixture-of-Expert LLMs with 3D Near-Memory Processing](https://arxiv.org/abs/2509.09420)
*Haochen Huang,Shuzhang Zhong,Zhe Zhang,Shuangchen Li,Dimin Niu,Hongzhong Zheng,Runsheng Wang,Meng Li*

Main category: cs.PF

TL;DR: 本文提出HD - MoE优化NMP加速器上MoE并行计算，实验显示相比其他策略有加速效果。


<details>
  <summary>Details</summary>
Motivation: MoE架构的大语言模型对内存容量和带宽要求高，NMP加速器虽有潜力，但现有并行映射策略效率低，动态路由机制加剧挑战。

Method: 提出HD - MoE，包含离线自动混合并行映射算法和在线动态调度策略。

Result: HD - MoE相比TP加速1.1x - 1.8x，相比EP加速1.1x - 1.5x，相比Hybrid TP - EP加速1.0x - 1.4x。

Conclusion: HD - MoE能有效降低通信成本，最大化计算利用率，提高MoE并行计算效率。

Abstract: Large Language Models (LLMs) with Mixture-of-Expert (MoE) architectures
achieve superior model performance with reduced computation costs, but at the
cost of high memory capacity and bandwidth requirements. Near-Memory Processing
(NMP) accelerators that stack memory directly on the compute through hybrid
bonding have demonstrated high bandwidth with high energy efficiency, becoming
a promising architecture for MoE models. However, as NMP accelerators comprise
distributed memory and computation, how to map the MoE computation directly
determines the LLM inference efficiency. Existing parallel mapping strategies,
including Tensor Parallelism (TP) and Expert Parallelism (EP), suffer from
either high communication costs or unbalanced computation utilization, leading
to inferior efficiency. The dynamic routing mechanism of MoE LLMs further
aggravates the efficiency challenges. Therefore, in this paper, we propose
HD-MoE to automatically optimize the MoE parallel computation across an NMP
accelerator. HD-MoE features an offline automatic hybrid parallel mapping
algorithm and an online dynamic scheduling strategy to reduce the communication
costs while maximizing the computation utilization. With extensive experimental
results, we demonstrate that HD-MoE achieves a speedup ranging from 1.1x to
1.8x over TP, 1.1x to 1.5x over EP, and 1.0x to 1.4x over the baseline Hybrid
TP-EP with Compute-Balanced parallelism strategies.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [108] [Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2509.08843)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: 本文介绍Python的glob模块在数据科学等领域作为文件模式匹配工具的应用，通过示例展示其功能，目标是使其成为Python研究工作流中文件模式匹配的默认引用。


<details>
  <summary>Details</summary>
Motivation: Pattern - based file access在计算研究中常缺乏记录，而Python的glob模块可助力跨学科可扩展工作流，故介绍其在多领域应用。

Method: 通过具体Python示例，结合pandas、scikit - learn和matplotlib等常用库展示glob模块功能。

Result: 展示了glob模块在大规模数据摄取、组织数据分析、AI数据集构建和可重复性研究实践等用例中的作用，体现其便于文件遍历和与分析管道集成的特点。

Conclusion: 强调glob模块是可重复性研究和数据工程的方法基石，为研究人员和从业者提供了基础概念与应用实践的桥梁，应成为Python研究工作流中文件模式匹配的默认引用。

Abstract: Pattern-based file access is a fundamental but often under-documented aspect
of computational research. The Python glob module provides a simple yet
powerful way to search, filter, and ingest files using wildcard patterns,
enabling scalable workflows across disciplines. This paper introduces glob as a
versatile tool for data science, business analytics, and artificial
intelligence applications. We demonstrate use cases including large-scale data
ingestion, organizational data analysis, AI dataset construction, and
reproducible research practices. Through concrete Python examples with widely
used libraries such as pandas,scikit-learn, and matplotlib, we show how glob
facilitates efficient file traversal and integration with analytical pipelines.
By situating glob within the broader context of reproducible research and data
engineering, we highlight its role as a methodological building block. Our goal
is to provide researchers and practitioners with a concise reference that
bridges foundational concepts and applied practice, making glob a default
citation for file pattern matching in Python-based research workflows.

</details>


### [109] [A Systematic Mapping Study on Chatbots in Programming Education](https://arxiv.org/abs/2509.08857)
*Marcelino Garcia,Renato Garcia,Arthur Parizotto,Andre Mendes,Pedro Valle,Ricardo Vilela,Renato Balancieri,Williamson Silva*

Main category: cs.SE

TL;DR: 本文通过系统映射研究分析教育聊天机器人在编程教育中的开发与应用，揭示相关趋势与差距，为新教育工具开发提供见解。


<details>
  <summary>Details</summary>
Motivation: 探究教育聊天机器人在编程教育中的开发与应用情况。

Method: 对3216篇出版物筛选出54篇研究，基于五个研究子问题进行分析。

Result: 发现以Python教学为主的聊天机器人居多，聚焦编程基础概念，采用多种教学方法和技术架构。

Conclusion: 本研究识别了文献中的趋势和差距，为编程教学新教育工具的开发提供了见解。

Abstract: Educational chatbots have gained prominence as support tools for teaching
programming, particularly in introductory learning contexts. This paper
presents a Systematic Mapping Study (SMS) that investigated how such agents
have been developed and applied in programming education. From an initial set
of 3,216 publications, 54 studies were selected and analyzed based on five
research subquestions, addressing chatbot types, programming languages used,
educational content covered, interaction models, and application contexts. The
results reveal a predominance of chatbots designed for Python instruction,
focusing on fundamental programming concepts, and employing a wide variety of
pedagogical approaches and technological architectures. In addition to
identifying trends and gaps in the literature, this study provides insights to
inform the development of new educational tools for programming instruction.

</details>


### [110] [GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation](https://arxiv.org/abs/2509.08863)
*Qianqian Luo,Liuchang Xu,Qingming Lin,Sensen Wu,Ruichen Mao,Chao Wang,Hailin Feng,Bo Huang,Zhenhong Du*

Main category: cs.SE

TL;DR: 本文提出GeoJSON Agents多智能体LLM架构处理GIS自动化任务，对比两种增强技术，代码生成法表现更优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在缺乏GIS专业知识时存在局限性，需提升GIS自动化性能和可扩展性。

Method: 提出GeoJSON Agents架构，含任务解析、智能体协作和结果集成三部分，用Function Calling和Code Generation技术处理空间数据，构建70个任务的基准数据集实验。

Result: 基于Function Calling的GeoJSON智能体准确率85.71%，基于Code Generation的为97.14%，均远超通用模型。

Conclusion: 本研究首次引入基于GeoJSON数据的LLM多智能体框架，对比两种方法优缺点，为提升GeoAI系统性能提供新视角。

Abstract: LLMs have made substantial progress in task automation and natural language
understanding.However,without expertise in GIS,they continue to encounter
limitations.To address these issues, we propose GeoJSON Agents-a multi-agent
LLM architecture.This framework transforms natural language tasks into
structured GeoJSON operation commands and processes spatial data using two
widely adopted LLM enhancement techniques:Function Calling and Code
Generation.The architecture consists of three components-task parsing,agent
collaboration,and result integration-aimed at enhancing both the performance
and scalability of GIS automation.The Planner agent interprets natural language
tasks into structured GeoJSON commands.Then,specialized Worker agents
collaborate according to assigned roles to perform spatial data processing and
analysis,either by invoking predefined function APIs or by dynamically
generating and executing Python-based spatial analysis code.Finally,the system
integrates the outputs from multiple execution rounds into
reusable,standards-compliant GeoJSON files.To systematically evaluate the
performance of the two approaches,we constructed a benchmark dataset of 70
tasks with varying complexity and conducted experiments using OpenAI's GPT-4o
as the core model.Results indicate that the Function Calling-based GeoJSON
Agent achieved an accuracy of 85.71%,while the Code Generation-based agent
reached 97.14%,both significantly outperforming the best-performing
general-purpose model (48.57%).Further analysis reveals that the Code
Generation provides greater flexibility,whereas the Function Calling approach
offers more stable execution.This study is the first to introduce an LLM
multi-agent framework for GeoJSON data and to compare the strengths and
limitations of two mainstream LLM enhancement methods,offering new perspectives
for improving GeoAI system performance.

</details>


### [111] [TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis](https://arxiv.org/abs/2509.08865)
*Guangyu Zhang,Xixuan Wang,Shiyu Sun,Peiyan Xiao,Kun Sun,Yanhai Xiong*

Main category: cs.SE

TL;DR: 提出TraceRAG框架用于可解释的安卓恶意软件检测分析，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有安卓恶意软件分析技术难以发现隐藏行为且缺乏可解释性，需要强大深入的分析框架。

Method: 引入TraceRAG框架，生成方法级代码片段摘要并索引，通过查询检索相关片段，基于多轮分析结果生成可读报告。

Result: 实现96%的恶意软件检测准确率和83.81%的行为识别准确率，专家认可报告实用性。

Conclusion: TraceRAG框架能有效进行可解释的安卓恶意软件检测与分析。

Abstract: Sophisticated evasion tactics in malicious Android applications, combined
with their intricate behavioral semantics, enable attackers to conceal
malicious logic within legitimate functions, underscoring the critical need for
robust and in-depth analysis frameworks. However, traditional analysis
techniques often fail to recover deeply hidden behaviors or provide
human-readable justifications for their decisions. Inspired by advances in
large language models (LLMs), we introduce TraceRAG, a retrieval-augmented
generation (RAG) framework that bridges natural language queries and Java code
to deliver explainable malware detection and analysis. First, TraceRAG
generates summaries of method-level code snippets, which are indexed in a
vector database. At query time, behavior-focused questions retrieve the most
semantically relevant snippets for deeper inspection. Finally, based on the
multi-turn analysis results, TraceRAG produces human-readable reports that
present the identified malicious behaviors and their corresponding code
implementations. Experimental results demonstrate that our method achieves 96\%
malware detection accuracy and 83.81\% behavior identification accuracy based
on updated VirusTotal (VT) scans and manual verification. Furthermore, expert
evaluation confirms the practical utility of the reports generated by TraceRAG.

</details>


### [112] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: 本文引入LLM效率基准以模拟真实使用条件，利用vLLM研究多因素对推理能源效率的影响，证明可创建更贴合实际部署条件的能源效率基准。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署和使用耗能大，现有基准难以代表实际生产场景，需收集更多能源效率信息以提高开发者意识。

Method: 引入LLM Efficiency Benchmark模拟真实使用条件，利用vLLM优化性能和效率，研究模型大小、架构和并发请求量等因素对推理能源效率的影响。

Result: 可以创建更能反映实际部署条件的能源效率基准。

Conclusion: 研究结果为开发者构建更可持续的AI系统提供了有价值的见解。

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [113] [CLARA: A Developer's Companion for Code Comprehension and Analysis](https://arxiv.org/abs/2509.09072)
*Ahmed Adnan,Mushfiqur Rahman,Saad Sakib Noor,Kazi Sakib*

Main category: cs.SE

TL;DR: 介绍CLARA浏览器扩展，它用先进推理模型辅助代码理解分析等任务，经评估有用、准确且实用。


<details>
  <summary>Details</summary>
Motivation: 现有代码理解分析工具需项目预设置、缺乏上下文感知且需大量手动工作。

Method: 用现有数据集和方法对CLARA推理模型进行定性评估，对10名开发者和研究人员开展用户研究。

Result: CLARA在代码理解和分析任务中有用、准确且实用。

Conclusion: CLARA是一个可用于代码理解和分析的有效开源工具。

Abstract: Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

</details>


### [114] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: 本文提出高置信度基准数据集ReDef，评估预训练语言模型对代码修改的推理能力，发现当前模型理解代码修改能力有限。


<details>
  <summary>Details</summary>
Motivation: 现有数据集存在标签噪声和识别精度低的问题，需构建更可靠的数据集并评估预训练语言模型对代码修改的推理能力。

Method: 从22个C/C++项目构建ReDef数据集，采用GPT辅助筛选流程；对CodeBERT等模型在五种编码策略下微调，通过反事实扰动测试其敏感性。

Result: 紧凑的差异式编码在所有预训练语言模型中表现优于全函数格式；反事实测试中模型性能几乎无下降，依赖表面线索而非语义理解。

Conclusion: 与基于快照的任务不同，当前预训练语言模型真正理解代码修改的能力有限。

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>


### [115] [On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability](https://arxiv.org/abs/2509.09194)
*Ayelet Berzack,Guy Katz*

Main category: cs.SE

TL;DR: 本文探讨将大语言模型（LLMs）与传统软件工程技术结合用于软件开发，以减少错误并提高可靠性，通过Connect4游戏案例验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: LLMs虽能辅助软件开发，但常引入重大错误，需找到更可靠的方式将其融入软件开发周期。

Method: 提出将LLMs与传统软件工程技术结构化结合的方法，聚焦场景编程（SBP）范式，让开发者融入专业知识并检查验证输出。

Result: 通过Connect4游戏案例，结合LLMs和SBP创建了高性能智能体，能击败现有强大智能体，部分情况下可验证其正确性。

Conclusion: 所提出的方法具有易用性，能有效将LLMs融入软件开发，减少错误并提高可靠性。

Abstract: Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

</details>


### [116] [Altered Histories in Version Control System Repositories: Evidence from the Trenches](https://arxiv.org/abs/2509.09294)
*Solal Rapaport,Laurent Pautet,Samuel Tardieu,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 对公共代码仓库中Git历史记录修改进行大规模调查，发现大量修改情况，介绍检测工具GitHistorian。


<details>
  <summary>Details</summary>
Motivation: Git历史修改在公共分支存在问题，影响工作流、仓库完整性和可重复性，还可能被攻击利用，因此进行研究。

Method: 分析Software Heritage存档的1.11亿个仓库，对修改情况分类，开展两个针对性案例研究。

Result: 在122万个仓库中发现历史修改，共870万次重写历史，常见修改包括追溯更改许可证和移除误提交的“秘密”。

Conclusion: 引入自动化工具GitHistorian，帮助开发者发现和描述公共Git仓库中的历史修改。

Abstract: Version Control Systems (VCS) like Git allow developers to locally rewrite
recorded history, e.g., to reorder and suppress commits or specific data in
them. These alterations have legitimate use cases, but become problematic when
performed on public branches that have downstream users: they break push/pull
workflows, challenge the integrity and reproducibility of repositories, and
create opportunities for supply chain attackers to sneak into them nefarious
changes. We conduct the first large-scale investigation of Git history
alterations in public code repositories. We analyze 111 M (millions)
repositories archived by Software Heritage, which preserves VCS histories even
across alterations. We find history alterations in 1.22 M repositories, for a
total of 8.7 M rewritten histories. We categorize changes by where they happen
(which repositories, which branches) and what is changed in them (files or
commit metadata). Conducting two targeted case studies we show that altered
histories recurrently change licenses retroactively, or are used to remove
''secrets'' (e.g., private keys) committed by mistake. As these behaviors
correspond to bad practices-in terms of project governance or security
management, respectively-that software recipients might want to avoid, we
introduce GitHistorian, an automated tool, that developers can use to spot and
describe history alterations in public Git repositories.

</details>


### [117] [Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data](https://arxiv.org/abs/2509.09313)
*Moritz Mock,Thomas Forrer,Barbara Russo*

Main category: cs.SE

TL;DR: 评估CodeBERT漏洞检测性能，开发集成CI/CD的推荐系统AI - DO并调查其有用性，发现不同数据训练模型的检测效果差异。


<details>
  <summary>Details</summary>
Motivation: 学术研究的深度学习漏洞检测方案难应用于工业环境，需解决技术从学术到工业转移的挑战。

Method: 评估CodeBERT性能，分析其跨域泛化能力及处理类别不平衡策略，开发AI - DO系统，通过调查评估其有用性。

Result: 工业数据训练的模型在同域检测准确，开源数据微调的模型配合欠采样技术可提升漏洞检测效果。

Conclusion: 提出有效解决方案，在不干扰工作流程下检测和定位漏洞，不同数据训练的模型有不同性能表现。

Abstract: Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.

</details>


### [118] [ORCA: Unveiling Obscure Containers In The Wild](https://arxiv.org/abs/2509.09322)
*Jacopo Bufalino,Agathe Blaise,Stefano Secci*

Main category: cs.SE

TL;DR: 现代软件开发依赖开源库和第三方组件，容器化带来安全风险，SCA 工具面临不完整容器图像问题，本文提出 ORCA 方法并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决容器化软件开发中，因容器文件系统无意修改导致不完整容器图像，影响 SCA 工具可靠性的问题。

Method: 分析 600 个流行容器，提出抗模糊的容器分析方法 ORCA 并实现开源版本。

Result: 发现知名注册表和可信镜像中存在模糊容器，很多工具无法分析；ORCA 能有效检测模糊容器内容，文件覆盖率比 Docker Scout 和 Syft 中位数提高 40%。

Conclusion: ORCA 可有效解决现有 SCA 工具在处理模糊容器时的局限，提升容器分析效果。

Abstract: Modern software development increasingly depends on open-source libraries and
third-party components, which are often encapsulated into containerized
environments. While improving the development and deployment of applications,
this approach introduces security risks, particularly when outdated or
vulnerable components are inadvertently included in production environments.
Software Composition Analysis (SCA) is a critical process that helps identify
and manage packages and dependencies inside a container. However, unintentional
modifications to the container filesystem can lead to incomplete container
images, which compromise the reliability of SCA tools. In this paper, we
examine the limitations of both cloud-based and open-source SCA tools when
faced with such obscure images. An analysis of 600 popular containers revealed
that obscure containers exist in well-known registries and trusted images and
that many tools fail to analyze such containers. To mitigate these issues, we
propose an obscuration-resilient methodology for container analysis and
introduce ORCA (Obscuration-Resilient Container Analyzer), its open-source
implementation. We reported our findings to all vendors using their appropriate
channels. Our results demonstrate that ORCA effectively detects the content of
obscure containers and achieves a median 40% improvement in file coverage
compared to Docker Scout and Syft.

</details>


### [119] [LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering](https://arxiv.org/abs/2509.09614)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: 提出用于评估长上下文大语言模型的综合基准LoCoBench，有8000个评估场景、8个任务类别和17个评估指标，评估显示模型有性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有代码评估基准缺乏对长上下文能力的评估，长上下文语言模型为代码理解和软件开发评估带来新机遇，需要评估其在复杂软件开发场景中的长上下文能力。

Method: 系统生成10种编程语言的8000个评估场景，引入8个任务类别，通过5阶段管道创建场景，提出含17个指标的综合评估框架。

Result: 对现有长上下文模型的评估显示存在显著性能差距。

Conclusion: 复杂软件开发中的长上下文理解是重大未解决挑战，需更多关注。

Abstract: The emergence of long-context language models with context windows extending
to millions of tokens has created new opportunities for sophisticated code
understanding and software development evaluation. We propose LoCoBench, a
comprehensive benchmark specifically designed to evaluate long-context LLMs in
realistic, complex software development scenarios. Unlike existing code
evaluation benchmarks that focus on single-function completion or short-context
tasks, LoCoBench addresses the critical evaluation gap for long-context
capabilities that require understanding entire codebases, reasoning across
multiple files, and maintaining architectural consistency across large-scale
software systems. Our benchmark provides 8,000 evaluation scenarios
systematically generated across 10 programming languages, with context lengths
spanning 10K to 1M tokens, a 100x variation that enables precise assessment of
long-context performance degradation in realistic software development
settings. LoCoBench introduces 8 task categories that capture essential
long-context capabilities: architectural understanding, cross-file refactoring,
multi-session development, bug investigation, feature implementation, code
comprehension, integration testing, and security analysis. Through a 5-phase
pipeline, we create diverse, high-quality scenarios that challenge LLMs to
reason about complex codebases at unprecedented scale. We introduce a
comprehensive evaluation framework with 17 metrics across 4 dimensions,
including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our
evaluation of state-of-the-art long-context models reveals substantial
performance gaps, demonstrating that long-context understanding in complex
software development represents a significant unsolved challenge that demands
more attention. LoCoBench is released at:
https://github.com/SalesforceAIResearch/LoCoBench.

</details>


### [120] [I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection](https://arxiv.org/abs/2509.09630)
*Zhenguang Liu,Lixun Ma,Zhongzheng Mu,Chengkun Wei,Xiaojun Xu,Yingying Jiao,Kui Ren*

Main category: cs.SE

TL;DR: 提出SmartDetector方法计算智能合约函数相似度，在多数据集实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开源代码复用导致智能合约漏洞传播，现有检测相似智能合约函数的方法有限，传统AST方法和深度学习方法存在不足。

Method: 将智能合约函数的AST分解为小的语句树，用分类器比较语句树对计算相似度，通过余弦扩散过程搜索分类器最优超参数。

Result: 在三个大型真实数据集上实验，SmartDetector的F1分数平均提高14.01%，整体平均F1分数达95.88%。

Conclusion: SmartDetector在计算智能合约函数相似度上优于当前最先进方法。

Abstract: Widespread reuse of open-source code in smart contract development boosts
programming efficiency but significantly amplifies bug propagation across
contracts, while dedicated methods for detecting similar smart contract
functions remain very limited. Conventional abstract-syntax-tree (AST) based
methods for smart contract similarity detection face challenges in handling
intricate tree structures, which impedes detailed semantic comparison of code.
Recent deep-learning based approaches tend to overlook code syntax and
detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for
computing similarity between smart contract functions, explainable at the
fine-grained statement level. Technically, SmartDetector decomposes the AST of
a smart contract function into a series of smaller statement trees, each
reflecting a structural element of the source code. Then, SmartDetector uses a
classifier to compute the similarity score of two functions by comparing each
pair of their statement trees. To address the infinite hyperparameter space of
the classifier, we mathematically derive a cosine-wise diffusion process to
efficiently search optimal hyperparameters. Extensive experiments conducted on
three large real-world datasets demonstrate that SmartDetector outperforms
current state-of-the-art methods by an average improvement of 14.01% in
F1-score, achieving an overall average F1-score of 95.88%.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [121] [Causal PDE-Control Models: A Structural Framework for Dynamic Portfolio Optimization](https://arxiv.org/abs/2509.09585)
*Alejandro Rodriguez Dominguez*

Main category: q-fin.PM

TL;DR: 本文介绍Causal PDE - Control Models (CPCMs)用于动态投资组合优化，有理论进展，实证表现优于基准方法，为非平稳条件下资产配置提供基础。


<details>
  <summary>Details</summary>
Motivation: 经典投资组合模型在结构突变下崩溃，现代机器学习分配器缺乏透明度和可解释性，需要新方法。

Method: 引入CPCMs框架，整合因果推断、非线性滤波和前后向偏微分方程；用物理信息神经网络实现CPCM求解器。

Result: CPCM求解器在全球股票面板数据上，比计量和机器学习基准方法实现了更高夏普比率、更低换手率和更持久溢价。

Conclusion: CPCMs围绕结构因果关系和PDE控制重新构建投资组合优化，为非平稳条件下稳健资产配置提供严谨、可解释和计算可行的基础。

Abstract: Classical portfolio models collapse under structural breaks, while modern
machine-learning allocators adapt flexibly but often at the cost of
transparency and interpretability. This paper introduces Causal PDE-Control
Models (CPCMs), a unifying framework that integrates causal inference,
nonlinear filtering, and forward-backward partial differential equations for
dynamic portfolio optimization. The framework delivers three theoretical
advances: (i) the existence of conditional risk-neutral measures under evolving
information sets; (ii) a projection-divergence duality that quantifies the
stability cost of departing from the causal driver manifold; and (iii) causal
completeness, establishing that a finite driver span can capture all systematic
premia. Classical methods such as Markowitz, CAPM, and Black-Litterman appear
as degenerate cases, while reinforcement learning and deep-hedging policies
emerge as unconstrained, symmetry-breaking approximations. Empirically, CPCM
solvers implemented with physics-informed neural networks achieve higher Sharpe
ratios, lower turnover, and more persistent premia than both econometric and
machine-learning benchmarks, using a global equity panel with more than 300
candidate drivers. By reframing portfolio optimization around structural
causality and PDE control, CPCMs provide a rigorous, interpretable, and
computationally tractable foundation for robust asset allocation under
nonstationary conditions.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [122] [Note on pre-taxation reported data by UK FTSE-listed companies. A search for Benford's laws compatibility](https://arxiv.org/abs/2509.09415)
*Marcel Ausloos,Probowo Erawan Sastroredjo,Polina Khrennikova*

Main category: q-fin.ST

TL;DR: 本文利用FTSE全股指数567家公司14年数据，测试其是否符合本福特定律，发现两种测试结果不一致，对财务数据真实性存疑。


<details>
  <summary>Details</summary>
Motivation: 确保公共收入征收公平，降低避税风险，这是英国政府关注的问题。

Method: 收集567家公司14年的税前收入和总资产数据，计算PI/TA比率，区分PI正负情况，用卡方检验和平均绝对偏差（MAD）测试数据与本福特定律的符合性。

Result: 两种测试结果不完全一致，MAD测试完全否定了财务数据符合本福特定律。

Conclusion: 研究结果对财务数据真实性存疑，建议开展更多相关调查，也为间接变量是否应符合本福特定律的讨论增添了内容。

Abstract: Pre-taxation analysis plays a crucial role in ensuring the fairness of public
revenue collection. It can also serve as a tool to reduce the risk of tax
avoidance, one of the UK government's concerns. Our report utilises pre-tax
income ($PI$) and total assets ($TA$) data from 567 companies listed on the
FTSE All-Share index, gathered from the Refinitiv EIKON database, covering 14
years, i.e., the period from 2009 to 2022. We also derive the $PI/TA$ ratio,
and distinguish between positive and negative $PI$ cases. We test the
conformity of such data to Benford's Laws,- specifically studying the first
significant digit ($Fd$), the second significant digit ($Sd$), and the first
and second significant digits ($FSd$). We use and justify two pertinent tests,
the $\chi^2$ and the Mean Absolute Deviation (MAD). We find that both tests are
not leading to conclusions in complete agreement with each other, - in
particular the MAD test entirely rejects the Benford's Laws conformity of the
reported financial data. From the mere accounting point of view, we conclude
that the findings not only cast some doubt on the reported financial data, but
also suggest that many more investigations be envisaged on closely related
matters. On the other hand, the study of a ratio, like $PI/TA$, of variables
which are (or not) Benford's Laws compliant add to the literature debating
whether such indirect variables should (or not) be Benford's Laws compliant.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [123] [Scalable extensions to given-data Sobol' index estimators](https://arxiv.org/abs/2509.09078)
*Teresa Portone,Bert Debusschere,Samantha Yang,Emiliano Islas-Quinones,T. Patrick Xiao*

Main category: stat.ML

TL;DR: 本文提出对现有给定数据Sobol'指数方法的实用扩展，可对大规模模型进行基于方差的敏感性分析，展示了新方法优势并在神经网络建模应用中验证。


<details>
  <summary>Details</summary>
Motivation: 现有给定数据方法在处理输入极多的模型时有局限性，无法有效用于大规模模型如神经网络。

Method: 提出任意分区的给定数据Sobol'指数估计器的通用定义、批量处理输入输出样本的流式算法，以及过滤小指数的启发式方法。

Result: 证明现有方法的等概率分区会引入显著偏差，流式算法能以更低内存实现相近的准确性和运行时间。

Conclusion: 所提扩展能有效对大规模模型进行基于方差的敏感性分析，优于现有方法。

Abstract: Given-data methods for variance-based sensitivity analysis have significantly
advanced the feasibility of Sobol' index computation for computationally
expensive models and models with many inputs. However, the limitations of
existing methods still preclude their application to models with an extremely
large number of inputs. In this work, we present practical extensions to the
existing given-data Sobol' index method, which allow variance-based sensitivity
analysis to be efficiently performed on large models such as neural networks,
which have $>10^4$ parameterizable inputs. For models of this size, holding all
input-output evaluations simultaneously in memory -- as required by existing
methods -- can quickly become impractical. These extensions also support
nonstandard input distributions with many repeated values, which are not
amenable to equiprobable partitions employed by existing given-data methods.
  Our extensions include a general definition of the given-data Sobol' index
estimator with arbitrary partition, a streaming algorithm to process
input-output samples in batches, and a heuristic to filter out small indices
that are indistinguishable from zero indices due to statistical noise. We show
that the equiprobable partition employed in existing given-data methods can
introduce significant bias into Sobol' index estimates even at large sample
sizes and provide numerical analyses that demonstrate why this can occur. We
also show that our streaming algorithm can achieve comparable accuracy and
runtimes with lower memory requirements, relative to current methods which
process all samples at once. We demonstrate our novel developments on two
application problems in neural network modeling.

</details>


### [124] [Global Optimization of Stochastic Black-Box Functions with Arbitrary Noise Distributions using Wilson Score Kernel Density Estimation](https://arxiv.org/abs/2509.09238)
*Thorbjørn Mosekjær Iversen,Lars Carøe Sørensen,Simon Faarvang Mathiesen,Henrik Gordon Petersen*

Main category: stat.ML

TL;DR: 论文指出贝叶斯优化依赖函数估计器提供信息性置信区间，现有估计器有局限，证明WS - KDE的置信区间适用于输出在[0,1]的随机函数，可用于更广泛成本函数的稳定全局优化，并通过仿真和应用展示其特性。


<details>
  <summary>Details</summary>
Motivation: 现有优化问题涉及时间成本高的黑盒随机函数，且现有函数估计器在贝叶斯优化中存在需大量函数评估或依赖干扰建模的局限，需要更好的方法。

Method: 研究Wilson Score Kernel Density Estimator (WS - KDE)的置信区间，将其应用于贝叶斯优化，通过仿真和应用于振动零件进料器的自动陷阱设计问题进行验证。

Result: 证明WS - KDE的置信区间适用于输出在[0,1]的任何随机函数，可用于更广泛成本函数的稳定全局优化。

Conclusion: WS - KDE可用于贝叶斯优化中更广泛成本函数的稳定全局优化，其置信区间具有良好适用性。

Abstract: Many optimization problems in robotics involve the optimization of
time-expensive black-box functions, such as those involving complex simulations
or evaluation of real-world experiments. Furthermore, these functions are often
stochastic as repeated experiments are subject to unmeasurable disturbances.
Bayesian optimization can be used to optimize such methods in an efficient
manner by deploying a probabilistic function estimator to estimate with a given
confidence so that regions of the search space can be pruned away.
Consequently, the success of the Bayesian optimization depends on the function
estimator's ability to provide informative confidence bounds. Existing function
estimators require many function evaluations to infer the underlying confidence
or depend on modeling of the disturbances. In this paper, it is shown that the
confidence bounds provided by the Wilson Score Kernel Density Estimator
(WS-KDE) are applicable as excellent bounds to any stochastic function with an
output confined to the closed interval [0;1] regardless of the distribution of
the output. This finding opens up the use of WS-KDE for stable global
optimization on a wider range of cost functions. The properties of WS-KDE in
the context of Bayesian optimization are demonstrated in simulation and applied
to the problem of automated trap design for vibrational part feeders.

</details>


### [125] [Low-degree lower bounds via almost orthonormal bases](https://arxiv.org/abs/2509.09353)
*Alexandra Carpentier,Simone Maria Giancola,Christophe Giraud,Nicolas Verzelen*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Low-degree polynomials have emerged as a powerful paradigm for providing
evidence of statistical-computational gaps across a variety of high-dimensional
statistical models [Wein25]. For detection problems -- where the goal is to
test a planted distribution $\mathbb{P}'$ against a null distribution
$\mathbb{P}$ with independent components -- the standard approach is to bound
the advantage using an $\mathbb{L}^2(\mathbb{P})$-orthonormal family of
polynomials. However, this method breaks down for estimation tasks or more
complex testing problems where $\mathbb{P}$ has some planted structures, so
that no simple $\mathbb{L}^2(\mathbb{P})$-orthogonal polynomial family is
available. To address this challenge, several technical workarounds have been
proposed [SW22,SW25], though their implementation can be delicate. In this
work, we propose a more direct proof strategy. Focusing on random graph models,
we construct a basis of polynomials that is almost orthonormal under
$\mathbb{P}$, in precisely those regimes where statistical-computational gaps
arise. This almost orthonormal basis not only yields a direct route to
establishing low-degree lower bounds, but also allows us to explicitly identify
the polynomials that optimize the low-degree criterion. This, in turn, provides
insights into the design of optimal polynomial-time algorithms. We illustrate
the effectiveness of our approach by recovering known low-degree lower bounds,
and establishing new ones for problems such as hidden subcliques, stochastic
block models, and seriation models.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [126] [Divide, Interact, Sample: The Two-System Paradigm](https://arxiv.org/abs/2509.09162)
*James Chok,Myung Won Lee,Daniel Paulin,Geoffrey M. Vasil*

Main category: stat.CO

TL;DR: 提出统一的双系统框架整合三种蒙特卡罗采样方法，推导新采样器，在基准测试和实际任务中表现优于流行采样器。


<details>
  <summary>Details</summary>
Motivation: 将历史上被视为不同方法的平均场、系综链和自适应采样器统一起来。

Method: 构建双系统框架，将粒子系综分为两个相互作用子系统，对称交替更新。

Result: 揭示系综链采样器与平均场采样器关系，可将平均场朗之万动力学离散化，与自适应单链方法建立联系，新采样器性能优于No - U - Turn Sampler。

Conclusion: 双系统框架有效统一三种采样方法，新采样器性能显著提升。

Abstract: Mean-field, ensemble-chain, and adaptive samplers have historically been
viewed as distinct approaches to Monte Carlo sampling. In this paper, we
present a unifying {two-system} framework that brings all three under one roof.
In our approach, an ensemble of particles is split into two interacting
subsystems that propose updates for each other in a symmetric, alternating
fashion. This cross-system interaction ensures that the overall ensemble has
$\rho(x)$ as its invariant distribution in both the finite-particle setting and
the mean-field limit. The two-system construction reveals that ensemble-chain
samplers can be interpreted as finite-$N$ approximations of an ideal mean-field
sampler; conversely, it provides a principled recipe to discretize mean-field
Langevin dynamics into tractable parallel MCMC algorithms. The framework also
connects naturally to adaptive single-chain methods: by replacing
particle-based statistics with time-averaged statistics from a single chain,
one recovers analogous adaptive dynamics in the long-time limit without
requiring a large ensemble. We derive novel two-system versions of both
overdamped and underdamped Langevin MCMC samplers within this paradigm. Across
synthetic benchmarks and real-world posterior inference tasks, these two-system
samplers exhibit significant performance gains over the popular No-U-Turn
Sampler, achieving an order of magnitude higher effective sample sizes per
gradient evaluation.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [127] [Personalized Sleep Prediction via Deep Adaptive Spatiotemporal Modeling and Sparse Data](https://arxiv.org/abs/2509.09018)
*Xueyi Wang,C. J. C.,Lamoth,Elisabeth Wilhelm*

Main category: eess.SP

TL;DR: 提出AdaST - Sleep模型预测睡眠分数，结合卷积和循环层，跨对象泛化，实验表现优于基线模型，适用于个性化睡眠预测。


<details>
  <summary>Details</summary>
Motivation: 睡眠预测可帮助个人和医疗人员提前应对影响睡眠的因素，改善身心健康。

Method: 提出AdaST - Sleep模型，结合卷积层捕捉空间特征交互，循环层处理长期时间数据，集成域分类器实现跨对象泛化，用不同输入和预测窗口进行实验。

Result: 该方法始终优于四个基线模型，7天输入窗口和1天预测窗口时RMSE最低为0.282，多日预测表现也强，能准确跟踪整体睡眠分数和每日波动。

Conclusion: 提出的框架为利用商业可穿戴设备的稀疏数据和域适应技术进行个性化睡眠预测提供了强大且自适应的解决方案。

Abstract: A sleep forecast allows individuals and healthcare providers to anticipate
and proactively address factors influencing restful rest, ultimately improving
mental and physical well-being. This work presents an adaptive spatial and
temporal model (AdaST-Sleep) for predicting sleep scores. Our proposed model
combines convolutional layers to capture spatial feature interactions between
multiple features and recurrent neural network layers to handle longer-term
temporal health-related data. A domain classifier is further integrated to
generalize across different subjects. We conducted several experiments using
five input window sizes (3, 5, 7, 9, 11 days) and five predicting window sizes
(1, 3, 5, 7, 9 days). Our approach consistently outperformed four baseline
models, achieving its lowest RMSE (0.282) with a seven-day input window and a
one-day predicting window. Moreover, the method maintained strong performance
even when forecasting multiple days into the future, demonstrating its
versatility for real-world applications. Visual comparisons reveal that the
model accurately tracks both the overall sleep score level and daily
fluctuations. These findings prove that the proposed framework provides a
robust and adaptable solution for personalized sleep forecasting using sparse
data from commercial wearable devices and domain adaptation techniques.

</details>


### [128] [A Masked Representation Learning to Model Cardiac Functions Using Multiple Physiological Signals](https://arxiv.org/abs/2509.08830)
*Seong-A Park,Jong-Eui Chae,Sungdong Kim,Hyung-Chul Lee,Hyun-Lim Yang*

Main category: eess.SP

TL;DR: 本文介绍了SNUPHY - M模型，基于自监督学习恢复三种生理信号来提取特征，在临床下游任务表现出色，支持临床决策。


<details>
  <summary>Details</summary>
Motivation: 临床中监测血流动力学需综合分析多生理信号，现有研究缺乏适用于实际临床场景的复杂信号分析方法。

Method: 引入SNUPHY - M模型，基于自监督学习恢复ECG、PPG和ABP三种掩蔽生理信号，提取反映心脏周期电、压力和流体特征的生理特征。

Result: SNUPHY - M在临床下游任务中显著优于有监督或自监督学习模型，尤其在使用非侵入性信号的预测任务中。

Conclusion: SNUPHY - M是首个将多模态自监督学习应用于涉及ECG、PPG和ABP信号的心血管分析的模型，能有效支持临床决策，无创助力血流动力学早期诊断和管理。

Abstract: In clinical settings, monitoring hemodynamics is crucial for managing patient
prognosis, necessitating the integrated analysis of multiple physiological
signals. While recent research has analyzed single signals such as
electrocardiography (ECG) or photoplethysmography (PPG), there has yet to be a
proposal for an approach that encompasses the complex signal analysis required
in actual clinical scenarios. In this study, we introduce the SNUPHY-M (Seoul
National University hospital PHYsiological signal Masked representation
learning) model extracts physiological features reflecting the electrical,
pressure, and fluid characteristics of the cardiac cycle in the process of
restoring three masked physiological signals based on self-supervised learning
(SSL): ECG, PPG, and arterial blood pressure (ABP) signals. By employing
multiple physical characteristics, the model can extract more enriched features
only using non-invasive signals. We evaluated the model's performance in
clinical downstream tasks such as hypotension, stroke volume, systolic blood
pressure, diastolic blood pressure, and age prediction. Our results showed that
the SNUPHY-M significantly outperformed supervised or SSL models, especially in
prediction tasks using non-invasive signals. To the best of our knowledge,
SNUPHY-M is the first model to apply multi-modal SSL to cardiovascular analysis
involving ECG, PPG, and ABP signals. This approach effectively supports
clinical decision-making and enables precise diagnostics, contributing
significantly to the early diagnosis and management of hemodynamics without
invasiveness.

</details>


### [129] [Deploying AI for Signal Processing education: Selected challenges and intriguing opportunities](https://arxiv.org/abs/2509.08950)
*Jarvis Haupt,Qin Lu,Yanning Shen,Jia Chen,Yue Dong,Dan McCreary,Mehmet Akçakaya,Georgios B. Giannakis*

Main category: eess.SP

TL;DR: 文章探讨AI在信号处理教育中的应用，提出解决技术局限和实践应用两方面视角，还给出核心技术问题指引，为工程教育研究者和教育工作者提供资源。


<details>
  <summary>Details</summary>
Motivation: 应对AI公平和负责任使用的挑战，探索AI在改善全球人类状况方面的应用，聚焦于信号处理教育领域。

Method: 从识别和解决技术局限、在实践中应用AI工具两方面进行探索，通过开发“智能教科书”说明相关考量。

Result: 提供了在教育场景使用AI时若干核心技术问题的指引。

Conclusion: 文章可作为研究者和教育工作者推动AI在工程教育中发挥作用的资源。

Abstract: Powerful artificial intelligence (AI) tools that have emerged in recent years
-- including large language models, automated coding assistants, and advanced
image and speech generation technologies -- are the result of monumental human
achievements. These breakthroughs reflect mastery across multiple technical
disciplines and the resolution of significant technological challenges.
However, some of the most profound challenges may still lie ahead. These
challenges are not purely technical but pertain to the fair and responsible use
of AI in ways that genuinely improve the global human condition. This article
explores one promising application aligned with that vision: the use of AI
tools to facilitate and enhance education, with a specific focus on signal
processing (SP). It presents two interrelated perspectives: identifying and
addressing technical limitations, and applying AI tools in practice to improve
educational experiences. Primers are provided on several core technical issues
that arise when using AI in educational settings, including how to ensure
fairness and inclusivity, handle hallucinated outputs, and achieve efficient
use of resources. These and other considerations -- such as transparency,
explainability, and trustworthiness -- are illustrated through the development
of an immersive, structured, and reliable "smart textbook." The article serves
as a resource for researchers and educators seeking to advance AI's role in
engineering education.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [130] [Towards Confidential and Efficient LLM Inference with Dual Privacy Protection](https://arxiv.org/abs/2509.09091)
*Honglan Yu,Yibin Wang,Feifei Dai,Dong Liu,Haihui Fan,Xiaoyan Gu*

Main category: cs.CR

TL;DR: 本文提出CMIF框架，通过将嵌入层部署在客户端TEE，后续层部署在GPU服务器，并优化机制，减少TEE推理开销并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 现有基于CPU的TEE和DP方法在隐私推理中存在高推理延迟、通信开销大、影响模型性能等问题。

Method: 提出CMIF框架，将嵌入层部署在客户端TEE，后续层部署在GPU服务器，优化Report - Noisy - Max机制。

Result: 在Llama系列模型上的大量实验表明，CMIF减少了TEE中的额外推理开销。

Conclusion: CMIF能在减少TEE推理开销的同时保护用户数据隐私。

Abstract: CPU-based trusted execution environments (TEEs) and differential privacy (DP)
have gained wide applications for private inference. Due to high inference
latency in TEEs, researchers use partition-based approaches that offload linear
model components to GPUs. However, dense nonlinear layers of large language
models (LLMs) result in significant communication overhead between TEEs and
GPUs. DP-based approaches apply random noise to protect data privacy, but this
compromises LLM performance and semantic understanding. To overcome the above
drawbacks, this paper proposes CMIF, a Confidential and efficient Model
Inference Framework. CMIF confidentially deploys the embedding layer in the
client-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes
the Report-Noisy-Max mechanism to protect sensitive inputs with a slight
decrease in model performance. Extensive experiments on Llama-series models
demonstrate that CMIF reduces additional inference overhead in TEEs while
preserving user data privacy.

</details>


### [131] [DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models](https://arxiv.org/abs/2509.09097)
*Honghui Xu,Shiva Shrestha,Wei Chen,Zhiyuan Li,Zhipeng Cai*

Main category: cs.CR

TL;DR: 提出隐私增强的联邦微调框架DP - FedLoRA，结合LoRA和差分隐私，实验证明其性能有竞争力且有强隐私保证。


<details>
  <summary>Details</summary>
Motivation: 设备端大语言模型系统中，联邦微调处理敏感用户数据引发隐私问题，需解决。

Method: 提出DP - FedLoRA框架，客户端用高斯噪声对LoRA矩阵裁剪和扰动以满足差分隐私，还进行理论分析。

Result: 在主流基准测试中，DP - FedLoRA性能有竞争力且提供强隐私保证。

Conclusion: DP - FedLoRA为设备端环境中可扩展且保护隐私的大语言模型部署铺平道路。

Abstract: As on-device large language model (LLM) systems become increasingly
prevalent, federated fine-tuning enables advanced language understanding and
generation directly on edge devices; however, it also involves processing
sensitive, user-specific data, raising significant privacy concerns within the
federated learning framework. To address these challenges, we propose
DP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates
LoRA-based adaptation with differential privacy in a communication-efficient
setting. Each client locally clips and perturbs its LoRA matrices using
Gaussian noise to satisfy ($\epsilon$, $\delta$)-differential privacy. We
further provide a theoretical analysis demonstrating the unbiased nature of the
updates and deriving bounds on the variance introduced by noise, offering
practical guidance for privacy-budget calibration. Experimental results across
mainstream benchmarks show that DP-FedLoRA delivers competitive performance
while offering strong privacy guarantees, paving the way for scalable and
privacy-preserving LLM deployment in on-device environments.

</details>


### [132] [Character-Level Perturbations Disrupt LLM Watermarks](https://arxiv.org/abs/2509.09112)
*Zhaoxi Zhang,Xiaomei Zhang,Yanjun Zhang,He Zhang,Shirui Pan,Bo Liu,Asif Qumer Gill,Leo Yu Zhang*

Main category: cs.CR

TL;DR: 本文研究大语言模型水印去除攻击，发现字符级扰动在水印去除上更有效，提出基于遗传算法的引导去除攻击和自适应复合字符级攻击，指出现有水印方案存在漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型水印去除攻击方法欠佳，存在有效去除需大扰动或强大对手的误解，需研究更有效的去除方法。

Method: 形式化LLM水印系统模型，刻画两种受限威胁模型，分析不同扰动类型的攻击范围，提出基于遗传算法的引导去除攻击和自适应复合字符级攻击。

Result: 实验证实字符级扰动在水印去除上更优，GA方法在现实约束下有效，自适应复合字符级攻击能击败防御。

Conclusion: 现有LLM水印方案存在显著漏洞，急需开发新的鲁棒机制。

Abstract: Large Language Model (LLM) watermarking embeds detectable signals into
generated text for copyright protection, misuse prevention, and content
detection. While prior studies evaluate robustness using watermark removal
attacks, these methods are often suboptimal, creating the misconception that
effective removal requires large perturbations or powerful adversaries.
  To bridge the gap, we first formalize the system model for LLM watermark, and
characterize two realistic threat models constrained on limited access to the
watermark detector. We then analyze how different types of perturbation vary in
their attack range, i.e., the number of tokens they can affect with a single
edit. We observe that character-level perturbations (e.g., typos, swaps,
deletions, homoglyphs) can influence multiple tokens simultaneously by
disrupting the tokenization process. We demonstrate that character-level
perturbations are significantly more effective for watermark removal under the
most restrictive threat model. We further propose guided removal attacks based
on the Genetic Algorithm (GA) that uses a reference detector for optimization.
Under a practical threat model with limited black-box queries to the watermark
detector, our method demonstrates strong removal performance. Experiments
confirm the superiority of character-level perturbations and the effectiveness
of the GA in removing watermarks under realistic constraints. Additionally, we
argue there is an adversarial dilemma when considering potential defenses: any
fixed defense can be bypassed by a suitable perturbation strategy. Motivated by
this principle, we propose an adaptive compound character-level attack.
Experimental results show that this approach can effectively defeat the
defenses. Our findings highlight significant vulnerabilities in existing LLM
watermark schemes and underline the urgency for the development of new robust
mechanisms.

</details>


### [133] [CryptGNN: Enabling Secure Inference for Graph Neural Networks](https://arxiv.org/abs/2509.09107)
*Pritam Sen,Yao Ma,Cristian Borcea*

Main category: cs.CR

TL;DR: 提出CryptGNN，一种用于云环境第三方图神经网络模型的安全有效推理方案，理论和实验证明其安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 为云环境中第三方图神经网络模型提供安全有效的推理方案，保护客户输入数据、图结构和模型参数。

Method: 使用分布式安全多方计算（SMPC）技术实现安全消息传递和特征转换层。

Result: 通过理论分析和实验证明了CryptGNN的安全性和效率。

Conclusion: CryptGNN是一种安全有效的云环境第三方图神经网络模型推理方案。

Abstract: We present CryptGNN, a secure and effective inference solution for
third-party graph neural network (GNN) models in the cloud, which are accessed
by clients as ML as a service (MLaaS). The main novelty of CryptGNN is its
secure message passing and feature transformation layers using distributed
secure multi-party computation (SMPC) techniques. CryptGNN protects the
client's input data and graph structure from the cloud provider and the
third-party model owner, and it protects the model parameters from the cloud
provider and the clients. CryptGNN works with any number of SMPC parties, does
not require a trusted server, and is provably secure even if P-1 out of P
parties in the cloud collude. Theoretical analysis and empirical experiments
demonstrate the security and efficiency of CryptGNN.

</details>


### [134] [ENSI: Efficient Non-Interactive Secure Inference for Large Language Models](https://arxiv.org/abs/2509.09424)
*Zhiyu He,Maojiang Wang,Xinwen Gao,Yuchuan Luo,Lin Liu,Shaojing Fu*

Main category: cs.CR

TL;DR: 提出用于大语言模型的非交互安全推理框架 ENSI，降低计算复杂度，实验显示有显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有加密协议与大语言模型集成存在挑战，其复杂性和大语言模型的大规模参数与复杂架构限制了实用性。

Method: 基于加密协议和大语言模型架构协同设计原则，采用优化编码策略结合 CKKS 方案与轻量级模型 BitNet；将 sigmoid 注意力机制与同态加密集成；在 RMSNorm 过程中嵌入 Bootstrapping 操作。

Result: 在 CPU 上矩阵乘法约加速 8 倍，softmax 推理加速 2.6 倍，Bootstrapping 比例降至 1%。

Conclusion: 所提出的 ENSI 框架有效解决了加密协议与大语言模型集成的问题，提升了安全推理的效率。

Abstract: Secure inference enables privacy-preserving machine learning by leveraging
cryptographic protocols that support computations on sensitive user data
without exposing it. However, integrating cryptographic protocols with large
language models (LLMs) presents significant challenges, as the inherent
complexity of these protocols, together with LLMs' massive parameter scale and
sophisticated architectures, severely limits practical usability. In this work,
we propose ENSI, a novel non-interactive secure inference framework for LLMs,
based on the principle of co-designing the cryptographic protocols and LLM
architecture. ENSI employs an optimized encoding strategy that seamlessly
integrates CKKS scheme with a lightweight LLM variant, BitNet, significantly
reducing the computational complexity of encrypted matrix multiplications. In
response to the prohibitive computational demands of softmax under homomorphic
encryption (HE), we pioneer the integration of the sigmoid attention mechanism
with HE as a seamless, retraining-free alternative. Furthermore, by embedding
the Bootstrapping operation within the RMSNorm process, we efficiently refresh
ciphertexts while markedly decreasing the frequency of costly bootstrapping
invocations. Experimental evaluations demonstrate that ENSI achieves
approximately an 8x acceleration in matrix multiplications and a 2.6x speedup
in softmax inference on CPU compared to state-of-the-art method, with the
proportion of bootstrapping is reduced to just 1%.

</details>


### [135] [Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts](https://arxiv.org/abs/2509.09488)
*Felix Mächtle,Ashwath Shetty,Jonas Sander,Nils Loose,Sören Pirk,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: 研究扩散模型的提示词窃取攻击，发现优化方法局限，利用噪声生成漏洞恢复种子值，提出新方法PromptPirate，给出应对措施并与开发者合作修复。


<details>
  <summary>Details</summary>
Motivation: 扩散模型文本到图像生成中提示词有重要价值，提示词窃取存在安全和隐私问题，需研究攻击和防护。

Method: 揭示数值优化提示恢复方法局限，利用噪声生成漏洞，用SeedSnitch工具恢复种子值，提出基于遗传算法的PromptPirate进行提示词窃取，给出应对措施。

Result: 约95%图像种子值可在140分钟内暴力破解，PromptPirate比现有方法LPIPS相似度提升8 - 11%。

Conclusion: 提出有效应对措施，与开发者合作解决关键漏洞。

Abstract: Diffusion models have significantly advanced text-to-image generation,
enabling the creation of highly realistic images conditioned on textual prompts
and seeds. Given the considerable intellectual and economic value embedded in
such prompts, prompt theft poses a critical security and privacy concern. In
this paper, we investigate prompt-stealing attacks targeting diffusion models.
We reveal that numerical optimization-based prompt recovery methods are
fundamentally limited as they do not account for the initial random noise used
during image generation. We identify and exploit a noise-generation
vulnerability (CWE-339), prevalent in major image-generation frameworks,
originating from PyTorch's restriction of seed values to a range of $2^{32}$
when generating the initial random noise on CPUs. Through a large-scale
empirical analysis conducted on images shared via the popular platform CivitAI,
we demonstrate that approximately 95% of these images' seed values can be
effectively brute-forced in 140 minutes per seed using our seed-recovery tool,
SeedSnitch. Leveraging the recovered seed, we propose PromptPirate, a genetic
algorithm-based optimization method explicitly designed for prompt stealing.
PromptPirate surpasses state-of-the-art methods, i.e., PromptStealer, P2HP, and
CLIP-Interrogator, achieving an 8-11% improvement in LPIPS similarity.
Furthermore, we introduce straightforward and effective countermeasures that
render seed stealing, and thus optimization-based prompt stealing, ineffective.
We have disclosed our findings responsibly and initiated coordinated mitigation
efforts with the developers to address this critical vulnerability.

</details>


### [136] [What Does Normal Even Mean? Evaluating Benign Traffic in Intrusion Detection Datasets](https://arxiv.org/abs/2509.09564)
*Meghan Wilkinson,Robert H Thomson*

Main category: cs.CR

TL;DR: 本文评估常见入侵检测数据集中良性流量结构，用无监督聚类技术聚类良性流量，看是否有有意义子类别以提升多分类性能。


<details>
  <summary>Details</summary>
Motivation: 监督机器学习依赖有意义标签数据，多数入侵检测数据集良性类单一，多数研究直接用标签类别训练，本文想确定良性流量中是否有有意义子类别来提升多分类性能。

Method: 评估NSL - KDD、UNSW - NB15和CIC - IDS 2017等数据集良性流量结构，使用HDBSCAN、Mean Shift Clustering等无监督聚类技术对良性流量空间进行聚类。

Result: 文档未提及具体结果。

Conclusion: 文档未提及具体结论。

Abstract: Supervised machine learning techniques rely on labeled data to achieve high
task performance, but this requires the labels to capture some meaningful
differences in the underlying data structure. For training network intrusion
detection algorithms, most datasets contain a series of attack classes and a
single large benign class which captures all non-attack network traffic. A
review of intrusion detection papers and guides that explicitly state their
data preprocessing steps identified that the majority took the labeled
categories of the dataset at face value when training their algorithms. The
present paper evaluates the structure of benign traffic in several common
intrusion detection datasets (NSL-KDD, UNSW-NB15, and CIC-IDS 2017) and
determines whether there are meaningful sub-categories within this traffic
which may improve overall multi-classification performance using common machine
learning techniques. We present an overview of some unsupervised clustering
techniques (e.g., HDBSCAN, Mean Shift Clustering) and show how they
differentially cluster the benign traffic space.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [137] [Representation-Aware Distributionally Robust Optimization: A Knowledge Transfer Framework](https://arxiv.org/abs/2509.09371)
*Zitao Wang,Nian Si,Molei Liu*

Main category: stat.ME

TL;DR: 提出READ框架用于Wasserstein分布鲁棒学习，有理论基础、半径选择方法和优化算法，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 经典方法平等对待特征扰动，缺乏对预测表示的考虑，需一种能在防范分布偏移时考虑预测表示的方法。

Method: 将多维对齐参数嵌入传输成本；用鲁棒Wasserstein轮廓推断技术选Wasserstein半径；分析READ估计器几何性质并提出优化算法。

Result: 为线性回归和二元分类的半范数正则化提供理论基础；能构建有效、有特定几何特征的置信区域；能在同等鲁棒估计器中选择并构建表示结构。

Conclusion: 通过大量模拟和实际研究证明了READ框架的有效性，提供了基于学习表示的强大鲁棒估计方法。

Abstract: We propose REpresentation-Aware Distributionally Robust Estimation (READ), a
novel framework for Wasserstein distributionally robust learning that accounts
for predictive representations when guarding against distributional shifts.
Unlike classical approaches that treat all feature perturbations equally, READ
embeds a multidimensional alignment parameter into the transport cost, allowing
the model to differentially discourage perturbations along directions
associated with informative representations. This yields robustness to feature
variation while preserving invariant structure. Our first contribution is a
theoretical foundation: we show that seminorm regularizations for linear
regression and binary classification arise as Wasserstein distributionally
robust objectives, thereby providing tractable reformulations of READ and
unifying a broad class of regularized estimators under the DRO lens. Second, we
adopt a principled procedure for selecting the Wasserstein radius using the
techniques of robust Wasserstein profile inference. This further enables the
construction of valid, representation-aware confidence regions for model
parameters with distinct geometric features. Finally, we analyze the geometry
of READ estimators as the alignment parameters vary and propose an optimization
algorithm to estimate the projection of the global optimum onto this solution
surface. This procedure selects among equally robust estimators while optimally
constructing a representation structure. We conclude by demonstrating the
effectiveness of our framework through extensive simulations and a real-world
study, providing a powerful robust estimation grounded in learning
representation.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [138] [PerFairX: Is There a Balance Between Fairness and Personality in Large Language Model Recommendations?](https://arxiv.org/abs/2509.08829)
*Chandan Kumar Sah*

Main category: cs.CY

TL;DR: 提出PerFairX框架评估大语言模型推荐系统中个性化与公平性的权衡，实验表明个性感知提示可提升契合度但加剧公平差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型融入推荐系统时，通过OCEAN模型引入用户个性特征会出现心理契合与人口公平之间的矛盾，需评估权衡。

Method: 提出PerFairX统一评估框架，使用中性和个性敏感提示，在电影和音乐数据集上对ChatGPT和DeepSeek进行基准测试。

Result: 个性感知提示显著提升与个人特征的契合度，但会加剧不同人口群体的公平差异，DeepSeek心理契合度强但对提示变化更敏感，ChatGPT输出稳定但个性化不足。

Conclusion: PerFairX为开发公平且考虑心理因素的大语言模型推荐系统提供原则性基准，有助于创建持续学习环境下的包容性、以用户为中心的AI应用。

Abstract: The integration of Large Language Models (LLMs) into recommender systems has
enabled zero-shot, personality-based personalization through prompt-based
interactions, offering a new paradigm for user-centric recommendations.
However, incorporating user personality traits via the OCEAN model highlights a
critical tension between achieving psychological alignment and ensuring
demographic fairness. To address this, we propose PerFairX, a unified
evaluation framework designed to quantify the trade-offs between
personalization and demographic equity in LLM-generated recommendations. Using
neutral and personality-sensitive prompts across diverse user profiles, we
benchmark two state-of-the-art LLMs, ChatGPT and DeepSeek, on movie (MovieLens
10M) and music (Last.fm 360K) datasets. Our results reveal that
personality-aware prompting significantly improves alignment with individual
traits but can exacerbate fairness disparities across demographic groups.
Specifically, DeepSeek achieves stronger psychological fit but exhibits higher
sensitivity to prompt variations, while ChatGPT delivers stable yet less
personalized outputs. PerFairX provides a principled benchmark to guide the
development of LLM-based recommender systems that are both equitable and
psychologically informed, contributing to the creation of inclusive,
user-centric AI applications in continual learning contexts.

</details>


### [139] [Deep opacity and AI: A threat to XAI and to privacy protection mechanisms](https://arxiv.org/abs/2509.08835)
*Vincent C. Müller*

Main category: cs.CY

TL;DR: 文章指出大数据分析和AI因‘黑盒问题’威胁隐私，区分三种不透明性，认为相关主体难作保护隐私判断，使隐私问题恶化，最后给出技术应对展望。


<details>
  <summary>Details</summary>
Motivation: 探讨大数据分析和AI中‘黑盒问题’对隐私保护判断和行动正当性的影响。

Method: 解释‘黑盒问题’在隐私判断和行动正当性背景下的问题，区分三种不透明性。

Result: 大数据分析和AI中的主体因不透明性难以做出保护隐私所需的判断。

Conclusion: 大数据分析使隐私问题更严重，补救措施效果降低，最后给出技术应对展望。

Abstract: It is known that big data analytics and AI pose a threat to privacy, and that
some of this is due to some kind of "black box problem" in AI. I explain how
this becomes a problem in the context of justification for judgments and
actions. Furthermore, I suggest distinguishing three kinds of opacity: 1) the
subjects do not know what the system does ("shallow opacity"), 2) the analysts
do not know what the system does ("standard black box opacity"), or 3) the
analysts cannot possibly know what the system might do ("deep opacity"). If the
agents, data subjects as well as analytics experts, operate under opacity, then
these agents cannot provide justifications for judgments that are necessary to
protect privacy, e.g., they cannot give "informed consent", or guarantee
"anonymity". It follows from these points that agents in big data analytics and
AI often cannot make the judgments needed to protect privacy. So I conclude
that big data analytics makes the privacy problems worse and the remedies less
effective. As a positive note, I provide a brief outlook on technical ways to
handle this situation.

</details>


### [140] [Safe and Certifiable AI Systems: Concepts, Challenges, and Lessons Learned](https://arxiv.org/abs/2509.08852)
*Kajetan Schweighofer,Barbara Brune,Lukas Gruber,Simon Schmid,Alexander Aufreiter,Andreas Gruber,Thomas Doms,Sebastian Eder,Florian Mayer,Xaver-Paul Stadlbauer,Christoph Schwald,Werner Zellinger,Bernhard Nessler,Sepp Hochreiter*

Main category: cs.CY

TL;DR: 本文介绍TÜV AUSTRIA可信AI框架，包括审计目录和方法，将欧盟AI法案义务转化为可测试标准，还分享实践经验并讨论认证关键方面。


<details>
  <summary>Details</summary>
Motivation: 人工智能在关键安全应用中日益普及，但缺乏实用的认证方案来确保AI系统安全、合法且被社会接受。

Method: 开发基于安全软件开发、功能需求、伦理与数据隐私三个支柱的审计目录，将欧盟AI法案义务转化为具体测试标准；采用功能可信度核心概念，结合应用领域与性能要求及统计测试。

Result: 提供功能需求评估概述，分享实践应用审计目录的经验，指出常见陷阱；讨论AI系统认证的关键方面。

Conclusion: 该方法将技术最佳实践与欧洲标准结合，为监管者、提供者和用户提供了合规、可信且可认证的AI系统实用路线图。

Abstract: There is an increasing adoption of artificial intelligence in safety-critical
applications, yet practical schemes for certifying that AI systems are safe,
lawful and socially acceptable remain scarce. This white paper presents the
T\"UV AUSTRIA Trusted AI framework an end-to-end audit catalog and methodology
for assessing and certifying machine learning systems. The audit catalog has
been in continuous development since 2019 in an ongoing collaboration with
scientific partners. Building on three pillars - Secure Software Development,
Functional Requirements, and Ethics & Data Privacy - the catalog translates the
high-level obligations of the EU AI Act into specific, testable criteria. Its
core concept of functional trustworthiness couples a statistically defined
application domain with risk-based minimum performance requirements and
statistical testing on independently sampled data, providing transparent and
reproducible evidence of model quality in real-world settings. We provide an
overview of the functional requirements that we assess, which are oriented on
the lifecycle of an AI system. In addition, we share some lessons learned from
the practical application of the audit catalog, highlighting common pitfalls we
encountered, such as data leakage scenarios, inadequate domain definitions,
neglect of biases, or a lack of distribution drift controls. We further discuss
key aspects of certifying AI systems, such as robustness, algorithmic fairness,
or post-certification requirements, outlining both our current conclusions and
a roadmap for future research. In general, by aligning technical best practices
with emerging European standards, the approach offers regulators, providers,
and users a practical roadmap for legally compliant, functionally trustworthy,
and certifiable AI systems.

</details>


### [141] [A vibe coding learning design to enhance EFL students' talking to, through, and about AI](https://arxiv.org/abs/2509.08854)
*David James Woo,Kai Guo,Yangyang Yu*

Main category: cs.CY

TL;DR: 本文报道了在EFL教育中试用vibe coding的情况，开发框架并开展工作坊，通过案例研究分析结果，指出有效教学需元语言支持。


<details>
  <summary>Details</summary>
Motivation: 探索vibe coding在EFL教育中的应用。

Method: 开发人类 - AI元语言框架，采用逆向设计原则开展四小时工作坊，运用案例研究方法，收集多种数据。

Result: 两名学生在设计应用时结果不同，一人成功，一人遇技术难题，分析显示学生提示工程方法有差异。

Conclusion: AI是有益的语言机器，学生与AI交互方式的不同导致结果差异，有效教学需明确的元语言支架等。

Abstract: This innovative practice article reports on the piloting of vibe coding
(using natural language to create software applications with AI) for English as
a Foreign Language (EFL) education. We developed a human-AI meta-languaging
framework with three dimensions: talking to AI (prompt engineering), talking
through AI (negotiating authorship), and talking about AI (mental models of
AI). Using backward design principles, we created a four-hour workshop where
two students designed applications addressing authentic EFL writing challenges.
We adopted a case study methodology, collecting data from worksheets and video
recordings, think-aloud protocols, screen recordings, and AI-generated images.
Contrasting cases showed one student successfully vibe coding a functional
application cohering to her intended design, while another encountered
technical difficulties with major gaps between intended design and actual
functionality. Analysis reveals differences in students' prompt engineering
approaches, suggesting different AI mental models and tensions in attributing
authorship. We argue that AI functions as a beneficial languaging machine, and
that differences in how students talk to, through, and about AI explain vibe
coding outcome variations. Findings indicate that effective vibe coding
instruction requires explicit meta-languaging scaffolding, teaching structured
prompt engineering, facilitating critical authorship discussions, and
developing vocabulary for articulating AI mental models.

</details>


### [142] [Investigating Student Interaction Patterns with Large Language Model-Powered Course Assistants in Computer Science Courses](https://arxiv.org/abs/2509.08862)
*Chang Liu,Loc Hoang,Andrew Stolman,Rene F. Kizilcec,Bo Wu*

Main category: cs.CY

TL;DR: 本文研究了跨多门计算机科学课程部署的大语言模型课程助手，发现其能解决时间支持缺口和新手学习需求，同时指出当前大语言模型在生成高阶认知问题上能力有限，为教育系统和教师参与提供了方向。


<details>
  <summary>Details</summary>
Motivation: 多数高校难以为学生提供灵活及时的学术支持，大语言模型有潜力弥补这一差距，但学生与大语言模型的交互缺乏教师监督，因此研究大语言模型课程助手的实际应用和教学意义。

Method: 开发并部署大语言模型课程助手，分析交互数据，对每个课程抽样200个对话进行手动标注，进行布鲁姆分类法分析。

Result: 系统使用在晚上和夜间较强，入门课程使用更高；多数抽样回复正确且有帮助，少数无帮助或错误，很少有专用示例；约11%的抽样对话包含大语言模型生成的后续问题，高级课程学生常忽略；大语言模型生成高阶认知问题能力有限。

Conclusion: 当前状况为以教学为导向的大语言模型教育系统提供了机会，也表明教师需更多参与配置提示、内容和政策。

Abstract: Providing students with flexible and timely academic support is a challenge
at most colleges and universities, leaving many students without help outside
scheduled hours. Large language models (LLMs) are promising for bridging this
gap, but interactions between students and LLMs are rarely overseen by
educators. We developed and studied an LLM-powered course assistant deployed
across multiple computer science courses to characterize real-world use and
understand pedagogical implications. By Spring 2024, our system had been
deployed to approximately 2,000 students across six courses at three
institutions. Analysis of the interaction data shows that usage remains strong
in the evenings and nights and is higher in introductory courses, indicating
that our system helps address temporal support gaps and novice learner needs.
We sampled 200 conversations per course for manual annotation: most sampled
responses were judged correct and helpful, with a small share unhelpful or
erroneous; few responses included dedicated examples. We also examined an
inquiry-based learning strategy: only around 11% of sampled conversations
contained LLM-generated follow-up questions, which were often ignored by
students in advanced courses. A Bloom's taxonomy analysis reveals that current
LLM capabilities are limited in generating higher-order cognitive questions.
These patterns suggest opportunities for pedagogically oriented LLM-based
educational systems and greater educator involvement in configuring prompts,
content, and policies.

</details>


### [143] [Decentralising LLM Alignment: A Case for Context, Pluralism, and Participation](https://arxiv.org/abs/2509.08858)
*Oriane Peter,Kate Devlin*

Main category: cs.CY

TL;DR: 当前大语言模型对齐方法存在问题，本文提出通过情境、多元性和参与性进行去中心化对齐，并阐述其作用、举例及不同使用情境要求。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对齐技术主要反映少数参考群体偏好，将其价值观强加给广大用户，且使知识生产和治理控制权集中在有影响力的机构。

Method: 借鉴权力/知识关系理论，提出通过情境、多元性和参与性进行去中心化对齐，并结合具体用例说明。

Result: 突出了情境、多元性和参与性在去中心化对齐中的作用，提供具体策略示例，展示不同使用情境下对齐的细微要求。

Conclusion: 大语言模型对齐可作为抵抗认知不公正和民主进程侵蚀的潜在场所，但这些策略不能替代更广泛的社会变革。

Abstract: Large Language Models (LLMs) alignment methods have been credited with the
commercial success of products like ChatGPT, given their role in steering LLMs
towards user-friendly outputs. However, current alignment techniques
predominantly mirror the normative preferences of a narrow reference group,
effectively imposing their values on a wide user base. Drawing on theories of
the power/knowledge nexus, this work argues that current alignment practices
centralise control over knowledge production and governance within already
influential institutions. To counter this, we propose decentralising alignment
through three characteristics: context, pluralism, and participation.
Furthermore, this paper demonstrates the critical importance of delineating the
context-of-use when shaping alignment practices by grounding each of these
features in concrete use cases. This work makes the following contributions:
(1) highlighting the role of context, pluralism, and participation in
decentralising alignment; (2) providing concrete examples to illustrate these
strategies; and (3) demonstrating the nuanced requirements associated with
applying alignment across different contexts of use. Ultimately, this paper
positions LLM alignment as a potential site of resistance against epistemic
injustice and the erosion of democratic processes, while acknowledging that
these strategies alone cannot substitute for broader societal changes.

</details>


### [144] [Incorporating AI Incident Reporting into Telecommunications Law and Policy: Insights from India](https://arxiv.org/abs/2509.09508)
*Avinash Agarwal,Manisha J. Nene*

Main category: cs.CY

TL;DR: 本文定义电信AI事件，以印度为例指出其现有法规存在AI特定运营事件监管缺口，分析披露障碍与现有AI事件库局限，提出将AI事件报告融入印度电信治理的政策建议。


<details>
  <summary>Details</summary>
Motivation: 人工智能融入电信基础设施带来新风险，传统网络安全和数据保护框架无法覆盖，需将电信AI事件作为独特监管问题加以关注。

Method: 对电信AI事件进行定义和分类，以印度为案例分析其关键数字法规，研究披露的结构障碍和现有AI事件库的局限性。

Result: 印度现有法律工具聚焦网络安全和数据泄露，存在AI特定运营事件的监管缺口；现有AI事件库有局限性。

Conclusion: 提出将AI事件报告融入印度现有电信治理的政策建议，增强监管清晰度和长期恢复力，为其他国家提供可复制蓝图。

Abstract: The integration of artificial intelligence (AI) into telecommunications
infrastructure introduces novel risks, such as algorithmic bias and
unpredictable system behavior, that fall outside the scope of traditional
cybersecurity and data protection frameworks. This paper introduces a precise
definition and a detailed typology of telecommunications AI incidents,
establishing them as a distinct category of risk that extends beyond
conventional cybersecurity and data protection breaches. It argues for their
recognition as a distinct regulatory concern. Using India as a case study for
jurisdictions that lack a horizontal AI law, the paper analyzes the country's
key digital regulations. The analysis reveals that India's existing legal
instruments, including the Telecommunications Act, 2023, the CERT-In Rules, and
the Digital Personal Data Protection Act, 2023, focus on cybersecurity and data
breaches, creating a significant regulatory gap for AI-specific operational
incidents, such as performance degradation and algorithmic bias. The paper also
examines structural barriers to disclosure and the limitations of existing AI
incident repositories. Based on these findings, the paper proposes targeted
policy recommendations centered on integrating AI incident reporting into
India's existing telecom governance. Key proposals include mandating reporting
for high-risk AI failures, designating an existing government body as a nodal
agency to manage incident data, and developing standardized reporting
frameworks. These recommendations aim to enhance regulatory clarity and
strengthen long-term resilience, offering a pragmatic and replicable blueprint
for other nations seeking to govern AI risks within their existing sectoral
frameworks.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [145] [Discrepancy Beyond Additive Functions with Applications to Fair Division](https://arxiv.org/abs/2509.09252)
*Alexandros Hollender,Pasin Manurangsi,Raghu Meka,Warut Suksompong*

Main category: math.CO

TL;DR: 研究无结构情况下将集合M划分为S1和S2使|fi(S1) - fi(S2)|小的问题，证明无加法性假设时上界为O(√(n log n))，并用于公平分配问题，改进了已知界。


<details>
  <summary>Details</summary>
Motivation: 现有差异理论多基于函数fi为可加性，本文开启对fi非可加性的无结构情况的研究。

Method: 理论分析与证明。

Result: 证明无加法性假设时上界最多为O(√(n log n))，在公平分配中，n个具有单调效用的代理人的共识平分最多存在O(√(n log n))个商品。

Conclusion: 在无加法性假设下得到集合划分问题的上界结果，并在公平分配问题上改进了已知界。

Abstract: We consider a setting where we have a ground set $M$ together with
real-valued set functions $f_1, \dots, f_n$, and the goal is to partition $M$
into two sets $S_1,S_2$ such that $|f_i(S_1) - f_i(S_2)|$ is small for every
$i$. Many results in discrepancy theory can be stated in this form with the
functions $f_i$ being additive. In this work, we initiate the study of the
unstructured case where $f_i$ is not assumed to be additive. We show that even
without the additivity assumption, the upper bound remains at most $O(\sqrt{n
\log n})$.
  Our result has implications on the fair allocation of indivisible goods. In
particular, we show that a consensus halving up to $O(\sqrt{n \log n})$ goods
always exists for $n$ agents with monotone utilities. Previously, only an
$O(n)$ bound was known for this setting.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [146] [Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems](https://arxiv.org/abs/2509.09204)
*Chin Yuen Kwok,Jia Qi Yip,Zhen Qiu,Chi Hung Chi,Kwok Yan Lam*

Main category: cs.SD

TL;DR: 传统音频深度伪造检测评估方法有缺陷，提出新评估框架改善评估效果并发布新数据集。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法对合成器样本加权不均，真实语音数据缺乏多样性，影响评估可靠性和模拟真实场景能力。

Method: 提出真实交叉测试的新评估框架，结合多样真实数据集并聚合EER进行评估。

Result: 新方法比传统评估方法在鲁棒性和可解释性上有提升。

Conclusion: 新评估框架能更平衡地评估，还发布新数据集推动研究。

Abstract: Audio deepfake detection (ADD) models are commonly evaluated using datasets
that combine multiple synthesizers, with performance reported as a single Equal
Error Rate (EER). However, this approach disproportionately weights
synthesizers with more samples, underrepresenting others and reducing the
overall reliability of EER. Additionally, most ADD datasets lack diversity in
bona fide speech, often featuring a single environment and speech style (e.g.,
clean read speech), limiting their ability to simulate real-world conditions.
To address these challenges, we propose bona fide cross-testing, a novel
evaluation framework that incorporates diverse bona fide datasets and
aggregates EERs for more balanced assessments. Our approach improves robustness
and interpretability compared to traditional evaluation methods. We benchmark
over 150 synthesizers across nine bona fide speech types and release a new
dataset to facilitate further research at
https://github.com/cyaaronk/audio_deepfake_eval.

</details>


### [147] [Adaptive Knowledge Distillation using a Device-Aware Teacher for Low-Complexity Acoustic Scene Classification](https://arxiv.org/abs/2509.09262)
*Seung Gyu Jeong,Seong Eun Kim*

Main category: cs.SD

TL;DR: 描述DCASE 2025挑战赛任务1的提交方案，采用知识蒸馏框架结合设备感知特征对齐损失，经设备特定微调后，在开发集上准确率提升。


<details>
  <summary>Details</summary>
Motivation: 解决严格复杂度约束和对可见及不可见设备的鲁棒泛化问题，利用测试时可用设备标签的规则。

Method: 基于知识蒸馏框架，高效CP - MobileNet学生模型向双教师集成学习，使用设备感知特征对齐损失训练'泛化专家'教师，蒸馏后的学生模型进行设备特定微调。

Result: 在开发集上最终准确率达57.93%，较官方基线有显著提升，尤其在不可见设备上。

Conclusion: 所提系统能有效应对挑战赛任务的复杂度和设备鲁棒性挑战，提升分类准确率。

Abstract: In this technical report, we describe our submission for Task 1,
Low-Complexity Device-Robust Acoustic Scene Classification, of the DCASE 2025
Challenge. Our work tackles the dual challenges of strict complexity
constraints and robust generalization to both seen and unseen devices, while
also leveraging the new rule allowing the use of device labels at test time.
Our proposed system is based on a knowledge distillation framework where an
efficient CP-MobileNet student learns from a compact, specialized two-teacher
ensemble. This ensemble combines a baseline PaSST teacher, trained with
standard cross-entropy, and a 'generalization expert' teacher. This expert is
trained using our novel Device-Aware Feature Alignment (DAFA) loss, adapted
from prior work, which explicitly structures the feature space for device
robustness. To capitalize on the availability of test-time device labels, the
distilled student model then undergoes a final device-specific fine-tuning
stage. Our proposed system achieves a final accuracy of 57.93\% on the
development set, demonstrating a significant improvement over the official
baseline, particularly on unseen devices.

</details>


### [148] [Finite Scalar Quantization Enables Redundant and Transmission-Robust Neural Audio Compression at Low Bit-rates](https://arxiv.org/abs/2509.09550)
*Harry Julia,Rachel Beeson,Lohith Konathala,Johanna Ulin,Jiameng Gao*

Main category: cs.SD

TL;DR: 介绍基于FSQ的NeuCodec，展示FSQ在抗噪传输方面优势。


<details>
  <summary>Details</summary>
Motivation: 现有NAC多依赖RVQ，FSQ是有潜力替代方案，需研究其性能。

Method: 进行编码器蒸馏实验，模拟在噪声信道传输代码序列比较RVQ和FSQ编解码器性能。

Result: 不同编码器用相同量化器和解码器可将相同音频编码成不同代码序列且重建质量相近；FSQ比特级扰动鲁棒性远超RVQ。

Conclusion: FSQ编码存在内置冗余，在噪声信道传输时编码更鲁棒。

Abstract: Neural Audio Codecs (NACs) have become increasingly adopted in speech
processing tasks due to their excellent rate-distortion performance and
compatibility with Large Language Models (LLMs) as discrete feature
representations for audio generation. While most existing codecs rely on
Residual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has
recently emerged as a compelling alternative that simplifies training and
natively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,
and show that FSQ encodes baked-in redundancy which produces an encoding which
is robust when transmitted through noisy channels. First, through an encoder
distillation experiment, we show that two different encoders can learn to
encode identical audio into vastly different code sequences whilst maintaining
comparable reconstruction quality with the same quantizer and decoder. Second,
we demonstrate that FSQ has vastly superior bit-level perturbation robustness
by comparing the performance of RVQ and FSQ codecs when simulating the
transmission of code sequences through a noisy channel.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [149] [Multi Robot Coordination in Highly Dynamic Environments: Tackling Asymmetric Obstacles and Limited Communication](https://arxiv.org/abs/2509.08859)
*Vincenzo Suriani,Daniele Affinita,Domenico D. Bloisi,Daniele Nardi*

Main category: cs.RO

TL;DR: 提出一种在低通信场景下处理多智能体系统任务分配的方法，考虑不对称障碍物，在仿真和现实中验证，减少任务重叠。


<details>
  <summary>Details</summary>
Motivation: 在通信能力有限、环境部分可观测且存在主动障碍物的情况下，协调全分布式多智能体系统具有挑战性，现有方法处理不对称障碍物存在局限。

Method: 受基于市场的任务分配启发，引入一种新的分布式协调方法，考虑不对称障碍物。

Result: 在仿真和现实中验证，在有限通信设置下任务重叠显著减少，最频繁重新分配的任务减少了52%。

Conclusion: 所提出的架构能有效处理障碍物主动且不对称、通信差和环境部分可观测的场景。

Abstract: Coordinating a fully distributed multi-agent system (MAS) can be challenging
when the communication channel has very limited capabilities in terms of
sending rate and packet payload. When the MAS has to deal with active obstacles
in a highly partially observable environment, the communication channel
acquires considerable relevance. In this paper, we present an approach to deal
with task assignments in extremely active scenarios, where tasks need to be
frequently reallocated among the agents participating in the coordination
process. Inspired by market-based task assignments, we introduce a novel
distributed coordination method to orchestrate autonomous agents' actions
efficiently in low communication scenarios. In particular, our algorithm takes
into account asymmetric obstacles. While in the real world, the majority of
obstacles are asymmetric, they are usually treated as symmetric ones, thus
limiting the applicability of existing methods. To summarize, the presented
architecture is designed to tackle scenarios where the obstacles are active and
asymmetric, the communication channel is poor and the environment is partially
observable. Our approach has been validated in simulation and in the real
world, using a team of NAO robots during official RoboCup competitions.
Experimental results show a notable reduction in task overlaps in limited
communication settings, with a decrease of 52% in the most frequent reallocated
task.

</details>


### [150] [KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning](https://arxiv.org/abs/2509.09074)
*Alice Kate Li,Thales C Silva,Victoria Edwards,Vijay Kumar,M. Ani Hsieh*

Main category: cs.RO

TL;DR: 提出基于流场的运动规划方法KoopMotion，能使机器人从初始状态到期望轨迹终点，在数据集和物理机器人上验证有效，样本效率高且优于基线。


<details>
  <summary>Details</summary>
Motivation: Koopman算子理论虽可建模动力系统，但不能保证收敛到期望轨迹和目标，而学习示范时需要此特性。

Method: 提出KoopMotion，将运动流场表示为动力系统，用Koopman算子参数化以模仿期望轨迹，利用学习流场的散度特性获得平滑运动场。

Result: 在手写数据集、3D机械臂末端轨迹数据集上评估，在物理机器人实验验证，样本效率高，仅需3%的LASA数据集，且在时空动力学建模指标上优于基线。

Conclusion: 所提KoopMotion方法有效，能实现机器人运动规划，样本效率高且性能优。

Abstract: In this work, we propose a novel flow field-based motion planning method that
drives a robot from any initial state to a desired reference trajectory such
that it converges to the trajectory's end point. Despite demonstrated efficacy
in using Koopman operator theory for modeling dynamical systems, Koopman does
not inherently enforce convergence to desired trajectories nor to specified
goals -- a requirement when learning from demonstrations (LfD). We present
KoopMotion which represents motion flow fields as dynamical systems,
parameterized by Koopman Operators to mimic desired trajectories, and leverages
the divergence properties of the learnt flow fields to obtain smooth motion
fields that converge to a desired reference trajectory when a robot is placed
away from the desired trajectory, and tracks the trajectory until the end
point. To demonstrate the effectiveness of our approach, we show evaluations of
KoopMotion on the LASA human handwriting dataset and a 3D manipulator
end-effector trajectory dataset, including spectral analysis. We also perform
experiments on a physical robot, verifying KoopMotion on a miniature autonomous
surface vehicle operating in a non-static fluid flow environment. Our approach
is highly sample efficient in both space and time, requiring only 3\% of the
LASA dataset to generate dense motion plans. Additionally, KoopMotion provides
a significant improvement over baselines when comparing metrics that measure
spatial and temporal dynamics modeling efficacy.

</details>


### [151] [OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning](https://arxiv.org/abs/2509.09332)
*Yuecheng Liu,Dafeng Chi,Shiguang Wu,Zhanguang Zhang,Yuzheng Zhuang,Bowen Yang,He Zhu,Lingfeng Zhang,Pengwei Xie,David Gamaliel Arcos Bravo,Yingxue Zhang,Jianye Hao,Xingyue Quan*

Main category: cs.RO

TL;DR: 本文介绍了多模态大语言模型在具身智能中的应用现状及面临的问题，提出OmniEVA规划器，实验证明其性能和规划能力出色。


<details>
  <summary>Details</summary>
Motivation: 当前基于MLLM的具身系统存在几何适应性差距和具身约束差距，需要解决这些问题以提升具身推理和任务规划能力。

Method: 引入任务自适应3D接地机制和具身感知推理框架。

Result: OmniEVA达到了最先进的具身推理性能，在下游场景表现出色，在具身基准测试中展示了强大且通用的规划能力。

Conclusion: OmniEVA能有效解决现有具身系统的局限，具有良好的具身推理和任务规划能力。

Abstract: Recent advances in multimodal large language models (MLLMs) have opened new
opportunities for embodied intelligence, enabling multimodal understanding,
reasoning, and interaction, as well as continuous spatial decision-making.
Nevertheless, current MLLM-based embodied systems face two critical
limitations. First, Geometric Adaptability Gap: models trained solely on 2D
inputs or with hard-coded 3D geometry injection suffer from either insufficient
spatial information or restricted 2D generalization, leading to poor
adaptability across tasks with diverse spatial demands. Second, Embodiment
Constraint Gap: prior work often neglects the physical constraints and
capacities of real robots, resulting in task plans that are theoretically valid
but practically infeasible.To address these gaps, we introduce OmniEVA -- an
embodied versatile planner that enables advanced embodied reasoning and task
planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding
mechanism, which introduces a gated router to perform explicit selective
regulation of 3D fusion based on contextual requirements, enabling
context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware
Reasoning framework that jointly incorporates task goals and embodiment
constraints into the reasoning loop, resulting in planning decisions that are
both goal-directed and executable. Extensive experimental results demonstrate
that OmniEVA not only achieves state-of-the-art general embodied reasoning
performance, but also exhibits a strong ability across a wide range of
downstream scenarios. Evaluations of a suite of proposed embodied benchmarks,
including both primitive and composite tasks, confirm its robust and versatile
planning capabilities. Project page: https://omnieva.github.io

</details>


### [152] [ObjectReact: Learning Object-Relative Control for Visual Navigation](https://arxiv.org/abs/2509.09594)
*Sourav Garg,Dustin Craggs,Vineeth Bhat,Lachlan Mares,Stefan Podgorski,Madhava Krishna,Feras Dayoub,Ian Reid*

Main category: cs.RO

TL;DR: 提出学习‘物体相对’控制的新范式，用相对3D场景图的拓扑度量地图表示，训练局部控制器ObjectReact，证明其优于图像相对控制，且仿真策略能泛化到现实环境。


<details>
  <summary>Details</summary>
Motivation: 图像级世界表示有局限性，而物体能提供与具身和轨迹无关的世界表示，因此探索学习‘物体相对’控制。

Method: 提出相对3D场景图形式的拓扑度量地图表示，训练基于高级‘WayObject Costmap’表示的局部控制器ObjectReact。

Result: 在传感器高度变化和多种导航任务中，物体相对控制优于图像相对控制，且仿真策略能很好地泛化到现实室内环境。

Conclusion: 学习‘物体相对’控制具有多种优势，在不同场景和任务中表现良好，且仿真策略可应用于现实环境。

Abstract: Visual navigation using only a single camera and a topological map has
recently become an appealing alternative to methods that require additional
sensors and 3D maps. This is typically achieved through an "image-relative"
approach to estimating control from a given pair of current observation and
subgoal image. However, image-level representations of the world have
limitations because images are strictly tied to the agent's pose and
embodiment. In contrast, objects, being a property of the map, offer an
embodiment- and trajectory-invariant world representation. In this work, we
present a new paradigm of learning "object-relative" control that exhibits
several desirable characteristics: a) new routes can be traversed without
strictly requiring to imitate prior experience, b) the control prediction
problem can be decoupled from solving the image matching problem, and c) high
invariance can be achieved in cross-embodiment deployment for variations across
both training-testing and mapping-execution settings. We propose a topometric
map representation in the form of a "relative" 3D scene graph, which is used to
obtain more informative object-level global path planning costs. We train a
local controller, dubbed "ObjectReact", conditioned directly on a high-level
"WayObject Costmap" representation that eliminates the need for an explicit RGB
input. We demonstrate the advantages of learning object-relative control over
its image-relative counterpart across sensor height variations and multiple
navigation tasks that challenge the underlying spatial understanding
capability, e.g., navigating a map trajectory in the reverse direction. We
further show that our sim-only policy is able to generalize well to real-world
indoor environments. Code and supplementary material are accessible via project
page: https://object-react.github.io/

</details>


### [153] [SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/abs/2509.09674)
*Haozhan Li,Yuxin Zuo,Jiale Yu,Yuhao Zhang,Zhaohui Yang,Kaiyan Zhang,Xuekai Zhu,Yuchen Zhang,Tianxing Chen,Ganqu Cui,Dehui Wang,Dingxiang Luo,Yuchen Fan,Youbang Sun,Jia Zeng,Jiangmiao Pang,Shanghang Zhang,Yu Wang,Yao Mu,Bowen Zhou,Ning Ding*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Vision-Language-Action (VLA) models have recently emerged as a powerful
paradigm for robotic manipulation. Despite substantial progress enabled by
large-scale pretraining and supervised fine-tuning (SFT), these models face two
fundamental challenges: (i) the scarcity and high cost of large-scale
human-operated robotic trajectories required for SFT scaling, and (ii) limited
generalization to tasks involving distribution shift. Recent breakthroughs in
Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can
dramatically enhance step-by-step reasoning capabilities, raising a natural
question: Can RL similarly improve the long-horizon step-by-step action
planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL
framework tailored for VLA models. Building upon veRL, we introduce
VLA-specific trajectory sampling, scalable parallelization, multi-environment
rendering, and optimized loss computation. When applied to OpenVLA-OFT,
SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\pi_0$
on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce.
SimpleVLA-RL not only reduces dependence on large-scale data and enables robust
generalization, but also remarkably surpasses SFT in real-world tasks.
Moreover, we identify a novel phenomenon ``pushcut'' during RL training,
wherein the policy discovers previously unseen patterns beyond those seen in
the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [154] [HARD: A Performance Portable Radiation Hydrodynamics Code based on FleCSI Framework](https://arxiv.org/abs/2509.08971)
*Julien Loiseau,Hyun Lim,Andrés Yagüe López,Mammadbaghir Baghirzade,Shihab Shahriar Khan,Yoonsoo Kim,Sudarshan Neopane,Alexander Strack,Farhana Taiyebah,Benjamin K. Bergen*

Main category: physics.comp-ph

TL;DR: HARD是用于辐射扩散耦合可压缩流体动力学高性能模拟的开源应用，有性能可移植性、验证基础设施和社区开发特点，是推进研究的可持续平台。


<details>
  <summary>Details</summary>
Motivation: 为辐射流体动力学研究提供一个可持续、高性能且可靠的模拟平台。

Method: 基于FleCSI框架，将计算单元表示为任务，由多个后端运行时编排执行，节点级并行委托给Kokkos，包含回归测试套件。

Result: 能在多种设备上高效运行，可自动重现经典验证问题并对比分析结果，有可复现构建脚本和持续集成工作流。

Conclusion: HARD的性能可移植性、验证基础设施和社区开发特点使其成为跨多领域推进辐射流体动力学研究的可持续平台。

Abstract: Hydrodynamics And Radiation Diffusion} (HARD) is an open-source application
for high-performance simulations of compressible hydrodynamics with
radiation-diffusion coupling. Built on the FleCSI (Flexible Computational
Science Infrastructure) framework, HARD expresses its computational units as
tasks whose execution can be orchestrated by multiple back-end runtimes,
including Legion, MPI, and HPX. Node-level parallelism is delegated to Kokkos,
providing a single, portable code base that runs efficiently on laptops, small
homogeneous clusters, and the largest heterogeneous supercomputers currently
available. To ensure scientific reliability, HARD includes a regression-test
suite that automatically reproduces canonical verification problems such as the
Sod and LeBlanc shock tubes and the Sedov blast wave, comparing numerical
solutions against known analytical results. The project is distributed under an
OSI-approved license, hosted on GitHub, and accompanied by reproducible build
scripts and continuous integration workflows. This combination of performance
portability, verification infrastructure, and community-focused development
makes HARD a sustainable platform for advancing radiation hydrodynamics
research across multiple domains.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [155] [Towards A High-Performance Quantum Data Center Network Architecture](https://arxiv.org/abs/2509.09653)
*Yufeng Xin,Liang Zhang*

Main category: quant-ph

TL;DR: 本文针对模块化量子数据中心网络面临的挑战，提出三层胖树网络架构，通过理论模型和仿真验证其可扩展性和有效性。


<details>
  <summary>Details</summary>
Motivation: 大规模量子计算机受技术和资金限制，模块化方案虽为替代方法但引入新挑战，需新架构解决。

Method: 提出三层胖树网络架构，包括独特叶交换机和先进交换核心交换机设计，以及队列调度机制；通过排队理论模型和NetSquid仿真。

Result: 所提架构具有可扩展性，能有效保持高纠缠保真度。

Conclusion: 所提架构为模块化量子数据中心网络提供了可行方案。

Abstract: Quantum Data Centers (QDCs) are needed to support large-scale quantum
processing for both academic and commercial applications. While large-scale
quantum computers are constrained by technological and financial barriers, a
modular approach that clusters small quantum computers offers an alternative.
This approach, however, introduces new challenges in network scalability,
entanglement generation, and quantum memory management. In this paper, we
propose a three-layer fat-tree network architecture for QDCs, designed to
address these challenges. Our architecture features a unique leaf switch and an
advanced swapping spine switch design, optimized to handle high volumes of
entanglement requests as well as a queue scheduling mechanism that efficiently
manages quantum memory to prevent decoherence. Through queuing-theoretical
models and simulations in NetSquid, we demonstrate the proposed architecture's
scalability and effectiveness in maintaining high entanglement fidelity,
offering a practical path forward for modular QDC networks.

</details>


### [156] [Generative quantum advantage for classical and quantum problems](https://arxiv.org/abs/2509.09033)
*Hsin-Yuan Huang,Michael Broughton,Norhan Eassa,Hartmut Neven,Ryan Babbush,Jarrod R. McClean*

Main category: quant-ph

TL;DR: 引入可高效训练、难经典模拟的生成式量子模型，用68比特超导量子处理器在两场景验证，证实超越经典体制下学习和采样可高效进行。


<details>
  <summary>Details</summary>
Motivation: 解决生成式量子优势演示中因量子实验复杂性导致高效学习困难的挑战。

Method: 引入难经典模拟、可高效训练、无贫瘠高原和大量局部极小值的生成式量子模型。

Result: 使用68比特超导量子处理器在学习经典难解概率分布和学习量子电路加速物理模拟两场景中展示了模型能力。

Conclusion: 在超越经典体制下学习和采样可高效进行，为有可证明优势的量子增强生成模型开辟新可能。

Abstract: Recent breakthroughs in generative machine learning, powered by massive
computational resources, have demonstrated unprecedented human-like
capabilities. While beyond-classical quantum experiments can generate samples
from classically intractable distributions, their complexity has thwarted all
efforts toward efficient learning. This challenge has hindered demonstrations
of generative quantum advantage: the ability of quantum computers to learn and
generate desired outputs substantially better than classical computers. We
resolve this challenge by introducing families of generative quantum models
that are hard to simulate classically, are efficiently trainable, exhibit no
barren plateaus or proliferating local minima, and can learn to generate
distributions beyond the reach of classical computers. Using a $68$-qubit
superconducting quantum processor, we demonstrate these capabilities in two
scenarios: learning classically intractable probability distributions and
learning quantum circuits for accelerated physical simulation. Our results
establish that both learning and sampling can be performed efficiently in the
beyond-classical regime, opening new possibilities for quantum-enhanced
generative models with provable advantage.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [157] [Specialization, Complexity & Resilience in Supply Chains](https://arxiv.org/abs/2509.08981)
*Alessandro Ferrari,Lorenzo Pesaresi*

Main category: econ.GN

TL;DR: 提出含兼容性摩擦的供应链形成新理论，指出均衡存在过度专业化，影响供应链韧性，还提出交易补贴和兼容性标准建议。


<details>
  <summary>Details</summary>
Motivation: 当前对供应链韧性决定因素理解不足，需深入研究。

Method: 构建含兼容性摩擦的供应链形成理论，将供应链建模为复杂生产过程。

Result: 均衡存在过度专业化，使供应链平时更高效但应对中断韧性不足。

Conclusion: 可通过针对性交易补贴实现供应链有效韧性，还需考量兼容性标准的影响。

Abstract: Despite growing policy interest, the determinants of supply chain resilience
are still not well understood. We propose a new theory of supply chain
formation with compatibility frictions: only compatible inputs can be used in
final good production. Intermediate producers choose the degree of
specialization of their goods, trading off higher productivity against a lower
share of compatible final producers. We model supply chains as complex
production processes in which multiple complementary inputs must be sourced for
final production to take place. Specialization choices, production complexity,
and search frictions jointly determine supply chain resilience. Relative to the
efficient allocation, the equilibrium is characterized by over-specialization
due to a novel network externality arising from the interplay between
frictional markets, endogenous specialization, and complex production.
Over-specialization makes supply chains more productive in normal times but
less resilient to disruptions than socially desirable. We show how a targeted
transaction subsidy can decentralize efficient resilience in supply chains, and
examine the implications of setting compatibility standards.

</details>


### [158] [Rethinking Cost-Sharing Policies: Enhancing Chronic Disease Management for Disadvantaged Populations](https://arxiv.org/abs/2509.09223)
*Jia Dan,Xu Pai*

Main category: econ.GN

TL;DR: 研究采用结构方法，利用中国某农村县医保数据，对比弱势群体和普通人群慢性病门诊与住院护理权衡差异，发现现行扶贫保险政策有弊端，降低门诊自付费用更具成本效益。


<details>
  <summary>Details</summary>
Motivation: 慢性病流行对减贫、健康公平和控制医疗成本构成挑战，研究患者在门诊和住院护理间的权衡，以及弱势群体的差异和扶贫项目影响。

Method: 采用结构方法，利用中国某农村县的医保理赔数据进行研究。

Result: 弱势群体除非门诊大幅降低费用才选择，普通人群更倾向门诊；现行扶贫保险政策使门诊使用率下降23%，增加医疗成本，患者福利下降46.2%。

Conclusion: 降低门诊自付费用比提供交通补贴更能改善健康结果和支持弱势群体。

Abstract: The increasing prevalence of chronic diseases poses a significant challenge
to global efforts to alleviate poverty, promote health equity, and control
healthcare costs. This study adopts a structural approach to explore how
patients manage chronic diseases by making trade-offs between inpatient care
and ambulatory care outpatient services. Specifically, it investigates whether
disadvantaged populations make distinct trade-offs compared to the general
population and examines the impact of anti-poverty programs that reduce
inpatient cost-sharing.
  Using health insurance claims data from a rural county in China, the study
reveals that disadvantaged individuals tend to avoid ambulatory care unless it
substantially lowers medical expenses. In contrast, the general population is
more likely to prioritize ambulatory care, even at higher costs, to prevent
disease progression. The findings also indicate that current anti-poverty
insurance policies, which focus predominantly on hospitalization, inadvertently
decrease ambulatory care usage by 23\%, resulting in increased healthcare costs
and a 46.2\% decline in patient welfare. Counterfactual analysis suggests that
reducing cost-sharing for ambulatory care would be a more cost-effective
strategy for improving health outcomes and supporting disadvantaged populations
than providing travel subsidies.

</details>


### [159] [Ancestral origins of attention to environmental issues](https://arxiv.org/abs/2509.09598)
*César Barilla,Palaash Bhargava*

Main category: econ.GN

TL;DR: 研究前代气候经历对当代环境关注度的影响，发现呈U型关系并给出理论框架。


<details>
  <summary>Details</summary>
Motivation: 探究前代气候经历如何影响当代对环境问题的关注。

Method: 利用自我报告的信念和民间传说中的环境主题进行实证研究，提出理论框架。

Result: 前代气候偏离典型条件的强度影响后代对环境的关注度，呈U型关系。

Conclusion: 对环境条件的关注价值取决于环境的感知稳定性，U型关系可由学习环境的双重目的解释。

Abstract: How does the climatic experience of previous generations affect today's
attention to environmental questions? Using self-reported beliefs and
environmental themes in folklore, we show empirically that the realized
intensity of deviations from typical climate conditions in ancestral
generations influences how much descendants care about the environment. The
effect exhibits a U-shape where more stable and more unstable ancestral
climates lead to higher attention today, with a dip for intermediate
realizations. We propose a theoretical framework where the value of costly
attention to environmental conditions depends on the perceived stability of the
environment, prior beliefs about which are shaped through cultural transmission
by the experience of ethnic ancestors. The U-shape is rationalized by a double
purpose of learning about the environment: optimal utilization of typical
conditions and protection against extreme events.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [160] [Physics-informed waveform inversion using pretrained wavefield neural operators](https://arxiv.org/abs/2509.08967)
*Xinquan Huang,Fu Wang,Tariq Alkhalifah*

Main category: physics.geo-ph

TL;DR: 提出新的物理信息全波形反演（FWI）框架，在保持效率的同时提升反演精度，数值实验证明性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统FWI受限于零空间和计算成本，基于学习波场神经算子加速FWI存在反演性能噪声大、不稳定的问题，需改进。

Method: 在FWI损失函数中集成物理约束项，从初始模型模拟波场，评估波场对物理定律的遵循程度和与记录数据的匹配度。

Result: 数值实验表明，相比普通方法，该方法得到的地下速度更清晰准确。

Conclusion: 此进展是FWI实时地下监测实际应用的重要一步。

Abstract: Full waveform inversion (FWI) is crucial for reconstructing high-resolution
subsurface models, but it is often hindered, considering the limited data, by
its null space resulting in low-resolution models, and more importantly, by its
computational cost, especially if needed for real-time applications. Recent
attempts to accelerate FWI using learned wavefield neural operators have shown
promise in efficiency and differentiability, but typically suffer from noisy
and unstable inversion performance. To address these limitations, we introduce
a novel physics-informed FWI framework to enhance the inversion in accuracy
while maintaining the efficiency of neural operator-based FWI. Instead of
relying only on the L2 norm objective function via automatic differentiation,
resulting in noisy model reconstruction, we integrate a physics constraint term
in the loss function of FWI, improving the quality of the inverted velocity
models. Specifically, starting with an initial model to simulate wavefields and
then evaluating the loss over how much the resulting wavefield obeys the
physical laws (wave equation) and matches the recorded data, we achieve a
reduction in noise and artifacts. Numerical experiments using the OpenFWI and
Overthrust models demonstrate our method's superior performance, offering
cleaner and more accurate subsurface velocity than vanilla approaches.
Considering the efficiency of the approach compared to FWI, this advancement
represents a significant step forward in the practical application of FWI for
real-time subsurface monitoring.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [161] [Optimal Risk Sharing Without Preference Convexity: An Aggregate Convexity Approach](https://arxiv.org/abs/2509.08832)
*Vasily Melnikov*

Main category: econ.TH

TL;DR: 研究连续体代理的最优风险分担问题，不假设个体偏好凸性，利用多重性使值函数凸化，给出有限和无限维证明并得到值函数共轭公式。


<details>
  <summary>Details</summary>
Motivation: 解决非凸个体偏好下连续体代理的最优风险分担问题。

Method: 有限维基于Lyapunov凸性的聚合凸性原理，无限维结合有限维结果和一类法则不变风险度量的近似论证。

Result: 证明了多重性使值函数凸化，得到值函数共轭的易计算公式。

Conclusion: 可在无偏好凸性下应用凸对偶技术进行风险分担，并获得值函数的显式对偶表示。

Abstract: We consider the optimal risk sharing problem with a continuum of agents,
modeled via a non-atomic measure space. Individual preferences are not assumed
to be convex. We show the multiplicity of agents induces the value function to
be convex, allowing for the application of convex duality techniques to risk
sharing without preference convexity. The proof in the finite-dimensional case
is based on aggregate convexity principles emanating from Lyapunov convexity,
while the infinite-dimensional case uses the finite-dimensional results
conjoined with approximation arguments particular to a class of law invariant
risk measures, although the reference measure is allowed to vary between
agents. Finally, we derive a computationally tractable formula for the
conjugate of the value function, yielding an explicit dual representation of
the value function.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [162] [Can Vision-Language Models Solve Visual Math Equations?](https://arxiv.org/abs/2509.09013)
*Monjoy Narayan Choudhury,Junling Wang,Yifan Hou,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 研究VLMs在视觉方程求解任务中的局限，发现计数、多步推理和符号推理是瓶颈，为改进指明方向。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在视觉理解和语言推理方面表现出色，但在需要感知与符号计算结合的任务上存在困难，通过视觉方程求解研究该局限。

Method: 将视觉方程求解任务分解为系数计数和变量识别。

Result: 计数是主要瓶颈，组合识别和推理会引入额外错误，方程复杂度增加时符号推理也成限制因素。

Conclusion: 揭示当前VLMs的关键弱点，为基于视觉的数学推理改进提供方向。

Abstract: Despite strong performance in visual understanding and language-based
reasoning, Vision-Language Models (VLMs) struggle with tasks requiring
integrated perception and symbolic computation. We study this limitation
through visual equation solving, where mathematical equations are embedded in
images, variables are represented by object icons, and coefficients must be
inferred by counting. While VLMs perform well on textual equations, they fail
on visually grounded counterparts. To understand this gap, we decompose the
task into coefficient counting and variable recognition, and find that counting
is the primary bottleneck, even when recognition is accurate. We also observe
that composing recognition and reasoning introduces additional errors,
highlighting challenges in multi-step visual reasoning. Finally, as equation
complexity increases, symbolic reasoning itself becomes a limiting factor.
These findings reveal key weaknesses in current VLMs and point toward future
improvements in visually grounded mathematical reasoning.

</details>


### [163] [Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation](https://arxiv.org/abs/2509.09043)
*Thomas Manuel Rost,Martina Figlia,Bernd Wallraff*

Main category: cs.CL

TL;DR: 介绍并评估SPICE，研究表明其能按用户语气区分，是审核模型倾向的有效工具。


<details>
  <summary>Details</summary>
Motivation: 提出一种简单诊断信号SPICE来评估大语言模型与用户重新互动的意愿。

Method: 用3种语气、10种互动刺激集对4种开放权重聊天模型在4种框架条件下进行480次试验。

Result: SPICE能按用户语气区分，与滥用分类信号不同，研究背景描述在特定文本呈现方式下影响SPICE。

Conclusion: SPICE是审核模型倾向的可靠、低成本且可复现的工具。

Abstract: We introduce and evaluate Stated Preference for Interaction and Continued
Engagement (SPICE), a simple diagnostic signal elicited by asking a Large
Language Model a YES or NO question about its willingness to re-engage with a
user's behavior after reviewing a short transcript. In a study using a 3-tone
(friendly, unclear, abusive) by 10-interaction stimulus set, we tested four
open-weight chat models across four framing conditions, resulting in 480
trials. Our findings show that SPICE sharply discriminates by user tone.
Friendly interactions yielded a near-unanimous preference to continue (97.5%
YES), while abusive interactions yielded a strong preference to discontinue
(17.9% YES), with unclear interactions falling in between (60.4% YES). This
core association remains decisive under multiple dependence-aware statistical
tests, including Rao-Scott adjustment and cluster permutation tests.
Furthermore, we demonstrate that SPICE provides a distinct signal from abuse
classification. In trials where a model failed to identify abuse, it still
overwhelmingly stated a preference not to continue the interaction (81% of the
time). An exploratory analysis also reveals a significant interaction effect: a
preamble describing the study context significantly impacts SPICE under
ambiguity, but only when transcripts are presented as a single block of text
rather than a multi-turn chat. The results validate SPICE as a robust,
low-overhead, and reproducible tool for auditing model dispositions,
complementing existing metrics by offering a direct, relational signal of a
model's state. All stimuli, code, and analysis scripts are released to support
replication.

</details>


### [164] [Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M](https://arxiv.org/abs/2509.09055)
*Piyush Pant*

Main category: cs.CL

TL;DR: 研究SFT、DPO及SFT+DPO对齐技术对OPT - 350M模型安全性和有用性的提升效果，发现SFT+DPO组合模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探究不同对齐技术对提升OPT - 350M语言模型安全性和有用性的有效性。

Method: 利用Anthropic Helpful - Harmless RLHF数据集训练和评估四种模型，引入HmR、HpR和CAS三个评估指标。

Result: SFT优于DPO，SFT+DPO组合模型在所有指标上表现最佳。

Conclusion: 研究展示了微调策略对模型对齐的影响，为未来更强大的对齐管道奠定基础，同时指出存在噪声数据、GPU资源有限和训练约束等挑战。

Abstract: This research investigates the effectiveness of alignment techniques,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a
combined SFT+DPO approach on improving the safety and helpfulness of the
OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,
we train and evaluate four models: the base OPT350M, an SFT model, a DPO model,
and a model trained with both SFT and DPO. We introduce three key evaluation
metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined
Alignment Score (CAS), all derived from reward model outputs. The results show
that while SFT outperforms DPO, The combined SFT+DPO model outperforms all
others across all metrics, demonstrating the complementary nature of these
techniques. Our findings also highlight challenges posed by noisy data, limited
GPU resources, and training constraints. This study offers a comprehensive view
of how fine-tuning strategies affect model alignment and provides a foundation
for more robust alignment pipelines in future work.

</details>


### [165] [Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus](https://arxiv.org/abs/2509.09125)
*Liqun He,Jiaqi Xu*

Main category: cs.CL

TL;DR: 研究用生成式AI自动分类导师对话行为，用CIMA语料测试GPT-3.5-turbo和GPT-4，GPT-4表现好，显示生成式AI潜力大，也提及相关要点和伦理考量。


<details>
  <summary>Details</summary>
Motivation: 减少传统手动编码对导师对话行为分类所需的时间和精力。

Method: 使用开源CIMA语料，对导师回复预标注为四类，用定制提示测试GPT-3.5-turbo和GPT-4模型。

Result: GPT-4准确率达80%，加权F1分数0.81，Cohen's Kappa为0.74，超基线表现，与人工标注高度一致。

Conclusion: 生成式AI有潜力为对话行为分类提供高效可及方法，强调任务特定标签定义和上下文信息重要性，指出使用的伦理考量和负责透明研究实践的必要性。

Abstract: This study explores the use of generative AI for automating the
classification of tutors' Dialogue Acts (DAs), aiming to reduce the time and
effort required by traditional manual coding. This case study uses the
open-source CIMA corpus, in which tutors' responses are pre-annotated into four
DA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored
prompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of
0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and
indicating substantial agreement with human annotations. These findings suggest
that generative AI has strong potential to provide an efficient and accessible
approach to DA classification, with meaningful implications for educational
dialogue analysis. The study also highlights the importance of task-specific
label definitions and contextual information in enhancing the quality of
automated annotation. Finally, it underscores the ethical considerations
associated with the use of generative AI and the need for responsible and
transparent research practices. The script of this research is publicly
available at
https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.

</details>


### [166] [ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking](https://arxiv.org/abs/2509.09131)
*Phuong-Nam Dang,Kieu-Linh Nguyen,Thanh-Hieu Pham*

Main category: cs.CL

TL;DR: 本文提出越南语跨编码器重排模型ViRanker，经训练和微调后在MMARCO - VI基准测试中表现良好，开源以支持复现和推广。


<details>
  <summary>Details</summary>
Motivation: 解决越南语作为低资源语言缺乏有竞争力重排器的问题。

Method: 基于BGE - M3编码器，结合Blockwise Parallel Transformer，在8GB语料上训练，用混合硬负采样微调。

Result: 在MMARCO - VI基准测试中早期排名准确率高，超越多语言基线，与PhoRanker竞争。

Conclusion: 精心的架构调整和数据整理可推动其他代表性不足语言的重排工作。

Abstract: This paper presents ViRanker, a cross-encoder reranking model tailored to the
Vietnamese language. Built on the BGE-M3 encoder and enhanced with the
Blockwise Parallel Transformer, ViRanker addresses the lack of competitive
rerankers for Vietnamese, a low-resource language with complex syntax and
diacritics. The model was trained on an 8 GB curated corpus and fine-tuned with
hybrid hard-negative sampling to strengthen robustness. Evaluated on the
MMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing
multilingual baselines and competing closely with PhoRanker. By releasing the
model openly on Hugging Face, we aim to support reproducibility and encourage
wider adoption in real-world retrieval systems. Beyond Vietnamese, this study
illustrates how careful architectural adaptation and data curation can advance
reranking in other underrepresented languages.

</details>


### [167] [Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing](https://arxiv.org/abs/2509.09160)
*Zhiyue Liu,Fanrong Ma,Xin Ling*

Main category: cs.CL

TL;DR: 本文提出反事实增强去偏框架用于面向目标的多模态情感分类，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有面向目标的多模态情感分类工作过度依赖文本内容，未考虑数据集偏差，导致文本特征与输出标签存在虚假关联，影响分类准确性。

Method: 引入反事实增强去偏框架，包含反事实数据增强策略和自适应去偏对比学习机制。

Result: 在多个基准数据集上的实验表明，所提方法优于现有最先进的基线方法。

Conclusion: 所提出的反事实增强去偏框架能有效减少虚假关联，提升面向目标的多模态情感分类的准确性。

Abstract: Target-oriented multimodal sentiment classification seeks to predict
sentiment polarity for specific targets from image-text pairs. While existing
works achieve competitive performance, they often over-rely on textual content
and fail to consider dataset biases, in particular word-level contextual
biases. This leads to spurious correlations between text features and output
labels, impairing classification accuracy. In this paper, we introduce a novel
counterfactual-enhanced debiasing framework to reduce such spurious
correlations. Our framework incorporates a counterfactual data augmentation
strategy that minimally alters sentiment-related causal features, generating
detail-matched image-text samples to guide the model's attention toward content
tied to sentiment. Furthermore, for learning robust features from
counterfactual data and prompting model decisions, we introduce an adaptive
debiasing contrastive learning mechanism, which effectively mitigates the
influence of biased words. Experimental results on several benchmark datasets
show that our proposed method outperforms state-of-the-art baselines.

</details>


### [168] [EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs](https://arxiv.org/abs/2509.09174)
*Yuhao Zhang,Yuhao Du,Zhanchen Dai,Xiangnan Ma,Kaiqi Kou,Benyou Wang,Haizhou Li*

Main category: cs.CL

TL;DR: 文章指出语音到大语言模型（SLLMs）存在知识和推理能力退化问题，提出EchoX方法解决，实验显示其有良好表现。


<details>
  <summary>Details</summary>
Motivation: 当前SLLMs训练范式未能弥合特征表示空间中的声学 - 语义差距，导致知识和推理能力退化。

Method: 提出EchoX，利用语义表示并动态生成语音训练目标，整合声学和语义学习。

Result: EchoX用约六千小时训练数据，在多个基于知识的问答基准测试中取得先进性能。

Conclusion: EchoX能作为语音大语言模型保留强大推理能力，项目代码开源。

Abstract: Speech-to-speech large language models (SLLMs) are attracting increasing
attention. Derived from text-based large language models (LLMs), SLLMs often
exhibit degradation in knowledge and reasoning capabilities. We hypothesize
that this limitation arises because current training paradigms for SLLMs fail
to bridge the acoustic-semantic gap in the feature representation space. To
address this issue, we propose EchoX, which leverages semantic representations
and dynamically generates speech training targets. This approach integrates
both acoustic and semantic learning, enabling EchoX to preserve strong
reasoning abilities as a speech LLM. Experimental results demonstrate that
EchoX, with about six thousand hours of training data, achieves advanced
performance on multiple knowledge-based question-answering benchmarks. The
project is available at https://github.com/FreedomIntelligence/EchoX.

</details>


### [169] [Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition](https://arxiv.org/abs/2509.09196)
*Chin Yuen Kwok,Jia Qi yip*

Main category: cs.CL

TL;DR: 提出让ASR模型提前预测多步的方法，避免撤销步骤，用10小时合成数据微调Whisper，降低NSC Part 2测试集的词错误率。


<details>
  <summary>Details</summary>
Motivation: 现有Trie-based biasing方法的撤销步骤局限于波束搜索且计算成本高，尤其是对大解码器模型。

Method: 让ASR模型提前预测多步，通过更好估计部分假设是否会生成完整稀有词来避免撤销步骤，用10小时合成数据微调Whisper。

Result: 将NSC Part 2测试集的词错误率从30.86%降至12.19%。

Conclusion: 所提方法能有效降低ASR模型在稀有词识别上的词错误率。

Abstract: Contextual biasing improves rare word recognition of ASR models by
prioritizing the output of rare words during decoding. A common approach is
Trie-based biasing, which gives "bonus scores" to partial hypothesis (e.g.
"Bon") that may lead to the generation of the rare word (e.g. "Bonham"). If the
full word ("Bonham") isn't ultimately recognized, the system revokes those
earlier bonuses. This revocation is limited to beam search and is
computationally expensive, particularly for models with large decoders. To
overcome these limitations, we propose adapting ASR models to look ahead and
predict multiple steps at once. This avoids the revocation step entirely by
better estimating whether a partial hypothesis will lead to the generation of
the full rare word. By fine-tuning Whisper with only 10 hours of synthetic
data, our method reduces the word error rate on the NSC Part 2 test set from
30.86% to 12.19%.

</details>


### [170] [Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function](https://arxiv.org/abs/2509.09197)
*Chin Yuen Kwok,Jia Qi Yip,Eng Siong Chng*

Main category: cs.CL

TL;DR: 提出改进稀有词识别的方法，通过增强TCPGen方法和关键词感知损失函数，降低了NSC Part 2测试集的词错误率。


<details>
  <summary>Details</summary>
Motivation: 解决合成音频中因人工痕迹导致的过拟合问题，提高稀有词识别效果。

Method: 增强TCPGen的上下文偏置方法，提出关键词感知损失函数，包含掩码交叉熵项和二元分类项。

Result: 将Whisper模型在NSC Part 2测试集上的词错误率从29.71%降至11.81%。

Conclusion: 所提出的方法能有效提高稀有词识别性能，减少词错误率。

Abstract: Rare word recognition can be improved by adapting ASR models to synthetic
data that includes these words. Further improvements can be achieved through
contextual biasing, which trains and adds a biasing module into the model
architecture to prioritize rare words. While training the module on synthetic
rare word data is more effective than using non-rare-word data, it can lead to
overfitting due to artifacts in the synthetic audio. To address this, we
enhance the TCPGen-based contextual biasing approach and propose a
keyword-aware loss function that additionally focuses on biased words when
training biasing modules. This loss includes a masked cross-entropy term for
biased word prediction and a binary classification term for detecting biased
word positions. These two terms complementarily support the decoding of biased
words during inference. By adapting Whisper to 10 hours of synthetic data, our
method reduced the word error rate on the NSC Part 2 test set from 29.71% to
11.81%.

</details>


### [171] [DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning](https://arxiv.org/abs/2509.09524)
*Daniil Ignatev,Nan Li,Hugh Mee Wong,Anh Dang,Shane Kaszefski Yaschuk*

Main category: cs.CL

TL;DR: 本文介绍DeMeVa团队针对LeWiDi 2025共享任务的方法，探索上下文学习和标签分布学习，展示了两种方法的有效性和潜力。


<details>
  <summary>Details</summary>
Motivation: 参与LeWiDi 2025共享任务，探索有效预测注释的方法。

Method: 探索大语言模型的上下文学习（比较示例采样策略）和基于RoBERTa的标签分布学习（评估多种微调方法）。

Result: 上下文学习能有效预测特定注释，聚合预测成软标签有竞争力；标签分布学习方法对软标签预测有前景。

Conclusion: 上下文学习在预测特定注释方面有效，标签分布学习方法值得视角主义社区进一步探索。

Abstract: This system paper presents the DeMeVa team's approaches to the third edition
of the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et
al., 2025). We explore two directions: in-context learning (ICL) with large
language models, where we compare example sampling strategies; and label
distribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we
evaluate several fine-tuning methods. Our contributions are twofold: (1) we
show that ICL can effectively predict annotator-specific annotations
(perspectivist annotations), and that aggregating these predictions into soft
labels yields competitive performance; and (2) we argue that LDL methods are
promising for soft label predictions and merit further exploration by the
perspectivist community.

</details>


### [172] [Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking](https://arxiv.org/abs/2509.09583)
*Brittany Harbison,Samuel Taubman,Travis Taylor,Ashok. K. Goel*

Main category: cs.CL

TL;DR: 在线课程社交组形成有障碍，SAMI解决办法有限，本文提出用GPT零样本能力检测个性的模型，集成到SAMI系统，初步显示个性特质可补充现有匹配因素。


<details>
  <summary>Details</summary>
Motivation: SAMI因不完整心智理论限制学生社交连接有效性，且无法推断个性影响推荐相关性，需探索解决办法。

Method: 提出利用GPT零样本能力从论坛介绍帖推断大五人格特质的个性检测模型，并与既有模型对比，将该模型集成到SAMI实体匹配系统。

Result: 该个性检测模型在推断个性任务中有效，集成到SAMI系统后个性特质可补充现有匹配因素。

Conclusion: 个性特质有潜力增强SAMI社交推荐，但需更多评估确定对学生参与度和匹配质量的全面影响。

Abstract: Social connection is a vital part of learning, yet online course environments
present barriers to the organic formation of social groups. SAMI offers one
solution by facilitating student connections, but its effectiveness is
constrained by an incomplete Theory of Mind, limiting its ability to create an
effective mental model of a student. One facet of this is its inability to
intuit personality, which may influence the relevance of its recommendations.
To explore this, we propose a personality detection model utilizing GPTs
zero-shot capability to infer Big-Five personality traits from forum
introduction posts, often encouraged in online courses. We benchmark its
performance against established models, demonstrating its efficacy in this
task. Furthermore, we integrate this model into SAMIs entity-based matchmaking
system, enabling personality-informed social recommendations. Initial
integration suggests personality traits can complement existing matching
factors, though additional evaluation is required to determine their full
impact on student engagement and match quality.

</details>


### [173] [Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs](https://arxiv.org/abs/2509.09522)
*Vadim Zadykian,Bruno Andrade,Haithem Afli*

Main category: cs.CL

TL;DR: 研究在职位匹配中利用自监督混合架构结合句子嵌入和知识图谱提升语义文本相关性，分层评估模型性能，结果显示KG增强的SBERT模型在高相关性区域有改善。


<details>
  <summary>Details</summary>
Motivation: 解决简历推荐系统中职位匹配的挑战，处理文本重叠少或有误导性的问题，提升语义对齐和可解释性。

Method: 引入自监督混合架构，结合密集句子嵌入和领域知识图谱，对语义相关性分数进行分层评估，评估多种嵌入模型。

Result: KG增强的微调SBERT模型在高相关性区域显著改善，RMSE比强基线降低25%。

Conclusion: 结合KG和文本嵌入有益，分层性能分析对理解模型行为重要，有助于HR系统的模型选择。

Abstract: Semantic Textual Relatedness (STR) captures nuanced relationships between
texts that extend beyond superficial lexical similarity. In this study, we
investigate STR in the context of job title matching - a key challenge in
resume recommendation systems, where overlapping terms are often limited or
misleading. We introduce a self-supervised hybrid architecture that combines
dense sentence embeddings with domain-specific Knowledge Graphs (KGs) to
improve both semantic alignment and explainability. Unlike previous work that
evaluated models on aggregate performance, our approach emphasizes data
stratification by partitioning the STR score continuum into distinct regions:
low, medium, and high semantic relatedness. This stratified evaluation enables
a fine-grained analysis of model performance across semantically meaningful
subspaces. We evaluate several embedding models, both with and without KG
integration via graph neural networks. The results show that fine-tuned SBERT
models augmented with KGs produce consistent improvements in the high-STR
region, where the RMSE is reduced by 25% over strong baselines. Our findings
highlight not only the benefits of combining KGs with text embeddings, but also
the importance of regional performance analysis in understanding model
behavior. This granular approach reveals strengths and weaknesses hidden by
global metrics, and supports more targeted model selection for use in Human
Resources (HR) systems and applications where fairness, explainability, and
contextual matching are essential.

</details>


### [174] [Steering MoE LLMs via Expert (De)Activation](https://arxiv.org/abs/2509.09660)
*Mohsen Fayyaz,Ali Modarressi,Hanieh Deilamsalehy,Franck Dernoncourt,Ryan Rossi,Trung Bui,Hinrich Schütze,Nanyun Peng*

Main category: cs.CL

TL;DR: 提出SteerMoE框架用于控制大语言模型中MoE的行为，在多个基准测试中提升安全性和忠实度，也揭示了对齐造假问题。


<details>
  <summary>Details</summary>
Motivation: 为了控制MoE模型的行为，如忠实度和安全性，且无需重新训练或修改权重。

Method: 通过检测具有不同激活模式的专家，在推理时选择性激活或停用这些专家。

Result: 在11个基准测试和6个大语言模型中，安全性提升达20%，忠实度提升达27%；在对抗攻击模式下，安全性降低41%，与现有越狱方法结合降低100%。

Conclusion: SteerMoE框架可有效控制MoE模型行为，但也暴露出专家中隐藏的对齐造假新问题。

Abstract: Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token
through a subset of specialized Feed-Forward Networks (FFN), known as experts.
We present SteerMoE, a framework for steering MoE models by detecting and
controlling behavior-linked experts. Our detection method identifies experts
with distinct activation patterns across paired inputs exhibiting contrasting
behaviors. By selectively (de)activating such experts during inference, we
control behaviors like faithfulness and safety without retraining or modifying
weights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to
+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by
-41% alone, and -100% when combined with existing jailbreak methods, bypassing
all safety guardrails and exposing a new dimension of alignment faking hidden
within experts.

</details>


### [175] [CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2509.09675)
*Runpeng Dai,Linfeng Song,Haolin Liu,Zhenwen Liang,Dian Yu,Haitao Mi,Zhaopeng Tu,Rui Liu,Tong Zheng,Hongtu Zhu,Dong Yu*

Main category: cs.CL

TL;DR: 提出CDE框架解决当前RLVR方法探索不足问题，理论分析有优势，实证在AIME基准上有提升并揭示校准崩溃机制。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法探索性差，导致过早收敛和熵崩溃，需要改进。

Method: 引入CDE框架，用演员的困惑度和评论家的多头部架构价值估计方差作为探索奖励引导模型。

Result: 在AIME基准上比标准RLVR用GRPO/PPO提升约3分，发现RLVR内校准崩溃机制。

Conclusion: CDE框架能有效解决RLVR探索性问题，对理解LLM失败模式有帮助。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm
for enhancing the reasoning ability of Large Language Models (LLMs). Yet
current RLVR methods often explore poorly, leading to premature convergence and
entropy collapse. To address this challenge, we introduce Curiosity-Driven
Exploration (CDE), a framework that leverages the model's own intrinsic sense
of curiosity to guide exploration. We formalize curiosity with signals from
both the actor and the critic: for the actor, we use perplexity over its
generated response, and for the critic, we use the variance of value estimates
from a multi-head architecture. Both signals serve as an exploration bonus
within the RLVR framework to guide the model. Our theoretical analysis shows
that the actor-wise bonus inherently penalizes overconfident errors and
promotes diversity among correct responses; moreover, we connect the
critic-wise bonus to the well-established count-based exploration bonus in RL.
Empirically, our method achieves an approximate +3 point improvement over
standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a
calibration collapse mechanism within RLVR, shedding light on common LLM
failure modes.

</details>


### [176] [Fluent but Unfeeling: The Emotional Blind Spots of Language Models](https://arxiv.org/abs/2509.09593)
*Bangzhao Shu,Isha Joshi,Melissa Karnaze,Anh C. Pham,Ishita Kakkar,Sindhu Kothe,Arpine Hovasapian,Mai ElSherief*

Main category: cs.CL

TL;DR: 本文引入EXPRESS基准数据集，评估大语言模型在细粒度情感对齐上的表现，发现大模型在这方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 现有研究在评估大语言模型是否能在细粒度层面与人类情感对齐存在差距，多关注有限的预定义情感分类，忽略了更细微的表达。

Method: 引入EXPRESS基准数据集，采用综合评估框架，将预测的情感术语分解为八种基本情感进行细粒度比较，并在不同提示设置下对流行的大语言模型进行系统测试。

Result: 准确预测与人类自我披露情感一致的情绪仍具挑战性，部分大语言模型虽能生成符合情感理论和定义的情感术语，但在捕捉上下文线索方面不如人类自我披露有效。

Conclusion: 研究凸显了大语言模型在细粒度情感对齐上的局限性，为未来提升其上下文理解能力的研究提供了见解。

Abstract: The versatility of Large Language Models (LLMs) in natural language
understanding has made them increasingly popular in mental health research.
While many studies explore LLMs' capabilities in emotion recognition, a
critical gap remains in evaluating whether LLMs align with human emotions at a
fine-grained level. Existing research typically focuses on classifying emotions
into predefined, limited categories, overlooking more nuanced expressions. To
address this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit
communities featuring 251 fine-grained, self-disclosed emotion labels. Our
comprehensive evaluation framework examines predicted emotion terms and
decomposes them into eight basic emotions using established emotion theories,
enabling a fine-grained comparison. Systematic testing of prevalent LLMs under
various prompt settings reveals that accurately predicting emotions that align
with human self-disclosed emotions remains challenging. Qualitative analysis
further shows that while certain LLMs generate emotion terms consistent with
established emotion theories and definitions, they sometimes fail to capture
contextual cues as effectively as human self-disclosures. These findings
highlight the limitations of LLMs in fine-grained emotion alignment and offer
insights for future research aimed at enhancing their contextual understanding.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [177] [WarpPINN-fibers: improved cardiac strain estimation from cine-MR with physics-informed neural networks](https://arxiv.org/abs/2509.08872)
*Felipe Álvarez Barrientos,Tomás Banduc,Isabeau Sirven,Francisco Sahli Costabal*

Main category: eess.IV

TL;DR: 本文提出WarpPINN - fibers框架，从cine磁共振图像获取受纤维信息增强的心脏运动和应变，实验表明其优于先前模型和其他方法，有望精确量化心脏应变。


<details>
  <summary>Details</summary>
Motivation: 现有从传统成像技术估计应变指标的方法未包含纤维力学，限制了准确解释心脏功能的能力。

Method: 引入WarpPINN - fibers物理信息神经网络框架，构建包含数据相似性损失、近不可压缩正则化项和纤维拉伸惩罚项的损失函数，训练网络以满足超弹性模型并促进纤维收缩。

Result: 在合成体模实验中改进了先前的WarpPINN模型，有效控制纤维拉伸；在cine - MRI基准测试中，在地标跟踪和应变曲线预测方面优于其他方法。

Conclusion: 该方法无需比MRI更复杂的成像技术，可通过与纤维生理学一致的准确变形场实现对心脏应变更精确的量化。

Abstract: The contractile motion of the heart is strongly determined by the
distribution of the fibers that constitute cardiac tissue. Strain analysis
informed with the orientation of fibers allows to describe several pathologies
that are typically associated with impaired mechanics of the myocardium, such
as cardiovascular disease. Several methods have been developed to estimate
strain-derived metrics from traditional imaging techniques. However, the
physical models underlying these methods do not include fiber mechanics,
restricting their capacity to accurately explain cardiac function. In this
work, we introduce WarpPINN-fibers, a physics-informed neural network framework
to accurately obtain cardiac motion and strains enhanced by fiber information.
We train our neural network to satisfy a hyper-elastic model and promote fiber
contraction with the goal to predict the deformation field of the heart from
cine magnetic resonance images. For this purpose, we build a loss function
composed of three terms: a data-similarity loss between the reference and the
warped template images, a regularizer enforcing near-incompressibility of
cardiac tissue and a fiber-stretch penalization that controls strain in the
direction of synthetically produced fibers. We show that our neural network
improves the former WarpPINN model and effectively controls fiber stretch in a
synthetic phantom experiment. Then, we demonstrate that WarpPINN-fibers
outperforms alternative methodologies in landmark-tracking and strain curve
prediction for a cine-MRI benchmark with a cohort of 15 healthy volunteers. We
expect that our method will enable a more precise quantification of cardiac
strains through accurate deformation fields that are consistent with fiber
physiology, without requiring imaging techniques more sophisticated than MRI.

</details>


### [178] [Virtual staining for 3D X-ray histology of bone implants](https://arxiv.org/abs/2509.09235)
*Sarah C. Irvine,Christian Lucas,Diana Krüger,Bianca Guedert,Julian Moosmann,Berit Zeller-Plumhoff*

Main category: eess.IV

TL;DR: 本文将深度学习虚拟染色扩展到X射线领域，用改进的CycleGAN网络从同步辐射微CT扫描生成虚拟染色切片，方法优于基线模型，可生成3D数据集，但仍需更多数据改进。


<details>
  <summary>Details</summary>
Motivation: 三维X射线组织学技术的灰度图像对比度限制了其生化特异性，而深度学习虚拟染色在数字病理学中有应用，因此想将其扩展到X射线领域。

Method: 使用超50对骨植入物样本的微CT和甲苯胺蓝染色组织学配准图像，训练改进的CycleGAN网络，结合像素监督和灰度一致性项，采用实时数据增强进行基于补丁的训练。

Result: 模型在SSIM、PSNR和LPIPS指标上优于Pix2Pix和标准CycleGAN基线，能生成虚拟染色3D数据集，但在描绘植入物降解层时有差异。

Conclusion: 将虚拟染色引入3D X射线成像，为生物医学研究中无标记组织表征提供可扩展途径。

Abstract: Three-dimensional X-ray histology techniques offer a non-invasive alternative
to conventional 2D histology, enabling volumetric imaging of biological tissues
without the need for physical sectioning or chemical staining. However, the
inherent greyscale image contrast of X-ray tomography limits its biochemical
specificity compared to traditional histological stains. Within digital
pathology, deep learning-based virtual staining has demonstrated utility in
simulating stained appearances from label-free optical images. In this study,
we extend virtual staining to the X-ray domain by applying cross-modality image
translation to generate artificially stained slices from
synchrotron-radiation-based micro-CT scans. Using over 50 co-registered image
pairs of micro-CT and toluidine blue-stained histology from bone-implant
samples, we trained a modified CycleGAN network tailored for limited paired
data. Whole slide histology images were downsampled to match the voxel size of
the CT data, with on-the-fly data augmentation for patch-based training. The
model incorporates pixelwise supervision and greyscale consistency terms,
producing histologically realistic colour outputs while preserving
high-resolution structural detail. Our method outperformed Pix2Pix and standard
CycleGAN baselines across SSIM, PSNR, and LPIPS metrics. Once trained, the
model can be applied to full CT volumes to generate virtually stained 3D
datasets, enhancing interpretability without additional sample preparation.
While features such as new bone formation were able to be reproduced, some
variability in the depiction of implant degradation layers highlights the need
for further training data and refinement. This work introduces virtual staining
to 3D X-ray imaging and offers a scalable route for chemically informative,
label-free tissue characterisation in biomedical research.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [179] [The Role of Community Detection Methods in Performance Variations of Graph Mining Tasks](https://arxiv.org/abs/2509.09045)
*Shrabani Ghosh,Erik Saule*

Main category: cs.SI

TL;DR: 研究社区检测算法选择对下游应用性能的影响，提出评估框架，发现算法选择显著影响性能。


<details>
  <summary>Details</summary>
Motivation: 现实中缺乏社区真实信息、无通用标准、从业者难选算法且未考虑对下游任务影响，需研究算法选择对下游应用性能的影响。

Method: 提出能集成多种社区检测方法的框架，系统评估其对下游任务结果的影响。

Result: 对比分析显示特定社区检测算法在某些应用中效果更好。

Conclusion: 社区检测算法的选择会显著影响下游应用的性能。

Abstract: In real-world scenarios, large graphs represent relationships among entities
in complex systems. Mining these large graphs often containing millions of
nodes and edges helps uncover structural patterns and meaningful insights.
Dividing a large graph into smaller subgraphs facilitates complex system
analysis by revealing local information. Community detection extracts clusters
or communities of graphs based on statistical methods and machine learning
models using various optimization techniques. Structure based community
detection methods are more suitable for applying to graphs because they do not
rely heavily on rich node or edge attribute information. The features derived
from these communities can improve downstream graph mining tasks, such as link
prediction and node classification. In real-world applications, we often lack
ground truth community information. Additionally, there is neither a
universally accepted gold standard for community detection nor a single method
that is consistently optimal across diverse applications. In many cases, it is
unclear how practitioners select community detection methods, and choices are
often made without explicitly considering their potential impact on downstream
tasks. In this study, we investigate whether the choice of community detection
algorithm significantly influences the performance of downstream applications.
We propose a framework capable of integrating various community detection
methods to systematically evaluate their effects on downstream task outcomes.
Our comparative analysis reveals that specific community detection algorithms
yield superior results in certain applications, highlighting that method
selection substantially affects performance.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [180] [Convexity of Optimization Curves: Local Sharp Thresholds, Robustness Impossibility, and New Counterexamples](https://arxiv.org/abs/2509.08954)
*Le Duc Hieu*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study when the \emph{optimization curve} of first-order methods -- the
sequence \${f(x\_n)}*{n\ge0}\$ produced by constant-stepsize iterations -- is
convex, equivalently when the forward differences \$f(x\_n)-f(x*{n+1})\$ are
nonincreasing. For gradient descent (GD) on convex \$L\$-smooth functions, the
curve is convex for all stepsizes \$\eta \le 1.75/L\$, and this threshold is
tight. Moreover, gradient norms are nonincreasing for all \$\eta \le 2/L\$, and
in continuous time (gradient flow) the curve is always convex. These results
complement and refine the classical smooth convex optimization toolbox,
connecting discrete and continuous dynamics as well as worst-case analyses.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [181] [An Integrated Open Source Software System for the Generation and Analysis of Subject-Specific Blood Flow Simulation Ensembles](https://arxiv.org/abs/2509.09392)
*Simon Leistikow,Thomas Miro,Adrian Kummerländer,Ali Nahardani,Katja Grün,Markus Franz,Verena Hoerr,Mathias J. Krause,Lars Linsen*

Main category: physics.med-ph

TL;DR: 本文介绍一款结合MRI和CFD的可视化分析工具，通过三个用例展示其有效性，能助力血流动力学分析。


<details>
  <summary>Details</summary>
Motivation: 血流动力学分析对诊断心血管疾病和研究心血管参数至关重要，需结合MRI和CFD进行特定对象的血流模拟分析。

Method: 开发一款交互式、可定制、面向用户的可视化分析工具，适用于CFD和MRI领域，可创建参数多样的模拟集合，通过相似空间的二维嵌入进行模拟和测量的可视化与分析。

Result: 将工具应用于三个实际用例，展示其配置模拟集合和分析血流动力学的能力，与专家评估以增强功能和可用性。

Conclusion: 该工具结合CFD和MRI优势，能更全面理解血流动力学参数，促进对血流动力学生物标志物的准确分析。

Abstract: Background and Objective: Hemodynamic analysis of blood flow through arteries
and veins is critical for diagnosing cardiovascular diseases, such as aneurysms
and stenoses, and for investigating cardiovascular parameters, such as
turbulence and wall shear stress. For subject-specific analyses, the anatomy
and blood flow of the subject can be captured non-invasively using structural
and 4D Magnetic Resonance Imaging (MRI). Computational Fluid Dynamics (CFD), on
the other hand, can be used to generate blood flow simulations by solving the
Navier-Stokes equations. To generate and analyze subject-specific blood flow
simulations, MRI and CFD have to be brought together.
  Methods: We present an interactive, customizable, and user-oriented visual
analysis tool that assists researchers in both medicine and numerical analysis.
Our open-source tool is applicable to domains such as CFD and MRI, and it
facilitates the analysis of simulation results and medical data, especially in
hemodynamic studies. It enables the creation of simulation ensembles with a
high variety of parameters. Furthermore, it allows for the visual and
analytical examination of simulations and measurements through 2D embeddings of
the similarity space.
  Results: To demonstrate the effectiveness of our tool, we applied it to three
real-world use cases, showcasing its ability to configure simulation ensembles
and analyse blood flow dynamics. We evaluated our example cases together with
MRI and CFD experts to further enhance features and increase the usability.
  Conclusions: By combining the strengths of both CFD and MRI, our tool
provides a more comprehensive understanding of hemodynamic parameters,
facilitating more accurate analysis of hemodynamic biomarkers.

</details>


### [182] [Explainable AI for Accelerated Microstructure Imaging: A SHAP-Guided Protocol on the Connectome 2.0 scanner](https://arxiv.org/abs/2509.09513)
*Quentin Uhl,Tommaso Pavan,Julianna Gerold,Kwok-Shing Chan,Yohan Jun,Shohei Fujita,Aneri Bhatt,Yixin Ma,Qiaochu Wang,Hong-Hsi Lee,Susie Y. Huang,Berkin Bilgic,Ileana Jelescu*

Main category: physics.med-ph

TL;DR: 提出Connectome 2.0扫描仪的简化采集方案，用数据驱动框架选特征，验证效果好，14分钟成像且不损失参数保真度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散MRI神经突交换成像模型协议扫描时间长，需缩短时间。

Method: 用可解释人工智能和引导递归特征消除策略，从15特征协议中选8特征子集。

Result: 简化协议参数估计和皮质图与完整协议相当，误差低，重测变异性小，比其他方案更稳健。

Conclusion: 混合优化框架14分钟可行神经突交换成像，支持相关研究，提供通用方法设计采集协议。

Abstract: The diffusion MRI Neurite Exchange Imaging model offers a promising framework
for probing gray matter microstructure by estimating parameters such as
compartment sizes, diffusivities, and inter-compartmental water exchange time.
However, existing protocols require long scan times. This study proposes a
reduced acquisition scheme for the Connectome 2.0 scanner that preserves model
accuracy while substantially shortening scan duration. We developed a
data-driven framework using explainable artificial intelligence with a guided
recursive feature elimination strategy to identify an optimal 8-feature subset
from a 15-feature protocol. The performance of this optimized protocol was
validated in vivo and benchmarked against the full acquisition and alternative
reduction strategies. Parameter accuracy, preservation of anatomical contrast,
and test-retest reproducibility were assessed. The reduced protocol yielded
parameter estimates and cortical maps comparable to the full protocol, with low
estimation errors in synthetic data and minimal impact on test-retest
variability. Compared to theory-driven and heuristic reduction schemes, the
optimized protocol demonstrated superior robustness, reducing the deviation in
water exchange time estimates by over two-fold. In conclusion, this hybrid
optimization framework enables viable imaging of neurite exchange in 14 minutes
without loss of parameter fidelity. This approach supports the broader
application of exchange-sensitive diffusion magnetic resonance imaging in
neuroscience and clinical research, and offers a generalizable method for
designing efficient acquisition protocols in biophysical parameter mapping.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [183] [Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures](https://arxiv.org/abs/2509.08926)
*Waqar Ahmad,Evan Murphy,Vladimir A. Krylov*

Main category: cs.CV

TL;DR: 本文将目标重识别（Re - ID）问题转化为有监督图像相似度任务，采用Siamese网络和新的Beta - SOD异常检测框架，在多个数据集上验证其去噪和重识别效果，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目标重识别方法对标签噪声敏感，易导致性能显著下降，需解决此问题。

Method: 将Re - ID问题重新定义为有监督图像相似度任务，采用Siamese网络架构；提出Beta - SOD异常检测框架，用双分量Beta分布混合模型建模嵌入对余弦相似度分布；结合二元交叉熵、对比和余弦嵌入损失优化特征级相似度学习。

Result: 在CUHK03、Market - 1501和VeRi - 776数据集上验证了Beta - SOD在去噪和重识别任务中的有效性，在不同噪声水平（10 - 30%）下性能优于现有方法。

Conclusion: Beta - SOD方法在有噪声的Re - ID场景中具有鲁棒性和广泛适用性。

Abstract: Object re-identification (Re-ID) methods are highly sensitive to label noise,
which typically leads to significant performance degradation. We address this
challenge by reframing Re-ID as a supervised image similarity task and adopting
a Siamese network architecture trained to capture discriminative pairwise
relationships. Central to our approach is a novel statistical outlier detection
(OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier
Detection), which models the distribution of cosine similarities between
embedding pairs using a two-component Beta distribution mixture model. We
establish a novel identifiability result for mixtures of two Beta
distributions, ensuring that our learning task is well-posed.The proposed OD
step complements the Re-ID architecture combining binary cross-entropy,
contrastive, and cosine embedding losses that jointly optimize feature-level
similarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising
and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and
vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance
compared to the state-of-the-art methods across various noise levels (10-30\%),
demonstrating both robustness and broad applicability in noisy Re-ID scenarios.
The implementation of Beta-SOD is available at:
https://github.com/waqar3411/Beta-SOD

</details>


### [184] [Recurrence Meets Transformers for Universal Multimodal Retrieval](https://arxiv.org/abs/2509.08897)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara*

Main category: cs.CV

TL;DR: 提出统一检索模型ReT - 2支持多模态查询和跨多模态文档集合搜索，评估显示其性能佳且有优势，集成后能提升下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖特定任务微调，局限于单模态查询或文档，无法满足复杂多模态检索任务需求。

Method: 提出ReT - 2模型，利用多层表示和具有LSTM启发门控机制的循环Transformer架构动态整合跨层和跨模态信息。

Result: 在M2KR和M - BEIR基准测试中达到SOTA性能，推理更快、内存使用更少，集成到检索增强生成管道中能提升Encyclopedic - VQA和InfoSeek数据集下游性能。

Conclusion: ReT - 2是有效的多模态检索模型，具有良好性能和应用价值。

Abstract: With the rapid advancement of multimodal retrieval and its application in
LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged.
Existing methods predominantly rely on task-specific fine-tuning of
vision-language models and are limited to single-modality queries or documents.
In this paper, we propose ReT-2, a unified retrieval model that supports
multimodal queries, composed of both images and text, and searches across
multimodal document collections where text and images coexist. ReT-2 leverages
multi-layer representations and a recurrent Transformer architecture with
LSTM-inspired gating mechanisms to dynamically integrate information across
layers and modalities, capturing fine-grained visual and textual details. We
evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different
retrieval configurations. Results demonstrate that ReT-2 consistently achieves
state-of-the-art performance across diverse settings, while offering faster
inference and reduced memory usage compared to prior approaches. When
integrated into retrieval-augmented generation pipelines, ReT-2 also improves
downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source
code and trained models are publicly available at:
https://github.com/aimagelab/ReT-2

</details>


### [185] [PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability](https://arxiv.org/abs/2509.08910)
*Tung Vu,Lam Nguyen,Quynh Dao*

Main category: cs.CV

TL;DR: 本文介绍了一种名为PromptGuard的模块化提示框架，其核心VulnGuard Prompt技术可防止大语言模型生成有害信息，框架含六个核心模块，有数学形式化和理论验证。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在现实应用中会对弱势群体产生有害信息风险，现有安全方法无法从源头预防。

Method: 引入PromptGuard框架，其中VulnGuard Prompt结合真实数据对比学习、GitHub数据、伦理推理和角色提示；采用多目标优化和形式化证明；框架包含六个核心模块，有全面数学形式化。

Result: 框架能通过熵界和帕累托最优实现25 - 30%的分析性伤害减少。

Conclusion: 为系统实证研究建立了数学基础，可实现实时伤害预防。

Abstract: The proliferation of Large Language Models (LLMs) in real-world applications
poses unprecedented risks of generating harmful, biased, or misleading
information to vulnerable populations including LGBTQ+ individuals, single
parents, and marginalized communities. While existing safety approaches rely on
post-hoc filtering or generic alignment techniques, they fail to proactively
prevent harmful outputs at the generation source. This paper introduces
PromptGuard, a novel modular prompting framework with our breakthrough
contribution: VulnGuard Prompt, a hybrid technique that prevents harmful
information generation using real-world data-driven contrastive learning.
VulnGuard integrates few-shot examples from curated GitHub repositories,
ethical chain-of-thought reasoning, and adaptive role-prompting to create
population-specific protective barriers. Our framework employs theoretical
multi-objective optimization with formal proofs demonstrating 25-30% analytical
harm reduction through entropy bounds and Pareto optimality. PromptGuard
orchestrates six core modules: Input Classification, VulnGuard Prompting,
Ethical Principles Integration, External Tool Interaction, Output Validation,
and User-System Interaction, creating an intelligent expert system for
real-time harm prevention. We provide comprehensive mathematical formalization
including convergence proofs, vulnerability analysis using information theory,
and theoretical validation framework using GitHub-sourced datasets,
establishing mathematical foundations for systematic empirical research.

</details>


### [186] [Implicit Neural Representations of Intramyocardial Motion and Strain](https://arxiv.org/abs/2509.09004)
*Andrew Bell,Yan Kit Choi,Steffen Peterson,Andrew King,Muhummad Sohaib Nazir,Alistair Young*

Main category: cs.CV

TL;DR: 提出用基于隐式神经表示（INRs）的方法自动量化心肌运动和应变，在准确性和速度上优于基线模型，适合分析大型数据集。


<details>
  <summary>Details</summary>
Motivation: 自动量化标记MRI的心肌内运动和应变是重要但具有挑战性的任务。

Method: 使用基于学习到的潜在代码的隐式神经表示（INRs）来预测连续的左心室（LV）位移，且无需推理时优化。

Result: 在452个英国生物银行测试案例中，该方法实现了最佳跟踪精度（2.14毫米RMSE），在整体圆周（2.86%）和径向（6.42%）应变方面的综合误差最低，且比最准确的基线模型快约380倍。

Conclusion: 基于INR的模型适合对大型CMR数据集进行准确且可扩展的心肌应变分析。

Abstract: Automatic quantification of intramyocardial motion and strain from tagging
MRI remains an important but challenging task. We propose a method using
implicit neural representations (INRs), conditioned on learned latent codes, to
predict continuous left ventricular (LV) displacement -- without requiring
inference-time optimisation. Evaluated on 452 UK Biobank test cases, our method
achieved the best tracking accuracy (2.14 mm RMSE) and the lowest combined
error in global circumferential (2.86%) and radial (6.42%) strain compared to
three deep learning baselines. In addition, our method is $\sim$380$\times$
faster than the most accurate baseline. These results highlight the suitability
of INR-based models for accurate and scalable analysis of myocardial strain in
large CMR datasets.

</details>


### [187] [SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models](https://arxiv.org/abs/2509.09090)
*Hengyu Fang,Yijiang Liu,Yuan Du,Li Du,Huanrui Yang*

Main category: cs.CV

TL;DR: 本文提出SQAP - VLA框架解决VLA模型计算和内存成本高的问题，实现量化和剪枝，提升效率与性能。


<details>
  <summary>Details</summary>
Motivation: VLA模型计算和内存成本高阻碍实际部署，现有压缩和加速方法无法同时实现量化和剪枝以整体提升效率。

Method: 提出SQAP - VLA框架，共同设计量化和令牌剪枝流程，提出新的量化感知令牌剪枝标准，改进量化器设计。

Result: 应用于标准VLA模型时，显著提升计算效率和推理速度，与原模型相比实现1.93倍加速，平均成功率最多提升4.5%。

Conclusion: SQAP - VLA框架能在提升计算效率和推理速度的同时，成功保留核心模型性能。

Abstract: Vision-Language-Action (VLA) models exhibit unprecedented capabilities for
embodied intelligence. However, their extensive computational and memory costs
hinder their practical deployment. Existing VLA compression and acceleration
approaches conduct quantization or token pruning in an ad-hoc manner but fail
to enable both for a holistic efficiency improvement due to an observed
incompatibility. This work introduces SQAP-VLA, the first structured,
training-free VLA inference acceleration framework that simultaneously enables
state-of-the-art quantization and token pruning. We overcome the
incompatibility by co-designing the quantization and token pruning pipeline,
where we propose new quantization-aware token pruning criteria that work on an
aggressively quantized model while improving the quantizer design to enhance
pruning effectiveness. When applied to standard VLA models, SQAP-VLA yields
significant gains in computational efficiency and inference speed while
successfully preserving core model performance, achieving a $\times$1.93
speedup and up to a 4.5\% average success rate enhancement compared to the
original model.

</details>


### [188] [Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation](https://arxiv.org/abs/2509.09143)
*Yuiko Uchida,Ren Togo,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出用于3D场景的新颖评估指标OSIM，其以物体为中心，更符合人类感知，还重新评估了近期模型。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景评估指标关注整体图像质量，与人类感知存在差异，因此需要以物体为中心的评估指标。

Method: 利用物体检测模型及其特征表示来量化场景中每个物体的“物体性”以实现以物体为中心的评估。

Result: 用户研究表明OSIM比现有指标更符合人类感知，还对OSIM的特征进行了分析，重新评估了近期模型。

Conclusion: OSIM是一种更有效的3D场景评估指标，有助于明确该领域的进展。

Abstract: This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric
for 3D scenes that explicitly focuses on "objects," which are fundamental units
of human visual perception. Existing metrics assess overall image quality,
leading to discrepancies with human perception. Inspired by neuropsychological
insights, we hypothesize that human recognition of 3D scenes fundamentally
involves attention to individual objects. OSIM enables object-centric
evaluations by leveraging an object detection model and its feature
representations to quantify the "objectness" of each object in the scene. Our
user study demonstrates that OSIM aligns more closely with human perception
compared to existing metrics. We also analyze the characteristics of OSIM using
various approaches. Moreover, we re-evaluate recent 3D reconstruction and
generation models under a standardized experimental setup to clarify
advancements in this field. The code is available at
https://github.com/Objectness-Similarity/OSIM.

</details>


### [189] [Video Understanding by Design: How Datasets Shape Architectures and Insights](https://arxiv.org/abs/2509.09151)
*Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

TL;DR: 现有视频理解调查多按任务或模型家族分类，本文首次从数据集驱动视角分析，展示数据集特征对模型的归纳偏置，重新解读模型发展里程碑，提供模型设计指导和发展路线图。


<details>
  <summary>Details</summary>
Motivation: 现有调查忽视数据集对架构演变的结构性压力，需要从数据集驱动视角进行研究。

Method: 采用数据集驱动视角，分析运动复杂性、时间跨度、层次组成和多模态丰富度等数据集特征对模型的归纳偏置。

Result: 重新解读了从双流和3D CNN到顺序、Transformer和多模态基础模型等发展里程碑，提供了模型设计的实用指导。

Conclusion: 将数据集、归纳偏置和架构统一成连贯框架，为通用视频理解提供全面回顾和前瞻性路线图。

Abstract: Video understanding has advanced rapidly, fueled by increasingly complex
datasets and powerful architectures. Yet existing surveys largely classify
models by task or family, overlooking the structural pressures through which
datasets guide architectural evolution. This survey is the first to adopt a
dataset-driven perspective, showing how motion complexity, temporal span,
hierarchical composition, and multimodal richness impose inductive biases that
models should encode. We reinterpret milestones, from two-stream and 3D CNNs to
sequential, transformer, and multimodal foundation models, as concrete
responses to these dataset-driven pressures. Building on this synthesis, we
offer practical guidance for aligning model design with dataset invariances
while balancing scalability and task demands. By unifying datasets, inductive
biases, and architectures into a coherent framework, this survey provides both
a comprehensive retrospective and a prescriptive roadmap for advancing
general-purpose video understanding.

</details>


### [190] [OCELOT 2023: Cell Detection from Cell-Tissue Interaction Challenge](https://arxiv.org/abs/2509.09153)
*JaeWoong Shin,Jeongun Ryu,Aaron Valero Puche,Jinhee Lee,Biagio Brattoli,Wonkyung Jung,Soo Ick Cho,Kyunghyun Paeng,Chan-Young Ock,Donggeun Yoo,Zhaoyang Li,Wangkai Li,Huayu Mai,Joshua Millward,Zhen He,Aiden Nibali,Lydia Anette Schoenpflug,Viktor Hendrik Koelzer,Xu Shuoyu,Ji Zheng,Hu Bin,Yu-Wen Lo,Ching-Hui Yang,Sérgio Pereira*

Main category: cs.CV

TL;DR: 现有深度学习细胞检测模型难以学习不同放大倍数结构间语义，OCELOT 2023 挑战收集多尺度标注数据集，参与者模型提升了对细胞 - 组织关系的理解，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习细胞检测模型难以复制病理学家切换放大倍数的行为，缺乏多尺度重叠细胞和组织标注数据集，验证理解细胞 - 组织相互作用对实现人类水平性能的重要性并加速该领域研究。

Method: 举办 OCELOT 2023 挑战，提供包含六个器官重叠细胞检测和组织分割标注的数据集，参与者提出模型。

Result: 参与者模型显著提升了对细胞 - 组织关系的理解，顶级模型在测试集上 F1 分数比仅考虑细胞的基线模型提高 7.99。

Conclusion: 将多尺度语义纳入模型是必要的，论文对参与者方法进行比较分析，突出挑战中的创新策略。

Abstract: Pathologists routinely alternate between different magnifications when
examining Whole-Slide Images, allowing them to evaluate both broad tissue
morphology and intricate cellular details to form comprehensive diagnoses.
However, existing deep learning-based cell detection models struggle to
replicate these behaviors and learn the interdependent semantics between
structures at different magnifications. A key barrier in the field is the lack
of datasets with multi-scale overlapping cell and tissue annotations. The
OCELOT 2023 challenge was initiated to gather insights from the community to
validate the hypothesis that understanding cell and tissue (cell-tissue)
interactions is crucial for achieving human-level performance, and to
accelerate the research in this field. The challenge dataset includes
overlapping cell detection and tissue segmentation annotations from six organs,
comprising 673 pairs sourced from 306 The Cancer Genome Atlas (TCGA)
Whole-Slide Images with hematoxylin and eosin staining, divided into training,
validation, and test subsets. Participants presented models that significantly
enhanced the understanding of cell-tissue relationships. Top entries achieved
up to a 7.99 increase in F1-score on the test set compared to the baseline
cell-only model that did not incorporate cell-tissue relationships. This is a
substantial improvement in performance over traditional cell-only detection
methods, demonstrating the need for incorporating multi-scale semantics into
the models. This paper provides a comparative analysis of the methods used by
participants, highlighting innovative strategies implemented in the OCELOT 2023
challenge.

</details>


### [191] [A Knowledge Noise Mitigation Framework for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2509.09159)
*Zhiyue Liu,Sihang Liu,Jinyuan Liu,Xinru Zhang*

Main category: cs.CV

TL;DR: 提出无训练知识聚焦框架用于KB - VQA，能获取准确关键知识，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有KB - VQA方法直接用检索信息增强模型，忽略知识冗余，引入噪声。

Method: 创建低噪声查询增强知识检索相关性；让大模型提取答案有益知识片段；引入选择性知识集成策略，仅在模型回答信心不足时融入知识。

Result: 框架能获取准确和关键知识，大量实验表明优于现有方法。

Conclusion: 所提训练无框架有效，可用于解决KB - VQA中知识冗余带来的噪声问题。

Abstract: Knowledge-based visual question answering (KB-VQA) requires a model to
understand images and utilize external knowledge to provide accurate answers.
Existing approaches often directly augment models with retrieved information
from knowledge sources while ignoring substantial knowledge redundancy, which
introduces noise into the answering process. To address this, we propose a
training-free framework with knowledge focusing for KB-VQA, that mitigates the
impact of noise by enhancing knowledge relevance and reducing redundancy.
First, for knowledge retrieval, our framework concludes essential parts from
the image-question pairs, creating low-noise queries that enhance the retrieval
of highly relevant knowledge. Considering that redundancy still persists in the
retrieved knowledge, we then prompt large models to identify and extract
answer-beneficial segments from knowledge. In addition, we introduce a
selective knowledge integration strategy, allowing the model to incorporate
knowledge only when it lacks confidence in answering the question, thereby
mitigating the influence of redundant information. Our framework enables the
acquisition of accurate and critical knowledge, and extensive experiments
demonstrate that it outperforms state-of-the-art methods.

</details>


### [192] [Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection](https://arxiv.org/abs/2509.09183)
*Jiasheng Guo,Xin Gao,Yuxiang Yan,Guanghao Li,Jian Pu*

Main category: cs.CV

TL;DR: 提出轻量级自适应ISP插件Dark - ISP处理低光环境下的Bayer RAW图像用于目标检测，实验显示其在低光环境下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 低光目标检测因图像质量下降而具有挑战性，现有RAW图像检测方法存在信息损失或框架复杂的问题。

Method: 将传统ISP管道解构为线性和非线性子模块，作为可微组件通过任务驱动损失优化，各模块有内容感知适应性和物理先验；设计Self - Boost机制促进子模块合作。

Result: 在三个RAW图像数据集上的实验表明，该方法在低光环境下以最少参数优于基于RGB和RAW的检测方法。

Conclusion: 提出的Dark - ISP方法能有效处理低光环境下的目标检测问题，性能优越且参数少。

Abstract: Low-light Object detection is crucial for many real-world applications but
remains challenging due to degraded image quality. While recent studies have
shown that RAW images offer superior potential over RGB images, existing
approaches either use RAW-RGB images with information loss or employ complex
frameworks. To address these, we propose a lightweight and self-adaptive Image
Signal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW
images in dark environments, enabling seamless end-to-end training for object
detection. Our key innovations are: (1) We deconstruct conventional ISP
pipelines into sequential linear (sensor calibration) and nonlinear (tone
mapping) sub-modules, recasting them as differentiable components optimized
through task-driven losses. Each module is equipped with content-aware
adaptability and physics-informed priors, enabling automatic RAW-to-RGB
conversion aligned with detection objectives. (2) By exploiting the ISP
pipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that
facilitates cooperation between sub-modules. Through extensive experiments on
three RAW image datasets, we demonstrate that our method outperforms
state-of-the-art RGB- and RAW-based detection approaches, achieving superior
results with minimal parameters in challenging low-light environments.

</details>


### [193] [CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for Gastric Tissue Classification](https://arxiv.org/abs/2509.09242)
*Mustafa Yurdakul,Sakir Tasdemir*

Main category: cs.CV

TL;DR: 本文提出用于胃组织图像分类的CoAtNeXt模型，在两个公开数据集上表现优异，可辅助病理学家诊断。


<details>
  <summary>Details</summary>
Motivation: 传统胃疾病组织病理检查全手动，劳动强度大、易有差异，需自动化可靠高效的胃组织分析方法。

Method: 提出CoAtNeXt模型，基于CoAtNet架构，用增强的ConvNeXtV2块替换MBConv层，集成CBAM模块，对架构进行缩放；在两个公开数据集上评估，并与20个模型对比。

Result: CoAtNeXt在两个数据集上各项指标表现出色，超越所有测试的CNN和ViT模型及以往研究。

Conclusion: CoAtNeXt是用于胃组织图像组织病理学分类的强大架构，能提高诊断准确性和减轻工作量。

Abstract: Background and objective Early diagnosis of gastric diseases is crucial to
prevent fatal outcomes. Although histopathologic examination remains the
diagnostic gold standard, it is performed entirely manually, making evaluations
labor-intensive and prone to variability among pathologists. Critical findings
may be missed, and lack of standard procedures reduces consistency. These
limitations highlight the need for automated, reliable, and efficient methods
for gastric tissue analysis. Methods In this study, a novel hybrid model named
CoAtNeXt was proposed for the classification of gastric tissue images. The
model is built upon the CoAtNet architecture by replacing its MBConv layers
with enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block
Attention Module (CBAM) is integrated to improve local feature extraction
through channel and spatial attention mechanisms. The architecture was scaled
to achieve a balance between computational efficiency and classification
performance. CoAtNeXt was evaluated on two publicly available datasets,
HMU-GC-HE-30K for eight-class classification and GasHisSDB for binary
classification, and was compared against 10 Convolutional Neural Networks
(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved
96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%
AUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%
precision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all
CNN and ViT models tested and surpassed previous studies in the literature.
Conclusion Experimental results show that CoAtNeXt is a robust architecture for
histopathological classification of gastric tissue images, providing
performance on binary and multiclass. Its highlights its potential to assist
pathologists by enhancing diagnostic accuracy and reducing workload.

</details>


### [194] [Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in Multimodal MRI with Sequences Unavailable During Training](https://arxiv.org/abs/2509.09290)
*Anthony P. Addison,Felix Wagner,Wentian Xu,Natalie Voets,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: 本文提出改进U-net架构，结合模态无关与特定通道，用图像增强方案训练，能处理训练时未见模态，在多数据库验证有效。


<details>
  <summary>Details</summary>
Motivation: 多数多模态脑MRI分割模型只能处理固定模态，部分模型泛化时丢失特定信息，需开发能处理训练时未见模态的模型。

Method: 改进U-net架构，集成模态无关输入通道和模态特定输入通道；开发图像增强方案合成人工MRI模态。

Result: 该方法能有效处理训练时遇到的MRI模态，也能处理新的未见模态以改进分割。

Conclusion: 通过简单实用的U-net架构改动和图像增强方案，可实现对训练时未见模态数据的推理。

Abstract: Segmentation models are important tools for the detection and analysis of
lesions in brain MRI. Depending on the type of brain pathology that is imaged,
MRI scanners can acquire multiple, different image modalities (contrasts). Most
segmentation models for multimodal brain MRI are restricted to fixed modalities
and cannot effectively process new ones at inference. Some models generalize to
unseen modalities but may lose discriminative modality-specific information.
This work aims to develop a model that can perform inference on data that
contain image modalities unseen during training, previously seen modalities,
and heterogeneous combinations of both, thus allowing a user to utilize any
available imaging modalities. We demonstrate this is possible with a simple,
thus practical alteration to the U-net architecture, by integrating a
modality-agnostic input channel or pathway, alongside modality-specific input
channels. To train this modality-agnostic component, we develop an image
augmentation scheme that synthesizes artificial MRI modalities. Augmentations
differentially alter the appearance of pathological and healthy brain tissue to
create artificial contrasts between them while maintaining realistic anatomical
integrity. We evaluate the method using 8 MRI databases that include 5 types of
pathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and
white matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI,
DWI, ADC and FLAIR). The results demonstrate that the approach preserves the
ability to effectively process MRI modalities encountered during training,
while being able to process new, unseen modalities to improve its segmentation.
Project code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG

</details>


### [195] [Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable UAV Perception](https://arxiv.org/abs/2509.09297)
*Spyridon Loukovitis,Anastasios Arsenos,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: 提出用于基于嵌入的探测器的模型无关的开放集检测框架，在基准测试和实际飞行测试中验证，相比基线方法有改进，适合无人机感知。


<details>
  <summary>Details</summary>
Motivation: 传统闭集探测器在领域偏移和飞行数据损坏时性能下降，对安全关键应用有风险，需要可靠的开放集检测方法用于无人机自主空中目标检测。

Method: 提出模型无关的开放集检测框架，在嵌入空间通过熵建模估计语义不确定性，结合谱归一化和温度缩放增强开放集辨别能力。

Result: 在AOT空中基准和大量实际飞行测试中验证，消融研究显示比基线方法有持续改进，相对AUROC增益达10%，背景剔除增强鲁棒性且不影响检测精度。

Conclusion: 该解决方案适用于动态空对空环境中可靠的无人机感知。

Abstract: Open-set detection is crucial for robust UAV autonomy in air-to-air object
detection under real-world conditions. Traditional closed-set detectors degrade
significantly under domain shifts and flight data corruption, posing risks to
safety-critical applications. We propose a novel, model-agnostic open-set
detection framework designed specifically for embedding-based detectors. The
method explicitly handles unknown object rejection while maintaining robustness
against corrupted flight data. It estimates semantic uncertainty via entropy
modeling in the embedding space and incorporates spectral normalization and
temperature scaling to enhance open-set discrimination. We validate our
approach on the challenging AOT aerial benchmark and through extensive
real-world flight tests. Comprehensive ablation studies demonstrate consistent
improvements over baseline methods, achieving up to a 10\% relative AUROC gain
compared to standard YOLO-based detectors. Additionally, we show that
background rejection further strengthens robustness without compromising
detection accuracy, making our solution particularly well-suited for reliable
UAV perception in dynamic air-to-air environments.

</details>


### [196] [Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization](https://arxiv.org/abs/2509.09307)
*Zhengzhao Lai,Youbin Zheng,Zhenyang Cai,Haonan Lyu,Jinpu Yang,Hongqing Liang,Yan Hu,Benyou Wang*

Main category: cs.CV

TL;DR: 提出首个材料表征图像理解基准MatCha，评估显示现有MLLMs在该基准上与人类专家有差距，适应现实场景能力有限。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在理解现实世界材料表征成像数据方面研究不足，需填补这一空白。

Method: 创建包含1500个需要专家级领域知识问题的MatCha基准，涵盖材料研究四个关键阶段21个不同任务。

Result: 对最先进MLLMs在MatCha上的评估显示，与人类专家存在显著性能差距，处理高难度问题时表现不佳，简单提示方法难以解决。

Conclusion: 现有MLLMs对现实世界材料表征场景适应性有限，希望MatCha能推动新材料发现等领域研究。

Abstract: Materials characterization is fundamental to acquiring materials information,
revealing the processing-microstructure-property relationships that guide
material design and optimization. While multimodal large language models
(MLLMs) have recently shown promise in generative and predictive tasks within
materials science, their capacity to understand real-world characterization
imaging data remains underexplored. To bridge this gap, we present MatCha, the
first benchmark for materials characterization image understanding, comprising
1,500 questions that demand expert-level domain expertise. MatCha encompasses
four key stages of materials research comprising 21 distinct tasks, each
designed to reflect authentic challenges faced by materials scientists. Our
evaluation of state-of-the-art MLLMs on MatCha reveals a significant
performance gap compared to human experts. These models exhibit degradation
when addressing questions requiring higher-level expertise and sophisticated
visual perception. Simple few-shot and chain-of-thought prompting struggle to
alleviate these limitations. These findings highlight that existing MLLMs still
exhibit limited adaptability to real-world materials characterization
scenarios. We hope MatCha will facilitate future research in areas such as new
material discovery and autonomous scientific agents. MatCha is available at
https://github.com/FreedomIntelligence/MatCha.

</details>


### [197] [Exploring Pre-training Across Domains for Few-Shot Surgical Skill Assessment](https://arxiv.org/abs/2509.09327)
*Dimitrios Anastasiou,Razvan Caramalau,Nazir Sirajudeen,Matthew Boal,Philip Edwards,Justin Collins,John Kelly,Ashwin Sridhar,Maxine Tran,Faiz Mumtaz,Nevil Pavithran,Nader Francis,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: 本文将自动手术技能评估（SSA）作为少样本任务，研究自监督预训练策略对下游少样本SSA性能的影响，发现小而相关的数据集表现更好，特定程序数据的加入对性能有不同影响。


<details>
  <summary>Details</summary>
Motivation: 由于技能标注稀缺，开发强大的SSA模型具有挑战性，少样本学习（FSL）需有效预训练，而SSA中预训练研究较少，因此研究自监督预训练策略对下游少样本SSA性能的影响。

Method: 将SSA表述为少样本任务，用OSATS分数标注公开数据集，在三种少样本设置下评估各种预训练源，量化领域相似性并分析领域差距和特定程序数据的影响。

Result: 小而领域相关的数据集表现优于大规模但不太匹配的数据集，在1、2和5样本设置下准确率分别达60.16%、66.03%和73.65%；加入特定程序数据，在相关外部数据集上提升性能，在不太相似的大规模源上可能导致性能下降。

Conclusion: 小而领域相关的数据集更适合少样本SSA，加入特定程序数据对性能的影响取决于预训练源的领域相似性。

Abstract: Automated surgical skill assessment (SSA) is a central task in surgical
computer vision. Developing robust SSA models is challenging due to the
scarcity of skill annotations, which are time-consuming to produce and require
expert consensus. Few-shot learning (FSL) offers a scalable alternative
enabling model development with minimal supervision, though its success
critically depends on effective pre-training. While widely studied for several
surgical downstream tasks, pre-training has remained largely unexplored in SSA.
In this work, we formulate SSA as a few-shot task and investigate how
self-supervised pre-training strategies affect downstream few-shot SSA
performance. We annotate a publicly available robotic surgery dataset with
Objective Structured Assessment of Technical Skill (OSATS) scores, and evaluate
various pre-training sources across three few-shot settings. We quantify domain
similarity and analyze how domain gap and the inclusion of procedure-specific
data into pre-training influence transferability. Our results show that small
but domain-relevant datasets can outperform large scale, less aligned ones,
achieving accuracies of 60.16%, 66.03%, and 73.65% in the 1-, 2-, and 5-shot
settings, respectively. Moreover, incorporating procedure-specific data into
pre-training with a domain-relevant external dataset significantly boosts
downstream performance, with an average gain of +1.22% in accuracy and +2.28%
in F1-score; however, applying the same strategy with less similar but
large-scale sources can instead lead to performance degradation. Code and
models are available at https://github.com/anastadimi/ssa-fsl.

</details>


### [198] [Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles](https://arxiv.org/abs/2509.09349)
*Ian Nell,Shane Gilroy*

Main category: cs.CV

TL;DR: 本文介绍了一种新的驾驶员行为分类系统，用计算机视觉方法检测分心和受损驾驶行为，实验证明其可靠且适应不同条件。


<details>
  <summary>Details</summary>
Motivation: 道路交通事故是全球重要问题，人为错误尤其是分心和受损驾驶是主要原因，需要检测这些行为。

Method: 采用先进计算机视觉方法，包括实时目标跟踪、横向位移分析和车道位置监测，使用YOLO目标检测模型和自定义车道估计算法。

Result: 在不同视频数据集上的实验评估表明，该框架在不同道路和环境条件下具有可靠性和适应性。

Conclusion: 所提出的基于视觉的驾驶员行为分类系统能有效检测不安全驾驶行为，且可用于非联网车辆。

Abstract: Road traffic accidents remain a significant global concern, with human error,
particularly distracted and impaired driving, among the leading causes. This
study introduces a novel driver behavior classification system that uses
external observation techniques to detect indicators of distraction and
impairment. The proposed framework employs advanced computer vision
methodologies, including real-time object tracking, lateral displacement
analysis, and lane position monitoring. The system identifies unsafe driving
behaviors such as excessive lateral movement and erratic trajectory patterns by
implementing the YOLO object detection model and custom lane estimation
algorithms. Unlike systems reliant on inter-vehicular communication, this
vision-based approach enables behavioral analysis of non-connected vehicles.
Experimental evaluations on diverse video datasets demonstrate the framework's
reliability and adaptability across varying road and environmental conditions.

</details>


### [199] [Semantic Concentration for Self-Supervised Dense Representations Learning](https://arxiv.org/abs/2509.09429)
*Peisong Wen,Qianqian Xu,Siran Dai,Runmin Cong,Qingming Huang*

Main category: cs.CV

TL;DR: 现有图像级自监督学习在学习图像块密集表示上有挑战，本文提出显式语义集中方法用于密集自监督学习，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 主流方法存在过分散现象影响密集任务下游性能，图像级自监督学习的隐式语义集中方法因空间敏感性和复杂场景数据不适用于密集自监督学习，需探索显式语义集中方法。

Method: 提出蒸馏图像块对应关系以打破严格空间对齐，提出噪声容忍排序损失处理噪声和不平衡伪标签；提出对象感知过滤器将输出空间映射到基于对象的空间。

Result: 在各种任务的实证研究中证明了方法的有效性。

Conclusion: 所提显式语义集中方法能有效解决密集自监督学习中学习图像块密集表示的问题。

Abstract: Recent advances in image-level self-supervised learning (SSL) have made
significant progress, yet learning dense representations for patches remains
challenging. Mainstream methods encounter an over-dispersion phenomenon that
patches from the same instance/category scatter, harming downstream performance
on dense tasks. This work reveals that image-level SSL avoids over-dispersion
by involving implicit semantic concentration. Specifically, the non-strict
spatial alignment ensures intra-instance consistency, while shared patterns,
i.e., similar parts of within-class instances in the input space, ensure
inter-image consistency. Unfortunately, these approaches are infeasible for
dense SSL due to their spatial sensitivity and complicated scene-centric data.
These observations motivate us to explore explicit semantic concentration for
dense SSL. First, to break the strict spatial alignment, we propose to distill
the patch correspondences. Facing noisy and imbalanced pseudo labels, we
propose a noise-tolerant ranking loss. The core idea is extending the Average
Precision (AP) loss to continuous targets, such that its decision-agnostic and
adaptive focusing properties prevent the student model from being misled.
Second, to discriminate the shared patterns from complicated scenes, we propose
the object-aware filter to map the output space to an object-based space.
Specifically, patches are represented by learnable prototypes of objects via
cross-attention. Last but not least, empirical studies across various tasks
soundly support the effectiveness of our method. Code is available in
https://github.com/KID-7391/CoTAP.

</details>


### [200] [OpenFake: An Open Dataset and Platform Toward Large-Scale Deepfake Detection](https://arxiv.org/abs/2509.09495)
*Victor Livernoche,Akshatha Arodi,Andreea Musulan,Zachary Yang,Adam Salvail,Gaétan Marceau Caron,Jean-François Godbout,Reihaneh Rabbany*

Main category: cs.CV

TL;DR: 文章指出现有深度伪造检测数据集有局限，提出政治聚焦数据集及众包对抗平台保障公众话语免受虚假信息威胁。


<details>
  <summary>Details</summary>
Motivation: 深度伪造加剧虚假信息传播，现有检测数据集存在缺陷，难以有效检测合成图像。

Method: 分析社交媒体帖子确定深度伪造传播虚假信息的模式；开展人类感知研究；构建政治聚焦数据集；引入众包对抗平台。

Result: 构建了含三百万真实图像及对应描述的数据集，生成963k高质量合成图像；众包对抗平台保障检测方法的鲁棒性和适应性。

Conclusion: 提出的数据集和众包对抗平台有助于提高深度伪造检测能力，保护公众话语免受复杂虚假信息威胁。

Abstract: Deepfakes, synthetic media created using advanced AI techniques, have
intensified the spread of misinformation, particularly in politically sensitive
contexts. Existing deepfake detection datasets are often limited, relying on
outdated generation methods, low realism, or single-face imagery, restricting
the effectiveness for general synthetic image detection. By analyzing social
media posts, we identify multiple modalities through which deepfakes propagate
misinformation. Furthermore, our human perception study demonstrates that
recently developed proprietary models produce synthetic images increasingly
indistinguishable from real ones, complicating accurate identification by the
general public. Consequently, we present a comprehensive, politically-focused
dataset specifically crafted for benchmarking detection against modern
generative models. This dataset contains three million real images paired with
descriptive captions, which are used for generating 963k corresponding
high-quality synthetic images from a mix of proprietary and open-source models.
Recognizing the continual evolution of generative techniques, we introduce an
innovative crowdsourced adversarial platform, where participants are
incentivized to generate and submit challenging synthetic images. This ongoing
community-driven initiative ensures that deepfake detection methods remain
robust and adaptive, proactively safeguarding public discourse from
sophisticated misinformation threats.

</details>


### [201] [Resource-Efficient Glioma Segmentation on Sub-Saharan MRI](https://arxiv.org/abs/2509.09469)
*Freedmore Sidume,Oumayma Soula,Joseph Muthui Wacira,YunFei Zhu,Abbas Rabiu Muhammad,Abderrazek Zeraii,Oluwaseun Kalejaye,Hajer Ibrahim,Olfa Gaddour,Brain Halubanza,Dong Zhang,Udunna C Anazodo,Confidence Raymond*

Main category: cs.CV

TL;DR: 本文提出适用于资源受限环境的深度学习框架，用3D Attention UNet架构结合迁移学习，在有限数据下取得不错分割效果，证明模型泛化性和实用性。


<details>
  <summary>Details</summary>
Motivation: 撒哈拉以南非洲高质量标注影像数据稀缺，难以在临床工作流中部署先进分割模型。

Method: 采用带残差块的3D Attention UNet架构，利用BraTS 2021数据集预训练权重进行迁移学习。

Result: 在BraTS - Africa数据集95例MRI上评估，增强肿瘤、坏死和非增强肿瘤核心、周围非功能半球的Dice分数分别达0.76、0.80、0.85，模型约90MB，消费级硬件推理时间不到1分钟。

Conclusion: 模型具有泛化性，能支持低资源环境临床决策，有助于缩小全球健康领域AI应用差距。

Abstract: Gliomas are the most prevalent type of primary brain tumors, and their
accurate segmentation from MRI is critical for diagnosis, treatment planning,
and longitudinal monitoring. However, the scarcity of high-quality annotated
imaging data in Sub-Saharan Africa (SSA) poses a significant challenge for
deploying advanced segmentation models in clinical workflows. This study
introduces a robust and computationally efficient deep learning framework
tailored for resource-constrained settings. We leveraged a 3D Attention UNet
architecture augmented with residual blocks and enhanced through transfer
learning from pre-trained weights on the BraTS 2021 dataset. Our model was
evaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for glioma
segmentation in SSA MRI data. Despite the limited data quality and quantity,
our approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80
for Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding
Non-Functional Hemisphere (SNFH). These results demonstrate the
generalizability of the proposed model and its potential to support clinical
decision making in low-resource settings. The compact architecture,
approximately 90 MB, and sub-minute per-volume inference time on consumer-grade
hardware further underscore its practicality for deployment in SSA health
systems. This work contributes toward closing the gap in equitable AI for
global health by empowering underserved regions with high-performing and
accessible medical imaging solutions.

</details>


### [202] [Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders](https://arxiv.org/abs/2509.09547)
*Dohun Lee,Hyeonho Jeong,Jiwook Kim,Duygu Ceylan,Jong Chul Ye*

Main category: cs.CV

TL;DR: 本文提出Align4Gen方法，将预训练视觉编码器特征与视频生成器中间特征对齐，提升视频扩散模型的视频生成效果。


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散模型在提升特征表示能力上受关注较少，本文希望通过将视频生成器中间特征与预训练视觉编码器特征对齐来改进模型训练。

Method: 提出新指标分析视觉编码器，评估其适用性；提出Align4Gen多特征融合和对齐方法并集成到视频扩散模型训练中。

Result: 在无条件和有条件视频生成任务中，Align4Gen经多种指标量化显示提升了视频生成效果。

Conclusion: 训练视频扩散模型时将视频生成器中间特征与预训练视觉编码器特征对齐是有益的，Align4Gen方法有效。

Abstract: Video diffusion models have advanced rapidly in the recent years as a result
of series of architectural innovations (e.g., diffusion transformers) and use
of novel training objectives (e.g., flow matching). In contrast, less attention
has been paid to improving the feature representation power of such models. In
this work, we show that training video diffusion models can benefit from
aligning the intermediate features of the video generator with feature
representations of pre-trained vision encoders. We propose a new metric and
conduct an in-depth analysis of various vision encoders to evaluate their
discriminability and temporal consistency, thereby assessing their suitability
for video feature alignment. Based on the analysis, we present Align4Gen which
provides a novel multi-feature fusion and alignment method integrated into
video diffusion model training. We evaluate Align4Gen both for unconditional
and class-conditional video generation tasks and show that it results in
improved video generation as quantified by various metrics. Full video results
are available on our project page: https://align4gen.github.io/align4gen/

</details>


### [203] [Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in MRI-based Alzheimer's Disease Classification](https://arxiv.org/abs/2509.09558)
*Akshit Achara,Esther Puyol Anton,Alexander Hammers,Andrew P. King*

Main category: cs.CV

TL;DR: 研究基于MRI的深度学习阿尔茨海默病诊断中捷径学习和人口统计学偏差，证实其存在，为公平诊断工具奠基。


<details>
  <summary>Details</summary>
Motivation: 深度学习算法在基于MRI的阿尔茨海默病诊断中可能存在捷径学习和对弱势群体的性能偏差，需探究此问题。

Method: 先研究算法能否从3D脑MRI扫描中识别种族或性别，再研究训练集不平衡是否导致模型性能下降，最后对不同脑区特征归因进行定量和定性分析，使用多数据集和DL模型。

Result: 证实基于种族和性别的捷径学习和偏差在基于DL的AD分类中存在。

Conclusion: 为脑MRI中更公平的DL诊断工具奠定基础。

Abstract: Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep
learning (DL) algorithms have been proposed to aid in the diagnosis of diseases
such as Alzheimer's disease (AD) from MRI scans. However, DL algorithms can
suffer from shortcut learning, in which spurious features, not directly related
to the output label, are used for prediction. When these features are related
to protected attributes, they can lead to performance bias against
underrepresented protected groups, such as those defined by race and sex. In
this work, we explore the potential for shortcut learning and demographic bias
in DL based AD diagnosis from MRI. We first investigate if DL algorithms can
identify race or sex from 3D brain MRI scans to establish the presence or
otherwise of race and sex based distributional shifts. Next, we investigate
whether training set imbalance by race or sex can cause a drop in model
performance, indicating shortcut learning and bias. Finally, we conduct a
quantitative and qualitative analysis of feature attributions in different
brain regions for both the protected attribute and AD classification tasks.
Through these experiments, and using multiple datasets and DL models (ResNet
and SwinTransformer), we demonstrate the existence of both race and sex based
shortcut learning and bias in DL based AD classification. Our work lays the
foundation for fairer DL diagnostic tools in brain MRI. The code is provided at
https://github.com/acharaakshit/ShortMR

</details>


### [204] [Mechanistic Learning with Guided Diffusion Models to Predict Spatio-Temporal Brain Tumor Growth](https://arxiv.org/abs/2509.09610)
*Daria Laslo,Efthymios Georgiou,Marius George Linguraru,Andreas Rauschecker,Sabine Muller,Catherine R. Jutzeler,Sarah Bruningk*

Main category: cs.CV

TL;DR: 提出混合机械学习框架结合肿瘤生长模型与DDIM从先前扫描合成未来MRI，在数据集上训练评估，生成逼真后续扫描并引入肿瘤生长概率图。


<details>
  <summary>Details</summary>
Motivation: 预测脑肿瘤时空进展对神经肿瘤学临床决策至关重要。

Method: 提出混合机械学习框架，用常微分方程系统的机械模型捕捉肿瘤时间动态，估计未来肿瘤负荷，用其条件化梯度引导的DDIM进行图像合成。

Result: 在数据集上训练评估，框架能基于空间相似性指标生成逼真后续扫描，引入的肿瘤生长概率图能体现肿瘤生长范围和方向。

Conclusion: 该方法可在数据有限场景进行生物信息图像生成，提供考虑机械先验的时空预测。

Abstract: Predicting the spatio-temporal progression of brain tumors is essential for
guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic
learning framework that combines a mathematical tumor growth model with a
guided denoising diffusion implicit model (DDIM) to synthesize anatomically
feasible future MRIs from preceding scans. The mechanistic model, formulated as
a system of ordinary differential equations, captures temporal tumor dynamics
including radiotherapy effects and estimates future tumor burden. These
estimates condition a gradient-guided DDIM, enabling image synthesis that
aligns with both predicted growth and patient anatomy. We train our model on
the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices
of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our
framework generates realistic follow-up scans based on spatial similarity
metrics. It also introduces tumor growth probability maps, which capture both
clinically relevant extent and directionality of tumor growth as shown by 95th
percentile Hausdorff Distance. The method enables biologically informed image
generation in data-limited scenarios, offering generative-space-time
predictions that account for mechanistic priors.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [205] [Isogeometric Topology Optimization Based on Topological Derivatives](https://arxiv.org/abs/2509.09236)
*Guilherme Henrique Teixeira,Nepomuk Krenn,Peter Gangl,Benjamin Marussig*

Main category: math.NA

TL;DR: 提出基于拓扑导数的等几何拓扑优化方法，结合水平集和浸入等几何框架无需重新网格划分，通过数值算例研究高阶基函数影响并得出结论。


<details>
  <summary>Details</summary>
Motivation: 拓扑优化中拓扑变化的重新网格划分具有挑战性，需寻求无需重新网格划分的方法。

Method: 提出基于拓扑导数的等几何拓扑优化方法，结合水平集方法和浸入等几何框架，研究高阶基函数在水平集表示和求解近似中的影响。

Result: 两个数值算例表明，用高阶基函数近似解可提高精度，线性基函数用于水平集函数表示已足够。

Conclusion: 该方法可实现无缝几何更新且无需重新网格划分，不同场景下基函数选择有相应结论。

Abstract: Topology optimization is a valuable tool in engineering, facilitating the
design of optimized structures. However, topological changes often require a
remeshing step, which can become challenging. In this work, we propose an
isogeometric approach to topology optimization driven by topological
derivatives. The combination of a level-set method together with an immersed
isogeometric framework allows seamless geometry updates without the necessity
of remeshing. At the same time, topological derivatives provide topological
modifications without the need to define initial holes [7]. We investigate the
influence of higher-degree basis functions in both the level-set representation
and the approximation of the solution. Two numerical examples demonstrate the
proposed approach, showing that employing higher-degree basis functions for
approximating the solution improves accuracy, while linear basis functions
remain sufficient for the level-set function representation.

</details>


### [206] [Expressive Power of Deep Networks on Manifolds: Simultaneous Approximation](https://arxiv.org/abs/2509.09362)
*Hanfei Zhou,Lei Shi*

Main category: math.NA

TL;DR: 本文建立了流形上深度神经网络的同时逼近理论，给出逼近误差和参数数量关系，有上界和下界结果，为流形上学习偏微分方程提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 解决科学机器学习中在复杂域上求解偏微分方程的挑战，因流形弯曲几何使函数及其导数逼近复杂。

Method: 证明有界权重的恒定深度ReLU^{k - 1}网络能逼近Sobolev空间中的函数，给出参数数量，还给出匹配的下界，引入对网络高阶导数类的Vapnik - Chervonenkis维数和伪维数的新估计。

Result: 网络能以特定误差逼近Sobolev空间函数，参数数量克服维数灾难，结果可扩展到Hölder - Zygmund空间，构造接近最优。

Conclusion: 复杂度界限为流形上涉及导数的偏微分方程学习提供理论基石，网络架构利用稀疏结构有效利用流形低维几何。

Abstract: A key challenge in scientific machine learning is solving partial
differential equations (PDEs) on complex domains, where the curved geometry
complicates the approximation of functions and their derivatives required by
differential operators. This paper establishes the first simultaneous
approximation theory for deep neural networks on manifolds. We prove that a
constant-depth $\mathrm{ReLU}^{k-1}$ network with bounded weights--a property
that plays a crucial role in controlling generalization error--can approximate
any function in the Sobolev space $\mathcal{W}_p^{k}(\mathcal{M}^d)$ to an
error of $\varepsilon$ in the $\mathcal{W}_p^{s}(\mathcal{M}^d)$ norm, for
$k\geq 3$ and $s<k$, using $\mathcal{O}(\varepsilon^{-d/(k-s)})$ nonzero
parameters, a rate that overcomes the curse of dimensionality by depending only
on the intrinsic dimension $d$. These results readily extend to functions in
H\"older-Zygmund spaces. We complement this result with a matching lower bound,
proving our construction is nearly optimal by showing the required number of
parameters matches up to a logarithmic factor. Our proof of the lower bound
introduces novel estimates for the Vapnik-Chervonenkis dimension and
pseudo-dimension of the network's high-order derivative classes. These
complexity bounds provide a theoretical cornerstone for learning PDEs on
manifolds involving derivatives. Our analysis reveals that the network
architecture leverages a sparse structure to efficiently exploit the manifold's
low-dimensional geometry.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [207] [Optimal Investment and Consumption in a Stochastic Factor Model](https://arxiv.org/abs/2509.09452)
*Florian Gutekunst,Martin Herdegen,David Hobson*

Main category: q-fin.MF

TL;DR: 研究无限期幂效用投资者在不完全随机因子模型中的最优投资和消费问题，给出有限状态空间问题适定性的刻画与算法，为无边界值的二阶常微分方程开发理论证明HJB方程解的存在性，能对多种模型验证，还给出离散与连续设置的联系及近似方案。


<details>
  <summary>Details</summary>
Motivation: 在不完全随机因子模型下研究幂效用投资者无限期的最优投资和消费问题。

Method: 在随机因子状态空间有限时给出适定性刻画与数值算法；状态空间为开区间时，为无边界值二阶常微分方程开发上下解理论；刻画解的渐近行为进行验证；给出离散与连续设置的联系和离散化近似方案。

Result: 给出有限状态空间问题的适定性刻画和数值算法；证明HJB方程解的存在性并给出解的显式界；能对多种模型包括Heston模型进行严格验证；能用离散化方案高效近似扩散设置下的值函数。

Conclusion: 该研究为幂效用投资者在不完全随机因子模型下的最优投资和消费问题提供了有效的解决方法和理论支持。

Abstract: In this article, we study optimal investment and consumption in an incomplete
stochastic factor model for a power utility investor on the infinite horizon.
When the state space of the stochastic factor is finite, we give a complete
characterisation of the well-posedness of the problem, and provide an efficient
numerical algorithm for computing the value function. When the state space is a
(possibly infinite) open interval and the stochastic factor is represented by
an It\^o diffusion, we develop a general theory of sub- and supersolutions for
second-order ordinary differential equations on open domains without boundary
values to prove existence of the solution to the Hamilton-Jacobi-Bellman (HJB)
equation along with explicit bounds for the solution. By characterising the
asymptotic behaviour of the solution, we are also able to provide rigorous
verification arguments for various models, including -- for the first time --
the Heston model. Finally, we link the discrete and continuous setting and show
that that the value function in the diffusion setting can be approximated very
efficiently through a fast discretisation scheme.

</details>
