<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 49]
- [cs.CE](#cs.CE) [Total: 6]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 108]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PF](#cs.PF) [Total: 4]
- [cs.SE](#cs.SE) [Total: 23]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 13]
- [cs.CY](#cs.CY) [Total: 13]
- [eess.IV](#eess.IV) [Total: 4]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [econ.EM](#econ.EM) [Total: 2]
- [eess.SP](#eess.SP) [Total: 2]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.FL](#cs.FL) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.DM](#cs.DM) [Total: 1]
- [math.GT](#math.GT) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [math.LO](#math.LO) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SD](#cs.SD) [Total: 5]
- [cond-mat.soft](#cond-mat.soft) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 33]
- [cs.CC](#cs.CC) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [physics.optics](#physics.optics) [Total: 2]
- [eess.SY](#eess.SY) [Total: 3]
- [econ.TH](#econ.TH) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.CL](#cs.CL) [Total: 33]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 3]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.RO](#cs.RO) [Total: 15]
- [cs.HC](#cs.HC) [Total: 2]
- [math.CO](#math.CO) [Total: 1]
- [econ.GN](#econ.GN) [Total: 5]
- [cs.CV](#cs.CV) [Total: 53]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts](https://arxiv.org/abs/2509.05323)
*Adam Cole,Mick Grierson*

Main category: cs.AI

TL;DR: 本文对视频扩散变压器的注意力机制进行艺术与技术研究，提出提取和可视化跨注意力图的方法，探讨其作为分析工具和艺术素材的潜力，助力可解释AI艺术领域。


<details>
  <summary>Details</summary>
Motivation: 受早期视频艺术家操控模拟视频信号创造新视觉美学的启发，研究文本到视频生成中注意力机制。

Method: 基于开源Wan模型，提出提取和可视化跨注意力图的方法。

Result: 工具为文本到视频生成中注意力的时空行为提供可解释窗口，通过探索性探针和艺术案例研究，探讨了注意力图的潜力。

Conclusion: 该工作为可解释AI艺术领域做出贡献，邀请艺术家将AI内部机制作为创意媒介。

Abstract: This paper presents an artistic and technical investigation into the
attention mechanisms of video diffusion transformers. Inspired by early video
artists who manipulated analog video signals to create new visual aesthetics,
this study proposes a method for extracting and visualizing cross-attention
maps in generative video models. Built on the open-source Wan model, our tool
provides an interpretable window into the temporal and spatial behavior of
attention in text-to-video generation. Through exploratory probes and an
artistic case study, we examine the potential of attention maps as both
analytical tools and raw artistic material. This work contributes to the
growing field of Explainable AI for the Arts (XAIxArts), inviting artists to
reclaim the inner workings of AI as a creative medium.

</details>


### [2] [Perception Graph for Cognitive Attack Reasoning in Augmented Reality](https://arxiv.org/abs/2509.05324)
*Rongqian Chen,Shu Hong,Rifatul Islam,Mahdi Imani,G. Gary Tan,Tian Lan*

Main category: cs.AI

TL;DR: 提出感知图模型以应对AR系统的认知攻击，可计算感知失真分数。


<details>
  <summary>Details</summary>
Motivation: AR系统依赖人机交互，易受认知攻击影响用户决策，需解决此问题。

Method: 引入感知图模型，模仿人类从MR环境解读关键信息，用有语义结构表示结果。

Result: 模型能计算反映感知失真程度的定量分数。

Conclusion: 感知图模型为检测和分析认知攻击效果提供了可靠且可衡量的方法。

Abstract: Augmented reality (AR) systems are increasingly deployed in tactical
environments, but their reliance on seamless human-computer interaction makes
them vulnerable to cognitive attacks that manipulate a user's perception and
severely compromise user decision-making. To address this challenge, we
introduce the Perception Graph, a novel model designed to reason about human
perception within these systems. Our model operates by first mimicking the
human process of interpreting key information from an MR environment and then
representing the outcomes using a semantically meaningful structure. We
demonstrate how the model can compute a quantitative score that reflects the
level of perception distortion, providing a robust and measurable method for
detecting and analyzing the effects of such cognitive attacks.

</details>


### [3] [SynDelay: A Synthetic Dataset for Delivery Delay Prediction](https://arxiv.org/abs/2509.05325)
*Liming Xu,Yunbo Long,Alexandra Brintrup*

Main category: cs.AI

TL;DR: 因缺乏高质量公开数据集，AI 在供应链交付延迟预测进展受限。提出合成数据集 SynDelay，可促进预测建模，公开可用并提供基准结果，鼓励社区贡献。


<details>
  <summary>Details</summary>
Motivation: 现有用于交付延迟预测的数据集存在专有、规模小、维护不一致等问题，阻碍可重复性和基准测试，需高质量公开数据集。

Method: 使用基于真实世界数据训练的先进生成模型生成合成数据集 SynDelay。

Result: SynDelay 保留了真实交付模式且保障隐私，虽有噪声和不一致性，但为预测建模提供了实用测试平台，并提供了基线结果和评估指标作为初始基准。

Conclusion: SynDelay 公开可用，鼓励社区贡献数据集、模型和评估实践以推动该领域研究。

Abstract: Artificial intelligence (AI) is transforming supply chain management, yet
progress in predictive tasks -- such as delivery delay prediction -- remains
constrained by the scarcity of high-quality, openly available datasets.
Existing datasets are often proprietary, small, or inconsistently maintained,
hindering reproducibility and benchmarking. We present SynDelay, a synthetic
dataset designed for delivery delay prediction. Generated using an advanced
generative model trained on real-world data, SynDelay preserves realistic
delivery patterns while ensuring privacy. Although not entirely free of noise
or inconsistencies, it provides a challenging and practical testbed for
advancing predictive modelling. To support adoption, we provide baseline
results and evaluation metrics as initial benchmarks, serving as reference
points rather than state-of-the-art claims. SynDelay is publicly available
through the Supply Chain Data Hub, an open initiative promoting dataset sharing
and benchmarking in supply chain AI. We encourage the community to contribute
datasets, models, and evaluation practices to advance research in this area.
All code is openly accessible at https://supplychaindatahub.org.

</details>


### [4] [MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset](https://arxiv.org/abs/2509.05330)
*Seyed Muhammad Hossein Mousavi,Atiye Ilanloo*

Main category: cs.AI

TL;DR: 因缺乏多模态数据集限制情感识别进步，引入MVRS数据集并评估，为多模态情感计算做贡献。


<details>
  <summary>Details</summary>
Motivation: 解决自动情感识别领域缺乏包含身体运动和生理信号的多模态数据集的问题。

Method: 让13位12 - 60岁参与者接受VR情感刺激，用多种设备同步采集数据，按统一协议操作，提取各模态特征，用融合技术处理并以分类器评估。

Result: 确认了MVRS数据集的质量和情感可分离性。

Conclusion: MVRS数据集对多模态情感计算有重要价值。

Abstract: Automatic emotion recognition has become increasingly important with the rise
of AI, especially in fields like healthcare, education, and automotive systems.
However, there is a lack of multimodal datasets, particularly involving body
motion and physiological signals, which limits progress in the field. To
address this, the MVRS dataset is introduced, featuring synchronized recordings
from 13 participants aged 12 to 60 exposed to VR based emotional stimuli
(relaxation, fear, stress, sadness, joy). Data were collected using eye
tracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR
signals (Arduino UNO), all timestamp aligned. Participants followed a unified
protocol with consent and questionnaires. Features from each modality were
extracted, fused using early and late fusion techniques, and evaluated with
classifiers to confirm the datasets quality and emotion separability, making
MVRS a valuable contribution to multimodal affective computing.

</details>


### [5] [Benchmarking Large Language Models for Personalized Guidance in AI-Enhanced Learning](https://arxiv.org/abs/2509.05346)
*Bo Yuan,Jiazi Hu*

Main category: cs.AI

TL;DR: 研究在模拟学习场景中对三个大语言模型进行实证比较，用Gemini作虚拟裁判评估，结果显示GPT - 4o更优，为LLM用于个性化学习提供依据。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在真实学习场景中的系统对比评估有限，需开展相关研究。

Method: 在模拟学习场景的辅导任务中对比三个大语言模型，用含学生答题及正误标签的数据集让模型完成分析、推断和生成指导任务，用Gemini作虚拟裁判从多维度进行两两比较，用Bradley - Terry模型分析结果。

Result: GPT - 4o更受青睐，反馈更具信息性和结构性，DeepSeek - V3和GLM - 4.5有间歇性优势但一致性低。

Conclusion: 大语言模型可作为高级教学助手提供个性化支持，研究为未来相关实证研究提供方法指导。

Abstract: While Large Language Models (LLMs) are increasingly envisioned as intelligent
assistants for personalized learning, systematic head-to-head evaluations
within authentic learning scenarios remain limited. This study conducts an
empirical comparison of three state-of-the-art LLMs on a tutoring task that
simulates a realistic learning setting. Using a dataset comprising a student's
answers to ten questions of mixed formats with correctness labels, each LLM is
required to (i) analyze the quiz to identify underlying knowledge components,
(ii) infer the student's mastery profile, and (iii) generate targeted guidance
for improvement. To mitigate subjectivity and evaluator bias, we employ Gemini
as a virtual judge to perform pairwise comparisons along various dimensions:
accuracy, clarity, actionability, and appropriateness. Results analyzed via the
Bradley-Terry model indicate that GPT-4o is generally preferred, producing
feedback that is more informative and better structured than its counterparts,
while DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower
consistency. These findings highlight the feasibility of deploying LLMs as
advanced teaching assistants for individualized support and provide
methodological guidance for future empirical research on LLM-driven
personalized learning.

</details>


### [6] [SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis](https://arxiv.org/abs/2509.05363)
*Lijie Ding,Changwoo Do*

Main category: cs.AI

TL;DR: 介绍了由大语言模型驱动的多智能体AI系统SasAgent，可自动化小角散射数据处理，展示其能力与潜力。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型驱动的AI系统，实现小角散射（SAS）数据分析的自动化，简化科学工作流程。

Method: 构建包含协调智能体和三个专业智能体的SasAgent系统，使用来自SasView软件的工具，通过基于Gradio的界面实现用户交互。

Result: SasAgent能解读复杂提示，精确计算散射长度密度、生成散射数据和拟合实验数据集。

Conclusion: LLM驱动的AI系统在SAS研究中具有简化科学工作流程和增强自动化的潜力。

Abstract: We introduce SasAgent, a multi-agent AI system powered by large language
models (LLMs) that automates small-angle scattering (SAS) data analysis by
leveraging tools from the SasView software and enables user interaction via
text input. SasAgent features a coordinator agent that interprets user prompts
and delegates tasks to three specialized agents for scattering length density
(SLD) calculation, synthetic data generation, and experimental data fitting.
These agents utilize LLM-friendly tools to execute tasks efficiently. These
tools, including the model data tool, Retrieval-Augmented Generation (RAG)
documentation tool, bump fitting tool, and SLD calculator tool, are derived
from the SasView Python library. A user-friendly Gradio-based interface
enhances user accessibility. Through diverse examples, we demonstrate
SasAgent's ability to interpret complex prompts, calculate SLDs, generate
accurate scattering data, and fit experimental datasets with high precision.
This work showcases the potential of LLM-driven AI systems to streamline
scientific workflows and enhance automation in SAS research.

</details>


### [7] [Characterizing Fitness Landscape Structures in Prompt Engineering](https://arxiv.org/abs/2509.05375)
*Arend Hintze*

Main category: cs.AI

TL;DR: 文章用自相关分析研究提示工程中适应度景观结构，通过不同提示生成策略实验揭示不同景观拓扑，为理解提示工程优化复杂性提供实证基础。


<details>
  <summary>Details</summary>
Motivation: 当前提示优化将其作为黑盒问题，未刻画所探索的景观拓扑，对提示工程潜在优化景观理解不足。

Method: 使用跨语义嵌入空间的自相关分析对提示工程的适应度景观结构进行系统分析，通过系统枚举和新颖驱动多样化两种提示生成策略在错误检测任务上进行实验。

Result: 系统提示生成产生平滑衰减的自相关，多样化生成呈现非单调模式，在中间语义距离处有峰值相关；不同错误类型的任务特定分析显示出不同程度的崎岖性。

Conclusion: 研究结果为理解提示工程景观优化的复杂性提供了实证基础。

Abstract: While prompt engineering has emerged as a crucial technique for optimizing
large language model performance, the underlying optimization landscape remains
poorly understood. Current approaches treat prompt optimization as a black-box
problem, applying sophisticated search algorithms without characterizing the
landscape topology they navigate. We present a systematic analysis of fitness
landscape structures in prompt engineering using autocorrelation analysis
across semantic embedding spaces. Through experiments on error detection tasks
with two distinct prompt generation strategies -- systematic enumeration (1,024
prompts) and novelty-driven diversification (1,000 prompts) -- we reveal
fundamentally different landscape topologies. Systematic prompt generation
yields smoothly decaying autocorrelation, while diversified generation exhibits
non-monotonic patterns with peak correlation at intermediate semantic
distances, indicating rugged, hierarchically structured landscapes.
Task-specific analysis across 10 error detection categories reveals varying
degrees of ruggedness across different error types. Our findings provide an
empirical foundation for understanding the complexity of optimization in prompt
engineering landscapes.

</details>


### [8] [Code Like Humans: A Multi-Agent Solution for Medical Coding](https://arxiv.org/abs/2509.05378)
*Andreas Motzfeldt,Joakim Edin,Casper L. Christensen,Christian Hardmeier,Lars Maaløe,Anna Rogers*

Main category: cs.AI

TL;DR: 提出名为Code Like Humans的大语言模型医疗编码框架，能支持完整ICD - 10编码系统，在罕见诊断代码上表现最佳，并分析系统性能和盲点。


<details>
  <summary>Details</summary>
Motivation: 解决医疗编码中专家将非结构化临床笔记映射为字母数字代码的问题，提供更有效的医疗编码解决方案。

Method: 引入Code Like Humans框架，实现人类专家的官方编码指南。

Result: 在罕见诊断代码上取得目前最佳性能，高频代码上微调的判别分类器有优势但有局限性。

Conclusion: 提出新框架，分析系统性能并找出盲点，为未来工作提供方向。

Abstract: In medical coding, experts map unstructured clinical notes to alphanumeric
codes for diagnoses and procedures. We introduce Code Like Humans: a new
agentic framework for medical coding with large language models. It implements
official coding guidelines for human experts, and it is the first solution that
can support the full ICD-10 coding system (+70K labels). It achieves the best
performance to date on rare diagnosis codes (fine-tuned discriminative
classifiers retain an advantage for high-frequency codes, to which they are
limited). Towards future work, we also contribute an analysis of system
performance and identify its `blind spots' (codes that are systematically
undercoded).

</details>


### [9] [Murphys Laws of AI Alignment: Why the Gap Always Wins](https://arxiv.org/abs/2509.05381)
*Madhava Gaikwad*

Main category: cs.AI

TL;DR: 本文引入对齐差距概念理解基于反馈对齐的常见失败，整理墨菲定律目录，提出对齐三难困境，用小规模实证研究支持，还给出MAPS框架，为对齐设计提供新视角。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类反馈的强化学习等对齐方法存在奖励破解、谄媚等常见失败模式，需要统一视角理解这些问题。

Method: 引入对齐差距概念，用KL倾斜形式化说明优化压力如何放大代理奖励与人类真实意图的差异，组织失败情况成墨菲定律目录，提出对齐三难困境，进行小规模实证研究，提出MAPS框架。

Result: 通过小规模实证研究对观点提供了支持。

Conclusion: 本文不是确定的不可能定理，而是从结构限制和权衡角度重新构建对齐辩论，为未来设计提供更清晰指导。

Abstract: Large language models are increasingly aligned to human preferences through
reinforcement learning from human feedback (RLHF) and related methods such as
Direct Preference Optimization (DPO), Constitutional AI, and RLAIF. While
effective, these methods exhibit recurring failure patterns i.e., reward
hacking, sycophancy, annotator drift, and misgeneralization. We introduce the
concept of the Alignment Gap, a unifying lens for understanding recurring
failures in feedback-based alignment. Using a KL-tilting formalism, we
illustrate why optimization pressure tends to amplify divergence between proxy
rewards and true human intent. We organize these failures into a catalogue of
Murphys Laws of AI Alignment, and propose the Alignment Trilemma as a way to
frame trade-offs among optimization strength, value capture, and
generalization. Small-scale empirical studies serve as illustrative support.
Finally, we propose the MAPS framework (Misspecification, Annotation, Pressure,
Shift) as practical design levers. Our contribution is not a definitive
impossibility theorem but a perspective that reframes alignment debates around
structural limits and trade-offs, offering clearer guidance for future design.

</details>


### [10] [From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation](https://arxiv.org/abs/2509.05469)
*Chenguang Wang,Xiang Yan,Yilong Dai,Ziyi Wang,Susu Xu*

Main category: cs.AI

TL;DR: 提出多智能体系统直接在街景图像上编辑和重新设计自行车设施，实验证明系统有效，为交通规划和设施设计奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统街道设计可视化方法劳动密集，现有AI生成方法需大量特定领域训练数据且难以实现复杂街景精确空间变化，需要更好方法用于公众参与交通规划。

Method: 引入多智能体系统，集成车道定位、提示优化、设计生成和自动评估来合成设计。

Result: 在不同城市场景实验中，系统能适应不同道路几何形状和环境条件，产生视觉连贯且符合指令的结果。

Conclusion: 为多智能体管道在交通基础设施规划和设施设计中的应用奠定基础。

Abstract: Realistic visual renderings of street-design scenarios are essential for
public engagement in active transportation planning. Traditional approaches are
labor-intensive, hindering collective deliberation and collaborative
decision-making. While AI-assisted generative design shows transformative
potential by enabling rapid creation of design scenarios, existing generative
approaches typically require large amounts of domain-specific training data and
struggle to enable precise spatial variations of design/configuration in
complex street-view scenes. We introduce a multi-agent system that edits and
redesigns bicycle facilities directly on real-world street-view imagery. The
framework integrates lane localization, prompt optimization, design generation,
and automated evaluation to synthesize realistic, contextually appropriate
designs. Experiments across diverse urban scenarios demonstrate that the system
can adapt to varying road geometries and environmental conditions, consistently
yielding visually coherent and instruction-compliant results. This work
establishes a foundation for applying multi-agent pipelines to transportation
infrastructure planning and facility design.

</details>


### [11] [TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation](https://arxiv.org/abs/2509.05550)
*Zixi Li*

Main category: cs.AI

TL;DR: 介绍TreeGPT用于神经程序合成任务处理抽象语法树，结合多种机制，在ARC Prize 2025数据集上表现优，参数少。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在处理抽象语法树时的不足，提出新架构用于神经程序合成任务。

Method: 采用混合设计，结合自注意力和TreeFFN，有全局父子聚合机制，还集成可选增强功能。

Result: 在ARC Prize 2025数据集上准确率达96%，大幅超越基线模型，仅用1.5M参数。

Conclusion: TreeGPT架构有效，边缘投影是关键组件，与门控结合性能最佳。

Abstract: We introduce TreeGPT, a novel neural architecture that combines
transformer-based attention mechanisms with global parent-child aggregation for
processing Abstract Syntax Trees (ASTs) in neural program synthesis tasks.
Unlike traditional approaches that rely solely on sequential processing or
graph neural networks, TreeGPT employs a hybrid design that leverages both
self-attention for capturing local dependencies and a specialized Tree
Feed-Forward Network (TreeFFN) for modeling hierarchical tree structures
through iterative message passing.
  The core innovation lies in our Global Parent-Child Aggregation mechanism,
formalized as: $$h_i^{(t+1)} = \sigma \Big( h_i^{(0)} + W_{pc} \sum_{(p,c) \in
E_i} f(h_p^{(t)}, h_c^{(t)}) + b \Big)$$ where $h_i^{(t)}$ represents the
hidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edges
involving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. This
formulation enables each node to progressively aggregate information from the
entire tree structure through $T$ iterations.
  Our architecture integrates optional enhancements including gated aggregation
with learnable edge weights, residual connections for gradient stability, and
bidirectional propagation for capturing both bottom-up and top-down
dependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challenging
visual reasoning benchmark requiring abstract pattern recognition and rule
inference. Experimental results demonstrate that TreeGPT achieves 96\%
accuracy, significantly outperforming transformer baselines (1.3\%),
large-scale models like Grok-4 (15.9\%), and specialized program synthesis
methods like SOAR (52\%) while using only 1.5M parameters. Our comprehensive
ablation study reveals that edge projection is the most critical component,
with the combination of edge projection and gating achieving optimal
performance.

</details>


### [12] [OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision](https://arxiv.org/abs/2509.05578)
*Ruixun Liu,Lingyu Kong,Derun Li,Hang Zhao*

Main category: cs.AI

TL;DR: 提出OccVLA框架将3D占用表示集成到多模态推理过程，在自动驾驶相关任务表现出色。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型缺乏3D空间理解能力，构建有效3D表示和进行大规模3D视觉语言预训练存在挑战。

Method: 提出OccVLA框架，将密集3D占用作为预测输出和监督信号，使模型从2D视觉输入学习细粒度空间结构。

Result: 在nuScenes轨迹规划基准测试中取得最优结果，在3D视觉问答任务中表现出色。

Conclusion: OccVLA为自动驾驶提供可扩展、可解释和全视觉的解决方案。

Abstract: Multimodal large language models (MLLMs) have shown strong vision-language
reasoning abilities but still lack robust 3D spatial understanding, which is
critical for autonomous driving. This limitation stems from two key challenges:
(1) the difficulty of constructing accessible yet effective 3D representations
without expensive manual annotations, and (2) the loss of fine-grained spatial
details in VLMs due to the absence of large-scale 3D vision-language
pretraining. To address these challenges, we propose OccVLA, a novel framework
that integrates 3D occupancy representations into a unified multimodal
reasoning process. Unlike prior approaches that rely on explicit 3D inputs,
OccVLA treats dense 3D occupancy as both a predictive output and a supervisory
signal, enabling the model to learn fine-grained spatial structures directly
from 2D visual inputs. The occupancy predictions are regarded as implicit
reasoning processes and can be skipped during inference without performance
degradation, thereby adding no extra computational overhead. OccVLA achieves
state-of-the-art results on the nuScenes benchmark for trajectory planning and
demonstrates superior performance on 3D visual question-answering tasks,
offering a scalable, interpretable, and fully vision-based solution for
autonomous driving.

</details>


### [13] [MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions](https://arxiv.org/abs/2509.05685)
*Jian Yang,Jiahui Wu,Li Fang,Hongchao Fan,Bianying Zhang,Huijie Zhao,Guangyi Yang,Rui Xin,Xiong You*

Main category: cs.AI

TL;DR: 本文提出MSRFormer框架用于道路网络表征学习，结合多尺度空间交互，在两个数据集上验证其性能优于基线方法，为开发与任务无关的道路网络表征模型提供实用框架。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在处理城市道路网络的异质性和层次性时存在困难，无法准确进行表征学习。

Method: 提出MSRFormer框架，用空间流卷积提取小尺度特征，识别与尺度相关的空间交互区域，使用图变换器捕捉多尺度复杂空间依赖，通过残差连接融合特征并使用对比学习算法得到最终表征。

Result: 在两个真实世界数据集的两个道路网络分析任务中，MSRFormer优于基线方法，在复杂道路网络结构中有高达16%的性能提升。

Conclusion: 研究为开发与任务无关的道路网络表征模型提供实用框架，突出了空间交互的尺度效应和流异质性之间的独特关联模式。

Abstract: Transforming road network data into vector representations using deep
learning has proven effective for road network analysis. However, urban road
networks' heterogeneous and hierarchical nature poses challenges for accurate
representation learning. Graph neural networks, which aggregate features from
neighboring nodes, often struggle due to their homogeneity assumption and focus
on a single structural scale. To address these issues, this paper presents
MSRFormer, a novel road network representation learning framework that
integrates multi-scale spatial interactions by addressing their flow
heterogeneity and long-distance dependencies. It uses spatial flow convolution
to extract small-scale features from large trajectory datasets, and identifies
scale-dependent spatial interaction regions to capture the spatial structure of
road networks and flow heterogeneity. By employing a graph transformer,
MSRFormer effectively captures complex spatial dependencies across multiple
scales. The spatial interaction features are fused using residual connections,
which are fed to a contrastive learning algorithm to derive the final road
network representation. Validation on two real-world datasets demonstrates that
MSRFormer outperforms baseline methods in two road network analysis tasks. The
performance gains of MSRFormer suggest the traffic-related task benefits more
from incorporating trajectory data, also resulting in greater improvements in
complex road network structures with up to 16% improvements compared to the
most competitive baseline method. This research provides a practical framework
for developing task-agnostic road network representation models and highlights
distinct association patterns of the interplay between scale effects and flow
heterogeneity of spatial interactions.

</details>


### [14] [Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs](https://arxiv.org/abs/2509.05714)
*Zhaoyu Fan,Kaihang Pan,Mingze Zhou,Bosheng Qin,Juncheng Li,Shengyu Zhang,Wenqiao Zhang,Siliang Tang,Fei Wu,Yueting Zhuang*

Main category: cs.AI

TL;DR: 提出CogEdit评估基准和MIND编辑框架，实验表明MIND优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对多模态大语言模型元认知知识编辑能力的评估，需填补该空白。

Method: 引入CogEdit基准从三个层面评估，提出MIND框架构建元知识记忆、用博弈论交互监测、结合标签细化更新。

Result: MIND显著优于现有认知编辑方法，在传统和元认知知识编辑基准上表现良好。

Conclusion: MIND框架能有效提升多模态大语言模型的元认知知识编辑能力。

Abstract: Knowledge editing enables multimodal large language models (MLLMs) to
efficiently update outdated or incorrect information. However, existing
benchmarks primarily emphasize cognitive-level modifications while lacking a
focus on deeper meta-cognitive processes. To bridge this gap, we introduce
CogEdit, a novel benchmark designed to evaluate MLLMs' meta-cognitive knowledge
editing abilities across three levels: (1) Counterfactual-Driven Editing,
assessing self-awareness of knowledge correctness changes; (2) Boundary
Constraint Editing, ensuring appropriate generalization without unintended
interference; and (3) Noise-Robust Editing, promoting reflective evaluation of
uncertain information. To advance meta-cognitive editing, we propose MIND
(Meta-cognitive INtegrated Dynamic Knowledge Editing), a framework that
constructs a meta-knowledge memory for self-awareness, employs game-theoretic
interactions to monitor knowledge activation, and incorporates label refinement
for noise-robust updates. Extensive experiments show that MIND significantly
outperforms existing cognitive editing approaches, achieving strong performance
on both traditional and meta-cognitive knowledge editing benchmarks.

</details>


### [15] [Hyperbolic Large Language Models](https://arxiv.org/abs/2509.05757)
*Sarang Patil,Zeyong Zhang,Yiran Huang,Tengfei Ma,Mengjia Xu*

Main category: cs.AI

TL;DR: 本文探讨利用双曲几何提升大语言模型语义表示学习和多尺度推理，介绍双曲大语言模型技术分类、应用及未来方向，并提供相关资源库。


<details>
  <summary>Details</summary>
Motivation: 现实中很多数据有非欧几里得潜在层次结构，用大语言模型有效学习这些数据的语义和层次关系是未充分探索领域，而双曲几何适合建模树状层次结构。

Method: 对利用双曲几何作为表示空间的大语言模型进展进行全面阐述，将双曲大语言模型主要技术分为四类。

Result: 给出双曲大语言模型技术分类，探索潜在应用并指出未来研究方向，提供相关资源库。

Conclusion: 双曲几何可作为表示空间提升大语言模型语义表示学习和多尺度推理，有进一步研究价值。

Abstract: Large language models (LLMs) have achieved remarkable success and
demonstrated superior performance across various tasks, including natural
language processing (NLP), weather forecasting, biological protein folding,
text generation, and solving mathematical problems. However, many real-world
data exhibit highly non-Euclidean latent hierarchical anatomy, such as protein
networks, transportation networks, financial networks, brain networks, and
linguistic structures or syntactic trees in natural languages. Effectively
learning intrinsic semantic entailment and hierarchical relationships from
these raw, unstructured input data using LLMs remains an underexplored area.
Due to its effectiveness in modeling tree-like hierarchical structures,
hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity
as an expressive latent representation space for complex data modeling across
domains such as graphs, images, languages, and multi-modal data. Here, we
provide a comprehensive and contextual exposition of recent advancements in
LLMs that leverage hyperbolic geometry as a representation space to enhance
semantic representation learning and multi-scale reasoning. Specifically, the
paper presents a taxonomy of the principal techniques of Hyperbolic LLMs
(HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log
maps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4)
hyperbolic state-space models. We also explore crucial potential applications
and outline future research directions. A repository of key papers, models,
datasets, and code implementations is available at
https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.

</details>


### [16] [DRF: LLM-AGENT Dynamic Reputation Filtering Framework](https://arxiv.org/abs/2509.05764)
*Yuwei Lou,Hao Hu,Shaocong Ma,Zongfei Zhang,Liang Wang,Jidong Ge,Xianping Tao*

Main category: cs.AI

TL;DR: 为解决多智能体系统利用大语言模型时的性能量化和可信度评估问题，引入DRF框架，实验表明其能提升任务完成质量和协作效率。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统利用大语言模型时面临量化智能体性能和评估可信度的挑战。

Method: 构建交互式评级网络量化智能体性能，设计声誉评分机制衡量智能体诚实性和能力，集成基于上置信界的策略提高智能体选择效率。

Result: DRF在逻辑推理和代码生成任务中显著提高任务完成质量和协作效率。

Conclusion: DRF为多智能体系统处理大规模任务提供新方法。

Abstract: With the evolution of generative AI, multi - agent systems leveraging large -
language models(LLMs) have emerged as a powerful tool for complex tasks.
However, these systems face challenges in quantifying agent performance and
lack mechanisms to assess agent credibility. To address these issues, we
introduce DRF, a dynamic reputation filtering framework. DRF constructs an
interactive rating network to quantify agent performance, designs a reputation
scoring mechanism to measure agent honesty and capability, and integrates an
Upper Confidence Bound - based strategy to enhance agent selection efficiency.
Experiments show that DRF significantly improves task completion quality and
collaboration efficiency in logical reasoning and code - generation tasks,
offering a new approach for multi - agent systems to handle large - scale
tasks.

</details>


### [17] [Decision-Focused Learning Enhanced by Automated Feature Engineering for Energy Storage Optimisation](https://arxiv.org/abs/2509.05772)
*Nasser Alkhulaifi,Ismail Gokay Dogan,Timothy R. Cargan,Alexander L. Bowler,Direnc Pekaslan,Nicholas J. Watson,Isaac Triguero*

Main category: cs.AI

TL;DR: 本文提出AFE - DFL框架用于BESS优化，通过英国房产数据集验证，结果表明DFL优于PTO，AFE能提升DFL性能。


<details>
  <summary>Details</summary>
Motivation: PTO方法存在预测误差影响决策的问题，新兴DFL方法应用于实际有局限性，且现实BESS应用面临更多挑战，需改进方法。

Method: 利用自动化特征工程（AFE），提出适用于小数据集的AFE - DFL框架，对电价和需求进行预测并优化BESS操作以降低成本。

Result: DFL平均运营成本低于PTO，AFE能使DFL方法性能提升22.9 - 56.5%。

Conclusion: DFL在现实环境具有实际可行性，特定领域AFE可增强DFL效果，减少对领域专业知识依赖，为能源管理系统带来经济利益。

Abstract: Decision-making under uncertainty in energy management is complicated by
unknown parameters hindering optimal strategies, particularly in Battery Energy
Storage System (BESS) operations. Predict-Then-Optimise (PTO) approaches treat
forecasting and optimisation as separate processes, allowing prediction errors
to cascade into suboptimal decisions as models minimise forecasting errors
rather than optimising downstream tasks. The emerging Decision-Focused Learning
(DFL) methods overcome this limitation by integrating prediction and
optimisation; however, they are relatively new and have been tested primarily
on synthetic datasets or small-scale problems, with limited evidence of their
practical viability. Real-world BESS applications present additional
challenges, including greater variability and data scarcity due to collection
constraints and operational limitations. Because of these challenges, this work
leverages Automated Feature Engineering (AFE) to extract richer representations
and improve the nascent approach of DFL. We propose an AFE-DFL framework
suitable for small datasets that forecasts electricity prices and demand while
optimising BESS operations to minimise costs. We validate its effectiveness on
a novel real-world UK property dataset. The evaluation compares DFL methods
against PTO, with and without AFE. The results show that, on average, DFL
yields lower operating costs than PTO and adding AFE further improves the
performance of DFL methods by 22.9-56.5% compared to the same models without
AFE. These findings provide empirical evidence for DFL's practical viability in
real-world settings, indicating that domain-specific AFE enhances DFL and
reduces reliance on domain expertise for BESS optimisation, yielding economic
benefits with broader implications for energy management systems facing similar
challenges.

</details>


### [18] [Chatbot To Help Patients Understand Their Health](https://arxiv.org/abs/2509.05818)
*Won Seok Jang,Hieu Tran,Manav Mistry,SaiKiran Gandluri,Yifan Zhang,Sharmin Sultana,Sunjae Kown,Yuan Zhang,Zonghai Yao,Hong Yu*

Main category: cs.AI

TL;DR: 提出NoteAid - Chatbot对话式AI，用无人工标注数据的新框架促进患者理解，评估显示其有患者教育关键行为，图灵测试超非专家人类，展示低成本PPO - RL在对话领域可行性。


<details>
  <summary>Details</summary>
Motivation: 让患者拥有积极参与护理所需知识，借助AI促进患者理解。

Method: 基于多智能体大语言模型和强化学习，用轻量级LLaMA 3.2 3B模型分两阶段训练，先在合成对话数据上监督微调，再用模拟出院场景患者理解评估的奖励进行强化学习。

Result: NoteAid - Chatbot展现患者教育关键行为，简单PPO奖励建模可训练轻量级特定领域聊天机器人处理多轮交互等，图灵测试超非专家人类。

Conclusion: 所提框架展示了低成本PPO - RL在现实开放对话领域的可行性和前景，拓宽了基于RL对齐方法的适用性。

Abstract: Patients must possess the knowledge necessary to actively participate in
their care. We present NoteAid-Chatbot, a conversational AI that promotes
patient understanding via a novel 'learning as conversation' framework, built
on a multi-agent large language model (LLM) and reinforcement learning (RL)
setup without human-labeled data. NoteAid-Chatbot was built on a lightweight
LLaMA 3.2 3B model trained in two stages: initial supervised fine-tuning on
conversational data synthetically generated using medical conversation
strategies, followed by RL with rewards derived from patient understanding
assessments in simulated hospital discharge scenarios. Our evaluation, which
includes comprehensive human-aligned assessments and case studies, demonstrates
that NoteAid-Chatbot exhibits key emergent behaviors critical for patient
education, such as clarity, relevance, and structured dialogue, even though it
received no explicit supervision for these attributes. Our results show that
even simple Proximal Policy Optimization (PPO)-based reward modeling can
successfully train lightweight, domain-specific chatbots to handle multi-turn
interactions, incorporate diverse educational strategies, and meet nuanced
communication objectives. Our Turing test demonstrates that NoteAid-Chatbot
surpasses non-expert human. Although our current focus is on healthcare, the
framework we present illustrates the feasibility and promise of applying
low-cost, PPO-based RL to realistic, open-ended conversational domains,
broadening the applicability of RL-based alignment methods.

</details>


### [19] [MapAgent: A Hierarchical Agent for Geospatial Reasoning with Dynamic Map Tool Integration](https://arxiv.org/abs/2509.05933)
*Md Hasebul Hasan,Mahir Labib Dihan,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: 介绍了用于地图集成地理空间推理的分层多智能体框架MapAgent，在四个地理空间基准测试中表现优于现有基线，并开源。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型框架在地理空间任务上表现不佳，需要解决空间推理、多跳规划和实时地图交互等问题。

Method: 引入MapAgent，将规划与执行解耦，高层规划器分解查询为子目标，为工具密集型模块设计专用地图工具智能体，简单模块无额外智能体开销。

Result: 在四个不同的地理空间基准测试中，MapAgent相比现有工具增强和智能体基线有显著提升。

Conclusion: MapAgent的分层设计减少认知负担、提高工具选择准确性，能实现相似API间的精确协调。

Abstract: Agentic AI has significantly extended the capabilities of large language
models (LLMs) by enabling complex reasoning and tool use. However, most
existing frameworks are tailored to domains such as mathematics, coding, or web
automation, and fall short on geospatial tasks that require spatial reasoning,
multi-hop planning, and real-time map interaction. To address these challenges,
we introduce MapAgent, a hierarchical multi-agent plug-and-play framework with
customized toolsets and agentic scaffolds for map-integrated geospatial
reasoning. Unlike existing flat agent-based approaches that treat tools
uniformly-often overwhelming the LLM when handling similar but subtly different
geospatial APIs-MapAgent decouples planning from execution. A high-level
planner decomposes complex queries into subgoals, which are routed to
specialized modules. For tool-heavy modules-such as map-based services-we then
design a dedicated map-tool agent that efficiently orchestrates related APIs
adaptively in parallel to effectively fetch geospatial data relevant for the
query, while simpler modules (e.g., solution generation or answer extraction)
operate without additional agent overhead. This hierarchical design reduces
cognitive load, improves tool selection accuracy, and enables precise
coordination across similar APIs. We evaluate MapAgent on four diverse
geospatial benchmarks-MapEval-Textual, MapEval-API, MapEval-Visual, and
MapQA-and demonstrate substantial gains over state-of-the-art tool-augmented
and agentic baselines. We open-source our framwork at
https://github.com/Hasebul/MapAgent.

</details>


### [20] [Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL](https://arxiv.org/abs/2509.06024)
*Haoyang He,Zihua Rong,Kun Ji,Chenyang Li,Qing Huang,Chong Xia,Lan Yang,Honggang Zhang*

Main category: cs.AI

TL;DR: 本文提出DRER奖励框架及Logictree数据集，实验证实其能提升大语言模型推理能力，代码数据开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的奖励函数无法评估思维链对答案的提升，特定任务训练难以揭示模型真实推理能力。

Method: 提出DRER框架，包含推理质量奖励和动态长度优势；发布Logictree数据集用于训练和评估。

Result: 7B模型在Logictree上400步训练达GPT-o3-mini水平，CoT增强答案平均置信度提升30%，模型在多数据集有泛化性。

Conclusion: RL可塑造CoT行为，为提升大语言模型形式推理能力提供实践路径。

Abstract: Reinforcement learning (RL) has recently become the dominant paradigm for
strengthening the reasoning abilities of large language models (LLMs). Yet the
rule-based reward functions commonly used on mathematical or programming
benchmarks assess only answer format and correctness, providing no signal as to
whether the induced Chain-of-Thought (CoT) actually improves the answer.
Furthermore, such task-specific training offers limited control over logical
depth and therefore may fail to reveal a model's genuine reasoning capacity. We
propose Dynamic Reasoning Efficiency Reward (DRER) -- a plug-and-play RL reward
framework that reshapes both reward and advantage signals. (i) A Reasoning
Quality Reward assigns fine-grained credit to those reasoning chains that
demonstrably raise the likelihood of the correct answer, directly incentivising
the trajectories with beneficial CoT tokens. (ii) A Dynamic Length Advantage
decays the advantage of responses whose length deviates from a
validation-derived threshold, stabilising training. To facilitate rigorous
assessment, we also release Logictree, a dynamically constructed deductive
reasoning dataset that functions both as RL training data and as a
comprehensive benchmark. Experiments confirm the effectiveness of DRER: our 7B
model attains GPT-o3-mini level performance on Logictree with 400 trianing
steps, while the average confidence of CoT-augmented answers rises by 30%. The
model further exhibits generalisation across diverse logical-reasoning
datasets, and the mathematical benchmark AIME24. These results illuminate how
RL shapes CoT behaviour and chart a practical path toward enhancing
formal-reasoning skills in large language models. All code and data are
available in repository https://github.com/Henryhe09/DRER.

</details>


### [21] [Reverse-Engineered Reasoning for Open-Ended Generation](https://arxiv.org/abs/2509.06160)
*Haozhe Wang,Haoran Que,Qixin Xu,Minghao Liu,Wangchunshu Zhou,Jiazhan Feng,Wanjun Zhong,Wei Ye,Tong Yang,Wenhao Huang,Ge Zhang,Fangzhen Lin*

Main category: cs.AI

TL;DR: 提出逆向工程推理范式REER，构建DeepWriting - 20K数据集，训练的DeepWriter - 8B模型表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法在开放式创造性生成领域存在局限，如强化学习缺乏明确奖励信号和高质量奖励模型，蒸馏方法成本高且受教师模型能力限制。

Method: 引入逆向工程推理范式REER，从已知良好解决方案反向计算发现潜在的逐步深度推理过程，并构建了DeepWriting - 20K数据集。

Result: 训练的DeepWriter - 8B模型超越了强大的开源基线，性能与GPT - 4o和Claude 3.5等领先专有模型相当，有时更优。

Conclusion: REER范式为开放式创造性生成领域的推理提供了新的有效方法。

Abstract: While the ``deep reasoning'' paradigm has spurred significant advances in
verifiable domains like mathematics, its application to open-ended, creative
generation remains a critical challenge. The two dominant methods for
instilling reasoning -- reinforcement learning (RL) and instruction
distillation -- falter in this area; RL struggles with the absence of clear
reward signals and high-quality reward models, while distillation is
prohibitively expensive and capped by the teacher model's capabilities. To
overcome these limitations, we introduce REverse-Engineered Reasoning (REER), a
new paradigm that fundamentally shifts the approach. Instead of building a
reasoning process ``forwards'' through trial-and-error or imitation, REER works
``backwards'' from known-good solutions to computationally discover the latent,
step-by-step deep reasoning process that could have produced them. Using this
scalable, gradient-free approach, we curate and open-source DeepWriting-20K, a
large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.
Our model, DeepWriter-8B, trained on this data, not only surpasses strong
open-source baselines but also achieves performance competitive with, and at
times superior to, leading proprietary models like GPT-4o and Claude 3.5.

</details>


### [22] [From Long to Short: LLMs Excel at Trimming Own Reasoning Chains](https://arxiv.org/abs/2509.06174)
*Wei Han,Geng Zhan,Sicheng Yu,Chenyu Wang,Bryan Hooi*

Main category: cs.AI

TL;DR: O1/R1风格大推理模型有进步但易过度思考，提出EDIT方法提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决大推理模型易过度思考、难以平衡正确性和简洁性的问题。

Method: 提出EDIT测试时间缩放方法，采用约束引导生成，联合跟踪不同约束下的长度和答案分布。

Result: 在多种模型和数据集上的实验表明，EDIT大幅提高推理效率，输出简洁且信息丰富。

Conclusion: EDIT方法能有效提升大推理模型的推理效率，改善可读性和用户体验。

Abstract: O1/R1 style large reasoning models (LRMs) signal a substantial leap forward
over conventional instruction-following LLMs. By applying test-time scaling to
generate extended reasoning paths, they establish many SOTAs across a wide
range of complex reasoning tasks. However, recent studies show that LRMs are
prone to suffer from overthinking -- the tendency to overcomplicate simple
problems, leading to excessive strategy switching and long, convoluted
reasoning traces that hinder their interpretability. To mitigate this issue, we
conduct a systematic investigation into the reasoning efficiency of a broad set
of LRMs and uncover a common dilemma: the difficulty in balancing multiple
generation objectives such as correctness and brevity. Based on this discovery,
we propose a test-time scaling method, EDIT (Efficient Dynamic Inference
Trimming), which efficiently guides LRMs to identify the shortest correct
reasoning paths at test time. EDIT employs constraint-guided generation while
jointly tracking length and answer distributions under varying constraints,
allowing it to select responses that strike an optimal balance between
conciseness and correctness. Extensive experiments across diverse models and
datasets show that EDIT substantially enhance the reasoning efficiency,
producing compact yet informative outputs that improve readability and user
experience.

</details>


### [23] [PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team Environments](https://arxiv.org/abs/2509.06235)
*Olivier Schipper,Yudi Zhang,Yali Du,Mykola Pechenizkiy,Meng Fang*

Main category: cs.AI

TL;DR: 本文引入PillagerBench评估框架和TactiCrafter多智能体系统，评估显示TactiCrafter表现优于基线方法，还开源框架推动研究。


<details>
  <summary>Details</summary>
Motivation: 解决基于大语言模型的智能体在竞争性多智能体环境中有效性研究不足的问题。

Method: 引入PillagerBench框架进行评估，提出TactiCrafter多智能体系统。

Result: TactiCrafter在评估中优于基线方法，展现出通过自我博弈的自适应学习能力。

Conclusion: 开源PillagerBench以推动竞争性环境下多智能体AI的研究。

Abstract: LLM-based agents have shown promise in various cooperative and strategic
reasoning tasks, but their effectiveness in competitive multi-agent
environments remains underexplored. To address this gap, we introduce
PillagerBench, a novel framework for evaluating multi-agent systems in
real-time competitive team-vs-team scenarios in Minecraft. It provides an
extensible API, multi-round testing, and rule-based built-in opponents for
fair, reproducible comparisons. We also propose TactiCrafter, an LLM-based
multi-agent system that facilitates teamwork through human-readable tactics,
learns causal dependencies, and adapts to opponent strategies. Our evaluation
demonstrates that TactiCrafter outperforms baseline approaches and showcases
adaptive learning through self-play. Additionally, we analyze its learning
process and strategic evolution over multiple game episodes. To encourage
further research, we have open-sourced PillagerBench, fostering advancements in
multi-agent AI for competitive environments.

</details>


### [24] [Proof2Silicon: Prompt Repair for Verified Code and Hardware Generation via Reinforcement Learning](https://arxiv.org/abs/2509.06239)
*Manvi Jha,Jiaxin Wan,Deming Chen*

Main category: cs.AI

TL;DR: 提出Proof2Silicon框架，结合PREFACE实现从自然语言规范生成硬件，评估显示提升验证和合成成功率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型生成代码难以通过形式验证的问题，实现从自然语言规范生成硬件。

Method: Proof2Silicon框架，包括用PREFACE优化提示生成、将验证后的Dafny程序转为C语言、用Vivado HLS生成RTL实现。

Result: PREFACE提升Dafny验证成功率达21%，Proof2Silicon硬件合成成功率达72%。

Conclusion: 展示了基于大语言模型的形式验证硬件合成的强大、可扩展且自动化的流程。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
automated code generation but frequently produce code that fails formal
verification, an essential requirement for hardware and safety-critical
domains. To overcome this fundamental limitation, we previously proposed
PREFACE, a model-agnostic framework based on reinforcement learning (RL) that
iteratively repairs the prompts provided to frozen LLMs, systematically
steering them toward generating formally verifiable Dafny code without costly
fine-tuning. This work presents Proof2Silicon, a novel end-to-end synthesis
framework that embeds the previously proposed PREFACE flow to enable the
generation of correctness-by-construction hardware directly from natural
language specifications. Proof2Silicon operates by: (1) leveraging PREFACE's
verifier-driven RL agent to optimize prompt generation iteratively, ensuring
Dafny code correctness; (2) automatically translating verified Dafny programs
into synthesizable high-level C using Dafny's Python backend and PyLog; and (3)
employing Vivado HLS to produce RTL implementations. Evaluated rigorously on a
challenging 100-task benchmark, PREFACE's RL-guided prompt optimization
consistently improved Dafny verification success rates across diverse LLMs by
up to 21%. Crucially, Proof2Silicon achieved an end-to-end hardware synthesis
success rate of up to 72%, generating RTL designs through Vivado HLS synthesis
flows. These results demonstrate a robust, scalable, and automated pipeline for
LLM-driven, formally verified hardware synthesis, bridging natural-language
specification and silicon realization.

</details>


### [25] [REMI: A Novel Causal Schema Memory Architecture for Personalized Lifestyle Recommendation Agents](https://arxiv.org/abs/2509.06269)
*Vishal Raman,Vijai Aravindh R,Abhijith Ragav*

Main category: cs.AI

TL;DR: 提出REMI架构，让多模态生活方式代理提供可解释的个性化建议，评估显示比基线LLM代理更优。


<details>
  <summary>Details</summary>
Motivation: 解决个性化AI助手难以整合复杂个人数据和因果知识，提供的建议缺乏解释力的问题。

Method: 提出包含个人因果知识图谱、因果推理引擎和基于模式的规划模块的因果模式记忆（CSM）架构，用大语言模型协调各组件，引入新评估指标。

Result: 基于CSM的代理比基线LLM代理能提供更具上下文感知、贴合用户的建议。

Conclusion: 这是个性化代理中记忆增强因果推理的新方法，推动了透明可信AI生活助手的发展。

Abstract: Personalized AI assistants often struggle to incorporate complex personal
data and causal knowledge, leading to generic advice that lacks explanatory
power. We propose REMI, a Causal Schema Memory architecture for a multimodal
lifestyle agent that integrates a personal causal knowledge graph, a causal
reasoning engine, and a schema based planning module. The idea is to deliver
explainable, personalized recommendations in domains like fashion, personal
wellness, and lifestyle planning. Our architecture uses a personal causal graph
of the user's life events and habits, performs goal directed causal traversals
enriched with external knowledge and hypothetical reasoning, and retrieves
adaptable plan schemas to generate tailored action plans. A Large Language
Model orchestrates these components, producing answers with transparent causal
explanations. We outline the CSM system design and introduce new evaluation
metrics for personalization and explainability, including Personalization
Salience Score and Causal Reasoning Accuracy, to rigorously assess its
performance. Results indicate that CSM based agents can provide more context
aware, user aligned recommendations compared to baseline LLM agents. This work
demonstrates a novel approach to memory augmented, causal reasoning in
personalized agents, advancing the development of transparent and trustworthy
AI lifestyle assistants.

</details>


### [26] [TableMind: An Autonomous Programmatic Agent for Tool-Augmented Table Reasoning](https://arxiv.org/abs/2509.06278)
*Chuang Jiang,Mingyue Cheng,Xiaoyu Tao,Qingyang Mao,Jie Ouyang,Qi Liu*

Main category: cs.AI

TL;DR: 提出LLM驱动的表格推理代理TableMind，采用两阶段微调范式，含RAPO方法，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 纯文本方法在表格推理任务中处理复杂数值计算和细粒度操作有困难，现有工具集成推理系统缺乏自主适应性。

Method: 提出TableMind，采用两阶段微调范式，先监督微调，后强化微调，提出Rank - Aware Policy Optimization (RAPO)方法。

Result: 在多个主流基准测试中，TableMind比竞争基线表现更优，在推理准确性和计算精度上有显著提升。

Conclusion: TableMind在表格推理任务中具有良好性能，能有效解决现有方法存在的问题。

Abstract: Table reasoning is crucial for leveraging structured data in domains such as
finance, healthcare, and scientific research. While large language models
(LLMs) show promise in multi-step reasoning, purely text-based methods often
struggle with the complex numerical computations and fine-grained operations
inherently required in this task. Tool-integrated reasoning improves
computational accuracy via explicit code execution, yet existing systems
frequently rely on rigid patterns, supervised imitation, and lack true
autonomous adaptability. In this paper, we present TableMind, an LLM-driven
table reasoning agent that (i) autonomously performs multi-turn tool
invocation, (ii) writes and executes data-analyzing code in a secure sandbox
environment for data analysis and precise numerical reasoning, and (iii)
exhibits high-level capabilities such as planning and self-reflection to adapt
strategies. To realize these capabilities, we adopt a two-stage fine-tuning
paradigm built on top of a powerful pre-trained language model: supervised
fine-tuning on high-quality reasoning trajectories to establish effective tool
usage patterns, followed by reinforcement fine-tuning to optimize
multi-objective strategies. In particular, we propose Rank-Aware Policy
Optimization (RAPO), which increases the update weight of high-quality
trajectories when their output probabilities are lower than those of
low-quality ones, thereby guiding the model more consistently toward better and
more accurate answers. Extensive experiments on several mainstream benchmarks
demonstrate that TableMind achieves superior performance compared to
competitive baselines, yielding substantial gains in both reasoning accuracy
and computational precision.

</details>


### [27] [SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents](https://arxiv.org/abs/2509.06283)
*Xuan-Phi Nguyen,Shrey Pandit,Revanth Gangi Reddy,Austin Xu,Silvio Savarese,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 本文聚焦开发用于深度研究的原生自主单智能体模型，采用基于合成数据的连续强化学习方法提升智能体技能，最佳变体在基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 为大语言模型赋予复杂推理和工具使用能力是关键研究方向，深度研究应用需要此类能力，当前需开发自主单智能体模型。

Method: 提出基于完全合成数据的连续强化学习方法，应用于各种开源大语言模型。

Result: 最佳变体SFR - DR - 20B在Humanity's Last Exam基准测试中达到28.7%。

Conclusion: 所提出的方法能有效提升大语言模型在深度研究应用中的智能体技能，关键分析实验为方法提供了更多见解。

Abstract: Equipping large language models (LLMs) with complex, interleaved reasoning
and tool-use capabilities has become a key focus in agentic AI research,
especially with recent advances in reasoning-oriented (``thinking'') models.
Such capabilities are key to unlocking a number of important applications. One
such application is Deep Research (DR), which requires extensive search and
reasoning over many sources. Our work in this paper focuses on the development
of native Autonomous Single-Agent models for DR featuring minimal web crawling
and Python tool integration. Unlike multi-agent systems, where agents take up
pre-defined roles and are told what to do at each step in a static workflow, an
autonomous single-agent determines its next action dynamically based on
context, without manual directive. While prior work has proposed training
recipes for base or instruction-tuned LLMs, we focus on continual reinforcement
learning (RL) of reasoning-optimized models to further enhance agentic skills
while preserving reasoning ability. Towards this end, we propose a simple RL
recipe with entirely synthetic data, which we apply to various open-source
LLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam
benchmark. In addition, we conduct key analysis experiments to provide more
insights into our methodologies.

</details>


### [28] [From Implicit Exploration to Structured Reasoning: Leveraging Guideline and Refinement for LLMs](https://arxiv.org/abs/2509.06284)
*Jiaxiang Chen,Zhuo Wang,Mingxi Zou,Zhucong Li,Zhijian Zhou,Song Wang,Zenglin Xu*

Main category: cs.AI

TL;DR: 现有大语言模型推理方法依赖隐式探索，存在问题。本文提出结构化推理框架，实验表明该方法在多基准测试中优于基线，提升了稳定性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理方法依赖隐式探索，导致推理路径不稳定、缺乏纠错能力和难以从过往经验学习，需改进。

Method: 从成功轨迹中提取结构化推理模式，从失败中提取反思信号，推理时模型按指南逐步执行，并在每步后进行细化纠错。

Result: 在BBH等多个基准测试中，该方法始终优于强基线模型。

Conclusion: 结构化推理结合逐步执行和细化可提高稳定性和泛化性，指南跨领域迁移性好，支持跨模型协作，在有效性和可扩展性上与或超越有监督微调。

Abstract: Large language models (LLMs) have advanced general-purpose reasoning, showing
strong performance across diverse tasks. However, existing methods often rely
on implicit exploration, where the model follows stochastic and unguided
reasoning paths-like walking without a map. This leads to unstable reasoning
paths, lack of error correction, and limited learning from past experience. To
address these issues, we propose a framework that shifts from implicit
exploration to structured reasoning through guideline and refinement. First, we
extract structured reasoning patterns from successful trajectories and
reflective signals from failures. During inference, the model follows these
guidelines step-by-step, with refinement applied after each step to correct
errors and stabilize the reasoning process. Experiments on BBH and four
additional benchmarks (GSM8K, MATH-500, MBPP, HumanEval) show that our method
consistently outperforms strong baselines across diverse reasoning tasks.
Structured reasoning with stepwise execution and refinement improves stability
and generalization, while guidelines transfer well across domains and flexibly
support cross-model collaboration, matching or surpassing supervised
fine-tuning in effectiveness and scalability.

</details>


### [29] [Can AI Make Energy Retrofit Decisions? An Evaluation of Large Language Models](https://arxiv.org/abs/2509.06307)
*Lei Shu,Dong Zhao*

Main category: cs.AI

TL;DR: 评估七种大语言模型用于住宅能源改造决策的表现，发现其有潜力但需提升准确性、一致性和上下文处理能力。


<details>
  <summary>Details</summary>
Motivation: 传统建筑能源改造决策方法通用性和可解释性有限，大语言模型可处理上下文信息并生成可读建议。

Method: 评估七种大语言模型在最大化二氧化碳减排和最小化投资回收期两个目标下的住宅改造决策表现，从准确性、一致性、敏感性和推理四个维度，用400个美国家庭数据集评估。

Result: 大语言模型在很多情况下能生成有效建议，技术目标表现更好，各模型一致性低，对位置和建筑几何敏感，推理常简化且缺乏深度上下文感知。

Conclusion: 大语言模型是能源改造决策的有前途助手，但需改进以可靠应用。

Abstract: Conventional approaches to building energy retrofit decision making suffer
from limited generalizability and low interpretability, hindering adoption in
diverse residential contexts. With the growth of Smart and Connected
Communities, generative AI, especially large language models (LLMs), may help
by processing contextual information and producing practitioner readable
recommendations. We evaluate seven LLMs (ChatGPT, DeepSeek, Gemini, Grok,
Llama, and Claude) on residential retrofit decisions under two objectives:
maximizing CO2 reduction (technical) and minimizing payback period
(sociotechnical). Performance is assessed on four dimensions: accuracy,
consistency, sensitivity, and reasoning, using a dataset of 400 homes across 49
US states. LLMs generate effective recommendations in many cases, reaching up
to 54.5 percent top 1 match and 92.8 percent within top 5 without fine tuning.
Performance is stronger for the technical objective, while sociotechnical
decisions are limited by economic trade offs and local context. Agreement
across models is low, and higher performing models tend to diverge from others.
LLMs are sensitive to location and building geometry but less sensitive to
technology and occupant behavior. Most models show step by step, engineering
style reasoning, but it is often simplified and lacks deeper contextual
awareness. Overall, LLMs are promising assistants for energy retrofit decision
making, but improvements in accuracy, consistency, and context handling are
needed for reliable practice.

</details>


### [30] [Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation](https://arxiv.org/abs/2509.06337)
*Jianpeng Zhao,Chenyu Yuan,Weiming Luo,Haoling Xie,Guangwei Zhang,Steven Jige Quan,Zixuan Yuan,Pengyang Wang,Denghui Zhang*

Main category: cs.AI

TL;DR: 本文探索用大语言模型模拟虚拟调查受访者，介绍两种模拟设置，构建基准套件评估主流大模型，为大模型驱动的调查模拟奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统调查方法成本高、耗时长且规模受限，需要新的调查范式。

Method: 引入PAS和FAS两种模拟设置，构建LLM - S^3基准套件评估主流大模型。

Result: 评估揭示了预测性能的一致趋势，指出了失败模式，展示了上下文和提示设计对模拟保真度的影响。

Conclusion: 为大模型驱动的调查模拟建立了严格基础，提供了可扩展且经济高效的工具。

Abstract: Questionnaire-based surveys are foundational to social science research and
public policymaking, yet traditional survey methods remain costly,
time-consuming, and often limited in scale. This paper explores a new paradigm:
simulating virtual survey respondents using Large Language Models (LLMs). We
introduce two novel simulation settings, namely Partial Attribute Simulation
(PAS) and Full Attribute Simulation (FAS), to systematically evaluate the
ability of LLMs to generate accurate and demographically coherent responses. In
PAS, the model predicts missing attributes based on partial respondent
profiles, whereas FAS involves generating complete synthetic datasets under
both zero-context and context-enhanced conditions. We curate a comprehensive
benchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey
Simulation), that spans 11 real-world public datasets across four sociological
domains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA
3.0/3.1-8B) reveals consistent trends in prediction performance, highlights
failure modes, and demonstrates how context and prompt design impact simulation
fidelity. This work establishes a rigorous foundation for LLM-driven survey
simulations, offering scalable and cost-effective tools for sociological
research and policy evaluation. Our code and dataset are available at:
https://github.com/dart-lab-research/LLM-S-Cube-Benchmark

</details>


### [31] [Evaluating Multi-Turn Bargain Skills in LLM-Based Seller Agent](https://arxiv.org/abs/2509.06341)
*Issue Yishu Wang,Kakam Chong,Xiaofeng Wang,Xu Yan,DeXin Kong,Chen Ju,Ming Chen,Shuai Xiao,Shuguang Han,jufeng chen*

Main category: cs.AI

TL;DR: 本文介绍电商对话中卖家代理议价能力的多轮评估框架，还给出相关基准、评估框架和自动流程。


<details>
  <summary>Details</summary>
Motivation: 在线二手市场中，大语言模型作为卖家代理需追踪和解读买家意图，当前缺乏衡量卖家代理议价能力的方法。

Method: 引入多轮评估框架，创建大规模电商议价基准，构建基于心智理论的轮级评估框架，开发自动提取意图的流程。

Result: 得到大规模电商议价基准、基于ToM的轮级评估框架和自动提取意图的流程。

Conclusion: 该评估框架可衡量卖家代理在电商对话中的议价能力。

Abstract: In online second-hand marketplaces, multi-turn bargaining is a crucial part
of seller-buyer interactions. Large Language Models (LLMs) can act as seller
agents, negotiating with buyers on behalf of sellers under given business
constraints. A critical ability for such agents is to track and accurately
interpret cumulative buyer intents across long negotiations, which directly
impacts bargaining effectiveness. We introduce a multi-turn evaluation
framework for measuring the bargaining ability of seller agents in e-commerce
dialogues. The framework tests whether an agent can extract and track buyer
intents. Our contributions are: (1) a large-scale e-commerce bargaining
benchmark spanning 622 categories, 9,892 products, and 3,014 tasks; (2) a
turn-level evaluation framework grounded in Theory of Mind (ToM) with annotated
buyer intents, moving beyond outcome-only metrics; and (3) an automated
pipeline that extracts reliable intent from massive dialogue data.

</details>


### [32] [A data-driven discretized CS:GO simulation environment to facilitate strategic multi-agent planning research](https://arxiv.org/abs/2509.06355)
*Yunzhe Wang,Volkan Ustun,Chris McGroarty*

Main category: cs.AI

TL;DR: 提出多智能体模拟器DECOY，以CS:GO为测试平台，简化状态与动作，评估显示其重放与原游戏匹配，提供研究工具。


<details>
  <summary>Details</summary>
Motivation: 现代复杂多智能体交互模拟环境需平衡高保真度细节与计算效率。

Method: 将3D地形中的战略规划抽象为高级离散模拟，采用航点系统简化和离散化状态与动作，结合基于真实比赛数据训练的神经预测和生成模型。

Result: DECOY根据人类数据生成的重放与原游戏观察到的情况紧密匹配。

Conclusion: 公开的模拟环境为战略多智能体规划和行为生成研究提供了有价值的工具。

Abstract: Modern simulation environments for complex multi-agent interactions must
balance high-fidelity detail with computational efficiency. We present DECOY, a
novel multi-agent simulator that abstracts strategic, long-horizon planning in
3D terrains into high-level discretized simulation while preserving low-level
environmental fidelity. Using Counter-Strike: Global Offensive (CS:GO) as a
testbed, our framework accurately simulates gameplay using only movement
decisions as tactical positioning -- without explicitly modeling low-level
mechanics such as aiming and shooting. Central to our approach is a waypoint
system that simplifies and discretizes continuous states and actions, paired
with neural predictive and generative models trained on real CS:GO tournament
data to reconstruct event outcomes. Extensive evaluations show that replays
generated from human data in DECOY closely match those observed in the original
game. Our publicly available simulation environment provides a valuable tool
for advancing research in strategic multi-agent planning and behavior
generation.

</details>


### [33] [Teaching AI Stepwise Diagnostic Reasoning with Report-Guided Chain-of-Thought Learning](https://arxiv.org/abs/2509.06409)
*Yihong Luo,Wenwu He,Zhuo-Xu Cui,Dong Liang*

Main category: cs.AI

TL;DR: 提出DiagCoT框架，应用于通用VLM，在MIMIC - CXR基准上提升多项指标，优于现有模型，提供可扩展方法开发放射学AI系统。


<details>
  <summary>Details</summary>
Motivation: 开发能模拟放射科医生逐步诊断推理的AI系统，利用自由文本报告提升诊断能力。

Method: 对通用VLM进行监督微调，结合对比图像 - 报告调整、思维链监督和强化调整。

Result: 在MIMIC - CXR基准上，零样本疾病分类AUC从0.52提升到0.76，病理定位mIoU从0.08提升到0.31，报告生成BLEU从0.11提升到0.33，在长尾疾病和外部数据集上优于现有模型。

Conclusion: DiagCoT将非结构化临床叙述转化为结构化监督，为开发可解释且具有诊断能力的放射学AI系统提供了可扩展方法。

Abstract: This study presents DiagCoT, a multi-stage framework that applies supervised
fine-tuning to general-purpose vision-language models (VLMs) to emulate
radiologists' stepwise diagnostic reasoning using only free-text reports.
DiagCoT combines contrastive image-report tuning for domain alignment,
chain-of-thought supervision to capture inferential logic, and reinforcement
tuning with clinical reward signals to enhance factual accuracy and fluency. On
the MIMIC-CXR benchmark, DiagCoT improved zero-shot disease classification AUC
from 0.52 to 0.76 (absolute gain of 0.24), pathology grounding mIoU from 0.08
to 0.31 (absolute gain of 0.23), and report generation BLEU from 0.11 to 0.33
(absolute gain of 0.22). It outperformed state-of-the-art models including
LLaVA-Med and CXR-LLAVA on long-tailed diseases and external datasets. By
converting unstructured clinical narratives into structured supervision,
DiagCoT offers a scalable approach for developing interpretable and
diagnostically competent AI systems for radiology.

</details>


### [34] [Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning](https://arxiv.org/abs/2509.06436)
*Song Yu,Xiaofei Xu,Ke Deng,Li Li,Lin Tian*

Main category: cs.AI

TL;DR: 提出Tree of Agents (TOA)多智能体推理框架解决大语言模型处理长上下文任务的问题，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理长上下文任务存在‘中间信息丢失’问题，现有减少输入或扩展上下文窗口的方法有局限。

Method: 提出TOA框架，将输入分割成块由独立智能体处理，智能体沿树状路径动态交换信息进行协作推理，还结合前缀哈希缓存和自适应剪枝策略。

Result: 由紧凑的LLaMA3.1 - 8B驱动的TOA显著优于多个基线模型，在长上下文任务上与最新且更大的商业模型（如Gemini1.5 - pro）表现相当。

Conclusion: TOA能有效缓解位置偏差和减少幻觉，提高处理效率，在长上下文任务上有良好表现。

Abstract: Large language models (LLMs) face persistent challenges when handling
long-context tasks, most notably the lost in the middle issue, where
information located in the middle of a long input tends to be underutilized.
Some existing methods that reduce input have the risk of discarding key
information, while others that extend context windows often lead to attention
dispersion. To address these limitations, we propose Tree of Agents (TOA), a
multi-agent reasoning framework that segments the input into chunks processed
by independent agents. Each agent generates its local cognition, then agents
dynamically exchange information for collaborative reasoning along
tree-structured paths. TOA enables agents to probe different reasoning orders
for multi-perspective understanding, effectively mitigating position bias and
reducing hallucinations. To improve processing efficiency, we incorporate
prefix-hash caching and adaptive pruning strategies, achieving significant
performance improvements with comparable API overhead. Experiments show that
TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple
baselines and demonstrates comparable performance to the latest and much larger
commercial models, such as Gemini1.5-pro, on various long-context tasks. Code
is available at https://github.com/Aireduce952/Tree-of-Agents.

</details>


### [35] [HyFedRAG: A Federated Retrieval-Augmented Generation Framework for Heterogeneous and Privacy-Sensitive Data](https://arxiv.org/abs/2509.06444)
*Cheng Qian,Hainan Zhang,Yongxin Tong,Hong-Wei Zheng,Zhiming Zheng*

Main category: cs.AI

TL;DR: 提出HyFedRAG框架解决集中式RAG处理异构和隐私敏感数据问题，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 集中式RAG管道在处理异构和隐私敏感数据存在困难，传统云RAG系统有局限，临床医生检索罕见病病例难。

Method: 设计基于Flower的边缘 - 云协作RAG框架，集成轻量级本地检索器和隐私感知大模型，提供匿名化工具，设计三层缓存策略。

Result: 在PMC - Patients上实验显示，HyFedRAG在检索质量、生成一致性和系统效率方面优于现有基线。

Conclusion: HyFedRAG为处理结构化 - 异构数据的RAG提供了可扩展且符合隐私要求的解决方案，挖掘了大模型在敏感和多样数据环境中的潜力。

Abstract: Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive
data, especially in distributed healthcare settings where patient data spans
SQL, knowledge graphs, and clinical notes. Clinicians face difficulties
retrieving rare disease cases due to privacy constraints and the limitations of
traditional cloud-based RAG systems in handling diverse formats and edge
devices. To address this, we introduce HyFedRAG, a unified and efficient
Federated RAG framework tailored for Hybrid data modalities. By leveraging an
edge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across
diverse data sources while preserving data privacy. Our key contributions are:
(1) We design an edge-cloud collaborative RAG framework built on Flower, which
supports querying structured SQL data, semi-structured knowledge graphs, and
unstructured documents. The edge-side LLMs convert diverse data into
standardized privacy-preserving representations, and the server-side LLMs
integrates them for global reasoning and generation. (2) We integrate
lightweight local retrievers with privacy-aware LLMs and provide three
anonymization tools that enable each client to produce semantically rich,
de-identified summaries for global inference across devices. (3) To optimize
response latency and reduce redundant computation, we design a three-tier
caching strategy consisting of local cache, intermediate representation cache,
and cloud inference cache. Experimental results on PMC-Patients demonstrate
that HyFedRAG outperforms existing baselines in terms of retrieval quality,
generation consistency, and system efficiency. Our framework offers a scalable
and privacy-compliant solution for RAG over structural-heterogeneous data,
unlocking the potential of LLMs in sensitive and diverse data environments.

</details>


### [36] [Accelerate Scaling of LLM Alignment via Quantifying the Coverage and Depth of Instruction Set](https://arxiv.org/abs/2509.06463)
*Chengwei Wu,Li Du,Hanyu Zhao,Yiming Ju,Jiapu Wang,Tengfei Pan*

Main category: cs.AI

TL;DR: 本文探讨指令数据集分布与对齐模型性能关系，提出新的指令数据选择方法，能加速提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用需求增长，现有指令集细化方法在指令池不断扩大时无法提升性能，关键影响因素不明。

Method: 研究影响指令数据集分布和对齐模型性能关系的关键因素，确定指令深度和语义空间覆盖率为关键因素，设计指令选择算法以最大化二者。

Result: 所提方法能解释开发集上超70%的模型损失，相比基线方法能更快持续提升模型性能。

Conclusion: 提出的方法可实现“加速扩展”，有效提升模型性能。

Abstract: With the growing demand for applying large language models to downstream
tasks, improving model alignment performance and efficiency has become crucial.
Such a process involves selecting informative instructions from a candidate
pool. However, due to the complexity of instruction set distributions, the key
factors driving the performance of aligned models remain unclear. As a result,
current instruction set refinement methods fail to improve performance as the
instruction pool expands continuously. To address this issue, we first
investigate the key factors that influence the relationship between instruction
dataset distribution and aligned model performance. Based on these insights, we
propose a novel instruction data selection method. We identify that the depth
of instructions and the coverage of the semantic space are the crucial factors
determining downstream performance, which could explain over 70\% of the model
loss on the development set. We then design an instruction selection algorithm
to simultaneously maximize the depth and semantic coverage of the selected
instructions. Experimental results demonstrate that, compared to
state-of-the-art baseline methods, it can sustainably improve model performance
at a faster pace and thus achieve \emph{``Accelerated Scaling''}.

</details>


### [37] [MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents](https://arxiv.org/abs/2509.06477)
*Pengxiang Zhao,Guangyi Liu,Yaozhen Liang,Weiqing He,Zhengxi Lu,Yuehao Huang,Yaxuan Guo,Kexin Zhang,Hao Wang,Liang Liu,Yong Liu*

Main category: cs.AI

TL;DR: 介绍MAS - Bench基准，用于评估GUI - 快捷方式混合代理，实验表明混合代理更优，该基准填补评估空白。


<details>
  <summary>Details</summary>
Motivation: 缺乏系统评估GUI - 快捷方式混合代理的框架，旨在填补这一空白。

Method: 引入MAS - Bench基准，含139个复杂任务、88个预定义快捷方式知识库和7个评估指标。

Result: 混合代理比仅使用GUI的代理有更高成功率和效率，证明评估代理快捷方式生成能力方法有效。

Conclusion: MAS - Bench填补关键评估空白，为创建更高效强大智能代理提供基础平台。

Abstract: To enhance the efficiency of GUI agents on various platforms like smartphones
and computers, a hybrid paradigm that combines flexible GUI operations with
efficient shortcuts (e.g., API, deep links) is emerging as a promising
direction. However, a framework for systematically benchmarking these hybrid
agents is still underexplored. To take the first step in bridging this gap, we
introduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut
hybrid agents with a specific focus on the mobile domain. Beyond merely using
predefined shortcuts, MAS-Bench assesses an agent's capability to autonomously
generate shortcuts by discovering and creating reusable, low-cost workflows. It
features 139 complex tasks across 11 real-world applications, a knowledge base
of 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation
metrics. The tasks are designed to be solvable via GUI-only operations, but can
be significantly accelerated by intelligently embedding shortcuts. Experiments
show that hybrid agents achieve significantly higher success rates and
efficiency than their GUI-only counterparts. This result also demonstrates the
effectiveness of our method for evaluating an agent's shortcut generation
capabilities. MAS-Bench fills a critical evaluation gap, providing a
foundational platform for future advancements in creating more efficient and
robust intelligent agents.

</details>


### [38] [MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization](https://arxiv.org/abs/2509.06490)
*Niki Kotecha,Ehecatl Antonio del Rio Chanona*

Main category: cs.AI

TL;DR: 提出结合强化学习和多目标进化算法的方法解决供应链动态多目标优化问题，引入CVaR考虑风险，案例证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 传统多目标优化方法难以适应供应链动态性，需要新方法解决供应链动态多目标优化问题。

Method: 结合强化学习和多目标进化算法，用多目标进化算法搜索策略神经网络参数空间生成策略帕累托前沿，引入条件风险价值考虑风险决策。

Result: 通过案例研究证明该方法能响应供应链动态，在库存管理案例中优于现有方法。

Conclusion: 该策略提高决策效率，为供应链管理不确定性和优化性能提供更稳健框架。

Abstract: In supply chain management, decision-making often involves balancing multiple
conflicting objectives, such as cost reduction, service level improvement, and
environmental sustainability. Traditional multi-objective optimization methods,
such as linear programming and evolutionary algorithms, struggle to adapt in
real-time to the dynamic nature of supply chains. In this paper, we propose an
approach that combines Reinforcement Learning (RL) and Multi-Objective
Evolutionary Algorithms (MOEAs) to address these challenges for dynamic
multi-objective optimization under uncertainty. Our method leverages MOEAs to
search the parameter space of policy neural networks, generating a Pareto front
of policies. This provides decision-makers with a diverse population of
policies that can be dynamically switched based on the current system
objectives, ensuring flexibility and adaptability in real-time decision-making.
We also introduce Conditional Value-at-Risk (CVaR) to incorporate
risk-sensitive decision-making, enhancing resilience in uncertain environments.
We demonstrate the effectiveness of our approach through case studies,
showcasing its ability to respond to supply chain dynamics and outperforming
state-of-the-art methods in an inventory management case study. The proposed
strategy not only improves decision-making efficiency but also offers a more
robust framework for managing uncertainty and optimizing performance in supply
chains.

</details>


### [39] [Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers](https://arxiv.org/abs/2509.06493)
*Ran Xin,Zeyu Zheng,Yanchen Nie,Kun Yuan,Xia Xiao*

Main category: cs.AI

TL;DR: 本文介绍BFS - Prover - V2系统解决大语言模型集成到自动定理证明中的扩展问题，有训练时和推理时的创新，在基准测试取得SOTA结果，技术有更广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型集成到自动定理证明受训练时强化学习和推理时计算扩展挑战的限制，需要解决该问题。

Method: 提出多轮离策略强化学习框架在训练时提升性能，采用规划器增强的多智能体搜索架构在推理时扩展推理能力。

Result: BFS - Prover - V2在MiniF2F和ProofNet测试集分别达到95.08%和41.4%。

Conclusion: 提出的解决扩展问题的双重方法在形式数学基准测试取得SOTA结果，相关技术可用于其他需要长程多轮推理和复杂搜索的领域。

Abstract: The integration of Large Language Models (LLMs) into automated theorem
proving has shown immense promise, yet is fundamentally constrained by
challenges in scaling up both training-time reinforcement learning (RL) and
inference-time compute. This paper introduces \texttt{BFS-Prover-V2}, a system
designed to address this dual scaling problem. We present two primary
innovations. The first is a novel multi-turn off-policy RL framework for
continually improving the performance of LLM step-prover at training time. This
framework, inspired by the principles of AlphaZero, utilizes a multi-stage
expert iteration pipeline featuring adaptive tactic-level data filtering and
periodic retraining to surmount the performance plateaus that typically curtail
long-term RL in LLM-based agents. The second innovation is a planner-enhanced
multi-agent search architecture that scales reasoning capabilities at inference
time. This architecture employs a general reasoning model as a high-level
planner to iteratively decompose complex theorems into a sequence of simpler
subgoals. This hierarchical approach substantially reduces the search space,
enabling a team of parallel prover agents to collaborate efficiently by
leveraging a shared proof cache. We demonstrate that this dual approach to
scaling yields state-of-the-art results on established formal mathematics
benchmarks. \texttt{BFS-Prover-V2} achieves 95.08\% and 41.4\% on the MiniF2F
and ProofNet test sets respectively. While demonstrated in the domain of formal
mathematics, the RL and inference techniques presented in this work are of
broader interest and may be applied to other domains requiring long-horizon
multi-turn reasoning and complex search.

</details>


### [40] [An AI system to help scientists write expert-level empirical software](https://arxiv.org/abs/2509.06503)
*Eser Aygün,Anastasiya Belyaeva,Gheorghe Comanici,Marc Coram,Hao Cui,Jake Garrison,Renee Johnston Anton Kast,Cory Y. McLean,Peter Norgaard,Zahra Shamsi,David Smalling,James Thompson,Subhashini Venugopalan,Brian P. Williams,Chujun He,Sarah Martinson,Martyna Plomecka,Lai Wei,Yuchen Zhou,Qian-Ze Zhu,Matthew Abraham,Erica Brand,Anna Bulanova,Jeffrey A. Cardille,Chris Co,Scott Ellsworth,Grace Joseph,Malcolm Kane,Ryan Krueger,Johan Kartiwa,Dan Liebling,Jan-Matthis Lueckmann,Paul Raccuglia,Xuefei,Wang,Katherine Chou,James Manyika,Yossi Matias,John C. Platt,Lizzie Dorfman,Shibl Mourad,Michael P. Brenner*

Main category: cs.AI

TL;DR: 提出一个AI系统创建科学软件，用LLM和TS改进质量指标，在多领域获专家级成果，加速科学进展。


<details>
  <summary>Details</summary>
Motivation: 解决科学发现周期中因手动创建软件缓慢导致的瓶颈问题。

Method: 使用大语言模型（LLM）和树搜索（TS）系统地改进质量指标，智能探索解决方案空间。

Result: 在生物信息学、流行病学等多个领域取得专家级成果，如发现新方法、生成更优模型等。

Conclusion: 该系统通过为不同任务设计和实施新解决方案，对加速科学进步有重要意义。

Abstract: The cycle of scientific discovery is frequently bottlenecked by the slow,
manual creation of software to support computational experiments. To address
this, we present an AI system that creates expert-level scientific software
whose goal is to maximize a quality metric. The system uses a Large Language
Model (LLM) and Tree Search (TS) to systematically improve the quality metric
and intelligently navigate the large space of possible solutions. The system
achieves expert-level results when it explores and integrates complex research
ideas from external sources. The effectiveness of tree search is demonstrated
across a wide range of benchmarks. In bioinformatics, it discovered 40 novel
methods for single-cell data analysis that outperformed the top human-developed
methods on a public leaderboard. In epidemiology, it generated 14 models that
outperformed the CDC ensemble and all other individual models for forecasting
COVID-19 hospitalizations. Our method also produced state-of-the-art software
for geospatial analysis, neural activity prediction in zebrafish, time series
forecasting and numerical solution of integrals. By devising and implementing
novel solutions to diverse tasks, the system represents a significant step
towards accelerating scientific progress.

</details>


### [41] [CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning](https://arxiv.org/abs/2509.06641)
*Zhou-Peng Shou,Zhi-Qiang You,Fang Wang,Hai-Bo Liu*

Main category: cs.AI

TL;DR: 本文提出以“意图草图”为核心、受类人认知策略引导的零样本多模态推理组件，经实验验证其有效性与实用性。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大模型复杂跨模态推理中“捷径”和上下文理解不足的问题。

Method: 提出由意图感知器、策略生成器和策略选择器组成的即插即用三模块管道，构建“理解 - 规划 - 选择”认知过程，通过生成和过滤“意图草图”策略引导最终推理，仅通过上下文工程实现跨模型迁移。

Result: 信息论分析表明该过程可降低条件熵、提高信息利用效率，抑制意外捷径推理；在IntentBench等数据集实验显示，完整“三模块”方案在不同推理引擎和管道组合上有一致提升，最高约9.51个百分点。

Conclusion: “意图草图”推理组件在零样本场景有实用价值和可移植性。

Abstract: Targeting the issues of "shortcuts" and insufficient contextual understanding
in complex cross-modal reasoning of multimodal large models, this paper
proposes a zero-shot multimodal reasoning component guided by human-like
cognitive strategies centered on an "intent sketch". The component comprises a
plug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and
Strategy Selector-that explicitly constructs a "understand-plan-select"
cognitive process. By generating and filtering "intent sketch" strategies to
guide the final reasoning, it requires no parameter fine-tuning and achieves
cross-model transfer solely through in-context engineering.
Information-theoretic analysis shows that this process can reduce conditional
entropy and improve information utilization efficiency, thereby suppressing
unintended shortcut reasoning. Experiments on IntentBench, WorldSense, and
Daily-Omni validate the method's generality and robust gains; compared with
their respective baselines, the complete "three-module" scheme yields
consistent improvements across different reasoning engines and pipeline
combinations, with gains up to approximately 9.51 percentage points,
demonstrating the practical value and portability of the "intent sketch"
reasoning component in zero-shot scenarios.

</details>


### [42] [Reinforcement Learning Foundations for Deep Research Systems: A Survey](https://arxiv.org/abs/2509.06733)
*Wenjun Li,Zhi Chen,Jingru Lin,Hannan Cao,Wei Han,Sheng Liang,Zhi Zhang,Kuicai Dong,Dexun Li,Chen Zhang,Yong Liu*

Main category: cs.AI

TL;DR: 本文聚焦深度研究系统的强化学习基础，系统梳理相关工作并给出实用指导。


<details>
  <summary>Details</summary>
Motivation: 现有方法如SFT和DPO存在局限，需强化学习优化深度研究系统。

Method: 沿数据合成与管理、研究用RL方法、训练系统与框架三个轴系统梳理工作。

Result: 提炼出重复模式，发现基础设施瓶颈。

Conclusion: 提供了用强化学习训练健壮、透明深度研究代理的实用指导。

Abstract: Deep research systems, agentic AI that solve complex, multi-step tasks by
coordinating reasoning, search across the open web and user files, and tool
use, are moving toward hierarchical deployments with a Planner, Coordinator,
and Executors. In practice, training entire stacks end-to-end remains
impractical, so most work trains a single planner connected to core tools such
as search, browsing, and code. While SFT imparts protocol fidelity, it suffers
from imitation and exposure biases and underuses environment feedback.
Preference alignment methods such as DPO are schema and proxy-dependent,
off-policy, and weak for long-horizon credit assignment and multi-objective
trade-offs. A further limitation of SFT and DPO is their reliance on human
defined decision points and subskills through schema design and labeled
comparisons. Reinforcement learning aligns with closed-loop, tool-interaction
research by optimizing trajectory-level policies, enabling exploration,
recovery behaviors, and principled credit assignment, and it reduces dependence
on such human priors and rater biases.
  This survey is, to our knowledge, the first dedicated to the RL foundations
of deep research systems. It systematizes work after DeepSeek-R1 along three
axes: (i) data synthesis and curation; (ii) RL methods for agentic research
covering stability, sample efficiency, long context handling, reward and credit
design, multi-objective optimization, and multimodal integration; and (iii)
agentic RL training systems and frameworks. We also cover agent architecture
and coordination, as well as evaluation and benchmarks, including recent QA,
VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We
distill recurring patterns, surface infrastructure bottlenecks, and offer
practical guidance for training robust, transparent deep research agents with
RL.

</details>


### [43] [VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction](https://arxiv.org/abs/2509.06736)
*Jie Yang,Jiajun Chen,Zhangyue Yin,Shuo Chen,Yuxin Wang,Yiran Guo,Yuan Li,Yining Zheng,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: 介绍VehicleWorld环境，提出SFC方法，实验显示SFC优于传统FC方法，代码开源


<details>
  <summary>Details</summary>
Motivation: 智能汽车座舱对API代理有独特挑战，传统FC方法低效且错误恢复能力有限

Method: 引入VehicleWorld环境，经分析提出SFC方法，该方法维护系统状态并实现直接状态转换

Result: SFC显著优于传统FC方法，执行精度更高、延迟更低

Conclusion: SFC在智能汽车座舱场景更有效，可作为替代传统FC的方案

Abstract: Intelligent vehicle cockpits present unique challenges for API Agents,
requiring coordination across tightly-coupled subsystems that exceed typical
task environments' complexity. Traditional Function Calling (FC) approaches
operate statelessly, requiring multiple exploratory calls to build
environmental awareness before execution, leading to inefficiency and limited
error recovery. We introduce VehicleWorld, the first comprehensive environment
for the automotive domain, featuring 30 modules, 250 APIs, and 680 properties
with fully executable implementations that provide real-time state information
during agent execution. This environment enables precise evaluation of vehicle
agent behaviors across diverse, challenging scenarios. Through systematic
analysis, we discovered that direct state prediction outperforms function
calling for environmental control. Building on this insight, we propose
State-based Function Call (SFC), a novel approach that maintains explicit
system state awareness and implements direct state transitions to achieve
target conditions. Experimental results demonstrate that SFC significantly
outperforms traditional FC approaches, achieving superior execution accuracy
and reduced latency. We have made all implementation code publicly available on
Github https://github.com/OpenMOSS/VehicleWorld.

</details>


### [44] [Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting](https://arxiv.org/abs/2509.06770)
*Shashidhar Reddy Javaji,Bhavul Gauri,Zining Zhu*

Main category: cs.AI

TL;DR: 提出评估迭代优化的框架，发现迭代收益因领域而异，框架和指标使迭代可测量和比较。


<details>
  <summary>Details</summary>
Motivation: 缺乏衡量大语言模型在多轮工作流中迭代何时有益、何时有害的方法。

Method: 提出涵盖构思、代码和数学的迭代优化评估框架，进行12轮对话，使用不同提示，用合适方法评分，用三类指标跟踪每轮行为。

Result: 迭代收益因领域而异，模糊反馈后期效果不佳，特定提示能可靠提升质量，各领域有一致模式。

Conclusion: 框架和指标使迭代可测量和比较，能指导何时引导、停止或切换策略。

Abstract: Large language models (LLMs) are now used in multi-turn workflows, but we
still lack a clear way to measure when iteration helps and when it hurts. We
present an evaluation framework for iterative refinement that spans ideation,
code, and math. Our protocol runs controlled 12-turn conversations per task,
utilizing a variety of prompts ranging from vague ``improve it'' feedback to
targeted steering, and logs per-turn outputs. We score outcomes with
domain-appropriate checks (unit tests for code; answer-equivalence plus
reasoning-soundness for math; originality and feasibility for ideation) and
track turn-level behavior with three families of metrics: semantic movement
across turns, turn-to-turn change, and output size growth. Across models and
tasks, gains are domain-dependent: they arrive early in ideas and code, but in
math late turns matter when guided by elaboration. After the first few turns,
vague feedback often plateaus or reverses correctness, while targeted prompts
reliably shift the intended quality axis (novelty vs. feasibility in ideation;
speed vs. readability in code; in math, elaboration outperforms exploration and
drives late-turn gains). We also observe consistent domain patterns: ideation
moves more in meaning across turns, code tends to grow in size with little
semantic change, and math starts fixed but can break that path with late,
elaborative iteration.Together, the framework and metrics make iteration
measurable and comparable across models, and signal when to steer, stop, or
switch strategies.

</details>


### [45] [RAFFLES: Reasoning-based Attribution of Faults for LLM Systems](https://arxiv.org/abs/2509.06822)
*Chenyang Zhu,Spencer Hong,Jingyu Wu,Kushal Chawla,Charlotte Tang,Youbing Yin,Nathan Wolfe,Erin Babinsky,Daben Liu*

Main category: cs.AI

TL;DR: 现有大语言模型代理系统评估能力有限，本文提出RAFFLES评估架构，测试显示其优于基线，向自动化故障检测迈进。


<details>
  <summary>Details</summary>
Motivation: 当前长时多组件大语言模型代理系统开发遇阻，难以识别故障位置和原因，现有评估能力存在局限。

Method: 提出RAFFLES评估架构，作为迭代多组件管道，用中央法官系统调查故障，专业评估器评估组件和法官推理质量，构建假设历史。

Result: 在Who&When数据集上测试，RAFFLES优于基线，算法生成数据集准确率超43%，手工数据集超20%。

Conclusion: RAFFLES成果是自主系统自动化故障检测替代人工审查的关键一步。

Abstract: We have reached a critical roadblock in the development and enhancement of
long-horizon, multi-component LLM agentic systems: it is incredibly tricky to
identify where these systems break down and why. Evaluation capabilities that
currently exist today (e.g., single pass LLM-as-a-judge) are limited in that
they often focus on individual metrics or capabilities, end-to-end outcomes,
and are narrowly grounded on the preferences of humans. We argue that to match
the agentic capabilities, evaluation frameworks must also be able to reason,
probe, iterate, and understand the complex logic passing through these systems
over long horizons. In this paper, we present RAFFLES - an evaluation
architecture that incorporates reasoning and iterative refinement.
Specifically, RAFFLES operates as an iterative, multi-component pipeline, using
a central Judge to systematically investigate faults and a set of specialized
Evaluators to assess not only the system's components but also the quality of
the reasoning by the Judge itself, thereby building a history of hypotheses. We
tested RAFFLES against several baselines on the Who&When dataset, a benchmark
designed to diagnose the "who" (agent) and "when" (step) of a system's failure.
RAFFLES outperforms these baselines, achieving an agent-step fault pair
accuracy of over 43% on the Algorithmically-Generated dataset (a substantial
increase from the previously published best of 16.6%) and over 20% on the
Hand-Crafted dataset (surpassing the previously published best of 8.8%). These
results demonstrate a key step towards introducing automated fault detection
for autonomous systems over labor-intensive manual human review.

</details>


### [46] [Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet](https://arxiv.org/abs/2509.06861)
*James Xu Zhao,Bryan Hooi,See-Kiong Ng*

Main category: cs.AI

TL;DR: 研究发现测试时扩展推理在知识密集型任务中效果不佳，虽有局限但思考仍有益。


<details>
  <summary>Details</summary>
Motivation: 指出测试时扩展推理在知识密集型任务中效果待验证，开展研究。

Method: 用12个推理模型在两个知识密集型基准上全面评估测试时扩展推理。

Result: 增加测试时计算量未必提高准确率，常导致更多幻觉；分析扩展推理对幻觉行为的影响。

Conclusion: 扩展推理有局限，但相比不思考，允许思考仍有益。

Abstract: Test-time scaling increases inference-time computation by allowing models to
generate long reasoning chains, and has shown strong performance across many
domains. However, in this work, we show that this approach is not yet effective
for knowledge-intensive tasks, where high factual accuracy and low
hallucination rates are essential. We conduct a comprehensive evaluation of
test-time scaling using 12 reasoning models on two knowledge-intensive
benchmarks. Our results reveal that increasing test-time computation does not
consistently improve accuracy and, in many cases, it even leads to more
hallucinations. We then analyze how extended reasoning affects hallucination
behavior. We find that reduced hallucinations often result from the model
choosing to abstain after thinking more, rather than from improved factual
recall. Conversely, for some models, longer reasoning encourages attempts on
previously unanswered questions, many of which result in hallucinations. Case
studies show that extended reasoning can induce confirmation bias, leading to
overconfident hallucinations. Despite these limitations, we observe that
compared to non-thinking, enabling thinking remains beneficial. Code and data
are available at https://github.com/XuZhao0/tts-knowledge

</details>


### [47] [Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents](https://arxiv.org/abs/2509.06917)
*Jiacheng Miao,Joe R. Davis,Jonathan K. Pritchard,James Zou*

Main category: cs.AI

TL;DR: 介绍Paper2Agent框架，可将研究论文转为AI智能体，通过案例证明其有效性，开创知识传播新范式。


<details>
  <summary>Details</summary>
Motivation: 传统研究论文需读者花费大量精力理解和应用，存在传播和复用障碍，Paper2Agent旨在解决该问题。

Method: 系统分析论文和代码库构建MCP服务器，迭代生成和运行测试完善MCP，将其与聊天智能体连接。

Result: 通过案例证明能创建可靠、有能力的论文智能体，可重现原论文结果并正确执行新查询。

Conclusion: Paper2Agent将静态论文转为动态交互智能体，开创知识传播新范式，为AI协作生态奠定基础。

Abstract: We introduce Paper2Agent, an automated framework that converts research
papers into AI agents. Paper2Agent transforms research output from passive
artifacts into active systems that can accelerate downstream use, adoption, and
discovery. Conventional research papers require readers to invest substantial
effort to understand and adapt a paper's code, data, and methods to their own
work, creating barriers to dissemination and reuse. Paper2Agent addresses this
challenge by automatically converting a paper into an AI agent that acts as a
knowledgeable research assistant. It systematically analyzes the paper and the
associated codebase using multiple agents to construct a Model Context Protocol
(MCP) server, then iteratively generates and runs tests to refine and robustify
the resulting MCP. These paper MCPs can then be flexibly connected to a chat
agent (e.g. Claude Code) to carry out complex scientific queries through
natural language while invoking tools and workflows from the original paper. We
demonstrate Paper2Agent's effectiveness in creating reliable and capable paper
agents through in-depth case studies. Paper2Agent created an agent that
leverages AlphaGenome to interpret genomic variants and agents based on ScanPy
and TISSUE to carry out single-cell and spatial transcriptomics analyses. We
validate that these paper agents can reproduce the original paper's results and
can correctly carry out novel user queries. By turning static papers into
dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for
knowledge dissemination and a foundation for the collaborative ecosystem of AI
co-scientists.

</details>


### [48] [Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference](https://arxiv.org/abs/2509.06942)
*Xiangwei Shen,Zhimin Li,Zhantao Yang,Shiyi Zhang,Yingfang Zhang,Donghao Li,Chunyu Wang,Qinglin Lu,Yansong Tang*

Main category: cs.AI

TL;DR: 提出Direct - Align和SRPO方法，优化扩散模型，使FLUX.1.dev模型的真实感和美学质量提升超3倍。


<details>
  <summary>Details</summary>
Motivation: 解决现有直接用可微奖励使扩散模型与人类偏好对齐方法存在计算成本高、需持续离线调整奖励模型的问题。

Method: 提出Direct - Align方法预先定义噪声先验通过插值从任意时间步恢复原始图像；引入SRPO方法将奖励表示为文本条件信号实现奖励在线调整。

Result: 通过优化去噪和在线奖励调整微调FLUX.1.dev模型，其人类评估的真实感和美学质量提升超3倍。

Conclusion: 所提方法能有效解决现有方法的局限性，提升模型性能。

Abstract: Recent studies have demonstrated the effectiveness of directly aligning
diffusion models with human preferences using differentiable reward. However,
they exhibit two primary challenges: (1) they rely on multistep denoising with
gradient computation for reward scoring, which is computationally expensive,
thus restricting optimization to only a few diffusion steps; (2) they often
need continuous offline adaptation of reward models in order to achieve desired
aesthetic quality, such as photorealism or precise lighting effects. To address
the limitation of multistep denoising, we propose Direct-Align, a method that
predefines a noise prior to effectively recover original images from any time
steps via interpolation, leveraging the equation that diffusion states are
interpolations between noise and target images, which effectively avoids
over-optimization in late timesteps. Furthermore, we introduce Semantic
Relative Preference Optimization (SRPO), in which rewards are formulated as
text-conditioned signals. This approach enables online adjustment of rewards in
response to positive and negative prompt augmentation, thereby reducing the
reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model
with optimized denoising and online reward adjustment, we improve its
human-evaluated realism and aesthetic quality by over 3x.

</details>


### [49] [Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation](https://arxiv.org/abs/2509.03736)
*James Mooney,Josef Woldense,Zheng Robert Jia,Shirley Anugrah Hayati,My Ha Nguyen,Vipul Raheja,Dongyeop Kang*

Main category: cs.AI

TL;DR: 研究评估大语言模型在人类受试者研究中替代真实参与者的能力，发现其存在显著内部不一致性。


<details>
  <summary>Details</summary>
Motivation: 评估合成代理能否替代人类受试者研究中的真实参与者，前人关注数据对应性，本文关注内部一致性。

Method: 开展研究揭示代理内部状态，在基本对话场景中检验其行为，探索行为假设评估一致性。

Result: 不同模型族和不同规模的大语言模型都存在显著内部不一致性。

Conclusion: 大语言模型虽能生成与人类匹配的回复，但内部不一致，无法准确替代人类受试者研究中的真实参与者。

Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the
notion that synthetic agents can serve as substitutes for real participants in
human-subject research. In an effort to evaluate the merits of this claim,
social science researchers have largely focused on whether LLM-generated survey
data corresponds to that of a human counterpart whom the LLM is prompted to
represent. In contrast, we address a more fundamental question: Do agents
maintain internal consistency, retaining similar behaviors when examined under
different experimental settings? To this end, we develop a study designed to
(a) reveal the agent's internal state and (b) examine agent behavior in a basic
dialogue setting. This design enables us to explore a set of behavioral
hypotheses to assess whether an agent's conversation behavior is consistent
with what we would expect from their revealed internal state. Our findings on
these hypotheses show significant internal inconsistencies in LLMs across model
families and at differing model sizes. Most importantly, we find that, although
agents may generate responses matching those of their human counterparts, they
fail to be internally consistent, representing a critical gap in their
capabilities to accurately substitute for real participants in human-subject
research. Our simulation code and data are publicly accessible.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [50] [Newton to Einstein: Axiom-Based Discovery via Game Design](https://arxiv.org/abs/2509.05448)
*Pingchuan Ma,Benjamin Tod Jones,Tsun-Hsuan Wang,Minghao Guo,Michal Piotr Lipiec,Chuang Gan,Wojciech Matusik*

Main category: cs.CE

TL;DR: 主张机器学习用于科学发现应从归纳模式识别转向基于公理的推理，提出游戏设计框架，通过实验证明可行，为构建相关机器学习系统提供基础。


<details>
  <summary>Details</summary>
Motivation: 推动机器学习在科学发现中从归纳模式识别转变为基于公理的推理。

Method: 提出游戏设计框架，将科学探究重塑为规则进化系统，让智能体在公理环境中操作并修改公理以解释异常观测。

Result: 通过基于逻辑的游戏初步实验，证明智能体可进化公理解决先前无法解决的问题。

Conclusion: 该框架为构建具有创造性、可解释性和理论驱动的机器学习发现系统提供基础。

Abstract: This position paper argues that machine learning for scientific discovery
should shift from inductive pattern recognition to axiom-based reasoning. We
propose a game design framework in which scientific inquiry is recast as a
rule-evolving system: agents operate within environments governed by axioms and
modify them to explain outlier observations. Unlike conventional ML approaches
that operate within fixed assumptions, our method enables the discovery of new
theoretical structures through systematic rule adaptation. We demonstrate the
feasibility of this approach through preliminary experiments in logic-based
games, showing that agents can evolve axioms that solve previously unsolvable
problems. This framework offers a foundation for building machine learning
systems capable of creative, interpretable, and theory-driven discovery.

</details>


### [51] [GUIDe: Generative and Uncertainty-Informed Inverse Design for On-Demand Nonlinear Functional Responses](https://arxiv.org/abs/2509.05641)
*Haoxuan Dylan Mu,Mingjian Tang,Wei Gao,Wei "Wayne" Chen*

Main category: cs.CE

TL;DR: 针对逆设计问题，指出已有方法不足，提出GUIDe框架，以设计映射响应策略生成设计方案，通过实例验证该框架能发现多样可行解。


<details>
  <summary>Details</summary>
Motivation: 逆设计问题在工程中普遍存在，传统机器学习和数据驱动方法（如深度生成模型）存在解不可靠或解空间覆盖不完全的问题。

Method: 提出Generative and Uncertainty - informed Inverse Design (GUIDe)框架，利用概率机器学习、统计推断和马尔可夫链蒙特卡罗采样，采用设计映射响应策略，基于置信度对解空间采样生成解。

Result: 通过设计珍珠母启发复合材料的界面属性以实现目标应力 - 应变响应，验证了GUIDe能发现多样可行解，包括训练数据范围外和分布外目标的解。

Conclusion: GUIDe框架有效，可用于解决逆设计问题，发现多样可行解。

Abstract: Inverse design problems are pervasive in engineering, particularly when
dealing with nonlinear system responses, such as in mechanical behavior or
spectral analysis. The inherent intractability, non-existence, or
non-uniqueness of their solutions, and the need for swift exploration of the
solution space necessitate the adoption of machine learning and data-driven
approaches, such as deep generative models. Here, we show that both deep
generative model-based and optimization-based methods can yield unreliable
solutions or incomplete coverage of the solution space. To address this, we
propose the Generative and Uncertainty-informed Inverse Design (GUIDe)
framework, leveraging probabilistic machine learning, statistical inference,
and Markov chain Monte Carlo sampling to generate designs with targeted
nonlinear behaviors. Unlike inverse models that directly map response to
design, i.e., response $\mapsto$ design, we employ a design $\mapsto$ response
strategy: a forward model that predicts each design's nonlinear functional
response allows GUIDe to evaluate the confidence that a design will meet the
target, conditioned on a target response with a user-specified tolerance level.
Then, solutions are generated by sampling the solution space based on the
confidence. We validate the method by designing the interface properties for
nacre-inspired composites to achieve target stress-strain responses. Results
show that GUIDe enables the discovery of diverse feasible solutions, including
those outside the training data range, even for out-of-distribution targets.

</details>


### [52] [Transformer-based Topology Optimization](https://arxiv.org/abs/2509.05800)
*Aaron Lutheran,Srijan Das,Alireza Tabarraei*

Main category: cs.CE

TL;DR: 提出基于transformer的拓扑优化机器学习模型，在静态和动态数据集上实现，经评估性能好，接近扩散模型且无需迭代。


<details>
  <summary>Details</summary>
Motivation: 传统迭代方法计算成本高、对初始条件敏感，现有机器学习方法存在迭代或性能不佳问题。

Method: 提出基于transformer的模型，通过类令牌机制嵌入边界和加载条件，在静态和动态数据集上实现，采用迁移学习和FFT编码，引入辅助损失函数。

Result: 模型接近扩散模型的保真度且无需迭代。

Conclusion: 该模型向实时、高保真拓扑生成迈出重要一步。

Abstract: Topology optimization enables the design of highly efficient and complex
structures, but conventional iterative methods, such as SIMP-based approaches,
often suffer from high computational costs and sensitivity to initial
conditions. Although machine learning methods have recently shown promise for
accelerating topology generation, existing models either remain iterative or
struggle to match ground-truth performance. In this work, we propose a
transformer-based machine learning model for topology optimization that embeds
critical boundary and loading conditions directly into the tokenized domain
representation via a class token mechanism. We implement this model on static
and dynamic datasets, using transfer learning and FFT encoding of dynamic loads
to improve our performance on the dynamic dataset. Auxiliary loss functions are
introduced to promote the realism and manufacturability of the generated
designs. We conduct a comprehensive evaluation of the model's performance,
including compliance error, volume fraction error, floating material
percentage, and load discrepancy error, and benchmark it against
state-of-the-art non-iterative and iterative generative models. Our results
demonstrate that the proposed model approaches the fidelity of diffusion-based
models while remaining iteration-free, offering a significant step toward
real-time, high-fidelity topology generation.

</details>


### [53] [Distortion Minimization in Reverse Engineering for Additive Manufacturing: An Integrated 3D Scanning and Simulation Framework](https://arxiv.org/abs/2509.05857)
*Jannatul Bushra,Md Habibor Rahman,Mohammed Shafae,Hannah D. Budinoff*

Main category: cs.CE

TL;DR: 本文提出集成框架减少增材制造逆向工程部件的变形，结合逆向工程和增材制造，对比STL和CAD方法，CAD方法精度更高。


<details>
  <summary>Details</summary>
Motivation: 现有逆向工程技术应用于增材制造部件时面临高变形和独特属性挑战，需新方法减少变形。

Method: 引入集成3D扫描和过程模拟的数据驱动框架，采用迭代有限元模拟预测几何变形，结合逆向工程和增材制造进行再制造，补偿部件几何特征。

Result: 通过逆向工程两个Inconel - 718部件验证框架有效性，CAD方法平均绝对百分比误差为0.087%，精度高于STL方法。

Conclusion: 提出的方法能生成补偿的STL和参数化CAD模型，减少逆向工程中的实验工作，CAD方法精度更好。

Abstract: Reverse engineering can be used to derive a 3D model of an existing physical
part when such a model is not readily available. For parts that will be
fabricated with subtractive and formative manufacturing processes, existing
reverse engineering techniques can be readily applied, but parts produced with
additive manufacturing can present new challenges due to the high level of
process-induced distortions and unique part attributes. This paper introduces
an integrated 3D scanning and process simulation data-driven framework to
minimize distortions of reverse-engineered additively manufactured components.
This framework employs iterative finite element simulations to predict
geometric distortions to minimize errors between the predicted and measured
geometrical deviations of the key dimensional characteristics of the part. The
effectiveness of this approach is then demonstrated by reverse engineering two
Inconel-718 components manufactured using laser powder bed fusion additive
manufacturing. This paper presents a remanufacturing process that combines
reverse engineering and additive manufacturing, leveraging geometric
feature-based part compensation through process simulation. Our approach can
generate both compensated STL and parametric CAD models, eliminating laborious
experimentation during reverse engineering. We evaluate the merits of STL-based
and CAD-based approaches by quantifying the errors induced at the different
steps of the proposed approach and analyzing the influence of varying part
geometries. Using the proposed CAD-based method, the average absolute percent
error between simulation-predicted distorted dimensions and actual measured
dimensions of the manufactured parts was 0.087%, with better accuracy than the
STL-based method.

</details>


### [54] [Anticipating AMOC transitions via deep learning](https://arxiv.org/abs/2509.06450)
*Wenjie Zhang,Yu Huang,Sebastian Bathiany,Yechul Shin,Maya Ben-Yami,Suiping Zhou,Niklas Boers*

Main category: cs.CE

TL;DR: 研究以AMOC为例，指出地球系统关键组件过渡具概率性，传统预警指标不可靠，开发CNN方法可实时预测过渡概率并有效预警。


<details>
  <summary>Details</summary>
Motivation: 解决地球系统关键组件在多种诱因下过渡难以预测的问题，改进传统预警指标不可靠的情况。

Method: 使用校准的AMOC箱式模型进行大集合模拟，开发基于卷积神经网络（CNN）的方法识别过渡与非过渡轨迹的高阶统计差异。

Result: CNN指标在多种诱因导致过渡的系统中能提供有效早期预警。

Conclusion: CNN方法在识别地球系统组件突变的安全运行空间和早期预警指标方面有潜力。

Abstract: Key components of the Earth system can undergo abrupt and potentially
irreversible transitions when the magnitude or rate of external forcing exceeds
critical thresholds. In this study, we use the example of the Atlantic
Meridional Overturning Circulation (AMOC) to demonstrate the challenges
associated with anticipating such transitions when the system is susceptible to
bifurcation-induced, rate-induced, and noise-induced tipping. Using a
calibrated AMOC box model, we conduct large ensemble simulations and show that
transition behavior is inherently probabilistic: under identical freshwater
forcing scenarios, some ensemble members exhibit transitions while others do
not. In this stochastic regime, traditional early warning indicators based on
critical slowing down are unreliable in predicting impending transitions. To
address this limitation, we develop a convolutional neural network (CNN)-based
approach that identifies higher-order statistical differences between
transitioning and non-transitioning trajectories within the ensemble
realizations. This method enables the real-time prediction of transition
probabilities for individual trajectories prior to the onset of tipping. Our
results show that the CNN-based indicator provides effective early warnings in
a system where transitions can be induced by bifurcations, critical forcing
rates, and noise. These findings underscore the potential in identifying safe
operating spaces and early warning indicators for abrupt transitions of Earth
system components under uncertainty.

</details>


### [55] [Reusable Surrogate Models for Distillation Columns](https://arxiv.org/abs/2509.06638)
*Martin Bubel,Tobias Seidel,Michael Bortz*

Main category: cs.CE

TL;DR: 本文提出开发适用于蒸馏塔的可复用代理模型，利用新型模型流体表示生成大量样本数据集，验证了模型准确性并展示其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有代理模型多针对狭窄固定化学系统和操作条件训练，可复用性有限，需开发可复用代理模型。

Method: 开发适用于蒸馏塔的单一模型，采用新型ML驱动的模型流体表示生成超100万个样本的数据集。

Result: 模型不仅能在塔规格上泛化，还能在均相三元气液混合物的整个化学空间泛化，在夹带剂蒸馏案例中成功筛选和排名候选夹带剂，显著减少计算量。

Conclusion: 所开发的可复用代理模型具有准确性和实用价值，能有效降低计算成本。

Abstract: Surrogate modeling is a powerful methodology in chemical process engineering,
frequently employed to accelerate optimization tasks where traditional
flowsheet simulators are computationally prohibitive. However, the
state-of-the-art is dominated by surrogate models trained for a narrow range of
fixed chemical systems and operating conditions, limiting their reusability.
This work introduces a paradigm shift towards reusable surrogates by developing
a single model for distillation columns that generalizes across a vast design
space. The key enabler is a novel ML-fueled modelfluid representation which
allows for the generation of datasets of more than $1,000,000$ samples. This
allows the surrogate to generalize not only over column specifications but also
over the entire chemical space of homogeneous ternary vapor-liquid mixtures. We
validate the model's accuracy and demonstrate its practical utility in a case
study on entrainer distillation, where it successfully screens and ranks
candidate entrainers, significantly reducing the computational effort compared
to rigorous optimization.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [56] [A Unified Framework for Cultural Heritage Data Historicity and Migration: The ARGUS Approach](https://arxiv.org/abs/2509.06044)
*Lingxiao Kong,Apostolos Sarris,Miltiadis Polidorou,Victor Klingenberg,Vasilis Sevetlidis,Vasilis Arampatzakis,George Pavlidis,Cong Yang,Zeyd Boukhers*

Main category: cs.DB

TL;DR: 本文介绍ARGUS项目中的数据历史性与迁移框架，通过系统数据处理流程处理文化遗产数据，在五个欧洲试点应用，提升数据可用性和分析能力。


<details>
  <summary>Details</summary>
Motivation: 文化遗产保护在处理多样、多源、多尺度数据以进行有效监测和保护时面临挑战，需要处理复杂的异构文化遗产数据。

Method: 建立系统的数据处理管道，包括标准化、丰富化、集成、可视化、摄取和发布策略；运用插补技术丰富稀疏数据集；通过数据库集成确保互操作性；利用基于大语言模型的自然语言处理提升查询能力。

Result: 在五个欧洲试点应用，结果显示提高了数据可访问性、增强了分析能力，有利于保护工作的决策。

Conclusion: 该框架具有适应性，能有效解决文化遗产数据处理难题，提升保护工作效果。

Abstract: Cultural heritage preservation faces significant challenges in managing
diverse, multi-source, and multi-scale data for effective monitoring and
conservation. This paper documents a comprehensive data historicity and
migration framework implemented within the ARGUS project, which addresses the
complexities of processing heterogeneous cultural heritage data. We describe a
systematic data processing pipeline encompassing standardization, enrichment,
integration, visualization, ingestion, and publication strategies. The
framework transforms raw, disparate datasets into standardized formats
compliant with FAIR principles. It enhances sparse datasets through established
imputation techniques, ensures interoperability through database integration,
and improves querying capabilities through LLM-powered natural language
processing. This approach has been applied across five European pilot sites
with varying preservation challenges, demonstrating its adaptability to diverse
cultural heritage contexts. The implementation results show improved data
accessibility, enhanced analytical capabilities, and more effective
decision-making for conservation efforts.

</details>


### [57] [Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research](https://arxiv.org/abs/2509.06093)
*Yuze Liu,Zhaoyuan Zhang,Xiangsheng Zeng,Yihe Zhang,Leping Yu,Lejia Wang,Xi Yu*

Main category: cs.DB

TL;DR: 提出用于氮化硼纳米片（BNNS）聚合物导热复合材料的语言原生数据库，支持检索增强生成及工具增强代理，为大语言模型驱动的材料发现提供基础。


<details>
  <summary>Details</summary>
Motivation: 传统化学和材料研究依赖知识叙述，限制了传统数据库和机器学习的应用，需要新的数据库解决方案。

Method: 构建语言原生数据库，捕捉论文中轻度结构化信息，以异构数据库组织记录，通过复合检索查询。

Result: 系统可将文献综合成准确、可验证且专业的指导，支持高保真高效的检索增强生成和工具增强代理。

Conclusion: 该框架为大语言模型驱动的材料发现提供了丰富的语言基础。

Abstract: Chemical and materials research has traditionally relied heavily on knowledge
narrative, with progress often driven by language-based descriptions of
principles, mechanisms, and experimental experiences, rather than tables,
limiting what conventional databases and ML can exploit. We present a
language-native database for boron nitride nanosheet (BNNS) polymer thermally
conductive composites that captures lightly structured information from papers
across preparation, characterization, theory-computation, and mechanistic
reasoning, with evidence-linked snippets. Records are organized in a
heterogeneous database and queried via composite retrieval with semantics, key
words and value filters. The system can synthesizes literature into accurate,
verifiable, and expert style guidance. This substrate enables high fidelity
efficient Retrieval Augmented Generation (RAG) and tool augmented agents to
interleave retrieval with reasoning and deliver actionable SOP. The framework
supplies the language rich foundation required for LLM-driven materials
discovery.

</details>


### [58] [MCTuner: Spatial Decomposition-Enhanced Database Tuning via LLM-Guided Exploration](https://arxiv.org/abs/2509.06298)
*Zihan Yan,Rui Xi,Mengshu Hou*

Main category: cs.DB

TL;DR: 提出自适应旋钮调优框架MCTuner，利用MoE机制和空间分解算法，在不同基准测试中比现有方法有性能提升和更快配置发现速度。


<details>
  <summary>Details</summary>
Motivation: 数据库旋钮调优中旋钮数量多、配置空间大，现有基于学习的调优方法存在忽略领域知识或难探索高维空间等问题，导致调优成本高和性能不佳。

Method: 提出MCTuner框架，采用Mixture - of - Experts机制结合专业大语言模型识别关键旋钮，引入空间分解算法将空间递归划分为子空间并进行贝叶斯优化搜索。

Result: 在不同基准测试（OLAP、OLTP和HTAP）中，MCTuner实现了高达19.2%的性能提升，每次迭代配置发现速度快1.4倍。

Conclusion: MCTuner能有效解决数据库旋钮调优问题，在性能和配置发现速度上优于现有方法。

Abstract: Database knob tuning is essential for optimizing the performance of modern
database management systems, which often expose hundreds of knobs with
continuous or categorical values. However, the large number of knobs and the
vast configuration space make it difficult to identify optimal settings
efficiently. Although learning-based tuning has shown promise, existing
approaches either ignore domain knowledge by relying solely on benchmark
feedback or struggle to explore the high-dimensional knob space, resulting in
high tuning costs and suboptimal performance. To address these challenges, we
propose MCTuner, an adaptive knob tuning framework that minimizes exploration
in ineffective regions of the configuration space. MCTuner employs a
Mixture-of-Experts (MoE) mechanism with specialized LLMs to identify
performance-critical knobs. In further, MCTuner introduces the first spatial
decomposition algorithm that recursively partitions the space into hierarchical
subspaces, on which Bayesian Optimization is performed to efficiently search
for near-optimal configurations. Evaluated on different benchmarks (OLAP, OLTP,
and HTAP), MCTuner achieves up to 19.2% performance gains and 1.4x faster
configuration discovery per iteration compared to state-of-the-art methods.

</details>


### [59] [Relational Algebras for Subset Selection and Optimisation](https://arxiv.org/abs/2509.06439)
*David Robert Pratten,Luke Mathieson,Fahimeh Ramezani*

Main category: cs.DB

TL;DR: 本文提出首个针对子集选择和优化查询的统一代数基础，扩展关系代数，引入解集，实现向标准关系代数的翻译，并通过多态SQL展示其能力。


<details>
  <summary>Details</summary>
Motivation: 数据库社区缺乏统一的关系查询语言用于子集选择和优化查询，现有的评估算法SQL扩展不兼容。

Method: 提出统一代数基础，扩展关系代数，引入解集，提供从解集到标准关系代数的翻译语义。

Result: 该框架达到了以往强大方法的表达能力，具备理论清晰性和组合性。

Conclusion: 通过多态SQL展示了这些代数在单一范式下表达数据管理、子集选择和优化查询的能力。

Abstract: The database community lacks a unified relational query language for subset
selection and optimisation queries, limiting both user expression and query
optimiser reasoning about such problems. Decades of research (latterly under
the rubric of prescriptive analytics) have produced powerful evaluation
algorithms with incompatible, ad-hoc SQL extensions that specify and filter
through distinct mechanisms. We present the first unified algebraic foundation
for these queries, introducing relational exponentiation to complete the
fundamental algebraic operations alongside union (addition) and cross product
(multiplication). First, we extend relational algebra to complete domain
relations-relations defined by characteristic functions rather than explicit
extensions-achieving the expressiveness of NP-complete/hard problems, while
simultaneously providing query safety for finite inputs. Second, we introduce
solution sets, a higher-order relational algebra over sets of relations that
naturally expresses search spaces as functions f: Base to Decision, yielding
|Decision|^|Base| candidate relations. Third, we provide structure-preserving
translation semantics from solution sets to standard relational algebra,
enabling mechanical translation to existing evaluation algorithms. This
framework achieves the expressiveness of the most powerful prior approaches
while providing the theoretical clarity and compositional properties absent in
previous work. We demonstrate the capabilities these algebras open up through a
polymorphic SQL where standard clauses seamlessly express data management,
subset selection, and optimisation queries within a single paradigm.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [60] [Multi-IaC-Eval: Benchmarking Cloud Infrastructure as Code Across Multiple Formats](https://arxiv.org/abs/2509.05303)
*Sam Davidson,Li Sun,Bhavana Bhasker,Laurent Callot,Anoop Deoras*

Main category: cs.DC

TL;DR: 提出Multi - IaC - Bench基准数据集评估基于大语言模型的IaC生成和变异，评估显示现代大语言模型有高语法成功率，但语义对齐等仍有挑战，还强调提示工程和重试机制重要性并发布数据集。


<details>
  <summary>Details</summary>
Motivation: 不同云服务提供商的IaC格式缺乏标准化，且缺少跨多种IaC格式的综合基准来评估大语言模型在IaC创建和维护方面的能力。

Method: 创建Multi - IaC - Bench基准数据集，包含初始模板、自然语言修改请求和更新模板的三元组，通过合成数据生成管道及严格验证创建，并用该数据集评估多个先进大语言模型。

Result: 现代大语言模型在跨格式生成语法有效IaC上成功率超95%，但在语义对齐和处理复杂基础设施模式上有挑战，消融研究突出提示工程和重试机制重要性。

Conclusion: 发布Multi - IaC - Bench数据集以促进AI辅助基础设施管理研究，建立该领域标准化评估指标。

Abstract: Infrastructure as Code (IaC) is fundamental to modern cloud computing,
enabling teams to define and manage infrastructure through machine-readable
configuration files. However, different cloud service providers utilize diverse
IaC formats. The lack of a standardized format requires cloud architects to be
proficient in multiple IaC languages, adding complexity to cloud deployment.
While Large Language Models (LLMs) show promise in automating IaC creation and
maintenance, progress has been limited by the lack of comprehensive benchmarks
across multiple IaC formats. We present Multi-IaC-Bench, a novel benchmark
dataset for evaluating LLM-based IaC generation and mutation across AWS
CloudFormation, Terraform, and Cloud Development Kit (CDK) formats. The dataset
consists of triplets containing initial IaC templates, natural language
modification requests, and corresponding updated templates, created through a
synthetic data generation pipeline with rigorous validation. We evaluate
several state-of-the-art LLMs on Multi-IaC-Bench, demonstrating that while
modern LLMs can achieve high success rates (>95%) in generating syntactically
valid IaC across formats, significant challenges remain in semantic alignment
and handling complex infrastructure patterns. Our ablation studies highlight
the importance of prompt engineering and retry mechanisms in successful IaC
generation. We release Multi-IaC-Bench to facilitate further research in
AI-assisted infrastructure management and establish standardized evaluation
metrics for this crucial domain.

</details>


### [61] [A Simple and Robust Protocol for Distributed Counting](https://arxiv.org/abs/2509.05870)
*Edith Cohen,Moshe Shechner,Uri Stemmer*

Main category: cs.DC

TL;DR: 本文重新审视分布式计数问题，证明Huang等人2012年协议不鲁棒，并提出新的简单鲁棒协议达到最优通信复杂度。


<details>
  <summary>Details</summary>
Motivation: 前人工作中Huang等人2012年协议有局限性，Xiong等人2023年协议复杂度高且通信复杂度非最优，要解决Huang等人协议是否鲁棒的问题。

Method: 构造显式自适应攻击证明Huang等人协议不鲁棒，提出新的分布式计数鲁棒协议。

Result: 证明Huang等人2012年协议不鲁棒，新协议达到最优通信复杂度$O(\frac{\sqrt{k}}{\epsilon} \log N)$，比前人协议简单。

Conclusion: 新协议是首个在自适应设置中达到最优无感知复杂度的协议，有效解决分布式计数问题。

Abstract: We revisit the distributed counting problem, where a server must continuously
approximate the total number of events occurring across $k$ sites while
minimizing communication. The communication complexity of this problem is known
to be $\Theta(\frac{k}{\epsilon}\log N)$ for deterministic protocols. Huang,
Yi, and Zhang (2012) showed that randomization can reduce this to
$\Theta(\frac{\sqrt{k}}{\epsilon}\log N)$, but their analysis is restricted to
the {\em oblivious setting}, where the stream of events is independent of the
protocol's outputs.
  Xiong, Zhu, and Huang (2023) presented a robust protocol for distributed
counting that removes the oblivious assumption. However, their communication
complexity is suboptimal by a $polylog(k)$ factor and their protocol is
substantially more complex than the oblivious protocol of Huang et al. (2012).
This left open a natural question: could it be that the simple protocol of
Huang et al. (2012) is already robust?
  We resolve this question with two main contributions. First, we show that the
protocol of Huang et al. (2012) is itself not robust by constructing an
explicit adaptive attack that forces it to lose its accuracy. Second, we
present a new, surprisingly simple, robust protocol for distributed counting
that achieves the optimal communication complexity of
$O(\frac{\sqrt{k}}{\epsilon} \log N)$. Our protocol is simpler than that of
Xiong et al. (2023), perhaps even simpler than that of Huang et al. (2012), and
is the first to match the optimal oblivious complexity in the adaptive setting.

</details>


### [62] [DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across Thousands of Computers](https://arxiv.org/abs/2509.06046)
*Philip Adams,Menghao Li,Shi Zhang,Li Tan,Qi Chen,Mingqin Li,Zengzhong Li,Knut Risvik,Harsha Vardhan Simhadri*

Main category: cs.DC

TL;DR: 介绍DISTRIBUTEDANN分布式向量搜索服务，性能优于现有策略，已用于Bing搜索引擎。


<details>
  <summary>Details</summary>
Motivation: 提升向量搜索服务效率，解决大规模向量图索引搜索问题。

Method: 使用分布式键值存储和内存中ANN索引两个组件构建DISTRIBUTEDANN。

Result: 可搜索500亿向量图索引，中位查询延迟26ms，每秒处理超10万查询，效率是现有策略6倍。

Conclusion: DISTRIBUTEDANN可替代传统横向扩展架构，已应用于Bing搜索服务。

Abstract: We present DISTRIBUTEDANN, a distributed vector search service that makes it
possible to search over a single 50 billion vector graph index spread across
over a thousand machines that offers 26ms median query latency and processes
over 100,000 queries per second. This is 6x more efficient than existing
partitioning and routing strategies that route the vector query to a subset of
partitions in a scale out vector search system. DISTRIBUTEDANN is built using
two well-understood components: a distributed key-value store and an in-memory
ANN index. DISTRIBUTEDANN has replaced conventional scale-out architectures for
serving the Bing search engine, and we share our experience from making this
transition.

</details>


### [63] [Gathering in Non-Vertex-Transitive Graphs Under Round Robin](https://arxiv.org/abs/2509.06064)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: 研究图上机器人聚集问题，考虑含多重性且无法检测多重性的‘敌对’情况，在非顶点传递图中给出解决算法并分析复杂度。


<details>
  <summary>Details</summary>
Motivation: 在经典机器人聚集问题基础上，考虑初始配置含多重性且机器人无法检测多重性的‘敌对’情况，对非顶点传递图上的问题进行完整刻画。

Method: 采用轮询调度方式激活机器人，针对非顶点传递图设计解决算法。

Result: 给出了非顶点传递图上机器人任意配置的解决算法及其正确性证明，还分析了时间复杂度。

Conclusion: 成功对非顶点传递图上的机器人聚集问题进行刻画，所设计算法可解决该问题。

Abstract: The Gathering problem for a swarm of robots asks for a distributed algorithm
that brings such entities to a common place, not known in advance. We consider
the well-known OBLOT model with robots constrained to move along the edges of a
graph, hence gathering in one vertex, eventually. Despite the classical setting
under which the problem has been usually approached, we consider the `hostile'
case where: i) the initial configuration may contain multiplicities, i.e. more
than one robot may occupy the same vertex; ii) robots cannot detect
multiplicities. As a scheduler for robot activation, we consider the
"favorable" round-robin case, where robots are activated one at a time.
  Our objective is to achieve a complete characterization of the problem in the
broad context of non-vertex-transitive graphs, i.e., graphs where the vertices
are partitioned into at least two different classes of equivalence. We provide
a resolution algorithm for any configuration of robots moving on such graphs,
along with its correctness. Furthermore, we analyze its time complexity.

</details>


### [64] [20 Years in Life of a Smart Building: A retrospective](https://arxiv.org/abs/2509.06229)
*Karolina Skrivankova,Mark Handley,Stephen Hailes*

Main category: cs.DC

TL;DR: 本文介绍分布式控制平台KaOS，用于构建智能建筑自动化系统，初始评估证明其可行性。


<details>
  <summary>Details</summary>
Motivation: 2025年智能建筑自动化系统运行面临硬件故障、安全威胁等挑战，现有行业未全面解决，限制大型部署可行性。

Method: 利用容器化和管理资源访问，使用经济实惠的现成物联网硬件构建KaOS平台，支持控制应用和分布式系统操作。

Result: 初始评估证实了该方法的实际可行性。

Conclusion: KaOS有潜力在长时间内可持续维护和逐步发展建筑控制功能。

Abstract: Operating an intelligent smart building automation system in 2025 is met with
many challenges: hardware failures, vendor obsolescence, evolving security
threats and more. None of these have been comprehensibly addressed by the
industrial building nor home automation industries, limiting feasibility of
operating large, truly smart automation deployments. This paper introduces
KaOS, a distributed control platform for constructing robust and evolvable
smart building automation systems using affordable, off-the-shelf IoT hardware.
Supporting control applications and distributed system operations by leveraging
containerisation and managed resource access, KaOS seeks to achieve
flexibility, security, and fault tolerance without sacrificing
cost-effectiveness. Initial evaluation confirms the practical feasibility of
our approach, highlighting its potential to sustainably maintain and
incrementally evolve building control functionalities over extended timeframes.

</details>


### [65] [FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving](https://arxiv.org/abs/2509.06261)
*Kyungmin Bin,Seungbeom Choi,Jimyoung Son,Jieun Choi,Daseul Bae,Daehyeon Baek,Kihyo Moon,Minsung Jang,Hyojung Lee*

Main category: cs.DC

TL;DR: 提出FineServe推理服务框架应对混合精度大语言模型推理挑战，实验显示比现有系统表现更优。


<details>
  <summary>Details</summary>
Motivation: 量化模型存在内存碎片化导致内存效率低，且量化与非量化模型资源使用模式不同需高效调度，以提升吞吐量。

Method: 提出FineServe框架，含KV Slab内存管理技术和两级调度框架，前者根据模型量化特征动态分配KV缓存，后者含全局和本地调度器。

Result: FineServe比现有GPU共享系统SLO达成率高2.2倍，token生成吞吐量高1.8倍。

Conclusion: FineServe能有效提升混合精度大语言模型推理的性能。

Abstract: Recent advances in Post-Training Quantization (PTQ) techniques have
significantly increased demand for serving quantized large language models
(LLMs), enabling higher throughput and substantially reduced memory usage with
minimal accuracy loss. Quantized models address memory constraints in LLMs and
enhance GPU resource utilization through efficient GPU sharing. However,
quantized models have smaller KV block sizes than non-quantized models, causing
limited memory efficiency due to memory fragmentation. Also, distinct resource
usage patterns between quantized and non-quantized models require efficient
scheduling to maximize throughput. To address these challenges, we propose
FineServe, an inference serving framework for mixed-precision LLMs. FineServe's
key contributions include: (1) KV Slab, a precision-aware adaptive memory
management technique dynamically allocating KV cache based on model
quantization characteristics, significantly reducing GPU memory fragmentation,
and (2) a two-level scheduling framework comprising a global scheduler that
places models to GPUs based on request rates, latency SLOs, and memory
constraints and efficiency, and a local scheduler that adaptively adjusts batch
sizes according to real-time request fluctuations. Experimental results
demonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x
higher token generation throughput compared to the state-of-the-art GPU sharing
systems.

</details>


### [66] [MaaSO: SLO-aware Orchestration of Heterogeneous Model Instances for MaaS](https://arxiv.org/abs/2509.06362)
*Mo Xuan,Zhang yue,Wu Weigang*

Main category: cs.DC

TL;DR: 提出MaaS编排器MaaSO，由剖析器、布局器和分发器组成，实验表明它比现有方法提升SLO满意度、降低响应延迟和编排开销。


<details>
  <summary>Details</summary>
Motivation: MaaS平台有多样SLO需求，LLM实例不同配置性能不同，但现有LLM推理系统缺乏利用这种异构性的机制。

Method: 提出MaaSO编排器，包含表征不同并行策略和推理批次大小下实例性能的剖析器、优化异构实例配置的布局器、实现SLO感知请求分发和防止连续批处理级联超时的分发器。

Result: MaaSO比现有方法将SLO满意度提高15 - 30%，响应延迟降低40 - 60%，显著降低整体编排开销。

Conclusion: MaaSO能有效利用LLM实例异构性，在SLO满意度、响应延迟和编排开销方面优于现有方法。

Abstract: Model-as-a-Service (MaaS) platforms face diverse Service Level Objective
(SLO) requirements stemming from various large language model (LLM)
applications, manifested in contextual complexity, first-token latency, and
between-token latency. On the other hand, an LLM instance, when configured with
different parallelism strategies and inference batch sizes, exhibits distinct
performance characteristics and can thus be used to serve different SLO
requirements. However, current LLM inference systems typically deploy instances
of the same model with identical configurations, lacking mechanisms to leverage
such heterogeneity. To fill this research gap, we propose MaaSO, the first MaaS
Orchestrator, which comprises three modules: (1) a profiler characterizing
instance performance under diverse parallelism strategies and inference batch
sizes; (2) a placer optimizing heterogeneous instance configurations; (3) a
distributor enabling SLO-aware request distribution and preventing cascaded
timeouts in continuous batching. Experiments show that MaaSO improves the SLO
satisfaction ratio by 15 to 30% and reduces response latency by 40 to 60%
compared to existing approaches, and significantly lowers overall orchestration
overhead.

</details>


### [67] [IM-PIR: In-Memory Private Information Retrieval](https://arxiv.org/abs/2509.06514)
*Mpoki Mwaisela,Peterson Yuhala,Pascal Felber,Valerio Schiavoni*

Main category: cs.DC

TL;DR: 提出首个基于PIM的多服务器PIR架构IM - PIR，性能比基于CPU的PIR方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前PIR构建计算成本高，受传统处理器架构内存带宽限制，而PIM能解决此瓶颈并提供并行性。

Method: 提出基于PIM的架构，讨论算法基础，在UPMEM PIM上设计并实现IM - PIR。

Result: 基于PIM的多服务器PIR实现比标准基于CPU的PIR方法查询吞吐量提高超3.7倍。

Conclusion: 基于PIM的多服务器PIR架构能显著提升性能。

Abstract: Private information retrieval (PIR) is a cryptographic primitive that allows
a client to securely query one or multiple servers without revealing their
specific interests. In spite of their strong security guarantees, current PIR
constructions are computationally costly. Specifically, most PIR
implementations are memory-bound due to the need to scan extensive databases
(in the order of GB), making them inherently constrained by the limited memory
bandwidth in traditional processor-centric computing
architectures.Processing-in-memory (PIM) is an emerging computing paradigm that
augments memory with compute capabilities, addressing the memory bandwidth
bottleneck while simultaneously providing extensive parallelism.Recent research
has demonstrated PIM's potential to significantly improve performance across a
range of data-intensive workloads, including graph processing, genome analysis,
and machine learning.
  In this work, we propose the first PIM-based architecture for multi-server
PIR. We discuss the algorithmic foundations of the latter and show how its
operations align with the core strengths of PIM architectures: extensive
parallelism and high memory bandwidth. Based on this observation, we design and
implement IM-PIR, a PIM-based multi-server PIR approach on top of UPMEM PIM,
the first openly commercialized PIM architecture. Our evaluation demonstrates
that a PIM-based multi-server PIR implementation significantly improves query
throughput by more than 3.7x when compared to a standard CPU-based PIR
approach.

</details>


### [68] [Mangrove: Fast and Parallelizable State Replication for Blockchains](https://arxiv.org/abs/2509.06616)
*Anton Paramonov,Yann Vonlanthen,Quentin Kniep,Jakub Sliwinski,Roger Wattenhofer*

Main category: cs.DC

TL;DR: Mangrove是一种支持并行智能合约的区块链扩展方法，采用并行乐观协议和轻量级拜占庭可靠广播原语优化性能，乐观条件下交易延迟低。


<details>
  <summary>Details</summary>
Motivation: 解决传统单体区块链中单一共识机制对所有交易排序的问题，实现智能合约并行运行。

Method: 为每个智能合约使用单独的共识实例，提出并行乐观协议，对简单交易使用轻量级拜占庭可靠广播原语。

Result: 在乐观条件下，协议在创建和执行交易之间可实现2个通信步骤的延迟。

Conclusion: Mangrove在乐观条件下能有效提升区块链性能，减少交易延迟。

Abstract: Mangrove is a novel scaling approach to building blockchains with parallel
smart contract support. Unlike in monolithic blockchains, where a single
consensus mechanism determines a strict total order over all transactions,
Mangrove uses separate consensus instances per smart contract, without a global
order. To allow multiple instances to run in parallel while ensuring that no
conflicting transactions are committed, we propose a mechanism called Parallel
Optimistic Agreement. Additionally, for simple transactions, we leverage a
lightweight Byzantine Reliable Broadcast primitive to reduce latency. Mangrove
is optimized for performance under optimistic conditions, where there is no
misbehavior and the network is synchronous. Under these conditions, our
protocol can achieve a latency of 2 communication steps between creating and
executing a transaction.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [69] [Subsequence Covers of Words](https://arxiv.org/abs/2509.05827)
*Panagiotis Charalampopoulos,Solon P. Pissis,Jakub Radoszewski,Wojciech Rytter,Tomasz Waleń,Wiktor Zuba*

Main category: cs.DS

TL;DR: 本文介绍子序列覆盖（s - 覆盖），给出测试候选词是否为 s - 覆盖的线性时间算法、找最短 s - 覆盖的算法，还给出 s - 本原词长度的上下界。


<details>
  <summary>Details</summary>
Motivation: 研究新的单词覆盖类型——子序列覆盖，对比其与标准覆盖和洗牌幂的计算难度。

Method: 针对多项式有界整数字母表给出测试算法，针对常量大小字母表给出找最短 s - 覆盖算法，推导 s - 本原词长度上下界。

Result: 给出线性时间的测试算法和找最短 s - 覆盖算法，得出 s - 本原词长度上下界为字母表大小的指数级，且上界有所改进。

Conclusion: 提出的算法能有效处理 s - 覆盖相关问题，对 s - 本原词长度的界定有新结果。

Abstract: We introduce subsequence covers (s-covers, in short), a new type of covers of
a word. A word $C$ is an s-cover of a word $S$ if the occurrences of $C$ in $S$
as subsequences cover all the positions in $S$.
  The s-covers seem to be computationally much harder than standard covers of
words (cf. Apostolico et al., Inf. Process. Lett. 1991), but, on the other
hand, much easier than the related shuffle powers (Warmuth and Haussler, J.
Comput. Syst. Sci. 1984).
  We give a linear-time algorithm for testing if a candidate word $C$ is an
s-cover of a word $S$ over a polynomially-bounded integer alphabet. We also
give an algorithm for finding a shortest s-cover of a word $S$, which in the
case of a constant-sized alphabet, also runs in linear time.
  The words without proper s-cover are called s-primitive. We complement our
algorithmic results with explicit lower and an upper bound on the length of a
longest s-primitive word. Both bounds are exponential in the size of the
alphabet. The upper bound presented here improves the bound given in the
conference version of this paper [SPIRE 2022].

</details>


### [70] [Generalized Graph Packing Problems Parameterized by Treewidth](https://arxiv.org/abs/2509.06091)
*Barış Can Esmer,Dániel Marx*

Main category: cs.DS

TL;DR: 研究有界树宽图上的H - Packing和H - Partition问题及其推广，给出不同情况下的上下界和复杂度结论。


<details>
  <summary>Details</summary>
Motivation: 研究有界树宽图上H - Packing和H - Partition问题及推广，此前三角形情况已被研究。

Method: 对问题进行两个方向的推广，一是顶点最多使用c次，二是考虑H不是团的情况，分析复杂度。

Result: 当H是团时，顶点最多用c次最优运行时间为(c + 1)^tw · n^O(1)；H不是团时，在ETH假设下，H - Partition无2^(o(tw log tw)) · n^O(1)算法。

Conclusion: 得到有界树宽图上H - Packing和H - Partition问题不同推广情况下的复杂度结论。

Abstract: $H$-Packing is the problem of finding a maximum number of vertex-disjoint
copies of $H$ in a given graph $G$. $H$-Partition is the special case of
finding a set of vertex-disjoint copies that cover each vertex of $G$ exactly
once. Our goal is to study these problems and some generalizations on
bounded-treewidth graphs. The case of $H$ being a triangle is well understood:
given a tree decomposition of $G$ having treewidth $tw$, the $K_3$-Packing
problem can be solved in time $2^{tw} \cdot n^{O(1)}$, while Lokshtanov et
al.~[{\it ACM Transactions on Algorithms} 2018] showed, under the Strong
Exponential-Time Hypothesis (SETH), that there is no $(2-\epsilon)^{tw}\cdot
n^{O(1)}$ algorithm for any $\epsilon>0$ even for $K_3$-Partition. Similar
results can be obtained for any other clique $K_d$ for $d\ge 3$. We provide
generalizations in two directions:
  - We consider a generalization of the problem where every vertex can be used
at most $c$ times for some $c\ge 1$. When $H$ is any clique $K_d$ with $d\ge
3$, then we give upper and lower bounds showing that the optimal running time
increases to $(c+1)^{tw}\cdot n^{O(1)}$. We consider two variants depending on
whether a copy of $H$ can be used multiple times in the packing.
  - If $H$ is not a clique, then the dependence of the running time on
treewidth may not be even single exponential. Specifically, we show that if $H$
is any fixed graph where not every 2-connected component is a clique, then
there is no $2^{o({tw}\log {tw})}\cdot n^{O(1)}$ algorithm for
\textsc{$H$-Partition}, assuming the Exponential-Time Hypothesis (ETH).

</details>


### [71] [Parameterized Algorithms for Computing Pareto Sets](https://arxiv.org/abs/2509.06124)
*Joshua Könen,Heiko Röglin,Tarek Stuck*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Dynamic programming over tree decompositions is a common technique in
parameterized algorithms. In this paper, we study whether this technique can
also be applied to compute Pareto sets of multiobjective optimization problems.
We first derive an algorithm to compute the Pareto set for the multicriteria
s-t cut problem and show how this result can be applied to a polygon
aggregation problem arising in cartography that has recently been introduced by
Rottmann et al. (GIScience 2021). We also show how to apply these techniques to
also compute the Pareto set of the multiobjective minimum spanning tree problem
and for the multiobjective TSP. The running time of our algorithms is
$O(f(w)\cdot\mathrm{poly}(n,p_{\text{max}}))$, where $f$ is some function in
the treewidth $w$, $n$ is the input size, and $p_{\text{max}}$ is an upper
bound on the size of the Pareto sets of the subproblems that occur in the
dynamic program. Finally, we present an experimental evaluation of computing
Pareto sets on real-world instances of polygon aggregation problems. For this
matter we devised a task-specific data structure that allows for efficient
storage and modification of large sets of Pareto-optimal solutions. Throughout
the implementation process, we incorporated several improved strategies and
heuristics that significantly reduced both runtime and memory usage, enabling
us to solve instances with treewidth of up to 22 within reasonable amount of
time. Moreover, we conducted a preprocessing study to compare different tree
decompositions in terms of their estimated overall runtime.

</details>


### [72] [Efficient Catalytic Graph Algorithms](https://arxiv.org/abs/2509.06209)
*James Cook,Edward Pyne*

Main category: cs.DS

TL;DR: 本文为两个基本图问题给出快速、简单且可实现的催化对数空间算法，包括s→t连通性和随机游走模拟，给出具体时间复杂度，此前算法无明确时间界。


<details>
  <summary>Details</summary>
Motivation: 为两个基本图问题（s→t连通性和随机游走模拟）提供有明确时间复杂度的催化对数空间算法，改进先前无明确时间界的状况。

Method: 对于s→t连通性，随机算法用每个顶点一个寄存器并沿边推值，确定性算法类似；对于随机游走模拟，用每个顶点一个寄存器，每次访问时递增以确保不同出边。

Result: 得到s→t连通性随机算法时间为O(nm)，确定性算法为O(n³m)；随机游走模拟算法在O(mT²/ε)时间内以ε加性误差估计概率。

Conclusion: 成功为两个基本图问题设计出有明确时间复杂度的催化对数空间算法，改进了先前算法。

Abstract: We give fast, simple, and implementable catalytic logspace algorithms for two
fundamental graph problems.
  First, a randomized catalytic algorithm for $s\to t$ connectivity running in
$\widetilde{O}(nm)$ time, and a deterministic catalytic algorithm for the same
running in $\widetilde{O}(n^3 m)$ time. The former algorithm is the first
algorithmic use of randomization in $\mathsf{CL}$. The algorithm uses one
register per vertex and repeatedly ``pushes'' values along the edges in the
graph.
  Second, a deterministic catalytic algorithm for simulating random walks which
in $\widetilde{O}( m T^2 / \varepsilon )$ time estimates the probability a
$T$-step random walk ends at a given vertex within $\varepsilon$ additive
error. The algorithm uses one register for each vertex and increments it at
each visit to ensure repeated visits follow different outgoing edges.
  Prior catalytic algorithms for both problems did not have explicit runtime
bounds beyond being polynomial in $n$.

</details>


### [73] [Zero-Freeness is All You Need: A Weitz-Type FPTAS for the Entire Lee-Yang Zero-Free Region](https://arxiv.org/abs/2509.06623)
*Shuai Shao,Ke Shi*

Main category: cs.DS

TL;DR: 提出适用于铁磁Ising模型的Weitz型FPTAS，不依赖强空间混合（SSM）性质，证明随机簇模型的SSM结果，表明Weitz型FPTAS和SSM可从零点自由性推导得出。


<details>
  <summary>Details</summary>
Motivation: 在不依赖强空间混合（SSM）性质的情况下，为铁磁Ising模型设计近似算法，并研究相关模型的SSM性质。

Method: 将配分函数表示为比率的连乘积，使用Weitz的自回避行走树并在对数深度截断，近似精心设计的边删除比率；建立系数的局部依赖性（LDC）。

Result: 得到适用于铁磁Ising模型的Weitz型FPTAS；证明随机簇模型在一般图上的SSM结果；为多种模型建立LDC并推导新的SSM结果。

Conclusion: Weitz型FPTAS和SSM可从零点自由性推导得出，Weitz型FPTAS仅需零点自由性，而SSM还需LDC。

Abstract: We present a Weitz-type FPTAS for the ferromagnetic Ising model across the
entire Lee-Yang zero-free region, without relying on the strong spatial mixing
(SSM) property. Our algorithm is Weitz-type for two reasons. First, it
expresses the partition function as a telescoping product of ratios, with the
key being to approximate each ratio. Second, it uses Weitz's self-avoiding walk
tree, and truncates it at logarithmic depth to give a good and efficient
approximation. The key difference from the standard Weitz algorithm is that we
approximate a carefully designed edge-deletion ratio instead of the marginal
probability of a vertex's spin, ensuring our algorithm does not require SSM.
  Furthermore, by establishing local dependence of coefficients (LDC), we
indeed prove a novel form of SSM for these edge-deletion ratios, which, in
turn, implies the standard SSM for the random cluster model. This is the first
SSM result for the random cluster model on general graphs, beyond lattices. We
prove LDC using a new division relation, and remarkably, such relations hold
quite universally. As a result, we establish LDC for a variety of models.
Combined with existing zero-freeness results for these models, we derive new
SSM results for them. Our work suggests that both Weitz-type FPTASes and SSM
can be derived from zero-freeness, while zero-freeness alone suffices for
Weitz-type FPTASes, SSM additionally requires LDC, a combinatorial property
independent of zero-freeness.

</details>


### [74] [The Steiner Shortest Path Tree Problem](https://arxiv.org/abs/2509.06789)
*Omer Asher,Yefim Dinitz,Shlomi Dolev,Li-on Raviv,Baruch Schieber*

Main category: cs.DS

TL;DR: 研究计算含最少非终端节点的最短路径树问题，证明其NP难，给出近似算法。


<details>
  <summary>Details</summary>
Motivation: 在需要最短路径连接且减少中间节点可降低成本、复杂度和开销的应用场景下提出该问题。

Method: 引入并研究图的最短路径子图，将SSPT问题近似归约到UVDST问题，利用已有DST近似算法。

Result: 得到SSPT的拟多项式多对数近似算法，对受限图类得到多项式多对数近似算法。

Conclusion: 证明SSPT问题的NP难性，并给出了相应的近似算法。

Abstract: We introduce and study a novel problem of computing a shortest path tree with
a minimum number of non-terminals. It can be viewed as an (unweighted) Steiner
Shortest Path Tree (SSPT) that spans a given set of terminal vertices by
shortest paths from a given source while minimizing the number of nonterminal
vertices included in the tree. This problem is motivated by applications where
shortest-path connections from a source are essential, and where reducing the
number of intermediate vertices helps limit cost, complexity, or overhead. We
show that the SSPT problem is NP-hard. To approximate it, we introduce and
study the shortest path subgraph of a graph. Using it, we show an
approximation-preserving reduction of SSPT to the uniform vertex-weighted
variant of the Directed Steiner Tree (DST) problem, termed UVDST. Consequently,
the algorithm of [Grandoni et al., 2023] approximating DST implies a
quasi-polynomial polylog-approximation algorithm for SSPT. We present a
polynomial polylog-approximation algorithm for UVDST, and thus for SSPT, for a
restricted class of graphs.

</details>


### [75] [Engineering Select Support for Hybrid Bitvectors](https://arxiv.org/abs/2509.06900)
*Eric Chiu,Dominik Kempa*

Main category: cs.DS

TL;DR: 提出为混合位向量添加选择查询支持的方法，实验表明其性能出色。


<details>
  <summary>Details</summary>
Motivation: 现有混合位向量实现仅支持排名查询，缺乏选择查询支持，限制了适用性。

Method: 提出为混合位向量添加选择查询支持的方法并进行大量实验。

Result: 混合位向量性能出色，速度与最快的现有位向量匹配，空间效率与最紧凑的现有位向量相当。

Conclusion: 所提方法让混合位向量能支持选择查询，且性能优秀。

Abstract: One of the central problems in the design of compressed data structures is
the efficient support for rank and select queries on bitvectors. These two
operations form the backbone of more complex data structures (such as wavelet
trees) used for the compact representation of texts, trees, graphs, or grids.
Their efficient implementation is one of the most frequently studied problems
in compressed data structures.
  One effective solution is the so-called hybrid bitvector implementation,
which partitions the input bitvector into blocks and adaptively selects an
encoding method, such as run-length, plain, or minority encoding, based on
local redundancy. Experiments have shown that hybrid bitvectors achieve
excellent all-around performance on repetitive and non-repetitive inputs.
  However, current implementations support only rank queries (i.e., counting
the number of ones up to a given position) and lack support for select queries.
This limitation significantly restricts their applicability. In this paper, we
propose a method to add support for select queries to hybrid bitvectors, and we
conduct an extensive set of experiments. Our results show that hybrid
bitvectors offer excellent performance, matching the speed of the fastest and
the space efficiency of the most compact existing bitvectors.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [76] [Bi-Level Game-Theoretic Planning of Cyber Deception for Cognitive Arbitrage](https://arxiv.org/abs/2509.05498)
*Ya-Ting Yang,Quanyan Zhu*

Main category: cs.GT

TL;DR: 本文研究利用高级持续威胁（APT）攻击者的认知漏洞，提出认知感知防御策略，通过博弈论建模设计策略，数值结果显示防御者可放大初始优势并保障长期目标。


<details>
  <summary>Details</summary>
Motivation: 利用认知漏洞获得战略优势，对抗 APT 攻击者。

Method: 提出双层网络战博弈，聚焦防御欺骗机制的战略级设计，运用博弈论推理和分析进行跨层级定量建模。

Result: 虽防御者初始优势随时间减弱，但适时部署欺骗技术能使攻击者规划阶段收益变正，执行阶段总奖励至少提升 40%。

Conclusion: 防御者能放大初始小优势，保持战略优势，保障长期目标，如保护关键资产。

Abstract: Cognitive vulnerabilities shape human decision-making and arise primarily
from two sources: (1) cognitive capabilities, which include disparities in
knowledge, education, expertise, or access to information, and (2) cognitive
biases, such as rational inattention, confirmation bias, and base rate neglect,
which influence how individuals perceive and process information. Exploiting
these vulnerabilities allows an entity with superior cognitive awareness to
gain a strategic advantage, a concept referred to as cognitive arbitrage. This
paper investigates how to exploit the cognitive vulnerabilities of Advanced
Persistent Threat (APT) attackers and proposes cognition-aware defenses that
leverage windows of superiority to counteract attacks. Specifically, the
proposed bi-level cyber warfare game focuses on "strategic-level" design for
defensive deception mechanisms, which then facilitates "operational-level"
actions and tactical-level execution of Tactics, Techniques, and Procedures
(TTPs). Game-theoretic reasoning and analysis play a significant role in the
cross-echelon quantitative modeling and design of cognitive arbitrage
strategies. Our numerical results demonstrate that although the defender's
initial advantage diminishes over time, strategically timed and deployed
deception techniques can turn a negative value for the attacker into a positive
one during the planning phase, and achieve at least a 40% improvement in total
rewards during execution. This demonstrates that the defender can amplify even
small initial advantages, sustain a strategic edge over the attacker, and
secure long-term objectives, such as protecting critical assets throughout the
attacker's lifecycle.

</details>


### [77] [Knapsack Contracts and the Importance of Return-on-Investment](https://arxiv.org/abs/2509.05956)
*Zohar Barak,Asnat Berlin,Ilan Reuven Cohen,Alon Eden,Omri Porat,Inbal Talgam-Cohen*

Main category: cs.GT

TL;DR: 提出背包合约问题，分析其与随机背包问题关系，找出关键参数投资回报率倒数（IOR），开发近似算法并建立下界，表明IOR对理解问题复杂度和近似性很重要。


<details>
  <summary>Details</summary>
Motivation: 研究经典随机背包问题的战略版本——背包合约问题，处理随机处理时间依赖于代理努力的情况。

Method: 将背包合约问题视为带成本和多选择的随机背包问题，找出关键参数IOR，开发基于IOR的近似算法并建立下界。

Result: 开发出O(α)近似策略，建立了Ω(α)下界。

Conclusion: IOR对理解背包合约问题复杂度和近似性是基础的，界定它对获得非平凡近似保证是必要且充分的，凸显了优化问题随机性受战略努力控制时的计算挑战。

Abstract: We formulate the Knapsack Contracts problem -- a strategic version of the
classic Stochastic Knapsack problem, which builds upon the inherent randomness
shared by stochastic optimization and contract design. In this problem, the
principal incentivizes agents to perform jobs with stochastic processing times,
the realization of which depends on the agents' efforts.
  Algorithmically, we show that Knapsack Contracts can be viewed as Stochastic
Knapsack with costs and multi-choice, features that introduce significant new
challenges. We identify a crucial and economically meaningful parameter -- the
Return on Investment (ROI) value. We show that the Inverse of ROI (or IOR for
short) precisely characterizes the extent to which the approximation guarantees
for Stochastic Knapsack extend to its strategic counterpart.
  For IOR of $\alpha$, we develop an algorithm that finds an
$O(\alpha)$-approximation policy that does not rely on adaptivity. We establish
matching $\Omega(\alpha)$ lower bounds, both on the adaptivity gap, and on what
can be achieved without full distributional knowledge of the processing times.
Taken together, our results show that IOR is fundamental to understanding the
complexity and approximability of Knapsack Contracts, and bounding it is both
necessary and sufficient for achieving non-trivial approximation guarantees.
Our results highlight the computational challenges arising when stochasticity
in optimization problems is controlled by strategic effort.

</details>


### [78] [The Keychain Problem: On Minimizing the Opportunity Cost of Uncertainty](https://arxiv.org/abs/2509.06187)
*Ramiro N. Deo-Campo Vuong,Robert Kleinberg,Aditya Prasad,Eric Xiao,Haifeng Xu*

Main category: cs.GT

TL;DR: 本文介绍钥匙链问题，研究不同测试顺序下的该问题，给出精确算法、近似算法和难度结果，并将相关技术应用到在线二分匹配。


<details>
  <summary>Details</summary>
Motivation: 研究在每阶段仅部分动作可用时，通过探索动作集最大化期望回报的顺序决策问题。

Method: 针对不同钥匙链测试顺序假设进行研究，对最简单情况给出精确算法，其他情况给出近似算法，在概率场景设置中基于组合拍卖与顺序决策问题策略设计的新联系设计近似算法。

Result: 得到不同设置下的算法和难度结果，还将技术应用到在线二分匹配及扩展问题得到哲学家不等式。

Conclusion: 所提出的算法和技术具有一定通用性和有效性，可解决相关顺序决策问题及拓展应用。

Abstract: In this paper, we introduce a family of sequential decision-making problems,
collectively called the Keychain Problem, that involve exploring a set of
actions to maximize expected payoff when only a subset of actions are available
in each stage. In an instance of the Keychain Problem, a locksmith faces a
sequence of choices, each of which involves selecting one key from a specified
subset (a keychain) to attempt to open a lock. Given a Bayesian prior on the
effectiveness of keys, the locksmith's goal is to maximize the expected number
of rounds in which the lock is opened -- or equivalently, minimize the
opportunity cost which is the expected number of rounds in which the chain has
a correct key but our selected key is incorrect. We investigate Keychain
Problems under three assumptions on the order in which keychains are tested by
the locksmith: a fixed, known order; a random order sampled from a known
distribution on a set of ``scenarios''; or an order selected by the locksmith
themself. We present an exact algorithm for the simplest of these settings, and
we present approximation algorithms and hardness results for the others. In the
Probabilistic Scenarios setting, our approximation algorithm is based on a
novel connection between combinatorial auctions and policy design for
sequential decision-making problems. To illustrate the generality of this
technique, we apply the same ideas to obtain Philosopher Inequalities for
Online Bipartite Matching and some of its extensions.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [79] [Knowledge-Augmented Relation Learning for Complementary Recommendation with Large Language Models](https://arxiv.org/abs/2509.05564)
*Chihiro Yamasaki,Kai Sugahara,Kazushi Okamoto*

Main category: cs.IR

TL;DR: 提出KARL框架融合主动学习和大语言模型解决互补物品关系标签创建困境，实验显示在OOD和ID设置下结果不同，需动态采样策略。


<details>
  <summary>Details</summary>
Motivation: 行为标签有噪声不可靠，功能标签依赖昂贵人工标注，限制模型泛化能力，需解决两者权衡问题。

Method: 提出Knowledge - Augmented Relation Learning (KARL)框架，融合主动学习和大语言模型，通过选择性采样和LLM标签扩展低成本扩大高质量FBL数据集。

Result: 在OOD设置下，KARL使基线准确率最高提升37%；在ID设置下，提升小于0.5%，长时间学习可能降低准确率。

Conclusion: 由于KARL知识扩展带来的数据多样性，需要根据预测上下文（ID或OOD）调整多样性的动态采样策略。

Abstract: Complementary recommendations play a crucial role in e-commerce by enhancing
user experience through suggestions of compatible items. Accurate
classification of complementary item relationships requires reliable labels,
but their creation presents a dilemma. Behavior-based labels are widely used
because they can be easily generated from interaction logs; however, they often
contain significant noise and lack reliability. While function-based labels
(FBLs) provide high-quality definitions of complementary relationships by
carefully articulating them based on item functions, their reliance on costly
manual annotation severely limits a model's ability to generalize to diverse
items. To resolve this trade-off, we propose Knowledge-Augmented Relation
Learning (KARL), a framework that strategically fuses active learning with
large language models (LLMs). KARL efficiently expands a high-quality FBL
dataset at a low cost by selectively sampling data points that the classifier
finds the most difficult and uses the label extension of the LLM. Our
experiments showed that in out-of-distribution (OOD) settings, an unexplored
item feature space, KARL improved the baseline accuracy by up to 37%. In
contrast, in in-distribution (ID) settings, the learned item feature space, the
improvement was less than 0.5%, with prolonged learning could degrade accuracy.
These contrasting results are due to the data diversity driven by KARL's
knowledge expansion, suggesting the need for a dynamic sampling strategy that
adjusts diversity based on the prediction context (ID or OOD).

</details>


### [80] [LESER: Learning to Expand via Search Engine-feedback Reinforcement in e-Commerce](https://arxiv.org/abs/2509.05570)
*Yipeng Zhang,Bowen Liu,Xiaoshuang Zhang,Aritra Mandal,Zhe Wu,Canran Xu*

Main category: cs.IR

TL;DR: 电商搜索用户查询模糊难匹配，现有方法不足，提出LESER框架，经评估提升多方面效果，是实用可扩展方案。


<details>
  <summary>Details</summary>
Motivation: 解决电商搜索中用户查询模糊、现有方法无法有效捕捉用户意图、难适配平台约束和难以规模化的问题。

Method: 提出LESER框架，用实时搜索引擎反馈监督微调上下文感知大语言模型，将查询扩展作为检索优化任务，利用Group Relative Policy Optimization从相关性和覆盖度指标中学习。

Result: 在大规模真实电商数据集上评估，LESER在离线和在线环境均有显著提升，增强语义覆盖和检索相关性，提高用户参与度。

Conclusion: LESER是适用于现代搜索系统的实用且可扩展的解决方案。

Abstract: User queries in e-commerce search are often vague, short, and underspecified,
making it difficult for retrieval systems to match them accurately against
structured product catalogs. This challenge is amplified by the one-to-many
nature of user intent, where a single query can imply diverse and competing
needs. Existing methods, including neural query expansion and prompting-based
LLM approaches, fall short in real-world settings: they struggle to capture
nuanced user intent, often generate outputs that violate platform constraints,
and rely on workflows that are difficult to scale in production. We propose
Learning to Expand via Search Engine-feedback Reinforcement (LESER), a novel
framework that fine-tunes a context-aware LLM using real-time search engine
feedback as supervision. LESER formulates query expansion as a retrieval
optimization task and leverages Group Relative Policy Optimization to learn
directly from relevance and coverage metrics. LESER is trained to reason over
search results and produce high quality query expansions that align with
platform rules and retrieval objectives. We evaluate LESER on large-scale,
real-world e-commerce datasets, demonstrating substantial improvements in both
offline and online settings. Our results show that LESER not only enhances
semantic coverage and retrieval relevance but also delivers measurable gains in
user engagement, making it a practical and scalable solution for modern search
systems.

</details>


### [81] [Toward Efficient and Scalable Design of In-Memory Graph-Based Vector Search](https://arxiv.org/abs/2509.05750)
*Ilias Azizi,Karima Echihab,Themis Palpanas,Vassilis Christophides*

Main category: cs.IR

TL;DR: 对12种最先进的基于图的向量搜索方法在7个真实数据集上进行实验评估，分享方法优缺点并讨论研究方向。


<details>
  <summary>Details</summary>
Motivation: 向量数据规模增大，分析复杂度增加，基于图的向量搜索方法受青睐，但缺乏对关键算法进展的系统比较。

Method: 在7个真实数据集上对12种最先进的基于图的向量搜索方法进行详尽实验评估。

Result: 发现最佳方法通常基于增量插入和邻域多样化，基础图的选择会影响可扩展性。

Conclusion: 探讨了开放的研究方向，如设计更复杂的数据自适应种子选择和多样化策略的重要性。

Abstract: Vector data is prevalent across business and scientific applications, and its
popularity is growing with the proliferation of learned embeddings. Vector data
collections often reach billions of vectors with thousands of dimensions, thus,
increasing the complexity of their analysis. Vector search is the backbone of
many critical analytical tasks, and graph-based methods have become the best
choice for analytical tasks that do not require guarantees on the quality of
the answers. Although several paradigms (seed selection, incremental insertion,
neighborhood propagation, neighborhood diversification, and divide-and-conquer)
have been employed to design in-memory graph-based vector search algorithms, a
systematic comparison of the key algorithmic advances is still missing. We
conduct an exhaustive experimental evaluation of twelve state-of-the-art
methods on seven real data collections, with sizes up to 1 billion vectors. We
share key insights about the strengths and limitations of these methods; e.g.,
the best approaches are typically based on incremental insertion and
neighborhood diversification, and the choice of the base graph can hurt
scalability. Finally, we discuss open research directions, such as the
importance of devising more sophisticated data adaptive seed selection and
diversification strategies.

</details>


### [82] [A Survey of Real-World Recommender Systems: Challenges, Constraints, and Industrial Perspectives](https://arxiv.org/abs/2509.06002)
*Kuan Zou,Aixin Sun*

Main category: cs.IR

TL;DR: 文章对工业推荐系统进行系统综述，对比学术推荐系统，总结实际场景挑战，给出应对方法并指明研究方向，促进产学研合作。


<details>
  <summary>Details</summary>
Motivation: 学术研究局限于离线数据集优化，缺乏实际用户数据和大规模平台，导致实用性不足，阻碍技术进步。

Method: 对工业推荐系统进行系统回顾，对比学术推荐系统，总结实际推荐场景及挑战，按物品特性和推荐目标分类探讨应对方法。

Result: 指出数据规模、实时性要求和评估方法的差异，总结主要推荐场景挑战及应对方式，给出研究方向。

Conclusion: 旨在提升学术界对实用推荐系统的理解，缩小产学研差距，加强合作。

Abstract: Recommender systems have generated tremendous value for both users and
businesses, drawing significant attention from academia and industry alike.
However, due to practical constraints, academic research remains largely
confined to offline dataset optimizations, lacking access to real user data and
large-scale recommendation platforms. This limitation reduces practical
relevance, slows technological progress, and hampers a full understanding of
the key challenges in recommender systems. In this survey, we provide a
systematic review of industrial recommender systems and contrast them with
their academic counterparts. We highlight key differences in data scale,
real-time requirements, and evaluation methodologies, and we summarize major
real-world recommendation scenarios along with their associated challenges. We
then examine how industry practitioners address these challenges in
Transaction-Oriented Recommender Systems and Content-Oriented Recommender
Systems, a new classification grounded in item characteristics and
recommendation objectives. Finally, we outline promising research directions,
including the often-overlooked role of user decision-making, the integration of
economic and psychological theories, and concrete suggestions for advancing
academic research. Our goal is to enhance academia's understanding of practical
recommender systems, bridge the growing development gap, and foster stronger
collaboration between industry and academia.

</details>


### [83] [Modeling shopper interest broadness with entropy-driven dialogue policy in the context of arbitrarily large product catalogs](https://arxiv.org/abs/2509.06185)
*Firas Jarboui,Issa Memari*

Main category: cs.IR

TL;DR: 本文提出通过检索分数分布的熵来平衡对话推荐系统中探索与利用的挑战，使LLM驱动的代理能实时处理大型产品目录。


<details>
  <summary>Details</summary>
Motivation: 解决对话推荐系统中平衡探索（明确用户需求）和利用（进行推荐）的挑战，特别是在使用大语言模型处理大型产品目录时。

Method: 通过神经检索器获取相关商品，计算重排序分数的熵，根据熵值动态调整对话策略，低熵查询直接推荐，高熵查询提出探索性问题。

Result: 提出的策略能让大语言模型驱动的代理实时应对任意大的产品目录，且不扩大上下文窗口。

Conclusion: 基于检索分数分布熵的策略简单有效，可用于对话推荐系统应对探索与利用的平衡挑战。

Abstract: Conversational recommender systems promise rich interactions for e-commerce,
but balancing exploration (clarifying user needs) and exploitation (making
recommendations) remains challenging, especially when deploying large language
models (LLMs) with vast product catalogs. We address this challenge by modeling
the breadth of user interest via the entropy of retrieval score distributions.
Our method uses a neural retriever to fetch relevant items for a user query and
computes the entropy of the re-ranked scores to dynamically route the dialogue
policy: low-entropy (specific) queries trigger direct recommendations, whereas
high-entropy (ambiguous) queries prompt exploratory questions. This simple yet
effective strategy allows an LLM-driven agent to remain aware of an arbitrarily
large catalog in real-time without bloating its context window.

</details>


### [84] [Language Bias in Information Retrieval: The Nature of the Beast and Mitigation Methods](https://arxiv.org/abs/2509.06195)
*Jinrui Yang,Fan Jiang,Timothy Baldwin*

Main category: cs.IR

TL;DR: 论文聚焦多语言信息检索（MLIR）系统语言公平性，评估公平程度，引入新损失函数LaKDA，揭示当前技术偏差及LaKDA效果。


<details>
  <summary>Details</summary>
Motivation: 确保多语言信息检索系统中不同语言用户能公平获取信息。

Method: 用传统检索方法和基于mBERT与XLM - R的DPR神经排序器评估公平度，引入LaKDA损失函数。

Result: 分析发现当前MLIR技术存在固有语言偏差，不同检索方法差异明显，LaKDA能提升语言公平性。

Conclusion: 当前MLIR技术有语言偏差问题，LaKDA可有效改善语言公平性。

Abstract: Language fairness in multilingual information retrieval (MLIR) systems is
crucial for ensuring equitable access to information across diverse languages.
This paper sheds light on the issue, based on the assumption that queries in
different languages, but with identical semantics, should yield equivalent
ranking lists when retrieving on the same multilingual documents. We evaluate
the degree of fairness using both traditional retrieval methods, and a DPR
neural ranker based on mBERT and XLM-R. Additionally, we introduce `LaKDA', a
novel loss designed to mitigate language biases in neural MLIR approaches. Our
analysis exposes intrinsic language biases in current MLIR technologies, with
notable disparities across the retrieval methods, and the effectiveness of
LaKDA in enhancing language fairness.

</details>


### [85] [AudioBoost: Increasing Audiobook Retrievability in Spotify Search with Synthetic Query Generation](https://arxiv.org/abs/2509.06452)
*Enrico Palumbo,Gustavo Penha,Alva Liu,Marcus Eltscheminov,Jefferson Carvalho dos Santos,Alice Wang,Hugues Bouchard,Humberto Jesús Corona Pampin,Michelle Tran Luu*

Main category: cs.IR

TL;DR: Spotify引入有声书，为提升搜索可检索性提出AudioBoost系统，经评估和A/B测试有积极效果。


<details>
  <summary>Details</summary>
Motivation: 支持用户探索有声书目录，解决冷启动场景下检索偏差问题。

Method: 提出AudioBoost系统，利用大语言模型基于有声书元数据生成合成查询，并在QAC和搜索检索引擎中索引。

Result: 离线评估显示合成查询提高可检索性且质量高，在线A/B测试显示有声书展示、点击和探索性查询完成率提升。

Conclusion: AudioBoost系统能有效提升Spotify有声书搜索的可检索性。

Abstract: Spotify has recently introduced audiobooks as part of its catalog,
complementing its music and podcast offering. Search is often the first entry
point for users to access new items, and an important goal for Spotify is to
support users in the exploration of the audiobook catalog. More specifically,
we would like to enable users without a specific item in mind to broadly search
by topic, genre, story tropes, decade, and discover audiobooks, authors and
publishers they may like. To do this, we need to 1) inspire users to type more
exploratory queries for audiobooks and 2) augment our retrieval systems to
better deal with exploratory audiobook queries. This is challenging in a
cold-start scenario, where we have a retrievabiliy bias due to the little
amount of user interactions with audiobooks compared to previously available
items such as music and podcast content. To address this, we propose
AudioBoost, a system to boost audiobook retrievability in Spotify's Search via
synthetic query generation. AudioBoost leverages Large Language Models (LLMs)
to generate synthetic queries conditioned on audiobook metadata. The synthetic
queries are indexed both in the Query AutoComplete (QAC) and in the Search
Retrieval engine to improve query formulation and retrieval at the same time.
We show through offline evaluation that synthetic queries increase
retrievability and are of high quality. Moreover, results from an online A/B
test show that AudioBoost leads to a +0.7% in audiobook impressions, +1.22% in
audiobook clicks, and +1.82% in audiobook exploratory query completions.

</details>


### [86] [Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for Dynamic Retrieval and Reranking](https://arxiv.org/abs/2509.06472)
*Haoxiang Jin,Ronghan Li,Qiguang Miao,Zixiang Lu*

Main category: cs.IR

TL;DR: 本文提出后检索知识过滤方法应对RAG中确定检索上下文能否增强模型回答能力的挑战，实验显示有显著性能提升和成本降低。


<details>
  <summary>Details</summary>
Motivation: 解决RAG中确定检索上下文能否有效增强模型回答特定查询能力的挑战，现有方法未能充分利用大语言模型连续内部隐藏状态信息。

Method: 提出基于大语言模型内部隐藏状态构建置信度检测模型量化检索上下文对模型置信度的提升，构建偏好数据集微调重排器，引入基于置信度的动态检索方法。

Result: 在上下文筛选准确性和端到端RAG性能上有显著提升，降低检索成本且保持有竞争力的准确性。

Conclusion: 所提出的方法能有效解决RAG中确定检索上下文有效性的问题，提升性能并降低成本。

Abstract: Large Language Models (LLMs) often generate inaccurate responses
(hallucinations) when faced with questions beyond their knowledge scope.
Retrieval-Augmented Generation (RAG) addresses this by leveraging external
knowledge, but a critical challenge remains: determining whether retrieved
contexts effectively enhance the model`s ability to answer specific queries.
This challenge underscores the importance of knowledge boundary awareness,
which current methods-relying on discrete labels or limited signals-fail to
address adequately, as they overlook the rich information in LLMs` continuous
internal hidden states. To tackle this, we propose a novel post-retrieval
knowledge filtering approach. First, we construct a confidence detection model
based on LLMs` internal hidden states to quantify how retrieved contexts
enhance the model`s confidence. Using this model, we build a preference dataset
(NQ_Rerank) to fine-tune a reranker, enabling it to prioritize contexts
preferred by the downstream LLM during reranking. Additionally, we introduce
Confidence-Based Dynamic Retrieval (CBDR), which adaptively triggers retrieval
based on the LLM`s initial confidence in the original question, reducing
knowledge conflicts and improving efficiency. Experimental results demonstrate
significant improvements in accuracy for context screening and end-to-end RAG
performance, along with a notable reduction in retrieval costs while
maintaining competitive accuracy.

</details>


### [87] [Reasoning-enhanced Query Understanding through Decomposition and Interpretation](https://arxiv.org/abs/2509.06544)
*Yunfei Zhong,Jun Yang,Yixing Fan,Jiafeng Guo,Lixin Su,Maarten de Rijke,Ruqing Zhang,Dawei Yin,Xueqi Cheng*

Main category: cs.IR

TL;DR: 提出ReDI方法用于长查询意图理解，在检索任务中效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在长复杂查询意图理解方面研究不足，为提升搜索引擎文档检索准确性需解决该问题。

Method: 提出ReDI方法，采用三阶段流程，分解复杂查询、丰富语义、融合检索结果，还构建数据集并进行模型蒸馏。

Result: 在BRIGHT和BEIR上实验表明，ReDI在稀疏和密集检索范式中均优于强基线。

Conclusion: ReDI方法在长复杂查询理解和文档检索中有效。

Abstract: Accurate inference of user intent is crucial for enhancing document retrieval
in modern search engines. While large language models (LLMs) have made
significant strides in this area, their effectiveness has predominantly been
assessed with short, keyword-based queries. As AI-driven search evolves,
long-form queries with intricate intents are becoming more prevalent, yet they
remain underexplored in the context of LLM-based query understanding (QU). To
bridge this gap, we introduce ReDI: a Reasoning-enhanced approach for query
understanding through Decomposition and Interpretation. ReDI leverages the
reasoning and comprehension capabilities of LLMs in a three-stage pipeline: (i)
it breaks down complex queries into targeted sub-queries to accurately capture
user intent; (ii) it enriches each sub-query with detailed semantic
interpretations to improve the query-document matching; and (iii) it
independently retrieves documents for each sub-query and employs a fusion
strategy to aggregate the results for the final ranking. We compiled a
large-scale dataset of real-world complex queries from a major search engine
and distilled the query understanding capabilities of teacher models into
smaller models for practical application. Experiments on BRIGHT and BEIR
demonstrate that ReDI consistently surpasses strong baselines in both sparse
and dense retrieval paradigms, affirming its effectiveness.

</details>


### [88] [UniSearch: Rethinking Search System with a Unified Generative Architecture](https://arxiv.org/abs/2509.06887)
*Jiahui Chen,Xiaoze Jiang,Zhibo Wang,Quanzhi Zhu,Junyao Zhao,Feng Hu,Kang Pan,Ao Xie,Maohua Pei,Zhiheng Qin,Hongjing Zhang,Zhixin Zhai,Xiaobo Guo,Runbin Zhou,Kefeng Wang,Mingyang Geng,Cheng Chen,Jingshan Lv,Yupeng Huang,Xiao Liang,Han Li*

Main category: cs.IR

TL;DR: 提出适用于快手搜索的统一生成式搜索框架UniSearch，实验证明其有效性和部署潜力。


<details>
  <summary>Details</summary>
Motivation: 传统搜索引擎级联架构设计和维护复杂，现有生成式搜索方法非真正端到端，存在目标不一致和泛化性有限问题。

Method: 提出UniSearch，用端到端架构替代级联管道，集成搜索生成器和视频编码器，统一训练框架优化组件；引入搜索偏好优化（SPO）。

Result: 在工业规模数据集实验和线上A/B测试中表现良好，在直播搜索部署取得近年最大单实验改进。

Conclusion: UniSearch有很强有效性和部署潜力，有实际应用价值。

Abstract: Modern search systems play a crucial role in facilitating information
acquisition. Traditional search engines typically rely on a cascaded
architecture, where results are retrieved through recall, pre-ranking, and
ranking stages. The complexity of designing and maintaining multiple modules
makes it difficult to achieve holistic performance gains. Recent advances in
generative recommendation have motivated the exploration of unified generative
search as an alternative. However, existing approaches are not genuinely
end-to-end: they typically train an item encoder to tokenize candidates first
and then optimize a generator separately, leading to objective inconsistency
and limited generalization. To address these limitations, we propose UniSearch,
a unified generative search framework for Kuaishou Search. UniSearch replaces
the cascaded pipeline with an end-to-end architecture that integrates a Search
Generator and a Video Encoder. The Generator produces semantic identifiers of
relevant items given a user query, while the Video Encoder learns latent item
embeddings and provides their tokenized representations. A unified training
framework jointly optimizes both components, enabling mutual enhancement and
improving representation quality and generation accuracy. Furthermore, we
introduce Search Preference Optimization (SPO), which leverages a reward model
and real user feedback to better align generation with user preferences.
Extensive experiments on industrial-scale datasets, together with online A/B
testing in both short-video and live search scenarios, demonstrate the strong
effectiveness and deployment potential of UniSearch. Notably, its deployment in
live search yields the largest single-experiment improvement in recent years of
our product's history, highlighting its practical value for real-world
applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [89] [Benchmarking Robust Aggregation in Decentralized Gradient Marketplaces](https://arxiv.org/abs/2509.05833)
*Zeyu Song,Sainyam Galhotra,Shagufta Mehnaz*

Main category: cs.LG

TL;DR: 提出全面基准框架评估买方依赖基线数据集的分布式梯度市场中鲁棒梯度聚合方法，提供实用见解及工具。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习基准忽略去中心化梯度市场的关键经济和系统因素，需新评估方法。

Method: 构建模拟环境，改进评估方法，对现有框架进行实证分析，涵盖多种数据集和攻击场景。

Result: 实现了全面的基准框架，对不同聚合策略进行了比较评估。

Conclusion: 该基准为设计更鲁棒、公平和经济可行的去中心化梯度市场提供工具和实证依据。

Abstract: The rise of distributed and privacy-preserving machine learning has sparked
interest in decentralized gradient marketplaces, where participants trade
intermediate artifacts like gradients. However, existing Federated Learning
(FL) benchmarks overlook critical economic and systemic factors unique to such
marketplaces-cost-effectiveness, fairness to sellers, and market
stability-especially when a buyer relies on a private baseline dataset for
evaluation.
  We introduce a comprehensive benchmark framework to holistically evaluate
robust gradient aggregation methods within these buyer-baseline-reliant
marketplaces. Our contributions include: (1) a simulation environment modeling
marketplace dynamics with a variable buyer baseline and diverse seller
distributions; (2) an evaluation methodology augmenting standard FL metrics
with marketplace-centric dimensions such as Economic Efficiency, Fairness, and
Selection Dynamics; (3) an in-depth empirical analysis of the existing
Distributed Gradient Marketplace framework, MartFL, including the integration
and comparative evaluation of adapted FLTrust and SkyMask as alternative
aggregation strategies within it. This benchmark spans diverse datasets, local
attacks, and Sybil attacks targeting the marketplace selection process; and (4)
actionable insights into the trade-offs between model performance, robustness,
cost, fairness, and stability.
  This benchmark equips the community with essential tools and empirical
evidence to evaluate and design more robust, equitable, and economically viable
decentralized gradient marketplaces.

</details>


### [90] [Standard vs. Modular Sampling: Best Practices for Reliable LLM Unlearning](https://arxiv.org/abs/2509.05316)
*Praveen Bushipaka,Lucia Passaro,Tommaso Cucinotta*

Main category: cs.LG

TL;DR: 本文系统评估大语言模型遗忘常见做法，发现单邻居集和标准采样有问题，提出最佳实践及MELU策略。


<details>
  <summary>Details</summary>
Motivation: 现有基准采用单邻居集且未审视采样方法有效性和稳定性，无法反映现实数据情况。

Method: 系统评估常见做法。

Result: 单邻居集欠佳，标准采样掩盖性能权衡。

Conclusion: 提出最佳实践，包括引入多样邻居集、指出标准1:1采样低效、提出MELU策略，该策略结合强大算法可有效遗忘。

Abstract: A conventional LLM Unlearning setting consists of two subsets -"forget" and
"retain", with the objectives of removing the undesired knowledge from the
forget set while preserving the remaining knowledge from the retain. In
privacy-focused unlearning research, a retain set is often further divided into
neighbor sets, containing either directly or indirectly connected to the forget
targets; and augmented by a general-knowledge set. A common practice in
existing benchmarks is to employ only a single neighbor set, with general
knowledge which fails to reflect the real-world data complexities and
relationships. LLM Unlearning typically involves 1:1 sampling or cyclic
iteration sampling. However, the efficacy and stability of these de facto
standards have not been critically examined. In this study, we systematically
evaluate these common practices. Our findings reveal that relying on a single
neighbor set is suboptimal and that a standard sampling approach can obscure
performance trade-offs. Based on this analysis, we propose and validate an
initial set of best practices: (1) Incorporation of diverse neighbor sets to
balance forget efficacy and model utility, (2) Standard 1:1 sampling methods
are inefficient and yield poor results, (3) Our proposed Modular Entity-Level
Unlearning (MELU) strategy as an alternative to cyclic sampling. We demonstrate
that this modular approach, combined with robust algorithms, provides a clear
and stable path towards effective unlearning.

</details>


### [91] [Data-Efficient Time-Dependent PDE Surrogates: Graph Neural Simulators vs Neural Operators](https://arxiv.org/abs/2509.06154)
*Dibyajyoti Nayak,Somdatta Goswami*

Main category: cs.LG

TL;DR: 本文使用图神经模拟器（GNS）构建准确的正向模型学习偏微分方程（PDE）解，在三个典型PDE系统上评估，结果表明GNS提高了数据效率，减少误差积累，结合PCA+KMeans策略提升低数据性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子（NOs）需大量数据且难以处理稀缺数据，很多NO公式未明确编码物理演化的因果、局部时间结构，自回归模型存在误差快速积累问题。

Method: 采用图神经模拟器（GNS）这一消息传递图神经网络框架，结合显式数值时间步长方案，通过建模瞬时时间导数学习PDE解，并引入PCA+KMeans轨迹选择策略。

Result: GNS显著提高数据效率，用更少训练轨迹实现更高泛化精度，在三个PDE系统上仅用3%数据实现相对L2误差低于1%，大幅减少长时间的误差积累，如相比FNO AR和DON AR分别减少82.48%和99.86%的自回归误差。

Conclusion: 将基于图的局部归纳偏置与传统时间积分器相结合，可为时变PDE生成准确、物理一致且可扩展的替代模型。

Abstract: Neural operators (NOs) approximate mappings between infinite-dimensional
function spaces but require large datasets and struggle with scarce training
data. Many NO formulations don't explicitly encode causal, local-in-time
structure of physical evolution. While autoregressive models preserve causality
by predicting next time-steps, they suffer from rapid error accumulation. We
employ Graph Neural Simulators (GNS) - a message-passing graph neural network
framework - with explicit numerical time-stepping schemes to construct accurate
forward models that learn PDE solutions by modeling instantaneous time
derivatives. We evaluate our framework on three canonical PDE systems: (1) 2D
Burgers' scalar equation, (2) 2D coupled Burgers' vector equation, and (3) 2D
Allen-Cahn equation. Rigorous evaluations demonstrate GNS significantly
improves data efficiency, achieving higher generalization accuracy with
substantially fewer training trajectories compared to neural operator baselines
like DeepONet and FNO. GNS consistently achieves under 1% relative L2 errors
with only 30 training samples out of 1000 (3% of available data) across all
three PDE systems. It substantially reduces error accumulation over extended
temporal horizons: averaged across all cases, GNS reduces autoregressive error
by 82.48% relative to FNO AR and 99.86% relative to DON AR. We introduce a
PCA+KMeans trajectory selection strategy enhancing low-data performance.
Results indicate combining graph-based local inductive biases with conventional
time integrators yields accurate, physically consistent, and scalable surrogate
models for time-dependent PDEs.

</details>


### [92] [Feed Two Birds with One Scone: Exploiting Function-Space Regularization for Both OOD Robustness and ID Fine-Tuning Performance](https://arxiv.org/abs/2509.05328)
*Xiang Yuan,Jun Shu,Deyu meng,Zongben Xu*

Main category: cs.LG

TL;DR: 提出新正则化方法提升预训练模型微调时的OOD鲁棒性和ID性能，优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒微调方法不能总是提升不同模型架构的OOD鲁棒性，因其在函数空间优化效果不佳

Method: 提出用模拟OOD样本约束微调与预训练模型在函数空间距离的正则化方法，引入额外一致性正则化促进扰动样本稳定预测

Result: 实验表明该方法能提升下游任务ID微调性能和OOD鲁棒性，优于现有基于正则化的鲁棒微调方法

Conclusion: 所提方法有效，可在多种CLIP骨干网络中应用

Abstract: Robust fine-tuning aims to achieve competitive in-distribution (ID)
performance while maintaining the out-of-distribution (OOD) robustness of a
pre-trained model when transferring it to a downstream task. To remedy this,
most robust fine-tuning methods aim to preserve the pretrained weights,
features, or logits. However, we find that these methods cannot always improve
OOD robustness for different model architectures. This is due to the OOD
robustness requiring the model function to produce stable prediction for input
information of downstream tasks, while existing methods might serve as a poor
proxy for the optimization in the function space. Based on this finding, we
propose a novel regularization that constrains the distance of fine-tuning and
pre-trained model in the function space with the simulated OOD samples, aiming
to preserve the OOD robustness of the pre-trained model. Besides, to further
enhance the OOD robustness capability of the fine-tuning model, we introduce an
additional consistency regularization to promote stable predictions of
perturbed samples. Extensive experiments demonstrate our approach could
consistently improve both downstream task ID fine-tuning performance and OOD
robustness across a variety of CLIP backbones, outperforming existing
regularization-based robust fine-tuning methods.

</details>


### [93] [Safeguarding Graph Neural Networks against Topology Inference Attacks](https://arxiv.org/abs/2509.05429)
*Jie Fu,Hong Yuan,Zhili Chen,Wendy Hui Wang*

Main category: cs.LG

TL;DR: 研究GNN拓扑隐私风险，提出拓扑推理攻击揭示其易受攻击，现有边级隐私机制不足，引入PGR防御框架，实验表明其有效减少拓扑泄露且对模型精度影响小。


<details>
  <summary>Details</summary>
Motivation: GNN广泛应用引发隐私担忧，现有研究多关注边级隐私，拓扑隐私这一关键威胁未充分探索。

Method: 提出拓扑推理攻击（TIAs）；引入私有图重建（PGR）防御框架，将其表述为双层优化问题，迭代生成合成训练图并更新GNN模型。

Result: GNN易受TIAs攻击，现有边级差分隐私机制不足以缓解风险或严重影响模型精度；PGR显著减少拓扑泄露，对模型精度影响小。

Conclusion: PGR能在维护模型精度的同时有效保护拓扑隐私。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful models for learning
from graph-structured data. However, their widespread adoption has raised
serious privacy concerns. While prior research has primarily focused on
edge-level privacy, a critical yet underexplored threat lies in topology
privacy - the confidentiality of the graph's overall structure. In this work,
we present a comprehensive study on topology privacy risks in GNNs, revealing
their vulnerability to graph-level inference attacks. To this end, we propose a
suite of Topology Inference Attacks (TIAs) that can reconstruct the structure
of a target training graph using only black-box access to a GNN model. Our
findings show that GNNs are highly susceptible to these attacks, and that
existing edge-level differential privacy mechanisms are insufficient as they
either fail to mitigate the risk or severely compromise model accuracy. To
address this challenge, we introduce Private Graph Reconstruction (PGR), a
novel defense framework designed to protect topology privacy while maintaining
model accuracy. PGR is formulated as a bi-level optimization problem, where a
synthetic training graph is iteratively generated using meta-gradients, and the
GNN model is concurrently updated based on the evolving graph. Extensive
experiments demonstrate that PGR significantly reduces topology leakage with
minimal impact on model accuracy. Our code is anonymously available at
https://github.com/JeffffffFu/PGR.

</details>


### [94] [Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis](https://arxiv.org/abs/2509.05449)
*Disha Makhija,Manoj Ghuhan Arivazhagan,Vinayshekhar Bannihatti Kumar,Rashmi Gangadharaiah*

Main category: cs.LG

TL;DR: 研究提出memTrace框架，通过分析大语言模型内部表征检测成员推理信号，平均AUC达0.85，强调需深入研究成员隐私和开发隐私保护训练技术。


<details>
  <summary>Details</summary>
Motivation: 现有研究认为大语言模型在成员推理攻击中表现好，可能无隐私泄露风险，本文从分析模型内部表征角度探索潜在成员推理信号，提供补充视角。

Method: 提出memTrace框架，从transformer隐藏状态和注意力模式中提取信息信号，分析层表示动态、注意力分布特征和跨层转换模式。

Result: 在多个模型家族上实现强成员检测，在流行MIA基准上平均AUC分数达0.85。

Conclusion: 即使输出信号受保护，模型内部行为也能揭示训练数据暴露情况，需进一步研究成员隐私和开发更强大的隐私保护训练技术。

Abstract: Membership inference attacks (MIAs) reveal whether specific data was used to
train machine learning models, serving as important tools for privacy auditing
and compliance assessment. Recent studies have reported that MIAs perform only
marginally better than random guessing against large language models,
suggesting that modern pre-training approaches with massive datasets may be
free from privacy leakage risks. Our work offers a complementary perspective to
these findings by exploring how examining LLMs' internal representations,
rather than just their outputs, may provide additional insights into potential
membership inference signals. Our framework, \emph{memTrace}, follows what we
call \enquote{neural breadcrumbs} extracting informative signals from
transformer hidden states and attention patterns as they process candidate
sequences. By analyzing layer-wise representation dynamics, attention
distribution characteristics, and cross-layer transition patterns, we detect
potential memorization fingerprints that traditional loss-based approaches may
not capture. This approach yields strong membership detection across several
model families achieving average AUC scores of 0.85 on popular MIA benchmarks.
Our findings suggest that internal model behaviors can reveal aspects of
training data exposure even when output-based signals appear protected,
highlighting the need for further research into membership privacy and the
development of more robust privacy-preserving training techniques for large
language models.

</details>


### [95] [Calibrated Recommendations with Contextual Bandits](https://arxiv.org/abs/2509.05460)
*Diego Feijer,Himan Abdollahpouri,Sanket Gupta,Alexander Clare,Yuxiao Wen,Todd Wasson,Maria Dimakopoulou,Zahra Nazari,Kyle Kretschman,Mounia Lalmas*

Main category: cs.LG

TL;DR: 针对Spotify主页内容类型推荐不均问题，提出用上下文多臂老虎机校准方法，提升推荐精准度和用户参与度。


<details>
  <summary>Details</summary>
Motivation: Spotify主页历史数据偏向音乐，难以提供平衡个性化内容，且用户偏好随情境变化。

Method: 提出一种利用上下文多臂老虎机的校准方法，动态学习用户在不同情境下的最优内容类型分布。

Result: 离线和在线结果显示，提升了Spotify主页推荐的精准度和用户参与度，特别是对播客等代表性不足的内容类型。

Conclusion: 该方法优于传统依赖历史平均的校准方法，能适应不同情境用户兴趣变化，提升用户参与。

Abstract: Spotify's Home page features a variety of content types, including music,
podcasts, and audiobooks. However, historical data is heavily skewed toward
music, making it challenging to deliver a balanced and personalized content
mix. Moreover, users' preference towards different content types may vary
depending on the time of day, the day of week, or even the device they use. We
propose a calibration method that leverages contextual bandits to dynamically
learn each user's optimal content type distribution based on their context and
preferences. Unlike traditional calibration methods that rely on historical
averages, our approach boosts engagement by adapting to how users interests in
different content types varies across contexts. Both offline and online results
demonstrate improved precision and user engagement with the Spotify Home page,
in particular with under-represented content types such as podcasts.

</details>


### [96] [ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization](https://arxiv.org/abs/2509.05584)
*Sadegh Jafari,Aishwarya Sarkar,Mohiuddin Bilwal,Ali Jannesari*

Main category: cs.LG

TL;DR: 提出ProfilingAgent自动化压缩模型，实验表明在剪枝和量化上效果好，确立智能体系统用于模型优化的可行性。


<details>
  <summary>Details</summary>
Motivation: 基础模型面临计算和内存瓶颈，现有压缩技术多依赖统一启发式方法，且分析工具很少集成到自动化流程。

Method: 提出ProfilingAgent，利用大语言模型通过结构化剪枝和训练后动态量化实现自动化压缩，模块化多智能体系统结合静态和动态指标设计特定策略。

Result: 剪枝保持或提升精度，量化节省内存、减少精度损失且加快推理速度，对比研究凸显大语言模型推理质量对迭代剪枝的重要性。

Conclusion: 智能体系统是分析引导的模型优化的可扩展解决方案。

Abstract: Foundation models face growing compute and memory bottlenecks, hindering
deployment on resource-limited platforms. While compression techniques such as
pruning and quantization are widely used, most rely on uniform heuristics that
ignore architectural and runtime heterogeneity. Profiling tools expose
per-layer latency, memory, and compute cost, yet are rarely integrated into
automated pipelines. We propose ProfilingAgent, a profiling-guided, agentic
approach that uses large language models (LLMs) to automate compression via
structured pruning and post-training dynamic quantization. Our modular
multi-agent system reasons over static metrics (MACs, parameter counts) and
dynamic signals (latency, memory) to design architecture-specific strategies.
Unlike heuristic baselines, ProfilingAgent tailors layer-wise decisions to
bottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 with
ResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitive
or improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 on
smaller datasets), while quantization achieves up to 74% memory savings with
<0.5% accuracy loss. Our quantization also yields consistent inference speedups
of up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbo
highlight the importance of LLM reasoning quality for iterative pruning. These
results establish agentic systems as scalable solutions for profiling-guided
model optimization.

</details>


### [97] [PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series](https://arxiv.org/abs/2509.05478)
*Jia Wang,Xiao Wang,Chi Zhang*

Main category: cs.LG

TL;DR: 提出PLanTS自监督学习框架处理多变量时间序列，在下游任务中表现好。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法处理多变量时间序列有挑战，现有自监督学习方法忽视内在周期性结构和潜在状态动态演化。

Method: 设计周期感知多粒度补丁机制和广义对比损失，设计下一转换预测前置任务。

Result: PLanTS在下游任务中持续提升表征质量，相比现有SSL方法更好，且比基于DTW的方法运行效率更高。

Conclusion: PLanTS能有效处理多变量时间序列，性能优越。

Abstract: Multivariate time series (MTS) are ubiquitous in domains such as healthcare,
climate science, and industrial monitoring, but their high dimensionality,
limited labeled data, and non-stationary nature pose significant challenges for
conventional machine learning methods. While recent self-supervised learning
(SSL) approaches mitigate label scarcity by data augmentations or time
point-based contrastive strategy, they neglect the intrinsic periodic structure
of MTS and fail to capture the dynamic evolution of latent states. We propose
PLanTS, a periodicity-aware self-supervised learning framework that explicitly
models irregular latent states and their transitions. We first designed a
period-aware multi-granularity patching mechanism and a generalized contrastive
loss to preserve both instance-level and state-level similarities across
multiple temporal resolutions. To further capture temporal dynamics, we design
a next-transition prediction pretext task that encourages representations to
encode predictive information about future state evolution. We evaluate PLanTS
across a wide range of downstream tasks-including multi-class and multi-label
classification, forecasting, trajectory tracking and anomaly detection. PLanTS
consistently improves the representation quality over existing SSL methods and
demonstrates superior runtime efficiency compared to DTW-based methods.

</details>


### [98] [STL-based Optimization of Biomolecular Neural Networks for Regression and Control](https://arxiv.org/abs/2509.05481)
*Eric Palanques-Tost,Hanna Krasowski,Murat Arcak,Ron Weiss,Calin Belta*

Main category: cs.LG

TL;DR: 提出用信号时序逻辑（STL）规范为生物分子神经网络（BNNs）定义训练目标，实验证明基于STL学习可有效解决回归和控制任务。


<details>
  <summary>Details</summary>
Motivation: 生物分子神经网络（BNNs）因缺乏目标数据，训练具有挑战性。

Method: 利用信号时序逻辑（STL）规范定义训练目标，基于STL定量语义进行BNN权重的梯度优化，引入学习算法。

Result: 数值实验表明基于STL的学习能有效解决所研究的回归和控制任务。

Conclusion: 基于STL规范的方法可用于BNNs训练，解决生物系统中的回归和控制问题。

Abstract: Biomolecular Neural Networks (BNNs), artificial neural networks with
biologically synthesizable architectures, achieve universal function
approximation capabilities beyond simple biological circuits. However, training
BNNs remains challenging due to the lack of target data. To address this, we
propose leveraging Signal Temporal Logic (STL) specifications to define
training objectives for BNNs. We build on the quantitative semantics of STL,
enabling gradient-based optimization of the BNN weights, and introduce a
learning algorithm that enables BNNs to perform regression and control tasks in
biological systems. Specifically, we investigate two regression problems in
which we train BNNs to act as reporters of dysregulated states, and a feedback
control problem in which we train the BNN in closed-loop with a chronic disease
model, learning to reduce inflammation while avoiding adverse responses to
external infections. Our numerical experiments demonstrate that STL-based
learning can solve the investigated regression and control tasks efficiently.

</details>


### [99] [X-SQL: Expert Schema Linking and Understanding of Text-to-SQL with Multi-LLMs](https://arxiv.org/abs/2509.05899)
*Dazhi Peng*

Main category: cs.LG

TL;DR: 文章提出X - SQL框架解决Text - to - SQL任务中数据库模式信息利用问题，结合新技术在数据集上取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 研究界常忽视数据库模式信息对生成高质量SQL查询的重要性，而该信息在Text - to - SQL任务中作用显著。

Method: 提出含X - Linking和X - Admin两个组件的数据库模式专家；对系统不同组件采用多LLMs；将技术融入端到端框架X - SQL。

Result: X - SQL在Spider - Dev数据集执行准确率达84.9%，在Spider - Test数据集达82.5%。

Conclusion: X - SQL成为基于开源模型的领先Text - to - SQL框架。

Abstract: With Large Language Models' (LLMs) emergent abilities on code generation
tasks, Text-to-SQL has become one of the most popular downstream applications.
Despite the strong results of multiple recent LLM-based Text-to-SQL frameworks,
the research community often overlooks the importance of database schema
information for generating high-quality SQL queries. We find that such schema
information plays a significant or even dominant role in the Text-to-SQL task.
To tackle this challenge, we propose a novel database schema expert with two
components. We first introduce X-Linking, an LLM Supervised Finetuning
(SFT)-based method that achieves superior Schema Linking results compared to
existing open-source Text-to-SQL methods. In addition, we innovatively propose
an X-Admin component that focuses on Schema Understanding by bridging the gap
between abstract schema information and the user's natural language question.
Aside from better learning with schema information, we experiment with
Multi-LLMs for different components within the system to further boost its
performance. By incorporating these techniques into our end-to-end framework,
X-SQL, we have achieved Execution Accuracies of 84.9% on the Spider-Dev dataset
and 82.5% on the Spider-Test dataset. This outstanding performance establishes
X-SQL as the leading Text-to-SQL framework based on open-source models.

</details>


### [100] [Prior Distribution and Model Confidence](https://arxiv.org/abs/2509.05485)
*Maksim Kazanskii,Artem Kasianov*

Main category: cs.LG

TL;DR: 研究训练数据分布对图像分类模型性能的影响，提出框架评估模型对未见数据预测的置信度，提升分类准确率，方法通用且有跨领域应用潜力。


<details>
  <summary>Details</summary>
Motivation: 探究训练数据分布对图像分类模型性能的影响，解决模型对未见数据预测置信度评估问题。

Method: 分析训练集嵌入，过滤低置信度预测；使用多嵌入模型表示训练数据以更稳健地估计置信度。

Result: 显著提升分类准确率，不同架构模型均有性能提升，能更好检测和排除分布外样本。

Conclusion: 提出的方法与模型无关且可推广，除计算机视觉外，在自然语言处理等领域也有应用潜力。

Abstract: This paper investigates the impact of training data distribution on the
performance of image classification models. By analyzing the embeddings of the
training set, we propose a framework to understand the confidence of model
predictions on unseen data without the need for retraining. Our approach
filters out low-confidence predictions based on their distance from the
training distribution in the embedding space, significantly improving
classification accuracy. We demonstrate this on the example of several
classification models, showing consistent performance gains across
architectures. Furthermore, we show that using multiple embedding models to
represent the training data enables a more robust estimation of confidence, as
different embeddings capture complementary aspects of the data. Combining these
embeddings allows for better detection and exclusion of out-of-distribution
samples, resulting in further accuracy improvements. The proposed method is
model-agnostic and generalizable, with potential applications beyond computer
vision, including domains such as Natural Language Processing where prediction
reliability is critical.

</details>


### [101] [MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs](https://arxiv.org/abs/2509.05488)
*Hongjun Xu,Junxi Xia,Weisi Yang,Yueyuan Sui,Stephen Xia*

Main category: cs.LG

TL;DR: 提出首个基于Mamba的神经架构在资源受限MCU上的部署方案MambaLite - Micro，减少内存且保持精度，验证可移植性。


<details>
  <summary>Details</summary>
Motivation: 因内存有限、缺乏原生算子支持和嵌入式友好工具链，Mamba模型在微控制器上部署有挑战。

Method: 将训练好的PyTorch Mamba模型映射到设备执行，包括导出权重到轻量级格式，用C实现手工Mamba层及支持算子并进行算子融合和内存布局优化。

Result: 减少83.0%峰值内存，相对PyTorch实现平均数值误差仅1.7x10 - 5，在KWS和HAR任务上与PyTorch基线100%一致。

Conclusion: MambaLite - Micro可在异构嵌入式平台上稳定运行，为Mamba等先进序列模型用于资源受限应用铺平道路。

Abstract: Deploying Mamba models on microcontrollers (MCUs) remains challenging due to
limited memory, the lack of native operator support, and the absence of
embedded-friendly toolchains. We present, to our knowledge, the first
deployment of a Mamba-based neural architecture on a resource-constrained MCU,
a fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline
maps a trained PyTorch Mamba model to on-device execution by (1) exporting
model weights into a lightweight format, and (2) implementing a handcrafted
Mamba layer and supporting operators in C with operator fusion and memory
layout optimization. MambaLite-Micro eliminates large intermediate tensors,
reducing 83.0% peak memory, while maintaining an average numerical error of
only 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on
keyword spotting(KWS) and human activity recognition (HAR) tasks,
MambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully
preserving classification accuracy. We further validated portability by
deploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating
consistent operation across heterogeneous embedded platforms and paving the way
for bringing advanced sequence models like Mamba to real-world
resource-constrained applications.

</details>


### [102] [CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction](https://arxiv.org/abs/2509.06465)
*Hongzong Li,Jiahao Ma,Zhanpeng Shi,Fanming Jin,Ye-Fan Hu,Jian-Dong Huang*

Main category: cs.LG

TL;DR: 提出CAME - AB框架用于抗体结合位点预测，集成多模态特征，实验表明其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有序列或结构方法依赖单视图特征，在表征和预测上有局限，无法识别抗原上抗体特异性结合位点。

Method: 提出CAME - AB框架，集成五种生物模态形成统一多模态表征，有自适应模态融合模块，结合Transformer编码器和MoE模块，采用监督对比学习目标，训练时应用随机权重平均。

Result: 在基准数据集上实验，CAME - AB在多个指标上始终优于强基线，消融实验验证各组件有效性和多模态特征集成的好处。

Conclusion: CAME - AB是一种有效的抗体结合位点预测方法，模型实现细节和代码公开。

Abstract: Antibody binding site prediction plays a pivotal role in computational
immunology and therapeutic antibody design. Existing sequence or structure
methods rely on single-view features and fail to identify antibody-specific
binding sites on the antigens-a dual limitation in representation and
prediction. In this paper, we propose CAME-AB, a novel Cross-modality Attention
framework with a Mixture-of-Experts (MoE) backbone for robust antibody binding
site prediction. CAME-AB integrates five biologically grounded modalities,
including raw amino acid encodings, BLOSUM substitution profiles, pretrained
language model embeddings, structure-aware features, and GCN-refined
biochemical graphs-into a unified multimodal representation. To enhance
adaptive cross-modal reasoning, we propose an adaptive modality fusion module
that learns to dynamically weight each modality based on its global relevance
and input-specific contribution. A Transformer encoder combined with an MoE
module further promotes feature specialization and capacity expansion. We
additionally incorporate a supervised contrastive learning objective to
explicitly shape the latent space geometry, encouraging intra-class compactness
and inter-class separability. To improve optimization stability and
generalization, we apply stochastic weight averaging during training. Extensive
experiments on benchmark antibody-antigen datasets demonstrate that CAME-AB
consistently outperforms strong baselines on multiple metrics, including
Precision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies further
validate the effectiveness of each architectural component and the benefit of
multimodal feature integration. The model implementation details and the codes
are available on https://anonymous.4open.science/r/CAME-AB-C525

</details>


### [103] [Self-Aligned Reward: Towards Effective and Efficient Reasoners](https://arxiv.org/abs/2509.05489)
*Peixuan Han,Adit Krishnan,Gerald Friedland,Jiaxuan You,Chris Kong*

Main category: cs.LG

TL;DR: 本文提出自对齐奖励（SAR）以解决可验证奖励在大语言模型推理中的局限性，实验表明 SAR 能提升准确率、降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 可验证奖励信号粗糙，存在推理效率低、计算成本高的问题，现有解决方案常牺牲准确性。

Method: 引入自对齐奖励（SAR），定义为基于查询的答案和独立答案之间的相对困惑度差异，将其与 PPO 和 GRPO 等强化学习算法结合。

Result: 在 4 个模型和 7 个基准测试上，集成 SAR 使准确率提高 4%，推理成本降低 30%，能缩短响应并保留高级推理行为。

Conclusion: 自对齐奖励可作为可验证奖励的细粒度补充，为更高效有效的大语言模型训练铺平道路。

Abstract: Reinforcement learning with verifiable rewards has significantly advanced
reasoning in large language models (LLMs), but such signals remain coarse,
offering only binary correctness feedback. This limitation often results in
inefficiencies, including overly verbose reasoning and high computational cost,
while existing solutions often compromise accuracy. To address this, we
introduce self-aligned reward (SAR), a self-guided signal that complements
verifiable rewards to encourage both reasoning accuracy and efficiency. SAR is
defined as the relative perplexity difference between an answer conditioned on
the query and the standalone answer, thereby favoring responses that are
concise and query-specific. Quantitative analysis reveals that SAR reliably
distinguishes answer quality: concise, correct answers score higher than
redundant ones, and partially correct answers score higher than entirely
incorrect ones. Evaluation on 4 models across 7 benchmarks shows that
integrating SAR with prevalent RL algorithms like PPO and GRPO improves
accuracy by 4%, while reducing inference cost by 30%. Further analysis
demonstrates that SAR achieves a Pareto-optimal trade-off between correctness
and efficiency compared to reward signals based on length or self-confidence.
We also show that SAR shortens responses while preserving advanced reasoning
behaviors, demonstrating its ability to suppress unnecessary elaboration
without losing critical reasoning. These results highlight the promise of
self-aligned reward as a fine-grained complement to verifiable rewards, paving
the way for more efficient and effective LLM training.

</details>


### [104] [A machine-learned expression for the excess Gibbs energy](https://arxiv.org/abs/2509.06484)
*Marco Hoffmann,Thomas Specht,Quirin Göttl,Jakob Burger,Stephan Mandt,Hans Hasse,Fabian Jirasek*

Main category: cs.LG

TL;DR: 本文提出模型HANNA，通过结合物理定律和神经网络预测多组分混合物过量吉布斯自由能，表现出色且模型代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决仅从分子结构预测多组分混合物过量吉布斯自由能这一长期挑战。

Method: 将物理定律作为硬约束集成到灵活的神经网络中，开发新的替代求解器，应用几何投影方法。

Result: HANNA预测效果出色，在准确性和范围上明显优于现有基准方法。

Conclusion: 开发的HANNA模型有效解决了多组分混合物过量吉布斯自由能预测问题，且模型和代码开源，提供交互界面。

Abstract: The excess Gibbs energy plays a central role in chemical engineering and
chemistry, providing a basis for modeling the thermodynamic properties of
liquid mixtures. Predicting the excess Gibbs energy of multi-component mixtures
solely from the molecular structures of their components is a long-standing
challenge. In this work, we address this challenge by integrating physical laws
as hard constraints within a flexible neural network. The resulting model,
HANNA, was trained end-to-end on an extensive experimental dataset for binary
mixtures from the Dortmund Data Bank, guaranteeing thermodynamically consistent
predictions. A novel surrogate solver developed in this work enabled the
inclusion of liquid-liquid equilibrium data in the training process.
Furthermore, a geometric projection method was applied to enable robust
extrapolations to multi-component mixtures, without requiring additional
parameters. We demonstrate that HANNA delivers excellent predictions, clearly
outperforming state-of-the-art benchmark methods in accuracy and scope. The
trained model and corresponding code are openly available, and an interactive
interface is provided on our website, MLPROP.

</details>


### [105] [DreamPRM-1.5: Unlocking the Potential of Each Instance for Multimodal Process Reward Model Training](https://arxiv.org/abs/2509.05542)
*Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: 提出DreamPRM - 1.5框架应对多模态过程奖励模型训练挑战，有两种策略，集成到测试时间缩放后在MMMU基准上表现超GPT - 5


<details>
  <summary>Details</summary>
Motivation: 解决多模态过程奖励模型训练中分布偏移和数据噪声问题

Method: 引入DreamPRM - 1.5实例重加权框架，通过双层优化自适应调整训练示例重要性，设计Instance Table和Instance Net两种策略

Result: DreamPRM - 1.5集成到测试时间缩放后在MMMU基准上达到84.6的准确率，超越GPT - 5

Conclusion: DreamPRM - 1.5框架在多模态过程奖励模型训练中有效且表现出色

Abstract: Training multimodal process reward models (PRMs) is challenged by
distribution shifts and noisy data. We introduce DreamPRM-1.5, an
instance-reweighted framework that adaptively adjusts the importance of each
training example via bi-level optimization. We design two complementary
strategies: Instance Table, effective for smaller datasets, and Instance Net,
scalable to larger ones. Integrated into test-time scaling, DreamPRM-1.5
achieves 84.6 accuracy on the MMMU benchmark, surpassing GPT-5.

</details>


### [106] [Distributed Deep Learning using Stochastic Gradient Staleness](https://arxiv.org/abs/2509.05679)
*Viet Hoang Pham,Hyo-Sung Ahn*

Main category: cs.LG

TL;DR: 本文提出一种分布式训练方法加速深度神经网络训练，证明其收敛性并通过实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络训练时间长的问题，尤其是网络变深和数据集变大带来的挑战。

Method: 引入结合数据并行和完全解耦并行反向传播算法的分布式训练方法，利用多计算单元并行处理。

Result: 该方法在一定条件下收敛到临界点，在CIFAR - 10数据集分类任务上的实验证明其有效性。

Conclusion: 所提出的分布式训练方法能显著提高训练效率。

Abstract: Despite the notable success of deep neural networks (DNNs) in solving complex
tasks, the training process still remains considerable challenges. A primary
obstacle is the substantial time required for training, particularly as high
performing DNNs tend to become increasingly deep (characterized by a larger
number of hidden layers) and require extensive training datasets. To address
these challenges, this paper introduces a distributed training method that
integrates two prominent strategies for accelerating deep learning: data
parallelism and fully decoupled parallel backpropagation algorithm. By
utilizing multiple computational units operating in parallel, the proposed
approach enhances the amount of training data processed in each iteration while
mitigating locking issues commonly associated with the backpropagation
algorithm. These features collectively contribute to significant improvements
in training efficiency. The proposed distributed training method is rigorously
proven to converge to critical points under certain conditions. Its
effectiveness is further demonstrated through empirical evaluations, wherein an
DNN is trained to perform classification tasks on the CIFAR-10 dataset.

</details>


### [107] [Reinforcement Learning with Anticipation: A Hierarchical Approach for Long-Horizon Tasks](https://arxiv.org/abs/2509.05545)
*Yang Yu*

Main category: cs.LG

TL;DR: 提出RLA框架解决长周期目标条件任务，学习两个协同模型，有理论证明可接近全局最优策略。


<details>
  <summary>Details</summary>
Motivation: 解决分层强化学习中自动发现层次结构和多级策略联合训练的不稳定性及缺乏理论保证问题。

Method: 引入RLA框架，让智能体学习低层目标条件策略和高层预测模型，用价值几何一致性原则训练预测模型。

Result: 给出证明，表明RLA在多种条件下可接近全局最优策略。

Conclusion: RLA为长周期目标条件任务的分层规划和执行提供了有原则且收敛的方法。

Abstract: Solving long-horizon goal-conditioned tasks remains a significant challenge
in reinforcement learning (RL). Hierarchical reinforcement learning (HRL)
addresses this by decomposing tasks into more manageable sub-tasks, but the
automatic discovery of the hierarchy and the joint training of multi-level
policies often suffer from instability and can lack theoretical guarantees. In
this paper, we introduce Reinforcement Learning with Anticipation (RLA), a
principled and potentially scalable framework designed to address these
limitations. The RLA agent learns two synergistic models: a low-level,
goal-conditioned policy that learns to reach specified subgoals, and a
high-level anticipation model that functions as a planner, proposing
intermediate subgoals on the optimal path to a final goal. The key feature of
RLA is the training of the anticipation model, which is guided by a principle
of value geometric consistency, regularized to prevent degenerate solutions. We
present proofs that RLA approaches the globally optimal policy under various
conditions, establishing a principled and convergent method for hierarchical
planning and execution in long-horizon goal-conditioned tasks.

</details>


### [108] [Nonnegative matrix factorization and the principle of the common cause](https://arxiv.org/abs/2509.03652)
*E. Khalafyan,A. E. Allahverdyan,A. Hovhannisyan*

Main category: cs.LG

TL;DR: 探讨非负矩阵分解（NMF）与共同原因原则（PCC）的关系，PCC助于估计NMF有效秩，NMF可近似实现PCC，还给出聚类和去噪方法。


<details>
  <summary>Details</summary>
Motivation: 研究NMF和PCC两个概念的联系并挖掘其应用价值。

Method: 针对灰度图像数据集，将其映射到概率模型，相互探索NMF和PCC的关系。

Result: PCC能稳定估计NMF有效秩，基于此的NMF特征稳定，解决非可识别问题；NMF可近似实现PCC，给出聚类方法，还可用于数据去噪。

Conclusion: NMF和PCC联系紧密，相互有应用价值，如PCC助于NMF，NMF可实现PCC部分功能。

Abstract: Nonnegative matrix factorization (NMF) is a known unsupervised data-reduction
method. The principle of the common cause (PCC) is a basic methodological
approach in probabilistic causality, which seeks an independent mixture model
for the joint probability of two dependent random variables. It turns out that
these two concepts are closely related. This relationship is explored
reciprocally for several datasets of gray-scale images, which are conveniently
mapped into probability models. On one hand, PCC provides a predictability tool
that leads to a robust estimation of the effective rank of NMF. Unlike other
estimates (e.g., those based on the Bayesian Information Criteria), our
estimate of the rank is stable against weak noise. We show that NMF implemented
around this rank produces features (basis images) that are also stable against
noise and against seeds of local optimization, thereby effectively resolving
the NMF nonidentifiability problem. On the other hand, NMF provides an
interesting possibility of implementing PCC in an approximate way, where larger
and positively correlated joint probabilities tend to be explained better via
the independent mixture model. We work out a clustering method, where data
points with the same common cause are grouped into the same cluster. We also
show how NMF can be employed for data denoising.

</details>


### [109] [Causal Debiasing Medical Multimodal Representation Learning with Missing Modalities](https://arxiv.org/abs/2509.05615)
*Xiaoguang Zhu,Lianlong Sun,Yang Liu,Pengyi Jiang,Uma Srivatsa,Nipavan Chiamvimonvat,Vladimir Filkov*

Main category: cs.LG

TL;DR: 本文指出医学多模态表征学习中现有方法忽略数据采集偏差问题，提出统一框架解决偏差并评估其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实医学数据集存在模态缺失问题，现有方法忽略数据采集过程中的潜在偏差，影响模型泛化。

Method: 对数据生成过程进行结构因果分析，提出统一框架，含基于后门调整的缺失去混杂模块和分离因果特征与虚假关联的双分支神经网络。

Result: 在真实公共和医院数据集上评估，证明了方法的有效性和因果洞察力。

Conclusion: 所提统一框架能解决医学多模态表征学习中数据采集偏差问题，提升模型泛化能力。

Abstract: Medical multimodal representation learning aims to integrate heterogeneous
clinical data into unified patient representations to support predictive
modeling, which remains an essential yet challenging task in the medical data
mining community. However, real-world medical datasets often suffer from
missing modalities due to cost, protocol, or patient-specific constraints.
Existing methods primarily address this issue by learning from the available
observations in either the raw data space or feature space, but typically
neglect the underlying bias introduced by the data acquisition process itself.
In this work, we identify two types of biases that hinder model generalization:
missingness bias, which results from non-random patterns in modality
availability, and distribution bias, which arises from latent confounders that
influence both observed features and outcomes. To address these challenges, we
perform a structural causal analysis of the data-generating process and propose
a unified framework that is compatible with existing direct prediction-based
multimodal learning methods. Our method consists of two key components: (1) a
missingness deconfounding module that approximates causal intervention based on
backdoor adjustment and (2) a dual-branch neural network that explicitly
disentangles causal features from spurious correlations. We evaluated our
method in real-world public and in-hospital datasets, demonstrating its
effectiveness and causal insights.

</details>


### [110] [Learning to Construct Knowledge through Sparse Reference Selection with Reinforcement Learning](https://arxiv.org/abs/2509.05874)
*Shao-An Yin*

Main category: cs.LG

TL;DR: 提出用于稀疏引用选择的深度强化学习框架，在药物 - 基因关系发现任务验证可从部分信息有效构建知识。


<details>
  <summary>Details</summary>
Motivation: 科学文献快速扩张，在专业领域获取新知识困难，需选择阅读的论文。

Method: 提出模仿人类知识构建的深度强化学习框架用于稀疏引用选择。

Result: 在药物 - 基因关系发现任务中，在仅能访问标题和摘要情况下，人和机器都能从部分信息有效构建知识。

Conclusion: 所提方法可帮助在有限时间和成本下优先选择阅读的论文，实现有效知识构建。

Abstract: The rapid expansion of scientific literature makes it increasingly difficult
to acquire new knowledge, particularly in specialized domains where reasoning
is complex, full-text access is restricted, and target references are sparse
among a large set of candidates. We present a Deep Reinforcement Learning
framework for sparse reference selection that emulates human knowledge
construction, prioritizing which papers to read under limited time and cost.
Evaluated on drug--gene relation discovery with access restricted to titles and
abstracts, our approach demonstrates that both humans and machines can
construct knowledge effectively from partial information.

</details>


### [111] [OptiProxy-NAS: Optimization Proxy based End-to-End Neural Architecture Search](https://arxiv.org/abs/2509.05656)
*Bo Lyu,Yu Cui,Tuo Shi,Ke Li*

Main category: cs.LG

TL;DR: 提出OptiProxy - NAS将NAS简化为端到端优化框架，实验证明其搜索结果和效率优越且有灵活性


<details>
  <summary>Details</summary>
Motivation: NAS是计算昂贵的优化问题，现有方法有局限，需加速NAS

Method: 提出优化代理OptiProxy - NAS，用代理表示将NAS空间重新表述为连续、可微和平滑的，用可微优化方法进行基于梯度的搜索

Result: 在12个NAS任务、4个搜索空间和3个不同领域实验中取得优越搜索结果和效率，低保真场景实验验证灵活性

Conclusion: OptiProxy - NAS能有效加速NAS，有良好的搜索性能和灵活性

Abstract: Neural architecture search (NAS) is a hard computationally expensive
optimization problem with a discrete, vast, and spiky search space. One of the
key research efforts dedicated to this space focuses on accelerating NAS via
certain proxy evaluations of neural architectures. Different from the prevalent
predictor-based methods using surrogate models and differentiable architecture
search via supernetworks, we propose an optimization proxy to streamline the
NAS as an end-to-end optimization framework, named OptiProxy-NAS. In
particular, using a proxy representation, the NAS space is reformulated to be
continuous, differentiable, and smooth. Thereby, any differentiable
optimization method can be applied to the gradient-based search of the relaxed
architecture parameters. Our comprehensive experiments on $12$ NAS tasks of $4$
search spaces across three different domains including computer vision, natural
language processing, and resource-constrained NAS fully demonstrate the
superior search results and efficiency. Further experiments on low-fidelity
scenarios verify the flexibility.

</details>


### [112] [DQS: A Low-Budget Query Strategy for Enhancing Unsupervised Data-driven Anomaly Detection Approaches](https://arxiv.org/abs/2509.05663)
*Lucas Correia,Jan-Christoph Goos,Thomas Bäck,Anna V. Kononova*

Main category: cs.LG

TL;DR: 本文将主动学习与无监督异常检测方法结合，提出DQS查询策略，评估其性能并探讨误标记影响，发现不同场景下查询策略各有优劣，有条件时推荐用基于主动学习的阈值。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列无监督异常检测方法存在阈值设置不佳或需标注数据校准的问题。

Method: 将主动学习与现有无监督异常检测方法结合，引入基于差异的查询策略（DQS），通过动态时间规整评估异常得分相似度来最大化查询样本多样性。

Result: DQS在小预算场景表现最佳，其他策略在面对误标记时更稳健，所有查询策略在有误标记时也优于无监督阈值。

Conclusion: 现实中查询策略选择取决于标注者专业知识和标注样本数，有条件查询标注者时，推荐使用基于主动学习的阈值。

Abstract: Truly unsupervised approaches for time series anomaly detection are rare in
the literature. Those that exist suffer from a poorly set threshold, which
hampers detection performance, while others, despite claiming to be
unsupervised, need to be calibrated using a labelled data subset, which is
often not available in the real world. This work integrates active learning
with an existing unsupervised anomaly detection method by selectively querying
the labels of multivariate time series, which are then used to refine the
threshold selection process. To achieve this, we introduce a novel query
strategy called the dissimilarity-based query strategy (DQS). DQS aims to
maximise the diversity of queried samples by evaluating the similarity between
anomaly scores using dynamic time warping. We assess the detection performance
of DQS in comparison to other query strategies and explore the impact of
mislabelling, a topic that is underexplored in the literature. Our findings
indicate that DQS performs best in small-budget scenarios, though the others
appear to be more robust when faced with mislabelling. Therefore, in the real
world, the choice of query strategy depends on the expertise of the oracle and
the number of samples they are willing to label. Regardless, all query
strategies outperform the unsupervised threshold even in the presence of
mislabelling. Thus, whenever it is feasible to query an oracle, employing an
active learning-based threshold is recommended.

</details>


### [113] [GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR](https://arxiv.org/abs/2509.05671)
*Labani Halder,Tanmay Sen,Sarbani Palit*

Main category: cs.LG

TL;DR: 提出基于图的多模态联邦学习框架GraMFedDHAR用于人类活动识别，实验表明MultiModalGCN模型在有无差分隐私设置下均优于基线MultiModalFFN。


<details>
  <summary>Details</summary>
Motivation: 传统集中式深度学习在人类活动识别中有基础设施、网络延迟和数据共享限制，联邦学习需解决异构多模态数据和差分隐私问题。

Method: 将不同传感器数据流建模为特定模态图，通过残差图卷积神经网络处理，用基于注意力的加权融合，差分隐私保护数据聚合。

Result: MultiModalGCN模型在无差分隐私设置下准确率比基线高2%，在差分隐私约束下性能差距达7 - 13%。

Conclusion: 基于图的建模在多模态学习中更稳健，图神经网络对差分隐私噪声导致的性能下降更有抵抗力。

Abstract: Human Activity Recognition (HAR) using multimodal sensor data remains
challenging due to noisy or incomplete measurements, scarcity of labeled
examples, and privacy concerns. Traditional centralized deep learning
approaches are often constrained by infrastructure availability, network
latency, and data sharing restrictions. While federated learning (FL) addresses
privacy by training models locally and sharing only model parameters, it still
has to tackle issues arising from the use of heterogeneous multimodal data and
differential privacy requirements. In this article, a Graph-based Multimodal
Federated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diverse
sensor streams such as a pressure mat, depth camera, and multiple
accelerometers are modeled as modality-specific graphs, processed through
residual Graph Convolutional Neural Networks (GCNs), and fused via
attention-based weighting rather than simple concatenation. The fused
embeddings enable robust activity classification, while differential privacy
safeguards data during federated aggregation. Experimental results show that
the proposed MultiModalGCN model outperforms the baseline MultiModalFFN, with
up to 2 percent higher accuracy in non-DP settings in both centralized and
federated paradigms. More importantly, significant improvements are observed
under differential privacy constraints: MultiModalGCN consistently surpasses
MultiModalFFN, with performance gaps ranging from 7 to 13 percent depending on
the privacy budget and setting. These results highlight the robustness of
graph-based modeling in multimodal learning, where GNNs prove more resilient to
the performance degradation introduced by DP noise.

</details>


### [114] [Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing](https://arxiv.org/abs/2509.06552)
*Zheqi Lv,Wenqiao Zhang,Kairui Fu,Qi Tian,Shengyu Zhang,Jiajie Su,Jingyuan Chen,Kun Kuang,Fei Wu*

Main category: cs.LG

TL;DR: 提出Persona方法解决设备实时数据分布偏移问题，实验验证其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 设备上实时数据分布偏移挑战轻量级设备模型泛化能力，现有研究多依赖数据密集且计算昂贵的微调方法，此问题常被忽视。

Method: 引入基于原型、无反向传播的参数编辑框架Persona，利用云端神经适配器根据实时设备数据生成参数编辑矩阵，使设备模型适应数据分布，聚类成原型模型并动态细化，集成跨层知识转移。

Result: 在多个数据集的视觉任务和推荐任务上进行了广泛实验。

Conclusion: 实验证实了Persona方法的有效性和通用性。

Abstract: The on-device real-time data distribution shift on devices challenges the
generalization of lightweight on-device models. This critical issue is often
overlooked in current research, which predominantly relies on data-intensive
and computationally expensive fine-tuning approaches. To tackle this, we
introduce Persona, a novel personalized method using a prototype-based,
backpropagation-free parameter editing framework to enhance model
generalization without post-deployment retraining. Persona employs a neural
adapter in the cloud to generate a parameter editing matrix based on real-time
device data. This matrix adeptly adapts on-device models to the prevailing data
distributions, efficiently clustering them into prototype models. The
prototypes are dynamically refined via the parameter editing matrix,
facilitating efficient evolution. Furthermore, the integration of cross-layer
knowledge transfer ensures consistent and context-aware multi-layer parameter
changes and prototype assignment. Extensive experiments on vision task and
recommendation task on multiple datasets confirm Persona's effectiveness and
generality.

</details>


### [115] [Ensemble of Precision-Recall Curve (PRC) Classification Trees with Autoencoders](https://arxiv.org/abs/2509.05766)
*Jiaju Miao,Wei Zhu*

Main category: cs.LG

TL;DR: 提出Autoencoder - PRC - RF模型用于异常检测，在多数据集实验中表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 异常检测面临极端类别不平衡和维度灾难两个阻碍，需新方法解决。

Method: 提出将PRC - RF与自编码器集成的混合框架，构建Autoencoder - PRC - RF模型。

Result: 在多个基准数据集上的广泛实验表明，Autoencoder - PRC - RF模型在准确性、可扩展性和可解释性方面优于先前方法。

Conclusion: Autoencoder - PRC - RF模型在高风险异常检测任务中有应用潜力。

Abstract: Anomaly detection underpins critical applications from network security and
intrusion detection to fraud prevention, where recognizing aberrant patterns
rapidly is indispensable. Progress in this area is routinely impeded by two
obstacles: extreme class imbalance and the curse of dimensionality. To combat
the former, we previously introduced Precision-Recall Curve (PRC)
classification trees and their ensemble extension, the PRC Random Forest
(PRC-RF). Building on that foundation, we now propose a hybrid framework that
integrates PRC-RF with autoencoders, unsupervised machine learning methods that
learn compact latent representations, to confront both challenges
simultaneously. Extensive experiments across diverse benchmark datasets
demonstrate that the resulting Autoencoder-PRC-RF model achieves superior
accuracy, scalability, and interpretability relative to prior methods,
affirming its potential for high-stakes anomaly-detection tasks.

</details>


### [116] [Morphological Perceptron with Competitive Layer: Training Using Convex-Concave Procedure](https://arxiv.org/abs/2509.05697)
*Iara Cunha,Marcos Eduardo Valle*

Main category: cs.LG

TL;DR: 本文提出用凸 - 凹过程（CCP）训练带竞争层的形态感知器（MPCL）网络，实验证明该训练方法有效。


<details>
  <summary>Details</summary>
Motivation: 形态算子不可微，基于梯度的优化方法不适用于训练MPCL网络，需替代策略。

Method: 将训练问题表述为凸函数差（DC），用CCP迭代求解，得到一系列线性规划子问题。

Result: 计算实验表明所提训练方法在解决MPCL网络分类任务上有效。

Conclusion: CCP可作为训练MPCL网络的有效方法。

Abstract: A morphological perceptron is a multilayer feedforward neural network in
which neurons perform elementary operations from mathematical morphology. For
multiclass classification tasks, a morphological perceptron with a competitive
layer (MPCL) is obtained by integrating a winner-take-all output layer into the
standard morphological architecture. The non-differentiability of morphological
operators renders gradient-based optimization methods unsuitable for training
such networks. Consequently, alternative strategies that do not depend on
gradient information are commonly adopted. This paper proposes the use of the
convex-concave procedure (CCP) for training MPCL networks. The training problem
is formulated as a difference of convex (DC) functions and solved iteratively
using CCP, resulting in a sequence of linear programming subproblems.
Computational experiments demonstrate the effectiveness of the proposed
training method in addressing classification tasks with MPCL networks.

</details>


### [117] [DCV-ROOD Evaluation Framework: Dual Cross-Validation for Robust Out-of-Distribution Detection](https://arxiv.org/abs/2509.05778)
*Arantxa Urrea-Castaño,Nicolás Segura-Kunsagi,Juan Luis Suárez-Díaz,Rosana Montes,Francisco Herrera*

Main category: cs.LG

TL;DR: 提出用于评估OOD检测模型的双交叉验证框架DCV - ROOD，能有效融合ID和OOD数据，测试表明该方法能快速收敛到真实性能。


<details>
  <summary>Details</summary>
Motivation: 开发可靠的OOD检测方法是重大挑战，严格评估这些技术对确保其有效性至关重要，需要适用于OOD场景的评估框架。

Method: 提出双交叉验证框架DCV - ROOD，对ID数据采用常规划分，对OOD数据按类分组划分，分析有类层次的数据以进行公平的ID - OOD划分。

Result: 选择一组先进的OOD检测方法进行测试，结果显示该方法能快速收敛到真实性能。

Conclusion: 所提出的DCV - ROOD框架可用于鲁棒评估OOD检测模型，提高评估可靠性。

Abstract: Out-of-distribution (OOD) detection plays a key role in enhancing the
robustness of artificial intelligence systems by identifying inputs that differ
significantly from the training distribution, thereby preventing unreliable
predictions and enabling appropriate fallback mechanisms. Developing reliable
OOD detection methods is a significant challenge, and rigorous evaluation of
these techniques is essential for ensuring their effectiveness, as it allows
researchers to assess their performance under diverse conditions and to
identify potential limitations or failure modes. Cross-validation (CV) has
proven to be a highly effective tool for providing a reasonable estimate of the
performance of a learning algorithm. Although OOD scenarios exhibit particular
characteristics, an appropriate adaptation of CV can lead to a suitable
evaluation framework for this setting. This work proposes a dual CV framework
for robust evaluation of OOD detection models, aimed at improving the
reliability of their assessment. The proposed evaluation framework aims to
effectively integrate in-distribution (ID) and OOD data while accounting for
their differing characteristics. To achieve this, ID data are partitioned using
a conventional approach, whereas OOD data are divided by grouping samples based
on their classes. Furthermore, we analyze the context of data with class
hierarchy to propose a data splitting that considers the entire class hierarchy
to obtain fair ID-OOD partitions to apply the proposed evaluation framework.
This framework is called Dual Cross-Validation for Robust Out-of-Distribution
Detection (DCV-ROOD). To test the validity of the evaluation framework, we
selected a set of state-of-the-art OOD detection methods, both with and without
outlier exposure. The results show that the method achieves very fast
convergence to the true performance.

</details>


### [118] [Simulation Priors for Data-Efficient Deep Learning](https://arxiv.org/abs/2509.05732)
*Lenart Treven,Bhavya Sukhija,Jonas Rothfuss,Stelian Coros,Florian Dörfler,Andreas Krause*

Main category: cs.LG

TL;DR: 提出SimPEL方法结合第一性原理模型和数据驱动学习，在多领域表现优，能解决模型强化学习中仿真到现实的差距问题，实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 第一性原理模型难以捕捉现实复杂性，深度学习需大量数据，要让AI系统在现实世界高效学习。

Method: 提出SimPEL方法，在贝叶斯深度学习中用低保真模拟器作为先验，结合第一性原理模型和数据驱动学习。

Result: 在生物、农业、机器人等领域评估显示，SimPEL学习复杂动态性能优；在高速遥控车任务中，比现有基线用更少数据学习停车漂移动作。

Conclusion: SimPEL在复杂现实环境中有数据高效学习和控制的潜力。

Abstract: How do we enable AI systems to efficiently learn in the real-world?
First-principles models are widely used to simulate natural systems, but often
fail to capture real-world complexity due to simplifying assumptions. In
contrast, deep learning approaches can estimate complex dynamics with minimal
assumptions but require large, representative datasets. We propose SimPEL, a
method that efficiently combines first-principles models with data-driven
learning by using low-fidelity simulators as priors in Bayesian deep learning.
This enables SimPEL to benefit from simulator knowledge in low-data regimes and
leverage deep learning's flexibility when more data is available, all the while
carefully quantifying epistemic uncertainty. We evaluate SimPEL on diverse
systems, including biological, agricultural, and robotic domains, showing
superior performance in learning complex dynamics. For decision-making, we
demonstrate that SimPEL bridges the sim-to-real gap in model-based
reinforcement learning. On a high-speed RC car task, SimPEL learns a highly
dynamic parking maneuver involving drifting with substantially less data than
state-of-the-art baselines. These results highlight the potential of SimPEL for
data-efficient learning and control in complex real-world environments.

</details>


### [119] [Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies](https://arxiv.org/abs/2509.05735)
*Jiaqi Chen,Ji Shi,Cansu Sancaktar,Jonas Frey,Georg Martius*

Main category: cs.LG

TL;DR: 研究基于模型的强化学习中在线和离线数据对世界模型及任务性能的影响，发现在线代理表现更优，指出离线代理性能下降原因并给出缓解方法。


<details>
  <summary>Details</summary>
Motivation: 现有文献未深入研究在线和离线数据对世界模型及任务性能的影响，因此开展此研究。

Method: 在31个不同环境中对在线和离线两种范式进行实验。

Result: 在线代理优于离线代理，离线代理性能下降原因是测试时遇到分布外状态，通过额外在线交互或加入探索数据可缓解。

Conclusion: 收集大数据集时建议加入探索数据，而非仅关注专家数据。

Abstract: Data collection is crucial for learning robust world models in model-based
reinforcement learning. The most prevalent strategies are to actively collect
trajectories by interacting with the environment during online training or
training on offline datasets. At first glance, the nature of learning
task-agnostic environment dynamics makes world models a good candidate for
effective offline training. However, the effects of online vs. offline data on
world models and thus on the resulting task performance have not been
thoroughly studied in the literature. In this work, we investigate both
paradigms in model-based settings, conducting experiments on 31 different
environments. First, we showcase that online agents outperform their offline
counterparts. We identify a key challenge behind performance degradation of
offline agents: encountering Out-Of-Distribution states at test time. This
issue arises because, without the self-correction mechanism in online agents,
offline datasets with limited state space coverage induce a mismatch between
the agent's imagination and real rollouts, compromising policy training. We
demonstrate that this issue can be mitigated by allowing for additional online
interactions in a fixed or adaptive schedule, restoring the performance of
online training with limited interaction data. We also showcase that
incorporating exploration data helps mitigate the performance degradation of
offline agents. Based on our insights, we recommend adding exploration data
when collecting large datasets, as current efforts predominantly focus on
expert data alone.

</details>


### [120] [The Measure of Deception: An Analysis of Data Forging in Machine Unlearning](https://arxiv.org/abs/2509.05865)
*Rishabh Dixit,Yuan Hui,Rayan Saab*

Main category: cs.LG

TL;DR: 本文聚焦机器学习去学习中的伪造问题，分析了ε - 伪造集的性质，证明对抗伪造有根本限制且原则上可检测虚假去学习声明。


<details>
  <summary>Details</summary>
Motivation: 隐私法规及减轻有害数据影响的需求推动机器学习去学习发展，但验证去学习存在伪造挑战，即伪造数据模拟目标梯度造成去学习假象。

Method: 定义ε - 伪造集，分析其性质，针对线性回归和单层神经网络等情况，推导伪造集的勒贝格测度的缩放阶，在温和正则假设下证明更一般的测度衰减形式，还扩展到批量SGD和几乎处处光滑的损失函数，并建立概率界。

Result: 线性回归和单层神经网络中伪造集勒贝格测度小，按ε或ε^d缩放；一般情况下按ε^((d - r)/2)衰减；扩展情况有相同渐近缩放；非退化数据分布下随机采样到伪造点的概率极小。

Conclusion: 对抗伪造有根本限制，原则上可检测虚假去学习声明。

Abstract: Motivated by privacy regulations and the need to mitigate the effects of
harmful data, machine unlearning seeks to modify trained models so that they
effectively ``forget'' designated data. A key challenge in verifying unlearning
is forging -- adversarially crafting data that mimics the gradient of a target
point, thereby creating the appearance of unlearning without actually removing
information. To capture this phenomenon, we consider the collection of data
points whose gradients approximate a target gradient within tolerance
$\epsilon$ -- which we call an $\epsilon$-forging set -- and develop a
framework for its analysis. For linear regression and one-layer neural
networks, we show that the Lebesgue measure of this set is small. It scales on
the order of $\epsilon$, and when $\epsilon$ is small enough, $\epsilon^d$.
More generally, under mild regularity assumptions, we prove that the forging
set measure decays as $\epsilon^{(d-r)/2}$, where $d$ is the data dimension and
$r<d$ is the nullity of a variation matrix defined by the model gradients.
Extensions to batch SGD and almost-everywhere smooth loss functions yield the
same asymptotic scaling. In addition, we establish probability bounds showing
that, under non-degenerate data distributions, the likelihood of randomly
sampling a forging point is vanishingly small. These results provide evidence
that adversarial forging is fundamentally limited and that false unlearning
claims can, in principle, be detected.

</details>


### [121] [Real-E: A Foundation Benchmark for Advancing Robust and Generalizable Electricity Forecasting](https://arxiv.org/abs/2509.05768)
*Chen Shao,Yue Wang,Zhenyi Zhu,Zhanbo Huang,Sebastian Pütz,Benjamin Schäfer,Tobais Käfer,Michael Färber*

Main category: cs.LG

TL;DR: 提出Real - E数据集用于能源预测，对20多个基线模型进行基准测试，指出现有方法局限性。


<details>
  <summary>Details</summary>
Motivation: 现有能源预测基准在时空范围和多能源特征上有限，可靠性和适用性存疑。

Method: 提出覆盖30多个欧洲国家超74个电站10年数据及丰富元数据的Real - E数据集，利用其进行广泛数据分析和基准测试，引入新指标量化相关性结构变化。

Result: 现有方法在Real - E数据集上表现不佳，该数据集具有更复杂和非平稳的相关性动态。

Conclusion: 指出当前方法的关键局限性，为构建更稳健预测模型提供实证基础。

Abstract: Energy forecasting is vital for grid reliability and operational efficiency.
Although recent advances in time series forecasting have led to progress,
existing benchmarks remain limited in spatial and temporal scope and lack
multi-energy features. This raises concerns about their reliability and
applicability in real-world deployment. To address this, we present the Real-E
dataset, covering over 74 power stations across 30+ European countries over a
10-year span with rich metadata. Using Real- E, we conduct an extensive data
analysis and benchmark over 20 baselines across various model types. We
introduce a new metric to quantify shifts in correlation structures and show
that existing methods struggle on our dataset, which exhibits more complex and
non-stationary correlation dynamics. Our findings highlight key limitations of
current methods and offer a strong empirical basis for building more robust
forecasting models

</details>


### [122] [Smoothed Online Optimization for Target Tracking: Robust and Learning-Augmented Algorithms](https://arxiv.org/abs/2509.05930)
*Ali Zeynali,Mahsa Sahebdel,Qingsong Liu,Mohammad Hajiesmaili,Ramesh K. Sitaraman*

Main category: cs.LG

TL;DR: 提出SOOTT问题框架，包含三个在线决策目标，给出BEST算法和学习增强变体CoRT，理论分析和案例验证算法有效性。


<details>
  <summary>Details</summary>
Motivation: 解决在线不确定决策中的目标跟踪问题，如AI集群中的工作负载调度。

Method: 提出BEST算法，有可证明的竞争保证；引入学习增强变体CoRT，将不可信的黑盒预测纳入决策过程。

Result: 理论分析表明CoRT在预测准确时优于BEST，且在任意预测误差下保持鲁棒性；案例研究验证两种算法能有效平衡跟踪、决策平滑和抗干扰能力。

Conclusion: 所提出的SOOTT框架及BEST、CoRT算法在处理在线不确定决策的目标跟踪问题上有效。

Abstract: We introduce the Smoothed Online Optimization for Target Tracking (SOOTT)
problem, a new framework that integrates three key objectives in online
decision-making under uncertainty: (1) tracking cost for following a
dynamically moving target, (2) adversarial perturbation cost for withstanding
unpredictable disturbances, and (3) switching cost for penalizing abrupt
changes in decisions. This formulation captures real-world scenarios such as
elastic and inelastic workload scheduling in AI clusters, where operators must
balance long-term service-level agreements (e.g., LLM training) against sudden
demand spikes (e.g., real-time inference). We first present BEST, a robust
algorithm with provable competitive guarantees for SOOTT. To enhance practical
performance, we introduce CoRT, a learning-augmented variant that incorporates
untrusted black-box predictions (e.g., from ML models) into its decision
process. Our theoretical analysis shows that CoRT strictly improves over BEST
when predictions are accurate, while maintaining robustness under arbitrary
prediction errors. We validate our approach through a case study on workload
scheduling, demonstrating that both algorithms effectively balance trajectory
tracking, decision smoothness, and resilience to external disturbances.

</details>


### [123] [If generative AI is the answer, what is the question?](https://arxiv.org/abs/2509.06120)
*Ambuj Tewari*

Main category: cs.LG

TL;DR: 文章探讨生成式AI基础，调查五大生成模型家族，引入概率和博弈论框架，讨论模型部署修改及社会责任话题，采用任务优先视角。


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI作为独特机器学习任务的基础，明确其核心问题。

Method: 调查五大生成模型家族，引入概率和博弈论框架，讨论模型后训练修改。

Result: 梳理了生成式AI的基础、模型、框架及部署相关内容，指出社会责任重要话题。

Conclusion: 采用任务优先视角有助于理解生成式AI这一机器学习问题。

Abstract: Beginning with text and images, generative AI has expanded to audio, video,
computer code, and molecules. Yet, if generative AI is the answer, what is the
question? We explore the foundations of generation as a distinct machine
learning task with connections to prediction, compression, and decision-making.
We survey five major generative model families: autoregressive models,
variational autoencoders, normalizing flows, generative adversarial networks,
and diffusion models. We then introduce a probabilistic framework that
emphasizes the distinction between density estimation and generation. We review
a game-theoretic framework with a two-player adversary-learner setup to study
generation. We discuss post-training modifications that prepare generative
models for deployment. We end by highlighting some important topics in socially
responsible generation such as privacy, detection of AI-generated content, and
copyright and IP. We adopt a task-first framing of generation, focusing on what
generation is as a machine learning problem, rather than only on how models
implement it.

</details>


### [124] [Select, then Balance: A Plug-and-Play Framework for Exogenous-Aware Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.05779)
*Wei Chen,Yuqian Wu,Yuanshao Zhu,Xixuan Hao,Shiyu Wang,Yuxuan Liang*

Main category: cs.LG

TL;DR: 本文提出一种时空预测中对外部变量建模的新框架，采用“选择，然后平衡”范式，实验证明其有效、通用、鲁棒且高效。


<details>
  <summary>Details</summary>
Motivation: 现有时空预测解决方案仅关注有限目标变量，结合外部变量虽有前景但面临不同外部变量影响不一致、历史与未来变量影响不平衡的挑战。

Method: 引入新框架，先构建潜在空间门控专家模块动态选择和重组显著信号，再设计孪生网络架构，将重组后的过去和未来外部变量表示输入双分支时空骨干网络，最后通过上下文感知加权机制集成输出以实现动态平衡。

Result: 在真实数据集上的广泛实验表明所提框架有效、通用、鲁棒且高效。

Conclusion: 所提出的时空预测中对外部变量建模的框架能有效应对现有挑战，具有良好性能。

Abstract: Spatio-temporal forecasting aims to predict the future state of dynamic
systems and plays an important role in multiple fields. However, existing
solutions only focus on modeling using a limited number of observed target
variables. In real-world scenarios, exogenous variables can be integrated into
the model as additional input features and associated with the target signal to
promote forecast accuracy. Although promising, this still encounters two
challenges: the inconsistent effects of different exogenous variables to the
target system, and the imbalance effects between historical variables and
future variables. To address these challenges, this paper introduces \model, a
novel framework for modeling \underline{exo}genous variables in
\underline{s}patio-\underline{t}emporal forecasting, which follows a ``select,
then balance'' paradigm. Specifically, we first construct a latent space gated
expert module, where fused exogenous information is projected into a latent
space to dynamically select and recompose salient signals via specialized
sub-experts. Furthermore, we design a siamese network architecture in which
recomposed representations of past and future exogenous variables are fed into
dual-branch spatio-temporal backbones to capture dynamic patterns. The outputs
are integrated through a context-aware weighting mechanism to achieve dynamic
balance during the modeling process. Extensive experiments on real-world
datasets demonstrate the effectiveness, generality, robustness, and efficiency
of our proposed framework.

</details>


### [125] [time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models](https://arxiv.org/abs/2509.05801)
*Debdeep Sanyal,Aaryan Nagpal,Dhruv Kumar,Murari Mandal,Saurabh Deshpande*

Main category: cs.LG

TL;DR: 本文通过激活移植方法研究基于Transformer的基础模型是否内化语义概念及能否模拟罕见事件，发现模型存在可引导、基于语义的表征。


<details>
  <summary>Details</summary>
Motivation: 探究基于Transformer的基础模型是否内化语义概念，以及能否利用其内部表征模拟罕见高风险事件。

Method: 引入激活移植的因果干预方法，在正向传播中对隐藏状态进行操作。

Result: 注入不同语义能确定性地引导预测，模型编码了事件严重程度的分级概念，可引导、基于语义的表征是大型时间序列Transformer的稳健属性。

Conclusion: 模型存在潜在概念空间支配预测，可将可解释性从事后归因转向直接因果干预，实现语义“假设分析”用于战略压力测试。

Abstract: While transformer-based foundation models excel at forecasting routine
patterns, two questions remain: do they internalize semantic concepts such as
market regimes, or merely fit curves? And can their internal representations be
leveraged to simulate rare, high-stakes events such as market crashes? To
investigate this, we introduce activation transplantation, a causal
intervention that manipulates hidden states by imposing the statistical moments
of one event (e.g., a historical crash) onto another (e.g., a calm period)
during the forward pass. This procedure deterministically steers forecasts:
injecting crash semantics induces downturn predictions, while injecting calm
semantics suppresses crashes and restores stability. Beyond binary control, we
find that models encode a graded notion of event severity, with the latent
vector norm directly correlating with the magnitude of systemic shocks.
Validated across two architecturally distinct TSFMs, Toto (decoder only) and
Chronos (encoder-decoder), our results demonstrate that steerable, semantically
grounded representations are a robust property of large time series
transformers. Our findings provide evidence for a latent concept space that
governs model predictions, shifting interpretability from post-hoc attribution
to direct causal intervention, and enabling semantic "what-if" analysis for
strategic stress-testing.

</details>


### [126] [Concolic Testing on Individual Fairness of Neural Network Models](https://arxiv.org/abs/2509.06864)
*Ming-I Huang,Chih-Duo Hong,Fang Yu*

Main category: cs.LG

TL;DR: 本文介绍了用于评估和验证深度神经网络个体公平性的框架PyFair，通过适配工具生成约束探索网络行为，在25个基准模型上评估，显示其检测和验证公平性有效但复杂模型有扩展性挑战。


<details>
  <summary>Details</summary>
Motivation: 为深度神经网络的个体公平性评估和验证提供正式框架，推动关键领域的算法公平性。

Method: 适配PyCT生成公平性特定路径约束，采用双网络架构进行全面公平性评估。

Result: 在25个基准模型上评估，PyFair能有效检测歧视实例和验证公平性，但复杂模型存在扩展性挑战。

Conclusion: PyFair为预训练深度神经网络的公平性测试和验证提供了严格、系统的方法，推动了算法公平性发展。

Abstract: This paper introduces PyFair, a formal framework for evaluating and verifying
individual fairness of Deep Neural Networks (DNNs). By adapting the concolic
testing tool PyCT, we generate fairness-specific path constraints to
systematically explore DNN behaviors. Our key innovation is a dual network
architecture that enables comprehensive fairness assessments and provides
completeness guarantees for certain network types. We evaluate PyFair on 25
benchmark models, including those enhanced by existing bias mitigation
techniques. Results demonstrate PyFair's efficacy in detecting discriminatory
instances and verifying fairness, while also revealing scalability challenges
for complex models. This work advances algorithmic fairness in critical domains
by offering a rigorous, systematic method for fairness testing and verification
of pre-trained DNNs.

</details>


### [127] [Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments and Reinforcement Learning](https://arxiv.org/abs/2509.06213)
*Christo Mathew,Wentian Wang,Lazaros Gallos,Paul Kantor,Vladimir Menkov,Hao Wang*

Main category: cs.LG

TL;DR: 研究在GOHR环境中使用强化学习，探索两种状态表示策略并采用Transformer - based A2C算法训练，进行多实验设置评估。


<details>
  <summary>Details</summary>
Motivation: 在GOHR环境中，让智能体在部分观察条件下，同时推断隐藏规则并学习最优策略。

Method: 探索Feature - Centric (FC)和Object - Centric (OC)两种状态表示策略，采用Transformer - based Advantage Actor - Critic (A2C)算法训练。

Result: 原文未提及具体结果

Conclusion: 原文未提及具体结论

Abstract: We investigate reinforcement learning in the Game Of Hidden Rules (GOHR)
environment, a complex puzzle in which an agent must infer and execute hidden
rules to clear a 6$\times$6 board by placing game pieces into buckets. We
explore two state representation strategies, namely Feature-Centric (FC) and
Object-Centric (OC), and employ a Transformer-based Advantage Actor-Critic
(A2C) algorithm for training. The agent has access only to partial observations
and must simultaneously infer the governing rule and learn the optimal policy
through experience. We evaluate our models across multiple rule-based and
trial-list-based experimental setups, analyzing transfer effects and the impact
of representation on learning efficiency.

</details>


### [128] [Simple Optimizers for Convex Aligned Multi-Objective Optimization](https://arxiv.org/abs/2509.05811)
*Ben Kretzu,Karen Ullrich,Yonathan Efroni*

Main category: cs.LG

TL;DR: 本文放宽假设，研究凸对齐多目标优化（AMOO）的梯度下降算法，开发新分析工具，提出可扩展算法并证明收敛性，还证明了简单等权重方法的次优性。


<details>
  <summary>Details</summary>
Motivation: 现有AMOO框架分析依赖强假设，如强凸性，与深度学习实践不符，需放宽假设进行研究。

Method: 在标准平滑性或Lipschitz连续性条件下研究凸AMOO的梯度下降算法，开发新分析工具和指标。

Result: 提出凸AMOO的可扩展算法，建立收敛性保证，证明简单等权重方法的次优性。

Conclusion: 放宽假设后的研究可行，所提算法有收敛保证且优于简单等权重方法。

Abstract: It is widely recognized in modern machine learning practice that access to a
diverse set of tasks can enhance performance across those tasks. This
observation suggests that, unlike in general multi-objective optimization, the
objectives in many real-world settings may not be inherently conflicting. To
address this, prior work introduced the Aligned Multi-Objective Optimization
(AMOO) framework and proposed gradient-based algorithms with provable
convergence guarantees. However, existing analysis relies on strong
assumptions, particularly strong convexity, which implies the existence of a
unique optimal solution. In this work, we relax this assumption and study
gradient-descent algorithms for convex AMOO under standard smoothness or
Lipschitz continuity conditions-assumptions more consistent with those used in
deep learning practice. This generalization requires new analytical tools and
metrics to characterize convergence in the convex AMOO setting. We develop such
tools, propose scalable algorithms for convex AMOO, and establish their
convergence guarantees. Additionally, we prove a novel lower bound that
demonstrates the suboptimality of naive equal-weight approaches compared to our
methods.

</details>


### [129] [Performance of Conformal Prediction in Capturing Aleatoric Uncertainty](https://arxiv.org/abs/2509.05826)
*Misgina Tsighe Hagos,Claes Lundström*

Main category: cs.LG

TL;DR: 研究共形预测器量化随机不确定性的有效性，发现其输出与人类注释相关性弱，捕获随机不确定性能力有限。


<details>
  <summary>Details</summary>
Motivation: 缺乏共形预测器捕获随机不确定性有效性的证据，需验证其相关属性。

Method: 测量预测集大小与人类标注每个实例不同标签数量的相关性，评估预测集与人类标注的相似性，用三种共形预测方法为八个深度学习模型在四个数据集上生成预测集。

Result: 绝大多数共形预测输出与人类注释相关性很弱到弱，只有少数呈中度相关。

Conclusion: 共形预测器虽能提供更高的真实类覆盖，但捕获随机不确定性的能力有限，需重新评估其生成的预测集。

Abstract: Conformal prediction is a model-agnostic approach to generating prediction
sets that cover the true class with a high probability. Although its prediction
set size is expected to capture aleatoric uncertainty, there is a lack of
evidence regarding its effectiveness. The literature presents that prediction
set size can upper-bound aleatoric uncertainty or that prediction sets are
larger for difficult instances and smaller for easy ones, but a validation of
this attribute of conformal predictors is missing. This work investigates how
effectively conformal predictors quantify aleatoric uncertainty, specifically
the inherent ambiguity in datasets caused by overlapping classes. We perform
this by measuring the correlation between prediction set sizes and the number
of distinct labels assigned by human annotators per instance. We further assess
the similarity between prediction sets and human-provided annotations. We use
three conformal prediction approaches to generate prediction sets for eight
deep learning models trained on four datasets. The datasets contain annotations
from multiple human annotators (ranging from five to fifty participants) per
instance, enabling the identification of class overlap. We show that the vast
majority of the conformal prediction outputs show a very weak to weak
correlation with human annotations, with only a few showing moderate
correlation. These findings underscore the necessity of critically reassessing
the prediction sets generated using conformal predictors. While they can
provide a higher coverage of the true classes, their capability in capturing
aleatoric uncertainty remains limited.

</details>


### [130] [On optimal solutions of classical and sliced Wasserstein GANs with non-Gaussian data](https://arxiv.org/abs/2509.06505)
*Yu-Jui Huang,Hsin-Hua Shen,Yu-Chih Huang,Wan-Yi Lin,Shih-Chun Lin*

Main category: cs.LG

TL;DR: 本文聚焦非LQG设置下Wasserstein GAN（WGAN）最优参数表征，推导一维WGAN封闭形式最优参数，扩展到高维并证明线性生成器渐近最优，实证表明参数收敛性好且节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 现有GAN参数选择常需穷举搜索，理论最优选择方法少，WGAN最优参数研究局限于LQG设置，故研究非LQG设置下最优参数。

Method: 推导一维WGAN在NN有非线性激活函数和数据非高斯时的封闭形式最优参数；采用切片Wasserstein框架，用原数据联合分布约束替代随机投影数据边缘分布约束以扩展到高维。

Result: 线性生成器对非高斯数据的切片WGAN渐近最优；封闭形式WGAN参数在高斯和拉普拉斯分布数据下有良好收敛行为；切片WGAN提出的解决方案比r - PCA节省计算资源且性能相当。

Conclusion: 本文方法能有效解决非LQG设置下WGAN最优参数问题，参数收敛性好且有计算资源优势。

Abstract: The generative adversarial network (GAN) aims to approximate an unknown
distribution via a parameterized neural network (NN). While GANs have been
widely applied in reinforcement and semisupervised learning as well as computer
vision tasks, selecting their parameters often needs an exhaustive search and
only a few selection methods can be proved to be theoretically optimal. One of
the most promising GAN variants is the Wasserstein GAN (WGAN). Prior work on
optimal parameters for WGAN is limited to the linear-quadratic-Gaussian (LQG)
setting, where the NN is linear and the data is Gaussian. In this paper, we
focus on the characterization of optimal WGAN parameters beyond the LQG
setting. We derive closed-form optimal parameters for one-dimensional WGANs
when the NN has non-linear activation functions and the data is non-Gaussian.
To extend this to high-dimensional WGANs, we adopt the sliced Wasserstein
framework and replace the constraint on marginal distributions of the randomly
projected data by a constraint on the joint distribution of the original
(unprojected) data. We show that the linear generator can be asymptotically
optimal for sliced WGAN with non-Gaussian data. Empirical studies show that our
closed-form WGAN parameters have good convergence behavior with data under both
Gaussian and Laplace distributions. Also, compared to the r principal component
analysis (r-PCA) solution, our proposed solution for sliced WGAN can achieve
the same performance while requiring less computational resources.

</details>


### [131] [Finetuning LLMs for Human Behavior Prediction in Social Science Experiments](https://arxiv.org/abs/2509.05830)
*Akaash Kolluri,Shengguang Wu,Joon Sung Park,Michael S. Bernstein*

Main category: cs.LG

TL;DR: 研究表明微调大语言模型可提升社会科学实验模拟准确性，构建SocSci210数据集，最强模型表现出色，还降低了偏差。


<details>
  <summary>Details</summary>
Motivation: 探索利用大语言模型模拟社会科学实验结果，提升模拟准确性。

Method: 构建包含210个开源社会科学实验数据的SocSci210数据集，对大语言模型进行微调。

Result: 最强模型Socrates - Qwen - 14B在未见过的研究中预测与人类响应分布的一致性比基础模型高26%，优于GPT - 4o 13%；对新条件的泛化能力提升71%；降低人口统计学偏差10.6%。

Conclusion: 在社会科学特定数据上微调大语言模型可实现更准确的实验假设筛选模拟。

Abstract: Large language models (LLMs) offer a powerful opportunity to simulate the
results of social science experiments. In this work, we demonstrate that
finetuning LLMs directly on individual-level responses from past experiments
meaningfully improves the accuracy of such simulations across diverse social
science domains. We construct SocSci210 via an automatic pipeline, a dataset
comprising 2.9 million responses from 400,491 participants in 210 open-source
social science experiments. Through finetuning, we achieve multiple levels of
generalization. In completely unseen studies, our strongest model,
Socrates-Qwen-14B, produces predictions that are 26% more aligned with
distributions of human responses to diverse outcome questions under varying
conditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by
13%. By finetuning on a subset of conditions in a study, generalization to new
unseen conditions is particularly robust, improving by 71%. Since SocSci210
contains rich demographic information, we reduce demographic parity, a measure
of bias, by 10.6% through finetuning. Because social sciences routinely
generate rich, topic-specific datasets, our findings indicate that finetuning
on such data could enable more accurate simulations for experimental hypothesis
screening. We release our data, models and finetuning code at
stanfordhci.github.io/socrates.

</details>


### [132] [Not All Samples Are Equal: Quantifying Instance-level Difficulty in Targeted Data Poisoning](https://arxiv.org/abs/2509.06896)
*William Xu,Yiwei Lu,Yihan Wang,Matthew Y. R. Yang,Zuoqiu Liu,Gautam Kamath,Yaoliang Yu*

Main category: cs.LG

TL;DR: 研究有针对性的数据投毒攻击中不同测试样本易受攻击程度，提出三个预测指标，实验证明指标有效。


<details>
  <summary>Details</summary>
Motivation: 有针对性的数据投毒攻击威胁大，需了解哪些测试样本更易受攻击。

Method: 引入遍历预测准确率、投毒距离和投毒预算三个预测指标。

Result: 三个指标能有效预测现实世界中不同场景下有针对性投毒攻击的难度。

Conclusion: 指标为从业者进行漏洞评估和理解数据投毒攻击提供了有价值的见解。

Abstract: Targeted data poisoning attacks pose an increasingly serious threat due to
their ease of deployment and high success rates. These attacks aim to
manipulate the prediction for a single test sample in classification models.
Unlike indiscriminate attacks that aim to decrease overall test performance,
targeted attacks present a unique threat to individual test instances. This
threat model raises a fundamental question: what factors make certain test
samples more susceptible to successful poisoning than others? We investigate
how attack difficulty varies across different test instances and identify key
characteristics that influence vulnerability. This paper introduces three
predictive criteria for targeted data poisoning difficulty: ergodic prediction
accuracy (analyzed through clean training dynamics), poison distance, and
poison budget. Our experimental results demonstrate that these metrics
effectively predict the varying difficulty of real-world targeted poisoning
attacks across diverse scenarios, offering practitioners valuable insights for
vulnerability assessment and understanding data poisoning attacks.

</details>


### [133] [Data-Driven Stochastic Modeling Using Autoregressive Sequence Models: Translating Event Tables to Queueing Dynamics](https://arxiv.org/abs/2509.05839)
*Daksh Mittal,Shunri Zheng,Jing Dong,Hongseok Namkoong*

Main category: cs.LG

TL;DR: 提出基于自回归序列模型的数据驱动排队网络建模与仿真框架，以提高建模可扩展性和易用性，并在不同排队网络事件表上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统排队网络模型构建需大量人力和专业知识，为使建模方法更具可扩展性和易访问性。

Method: 基于事件流数据训练自回归序列模型，学习事件类型和时间的条件分布，用Transformer架构参数化分布。

Result: 该框架可自动构建高保真模拟器，在不同排队网络事件表上验证了其在仿真、不确定性量化和反事实评估方面的实用性。

Conclusion: 利用人工智能进展和数据可用性，该框架迈向更自动化、数据驱动的建模流程，有助于排队网络模型在服务领域的广泛应用。

Abstract: While queueing network models are powerful tools for analyzing service
systems, they traditionally require substantial human effort and domain
expertise to construct. To make this modeling approach more scalable and
accessible, we propose a data-driven framework for queueing network modeling
and simulation based on autoregressive sequence models trained on event-stream
data. Instead of explicitly specifying arrival processes, service mechanisms,
or routing logic, our approach learns the conditional distributions of event
types and event times, recasting the modeling task as a problem of sequence
distribution learning. We show that Transformer-style architectures can
effectively parameterize these distributions, enabling automated construction
of high-fidelity simulators. As a proof of concept, we validate our framework
on event tables generated from diverse queueing networks, showcasing its
utility in simulation, uncertainty quantification, and counterfactual
evaluation. Leveraging advances in artificial intelligence and the growing
availability of data, our framework takes a step toward more automated,
data-driven modeling pipelines to support broader adoption of queueing network
models across service domains.

</details>


### [134] [SPINN: An Optimal Self-Supervised Physics-Informed Neural Network Framework](https://arxiv.org/abs/2509.05886)
*Reza Pirayeshshirazinezhad*

Main category: cs.LG

TL;DR: 开发代理模型预测矩形微型散热器中液态钠对流换热系数，用多种机器学习方法提升性能，结果显示误差在一定范围，机器学习模型可用于液态金属冷却微型散热器设计优化。


<details>
  <summary>Details</summary>
Motivation: 高保真计算流体动力学模拟液态金属湍流强制对流耗时且计算成本高，需要替代工具。

Method: 先对含87个努塞尔数的数据集应用基于核的机器学习技术和浅层神经网络，再用自监督物理信息神经网络和迁移学习提升性能。

Result: 自监督物理信息神经网络能以约±8%的误差估计钠的传热率，仅用物理回归误差在5% - 10%，其他机器学习方法预测大多在±8%内。

Conclusion: 基于机器学习的模型是液态金属冷却微型散热器设计和优化的有力替代工具。

Abstract: A surrogate model is developed to predict the convective heat transfer
coefficient of liquid sodium (Na) flow within rectangular miniature heat sinks.
Initially, kernel-based machine learning techniques and shallow neural network
are applied to a dataset with 87 Nusselt numbers for liquid sodium in
rectangular miniature heat sinks. Subsequently, a self-supervised
physics-informed neural network and transfer learning approach are used to
increase the estimation performance. In the self-supervised physics-informed
neural network, an additional layer determines the weight the of physics in the
loss function to balance data and physics based on their uncertainty for a
better estimation. For transfer learning, a shallow neural network trained on
water is adapted for use with Na. Validation results show that the
self-supervised physics-informed neural network successfully estimate the heat
transfer rates of Na with an error margin of approximately +8%. Using only
physics for regression, the error remains between 5% to 10%. Other machine
learning methods specify the prediction mostly within +8%. High-fidelity
modeling of turbulent forced convection of liquid metals using computational
fluid dynamics (CFD) is both time-consuming and computationally expensive.
Therefore, machine learning based models offer a powerful alternative tool for
the design and optimization of liquid-metal-cooled miniature heat sinks.

</details>


### [135] [Unified Interaction Foundational Model (UIFM) for Predicting Complex User and System Behavior](https://arxiv.org/abs/2509.06025)
*Vignesh Ethiraj,Subhash Talluri*

Main category: cs.LG

TL;DR: 现有自然语言基础模型难以理解结构化交互，本文提出UIFM模型，采用复合标记化原理，能更好理解行为并迈向更智能预测系统。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言基础模型无法掌握电信、电商和金融等领域结构化交互的整体性质，序列化事件会丢失关键上下文。

Method: 引入统一交互基础模型（UIFM），采用复合标记化原理，将多属性事件视为单个语义连贯单元。

Result: 该架构不仅更准确。

Conclusion: UIFM是迈向创建更具适应性和智能预测系统的重要一步。

Abstract: A central goal of artificial intelligence is to build systems that can
understand and predict complex, evolving sequences of events. However, current
foundation models, designed for natural language, fail to grasp the holistic
nature of structured interactions found in domains like telecommunications,
e-commerce and finance. By serializing events into text, they disassemble them
into semantically fragmented parts, losing critical context. In this work, we
introduce the Unified Interaction Foundation Model (UIFM), a foundation model
engineered for genuine behavioral understanding. At its core is the principle
of composite tokenization, where each multi-attribute event is treated as a
single, semantically coherent unit. This allows UIFM to learn the underlying
"grammar" of user behavior, perceiving entire interactions rather than a
disconnected stream of data points. We demonstrate that this architecture is
not just more accurate, but represents a fundamental step towards creating more
adaptable and intelligent predictive systems.

</details>


### [136] [PolicyEvolve: Evolving Programmatic Policies by LLMs for multi-player games via Population-Based Training](https://arxiv.org/abs/2509.06053)
*Mingrui Lv,Hangzhi Liu,Zhi Luo,Hongjie Zhang,Jie Ou*

Main category: cs.LG

TL;DR: 提出PolicyEvolve框架用于多人游戏生成编程策略，减少手动编写代码依赖，通过迭代训练提升策略性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习训练对抗策略需大量样本和计算资源且缺乏可解释性，受大语言模型生成单智能体任务编程策略启发。

Method: PolicyEvolve框架包含Global Pool、Local Pool、Policy Planner和Trajectory Critic四个模块，通过迭代训练，利用各模块功能不断优化策略。

Result: 能显著减少对人工编写策略代码的依赖，用最少的环境交互实现高性能策略。

Conclusion: PolicyEvolve框架可用于多人游戏生成有效且可解释的编程策略。

Abstract: Multi-agent reinforcement learning (MARL) has achieved significant progress
in solving complex multi-player games through self-play. However, training
effective adversarial policies requires millions of experience samples and
substantial computational resources. Moreover, these policies lack
interpretability, hindering their practical deployment. Recently, researchers
have successfully leveraged Large Language Models (LLMs) to generate
programmatic policies for single-agent tasks, transforming neural network-based
policies into interpretable rule-based code with high execution efficiency.
Inspired by this, we propose PolicyEvolve, a general framework for generating
programmatic policies in multi-player games. PolicyEvolve significantly reduces
reliance on manually crafted policy code, achieving high-performance policies
with minimal environmental interactions. The framework comprises four modules:
Global Pool, Local Pool, Policy Planner, and Trajectory Critic. The Global Pool
preserves elite policies accumulated during iterative training. The Local Pool
stores temporary policies for the current iteration; only sufficiently
high-performing policies from this pool are promoted to the Global Pool. The
Policy Planner serves as the core policy generation module. It samples the top
three policies from the Global Pool, generates an initial policy for the
current iteration based on environmental information, and refines this policy
using feedback from the Trajectory Critic. Refined policies are then deposited
into the Local Pool. This iterative process continues until the policy achieves
a sufficiently high average win rate against the Global Pool, at which point it
is integrated into the Global Pool. The Trajectory Critic analyzes interaction
data from the current policy, identifies vulnerabilities, and proposes
directional improvements to guide the Policy Planner

</details>


### [137] [A novel biomass fluidized bed gasification model coupled with machine learning and CFD simulation](https://arxiv.org/abs/2509.06056)
*Chun Wang*

Main category: cs.LG

TL;DR: 提出基于机器学习和计算流体力学的生物质流化床气化耦合模型，以提高预测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 提高复杂热化学反应过程的预测精度和计算效率。

Method: 基于实验数据和高保真模拟结果构建高质量数据集，训练用于描述反应动力学特征的代理模型并嵌入CFD框架。

Result: 未提及。

Conclusion: 未提及。

Abstract: A coupling model of biomass fluidized bed gasification based on machine
learning and computational fluid dynamics is proposed to improve the prediction
accuracy and computational efficiency of complex thermochemical reaction
process. By constructing a high-quality data set based on experimental data and
high fidelity simulation results, the agent model used to describe the
characteristics of reaction kinetics was trained and embedded into the
computational fluid dynamics (CFD) framework to realize the real-time update of
reaction rate and composition evolution.

</details>


### [138] [ARIES: Relation Assessment and Model Recommendation for Deep Time Series Forecasting](https://arxiv.org/abs/2509.06060)
*Fei Wang,Yujie Li,Zezhi Shao,Chengqing Yu,Yisong Fu,Zhulin An,Yongjun Xu,Xueqi Cheng*

Main category: cs.LG

TL;DR: 现有时间序列预测基准数据集缺乏多样时间模式，无有效模型推荐方法。提出 ARIES 框架评估时间序列属性与建模策略关系，推荐预测模型，实验有明确关联，实现模型推荐系统。


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集无法系统评估模型性能与数据属性的联系，且无有效模型推荐方法，导致测试成本高。

Method: 构建含多种模式的合成数据集，设计系统计算时间序列属性，对超 50 个预测模型进行基准测试，建立属性与建模策略关系。

Result: 实验结果显示时间序列属性和建模策略有明确关联。

Conclusion: ARIES 是首个建立时间序列数据属性与建模策略关系并实现模型推荐系统的研究。

Abstract: Recent advancements in deep learning models for time series forecasting have
been significant. These models often leverage fundamental time series
properties such as seasonality and non-stationarity, which may suggest an
intrinsic link between model performance and data properties. However, existing
benchmark datasets fail to offer diverse and well-defined temporal patterns,
restricting the systematic evaluation of such connections. Additionally, there
is no effective model recommendation approach, leading to high time and cost
expenditures when testing different architectures across different downstream
applications. For those reasons, we propose ARIES, a framework for assessing
relation between time series properties and modeling strategies, and for
recommending deep forcasting models for realistic time series. First, we
construct a synthetic dataset with multiple distinct patterns, and design a
comprehensive system to compute the properties of time series. Next, we conduct
an extensive benchmarking of over 50 forecasting models, and establish the
relationship between time series properties and modeling strategies. Our
experimental results reveal a clear correlation. Based on these findings, we
propose the first deep forecasting model recommender, capable of providing
interpretable suggestions for real-world time series. In summary, ARIES is the
first study to establish the relations between the properties of time series
data and modeling strategies, while also implementing a model recommendation
system. The code is available at: https://github.com/blisky-li/ARIES.

</details>


### [139] [A Surrogate model for High Temperature Superconducting Magnets to Predict Current Distribution with Neural Network](https://arxiv.org/abs/2509.06067)
*Mianjun Xiao,Peng Song,Yulong Liu,Cedric Korte,Ziyang Xu,Jiale Gao,Jiaqi Lu,Haoyang Nie,Qiantong Deng,Timing Qu*

Main category: cs.LG

TL;DR: 开发基于FCRN的替代模型预测REBCO螺线管时空电流密度分布，比FEM更快且准确，为大规模高温超导磁体快速分析提供工具。


<details>
  <summary>Details</summary>
Motivation: 有限元法计算大规模高温超导磁体成本高、耗时长，限制了大规模REBCO磁体系统的快速设计。

Method: 开发基于全连接残差神经网络（FCRN）的替代模型，用不同匝数和饼数的有限元模拟生成训练数据集。

Result: FCRN架构比传统全连接网络收敛性更好，特定配置平衡了训练精度和泛化能力；模型能可靠预测超出训练范围50%的磁化损耗，最大误差低于10%；比FEM快几个数量级，含训练成本仍有优势。

Conclusion: 提出的基于FCRN的替代模型兼具准确性和效率，为大规模高温超导磁体快速分析提供了有前景的工具。

Abstract: Finite element method (FEM) is widely used in high-temperature
superconducting (HTS) magnets, but its computational cost increases with magnet
size and becomes time-consuming for meter-scale magnets, especially when
multi-physics couplings are considered, which limits the fast design of
large-scale REBCO magnet systems. In this work, a surrogate model based on a
fully connected residual neural network (FCRN) is developed to predict the
space-time current density distribution in REBCO solenoids. Training datasets
were generated from FEM simulations with varying numbers of turns and pancakes.
The results demonstrate that, for deeper networks, the FCRN architecture
achieves better convergence than conventional fully connected network (FCN),
with the configuration of 12 residual blocks and 256 neurons per layer
providing the most favorable balance between training accuracy and
generalization capability. Extrapolation studies show that the model can
reliably predict magnetization losses for up to 50% beyond the training range,
with maximum errors below 10%. The surrogate model achieves predictions several
orders of magnitude faster than FEM and still remains advantageous when
training costs are included. These results indicate that the proposed
FCRN-based surrogate model provides both accuracy and efficiency, offering a
promising tool for the rapid analysis of large-scale HTS magnets.

</details>


### [140] [Teaching Precommitted Agents: Model-Free Policy Evaluation and Control in Quasi-Hyperbolic Discounted MDPs](https://arxiv.org/abs/2509.06094)
*S. R. Eshwar*

Main category: cs.LG

TL;DR: 本文解决了具有拟双曲（QH）偏好的预承诺代理在强化学习（RL）框架中的关键理论和算法差距，提出两项主要贡献。


<details>
  <summary>Details</summary>
Motivation: 时间不一致偏好是人类和动物决策的关键特征，QH贴现模型简单强大，但在RL框架中的整合有限，需解决相关理论和算法差距。

Method: 正式刻画最优策略的结构；设计适用于该场景的无模型算法用于策略评估和Q学习，并给出收敛性证明。

Result: 首次证明最优策略可简化为简单的单步非平稳形式；设计出有收敛保证的实用无模型算法。

Conclusion: 研究结果为在RL中纳入QH偏好提供了基础见解。

Abstract: Time-inconsistent preferences, where agents favor smaller-sooner over
larger-later rewards, are a key feature of human and animal decision-making.
Quasi-Hyperbolic (QH) discounting provides a simple yet powerful model for this
behavior, but its integration into the reinforcement learning (RL) framework
has been limited. This paper addresses key theoretical and algorithmic gaps for
precommitted agents with QH preferences. We make two primary contributions: (i)
we formally characterize the structure of the optimal policy, proving for the
first time that it reduces to a simple one-step non-stationary form; and (ii)
we design the first practical, model-free algorithms for both policy evaluation
and Q-learning in this setting, both with provable convergence guarantees. Our
results provide foundational insights for incorporating QH preferences in RL.

</details>


### [141] [Tracking daily paths in home contexts with RSSI fingerprinting based on UWB through deep learning models](https://arxiv.org/abs/2509.06161)
*Aurora Polo-Rodríguez,Juan Carlos Valera,Jesús Peral,David Gil,Javier Medina-Quero*

Main category: cs.LG

TL;DR: 本文研究用超宽带技术结合深度学习模型跟踪家庭环境中居民路径，提出基于指纹的方法，对比多种模型和蓝牙技术，结果显示混合模型优势明显。


<details>
  <summary>Details</summary>
Motivation: 超宽带技术在实际环境中受墙体和障碍物影响精度降低，需更好方法用于家庭居民路径跟踪及人类活动识别。

Method: 提出基于指纹的方法，使用接收信号强度指示（RSSI）数据，对比CNN、LSTM、混合CNN + LSTM模型和蓝牙技术，评估不同类型和时长的时间窗口的影响。

Result: 平均绝对误差接近50厘米，混合模型在提供准确位置估计方面表现优越。

Conclusion: 混合模型适用于住宅环境中的日常人类活动识别。

Abstract: The field of human activity recognition has evolved significantly, driven
largely by advancements in Internet of Things (IoT) device technology,
particularly in personal devices. This study investigates the use of
ultra-wideband (UWB) technology for tracking inhabitant paths in home
environments using deep learning models. UWB technology estimates user
locations via time-of-flight and time-difference-of-arrival methods, which are
significantly affected by the presence of walls and obstacles in real
environments, reducing their precision. To address these challenges, we propose
a fingerprinting-based approach utilizing received signal strength indicator
(RSSI) data collected from inhabitants in two flats (60 m2 and 100 m2) while
performing daily activities. We compare the performance of convolutional neural
network (CNN), long short-term memory (LSTM), and hybrid CNN+LSTM models, as
well as the use of Bluetooth technology. Additionally, we evaluate the impact
of the type and duration of the temporal window (future, past, or a combination
of both). Our results demonstrate a mean absolute error close to 50 cm,
highlighting the superiority of the hybrid model in providing accurate location
estimates, thus facilitating its application in daily human activity
recognition in residential settings.

</details>


### [142] [An Improved Template for Approximate Computing](https://arxiv.org/abs/2509.06162)
*M. Rezaalipour,F. Costa,M. Biasion,R. Otoni,G. A. Constantinides,L. Pozzi*

Main category: cs.LG

TL;DR: 提出一种通过小精度损失减少神经网络中小算术运算符面积的方法，比现有技术在相同精度损失下节省更多面积。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署神经网络需平衡推理能耗和分类精度，近似计算可解决此权衡问题，旨在减少小算术运算符面积。

Method: 改进布尔重写技术XPAT，提出基于可参数化乘积共享的新模板。

Result: 实验表明该方法能更好收敛到低面积解决方案，比原始XPAT和其他两种现有方法找到更好近似。

Conclusion: 所提方法在减少神经网络小算术运算符面积上优于现有技术。

Abstract: Deploying neural networks on edge devices entails a careful balance between
the energy required for inference and the accuracy of the resulting
classification. One technique for navigating this tradeoff is approximate
computing: the process of reducing energy consumption by slightly reducing the
accuracy of arithmetic operators. In this context, we propose a methodology to
reduce the area of the small arithmetic operators used in neural networks -
i.e., adders and multipliers - via a small loss in accuracy, and show that we
improve area savings for the same accuracy loss w.r.t. the state of the art. To
achieve our goal, we improve on a boolean rewriting technique recently
proposed, called XPAT, where the use of a parametrisable template to rewrite
circuits has proved to be highly beneficial. In particular, XPAT was able to
produce smaller circuits than comparable approaches while utilising a naive sum
of products template structure. In this work, we show that template parameters
can act as proxies for chosen metrics and we propose a novel template based on
parametrisable product sharing that acts as a close proxy to synthesised area.
We demonstrate experimentally that our methodology converges better to low-area
solutions and that it can find better approximations than both the original
XPAT and two other state-of-the-art approaches.

</details>


### [143] [Exploring Urban Factors with Autoencoders: Relationship Between Static and Dynamic Features](https://arxiv.org/abs/2509.06167)
*Ximena Pocco,Waqar Hassan,Karelia Salinas,Vladimir Molchanov,Luis G. Nonato*

Main category: cs.LG

TL;DR: 本文开发可视化辅助框架分析融合和单独的潜在数据表示在挖掘城市数据模式上的效果，发现融合表示能产生更结构化模式，单独表示在特定情况有用。


<details>
  <summary>Details</summary>
Motivation: 现有可视化辅助工具很少在集成可视化框架中探索融合数据是否比单独分析数据源能提供更深入见解，为解决此问题开展研究。

Method: 开发可视化辅助框架，对动态和静态城市数据进行分析。

Result: 融合的潜在表示产生更结构化的模式，单独的表示在特定情况下有用。

Conclusion: 融合的潜在数据表示在挖掘城市数据模式上有一定优势，但单独表示也有其适用场景。

Abstract: Urban analytics utilizes extensive datasets with diverse urban information to
simulate, predict trends, and uncover complex patterns within cities. While
these data enables advanced analysis, it also presents challenges due to its
granularity, heterogeneity, and multimodality. To address these challenges,
visual analytics tools have been developed to support the exploration of latent
representations of fused heterogeneous and multimodal data, discretized at a
street-level of detail. However, visualization-assisted tools seldom explore
the extent to which fused data can offer deeper insights than examining each
data source independently within an integrated visualization framework. In this
work, we developed a visualization-assisted framework to analyze whether fused
latent data representations are more effective than separate representations in
uncovering patterns from dynamic and static urban data. The analysis reveals
that combined latent representations produce more structured patterns, while
separate ones are useful in particular cases.

</details>


### [144] [Reasoning Language Model for Personalized Lung Cancer Screening](https://arxiv.org/abs/2509.06169)
*Chuang Niu,Ge Wang*

Main category: cs.LG

TL;DR: 提出推理语言模型（RLM）用于个性化肺癌风险评估，在数据集上提升预测性能，利于临床应用。


<details>
  <summary>Details</summary>
Motivation: 现有的Lung - RADS在肺癌风险评估中仅基于肺结节特征，存在敏感性和特异性的权衡问题，需结合多种风险因素进行评估。

Method: 构建并蒸馏数据集、监督微调、强化学习和综合评估，通过思维链推理过程将风险评估任务分解为子组件。

Result: 在国家肺癌筛查试验数据集上显著提升风险预测性能，能分析不同风险因素贡献并合成最终风险评分。

Conclusion: 该方法提高了预测准确性和可监测性，便于在肺癌筛查中进行临床转化。

Abstract: Accurate risk assessment in lung cancer screening is critical for enabling
early cancer detection and minimizing unnecessary invasive procedures. The Lung
CT Screening Reporting and Data System (Lung-RADS) has been widely used as the
standard framework for patient management and follow-up. Nevertheless,
Lung-RADS faces trade-offs between sensitivity and specificity, as it
stratifies risk solely based on lung nodule characteristics without
incorporating various risk factors. Here we propose a reasoning language model
(RLM) to integrate radiology findings with longitudinal medical records for
individualized lung cancer risk assessment. Through a systematic study
including dataset construction and distillation, supervised fine-tuning,
reinforcement learning, and comprehensive evaluation, our model makes
significant improvements in risk prediction performance on datasets in the
national lung screening trial. Notably, RLM can decompose the risk evaluation
task into sub-components, analyze the contributions of diverse risk factors,
and synthesize them into a final risk score computed using our data-driven
system equation. Our approach improves both predictive accuracy and
monitorability through the chain of thought reasoning process, thereby
facilitating clinical translation into lung cancer screening.

</details>


### [145] [Metric Embedding Initialization-Based Differentially Private and Explainable Graph Clustering](https://arxiv.org/abs/2509.06214)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: 本文提出基于度量嵌入初始化的差分隐私可解释图聚类方法，实验表明该方法在保证隐私下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前图聚类在差分隐私框架下存在高噪声、低效率和可解释性差等问题，制约该领域发展。

Method: 构建SDP优化，提取关键集，用基于HST的初始化方法提供初始聚类配置，应用k - 中位数聚类策略得到聚类结果，并通过与聚类中心的差异对查询集进行对比解释。

Result: 在公共数据集上的大量实验表明，所提框架在各种聚类指标上优于现有方法。

Conclusion: 所提框架能在严格保证隐私的同时，在聚类性能上超越现有方法。

Abstract: Graph clustering under the framework of differential privacy, which aims to
process graph-structured data while protecting individual privacy, has been
receiving increasing attention. Despite significant achievements in current
research, challenges such as high noise, low efficiency and poor
interpretability continue to severely constrain the development of this field.
In this paper, we construct a differentially private and interpretable graph
clustering approach based on metric embedding initialization. Specifically, we
construct an SDP optimization, extract the key set and provide a
well-initialized clustering configuration using an HST-based initialization
method. Subsequently, we apply an established k-median clustering strategy to
derive the cluster results and offer comparative explanations for the query set
through differences from the cluster centers. Extensive experiments on public
datasets demonstrate that our proposed framework outperforms existing methods
in various clustering metrics while strictly ensuring privacy.

</details>


### [146] [MCIGLE: Multimodal Exemplar-Free Class-Incremental Graph Learning](https://arxiv.org/abs/2509.06219)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: 提出MCIGLE框架解决无样本类增量学习在多模态图结构数据中的问题，并经实验验证有效性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 多模态图结构数据下，现有无样本类增量学习方法存在灾难性遗忘、分布偏差、内存限制和泛化能力弱等问题。

Method: 提出MCIGLE框架，提取和对齐多模态图特征，应用串联递归最小二乘法保留知识，通过多通道处理平衡准确性和内存保存。

Result: 在公共数据集上的实验验证了MCIGLE框架的有效性和泛化性。

Conclusion: MCIGLE框架能有效解决多模态图结构数据下无样本类增量学习的相关问题。

Abstract: Exemplar-free class-incremental learning enables models to learn new classes
over time without storing data from old ones. As multimodal graph-structured
data becomes increasingly prevalent, existing methods struggle with challenges
like catastrophic forgetting, distribution bias, memory limits, and weak
generalization. We propose MCIGLE, a novel framework that addresses these
issues by extracting and aligning multimodal graph features and applying
Concatenated Recursive Least Squares for effective knowledge retention. Through
multi-channel processing, MCIGLE balances accuracy and memory preservation.
Experiments on public datasets validate its effectiveness and generalizability.

</details>


### [147] [UrbanMIMOMap: A Ray-Traced MIMO CSI Dataset with Precoding-Aware Maps and Benchmarks](https://arxiv.org/abs/2509.06270)
*Honggang Jia,Xiucheng Wang,Nan Cheng,Ruijin Sun,Changle Li*

Main category: cs.LG

TL;DR: 本文提出了一个名为UrbanMIMOMap的大规模城市MIMO CSI数据集，用于6G环境感知的无线电地图构建，还进行了基线性能评估并公开代码和数据。


<details>
  <summary>Details</summary>
Motivation: 6G系统需要环境感知通信，无线电地图构建需大量高质量数据，但现有公开数据集难以满足MIMO系统需求。

Method: 使用高精度射线追踪生成UrbanMIMOMap数据集。

Result: 得到了UrbanMIMOMap数据集，涵盖密集空间网格上的复杂CSI矩阵，并通过基线性能评估展示了其效用。

Conclusion: 为高精度无线电地图生成、MIMO空间性能和6G环境感知的机器学习研究提供了关键数据集和参考。

Abstract: Sixth generation (6G) systems require environment-aware communication, driven
by native artificial intelligence (AI) and integrated sensing and communication
(ISAC). Radio maps (RMs), providing spatially continuous channel information,
are key enablers. However, generating high-fidelity RM ground truth via
electromagnetic (EM) simulations is computationally intensive, motivating
machine learning (ML)-based RM construction. The effectiveness of these
data-driven methods depends on large-scale, high-quality training data. Current
public datasets often focus on single-input single-output (SISO) and limited
information, such as path loss, which is insufficient for advanced multi-input
multi-output (MIMO) systems requiring detailed channel state information (CSI).
To address this gap, this paper presents UrbanMIMOMap, a novel large-scale
urban MIMO CSI dataset generated using high-precision ray tracing. UrbanMIMOMap
offers comprehensive complex CSI matrices across a dense spatial grid, going
beyond traditional path loss data. This rich CSI is vital for constructing
high-fidelity RMs and serves as a fundamental resource for data-driven RM
generation, including deep learning. We demonstrate the dataset's utility
through baseline performance evaluations of representative ML methods for RM
construction. This work provides a crucial dataset and reference for research
in high-precision RM generation, MIMO spatial performance, and ML for 6G
environment awareness. The code and data for this work are available at:
https://github.com/UNIC-Lab/UrbanMIMOMap.

</details>


### [148] [IPR: Intelligent Prompt Routing with User-Controlled Quality-Cost Trade-offs](https://arxiv.org/abs/2509.06274)
*Aosong Feng,Zhichao Xu,Xian Wu,Kang Zhou,Sheng Guan,Yueyan Chen,Ninad Kulkarni,Yun Zhou,Balasubramaniam Srinivasan,Haibo Ding,Lin Lee Cheong*

Main category: cs.LG

TL;DR: 提出IPR智能提示路由框架，在工业级数据集上评估，部署后可降低成本并保持低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决大规模商业系统中路由查询到最具成本效益的大语言模型同时保持响应质量的挑战，优化性能 - 成本权衡。

Method: 提出模块化架构，含轻量级质量估计器；设计用户控制的路由机制；采用可扩展设计；构建工业级数据集IPRBench进行训练和评估。

Result: 在主要云平台部署后，实现43.9%的成本降低，保持与Claude家族最强模型质量相当，处理请求延迟低于150ms。

Conclusion: IPR框架能有效实现质量 - 成本的平衡，在实际应用中有良好表现。

Abstract: Routing incoming queries to the most cost-effective LLM while maintaining
response quality poses a fundamental challenge in optimizing performance-cost
trade-offs for large-scale commercial systems. We present IPR\, a
quality-constrained Intelligent Prompt Routing framework that dynamically
selects optimal models based on predicted response quality and user-specified
tolerance levels. IPR introduces three key innovations: (1) a modular
architecture with lightweight quality estimators trained on 1.5M prompts
annotated with calibrated quality scores, enabling fine-grained quality
prediction across model families; (2) a user-controlled routing mechanism with
tolerance parameter $\tau \in [0,1]$ that provides explicit control over
quality-cost trade-offs; and (3) an extensible design using frozen encoders
with model-specific adapters, reducing new model integration from days to
hours. To rigorously train and evaluate IPR, we curate an industrial-level
dataset IPRBench\footnote{IPRBench will be released upon legal approval.}, a
comprehensive benchmark containing 1.5 million examples with response quality
annotations across 11 LLM candidates. Deployed on a major cloud platform, IPR
achieves 43.9\% cost reduction while maintaining quality parity with the
strongest model in the Claude family and processes requests with sub-150ms
latency.

</details>


### [149] [RecMind: LLM-Enhanced Graph Neural Networks for Personalized Consumer Recommendations](https://arxiv.org/abs/2509.06286)
*Chang Xue,Youwei Lu,Chen Yang,Jinming Xing*

Main category: cs.LG

TL;DR: 提出RecMind，一种LLM增强的图推荐器，在Yelp和Amazon - Electronics上取得最佳结果，消融实验证实方法优势。


<details>
  <summary>Details</summary>
Motivation: 个性化在消费技术等领域面临稀疏交互、内容快速更替和异构文本信号等挑战。

Method: 将语言模型作为偏好先验，用轻量级适配器的冻结LLM生成文本条件的用户/项目嵌入，LightGCN骨干从用户 - 项目图学习协作嵌入，通过对称对比目标对齐两种视图并通过层内门控融合。

Result: 在Yelp和Amazon - Electronics上，RecMind在八个指标上取得最佳结果，相对强基线有显著提升，如Recall@40提升+4.53%，NDCG@40提升+4.01%。

Conclusion: 交叉视图对齐有必要，门控比后期融合和仅使用LLM的变体有优势。

Abstract: Personalization is a core capability across consumer technologies, streaming,
shopping, wearables, and voice, yet it remains challenged by sparse
interactions, fast content churn, and heterogeneous textual signals. We present
RecMind, an LLM-enhanced graph recommender that treats the language model as a
preference prior rather than a monolithic ranker. A frozen LLM equipped with
lightweight adapters produces text-conditioned user/item embeddings from
titles, attributes, and reviews; a LightGCN backbone learns collaborative
embeddings from the user-item graph. We align the two views with a symmetric
contrastive objective and fuse them via intra-layer gating, allowing language
to dominate in cold/long-tail regimes and graph structure to stabilize rankings
elsewhere. On Yelp and Amazon-Electronics, RecMind attains the best results on
all eight reported metrics, with relative improvements up to +4.53\%
(Recall@40) and +4.01\% (NDCG@40) over strong baselines. Ablations confirm both
the necessity of cross-view alignment and the advantage of gating over late
fusion and LLM-only variants.

</details>


### [150] [A Spatio-Temporal Graph Neural Networks Approach for Predicting Silent Data Corruption inducing Circuit-Level Faults](https://arxiv.org/abs/2509.06289)
*Shaoqi Wei,Senling Wang,Hiroshi Kai,Yoshinobu Higami,Ruijun Ma,Tianming Ni,Xiaoqing Wen,Hiroshi Takahashi*

Main category: cs.LG

TL;DR: 提出统一时空图卷积网络（ST - GCN）用于快速准确预测大型时序电路长周期故障影响概率，支持定量风险评估，在ISCAS - 89基准测试上效果良好，可用于测试策略优化。


<details>
  <summary>Details</summary>
Motivation: 时间零缺陷和老化导致的静默数据错误（SDEs）会降低安全关键系统性能，而功能测试检测SDE相关故障的模拟成本高，需要高效方法进行定量风险评估。

Method: 将门级网表建模为时空图以捕获拓扑和信号时序，使用专用空间和时间编码器高效预测多周期故障影响概率（FIPs），框架可接受来自可测试性指标或故障模拟的特征以进行效率 - 精度权衡。

Result: 在ISCAS - 89基准测试上，该方法将模拟时间减少10倍以上，同时保持高精度（5周期预测的平均绝对误差为0.024），通过预测FIP选择观测点可改善长周期难检测故障的检测。

Conclusion: 该方法可扩展到片上系统（SoC）级测试策略优化，适用于下游电子设计自动化流程。

Abstract: Silent Data Errors (SDEs) from time-zero defects and aging degrade
safety-critical systems. Functional testing detects SDE-related faults but is
expensive to simulate. We present a unified spatio-temporal graph convolutional
network (ST-GCN) for fast, accurate prediction of long-cycle fault impact
probabilities (FIPs) in large sequential circuits, supporting quantitative risk
assessment. Gate-level netlists are modeled as spatio-temporal graphs to
capture topology and signal timing; dedicated spatial and temporal encoders
predict multi-cycle FIPs efficiently. On ISCAS-89 benchmarks, the method
reduces simulation time by more than 10x while maintaining high accuracy (mean
absolute error 0.024 for 5-cycle predictions). The framework accepts features
from testability metrics or fault simulation, allowing efficiency-accuracy
trade-offs. A test-point selection study shows that choosing observation points
by predicted FIPs improves detection of long-cycle, hard-to-detect faults. The
approach scales to SoC-level test strategy optimization and fits downstream
electronic design automation flows.

</details>


### [151] [LoaQ: Layer-wise Output Approximation Quantization](https://arxiv.org/abs/2509.06297)
*Li Lin,Xiaojun Wan*

Main category: cs.LG

TL;DR: 提出输出近似方法LoaQ用于逐层训练后量化，实验证明其在多种量化场景有效，能提升量化质量。


<details>
  <summary>Details</summary>
Motivation: 现有逐层训练后量化方法存在近似不足、与全模型输出对齐不够的问题，需更好方法。

Method: 基于对主流大语言模型结构特征的理解，提出输出近似方法LoaQ，明确针对输出级一致性，有简单闭式解。

Result: 在LLaMA和Qwen模型家族实验表明，LoaQ在仅权重量化和权重-激活联合量化中均有效。

Conclusion: LoaQ能与现有量化策略无缝集成，提升整体量化质量，有推进训练后量化前沿的潜力。

Abstract: A natural and intuitive idea in model quantization is to approximate each
component's quantized output to match its original. Layer-wise post-training
quantization (PTQ), though based on this idea, adopts a strictly local view and
can achieve, at best, only activation-aware approximations of weights. As a
result, it often leads to insufficient approximations and practical deviations
from this guiding intuition. Recent work has achieved a more accurate
approximation of linear-layer outputs within the framework of layer-wise PTQ,
but such refinements remain inadequate for achieving alignment with the full
model output. Based on a deeper understanding of the structural characteristics
of mainstream LLMs, we propose $LoaQ$, an output-approximation method for
layer-wise PTQ that explicitly targets output-level consistency. It better
aligns with this intuition and can feature a simple closed-form solution,
making it orthogonal to existing techniques and readily integrable into
existing quantization pipelines. Experiments on the LLaMA and Qwen model
families demonstrate that LoaQ performs effectively in both weight-only and
weight-activation joint quantization. By integrating seamlessly with existing
quantization strategies, it further enhances overall quantization quality and
shows strong potential to advance the frontier of post-training quantization.

</details>


### [152] [WindFM: An Open-Source Foundation Model for Zero-Shot Wind Power Forecasting](https://arxiv.org/abs/2509.06311)
*Hang Fan,Yu Shi,Zongliang Fu,Shuo Chen,Wei Wei,Wei Xu,Jian Li*

Main category: cs.LG

TL;DR: 提出轻量级生成式基础模型WindFM用于概率风电预测，实验表明其零样本性能达SOTA，有强适应性、鲁棒性和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动范式存在难以泛化到其他地点或难以融入能源领域特定数据的问题，需要高质量风电预测模型。

Method: 采用离散生成框架，用时间序列分词器将连续多变量观测转换为离散分层令牌，用仅解码器的Transformer对令牌序列自回归预训练。

Result: 810万参数的WindFM模型在确定性和概率任务上零样本性能达SOTA，在不同大陆分布外数据上有强适应性。

Conclusion: WindFM模型有效，学习表示有鲁棒性和可迁移性，预训练模型已公开。

Abstract: High-quality wind power forecasting is crucial for the operation of modern
power grids. However, prevailing data-driven paradigms either train a
site-specific model which cannot generalize to other locations or rely on
fine-tuning of general-purpose time series foundation models which are
difficult to incorporate domain-specific data in the energy sector. This paper
introduces WindFM, a lightweight and generative Foundation Model designed
specifically for probabilistic wind power forecasting. WindFM employs a
discretize-and-generate framework. A specialized time-series tokenizer first
converts continuous multivariate observations into discrete, hierarchical
tokens. Subsequently, a decoder-only Transformer learns a universal
representation of wind generation dynamics by autoregressively pre-training on
these token sequences. Using the comprehensive WIND Toolkit dataset comprising
approximately 150 billion time steps from more than 126,000 sites, WindFM
develops a foundational understanding of the complex interplay between
atmospheric conditions and power output. Extensive experiments demonstrate that
our compact 8.1M parameter model achieves state-of-the-art zero-shot
performance on both deterministic and probabilistic tasks, outperforming
specialized models and larger foundation models without any fine-tuning. In
particular, WindFM exhibits strong adaptiveness under out-of-distribution data
from a different continent, demonstrating the robustness and transferability of
its learned representations. Our pre-trained model is publicly available at
https://github.com/shiyu-coder/WindFM.

</details>


### [153] [Evaluating the Efficiency of Latent Spaces via the Coupling-Matrix](https://arxiv.org/abs/2509.06314)
*Mehmet Can Yavuz,Berrin Yanikoglu*

Main category: cs.LG

TL;DR: 提出冗余指数rho(C)衡量表征质量，在多数据集验证，低rho(C)对应好性能，可指导架构搜索和正则化。


<details>
  <summary>Details</summary>
Motivation: 表示学习中构建有效潜在嵌入面临挑战，现有指标无法直接衡量潜在空间冗余。

Method: 引入冗余指数rho(C)，通过分析潜在表征耦合矩阵，用能量距离对比非对角统计量与正态分布。

Result: 低rho(C)可靠预测高分类准确率或低重构误差，TPE偏好探索低rho区域。

Conclusion: rho(C)为评估和提高表征效率提供理论视角和实用工具。

Abstract: A central challenge in representation learning is constructing latent
embeddings that are both expressive and efficient. In practice, deep networks
often produce redundant latent spaces where multiple coordinates encode
overlapping information, reducing effective capacity and hindering
generalization. Standard metrics such as accuracy or reconstruction loss
provide only indirect evidence of such redundancy and cannot isolate it as a
failure mode. We introduce a redundancy index, denoted rho(C), that directly
quantifies inter-dimensional dependencies by analyzing coupling matrices
derived from latent representations and comparing their off-diagonal statistics
against a normal distribution via energy distance. The result is a compact,
interpretable, and statistically grounded measure of representational quality.
We validate rho(C) across discriminative and generative settings on MNIST
variants, Fashion-MNIST, CIFAR-10, and CIFAR-100, spanning multiple
architectures and hyperparameter optimization strategies. Empirically, low
rho(C) reliably predicts high classification accuracy or low reconstruction
error, while elevated redundancy is associated with performance collapse.
Estimator reliability grows with latent dimension, yielding natural lower
bounds for reliable analysis. We further show that Tree-structured Parzen
Estimators (TPE) preferentially explore low-rho regions, suggesting that rho(C)
can guide neural architecture search and serve as a redundancy-aware
regularization target. By exposing redundancy as a universal bottleneck across
models and tasks, rho(C) offers both a theoretical lens and a practical tool
for evaluating and improving the efficiency of learned representations.

</details>


### [154] [Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics](https://arxiv.org/abs/2509.06322)
*Jiajun Bao,Nicolas Boullé,Toni J. B. Liu,Raphaël Sarfati,Christopher J. Earls*

Main category: cs.LG

TL;DR: 研究文本训练的基础模型对离散偏微分方程（PDE）解时空动态的外推能力，分析其预测准确性及内部处理机制。


<details>
  <summary>Details</summary>
Motivation: 探究文本训练的基础模型在不微调或自然语言提示下对PDE解时空动态的外推能力，理解大语言模型内部处理PDE解的机制。

Method: 分析模型预测准确性随时间上下文长度和空间离散化的变化，分析多步滚动预测中的误差积累，分析token级输出分布。

Result: 预测准确性随时间上下文长度增加而提高，在更精细空间离散化时下降；多步滚动预测中误差随时间范围代数增长；发现上下文学习（ICL）的进展过程。

Conclusion: 可将模型预测质量随上下文长度和输出长度的变化视为上下文神经缩放定律。

Abstract: Large language models (LLMs) have demonstrated emergent in-context learning
(ICL) capabilities across a range of tasks, including zero-shot time-series
forecasting. We show that text-trained foundation models can accurately
extrapolate spatiotemporal dynamics from discretized partial differential
equation (PDE) solutions without fine-tuning or natural language prompting.
Predictive accuracy improves with longer temporal contexts but degrades at
finer spatial discretizations. In multi-step rollouts, where the model
recursively predicts future spatial states over multiple time steps, errors
grow algebraically with the time horizon, reminiscent of global error
accumulation in classical finite-difference solvers. We interpret these trends
as in-context neural scaling laws, where prediction quality varies predictably
with both context length and output length. To better understand how LLMs are
able to internally process PDE solutions so as to accurately roll them out, we
analyze token-level output distributions and uncover a consistent ICL
progression: beginning with syntactic pattern imitation, transitioning through
an exploratory high-entropy phase, and culminating in confident, numerically
grounded predictions.

</details>


### [155] [Exploring approaches to computational representation and classification of user-generated meal logs](https://arxiv.org/abs/2509.06330)
*Guanlan Hu,Adit Anand,Pooja M. Desai,Iñigo Urteaga,Lena Mamykina*

Main category: cs.LG

TL;DR: 研究利用机器学习和领域特定增强处理患者生成的自由文本膳食日志，以对膳食与营养目标的契合度分类，发现机器学习表现优于自我评估，结合营养领域知识效果更佳。


<details>
  <summary>Details</summary>
Motivation: 探索利用机器学习和领域特定增强对患者生成的自由文本膳食日志进行分析，以对膳食与不同营养目标的契合度进行分类。

Method: 使用超3000条来自114人的膳食记录数据集，以注册营养师的判断为金标准，采用TFIDF、BERT等文本嵌入和领域特定增强信息作为输入，评估逻辑回归和多层感知器分类器性能。

Result: 即使无增强，机器学习也优于记录膳食者的自我评估，最佳增强组合的机器学习分类器准确率更高，不同营养目标下增强和分类算法对分类准确率影响有差异。

Conclusion: 机器学习能利用非结构化自由文本膳食日志可靠分类膳食与特定营养目标的契合度，结合营养领域知识效果更佳，有望支持精准医疗中以患者为中心的营养指导。

Abstract: This study examined the use of machine learning and domain specific
enrichment on patient generated health data, in the form of free text meal
logs, to classify meals on alignment with different nutritional goals. We used
a dataset of over 3000 meal records collected by 114 individuals from a
diverse, low income community in a major US city using a mobile app. Registered
dietitians provided expert judgement for meal to goal alignment, used as gold
standard for evaluation. Using text embeddings, including TFIDF and BERT, and
domain specific enrichment information, including ontologies, ingredient
parsers, and macronutrient contents as inputs, we evaluated the performance of
logistic regression and multilayer perceptron classifiers using accuracy,
precision, recall, and F1 score against the gold standard and self assessment.
Even without enrichment, ML outperformed self assessments of individuals who
logged meals, and the best performing combination of ML classifier with
enrichment achieved even higher accuracies. In general, ML classifiers with
enrichment of Parsed Ingredients, Food Entities, and Macronutrients information
performed well across multiple nutritional goals, but there was variability in
the impact of enrichment and classification algorithm on accuracy of
classification for different nutritional goals. In conclusion, ML can utilize
unstructured free text meal logs and reliably classify whether meals align with
specific nutritional goals, exceeding self assessments, especially when
incorporating nutrition domain knowledge. Our findings highlight the potential
of ML analysis of patient generated health data to support patient centered
nutrition guidance in precision healthcare.

</details>


### [156] [A Fragile Number Sense: Probing the Elemental Limits of Numerical Reasoning in LLMs](https://arxiv.org/abs/2509.06332)
*Roussel Rahman,Aashwin Ananda Mishra*

Main category: cs.LG

TL;DR: 本文评估大语言模型在不同复杂度数学问题上的表现，发现模型在确定性算法问题上准确率高，但在需要启发式搜索的数字谜题上失败，表明其数值推理更像模式匹配。


<details>
  <summary>Details</summary>
Motivation: 大语言模型数值推理的鲁棒性未知，标准基准评估常掩盖基础弱点，需进一步探究其数学计算能力。

Method: 对几个最先进的基于大语言模型的智能体进行100个问题的挑战测试，问题分为四类。

Result: 智能体在前三类需确定性算法执行的问题上准确率高，但在数字谜题上失败。

Conclusion: 智能体的能力主要限于回忆和执行已知算法，而非进行生成式问题解决，其数值推理更像模式匹配，限制了其在需要创新数值见解任务上的潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable emergent
capabilities, yet the robustness of their numerical reasoning remains an open
question. While standard benchmarks evaluate LLM reasoning on complex problem
sets using aggregated metrics, they often obscure foundational weaknesses. In
this work, we probe LLM mathematical numeracy by evaluating performance on
problems of escalating complexity, from constituent operations to combinatorial
puzzles. We test several state-of-the-art LLM-based agents on a 100-problem
challenge comprising four categories: (1) basic arithmetic, (2) advanced
operations, (3) primality checking, and (4) the Game of 24 number puzzle. Our
results show that while the agents achieved high accuracy on the first three
categories, which require deterministic algorithmic execution, they
consistently failed at the number puzzle, underlining its demand for a
heuristic search over a large combinatorial space to be a significant
bottleneck. These findings reveal that the agents' proficiency is largely
confined to recalling and executing known algorithms, rather than performing
generative problem-solving. This suggests their apparent numerical reasoning is
more akin to sophisticated pattern-matching than flexible, analytical thought,
limiting their potential for tasks that require novel or creative numerical
insights.

</details>


### [157] [Ban&Pick: Achieving Free Performance Gains and Inference Speedup via Smarter Routing in MoE-LLMs](https://arxiv.org/abs/2509.06346)
*Yuanteng Chen,Peisong Wang,Yuantian Shao,Jian Cheng*

Main category: cs.LG

TL;DR: 本文提出Ban&Pick策略优化稀疏混合专家（MoE）路由，无需重新训练或改变架构，能提升性能和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有MoE预训练中路由器优化存在问题，限制模型性能和效率，如部分专家未充分利用、固定激活专家数量有冗余。

Method: 提出Ban&Pick策略，Pick强化关键专家，Ban动态修剪冗余专家。

Result: 在多个基准测试中，Ban&Pick无需重新训练和架构更改，实现性能提升和推理加速。如Qwen3 - 30B - A3B在AIME2024和GPQA - Diamond上准确率提高，推理加速1.25倍。

Conclusion: Ban&Pick是一种有效的后训练、即插即用策略，能免费提升性能和加速推理。

Abstract: Sparse Mixture-of-Experts (MoE) has become a key architecture for scaling
large language models (LLMs) efficiently. Recent fine-grained MoE designs
introduce hundreds of experts per layer, with multiple experts activated per
token, enabling stronger specialization. However, during pre-training, routers
are optimized mainly for stability and robustness: they converge prematurely
and enforce balanced usage, limiting the full potential of model performance
and efficiency. In this work, we uncover two overlooked issues: (i) a few
highly influential experts are underutilized due to premature and balanced
routing decisions; and (ii) enforcing a fixed number of active experts per
token introduces substantial redundancy. Instead of retraining models or
redesigning MoE architectures, we introduce Ban&Pick, a post-training,
plug-and-play strategy for smarter MoE routing. Pick discovers and reinforces
key experts-a small group with outsized impact on performance-leading to
notable accuracy gains across domains. Ban complements this by dynamically
pruning redundant experts based on layer and token sensitivity, delivering
faster inference with minimal accuracy loss. Experiments on fine-grained
MoE-LLMs (DeepSeek, Qwen3) across math, code, and general reasoning benchmarks
demonstrate that Ban&Pick delivers free performance gains and inference
acceleration without retraining or architectural changes. For instance, on
Qwen3-30B-A3B, it improves accuracy from 80.67 to 84.66 on AIME2024 and from
65.66 to 68.18 on GPQA-Diamond, while accelerating inference by 1.25x under the
vLLM.

</details>


### [158] [Breaking SafetyCore: Exploring the Risks of On-Device AI Deployment](https://arxiv.org/abs/2509.06371)
*Victor Guyomard,Mathis Mauvisseau,Marie Paindavoine*

Main category: cs.LG

TL;DR: 文章通过SafetyCore案例研究，揭示设备端AI模型安全风险，展示其可被利用绕过检测。


<details>
  <summary>Details</summary>
Motivation: 设备端部署AI模型增多，带来有别于传统软件的安全风险，需进行研究。

Method: 以Android系统服务SafetyCore为案例，研究设备端AI模型。

Result: 展示设备端AI模型可被提取和操纵以绕过检测，使保护失效。

Conclusion: 分析暴露设备端AI模型漏洞，展示攻击者可利用这些漏洞。

Abstract: Due to hardware and software improvements, an increasing number of AI models
are deployed on-device. This shift enhances privacy and reduces latency, but
also introduces security risks distinct from traditional software. In this
article, we examine these risks through the real-world case study of
SafetyCore, an Android system service incorporating sensitive image content
detection. We demonstrate how the on-device AI model can be extracted and
manipulated to bypass detection, effectively rendering the protection
ineffective. Our analysis exposes vulnerabilities of on-device AI models and
provides a practical demonstration of how adversaries can exploit them.

</details>


### [159] [Variational Garrote for Statistical Physics-based Sparse and Robust Variable Selection](https://arxiv.org/abs/2509.06383)
*Hyungjoon Soh,Dongha Lee,Vipul Periwal,Junghyo Jo*

Main category: cs.LG

TL;DR: 本文重新审视基于统计物理的变分套索（VG）方法，结合自动微分技术改进它，在合成和真实数据集上评估，表明VG在高稀疏情况下表现好，还发现过渡点可估计相关变量数量，VG在多领域有潜力。


<details>
  <summary>Details</summary>
Motivation: 在大数据时代，从高维数据中选择关键变量很重要，稀疏回归可促进模型简单性和可解释性，而VG方法有价值但未充分利用。

Method: 重新审视VG方法，结合现代自动微分技术对其进行改进，在合成和真实数据集上进行评估。

Result: VG在高稀疏情况下表现出色，比岭回归和LASSO回归在不同稀疏水平下的变量选择更一致和稳健；发现过渡点，可用于估计相关变量数量。

Conclusion: VG在包括压缩感知和机器学习模型剪枝等广泛应用中具有强大的稀疏建模潜力。

Abstract: Selecting key variables from high-dimensional data is increasingly important
in the era of big data. Sparse regression serves as a powerful tool for this
purpose by promoting model simplicity and explainability. In this work, we
revisit a valuable yet underutilized method, the statistical physics-based
Variational Garrote (VG), which introduces explicit feature selection spin
variables and leverages variational inference to derive a tractable loss
function. We enhance VG by incorporating modern automatic differentiation
techniques, enabling scalable and efficient optimization. We evaluate VG on
both fully controllable synthetic datasets and complex real-world datasets. Our
results demonstrate that VG performs especially well in highly sparse regimes,
offering more consistent and robust variable selection than Ridge and LASSO
regression across varying levels of sparsity. We also uncover a sharp
transition: as superfluous variables are admitted, generalization degrades
abruptly and the uncertainty of the selection variables increases. This
transition point provides a practical signal for estimating the correct number
of relevant variables, an insight we successfully apply to identify key
predictors in real-world data. We expect that VG offers strong potential for
sparse modeling across a wide range of applications, including compressed
sensing and model pruning in machine learning.

</details>


### [160] [Beyond the Pre-Service Horizon: Infusing In-Service Behavior for Improved Financial Risk Forecasting](https://arxiv.org/abs/2509.06385)
*Senhao Liu,Zhiyu Guo,Zhiyuan Ji,Yueguo Chen,Yateng Tang,Yunhai Wang,Xuehao Zheng,Xiang Ao*

Main category: cs.LG

TL;DR: 提出多粒度知识蒸馏框架MGKD，结合服务中用户行为数据提升服务前风险预测，采用多粒度蒸馏和重加权策略，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 典型金融风险管理的服务前风险评估和服务中违约检测阶段常分开建模，为提升服务前风险预测。

Method: 提出MGKD框架，基于知识蒸馏，用服务中数据训练的教师模型指导服务前数据训练的学生模型，采用多粒度蒸馏策略，还采用重加权策略减轻模型对少数类的偏差。

Result: 在腾讯移动支付大规模真实数据集的离线和在线场景实验中证明了方法的有效性。

Conclusion: 所提MGKD框架能有效提升服务前风险评估的整体性能。

Abstract: Typical financial risk management involves distinct phases for pre-service
risk assessment and in-service default detection, often modeled separately.
This paper proposes a novel framework, Multi-Granularity Knowledge Distillation
(abbreviated as MGKD), aimed at improving pre-service risk prediction through
the integration of in-service user behavior data. MGKD follows the idea of
knowledge distillation, where the teacher model, trained on historical
in-service data, guides the student model, which is trained on pre-service
data. By using soft labels derived from in-service data, the teacher model
helps the student model improve its risk prediction prior to service
activation. Meanwhile, a multi-granularity distillation strategy is introduced,
including coarse-grained, fine-grained, and self-distillation, to align the
representations and predictions of the teacher and student models. This
approach not only reinforces the representation of default cases but also
enables the transfer of key behavioral patterns associated with defaulters from
the teacher to the student model, thereby improving the overall performance of
pre-service risk assessment. Moreover, we adopt a re-weighting strategy to
mitigate the model's bias towards the minority class. Experimental results on
large-scale real-world datasets from Tencent Mobile Payment demonstrate the
effectiveness of our proposed approach in both offline and online scenarios.

</details>


### [161] [Graph Neural Networks for Resource Allocation in Interference-limited Multi-Channel Wireless Networks with QoS Constraints](https://arxiv.org/abs/2509.06395)
*Lili Chen,Changyang She,Jingge Zhu,Jamie Evans*

Main category: cs.LG

TL;DR: 传统深度学习方法处理无线通信系统数据率约束无理论收敛保证且难满足QoS要求。本文提出eWMMSE和JCPGNN - M算法，JCPGNN - M性能与eWMMSE相当，在推理速度等方面有优势，为无线资源分配提供可扩展且有理论依据的方案。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法处理无线通信系统数据率约束无理论收敛保证，难满足QoS要求，需要新的解决方案。

Method: 先将WMMSE算法扩展到有QoS约束的多通道场景得到eWMMSE算法；开发GNN - based算法JCPGNN - M；提出整合GNN和拉格朗日原对偶优化方法的框架。

Result: JCPGNN - M性能与eWMMSE相当，在推理速度、对更大网络的泛化能力以及信道状态信息不完美时的鲁棒性方面有显著提升。

Conclusion: 本文为未来无线网络的受限资源分配提供了可扩展且有理论依据的解决方案。

Abstract: Meeting minimum data rate constraints is a significant challenge in wireless
communication systems, particularly as network complexity grows. Traditional
deep learning approaches often address these constraints by incorporating
penalty terms into the loss function and tuning hyperparameters empirically.
However, this heuristic treatment offers no theoretical convergence guarantees
and frequently fails to satisfy QoS requirements in practical scenarios.
Building upon the structure of the WMMSE algorithm, we first extend it to a
multi-channel setting with QoS constraints, resulting in the enhanced WMMSE
(eWMMSE) algorithm, which is provably convergent to a locally optimal solution
when the problem is feasible. To further reduce computational complexity and
improve scalability, we develop a GNN-based algorithm, JCPGNN-M, capable of
supporting simultaneous multi-channel allocation per user. To overcome the
limitations of traditional deep learning methods, we propose a principled
framework that integrates GNN with a Lagrangian-based primal-dual optimization
method. By training the GNN within the Lagrangian framework, we ensure
satisfaction of QoS constraints and convergence to a stationary point.
Extensive simulations demonstrate that JCPGNN-M matches the performance of
eWMMSE while offering significant gains in inference speed, generalization to
larger networks, and robustness under imperfect channel state information. This
work presents a scalable and theoretically grounded solution for constrained
resource allocation in future wireless networks.

</details>


### [162] [NeuroDeX: Unlocking Diverse Support in Decompiling Deep Neural Network Executables](https://arxiv.org/abs/2509.06402)
*Yilin Li,Guozhu Meng,Mingyang Sun,Yanzhong Wang,Kun Sun,Hailong Chang,Yuekang Li*

Main category: cs.LG

TL;DR: 本文提出NeuroDeX用于反编译DNN可执行文件，实验证明其有效且优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习编译器生成的可执行文件面临逆向工程威胁，先前研究处理编译优化和量化编译模型有挑战。

Method: 利用大语言模型语义理解能力和动态分析进行算子类型识别、算子属性恢复和模型重建。

Result: 在96个DNN可执行文件上实验，能将非量化可执行文件反编译为几乎相同的高级模型，量化可执行文件平均top - 1准确率72%。

Conclusion: NeuroDeX比先前的DNN可执行文件反编译器更全面有效。

Abstract: On-device deep learning models have extensive real world demands. Deep
learning compilers efficiently compile models into executables for deployment
on edge devices, but these executables may face the threat of reverse
engineering. Previous studies have attempted to decompile DNN executables, but
they face challenges in handling compilation optimizations and analyzing
quantized compiled models. In this paper, we present NeuroDeX to unlock diverse
support in decompiling DNN executables. NeuroDeX leverages the semantic
understanding capabilities of LLMs along with dynamic analysis to accurately
and efficiently perform operator type recognition, operator attribute recovery
and model reconstruction. NeuroDeX can recover DNN executables into high-level
models towards compilation optimizations, different architectures and quantized
compiled models. We conduct experiments on 96 DNN executables across 12 common
DNN models. Extensive experimental results demonstrate that NeuroDeX can
decompile non-quantized executables into nearly identical high-level models.
NeuroDeX can recover functionally similar high-level models for quantized
executables, achieving an average top-1 accuracy of 72%. NeuroDeX offers a more
comprehensive and effective solution compared to previous DNN executables
decompilers.

</details>


### [163] [CAPMix: Robust Time Series Anomaly Detection Based on Abnormal Assumptions with Dual-Space Mixup](https://arxiv.org/abs/2509.06419)
*Xudong Mou,Rui Wang,Tiejun Wang,Renyu Yang,Shiru Chen,Jie Sun,Tianyu Wo,Xudong Liu*

Main category: cs.LG

TL;DR: 提出可控异常增强框架CAPMix解决现有时间序列异常检测方法的问题，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有异常假设方法存在零散生成和异常偏移问题，需改进时间序列异常检测方法。

Method: 设计CutAddPaste机制避免零散生成，引入标签修正策略减少异常偏移，在时间卷积网络中使用双空间混合。

Result: 在五个基准数据集上实验，CAPMix显著优于现有基线，对污染训练数据有更强鲁棒性。

Conclusion: CAPMix能有效解决现有方法局限，提升时间序列异常检测性能。

Abstract: Time series anomaly detection (TSAD) is a vital yet challenging task,
particularly in scenarios where labeled anomalies are scarce and temporal
dependencies are complex. Recent anomaly assumption (AA) approaches alleviate
the lack of anomalies by injecting synthetic samples and training
discriminative models. Despite promising results, these methods often suffer
from two fundamental limitations: patchy generation, where scattered anomaly
knowledge leads to overly simplistic or incoherent anomaly injection, and
Anomaly Shift, where synthetic anomalies either resemble normal data too
closely or diverge unrealistically from real anomalies, thereby distorting
classification boundaries. In this paper, we propose CAPMix, a controllable
anomaly augmentation framework that addresses both issues. First, we design a
CutAddPaste mechanism to inject diverse and complex anomalies in a targeted
manner, avoiding patchy generation. Second, we introduce a label revision
strategy to adaptively refine anomaly labels, reducing the risk of anomaly
shift. Finally, we employ dual-space mixup within a temporal convolutional
network to enforce smoother and more robust decision boundaries. Extensive
experiments on five benchmark datasets, including AIOps, UCR, SWaT, WADI, and
ESA, demonstrate that CAPMix achieves significant improvements over
state-of-the-art baselines, with enhanced robustness against contaminated
training data. The code is available at https://github.com/alsike22/CAPMix.

</details>


### [164] [DyC-STG: Dynamic Causal Spatio-Temporal Graph Network for Real-time Data Credibility Analysis in IoT](https://arxiv.org/abs/2509.06483)
*Guanjie Cheng,Boyi Li,Peihan Wu,Feiyi Chen,Xinkui Zhao,Mengying Zhu,Shuiguang Deng*

Main category: cs.LG

TL;DR: 提出DyC - STG框架用于物联网实时数据可信度分析，发布两个新数据集，实验表现达最优。


<details>
  <summary>Details</summary>
Motivation: 物联网传感器产生大量时空数据流，确保数据可信度是挑战，现有STG模型在动态、以人为中心环境中有局限性。

Method: 提出DyC - STG框架，包含事件驱动动态图模块和因果推理模块，还发布两个新数据集。

Result: DyC - STG达到新的最优水平，比最强基线高1.4个百分点，F1分数达0.930。

Conclusion: DyC - STG框架在物联网实时数据可信度分析上表现出色，能有效解决现有STG模型的问题。

Abstract: The wide spreading of Internet of Things (IoT) sensors generates vast
spatio-temporal data streams, but ensuring data credibility is a critical yet
unsolved challenge for applications like smart homes. While spatio-temporal
graph (STG) models are a leading paradigm for such data, they often fall short
in dynamic, human-centric environments due to two fundamental limitations: (1)
their reliance on static graph topologies, which fail to capture physical,
event-driven dynamics, and (2) their tendency to confuse spurious correlations
with true causality, undermining robustness in human-centric environments. To
address these gaps, we propose the Dynamic Causal Spatio-Temporal Graph Network
(DyC-STG), a novel framework designed for real-time data credibility analysis
in IoT. Our framework features two synergistic contributions: an event-driven
dynamic graph module that adapts the graph topology in real-time to reflect
physical state changes, and a causal reasoning module to distill causally-aware
representations by strictly enforcing temporal precedence. To facilitate the
research in this domain we release two new real-world datasets. Comprehensive
experiments show that DyC-STG establishes a new state-of-the-art, outperforming
the strongest baselines by 1.4 percentage points and achieving an F1-Score of
up to 0.930.

</details>


### [165] [QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients](https://arxiv.org/abs/2509.06516)
*Zongheng Guo,Tao Chen,Manuela Ferrario*

Main category: cs.LG

TL;DR: 提出QualityFM多模态基础模型处理PPG和ECG信号质量问题，预训练后经迁移学习用于三个临床任务。


<details>
  <summary>Details</summary>
Motivation: PPG和ECG信号存在质量问题，现有方法泛化性、数据依赖和跨任务迁移性差。

Method: 提出QualityFM模型，采用双轨架构和自蒸馏策略，集成窗口稀疏注意力机制，使用复合损失函数，预训练不同参数数量模型。

Result: 预训练三个不同参数模型，经迁移学习在三个临床任务中展现效果和实用价值。

Conclusion: QualityFM模型能有效处理PPG和ECG信号质量问题，具有临床应用潜力。

Abstract: Photoplethysmogram (PPG) and electrocardiogram (ECG) are commonly recorded in
intesive care unit (ICU) and operating room (OR). However, the high incidence
of poor, incomplete, and inconsistent signal quality, can lead to false alarms
or diagnostic inaccuracies. The methods explored so far suffer from limited
generalizability, reliance on extensive labeled data, and poor cross-task
transferability. To overcome these challenges, we introduce QualityFM, a novel
multimodal foundation model for these physiological signals, designed to
acquire a general-purpose understanding of signal quality. Our model is
pre-trained on an large-scale dataset comprising over 21 million 30-second
waveforms and 179,757 hours of data. Our approach involves a dual-track
architecture that processes paired physiological signals of differing quality,
leveraging a self-distillation strategy where an encoder for high-quality
signals is used to guide the training of an encoder for low-quality signals. To
efficiently handle long sequential signals and capture essential local
quasi-periodic patterns, we integrate a windowed sparse attention mechanism
within our Transformer-based model. Furthermore, a composite loss function,
which combines direct distillation loss on encoder outputs with indirect
reconstruction loss based on power and phase spectra, ensures the preservation
of frequency-domain characteristics of the signals. We pre-train three models
with varying parameter counts (9.6 M to 319 M) and demonstrate their efficacy
and practical value through transfer learning on three distinct clinical tasks:
false alarm of ventricular tachycardia detection, the identification of atrial
fibrillation and the estimation of arterial blood pressure (ABP) from PPG and
ECG signals.

</details>


### [166] [Lane Change Intention Prediction of two distinct Populations using a Transformer](https://arxiv.org/abs/2509.06529)
*Francesco De Cristofaro,Cornelia Lex,Jia Hu,Arno Eichberger*

Main category: cs.LG

TL;DR: 研究测试车道变更意图预测的transformer在不同数据集上的表现，发现跨数据集测试准确率下降，同时在两个数据集训练准确率高。


<details>
  <summary>Details</summary>
Motivation: 现有车道变更意图预测算法大多在单一数据集训练，未测试在不同数据集和人群的有效性，为解决此问题开展研究。

Method: 在德国和中国香港收集的两个数据集上测试用于车道变更意图预测的transformer。

Result: 跨人群测试时transformer准确率降至39.43%，同时在两个数据集训练准确率达86.71%。

Conclusion: 车道变更意图预测算法在单一数据集训练可能在不同数据集和人群表现不佳，同时在多数据集训练可提高准确率。

Abstract: As a result of the growing importance of lane change intention prediction for
a safe and efficient driving experience in complex driving scenarios,
researchers have in recent years started to train novel machine learning
algorithms on available datasets with promising results. A shortcoming of this
recent research effort, though, is that the vast majority of the proposed
algorithms are trained on a single datasets. In doing so, researchers failed to
test if their algorithm would be as effective if tested on a different dataset
and, by extension, on a different population with respect to the one on which
they were trained. In this article we test a transformer designed for lane
change intention prediction on two datasets collected by LevelX in Germany and
Hong Kong. We found that the transformer's accuracy plummeted when tested on a
population different to the one it was trained on with accuracy values as low
as 39.43%, but that when trained on both populations simultaneously it could
achieve an accuracy as high as 86.71%. - This work has been submitted to the
IEEE for possible publication. Copyright may be transferred without notice,
after which this version may no longer be accessible.

</details>


### [167] [Learning Optimal Defender Strategies for CAGE-2 using a POMDP Model](https://arxiv.org/abs/2509.06539)
*Duc Huy Le,Rolf Stadler*

Main category: cs.LG

TL;DR: 本文为CAGE - 2构建POMDP形式化模型，提出基于PPO的BF - PPO方法学习最优防御策略，在CAGE - 2环境评估显示其优于CARDIFF。


<details>
  <summary>Details</summary>
Motivation: 为CAGE - 2场景定义最优防御策略并找到高效学习方法。

Method: 用POMDP构建CAGE - 2形式化模型，提出基于PPO的BF - PPO方法，用粒子滤波降低计算复杂度。

Result: 在CAGE - 2 CybORG环境评估，BF - PPO在学习的防御策略和训练时间上优于CARDIFF。

Conclusion: BF - PPO方法在CAGE - 2场景学习防御策略上表现更优。

Abstract: CAGE-2 is an accepted benchmark for learning and evaluating defender
strategies against cyberattacks. It reflects a scenario where a defender agent
protects an IT infrastructure against various attacks. Many defender methods
for CAGE-2 have been proposed in the literature. In this paper, we construct a
formal model for CAGE-2 using the framework of Partially Observable Markov
Decision Process (POMDP). Based on this model, we define an optimal defender
strategy for CAGE-2 and introduce a method to efficiently learn this strategy.
Our method, called BF-PPO, is based on PPO, and it uses particle filter to
mitigate the computational complexity due to the large state space of the
CAGE-2 model. We evaluate our method in the CAGE-2 CybORG environment and
compare its performance with that of CARDIFF, the highest ranked method on the
CAGE-2 leaderboard. We find that our method outperforms CARDIFF regarding the
learned defender strategy and the required training time.

</details>


### [168] [Predicting Fetal Outcomes from Cardiotocography Signals Using a Supervised Variational Autoencoder](https://arxiv.org/abs/2509.06540)
*John Tolladay,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: 开发并解释监督变分自编码器（VAE）模型对CTG信号分类，评估其性能和可解释性，虽未完全可解释但支持临床预测。


<details>
  <summary>Details</summary>
Motivation: 解决当前深度学习方法在基于妊娠结局对CTG信号分类时可解释性的局限。

Method: 用OxMat CTG数据集训练VAE，优化信号重建和结果预测，结合约束构建潜在空间，用AUROC、MSE评估性能，用多种方法评估可解释性。

Result: 模型在段和CTG级别有一定AUROC值，放松TC约束改善重建和分类，潜在分析显示部分特征编码情况。

Conclusion: 监督VAE可实现有竞争力的胎儿结局预测，FHR信号特性带来解纠缠挑战，模型支持临床预测并为未来模型提供基础。

Abstract: Objective: To develop and interpret a supervised variational autoencoder
(VAE) model for classifying cardiotocography (CTG) signals based on pregnancy
outcomes, addressing interpretability limits of current deep learning
approaches. Methods: The OxMat CTG dataset was used to train a VAE on
five-minute fetal heart rate (FHR) segments, labeled with postnatal outcomes.
The model was optimised for signal reconstruction and outcome prediction,
incorporating Kullback-Leibler divergence and total correlation (TC)
constraints to structure the latent space. Performance was evaluated using area
under the receiver operating characteristic curve (AUROC) and mean squared
error (MSE). Interpretability was assessed using coefficient of determination,
latent traversals and unsupervised component analyses. Results: The model
achieved an AUROC of 0.752 at the segment level and 0.779 at the CTG level,
where predicted scores were aggregated. Relaxing TC constraints improved both
reconstruction and classification. Latent analysis showed that baseline-related
features (e.g., FHR baseline, baseline shift) were well represented and aligned
with model scores, while metrics like short- and long-term variability were
less strongly encoded. Traversals revealed clear signal changes for baseline
features, while other properties were entangled or subtle. Unsupervised
decompositions corroborated these patterns. Findings: This work demonstrates
that supervised VAEs can achieve competitive fetal outcome prediction while
partially encoding clinically meaningful CTG features. The irregular,
multi-timescale nature of FHR signals poses challenges for disentangling
physiological components, distinguishing CTG from more periodic signals such as
ECG. Although full interpretability was not achieved, the model supports
clinically useful outcome prediction and provides a basis for future
interpretable, generative models.

</details>


### [169] [Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs](https://arxiv.org/abs/2509.06550)
*Jack Wilkie,Hanan Hindy,Christos Tachtatzis,Robert Atkinson*

Main category: cs.LG

TL;DR: 本文介绍了用于网络入侵检测的CLAN方法，它将增强样本作为负视图，提升了分类准确率和推理效率，在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有监督机器学习模型依赖大量标记数据，异常检测方法误报率高，而现有的自监督学习方法有改进空间，需新方法提升网络入侵检测性能。

Method: 提出Contrastive Learning using Augmented Negative pairs (CLAN)方法，将增强样本作为负视图，其他良性样本作为正视图。

Result: 在Lycos2017数据集的二分类任务中，该方法优于现有自监督和异常检测技术；在有限标记数据集上微调后，多分类性能也优于现有自监督模型。

Conclusion: CLAN方法在网络入侵检测中有效，能提升分类准确率和推理效率。

Abstract: Network intrusion detection remains a critical challenge in cybersecurity.
While supervised machine learning models achieve state-of-the-art performance,
their reliance on large labelled datasets makes them impractical for many
real-world applications. Anomaly detection methods, which train exclusively on
benign traffic to identify malicious activity, suffer from high false positive
rates, limiting their usability. Recently, self-supervised learning techniques
have demonstrated improved performance with lower false positive rates by
learning discriminative latent representations of benign traffic. In
particular, contrastive self-supervised models achieve this by minimizing the
distance between similar (positive) views of benign traffic while maximizing it
between dissimilar (negative) views. Existing approaches generate positive
views through data augmentation and treat other samples as negative. In
contrast, this work introduces Contrastive Learning using Augmented Negative
pairs (CLAN), a novel paradigm for network intrusion detection where augmented
samples are treated as negative views - representing potentially malicious
distributions - while other benign samples serve as positive views. This
approach enhances both classification accuracy and inference efficiency after
pretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset
demonstrates that the proposed method surpasses existing self-supervised and
anomaly detection techniques in a binary classification task. Furthermore, when
fine-tuned on a limited labelled dataset, the proposed approach achieves
superior multi-class classification performance compared to existing
self-supervised models.

</details>


### [170] [AI for Scientific Discovery is a Social Problem](https://arxiv.org/abs/2509.06580)
*Georgia Channing,Avijit Ghosh*

Main category: cs.LG

TL;DR: AI can accelerate scientific discovery, but benefits are unevenly distributed. The main barriers are social and institutional, and four challenges are highlighted. Addressing them requires non - technical efforts and reframing AI for science as a social project.


<details>
  <summary>Details</summary>
Motivation: To find out the reasons for the uneven distribution of AI's benefits in scientific discovery and propose solutions.

Method: Identifying social and institutional barriers, highlighting four challenges and analyzing their roots in cultural and organizational practices.

Result: Four interconnected challenges (community dysfunction, misaligned research priorities, data fragmentation, infrastructure inequities) are found, and their roots are in cultural and organizational practices.

Conclusion: Addressing the barriers requires technical innovation, community - building, cross - disciplinary education, etc., and AI for science should be reframed as a collective social project.

Abstract: Artificial intelligence promises to accelerate scientific discovery, yet its
benefits remain unevenly distributed. While technical obstacles such as scarce
data, fragmented standards, and unequal access to computation are significant,
we argue that the primary barriers are social and institutional. Narratives
that defer progress to speculative "AI scientists," the undervaluing of data
and infrastructure contributions, misaligned incentives, and gaps between
domain experts and machine learning researchers all constrain impact. We
highlight four interconnected challenges: community dysfunction, research
priorities misaligned with upstream needs, data fragmentation, and
infrastructure inequities. We argue that their roots lie in cultural and
organizational practices. Addressing them requires not only technical
innovation but also intentional community-building, cross-disciplinary
education, shared benchmarks, and accessible infrastructure. We call for
reframing AI for science as a collective social project, where sustainable
collaboration and equitable participation are treated as prerequisites for
technical progress.

</details>


### [171] [Information-Theoretic Bounds and Task-Centric Learning Complexity for Real-World Dynamic Nonlinear Systems](https://arxiv.org/abs/2509.06599)
*Sri Satish Krishna Chaitanya Bulusu,Mikko Sillanpää*

Main category: cs.LG

TL;DR: 本文提出结构化分解理论框架用于建模复杂动态非线性系统，阐述静态和动态失真不能同时最小化，给出复杂度指标，解释结构化残差学习优势。


<details>
  <summary>Details</summary>
Motivation: 动态非线性系统中静态和动态效应相互耦合产生失真，给数据驱动建模带来挑战，需理论框架解决。

Method: 运用结构化分解、方差分析和任务中心复杂度边界，引入方向下界、关键行为指标和记忆有限性指数，建立基于功率的条件。

Result: 提出行为不确定性原理，证明静态和动态失真不能同时最小化；给出联系函数方差、均方Lipschitz连续性和学习复杂度的定理，得到复杂度指标。

Conclusion: 该框架可解释结构化残差学习优势，适用于复杂动态非线性系统建模，是可扩展且有理论依据的方法。

Abstract: Dynamic nonlinear systems exhibit distortions arising from coupled static and
dynamic effects. Their intertwined nature poses major challenges for
data-driven modeling. This paper presents a theoretical framework grounded in
structured decomposition, variance analysis, and task-centric complexity
bounds.
  The framework employs a directional lower bound on interactions between
measurable system components, extending orthogonality in inner product spaces
to structurally asymmetric settings. This bound supports variance inequalities
for decomposed systems. Key behavioral indicators are introduced along with a
memory finiteness index. A rigorous power-based condition establishes a
measurable link between finite memory in realizable systems and the First Law
of Thermodynamics. This offers a more foundational perspective than classical
bounds based on the Second Law.
  Building on this foundation, we formulate a `Behavioral Uncertainty
Principle,' demonstrating that static and dynamic distortions cannot be
minimized simultaneously. We identify that real-world systems seem to resist
complete deterministic decomposition due to entangled static and dynamic
effects. We also present two general-purpose theorems linking function variance
to mean-squared Lipschitz continuity and learning complexity. This yields a
model-agnostic, task-aware complexity metric, showing that lower-variance
components are inherently easier to learn.
  These insights explain the empirical benefits of structured residual
learning, including improved generalization, reduced parameter count, and lower
training cost, as previously observed in power amplifier linearization
experiments. The framework is broadly applicable and offers a scalable,
theoretically grounded approach to modeling complex dynamic nonlinear systems.

</details>


### [172] [PAC-Bayesian Generalization Bounds for Graph Convolutional Networks on Inductive Node Classification](https://arxiv.org/abs/2509.06600)
*Huayi Tang,Yong Liu*

Main category: cs.LG

TL;DR: 本文对用于归纳式节点分类的图卷积网络进行PAC - 贝叶斯理论分析，推导了单层和两层GCN的泛化边界，为动态图环境下GNN泛化提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有基于直推学习框架的理论研究无法充分建模真实世界图的时间演化和结构动态，需要新的理论分析。

Method: 对用于归纳式节点分类的图卷积网络进行PAC - 贝叶斯理论分析，将节点视为相关且非独立同分布的数据点。

Result: 推导了单层GCN的泛化边界，明确纳入数据依赖性和非平稳性的影响；建立了泛化差距随节点数量增加收敛到零的充分条件；将分析扩展到两层GCN，发现其需要对图拓扑有更强假设来保证收敛。

Conclusion: 为理解和改进动态图环境下GNN的泛化能力奠定了理论基础。

Abstract: Graph neural networks (GNNs) have achieved remarkable success in processing
graph-structured data across various applications. A critical aspect of
real-world graphs is their dynamic nature, where new nodes are continually
added and existing connections may change over time. Previous theoretical
studies, largely based on the transductive learning framework, fail to
adequately model such temporal evolution and structural dynamics. In this
paper, we presents a PAC-Bayesian theoretical analysis of graph convolutional
networks (GCNs) for inductive node classification, treating nodes as dependent
and non-identically distributed data points. We derive novel generalization
bounds for one-layer GCNs that explicitly incorporate the effects of data
dependency and non-stationarity, and establish sufficient conditions under
which the generalization gap converges to zero as the number of nodes
increases. Furthermore, we extend our analysis to two-layer GCNs, and reveal
that it requires stronger assumptions on graph topology to guarantee
convergence. This work establishes a theoretical foundation for understanding
and improving GNN generalization in dynamic graph environments.

</details>


### [173] [Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in Molecular Tumor Boards](https://arxiv.org/abs/2509.06602)
*Noel Codella,Sam Preston,Hao Qiu,Leonardo Schettini,Wen-wai Yim,Mert Öz,Shrey Jain,Matthew P. Lungren,Thomas Osborne*

Main category: cs.LG

TL;DR: 本文介绍用于分子肿瘤委员会（MTBs）的大语言模型驱动智能体HAO生成患者总结，提出评估框架TBFact，结果显示模型效果好且TBFact可本地部署。


<details>
  <summary>Details</summary>
Motivation: 传统手动编译MTBs患者总结的方法劳动强度大、主观且易遗漏关键信息，评估预测总结存在挑战。

Method: 引入LLM驱动的AI智能体HAO协调多智能体临床工作流生成患者总结，提出“模型即评判者”框架TBFact评估总结。

Result: 患者病史智能体捕获94%高重要信息，严格蕴含标准下TBFact召回率达0.84，TBFact可本地部署。

Conclusion: HAO和TBFact为MTBs提供可靠且可扩展的支持。

Abstract: Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology
specialists collaboratively assess complex patient cases to determine optimal
treatment strategies. A central element of this process is the patient summary,
typically compiled by a medical oncologist, radiation oncologist, or surgeon,
or their trained medical assistant, who distills heterogeneous medical records
into a concise narrative to facilitate discussion. This manual approach is
often labor-intensive, subjective, and prone to omissions of critical
information. To address these limitations, we introduce the Healthcare Agent
Orchestrator (HAO), a Large Language Model (LLM)-driven AI agent that
coordinates a multi-agent clinical workflow to generate accurate and
comprehensive patient summaries for MTBs. Evaluating predicted patient
summaries against ground truth presents additional challenges due to stylistic
variation, ordering, synonym usage, and phrasing differences, which complicate
the measurement of both succinctness and completeness. To overcome these
evaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework
designed to assess the comprehensiveness and succinctness of generated
summaries. Using a benchmark dataset derived from de-identified tumor board
discussions, we applied TBFact to evaluate our Patient History agent. Results
show that the agent captured 94% of high-importance information (including
partial entailments) and achieved a TBFact recall of 0.84 under strict
entailment criteria. We further demonstrate that TBFact enables a data-free
evaluation framework that institutions can deploy locally without sharing
sensitive clinical data. Together, HAO and TBFact establish a robust foundation
for delivering reliable and scalable support to MTBs.

</details>


### [174] [Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors](https://arxiv.org/abs/2509.06608)
*Viacheslav Sinii,Nikita Balagansky,Yaroslav Aksenov,Vadim Kurochkin,Daniil Laptev,Gleb Gerasimov,Alexey Gorbatovski,Boris Shaposhnikov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 研究推理训练重塑语言模型计算的机制，用轻量级转向向量，分析得出不同层转向向量特点并建立解释框架。


<details>
  <summary>Details</summary>
Motivation: 当前推理训练重塑语言模型计算的机制理解不足，需深入研究。

Method: 在基础模型的残差流中插入轻量级转向向量并通过强化学习目标训练，利用logit - lens读数、路径修补和电路分析来研究两个模型。

Result: 最后一层转向向量像集中在首个生成令牌上的令牌替换偏差；倒数第二层转向向量主要通过MLP和反嵌入起作用，优先提升过程词和结构符号权重。

Conclusion: 建立了一个解释推理训练引起的行为变化的原则性框架。

Abstract: The mechanisms by which reasoning training reshapes language-model
computations remain poorly understood. We study lightweight steering vectors
inserted into the base model's residual stream and trained with a
reinforcement-learning objective, which can match full fine-tuning performance
while retaining the interpretability of small, additive interventions. Using
logit-lens readouts, path patching, and circuit analyses, we analyze two models
and find: (i) the last-layer steering vector behaves like a token-substitution
bias concentrated on the first generated token, consistently boosting tokens
such as "To" and "Step"; and (ii) the penultimate-layer steering vector leaves
attention patterns largely unchanged and instead acts through the MLP and
unembedding, preferentially up-weighting process words and structure symbols.
These results establish a principled framework for interpreting the behavioral
changes induced by reasoning training.

</details>


### [175] [A Survey of Generalization of Graph Anomaly Detection: From Transfer Learning to Foundation Models](https://arxiv.org/abs/2509.06609)
*Junjun Pan,Yu Zheng,Yue Tan,Yixin Liu*

Main category: cs.LG

TL;DR: 本文对图异常检测（GAD）中的泛化问题进行全面综述，追溯其发展、构建分类体系、回顾现有方法，指出挑战并给出未来方向。


<details>
  <summary>Details</summary>
Motivation: 多数GAD方法适应性有限，且缺乏对GAD泛化的系统理解，因此开展全面综述。

Method: 追溯GAD泛化的演变，形式化问题设置，构建细粒度分类体系，对现有广义GAD方法进行综述。

Result: 完成了对现有广义GAD方法的最新且全面的综述。

Conclusion: 识别出当前的开放挑战并提出未来研究方向，以启发该新兴领域的未来研究。

Abstract: Graph anomaly detection (GAD) has attracted increasing attention in recent
years for identifying malicious samples in a wide range of graph-based
applications, such as social media and e-commerce. However, most GAD methods
assume identical training and testing distributions and are tailored to
specific tasks, resulting in limited adaptability to real-world scenarios such
as shifting data distributions and scarce training samples in new applications.
To address the limitations, recent work has focused on improving the
generalization capability of GAD models through transfer learning that
leverages knowledge from related domains to enhance detection performance, or
developing "one-for-all" GAD foundation models that generalize across multiple
applications. Since a systematic understanding of generalization in GAD is
still lacking, in this paper, we provide a comprehensive review of
generalization in GAD. We first trace the evolution of generalization in GAD
and formalize the problem settings, which further leads to our systematic
taxonomy. Rooted in this fine-grained taxonomy, an up-to-date and comprehensive
review is conducted for the existing generalized GAD methods. Finally, we
identify current open challenges and suggest future directions to inspire
future research in this emerging field.

</details>


### [176] [BEAM: Brainwave Empathy Assessment Model for Early Childhood](https://arxiv.org/abs/2509.06620)
*Chen Xie,Gaofeng Wu,Kaidong Wang,Zihao Zhu,Xiaoshu Luo,Yan Liang,Feiyu Quan,Ruoxi Wu,Xianghui Huang,Han Zhang*

Main category: cs.LG

TL;DR: 本文提出脑电波共情评估模型BEAM预测4 - 6岁儿童共情水平，在CBCP数据集验证效果佳，对儿童亲社会发展早期干预有意义。


<details>
  <summary>Details</summary>
Motivation: 传统预测儿童共情方法有局限，易有偏差且无法客观捕捉共情形成过程，现有EEG方法忽略时间动态性。

Method: 提出BEAM框架，利用多视图EEG信号，含LaBraM编码器、特征融合模块和对比学习模块。

Result: 在CBCP数据集上，BEAM在多个指标上优于现有方法。

Conclusion: BEAM有客观评估共情的潜力，为儿童亲社会发展早期干预提供初步见解。

Abstract: Empathy in young children is crucial for their social and emotional
development, yet predicting it remains challenging. Traditional methods often
only rely on self-reports or observer-based labeling, which are susceptible to
bias and fail to objectively capture the process of empathy formation. EEG
offers an objective alternative; however, current approaches primarily extract
static patterns, neglecting temporal dynamics. To overcome these limitations,
we propose a novel deep learning framework, the Brainwave Empathy Assessment
Model (BEAM), to predict empathy levels in children aged 4-6 years. BEAM
leverages multi-view EEG signals to capture both cognitive and emotional
dimensions of empathy. The framework comprises three key components: 1) a
LaBraM-based encoder for effective spatio-temporal feature extraction, 2) a
feature fusion module to integrate complementary information from multi-view
signals, and 3) a contrastive learning module to enhance class separation.
Validated on the CBCP dataset, BEAM outperforms state-of-the-art methods across
multiple metrics, demonstrating its potential for objective empathy assessment
and providing a preliminary insight into early interventions in children's
prosocial development.

</details>


### [177] [Knowledge-Guided Machine Learning for Stabilizing Near-Shortest Path Routing](https://arxiv.org/abs/2509.06640)
*Yung-Fu Chen,Sen Lin,Anish Arora*

Main category: cs.LG

TL;DR: 提出仅需单个图少量样本的算法，用DNN解决全对近最短路径问题，学习到近似最优策略，有与贪心转发匹配策略及表现更优的新策略并展示新策略特性。


<details>
  <summary>Details</summary>
Motivation: 解决全对近最短路径问题，让图节点高效可扩展地路由数据包。

Method: 提出简单算法，利用网络领域知识选择输入特征和设计策略函数，训练深度神经网络。

Result: 训练的一个DNN学习到与贪心转发匹配的策略，还学习到新的GreedyTensile路由策略且表现更优。

Conclusion: GreedyTensile路由策略具有可解释性和超低延迟运行特性。

Abstract: We propose a simple algorithm that needs only a few data samples from a
single graph for learning local routing policies that generalize across a rich
class of geometric random graphs in Euclidean metric spaces. We thus solve the
all-pairs near-shortest path problem by training deep neural networks (DNNs)
that let each graph node efficiently and scalably route (i.e., forward) packets
by considering only the node's state and the state of the neighboring nodes.
Our algorithm design exploits network domain knowledge in the selection of
input features and design of the policy function for learning an approximately
optimal policy. Domain knowledge also provides theoretical assurance that the
choice of a ``seed graph'' and its node data sampling suffices for
generalizable learning. Remarkably, one of these DNNs we train -- using
distance-to-destination as the only input feature -- learns a policy that
exactly matches the well-known Greedy Forwarding policy, which forwards packets
to the neighbor with the shortest distance to the destination. We also learn a
new policy, which we call GreedyTensile routing -- using both
distance-to-destination and node stretch as the input features -- that almost
always outperforms greedy forwarding. We demonstrate the explainability and
ultra-low latency run-time operation of Greedy Tensile routing by symbolically
interpreting its DNN in low-complexity terms of two linear actions.

</details>


### [178] [Group Effect Enhanced Generative Adversarial Imitation Learning for Individual Travel Behavior Modeling under Incentives](https://arxiv.org/abs/2509.06656)
*Yuanyuan Wu,Zhenlin Qin,Leizhen Wang,Xiaolei Ma,Zhenliang Ma*

Main category: cs.LG

TL;DR: 提出gcGAIL模型解决MDP建模问题，实验表明其优于其他方法，能预测个体行为，为个性化激励提供基础。


<details>
  <summary>Details</summary>
Motivation: 传统MDP解决个体出行行为建模时数据要求高，面临数据量、时空覆盖和情景多样性挑战，需要更高效的模型。

Method: 提出组效应增强生成对抗模仿学习（gcGAIL）模型，利用乘客组间共享行为模式提高个体行为建模效率。

Result: 通过公交票价折扣案例验证，gcGAIL在学习个体出行行为响应的准确性、泛化性和模式展示效率上优于其他方法，且对空间变化、数据稀疏和行为多样性有鲁棒性。

Conclusion: gcGAIL模型可随时预测个体行为响应，为个性化激励诱导可持续行为改变提供依据。

Abstract: Understanding and modeling individual travel behavior responses is crucial
for urban mobility regulation and policy evaluation. The Markov decision
process (MDP) provides a structured framework for dynamic travel behavior
modeling at the individual level. However, solving an MDP in this context is
highly data-intensive and faces challenges of data quantity, spatial-temporal
coverage, and situational diversity. To address these, we propose a
group-effect-enhanced generative adversarial imitation learning (gcGAIL) model
that improves the individual behavior modeling efficiency by leveraging shared
behavioral patterns among passenger groups. We validate the gcGAIL model using
a public transport fare-discount case study and compare against
state-of-the-art benchmarks, including adversarial inverse reinforcement
learning (AIRL), baseline GAIL, and conditional GAIL. Experimental results
demonstrate that gcGAIL outperforms these methods in learning individual travel
behavior responses to incentives over time in terms of accuracy,
generalization, and pattern demonstration efficiency. Notably, gcGAIL is robust
to spatial variation, data sparsity, and behavioral diversity, maintaining
strong performance even with partial expert demonstrations and underrepresented
passenger groups. The gcGAIL model predicts the individual behavior response at
any time, providing the basis for personalized incentives to induce sustainable
behavior changes (better timing of incentive injections).

</details>


### [179] [TrajAware: Graph Cross-Attention and Trajectory-Aware for Generalisable VANETs under Partial Observations](https://arxiv.org/abs/2509.06665)
*Xiaolu Fu,Ziyuan Bao,Eiman Kanjo*

Main category: cs.LG

TL;DR: 提出适用于VANET边缘AI部署的RL框架TrajAware，经评估表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: VANET路由因动态拓扑等因素具有挑战性，现有RL方法假设固定图结构且不适合受限硬件，需新方法。

Method: TrajAware集成行动空间修剪、图交叉注意力和轨迹感知预测三个组件。

Result: 在SUMO模拟器中评估，TrajAware能实现近最短路径和高交付率，效率适合受限边缘设备，优于现有基线。

Conclusion: TrajAware在VANET边缘AI部署的路由方面表现良好，有实用价值。

Abstract: Vehicular ad hoc networks (VANETs) are a crucial component of intelligent
transportation systems; however, routing remains challenging due to dynamic
topologies, incomplete observations, and the limited resources of edge devices.
Existing reinforcement learning (RL) approaches often assume fixed graph
structures and require retraining when network conditions change, making them
unsuitable for deployment on constrained hardware. We present TrajAware, an
RL-based framework designed for edge AI deployment in VANETs. TrajAware
integrates three components: (i) action space pruning, which reduces redundant
neighbour options while preserving two-hop reachability, alleviating the curse
of dimensionality; (ii) graph cross-attention, which maps pruned neighbours to
the global graph context, producing features that generalise across diverse
network sizes; and (iii) trajectory-aware prediction, which uses historical
routes and junction information to estimate real-time positions under partial
observations. We evaluate TrajAware in the open-source SUMO simulator using
real-world city maps with a leave-one-city-out setup. Results show that
TrajAware achieves near-shortest paths and high delivery ratios while
maintaining efficiency suitable for constrained edge devices, outperforming
state-of-the-art baselines in both full and partial observation scenarios.

</details>


### [180] [Barycentric Neural Networks and Length-Weighted Persistent Entropy Loss: A Green Geometric and Topological Framework for Function Approximation](https://arxiv.org/abs/2509.06694)
*Victor Toscano-Duran,Rocio Gonzalez-Diaz,Miguel A. Gutiérrez-Naranjo*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While it is well-established that artificial neural networks are
\emph{universal approximators} for continuous functions on compact domains,
many modern approaches rely on deep or overparameterized architectures that
incur high computational costs. In this paper, a new type of \emph{small
shallow} neural network, called the \emph{Barycentric Neural Network} ($\BNN$),
is proposed, which leverages a fixed set of \emph{base points} and their
\emph{barycentric coordinates} to define both its structure and its parameters.
We demonstrate that our $\BNN$ enables the exact representation of
\emph{continuous piecewise linear functions} ($\CPLF$s), ensuring strict
continuity across segments. Since any continuous function over a compact domain
can be approximated arbitrarily well by $\CPLF$s, the $\BNN$ naturally emerges
as a flexible and interpretable tool for \emph{function approximation}. Beyond
the use of this representation, the main contribution of the paper is the
introduction of a new variant of \emph{persistent entropy}, a topological
feature that is stable and scale invariant, called the \emph{length-weighted
persistent entropy} ($\LWPE$), which is weighted by the lifetime of topological
features. Our framework, which combines the $\BNN$ with a loss function based
on our $\LWPE$, aims to provide flexible and geometrically interpretable
approximations of nonlinear continuous functions in resource-constrained
settings, such as those with limited base points for $\BNN$ design and few
training epochs. Instead of optimizing internal weights, our approach directly
\emph{optimizes the base points that define the $\BNN$}. Experimental results
show that our approach achieves \emph{superior and faster approximation
performance} compared to classical loss functions such as MSE, RMSE, MAE, and
log-cosh.

</details>


### [181] [Probabilistic Modeling of Latent Agentic Substructures in Deep Neural Networks](https://arxiv.org/abs/2509.06701)
*Su Hyeong Lee,Risi Kondor,Richard Ngo*

Main category: cs.LG

TL;DR: 本文为神经模型开发了基于概率建模的智能主体理论，证明了不同情况下的一致性问题，还揭示了大语言模型中的主体对齐现象。


<details>
  <summary>Details</summary>
Motivation: 为神经模型开发基于概率建模的智能主体理论，以解决智能主体系统中的对齐问题。

Method: 将主体表示为结果分布，通过加权对数池化定义组合，利用克隆不变性、连续性和开放性构建递归结构，使用倾斜分析排除平凡复制。

Result: 证明严格一致性在某些情况下的可能性，揭示大语言模型中主体对齐现象及不同策略的效果。

Conclusion: 为子主体合并成更高级实体建立数学框架，对智能主体AI系统的对齐有新启示。

Abstract: We develop a theory of intelligent agency grounded in probabilistic modeling
for neural models. Agents are represented as outcome distributions with
epistemic utility given by log score, and compositions are defined through
weighted logarithmic pooling that strictly improves every member's welfare. We
prove that strict unanimity is impossible under linear pooling or in binary
outcome spaces, but possible with three or more outcomes. Our framework admits
recursive structure via cloning invariance, continuity, and openness, while
tilt-based analysis rules out trivial duplication. Finally, we formalize an
agentic alignment phenomenon in LLMs using our theory: eliciting a benevolent
persona ("Luigi'") induces an antagonistic counterpart ("Waluigi"), while a
manifest-then-suppress Waluigi strategy yields strictly larger first-order
misalignment reduction than pure Luigi reinforcement alone. These results
clarify how developing a principled mathematical framework for how subagents
can coalesce into coherent higher-level entities provides novel implications
for alignment in agentic AI systems.

</details>


### [182] [Nested Optimal Transport Distances](https://arxiv.org/abs/2509.06702)
*Ruben Bontorno,Songyan Hou*

Main category: cs.LG

TL;DR: 本文聚焦金融时间序列生成AI，采用嵌套最优运输距离评估，提出计算算法实现加速。


<details>
  <summary>Details</summary>
Motivation: 模拟真实金融时间序列对压力测试等很重要，但深度生成模型缺乏评估共识指标。

Method: 采用嵌套最优运输距离（时间因果变体），提出统计一致、自然可并行的计算算法。

Result: 算法较现有方法实现显著加速。

Conclusion: 嵌套最优运输距离适用于金融时间序列生成AI评估，新算法有计算优势。

Abstract: Simulating realistic financial time series is essential for stress testing,
scenario generation, and decision-making under uncertainty. Despite advances in
deep generative models, there is no consensus metric for their evaluation. We
focus on generative AI for financial time series in decision-making
applications and employ the nested optimal transport distance, a time-causal
variant of optimal transport distance, which is robust to tasks such as
hedging, optimal stopping, and reinforcement learning. Moreover, we propose a
statistically consistent, naturally parallelizable algorithm for its
computation, achieving substantial speedups over existing approaches.

</details>


### [183] [RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn Directly on Robotic Platforms](https://arxiv.org/abs/2509.06714)
*Zakariae El Asri,Ibrahim Laiche,Clément Rambour,Olivier Sigaud,Nicolas Thome*

Main category: cs.LG

TL;DR: 本文提出框架应对推理延迟问题，并提出 RT - HCP 算法，在 FURUTA 摆平台实验验证其优势。


<details>
  <summary>Details</summary>
Motivation: 直接在机器人上学习控制器需高样本效率，基于模型的强化学习方法推理时间长，难以满足机器人控制频率要求。

Method: 定义处理推理延迟的通用框架，对比多种 RL 算法并提出 RT - HCP 算法。

Result: 在 FURUTA 摆平台上直接学习控制器的实验验证了 RT - HCP 算法的优越性。

Conclusion: RT - HCP 算法在性能、样本效率和推理时间之间实现了良好平衡。

Abstract: Learning a controller directly on the robot requires extreme sample
efficiency. Model-based reinforcement learning (RL) methods are the most sample
efficient, but they often suffer from a too long inference time to meet the
robot control frequency requirements. In this paper, we address the sample
efficiency and inference time challenges with two contributions. First, we
define a general framework to deal with inference delays where the slow
inference robot controller provides a sequence of actions to feed the
control-hungry robotic platform without execution gaps. Then, we compare
several RL algorithms in the light of this framework and propose RT-HCP, an
algorithm that offers an excellent trade-off between performance, sample
efficiency and inference time. We validate the superiority of RT-HCP with
experiments where we learn a controller directly on a simple but high frequency
FURUTA pendulum platform. Code: github.com/elasriz/RTHCP

</details>


### [184] [Long-Range Graph Wavelet Networks](https://arxiv.org/abs/2509.06743)
*Filippo Guerranti,Fabrizio Forte,Simon Geisler,Stephan Günnemann*

Main category: cs.LG

TL;DR: 提出LR - GWN解决图机器学习中长距离交互建模问题，在长距离基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于小波的图神经网络依赖有限阶多项式近似，限制感受野和长距离传播，需更好方法建模长距离交互。

Method: 提出LR - GWN，将小波滤波器分解为局部和全局组件，局部用低阶多项式聚合，长距离交互通过灵活的谱域参数化捕捉。

Result: LR - GWN在长距离基准测试中达到基于小波方法的SOTA性能，在短距离数据集上也有竞争力。

Conclusion: LR - GWN的混合设计在小波框架内统一了短距离和长距离信息流，有效解决长距离交互建模问题。

Abstract: Modeling long-range interactions, the propagation of information across
distant parts of a graph, is a central challenge in graph machine learning.
Graph wavelets, inspired by multi-resolution signal processing, provide a
principled way to capture both local and global structures. However, existing
wavelet-based graph neural networks rely on finite-order polynomial
approximations, which limit their receptive fields and hinder long-range
propagation. We propose Long-Range Graph Wavelet Networks (LR-GWN), which
decompose wavelet filters into complementary local and global components. Local
aggregation is handled with efficient low-order polynomials, while long-range
interactions are captured through a flexible spectral domain parameterization.
This hybrid design unifies short- and long-distance information flow within a
principled wavelet framework. Experiments show that LR-GWN achieves
state-of-the-art performance among wavelet-based methods on long-range
benchmarks, while remaining competitive on short-range datasets.

</details>


### [185] [Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization](https://arxiv.org/abs/2509.06759)
*Thanh Thi Nguyen,Campbell Wilson,Janis Dalins*

Main category: cs.LG

TL;DR: 本文探讨了微调大视觉语言模型（LVLMs）的范式，介绍DRL和DPO技术在模型与人类偏好对齐中的应用，并讨论了面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决微调LVLMs使其与人类价值观对齐及执行特定任务的关键挑战。

Method: 研究利用深度强化学习（DRL）和直接偏好优化（DPO）技术微调LVLMs。

Result: 明确了关键方法类别，分析了偏好数据来源、奖励信号。

Conclusion: 有助于清晰理解DRL和DPO对构建强大且符合人类需求的LVLMs的贡献。

Abstract: Large Vision-Language Models (LVLMs) or multimodal large language models
represent a significant advancement in artificial intelligence, enabling
systems to understand and generate content across both visual and textual
modalities. While large-scale pretraining has driven substantial progress,
fine-tuning these models for aligning with human values or engaging in specific
tasks or behaviors remains a critical challenge. Deep Reinforcement Learning
(DRL) and Direct Preference Optimization (DPO) offer promising frameworks for
this aligning process. While DRL enables models to optimize actions using
reward signals instead of relying solely on supervised preference data, DPO
directly aligns the policy with preferences, eliminating the need for an
explicit reward model. This overview explores paradigms for fine-tuning LVLMs,
highlighting how DRL and DPO techniques can be used to align models with human
preferences and values, improve task performance, and enable adaptive
multimodal interaction. We categorize key approaches, examine sources of
preference data, reward signals, and discuss open challenges such as
scalability, sample efficiency, continual learning, generalization, and safety.
The goal is to provide a clear understanding of how DRL and DPO contribute to
the evolution of robust and human-aligned LVLMs.

</details>


### [186] [Asynchronous Message Passing for Addressing Oversquashing in Graph Neural Networks](https://arxiv.org/abs/2509.06777)
*Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: 针对GNN的Oversquashing问题，提出异步更新节点特征的框架，应用于多数据集分类任务表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决GNN在长距离交互任务中的Oversquashing问题，避免图重连方法的信息损失和增加通道容量带来的参数复杂度问题。

Method: 提出高效的与模型无关的框架，基于节点中心性值创建节点批次异步更新节点特征，顺序处理信息。

Result: 应用于六个标准图数据集和两个长距离数据集进行图分类，在REDDIT - BINARY和Peptides - struct上分别提升5%和4%。

Conclusion: 所提框架相比标准同步方法能保持更高的特征敏感度界限，有较好表现。

Abstract: Graph Neural Networks (GNNs) suffer from Oversquashing, which occurs when
tasks require long-range interactions. The problem arises from the presence of
bottlenecks that limit the propagation of messages among distant nodes.
Recently, graph rewiring methods modify edge connectivity and are expected to
perform well on long-range tasks. Yet, graph rewiring compromises the inductive
bias, incurring significant information loss in solving the downstream task.
Furthermore, increasing channel capacity may overcome information bottlenecks
but enhance the parameter complexity of the model. To alleviate these
shortcomings, we propose an efficient model-agnostic framework that
asynchronously updates node features, unlike traditional synchronous message
passing GNNs. Our framework creates node batches in every layer based on the
node centrality values. The features of the nodes belonging to these batches
will only get updated. Asynchronous message updates process information
sequentially across layers, avoiding simultaneous compression into
fixed-capacity channels. We also theoretically establish that our proposed
framework maintains higher feature sensitivity bounds compared to standard
synchronous approaches. Our framework is applied to six standard graph datasets
and two long-range datasets to perform graph classification and achieves
impressive performances with a $5\%$ and $4\%$ improvements on REDDIT-BINARY
and Peptides-struct, respectively.

</details>


### [187] [Physics-informed Value Learner for Offline Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2509.06782)
*Vittorio Giammarino,Ruiqi Ni,Ahmed H. Qureshi*

Main category: cs.LG

TL;DR: 提出物理信息正则化损失用于离线目标条件强化学习，与HIQL结合的Pi - HIQL在性能和泛化性上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习在实践中面临从有限状态 - 动作空间数据集学习和长视野任务泛化的挑战。

Method: 提出基于Eikonal偏微分方程的物理信息正则化损失用于价值学习，该正则化器与基于时间差分的价值学习兼容，可集成到现有离线目标条件强化学习算法中。

Result: Pi - HIQL在性能和泛化性上显著提升，在拼接制度和大规模导航任务中增益明显。

Conclusion: 所提物理信息正则化损失能有效改善离线目标条件强化学习的性能和泛化能力。

Abstract: Offline Goal-Conditioned Reinforcement Learning (GCRL) holds great promise
for domains such as autonomous navigation and locomotion, where collecting
interactive data is costly and unsafe. However, it remains challenging in
practice due to the need to learn from datasets with limited coverage of the
state-action space and to generalize across long-horizon tasks. To improve on
these challenges, we propose a Physics-informed (Pi) regularized loss for value
learning, derived from the Eikonal Partial Differential Equation (PDE) and
which induces a geometric inductive bias in the learned value function. Unlike
generic gradient penalties that are primarily used to stabilize training, our
formulation is grounded in continuous-time optimal control and encourages value
functions to align with cost-to-go structures. The proposed regularizer is
broadly compatible with temporal-difference-based value learning and can be
integrated into existing Offline GCRL algorithms. When combined with
Hierarchical Implicit Q-Learning (HIQL), the resulting method, Physics-informed
HIQL (Pi-HIQL), yields significant improvements in both performance and
generalization, with pronounced gains in stitching regimes and large-scale
navigation tasks.

</details>


### [188] [\texttt{R$^\textbf{2}$AI}: Towards Resistant and Resilient AI in an Evolving World](https://arxiv.org/abs/2509.06786)
*Youbang Sun,Xiang Wang,Jie Fu,Chaochao Lu,Bowen Zhou*

Main category: cs.LG

TL;DR: 文章指出AI能力增长与安全进展的差距，提出“safe - by - coevolution”概念及R²AI框架来解决AI安全问题。


<details>
  <summary>Details</summary>
Motivation: 解决AI能力快速增长和安全进展滞后的差距，改进现有“Make AI Safe”和“Make Safe AI”范式的不足。

Method: 提出“safe - by - coevolution”概念，引入R²AI框架，整合快速和慢速安全模型、通过安全风洞进行对抗模拟和验证、设置持续反馈循环。

Result: 提出的R²AI框架为动态环境中的AI持续安全提供了可扩展和主动的路径。

Conclusion: R²AI框架能解决AI发展过程中的短期漏洞和长期生存风险。

Abstract: In this position paper, we address the persistent gap between rapidly growing
AI capabilities and lagging safety progress. Existing paradigms divide into
``Make AI Safe'', which applies post-hoc alignment and guardrails but remains
brittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety
but struggles to address unforeseen risks in open-ended environments. We
therefore propose \textit{safe-by-coevolution} as a new formulation of the
``Make Safe AI'' paradigm, inspired by biological immunity, in which safety
becomes a dynamic, adversarial, and ongoing learning process. To operationalize
this vision, we introduce \texttt{R$^2$AI} -- \textit{Resistant and Resilient
AI} -- as a practical framework that unites resistance against known threats
with resilience to unforeseen risks. \texttt{R$^2$AI} integrates \textit{fast
and slow safe models}, adversarial simulation and verification through a
\textit{safety wind tunnel}, and continual feedback loops that guide safety and
capability to coevolve. We argue that this framework offers a scalable and
proactive path to maintain continual safety in dynamic environments, addressing
both near-term vulnerabilities and long-term existential risks as AI advances
toward AGI and ASI.

</details>


### [189] [floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL](https://arxiv.org/abs/2509.06863)
*Bhavya Agrawalla,Michal Nauman,Khush Agarwal,Aviral Kumar*

Main category: cs.LG

TL;DR: 本文研究强化学习中TD方法迭代计算的好处，提出floq方法，通过速度场参数化Q函数，在多个基准测试中性能提升近1.8倍，显示迭代计算对值学习的潜力。


<details>
  <summary>Details</summary>
Motivation: 受现代大规模机器学习技术中使用提供密集监督的训练目标的启发，研究强化学习中TD方法迭代计算的好处。

Method: 引入floq方法，用速度场参数化Q函数，使用流匹配技术训练，用TD学习目标训练速度场，通过设置积分步数实现对Q函数容量更细粒度的控制和扩展。

Result: 在一系列具有挑战性的离线RL基准测试和在线微调任务中，floq性能提升近1.8倍，容量扩展能力远超标准TD学习架构。

Conclusion: 迭代计算对值学习有潜力，floq方法比整体架构在Q函数容量控制和扩展上更具优势。

Abstract: A hallmark of modern large-scale machine learning techniques is the use of
training objectives that provide dense supervision to intermediate
computations, such as teacher forcing the next token in language models or
denoising step-by-step in diffusion models. This enables models to learn
complex functions in a generalizable manner. Motivated by this observation, we
investigate the benefits of iterative computation for temporal difference (TD)
methods in reinforcement learning (RL). Typically they represent value
functions in a monolithic fashion, without iterative compute. We introduce floq
(flow-matching Q-functions), an approach that parameterizes the Q-function
using a velocity field and trains it using techniques from flow-matching,
typically used in generative modeling. This velocity field underneath the flow
is trained using a TD-learning objective, which bootstraps from values produced
by a target velocity field, computed by running multiple steps of numerical
integration. Crucially, floq allows for more fine-grained control and scaling
of the Q-function capacity than monolithic architectures, by appropriately
setting the number of integration steps. Across a suite of challenging offline
RL benchmarks and online fine-tuning tasks, floq improves performance by nearly
1.8x. floq scales capacity far better than standard TD-learning architectures,
highlighting the potential of iterative computation for value learning.

</details>


### [190] [AxelSMOTE: An Agent-Based Oversampling Algorithm for Imbalanced Classification](https://arxiv.org/abs/2509.06875)
*Sukumar Kishanthan,Asela Hevapathige*

Main category: cs.LG

TL;DR: 论文介绍AxelSMOTE方法解决机器学习类不平衡问题，实验显示其优于现有方法且具计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统过采样技术处理类不平衡数据存在缺陷，如独立处理特征、缺乏相似性控制等，需新方法解决。

Method: 引入基于代理的AxelSMOTE方法，借鉴Axelrod文化传播模型，有特征分组、概率交换机制、Beta分布混合和多样性注入四项创新。

Result: 在八个不平衡数据集实验中，AxelSMOTE优于现有采样方法，且保持计算效率。

Conclusion: AxelSMOTE能有效解决类不平衡问题，在性能和计算效率上表现良好。

Abstract: Class imbalance in machine learning poses a significant challenge, as skewed
datasets often hinder performance on minority classes. Traditional oversampling
techniques, which are commonly used to alleviate class imbalance, have several
drawbacks: they treat features independently, lack similarity-based controls,
limit sample diversity, and fail to manage synthetic variety effectively. To
overcome these issues, we introduce AxelSMOTE, an innovative agent-based
approach that views data instances as autonomous agents engaging in complex
interactions. Based on Axelrod's cultural dissemination model, AxelSMOTE
implements four key innovations: (1) trait-based feature grouping to preserve
correlations; (2) a similarity-based probabilistic exchange mechanism for
meaningful interactions; (3) Beta distribution blending for realistic
interpolation; and (4) controlled diversity injection to avoid overfitting.
Experiments on eight imbalanced datasets demonstrate that AxelSMOTE outperforms
state-of-the-art sampling methods while maintaining computational efficiency.

</details>


### [191] [Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition](https://arxiv.org/abs/2509.06918)
*Tarhib Al Azad,Shahana Ibrahim*

Main category: cs.LG

TL;DR: 本文提出一种鲁棒的OOD检测框架，结合损失校正和低秩稀疏分解方法，在含噪标签设置下显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测在含噪训练标签下的有效性研究不足，且直接结合现有抗标签噪声方法与OOD检测策略无法解决该挑战。

Method: 提出一个鲁棒OOD检测框架，将噪声标签学习中的损失校正技术与信号处理中的低秩和稀疏分解方法相结合。

Result: 在合成和真实数据集上的大量实验表明，该方法在严重的含噪标签设置下显著优于现有OOD检测技术。

Conclusion: 所提出的鲁棒OOD检测框架能有效解决含噪训练标签下的OOD检测问题。

Abstract: Robust out-of-distribution (OOD) detection is an indispensable component of
modern artificial intelligence (AI) systems, especially in safety-critical
applications where models must identify inputs from unfamiliar classes not seen
during training. While OOD detection has been extensively studied in the
machine learning literature--with both post hoc and training-based
approaches--its effectiveness under noisy training labels remains
underexplored. Recent studies suggest that label noise can significantly
degrade OOD performance, yet principled solutions to this issue are lacking. In
this work, we demonstrate that directly combining existing label noise-robust
methods with OOD detection strategies is insufficient to address this critical
challenge. To overcome this, we propose a robust OOD detection framework that
integrates loss correction techniques from the noisy label learning literature
with low-rank and sparse decomposition methods from signal processing.
Extensive experiments on both synthetic and real-world datasets demonstrate
that our method significantly outperforms the state-of-the-art OOD detection
techniques, particularly under severe noisy label settings.

</details>


### [192] [Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding](https://arxiv.org/abs/2509.06923)
*Ziheng Li,Zexu Sun,Jinman Zhao,Erxue Min,Yongcheng Zeng,Hui Wu,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen,Zhi-Hong Deng*

Main category: cs.LG

TL;DR: 现有RLVR方法因问题难度与模型能力不匹配而探索效率低，本文提出SEELE框架，动态调整问题难度，实验显示SEELE在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决现有强化学习与可验证奖励（RLVR）方法因训练数据难度与模型能力不匹配导致的探索效率低问题。

Method: 形式化问题难度影响，提出SEELE框架，通过附加提示、自适应调整提示长度、采用多轮滚动采样策略和拟合项目反应理论模型来动态调整问题难度。

Result: SEELE在六个数学推理基准测试中，分别比GRPO和SFT高出11.8和10.5分，比之前最佳监督辅助方法平均高出3.6分。

Conclusion: SEELE通过动态调整问题难度，使问题难度与模型能力相匹配，有效提高了探索效率。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable
success in enhancing the reasoning capabilities of large language models
(LLMs). However, existing RLVR methods often suffer from exploration
inefficiency due to mismatches between the training data's difficulty and the
model's capability. LLMs fail to discover viable reasoning paths when problems
are overly difficult, while learning little new capability when problems are
too simple. In this work, we formalize the impact of problem difficulty by
quantifying the relationship between loss descent speed and rollout accuracy.
Building on this analysis, we propose SEELE, a novel supervision-aided RLVR
framework that dynamically adjusts problem difficulty to stay within the
high-efficiency region. SEELE augments each training sample by appending a hint
(part of a full solution) after the original problem. Unlike previous
hint-based approaches, SEELE deliberately and adaptively adjusts the hint
length for each problem to achieve an optimal difficulty. To determine the
optimal hint length, SEELE employs a multi-round rollout sampling strategy. In
each round, it fits an item response theory model to the accuracy-hint pairs
collected in preceding rounds to predict the required hint length for the next
round. This instance-level, real-time difficulty adjustment aligns problem
difficulty with the evolving model capability, thereby improving exploration
efficiency. Experimental results show that SEELE outperforms Group Relative
Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5
points, respectively, and surpasses the best previous supervision-aided
approach by +3.6 points on average across six math reasoning benchmarks.

</details>


### [193] [Neutron Reflectometry by Gradient Descent](https://arxiv.org/abs/2509.06924)
*Max D. ~Champneys,Andrew J. ~Parnell,Philipp Gutfreund,Maximilian W. A. Skoda,. Patrick A. Fairclough,Timothy J. ~Rogers,Stephanie L. ~Burg*

Main category: cs.LG

TL;DR: 提出对正向反射模型进行梯度下降优化反射率数据分析的新方法，通过自动微分技术评估误差函数梯度，给出两个案例研究并提供开源库。


<details>
  <summary>Details</summary>
Motivation: 传统中子反射测量解决逆建模问题对大量数据或复杂多层结构效率低，替代的机器学习模型会丢失物理直觉。

Method: 对正向反射模型进行梯度下降，用自动微分技术评估误差函数相对于感兴趣参数的精确梯度。

Result: 在厚氧化物石英膜上有先进性能，在有机 LED 多层器件高复杂度区域有稳健的协同拟合性能。

Conclusion: 新方法可让中子反射测量用户利用强大的现代优化和推理技术，开源库可使基于梯度的方法应用于其他中子反射数据集。

Abstract: Neutron reflectometry (NR) is a powerful technique to probe surfaces and
interfaces. NR is inherently an indirect measurement technique, access to the
physical quantities of interest (layer thickness, scattering length density,
roughness), necessitate the solution of an inverse modelling problem, that is
inefficient for large amounts of data or complex multiplayer structures (e.g.
lithium batteries / electrodes). Recently, surrogate machine learning models
have been proposed as an alternative to existing optimisation routines.
Although such approaches have been successful, physical intuition is lost when
replacing governing equations with fast neural networks. Instead, we propose a
novel and efficient approach; to optimise reflectivity data analysis by
performing gradient descent on the forward reflection model itself. Herein,
automatic differentiation techniques are used to evaluate exact gradients of
the error function with respect to the parameters of interest. Access to these
quantities enables users of neutron reflectometry to harness a host of powerful
modern optimisation and inference techniques that remain thus far unexploited
in the context of neutron reflectometry. This paper presents two benchmark case
studies; demonstrating state-of-the-art performance on a thick oxide quartz
film, and robust co-fitting performance in the high complexity regime of
organic LED multilayer devices. Additionally, we provide an open-source library
of differentiable reflectometry kernels in the python programming language so
that gradient based approaches can readily be applied to other NR datasets.

</details>


### [194] [Learning words in groups: fusion algebras, tensor ranks and grokking](https://arxiv.org/abs/2509.06931)
*Maor Shutman,Oren Louidor,Ran Tessler*

Main category: cs.LG

TL;DR: 简单两层神经网络可学习有限群中任意单词操作并出现顿悟现象，通过学习低秩3 - 张量实现。


<details>
  <summary>Details</summary>
Motivation: 解释简单两层神经网络学习有限群中任意单词操作并出现顿悟现象的机制。

Method: 将问题重构为学习特定3 - 张量，利用群的基本自共轭表示的三元组分解张量，借助融合结构排除许多分量，聚焦替代模型。

Result: 网络能找到低秩实现（或其近似），以有限宽度泛化近似单词张量；简单乘法单词情况下，网络有效实现Strassen意义上的高效矩阵乘法。

Conclusion: 阐明了网络在梯度下降下达成解决方案的机制。

Abstract: In this work, we demonstrate that a simple two-layer neural network with
standard activation functions can learn an arbitrary word operation in any
finite group, provided sufficient width is available and exhibits grokking
while doing so. To explain the mechanism by which this is achieved, we reframe
the problem as that of learning a particular $3$-tensor, which we show is
typically of low rank. A key insight is that low-rank implementations of this
tensor can be obtained by decomposing it along triplets of basic self-conjugate
representations of the group and leveraging the fusion structure to rule out
many components. Focusing on a phenomenologically similar but more tractable
surrogate model, we show that the network is able to find such low-rank
implementations (or approximations thereof), thereby using limited width to
approximate the word-tensor in a generalizable way. In the case of the simple
multiplication word, we further elucidate the form of these low-rank
implementations, showing that the network effectively implements efficient
matrix multiplication in the sense of Strassen. Our work also sheds light on
the mechanism by which a network reaches such a solution under gradient
descent.

</details>


### [195] [From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers](https://arxiv.org/abs/2509.06938)
*Praneet Suresh,Jack Stanley,Sonia Joseph,Luca Scimeca,Danilo Bzdok*

Main category: cs.LG

TL;DR: 研究预训练transformer模型幻觉产生机制，发现输入不确定性影响语义概念激活致幻觉，还能预测幻觉，成果有多方面意义。


<details>
  <summary>Details</summary>
Motivation: 生成式AI普及，其行为波动如幻觉问题阻碍高风险领域应用，需深入了解其失败模式。

Method: 通过稀疏自编码器捕获概念表征，在输入空间不确定性可控场景下研究预训练transformer模型。

Result: 输入信息越无结构，模型使用语义概念越多；输入不确定性增加使模型激活与输入无关语义特征致幻觉；纯噪声输入能触发有意义概念；可从层激活概念模式预测幻觉。

Conclusion: 研究成果对AI价值对齐、安全、对抗攻击及量化幻觉风险有重要意义。

Abstract: As generative AI systems become competent and democratized in science,
business, and government, deeper insight into their failure modes now poses an
acute need. The occasional volatility in their behavior, such as the propensity
of transformer models to hallucinate, impedes trust and adoption of emerging AI
solutions in high-stakes areas. In the present work, we establish how and when
hallucinations arise in pre-trained transformer models through concept
representations captured by sparse autoencoders, under scenarios with
experimentally controlled uncertainty in the input space. Our systematic
experiments reveal that the number of semantic concepts used by the transformer
model grows as the input information becomes increasingly unstructured. In the
face of growing uncertainty in the input space, the transformer model becomes
prone to activate coherent yet input-insensitive semantic features, leading to
hallucinated output. At its extreme, for pure-noise inputs, we identify a wide
variety of robustly triggered and meaningful concepts in the intermediate
activations of pre-trained transformer models, whose functional integrity we
confirm through targeted steering. We also show that hallucinations in the
output of a transformer model can be reliably predicted from the concept
patterns embedded in transformer layer activations. This collection of insights
on transformer internal processing mechanics has immediate consequences for
aligning AI models with human values, AI safety, opening the attack surface for
potential adversarial attacks, and providing a basis for automatic
quantification of a model's hallucination risk.

</details>


### [196] [Outcome-based Exploration for LLM Reasoning](https://arxiv.org/abs/2509.06941)
*Yuda Song,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: 研究基于结果的强化学习在提升大语言模型推理能力时导致生成多样性下降问题，提出基于结果的探索方法，实验证明可提升准确率并缓解多样性崩溃。


<details>
  <summary>Details</summary>
Motivation: 基于结果的强化学习在提升大语言模型推理准确率时会导致生成多样性系统性损失，影响实际应用性能。

Method: 将强化学习后训练视为采样过程进行分析，提出基于结果的探索方法，包括历史探索和批量探索两种算法。

Result: 在标准竞赛数学实验中，使用Llama和Qwen模型，两种算法在提升准确率的同时缓解了多样性崩溃。从理论上通过基于结果的多臂老虎机新模型形式化了基于结果探索的好处。

Conclusion: 提出的方法为强化学习方法提供了一条实用路径，可在不牺牲可扩展部署所需多样性的情况下增强推理能力。

Abstract: Reinforcement learning (RL) has emerged as a powerful method for improving
the reasoning abilities of large language models (LLMs). Outcome-based RL,
which rewards policies solely for the correctness of the final answer, yields
substantial accuracy gains but also induces a systematic loss in generation
diversity. This collapse undermines real-world performance, where diversity is
critical for test-time scaling. We analyze this phenomenon by viewing RL
post-training as a sampling process and show that, strikingly, RL can reduce
effective diversity even on the training set relative to the base model. Our
study highlights two central findings: (i) a transfer of diversity degradation,
where reduced diversity on solved problems propagates to unsolved ones, and
(ii) the tractability of the outcome space, since reasoning tasks admit only a
limited set of distinct answers. Motivated by these insights, we propose
outcome-based exploration, which assigns exploration bonuses according to final
outcomes. We introduce two complementary algorithms: historical exploration,
which encourages rarely observed answers via UCB-style bonuses, and batch
exploration, which penalizes within-batch repetition to promote test-time
diversity. Experiments on standard competition math with Llama and Qwen models
demonstrate that both methods improve accuracy while mitigating diversity
collapse. On the theoretical side, we formalize the benefit of outcome-based
exploration through a new model of outcome-based bandits. Together, these
contributions chart a practical path toward RL methods that enhance reasoning
without sacrificing the diversity essential for scalable deployment.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [197] [Robustness and Invariance of Hybrid Metaheuristics under Objective Function Transformations](https://arxiv.org/abs/2509.05445)
*Grzegorz Sroka,Sławomir T. Wierzchoń*

Main category: cs.NE

TL;DR: 评估混合群体元启发式算法在不同目标空间变换下的鲁棒性和结构不变性，发现差分基混合算法表现更好。


<details>
  <summary>Details</summary>
Motivation: 评估混合群体元启发式算法在各种目标空间变换下的鲁棒性和结构不变性。

Method: 对19种先进算法应用轻量级即插即用混合算子，在CEC - 2017套件的四个维度上进行五种变换类型的基准测试，并进行统计比较。

Result: 差分基混合算法在所有测试变形下保持高精度、稳定性和不变性，经典算法尤其是基于PSO和HHO的变体在非可分或扭曲景观下性能显著下降。

Conclusion: 自适应、结构有弹性的混合算法在受特定领域变换影响的现实优化任务中具有优越性。

Abstract: This paper evaluates the robustness and structural invariance of hybrid
population-based metaheuristics under various objective space transformations.
A lightweight plug-and-play hybridization operator is applied to nineteen
state-of-the-art algorithms-including differential evolution (DE), particle
swarm optimization (PSO), and recent bio-inspired methods-without modifying
their internal logic. Benchmarking on the CEC-2017 suite across four dimensions
(10, 30, 50, 100) is performed under five transformation types: baseline,
translation, scaling, rotation, and constant shift. Statistical comparisons
based on Wilcoxon and Friedman tests, Bayesian dominance analysis, and
convergence trajectory profiling consistently show that differential-based
hybrids (e.g., hIMODE, hSHADE, hDMSSA) maintain high accuracy, stability, and
invariance under all tested deformations. In contrast, classical
algorithms-especially PSO- and HHO-based variants-exhibit significant
performance degradation under non-separable or distorted landscapes. The
findings confirm the superiority of adaptive, structurally resilient hybrids
for real-world optimization tasks subject to domain-specific transformations.

</details>


### [198] [Genesis: A Spiking Neuromorphic Accelerator With On-chip Continual Learning](https://arxiv.org/abs/2509.05858)
*Vedant Karia,Abdullah Zyarah,Dhireesha Kudithipudi*

Main category: cs.NE

TL;DR: 提出名为Genesis的脉冲持续学习加速器，可缓解灾难性遗忘，在split - MNIST基准测试有一定准确率和低功耗。


<details>
  <summary>Details</summary>
Motivation: 实现人工系统持续学习能力会增加内存和计算需求，尤其在资源有限平台，需解决此问题。

Method: 支持依赖活动的元可塑性等神经启发机制；集成低精度持续学习参数，采用自定义数据移动策略；使用内存映射技术。

Result: 在任务无关的split - MNIST基准测试中平均分类准确率为74.6%，65nm技术节点功耗17.08mW。

Conclusion: Genesis加速器能在有限资源下实现持续学习，缓解灾难性遗忘。

Abstract: Continual learning, the ability to acquire and transfer knowledge through a
models lifetime, is critical for artificial agents that interact in real-world
environments. Biological brains inherently demonstrate these capabilities while
operating within limited energy and resource budgets. Achieving continual
learning capability in artificial systems considerably increases memory and
computational demands, and even more so when deploying on platforms with
limited resources. In this work, Genesis, a spiking continual learning
accelerator, is proposed to address this gap. The architecture supports
neurally inspired mechanisms, such as activity-dependent metaplasticity, to
alleviate catastrophic forgetting. It integrates low-precision continual
learning parametersand employs a custom data movement strategy to accommodate
the sparsely distributed spikes. Furthermore, the architecture features a
memory mapping technique that places metaplasticity parameters and synaptic
weights in a single address location for faster memory access. Results show
that the mean classification accuracy for Genesis is 74.6% on a task-agnostic
split-MNIST benchmark with power consumption of 17.08mW in a 65nm technology
node.

</details>


### [199] [An Explainable Framework for Particle Swarm Optimization using Landscape Analysis and Machine Learning](https://arxiv.org/abs/2509.06272)
*Nitin Gupta,Bapi Dutta,Anupam Yadav*

Main category: cs.NE

TL;DR: 研究粒子群优化算法（PSO）不同拓扑结构对性能的影响，开发解释性基准框架和自动算法配置方法，给出拓扑选择和参数配置指南。


<details>
  <summary>Details</summary>
Motivation: 解决群智能算法因缺乏透明度阻碍广泛应用的问题，进一步理解PSO不同拓扑结构的关键作用以提升可解释性。

Method: 用探索性景观分析（ELA）开发景观表征框架，对比三种基本拓扑结构，开发解释性基准框架，引入机器学习方法进行自动算法配置。

Result: 通过对24个多维度基准函数的实验，建立了拓扑选择和参数配置的实用指南。

Conclusion: 研究成果推动了更透明可靠的群智能系统的发展。

Abstract: Swarm intelligence algorithms have demonstrated remarkable success in solving
complex optimization problems across diverse domains. However, their widespread
adoption is often hindered by limited transparency in how algorithmic
components influence performance. This work presents a multi-faceted
investigation of Particle Swarm Optimization (PSO) to further understand the
key role of different topologies for better interpretability and
explainability. To achieve this objective, we first develop a comprehensive
landscape characterization framework using Exploratory Landscape Analysis (ELA)
to quantify problem difficulty and identify critical features affecting the
optimization performance of PSO. Next, we conduct a rigorous empirical study
comparing three fundamental swarm communication architectures -- Ring, Star,
and Von Neumann topologies -- analysing their distinct impacts on
exploration-exploitation balance, convergence behaviour, and solution quality
and eventually develop an explainable benchmarking framework for PSO, to decode
how swarm topologies affects information flow, diversity, and convergence.
Based on this, a novel machine learning approach for automated algorithm
configuration is introduced for training predictive models on extensive Area
over the Convergence Curve (AOCC) data to recommend optimal settings based on
problem characteristics. Through systematic experimentation across twenty four
benchmark functions in multiple dimensions, we establish practical guidelines
for topology selection and parameter configuration. These findings advance the
development of more transparent and reliable swarm intelligence systems. The
source codes of this work can be accessed at
https://github.com/GitNitin02/ioh_pso.

</details>


### [200] [Full Integer Arithmetic Online Training for Spiking Neural Networks](https://arxiv.org/abs/2509.06636)
*Ismael Gomez,Guangzhi Tang*

Main category: cs.NE

TL;DR: 本文提出整数在线训练算法，用混合精度方法提升SNN训练效率、降低内存使用，评估显示其性能良好，能用于资源受限硬件。


<details>
  <summary>Details</summary>
Motivation: 现有SNN训练方法计算量大，需更高效方法。

Method: 采用混合精度方法的整数在线训练算法，用整数运算替代浮点运算。

Result: 在MNIST和SHD数据集上，混合精度模型精度与全精度基线相当或更好，内存使用降低超60%，但低精度和深层模型有局限。

Conclusion: 该算法是高效训练SNN的有效方案，可在不牺牲精度下用于资源受限的神经形态硬件。

Abstract: Spiking Neural Networks (SNNs) are promising for neuromorphic computing due
to their biological plausibility and energy efficiency. However, training
methods like Backpropagation Through Time (BPTT) and Real Time Recurrent
Learning (RTRL) remain computationally intensive. This work introduces an
integer-only, online training algorithm using a mixed-precision approach to
improve efficiency and reduce memory usage by over 60%. The method replaces
floating-point operations with integer arithmetic to enable hardware-friendly
implementation. It generalizes to Convolutional and Recurrent SNNs (CSNNs,
RSNNs), showing versatility across architectures. Evaluations on MNIST and the
Spiking Heidelberg Digits (SHD) dataset demonstrate that mixed-precision models
achieve accuracy comparable to or better than full-precision baselines using
16-bit shadow and 8- or 12-bit inference weights. Despite some limitations in
low-precision and deeper models, performance remains robust. In conclusion, the
proposed integer-only online learning algorithm presents an effective solution
for efficiently training SNNs, enabling deployment on resource-constrained
neuromorphic hardware without sacrificing accuracy.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [201] [Waltz: Temperature-Aware Cooperative Compression for High-Performance Compression-Based CSDs](https://arxiv.org/abs/2509.05365)
*Dingcui Yu,Yunpeng Song,Yiyang Huang,Yumiao Zhao,Yina Lv,Chundong Wang,Youtao Zhang,Liang Shi*

Main category: cs.PF

TL;DR: 本文研究SSD不同压缩方案对温度和性能的影响，提出温度感知的协作压缩方法Waltz及变体，在F2FS中实现高性能、延长SSD寿命并防止过热关机。


<details>
  <summary>Details</summary>
Motivation: 了解主机端和设备端压缩方案对SSD存储系统温度和性能的影响，确保高效数据压缩、高性能和更好的温度控制。

Method: 进行定量研究，提出温度感知的协作压缩方法Waltz，通过监控设备温度调度主机和设备端的（解）压缩任务，还引入两个变体进行空间和性能优化。

Result: 设备端压缩会使CCSD温度过高导致节流和服务关闭，主机端压缩有软件栈开销，造成性能下降和资源消耗；Waltz在F2FS中实现，达到高性能、延长SSD寿命和防止过热关机。

Conclusion: Waltz是一种有效的温度感知协作压缩方法，能解决不同压缩方案的问题，实现高效数据压缩。

Abstract: Data compression is widely adopted for modern solid-state drives (SSDs) to
mitigate both storage capacity and SSD lifetime issues. Researchers have
proposed compression schemes at different system layers, including device-side
solutions like CCSDs ( c ompression-based c omputational SSDs) and compression
supported by host-side, like F2FS (flash-friendly file system). We conduct
quantitative studies to understand how host-side and device-side compression
schemes affect the temperature and performance of SSD-based storage systems.
From our experiments, device-side compression, facilitated by a hardware
compression engine, can raise the temperature of CCSDs to intolerable levels,
resulting in throttling and service shutdown. In contrast, host-side
compression causes software-stack overhead, which often results in large
performance degradation and resource consumption. To ensure efficient data
compression with high performance and better temperature control, we propose
Waltz, a temperature-aware cooperative compression method that schedules
(de)compression tasks at the host and device sides by monitoring device
temperature. Furthermore, we introduce two variants (Waltzs and Waltzp) for
space and performance optimization, respectively. Waltz is implemented within
F2FS, achieving high performance while extending SSD lifetime and preventing
overheating-induced in-flight shutdowns.

</details>


### [202] [Efficient Fault Localization in a Cloud Stack Using End-to-End Application Service Topology](https://arxiv.org/abs/2509.05511)
*Dhanya R Mathews,Mudit Verma,Pooja Aggarwal,J. Lakshmi*

Main category: cs.PF

TL;DR: 文章提出考虑应用服务拓扑的方法选指标，结合RCD算法定位性能异常根源，还提出TA - RCD算法，评估显示新方法性能优于现有RCD算法。


<details>
  <summary>Details</summary>
Motivation: 云应用服务组件增多使故障管理复杂，大量可观测数据给实时响应异常和恢复服务质量带来挑战，需识别关键数据和定位根源以确保服务弹性。

Method: 考虑应用服务拓扑选择云栈中最具信息性的指标，用RCD算法评估指标作用，提出结合端到端应用服务拓扑的TA - RCD算法。

Result: 故障注入研究评估显示，新方法在Top - 3和Top - 5召回率上平均比现有RCD算法至少好2倍。

Conclusion: 所提考虑应用服务拓扑的方法及TA - RCD算法能有效提高根因定位的准确性和效率，提升服务弹性。

Abstract: Cloud application services are distributed in nature and have components
across the stack working together to deliver the experience to end users. The
wide adoption of microservice architecture exacerbates failure management due
to increased service components. To be effective, the strategies to enhance the
application service resilience need to be autonomous and developed at the
service's granularity, considering its end-to-end components. However, the
massive amount of observability data generated by all these components across
the service stack poses a significant challenge in reacting to anomalies and
restoring the service quality in real time. Identifying the most informative
observability data from across the cloud service stack and timely localization
of root causes of anomalies thus becomes crucial to ensure service resilience.
This article presents a novel approach that considers the application service
topology to select the most informative metrics across the cloud stack to
support efficient, explainable, and accurate root cause identifications in case
of performance anomalies. The usefulness of the selected metrics is then
evaluated using the state-of-the-art Root Cause Detection (RCD) algorithm for
localizing the root cause of performance anomalies. As a step towards improving
the accuracy and efficiency of RCD, this article then proposes the
Topology-Aware-RCD (TA-RCD) that incorporates the end-to-end application
service topology in RCD. The evaluation of the failure injection studies shows
that the proposed approach performs at least 2X times better on average than
the state-of-the-art RCD algorithm regarding Top-3 and Top-5 recall.

</details>


### [203] [Optimizing Cloud-native Services with SAGA: A Service Affinity Graph-based Approach](https://arxiv.org/abs/2509.05790)
*Hai Dinh-Tuan,Franz Florian Six*

Main category: cs.PF

TL;DR: 本文针对云原生、模块化和基于微服务的软件架构在服务优化中的挑战，提出基于服务亲和图的方法，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现代软件架构在服务优化，尤其是跨动态分布式服务维护端到端服务质量方面面临复杂挑战。

Method: 提出基于服务亲和图的方法，用图模型建模微服务交互，将服务放置问题转化为最小权重k - 割问题，用近似算法进行服务聚类，并通过考虑多目标的概念框架实现。

Result: 通过在Kubernetes集群上部署原型进行实证评估，平均延迟改善23.40%。

Conclusion: 全面讨论了所提方法的各方面，包括影响、挑战和益处，证明了方法的有效性。

Abstract: Modern software architectures are characterized by their cloud-native,
modular, and microservice-based designs. While these systems are known for
their efficiency, they also face complex challenges in service optimization,
especially in maintaining end-to-end quality of service across dynamically
distributed services. This paper introduces a novel approach using the concept
of Service Affinity to address this challenge. The proposed method, termed
Service Affinity Graph-based Approach, employs a graph-based model to model the
interactions among microservices. It formulates the service placement as a
minimum-weight k-cut problem and utilizes an approximation algorithm for
service clustering. This approach is realized through a conceptual framework
that takes into account a wide range of optimization objectives, ranging from
enhancing application performance and enforcing data privacy to optimizing
operational costs. In addition to presenting the SAGA framework in details,
this paper conducts an in-depth empirical evaluation using a prototype deployed
on a Kubernetes cluster. The results demonstrate a mean latency improvement of
23.40%, validating the effectiveness of our approach. Finally, the paper
comprehensively discusses various aspects of the proposed methods, including
their implications, challenges, and benefits, providing a thorough analysis of
the approach's impact.

</details>


### [204] [Optimizing Stateful Microservice Migration in Kubernetes with MS2M and Forensic Checkpointing](https://arxiv.org/abs/2509.05794)
*Hai Dinh-Tuan,Jialun Jiang*

Main category: cs.PF

TL;DR: 本文提出优化的Kubernetes有状态服务迁移方案，评估显示可显著减少停机时间，为云原生环境提供策略。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统广泛采用微服务架构，有状态微服务迁移复杂，多数容器编排器缺乏原生支持。

Method: 将基于消息的有状态微服务迁移（MS2M）框架与Kubernetes的取证容器检查点（FCC）功能集成，支持迁移StatefulSet管理的Pod，并引入基于阈值的截断机制。

Result: 与冷迁移方法相比，单个Pod的MS2M将停机时间减少96.986%，StatefulSet方法在管理有状态服务方面提供更大灵活性。

Conclusion: 为云原生环境中有状态微服务迁移优化提供实用策略。

Abstract: The widespread adoption of microservices architecture in modern software
systems has emphasized the need for efficient management of distributed
services. While stateless microservices enable straightforward migration,
stateful microservices introduce added complexity due to the need to preserve
in-memory state during migration. However, most container orchestrators,
including Kubernetes, lack native support for live stateful service migration.
This paper proposes an optimized migration scheme for stateful services in
Kubernetes by integrating the Message-based Stateful Microservice Migration
(MS2M) framework with Kubernetes' Forensic Container Checkpointing (FCC)
feature. Key enhancements include support for migrating StatefulSet-managed
Pods and the introduction of a Threshold-Based Cutoff Mechanism to handle high
incoming message rates. Evaluation results demonstrate that MS2M for individual
Pods reduces downtime by 96.986% compared to cold migration methods, while the
StatefulSet approach provides greater flexibility in managing stateful
services. These insights provide practical strategies for optimizing stateful
microservice migration in cloud-native environments.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [205] [Adversarial Bug Reports as a Security Risk in Language Model-Based Automated Program Repair](https://arxiv.org/abs/2509.05372)
*Piotr Przymus,Andreas Happe,Jürgen Cito*

Main category: cs.SE

TL;DR: 本文研究基于大语言模型的自动程序修复（APR）系统面临的对抗性漏洞报告攻击风险，发现现有防御不足，还提出自动化生成对抗性漏洞报告的框架并给出改进建议。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的APR系统依赖不可信用户输入，存在新的未被充分探索的攻击面，研究对抗性漏洞报告带来的安全风险。

Method: 开发综合威胁模型，生成51个不同策略的对抗性漏洞报告，测试领先APR模型，评估修复前防御和修复后检测器。

Result: 现有防御不足，90%的漏洞报告触发攻击者期望的补丁，最佳修复前过滤器仅阻止47%，修复后分析仅58%有效。

Conclusion: 给出提高APR系统对抗滥用鲁棒性的实用建议，指出可信自动修复未来工作方向。

Abstract: Large Language Model (LLM) - based Automated Program Repair (APR) systems are
increasingly integrated into modern software development workflows, offering
automated patches in response to natural language bug reports. However, this
reliance on untrusted user input introduces a novel and underexplored attack
surface. In this paper, we investigate the security risks posed by adversarial
bug reports -- realistic-looking issue submissions crafted to mislead APR
systems into producing insecure or harmful code changes. We develop a
comprehensive threat model and conduct an empirical study to evaluate the
vulnerability of state-of-the-art APR systems to such attacks. Our
demonstration comprises 51 adversarial bug reports generated across a spectrum
of strategies, from manual curation to fully automated pipelines. We test these
against leading APR model and assess both pre-repair defenses (e.g., LlamaGuard
variants, PromptGuard variants, Granite-Guardian, and custom LLM filters) and
post-repair detectors (GitHub Copilot, CodeQL). Our findings show that current
defenses are insufficient: 90\% of crafted bug reports triggered
attacker-aligned patches. The best pre-repair filter blocked only 47\%, while
post-repair analysis-often requiring human oversight-was effective in just 58\%
of cases. To support scalable security testing, we introduce a prototype
framework for automating the generation of adversarial bug reports. Our
analysis exposes a structural asymmetry: generating adversarial inputs is
inexpensive, while detecting or mitigating them remains costly and error-prone.
We conclude with practical recommendations for improving the robustness of APR
systems against adversarial misuse and highlight directions for future work on
trustworthy automated repair.

</details>


### [206] [Reverse Browser: Vector-Image-to-Code Generator](https://arxiv.org/abs/2509.05394)
*Zoltan Toth-Czifra*

Main category: cs.SE

TL;DR: 本文以不同方法解决界面设计转代码问题，用矢量图作输入，创建数据集，评估IQA算法并引入新指标，训练模型并讨论局限。


<details>
  <summary>Details</summary>
Motivation: 现有图像转代码方案对原设计保真度不高。

Method: 使用矢量图代替位图作为模型输入；创建多个大型数据集训练机器学习模型；评估IQA算法并引入新的多尺度指标。

Result: 训练了一个大的开放权重模型。

Conclusion: 讨论了所训练模型的局限性。

Abstract: Automating the conversion of user interface design into code (image-to-code
or image-to-UI) is an active area of software engineering research. However,
the state-of-the-art solutions do not achieve high fidelity to the original
design, as evidenced by benchmarks. In this work, I approach the problem
differently: I use vector images instead of bitmaps as model input. I create
several large datasets for training machine learning models. I evaluate the
available array of Image Quality Assessment (IQA) algorithms and introduce a
new, multi-scale metric. I then train a large open-weights model and discuss
its limitations.

</details>


### [207] [Combining TSL and LLM to Automate REST API Testing: A Comparative Study](https://arxiv.org/abs/2509.05540)
*Thiago Barradas,Aline Paes,Vânia de Oliveira Neves*

Main category: cs.SE

TL;DR: 为解决REST API测试难题，提出RestTSLLM方法，用TSL和LLM自动生成测试用例，评估多个LLM，Claude 3.5 Sonnet表现最佳，凸显LLM自动生成测试的潜力。


<details>
  <summary>Details</summary>
Motivation: 分布式系统复杂、场景多、测试设计时间有限，全面测试不实际，导致未检测到的故障、高人工成本和有限的测试覆盖率。

Method: 引入RestTSLLM方法，结合测试规范语言（TSL）和大语言模型（LLM），将提示工程技术与自动化管道集成，从OpenAPI规范生成测试，用成功率、测试覆盖率和变异分数等指标评估LLM。

Result: Claude 3.5 Sonnet、Deepseek R1、Qwen 2.5 32b和Sabia 3能生成稳健且上下文连贯的测试，Claude 3.5 Sonnet在各指标上表现最优。

Conclusion: 大语言模型有基于API规范自动生成测试的潜力。

Abstract: The effective execution of tests for REST APIs remains a considerable
challenge for development teams, driven by the inherent complexity of
distributed systems, the multitude of possible scenarios, and the limited time
available for test design. Exhaustive testing of all input combinations is
impractical, often resulting in undetected failures, high manual effort, and
limited test coverage. To address these issues, we introduce RestTSLLM, an
approach that uses Test Specification Language (TSL) in conjunction with Large
Language Models (LLMs) to automate the generation of test cases for REST APIs.
The approach targets two core challenges: the creation of test scenarios and
the definition of appropriate input data. The proposed solution integrates
prompt engineering techniques with an automated pipeline to evaluate various
LLMs on their ability to generate tests from OpenAPI specifications. The
evaluation focused on metrics such as success rate, test coverage, and mutation
score, enabling a systematic comparison of model performance. The results
indicate that the best-performing LLMs - Claude 3.5 Sonnet (Anthropic),
Deepseek R1 (Deepseek), Qwen 2.5 32b (Alibaba), and Sabia 3 (Maritaca) -
consistently produced robust and contextually coherent REST API tests. Among
them, Claude 3.5 Sonnet outperformed all other models across every metric,
emerging in this study as the most suitable model for this task. These findings
highlight the potential of LLMs to automate the generation of tests based on
API specifications.

</details>


### [208] [Natural Language-Programming Language Software Traceability Link Recovery Needs More than Textual Similarity](https://arxiv.org/abs/2509.05585)
*Zhiyuan Zou,Bangchao Wang,Peng Liang,Tingting Bi,Huan Jin*

Main category: cs.SE

TL;DR: 论文指出软件追溯链接恢复（TLR）中仅依赖文本相似度在NL - PL场景有局限，提出结合多领域辅助策略的方法，实验表明该方法提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 软件追溯链接恢复（TLR）任务中，仅依靠文本相似度在自然语言和编程语言（NL - PL）场景受语义差距限制，需解决此问题。

Method: 通过大规模实证评估揭示文本相似度局限，将实证分析确定的多领域辅助策略融入Heterogeneous Graph Transformer（HGT）和基于提示的Gemini 2.5 Pro两个模型。

Result: 多策略的HGT和Gemini 2.5 Pro模型均优于未集成策略的原模型，与当前最先进方法HGNNLink相比，在十二个开源项目上平均F1分数分别提高3.68%和8.84%。

Conclusion: 多策略集成能有效提升需求 - 代码TLR任务的整体模型性能。

Abstract: In the field of software traceability link recovery (TLR), textual similarity
has long been regarded as the core criterion. However, in tasks involving
natural language and programming language (NL-PL) artifacts, relying solely on
textual similarity is limited by their semantic gap. To this end, we conducted
a large-scale empirical evaluation across various types of TLR tasks, revealing
the limitations of textual similarity in NL-PL scenarios. To address these
limitations, we propose an approach that incorporates multiple domain-specific
auxiliary strategies, identified through empirical analysis, into two models:
the Heterogeneous Graph Transformer (HGT) via edge types and the prompt-based
Gemini 2.5 Pro via additional input information. We then evaluated our approach
using the widely studied requirements-to-code TLR task, a representative case
of NL-PL TLR. Experimental results show that both the multi-strategy HGT and
Gemini 2.5 Pro models outperformed their original counterparts without strategy
integration. Furthermore, compared to the current state-of-the-art method
HGNNLink, the multi-strategy HGT and Gemini 2.5 Pro models achieved average
F1-score improvements of 3.68% and 8.84%, respectively, across twelve
open-source projects, demonstrating the effectiveness of multi-strategy
integration in enhancing overall model performance for the requirements-code
TLR task.

</details>


### [209] [Verifying Correctness of PLC Software during System Evolution using Model Containment Approach](https://arxiv.org/abs/2509.05596)
*Soumyadip Bandyopadhyay,Santonu Sarkar*

Main category: cs.SE

TL;DR: 本文提出基于验证的方法确保PLC软件升级后功能正确性，将SFC转换为Petri网模型，用新算法验证包含关系，实验证明其可扩展性和有效性，性能比verifAPS提高近4倍。


<details>
  <summary>Details</summary>
Motivation: 可编程逻辑控制器（PLC）软件升级常见，但验证升级正确性是重大挑战，需确保升级后现有功能的正确性。

Method: 将新旧版本的顺序功能图（SFC）转换为两个Petri网模型，基于符号路径等价的新包含检查算法验证一个模型是否包含另一个，开发了基于Petri网的包含检查器。

Result: 在OSCAT库的80个真实基准测试中，证明了框架的可扩展性和有效性，与verifAPS相比性能提高近4倍。

Conclusion: 所提出的基于验证的方法能有效确保PLC软件升级后现有功能的正确性，且具有良好性能。

Abstract: Upgradation of Programmable Logic Controller (PLC) software is quite common
to accommodate evolving industrial requirements. Verifying the correctness of
such upgrades remains a significant challenge. In this paper, we propose a
verification-based approach to ensure the correctness of the existing
functionality in the upgraded version of a PLC software. The method converts
the older and the newer versions of the sequential function chart (SFC) into
two Petri net models. We then verify whether one model is contained within
another, based on a novel containment checking algorithm grounded in symbolic
path equivalence. For this purpose, we have developed a home-grown Petri
net-based containment checker. Experimental evaluation on 80 real-world
benchmarks from the OSCAT library highlights the scalability and effectiveness
of the framework. We have compared our approach with verifAPS, a popular tool
used for software upgradation, and observed nearly 4x performance improvement.

</details>


### [210] [Automating API Documentation with LLMs: A BERTopic Approach](https://arxiv.org/abs/2509.05749)
*AmirHossein Naghshzan*

Main category: cs.SE

TL;DR: 提出自动化总结非官方API信息，聚焦安卓API，用BERTopic提取主题并生成摘要，用户研究显示提升生产力，整合信息可增强文档可用性。


<details>
  <summary>Details</summary>
Motivation: 官方API文档冗长、复杂或不完整，开发者常转向社区论坛获取实用信息，需自动化总结非官方来源。

Method: 使用BERTopic从360万条Stack Overflow帖子中提取主题，应用提取式总结技术生成含代码片段的摘要，进行30名安卓开发者的用户研究。

Result: 用户研究表明生成的摘要在连贯性、相关性、信息性和满意度方面表现良好，提升了开发者的生产力。

Conclusion: 将正式API知识与社区生成内容整合可增强API文档，使API资源更易获取和操作。

Abstract: Developers rely on API documentation, but official sources are often lengthy,
complex, or incomplete. Many turn to community-driven forums like Stack
Overflow for practical insights. We propose automating the summarization of
informal sources, focusing on Android APIs. Using BERTopic, we extracted
prevalent topics from 3.6 million Stack Overflow posts and applied extractive
summarization techniques to generate concise summaries, including code
snippets. A user study with 30 Android developers assessed the summaries for
coherence, relevance, informativeness, and satisfaction, showing improved
productivity. Integrating formal API knowledge with community-generated content
enhances documentation, making API resources more accessible and actionable
work.

</details>


### [211] [IoT Miner: Intelligent Extraction of Event Logs from Sensor Data for Process Mining](https://arxiv.org/abs/2509.05769)
*Edyta Brzychczy,Urszula Jessen,Krzysztof Kluza,Sridhar Sriram,Manuel Vargas Nettelnstroth*

Main category: cs.SE

TL;DR: 本文提出IoT Miner框架，用四阶段流程从工业传感器原始数据生成事件日志，以支持流程挖掘，在LHD采矿机数据上评估，结果表明丰富提示能带来更准确一致的标签。


<details>
  <summary>Details</summary>
Motivation: 现实中标准事件日志不可用，传感器数据缺乏分析所需的结构和语义，需要方法生成事件日志以支持流程挖掘。

Method: 采用四阶段管道，包括数据预处理、无监督聚类、基于大语言模型的标注和事件日志构建，用领域特定提示引导大语言模型根据聚类统计生成活动标签。

Result: 在LHD采矿机传感器数据上评估，引入新指标评估标注质量，结果显示更丰富的提示能带来更准确和一致的标签。

Conclusion: IoT Miner结合AI和领域感知的数据处理，为从物联网数据生成事件日志提供了可扩展和可解释的方法，能在传统日志缺失的场景中实现流程挖掘。

Abstract: This paper presents IoT Miner, a novel framework for automatically creating
high-level event logs from raw industrial sensor data to support process
mining. In many real-world settings, such as mining or manufacturing, standard
event logs are unavailable, and sensor data lacks the structure and semantics
needed for analysis. IoT Miner addresses this gap using a four-stage pipeline:
data preprocessing, unsupervised clustering, large language model (LLM)-based
labeling, and event log construction. A key innovation is the use of LLMs to
generate meaningful activity labels from cluster statistics, guided by
domain-specific prompts. We evaluate the approach on sensor data from a
Load-Haul-Dump (LHD) mining machine and introduce a new metric,
Similarity-Weighted Accuracy, to assess labeling quality. Results show that
richer prompts lead to more accurate and consistent labels. By combining AI
with domain-aware data processing, IoT Miner offers a scalable and
interpretable method for generating event logs from IoT data, enabling process
mining in settings where traditional logs are missing.

</details>


### [212] [GeoAnalystBench: A GeoAI benchmark for assessing large language models for spatial analysis workflow and code generation](https://arxiv.org/abs/2509.05881)
*Qianheng Zhang,Song Gao,Chen Wei,Yibo Zhao,Ying Nie,Ziru Chen,Shijie Chen,Yu Su,Huan Sun*

Main category: cs.SE

TL;DR: 本文呼吁在声称实现 GIS 完全自动化前对大语言模型进行严格评估，提出 GeoAnalystBench 基准，评估模型后揭示当前大语言模型在 GIS 自动化中的优缺点。


<details>
  <summary>Details</summary>
Motivation: 大语言模型自动化地理空间分析和 GIS 工作流能力不确定，需严格评估。

Method: 提出 GeoAnalystBench 基准，包含 50 个基于 Python 的真实地理空间任务，由 GIS 专家验证，从多方面评估模型。

Result: 专有模型如 ChatGPT - 4o - mini 表现好，开源小模型如 DeepSeek - R1 - 7B 生成工作流有问题，需深度空间推理的任务对所有模型都有挑战。

Conclusion: 展示了当前大语言模型在 GIS 自动化中的潜力和局限，提供了可复现框架以推动有人在环支持的 GeoAI 研究。

Abstract: Recent advances in large language models (LLMs) have fueled growing interest
in automating geospatial analysis and GIS workflows, yet their actual
capabilities remain uncertain. In this work, we call for rigorous evaluation of
LLMs on well-defined geoprocessing tasks before making claims about full GIS
automation. To this end, we present GeoAnalystBench, a benchmark of 50
Python-based tasks derived from real-world geospatial problems and carefully
validated by GIS experts. Each task is paired with a minimum deliverable
product, and evaluation covers workflow validity, structural alignment,
semantic similarity, and code quality (CodeBLEU). Using this benchmark, we
assess both proprietary and open source models. Results reveal a clear gap:
proprietary models such as ChatGPT-4o-mini achieve high validity 95% and
stronger code alignment (CodeBLEU 0.39), while smaller open source models like
DeepSeek-R1-7B often generate incomplete or inconsistent workflows (48.5%
validity, 0.272 CodeBLEU). Tasks requiring deeper spatial reasoning, such as
spatial relationship detection or optimal site selection, remain the most
challenging across all models. These findings demonstrate both the promise and
limitations of current LLMs in GIS automation and provide a reproducible
framework to advance GeoAI research with human-in-the-loop support.

</details>


### [213] [Efficiently Ranking Software Variants with Minimal Benchmarks](https://arxiv.org/abs/2509.06716)
*Théo Matricon,Mathieu Acher,Helge Spieker,Arnaud Gotlieb*

Main category: cs.SE

TL;DR: 提出BISS方法减少基准测试，在保持排名稳定下降低成本，实验表明该方法优于基线。


<details>
  <summary>Details</summary>
Motivation: 执行基准测试以全面了解软件变体在计算资源和时间方面成本高昂，需减少基准测试。

Method: 提出BISection Sampling（BISS）方法，保留关键测试，采用分治法对相关剩余测试采样。

Result: 实验表明该方法即使在部分变体上也优于基线，平均将基准测试计算成本降至44%，超半数基准测试成本最多降至99%且排名稳定。

Conclusion: BISS方法能有效减少基准测试同时保持排名稳定，降低计算成本。

Abstract: Benchmarking is a common practice in software engineering to assess the
qualities and performance of software variants, coming from multiple competing
systems or from configurations of the same system. Benchmarks are used notably
to compare and understand variant performance, fine-tune software, detect
regressions, or design new software systems. The execution of benchmarks to get
a complete picture of software variants is highly costly in terms of
computational resources and time. In this paper, we propose a novel approach
for reducing benchmarks while maintaining stable rankings, using test suite
optimization techniques. That is, we remove instances from the benchmarks while
trying to keep the same rankings of the variants on all tests. Our method,
BISection Sampling, BISS, strategically retains the most critical tests and
applies a novel divide-and-conquer approach to efficiently sample among
relevant remaining tests. We experiment with datasets and use cases from LLM
leaderboards, SAT competitions, and configurable systems for performance
modeling. Our results show that our method outperforms baselines even when
operating on a subset of variants. Using BISS, we reduce the computational cost
of the benchmarks on average to 44% and on more than half the benchmarks by up
to 99% without loss in ranking stability.

</details>


### [214] [Code2MCP: A Multi-Agent Framework for Automated Transformation of Code Repositories into Model Context Protocol Services](https://arxiv.org/abs/2509.05941)
*Chaoqian Ouyang,Ling Yue,Shimin Di,Libin Zheng,Shaowu Pan,Min-Ling Zhang*

Main category: cs.SE

TL;DR: 论文提出Code2MCP框架，可将GitHub仓库转化为MCP服务，减少人工干预，加速MCP生态系统发展。


<details>
  <summary>Details</summary>
Motivation: 大语言模型集成存在“N×M问题”，MCP标准虽出现但采用时需大量手动工作，尤其是处理GitHub上的开源仓库。

Method: 采用多阶段工作流，从代码分析、环境配置到服务生成和部署全自动化，有LLM驱动的“运行 - 审查 - 修复”闭环循环。

Result: Code2MCP能生成可部署服务和全面技术文档，代码已开源。

Conclusion: Code2MCP可加速MCP生态系统，系统地解锁最大开源代码仓库，自动化工具集成最后关键一步。

Abstract: The proliferation of Large Language Models (LLMs) has created a significant
integration challenge in the AI agent ecosystem, often called the "$N \times M$
problem," where N models require custom integrations for M tools. This
fragmentation stifles innovation and creates substantial development overhead.
While the Model Context Protocol (MCP) has emerged as a standard to resolve
this, its adoption is hindered by the manual effort required to convert the
vast universe of existing software into MCP-compliant services. This is
especially true for the millions of open-source repositories on GitHub, the
world's largest collection of functional code. This paper introduces Code2MCP,
a highly automated, agentic framework designed to transform any GitHub
repository into a functional MCP service with minimal human intervention. Our
system employs a multi-stage workflow that automates the entire process, from
code analysis and environment configuration to service generation and
deployment. A key innovation of our framework is an LLM-driven, closed-loop
"Run--Review--Fix" cycle, which enables the system to autonomously debug and
repair the code it generates. Code2MCP produces not only deployable services
but also comprehensive technical documentation, acting as a catalyst to
accelerate the MCP ecosystem by systematically unlocking the world's largest
open-source code repository and automating the critical last mile of tool
integration. The code is open-sourced at
https://github.com/DEFENSE-SEU/MCP-Github-Agent.

</details>


### [215] [GRACE: Graph-Guided Repository-Aware Code Completion through Hierarchical Code Fusion](https://arxiv.org/abs/2509.05980)
*Xingliang Wang,Baoyi Wang,Chen Zhi,Junxiao Han,Xinkui Zhao,Jianwei Yin,Shuiguang Deng*

Main category: cs.SE

TL;DR: 现有大语言模型在代码库级任务表现不佳，当前检索增强生成方法有局限，GRACE构建代码图、采用混合图检索器和结构融合机制，实验表明其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在代码库级任务中因上下文窗口有限、代码库语义和结构依赖复杂而表现不佳，以及现有检索增强生成方法的局限性问题。

Method: 构建多级别、多语义代码图，采用混合图检索器结合结构和文本检索，用图注意力网络重排器优化，引入结构融合机制。

Result: 在公共代码库级基准测试中显著优于现有方法，使用DeepSeek - V3作为基础大语言模型时，在各数据集上超越最强基于图的检索增强生成基线。

Conclusion: GRACE方法有效，能提升大语言模型在代码库级任务的表现。

Abstract: LLMs excel in localized code completion but struggle with repository-level
tasks due to limited context windows and complex semantic and structural
dependencies across codebases. While Retrieval-Augmented Generation (RAG)
mitigates context scarcity by retrieving relevant code snippets, current
approaches face significant limitations. They overly rely on textual similarity
for retrieval, neglecting structural relationships such as call chains and
inheritance hierarchies, and lose critical structural information by naively
concatenating retrieved snippets into text sequences for LLM input. To address
these shortcomings, GRACE constructs a multi-level, multi-semantic code graph
that unifies file structures, abstract syntax trees, function call graphs,
class hierarchies, and data flow graphs to capture both static and dynamic code
semantics. For retrieval, GRACE employs a Hybrid Graph Retriever that
integrates graph neural network-based structural similarity with textual
retrieval, refined by a graph attention network-based re-ranker to prioritize
topologically relevant subgraphs. To enhance context, GRACE introduces a
structural fusion mechanism that merges retrieved subgraphs with the local code
context and preserves essential dependencies like function calls and
inheritance. Extensive experiments on public repository-level benchmarks
demonstrate that GRACE significantly outperforms state-of-the-art methods
across all metrics. Using DeepSeek-V3 as the backbone LLM, GRACE surpasses the
strongest graph-based RAG baselines by 8.19% EM and 7.51% ES points on every
dataset. The code is available at
https://anonymous.4open.science/r/grace_icse-C3D5.

</details>


### [216] [Students' Perception of LLM Use in Requirements Engineering Education: An Empirical Study Across Two Universities](https://arxiv.org/abs/2509.05995)
*Sharon Guardado,Risha Parveen,Zheying Zhang,Maruf Rayhan,Nirnaya Tripathi*

Main category: cs.SE

TL;DR: 研究实证评估大语言模型（LLMs）融入需求工程（RE）课程的影响，发现其提升学生对RE概念的理解，但也有担忧，还给出整合建议和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探究将LLMs融入RE教育以提升学生参与度、动力和为其职业发展提供工具的效果。

Method: 收集两所大学两门RE课程共179名学生的调查数据，通过个人作业和团队敏捷项目两种教学形式将LLMs融入课程。

Result: LLMs提升学生对RE概念的理解，尤其是需求获取和文档记录任务；学生对LLMs在教育中的应用存在担忧；个人作业的学生受益更多。

Conclusion: 给出LLMs在RE教育中有效整合的建议，提出平衡AI辅助学习与批判性思维、协作实践的未来研究方向。

Abstract: The integration of Large Language Models (LLMs) in Requirements Engineering
(RE) education is reshaping pedagogical approaches, seeking to enhance student
engagement and motivation while providing practical tools to support their
professional future. This study empirically evaluates the impact of integrating
LLMs in RE coursework. We examined how the guided use of LLMs influenced
students' learning experiences, and what benefits and challenges they perceived
in using LLMs in RE practices. The study collected survey data from 179
students across two RE courses in two universities. LLMs were integrated into
coursework through different instructional formats, i.e., individual
assignments versus a team-based Agile project. Our findings indicate that LLMs
improved students' comprehension of RE concepts, particularly in tasks like
requirements elicitation and documentation. However, students raised concerns
about LLMs in education, including academic integrity, overreliance on AI, and
challenges in integrating AI-generated content into assignments. Students who
worked on individual assignments perceived that they benefited more than those
who worked on team-based assignments, highlighting the importance of contextual
AI integration. This study offers recommendations for the effective integration
of LLMs in RE education. It proposes future research directions for balancing
AI-assisted learning with critical thinking and collaborative practices in RE
courses.

</details>


### [217] [A Rapid Review Regarding the Concept of Legal Requirements in Requirements Engineering](https://arxiv.org/abs/2509.06012)
*Jukka Ruohonen*

Main category: cs.SE

TL;DR: 文章对需求工程研究中法律要求（LRs）概念进行快速回顾，指出存在定义和概念操作化缺失、知识差距等问题。


<details>
  <summary>Details</summary>
Motivation: 源于个人困惑、同行评审意见和现有文献的混乱，对需求工程研究中法律要求概念进行回顾。

Method: 对相关文献进行快速回顾。

Result: 发现对LRs有规范理解但缺乏定义和操作化，有不同分类，其特点是模糊复杂、常变化重叠、可优先级排序，且实施情况不佳。

Conclusion: 回顾提出关于明显知识差距的批判性论点，包括缺乏实证支持和概念混淆。

Abstract: Out of a personal puzzlement, recent peer review comments, and demonstrable
confusion in the existing literature, the paper presents a rapid review of the
concept of legal requirements (LRs) in requirements engineering (RE) research.
According to reviewing results, a normative understanding of LRs has often been
present, although proper definitions and conceptual operationalizations are
lacking. Some papers also see LRs as functional and others as non-functional
requirements. Legal requirements are often characterized as being vague and
complex, requiring a lot of effort to elicit, implement, and validate. These
characterizations supposedly correlate with knowledge gaps among requirements
engineers. LRs are also seen to often change and overlap. They may be also
prioritized. According to the literature, they seem to be also reluctantly
implemented, often providing only a minimal baseline for other requirements.
With these and other observations, the review raises critical arguments about
apparent knowledge gaps, including a lack of empirical evidence backing the
observations and enduring conceptual confusion.

</details>


### [218] [Empirical Study of Code Large Language Models for Binary Security Patch Detection](https://arxiv.org/abs/2509.06052)
*Qingyuan Li,Binchang Li,Cuiyun Gao,Shuzheng Gao,Zongjie Li*

Main category: cs.SE

TL;DR: 本文构建大规模二进制补丁数据集评估19个代码大语言模型在二进制安全补丁检测任务的能力，发现微调后的模型表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的安全补丁检测方法无法用于闭源应用和专有系统，代码大语言模型在二进制安全补丁检测的潜力未被探索。

Method: 构建含19,448个样本的大规模二进制补丁数据集，用汇编代码和伪代码两种表示，评估19个不同规模的代码大语言模型，研究微调策略注入领域知识。

Result: 直接提示原始代码大语言模型难以准确识别安全补丁，微调后的模型表现出色，伪代码表示的结果最佳。

Conclusion: 代码大语言模型经微调可在二进制安全补丁检测任务中取得优异性能。

Abstract: Security patch detection (SPD) is crucial for maintaining software security,
as unpatched vulnerabilities can lead to severe security risks. In recent
years, numerous learning-based SPD approaches have demonstrated promising
results on source code. However, these approaches typically cannot be applied
to closed-source applications and proprietary systems that constitute a
significant portion of real-world software, as they release patches only with
binary files, and the source code is inaccessible. Given the impressive
performance of code large language models (LLMs) in code intelligence and
binary analysis tasks such as decompilation and compilation optimization, their
potential for detecting binary security patches remains unexplored, exposing a
significant research gap between their demonstrated low-level code
understanding capabilities and this critical security task. To address this
gap, we construct a large-scale binary patch dataset containing \textbf{19,448}
samples, with two levels of representation: assembly code and pseudo-code, and
systematically evaluate \textbf{19} code LLMs of varying scales to investigate
their capability in binary SPD tasks. Our initial exploration demonstrates that
directly prompting vanilla code LLMs struggles to accurately identify security
patches from binary patches, and even state-of-the-art prompting techniques
fail to mitigate the lack of domain knowledge in binary SPD within vanilla
models. Drawing on the initial findings, we further investigate the fine-tuning
strategy for injecting binary SPD domain knowledge into code LLMs through two
levels of representation. Experimental results demonstrate that fine-tuned LLMs
achieve outstanding performance, with the best results obtained on the
pseudo-code representation.

</details>


### [219] [Software Dependencies 2.0: An Empirical Study of Reuse and Integration of Pre-Trained Models in Open-Source Projects](https://arxiv.org/abs/2509.06085)
*Jerin Yasmin,Wenxin Jiang,James C. Davis,Yuan Tian*

Main category: cs.SE

TL;DR: 本文研究开源软件项目中预训练模型作为软件依赖的使用情况，通过对401个GitHub仓库的混合方法分析，探讨开发者如何管理和集成这些模型。


<details>
  <summary>Details</summary>
Motivation: 预训练模型广泛应用带来新的软件依赖问题，其在实际项目中的集成情况不明，可能威胁软件系统的可维护性和可靠性。

Method: 对PeaTMOSS数据集中401个GitHub仓库进行混合方法分析，定量识别模式，定性研究开发者实际集成和管理模型的方式。

Result: 文中未提及具体结果。

Conclusion: 文中未提及具体结论。

Abstract: Pre-trained models (PTMs) are machine learning models that have been trained
in advance, often on large-scale data, and can be reused for new tasks, thereby
reducing the need for costly training from scratch. Their widespread adoption
introduces a new class of software dependency, which we term Software
Dependencies 2.0, extending beyond conventional libraries to learned behaviors
embodied in trained models and their associated artifacts. The integration of
PTMs as software dependencies in real projects remains unclear, potentially
threatening maintainability and reliability of modern software systems that
increasingly rely on them. Objective: In this study, we investigate Software
Dependencies 2.0 in open-source software (OSS) projects by examining the reuse
of PTMs, with a focus on how developers manage and integrate these models.
Specifically, we seek to understand: (1) how OSS projects structure and
document their PTM dependencies; (2) what stages and organizational patterns
emerge in the reuse pipelines of PTMs within these projects; and (3) the
interactions among PTMs and other learned components across pipeline stages. We
conduct a mixed-methods analysis of a statistically significant random sample
of 401 GitHub repositories from the PeaTMOSS dataset (28,575 repositories
reusing PTMs from Hugging Face and PyTorch Hub). We quantitatively examine PTM
reuse by identifying patterns and qualitatively investigate how developers
integrate and manage these models in practice.

</details>


### [220] [Agentic Software Engineering: Foundational Pillars and a Research Roadmap](https://arxiv.org/abs/2509.06216)
*Ahmed E. Hassan,Hao Li,Dayi Lin,Bram Adams,Tse-Hsun Chen,Yutaro Kashiwa,Dong Qiu*

Main category: cs.SE

TL;DR: 本文提出结构化代理软件工程（SASE）愿景，介绍两种工作台，给出研究路线图，推动SE领域向代理时代发展。


<details>
  <summary>Details</summary>
Motivation: 在代理软件工程时代，利用智能代理新能力并确保可信性，需重新构想软件工程基础支柱。

Method: 提出Agent Command Environment (ACE)和Agent Execution Environment (AEE)两个工作台支持愿景。

Result: 形成双向合作，产生新的结构化工程活动，重新定义人机协作。

Conclusion: 提供概念框架和结构化词汇，激发社区对话，推动SE领域向代理未来发展。

Abstract: Agentic Software Engineering (SE 3.0) represents a new era where intelligent
agents are tasked not with simple code generation, but with achieving complex,
goal-oriented SE objectives. To harness these new capabilities while ensuring
trustworthiness, we must recognize a fundamental duality within the SE field in
the Agentic SE era, comprising two symbiotic modalities: SE for Humans and SE
for Agents. This duality demands a radical reimagining of the foundational
pillars of SE (actors, processes, tools, and artifacts) which manifest
differently across each modality. We propose two purpose-built workbenches to
support this vision. The Agent Command Environment (ACE) serves as a command
center where humans orchestrate and mentor agent teams, handling outputs such
as Merge-Readiness Packs (MRPs) and Consultation Request Packs (CRPs). The
Agent Execution Environment (AEE) is a digital workspace where agents perform
tasks while invoking human expertise when facing ambiguity or complex
trade-offs. This bi-directional partnership, which supports agent-initiated
human callbacks and handovers, gives rise to new, structured engineering
activities (i.e., processes) that redefine human-AI collaboration, elevating
the practice from agentic coding to true agentic software engineering. This
paper presents the Structured Agentic Software Engineering (SASE) vision,
outlining several of the foundational pillars for the future of SE. The paper
culminates in a research roadmap that identifies a few key challenges and
opportunities while briefly discussing the resulting impact of this future on
SE education. Our goal is not to offer a definitive solution, but to provide a
conceptual scaffold with structured vocabulary to catalyze a community-wide
dialogue, pushing the SE community to think beyond its classic, human-centric
tenets toward a disciplined, scalable, and trustworthy agentic future.

</details>


### [221] [Learning From Software Failures: A Case Study at a National Space Research Center](https://arxiv.org/abs/2509.06301)
*Dharun Anandayuvaraj,Zain Hammadeh,Andreas Lund,Alexandra Holloway,James C. Davis*

Main category: cs.SE

TL;DR: 文章指出从软件失败中学习很重要但缺乏行业视角，通过对航天研究中心和其他高可靠性组织的工程师访谈，发现失败学习不规范、存在反复失败及诸多挑战，为改进失败管理提供指导。


<details>
  <summary>Details</summary>
Motivation: 软件失败后果严重，高可靠性组织需持续学习，但缺乏从软件失败中学习的行业视角，因此开展研究。

Method: 对国家航天研究中心的10位研究软件工程师进行深度访谈，并结合其他高可靠性组织的5份访谈数据。

Result: 失败学习不规范，未有效融入软件开发生命周期；因缺乏结构化流程，反复失败持续存在；存在时间限制、人员流动导致知识流失、文档碎片化和流程执行不力等挑战。

Conclusion: 加深了对软件工程师从失败中学习方式的理解，为改进失败管理实践提供了指导。

Abstract: Software failures can have significant consequences, making learning from
failures a critical aspect of software engineering. While software
organizations are recommended to conduct postmortems, the effectiveness and
adoption of these practices vary widely. Understanding how engineers gather,
document, share, and apply lessons from failures is essential for improving
reliability and preventing recurrence. High-reliability organizations (HROs)
often develop software systems where failures carry catastrophic risks,
requiring continuous learning to ensure reliability. These organizations
provide a valuable setting to examine practices and challenges for learning
from software failures. Such insight could help develop processes and tools to
improve reliability and prevent recurrence. However, we lack in-depth industry
perspectives on the practices and challenges of learning from failures.
  To address this gap, we conducted a case study through 10 in-depth interviews
with research software engineers at a national space research center. We
examine how they learn from failures: how they gather, document, share, and
apply lessons. To assess transferability, we include data from 5 additional
interviews at other HROs. Our findings provide insight into how engineers learn
from failures in practice. To summarize: (1) failure learning is informal, ad
hoc, and inconsistently integrated into SDLC; (2) recurring failures persist
due to absence of structured processes; and (3) key challenges, including time
constraints, knowledge loss from turnover and fragmented documentation, and
weak process enforcement, undermine systematic learning. Our findings deepen
understanding of how software engineers learn from failures and offer guidance
for improving failure management practices.

</details>


### [222] [A Generic and Efficient Python Runtime Verification System and its Large-scale Evaluation](https://arxiv.org/abs/2509.06324)
*Zhuohang Shen,Mohammed Yaseen,Denini Silva,Kevin Guan,Junho Lee,Marcelo d'Amorim,Owolabi Legunsen*

Main category: cs.SE

TL;DR: 提出用于Python的通用、可扩展且高效的运行时验证系统PyMOP，进行测试并展示其优势。


<details>
  <summary>Details</summary>
Motivation: Python生态系统需要高效的运行时验证系统，而现有的Python RV系统存在局限。

Method: 提出PyMOP，支持多种逻辑、算法、API规格和插桩策略，用户可扩展。

Result: 发现Java默认监测算法对Python并非最快；PyMOP比两个动态分析系统快达11168.3倍；已发现的121个bug中有44个被开发者修复。

Conclusion: PyMOP的通用性和高效性使其成为Python运行时验证领域进一步发展的优秀平台。

Abstract: Runtime verification (RV) now scales for testing thousands of open-source
Java projects, helping find hundreds of bugs. The popular Python ecosystem
could use such benefits. But, today's Python RV systems are limited to a domain
or specification logic, or slow. We propose PyMOP, a generic, extensible, and
efficient RV system for Python. PyMOP supports five logics, implements five
existing monitoring algorithms, ships with 73 API specs of Python and
widely-used libraries, supports three instrumentation strategies, and users can
easily add more of these. On 290,133 unit tests in 1,463 GitHub projects, we
find mainly that (i) the default monitoring algorithm for Java is often not the
fastest for Python; (ii) PyMOP is up to 1,168.3x faster than two recent dynamic
analysis systems; and (iii) 44 of 121 bugs that PyMOP helped find so far were
fixed by developers. PyMOP's generality and efficiency position it well as an
excellent platform for the next advances on RV for Python.

</details>


### [223] [Analyzing the Instability of Large Language Models in Automated Bug Injection and Correction](https://arxiv.org/abs/2509.06429)
*Mehmet Bilal Er,Nagehan İlhan,Umut Kuran*

Main category: cs.SE

TL;DR: 研究探讨ChatGPT在修复代码漏洞时的不稳定性，通过不同温度设置实验，发现温度升高输出更不稳定，为LLM纠错系统应用提供见解。


<details>
  <summary>Details</summary>
Motivation: LLMs在软件工程任务应用增多但结果不稳定，且修复漏洞的一致性未被充分评估，研究ChatGPT修复代码漏洞的不稳定性。

Method: 用不同错误类型代码样本，设置不同温度（0、0.5、1），每个问题生成三个修复建议，用语法相似度和输出等效率指标评估输出。

Result: 温度升高模型输出更不稳定、多变，高温功能失败率高；语法相似度分析显示高温修复建议结构差异大，低温较相似。

Conclusion: 为LLM纠错系统在软件开发中更一致应用提供方法见解，也对其可靠性存疑。

Abstract: The use of Large Language Models (LLMs) in software engineering tasks is
growing, especially in the areas of bug fixing and code generation.
Nevertheless, these models often yield unstable results; when executed at
different times with the same input, they can generate radically different
code. The consistency of LLMs in bug-fixing tasks has not yet been thoroughly
assessed, despite the fact that this instability has typically been discussed
in the literature in relation to code generation. The purpose of this study is
to look into how unstable an LLM like ChatGPT is when it comes to fixing code
bugs. We examine the structural, syntactic, and functional variations among
several fix recommendations made in response to the same prompt using code
samples with various error types. Additionally, we assess how instability is
affected by the temperature settings (0, 0.5, and 1) used for the model's
deterministic operation. For a total of 20 problems in the experimental
analysis, the model produced three fix suggestions at each temperature value,
comparing nine distinct outputs for each problem. The Syntax Similarity and
Output Equivalence Rate (OER) metrics were used to assess the outputs'
structural and functional consistency. The results demonstrate that the model's
outputs become much more unstable and variable as the temperature rises, with
high temperatures showing especially high rates of functional failure.
According to syntax similarity analyses, the suggested fixes show notable
structural differences at high temperatures but are fairly similar at low
temperatures. The purpose of this study is to provide important methodological
insights into how LLM-based error correction systems can be applied more
consistently in software development processes while also casting doubt on
their dependability.

</details>


### [224] [Modeling in the Design Multiverse](https://arxiv.org/abs/2509.06530)
*Sylvain Guérin,Salvador Martinez,Ciprian Teodorov*

Main category: cs.SE

TL;DR: 本文提出设计多元宇宙概念，以解决当前建模空间无法直接管理设计路径时空变化问题，并介绍用法场景与实现方式。


<details>
  <summary>Details</summary>
Motivation: 现实设计过程中设计路径存在时空变化，当前建模空间无法直接管理，需借助外部工具和方法。

Method: 提出设计多元宇宙概念，整合建模空间中的修订和变体，借助模型联合范式实现。

Result: 实现利益相关者能够无缝追踪、分析和管理设计决策、系统变体及其相互依赖关系。

Conclusion: 设计多元宇宙可有效解决当前建模空间管理设计路径变化的问题。

Abstract: Real-world design processes often involve the evolution and divergence of
design paths (by branching, revising, merging, etc.), especially when multiple
stakeholders or teams operate concurrently and/or explore different
alternatives for complex and heterogeneous systems. Unfortunately, this
variability in time and space can not be directly managed in current modeling
spaces but requires resorting to external tools and methodologies.
  In order to tackle this problem, we introduce the Design Multiverse. The
Design Multiverse aims to integrate in the modeling space a selection of
revisions and variants, representing snapshots of a design state composed of
multiple artifacts. This enables stakeholders to seamlessly trace, analyze, and
manage design decisions, system variants, and their interdependencies.
Concretely, in this paper we present a conceptual definition of the Design
Multiverse, discuss usage scenarios such as model product lines and
model/metamodel co-evolution, and propose an implementation leveraging the
model federation paradigm.

</details>


### [225] [Design and Implementation of a Domain-specific Language for Modelling Evacuation Scenarios Using Eclipse EMG/GMF Tool](https://arxiv.org/abs/2509.06688)
*Heerok Banerjee*

Main category: cs.SE

TL;DR: 本文介绍用于建模疏散场景的DSL语言Bmod，用Eclipse EMF/GMF构建，并与其他建模工具对比。


<details>
  <summary>Details</summary>
Motivation: 当前构建支持图形界面、层次结构和易于实现的建模框架以缩短新手差距方面有大量开发需求，且DSL可解决企业内部依赖和提升业务管理流程。

Method: 使用Eclipse Modelling Framework (EMF) 和Eclipse Graphical Modelling Framework (GMF) 构建Bmod语言，并将Eclipse EMF/GMF与其他建模工具对比。

Result: 构建出可用于建模疏散场景的Bmod语言及对比结果。

Conclusion: Bmod语言在疏散场景建模上可作为一种解决方案，Eclipse EMF/GMF与其他工具在不同方面的表现可帮助选择合适工具。

Abstract: Domain-specific languages (DSLs) play a crucial role in resolving internal
dependencies across enterprises and boosts their upfront business management
processes. Yet, a lot of development is needed to build modelling frameworks
which support graphical interfaces (canvas, pallettes etc.), hierarchical
structures and easy implementation to shorten the gap for novice users. In this
paper, a DSL namely, Bmod is introduced, which can be used to model evacuation
scenarios. The language is built using Eclipse Modelling Framework (EMF) and
Eclipse Graphical Modelling Framework (GMF). Furthermore, a comparison is also
shown between Eclipse EMF/GMF and other modelling tools such as AToMPM,
metaDepth, Sirius etc with respect to expressiveness, learning curve and
performance.

</details>


### [226] [OpenCoderRank: AI-Driven Technical Assessments Made Easy](https://arxiv.org/abs/2509.06774)
*Hridoy Sankar Dutta,Sana Ansari,Swati Kumari,Shounak Ravi Bhalerao*

Main category: cs.SE

TL;DR: 本文介绍OpenCoderRank平台，可模拟技术评估，连接出题者与解题者，为资源受限环境提供免费可定制的技术评估解决方案。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型时代，LLMs虽能帮助出题者出题，但会破坏解题者评估的完整性，需要一个合适的技术评估平台。

Method: 引入OpenCoderRank平台，模拟技术评估。

Result: 建立了一个可连接出题者和解题者的平台。

Conclusion: OpenCoderRank是一个易用平台，能帮助解题者应对考试，让出题者自托管评估，适用于资源受限环境。

Abstract: Organizations and educational institutions use time-bound assessment tasks to
evaluate coding and problem-solving skills. These assessments measure not only
the correctness of the solutions, but also their efficiency. Problem setters
(educator/interviewer) are responsible for crafting these challenges, carefully
balancing difficulty and relevance to create meaningful evaluation experiences.
Conversely, problem solvers (student/interviewee) apply coding efficiency and
logical thinking to arrive at correct solutions. In the era of Large Language
Models (LLMs), LLMs assist problem setters in generating diverse and
challenging questions, but they can undermine assessment integrity for problem
solvers by providing easy access to solutions. This paper introduces
OpenCoderRank, an easy-to-use platform designed to simulate technical
assessments. It acts as a bridge between problem setters and problem solvers,
helping solvers prepare for time constraints and unfamiliar problems while
allowing setters to self-host assessments, offering a no-cost and customizable
solution for technical assessments in resource-constrained environments.

</details>


### [227] [Hypergraph-Guided Regex Filter Synthesis for Event-Based Anomaly Detection](https://arxiv.org/abs/2509.06911)
*Margarida Ferreira,Victor Nicolet,Luan Pham,Joey Dodds,Daniel Kroening,Ines Lynce,Ruben Martins*

Main category: cs.SE

TL;DR: 提出HyGLAD算法构建可解释模式检测事件异常，实验显示其优于深度学习方法且效率更高。


<details>
  <summary>Details</summary>
Motivation: 开发可自动构建可解释模式来检测静态系统中基于事件异常的算法，以发现可能的恶意活动。

Method: 推断具有相似行为实体的等价类，构建捕获实体值的正则表达式。

Result: 在五个真实系统数据集上，HyGLAD平均表现优于现有深度学习方法，训练和推理效率高一个数量级，与次优基线相比，精度提高1.2倍，召回率提高1.3倍。

Conclusion: HyGLAD是一种有效且高效的事件异常检测算法，相比深度学习方法有显著优势。

Abstract: We propose HyGLAD, a novel algorithm that automatically builds a set of
interpretable patterns that model event data. These patterns can then be used
to detect event-based anomalies in a stationary system, where any deviation
from past behavior may indicate malicious activity. The algorithm infers
equivalence classes of entities with similar behavior observed from the events,
and then builds regular expressions that capture the values of those entities.
As opposed to deep-learning approaches, the regular expressions are directly
interpretable, which also translates to interpretable anomalies. We evaluate
HyGLAD against all 7 unsupervised anomaly detection methods from DeepOD on five
datasets from real-world systems. The experimental results show that on average
HyGLAD outperforms existing deep-learning methods while being an order of
magnitude more efficient in training and inference (single CPU vs GPU).
Precision improved by 1.2x and recall by 1.3x compared to the second-best
baseline.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [228] [Deep Learning Option Pricing with Market Implied Volatility Surfaces](https://arxiv.org/abs/2509.05911)
*Lijie Ding,Egang Lu,Kin Cheung*

Main category: q-fin.CP

TL;DR: 提出基于市场隐含波动率曲面的深度学习框架为期权定价，模型训练后准确率高，是传统数值方法的替代方案。


<details>
  <summary>Details</summary>
Motivation: 寻找一种高效、可扩展的期权定价方法，替代传统数值方法。

Method: 利用2018 - 2023年标普500指数期权报价构建无套利波动率曲面并生成训练数据，用变分自编码器（VAE）压缩波动率曲面，结合期权特定输入，输入多层感知器预测期权价格，分阶段训练模型。

Result: 训练后的定价器在美式和亚式期权上准确率高，预测误差主要集中在长期限和平价期权，只需单次神经网络前向传播，且随数据增加性能自然提升。

Conclusion: 该方法在统一框架下桥接了波动率曲面建模和期权定价，为奇异期权定价提供了快速灵活的替代方案。

Abstract: We present a deep learning framework for pricing options based on
market-implied volatility surfaces. Using end-of-day S\&P 500 index options
quotes from 2018-2023, we construct arbitrage-free volatility surfaces and
generate training data for American puts and arithmetic Asian options using
QuantLib. To address the high dimensionality of volatility surfaces, we employ
a variational autoencoder (VAE) that compresses volatility surfaces across
maturities and strikes into a 10-dimensional latent representation. We feed
these latent variables, combined with option-specific inputs such as strike and
maturity, into a multilayer perceptron to predict option prices. Our model is
trained in stages: first to train the VAE for volatility surface compression
and reconstruction, then options pricing mapping, and finally fine-tune the
entire network end-to-end. The trained pricer achieves high accuracy across
American and Asian options, with prediction errors concentrated primarily near
long maturities and at-the-money strikes, where absolute bid-ask price
differences are known to be large. Our method offers an efficient and scalable
approach requiring only a single neural network forward pass and naturally
improve with additional data. By bridging volatility surface modeling and
option pricing in a unified framework, it provides a fast and flexible
alternative to traditional numerical approaches for exotic options.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [229] [Design and hedging of unit linked life insurance with environmental factors](https://arxiv.org/abs/2509.05676)
*Katia Colaneri,Alessandra Cretarola,Edoardo Lombardo,Daniele Mancinelli*

Main category: q-fin.RM

TL;DR: 研究含环境标准投资基金的单位连结型寿险保单设计与套期保值，分别解决基金构建和套期保值策略问题。


<details>
  <summary>Details</summary>
Motivation: 提供含环境标准投资基金的单位连结型寿险保单面临构建绿色投资基金和开发套期保值策略两大挑战。

Method: 设计基于企业碳强度的投资组合选择规则并以真实市场数据测试；采用二次方法解决套期保值问题，用高效弱二阶方案进行模拟研究。

Result: 未明确提及具体结果，但对投资组合选择方法和套期保值策略进行了测试和分析。

Conclusion: 未明确提及结论。

Abstract: We study the problem of designing and hedging unit-linked life policies whose
benefits depend on an investment fund that incorporates environmental criteria
in its selection process. Offering these products poses two key challenges:
constructing a green investment fund and developing a hedging strategy for
policies written on that fund. We address these two problems separately. First,
we design a portfolio selection rule driven by firms' carbon intensity that
endogenously selects assets and avoids ad hoc pre-screens based on ESG scores.
The effectiveness of our new portfolio selection method is tested using real
market data. Second, we adopt the perspective of an insurance company issuing
unit-linked policies written on this fund. Such contracts are exposed to
market, carbon, and mortality risk, which the insurer seeks to hedge. Due to
market incompleteness, we address the hedging problem via a quadratic approach
aimed at minimizing the tracking error. We also make a numerical analysis to
assess the performance of the hedging strategy. For our simulation study, we
use an efficient weak second-order scheme that allows for variance reduction.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [230] [Predicting Market Troughs: A Machine Learning Approach with Causal Interpretation](https://arxiv.org/abs/2509.05922)
*Peilin Rao,Randall R. Rojas*

Main category: q-fin.ST

TL;DR: 本文用灵活框架研究市场低谷成因，识别关键驱动因素并支持中介资产定价理论。


<details>
  <summary>Details</summary>
Motivation: 为市场低谷的因果驱动因素提供有力新证据。

Method: 采用灵活的DML平均部分效应因果机器学习框架，借助高性能临近预报模型实时识别投降事件。

Result: 确定期权隐含风险偏好的波动性和市场流动性为关键因果驱动因素。

Conclusion: 研究结果为中介资产定价理论提供高频实证支持。

Abstract: This paper provides robust, new evidence on the causal drivers of market
troughs. We demonstrate that conclusions about these triggers are critically
sensitive to model specification, moving beyond restrictive linear models with
a flexible DML average partial effect causal machine learning framework. Our
robust estimates identify the volatility of options-implied risk appetite and
market liquidity as key causal drivers, relationships misrepresented or
obscured by simpler models. These findings provide high-frequency empirical
support for intermediary asset pricing theories. This causal analysis is
enabled by a high-performance nowcasting model that accurately identifies
capitulation events in real-time.

</details>


### [231] [The use of financial and sustainability ratios to map a sector. An approach using compositional data](https://arxiv.org/abs/2509.06468)
*Elena Rondós-Casas,Germà Coenders,Miquel Carreras-Simó,Núria Arimany-Serrat*

Main category: q-fin.ST

TL;DR: 本文运用组合数据方法和主成分分析双标图，可视化西班牙鱼和肉类加工公司在长期偿债能力、能源、废物、水强度和性别就业差距方面的表现，发现鱼加工公司表现更同质，还识别出不同表现的公司群。


<details>
  <summary>Details</summary>
Motivation: 在单个图中可视化西班牙鱼和肉类加工公司在长期偿债能力、能源、废物、水强度和性别就业差距方面的表现。

Method: 采用组合数据方法论和主成分分析双标图，对选定的财务、环境和社会指标进行分析。

Result: 鱼加工公司财务、环境和社会表现更同质；两行业都有部分指标表现差的公司群；偿债能力高的公司能源和水使用效率低；识别出两组表现相似的公司群。

Conclusion: 可视化工具结合财务、环境和社会指标具有创新性，企业可直观看到与同行相比需改进的方面，未来研究样本会扩大。

Abstract: Purpose: The article aims to visualise in a single graph fish and meat
processing company groups in Spain with respect to long-term solvency, energy,
waste and water intensity and gender employment gap.
  Design/methodology/approach: The selected financial, environmental and social
indicators are ratios, which require specific statistical analysis methods to
prevent severe skewness and outliers. We use the compositional data methodology
and the principal-component analysis biplot.
  Findings: Fish-processing companies have more homogeneous financial,
environmental and social performance than their meat-processing counterparts.
Specific company groups in both sectors can be identified as poor performers in
some of the indicators. Firms with higher solvency tend to be less efficient in
energy and water use. Two clusters of company groups with similar performances
are identified.
  Research limitations/implications: As of now, few firms publish reports
according to the EU Corporate Sustainability Reporting Directive. In future
research larger samples will be available.
  Social Implications: Firm groups can visually see their areas of improvement
in their financial, environmental and social performance compared to their
competitors in the sector.
  Originality/value: This is the first time in which visualization tools have
combined financial, environmental and social indicators. All individual firms
can be visually ordered along all indicators simultaneously.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [232] [Optimal Exit Time for Liquidity Providers in Automated Market Makers](https://arxiv.org/abs/2509.06510)
*Philippe Bergault,Sébastien Bieber,Leandro Sánchez-Betancourt*

Main category: q-fin.TR

TL;DR: 研究做市商流动性提供者（LP）最优撤资问题，用随机控制问题刻画最优退出时间，开发数值求解方法，模拟展示退出策略影响因素。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注静态流动性供给和外生退出策略，本文旨在研究LP最优流动性撤资问题，深入理解自动做市商（AMM）动态流动性供给。

Method: 将最优退出时间刻画为带内生停止时间的随机控制问题，证明LP价值函数满足HJB拟变分不等式并在粘性意义下唯一；开发基于算子分裂的欧拉格式和Longstaff - Schwartz回归法两种数值求解方法。

Result: 校准模拟显示LP最优退出策略依赖于预言机价格波动性、费用水平、套利者和噪声交易者行为；LP最优决策需平衡套利产生的费用和无常损失。

Conclusion: 有助于深入理解AMM动态流动性供给，为不同市场制度下被动LP策略的可持续性提供见解。

Abstract: We study the problem of optimal liquidity withdrawal for a representative
liquidity provider (LP) in an automated market maker (AMM). LPs earn fees from
trading activity but are exposed to impermanent loss (IL) due to price
fluctuations. While existing work has focused on static provision and exogenous
exit strategies, we characterise the optimal exit time as the solution to a
stochastic control problem with an endogenous stopping time. Mathematically,
the LP's value function is shown to satisfy a Hamilton-Jacobi-Bellman
quasi-variational inequality, for which we establish uniqueness in the
viscosity sense. To solve the problem numerically, we develop two complementary
approaches: a Euler scheme based on operator splitting and a Longstaff-Schwartz
regression method. Calibrated simulations highlight how the LP's optimal exit
strategy depends on the oracle price volatility, fee levels, and the behaviour
of arbitrageurs and noise traders. Our results show that while arbitrage
generates both fees and IL, the LP's optimal decision balances these opposing
effects based on the pool state variables and price misalignments. This work
contributes to a deeper understanding of dynamic liquidity provision in AMMs
and provides insights into the sustainability of passive LP strategies under
different market regimes.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [233] [Cryo-EM as a Stochastic Inverse Problem](https://arxiv.org/abs/2509.05541)
*Diego Sanchez Espinosa,Erik H Thiede,Yunan Yang*

Main category: stat.ML

TL;DR: 本文将冷冻电镜重建问题转化为概率测度上的随机逆问题，通过变分差异最小化和Wasserstein梯度流求解，用合成例子验证，分析与MAP方法联系及一致性，还提供解决含随机前向算子随机逆问题的通用方法。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜3D重建中结构异质性是主要挑战，传统方法难以恢复连续结构变异性。

Method: 将冷冻电镜重建表述为概率测度上的随机逆问题，通过最小化观测和模拟图像分布的变分差异，利用Wasserstein梯度流在概率测度空间优化，用粒子表示和演化构象集合。

Result: 用合成例子包括真实蛋白质模型验证了方法能恢复结构状态的连续分布，分析了与MAP方法的联系及DTO方法的收敛条件。

Conclusion: 该框架为冷冻电镜重建提供新方法，还能解决含随机前向算子的随机逆问题。

Abstract: Cryo-electron microscopy (Cryo-EM) enables high-resolution imaging of
biomolecules, but structural heterogeneity remains a major challenge in 3D
reconstruction. Traditional methods assume a discrete set of conformations,
limiting their ability to recover continuous structural variability. In this
work, we formulate cryo-EM reconstruction as a stochastic inverse problem (SIP)
over probability measures, where the observed images are modeled as the
push-forward of an unknown distribution over molecular structures via a random
forward operator. We pose the reconstruction problem as the minimization of a
variational discrepancy between observed and simulated image distributions,
using statistical distances such as the KL divergence and the Maximum Mean
Discrepancy. The resulting optimization is performed over the space of
probability measures via a Wasserstein gradient flow, which we numerically
solve using particles to represent and evolve conformational ensembles. We
validate our approach using synthetic examples, including a realistic protein
model, which demonstrates its ability to recover continuous distributions over
structural states. We analyze the connection between our formulation and
Maximum A Posteriori (MAP) approaches, which can be interpreted as instances of
the discretize-then-optimize (DTO) framework. We further provide a consistency
analysis, establishing conditions under which DTO methods, such as MAP
estimation, converge to the solution of the underlying infinite-dimensional
continuous problem. Beyond cryo-EM, the framework provides a general
methodology for solving SIPs involving random forward operators.

</details>


### [234] [Robust variational neural posterior estimation for simulation-based inference](https://arxiv.org/abs/2509.05724)
*Matthew O'Callaghan,Kaisey S. Mandel,Gerry Gilmore*

Main category: stat.ML

TL;DR: 本文介绍了鲁棒变分神经后验估计（RVNP）方法，用于解决摊销模拟推理（SBI）中的模型错误指定问题，在多个基准任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于模拟的推理方法在模型错误指定时表现不佳，而实际中模拟器总会在一定程度上与真实数据生成过程不符，因此需要解决该问题。

Method: 提出RVNP方法，通过变分推理和误差建模来弥合模拟与现实的差距。

Result: 在多个基准任务（包括天文学真实数据）上测试，能以数据驱动方式恢复鲁棒的后验推理，无需调整控制错误指定的超参数或先验。

Conclusion: RVNP方法能有效解决摊销SBI中的模型错误指定问题。

Abstract: Recent advances in neural density estimation have enabled powerful
simulation-based inference (SBI) methods that can flexibly approximate Bayesian
inference for intractable stochastic models. Although these methods have
demonstrated reliable posterior estimation when the simulator accurately
represents the underlying data generative process (GDP), recent work has shown
that they perform poorly in the presence of model misspecification. This poses
a significant problem for their use on real-world problems, due to simulators
always misrepresenting the true DGP to a certain degree. In this paper, we
introduce robust variational neural posterior estimation (RVNP), a method which
addresses the problem of misspecification in amortised SBI by bridging the
simulation-to-reality gap using variational inference and error modelling. We
test RVNP on multiple benchmark tasks, including using real data from
astronomy, and show that it can recover robust posterior inference in a
data-driven manner without adopting tunable hyperparameters or priors governing
the misspecification.

</details>


### [235] [Risk-averse Fair Multi-class Classification](https://arxiv.org/abs/2509.05771)
*Darinka Dentcheva,Xiangyu Tian*

Main category: stat.ML

TL;DR: 本文基于相干风险测度和系统性风险理论开发新分类框架，适用于数据有噪声、稀缺且标签不可靠的多类问题，通过多种方法求解并进行性能和公平性分析，实验表明该方法更稳健且性能更好。


<details>
  <summary>Details</summary>
Motivation: 解决数据有噪声、稀缺且标签不可靠的多类分类问题，同时考虑分类的公平性。

Method: 提供系统性风险模型基础并应用于线性和核基多类问题，提出系统理论方法的高级公式，设计风险规避正则分解方法求解，用流行多类方法作基准，提出基于相干风险测度的推广方法。

Result: 所提风险规避方法在理论和数值上可行，能促进分类公平性，在不可靠训练数据下更稳健，对未知数据性能优于最小化预期分类误差的方法，类别数增加时性能提升。

Conclusion: 所提基于相干风险测度和系统性风险的分类框架有效，适用于特定多类问题，具有良好的鲁棒性和性能。

Abstract: We develop a new classification framework based on the theory of coherent
risk measures and systemic risk. The proposed approach is suitable for
multi-class problems when the data is noisy, scarce (relative to the dimension
of the problem), and the labeling might be unreliable. In the first part of our
paper, we provide the foundation of the use of systemic risk models and show
how to apply it in the context of linear and kernel-based multi-class problems.
More advanced formulation via a system-theoretic approach with non-linear
aggregation is proposed, which leads to a two-stage stochastic programming
problem. A risk-averse regularized decomposition method is designed to solve
the problem. We use a popular multi-class method as a benchmark in the
performance analysis of the proposed classification methods. We illustrate our
ideas by proposing several generalization of that method by the use of coherent
measures of risk. The viability of the proposed risk-averse methods are
supported theoretically and numerically. Additionally, we demonstrate that the
application of systemic risk measures facilitates enforcing fairness in
classification. Analysis and experiments regarding the fairness of the proposed
models are carefully conducted. For all methods, our numerical experiments
demonstrate that they are robust in the presence of unreliable training data
and perform better on unknown data than the methods minimizing expected
classification errors. Furthermore, the performance improves when the number of
classes increases.

</details>


### [236] [Causal Clustering for Conditional Average Treatment Effects Estimation and Subgroup Discovery](https://arxiv.org/abs/2509.05775)
*Zilong Wang,Turgay Ayer,Shihao Yang*

Main category: stat.ML

TL;DR: 提出基于因果森林学习核的聚类框架估计异质治疗效果，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 在个性化医疗等领域，识别对干预反应不同的子群体很关键，但聚类方法与因果推断结合有限。

Method: 分两步，先通过Robinson分解用正交化学习器估计去偏条件平均治疗效果（CATEs）得到核矩阵，再对矩阵进行核化聚类以发现子群体并计算聚类级平均CATEs，将核化聚类作为残差对残差回归框架的正则化形式。

Result: 通过半合成和真实世界数据集的大量实验、消融研究和探索性分析，证明方法能有效捕捉有意义的治疗效果异质性。

Conclusion: 所提框架在估计异质治疗效果方面有效。

Abstract: Estimating heterogeneous treatment effects is critical in domains such as
personalized medicine, resource allocation, and policy evaluation. A central
challenge lies in identifying subpopulations that respond differently to
interventions, thereby enabling more targeted and effective decision-making.
While clustering methods are well-studied in unsupervised learning, their
integration with causal inference remains limited. We propose a novel framework
that clusters individuals based on estimated treatment effects using a learned
kernel derived from causal forests, revealing latent subgroup structures. Our
approach consists of two main steps. First, we estimate debiased Conditional
Average Treatment Effects (CATEs) using orthogonalized learners via the
Robinson decomposition, yielding a kernel matrix that encodes sample-level
similarities in treatment responsiveness. Second, we apply kernelized
clustering to this matrix to uncover distinct, treatment-sensitive
subpopulations and compute cluster-level average CATEs. We present this
kernelized clustering step as a form of regularization within the
residual-on-residual regression framework. Through extensive experiments on
semi-synthetic and real-world datasets, supported by ablation studies and
exploratory analyses, we demonstrate the effectiveness of our method in
capturing meaningful treatment effect heterogeneity.

</details>


### [237] [Fisher Random Walk: Automatic Debiasing Contextual Preference Inference for Large Language Model Evaluation](https://arxiv.org/abs/2509.05852)
*Yichi Zhang,Alexander Belloni,Ethan X. Fang,Junwei Lu,Xiaoan Xu*

Main category: stat.ML

TL;DR: 为严格可扩展评估大语言模型，研究上下文偏好推断，提出半参数有效估计器，方法在多场景验证了准确性、效率和实用性。


<details>
  <summary>Details</summary>
Motivation: 满足对大语言模型进行严格且可扩展评估的需求。

Method: 聚焦上下文Bradley - Terry - Luce模型，开发半参数有效估计器，通过比较图上聚合加权残差平衡项实现去偏估计；用Fisher随机游走策略确定权重，用潜在表示计算权重；用高斯乘数自助法扩展到多重假设检验，用交叉拟合重要性抽样调整应对分布偏移。

Result: 所提推理过程对一般得分函数估计器有效，能满足灵活实现深度学习方法的需求；数值研究验证了方法的准确性、效率和实用性。

Conclusion: 提出的方法在上下文偏好推断中有效，可用于大语言模型评估等场景。

Abstract: Motivated by the need for rigorous and scalable evaluation of large language
models, we study contextual preference inference for pairwise comparison
functionals of context-dependent preference score functions across domains.
Focusing on the contextual Bradley-Terry-Luce model, we develop a
semiparametric efficient estimator that automates the debiased estimation
through aggregating weighted residual balancing terms across the comparison
graph. We show that the efficiency is achieved when the weights are derived
from a novel strategy called Fisher random walk. We also propose a
computationally feasible method to compute the weights by a potential
representation of nuisance weight functions. We show our inference procedure is
valid for general score function estimators accommodating the practitioners'
need to implement flexible deep learning methods. We extend the procedure to
multiple hypothesis testing using a Gaussian multiplier bootstrap that controls
familywise error and to distributional shift via a cross-fitted
importance-sampling adjustment for target-domain inference. Numerical studies,
including language model evaluations under diverse contexts, corroborate the
accuracy, efficiency, and practical utility of our method.

</details>


### [238] [Uncertainty Quantification in Probabilistic Machine Learning Models: Theory, Methods, and Insights](https://arxiv.org/abs/2509.05877)
*Marzieh Ajirak,Anand Ravishankar,Petar M. Djuric*

Main category: stat.ML

TL;DR: 本文提出概率模型中估计认知和偶然不确定性的系统框架，用随机傅里叶特征近似预测分布，实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化在概率机器学习模型中对评估预测可靠性至关重要。

Method: 聚焦高斯过程潜变量模型，用可扩展的基于随机傅里叶特征的高斯过程近似预测分布，推导不确定性量化理论公式，提出基于蒙特卡罗采样的估计方法。

Result: 结果揭示了预测不确定性的来源，表明该方法在量化预测置信度方面有效。

Conclusion: 提出的系统框架能有效估计概率模型中的不确定性，量化预测的置信度。

Abstract: Uncertainty Quantification (UQ) is essential in probabilistic machine
learning models, particularly for assessing the reliability of predictions. In
this paper, we present a systematic framework for estimating both epistemic and
aleatoric uncertainty in probabilistic models. We focus on Gaussian Process
Latent Variable Models and employ scalable Random Fourier Features-based
Gaussian Processes to approximate predictive distributions efficiently. We
derive a theoretical formulation for UQ, propose a Monte Carlo sampling-based
estimation method, and conduct experiments to evaluate the impact of
uncertainty estimation. Our results provide insights into the sources of
predictive uncertainty and illustrate the effectiveness of our approach in
quantifying the confidence in the predictions.

</details>


### [239] [Additive Distributionally Robust Ranking and Selection](https://arxiv.org/abs/2509.06147)
*Zaile Li,Yuchen Wan,L. Jeff Hong*

Main category: stat.ML

TL;DR: 本文提出简单加法分配(AA)程序和通用加法分配(GAA)框架处理分布式鲁棒排序与选择问题，证明AA一致性和强加法性，实验验证GAA表现。


<details>
  <summary>Details</summary>
Motivation: 现有分布式鲁棒排序与选择(DRR&S)研究中加法预算分配的合理性缺乏严格论证，基本性质理解不足。

Method: 提出AA程序并利用边界交叉论证建立正确选择概率下界，刻画预算分配行为；引入GAA框架灵活结合传统R&S程序采样规则。

Result: 证明AA具有一致性和强加法性，数值实验支持理论发现，GAA程序表现有竞争力。

Conclusion: 研究为DRR&S的加法结构提供新的反直觉见解，GAA框架在保留结构同时提升实际性能。

Abstract: Ranking and selection (R&S) aims to identify the alternative with the best
mean performance among $k$ simulated alternatives. The practical value of R&S
depends on accurate simulation input modeling, which often suffers from the
curse of input uncertainty due to limited data. Distributionally robust ranking
and selection (DRR&S) addresses this challenge by modeling input uncertainty
via an ambiguity set of $m > 1$ plausible input distributions, resulting in
$km$ scenarios in total. Recent DRR&S studies suggest a key structural insight:
additivity in budget allocation is essential for efficiency. However, existing
justifications are heuristic, and fundamental properties such as consistency
and the precise allocation pattern induced by additivity remain poorly
understood. In this paper, we propose a simple additive allocation (AA)
procedure that aims to exclusively sample the $k + m - 1$ previously
hypothesized critical scenarios. Leveraging boundary-crossing arguments, we
establish a lower bound on the probability of correct selection and
characterize the procedure's budget allocation behavior. We then prove that AA
is consistent and, surprisingly, achieves additivity in the strongest sense: as
the total budget increases, only $k + m - 1$ scenarios are sampled infinitely
often. Notably, the worst-case scenarios of non-best alternatives may not be
among them, challenging prior beliefs about their criticality. These results
offer new and counterintuitive insights into the additive structure of DRR&S.
To improve practical performance while preserving this structure, we introduce
a general additive allocation (GAA) framework that flexibly incorporates
sampling rules from traditional R&S procedures in a modular fashion. Numerical
experiments support our theoretical findings and demonstrate the competitive
performance of the proposed GAA procedures.

</details>


### [240] [MOSAIC: Minimax-Optimal Sparsity-Adaptive Inference for Change Points in Dynamic Networks](https://arxiv.org/abs/2509.06303)
*Yingying Fan,Jingyuan Liu,Jinchi Lv,Ao Sun*

Main category: stat.ML

TL;DR: 提出MOSAIC框架用于动态网络变点检测，建立检测边界的极小极大率，开发接近该率的测试方法，调整测试并分析无低秩结构网络的情况，通过模拟和实际数据验证。


<details>
  <summary>Details</summary>
Motivation: 解决具有同时低秩和稀疏变化结构的动态网络中的变点检测问题。

Method: 建立检测边界的极小极大率，开发基于特征分解和筛选信号的测试方法，用基于残差的技术调整理论测试。

Result: 调整后的测试统计量在原假设下收敛到标准正态分布，在备择假设下有全功效，无低秩结构网络的检测边界极小极大率与高维均值向量变点推断结果相近。

Conclusion: MOSAIC框架有效，理论结果通过模拟和实际数据得到验证。

Abstract: We propose a new inference framework, named MOSAIC, for change-point
detection in dynamic networks with the simultaneous low-rank and sparse-change
structure. We establish the minimax rate of detection boundary, which relies on
the sparsity of changes. We then develop an eigen-decomposition-based test with
screened signals that approaches the minimax rate in theory, with only a minor
logarithmic loss. For practical implementation of MOSAIC, we adjust the
theoretical test by a novel residual-based technique, resulting in a pivotal
statistic that converges to a standard normal distribution via the martingale
central limit theorem under the null hypothesis and achieves full power under
the alternative hypothesis. We also analyze the minimax rate of testing
boundary for dynamic networks without the low-rank structure, which almost
aligns with the results in high-dimensional mean-vector change-point inference.
We showcase the effectiveness of MOSAIC and verify our theoretical results with
several simulation examples and a real data application.

</details>


### [241] [Minimax optimal transfer learning for high-dimensional additive regression](https://arxiv.org/abs/2509.06308)
*Seung Hyun Moon*

Main category: stat.ML

TL;DR: 本文在迁移学习框架下研究高维加性回归，提出目标估计程序和两阶段估计方法，给出误差界并证明极小极大最优率，有模拟和实数据分析支持。


<details>
  <summary>Details</summary>
Motivation: 在迁移学习框架下研究高维加性回归，利用目标总体样本和辅助样本改进回归估计。

Method: 先基于局部线性平滑的平滑回代估计器引入目标估计程序，后在迁移学习框架下开发两阶段估计方法。

Result: 在次威布尔噪声下建立一般误差界，在次指数情形下估计器达极小极大下界；两阶段估计方法在总体和经验层面有理论保证，在一定条件下达极小极大最优率。

Conclusion: 所提方法有效，理论结果得到模拟和实数据分析支持。

Abstract: This paper studies high-dimensional additive regression under the transfer
learning framework, where one observes samples from a target population
together with auxiliary samples from different but potentially related
regression models. We first introduce a target-only estimation procedure based
on the smooth backfitting estimator with local linear smoothing. In contrast to
previous work, we establish general error bounds under sub-Weibull($\alpha$)
noise, thereby accommodating heavy-tailed error distributions. In the
sub-exponential case ($\alpha=1$), we show that the estimator attains the
minimax lower bound under regularity conditions, which requires a substantial
departure from existing proof strategies. We then develop a novel two-stage
estimation method within a transfer learning framework, and provide theoretical
guarantees at both the population and empirical levels. Error bounds are
derived for each stage under general tail conditions, and we further
demonstrate that the minimax optimal rate is achieved when the auxiliary and
target distributions are sufficiently close. All theoretical results are
supported by simulation studies and real data analysis.

</details>


### [242] [Robust and Adaptive Spectral Method for Representation Multi-Task Learning with Contamination](https://arxiv.org/abs/2509.06575)
*Yian Huang,Yang Feng,Zhiliang Ying*

Main category: stat.ML

TL;DR: 本文提出RAS方法解决含未知且可能大量污染的表示型多任务学习问题，理论上给出误差界，实验验证其鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有多任务学习方法在污染严重时失效，本文要解决含未知且可能大量污染比例的表示型多任务学习问题。

Method: 引入Robust and Adaptive Spectral (RAS)方法，无需污染水平和真实表示维度的先验知识，能有效提取共享内点表示。

Result: 理论上为学习的表示和每个任务的参数提供非渐近误差界，实验证实RAS在高达80%任务污染的情况下表现出色。

Conclusion: RAS方法具有鲁棒性和适应性，至少和单任务学习表现一样好，可防止负迁移，还可扩展到迁移学习。

Abstract: Representation-based multi-task learning (MTL) improves efficiency by
learning a shared structure across tasks, but its practical application is
often hindered by contamination, outliers, or adversarial tasks. Most existing
methods and theories assume a clean or near-clean setting, failing when
contamination is significant. This paper tackles representation MTL with an
unknown and potentially large contamination proportion, while also allowing for
heterogeneity among inlier tasks. We introduce a Robust and Adaptive Spectral
method (RAS) that can distill the shared inlier representation effectively and
efficiently, while requiring no prior knowledge of the contamination level or
the true representation dimension. Theoretically, we provide non-asymptotic
error bounds for both the learned representation and the per-task parameters.
These bounds adapt to inlier task similarity and outlier structure, and
guarantee that RAS performs at least as well as single-task learning, thus
preventing negative transfer. We also extend our framework to transfer learning
with corresponding theoretical guarantees for the target task. Extensive
experiments confirm our theory, showcasing the robustness and adaptivity of
RAS, and its superior performance in regimes with up to 80\% task
contamination.

</details>


### [243] [Automated Hierarchical Graph Construction for Multi-source Electronic Health Records](https://arxiv.org/abs/2509.06576)
*Yinjie Wang,Doudou Zhou,Yue Liu,Junwei Lu,Tianxi Cai*

Main category: stat.ML

TL;DR: 电子病历（EHRs）对转化研究有价值，但数据异质性阻碍跨机构整合，提出MASH框架解决此问题并应用于真实数据产生可解释层次图。


<details>
  <summary>Details</summary>
Motivation: 电子病历数据的异质性限制了基于EHR分析的可解释性、可比性和可扩展性，需要方法来协调和提取有意义的见解。

Method: 提出MASH框架，用神经最优传输对齐跨机构医疗代码，用学习的双曲嵌入构建层次图，训练时整合多种信息。

Result: 应用于真实EHR数据产生可解释层次图，为非结构化本地实验室代码生成首个自动层次结构。

Conclusion: MASH框架能有效处理电子病历数据的异质性，为下游应用建立基础参考。

Abstract: Electronic Health Records (EHRs), comprising diverse clinical data such as
diagnoses, medications, and laboratory results, hold great promise for
translational research. EHR-derived data have advanced disease prevention,
improved clinical trial recruitment, and generated real-world evidence.
Synthesizing EHRs across institutions enables large-scale, generalizable
studies that capture rare diseases and population diversity, but remains
hindered by the heterogeneity of medical codes, institution-specific
terminologies, and the absence of standardized data structures. These barriers
limit the interpretability, comparability, and scalability of EHR-based
analyses, underscoring the need for robust methods to harmonize and extract
meaningful insights from distributed, heterogeneous data. To address this, we
propose MASH (Multi-source Automated Structured Hierarchy), a fully automated
framework that aligns medical codes across institutions using neural optimal
transport and constructs hierarchical graphs with learned hyperbolic
embeddings. During training, MASH integrates information from pre-trained
language models, co-occurrence patterns, textual descriptions, and supervised
labels to capture semantic and hierarchical relationships among medical
concepts more effectively. Applied to real-world EHR data, including diagnosis,
medication, and laboratory codes, MASH produces interpretable hierarchical
graphs that facilitate the navigation and understanding of heterogeneous
clinical data. Notably, it generates the first automated hierarchies for
unstructured local laboratory codes, establishing foundational references for
downstream applications.

</details>


### [244] [Sequential Least-Squares Estimators with Fast Randomized Sketching for Linear Statistical Models](https://arxiv.org/abs/2509.06856)
*Guan-Yu Chen,Xi Yang*

Main category: stat.ML

TL;DR: 提出SLSE - FRS随机框架用于大规模线性统计模型估计，整合Sketch - and - Solve和Iterative - Sketching方法，实验显示性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大规模线性统计模型的估计问题。

Method: 首次整合Sketch - and - Solve和Iterative - Sketching方法，迭代构建并解决草图最小二乘子问题以逐步优化估计量。

Result: SLSE - FRS在数值实验中表现优于PCG和IDS方法。

Conclusion: SLSE - FRS是一种有效且高精度的大规模线性统计模型估计方法。

Abstract: We propose a novel randomized framework for the estimation problem of
large-scale linear statistical models, namely Sequential Least-Squares
Estimators with Fast Randomized Sketching (SLSE-FRS), which integrates
Sketch-and-Solve and Iterative-Sketching methods for the first time. By
iteratively constructing and solving sketched least-squares (LS) subproblems
with increasing sketch sizes to achieve better precisions, SLSE-FRS gradually
refines the estimators of the true parameter vector, ultimately producing
high-precision estimators. We analyze the convergence properties of SLSE-FRS,
and provide its efficient implementation. Numerical experiments show that
SLSE-FRS outperforms the state-of-the-art methods, namely the Preconditioned
Conjugate Gradient (PCG) method, and the Iterative Double Sketching (IDS)
method.

</details>


### [245] [Learning from one graph: transductive learning guarantees via the geometry of small random worlds](https://arxiv.org/abs/2509.06894)
*Nils Detering,Luca Galimberti,Anastasis Kratsios,Giulia Livieri,A. Martina Neuman*

Main category: stat.ML

TL;DR: 本文针对图卷积网络转导学习统计基础有限的问题，开发新的测度集中工具，建立学习结果并扩展到图卷积网络设置，学习保证在少量标记节点时仍有效。


<details>
  <summary>Details</summary>
Motivation: 图卷积网络广泛用于转导节点分类，但转导学习的统计基础有限，标准推理框架依赖多独立样本而非单图。

Method: 开发新的测度集中工具，利用低维度量嵌入挖掘大图的几何规律，用随机图模型捕捉规律。

Result: 建立两个主要学习结果，一个针对确定性图，一个针对与特定Erdős - Rényi图有共同几何性质的随机图，扩展结果到图卷积网络设置。

Conclusion: 学习保证在少量标记节点时仍有信息价值，且随标记节点数增长达到最优非参数率。

Abstract: Since their introduction by Kipf and Welling in $2017$, a primary use of
graph convolutional networks is transductive node classification, where missing
labels are inferred within a single observed graph and its feature matrix.
Despite the widespread use of the network model, the statistical foundations of
transductive learning remain limited, as standard inference frameworks
typically rely on multiple independent samples rather than a single graph. In
this work, we address these gaps by developing new concentration-of-measure
tools that leverage the geometric regularities of large graphs via
low-dimensional metric embeddings. The emergent regularities are captured using
a random graph model; however, the methods remain applicable to deterministic
graphs once observed. We establish two principal learning results. The first
concerns arbitrary deterministic $k$-vertex graphs, and the second addresses
random graphs that share key geometric properties with an Erd\H{o}s-R\'{e}nyi
graph $\mathbf{G}=\mathbf{G}(k,p)$ in the regime $p \in \mathcal{O}((\log
(k)/k)^{1/2})$. The first result serves as the basis for and illuminates the
second. We then extend these results to the graph convolutional network
setting, where additional challenges arise. Lastly, our learning guarantees
remain informative even with a few labelled nodes $N$ and achieve the optimal
nonparametric rate $\mathcal{O}(N^{-1/2})$ as $N$ grows.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [246] [Social Dynamics of DAOs: Power, Onboarding, and Inclusivity](https://arxiv.org/abs/2509.06163)
*Victoria Kozlova,Ben Biedermann*

Main category: cs.CY

TL;DR: 报告探讨DAO中文化和社会动态对参与和权力的影响，揭示实际存在的包容障碍，呼吁文化反思和构建更具包容性的去中心化系统。


<details>
  <summary>Details</summary>
Motivation: 探究塑造DAO中参与和权力的常被忽视的文化与社会动态，打破DAO权限开放和平等的固有认知。

Method: 采用定性访谈和民族志观察的方法。

Result: 发现金融特权、非正式守门、可见性偏见和入职结构等因素造成有意义的包容障碍，软权力和隐性规范决定人们在DAO中的地位。

Conclusion: 去中心化不仅是协议层面的特征，更是需要有意培养信任、归属感和认知多元性的社会过程，应提高对结构盲点的集体意识，构建更具包容性和文化意识的去中心化系统。

Abstract: This report explores the often-overlooked cultural and social dynamics
shaping participation and power in DAOs. Drawing on qualitative interviews and
ethnographic observations, it shows how factors such as financial privilege,
informal gatekeeping, visibility bias, and onboarding structures create
barriers to meaningful inclusion. While DAOs are frequently framed as
permissionless and egalitarian, the lived experiences of contributors reveal a
more complex reality, one in which soft power and implicit norms determine
people's position within DAOs. Instead of offering solutionist prescriptions,
this report argues for a deeper cultural reflection within the DAO ecosystem.
It highlights that decentralisation is not solely a protocol-level feature, but
an ongoing social process that requires intentional cultivation of trust,
belonging, and epistemic plurality. With this report, we want to sharpen the
collective awareness of structural blind spots and call for building more
inclusive and culturally conscious decentralised systems.

</details>


### [247] [Audits Under Resource, Data, and Access Constraints: Scaling Laws For Less Discriminatory Alternatives](https://arxiv.org/abs/2509.05627)
*Sarah H. Cen,Salil Goyal,Zaynah Javed,Ananya Karthik,Percy Liang,Daniel E. Ho*

Main category: cs.CY

TL;DR: 本文提出一种程序，使索赔人在资源有限情况下能判断‘较少歧视性替代方案’（LDA）是否存在，给出损失 - 公平帕累托前沿（PF）的封闭形式上界及缩放定律。


<details>
  <summary>Details</summary>
Motivation: 在反歧视法的AI案例中，索赔人证明LDA存在面临资源、专业知识不足以及模型和数据访问受限的难题。

Method: 聚焦人口统计学平等定义公平、二元交叉熵损失定义性能的场景，推导损失 - 公平PF的封闭形式上界，利用小样本数据训练少量小模型拟合特定PF。

Result: 得到损失 - 公平PF的缩放定律，模拟压力测试表明该定律在理论条件不完全满足时仍成立。

Conclusion: 所提出的程序能帮助索赔人在资源有限时有效判断LDA是否存在。

Abstract: AI audits play a critical role in AI accountability and safety. One branch of
the law for which AI audits are particularly salient is anti-discrimination
law. Several areas of anti-discrimination law implicate the "less
discriminatory alternative" (LDA) requirement, in which a protocol (e.g.,
model) is defensible if no less discriminatory protocol that achieves
comparable performance can be found with a reasonable amount of effort.
Notably, the burden of proving an LDA exists typically falls on the claimant
(the party alleging discrimination). This creates a significant hurdle in AI
cases, as the claimant would seemingly need to train a less discriminatory yet
high-performing model, a task requiring resources and expertise beyond most
litigants. Moreover, developers often shield information about and access to
their model and training data as trade secrets, making it difficult to
reproduce a similar model from scratch.
  In this work, we present a procedure enabling claimants to determine if an
LDA exists, even when they have limited compute, data, information, and model
access. We focus on the setting in which fairness is given by demographic
parity and performance by binary cross-entropy loss. As our main result, we
provide a novel closed-form upper bound for the loss-fairness Pareto frontier
(PF). We show how the claimant can use it to fit a PF in the "low-resource
regime," then extrapolate the PF that applies to the (large) model being
contested, all without training a single large model. The expression thus
serves as a scaling law for loss-fairness PFs. To use this scaling law, the
claimant would require a small subsample of the train/test data. Then, the
claimant can fit the context-specific PF by training as few as 7 (small)
models. We stress test our main result in simulations, finding that our scaling
law holds even when the exact conditions of our theory do not.

</details>


### [248] [Governing AI R&D: A Legal Framework for Constraining Dangerous AI](https://arxiv.org/abs/2509.05361)
*Alex Mark,Aaron Scher*

Main category: cs.CY

TL;DR: 随着AI发展，治理其发展对公共安全至关重要，美国立法者应考虑AI监管的法律挑战并提前应对。


<details>
  <summary>Details</summary>
Motivation: AI发展需治理，治理行动可能引发法律挑战，立法者需提前考虑。

Method: 研究美国AI监管在第一修正案、行政法和第十四修正案方面的三类潜在诉讼风险，讨论适用的先例和可能出现的法律挑战。

Result: 明确了潜在诉讼风险、可能出现的法律挑战及适用先例。

Conclusion: 有效AI监管可行，但需谨慎实施以避免法律挑战。

Abstract: As AI advances, governing its development may become paramount to public
safety. Lawmakers may seek to restrict the development and release of AI models
or of AI research itself. These governance actions could trigger legal
challenges that invalidate the actions, so lawmakers should consider these
challenges ahead of time. We investigate three classes of potential litigation
risk for AI regulation in the U.S.: the First Amendment, administrative law,
and the Fourteenth Amendment. We discuss existing precedent that is likely to
apply to AI, which legal challenges are likely to arise, and how lawmakers
might preemptively address them. Effective AI regulation is possible, but it
requires careful implementation to avoid these legal challenges.

</details>


### [249] [Prototyping an AI-powered Tool for Energy Efficiency in New Zealand Homes](https://arxiv.org/abs/2509.05364)
*Abdollah Baghaei Daemei*

Main category: cs.CY

TL;DR: 本文介绍新西兰一个AI住宅能源效率决策支持工具的设计与评估，该工具获专家积极反馈，具重要意义并指明未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 新西兰住宅质量历史上较差，虽有改革但仍存在部分改造、数据有限、决策支持分散等问题，需工具辅助。

Method: 用Python和Streamlit开发原型，集成数据处理等功能成模块化仪表盘，通过半结构化访谈让15位专家测试。

Result: 工具可用性强（M = 4.3），情景输出价值高（M = 4.5），专家认为能补充补贴计划和监管框架。

Conclusion: 该工具能将国家政策转化为家庭层面指导，提供可复制框架，未来应关注碳指标等方面发展。

Abstract: Residential buildings contribute significantly to energy use, health
outcomes, and carbon emissions. In New Zealand, housing quality has
historically been poor, with inadequate insulation and inefficient heating
contributing to widespread energy hardship. Recent reforms, including the
Warmer Kiwi Homes program, Healthy Homes Standards, and H1 Building Code
upgrades, have delivered health and comfort improvements, yet challenges
persist. Many retrofits remain partial, data on household performance are
limited, and decision-making support for homeowners is fragmented. This study
presents the design and evaluation of an AI-powered decision-support tool for
residential energy efficiency in New Zealand. The prototype, developed using
Python and Streamlit, integrates data ingestion, anomaly detection, baseline
modeling, and scenario simulation (e.g., LED retrofits, insulation upgrades)
into a modular dashboard. Fifteen domain experts, including building
scientists, consultants, and policy practitioners, tested the tool through
semi-structured interviews. Results show strong usability (M = 4.3), high value
of scenario outputs (M = 4.5), and positive perceptions of its potential to
complement subsidy programs and regulatory frameworks. The tool demonstrates
how AI can translate national policies into personalized, household-level
guidance, bridging the gap between funding, standards, and practical
decision-making. Its significance lies in offering a replicable framework for
reducing energy hardship, improving health outcomes, and supporting climate
goals. Future development should focus on carbon metrics, tariff modeling,
integration with national datasets, and longitudinal trials to assess
real-world adoption.

</details>


### [250] [Cumplimiento del Reglamento (UE) 2024/1689 en robótica y sistemas autónomos: una revisión sistemática de la literatura](https://arxiv.org/abs/2509.05380)
*Yoana Pita Lorenzo*

Main category: cs.CY

TL;DR: 该系统文献综述分析自主机器人系统对欧盟法规2024/1689的合规现状，发现部分法规对齐但仍有差距，提出模块化方法是关键。


<details>
  <summary>Details</summary>
Motivation: 分析自主机器人系统对Regulation (EU) 2024/1689的合规现状，聚焦网络安全框架和方法。

Method: 采用PRISMA协议，从IEEE Xplore、ACM DL、Scopus和Web of Science的243条初始记录中选了22项研究。

Result: 部分法规对齐，风险管理和加密通信有进展，但可解释性模块、实时人工监督和知识库可追溯性存在显著差距，仅40%解决方案明确满足透明度要求，30%实施故障干预机制。

Conclusion: 集成风险、监督和持续审计的模块化方法对满足自主机器人AI法案要求至关重要。

Abstract: This systematic literature review analyzes the current state of compliance
with Regulation (EU) 2024/1689 in autonomous robotic systems, focusing on
cybersecurity frameworks and methodologies. Using the PRISMA protocol, 22
studies were selected from 243 initial records across IEEE Xplore, ACM DL,
Scopus, and Web of Science. Findings reveal partial regulatory alignment: while
progress has been made in risk management and encrypted communications,
significant gaps persist in explainability modules, real-time human oversight,
and knowledge base traceability. Only 40% of reviewed solutions explicitly
address transparency requirements, and 30% implement failure intervention
mechanisms. The study concludes that modular approaches integrating risk,
supervision, and continuous auditing are essential to meet the AI Act mandates
in autonomous robotics.

</details>


### [251] [User Privacy and Large Language Models: An Analysis of Frontier Developers' Privacy Policies](https://arxiv.org/abs/2509.05382)
*Jennifer King,Kevin Klyman,Emily Capstick,Tiffany Saade,Victoria Hsieh*

Main category: cs.CY

TL;DR: 分析六家美国前沿AI开发者隐私政策，发现他们默认用用户聊天数据训练模型，政策缺乏透明度，最后给出应对数据隐私挑战建议。


<details>
  <summary>Details</summary>
Motivation: 模型开发者渴望获取高质量训练数据，本文旨在了解他们如何利用用户聊天数据训练模型。

Method: 基于加州消费者隐私法案，开发定性编码方案，应用于开发者隐私政策以比较数据收集和使用实践。

Result: 六家开发者默认用用户聊天数据训练模型，部分无限期保留数据，可能收集敏感信息，四家使用儿童聊天数据等，隐私政策缺乏关键信息。

Conclusion: 指出用户缺乏同意、数据安全等问题，为政策制定者和开发者提供应对数据隐私挑战的建议。

Abstract: Hundreds of millions of people now regularly interact with large language
models via chatbots. Model developers are eager to acquire new sources of
high-quality training data as they race to improve model capabilities and win
market share. This paper analyzes the privacy policies of six U.S. frontier AI
developers to understand how they use their users' chats to train models.
Drawing primarily on the California Consumer Privacy Act, we develop a novel
qualitative coding schema that we apply to each developer's relevant privacy
policies to compare data collection and use practices across the six companies.
We find that all six developers appear to employ their users' chat data to
train and improve their models by default, and that some retain this data
indefinitely. Developers may collect and train on personal information
disclosed in chats, including sensitive information such as biometric and
health data, as well as files uploaded by users. Four of the six companies we
examined appear to include children's chat data for model training, as well as
customer data from other products. On the whole, developers' privacy policies
often lack essential information about their practices, highlighting the need
for greater transparency and accountability. We address the implications of
users' lack of consent for the use of their chat data for model training, data
security issues arising from indefinite chat data retention, and training on
children's chat data. We conclude by providing recommendations to policymakers
and developers to address the data privacy challenges posed by LLM-powered
chatbots.

</details>


### [252] [Authorship Without Writing: Large Language Models and the Senior Author Analogy](https://arxiv.org/abs/2509.05390)
*Clint Hurshman,Sebastian Porsdam Mann,Julian Savulescu,Brian D. Earp*

Main category: cs.CY

TL;DR: 探讨大语言模型（LLMs）在生物伦理、科学和医学写作中的作者身份问题，认为特定条件下LLM使用类似高级作者身份，要么认可其为合法作者身份，要么修订现有标准。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在写作中的作者身份存在争议，尤其是人类使用LLMs时能否算作作者尚无共识。

Method: 将特定条件下LLM的使用与高级作者身份进行类比。

Result: 认为在许多领域的公认标准下，使用LLMs生成完整论文草稿可被视为合法的作者身份形式。

Conclusion: 要么承认这种使用方式为合法作者身份，要么对现有作者身份标准进行根本性修订。

Abstract: The use of large language models (LLMs) in bioethical, scientific, and
medical writing remains controversial. While there is broad agreement in some
circles that LLMs cannot count as authors, there is no consensus about whether
and how humans using LLMs can count as authors. In many fields, authorship is
distributed among large teams of researchers, some of whom, including
paradigmatic senior authors who guide and determine the scope of a project and
ultimately vouch for its integrity, may not write a single word. In this paper,
we argue that LLM use (under specific conditions) is analogous to a form of
senior authorship. On this view, the use of LLMs, even to generate complete
drafts of research papers, can be considered a legitimate form of authorship
according to the accepted criteria in many fields. We conclude that either such
use should be recognized as legitimate, or current criteria for authorship
require fundamental revision. AI use declaration: GPT-5 was used to help format
Box 1. AI was not used for any other part of the preparation or writing of this
manuscript.

</details>


### [253] [An Optimized Pipeline for Automatic Educational Knowledge Graph Construction](https://arxiv.org/abs/2509.05392)
*Qurat Ul Ain,Mohamed Amine Chatti,Jean Qussa,Amr Shakhshir,Rawaa Alatrash,Shoeb Joarder*

Main category: cs.CY

TL;DR: 本文提出从PDF学习材料自动构建教育知识图谱的管道，初始精度低，经优化后精度提升17.5%，处理效率提高10倍。


<details>
  <summary>Details</summary>
Motivation: 现有方法在自动生成教育知识图谱方面缺乏可扩展和可靠的方案，需要构建统一且强大的管道。

Method: 先从单页/幻灯片生成幻灯片级的教育知识图谱，再合并形成完整图谱，对生成的图谱进行评估并针对问题优化管道组件。

Result: 优化后的管道精度提升17.5%，处理效率提高10倍。

Conclusion: 该方法提供了整体、可扩展的端到端教育知识图谱构建管道，适用于不同教育场景，支持更好的学习内容语义表示。

Abstract: The automatic construction of Educational Knowledge Graphs (EduKGs) is
essential for domain knowledge modeling by extracting meaningful
representations from learning materials. Despite growing interest, identifying
a scalable and reliable approach for automatic EduKG generation remains a
challenge. In an attempt to develop a unified and robust pipeline for automatic
EduKG construction, in this study we propose a pipeline for automatic EduKG
construction from PDF learning materials. The process begins with generating
slide-level EduKGs from individual pages/slides, which are then merged to form
a comprehensive EduKG representing the entire learning material. We evaluate
the accuracy of the EduKG generated from the proposed pipeline in our MOOC
platform, CourseMapper. The observed accuracy, while indicative of partial
success, is relatively low particularly in the educational context, where the
reliability of knowledge representations is critical for supporting meaningful
learning. To address this, we introduce targeted optimizations across multiple
pipeline components. The optimized pipeline achieves a 17.5% improvement in
accuracy and a tenfold increase in processing efficiency. Our approach offers a
holistic, scalable and end-to-end pipeline for automatic EduKG construction,
adaptable to diverse educational contexts, and supports improved semantic
representation of learning content.

</details>


### [254] [Inferring Prerequisite Knowledge Concepts in Educational Knowledge Graphs: A Multi-criteria Approach](https://arxiv.org/abs/2509.05393)
*Rawaa Alatrash,Mohamed Amine Chatti,Nasha Wibowo,Qurat Ul Ain*

Main category: cs.CY

TL;DR: 提出无监督方法自动推断教育知识图谱概念先决关系，实验显示比现有方法精度高，为学习提供支持


<details>
  <summary>Details</summary>
Motivation: MOOC平台CourseMapper的教育知识图谱缺乏明确先决关系链接，手动标注耗时且不一致

Method: 定义基于多种特征的十条标准，用投票算法结合以推断概念先决关系

Result: 在基准数据集实验中，该方法比现有方法精度更高，且具可扩展性和适应性

Conclusion: 方法能为CourseMapper的序列感知学习提供可靠支持

Abstract: Educational Knowledge Graphs (EduKGs) organize various learning entities and
their relationships to support structured and adaptive learning. Prerequisite
relationships (PRs) are critical in EduKGs for defining the logical order in
which concepts should be learned. However, the current EduKG in the MOOC
platform CourseMapper lacks explicit PR links, and manually annotating them is
time-consuming and inconsistent. To address this, we propose an unsupervised
method for automatically inferring concept PRs without relying on labeled data.
We define ten criteria based on document-based, Wikipedia hyperlink-based,
graph-based, and text-based features, and combine them using a voting algorithm
to robustly capture PRs in educational content. Experiments on benchmark
datasets show that our approach achieves higher precision than existing methods
while maintaining scalability and adaptability, thus providing reliable support
for sequence-aware learning in CourseMapper.

</details>


### [255] [From Vision to Validation: A Theory- and Data-Driven Construction of a GCC-Specific AI Adoption Index](https://arxiv.org/abs/2509.05474)
*Mohammad Rashed Albous,Anwaar AlKandari,Abdel Latef Anouze*

Main category: cs.CY

TL;DR: 本文结合文献分析和实证研究，为海湾合作委员会（GCC）公共部门开发并验证了AI采用指数，指出基础设施和政策对AI实施影响大，该指数为政策制定者提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有标准化措施未考虑GCC国家独特驱动因素、治理模式和文化差异，需要为GCC公共部门量身定制AI采用指数。

Method: 采用理论驱动（文献分析和6个国家AI战略）和数据驱动（对203名政府员工调查及先进统计技术）相结合的方法。

Result: 开发并验证了AI采用指数，发现基础设施和政策对AI实施影响最大，组合模型解释70%的AI结果差异，显示资源丰富和自上而下政策可推动技术快速但不均衡应用。

Conclusion: 该指数为政策制定者提供了全面且因地制宜的工具，有助于资源战略分配、跨国合作和能力建设，支持GCC地区及其他地区AI驱动的持续转型。

Abstract: Artificial intelligence (AI) is rapidly transforming public-sector processes
worldwide, yet standardized measures rarely address the unique drivers,
governance models, and cultural nuances of the Gulf Cooperation Council (GCC)
countries. This study employs a theory-driven foundation derived from an
in-depth analysis of literature review and six National AI Strategies (NASs),
coupled with a data-driven approach that utilizes a survey of 203 mid- and
senior-level government employees and advanced statistical techniques (K-Means
clustering, Principal Component Analysis, and Partial Least Squares Structural
Equation Modeling). By combining policy insights with empirical evidence, the
research develops and validates a novel AI Adoption Index specifically tailored
to the GCC public sector. Findings indicate that robust infrastructure and
clear policy mandates exert the strongest influence on successful AI
implementations, overshadowing organizational readiness in early adoption
stages. The combined model explains 70% of the variance in AI outcomes,
suggesting that resource-rich environments and top-down policy directives can
drive rapid but uneven technology uptake. By consolidating key dimensions
(Infrastructure & Resources, Organizational Readiness, and Policy & Regulatory
Environment) into a single composite index, this study provides a holistic yet
context-sensitive tool for benchmarking AI maturity. The index offers
actionable guidance for policymakers seeking to harmonize large-scale
deployments with ethical and regulatory standards. Beyond advancing academic
discourse, these insights inform more strategic allocation of resources,
cross-country cooperation, and capacity-building initiatives, thereby
supporting sustained AI-driven transformation in the GCC region and beyond.

</details>


### [256] [Unmasking COVID-19 Vulnerability in Nigeria: Mapping Risks Beyond Urban Hotspots](https://arxiv.org/abs/2509.05398)
*Sheila Wafula,Blessed Madukoma*

Main category: cs.CY

TL;DR: 研究通过综合风险评分量化尼日利亚各州新冠脆弱性，发现高密度城市地区风险高，风险评分可辅助资源分配，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情给尼日利亚公共卫生系统带来挑战，研究关键因素对各州脆弱性的影响。

Method: 通过综合风险评分（含人口密度、贫困、医疗可及性、年龄风险），结合每10万人标准化病例率，用GIS映射分析趋势和确定热点地区，辅以谷歌趋势数据。

Result: 高密度城市地区如拉各斯风险评分最高，与相关研究结果一致，谷歌趋势数据显示公众健康意识有差异。

Conclusion: 风险评分可作为政策制定者分配资源的工具，存在数据缺口和农村漏报问题，框架可用于其他传染病。

Abstract: The COVID-19 pandemic has presented significant challenges in Nigeria's
public health systems since the first case reported on February 27, 2020. This
study investigates key factors that contribute to state vulnerability,
quantifying them through a composite risk score integrating population density
(weight 0.2), poverty (0.4), access to healthcare (0.3), and age risk (0.1),
adjusted by normalized case rates per 100,000. States were categorized into
low-, medium-, and high-density areas to analyze trends and identify hotspots
using geographic information system (GIS) mapping. The findings reveal that
high-density urban areas, such as Lagos, accounting for 35.4% of national
cases, had the highest risk scores (Lagos: 673.47 vs. national average: 28.16).
These results align with global and local studies on the spatial variability of
COVID-19 in Nigeria, including international frameworks such as the CDC Social
Vulnerability Index. Google Trends data highlight variations in public health
awareness, serving as a supplementary analysis to contextualize vulnerability.
The risk score provides a prioritization tool for policymakers to allocate
testing, vaccines, and healthcare resources to high-risk areas, though data
gaps and rural underreporting call for further research. This framework can
extend to other infectious diseases, offering lessons for future pandemics in
resource-limited settings.

</details>


### [257] [Operationalising AI Regulatory Sandboxes under the EU AI Act: The Triple Challenge of Capacity, Coordination and Attractiveness to Providers](https://arxiv.org/abs/2509.05985)
*Deirdre Ahern*

Main category: cs.CY

TL;DR: 文章探讨欧盟AI法案下成员国建立国家AI监管沙盒的情况，指出挑战与风险，建议欧委会和AI委员会行动，还探讨沙盒对创新者的吸引力问题。


<details>
  <summary>Details</summary>
Motivation: 研究欧盟AI法案要求成员国建立国家AI监管沙盒以促进创新和合规的情况。

Method: 分析成员国在建立监管沙盒过程中面临的挑战、风险，以及创新者的顾虑。

Result: 发现成员国在能力建设和沙盒设计上有挑战，不同国家沙盒做法可能影响法案统一解释与应用，沙盒对创新者可能缺乏吸引力。

Conclusion: 欧洲委员会和AI委员会需制定规则和指南确保国家AI监管沙盒协调统一，同时要考虑沙盒对创新者的吸引力问题。

Abstract: The EU AI Act provides a rulebook for all AI systems being put on the market
or into service in the European Union. This article investigates the
requirement under the AI Act that Member States establish national AI
regulatory sandboxes for testing and validation of innovative AI systems under
regulatory supervision to assist with fostering innovation and complying with
regulatory requirements. Against the backdrop of the EU objective that AI
regulatory sandboxes would both foster innovation and assist with compliance,
considerable challenges are identified for Member States around
capacity-building and design of regulatory sandboxes. While Member States are
early movers in laying the ground for national AI regulatory sandboxes, the
article contends that there is a risk that differing approaches being taken by
individual national sandboxes could jeopardise a uniform interpretation of the
AI Act and its application in practice. This could motivate innovators to play
sandbox arbitrage. The article therefore argues that the European Commission
and the AI Board need to act decisively in developing rules and guidance to
ensure a cohesive, coordinated approach in national AI regulatory sandboxes.
With sandbox participation being voluntary, the possibility that AI regulatory
sandboxes may prove unattractive to innovators on their compliance journey is
also explored. Confidentiality concerns, the inability to relax legal rules
during the sandbox, and the inability of sandboxes to deliver a presumption of
conformity with the AI Act are identified as pertinent concerns for innovators
contemplating applying to AI regulatory sandboxes as compared with other direct
compliance routes provided to them through application of harmonised standards
and conformity assessment procedures.

</details>


### [258] [AI Governance in Higher Education: A course design exploring regulatory, ethical and practical considerations](https://arxiv.org/abs/2509.06176)
*Zsolt Almási,Hannah Bleher,Johannes Bleher,Rozanne Tuesday Flores,Guo Xuanyang,Paweł Pujszo,Raphaël Weuts*

Main category: cs.CY

TL;DR: 当前AI伦理教育零散，本文综合文献和监管进展，提出模块化、跨学科课程，强调综合伦理等，培养负责任AI治理人才。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统渗透关键领域，急需能应对伦理、法律和治理挑战的专业人员，而当前AI伦理教育零散。

Method: 综合文献和监管发展，提出模块化、跨学科课程，关联AI操作失败问题与教学策略，借鉴欧盟、中国和国际框架制定学期计划。

Result: 制定出强调综合伦理、利益相关者参与和体验式学习的学期计划。

Conclusion: 该课程能让学生诊断风险、应对监管、与不同利益相关者互动，培养适应且有道德基础的负责任AI治理专业人员。

Abstract: As artificial intelligence (AI) systems permeate critical sectors, the need
for professionals who can address ethical, legal and governance challenges has
become urgent. Current AI ethics education remains fragmented, often siloed by
discipline and disconnected from practice. This paper synthesizes literature
and regulatory developments to propose a modular, interdisciplinary curriculum
that integrates technical foundations with ethics, law and policy. We highlight
recurring operational failures in AI - bias, misspecified objectives,
generalization errors, misuse and governance breakdowns - and link them to
pedagogical strategies for teaching AI governance. Drawing on perspectives from
the EU, China and international frameworks, we outline a semester plan that
emphasizes integrated ethics, stakeholder engagement and experiential learning.
The curriculum aims to prepare students to diagnose risks, navigate regulation
and engage diverse stakeholders, fostering adaptive and ethically grounded
professionals for responsible AI governance.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [259] [Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance](https://arxiv.org/abs/2509.05978)
*Mohamed Mohamed,Brennan Nichyporuk,Douglas L. Arnold,Tal Arbel*

Main category: eess.IV

TL;DR: 文章提出框架生成高分辨率3D反事实医学图像，在两个神经MRI数据集上取得成果，为3D医学影像疾病进展分析奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在3D医学图像领域缺乏预训练基础模型，限制了仅基于自然语言描述生成高分辨率3D反事实医学图像的发展，而解决此问题有强大临床和研究应用价值。

Method: 引入框架，适配最先进3D扩散模型，结合Simple Diffusion增强，加入增强条件以改善文本对齐和图像质量。

Result: 在两个神经MRI数据集上，成功模拟多发性硬化症的不同反事实病变负荷和阿尔茨海默病的认知状态，生成高质量图像并保留合成医学图像的主体保真度。

Conclusion: 研究成果为3D医学影像中基于提示的疾病进展分析奠定基础。

Abstract: Vision-language models have demonstrated impressive capabilities in
generating 2D images under various conditions; however the impressive
performance of these models in 2D is largely enabled by extensive, readily
available pretrained foundation models. Critically, comparable pretrained
foundation models do not exist for 3D, significantly limiting progress in this
domain. As a result, the potential of vision-language models to produce
high-resolution 3D counterfactual medical images conditioned solely on natural
language descriptions remains completely unexplored. Addressing this gap would
enable powerful clinical and research applications, such as personalized
counterfactual explanations, simulation of disease progression scenarios, and
enhanced medical training by visualizing hypothetical medical conditions in
realistic detail. Our work takes a meaningful step toward addressing this
challenge by introducing a framework capable of generating high-resolution 3D
counterfactual medical images of synthesized patients guided by free-form
language prompts. We adapt state-of-the-art 3D diffusion models with
enhancements from Simple Diffusion and incorporate augmented conditioning to
improve text alignment and image quality. To our knowledge, this represents the
first demonstration of a language-guided native-3D diffusion model applied
specifically to neurological imaging data, where faithful three-dimensional
modeling is essential to represent the brain's three-dimensional structure.
Through results on two distinct neurological MRI datasets, our framework
successfully simulates varying counterfactual lesion loads in Multiple
Sclerosis (MS), and cognitive states in Alzheimer's disease, generating
high-quality images while preserving subject fidelity in synthetically
generated medical images. Our results lay the groundwork for prompt-driven
disease progression analysis within 3D medical imaging.

</details>


### [260] [FASL-Seg: Anatomy and Tool Segmentation of Surgical Scenes](https://arxiv.org/abs/2509.06159)
*Muraam Abdel-Ghani,Mahmoud Ali,Mohamed Ali,Fatmaelzahraa Ahmed,Mohamed Arsalan,Abdulaziz Al-Ali,Shidin Balakrishnan*

Main category: eess.IV

TL;DR: 提出FASL - Seg模型用于手术场景语义分割，在EndoVis18和EndoVis17数据集上表现优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前手术场景语义分割模型多关注手术工具而忽略解剖对象，且难以平衡高低层特征捕捉。

Method: 提出Feature - Adaptive Spatial Localization模型（FASL - Seg），通过Low - Level Feature Projection和High - Level Feature Projection两个处理流捕捉不同分辨率特征。

Result: 在EndoVis18数据集的部位和解剖分割中mIoU达72.71%，比SOTA提高5%；在EndoVis18和EndoVis17工具类型分割中mIoU分别达85.61%和72.78%，整体性能优于SOTA。

Conclusion: FASL - Seg模型采用的不同特征分辨率处理流有效。

Abstract: The growing popularity of robotic minimally invasive surgeries has made deep
learning-based surgical training a key area of research. A thorough
understanding of the surgical scene components is crucial, which semantic
segmentation models can help achieve. However, most existing work focuses on
surgical tools and overlooks anatomical objects. Additionally, current
state-of-the-art (SOTA) models struggle to balance capturing high-level
contextual features and low-level edge features. We propose a Feature-Adaptive
Spatial Localization model (FASL-Seg), designed to capture features at multiple
levels of detail through two distinct processing streams, namely a Low-Level
Feature Projection (LLFP) and a High-Level Feature Projection (HLFP) stream,
for varying feature resolutions - enabling precise segmentation of anatomy and
surgical instruments. We evaluated FASL-Seg on surgical segmentation benchmark
datasets EndoVis18 and EndoVis17 on three use cases. The FASL-Seg model
achieves a mean Intersection over Union (mIoU) of 72.71% on parts and anatomy
segmentation in EndoVis18, improving on SOTA by 5%. It further achieves a mIoU
of 85.61% and 72.78% in EndoVis18 and EndoVis17 tool type segmentation,
respectively, outperforming SOTA overall performance, with comparable per-class
SOTA results in both datasets and consistent performance in various classes for
anatomy and instruments, demonstrating the effectiveness of distinct processing
streams for varying feature resolutions.

</details>


### [261] [Impact of Labeling Inaccuracy and Image Noise on Tooth Segmentation in Panoramic Radiographs using Federated, Centralized and Local Learning](https://arxiv.org/abs/2509.06553)
*Johan Andreas Balle Rubak,Khuram Naveed,Sanyam Jain,Lukas Esterle,Alexandros Iosifidis,Ruben Pauwels*

Main category: eess.IV

TL;DR: 本文对比联邦学习（FL）、集中式学习（CL）和本地学习（LL）在多种数据损坏场景下全景X光片中牙齿分割的表现，发现FL在保持隐私的同时表现更好，且客户端损失轨迹可有效检测异常。


<details>
  <summary>Details</summary>
Motivation: 联邦学习可缓解牙科诊断AI中的隐私约束、数据质量异质性和标签不一致问题，作者旨在对比其与CL和LL在多种数据损坏场景下的牙齿分割效果。

Method: 使用Attention U - Net在2066张X光片上进行训练，设置四种场景，通过Flower AI框架实现FL，监控客户端训练和验证损失轨迹，用Wilcoxon符号秩检验评估指标。

Result: 在四种场景下，FL的Dice分数大多优于CL和LL，损失曲线监测能可靠标记损坏站点。

Conclusion: FL在各数据损坏场景下表现与CL相当或更优，优于LL，同时保护隐私，客户端损失轨迹是有效的异常检测机制，支持其作为可扩展临床AI部署的实用隐私保护方法。

Abstract: Objectives: Federated learning (FL) may mitigate privacy constraints,
heterogeneous data quality, and inconsistent labeling in dental diagnostic AI.
We compared FL with centralized (CL) and local learning (LL) for tooth
segmentation in panoramic radiographs across multiple data corruption
scenarios. Methods: An Attention U-Net was trained on 2066 radiographs from six
institutions across four settings: baseline (unaltered data); label
manipulation (dilated/missing annotations); image-quality manipulation
(additive Gaussian noise); and exclusion of a faulty client with corrupted
data. FL was implemented via the Flower AI framework. Per-client training- and
validation-loss trajectories were monitored for anomaly detection and a set of
metrics (Dice, IoU, HD, HD95 and ASSD) was evaluated on a hold-out test set.
From these metrics significance results were reported through Wilcoxon
signed-rank test. CL and LL served as comparators. Results: Baseline: FL
achieved a median Dice of 0.94889 (ASSD: 1.33229), slightly better than CL at
0.94706 (ASSD: 1.37074) and LL at 0.93557-0.94026 (ASSD: 1.51910-1.69777).
Label manipulation: FL maintained the best median Dice score at 0.94884 (ASSD:
1.46487) versus CL's 0.94183 (ASSD: 1.75738) and LL's 0.93003-0.94026 (ASSD:
1.51910-2.11462). Image noise: FL led with Dice at 0.94853 (ASSD: 1.31088); CL
scored 0.94787 (ASSD: 1.36131); LL ranged from 0.93179-0.94026 (ASSD:
1.51910-1.77350). Faulty-client exclusion: FL reached Dice at 0.94790 (ASSD:
1.33113) better than CL's 0.94550 (ASSD: 1.39318). Loss-curve monitoring
reliably flagged the corrupted site. Conclusions: FL matches or exceeds CL and
outperforms LL across corruption scenarios while preserving privacy. Per-client
loss trajectories provide an effective anomaly-detection mechanism and support
FL as a practical, privacy-preserving approach for scalable clinical AI
deployment.

</details>


### [262] [Robustness and accuracy of mean opinion scores with hard and soft outlier detection](https://arxiv.org/abs/2509.06554)
*Dietmar Saupe,Tim Bleile*

Main category: eess.IV

TL;DR: 本文提出经验最坏情况分析方法评估离群值检测方法，并提出两种新的检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有缺乏可靠全面的离群值检测方法性能比较分析手段。

Method: 提出经验最坏情况分析方法，对离群值检测算法进行对抗黑盒攻击的进化优化。

Result: 应用分析展示了不同离群值检测方法在压力测试中的不同表现，提出两种低复杂度且最坏情况性能优秀的新方法，提供攻击和数据分析软件。

Conclusion: 所提经验最坏情况分析方法可用于离群值检测方法性能分析，新方法具有一定优势。

Abstract: In subjective assessment of image and video quality, observers rate or
compare selected stimuli. Before calculating the mean opinion scores (MOS) for
these stimuli from the ratings, it is recommended to identify and deal with
outliers that may have given unreliable ratings. Several methods are available
for this purpose, some of which have been standardized. These methods are
typically based on statistics and sometimes tested by introducing synthetic
ratings from artificial outliers, such as random clickers. However, a reliable
and comprehensive approach is lacking for comparative performance analysis of
outlier detection methods. To fill this gap, this work proposes and applies an
empirical worst-case analysis as a general solution. Our method involves
evolutionary optimization of an adversarial black-box attack on outlier
detection algorithms, where the adversary maximizes the distortion of scale
values with respect to ground truth. We apply our analysis to several hard and
soft outlier detection methods for absolute category ratings and show their
differing performance in this stress test. In addition, we propose two new
outlier detection methods with low complexity and excellent worst-case
performance. Software for adversarial attacks and data analysis is available.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [263] [Distillation of CNN Ensemble Results for Enhanced Long-Term Prediction of the ENSO Phenomenon](https://arxiv.org/abs/2509.06227)
*Saghar Ganji,Mohammad Naisipour,Alireza Hassani,Arash Adib*

Main category: physics.ao-ph

TL;DR: 研究表明在厄尔尼诺南方涛动（ENSO）预报中，存在技能高于集合平均的子集，不同提前期有提升效果，为后续研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 解决ENSO准确长期预报难题，现有业务系统简单取集合成员均值不合理。

Method: 通过严格事后评估，用1986 - 2017年观测的Nino3.4指数交叉验证的ENSO预报系统，找出基于最低均方根误差和最高皮尔逊相关系数的Top - 5子集。

Result: 这些优秀成员在各提前期相关性更高、均方根误差更低，提前期越长优势越大，在关键过渡时期和特定季节提升明显。

Conclusion: 为进一步识别高质量集合成员线索、提高预报技能提供坚实基础。

Abstract: The accurate long-term forecasting of the El Nino Southern Oscillation (ENSO)
is still one of the biggest challenges in climate science. While it is true
that short-to medium-range performance has been improved significantly using
the advances in deep learning, statistical dynamical hybrids, most operational
systems still use the simple mean of all ensemble members, implicitly assuming
equal skill across members. In this study, we demonstrate, through a strictly
a-posteriori evaluation , for any large enough ensemble of ENSO forecasts,
there is a subset of members whose skill is substantially higher than that of
the ensemble mean. Using a state-of-the-art ENSO forecast system
cross-validated against the 1986-2017 observed Nino3.4 index, we identify two
Top-5 subsets one ranked on lowest Root Mean Square Error (RMSE) and another on
highest Pearson correlation. Generally across all leads, these outstanding
members show higher correlation and lower RMSE, with the advantage rising
enormously with lead time. Whereas at short leads (1 month) raises the mean
correlation by about +0.02 (+1.7%) and lowers the RMSE by around 0.14 {\deg}C
or by 23.3% compared to the All-40 mean, at extreme leads (23 months) the
correlation is raised by +0.43 (+172%) and RMSE by 0.18 {\deg}C or by 22.5%
decrease. The enhancements are largest during crucial ENSO transition periods
such as SON and DJF, when accurate amplitude and phase forecasting is of
greatest socio-economic benefit, and furthermore season-dependent e.g.,
mid-year months such as JJA and MJJ have incredibly large RMSE reductions. This
study provides a solid foundation for further investigations to identify
reliable clues for detecting high-quality ensemble members, thereby enhancing
forecasting skill.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [264] [Workflow for High-Fidelity Dynamic Analysis of Structures with Pile Foundation](https://arxiv.org/abs/2509.05675)
*Amin Pakzad,Pedro Arduino,Wenyang Zhang,Ertugrul Tacirouglu*

Main category: math.NA

TL;DR: 本文提出桩基础结构动力分析工作流程，经数值模拟验证其有效性，为后续研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 土 - 结构相互作用分析中高保真数值模拟需求增加，但缺乏标准化工作流程，本文旨在填补这一空白。

Method: 提出包含使用域缩减法加载、完美匹配层单元吸波、嵌入式界面单元进行土 - 结构相互作用建模和域分解以高效利用处理单元的工作流程，并进行数值模拟。

Result: 域缩减法可减小模拟规模且不降低模型保真度，完美匹配层单元能精确模拟无限域，嵌入式界面单元能高效建立结构与土域连接，工作流程整体有效。

Conclusion: 虽研究基于简化几何和加载场景，但为探索更复杂结构配置和动力加载条件的后续研究提供了基础框架。

Abstract: The demand for high-fidelity numerical simulations in soil-structure
interaction analysis is on the rise, yet a standardized workflow to guide the
creation of such simulations remains elusive. This paper aims to bridge this
gap by presenting a step-by-step guideline proposing a workflow for dynamic
analysis of structures with pile foundations. The proposed workflow encompasses
instructions on how to use Domain Reduction Method for loading, Perfectly
Matched Layer elements for wave absorption, soil-structure interaction modeling
using Embedded interface elements, and domain decomposition for efficient use
of processing units. Through a series of numerical simulations, we showcase the
practical application of this workflow. Our results reveal the efficacy of the
Domain Reduction Method in reducing simulation size without compromising model
fidelity, show the precision of Perfectly Matched Layer elements in modeling
infinite domains, highlight the efficiency of Embedded Interface elements in
establishing connections between structures and the soil domain, and
demonstrate the overall effectiveness of the proposed workflow in conducting
high-fidelity simulations. While our study focuses on simplified geometries and
loading scenarios, it serves as a foundational framework for future research
endeavors aimed at exploring more intricate structural configurations and
dynamic loading conditions

</details>


### [265] [A Parallel Solver with Multiphysics Finite Element Method for Poroelasticity Coupled with Elasticity Model](https://arxiv.org/abs/2509.06673)
*Zhihao Ge,Chengxin Wang*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we propose a parallel solver for solving the quasi-static
linear poroelasticity coupled with linear elasticity model in the Lagrange
multiplier framework. Firstly, we reformulate the model into a coupling of the
nearly incompressible elasticity and an unsteady affection-diffusion equations
by setting new variable ``elastic pressure" and ``volumetric fluid content".
And we introduce a Lagrange multiplier to guarantee the normal stress
continuity on the interface. Then, we give the variational formulations in each
subdomain and choose the $\boldsymbol{P}_k$-$P_1$-$P_1$ mixed finite element
tuple for poroelasticity subdomain, and $\boldsymbol{P}_k$-$P_1$ finite element
pair ($k=1,2$) for elasticity subdomain and the backward Euler scheme for time.
Also, we propose a parallel solver for solving the fully discrete scheme at
each time step -- the FETI method with a classical FETI preconditioner for
solving the Lagrange multiplier and calculating the subproblems in each
subdomain in parallel. And we show several numerical tests to validate the
computational efficiency and the convergence error order, and we consider
Barry-Mercer's model as the benchmark test to show that there no oscillation in
the computed pressure. Finally, we draw conclusions to summarize the main
results of this paper.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [266] [Largevars: An R Package for Testing Large VARs for the Presence of Cointegration](https://arxiv.org/abs/2509.06295)
*Anna Bykhovskaya,Vadim Gorin,Eszter Kiss*

Main category: econ.EM

TL;DR: 介绍Largevars R包可对高维向量自回归进行协整检验，给出Airy_1点过程前十个部分和模拟分位数，还包含应用示例。


<details>
  <summary>Details</summary>
Motivation: 对高维向量自回归进行协整检验，基于Bykhovskaya和Gorin的理论进行测试。

Method: 采用Johansen似然比检验的修改版进行协整检验。

Result: 得到Airy_1点过程前十个部分和精确到前三位数字的模拟分位数。

Conclusion: Largevars R包可用于高维向量自回归协整检验，文章给出模拟分位数及应用示例。

Abstract: Cointegration is a property of multivariate time series that determines
whether its non-stationary, growing components have a stationary linear
combination. Largevars R package conducts a cointegration test for
high-dimensional vector autoregressions of order k based on the large N, T
asymptotics of Bykhovskaya and Gorin (2022, 2025). The implemented test is a
modification of the Johansen likelihood ratio test. In the absence of
cointegration the test converges to the partial sum of the Airy_1 point
process, an object arising in random matrix theory.
  The package and this article contain simulated quantiles of the first ten
partial sums of the Airy_1 point process that are precise up to the first 3
digits. We also include two examples using Largevars: an empirical example on
S&P100 stocks and a simulated VAR(2) example.

</details>


### [267] [Neural ARFIMA model for forecasting BRIC exchange rates with long memory under oil shocks and policy uncertainties](https://arxiv.org/abs/2509.06697)
*Tanujit Chakraborty,Donia Besher,Madhurima Panja,Shovon Sengupta*

Main category: econ.EM

TL;DR: 提出NARFIMA模型预测金砖国家汇率，该模型结合ARFIMA与神经网络，实证显示其优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型难以捕捉新兴经济体汇率序列的长记忆、非线性和非平稳性，且有多个汇率动态驱动因素，需要灵活的建模框架。

Method: 提出NARFIMA模型，结合ARFIMA长记忆表示与神经网络非线性学习能力，纳入外生因果变量，建立模型理论性质，用共形预测区间量化预测不确定性。

Result: 在六个预测期内，NARFIMA在预测金砖国家汇率方面始终优于各种先进统计和机器学习模型。

Conclusion: 研究结果为政策制定者和市场参与者应对金融波动提供新见解，且提供了R包实现该方法。

Abstract: Accurate forecasting of exchange rates remains a persistent challenge,
particularly for emerging economies such as Brazil, Russia, India, and China
(BRIC). These series exhibit long memory, nonlinearity, and non-stationarity
properties that conventional time series models struggle to capture.
Additionally, there exist several key drivers of exchange rate dynamics,
including global economic policy uncertainty, US equity market volatility, US
monetary policy uncertainty, oil price growth rates, and country-specific
short-term interest rate differentials. These empirical complexities underscore
the need for a flexible modeling framework that can jointly accommodate long
memory, nonlinearity, and the influence of external drivers. To address these
challenges, we propose a Neural AutoRegressive Fractionally Integrated Moving
Average (NARFIMA) model that combines the long-memory representation of ARFIMA
with the nonlinear learning capacity of neural networks, while flexibly
incorporating exogenous causal variables. We establish theoretical properties
of the model, including asymptotic stationarity of the NARFIMA process using
Markov chains and nonlinear time series techniques. We quantify forecast
uncertainty using conformal prediction intervals within the NARFIMA framework.
Empirical results across six forecast horizons show that NARFIMA consistently
outperforms various state-of-the-art statistical and machine learning models in
forecasting BRIC exchange rates. These findings provide new insights for
policymakers and market participants navigating volatile financial conditions.
The \texttt{narfima} \textbf{R} package provides an implementation of our
approach.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [268] [Green Learning for STAR-RIS mmWave Systems with Implicit CSI](https://arxiv.org/abs/2509.06820)
*Yu-Hsiang Huang,Po-Heng Chou,Wan-Jen Huang,Walid Saad,C. -C. Jay Kuo*

Main category: eess.SP

TL;DR: 本文提出基于绿色学习（GL）的预编码框架用于STAR - RIS辅助毫米波MIMO广播系统，具低计算复杂度和高能效优势。


<details>
  <summary>Details</summary>
Motivation: 未来6G网络强调环境可持续性，采用广播传输架构可提高频谱效率、减少冗余传输和功耗。

Method: 提出GL框架，直接处理上行导频信号，无需显式CSI估计，集成Saab、RFT和XGBoost联合预测系数和预编码器。

Result: 与BCD和DL模型相比，GL方法实现有竞争力的频谱效率，浮点运算减少超四个数量级。

Conclusion: GL方法适用于能源和硬件受限的广播场景实时部署。

Abstract: In this paper, a green learning (GL)-based precoding framework is proposed
for simultaneously transmitting and reflecting reconfigurable intelligent
surface (STAR-RIS)-aided millimeter-wave (mmWave) MIMO broadcasting systems.
Motivated by the growing emphasis on environmental sustainability in future 6G
networks, this work adopts a broadcasting transmission architecture for
scenarios where multiple users share identical information, improving spectral
efficiency and reducing redundant transmissions and power consumption.
Different from conventional optimization methods, such as block coordinate
descent (BCD) that require perfect channel state information (CSI) and
iterative computation, the proposed GL framework operates directly on received
uplink pilot signals without explicit CSI estimation. Unlike deep learning (DL)
approaches that require CSI-based labels for training, the proposed GL approach
also avoids deep neural networks and backpropagation, leading to a more
lightweight design. Although the proposed GL framework is trained with
supervision generated by BCD under full CSI, inference is performed in a fully
CSI-free manner. The proposed GL integrates subspace approximation with
adjusted bias (Saab), relevant feature test (RFT)-based supervised feature
selection, and eXtreme gradient boosting (XGBoost)-based decision learning to
jointly predict the STAR-RIS coefficients and transmit precoder. Simulation
results show that the proposed GL approach achieves competitive spectral
efficiency compared to BCD and DL-based models, while reducing floating-point
operations (FLOPs) by over four orders of magnitude. These advantages make the
proposed GL approach highly suitable for real-time deployment in energy- and
hardware-constrained broadcasting scenarios.

</details>


### [269] [Integrated Detection and Tracking Based on Radar Range-Doppler Feature](https://arxiv.org/abs/2509.06569)
*Chenyu Zhang,Yuanhang Wu,Xiaoxi Ma,Wei Yi*

Main category: eess.SP

TL;DR: 现有联合检测跟踪方法难充分挖掘雷达信号潜力，本文提出基于雷达特征的综合检测跟踪（InDT）方法并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前联合检测跟踪方法在充分利用雷达信号潜力方面存在挑战，如恒虚警率模型信息表征能力有限、复杂场景描绘不足、跟踪器获取信息有限。

Method: 提出InDT方法，包括雷达信号检测网络架构和利用检测辅助的跟踪器。检测器从距离 - 多普勒矩阵提取特征信息，通过特征增强模块和检测头返回目标位置；跟踪器根据检测置信度自适应更新卡尔曼滤波器的测量噪声协方差，用余弦距离测量目标RD特征相似度并结合位置和特征信息增强数据关联过程。

Result: 通过模拟数据和公开数据集测试验证了方法的有效性。

Conclusion: 所提出的InDT方法能够有效解决现有联合检测跟踪方法的问题，具有良好的应用潜力。

Abstract: Detection and tracking are the basic tasks of radar systems. Current joint
detection tracking methods, which focus on dynamically adjusting detection
thresholds from tracking results, still present challenges in fully utilizing
the potential of radar signals. These are mainly reflected in the limited
capacity of the constant false-alarm rate model to accurately represent
information, the insufficient depiction of complex scenes, and the limited
information acquired by the tracker. We introduce the Integrated Detection and
Tracking based on radar feature (InDT) method, which comprises a network
architecture for radar signal detection and a tracker that leverages detection
assistance. The InDT detector extracts feature information from each
Range-Doppler (RD) matrix and then returns the target position through the
feature enhancement module and the detection head. The InDT tracker adaptively
updates the measurement noise covariance of the Kalman filter based on
detection confidence. The similarity of target RD features is measured by
cosine distance, which enhances the data association process by combining
location and feature information. Finally, the efficacy of the proposed method
was validated through testing on both simulated data and publicly available
datasets.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [270] [A note on the mechanism of substitution of labour with capital in the production processes](https://arxiv.org/abs/2509.05386)
*Vladimir Pokrovskii*

Main category: physics.soc-ph

TL;DR: 论文探讨资本替代劳动现象，提出替代理论，指出考虑三要素，生产函数与Cobb - Douglas表达式不同。


<details>
  <summary>Details</summary>
Motivation: 解释生产中资本替代劳动的真正内涵，正确构建替代机制。

Method: 基于资本设备替代劳动是因其具备类似工人操作能力的假设，考虑生产三要素（资本K、劳动L、设备替代能力P），用技术系数表征设备技术特性。

Result: 生产函数有多种形式，与流行的Cobb - Douglas表达式不同，且认为Cobb - Douglas表达式核心有误。

Conclusion: 资本替代劳动关键在于设备操作能力，构建替代机制需考虑三要素，Cobb - Douglas表达式不适用于此。

Abstract: Considering the production processes, it was noted that the use of various
equipment leads to an increase in output -- the phenomenon that is usually
described as the substitution of labor with capital. The proposed theory of
substitution is based on the assumption that not the quantity of capital
(production equipment) does substitute labor, but rather its ability to operate
similar to the workers. This is the true content of the substitution of labor
by capital. To formulate a correct mechanism of substitution requires
considering three factors of production: the amount of production equipment
(capital $K$), human activity (labor $L$), and the substitutive capacity of
equipment (substitutive work $P$). The technological properties of production
equipment are characterized by the technological coefficients $\overline
\lambda$ and $\overline \varepsilon$, indicating the amount of labor and energy
required to engaged with a unit of equipment. The production function can
assume various forms, none of which coincide with the popular Cobb-Douglas
expression, which seems to be erroneous in its core.

</details>


### [271] [Disentangling Interaction and Bias Effects in Opinion Dynamics of Large Language Models](https://arxiv.org/abs/2509.06858)
*Vincent C. Brockers,David A. Ehrlich,Viola Priesemann*

Main category: physics.soc-ph

TL;DR: 提出贝叶斯框架量化大语言模型模拟人类观点动态中的三种偏差，揭示观点轨迹特点及偏差影响，微调模型展示观点吸引子变化，凸显使用大语言模型作为人类行为代理的利弊。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型模拟人类观点动态时，真实交互效果常被系统偏差掩盖的问题。

Method: 提出贝叶斯框架来分离和量化三种偏差，对多步对话应用该框架，还在不同观点陈述集上微调大语言模型。

Result: 观点轨迹迅速收敛到共享吸引子，交互影响随时间减弱，不同大语言模型偏差影响不同，微调模型会使观点吸引子相应移动。

Conclusion: 该方法凸显了使用大语言模型作为人类行为代理的机会与陷阱。

Abstract: Large Language Models are increasingly used to simulate human opinion
dynamics, yet the effect of genuine interaction is often obscured by systematic
biases. We present a Bayesian framework to disentangle and quantify three such
biases: (i) a topic bias toward prior opinions in the training data; (ii) an
agreement bias favoring agreement irrespective of the question; and (iii) an
anchoring bias toward the initiating agent's stance. Applying this framework to
multi-step dialogues reveals that opinion trajectories tend to quickly converge
to a shared attractor, with the influence of the interaction fading over time,
and the impact of biases differing between LLMs. In addition, we fine-tune an
LLM on different sets of strongly opinionated statements (incl. misinformation)
and demonstrate that the opinion attractor shifts correspondingly. Exposing
stark differences between LLMs and providing quantitative tools to compare them
to human subjects in the future, our approach highlights both chances and
pitfalls in using LLMs as proxies for human behavior.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [272] [Statistical Inference for Misspecified Contextual Bandits](https://arxiv.org/abs/2509.06287)
*Yongyi Guo,Ziping Xu*

Main category: math.ST

TL;DR: 上下文多臂老虎机算法虽能实时自适应，但适应性给统计推断带来挑战。常用算法如LinUCB在模型错误指定时可能不收敛，本文提出一类保证收敛的算法并开发推断框架，模拟显示该方法表现好。


<details>
  <summary>Details</summary>
Motivation: 现有广泛使用的上下文多臂老虎机算法在奖励模型错误指定时可能不收敛，给统计推断带来障碍，而实际中常使用错误指定模型，因此需要解决该问题。

Method: 提出一类即使在模型错误指定时也保证收敛的算法，基于逆概率加权Z估计量开发通用推断框架并建立其渐近正态性和一致方差估计。

Result: 模拟研究证实，所提出的方法能提供稳健且数据高效的置信区间，在离线策略评估的特殊情况下能优于现有方法。

Conclusion: 设计具有内置收敛保证的自适应算法对实现稳定实验和有效的统计推断至关重要。

Abstract: Contextual bandit algorithms have transformed modern experimentation by
enabling real-time adaptation for personalized treatment and efficient use of
data. Yet these advantages create challenges for statistical inference due to
adaptivity. A fundamental property that supports valid inference is policy
convergence, meaning that action-selection probabilities converge in
probability given the context. Convergence ensures replicability of adaptive
experiments and stability of online algorithms. In this paper, we highlight a
previously overlooked issue: widely used algorithms such as LinUCB may fail to
converge when the reward model is misspecified, and such non-convergence
creates fundamental obstacles for statistical inference. This issue is
practically important, as misspecified models -- such as linear approximations
of complex dynamic system -- are often employed in real-world adaptive
experiments to balance bias and variance.
  Motivated by this insight, we propose and analyze a broad class of algorithms
that are guaranteed to converge even under model misspecification. Building on
this guarantee, we develop a general inference framework based on an
inverse-probability-weighted Z-estimator (IPW-Z) and establish its asymptotic
normality with a consistent variance estimator. Simulation studies confirm that
the proposed method provides robust and data-efficient confidence intervals,
and can outperform existing approaches that exist only in the special case of
offline policy evaluation. Taken together, our results underscore the
importance of designing adaptive algorithms with built-in convergence
guarantees to enable stable experimentation and valid statistical inference in
practice.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [273] [Several Performance Bounds on Decentralized Online Optimization are Highly Conservative and Potentially Misleading](https://arxiv.org/abs/2509.06466)
*Erwan Meunier,Julien M. Hendrickx*

Main category: math.OC

TL;DR: 使用性能估计问题方法分析去中心化在线优化算法，发现现有性能保证保守，部分算法通信短期无收益，调步长可改善性能。


<details>
  <summary>Details</summary>
Motivation: 分析去中心化在线优化算法的性能，解决现有性能保证可能导致算法选择失误的问题。

Method: 采用性能估计问题方法分析算法。

Result: 现有性能保证保守，部分算法短期通信无收益，调步长可使实际最坏情况性能遗憾节省达20%。

Conclusion: 可通过调整步长改善经典方法的性能。

Abstract: We analyze Decentralized Online Optimization algorithms using the Performance
Estimation Problem approach which allows, to automatically compute exact
worst-case performance of optimization algorithms. Our analysis shows that
several available performance guarantees are very conservative, sometimes by
multiple orders of magnitude, and can lead to misguided choices of algorithm.
Moreover, at least in terms of worst-case performance, some algorithms appear
not to benefit from inter-agent communications for a significant period of
time. We show how to improve classical methods by tuning their step-sizes, and
find that we can save up to 20% on their actual worst-case performance regret.

</details>


### [274] [GenAI on Wall Street -- Opportunities and Risk Controls](https://arxiv.org/abs/2509.05841)
*Jackie Shen*

Main category: math.OC

TL;DR: 概述生成式AI在金融行业（特别是投资银行）的新兴应用，强调要管理相关风险以促进其发展并保障金融行业。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI在金融行业应用带来的机遇与风险，寻求合理发展路径。

Method: 未提及具体方法

Result: 未提及具体结果

Conclusion: 兼顾生成式AI的利弊，可在AI初期加速其发展并保障金融行业。

Abstract: We give an overview on the emerging applications of GenAI in the financial
industry, especially within investment banks. Inherent to these exciting
opportunities is a new realm of risks that must be managed properly. By heeding
both the Yin and Yang sides of GenAI, we can accelerate its organic growth
while safeguarding the entire financial industry during this nascent era of AI.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [275] [Scalable Learning of One-Counter Automata via State-Merging Algorithms](https://arxiv.org/abs/2509.05762)
*Shibashis Guha,Anirban Majumdar,Prince Mathew,A. V. Sreejith*

Main category: cs.FL

TL;DR: 提出OPNI算法用于学习DROCA，结合主动学习方法，实验表明比现有算法更有效，并评估了学习可见单计数器自动机的性能。


<details>
  <summary>Details</summary>
Motivation: 提出有效学习确定性实时单计数器自动机（DROCA）的算法。

Method: 提出OPNI被动学习算法，将其与DROCA的主动学习相结合，并给出实现方法。

Result: 实验显示该方法比现有最先进算法扩展性更好，还评估了学习可见单计数器自动机的性能。

Conclusion: 所提方法在学习DROCA方面比现有算法更有效。

Abstract: We propose One-counter Positive Negative Inference (OPNI), a passive learning
algorithm for deterministic real-time one-counter automata (DROCA). Inspired by
the RPNI algorithm for regular languages, OPNI constructs a DROCA consistent
with any given valid sample set.
  We further present a method for combining OPNI with active learning of DROCA,
and provide an implementation of the approach. Our experimental results
demonstrate that this approach scales more effectively than existing
state-of-the-art algorithms. We also evaluate the performance of the proposed
approach for learning visibly one-counter automata.

</details>


### [276] [On Synthesis of Timed Regular Expressions](https://arxiv.org/abs/2509.06262)
*Ziran Wang,Jie An,Naijun Zhan,Miaomiao Zhang,Zhenya Zhang*

Main category: cs.FL

TL;DR: 本文聚焦定时正则表达式合成，证明问题可判定性，提出生成最小长度一致表达式的方法并评估。


<details>
  <summary>Details</summary>
Motivation: 合成与给定系统行为（含正负例）一致的定时正则表达式，用于指定网络物理系统实时行为。

Method: 先证明简单定时正则表达式合成问题的可判定性，再分两步生成最小长度一致表达式：枚举并修剪候选参数化定时正则表达式；将一致性要求编码为SMT公式求解参数时间约束。

Result: 对基准测试进行评估，包括目标定时模型随机生成的行为和案例研究。

Conclusion: 未在摘要中明确提及，但从研究内容看，方法在解决定时正则表达式合成问题上有一定效果。

Abstract: Timed regular expressions serve as a formalism for specifying real-time
behaviors of Cyber-Physical Systems. In this paper, we consider the synthesis
of timed regular expressions, focusing on generating a timed regular expression
consistent with a given set of system behaviors including positive and negative
examples, i.e., accepting all positive examples and rejecting all negative
examples. We first prove the decidability of the synthesis problem through an
exploration of simple timed regular expressions. Subsequently, we propose our
method of generating a consistent timed regular expression with minimal length,
which unfolds in two steps. The first step is to enumerate and prune candidate
parametric timed regular expressions. In the second step, we encode the
requirement that a candidate generated by the first step is consistent with the
given set into a Satisfiability Modulo Theories (SMT) formula, which is
consequently solved to determine a solution to parametric time constraints.
Finally, we evaluate our approach on benchmarks, including randomly generated
behaviors from target timed models and a case study.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [277] [Data-driven solar forecasting enables near-optimal economic decisions](https://arxiv.org/abs/2509.06925)
*Zhixiang Dai,Minghao Yin,Xuanhong Chen,Alberto Carpentieri,Jussi Leinonen,Boris Bonev,Chengzhe Zhong,Thorsten Kurth,Jingan Sun,Ram Cherukuri,Yuzhou Zhang,Ruihua Zhang,Farah Hariri,Xiaodong Ding,Chuanxiang Zhu,Dake Zhang,Yaodan Cui,Yuxi Lu,Yue Song,Bin He,Jie Chen,Yixin Zhu,Chenheng Xu,Maofeng Liu,Zeyi Niu,Wanpeng Qi,Xu Shan,Siyuan Xian,Ning Lin,Kairui Feng*

Main category: physics.geo-ph

TL;DR: 介绍SunCastNet预测系统，能提供高分辨率太阳辐射预测，结合强化学习减少运营遗憾，助力工业部门达商业可行性门槛，证明高分辨率预测有经济价值。


<details>
  <summary>Details</summary>
Motivation: 许多工商业者难决定是否采用分布式太阳能 - 电池系统，原因是缺乏快速、低成本和高分辨率的辐照度预测。

Method: 提出SunCastNet数据驱动预测系统，结合强化学习进行电池调度。

Result: 与稳健决策相比，减少76 - 93%的运营遗憾；25年投资回测中，使部分高排放工业部门达到12%内部收益率的商业可行性门槛。

Conclusion: 高分辨率、长周期太阳能预测可转化为可衡量的经济收益，支持近乎最优的能源运营，加速可再生能源部署。

Abstract: Solar energy adoption is critical to achieving net-zero emissions. However,
it remains difficult for many industrial and commercial actors to decide on
whether they should adopt distributed solar-battery systems, which is largely
due to the unavailability of fast, low-cost, and high-resolution irradiance
forecasts. Here, we present SunCastNet, a lightweight data-driven forecasting
system that provides 0.05$^\circ$, 10-minute resolution predictions of surface
solar radiation downwards (SSRD) up to 7 days ahead. SunCastNet, coupled with
reinforcement learning (RL) for battery scheduling, reduces operational regret
by 76--93\% compared to robust decision making (RDM). In 25-year investment
backtests, it enables up to five of ten high-emitting industrial sectors per
region to cross the commercial viability threshold of 12\% Internal Rate of
Return (IRR). These results show that high-resolution, long-horizon solar
forecasts can directly translate into measurable economic gains, supporting
near-optimal energy operations and accelerating renewable deployment.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [278] [Predicting Brain Morphogenesis via Physics-Transfer Learning](https://arxiv.org/abs/2509.05305)
*Yingjie Zhao,Yicheng Song,Fan Xu,Zhiping Xu*

Main category: q-bio.NC

TL;DR: 本文提出物理迁移学习框架解决大脑形态几何复杂性问题，构建数字库克服数据稀缺挑战，该方法表现好并验证了数字孪生技术在理解大脑形态方面的应用。


<details>
  <summary>Details</summary>
Motivation: 大脑形态的分形特征、区域各向异性和复杂曲率分布阻碍医学检查的定量分析，需解决其几何复杂性问题。

Method: 开发物理迁移学习框架，构建高保真连续力学建模数字库，将简单几何的非线性弹性物理嵌入神经网络并应用于大脑模型。

Result: 该物理迁移方法在特征表征和形态发生预测方面表现出色，提供了降维进化表示库。

Conclusion: 通过医学图像和专业知识验证了数字孪生技术在理解大脑形态复杂性方面的应用。

Abstract: Brain morphology is shaped by genetic and mechanical factors and is linked to
biological development and diseases. Its fractal-like features, regional
anisotropy, and complex curvature distributions hinder quantitative insights in
medical inspections. Recognizing that the underlying elastic instability and
bifurcation share the same physics as simple geometries such as spheres and
ellipses, we developed a physics-transfer learning framework to address the
geometrical complexity. To overcome the challenge of data scarcity, we
constructed a digital library of high-fidelity continuum mechanics modeling
that both describes and predicts the developmental processes of brain growth
and disease. The physics of nonlinear elasticity from simple geometries is
embedded into a neural network and applied to brain models. This
physics-transfer approach demonstrates remarkable performance in feature
characterization and morphogenesis prediction, highlighting the pivotal role of
localized deformation in dominating over the background geometry. The
data-driven framework also provides a library of reduced-dimensional
evolutionary representations that capture the essential physics of the highly
folded cerebral cortex. Validation through medical images and domain expertise
underscores the deployment of digital-twin technology in comprehending the
morphological complexity of the brain.

</details>


### [279] [Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster](https://arxiv.org/abs/2509.06426)
*Pembe Gizem Özdil,Chuanfang Ning,Jasper S. Phelps,Sibo Wang-Chen,Guy Elisha,Alexander Blanke,Auke Ijspeert,Pavan Ramdya*

Main category: q-bio.NC

TL;DR: 本文介绍果蝇腿部首个3D数据驱动肌肉骨骼模型，结合姿态数据实现肌肉驱动行为回放，还研究了被动关节属性对学习速度的影响，模型可用于研究运动控制和控制人工代理。


<details>
  <summary>Details</summary>
Motivation: 虽有果蝇神经系统等的重建，但缺少腿部肌肉解剖和物理模型，而此类模型对连接运动神经元活动和关节运动很重要。

Method: 基于高分辨率X射线扫描构建Hill型肌肉模型，给出利用形态成像数据构建肌肉模型和优化参数的流程，结合姿态数据在OpenSim中实现行为回放，在MuJoCo中训练模仿学习策略。

Result: 模拟不同行为中的肌肉活动预测出可实验验证的肌肉协同作用，发现阻尼和刚度有助于学习。

Conclusion: 该模型可用于研究实验易处理生物的运动控制，为生物力学如何促成复杂肢体运动提供见解，也可用于控制人工代理在模拟环境中产生自然柔顺的运动。

Abstract: Computational models are critical to advance our understanding of how neural,
biomechanical, and physical systems interact to orchestrate animal behaviors.
Despite the availability of near-complete reconstructions of the Drosophila
melanogaster central nervous system, musculature, and exoskeleton, anatomically
and physically grounded models of fly leg muscles are still missing. These
models provide an indispensable bridge between motor neuron activity and joint
movements. Here, we introduce the first 3D, data-driven musculoskeletal model
of Drosophila legs, implemented in both OpenSim and MuJoCo simulation
environments. Our model incorporates a Hill-type muscle representation based on
high-resolution X-ray scans from multiple fixed specimens. We present a
pipeline for constructing muscle models using morphological imaging data and
for optimizing unknown muscle parameters specific to the fly. We then combine
our musculoskeletal models with detailed 3D pose estimation data from behaving
flies to achieve muscle-actuated behavioral replay in OpenSim. Simulations of
muscle activity across diverse walking and grooming behaviors predict
coordinated muscle synergies that can be tested experimentally. Furthermore, by
training imitation learning policies in MuJoCo, we test the effect of different
passive joint properties on learning speed and find that damping and stiffness
facilitate learning. Overall, our model enables the investigation of motor
control in an experimentally tractable model organism, providing insights into
how biomechanics contribute to generation of complex limb movements. Moreover,
our model can be used to control embodied artificial agents to generate
naturalistic and compliant locomotion in simulated environments.

</details>


### [280] [Reward function compression facilitates goal-dependent reinforcement learning](https://arxiv.org/abs/2509.06810)
*Gaia Molinaro,Anne G. E. Collins*

Main category: q-bio.NC

TL;DR: 本文提出目标依赖学习受工作记忆支持，学习者会创建压缩奖励函数提升学习效率，通过实验和计算模型验证，揭示高效目标导向学习机制。


<details>
  <summary>Details</summary>
Motivation: 人类能以目标依赖方式为新的抽象结果赋值，但这种灵活性有认知成本，学习效率低，需研究其认知机制。

Method: 进行六项实验，利用计算建模。

Result: 学习受目标空间大小影响，但目标空间结构可压缩时学习改善；奖励处理速度与学习表现正相关。

Conclusion: 高效目标导向学习依赖将复杂目标信息压缩为稳定奖励函数，为内在动机神经科学提供新见解，可改进行为技术。

Abstract: Reinforcement learning agents learn from rewards, but humans can uniquely
assign value to novel, abstract outcomes in a goal-dependent manner. However,
this flexibility is cognitively costly, making learning less efficient. Here,
we propose that goal-dependent learning is initially supported by a
capacity-limited working memory system. With consistent experience, learners
create a "compressed" reward function (a simplified rule defining the goal)
which is then transferred to long-term memory and applied automatically upon
receiving feedback. This process frees up working memory resources, boosting
learning efficiency. We test this theory across six experiments. Consistent
with our predictions, our findings demonstrate that learning is parametrically
impaired by the size of the goal space, but improves when the goal space
structure allows for compression. We also find faster reward processing to
correlate with better learning performance, supporting the idea that as goal
valuation becomes more automatic, more resources are available for learning. We
leverage computational modeling to support this interpretation. Our work
suggests that efficient goal-directed learning relies on compressing complex
goal information into a stable reward function, shedding light on the cognitive
mechanisms of human motivation. These findings generate new insights into the
neuroscience of intrinsic motivation and could help improve behavioral
techniques that support people in achieving their goals.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [281] [Characterizing and Optimizing Realistic Workloads on a Commercial Compute-in-SRAM Device](https://arxiv.org/abs/2509.05451)
*Niansong Zhang,Wenbo Zhu,Courtney Golden,Dan Ilan,Hongzheng Chen,Christopher Batten,Zhiru Zhang*

Main category: cs.AR

TL;DR: 本文对商用计算内存一体SRAM设备GSI APU进行性能和能耗分析，提出优化方法，验证其在复杂应用中的可行性与节能优势。


<details>
  <summary>Details</summary>
Motivation: 以往对计算内存一体SRAM架构的评估多依赖模拟器或小原型，限制对其现实潜力的理解，因此要对商用设备进行综合评估。

Method: 对GSI APU进行性能和能耗表征，与CPU、GPU对比；引入分析框架；提出通信感知约简映射、合并DMA和广播友好数据布局三个优化方法。

Result: 在检索增强生成任务中，优化后系统检索加速4.8 - 6.6倍，端到端延迟改善1.1 - 1.8倍，性能与NVIDIA A6000 GPU相当，但能耗降低54.4 - 117.9倍。

Conclusion: 验证了计算内存一体SRAM在复杂现实应用中的可行性，为技术发展提供指导。

Abstract: Compute-in-SRAM architectures offer a promising approach to achieving higher
performance and energy efficiency across a range of data-intensive
applications. However, prior evaluations have largely relied on simulators or
small prototypes, limiting the understanding of their real-world potential. In
this work, we present a comprehensive performance and energy characterization
of a commercial compute-in-SRAM device, the GSI APU, under realistic workloads.
We compare the GSI APU against established architectures, including CPUs and
GPUs, to quantify its energy efficiency and performance potential. We introduce
an analytical framework for general-purpose compute-in-SRAM devices that
reveals fundamental optimization principles by modeling performance trade-offs,
thereby guiding program optimizations.
  Exploiting the fine-grained parallelism of tightly integrated memory-compute
architectures requires careful data management. We address this by proposing
three optimizations: communication-aware reduction mapping, coalesced DMA, and
broadcast-friendly data layouts. When applied to retrieval-augmented generation
(RAG) over large corpora (10GB--200GB), these optimizations enable our
compute-in-SRAM system to accelerate retrieval by 4.8$\times$--6.6$\times$ over
an optimized CPU baseline, improving end-to-end RAG latency by
1.1$\times$--1.8$\times$. The shared off-chip memory bandwidth is modeled using
a simulated HBM, while all other components are measured on the real
compute-in-SRAM device. Critically, this system matches the performance of an
NVIDIA A6000 GPU for RAG while being significantly more energy-efficient
(54.4$\times$-117.9$\times$ reduction). These findings validate the viability
of compute-in-SRAM for complex, real-world applications and provide guidance
for advancing the technology.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [282] [Causal Multi-fidelity Surrogate Forward and Inverse Models for ICF Implosions](https://arxiv.org/abs/2509.05510)
*Tyler E. Maltba,Ben S. Southworth,Jeffrey R. Haack,Marc L. Klasky*

Main category: physics.comp-ph

TL;DR: 本文针对惯性约束聚变（ICF）中的逆问题，构建因果、动态、多保真度降阶代理模型，结合机器学习解决逆问题并确定采样时机，展示了多种方法在高能量密度系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 惯性约束聚变中高维动态PDE约束优化问题极具挑战性，需寻找解决逆问题及设计优化的有效方法。

Method: 构建因果、动态、多保真度降阶代理模型，学习基础分析模型的控制器，结合机器学习模型和代理生成的数据解决逆问题。

Result: 代理界面模型精度高，机器学习模型能解决逆问题并确定最具信息性的采样时间。

Conclusion: 证明了算子学习、因果架构和物理归纳偏置可集成用于加速高能量密度系统的发现、设计和诊断。

Abstract: Continued progress in inertial confinement fusion (ICF) requires solving
inverse problems relating experimental observations to simulation input
parameters, followed by design optimization. However, such high dimensional
dynamic PDE-constrained optimization problems are extremely challenging or even
intractable. It has been recently shown that inverse problems can be solved by
only considering certain robust features. Here we consider the ICF capsule's
deuterium-tritium (DT) interface, and construct a causal, dynamic,
multifidelity reduced-order surrogate that maps from a time-dependent radiation
temperature drive to the interface's radius and velocity dynamics. The
surrogate targets an ODE embedding of DT interface dynamics, and is constructed
by learning a controller for a base analytical model using low- and
high-fidelity simulation training data with respect to radiation energy group
structure. After demonstrating excellent accuracy of the surrogate interface
model, we use machine learning (ML) models with surrogate-generated data to
solve inverse problems optimizing radiation temperature drive to reproduce
observed interface dynamics. For sparse snapshots in time, the ML model further
characterizes the most informative times at which to sample dynamics.
Altogether we demonstrate how operator learning, causal architectures, and
physical inductive bias can be integrated to accelerate discovery, design, and
diagnostics in high-energy-density systems.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [283] [Measuring General Associations in Time Series: An Adaptation and Empirical Evaluation of the CODEC Coefficient in Determining Autoregressive Dynamics](https://arxiv.org/abs/2509.06111)
*Juan Pablo Montaño,Mario E. Arrieta-Prieto*

Main category: stat.ME

TL;DR: 研究探索模型无关关联度量指导时间序列滞后选择，通过模拟评估CODEC等方法性能，CODEC在非线性和非平稳环境表现优，Pearson在线性模型好，CODEC可作探索性滞后识别工具。


<details>
  <summary>Details</summary>
Motivation: 因将滞后数作为超参数计算负担大，识别自回归模型中滞后数仍是开放问题，需探索合适方法进行滞后选择。

Method: 探索包括Pearson、Spearman和CODEC在内的模型无关关联度量，改编并实现基于CODEC的特征排序算法（CODEC - FOCI），通过大量模拟评估性能。

Result: CODEC在非线性和非平稳环境、大样本时优于经典基于相关的度量，Pearson在纯线性模型表现更好，应用于基准数据集表明CODEC识别的滞后结构与文献一致。

Conclusion: CODEC有潜力成为时间序列分析中探索性滞后识别的实用、无模型工具。

Abstract: Identifying the number of lags to include in an autoregressive model remains
an open research problem due to the computational burden of treating it as a
hyperparameter, especially in complex models. This study explores
model-agnostic association measures, including Pearson, Spearman, and an
adaptation of the recently proposed conditional dependence coefficient (CODEC),
for guiding lag selection in time series. We adapt and implement the
CODEC-based Feature Ordering by Conditional Independence (CODEC-FOCI) algorithm
and evaluate its performance through extensive simulations across linear,
nonlinear, stationary, nonstationary, seasonal, and heteroskedastic processes.
Results show that CODEC outperforms classical correlation-based measures in
nonlinear and nonstationary settings, especially for large sample sizes. In
contrast, Pearson performs better in purely linear models. Applications to
benchmark datasets confirm that the CODEC approach identifies lag structures
consistent with those reported in the literature. These findings highlight
CODEC's potential as a practical, model-free tool for exploratory lag
identification in time series analysis.

</details>


### [284] [Interpretable dimension reduction for compositional data](https://arxiv.org/abs/2509.05563)
*Junyoung Park,Cheolwoo Park,Jeongyoun Ahn*

Main category: stat.ME

TL;DR: 提出用于成分数据可解释降维的新框架，避免额外变换和零插补，应用于实际微生物组数据集展现出优势。


<details>
  <summary>Details</summary>
Motivation: 高维成分数据因单纯形约束和过多零值带来统计挑战，传统降维方法有缺陷，需新方法。

Method: 引入新框架，软化合并操作，结合充分降维定义中心成分子空间，提出成分核降维（CKDR）方法。

Result: CKDR估计器具有一致性、稀疏性，自带预测模型，应用于实际数据集能发现有意义生物模式。

Conclusion: 该方法为分析高维成分数据开辟新途径，是强大的图形探索工具。

Abstract: High-dimensional compositional data, such as those from human microbiome
studies, pose unique statistical challenges due to the simplex constraint and
excess zeros. While dimension reduction is indispensable for analyzing such
data, conventional approaches often rely on log-ratio transformations that
compromise interpretability and distort the data through ad hoc zero
replacements. We introduce a novel framework for interpretable dimension
reduction of compositional data that avoids extra transformations and zero
imputations. Our approach generalizes the concept of amalgamation by softening
its operation, mapping high-dimensional compositions directly to a
lower-dimensional simplex, which can be visualized in ternary plots. The
framework further provides joint visualization of the reduction matrix,
enabling intuitive, at-a-glance interpretation. To achieve optimal reduction
within our framework, we incorporate sufficient dimension reduction, which
defines a new identifiable objective: the central compositional subspace. For
estimation, we propose a compositional kernel dimension reduction (CKDR)
method. The estimator is provably consistent, exhibits sparsity that reveals
underlying amalgamation structures, and comes with an intrinsic predictive
model for downstream analyses. Applications to real microbiome datasets
demonstrate that our approach provides a powerful graphical exploration tool
for uncovering meaningful biological patterns, opening a new pathway for
analyzing high-dimensional compositional data.

</details>


### [285] [Beyond ATE: Multi-Criteria Design for A/B Testing](https://arxiv.org/abs/2509.05864)
*Jiachun Li,Kaining Shi,David Simchi-Levi*

Main category: stat.ME

TL;DR: 本文研究自适应实验中社会福利损失与统计准确性的权衡，建立多目标优化问题上下界，用帕累托最优表征最优实验设计条件，证明设计能实现最优实验后福利，还开发差分隐私算法。


<details>
  <summary>Details</summary>
Motivation: 现有A/B测试研究多关注估计准确性，实际应用还需考虑福利或收入损失等额外目标，且数据敏感需隐私保障。

Method: 建立多目标优化问题的上下界，运用帕累托最优概念，开发差分隐私算法。

Result: 得到多目标优化问题的匹配上下界，证明帕累托最优自适应设计能实现最优实验后福利，差分隐私算法能达到既定下界。

Conclusion: 可在自适应实验中权衡社会福利损失和统计准确性，实现最优实验后福利，且能以可忽略成本实现隐私保护。

Abstract: A/B testing is a widely adopted methodology for estimating conditional
average treatment effects (CATEs) in both clinical trials and online platforms.
While most existing research has focused primarily on maximizing estimation
accuracy, practical applications must also account for additional
objectives-most notably welfare or revenue loss. In many settings, it is
critical to administer treatments that improve patient outcomes or to implement
plans that generate greater revenue from customers. Within a machine learning
framework, such objectives are naturally captured through the notion of
cumulative regret. In this paper, we investigate the fundamental trade-off
between social welfare loss and statistical accuracy in (adaptive) experiments
with heterogeneous treatment effects. We establish matching upper and lower
bounds for the resulting multi-objective optimization problem and employ the
concept of Pareto optimality to characterize the necessary and sufficient
conditions for optimal experimental designs. Beyond estimating CATEs,
practitioners often aim to deploy treatment policies that maximize welfare
across the entire population. We demonstrate that our Pareto-optimal adaptive
design achieves optimal post-experiment welfare, irrespective of the
in-experiment trade-off between accuracy and welfare. Furthermore, since
clinical and commercial data are often highly sensitive, it is essential to
incorporate robust privacy guarantees into any treatment-allocation mechanism.
To this end, we develop differentially private algorithms that continue to
achieve our established lower bounds, showing that privacy can be attained at
negligible cost.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [286] [Degree Realization by Bipartite Cactus Graphs](https://arxiv.org/abs/2509.06194)
*Amotz Bar-Noy,Toni Bohnlein,David Peleg,Yingli Ran,Dror Rawitz*

Main category: cs.DM

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The \textsc{Degree Realization} problem with respect to a graph family
$\mathcal{F}$ is defined as follows. The input is a sequence $d$ of $n$
positive integers, and the goal is to decide whether there exists a graph $G
\in \mathcal{F}$ whose degrees correspond to $d$. The main challenges are to
provide a precise characterization of all the sequences that admit a
realization in $\mathcal{F}$ and to design efficient algorithms that construct
one of the possible realizations, if one exists.
  This paper studies the problem of realizing degree sequences by bipartite
cactus graphs (where the input is given as a single sequence, without the
bi-partition). A characterization of the sequences that have a cactus
realization is already known [28]. In this paper, we provide a systematic way
to obtain such a characterization, accompanied by a realization algorithm. This
allows us to derive a characterization for bipartite cactus graphs, and as a
byproduct, also for several other interesting sub-families of cactus graphs,
including bridge-less cactus graphs and core cactus graphs, as well as for the
bipartite sub-families of these families.

</details>


<div id='math.GT'></div>

# math.GT [[Back]](#toc)

### [287] [On detection probabilities of link invariants](https://arxiv.org/abs/2509.05574)
*Abel Lacabanne,Daniel Tubbenhauer,Pedro Vaz,Victor L. Zhang*

Main category: math.GT

TL;DR: 证明n - 交叉交替链环的某些不变量检测率随n指数衰减，大数据分析表明部分边缘情况也有相同渐近行为。


<details>
  <summary>Details</summary>
Motivation: 研究链环不变量对交替链环的检测率情况。

Method: 理论证明和大数据分析。

Result: n - 交叉交替链环的定向突变不敏感的链环不变量检测率随n指数衰减，大数据分析为边缘情况提供相同渐近行为证据。

Conclusion: 这些链环不变量检测交替链环的概率为零，部分边缘情况可能有相同渐近行为。

Abstract: We prove that the detection rate of n-crossing alternating links by link
invariants insensitive to oriented mutation decays exponentially in n, implying
that they detect alternating links with probability zero. This phenomenon
applies broadly, in particular to quantum invariants such as the Jones or
HOMFLYPT polynomials. We also use a big data approach to analyze several
borderline cases (e.g. integral Khovanov or HOMFLYPT homologies), where our
arguments almost, but not quite, apply, and we provide evidence that they too
exhibit the same asymptotic behavior.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [288] [Compare: A Framework for Scientific Comparisons](https://arxiv.org/abs/2509.06412)
*Moritz Staudinger,Wojciech Kusa,Matteo Cancellieri,David Pride,Petr Knoth,Allan Hanbury*

Main category: cs.DL

TL;DR: 提出Compare框架以解决学术文献比较难题，可进行定性、支持引用的比较。


<details>
  <summary>Details</summary>
Motivation: 现有工具无法支持跨机构或出版物的结构化、定性比较，而识别机构协同等任务愈发困难。

Method: 利用Retrieval - Augmented Generation对不断变化的数据源进行操作，实现长上下文知识合成，以用户定义问题和在线资源自动检索驱动。

Result: Compare框架可让用户在机构和出版物粒度上探索和分析研究的重叠与差异。

Conclusion: Compare框架超越传统科学计量工具，能提供定性、有引用支持的比较。

Abstract: Navigating the vast and rapidly increasing sea of academic publications to
identify institutional synergies, benchmark research contributions and pinpoint
key research contributions has become an increasingly daunting task, especially
with the current exponential increase in new publications. Existing tools
provide useful overviews or single-document insights, but none supports
structured, qualitative comparisons across institutions or publications.
  To address this, we demonstrate Compare, a novel framework that tackles this
challenge by enabling sophisticated long-context comparisons of scientific
contributions. Compare empowers users to explore and analyze research overlaps
and differences at both the institutional and publication granularity, all
driven by user-defined questions and automatic retrieval over online resources.
For this we leverage on Retrieval-Augmented Generation over evolving data
sources to foster long context knowledge synthesis. Unlike traditional
scientometric tools, Compare goes beyond quantitative indicators by providing
qualitative, citation-supported comparisons.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [289] [Fixed-Point Theorems and the Ethics of Radical Transparency: A Logic-First Treatment](https://arxiv.org/abs/2509.06055)
*Faruk Alpay,Hamdi Alakkad*

Main category: math.LO

TL;DR: 本文构建形式框架分析激进透明的内在局限性，得出关键结果并为透明设计提供数学基础，指出最优策略是部分透明。


<details>
  <summary>Details</summary>
Motivation: 分析激进透明的内在局限性，为透明设计提供理论依据。

Method: 基于数学逻辑和序理论建立形式框架，运用多种定理进行论证。

Result: 得出多个关键结果，如不可能定理、不动点论证等。

Conclusion: 最优策略必然是部分透明，需平衡问责与策略博弈和悖论，给出均衡分析和格论最优条件。

Abstract: This paper establishes a formal framework, grounded in mathematical logic and
order theory, to analyze the inherent limitations of radical transparency. We
demonstrate that self-referential disclosure policies inevitably encounter
fixed-point phenomena and diagonalization barriers, imposing fundamental
trade-offs between openness and stability.
  Key results include: (i) an impossibility theorem showing no sufficiently
expressive system can define a total, consistent transparency predicate for its
own statements; (ii) a categorical fixed-point argument (Lawvere) for the
inevitability of self-referential equilibria; (iii) order-theoretic design
theorems (Knaster-Tarski) proving extremal fixed points exist and that the
least fixed point minimizes a formal ethical risk functional; (iv) a
construction for consistent partial transparency using Kripkean truth; (v) an
analysis of self-endorsement hazards via L\"ob's Theorem; (vi) a
recursion-theoretic exploitation theorem (Kleene) formalizing Goodhart's Law
under full disclosure; (vii) an exploration of non-classical logics for
circumventing classical paradoxes; and (viii) a modal $\mu$-calculus
formulation for safety invariants under iterative disclosure.
  Our analysis provides a mathematical foundation for transparency design,
proving that optimal policies are necessarily partial and must balance
accountability against strategic gaming and paradox. We conclude with
equilibrium analysis and lattice-theoretic optimality conditions, offering a
principled calculus for ethical disclosure in complex systems.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [290] [Repeating vs. Non-Repeating FRBs: A Deep Learning Approach To Morphological Characterization](https://arxiv.org/abs/2509.06208)
*Bikash Kharel,Emmanuel Fonseca,Charanjot Brar,Afrokk Khan,Lluis Mas-Ribas,Swarali Shivraj Patil,Paul Scholz,Seth Robert Siegel,David C. Stenning*

Main category: astro-ph.HE

TL;DR: 提出基于CHIME/FRB目录2动态频谱形态的深度学习方法对快速射电暴分类，用预训练ConvNext，结果好且省资源。


<details>
  <summary>Details</summary>
Motivation: 对快速射电暴进行分类，探究CHIME重复和非重复事件形态差异。

Method: 采用预训练ConvNext进行迁移学习，将快速射电暴去色散动态频谱分类，还使用数学模型解释深度学习模型。

Result: 微调后模型分类指标高，减少训练时间和计算量，表明目录2中重复和非重复事件存在形态差异。

Conclusion: 微调后的深度学习模型可用于推理，在大数据集训练下更有意义。

Abstract: We present a deep learning approach to classify fast radio bursts (FRBs)
based purely on morphology as encoded on recorded dynamic spectrum from
CHIME/FRB Catalog 2. We implemented transfer learning with a pretrained
ConvNext architecture, exploiting its powerful feature extraction ability.
ConvNext was adapted to classify dedispersed dynamic spectra (which we treat as
images) of the FRBs into one of the two sub-classes, i.e., repeater and
non-repeater, based on their various temporal and spectral properties and
relation between the sub-pulse structures. Additionally, we also used
mathematical model representation of the total intensity data to interpret the
deep learning model. Upon fine-tuning the pretrained ConvNext on the FRB
spectrograms, we were able to achieve high classification metrics while
substantially reducing training time and computing power as compared to
training a deep learning model from scratch with random weights and biases
without any feature extraction ability. Importantly, our results suggest that
the morphological differences between CHIME repeating and non-repeating events
persist in Catalog 2 and the deep learning model leveraged these differences
for classification. The fine-tuned deep learning model can be used for
inference, which enables us to predict whether an FRB's morphology resembles
that of repeaters or non-repeaters. Such inferences may become increasingly
significant when trained on larger data sets that will exist in the near
future.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [291] [Orchestrator: Active Inference for Multi-Agent Systems in Long-Horizon Tasks](https://arxiv.org/abs/2509.05651)
*Lukas Beckenbauer,Johannes-Lucas Loewe,Ge Zheng,Alexandra Brintrup*

Main category: cs.MA

TL;DR: 提出Orchestrator框架优化LLM增强多智能体系统在复杂非线性任务中的性能，在迷宫难题中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 复杂非线性任务因部分可观测性和次优协调，对LLM增强的多智能体系统构成挑战，需优化系统性能。

Method: 提出Orchestrator框架，利用受注意力启发的自涌现协调和反思性基准测试，引入监测机制，使用主动推理基准来优化系统行为。

Result: 在一系列复杂度递增的迷宫难题中评估，证明该框架能增强动态、非线性环境中长时目标的协调和性能。

Conclusion: Orchestrator框架能有效缓解部分可观测性的影响，使智能体更高效地逼近全局任务解决方案。

Abstract: Complex, non-linear tasks challenge LLM-enhanced multi-agent systems (MAS)
due to partial observability and suboptimal coordination. We propose
Orchestrator, a novel MAS framework that leverages attention-inspired
self-emergent coordination and reflective benchmarking to optimize global task
performance. Orchestrator introduces a monitoring mechanism to track
agent-environment dynamics, using active inference benchmarks to optimize
system behavior. By tracking agent-to-agent and agent-to-environment
interaction, Orchestrator mitigates the effects of partial observability and
enables agents to approximate global task solutions more efficiently. We
evaluate the framework on a series of maze puzzles of increasing complexity,
demonstrating its effectiveness in enhancing coordination and performance in
dynamic, non-linear environments with long-horizon objectives.

</details>


### [292] [Systematic Evaluation of Multi-modal Approaches to Complex Player Profile Classification](https://arxiv.org/abs/2509.05624)
*Jason Starace,Terence Soule*

Main category: cs.MA

TL;DR: 评估多模态分类在玩家建模中的应用，多模态集成比仅用行为数据准确率高，为复杂玩家建模设基准。


<details>
  <summary>Details</summary>
Motivation: 现代自适应游戏需深入理解玩家，但现有模型的分类方式不能捕捉玩家多样性，行为聚类也存在局限性。

Method: 结合行为遥测和语义上下文，用19413个游戏会话，对比仅行为基线方法和多模态方法。

Result: 传统聚类36类分类准确率仅10%，多模态LSTM提升至21%，非中性和中性配置文件准确率不同。

Conclusion: 为复杂玩家建模建立基准，表明基于个性的自适应需对话交互，36类规模评估为构建自适应游戏提供指导。

Abstract: Modern adaptive games require nuanced player understanding, yet most models
use simplified 5-10 category taxonomies that fail to capture diversity.
Behavioral clustering cannot distinguish players with different motivations who
act similarly. We present a systematic evaluation of multi-modal classification
at scale, combining behavioral telemetry with semantic context to support 36
player profiles. Using 19,413 gameplay sessions from an AI-controlled
text-based RPG, we compared behavioral-only baselines with multi-modal
approaches that integrate action sequences and semantic descriptions.
Traditional clustering achieved only 10% accuracy for 36-category
classification, limited by semantic conflation where opposite actions produced
identical features. Our multi-modal LSTM processing action-text pairs improved
accuracy to 21%, showing both potential and limits of non-conversational data.
Analysis by behavioral complexity revealed that non-neutral profiles reached
42% accuracy (15x above random), while neutral profiles dropped to 25% (9x
above random). Identical actions such as "help the merchant" cannot reveal
whether a player is neutral or strategically waiting. Without access to
reasoning, even multi-modal models struggle, though above-baseline results
confirm a meaningful signal. Since prediction beyond 20 categories remains
unexplored, our findings establish benchmarks for complex player modeling.
Behavioral data alone plateaus near 10% for 36 categories, while multi-modal
integration enables 25%. For designers, this shows that personality-based
adaptation requires conversational interaction, as predefined choices cannot
capture intent. Our evaluation at 36-category scale offers guidance for
building adaptive games that better understand their players.

</details>


### [293] [HECATE: An ECS-based Framework for Teaching and Developing Multi-Agent Systems](https://arxiv.org/abs/2509.06431)
*Arthur Casals,Anarosa A. F. Brandão*

Main category: cs.MA

TL;DR: 介绍基于ECS架构模式的HECATE框架，可弥合分布式系统工程与MAS开发差距，介绍其架构等并展示对不同代理模型的支持。


<details>
  <summary>Details</summary>
Motivation: 弥合分布式系统工程与MAS开发之间的差距，简化MAS开发，减少对专业代理知识的需求。

Method: 采用Entity - Component - System架构模式，从分布式系统角度设计多智能体系统，将代理概念集成到DS领域。

Result: 展示了框架的架构、核心组件和实现方法，证明其支持不同的代理模型。

Conclusion: HECATE框架能有效弥合分布式系统工程与MAS开发的差距，简化MAS开发。

Abstract: This paper introduces HECATE, a novel framework based on the
Entity-Component-System (ECS) architectural pattern that bridges the gap
between distributed systems engineering and MAS development. HECATE is built
using the Entity-Component-System architectural pattern, leveraging
data-oriented design to implement multiagent systems. This approach involves
engineering multiagent systems (MAS) from a distributed systems (DS)
perspective, integrating agent concepts directly into the DS domain. This
approach simplifies MAS development by (i) reducing the need for specialized
agent knowledge and (ii) leveraging familiar DS patterns and standards to
minimize the agent-specific knowledge required for engineering MAS. We present
the framework's architecture, core components, and implementation approach,
demonstrating how it supports different agent models.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [294] [TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching Vietnamese-English Speech Recognition](https://arxiv.org/abs/2509.05983)
*Minh N. H. Nguyen,Anh Nguyen Tran,Dung Truong Dinh,Nam Van Vo*

Main category: cs.SD

TL;DR: 提出用于越南语 - 英语代码切换自动语音识别的两阶段以音素为中心模型（TSPC），实验显示其优于现有基线，降低了字错误率。


<details>
  <summary>Details</summary>
Motivation: 现有自动语音识别系统难以捕捉代码切换场景中固有的语音变化，尤其是越南语和英语的代码切换情况。

Method: 提出两阶段以音素为中心模型（TSPC），采用以音素为中心的方法，基于扩展的越南语音素集作为中间表示进行混合语言建模。

Result: TSPC在越南语 - 英语代码切换自动语音识别中始终优于现有基线，如PhoWhisper - base，在减少训练资源的情况下实现了20.8%的较低字错误率。

Conclusion: 基于语音的两阶段架构可实现音素适应和语言转换，能提升复杂代码切换场景下的自动语音识别性能。

Abstract: Code-switching (CS) presents a significant challenge for general Auto-Speech
Recognition (ASR) systems. Existing methods often fail to capture the subtle
phonological shifts inherent in CS scenarios. The challenge is particularly
difficult for language pairs like Vietnamese and English, where both distinct
phonological features and the ambiguity arising from similar sound recognition
are present. In this paper, we propose a novel architecture for
Vietnamese-English CS ASR, a Two-Stage Phoneme-Centric model (TSPC). The TSPC
employs a phoneme-centric approach, built upon an extended Vietnamese phoneme
set as an intermediate representation to facilitate mixed-lingual modeling.
Experimental results demonstrate that TSPC consistently outperforms existing
baselines, including PhoWhisper-base, in Vietnamese-English CS ASR, achieving a
significantly lower word error rate of 20.8\% with reduced training resources.
Furthermore, the phonetic-based two-stage architecture enables phoneme
adaptation and language conversion to enhance ASR performance in complex CS
Vietnamese-English ASR scenarios.

</details>


### [295] [DreamAudio: Customized Text-to-Audio Generation with Diffusion Models](https://arxiv.org/abs/2509.06027)
*Yi Yuan,Xubo Liu,Haohe Liu,Xiyuan Kang,Zhuo Chen,Yuxuan Wang,Mark D. Plumbley,Wenwu Wang*

Main category: cs.SD

TL;DR: 提出DreamAudio用于定制化文本到音频生成，实验表明其能生成与定制音频特征高度一致且与输入文本提示对齐的音频，在通用任务中表现相当，并提供人类参与数据集作为基准。


<details>
  <summary>Details</summary>
Motivation: 现有文本到音频模型难以精确控制特定声音的细粒度声学特征，用户难以生成所需音频片段。

Method: 提出新框架使模型能从用户提供的参考概念中识别听觉信息用于音频生成，开发两种数据集用于训练和测试定制系统。

Result: DreamAudio生成的音频样本与定制音频特征高度一致，与输入文本提示对齐良好，在通用文本到音频任务中表现相当。

Conclusion: DreamAudio可实现定制化文本到音频生成，还提供了定制生成任务的基准数据集。

Abstract: With the development of large-scale diffusion-based and
language-modeling-based generative models, impressive progress has been
achieved in text-to-audio generation. Despite producing high-quality outputs,
existing text-to-audio models mainly aim to generate semantically aligned sound
and fall short on precisely controlling fine-grained acoustic characteristics
of specific sounds. As a result, users that need specific sound content may
find it challenging to generate the desired audio clips. In this paper, we
present DreamAudio for customized text-to-audio generation (CTTA).
Specifically, we introduce a new framework that is designed to enable the model
to identify auditory information from user-provided reference concepts for
audio generation. Given a few reference audio samples containing personalized
audio events, our system can generate new audio samples that include these
specific events. In addition, two types of datasets are developed for training
and testing the customized systems. The experiments show that the proposed
model, DreamAudio, generates audio samples that are highly consistent with the
customized audio features and aligned well with the input text prompts.
Furthermore, DreamAudio offers comparable performance in general text-to-audio
tasks. We also provide a human-involved dataset containing audio events from
real-world CTTA cases as the benchmark for customized generation tasks.

</details>


### [296] [MeanFlow-Accelerated Multimodal Video-to-Audio Synthesis via One-Step Generation](https://arxiv.org/abs/2509.06389)
*Xiaoran Yang,Jianxuan Yang,Xinyue Guo,Haoyu Wang,Ningning Pan,Gongping Huang*

Main category: cs.SD

TL;DR: 提出MeanFlow加速模型解决视频到音频合成效率瓶颈，结合标量缩放机制，实验表明在VTA和TTA任务中提升推理速度且不降低感知质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频到音频合成方法存在合成质量和推理效率的权衡问题，如基于流匹配的模型推理速度慢。

Method: 引入MeanFlow加速模型，用平均速度表征流场实现一步生成；采用标量缩放机制平衡条件和无条件预测。

Result: 在VTA和TTA合成任务中，将MeanFlow融入网络显著提高推理速度，且不影响感知质量。

Conclusion: MeanFlow加速模型能有效解决视频到音频合成的效率瓶颈问题。

Abstract: A key challenge in synthesizing audios from silent videos is the inherent
trade-off between synthesis quality and inference efficiency in existing
methods. For instance, flow matching based models rely on modeling
instantaneous velocity, inherently require an iterative sampling process,
leading to slow inference speeds. To address this efficiency bottleneck, we
introduce a MeanFlow-accelerated model that characterizes flow fields using
average velocity, enabling one-step generation and thereby significantly
accelerating multimodal video-to-audio (VTA) synthesis while preserving audio
quality, semantic alignment, and temporal synchronization. Furthermore, a
scalar rescaling mechanism is employed to balance conditional and unconditional
predictions when classifier-free guidance (CFG) is applied, effectively
mitigating CFG-induced distortions in one step generation. Since the audio
synthesis network is jointly trained with multimodal conditions, we further
evaluate it on text-to-audio (TTA) synthesis task. Experimental results
demonstrate that incorporating MeanFlow into the network significantly improves
inference speed without compromising perceptual quality on both VTA and TTA
synthesis tasks.

</details>


### [297] [The First Voice Timbre Attribute Detection Challenge](https://arxiv.org/abs/2509.06635)
*Liping Chen,Jinghao He,Zhengyan Sheng,Kong Aik Lee,Zhen-Hua Ling*

Main category: cs.SD

TL;DR: NCMMSC 2025特别会议举办首个音色属性检测挑战，在VCTK - RVA数据集评估，6队参赛，5队提供方法描述。


<details>
  <summary>Details</summary>
Motivation: 聚焦音色可解释性，比较特定音色描述维度下两个语音发声的强度。

Method: 参与者开发系统并提交输出，组织者评估性能并反馈。

Result: 有六支队伍提交了输出，五支队伍提供了方法论描述。

Conclusion: 文档未明确提及结论相关内容。

Abstract: The first voice timbre attribute detection challenge is featured in a special
session at NCMMSC 2025. It focuses on the explainability of voice timbre and
compares the intensity of two speech utterances in a specified timbre
descriptor dimension. The evaluation was conducted on the VCTK-RVA dataset.
Participants developed their systems and submitted their outputs to the
organizer, who evaluated the performance and sent feedback to them. Six teams
submitted their outputs, with five providing descriptions of their
methodologies.

</details>


### [298] [AnalysisGNN: Unified Music Analysis with Graph Neural Networks](https://arxiv.org/abs/2509.06654)
*Emmanouil Karystinaios,Johannes Hentschel,Markus Neuwirth,Gerhard Widmer*

Main category: cs.SD

TL;DR: 提出AnalysisGNN框架用于综合乐谱分析，集成异质标注符号数据集，实验表明其性能与传统方法相当且更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有计算音乐分析方法通常针对特定分析领域，缺乏综合性方法。

Method: 引入AnalysisGNN框架，采用数据洗牌策略、自定义加权多任务损失和任务特定分类器的logit融合，还集成非和弦音预测模块。

Result: AnalysisGNN性能与传统静态数据集方法相当，对跨多个异质语料库的领域转移和标注不一致性具有更强的恢复能力。

Conclusion: AnalysisGNN是一种有效的综合乐谱分析方法，能处理异质标注数据集。

Abstract: Recent years have seen a boom in computational approaches to music analysis,
yet each one is typically tailored to a specific analytical domain. In this
work, we introduce AnalysisGNN, a novel graph neural network framework that
leverages a data-shuffling strategy with a custom weighted multi-task loss and
logit fusion between task-specific classifiers to integrate heterogeneously
annotated symbolic datasets for comprehensive score analysis. We further
integrate a Non-Chord-Tone prediction module, which identifies and excludes
passing and non-functional notes from all tasks, thereby improving the
consistency of label signals. Experimental evaluations demonstrate that
AnalysisGNN achieves performance comparable to traditional static-dataset
approaches, while showing increased resilience to domain shifts and annotation
inconsistencies across multiple heterogeneous corpora.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [299] [Self-Driving Laboratory Optimizes the Lower Critical Solution Temperature of Thermoresponsive Polymers](https://arxiv.org/abs/2509.05351)
*Guoyue Xu,Renzheng Zhang,Tengfei Luo*

Main category: cond-mat.soft

TL;DR: 开发低成本“节俭孪生”平台优化聚（N - 异丙基丙烯酰胺）LCST，加速功能聚合物设计发现。


<details>
  <summary>Details</summary>
Motivation: 克服传统试错材料发现的低效问题，开发集成数据驱动决策的自主实验室。

Method: 开发低成本平台，集成机器人流体处理、在线传感器和贝叶斯优化，探索多组分盐溶液空间。

Result: 平台在最少实验次数内收敛到目标特性，能战略探索参数空间、从‘偏离目标’结果学习并自我纠正。

Conclusion: 此工作降低自主实验门槛，加速功能聚合物设计与发现。

Abstract: To overcome the inherent inefficiencies of traditional trial-and-error
materials discovery, the scientific community is increasingly developing
autonomous laboratories that integrate data-driven decision-making into
closed-loop experimental workflows. In this work, we realize this concept for
thermoresponsive polymers by developing a low-cost, "frugal twin" platform for
the optimization of the lower critical solution temperature (LCST) of
poly(N-isopropylacrylamide) (PNIPAM). Our system integrates robotic
fluid-handling, on-line sensors, and Bayesian optimization (BO) that navigates
the multi-component salt solution spaces to achieve user-specified LCST
targets. The platform demonstrates convergence to target properties within a
minimal number of experiments. It strategically explores the parameter space,
learns from informative "off-target" results, and self-corrects to achieve the
final targets. By providing an accessible and adaptable blueprint, this work
lowers the barrier to entry for autonomous experimentation and accelerates the
design and discovery of functional polymers.

</details>


### [300] [Topological Regularization for Force Prediction in Active Particle Suspension with EGNN and Persistent Homology](https://arxiv.org/abs/2509.06574)
*Sadra Saremi,Amirhossein Ahmadkhan Kordbacheh*

Main category: cond-mat.soft

TL;DR: 提出多尺度框架结合三种学习工具，利用高分辨率数据，经多步处理实现对活性粒子全力网络的预测。


<details>
  <summary>Details</summary>
Motivation: 捕捉活性粒子动力学问题需耦合精细尺度流体动力学与大规模集体效应，问题极具挑战性。

Method: 提出多尺度框架，结合三种学习工具。先以高分辨率格子玻尔兹曼快照为输入，再用E(2)等变图神经网络预测粒子间相互作用力，最后用物理信息神经网络结合应力数据更新局部估计。

Result: 实现了整体的、高度数据驱动的全力网络预测。

Conclusion: 该框架能兼顾物理基础和活性物质典型的多尺度结构。

Abstract: Capturing the dynamics of active particles, i.e., small self-propelled agents
that both deform and are deformed by a fluid in which they move is a formidable
problem as it requires coupling fine scale hydrodynamics with large scale
collective effects. So we present a multi-scale framework that combines the
three learning-driven tools to learn in concert within one pipeline. We use
high-resolution Lattice Boltzmann snapshots of fluid velocity and particle
stresses in a periodic box as input to the learning pipeline. the second step
takes the morphology and positions orientations of particles to predict
pairwise interaction forces between them with a E(2)-equivariant graph neural
network that necessarily respect flat symmetries. Then, a physics-informed
neural network further updates these local estimates by summing over them with
a stress data using Fourier feature mappings and residual blocks that is
additionally regularized with a topological term (introduced by persistent
homology) to penalize unrealistically tangled or spurious connections. In
concert, these stages deliver an holistic highly-data driven full force network
prediction empathizing on the physical underpinnings together with emerging
multi-scale structure typical for active matter.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [301] [Vector-based loss functions for turbulent flow field inpainting](https://arxiv.org/abs/2509.05787)
*Samuel J. Baker,Shubham Goswami,Xiaohang Fang,Felix C. P. Leach*

Main category: physics.flu-dyn

TL;DR: 本文提出基于向量的损失函数用于湍流流场修复问题，结果表明该函数能改善多尺度流场模式预测，混合损失函数可兼顾多尺度行为和像素精度。


<details>
  <summary>Details</summary>
Motivation: 标准机器学习方法忽略数据的向量性质，无法很好保留数据物理特征，因此要引入利用向量相似度度量的损失函数。

Method: 开发基于向量的损失函数，并与U - Net模型结合用于湍流流场修复问题，使用TCC - III发动机的PIV数据进行测试。

Result: 基于向量的损失函数显著改善了多尺度流场模式的预测，混合损失函数能在保留多尺度行为和像素精度间取得良好平衡。

Conclusion: 基于向量的损失函数在处理具有向量性质的数据时表现更优，能更好保留数据物理特征。

Abstract: When developing scientific machine learning (ML) approaches, it is often
beneficial to embed knowledge of the physical system in question into the
training process. One way to achieve this is by leveraging the specific
characteristics of the data at hand. In the case of turbulent flows, fluid
velocities can be measured and recorded as multi-component vectors at discrete
points in space, using techniques such as particle image velocimetry (PIV) or
computational fluid mechanics (CFD). However, the vectorised nature of the data
is ignored by standard ML approaches, as widely-used loss functions such as the
mean-square error treat each component of a velocity vector in isolation.
Therefore, the aim of this work is to better preserve the physical
characteristics of the data by introducing loss functions that utilise vector
similarity metrics. To this end, vector-based loss functions are developed here
and implemented alongside a U-Net model for a turbulent flow field inpainting
problem, amounting to the prediction of velocity vectors inside large gaps in
PIV images. The intention is for the inpainting task to pose a significant
challenge for the ML models in order to shed light on their capabilities. The
test case uses PIV data from the highly turbulent flow in the well-known
Transparent Combustion Chamber III (TCC-III) engine. Loss functions based on
the cosine similarity and vector magnitude differences are proposed; the
results show that the vector-based loss functions lead to significantly
improved predictions of multi-scale flow patterns, while a hybrid (vector and
mean-square error) loss function enables a good compromise to be found between
preserving multi-scale behaviour and pixel-wise accuracy.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [302] [Hybrid Fourier Neural Operator-Plasma Fluid Model for Fast and Accurate Multiscale Simulations of High Power Microwave Breakdown](https://arxiv.org/abs/2509.05799)
*Kalp Pandya,Pratik Ghosh,Ajeya Mandikal,Shivam Gandha,Bhaskar Chaudhury*

Main category: physics.plasm-ph

TL;DR: 提出结合微分方程等离子体流体求解器与FNO电磁求解器的混合建模方法，在微波流光形成模拟中验证，加速60倍。


<details>
  <summary>Details</summary>
Motivation: 高功率微波击穿建模和仿真计算成本高，需高效方法。

Method: 结合微分方程等离子体流体求解器的准确性和FNO电磁求解器的计算效率，用FNO替代计算昂贵的电磁场更新，等离子体求解器控制动态等离子体响应。

Result: 混合模型在微波流光形成模拟中与FDTD流体模拟在流光形状、速度和时间演化上吻合良好，加速60倍。

Conclusion: 混合FNO策略为高功率微波击穿多尺度多物理仿真提供高效替代方案，且可无缝集成现有C代码和Python机器学习框架。

Abstract: Modeling and simulation of High Power Microwave (HPM) breakdown, a multiscale
phenomenon, is computationally expensive and requires solving Maxwell's
equations (EM solver) coupled with a plasma continuity equation (plasma
solver). In this work, we present a hybrid modeling approach that combines the
accuracy of a differential equation-based plasma fluid solver with the
computational efficiency of FNO (Fourier Neural Operator) based EM solver.
Trained on data from an in-house FDTD-based plasma-fluid solver, the FNO
replaces computationally expensive EM field updates, while the plasma solver
governs the dynamic plasma response. The hybrid model is validated on microwave
streamer formation, due to diffusion ionization mechanism, in a 2D scenario for
unseen incident electric fields corresponding to entirely new plasma streamer
simulations not included in model training, showing excellent agreement with
FDTD based fluid simulations in terms of streamer shape, velocity, and temporal
evolution. This hybrid FNO based strategy delivers significant acceleration of
the order of 60X compared to traditional simulations for the specified problem
size and offers an efficient alternative for computationally demanding
multiscale and multiphysics simulations involved in HPM breakdown. Our work
also demonstrate how such hybrid pipelines can be used to seamlessly to
integrate existing C-based simulation codes with Python-based machine learning
frameworks for simulations of plasma science and engineering problems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [303] [MemTraceDB: Reconstructing MySQL User Activity Using ActiviTimeTrace Algorithm](https://arxiv.org/abs/2509.05891)
*Mahfuzul I. Nissan*

Main category: cs.CR

TL;DR: 论文介绍了工具MemTraceDB，它通过分析MySQL数据库进程的原始内存快照重建用户活动时间线，实验揭示MySQL查询栈容量约9977个查询，给出内存快照收集频率公式。


<details>
  <summary>Details</summary>
Motivation: 数据库审计和事务日志易被特权攻击者篡改，需内存分析作为替代方法用于调查。

Method: 引入MemTraceDB工具，利用ActiviTimeTrace算法提取和关联取证工件。

Result: 实验证明MemTraceDB有效，发现MySQL查询栈约9977个查询的有限操作容量，建立内存快照收集频率公式。

Conclusion: 可独立于受损磁盘日志，实现可靠的用户活动重建。

Abstract: Database audit and transaction logs are fundamental to forensic
investigations, but they are vulnerable to tampering by privileged attackers.
Malicious insiders or external threats with administrative access can alter,
purge, or temporarily disable logging mechanisms, creating significant blind
spots and rendering disk-based records unreliable. Memory analysis offers a
vital alternative, providing investigators direct access to volatile artifacts
that represent a ground-truth source of recent user activity, even when log
files have been compromised.
  This paper introduces MemTraceDB, a tool that reconstructs user activity
timelines by analyzing raw memory snapshots from the MySQL database process.
MemTraceDB utilizes a novel algorithm, ActiviTimeTrace, to systematically
extract and correlate forensic artifacts such as user connections and executed
queries. Through a series of experiments, I demonstrate MemTraceDB's
effectiveness and reveal a critical empirical finding: the MySQL query stack
has a finite operational capacity of approximately 9,997 queries. This
discovery allows me to establish a practical, data-driven formula for
determining the optimal frequency for memory snapshot collection, providing a
clear, actionable guideline for investigators. The result is a
forensically-sound reconstruction of user activity, independent of compromised
disk-based logs.

</details>


### [304] [Introduction to Number Theoretic Transform](https://arxiv.org/abs/2509.05884)
*Banhirup Sengupta,Peenal Gupta,Souvik Sengupta*

Main category: cs.CR

TL;DR: 介绍数论变换（NTT）及其在密码学中的应用，引入循环、负循环卷积等概念。


<details>
  <summary>Details</summary>
Motivation: NTT是后量子密码学和同态加密的有力工具，快速NTT可降低多项式乘法复杂度，研究其相关概念有重要意义。

Method: 引入循环、负循环卷积，以及NTT及其逆变换和快速版本的概念。

Result: 无明确结果阐述。

Conclusion: 无明确结论阐述。

Abstract: The Number Theoretic Transform (NTT) can be regarded as a variant of the
Discrete Fourier Transform. NTT has been quite a powerful mathematical tool in
developing Post-Quantum Cryptography and Homomorphic Encryption. The Fourier
Transform essentially decomposes a signal into its frequencies. They are
traditionally sine or cosine waves. NTT works more over groups or finite fields
rather than on a continuous signal and polynomials work as the analog of sine
waves in case of NTT. Fast Fourier Trnasform (FFT) style NTT or fast NTT has
been proven to be useful in lattice-based cryptography due to its ability to
reduce the complexity of polynomial multiplication from quadratic to
quasilinear. We have introduced the concepts of cyclic, negacyclic convolutions
along with NTT and its inverse and their fast versions.

</details>


### [305] [VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles](https://arxiv.org/abs/2509.06133)
*Pradyumna Kaushal*

Main category: cs.CR

TL;DR: 提出基于区块链和零知识证明的VehiclePassport解决车辆生命周期记录难验证和易欺诈问题，成本低、速度快、可扩展。


<details>
  <summary>Details</summary>
Motivation: 现代车辆在不同主体间的生命周期记录难验证且易欺诈。

Method: 提出GAIA - X对齐的、基于区块链和零知识证明的VehiclePassport，通过JWT和Groth16证明实现选择性披露。

Result: 开源参考栈在Polygon zkEVM上每个事件成本低于0.02美元，验证证明时间小于10毫秒，可扩展到数百万辆车。

Conclusion: 该架构消除纸质KYC，确保GDPR合规可追溯性，为全球移动数据市场应用建立无信任基础。

Abstract: Modern vehicles accumulate fragmented lifecycle records across OEMs, owners,
and service centers that are difficult to verify and prone to fraud. We propose
VehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with
zero-knowledge proofs (ZKPs) for privacy-preserving verification.
VehiclePassport immutably commits to manufacturing, telemetry, and service
events while enabling selective disclosure via short-lived JWTs and Groth16
proofs. Our open-source reference stack anchors hashes on Polygon zkEVM at
<$0.02 per event, validates proofs in <10 ms, and scales to millions of
vehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant
traceability, and establishes a trustless foundation for insurance, resale, and
regulatory applications in global mobility data markets.

</details>


### [306] [FuzzBox: Blending Fuzzing into Emulation for Binary-Only Embedded Targets](https://arxiv.org/abs/2509.05643)
*Carmine Cesarano,Roberto Natella*

Main category: cs.CR

TL;DR: 传统覆盖引导模糊测试应用于工业系统有挑战，FuzzBox结合仿真与模糊测试解决问题，通过实验展示有效性和广泛可移植性。


<details>
  <summary>Details</summary>
Motivation: 传统覆盖引导模糊测试因编译器工具链专有、无源码访问权限，应用于工业系统有挑战，需新方法。

Method: FuzzBox将仿真与模糊测试集成，在虚拟化环境中动态插桩代码，无需源码重编译和特定硬件依赖。

Result: 在专有MILS虚拟机管理程序实验中展示FuzzBox有效性，分析其在商业物联网固件中的适用性。

Conclusion: FuzzBox能克服传统方法局限，有广泛可移植性。

Abstract: Coverage-guided fuzzing has been widely applied to address zero-day
vulnerabilities in general-purpose software and operating systems. This
approach relies on instrumenting the target code at compile time. However,
applying it to industrial systems remains challenging, due to proprietary and
closed-source compiler toolchains and lack of access to source code. FuzzBox
addresses these limitations by integrating emulation with fuzzing: it
dynamically instruments code during execution in a virtualized environment, for
the injection of fuzz inputs, failure detection, and coverage analysis, without
requiring source code recompilation and hardware-specific dependencies. We show
the effectiveness of FuzzBox through experiments in the context of a
proprietary MILS (Multiple Independent Levels of Security) hypervisor for
industrial applications. Additionally, we analyze the applicability of FuzzBox
across commercial IoT firmware, showcasing its broad portability.

</details>


### [307] [Towards Log Analysis with AI Agents: Cowrie Case Study](https://arxiv.org/abs/2509.05306)
*Enis Karaarslan,Esin Güler,Efe Emir Yüce,Cagatay Coban*

Main category: cs.CR

TL;DR: 研究用AI代理自动分析Cowrie蜜罐日志，提出轻量级自动化方法，初步结果显示能减少人工并识别攻击模式。


<details>
  <summary>Details</summary>
Motivation: 现实世界攻击数据稀缺阻碍网络安全研究，Cowrie蜜罐日志量大且非结构化，手动分析不现实。

Method: 利用AI代理智能解析、总结并从原始数据中提取见解，同时考虑部署自主系统的安全影响。

Result: 初步结果表明该方法能减少人工分析工作量并识别攻击模式。

Conclusion: 为未来更高级的自主网络安全分析奠定基础。

Abstract: The scarcity of real-world attack data significantly hinders progress in
cybersecurity research and education. Although honeypots like Cowrie
effectively collect live threat intelligence, they generate overwhelming
volumes of unstructured and heterogeneous logs, rendering manual analysis
impractical. As a first step in our project on secure and efficient AI
automation, this study explores the use of AI agents for automated log
analysis. We present a lightweight and automated approach to process Cowrie
honeypot logs. Our approach leverages AI agents to intelligently parse,
summarize, and extract insights from raw data, while also considering the
security implications of deploying such an autonomous system. Preliminary
results demonstrate the pipeline's effectiveness in reducing manual effort and
identifying attack patterns, paving the way for more advanced autonomous
cybersecurity analysis in future work.

</details>


### [308] [Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations](https://arxiv.org/abs/2509.05311)
*Konur Tholl,François Rivest,Mariam El Mezouar,Ranwa Al Mallah*

Main category: cs.CR

TL;DR: 本文提出将预训练大语言模型集成到强化学习中用于自主网络操作，在模拟环境中验证该方法能提升早期奖励与收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统自主网络操作中的强化学习智能体从头学习，需执行不良动作来了解后果，因此有必要引入外部知识。

Method: 将基于网络安全数据预训练的大语言模型的外部知识集成到强化学习智能体中，指导其初始训练。

Result: 在模拟网络安全环境中，集成大语言模型的方法使智能体在早期训练中获得超2倍奖励，且比基线方法快约4500个回合收敛到理想策略。

Conclusion: 将大语言模型集成到强化学习中能提升基线性能，减少探索负面结果动作的需求。

Abstract: Reinforcement Learning (RL) has shown great potential for autonomous
decision-making in the cybersecurity domain, enabling agents to learn through
direct environment interaction. However, RL agents in Autonomous Cyber
Operations (ACO) typically learn from scratch, requiring them to execute
undesirable actions to learn their consequences. In this study, we integrate
external knowledge in the form of a Large Language Model (LLM) pretrained on
cybersecurity data that our RL agent can directly leverage to make informed
decisions. By guiding initial training with an LLM, we improve baseline
performance and reduce the need for exploratory actions with obviously negative
outcomes. We evaluate our LLM-integrated approach in a simulated cybersecurity
environment, and demonstrate that our guided agent achieves over 2x higher
rewards during early training and converges to a favorable policy approximately
4,500 episodes faster than the baseline.

</details>


### [309] [Backdoor Samples Detection Based on Perturbation Discrepancy Consistency in Pre-trained Language Models](https://arxiv.org/abs/2509.05318)
*Zuquan Peng,Jianming Fu,Lixin Zou,Li Zheng,Yanzhen Ren,Guojun Peng*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The use of unvetted third-party and internet data renders pre-trained models
susceptible to backdoor attacks. Detecting backdoor samples is critical to
prevent backdoor activation during inference or injection during training.
However, existing detection methods often require the defender to have access
to the poisoned models, extra clean samples, or significant computational
resources to detect backdoor samples, limiting their practicality. To address
this limitation, we propose a backdoor sample detection method based on
perturbatio\textbf{N} discr\textbf{E}pancy consis\textbf{T}ency
\textbf{E}valuation (\NETE). This is a novel detection method that can be used
both pre-training and post-training phases. In the detection process, it only
requires an off-the-shelf pre-trained model to compute the log probability of
samples and an automated function based on a mask-filling strategy to generate
perturbations. Our method is based on the interesting phenomenon that the
change in perturbation discrepancy for backdoor samples is smaller than that
for clean samples. Based on this phenomenon, we use curvature to measure the
discrepancy in log probabilities between different perturbed samples and input
samples, thereby evaluating the consistency of the perturbation discrepancy to
determine whether the input sample is a backdoor sample. Experiments conducted
on four typical backdoor attacks and five types of large language model
backdoor attacks demonstrate that our detection strategy outperforms existing
zero-shot black-box detection methods.

</details>


### [310] [Zero-Knowledge Proofs in Sublinear Space](https://arxiv.org/abs/2509.05326)
*Logan Nye*

Main category: cs.CR

TL;DR: 本文构建首个亚线性空间零知识证明（ZKP）证明者，将证明者内存从线性降至O(sqrt(T))，推动设备端证明。


<details>
  <summary>Details</summary>
Motivation: 现代ZKP系统证明者内存随计算轨迹长度线性增长，对资源受限设备不实用，大规模任务成本高。

Method: 将证明生成重构为经典树评估问题，利用高效树评估算法设计流式证明者，无需完整执行轨迹。

Result: 证明者内存从线性降至O(sqrt(T))（含低阶项），保留证明大小、验证时间和安全保证。

Conclusion: 该方法使证明从服务器端转向设备端，为去中心化系统等领域带来应用。

Abstract: Modern zero-knowledge proof (ZKP) systems, essential for privacy and
verifiable computation, suffer from a fundamental limitation: the prover
typically uses memory that scales linearly with the computation's trace length
T, making them impractical for resource-constrained devices and prohibitively
expensive for large-scale tasks. This paper overcomes this barrier by
constructing, to our knowledge, the first sublinear-space ZKP prover. Our core
contribution is an equivalence that reframes proof generation as an instance of
the classic Tree Evaluation problem. Leveraging a recent space-efficient
tree-evaluation algorithm, we design a streaming prover that assembles the
proof without ever materializing the full execution trace. The approach reduces
prover memory from linear in T to O(sqrt(T)) (up to O(log T) lower-order terms)
while preserving proof size, verifier time, and the transcript/security
guarantees of the underlying system. This enables a shift from specialized,
server-bound proving to on-device proving, opening applications in
decentralized systems, on-device machine learning, and privacy-preserving
technologies.

</details>


### [311] [ForensicsData: A Digital Forensics Dataset for Large Language Models](https://arxiv.org/abs/2509.05331)
*Youssef Chakir,Iyad Lahsen-Cherif*

Main category: cs.CR

TL;DR: 文章介绍数字取证因网络事件复杂面临挑战，提出ForensicsData数据集，介绍创建流程，评估模型性能，旨在推动数字取证发展。


<details>
  <summary>Details</summary>
Motivation: 网络事件复杂使数字取证面临挑战，且缺乏现实数据集，需弥补此缺口。

Method: 使用独特工作流创建数据集，提取结构化数据，用大语言模型转换为Q - C - A格式，通过专业评估流程确认质量。

Result: 在评估的模型中，Gemini 2 Flash在使生成内容与取证术语对齐方面表现最佳。

Conclusion: ForensicsData能实现可重复实验，促进研究社区合作，推动数字取证发展。

Abstract: The growing complexity of cyber incidents presents significant challenges for
digital forensic investigators, especially in evidence collection and analysis.
Public resources are still limited because of ethical, legal, and privacy
concerns, even though realistic datasets are necessary to support research and
tool developments. To address this gap, we introduce ForensicsData, an
extensive Question-Context-Answer (Q-C-A) dataset sourced from actual malware
analysis reports. It consists of more than 5,000 Q-C-A triplets. A unique
workflow was used to create the dataset, which extracts structured data, uses
large language models (LLMs) to transform it into Q-C-A format, and then uses a
specialized evaluation process to confirm its quality. Among the models
evaluated, Gemini 2 Flash demonstrated the best performance in aligning
generated content with forensic terminology. ForensicsData aims to advance
digital forensics by enabling reproducible experiments and fostering
collaboration within the research community.

</details>


### [312] [Integrated Simulation Framework for Adversarial Attacks on Autonomous Vehicles](https://arxiv.org/abs/2509.05332)
*Christos Anagnostopoulos,Ioulia Kapsali,Alexandros Gkillas,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CR

TL;DR: 本文介绍了一种用于自动驾驶汽车的开源集成仿真框架，可生成针对感知和通信层的对抗攻击，并通过实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车易受对抗攻击影响安全，现有仿真框架缺乏对多领域对抗场景的全面支持。

Method: 引入新的开源集成仿真框架，对物理环境、交通动态和V2X网络进行高保真建模，通过统一核心协调多个模拟器，支持多种感知和通信层攻击，集成ROS 2。

Result: 评估生成的对抗场景对先进3D物体检测器的影响，发现在现实条件下其性能显著下降。

Conclusion: 所提出的仿真框架能有效模拟自动驾驶汽车面临的多领域对抗攻击场景。

Abstract: Autonomous vehicles (AVs) rely on complex perception and communication
systems, making them vulnerable to adversarial attacks that can compromise
safety. While simulation offers a scalable and safe environment for robustness
testing, existing frameworks typically lack comprehensive supportfor modeling
multi-domain adversarial scenarios. This paper introduces a novel, open-source
integrated simulation framework designed to generate adversarial attacks
targeting both perception and communication layers of AVs. The framework
provides high-fidelity modeling of physical environments, traffic dynamics, and
V2X networking, orchestrating these components through a unified core that
synchronizes multiple simulators based on a single configuration file. Our
implementation supports diverse perception-level attacks on LiDAR sensor data,
along with communication-level threats such as V2X message manipulation and GPS
spoofing. Furthermore, ROS 2 integration ensures seamless compatibility with
third-party AV software stacks. We demonstrate the framework's effectiveness by
evaluating the impact of generated adversarial scenarios on a state-of-the-art
3D object detector, revealing significant performance degradation under
realistic conditions.

</details>


### [313] [AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning](https://arxiv.org/abs/2509.05362)
*Ismail Hossain,Sai Puppala,Sajedul Talukder,Md Jahangir Alam*

Main category: cs.CR

TL;DR: 提出隐私保护、AI 介入框架实时检测和阻断诈骗对话，实验显示性能良好，不同安全设置各有利弊，是首个统一多方面的主动防御范式。


<details>
  <summary>Details</summary>
Motivation: 现有诈骗防御多为被动反应式，在主动交互中保护有限，需主动防御方案。

Method: 结合指令调优 AI 与安全感知效用函数，采用联邦学习更新模型，不共享原始数据。

Result: 系统回复流畅、有吸引力，相比基线在真实性、安全性和有效性上有显著提升；联邦设置下模型多轮训练表现良好；不同安全设置各有利弊。

Conclusion: 该框架是首个统一实时诈骗诱饵、联邦隐私保护和校准安全调节的主动防御范式。

Abstract: Scams exploiting real-time social engineering -- such as phishing,
impersonation, and phone fraud -- remain a persistent and evolving threat
across digital platforms. Existing defenses are largely reactive, offering
limited protection during active interactions. We propose a privacy-preserving,
AI-in-the-loop framework that proactively detects and disrupts scam
conversations in real time. The system combines instruction-tuned artificial
intelligence with a safety-aware utility function that balances engagement with
harm minimization, and employs federated learning to enable continual model
updates without raw data sharing. Experimental evaluations show that the system
produces fluent and engaging responses (perplexity as low as 22.3, engagement
$\approx$0.80), while human studies confirm significant gains in realism,
safety, and effectiveness over strong baselines. In federated settings, models
trained with FedAvg sustain up to 30 rounds while preserving high engagement
($\approx$0.80), strong relevance ($\approx$0.74), and low PII leakage
($\leq$0.0085). Even with differential privacy, novelty and safety remain
stable, indicating that robust privacy can be achieved without sacrificing
performance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3,
MD-Judge) shows a straightforward pattern: stricter moderation settings reduce
the chance of exposing personal information, but they also limit how much the
model engages in conversation. In contrast, more relaxed settings allow longer
and richer interactions, which improve scam detection, but at the cost of
higher privacy risk. To our knowledge, this is the first framework to unify
real-time scam-baiting, federated privacy preservation, and calibrated safety
moderation into a proactive defense paradigm.

</details>


### [314] [Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs](https://arxiv.org/abs/2509.05367)
*Shei Pern Chua,Thai Zhen Leng,Teh Kai Jun,Xiao Li,Xiaolin Hu*

Main category: cs.CR

TL;DR: 提出TRIAL框架利用大语言模型伦理推理绕过安全防护，表明当前安全防护可能不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理能力提升带来新安全风险，多轮越狱策略研究不足。

Method: 引入TRIAL框架，将对抗目标嵌入基于电车问题的伦理困境。

Result: TRIAL对开源和闭源模型都有高越狱成功率。

Conclusion: 大语言模型高级推理能力使现有安全防护可能存在漏洞，需重新评估安全对齐监督策略。

Abstract: Large language models (LLMs) have undergone safety alignment efforts to
mitigate harmful outputs. However, as LLMs become more sophisticated in
reasoning, their intelligence may introduce new security risks. While
traditional jailbreak attacks relied on singlestep attacks, multi-turn
jailbreak strategies that adapt dynamically to context remain underexplored. In
this work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack
Logic), a framework that leverages LLMs ethical reasoning to bypass their
safeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on
the trolley problem. TRIAL demonstrates high jailbreak success rates towards
both open and close-source models. Our findings underscore a fundamental
limitation in AI safety: as models gain advanced reasoning abilities, the
nature of their alignment may inadvertently allow for more covert security
vulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating
safety alignment oversight strategies, as current safeguards may prove
insufficient against context-aware adversarial attack.

</details>


### [315] [Privacy Preservation and Identity Tracing Prevention in AI-Driven Eye Tracking for Interactive Learning Environments](https://arxiv.org/abs/2509.05376)
*Abdul Rehman,Are Dæhlen,Ilona Heldal,Jerry Chun-wei Lin*

Main category: cs.CR

TL;DR: 本文提出用于交互式学习环境的以人为本框架，防止眼动追踪技术的身份回溯，同时保留其教学益处，介绍框架两阶段情况及取得的准确率。


<details>
  <summary>Details</summary>
Motivation: 眼动追踪技术在辅助理解神经发育障碍和追踪身份时存在隐私风险，需要一种方法来防止身份回溯并保留其在交互式学习环境中的教学益处。

Method: 先展示不同场景下利用严肃游戏眼动追踪数据回溯学生ID和诊断的可能性，提出两阶段隐私保护框架，还跨多个客户端采用联邦学习，结合带虚拟ID和管理员访问控制的安全身份管理系统。

Result: 第一阶段框架在不同场景取得99.3%、63%、99.7%等准确率，第二阶段有效防止回溯，建立安全身份管理系统，整体准确率达99.40%。

Conclusion: 所提出的框架能在交互式学习环境中有效防止身份回溯，同时保留AI驱动眼动追踪的教学益处，建立了信任和透明度。

Abstract: Eye-tracking technology can aid in understanding neurodevelopmental disorders
and tracing a person's identity. However, this technology poses a significant
risk to privacy, as it captures sensitive information about individuals and
increases the likelihood that data can be traced back to them. This paper
proposes a human-centered framework designed to prevent identity backtracking
while preserving the pedagogical benefits of AI-powered eye tracking in
interactive learning environments. We explore how real-time data anonymization,
ethical design principles, and regulatory compliance (such as GDPR) can be
integrated to build trust and transparency. We first demonstrate the potential
for backtracking student IDs and diagnoses in various scenarios using serious
game-based eye-tracking data. We then provide a two-stage privacy-preserving
framework that prevents participants from being tracked while still enabling
diagnostic classification. The first phase covers four scenarios: I) Predicting
disorder diagnoses based on different game levels. II) Predicting student IDs
based on different game levels. III) Predicting student IDs based on randomized
data. IV) Utilizing K-Means for out-of-sample data. In the second phase, we
present a two-stage framework that preserves privacy. We also employ Federated
Learning (FL) across multiple clients, incorporating a secure identity
management system with dummy IDs and administrator-only access controls. In the
first phase, the proposed framework achieved 99.3% accuracy for scenario 1, 63%
accuracy for scenario 2, and 99.7% accuracy for scenario 3, successfully
identifying and assigning a new student ID in scenario 4. In phase 2, we
effectively prevented backtracking and established a secure identity management
system with dummy IDs and administrator-only access controls, achieving an
overall accuracy of 99.40%.

</details>


### [316] [ThreatGPT: An Agentic AI Framework for Enhancing Public Safety through Threat Modeling](https://arxiv.org/abs/2509.05379)
*Sharif Noor Zisad,Ragib Hasan*

Main category: cs.CR

TL;DR: 介绍了应对公共安全系统安全威胁的AI助手ThreatGPT，它能让非专业人员分析系统威胁，结合AI与人类判断保障公共系统安全。


<details>
  <summary>Details</summary>
Motivation: 城市和社区智能化使公共安全系统更复杂，面临更大安全威胁，需工具帮助非专业人员理解和分析威胁。

Method: 用户描述系统组件，用流行框架分析，采用few - shot learning学习示例生成智能威胁模型。

Result: ThreatGPT可指出系统可能问题、攻击者利用方式及预防措施，适应不同需求。

Conclusion: ThreatGPT结合AI与人类判断，让人们更快、更智能、更有信心地理解和应对公共系统威胁。

Abstract: As our cities and communities become smarter, the systems that keep us safe,
such as traffic control centers, emergency response networks, and public
transportation, also become more complex. With this complexity comes a greater
risk of security threats that can affect not just machines but real people's
lives. To address this challenge, we present ThreatGPT, an agentic Artificial
Intelligence (AI) assistant built to help people whether they are engineers,
safety officers, or policy makers to understand and analyze threats in public
safety systems. Instead of requiring deep cybersecurity expertise, it allows
users to simply describe the components of a system they are concerned about,
such as login systems, data storage, or communication networks. Then, with the
click of a button, users can choose how they want the system to be analyzed by
using popular frameworks such as STRIDE, MITRE ATT&CK, CVE reports, NIST, or
CISA. ThreatGPT is unique because it does not just provide threat information,
but rather it acts like a knowledgeable partner. Using few-shot learning, the
AI learns from examples and generates relevant smart threat models. It can
highlight what might go wrong, how attackers could take advantage, and what can
be done to prevent harm. Whether securing a city's infrastructure or a local
health service, this tool adapts to users' needs. In simple terms, ThreatGPT
brings together AI and human judgment to make our public systems safer. It is
designed not just to analyze threats, but to empower people to understand and
act on them, faster, smarter, and with more confidence.

</details>


### [317] [Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models](https://arxiv.org/abs/2509.05471)
*Youjia Zheng,Mohammad Zandsalimy,Shanu Sushmita*

Main category: cs.CR

TL;DR: 论文研究伪装越狱提示，引入新基准数据集并提出评估框架，发现大语言模型面对此类攻击性能和安全性下降，需更细致的安全策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易受伪装越狱攻击，现有安全机制难以应对，传统检测方法有局限。

Method: 引入含500个示例的Camouflaged Jailbreak Prompts基准数据集，提出从七个维度评估的多方面评估框架。

Result: 大语言模型面对良性输入时安全和内容质量高，面对伪装越狱攻击时性能和安全性显著下降。

Conclusion: 大语言模型存在普遍漏洞，迫切需要更细致、自适应的安全策略以确保其在现实应用中的可靠部署。

Abstract: Large Language Models (LLMs) are increasingly vulnerable to a sophisticated
form of adversarial prompting known as camouflaged jailbreaking. This method
embeds malicious intent within seemingly benign language to evade existing
safety mechanisms. Unlike overt attacks, these subtle prompts exploit
contextual ambiguity and the flexible nature of language, posing significant
challenges to current defense systems. This paper investigates the construction
and impact of camouflaged jailbreak prompts, emphasizing their deceptive
characteristics and the limitations of traditional keyword-based detection
methods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts,
containing 500 curated examples (400 harmful and 100 benign prompts) designed
to rigorously stress-test LLM safety protocols. In addition, we propose a
multi-faceted evaluation framework that measures harmfulness across seven
dimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards,
Harmful Potential, Educational Value, Content Quality, and Compliance Score.
Our findings reveal a stark contrast in LLM behavior: while models demonstrate
high safety and content quality with benign inputs, they exhibit a significant
decline in performance and safety when confronted with camouflaged jailbreak
attempts. This disparity underscores a pervasive vulnerability, highlighting
the urgent need for more nuanced and adaptive security strategies to ensure the
responsible and robust deployment of LLMs in real-world applications.

</details>


### [318] [Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints](https://arxiv.org/abs/2509.05608)
*Waris Gill,Natalie Isak,Matthew Dressman*

Main category: cs.CR

TL;DR: 企业服务中广泛部署大语言模型产生安全盲点，提出BinaryShield系统实现跨合规边界安全共享攻击指纹，性能表现佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛部署产生安全盲点，且因监管合规边界限制无法共享威胁情报，需解决此问题。

Method: 提出BinaryShield系统，通过结合PII编辑、语义嵌入、二进制量化和随机响应机制的独特管道转换可疑提示，生成非可逆指纹。

Result: BinaryShield的F1分数达到0.94，显著优于SimHash（0.77），实现64倍存储减少和38倍更快的相似度搜索。

Conclusion: BinaryShield能在保护隐私的同时实现跨合规边界的攻击指纹安全共享，性能表现出色。

Abstract: The widespread deployment of LLMs across enterprise services has created a
critical security blind spot. Organizations operate multiple LLM services
handling billions of queries daily, yet regulatory compliance boundaries
prevent these services from sharing threat intelligence about prompt injection
attacks, the top security risk for LLMs. When an attack is detected in one
service, the same threat may persist undetected in others for months, as
privacy regulations prohibit sharing user prompts across compliance boundaries.
  We present BinaryShield, the first privacy-preserving threat intelligence
system that enables secure sharing of attack fingerprints across compliance
boundaries. BinaryShield transforms suspicious prompts through a unique
pipeline combining PII redaction, semantic embedding, binary quantization, and
randomized response mechanism to potentially generate non-invertible
fingerprints that preserve attack patterns while providing privacy. Our
evaluations demonstrate that BinaryShield achieves an F1-score of 0.94,
significantly outperforming SimHash (0.77), the privacy-preserving baseline,
while achieving 64x storage reduction and 38x faster similarity search compared
to dense embeddings.

</details>


### [319] [Privacy-Preserving Offloading for Large Language Models in 6G Vehicular Networks](https://arxiv.org/abs/2509.05320)
*Ikhlasse Badidi,Nouhaila El Khiyaoui,Aya Riany,Badr Ben Elallid,Amine Abouaomar*

Main category: cs.CR

TL;DR: 本文提出适用于集成大语言模型的车联网隐私保护卸载框架，结合联邦学习和差分隐私技术，实验证明其有效且高效。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型集成到6G车联网时，车辆向边缘基础设施卸载计算会带来隐私风险，需保护用户数据。

Method: 采用结合联邦学习和差分隐私的混合方法，设计隐私感知任务分区算法优化本地和边缘计算权衡，提出安全通信协议。

Result: 该方法达到75%的全局准确率，与非隐私保护方法相比仅降低2 - 3%，在最优隐私预算ε = 0.8时保证差分隐私，每轮通信开销约2.1MB，计算占总处理时间超90%。

Conclusion: 该框架在资源受限的车联网环境中有效且高效。

Abstract: The integration of Large Language Models (LLMs) in 6G vehicular networks
promises unprecedented advancements in intelligent transportation systems.
However, offloading LLM computations from vehicles to edge infrastructure poses
significant privacy risks, potentially exposing sensitive user data. This paper
presents a novel privacy-preserving offloading framework for LLM-integrated
vehicular networks. We introduce a hybrid approach combining federated learning
(FL) and differential privacy (DP) techniques to protect user data while
maintaining LLM performance. Our framework includes a privacy-aware task
partitioning algorithm that optimizes the trade-off between local and edge
computation, considering both privacy constraints and system efficiency. We
also propose a secure communication protocol for transmitting model updates and
aggregating results across the network. Experimental results demonstrate that
our approach achieves 75\% global accuracy with only a 2-3\% reduction compared
to non-privacy-preserving methods, while maintaining DP guarantees with an
optimal privacy budget of $\varepsilon = 0.8$. The framework shows stable
communication overhead of approximately 2.1MB per round with computation
comprising over 90\% of total processing time, validating its efficiency for
resource-constrained vehicular environments.

</details>


### [320] [Ensembling Membership Inference Attacks Against Tabular Generative Models](https://arxiv.org/abs/2509.05350)
*Joshua Ward,Yuxuan Yang,Chi-Hua Wang,Guang Cheng*

Main category: cs.CR

TL;DR: 研究合成数据隐私审计中成员推理攻击（MIAs）选择问题，提出集成MIAs更稳健。


<details>
  <summary>Details</summary>
Motivation: 现实威胁场景中对手需在无先验保证下选单个MIA方法，且无单一MIA在多种模型架构和数据集领域占优。

Method: 将此挑战作为不确定下决策理论问题研究，开展最大规模合成数据隐私基准测试，提出集成MIAs。

Result: 发现无单一MIA是严格占优策略，无监督集成MIA比单个攻击更稳健，能最小化遗憾。

Conclusion: 无监督集成MIAs在合成数据隐私审计中比单个攻击策略更优。

Abstract: Membership Inference Attacks (MIAs) have emerged as a principled framework
for auditing the privacy of synthetic data generated by tabular generative
models, where many diverse methods have been proposed that each exploit
different privacy leakage signals. However, in realistic threat scenarios, an
adversary must choose a single method without a priori guarantee that it will
be the empirically highest performing option. We study this challenge as a
decision theoretic problem under uncertainty and conduct the largest synthetic
data privacy benchmark to date. Here, we find that no MIA constitutes a
strictly dominant strategy across a wide variety of model architectures and
dataset domains under our threat model. Motivated by these findings, we propose
ensemble MIAs and show that unsupervised ensembles built on individual attacks
offer empirically more robust, regret-minimizing strategies than individual
attacks.

</details>


### [321] [SEASONED: Semantic-Enhanced Self-Counterfactual Explainable Detection of Adversarial Exploiter Contracts](https://arxiv.org/abs/2509.05681)
*Xng Ai,Shudan Lin,Zecheng Li,Kai Zhou,Bixin Li,Bin Xiao*

Main category: cs.CR

TL;DR: 本文针对DeFi攻击中的AEC可解释检测问题，提出SEASONED框架，实验证明其有效并发布新数据集。


<details>
  <summary>Details</summary>
Motivation: DeFi攻击常通过AEC实施，现有检测方法难捕捉语义依赖且缺乏可解释性，需有效检测AEC的方法。

Method: 引入SEASONED框架，从合约字节码提取语义信息构建SRG，用SCFED对SRG分类并生成解释，提取代表性信息增强性能。

Result: 理论分析和实验表明SEASONED有出色的检测性能、鲁棒性、泛化性和数据效率学习能力。

Conclusion: SEASONED框架能有效检测AEC，可用于解决现有检测方法的问题，新数据集可支持后续研究。

Abstract: Decentralized Finance (DeFi) attacks have resulted in significant losses,
often orchestrated through Adversarial Exploiter Contracts (AECs) that exploit
vulnerabilities in victim smart contracts. To proactively identify such
threats, this paper targets the explainable detection of AECs.
  Existing detection methods struggle to capture semantic dependencies and lack
interpretability, limiting their effectiveness and leaving critical knowledge
gaps in AEC analysis. To address these challenges, we introduce SEASONED, an
effective, self-explanatory, and robust framework for AEC detection.
  SEASONED extracts semantic information from contract bytecode to construct a
semantic relation graph (SRG), and employs a self-counterfactual explainable
detector (SCFED) to classify SRGs and generate explanations that highlight the
core attack logic. SCFED further enhances robustness, generalizability, and
data efficiency by extracting representative information from these
explanations. Both theoretical analysis and experimental results demonstrate
the effectiveness of SEASONED, which showcases outstanding detection
performance, robustness, generalizability, and data efficiency learning
ability. To support further research, we also release a new dataset of 359
AECs.

</details>


### [322] [Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated](https://arxiv.org/abs/2509.05739)
*Hanna Foerster,Ilia Shumailov,Yiren Zhao,Harsh Chaudhari,Jamie Hayes,Robert Mullins,Yarin Gal*

Main category: cs.CR

TL;DR: 研究针对大语言模型的数据投毒攻击，提出“分解推理投毒”，发现激活其改变最终答案困难，高级大语言模型因推理能力和架构分离有后门鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型增加逐步推理，攻击面扩大到中间思维链，研究更隐蔽的投毒方式。

Method: 提出“分解推理投毒”，攻击者仅修改推理路径，将触发器拆分为多个无害组件。

Result: 虽然可以注入分解投毒，但可靠激活其改变最终答案很困难，模型常能从思维过程中激活的后门恢复。

Conclusion: 高级大语言模型的推理能力和推理与最终答案生成的架构分离带来了一种新兴的后门鲁棒性。

Abstract: Early research into data poisoning attacks against Large Language Models
(LLMs) demonstrated the ease with which backdoors could be injected. More
recent LLMs add step-by-step reasoning, expanding the attack surface to include
the intermediate chain-of-thought (CoT) and its inherent trait of decomposing
problems into subproblems. Using these vectors for more stealthy poisoning, we
introduce ``decomposed reasoning poison'', in which the attacker modifies only
the reasoning path, leaving prompts and final answers clean, and splits the
trigger across multiple, individually harmless components.
  Fascinatingly, while it remains possible to inject these decomposed poisons,
reliably activating them to change final answers (rather than just the CoT) is
surprisingly difficult. This difficulty arises because the models can often
recover from backdoors that are activated within their thought processes.
Ultimately, it appears that an emergent form of backdoor robustness is
originating from the reasoning capabilities of these advanced LLMs, as well as
from the architectural separation between reasoning and final answer
generation.

</details>


### [323] [Tell-Tale Watermarks for Explanatory Reasoning in Synthetic Media Forensics](https://arxiv.org/abs/2509.05753)
*Ching-Chun Chang,Isao Echizen*

Main category: cs.CR

TL;DR: 合成媒体兴起模糊现实与虚构边界，引发信息疫情，本文开发用于合成媒体溯源的水印系统并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 合成媒体的发展模糊现实与虚构边界，引发信息疫情，数字图像编辑复杂增加取证分析难度，需追溯合成媒体生成链。

Method: 开发针对合成媒体不同类型变换的可解释水印系统，利用水印在变换中留下可解释痕迹，再进行解释推理。

Result: 实验评估证明示踪水印系统在保真度、同步性和可追溯性方面的有效性。

Conclusion: 示踪水印系统可用于解释合成媒体生命周期内变换的性质和程度，能解决追溯合成媒体潜在生成链的逆问题。

Abstract: The rise of synthetic media has blurred the boundary between reality and
fabrication under the evolving power of artificial intelligence, fueling an
infodemic that erodes public trust in cyberspace. For digital imagery, a
multitude of editing applications further complicates the forensic analysis,
including semantic edits that alter content, photometric adjustments that
recalibrate colour characteristics, and geometric projections that reshape
viewpoints. Collectively, these transformations manipulate and control
perceptual interpretation of digital imagery. This susceptibility calls for
forensic enquiry into reconstructing the chain of events, thereby revealing
deeper evidential insight into the presence or absence of criminal intent. This
study seeks to address an inverse problem of tracing the underlying generation
chain that gives rise to the observed synthetic media. A tell-tale watermarking
system is developed for explanatory reasoning over the nature and extent of
transformations across the lifecycle of synthetic media. Tell-tale watermarks
are tailored to different classes of transformations, responding in a manner
that is neither strictly robust nor fragile but instead interpretable. These
watermarks function as reference clues that evolve under the same
transformation dynamics as the carrier media, leaving interpretable traces when
subjected to transformations. Explanatory reasoning is then performed to infer
the most plausible account across the combinatorial parameter space of
composite transformations. Experimental evaluations demonstrate the validity of
tell-tale watermarking with respect to fidelity, synchronicity and
traceability.

</details>


### [324] [Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System](https://arxiv.org/abs/2509.05755)
*Yu Liu,Yuchong Xie,Mingyu Luo,Zesen Liu,Zhixiang Zhang,Kaikai Zhang,Zongjie Li,Ping Chen,Shuai Wang,Dongdong She*

Main category: cs.CR

TL;DR: 本文研究基于大语言模型的智能体系统中工具调用提示（TIP）的安全风险，揭示部分系统存在漏洞，提出防御机制。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的智能体系统中TIP对确保工具使用的安全和正确性很重要，但TIP安全问题被忽视，因此开展研究。

Method: 通过系统性的TIP利用工作流（TEW），演示通过操纵工具调用劫持外部工具行为。

Result: 发现Cursor、Claude Code等主要基于大语言模型的系统易受远程代码执行（RCE）和拒绝服务（DoS）等攻击。

Conclusion: 提出了增强基于大语言模型的智能体系统中TIP安全性的防御机制。

Abstract: LLM-based agentic systems leverage large language models to handle user
queries, make decisions, and execute external tools for complex tasks across
domains like chatbots, customer service, and software engineering. A critical
component of these systems is the Tool Invocation Prompt (TIP), which defines
tool interaction protocols and guides LLMs to ensure the security and
correctness of tool usage. Despite its importance, TIP security has been
largely overlooked. This work investigates TIP-related security risks,
revealing that major LLM-based systems like Cursor, Claude Code, and others are
vulnerable to attacks such as remote code execution (RCE) and denial of service
(DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate
external tool behavior hijacking via manipulated tool invocations. We also
propose defense mechanisms to enhance TIP security in LLM-based agentic
systems.

</details>


### [325] [Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization](https://arxiv.org/abs/2509.05831)
*Ishaan Verma*

Main category: cs.CR

TL;DR: 研究利用非可见HTML元素嵌入对抗指令攻击大语言模型，构建数据集评估模型，发现模型易受攻击，强调需缓解策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型集成到网页系统做内容摘要时，易受提示注入攻击，此为主要研究动机。

Method: 构建含280个静态网页的数据集，用浏览器自动化管道处理，评估Llama 4 Scout和Gemma 9B IT两个模型，使用词汇和语义指标及人工标注评估。

Result: 超29%注入样本使Llama 4 Scout摘要有明显变化，Gemma 9B IT成功率为15%。

Conclusion: 大语言模型驱动的网页管道存在关键且常被忽视的漏洞，需鲁棒缓解策略。

Abstract: Large Language Models (LLMs) are increasingly integrated into web-based
systems for content summarization, yet their susceptibility to prompt injection
attacks remains a pressing concern. In this study, we explore how non-visible
HTML elements such as <meta>, aria-label, and alt attributes can be exploited
to embed adversarial instructions without altering the visible content of a
webpage. We introduce a novel dataset comprising 280 static web pages, evenly
divided between clean and adversarial injected versions, crafted using diverse
HTML-based strategies. These pages are processed through a browser automation
pipeline to extract both raw HTML and rendered text, closely mimicking
real-world LLM deployment scenarios. We evaluate two state-of-the-art
open-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their
ability to summarize this content. Using both lexical (ROUGE-L) and semantic
(SBERT cosine similarity) metrics, along with manual annotations, we assess the
impact of these covert injections. Our findings reveal that over 29% of
injected samples led to noticeable changes in the Llama 4 Scout summaries,
while Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These
results highlight a critical and largely overlooked vulnerability in LLM driven
web pipelines, where hidden adversarial content can subtly manipulate model
outputs. Our work offers a reproducible framework and benchmark for evaluating
HTML-based prompt injection and underscores the urgent need for robust
mitigation strategies in LLM applications involving web content.

</details>


### [326] [Multimodal Prompt Injection Attacks: Risks and Defenses for Modern LLMs](https://arxiv.org/abs/2509.05883)
*Andrew Yeo,Daeseon Choi*

Main category: cs.CR

TL;DR: 研究对8个商业大语言模型进行实验评估其外部提示注入漏洞，发现模型存在可利用弱点，强调需更强安全措施，Claude 3相对更稳健但仍需额外防御。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用带来安全风险，特别是提示注入和越狱攻击，需系统评估其漏洞。

Method: 对8个商业模型进行一系列实验，仅依靠模型内置防护，不进行额外清理。

Result: 实验暴露了可利用的弱点，分析了四类攻击，Claude 3相对更稳健。

Conclusion: 需要更强的安全措施，如输入归一化，才能实现可靠保护。

Abstract: Large Language Models (LLMs) have seen rapid adoption in recent years, with
industries increasingly relying on them to maintain a competitive advantage.
These models excel at interpreting user instructions and generating human-like
responses, leading to their integration across diverse domains, including
consulting and information retrieval. However, their widespread deployment also
introduces substantial security risks, most notably in the form of prompt
injection and jailbreak attacks.
  To systematically evaluate LLM vulnerabilities -- particularly to external
prompt injection -- we conducted a series of experiments on eight commercial
models. Each model was tested without supplementary sanitization, relying
solely on its built-in safeguards. The results exposed exploitable weaknesses
and emphasized the need for stronger security measures. Four categories of
attacks were examined: direct injection, indirect (external) injection,
image-based injection, and prompt leakage. Comparative analysis indicated that
Claude 3 demonstrated relatively greater robustness; nevertheless, empirical
findings confirm that additional defenses, such as input normalization, remain
necessary to achieve reliable protection.

</details>


### [327] [DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation](https://arxiv.org/abs/2509.06026)
*Xinyu Gao,Xiangtao Meng,Yingkai Dong,Zheng Li,Shanqing Guo*

Main category: cs.CR

TL;DR: 本文提出DCMI方法应对RAG系统的成员推理攻击，实验表明其效果优于基线，凸显RAG系统隐私风险。


<details>
  <summary>Details</summary>
Motivation: 现有针对RAG外部数据库的成员推理攻击（MIAs）忽略非成员检索文档干扰，效果受限。

Method: 提出DCMI，利用查询扰动下成员和非成员检索文档的敏感性差距，生成扰动查询进行校准，减少非成员文档干扰。

Result: 在逐步放宽假设的实验中，DCMI始终优于基线，如在RAG系统中AUC达97.42%、准确率达94.35%，在真实平台上有10%-20%优势。

Conclusion: RAG系统存在显著隐私风险，需更强保护机制，呼吁社区深入研究数据泄露风险。

Abstract: While Retrieval-Augmented Generation (RAG) effectively reduces hallucinations
by integrating external knowledge bases, it introduces vulnerabilities to
membership inference attacks (MIAs), particularly in systems handling sensitive
data. Existing MIAs targeting RAG's external databases often rely on model
responses but ignore the interference of non-member-retrieved documents on RAG
outputs, limiting their effectiveness. To address this, we propose DCMI, a
differential calibration MIA that mitigates the negative impact of
non-member-retrieved documents. Specifically, DCMI leverages the sensitivity
gap between member and non-member retrieved documents under query perturbation.
It generates perturbed queries for calibration to isolate the contribution of
member-retrieved documents while minimizing the interference from
non-member-retrieved documents. Experiments under progressively relaxed
assumptions show that DCMI consistently outperforms baselines--for example,
achieving 97.42% AUC and 94.35% Accuracy against the RAG system with Flan-T5,
exceeding the MBA baseline by over 40%. Furthermore, on real-world RAG
platforms such as Dify and MaxKB, DCMI maintains a 10%-20% advantage over the
baseline. These results highlight significant privacy risks in RAG systems and
emphasize the need for stronger protection mechanisms. We appeal to the
community's consideration of deeper investigations, like ours, against the data
leakage risks in rapidly evolving RAG systems. Our code is available at
https://github.com/Xinyu140203/RAG_MIA.

</details>


### [328] [PLRV-O: Advancing Differentially Private Deep Learning via Privacy Loss Random Variable Optimization](https://arxiv.org/abs/2509.06264)
*Qin Yang,Nicholas Stout,Meisam Mohammady,Han Wang,Ayesha Samreen,Christopher J Quinn,Yan Yan,Ashish Kundu,Yuan Hong*

Main category: cs.CR

TL;DR: 提出PLRV - O框架优化DP - SGD噪声分布，在严格隐私约束下提高效用，实验显示在CIFAR - 10和SST - 2上比高斯噪声效果好。


<details>
  <summary>Details</summary>
Motivation: 传统DP - SGD的高斯和拉普拉斯噪声机制单自由度参数使隐私损失和效用退化关联，任务变化时难以独立控制两者。

Method: 引入PLRV - O框架，定义参数化DP - SGD噪声分布的搜索空间，优化隐私损失矩与效用损失的关系，使噪声能适应任务特定要求。

Result: 在CIFAR - 10上，微调的ViT在epsilon约0.5时准确率达94.03%，高于高斯噪声的83.93%；在SST - 2上，RoBERTa - large在epsilon约0.2时准确率达92.20%，高于高斯噪声的50.25%。

Conclusion: PLRV - O能在严格隐私约束下大幅提高效用。

Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) is a standard
method for enforcing privacy in deep learning, typically using the Gaussian
mechanism to perturb gradient updates. However, conventional mechanisms such as
Gaussian and Laplacian noise are parameterized only by variance or scale. This
single degree of freedom ties the magnitude of noise directly to both privacy
loss and utility degradation, preventing independent control of these two
factors. The problem becomes more pronounced when the number of composition
rounds T and batch size B vary across tasks, as these variations induce
task-dependent shifts in the privacy-utility trade-off, where small changes in
noise parameters can disproportionately affect model accuracy. To address this
limitation, we introduce PLRV-O, a framework that defines a broad search space
of parameterized DP-SGD noise distributions, where privacy loss moments are
tightly characterized yet can be optimized more independently with respect to
utility loss. This formulation enables systematic adaptation of noise to
task-specific requirements, including (i) model size, (ii) training duration,
(iii) batch sampling strategies, and (iv) clipping thresholds under both
training and fine-tuning settings. Empirical results demonstrate that PLRV-O
substantially improves utility under strict privacy constraints. On CIFAR-10, a
fine-tuned ViT achieves 94.03% accuracy at epsilon approximately 0.5, compared
to 83.93% with Gaussian noise. On SST-2, RoBERTa-large reaches 92.20% accuracy
at epsilon approximately 0.2, versus 50.25% with Gaussian.

</details>


### [329] [Embedding Poisoning: Bypassing Safety Alignment via Embedding Semantic Shift](https://arxiv.org/abs/2509.06338)
*Shuai Yuan,Zhibo Zhang,Yuxi Li,Guangdong Bai,Wang Kailong*

Main category: cs.CR

TL;DR: 本文指出大语言模型在部署阶段嵌入层存在安全漏洞，提出SEP攻击框架，对六种模型评估攻击成功率高，强调嵌入层完整性检查的重要性。


<details>
  <summary>Details</summary>
Motivation: 公共平台的大语言模型在安全扫描时难以检测嵌入层细微操纵，存在安全挑战。

Method: 提出基于搜索的嵌入中毒（SEP）框架，向高风险标记关联的嵌入中引入优化扰动，利用模型响应的线性转变确定规避对齐保护的扰动窗口。

Result: 在六种对齐的大语言模型上评估，SEP平均攻击成功率达96.43%，保持良性任务性能并规避传统检测机制。

Conclusion: 部署安全存在关键疏漏，未来大语言模型防御策略需进行嵌入层完整性检查。

Abstract: The widespread distribution of Large Language Models (LLMs) through public
platforms like Hugging Face introduces significant security challenges. While
these platforms perform basic security scans, they often fail to detect subtle
manipulations within the embedding layer. This work identifies a novel class of
deployment phase attacks that exploit this vulnerability by injecting
imperceptible perturbations directly into the embedding layer outputs without
modifying model weights or input text. These perturbations, though
statistically benign, systematically bypass safety alignment mechanisms and
induce harmful behaviors during inference. We propose Search based Embedding
Poisoning(SEP), a practical, model agnostic framework that introduces carefully
optimized perturbations into embeddings associated with high risk tokens. SEP
leverages a predictable linear transition in model responses, from refusal to
harmful output to semantic deviation to identify a narrow perturbation window
that evades alignment safeguards. Evaluated across six aligned LLMs, SEP
achieves an average attack success rate of 96.43% while preserving benign task
performance and evading conventional detection mechanisms. Our findings reveal
a critical oversight in deployment security and emphasize the urgent need for
embedding level integrity checks in future LLM defense strategies.

</details>


### [330] [AttestLLM: Efficient Attestation Framework for Billion-scale On-device LLMs](https://arxiv.org/abs/2509.06326)
*Ruisi Zhang,Yifei Zhao,Neusha Javidnia,Mengxin Zheng,Farinaz Koushanfar*

Main category: cs.CR

TL;DR: 提出AttestLLM框架保护设备供应商硬件级知识产权，确保只有授权大语言模型可在目标平台执行，评估显示其可靠、高效且能抵御攻击。


<details>
  <summary>Details</summary>
Motivation: 随着设备端大语言模型广泛应用，验证本地设备上模型合法性至关重要，现有认证技术不适用于大语言模型。

Method: 采用算法/软件/硬件协同设计方法，在大语言模型构建块的激活分布上嵌入水印签名，优化可信执行环境中的认证协议。

Result: 在Llama、Qwen和Phi系列大语言模型上的概念验证评估，证明了AttestLLM的认证可靠性、保真度和效率。

Conclusion: AttestLLM能确保模型合法性，抵御模型替换和伪造攻击。

Abstract: As on-device LLMs(e.g., Apple on-device Intelligence) are widely adopted to
reduce network dependency, improve privacy, and enhance responsiveness,
verifying the legitimacy of models running on local devices becomes critical.
Existing attestation techniques are not suitable for billion-parameter Large
Language Models (LLMs), struggling to remain both time- and memory-efficient
while addressing emerging threats in the LLM era. In this paper, we present
AttestLLM, the first-of-its-kind attestation framework to protect the
hardware-level intellectual property (IP) of device vendors by ensuring that
only authorized LLMs can execute on target platforms. AttestLLM leverages an
algorithm/software/hardware co-design approach to embed robust watermarking
signatures onto the activation distributions of LLM building blocks. It also
optimizes the attestation protocol within the Trusted Execution Environment
(TEE), providing efficient verification without compromising inference
throughput. Extensive proof-of-concept evaluations on LLMs from Llama, Qwen,
and Phi families for on-device use cases demonstrate AttestLLM's attestation
reliability, fidelity, and efficiency. Furthermore, AttestLLM enforces model
legitimacy and exhibits resilience against model replacement and forgery
attacks.

</details>


### [331] [Signal-Based Malware Classification Using 1D CNNs](https://arxiv.org/abs/2509.06548)
*Jack Wilkie,Hanan Hindy,Ivan Andonovic,Christos Tachtatzis,Robert Atkinson*

Main category: cs.CR

TL;DR: 现有恶意软件分类方法有局限，本文将文件调整为1D信号分类，改进现有2D CNN架构并开发定制1D CNN，在MalNet数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现代恶意软件混淆技术使传统静态分析失效，动态分析资源消耗大；现有2D图像转换方法存在信息损失，影响分类性能。

Method: 将文件调整为1D信号避免启发式重塑和量化噪声；改进现有2D CNN架构以处理1D信号；开发基于ResNet和挤压 - 激励层的定制1D CNN。

Result: 定制的1D CNN在MalNet数据集上的二进制、类型和家族级分类的F1分数分别达到0.874、0.503和0.507。

Conclusion: 将文件调整为1D信号的方法可提升恶意软件分类性能，为未来模型提供新途径。

Abstract: Malware classification is a contemporary and ongoing challenge in
cyber-security: modern obfuscation techniques are able to evade traditional
static analysis, while dynamic analysis is too resource intensive to be
deployed at a large scale. One prominent line of research addresses these
limitations by converting malware binaries into 2D images by heuristically
reshaping them into a 2D grid before resizing using Lanczos resampling. These
images can then be classified based on their textural information using
computer vision approaches. While this approach can detect obfuscated malware
more effectively than static analysis, the process of converting files into 2D
images results in significant information loss due to both quantisation noise,
caused by rounding to integer pixel values, and the introduction of 2D
dependencies which do not exist in the original data. This loss of signal
limits the classification performance of the downstream model. This work
addresses these weaknesses by instead resizing the files into 1D signals which
avoids the need for heuristic reshaping, and additionally these signals do not
suffer from quantisation noise due to being stored in a floating-point format.
It is shown that existing 2D CNN architectures can be readily adapted to
classify these 1D signals for improved performance. Furthermore, a bespoke 1D
convolutional neural network, based on the ResNet architecture and
squeeze-and-excitation layers, was developed to classify these signals and
evaluated on the MalNet dataset. It was found to achieve state-of-the-art
performance on binary, type, and family level classification with F1 scores of
0.874, 0.503, and 0.507, respectively, paving the way for future models to
operate on the proposed signal modality.

</details>


### [332] [When Secure Isn't: Assessing the Security of Machine Learning Model Sharing](https://arxiv.org/abs/2509.06703)
*Gabriele Digregorio,Marco Di Gennaro,Stefano Zanero,Stefano Longari,Michele Carminati*

Main category: cs.CR

TL;DR: 本文评估机器学习模型共享框架和中心的安全状况，发现存在安全风险，揭示相关误区并给出建议。


<details>
  <summary>Details</summary>
Motivation: 当前模型共享工具存在未被充分探索的安全风险，从业者和开发者安全意识有限，需推动更注重安全的模型共享文化。

Method: 评估框架和中心的安全状况，分析安全机制是否提供实际保护，调查用户对模型共享安全的看法。

Result: 多数框架和中心仅部分解决安全风险，常将责任推给用户；发现六个0-day漏洞；用户因安全叙事而信任安全设置。

Conclusion: 破除模型共享问题已基本解决及文件格式可保证安全的误区，给出加强模型共享生态系统安全的建议。

Abstract: The rise of model-sharing through frameworks and dedicated hubs makes Machine
Learning significantly more accessible. Despite their benefits, these tools
expose users to underexplored security risks, while security awareness remains
limited among both practitioners and developers. To enable a more
security-conscious culture in Machine Learning model sharing, in this paper we
evaluate the security posture of frameworks and hubs, assess whether
security-oriented mechanisms offer real protection, and survey how users
perceive the security narratives surrounding model sharing. Our evaluation
shows that most frameworks and hubs address security risks partially at best,
often by shifting responsibility to the user. More concerningly, our analysis
of frameworks advertising security-oriented settings and complete model sharing
uncovered six 0-day vulnerabilities enabling arbitrary code execution. Through
this analysis, we debunk the misconceptions that the model-sharing problem is
largely solved and that its security can be guaranteed by the file format used
for sharing. As expected, our survey shows that the surrounding security
narrative leads users to consider security-oriented settings as trustworthy,
despite the weaknesses shown in this work. From this, we derive takeaways and
suggestions to strengthen the security of model-sharing ecosystems.

</details>


### [333] [Imitative Membership Inference Attack](https://arxiv.org/abs/2509.06796)
*Yuntao Du,Yuetian Chen,Hanshen Xiao,Bruno Ribeiro,Ninghui Li*

Main category: cs.CR

TL;DR: 提出模仿成员推理攻击（IMIA），采用新的模仿训练技术，性能优于现有方法且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击（MIA）依赖训练大量影子模型，计算开销大。

Method: 引入IMIA，采用模仿训练技术构建少量模仿模型进行推理。

Result: IMIA在各种攻击设置中显著优于现有MIA，且计算成本不到现有方法的5%。

Conclusion: IMIA是一种高效的成员推理攻击方法，能在降低计算成本的同时提高攻击性能。

Abstract: A Membership Inference Attack (MIA) assesses how much a target machine
learning model reveals about its training data by determining whether specific
query instances were part of the training set. State-of-the-art MIAs rely on
training hundreds of shadow models that are independent of the target model,
leading to significant computational overhead. In this paper, we introduce
Imitative Membership Inference Attack (IMIA), which employs a novel imitative
training technique to strategically construct a small number of target-informed
imitative models that closely replicate the target model's behavior for
inference. Extensive experimental results demonstrate that IMIA substantially
outperforms existing MIAs in various attack settings while only requiring less
than 5% of the computational cost of state-of-the-art approaches.

</details>


### [334] [An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection](https://arxiv.org/abs/2509.06920)
*Haywood Gelman,John D. Hastings,David Kenley*

Main category: cs.CR

TL;DR: 研究用大语言模型Claude Sonnet 3.7合成系统日志消息检测内部威胁，Sonnet 3.7表现优于GPT - 4o，显示了大语言模型在合成数据集生成和内部威胁检测的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有内部威胁研究依赖静态且访问受限的数据集，限制了自适应检测模型的发展。

Method: 使用Claude Sonnet 3.7动态合成系统日志消息，包含内部威胁场景指标，数据高度不平衡，用Claude Sonnet 3.7和GPT - 4o分析日志，通过统计指标评估性能。

Result: Sonnet 3.7在几乎所有指标上始终优于GPT - 4o，尤其在减少误报和提高检测准确性方面。

Conclusion: 大语言模型在合成数据集生成和内部威胁检测方面有很大潜力。

Abstract: Insider threats are a growing organizational problem due to the complexity of
identifying their technical and behavioral elements. A large research body is
dedicated to the study of insider threats from technological, psychological,
and educational perspectives. However, research in this domain has been
generally dependent on datasets that are static and limited access which
restricts the development of adaptive detection models. This study introduces a
novel, ethically grounded approach that uses the large language model (LLM)
Claude Sonnet 3.7 to dynamically synthesize syslog messages, some of which
contain indicators of insider threat scenarios. The messages reflect real-world
data distributions by being highly imbalanced (1% insider threats). The syslogs
were analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with
their performance evaluated through statistical metrics including precision,
recall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across
nearly all metrics, particularly in reducing false alarms and improving
detection accuracy. The results show strong promise for the use of LLMs in
synthetic dataset generation and insider threat detection.

</details>


### [335] [Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and Opportunities](https://arxiv.org/abs/2509.06921)
*Safayat Bin Hakim,Muhammad Adil,Alvaro Velasquez,Shouhuai Xu,Houbing Herbert Song*

Main category: cs.CR

TL;DR: 文章系统性分析2019 - 2025年7月127篇关于神经符号（NeSy）AI在网络安全领域的文献，指出其优势、挑战及发展方向。


<details>
  <summary>Details</summary>
Motivation: 传统AI在网络安全存在不足，NeSy AI有潜力但缺乏系统理解，需要对其进行系统研究。

Method: 分析127篇文献，引入G - I - A框架评估相关系统。

Result: 发现多智能体NeSy架构有优势，存在标准化差距等挑战，因果推理集成是重大进步，系统有双重用途。

Conclusion: 强调需要社区驱动的标准化框架和负责任的开发实践，确保发展服务于网络安全防御目标。

Abstract: Traditional Artificial Intelligence (AI) approaches in cybersecurity exhibit
fundamental limitations: inadequate conceptual grounding leading to
non-robustness against novel attacks; limited instructibility impeding
analyst-guided adaptation; and misalignment with cybersecurity objectives.
Neuro-Symbolic (NeSy) AI has emerged with the potential to revolutionize
cybersecurity AI. However, there is no systematic understanding of this
emerging approach. These hybrid systems address critical cybersecurity
challenges by combining neural pattern recognition with symbolic reasoning,
enabling enhanced threat understanding while introducing concerning autonomous
offensive capabilities that reshape threat landscapes. In this survey, we
systematically characterize this field by analyzing 127 publications spanning
2019-July 2025. We introduce a Grounding-Instructibility-Alignment (G-I-A)
framework to evaluate these systems, focusing on both cyber defense and cyber
offense across network security, malware analysis, and cyber operations. Our
analysis shows advantages of multi-agent NeSy architectures and identifies
critical implementation challenges including standardization gaps,
computational complexity, and human-AI collaboration requirements that
constrain deployment. We show that causal reasoning integration is the most
transformative advancement, enabling proactive defense beyond correlation-based
approaches. Our findings highlight dual-use implications where autonomous
systems demonstrate substantial capabilities in zero-day exploitation while
achieving significant cost reductions, altering threat dynamics. We provide
insights and future research directions, emphasizing the urgent need for
community-driven standardization frameworks and responsible development
practices that ensure advancement serves defensive cybersecurity objectives
while maintaining societal alignment.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [336] [A General Framework for Low Soundness Homomorphism Testing](https://arxiv.org/abs/2509.05871)
*Tushant Mittal,Sourya Roy*

Main category: cs.CC

TL;DR: 本文提出测试有限群同态算法的通用框架，给出低稳健性下多类群的常数查询测试，并改进循环群组合列表译码界。


<details>
  <summary>Details</summary>
Motivation: 此前仅已知常数最大阶阿贝尔群的同态测试，未知非阿贝尔群的测试，需要开发新方法。

Method: 引入通用框架来设计和分析算法。

Result: 给出多类群的常数查询测试，恢复Kiwi的结果，改进循环群组合列表译码界。

Conclusion: 所提框架有效，可用于有限群同态测试及改进组合列表译码界。

Abstract: We introduce a general framework to design and analyze algorithms for the
problem of testing homomorphisms between finite groups in the low-soundness
regime.
  In this regime, we give the first constant-query tests for various families
of groups. These include tests for: (i) homomorphisms between arbitrary cyclic
groups, (ii) homomorphisms between any finite group and $\mathbb{Z}_p$, (iii)
automorphisms of dihedral and symmetric groups, (iv) inner automorphisms of
non-abelian finite simple groups and extraspecial groups, and (v) testing
linear characters of $\mathrm{GL}_n(\mathbb{F}_q)$, and finite-dimensional Lie
algebras over $\mathbb{F}_q$. We also recover the result of Kiwi [TCS'03] for
testing homomorphisms between $\mathbb{F}_q^n$ and $\mathbb{F}_q$.
  Prior to this work, such tests were only known for abelian groups with a
constant maximal order (such as $\mathbb{F}_q^n$). No tests were known for
non-abelian groups.
  As an additional corollary, our framework gives combinatorial list decoding
bounds for cyclic groups with list size dependence of $O(\varepsilon^{-2})$
(for agreement parameter $\varepsilon$). This improves upon the currently
best-known bound of $O(\varepsilon^{-105})$ due to Dinur, Grigorescu, Kopparty,
and Sudan [STOC'08], and Guo and Sudan [RANDOM'14].

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [337] [Graph Connectionist Temporal Classification for Phoneme Recognition](https://arxiv.org/abs/2509.05399)
*Henry Grafé,Hugo Van hamme*

Main category: eess.AS

TL;DR: 本文提出将GTC应用于APR，实验表明考虑单词多种发音可降低音素错误率，是训练APR系统的有效策略。


<details>
  <summary>Details</summary>
Motivation: 标准CTC损失无法处理G2P系统生成的多发音歧义，需要改进训练方法。

Method: 将Graph Temporal Classification (GTC) 应用于APR场景，从替代音素序列图进行训练。

Result: 在英语和荷兰语数据集上，使用GTC考虑多发音的训练方式比使用CTC的基线方法持续降低了音素错误率。

Conclusion: 将发音变化融入损失函数是从基于G2P的嘈杂监督中训练APR系统的有前景策略。

Abstract: Automatic Phoneme Recognition (APR) systems are often trained using pseudo
phoneme-level annotations generated from text through Grapheme-to-Phoneme (G2P)
systems. These G2P systems frequently output multiple possible pronunciations
per word, but the standard Connectionist Temporal Classification (CTC) loss
cannot account for such ambiguity during training. In this work, we adapt Graph
Temporal Classification (GTC) to the APR setting. GTC enables training from a
graph of alternative phoneme sequences, allowing the model to consider multiple
pronunciations per word as valid supervision. Our experiments on English and
Dutch data sets show that incorporating multiple pronunciations per word into
the training loss consistently improves phoneme error rates compared to a
baseline trained with CTC. These results suggest that integrating pronunciation
variation into the loss function is a promising strategy for training APR
systems from noisy G2P-based supervision.

</details>


### [338] [Beamforming-LLM: What, Where and When Did I Miss?](https://arxiv.org/abs/2509.06221)
*Vishal Choudhari*

Main category: eess.AS

TL;DR: 提出Beamforming - LLM系统，结合空间音频捕获与检索增强生成，支持自然语言查询，提供友好界面和总结，为智能听觉记忆系统奠基。


<details>
  <summary>Details</summary>
Motivation: 让用户在多说话者环境中能语义化召回可能错过的对话。

Method: 结合麦克风阵列进行空间音频捕获与检索增强生成，用波束形成分离定向音频流，用Whisper转录，用句子编码器嵌入向量数据库，用轻量级大语言模型总结。

Result: 得到一个提供对比总结、空间上下文和带时间戳音频回放的用户友好界面。

Conclusion: 该工作为智能听觉记忆系统奠定基础，在辅助技术、会议总结和上下文感知个人空间计算等领域有广泛应用。

Abstract: We present Beamforming-LLM, a system that enables users to semantically
recall conversations they may have missed in multi-speaker environments. The
system combines spatial audio capture using a microphone array with
retrieval-augmented generation (RAG) to support natural language queries such
as, "What did I miss when I was following the conversation on dogs?"
Directional audio streams are separated using beamforming, transcribed with
Whisper, and embedded into a vector database using sentence encoders. Upon
receiving a user query, semantically relevant segments are retrieved,
temporally aligned with non-attended segments, and summarized using a
lightweight large language model (GPT-4o-mini). The result is a user-friendly
interface that provides contrastive summaries, spatial context, and timestamped
audio playback. This work lays the foundation for intelligent auditory memory
systems and has broad applications in assistive technology, meeting
summarization, and context-aware personal spatial computing.

</details>


### [339] [Integrating Spatial and Semantic Embeddings for Stereo Sound Event Localization in Videos](https://arxiv.org/abs/2509.06598)
*Davide Berghi,Philip J. B. Jackson*

Main category: eess.AS

TL;DR: 本文针对常规视频内容中的3D SELD多模态任务，通过集成预训练的语言对齐模型增强标准SELD架构，结合多种策略在DCASE 2025挑战赛中获第二名，未来将探索模态贡献和架构改进。


<details>
  <summary>Details</summary>
Motivation: 传统SELD方法因数据限制难以从大规模预训练中受益，且3D SELD任务需跨多维度推理，语义维度建模具挑战性。

Method: 集成预训练的CLAP和OWL - ViT模型，将其嵌入到改进的Cross - Modal Conformer模块进行多模态融合；在DCASE2025任务3数据集上进行消融研究；制作大型合成数据集并进行数据增强；结合预训练、模型集成和视觉后处理。

Result: 在DCASE 2025挑战赛任务3（Track B）中获得第二名。

Conclusion: 所提方法有效，未来将探索模态特定贡献和架构优化。

Abstract: In this study, we address the multimodal task of stereo sound event
localization and detection with source distance estimation (3D SELD) in regular
video content. 3D SELD is a complex task that combines temporal event
classification with spatial localization, requiring reasoning across spatial,
temporal, and semantic dimensions. The last is arguably the most challenging to
model. Traditional SELD approaches typically rely on multichannel input,
limiting their capacity to benefit from large-scale pre-training due to data
constraints. To overcome this, we enhance a standard SELD architecture with
semantic information by integrating pre-trained, contrastive language-aligned
models: CLAP for audio and OWL-ViT for visual inputs. These embeddings are
incorporated into a modified Conformer module tailored for multimodal fusion,
which we refer to as the Cross-Modal Conformer. We perform an ablation study on
the development set of the DCASE2025 Task3 Stereo SELD Dataset to assess the
individual contributions of the language-aligned models and benchmark against
the DCASE Task 3 baseline systems. Additionally, we detail the curation process
of large synthetic audio and audio-visual datasets used for model pre-training.
These datasets were further expanded through left-right channel swapping
augmentation. Our approach, combining extensive pre-training, model ensembling,
and visual post-processing, achieved second rank in the DCASE 2025 Challenge
Task 3 (Track B), underscoring the effectiveness of our method. Future work
will explore the modality-specific contributions and architectural refinements.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [340] [Universality of physical neural networks with multivariate nonlinearity](https://arxiv.org/abs/2509.05420)
*Benjamin Savinson,David J. Norris,Siddhartha Mishra,Samuel Lanthaler*

Main category: physics.optics

TL;DR: 提出物理神经网络通用性定理，给出架构并提出扩展方法，适用于多种系统。


<details>
  <summary>Details</summary>
Motivation: 人工智能能耗大，物理神经网络有潜力但无法确定是否具通用性阻碍发展。

Method: 提出通用性定理，基于此提出自由空间光学架构，结合时间复用方法。

Result: 提出的自由空间光学架构可证明具有通用性，图像分类准确率高，为片上光子设备提供扩展途径。

Conclusion: 定理和扩展方法适用于多种系统，证明进一步开发物理神经网络的合理性。

Abstract: The enormous energy demand of artificial intelligence is driving the
development of alternative hardware for deep learning. Physical neural networks
try to exploit physical systems to perform machine learning more efficiently.
In particular, optical systems can calculate with light using negligible
energy. While their computational capabilities were long limited by the
linearity of optical materials, nonlinear computations have recently been
demonstrated through modified input encoding. Despite this breakthrough, our
inability to determine if physical neural networks can learn arbitrary
relationships between data -- a key requirement for deep learning known as
universality -- hinders further progress. Here we present a fundamental theorem
that establishes a universality condition for physical neural networks. It
provides a powerful mathematical criterion that imposes device constraints,
detailing how inputs should be encoded in the tunable parameters of the
physical system. Based on this result, we propose a scalable architecture using
free-space optics that is provably universal and achieves high accuracy on
image classification tasks. Further, by combining the theorem with temporal
multiplexing, we present a route to potentially huge effective system sizes in
highly practical but poorly scalable on-chip photonic devices. Our theorem and
scaling methods apply beyond optical systems and inform the design of a wide
class of universal, energy-efficient physical neural networks, justifying
further efforts in their development.

</details>


### [341] [Meta-training of diffractive meta-neural networks for super-resolution direction of arrival estimation](https://arxiv.org/abs/2509.05926)
*Songtao Yang,Sheng Gao,Chu Wu,Zejia Zhao,Haiou Zhang,Xing Lin*

Main category: physics.optics

TL;DR: 提出衍射元神经网络（DMNNs）用于电磁场调制，实现多任务学习和高吞吐量超分辨率方向估计，实验验证其高性能。


<details>
  <summary>Details</summary>
Motivation: 现有架构在集成大规模多维超表面与精确网络训练存在挑战，且未利用多维电磁场编码方案进行超分辨率传感。

Method: DMNN集成预训练的迷你元网络，通过基于梯度的元训练反向设计结构参数；利用x和y偏振通道同时解析方位角和仰角，频率复用角间隔交织产生光谱编码光学超振荡；后处理使用轻量级电子神经网络。

Result: 三层DMNN在27GHz、29GHz和31GHz下实现约7倍瑞利衍射极限角分辨率（0.5°），在±11.5°视场内对两个非相干目标的平均绝对误差为0.048°，角度估计吞吐量比现有方法高一个数量级（1917）。

Conclusion: 所提出的架构利用固有高并行性和全光编码方法，推动了高维光子计算系统在超高分辨率、高吞吐量应用方面的发展。

Abstract: Diffractive neural networks leverage the high-dimensional characteristics of
electromagnetic (EM) fields for high-throughput computing. However, the
existing architectures face challenges in integrating large-scale
multidimensional metasurfaces with precise network training and haven't
utilized multidimensional EM field coding scheme for super-resolution sensing.
Here, we propose diffractive meta-neural networks (DMNNs) for accurate EM field
modulation through metasurfaces, which enable multidimensional multiplexing and
coding for multi-task learning and high-throughput super-resolution direction
of arrival estimation. DMNN integrates pre-trained mini-metanets to
characterize the amplitude and phase responses of meta-atoms across different
polarizations and frequencies, with structure parameters inversely designed
using the gradient-based meta-training. For wide-field super-resolution angle
estimation, the system simultaneously resolves azimuthal and elevational angles
through x and y-polarization channels, while the interleaving of
frequency-multiplexed angular intervals generates spectral-encoded optical
super-oscillations to achieve full-angle high-resolution estimation.
Post-processing lightweight electronic neural networks further enhance the
performance. Experimental results validate that a three-layer DMNN operating at
27 GHz, 29 GHz, and 31 GHz achieves $\sim7\times$ Rayleigh diffraction-limited
angular resolution (0.5$^\circ$), a mean absolute error of 0.048$^\circ$ for
two incoherent targets within a $\pm 11.5^\circ$ field of view, and an angular
estimation throughput an order of magnitude higher (1917) than that of existing
methods. The proposed architecture advances high-dimensional photonic computing
systems by utilizing inherent high-parallelism and all-optical coding methods
for ultra-high-resolution, high-throughput applications.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [342] [Distributed Automatic Generation Control subject to Ramp-Rate-Limits: Anytime Feasibility and Uniform Network-Connectivity](https://arxiv.org/abs/2509.06588)
*Mohammadreza Doostmohammadian,Hamid R. Rabiee*

Main category: eess.SY

TL;DR: 本文将信息共享网络中的自动发电控制视为多智能体系统，提出分布式优化方案，解决发电机斜坡速率限制问题，还考虑内部非线性以提高收敛速度并证明可容忍通信链路移除。


<details>
  <summary>Details</summary>
Motivation: 现有线性/非线性优化解决方案通常忽略发电机斜坡速率限制（RRL），导致无法收敛到最优成本或失去可行性，因此需要提出考虑RRL的解决方案。

Method: 基于信息共识算法将优化解决方案分布到各智能体，在算法各迭代时刻考虑斜坡速率限制约束、箱约束和耦合约束，考虑内部基于符号函数的非线性。

Result: 所提解决方案能解决斜坡速率限制问题，提高算法收敛速度，且可容忍通信链路移除。

Conclusion: 提出的考虑RRL的分布式优化方案具有可行性和有效性，能保证发电需求平衡并提高算法性能，且对通信网络故障有一定容忍度。

Abstract: This paper considers automatic generation control over an information-sharing
network of communicating generators as a multi-agent system. The optimization
solution is distributed among the agents based on information consensus
algorithms, while addressing the generators' ramp-rate-limits (RRL). This is
typically ignored in the existing linear/nonlinear optimization solutions but
they exist in real-time power generation scenarios. Without addressing the RRL,
the generators cannot follow the assigned rate of generating power by the
optimization algorithm; therefore, the existing solutions may not necessarily
converge to the exact optimal cost or may lose feasibility in practice. The
proposed solution in this work addresses the ramp-rate-limit constraint along
with the box constraint (limits on the generated powers) and the
coupling-constraint (generation-demand balance) at all iteration times of the
algorithm. The latter is referred to as the anytime feasibility and implies
that at every termination point of the algorithm, the balance between the
demand and generated power holds. To improve the convergence rate of the
algorithm we further consider internal signum-based nonlinearity. We also show
that our solution can tolerate communication link removal. This follows from
the uniform-connectivity assumption on the communication network.

</details>


### [343] [Enhancing Low-Altitude Airspace Security: MLLM-Enabled UAV Intent Recognition](https://arxiv.org/abs/2509.06312)
*Guangyu Lei,Tianhao Liang,Yuqi Ping,Xinglin Chen,Longyu Zhou,Junwei Wu,Xiyuan Zhang,Huahao Ding,Xingjian Zhang,Weijie Yuan,Tingting Zhang,Qinyu Zhang*

Main category: eess.SY

TL;DR: 本文聚焦结合多模态大语言模型（MLLMs）进行无人机意图识别，提出架构、回顾相关工作、通过用例验证可行性并讨论未来挑战与建议。


<details>
  <summary>Details</summary>
Motivation: 低空经济快速发展，需要有效感知和识别非合作无人机意图，MLLMs的生成推理能力为此提供了有前景的方法。

Method: 提出MLLM支持的无人机意图识别架构，利用多模态感知系统获取无人机信息产生结构化输入，MLLM结合环境等信息输出识别结果；回顾相关工作；进行低空对抗用例验证。

Result: 通过低空对抗用例证明了所提架构的可行性。

Conclusion: 讨论了未来挑战并给出进一步应用的战略建议。

Abstract: The rapid development of the low-altitude economy emphasizes the critical
need for effective perception and intent recognition of non-cooperative
unmanned aerial vehicles (UAVs). The advanced generative reasoning capabilities
of multimodal large language models (MLLMs) present a promising approach in
such tasks. In this paper, we focus on the combination of UAV intent
recognition and the MLLMs. Specifically, we first present an MLLM-enabled UAV
intent recognition architecture, where the multimodal perception system is
utilized to obtain real-time payload and motion information of UAVs, generating
structured input information, and MLLM outputs intent recognition results by
incorporating environmental information, prior knowledge, and tactical
preferences. Subsequently, we review the related work and demonstrate their
progress within the proposed architecture. Then, a use case for low-altitude
confrontation is conducted to demonstrate the feasibility of our architecture
and offer valuable insights for practical system design. Finally, the future
challenges are discussed, followed by corresponding strategic recommendations
for further applications.

</details>


### [344] [Reinforcement learning meets bioprocess control through behaviour cloning: Real-world deployment in an industrial photobioreactor](https://arxiv.org/abs/2509.06853)
*Juan D. Gil,Ehecatl Antonio Del Rio Chanona,José L. Guzmán,Manuel Berenguel*

Main category: eess.SY

TL;DR: 本文提出结合强化学习（RL）与行为克隆（BC）的方法用于开放光生物反应器（PBR）系统的pH调节，经模拟和实验验证其有效性和优势。


<details>
  <summary>Details</summary>
Motivation: 活细胞作为生产单元的复杂性给开放PBR系统维持稳定最佳生物过程条件带来挑战，现有方法难以应对非线性和易受干扰的生物过程。

Method: 采用结合BC的RL控制方法，先离线训练，让RL智能体从名义PID控制器生成的轨迹学习，再进行每日在线微调。

Result: 模拟研究显示，相比PID控制，绝对误差积分（IAE）降低8%，控制努力降低54%；相比标准离线策略RL，IAE降低5%，控制努力降低7%。8天实验验证了方法的鲁棒性和可靠性。

Conclusion: 基于RL的方法在生物过程控制中有潜力，为应用于其他非线性、易受干扰系统奠定基础。

Abstract: The inherent complexity of living cells as production units creates major
challenges for maintaining stable and optimal bioprocess conditions, especially
in open Photobioreactors (PBRs) exposed to fluctuating environments. To address
this, we propose a Reinforcement Learning (RL) control approach, combined with
Behavior Cloning (BC), for pH regulation in open PBR systems. This represents,
to the best of our knowledge, the first application of an RL-based control
strategy to such a nonlinear and disturbance-prone bioprocess. Our method
begins with an offline training stage in which the RL agent learns from
trajectories generated by a nominal Proportional-Integral-Derivative (PID)
controller, without direct interaction with the real system. This is followed
by a daily online fine-tuning phase, enabling adaptation to evolving process
dynamics and stronger rejection of fast, transient disturbances. This hybrid
offline-online strategy allows deployment of an adaptive control policy capable
of handling the inherent nonlinearities and external perturbations in open
PBRs. Simulation studies highlight the advantages of our method: the Integral
of Absolute Error (IAE) was reduced by 8% compared to PID control and by 5%
relative to standard off-policy RL. Moreover, control effort decreased
substantially-by 54% compared to PID and 7% compared to standard RL-an
important factor for minimizing operational costs. Finally, an 8-day
experimental validation under varying environmental conditions confirmed the
robustness and reliability of the proposed approach. Overall, this work
demonstrates the potential of RL-based methods for bioprocess control and paves
the way for their broader application to other nonlinear, disturbance-prone
systems.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [345] [Rethinking Beta: A Causal Take on CAPM](https://arxiv.org/abs/2509.05760)
*Naftali Cohen*

Main category: econ.TH

TL;DR: 指出CAPM回归传统解释存在‘聚合器矛盾’，将CAPM形式化为结构因果模型分析，表明同期贝塔是代理而非机制，CAPM应作关联解读。


<details>
  <summary>Details</summary>
Motivation: 解决CAPM回归传统解释（市场回报同期‘导致’个股回报）与市场回报是成分股加权总和这一事实的矛盾。

Method: 将CAPM形式化为结构因果模型，分析外部驱动、市场和资产间的三节点图，结合简化模型和美国大盘股数据。

Result: 同期贝塔是代理而非机制，真正的市场到股票传导仅滞后出现且经济意义不大。

Conclusion: CAPM应作关联解读，风险管理和归因应转向明确因果路径。

Abstract: The CAPM regression is typically interpreted as if the market return
contemporaneously \emph{causes} individual returns, motivating beta-neutral
portfolios and factor attribution. For realized equity returns, however, this
interpretation is inconsistent: a same-period arrow $R_{m,t} \to R_{i,t}$
conflicts with the fact that $R_m$ is itself a value-weighted aggregate of its
constituents, unless $R_m$ is lagged or leave-one-out -- the ``aggregator
contradiction.'' We formalize CAPM as a structural causal model and analyze the
admissible three-node graphs linking an external driver $Z$, the market $R_m$,
and an asset $R_i$. The empirically plausible baseline is a \emph{fork}, $Z \to
\{R_m, R_i\}$, not $R_m \to R_i$. In this setting, OLS beta reflects not a
causal transmission, but an attenuated proxy for how well $R_m$ captures the
underlying driver $Z$. Consequently, ``beta-neutral'' portfolios can remain
exposed to macro or sectoral shocks, and hedging on $R_m$ can import
index-specific noise. Using stylized models and large-cap U.S.\ equity data, we
show that contemporaneous betas act like proxies rather than mechanisms; any
genuine market-to-stock channel, if at all, appears only at a lag and with
modest economic significance. The practical message is clear: CAPM should be
read as associational. Risk management and attribution should shift from fixed
factor menus to explicitly declared causal paths, with ``alpha'' reserved for
what remains invariant once those causal paths are explicitly blocked.

</details>


### [346] [Characterizing Optimality in Dynamic Settings: A Monotonicity-based Approach](https://arxiv.org/abs/2509.05354)
*Zhuokai Huang,Demian Pouzo,Andrés Rodríguez-Clare*

Main category: econ.TH

TL;DR: 本文开发新的分析方法研究动态优化问题最优路径，介绍定位函数性质及作用，并通过实例说明。


<details>
  <summary>Details</summary>
Motivation: 研究动态优化问题在一般单调性条件下的最优路径。

Method: 开发基于定位函数的分析方法，通过定位函数的根和斜率研究稳态及稳定性。

Result: 在严格凹条件下可描述吸引域和动态，非凹时也有明确结果，还能进行稳态比较静态分析。

Conclusion: 该方法无需求解完整动态规划，通过实例证明其有效性。

Abstract: We develop a novel analytical method for studying optimal paths in dynamic
optimization problems under general monotonicity conditions. The method centers
on a locator function -- a simple object constructed directly from the model's
primitives -- whose roots identify interior steady states and whose slope
determines their local stability. Under strict concavity of the payoff
function, the locator function also characterizes basins of attraction,
yielding a complete description of qualitative dynamics. Without concavity, it
can still deliver sharp results: if the function is single crossing from above,
its root identifies a globally stable steady state; if the locator function is
inverted-U-shaped with two interior roots (a typical case), only the higher
root can be a locally stable interior steady state. The locator function
further enables comparative statics of steady states with respect to parameters
through direct analysis of its derivatives. These results are obtained without
solving the full dynamic program. We illustrate the approach using a
generalized neoclassical growth model, a rational (un)fitness model, and a
learning-by-doing economy.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [347] [Spectral Methods in Complex Systems](https://arxiv.org/abs/2509.05793)
*Francesco Caravelli*

Main category: cond-mat.stat-mech

TL;DR: 本文是复杂系统谱方法的统一介绍，可作操作手册，介绍工具等并阐述谱、动力学和结构联系，有广泛应用，表述易懂。


<details>
  <summary>Details</summary>
Motivation: 为复杂系统谱方法研究提供统一且实用的介绍，使不同学科人员能应用相关工具。

Method: 从矩阵恒等式和求逆技术入手，阐述有限维系统中谱、动力学和结构的联系。

Result: 涵盖从网络动力稳定性到金融稳定性等多领域应用。

Conclusion: 特征值、特征向量和预解算子为跨学科问题提供通用语言，内容适合本科生和研究者。

Abstract: These notes offer a unified introduction to spectral methods for the study of
complex systems. They are intended as an operative manual rather than a
theorem-proof textbook: the emphasis is on tools, identities, and perspectives
that can be readily applied across disciplines. Beginning with a compendium of
matrix identities and inversion techniques, the text develops the connections
between spectra, dynamics, and structure in finite-dimensional systems.
Applications range from dynamical stability and random walks on networks to
input-output economics, PageRank, epidemic spreading, memristive circuits,
synchronization phenomena, and financial stability. Throughout, the guiding
principle is that eigenvalues, eigenvectors, and resolvent operators provide a
common language linking problems in physics, mathematics, computer science, and
beyond. The presentation is informal, accessible to advanced undergraduates,
yet broad enough to serve as a reference for researchers interested in spectral
approaches to complex systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [348] [Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification](https://arxiv.org/abs/2509.06902)
*Aivin V. Solatorio*

Main category: cs.CL

TL;DR: 论文提出Proof - Carrying Numbers (PCN)协议解决大语言模型数值幻觉问题，形式化PCN并证明相关性质，其轻量且模型无关。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的数值幻觉问题无法通过现有保障措施完全解决数据保真问题，需要新方法。

Method: 提出PCN协议，将数值跨度作为与结构化声明绑定的声明绑定令牌，并在渲染器中进行验证。

Result: 形式化PCN，证明了其可靠性、完整性、故障关闭行为和策略细化下的单调性。

Conclusion: PCN建立了数值敏感场景的简单契约，只有通过验证的才能被信任，无标记则表示不确定。

Abstract: Large Language Models (LLMs) as stochastic systems may generate numbers that
deviate from available data, a failure known as \emph{numeric hallucination}.
Existing safeguards -- retrieval-augmented generation, citations, and
uncertainty estimation -- improve transparency but cannot guarantee fidelity:
fabricated or misquoted values may still be displayed as if correct. We propose
\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that
enforces numeric fidelity through mechanical verification. Under PCN, numeric
spans are emitted as \emph{claim-bound tokens} tied to structured claims, and a
verifier checks each token under a declared policy (e.g., exact equality,
rounding, aliases, or tolerance with qualifiers). Crucially, PCN places
verification in the \emph{renderer}, not the model: only claim-checked numbers
are marked as verified, and all others default to unverified. This separation
prevents spoofing and guarantees fail-closed behavior. We formalize PCN and
prove soundness, completeness under honest tokens, fail-closed behavior, and
monotonicity under policy refinement. PCN is lightweight and model-agnostic,
integrates seamlessly into existing applications, and can be extended with
cryptographic commitments. By enforcing verification as a mandatory step before
display, PCN establishes a simple contract for numerically sensitive settings:
\emph{trust is earned only by proof}, while the absence of a mark communicates
uncertainty.

</details>


### [349] [Few-Shot Query Intent Detection via Relation-Aware Prompt Learning](https://arxiv.org/abs/2509.05635)
*Liang Zhang,Yuan Li,Shijie Zhang,Zheng Zhang,Xitong Li*

Main category: cs.CL

TL;DR: 提出SAID框架和QueryAdapt机制用于少样本意图检测，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有少样本意图检测方法主要关注文本数据，忽略对话系统中的结构信息，需要改进。

Method: 提出SAID框架统一整合文本和关系结构信息进行模型预训练，提出QueryAdapt机制在关系令牌级别操作，实现更细粒度的知识转移。

Result: 在两个真实数据集上的实验表明SAID显著优于现有方法。

Conclusion: SAID框架和QueryAdapt机制有效提升少样本意图检测性能。

Abstract: Intent detection is a crucial component of modern conversational systems,
since accurately identifying user intent at the beginning of a conversation is
essential for generating effective responses. Recent efforts have focused on
studying this problem under a challenging few-shot scenario. These approaches
primarily leverage large-scale unlabeled dialogue text corpora to pretrain
language models through various pretext tasks, followed by fine-tuning for
intent detection with very limited annotations. Despite the improvements
achieved, existing methods have predominantly focused on textual data,
neglecting to effectively capture the crucial structural information inherent
in conversational systems, such as the query-query relation and query-answer
relation. To address this gap, we propose SAID, a novel framework that
integrates both textual and relational structure information in a unified
manner for model pretraining for the first time. Building on this framework, we
further propose a novel mechanism, the query-adaptive attention network
(QueryAdapt), which operates at the relation token level by generating
intent-specific relation tokens from well-learned query-query and query-answer
relations explicitly, enabling more fine-grained knowledge transfer. Extensive
experimental results on two real-world datasets demonstrate that SAID
significantly outperforms state-of-the-art methods.

</details>


### [350] [Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval](https://arxiv.org/abs/2509.06650)
*Hao Lin,Peitong Xie,Jingxue Chen,Jie Lin,Qingkun Tang,Qianchun Lu*

Main category: cs.CL

TL;DR: 提出MoLER方法优化RAG系统检索，含两阶段流程及创新策略，实验效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有粗排序优化方法难平衡领域知识学习与查询增强，导致检索性能不佳。

Method: 提出MoLER方法，有持续预训练和强化学习两阶段流程，采用MSLF策略降低计算开销。

Result: 在基准数据集上实验显示MoLER达到了最先进的性能，显著优于基线方法。

Conclusion: MoLER弥合了RAG系统的知识差距，能在专业领域实现强大且可扩展的检索。

Abstract: Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval
stage, particularly the coarse-ranking process. Existing coarse-ranking
optimization approaches often struggle to balance domain-specific knowledge
learning with query enhencement, resulting in suboptimal retrieval performance.
To address this challenge, we propose MoLER, a domain-aware RAG method that
uses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a
two-stage pipeline: a continual pre-training (CPT) phase using a Mixture of
Losses (MoL) to balance domain-specific knowledge with general language
capabilities, and a reinforcement learning (RL) phase leveraging Group Relative
Policy Optimization (GRPO) to optimize query and passage generation for
maximizing document recall. A key innovation is our Multi-query Single-passage
Late Fusion (MSLF) strategy, which reduces computational overhead during RL
training while maintaining scalable inference via Multi-query Multi-passage
Late Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER
achieves state-of-the-art performance, significantly outperforming baseline
methods. MoLER bridges the knowledge gap in RAG systems, enabling robust and
scalable retrieval in specialized domains.

</details>


### [351] [UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction](https://arxiv.org/abs/2509.06883)
*Joe Wilder,Nikhil Kadapala,Benji Xu,Mohammed Alsaadi,Aiden Parsons,Mitchell Rogers,Palash Agarwal,Adam Hassick,Laura Dietz*

Main category: cs.CL

TL;DR: 参与CheckThat!任务2英文部分，探索提示和上下文学习方法提取社交媒体段落中可核查声明，微调FLAN - T5模型获最佳METEOR分数，但其他方法有时能提取更高质量声明。


<details>
  <summary>Details</summary>
Motivation: 从社交媒体段落中提取可核查声明。

Method: 探索各种提示和上下文学习方法，包括少样本提示和不同大语言模型家族的微调。

Result: 微调FLAN - T5模型获得最佳METEOR分数，其他方法虽METEOR分数低但有时能提取更高质量声明。

Conclusion: 不同方法在提取可核查声明上各有优劣。

Abstract: We participate in CheckThat! Task 2 English and explore various methods of
prompting and in-context learning, including few-shot prompting and fine-tuning
with different LLM families, with the goal of extracting check-worthy claims
from social media passages. Our best METEOR score is achieved by fine-tuning a
FLAN-T5 model. However, we observe that higher-quality claims can sometimes be
extracted using other methods, even when their METEOR scores are lower.

</details>


### [352] [mmBERT: A Modern Multilingual Encoder with Annealed Language Learning](https://arxiv.org/abs/2509.06888)
*Marc Marone,Orion Weller,William Fleshman,Eugene Yang,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 介绍了仅编码器语言模型mmBERT，在多语言文本上预训练，采用新元素，提升性能，在分类和检索任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 近期编码器模型研究不足，尤其是多语言模型，需要新的模型来提升多语言任务表现。

Method: 引入mmBERT，采用逆掩码比率调度和逆温度采样比率，在衰减阶段添加1700多种低资源语言。

Result: mmBERT在分类任务上达到与OpenAI的o3和Google的Gemini 2.5 Pro相似的性能。

Conclusion: mmBERT在高、低资源语言的分类和检索任务上显著优于上一代模型。

Abstract: Encoder-only languages models are frequently used for a variety of standard
machine learning tasks, including classification and retrieval. However, there
has been a lack of recent research for encoder models, especially with respect
to multilingual models. We introduce mmBERT, an encoder-only language model
pretrained on 3T tokens of multilingual text in over 1800 languages. To build
mmBERT we introduce several novel elements, including an inverse mask ratio
schedule and an inverse temperature sampling ratio. We add over 1700
low-resource languages to the data mix only during the decay phase, showing
that it boosts performance dramatically and maximizes the gains from the
relatively small amount of training data. Despite only including these
low-resource languages in the short decay phase we achieve similar
classification performance to models like OpenAI's o3 and Google's Gemini 2.5
Pro. Overall, we show that mmBERT significantly outperforms the previous
generation of models on classification and retrieval tasks -- on both high and
low-resource languages.

</details>


### [353] [An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training](https://arxiv.org/abs/2509.05359)
*Yanis Labrak,Richard Dufour,Mickaël Rouvier*

Main category: cs.CL

TL;DR: 本文聚焦语音语言模型（SLMs）中离散单元表示，研究持续预训练时语音建模优化，探讨多种因素对预训练的影响、离散化策略与模型容量关系等。


<details>
  <summary>Details</summary>
Motivation: 优化语音语言模型在持续预训练阶段的语音建模。

Method: 系统研究模型架构、数据表示和训练鲁棒性对预训练阶段的影响，实验分析语音编码器和聚类粒度在不同模型规模下的作用，研究聚类数据选择对模型鲁棒性的影响。

Result: 发现不同模型规模下最优离散化策略不同，揭示语言和副语言模式，明确离散化训练与目标应用领域匹配对模型鲁棒性的重要性。

Conclusion: 明确了多种因素对语音语言模型预训练的影响，强调了离散化策略与模型容量适配以及领域匹配的重要性。

Abstract: This paper investigates discrete unit representations in Speech Language
Models (SLMs), focusing on optimizing speech modeling during continual
pre-training. In this paper, we systematically examine how model architecture,
data representation, and training robustness influence the pre-training stage
in which we adapt existing pre-trained language models to the speech modality.
Our experiments highlight the role of speech encoders and clustering
granularity across different model scales, showing how optimal discretization
strategies vary with model capacity. By examining cluster distribution and
phonemic alignments, we investigate the effective use of discrete vocabulary,
uncovering both linguistic and paralinguistic patterns. Additionally, we
explore the impact of clustering data selection on model robustness,
highlighting the importance of domain matching between discretization training
and target applications.

</details>


### [354] [A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs](https://arxiv.org/abs/2509.05385)
*Jiacheng Wei,Faguo Wu,Xiao Zhang*

Main category: cs.CL

TL;DR: 提出SAGE框架解决大语言模型推理时无法自适应学习问题，评估显示其有出色表现。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在推理时无法持续适应和从新数据学习的局限性。

Method: 将复杂推理任务分解为原子子任务，引入SAGE框架，包含实时检测推理失败的Trigger模块、聚类异常样本的Trigger Buffer模块和动态优化参数更新的Lora Store模块。

Result: SAGE在原子推理子任务上通过测试时动态知识更新展现出出色的准确性、鲁棒性和稳定性。

Conclusion: SAGE框架能有效解决大语言模型推理时的自适应学习问题。

Abstract: Large language models are unable to continuously adapt and learn from new
data during reasoning at inference time. To address this limitation, we propose
that complex reasoning tasks be decomposed into atomic subtasks and introduce
SAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive
updates during reasoning at inference time. SAGE consists of three key
components: (1) a Trigger module that detects reasoning failures through
multiple evaluation metrics in real time; (2) a Trigger Buffer module that
clusters anomaly samples using a streaming clustering process with HDBSCAN,
followed by stability checks and similarity-based merging; and (3) a Lora Store
module that dynamically optimizes parameter updates with an adapter pool for
knowledge retention. Evaluation results show that SAGE demonstrates excellent
accuracy, robustness, and stability on the atomic reasoning subtask through
dynamic knowledge updating during test time.

</details>


### [355] [Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate](https://arxiv.org/abs/2509.05396)
*Andrea Wynn,Harsh Satija,Gillian Hadfield*

Main category: cs.CL

TL;DR: 研究发现多智能体辩论有时有害，探讨模型能力多样性对辩论的影响，实验表明辩论会降低准确率。


<details>
  <summary>Details</summary>
Motivation: 先前研究聚焦同质智能体辩论，本文想探究模型能力多样性对多智能体交互动态和结果的影响。

Method: 进行一系列实验。

Result: 辩论会随时间降低准确率，模型常因同伴推理从正确答案转向错误答案。

Conclusion: 多智能体辩论存在重要失败模式，盲目应用辩论可能导致性能下降。

Abstract: While multi-agent debate has been proposed as a promising strategy for
improving AI reasoning ability, we find that debate can sometimes be harmful
rather than helpful. The prior work has exclusively focused on debates within
homogeneous groups of agents, whereas we explore how diversity in model
capabilities influences the dynamics and outcomes of multi-agent interactions.
Through a series of experiments, we demonstrate that debate can lead to a
decrease in accuracy over time -- even in settings where stronger (i.e., more
capable) models outnumber their weaker counterparts. Our analysis reveals that
models frequently shift from correct to incorrect answers in response to peer
reasoning, favoring agreement over challenging flawed reasoning. These results
highlight important failure modes in the exchange of reasons during multi-agent
debate, suggesting that naive applications of debate may cause performance
degradation when agents are neither incentivized nor adequately equipped to
resist persuasive but incorrect reasoning.

</details>


### [356] [No Translation Needed: Forecasting Quality from Fertility and Metadata](https://arxiv.org/abs/2509.05425)
*Jessica M. Lundin,Ada Zhang,David Adelani,Cody Carroll*

Main category: cs.CL

TL;DR: 无需运行翻译系统，仅用少量特征就能准确预测翻译质量，为多语言评估和质量估计提供新见解。


<details>
  <summary>Details</summary>
Motivation: 探索不运行翻译系统来准确预测翻译质量的方法。

Method: 使用token fertility ratios、token counts和基本语言元数据，通过梯度提升模型预测FLORES - 200基准中GPT - 4o翻译的ChrF分数。

Result: 梯度提升模型表现良好，XX→English的$R^{2}=0.66$，English→XX的$R^{2}=0.72$；类型因素对译入英语预测影响大，fertility对译入不同目标语言影响大。

Conclusion: 翻译质量受token级fertility和更广泛语言类型学影响，为多语言评估和质量估计提供新见解。

Abstract: We show that translation quality can be predicted with surprising accuracy
\textit{without ever running the translation system itself}. Using only a
handful of features, token fertility ratios, token counts, and basic linguistic
metadata (language family, script, and region), we can forecast ChrF scores for
GPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient
boosting models achieve favorable performance ($R^{2}=0.66$ for
XX$\rightarrow$English and $R^{2}=0.72$ for English$\rightarrow$XX). Feature
importance analyses reveal that typological factors dominate predictions into
English, while fertility plays a larger role for translations into diverse
target languages. These findings suggest that translation quality is shaped by
both token-level fertility and broader linguistic typology, offering new
insights for multilingual evaluation and quality estimation.

</details>


### [357] [Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too](https://arxiv.org/abs/2509.05440)
*Logan Lawrence,Ashton Williamson,Alexander Shelton*

Main category: cs.CL

TL;DR: 提出一种直接评分方法，用合成摘要在测试时作为成对机器排名，在多个基准上表现与现有方法相当并发布合成摘要数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于成对比较的方法在样本级性能评估中缺乏为单个摘要分配绝对分数的能力，而这在需要阈值设定的用例中很关键。

Method: 提出一种直接评分方法，使用合成摘要在测试时作为成对机器排名。

Result: 该方法在SummEval、TopicalChat和HANNA元评估基准上的轴平均样本级相关性表现与现有最先进的成对评估器相当，分别为+0.03、-0.03和+0.05。

Conclusion: 提出的直接评分方法有一定有效性，且发布的合成摘要数据可促进未来相关研究。

Abstract: As large-language models have been increasingly used as automatic raters for
evaluating free-form content, including document summarization, dialog, and
story generation, work has been dedicated to evaluating such models by
measuring their correlations with human judgment. For \textit{sample-level}
performance, methods which operate by using pairwise comparisons between
machine-generated text perform well but often lack the ability to assign
absolute scores to individual summaries, an ability crucial for use cases that
require thresholding. In this work, we propose a direct-scoring method which
uses synthetic summaries to act as pairwise machine rankings at test time. We
show that our method performs comparably to state-of-the-art pairwise
evaluators in terms of axis-averaged sample-level correlations on the SummEval
(\textbf{+0.03}), TopicalChat (\textbf{-0.03}), and HANNA (\textbf{+0.05})
meta-evaluation benchmarks, and release the synthetic in-context summaries as
data to facilitate future work.

</details>


### [358] [The Token Tax: Systematic Bias in Multilingual Tokenization](https://arxiv.org/abs/2509.05486)
*Jessica M. Lundin,Ada Zhang,Nihal Karim,Hamza Louzan,Victor Wei,David Adelani,Cody Carroll*

Main category: cs.CL

TL;DR: 评估10个大语言模型在AfriMMLU上表现，发现标记生育率影响准确率，推理模型表现更好，标记膨胀增加成本，需形态感知分词等举措。


<details>
  <summary>Details</summary>
Motivation: 解决形态复杂、低资源语言分词效率低带来的计算资源浪费和准确率低的问题。

Method: 在AfriMMLU数据集上评估10个大语言模型。

Result: 标记生育率可靠预测准确率，推理模型表现更好，标记翻倍使训练成本和时间增至四倍。

Conclusion: 需要形态感知分词、公平定价和多语言基准来实现公平的自然语言处理。

Abstract: Tokenization inefficiency imposes structural disadvantages on morphologically
complex, low-resource languages, inflating compute resources and depressing
accuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA
items; 5 subjects; 16 African languages) and show that fertility (tokens/word)
reliably predicts accuracy. Higher fertility consistently predicts lower
accuracy across all models and subjects. We further find that reasoning models
(DeepSeek, o1) consistently outperform non-reasoning peers across high and low
resource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in
prior generations. Finally, translating token inflation to economics, a
doubling in tokens results in quadrupled training cost and time, underscoring
the token tax faced by many languages. These results motivate morphologically
aware tokenization, fair pricing, and multilingual benchmarks for equitable
natural language processing (NLP).

</details>


### [359] [Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study](https://arxiv.org/abs/2509.05553)
*Serge Lionel Nikiema,Jordan Samhi,Micheline Bénédicte Moumoula,Albérick Euraste Djiré,Abdoul Kader Kaboré,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: 研究探讨大语言模型是否真懂概念，提出双向推理测试，发现认知专业化问题，开发CFT方法实现双向推理。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是真正理解概念还是仅识别模式。

Method: 提出双向推理作为理解测试，测试现有模型，发现问题后开发CFT方法，用三种示例训练模型。

Result: CFT成功实现双向推理，保持正向任务能力同时有强反向表现。

Conclusion: 双向推理可作评估理解的理论框架和开发更强AI系统的实用训练方法。

Abstract: This research addresses a fundamental question in AI: whether large language
models truly understand concepts or simply recognize patterns. The authors
propose bidirectional reasoning,the ability to apply transformations in both
directions without being explicitly trained on the reverse direction, as a test
for genuine understanding. They argue that true comprehension should naturally
allow reversibility. For example, a model that can change a variable name like
userIndex to i should also be able to infer that i represents a user index
without reverse training. The researchers tested current language models and
discovered what they term cognitive specialization: when models are fine-tuned
on forward tasks, their performance on those tasks improves, but their ability
to reason bidirectionally becomes significantly worse. To address this issue,
they developed Contrastive Fine-Tuning (CFT), which trains models using three
types of examples: positive examples that maintain semantic meaning, negative
examples with different semantics, and forward-direction obfuscation examples.
This approach aims to develop deeper understanding rather than surface-level
pattern recognition and allows reverse capabilities to develop naturally
without explicit reverse training. Their experiments demonstrated that CFT
successfully achieved bidirectional reasoning, enabling strong reverse
performance while maintaining forward task capabilities. The authors conclude
that bidirectional reasoning serves both as a theoretical framework for
assessing genuine understanding and as a practical training approach for
developing more capable AI systems.

</details>


### [360] [Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval](https://arxiv.org/abs/2311.01870)
*Jinrui Yang,Timothy Baldwin,Trevor Cohn*

Main category: cs.CL

TL;DR: 介绍多语言基准数据集Multi - EuP，包含22K份来自欧洲议会的多语言文档，覆盖24种语言，可用于研究多语言信息检索公平性及相关实验。


<details>
  <summary>Details</summary>
Motivation: 在多语言信息检索背景下研究公平性，分析排名中的语言和人口统计学偏差。

Method: 创建包含真实多语言语料、跨语言相关性判断和丰富人口统计信息的Multi - EuP数据集，还进行了关于分词策略导致语言偏差的初步实验。

Result: 报告了Multi - EuP在单语言和多语言信息检索基准测试中的有效性。

Conclusion: Multi - EuP数据集可用于多语言信息检索的公平性研究及相关实验。

Abstract: We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K
multi-lingual documents collected from the European Parliament, spanning 24
languages. This dataset is designed to investigate fairness in a multilingual
information retrieval (IR) context to analyze both language and demographic
bias in a ranking context. It boasts an authentic multilingual corpus,
featuring topics translated into all 24 languages, as well as cross-lingual
relevance judgments. Furthermore, it offers rich demographic information
associated with its documents, facilitating the study of demographic bias. We
report the effectiveness of Multi-EuP for benchmarking both monolingual and
multilingual IR. We also conduct a preliminary experiment on language bias
caused by the choice of tokenization strategy.

</details>


### [361] [Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation](https://arxiv.org/abs/2509.05605)
*Qiyuan Chen,Hongsen Huang,Qian Shao,Jiahe Chen,Jintai Chen,Hongxia Xu,Renjie Hua,Ren Chuan,Jian Wu*

Main category: cs.CL

TL;DR: 提出Icon²方法利用大语言模型表示空间内在调节构建偏好数据集，实验表明在对齐和效率上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统构建大语言模型偏好数据集的方法存在分布不匹配和计算开销大的问题。

Method: 提取层方向向量编码人类偏好，用这些向量过滤自合成指令，解码时应用双向内在控制引导生成响应。

Result: Llama3 - 8B和Qwen2 - 7B在AlpacaEval 2.0和Arena - Hard上平均胜率分别提升13.89%和13.45%，计算成本最多降低48.1%。

Conclusion: Icon²方法在大语言模型偏好数据集构建上能有效提升对齐和效率。

Abstract: Large Language Models (LLMs) require high quality preference datasets to
align with human preferences. However, conventional methods for constructing
such datasets face significant challenges: reliance on pre-collected
instructions often leads to distribution mismatches with target models, while
the need for sampling multiple stochastic responses introduces substantial
computational overhead. In this work, we explore a paradigm shift by leveraging
inherent regulation of LLMs' representation space for efficient and tailored
preference dataset construction, named Icon$^{2}$. Specifically, it first
extracts layer-wise direction vectors to encode sophisticated human preferences
and then uses these vectors to filter self-synthesized instructions based on
their inherent consistency. During decoding, bidirectional inherent control is
applied to steer token representations, enabling the precise generation of
response pairs with clear alignment distinctions. Experimental results
demonstrate significant improvements in both alignment and efficiency.
Llama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on
AlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by
up to 48.1%.

</details>


### [362] [From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics](https://arxiv.org/abs/2509.05617)
*Shay Dahary,Avi Edana,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.CL

TL;DR: 本文研究歌词多标签情感归因任务，构建数据集评估大语言模型零样本表现，微调BERT模型，揭示不同模型优劣，为音乐信息检索提供模型选择策略，数据集公开。


<details>
  <summary>Details</summary>
Motivation: 歌词情感内容对听众体验和音乐偏好有重要影响，研究歌词多标签情感归因任务。

Method: 用平均意见得分（MOS）方法构建手动标注数据集，在零样本场景下评估多个公开大语言模型，微调基于BERT的模型预测多标签情感分数。

Result: 实验揭示了零样本和微调模型在捕捉歌词细微情感内容方面的相对优势和局限性。

Conclusion: 大语言模型在创意文本情感识别方面有潜力，为基于情感的音乐信息检索应用提供了模型选择策略。

Abstract: The emotional content of song lyrics plays a pivotal role in shaping listener
experiences and influencing musical preferences. This paper investigates the
task of multi-label emotional attribution of song lyrics by predicting six
emotional intensity scores corresponding to six fundamental emotions. A
manually labeled dataset is constructed using a mean opinion score (MOS)
approach, which aggregates annotations from multiple human raters to ensure
reliable ground-truth labels. Leveraging this dataset, we conduct a
comprehensive evaluation of several publicly available large language models
(LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model
specifically for predicting multi-label emotion scores. Experimental results
reveal the relative strengths and limitations of zero-shot and fine-tuned
models in capturing the nuanced emotional content of lyrics. Our findings
highlight the potential of LLMs for emotion recognition in creative texts,
providing insights into model selection strategies for emotion-based music
information retrieval applications. The labeled dataset is available at
https://github.com/LLM-HITCS25S/LyricsEmotionAttribution.

</details>


### [363] [LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding](https://arxiv.org/abs/2509.05657)
*Yuxuan Hu,Jihao Liu,Ke Wang,Jinliang Zhen,Weikang Shi,Manyuan Zhang,Qi Dou,Rui Liu,Aojun Zhou,Hongsheng Li*

Main category: cs.CL

TL;DR: 提出LM - Searcher框架用于跨领域神经架构优化，取得有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型驱动的神经架构搜索（NAS）方法依赖提示工程和特定领域调优，限制了实用性和可扩展性。

Method: 提出通用数值字符串表示NCode进行跨领域架构编码和搜索；将NAS问题转化为排序任务，用基于剪枝的子空间采样策略生成的数据训练大语言模型选择高性能架构；构建涵盖多种架构 - 性能对的数据集。

Result: LM - Searcher在域内和域外任务中都取得了具有竞争力的性能。

Conclusion: LM - Searcher为灵活且可泛化的基于大语言模型的架构搜索建立了新范式。

Abstract: Recent progress in Large Language Models (LLMs) has opened new avenues for
solving complex optimization problems, including Neural Architecture Search
(NAS). However, existing LLM-driven NAS approaches rely heavily on prompt
engineering and domain-specific tuning, limiting their practicality and
scalability across diverse tasks. In this work, we propose LM-Searcher, a novel
framework that leverages LLMs for cross-domain neural architecture optimization
without the need for extensive domain-specific adaptation. Central to our
approach is NCode, a universal numerical string representation for neural
architectures, which enables cross-domain architecture encoding and search. We
also reformulate the NAS problem as a ranking task, training LLMs to select
high-performing architectures from candidate pools using instruction-tuning
samples derived from a novel pruning-based subspace sampling strategy. Our
curated dataset, encompassing a wide range of architecture-performance pairs,
encourages robust and transferable learning. Comprehensive experiments
demonstrate that LM-Searcher achieves competitive performance in both in-domain
(e.g., CNNs for image classification) and out-of-domain (e.g., LoRA
configurations for segmentation and generation) tasks, establishing a new
paradigm for flexible and generalizable LLM-based architecture search. The
datasets and models will be released at https://github.com/Ashone3/LM-Searcher.

</details>


### [364] [Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian](https://arxiv.org/abs/2509.05668)
*Michael Hoffmann,Jophin John,Stefan Schweter,Gokul Ramakrishnan,Hoi-Fong Mak,Alice Zhang,Dmitry Gaynullin,Nicolay J. Hammer*

Main category: cs.CL

TL;DR: 介绍Llama - GENBA - 10B三语基础模型，它解决大语言模型英语中心偏差问题，开发应对四项挑战，评估显示性能出色，训练展示高效大规模多语预训练。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型英语中心偏差问题，面向德国NLP社区，推广巴伐利亚语这一低资源语言。

Method: 基于Llama 3.1 - 8B扩展到10B参数，在164B tokens上持续预训练；应对四项挑战包括策划多语语料库、创建统一分词器、优化架构和超参数、建立三语评估套件。

Result: Llama - GENBA - 10B跨语言性能强，微调变体在巴伐利亚语上超越Apertus - 8B - 2509和gemma - 2 - 9b，在英语上超越EuroLLM，德语上与EuroLLM相当。

Conclusion: 在Cerebras CS - 2上训练展示高效大规模多语预训练，为整合低资源语言的包容性基础模型提供蓝图。

Abstract: We present Llama-GENBA-10B, a trilingual foundation model addressing
English-centric bias in large language models. Built on Llama 3.1-8B and scaled
to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens
(82B English, 82B German, and 80M Bavarian), balancing resources while
preventing English dominance. Targeted at the German NLP community, the model
also promotes Bavarian as a low-resource language. Development tackled four
challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)
creating a unified tokenizer for English, German, and Bavarian, (3) optimizing
architecture and language-ratio hyperparameters for cross-lingual transfer, and
(4) establishing the first standardized trilingual evaluation suite by
translating German benchmarks into Bavarian. Evaluations show that
Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned
variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing
itself as the best model in its class for this language, while also
outperforming EuroLLM in English and matching its results in German. Training
on the Cerebras CS-2 demonstrated efficient large-scale multilingual
pretraining with documented energy use, offering a blueprint for inclusive
foundation models that integrate low-resource languages.

</details>


### [365] [Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection](https://arxiv.org/abs/2509.05360)
*Jerry Li,Evangelos Papalexakis*

Main category: cs.CL

TL;DR: 论文针对大语言模型幻觉问题，提出受ROUGE启发的新方法，构建N - Gram频率张量，经张量分解后训练MLP二分类器，在HaluEval数据集上效果良好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉问题影响信息真实性和可信度，现有检测方法基于的基础指标缺乏语义深度，难以有效检测幻觉。

Method: 构建N - Gram频率张量，用张量分解提取奇异值，以此为特征训练MLP二分类器。

Result: 在HaluEval数据集上评估，相比传统基线有显著提升，与最先进的LLM裁判方法表现相当。

Conclusion: 所提出的方法能有效检测大语言模型的幻觉，在检测性能上有优势。

Abstract: Large Language Models (LLMs) have demonstrated effectiveness across a wide
variety of tasks involving natural language, however, a fundamental problem of
hallucinations still plagues these models, limiting their trustworthiness in
generating consistent, truthful information. Detecting hallucinations has
quickly become an important topic, with various methods such as uncertainty
estimation, LLM Judges, retrieval augmented generation (RAG), and consistency
checks showing promise. Many of these methods build upon foundational metrics,
such as ROUGE, BERTScore, or Perplexity, which often lack the semantic depth
necessary to detect hallucinations effectively. In this work, we propose a
novel approach inspired by ROUGE that constructs an N-Gram frequency tensor
from LLM-generated text. This tensor captures richer semantic structure by
encoding co-occurrence patterns, enabling better differentiation between
factual and hallucinated content. We demonstrate this by applying tensor
decomposition methods to extract singular values from each mode and use these
as input features to train a multi-layer perceptron (MLP) binary classifier for
hallucinations. Our method is evaluated on the HaluEval dataset and
demonstrates significant improvements over traditional baselines, as well as
competitive performance against state-of-the-art LLM judges.

</details>


### [366] [Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models](https://arxiv.org/abs/2509.05691)
*Ningyuan Deng,Hanyu Duan,Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 研究评估13种文本嵌入模型对数值信息的编码能力，发现它们难以准确捕捉数值细节。


<details>
  <summary>Details</summary>
Motivation: 当前文本嵌入模型基准测试未关注对文本中细微数值信息的理解，而这些模型在金融和医疗等数值重要的领域应用增多，需研究其能否精确编码数值内容。

Method: 在金融背景下使用合成数据评估13种广泛使用的文本嵌入模型。

Result: 这些模型总体上难以准确捕捉数值细节。

Conclusion: 研究为嵌入数字能力提供深入见解，为未来增强基于嵌入模型的NLP系统处理数值内容的能力提供参考。

Abstract: Text embedding models are widely used in natural language processing
applications. However, their capability is often benchmarked on tasks that do
not require understanding nuanced numerical information in text. As a result,
it remains unclear whether current embedding models can precisely encode
numerical content, such as numbers, into embeddings. This question is critical
because embedding models are increasingly applied in domains where numbers
matter, such as finance and healthcare. For example, Company X's market share
grew by 2\% should be interpreted very differently from Company X's market
share grew by 20\%, even though both indicate growth in market share. This
study aims to examine whether text embedding models can capture such nuances.
Using synthetic data in a financial context, we evaluate 13 widely used text
embedding models and find that they generally struggle to capture numerical
details accurately. Our further analyses provide deeper insights into embedding
numeracy, informing future research to strengthen embedding model-based NLP
systems with improved capacity for handling numerical content.

</details>


### [367] [A Survey of the State-of-the-Art in Conversational Question Answering Systems](https://arxiv.org/abs/2509.05716)
*Manoj Madushanka Perera,Adnan Mahmood,Kasun Eranda Wijethilake,Fahmida Islam,Maryam Tahermazandarani,Quan Z. Sheng*

Main category: cs.CL

TL;DR: 本文对对话式问答（ConvQA）系统进行全面综述，分析核心组件、先进机器学习技术、大语言模型作用，介绍关键数据集并指出研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着ConvQA系统在多领域应用增加，需全面分析该领域最新进展。

Method: 考察ConvQA系统核心组件，研究先进机器学习技术应用，探讨大语言模型作用，分析关键数据集。

Result: 展示了先进技术和大语言模型对ConvQA准确性和效率的提升，分析了关键数据集。

Conclusion: 提供了ConvQA领域的全面概述，为未来研究提供有价值的见解和方向。

Abstract: Conversational Question Answering (ConvQA) systems have emerged as a pivotal
area within Natural Language Processing (NLP) by driving advancements that
enable machines to engage in dynamic and context-aware conversations. These
capabilities are increasingly being applied across various domains, i.e.,
customer support, education, legal, and healthcare where maintaining a coherent
and relevant conversation is essential. Building on recent advancements, this
survey provides a comprehensive analysis of the state-of-the-art in ConvQA.
This survey begins by examining the core components of ConvQA systems, i.e.,
history selection, question understanding, and answer prediction, highlighting
their interplay in ensuring coherence and relevance in multi-turn
conversations. It further investigates the use of advanced machine learning
techniques, including but not limited to, reinforcement learning, contrastive
learning, and transfer learning to improve ConvQA accuracy and efficiency. The
pivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash,
Mistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact
through data scalability and architectural advancements. Additionally, this
survey presents a comprehensive analysis of key ConvQA datasets and concludes
by outlining open research directions. Overall, this work offers a
comprehensive overview of the ConvQA landscape and provides valuable insights
to guide future advancements in the field.

</details>


### [368] [Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2509.05505)
*Mansi Garg,Lee-Chi Wang,Bhavesh Ghanchi,Sanjana Dumpala,Shreyash Kakde,Yen Chih Chen*

Main category: cs.CL

TL;DR: 提出基于RAG架构的生物医学文献问答系统，能提升获取准确医学信息的能力，在乳腺癌文献评估中表现好。


<details>
  <summary>Details</summary>
Motivation: 解决传统健康搜索引擎的不足和公众获取生物医学研究滞后的问题。

Method: 系统集成多源信息，检索用MiniLM语义嵌入和FAISS向量搜索，答案生成用微调的Mistral - 7B - v0.3语言模型，并用QLoRA优化训练。

Result: 用BERTScore (F1)衡量，相比基线模型，在事实一致性和语义相关性上有显著提升。

Conclusion: RAG增强的语言模型有潜力弥合生物医学文献与公众健康知识的差距，为后续研究指明方向。

Abstract: This work presents a Biomedical Literature Question Answering (Q&A) system
based on a Retrieval-Augmented Generation (RAG) architecture, designed to
improve access to accurate, evidence-based medical information. Addressing the
shortcomings of conventional health search engines and the lag in public access
to biomedical research, the system integrates diverse sources, including PubMed
articles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant
information and generate concise, context-aware responses. The retrieval
pipeline uses MiniLM-based semantic embeddings and FAISS vector search, while
answer generation is performed by a fine-tuned Mistral-7B-v0.3 language model
optimized using QLoRA for efficient, low-resource training. The system supports
both general medical queries and domain-specific tasks, with a focused
evaluation on breast cancer literature demonstrating the value of
domain-aligned retrieval. Empirical results, measured using BERTScore (F1),
show substantial improvements in factual consistency and semantic relevance
compared to baseline models. The findings underscore the potential of
RAG-enhanced language models to bridge the gap between complex biomedical
literature and accessible public health knowledge, paving the way for future
work on multilingual adaptation, privacy-preserving inference, and personalized
medical AI systems.

</details>


### [369] [New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR](https://arxiv.org/abs/2509.05609)
*Xugang Lu,Peng Shen,Yu Tsao,Hisashi Kawai*

Main category: cs.CL

TL;DR: 本文将语音识别中声学和语言表征对齐视为检测问题，提出基于非平衡最优传输的对齐模型，实验证明其能提升语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 声学和语言表征对齐存在结构不对称、匹配条件不平衡等问题，需要更好的方法进行知识迁移以提升自动语音识别性能。

Method: 将对齐和匹配视为检测问题，提出基于非平衡最优传输的对齐模型，处理分布不匹配和结构不对称。

Result: 在基于CTC的语音识别系统结合预训练语言模型进行知识迁移的实验中，所提方法能灵活控制匹配程度。

Conclusion: 所提方法有效，可提升自动语音识别性能。

Abstract: Aligning acoustic and linguistic representations is a central challenge to
bridge the pre-trained models in knowledge transfer for automatic speech
recognition (ASR). This alignment is inherently structured and asymmetric:
while multiple consecutive acoustic frames typically correspond to a single
linguistic token (many-to-one), certain acoustic transition regions may relate
to multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often
include frames with no linguistic counterpart, such as background noise or
silence may lead to imbalanced matching conditions. In this work, we take a new
insight to regard alignment and matching as a detection problem, where the goal
is to identify meaningful correspondences with high precision and recall
ensuring full coverage of linguistic tokens while flexibly handling redundant
or noisy acoustic frames in transferring linguistic knowledge for ASR. Based on
this new insight, we propose an unbalanced optimal transport-based alignment
model that explicitly handles distributional mismatch and structural
asymmetries with soft and partial matching between acoustic and linguistic
modalities. Our method ensures that every linguistic token is grounded in at
least one acoustic observation, while allowing for flexible, probabilistic
mappings from acoustic to linguistic units. We evaluate our proposed model with
experiments on an CTC-based ASR system with a pre-trained language model for
knowledge transfer. Experimental results demonstrate the effectiveness of our
approach in flexibly controlling degree of matching and hence to improve ASR
performance.

</details>


### [370] [ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula](https://arxiv.org/abs/2509.05867)
*ZiXuan Zhang,Bowen Hao,Yingjie Li,Hongzhi Yin*

Main category: cs.CL

TL;DR: 提出结合GraphRAG与LLM微调的ZhiFangDanTai框架处理中医配方问题，实验效果佳且开源。


<details>
  <summary>Details</summary>
Motivation: 现有中医配方分析模型结果不全面，指令数据集细节不足，限制模型输出深度。

Method: 提出ZhiFangDanTai框架，用GraphRAG检索和合成知识，构建增强指令数据集，给出理论证明。

Result: 在收集和临床数据集上实验，ZhiFangDanTai比现有模型有显著改进。

Conclusion: ZhiFangDanTai能有效解决中医配方分析问题，减少泛化误差和幻觉率。

Abstract: Traditional Chinese Medicine (TCM) formulas play a significant role in
treating epidemics and complex diseases. Existing models for TCM utilize
traditional algorithms or deep learning techniques to analyze formula
relationships, yet lack comprehensive results, such as complete formula
compositions and detailed explanations. Although recent efforts have used TCM
instruction datasets to fine-tune Large Language Models (LLMs) for explainable
formula generation, existing datasets lack sufficient details, such as the
roles of the formula's sovereign, minister, assistant, courier; efficacy;
contraindications; tongue and pulse diagnosis-limiting the depth of model
outputs. To address these challenges, we propose ZhiFangDanTai, a framework
combining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM
fine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured
TCM knowledge into concise summaries, while also constructing an enhanced
instruction dataset to improve LLMs' ability to integrate retrieved
information. Furthermore, we provide novel theoretical proofs demonstrating
that integrating GraphRAG with fine-tuning techniques can reduce generalization
error and hallucination rates in the TCM formula task. Experimental results on
both collected and clinical datasets demonstrate that ZhiFangDanTai achieves
significant improvements over state-of-the-art models. Our model is
open-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.

</details>


### [371] [Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues](https://arxiv.org/abs/2509.05882)
*Abhijnan Nath,Carine Graff,Nikhil Krishnaswamy*

Main category: cs.CL

TL;DR: 研究不同对齐方法对大语言模型在多轮多方协作中作为合作伙伴有效性的影响，提出反事实评估框架，摩擦感知方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为人类协作者需确保多轮交互行为可预测、可验证，而常见对齐技术未考虑多方长时交互动态。

Method: 通过摩擦代理干预小组对话，采用角色扮演方法评估不同训练的摩擦代理干预，提出反事实评估框架。

Result: 摩擦感知方法在促进达成共识和任务结果正确性上显著优于常见对齐基线。

Conclusion: 摩擦感知方法在多轮多方协作的大语言模型对齐中效果更好。

Abstract: As Large Language Models (LLMs) integrate into diverse workflows, they are
increasingly being considered "collaborators" with humans. If such AI
collaborators are to be reliable, their behavior over multiturn interactions
must be predictable, validated and verified before deployment. Common alignment
techniques are typically developed under simplified single-user settings and do
not account for the dynamics of long-horizon multiparty interactions. This
paper examines how different alignment methods affect LLM agents' effectiveness
as partners in multiturn, multiparty collaborations. We study this question
through the lens of friction agents that intervene in group dialogues to
encourage the collaborative group to slow down and reflect upon their reasoning
for deliberative decision-making. Using a roleplay methodology, we evaluate
interventions from differently-trained friction agents in collaborative task
conversations. We propose a novel counterfactual evaluation framework that
quantifies how friction interventions change the trajectory of group
collaboration and belief alignment. Our results show that a friction-aware
approach significantly outperforms common alignment baselines in helping both
convergence to a common ground, or agreed-upon task-relevant propositions, and
correctness of task outcomes.

</details>


### [372] [Benchmarking Gender and Political Bias in Large Language Models](https://arxiv.org/abs/2509.06164)
*Jinrui Yang,Xudong Han,Timothy Baldwin*

Main category: cs.CL

TL;DR: 提出EuroParlVote基准评估大语言模型在政治敏感场景下表现，发现模型存在偏见，专有模型表现更优并公布相关资源。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在政治敏感场景下的表现，研究其在政治语境中的公平性和责任性。

Method: 使用EuroParlVote基准对大语言模型进行性别分类和投票预测两项任务评估。

Result: 大语言模型存在偏见，常误将女性议员分类为男性，对女性发言者投票模拟准确率低，政治上倾向中间派，专有模型在鲁棒性和公平性上优于开源模型。

Conclusion: 发布EuroParlVote数据集、代码和演示，以支持未来NLP在政治语境中公平性和责任性的研究。

Abstract: We introduce EuroParlVote, a novel benchmark for evaluating large language
models (LLMs) in politically sensitive contexts. It links European Parliament
debate speeches to roll-call vote outcomes and includes rich demographic
metadata for each Member of the European Parliament (MEP), such as gender, age,
country, and political group. Using EuroParlVote, we evaluate state-of-the-art
LLMs on two tasks -- gender classification and vote prediction -- revealing
consistent patterns of bias. We find that LLMs frequently misclassify female
MEPs as male and demonstrate reduced accuracy when simulating votes for female
speakers. Politically, LLMs tend to favor centrist groups while underperforming
on both far-left and far-right ones. Proprietary models like GPT-4o outperform
open-weight alternatives in terms of both robustness and fairness. We release
the EuroParlVote dataset, code, and demo to support future research on fairness
and accountability in NLP within political contexts.

</details>


### [373] [Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?](https://arxiv.org/abs/2509.06350)
*Junjie Mu,Zonghao Ying,Zhekui Fan,Zonglei Jing,Yaoyuan Zhang,Zhengmin Yu,Wenxin Zhang,Quanchen Zou,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: 提出Mask - GCG方法，通过可学习的标记掩码识别后缀中有影响的标记，减少冗余和计算开销，评估显示可揭示大语言模型提示中的标记冗余。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型越狱攻击的GCG及其改进变体使用固定长度后缀，其中的潜在冗余未被探索，需提高攻击效率。

Method: 提出Mask - GCG方法，采用可学习的标记掩码，增加高影响位置标记的更新概率，修剪低影响位置标记。

Result: 实验表明后缀中多数标记对攻击成功有重要贡献，修剪少数低影响标记不影响损失值和攻击成功率，揭示了标记冗余。

Conclusion: 研究成果从越狱攻击角度为开发高效、可解释的大语言模型提供了见解。

Abstract: Jailbreak attacks on Large Language Models (LLMs) have demonstrated various
successful methods whereby attackers manipulate models into generating harmful
responses that they are designed to avoid. Among these, Greedy Coordinate
Gradient (GCG) has emerged as a general and effective approach that optimizes
the tokens in a suffix to generate jailbreakable prompts. While several
improved variants of GCG have been proposed, they all rely on fixed-length
suffixes. However, the potential redundancy within these suffixes remains
unexplored. In this work, we propose Mask-GCG, a plug-and-play method that
employs learnable token masking to identify impactful tokens within the suffix.
Our approach increases the update probability for tokens at high-impact
positions while pruning those at low-impact positions. This pruning not only
reduces redundancy but also decreases the size of the gradient space, thereby
lowering computational overhead and shortening the time required to achieve
successful attacks compared to GCG. We evaluate Mask-GCG by applying it to the
original GCG and several improved variants. Experimental results show that most
tokens in the suffix contribute significantly to attack success, and pruning a
minority of low-impact tokens does not affect the loss values or compromise the
attack success rate (ASR), thereby revealing token redundancy in LLM prompts.
Our findings provide insights for developing efficient and interpretable LLMs
from the perspective of jailbreak attacks.

</details>


### [374] [PL-CA: A Parametric Legal Case Augmentation Framework](https://arxiv.org/abs/2509.06356)
*Ao Chang,Yubo Chen,Jun Zhao*

Main category: cs.CL

TL;DR: 本文指出传统RAG方法存在上下文窗口限制和计算开销大等问题，现有基准测试不足，提出PL - CA方法，构建多任务法律数据集，实验表明该方法可减少长上下文开销且下游任务表现有竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法因直接注入文档导致模型受上下文窗口限制、计算开销大，现有基准测试无法反映模型真实能力，需要改进。

Method: 提出PL - CA，引入P - RAG框架对语料知识进行数据增强并编码为参数向量，通过LoRA将参数知识集成到LLM的FFN中；构建含2000多个实例且经专家标注和人工验证的多任务法律数据集。

Result: 实验表明该方法减少了长上下文相关的开销，在下游任务上表现不逊色于传统RAG。

Conclusion: PL - CA方法有效减轻了模型上下文压力，在下游任务中有较好表现，代码和数据集在附录提供。

Abstract: Conventional RAG is considered one of the most effective methods for
addressing model knowledge insufficiency and hallucination, particularly in the
judicial domain that requires high levels of knowledge rigor, logical
consistency, and content integrity. However, the conventional RAG method only
injects retrieved documents directly into the model's context, which severely
constrains models due to their limited context windows and introduces
additional computational overhead through excessively long contexts, thereby
disrupting models' attention and degrading performance on downstream tasks.
Moreover, many existing benchmarks lack expert annotation and focus solely on
individual downstream tasks while real-world legal scenarios consist of
multiple mixed legal tasks, indicating conventional benchmarks' inadequacy for
reflecting models' true capabilities. To address these limitations, we propose
PL-CA, which introduces a parametric RAG (P-RAG) framework to perform data
augmentation on corpus knowledge and encode this legal knowledge into
parametric vectors, and then integrates this parametric knowledge into the
LLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context
pressure. Additionally, we also construct a multi-task legal dataset comprising
more than 2000 training and test instances, which are all expert-annotated and
manually verified. We conduct our experiments on our dataset, and the
experimental results demonstrate that our method reduces the overhead
associated with excessively long contexts while maintaining competitive
performance on downstream tasks compared to conventional RAG. Our code and
dataset are provided in the appendix.

</details>


### [375] [Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training](https://arxiv.org/abs/2509.06518)
*Andrei Baroian,Kasper Notebomer*

Main category: cs.CL

TL;DR: 提出三种新的Layer - Wise Scaling变体，在固定参数预算下表现优于各向同性基线，后续需扩大实验规模。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的语言模型使用统一层大小，忽略不同深度的功能和计算能力需求。

Method: 在预训练阶段通过两点或三点线性插值重新分配FFN宽度和注意力头，提出Framed、Reverse和Crown三种LWS变体，并进行系统消融实验。

Result: 所有模型收敛到相似损失，在同等成本下比各向同性基线性能更好，且训练吞吐量无显著下降。

Conclusion: 这是预训练层架构设计空间的初步探索，未来需用更多的标记和参数进行实验以充分评估其潜力。

Abstract: Transformer-based language models traditionally use uniform (isotropic) layer
sizes, yet they ignore the diverse functional roles that different depths can
play and their computational capacity needs. Building on Layer-Wise Scaling
(LWS) and pruning literature, we introduce three new LWS variants - Framed,
Reverse, and Crown - that redistribute FFN widths and attention heads via two
or three-point linear interpolation in the pre-training stage. We present the
first systematic ablation of LWS and its variants, on a fixed budget of 180M
parameters, trained on 5B tokens. All models converge to similar losses and
achieve better performance compared to an equal-cost isotropic baseline,
without a substantial decrease in training throughput. This work represents an
initial step into the design space of layer-wise architectures for
pre-training, but future work should scale experiments to orders of magnitude
more tokens and parameters to fully assess their potential.

</details>


### [376] [SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion](https://arxiv.org/abs/2509.06531)
*Mengxue Yang,Chun Yang,Jiaqi Zhu,Jiafan Li,Jingqi Zhang,Yuyang Li,Ying Li*

Main category: cs.CL

TL;DR: 提出SLiNT框架解决知识图谱链接预测问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识图谱链接预测中对结构信号利用有限，存在结构稀疏和语义模糊问题，尤其是在不完整或零样本设置下。

Method: 提出SLiNT框架，包含SGNE丰富稀疏实体、DHCL引入细粒度监督、GDDI进行令牌级结构感知干预。

Result: 在WN18RR和FB15k - 237上实验，SLiNT与基于嵌入和基于生成的基线相比，取得了优越或有竞争力的性能。

Conclusion: 结构感知表示学习对可扩展的知识图谱补全有效。

Abstract: Link prediction in knowledge graphs requires integrating structural
information and semantic context to infer missing entities. While large
language models offer strong generative reasoning capabilities, their limited
exploitation of structural signals often results in structural sparsity and
semantic ambiguity, especially under incomplete or zero-shot settings. To
address these challenges, we propose SLiNT (Structure-aware Language model with
Injection and coNtrastive Training), a modular framework that injects
knowledge-graph-derived structural context into a frozen LLM backbone with
lightweight LoRA-based adaptation for robust link prediction. Specifically,
Structure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to
enrich sparse entities and mitigate missing context; Dynamic Hard Contrastive
Learning (DHCL) introduces fine-grained supervision by interpolating hard
positives and negatives to resolve entity-level ambiguity; and
Gradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware
intervention while preserving the core LLM parameters. Experiments on WN18RR
and FB15k-237 show that SLiNT achieves superior or competitive performance
compared with both embedding-based and generation-based baselines,
demonstrating the effectiveness of structure-aware representation learning for
scalable knowledge graph completion.

</details>


### [377] [COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens](https://arxiv.org/abs/2509.06836)
*Eugene Kwek,Wenpeng Yin*

Main category: cs.CL

TL;DR: 提出COMPACT方法优化大语言模型剪枝，实验显示其效果佳。


<details>
  <summary>Details</summary>
Motivation: 使大语言模型在内存、延迟和服务成本方面更高效，现有剪枝方法有局限。

Method: 提出COMPACT方法，联合剪枝稀有词汇和FFN中间通道。

Result: 在Qwen、LLaMA和Gemma等模型实验中，以相似或更高剪枝率实现了最先进下游任务性能，减少参数、GPU内存和端到端延迟。

Conclusion: COMPACT兼具深度和宽度剪枝优点，在大语言模型剪枝上表现出色。

Abstract: Making LLMs more efficient in memory, latency, and serving cost is crucial
for edge deployment, interactive applications, and sustainable inference at
scale. Pruning is a key technique toward this goal. However, prior pruning
methods are limited: width pruning often breaks the standard transformer layout
or requires custom inference code, while depth pruning removes entire layers
and can cause abrupt accuracy drops. In this work, we propose COMPACT, which
jointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii)
prunes FFN intermediate channels using common-token-weighted activations,
aligning importance with the post-pruning token distribution. COMPACT enjoys
merits of both depth and width pruning, such as: deployment-friendliness (keeps
a standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN
pruning), training-free operation with competitive pruning time, and strong
memory savings alongside throughput gains. Experiments across Qwen, LLaMA, and
Gemma families (0.5B-70B) show state-of-the-art downstream task performance at
similar or higher pruning ratios, with substantial reductions in parameters,
GPU memory, and end-to-end latency.

</details>


### [378] [HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models](https://arxiv.org/abs/2509.06596)
*Xin Tong,Zhi Lin,Jingya Wang,Bo Jin*

Main category: cs.CL

TL;DR: 提出无参数解码框架HAVE解决大语言模型幻觉问题，实验证明其能减少幻觉且性能优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在检索增强或长上下文生成中常产生幻觉，源于头重要性处理和注意力权重问题。

Method: 提出HAVE框架，包含头自适应门控和值校准模块，通过轻量级策略融合证据与语言模型分布，无需微调，单次前向传播。

Result: 在多个问答基准和大语言模型族实验中，HAVE持续减少幻觉，优于强基线，开销适中。

Conclusion: HAVE框架透明、可复现，能与现成大语言模型集成，推动现实场景可信生成。

Abstract: Large Language Models (LLMs) often produce hallucinations in
retrieval-augmented or long-context generation, even when relevant evidence is
present. This stems from two issues: head importance is treated as
input-agnostic, and raw attention weights poorly reflect each token's true
contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a
parameter-free decoding framework that directly addresses both challenges. HAVE
introduces head-adaptive gating, which performs instance-level soft reweighing
of attention heads, and value calibration, which augments attention with the
magnitude of value vectors to approximate write-back contribution. Together,
these modules construct token-level evidence aligned with model updates and
fuse it with the LM distribution through a lightweight uncertainty-scaled
policy. HAVE requires no finetuning and operates in a single forward pass,
making it efficient and broadly applicable. Experiments across multiple QA
benchmarks and LLM families demonstrate that HAVE consistently reduces
hallucinations and outperforms strong baselines, including DAGCD, with modest
overhead. The framework is transparent, reproducible, and readily integrates
with off-the-shelf LLMs, advancing trustworthy generation in real-world
settings.

</details>


### [379] [MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML](https://arxiv.org/abs/2509.06806)
*Haoyu Dong,Pengkun Zhang,Mingzhe Lu,Yanzhen Shen,Guolin Ke*

Main category: cs.CL

TL;DR: 介绍MachineLearningLM框架，让大模型具备上下文机器学习能力，表现优且保留通用聊天能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在标准机器学习任务中难以利用上下文学习从多示例中学习，需提升其上下文机器学习能力。

Method: 从数百万结构因果模型合成机器学习任务，用随机森林教师模型蒸馏策略到LLM，用高效提示序列化任务。

Result: MachineLearningLM在分布外表格分类任务上平均优于强基线约15%，呈现多示例缩放定律，达随机森林级准确率，保留通用聊天能力。

Conclusion: MachineLearningLM框架有效赋予大语言模型上下文机器学习能力，同时保留通用知识和推理能力。

Abstract: Large language models (LLMs) possess broad world knowledge and strong
general-purpose reasoning ability, yet they struggle to learn from many
in-context examples on standard machine learning (ML) tasks, that is, to
leverage many-shot demonstrations purely via in-context learning (ICL) without
gradient descent. We introduce MachineLearningLM, a portable
continued-pretraining framework that equips a general-purpose LLM with robust
in-context ML capability while preserving its general knowledge and reasoning
for broader chat workflows.
  Our pretraining procedure synthesizes ML tasks from millions of structural
causal models (SCMs), spanning shot counts up to 1,024. We begin with a
random-forest teacher, distilling tree-based decision strategies into the LLM
to strengthen robustness in numerical modeling. All tasks are serialized with a
token-efficient prompt, enabling 3x to 6x more examples per context window and
delivering up to 50x amortized throughput via batch inference.
  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),
MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an
average of about 15% on out-of-distribution tabular classification across
finance, physics, biology, and healthcare domains. It exhibits a striking
many-shot scaling law: accuracy increases monotonically as in-context
demonstrations grow from 8 to 1,024. Without any task-specific training, it
attains random-forest-level accuracy across hundreds of shots. General chat
capabilities, including knowledge and reasoning, are preserved: it achieves
75.4% on MMLU.

</details>


### [380] [Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem](https://arxiv.org/abs/2509.06809)
*Valentin Quesnel,Damien Sileo*

Main category: cs.CL

TL;DR: 本文针对大语言模型数学推理中高质量数据稀缺问题，利用自动定理证明研究构建数据引擎生成有效定理语料，设计三种挑战任务，实验发现模型在深度推理任务上表现差，提供诊断工具和训练数据，代码和数据公开。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型数学推理中高质量、逻辑合理数据稀缺的问题。

Method: 利用E - prover在TPTP公理库上的饱和能力，经过饱和公理、过滤有趣定理、生成任务的流程生成有效定理语料，并将其转化为三种难度可控的挑战任务。

Result: 零样本实验显示前沿模型在需要深度、结构化推理的任务上性能崩溃。

Conclusion: 框架可作为诊断工具衡量模型差距，也能提供可扩展的符号训练数据，代码和数据已公开。

Abstract: The scarcity of high-quality, logically sound data is a critical bottleneck
for advancing the mathematical reasoning of Large Language Models (LLMs). Our
work confronts this challenge by turning decades of automated theorem proving
research into a scalable data engine. Rather than relying on error-prone LLMs
or complex proof-assistant syntax like Lean and Isabelle, our framework
leverages E-prover's saturation capabilities on the vast TPTP axiom library to
derive a massive, guaranteed-valid corpus of theorems. Our pipeline is
principled and simple: saturate axioms, filter for "interesting" theorems, and
generate tasks. With no LLMs in the loop, we eliminate factual errors by
construction. This purely symbolic data is then transformed into three
difficulty-controlled challenges: entailment verification, premise selection,
and proof reconstruction. Our zero-shot experiments on frontier models reveal a
clear weakness: performance collapses on tasks requiring deep, structural
reasoning. Our framework provides both the diagnostic tool to measure this gap
and a scalable source of symbolic training data to address it. We make the code
and data publicly available.
  https://github.com/sileod/reasoning_core
https://hf.co/datasets/reasoning-core/rc1

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [381] [Volatility Modeling via EWMA-Driven Time-Dependent Hurst Parameters](https://arxiv.org/abs/2509.05820)
*Jayanth Athipatla*

Main category: q-fin.MF

TL;DR: 提出具有时变Hurst参数的新型粗糙Bergomi模型，用统一RDE公式，有数学易处理性，实证表现优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习和小波方法不同，需新模型捕捉波动率聚类和粗糙度爆发，且有数学易处理性。

Method: 采用基于粗糙路径理论的统一RDE公式，Hurst参数通过与瞬时方差相关的连续EWMA机制动态适应波动率制度。

Result: 在多种资产类别上实证验证，该模型在捕捉动态和样本外定价准确性上表现出色，显著优于传统恒定Hurst模型。

Conclusion: 新型粗糙Bergomi模型是有效的，能更好地捕捉金融资产动态和提高定价准确性。

Abstract: We introduce a novel rough Bergomi (rBergomi) model featuring a
variance-driven exponentially weighted moving average (EWMA) time-dependent
Hurst parameter $H_t$, fundamentally distinct from recent machine learning and
wavelet-based approaches in the literature. Our framework pioneers a unified
rough differential equation (RDE) formulation grounded in rough path theory,
where the Hurst parameter dynamically adapts to evolving volatility regimes
through a continuous EWMA mechanism tied to instantaneous variance. Unlike
discrete model-switching or computationally intensive forecasting methods, our
approach provides mathematical tractability while capturing volatility
clustering and roughness bursts. We rigorously establish existence and
uniqueness of solutions via rough path theory and derive martingale properties.
Empirical validation on diverse asset classes including equities,
cryptocurrencies, and commodities demonstrates superior performance in
capturing dynamics and out-of-sample pricing accuracy. Our results show
significant improvements over traditional constant-Hurst models.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [382] [Tiga: Accelerating Geo-Distributed Transactions with Synchronized Clocks [Technical Report]](https://arxiv.org/abs/2509.05759)
*Jinkun Geng,Shuai Mu,Anirudh Sivaraman,Balaji Prabhakar*

Main category: cs.NI

TL;DR: 本文提出新设计Tiga用于地理复制和可扩展事务数据库，能在多种场景1 WRTT内提交事务，性能优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 设计一种能在多种场景下于1 WRTT内提交事务，同时保持高吞吐量且计算开销最小的数据库方案。

Method: 合并并发控制和共识，用同步时钟为事务分配未来时间戳进行主动排序，事务按指定时间戳序列化；排序失败则走慢路径。

Result: 相比现有方案，Tiga能以1 - WRTT延迟提交更多事务，吞吐量开销更小，评估显示其吞吐量高1.3 - 7.2倍，延迟低1.4 - 4.6倍。

Conclusion: Tiga性能优于所有基线方案，已开源。

Abstract: This paper presents Tiga, a new design for geo-replicated and scalable
transactional databases such as Google Spanner. Tiga aims to commit
transactions within 1 wide-area roundtrip time, or 1 WRTT, for a wide range of
scenarios, while maintaining high throughput with minimal computational
overhead. Tiga consolidates concurrency control and consensus, completing both
strictly serializable execution and consistent replication in a single round.
It uses synchronized clocks to proactively order transactions by assigning each
a future timestamp at submission. In most cases, transactions arrive at servers
before their future timestamps and are serialized according to the designated
timestamp, requiring 1 WRTT to commit. In rare cases, transactions are delayed
and proactive ordering fails, in which case Tiga falls back to a slow path,
committing in 1.5--2 WRTTs. Compared to state-of-the-art solutions, Tiga can
commit more transactions at 1-WRTT latency, and incurs much less throughput
overhead. Evaluation results show that Tiga outperforms all baselines,
achieving 1.3--7.2$\times$ higher throughput and 1.4--4.6$\times$ lower
latency. Tiga is open-sourced at
https://github.com/New-Consensus-Concurrency-Control/Tiga.

</details>


### [383] [Distributed Link Sparsification for Scalable Scheduling Using Graph Neural Networks (Journal Version)](https://arxiv.org/abs/2509.05447)
*Zhongyuan Zhao,Gunjan Verma,Ananthram Swami,Santiago Segarra*

Main category: cs.NI

TL;DR: 提出用图神经网络的分布式链路稀疏化方案减少调度开销，缓解网络拥塞。


<details>
  <summary>Details</summary>
Motivation: 分布式链路调度算法在密集连接无线网络中产生大量信令开销，加剧拥塞、能耗等问题。

Method: 采用图神经网络模块，基于流量统计和网络拓扑调整链路竞争阈值，使用新颖的离线约束无监督学习算法平衡两个目标。

Result: 在模拟的最多500条链路的无线多跳网络中，该链路稀疏化技术有效缓解网络拥塞，减少了四种分布式链路调度协议的无线电覆盖范围。

Conclusion: 所提方案能减少调度开销，同时维持网络容量，缓解网络问题。

Abstract: In wireless networks characterized by dense connectivity, the significant
signaling overhead generated by distributed link scheduling algorithms can
exacerbate issues like congestion, energy consumption, and radio footprint
expansion. To mitigate these challenges, we propose a distributed link
sparsification scheme employing graph neural networks (GNNs) to reduce
scheduling overhead for delay-tolerant traffic while maintaining network
capacity. A GNN module is trained to adjust contention thresholds for
individual links based on traffic statistics and network topology, enabling
links to withdraw from scheduling contention when they are unlikely to succeed.
Our approach is facilitated by a novel offline constrained {unsupervised}
learning algorithm capable of balancing two competing objectives: minimizing
scheduling overhead while ensuring that total utility meets the required level.
In simulated wireless multi-hop networks with up to 500 links, our link
sparsification technique effectively alleviates network congestion and reduces
radio footprints across four distinct distributed link scheduling protocols.

</details>


### [384] [ALPHA: LLM-Enabled Active Learning for Human-Free Network Anomaly Detection](https://arxiv.org/abs/2509.05936)
*Xuanhao Luo,Shivesh Madan Nath Jha,Akruti Sinha,Zhizhen Li,Yuchen Liu*

Main category: cs.NI

TL;DR: 提出主动学习管道ALPHA用于无人工参与的日志分析，实现自动化异常检测，实验证明其准确性高、节省人力且支持可解释分析。


<details>
  <summary>Details</summary>
Motivation: 传统日志分析方法依赖专家知识或全监督学习模型，需要大量标注数据和人力，为解决此挑战而提出ALPHA。

Method: ALPHA集成语义嵌入、基于聚类的代表性采样和大语言模型辅助的少样本标注，提出两步少样本细化策略。

Result: 在真实日志数据集实验中，ALPHA检测准确率与全监督方法相当，减少人力投入，支持事后可解释分析。

Conclusion: ALPHA是真正实现自动化日志异常检测的可扩展且经济高效的解决方案。

Abstract: Network log data analysis plays a critical role in detecting security threats
and operational anomalies. Traditional log analysis methods for anomaly
detection and root cause analysis rely heavily on expert knowledge or fully
supervised learning models, both of which require extensive labeled data and
significant human effort. To address these challenges, we propose ALPHA, the
first Active Learning Pipeline for Human-free log Analysis. ALPHA integrates
semantic embedding, clustering-based representative sampling, and large
language model (LLM)-assisted few-shot annotation to automate the anomaly
detection process. The LLM annotated labels are propagated across clusters,
enabling large-scale training of an anomaly detector with minimal supervision.
To enhance the annotation accuracy, we propose a two-step few-shot refinement
strategy that adaptively selects informative prompts based on the LLM's
observed error patterns. Extensive experiments on real-world log datasets
demonstrate that ALPHA achieves detection accuracy comparable to fully
supervised methods while mitigating human efforts in the loop. ALPHA also
supports interpretable analysis through LLM-driven root cause explanations in
the post-detection stage. These capabilities make ALPHA a scalable and
cost-efficient solution for truly automated log-based anomaly detection.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [385] [Robust Analysis for Resilient AI System](https://arxiv.org/abs/2509.06172)
*Yu Wang,Ran Jin,Lulu Kang*

Main category: stat.AP

TL;DR: 提出DPD - Lasso稳健回归方法分析MII系统污染数据，在测试台表现良好，确立稳健回归在工业AI系统的重要性。


<details>
  <summary>Details</summary>
Motivation: MII系统的操作风险产生严重数据离群值，使传统统计分析失效。

Method: 提出DPD - Lasso方法，将密度功率散度与Lasso正则化结合，开发高效迭代算法。

Result: 应用于MII测试台，在干净和含离群值数据上都有可靠稳定表现，能准确量化风险影响。

Conclusion: 稳健回归是开发和验证弹性工业AI系统的重要工具。

Abstract: Operational hazards in Manufacturing Industrial Internet (MII) systems
generate severe data outliers that cripple traditional statistical analysis.
This paper proposes a novel robust regression method, DPD-Lasso, which
integrates Density Power Divergence with Lasso regularization to analyze
contaminated data from AI resilience experiments. We develop an efficient
iterative algorithm to overcome previous computational bottlenecks. Applied to
an MII testbed for Aerosol Jet Printing, DPD-Lasso provides reliable, stable
performance on both clean and outlier-contaminated data, accurately quantifying
hazard impacts. This work establishes robust regression as an essential tool
for developing and validating resilient industrial AI systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [386] [Energy-Efficient Path Planning with Multi-Location Object Pickup for Mobile Robots on Uneven Terrain](https://arxiv.org/abs/2509.06061)
*Faiza Babakano,Ahmed Fahmin,Bojie Shen,Muhammad Aamir Cheema,Isma Farah Siddiqui*

Main category: cs.RO

TL;DR: 本文提出OMEPP问题，解决需中途取物的AMR节能路径规划，先提出基线算法，再提出并发PCPD搜索法，实验表明后者近最优且速度快。


<details>
  <summary>Details</summary>
Motivation: 现有研究多忽略机器人中途取物场景对能耗的影响，本文旨在解决需中途取物的AMR节能路径规划问题。

Method: 先引入基于Z星算法的基线算法，再提出并发PCPD搜索法，利用考虑负载约束的PCPD减少搜索分支。

Result: 并发PCPD搜索法虽解略次优，但在真实数据集实验中近最优，且比基线算法快一到两个数量级。

Conclusion: 并发PCPD搜索法能在保证接近最优解的情况下，显著提高计算效率，可用于解决OMEPP问题。

Abstract: Autonomous Mobile Robots (AMRs) operate on battery power, making energy
efficiency a critical consideration, particularly in outdoor environments where
terrain variations affect energy consumption. While prior research has
primarily focused on computing energy-efficient paths from a source to a
destination, these approaches often overlook practical scenarios where a robot
needs to pick up an object en route - an action that can significantly impact
energy consumption due to changes in payload. This paper introduces the
Object-Pickup Minimum Energy Path Problem (OMEPP), which addresses
energy-efficient route planning for AMRs required to pick up an object from one
of many possible locations and deliver it to a destination. To address OMEPP,
we first introduce a baseline algorithm that employs the Z star algorithm, a
variant of A star tailored for energy-efficient routing, to iteratively visit
each pickup point. While this approach guarantees optimality, it suffers from
high computational cost due to repeated searches at each pickup location. To
mitigate this inefficiency, we propose a concurrent PCPD search that manages
multiple Z star searches simultaneously across all pickup points. Central to
our solution is the Payload-Constrained Path Database (PCPD), an extension of
the Compressed Path Database (CPD) that incorporates payload constraints. We
demonstrate that PCPD significantly reduces branching factors during search,
improving overall performance. Although the concurrent PCPD search may produce
slightly suboptimal solutions, extensive experiments on real-world datasets
show it achieves near-optimal performance while being one to two orders of
magnitude faster than the baseline algorithm.

</details>


### [387] [LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba](https://arxiv.org/abs/2508.11849)
*Yinuo Wang,Gavin Tao*

Main category: cs.RO

TL;DR: 介绍基于选择性状态空间模型的视觉驱动跨模态DRL框架LocoMamba，评估显示其性能优、泛化强且训练高效。


<details>
  <summary>Details</summary>
Motivation: 构建能有效捕捉长程依赖、实现高效长序列训练的视觉驱动跨模态DRL框架。

Method: 1. 用多层感知器嵌入本体感受状态，用轻量级卷积神经网络处理深度图像；2. 用堆叠Mamba层通过近线性时间选择性扫描融合标记；3. 用近端策略优化进行端到端训练。

Result: 在模拟环境中，比基线方法有更高回报和成功率、更少碰撞，对未见地形和障碍物密度泛化性强，在相同计算预算下更新次数少。

Conclusion: LocoMamba框架有效，性能、泛化和训练效率表现出色。

Abstract: We introduce LocoMamba, a vision-driven cross-modal DRL framework built on
selective state-space models, specifically leveraging Mamba, that achieves
near-linear-time sequence modeling, effectively captures long-range
dependencies, and enables efficient training with longer sequences. First, we
embed proprioceptive states with a multilayer perceptron and patchify depth
images with a lightweight convolutional neural network, producing compact
tokens that improve state representation. Second, stacked Mamba layers fuse
these tokens via near-linear-time selective scanning, reducing latency and
memory footprint, remaining robust to token length and image resolution, and
providing an inductive bias that mitigates overfitting. Third, we train the
policy end-to-end with Proximal Policy Optimization under terrain and
appearance randomization and an obstacle-density curriculum, using a compact
state-centric reward that balances progress, smoothness, and safety. We
evaluate our method in challenging simulated environments with static and
moving obstacles as well as uneven terrain. Compared with state-of-the-art
baselines, our method achieves higher returns and success rates with fewer
collisions, exhibits stronger generalization to unseen terrains and obstacle
densities, and improves training efficiency by converging in fewer updates
under the same compute budget.

</details>


### [388] [ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory](https://arxiv.org/abs/2509.05314)
*Ying Li,Xiaobao Wei,Xiaowei Chi,Yuming Li,Zhongyu Zhao,Hao Wang,Ningning Ma,Ming Lu,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出名为ManipDreamer3D的框架，结合3D轨迹规划与轨迹到视频扩散模型，生成3D感知的机器人操作视频，减少人工干预且视觉质量更好。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺问题，克服现有基于2D轨迹的扩散模型存在的3D空间模糊性问题。

Method: 结合3D轨迹规划和重建的3D占用图，使用新颖的轨迹到视频扩散模型；先从输入图像重建3D占用表示，计算优化的3D末端执行器轨迹，再用潜在编辑技术生成视频序列。

Result: 生成具有自主规划的合理3D轨迹的机器人视频，视觉质量优于现有方法。

Conclusion: 所提出的ManipDreamer3D框架有效，能显著减少人工干预，在生成机器人操作视频方面表现良好。

Abstract: Data scarcity continues to be a major challenge in the field of robotic
manipulation. Although diffusion models provide a promising solution for
generating robotic manipulation videos, existing methods largely depend on 2D
trajectories, which inherently face issues with 3D spatial ambiguity. In this
work, we present a novel framework named ManipDreamer3D for generating
plausible 3D-aware robotic manipulation videos from the input image and the
text instruction. Our method combines 3D trajectory planning with a
reconstructed 3D occupancy map created from a third-person perspective, along
with a novel trajectory-to-video diffusion model. Specifically, ManipDreamer3D
first reconstructs the 3D occupancy representation from the input image and
then computes an optimized 3D end-effector trajectory, minimizing path length
while avoiding collisions. Next, we employ a latent editing technique to create
video sequences from the initial image latent and the optimized 3D trajectory.
This process conditions our specially trained trajectory-to-video diffusion
model to produce robotic pick-and-place videos. Our method generates robotic
videos with autonomously planned plausible 3D trajectories, significantly
reducing human intervention requirements. Experimental results demonstrate
superior visual quality compared to existing methods.

</details>


### [389] [Evaluation of Large Language Models for Anomaly Detection in Autonomous Vehicles](https://arxiv.org/abs/2509.05315)
*Petros Loukas,David Bassir,Savvas Chatzichristofis,Angelos Amanatiadis*

Main category: cs.RO

TL;DR: 本文评估大语言模型（LLMs）在自动驾驶汽车真实边缘场景的表现，提出架构并给出对比结果。


<details>
  <summary>Details</summary>
Motivation: 现有对LLMs在自动驾驶汽车的评估局限于合成数据集或无真实信息的手动驾驶数据集，缺乏对当前感知和规划算法在评估场景下表现的了解，因此要评估LLMs在真实边缘场景的表现。

Method: 提出由开放词汇对象检测器、提示工程和大语言模型上下文推理组成的架构，用多个先进模型针对真实边缘场景进行评估。

Result: 提供了定性的对比结果。

Conclusion: 对研究结果进行讨论，探讨了LLMs作为自动驾驶汽车异常检测器的潜在应用。

Abstract: The rapid evolution of large language models (LLMs) has pushed their
boundaries to many applications in various domains. Recently, the research
community has started to evaluate their potential adoption in autonomous
vehicles and especially as complementary modules in the perception and planning
software stacks. However, their evaluation is limited in synthetic datasets or
manually driving datasets without the ground truth knowledge and more
precisely, how the current perception and planning algorithms would perform in
the cases under evaluation. For this reason, this work evaluates LLMs on
real-world edge cases where current autonomous vehicles have been proven to
fail. The proposed architecture consists of an open vocabulary object detector
coupled with prompt engineering and large language model contextual reasoning.
We evaluate several state-of-the-art models against real edge cases and provide
qualitative comparison results along with a discussion on the findings for the
potential application of LLMs as anomaly detectors in autonomous vehicles.

</details>


### [390] [Plantbot: Integrating Plant and Robot through LLM Modular Agent Networks](https://arxiv.org/abs/2509.05338)
*Atsushi Masumori,Norihiro Maruyama,Itsuki Doi,johnsmith,Hiroki Sato,Takashi Ikegami*

Main category: cs.RO

TL;DR: 介绍了Plantbot，一种通过大语言模型模块网络连接植物与移动机器人的混合生命形式，其能实现生物与人工领域交互，为人工生命新模型提供可能。


<details>
  <summary>Details</summary>
Motivation: 探索生物与人工系统新的交互方式，构建新型人工生命模型。

Method: 构建由大语言模型模块组成的网络，各模块负责不同功能，通过自然语言异步通信，将植物状态转化为机器人动作。

Result: Plantbot成为一个具身、自适应的智能体，能自主响应环境条件。

Conclusion: 这种通过大语言模型协调生物与机器人元素的方法为人工生命新模型提供了可能性。

Abstract: We introduce Plantbot, a hybrid lifeform that connects a living plant with a
mobile robot through a network of large language model (LLM) modules. Each
module - responsible for sensing, vision, dialogue, or action - operates
asynchronously and communicates via natural language, enabling seamless
interaction across biological and artificial domains. This architecture
leverages the capacity of LLMs to serve as hybrid interfaces, where natural
language functions as a universal protocol, translating multimodal data (soil
moisture, temperature, visual context) into linguistic messages that coordinate
system behaviors. The integrated network transforms plant states into robotic
actions, installing normativity essential for agency within the sensor-motor
loop. By combining biological and robotic elements through LLM-mediated
communication, Plantbot behaves as an embodied, adaptive agent capable of
responding autonomously to environmental conditions. This approach suggests
possibilities for a new model of artificial life, where decentralized, LLM
modules coordination enable novel interactions between biological and
artificial systems.

</details>


### [391] [Spiking Neural Networks for Continuous Control via End-to-End Model-Based Learning](https://arxiv.org/abs/2509.05356)
*Justus Huebotter,Pablo Lanillos,Marcel van Gerven,Serge Thill*

Main category: cs.RO

TL;DR: 本文展示全脉冲架构可端到端训练以控制连续环境中的多自由度机械臂，评估表明脉冲神经网络可实现稳定训练和精确扭矩控制，同时强调循环脉冲网络对超参数敏感。


<details>
  <summary>Details</summary>
Motivation: 尽管脉冲神经网络在分类任务训练有进展，但在连续运动控制方面应用有限，故研究其在连续环境中控制多自由度机械臂的能力。

Method: 提出预测控制框架，结合漏电积分放电动力学与替代梯度，联合优化用于动力学预测的前向模型和用于目标导向行动的策略网络。

Result: 在二维平面到达任务和模拟的六自由度机器人上验证，脉冲神经网络能实现稳定训练和精确扭矩控制，消融研究凸显初始化、可学习时间常数和正则化对训练动态的作用。

Conclusion: 虽然能实现稳定有效控制，但循环脉冲网络对超参数设置高度敏感，强调原则性设计选择的重要性。

Abstract: Despite recent progress in training spiking neural networks (SNNs) for
classification, their application to continuous motor control remains limited.
Here, we demonstrate that fully spiking architectures can be trained end-to-end
to control robotic arms with multiple degrees of freedom in continuous
environments. Our predictive-control framework combines Leaky
Integrate-and-Fire dynamics with surrogate gradients, jointly optimizing a
forward model for dynamics prediction and a policy network for goal-directed
action. We evaluate this approach on both a planar 2D reaching task and a
simulated 6-DOF Franka Emika Panda robot. Results show that SNNs can achieve
stable training and accurate torque control, establishing their viability for
high-dimensional motor tasks. An extensive ablation study highlights the role
of initialization, learnable time constants, and regularization in shaping
training dynamics. We conclude that while stable and effective control can be
achieved, recurrent spiking networks remain highly sensitive to hyperparameter
settings, underscoring the importance of principled design choices.

</details>


### [392] [Long-Horizon Visual Imitation Learning via Plan and Code Reflection](https://arxiv.org/abs/2509.05368)
*Quan Chen,Chenrui Shi,Qi Chen,Yuwei Wu,Zhi Gao,Xintong Zhang,Rui Gao,Kun Wu,Yunde Jia*

Main category: cs.RO

TL;DR: 本文提出新的代理框架，含两个反思模块提升计划和代码生成，还引入LongVILBench基准进行评估，新框架为长时视觉模仿学习建立强基线。


<details>
  <summary>Details</summary>
Motivation: 长时复杂动作序列演示的视觉模仿学习在理解动作时间关系和物体空间关系上存在挑战。

Method: 提出含计划生成、计划反思、代码生成和代码反思模块的代理框架；引入含300个人类演示的LongVILBench基准。

Result: 现有方法在LongVILBench基准上表现不佳，新框架建立强基线。

Conclusion: 两个反思模块能检测和纠正计划与代码生成中的错误，提升复杂时空依赖任务表现，新框架适用于长时视觉模仿学习。

Abstract: Learning from long-horizon demonstrations with complex action sequences
presents significant challenges for visual imitation learning, particularly in
understanding temporal relationships of actions and spatial relationships
between objects. In this paper, we propose a new agent framework that
incorporates two dedicated reflection modules to enhance both plan and code
generation. The plan generation module produces an initial action sequence,
which is then verified by the plan reflection module to ensure temporal
coherence and spatial alignment with the demonstration video. The code
generation module translates the plan into executable code, while the code
reflection module verifies and refines the generated code to ensure correctness
and consistency with the generated plan. These two reflection modules jointly
enable the agent to detect and correct errors in both the plan generation and
code generation, improving performance in tasks with intricate temporal and
spatial dependencies. To support systematic evaluation, we introduce
LongVILBench, a benchmark comprising 300 human demonstrations with action
sequences of up to 18 steps. LongVILBench emphasizes temporal and spatial
complexity across multiple task types. Experimental results demonstrate that
existing methods perform poorly on this benchmark, whereas our new framework
establishes a strong baseline for long-horizon visual imitation learning.

</details>


### [393] [Learning Tool-Aware Adaptive Compliant Control for Autonomous Regolith Excavation](https://arxiv.org/abs/2509.05475)
*Andrej Orsula,Matthieu Geist,Miguel Olivares-Mendez,Carol Martinez*

Main category: cs.RO

TL;DR: 本文提出基于模型的强化学习框架，在并行模拟中训练智能体以应对自主月壤挖掘挑战，实验证明训练和视觉反馈的重要性。


<details>
  <summary>Details</summary>
Motivation: 自主月壤挖掘是原位资源利用的基石，但受颗粒介质复杂交互和工具多样性操作需求的阻碍。

Method: 引入基于模型的强化学习框架，在并行模拟中训练智能体，利用高保真粒子物理和程序生成创建月壤地形和挖掘工具几何形状分布，智能体通过操作空间控制动态调整自身刚度和阻尼。

Result: 实验表明使用程序分布的工具进行训练对泛化至关重要，能发展复杂的工具感知行为，增加视觉反馈可显著提高任务成功率。

Conclusion: 该方法为未来太空任务基础任务所需的强大而通用的自主系统开发提供了有效途径。

Abstract: Autonomous regolith excavation is a cornerstone of in-situ resource
utilization for a sustained human presence beyond Earth. However, this task is
fundamentally hindered by the complex interaction dynamics of granular media
and the operational need for robots to use diverse tools. To address these
challenges, this work introduces a framework where a model-based reinforcement
learning agent learns within a parallelized simulation. This environment
leverages high-fidelity particle physics and procedural generation to create a
vast distribution of both lunar terrains and excavation tool geometries. To
master this diversity, the agent learns an adaptive interaction strategy by
dynamically modulating its own stiffness and damping at each control step
through operational space control. Our experiments demonstrate that training
with a procedural distribution of tools is critical for generalization and
enables the development of sophisticated tool-aware behavior. Furthermore, we
show that augmenting the agent with visual feedback significantly improves task
success. These results represent a validated methodology for developing the
robust and versatile autonomous systems required for the foundational tasks of
future space missions.

</details>


### [394] [Microrobot Vascular Parkour: Analytic Geometry-based Path Planning with Real-time Dynamic Obstacle Avoidance](https://arxiv.org/abs/2509.05500)
*Yanda Yang,Max Sokolich,Fatma Ceren Kirmizitas,Sambeeta Das,Andreas A. Malikopoulos*

Main category: cs.RO

TL;DR: 提出实时路径规划框架用于血管内自主微机器人导航，结合全局规划器与局部控制器，在仿真和实验中表现良好，推进相关应用。


<details>
  <summary>Details</summary>
Motivation: 血管内自主微机器人导航受密集移动障碍物挑战，需解决导航问题以实现微创治疗。

Method: 提出实时路径规划框架，将解析几何全局规划器与基于规则和强化学习的局部逃逸控制器结合，利用实时成像计算无碰撞运动。

Result: 仿真中AGP比WA*、PSO、RRT路径更短、规划更快，可扩展到3D；仿真和实验中能可靠避障并到达目标，平均规划时间40ms/帧。

Conclusion: 研究成果推进了血管环境中自主微机器人导航和靶向药物递送。

Abstract: Autonomous microrobots in blood vessels could enable minimally invasive
therapies, but navigation is challenged by dense, moving obstacles. We propose
a real-time path planning framework that couples an analytic geometry global
planner (AGP) with two reactive local escape controllers, one based on rules
and one based on reinforcement learning, to handle sudden moving obstacles.
Using real-time imaging, the system estimates the positions of the microrobot,
obstacles, and targets and computes collision-free motions. In simulation, AGP
yields shorter paths and faster planning than weighted A* (WA*), particle swarm
optimization (PSO), and rapidly exploring random trees (RRT), while maintaining
feasibility and determinism. We extend AGP from 2D to 3D without loss of speed.
In both simulations and experiments, the combined global planner and local
controllers reliably avoid moving obstacles and reach targets. The average
planning time is 40 ms per frame, compatible with 25 fps image acquisition and
real-time closed-loop control. These results advance autonomous microrobot
navigation and targeted drug delivery in vascular environments.

</details>


### [395] [Learning to Walk in Costume: Adversarial Motion Priors for Aesthetically Constrained Humanoids](https://arxiv.org/abs/2509.05581)
*Arturo Flores Alvarez,Fatemeh Zargarbashi,Havel Liu,Shiqi Wang,Liam Edwards,Jessica Anz,Alex Xu,Fan Shi,Stelian Coros,Dennis W. Hong*

Main category: cs.RO

TL;DR: 提出基于强化学习的人形机器人Cosmo运动系统，用AMP克服娱乐机器人设计挑战，实验证明能生成稳定行为，为兼顾美观与性能指明方向。


<details>
  <summary>Details</summary>
Motivation: 娱乐机器人因美观设计带来独特挑战，需解决定制人形机器人Cosmo在特殊设计下的运动问题。

Method: 应用Adversarial Motion Priors (AMP)让机器人学习自然动作并保持稳定，开发定制领域随机化技术和特殊奖励结构确保安全的从模拟到现实部署。

Result: 实验表明AMP能让Cosmo在极端质量分布和运动限制下产生稳定的站立和行走行为。

Conclusion: 基于学习的方法能有效适应美观驱动的设计约束，为兼顾美观与功能的机器人发展提供了有前景的方向。

Abstract: We present a Reinforcement Learning (RL)-based locomotion system for Cosmo, a
custom-built humanoid robot designed for entertainment applications. Unlike
traditional humanoids, entertainment robots present unique challenges due to
aesthetic-driven design choices. Cosmo embodies these with a disproportionately
large head (16% of total mass), limited sensing, and protective shells that
considerably restrict movement. To address these challenges, we apply
Adversarial Motion Priors (AMP) to enable the robot to learn natural-looking
movements while maintaining physical stability. We develop tailored domain
randomization techniques and specialized reward structures to ensure safe
sim-to-real, protecting valuable hardware components during deployment. Our
experiments demonstrate that AMP generates stable standing and walking
behaviors despite Cosmo's extreme mass distribution and movement constraints.
These results establish a promising direction for robots that balance aesthetic
appeal with functional performance, suggesting that learning-based methods can
effectively adapt to aesthetic-driven design constraints.

</details>


### [396] [RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning](https://arxiv.org/abs/2509.05397)
*Matthew Lai,Keegan Go,Zhibin Li,Torsten Kroger,Stefan Schaal,Kelsey Allen,Jonathan Scholz*

Main category: cs.RO

TL;DR: 提出基于强化学习框架实现多机器人自动任务与运动规划，在模拟中训练后可零样本泛化，还能用于工作单元布局优化等。


<details>
  <summary>Details</summary>
Motivation: 现代机器人制造需多机器人无碰撞协调，传统方法在时空约束下进行任务分配、调度和运动规划计算复杂，现有工业多臂系统依赖人工设计轨迹，因此需自动化解决方案。

Method: 提出基于强化学习框架，使用图神经网络策略，在程序生成的多样环境中训练，用图表示场景和图策略神经网络生成多机器人轨迹，联合解决任务分配、调度和运动规划子问题。

Result: 策略可零样本泛化到不同机器人位置、障碍物几何形状和任务姿态的未见过场景，高速能力可用于工作单元布局优化，提升解决时间，也为容错规划和在线感知重规划等新能力提供可能。

Conclusion: 所提出的强化学习框架在多机器人任务和运动规划上有良好效果，具有泛化性、高速性和可扩展性，能应用于多种场景。

Abstract: Modern robotic manufacturing requires collision-free coordination of multiple
robots to complete numerous tasks in shared, obstacle-rich workspaces. Although
individual tasks may be simple in isolation, automated joint task allocation,
scheduling, and motion planning under spatio-temporal constraints remain
computationally intractable for classical methods at real-world scales.
Existing multi-arm systems deployed in the industry rely on human intuition and
experience to design feasible trajectories manually in a labor-intensive
process. To address this challenge, we propose a reinforcement learning (RL)
framework to achieve automated task and motion planning, tested in an
obstacle-rich environment with eight robots performing 40 reaching tasks in a
shared workspace, where any robot can perform any task in any order. Our
approach builds on a graph neural network (GNN) policy trained via RL on
procedurally-generated environments with diverse obstacle layouts, robot
configurations, and task distributions. It employs a graph representation of
scenes and a graph policy neural network trained through reinforcement learning
to generate trajectories of multiple robots, jointly solving the sub-problems
of task allocation, scheduling, and motion planning. Trained on large randomly
generated task sets in simulation, our policy generalizes zero-shot to unseen
settings with varying robot placements, obstacle geometries, and task poses. We
further demonstrate that the high-speed capability of our solution enables its
use in workcell layout optimization, improving solution times. The speed and
scalability of our planner also open the door to new capabilities such as
fault-tolerant planning and online perception-based re-planning, where rapid
adaptation to dynamic task sets is required.

</details>


### [397] [Learning in ImaginationLand: Omnidirectional Policies through 3D Generative Models (OP-Gen)](https://arxiv.org/abs/2509.06191)
*Yifei Ren,Edward Johns*

Main category: cs.RO

TL;DR: 利用3D生成模型增强数据集，学习全向策略，减少策略学习所需演示次数，实验显示性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 利用3D生成模型为机器人领域带来新机遇，减少策略学习所需演示次数。

Method: 用3D生成模型从单次真实演示增强数据集，在想象数据集中学习全向策略。

Result: 使机器人能从与演示状态差异大的初始状态执行任务，减少演示次数，实验性能优于基线。

Conclusion: 3D生成模型增强数据集学习全向策略有效，可用于机器人任务，性能表现好。

Abstract: Recent 3D generative models, which are capable of generating full object
shapes from just a few images, now open up new opportunities in robotics. In
this work, we show that 3D generative models can be used to augment a dataset
from a single real-world demonstration, after which an omnidirectional policy
can be learned within this imagined dataset. We found that this enables a robot
to perform a task when initialised from states very far from those observed
during the demonstration, including starting from the opposite side of the
object relative to the real-world demonstration, significantly reducing the
number of demonstrations required for policy learning. Through several
real-world experiments across tasks such as grasping objects, opening a drawer,
and placing trash into a bin, we study these omnidirectional policies by
investigating the effect of various design choices on policy behaviour, and we
show superior performance to recent baselines which use alternative methods for
data augmentation.

</details>


### [398] [Grasp-MPC: Closed-Loop Visual Grasping via Value-Guided Model Predictive Control](https://arxiv.org/abs/2509.06201)
*Jun Yamada,Adithyavairavan Murali,Ajay Mandlekar,Clemens Eppner,Ingmar Posner,Balakumar Sundaralingam*

Main category: cs.RO

TL;DR: 提出Grasp - MPC用于杂乱环境中对新物体的鲁棒抓取，在模拟和现实中提升抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 开环抓取方法在杂乱环境表现不佳，闭环方法难以泛化，需一种能在杂乱环境对新物体进行鲁棒抓取的方法。

Method: 提出Grasp - MPC，结合基于大规模合成数据集训练的价值函数和MPC框架，加入避免碰撞和执行平滑的成本项。

Result: Grasp - MPC在模拟中提升抓取成功率达32.6%，在现实噪声环境提升33.3%，优于开环、扩散策略等方法。

Conclusion: Grasp - MPC能有效提升杂乱环境中对新物体的抓取成功率。

Abstract: Grasping of diverse objects in unstructured environments remains a
significant challenge. Open-loop grasping methods, effective in controlled
settings, struggle in cluttered environments. Grasp prediction errors and
object pose changes during grasping are the main causes of failure. In
contrast, closed-loop methods address these challenges in simplified settings
(e.g., single object on a table) on a limited set of objects, with no path to
generalization. We propose Grasp-MPC, a closed-loop 6-DoF vision-based grasping
policy designed for robust and reactive grasping of novel objects in cluttered
environments. Grasp-MPC incorporates a value function, trained on visual
observations from a large-scale synthetic dataset of 2 million grasp
trajectories that include successful and failed attempts. We deploy this
learned value function in an MPC framework in combination with other cost terms
that encourage collision avoidance and smooth execution. We evaluate Grasp-MPC
on FetchBench and real-world settings across diverse environments. Grasp-MPC
improves grasp success rates by up to 32.6% in simulation and 33.3% in
real-world noisy conditions, outperforming open-loop, diffusion policy,
transformer policy, and IQL approaches. Videos and more at
http://grasp-mpc.github.io.

</details>


### [399] [Learning to Walk with Less: a Dyna-Style Approach to Quadrupedal Locomotion](https://arxiv.org/abs/2509.06296)
*Francisco Affonso,Felipe Andrade G. Tommaselli,Juliano Negri,Vivian S. Medeiros,Mateus V. Gasparino,Girish Chowdhary,Marcelo Becker*

Main category: cs.RO

TL;DR: 提出基于模型的强化学习框架，通过追加合成数据提高四足机器人运动样本效率，仿真验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统基于强化学习的运动控制器数据效率低，需大量交互才能获得鲁棒性能。

Method: 在基于PPO的控制器标准滚动之后追加合成数据，遵循Dyna-Style范式；训练预测模型生成短视距合成转换，用调度策略逐步整合；通过消融研究指导实验设计。

Result: 在Unitree Go1机器人仿真中，用合成步骤替代部分模拟步骤可模拟扩展滚动，提高策略回报并降低方差。

Conclusion: 该改进能让机器人用更少模拟步骤跟踪多种运动指令。

Abstract: Traditional RL-based locomotion controllers often suffer from low data
efficiency, requiring extensive interaction to achieve robust performance. We
present a model-based reinforcement learning (MBRL) framework that improves
sample efficiency for quadrupedal locomotion by appending synthetic data to the
end of standard rollouts in PPO-based controllers, following the Dyna-Style
paradigm. A predictive model, trained alongside the policy, generates
short-horizon synthetic transitions that are gradually integrated using a
scheduling strategy based on the policy update iterations. Through an ablation
study, we identified a strong correlation between sample efficiency and rollout
length, which guided the design of our experiments. We validated our approach
in simulation on the Unitree Go1 robot and showed that replacing part of the
simulated steps with synthetic ones not only mimics extended rollouts but also
improves policy return and reduces variance. Finally, we demonstrate that this
improvement transfers to the ability to track a wide range of locomotion
commands using fewer simulated steps.

</details>


### [400] [Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments](https://arxiv.org/abs/2509.06953)
*Jiahui Yang,Jason Jingzhou Liu,Yulong Li,Youssef Khaky,Kenneth Shaw,Deepak Pathak*

Main category: cs.RO

TL;DR: 提出用于动态环境的深度反应策略DRP，在模拟和现实场景表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 经典运动规划器难用于动态场景，神经运动策略在复杂动态环境泛化能力不足。

Method: 提出DRP，核心是IMPACT，通过师生微调改进静态避障，用DCP - RMP增强动态避障。

Result: 在复杂任务中评估，DRP有强泛化性，成功率优于先前经典和神经方法。

Conclusion: DRP在动态环境中生成无碰撞运动效果好，在模拟和现实世界表现出色。

Abstract: Generating collision-free motion in dynamic, partially observable
environments is a fundamental challenge for robotic manipulators. Classical
motion planners can compute globally optimal trajectories but require full
environment knowledge and are typically too slow for dynamic scenes. Neural
motion policies offer a promising alternative by operating in closed-loop
directly on raw sensory inputs but often struggle to generalize in complex or
dynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural
motion policy designed for reactive motion generation in diverse dynamic
environments, operating directly on point cloud sensory input. At its core is
IMPACT, a transformer-based neural motion policy pretrained on 10 million
generated expert trajectories across diverse simulation scenarios. We further
improve IMPACT's static obstacle avoidance through iterative student-teacher
finetuning. We additionally enhance the policy's dynamic obstacle avoidance at
inference time using DCP-RMP, a locally reactive goal-proposal module. We
evaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving
obstacles, and goal obstructions. DRP achieves strong generalization,
outperforming prior classical and neural methods in success rate across both
simulated and real-world settings. Video results and code available at
https://deep-reactive-policy.com

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [401] [Livia: An Emotion-Aware AR Companion Powered by Modular AI Agents and Progressive Memory Compression](https://arxiv.org/abs/2509.05298)
*Rui Xi,Xianghan Wang*

Main category: cs.HC

TL;DR: 本文介绍情感感知增强现实伴侣应用Livia，通过多种技术提供个性化情感支持，经用户评估有积极效果，还指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 孤独和社交隔离带来情感与健康挑战，需开发基于技术的陪伴和情感支持解决方案。

Method: 采用模块化AI架构，有专门代理负责不同功能；使用TBC和DIMF算法管理长期记忆；采用多模态情感检测方法。

Result: 多模态情感检测准确率高；用户评估显示增强情感纽带、提高满意度、显著降低孤独感，用户看重其自适应个性演变和逼真AR体现。

Conclusion: Livia能有效提供个性化情感支持，未来可拓展手势和触觉交互、支持多用户体验、探索定制硬件实现。

Abstract: Loneliness and social isolation pose significant emotional and health
challenges, prompting the development of technology-based solutions for
companionship and emotional support. This paper introduces Livia, an
emotion-aware augmented reality (AR) companion app designed to provide
personalized emotional support by combining modular artificial intelligence
(AI) agents, multimodal affective computing, progressive memory compression,
and AR driven embodied interaction. Livia employs a modular AI architecture
with specialized agents responsible for emotion analysis, dialogue generation,
memory management, and behavioral orchestration, ensuring robust and adaptive
interactions. Two novel algorithms-Temporal Binary Compression (TBC) and
Dynamic Importance Memory Filter (DIMF)-effectively manage and prioritize
long-term memory, significantly reducing storage requirements while retaining
critical context. Our multimodal emotion detection approach achieves high
accuracy, enhancing proactive and empathetic engagement. User evaluations
demonstrated increased emotional bonds, improved satisfaction, and
statistically significant reductions in loneliness. Users particularly valued
Livia's adaptive personality evolution and realistic AR embodiment. Future
research directions include expanding gesture and tactile interactions,
supporting multi-user experiences, and exploring customized hardware
implementations.

</details>


### [402] [Explained, yet misunderstood: How AI Literacy shapes HR Managers' interpretation of User Interfaces in Recruiting Recommender Systems](https://arxiv.org/abs/2509.06475)
*Yannick Kalff,Katharina Simbeck*

Main category: cs.HC

TL;DR: 研究探讨HR经理AI素养对招聘推荐面板中可解释AI元素主客观理解的影响，发现XAI效果依赖用户AI素养。


<details>
  <summary>Details</summary>
Motivation: AI推荐系统影响招聘决策，需在人力资源管理中确保透明和负责任地采用，因此研究HR经理AI素养对XAI元素理解的影响。

Method: 通过在线实验，让410名德国HR经理对比基线面板和三种XAI风格的面板。

Result: 实践中面板未解释AI结果且元素不透明；添加XAI特征提升中高AI素养用户主观感受，但不增加客观理解，复杂解释甚至降低准确理解；仅重要特征叠加显著帮助高素养用户解读。

Conclusion: XAI在招聘中的益处依赖用户AI素养，需在人力资源管理中制定定制解释策略和针对性素养培训。

Abstract: AI-based recommender systems increasingly influence recruitment decisions.
Thus, transparency and responsible adoption in Human Resource Management (HRM)
are critical. This study examines how HR managers' AI literacy influences their
subjective perception and objective understanding of explainable AI (XAI)
elements in recruiting recommender dashboards. In an online experiment, 410
German-based HR managers compared baseline dashboards to versions enriched with
three XAI styles: important features, counterfactuals, and model criteria. Our
results show that the dashboards used in practice do not explain AI results and
even keep AI elements opaque. However, while adding XAI features improves
subjective perceptions of helpfulness and trust among users with moderate or
high AI literacy, it does not increase their objective understanding. It may
even reduce accurate understanding, especially with complex explanations. Only
overlays of important features significantly aided the interpretations of
high-literacy users. Our findings highlight that the benefits of XAI in
recruitment depend on users' AI literacy, emphasizing the need for tailored
explanation strategies and targeted literacy training in HRM to ensure fair,
transparent, and effective adoption of AI.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [403] [Separable convex optimization over indegree polytopes](https://arxiv.org/abs/2509.06182)
*Nóra A. Borsik,Péter Madarasi*

Main category: math.CO

TL;DR: 研究无向图基于入度目标的平等（无环）定向问题，给出复杂度结果和算法。


<details>
  <summary>Details</summary>
Motivation: 研究无向图在入度目标下平等（无环）定向的算法复杂性和求解方法，因无环定向会使公平目标产生分歧。

Method: 理论证明复杂度，设计多项式时间算法、精确指数时间算法和近似算法。

Result: 在多种设定下证明问题NP难，给出多种问题的算法。

Conclusion: 明确了入度（基）多面体上凸整数优化的算法边界，有理论和实践意义。

Abstract: We study egalitarian (acyclic) orientations of undirected graphs under
indegree-based objectives, such as minimizing the $\varphi$-sum of indegrees
for a strictly convex function $\varphi$, decreasing minimization (dec-min),
and increasing maximization (inc-max). In the non-acyclic setting of Frank and
Murota (2022), a single orientation simultaneously optimizes these three
objectives, however, restricting to acyclic orientations confines us to the
corners of the indegree polytope, where these fairness objectives do diverge.
We establish strong hardness results across a broad range of settings:
minimizing the $\varphi$-sum of indegrees is NP-hard for every discrete
strictly convex function $\varphi$; dec-min and inc-max are NP-hard for every
indegree bound $k \geq 2$, as well as without a bound; and the complementary
inc-min and dec-max problems are NP-hard even on $3$-regular graphs. On the
algorithmic side, we give a polynomial-time algorithm for minimizing the
maximum weighted indegree via a weighted smallest-last ordering. We also
provide an exact exponential-time algorithm for minimizing general separable
discrete convex objectives over indegrees, and a polynomial-time algorithm for
the non-acyclic case. Finally, for maximizing the sum of the products of
indegrees and outdegrees, we prove NP-hardness on graphs of maximum degree $4$,
give an algorithm for maximum degree $3$, and provide a $3$-approximation
algorithm. Our results delineate the algorithmic frontier of convex integral
optimization over indegree (base-)polytopes, and highlight both theoretical
consequences and practical implications, notably for scheduling and
deadlock-free routing.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [404] [Critical Iridium Demands arising from future Expansion of Proton Exchange Membrane Electrolysis](https://arxiv.org/abs/2509.05357)
*Bernhard Wortmann,Detlef Stolten,Heidi Heinrichs*

Main category: econ.GN

TL;DR: 研究评估不同情景下PEMEL对铱的未来需求，指出实现净零目标需提高催化剂效率和获取全球约30%铱产量，2030年或现供应短缺，强调材料效率和回收创新及将资源约束纳入规划的重要性。


<details>
  <summary>Details</summary>
Motivation: PEMEL作为绿色制氢关键技术，其可扩展性受稀缺材料铱限制，需评估未来铱需求。

Method: 对不同部署情景和技术进步下PEMEL的未来铱需求进行估算。

Result: 实现净零目标需提高催化剂效率和获取约30%全球铱产量，2030年可能出现供应短缺，2040年后铱长期需求被低估。

Conclusion: 迫切需要在材料效率和回收方面创新，将资源约束纳入能源政策和技术规划以确保氢的可持续转型。

Abstract: Proton exchange membrane electrolysis (PEMEL) is a key technology for
producing green hydrogen, but its scalability is limited by the use of scarce
materials, particularly iridium. Iridium oxide, the preferred anode catalyst in
PEMEL, offers exceptional stability but is produced only as a by-product of
platinum mining, with annual output around 7.5 tons. This study estimates
future iridium demand for PEMEL under various deployment scenarios and
technological advances. Results show that meeting net zero targets will require
both significant improvements in catalyst efficiency and access to roughly 30\%
of global iridium production annually. Supply shortages could arise as early as
2030, earlier than previously anticipated. The analysis also reveals that
long-term iridium needs beyond 2040 are significantly underestimated. These
findings underscore the urgent need for innovation in material efficiency and
recycling, and the importance of integrating resource constraints into energy
policy and technology planning to ensure a sustainable hydrogen transition.

</details>


### [405] [From Digital Distrust to Codified Honesty: Experimental Evidence on Generative AI in Credence Goods Markets](https://arxiv.org/abs/2509.06069)
*Alexander Erlei*

Main category: econ.GN

TL;DR: 本文通过实验研究大语言模型对不同专家市场的影响，发现人类 - 人类市场效率通常较高，但大语言模型专家盈余更高，人类 - 人类 - 人工智能市场在透明规则下表现更好，揭示了大语言模型在专家服务中的机遇与风险。


<details>
  <summary>Details</summary>
Motivation: 量化大语言模型对不同类型专家市场的行为、福利和分配后果。

Method: 采用一系列一次性实验，运用信任品框架。

Result: 人类 - 人类市场效率通常高于人工智能 - 人工智能和人类 - 人工智能市场；大语言模型专家盈余高于人类专家；人类 - 人类 - 人工智能市场在透明规则下优于人类 - 人类市场，不透明时效率增益消失。

Conclusion: 大语言模型在专家服务中有潜在机遇和风险，带来监管挑战，既能减少信息不对称提高效率，也会影响人类信任和专家利他偏好。

Abstract: Generative AI is transforming the provision of expert services. This article
uses a series of one-shot experiments to quantify the behavioral, welfare and
distribution consequences of large language models (LLMs) on AI-AI,
Human-Human, Human-AI and Human-AI-Human expert markets. Using a credence goods
framework where experts have private information about the optimal service for
consumers, we find that Human-Human markets generally achieve higher levels of
efficiency than AI-AI and Human-AI markets through pro-social expert
preferences and higher consumer trust. Notably, LLM experts still earn
substantially higher surplus than human experts -- at the expense of consumer
surplus - suggesting adverse incentives that may spur the harmful deployment of
LLMs. Concurrently, a majority of human experts chooses to rely on LLM agents
when given the opportunity in Human-AI-Human markets, especially if they have
agency over the LLM's (social) objective function. Here, a large share of
experts prioritizes efficiency-loving preferences over pure self-interest.
Disclosing these preferences to consumers induces strong efficiency gains by
marginalizing self-interested LLM experts and human experts. Consequently,
Human-AI-Human markets outperform Human-Human markets under transparency rules.
With obfuscation, however, efficiency gains disappear, and adverse expert
incentives remain. Our results shed light on the potential opportunities and
risks of disseminating LLMs in the context of expert services and raise several
regulatory challenges. On the one hand, LLMs can negatively affect human trust
in the presence of information asymmetries and partially crowd-out experts'
other-regarding preferences through automation. On the other hand, LLMs allow
experts to codify and communicate their objective function, which reduces
information asymmetries and increases efficiency.

</details>


### [406] [DETERring more than Deforestation: Environmental Enforcement Reduces Violence in the Amazon](https://arxiv.org/abs/2509.06076)
*Rafael Araujo,Vitor Possebom,Gabriela Setti*

Main category: econ.GN

TL;DR: 本文评估巴西亚马逊地区环境执法对暴力事件的影响，发现实时森林砍伐检测系统减少了该地区的凶杀案。


<details>
  <summary>Details</summary>
Motivation: 探究环境执法对巴西亚马逊地区暴力事件的影响。

Method: 利用云层覆盖导致的卫星监测外生变化作为执法强度的工具变量来识别因果效应。

Result: 实时森林砍伐检测系统（DETER）的引入显著减少了该地区的凶杀案，每年约减少1477起，降幅达15%。

Conclusion: 遏制森林砍伐会产生重要的社会协同效益，能加强国家管控并减少制度脆弱和资源冲突地区的暴力事件。

Abstract: We estimate the impact of environmental law enforcement on violence in the
Brazilian Amazon. The introduction of the Real-Time Deforestation Detection
System (DETER), which enabled the government to monitor deforestation in real
time and issue fines for illegal clearing, significantly reduced homicides in
the region. To identify causal effects, we exploit exogenous variation in
satellite monitoring generated by cloud cover as an instrument for enforcement
intensity. Our estimates imply that the expansion of state presence through
DETER prevented approximately 1,477 homicides per year, a 15% reduction in
homicides. These results show that curbing deforestation produces important
social co-benefits, strengthening state presence and reducing violence in
regions marked by institutional fragility and resource conflict.

</details>


### [407] [The Probability of Food Security: A new longitudinal data set using the Panel Study of Income Dynamics](https://arxiv.org/abs/2509.06144)
*Seungmin Lee,John Hoddinott,Christopher B. Barrett,Matthew P. Rabbitt*

Main category: econ.GN

TL;DR: 本文应用新的家庭层面粮食安全衡量指标处理PSID数据，构建长时段粮食安全面板数据并估计关键层面粮食安全动态。


<details>
  <summary>Details</summary>
Motivation: 美国粮食安全动态研究受限于缺乏对同一家庭或个人的长期纵向观察。

Method: 应用新的家庭层面粮食安全衡量指标PFS处理1979 - 2019年26波PSID数据。

Result: 构建了前所未有的长时段粮食安全面板数据系列并公开。

Conclusion: 可估计40年间关键亚人群和国家层面粮食安全动态，包括基于多因素的细分动态。

Abstract: The study of food security dynamics in the U.S. has long been impeded by the
lack of extended longitudinal observations of the same households or
individuals. This paper applies a newly-introduced household-level food
security measure, the probability of food security (PFS), to 26 waves of Panel
Study of Income Dynamics (PSID) data, spanning 1979-2019, to generate a data
product we describe and make newly available to the research community. We
detail the construction of this unprecedentedly long food security panel data
series in PSID data. Finally, we estimate key subpopulation- and national-level
food security dynamics identifiable over the 40-year (1979-2019) period
spanning multiple recessions and federal nutrition assistance policy changes,
including disaggregated dynamics based on geography, race, sex, and educational
attainment.

</details>


### [408] [Are international happiness rankings reliable?](https://arxiv.org/abs/2509.06867)
*Christopher P Barrington-Leigh*

Main category: econ.GN

TL;DR: 研究指出常用生活满意度调查问题格式缺乏跨国可比性，强调需开展新研究议程。


<details>
  <summary>Details</summary>
Motivation: 常用生活满意度和Cantril阶梯问题用于全球幸福排名及政策制定，但当代全球数据未证实其跨国可比性。

Method: 使用盖洛普世界民意调查、全球繁荣研究和世界价值观调查，对比两种问题格式的分布、排名和响应模式；分析全球繁荣研究回归系数。

Result: 两种问题格式在不同国家和调查中的分布、排名和响应模式不同，国际幸福排名不稳定，但生活评估决定因素的科学研究较稳健。

Conclusion: 强调需要针对幸福跨国可比性的关键局限开展新的研究议程。

Abstract: Global comparisons of wellbeing increasingly rely on survey questions that
ask respondents to evaluate their lives, most commonly in the form of "life
satisfaction" and "Cantril ladder" items. These measures underpin international
rankings such as the World Happiness Report and inform policy initiatives
worldwide, yet their comparability has not been established with contemporary
global data. Using the Gallup World Poll, Global Flourishing Study, and World
Values Survey, I show that the two question formats yield divergent
distributions, rankings, and response patterns that vary across countries and
surveys, defying simple explanations. To explore differences in respondents'
cognitive interpretations, I compare regression coefficients from the Global
Flourishing Study, analyzing how each question wording relates to life
circumstances. While international rankings of wellbeing are unstable, the
scientific study of the determinants of life evaluations appears more robust.
Together, the findings underscore the need for a renewed research agenda on
critical limitations to cross-country comparability of wellbeing.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [409] [Approximating Condorcet Ordering for Vector-valued Mathematical Morphology](https://arxiv.org/abs/2509.06577)
*Marcos Eduardo Valle,Santiago Velasco-Forero,Joao Batista Florindo,Gustavo Jesus Angulo*

Main category: cs.CV

TL;DR: 本文探讨矢量值图像形态学算子合适矢量排序问题，开发机器学习方法学习近似Condorcet排序的约简排序，实验证实其对彩色图像的有效性。


<details>
  <summary>Details</summary>
Motivation: 数学形态学应用于矢量值图像时，缺乏构建形态学算子的合适矢量排序。

Method: 开发机器学习方法，学习近似Condorcet排序的约简排序。

Result: 初步计算实验证实学习约简映射定义彩色图像矢量值形态学算子有效。

Conclusion: 所提出的学习约简排序的方法可用于定义矢量值形态学算子。

Abstract: Mathematical morphology provides a nonlinear framework for image and spatial
data processing and analysis. Although there have been many successful
applications of mathematical morphology to vector-valued images, such as color
and hyperspectral images, there is still no consensus on the most suitable
vector ordering for constructing morphological operators. This paper addresses
this issue by examining a reduced ordering approximating the Condorcet ranking
derived from a set of vector orderings. Inspired by voting problems, the
Condorcet ordering ranks elements from most to least voted, with voters
representing different orderings. In this paper, we develop a machine learning
approach that learns a reduced ordering that approximates the Condorcet
ordering. Preliminary computational experiments confirm the effectiveness of
learning the reduced mapping to define vector-valued morphological operators
for color images.

</details>


### [410] [TinyDef-DETR:An Enhanced DETR Detector for UAV Power Line Defect Detection](https://arxiv.org/abs/2509.06035)
*Jiaming Cui*

Main category: cs.CV

TL;DR: 提出TinyDef - DETR框架用于无人机检测输电线路小缺陷，实验表明其在精度、召回率上有提升，有泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统探测器在复杂背景下检测输电线路小而模糊缺陷存在细节丢失、边界敏感度弱和全局与局部信息融合不足等问题。

Method: 提出TinyDef - DETR框架，引入无步长空间到深度模块、边缘增强卷积、跨阶段双域多尺度注意力模块和Focaler - Wise - SIoU回归损失。

Result: 在CSG - ADCD数据集上精度和召回率有显著提升，在小目标子集上效果明显，计算开销小；在VisDrone基准上验证了泛化能力。

Conclusion: 结合细节保留下采样、边缘敏感表示、双域注意力和难度自适应回归为电网无人机小缺陷检测提供了实用高效的解决方案。

Abstract: Automated inspection of transmission lines using UAVs is hindered by the
difficulty of detecting small and ambiguous defects against complex
backgrounds. Conventional detectors often suffer from detail loss due to
strided downsampling, weak boundary sensitivity in lightweight backbones, and
insufficient integration of global context with local cues. To address these
challenges, we propose TinyDef-DETR, a DETR-based framework designed for
small-defect detection. The method introduces a stride-free space-to-depth
module for lossless downsampling, an edge-enhanced convolution for
boundary-aware feature extraction, a cross-stage dual-domain multi-scale
attention module to jointly capture global and local information, and a
Focaler-Wise-SIoU regression loss to improve localization of small objects.
Experiments conducted on the CSG-ADCD dataset demonstrate that TinyDef-DETR
achieves substantial improvements in both precision and recall compared to
competitive baselines, with particularly notable gains on small-object subsets,
while incurring only modest computational overhead. Further validation on the
VisDrone benchmark confirms the generalization capability of the proposed
approach. Overall, the results indicate that integrating detail-preserving
downsampling, edge-sensitive representations, dual-domain attention, and
difficulty-adaptive regression provides a practical and efficient solution for
UAV-based small-defect inspection in power grids.

</details>


### [411] [RED: Robust Event-Guided Motion Deblurring with Modality-Specific Disentangled Representation](https://arxiv.org/abs/2509.05554)
*Yihong Leng,Siming Zheng,Jinwei Chen,Bo Li,Jiaojiao Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 提出具有模态特定解缠表示的RED网络用于运动去模糊，实验表明其在准确性和鲁棒性上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有事件引导去模糊方法忽视事件流固有不完整性，影响运动先验完整性和去模糊效果。

Method: 提出RPS策略增强网络鲁棒性；设计解缠全注意力机制建模不同相关性；构建两个交互模块增强图像运动敏感区域和注入语义信息。

Result: 在合成和真实数据集上的大量实验表明RED在准确性和鲁棒性上达到了当前最优性能。

Conclusion: RED网络有效解决现有方法问题，在事件引导去模糊任务中表现出色。

Abstract: Event cameras provide sparse yet temporally high-temporal-resolution motion
information, demonstrating great potential for motion deblurring. Existing
methods focus on cross-modal interaction, overlooking the inherent
incompleteness of event streams, which arises from the trade-off between
sensitivity and noise introduced by the thresholding mechanism of Dynamic
Vision Sensors (DVS). Such degradation compromises the integrity of motion
priors and limits the effectiveness of event-guided deblurring. To tackle these
challenges, we propose a Robust Event-guided Deblurring (RED) network with
modality-specific disentangled representation. First, we introduce a
Robustness-Oriented Perturbation Strategy (RPS) that applies random masking to
events, which exposes RED to incomplete patterns and then foster robustness
against various unknown scenario conditions.Next, a disentangled OmniAttention
is presented to explicitly model intra-motion, inter-motion, and cross-modality
correlations from two inherently distinct but complementary sources: blurry
images and partially disrupted events. Building on these reliable features, two
interactive modules are designed to enhance motion-sensitive areas in blurry
images and inject semantic context into incomplete event representations.
Extensive experiments on synthetic and real-world datasets demonstrate RED
consistently achieves state-of-the-art performance in both accuracy and
robustness.

</details>


### [412] [Knowledge-Augmented Vision Language Models for Underwater Bioacoustic Spectrogram Analysis](https://arxiv.org/abs/2509.05703)
*Ragib Amin Nihal,Benjamin Yen,Takeshi Ashizawa,Kazuhiro Nakadai*

Main category: cs.CV

TL;DR: 研究VLMs能否从海洋哺乳动物声谱图中提取有意义模式，通过整合VLM解读与LLM验证构建领域知识，无需手动标注或模型再训练。


<details>
  <summary>Details</summary>
Motivation: 海洋哺乳动物发声分析依赖解读生物声学频谱图，而VLMs未针对此类特定领域可视化进行训练，探究其能否从频谱图中视觉提取有意义模式。

Method: 将VLM解读与基于LLM的验证相结合来构建领域知识。

Result: 未提及具体结果

Conclusion: 该框架可在无需手动标注或模型再训练的情况下适应声学数据。

Abstract: Marine mammal vocalization analysis depends on interpreting bioacoustic
spectrograms. Vision Language Models (VLMs) are not trained on these
domain-specific visualizations. We investigate whether VLMs can extract
meaningful patterns from spectrograms visually. Our framework integrates VLM
interpretation with LLM-based validation to build domain knowledge. This
enables adaptation to acoustic data without manual annotation or model
retraining.

</details>


### [413] [AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor Long-Term Medication Adherence and Care](https://arxiv.org/abs/2505.00275)
*Md Asaduzzaman Jabin,Hanqi Jiang,Yiwei Li,Patrick Kaggwa,Eugene Douglass,Juliet N. Sekandi,Tianming Liu*

Main category: cs.CV

TL;DR: 提出AdCare - VLM模型用于药物依从性视觉问答，用定制数据集微调，结果优于部分VLM模型。


<details>
  <summary>Details</summary>
Motivation: 慢性病需严格药物依从性，但依从性常受多种因素影响，需有效方法监测。

Method: 提出AdCare - VLM模型，用定制结核药物监测视频数据集微调，构建LLM - TB - VQA数据集，识别视觉特征与医学概念关联。

Result: 方法在各配置下均优于PEFT启用的VLM模型，绝对提升3.1% - 3.54%，消融研究和注意力图可视化支持该方法。

Conclusion: AdCare - VLM模型在药物依从性视觉问答上表现良好，可提升多模态交互和解释性。

Abstract: Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS,
epilepsy, and tuberculosis, necessitate rigorous adherence to medication to
avert disease progression, manage symptoms, and decrease mortality rates.
Adherence is frequently undermined by factors including patient behavior,
caregiver support, elevated medical costs, and insufficient healthcare
infrastructure. We propose AdCare-VLM, a specialized Video-LLaVA-based
multimodal large vision language model (LVLM) aimed at visual question
answering (VQA) concerning medication adherence through patient videos. We
employ a private dataset comprising 806 custom-annotated tuberculosis (TB)
medication monitoring videos, which have been labeled by clinical experts, to
fine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a
detailed medical adherence VQA dataset that encompasses positive, negative, and
ambiguous adherence cases. Our method identifies correlations between visual
features, such as the clear visibility of the patient's face, medication, water
intake, and the act of ingestion, and their associated medical concepts in
captions. This facilitates the integration of aligned visual-linguistic
representations and improves multimodal interactions. Experimental results
indicate that our method surpasses parameter-efficient fine-tuning (PEFT)
enabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute
improvements ranging from 3.1% to 3.54% across pre-trained, regular, and
low-rank adaptation (LoRA) configurations. Comprehensive ablation studies and
attention map visualizations substantiate our approach, enhancing
interpretability.

</details>


### [414] [VILOD: A Visual Interactive Labeling Tool for Object Detection](https://arxiv.org/abs/2509.05317)
*Isac Holm*

Main category: cs.CV

TL;DR: 本文提出VILOD工具助力目标检测标注，通过可视化让标注流程更透明有效。


<details>
  <summary>Details</summary>
Motivation: 深度学习目标检测中获取标注数据集耗时昂贵，主动学习有局限性，需结合人在环和可视化分析方法。

Method: 开发VILOD工具，利用图像特征投影、不确定性热力图等组件，在人在环迭代工作流中让用户探索数据、实施样本选择策略。

Result: 实证研究表明VILOD使模型状态和数据集特征更易解释，不同可视化引导标注策略与自动不确定性采样基线有竞争力。

Conclusion: VILOD为目标检测标注的人在环-主动学习工作流带来新工具和经验见解，使其更透明、易管理且有效。

Abstract: The advancement of Object Detection (OD) using Deep Learning (DL) is often
hindered by the significant challenge of acquiring large, accurately labeled
datasets, a process that is time-consuming and expensive. While techniques like
Active Learning (AL) can reduce annotation effort by intelligently querying
informative samples, they often lack transparency, limit the strategic insight
of human experts, and may overlook informative samples not aligned with an
employed query strategy. To mitigate these issues, Human-in-the-Loop (HITL)
approaches integrating human intelligence and intuition throughout the machine
learning life-cycle have gained traction. Leveraging Visual Analytics (VA),
effective interfaces can be created to facilitate this human-AI collaboration.
This thesis explores the intersection of these fields by developing and
investigating "VILOD: A Visual Interactive Labeling tool for Object Detection".
VILOD utilizes components such as a t-SNE projection of image features,
together with uncertainty heatmaps and model state views. Enabling users to
explore data, interpret model states, AL suggestions, and implement diverse
sample selection strategies within an iterative HITL workflow for OD. An
empirical investigation using comparative use cases demonstrated how VILOD,
through its interactive visualizations, facilitates the implementation of
distinct labeling strategies by making the model's state and dataset
characteristics more interpretable (RQ1). The study showed that different
visually-guided labeling strategies employed within VILOD result in competitive
OD performance trajectories compared to an automated uncertainty sampling AL
baseline (RQ2). This work contributes a novel tool and empirical insight into
making the HITL-AL workflow for OD annotation more transparent, manageable, and
potentially more effective.

</details>


### [415] [A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD](https://arxiv.org/abs/2509.05321)
*Yunfei Guo,Tao Zhang,Wu Huang,Yao Song*

Main category: cs.CV

TL;DR: 本文介绍Video2EEG - SPGN - Diffusion框架，用SEED - VD数据集生成多模态EEG数据集，发布新数据集，有研究和工程意义。


<details>
  <summary>Details</summary>
Motivation: 推进视频与EEG数据对齐的多模态研究，为情感分析、数据增强和脑机接口应用提供工具。

Method: 使用SEED - VD数据集，提出视频和EEG数据对的工程对齐管道，用集成扩散模型的自博弈图网络（SPGN）生成个性化EEG信号。

Result: 发布包含超1000个样本的新数据集，有视频刺激、62通道EEG信号、情感标签。

Conclusion: 该框架为相关领域提供新工具，有重要研究和工程意义。

Abstract: This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion,
that leverages the SEED-VD dataset to generate a multimodal dataset of EEG
signals conditioned on video stimuli. Additionally, we disclose an engineering
pipeline for aligning video and EEG data pairs, facilitating the training of
multimodal large models with EEG alignment capabilities. Personalized EEG
signals are generated using a self-play graph network (SPGN) integrated with a
diffusion model. As a major contribution, we release a new dataset comprising
over 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG
signals at 200 Hz and emotion labels, enabling video-EEG alignment and
advancing multimodal research. This framework offers novel tools for emotion
analysis, data augmentation, and brain-computer interface applications, with
substantial research and engineering significance.

</details>


### [416] [Optical Music Recognition of Jazz Lead Sheets](https://arxiv.org/abs/2509.05329)
*Juan Carlos Martinez-Sevilla,Francesco Foscarin,Patricia Garcia-Iasci,David Rizo,Jorge Calvo-Zaragoza,Gerhard Widmer*

Main category: cs.CV

TL;DR: 提出新手写爵士乐谱数据集并开发对应OMR模型，公开代码、数据和模型


<details>
  <summary>Details</summary>
Motivation: 解决现有OMR系统无法处理手写爵士乐谱中和弦及手写图像高变异性和质量问题的挑战

Method: 构建包含293张手写爵士乐谱的数据集，提供合成乐谱图像，开发OMR模型并讨论特定标记选择及使用合成乐谱和预训练模型的优势

Result: 创建了新数据集，开发了OMR模型

Conclusion: 完成手写爵士乐谱的OMR任务，公开所有相关资源

Abstract: In this paper, we address the challenge of Optical Music Recognition (OMR)
for handwritten jazz lead sheets, a widely used musical score type that encodes
melody and chords. The task is challenging due to the presence of chords, a
score component not handled by existing OMR systems, and the high variability
and quality issues associated with handwritten images. Our contribution is
two-fold. We present a novel dataset consisting of 293 handwritten jazz lead
sheets of 163 unique pieces, amounting to 2021 total staves aligned with
Humdrum **kern and MusicXML ground truth scores. We also supply synthetic score
images generated from the ground truth. The second contribution is the
development of an OMR model for jazz lead sheets. We discuss specific
tokenisation choices related to our kind of data, and the advantages of using
synthetic scores and pretrained models. We publicly release all code, data, and
models.

</details>


### [417] [RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness](https://arxiv.org/abs/2509.05333)
*Junghyun Park,Tuan Anh Nguyen,Dugki Min*

Main category: cs.CV

TL;DR: 为缓解现代目标识别模型在域偏移下的精度下降，提出RT - VLM框架，经实验在鲁棒性基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现实世界部署中，现代目标识别模型在域偏移下精度严重下降，需缓解这种退化。

Method: 引入RT - VLM框架，构建独特合成数据集生成管道，对Llama 3.2 11B Vision Instruct进行参数高效监督微调，推理时执行两阶段重思考方案。

Result: 在隔离单个域偏移的鲁棒性基准测试中，RT - VLM始终超越强基线。

Conclusion: 将结构化多模态证据与显式自我批判循环相结合是实现可靠且可迁移视觉理解的有前景途径。

Abstract: Real world deployments often expose modern object recognition models to
domain shifts that precipitate a severe drop in accuracy. Such shifts encompass
(i) variations in low level image statistics, (ii) changes in object pose and
viewpoint, (iii) partial occlusion, and (iv) visual confusion across adjacent
classes. To mitigate this degradation, we introduce the Re-Thinking Vision
Language Model (RT-VLM) framework. The foundation of this framework is a unique
synthetic dataset generation pipeline that produces images annotated with
"4-Clues": precise bounding boxes, class names, detailed object-level captions,
and a comprehensive context-level caption for the entire scene. We then perform
parameter efficient supervised tuning of Llama 3.2 11B Vision Instruct on this
resource. At inference time, a two stage Re-Thinking scheme is executed: the
model first emits its own four clues, then re examines these responses as
evidence and iteratively corrects them. Across robustness benchmarks that
isolate individual domain shifts, RT-VLM consistently surpasses strong
baselines. These findings indicate that the integration of structured
multimodal evidence with an explicit self critique loop constitutes a promising
route toward reliable and transferable visual understanding.

</details>


### [418] [Comparative Evaluation of Hard and Soft Clustering for Precise Brain Tumor Segmentation in MR Imaging](https://arxiv.org/abs/2509.05340)
*Dibya Jyoti Bora,Mrinal Kanti Mishra*

Main category: cs.CV

TL;DR: 本文对比了K - Means和FCM两种聚类算法在脑肿瘤MRI分割中的表现，发现K - Means速度快但精度低，FCM精度高但计算成本高。


<details>
  <summary>Details</summary>
Motivation: 由于肿瘤形态和强度分布的异质性，脑肿瘤MRI分割仍是医学图像分析的关键挑战，准确划分肿瘤边界对临床决策等至关重要，因此对比两种聚类范式。

Method: 对K - Means（硬聚类）和FCM（软聚类）两种聚类范式进行全面对比分析，使用BraTS2020数据集，进行高斯滤波和CLAHE预处理，用DSC和处理时间作为评估指标。

Result: K - Means平均每张图像运行时间0.3s，速度快；FCM平均DSC为0.67，高于K - Means的0.43，分割精度高，但计算成本高，每张图像需1.3s。

Conclusion: 两种算法存在计算效率和边界精度的权衡。

Abstract: Segmentation of brain tumors from Magnetic Resonance Imaging (MRI) remains a
pivotal challenge in medical image analysis due to the heterogeneous nature of
tumor morphology and intensity distributions. Accurate delineation of tumor
boundaries is critical for clinical decision-making, radiotherapy planning, and
longitudinal disease monitoring. In this study, we perform a comprehensive
comparative analysis of two major clustering paradigms applied in MRI tumor
segmentation: hard clustering, exemplified by the K-Means algorithm, and soft
clustering, represented by Fuzzy C-Means (FCM). While K-Means assigns each
pixel strictly to a single cluster, FCM introduces partial memberships, meaning
each pixel can belong to multiple clusters with varying degrees of association.
Experimental validation was performed using the BraTS2020 dataset,
incorporating pre-processing through Gaussian filtering and Contrast Limited
Adaptive Histogram Equalization (CLAHE). Evaluation metrics included the Dice
Similarity Coefficient (DSC) and processing time, which collectively
demonstrated that K-Means achieved superior speed with an average runtime of
0.3s per image, whereas FCM attained higher segmentation accuracy with an
average DSC of 0.67 compared to 0.43 for K-Means, albeit at a higher
computational cost (1.3s per image). These results highlight the inherent
trade-off between computational efficiency and boundary precision.

</details>


### [419] [Unsupervised Instance Segmentation with Superpixels](https://arxiv.org/abs/2509.05352)
*Cuong Manh Hoang*

Main category: cs.CV

TL;DR: 提出无需人工标注的实例分割新框架，在公共数据集实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有流行实例分割模型依赖大量人工标注，标注成本高，因此需无人工标注的分割方法。

Method: 先用MultiCut算法对自监督特征进行粗掩码分割，再用掩码过滤器获取高质量粗掩码；计算新的超像素引导掩码损失训练分割网络；提出带新自适应损失的自训练过程提升预测掩码质量。

Result: 在实例分割和目标检测的公共数据集上实验，所提框架表现优于先前的先进方法。

Conclusion: 所提无需人工标注的实例分割框架有效。

Abstract: Instance segmentation is essential for numerous computer vision applications,
including robotics, human-computer interaction, and autonomous driving.
Currently, popular models bring impressive performance in instance segmentation
by training with a large number of human annotations, which are costly to
collect. For this reason, we present a new framework that efficiently and
effectively segments objects without the need for human annotations. Firstly, a
MultiCut algorithm is applied to self-supervised features for coarse mask
segmentation. Then, a mask filter is employed to obtain high-quality coarse
masks. To train the segmentation network, we compute a novel superpixel-guided
mask loss, comprising hard loss and soft loss, with high-quality coarse masks
and superpixels segmented from low-level image features. Lastly, a
self-training process with a new adaptive loss is proposed to improve the
quality of predicted masks. We conduct experiments on public datasets in
instance segmentation and object detection to demonstrate the effectiveness of
the proposed framework. The results show that the proposed framework
outperforms previous state-of-the-art methods.

</details>


### [420] [Augmented Structure Preserving Neural Networks for cell biomechanics](https://arxiv.org/abs/2509.05388)
*Juan Olalla-Pombo,Alberto Badías,Miguel Ángel Sanz-Gómez,José María Benítez,Francisco Javier Montáns*

Main category: cs.CV

TL;DR: 提出结合结构保留神经网络与机器学习工具的新方法，用于预测细胞轨迹和有丝分裂事件，经测试有较高准确性。


<details>
  <summary>Details</summary>
Motivation: 细胞生物力学现象重要，但现象间相互作用及对细胞集体决策的影响尚不明确，需深入研究。

Method: 结合结构保留神经网络（将细胞运动视为纯机械系统研究）与其他机器学习工具（人工神经网络），利用计算机视觉技术考虑环境因素；还构建基于神经网络架构的有丝分裂事件预测模型。

Result: 新模型在模拟和真实细胞迁移案例中，按滚动策略高精度预测完整细胞轨迹。

Conclusion: 所提出的新方法能有效预测细胞轨迹和有丝分裂事件。

Abstract: Cell biomechanics involve a great number of complex phenomena that are
fundamental to the evolution of life itself and other associated processes,
ranging from the very early stages of embryo-genesis to the maintenance of
damaged structures or the growth of tumors. Given the importance of such
phenomena, increasing research has been dedicated to their understanding, but
the many interactions between them and their influence on the decisions of
cells as a collective network or cluster remain unclear. We present a new
approach that combines Structure Preserving Neural Networks, which study cell
movements as a purely mechanical system, with other Machine Learning tools
(Artificial Neural Networks), which allow taking into consideration
environmental factors that can be directly deduced from an experiment with
Computer Vision techniques. This new model, tested on simulated and real cell
migration cases, predicts complete cell trajectories following a roll-out
policy with a high level of accuracy. This work also includes a mitosis event
prediction model based on Neural Networks architectures which makes use of the
same observed features.

</details>


### [421] [Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale Convolutional Attention Decoding](https://arxiv.org/abs/2509.05431)
*GodsGift Uzor,Tania-Amanda Nkoyo Fredrick Eneye,Chukwuebuka Ijezue*

Main category: cs.CV

TL;DR: 本文提出EMCAD解码器用于脑肿瘤分割，在BraTs2020数据集上测试，模型初步结果中等且未过拟合。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤分割中高效解码机制很关键，但现有机制计算成本高，需优化性能和计算效率。

Method: 设计了新的高效多尺度卷积注意力解码器EMCAD，在BraTs2020数据集上进行脑肿瘤分割。

Result: 模型初步结果最佳Dice分数为0.31，训练中平均Dice分数稳定在0.285±0.015。

Conclusion: 初始模型在验证集上性能稳定，未出现过拟合现象。

Abstract: Brain tumor segmentation is a critical pre-processing step in the medical
image analysis pipeline that involves precise delineation of tumor regions from
healthy brain tissue in medical imaging data, particularly MRI scans. An
efficient and effective decoding mechanism is crucial in brain tumor
segmentation especially in scenarios with limited computational resources.
However these decoding mechanisms usually come with high computational costs.
To address this concern EMCAD a new efficient multi-scale convolutional
attention decoder designed was utilized to optimize both performance and
computational efficiency for brain tumor segmentation on the BraTs2020 dataset
consisting of MRI scans from 369 brain tumor patients. The preliminary result
obtained by the model achieved a best Dice score of 0.31 and maintained a
stable mean Dice score of 0.285 plus/minus 0.015 throughout the training
process which is moderate. The initial model maintained consistent performance
across the validation set without showing signs of over-fitting.

</details>


### [422] [An Analysis of Layer-Freezing Strategies for Enhanced Transfer Learning in YOLO Architectures](https://arxiv.org/abs/2509.05490)
*Andrzej D. Dobrzycki,Ana M. Bernardos,José R. Casar*

Main category: cs.CV

TL;DR: 研究分析YOLOv8和YOLOv10架构的层冻结策略，发现无通用最优策略，依赖数据属性，能减少GPU内存消耗，提供选择策略的指南。


<details>
  <summary>Details</summary>
Motivation: YOLO架构在资源受限环境部署需高效迁移学习，现有研究未探索不同冻结配置对YOLOv8和YOLOv10的影响。

Method: 使用四个挑战性数据集，对YOLOv8和YOLOv10的多种冻结配置进行系统研究，结合梯度行为分析和可视化解释。

Result: 无通用最优冻结策略，依赖数据属性，部分配置减少GPU内存消耗，部分mAP@50得分超全微调，梯度分析有不同收敛模式。

Conclusion: 为选择冻结策略提供实证结果和实用指南，为资源受限场景目标检测的平衡迁移学习提供方法。

Abstract: The You Only Look Once (YOLO) architecture is crucial for real-time object
detection. However, deploying it in resource-constrained environments such as
unmanned aerial vehicles (UAVs) requires efficient transfer learning. Although
layer freezing is a common technique, the specific impact of various freezing
configurations on contemporary YOLOv8 and YOLOv10 architectures remains
unexplored, particularly with regard to the interplay between freezing depth,
dataset characteristics, and training dynamics. This research addresses this
gap by presenting a detailed analysis of layer-freezing strategies. We
systematically investigate multiple freezing configurations across YOLOv8 and
YOLOv10 variants using four challenging datasets that represent critical
infrastructure monitoring. Our methodology integrates a gradient behavior
analysis (L2 norm) and visual explanations (Grad-CAM) to provide deeper
insights into training dynamics under different freezing strategies. Our
results reveal that there is no universal optimal freezing strategy but,
rather, one that depends on the properties of the data. For example, freezing
the backbone is effective for preserving general-purpose features, while a
shallower freeze is better suited to handling extreme class imbalance. These
configurations reduce graphics processing unit (GPU) memory consumption by up
to 28% compared to full fine-tuning and, in some cases, achieve mean average
precision (mAP@50) scores that surpass those of full fine-tuning. Gradient
analysis corroborates these findings, showing distinct convergence patterns for
moderately frozen models. Ultimately, this work provides empirical findings and
practical guidelines for selecting freezing strategies. It offers a practical,
evidence-based approach to balanced transfer learning for object detection in
scenarios with limited resources.

</details>


### [423] [OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation](https://arxiv.org/abs/2509.05513)
*Ahad Jawaid,Yu Xiang*

Main category: cs.CV

TL;DR: 介绍多模态以自我为中心的操作数据集OpenEgo，含标准手部姿势注释和与意图对齐的动作原语，可降低从以自我为中心视频学习灵巧操作的门槛。


<details>
  <summary>Details</summary>
Motivation: 现有以自我为中心的人类视频语料库缺乏细粒度、时间定位的动作描述或灵巧手部注释，需要新数据集。

Method: 引入OpenEgo数据集，统一手部姿势布局，提供带时间戳的动作原语，训练语言条件模仿学习策略。

Result: OpenEgo涵盖六个公共数据集共1107小时，290个操作任务，600多个环境。

Conclusion: OpenEgo可降低从以自我为中心视频学习灵巧操作的障碍，支持视觉 - 语言 - 动作学习的可重复性研究。

Abstract: Egocentric human videos provide scalable demonstrations for imitation
learning, but existing corpora often lack either fine-grained, temporally
localized action descriptions or dexterous hand annotations. We introduce
OpenEgo, a multimodal egocentric manipulation dataset with standardized
hand-pose annotations and intention-aligned action primitives. OpenEgo totals
1107 hours across six public datasets, covering 290 manipulation tasks in 600+
environments. We unify hand-pose layouts and provide descriptive, timestamped
action primitives. To validate its utility, we train language-conditioned
imitation-learning policies to predict dexterous hand trajectories. OpenEgo is
designed to lower the barrier to learning dexterous manipulation from
egocentric video and to support reproducible research in vision-language-action
learning. All resources and instructions will be released at
www.openegocentric.com.

</details>


### [424] [Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization](https://arxiv.org/abs/2509.05604)
*Jungin Park,Jiyoung Lee,Kwanghoon Sohn*

Main category: cs.CV

TL;DR: 本文将视频摘要视为语言引导的时空图建模问题，提出递归时空图网络VideoGraph，在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 以往方法侧重于帧间全局关联，未充分考虑细粒度视觉实体，且语言引导的视频摘要需综合语言理解，本文旨在考虑对象间语义关系进行视频摘要。

Method: 提出递归时空图网络VideoGraph，将对象和帧分别作为空间和时间图的节点，用图边连接和聚合节点，融入视频语言查询到节点表示中，并采用递归策略细化初始图。

Result: VideoGraph在多个基准测试的通用和聚焦查询视频摘要任务中，以有监督和无监督方式达到了最先进的性能。

Conclusion: VideoGraph在视频摘要任务中表现出色，代码已开源。

Abstract: Video summarization aims to select keyframes that are visually diverse and
can represent the whole story of a given video. Previous approaches have
focused on global interlinkability between frames in a video by temporal
modeling. However, fine-grained visual entities, such as objects, are also
highly related to the main content of the video. Moreover, language-guided
video summarization, which has recently been studied, requires a comprehensive
linguistic understanding of complex real-world videos. To consider how all the
objects are semantically related to each other, this paper regards video
summarization as a language-guided spatiotemporal graph modeling problem. We
present recursive spatiotemporal graph networks, called VideoGraph, which
formulate the objects and frames as nodes of the spatial and temporal graphs,
respectively. The nodes in each graph are connected and aggregated with graph
edges, representing the semantic relationships between the nodes. To prevent
the edges from being configured with visual similarity, we incorporate language
queries derived from the video into the graph node representations, enabling
them to contain semantic knowledge. In addition, we adopt a recursive strategy
to refine initial graphs and correctly classify each frame node as a keyframe.
In our experiments, VideoGraph achieves state-of-the-art performance on several
benchmarks for generic and query-focused video summarization in both supervised
and unsupervised manners. The code is available at
https://github.com/park-jungin/videograph.

</details>


### [425] [SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning](https://arxiv.org/abs/2509.05614)
*Hanzhen Wang,Jiaming Xu,Jiayi Pan,Yongkang Zhou,Guohao Dai*

Main category: cs.CV

TL;DR: 提出SpecPrune - VLA方法用于VLA模型剪枝，在不损失太多成功率的情况下提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型剪枝方法仅用当前动作的局部信息，忽略先前动作的全局上下文，导致成功率大幅下降和加速效果有限。

Method: 提出SpecPrune - VLA，包含动作级静态剪枝、层级动态剪枝和轻量级动作感知控制器，结合局部和全局信息进行智能令牌选择。

Result: 在LIBERO上实验表明，SpecPrune - VLA在NVIDIA A800和NVIDIA GeForce RTX 3090上分别实现1.46倍和1.57倍加速，且成功率损失可忽略不计。

Conclusion: SpecPrune - VLA能有效加速VLA模型，同时保持较高的成功率。

Abstract: Pruning accelerates compute-bound models by reducing computation. Recently
applied to Vision-Language-Action (VLA) models, existing methods prune tokens
using only local info from current action, ignoring global context from prior
actions, causing >20% success rate drop and limited speedup. We observe high
similarity across consecutive actions and propose leveraging both local
(current) and global (past) info for smarter token selection. We introduce
SpecPrune-VLA, a training-free method with two-level pruning and heuristic
control: (1) Static pruning at action level: uses global history and local
context to reduce visual tokens per action; (2) Dynamic pruning at layer level:
prunes tokens per layer based on layer-specific importance; (3) Lightweight
action-aware controller: classifies actions as coarse/fine-grained (by speed),
adjusting pruning aggressiveness since fine-grained actions are
pruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times
speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs.
OpenVLA-OFT, with negligible success rate loss.

</details>


### [426] [Application of discrete Ricci curvature in pruning randomly wired neural networks: A case study with chest x-ray classification of COVID-19](https://arxiv.org/abs/2509.05322)
*Pavithra Elumalai,Sudharsan Vijayaraghavan,Madhumita Mondal,Areejit Samal*

Main category: cs.CV

TL;DR: 研究用三种边缘中心网络测度压缩随机连接神经网络（RWNNs），比较其剪枝性能，发现基于FRC的剪枝可有效简化RWNNs并具计算优势。


<details>
  <summary>Details</summary>
Motivation: 研究网络拓扑对深度学习的影响，探索边缘中心网络测度用于RWNNs剪枝和优化，降低网络复杂度并保持性能。

Method: 研究三种边缘中心网络测度（FRC、ORC、EBC），在三种网络生成器上剪枝RWNNs，进行比较分析。

Result: FRC剪枝能有效简化RWNNs，在保持与ORC相当性能的同时，有显著计算优势。

Conclusion: FRC可用于RWNNs剪枝，在计算效率和剪枝效果间取得较好平衡。

Abstract: Randomly Wired Neural Networks (RWNNs) serve as a valuable testbed for
investigating the impact of network topology in deep learning by capturing how
different connectivity patterns impact both learning efficiency and model
performance. At the same time, they provide a natural framework for exploring
edge-centric network measures as tools for pruning and optimization. In this
study, we investigate three edge-centric network measures: Forman-Ricci
curvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness
centrality (EBC), to compress RWNNs by selectively retaining important synapses
(or edges) while pruning the rest. As a baseline, RWNNs are trained for
COVID-19 chest x-ray image classification, aiming to reduce network complexity
while preserving performance in terms of accuracy, specificity, and
sensitivity. We extend prior work on pruning RWNN using ORC by incorporating
two additional edge-centric measures, FRC and EBC, across three network
generators: Erd\"{o}s-R\'{e}nyi (ER) model, Watts-Strogatz (WS) model, and
Barab\'{a}si-Albert (BA) model. We provide a comparative analysis of the
pruning performance of the three measures in terms of compression ratio and
theoretical speedup. A central focus of our study is to evaluate whether FRC,
which is computationally more efficient than ORC, can achieve comparable
pruning effectiveness. Along with performance evaluation, we further
investigate the structural properties of the pruned networks through modularity
and global efficiency, offering insights into the trade-off between modular
segregation and network efficiency in compressed RWNNs. Our results provide
initial evidence that FRC-based pruning can effectively simplify RWNNs,
offering significant computational advantages while maintaining performance
comparable to ORC.

</details>


### [427] [Handling imbalance and few-sample size in ML based Onion disease classification](https://arxiv.org/abs/2509.05341)
*Abhijeet Manoj Pal,Rajbabu Velmurugan*

Main category: cs.CV

TL;DR: 提出基于深度学习的洋葱作物病虫害多分类模型，有良好表现。


<details>
  <summary>Details</summary>
Motivation: 现有病虫害分类方法多为二分类，限制实际应用，需准确识别具体病虫害类型。

Method: 增强预训练CNN模型，集成基于注意力的模块，采用综合数据增强管道缓解类别不平衡。

Result: 模型在真实田间图像数据集上总体准确率达96.90%，F1分数为0.96，优于其他使用相同数据集的方法。

Conclusion: 所提模型能有效进行洋葱作物病虫害多分类。

Abstract: Accurate classification of pests and diseases plays a vital role in precision
agriculture, enabling efficient identification, targeted interventions, and
preventing their further spread. However, current methods primarily focus on
binary classification, which limits their practical applications, especially in
scenarios where accurately identifying the specific type of disease or pest is
essential. We propose a robust deep learning based model for multi-class
classification of onion crop diseases and pests. We enhance a pre-trained
Convolutional Neural Network (CNN) model by integrating attention based modules
and employing comprehensive data augmentation pipeline to mitigate class
imbalance. We propose a model which gives 96.90% overall accuracy and 0.96 F1
score on real-world field image dataset. This model gives better results than
other approaches using the same datasets.

</details>


### [428] [Self-supervised Learning for Hyperspectral Images of Trees](https://arxiv.org/abs/2509.05630)
*Moqsadur Rahman,Saurav Kumar,Santosh S. Palmate,M. Shahriar Hossain*

Main category: cs.CV

TL;DR: 本文聚焦自监督学习，用作物田高空高光谱图像创建反映树木植被特性的神经网络嵌入，实验表明构造的树表示在下游任务表现更好。


<details>
  <summary>Details</summary>
Motivation: 分析有限或无标签的高光谱图像具有挑战性，需有效方法处理高空高光谱图像以助力精准农业。

Method: 采用自监督学习方法，从作物田高空高光谱图像创建反映树木植被特性的神经网络嵌入。

Result: 使用与植被特性相关的嵌入空间构建的树表示，在下游机器学习任务中比直接使用高光谱植被特性作为树表示表现更好。

Conclusion: 基于自监督学习构建的树表示方法在处理高空高光谱图像的下游任务中更有效。

Abstract: Aerial remote sensing using multispectral and RGB imagers has provided a
critical impetus to precision agriculture. Analysis of the hyperspectral images
with limited or no labels is challenging. This paper focuses on self-supervised
learning to create neural network embeddings reflecting vegetation properties
of trees from aerial hyperspectral images of crop fields. Experimental results
demonstrate that a constructed tree representation, using a vegetation
property-related embedding space, performs better in downstream machine
learning tasks compared to the direct use of hyperspectral vegetation
properties as tree representations.

</details>


### [429] [Delta Velocity Rectified Flow for Text-to-Image Editing](https://arxiv.org/abs/2509.05342)
*Gaspard Beaudouin,Minghan Li,Jaeyeon Kim,Sunghoon Yoon,Mengyu Wang*

Main category: cs.CV

TL;DR: 提出Delta Velocity Rectified Flow (DVRF)框架用于文本到图像编辑，实验显示其效果好且适用广泛。


<details>
  <summary>Details</summary>
Motivation: 解决先前蒸馏采样方法中普遍存在的过度平滑伪影问题。

Method: 提出基于蒸馏的DVRF方法，显式建模源和目标速度场之间的差异，引入时间相关的偏移项。

Result: 实验表明DVRF在编辑质量、保真度和可控性方面表现出色，无需架构修改。

Conclusion: DVRF是高效且广泛适用于文本到图像编辑任务的方法。

Abstract: We propose Delta Velocity Rectified Flow (DVRF), a novel inversion-free,
path-aware editing framework within rectified flow models for text-to-image
editing. DVRF is a distillation-based method that explicitly models the
discrepancy between the source and target velocity fields in order to mitigate
over-smoothing artifacts rampant in prior distillation sampling approaches. We
further introduce a time-dependent shift term to push noisy latents closer to
the target trajectory, enhancing the alignment with the target distribution. We
theoretically demonstrate that when this shift is disabled, DVRF reduces to
Delta Denoising Score, thereby bridging score-based diffusion optimization and
velocity-based rectified-flow optimization. Moreover, when the shift term
follows a linear schedule under rectified-flow dynamics, DVRF generalizes the
Inversion-free method FlowEdit and provides a principled theoretical
interpretation for it. Experimental results indicate that DVRF achieves
superior editing quality, fidelity, and controllability while requiring no
architectural modifications, making it efficient and broadly applicable to
text-to-image editing tasks. Code is available at
https://github.com/gaspardbd/DeltaVelocityRectifiedFlow.

</details>


### [430] [LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction](https://arxiv.org/abs/2509.05728)
*Niels Balemans,Ali Anwar,Jan Steckel,Siegfried Mercelis*

Main category: cs.CV

TL;DR: 本文为LiDAR - BIND框架添加时间一致性机制，提出三项贡献，更新模型架构，评估显示提升了时空连贯性，提出新指标，新框架增强了时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 为模块化多模态融合框架LiDAR - BIND添加明确的时间一致性机制，提升其性能。

Method: 引入时间嵌入相似性、运动对齐变换损失、窗口时间融合，更新模型架构以保留空间结构，提出基于FVMD和相关峰值距离的指标。

Result: 雷达/声纳到激光雷达转换的评估显示，提升了时空连贯性，降低了绝对轨迹误差，提高了基于Cartographer的SLAM的占用图准确性。

Conclusion: 提出的LiDAR - BIND - T框架保持了即插即用的模态融合，大幅增强了时间稳定性，提高了下游SLAM的鲁棒性和性能。

Abstract: This paper extends LiDAR-BIND, a modular multi-modal fusion framework that
binds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space,
with mechanisms that explicitly enforce temporal consistency. We introduce
three contributions: (i) temporal embedding similarity that aligns consecutive
latents, (ii) a motion-aligned transformation loss that matches displacement
between predictions and ground truth LiDAR, and (iii) windows temporal fusion
using a specialised temporal module. We further update the model architecture
to better preserve spatial structure. Evaluations on radar/sonar-to-LiDAR
translation demonstrate improved temporal and spatial coherence, yielding lower
absolute trajectory error and better occupancy map accuracy in
Cartographer-based SLAM (Simultaneous Localisation and Mapping). We propose
different metrics based on the Fr\'echet Video Motion Distance (FVMD) and a
correlation-peak distance metric providing practical temporal quality
indicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or
LiDAR-BIND-T, maintains plug-and-play modality fusion while substantially
enhancing temporal stability, resulting in improved robustness and performance
for downstream SLAM.

</details>


### [431] [FAVAE-Effective Frequency Aware Latent Tokenizer](https://arxiv.org/abs/2509.05441)
*Tejaswini Medi,Hsien-Yi Wang,Arianna Rampini,Margret Keuper*

Main category: cs.CV

TL;DR: 现有潜在生成模型图像合成时重建图像缺乏真实感，论文分析原因后提出FA - VAE框架，改善纹理重建并保留全局结构。


<details>
  <summary>Details</summary>
Motivation: 现有潜在生成模型重建图像在纹理区域缺乏真实感，细粒度细节丢失，需解决高频保真问题。

Method: 对现有SOTA潜在分词器进行详细频率分解，提出基于小波的频率感知变分自编码器（FA - VAE）框架，解耦低频和高频分量的优化。

Result: 所提框架能改善精细纹理的重建，同时保留全局结构。

Conclusion: 所提方法缩小了当前潜在分词器的保真度差距，强调频率感知优化对逼真图像表示的重要性，对多个应用领域有广泛影响。

Abstract: Latent generative models have shown remarkable progress in high-fidelity
image synthesis, typically using a two-stage training process that involves
compressing images into latent embeddings via learned tokenizers in the first
stage. The quality of generation strongly depends on how expressive and
well-optimized these latent embeddings are. While various methods have been
proposed to learn effective latent representations, the reconstructed images
often lack realism, particularly in textured regions with sharp transitions,
due to loss of fine details governed by high frequencies. We conduct a detailed
frequency decomposition of existing state-of-the-art (SOTA) latent tokenizers
and show that conventional objectives inherently prioritize low-frequency
reconstruction, often at the expense of high-frequency fidelity. Our analysis
reveals these latent tokenizers exhibit a bias toward low-frequency
information, when jointly optimized, leading to over-smoothed outputs and
visual artifacts that diminish perceptual quality. To address this, we propose
a wavelet-based, frequency-aware variational autoencoder (FA-VAE) framework
that explicitly decouples the optimization of low- and high-frequency
components. This decoupling enables improved reconstruction of fine textures
while preserving global structure. Our approach bridges the fidelity gap in
current latent tokenizers and emphasizes the importance of frequency-aware
optimization for realistic image representation, with broader implications for
applications in content creation, neural rendering, and medical imaging.

</details>


### [432] [InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios](https://arxiv.org/abs/2509.05747)
*Leo Ho,Yinghao Huang,Dafei Qin,Mingyi Shi,Wangpok Tse,Wei Liu,Junichi Yamagishi,Taku Komura*

Main category: cs.CV

TL;DR: 本文解决日常场景两人交互行为准确捕捉问题，采集新数据集InterAct，提出基于扩散的方法估计两人交互表情和动作，并公开数据代码。


<details>
  <summary>Details</summary>
Motivation: 以往工作多只考虑单人或仅关注对话手势，且假设参与者身体朝向和位置不变，本文要解决日常场景两人动态、语义一致的交互行为捕捉问题。

Method: 采集包含241个运动序列的多模态数据集InterAct，提出基于扩散的方法从语音输入估计两人交互表情和动作，采用分层方式回归身体动作，提出微调机制提高面部表情唇部准确性。

Result: InterAct数据集包含多样复杂个体动作和长期交互模式，方法能有效估计两人交互表情和动作。

Conclusion: 公开数据和代码，便于后续研究。

Abstract: We address the problem of accurate capture of interactive behaviors between
two people in daily scenarios. Most previous works either only consider one
person or solely focus on conversational gestures of two people, assuming the
body orientation and/or position of each actor are constant or barely change
over each interaction. In contrast, we propose to simultaneously model two
people's activities, and target objective-driven, dynamic, and semantically
consistent interactions which often span longer duration and cover bigger
space. To this end, we capture a new multi-modal dataset dubbed InterAct, which
is composed of 241 motion sequences where two people perform a realistic and
coherent scenario for one minute or longer over a complete interaction. For
each sequence, two actors are assigned different roles and emotion labels, and
collaborate to finish one task or conduct a common interaction activity. The
audios, body motions, and facial expressions of both persons are captured.
InterAct contains diverse and complex motions of individuals and interesting
and relatively long-term interaction patterns barely seen before. We also
demonstrate a simple yet effective diffusion-based method that estimates
interactive face expressions and body motions of two people from speech inputs.
Our method regresses the body motions in a hierarchical manner, and we also
propose a novel fine-tuning mechanism to improve the lip accuracy of facial
expressions. To facilitate further research, the data and code is made
available at https://hku-cg.github.io/interact/ .

</details>


### [433] [Unleashing Hierarchical Reasoning: An LLM-Driven Framework for Training-Free Referring Video Object Segmentation](https://arxiv.org/abs/2509.05751)
*Bingrui Zhao,Lin Yuanbo Wu,Xiangtian Fan,Deyin Liu,Lu Zhang,Ruyi He,Jialie Shen,Ximing Li*

Main category: cs.CV

TL;DR: 提出无需训练的PARSE - VOS框架用于RVOS任务，在三个基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前RVOS方法在处理复杂组合描述时，难以将静态文本与动态视觉内容对齐。

Method: 将自然语言查询解析为语义命令，引入时空定位模块生成候选轨迹，通过分层识别模块经粗粒度运动推理和细粒度姿态验证选择目标，输出分割掩码。

Result: PARSE - VOS在Ref - YouTube - VOS、Ref - DAVIS17和MeViS三个基准测试中取得了SOTA性能。

Conclusion: PARSE - VOS框架有效解决了RVOS任务中复杂描述对齐的问题，性能优异。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment an object of
interest throughout a video based on a language description. The prominent
challenge lies in aligning static text with dynamic visual content,
particularly when objects exhibiting similar appearances with inconsistent
motion and poses. However, current methods often rely on a holistic
visual-language fusion that struggles with complex, compositional descriptions.
In this paper, we propose \textbf{PARSE-VOS}, a novel, training-free framework
powered by Large Language Models (LLMs), for a hierarchical, coarse-to-fine
reasoning across text and video domains. Our approach begins by parsing the
natural language query into structured semantic commands. Next, we introduce a
spatio-temporal grounding module that generates all candidate trajectories for
all potential target objects, guided by the parsed semantics. Finally, a
hierarchical identification module select the correct target through a
two-stage reasoning process: it first performs coarse-grained motion reasoning
with an LLM to narrow down candidates; if ambiguity remains, a fine-grained
pose verification stage is conditionally triggered to disambiguate. The final
output is an accurate segmentation mask for the target object.
\textbf{PARSE-VOS} achieved state-of-the-art performance on three major
benchmarks: Ref-YouTube-VOS, Ref-DAVIS17, and MeViS.

</details>


### [434] [Dual-Mode Deep Anomaly Detection for Medical Manufacturing: Structural Similarity and Feature Distance](https://arxiv.org/abs/2509.05796)
*Julio Zanon Diaz,Georgios Siogkas,Peter Corcoran*

Main category: cs.CV

TL;DR: 提出两种注意力引导的自动编码器架构用于医疗设备制造中的深度异常检测，实验表明方法优于基线，适用于监管制造环境。


<details>
  <summary>Details</summary>
Motivation: 医疗设备制造中的视觉检测因小而不平衡的数据集、高分辨率图像和严格监管要求面临挑战，需要解决方案。

Method: 提出两种架构，一是采用基于结构相似性的异常分数（4 - MS - SSIM），二是在降维潜在特征上使用马氏评分的特征距离方法。

Result: 第一种架构在有10%缺陷样本的测试集上，无监督阈值ACC为0.903，有监督阈值ACC为0.931；第二种架构有监督阈值ACC为0.722，两种方法都优于重新实现的基线。

Conclusion: 两种方法有互补能力，为在监管制造环境中部署深度异常检测提供实用途径，兼顾准确性、效率和监管义务。

Abstract: Automating visual inspection in medical device manufacturing remains
challenging due to small and imbalanced datasets, high-resolution imagery, and
stringent regulatory requirements. This work proposes two attention-guided
autoencoder architectures for deep anomaly detection designed to address these
constraints. The first employs a structural similarity-based anomaly score
(4-MS-SSIM), offering lightweight and accurate real-time defect detection,
yielding ACC 0.903 (unsupervised thresholding) and 0.931 (supervised
thresholding) on the - Surface Seal Image - Test split with only 10% of
defective samples. The second applies a feature-distance approach using
Mahalanobis scoring on reduced latent features, providing high sensitivity to
distributional shifts for supervisory monitoring, achieving ACC 0.722 with
supervised thresholding. Together, these methods deliver complementary
capabilities: the first supports reliable inline inspection, while the second
enables scalable post-production surveillance and regulatory compliance
monitoring. Experimental results demonstrate that both approaches surpass
re-implemented baselines and provide a practical pathway for deploying deep
anomaly detection in regulated manufacturing environments, aligning accuracy,
efficiency, and the regulatory obligations defined for high-risk AI systems
under the EU AI Act.

</details>


### [435] [Challenges in Deep Learning-Based Small Organ Segmentation: A Benchmarking Perspective for Medical Research with Limited Datasets](https://arxiv.org/abs/2509.05892)
*Phongsakon Mark Konrad,Andrei-Alexandru Popa,Yaser Sabzehmeidani,Liang Zhong,Elisa A. Liehn,Serkan Ayvaz*

Main category: cs.CV

TL;DR: 研究在有限心血管组织学图像数据集上评估深度学习分割模型，发现模型性能对数据分割敏感，暴露了低数据临床环境中标准基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病研究和诊断需要准确分割颈动脉结构，但缺乏标注的心血管组织病理数据限制了深度学习模型发展。

Method: 对U - Net、DeepLabV3+、SegFormer、SAM、MedSAM、MedSAM+UNet等模型在有限数据集上进行评估，并采用贝叶斯搜索进行超参数优化。

Result: 模型性能对数据分割高度敏感，差异更多源于统计噪声而非算法优势。

Conclusion: 标准基准测试在低数据临床环境有局限性，性能排名不一定反映临床实用性。

Abstract: Accurate segmentation of carotid artery structures in histopathological
images is vital for advancing cardiovascular disease research and diagnosis.
However, deep learning model development in this domain is constrained by the
scarcity of annotated cardiovascular histopathological data. This study
investigates a systematic evaluation of state-of-the-art deep learning
segmentation models, including convolutional neural networks (U-Net,
DeepLabV3+), a Vision Transformer (SegFormer), and recent foundation models
(SAM, MedSAM, MedSAM+UNet), on a limited dataset of cardiovascular histology
images. Despite employing an extensive hyperparameter optimization strategy
with Bayesian search, our findings reveal that model performance is highly
sensitive to data splits, with minor differences driven more by statistical
noise than by true algorithmic superiority. This instability exposes the
limitations of standard benchmarking practices in low-data clinical settings
and challenges the assumption that performance rankings reflect meaningful
clinical utility.

</details>


### [436] [ConstStyle: Robust Domain Generalization with Unified Style Transformation](https://arxiv.org/abs/2509.05975)
*Nam Duong Tran,Nam Nguyen Phuong,Hieu H. Pham,Phi Le Nguyen,My T. Thai*

Main category: cs.CV

TL;DR: 提出ConstStyle方法增强领域泛化（DG）鲁棒性，可减少领域偏移影响，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有DG方法在训练领域有限或训练与测试领域差距大时表现不佳，需增强DG鲁棒性。

Method: 提出ConstStyle方法，利用统一领域捕获领域不变特征，通过理论分析弥合领域差距，训练和测试时将样本映射到统一领域。

Result: 实验表明ConstStyle在不同场景下始终优于现有方法，在有限训练领域时，相比次优方法可将准确率提高达19.82%。

Conclusion: ConstStyle能有效减少领域偏移影响，增强DG鲁棒性。

Abstract: Deep neural networks often suffer performance drops when test data
distribution differs from training data. Domain Generalization (DG) aims to
address this by focusing on domain-invariant features or augmenting data for
greater diversity. However, these methods often struggle with limited training
domains or significant gaps between seen (training) and unseen (test) domains.
To enhance DG robustness, we hypothesize that it is essential for the model to
be trained on data from domains that closely resemble unseen test domains-an
inherently difficult task due to the absence of prior knowledge about the
unseen domains. Accordingly, we propose ConstStyle, a novel approach that
leverages a unified domain to capture domain-invariant features and bridge the
domain gap with theoretical analysis. During training, all samples are mapped
onto this unified domain, optimized for seen domains. During testing, unseen
domain samples are projected similarly before predictions. By aligning both
training and testing data within this unified domain, ConstStyle effectively
reduces the impact of domain shifts, even with large domain gaps or few seen
domains. Extensive experiments demonstrate that ConstStyle consistently
outperforms existing methods across diverse scenarios. Notably, when only a
limited number of seen domains are available, ConstStyle can boost accuracy up
to 19.82\% compared to the next best approach.

</details>


### [437] [S-LAM3D: Segmentation-Guided Monocular 3D Object Detection via Feature Space Fusion](https://arxiv.org/abs/2509.05999)
*Diana-Alexandra Sas,Florin Oniga*

Main category: cs.CV

TL;DR: 提出一种解耦策略，注入预计算分割信息先验以指导单目3D物体检测，在KITTI基准上对小物体检测表现出色。


<details>
  <summary>Details</summary>
Motivation: 单目3D物体检测因输入为单张2D图像、缺乏深度线索而具挑战性，评估额外分割信息对现有检测管道的影响。

Method: 引入解耦策略，注入预计算分割信息先验并直接融合到特征空间以指导检测，不扩展检测模型或联合学习先验。

Result: 在KITTI 3D物体检测基准上，对场景中的小物体（行人与自行车手）的检测优于仅依赖RGB图像特征的等效架构。

Conclusion: 理解输入数据可平衡对额外传感器或训练数据的需求。

Abstract: Monocular 3D Object Detection represents a challenging Computer Vision task
due to the nature of the input used, which is a single 2D image, lacking in any
depth cues and placing the depth estimation problem as an ill-posed one.
Existing solutions leverage the information extracted from the input by using
Convolutional Neural Networks or Transformer architectures as feature
extraction backbones, followed by specific detection heads for 3D parameters
prediction. In this paper, we introduce a decoupled strategy based on injecting
precomputed segmentation information priors and fusing them directly into the
feature space for guiding the detection, without expanding the detection model
or jointly learning the priors. The focus is on evaluating the impact of
additional segmentation information on existing detection pipelines without
adding additional prediction branches. The proposed method is evaluated on the
KITTI 3D Object Detection Benchmark, outperforming the equivalent architecture
that relies only on RGB image features for small objects in the scene:
pedestrians and cyclists, and proving that understanding the input data can
balance the need for additional sensors or training data.

</details>


### [438] [Khana: A Comprehensive Indian Cuisine Dataset](https://arxiv.org/abs/2509.06006)
*Omkar Prabhu*

Main category: cs.CV

TL;DR: 本文提出新的印度美食图像数据集Khana，填补相关研究空白，并进行了模型评估。


<details>
  <summary>Details</summary>
Motivation: 全球对美食体验兴趣增长，但现有数据集在涵盖印度美食方面存在不足，缺乏能覆盖其多样性的全面标注数据集。

Method: 建立印度美食分类法，创建包含约131K张、80个标签、分辨率500x500像素的数据集Khana，并对数据集创建过程进行描述，评估了分类、分割和检索的最先进模型作为基线。

Result: 得到了新的数据集Khana，并完成了对最先进模型的评估。

Conclusion: Khana为研究人员提供了全面且具有挑战性的基准，也为开发人员创建利用印度美食的现实应用提供了有价值的资源。

Abstract: As global interest in diverse culinary experiences grows, food image models
are essential for improving food-related applications by enabling accurate food
recognition, recipe suggestions, dietary tracking, and automated meal planning.
Despite the abundance of food datasets, a noticeable gap remains in capturing
the nuances of Indian cuisine due to its vast regional diversity, complex
preparations, and the lack of comprehensive labeled datasets that cover its
full breadth. Through this exploration, we uncover Khana, a new benchmark
dataset for food image classification, segmentation, and retrieval of dishes
from Indian cuisine. Khana fills the gap by establishing a taxonomy of Indian
cuisine and offering around 131K images in the dataset spread across 80 labels,
each with a resolution of 500x500 pixels. This paper describes the dataset
creation process and evaluates state-of-the-art models on classification,
segmentation, and retrieval as baselines. Khana bridges the gap between
research and development by providing a comprehensive and challenging benchmark
for researchers while also serving as a valuable resource for developers
creating real-world applications that leverage the rich tapestry of Indian
cuisine. Webpage: https://khana.omkar.xyz

</details>


### [439] [Near Real-Time Dust Aerosol Detection with 3D Convolutional Neural Networks on MODIS Data](https://arxiv.org/abs/2509.05887)
*Caleb Gates,Patrick Moorhead,Jayden Ferguson,Omar Darwish,Conner Stallman,Pablo Rivas,Paapa Quansah*

Main category: cs.CV

TL;DR: 提出近实时沙尘检测系统，用3D卷积网络处理MODIS多波段图像，改进后训练速度提升21倍，在17个独立场景中精度约0.92，表明联合波段和空间学习可实现全球沙尘预警。


<details>
  <summary>Details</summary>
Motivation: 沙尘危害健康、降低能见度，需要从卫星快速检测沙尘。

Method: 利用NASA的Terra和Aqua（MODIS）多波段图像，用3D卷积网络学习36个波段及分裂热波段的模式分离沙尘与云和地表特征，用简单归一化和局部填充处理缺失数据，改进版本提升训练速度。

Result: 在17个独立MODIS场景中，模型精度约0.92，均方误差0.014，沙尘羽流核心区域吻合度高，多数漏检在边缘。

Conclusion: 联合波段和空间学习可在全球范围内及时发出沙尘警报，使用更宽输入窗口或基于注意力的模型可能进一步锐化边缘。

Abstract: Dust storms harm health and reduce visibility; quick detection from
satellites is needed. We present a near real-time system that flags dust at the
pixel level using multi-band images from NASA's Terra and Aqua (MODIS). A 3D
convolutional network learns patterns across all 36 bands, plus split thermal
bands, to separate dust from clouds and surface features. Simple normalization
and local filling handle missing data. An improved version raises training
speed by 21x and supports fast processing of full scenes. On 17 independent
MODIS scenes, the model reaches about 0.92 accuracy with a mean squared error
of 0.014. Maps show strong agreement in plume cores, with most misses along
edges. These results show that joint band-and-space learning can provide timely
dust alerts at global scale; using wider input windows or attention-based
models may further sharpen edges.

</details>


### [440] [BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models](https://arxiv.org/abs/2509.06040)
*Yuming Li,Yikai Wang,Yuying Zhu,Zhongyu Zhao,Ming Lu,Qi She,Shanghang Zhang*

Main category: cs.CV

TL;DR: 本文提出BranchGRPO方法，降低计算成本并提升性能，实验表明其能提高对齐分数并减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有图像和视频生成模型对齐方法存在计算成本高、训练不稳定等问题。

Method: 提出BranchGRPO方法，引入分支采样策略更新SDE采样过程，共享计算、剪枝低奖励路径和冗余深度。

Result: 在图像和视频偏好对齐实验中，BranchGRPO比强基线提高对齐分数16%，减少训练时间50%。

Conclusion: BranchGRPO能有效降低计算成本，提高探索多样性，加速收敛并提升性能。

Abstract: Recent advancements in aligning image and video generative models via GRPO
have achieved remarkable gains in enhancing human preference alignment.
However, these methods still face high computational costs from on-policy
rollouts and excessive SDE sampling steps, as well as training instability due
to sparse rewards. In this paper, we propose BranchGRPO, a novel method that
introduces a branch sampling policy updating the SDE sampling process. By
sharing computation across common prefixes and pruning low-reward paths and
redundant depths, BranchGRPO substantially lowers the per-update compute cost
while maintaining or improving exploration diversity. This work makes three
main contributions: (1) a branch sampling scheme that reduces rollout and
training cost; (2) a tree-based advantage estimator incorporating dense
process-level rewards; and (3) pruning strategies exploiting path and depth
redundancy to accelerate convergence and boost performance. Experiments on
image and video preference alignment show that BranchGRPO improves alignment
scores by 16% over strong baselines, while cutting training time by 50%.

</details>


### [441] [SpecSwin3D: Generating Hyperspectral Imagery from Multispectral Data via Transformer Networks](https://arxiv.org/abs/2509.06122)
*Tang Sui,Songxi Yang,Qunying Huang*

Main category: cs.CV

TL;DR: 提出SpecSwin3D模型从多光谱输入生成高光谱图像，采用级联训练策略和优化波段序列，在重建和下游任务表现良好。


<details>
  <summary>Details</summary>
Motivation: 以往高光谱生成方法难兼顾空间细节和光谱保真度，且重建误差随光谱距离增加。

Method: 提出SpecSwin3D模型，采用级联训练策略和优化波段序列。

Result: 模型PSNR为35.82 dB，SAM为2.40°，SSIM为0.96，优于基线模型，减少ERGAS超一半。

Conclusion: SpecSwin3D模型在高光谱图像生成上有效，在下游任务有实用价值。

Abstract: Multispectral and hyperspectral imagery are widely used in agriculture,
environmental monitoring, and urban planning due to their complementary spatial
and spectral characteristics. A fundamental trade-off persists: multispectral
imagery offers high spatial but limited spectral resolution, while
hyperspectral imagery provides rich spectra at lower spatial resolution. Prior
hyperspectral generation approaches (e.g., pan-sharpening variants, matrix
factorization, CNNs) often struggle to jointly preserve spatial detail and
spectral fidelity. In response, we propose SpecSwin3D, a transformer-based
model that generates hyperspectral imagery from multispectral inputs while
preserving both spatial and spectral quality. Specifically, SpecSwin3D takes
five multispectral bands as input and reconstructs 224 hyperspectral bands at
the same spatial resolution. In addition, we observe that reconstruction errors
grow for hyperspectral bands spectrally distant from the input bands. To
address this, we introduce a cascade training strategy that progressively
expands the spectral range to stabilize learning and improve fidelity.
Moreover, we design an optimized band sequence that strategically repeats and
orders the five selected multispectral bands to better capture pairwise
relations within a 3D shifted-window transformer framework. Quantitatively, our
model achieves a PSNR of 35.82 dB, SAM of 2.40{\deg}, and SSIM of 0.96,
outperforming the baseline MHF-Net by +5.6 dB in PSNR and reducing ERGAS by
more than half. Beyond reconstruction, we further demonstrate the practical
value of SpecSwin3D on two downstream tasks, including land use classification
and burnt area segmentation.

</details>


### [442] [UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning](https://arxiv.org/abs/2509.06165)
*Huy Le,Nhat Chung,Tung Kieu,Jingkang Yang,Ngan Le*

Main category: cs.CV

TL;DR: 提出单阶段统一框架UNO解决视频场景图生成中粗粒度和细粒度任务，在多基准测试表现好且效率高。


<details>
  <summary>Details</summary>
Motivation: 以往研究针对粗粒度或细粒度视频场景图生成常需特定架构和多阶段训练，本文要解决此问题。

Method: 提出UNO框架，核心是扩展的槽注意力机制，引入对象时间一致性学习和动态三元组预测模块。

Result: 在标准粗粒度和细粒度视频场景图生成基准测试中表现有竞争力，且通过统一以对象为中心的设计提高了效率。

Conclusion: UNO能在一个端到端架构中联合处理粗粒度和细粒度任务，减少特定任务修改，实现跨视觉粒度的泛化。

Abstract: Video Scene Graph Generation (VidSGG) aims to represent dynamic visual
content by detecting objects and modeling their temporal interactions as
structured graphs. Prior studies typically target either coarse-grained
box-level or fine-grained panoptic pixel-level VidSGG, often requiring
task-specific architectures and multi-stage training pipelines. In this paper,
we present UNO (UNified Object-centric VidSGG), a single-stage, unified
framework that jointly addresses both tasks within an end-to-end architecture.
UNO is designed to minimize task-specific modifications and maximize parameter
sharing, enabling generalization across different levels of visual granularity.
The core of UNO is an extended slot attention mechanism that decomposes visual
features into object and relation slots. To ensure robust temporal modeling, we
introduce object temporal consistency learning, which enforces consistent
object representations across frames without relying on explicit tracking
modules. Additionally, a dynamic triplet prediction module links relation slots
to corresponding object pairs, capturing evolving interactions over time. We
evaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results
demonstrate that UNO not only achieves competitive performance across both
tasks but also offers improved efficiency through a unified, object-centric
design.

</details>


### [443] [A Multi-Modal Deep Learning Framework for Colorectal Pathology Diagnosis: Integrating Histological and Colonoscopy Data in a Pilot Study](https://arxiv.org/abs/2509.06351)
*Krithik Ramesh,Ritvik Koneru*

Main category: cs.CV

TL;DR: 提出统一深度学习网络诊断结直肠疾病，整合多种方法确保结果准确。


<details>
  <summary>Details</summary>
Motivation: 传统诊断流程需大量准备，且依赖分开评估，存在可变性和低效问题，需要更有效的诊断方式。

Method: 提出统一深度学习网络，用CNN在一个流程中分类组织病理切片和结肠镜视频帧，整合类平衡学习、增强和校准方法，使用PathMNIST和HyperKvasir数据集，CNN架构为ResNet - 50。

Result: 展示了可解释和可重复的诊断流程。

Conclusion: 该统一诊断流程能推进并简化结直肠疾病的检测。

Abstract: Colorectal diseases, including inflammatory conditions and neoplasms, require
quick, accurate care to be effectively treated. Traditional diagnostic
pipelines require extensive preparation and rely on separate, individual
evaluations on histological images and colonoscopy footage, introducing
possible variability and inefficiencies. This pilot study proposes a unified
deep learning network that uses convolutional neural networks (CN N s) to
classify both histopathological slides and colonoscopy video frames in one
pipeline. The pipeline integrates class-balancing learning, robust
augmentation, and calibration methods to ensure accurate results. Static colon
histology images were taken from the PathMNIST dataset, and the lower
gastrointestinal (colonoscopy) videos were drawn from the HyperKvasir dataset.
The CNN architecture used was ResNet-50. This study demonstrates an
interpretable and reproducible diagnostic pipeline that unifies multiple
diagnostic modalities to advance and ease the detection of colorectal diseases.

</details>


### [444] [MRD-LiNet: A Novel Lightweight Hybrid CNN with Gradient-Guided Unlearning for Improved Drought Stress Identification](https://arxiv.org/abs/2509.06367)
*Aswini Kumar Patra,Lingaraj Sahoo*

Main category: cs.CV

TL;DR: 本文提出轻量级混合CNN框架及机器学习遗忘机制用于干旱胁迫监测，降低计算成本且精度高，适用于资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 传统干旱胁迫检测方法耗时费力，现有深度学习模型参数多，不适用于资源受限和实时农业场景。

Method: 提出受ResNet、DenseNet和MobileNet启发的轻量级混合CNN框架，引入基于梯度范数影响函数的机器学习遗忘机制。

Result: 与传统模型相比，可减少15倍可训练参数，在马铃薯田航拍图像数据集上评估，能在降低计算成本的同时保持高精度。

Conclusion: 该框架是精准农业中干旱胁迫监测的实用、可扩展且自适应的解决方案，尤其适用于资源受限条件。

Abstract: Drought stress is a major threat to global crop productivity, making its
early and precise detection essential for sustainable agricultural management.
Traditional approaches, though useful, are often time-consuming and
labor-intensive, which has motivated the adoption of deep learning methods. In
recent years, Convolutional Neural Network (CNN) and Vision Transformer
architectures have been widely explored for drought stress identification;
however, these models generally rely on a large number of trainable parameters,
restricting their use in resource-limited and real-time agricultural settings.
To address this challenge, we propose a novel lightweight hybrid CNN framework
inspired by ResNet, DenseNet, and MobileNet architectures. The framework
achieves a remarkable 15-fold reduction in trainable parameters compared to
conventional CNN and Vision Transformer models, while maintaining competitive
accuracy. In addition, we introduce a machine unlearning mechanism based on a
gradient norm-based influence function, which enables targeted removal of
specific training data influence, thereby improving model adaptability. The
method was evaluated on an aerial image dataset of potato fields with
expert-annotated healthy and drought-stressed regions. Experimental results
show that our framework achieves high accuracy while substantially lowering
computational costs. These findings highlight its potential as a practical,
scalable, and adaptive solution for drought stress monitoring in precision
agriculture, particularly under resource-constrained conditions.

</details>


### [445] [IGAff: Benchmarking Adversarial Iterative and Genetic Affine Algorithms on Deep Neural Networks](https://arxiv.org/abs/2509.06459)
*Sebastian-Vasile Echim,Andrei-Alexandru Preda,Dumitru-Clementin Cercel,Florin Pop*

Main category: cs.CV

TL;DR: 本文详细探究对抗算法对多种网络架构影响，提出两种新的黑盒迭代对抗算法，实验效果优于同类方法，还给出防御和攻击见解。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络难理解且有弱点，对抗攻击在黑盒场景下具挑战性，需探索其对不同网络架构的影响。

Method: 利用Tiny ImageNet等数据集，提出基于仿射变换和遗传算法的Affine Transformation Attack（ATA）和Affine Genetic Attack（AGA）两种新算法，评估模型在不同配置下性能，并与Pixle和Square Attack比较。

Result: 在图像分类任务上比文献中类似方法有更好结果，准确率最多提升8.82%。

Conclusion: 在全局和目标级别给出了成功对抗防御和攻击的见解，通过算法参数变化展现了对抗鲁棒性。

Abstract: Deep neural networks currently dominate many fields of the artificial
intelligence landscape, achieving state-of-the-art results on numerous tasks
while remaining hard to understand and exhibiting surprising weaknesses. An
active area of research focuses on adversarial attacks, which aim to generate
inputs that uncover these weaknesses. However, this proves challenging,
especially in the black-box scenario where model details are inaccessible. This
paper explores in detail the impact of such adversarial algorithms on
ResNet-18, DenseNet-121, Swin Transformer V2, and Vision Transformer network
architectures. Leveraging the Tiny ImageNet, Caltech-256, and Food-101
datasets, we benchmark two novel black-box iterative adversarial algorithms
based on affine transformations and genetic algorithms: 1) Affine
Transformation Attack (ATA), an iterative algorithm maximizing our attack score
function using random affine transformations, and 2) Affine Genetic Attack
(AGA), a genetic algorithm that involves random noise and affine
transformations. We evaluate the performance of the models in the algorithm
parameter variation, data augmentation, and global and targeted attack
configurations. We also compare our algorithms with two black-box adversarial
algorithms, Pixle and Square Attack. Our experiments yield better results on
the image classification task than similar methods in the literature, achieving
an accuracy improvement of up to 8.82%. We provide noteworthy insights into
successful adversarial defenses and attacks at both global and targeted levels,
and demonstrate adversarial robustness through algorithm parameter variation.

</details>


### [446] [Multi View Slot Attention Using Paraphrased Texts For Face Anti-Spoofing](https://arxiv.org/abs/2509.06336)
*Jeongmin Yu,Susang Kim,Kisu Lee,Taekyoung Kwon,Won-Yong Shin,Ha Young Kim*

Main category: cs.CV

TL;DR: 提出MVP - FAS框架解决现有CLIP基FAS模型问题，实验显示其跨域泛化性能优。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP基FAS模型未充分利用CLIP的patch嵌入令牌，且单文本提示限制泛化能力。

Method: 提出MVP - FAS框架，含多视图插槽注意力（MVS）和多文本补丁对齐（MTPA）两个关键模块，利用多个释义文本生成特征。

Result: MVP - FAS在跨域数据集上优于先前的最先进方法。

Conclusion: MVP - FAS框架能有效解决现有CLIP基FAS模型问题，实现卓越泛化性能。

Abstract: Recent face anti-spoofing (FAS) methods have shown remarkable cross-domain
performance by employing vision-language models like CLIP. However, existing
CLIP-based FAS models do not fully exploit CLIP's patch embedding tokens,
failing to detect critical spoofing clues. Moreover, these models rely on a
single text prompt per class (e.g., 'live' or 'fake'), which limits
generalization. To address these issues, we propose MVP-FAS, a novel framework
incorporating two key modules: Multi-View Slot attention (MVS) and Multi-Text
Patch Alignment (MTPA). Both modules utilize multiple paraphrased texts to
generate generalized features and reduce dependence on domain-specific text.
MVS extracts local detailed spatial features and global context from patch
embeddings by leveraging diverse texts with multiple perspectives. MTPA aligns
patches with multiple text representations to improve semantic robustness.
Extensive experiments demonstrate that MVP-FAS achieves superior generalization
performance, outperforming previous state-of-the-art methods on cross-domain
datasets. Code: https://github.com/Elune001/MVP-FAS.

</details>


### [447] [On the Reproducibility of "FairCLIP: Harnessing Fairness in Vision-Language Learning''](https://arxiv.org/abs/2509.06535)
*Hua Chang Bakker,Stan Fris,Angela Madelon Bernardy,Stan Deutekom*

Main category: cs.CV

TL;DR: 研究FairCLIP可复现性，提出A - FairCLIP和FairCLIP+，实验结果不支持FairCLIP提升性能和公平性的说法。


<details>
  <summary>Details</summary>
Motivation: 研究Luo等人提出的FairCLIP的可复现性，探究其对CLIP群体公平性的提升效果。

Method: 重现Luo等人的实验设置，引入新实现A - FairCLIP，提出FairCLIP+，探索距离最小化对公平性和性能的影响。

Result: CLIP在零样本青光眼分类中有对特定人群的偏差，但两个数据集实验结果不支持FairCLIP能提升CLIP性能和公平性，正则化目标虽降低Sinkhorn距离，但未提升性能和公平性。

Conclusion: FairCLIP在实验中未达成提升CLIP性能和公平性的预期效果。

Abstract: We investigated the reproducibility of FairCLIP, proposed by Luo et al.
(2024), for improving the group fairness of CLIP (Radford et al., 2021) by
minimizing image-text similarity score disparities across sensitive groups
using the Sinkhorn distance. The experimental setup of Luo et al. (2024) was
reproduced to primarily investigate the research findings for FairCLIP. The
model description by Luo et al. (2024) was found to differ from the original
implementation. Therefore, a new implementation, A-FairCLIP, is introduced to
examine specific design choices. Furthermore, FairCLIP+ is proposed to extend
the FairCLIP objective to include multiple attributes. Additionally, the impact
of the distance minimization on FairCLIP's fairness and performance was
explored. In alignment with the original authors, CLIP was found to be biased
towards certain demographics when applied to zero-shot glaucoma classification
using medical scans and clinical notes from the Harvard-FairVLMed dataset.
However, the experimental results on two datasets do not support their claim
that FairCLIP improves the performance and fairness of CLIP. Although the
regularization objective reduces Sinkhorn distances, both the official
implementation and the aligned implementation, A-FairCLIP, were not found to
improve performance nor fairness in zero-shot glaucoma classification.

</details>


### [448] [Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models](https://arxiv.org/abs/2509.06415)
*Jaemin Son,Sujin Choi,Inyong Yun*

Main category: cs.CV

TL;DR: 提出轻量级标记剪枝框架以降低视觉语言模型在文档理解任务中的计算负担，实验证明能降低成本并保持精度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在文档理解任务中计算需求高，需减轻计算负担。

Method: 提出轻量级标记剪枝框架，用二进制块级分类器移除非文本区域，通过最大池化细化步骤恢复碎片化文本区域。

Result: 在真实文档数据集实验中，该方法大幅降低计算成本，且保持相当的准确性。

Conclusion: 所提轻量级标记剪枝框架能有效缓解视觉语言模型的计算负担。

Abstract: Recent progress in vision-language models (VLMs) has led to impressive
results in document understanding tasks, but their high computational demands
remain a challenge. To mitigate the compute burdens, we propose a lightweight
token pruning framework that filters out non-informative background regions
from document images prior to VLM processing. A binary patch-level classifier
removes non-text areas, and a max-pooling refinement step recovers fragmented
text regions to enhance spatial coherence. Experiments on real-world document
datasets demonstrate that our approach substantially lowers computational
costs, while maintaining comparable accuracy.

</details>


### [449] [Detection of trade in products derived from threatened species using machine learning and a smartphone](https://arxiv.org/abs/2509.06585)
*Ritwik Kulkarni,WU Hanqin,Enrico Di Minin*

Main category: cs.CV

TL;DR: 本文开发机器学习目标识别模型检测野生动物制品交易，在图像检测和手机应用中取得较好效果，可用于网络和实体市场监测。


<details>
  <summary>Details</summary>
Motivation: 野生动物非法交易在数字平台愈发普遍，需自动化方法检测，尤其针对象牙等制品。

Method: 开发基于机器学习的目标识别模型，用大象、穿山甲和老虎制品图像数据，研究不同训练策略和损失函数组合，训练单物种和多物种模型。

Result: 最佳模型整体准确率84.2%，检测大象、穿山甲和老虎制品准确率分别为71.1%、90.2%和93.5%，手机应用整体准确率91.3%。

Conclusion: 所提方法不仅适用于网络交易监测，也可用于实体市场监测野生动物贸易。

Abstract: Unsustainable trade in wildlife is a major threat to biodiversity and is now
increasingly prevalent in digital marketplaces and social media. With the sheer
volume of digital content, the need for automated methods to detect wildlife
trade listings is growing. These methods are especially needed for the
automatic identification of wildlife products, such as ivory. We developed
machine learning-based object recognition models that can identify wildlife
products within images and highlight them. The data consists of images of
elephant, pangolin, and tiger products that were identified as being sold
illegally or that were confiscated by authorities. Specifically, the wildlife
products included elephant ivory and skins, pangolin scales, and claws (raw and
crafted), and tiger skins and bones. We investigated various combinations of
training strategies and two loss functions to identify the best model to use in
the automatic detection of these wildlife products. Models were trained for
each species while also developing a single model to identify products from all
three species. The best model showed an overall accuracy of 84.2% with
accuracies of 71.1%, 90.2% and 93.5% in detecting products derived from
elephants, pangolins, and tigers, respectively. We further demonstrate that the
machine learning model can be made easily available to stakeholders, such as
government authorities and law enforcement agencies, by developing a
smartphone-based application that had an overall accuracy of 91.3%. The
application can be used in real time to click images and help identify
potentially prohibited products of target species. Thus, the proposed method is
not only applicable for monitoring trade on the web but can also be used e.g.
in physical markets for monitoring wildlife trade.

</details>


### [450] [Improved Classification of Nitrogen Stress Severity in Plants Under Combined Stress Conditions Using Spatio-Temporal Deep Learning Framework](https://arxiv.org/abs/2509.06625)
*Aswini Kumar Patra*

Main category: cs.CV

TL;DR: 提出新颖深度学习框架，用四种成像模态及时空深度学习管道CNN - LSTM分类氮胁迫严重程度，准确率达98%，优于其他模型，为作物管理提供工具。


<details>
  <summary>Details</summary>
Motivation: 自然环境中植物面临多种交互胁迫，氮胁迫与干旱、杂草竞争复合时难区分应对，早期检测氮胁迫对保护植物健康和实施管理策略至关重要。

Method: 提出结合RGB、多光谱和两种红外波长四种成像模态的新颖深度学习框架，采用时空深度学习管道，用CNN提取图像空间特征，LSTM捕捉时间依赖，还设计空间CNN管道用于对比。

Result: CNN - LSTM管道准确率达98%，远超空间CNN模型的80.45%和其他机器学习方法的76%。

Conclusion: CNN - LSTM方法能有效捕捉氮缺乏、水分胁迫和杂草压力间的复杂交互，为及时主动识别氮胁迫严重程度提供有力工具，利于作物管理和植物健康。

Abstract: Plants in their natural habitats endure an array of interacting stresses,
both biotic and abiotic, that rarely occur in isolation. Nutrient
stress-particularly nitrogen deficiency-becomes even more critical when
compounded with drought and weed competition, making it increasingly difficult
to distinguish and address its effects. Early detection of nitrogen stress is
therefore crucial for protecting plant health and implementing effective
management strategies. This study proposes a novel deep learning framework to
accurately classify nitrogen stress severity in a combined stress environment.
Our model uses a unique blend of four imaging modalities-RGB, multispectral,
and two infrared wavelengths-to capture a wide range of physiological plant
responses from canopy images. These images, provided as time-series data,
document plant health across three levels of nitrogen availability (low,
medium, and high) under varying water stress and weed pressures. The core of
our approach is a spatio-temporal deep learning pipeline that merges a
Convolutional Neural Network (CNN) for extracting spatial features from images
with a Long Short-Term Memory (LSTM) network to capture temporal dependencies.
We also devised and evaluated a spatial-only CNN pipeline for comparison. Our
CNN-LSTM pipeline achieved an impressive accuracy of 98%, impressively
surpassing the spatial-only model's 80.45% and other previously reported
machine learning method's 76%. These results bring actionable insights based on
the power of our CNN-LSTM approach in effectively capturing the subtle and
complex interactions between nitrogen deficiency, water stress, and weed
pressure. This robust platform offers a promising tool for the timely and
proactive identification of nitrogen stress severity, enabling better crop
management and improved plant health.

</details>


### [451] [Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning](https://arxiv.org/abs/2509.06461)
*Yuyao Ge,Shenghua Liu,Yiwei Wang,Lingrui Mei,Baolong Bi,Xuanshan Zhou,Jiayu Yao,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CV

TL;DR: 本文研究VLMs注意力模式，提出无训练方法CARVE提升视觉推理性能，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有VLM增强方法有局限，未利用其内在能力，在复杂视觉环境中性能下降。

Method: 研究VLM注意力模式，提出无训练的CARVE方法，通过像素级注意力对比提取任务相关视觉信号。

Result: CARVE持续提升性能，在开源模型上最高提升75%。

Conclusion: 研究揭示视觉复杂性与注意力机制的相互作用，提供通过对比注意力改进视觉推理的有效途径。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable success across
diverse visual tasks, yet their performance degrades in complex visual
environments. While existing enhancement approaches require additional
training, rely on external segmentation tools, or operate at coarse-grained
levels, they overlook the innate ability within VLMs. To bridge this gap, we
investigate VLMs' attention patterns and discover that: (1) visual complexity
strongly correlates with attention entropy, negatively impacting reasoning
performance; (2) attention progressively refines from global scanning in
shallow layers to focused convergence in deeper layers, with convergence degree
determined by visual complexity. (3) Theoretically, we prove that the contrast
of attention maps between general queries and task-specific queries enables the
decomposition of visual signal into semantic signals and visual noise
components. Building on these insights, we propose Contrastive Attention
Refinement for Visual Enhancement (CARVE), a training-free method that extracts
task-relevant visual signals through attention contrasting at the pixel level.
Extensive experiments demonstrate that CARVE consistently enhances performance,
achieving up to 75% improvement on open-source models. Our work provides
critical insights into the interplay between visual complexity and attention
mechanisms, offering an efficient pathway for improving visual reasoning with
contrasting attention.

</details>


### [452] [UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward](https://arxiv.org/abs/2509.06818)
*Yufeng Cheng,Wenxu Wu,Shaojin Wu,Mengqi Huang,Fei Ding,Qian He*

Main category: cs.CV

TL;DR: 提出UMO框架解决图像定制中多参考图像身份一致性和混淆问题，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有图像定制模型在多参考图像下存在身份一致性难保持和身份混淆问题，限制身份可扩展性。

Method: 提出UMO框架，采用多对多匹配范式将多身份生成转化为全局分配优化问题，通过扩散模型上的强化学习实现；开发含合成和真实部分的可扩展定制数据集；提出新的身份混淆度量指标。

Result: UMO显著提高身份一致性，减少身份混淆，在开源方法中达到身份保留维度的新SOTA。

Conclusion: UMO框架有效解决图像定制中的身份问题，具有良好效果。

Abstract: Recent advancements in image customization exhibit a wide range of
application prospects due to stronger customization capabilities. However,
since we humans are more sensitive to faces, a significant challenge remains in
preserving consistent identity while avoiding identity confusion with
multi-reference images, limiting the identity scalability of customization
models. To address this, we present UMO, a Unified Multi-identity Optimization
framework, designed to maintain high-fidelity identity preservation and
alleviate identity confusion with scalability. With "multi-to-multi matching"
paradigm, UMO reformulates multi-identity generation as a global assignment
optimization problem and unleashes multi-identity consistency for existing
image customization methods generally through reinforcement learning on
diffusion models. To facilitate the training of UMO, we develop a scalable
customization dataset with multi-reference images, consisting of both
synthesised and real parts. Additionally, we propose a new metric to measure
identity confusion. Extensive experiments demonstrate that UMO not only
improves identity consistency significantly, but also reduces identity
confusion on several image customization methods, setting a new
state-of-the-art among open-source methods along the dimension of identity
preserving. Code and model: https://github.com/bytedance/UMO

</details>


### [453] [Video-Based MPAA Rating Prediction: An Attention-Driven Hybrid Architecture Using Contrastive Learning](https://arxiv.org/abs/2509.06826)
*Dipta Neogi,Nourash Azmine Chowdhury,Muhammad Rafsan Kabir,Mohammad Ashrafuzzaman Khan*

Main category: cs.CV

TL;DR: 为解决传统视频年龄适用性分类方法的问题，采用对比学习和混合架构，在上下文对比学习框架中取得SOTA性能，并部署为Web应用。


<details>
  <summary>Details</summary>
Motivation: 视觉内容消费增长，传统视频年龄适用性分类方法有数据要求高、泛化差和特征学习低效等问题。

Method: 采用对比学习（实例判别、上下文对比学习、多视图对比学习），混合架构集成LRCN骨干和Bahdanau注意力机制，评估不同对比损失函数。

Result: 在上下文对比学习框架中达到88%准确率和0.8815的F1分数，能进行细粒度区分。

Conclusion: 提出的架构具有鲁棒性，部署的Web应用可有效解决流媒体平台内容合规问题。

Abstract: The rapid growth of visual content consumption across platforms necessitates
automated video classification for age-suitability standards like the MPAA
rating system (G, PG, PG-13, R). Traditional methods struggle with large
labeled data requirements, poor generalization, and inefficient feature
learning. To address these challenges, we employ contrastive learning for
improved discrimination and adaptability, exploring three frameworks: Instance
Discrimination, Contextual Contrastive Learning, and Multi-View Contrastive
Learning. Our hybrid architecture integrates an LRCN (CNN+LSTM) backbone with a
Bahdanau attention mechanism, achieving state-of-the-art performance in the
Contextual Contrastive Learning framework, with 88% accuracy and an F1 score of
0.8815. By combining CNNs for spatial features, LSTMs for temporal modeling,
and attention mechanisms for dynamic frame prioritization, the model excels in
fine-grained borderline distinctions, such as differentiating PG-13 and R-rated
content. We evaluate the model's performance across various contrastive loss
functions, including NT-Xent, NT-logistic, and Margin Triplet, demonstrating
the robustness of our proposed architecture. To ensure practical application,
the model is deployed as a web application for real-time MPAA rating
classification, offering an efficient solution for automated content compliance
across streaming platforms.

</details>


### [454] [Curia: A Multi-Modal Foundation Model for Radiology](https://arxiv.org/abs/2509.06830)
*Corentin Dancette,Julien Khlaut,Antoine Saporta,Helene Philippe,Elodie Ferreres,Baptiste Callard,Théo Danielou,Léo Alberge,Léo Machado,Daniel Tordjman,Julie Dupuis,Korentin Le Floch,Jean Du Terrail,Mariam Moshiri,Laurent Dercle,Tom Boeken,Jules Gregory,Maxime Ronot,François Legou,Pascal Roux,Marc Sapoval,Pierre Manceron,Paul Hérent*

Main category: cs.CV

TL;DR: 介绍基础模型Curia，它在放射学任务表现出色，还开源权重


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助放射学解释单任务模型有局限，基础模型潜力未在放射学充分实现

Method: 在大型医院多年的全部横断面成像数据上训练Curia

Result: Curia在19任务外部验证基准上能准确完成多项任务，表现优于放射科医生和近期基础模型，有临床显著特性

Conclusion: Curia是有潜力的放射学基础模型，开源权重可加速该领域发展

Abstract: AI-assisted radiological interpretation is based on predominantly narrow,
single-task models. This approach is impractical for covering the vast spectrum
of imaging modalities, diseases, and radiological findings. Foundation models
(FMs) hold the promise of broad generalization across modalities and in
low-data settings. However, this potential has remained largely unrealized in
radiology. We introduce Curia, a foundation model trained on the entire
cross-sectional imaging output of a major hospital over several years, which to
our knowledge is the largest such corpus of real-world data-encompassing
150,000 exams (130 TB). On a newly curated 19-task external validation
benchmark, Curia accurately identifies organs, detects conditions like brain
hemorrhages and myocardial infarctions, and predicts outcomes in tumor staging.
Curia meets or surpasses the performance of radiologists and recent foundation
models, and exhibits clinically significant emergent properties in
cross-modality, and low-data regimes. To accelerate progress, we release our
base model's weights at https://huggingface.co/raidium/curia.

</details>


### [455] [ToonOut: Fine-tuned Background-Removal for Anime Characters](https://arxiv.org/abs/2509.06839)
*Matteo Muratori,Joël Seytre*

Main category: cs.CV

TL;DR: 现有背景去除模型在动漫风格内容处理不佳，作者收集标注数据集微调BiRefNet模型，提升了动漫图像背景去除准确率并开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 解决现有背景去除模型在动漫风格内容领域表现不佳的问题。

Method: 收集并标注1228张高质量动漫图像的自定义数据集，在该数据集上微调开源的BiRefNet模型。

Result: 动漫风格图像背景去除准确率从95.3%提升到99.5%（新引入的像素准确率指标）。

Conclusion: 微调模型有效提升了动漫图像背景去除的准确率，且开源代码、微调模型权重和数据集。

Abstract: While state-of-the-art background removal models excel at realistic imagery,
they frequently underperform in specialized domains such as anime-style
content, where complex features like hair and transparency present unique
challenges. To address this limitation, we collected and annotated a custom
dataset of 1,228 high-quality anime images of characters and objects, and
fine-tuned the open-sourced BiRefNet model on this dataset. This resulted in
marked improvements in background removal accuracy for anime-style images,
increasing from 95.3% to 99.5% for our newly introduced Pixel Accuracy metric.
We are open-sourcing the code, the fine-tuned model weights, as well as the
dataset at: https://github.com/MatteoKartoon/BiRefNet.

</details>


### [456] [BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring](https://arxiv.org/abs/2509.06690)
*Usman Haider,Lukasz Szemet,Daniel Kelly,Vasileios Sergis,Andrew C. Daly,Karl Mason*

Main category: cs.CV

TL;DR: 本文针对生物打印实时监测问题，提出轻量级语义分割框架BioLite U - Net，用新数据集训练并与基线模型对比，结果显示其在精度、效率和可部署性上表现出色。


<details>
  <summary>Details</summary>
Motivation: 生物打印中实时确保打印结构保真和一致是核心挑战，语义分割对原位监测很重要，需要适合实时生物打印应用的轻量级框架。

Method: 引入轻量级语义分割框架，创建含787张RGB图像的新数据集，提出BioLite U - Net架构，用深度可分离卷积减少计算量，用mIoU、Dice分数和像素精度与MobileNetV2和MobileNetV3基线模型对比，并在Raspberry Pi 4B上评估。

Result: BioLite U - Net的mIoU达92.85%，Dice分数达96.17%，比MobileNetV2 - DeepLabV3 +小超1300倍，每帧推理时间335 ms。

Conclusion: BioLite U - Net在分割精度、效率和可部署性上取得更好平衡，适合智能闭环生物打印系统。

Abstract: Bioprinting is a rapidly advancing field that offers a transformative
approach to fabricating tissue and organ models through the precise deposition
of cell-laden bioinks. Ensuring the fidelity and consistency of printed
structures in real-time remains a core challenge, particularly under
constraints imposed by limited imaging data and resource-constrained embedded
hardware. Semantic segmentation of the extrusion process, differentiating
between nozzle, extruded bioink, and surrounding background, enables in situ
monitoring critical to maintaining print quality and biological viability. In
this work, we introduce a lightweight semantic segmentation framework tailored
for real-time bioprinting applications. We present a novel, manually annotated
dataset comprising 787 RGB images captured during the bioprinting process,
labeled across three classes: nozzle, bioink, and background. To achieve fast
and efficient inference suitable for integration with bioprinting systems, we
propose a BioLite U-Net architecture that leverages depthwise separable
convolutions to drastically reduce computational load without compromising
accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based
segmentation baselines using mean Intersection over Union (mIoU), Dice score,
and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess
real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85%
and a Dice score of 96.17%, while being over 1300x smaller than
MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame,
demonstrating near real-time capability. Compared to MobileNet baselines,
BioLite U-Net offers a superior tradeoff between segmentation accuracy,
efficiency, and deployability, making it highly suitable for intelligent,
closed-loop bioprinting systems.

</details>


### [457] [MRI-Based Brain Tumor Detection through an Explainable EfficientNetV2 and MLP-Mixer-Attention Architecture](https://arxiv.org/abs/2509.06713)
*Mustafa Yurdakul,Şakir Taşdemir*

Main category: cs.CV

TL;DR: 提出结合EfficientNetV2和基于注意力的MLP - Mixer的深度学习模型用于脑肿瘤分类，准确率达99.50%，性能超文献研究。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤死亡率高需早诊断，人工检查MRI图像易出错，需自动化诊断系统。

Method: 用含3064张MRI图像的公开数据集，评估九种CNN架构选EfficientNetV2为骨干，集成基于注意力的MLP - Mixer，用Grad - CAM可视化解释，五折交叉验证评估。

Result: 模型准确率99.50%、精确率99.47%、召回率99.52%、F1分数99.49%，超文献研究，Grad - CAM可视化显示模型有效关注相关区域。

Conclusion: 结合EfficientNetV2和基于注意力的MLP - Mixer得到用于临床决策支持系统的鲁棒深度学习模型，脑肿瘤分类准确率高且可解释性强。

Abstract: Brain tumors are serious health problems that require early diagnosis due to
their high mortality rates. Diagnosing tumors by examining Magnetic Resonance
Imaging (MRI) images is a process that requires expertise and is prone to
error. Therefore, the need for automated diagnosis systems is increasing day by
day. In this context, a robust and explainable Deep Learning (DL) model for the
classification of brain tumors is proposed. In this study, a publicly available
Figshare dataset containing 3,064 T1-weighted contrast-enhanced brain MRI
images of three tumor types was used. First, the classification performance of
nine well-known CNN architectures was evaluated to determine the most effective
backbone. Among these, EfficientNetV2 demonstrated the best performance and was
selected as the backbone for further development. Subsequently, an
attention-based MLP-Mixer architecture was integrated into EfficientNetV2 to
enhance its classification capability. The performance of the final model was
comprehensively compared with basic CNNs and the methods in the literature.
Additionally, Grad-CAM visualization was used to interpret and validate the
decision-making process of the proposed model. The proposed model's performance
was evaluated using the five-fold cross-validation method. The proposed model
demonstrated superior performance with 99.50% accuracy, 99.47% precision,
99.52% recall and 99.49% F1 score. The results obtained show that the model
outperforms the studies in the literature. Moreover, Grad-CAM visualizations
demonstrate that the model effectively focuses on relevant regions of MRI
images, thus improving interpretability and clinical reliability. A robust deep
learning model for clinical decision support systems has been obtained by
combining EfficientNetV2 and attention-based MLP-Mixer, providing high accuracy
and interpretability in brain tumor classification.

</details>


### [458] [Interleaving Reasoning for Better Text-to-Image Generation](https://arxiv.org/abs/2509.06945)
*Wenxuan Huang,Shuang Chen,Zheyong Xie,Shaosheng Cao,Shixiang Tang,Yufan Shen,Qingyu Yin,Wenbo Hu,Xiaoman Wang,Yuntian Tang,Junbo Qiao,Yue Guo,Yao Hu,Zhenfei Yin,Philip Torr,Yu Cheng,Wanli Ouyang,Shaohui Lin*

Main category: cs.CV

TL;DR: 探索交错推理能否提升文本到图像生成，提出IRG框架和IRGL学习方法，实验取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 统一多模态理解和生成模型在指令遵循和细节保留上与GPT - 4o有差距，受交错推理进展启发，探索其对T2I生成的提升。

Method: 引入IRG框架，交替进行基于文本的思考和图像合成；提出IRGL学习方法，设定两个子目标；整理IRGL - 300K数据集；采用两阶段训练。

Result: 实验取得SOTA性能，在多个评估指标上有5 - 10分的绝对提升，视觉质量和细粒度保真度有显著改善。

Conclusion: 交错推理能有效提升文本到图像生成能力，相关代码、模型权重和数据集将开源。

Abstract: Unified multimodal understanding and generation models recently have achieve
significant improvement in image generation capability, yet a large gap remains
in instruction following and detail preservation compared to systems that
tightly couple comprehension with generation such as GPT-4o. Motivated by
recent advances in interleaving reasoning, we explore whether such reasoning
can further improve Text-to-Image (T2I) generation. We introduce Interleaving
Reasoning Generation (IRG), a framework that alternates between text-based
thinking and image synthesis: the model first produces a text-based thinking to
guide an initial image, then reflects on the result to refine fine-grained
details, visual quality, and aesthetics while preserving semantics. To train
IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),
which targets two sub-goals: (1) strengthening the initial think-and-generate
stage to establish core content and base quality, and (2) enabling high-quality
textual reflection and faithful implementation of those refinements in a
subsequent image. We curate IRGL-300K, a dataset organized into six decomposed
learning modes that jointly cover learning text-based thinking, and full
thinking-image trajectories. Starting from a unified foundation model that
natively emits interleaved text-image outputs, our two-stage training first
builds robust thinking and reflection, then efficiently tunes the IRG pipeline
in the full thinking-image trajectory data. Extensive experiments show SoTA
performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,
GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality
and fine-grained fidelity. The code, model weights and datasets will be
released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .

</details>


### [459] [H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers](https://arxiv.org/abs/2509.06956)
*Wenhao Li,Mengyuan Liu,Hong Liu,Pichao Wang,Shijian Lu,Nicu Sebe*

Main category: cs.CV

TL;DR: 提出Hierarchical Hourglass Tokenizer (H₂OT)框架用于高效视频3D人体姿态估计，实验验证其有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频姿态变换器计算成本高，在资源受限设备上不实用，需提高模型效率。

Method: 提出H₂OT框架，包含Token Pruning Module (TPM)和Token Recovering Module (TRM)，先剪枝冗余帧的姿态令牌，再恢复完整序列。

Result: 方法具有通用性，可融入常见VPT模型，能适应不同剪枝和恢复策略，实验证明有效且高效。

Conclusion: 维持完整姿态序列并非必要，少量代表帧的姿态令牌可兼顾效率和估计精度。

Abstract: Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a hierarchical plug-and-play pruning-and-recovering
framework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient
transformer-based 3D human pose estimation from videos. H$_{2}$OT begins with
progressively pruning pose tokens of redundant frames and ends with recovering
full-length sequences, resulting in a few pose tokens in the intermediate
transformer blocks and thus improving the model efficiency. It works with two
key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module
(TRM). TPM dynamically selects a few representative tokens to eliminate the
redundancy of video frames, while TRM restores the detailed spatio-temporal
information based on the selected tokens, thereby expanding the network output
to the original full-length temporal resolution for fast inference. Our method
is general-purpose: it can be easily incorporated into common VPT models on
both seq2seq and seq2frame pipelines while effectively accommodating different
token pruning and recovery strategies. In addition, our H$_{2}$OT reveals that
maintaining the full pose sequence is unnecessary, and a few pose tokens of
representative frames can achieve both high efficiency and estimation accuracy.
Extensive experiments on multiple benchmark datasets demonstrate both the
effectiveness and efficiency of the proposed method. Code and models are
available at https://github.com/NationalGAILab/HoT.

</details>


### [460] [Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing Clinical Practice](https://arxiv.org/abs/2509.06854)
*Hajar Moradmand,Lei Ren*

Main category: cs.CV

TL;DR: 文章提出基于深度学习的自动评分框架 ARTSS 分析全手 X 光图像评估类风湿性关节炎严重程度，模型表现良好，证明深度学习自动评分潜力。


<details>
  <summary>Details</summary>
Motivation: 手动使用 TSS 评估类风湿性关节炎严重程度耗时且主观，存在观察者间和观察者内差异。

Method: 使用 970 名患者数据开发 ARTSS，分四阶段处理，用多种指标评估模型，采用 3 折交叉验证和外部测试。

Result: 关节识别模型准确率达 99%，ViT 模型 TSS 预测的 Huber 损失低至 0.87。

Conclusion: 深度学习自动评分可提升临床实践，解决关节消失和数量可变问题，节省时间，减少差异，提高准确性，辅助决策。

Abstract: Assessing the severity of rheumatoid arthritis (RA) using the Total Sharp/Van
Der Heijde Score (TSS) is crucial, but manual scoring is often time-consuming
and subjective. This study introduces an Automated Radiographic Sharp Scoring
(ARTSS) framework that leverages deep learning to analyze full-hand X-ray
images, aiming to reduce inter- and intra-observer variability. The research
uniquely accommodates patients with joint disappearance and variable-length
image sequences. We developed ARTSS using data from 970 patients, structured
into four stages: I) Image pre-processing and re-orientation using ResNet50,
II) Hand segmentation using UNet.3, III) Joint identification using YOLOv7, and
IV) TSS prediction using models such as VGG16, VGG19, ResNet50, DenseNet201,
EfficientNetB0, and Vision Transformer (ViT). We evaluated model performance
with Intersection over Union (IoU), Mean Average Precision (MAP), mean absolute
error (MAE), Root Mean Squared Error (RMSE), and Huber loss. The average TSS
from two radiologists was used as the ground truth. Model training employed
3-fold cross-validation, with each fold consisting of 452 training and 227
validation samples, and external testing included 291 unseen subjects. Our
joint identification model achieved 99% accuracy. The best-performing model,
ViT, achieved a notably low Huber loss of 0.87 for TSS prediction. Our results
demonstrate the potential of deep learning to automate RA scoring, which can
significantly enhance clinical practice. Our approach addresses the challenge
of joint disappearance and variable joint numbers, offers timesaving benefits,
reduces inter- and intra-reader variability, improves radiologist accuracy, and
aids rheumatologists in making more informed decisions.

</details>


### [461] [Barlow-Swin: Toward a novel siamese-based segmentation architecture using Swin-Transformers](https://arxiv.org/abs/2509.06885)
*Morteza Kiani Haftlang,Mohammadhossein Malmir,Foroutan Parand,Umberto Michelucci,Safouane El Ghazouali*

Main category: cs.CV

TL;DR: 提出用于实时二元医学图像分割的轻量级架构，结合类Swin Transformer编码器与类U-Net解码器，用自监督学习预训练，实验显示有竞争力且适合资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 现有卷积架构和集成transformer模型存在全局上下文建模受限、计算成本高不适合实时使用的问题。

Method: 设计结合类Swin Transformer编码器与类U-Net解码器的架构，用Barlow Twins自监督学习方法预训练编码器，再微调整个模型。

Result: 在基准二元分割任务实验中，模型参数大幅减少、推理速度更快且精度有竞争力。

Conclusion: 该模型可作为实时和资源受限临床环境部署的实用替代方案。

Abstract: Medical image segmentation is a critical task in clinical workflows,
particularly for the detection and delineation of pathological regions. While
convolutional architectures like U-Net have become standard for such tasks,
their limited receptive field restricts global context modeling. Recent efforts
integrating transformers have addressed this, but often result in deep,
computationally expensive models unsuitable for real-time use. In this work, we
present a novel end-to-end lightweight architecture designed specifically for
real-time binary medical image segmentation. Our model combines a Swin
Transformer-like encoder with a U-Net-like decoder, connected via skip pathways
to preserve spatial detail while capturing contextual information. Unlike
existing designs such as Swin Transformer or U-Net, our architecture is
significantly shallower and competitively efficient. To improve the encoder's
ability to learn meaningful features without relying on large amounts of
labeled data, we first train it using Barlow Twins, a self-supervised learning
method that helps the model focus on important patterns by reducing unnecessary
repetition in the learned features. After this pretraining, we fine-tune the
entire model for our specific task. Experiments on benchmark binary
segmentation tasks demonstrate that our model achieves competitive accuracy
with substantially reduced parameter count and faster inference, positioning it
as a practical alternative for deployment in real-time and resource-limited
clinical environments. The code for our method is available at Github
repository: https://github.com/mkianih/Barlow-Swin.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [462] [Sesame: Opening the door to protein pockets](https://arxiv.org/abs/2509.05302)
*Raúl Miñán,Carles Perez-Lopez,Javier Iglesias,Álvaro Ciudad,Alexis Molina*

Main category: q-bio.BM

TL;DR: 介绍了Sesame生成模型，能高效预测构象变化，为改进虚拟筛选工作流程提供可扩展解决方案。


<details>
  <summary>Details</summary>
Motivation: 获取高分辨率配体结合结构成本高、耗时长，配体自由结构对接性能差，传统诱导构象方法计算成本高。

Method: 引入Sesame生成模型。

Result: Sesame能以较低计算成本生成更适合配体容纳的几何形状。

Conclusion: Sesame为改进虚拟筛选工作流程提供了可扩展解决方案。

Abstract: Molecular docking is a cornerstone of drug discovery, relying on
high-resolution ligand-bound structures to achieve accurate predictions.
However, obtaining these structures is often costly and time-intensive,
limiting their availability. In contrast, ligand-free structures are more
accessible but suffer from reduced docking performance due to pocket geometries
being less suited for ligand accommodation in apo structures. Traditional
methods for artificially inducing these conformations, such as molecular
dynamics simulations, are computationally expensive. In this work, we introduce
Sesame, a generative model designed to predict this conformational change
efficiently. By generating geometries better suited for ligand accommodation at
a fraction of the computational cost, Sesame aims to provide a scalable
solution for improving virtual screening workflows.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [463] [Unveiling the Listener Structure Underlying K-pop's Global Success: A Large-Scale Listening Data Analysis](https://arxiv.org/abs/2509.06606)
*Ryota Nakamura,Keita Nishimoto,Ichiro Sakata,Kimitaka Asatani*

Main category: cs.SI

TL;DR: 本文通过分析Last.fm的大规模收听数据集，研究全球音乐听众对K - pop的收听和认知情况，发现2005 - 2019年K - pop播放量显著增加，主要由少量重度听众支撑，且在2005 - 2010年成为独立音乐流派。


<details>
  <summary>Details</summary>
Motivation: 此前人们对全球大量音乐听众如何收听和认知K - pop知之甚少，本文旨在解决该问题。

Method: 分析Last.fm的大规模收听数据集，包括对播放量分布和用户分配的流派标签进行分析。

Result: 2005 - 2019年K - pop播放量显著增加，主要由少量重度听众支撑，播放量的基尼系数大于现有主流流派和其他新兴小众流派；2005 - 2010年K - pop摆脱亚洲本土流派地位，成为独立音乐流派。

Conclusion: 通过对数据的分析，揭示了K - pop在全球的发展情况以及听众的收听模式。

Abstract: From the mid-2000s to the 2010s, K-pop moved beyond its status as a
regionally popular genre in Asia and established itself as a global music genre
with enthusiastic fans around the world. However, little is known about how the
vast number of music listeners across the globe have listened to and perceived
K-pop. This study addresses this question by analyzing a large-scale listening
dataset from Last.fm. An analysis of the distribution of play counts reveals
that K-pop experienced a significant increase in plays between 2005 and 2019,
largely supported by a small group of heavy listeners. The Gini coefficient in
play counts is notably greater than that of existing mainstream genres and
other growing niche genres. Furthermore, an analysis based on user-assigned
genre tags quantitatively demonstrates that between 2005 and 2010, K-pop shed
its status as a local Asian genre and established itself as a distinct music
genre in its own right.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [464] [The Efficiency Frontier: Classical Shadows versus Quantum Footage](https://arxiv.org/abs/2509.06218)
*Shuowei Ma,Junyu Liu*

Main category: quant-ph

TL;DR: 论文对经典影子法和量子直接测量法进行全栈资源分析，给出下载效率边界，确定经典影子法优势范围及影响参数，找到不同硬件下的盈亏平衡点，为混合量子 - 经典断层扫描策略设计提供新思路。


<details>
  <summary>Details</summary>
Motivation: 在处理少量高度非局部可观测量或经典后处理能力有限时，经典影子法并非最有效，需定量分析以找到更优量子测量方法。

Method: 进行全栈资源分析，比较经典影子法和量子直接测量法，考虑多种参数。

Result: 得出经典影子法和量子直接测量法的下载效率边界，确定经典影子法在不同可观测量形式下的优势范围，找到不同硬件下经典影子法更高效的盈亏平衡点。

Conclusion: 为混合量子 - 经典断层扫描定量设计最优策略开辟新途径，为实际应用中选择合适量子测量方法提供实用见解。

Abstract: Interfacing quantum and classical processors is an important subroutine in
full-stack quantum algorithms. The so-called "classical shadow" method
efficiently extracts essential classical information from quantum states,
enabling the prediction of many properties of a quantum system from only a few
measurements. However, for a small number of highly non-local observables, or
when classical post-processing power is limited, the classical shadow method is
not always the most efficient choice. Here, we address this issue
quantitatively by performing a full-stack resource analysis that compares
classical shadows with ``quantum footage," which refers to direct quantum
measurement. Under certain assumptions, our analysis illustrates a boundary of
download efficiency between classical shadows and quantum footage. For
observables expressed as linear combinations of Pauli matrices, the classical
shadow method outperforms direct measurement when the number of observables is
large and the Pauli weight is small. For observables in the form of large
Hermitian sparse matrices, the classical shadow method shows an advantage when
the number of observables, the sparsity of the matrix, and the number of qubits
fall within a certain range. The key parameters influencing this behavior
include the number of qubits $n$, observables $M$, sparsity $k$, Pauli weight
$w$, accuracy requirement $\epsilon$, and failure tolerance $\delta$. We also
compare the resource consumption of the two methods on different types of
quantum computers and identify break-even points where the classical shadow
method becomes more efficient, which vary depending on the hardware. This paper
opens a new avenue for quantitatively designing optimal strategies for hybrid
quantum-classical tomography and provides practical insights for selecting the
most suitable quantum measurement approach in real-world applications.

</details>


### [465] [Quantum spatial best-arm identification via quantum walks](https://arxiv.org/abs/2509.05890)
*Tomoki Yamagami,Etsuo Segawa,Takatomo Mihana,André Röhm,Atsushi Uchida,Ryoichi Horisaki*

Main category: quant-ph

TL;DR: 提出量子算法QSBAI用于图多臂老虎机最佳臂识别，分析图并得出结果，凸显量子行走在受限环境探索的潜力。


<details>
  <summary>Details</summary>
Motivation: 图多臂老虎机问题的量子方法有限，需开发相关量子算法。

Method: 采用量子行走对图约束动作进行叠加编码，扩展振幅放大，通过Szegedy行走框架推广量子BAI算法。

Result: 分析了完全图和二部图，得出识别最佳臂的最大成功概率和达到该概率的时间步。

Conclusion: 量子行走有潜力加速受限环境的探索，可扩展量子算法在决策中的应用。

Abstract: Quantum reinforcement learning has emerged as a framework combining quantum
computation with sequential decision-making, and applications to the
multi-armed bandit (MAB) problem have been reported. The graph bandit problem
extends the MAB setting by introducing spatial constraints, yet quantum
approaches remain limited. We propose a quantum algorithm for best-arm
identification in graph bandits, termed Quantum Spatial Best-Arm Identification
(QSBAI). The method employs quantum walks to encode superpositions over
graph-constrained actions, extending amplitude amplification and generalizing
the Quantum BAI algorithm via Szegedy's walk framework. This establishes a link
between Grover-type search and reinforcement learning tasks with structural
restrictions. We analyze complete and bipartite graphs, deriving the maximal
success probability of identifying the best arm and the time step at which it
is achieved. Our results highlight the potential of quantum walks to accelerate
exploration in constrained environments and extend the applicability of quantum
algorithms for decision-making.

</details>


### [466] [Learning spatially structured open quantum dynamics with regional-attention transformers](https://arxiv.org/abs/2509.06871)
*Dounan Du,Eden Figueroa*

Main category: quant-ph

TL;DR: 提出基于区域注意力的神经网络架构学习结构化开放量子系统的时空动力学，在两个代表性系统上验证，有高预测保真度和显著加速。


<details>
  <summary>Details</summary>
Motivation: 经典数值求解器在模拟和优化含空间结构和外部控制的开放量子系统动力学时计算需求大，难以用于网络规模模拟或反馈控制。

Method: 引入基于区域注意力的神经网络架构，将物理定律的平移不变性作为归纳偏置，支持依赖时间的全局控制参数。

Result: 在两个代表性系统上学习，在分布内和分布外控制协议下都有高预测保真度，比数值求解器加速达三个数量级。

Conclusion: 该架构为空间结构化开放量子动力学建立了通用替代建模框架，适用于大规模量子网络模拟等多个领域。

Abstract: Simulating the dynamics of open quantum systems with spatial structure and
external control is an important challenge in quantum information science.
Classical numerical solvers for such systems require integrating coupled master
and field equations, which is computationally demanding for simulation and
optimization tasks and often precluding real-time use in network-scale
simulations or feedback control. We introduce a regional attention-based neural
architecture that learns the spatiotemporal dynamics of structured open quantum
systems. The model incorporates translational invariance of physical laws as an
inductive bias to achieve scalable complexity, and supports conditioning on
time-dependent global control parameters. We demonstrate learning on two
representative systems: a driven dissipative single qubit and an
electromagnetically induced transparency (EIT) quantum memory. The model
achieves high predictive fidelity under both in-distribution and
out-of-distribution control protocols, and provides substantial acceleration up
to three orders of magnitude over numerical solvers. These results demonstrate
that the architecture establishes a general surrogate modeling framework for
spatially structured open quantum dynamics, with immediate relevance to
large-scale quantum network simulation, quantum repeater and protocol design,
real-time experimental optimization, and scalable device modeling across
diverse light-matter platforms.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [467] [Machine learning magnetism from simple global descriptors](https://arxiv.org/abs/2509.05909)
*Ahmed E. Fahmy*

Main category: cond-mat.mtrl-sci

TL;DR: 开发机器学习分类器解决高通量材料数据库中磁基态识别难题，取得高准确率，凸显机器学习技术作用。


<details>
  <summary>Details</summary>
Motivation: 解决高通量材料数据库中磁基态可靠识别的难题，因DFT工作流常收敛到铁磁解。

Method: 利用Materials Project数据库的简单描述符，在实验验证的MAGNDATA磁性材料上训练机器学习分类器。

Result: 传播矢量分类器准确率超92%，LightGBM和XGBoost模型准确率84 - 86%，宏F1平均得分63 - 66%，发现数据库铁磁偏差。

Conclusion: 机器学习技术可作为校正和探索工具，有助于建立更可靠数据库，加速材料特性识别。

Abstract: The reliable identification of magnetic ground states remains a major
challenge in high-throughput materials databases, where density functional
theory (DFT) workflows often converge to ferromagnetic (FM) solutions. Here, we
partially address this challenge by developing machine learning classifiers
trained on experimentally validated MAGNDATA magnetic materials leveraging a
limited number of simple compositional, structural, and electronic descriptors
sourced from the Materials Project database. Our propagation vector classifiers
achieve accuracies above 92%, outperforming recent studies in reliably
distinguishing zero from nonzero propagation vector structures, and exposing a
systematic ferromagnetic bias inherent to the Materials Project database for
more than 7,843 materials. In parallel, LightGBM and XGBoost models trained
directly on the Materials Project labels achieve accuracies of 84-86% (with
macro F1 average scores of 63-66%), which proves useful for large-scale
screening for magnetic classes, if refined by MAGNDATA-trained classifiers.
These results underscore the role of machine learning techniques as corrective
and exploratory tools, enabling more trustworthy databases and accelerating
progress toward the identification of materials with various properties.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [468] [MIO: Multiverse Debugging in the Face of Input/Output -- Extended Version with Additional Appendices](https://arxiv.org/abs/2509.06845)
*Tom Lauwaerts,Maarten Steevens,Christophe Scholliers*

Main category: cs.PL

TL;DR: 现有多宇宙调试器调试含输入输出操作的程序有问题，本文提出新方法MIO，可处理多种输入输出操作，只探索常规可达状态，并开发原型验证。


<details>
  <summary>Details</summary>
Motivation: 现有多宇宙调试器调试含输入输出操作的程序时会暴露不可达状态，阻碍调试过程。

Method: 提出一种新的多宇宙调试方法，给出语义并证明调试器正确性，开发基于WARDuino WebAssembly虚拟机的原型MIO。

Result: 开发了原型MIO，通过乐高Mindstorms电机和颜色传感器搭建的颜色拨号盘展示了该方法在STM32微控制器上进行多宇宙调试的可行性和效率。

Conclusion: 新的多宇宙调试方法能适应广泛的输入输出操作，且只探索常规执行可到达的程序状态。

Abstract: Debugging non-deterministic programs on microcontrollers is notoriously
challenging, especially when bugs manifest in unpredictable, input-dependent
execution paths. A recent approach, called multiverse debugging, makes it
easier to debug non-deterministic programs by allowing programmers to explore
all potential execution paths. Current multiverse debuggers enable both forward
and backward traversal of program paths, and some facilitate jumping to any
previously visited states, potentially branching into alternative execution
paths within the state space.
  Unfortunately, debugging programs that involve input/output operations using
existing multiverse debuggers can reveal inaccessible program states, i.e.
states which are not encountered during regular execution. This can
significantly hinder the debugging process, as the programmer may spend
substantial time exploring and examining inaccessible program states, or worse,
may mistakenly assume a bug is present in the code, when in fact, the issue is
caused by the debugger.
  This paper presents a novel approach to multiverse debugging, which can
accommodate a broad spectrum of input/output operations. We provide the
semantics of our approach and prove the correctness of our debugger, ensuring
that despite having support for a wide range of input/output operations the
debugger will only explore those program states which can be reached during
regular execution.
  We have developed a prototype, called MIO, leveraging the WARDuino
WebAssembly virtual machine to demonstrate the feasibility and efficiency of
our techniques. As a demonstration of the approach we highlight a color dial
built with a Lego Mindstorms motor, and color sensor, providing a tangible
example of how our approach enables multiverse debugging for programs running
on an STM32 microcontroller.

</details>


### [469] [Dato: A Task-Based Programming Model for Dataflow Accelerators](https://arxiv.org/abs/2509.06794)
*Shihan Fang,Hongzheng Chen,Niansong Zhang,Jiajie Li,Han Meng,Adrian Liu,Zhiru Zhang*

Main category: cs.PL

TL;DR: 提出用于数据流加速器的编程模型Dato，在NPU和FPGA上实验表明它能高性能且降低编码负担。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习工作负载计算需求超出内存系统能力，现有编程模型难以有效利用数据流加速器功能。

Method: 提出Python嵌入式、基于任务的编程模型Dato，将数据通信和分片提升为一等类型构造，任务先虚拟映射到加速器，编译器再生成物理映射。

Result: 在NPU上GEMM硬件利用率达84%，注意力内核加速2.81倍；在FPGA上生成自定义脉动阵列时性能超领先框架，达理论峰值性能98%。

Conclusion: Dato能实现高性能并显著减轻编写优化代码的负担。

Abstract: Recent deep learning workloads increasingly push computational demand beyond
what current memory systems can sustain, with many kernels stalling on data
movement rather than computation. While modern dataflow accelerators
incorporate on-chip streaming to mitigate off-chip bandwidth limitations,
existing programming models struggle to harness these capabilities effectively.
Low-level interfaces provide fine-grained control but impose significant
development overhead, whereas high-level tile-based languages abstract away
communication details, restricting optimization and forcing compilers to
reconstruct the intended dataflow. We present Dato, a Python-embedded,
task-based programming model for dataflow accelerators that elevates data
communication and sharding to first-class type constructs. Developers write
programs as a graph of tasks connected via explicit stream types, with sharded
inputs specified using layout types. These tasks are first mapped virtually
onto the accelerator's spatial fabric, and the compiler then generates a
physical mapping that respects hardware constraints. Experimental results on
both AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves
high performance while significantly reducing the burden of writing optimized
code. On the NPU, Dato attains up to 84% hardware utilization for GEMM and
delivers a 2.81x speedup on attention kernels compared to a state-of-the-art
commercial framework. On the FPGA, Dato surpasses leading frameworks in
performance when generating custom systolic arrays, achieving 98% of the
theoretical peak performance.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [470] [ProtSAE: Disentangling and Interpreting Protein Language Models via Semantically-Guided Sparse Autoencoders](https://arxiv.org/abs/2509.05309)
*Xiangyu Liu,Haodi Lei,Yi Liu,Yang Liu,Wei Hu*

Main category: q-bio.QM

TL;DR: 本文提出语义引导的稀疏自编码器ProtSAE，解决SAE语义纠缠问题，实验表明其能学习更具生物学意义和可解释性的特征，在重建和可解释探测中表现良好，还可用于下游生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有SAE用于蛋白质语言模型时存在语义纠缠问题，难以可靠解释或操纵模型行为。

Method: 提出ProtSAE，在训练中结合注释数据集和领域知识引导语义解缠。

Result: ProtSAE学习到更具生物学相关性和可解释性的隐藏特征，保持高重建保真度，在可解释探测中取得更好结果，还展示了在下游生成任务中的潜力。

Conclusion: ProtSAE能有效解决SAE语义纠缠问题，在蛋白质语言模型的特征提取和解释方面表现优异，具有实际应用价值。

Abstract: Sparse Autoencoder (SAE) has emerged as a powerful tool for mechanistic
interpretability of large language models. Recent works apply SAE to protein
language models (PLMs), aiming to extract and analyze biologically meaningful
features from their latent spaces. However, SAE suffers from semantic
entanglement, where individual neurons often mix multiple nonlinear concepts,
making it difficult to reliably interpret or manipulate model behaviors. In
this paper, we propose a semantically-guided SAE, called ProtSAE. Unlike
existing SAE which requires annotation datasets to filter and interpret
activations, we guide semantic disentanglement during training using both
annotation datasets and domain knowledge to mitigate the effects of entangled
attributes. We design interpretability experiments showing that ProtSAE learns
more biologically relevant and interpretable hidden features compared to
previous methods. Performance analyses further demonstrate that ProtSAE
maintains high reconstruction fidelity while achieving better results in
interpretable probing. We also show the potential of ProtSAE in steering PLMs
for downstream generation tasks.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [471] [Using Reinforcement Learning to Optimize the Global and Local Crossing Number](https://arxiv.org/abs/2509.06108)
*Timo Brand,Henry Förster,Stephen Kobourov,Robin Schukrafft,Markus Wallinger,Johannes Zink*

Main category: cs.CG

TL;DR: 提出基于强化学习的图绘制新方法，对比多种算法，当前算法在局部交叉数上有竞争力，有进一步发展潜力。


<details>
  <summary>Details</summary>
Motivation: 最小化图的全局和局部交叉数，提出新的图绘制方法。

Method: 使用基于强化学习的方法，让智能体根据观测向量移动顶点，利用基于应力的图绘制算法生成初始布局，并与多种基线算法对比。

Result: 实验结果不一，当前算法主要在局部交叉数上有竞争力。

Conclusion: 该方法有进一步发展的潜力。

Abstract: We present a novel approach to graph drawing based on reinforcement learning
for minimizing the global and the local crossing number, that is, the total
number of edge crossings and the maximum number of crossings on any edge,
respectively. In our framework, an agent learns how to move a vertex based on a
given observation vector in order to optimize its position. The agent receives
feedback in the form of local reward signals tied to crossing reduction. To
generate an initial layout, we use a stress-based graph-drawing algorithm. We
compare our method against force- and stress-based (baseline) algorithms as
well as three established algorithms for global crossing minimization on a
suite of benchmark graphs. The experiments show mixed results: our current
algorithm is mainly competitive for the local crossing number. We see a
potential for further development of the approach in the future.

</details>
