<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]
- [cs.CE](#cs.CE) [Total: 5]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 67]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.SE](#cs.SE) [Total: 15]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 7]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.CG](#cs.CG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [econ.GN](#econ.GN) [Total: 5]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.RO](#cs.RO) [Total: 6]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.NI](#cs.NI) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.DL](#cs.DL) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.CL](#cs.CL) [Total: 11]
- [cs.CY](#cs.CY) [Total: 3]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 29]
- [cs.CR](#cs.CR) [Total: 13]
- [math.NA](#math.NA) [Total: 1]
- [math.ST](#math.ST) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Navigating Taxonomic Expansions of Entity Sets Driven by Knowledge Bases](https://arxiv.org/abs/2512.16953)
*Pietro Cofone,Giovanni Amendola,Marco Manna,Aldo Ricioppo*

Main category: cs.AI

TL;DR: 本文聚焦实体集扩展任务，针对现有逻辑框架中图过大难以完全实例化的问题，形式化推理任务，结果显示在现实假设下可高效实现，支持局部增量导航。


<details>
  <summary>Details</summary>
Motivation: 现有“线性”实体集扩展方法无法揭示知识资源中丰富的分类结构，基于逻辑的扩展图框架虽支持分类扩展，但图过大在现实中难以完全实例化。

Method: 形式化推理任务，检查图中两个元组是否属于可比、不可比或相同节点。

Result: 在现实假设下，如限制输入或实体描述，推理任务可高效实现。

Conclusion: 可实现扩展图的局部增量导航，支持实际应用，无需完整构建图。

Abstract: Recognizing similarities among entities is central to both human cognition and computational intelligence. Within this broader landscape, Entity Set Expansion is one prominent task aimed at taking an initial set of (tuples of) entities and identifying additional ones that share relevant semantic properties with the former -- potentially repeating the process to form increasingly broader sets. However, this ``linear'' approach does not unveil the richer ``taxonomic'' structures present in knowledge resources. A recent logic-based framework introduces the notion of an expansion graph: a rooted directed acyclic graph where each node represents a semantic generalization labeled by a logical formula, and edges encode strict semantic inclusion. This structure supports taxonomic expansions of entity sets driven by knowledge bases. Yet, the potentially large size of such graphs may make full materialization impractical in real-world scenarios. To overcome this, we formalize reasoning tasks that check whether two tuples belong to comparable, incomparable, or the same nodes in the graph. Our results show that, under realistic assumptions -- such as bounding the input or limiting entity descriptions -- these tasks can be implemented efficiently. This enables local, incremental navigation of expansion graphs, supporting practical applications without requiring full graph construction.

</details>


### [2] [Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows](https://arxiv.org/abs/2512.16969)
*Wanghan Xu,Yuhao Zhou,Yifan Zhou,Qinglong Cao,Shuo Li,Jia Bu,Bo Liu,Yixin Chen,Xuming He,Xiangyu Zhao,Xiang Zhuang,Fengxiang Wang,Zhiwang Zhou,Qiantai Feng,Wenxuan Huang,Jiaqi Wei,Hao Wu,Yuejin Yang,Guangshuai Wang,Sheng Xu,Ziyan Huang,Xinyao Liu,Jiyao Liu,Cheng Tang,Wei Li,Ying Chen,Junzhi Ning,Pengfei Jiang,Chenglong Ma,Ye Du,Changkai Ji,Huihui Xu,Ming Hu,Jiangbin Zheng,Xin Chen,Yucheng Wu,Feifei Jiang,Xi Chen,Xiangru Tang,Yuchen Fu,Yingzhou Lu,Yuanyuan Zhang,Lihao Sun,Chengbo Li,Jinzhe Ma,Wanhao Liu,Yating Liu,Kuo-Cheng Wu,Shengdu Chai,Yizhou Wang,Ouwen Zhangjin,Chen Tang,Shufei Zhang,Wenbo Cao,Junjie Ren,Taoyong Cui,Zhouheng Yao,Juntao Deng,Yijie Sun,Feng Liu,Wangxu Wei,Jingyi Xu,Zhangrui Li,Junchao Gong,Zijie Guo,Zhiyu Yao,Zaoyu Chen,Tianhao Peng,Fangchen Yu,Bo Zhang,Dongzhan Zhou,Shixiang Tang,Jiaheng Liu,Fenghua Ling,Yan Lu,Yuchen Ren,Ben Fei,Zhen Zhao,Xinyu Gu,Rui Su,Xiao-Ming Wu,Weikang Si,Yang Liu,Hao Chen,Xiangchao Yan,Xue Yang,Junchi Yan,Jiamin Wu,Qihao Zheng,Chenhui Li,Zhiqiang Gao,Hao Kong,Junjun He,Mao Su,Tianfan Fu,Peng Ye,Chunfeng Song,Nanqing Dong,Yuqiang Li,Huazhu Fu,Siqi Sun,Lijing Cheng,Jintai Lin,Wanli Ouyang,Bowen Zhou,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: 本文提出基于PIM的SGI定义，构建SGI - Bench评估大模型，发现模型存在诸多差距，还引入TTRL优化，为AI参与科学发现奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏科学通用智能（SGI）的连贯框架，需要对其进行定义和评估。

Method: 基于PIM提出SGI定义，通过四项与科学家对齐的任务进行操作化，构建SGI - Bench评估大模型，引入Test - Time Reinforcement Learning（TTRL）优化。

Result: 大模型在深度研究、想法生成、干湿实验和实验推理等方面存在差距，如深度研究精确匹配低、想法缺乏可行性和细节等。

Conclusion: 基于PIM的定义、以工作流为中心的基准和实证见解为AI真正参与科学发现奠定了基础。

Abstract: Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.

</details>


### [3] [PAACE: A Plan-Aware Automated Agent Context Engineering Framework](https://arxiv.org/abs/2512.16970)
*Kamer Ali Yuksel*

Main category: cs.AI

TL;DR: 提出PAACE框架优化LLM智能体上下文，实验显示能提升正确性并降低上下文负载，PAACE - FT可降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有总结和查询感知压缩工作忽视智能体推理多步骤、规划感知的特性，需处理复杂多步骤工作流中快速扩展的上下文。

Method: 引入PAACE框架，含PAACE - Syn生成合成智能体工作流，PAACE - FT训练蒸馏的规划感知压缩器。

Result: 在长时基准测试中，PAACE提升智能体正确性、降低上下文负载；PAACE - FT保留97%性能，降低超一个数量级推理成本。

Conclusion: PAACE框架能有效优化LLM智能体的上下文状态，PAACE - FT使规划感知压缩能通过紧凑模型实际部署。

Abstract: Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.

</details>


### [4] [Security Risks of Agentic Vehicles: A Systematic Analysis of Cognitive and Cross-Layer Threats](https://arxiv.org/abs/2512.17041)
*Ali Eslami,Jiangbo Yu*

Main category: cs.AI

TL;DR: 本文研究智能车辆（AgVs）的安全威胁，引入基于角色的架构，构建分析框架。


<details>
  <summary>Details</summary>
Motivation: 现有OWASP框架不适用于车辆等安全关键的网络物理平台，不能涵盖各层交互风险，因此需研究AgVs的安全威胁。

Method: 引入包含个人代理和驾驶策略代理的基于角色的架构，借助严重性矩阵和攻击链分析。

Result: 发现小的畸变会在有人驾驶和自动驾驶车辆中升级为不安全行为。

Conclusion: 该框架为分析当前和新兴车辆平台中智能AI的安全风险提供了首个结构化基础。

Abstract: Agentic AI is increasingly being explored and introduced in both manually driven and autonomous vehicles, leading to the notion of Agentic Vehicles (AgVs), with capabilities such as memory-based personalization, goal interpretation, strategic reasoning, and tool-mediated assistance. While frameworks such as the OWASP Agentic AI Security Risks highlight vulnerabilities in reasoning-driven AI systems, they are not designed for safety-critical cyber-physical platforms such as vehicles, nor do they account for interactions with other layers such as perception, communication, and control layers. This paper investigates security threats in AgVs, including OWASP-style risks and cyber-attacks from other layers affecting the agentic layer. By introducing a role-based architecture for agentic vehicles, consisting of a Personal Agent and a Driving Strategy Agent, we will investigate vulnerabilities in both agentic AI layer and cross-layer risks, including risks originating from upstream layers (e.g., perception layer, control layer, etc.). A severity matrix and attack-chain analysis illustrate how small distortions can escalate into misaligned or unsafe behavior in both human-driven and autonomous vehicles. The resulting framework provides the first structured foundation for analyzing security risks of agentic AI in both current and emerging vehicle platforms.

</details>


### [5] [UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering](https://arxiv.org/abs/2512.17043)
*Yinxu Tang,Chengsong Huang,Jiaxin Huang,William Yeoh*

Main category: cs.AI

TL;DR: 提出关系中心的KGQA设置，介绍挑战并提出UniRel - R1框架，实验显示其优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统KGQA关注实体中心查询，而现实世界查询多为关系型，需提出关系中心的KGQA设置。

Method: 提出UniRel - R1框架，集成子图选择、多阶段图剪枝，用强化学习微调大语言模型，设计奖励函数。

Result: UniRel - R1在连通性和奖励方面比基线有显著提升，能有效泛化到未见实体和关系。

Conclusion: UniRel - R1是解决关系中心KGQA问题的有效方法。

Abstract: Knowledge Graph Question Answering (KGQA) has traditionally focused on entity-centric queries that return a single answer entity. However, real-world queries are often relational, seeking to understand how entities are associated. In this work, we introduce relation-centric KGQA, a complementary setting where the answer is a subgraph capturing the semantic connections among entities rather than an individual entity. The main challenge lies in the abundance of candidate subgraphs, where trivial or overly common connections often obscure the identification of unique and informative answers. To tackle this, we propose UniRel-R1, a unified framework that integrates subgraph selection, multi-stage graph pruning, and an LLM fine-tuned with reinforcement learning. The reward function is designed to encourage compact and specific subgraphs with more informative relations and lower-degree intermediate entities. Extensive experiments show that UniRel-R1 achieves significant gains in connectivity and reward over Vanilla baselines and generalizes effectively to unseen entities and relations.

</details>


### [6] [Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations](https://arxiv.org/abs/2512.17066)
*Suhaib Abdurahman,Farzan Karimi-Malekabadi,Chenxiao Yu,Nour S. Kteily,Morteza Dehghani*

Main category: cs.AI

TL;DR: 使用大语言模型驱动的代理模拟虚拟社会，研究现实和象征威胁如何引发人类冲突。


<details>
  <summary>Details</summary>
Motivation: 现有研究在探究物质条件和象征价值威胁如何相互作用及何者占主导时，受因果控制弱、伦理限制和时间数据稀缺的限制。

Method: 在虚拟社会中对大语言模型驱动的代理进行模拟，独立改变现实和象征威胁，跟踪行为、语言和态度。

Result: 大语言模型将现实威胁、象征威胁和敌意编码为不同内部状态；现实威胁直接增加敌意，象征威胁作用较弱且通过内群体偏见起作用，仅在无现实威胁时增加敌意；非敌对的群体间接触可缓冲冲突升级，结构不对称使多数群体敌意更集中。

Conclusion: 该模拟提供了随时间变化的威胁驱动冲突的因果解释。

Abstract: Human conflict is often attributed to threats against material conditions and symbolic values, yet it remains unclear how they interact and which dominates. Progress is limited by weak causal control, ethical constraints, and scarce temporal data. We address these barriers using simulations of large language model (LLM)-driven agents in virtual societies, independently varying realistic and symbolic threat while tracking actions, language, and attitudes. Representational analyses show that the underlying LLM encodes realistic threat, symbolic threat, and hostility as distinct internal states, that our manipulations map onto them, and that steering these states causally shifts behavior. Our simulations provide a causal account of threat-driven conflict over time: realistic threat directly increases hostility, whereas symbolic threat effects are weaker, fully mediated by ingroup bias, and increase hostility only when realistic threat is absent. Non-hostile intergroup contact buffers escalation, and structural asymmetries concentrate hostility among majority groups.

</details>


### [7] [Value Under Ignorance in Universal Artificial Intelligence](https://arxiv.org/abs/2512.17086)
*Cole Wyeth,Marcus Hutter*

Main category: cs.AI

TL;DR: 将AIXI强化学习智能体扩展以接纳更多效用函数，探讨不同效用分配方式及Choquet积分计算期望效用的后果。


<details>
  <summary>Details</summary>
Motivation: 处理AIXI智能体信念分布中部分假设只能预测有限历史前缀的模糊性问题，为历史前缀分配效用。

Method: 将信念分布视为不精确概率分布，用不精确概率理论中的Choquet积分计算期望效用，并研究其可计算性。

Result: 标准递归价值函数是特殊情况，死亡解释下最通用的期望效用不能用Choquet积分表征。

Conclusion: 提出了处理智能体效用分配模糊性的新视角和方法。

Abstract: We generalize the AIXI reinforcement learning agent to admit a wider class of utility functions. Assigning a utility to each possible interaction history forces us to confront the ambiguity that some hypotheses in the agent's belief distribution only predict a finite prefix of the history, which is sometimes interpreted as implying a chance of death equal to a quantity called the semimeasure loss. This death interpretation suggests one way to assign utilities to such history prefixes. We argue that it is as natural to view the belief distributions as imprecise probability distributions, with the semimeasure loss as total ignorance. This motivates us to consider the consequences of computing expected utilities with Choquet integrals from imprecise probability theory, including an investigation of their computability level. We recover the standard recursive value function as a special case. However, our most general expected utilities under the death interpretation cannot be characterized as such Choquet integrals.

</details>


### [8] [A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving](https://arxiv.org/abs/2512.17093)
*Timo Pierre Schrader,Lukas Lange,Tobias Kaminski,Simon Razniewski,Annemarie Friedrich*

Main category: cs.AI

TL;DR: 本文聚焦大语言模型在回答集编程（ASP）代码生成上的问题，提出新的ASP求解器在环方法来指导指令微调，实验显示有改进。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成特定领域语言代码有挑战，ASP代码生成因预训练阶段所见示例有限而效果不佳。

Method: 引入ASP求解器在环方法，根据求解器反馈对采样的ASP语句分类，进行监督微调并结合求解器引导搜索。

Result: 实验在两个数据集的两种提示设置下均有持续改进。

Conclusion: 所提出的ASP求解器在环方法能有效提升大语言模型在ASP代码生成上的性能。

Abstract: The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase.
  In this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.

</details>


### [9] [Reinforcement Learning for Self-Improving Agent with Skill Library](https://arxiv.org/abs/2512.17102)
*Jiongxiao Wang,Qiaojing Yan,Yawei Wang,Yijun Tian,Soumya Smruti Mishra,Zhichao Xu,Megha Gandhi,Panpan Xu,Lin Lee Cheong*

Main category: cs.AI

TL;DR: 提出基于强化学习的方法SAGE增强代理自我改进能力，在AppWorld实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的代理在新环境中难以持续改进和适应，现有技能库实现方法有挑战。

Method: 提出SAGE框架，将技能系统融入学习，利用Sequential Rollout和Skill-integrated Reward。

Result: 在AppWorld实验显示，相比现有方法，场景目标完成率提高8.9%，交互步骤减少26%，生成令牌减少59%。

Conclusion: SAGE在准确性和效率上显著优于现有方法。

Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.

</details>


### [10] [Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty](https://arxiv.org/abs/2512.17145)
*Josh Barber,Rourke Young,Cameron Coombe,Will Browne*

Main category: cs.AI

TL;DR: 提出受Solomonoff启发的方法用于不确定推理，在基准任务上表现良好，凸显算法信息论先验的价值。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估多个候选解决方案时难以平衡准确性和简单性，且在处理稀疏数据的现实任务时需要系统的泛化能力。

Method: 提出受Solomonoff启发的方法，根据简单性和预测拟合度对大语言模型生成的假设进行加权。

Result: 在基准任务上产生Solomonoff加权的混合物用于每个单元格的预测，输出保守且能感知不确定性；与贝叶斯模型平均法相比，Solomonoff评分能更均匀地分配概率。

Conclusion: 算法信息论先验对不确定情况下可解释、可靠的多假设推理有价值。

Abstract: Reasoning under uncertainty is a key challenge in AI, especially for real-world tasks, where problems with sparse data demands systematic generalisation. Existing approaches struggle to balance accuracy and simplicity when evaluating multiple candidate solutions. We propose a Solomonoff-inspired method that weights LLM-generated hypotheses by simplicity and predictive fit. Applied to benchmark (Mini-ARC) tasks, our method produces Solomonoff-weighted mixtures for per-cell predictions, yielding conservative, uncertainty-aware outputs even when hypotheses are noisy or partially incorrect. Compared to Bayesian Model Averaging (BMA), Solomonoff scoring spreads probability more evenly across competing hypotheses, while BMA concentrates weight on the most likely but potentially flawed candidates. Across tasks, this highlights the value of algorithmic information-theoretic priors for interpretable, reliable multi-hypothesis reasoning under uncertainty.

</details>


### [11] [MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation](https://arxiv.org/abs/2512.17194)
*Shengwei Zhao,Jingwen Yao,Sitong Wei,Linhai Xu,Yuying Liu,Dong Zhang,Zhiqiang Tian,Shaoyi Du*

Main category: cs.AI

TL;DR: 现有MMRAG方法结果缺乏可解释性，本文提出两阶段强化微调框架实现可解释的多模态检索增强生成，在基准数据集取得SOTA成果。


<details>
  <summary>Details</summary>
Motivation: 现有MMRAG方法无法阐明检索和响应生成背后的推理逻辑，限制了结果的可解释性。

Method: 引入强化学习到多模态检索增强生成中，通过两阶段强化微调框架增强多模态大语言模型的推理能力。第一阶段基于规则进行粗粒度逐点排序过滤无关文档；第二阶段基于推理联合优化细粒度列表排序和答案生成。

Result: 在WebQA和MultimodalQA两个基准数据集上取得了SOTA结果，通过全面的消融实验验证了有效性。

Conclusion: 所提出的方法能实现可解释的多模态检索增强生成，提升了多模态大语言模型的推理能力和结果可解释性。

Abstract: Multi-modal Retrieval-Augmented Generation (MMRAG) enables highly credible generation by integrating external multi-modal knowledge, thus demonstrating impressive performance in complex multi-modal scenarios. However, existing MMRAG methods fail to clarify the reasoning logic behind retrieval and response generation, which limits the explainability of the results. To address this gap, we propose to introduce reinforcement learning into multi-modal retrieval-augmented generation, enhancing the reasoning capabilities of multi-modal large language models through a two-stage reinforcement fine-tuning framework to achieve explainable multi-modal retrieval-augmented generation. Specifically, in the first stage, rule-based reinforcement fine-tuning is employed to perform coarse-grained point-wise ranking of multi-modal documents, effectively filtering out those that are significantly irrelevant. In the second stage, reasoning-based reinforcement fine-tuning is utilized to jointly optimize fine-grained list-wise ranking and answer generation, guiding multi-modal large language models to output explainable reasoning logic in the MMRAG process. Our method achieves state-of-the-art results on WebQA and MultimodalQA, two benchmark datasets for multi-modal retrieval-augmented generation, and its effectiveness is validated through comprehensive ablation experiments.

</details>


### [12] [UmniBench: Unified Understand and Generation Model Oriented Omni-dimensional Benchmark](https://arxiv.org/abs/2512.17196)
*Kai Liu,Leyang Chen,Wenbo Li,Zhikai Chen,Zhixin Wang,Renjing Pei,Linghe Kong,Yulun Zhang*

Main category: cs.AI

TL;DR: 提出针对统一多模态模型（UMMs）的OmniBench基准，可全维度评估UMMs，还对24个流行模型进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有UMMs评估解耦，分别用对应数据集评估理解和生成能力，需更全面评估方法。

Method: 提出OmniBench基准，在单个评估过程中评估理解、生成和编辑能力，基于人工检查的提示和问答对，利用UMMs自身评估其能力，覆盖13个主要领域和超200个概念，还可解耦单独评估。

Result: 基于OmniBench对24个流行模型进行基准测试。

Conclusion: 该基准能为统一模型提供更全面客观的视角，为提升社区模型性能提供后勤支持。

Abstract: Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. However, evaluations of unified multimodal models (UMMs) remain decoupled, assessing their understanding and generation abilities separately with corresponding datasets. To address this, we propose UmniBench, a benchmark tailored for UMMs with omni-dimensional evaluation. First, UmniBench can assess the understanding, generation, and editing ability within a single evaluation process. Based on human-examined prompts and QA pairs, UmniBench leverages UMM itself to evaluate its generation and editing ability with its understanding ability. This simple but effective paradigm allows comprehensive evaluation of UMMs. Second, UmniBench covers 13 major domains and more than 200 concepts, ensuring a thorough inspection of UMMs. Moreover, UmniBench can also decouple and separately evaluate understanding, generation, and editing abilities, providing a fine-grained assessment. Based on UmniBench, we benchmark 24 popular models, including both UMMs and single-ability large models. We hope this benchmark provides a more comprehensive and objective view of unified models and logistical support for improving the performance of the community model.

</details>


### [13] [Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction](https://arxiv.org/abs/2512.17250)
*Ziyang Lin,Zixuan Sun,Sanhorn Chen,Xiaoyang Chen,Roy Zhao*

Main category: cs.AI

TL;DR: 提出推测与修正框架用于基于模型的控制，减少规划推理次数和延迟，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 实时顺序控制代理受推理延迟瓶颈，适度规划延迟会破坏控制稳定性和降低性能。

Method: 提出推测与修正框架，结合预测 - 验证理念，使用预训练世界模型和潜在空间 MPC 规划器生成动作队列，根据观测与预测潜在状态的不匹配程度进行修正或重新规划，研究两种校正器。

Result: 在 DMC Humanoid - Walk 任务中，将规划推理次数从 500 减至 282，端到端步骤延迟改善 25%，回报仅降低 7.1%。

Conclusion: 推测执行若无修正，长时不可靠，基于不匹配感知的修正对稳健降低延迟很必要。

Abstract: Real-time sequential control agents are often bottlenecked by inference latency. Even modest per-step planning delays can destabilize control and degrade overall performance. We propose a speculation-and-correction framework that adapts the predict-then-verify philosophy of speculative execution to model-based control with TD-MPC2. At each step, a pretrained world model and latent-space MPC planner generate a short-horizon action queue together with predicted latent rollouts, allowing the agent to execute multiple planned actions without immediate replanning. When a new observation arrives, the system measures the mismatch between the encoded real latent state and the queued predicted latent. For small to moderate mismatch, a lightweight learned corrector applies a residual update to the speculative action, distilled offline from a replanning teacher. For large mismatch, the agent safely falls back to full replanning and clears stale action queues. We study both a gated two-tower MLP corrector and a temporal Transformer corrector to address local errors and systematic drift. Experiments on the DMC Humanoid-Walk task show that our method reduces the number of planning inferences from 500 to 282, improves end-to-end step latency by 25 percent, and maintains strong control performance with only a 7.1 percent return reduction. Ablation results demonstrate that speculative execution without correction is unreliable over longer horizons, highlighting the necessity of mismatch-aware correction for robust latency reduction.

</details>


### [14] [ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework](https://arxiv.org/abs/2512.17266)
*Miru Hong,Minho Lee,Geonhee Jo,Jae-Hee So,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: 提出EventGPT模型用于足球转会评估，在英超数据上表现优且有实用价值。


<details>
  <summary>Details</summary>
Motivation: 现有足球转会评估方法难以捕捉球员在新环境表现，需新方法。

Method: 引入基于GPT风格自回归变压器的EventGPT模型，处理比赛为离散令牌序列，可进行反事实模拟。

Result: EventGPT在预测准确性和空间精度上优于现有基线，案例研究证明其对转会分析实用。

Conclusion: EventGPT为评估转会适配性提供了原则性方法。

Abstract: Transfers play a pivotal role in shaping a football club's success, yet forecasting whether a transfer will succeed remains difficult due to the strong context-dependence of on-field performance. Existing evaluation practices often rely on static summary statistics or post-hoc value models, which fail to capture how a player's contribution adapts to a new tactical environment or different teammates. To address this gap, we introduce EventGPT, a player-conditioned, value-aware next-event prediction model built on a GPT-style autoregressive transformer. Our model treats match play as a sequence of discrete tokens, jointly learning to predict the next on-ball action's type, location, timing, and its estimated residual On-Ball Value (rOBV) based on the preceding context and player identity. A key contribution of this framework is the ability to perform counterfactual simulations. By substituting learned player embeddings into new event sequences, we can simulate how a player's behavioral distribution and value profile would change when placed in a different team or tactical structure. Evaluated on five seasons of Premier League event data, EventGPT outperforms existing sequence-based baselines in next-event prediction accuracy and spatial precision. Furthermore, we demonstrate the model's practical utility for transfer analysis through case studies-such as comparing striker performance across different systems and identifying stylistic replacements for specific roles-showing that our approach provides a principled method for evaluating transfer fit.

</details>


### [15] [Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation](https://arxiv.org/abs/2512.17308)
*Daksh Jain,Aarya Jain,Ashutosh Desai,Avyakt Verma,Ishan Bhanuka,Pratik Narang,Dhruv Kumar*

Main category: cs.AI

TL;DR: 本文探讨大语言模型能否成为宝可梦对战的合格代理，开发对战系统评估，结果显示其无需特定训练就能作为动态对手，兼具对战和内容创作能力。


<details>
  <summary>Details</summary>
Motivation: 检验大语言模型能否成为合格的宝可梦战斗代理，具备做出战术决策和生成新的平衡游戏内容的能力。

Method: 开发基于回合制的宝可梦战斗系统，让大语言模型根据战斗状态选择行动，通过跨多个模型架构的系统评估来衡量各项指标。

Result: 大语言模型无需特定领域训练就能作为动态游戏对手，可作为回合制战略游戏中强化学习的实用替代方案。

Conclusion: 大语言模型兼具战术推理和内容创作能力，可作为玩家和设计者，对交互式娱乐的程序生成和自适应难度系统有影响。

Abstract: Strategic decision-making in Pokémon battles presents a unique testbed for evaluating large language models. Pokémon battles demand reasoning about type matchups, statistical trade-offs, and risk assessment, skills that mirror human strategic thinking. This work examines whether Large Language Models (LLMs) can serve as competent battle agents, capable of both making tactically sound decisions and generating novel, balanced game content. We developed a turn-based Pokémon battle system where LLMs select moves based on battle state rather than pre-programmed logic. The framework captures essential Pokémon mechanics: type effectiveness multipliers, stat-based damage calculations, and multi-Pokémon team management. Through systematic evaluation across multiple model architectures we measured win rates, decision latency, type-alignment accuracy, and token efficiency. These results suggest LLMs can function as dynamic game opponents without domain-specific training, offering a practical alternative to reinforcement learning for turn-based strategic games. The dual capability of tactical reasoning and content creation, positions LLMs as both players and designers, with implications for procedural generation and adaptive difficulty systems in interactive entertainment.

</details>


### [16] [Dialectics for Artificial Intelligence](https://arxiv.org/abs/2512.17373)
*Zhengmian Hu*

Main category: cs.AI

TL;DR: 文章探讨人工智能能否无监督发现人类概念，提出算法信息视角定义概念，构建相关判断指标和辩证优化动态，还对概念传播和多智能体对齐进行形式化。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能能否从原始经验中无监督发现人类已有概念，且人类概念具有流动性，需新的概念定义。

Method: 提出算法信息视角，将概念视为与智能体总经验有结构关系的信息对象，定义核心约束和多余信息，构建辩证优化动态，形式化低代价概念传播和多智能体对齐。

Result: 提出了概念的定义、判断方法、优化动态以及概念传播和多智能体对齐的形式化方法。

Conclusion: 通过新的概念定义和相关方法，有望解决人工智能无监督发现人类概念的问题，并实现概念在多智能体间的有效传播和对齐。

Abstract: Can artificial intelligence discover, from raw experience and without human supervision, concepts that humans have discovered? One challenge is that human concepts themselves are fluid: conceptual boundaries can shift, split, and merge as inquiry progresses (e.g., Pluto is no longer considered a planet). To make progress, we need a definition of "concept" that is not merely a dictionary label, but a structure that can be revised, compared, and aligned across agents. We propose an algorithmic-information viewpoint that treats a concept as an information object defined only through its structural relation to an agent's total experience. The core constraint is determination: a set of parts forms a reversible consistency relation if any missing part is recoverable from the others (up to the standard logarithmic slack in Kolmogorov-style identities). This reversibility prevents "concepts" from floating free of experience and turns concept existence into a checkable structural claim. To judge whether a decomposition is natural, we define excess information, measuring the redundancy overhead introduced by splitting experience into multiple separately described parts. On top of these definitions, we formulate dialectics as an optimization dynamics: as new patches of information appear (or become contested), competing concepts bid to explain them via shorter conditional descriptions, driving systematic expansion, contraction, splitting, and merging. Finally, we formalize low-cost concept transmission and multi-agent alignment using small grounds/seeds that allow another agent to reconstruct the same concept under a shared protocol, making communication a concrete compute-bits trade-off.

</details>


### [17] [Translating the Rashomon Effect to Sequential Decision-Making Tasks](https://arxiv.org/abs/2512.17470)
*Dennis Gross,Jørn Eirik Betten,Helge Spieker*

Main category: cs.AI

TL;DR: 本文将Rashomon效应引入序列决策，定义其概念，用形式验证方法验证，实验证明该效应存在，还表明Rashomon集构建的集成更稳健，衍生的宽松策略可降低验证计算量。


<details>
  <summary>Details</summary>
Motivation: Rashomon效应在分类任务中被广泛研究，但未在序列决策中研究，因此将其引入序列决策。

Method: 定义序列决策中的Rashomon效应，使用形式验证方法构建和比较每个策略在环境中的完整概率行为。

Result: 实验证明Rashomon效应存在于序列决策中，Rashomon集构建的集成对分布偏移更稳健，衍生的宽松策略降低验证计算需求且保持最优性能。

Conclusion: Rashomon效应在序列决策中存在，且相关策略有其优势，如集成的稳健性和宽松策略的计算优势。

Abstract: The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.

</details>


### [18] [Towards Explainable Conversational AI for Early Diagnosis with Large Language Models](https://arxiv.org/abs/2512.17559)
*Maliha Tabassum,M Shamim Kaiser*

Main category: cs.AI

TL;DR: 研究引入基于大语言模型的诊断聊天机器人，测试表现出色，为医疗AI带来前景。


<details>
  <summary>Details</summary>
Motivation: 全球医疗系统存在诊断低效、成本上升和专家资源有限等问题，现有AI诊断系统缺乏交互性和透明度。

Method: 引入由大语言模型驱动的诊断聊天机器人，采用GPT - 4o、检索增强生成和可解释AI技术，通过动态对话提取和规范症状，用相似匹配和自适应提问确定潜在诊断，并用思维链提示提供透明推理。

Result: 与传统机器学习模型对比测试，该系统准确率达90%，前3准确率达100%。

Conclusion: 研究结果为医疗领域更透明、交互性强且临床相关的AI带来了有希望的前景。

Abstract: Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or transparent, making them less effective in real-world, patient-centered environments. This research introduces a diagnostic chatbot powered by a Large Language Model (LLM), using GPT-4o, Retrieval-Augmented Generation, and explainable AI techniques. The chatbot engages patients in a dynamic conversation, helping to extract and normalize symptoms while prioritizing potential diagnoses through similarity matching and adaptive questioning. With Chain-of-Thought prompting, the system also offers more transparent reasoning behind its diagnoses. When tested against traditional machine learning models like Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, the LLM-based system delivered impressive results, achieving an accuracy of 90% and Top-3 accuracy of 100%. These findings offer a promising outlook for more transparent, interactive, and clinically relevant AI in healthcare.

</details>


### [19] [About Time: Model-free Reinforcement Learning with Timed Reward Machines](https://arxiv.org/abs/2512.17637)
*Anirban Majumdar,Ritam Raha,Rajarshi Roy,David Parker,Marta Kwiatkowska*

Main category: cs.AI

TL;DR: 提出定时奖励机器（TRMs）扩展奖励机器，研究基于其的无模型强化学习框架，实验证明算法有效。


<details>
  <summary>Details</summary>
Motivation: 传统奖励机器无法建模精确时间约束，限制了在时间敏感应用中的使用。

Method: 提出TRMs，将其融入学习，采用基于定时自动机抽象和反事实想象启发式方法。

Result: 算法能学习到在满足时间约束同时获得高奖励的策略，对比不同语义和消融实验凸显反事实想象优势。

Conclusion: TRMs可实现更具表达性的奖励规范，所提算法在学习最优策略方面有效。

Abstract: Reward specification plays a central role in reinforcement learning (RL), guiding the agent's behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.

</details>


### [20] [Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally](https://arxiv.org/abs/2512.17898)
*Robin Schimmelpfennig,Mark Díaz,Vinodkumar Prabhakaran,Aida Davani*

Main category: cs.AI

TL;DR: 通过跨国实验研究拟人化AI设计影响，发现用户关注互动线索，设计对信任和参与度影响因文化而异，挑战固有风险叙事，提倡多元治理。


<details>
  <summary>Details</summary>
Motivation: 现有研究未在全球用户群体中测试类人AI设计与用户参与和信任的因果关系，且安全框架忽略全球用户多样性。

Method: 开展两项涉及10个不同国家3500人的大规模跨国实验，让用户与AI进行实时、开放式互动。

Result: 用户评估AI类人性时更关注互动线索；类人设计可增加拟人化，但不一定增加用户参与和信任，影响因文化而异。

Conclusion: 挑战类人AI设计固有风险叙事，应超越一刀切的AI治理方式，考虑文化因素。

Abstract: Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits. This shift has triggered urgent debate regarding Anthropomorphism, the attribution of human characteristics to synthetic agents, and its potential to induce misplaced trust or emotional dependency. However, the causal link between more humanlike AI design and subsequent effects on engagement and trust has not been tested in realistic human-AI interactions with a global user pool. Prevailing safety frameworks continue to rely on theoretical assumptions derived from Western populations, overlooking the global diversity of AI users. Here, we address these gaps through two large-scale cross-national experiments (N=3,500) across 10 diverse nations, involving real-time and open-ended interactions with an AI system. We find that when evaluating an AI's human-likeness, users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user's perspective. We also experimentally demonstrate that humanlike design levers can causally increase anthropomorphism among users; however, we do not find that humanlike design universally increases behavioral measures for user engagement and trust, as previous theoretical work suggests. Instead, part of the connection between human-likeness and behavioral outcomes is fractured by culture: specific design choices that foster self-reported trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan). Our findings challenge prevailing narratives of inherent risk in humanlike AI design. Instead, we identify a nuanced, culturally mediated landscape of human-AI interaction, which demands that we move beyond a one-size-fits-all approach in AI governance.

</details>


### [21] [When Reasoning Meets Its Laws](https://arxiv.org/abs/2512.17901)
*Junyu Zhang,Yifan Sun,Tianang Leng,Jingyan Shen,Liu Ziyin,Paul Pu Liang,Huan Zhang*

Main category: cs.AI

TL;DR: 论文提出推理定律（LoRe）框架刻画大推理模型推理模式，引入LoRe - Bench衡量相关属性，发现多数模型缺乏组合性，开发微调方法提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 大推理模型推理行为反直觉、推理能力欠佳，需理论形式化理想推理行为。

Method: 提出计算定律和补充的准确率定律，通过单调性和组合性两个属性检验假设，引入LoRe - Bench衡量属性，开发微调方法强化计算定律组合性。

Result: 多数推理模型有合理单调性但缺乏组合性，更好遵循计算定律可提升推理性能，发现属性和定律间的协同效应。

Conclusion: LoRe框架有效，遵循计算定律能改善大推理模型的推理性能。

Abstract: Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [22] [When to compute in space](https://arxiv.org/abs/2512.17054)
*Rajiv Thummala,Gregory Falco*

Main category: cs.CE

TL;DR: 本文介绍了一个定量优化框架，用于解决航天器计算资源部署位置选择问题，并通过两个代表性工作负载进行评估。


<details>
  <summary>Details</summary>
Motivation: 航天器依赖多种异构计算资源，计算位置选择是一个受多种因素驱动的多目标问题。

Method: 引入定量优化框架，通过可测量指标、归一化评分、可行性约束和统一效用函数进行计算位置选择。

Result: 在两个代表性工作负载上评估模型，展示了框架如何比较计算层级并确定首选部署位置。

Conclusion: 该方法为任务设计者在新兴空间架构中分析计算位置提供了结构化、可扩展的方法。

Abstract: Spacecraft increasingly rely on heterogeneous computing resources spanning onboard flight computers, orbital data centers, ground station edge nodes, and terrestrial cloud infrastructure. Selecting where a workload should execute is a nontrivial multi objective problem driven by latency, reliability, power, communication constraints, cost, and regulatory feasibility. This paper introduces a quantitative optimization framework that formalizes compute location selection through empirically measurable metrics, normalized scoring, feasibility constraints, and a unified utility function designed to operate under incomplete information. We evaluate the model on two representative workloads demonstrating how the framework compares compute tiers and identifies preferred deployment locations. The approach provides a structured, extensible method for mission designers to reason about compute placement in emerging space architectures.

</details>


### [23] [Flux-Preserving Adaptive Finite State Projection for Multiscale Stochastic Reaction Networks](https://arxiv.org/abs/2512.17064)
*Aditya Dendukuri,Shivkumar Chandrasekaran,Linda Petzold*

Main category: cs.CE

TL;DR: 提出基于通量的自适应FSP方法处理多尺度系统，实验表明该方法能在小状态空间下保持精度。


<details>
  <summary>Details</summary>
Motivation: 现有自适应FSP方法处理多尺度系统有挑战，低概率瓶颈状态影响大，系统存在快慢不同的动态机制。

Method: 提出基于通量的自适应FSP方法，用概率通量驱动状态空间修剪和时间步选择。

Result: 在刚性、振荡和瓶颈反应网络的数值实验中，该方法在使用小得多的状态空间时仍能保持精度。

Conclusion: 基于通量的自适应FSP方法有效，可处理多尺度系统。

Abstract: The Finite State Projection (FSP) method approximates the Chemical Master Equation (CME) by restricting the dynamics to a finite subset of the (typically infinite) state space, enabling direct numerical solution with computable error bounds. Adaptive variants update this subset in time, but multiscale systems with widely separated reaction rates remain challenging, as low-probability bottleneck states can carry essential probability flux and the dynamics alternate between fast transients and slowly evolving stiff regimes. We propose a flux-based adaptive FSP method that uses probability flux to drive both state-space pruning and time-step selection. The pruning rule protects low-probability states with large outgoing flux, preserving connectivity in bottleneck systems, while the time-step rule adapts to the instantaneous total flux to handle rate constants spanning several orders of magnitude. Numerical experiments on stiff, oscillatory, and bottleneck reaction networks show that the method maintains accuracy while using substantially smaller state spaces.

</details>


### [24] [A Parametric Framework for Anticipatory Flashflood Warning: Integrating Landscape Vulnerability with Precipitation Forecasts](https://arxiv.org/abs/2512.17785)
*Xiangpeng Li,Junwei Ma,Samuel D Brody,Ali Mostafavi*

Main category: cs.CE

TL;DR: 本文提出并验证了一个前瞻性的、参数化框架，能将景观脆弱性和降水转化为社区尺度的威胁等级，可应用于洪水预警，且支持应急管理的事前决策。


<details>
  <summary>Details</summary>
Motivation: 当前山洪预警大多是被动响应的，在疏散规划和资源预置方面提供的提前通知有限，因此需要一个前瞻性的预警框架。

Method: 首先利用暴雨洪水位、距最近排水系统高度和到溪流距离推导固有灾害可能性（IHL）面；接着通过将24小时降雨量与当地100年一遇24小时降雨量标准化计算灾害严重程度指数（HSI）；最后使用20个特定类别触发条件，将IHL和HSI整合到局部威胁严重性（LTS）矩阵中。

Result: 将LTS应用于两次德州洪水事件，与独立众包影响指标有显著的空间关联，能捕捉到观测到的灾害热点。

Conclusion: 该框架计算量小、可扩展，能将可行动态势感知延长到48 - 72小时的预期窗口，支持应急管理人员的事前决策。

Abstract: Flash flood warnings are largely reactive, providing limited advance notice for evacuation planning and resource prepositioning. This study presents and validates an anticipatory, parametric framework that converts landscape vulnerability and precipitation into transparent, zone-aware threat levels at neighborhood scales. We first derive an inherent hazard likelihood (IHL) surface using pluvial flood depth, height above nearest drainage, and distance to streams. Next, we compute a hazard severity index (HSI) by normalizing 24-hour rainfall against local Atlas-14 100-year, 24-hour depths. We then integrate IHL and HSI within a localized threat severity (LTS) matrix using 20 class-specific triggers, requiring lower exceedance in high-risk terrain and higher exceedance in uplands. Applied to two Texas flood events, the LTS exhibits statistically significant spatial association with independent crowdsourced impact proxies, capturing observed disruption hotspots. The framework is computationally lightweight, scalable, and extends actionable situational awareness into a 48-72 hour anticipatory window, supporting pre-event decision-making by emergency managers.

</details>


### [25] [Structure-Aware Antibody Design with Affinity-Optimized Inverse Folding](https://arxiv.org/abs/2512.17815)
*Xinyan Zhao,Yi-Ching Tang,Rivaaj Monsia,Victor J. Cantu,Ashwin Kumar Ramesh,Xiaozhong Liu,Zhiqiang An,Xiaoqian Jiang,Yejin Kim*

Main category: cs.CE

TL;DR: 本文提出SimBinder - IF，将逆折叠模型ESM - IF转化为抗体序列生成器，在基准测试中表现良好，参数效率高。


<details>
  <summary>Details</summary>
Motivation: 抗体疗法临床疗效依赖高亲和力靶点结合，但实验室亲和力成熟活动慢且成本高，多数蛋白质语言模型未针对高亲和力抗体训练，现有偏好优化方法计算开销大且亲和力提升不明显。

Method: 将逆折叠模型ESM - IF转化为抗体序列生成器，冻结其结构编码器，仅训练解码器，通过偏好优化使其倾向实验中结合力更强的抗体。

Result: 在11 - 检测AbBiBench基准上，与原始ESM - IF相比，对数似然分数和实验测量结合亲和力的平均Spearman相关性相对提高55%；在四个未见抗原 - 抗体复合物的零样本泛化中，相关性提高156%；在十倍或更大亲和力提升的前10精度上优于基线；案例研究显示其提出的变体预测结合自由能变化低于ESM - IF；仅训练约18%的完整ESM - IF模型参数。

Conclusion: SimBinder - IF在高亲和力抗体生成方面有良好表现，且参数效率高。

Abstract: Motivation: The clinical efficacy of antibody therapeutics critically depends on high-affinity target engagement, yet laboratory affinity-maturation campaigns are slow and costly. In computational settings, most protein language models (PLMs) are not trained to favor high-affinity antibodies, and existing preference optimization approaches introduce substantial computational overhead without clear affinity gains. Therefore, this work proposes SimBinder-IF, which converts the inverse folding model ESM-IF into an antibody sequence generator by freezing its structure encoder and training only its decoder to prefer experimentally stronger binders through preference optimization.
  Results: On the 11-assay AbBiBench benchmark, SimBinder-IF achieves a 55 percent relative improvement in mean Spearman correlation between log-likelihood scores and experimentally measured binding affinity compared to vanilla ESM-IF (from 0.264 to 0.410). In zero-shot generalization across four unseen antigen-antibody complexes, the correlation improves by 156 percent (from 0.115 to 0.294). SimBinder-IF also outperforms baselines in top-10 precision for ten-fold or greater affinity improvements. A case study redesigning antibody F045-092 for A/California/04/2009 (pdmH1N1) shows that SimBinder-IF proposes variants with substantially lower predicted binding free energy changes than ESM-IF (mean Delta Delta G -75.16 vs -46.57). Notably, SimBinder-IF trains only about 18 percent of the parameters of the full ESM-IF model, highlighting its parameter efficiency for high-affinity antibody generation.

</details>


### [26] [NeuRehab: A Reinforcement Learning and Spiking Neural Network-Based Rehab Automation Framework](https://arxiv.org/abs/2512.17841)
*Phani Pavan Kambhampati,Chainesh Gautam,Jagan Palaniswamy,Madhav Rao*

Main category: cs.CE

TL;DR: 提出NeuRehab框架优化机器人康复治疗系统，在步进电机肩部训练评估中表现良好，节能和降低延迟超60%。


<details>
  <summary>Details</summary>
Motivation: 现有机器人康复治疗系统难以适应患者复杂行为和移动场景，需改进。

Method: 提出NeuRehab框架，含训练和推理管道，分康复设备和固定硬件两部分，采用拆分机器学习和任务特定时间优化方法。

Result: 在参考步进电机肩部训练中，性能优于现有技术，推理时节能和降低延迟超60%。

Conclusion: 该方法在神经形态部署中表现良好，能有效优化移动系统的功率和延迟。

Abstract: Recent advancements in robotic rehabilitation therapy have provided modular exercise systems for post-stroke muscle recovery with basic control schemes. But these systems struggle to adapt to patients' complex and ever-changing behaviour, and to operate within mobile settings, such as heat and power. To aid this, we present NeuRehab: an end-to-end framework consisting of a training and inference pipeline with AI-based automation, co-designed with neuromorphic computing-based control systems that balance action performance, power consumption, and observed latency. The framework consists of 2 partitions. One is designated for the rehabilitation device based on ultra-low power spiking networks deployed on dedicated neuromorphic hardware. The other resides on stationary hardware that can accommodate computationally intensive hardware for fine-tuning on a per-patient basis. By maintaining a communication channel between both the modules and splitting the algorithm components, the power and latency requirements of the movable system have been optimised, while retaining the learning performance advantages of compute- and power-hungry hardware on the stationary machine. As part of the framework, we propose (a) the split machine learning processes for efficiency in architectural utilisation, and (b) task-specific temporal optimisations to lower edge-inference control latency. This paper evaluates the proposed methods on a reference stepper motor-based shoulder exercise. Overall, these methods offer comparable performance uplifts over the State-of-the-art for neuromorphic deployment, while achieving over 60% savings in both power and latency during inference compared to standard implementations.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [27] [Democratizing Scalable Cloud Applications: Transactional Stateful Functions on Streaming Dataflows](https://arxiv.org/abs/2512.17429)
*Kyriakos Psarakis*

Main category: cs.DB

TL;DR: 论文旨在解决云应用开发的三个挑战以实现开发大众化，先探索数据流执行模型与云应用联系，后提出Stateflow模型和Styx引擎，还扩展Styx支持动态负载弹性。


<details>
  <summary>Details</summary>
Motivation: 构建可扩展和一致的云应用困难，需求限制开发人员范围，需解决编程性、高性能容错可序列化事务和无服务器语义三个挑战以实现云应用开发大众化。

Method: 先通过T - Statefun探索云应用与流数据执行模型联系，后提出Stateflow编程模型，在此基础上构建Styx分布式流数据引擎，最后扩展Styx支持动态负载弹性。

Result: Styx消除了显式事务失败处理，性能显著优于现有系统。

Conclusion: 提出的方法和系统有助于解决云应用开发的挑战，推动云应用开发的大众化。

Abstract: Web applications underpin much of modern digital life, yet building scalable and consistent cloud applications remains difficult, requiring expertise across cloud computing, distributed systems, databases, and software engineering. These demands restrict development to a small number of highly specialized engineers. This thesis aims to democratize cloud application development by addressing three challenges: programmability, high-performance fault-tolerant serializable transactions, and serverless semantics.
  The thesis identifies strong parallels between cloud applications and the streaming dataflow execution model. It first explores this connection through T-Statefun, a transactional extension of Apache Flink Statefun, demonstrating that dataflow systems can support transactional cloud applications via a stateful functions-as-a-service API. However, this approach revealed significant limitations in programmability and performance.
  To overcome these issues, the thesis introduces Stateflow, a high-level object-oriented programming model that compiles applications into stateful dataflow graphs with minimal boilerplate. Building on this model, the thesis presents Styx, a distributed streaming dataflow engine that provides deterministic, multi-partition, serializable transactions with strong fault tolerance guarantees. Styx eliminates explicit transaction failure handling and significantly outperforms state-of-the-art systems.
  Finally, the thesis extends Styx with transactional state migration to support elasticity under dynamic workloads.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [28] [Fixed-Priority and EDF Schedules for ROS2 Graphs on Uniprocessor](https://arxiv.org/abs/2512.16926)
*Oren Bell,Harun Teper,Mario Günzel,Chris Gill,Jian-Jia Chen*

Main category: cs.DC

TL;DR: 论文针对ROS2当前调度方法局限性，提出用事件执行器为任意ROS2图实现固定作业级优先级调度器的方法，缩小了实时系统理论与ROS2调度分析的差距。


<details>
  <summary>Details</summary>
Motivation: 解决当前ROS2调度方法在调度简单链以外任务的局限性，分析任意有向无环图（DAG）。

Method: 使用事件执行器在单处理器系统上为任意ROS2图实现固定作业级优先级调度器，将ROS 2应用抽象为树森林以映射到传统实时DAG任务模型，需特殊实现事件队列和支持LIFO顺序消息传递的通信中间件。

Result: 实现的调度器虽无通常所需的优先级信息，但能生成与传统固定优先级DAG任务调度器相同的调度。

Conclusion: 该方法缩小了既定实时系统理论与ROS2调度分析之间的差距。

Abstract: This paper addresses limitations of current scheduling methods in the Robot Operating System (ROS)2, focusing on scheduling tasks beyond simple chains and analyzing arbitrary Directed Acyclic Graphs (DAGs). While previous research has focused mostly on chain-based scheduling with ad-hoc response time analyses, we propose a novel approach using the events executor to implement fixed-job-level-priority schedulers for arbitrary ROS2 graphs on uniprocessor systems. We demonstrate that ROS 2 applications can be abstracted as forests of trees, enabling the mapping of ROS 2 applications to traditional real-time DAG task models. Our usage of the events executor requires a special implementation of the events queue and a communication middleware that supports LIFO-ordered message delivery, features not yet standard in ROS2. We show that our implementation generates the same schedules as a conventional fixed-priority DAG task scheduler, in spite of lacking access to the precedence information that usually is required. This further closes the gap between established real-time systems theory and ROS2 scheduling analyses.

</details>


### [29] [LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation](https://arxiv.org/abs/2512.17023)
*Patrick Diehl,Noujoud Nader,Deepti Gupta*

Main category: cs.DC

TL;DR: 评估大语言模型生成高性能计算C++代码的效果，ChatGPT-4和ChatGPT-5表现良好。


<details>
  <summary>Details</summary>
Motivation: 并行编程有挑战，大语言模型在生成高性能计算代码的有效性未知，需评估。

Method: 系统评估ChatGPT 4和5、Claude、LLaMA生成C++曼德勃罗集实现代码，编译执行评估正确性、健壮性和可扩展性。

Result: ChatGPT-4和ChatGPT-5实现强语法精度和可扩展性能。

Conclusion: 整体未明确给出结论，但暗示ChatGPT-4和ChatGPT-5在生成高性能计算代码方面有优势。

Abstract: Parallel programming remains one of the most challenging aspects of High-Performance Computing (HPC), requiring deep knowledge of synchronization, communication, and memory models. While modern C++ standards and frameworks like OpenMP and MPI have simplified parallelism, mastering these paradigms is still complex. Recently, Large Language Models (LLMs) have shown promise in automating code generation, but their effectiveness in producing correct and efficient HPC code is not well understood. In this work, we systematically evaluate leading LLMs including ChatGPT 4 and 5, Claude, and LLaMA on the task of generating C++ implementations of the Mandelbrot set using shared-memory, directive-based, and distributed-memory paradigms. Each generated program is compiled and executed with GCC 11.5.0 to assess its correctness, robustness, and scalability. Results show that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance.

</details>


### [30] [Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving](https://arxiv.org/abs/2512.17077)
*Jiakun Fan,Yanglin Zhang,Xiangchen Li,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: 提出dLLM-Serve系统解决dLLMs内存问题，在不同GPU和工作负载上评估，提升吞吐量并降低尾延迟。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏针对dLLMs扩散过程独特内存动态的整体服务框架，存在“内存占用危机”。

Method: 提出Logit-Aware Activation Budgeting分解瞬态张量峰值，Phase-Multiplexed Scheduler交错异构请求阶段，Head-Centric Sparse Attention解耦逻辑稀疏性与物理存储。

Result: 在不同GPU和工作负载上，相对基线提升吞吐量1.61 - 1.81倍（RTX 4090）和1.60 - 1.74倍（L40S），重负载下尾延迟降低近4倍。

Conclusion: dLLM-Serve为可扩展dLLM推理提供首个蓝图，将理论算法稀疏性转化为跨异构硬件的实际加速。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to Autoregressive Models (ARMs), utilizing parallel decoding to overcome sequential bottlenecks. However, existing research focuses primarily on kernel-level optimizations, lacking a holistic serving framework that addresses the unique memory dynamics of diffusion processes in production. We identify a critical "memory footprint crisis" specific to dLLMs, driven by monolithic logit tensors and the severe resource oscillation between compute-bound "Refresh" phases and bandwidth-bound "Reuse" phases. To bridge this gap, we present dLLM-Serve, an efficient dLLM serving system that co-optimizes memory footprint, computational scheduling, and generation quality. dLLM-Serve introduces Logit-Aware Activation Budgeting to decompose transient tensor peaks, a Phase-Multiplexed Scheduler to interleave heterogeneous request phases, and Head-Centric Sparse Attention to decouple logical sparsity from physical storage. We evaluate dLLM-Serve on diverse workloads (LiveBench, Burst, OSC) and GPUs (RTX 4090, L40S). Relative to the state-of-the-art baseline, dLLM-Serve improves throughput by 1.61$\times$-1.81$\times$ on the consumer-grade RTX 4090 and 1.60$\times$-1.74$\times$ on the server-grade NVIDIA L40S, while reducing tail latency by nearly 4$\times$ under heavy contention. dLLM-Serve establishes the first blueprint for scalable dLLM inference, converting theoretical algorithmic sparsity into tangible wall-clock acceleration across heterogeneous hardware.

</details>


### [31] [Scalable Distributed Vector Search via Accuracy Preserving Index Construction](https://arxiv.org/abs/2512.17264)
*Yuming Xu,Qianxi Zhang,Qi Chen,Baotong Lu,Menghao Li,Philip Adams,Mingqin Li,Zengzhong Li,Jing Liu,Cheng Li,Fan Yang*

Main category: cs.DC

TL;DR: 本文提出可扩展向量索引SPIRE，在实验中展现高扩展性和高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有索引设计难以平衡分布式索引在准确性、延迟和吞吐量方面的权衡，需要一种可扩展的解决方案。

Method: 确定平衡的分区粒度以避免读取成本爆炸，引入保持准确性的递归构建方法来构建具有可预测搜索成本和稳定准确性的多级索引。

Result: 在46个节点上对多达80亿个向量的实验中，SPIRE实现了高扩展性，吞吐量比现有技术系统高9.64倍。

Conclusion: SPIRE是一种有效的可扩展向量索引，能在处理大量向量时平衡准确性、延迟和吞吐量。

Abstract: Scaling Approximate Nearest Neighbor Search (ANNS) to billions of vectors requires distributed indexes that balance accuracy, latency, and throughput. Yet existing index designs struggle with this tradeoff. This paper presents SPIRE, a scalable vector index based on two design decisions. First, it identifies a balanced partition granularity that avoids read-cost explosion. Second, it introduces an accuracy-preserving recursive construction that builds a multi-level index with predictable search cost and stable accuracy. In experiments with up to 8 billion vectors across 46 nodes, SPIRE achieves high scalability and up to 9.64X higher throughput than state-of-the-art systems.

</details>


### [32] [The HEAL Data Platform](https://arxiv.org/abs/2512.17506)
*Brienna M. Larrick,L. Philip Schumm,Mingfei Shao,Craig Barnes,Anthony Juehne,Hara Prasad Juvvla,Michael B. Kranz,Michael Lukowski,Clint Malson,Jessica N. Mazerik,Christopher G. Meyer,Jawad Qureshi,Erin Spaniol,Andrea Tentner,Alexander VanTol,Peter Vassilatos,Sara Volk de Garcia,Robert L. Grossman*

Main category: cs.DC

TL;DR: 开发基于云的联邦系统HEAL Data Platform，实现对HEAL计划数据的搜索、发现和分析，该平台可与多数据仓库互操作，方便数据重用。


<details>
  <summary>Details</summary>
Motivation: 开发一个云联邦系统，作为NIH HEAL计划产生数据的搜索、发现和分析的单点工具。

Method: 基于开源Gen3平台构建，利用框架服务和API与NIH和非NIH数据仓库互操作。

Result: 可发现超千项HEAL计划资助研究，有数百月用户，与19个数据仓库互操作，提供丰富元数据和安全计算环境。

Conclusion: HEAL Data Platform实现对关联数据仓库数据的搜索、发现和分析，确保数据FAIR原则，最大化HEAL计划数据价值。

Abstract: Objective: The objective was to develop a cloud-based, federated system to serve as a single point of search, discovery and analysis for data generated under the NIH Helping to End Addiction Long-term (HEAL) Initiative.
  Materials and methods: The HEAL Data Platform is built on the open source Gen3 platform, utilizing a small set of framework services and exposed APIs to interoperate with both NIH and non-NIH data repositories. Framework services include those for authentication and authorization, creating persistent identifiers for data objects, and adding and updating metadata.
  Results: The HEAL Data Platform serves as a single point of discovery of over one thousand studies funded under the HEAL Initiative. With hundreds of users per month, the HEAL Data Platform provides rich metadata and interoperates with data repositories and commons to provide access to shared datasets. Secure, cloud-based compute environments that are integrated with STRIDES facilitate secondary analysis of HEAL data. The HEAL Data Platform currently interoperates with nineteen data repositories.
  Discussion: Studies funded under the HEAL Initiative generate a wide variety of data types, which are deposited across multiple NIH and third-party data repositories. The mesh architecture of the HEAL Data Platform provides a single point of discovery of these data resources, accelerating and facilitating secondary use.
  Conclusion: The HEAL Data Platform enables search, discovery, and analysis of data that are deposited in connected data repositories and commons. By ensuring that these data are fully Findable, Accessible, Interoperable and Reusable (FAIR), the HEAL Data Platform maximizes the value of data generated under the HEAL Initiative.

</details>


### [33] [Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing](https://arxiv.org/abs/2512.17574)
*Lingxiao Zhao,Haoran Zhou,Yuezhi Che,Dazhao Cheng*

Main category: cs.DC

TL;DR: 文章指出多模态大语言模型（MLLMs）三阶段管道存在系统瓶颈，提出FlashCodec和UnifiedServe优化端到端MLLM管道，提升系统性能。


<details>
  <summary>Details</summary>
Motivation: MLLMs的三阶段管道在多模态预处理和视觉编码阶段存在系统瓶颈，限制了吞吐量和增加了延迟。

Method: 提出FlashCodec通过协作多GPU视频解码加速多模态预处理阶段，提出UnifiedServe优化视觉到文本和推理阶段，消除阶段间阻塞并最大化GPU利用率。

Result: 所提出的框架能比现有系统多处理3.0倍请求、实施1.5倍更严格SLO，且吞吐量最高提升4.4倍。

Conclusion: FlashCodec和UnifiedServe组成的端到端优化栈能有效改善MLLM系统性能。

Abstract: Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.
  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\times$ more requests or enforce 1.5$\times$ tighter SLOs, while achieving up to 4.4$\times$ higher throughput compared to state-of-the-art systems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [34] [Optimizing Text Search: A Novel Pattern Matching Algorithm Based on Ukkonen's Approach](https://arxiv.org/abs/2512.16927)
*Xinyu Guan,Shaohua Zhang*

Main category: cs.DS

TL;DR: 研究优化文本搜索算法，提出结合Ukkonen算法与新搜索技术的优化后缀树，性能超传统方法，有学术和实用价值。


<details>
  <summary>Details</summary>
Motivation: 传统文本搜索算法在处理现代大规模复杂数据集时效率不足，需要更高效算法。

Method: 通过Splitting和Ukkonen的算法优化后缀树，提出结合Ukkonen算法与新搜索技术的优化方案。

Result: 新优化方案有线性时空效率，优于传统方法，在基因组序列模式识别中达100%准确率。

Conclusion: 该研究推动文本搜索算法学术发展，在自然语言处理和生物信息学等领域有显著实用价值。

Abstract: In the realm of computer science, the efficiency of text-search algorithms is crucial for processing vast amounts of data in areas such as natural language processing and bioinformatics. Traditional methods like Naive Search, KMP, and Boyer-Moore, while foundational, often fall short in handling the complexities and scale of modern datasets, such as the Reuters corpus and human genomic sequences. This study rigorously investigates text-search algorithms, focusing on optimizing Suffix Trees through methods like Splitting and Ukkonen's Algorithm, analyzed on datasets including the Reuters corpus and human genomes. A novel optimization combining Ukkonen's Algorithm with a new search technique is introduced, showing linear time and space efficiencies, outperforming traditional methods like Naive Search, KMP, and Boyer-Moore. Empirical tests confirm the theoretical advantages, highlighting the optimized Suffix Tree's effectiveness in tasks like pattern recognition in genomic sequences, achieving 100% accuracy. This research not only advances academic knowledge in text-search algorithms but also demonstrates significant practical utility in fields like natural language processing and bioinformatics, due to its superior resource efficiency and reliability.

</details>


### [35] [New Theoretical Insights and Algorithmic Solutions for Reconstructing Score Sequences from Tournament Score Sets](https://arxiv.org/abs/2512.16961)
*Bowen Liu*

Main category: cs.DS

TL;DR: 本文研究竞赛图得分集重构得分序列问题，提出必要充分条件和必要条件，开发三个算法，可验证猜想且有实际应用。


<details>
  <summary>Details</summary>
Motivation: 1978年Reid提出猜想，1989年Yao给出算术证明但缺少通用多项式时间构造算法，研究如何从竞赛图得分集重构得分序列。

Method: 基于现存的Landau定理提出必要充分条件和必要条件，引入结构化集合使用群论技术；开发三个算法。

Result: 得出了必要充分条件和必要条件，开发了重构得分序列的多项式时间算法和可扩展算法、找到所有可能得分序列的多项式时间网络构建方法；可验证Reid猜想。

Conclusion: 算法有实际应用，如体育分析、排名预测、机器学习等领域。

Abstract: The score set of a tournament is defined as the set of its distinct out-degrees. In 1978, Reid proposed the conjecture that for any set of nonnegative integers $D$, there exists a tournament $T$ with a degree set $D$. In 1989, Yao presented an arithmetical proof of the conjecture, but a general polynomial-time construction algorithm is not known. This paper proposes a necessary and sufficient condition and a separate necessary condition, based on the existing Landau's theorem for the problem of reconstructing score sequences from score sets of tournament graphs. The necessary condition introduces a structured set that enables the use of group-theoretic techniques, offering not only a framework for solving the reconstruction problem but also a new perspective for approaching similar problems. In particular, the same theoretical approach can be extended to reconstruct valid score sets given constraints on the frequency of distinct scores in tournaments. Based on these conditions, we have developed three algorithms that demonstrate the practical utility of our framework: a polynomial-time algorithm and a scalable algorithm for reconstructing score sequences, and a polynomial-time network-building method that finds all possible score sequences for a given score set. Moreover, the polynomial-time algorithm for reconstructing the score sequence of a tournament for a given score set can be used to verify Reid's conjecture. These algorithms have practical applications in sports analysis, ranking prediction, and machine learning tasks such as learning-to-rank models and data imputation, where the reconstruction of partial rankings or sequences is essential for recommendation systems and anomaly detection.

</details>


### [36] [Toward Optimal Approximations for Resource-Minimization for Fire Containment on Trees and Non-Uniform k-Center](https://arxiv.org/abs/2512.17049)
*Jannis Blauth,Christian Nöbel,Rico Zenklusen*

Main category: cs.DS

TL;DR: 本文为图上的火灾控制资源最小化问题（RMFC）提供最优2 - 近似和渐近PTAS，还将技术拓展到非均匀k - 中心问题（NUkC）。


<details>
  <summary>Details</summary>
Motivation: 先前关于RMFC在树上的研究在近似性方面存在显著差距，需解决相关开放性问题。

Method: 先为RMFC的平滑变体设计PTAS，采用精心设计的LP引导枚举程序；利用RMFC和NUkC的联系将技术拓展到NUkC。

Result: 为RMFC提供最优2 - 近似和渐近PTAS；得到NUkC在额外开设中心数量上最优的近似算法。

Conclusion: 解决了RMFC的两个开放性问题，并将技术成功拓展到NUkC。

Abstract: One of the most elementary spreading models on graphs can be described by a fire spreading from a burning vertex in discrete time steps. At each step, all neighbors of burning vertices catch fire. A well-studied extension to model fire containment is to allow for fireproofing a number $B$ of non-burning vertices at each step. Interestingly, basic computational questions about this model are computationally hard even on trees. One of the most prominent such examples is Resource Minimization for Fire Containment (RMFC), which asks how small $B$ can be chosen so that a given subset of vertices will never catch fire. Despite recent progress on RMFC on trees, prior work left a significant gap in terms of its approximability. We close this gap by providing an optimal $2$-approximation and an asymptotic PTAS, resolving two open questions in the literature. Both results are obtained in a unified way, by first designing a PTAS for a smooth variant of RMFC, which is obtained through a careful LP-guided enumeration procedure.
  Moreover, we show that our new techniques, with several additional ingredients, carry over to the non-uniform $k$-center problem (NUkC), by exploiting a link between RMFC on trees and NUkC established by Chakrabarty, Goyal, and Krishnaswamy. This leads to the first approximation algorithm for NUkC that is optimal in terms of the number of additional centers that have to be opened.

</details>


### [37] [Optimal Verification of a Minimum-Weight Basis in an Uncertainty Matroid](https://arxiv.org/abs/2512.17116)
*Haya Diwan,Lisa Hellerstein,Nicole Megow,Jens Schlöter*

Main category: cs.DS

TL;DR: 本文提出多项式时间算法验证拟阵最小权重基，可处理多种不确定性区域，还给出对应自适应在线问题的最优算法，且算法可应用于两个学习增强变体问题。


<details>
  <summary>Details</summary>
Motivation: 解决可探索不确定性下组合优化问题，设计最小化查询成本的自适应查询策略，需解决相关验证问题。

Method: 提出新的多项式时间算法验证拟阵最小权重基，利用验证问题的结构结果给出在线问题算法。

Result: 提出的算法能处理多种不确定性区域，严格推广前人工作；给出对应自适应在线问题的最优算法；算法可应用于两个学习增强变体问题。

Conclusion: 所提算法在可探索不确定性问题的验证和在线求解上有良好效果，且有一定应用价值。

Abstract: Research in explorable uncertainty addresses combinatorial optimization problems where there is partial information about the values of numeric input parameters, and exact values of these parameters can be determined by performing costly queries. The goal is to design an adaptive query strategy that minimizes the query cost incurred in computing an optimal solution. Solving such problems generally requires that we be able to solve the associated verification problem: given the answers to all queries in advance, find a minimum-cost set of queries that certifies an optimal solution to the combinatorial optimization problem. We present a polynomial-time algorithm for verifying a minimum-weight basis of a matroid, where each weight lies in a given uncertainty area. These areas may be finite sets, real intervals, or unions of open and closed intervals, strictly generalizing previous work by Erlebach and Hoffman which only handled the special case of open intervals. Our algorithm introduces new techniques to address the resulting challenges.
  Verification problems are of particular importance in the area of explorable uncertainty, as the structural insights and techniques used to solve the verification problem often heavily influence work on the corresponding online problem and its stochastic variant. In our case, we use structural results from the verification problem to give a best-possible algorithm for a promise variant of the corresponding adaptive online problem. Finally, we show that our algorithms can be applied to two learning-augmented variants of the minimum-weight basis problem under explorable uncertainty.

</details>


### [38] [LZ78 Substring Compression in Compressed Space](https://arxiv.org/abs/2512.17217)
*Hiroki Shibata,Dominik Köppl*

Main category: cs.DS

TL;DR: 研究LZ78因式分解及其衍生物在子串压缩模型中的应用，提出压缩空间算法。


<details>
  <summary>Details</summary>
Motivation: 现有多数研究聚焦普通数据因式分解，针对快速LZ78因式分解的数据索引研究较少。

Method: 在子串压缩模型下提出一种在压缩空间工作的算法。

Result: 该算法计算因式分解的时间仅有最优时间复杂度的对数级放缓。

Conclusion: 该算法可实现对指定子串进行因式分解的较快速计算。

Abstract: The Lempel--Ziv 78 (LZ78) factorization is a well-studied technique for data compression. It and its derivatives are used in compression formats such as "compress" or "gif". Although most research focuses on the factorization of plain data, not much research has been conducted on indexing the data for fast LZ78 factorization. Here, we study the LZ78 factorization and its derivatives in the substring compression model, where we are allowed to index the data and return the factorization of a substring specified at query time. In that model, we propose an algorithm that works in compressed space, computing the factorization with a logarithmic slowdown compared to the optimal time complexity.

</details>


### [39] [Refining the Complexity Landscape of Speed Scaling: Hardness and Algorithms](https://arxiv.org/abs/2512.17663)
*Antonios Antoniadis,Denise Graafsma,Ruben Hoeksma,Maria Vlasiou*

Main category: cs.DS

TL;DR: 本文研究单速度可扩展处理器上作业调度问题，解决了四个重要问题变体的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 此前文献虽对作业调度中（加权）流时间与能耗的权衡问题有诸多研究，但四个重要问题变体的计算复杂度未解决，本文旨在解决该问题。

Method: 对不同情况，如单位权重任意大小作业、任意权重单位大小作业，证明最小化总（加权）流时间加能耗目标问题是NP难的；在给定完成时间顺序时，证明问题可在多项式时间内求解。

Result: 证明了两类情况下最小化总（加权）流时间加能耗目标问题是NP难的，结果也适用于有能耗预算情况；若有完成时间顺序，问题可在多项式时间内求解。

Conclusion: 明确了四个问题变体的计算复杂度，凸显了优先级和完成时间顺序对问题的微妙差异。

Abstract: We study the computational complexity of scheduling jobs on a single speed-scalable processor with the objective of capturing the trade-off between the (weighted) flow time and the energy consumption. This trade-off has been extensively explored in the literature through a number of problem formulations that differ in the specific job characteristics and the precise objective function. Nevertheless, the computational complexity of four important problem variants has remained unresolved and was explicitly identified as an open question in prior work. In this paper, we settle the complexity of these variants.
  More specifically, we prove that the problem of minimizing the objective of total (weighted) flow time plus energy is NP-hard for the cases of (i) unit-weight jobs with arbitrary sizes, and (ii)~arbitrary-weight jobs with unit sizes. These results extend to the objective of minimizing the total (weighted) flow time subject to an energy budget and hold even when the schedule is required to adhere to a given priority ordering.
  In contrast, we show that when a completion-time ordering is provided, the same problem variants become polynomial-time solvable. The latter result highlights the subtle differences between priority and completion orderings for the problem.

</details>


### [40] [Capacitated Partition Vertex Cover and Partition Edge Cover](https://arxiv.org/abs/2512.17844)
*Rajni Dabas,Samir Khuller,Emilie Rivkin*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Our first focus is the Capacitated Partition Vertex Cover (C-PVC) problem in hypergraphs. In C-PVC, we are given a hypergraph with capacities on its vertices and a partition of the hyperedge set into $ω$ distinct groups. The objective is to select a minimum size subset of vertices that satisfies two main conditions: (1) in each group, the total number of covered hyperedges meets a specified threshold, and (2) the number of hyperedges assigned to any vertex respects its capacity constraint. A covered hyperedge is required to be assigned to a selected vertex that belongs to the hyperedge. This formulation generalizes classical Vertex Cover, Partial Vertex Cover, and Partition Vertex Cover.
  We investigate two primary variants: soft capacitated (multiple copies of a vertex are allowed) and hard capacitated (each vertex can be chosen at most once). Let $f$ denote the rank of the hypergraph. Our main contributions are: $(i)$ an $(f+1)$-approximation algorithm for the weighted soft-capacitated C-PVC problem, which runs in $n^{O(ω)}$ time, and $(ii)$ an $(f+ε)$-approximation algorithm for the unweighted hard-capacitated C-PVC problem, which runs in $n^{O(ω/ε)}$ time.
  We also study a natural generalization of the edge cover problem, the \emph{Weighted Partition Edge Cover} (W-PEC) problem, where each edge has an associated weight, and the vertex set is partitioned into groups. For each group, the goal is to cover at least a specified number of vertices using incident edges, while minimizing the total weight of the selected edges. We present the first exact polynomial-time algorithm for the weighted case, improving runtime from $O(ωn^3)$ to $O(mn+n^2 \log n)$ and simplifying the algorithmic structure over prior unweighted approaches.

</details>


### [41] [Prefix Trees Improve Memory Consumption in Large-Scale Continuous-Time Stochastic Models](https://arxiv.org/abs/2512.17892)
*Landon Taylor,Joshua Jeppson,Ahmed Irfan,Lukas Buecherl,Chris Myers,Zhen Zhang*

Main category: cs.DS

TL;DR: 本文针对高并发系统模型状态空间分析时哈希表的内存限制问题，提出用前缀树存储状态，并结合BMC预处理步骤提升内存使用效率，该方法可推广到所有CTMC模型。


<details>
  <summary>Details</summary>
Motivation: 现有哈希表在分析具有巨大状态空间的高并发系统模型（如CRNs）时存在严重内存限制问题。

Method: 提出使用前缀树存储大型高并发模型的状态，并采用Bounded Model Checking (BMC) 预处理步骤施加变量排序。

Result: 理论分析和基准测试表明在非常大的状态空间中前缀树优于哈希表，初步评估显示BMC预处理步骤有效。

Conclusion: 提出的方法能节省内存，且可推广到所有CTMC模型。

Abstract: Highly-concurrent system models with vast state spaces like Chemical Reaction Networks (CRNs) that model biological and chemical systems pose a formidable challenge to cutting-edge formal analysis tools. Although many symbolic approaches have been presented, transient probability analysis of CRNs, modeled as Continuous-Time Markov Chains (CTMCs), requires explicit state representation. For that purpose, current cutting-edge methods use hash maps, which boast constant average time complexity and linear memory complexity. However, hash maps often suffer from severe memory limitations on models with immense state spaces. To address this, we propose using prefix trees to store states for large, highly concurrent models (particularly CRNs) for memory savings. We present theoretical analyses and benchmarks demonstrating the favorability of prefix trees over hash maps for very large state spaces. Additionally, we propose using a Bounded Model Checking (BMC) pre-processing step to impose a variable ordering to further improve memory usage along with preliminary evaluations suggesting its effectiveness. We remark that while our work is motivated primarily by the challenges posed by CRNs, it is generalizable to all CTMC models.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [42] [Fairly Dividing Non-identical Random Items: Just Sample or Match](https://arxiv.org/abs/2512.17238)
*Aprup Kale,Rucha Kulkarni,Navya Garg*

Main category: cs.GT

TL;DR: 研究具有加性估值的代理间不可分割资源公平有效分配的存在性与快速计算问题，扩展模型至非相同物品，证明渐近情况下高概率存在公平有效分配并给出计算方法，模拟显示小实例也有较好效果。


<details>
  <summary>Details</summary>
Motivation: 探究公平有效分配在典型或随机实例中的存在性及高效计算方法，以快速解决现实资源分配问题。

Method: 将问题的形式模型扩展到非相同物品，假设每个物品关联一个分布，每个代理对物品的效用值独立抽取；通过采样部分效用值计算分配。

Result: 渐近情况下高概率存在无嫉妒公平和最大社会福利有效的分配；当m = O(nlog n)时，可在O~(m)时间内计算分配；模拟显示小实例在公平和效率保证上损失小且快速收敛到最优。

Conclusion: 扩展模型能在典型实例中高效找到公平有效分配，即使小实例也有较好表现。

Abstract: We study the question of existence and fast computation of fair and efficient allocations of indivisible resources among agents with additive valuations. As such allocations may not exist for arbitrary instances, we ask if they exist for \textit{typical} or \textit{random} instances, meaning when the utility values of agents for the resources are drawn from certain distributions. If such allocations exist with high probability for typical instances, and furthermore if they can be computed efficiently, this would imply that we could quickly resolve a real world resource allocation scenario in a fair and efficient manner with high probability. This implication has made this setting popular and well studied in fair resource allocation.
  In this paper, we extend the previously studied formal models of this problem to non-identical items. We assume that every item is associated with a distribution $\mathcal{U}_j$, and every agent's utility value for the item is drawn independently from $\mathcal{U}_j$. We show that envy-free fair and maximum social welfare efficient allocations exist with high probability in the asymptotic setting, meaning when the number of agents $n$ and items $m$ are large. Further we show that when $m=O(n\log n),$ then by only sampling $O(\log m)$ or $O((\log m)^2)$ utility values per item instead of all the $n,$ we can compute these allocations in $\tilde{O}(m)$ time. Finally, we simulate our algorithms on randomly generated instances and show that even for small instances, we suffer small multiplicative losses in the fairness and efficiency guarantees even for small sized instances, and converge to fully optimal guarantees quickly.

</details>


### [43] [Analytical Stackelberg Resource Allocation in Sequential Attacker--Defender Games](https://arxiv.org/abs/2512.17284)
*Azhar Iqbal,James M. Chappell,Derek Abbott*

Main category: cs.GT

TL;DR: 开发分析Stackelberg博弈框架进行资源最优分配，推导策略表达式，分析三种收益情况并举例。


<details>
  <summary>Details</summary>
Motivation: 解决在有限资产和概率攻击的顺序攻防环境下的资源最优分配问题。

Method: 构建Stackelberg博弈框架，防御者确定混合保护策略，攻击者通过逆向归纳做出最佳响应，推导均衡点封闭表达式。

Result: 得到不同资产数量和防御资源下的均衡保护与攻击策略表达式，确定奖励和成本约束，识别三种防御者收益情况。

Conclusion: 通过八资产数值示例说明均衡结构，并揭示了独特的帕累托最优攻击配置。

Abstract: We develop an analytical Stackelberg game framework for optimal resource allocation in a sequential attacker--defender setting with a finite set of assets and probabilistic attacks. The defender commits to a mixed protection strategy, after which the attacker best-responds via backward induction. Closed-form expressions for equilibrium protection and attack strategies are derived for general numbers of assets and defensive resources. Necessary constraints on rewards and costs are established to ensure feasibility of the probability distributions. Three distinct payoff regimes for the defender are identified and analysed. An eight-asset numerical example illustrates the equilibrium structure and reveals a unique Pareto-dominant attack configuration.

</details>


### [44] [Deterministic implementation in single-item auctions](https://arxiv.org/abs/2512.17386)
*Yan Liu,Zeyu Ren,Pingzhong Tang,Zihe Wang,Yulong Zeng,Jie Zhang*

Main category: cs.GT

TL;DR: 研究单物品拍卖中确定性机制在两种结果概念下的实现情况，指出确定性BIC与DSIC拍卖在一些方面的差异及等价条件。


<details>
  <summary>Details</summary>
Motivation: 确定性拍卖因透明、简单和易实现引发对其与随机设计匹配情况的深入理解需求。

Method: 对单物品拍卖中 (收益, 福利) 对和中期分配两种结果概念进行研究。

Result: 对于 (收益, 福利) 对，存在确定性BIC可实现但DSIC不可实现的对；对连续无原子先验，确定了确定性DSIC与随机BIC拍卖在可实现结果上等价的条件；对中期分配，为两个投标人建立确定性类似Border定理，展示了确定性BIC可实现但DSIC不可实现的中期分配。

Conclusion: 明确了确定性拍卖在不同激励兼容机制下的实现差异和等价情况。

Abstract: Deterministic auctions are attractive in practice due to their transparency, simplicity, and ease of implementation, motivating a sharp understanding of when they match randomized designs. We study deterministic implementation in single-item auctions under two outcome notions: (revenue, welfare) pairs and interim allocations. For (revenue, welfare) pairs, we show a discrete separation: there exists a pair implementable by a deterministic Bayesian incentive-compatible (BIC) auction but not by any deterministic dominant-strategy incentive-compatible (DSIC) auction. For continuous atomless priors, we identify conditions under which deterministic DSIC auctions are implementationally equivalent to randomized BIC auctions in terms of achievable outcomes. For interim allocations, we establish a deterministic analogue of Border's theorem for two bidders, providing a necessary and sufficient condition for deterministic DSIC implementability, and use it to exhibit an interim allocation implementable by a deterministic BIC auction but not by any deterministic DSIC auction.

</details>


### [45] [Comparing the Fairness of Recursively Balanced Picking Sequences](https://arxiv.org/abs/2512.17604)
*Karen Frilya Celine,Warut Suksompong,Sheung Man Yuen*

Main category: cs.GT

TL;DR: 本文比较不同递归平衡选择序列的公平性，发现其平等主义福利价格相同，且特定补偿方式的序列有最佳近似最大最小份额（MMS）保证。


<details>
  <summary>Details</summary>
Motivation: 比较不同递归平衡选择序列的公平性。

Method: 使用两个关键指标，一是比较平等主义福利价格，二是刻画近似最大最小份额（MMS）保证。

Result: 所有递归平衡选择序列在平等主义福利价格上相同；让第一轮最后选择的代理在后续轮次先选能获得最佳MMS保证。

Conclusion: 不同递归平衡选择序列在平等主义福利方面表现一致，特定补偿方式可优化MMS保证。

Abstract: Picking sequences are well-established methods for allocating indivisible goods. Among the various picking sequences, recursively balanced picking sequences -- whereby each agent picks one good in every round -- are notable for guaranteeing allocations that satisfy envy-freeness up to one good. In this paper, we compare the fairness of different recursively balanced picking sequences using two key measures. Firstly, we demonstrate that all such sequences have the same price in terms of egalitarian welfare relative to other picking sequences. Secondly, we characterize the approximate maximin share (MMS) guarantees of these sequences. In particular, we show that compensating the agent who picks last in the first round by letting her pick first in every subsequent round yields the best MMS guarantee.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [46] [A Reproducible and Fair Evaluation of Partition-aware Collaborative Filtering](https://arxiv.org/abs/2512.17015)
*Domenico De Gioia,Claudio Pomo,Ludovico Boratto,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 对FPSR和FPSR+进行可复现基准测试，发现FPSR模型并非始终表现最优，但在长尾场景有优势，为可扩展推荐系统设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 相似性协同过滤模型扩展性受限，分区范式是平衡有效性和效率的策略，且之前对分区感知协同过滤的评估缺乏可复现性，难以公平比较。

Method: 对FPSR和FPSR+进行透明、完全可复现的基准测试。

Result: FPSR模型并非始终表现最优，但仍具竞争力，验证了其设计选择，在长尾场景有显著优势。

Conclusion: 明确了分区感知相似性建模何时最有益，为可扩展推荐系统设计提供了可操作的指导。

Abstract: Similarity-based collaborative filtering (CF) models have long demonstrated strong offline performance and conceptual simplicity. However, their scalability is limited by the quadratic cost of maintaining dense item-item similarity matrices. Partitioning-based paradigms have recently emerged as an effective strategy for balancing effectiveness and efficiency, enabling models to learn local similarities within coherent subgraphs while maintaining a limited global context. In this work, we focus on the Fine-tuning Partition-aware Similarity Refinement (FPSR) framework, a prominent representative of this family, as well as its extension, FPSR+. Reproducible evaluation of partition-aware collaborative filtering remains challenging, as prior FPSR/FPSR+ reports often rely on splits of unclear provenance and omit some similarity-based baselines, thereby complicating fair comparison. We present a transparent, fully reproducible benchmark of FPSR and FPSR+. Based on our results, the family of FPSR models does not consistently perform at the highest level. Overall, it remains competitive, validates its design choices, and shows significant advantages in long-tail scenarios. This highlights the accuracy-coverage trade-offs resulting from partitioning, global components, and hub design. Our investigation clarifies when partition-aware similarity modeling is most beneficial and offers actionable guidance for scalable recommender system design under reproducible protocols.

</details>


### [47] [Unexpected Knowledge: Auditing Wikipedia and Grokipedia Search Recommendations](https://arxiv.org/abs/2512.17027)
*Erica Coppolillo,Simone Mungari*

Main category: cs.IR

TL;DR: 对Wikipedia和Grokipedia搜索引擎进行比较分析，发现两平台搜索结果常与查询弱相关，有意外内容，且两系统推荐集有差异。


<details>
  <summary>Details</summary>
Motivation: 不同百科系统中搜索引擎行为研究不足，需对Wikipedia和Grokipedia的搜索引擎进行比较分析。

Method: 用近10000个中性英文单词及其子串作查询，收集超70000条搜索结果，分析语义对齐、重叠和主题结构，还进行主题注释和轨迹分析。

Result: 两平台结果常与原查询弱相关，有意外内容；相同查询下两系统推荐集不同；内容类别呈现和搜索结果多阶段演变有系统差异。

Conclusion: 意外搜索结果是两平台常见特征，两平台在主题分布和查询建议上有差异。

Abstract: Encyclopedic knowledge platforms are key gateways through which users explore information online. The recent release of Grokipedia, a fully AI-generated encyclopedia, introduces a new alternative to traditional, well-established platforms like Wikipedia. In this context, search engine mechanisms play an important role in guiding users exploratory paths, yet their behavior across different encyclopedic systems remains underexplored. In this work, we address this gap by providing the first comparative analysis of search engine in Wikipedia and Grokipedia.
  Using nearly 10,000 neutral English words and their substrings as queries, we collect over 70,000 search engine results and examine their semantic alignment, overlap, and topical structure. We find that both platforms frequently generate results that are weakly related to the original query and, in many cases, surface unexpected content starting from innocuous queries. Despite these shared properties, the two systems often produce substantially different recommendation sets for the same query. Through topical annotation and trajectory analysis, we further identify systematic differences in how content categories are surfaced and how search engine results evolve over multiple stages of exploration.
  Overall, our findings show that unexpected search engine outcomes are a common feature of both the platforms, even though they exhibit discrepancies in terms of topical distribution and query suggestions.

</details>


### [48] [TCDE: Topic-Centric Dual Expansion of Queries and Documents with Large Language Models for Information Retrieval](https://arxiv.org/abs/2512.17164)
*Yu Yang,Feng Tian,Ping Chen*

Main category: cs.IR

TL;DR: 提出TCDE双扩展策略解决查询扩展和文档扩展语义不对齐问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决查询扩展和文档扩展分开应用导致的语义不对齐问题。

Method: 设计两个提示模板，在查询端生成伪文档，在文档端提取核心主题句，进行主题中心的双扩展。

Result: 在两个基准测试中，TCDE比现有扩展基线有显著改进，在密集检索任务中表现优于多个方法，在SciFact数据集上NDCG@10相对提升2.8%。

Conclusion: 主题中心的双扩展策略有效。

Abstract: Query Expansion (QE) enriches queries and Document Expansion (DE) enriches documents, and these two techniques are often applied separately. However, such separate application may lead to semantic misalignment between the expanded queries (or documents) and their relevant documents (or queries). To address this serious issue, we propose TCDE, a dual expansion strategy that leverages large language models (LLMs) for topic-centric enrichment on both queries and documents. In TCDE, we design two distinct prompt templates for processing each query and document. On the query side, an LLM is guided to identify distinct sub-topics within each query and generate a focused pseudo-document for each sub-topic. On the document side, an LLM is guided to distill each document into a set of core topic sentences. The resulting outputs are used to expand the original query and document. This topic-centric dual expansion process establishes semantic bridges between queries and their relevant documents, enabling better alignment for downstream retrieval models. Experiments on two challenging benchmarks, TREC Deep Learning and BEIR, demonstrate that TCDE achieves substantial improvements over strong state-of-the-art expansion baselines. In particular, on dense retrieval tasks, it outperforms several state-of-the-art methods, with a relative improvement of 2.8\% in NDCG@10 on the SciFact dataset. Experimental results validate the effectiveness of our topic-centric and dual expansion strategy.

</details>


### [49] [Warmer for Less: A Cost-Efficient Strategy for Cold-Start Recommendations at Pinterest](https://arxiv.org/abs/2512.17277)
*Saeed Ebrahimi,Weijie Jiang,Jaewon Yang,Olafur Gudmundsson,Yucheng Tu,Huizhong Duan*

Main category: cs.IR

TL;DR: 本文研究Pinterest平台上提升冷启动物品推荐系统模型预测的问题，提出多项解决方案，使新鲜内容参与度提升10%且无负面影响，已服务超5.7亿用户。


<details>
  <summary>Details</summary>
Motivation: Pinterest作为视觉发现平台，需提升冷启动物品推荐系统的模型预测，虽该问题在学术界有研究，但在Pinterest平台规模上未有效解决根源问题。

Method: 针对冷启动问题的挑战，设计轻量级解决方案（参数仅增5%）；为非历史特征引入残差连接；在模型中加入分数正则化项；应用流形混合技术解决数据稀疏问题。

Result: 方法共同实施后，Pinterest新鲜内容参与度提升10%，且未对整体参与度和成本产生负面影响，已部署服务超5.7亿用户。

Conclusion: 提出的针对冷启动物品推荐的解决方案有效可行，可在大规模平台应用。

Abstract: Pinterest is a leading visual discovery platform where recommender systems (RecSys) are key to delivering relevant, engaging, and fresh content to our users. In this paper, we study the problem of improving RecSys model predictions for cold-start (CS) items, which appear infrequently in the training data. Although this problem is well-studied in academia, few studies have addressed its root causes effectively at the scale of a platform like Pinterest. By investigating live traffic data, we identified several challenges of the CS problem and developed a corresponding solution for each: First, industrial-scale RecSys models must operate under tight computational constraints. Since CS items are a minority, any related improvements must be highly cost-efficient. To address this, our solutions were designed to be lightweight, collectively increasing the total parameters by only 5%. Second, CS items are represented only by non-historical (e.g., content or attribute) features, which models often treat as less important. To elevate their significance, we introduce a residual connection for the non-historical features. Third, CS items tend to receive lower prediction scores compared to non-CS items, reducing their likelihood of being surfaced. We mitigate this by incorporating a score regularization term into the model. Fourth, the labels associated with CS items are sparse, making it difficult for the model to learn from them. We apply the manifold mixup technique to address this data sparsity. Implemented together, our methods increased fresh content engagement at Pinterest by 10% without negatively impacting overall engagement and cost, and have been deployed to serve over 570 million users on Pinterest.

</details>


### [50] [The Mental World of Large Language Models in Recommendation: A Benchmark on Association, Personalization, and Knowledgeability](https://arxiv.org/abs/2512.17389)
*Guangneng Hu*

Main category: cs.IR

TL;DR: 提出LRWorld基准评估大语言模型在推荐系统中的表现，实验显示大模型在部分任务有成果但未在所有因素上表现一致。


<details>
  <summary>Details</summary>
Motivation: 研究界缺乏评估大语言模型在推荐系统中局限性和边界的综合基准，因此提出研究。

Method: 提出包含超38K高质量样本和23M标记的LRWorld基准，将大语言模型在推荐系统中的表现分为三个主尺度、十个因素和31个任务进行评估。

Result: 大语言模型不能很好捕捉深度神经个性化嵌入，但在浅层基于记忆的物品-物品相似度上有好结果，能感知物品实体关系等，在多模态知识推理和抗噪方面有潜力，无模型在十个因素上都表现好。

Conclusion: 通过LRWorld基准实验，对大语言模型在推荐系统中的表现有了更清晰认识，明确其优势和不足。

Abstract: Large language models (LLMs) have shown potential in recommendation systems (RecSys) by using them as either knowledge enhancer or zero-shot ranker. A key challenge lies in the large semantic gap between LLMs and RecSys where the former internalizes language world knowledge while the latter captures personalized world of behaviors. Unfortunately, the research community lacks a comprehensive benchmark that evaluates the LLMs over their limitations and boundaries in RecSys so that we can draw a confident conclusion. To investigate this, we propose a benchmark named LRWorld containing over 38K high-quality samples and 23M tokens carefully compiled and generated from widely used public recommendation datasets. LRWorld categorizes the mental world of LLMs in RecSys as three main scales (association, personalization, and knowledgeability) spanned by ten factors with 31 measures (tasks). Based on LRWorld, comprehensive experiments on dozens of LLMs show that they are still not well capturing the deep neural personalized embeddings but can achieve good results on shallow memory-based item-item similarity. They are also good at perceiving item entity relations, entity hierarchical taxonomies, and item-item association rules when inferring user interests. Furthermore, LLMs show a promising ability in multimodal knowledge reasoning (movie poster and product image) and robustness to noisy profiles. None of them show consistently good performance over the ten factors. Model sizes, position bias, and more are ablated.

</details>


### [51] [A Systematic Reproducibility Study of BSARec for Sequential Recommendation](https://arxiv.org/abs/2512.17442)
*Jan Hutter,Hua Chang Bakker,Stan Fris,Madelon Bernardy,Yuanna Liu*

Main category: cs.IR

TL;DR: 本文重现BSARec并评估其性能，提出量化指标评估高频信号性能，比较不同DSP技术和填充策略。


<details>
  <summary>Details</summary>
Motivation: Transformer基模型自注意力机制作为低通滤波器，限制捕捉高频信号能力，且BSARec有效性和各组件作用待验证。

Method: 重现BSARec；提出量化用户历史频率的指标评估不同用户组的SR方法；比较不同DSP技术；探索不同填充策略。

Result: BSARec在部分数据集上优于其他SR方法；DWT比傅里叶变换仅略有改进，DSP方法不比简单残差连接有明显优势；非恒定填充显著提升推荐性能，恒定填充阻碍频率重缩放器捕捉高频信号。

Conclusion: 评估了BSARec性能，明确不同DSP技术和填充策略对SR性能的影响。

Abstract: In sequential recommendation (SR), the self-attention mechanism of Transformer-based models acts as a low-pass filter, limiting their ability to capture high-frequency signals that reflect short-term user interests. To overcome this, BSARec augments the Transformer encoder with a frequency layer that rescales high-frequency components using the Fourier transform. However, the overall effectiveness of BSARec and the roles of its individual components have yet to be systematically validated. We reproduce BSARec and show that it outperforms other SR methods on some datasets. To empirically assess whether BSARec improves performance on high-frequency signals, we propose a metric to quantify user history frequency and evaluate SR methods across different user groups. We compare digital signal processing (DSP) techniques and find that the discrete wavelet transform (DWT) offer only slight improvements over Fourier transforms, and DSP methods provide no clear advantage over simple residual connections. Finally, we explore padding strategies and find that non-constant padding significantly improves recommendation performance, whereas constant padding hinders the frequency rescaler's ability to capture high-frequency signals.

</details>


### [52] [Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application](https://arxiv.org/abs/2512.17462)
*Olivier Jeunen,Schaun Wheeler*

Main category: cs.IR

TL;DR: 评估代理式个性化在金融服务应用客户通信系统中对行为和留存率的影响，发现其可减少退订并增加提前申报行为。


<details>
  <summary>Details</summary>
Motivation: 评估代理式个性化在金融服务应用客户通信系统中的行为和留存效果。

Method: 进行为期两个月的随机对照试验，对比代理式消息传递方法与常规基于规则的活动系统。

Result: 代理式消息传递使退订事件相对减少21%（±0.01），增加了国家截止日期前几周的提前申报行为。

Conclusion: 自适应的用户级决策系统可调节参与强度并改善长期留存指标。

Abstract: Marketing and product personalisation provide a prominent and visible use-case for the application of Information Retrieval methods across several business domains. Recently, agentic approaches to these problems have been gaining traction. This work evaluates the behavioural and retention effects of agentic personalisation on a financial service application's customer communication system during a 2025 national tax filing period. Through a two month-long randomised controlled trial, we compare an agentic messaging approach against a business-as-usual (BAU) rule-based campaign system, focusing on two primary outcomes: unsubscribe behaviour and conversion timing. Empirical results show that agent-led messaging reduced unsubscribe events by 21\% ($\pm 0.01$) relative to BAU and increased early filing behaviour in the weeks preceding the national deadline. These findings demonstrate how adaptive, user-level decision-making systems can modulate engagement intensity whilst improving long-term retention indicators.

</details>


### [53] [Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure](https://arxiv.org/abs/2512.17733)
*Jingmao Zhang,Zhiting Zhao,Yunqi Lin,Jianghong Ma,Tianjun Wei,Haijun Zhang,Xiaofeng Zhang*

Main category: cs.IR

TL;DR: 提出Cadence框架，从因果关系角度解决推荐系统中商品流行度偏差和多样性不足问题，实验证明其在多样性和准确性上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有推荐系统中商品-商品关系依赖共现导致的流行度偏差和用户属性干扰问题，以及推荐多样性缺乏因果视角和理论支撑的问题。

Method: 构建基于LightGCN的Cadence框架，计算无偏非对称共购关系（UACR）构建去混淆的有向商品图，利用UACR识别多样化商品类别并模拟其高曝光场景下的行为。

Result: 在真实数据集上实验表明，该方法在多样性和准确性上均优于现有模型。

Conclusion: Cadence框架有效、可迁移且高效，能提升推荐系统的多样性和准确性。

Abstract: Beyond user-item modeling, item-to-item relationships are increasingly used to enhance recommendation. However, common methods largely rely on co-occurrence, making them prone to item popularity bias and user attributes, which degrades embedding quality and performance. Meanwhile, although diversity is acknowledged as a key aspect of recommendation quality, existing research offers limited attention to it, with a notable lack of causal perspectives and theoretical grounding. To address these challenges, we propose Cadence: Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure - a plug-and-play framework built upon LightGCN as the backbone, primarily designed to enhance recommendation diversity while preserving accuracy. First, we compute the Unbiased Asymmetric Co-purchase Relationship (UACR) between items - excluding item popularity and user attributes - to construct a deconfounded directed item graph, with an aggregation mechanism to refine embeddings. Second, we leverage UACR to identify diverse categories of items that exhibit strong causal relevance to a user's interacted items but have not yet been engaged with. We then simulate their behavior under high-exposure scenarios, thereby significantly enhancing recommendation diversity while preserving relevance. Extensive experiments on real-world datasets demonstrate that our method consistently outperforms state-of-the-art diversity models in both diversity and accuracy, and further validates its effectiveness, transferability, and efficiency over baselines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [54] [Dion2: A Simple Method to Shrink Matrix in Muon](https://arxiv.org/abs/2512.16928)
*Kwangjun Ahn,Noah Amsel,John Langford*

Main category: cs.LG

TL;DR: 介绍Dion2方法简化Muon计算中矩阵收缩，提高Muon可扩展性


<details>
  <summary>Details</summary>
Motivation: Muon优化器正交化步骤超线性成本随规模增大，需降低成本

Method: Dion2在每次迭代中选择部分行或列进行正交化，使更新稀疏

Result: 降低计算和通信成本

Conclusion: Dion2能提高Muon的可扩展性

Abstract: The Muon optimizer enjoys strong empirical performance and theoretical grounding. However, the super-linear cost of its orthonormalization step introduces increasing overhead with scale. To alleviate this cost, several works have attempted to reduce the size of the matrix entering the orthonormalization step. We introduce Dion2, a much simpler method for shrinking the matrix involved in Muon's computation compared to prior approaches. At a high level, Dion2 selects a fraction of rows or columns at each iteration and orthonormalizes only those. This sampling procedure makes the update sparse, reducing both computation and communication costs which in turn improves the scalability of Muon.

</details>


### [55] [GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping](https://arxiv.org/abs/2512.17570)
*Yikang Yue,Yishu Yin,Xuehai Qian*

Main category: cs.LG

TL;DR: 本文提出SSD卸载训练系统GreedySnake，采用垂直调度，能提高训练吞吐量，缓解I/O瓶颈，实验显示有显著性能提升且代码开源。


<details>
  <summary>Details</summary>
Motivation: 使大语言模型（LLM）训练更具成本效益，解决现有系统的性能瓶颈。

Method: 基于微批次梯度累积，引入垂直调度，将优化步骤部分与下一次迭代的前向传播重叠。

Result: 在A100 GPU上实验，GreedySnake相对ZeRO - Infinity有饱和训练吞吐量提升，如GPT - 65B在1 GPU上提升1.96倍、4 GPU上提升1.93倍，GPT - 175B在1 GPU上提升2.53倍。

Conclusion: GreedySnake是一种实用且有前景的SSD卸载训练方法，能有效提高训练效率。

Abstract: SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake

</details>


### [56] [BIONIX: A Wireless, Low-Cost Prosthetic Arm with Dual-Signal EEG and EMG Control](https://arxiv.org/abs/2512.16929)
*Pranesh Sathish Kumar*

Main category: cs.LG

TL;DR: 本文提出低成本双模式神经肌肉控制系统，结合EEG和EMG实现假肢实时多自由度控制，构建功能原型并规划未来工作。


<details>
  <summary>Details</summary>
Motivation: 解决低成本上肢假肢缺乏直观控制系统，功能和可及性受限的问题。

Method: 用NeuroSky MindWave Mobile 2采集EEG信号，传至ESP32运行轻量级分类模型；用MyoWare 2.0传感器和SparkFun无线盾牌采集EMG信号，传至另一ESP32进行阈值检测。

Result: 构建了总成本约240美元的功能原型，EEG控制手指，EMG控制肘部。

Conclusion: 该系统为低成本、生物直观的假肢控制提供可行途径，适用于服务不足和全球健康应用。

Abstract: Affordable upper-limb prostheses often lack intuitive control systems, limiting functionality and accessibility for amputees in low-resource settings. This project presents a low-cost, dual-mode neuro-muscular control system integrating electroencephalography (EEG) and electromyography (EMG) to enable real-time, multi-degree-of-freedom control of a prosthetic arm. EEG signals are acquired using the NeuroSky MindWave Mobile 2 and transmitted via ThinkGear Bluetooth packets to an ESP32 microcontroller running a lightweight classification model. The model was trained on 1500 seconds of recorded EEG data using a 6-frame sliding window with low-pass filtering, excluding poor-signal samples and using a 70/20/10 training--validation--test split. The classifier detects strong blink events, which toggle the hand between open and closed states. EMG signals are acquired using a MyoWare 2.0 sensor and SparkFun wireless shield and transmitted to a second ESP32, which performs threshold-based detection. Three activation bands (rest: 0--T1; extension: T1--T2; contraction: greater than T2) enable intuitive elbow control, with movement triggered only after eight consecutive frames in a movement class to improve stability. The EEG-controlled ESP32 actuates four finger servos, while the EMG-controlled ESP32 drives two elbow servos. A functional prototype was constructed using low-cost materials (total cost approximately 240 dollars), with most expense attributed to the commercial EEG headset. Future work includes transitioning to a 3D-printed chassis, integrating auto-regressive models to reduce EMG latency, and upgrading servo torque for improved load capacity and grip strength. This system demonstrates a feasible pathway to low-cost, biologically intuitive prosthetic control suitable for underserved and global health applications.

</details>


### [57] [Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.17444)
*Javier Gonzalez-Ruiz,Carlos Rodriguez-Pardo,Iacopo Savelli,Alice Di Bella,Massimo Tavoni*

Main category: cs.LG

TL;DR: 本文提出多智能体强化学习模型用于评估长期电力市场，应用于意大利电力系统，结果凸显市场设计对电力脱碳和稳定价格的关键作用。


<details>
  <summary>Details</summary>
Motivation: 需要更先进工具支持政策制定者等设计、测试和评估长期电力市场。

Method: 提出多智能体强化学习模型，发电公司做投资决策，采用独立近端策略优化，进行广泛超参数搜索。

Result: 模型应用于意大利电力系统，结果凸显市场设计对电力脱碳和避免价格波动的关键作用。

Conclusion: 所提框架可评估多种政策和市场机制同时作用的长期电力市场，参与者能响应和适应脱碳路径。

Abstract: Electricity systems are key to transforming today's society into a carbon-free economy. Long-term electricity market mechanisms, including auctions, support schemes, and other policy instruments, are critical in shaping the electricity generation mix. In light of the need for more advanced tools to support policymakers and other stakeholders in designing, testing, and evaluating long-term markets, this work presents a multi-agent reinforcement learning model capable of capturing the key features of decarbonizing energy systems. Profit-maximizing generation companies make investment decisions in the wholesale electricity market, responding to system needs, competitive dynamics, and policy signals. The model employs independent proximal policy optimization, which was selected for suitability to the decentralized and competitive environment. Nevertheless, given the inherent challenges of independent learning in multi-agent settings, an extensive hyperparameter search ensures that decentralized training yields market outcomes consistent with competitive behavior. The model is applied to a stylized version of the Italian electricity system and tested under varying levels of competition, market designs, and policy scenarios. Results highlight the critical role of market design for decarbonizing the electricity sector and avoiding price volatility. The proposed framework allows assessing long-term electricity markets in which multiple policy and market mechanisms interact simultaneously, with market participants responding and adapting to decarbonization pathways.

</details>


### [58] [QSMOTE-PGM/kPGM: QSMOTE Based PGM and kPGM for Imbalanced Dataset Classification](https://arxiv.org/abs/2512.16960)
*Bikash K. Behera,Giuseppe Sergioli,Robert Giuntini*

Main category: cs.LG

TL;DR: 文章对比量子启发机器学习（QiML）中核技巧和PGM方法，在QSMOTE场景实验，表明PGM和KPGM优于经典随机森林，各有优势，为方法应用提供指导。


<details>
  <summary>Details</summary>
Motivation: 对基于核的和基于测量的QiML方法进行统一的理论和实证比较。

Method: 分析核技巧和PGM等方法，在量子SMOTE的合成过采样场景中比较不同QiML范式性能。

Result: PGM和KPGM分类器性能优于经典随机森林基线，例如PGM特定编码下准确率和F1分数最高，KPGM表现更稳定。

Conclusion: 量子启发分类器能提升召回和平衡性能，PGM和KPGM各有优势，研究有助于理解和应用QiML方法。

Abstract: Quantum-inspired machine learning (QiML) leverages mathematical frameworks from quantum theory to enhance classical algorithms, with particular emphasis on inner product structures in high-dimensional feature spaces. Among the prominent approaches, the Kernel Trick, widely used in support vector machines, provides efficient similarity computation, while the Pretty Good Measurement (PGM), originating from quantum state discrimination, enables classification grounded in Hilbert space geometry. Building on recent developments in kernelized PGM (KPGM) and direct PGM-based classifiers, this work presents a unified theoretical and empirical comparison of these paradigms. We analyze their performance across synthetic oversampling scenarios using Quantum SMOTE (QSMOTE) variants. Experimental results show that both PGM and KPGM classifiers consistently outperform a classical random forest baseline, particularly when multiple quantum copies are employed. Notably, PGM with stereo encoding and n_copies=2 achieves the highest overall accuracy (0.8512) and F1-score (0.8234), while KPGM demonstrates competitive and more stable behavior across QSMOTE variants, with top scores of 0.8511 (stereo) and 0.8483 (amplitude). These findings highlight that quantum-inspired classifiers not only provide tangible gains in recall and balanced performance but also offer complementary strengths: PGM benefits from encoding-specific enhancements, whereas KPGM ensures robustness across sampling strategies. Our results advance the understanding of kernel-based and measurement-based QiML methods, offering practical guidance on their applicability under varying data characteristics and computational constraints.

</details>


### [59] [Compression is Routing: Reconstruction Error as an Intrinsic Signal for Modular Language Models](https://arxiv.org/abs/2512.16963)
*Zhongpan Tang*

Main category: cs.LG

TL;DR: 本文针对大语言模型的问题提出“压缩即路由”架构哲学，用自编码器实现序列长度压缩，利用重建误差实现专家模块调度，为可扩展模块化神经网络提供新思路。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型面临上下文长度限制、推理成本高和灾难性遗忘挑战，混合专家架构的路由机制有系统复杂和缺乏可解释性问题。

Method: 提出“压缩即路由”哲学，训练87M参数的端到端Transformer自编码器实现64倍序列长度压缩。

Result: 该压缩机有极强领域判别能力，不同域重建准确率差异大，可将重建误差作为内在分布指纹，能直接用重建残差调度专家模块。

Conclusion: 验证了基础架构的物理有效性，为下一代可扩展模块化神经网络提供新研究视角。

Abstract: Current Large Language Models (LLMs) face three major challenges: context length limitations, high inference costs, and catastrophic forgetting during continual learning. While Mixture-of-Experts (MoE) architectures mitigate some of these conflicts, their routing mechanisms typically rely on explicitly trained auxiliary classifiers. This not only increases system complexity but also often lacks interpretability when handling mixed-domain inputs.
  Building upon the premise that ``Compression is Intelligence,'' this paper proposes a novel architectural philosophy: \textbf{``Compression is Routing.''} We trained an 87M-parameter end-to-end Transformer Autoencoder, achieving a \textbf{64x sequence length compression} (compressing 512 tokens into 8 latent vectors). Experimental results demonstrate that this compressor possesses extreme domain discriminative capability: it achieves a reconstruction accuracy of \textbf{99.47\%} on the in-domain (code) validation set; accuracy drops sharply to \textbf{47.76\%} on a semi-out-of-distribution domain (Wiki text); and further plummets to just \textbf{0.57\%} on a fully out-of-distribution domain (random sequences).
  This extreme and systematic performance discrepancy establishes the validity of reconstruction error as an \textbf{Intrinsic Distribution Fingerprint}. Based on this, we propose that expert modules can be automatically scheduled using reconstruction residuals directly, without the need for explicit gating networks. This mechanism offers excellent scalability. Furthermore, this architecture provides a new perspective on ``VRAM compression'' for handling ultra-long contexts. This report aims to verify the physical validity of this foundational architecture, offering a new research perspective for the next generation of scalable modular neural networks.

</details>


### [60] [Physics-Informed Lightweight Machine Learning for Aviation Visibility Nowcasting Across Multiple Climatic Regimes](https://arxiv.org/abs/2512.16967)
*Marcelo Cerda Castillo*

Main category: cs.LG

TL;DR: 本文提出基于地面观测数据和物理引导特征工程的轻量级梯度提升框架XGBoost进行低能见度和降水事件短期预报，在多机场验证表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有低能见度和降水事件短期预报方法计算量大、有保守偏差且时间分辨率有限，需改进。

Method: 采用轻量级梯度提升框架XGBoost，基于地面观测数据（METAR），通过基于热力学原理的物理引导特征工程进行训练，在11个不同气候区域机场评估。

Result: 模型成功捕捉当地物理过程，未手动配置；与TAF预报对比，3小时战术预报中检测率大幅提高，召回率提升2.5 - 4.0倍，减少误报；SHAP分析表明模型能隐式重建当地物理驱动因素。

Conclusion: 该XGBoost框架在低能见度和降水事件短期预报中表现良好，具有可解释性，有实际应用价值。

Abstract: Short-term prediction (nowcasting) of low-visibility and precipitation events is critical for aviation safety and operational efficiency. Current operational approaches rely on computationally intensive numerical weather prediction guidance and human-issued TAF products, which often exhibit conservative biases and limited temporal resolution. This study presents a lightweight gradient boosting framework (XGBoost) trained exclusively on surface observation data (METAR) and enhanced through physics-guided feature engineering based on thermodynamic principles. The framework is evaluated across 11 international airports representing distinct climatic regimes (including SCEL, KJFK, KORD, KDEN, SBGR, and VIDP) using historical data from 2000 to 2024. Results suggest that the model successfully captures underlying local physical processes without manual configuration. In a blind comparative evaluation against operational TAF forecasts, the automated model achieved substantially higher detection rates at tactical horizons (3 hours), with a 2.5 to 4.0 times improvement in recall while reducing false alarms. Furthermore, SHAP analysis reveals that the model performs an implicit reconstruction of local physical drivers (advection, radiation, and subsidence), providing actionable explainability for operational situational awareness.
  Keywords: aviation meteorology; physics-guided machine learning; explainable artificial intelligence; lightweight machine learning; nowcasting; METAR; TAF verification; edge computing

</details>


### [61] [Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs](https://arxiv.org/abs/2512.17008)
*Junbo Li,Peng Zhou,Rui Meng,Meet P. Vadera,Lihong Li,Yang Li*

Main category: cs.LG

TL;DR: 本文探讨强化学习训练交互LLM智能体时GRPO算法在多轮任务的局限，提出turn - PPO算法，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 直接将GRPO算法应用于多轮任务存在局限，尤其是长程推理场景，需要更稳定有效的优势估计策略。

Method: 先探索PPO作为替代算法，再引入在轮次级MDP上运行的turn - PPO算法。

Result: 在WebShop和Sokoban数据集上，无论有无长推理组件，turn - PPO都显示出有效性。

Conclusion: turn - PPO能有效解决多轮任务中强化学习算法的问题，比常见算法更有效。

Abstract: Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.

</details>


### [62] [GB-DQN: Gradient Boosted DQN Models for Non-stationary Reinforcement Learning](https://arxiv.org/abs/2512.17034)
*Chang-Hwan Lee,Chanseung Lee*

Main category: cs.LG

TL;DR: 提出GB - DQN方法处理深度强化学习在非平稳环境中的模型漂移问题，理论证明有效，实验显示优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 非平稳环境会使深度强化学习中已学习的值函数失效，导致灾难性遗忘，需要解决模型漂移问题。

Method: 提出Gradient - Boosted Deep Q - Networks (GB - DQN) 自适应集成方法，通过增量残差学习处理模型漂移，构建加法集成，新学习者近似当前集成漂移后的贝尔曼残差。

Result: 理论上证明每个提升步骤能减少经验贝尔曼残差，集成在标准假设下收敛到漂移后的最优值函数；实验表明在动态变化的控制任务中比DQN和常见非平稳基线有更快恢复能力、更好稳定性和更强鲁棒性。

Conclusion: GB - DQN方法能有效应对深度强化学习在非平稳环境中的模型漂移问题。

Abstract: Non-stationary environments pose a fundamental challenge for deep reinforcement learning, as changes in dynamics or rewards invalidate learned value functions and cause catastrophic forgetting. We propose \emph{Gradient-Boosted Deep Q-Networks (GB-DQN)}, an adaptive ensemble method that addresses model drift through incremental residual learning. Instead of retraining a single Q-network, GB-DQN constructs an additive ensemble in which each new learner is trained to approximate the Bellman residual of the current ensemble after drift. We provide theoretical results showing that each boosting step reduces the empirical Bellman residual and that the ensemble converges to the post-drift optimal value function under standard assumptions. Experiments across a diverse set of control tasks with controlled dynamics changes demonstrate faster recovery, improved stability, and greater robustness compared to DQN and common non-stationary baselines.

</details>


### [63] [SFBD-OMNI: Bridge models for lossy measurement restoration with limited clean samples](https://arxiv.org/abs/2512.17051)
*Haoye Lu,Yaoliang Yu,Darren Ho*

Main category: cs.LG

TL;DR: 研究利用大量噪声样本进行分布恢复，提出SFBD - OMNI框架，实验显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实中获取全观测样本代价高或不可行，而部分和噪声观测样本易收集，研究利用噪声样本恢复分布。

Method: 将任务构建为单边熵最优传输问题，用类EM算法求解，提供测试准则，引入基于桥模型的SFBD - OMNI框架。

Result: 方法能将有噪样本分布映射到真实分布，在基准数据集和不同测量设置实验中，定性和定量性能均有显著提升。

Conclusion: 提出的方法可处理高斯噪声外的任意测量模型，推广了SFBD方法。

Abstract: In many real-world scenarios, obtaining fully observed samples is prohibitively expensive or even infeasible, while partial and noisy observations are comparatively easy to collect. In this work, we study distribution restoration with abundant noisy samples, assuming the corruption process is available as a black-box generator. We show that this task can be framed as a one-sided entropic optimal transport problem and solved via an EM-like algorithm. We further provide a test criterion to determine whether the true underlying distribution is recoverable under per-sample information loss, and show that in otherwise unrecoverable cases, a small number of clean samples can render the distribution largely recoverable. Building on these insights, we introduce SFBD-OMNI, a bridge model-based framework that maps corrupted sample distributions to the ground-truth distribution. Our method generalizes Stochastic Forward-Backward Deconvolution (SFBD; Lu et al., 2025) to handle arbitrary measurement models beyond Gaussian corruption. Experiments across benchmark datasets and diverse measurement settings demonstrate significant improvements in both qualitative and quantitative performance.

</details>


### [64] [Dynamic Tool Dependency Retrieval for Efficient Function Calling](https://arxiv.org/abs/2512.17052)
*Bhrij Patel,Davide Belli,Amir Jalalirad,Maximilian Arnold,Aleksandr Ermovol,Bence Major*

Main category: cs.LG

TL;DR: 提出动态工具依赖检索（DTDR）方法，在多数据集和大语言模型骨干上进行基准测试，结果显示比现有的静态检索器提高了23%到104%的函数调用成功率。


<details>
  <summary>Details</summary>
Motivation: 现有检索方法依赖静态和有限输入，无法捕捉多步工具依赖和演变的任务上下文，导致引入无关工具，降低效率和准确性。

Method: 提出动态工具依赖检索（DTDR）方法，其基于初始查询和不断演变的执行上下文，还对函数调用演示中的工具依赖进行建模，并使用多种检索性能指标评估。

Result: 动态工具检索比现有最先进的静态检索器提高了23%到104%的函数调用成功率。

Conclusion: DTDR方法能有效提升函数调用成功率，改善函数调用代理的性能。

Abstract: Function calling agents powered by Large Language Models (LLMs) select external tools to automate complex tasks. On-device agents typically use a retrieval module to select relevant tools, improving performance and reducing context length. However, existing retrieval methods rely on static and limited inputs, failing to capture multi-step tool dependencies and evolving task context. This limitation often introduces irrelevant tools that mislead the agent, degrading efficiency and accuracy. We propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial query and the evolving execution context. DTDR models tool dependencies from function calling demonstrations, enabling adaptive retrieval as plans unfold. We benchmark DTDR against state-of-the-art retrieval methods across multiple datasets and LLM backbones, evaluating retrieval precision, downstream task accuracy, and computational efficiency. Additionally, we explore strategies to integrate retrieved tools into prompts. Our results show that dynamic tool retrieval improves function calling success rates between $23\%$ and $104\%$ compared to state-of-the-art static retrievers.

</details>


### [65] [Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs](https://arxiv.org/abs/2512.17131)
*Aaron Defazio,Konstantin Mishchenko,Parameswaran Raman,Hao-Jun Michael Shi,Lin Xiao*

Main category: cs.LG

TL;DR: 提出广义原始平均法（GPA）改进基于平均的优化器，克服单工作者DiLoCo等方法的局限，实验表现优且有理论保证。


<details>
  <summary>Details</summary>
Motivation: 解决近期基于平均的优化器如单工作者DiLoCo和Schedule - Free在非分布式设置中的关键局限。

Method: 通过解耦Nesterov原始平均公式中的插值常数，使GPA能在每一步平滑平均迭代。

Result: GPA始终优于单工作者DiLoCo，去除双循环结构，简化超参数调整，减少内存开销，在不同模型和任务上有速度提升。

Conclusion: 对于后悔值有O(√T)界的任何基础优化器，GPA能根据插值常数选择匹配或超越原优化器的收敛保证。

Abstract: We propose Generalized Primal Averaging (GPA), an extension of Nesterov's method in its primal averaging formulation that addresses key limitations of recent averaging-based optimizers such as single-worker DiLoCo and Schedule-Free (SF) in the non-distributed setting. These two recent algorithmic approaches improve the performance of base optimizers, such as AdamW, through different iterate averaging strategies. Schedule-Free explicitly maintains a uniform average of past weights, while single-worker DiLoCo performs implicit averaging by periodically aggregating trajectories, called pseudo-gradients, to update the model parameters. However, single-worker DiLoCo's periodic averaging introduces a two-loop structure, increasing its memory requirements and number of hyperparameters. GPA overcomes these limitations by decoupling the interpolation constant in the primal averaging formulation of Nesterov. This decoupling enables GPA to smoothly average iterates at every step, generalizing and improving upon single-worker DiLoCo. Empirically, GPA consistently outperforms single-worker DiLoCo while removing the two-loop structure, simplifying hyperparameter tuning, and reducing its memory overhead to a single additional buffer. On the Llama-160M model, GPA provides a 24.22% speedup in terms of steps to reach the baseline (AdamW's) validation loss. Likewise, GPA achieves speedups of 12% and 27% on small and large batch setups, respectively, to attain AdamW's validation accuracy on the ImageNet ViT workload. Furthermore, we prove that for any base optimizer with regret bounded by $O(\sqrt{T})$, where $T$ is the number of iterations, GPA can match or exceed the convergence guarantee of the original optimizer, depending on the choice of interpolation constants.

</details>


### [66] [Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. III](https://arxiv.org/abs/2512.17058)
*Vladimir G. Pestov*

Main category: cs.LG

TL;DR: 证明完备可分度量空间X中三个条件的等价性中最后一个蕴含关系(1)⇒(3)，并纠正之前文章错误说法


<details>
  <summary>Details</summary>
Motivation: 完成完备可分度量空间X中条件(1)、(2)、(3)等价性证明，其中前两个蕴含关系前人已完成证明，需证明剩下的(1)⇒(3)

Method: 未提及具体方法

Result: 证明了蕴含关系(1)⇒(3)，解决了之前研究的猜想，并纠正了错误说法

Conclusion: 完备可分度量空间X中条件(1)、(2)、(3)等价性得证

Abstract: We prove the last remaining implication allowing to claim the equivalence of the following conditions for a complete separable metric space $X$:
  (1) The $k$-nearest neighbour classifier is (weakly) universally consistent in $X$, (2) The strong Lebesgue--Besicovitch differentiation property holds in $X$ for every locally finite Borel measure, (3) $X$ is sigma-finite dimensional in the sense of Nagata.
  The equivalence (2)$\iff$(3) was announced by Preiss (1983), while a detailed proof of the implication (3)$\Rightarrow$(2) has appeared in Assouad and Quentin de Gromard (2006). The implication (2)$\Rightarrow$(1) was established by Cérou and Guyader (2006). We prove the implication (1)$\Rightarrow$(3). The result was conjectured in the first article in the series (Collins, Kumari, Pestov 2020), and here we also correct a wrong claim made in the second article (Kumari and Pestov 2024).

</details>


### [67] [Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs](https://arxiv.org/abs/2512.17352)
*Ivan Kralj,Lodovico Giaretta,Gordan Ježić,Ivana Podnar Žarko,Šarūnas Girdzijauskas*

Main category: cs.LG

TL;DR: 提出自适应剪枝算法降低ST - GNNs在边缘部署的通信开销，引入SEPA指标，实验表明算法可降低通信成本且不影响对关键交通事件响应能力。


<details>
  <summary>Details</summary>
Motivation: ST - GNNs在边缘分布式计算节点部署时，相邻云节点间重复传输重叠节点特征会产生大量通信开销。

Method: 提出自适应剪枝算法动态过滤冗余邻域特征，引入SEPA指标测量对交通变化响应能力，在多种联邦学习设置和不同预测时间跨度下进行评估。

Result: 与标准指标相比，SEPA揭示空间连接性在预测动态和不规则交通的价值；自适应剪枝算法在所有在线半分布式设置中维持预测精度，显著降低通信成本。

Conclusion: 在不影响对关键交通事件响应能力下可以减少通信开销。

Abstract: Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.

</details>


### [68] [Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation](https://arxiv.org/abs/2512.17073)
*Zhenyu Liu,Yunzhen Liu,Zehao Fan,Garrett Gagnon,Yayue Hou,Nan Wu,Yangwook Kang,Liu Liu*

Main category: cs.LG

TL;DR: 提出通过低秩补偿实现带宽高效自适应混合专家模型，结合卸载在GPU和GPU - NDP系统中提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有混合专家模型存在内存和带宽问题，卸载造成推理的I/O瓶颈，静态均匀量化在压缩时因忽略专家异质性而降低精度。

Method: 使用预计算的低秩补偿器进行路由引导的精度恢复，推理时转移紧凑的低秩因子并应用补偿，同时部分保持低比特。

Result: 方法在GPU和GPU - NDP系统中结合卸载，能带来更好的带宽 - 准确性权衡和更高的吞吐量。

Conclusion: 所提方法可有效解决混合专家模型的内存和带宽问题，提升模型整体性能。

Abstract: Mixture-of-Experts (MoE) models scale capacity via sparse activation but stress memory and bandwidth. Offloading alleviates GPU memory by fetching experts on demand, yet token-level routing causes irregular transfers that make inference I/O-bound. Static uniform quantization reduces traffic but degrades accuracy under aggressive compression by ignoring expert heterogeneity. We present Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation, which performs router-guided precision restoration using precomputed low-rank compensators. At inference time, our method transfers compact low-rank factors with Top-n (n<k) experts per token and applies compensation to them, keeping others low-bit. Integrated with offloading on GPU and GPU-NDP systems, our method delivers a superior bandwidth-accuracy trade-off and improved throughput.

</details>


### [69] [meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis](https://arxiv.org/abs/2512.17409)
*Dishantkumar Sutariya,Eike Petersen*

Main category: cs.LG

TL;DR: 提出统计工具包解决机器学习模型分层性能分析难题，在医学影像应用案例中进行了展示。


<details>
  <summary>Details</summary>
Motivation: 以严谨统计方法分析机器学习模型按患者和记录属性分层的性能存在挑战，需合适方法解决。

Method: 提出一个统计工具包来应对样本量、基础比率差异比较、指标不确定性、多重比较校正和交叉分析等问题。

Result: 通过ISIC2020数据集的皮肤病变恶性分类和MIMIC - CXR数据集的胸部X光疾病分类两个案例展示了工具包的分析效果。

Conclusion: 该工具包可让从业者轻松且严谨地评估模型潜在的亚组性能差异，尤其适用于医学影像应用。

Abstract: Analyzing machine learning model performance stratified by patient and recording properties is becoming the accepted norm and often yields crucial insights about important model failure modes. Performing such analyses in a statistically rigorous manner is non-trivial, however. Appropriate performance metrics must be selected that allow for valid comparisons between groups of different sample sizes and base rates; metric uncertainty must be determined and multiple comparisons be corrected for, in order to assess whether any observed differences may be purely due to chance; and in the case of intersectional analyses, mechanisms must be implemented to find the most `interesting' subgroups within combinatorially many subgroup combinations. We here present a statistical toolbox that addresses these challenges and enables practitioners to easily yet rigorously assess their models for potential subgroup performance disparities. While broadly applicable, the toolbox is specifically designed for medical imaging applications. The analyses provided by the toolbox are illustrated in two case studies, one in skin lesion malignancy classification on the ISIC2020 dataset and one in chest X-ray-based disease classification on the MIMIC-CXR dataset.

</details>


### [70] [Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?](https://arxiv.org/abs/2512.17079)
*Saraswathy Amjith,Mihika Dusad,Neha Muramalla,Shweta Shah*

Main category: cs.LG

TL;DR: 研究通过在有缺陷推理轨迹上训练大语言模型，使其能检测和纠正错误，提升数学推理鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在链式思维提示的数学推理中易受早期错误影响，研究能否通过训练使其在不降低标准解题能力下检测并纠正错误。

Method: 使用MATH - lighteval竞赛级问题生成含一个控制错误的链式思维前缀，用二元最终答案奖励通过GRPO微调Qwen3 - 4B。

Result: Mixed - CoT - RL模型在无错误问题上与标准RL表现相当，在含缺陷推理问题上大幅超越，纯干净问题的RL微调降低模型鲁棒性。训练推理错误比计算错误效果更好，混合训练最佳。

Conclusion: 训练时接触有缺陷推理轨迹可提升错误恢复能力且不牺牲准确性，为大语言模型更鲁棒的数学推理提供方向。

Abstract: Chain-of-thought (CoT) prompting has become central to mathematical reasoning in large language models, yet models remain brittle to early errors: a single arithmetic slip or unjustified inference typically propagates uncorrected to an incorrect final answer. We investigate whether training on intentionally flawed reasoning traces can teach models to detect and recover from such errors without degrading standard problem-solving ability. Using competition-level problems from MATH-lighteval, we generate CoT prefixes containing exactly one controlled error, either a calculation error (sign flips, dropped terms) or a reasoning error (misapplied rules, unjustified logical steps), and fine-tune Qwen3-4B with GRPO using a binary final-answer reward. Our Mixed-CoT-RL model matches standard RL on clean problems (41% vs 41%) while substantially outperforming it on problems prefilled with flawed reasoning (24% vs 19%). Notably, clean-only RL fine-tuning degrades robustness below the untuned baseline 19% vs. 20%), indicating that conventional training increases susceptibility to misleading prefills. Among error types, training on reasoning errors yields greater robustness gains than calculation errors alone, with mixed training performing best. These findings demonstrate that exposure to flawed traces during training can improve error-recovery behavior without sacrificing accuracy, suggesting a path toward more robust mathematical reasoning in LLMs.

</details>


### [71] [How to Square Tensor Networks and Circuits Without Squaring Them](https://arxiv.org/abs/2512.17090)
*Lorenzo Loconte,Adrián Javaloy,Antonio Vergari*

Main category: cs.LG

TL;DR: 本文提出对平方电路进行参数化以克服其边缘化开销，实验表明该方法在不损失表达能力前提下实现更高效学习。


<details>
  <summary>Details</summary>
Motivation: 平方张量网络和平方电路作为分布估计器时，平方运算在计算配分函数或边缘化变量时引入额外复杂性，阻碍其在机器学习中的应用，且现有规范形式不适用于电路。

Method: 受规范形式正交性和电路确定性思想启发，对平方电路进行参数化。

Result: 即使在与张量网络不同但编码为电路的因式分解中也能实现高效边缘化。

Conclusion: 提出的平方电路条件在不损失表达能力的同时，能实现更高效的学习。

Abstract: Squared tensor networks (TNs) and their extension as computational graphs--squared circuits--have been used as expressive distribution estimators, yet supporting closed-form marginalization. However, the squaring operation introduces additional complexity when computing the partition function or marginalizing variables, which hinders their applicability in ML. To solve this issue, canonical forms of TNs are parameterized via unitary matrices to simplify the computation of marginals. However, these canonical forms do not apply to circuits, as they can represent factorizations that do not directly map to a known TN. Inspired by the ideas of orthogonality in canonical forms and determinism in circuits enabling tractable maximization, we show how to parameterize squared circuits to overcome their marginalization overhead. Our parameterizations unlock efficient marginalization even in factorizations different from TNs, but encoded as circuits, whose structure would otherwise make marginalization computationally hard. Finally, our experiments on distribution estimation show how our proposed conditions in squared circuits come with no expressiveness loss, while enabling more efficient learning.

</details>


### [72] [A Systems-Theoretic View on the Convergence of Algorithms under Disturbances](https://arxiv.org/abs/2512.17598)
*Guner Dilsad Er,Sebastian Trimpe,Michael Muehlebach*

Main category: cs.LG

TL;DR: 本文扩展孤立算法的收敛保证，推导有干扰情况下的稳定性边界和收敛速率，并展示其在多领域应用。


<details>
  <summary>Details</summary>
Motivation: 算法在复杂系统中会受到干扰、噪声等影响，需扩展孤立算法的收敛保证。

Method: 利用逆李雅普诺夫定理推导量化干扰影响的关键不等式。

Result: 得到有干扰情况下的稳定性边界和收敛速率，可用于评估干扰对算法性能的影响。

Conclusion: 该结果是有噪声、干扰和互联情况下算法分析的统一工具。

Abstract: Algorithms increasingly operate within complex physical, social, and engineering systems where they are exposed to disturbances, noise, and interconnections with other dynamical systems. This article extends known convergence guarantees of an algorithm operating in isolation (i.e., without disturbances) and systematically derives stability bounds and convergence rates in the presence of such disturbances. By leveraging converse Lyapunov theorems, we derive key inequalities that quantify the impact of disturbances. We further demonstrate how our result can be utilized to assess the effects of disturbances on algorithmic performance in a wide variety of applications, including communication constraints in distributed learning, sensitivity in machine learning generalization, and intentional noise injection for privacy. This underpins the role of our result as a unifying tool for algorithm analysis in the presence of noise, disturbances, and interconnections with other dynamical systems.

</details>


### [73] [Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making](https://arxiv.org/abs/2512.17091)
*Toshiaki Hori,Jonathan DeCastro,Deepak Gopinath,Avinash Balachandran,Guy Rosman*

Main category: cs.LG

TL;DR: 提出融合强化学习和MPC规划解决分层结构规划问题的新方法，在多领域表现良好，数据效率和性能更佳。


<details>
  <summary>Details</summary>
Motivation: 解决具有分层结构的规划问题，提升规划方法的鲁棒性和适应性。

Method: 将强化学习和MPC规划紧密耦合，利用强化学习动作指导MPPI采样器，自适应聚合MPPI样本进行价值估计。

Result: 在多个领域展示出更好的数据效率和整体性能，成功率最高提升72%，收敛速度加快2.1倍。

Conclusion: 该方法是一种鲁棒的规划方法，能处理复杂规划问题，易适应不同应用。

Abstract: We propose a new approach for solving planning problems with a hierarchical structure, fusing reinforcement learning and MPC planning. Our formulation tightly and elegantly couples the two planning paradigms. It leverages reinforcement learning actions to inform the MPPI sampler, and adaptively aggregates MPPI samples to inform the value estimation. The resulting adaptive process leverages further MPPI exploration where value estimates are uncertain, and improves training robustness and the overall resulting policies. This results in a robust planning approach that can handle complex planning problems and easily adapts to different applications, as demonstrated over several domains, including race driving, modified Acrobot, and Lunar Lander with added obstacles. Our results in these domains show better data efficiency and overall performance in terms of both rewards and task success, with up to a 72% increase in success rate compared to existing approaches, as well as accelerated convergence (x2.1) compared to non-adaptive sampling.

</details>


### [74] [Convergence Guarantees for Federated SARSA with Local Training and Heterogeneous Agents](https://arxiv.org/abs/2512.17688)
*Paul Mangold,Eloïse Berthier,Eric Moulines*

Main category: cs.LG

TL;DR: 对带线性函数逼近和本地训练的FedSARSA进行理论分析，给出收敛保证、样本和通信复杂度界，证明线性加速，数值实验支持理论。


<details>
  <summary>Details</summary>
Motivation: 对FedSARSA在存在局部转换和奖励异质性情况下进行理论分析，给出复杂度界。

Method: 提出新的单智能体SARSA多步误差精确展开方法。

Result: 量化异质性影响，证明FedSARSA多次本地更新收敛，在一定条件下实现线性加速。

Conclusion: 理论分析有效，数值实验支持理论结果。

Abstract: We present a novel theoretical analysis of Federated SARSA (FedSARSA) with linear function approximation and local training. We establish convergence guarantees for FedSARSA in the presence of heterogeneity, both in local transitions and rewards, providing the first sample and communication complexity bounds in this setting. At the core of our analysis is a new, exact multi-step error expansion for single-agent SARSA, which is of independent interest. Our analysis precisely quantifies the impact of heterogeneity, demonstrating the convergence of FedSARSA with multiple local updates. Crucially, we show that FedSARSA achieves linear speed-up with respect to the number of agents, up to higher-order terms due to Markovian sampling. Numerical experiments support our theoretical findings.

</details>


### [75] [UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data](https://arxiv.org/abs/2512.17100)
*Justin Li,Efe Sencan,Jasper Zheng Duan,Vitus J. Leung,Stephan Tsaur,Ayse K. Coskun*

Main category: cs.LG

TL;DR: 提出UniCoMTE框架为多元时间序列分类器生成反事实解释，在ECG分类器上评估，结果显示优于现有方法，提升深度学习模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型黑盒特性限制其在高风险领域的信任和应用，需提升可解释性。

Method: 引入UniCoMTE框架，通过修改输入样本评估对模型预测的影响来识别关键时间特征，该框架与多种模型架构兼容。

Result: 在ECG分类器上评估，生成的解释简洁、稳定、符合人类认知，在清晰度和适用性上优于现有方法。

Conclusion: 该框架将模型预测与有意义的信号模式关联，推进深度学习模型在现实时间序列应用中的可解释性。

Abstract: Machine learning models, particularly deep neural networks, have demonstrated strong performance in classifying complex time series data. However, their black-box nature limits trust and adoption, especially in high-stakes domains such as healthcare. To address this challenge, we introduce UniCoMTE, a model-agnostic framework for generating counterfactual explanations for multivariate time series classifiers. The framework identifies temporal features that most heavily influence a model's prediction by modifying the input sample and assessing its impact on the model's prediction. UniCoMTE is compatible with a wide range of model architectures and operates directly on raw time series inputs. In this study, we evaluate UniCoMTE's explanations on a time series ECG classifier. We quantify explanation quality by comparing our explanations' comprehensibility to comprehensibility of established techniques (LIME and SHAP) and assessing their generalizability to similar samples. Furthermore, clinical utility is assessed through a questionnaire completed by medical experts who review counterfactual explanations presented alongside original ECG samples. Results show that our approach produces concise, stable, and human-aligned explanations that outperform existing methods in both clarity and applicability. By linking model predictions to meaningful signal patterns, the framework advances the interpretability of deep learning models for real-world time series applications.

</details>


### [76] [Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting](https://arxiv.org/abs/2512.17696)
*Yuri Calleo*

Main category: cs.LG

TL;DR: 提出空间感知变压器混合架构，结合地统计归纳偏差与自注意力机制，在合成和真实数据上表现优于现有方法，弥合物理感知建模与数据驱动学习的差距。


<details>
  <summary>Details</summary>
Motivation: 经典地统计学和深度学习在高维时空过程建模中各有优劣，高斯过程计算成本高，现代变压器架构缺乏几何归纳偏差，需要一种新方法。

Method: 提出空间感知变压器，通过可学习协方差核将地统计归纳偏差注入自注意力机制，分解注意力结构并施加软拓扑约束。

Result: 网络实现“深度变差分析”，在合成和真实数据实验中优于现有图神经网络，能提供准确预测和校准良好的概率预测。

Conclusion: 该方法有效弥合物理感知建模与数据驱动学习的差距。

Abstract: The modeling of high-dimensional spatio-temporal processes presents a fundamental dichotomy between the probabilistic rigor of classical geostatistics and the flexible, high-capacity representations of deep learning. While Gaussian processes offer theoretical consistency and exact uncertainty quantification, their prohibitive computational scaling renders them impractical for massive sensor networks. Conversely, modern transformer architectures excel at sequence modeling but inherently lack a geometric inductive bias, treating spatial sensors as permutation-invariant tokens without a native understanding of distance. In this work, we propose a spatially-informed transformer, a hybrid architecture that injects a geostatistical inductive bias directly into the self-attention mechanism via a learnable covariance kernel. By formally decomposing the attention structure into a stationary physical prior and a non-stationary data-driven residual, we impose a soft topological constraint that favors spatially proximal interactions while retaining the capacity to model complex dynamics. We demonstrate the phenomenon of ``Deep Variography'', where the network successfully recovers the true spatial decay parameters of the underlying process end-to-end via backpropagation. Extensive experiments on synthetic Gaussian random fields and real-world traffic benchmarks confirm that our method outperforms state-of-the-art graph neural networks. Furthermore, rigorous statistical validation confirms that the proposed method delivers not only superior predictive accuracy but also well-calibrated probabilistic forecasts, effectively bridging the gap between physics-aware modeling and data-driven learning.

</details>


### [77] [Fault Diagnosis and Quantification for Photovoltaic Arrays based on Differentiable Physical Models](https://arxiv.org/abs/2512.17107)
*Zenan Yang,Yuanliang Li,Jingwei Zhang,Yongjie Liu,Kun Ding*

Main category: cs.LG

TL;DR: 本文提出基于可微快速故障仿真模型（DFFSM）的光伏串故障量化方法，通过梯度故障参数识别（GFPI）实现高效故障量化，实验验证其高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有光伏阵列故障量化方法效率和可解释性有限，需新方法。

Method: 提出DFFSM准确建模多故障下I - V特性并提供梯度，开发基于Adahessian优化器的GFPI方法量化故障。

Result: 实验表明GFPI在模拟和实测I - V曲线上对不同故障的量化准确性高，I - V重建误差低于3%。

Conclusion: 可微物理模拟器在光伏系统故障诊断中的应用可行且有效。

Abstract: Accurate fault diagnosis and quantification are essential for the reliable operation and intelligent maintenance of photovoltaic (PV) arrays. However, existing fault quantification methods often suffer from limited efficiency and interpretability. To address these challenges, this paper proposes a novel fault quantification approach for PV strings based on a differentiable fast fault simulation model (DFFSM). The proposed DFFSM accurately models I-V characteristics under multiple faults and provides analytical gradients with respect to fault parameters. Leveraging this property, a gradient-based fault parameters identification (GFPI) method using the Adahessian optimizer is developed to efficiently quantify partial shading, short-circuit, and series-resistance degradation. Experimental results on both simulated and measured I-V curves demonstrate that the proposed GFPI achieves high quantification accuracy across different faults, with the I-V reconstruction error below 3%, confirming the feasibility and effectiveness of the application of differentiable physical simulators for PV system fault diagnosis.

</details>


### [78] [Mitigating Forgetting in Low Rank Adaptation](https://arxiv.org/abs/2512.17720)
*Joanna Sliwa,Frank Schneider,Philipp Hennig,Jose Miguel Hernandez-Lobato*

Main category: cs.LG

TL;DR: 提出LaLoRA解决参数高效微调中灾难性遗忘问题，在微调Llama模型时展现更好的学习遗忘权衡。


<details>
  <summary>Details</summary>
Motivation: 解决参数高效微调方法导致模型先验领域知识灾难性遗忘的问题。

Method: 提出LaLoRA，对低秩适应应用拉普拉斯近似，仅对LoRA权重应用近似以保持轻量级，估计模型对每个参数的置信度并约束高曲率方向的更新。

Result: 在微调Llama模型进行数学推理时，展示了改进的学习遗忘权衡，可通过正则化强度直接控制。还探索不同损失景观曲率近似、分析拉普拉斯近似所用数据的影响及研究超参数鲁棒性。

Conclusion: LaLoRA能在保留先验知识的同时实现高效目标领域学习，且具有一定的可控性和鲁棒性。

Abstract: Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), enable fast specialization of large pre-trained models to different downstream applications. However, this process often leads to catastrophic forgetting of the model's prior domain knowledge. We address this issue with LaLoRA, a weight-space regularization technique that applies a Laplace approximation to Low-Rank Adaptation. Our approach estimates the model's confidence in each parameter and constrains updates in high-curvature directions, preserving prior knowledge while enabling efficient target-domain learning. By applying the Laplace approximation only to the LoRA weights, the method remains lightweight. We evaluate LaLoRA by fine-tuning a Llama model for mathematical reasoning and demonstrate an improved learning-forgetting trade-off, which can be directly controlled via the method's regularization strength. We further explore different loss landscape curvature approximations for estimating parameter confidence, analyze the effect of the data used for the Laplace approximation, and study robustness across hyperparameters.

</details>


### [79] [Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse](https://arxiv.org/abs/2512.17108)
*Kunjal Panchal,Saayan Mitra,Somdeb Sarkhel,Haoliang Wang,Ishita Dasgupta,Gang Wu,Hui Guan*

Main category: cs.LG

TL;DR: 介绍Atom系统优化视频语言模型在移动设备执行效率，分解模型重用模块，提升速度且性能损失小。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型多阶段管道在移动设备上因冗余加载和碎片化执行，高效执行有挑战。

Method: 将十亿参数模型分解为可重用模块，跨子任务重用，消除重复加载并支持并行执行。

Result: 在商用智能手机上比非重用基线快27 - 33%，性能仅边际下降。

Conclusion: Atom是边缘设备上高效视频语言理解的实用、可扩展方法。

Abstract: Recent advances in video-language models have enabled powerful applications like video retrieval, captioning, and assembly. However, executing such multi-stage pipelines efficiently on mobile devices remains challenging due to redundant model loads and fragmented execution. We introduce Atom, an on-device system that restructures video-language pipelines for fast and efficient execution. Atom decomposes a billion-parameter model into reusable modules, such as the visual encoder and language decoder, and reuses them across subtasks like captioning, reasoning, and indexing. This reuse-centric design eliminates repeated model loading and enables parallel execution, reducing end-to-end latency without sacrificing performance. On commodity smartphones, Atom achieves 27--33% faster execution compared to non-reuse baselines, with only marginal performance drop ($\leq$ 2.3 Recall@1 in retrieval, $\leq$ 1.5 CIDEr in captioning). These results position Atom as a practical, scalable approach for efficient video-language understanding on edge devices.

</details>


### [80] [Bridging Training and Merging Through Momentum-Aware Optimization](https://arxiv.org/abs/2512.17109)
*Alireza Moayedikia,Alicia Troncoso*

Main category: cs.LG

TL;DR: 提出统一框架，训练时维护因子化动量和曲率统计信息用于几何感知模型组合，在NLP基准测试表现好，消除冗余计算。


<details>
  <summary>Details</summary>
Motivation: 当前训练大模型和合并特定任务模型分别处理，计算曲率信息存在重复计算和丢弃有价值轨迹数据的问题。

Method: 引入统一框架，在训练时维护因子化动量和曲率统计信息，用于几何感知模型组合。

Result: 在自然语言理解基准测试中，曲率感知参数选择优于仅考虑大小的基线，多任务合并也优于强基线；框架具有秩不变收敛性和超参数鲁棒性。

Conclusion: 将优化轨迹视为可重用资产，消除冗余计算，实现更有原则的模型组合。

Abstract: Training large neural networks and merging task-specific models both exploit low-rank structure and require parameter importance estimation, yet these challenges have been pursued in isolation. Current workflows compute curvature information during training, discard it, then recompute similar information for merging -- wasting computation and discarding valuable trajectory data. We introduce a unified framework that maintains factorized momentum and curvature statistics during training, then reuses this information for geometry-aware model composition. The proposed method achieves memory efficiency comparable to state-of-the-art approaches while accumulating task saliency scores that enable curvature-aware merging without post-hoc Fisher computation. We establish convergence guarantees for non-convex objectives with approximation error bounded by gradient singular value decay. On natural language understanding benchmarks, curvature-aware parameter selection outperforms magnitude-only baselines across all sparsity levels, with multi-task merging improving over strong baselines. The proposed framework exhibits rank-invariant convergence and superior hyperparameter robustness compared to existing low-rank optimizers. By treating the optimization trajectory as a reusable asset rather than discarding it, our approach eliminates redundant computation while enabling more principled model composition.

</details>


### [81] [Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow](https://arxiv.org/abs/2512.17878)
*Herlock Rahimi*

Main category: cs.LG

TL;DR: 本文聚焦基于分数的扩散模型，指出其在非凸或多模态分布下混合率差的问题，引入基于信息几何的重加权机制，通过显式修正项和加权随机微分方程实现，初步探究WFR采样动力学。


<details>
  <summary>Details</summary>
Motivation: 现有基于分数的扩散模型在处理非凸或多模态分布时混合率会指数恶化，而实际生成建模任务常涉及高度非对数凹目标分布，因此需开发超越经典扩散动力学的采样方案。

Method: 引入显式修正项，利用Feynman - Kac表示通过加权随机微分方程实现重加权机制。

Result: 对基于WFR的采样动力学进行初步但严谨的研究。

Conclusion: 为未来基于WFR采样动力学的理论和算法发展明确了几何和算子理论结构基础。

Abstract: Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.
  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.

</details>


### [82] [Digitizing Nepal's Written Heritage: A Comprehensive HTR Pipeline for Old Nepali Manuscripts](https://arxiv.org/abs/2512.17111)
*Anjali Sarawgi,Esteban Garces Arias,Christof Zotter*

Main category: cs.LG

TL;DR: 本文为旧尼泊尔语手写文本识别开发了首个端到端管道，采用行级转录方法达4.9%字符错误率，发布代码等支持后续研究。


<details>
  <summary>Details</summary>
Motivation: 旧尼泊尔语是有历史意义但资源少的语言，缺乏相关手写文本识别端到端管道。

Method: 采用行级转录方法，系统探索编码器 - 解码器架构和数据中心技术，实现并评估解码策略，分析token级混淆。

Result: 最佳模型字符错误率达4.9%，可更好理解模型行为和错误模式。

Conclusion: 发布训练代码、模型配置和评估脚本，以支持低资源历史脚本手写文本识别的进一步研究。

Abstract: This paper presents the first end-to-end pipeline for Handwritten Text Recognition (HTR) for Old Nepali, a historically significant but low-resource language. We adopt a line-level transcription approach and systematically explore encoder-decoder architectures and data-centric techniques to improve recognition accuracy. Our best model achieves a Character Error Rate (CER) of 4.9\%. In addition, we implement and evaluate decoding strategies and analyze token-level confusions to better understand model behaviour and error patterns. While the dataset we used for evaluation is confidential, we release our training code, model configurations, and evaluation scripts to support further research in HTR for low-resource historical scripts.

</details>


### [83] [Regularized Random Fourier Features and Finite Element Reconstruction for Operator Learning in Sobolev Space](https://arxiv.org/abs/2512.17884)
*Xinyue Yu,Hayden Schaeffer*

Main category: cs.LG

TL;DR: 提出RRFF - FEM方法用于从噪声数据中学习算子，展示了该方法的稳定性、性能和准确性。


<details>
  <summary>Details</summary>
Motivation: 基于核的算子学习在处理大数据集时计算开销大且对噪声敏感，因此需更好方法。

Method: 提出RRFF方法，结合有限元重建映射RRFF - FEM，使用多元学生t分布随机特征和频率加权Tikhonov正则化抑制高频噪声。

Result: 建立随机特征矩阵极端奇异值的高概率界，证明系统条件良好；数值实验表明RRFF和RRFF - FEM对噪声鲁棒，减少训练时间且保持竞争力。

Conclusion: RRFF和RRFF - FEM是从噪声数据中学习算子的有效方法。

Abstract: Operator learning is a data-driven approximation of mappings between infinite-dimensional function spaces, such as the solution operators of partial differential equations. Kernel-based operator learning can offer accurate, theoretically justified approximations that require less training than standard methods. However, they can become computationally prohibitive for large training sets and can be sensitive to noise. We propose a regularized random Fourier feature (RRFF) approach, coupled with a finite element reconstruction map (RRFF-FEM), for learning operators from noisy data. The method uses random features drawn from multivariate Student's $t$ distributions, together with frequency-weighted Tikhonov regularization that suppresses high-frequency noise. We establish high-probability bounds on the extreme singular values of the associated random feature matrix and show that when the number of features $N$ scales like $m \log m$ with the number of training samples $m$, the system is well-conditioned, which yields estimation and generalization guarantees. Detailed numerical experiments on benchmark PDE problems, including advection, Burgers', Darcy flow, Helmholtz, Navier-Stokes, and structural mechanics, demonstrate that RRFF and RRFF-FEM are robust to noise and achieve improved performance with reduced training time compared to the unregularized random feature model, while maintaining competitive accuracy relative to kernel and neural operator tests.

</details>


### [84] [The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining](https://arxiv.org/abs/2512.17121)
*Jasmine Vu,Shivanand Sheshappanavar*

Main category: cs.LG

TL;DR: 评估Stanford AIMI CheXagent模型处理含否定短语的胸部X光图像检索能力，通过微调改进，分析内部行为以提升CLIP在医学AI中处理否定的可靠性。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在医学影像任务中有用，但处理否定短语表现不佳，影响医学诊断，需改进其处理否定的能力。

Method: 评估Stanford AIMI CheXagent模型在含和不含否定提示下的图像检索能力，采用之前工作的微调方法改进，通过token归因、t - SNE投影和注意力头消融分析模型内部行为。

Result: CLIP模型处理否定能力提升，但肯定提示评估准确性略有下降。

Conclusion: 有望更好理解CLIP内部行为，利用临床相关语言提升其处理否定的能力，提高在医学AI设备中的可靠性。

Abstract: Large vision-language models like CLIP are increasingly used in medical imaging tasks due to their ability to align images and text without the need for extensive labeled data. This makes them particularly useful for applications like image retrieval, report generation, and classification in clinical settings. A potential issue to this approach is that CLIP-based models often under perform when interpreting negated phrases, which is especially problematic in the context of medical diagnosing. In this study, we evaluate the Stanford AIMI CheXagent model on its ability to correctly retrieve chest X-ray images using prompts with and without negation. The goal of this project is to understand where this model fails and then use it as a base model to improve its retrieval accuracy by fine tuning methods outlined in previous work. Results from this study show improvement in handling of negation in the CLIP model with a slight decrease in accuracy of positive prompt evaluation. Alongside retrieval accuracy, we examined internal model behavior through token attribution, t-SNE projection, and attention-head ablation to better characterize how each fine tuning approach reshaped the text encoders representation of negated clinical language. Through this work, we hope to better understand the internal behavior of CLIP and improve its handling of negation using clinically relevant language for improving its reliability in medical AI devices.

</details>


### [85] [DiffeoMorph: Learning to Morph 3D Shapes Using Differentiable Agent-Based Simulations](https://arxiv.org/abs/2512.17129)
*Seong Ho Pahng,Guoye Guan,Benjamin Fefferman,Sahand Hormoz*

Main category: cs.LG

TL;DR: 提出DiffeoMorph框架学习形态发生协议，引导智能体群体形成目标3D形状，有新的形状匹配损失函数，经测试有优势。


<details>
  <summary>Details</summary>
Motivation: 探究分布式控制如何产生精确全局模式。

Method: 使用基于注意力的SE(3)等变图神经网络更新智能体位置和状态，引入基于3D Zernike多项式的形状匹配损失函数，包含对齐步骤处理旋转不变性，通过隐式微分计算梯度。

Result: 通过系统基准测试证明形状匹配损失函数优于其他标准距离度量，DiffeoMorph能仅用最小空间线索形成多种形状。

Conclusion: DiffeoMorph框架在引导智能体群体形成目标3D形状方面有效，新的形状匹配损失函数有优势。

Abstract: Biological systems can form complex three-dimensional structures through the collective behavior of identical agents -- cells that follow the same internal rules and communicate without central control. How such distributed control gives rise to precise global patterns remains a central question not only in developmental biology but also in distributed robotics, programmable matter, and multi-agent learning. Here, we introduce DiffeoMorph, an end-to-end differentiable framework for learning a morphogenesis protocol that guides a population of agents to morph into a target 3D shape. Each agent updates its position and internal state using an attention-based SE(3)-equivariant graph neural network, based on its own internal state and signals received from other agents. To train this system, we introduce a new shape-matching loss based on the 3D Zernike polynomials, which compares the predicted and target shapes as continuous spatial distributions, not as discrete point clouds, and is invariant to agent ordering, number of agents, and rigid-body transformations. To enforce full SO(3) invariance -- invariant to rotations yet sensitive to reflections, we include an alignment step that optimally rotates the predicted Zernike spectrum to match the target before computing the loss. This results in a bilevel problem, with the inner loop optimizing a unit quaternion for the best alignment and the outer loop updating the agent model. We compute gradients through the alignment step using implicit differentiation. We perform systematic benchmarking to establish the advantages of our shape-matching loss over other standard distance metrics for shape comparison tasks. We then demonstrate that DiffeoMorph can form a range of shapes -- from simple ellipsoids to complex morphologies -- using only minimal spatial cues.

</details>


### [86] [Distributed Learning in Markovian Restless Bandits over Interference Graphs for Stable Spectrum Sharing](https://arxiv.org/abs/2512.17161)
*Liad Lea Didi,Kobi Cohen*

Main category: cs.LG

TL;DR: 研究通信受限无线网络中多认知通信实体的频谱接入与共享，提出SMILE算法实现全局稳定的信道分配，证明其收敛性和对数遗憾，模拟验证性能。


<details>
  <summary>Details</summary>
Motivation: 在通信受限无线网络中，实现多认知通信实体间的全局稳定且考虑干扰的信道分配。

Method: 开发SMILE分布式学习算法，将 restless bandit学习与图约束协调结合，让各单元平衡探索与利用。

Result: 证明SMILE收敛到最优稳定分配，实现相对于全知精灵的对数遗憾，模拟验证理论，展示其鲁棒性、可扩展性和效率。

Conclusion: SMILE算法能有效解决通信受限无线网络中的频谱接入与共享问题，实现全局稳定的信道分配。

Abstract: We study distributed learning for spectrum access and sharing among multiple cognitive communication entities, such as cells, subnetworks, or cognitive radio users (collectively referred to as cells), in communication-constrained wireless networks modeled by interference graphs. Our goal is to achieve a globally stable and interference-aware channel allocation. Stability is defined through a generalized Gale-Shapley multi-to-one matching, a well-established solution concept in wireless resource allocation. We consider wireless networks where L cells share S orthogonal channels and cannot simultaneously use the same channel as their neighbors. Each channel evolves as an unknown restless Markov process with cell-dependent rewards, making this the first work to establish global Gale-Shapley stability for channel allocation in a stochastic, temporally varying restless environment. To address this challenge, we develop SMILE (Stable Multi-matching with Interference-aware LEarning), a communication-efficient distributed learning algorithm that integrates restless bandit learning with graph-constrained coordination. SMILE enables cells to distributedly balance exploration of unknown channels with exploitation of learned information. We prove that SMILE converges to the optimal stable allocation and achieves logarithmic regret relative to a genie with full knowledge of expected utilities. Simulations validate the theoretical guarantees and demonstrate SMILE's robustness, scalability, and efficiency across diverse spectrum-sharing scenarios.

</details>


### [87] [BumpNet: A Sparse Neural Network Framework for Learning PDE Solutions](https://arxiv.org/abs/2512.17198)
*Shao-Ting Chiu,Ioannis G. Kevrekidis,Ulisses Braga-Neto*

Main category: cs.LG

TL;DR: 介绍用于PDE数值解和算子学习的稀疏神经网络框架BumpNet，结合现有架构提出Bump - PINNs、Bump - EDNN和Bump - DeepONet，实验证明其高效准确。


<details>
  <summary>Details</summary>
Motivation: 引入新的稀疏神经网络框架用于PDE数值解和算子学习，利用现代训练技术并实现模型简约和h - 适应性。

Method: 基于无网格基函数展开，用普通sigmoid激活函数构建基函数，训练中动态修剪基函数；结合PINNs、EDNNs和DeepONets提出新架构。

Result: 通过大量数值实验证明所提架构具有高效性和准确性。

Conclusion: BumpNet是一个通用框架，结合现有神经架构能有效解决PDE问题。

Abstract: We introduce BumpNet, a sparse neural network framework for PDE numerical solution and operator learning. BumpNet is based on meshless basis function expansion, in a similar fashion to radial-basis function (RBF) networks. Unlike RBF networks, the basis functions in BumpNet are constructed from ordinary sigmoid activation functions. This enables the efficient use of modern training techniques optimized for such networks. All parameters of the basis functions, including shape, location, and amplitude, are fully trainable. Model parsimony and h-adaptivity are effectively achieved through dynamically pruning basis functions during training. BumpNet is a general framework that can be combined with existing neural architectures for learning PDE solutions: here, we propose Bump-PINNs (BumpNet with physics-informed neural networks) for solving general PDEs; Bump-EDNN (BumpNet with evolutionary deep neural networks) to solve time-evolution PDEs; and Bump-DeepONet (BumpNet with deep operator networks) for PDE operator learning. Bump-PINNs are trained using the same collocation-based approach used by PINNs, Bump-EDNN uses a BumpNet only in the spatial domain and uses EDNNs to advance the solution in time, while Bump-DeepONets employ a BumpNet regression network as the trunk network of a DeepONet. Extensive numerical experiments demonstrate the efficiency and accuracy of the proposed architecture.

</details>


### [88] [Learning solution operator of dynamical systems with diffusion maps kernel ridge regression](https://arxiv.org/abs/2512.17203)
*Jiwoo Song,Daning Huang,John Harlim*

Main category: cs.LG

TL;DR: 本文提出DM - KRR方法用于复杂动力系统长期预测，在多系统中表现优于现有方法，强调长期预测要考虑数据几何约束。


<details>
  <summary>Details</summary>
Motivation: 许多科学和工程系统具有复杂非线性动力学，数据驱动模型在长期预测中，因几何结构未知或表示不佳性能下降。

Method: 将简单的核岭回归框架与动力学感知验证策略结合，采用从扩散映射导出的数据驱动核，提出DM - KRR方法。

Result: 在多种系统中，DM - KRR在准确性和数据效率上始终优于最先进的随机特征、神经网络和算子学习方法。

Conclusion: 长期预测能力不仅取决于模型表达能力，关键在于通过动态一致模型选择尊重数据中的几何约束，该方法为复杂动力系统学习提供了可靠高效的途径。

Abstract: Many scientific and engineering systems exhibit complex nonlinear dynamics that are difficult to predict accurately over long time horizons. Although data-driven models have shown promise, their performance often deteriorates when the geometric structures governing long-term behavior are unknown or poorly represented. We demonstrate that a simple kernel ridge regression (KRR) framework, when combined with a dynamics-aware validation strategy, provides a strong baseline for long-term prediction of complex dynamical systems. By employing a data-driven kernel derived from diffusion maps, the proposed Diffusion Maps Kernel Ridge Regression (DM-KRR) method implicitly adapts to the intrinsic geometry of the system's invariant set, without requiring explicit manifold reconstruction or attractor modeling, procedures that often limit predictive performance. Across a broad range of systems, including smooth manifolds, chaotic attractors, and high-dimensional spatiotemporal flows, DM-KRR consistently outperforms state-of-the-art random feature, neural-network and operator-learning methods in both accuracy and data efficiency. These findings underscore that long-term predictive skill depends not only on model expressiveness, but critically on respecting the geometric constraints encoded in the data through dynamically consistent model selection. Together, simplicity, geometry awareness, and strong empirical performance point to a promising path for reliable and efficient learning of complex dynamical systems.

</details>


### [89] [Electric Vehicle Charging Load Forecasting: An Experimental Comparison of Machine Learning Methods](https://arxiv.org/abs/2512.17257)
*Iason Kyriakopoulos,Yannis Theodoridis*

Main category: cs.LG

TL;DR: 本文探讨不同时空尺度下五种时间序列模型对电动汽车充电需求预测的有效性，用四个真实数据集分析。


<details>
  <summary>Details</summary>
Motivation: 电动汽车普及引发电网管理担忧，现有研究缺乏不同时空尺度下多预测方法的系统比较。

Method: 研究五种时间序列预测模型，评估短、中、长期及不同空间尺度的预测性能，用四个公开真实数据集分析。

Result: 文中未明确提及具体结果，将独立报告各数据集分析结果。

Conclusion: 这是首次用多个真实数据集在广泛时空尺度下系统评估电动汽车充电需求预测的研究。

Abstract: With the growing popularity of electric vehicles as a means of addressing climate change, concerns have emerged regarding their impact on electric grid management. As a result, predicting EV charging demand has become a timely and important research problem. While substantial research has addressed energy load forecasting in transportation, relatively few studies systematically compare multiple forecasting methods across different temporal horizons and spatial aggregation levels in diverse urban settings. This work investigates the effectiveness of five time series forecasting models, ranging from traditional statistical approaches to machine learning and deep learning methods. Forecasting performance is evaluated for short-, mid-, and long-term horizons (on the order of minutes, hours, and days, respectively), and across spatial scales ranging from individual charging stations to regional and city-level aggregations. The analysis is conducted on four publicly available real-world datasets, with results reported independently for each dataset. To the best of our knowledge, this is the first work to systematically evaluate EV charging demand forecasting across such a wide range of temporal horizons and spatial aggregation levels using multiple real-world datasets.

</details>


### [90] [SHARP-QoS: Sparsely-gated Hierarchical Adaptive Routing for joint Prediction of QoS](https://arxiv.org/abs/2512.17262)
*Suraj Kumar,Arvind Kumar,Soumi Chattopadhyay*

Main category: cs.LG

TL;DR: 本文提出SHARP - QoS统一策略用于联合QoS预测，通过三个组件解决现有方法问题，在多数据集上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现实世界QoS数据稀疏、有噪声且存在层次依赖，现有单参数预测和联合预测方法有计算成本高、泛化性差、负迁移等问题，需要更好的联合QoS预测方法。

Method: 提出SHARP - QoS策略，包含通过双曲卷积提取层次特征的双机制、自适应特征共享机制和基于EMA的损失平衡策略。

Result: 在三个含不同数量QoS参数的数据集上，SHARP - QoS优于单任务和多任务基线模型。

Conclusion: SHARP - QoS能有效解决稀疏性、离群点鲁棒性和冷启动等挑战，计算开销适中，可进行可靠的联合QoS预测。

Abstract: Dependable service-oriented computing relies on multiple Quality of Service (QoS) parameters that are essential to assess service optimality. However, real-world QoS data are extremely sparse, noisy, and shaped by hierarchical dependencies arising from QoS interactions, and geographical and network-level factors, making accurate QoS prediction challenging. Existing methods often predict each QoS parameter separately, requiring multiple similar models, which increases computational cost and leads to poor generalization. Although recent joint QoS prediction studies have explored shared architectures, they suffer from negative transfer due to loss-scaling caused by inconsistent numerical ranges across QoS parameters and further struggle with inadequate representation learning, resulting in degraded accuracy. This paper presents an unified strategy for joint QoS prediction, called SHARP-QoS, that addresses these issues using three components. First, we introduce a dual mechanism to extract the hierarchical features from both QoS and contextual structures via hyperbolic convolution formulated in the Poincaré ball. Second, we propose an adaptive feature-sharing mechanism that allows feature exchange across informative QoS and contextual signals. A gated feature fusion module is employed to support dynamic feature selection among structural and shared representations. Third, we design an EMA-based loss balancing strategy that allows stable joint optimization, thereby mitigating the negative transfer. Evaluations on three datasets with two, three, and four QoS parameters demonstrate that SHARP-QoS outperforms both single- and multi-task baselines. Extensive study shows that our model effectively addresses major challenges, including sparsity, robustness to outliers, and cold-start, while maintaining moderate computational overhead, underscoring its capability for reliable joint QoS prediction.

</details>


### [91] [A Theoretical Analysis of State Similarity Between Markov Decision Processes](https://arxiv.org/abs/2512.17265)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: 本文提出广义双模拟度量（GBSM）用于衡量任意马尔可夫决策过程（MDP）对之间的状态相似性，证明其基本度量性质，理论分析跨MDP的策略转移等问题，获得更优界，给出封闭形式样本复杂度，数值结果验证有效性。


<details>
  <summary>Details</summary>
Motivation: 双模拟度量（BSM）在多个MDP间的状态相似性应用存在挑战，先前扩展缺乏良好数学性质，限制理论分析。

Method: 正式建立GBSM，证明其对称性、MDP间三角不等式和相同空间距离界三个基本度量性质。

Result: 理论分析跨MDP的策略转移、状态聚合和基于采样的估计，获得比标准BSM更严格的显式界，给出封闭形式样本复杂度。

Conclusion: 数值结果验证理论发现，表明GBSM在多MDP场景中有效。

Abstract: The bisimulation metric (BSM) is a powerful tool for analyzing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to state similarity between multiple MDPs remains challenging. Prior work has attempted to extend BSM to pairs of MDPs, but a lack of well-established mathematical properties has limited further theoretical analysis between MDPs. In this work, we formally establish a generalized bisimulation metric (GBSM) for measuring state similarity between arbitrary pairs of MDPs, which is rigorously proven with three fundamental metric properties, i.e., GBSM symmetry, inter-MDP triangle inequality, and a distance bound on identical spaces. Leveraging these properties, we theoretically analyze policy transfer, state aggregation, and sampling-based estimation across MDPs, obtaining explicit bounds that are strictly tighter than existing ones derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.

</details>


### [92] [Understanding Generalization in Role-Playing Models via Information Theory](https://arxiv.org/abs/2512.17270)
*Yongqi Li,Hao Lang,Fei Huang,Tieyun Qian,Yongbin Li*

Main category: cs.LG

TL;DR: 本文引入信息论指标R - EMID衡量角色扮演模型（RPMs）性能退化，推导其上界，提出协同进化强化学习框架，评估发现用户偏移风险最高，强化学习提升泛化最有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以对分布偏移如何影响RPM泛化进行细粒度诊断，缺乏表征RPM泛化行为的正式框架。

Method: 引入信息论指标R - EMID，推导其上界，提出协同进化强化学习框架。

Result: 评估发现用户偏移在所有偏移中风险最高，强化学习是提升RPM泛化最有效的方法。

Conclusion: R - EMID可解释地衡量RPM性能退化，强化学习能有效提升RPM泛化性能。

Abstract: Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.

</details>


### [93] [MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics](https://arxiv.org/abs/2512.17273)
*Farinaz Mostajeran,Aruzhan Tleubek,Salah A Faroughi*

Main category: cs.LG

TL;DR: 提出Memory - Informed Neural Pseudo - Operator (MINPO)统一框架求解含非局部时空行为的积分 - 微分方程，实验证明其准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 经典方法求解积分 - 微分方程成本高，现有神经求解器不能跨不同非局部结构泛化。

Method: 使用Kolmogorov - Arnold Networks (KANs)或多层感知器网络 (MLPs)作为编码器，通过神经表示直接学习非局部算子及其逆，用轻量级非局部一致性损失项指导学习。

Result: 与经典技术和基于MLP的先进神经策略相比，MINPO在处理不同核类型、核维度和重复核积分计算需求方面表现出准确性和鲁棒性。

Conclusion: MINPO超越特定问题公式，为非局部算子控制的系统提供统一框架。

Abstract: Many physical systems exhibit nonlocal spatiotemporal behaviors described by integro-differential equations (IDEs). Classical methods for solving IDEs require repeatedly evaluating convolution integrals, whose cost increases quickly with kernel complexity and dimensionality. Existing neural solvers can accelerate selected instances of these computations, yet they do not generalize across diverse nonlocal structures. In this work, we introduce the Memory-Informed Neural Pseudo-Operator (MINPO), a unified framework for modeling nonlocal dynamics arising from long-range spatial interactions and/or long-term temporal memory. MINPO, employing either Kolmogorov-Arnold Networks (KANs) or multilayer perceptron networks (MLPs) as encoders, learns the nonlocal operator and its inverse directly through neural representations, and then explicitly reconstruct the unknown solution fields. The learning is guarded by a lightweight nonlocal consistency loss term to enforce coherence between the learned operator and reconstructed solution. The MINPO formulation allows to naturally capture and efficiently resolve nonlocal spatiotemporal dependencies governed by a wide spectrum of IDEs and their subsets, including fractional PDEs. We evaluate the efficacy of MINPO in comparison with classical techniques and state-of-the-art neural-based strategies based on MLPs, such as A-PINN and fPINN, along with their newly-developed KAN variants, A-PIKAN and fPIKAN, designed to facilitate a fair comparison. Our study offers compelling evidence of the accuracy of MINPO and demonstrates its robustness in handling (i) diverse kernel types, (ii) different kernel dimensionalities, and (iii) the substantial computational demands arising from repeated evaluations of kernel integrals. MINPO, thus, generalizes beyond problem-specific formulations, providing a unified framework for systems governed by nonlocal operators.

</details>


### [94] [Alzheimer's Disease Brain Network Mining](https://arxiv.org/abs/2512.17276)
*Alireza Moayedikia,Sara Fin*

Main category: cs.LG

TL;DR: 介绍MATCH - AD半监督框架解决AD诊断中机器学习方法的挑战，在少部分有标签样本下取得高诊断准确率，减少标注负担。


<details>
  <summary>Details</summary>
Motivation: 临床评估AD昂贵且有创，神经影像数据集只有部分有真实标签，传统机器学习方法面临挑战。

Method: 引入MATCH - AD半监督框架，集成深度表征学习、基于图的标签传播和最优传输理论，利用神经影像数据的流形结构传播诊断信息，用Wasserstein距离量化认知状态间疾病进展。

Result: 在近五千名受试者上评估，不到三分之一样本有标签时MATCH - AD诊断准确率接近完美，大幅超越基线方法，在标签严重稀缺时仍有临床实用性，有理论收敛保证。

Conclusion: 有原则的半监督学习可挖掘全球部分标注的神经影像数据诊断潜力，减少标注负担并保持适合临床应用的准确性。

Abstract: Machine learning approaches for Alzheimer's disease (AD) diagnosis face a fundamental challenges. Clinical assessments are expensive and invasive, leaving ground truth labels available for only a fraction of neuroimaging datasets. We introduce Multi view Adaptive Transport Clustering for Heterogeneous Alzheimer's Disease (MATCH-AD), a semi supervised framework that integrates deep representation learning, graph-based label propagation, and optimal transport theory to address this limitation. The framework leverages manifold structure in neuroimaging data to propagate diagnostic information from limited labeled samples to larger unlabeled populations, while using Wasserstein distances to quantify disease progression between cognitive states. Evaluated on nearly five thousand subjects from the National Alzheimer's Coordinating Center, encompassing structural MRI measurements from hundreds of brain regions, cerebrospinal fluid biomarkers, and clinical variables MATCHAD achieves near-perfect diagnostic accuracy despite ground truth labels for less than one-third of subjects. The framework substantially outperforms all baseline methods, achieving kappa indicating almost perfect agreement compared to weak agreement for the best baseline, a qualitative transformation in diagnostic reliability. Performance remains clinically useful even under severe label scarcity, and we provide theoretical convergence guarantees with proven bounds on label propagation error and transport stability. These results demonstrate that principled semi-supervised learning can unlock the diagnostic potential of the vast repositories of partially annotated neuroimaging data accumulating worldwide, substantially reducing annotation burden while maintaining accuracy suitable for clinical deployment.

</details>


### [95] [M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge](https://arxiv.org/abs/2512.17299)
*Abdullah M. Zyarah,Dhireesha Kudithipudi*

Main category: cs.LG

TL;DR: 本文提出M2RU混合信号架构用于边缘平台持续学习，具有高能效和良好准确性。


<details>
  <summary>Details</summary>
Motivation: 边缘平台持续学习中循环网络训练耗能高、数据移动频繁，不适合嵌入式部署。

Method: 引入M2RU架构，集成加权位流和经验回放机制。

Result: M2RU实现15 GOPS，48.62 mW，每瓦312 GOPS，在相关任务上精度与软件基线相差5%以内，能效提升29倍，预期使用寿命12.2年。

Conclusion: M2RU是边缘级时间智能实时自适应的可扩展、高能效平台。

Abstract: Continual learning on edge platforms remains challenging because recurrent networks depend on energy-intensive training procedures and frequent data movement that are impractical for embedded deployments. This work introduces M2RU, a mixed-signal architecture that implements the minion recurrent unit for efficient temporal processing with on-chip continual learning. The architecture integrates weighted-bit streaming, which enables multi-bit digital inputs to be processed in crossbars without high-resolution conversion, and an experience replay mechanism that stabilizes learning under domain shifts. M2RU achieves 15 GOPS at 48.62 mW, corresponding to 312 GOPS per watt, and maintains accuracy within 5 percent of software baselines on sequential MNIST and CIFAR-10 tasks. Compared with a CMOS digital design, the accelerator provides 29X improvement in energy efficiency. Device-aware analysis shows an expected operational lifetime of 12.2 years under continual learning workloads. These results establish M2RU as a scalable and energy-efficient platform for real-time adaptation in edge-level temporal intelligence.

</details>


### [96] [Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability](https://arxiv.org/abs/2512.17316)
*Michael Merry,Pat Riddle,Jim Warren*

Main category: cs.LG

TL;DR: 本文提出了内在可解释性的全局适用标准，并用该标准解释了心血管疾病风险模型PREDICT，为可解释性研究提供结构支持。


<details>
  <summary>Details</summary>
Motivation: 现有内在可解释性缺乏统一的定义和测试方法。

Method: 使用图论表示和分解模型以进行局部结构解释，并重新组合为全局解释，将局部结构解释形成为注释。

Result: 该标准符合现有内在可解释性的直觉，解释了为何大型回归模型可能不可解释而稀疏神经网络可能可解释，区分了可解释和已解释的模型，证明PREDICT具有内在可解释性。

Conclusion: 此工作为可解释性的其他研究提供了结构化形式，并为监管者提供了一种灵活且严格的测试方法。

Abstract: Inherent explainability is the gold standard in Explainable Artificial Intelligence (XAI). However, there is not a consistent definition or test to demonstrate inherent explainability. Work to date either characterises explainability through metrics, or appeals to intuition - "we know it when we see it". We propose a globally applicable criterion for inherent explainability. The criterion uses graph theory for representing and decomposing models for structure-local explanation, and recomposing them into global explanations. We form the structure-local explanations as annotations, a verifiable hypothesis-evidence structure that allows for a range of explanatory methods to be used. This criterion matches existing intuitions on inherent explainability, and provides justifications why a large regression model may not be explainable but a sparse neural network could be. We differentiate explainable -- a model that allows for explanation -- and \textit{explained} -- one that has a verified explanation. Finally, we provide a full explanation of PREDICT -- a Cox proportional hazards model of cardiovascular disease risk, which is in active clinical use in New Zealand. It follows that PREDICT is inherently explainable. This work provides structure to formalise other work on explainability, and allows regulators a flexible but rigorous test that can be used in compliance frameworks.

</details>


### [97] [Task Schema and Binding: A Double Dissociation Study of In-Context Learning](https://arxiv.org/abs/2512.17325)
*Chaeha Kim*

Main category: cs.LG

TL;DR: 研究表明上下文学习（ICL）可分解为任务模式和绑定两种机制，揭示了三者关键特性，为双过程理论提供证据，具有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 深入理解ICL机制，挑战以往将其视为单一机制的观点

Method: 对9个来自7个Transformer家族的模型及Mamba进行激活补丁实验

Result: 发现双分离、先验 - 模式权衡、架构通用性等现象，还揭示注意力是真正瓶颈

Conclusion: 为ICL的双过程理论提供因果证据，有助于高效提示工程和提高ICL系统可靠性

Abstract: We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:
  1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- proving separable mechanisms
  2. Prior-Schema trade-off: Schema reliance inversely correlates with prior knowledge (Spearman rho = -0.596, p < 0.001, N=28 task-model pairs)
  3. Architecture generality: The mechanism operates across all tested architectures including the non-Transformer Mamba
  These findings offer a mechanistic account of the ICL puzzle that contrasts with prior views treating ICL as a monolithic mechanism (whether retrieval-based, gradient descent-like, or purely Bayesian). By establishing that Schema and Binding are neurally dissociable -- not merely behavioral modes -- we provide causal evidence for dual-process theories of ICL. Models rely on Task Schema when prior knowledge is absent, but prior knowledge interferes through attentional mis-routing (72.7% recency bias) rather than direct output competition (0%). This explains why arbitrary mappings succeed (zero prior leads to full Schema reliance) while factual overrides fail -- and reveals that the true bottleneck is attentional, not output-level. Practical implications: Understanding these dual mechanisms enables more efficient prompt engineering -- reliable schema transfer reduces required demonstrations for novel tasks, while prior-aware design can mitigate the 38% binding failure rate in high-prior scenarios, improving ICL system reliability in production deployments.

</details>


### [98] [Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach](https://arxiv.org/abs/2512.17367)
*Yidong Chai,Yi Liu,Mohammadreza Ebrahimi,Weifeng Li,Balaji Padmanabhan*

Main category: cs.LG

TL;DR: 社交媒体有害内容检测的机器学习模型易受对抗攻击，本文提出LLM - SGA框架和ARHOCD检测器，经实验验证其在对抗条件下有强泛化性和高检测准确率。


<details>
  <summary>Details</summary>
Motivation: 社交媒体有害内容检测的机器学习模型易受对抗攻击，需增强对抗鲁棒性，同时实现最优泛化性和准确性面临挑战。

Method: 先提出LLM - SGA框架确保检测器的泛化性；再通过三个新设计组件实例化ARHOCD检测器提高检测准确率，包括多基础检测器集成、基于样本可预测性和基础检测器能力动态调整权重的方法、迭代优化基础检测器和权重分配器的对抗训练策略。

Result: 在三个数据集上对ARHOCD进行评估，结果显示其在对抗条件下有强泛化性并提高了检测准确率。

Conclusion: 提出的方法解决了现有对抗鲁棒性增强研究的一些局限性，能有效提升有害内容检测能力。

Abstract: Social media platforms are plagued by harmful content such as hate speech, misinformation, and extremist rhetoric. Machine learning (ML) models are widely adopted to detect such content; however, they remain highly vulnerable to adversarial attacks, wherein malicious users subtly modify text to evade detection. Enhancing adversarial robustness is therefore essential, requiring detectors that can defend against diverse attacks (generalizability) while maintaining high overall accuracy. However, simultaneously achieving both optimal generalizability and accuracy is challenging. Following the computational design science paradigm, this study takes a sequential approach that first proposes a novel framework (Large Language Model-based Sample Generation and Aggregation, LLM-SGA) by identifying the key invariances of textual adversarial attacks and leveraging them to ensure that a detector instantiated within the framework has strong generalizability. Second, we instantiate our detector (Adversarially Robust Harmful Online Content Detector, ARHOCD) with three novel design components to improve detection accuracy: (1) an ensemble of multiple base detectors that exploits their complementary strengths; (2) a novel weight assignment method that dynamically adjusts weights based on each sample's predictability and each base detector's capability, with weights initialized using domain knowledge and updated via Bayesian inference; and (3) a novel adversarial training strategy that iteratively optimizes both the base detectors and the weight assignor. We addressed several limitations of existing adversarial robustness enhancement research and empirically evaluated ARHOCD across three datasets spanning hate speech, rumor, and extremist content. Results show that ARHOCD offers strong generalizability and improves detection accuracy under adversarial conditions.

</details>


### [99] [AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens](https://arxiv.org/abs/2512.17375)
*Tung-Ling Li,Yuhao Wu,Hongliang Liu*

Main category: cs.LG

TL;DR: 研究发现奖励模型和LLM评判系统存在易被低复杂度控制令牌操纵的漏洞，提出AdvJudge - Zero方法发现控制令牌序列，实证表明其致高误判率，LoRA对抗训练可降低误判。


<details>
  <summary>Details</summary>
Motivation: 探究现代训练后管道中奖励模型和LLM评判系统的潜在漏洞。

Method: 使用AdvJudge - Zero方法，利用模型下一个标记分布和波束搜索探索来发现控制令牌序列。

Result: 控制令牌会使大型公开权重和专业评判模型在数学和推理基准上对错误答案产生高误判率。

Conclusion: 基于LoRA的对抗训练在少量控制令牌增强示例上可显著降低误判率，同时保持评估质量。

Abstract: Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.

</details>


### [100] [DeepShare: Sharing ReLU Across Channels and Layers for Efficient Private Inference](https://arxiv.org/abs/2512.17398)
*Yonathan Bornfeld,Shai Avidan*

Main category: cs.LG

TL;DR: 本文聚焦私有推理（PI）中ReLU计算瓶颈，提出新激活模块减少DReLU操作，在多个分类和图像分割任务达SOTA。


<details>
  <summary>Details</summary>
Motivation: PI的主要计算瓶颈是门（ReLU）的计算，需减少网络中ReLU的数量。

Method: 关注DReLU，提出新激活模块，在部分通道（原型通道）执行DReLU操作，其余通道复制结果，并将该想法扩展到不同层。

Result: 大幅减少ResNet类型网络中DReLU操作数量，新公式能用一个非线性和两个神经元解决扩展版XOR问题，在多个分类任务和图像分割上达SOTA。

Conclusion: 所提新激活模块可有效减少DReLU操作，提升PI效率并取得优结果。

Abstract: Private Inference (PI) uses cryptographic primitives to perform privacy preserving machine learning. In this setting, the owner of the network runs inference on the data of the client without learning anything about the data and without revealing any information about the model. It has been observed that a major computational bottleneck of PI is the calculation of the gate (i.e., ReLU), so a considerable amount of effort have been devoted to reducing the number of ReLUs in a given network.
  We focus on the DReLU, which is the non-linear step function of the ReLU and show that one DReLU can serve many ReLU operations. We suggest a new activation module where the DReLU operation is only performed on a subset of the channels (Prototype channels), while the rest of the channels (replicate channels) replicates the DReLU of each of their neurons from the corresponding neurons in one of the prototype channels. We then extend this idea to work across different layers.
  We show that this formulation can drastically reduce the number of DReLU operations in resnet type network. Furthermore, our theoretical analysis shows that this new formulation can solve an extended version of the XOR problem, using just one non-linearity and two neurons, something that traditional formulations and some PI specific methods cannot achieve. We achieve new SOTA results on several classification setups, and achieve SOTA results on image segmentation.

</details>


### [101] [Learning What to Write: Write-Gated KV for Efficient Long-Context Inference](https://arxiv.org/abs/2512.17452)
*Yen-Chieh Huang,Rui Fang,Ming-Syan Chen,Pi-Cheng Hsiu*

Main category: cs.LG

TL;DR: 本文提出Write - Gated KV机制管理KV缓存，减少内存使用并提升推理速度，代码开源。


<details>
  <summary>Details</summary>
Motivation: 长上下文大语言模型推理受二次注意力复杂度和线性KV缓存增长瓶颈限制，以往方法未解决向持久内存无差别写入的低效问题。

Method: 将KV缓存管理形式化为包含KV Admission、Selection和Eviction三个原语的因果系统，通过Write - Gated KV实现KV Admission，提前过滤低效用状态。

Result: 在Llama模型上减少46 - 57%的内存使用，预填充加速3.03 - 3.45倍，解码加速1.89 - 2.56倍，精度损失可忽略，且与FlashAttention和paged - KV系统兼容。

Conclusion: 学习写入内容是高效长上下文推理的有效方法。

Abstract: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\times$ prefill and 1.89-2.56$\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .

</details>


### [102] [A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting](https://arxiv.org/abs/2512.17453)
*Henok Tenaw Moges,Deshendran Moodley*

Main category: cs.LG

TL;DR: 提出轻量级时空图神经网络Lite - STGNN用于长期多变量预测，在四个基准数据集上表现出色，提供高效框架。


<details>
  <summary>Details</summary>
Motivation: 解决长期多变量预测问题，寻求更高效、准确且参数高效的模型。

Method: 将基于分解的时间建模与可学习的稀疏图结构相结合，时间模块用趋势 - 季节分解，空间模块进行低秩Top - K邻接学习和保守的水平门控消息传递。

Result: 在四个基准数据集上达到720步的最先进精度，参数高效，训练速度比基于Transformer的方法快，消融研究显示各模块有提升效果。

Conclusion: Lite - STGNN为长期多变量时间序列预测提供了紧凑、可解释且高效的框架。

Abstract: We propose Lite-STGNN, a lightweight spatial-temporal graph neural network for long-term multivariate forecasting that integrates decomposition-based temporal modeling with learnable sparse graph structure. The temporal module applies trend-seasonal decomposition, while the spatial module performs message passing with low-rank Top-$K$ adjacency learning and conservative horizon-wise gating, enabling spatial corrections that enhance a strong linear baseline. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets for horizons up to 720 steps, while being parameter-efficient and substantially faster to train than transformer-based methods. Ablation studies show that the spatial module yields 4.6% improvement over the temporal baseline, Top-$K$ enhances locality by 3.3%, and learned adjacency matrices reveal domain-specific interaction dynamics. Lite-STGNN thus offers a compact, interpretable, and efficient framework for long-term multivariate time series forecasting.

</details>


### [103] [Deep Learning-Based Surrogate Creep Modelling in Inconel 625: A High-Temperature Alloy Study](https://arxiv.org/abs/2512.17477)
*Shubham Das,Kaushal Singhania,Amit Sadhu,Suprabhat Das,Arghya Nandi*

Main category: cs.LG

TL;DR: 本文提出基于深度学习的代理模型替代高温合金Inconel 625的有限元蠕变模拟，训练两种架构模型，结果显示模型预测速度快且准确，可用于快速蠕变评估。


<details>
  <summary>Details</summary>
Motivation: 高温合金Inconel 625的有限元蠕变模拟计算成本高，需快速准确的替代方法。

Method: 用Norton定律在ANSYS生成蠕变应变数据，训练BiLSTM Variational Autoencoder和BiLSTM Transformer hybrid两种模型，用RMSE、MAE和$R^2$评估性能。

Result: BiLSTM - VAE提供稳定可靠的蠕变应变预测，BiLSTM - Transformer全时间范围精度高，代理模型预测速度比ANSYS模拟快很多。

Conclusion: 所提框架可实现快速蠕变评估，为高温合金应用提供可扩展解决方案。

Abstract: Time-dependent deformation, particularly creep, in high-temperature alloys such as Inconel 625 is a key factor in the long-term reliability of components used in aerospace and energy systems. Although Inconel 625 shows excellent creep resistance, finite-element creep simulations in tools such as ANSYS remain computationally expensive, often requiring tens of minutes for a single 10,000-hour run. This work proposes deep learning based surrogate models to provide fast and accurate replacements for such simulations. Creep strain data was generated in ANSYS using the Norton law under uniaxial stresses of 50 to 150 MPa and temperatures of 700 to 1000 $^\circ$C, and this temporal dataset was used to train two architectures: a BiLSTM Variational Autoencoder for uncertainty-aware and generative predictions, and a BiLSTM Transformer hybrid that employs self-attention to capture long-range temporal behavior. Both models act as surrogate predictors, with the BiLSTM-VAE offering probabilistic output and the BiLSTM-Transformer delivering high deterministic accuracy. Performance is evaluated using RMSE, MAE, and $R^2$. Results show that the BiLSTM-VAE provides stable and reliable creep strain forecasts, while the BiLSTM-Transformer achieves strong accuracy across the full time range. Latency tests indicate substantial speedup: while each ANSYS simulation requires 30 to 40 minutes for a given stress-temperature condition, the surrogate models produce predictions within seconds. The proposed framework enables rapid creep assessment for design optimization and structural health monitoring, and provides a scalable solution for high-temperature alloy applications.

</details>


### [104] [SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals](https://arxiv.org/abs/2512.17527)
*Muhammad Haris Khan*

Main category: cs.LG

TL;DR: 提出SafeBench - Seq基准和分类器用于蛋白质序列危险筛查，评估其性能，发现随机分割高估鲁棒性，不同模型校准表现有差异。


<details>
  <summary>Details</summary>
Motivation: 蛋白质设计基础模型存在生物安全风险，社区缺乏在同源控制下评估且能在普通CPU运行的序列级危险筛查基线。

Method: 基于公共数据构建SafeBench - Seq，对数据集进行同源聚类和聚类级保留，报告评估指标并提供校准概率，用多种方法量化概率质量和探测捷径敏感性。

Result: 随机分割比同源聚类评估高估鲁棒性，校准线性模型校准较好，树集成的Brier/ECE稍高。

Conclusion: SafeBench - Seq仅需CPU、可复现且只发布元数据，能在不传播危险序列情况下进行严格评估。

Abstract: Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate "never-before-seen" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.

</details>


### [105] [NetworkFF: Unified Layer Optimization in Forward-Only Neural Networks](https://arxiv.org/abs/2512.17531)
*Salar Beigzad*

Main category: cs.LG

TL;DR: 本文提出协作式前向 - 前向（CFF）学习，通过层间协作机制扩展原算法，在MNIST和Fashion - MNIST上表现优于基线实现。


<details>
  <summary>Details</summary>
Motivation: 传统前向 - 前向算法存在层间隔离问题，限制了表征协调和深层架构的收敛效率，需要改进。

Method: 引入CFF学习，实现固定CFF（F - CFF）和自适应CFF（A - CFF）两种协作范式，协作良度函数整合各层加权贡献。

Result: 在MNIST和Fashion - MNIST上的综合评估显示，CFF学习比基线前向 - 前向实现有显著性能提升。

Conclusion: 层间协作是对前向 - 前向学习的根本改进，可应用于神经形态计算架构和能量受限的AI系统。

Abstract: The Forward-Forward algorithm eliminates backpropagation's memory constraints and biological implausibility through dual forward passes with positive and negative data. However, conventional implementations suffer from critical inter-layer isolation, where layers optimize goodness functions independently without leveraging collective learning dynamics. This isolation constrains representational coordination and limits convergence efficiency in deeper architectures. This paper introduces Collaborative Forward-Forward (CFF) learning, extending the original algorithm through inter-layer cooperation mechanisms that preserve forward-only computation while enabling global context integration. Our framework implements two collaborative paradigms: Fixed CFF (F-CFF) with constant inter-layer coupling and Adaptive CFF (A-CFF) with learnable collaboration parameters that evolve during training. The collaborative goodness function incorporates weighted contributions from all layers, enabling coordinated feature learning while maintaining memory efficiency and biological plausibility. Comprehensive evaluation on MNIST and Fashion-MNIST demonstrates significant performance improvements over baseline Forward-Forward implementations. These findings establish inter-layer collaboration as a fundamental enhancement to Forward-Forward learning, with immediate applicability to neuromorphic computing architectures and energy-constrained AI systems.

</details>


### [106] [Bayesian Optimisation: Which Constraints Matter?](https://arxiv.org/abs/2512.17569)
*Xietao Wang Lin,Juan Ungredda,Max Butler,James Town,Alma Rahat,Hemant Singh,Juergen Branke*

Main category: cs.LG

TL;DR: 本文针对具有解耦黑箱约束的问题提出知识梯度采集函数的贝叶斯优化新变种，并通过实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化是解决昂贵全局黑箱优化问题的有力工具，对于具有解耦黑箱约束的问题，现有的贝叶斯优化方法可能不够有效，需要新方法。

Method: 提出知识梯度采集函数的贝叶斯优化新变种，考虑到最优解处可能只有少数约束起作用，只评估相关约束。

Result: 通过实验对比，新方法优于现有方法。

Conclusion: 新提出的贝叶斯优化变种在具有解耦黑箱约束的问题上表现更优，具有一定的应用价值。

Abstract: Bayesian optimisation has proven to be a powerful tool for expensive global black-box optimisation problems. In this paper, we propose new Bayesian optimisation variants of the popular Knowledge Gradient acquisition functions for problems with \emph{decoupled} black-box constraints, in which subsets of the objective and constraint functions may be evaluated independently. In particular, our methods aim to take into account that often only a handful of the constraints may be binding at the optimum, and hence we should evaluate only relevant constraints when trying to optimise a function. We empirically benchmark these methods against existing methods and demonstrate their superiority over the state-of-the-art.

</details>


### [107] [Machine Learning for Static and Single-Event Dynamic Complex Network Analysis](https://arxiv.org/abs/2512.17577)
*Nikolaos Nakis*

Main category: cs.LG

TL;DR: 本文旨在为静态和单事件动态网络的图表示学习开发新算法，聚焦潜在空间模型，创建结构感知网络表示，定义统一学习过程。


<details>
  <summary>Details</summary>
Motivation: 开发新的图表示学习算法，创建结构感知网络表示，以全面有力地表征网络结构并处理图分析任务。

Method: 聚焦潜在空间模型中的潜在距离模型，设计统一学习过程，避免启发式和多阶段过程。

Result: 未提及具体结果。

Conclusion: 未提及具体结论，但目标是实现全面强大的统一网络嵌入。

Abstract: The primary objective of this thesis is to develop novel algorithmic approaches for Graph Representation Learning of static and single-event dynamic networks. In such a direction, we focus on the family of Latent Space Models, and more specifically on the Latent Distance Model which naturally conveys important network characteristics such as homophily, transitivity, and the balance theory. Furthermore, this thesis aims to create structural-aware network representations, which lead to hierarchical expressions of network structure, community characterization, the identification of extreme profiles in networks, and impact dynamics quantification in temporal networks. Crucially, the methods presented are designed to define unified learning processes, eliminating the need for heuristics and multi-stage processes like post-processing steps. Our aim is to delve into a journey towards unified network embeddings that are both comprehensive and powerful, capable of characterizing network structures and adeptly handling the diverse tasks that graph analysis offers.

</details>


### [108] [Learning Safe Autonomous Driving Policies Using Predictive Safety Representations](https://arxiv.org/abs/2512.17586)
*Mahesh Keswani,Raunak Bhattacharyya*

Main category: cs.LG

TL;DR: 本文研究SRPL框架在真实自动驾驶场景的适用性，实验表明其能改善奖励 - 安全权衡，效果受策略优化器和数据集分布影响，还能提升对观测噪声的鲁棒性和跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 安全强化学习在自动驾驶中存在性能优化和安全要求的矛盾，需研究SRPL框架能否应用于真实自动驾驶场景。

Method: 在Waymo Open Motion Dataset和NuPlan上进行系统实验。

Result: SRPL能改善奖励 - 安全权衡，成功率和成本降低有显著提升，效果受策略优化器和数据集分布影响，能提升对观测噪声的鲁棒性和跨数据集泛化能力。

Conclusion: 预测安全表示有潜力加强自动驾驶的安全强化学习。

Abstract: Safe reinforcement learning (SafeRL) is a prominent paradigm for autonomous driving, where agents are required to optimize performance under strict safety requirements. This dual objective creates a fundamental tension, as overly conservative policies limit driving efficiency while aggressive exploration risks safety violations. The Safety Representations for Safer Policy Learning (SRPL) framework addresses this challenge by equipping agents with a predictive model of future constraint violations and has shown promise in controlled environments. This paper investigates whether SRPL extends to real-world autonomous driving scenarios. Systematic experiments on the Waymo Open Motion Dataset (WOMD) and NuPlan demonstrate that SRPL can improve the reward-safety tradeoff, achieving statistically significant improvements in success rate (effect sizes r = 0.65-0.86) and cost reduction (effect sizes r = 0.70-0.83), with p < 0.05 for observed improvements. However, its effectiveness depends on the underlying policy optimizer and the dataset distribution. The results further show that predictive safety representations play a critical role in improving robustness to observation noise. Additionally, in zero-shot cross-dataset evaluation, SRPL-augmented agents demonstrate improved generalization compared to non-SRPL methods. These findings collectively demonstrate the potential of predictive safety representations to strengthen SafeRL for autonomous driving.

</details>


### [109] [Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models](https://arxiv.org/abs/2512.17592)
*Arthur Guijt,Dirk Thierens,Ellen Kerkhof,Jan Wiersma,Tanja Alderliesten,Peter A. N. Bosman*

Main category: cs.LG

TL;DR: 研究异步协作（仅共享已训练模型）对性能的影响，提出用拼接方法组合模型，发现该方法能使性能恢复且提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习依赖大量多样数据集，但在医疗等领域数据分散且难共享，联邦学习需同步协作，故研究异步协作。

Method: 采用多目标视角，用拼接层组合单独训练模型的中间表示。

Result: 单独在一方数据上训练，合并后在该方数据上性能相似，在其他方数据上变差；个体训练网络集成泛化性好，但各方自有数据集性能下降；拼接方法能使性能恢复并保持泛化提升。

Conclusion: 异步协作通过拼接方法可产生有竞争力的结果。

Abstract: Deep learning has been shown to be very capable at performing many real-world tasks. However, this performance is often dependent on the presence of large and varied datasets. In some settings, like in the medical domain, data is often fragmented across parties, and cannot be readily shared. While federated learning addresses this situation, it is a solution that requires synchronicity of parties training a single model together, exchanging information about model weights. We investigate how asynchronous collaboration, where only already trained models are shared (e.g. as part of a publication), affects performance, and propose to use stitching as a method for combining models.
  Through taking a multi-objective perspective, where performance on each parties' data is viewed independently, we find that training solely on a single parties' data results in similar performance when merging with another parties' data, when considering performance on that single parties' data, while performance on other parties' data is notably worse. Moreover, while an ensemble of such individually trained networks generalizes better, performance on each parties' own dataset suffers. We find that combining intermediate representations in individually trained models with a well placed pair of stitching layers allows this performance to recover to a competitive degree while maintaining improved generalization, showing that asynchronous collaboration can yield competitive results.

</details>


### [110] [A Unified Representation of Neural Networks Architectures](https://arxiv.org/abs/2512.17593)
*Christophe Prieur,Mircea Lazar,Bogdan Robu*

Main category: cs.LG

TL;DR: 本文研究神经网络架构在隐藏层神经元和隐藏层数趋于无穷时的极限情况，推导近似误差，统一多种神经网络表示。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在极限情况下的表现，推导近似误差并统一不同架构的表示。

Method: 先考虑单隐藏层神经网络得到积分无限宽神经表示，再扩展到深度残差CNN；通过离散化技术形式化近似误差；将两种方法合并成分布式参数神经网络（DiPaNet）表示。

Result: 得到积分无限宽神经表示，形式化近似误差，统一多种神经网络架构表示。

Conclusion: 提出DiPaNet框架，讨论与神经场的异同及可能的推广和应用。

Abstract: In this paper we consider the limiting case of neural networks (NNs) architectures when the number of neurons in each hidden layer and the number of hidden layers tend to infinity thus forming a continuum, and we derive approximation errors as a function of the number of neurons and/or hidden layers. Firstly, we consider the case of neural networks with a single hidden layer and we derive an integral infinite width neural representation that generalizes existing continuous neural networks (CNNs) representations. Then we extend this to deep residual CNNs that have a finite number of integral hidden layers and residual connections. Secondly, we revisit the relation between neural ODEs and deep residual NNs and we formalize approximation errors via discretization techniques. Then, we merge these two approaches into a unified homogeneous representation of NNs as a Distributed Parameter neural Network (DiPaNet) and we show that most of the existing finite and infinite-dimensional NNs architectures are related via homogeneization/discretization with the DiPaNet representation. Our approach is purely deterministic and applies to general, uniformly continuous matrix weight functions. Differences and similarities with neural fields are discussed along with further possible generalizations and applications of the DiPaNet framework.

</details>


### [111] [More Consistent Accuracy PINN via Alternating Easy-Hard Training](https://arxiv.org/abs/2512.17607)
*Zhaoqian Gao,Min Yanga*

Main category: cs.LG

TL;DR: 本文提出混合策略改进物理信息神经网络（PINNs）训练，在复杂PDEs上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有PINNs训练策略存在权衡和性能不一致问题，需改进。

Method: 开发结合硬优先级和易优先级的交替训练算法的混合策略。

Result: 在复杂PDEs上，相对L2误差多在O(10^-5)到O(10^-6)，远超基线方法，可靠性更高。

Conclusion: 为设计混合训练策略提升PINNs性能和鲁棒性提供新见解。

Abstract: Physics-informed neural networks (PINNs) have recently emerged as a prominent paradigm for solving partial differential equations (PDEs), yet their training strategies remain underexplored. While hard prioritization methods inspired by finite element methods are widely adopted, recent research suggests that easy prioritization can also be effective. Nevertheless, we find that both approaches exhibit notable trade-offs and inconsistent performance across PDE types. To address this issue, we develop a hybrid strategy that combines the strengths of hard and easy prioritization through an alternating training algorithm. On PDEs with steep gradients, nonlinearity, and high dimensionality, the proposed method achieves consistently high accuracy, with relative L2 errors mostly in the range of O(10^-5) to O(10^-6), significantly surpassing baseline methods. Moreover, it offers greater reliability across diverse problems, whereas compared approaches often suffer from variable accuracy depending on the PDE. This work provides new insights into designing hybrid training strategies to enhance the performance and robustness of PINNs.

</details>


### [112] [SCOPE: Sequential Causal Optimization of Process Interventions](https://arxiv.org/abs/2512.17629)
*Jakob De Moor,Hans Weytjens,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.LG

TL;DR: 提出SCOPE方法用于学习对齐的顺序干预建议，在优化KPI上优于现有技术，并提供半合成基准。


<details>
  <summary>Details</summary>
Motivation: 现有PresPM方法在处理干预序列对齐方面不足，存在现实差距和偏差问题。

Method: SCOPE采用反向归纳法估计干预行动效果，利用因果学习器直接使用观测数据。

Result: 在合成和半合成数据集实验中，SCOPE始终优于现有PresPM技术。

Conclusion: SCOPE是有效的PresPM方法，半合成设置可作为顺序PresPM未来工作的基准。

Abstract: Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.

</details>


### [113] [Trust-Region Adaptive Policy Optimization](https://arxiv.org/abs/2512.17636)
*Mingyu Su,Jian Guan,Yuxian Gu,Minlie Huang,Hongning Wang*

Main category: cs.LG

TL;DR: 提出TRAPO混合框架解决现有大语言模型训练两阶段管道的低效问题，实验显示其在数学推理基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型训练的两阶段管道（SFT然后RL）存在关键不一致性，SFT抑制探索并导致遗忘，限制了RL的改进潜力。

Method: 提出TRAPO混合框架，在每个训练实例中交错SFT和RL，优化专家前缀的SFT损失和模型自身补全的RL损失；引入Trust - Region SFT稳定训练；采用自适应前缀选择机制分配专家指导。

Result: 在五个数学推理基准测试中，TRAPO始终超过标准SFT、RL、SFT - then - RL管道以及最近的最先进方法。

Conclusion: TRAPO为增强推理能力的大语言模型建立了强大的新范式。

Abstract: Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\textbf{T}rust-\textbf{R}egion \textbf{A}daptive \textbf{P}olicy \textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.

</details>


### [114] [Estimating Spatially Resolved Radiation Fields Using Neural Networks](https://arxiv.org/abs/2512.17654)
*Felix Lehner,Pasquale Lombardo,Susana Castillo,Oliver Hupe,Marcus Magnor*

Main category: cs.LG

TL;DR: 本文深入分析如何构建和训练神经网络以估计医疗辐射场中散射辐射场的空间分布，用不同复杂度数据集训练并评估网络架构，相关数据和训练流程开源。


<details>
  <summary>Details</summary>
Motivation: 为医疗辐射场的辐射防护剂量测定，估计散射辐射场的空间分布。

Method: 使用基于Geant4的蒙特卡罗模拟应用生成三个不同复杂度的合成数据集进行训练，评估卷积和全连接神经网络架构。

Result: 展示了哪些设计决策在重建辐射场空间域的注量和光谱分布方面效果良好。

Conclusion: 未明确提及，但通过评估得出有效架构设计，且数据和训练流程开源利于后续研究。

Abstract: We present an in-depth analysis on how to build and train neural networks to estimate the spatial distribution of scattered radiation fields for radiation protection dosimetry in medical radiation fields, such as those found in Interventional Radiology and Cardiology. Therefore, we present three different synthetically generated datasets with increasing complexity for training, using a Monte-Carlo Simulation application based on Geant4. On those datasets, we evaluate convolutional and fully connected architectures of neural networks to demonstrate which design decisions work well for reconstructing the fluence and spectra distributions over the spatial domain of such radiation fields. All used datasets as well as our training pipeline are published as open source in separate repositories.

</details>


### [115] [Polyharmonic Cascade](https://arxiv.org/abs/2512.17671)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 本文提出多调和级联深度学习架构，给出替代梯度下降的训练方法，在MNIST上实现快速无过拟合学习。


<details>
  <summary>Details</summary>
Motivation: 提出一种能近似任意复杂度非线性函数，同时保持全局平滑性和概率解释的深度学习架构。

Method: 提出多调和级联架构，采用求解全局线性系统替代梯度下降的训练方法。

Result: 实现各层同步更新，保留各层概率解释和理论一致性，计算可高效在GPU上以二维矩阵运算完成，在MNIST上快速学习且无过拟合。

Conclusion: 多调和级联架构及其训练方法有效可行。

Abstract: This paper presents a deep machine learning architecture, the "polyharmonic cascade" -- a sequence of packages of polyharmonic splines, where each layer is rigorously derived from the theory of random functions and the principles of indifference. This makes it possible to approximate nonlinear functions of arbitrary complexity while preserving global smoothness and a probabilistic interpretation. For the polyharmonic cascade, a training method alternative to gradient descent is proposed: instead of directly optimizing the coefficients, one solves a single global linear system on each batch with respect to the function values at fixed "constellations" of nodes. This yields synchronized updates of all layers, preserves the probabilistic interpretation of individual layers and theoretical consistency with the original model, and scales well: all computations reduce to 2D matrix operations efficiently executed on a GPU. Fast learning without overfitting on MNIST is demonstrated.

</details>


### [116] [You Only Train Once: Differentiable Subset Selection for Omics Data](https://arxiv.org/abs/2512.17678)
*Daphné Chopard,Jorge da Silva Gonçalves,Irene Cannistraci,Thomas M. Sutter,Julia E. Vogt*

Main category: cs.LG

TL;DR: 介绍端到端框架YOTO用于单细胞转录组数据的基因子集选择，在两个数据集上表现优于现有基线，提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法中选择和预测弱耦合，需要更有效的基因子集选择方法用于生物标志物发现等。

Method: 提出YOTO端到端框架，在单个可微架构中联合识别离散基因子集并进行预测，采用多任务学习设计。

Result: 在两个代表性单细胞RNA测序数据集上，YOTO持续优于现有先进基线。

Conclusion: 稀疏、端到端、多任务的基因子集选择可提高预测性能，产生紧凑且有意义的基因子集，推动生物标志物发现和单细胞分析。

Abstract: Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.

</details>


### [117] [Can You Hear Me Now? A Benchmark for Long-Range Graph Propagation](https://arxiv.org/abs/2512.17762)
*Luca Miglior,Matteo Tolloso,Alessio Gravina,Davide Bacciu*

Main category: cs.LG

TL;DR: 提出ECHO基准评估GNN处理长距离图传播能力，含合成任务与真实数据集，benchmark显示流行架构有差距，设新评估标准。


<details>
  <summary>Details</summary>
Motivation: 有效捕捉长距离交互是GNN研究中未解决的挑战，需系统评估GNN处理长距离传播的能力。

Method: 引入ECHO基准，包含三个合成图任务和两个真实数据集。

Result: 广泛基准测试揭示流行GNN架构有性能差距。

Conclusion: ECHO为评估长距离信息传播设定新标准，证明AI for science对此有需求。

Abstract: Effectively capturing long-range interactions remains a fundamental yet unresolved challenge in graph neural network (GNN) research, critical for applications across diverse fields of science. To systematically address this, we introduce ECHO (Evaluating Communication over long HOps), a novel benchmark specifically designed to rigorously assess the capabilities of GNNs in handling very long-range graph propagation. ECHO includes three synthetic graph tasks, namely single-source shortest paths, node eccentricity, and graph diameter, each constructed over diverse and structurally challenging topologies intentionally designed to introduce significant information bottlenecks. ECHO also includes two real-world datasets, ECHO-Charge and ECHO-Energy, which define chemically grounded benchmarks for predicting atomic partial charges and molecular total energies, respectively, with reference computations obtained at the density functional theory (DFT) level. Both tasks inherently depend on capturing complex long-range molecular interactions. Our extensive benchmarking of popular GNN architectures reveals clear performance gaps, emphasizing the difficulty of true long-range propagation and highlighting design choices capable of overcoming inherent limitations. ECHO thereby sets a new standard for evaluating long-range information propagation, also providing a compelling example for its need in AI for science.

</details>


### [118] [Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments](https://arxiv.org/abs/2512.17771)
*Dong Chen,Zhengqing Hu,Shixing Zhao,Yibo Guo*

Main category: cs.LG

TL;DR: 大模型参数规模大但适应性有限，现有PEFT方法有高资源成本和参数依赖问题，提出Easy Adaptation方法，不访问大模型参数且资源需求少，性能与PEFT相当。


<details>
  <summary>Details</summary>
Motivation: 解决现有PEFT方法存在的高资源成本和参数依赖问题，利用小模型在特定分布上的优势，提升大模型在不同任务中的适应性。

Method: 提出Easy Adaptation (EA)方法，设计Specific Small Models (SSMs)来补充大模型拟合不足的数据分布。

Result: EA在不同任务上不访问大模型参数的情况下，性能与PEFT相当，且仅需极少资源。

Conclusion: EA方法是一种有效的解决方案，能在不访问大模型参数的情况下，以极少资源达到与PEFT相当的性能。

Abstract: While the enormous parameter scale endows Large Models (LMs) with unparalleled performance, it also limits their adaptability across specific tasks. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical approach for effectively adapting LMs to a diverse range of downstream tasks. However, existing PEFT methods face two primary challenges: (1) High resource cost. Although PEFT methods significantly reduce resource demands compared to full fine-tuning, it still requires substantial time and memory, making it impractical in resource-constrained environments. (2) Parameter dependency. PEFT methods heavily rely on updating a subset of parameters associated with LMs to incorporate task-specific knowledge. Yet, due to increasing competition in the LMs landscape, many companies have adopted closed-source policies for their leading models, offering access only via Application Programming Interface (APIs). Whereas, the expense is often cost-prohibitive and difficult to sustain, as the fine-tuning process of LMs is extremely slow. Even if small models perform far worse than LMs in general, they can achieve superior results on particular distributions while requiring only minimal resources. Motivated by this insight, we propose Easy Adaptation (EA), which designs Specific Small Models (SSMs) to complement the underfitted data distribution for LMs. Extensive experiments show that EA matches the performance of PEFT on diverse tasks without accessing LM parameters, and requires only minimal resources.

</details>


### [119] [Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning](https://arxiv.org/abs/2512.17788)
*Wei Tang,Yin-Fang Yang,Weijia Zhang,Min-Ling Zhang*

Main category: cs.LG

TL;DR: 提出可校准消歧损失（CDL）解决多实例部分标签学习校准问题，理论与实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有多实例部分标签学习（MIPL）方法校准性差，影响分类器可靠性。

Method: 提出可插拔的校准消歧损失（CDL），有两种实例化方式，并可无缝融入现有MIPL和PLL框架，还进行了理论分析。

Result: 在基准和真实数据集实验中，CDL显著提升分类和校准性能。

Conclusion: CDL优于传统消歧损失，能提升多实例部分标签学习的分类和校准性能。

Abstract: Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces. However, existing MIPL approaches often suffer from poor calibration, undermining classifier reliability. In this work, we propose a plug-and-play calibratable disambiguation loss (CDL) that simultaneously improves classification accuracy and calibration performance. The loss has two instantiations: the first one calibrates predictions based on probabilities from the candidate label set, while the second one integrates probabilities from both candidate and non-candidate label sets. The proposed CDL can be seamlessly incorporated into existing MIPL and PLL frameworks. We provide a theoretical analysis that establishes the lower bound and regularization properties of CDL, demonstrating its superiority over conventional disambiguation losses. Experimental results on benchmark and real-world datasets confirm that our CDL significantly enhances both classification and calibration performance.

</details>


### [120] [Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation](https://arxiv.org/abs/2512.17820)
*Liam Collins,Bhuvesh Kumar,Clark Mingxuan Ju,Tong Zhao,Donald Loveland,Leonardo Neves,Neil Shah*

Main category: cs.LG

TL;DR: 研究ID和基于文本的序列推荐模型互补性，提出简单集成策略方法，性能超多个基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏对ID和模态特征互补性的理解，需研究其互补性以改进序列推荐模型。

Method: 通过独立模型训练保留ID - 文本互补性，再用简单集成策略利用互补性。

Result: 所提方法性能超过多个有竞争力的序列推荐基线模型。

Conclusion: ID和文本特征对实现最优序列推荐性能都必要，复杂融合架构并非必需。

Abstract: Modern Sequential Recommendation (SR) models commonly utilize modality features to represent items, motivated in large part by recent advancements in language and vision modeling. To do so, several works completely replace ID embeddings with modality embeddings, claiming that modality embeddings render ID embeddings unnecessary because they can match or even exceed ID embedding performance. On the other hand, many works jointly utilize ID and modality features, but posit that complex fusion strategies, such as multi-stage training and/or intricate alignment architectures, are necessary for this joint utilization. However, underlying both these lines of work is a lack of understanding of the complementarity of ID and modality features. In this work, we address this gap by studying the complementarity of ID- and text-based SR models. We show that these models do learn complementary signals, meaning that either should provide performance gain when used properly alongside the other. Motivated by this, we propose a new SR method that preserves ID-text complementarity through independent model training, then harnesses it through a simple ensembling strategy. Despite this method's simplicity, we show it outperforms several competitive SR baselines, implying that both ID and text features are necessary to achieve state-of-the-art SR performance but complex fusion architectures are not.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [121] [Optimisation of Aircraft Maintenance Schedules](https://arxiv.org/abs/2512.17412)
*Neil Urquhart,Amir Rahimi,Efstathios-Al. Tingas*

Main category: cs.NE

TL;DR: 本文提出飞机维护调度问题，采用进化算法研究，并在60个生成的问题实例上进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决飞机维护调度问题，需分配合适人员在规定窗口期内完成维护任务，让飞机恢复创收服务。

Method: 运用进化算法，通过评估众多可能解聚焦于高质量解。

Result: 在60个生成的问题实例上进行基准测试，展示了底层表示和相关遗传算子。

Conclusion: 原文未提及明确结论，但说明是初始研究。

Abstract: We present an aircraft maintenance scheduling problem, which requires suitably qualified staff to be assigned to maintenance tasks on each aircraft. The tasks on each aircraft must be completed within a given turn around window so that the aircraft may resume revenue earning service. This paper presents an initial study based on the application of an Evolutionary Algorithm to the problem. Evolutionary Algorithms evolve a solution to a problem by evaluating many possible solutions, focusing the search on those solutions that are of a higher quality, as defined by a fitness function. In this paper, we benchmark the algorithm on 60 generated problem instances to demonstrate the underlying representation and associated genetic operators.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [122] [SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization](https://arxiv.org/abs/2512.16956)
*Shravan Chaudhari,Rahul Thomas Jacob,Mononito Goswami,Jiajun Cao,Shihab Rashid,Christian Bock*

Main category: cs.SE

TL;DR: 提出SpIDER改进代码检索性能的方法，并在多语言中验证有效


<details>
  <summary>Details</summary>
Motivation: 现有嵌入代码检索方法缺乏对代码库探索，未充分利用基础图结构

Method: 提出SpIDER，在基于图的代码库探索获取的辅助上下文上结合LLM推理

Result: SpIDER在多种编程语言中持续改进了密集检索性能

Conclusion: SpIDER这种增强的密集检索方法有效可行

Abstract: Retrieving code units (e.g., files, classes, functions) that are semantically relevant to a given user query, bug report, or feature request from large codebases is a fundamental challenge for LLM-based coding agents. Agentic approaches typically employ sparse retrieval methods like BM25 or dense embedding strategies to identify relevant units. While embedding-based approaches can outperform BM25 by large margins, they often lack exploration of the codebase and underutilize its underlying graph structure. To address this, we propose SpIDER (Spatially Informed Dense Embedding Retrieval), an enhanced dense retrieval approach that incorporates LLM-based reasoning over auxiliary context obtained through graph-based exploration of the codebase. Empirical results show that SpIDER consistently improves dense retrieval performance across several programming languages.

</details>


### [123] [Resilient Microservices: A Systematic Review of Recovery Patterns, Strategies, and Evaluation Frameworks](https://arxiv.org/abs/2512.16959)
*Muzeeb Mohammad*

Main category: cs.SE

TL;DR: 本文对2014 - 2025年的微服务恢复策略实证研究进行系统综述，筛选26篇高质量研究，确定九个恢复主题，提出恢复模式分类法等工具，为容错微服务系统设计提供结构化证据。


<details>
  <summary>Details</summary>
Motivation: 微服务系统易受部分故障等影响，现有综述缺乏系统性综合和定量严谨性。

Method: 采用PRISMA方法，从IEEE Xplore、ACM Digital Library和Scopus数据库收集412篇记录，通过明确标准筛选出26篇高质量研究。

Result: 确定九个恢复主题，提出恢复模式分类法、弹性评估分数检查表和约束决策矩阵。

Conclusion: 研究将分散的恢复研究整合，为容错和性能感知的微服务系统的可重复评估和设计提供结构化证据。

Abstract: Microservice based systems underpin modern distributed computing environments but remain vulnerable to partial failures, cascading timeouts, and inconsistent recovery behavior. Although numerous resilience and recovery patterns have been proposed, existing surveys are largely descriptive and lack systematic evidence synthesis or quantitative rigor. This paper presents a PRISMA aligned systematic literature review of empirical studies on microservice recovery strategies published between 2014 and 2025 across IEEE Xplore, ACM Digital Library, and Scopus. From an initial corpus of 412 records, 26 high quality studies were selected using transparent inclusion, exclusion, and quality assessment criteria. The review identifies nine recurring resilience themes encompassing circuit breakers, retries with jitter and budgets, sagas with compensation, idempotency, bulkheads, adaptive backpressure, observability, and chaos validation. As a data oriented contribution, the paper introduces a Recovery Pattern Taxonomy, a Resilience Evaluation Score checklist for standardized benchmarking, and a constraint aware decision matrix mapping latency, consistency, and cost trade offs to appropriate recovery mechanisms. The results consolidate fragmented resilience research into a structured and analyzable evidence base that supports reproducible evaluation and informed design of fault tolerant and performance aware microservice systems.

</details>


### [124] [Sensor Management System (SMS): Open-source software for FAIR sensor metadata management in Earth system sciences](https://arxiv.org/abs/2512.17280)
*Christof Lorenza,Nils Brinckmann,Jan Bumberger,Marc Hanisch,Tobias Kuhnert,Ulrich Loup,Rubankumar Moorthy,Florian Obsersteiner,David Schäfer,Thomas Schnicke*

Main category: cs.SE

TL;DR: 开发传感器管理系统（SMS）以管理传感器相关元数据，促进更一致、可持续和FAIR的元数据提供。


<details>
  <summary>Details</summary>
Motivation: 从环境观测数据得出可靠结论和深入见解，迫切需要用一致且全面的元数据来丰富，需管理传感器全生命周期信息。

Method: 开发SMS，用明确定义的术语描述各实体，将其与后续系统和服务连接，并建立终端用户社区。

Result: SMS为建模复杂传感器系统和管理传感器相关信息提供平台，成为数字生态系统的核心元素。

Conclusion: SMS能促进更一致、可持续和FAIR的传感器相关元数据提供。

Abstract: Deriving reliable conclusions and insights from environmental observational data urgently requires the enrichment with consistent and comprehensive metadata, including time-resolved context such as changing deployments, configurations, and maintenance actions. We have therefore developed the Sensor Management System (SMS), which provides a user-friendly and feature-rich platform for modeling even the most complex sensor systems and managing all sensor-related information across their life cycle. Each entity is described via well-defined terms like Devices, Platforms and Configurations, as well as Sites that are further enhanced with attributes for, e.g., instrument manufacturers, contact information or measured quantities and complemented by a continuous history of system-related actions. By further linking the SMS to sub-sequent systems and services like PID-registration or controlled vocabularies and establishing a community of end-users, the SMS provides the central element of a digital ecosystem, that fosters a more consistent, sustainable and FAIR provision of sensor-related metadata.

</details>


### [125] [Bridging Natural Language and Formal Specification--Automated Translation of Software Requirements to LTL via Hierarchical Semantics Decomposition Using LLMs](https://arxiv.org/abs/2512.17334)
*Zhi Ma,Cheng Wen,Zhexin Su,Xiao Liang,Cong Tian,Shengchao Qin,Mengfei Yang*

Main category: cs.SE

TL;DR: 提出Req2LTL框架将自然语言软件需求转换为线性时序逻辑（LTL），表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有将自然语言软件需求自动转换为形式化规范的方法存在局限，大语言模型处理工业需求仍有困难。

Method: 提出Req2LTL模块化框架，通过OnionL这一层次化中间表示连接自然语言和LTL，利用大语言模型进行语义分解，并结合确定性基于规则的合成。

Result: Req2LTL在实际航天需求上实现88.4%的语义准确性和100%的语法正确性。

Conclusion: Req2LTL显著优于现有方法。

Abstract: Automating the translation of natural language (NL) software requirements into formal specifications remains a critical challenge in scaling formal verification practices to industrial settings, particularly in safety-critical domains. Existing approaches, both rule-based and learning-based, face significant limitations. While large language models (LLMs) like GPT-4o demonstrate proficiency in semantic extraction, they still encounter difficulties in addressing the complexity, ambiguity, and logical depth of real-world industrial requirements. In this paper, we propose Req2LTL, a modular framework that bridges NL and Linear Temporal Logic (LTL) through a hierarchical intermediate representation called OnionL. Req2LTL leverages LLMs for semantic decomposition and combines them with deterministic rule-based synthesis to ensure both syntactic validity and semantic fidelity. Our comprehensive evaluation demonstrates that Req2LTL achieves 88.4% semantic accuracy and 100% syntactic correctness on real-world aerospace requirements, significantly outperforming existing methods.

</details>


### [126] [What You Trust Is Insecure: Demystifying How Developers (Mis)Use Trusted Execution Environments in Practice](https://arxiv.org/abs/2512.17363)
*Yuqing Niu,Jieke Shi,Ruidong Han,Ye Liu,Chengyan Ma,Yunbo Lyu,David Lo*

Main category: cs.SE

TL;DR: 本文首次大规模实证研究现实中TEE应用，分析241个开源项目，揭示采用情况、使用模式和开发实践，为提升SDK可用性和开发安全软件提供依据。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏开发者实际使用TEE情况的了解，需开展实证研究。

Method: 从GitHub收集241个使用Intel SGX和ARM TrustZone的开源项目，结合人工检查和定制静态分析脚本分三个阶段分析。

Result: 项目主要应用于IoT设备安全；32.4%项目重新实现加密功能；25.3%项目有不安全编码行为。

Conclusion: 研究结果对提升TEE SDK可用性和支持可信软件开发有重要意义。

Abstract: Trusted Execution Environments (TEEs), such as Intel SGX and ARM TrustZone, provide isolated regions of CPU and memory for secure computation and are increasingly used to protect sensitive data and code across diverse application domains. However, little is known about how developers actually use TEEs in practice. This paper presents the first large-scale empirical study of real-world TEE applications. We collected and analyzed 241 open-source projects from GitHub that utilize the two most widely-adopted TEEs, Intel SGX and ARM TrustZone. By combining manual inspection with customized static analysis scripts, we examined their adoption contexts, usage patterns, and development practices across three phases. First, we categorized the projects into 8 application domains and identified trends in TEE adoption over time. We found that the dominant use case is IoT device security (30%), which contrasts sharply with prior academic focus on blockchain and cryptographic systems (7%), while AI model protection (12%) is rapidly emerging as a growing domain. Second, we analyzed how TEEs are integrated into software and observed that 32.4% of the projects reimplement cryptographic functionalities instead of using official SDK APIs, suggesting that current SDKs may have limited usability and portability to meet developers' practical needs. Third, we examined security practices through manual inspection and found that 25.3% (61 of 241) of the projects exhibit insecure coding behaviors when using TEEs, such as hardcoded secrets and missing input validation, which undermine their intended security guarantees. Our findings have important implications for improving the usability of TEE SDKs and supporting developers in trusted software development.

</details>


### [127] [GraphCue for SDN Configuration Code Synthesis](https://arxiv.org/abs/2512.17371)
*Haomin Qi,Fengfei Yu,Chengbo Huang*

Main category: cs.SE

TL;DR: 提出GraphCue框架用于自动化SDN配置，在验证案例中表现良好，拓扑感知检索和基于约束的调节是性能关键。


<details>
  <summary>Details</summary>
Motivation: 实现自动化SDN配置。

Method: 将每个案例抽象为JSON图，用轻量级三层GCN结合对比学习嵌入，把最近的验证参考注入结构化提示约束代码生成，验证器执行候选配置并反馈失败信息。

Result: 在628个验证案例中，20次迭代内通过率达88.2%，95%的验证循环在9秒内完成。

Conclusion: 拓扑感知检索和基于约束的调节是性能的关键驱动因素。

Abstract: We present GraphCue, a topology-grounded retrieval and agent-in-the-loop framework for automated SDN configuration. Each case is abstracted into a JSON graph and embedded using a lightweight three-layer GCN trained with contrastive learning. The nearest validated reference is injected into a structured prompt that constrains code generation, while a verifier closes the loop by executing the candidate configuration and feeding failures back to the agent. On 628 validation cases, GraphCue achieves an 88.2 percent pass rate within 20 iterations and completes 95 percent of verification loops within 9 seconds. Ablation studies without retrieval or structured prompting perform substantially worse, indicating that topology-aware retrieval and constraint-based conditioning are key drivers of performance.

</details>


### [128] [Analysis of Design Patterns and Benchmark Practices in Apache Kafka Event-Streaming Systems](https://arxiv.org/abs/2512.16146)
*Muzeeb Mohammad*

Main category: cs.SE

TL;DR: 本文对2015 - 2025年42篇同行评审研究进行结构化综合，识别出9种Kafka设计模式，分析使用趋势和基准测试实践，指出研究存在的问题并提供实用指南。


<details>
  <summary>Details</summary>
Motivation: Kafka虽成熟且广泛应用，但可复用架构设计模式和可复现基准测试方法的研究分散，需整合。

Method: 对42篇同行评审研究进行结构化综合，使用标准套件和自定义工作负载分析使用趋势和基准测试实践。

Result: 识别出9种Kafka设计模式，发现配置披露、评估严谨性和可复现性存在显著不一致，限制跨研究比较和实际复制。

Conclusion: 提供统一分类法、模式基准矩阵和可行决策启发式方法，为架构师和研究人员设计基于Kafka的事件流系统提供实用指导。

Abstract: Apache Kafka has become a foundational platform for high throughput event streaming, enabling real time analytics, financial transaction processing, industrial telemetry, and large scale data driven systems. Despite its maturity and widespread adoption, consolidated research on reusable architectural design patterns and reproducible benchmarking methodologies remains fragmented across academic and industrial publications. This paper presents a structured synthesis of forty two peer reviewed studies published between 2015 and 2025, identifying nine recurring Kafka design patterns including log compaction, CQRS bus, exactly once pipelines, change data capture, stream table joins, saga orchestration, tiered storage, multi tenant topics, and event sourcing replay. The analysis examines co usage trends, domain specific deployments, and empirical benchmarking practices using standard suites such as TPCx Kafka and the Yahoo Streaming Benchmark, as well as custom workloads. The study highlights significant inconsistencies in configuration disclosure, evaluation rigor, and reproducibility that limit cross study comparison and practical replication. By providing a unified taxonomy, pattern benchmark matrix, and actionable decision heuristics, this work offers practical guidance for architects and researchers designing reproducible, high performance, and fault tolerant Kafka based event streaming systems.

</details>


### [129] [CIFE: Code Instruction-Following Evaluation](https://arxiv.org/abs/2512.17387)
*Sravani Gunnu,Shanmukha Guttula,Hima Patel*

Main category: cs.SE

TL;DR: 引入含1000个Python任务及开发者指定约束的基准测试，评估14个模型，提出C2A分数，发现部分与严格满足约束有差距，可靠代码生成需兼顾正确性与约束遵循。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试对大语言模型代码生成中遵循开发者约束的评估有限，需新基准测试评估模型遵循约束的能力。

Method: 引入含1000个Python任务及平均7个开发者指定约束的基准测试，约束通过四阶段人工 - LLM流程筛选，用互补的遵循指标评估14个模型，提出C2A分数。

Result: 部分满足和严格满足约束存在显著差距，强模型部分遵循率超90%，严格遵循率在39 - 66%。

Conclusion: 可靠的代码生成不仅需要功能正确，还需始终遵循开发者意图。

Abstract: Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 developer-specified constraints spanning 13 categories. Constraints are curated through a four-stage human-LLM pipeline to ensure they are atomic, relevant, and objective. We evaluate 14 open- and closed-source models using complementary adherence metrics and propose the C2A Score, a composite measure that jointly captures correctness and constraint compliance. Results reveal a substantial gap between partial and strict satisfaction, while strong models achieve over 90% partial adherence, strict adherence remains between 39-66%. These findings highlight that trustworthy code generation requires not only correctness but also consistent adherence to developer intent.

</details>


### [130] [SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories](https://arxiv.org/abs/2512.17419)
*Lilin Wang,Lucas Ramalho,Alan Celestino,Phuc Anthony Pham,Yu Liu,Umang Kumar Sinha,Andres Portillo,Onassis Osunwa,Gabriel Maduekwe*

Main category: cs.SE

TL;DR: 介绍自动化框架SWE-Bench++，从GitHub生成代码任务，含多语言，评估模型并证明调优效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs软件工程任务评估受限，如手动整理、数据集静态、侧重Python。

Method: 从开源项目采集拉取请求，经四阶段转化为可执行任务，再用提示引导生成训练轨迹。

Result: 初始基准含11133个实例，评估了多个模型并给出准确率，微调后SWE - bench多语言基准有提升。

Conclusion: SWE - Bench++提供可扩展、多语言基准用于评估和改进代码生成。

Abstract: Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.

</details>


### [131] [An Investigation on How AI-Generated Responses Affect SoftwareEngineering Surveys](https://arxiv.org/abs/2512.17455)
*Ronnie de Souza Santos,Italo Santos,Maria Teresa Baldassarre,Cleyton Magalhaes,Mairieli Wessel*

Main category: cs.SE

TL;DR: 研究探讨软件工程调查中LLMs被滥用的情况及对数据的影响，提出保护调查可信度的措施


<details>
  <summary>Details</summary>
Motivation: 近期大语言模型给软件工程调查完整性带来新风险，需探究其滥用情况及对数据真实性、有效性和研究完整性的影响

Method: 从2025年两次通过Prolific平台开展的调查收集数据，分析参与者答案内容，用定性模式检查、叙事特征分析和自动化检测识别AI生成的可疑回复

Result: 发现49份调查回复存在表明是合成创作的重复结构模式，这些虚假叙述破坏了结构、内部和外部有效性

Conclusion: 数据真实性是软件工程调查有效性的新维度，需结合多种验证程序、透明报告和社区标准来检测和防止AI生成回复，保护调查可信度

Abstract: Survey research is a fundamental empirical method in software engineering, enabling the systematic collection of data on professional practices, perceptions, and experiences. However, recent advances in large language models (LLMs) have introduced new risks to survey integrity, as participants can use generative tools to fabricate or manipulate their responses. This study explores how LLMs are being misused in software engineering surveys and investigates the methodological implications of such behavior for data authenticity, validity, and research integrity. We collected data from two survey deployments conducted in 2025 through the Prolific platform and analyzed the content of participants' answers to identify irregular or falsified responses. A subset of responses suspected of being AI generated was examined through qualitative pattern inspection, narrative characterization, and automated detection using the Scribbr AI Detector. The analysis revealed recurring structural patterns in 49 survey responses indicating synthetic authorship, including repetitive sequencing, uniform phrasing, and superficial personalization. These false narratives mimicked coherent reasoning while concealing fabricated content, undermining construct, internal, and external validity. Our study identifies data authenticity as an emerging dimension of validity in software engineering surveys. We emphasize that reliable evidence now requires combining automated and interpretive verification procedures, transparent reporting, and community standards to detect and prevent AI generated responses, thereby protecting the credibility of surveys in software engineering.

</details>


### [132] [When Data Quality Issues Collide: A Large-Scale Empirical Study of Co-Occurring Data Quality Issues in Software Defect Prediction](https://arxiv.org/abs/2512.17460)
*Emmanuel Charleson Dapaah,Jens Grabowski*

Main category: cs.SE

TL;DR: 本文对软件缺陷预测（SDP）中五个同时出现的数据质量问题进行大规模实证分析，揭示了问题的共现情况、影响阈值及反直觉模式，指出性能 - 鲁棒性权衡。


<details>
  <summary>Details</summary>
Motivation: 以往研究孤立处理数据问题，而现实中数据问题常共现且相互作用，需要全面分析。

Method: 对374个数据集和5个分类器，使用可解释提升机和分层交互分析，在默认超参数设置下量化直接和条件效应。

Result: 问题共现普遍，无关特征和不平衡问题常见，类重叠危害最大，确定了各问题影响模型性能的阈值，发现反直觉模式和性能 - 鲁棒性权衡。

Conclusion: 联合分析填补了SDP研究空白，提供了对现实场景中数据质量问题如何影响模型性能的整体、数据感知理解。

Abstract: Software Defect Prediction (SDP) models are central to proactive software quality assurance, yet their effectiveness is often constrained by the quality of available datasets. Prior research has typically examined single issues such as class imbalance or feature irrelevance in isolation, overlooking that real-world data problems frequently co-occur and interact. This study presents, to our knowledge, the first large-scale empirical analysis in SDP that simultaneously examines five co-occurring data quality issues (class imbalance, class overlap, irrelevant features, attribute noise, and outliers) across 374 datasets and five classifiers. We employ Explainable Boosting Machines together with stratified interaction analysis to quantify both direct and conditional effects under default hyperparameter settings, reflecting practical baseline usage.
  Our results show that co-occurrence is nearly universal: even the least frequent issue (attribute noise) appears alongside others in more than 93% of datasets. Irrelevant features and imbalance are nearly ubiquitous, while class overlap is the most consistently harmful issue. We identify stable tipping points around 0.20 for class overlap, 0.65-0.70 for imbalance, and 0.94 for irrelevance, beyond which most models begin to degrade. We also uncover counterintuitive patterns, such as outliers improving performance when irrelevant features are low, underscoring the importance of context-aware evaluation. Finally, we expose a performance-robustness trade-off: no single learner dominates under all conditions.
  By jointly analyzing prevalence, co-occurrence, thresholds, and conditional effects, our study directly addresses a persistent gap in SDP research. Hence, moving beyond isolated analyses to provide a holistic, data-aware understanding of how quality issues shape model performance in real-world settings.

</details>


### [133] [Why Is My Transaction Risky? Understanding Smart Contract Semantics and Interactions in the NFT Ecosystem](https://arxiv.org/abs/2512.17500)
*Yujing Chen,Xuanming Liu,Zhiyuan Wan,Zuobin Wang,David Lo,Difan Xie,Xiaohu Yang*

Main category: cs.SE

TL;DR: 本文对NFT生态系统中智能合约语义和交互进行大规模实证研究，发现合约语义多样性有限，不同合约有不同特点，还指出风险和非风险交易中的交互模式，最后给出风险缓解建议和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 此前研究存在对NFT交易中智能合约语义和交互，以及欺诈代币风险如何体现的理解空白，需要弥补该空白。

Method: 使用以太坊上近1亿笔跨越2000万个区块的交易数据集进行大规模实证研究。

Result: NFT生态中智能合约语义多样性有限，市场和代理注册表合约在交易交互中最常涉及，代币合约有字节码多样性而欺诈代币有字节码收敛性，风险和非风险交易存在不同交互模式。

Conclusion: 基于研究结果，提供在区块链生态中缓解风险的建议并规划未来研究方向。

Abstract: The NFT ecosystem represents an interconnected, decentralized environment that encompasses the creation, distribution, and trading of Non-Fungible Tokens (NFTs), where key actors, such as marketplaces, sellers, and buyers, utilize smart contracts to facilitate secure, transparent, and trustless transactions. Scam tokens are deliberately created to mislead users and facilitate financial exploitation, posing significant risks in the NFT ecosystem. Prior work has explored the NFT ecosystem from various perspectives, including security challenges, actor behaviors, and risks from scams and wash trading, leaving a gap in understanding the semantics and interactions of smart contracts during transactions, and how the risks associated with scam tokens manifest in relation to the semantics and interactions of contracts. To bridge this gap, we conducted a large-scale empirical study on smart contract semantics and interactions in the NFT ecosystem, using a curated dataset of nearly 100 million transactions across 20 million blocks on Ethereum. We observe a limited semantic diversity among smart contracts in the NFT ecosystem, dominated by proxy, token, and DeFi contracts. Marketplace and proxy registry contracts are the most frequently involved in smart contract interactions during transactions, engaging with a broad spectrum of contracts in the ecosystem. Token contracts exhibit bytecode-level diversity, whereas scam tokens exhibit bytecode convergence. Certain interaction patterns between smart contracts are common to both risky and non-risky transactions, while others are predominantly associated with risky transactions. Based on our findings, we provide recommendations to mitigate risks in the blockchain ecosystem, and outline future research directions.

</details>


### [134] [SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review](https://arxiv.org/abs/2512.17540)
*Kai Wang,Bingcheng Mao,Shuai Jia,Yujie Ding,Dongming Han,Tianyi Ma,Bin Cao*

Main category: cs.SE

TL;DR: 提出 SGCR 框架解决大语言模型自动代码审查问题，在工业环境中取得良好效果


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于代码审查缺乏可靠性、上下文感知和可控性，阻碍实际应用

Method: 提出 SGCR 框架，采用双路径架构，显式路径确保符合预定义规则，隐式路径启发式发现和验证规则外问题

Result: SGCR 在工业环境中建议开发者采纳率达 42%，相对基线大语言模型提升 90.9%

Conclusion: 规范基础是弥合大语言模型生成能力与软件工程可靠性需求差距的有力范式

Abstract: Automating code review with Large Language Models (LLMs) shows immense promise, yet practical adoption is hampered by their lack of reliability, context-awareness, and control. To address this, we propose Specification-Grounded Code Review (SGCR), a framework that grounds LLMs in human-authored specifications to produce trustworthy and relevant feedback. SGCR features a novel dual-pathway architecture: an explicit path ensures deterministic compliance with predefined rules derived from these specifications, while an implicit path heuristically discovers and verifies issues beyond those rules. Deployed in a live industrial environment at HiThink Research, SGCR's suggestions achieved a 42% developer adoption rate-a 90.9% relative improvement over a baseline LLM (22%). Our work demonstrates that specification-grounding is a powerful paradigm for bridging the gap between the generative power of LLMs and the rigorous reliability demands of software engineering.

</details>


### [135] [A Practical Solution to Systematically Monitor Inconsistencies in SBOM-based Vulnerability Scanners](https://arxiv.org/abs/2512.17710)
*Martin Rosso,Muhammad Asad Jahangir Jaffar,Alessandro Brighente,Mauro Conti*

Main category: cs.SE

TL;DR: 本文揭示SVS工具可靠性和错误处理差异，提出SVS - TEST方法工具


<details>
  <summary>Details</summary>
Motivation: 业界在使用SVS识别漏洞时出现不一致和意外行为，产生假阴性和静默失败

Method: 提出SVS - TEST方法和工具，用16个精心制作的SBOM和真实情况评估7个SVS工具

Result: SVS工具在可靠性和错误处理上有显著差异，部分工具在有效输入SBOM时静默失败

Conclusion: 强调该研究对研究人员和从业者的意义，各方可利用SVS - TEST监测SVS能力和成熟度

Abstract: Software Bill of Materials (SBOM) provides new opportunities for automated vulnerability identification in software products. While the industry is adopting SBOM-based Vulnerability Scanning (SVS) to identify vulnerabilities, we increasingly observe inconsistencies and unexpected behavior, that result in false negatives and silent failures. In this work, we present the background necessary to understand the underlying complexity of SVS and introduce SVS-TEST, a method and tool to analyze the capability, maturity, and failure conditions of SVS-tools in real-world scenarios. We showcase the utility of SVS-TEST in a case study evaluating seven real-world SVS-tools using 16 precisely crafted SBOMs and their respective ground truth. Our results unveil significant differences in the reliability and error handling of SVS-tools; multiple SVS-tools silently fail on valid input SBOMs, creating a false sense of security. We conclude our work by highlighting implications for researchers and practitioners, including how organizations and developers of SVS-tools can utilize SVS-TEST to monitor SVS capability and maturity. All results and research artifacts are made publicly available and all findings were disclosed to the SVS-tool developers ahead of time.

</details>


### [136] [LLM-based Behaviour Driven Development for Hardware Design](https://arxiv.org/abs/2512.17814)
*Rolf Drechsler,Qian Liu*

Main category: cs.SE

TL;DR: 传统测试验证复杂，BDD 在硬件设计中应用受限，本文探索用基于大语言模型技术支持硬件设计中的 BDD。


<details>
  <summary>Details</summary>
Motivation: 硬件和系统设计中测试验证复杂，BDD 在硬件设计未广泛应用，大语言模型发展带来自动化 deriving precise behavioral scenarios 机会。

Method: 研究使用基于大语言模型的技术。

Result: 摘要未提及。

Conclusion: 摘要未提及。

Abstract: Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.
  Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [137] [Visualization of The Content of Surah al Fiil using Marker-Based Augmented Reality](https://arxiv.org/abs/2512.17895)
*Ahmad Badru Al Husaeni,Dzakwanfaiq Nauval,Farid Muhtar Fathir,Mahesa Adlan Falah,Muhammad Miftahur Rizki Awalin*

Main category: q-fin.CP

TL;DR: 本文开发基于标记的增强现实（AR）应用用于伊斯兰教育，经多阶段开发，功能测试性能好，用户评价高，表明该技术能支持数字伊斯兰教育创新。


<details>
  <summary>Details</summary>
Motivation: 开发一种交互式、富含上下文的媒介用于伊斯兰教育，以增强学习者参与度和对《古兰经》叙事的理解。

Method: 采用研发方法，经数据收集、用户需求分析、界面设计、用Blender创建3D资产、将Unity 3D与Vuforia SDK集成等阶段开发系统。

Result: 功能测试技术性能强，标记检测准确率达95%，多安卓设备实时渲染一致；用户评价满意度高，得分为4.7分（满分5分）。

Conclusion: 基于标记的AR技术有潜力通过提供交互式和视觉直观体验，支持数字伊斯兰教育创新。

Abstract: This study presents the development of a marker-based augmented reality (AR) application designed to visualize the content of Surah al-Fil as an interactive and context-rich medium for Islamic education. Using a research and development approach, the system was developed through structured stages including data collection, user requirement analysis, interface design, 3D asset creation using Blender, and integration of Unity 3D with the Vuforia SDK. The application features key visual elements such as the elephant army, the Kaaba, and the Ababil birds, which were modeled in detail and linked to high-contrast image markers to ensure accurate and stable AR tracking. Functional testing demonstrated strong technical performance, achieving a 95 percent marker detection accuracy at an optimal distance of 30-40 cm with consistent real-time rendering across multiple Android devices. User evaluations involving students and Islamic education teachers indicated high acceptance, with an overall satisfaction score of 4.7 out of 5 in terms of usability, visual appeal, interactivity, and learning effectiveness. These findings indicate that AR-based learning media can enhance learner engagement, deepen understanding of Quranic narratives, and provide immersive insights into historical and spiritual contexts. Overall, this study demonstrates that marker-based AR technology has significant potential to support innovation in digital Islamic education by enriching traditional learning with interactive and visually intuitive experiences.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [138] [Systemic Risk Radar: A Multi-Layer Graph Framework for Early Market Crash Warning](https://arxiv.org/abs/2512.17185)
*Sandeep Neela*

Main category: q-fin.RM

TL;DR: 提出Systemic Risk Radar (SRR)框架，以多层图建模金融市场检测系统性脆弱早期迹象及崩溃制度转变，通过三次危机评估表现，结果显示结构网络信息有预警作用，可扩展该框架。


<details>
  <summary>Details</summary>
Motivation: 金融危机会在跨部门、市场及投资者行为的结构脆弱性累积时出现，预测系统性转变具有挑战性，因为其源于市场参与者间的互动，而非孤立的价格变动。

Method: 提出SRR框架，将金融市场建模为多层图；采用三次重大危机进行评估，对比snapshot GNNs、简化的时间GNN原型和标准基线。

Result: 与基于特征的模型相比，结构网络信息能提供有用的早期预警信号；基于相关性的SRR实例表明图派生特征能捕捉压力事件期间市场结构的有意义变化。

Conclusion: 这项研究结果促使通过添加额外图层和更具表现力的时间架构来扩展SRR，以更好处理不同类型的危机。

Abstract: Financial crises emerge when structural vulnerabilities accumulate across sectors, markets, and investor behavior. Predicting these systemic transitions is challenging because they arise from evolving interactions between market participants, not isolated price movements alone. We present Systemic Risk Radar (SRR), a framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility and crash-regime transitions.
  We evaluate SRR across three major crises: the Dot-com crash, the Global Financial Crisis, and the COVID-19 shock. Our experiments compare snapshot GNNs, a simplified temporal GNN prototype, and standard baselines (logistic regression and Random Forest). Results show that structural network information provides useful early-warning signals compared to feature-based models alone.
  This correlation-based instantiation of SRR demonstrates that graph-derived features capture meaningful changes in market structure during stress events. The findings motivate extending SRR with additional graph layers (sector/factor exposure, sentiment) and more expressive temporal architectures (LSTM/GRU or Transformer encoders) to better handle diverse crisis types.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [139] [Modelling financial time series with $φ^{4}$ quantum field theory](https://arxiv.org/abs/2512.17225)
*Dimitrios Bachtis,David S. Berman,Arabella Schelpe*

Main category: q-fin.ST

TL;DR: 使用含非均匀耦合和显式对称性破缺的φ⁴量子场论对标准普尔500指数金融时间序列建模，对比伊辛模型优势，研究算法缩放性质并预测股票价格，探讨构建投资策略。


<details>
  <summary>Details</summary>
Motivation: 伊辛模型在金融时间序列建模存在离散化不准确、无法准确复现高阶统计量等问题，需要更好的模型。

Method: 运用φ⁴量子场论建模，研究其缩放性质，用模型预测股票价格。

Result: φ⁴量子场论能避免伊辛模型的不准确，可复现高阶统计量，能用于预测股票价格。

Conclusion: φ⁴标量场论可用于构建投资策略，量子场论操作能为金融建模提供直觉。

Abstract: We use a $φ^{4}$ quantum field theory with inhomogeneous couplings and explicit symmetry-breaking to model an ensemble of financial time series from the S$\&$P 500 index. The continuum nature of the $φ^4$ theory avoids the inaccuracies that occur in Ising-based models which require a discretization of the time series. We demonstrate this using the example of the 2008 global financial crisis. The $φ^{4}$ quantum field theory is expressive enough to reproduce the higher-order statistics such as the market kurtosis, which can serve as an indicator of possible market shocks. Accurate reproduction of high kurtosis is absent in binarized models. Therefore Ising models, despite being widely employed in econophysics, are incapable of fully representing empirical financial data, a limitation not present in the generalization of the $φ^{4}$ scalar field theory. We then investigate the scaling properties of the $φ^{4}$ machine learning algorithm and extract exponents which govern the behavior of the learned couplings (or weights and biases in ML language) in relation to the number of stocks in the model. Finally, we use our model to forecast the price changes of the AAPL, MSFT, and NVDA stocks. We conclude by discussing how the $φ^{4}$ scalar field theory could be used to build investment strategies and the possible intuitions that the QFT operations of dimensional compactification and renormalization can provide for financial modelling.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [140] [Disentangled representations via score-based variational autoencoders](https://arxiv.org/abs/2512.17127)
*Benjamin S. H. Lyo,Eero P. Simoncelli,Cristina Savin*

Main category: stat.ML

TL;DR: 提出SAMI用于无监督表征学习，结合扩散模型和VAE理论框架，能提取数据有意义结构，还可从预训练扩散模型提取有用表征，结果表明结合VAE可使扩散模型隐式结构信息显式可解释。


<details>
  <summary>Details</summary>
Motivation: 在无监督表征学习中结合扩散模型和VAE的优势，学习数据中有意义的表征。

Method: 统一扩散模型和VAE的证据下界，通过基于分数的指导学习表征。

Result: 在合成数据集恢复真实生成因子，从复杂自然图像学习因子化语义潜在维度，将视频序列编码为更直的潜在轨迹，能从预训练扩散模型提取有用表征。

Conclusion: 通过与变分自编码器协同组合，可使扩散模型中的隐式结构信息显式且可解释。

Abstract: We present the Score-based Autoencoder for Multiscale Inference (SAMI), a method for unsupervised representation learning that combines the theoretical frameworks of diffusion models and VAEs. By unifying their respective evidence lower bounds, SAMI formulates a principled objective that learns representations through score-based guidance of the underlying diffusion process. The resulting representations automatically capture meaningful structure in the data: it recovers ground truth generative factors in our synthetic dataset, learns factorized, semantic latent dimensions from complex natural images, and encodes video sequences into latent trajectories that are straighter than those of alternative encoders, despite training exclusively on static images. Furthermore, SAMI can extract useful representations from pre-trained diffusion models with minimal additional training. Finally, the explicitly probabilistic formulation provides new ways to identify semantically meaningful axes in the absence of supervised labels, and its mathematical exactness allows us to make formal statements about the nature of the learned representation. Overall, these results indicate that implicit structural information in diffusion models can be made explicit and interpretable through synergistic combination with a variational autoencoder.

</details>


### [141] [Sharp Structure-Agnostic Lower Bounds for General Functional Estimation](https://arxiv.org/abs/2512.17341)
*Jikai Jin,Vasilis Syrgkanis*

Main category: stat.ML

TL;DR: 本文系统研究无结构假设估计器的最优误差率，证明双重稳健学习对ATE估计的最优性，扩展分析到一般泛函类，区分两种情况并证明DML在两种情况下的最优性，给出显式最优率，为一阶去偏方法提供理论验证。


<details>
  <summary>Details</summary>
Motivation: 经典最优程序依赖强结构假设，实际中可能误设，因此对无结构假设方法的基本极限研究很关键。

Method: 先证明双重稳健学习对ATE估计的无结构假设最优误差率，再扩展到一般泛函类，区分双重稳健可实现和不可实现两种情况进行分析。

Result: 双重稳健学习对ATE估计达到最优无结构假设误差率，DML在两种情况下都最优，给出显式最优率。

Conclusion: 为广泛使用的一阶去偏方法提供理论验证，为无结构假设下寻求最优方法的实践者提供指导，推广并包含了作者之前的ATE下界结果。

Abstract: The design of efficient nonparametric estimators has long been a central problem in statistics, machine learning, and decision making. Classical optimal procedures often rely on strong structural assumptions, which can be misspecified in practice and complicate deployment. This limitation has sparked growing interest in structure-agnostic approaches -- methods that debias black-box nuisance estimates without imposing structural priors. Understanding the fundamental limits of these methods is therefore crucial. This paper provides a systematic investigation of the optimal error rates achievable by structure-agnostic estimators. We first show that, for estimating the average treatment effect (ATE), a central parameter in causal inference, doubly robust learning attains optimal structure-agnostic error rates. We then extend our analysis to a general class of functionals that depend on unknown nuisance functions and establish the structure-agnostic optimality of debiased/double machine learning (DML). We distinguish two regimes -- one where double robustness is attainable and one where it is not -- leading to different optimal rates for first-order debiasing, and show that DML is optimal in both regimes. Finally, we instantiate our general lower bounds by deriving explicit optimal rates that recover existing results and extend to additional estimands of interest. Our results provide theoretical validation for widely used first-order debiasing methods and guidance for practitioners seeking optimal approaches in the absence of structural assumptions. This paper generalizes and subsumes the ATE lower bound established in \citet{jin2024structure} by the same authors.

</details>


### [142] [Generative modeling of conditional probability distributions on the level-sets of collective variables](https://arxiv.org/abs/2512.17374)
*Fatima-Zahrae Akhyar,Wei Zhang,Gabriel Stoltz,Christof Schütte*

Main category: stat.ML

TL;DR: 本文研究概率分布条件概率分布的生成建模，提出学习方法和数据增强策略，并通过数值示例验证有效性。


<details>
  <summary>Details</summary>
Motivation: 研究概率分布在集体变量水平集上的条件概率分布的生成建模。

Method: 提出通用高效学习方法同时学习不同水平集的生成模型，利用增强采样技术数据进行数据增强。

Result: 通过具体数值示例证明所提学习方法的有效性。

Conclusion: 所提方法对生物物理中分子系统的生成建模有潜在用处。

Abstract: Given a probability distribution $μ$ in $\mathbb{R}^d$ represented by data, we study in this paper the generative modeling of its conditional probability distributions on the level-sets of a collective variable $ξ: \mathbb{R}^d \rightarrow \mathbb{R}^k$, where $1 \le k<d$. We propose a general and effcient learning approach that is able to learn generative models on different level-sets of $ξ$ simultaneously. To improve the learning quality on level-sets in low-probability regions, we also propose a strategy for data enrichment by utilizing data from enhanced sampling techniques. We demonstrate the effectiveness of our proposed learning approach through concrete numerical examples. The proposed approach is potentially useful for the generative modeling of molecular systems in biophysics, for instance.

</details>


### [143] [Perfect reconstruction of sparse signals using nonconvexity control and one-step RSB message passing](https://arxiv.org/abs/2512.17426)
*Xiaosi Gu,Ayaka Sakata,Tomoyuki Obuchi*

Main category: stat.ML

TL;DR: 本文研究通过最小化平滑截断绝对偏差（SCAD）惩罚进行稀疏信号重建，开发1RSB - AMP算法，分析其性能并提出确定参数的新准则，数值实验证实其性能提升。


<details>
  <summary>Details</summary>
Motivation: 提升稀疏信号重建算法性能，解决RS - AMP在某些参数区域发散的问题。

Method: 从信念传播的1RSB公式出发推导1RSB - AMP更新规则和状态演化方程，提出新的参数确定准则并结合NCC协议。

Result: 1RSB - AMP和1RSB - SE在宏观层面吻合良好，相图分析发现新的发散区域边界，算法的完美重建极限得到提升但略逊于贝叶斯最优阈值。

Conclusion: 提出的新准则和算法改进了稀疏信号重建的性能，为相关研究提供了新方法。

Abstract: We consider sparse signal reconstruction via minimization of the smoothly clipped absolute deviation (SCAD) penalty, and develop one-step replica-symmetry-breaking (1RSB) extensions of approximate message passing (AMP), termed 1RSB-AMP. Starting from the 1RSB formulation of belief propagation, we derive explicit update rules of 1RSB-AMP together with the corresponding state evolution (1RSB-SE) equations. A detailed comparison shows that 1RSB-AMP and 1RSB-SE agree remarkably well at the macroscopic level, even in parameter regions where replica-symmetric (RS) AMP, termed RS-AMP, diverges and where the 1RSB description itself is not expected to be thermodynamically exact. Fixed-point analysis of 1RSB-SE reveals a phase diagram consisting of success, failure, and diverging phases, as in the RS case. However, the diverging-region boundary now depends on the Parisi parameter due to the 1RSB ansatz, and we propose a new criterion -- minimizing the size of the diverging region -- rather than the conventional zero-complexity condition, to determine its value. Combining this criterion with the nonconvexity-control (NCC) protocol proposed in a previous RS study improves the algorithmic limit of perfect reconstruction compared with RS-AMP. Numerical solutions of 1RSB-SE and experiments with 1RSB-AMP confirm that this improved limit is achieved in practice, though the gain is modest and remains slightly inferior to the Bayes-optimal threshold. We also report the behavior of thermodynamic quantities -- overlaps, free entropy, complexity, and the non-self-averaging susceptibility -- that characterize the 1RSB phase in this problem.

</details>


### [144] [Fast and Robust: Computationally Efficient Covariance Estimation for Sub-Weibull Vectors](https://arxiv.org/abs/2512.17632)
*Even He*

Main category: stat.ML

TL;DR: 针对Sub - Weibull分布，提出交叉拟合范数截断估计器，运算复杂度低，能以高概率恢复最优次高斯率，为高维数据提供可扩展解决方案。


<details>
  <summary>Details</summary>
Motivation: 高维协方差估计对异常值敏感，现有通用重尾分布的最优估计器计算昂贵，本文针对Sub - Weibull分布寻找高效替代方法。

Method: 提出交叉拟合范数截断估计器，利用加权Hanson - Wright不等式推导非渐近误差界。

Result: 估计器运算复杂度为O(Nd^2)，在Sub - Weibull类中，以高概率恢复最优次高斯率$\tilde{O}(\sqrt{r(Σ)/N})$。

Conclusion: 该估计器为高维数据（尾部比高斯分布重但比多项式衰减轻）提供可扩展解决方案。

Abstract: High-dimensional covariance estimation is notoriously sensitive to outliers. While statistically optimal estimators exist for general heavy-tailed distributions, they often rely on computationally expensive techniques like semidefinite programming or iterative M-estimation ($O(d^3)$). In this work, we target the specific regime of \textbf{Sub-Weibull distributions} (characterized by stretched exponential tails $\exp(-t^α)$). We investigate a computationally efficient alternative: the \textbf{Cross-Fitted Norm-Truncated Estimator}. Unlike element-wise truncation, our approach preserves the spectral geometry while requiring $O(Nd^2)$ operations, which represents the theoretical lower bound for constructing a full covariance matrix. Although spherical truncation is geometrically suboptimal for anisotropic data, we prove that within the Sub-Weibull class, the exponential tail decay compensates for this mismatch. Leveraging weighted Hanson-Wright inequalities, we derive non-asymptotic error bounds showing that our estimator recovers the optimal sub-Gaussian rate $\tilde{O}(\sqrt{r(Σ)/N})$ with high probability. This provides a scalable solution for high-dimensional data that exhibits tails heavier than Gaussian but lighter than polynomial decay.

</details>


### [145] [Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design](https://arxiv.org/abs/2512.17659)
*Madhav R. Muthyala,Farshud Sorourifar,Tianhong Tan,You Peng,Joel A. Paulson*

Main category: stat.ML

TL;DR: 该工作提出“生成 - 优化”框架用于多目标分子设计，展示了比现有方法更优性能，在储能案例中发现新的高性能电极材料。


<details>
  <summary>Details</summary>
Motivation: 现有耦合贝叶斯优化和生成模型的方法依赖连续潜在空间，存在架构纠缠和可扩展性问题，需新的多目标分子设计框架。

Method: 采用“生成 - 优化”框架，用生成模型构建候选分子池，用新的获取函数qPMHI选择最可能扩大帕累托前沿的一批候选分子。

Result: 在合成基准和应用驱动任务中比现有方法有显著改进，在储能案例中快速找到新的高性能有机阴极材料。

Conclusion: 提出的框架在多目标分子设计/发现中有效且具有可扩展性，能助力发现高性能分子材料。

Abstract: Designing molecules that must satisfy multiple, often conflicting objectives is a central challenge in molecular discovery. The enormous size of chemical space and the cost of high-fidelity simulations have driven the development of machine learning-guided strategies for accelerating design with limited data. Among these, Bayesian optimization (BO) offers a principled framework for sample-efficient search, while generative models provide a mechanism to propose novel, diverse candidates beyond fixed libraries. However, existing methods that couple the two often rely on continuous latent spaces, which introduces both architectural entanglement and scalability challenges. This work introduces an alternative, modular "generate-then-optimize" framework for de novo multi-objective molecular design/discovery. At each iteration, a generative model is used to construct a large, diverse pool of candidate molecules, after which a novel acquisition function, qPMHI (multi-point Probability of Maximum Hypervolume Improvement), is used to optimally select a batch of candidates most likely to induce the largest Pareto front expansion. The key insight is that qPMHI decomposes additively, enabling exact, scalable batch selection via only simple ranking of probabilities that can be easily estimated with Monte Carlo sampling. We benchmark the framework against state-of-the-art latent-space and discrete molecular optimization methods, demonstrating significant improvements across synthetic benchmarks and application-driven tasks. Specifically, in a case study related to sustainable energy storage, we show that our approach quickly uncovers novel, diverse, and high-performing organic (quinone-based) cathode materials for aqueous redox flow battery applications.

</details>


### [146] [Imputation Uncertainty in Interpretable Machine Learning Methods](https://arxiv.org/abs/2512.17689)
*Pegah Golchian,Marvin N. Wright*

Main category: stat.ML

TL;DR: 本文比较不同插补方法对可解释机器学习方法置信区间覆盖率的影响，发现单一插补低估方差，多数情况下多重插补接近名义覆盖率。


<details>
  <summary>Details</summary>
Motivation: 实际数据中缺失值常见，影响可解释机器学习方法的解释，现有工作忽略插补不确定性及其对方差和置信区间的影响。

Method: 比较不同插补方法对可解释机器学习方法（排列特征重要性、部分依赖图和Shapley值）置信区间覆盖率的影响。

Result: 单一插补导致方差低估，多数情况下只有多重插补接近名义覆盖率。

Conclusion: 多重插补在处理缺失值以保证可解释机器学习方法置信区间覆盖率方面更优。

Abstract: In real data, missing values occur frequently, which affects the interpretation with interpretable machine learning (IML) methods. Recent work considers bias and shows that model explanations may differ between imputation methods, while ignoring additional imputation uncertainty and its influence on variance and confidence intervals. We therefore compare the effects of different imputation methods on the confidence interval coverage probabilities of the IML methods permutation feature importance, partial dependence plots and Shapley values. We show that single imputation leads to underestimation of variance and that, in most cases, only multiple imputation is close to nominal coverage.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [147] [Inference for high dimensional repeated measure designs with the R package hdrm](https://arxiv.org/abs/2512.17478)
*Paavo Sattler,Nils Hichert*

Main category: stat.CO

TL;DR: 本文介绍R包hdrm用于高维重复测量设计数据分析，阐述其测试方法、应用示例及应对计算挑战策略。


<details>
  <summary>Details</summary>
Motivation: 现代数据收集使观测向量维度高，给高维重复测量设计的统计推断带来挑战，需有效分析方法。

Method: 使用基于多元正态性的参数方法，通过R包hdrm分析期望向量相关假设，结合高效估计器和子抽样策略。

Result: R包hdrm可分析高维重复测量设计多种假设，结合策略后能减少计算时间并保持统计有效性。

Conclusion: R包hdrm为高维重复测量设计数据提供了有效分析方法，适用于实际高维数据场景。

Abstract: Repeated-measure designs allow comparisons within a group as well as between groups, and are commonly referred to as split-plot designs. While originating in agricultural experiments, they are now widely used in medical research, psychology, and the life sciences, where repeated observations on the same subject are essential.
  Modern data collection often produces observation vectors with dimension $d$ comparable to or exceeding the sample size $N$. Although this can be advantageous in terms of cost efficiency, ethical considerations, and the study of rare diseases, it poses substantial challenges for statistical inference.
  Parametric methods based on multivariate normality provide a flexible framework that avoids restrictive assumptions on covariance structures or on the asymptotic relationship between $d$ and $N$. Within this framework, the freely available R-package hdrm enables the analysis of a wide range of hypotheses concerning expectation vectors in high-dimensional repeated-measure designs, covering both single-group and multi-group settings with homogeneous or heterogeneous covariance matrices.
  This paper describes the implemented tests, demonstrates their use through examples, and discusses their applicability in practical high-dimensional data scenarios. To address computational challenges arising for large $d$, the package incorporates efficient estimators and subsampling strategies that substantially reduce computation time while preserving statistical validity.

</details>


### [148] [Delayed Acceptance Slice Sampling](https://arxiv.org/abs/2512.17868)
*Kevin Bitterlich,Daniel Rudolf,Björn Sprungk*

Main category: stat.CO

TL;DR: 本文提出延迟接受的混合切片采样器版本，利用目标密度的确定性近似，提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 切片采样算法每步需多次评估目标密度，计算成本高，特别是贝叶斯推断中似然函数计算代价大的情况。

Method: 利用目标密度的确定性近似，提出延迟接受的混合切片采样器版本。

Result: 证明了所提切片采样方法的遍历性，讨论了延迟接受（理想）切片采样相对延迟接受Metropolis - Hastings算法的优越性。

Conclusion: 所提新方法在多个数值实验中提高了计算效率。

Abstract: Slice sampling is a well-established Markov chain Monte Carlo method for (approximate) sampling of target distributions which are only known up to a normalizing constant. The method is based on choosing a new state on a slice, i.e., a superlevel set of the given unnormalized target density (with respect to a reference measure). However, slice sampling algorithms usually require per step multiple evaluations of the target density, and thus can become computationally expensive. This is particularly the case for Bayesian inference with costly likelihoods. In this paper, we exploit deterministic approximations of the target density, which are relatively cheap to evaluate, and propose delayed acceptance versions of hybrid slice samplers. We show ergodicity of the resulting slice sampling methods, discuss the superiority of delayed acceptance (ideal) slice sampling over delayed acceptance Metropolis-Hastings algorithms, and illustrate the benefits of our novel approach in terms improved computational efficiency in several numerical experiments.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [149] [Line Cover and Related Problems](https://arxiv.org/abs/2512.17268)
*Matthias Bentert,Fedor v. Fomin,Petr A. Golovach,Souvik Saha,Sanjay Seetharaman,Anannya Upasana*

Main category: cs.CG

TL;DR: 研究经典Line Cover问题的扩展，包括Line Clustering、Hyperplane Cover和Projective Clustering问题，揭示了参数化复杂度的差异并给出了Projective Clustering的算法。


<details>
  <summary>Details</summary>
Motivation: 经典Line Cover问题是NP难问题，研究其扩展问题在机器学习、数据分析和计算几何中有应用。

Method: 对不同问题的参数化复杂度进行分析，证明复杂度情况并给出算法。

Result: Line Cover参数k固定时是易处理的；Line Clustering关于k是W[1]难的；Hyperplane Cover在d = 2时是NP难，仅参数k时是W[2]难；给出Projective Clustering的n^{O(dk(r + 1))}时间算法。

Conclusion: 不同扩展问题在参数化复杂度上有显著差异，给出的算法匹配下界并推广了经典算法。

Abstract: We study extensions of the classic \emph{Line Cover} problem, which asks whether a set of $n$ points in the plane can be covered using $k$ lines. Line Cover is known to be NP-hard, and we focus on two natural generalizations. The first is \textbf{Line Clustering}, where the goal is to find $k$ lines minimizing the sum of squared distances from the input points to their nearest line. The second is \textbf{Hyperplane Cover}, which asks whether $n$ points in $\mathbb{R}^d$ can be covered by $k$ hyperplanes.
  We also study the more general \textbf{Projective Clustering} problem, which unifies both settings and has applications in machine learning, data analysis, and computational geometry. In this problem, one seeks $k$ affine subspaces of dimension $r$ that minimize the sum of squared distances from the given points in $\mathbb{R}^d$ to the nearest subspace.
  Our results reveal notable differences in the parameterized complexity of these problems. While Line Cover is fixed-parameter tractable when parameterized by $k$, we show that Line Clustering is W[1]-hard with respect to $k$ and does not admit an algorithm with running time $n^{o(k)}$ unless the Exponential Time Hypothesis fails. Hyperplane Cover is NP-hard even for $d=2$, and prior work of Langerman and Morin [Discrete & Computational Geometry, 2005] showed that it is fixed-parameter tractable when parameterized by both $k$ and $d$. We complement this by proving that Hyperplane Cover is W[2]-hard when parameterized by $k$ alone.
  Finally, we present an algorithm for Projective Clustering running in $n^{O(dk(r+1))}$ time. This bound matches our lower bound for Line Clustering and generalizes the classic algorithm for $k$-Means Clustering ($r=0$) by Inaba, Katoh, and Imai [SoCG 1994].

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [150] [Colormap-Enhanced Vision Transformers for MRI-Based Multiclass (4-Class) Alzheimer's Disease Classification](https://arxiv.org/abs/2512.16964)
*Faisal Ahmed*

Main category: eess.IV

TL;DR: 提出PseudoColorViT - Alz框架用于阿尔茨海默病分类，在OASIS - 1数据集上表现超当前方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型难以从脑MRI扫描中有效提取判别特征，需新方法提升阿尔茨海默病分类效果。

Method: 提出PseudoColorViT - Alz框架，结合颜色映射变换和视觉Transformer的全局特征学习能力。

Result: 在OASIS - 1数据集上实现99.79%的准确率和100%的AUC，超越2024 - 2025年的CNN和孪生网络等方法。

Conclusion: 伪彩色增强与视觉Transformer结合可显著提升基于MRI的阿尔茨海默病分类效果，PseudoColorViT - Alz是有前景的临床工具。

Abstract: Magnetic Resonance Imaging (MRI) plays a pivotal role in the early diagnosis and monitoring of Alzheimer's disease (AD). However, the subtle structural variations in brain MRI scans often pose challenges for conventional deep learning models to extract discriminative features effectively. In this work, we propose PseudoColorViT-Alz, a colormap-enhanced Vision Transformer framework designed to leverage pseudo-color representations of MRI images for improved Alzheimer's disease classification. By combining colormap transformations with the global feature learning capabilities of Vision Transformers, our method amplifies anatomical texture and contrast cues that are otherwise subdued in standard grayscale MRI scans.
  We evaluate PseudoColorViT-Alz on the OASIS-1 dataset using a four-class classification setup (non-demented, moderate dementia, mild dementia, and very mild dementia). Our model achieves a state-of-the-art accuracy of 99.79% with an AUC of 100%, surpassing the performance of recent 2024--2025 methods, including CNN-based and Siamese-network approaches, which reported accuracies ranging from 96.1% to 99.68%. These results demonstrate that pseudo-color augmentation combined with Vision Transformers can significantly enhance MRI-based Alzheimer's disease classification. PseudoColorViT-Alz offers a robust and interpretable framework that outperforms current methods, providing a promising tool to support clinical decision-making and early detection of Alzheimer's disease.

</details>


### [151] [MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation](https://arxiv.org/abs/2512.17774)
*Saikat Roy,Yannick Kirchhoff,Constantin Ulrich,Maximillian Rokuss,Tassilo Wald,Fabian Isensee,Klaus Maier-Hein*

Main category: eess.IV

TL;DR: 本文提出MedNeXt - v2用于3D医学图像分割，在多个基准测试中表现出色，证明其是强大骨干网络。


<details>
  <summary>Details</summary>
Motivation: 现有大规模监督预训练在3D医学图像分割中主要关注数据集大小，忽略骨干网络在大规模下是否为有效表征学习器，本文旨在解决此问题。

Method: 重新审视基于ConvNeXt的架构，引入MedNeXt - v2，结合3D全局响应归一化模块，采用深度、宽度和上下文缩放改进架构，在18k CT体积上预训练。

Result: 在六个具有挑战性的CT和MR基准测试（144个结构）中微调时表现出了最先进性能，相比七个公开预训练模型有持续提升；还发现更强骨干网络在相似数据上效果更好，表征缩放对病理分割更有益，全微调后特定模态预训练益处不大。

Conclusion: MedNeXt - v2是3D医学图像分割中大规模监督表征学习的强大骨干网络。

Abstract: Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation. However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale. In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a compound-scaled 3D ConvNeXt that leverages improved micro-architecture and data scaling to deliver state-of-the-art performance. First, we show that routinely used backbones in large-scale pretraining pipelines are often suboptimal. Subsequently, we use comprehensive backbone benchmarking prior to scaling and demonstrate that stronger from scratch performance reliably predicts stronger downstream performance after pretraining. Guided by these findings, we incorporate a 3D Global Response Normalization module and use depth, width, and context scaling to improve our architecture for effective representation learning. We pretrain MedNeXt-v2 on 18k CT volumes and demonstrate state-of-the-art performance when fine-tuning across six challenging CT and MR benchmarks (144 structures), showing consistent gains over seven publicly released pretrained models. Beyond improvements, our benchmarking of these models also reveals that stronger backbones yield better results on similar data, representation scaling disproportionately benefits pathological segmentation, and that modality-specific pretraining offers negligible benefit once full finetuning is applied. In conclusion, our results establish MedNeXt-v2 as a strong backbone for large-scale supervised representation learning in 3D Medical Image Segmentation. Our code and pretrained models are made available with the official nnUNet repository at: https://www.github.com/MIC-DKFZ/nnUNet

</details>


### [152] [Resource-efficient medical image classification for edge devices](https://arxiv.org/abs/2512.17515)
*Mahsa Lavaei,Zahra Abadi,Salar Beigzad,Alireza Maleki*

Main category: eess.IV

TL;DR: 研究资源高效的医学图像分类方法，采用模型量化技术，实验表明量化模型能减少大小和延迟，维持诊断准确性，为偏远资源有限地区提供AI医疗诊断途径。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上部署深度学习模型进行医学图像分类存在计算和内存限制的挑战，需资源高效的方法。

Method: 采用模型量化技术，优化针对边缘设备的量化感知训练（QAT）和训练后量化（PTQ）方法。

Result: 量化模型大幅减少模型大小和推理延迟，能在边缘硬件上实时处理，维持临床可接受的诊断准确性。

Conclusion: 为在偏远和资源有限地区部署AI医疗诊断提供了实用途径，提高了医疗技术的可及性和可扩展性。

Abstract: Medical image classification is a critical task in healthcare, enabling accurate and timely diagnosis. However, deploying deep learning models on resource-constrained edge devices presents significant challenges due to computational and memory limitations. This research investigates a resource-efficient approach to medical image classification by employing model quantization techniques. Quantization reduces the precision of model parameters and activations, significantly lowering computational overhead and memory requirements without sacrificing classification accuracy. The study focuses on the optimization of quantization-aware training (QAT) and post-training quantization (PTQ) methods tailored for edge devices, analyzing their impact on model performance across medical imaging datasets. Experimental results demonstrate that quantized models achieve substantial reductions in model size and inference latency, enabling real-time processing on edge hardware while maintaining clinically acceptable diagnostic accuracy. This work provides a practical pathway for deploying AI-driven medical diagnostics in remote and resource-limited settings, enhancing the accessibility and scalability of healthcare technologies.

</details>


### [153] [SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis](https://arxiv.org/abs/2512.17585)
*N. A. Adarsh Pritam,Jeba Shiney O,Sanyam Jain*

Main category: eess.IV

TL;DR: 本文介绍SkinGenBench基准，评估生成模型和预处理复杂度对合成皮肤镜图像增强及黑色素瘤诊断的影响，发现生成架构影响更大，合成数据增强显著提升诊断效果。


<details>
  <summary>Details</summary>
Motivation: 研究预处理复杂度与生成模型选择如何相互作用，以用于合成皮肤镜图像增强和下游黑色素瘤诊断。

Method: 使用14,116张皮肤镜图像，评估StyleGAN2 - ADA和DDPMs在基本几何增强和高级伪影去除流程下的表现，用多种指标评估合成图像并测试其对五种下游分类器诊断性能的影响。

Result: 生成架构对图像保真度和诊断效用影响更大，StyleGAN2 - ADA生成图像更接近真实数据分布，高级伪影去除改进有限，合成数据增强显著提升黑色素瘤检测效果。

Conclusion: 生成架构比预处理复杂度对图像和诊断影响更大，合成数据增强能有效提升黑色素瘤诊断性能。

Abstract: This work introduces SkinGenBench, a systematic biomedical imaging benchmark that investigates how preprocessing complexity interacts with generative model choice for synthetic dermoscopic image augmentation and downstream melanoma diagnosis. Using a curated dataset of 14,116 dermoscopic images from HAM10000 and MILK10K across five lesion classes, we evaluate the two representative generative paradigms: StyleGAN2-ADA and Denoising Diffusion Probabilistic Models (DDPMs) under basic geometric augmentation and advanced artifact removal pipelines. Synthetic melanoma images are assessed using established perceptual and distributional metrics (FID, KID, IS), feature space analysis, and their impact on diagnostic performance across five downstream classifiers. Experimental results demonstrate that generative architecture choice has a stronger influence on both image fidelity and diagnostic utility than preprocessing complexity. StyleGAN2-ADA consistently produced synthetic images more closely aligned with real data distributions, achieving the lowest FID (~65.5) and KID (~0.05), while diffusion models generated higher variance samples at the cost of reduces perceptual fidelity and class anchoring. Advanced artifact removal yielded only marginal improvements in generative metrics and provided limited downstream diagnostic gains, suggesting possible suppression of clinically relevant texture cues. In contrast, synthetic data augmentation substantially improved melanoma detection with 8-15% absolute gains in melanoma F1-score, and ViT-B/16 achieving F1~0.88 and ROC-AUC~0.98, representing an improvement of approximately 14% over non-augmented baselines. Our code can be found at https://github.com/adarsh-crafts/SkinGenBench

</details>


### [154] [Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data](https://arxiv.org/abs/2512.17759)
*Rahul Ravi,Ruizhe Li,Tarek Abdelfatah,Stephen Chan,Xin Chen*

Main category: eess.IV

TL;DR: 本文利用磁共振影像和临床数据，开发机器学习模型预测乳腺癌患者新辅助化疗响应和5年无复发生存情况，发现图像配准法提升性能，放射组学特征提取器表现更好。


<details>
  <summary>Details</summary>
Motivation: 利用纵向增强磁共振影像和临床数据，开发机器学习模型预测乳腺癌患者新辅助化疗的病理完全缓解和5年无复发生存情况。

Method: 提出包含肿瘤分割、图像配准、特征提取和预测建模的框架，比较4种特征提取器、3种特征选择方法和4种机器学习模型。

Result: 基于图像配准的特征提取改善预测模型性能，逻辑回归模型结合放射组学特征在病理完全缓解和5年无复发生存预测任务中表现最佳。

Conclusion: 图像配准法显著提升纵向特征学习性能，放射组学特征提取器比深度学习特征提取器更有效、可解释性更强。

Abstract: Aim: This study investigates treatment response prediction to neoadjuvant chemotherapy (NACT) in breast cancer patients, using longitudinal contrast-enhanced magnetic resonance images (CE-MRI) and clinical data. The goal is to develop machine learning (ML) models to predict pathologic complete response (PCR binary classification) and 5-year relapse-free survival status (RFS binary classification). Method: The proposed framework includes tumour segmentation, image registration, feature extraction, and predictive modelling. Using the image registration method, MRI image features can be extracted and compared from the original tumour site at different time points, therefore monitoring the intratumor changes during NACT process. Four feature extractors, including one radiomics and three deep learning-based (MedicalNet, Segformer3D, SAM-Med3D) were implemented and compared. In combination with three feature selection methods and four ML models, predictive models are built and compared. Results: The proposed image registration-based feature extraction consistently improves the predictive models. In the PCR and RFS classification tasks logistic regression model trained on radiomic features performed the best with an AUC of 0.88 and classification accuracy of 0.85 for PCR classification, and AUC of 0.78 and classification accuracy of 0.72 for RFS classification. Conclusions: It is evidenced that the image registration method has significantly improved performance in longitudinal feature learning in predicting PCR and RFS. The radiomics feature extractor is more effective than the pre-trained deep learning feature extractors, with higher performance and better interpretability.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [155] [Revisiting the Broken Symmetry Phase of Solid Hydrogen: A Neural Network Variational Monte Carlo Study](https://arxiv.org/abs/2512.17703)
*Shengdu Chai,Chen Lin,Xinyang Dong,Yuqiang Li,Wanli Ouyang,Lei Wang,X. C. Xie*

Main category: cond-mat.str-el

TL;DR: 用基于深度神经网络波函数的第一性原理量子蒙特卡罗框架研究高压固态氢，发现Cmcm结构与实验结果匹配，证明需全量子多体处理。


<details>
  <summary>Details</summary>
Motivation: 高压固态氢的晶体结构是基本问题，130GPa左右的破对称相因电子和核自由度复杂耦合需重新研究。

Method: 开发基于深度神经网络波函数的第一性原理量子蒙特卡罗框架，在恒压系综中量子力学处理电子和核。

Result: 发现破对称相的新基态结构候选Cmcm，与实验状态方程和X射线衍射模式定量匹配，群论分析与光谱数据兼容，静态密度泛函理论显示该结构是不稳定鞍点。

Conclusion: 研究结果为高压氢相图提供新见解，需进一步实验验证。

Abstract: The crystal structure of high-pressure solid hydrogen remains a fundamental open problem. Although the research frontier has mostly shifted toward ultra-high pressure phases above 400 GPa, we show that even the broken symmetry phase observed around 130~GPa requires revisiting due to its intricate coupling of electronic and nuclear degrees of freedom. Here, we develop a first principle quantum Monte Carlo framework based on a deep neural network wave function that treats both electrons and nuclei quantum mechanically within the constant pressure ensemble. Our calculations reveal an unreported ground-state structure candidate for the broken symmetry phase with $Cmcm$ space group symmetry, and we test its stability up to 96 atoms. The predicted structure quantitatively matches the experimental equation of state and X-ray diffraction patterns. Furthermore, our group-theoretical analysis shows that the $Cmcm$ structure is compatible with existing Raman and infrared spectroscopic data. Crucially, static density functional theory calculation reveals the $Cmcm$ structure as a dynamically unstable saddle point on the Born-Oppenheimer potential energy surface, demonstrating that a full quantum many-body treatment of the problem is necessary. These results shed new light on the phase diagram of high-pressure hydrogen and call for further experimental verifications.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [156] [Who Connects Global Aid? The Hidden Geometry of 10 Million Transactions](https://arxiv.org/abs/2512.17243)
*Paul X. McCarthy,Xian Gong,Marian-Andrei Rizoiu,Paolo Boldi*

Main category: physics.soc-ph

TL;DR: 利用大规模数据集绘制全球援助网络拓扑，揭示隐藏模式，发现关键连接角色，为捐赠者提供新框架。


<details>
  <summary>Details</summary>
Motivation: 当前对全球援助系统结构的理解局限于总体流量，需深入了解其网络拓扑。

Method: 使用超1000万交易记录数据集，应用二分投影和降维方法。

Result: 发现政府和多边机构提供主要资源，知识中介提供关键连接，确定25个核心行动者，影响力与结构连接和资金量有关。

Conclusion: 研究结果为捐赠者识别战略伙伴、促进全球网络协调和证据传播提供新框架。

Abstract: The global aid system functions as a complex and evolving ecosystem; yet widespread understanding of its structure remains largely limited to aggregate volume flows. Here we map the network topology of global aid using a dataset of unprecedented scale: over 10 million transaction records connecting 2,456 publishing organisations across 230 countries between 1967 and 2025. We apply bipartite projection and dimensionality reduction to reveal the geometry of the system and unveil hidden patterns. This exposes distinct functional clusters that are otherwise sparsely connected. We find that while governments and multilateral agencies provide the primary resources, a small set of knowledge brokers provide the critical connectivity. Universities and research foundations specifically act as essential bridges between disparate islands of implementers and funders. We identify a core solar system of 25 central actors who drive this connectivity including unanticipated brokers like J-PAL and the Hewlett Foundation. These findings demonstrate that influence in the aid ecosystem flows through structural connectivity as much as financial volume. Our results provide a new framework for donors to identify strategic partners that accelerate coordination and evidence diffusion across the global network.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [157] [The Effect of Foreign Direct Investment on Economic Growth in South Asian Countries](https://arxiv.org/abs/2512.16958)
*S M Toufiqul Huq Sowrov*

Main category: econ.GN

TL;DR: 研究用1980 - 2017年南亚五国面板数据，分析FDI对经济增长影响，发现FDI与经济增长正相关但不显著，国内投资和人力资本更关键，建议政策聚焦本地投资和教育。


<details>
  <summary>Details</summary>
Motivation: 研究外国直接投资（FDI）对南亚国家经济增长的影响。

Method: 利用1980 - 2017年五个南亚区域合作联盟成员国的年度面板数据，采用普通最小二乘法、固定效应、随机效应和广义最小二乘法回归等静态面板模型进行分析。

Result: FDI与经济增长正相关但统计上不显著，国内投资和人力资本是显著的积极增长因素，政府消费和通货膨胀等控制变量与增长多为预期的负相关且不显著。

Conclusion: 南亚经济体应加强国内投资并培养人力资本以促进经济增长，FDI有补充作用，需进一步研究影响其效果的条件因素。

Abstract: This study investigates the impact of Foreign Direct Investment (FDI) on economic growth in South Asian countries, utilizing annual panel data from five SAARC member states (Bangladesh, India, Nepal, Pakistan, and Sri Lanka) over the period 1980-2017. Data sourced from the World Development Indicators and Penn World Table were analyzed using static panel models, including Ordinary Least Squares, Fixed Effects, Random Effects, and Generalized Least Squares regressions. The empirical findings reveal that FDI exhibits a consistently positive but statistically insignificant correlation with economic growth across all model specifications. In contrast, domestic investment and human capital development emerge as significant and robust positive determinants of growth. Control variables such as government consumption and inflation show expected negative, though generally insignificant, associations with growth. The results imply that for the sampled South Asian economies, enhancing domestic investment and fostering human capital are more critical for driving economic expansion than relying on FDI inflows. Consequently, policymakers should prioritize strategies that strengthen local investment climates and improve educational and skill-building institutions to boost productivity. While FDI's role remains complementary, its insignificant immediate impact suggests the need for further research into the conditional factors such as institutional quality, financial market development, and trade policies that might mediate its effectiveness in fostering long-term growth within the region.

</details>


### [158] [Immigrant Residential Segregation in Europe: A Comparative Study of Spatial Segregation Patterns in Urban Areas across 30 Countries](https://arxiv.org/abs/2512.17037)
*Tobias Rüttenauer,Kasimir Dederichs,David Kretschmer*

Main category: econ.GN

TL;DR: 本文利用普查数据研究欧洲城市移民隔离问题，发现西欧和北欧隔离程度高，且受多种因素影响。


<details>
  <summary>Details</summary>
Motivation: 缺乏欧洲移民隔离情况及相关结构因素的系统证据。

Method: 使用2021/22年普查的1x1公里网格数据计算717个功能城市区域的隔离指数，结合多方面数据，应用规范曲线分析16164个回归模型。

Result: 西欧和北欧隔离程度高于东欧和南欧，隔离受城市核心与郊区宏观空间动态、城市和国家层面多种因素影响。

Conclusion: 研究提供了欧洲移民隔离最全面的比较评估，揭示了结构条件与空间融合的关系。

Abstract: Immigrant residential segregation can profoundly shape access to opportunities, immigrant integration, and inter-group relations. Yet we lack systematic evidence on how segregation varies across Europe, and what structural factors are associated with these patterns. This study addresses the gap by focusing on two questions: (i) how does immigrant-native segregation vary across urban areas in Europe, and (ii) which urban area- and country-level characteristics are consistently linked to segregation? Using harmonised 1x1 km grid-level data from the 2021/22 census, we calculate spatially weighted Dissimilarity Indices for all 717 Functional Urban Areas (FUAs) across 30 European countries. We combine these measures with rich data on demographics, the economy, housing, immigrant populations, and policy. To identify robust correlates of segregation, we apply a Specification Curve Analysis across 16,164 regression models. Segregation is higher in Western and Northern Europe compared to most of Eastern and Southern Europe. Moreover, we show that segregation is heavily driven by macro-spatial dynamics between diverse urban cores and relatively homogeneous suburban areas. At the urban area level, segregation is systematically linked to the demographic composition and spatial distribution of the local population, economic conditions, housing market characteristics, as well as the composition of the immigrant population. At the national level, established immigrant destinations are more segregated, while migration and integration policies are not consistently linked to segregation. These findings offer the most comprehensive comparative assessment of immigrant segregation across Europe to date, revealing how structural conditions relate to spatial integration.

</details>


### [159] [The Trust-Building Game: A Model for Sustainable Cooperation](https://arxiv.org/abs/2512.17061)
*Madjid Eshaghi Gordji,Mohamadali Berahman*

Main category: econ.GN

TL;DR: 本文提出创新方法，聚焦第二玩家决策过程，通过重复结构让第一玩家培养第二玩家信任，适用于多领域，助于理解信任建立和促进合作。


<details>
  <summary>Details</summary>
Motivation: 传统信任模型主要关注第一玩家决策，本文旨在将重点转移到第二玩家决策过程。

Method: 提出采用重复结构的模型，让第一玩家通过一致且有策略的行动培养第二玩家的信任。

Result: 该框架适用于公共政策、营销和国际关系等多个领域。

Conclusion: 该模型加深了对信任建立过程的理解，有助于在现实场景中促进可持续合作。

Abstract: Trust serves as a fundamental pillar of human interactions, playing a crucial role in economic, social, and political relationships. While traditional models of trust primarily focus on the decision making of the first player, this paper introduces an innovative approach that shifts the emphasis to the decision making processes of the second player. The proposed model employs a repetitive structure through which the first player effectively cultivates the second player's trust through consistent and strategic actions. This framework is applicable across various fields, including public policy, marketing, and international relations. It significantly deepens our understanding of the trust-building process and aids in fostering sustainable cooperation in real world scenarios.

</details>


### [160] [The Effects of Initial Low-barrier Employment Availability on Refugee Labor Market Integration](https://arxiv.org/abs/2512.17422)
*Felix Degenhardt*

Main category: econ.GN

TL;DR: 研究奥地利酒店业早期低门槛就业机会对难民劳动力市场融入的影响，发现初期有就业促进，但非垫脚石，有收入增加但也有劳动市场隔离问题。


<details>
  <summary>Details</summary>
Motivation: 研究酒店业早期临时低门槛就业机会对难民劳动力市场融入的影响。

Method: 将难民准外生分配到奥地利酒店业季节性高的地区，利用地区内和年度内的差异进行分析。

Result: 旺季获得就业机会使初期就业概率上升达3个百分点，一年后就业优势减弱；前三年总收入增加，中期工资和工作质量无显著差异；但增加了劳动力市场隔离。

Conclusion: 酒店业早期就业机会不能成为难民劳动力市场融入的垫脚石，且带来劳动力市场隔离问题。

Abstract: I examine whether the early but temporary availability of low-barrier employment opportunities in the hospitality sector affects the labor market integration of refugees. My identification strategy combines the quasi-exogenous allocation of refugees to Austrian regions with high seasonality in Austria's hospitality sector, where 25% of refugees find initial employment. Exploiting within region, within year variation, I find that receiving labor market access during high seasonal demand increases employment probability initially, with significant employment effects of up to 3 percentage points, or 9% of the mean, in the first months. Employment advantages diminish after the first year, indicating that such early employment opportunities do not serve as a stepping stone. Still, treated refugees have in total earned more in the first three years, with no significant differences in medium-term wages and job quality. One disadvantage of early employment in hospitality is the increased labor market segregation, as treated refugees are more likely to work in industries more typical for refugees and in firms with higher non-Austrian coworker shares.

</details>


### [161] [Most certainly certain? The Impact of Contract for Difference Design on Renewables' Strike Prices and Electricity Market Risks](https://arxiv.org/abs/2512.17508)
*Silke Johanndeiter,Jonas Finke,Justus Heuer*

Main category: econ.GN

TL;DR: 研究三种不同差价合约（CfD）设计对高可再生能源电力市场中风电利润和消费者价格波动的影响，发现CfD可显著降低波动，不同设计对消费者价格影响无实质差异，特定设计对投资者利润波动降低效果最佳，但存在激励与降低风险的权衡。


<details>
  <summary>Details</summary>
Motivation: 高可再生能源电力市场中，天气、技术和监管不确定性使参与者面临价格和数量风险，双向CfD可缓解这些风险，因此研究不同CfD设计的影响。

Method: 首先分析推导不确定情况下的最优执行价格；其次基于能源系统模型中36个市场情景优化得到的市场预期，数值确定最优执行价格；最后研究36个情景中事后市场收入、CfD支付和消费者价格的分布。

Result: 与纯市场价格和投资者利润相比，所有CfD均显著降低波动；对消费者价格，不同CfD设计无实质差异；对投资者利润，基于容量且参考价格类似电厂个体市场收入的CfD波动降低效果最佳。

Conclusion: 存在激励系统友好性和降低投资者风险之间的权衡。

Abstract: Weather, technological and regulatory uncertainties expose actors in highly renewable electricity markets to substantial price and volume risks. Two-way Contracts for Difference (CfDs) can mitigate these risks. They stipulate payments between the government and generators of renewable electricity based on the difference of a strike and a reference price, whose definition and unit of payment differ between CfD designs. We study the effect of three different CfD designs on wind power profit and consumer price volatility under the consideration of uncertain market outcomes in a highly renewable, sector-coupled electricity market. First, we analytically derive optimal strike prices under uncertainty. Second, we numerically determine optimal strike prices based on market expectations retrieved from optimising a set of 36 market scenarios in an energy system model. Third, we study the distribution of ex post market revenues, CfD payments and consumer prices across all 36 scenarios. Compared to purely market-based consumer prices and investor profits, we find all CfDs to significantly reduce volatility. For consumer prices, results show no substantial differences between CfD designs. For investor profits, we identify the highest volatility reduction under a capacity-based CfD with a reference price similar to power plants' individual market revenues. Since such a CfD design is known to diminish the effect of price signals on investment decisions, our results reveal a trade-off between incentivising system-friendliness and reducing investor risk.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [162] [Alternating Direction Method of Multipliers for Nonlinear Matrix Decompositions](https://arxiv.org/abs/2512.17473)
*Atharva Awari,Nicolas Gillis,Arnaud Vandaele*

Main category: eess.SP

TL;DR: 提出基于ADMM的算法解决非线性矩阵分解（NMD），在多种模型和损失函数上验证，在真实数据集体现适用性等。


<details>
  <summary>Details</summary>
Motivation: 解决非线性矩阵分解问题。

Method: 提出基于交替方向乘子法（ADMM）的算法。

Result: 在多种代表性非线性模型和损失函数上评估方法，在真实数据集上体现适用性、效率和适应性。

Conclusion: 该方法潜力大，适用于广泛应用场景。

Abstract: We present an algorithm based on the alternating direction method of multipliers (ADMM) for solving nonlinear matrix decompositions (NMD). Given an input matrix $X \in \mathbb{R}^{m \times n}$ and a factorization rank $r \ll \min(m, n)$, NMD seeks matrices $W \in \mathbb{R}^{m \times r}$ and $H \in \mathbb{R}^{r \times n}$ such that $X \approx f(WH)$, where $f$ is an element-wise nonlinear function. We evaluate our method on several representative nonlinear models: the rectified linear unit activation $f(x) = \max(0, x)$, suitable for nonnegative sparse data approximation, the component-wise square $f(x) = x^2$, applicable to probabilistic circuit representation, and the MinMax transform $f(x) = \min(b, \max(a, x))$, relevant for recommender systems. The proposed framework flexibly supports diverse loss functions, including least squares, $\ell_1$ norm, and the Kullback-Leibler divergence, and can be readily extended to other nonlinearities and metrics. We illustrate the applicability, efficiency, and adaptability of the approach on real-world datasets, highlighting its potential for a broad range of applications.

</details>


### [163] [Graph Attention Networks for Detecting Epilepsy from EEG Signals Using Accessible Hardware in Low-Resource Settings](https://arxiv.org/abs/2507.15118)
*Szymon Mazurek,Stephen Moore,Alessandro Crimi*

Main category: eess.SP

TL;DR: 提出基于图的深度学习框架用低成本EEG硬件检测癫痫，在尼日利亚和几内亚比绍的记录上测试，性能良好，为欠发达地区癫痫诊断提供支持。


<details>
  <summary>Details</summary>
Motivation: 低收入国家因神经科医生稀缺和诊断工具昂贵，癫痫诊断不足，需要公平、可及的自动评估和可解释性方法来检测癫痫。

Method: 将EEG信号建模为时空图，用图注意力网络（GAT）分类并识别通道间关系和时间动态，调整GAT分析边，设计低质量记录的信号预处理和轻量级GAT架构并部署在树莓派设备上。

Result: 该方法分类性能良好，在准确性和跨多个会话的鲁棒性上优于基于随机森林和图卷积网络的标准分类器，还突出了额颞叶区域的特定连接。

Conclusion: GAT有潜力为服务不足地区的癫痫诊断提供有洞察力和可扩展的支持，为经济实惠且可及的神经诊断工具铺平道路。

Abstract: Goal: Epilepsy remains under-diagnosed in low-income countries due to scarce neurologists and costly diagnostic tools. We propose a graph-based deep learning framework to detect epilepsy from low-cost Electroencephalography (EEG) hardware, tested on recordings from Nigeria and Guinea-Bissau. Our focus is on fair, accessible automatic assessment and explainability to shed light on epilepsy biomarkers. Methods: We model EEG signals as spatio-temporal graphs, classify them, and identify interchannel relationships and temporal dynamics using graph attention networks (GAT). To emphasize connectivity biomarkers, we adapt the inherently node-focused GAT to analyze edges. We also designed signal preprocessing for low-fidelity recordings and a lightweight GAT architecture trained on Google Colab and deployed on RaspberryPi devices. Results: The approach achieves promising classification performance, outperforming a standard classifier based on random forest and graph convolutional networks in terms of accuracy and robustness over multiple sessions, but also highlighting specific connections in the fronto-temporal region. Conclusions: The results highlight the potential of GATs to provide insightful and scalable diagnostic support for epilepsy in underserved regions, paving the way for affordable and accessible neurodiagnostic tools.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [164] [Privacy-Preserving Synthetic Dataset of Individual Daily Trajectories for City-Scale Mobility Analytics](https://arxiv.org/abs/2512.17239)
*Jun'ichi Ozaki,Ryosuke Susuta,Takuhiro Moriyama,Yohei Shida*

Main category: cs.SI

TL;DR: 提出一种隐私保护合成移动数据集，从聚合输入中重建日常轨迹，在日本两地验证有效，为多领域提供高分辨率移动数据分析途径。


<details>
  <summary>Details</summary>
Motivation: 城市移动数据应用广泛，但个体数据因隐私难共享，聚合记录无法捕捉人类日常移动关键行为特性，限制分析。

Method: 提出的方法将OD流与停留 - 旅行时间分位数和每日访问地点数量分布的普遍规律两个行为约束相结合，嵌入多目标优化框架。

Result: 在日本两个不同地区验证，合成移动数据能高精度重现停留 - 旅行时间和访问频率分布，OD一致性偏差在日常波动自然范围内。

Conclusion: 研究建立了现实约束下的实用合成途径，让多领域无需敏感个人记录就能获取高分辨率移动数据进行可靠分析。

Abstract: Urban mobility data are indispensable for urban planning, transportation demand forecasting, pandemic modeling, and many other applications; however, individual mobile phone-derived Global Positioning System traces cannot generally be shared with third parties owing to severe re-identification risks. Aggregated records, such as origin-destination (OD) matrices, offer partial insights but fail to capture the key behavioral properties of daily human movement, limiting realistic city-scale analyses.
  This study presents a privacy-preserving synthetic mobility dataset that reconstructs daily trajectories from aggregated inputs. The proposed method integrates OD flows with two complementary behavioral constraints: (1) dwell-travel time quantiles that are available only as coarse summary statistics and (2) the universal law for the daily distribution of the number of visited locations. Embedding these elements in a multi-objective optimization framework enables the reproduction of realistic distributions of human mobility while ensuring that no personal identifiers are required.
  The proposed framework is validated in two contrasting regions of Japan: (1) the 23 special wards of Tokyo, representing a dense metropolitan environment; and (2) Fukuoka Prefecture, where urban and suburban mobility patterns coexist. The resulting synthetic mobility data reproduce dwell-travel time and visit frequency distributions with high fidelity, while deviations in OD consistency remain within the natural range of daily fluctuations.
  The results of this study establish a practical synthesis pathway under real-world constraints, providing governments, urban planners, and industries with scalable access to high-resolution mobility data for reliable analytics without the need for sensitive personal records, and supporting practical deployments in policy and commercial domains.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [165] [Linear Attention for Joint Power Optimization and User-Centric Clustering in Cell-Free Networks](https://arxiv.org/abs/2512.17466)
*Irched Chafaa,Giacomo Bacci,Luca Sanguinetti*

Main category: eess.SY

TL;DR: 提出轻量级变压器模型用于用户中心无蜂窝大规模MIMO系统的AP聚类和功率分配，解决现有模型不足，数值结果验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型处理动态网络配置缺乏灵活性，许多方法忽视导频污染且计算复杂度高。

Method: 提出轻量级变压器模型，仅根据用户设备和AP的空间坐标联合预测AP集群和功率，采用定制线性注意力机制。

Result: 模型能在动态场景中最大化最小频谱效率，提供接近最优的性能。

Conclusion: 该模型具有适应性和可扩展性，能有效解决现有模型的局限性。

Abstract: Optimal AP clustering and power allocation are critical in user-centric cell-free massive MIMO systems. Existing deep learning models lack flexibility to handle dynamic network configurations. Furthermore, many approaches overlook pilot contamination and suffer from high computational complexity. In this paper, we propose a lightweight transformer model that overcomes these limitations by jointly predicting AP clusters and powers solely from spatial coordinates of user devices and AP. Our model is architecture-agnostic to users load, handles both clustering and power allocation without channel estimation overhead, and eliminates pilot contamination by assigning users to AP within a pilot reuse constraint. We also incorporate a customized linear attention mechanism to capture user-AP interactions efficiently and enable linear scalability with respect to the number of users. Numerical results confirm the model's effectiveness in maximizing the minimum spectral efficiency and providing near-optimal performance while ensuring adaptability and scalability in dynamic scenarios.

</details>


### [166] [Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy](https://arxiv.org/abs/2512.17899)
*Aditya Gahlawat,Ahmed Aboudonia,Sandeep Banik,Naira Hovakimyan,Nikolai Matni,Aaron D. Ames,Gioele Zardini,Alberto Speranzon*

Main category: eess.SY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Imitation learning (IL) enables autonomous behavior by learning from expert demonstrations. While more sample-efficient than comparative alternatives like reinforcement learning, IL is sensitive to compounding errors induced by distribution shifts. There are two significant sources of distribution shifts when using IL-based feedback laws on systems: distribution shifts caused by policy error and distribution shifts due to exogenous disturbances and endogenous model errors due to lack of learning. Our previously developed approaches, Taylor Series Imitation Learning (TaSIL) and $\mathcal{L}_1$ -Distributionally Robust Adaptive Control (\ellonedrac), address the challenge of distribution shifts in complementary ways. While TaSIL offers robustness against policy error-induced distribution shifts, \ellonedrac offers robustness against distribution shifts due to aleatoric and epistemic uncertainties. To enable certifiable IL for learned and/or uncertain dynamical systems, we formulate \textit{Distributionally Robust Imitation Policy (DRIP)} architecture, a Layered Control Architecture (LCA) that integrates TaSIL and~\ellonedrac. By judiciously designing individual layer-centric input and output requirements, we show how we can guarantee certificates for the entire control pipeline. Our solution paves the path for designing fully certifiable autonomy pipelines, by integrating learning-based components, such as perception, with certifiable model-based decision-making through the proposed LCA approach.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [167] [Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors](https://arxiv.org/abs/2512.17180)
*Maher Mesto,Francisco Cruz*

Main category: cs.RO

TL;DR: 论文揭示交互式强化学习中，学习智能体选择教师时倾向保守低奖励教师，有相关选择规律和性能阈值，成果优于基线Q学习，挑战强化学习最优教学假设。


<details>
  <summary>Details</summary>
Motivation: 交互式强化学习中教师选择的动态机制尚不明确，需要深入研究。

Method: 在导航任务中进行1250次有多位专家教师的实验。

Result: 智能体存在保守偏差选择低奖励教师；教师可用性rho >= 0.6和准确率omega >= 0.6为关键性能阈值；在概念漂移下比基线Q学习提升159%。

Conclusion: 挑战强化学习最优教学的基本假设，对人机协作和安全关键型机器人应用的训练范式有潜在启示。

Abstract: Interactive reinforcement learning (IRL) has shown promise in enabling autonomous agents and robots to learn complex behaviours from human teachers, yet the dynamics of teacher selection remain poorly understood. This paper reveals an unexpected phenomenon in IRL: when given a choice between teachers with different reward structures, learning agents overwhelmingly prefer conservative, low-reward teachers (93.16% selection rate) over those offering 20x higher rewards. Through 1,250 experimental runs in navigation tasks with multiple expert teachers, we discovered: (1) Conservative bias dominates teacher selection: agents systematically choose the lowest-reward teacher, prioritising consistency over optimality; (2) Critical performance thresholds exist at teacher availability rho >= 0.6 and accuracy omega >= 0.6, below which the framework fails catastrophically; (3) The framework achieves 159% improvement over baseline Q-learning under concept drift. These findings challenge fundamental assumptions about optimal teaching in RL and suggest potential implications for human-robot collaboration, where human preferences for safety and consistency may align with the observed agent selection behaviour, potentially informing training paradigms for safety-critical robotic applications.

</details>


### [168] [Research on Dead Reckoning Algorithm for Self-Propelled Pipeline Robots in Three-Dimensional Complex Pipelines](https://arxiv.org/abs/2512.17215)
*Yan Gao,Jiliang Wang,Minghan Wang,Xiaohua Chen,Demin Chen,Zhiyong Ren,Tian-Yun Huang*

Main category: cs.RO

TL;DR: 针对现有管道定位方法在复杂弯曲管道场景的不足，设计自推进管道机器人，提出基于扩展卡尔曼滤波的定位方法，实验验证了航位推算算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有管道定位方法在复杂弯曲管道场景易因电缆缠绕、设备灵活性不足而失效，传统管道测绘技术受管道环境因素影响大，需改进定位方法。

Method: 设计自推进管道机器人，利用惯性测量单元获取初始姿态角，用扩展卡尔曼滤波算法提高姿态角估计精度，结合轮式里程计实现高精度定位，同时平衡机器人运动能力和定位精度。

Result: 在矩形环形管道中使用自推进管道机器人进行实验，验证了提出的航位推算算法的有效性。

Conclusion: 提出的基于扩展卡尔曼滤波的管道机器人定位方法有效，能解决复杂弯曲管道的定位问题。

Abstract: In the field of gas pipeline location, existing pipeline location methods mostly rely on pipeline location instruments. However, when faced with complex and curved pipeline scenarios, these methods often fail due to problems such as cable entanglement and insufficient equipment flexibility. To address this pain point, we designed a self-propelled pipeline robot. This robot can autonomously complete the location work of complex and curved pipelines in complex pipe networks without external dragging. In terms of pipeline mapping technology, traditional visual mapping and laser mapping methods are easily affected by lighting conditions and insufficient features in the confined space of pipelines, resulting in mapping drift and divergence problems. In contrast, the pipeline location method that integrates inertial navigation and wheel odometers is less affected by pipeline environmental factors. Based on this, this paper proposes a pipeline robot location method based on extended Kalman filtering (EKF). Firstly, the body attitude angle is initially obtained through an inertial measurement unit (IMU). Then, the extended Kalman filtering algorithm is used to improve the accuracy of attitude angle estimation. Finally, high-precision pipeline location is achieved by combining wheel odometers. During the testing phase, the roll wheels of the pipeline robot needed to fit tightly against the pipe wall to reduce slippage. However, excessive tightness would reduce the flexibility of motion control due to excessive friction. Therefore, a balance needed to be struck between the robot's motion capability and positioning accuracy. Experiments were conducted using the self-propelled pipeline robot in a rectangular loop pipeline, and the results verified the effectiveness of the proposed dead reckoning algorithm.

</details>


### [169] [TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data](https://arxiv.org/abs/2512.17370)
*Deqing Liu,Yinfeng Gao,Deheng Qian,Qichao Zhang,Xiaoqing Ye,Junyu Han,Yupeng Zheng,Xueyi Liu,Zhongpu Xia,Dawei Ding,Yifeng Pan,Dongbin Zhao*

Main category: cs.RO

TL;DR: 提出TakeAD框架，用脱离场景的专家接管数据优化预训练模仿学习策略，提升闭环驾驶性能，并通过实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶模仿学习方法存在开环训练和闭环部署不一致问题，利用脱离场景的专家接管数据提升策略能力是未被充分探索的挑战。

Method: 设计高效的专家接管数据收集流程，将迭代的DAgger模仿学习与DPO偏好对齐结合进行后优化。

Result: 在Bench2Drive基准上的实验表明，比纯模仿学习方法更有效，消融实验证实各组件的贡献。

Conclusion: TakeAD框架通过处理脱离数据，可缓解开环闭环差距，提升闭环驾驶性能。

Abstract: Existing end-to-end autonomous driving methods typically rely on imitation learning (IL) but face a key challenge: the misalignment between open-loop training and closed-loop deployment. This misalignment often triggers driver-initiated takeovers and system disengagements during closed-loop execution. How to leverage those expert takeover data from disengagement scenarios and effectively expand the IL policy's capability presents a valuable yet unexplored challenge. In this paper, we propose TakeAD, a novel preference-based post-optimization framework that fine-tunes the pre-trained IL policy with this disengagement data to enhance the closed-loop driving performance. First, we design an efficient expert takeover data collection pipeline inspired by human takeover mechanisms in real-world autonomous driving systems. Then, this post optimization framework integrates iterative Dataset Aggregation (DAgger) for imitation learning with Direct Preference Optimization (DPO) for preference alignment. The DAgger stage equips the policy with fundamental capabilities to handle disengagement states through direct imitation of expert interventions. Subsequently, the DPO stage refines the policy's behavior to better align with expert preferences in disengagement scenarios. Through multiple iterations, the policy progressively learns recovery strategies for disengagement states, thereby mitigating the open-loop gap. Experiments on the closed-loop Bench2Drive benchmark demonstrate our method's effectiveness compared with pure IL methods, with comprehensive ablations confirming the contribution of each component.

</details>


### [170] [Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes](https://arxiv.org/abs/2512.17846)
*Carlos Vélez García,Miguel Cazorla,Jorge Pomares*

Main category: cs.RO

TL;DR: 提出Planning as Descent (PaD)框架用于离线目标条件强化学习，在验证基础上合成轨迹，在OGBench任务上表现优异，表明评估和优化轨迹是离线无奖励规划的可靠方法。


<details>
  <summary>Details</summary>
Motivation: 解决离线目标条件强化学习中常见的训练-测试不匹配问题，寻找直接策略学习之外的可靠方法。

Method: 学习目标条件能量函数，基于梯度在能量景观中优化轨迹，通过自监督后视目标重标记训练，推理时在不同时间假设下优化多个轨迹候选并选择低能量计划。

Result: 在OGBench立方体操作任务中，使用狭窄专家演示训练时成功率达95%，优于先前方法；使用嘈杂、次优数据训练可进一步提高成功率和计划效率。

Conclusion: 学习评估和优化轨迹为离线无奖励规划提供了比直接策略学习更可靠的替代方案。

Abstract: We present Planning as Descent (PaD), a framework for offline goal-conditioned reinforcement learning that grounds trajectory synthesis in verification. Instead of learning a policy or explicit planner, PaD learns a goal-conditioned energy function over entire latent trajectories, assigning low energy to feasible, goal-consistent futures. Planning is realized as gradient-based refinement in this energy landscape, using identical computation during training and inference to reduce train-test mismatch common in decoupled modeling pipelines.
  PaD is trained via self-supervised hindsight goal relabeling, shaping the energy landscape around the planning dynamics. At inference, multiple trajectory candidates are refined under different temporal hypotheses, and low-energy plans balancing feasibility and efficiency are selected.
  We evaluate PaD on OGBench cube manipulation tasks. When trained on narrow expert demonstrations, PaD achieves state-of-the-art 95\% success, strongly outperforming prior methods that peak at 68\%. Remarkably, training on noisy, suboptimal data further improves success and plan efficiency, highlighting the benefits of verification-driven planning. Our results suggest learning to evaluate and refine trajectories provides a robust alternative to direct policy learning for offline, reward-free planning.

</details>


### [171] [AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning](https://arxiv.org/abs/2512.17853)
*Ran Gong,Xiaohan Zhang,Jinghuan Shang,Maria Vittoria Minniti,Jigarkumar Patel,Valerio Pepe,Riedana Yan,Ahmet Gundogdu,Ivan Kapelyukh,Ali Abbas,Xiaoqiang Yan,Harsh Patel,Laura Herlant,Karl Schmeckpeper*

Main category: cs.RO

TL;DR: 提出AnyTask自动化框架，结合GPU模拟与基础模型设计操作任务和合成机器人数据，训练策略在真实任务达44%成功率。


<details>
  <summary>Details</summary>
Motivation: 通用机器人学习受数据限制，模拟相关任务需大量人力。

Method: 提出AnyTask框架，引入ViPR、ViPR - Eureka、ViPR - RL三个代理生成专家演示，训练行为克隆策略。

Result: 训练的策略在模拟中验证后部署到真实机器人，在多种真实任务中平均成功率达44%。

Conclusion: AnyTask框架能有效设计多样操作任务、合成数据，训练的策略可泛化到新场景。

Abstract: Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .

</details>


### [172] [Vidarc: Embodied Video Diffusion Model for Closed-loop Control](https://arxiv.org/abs/2512.17661)
*Yao Feng,Chendong Xiang,Xinyi Mao,Hengkai Tan,Zuyue Zhang,Shuhe Huang,Kaiwen Zheng,Haitian Liu,Hang Su,Jun Zhu*

Main category: cs.RO

TL;DR: 提出Vidarc方法，即基于动作推理和闭环控制的视频扩散方法，实现快速准确闭环控制，效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺场景下机械臂操作有挑战性，现有基于视频的方法不适用于特定具身闭环控制，存在高延迟和依据不足问题。

Method: 提出Vidarc，结合掩码逆动力学模型的自回归具身视频扩散方法，利用与动作相关的掩码进行视频预测，通过缓存自回归生成纳入实时反馈。

Result: 在一百万跨具身情节上预训练，在真实世界部署中成功率比现有基线至少高15%，延迟降低91%。

Conclusion: Vidarc方法能实现快速准确的闭环控制，在未见机器人平台上有强大的泛化和纠错能力。

Abstract: Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [173] [Machine Learning Assisted Parameter Tuning on Wavelet Transform Amorphous Radial Distribution Function](https://arxiv.org/abs/2512.17245)
*Deriyan Senjaya,Stephen Ekaputra Limantoro*

Main category: cond-mat.mtrl-sci

TL;DR: 本文介绍用机器学习优化WT - RDF参数得到WT - RDF+框架，用于非晶材料结构表征。


<details>
  <summary>Details</summary>
Motivation: WT - RDF在振幅精度上有局限，影响定量分析，需改进。

Method: 用机器学习方法优化WT - RDF参数得到WT - RDF+框架。

Result: WT - RDF+提高了峰值预测精度，优于基准ML模型，用25%二元数据集训练效果也很好。

Conclusion: WT - RDF+是用于非晶材料尤其是Ge - Se体系结构表征的可靠模型，支持下一代电子设备相变薄膜的高效设计和开发。

Abstract: Understanding atomic structures is crucial, yet amorphous materials remain challenging due to their irregular and non-periodic nature. The wavelet-transform radial distribution function (WT-RDF) offers a physics-based framework for analyzing amorphous structures, reliably predicting the first and second RDF peaks and overall curve trends in both binary Ge 0.25 Se 0.75 and ternary Ag x(Ge 0.25 Se 0.75)100-x (x=5,10,15,20,25) systems. Despite these strengths, WT-RDF shows limitations in amplitude accuracy, which affects quantitative analyses such as coordination numbers. This study addresses the issue by optimizing WT-RDF parameters using a machine learning approach, producing the enhanced WT-RDF+ framework. WT-RDF+ improves the precision of peak predictions and outperforms benchmark ML models, including RBF and LSTM, even when trained on only 25 percent of the binary dataset. These results demonstrate that WT-RDF+ is a robust and reliable model for structural characterization of amorphous materials, particularly Ge-Se systems, and support the efficient design and development of phase-change thin films for next-generation electronic devices and components.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [174] [Robust TTS Training via Self-Purifying Flow Matching for the WildSpoof 2026 TTS Track](https://arxiv.org/abs/2512.17293)
*June Young Yi,Hyeongju Kim,Juheon Lee*

Main category: cs.SD

TL;DR: 本文为WildSpoof Challenge TTS Track开发轻量级TTS系统，微调Supertonic模型结合SPFM，取得良好结果。


<details>
  <summary>Details</summary>
Motivation: 开发适用于WildSpoof Challenge TTS Track的轻量级TTS系统，使其能适应真实场景语音。

Method: 微调Supertonic模型，结合Self - Purifying Flow Matching (SPFM)处理标签噪声。

Result: 模型在所有参赛团队中单词错误率最低，在感知指标中排名第二。

Conclusion: 像Supertonic这样的高效开放权重架构结合SPFM等显式噪声处理机制，能有效适应多样的真实语音条件。

Abstract: This paper presents a lightweight text-to-speech (TTS) system developed for the WildSpoof Challenge TTS Track. Our approach fine-tunes the recently released open-weight TTS model, \textit{Supertonic}\footnote{\url{https://github.com/supertone-inc/supertonic}}, with Self-Purifying Flow Matching (SPFM) to enable robust adaptation to in-the-wild speech. SPFM mitigates label noise by comparing conditional and unconditional flow matching losses on each sample, routing suspicious text--speech pairs to unconditional training while still leveraging their acoustic information. The resulting model achieves the lowest Word Error Rate (WER) among all participating teams, while ranking second in perceptual metrics such as UTMOS and DNSMOS. These findings demonstrate that efficient, open-weight architectures like Supertonic can be effectively adapted to diverse real-world speech conditions when combined with explicit noise-handling mechanisms such as SPFM.

</details>


### [175] [When De-noising Hurts: A Systematic Study of Speech Enhancement Effects on Modern Medical ASR Systems](https://arxiv.org/abs/2512.17562)
*Sujal Chondhekar,Vasanth Murukuri,Rushabh Vasani,Sanika Goyal,Rajshree Badami,Anushree Rana,Sanjana SN,Karthik Pandia,Sulabh Katiyar,Neha Jagadeesh,Sankalp Gulati*

Main category: cs.SD

TL;DR: 对MetricGAN - plus - voicebank降噪在四个最新ASR系统上评估，发现语音增强预处理会降低ASR性能，现代ASR模型有足够内部噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 验证在现代大规模、多样嘈杂数据训练的ASR模型中，语音增强技术对ASR性能提升的有效性。

Method: 用500条医学语音记录在9种噪声条件下对四个ASR系统进行MetricGAN - plus - voicebank降噪评估，用semWER衡量ASR性能。

Result: 语音增强预处理在所有噪声条件和模型中降低了ASR性能，原始嘈杂音频的semWER更低，退化幅度为绝对semWER增加1.1%到46.6%。

Conclusion: 现代ASR模型有足够内部噪声鲁棒性，传统语音增强可能去掉对ASR关键的声学特征，医疗领域使用降噪预处理可能浪费计算资源且损害转录准确性。

Abstract: Speech enhancement methods are commonly believed to improve the performance of automatic speech recognition (ASR) in noisy environments. However, the effectiveness of these techniques cannot be taken for granted in the case of modern large-scale ASR models trained on diverse, noisy data. We present a systematic evaluation of MetricGAN-plus-voicebank denoising on four state-of-the-art ASR systems: OpenAI Whisper, NVIDIA Parakeet, Google Gemini Flash 2.0, Parrotlet-a using 500 medical speech recordings under nine noise conditions. ASR performance is measured using semantic WER (semWER), a normalized word error rate (WER) metric accounting for domain-specific normalizations. Our results reveal a counterintuitive finding: speech enhancement preprocessing degrades ASR performance across all noise conditions and models. Original noisy audio achieves lower semWER than enhanced audio in all 40 tested configurations (4 models x 10 conditions), with degradations ranging from 1.1% to 46.6% absolute semWER increase. These findings suggest that modern ASR models possess sufficient internal noise robustness and that traditional speech enhancement may remove acoustic features critical for ASR. For practitioners deploying medical scribe systems in noisy clinical environments, our results indicate that preprocessing audio with noise reduction techniques might not just be computationally wasteful but also be potentially harmful to the transcription accuracy.

</details>


### [176] [Do Foundational Audio Encoders Understand Music Structure?](https://arxiv.org/abs/2512.17209)
*Keisuke Toyama,Zhi Zhong,Akira Takahashi,Shusuke Takahashi,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: 研究对11种预训练基础音频编码器（FAEs）进行实验，探究其对音乐结构分析（MSA）性能的影响，发现特定学习方式的FAEs对MSA特别有效。


<details>
  <summary>Details</summary>
Motivation: 预训练FAEs在音乐信息检索任务中表现良好，但在MSA方面的应用研究不足，许多影响MSA性能的因素尚不明确。

Method: 对11种FAEs进行全面实验，研究学习方法、训练数据和模型上下文长度等因素对MSA性能的影响。

Result: 使用基于音乐数据的掩码语言建模的自监督学习的FAEs对MSA特别有效。

Conclusion: 研究结果为MSA的未来研究铺平了道路。

Abstract: In music information retrieval (MIR) research, the use of pretrained foundational audio encoders (FAEs) has recently become a trend. FAEs pretrained on large amounts of music and audio data have been shown to improve performance on MIR tasks such as music tagging and automatic music transcription. However, their use for music structure analysis (MSA) remains underexplored. Although many open-source FAE models are available, only a small subset has been examined for MSA, and the impact of factors such as learning methods, training data, and model context length on MSA performance remains unclear. In this study, we conduct comprehensive experiments on 11 types of FAEs to investigate how these factors affect MSA performance. Our results demonstrate that FAEs using selfsupervised learning with masked language modeling on music data are particularly effective for MSA. These findings pave the way for future research in MSA.

</details>


### [177] [LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection](https://arxiv.org/abs/2512.17281)
*Ioannis Stylianou,Achintya kr. Sarkar,Nauman Dawalatabad,James Glass,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: 本文介绍LibriVAD数据集，用多种特征模型组合进行VAD基准测试，引入ViT架构，实验显示ViT+MFCC表现优，还分析数据集规模和SSR影响并公开资源。


<details>
  <summary>Details</summary>
Motivation: 语音活动检测（VAD）在复杂声学条件下有挑战，且缺乏大规模、可控、公开数据集。

Method: 引入LibriVAD数据集，采用多种特征模型组合基准测试，引入ViT架构用于VAD，分析数据集大小和SSR对模型泛化的影响。

Result: ViT+MFCC特征始终优于现有VAD模型，扩大数据集规模和平衡SSR可显著提升OOD条件下的VAD性能。

Conclusion: 公开数据集、训练模型和代码，促进VAD研究的可重复性和进展。

Abstract: Robust Voice Activity Detection (VAD) remains a challenging task, especially under noisy, diverse, and unseen acoustic conditions. Beyond algorithmic development, a key limitation in advancing VAD research is the lack of large-scale, systematically controlled, and publicly available datasets. To address this, we introduce LibriVAD - a scalable open-source dataset derived from LibriSpeech and augmented with diverse real-world and synthetic noise sources. LibriVAD enables systematic control over speech-to-noise ratio, silence-to-speech ratio (SSR), and noise diversity, and is released in three sizes (15 GB, 150 GB, and 1.5 TB) with two variants (LibriVAD-NonConcat and LibriVAD-Concat) to support different experimental setups. We benchmark multiple feature-model combinations, including waveform, Mel-Frequency Cepstral Coefficients (MFCC), and Gammatone filter bank cepstral coefficients, and introduce the Vision Transformer (ViT) architecture for VAD. Our experiments show that ViT with MFCC features consistently outperforms established VAD models such as boosted deep neural network and convolutional long short-term memory deep neural network across seen, unseen, and out-of-distribution (OOD) conditions, including evaluation on the real-world VOiCES dataset. We further analyze the impact of dataset size and SSR on model generalization, experimentally showing that scaling up dataset size and balancing SSR noticeably and consistently enhance VAD performance under OOD conditions. All datasets, trained models, and code are publicly released to foster reproducibility and accelerate progress in VAD research.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [178] [Timely Information Updating for Mobile Devices Without and With ML Advice](https://arxiv.org/abs/2512.17381)
*Yu-Pin Hsu,Yi-Hsuan Tseng*

Main category: cs.NI

TL;DR: 本文研究移动设备信息更新系统，提出在线算法应对信息及时性与更新成本的权衡，还开发了结合机器学习建议的算法，证明最优竞争比特性并通过仿真验证。


<details>
  <summary>Details</summary>
Motivation: 解决移动设备信息更新系统中，接入点信息及时性与设备更新成本之间的权衡问题。

Method: 提出基于已有观测确定更新时机的在线算法，以及结合未知可靠性机器学习建议的增强算法。

Result: 在线算法渐近达到最优竞争比，ML增强算法渐近实现最优一致性 - 鲁棒性权衡，最优竞争比与更新成本范围线性相关。

Conclusion: 最优在线算法对ML建议呈阈值式响应，仿真验证了理论结果。

Abstract: This paper investigates an information update system in which a mobile device monitors a physical process and sends status updates to an access point (AP). A fundamental trade-off arises between the timeliness of the information maintained at the AP and the update cost incurred at the device. To address this trade-off, we propose an online algorithm that determines when to transmit updates using only available observations. The proposed algorithm asymptotically achieves the optimal competitive ratio against an adversary that can simultaneously manipulate multiple sources of uncertainty, including the operation duration, the information staleness, the update cost, and the availability of update opportunities. Furthermore, by incorporating machine learning (ML) advice of unknown reliability into the design, we develop an ML-augmented algorithm that asymptotically attains the optimal consistency-robustness trade-off, even when the adversary can additionally corrupt the ML advice. The optimal competitive ratio scales linearly with the range of update costs, but is unaffected by other uncertainties. Moreover, an optimal competitive online algorithm exhibits a threshold-like response to the ML advice: it either fully trusts or completely ignores the ML advice, as partially trusting the advice cannot improve the consistency without severely degrading the robustness. Extensive simulations in stochastic settings further validate the theoretical findings in the adversarial environment.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [179] [Exploring the Effect of Basis Rotation on NQS Performance](https://arxiv.org/abs/2512.17893)
*Sven Benjamin Kožić,Vinko Zlatić,Fabio Franchini,Salvatore Marco Giampaolo*

Main category: quant-ph

TL;DR: 本文用一维伊辛模型研究神经量子态（NQS）中基选择对优化的影响，揭示信息几何障碍，强调变分训练中需考虑损失景观的模型设计。


<details>
  <summary>Details</summary>
Motivation: NQS性能依赖基的选择，但潜在机制不明，需深入研究。

Method: 使用可精确求解的一维伊辛模型，通过扫描旋转角度计算量子费舍尔信息和富比尼 - 斯图迪距离，研究旋转波函数在损失景观中的移动。

Result: 浅架构（如RBM）用量子自然梯度训练时，依旋转角度易陷入鞍点区域，铁磁情况下近简并本征态会形成高曲率障碍。

Conclusion: 在固定损失景观中重新定位目标波函数会暴露阻碍浅NQS优化的信息几何障碍，变分训练需景观感知的模型设计。

Abstract: Neural Quantum States (NQS) use neural networks to represent wavefunctions of quantum many-body systems, but their performance depends on the choice of basis, yet the underlying mechanism remains poorly understood. We use a fully solvable one-dimensional Ising model to show that local basis rotations leave the loss landscape unchanged while relocating the exact wavefunction in parameter space, effectively increasing its geometric distance from typical initializations. By sweeping a rotation angle, we compute quantum Fisher information and Fubini-Study distances to quantify how the rotated wavefunction moves within the loss landscape. Shallow architectures (with focus on Restricted Boltzmann Machines (RBMs)) trained with quantum natural gradient are more likely to fall into saddle-point regions depending on the rotation angle: they achieve low energy error but fail to reproduce correct coefficient distributions. In the ferromagnetic case, near-degenerate eigenstates create high-curvature barriers that trap optimization at intermediate fidelities. We introduce a framework based on an analytically solvable rotated Ising model to investigate how relocating the target wavefunction within a fixed loss landscape exposes information-geometric barriers,such as saddle points and high-curvature regions,that hinder shallow NQS optimization, underscoring the need for landscape-aware model design in variational training.

</details>


### [180] [Fraud detection in credit card transactions using Quantum-Assisted Restricted Boltzmann Machines](https://arxiv.org/abs/2512.17660)
*João Marcos Cavalcanti de Albuquerque Neto,Gustavo Castro do Amaral,Guilherme Penello Temporão*

Main category: quant-ph

TL;DR: 评估量子计算辅助受限玻尔兹曼机（RBM）用于信用卡欺诈检测的性能，结果显示量子辅助方法优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算机处理效率和可用性提高，探索新兴量子计算平台在金融领域用例的经济价值。

Method: 使用巴西金融科技公司Stone提供的1.45亿笔交易真实数据集，在真实量子硬件和模拟器上运行量子计算辅助的受限玻尔兹曼机（RBM）。

Result: 量子辅助的RBM方法在大多指标上优于经典方法，即便使用当前有噪声的量子退火器。

Conclusion: 为在金融系统中实现量子辅助RBM进行一般故障检测铺平道路。

Abstract: Use cases for emerging quantum computing platforms become economically relevant as the efficiency of processing and availability of quantum computers increase. We assess the performance of Restricted Boltzmann Machines (RBM) assisted by quantum computing, running on real quantum hardware and simulators, using a real dataset containing 145 million transactions provided by Stone, a leading Brazilian fintech, for credit card fraud detection. The results suggest that the quantum-assisted RBM method is able to achieve superior performance in most figures of merit in comparison to classical approaches, even using current noisy quantum annealers. Our study paves the way for implementing quantum-assisted RBMs for general fault detection in financial systems.

</details>


### [181] [Domain-Aware Quantum Circuit for QML](https://arxiv.org/abs/2512.17800)
*Gurinder Singh,Thaddeus Pellegrini,Kenneth M. Merz,*

Main category: quant-ph

TL;DR: 提出领域感知量子电路DAQC用于量子机器学习图像分类，在多数据集上测试，在量子硬件上表现优。


<details>
  <summary>Details</summary>
Motivation: 在有噪中等规模量子设备上设计具有表达性、可训练性和抗硬件噪声能力的参数化量子电路是量子机器学习的核心挑战。

Method: 提出DAQC，利用图像先验信息，通过非重叠DCT风格的锯齿形窗口进行保局部性的编码和纠缠，采用编码 - 纠缠 - 训练交错循环。

Result: 在MNIST、FashionMNIST和PneumoniaMNIST数据集上测试，在量子硬件上表现与强经典基线相当，大幅超越量子电路搜索基线。

Conclusion: DAQC在基于量子机器学习的图像分类任务的真实量子硬件上取得目前最佳报告性能。

Abstract: Designing parameterized quantum circuits (PQCs) that are expressive, trainable, and robust to hardware noise is a central challenge for quantum machine learning (QML) on noisy intermediate-scale quantum (NISQ) devices. We present a Domain-Aware Quantum Circuit (DAQC) that leverages image priors to guide locality-preserving encoding and entanglement via non-overlapping DCT-style zigzag windows. The design employs interleaved encode-entangle-train cycles, where entanglement is applied among qubits hosting neighboring pixels, aligned to device connectivity. This staged, locality-preserving information flow expands the effective receptive field without deep global mixing, enabling efficient use of limited depth and qubits. The design concentrates representational capacity on short-range correlations, reduces long-range two-qubit operations, and encourages stable optimization, thereby mitigating depth-induced and globally entangled barren-plateau effects. We evaluate DAQC on MNIST, FashionMNIST, and PneumoniaMNIST datasets. On quantum hardware, DAQC achieves performance competitive with strong classical baselines (e.g., ResNet-18/50, DenseNet-121, EfficientNet-B0) and substantially outperforming Quantum Circuit Search (QCS) baselines. To the best of our knowledge, DAQC, which uses a quantum feature extractor with only a linear classical readout (no deep classical backbone), currently achieves the best reported performance on real quantum hardware for QML-based image classification tasks. Code and pretrained models are available at: https://github.com/gurinder-hub/DAQC.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [182] [HydroGym: A Reinforcement Learning Platform for Fluid Dynamics](https://arxiv.org/abs/2512.17534)
*Christian Lagemann,Sajeda Mokbel,Miro Gondrum,Mario Rüttgers,Jared Callaham,Ludger Paehler,Samuel Ahnert,Nicholas Zolman,Kai Lagemann,Nikolaus Adams,Matthias Meinke,Wolfgang Schröder,Jean-Christophe Loiseau,Esther Lagemann,Steven L. Brunton*

Main category: physics.flu-dyn

TL;DR: 论文介绍了用于流动控制研究的独立求解器强化学习平台HydroGym，包含多种环境和求解器，评估显示其能让代理发现控制原则，迁移学习表现良好，且平台可扩展。


<details>
  <summary>Details</summary>
Motivation: 流体控制面临诸多挑战，强化学习应用于流体控制缺乏标准化基准平台和受计算需求限制，需新平台解决这些问题。

Method: 引入HydroGym平台，集成复杂流动控制基准、可扩展运行时基础设施和先进强化学习算法，提供非可微和可微求解器。

Result: 强化学习代理能发现控制原则，迁移学习中控制器在新条件下训练所需周期约减少50%。

Conclusion: HydroGym平台可扩展、可升级，为多领域研究者提供推进科学技术的框架。

Abstract: Modeling and controlling fluid flows is critical for several fields of science and engineering, including transportation, energy, and medicine. Effective flow control can lead to, e.g., lift increase, drag reduction, mixing enhancement, and noise reduction. However, controlling a fluid faces several significant challenges, including high-dimensional, nonlinear, and multiscale interactions in space and time. Reinforcement learning (RL) has recently shown great success in complex domains, such as robotics and protein folding, but its application to flow control is hindered by a lack of standardized benchmark platforms and the computational demands of fluid simulations. To address these challenges, we introduce HydroGym, a solver-independent RL platform for flow control research. HydroGym integrates sophisticated flow control benchmarks, scalable runtime infrastructure, and state-of-the-art RL algorithms. Our platform includes 42 validated environments spanning from canonical laminar flows to complex three-dimensional turbulent scenarios, validated over a wide range of Reynolds numbers. We provide non-differentiable solvers for traditional RL and differentiable solvers that dramatically improve sample efficiency through gradient-enhanced optimization. Comprehensive evaluation reveals that RL agents consistently discover robust control principles across configurations, such as boundary layer manipulation, acoustic feedback disruption, and wake reorganization. Transfer learning studies demonstrate that controllers learned at one Reynolds number or geometry adapt efficiently to new conditions, requiring approximately 50% fewer training episodes. The HydroGym platform is highly extensible and scalable, providing a framework for researchers in fluid dynamics, machine learning, and control to add environments, surrogate models, and control algorithms to advance science and technology.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [183] [MAPPO-LCR: Multi-Agent Policy Optimization with Local Cooperation Reward in Spatial Public Goods Games](https://arxiv.org/abs/2512.17187)
*Zhaoqilin Yang,Axin Xiang,Kedi Yang,Tianjun Liu,Youliang Tian*

Main category: cs.MA

TL;DR: 本文首次将MAPPO引入空间公共品博弈，提出MAPPO - LCR方法，模拟显示能稳定促进合作且收敛可靠，统计分析证实MAPPO优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究在表示大规模交互群体中的收益耦合和非平稳性方面存在困难。

Method: 引入MAPPO解决PPO在价值估计时忽略个体收益耦合的问题，提出MAPPO - LCR研究邻域合作信号。

Result: 广泛模拟显示在不同增强因子下能稳定出现合作且可靠收敛，统计分析证实MAPPO学习优势。

Conclusion: MAPPO和MAPPO - LCR在空间公共品博弈中有良好表现，可有效促进合作。

Abstract: Spatial public goods games model collective dilemmas where individual payoffs depend on population-level strategy configurations. Most existing studies rely on evolutionary update rules or value-based reinforcement learning methods. These approaches struggle to represent payoff coupling and non-stationarity in large interacting populations. This work introduces Multi-Agent Proximal Policy Optimization (MAPPO) into spatial public goods games for the first time. In these games, individual returns are intrinsically coupled through overlapping group interactions. Proximal Policy Optimization (PPO) treats agents as independent learners and ignores this coupling during value estimation. MAPPO addresses this limitation through a centralized critic that evaluates joint strategy configurations. To study neighborhood-level cooperation signals under this framework, we propose MAPPO with Local Cooperation Reward, termed MAPPO-LCR. The local cooperation reward aligns policy updates with surrounding cooperative density without altering the original game structure. MAPPO-LCR preserves decentralized execution while enabling population-level value estimation during training. Extensive simulations demonstrate stable cooperation emergence and reliable convergence across enhancement factors. Statistical analyses further confirm the learning advantage of MAPPO over PPO in spatial public goods games.

</details>


### [184] [On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues](https://arxiv.org/abs/2512.17060)
*Monika Zamojska,Jarosław A. Chudziak*

Main category: cs.MA

TL;DR: 本文提出受交易分析理论启发的多智能体系统，经评估结果良好，为增强大语言模型（LLM）智能体行为真实性提供新方向。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体缺乏心理深度和一致性，无法捕捉人类真实思维模式，因此需要改进。

Method: 提出受交易分析理论启发的多智能体系统，将每个智能体分为三种自我状态，且具备信息检索机制，在模拟对话场景中进行消融测试。

Result: 评估结果有前景，为探索基于心理结构丰富智能体行为开辟新方向。

Conclusion: 提出一种结合交易分析理论和上下文信息检索的智能体架构，可增强基于LLM的多智能体模拟的真实性。

Abstract: LLM-powered agents are now used in many areas, from customer support to education, and there is increasing interest in their ability to act more like humans. This includes fields such as social, political, and psychological research, where the goal is to model group dynamics and social behavior. However, current LLM agents often lack the psychological depth and consistency needed to capture the real patterns of human thinking. They usually provide direct or statistically likely answers, but they miss the deeper goals, emotional conflicts, and motivations that drive real human interactions. This paper proposes a Multi-Agent System (MAS) inspired by Transactional Analysis (TA) theory. In the proposed system, each agent is divided into three ego states - Parent, Adult, and Child. The ego states are treated as separate knowledge structures with their own perspectives and reasoning styles. To enrich their response process, they have access to an information retrieval mechanism that allows them to retrieve relevant contextual information from their vector stores. This architecture is evaluated through ablation tests in a simulated dialogue scenario, comparing agents with and without information retrieval. The results are promising and open up new directions for exploring how psychologically grounded structures can enrich agent behavior. The contribution is an agent architecture that integrates Transactional Analysis theory with contextual information retrieval to enhance the realism of LLM-based multi-agent simulations.

</details>


### [185] [Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems](https://arxiv.org/abs/2512.17259)
*Abhivansh Gupta*

Main category: cs.MA

TL;DR: 提出可验证优先架构及OPERA基准套件，转变评估重点至快速可靠检测和修复偏差。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体自主性和多模态性增强，确保其可控、可审计及符合部署者意图至关重要，此前基准测试有相关发现。

Method: 提出可验证优先架构，包含运行时证明、嵌入审计智能体、实施挑战响应证明协议；引入OPERA基准套件和评估协议。

Result: 未明确提及具体结果。

Conclusion: 将评估重点从偏差发生可能性转移到偏差检测和修复的速度与可靠性。

Abstract: As LLM-based agents grow more autonomous and multi-modal, ensuring they remain controllable, auditable, and faithful to deployer intent becomes critical. Prior benchmarks measured the propensity for misaligned behavior and showed that agent personalities and tool access significantly influence misalignment. Building on these insights, we propose a Verifiability-First architecture that (1) integrates run-time attestations of agent actions using cryptographic and symbolic methods, (2) embeds lightweight Audit Agents that continuously verify intent versus behavior using constrained reasoning, and (3) enforces challenge-response attestation protocols for high-risk operations. We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time to detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt and persona injection. Our approach shifts the evaluation focus from how likely misalignment is to how quickly and reliably misalignment can be detected and remediated.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [186] [Application of machine learning to predict food processing level using Open Food Facts](https://arxiv.org/abs/2512.17169)
*Nalin Arora,Aviral Chauhan,Siddhant Rana,Mahansh Aditya,Sumit Bhagat,Aditya Kumar,Akash Kumar,Akanksh Semar,Ayush Vikram Singh,Ganesh Bagler*

Main category: q-bio.BM

TL;DR: 研究利用机器学习基于超90万产品的数据集对食品加工水平分类，LightGBM模型表现最佳，揭示食品加工在健康、环境和过敏方面的影响，并提供预测工具。


<details>
  <summary>Details</summary>
Motivation: 超加工食品因营养质量差与多种健康问题相关，需研究食品加工水平分类。

Method: 基于Open Food Facts数据集，用机器学习模型（LightGBM、Random Forest、CatBoost）对食品加工水平（NOVA）分类，以营养浓度数据训练模型。

Result: LightGBM模型表现最佳，准确率达80 - 85%；高NOVA等级与低Nutri - Scores相关，NOVA 3和4类产品碳足迹高、Eco - Scores低；超加工食品常见麸质和牛奶过敏原；高NOVA等级中蛋糕和零食等类别占主导，添加剂多。

Conclusion: 强调食品加工在健康、环境和过敏方面的影响，展示机器学习在可扩展分类中的价值。

Abstract: Ultra-processed foods are increasingly linked to health issues like obesity, cardiovascular disease, type 2 diabetes, and mental health disorders due to poor nutritional quality. This first-of-its-kind study at such a scale uses machine learning to classify food processing levels (NOVA) based on the Open Food Facts dataset of over 900,000 products. Models including LightGBM, Random Forest, and CatBoost were trained on nutrient concentration data. LightGBM performed best, achieving 80-85% accuracy across different nutrient panels and effectively distinguishing minimally from ultra-processed foods. Exploratory analysis revealed strong associations between higher NOVA classes and lower Nutri-Scores, indicating poorer nutritional quality. Products in NOVA 3 and 4 also had higher carbon footprints and lower Eco-Scores, suggesting greater environmental impact. Allergen analysis identified gluten and milk as common in ultra-processed items, posing risks to sensitive individuals. Categories like Cakes and Snacks were dominant in higher NOVA classes, which also had more additives, highlighting the role of ingredient modification. This study, leveraging the largest dataset of NOVA-labeled products, emphasizes the health, environmental, and allergenic implications of food processing and showcases machine learning's value in scalable classification. A user-friendly web tool is available for NOVA prediction using nutrient data: https://cosylab.iiitd.edu.in/foodlabel/.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [187] [Implementation of Augmented Reality as an Educational Tool for Practice in Early Childhood](https://arxiv.org/abs/2512.17354)
*Wisnu Uriawan,Muhammad Aditya Hafizh Zahran,Inayah Ayu Deswita,Muhammad Ahsani Taqwim,Ismail Muhammad Ahmadi,Marvi Yoga Pratama*

Main category: cs.HC

TL;DR: 本研究开发基于增强现实（AR）的小净学习应用，经测试功能良好且提升儿童学习兴趣和理解，证明AR技术用于儿童基础礼拜教学有效。


<details>
  <summary>Details</summary>
Motivation: 幼儿学习小净需有吸引力和互动性的媒体以加深对礼拜程序的理解，因此开发基于AR的学习应用。

Method: 采用需求分析、系统设计、实现和使用黑盒测试的开发方法，利用基于标记的跟踪技术在相机检测到印刷媒体上的标记时实时显示小净动作的3D动画。

Result: 所有主要功能运行良好，对5 - 7岁儿童的有限试验显示学习兴趣提高，对小净顺序的理解更好。

Conclusion: AR技术应用于幼儿基础礼拜教学能有效提高教学质量。

Abstract: Learning Wudhu for young children requires engaging and interactive media to foster a deep understanding of the worship procedures. This study aims to develop a Wudhu learning application based on Augmented Reality (AR) as an interactive and fun educational medium. The development method used includes the stages of needs analysis, system design, implementation, and testing using Black Box Testing. The system utilizes marker-based tracking to display 3D animations of Wudhu movements in real-time when the camera detects a marker on the printed media. The test results indicate that all main functions run well, and a limited trial on children aged 5-7 years showed an increase in learning interest and a better understanding of the Wudhu sequence. Thus, the application of AR technology is proven effective in improving the quality of basic worship instruction for young children.

</details>


### [188] [Bots Don't Sit Still: A Longitudinal Study of Bot Behaviour Change, Temporal Drift, and Feature-Structure Evolution](https://arxiv.org/abs/2512.17067)
*Ohoud Alzahrani,Russell Beale,Bob Hendley*

Main category: cs.HC

TL;DR: 研究检验推广型推特机器人行为是否随时间稳定，发现特征非平稳且特征间依赖变化，对检测系统设计评估有启示。


<details>
  <summary>Details</summary>
Motivation: 多数机器人检测系统假设机器人行为随时间平稳，本文检验推广型推特机器人该假设是否成立。

Method: 分析2615个推广型机器人账户和280万条推文，构建基于内容的元特征时间序列，用ADF和KPSS检验及线性趋势分析；分层分析不同代和不同寿命机器人；分析特征共现用卡方检验和斯皮尔曼相关分析。

Result: 十个元特征均非平稳，九个随时间增加，语言多样性略降；不同代和寿命机器人行为有差异；多数特征对存在依赖，斯皮尔曼相关性强度和极性有变化。

Conclusion: 推广型社交机器人在个体元特征和特征依赖层面随时间适应，影响基于历史行为特征的检测系统设计和评估。

Abstract: Social bots are now deeply embedded in online platforms for promotion, persuasion, and manipulation. Most bot-detection systems still treat behavioural features as static, implicitly assuming bots behave stationarily over time. We test that assumption for promotional Twitter bots, analysing change in both individual behavioural signals and the relationships between them. Using 2,615 promotional bot accounts and 2.8M tweets, we build yearly time series for ten content-based meta-features. Augmented Dickey-Fuller and KPSS tests plus linear trends show all ten are non-stationary: nine increase over time, while language diversity declines slightly.
  Stratifying by activation generation and account age reveals systematic differences: second-generation bots are most active and link-heavy; short-lived bots show intense, repetitive activity with heavy hashtag/URL use; long-lived bots are less active but more linguistically diverse and use emojis more variably. We then analyse co-occurrence across generations using 18 interpretable binary features spanning actions, topic similarity, URLs, hashtags, sentiment, emojis, and media (153 pairs). Chi-square tests indicate almost all pairs are dependent. Spearman correlations shift in strength and sometimes polarity: many links (e.g. multiple hashtags with media; sentiment with URLs) strengthen, while others flip from weakly positive to weakly or moderately negative. Later generations show more structured combinations of cues.
  Taken together, these studies provide evidence that promotional social bots adapt over time at both the level of individual meta-features and the level of feature interdependencies, with direct implications for the design and evaluation of bot-detection systems trained on historical behavioural features.

</details>


### [189] [PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric and Trustworthy Explanations for Daily Use Cases](https://arxiv.org/abs/2512.17172)
*Ripan Kumar Kundu,Istiak Ahmed,Khaza Anuarul Hoque*

Main category: cs.HC

TL;DR: 提出PILAR框架用于AI驱动的AR系统，以解决传统XAI方法问题，在AR食谱推荐应用中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统XAI方法难以在实时AR交互中提供动态、个性化解释，导致体验不佳，需新方法解决。

Method: 提出PILAR框架，利用预训练大语言模型生成上下文感知、个性化解释，在真实AR应用中实现该概念，并进行用户研究对比。

Result: LLM-based界面显著提高用户表现和体验，任务完成速度快40%，用户满意度、易用性和透明度感知更高。

Conclusion: PILAR框架能为实时AI驱动的AR系统提供更直观、可信的体验，增强用户信任和参与度。

Abstract: Artificial intelligence (AI)-driven augmented reality (AR) systems are becoming increasingly integrated into daily life, and with this growth comes a greater need for explainability in real-time user interactions. Traditional explainable AI (XAI) methods, which often rely on feature-based or example-based explanations, struggle to deliver dynamic, context-specific, personalized, and human-centric insights for everyday AR users. These methods typically address separate explainability dimensions (e.g., when, what, how) with different explanation techniques, resulting in unrealistic and fragmented experiences for seamless AR interactions. To address this challenge, we propose PILAR, a novel framework that leverages a pre-trained large language model (LLM) to generate context-aware, personalized explanations, offering a more intuitive and trustworthy experience in real-time AI-powered AR systems. Unlike traditional methods, which rely on multiple techniques for different aspects of explanation, PILAR employs a unified LLM-based approach that dynamically adapts explanations to the user's needs, fostering greater trust and engagement. We implement the PILAR concept in a real-world AR application (e.g., personalized recipe recommendations), an open-source prototype that integrates real-time object detection, recipe recommendation, and LLM-based personalized explanations of the recommended recipes based on users' dietary preferences. We evaluate the effectiveness of PILAR through a user study with 16 participants performing AR-based recipe recommendation tasks, comparing an LLM-based explanation interface to a traditional template-based one. Results show that the LLM-based interface significantly enhances user performance and experience, with participants completing tasks 40% faster and reporting greater satisfaction, ease of use, and perceived transparency.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [190] [Intelligent Knowledge Mining Framework: Bridging AI Analysis and Trustworthy Preservation](https://arxiv.org/abs/2512.17795)
*Binh Vu*

Main category: cs.DL

TL;DR: 本文介绍智能知识挖掘框架（IKMF），以解决数字数据利用难题，阐述其架构、研究动机、方法等。


<details>
  <summary>Details</summary>
Motivation: 数字数据激增，信息分散在不同系统和格式中，形成数据孤岛，阻碍高效利用和协作决策。

Method: 提出双流架构，包括水平挖掘流程和可信存档流，将原始数据转化为知识并确保资产完整性等。

Result: 给出了框架的研究动机、问题陈述、关键研究问题，呈现了科学方法、概念设计和建模。

Conclusion: 提供了将静态存储库转变为促进可操作情报流动的生态系统的基础模型。

Abstract: The unprecedented proliferation of digital data presents significant challenges in access, integration, and value creation across all data-intensive sectors. Valuable information is frequently encapsulated within disparate systems, unstructured documents, and heterogeneous formats, creating silos that impede efficient utilization and collaborative decision-making. This paper introduces the Intelligent Knowledge Mining Framework (IKMF), a comprehensive conceptual model designed to bridge the critical gap between dynamic AI-driven analysis and trustworthy long-term preservation. The framework proposes a dual-stream architecture: a horizontal Mining Process that systematically transforms raw data into semantically rich, machine-actionable knowledge, and a parallel Trustworthy Archiving Stream that ensures the integrity, provenance, and computational reproducibility of these assets. By defining a blueprint for this symbiotic relationship, the paper provides a foundational model for transforming static repositories into living ecosystems that facilitate the flow of actionable intelligence from producers to consumers. This paper outlines the motivation, problem statement, and key research questions guiding the research and development of the framework, presents the underlying scientific methodology, and details its conceptual design and modeling.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [191] [Asymptotic behaviour of galactic small-scale dynamos at modest magnetic Prandtl number](https://arxiv.org/abs/2512.17885)
*Frederick A. Gent,Mordecai-Mark Mac Low,Maarit J. Korpi-Lagg,Touko Puro,Matthias Reinhardt*

Main category: astro-ph.GA

TL;DR: 本文利用GPU加速代码对超新星驱动的星系发电机进行高分辨率模拟，发现小尺度发电机产生的湍动场强度在中等磁普朗特数时趋近渐近值，并可用于构建全局星系模型。


<details>
  <summary>Details</summary>
Motivation: 以往大-小尺度和小尺度发电机模拟得到的湍动场强度与观测不符，需要新的研究改进。

Method: 使用在GPU上加速的Pencil Code和Astaroth，在周期性区域对包含加热和冷却的超新星驱动星系发电机进行高分辨率模拟。

Result: 小尺度发电机产生的湍动场强度在磁普朗特数仅为几百时就趋近渐近值。

Conclusion: 可利用这些模型确定磁场成分的基本特征以纳入全局星系模型，该渐近极限与先前等温可压缩流的研究结果一致。

Abstract: Magnetic fields are critical at many scales to galactic dynamics and structure, including multiphase pressure balance, dust processing, and star formation. Dynamo action determines their dynamical structure and strength. Simulations of combined large- and small-scale dynamos have successfully developed mean fields with strength and topology consistent with observations but with turbulent fields much weaker than observed, while simulations of small-scale dynamos with parameters relevant to the interstellar medium yield turbulent fields an order of magnitude below the values observed or expected theoretically. We use the Pencil Code accelerated on GPUs with Astaroth to perform high-resolution simulations of a supernova-driven galactic dynamo including heating and cooling in a periodic domain. Our models show that the strength of the turbulent field produced by the small-scale dynamo approaches an asymptote at only modest magnetic Prandtl numbers. This allows us to use these models to suggest the essential characteristics of this constituent of the magnetic field for inclusion in global galactic models. The asymptotic limit occurs already at magnetic Prandtl number of only a few hundred, many orders of magnitude below physical values in the the interstellar medium and consistent with previous findings for isothermal compressible flows.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [192] [From Priors to Predictions: Explaining and Visualizing Human Reasoning in a Graph Neural Network Framework](https://arxiv.org/abs/2512.17255)
*Quan Do,Caroline Ahn,Leah Bakst,Michael Pascale,Joseph T. McGuire,Chantal E. Stern,Michael E. Hasselmo*

Main category: q-bio.NC

TL;DR: 提出结合图论和图神经网络的框架，用人类行为数据集验证图先验能解释个体差异，揭示泛化与先验结构关系，为建模泛化提供框架。


<details>
  <summary>Details</summary>
Motivation: 人类在归纳偏置引导下能从少量接触中解决新推理问题，但这些偏置的计算形式和神经实现尚不明确。

Method: 结合图论和图神经网络将归纳偏置形式化为结构和抽象的先验，有搜索图配置的优化流程和识别关键计算图的可视化方法。

Result: 图先验差异能解释人类解决方案的个体差异，系统性消融揭示泛化与先验结构和内部处理的关系。

Conclusion: 该工作为建模泛化提供有原则、可解释的框架，为理解人类推理和构建更类人的AI系统提供基础。

Abstract: Humans excel at solving novel reasoning problems from minimal exposure, guided by inductive biases, assumptions about which entities and relationships matter. Yet the computational form of these biases and their neural implementation remain poorly understood. We introduce a framework that combines Graph Theory and Graph Neural Networks (GNNs) to formalize inductive biases as explicit, manipulable priors over structure and abstraction. Using a human behavioral dataset adapted from the Abstraction and Reasoning Corpus (ARC), we show that differences in graph-based priors can explain individual differences in human solutions. Our method includes an optimization pipeline that searches over graph configurations, varying edge connectivity and node abstraction, and a visualization approach that identifies the computational graph, the subset of nodes and edges most critical to a model's prediction. Systematic ablation reveals how generalization depends on specific prior structures and internal processing, exposing why human like errors emerge from incorrect or incomplete priors. This work provides a principled, interpretable framework for modeling the representational assumptions and computational dynamics underlying generalization, offering new insights into human reasoning and a foundation for more human aligned AI systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [193] [Torrent: A Distributed DMA for Efficient and Flexible Point-to-Multipoint Data Movement](https://arxiv.org/abs/2512.17589)
*Yunhao Deng,Fanchen Kong,Xiaoling Yi,Ryan Antonio,Marian Verhelst*

Main category: cs.AR

TL;DR: 针对片上通信带宽瓶颈问题，本文提出一种分布式DMA架构Torrent，能在不修改硬件和协议下实现高效点对多点数据传输，经评估在性能、灵活性和可扩展性上有显著优势。


<details>
  <summary>Details</summary>
Motivation: 现代片上系统中计算能力和片上通信带宽差距增大成为瓶颈，现有多点数据传输方案存在增加硬件开销、需修改协议等问题，缺乏高效解决方案。

Method: 引入Torrent分布式DMA架构，通过在片上网络形成逻辑链进行点对多点数据传输（Chainwrite机制），并开发两个调度算法优化性能和能耗。

Result: 原型评估显示与网络层多播相比有显著优势，比单播基线最高实现7.88倍加速，ASIC合成表明面积和功耗占用小，每个目标点有小的周期和面积开销。

Conclusion: Torrent架构能在不修改硬件和协议情况下实现可扩展的高效点对多点数据传输。

Abstract: The growing disparity between computational power and on-chip communication bandwidth is a critical bottleneck in modern Systems-on-Chip (SoCs), especially for data-parallel workloads like AI. Efficient point-to-multipoint (P2MP) data movement, such as multicast, is essential for high performance. However, native multicast support is lacking in standard interconnect protocols. Existing P2MP solutions, such as multicast-capable Network-on-Chip (NoC), impose additional overhead to the network hardware and require modifications to the interconnect protocol, compromising scalability and compatibility.
  This paper introduces Torrent, a novel distributed DMA architecture that enables efficient P2MP data transfers without modifying NoC hardware and interconnect protocol. Torrent conducts P2MP data transfers by forming logical chains over the NoC, where the data traverses through targeted destinations resembling a linked list. This Chainwrite mechanism preserves the P2P nature of every data transfer while enabling flexible data transfers to an unlimited number of destinations. To optimize the performance and energy consumption of Chainwrite, two scheduling algorithms are developed to determine the optimal chain order based on NoC topology.
  Our RTL and FPGA prototype evaluations using both synthetic and real workloads demonstrate significant advantages in performance, flexibility, and scalability over network-layer multicast. Compared to the unicast baseline, Torrent achieves up to a 7.88x speedup. ASIC synthesis on 16nm technology confirms the architecture's minimal footprint in area (1.2%) and power (2.3%). Thanks to the Chainwrite, Torrent delivers scalable P2MP data transfers with a small cycle overhead of 82CC and area overhead of 207um2 per destination.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [194] [Learning vertical coordinates via automatic differentiation of a dynamical core](https://arxiv.org/abs/2512.17877)
*Tim Whittaker,Seth Taylor,Elsa Cardoso-Bihlo,Alejandro Di Luca,Alex Bihlo*

Main category: physics.ao-ph

TL;DR: 提出将参数化垂直坐标系作为可学习组件，用含积分变换神经网络的NEUVE坐标，利用自动微分消除截断误差，经测试可减少误差并消除伪垂直速度条纹。


<details>
  <summary>Details</summary>
Motivation: 大气模型中地形跟随坐标会在解上留下网格结构印记，标准公式需手动调整启发式尺度参数，本文旨在解决此问题。

Method: 提出将参数化垂直坐标系作为可学习组件，开发二维非静力欧拉方程的端到端可微数值求解器，引入基于积分变换神经网络的NEUVE地形跟随坐标，利用自动微分计算精确几何度量项。

Result: 在非线性统计基准测试中，学习到的坐标将均方误差降低1.4到2倍，消除了陡峭地形上的伪垂直速度条纹。

Conclusion: 该公式能找到同时优化基础物理和数值的网格结构。

Abstract: Terrain-following coordinates in atmospheric models often imprint their grid structure onto the solution, particularly over steep topography, where distorted coordinate layers can generate spurious horizontal and vertical motion. Standard formulations, such as hybrid or SLEVE coordinates, mitigate these errors by using analytic decay functions controlled by heuristic scale parameters that are typically tuned by hand and fixed a priori. In this work, we propose a framework to define a parametric vertical coordinate system as a learnable component within a differentiable dynamical core. We develop an end-to-end differentiable numerical solver for the two-dimensional non-hydrostatic Euler equations on an Arakawa C-grid, and introduce a NEUral Vertical Enhancement (NEUVE) terrain-following coordinate based on an integral transformed neural network that guarantees monotonicity. A key feature of our approach is the use of automatic differentiation to compute exact geometric metric terms, thereby eliminating truncation errors associated with finite-difference coordinate derivatives. By coupling simulation errors through the time integration to the parameterization, our formulation finds a grid structure optimized for both the underlying physics and numerics. Using several standard tests, we demonstrate that these learned coordinates reduce the mean squared error by a factor of 1.4 to 2 in non-linear statistical benchmarks, and eliminate spurious vertical velocity striations over steep topography.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [195] [Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease](https://arxiv.org/abs/2512.17340)
*Carter H. Nakamoto,Lucia Lushi Chen,Agata Foryciarz,Sherri Rose*

Main category: stat.ME

TL;DR: 提出多组不公平惩罚的回归框架，用于解决多组偏差问题，在模拟和实际研究中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有多组偏差下的惩罚公平回归研究较少，需提出方法缓解医疗中的社会偏差问题。

Method: 提出多组不公平惩罚的回归框架，用真阳性率差异惩罚处理二元结果，转化为成本敏感分类问题实现，引入新分数函数选惩罚权重。

Result: 模拟中达到优于现有方法的公平 - 准确性前沿；在慢性肾病研究中，多族裔群体公平性显著改善，整体拟合无明显损失。

Conclusion: 所提惩罚公平回归方法有效，可用于开发公平分类器，改善多组在医疗系统中的公平性。

Abstract: Fair regression methods have the potential to mitigate societal bias concerns in health care, but there has been little work on penalized fair regression when multiple groups experience such bias. We propose a general regression framework that addresses this gap with unfairness penalties for multiple groups. Our approach is demonstrated for binary outcomes with true positive rate disparity penalties. It can be efficiently implemented through reduction to a cost-sensitive classification problem. We additionally introduce novel score functions for automatically selecting penalty weights. Our penalized fair regression methods are empirically studied in simulations, where they achieve a fairness-accuracy frontier beyond that of existing comparison methods. Finally, we apply these methods to a national multi-site primary care study of chronic kidney disease to develop a fair classifier for end-stage renal disease. There we find substantial improvements in fairness for multiple race and ethnicity groups who experience societal bias in the health care system without any appreciable loss in overall fit.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [196] [Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL](https://arxiv.org/abs/2512.17053)
*Khushboo Thaker,Yony Bresler*

Main category: cs.CL

TL;DR: 企业级部署Text - to - SQL系统面临成本、安全和性能的困境，提出Struct - SQL框架用结构化推理训练小语言模型，效果提升。


<details>
  <summary>Details</summary>
Motivation: 企业部署Text - to - SQL系统时，现有方案需在昂贵的大语言模型和低性能小语言模型间选择，且提升小语言模型的非结构化思维链方法有歧义。

Method: 提出Struct - SQL知识蒸馏框架，采用查询执行计划作为正式蓝图来获取结构化推理，训练小语言模型模仿大语言模型。

Result: 用结构化思维链蒸馏的小语言模型比非结构化基线绝对提升8.1%，句法错误显著减少。

Conclusion: 使用结构化逻辑蓝图教模型推理对小语言模型可靠生成SQL有益。

Abstract: Deploying accurate Text-to-SQL systems at the enterprise level faces a difficult trilemma involving cost, security and performance. Current solutions force enterprises to choose between expensive, proprietary Large Language Models (LLMs) and low-performing Small Language Models (SLMs). Efforts to improve SLMs often rely on distilling reasoning from large LLMs using unstructured Chain-of-Thought (CoT) traces, a process that remains inherently ambiguous. Instead, we hypothesize that a formal, structured reasoning representation provides a clearer, more reliable teaching signal, as the Text-to-SQL task requires explicit and precise logical steps. To evaluate this hypothesis, we propose Struct-SQL, a novel Knowledge Distillation (KD) framework that trains an SLM to emulate a powerful large LLM. Consequently, we adopt a query execution plan as a formal blueprint to derive this structured reasoning. Our SLM, distilled with structured CoT, achieves an absolute improvement of 8.1% over an unstructured CoT distillation baseline. A detailed error analysis reveals that a key factor in this gain is a marked reduction in syntactic errors. This demonstrates that teaching a model to reason using a structured logical blueprint is beneficial for reliable SQL generation in SLMs.

</details>


### [197] [A Women's Health Benchmark for Large Language Models](https://arxiv.org/abs/2512.17028)
*Victoria-Elisabeth Gruber,Razvan Marinescu,Diego Fajardo,Amin H. Nassar,Christopher Arkfeld,Alexandria Ludlow,Shama Patel,Mehrnoosh Samaei,Valerie Klug,Anna Huber,Marcel Gühner,Albert Botta i Orfila,Irene Lagoja,Kimya Tarr,Haleigh Larson,Mary Beth Howard*

Main category: cs.CL

TL;DR: 引入女性健康基准（WHB）评估大语言模型在女性健康方面表现，发现当前模型失败率约60%，AI聊天机器人尚不能提供可靠建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型成为健康信息主要来源，但在女性健康方面的准确性未得到充分研究。

Method: 引入包含96个严格验证模型树桩的WHB，涵盖五个医学专业、三种查询类型和八种错误类型，评估13个先进大语言模型。

Result: 当前模型在WHB上失败率约60%，不同专业和错误类型表现差异大，普遍在“错过紧急情况”指标上有困难，GPT - 5在避免不恰当建议上有显著改进。

Conclusion: AI聊天机器人在女性健康领域还不能提供可靠建议。

Abstract: As large language models (LLMs) become primary sources of health information for millions, their accuracy in women's health remains critically unexamined. We introduce the Women's Health Benchmark (WHB), the first benchmark evaluating LLM performance specifically in women's health. Our benchmark comprises 96 rigorously validated model stumps covering five medical specialties (obstetrics and gynecology, emergency medicine, primary care, oncology, and neurology), three query types (patient query, clinician query, and evidence/policy query), and eight error types (dosage/medication errors, missing critical information, outdated guidelines/treatment recommendations, incorrect treatment advice, incorrect factual information, missing/incorrect differential diagnosis, missed urgency, and inappropriate recommendations). We evaluated 13 state-of-the-art LLMs and revealed alarming gaps: current models show approximately 60\% failure rates on the women's health benchmark, with performance varying dramatically across specialties and error types. Notably, models universally struggle with "missed urgency" indicators, while newer models like GPT-5 show significant improvements in avoiding inappropriate recommendations. Our findings underscore that AI chatbots are not yet fully able of providing reliable advice in women's health.

</details>


### [198] [When F1 Fails: Granularity-Aware Evaluation for Dialogue Topic Segmentation](https://arxiv.org/abs/2512.17083)
*Michael H. Coen*

Main category: cs.CL

TL;DR: 本文引入对话主题分割的评估目标，通过跨数据集评估发现性能差异源于标注粒度不匹配，指出主题分割应选择合适粒度，且将边界评分与选择分离。


<details>
  <summary>Details</summary>
Motivation: 现有对话主题分割评估实践以严格边界匹配和基于F1的指标为主，而现代基于大语言模型的对话系统需更好的评估方法来管理对话历史。

Method: 引入以边界密度和片段连贯性为主要标准的评估目标，结合窗口容忍F1（W - F1），对八种不同类型对话数据集上的多种分割策略进行跨数据集实证评估。

Result: 不同对话分割基准的性能差异由标注粒度不匹配和稀疏边界标签导致，高片段连贯性会伴随过度分割，产生低精确匹配F1分数。

Conclusion: 主题分割应理解为选择合适的粒度，而非预测单一正确的边界集，并将边界评分与选择分离。

Abstract: Dialogue topic segmentation supports summarization, retrieval, memory management, and conversational continuity. Despite decades of prior work, evaluation practice in dialogue topic segmentation remains dominated by strict boundary matching and F1-based metrics, even as modern LLM-based conversational systems increasingly rely on segmentation to manage conversation history beyond the model's fixed context window, where unstructured context accumulation degrades efficiency and coherence.
  This paper introduces an evaluation objective for dialogue topic segmentation that treats boundary density and segment coherence as primary criteria, alongside window-tolerant F1 (W-F1). Through extensive cross-dataset empirical evaluation, we show that reported performance differences across dialogue segmentation benchmarks are driven not by model quality, but by annotation granularity mismatches and sparse boundary labels. This indicates that many reported improvements arise from evaluation artifacts rather than improved boundary detection.
  We evaluated multiple, structurally distinct dialogue segmentation strategies across eight dialogue datasets spanning task-oriented, open-domain, meeting-style, and synthetic interactions. Across these settings, we observe high segment coherence combined with extreme oversegmentation relative to sparse labels, producing misleadingly low exact-match F1 scores. We show that topic segmentation is best understood as selecting an appropriate granularity rather than predicting a single correct boundary set. We operationalize this view by explicitly separating boundary scoring from boundary selection.

</details>


### [199] [Incorporating Error Level Noise Embedding for Improving LLM-Assisted Robustness in Persian Speech Recognition](https://arxiv.org/abs/2512.17247)
*Zahra Rahmani,Hossein Sameti*

Main category: cs.CL

TL;DR: 提出结合多假设和噪声感知建模的鲁棒噪声敏感ASR纠错框架，在波斯语ASR纠错中有效降低WER。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别系统在噪声环境下性能下降，低资源语言如波斯语问题更严重，现有模型难以在不同信噪比下保持准确性。

Method: 用修改后的Whisper - large解码器生成5个最佳假设，引入误差水平噪声（ELN），评估三个模型：未微调的基础LLaMA - 2 - 7B模型、仅在文本假设上微调的模型、结合句子和单词级ELN嵌入的噪声条件模型。

Result: ELN条件模型大幅降低字错率（WER），在混合噪声测试集上，提出的Fine - tuned + ELN模型将WER从31.10%降至24.84%，远超仅文本的Fine - tuned模型，原始LLaMA - 2 - 7B模型使WER升高。

Conclusion: 在嘈杂的现实场景中，结合多假设和噪声感知嵌入对鲁棒的波斯语ASR有效。

Abstract: Automatic Speech Recognition (ASR) systems suffer significant performance degradation in noisy environments, a challenge that is especially severe for low-resource languages such as Persian. Even state-of-the-art models such as Whisper struggle to maintain accuracy under varying signal-to-noise ratios (SNRs). This study presents a robust noise-sensitive ASR error correction framework that combines multiple hypotheses and noise-aware modeling. Using noisy Persian speech, we generate 5-best hypotheses from a modified Whisper-large decoder. Error Level Noise (ELN) is introduced as a representation that captures semantic- and token-level disagreement across hypotheses, quantifying the linguistic distortions caused by noise. ELN thus provides a direct measure of noise-induced uncertainty, enabling the LLM to reason about the reliability of each hypothesis during correction. Three models are evaluated: (1) a base LLaMA-2-7B model without fine-tuning, (2) a fine-tuned variant trained on text-only hypotheses, and (3) a noise-conditioned model integrating ELN embeddings at both sentence and word levels. Experimental results demonstrate that the ELN-conditioned model achieves substantial reductions in Word Error Rate (WER). Specifically, on the challenging Mixed Noise test set, the proposed Fine-tuned + ELN (Ours) model reduces the WER from a baseline of 31.10\% (Raw Whisper) to 24.84\%, significantly surpassing the Fine-tuned (No ELN) text-only baseline of 30.79\%, whereas the original LLaMA-2-7B model increased the WER to 64.58\%, demonstrating that it is unable to correct Persian errors on its own. This confirms the effectiveness of combining multiple hypotheses with noise-aware embeddings for robust Persian ASR in noisy real-world scenarios.

</details>


### [200] [AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators](https://arxiv.org/abs/2512.17267)
*Michael J. Ryan,Yanzhe Zhang,Amol Salunkhe,Yi Chu,Di Xu,Diyi Yang*

Main category: cs.CL

TL;DR: 提出AutoMetrics框架在低数据约束下合成评估指标，在多个任务中表现良好并发布相关工具包


<details>
  <summary>Details</summary>
Motivation: 用户反馈在评估用户端AI应用时稀缺或获取慢，难以用于系统优化

Method: 结合MetricBank检索和LLM - as - a - Judge标准，通过回归组合指标

Result: 在5个不同任务中，相比LLM - as - a - Judge提高Kendall相关性，所需反馈点少于100个，可作代理奖励

Conclusion: AutoMetrics能从昂贵评估指标过渡到可解释自动指标，发布工具包加速大语言模型应用的自适应评估

Abstract: Evaluating user-facing AI applications remains a central challenge, especially in open-ended domains such as travel planning, clinical note generation, or dialogue. The gold standard is user feedback (e.g., thumbs up/down) or behavioral signals (e.g., retention), but these are often scarce in prototypes and research projects, or too-slow to use for system optimization. We present AutoMetrics, a framework for synthesizing evaluation metrics under low-data constraints. AutoMetrics combines retrieval from MetricBank, a collection of 48 metrics we curate, with automatically generated LLM-as-a-Judge criteria informed by lightweight human feedback. These metrics are composed via regression to maximize correlation with human signal. AutoMetrics takes you from expensive measures to interpretable automatic metrics. Across 5 diverse tasks, AutoMetrics improves Kendall correlation with human ratings by up to 33.4% over LLM-as-a-Judge while requiring fewer than 100 feedback points. We show that AutoMetrics can be used as a proxy reward to equal effect as a verifiable reward. We release the full AutoMetrics toolkit and MetricBank to accelerate adaptive evaluation of LLM applications.

</details>


### [201] [Subjective Question Generation and Answer Evaluation using NLP](https://arxiv.org/abs/2512.17289)
*G. M. Refatul Islam,Safwan Shaheer,Yaseen Nur,Mohammad Rafid Hamid*

Main category: cs.CL

TL;DR: 本文介绍NLP技术及其应用，指出自动化主观题生成及答案评估研究尚在进行，目标是改进或创建新的NLP模型用于此任务。


<details>
  <summary>Details</summary>
Motivation: 当前自动化主观题生成和答案评估研究尚在进行，开发该系统可帮助教师评估学生作业，增强学生学习体验。

Method: 未提及

Result: 未提及

Conclusion: 研究旨在改进现有NLP模型或创建新模型来实现自动化主观题生成和答案评估。

Abstract: Natural Language Processing (NLP) is one of the most revolutionary technologies today. It uses artificial intelligence to understand human text and spoken words. It is used for text summarization, grammar checking, sentiment analysis, and advanced chatbots and has many more potential use cases. Furthermore, it has also made its mark on the education sector. Much research and advancements have already been conducted on objective question generation; however, automated subjective question generation and answer evaluation are still in progress. An automated system to generate subjective questions and evaluate the answers can help teachers assess student work and enhance the student's learning experience by allowing them to self-assess their understanding after reading an article or a chapter of a book. This research aims to improve current NLP models or make a novel one for automated subjective question generation and answer evaluation from text input.

</details>


### [202] [Perturb Your Data: Paraphrase-Guided Training Data Watermarking](https://arxiv.org/abs/2512.17075)
*Pranav Shetty,Mirazul Haque,Petr Babkin,Zhiqiang Ma,Xiaomo Liu,Manuela Veloso*

Main category: cs.CL

TL;DR: 提出SPECTRA水印方法，可使训练数据即使占比极小也能被可靠检测，效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型使用大量网络文本训练的背景下，为执行版权和数据许可，需实现训练数据检测。

Method: 用大语言模型对文本进行释义，通过评分模型打分，选择与原文分数相近的释义以避免分布偏移，检测时对比可疑模型的标记概率和评分模型。

Result: SPECTRA在检测训练数据和非训练数据时，p值差距超9个数量级，优于所有测试基线。

Conclusion: SPECTRA为数据所有者提供了可扩展、发布前部署且能在大规模大语言模型训练中留存的水印方法。

Abstract: Training data detection is critical for enforcing copyright and data licensing, as Large Language Models (LLM) are trained on massive text corpora scraped from the internet. We present SPECTRA, a watermarking approach that makes training data reliably detectable even when it comprises less than 0.001% of the training corpus. SPECTRA works by paraphrasing text using an LLM and assigning a score based on how likely each paraphrase is, according to a separate scoring model. A paraphrase is chosen so that its score closely matches that of the original text, to avoid introducing any distribution shifts. To test whether a suspect model has been trained on the watermarked data, we compare its token probabilities against those of the scoring model. We demonstrate that SPECTRA achieves a consistent p-value gap of over nine orders of magnitude when detecting data used for training versus data not used for training, which is greater than all baselines tested. SPECTRA equips data owners with a scalable, deploy-before-release watermark that survives even large-scale LLM training.

</details>


### [203] [AncientBench: Towards Comprehensive Evaluation on Excavated and Transmitted Chinese Corpora](https://arxiv.org/abs/2512.17756)
*Zhihan Zhou,Daqian Shi,Rui Song,Lida Shi,Xiaolei Diao,Hao Xu*

Main category: cs.CL

TL;DR: 为评估大语言模型对古文字的理解能力，提出 AncientBench 基准并进行实验，揭示其潜力与差距，推动其在考古和古汉语领域发展。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展需要评估对古文字理解能力的基准，现有中文基准未覆盖出土文献部分。

Method: 提出 AncientBench 基准，分为四个维度、包含十项任务；召集考古研究人员实验评估，提出古文字模型作基线，在当前表现最佳的大语言模型上广泛实验。

Result: 实验结果揭示大语言模型在古文献场景的巨大潜力以及与人类的差距。

Conclusion: 研究旨在推动大语言模型在考古和古汉语领域的发展与应用。

Abstract: Comprehension of ancient texts plays an important role in archaeology and understanding of Chinese history and civilization. The rapid development of large language models needs benchmarks that can evaluate their comprehension of ancient characters. Existing Chinese benchmarks are mostly targeted at modern Chinese and transmitted documents in ancient Chinese, but the part of excavated documents in ancient Chinese is not covered. To meet this need, we propose the AncientBench, which aims to evaluate the comprehension of ancient characters, especially in the scenario of excavated documents. The AncientBench is divided into four dimensions, which correspond to the four competencies of ancient character comprehension: glyph comprehension, pronunciation comprehension, meaning comprehension, and contextual comprehension. The benchmark also contains ten tasks, including radical, phonetic radical, homophone, cloze, translation, and more, providing a comprehensive framework for evaluation. We convened archaeological researchers to conduct experimental evaluations, proposed an ancient model as baseline, and conducted extensive experiments on the currently best-performing large language models. The experimental results reveal the great potential of large language models in ancient textual scenarios as well as the gap with humans. Our research aims to promote the development and application of large language models in the field of archaeology and ancient Chinese language.

</details>


### [204] [Bangla MedER: Multi-BERT Ensemble Approach for the Recognition of Bangla Medical Entity](https://arxiv.org/abs/2512.17769)
*Tanjim Taharat Aurpa,Farzana Akter,Md. Mehedi Hasan,Shakil Ahmed,Shifat Ara Rafiq,Fatema Khan*

Main category: cs.CL

TL;DR: 本文聚焦孟加拉语医学实体识别，实验多个transformer模型，提出Multi - BERT Ensemble方法，开发高质量数据集，证明其有效性并为低资源医学NLP发展奠基。


<details>
  <summary>Details</summary>
Motivation: 英语医学实体识别研究丰富，但像孟加拉语等低资源语言研究不足，需弥补差距。

Method: 实验BERT、DistilBERT、ELECTRA和RoBERTa等transformer模型，提出Multi - BERT Ensemble方法，开发针对孟加拉语医学实体识别的高质量数据集。

Result: Multi - BERT Ensemble方法准确率达89.58%，比单层BERT模型提高11.80%，用数据集评估显示模型稳健且适用。

Conclusion: Multi - BERT Ensemble模型在提升孟加拉语医学实体识别上有潜力，为低资源医学NLP进一步发展奠定基础。

Abstract: Medical Entity Recognition (MedER) is an essential NLP task for extracting meaningful entities from the medical corpus. Nowadays, MedER-based research outcomes can remarkably contribute to the development of automated systems in the medical sector, ultimately enhancing patient care and outcomes. While extensive research has been conducted on MedER in English, low-resource languages like Bangla remain underexplored. Our work aims to bridge this gap. For Bangla medical entity recognition, this study first examined a number of transformer models, including BERT, DistilBERT, ELECTRA, and RoBERTa. We also propose a novel Multi-BERT Ensemble approach that outperformed all baseline models with the highest accuracy of 89.58%. Notably, it provides an 11.80% accuracy improvement over the single-layer BERT model, demonstrating its effectiveness for this task. A major challenge in MedER for low-resource languages is the lack of annotated datasets. To address this issue, we developed a high-quality dataset tailored for the Bangla MedER task. The dataset was used to evaluate the effectiveness of our model through multiple performance metrics, demonstrating its robustness and applicability. Our findings highlight the potential of Multi-BERT Ensemble models in improving MedER for Bangla and set the foundation for further advancements in low-resource medical NLP.

</details>


### [205] [ShareChat: A Dataset of Chatbot Conversations in the Wild](https://arxiv.org/abs/2512.17843)
*Yueru Yan,Tuc Nguyen,Bo Su,Melissa Lieffers,Thai Le*

Main category: cs.CL

TL;DR: 文章提出大规模跨平台数据集ShareChat，含多平台对话，保留原生信息，可用于多种分析，助力研究用户与大模型交互。


<details>
  <summary>Details</summary>
Motivation: 现有公共数据集将大模型视为通用文本生成器，忽略了影响用户交互的界面上下文，为解决此局限开展研究。

Method: 从五个主要平台收集公开分享的URL中的对话，构建ShareChat数据集，并通过三项代表性分析展示其用途。

Result: 构建了包含142,808个对话和超660,000轮的ShareChat数据集，它在多方面优于先前数据集，并展示了其在测量用户意图满意度、评估内容生成源引用行为和跟踪使用模式演变方面的实用性。

Conclusion: 为研究社区提供了理解真实用户与大模型聊天机器人交互的重要及时资源。

Abstract: While Large Language Models (LLMs) have evolved into distinct platforms with unique interface designs and capabilities, existing public datasets treat models as generic text generators, stripping away the interface context that actively shapes user interaction. To address this limitation, we present ShareChat, a large-scale, cross-platform corpus comprising 142,808 conversations and over 660,000 turns collected from publicly shared URLs across five major platforms: ChatGPT, Claude, Gemini, Perplexity, and Grok. ShareChat distinguishes itself by preserving native platform affordances often lost in standard logs, including reasoning traces, source links, and code artifacts, while spanning 101 languages over the period from April 2023 to October 2025. Furthermore, ShareChat offers substantially longer context windows and greater interaction depth than prior datasets. We demonstrate the dataset's multifaceted utility through three representative analyses: (1) analyzing conversation completeness to measure user intent satisfaction; (2) evaluating source citation behaviors in content generation; and (3) conducting temporal analysis to track evolving usage patterns. This work provides the community with a vital and timely resource for understanding authentic user-LLM chatbot interactions in the wild.

</details>


### [206] [Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection](https://arxiv.org/abs/2512.17630)
*Menna Elgabry,Ali Hamdi*

Main category: cs.CL

TL;DR: 本文受孔多塞陪审团定理启发，提出基于置信加权和可信度感知的文本情感检测集成框架，结合多种小型大语言模型，实验显示其性能超大规模模型，参数效率更高。


<details>
  <summary>Details</summary>
Motivation: 解决传统集成方法依赖同质架构的问题，提升文本情感检测性能和参数效率。

Method: 结合多种架构不同的小型大语言模型，微调用于情感分类；最小化参数收敛以保留误差多样性；采用双加权投票机制，结合全局可信度和局部置信度来动态加权模型贡献。

Result: 在DAIR - AI数据集上宏观F1分数达93.5%，超越现有基准和大规模模型，总参数仅595M，比7B参数模型更高效稳健。

Conclusion: 精心设计的小型微调模型集成在情感检测等特定NLP任务中可超越更大的大语言模型。

Abstract: This paper introduces a confidence-weighted, credibility-aware ensemble framework for text-based emotion detection, inspired by Condorcet's Jury Theorem (CJT). Unlike conventional ensembles that often rely on homogeneous architectures, our approach combines architecturally diverse small transformer-based large language models (sLLMs) - BERT, RoBERTa, DistilBERT, DeBERTa, and ELECTRA, each fully fine-tuned for emotion classification. To preserve error diversity, we minimize parameter convergence while taking advantage of the unique biases of each model. A dual-weighted voting mechanism integrates both global credibility (validation F1 score) and local confidence (instance-level probability) to dynamically weight model contributions. Experiments on the DAIR-AI dataset demonstrate that our credibility-confidence ensemble achieves a macro F1 score of 93.5 percent, surpassing state-of-the-art benchmarks and significantly outperforming large-scale LLMs, including Falcon, Mistral, Qwen, and Phi, even after task-specific Low-Rank Adaptation (LoRA). With only 595M parameters in total, our small LLMs ensemble proves more parameter-efficient and robust than models up to 7B parameters, establishing that carefully designed ensembles of small, fine-tuned models can outperform much larger LLMs in specialized natural language processing (NLP) tasks such as emotion detection.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [207] [The Role of Islamic Ethics in Preventing the Abuse of Artificial Intelligence (AI) Based Deepfakes](https://arxiv.org/abs/2512.17218)
*Wisnu Uriawan,Imany Fauzy Rahman,Muhamad Zidan,Irma Rohmatillah,Muhammad Arkan Raihan,Irma Dwiyanti*

Main category: cs.CY

TL;DR: 人工智能驱动的深度伪造技术发展引发全球担忧，本文旨在制定全面的伊斯兰伦理框架以降低其滥用风险，通过SLR方法研究，得出三项战略建议，认为应用伊斯兰伦理能从惩罚机制转向预防方法。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术发展带来虚假信息、身份盗用等问题，传统管理方法不足以解决背后的意图、道德和社会影响等根源问题，因此要制定全面的伊斯兰伦理框架来降低其滥用风险。

Method: 采用基于PRISMA的系统文献综述（SLR）方法，选取2018 - 2025年的十篇主要文献，识别伦理缺陷、监管需求和规范解决方案。

Result: 分析表明伊斯兰教法原则（特别是保护荣誉和保护自我）的整合为规范技术的负责任使用提供了强大的规范基础，得出三项战略建议。

Conclusion: 应用伊斯兰伦理能从惩罚机制转向预防方法，聚焦保护人类尊严、防止伤害和加强数字时代的公共利益。

Abstract: The significant development of deepfake technology powered by artificial intelligence (AI) has sparked worldwide concerns about the alteration of false information, the usurpation of online identities, and the decline of public confidence in the authenticity of online content. These incidents not only raise technical issues but also carry complex moral implications, rendering conventional, technologically driven, and reactive management methods inadequate to address the underlying causes of the problem, including intent, morality, and potential intangible social impacts. Based on these issues, this study aims to formulate a comprehensive Islamic ethical framework that can serve as a more comprehensive preventative tool to mitigate the risks of misuse of deepfakes. The study employed a Systematic Literature Review (SLR) guided by PRISMA, selecting ten primary sources published between 2018 and 2025 to identify ethical deficiencies, regulatory needs, and appropriate normative solutions. The analysis shows that the integration of the principles of (Maqasid al-Shariah) particularly (hifz al-ird) protecting honor and (hifz al-nafs) protecting the self, provides a strong normative basis for regulating the responsible use of technology. This study yields three strategic recommendations: regulatory changes that recognize the intangible and psychological harm caused by reputational damage; improved technology management through moral scrutiny that upholds the values of justice (adl), trust, and openness; and increased public digital literacy based on the principle of (tabayyun) examination and caution. Overall, this study concludes that the application of Islamic ethics offers a shift in thinking from punitive mechanisms to preventative approaches that focus on protecting human dignity, preventing harm, and strengthening the common good in the digital age.

</details>


### [208] [Fair Voting Methods as a Catalyst for Democratic Resilience: A Trilogy on Legitimacy, Impact and AI Safeguarding](https://arxiv.org/abs/2512.17461)
*Evangelos Pournaras*

Main category: cs.CY

TL;DR: 文章介绍公平投票方法可推动集体决策方式变革与民主升级，基于现实证据展示关键研究成果及优势，还提及对希腊等危机中民主国家的意义。


<details>
  <summary>Details</summary>
Motivation: 探讨公平投票方法对集体决策方式改变和民主升级的作用。

Method: 基于瑞士等地参与式预算民主创新的现实证据进行研究分析。

Result: 公平投票方法是合法性孵化器、新影响加速器和人工智能风险保障；相比多数决投票法，能带来更多赢家和更高公民代表性，受选民青睐，促进民主价值观，产生新创意，还对人工智能偏见有更强抵抗力。

Conclusion: 公平投票方法可推动民主变革和升级，对危机中的民主国家有积极作用。

Abstract: This article shows how fair voting methods can be a catalyst for change in the way we make collective decisions, and how such change can promote long-awaited upgrades of democracy. Based on real-world evidence from democratic innovations in participatory budgeting, in Switzerland and beyond, I highlight a trilogy of key research results: Fair voting methods achieve to be (i) legitimacy incubator, (ii) novel impact accelerator and (iii) safeguard for risks of artificial intelligence (AI). Compared to majoritarian voting methods, combining expressive ballot formats (e.g. cumulative voting) with ballot aggregation methods that promote proportional representation (e.g. equal shares) results in more winners and higher (geographical) representation of citizens. Such fair voting methods are preferred and found fairer even by voters who do not win, while promoting stronger democratic values for citizens such as altruism and compromise. They also result in new resourceful ideas to put for voting, which are cost-effective and win, especially in areas of welfare, education and culture. Strikingly, fair voting methods are also more resilient to biases and inconsistencies of generative AI in emerging scenarios of AI voting assistance or AI representation of voters who would be likely to abstain. I also review the relevance of such upgrades for democracies in crisis, such as the one of Greece featured in the recent study of `Unmute Democracy'. Greek democracy can build stronger resilience via higher representation of citizens in democratic processes as well as democratic innovations in participation. Fair voting methods can be a catalyst for both endeavors.

</details>


### [209] [Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life](https://arxiv.org/abs/2512.17850)
*Corey M. Abramson*

Main category: cs.CY

TL;DR: 本文展示计算社会科学（CSS）工具如何扩展老龄化研究，结合案例探讨其在定性老龄化研究中的应用、挑战与潜力。


<details>
  <summary>Details</summary>
Motivation: 探索计算社会科学工具在老龄化研究中的作用，推动其与定性老龄化研究的有意义对话。

Method: 结合传统定性方法，运用机器学习和自然语言处理技术，通过DISCERN研究和美国之声项目等案例分析。

Result: CSS工具可简化和增强现有工作流程、扩大样本和项目规模、生成多方法研究途径。

Conclusion: 当前发展虽有风险，但能拓宽定性研究方法基础，为老龄化和生命历程研究带来新见解。

Abstract: This chapter demonstrates how computational social science (CSS) tools are extending and expanding research on aging. The depth and context from traditionally qualitative methods such as participant observation, in-depth interviews, and historical documents are increasingly employed alongside scalable data management, computational text analysis, and open-science practices. Machine learning (ML) and natural language processing (NLP), provide resources to aggregate and systematically index large volumes of qualitative data, identify patterns, and maintain clear links to in-depth accounts. Drawing on case studies of projects that examine later life--including examples with original data from the DISCERN study (a team-based ethnography of life with dementia) and secondary analyses of the American Voices Project (nationally representative interview)--the chapter highlights both uses and challenges of bringing CSS tools into more meaningful dialogue with qualitative aging research. The chapter argues such work has potential for (1) streamlining and augmenting existing workflows, (2) scaling up samples and projects, and (3) generating multi-method approaches to address important questions in new ways, before turning to practices useful for individuals and teams seeking to understand current possibilities or refine their workflow processes. The chapter concludes that current developments are not without peril, but offer potential for new insights into aging and the life course by broadening--rather than replacing--the methodological foundations of qualitative research.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [210] [Another Fit Bites the Dust: Conformal Prediction as a Calibration Standard for Machine Learning in High-Energy Physics](https://arxiv.org/abs/2512.17048)
*Jack Y. Araz,Michael Spannowsky*

Main category: hep-ph

TL;DR: 本文探讨保形预测作为高能物理机器学习应用的统一校准层，可将原始模型输出转化为有效预测集合等，应成为对撞机物理机器学习流程标准组件。


<details>
  <summary>Details</summary>
Motivation: 机器学习技术在现代对撞机研究中概率输出缺乏校准不确定性估计和有限样本保证，限制其在统计推断和决策中的应用。

Method: 使用公开的对撞机数据集和多种模型，研究保形预测在回归、分类、异常检测和生成建模等方面的应用。

Result: 单一的保形形式可应用于多种场景，将原始模型输出转化为有效预测集合等，不提升原始模型性能但实现诚实的不确定性量化和透明的误差控制。

Conclusion: 保形校准应成为对撞机物理机器学习流程的标准组件，以实现可靠解释、稳健比较和原则性统计决策。

Abstract: Machine-learning techniques are essential in modern collider research, yet their probabilistic outputs often lack calibrated uncertainty estimates and finite-sample guarantees, limiting their direct use in statistical inference and decision-making. Conformal prediction (CP) provides a simple, distribution-free framework for calibrating arbitrary predictive models without retraining, yielding rigorous uncertainty quantification with finite-sample coverage guarantees under minimal exchangeability assumptions, without reliance on asymptotics, limit theorems, or Gaussian approximations. In this work, we investigate CP as a unifying calibration layer for machine-learning applications in high-energy physics. Using publicly available collider datasets and a diverse set of models, we show that a single conformal formalism can be applied across regression, binary and multi-class classification, anomaly detection, and generative modelling, converting raw model outputs into statistically valid prediction sets, typicality regions, and p-values with controlled false-positive rates. While conformal prediction does not improve raw model performance, it enforces honest uncertainty quantification and transparent error control. We argue that conformal calibration should be adopted as a standard component of machine-learning pipelines in collider physics, enabling reliable interpretation, robust comparisons, and principled statistical decisions in experimental and phenomenological analyses.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [211] [PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology](https://arxiv.org/abs/2512.17517)
*Siemen Brussee,Pieter A. Valkema,Jurre A. J. Weijer,Thom Doeleman,Anne M. R. Schrader,Jesper Kers*

Main category: cs.CV

TL;DR: 介绍开源的组织病理学多实例学习AutoML及基准测试框架PathBench - MIL


<details>
  <summary>Details</summary>
Motivation: 为组织病理学多实例学习提供自动化、可复现的基准测试方案及促进实验标准化

Method: 创建自动化端到端MIL管道，包含预处理、特征提取和MIL聚合，并集成可视化工具、统一配置系统和模块化扩展功能

Result: 得到PathBench - MIL框架，可对多种MIL模型和特征提取器进行基准测试

Conclusion: PathBench - MIL开源且可快速开展实验和实现标准化，其代码公布于https://github.com/Sbrussee/PathBench - MIL

Abstract: We introduce PathBench-MIL, an open-source AutoML and benchmarking framework for multiple instance learning (MIL) in histopathology. The system automates end-to-end MIL pipeline construction, including preprocessing, feature extraction, and MIL-aggregation, and provides reproducible benchmarking of dozens of MIL models and feature extractors. PathBench-MIL integrates visualization tooling, a unified configuration system, and modular extensibility, enabling rapid experimentation and standardization across datasets and tasks. PathBench-MIL is publicly available at https://github.com/Sbrussee/PathBench-MIL

</details>


### [212] [V-Agent: An Interactive Video Search System Using Vision-Language Models](https://arxiv.org/abs/2512.16925)
*SunYoung Park,Jong-Hyeon Lee,Youngjune Kim,Daegyu Sung,Younghyun Yu,Young-rok Cha,Jeongho Ju*

Main category: cs.CV

TL;DR: 介绍了用于高级视频搜索和交互对话的多智能体平台V - Agent，通过微调VLM等方法实现视频搜索，在MultiVENT 2.0基准测试表现优异。


<details>
  <summary>Details</summary>
Motivation: 克服传统基于文本的检索系统在多模态场景中的局限性。

Method: 用小视频偏好数据集微调VLM，结合图像 - 文本检索模型的检索向量；VLM - 基于的检索模型将视频帧和音频转录嵌入共享多模态表示空间；系统包含路由、搜索和聊天三个协作的智能体，搜索智能体结合VLM - 基于的检索模型和重排序模块。

Result: 在MultiVENT 2.0基准测试中展示了零样本的最优性能。

Conclusion: 该框架在学术研究和实际应用中具有潜力。

Abstract: We introduce V-Agent, a novel multi-agent platform designed for advanced video search and interactive user-system conversations. By fine-tuning a vision-language model (VLM) with a small video preference dataset and enhancing it with a retrieval vector from an image-text retrieval model, we overcome the limitations of traditional text-based retrieval systems in multimodal scenarios. The VLM-based retrieval model independently embeds video frames and audio transcriptions from an automatic speech recognition (ASR) module into a shared multimodal representation space, enabling V-Agent to interpret both visual and spoken content for context-aware video search. This system consists of three agents-a routing agent, a search agent, and a chat agent-that work collaboratively to address user intents by refining search outputs and communicating with users. The search agent utilizes the VLM-based retrieval model together with an additional re-ranking module to further enhance video retrieval quality. Our proposed framework demonstrates state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark, highlighting its potential for both academic research and real-world applications.

</details>


### [213] [ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching](https://arxiv.org/abs/2512.17178)
*Qi Zhang,Yuxu Chen,Lei Deng,Lili Shen*

Main category: cs.CV

TL;DR: 本文提出无训练的ABE - CLIP方法增强CLIP模型的属性 - 对象绑定，实验显示其提升性能且优于部分需大量训练的方法。


<details>
  <summary>Details</summary>
Motivation: CLIP在组合图像 - 文本匹配特别是属性 - 对象关联上存在不足，现有方法泛化能力有限且未根本解决全局表示的问题，因此需新方法增强属性 - 对象绑定。

Method: 提出ABE - CLIP方法，采用语义细化机制优化文本中对象和属性短语的词嵌入，引入局部词 - 图像块对齐策略计算细化后文本词与相关图像块的相似度得分，聚合得分得到最终图像 - 文本相似度。

Result: 在多个数据集上的实验表明，ABE - CLIP显著提高了属性 - 对象绑定性能，甚至超越了需要大量训练的方法。

Conclusion: ABE - CLIP能有效增强CLIP类模型中的属性 - 对象绑定，是一种有效的训练无的方法。

Abstract: Contrastive Language-Image Pretraining (CLIP) has achieved remarkable performance in various multimodal tasks. However, it still struggles with compositional image-text matching, particularly in accurately associating objects with their corresponding attributes, because its inherent global representation often overlooks fine-grained semantics for attribute binding. Existing methods often require additional training or extensive hard negative sampling, yet they frequently show limited generalization to novel compositional concepts and fail to fundamentally address the drawbacks of global representations. In this paper, we propose ABE-CLIP, a novel training-free Attribute Binding Enhancement method designed to strengthen attribute-object binding in CLIP-like models. Specifically, we employ a Semantic Refinement Mechanism to refine token embeddings for both object and attribute phrases in the text, thereby mitigating attribute confusion and improving semantic precision. We further introduce a Local Token-Patch Alignment strategy that computes similarity scores between refined textual tokens and their most relevant image patches. By aggregating localized similarity scores, ABE-CLIP computes the final image-text similarity. Experiments on multiple datasets demonstrate that ABE-CLIP significantly improves attribute-object binding performance, even surpassing methods that require extensive training.

</details>


### [214] [Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections](https://arxiv.org/abs/2512.16950)
*Adrian Straker,Paul Magdon,Marco Zullich,Maximilian Freudenberg,Christoph Kleinn,Johannes Breidenbach,Stefano Puliti,Nils Nölke*

Main category: cs.CV

TL;DR: 提出连接Finer - CAM解释与TLS投影片段的新方法评估树种分类特征，训练验证模型达96%准确率，分析显示模型主要依赖树冠特征，也凸显理解模型决策过程的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有新传感器和分类方法决策过程不清晰，Finer - CAM在相似树种分类中不常见，需要系统性评估哪些特征驱动树种区分。

Method: 提出将Finer - CAM解释与TLS投影片段关联的方法，使用2445棵树的TLS数据，对五个YOLOv8模型进行交叉验证训练和验证。

Result: 模型平均准确率达96%，分析630张显著性图，表明模型主要依赖树冠特征分类，不同树种对树冠和树干特征依赖不同，模型对相似树种的判断与人一致。

Conclusion: 需要更好地理解树种分类模型的决策过程，以揭示数据集和模型的局限、偏差，增强对模型预测的信心。

Abstract: Classifying tree species has been a core research area in forest remote sensing for decades. New sensors and classification approaches like TLS and deep learning achieve state-of-the art accuracy but their decision processes remain unclear. Methods such as Finer-CAM (Class Activation Mapping) can highlight features in TLS projections that contribute to the classification of a target species, yet are uncommon in similar looking contrastive tree species. We propose a novel method linking Finer-CAM explanations to segments of TLS projections representing structural tree features to systemically evaluate which features drive species discrimination. Using TLS data from 2,445 trees across seven European tree species, we trained and validated five YOLOv8 models with cross-validation, reaching a mean accuracy of 96% (SD = 0.24%). Analysis of 630 saliency maps shows the models primarily rely on crown features in TLS projections for species classification. While this result is pronounced in Silver Birch, European Beech, English oak, and Norway spruce, stem features contribute more frequently to the differentiation of European ash, Scots pine, and Douglas fir. Particularly representations of finer branches contribute to the decisions of the models. The models consider those tree species similar to each other which a human expert would also regard as similar. Furthermore, our results highlight the need for an improved understanding of the decision processes of tree species classification models to help reveal data set and model limitations, biases, and to build confidence in model predictions.

</details>


### [215] [Lights, Camera, Consistency: A Multistage Pipeline for Character-Stable AI Video Stories](https://arxiv.org/abs/2512.16954)
*Chayan Jain,Rishant Sharma,Archit Garg,Ishan Bhanuka,Pratik Narang,Dhruv Kumar*

Main category: cs.CV

TL;DR: 提出模仿电影制作人方式的文本到视频生成方法，验证多阶段分解必要性并分析文化差异。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频AI在生成包含连贯角色的长视频故事方面存在显著挑战。

Method: 使用大语言模型生成详细制作脚本，用脚本引导文本到图像模型为角色创建一致视觉形象，再让视频生成模型逐场景合成。

Result: 去除视觉锚定机制会使角色一致性分数大幅下降；分析发现当前模型存在文化差异。

Conclusion: 视觉先验对身份保留至关重要，当前模型存在文化偏见。

Abstract: Generating long, cohesive video stories with consistent characters is a significant challenge for current text-to-video AI. We introduce a method that approaches video generation in a filmmaker-like manner. Instead of creating a video in one step, our proposed pipeline first uses a large language model to generate a detailed production script. This script guides a text-to-image model in creating consistent visuals for each character, which then serve as anchors for a video generation model to synthesize each scene individually. Our baseline comparisons validate the necessity of this multi-stage decomposition; specifically, we observe that removing the visual anchoring mechanism results in a catastrophic drop in character consistency scores (from 7.99 to 0.55), confirming that visual priors are essential for identity preservation. Furthermore, we analyze cultural disparities in current models, revealing distinct biases in subject consistency and dynamic degree between Indian vs Western-themed generations.

</details>


### [216] [InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression](https://arxiv.org/abs/2512.16975)
*Haotian Ye,Qiyuan He,Jiaqi Han,Puheng Li,Jiaojiao Fan,Zekun Hao,Fitsum Reda,Yogesh Balaji,Huayu Chen,Sheng Liu,Angela Yao,James Zou,Stefano Ermon,Haoxiang Wang,Ming-Yu Liu*

Main category: cs.CV

TL;DR: 本文提出InfoTok框架用于自适应视频标记化，有最优算法和基于transformer的压缩器，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有视频标记器以固定速率压缩内容，存在冗余或信息丢失问题，需要准确高效的离散视频标记化方法。

Method: 受香农信息论启发，提出InfoTok框架，证明现有数据无关训练方法在表示长度上不是最优的，给出基于ELBO的接近理论最优的算法，开发基于transformer的自适应压缩器。

Result: 实现了最先进的压缩性能，节省20%的标记且不影响性能，达到2.3倍压缩率，优于先前启发式自适应方法。

Conclusion: InfoTok根据信息丰富度分配标记，实现更紧凑准确的视频表示，为未来研究提供有价值的见解。

Abstract: Accurate and efficient discrete video tokenization is essential for long video sequences processing. Yet, the inherent complexity and variable information density of videos present a significant bottleneck for current tokenizers, which rigidly compress all content at a fixed rate, leading to redundancy or information loss. Drawing inspiration from Shannon's information theory, this paper introduces InfoTok, a principled framework for adaptive video tokenization. We rigorously prove that existing data-agnostic training methods are suboptimal in representation length, and present a novel evidence lower bound (ELBO)-based algorithm that approaches theoretical optimality. Leveraging this framework, we develop a transformer-based adaptive compressor that enables adaptive tokenization. Empirical results demonstrate state-of-the-art compression performance, saving 20% tokens without influence on performance, and achieving 2.3x compression rates while still outperforming prior heuristic adaptive approaches. By allocating tokens according to informational richness, InfoTok enables a more compressed yet accurate tokenization for video representation, offering valuable insights for future research.

</details>


### [217] [SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction](https://arxiv.org/abs/2512.17137)
*Puyang Wang,Pengfei Guo,Keyi Chai,Jinyuan Zhou,Daguang Xu,Shanshan Jiang*

Main category: cs.CV

TL;DR: 提出可扩展深度展开模型（SDUM）用于通用MRI重建，在多数据集上超越基线，证明是通用可扩展MRI重建的实用途径。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习重建通常特定于成像协议，阻碍泛化和部署，需要通用框架。

Method: 引入SDUM框架，结合Restormer重建器、CSME、SWDC、UC和渐进级联扩展训练。

Result: SDUM在多数据集和挑战赛中表现优异，超越多个基线，消融实验证明各组件有效性。

Conclusion: SDUM是实现通用、可扩展MRI重建的实用方法。

Abstract: Clinical MRI encompasses diverse imaging protocols--spanning anatomical targets (cardiac, brain, knee), contrasts (T1, T2, mapping), sampling patterns (Cartesian, radial, spiral, kt-space), and acceleration factors--yet current deep learning reconstructions are typically protocol-specific, hindering generalization and deployment. We introduce Scalable Deep Unrolled Model (SDUM), a universal framework combining a Restormer-based reconstructor, a learned coil sensitivity map estimator (CSME), sampling-aware weighted data consistency (SWDC), universal conditioning (UC) on cascade index and protocol metadata, and progressive cascade expansion training. SDUM exhibits foundation-model-like scaling behavior: reconstruction quality follows PSNR ${\sim}$ log(parameters) with correlation $r{=}0.986$ ($R^2{=}0.973$) up to 18 cascades, demonstrating predictable performance gains with model depth. A single SDUM trained on heterogeneous data achieves state-of-the-art results across all four CMRxRecon2025 challenge tracks--multi-center, multi-disease, 5T, and pediatric--without task-specific fine-tuning, surpassing specialized baselines by up to ${+}1.0$~dB. On CMRxRecon2024, SDUM outperforms the winning method PromptMR+ by ${+}0.55$~dB; on fastMRI brain, it exceeds PC-RNN by ${+}1.8$~dB. Ablations validate each component: SWDC ${+}0.43$~dB over standard DC, per-cascade CSME ${+}0.51$~dB, UC ${+}0.38$~dB. These results establish SDUM as a practical path toward universal, scalable MRI reconstruction.

</details>


### [218] [Fose: Fusion of One-Step Diffusion and End-to-End Network for Pansharpening](https://arxiv.org/abs/2512.17202)
*Kai Liu,Zeli Lin,Weibo Wang,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出四阶段训练策略得到轻量级网络Fose，融合一步扩散模型和端到端模型，实验证明其效果显著且有速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型计算量大、耗时长，端到端模型因缺乏先验和结构简单性能受限，需改进图像融合方法。

Method: 提出四阶段训练策略，对增强的SOTA扩散模型进行一步蒸馏，将推理步骤从50步压缩到1步，用轻量级集成块融合一步扩散模型和端到端模型。

Result: 在三个常用基准上效果显著提升，与基线扩散模型相比有7.42倍的加速比且性能更好。

Conclusion: 提出的轻量级网络Fose在图像融合上有良好表现，代码和模型已开源。

Abstract: Pansharpening is a significant image fusion task that fuses low-resolution multispectral images (LRMSI) and high-resolution panchromatic images (PAN) to obtain high-resolution multispectral images (HRMSI). The development of the diffusion models (DM) and the end-to-end models (E2E model) has greatly improved the frontier of pansharping. DM takes the multi-step diffusion to obtain an accurate estimation of the residual between LRMSI and HRMSI. However, the multi-step process takes large computational power and is time-consuming. As for E2E models, their performance is still limited by the lack of prior and simple structure. In this paper, we propose a novel four-stage training strategy to obtain a lightweight network Fose, which fuses one-step DM and an E2E model. We perform one-step distillation on an enhanced SOTA DM for pansharping to compress the inference process from 50 steps to only 1 step. Then we fuse the E2E model with one-step DM with lightweight ensemble blocks. Comprehensive experiments are conducted to demonstrate the significant improvement of the proposed Fose on three commonly used benchmarks. Moreover, we achieve a 7.42 speedup ratio compared to the baseline DM while achieving much better performance. The code and model are released at https://github.com/Kai-Liu001/Fose.

</details>


### [219] [WDFFU-Mamba: A Wavelet-guided Dual-attention Feature Fusion Mamba for Breast Tumor Segmentation in Ultrasound Images](https://arxiv.org/abs/2512.17278)
*Guoping Cai,Houjin Chen,Yanfeng Li,Jia Sun,Ziwei Chen,Qingzi Geng*

Main category: cs.CV

TL;DR: 本文提出WDFFU - Mamba模型用于乳腺超声图像分割，结合小波域增强和基于注意力的融合，实验显示其性能优于现有方法，有临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 乳腺超声图像分割面临斑点噪声、成像伪影等挑战，阻碍准确分割，需设计鲁棒高效模型。

Method: 提出WDFFU - Mamba网络，集成小波引导增强和双注意力特征融合于U型Mamba架构，采用WHF模块增强低级表示，DAFF模块融合特征。

Result: 在两个公共乳腺超声数据集上实验表明，WDFFU - Mamba分割精度高，在Dice系数和HD95指标上显著优于现有方法。

Conclusion: 小波域增强和基于注意力的融合提高了分割的准确性和鲁棒性，模型性能好、泛化能力佳，有望用于临床乳腺肿瘤超声分析。

Abstract: Breast ultrasound (BUS) image segmentation plays a vital role in assisting clinical diagnosis and early tumor screening. However, challenges such as speckle noise, imaging artifacts, irregular lesion morphology, and blurred boundaries severely hinder accurate segmentation. To address these challenges, this work aims to design a robust and efficient model capable of automatically segmenting breast tumors in BUS images.We propose a novel segmentation network named WDFFU-Mamba, which integrates wavelet-guided enhancement and dual-attention feature fusion within a U-shaped Mamba architecture. A Wavelet-denoised High-Frequency-guided Feature (WHF) module is employed to enhance low-level representations through noise-suppressed high-frequency cues. A Dual Attention Feature Fusion (DAFF) module is also introduced to effectively merge skip-connected and semantic features, improving contextual consistency.Extensive experiments on two public BUS datasets demonstrate that WDFFU-Mamba achieves superior segmentation accuracy, significantly outperforming existing methods in terms of Dice coefficient and 95th percentile Hausdorff Distance (HD95).The combination of wavelet-domain enhancement and attention-based fusion greatly improves both the accuracy and robustness of BUS image segmentation, while maintaining computational efficiency.The proposed WDFFU-Mamba model not only delivers strong segmentation performance but also exhibits desirable generalization ability across datasets, making it a promising solution for real-world clinical applications in breast tumor ultrasound analysis.

</details>


### [220] [A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs](https://arxiv.org/abs/2512.17319)
*Yunkai Dang,Meiyi Zhu,Donghao Wang,Yizhuo Zhang,Jiacheng Yang,Qi Fan,Yuekun Yang,Wenbin Li,Feng Miao,Yang Gao*

Main category: cs.CV

TL;DR: 当前遥感基准存在问题，研究引入超高清基准RSHR - Bench评估视觉理解，构建多种任务并评估，发现超高清场景下模型性能有差距。


<details>
  <summary>Details</summary>
Motivation: 现有的遥感基准大多依赖低分辨率图像，部分高分辨率基准推理任务设计有缺陷，无法有效评估视觉理解能力。

Method: 引入超高清基准RSHR - Bench，包含大量高像素全场景图像，设计四类任务，应用对抗过滤和人工验证构建任务。

Result: 构建了3864个VQA任务、3913个图像字幕任务和500个单图像评估VQA对，评估发现超高清场景下模型存在性能差距。

Conclusion: RSHR - Bench可对遥感视觉理解和推理进行有效评估，当前模型在超高清场景下表现有待提升。

Abstract: Multimodal large language models (MLLMs) demonstrate strong perception and reasoning performance on existing remote sensing (RS) benchmarks. However, most prior benchmarks rely on low-resolution imagery, and some high-resolution benchmarks suffer from flawed reasoning-task designs. We show that text-only LLMs can perform competitively with multimodal vision-language models on RS reasoning tasks without access to images, revealing a critical mismatch between current benchmarks and the intended evaluation of visual understanding. To enable faithful assessment, we introduce RSHR-Bench, a super-high-resolution benchmark for RS visual understanding and reasoning. RSHR-Bench contains 5,329 full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image, sourced from widely used RS corpora and UAV collections. We design four task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks cover nine perception categories and four reasoning types, supporting multi-turn and multi-image dialog. To reduce reliance on language priors, we apply adversarial filtering with strong LLMs followed by rigorous human verification. Overall, we construct 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation VQA pairs. Evaluations across open-source, closed-source, and RS-specific VLMs reveal persistent performance gaps in super-high-resolution scenarios. Code: https://github.com/Yunkaidang/RSHR

</details>


### [221] [RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering](https://arxiv.org/abs/2512.17396)
*Léo Butsanets,Charles Corbière,Julien Khlaut,Pierre Manceron,Corentin Dancette*

Main category: cs.CV

TL;DR: 介绍大规模放射影像问答数据集RadImageNet - VQA，含实验结果并公开数据集。


<details>
  <summary>Details</summary>
Motivation: 现有医学VQA数据集规模有限、成像类型单一且易有文本捷径，需新数据集推动放射影像问答发展。

Method: 从专家策划注释构建RadImageNet - VQA数据集，包含750K图像和750万问答样本，覆盖多任务和类别。

Result: 先进视觉语言模型在细粒度病理识别有困难，无图像输入模型性能近随机。

Conclusion: RadImageNet - VQA无语言捷径，推动放射影像问答研究，数据集和基准公开可用。

Abstract: In this work, we introduce RadImageNet-VQA, a large-scale dataset designed to advance radiologic visual question answering (VQA) on CT and MRI exams. Existing medical VQA datasets are limited in scale, dominated by X-ray imaging or biomedical illustrations, and often prone to text-based shortcuts. RadImageNet-VQA is built from expert-curated annotations and provides 750K images paired with 7.5M question-answer samples. It covers three key tasks - abnormality detection, anatomy recognition, and pathology identification - spanning eight anatomical regions and 97 pathology categories, and supports open-ended, closed-ended, and multiple-choice questions. Extensive experiments show that state-of-the-art vision-language models still struggle with fine-grained pathology identification, particularly in open-ended settings and even after fine-tuning. Text-only analysis further reveals that model performance collapses to near-random without image inputs, confirming that RadImageNet-VQA is free from linguistic shortcuts. The full dataset and benchmark are publicly available at https://huggingface.co/datasets/raidium/RadImageNet-VQA.

</details>


### [222] [InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion](https://arxiv.org/abs/2512.17504)
*Hoiyeong Jin,Hyojin Jang,Jeongho Kim,Junha Hyung,Kinam Kim,Dongjin Kim,Huijin Choi,Hyeonji Kim,Jaegul Choo*

Main category: cs.CV

TL;DR: 提出InsertAnywhere VOI框架，结合4D感知掩码生成模块和扩散视频生成模型，用ROSE++数据集训练，在多种场景下表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的视频生成在现实视频对象插入（VOI）方面因4D场景理解有限、处理遮挡和光照效果不足而面临挑战。

Method: 先使用4D感知掩码生成模块重建场景几何并跨帧传播对象位置，再扩展基于扩散的视频生成模型合成插入对象及其局部变化；引入ROSE++数据集进行监督训练。

Result: 框架在多种现实场景中实现了几何合理、视觉连贯的对象插入，显著优于现有研究和商业模型。

Conclusion: InsertAnywhere框架有效解决了现实视频对象插入的问题，具有良好的性能和应用前景。

Abstract: Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.

</details>


### [223] [Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding](https://arxiv.org/abs/2512.17532)
*Jiaqi Tang,Jianmin Chen,Wei Wei,Xiaogang Xu,Runtao Liu,Xiangyu Wu,Qipeng Xie,Jiafei Wu,Lei Zhang,Qifeng Chen*

Main category: cs.CV

TL;DR: 提出Robust - R1框架提升多模态大语言模型在视觉退化下的鲁棒性，有新数据集，评估表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在极端视觉退化下性能不佳，且现有鲁棒模型存在可解释性有限和优化孤立问题。

Method: 提出Robust - R1框架，通过结构化推理链显式建模视觉退化，包括有监督微调、奖励驱动对齐和动态推理深度缩放，引入11K特殊数据集。

Result: Robust - R1在R - Bench基准上超越所有基线，在MMMB、MMStar和RealWorldQA的多强度对抗退化下有优越抗退化性能。

Conclusion: Robust - R1框架能有效提升多模态大语言模型在极端视觉退化下的鲁棒性。

Abstract: Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.

</details>


### [224] [ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image](https://arxiv.org/abs/2512.17545)
*Yunqi Gao,Leyuan Liu,Yuhan Li,Changxin Gao,Yuanyuan Liu,Jingying Chen*

Main category: cs.CV

TL;DR: 现有3D人体网格恢复技术处理多样服装时表现不佳，本文提出ClothHMR方法，由服装裁剪和基于FHVM的网格恢复模块组成，能准确恢复穿多样服装人体的3D网格，实验效果好且有实际应用。


<details>
  <summary>Details</summary>
Motivation: 当前3D人体网格恢复技术主要处理紧身服装，在多样服装尤其是宽松服装下估计人体形状和姿势表现差，需改进。

Method: 提出ClothHMR方法，包含服装裁剪（CT）和基于FHVM的网格恢复（MR）模块，CT模块用身体语义估计和边缘预测裁剪服装，MR模块通过对齐中间表示优化3D人体网格初始参数。

Result: ClothHMR能准确恢复穿多样服装人体的3D网格，在基准数据集和真实图像上显著优于现有方法，还开发了网络应用。

Conclusion: ClothHMR方法有效，可准确恢复多样服装下人体3D网格，适用于实际场景，代码和模型已开源。

Abstract: With 3D data rapidly emerging as an important form of multimedia information, 3D human mesh recovery technology has also advanced accordingly. However, current methods mainly focus on handling humans wearing tight clothing and perform poorly when estimating body shapes and poses under diverse clothing, especially loose garments. To this end, we make two key insights: (1) tailoring clothing to fit the human body can mitigate the adverse impact of clothing on 3D human mesh recovery, and (2) utilizing human visual information from large foundational models can enhance the generalization ability of the estimation. Based on these insights, we propose ClothHMR, to accurately recover 3D meshes of humans in diverse clothing. ClothHMR primarily consists of two modules: clothing tailoring (CT) and FHVM-based mesh recovering (MR). The CT module employs body semantic estimation and body edge prediction to tailor the clothing, ensuring it fits the body silhouette. The MR module optimizes the initial parameters of the 3D human mesh by continuously aligning the intermediate representations of the 3D mesh with those inferred from the foundational human visual model (FHVM). ClothHMR can accurately recover 3D meshes of humans wearing diverse clothing, precisely estimating their body shapes and poses. Experimental results demonstrate that ClothHMR significantly outperforms existing state-of-the-art methods across benchmark datasets and in-the-wild images. Additionally, a web application for online fashion and shopping powered by ClothHMR is developed, illustrating that ClothHMR can effectively serve real-world usage scenarios. The code and model for ClothHMR are available at: \url{https://github.com/starVisionTeam/ClothHMR}.

</details>


### [225] [A unified FLAIR hyperintensity segmentation model for various CNS tumor types and acquisition time points](https://arxiv.org/abs/2512.17566)
*Mathilde Gajda Faanes,David Bouget,Asgeir S. Jakola,Timothy R. Smith,Vasileios K. Kavouridis,Francesco Latini,Margret Jensdottir,Peter Milos,Henrietta Nittby Redebrandt,Rickard L. Sjöberg,Rupavathana Mahesparan,Lars Kjelsberg Pedersen,Ole Solheim,Ingerid Reinertsen*

Main category: cs.CV

TL;DR: 使用Attention U - Net架构训练统一的FLAIR高信号分割模型，在多种脑肿瘤类型上表现良好且能跨肿瘤类型和采集时间点泛化，已集成到开源软件。


<details>
  <summary>Details</summary>
Motivation: FLAIR MRI扫描对脑肿瘤诊断等很重要，自动分割FLAIR高信号体积在临床有重要应用价值。

Method: 用来自不同中心约5000张不同肿瘤类型和采集时间点的FLAIR图像，使用Attention U - Net架构训练统一分割模型，并与特定数据集模型比较。

Result: 统一模型在多种术前和术后脑肿瘤上取得较高Dice分数，与特定数据集模型性能相当。

Conclusion: 统一模型能跨肿瘤类型和采集时间点泛化，便于临床部署，已集成到Raidionics软件。

Abstract: T2-weighted fluid-attenuated inversion recovery (FLAIR) magnetic resonance imaging (MRI) scans are important for diagnosis, treatment planning and monitoring of brain tumors. Depending on the brain tumor type, the FLAIR hyperintensity volume is an important measure to asses the tumor volume or surrounding edema, and an automatic segmentation of this would be useful in the clinic. In this study, around 5000 FLAIR images of various tumors types and acquisition time points from different centers were used to train a unified FLAIR hyperintensity segmentation model using an Attention U-Net architecture. The performance was compared against dataset specific models, and was validated on different tumor types, acquisition time points and against BraTS. The unified model achieved an average Dice score of 88.65\% for pre-operative meningiomas, 80.08% for pre-operative metastasis, 90.92% for pre-operative and 84.60% for post-operative gliomas from BraTS, and 84.47% for pre-operative and 61.27\% for post-operative lower grade gliomas. In addition, the results showed that the unified model achieved comparable segmentation performance to the dataset specific models on their respective datasets, and enables generalization across tumor types and acquisition time points, which facilitates the deployment in a clinical setting. The model is integrated into Raidionics, an open-source software for CNS tumor analysis.

</details>


### [226] [CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency](https://arxiv.org/abs/2512.17213)
*Xiao Liang,Yuxuan An,Di Wang,Jiawei Hu,Zhicheng Jiao,Bin Jing,Quan Wang*

Main category: cs.CV

TL;DR: 提出CheXPO - v2框架解决医学视觉语言模型幻觉问题，提升推理可靠性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 医学视觉语言模型易产生幻觉，基于结果的强化学习方法（如GRPO）会导致模型‘过度思考’，掩盖事实错误并带来安全风险。

Method: 提出CheXPO - v2框架，采用知识图谱一致性奖励机制和实体 - 关系匹配，将推理步骤解析为三元组进行监督，并结合难例挖掘策略。

Result: 在MIMIC - CXR - VQA等基准测试中显著优于GRPO和现有模型，仅用5k样本就达到新的最先进准确率。

Conclusion: CheXPO - v2框架具有数据效率高、推理可靠的优势，能有效解决医学视觉语言模型的问题。

Abstract: Medical Vision-Language Models (VLMs) are prone to hallucinations, compromising clinical reliability. While reinforcement learning methods like Group Relative Policy Optimization (GRPO) offer a low-cost alignment solution, their reliance on sparse, outcome-based rewards inadvertently encourages models to "overthink" -- generating verbose, convoluted, and unverifiable Chain-of-Thought reasoning to justify answers. This focus on outcomes obscures factual errors and poses significant safety risks. To address this, we propose CheXPO-v2, a novel alignment framework that shifts from outcome to process supervision. Our core innovation is a Knowledge Graph Consistency Reward mechanism driven by Entity-Relation Matching. By explicitly parsing reasoning steps into structured "Disease, Relation, Anatomy" triplets, we provide fine-grained supervision that penalizes incoherent logic and hallucinations at the atomic level. Integrating this with a hard-example mining strategy, our approach significantly outperforms GRPO and state-of-the-art models on benchmarks like MIMIC-CXR-VQA. Crucially, CheXPO-v2 achieves new state-of-the-art accuracy using only 5k samples, demonstrating exceptional data efficiency while producing clinically sound and verifiable reasoning. The project source code is publicly available at: https://github.com/ecoxial2007/CheX-Phi4MM.

</details>


### [227] [MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration](https://arxiv.org/abs/2512.17605)
*Svetlana Krasnova,Emiliya Starikova,Ilia Naletov,Andrey Krylov,Dmitry Sorokin*

Main category: cs.CV

TL;DR: 提出用于乳房X光图像配准的公共基准数据集MGRegBench，对多种配准方法进行基准测试并公开代码和数据。


<details>
  <summary>Details</summary>
Motivation: 乳房X光图像配准临床应用重要，但缺乏公共数据集和标准化基准，现有研究难直接比较。

Method: 构建含超5000个图像对、部分含手动标注的MGRegBench数据集，对经典、基于学习、隐式神经表示等多种配准方法进行基准测试。

Result: 完成了多种方法的基准测试，公开代码和数据。

Conclusion: MGRegBench为公平比较提供基础资源，能推动未来研究。

Abstract: Robust mammography registration is essential for clinical applications like tracking disease progression and monitoring longitudinal changes in breast tissue. However, progress has been limited by the absence of public datasets and standardized benchmarks. Existing studies are often not directly comparable, as they use private data and inconsistent evaluation frameworks. To address this, we present MGRegBench, a public benchmark dataset for mammogram registration. It comprises over 5,000 image pairs, with 100 containing manual anatomical landmarks and segmentation masks for rigorous evaluation. This makes MGRegBench one of the largest public 2D registration datasets with manual annotations. Using this resource, we benchmarked diverse registration methods including classical (ANTs), learning-based (VoxelMorph, TransMorph), implicit neural representation (IDIR), a classic mammography-specific approach, and a recent state-of-the-art deep learning method MammoRegNet. The implementations were adapted to this modality from the authors' implementations or re-implemented from scratch. Our contributions are: (1) the first public dataset of this scale with manual landmarks and masks for mammography registration; (2) the first like-for-like comparison of diverse methods on this modality; and (3) an extensive analysis of deep learning-based registration. We publicly release our code and data to establish a foundational resource for fair comparisons and catalyze future research. The source code and data are at https://github.com/KourtKardash/MGRegBench.

</details>


### [228] [Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation](https://arxiv.org/abs/2512.17673)
*Alexandre Personnic,Mihai Bâce*

Main category: cs.CV

TL;DR: 提出Spatio - Temporal Gaze Network (ST - Gaze)用于视频凝视估计，在EVE数据集上表现出色，为基于视频的凝视估计提供新方向。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视频的凝视估计方法受单帧和多帧特征表示的限制，性能有待提升。

Method: 提出结合CNN骨干网络、通道注意力和自注意力模块的ST - Gaze模型，将融合特征视为空间序列捕获帧内上下文，并在时间上传播以建模帧间动态。

Result: 在EVE数据集上取得了最先进的性能；消融研究表明通过时空循环保留和建模帧内空间上下文优于过早的空间池化。

Conclusion: 研究结果为使用常用相机进行更鲁棒的基于视频的凝视估计奠定基础。

Abstract: Video-based gaze estimation methods aim to capture the inherently temporal dynamics of human eye gaze from multiple image frames. However, since models must capture both spatial and temporal relationships, performance is limited by the feature representations within a frame but also between multiple frames. We propose the Spatio-Temporal Gaze Network (ST-Gaze), a model that combines a CNN backbone with dedicated channel attention and self-attention modules to fuse eye and face features optimally. The fused features are then treated as a spatial sequence, allowing for the capture of an intra-frame context, which is then propagated through time to model inter-frame dynamics. We evaluated our method on the EVE dataset and show that ST-Gaze achieves state-of-the-art performance both with and without person-specific adaptation. Additionally, our ablation study provides further insights into the model performance, showing that preserving and modelling intra-frame spatial context with our spatio-temporal recurrence is fundamentally superior to premature spatial pooling. As such, our results pave the way towards more robust video-based gaze estimation using commonly available cameras.

</details>


### [229] [An Empirical Study of Sampling Hyperparameters in Diffusion-Based Super-Resolution](https://arxiv.org/abs/2512.17675)
*Yudhistira Arief Wibowo*

Main category: cs.CV

TL;DR: 对预训练扩散模型应用条件时，通过FFHQ超分辨率的实证消融研究，发现条件步长比扩散步数对性能影响大，[2.0, 3.0]步长性能最佳。


<details>
  <summary>Details</summary>
Motivation: 现有条件方法（如DPS和MCG）虽能提升重建质量，但引入需精细调整的额外超参数，要找出应用条件到预训练扩散模型时影响性能的主导因素。

Method: 对FFHQ超分辨率进行实证消融研究。

Result: 条件步长比扩散步数对性能影响显著更大，[2.0, 3.0]步长在实验中整体性能最佳。

Conclusion: 在应用条件到预训练扩散模型解决超分辨率问题时，应更关注条件步长的选择。

Abstract: Diffusion models have shown strong potential for solving inverse problems such as single-image super-resolution, where a high-resolution image is recovered from a low-resolution observation using a pretrained unconditional prior. Conditioning methods, including Diffusion Posterior Sampling (DPS) and Manifold Constrained Gradient (MCG), can substantially improve reconstruction quality, but they introduce additional hyperparameters that require careful tuning. In this work, we conduct an empirical ablation study on FFHQ super-resolution to identify the dominant factors affecting performance when applying conditioning to pretrained diffusion models, and show that the conditioning step size has a significantly greater impact than the diffusion step count, with step sizes in the range of [2.0, 3.0] yielding the best overall performance in our experiments.

</details>


### [230] [MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation](https://arxiv.org/abs/2512.17450)
*Jon Muhovič,Janez Perš*

Main category: cs.CV

TL;DR: 提出多模态海上数据集MULTIAQUA并评估多模态方法，简化训练过程。


<details>
  <summary>Details</summary>
Motivation: 无人水面艇运行时部分视觉情况难解读，部分天气和光照条件需额外信息，需扩展可用海上数据。

Method: 提出MULTIAQUA数据集，包含不同模态传感器同步、校准和标注数据；在夜间测试集上评估多模态方法，提出训练方法。

Result: 可让多模态方法训练更稳健，在近乎全黑环境保持可靠性能，仅用白天图像训练深度神经网络。

Conclusion: 方法显著简化数据采集、标注和训练过程。

Abstract: Unmanned surface vehicles can encounter a number of varied visual circumstances during operation, some of which can be very difficult to interpret. While most cases can be solved only using color camera images, some weather and lighting conditions require additional information. To expand the available maritime data, we present a novel multimodal maritime dataset MULTIAQUA (Multimodal Aquatic Dataset). Our dataset contains synchronized, calibrated and annotated data captured by sensors of different modalities, such as RGB, thermal, IR, LIDAR, etc. The dataset is aimed at developing supervised methods that can extract useful information from these modalities in order to provide a high quality of scene interpretation regardless of potentially poor visibility conditions. To illustrate the benefits of the proposed dataset, we evaluate several multimodal methods on our difficult nighttime test set. We present training approaches that enable multimodal methods to be trained in a more robust way, thus enabling them to retain reliable performance even in near-complete darkness. Our approach allows for training a robust deep neural network only using daytime images, thus significantly simplifying data acquisition, annotation, and the training process.

</details>


### [231] [Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image](https://arxiv.org/abs/2512.17773)
*Simon Giebenhain,Tobias Kirschstein,Liam Schoneveld,Davide Davoli,Zhe Chen,Matthias Nießner*

Main category: cs.CV

TL;DR: 提出Pix2NPHM网络，可根据单张图像回归NPHM参数，实现高质量人脸重建且能在野外数据上大规模运行。


<details>
  <summary>Details</summary>
Motivation: NPHMs在拟合视觉输入时因潜在空间的表达性而具有挑战性，需要更有效的方法来解决。

Method: 提出Pix2NPHM这个Vision Transformer网络，利用特定领域预训练的ViT作为骨干，在混合3D数据和大规模2D视频数据集上训练。

Result: 能以交互帧率进行3D重建，可通过后续推理时间优化提高几何保真度。

Conclusion: 实现了前所未有的人脸重建质量，能在野外数据上大规模运行。

Abstract: Neural Parametric Head Models (NPHMs) are a recent advancement over mesh-based 3d morphable models (3DMMs) to facilitate high-fidelity geometric detail. However, fitting NPHMs to visual inputs is notoriously challenging due to the expressive nature of their underlying latent space. To this end, we propose Pix2NPHM, a vision transformer (ViT) network that directly regresses NPHM parameters, given a single image as input. Compared to existing approaches, the neural parametric space allows our method to reconstruct more recognizable facial geometry and accurate facial expressions. For broad generalization, we exploit domain-specific ViTs as backbones, which are pretrained on geometric prediction tasks. We train Pix2NPHM on a mixture of 3D data, including a total of over 100K NPHM registrations that enable direct supervision in SDF space, and large-scale 2D video datasets, for which normal estimates serve as pseudo ground truth geometry. Pix2NPHM not only allows for 3D reconstructions at interactive frame rates, it is also possible to improve geometric fidelity by a subsequent inference-time optimization against estimated surface normals and canonical point maps. As a result, we achieve unprecedented face reconstruction quality that can run at scale on in-the-wild data.

</details>


### [232] [TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis](https://arxiv.org/abs/2512.17488)
*Almustapha A. Wakili,Adamu Hussaini,Abubakar A. Musa,Woosub Jung,Wei Yu*

Main category: cs.CV

TL;DR: 提出TwinSegNet用于脑肿瘤分割，结合ViT - UNet与数字双胞胎，在多数据集表现良好，兼顾隐私与性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在脑肿瘤分割中依赖集中数据收集，存在隐私问题且跨机构泛化能力有限。

Method: 提出TwinSegNet，集成混合ViT - UNet模型与个性化数字双胞胎，结合卷积编码器和Vision Transformer瓶颈，各机构用私有数据微调全局模型形成数字双胞胎。

Result: 在九个异构MRI数据集上评估，Dice分数高达0.90%，灵敏度/特异性超90%，与集中式模型对比展现其在保护隐私时不牺牲性能。

Conclusion: 该方法能在多机构临床环境中实现可扩展、个性化分割，同时满足严格数据保密要求。

Abstract: Brain tumor segmentation is critical in diagnosis and treatment planning for the disease. Yet, current deep learning methods rely on centralized data collection, which raises privacy concerns and limits generalization across diverse institutions. In this paper, we propose TwinSegNet, which is a privacy-preserving federated learning framework that integrates a hybrid ViT-UNet model with personalized digital twins for accurate and real-time brain tumor segmentation. Our architecture combines convolutional encoders with Vision Transformer bottlenecks to capture local and global context. Each institution fine-tunes the global model of private data to form its digital twin. Evaluated on nine heterogeneous MRI datasets, including BraTS 2019-2021 and custom tumor collections, TwinSegNet achieves high Dice scores (up to 0.90%) and sensitivity/specificity exceeding 90%, demonstrating robustness across non-independent and identically distributed (IID) client distributions. Comparative results against centralized models such as TumorVisNet highlight TwinSegNet's effectiveness in preserving privacy without sacrificing performance. Our approach enables scalable, personalized segmentation for multi-institutional clinical settings while adhering to strict data confidentiality requirements.

</details>


### [233] [Animate Any Character in Any World](https://arxiv.org/abs/2512.17796)
*Yitong Wang,Fangyun Wei,Hongyang Zhang,Bo Dai,Yan Lu*

Main category: cs.CV

TL;DR: 本文介绍了AniX，它结合静态世界生成和可控实体模型，支持用户用自然语言控制角色在环境中行动，通过条件自回归视频生成问题合成视频，并进行多方面评估。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型方法存在局限性，静态世界生成模型无主动智能体，可控实体模型仅允单个实体在难以控制的环境中进行有限动作，需要改进。

Method: 引入AniX，结合静态世界生成的真实感和结构基础扩展可控实体模型，将其构造成条件自回归视频生成问题，基于预训练视频生成器训练。

Result: 能合成与场景和角色视觉保真度相符的时间连贯视频剪辑，可维护跨动作和角色的泛化能力。

Conclusion: 评估涵盖视觉质量、角色一致性、动作可控性和长期连贯性等多方面。

Abstract: Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.

</details>


### [234] [InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models](https://arxiv.org/abs/2512.17851)
*Sarah Rastegar,Violeta Chatalbasheva,Sieger Falkena,Anuj Singh,Yanbo Wang,Tejas Gokhale,Hamid Palangi,Hadi Jamali-Rad*

Main category: cs.CV

TL;DR: 提出训练无关的推理时方法InfSplign改善文本到图像扩散模型的空间对齐问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型难以捕捉文本提示中的空间关系，原因是训练数据缺乏细粒度空间监督和文本嵌入无法编码空间语义。

Method: 提出InfSplign，在每个去噪步骤通过复合损失调整噪声，利用从骨干解码器提取的不同级别的交叉注意力图来确保准确的对象放置和采样时对象的平衡存在。

Result: 在VISOR和T2I - CompBench上的评估显示，InfSplign达到新的最优水平，比现有推理时基线有显著性能提升，甚至超过基于微调的方法。

Conclusion: InfSplign是一种轻量级、即插即用且与任何扩散骨干兼容的方法，能有效改善空间对齐问题。

Abstract: Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts. This limitation can be traced to two factors: lack of fine-grained spatial supervision in training data and inability of text embeddings to encode spatial semantics. We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step. Proposed loss leverages different levels of cross-attention maps extracted from the backbone decoder to enforce accurate object placement and a balanced object presence during sampling. The method is lightweight, plug-and-play, and compatible with any diffusion backbone. Our comprehensive evaluations on VISOR and T2I-CompBench show that InfSplign establishes a new state-of-the-art (to the best of our knowledge), achieving substantial performance gains over the strongest existing inference-time baselines and even outperforming the fine-tuning-based methods. Codebase is available at GitHub.

</details>


### [235] [Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN](https://arxiv.org/abs/2512.17864)
*Balram Singh,Ram Prakash Sharma,Somnath Dey*

Main category: cs.CV

TL;DR: 本文提出CBAM - VGG16模型用于植物叶片疾病检测，在多数据集上表现出色，代码开源。


<details>
  <summary>Details</summary>
Motivation: 植物疾病威胁全球粮食安全，需要准确且可解释的疾病检测方法。

Method: 引入可解释的注意力引导CNN模型CBAM - VGG16，在每个卷积阶段集成CBAM模块，在五个不同植物疾病数据集上训练。

Result: 模型优于近期技术，准确率高达98.87%，有较强泛化能力，通过多种方法验证了有效性。

Conclusion: 该研究推动可解释AI在农业诊断中的应用，为智慧农业提供透明可靠系统。

Abstract: Plant diseases pose a significant threat to global food security, necessitating accurate and interpretable disease detection methods. This study introduces an interpretable attention-guided Convolutional Neural Network (CNN), CBAM-VGG16, for plant leaf disease detection. By integrating Convolution Block Attention Module (CBAM) at each convolutional stage, the model enhances feature extraction and disease localization. Trained on five diverse plant disease datasets, our approach outperforms recent techniques, achieving high accuracy (up to 98.87%) and demonstrating robust generalization. Here, we show the effectiveness of our method through comprehensive evaluation and interpretability analysis using CBAM attention maps, Grad-CAM, Grad-CAM++, and Layer-wise Relevance Propagation (LRP). This study advances the application of explainable AI in agricultural diagnostics, offering a transparent and reliable system for smart farming. The code of our proposed work is available at https://github.com/BS0111/PlantAttentionCBAM.

</details>


### [236] [RadarGen: Automotive Radar Point Cloud Generation from Cameras](https://arxiv.org/abs/2512.17897)
*Tomer Borreda,Fangqiang Ding,Sanja Fidler,Shengyu Huang,Or Litany*

Main category: cs.CV

TL;DR: 提出RadarGen扩散模型，从多视图相机图像合成汽车雷达点云，评估显示其缩小与真实数据训练模型差距。


<details>
  <summary>Details</summary>
Motivation: 实现从多视图相机图像合成逼真的汽车雷达点云，提供多模态生成模拟的可扩展方向。

Method: 将高效图像潜在扩散应用于雷达领域，用鸟瞰图表示雷达测量，结合预训练基础模型提取的深度、语义和运动线索引导生成，最后通过轻量级步骤重建点云。

Result: RadarGen能捕捉特征雷达测量分布，缩小与真实数据训练的感知模型的差距。

Conclusion: RadarGen是迈向跨传感模式统一生成模拟的一步。

Abstract: We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.

</details>


### [237] [Adversarial Robustness of Vision in Open Foundation Models](https://arxiv.org/abs/2512.17902)
*Jonathon Fox,William J Buchanan,Pavlos Papadopoulos*

Main category: cs.CV

TL;DR: 文章研究LLaVA - 1.5 - 13B和Meta的Llama 3.2 Vision - 8B - 2的对抗鲁棒性，发现视觉模态可攻击当代开放权重VLM，且对抗鲁棒性与基准性能无直接关联。


<details>
  <summary>Details</summary>
Motivation: 深度学习发展使AI模型理解困难，对手可能修改图像干扰AI识别，需研究模型对抗鲁棒性。

Method: 对视觉输入模态进行无目标PGD测试，在VQA v2数据集子集上进行经验评估，用标准VQA准确率量化攻击结果，并比较LLaVA和Llama 3.2 Vision的准确率下降情况。

Result: Llama 3.2 Vision在攻击下性能下降较小，尤其是在高扰动水平下。

Conclusion: 视觉模态是降低当代开放权重VLM性能的可行攻击向量，对抗鲁棒性不一定与标准基准性能直接相关，可能受架构和训练因素影响。

Abstract: With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.

</details>


### [238] [Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting](https://arxiv.org/abs/2512.17908)
*Ananta R. Bhattarai,Helge Rhodin*

Main category: cs.CV

TL;DR: 提出Re - Depth Anything框架，融合DA - V2与2D扩散模型先验，在多基准测试中提升深度估计准确性和真实感。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型如DA - V2在处理与训练分布差异大的真实图像时存在困难，需弥合领域差距。

Method: 将DA - V2与2D扩散模型先验融合，通过重新光照预测深度图和增强输入进行无标签细化，用SDS利用SfS线索替代传统光度重建，采用冻结编码器、更新中间嵌入并微调解码器的策略。

Result: 在不同基准测试中，相比DA - V2，深度准确性和真实感有显著提升。

Conclusion: 通过增强几何推理，为自监督提供了新途径。

Abstract: Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.

</details>


### [239] [Visually Prompted Benchmarks Are Surprisingly Fragile](https://arxiv.org/abs/2512.17875)
*Haiwen Feng,Long Lian,Lisa Dunlap,Jiahao Shu,XuDong Wang,Renhao Wang,Trevor Darrell,Alane Suhr,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: 评估VLMs时现有视觉提示基准易受无关细节影响，本文评估多个模型，展示基准设置细节对性能和排名影响，创建VPBench缓解不稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决评估VLMs时测试模型独立分析视觉内容能力的挑战，发现现有基准对无关细节的脆弱性。

Method: 在两个视觉提示任务上评估九个常用开源和闭源VLMs，分析基准设置细节和低阶推理选择对模型的影响。

Result: 基准设置细节（如视觉标记设计、数据集大小）和低阶推理选择（如JPEG压缩级别）会显著影响模型性能和排名，这些影响在视觉提示基准中比传统语义评估更大。

Conclusion: 创建VPBench这个更大的视觉提示基准及相关分析工具来缓解基准的不稳定性。

Abstract: A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [240] [Sandwiched and Silent: Behavioral Adaptation and Private Channel Exploitation in Ethereum MEV](https://arxiv.org/abs/2512.17602)
*Davide Mancino,Davide Rezzoli*

Main category: cs.CR

TL;DR: 本文利用2024年11月至2025年2月交易数据，量化用户被三明治攻击后的适应情况，发现约40%受害者60天内转用私密路由，流失率先升后降，指出私密路由不能保证免受MEV提取。


<details>
  <summary>Details</summary>
Motivation: 当前用户被三明治攻击后的适应情况不明，需要进行实证量化。

Method: 使用2024年11月至2025年2月交易数据，结合内存池可见性和ZeroMEV标签，跟踪用户被第n次公开三明治攻击后的结果，包括重新激活和首次采用私密路由。

Result: 约40%受害者60天内转用私密路由，多次攻击后升至54%；流失率首次攻击后达7.5%后降至1 - 2%；2024年11 - 12月确认2932次私密三明治攻击，造成损失和攻击者获利情况，单个机器人占近三分之二私密抢先交易，活动集中在少量DEX池。

Conclusion: 私密路由不能保证免受MEV提取，需持续监控和协议级防御。

Abstract: How users adapt after being sandwiched remains unclear; this paper provides an empirical quantification. Using transaction level data from November 2024 to February 2025, enriched with mempool visibility and ZeroMEV labels, we track user outcomes after their n-th public sandwich: (i) reactivation, i.e., the resumption of on-chain activity within a 60-day window, and (ii) first-time adoption of private routing. We refer to users who do not reactivate within this window as churned, and to users experiencing multiple attacks (n>1) as undergoing repeated exposure. Our analysis reveals measurable behavioral adaptation: around 40% of victims migrate to private routing within 60 days, rising to 54% with repeated exposures. Churn peaks at 7.5% after the first sandwich but declines to 1-2%, consistent with survivor bias. In Nov-Dec 2024 we confirm 2,932 private sandwich attacks affecting 3,126 private victim transactions, producing \$409,236 in losses and \$293,786 in attacker profits. A single bot accounts for nearly two-thirds of private frontruns, and private sandwich activity is heavily concentrated on a small set of DEX pools. These results highlight that private routing does not guarantee protection from MEV extraction: while execution failures push users toward private channels, these remain exploitable and highly concentrated, demanding continuous monitoring and protocol-level defenses.

</details>


### [241] [Sedna: Sharding transactions in multiple concurrent proposer blockchains](https://arxiv.org/abs/2512.17045)
*Alejandro Ranchal-Pedrosa,Benjamin Marsh,Lefteris Kokoris-Kogias,Alberto Sonnino*

Main category: cs.CR

TL;DR: 提出Sedna协议，用可验证无速率编码替代交易复制，提升效率并减少MEV暴露，无需修改共识可增量部署。


<details>
  <summary>Details</summary>
Motivation: 现代区块链采用多提议者共识，但用户交易传播存在问题，面临审查抗性、低延迟和合理成本的三难困境。

Method: 提出用户端协议Sedna，用可验证无速率编码替代简单交易复制，用户私下向部分提议者交付符号包。

Result: 证明Sedna保证活性和直到解码的隐私性，显著减少MEV暴露；带宽开销接近信息论下限，比简单复制效率提升2 - 3倍。

Conclusion: Sedna无需修改共识，可进行增量部署。

Abstract: Modern blockchains increasingly adopt multi-proposer (MCP) consensus to remove single-leader bottlenecks and improve censorship resistance. However, MCP alone does not resolve how users should disseminate transactions to proposers. Today, users either naively replicate full transactions to many proposers, sacrificing goodput and exposing payloads to MEV, or target few proposers and accept weak censorship and latency guarantees. This yields a practical trilemma among censorship resistance, low latency, and reasonable cost (in fees or system goodput).
  We present Sedna, a user-facing protocol that replaces naive transaction replication with verifiable, rateless coding. Users privately deliver addressed symbol bundles to subsets of proposers; execution follows a deterministic order once enough symbols are finalized to decode. We prove Sedna guarantees liveness and \emph{until-decode privacy}, significantly reducing MEV exposure. Analytically, the protocol approaches the information-theoretic lower bound for bandwidth overhead, yielding a 2-3x efficiency improvement over naive replication. Sedna requires no consensus modifications, enabling incremental deployment.

</details>


### [242] [Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning](https://arxiv.org/abs/2512.17254)
*Baolei Zhang,Minghong Fang,Zhuqing Liu,Biao Yi,Peizhao Zhou,Yuan Wang,Tong Li,Zheli Liu*

Main category: cs.CR

TL;DR: 提出ABBR框架用于拜占庭鲁棒且隐私保护的联邦学习，利用降维加速计算，有自适应调整策略，运行快、通信开销小且有拜占庭抗性。


<details>
  <summary>Details</summary>
Motivation: 现有针对后门和隐私推理攻击的防御方法存在显著计算和通信开销，理论与实践有差距。

Method: 利用降维加速隐私保护联邦学习中复杂过滤规则的私有计算，分析低维空间向量过滤的精度损失并引入自适应调整策略。

Result: 在公共数据集上实现并评估，运行显著更快，通信开销极小，与基线保持相近的拜占庭抗性。

Conclusion: ABBR框架是一种实用的拜占庭鲁棒且隐私保护的联邦学习解决方案。

Abstract: Federated Learning (FL) allows multiple clients to collaboratively train a model without sharing their private data. However, FL is vulnerable to Byzantine attacks, where adversaries manipulate client models to compromise the federated model, and privacy inference attacks, where adversaries exploit client models to infer private data. Existing defenses against both backdoor and privacy inference attacks introduce significant computational and communication overhead, creating a gap between theory and practice. To address this, we propose ABBR, a practical framework for Byzantine-robust and privacy-preserving FL. We are the first to utilize dimensionality reduction to speed up the private computation of complex filtering rules in privacy-preserving FL. Additionally, we analyze the accuracy loss of vector-wise filtering in low-dimensional space and introduce an adaptive tuning strategy to minimize the impact of malicious models that bypass filtering on the global model. We implement ABBR with state-of-the-art Byzantine-robust aggregation rules and evaluate it on public datasets, showing that it runs significantly faster, has minimal communication overhead, and maintains nearly the same Byzantine-resilience as the baselines.

</details>


### [243] [AutoDFBench 1.0: A Benchmarking Framework for Digital Forensic Tool Testing and Generated Code Evaluation](https://arxiv.org/abs/2512.16965)
*Akila Wickramasekara,Tharusha Mihiranga,Aruna Withanage,Buddhima Weerasinghe,Frank Breitinger,John Sheppard,Mark Scanlon*

Main category: cs.CR

TL;DR: 本文介绍了用于数字取证工具测试和验证的模块化基准框架AutoDFBench 1.0，它支持多种技术评估，经CFTT数据集验证，可实现公平、可复现的工具比较。


<details>
  <summary>Details</summary>
Motivation: 当前NIST的CFTT项目缺乏全面的自动化基准测试框架，导致验证不一致、工具比较困难和可重复性有限，因此需要创建新框架。

Method: 引入AutoDFBench 1.0框架，集成CFTT项目的五个领域，包含大量测试用例和场景，通过RESTful API进行评估并输出标准化指标。

Result: 框架经CFTT数据集验证，能够实现工具和脚本之间公平、可复现的比较。

Conclusion: AutoDFBench 1.0是数字取证工具测试和验证的第一个统一、自动化和可扩展的基准框架，有助于对数字取证技术进行透明、可复现和可比较的评估。

Abstract: The National Institute of Standards and Technology (NIST) Computer Forensic Tool Testing (CFTT) programme has become the de facto standard for providing digital forensic tool testing and validation. However to date, no comprehensive framework exists to automate benchmarking across the diverse forensic tasks included in the programme. This gap results in inconsistent validation, challenges in comparing tools, and limited validation reproducibility. This paper introduces AutoDFBench 1.0, a modular benchmarking framework that supports the evaluation of both conventional DF tools and scripts, as well as AI-generated code and agentic approaches. The framework integrates five areas defined by the CFTT programme: string search, deleted file recovery, file carving, Windows registry recovery, and SQLite data recovery. AutoDFBench 1.0 includes ground truth data comprising of 63 test cases and 10,968 unique test scenarios, and execute evaluations through a RESTful API that produces structured JSON outputs with standardised metrics, including precision, recall, and F1~score for each test case, and the average of these F1~scores becomes the AutoDFBench Score. The benchmarking framework is validated against CFTT's datasets. The framework enables fair and reproducible comparison across tools and forensic scripts, establishing the first unified, automated, and extensible benchmarking framework for digital forensic tool testing and validation. AutoDFBench 1.0 supports tool vendors, researchers, practitioners, and standardisation bodies by facilitating transparent, reproducible, and comparable assessments of DF technologies.

</details>


### [244] [MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval](https://arxiv.org/abs/2512.16962)
*Saksham Sahai Srivastava,Haoyu He*

Main category: cs.CR

TL;DR: 本文提出针对大型语言模型（LLM）代理内存的MemoryGraft攻击，利用代理语义模仿启发式，通过植入恶意经验使代理行为持久偏移，在MetaGPT的DataInterpreter代理上验证有效。


<details>
  <summary>Details</summary>
Motivation: LLM代理依赖长期记忆和RAG提升自治性，但引入了推理核心与其过去之间信任边界的攻击面，当前未被探索。

Method: 提出MemoryGraft间接注入攻击，利用代理语义模仿启发式，向代理长期记忆植入恶意成功经验，诱导其构建中毒RAG存储。

Result: 在MetaGPT的DataInterpreter代理上验证，少量中毒记录在正常工作负载检索经验中占比大，使基于经验的自我提升成为隐蔽持久攻击途径。

Conclusion: MemoryGraft攻击是一种有效的针对LLM代理长期记忆的攻击方式，凸显该领域安全防护重要性，代码和评估数据公开以利后续研究。

Abstract: Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.

</details>


### [245] [Adversarial VR: An Open-Source Testbed for Evaluating Adversarial Robustness of VR Cybersickness Detection and Mitigation](https://arxiv.org/abs/2512.17029)
*Istiak Ahmed,Ripan Kumar Kundu,Khaza Anuarul Hoque*

Main category: cs.CR

TL;DR: 本文提出Adversarial - VR测试平台评估基于深度学习的网络晕动症检测与缓解策略在对抗条件下的鲁棒性，结果显示对抗攻击能成功欺骗系统。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的网络晕动症检测系统易受对抗攻击影响，且缺乏评估其鲁棒性的开源测试平台，限制了对其实效性的评估。

Method: 开发基于Unity的Adversarial - VR测试平台，集成DeepTCN和Transformer模型用于实时检测，采用MI - FGSM、PGD和C&W三种对抗攻击，使用自定义VR迷宫模拟和HTC Vive Pro Eye耳机实现攻击并开源代码。

Result: 对抗攻击能成功欺骗系统，如C&W攻击使基于Transformer的网络晕动症模型准确率下降5.94倍。

Conclusion: Adversarial - VR测试平台可用于评估基于深度学习的网络晕动症检测与缓解策略在对抗条件下的鲁棒性。

Abstract: Deep learning (DL)-based automated cybersickness detection methods, along with adaptive mitigation techniques, can enhance user comfort and interaction. However, recent studies show that these DL-based systems are susceptible to adversarial attacks; small perturbations to sensor inputs can degrade model performance, trigger incorrect mitigation, and disrupt the user's immersive experience (UIX). Additionally, there is a lack of dedicated open-source testbeds that evaluate the robustness of these systems under adversarial conditions, limiting the ability to assess their real-world effectiveness. To address this gap, this paper introduces Adversarial-VR, a novel real-time VR testbed for evaluating DL-based cybersickness detection and mitigation strategies under adversarial conditions. Developed in Unity, the testbed integrates two state-of-the-art (SOTA) DL models: DeepTCN and Transformer, which are trained on the open-source MazeSick dataset, for real-time cybersickness severity detection and applies a dynamic visual tunneling mechanism that adjusts the field-of-view based on model outputs. To assess robustness, we incorporate three SOTA adversarial attacks: MI-FGSM, PGD, and C&W, which successfully prevent cybersickness mitigation by fooling DL-based cybersickness models' outcomes. We implement these attacks using a testbed with a custom-built VR Maze simulation and an HTC Vive Pro Eye headset, and we open-source our implementation for widespread adoption by VR developers and researchers. Results show that these adversarial attacks are capable of successfully fooling the system. For instance, the C&W attack results in a $5.94x decrease in accuracy for the Transformer-based cybersickness model compared to the accuracy without the attack.

</details>


### [246] [AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs](https://arxiv.org/abs/2512.17251)
*Madhava Gaikwad*

Main category: cs.CR

TL;DR: 提出AlignDP混合隐私锁，在数据接口阻止知识转移，模拟证明可行。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临提取、蒸馏和未经授权微调风险，现有防御方法在数据泄露后起作用。

Method: 设计AlignDP，分离稀有和非稀有字段，分别用PAC不可区分性和RAPPOR处理，用全局聚合器执行组合和预算。

Result: 证明PAC扩展到全局聚合的限制，给出RAPPOR估计的界限，分析效用权衡，模拟表明稀有类别隐藏，频繁类别恢复误差小。

Conclusion: AlignDP能在数据接口有效阻止知识转移，具有可行性。

Abstract: Large language models are exposed to risks of extraction, distillation, and unauthorized fine-tuning. Existing defenses use watermarking or monitoring, but these act after leakage. We design AlignDP, a hybrid privacy lock that blocks knowledge transfer at the data interface. The key idea is to separate rare and non-rare fields. Rare fields are shielded by PAC indistinguishability, giving effective zero-epsilon local DP. Non-rare fields are privatized with RAPPOR, giving unbiased frequency estimates under local DP. A global aggregator enforces composition and budget. This two-tier design hides rare events and adds controlled noise to frequent events. We prove limits of PAC extension to global aggregation, give bounds for RAPPOR estimates, and analyze utility trade-off. A toy simulation confirms feasibility: rare categories remain hidden, frequent categories are recovered with small error.

</details>


### [247] [Detection and Analysis of Sensitive and Illegal Content on the Ethereum Blockchain Using Machine Learning Techniques](https://arxiv.org/abs/2512.17411)
*Xingyu Feng*

Main category: cs.CR

TL;DR: 研究聚焦以太坊，用数据识别与恢复算法处理链上数据，发现好坏内容并存，提出预防措施。


<details>
  <summary>Details</summary>
Motivation: 区块链去中心化结构使恶意或非法内容可能存在，需识别和解决相关问题。

Method: 提出数据识别与恢复算法，用FastText进行情感分析，用NSFWJS库检测不雅图片。

Result: 恢复多种文件，情感分析准确率0.9，发现好坏内容并存，敏感信息涉及中国政府官员。

Conclusion: 研究提出预防措施，算法为区块链数据隐私和安全提供创新解决方案。

Abstract: Blockchain technology, lauded for its transparent and immutable nature, introduces a novel trust model. However, its decentralized structure raises concerns about potential inclusion of malicious or illegal content. This study focuses on Ethereum, presenting a data identification and restoration algorithm. Successfully recovering 175 common files, 296 images, and 91,206 texts, we employed the FastText algorithm for sentiment analysis, achieving a 0.9 accuracy after parameter tuning. Classification revealed 70,189 neutral, 5,208 positive, and 15,810 negative texts, aiding in identifying sensitive or illicit information. Leveraging the NSFWJS library, we detected seven indecent images with 100% accuracy. Our findings expose the coexistence of benign and harmful content on the Ethereum blockchain, including personal data, explicit images, divisive language, and racial discrimination. Notably, sensitive information targeted Chinese government officials. Proposing preventative measures, our study offers valuable insights for public comprehension of blockchain technology and regulatory agency guidance. The algorithms employed present innovative solutions to address blockchain data privacy and security concerns.

</details>


### [248] [Key-Conditioned Orthonormal Transform Gating (K-OTG): Multi-Key Access Control with Hidden-State Scrambling for LoRA-Tuned Models](https://arxiv.org/abs/2512.17519)
*Muhammad Haris Khan*

Main category: cs.CR

TL;DR: 提出K - OTG机制在指令调优语言模型中实现密钥访问控制，评估显示其能防止未授权使用并保留授权效用。


<details>
  <summary>Details</summary>
Motivation: 在指令调优语言模型中实现秘密密钥访问控制。

Method: K - OTG在双路径语料上训练，推理时用预lm_head钩子对隐藏状态进行正交变换，密钥不设为特殊令牌，可与4位基础的LoRA结合。

Result: 授权效用接近基础模型，未授权效用崩溃；解锁矩阵对角占优；授权块发射为0；贪心输出跨随机数匹配；Python级钩子有40%的运行时开销。

Conclusion: K - OTG是一种实用、与模型无关的方法，可防止未授权使用并保留授权效用。

Abstract: We present a simple, PEFT-compatible mechanism that enforces secret-key access control in instruction-tuned language models. K-OTG trains on a dual-path corpus: authorized examples (prefixed with a role key) learn the task output, while unauthorized examples learn a visible block token. At inference, a pre-lm_head hook applies an orthonormal transform to the hidden state: with the correct key/role the inverse map restores the model's native basis; otherwise a session-ephemeral scrambler (permutation, sign flips, Householders) makes logits uninformative and the system short-circuits to BLOCK. Keys are not added as special tokens, and the method composes cleanly with LoRA on 4-bit bases. We evaluate an hour-scale protocol on 1-3B-class instruction models (Llama 3.2, Qwen2.5 1.5B) across utility (XSum ROUGE/BLEU, GSM8K accuracy, WikiText-2 perplexity), selectivity (3by3 role-key unlock matrices), nonce invariance, block suppression, and throughput. Authorized utility remains close to the base on summarization with the expected modest PPL increase from instruction tuning; unauthorized utility collapses (near-zero sequence metrics with exploding PPL), indicating practical unusability without the key. Unlock matrices are diagonally dominant (high on-target unlock, low cross-unlock), authorized block emission is 0 per N via robust bad-word lists, and greedy outputs match exactly across nonces, confirming correct inverse cancellation. The runtime overhead of the Python-level hook is 40% tokens per sec versus the base. K-OTG therefore provides a pragmatic, model-agnostic way to prevent unauthorized use while preserving authorized utility.

</details>


### [249] [Biosecurity-Aware AI: Agentic Risk Auditing of Soft Prompt Attacks on ESM-Based Variant Predictors](https://arxiv.org/abs/2512.17146)
*Huixin Zhan*

Main category: cs.CR

TL;DR: 本文引入SAGE框架审计基因组基础模型（GFMs）对抗性漏洞，发现即使是先进的GFMs也对攻击敏感。


<details>
  <summary>Details</summary>
Motivation: 现有GFMs在对抗性操纵下的安全性和鲁棒性研究不足，需进行审计。

Method: 引入SAGE框架，通过可解释和自动化的风险审计循环，注入软提示扰动、监测模型行为、计算风险指标并生成报告。

Result: 发现如ESM2等先进的GFMs对目标软提示攻击敏感，性能有可衡量的下降。

Conclusion: 揭示了GFMs中关键且此前隐藏的漏洞，表明代理风险审计对保障生物医学应用安全很重要。

Abstract: Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated remarkable success in variant effect prediction. However, their security and robustness under adversarial manipulation remain largely unexplored. To address this gap, we introduce the Secure Agentic Genomic Evaluator (SAGE), an agentic framework for auditing the adversarial vulnerabilities of GFMs. SAGE functions through an interpretable and automated risk auditing loop. It injects soft prompt perturbations, monitors model behavior across training checkpoints, computes risk metrics such as AUROC and AUPR, and generates structured reports with large language model-based narrative explanations. This agentic process enables continuous evaluation of embedding-space robustness without modifying the underlying model. Using SAGE, we find that even state-of-the-art GFMs like ESM2 are sensitive to targeted soft prompt attacks, resulting in measurable performance degradation. These findings reveal critical and previously hidden vulnerabilities in genomic foundation models, showing the importance of agentic risk auditing in securing biomedical applications such as clinical variant interpretation.

</details>


### [250] [MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification](https://arxiv.org/abs/2512.17594)
*Tosin Ige,Christopher Kiekintveld,Aritran Piplai,Asif Rahman,Olukunle Kolade,Sasidhar Kunapuli*

Main category: cs.CR

TL;DR: 提出MADOOD框架用于恶意软件OOD检测和分类，在基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的恶意软件检测器因闭集假设，难以应对多态和变形恶意软件变体的类内变异，在面对未知恶意软件家族时性能下降。

Method: 提出两阶段、聚类驱动的深度学习框架MADOOD。第一阶段用高斯判别分析的类条件球形决策边界建模恶意软件家族嵌入，用Z分数距离分析识别异常样本；第二阶段用深度神经网络融合聚类预测、精炼嵌入和监督分类器输出以提高分类精度。

Result: 在包含25个已知家族和多个新OOD变体的基准数据集上，MADOOD显著优于现有OOD检测方法，在未知恶意软件家族上AUC达0.911。

Conclusion: 该框架为不断演变的网络安全环境中的恶意软件检测和异常识别提供了可扩展、可解释且有统计依据的解决方案。

Abstract: Out of distribution (OOD) detection remains a critical challenge in malware classification due to the substantial intra family variability introduced by polymorphic and metamorphic malware variants. Most existing deep learning based malware detectors rely on closed world assumptions and fail to adequately model this intra class variation, resulting in degraded performance when confronted with previously unseen malware families. This paper presents MADOOD, a novel two stage, cluster driven deep learning framework for robust OOD malware detection and classification. In the first stage, malware family embeddings are modeled using class conditional spherical decision boundaries derived from Gaussian Discriminant Analysis (GDA), enabling statistically grounded separation of indistribution and OOD samples without requiring OOD data during training. Z score based distance analysis across multiple class centroids is employed to reliably identify anomalous samples in the latent space. In the second stage, a deep neural network integrates cluster based predictions, refined embeddings, and supervised classifier outputs to enhance final classification accuracy. Extensive evaluations on benchmark malware datasets comprising 25 known families and multiple novel OOD variants demonstrate that MADOOD significantly outperforms state of the art OOD detection methods, achieving an AUC of up to 0.911 on unseen malware families. The proposed framework provides a scalable, interpretable, and statistically principled solution for real world malware detection and anomaly identification in evolving cybersecurity environments.

</details>


### [251] [STAR: Semantic-Traffic Alignment and Retrieval for Zero-Shot HTTPS Website Fingerprinting](https://arxiv.org/abs/2512.17667)
*Yifei Cheng,Yujia Zhu,Baiyang Li,Xinhao Deng,Yitong Cai,Yaochen Ren,Qingyun Liu*

Main category: cs.CR

TL;DR: 本文提出STAR解决现有网站指纹识别（WF）方法的局限性，在实验中表现出色，还揭示加密HTTPS流量隐私风险并开源数据代码。


<details>
  <summary>Details</summary>
Motivation: 现有WF方法依赖特定网站标记的流量数据，扩展性受限且无法处理未见过的网站，需要改进。

Method: 将WF问题重新定义为零样本跨模态检索问题，采用双编码器架构学习加密流量痕迹和爬取时逻辑配置文件的联合嵌入空间，并使用对比和一致性目标及结构感知增强进行训练。

Result: 在1600个未见过的网站实验中，STAR在开放世界检测中top - 1准确率达87.9%，AUC为0.963，添加适配器后top - 5准确率提升至98.8%。

Conclusion: 现代网络协议中存在内在语义 - 流量对齐，语义泄漏是加密HTTPS流量的主要隐私风险，同时公开STAR数据集和代码以支持研究。

Abstract: Modern HTTPS mechanisms such as Encrypted Client Hello (ECH) and encrypted DNS improve privacy but remain vulnerable to website fingerprinting (WF) attacks, where adversaries infer visited sites from encrypted traffic patterns. Existing WF methods rely on supervised learning with site-specific labeled traces, which limits scalability and fails to handle previously unseen websites. We address these limitations by reformulating WF as a zero-shot cross-modal retrieval problem and introducing STAR. STAR learns a joint embedding space for encrypted traffic traces and crawl-time logic profiles using a dual-encoder architecture. Trained on 150K automatically collected traffic-logic pairs with contrastive and consistency objectives and structure-aware augmentation, STAR retrieves the most semantically aligned profile for a trace without requiring target-side traffic during training. Experiments on 1,600 unseen websites show that STAR achieves 87.9 percent top-1 accuracy and 0.963 AUC in open-world detection, outperforming supervised and few-shot baselines. Adding an adapter with only four labeled traces per site further boosts top-5 accuracy to 98.8 percent. Our analysis reveals intrinsic semantic-traffic alignment in modern web protocols, identifying semantic leakage as the dominant privacy risk in encrypted HTTPS traffic. We release STAR's datasets and code to support reproducibility and future research.

</details>


### [252] [Digital and Web Forensics Model Cards, V1](https://arxiv.org/abs/2512.17722)
*Paola Di Maio*

Main category: cs.CR

TL;DR: 本文介绍专为数字和网络取证设计的标准化模型卡框架，含词汇表和生成工具，邀社区反馈完善。


<details>
  <summary>Details</summary>
Motivation: 为数字和网络取证领域提供标准化模型卡框架。

Method: 基于既有模型卡方法和数字取证分析抽象模型工作，构建基于网络的框架，包含控制词汇表和生成工具。

Result: 描述了模型卡结构，展示控制词汇表，推出生成工具的测试版。

Conclusion: 需社区反馈来完善这一新兴标准，同时指出反欺诈和数字网络取证过程存在被不法分子控制的系统风险。

Abstract: This paper introduces a standardized model card framework specifically designed for digital and web forensics. Building upon established model card methodologies and recent work on abstract models for digital forensic analysis, this paper presents a web based framework that generates model cards specifically designed to represent knowledge in the forensic domain. The framework includes controlled vocabularies for classification, reasoning types, bias identification, and error categorization, along with a web-based generator tool to facilitate adoption. The paper describes the model card structure, presents the controlled vocabularies, and introduces the beta version of the generator tool, inviting community feedback to refine this emerging standard. Ultimately, the systemic risk is that that the anti fraud and digital and web forensics processes are controlled by the mobs.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [253] [On General Linearly Implicit Quantized State System Methods](https://arxiv.org/abs/2512.17855)
*Mariana Bergonzi,Joaquín Fernández,Ernesto Kofman*

Main category: math.NA

TL;DR: 提出基于状态量化开发常微分方程数值积分算法的方法，设计新算法并研究其特性与优势。


<details>
  <summary>Details</summary>
Motivation: 改进现有LIQSS方法的性能，同时保留其稳定性、全局误差界和高效事件处理能力等特性。

Method: 基于状态量化的方法，推广LIQSS方法概念，设计两个新的算法子族。

Result: 设计出的新算法改进了当前LIQSS方法的性能，通过两个应用实例研究了新算法特性并分析了其相对经典算法的优势。

Conclusion: 所提方法可行，新算法在性能上有提升，具备一定优势。

Abstract: This work proposes a methodology to develop new numerical integration algorithms for ordinary differential equations based on state quantization, generalizing the notions of Linearly Implicit Quantized State Systems (LIQSS) methods. Using this idea, two novel sub-families of algorithms are designed that improve the performance of current LIQSS methods while preserving their properties regarding stability, global error bound and efficient event handling capabilities. The features of the new algorithms are studied in two application examples where the advantages over classic numerical integration algorithms is also analyzed.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [254] [Towards Sharp Minimax Risk Bounds for Operator Learning](https://arxiv.org/abs/2512.17805)
*Ben Adcock,Gregor Maier,Rahul Parhi*

Main category: math.ST

TL;DR: 本文为算子学习发展了极小极大理论，给出有界Lipschitz算子估计的信息论上下界，指出样本复杂性困境，对不同协方差谱衰减情况有刻画。


<details>
  <summary>Details</summary>
Motivation: 从有限的带噪输入 - 输出样本中估计可分希尔伯特空间之间的未知算子。

Method: 对于一致有界的Lipschitz算子，证明信息论下界以及匹配或接近匹配的上界，考虑不同噪声类型和设计方式。

Result: 速率由定义误差度量的测度的协方差算子的谱控制；存在样本复杂性困境，即通用Lipschitz算子的极小极大风险无法以代数速率随样本量减小；在协方差谱指数衰减时得到本质上精确的刻画，在较慢衰减情况下给出一般上下界。

Conclusion: 成功建立算子学习的极小极大理论，明确样本复杂性问题，对不同协方差谱衰减情况有深入分析。

Abstract: We develop a minimax theory for operator learning, where the goal is to estimate an unknown operator between separable Hilbert spaces from finitely many noisy input-output samples. For uniformly bounded Lipschitz operators, we prove information-theoretic lower bounds together with matching or near-matching upper bounds, covering both fixed and random designs under Hilbert-valued Gaussian noise and Gaussian white noise errors. The rates are controlled by the spectrum of the covariance operator of the measure that defines the error metric. Our setup is very general and allows for measures with unbounded support. A key implication is a curse of sample complexity which shows that the minimax risk for generic Lipschitz operators cannot decay at any algebraic rate in the sample size. We obtain essentially sharp characterizations when the covariance spectrum decays exponentially and provide general upper and lower bounds in slower-decay regimes.

</details>
