<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 101]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 11]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [quant-ph](#quant-ph) [Total: 15]
- [cs.ET](#cs.ET) [Total: 1]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.HC](#cs.HC) [Total: 6]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CR](#cs.CR) [Total: 10]
- [physics.soc-ph](#physics.soc-ph) [Total: 3]
- [cs.CV](#cs.CV) [Total: 30]
- [math.CO](#math.CO) [Total: 2]
- [cs.CL](#cs.CL) [Total: 52]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 6]
- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [econ.GN](#econ.GN) [Total: 7]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 7]
- [math.ST](#math.ST) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning](https://arxiv.org/abs/2510.06261)
*Zhanke Zhou,Chentao Cao,Xiao Feng,Xuan Li,Zongze Li,Xiangyu Lu,Jiangchao Yao,Weikai Huang,Linrui Xu,Tian Cheng,Guanyu Jiang,Yiming Zheng,Brando Miranda,Tongliang Liu,Sanmi Koyejo,Masashi Sugiyama,Bo Han*

Main category: cs.AI

TL;DR: 介绍了自进化代理推理系统AlphaApollo，能解决基础模型推理瓶颈，结合工具实现推理，评估有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型推理中模型内在能力有限和测试时迭代不可靠的两个瓶颈。

Method: 协调多个模型与专业工具，耦合计算和检索工具，通过共享状态图支持多轮、多模型解决方案进化。

Result: 在AIME 2024/2025评估中，不同模型有显著提升，超80%工具调用成功，优于非工具基线。

Conclusion: AlphaApollo提升了基础模型的能力上限。

Abstract: We present AlphaApollo, a self-evolving agentic reasoning system that aims to
address two bottlenecks in foundation model (FM) reasoning-limited
model-intrinsic capacity and unreliable test-time iteration. AlphaApollo
orchestrates multiple models with professional tools to enable deliberate,
verifiable reasoning. It couples (i) a computation tool (Python with numerical
and symbolic libraries) and (ii) a retrieval tool (task-relevant external
information) to execute exact calculations and ground decisions. The system
further supports multi-round, multi-model solution evolution via a shared state
map that records candidates, executable checks, and feedback for iterative
refinement. In evaluations on AIME 2024/2025 across multiple models,
AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32
for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for
Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool
calls are successfully executed, with consistent outperformance of non-tool
baselines, thereby lifting the capability ceiling of FMs. More empirical
results and implementation details will be updated at
https://github.com/tmlr-group/AlphaApollo.

</details>


### [2] [Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization](https://arxiv.org/abs/2510.06274)
*Mohammad Mahdi Samiei Paqaleh,Arash Marioriyad,Arman Tahmasebi-Zadeh,Mohamadreza Fereydooni,Mahdi Ghaznavai,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: 提出Complexity OoD泛化框架定义和衡量推理能力，统一学习与推理，并给出实践建议，指出提升鲁棒推理需考虑复杂度的架构和训练机制。


<details>
  <summary>Details</summary>
Motivation: 当前推理能力缺乏清晰一致的定义和度量，需解决这一问题。

Method: 提出Complexity OoD泛化框架，通过解描述的Kolmogorov复杂度和操作代理来形式化复杂度，统一学习和推理。

Result: 明确Complexity OoD与长度和组合OoD的区别，给出跨堆栈操作Complexity OoD的建议。

Conclusion: 仅靠扩展数据无法解决Complexity OoD，提升鲁棒推理需要明确考虑复杂度进行计算分配的架构和训练机制。

Abstract: Recent progress has pushed AI frontiers from pattern recognition tasks toward
problems that require step by step, System2 style reasoning, especially with
large language models. Yet, unlike learning, where generalization and out of
distribution (OoD) evaluation concepts are well formalized, there is no clear,
consistent definition or metric for reasoning ability. We propose Complexity
Out of Distribution (Complexity OoD) generalization as a framework and problem
setting to define and measure reasoning. A model exhibits Complexity OoD
generalization when it maintains performance on test instances whose minimal
required solution complexity, either representational (richer solution
structure) or computational (more reasoning steps/program length), exceeds that
of all training examples. We formalize complexity via solution description
Kolmogorov complexity and operational proxies (e.g., object/relation counts;
reasoning step counts), clarifying how Complexity OoD differs from length and
compositional OoD. This lens unifies learning and reasoning: many cases
solvable with System1 like processing at low complexity become System2 like
under complexity pressure, while System2 can be viewed as generalization over
solution structures. We translate this perspective into practice with
recommendations for operationalizing Complexity OoD across the stack:
incorporating complexity into benchmark and evaluation metric design,
rethinking supervision to target solution traces, seeking and designing
inductive biases for Complexity OoD generalization, addressing learning to
reason spillovers such as spurious shortcuts, semantic robustness, catastrophic
forgetting, and step wise calibration. Because Complexity OoD cannot be solved
by scaling data alone, progress toward robust reasoning will require
architectures and training regimes that explicitly model and allocate
computation with respect to complexity.

</details>


### [3] [BuilderBench -- A benchmark for generalist agents](https://arxiv.org/abs/2510.06288)
*Raj Ghugare,Catherine Ji,Kathryn Wantlin,Jin Schofield,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: 介绍用于开放式探索的智能体预训练基准BuilderBench，含硬件加速模拟器和任务套件，实验显示任务有挑战性并提供训练协议和算法实现。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型难以解决超出数据范围问题，需要可扩展的通过交互学习的机制，加速智能体预训练研究。

Method: 引入BuilderBench基准，包含硬件加速模拟器和任务套件，让智能体无监督探索学习，求解任务需具具身推理能力。

Result: 实验表明任务对当前算法有挑战，提供训练协议和单文件算法实现。

Conclusion: BuilderBench可助力智能体预训练研究，解决开放式探索学习问题。

Abstract: Today's AI models learn primarily through mimicry and sharpening, so it is
not surprising that they struggle to solve problems beyond the limits set by
existing data. To solve novel problems, agents should acquire skills for
exploring and learning through experience. Finding a scalable learning
mechanism for developing agents that learn through interaction remains a major
open problem. In this work, we introduce BuilderBench, a benchmark to
accelerate research into agent pre-training that centers open-ended
exploration. BuilderBench requires agents to learn how to build any structure
using blocks. BuilderBench is equipped with $(1)$ a hardware accelerated
simulator of a robotic agent interacting with various physical blocks, and
$(2)$ a task-suite with over 42 diverse target structures that are carefully
curated to test an understanding of physics, mathematics, and long-horizon
planning. During training, agents have to explore and learn general principles
about the environment without any external supervision. During evaluation,
agents have to build the unseen target structures from the task suite. Solving
these tasks requires a sort of \emph{embodied reasoning} that is not reflected
in words but rather in actions, experimenting with different strategies and
piecing them together. Our experiments show that many of these tasks challenge
the current iteration of algorithms. Hence, we also provide a ``training
wheels'' protocol, in which agents are trained and evaluated to build a single
target structure from the task suite. Finally, we provide single-file
implementations of six different algorithms as a reference point for
researchers.

</details>


### [4] [Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration](https://arxiv.org/abs/2510.06302)
*Ksenija Lace,Marite Kirikova*

Main category: cs.AI

TL;DR: 论文探讨如何用基于游戏的学习设计解决信息系统整合方法培训的问题，分析相关理论确定设计框架要求并给出开发评估计划。


<details>
  <summary>Details</summary>
Motivation: 企业并购后信息系统整合培训存在空白，现有方法培训学习曲线高、学习者动力低。

Method: 分析基础学习理论、认知负荷和动机模型以及严肃游戏设计框架，将需求分为转换过程和学习体验两部分。

Result: 确定了适用于并购后信息系统整合的基于游戏的学习设计框架的基本要求。

Conclusion: 提出通过迭代设计和实际验证来开发和评估该框架的计划。

Abstract: Post-merger integration states unique challenges for professionals
responsible for information system integration aimed on alignment and
combination diverse system architectures of merging organizations. Although the
theoretical and practical guidance exists for post-merger integration on the
business level, there is a significant gap in training for information system
integration in this context. In prior research specific methods AMILI (Support
method for informed decision identification) and AMILP (Support method for
informed decision-making) were introduced for the support of information system
integration decisions in the post-merger integration. But during the practical
application was reported high learning curve and low learner motivation. This
paper explores how game-based learning design can address these limitations by
transforming static method training into engaging learning experience. The
study analyzes foundational learning theories, cognitive load and motivation
models, and serious game design frameworks to identify the essential
requirements for a game-based learning design framework tailored to information
system integration in post-merger integration. Requirements are structured in
two components: the transformation process and resulting learning experience.
The paper concludes with a plan for developing and evaluating the proposed
framework through iterative design and real-world validation.

</details>


### [5] [Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks](https://arxiv.org/abs/2510.06307)
*Wentao Deng,Jiahuan Pei,Zhiwei Xu,Zhaochun Ren,Zhumin Chen,Pengjie Ren*

Main category: cs.AI

TL;DR: 提出BCCS框架解决多智能体系统共识寻求问题，实验显示其在基准数据集上优于现有结果。


<details>
  <summary>Details</summary>
Motivation: 现有共识寻求方法依赖投票机制，忽视系统内部信念矛盾，且统一交互难以找到最优协作伙伴，影响共识稳定性。

Method: 提供选择最优协作伙伴的理论框架，提出BCCS框架，通过选择最优协作伙伴和校准系统内部信念来促进稳定共识。

Result: 在MATH和MMLU基准数据集的挑战性任务上，BCCS框架准确率分别比现有最佳结果高2.23%和3.95%。

Conclusion: BCCS框架能有效解决多智能体系统共识寻求问题，提升共识稳定性。

Abstract: A multi-agent system (MAS) enhances its capacity to solve complex natural
language processing (NLP) tasks through collaboration among multiple agents,
where consensus-seeking serves as a fundamental mechanism. However, existing
consensus-seeking approaches typically rely on voting mechanisms to judge
consensus, overlooking contradictions in system-internal beliefs that
destabilize the consensus. Moreover, these methods often involve agents
updating their results through indiscriminate collaboration with every other
agent. Such uniform interaction fails to identify the optimal collaborators for
each agent, hindering the emergence of a stable consensus. To address these
challenges, we provide a theoretical framework for selecting optimal
collaborators that maximize consensus stability. Based on the theorems, we
propose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate
stable consensus via selecting optimal collaborators and calibrating the
consensus judgment by system-internal beliefs. Experimental results on the MATH
and MMLU benchmark datasets demonstrate that the proposed BCCS framework
outperforms the best existing results by 2.23% and 3.95% of accuracy on
challenging tasks, respectively. Our code and data are available at
https://github.com/dengwentao99/BCCS.

</details>


### [6] [Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?](https://arxiv.org/abs/2510.06410)
*Aochong Oliver Li,Tanya Goyal*

Main category: cs.AI

TL;DR: 研究标准单推理训练管道能否产生所需的离轨推理行为，通过双测试评估15个大模型，发现基准测试中‘更强’的模型在干扰下更脆弱，模型难以利用协作步骤，研究训练因素影响，为评估多模型协作奠定基础。


<details>
  <summary>Details</summary>
Motivation: 探讨标准单推理训练管道能否实现离轨推理行为，以支持多模型在共享轨迹中协作。

Method: 提出Recoverability和Guidability双测试，评估15个开放权重的大模型，并进行控制研究分析训练因素影响。

Result: 基准测试中‘更强’的模型在干扰下更脆弱，所有测试模型在超出自身能力问题上利用协作步骤的解决率低于9.2%，训练因素会影响离轨推理行为。

Conclusion: 为评估多模型在共享推理轨迹中的协作奠定基础，指出现成推理大模型的局限性。

Abstract: Reasoning LLMs are trained to verbalize their reasoning process, yielding
strong gains on complex tasks. This transparency also opens a promising
direction: multiple reasoners can directly collaborate on each other's thinking
within a shared trajectory, yielding better inference efficiency and
exploration. A key prerequisite, however, is the ability to assess the
usefulness and build on another model's partial thinking -- we call this
off-trajectory reasoning. Our paper investigates a critical question: can
standard solo-reasoning training pipelines deliver desired off-trajectory
behaviors? We propose twin tests that capture the two extremes of the
off-trajectory spectrum, namely Recoverability, which tests whether LLMs can
backtrack from "distractions" induced by misleading reasoning traces, and
Guidability, which tests their ability to build upon correct reasoning from
stronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and
reveals a counterintuitive finding -- "stronger" LLMs on benchmarks are often
more fragile under distraction. Moreover, all models tested fail to effectively
leverage guiding steps from collaborators on problems beyond their inherent
capabilities with solve rates remaining under 9.2%. Finally, we conduct control
studies to isolate the effects of three factors in post-training on these
behaviors: the choice of distillation teacher, the use of RL, and data
selection strategy. Our results provide actionable insights for training
natively strong reasoning collaborators; e.g., we find that suboptimal
recoverability behaviors of teacher models are transferred to distilled
students even if the distillation trajectories are correct. Taken together,
this work lays the groundwork for evaluating multi-model collaborations in
shared reasoning trajectories and highlights the limitations of off-the-shelf
reasoning LLMs.

</details>


### [7] [Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health](https://arxiv.org/abs/2510.06433)
*Aryan Singh Dalal,Yinglun Zhang,Duru Doğan,Atalay Mert İleri,Hande Küçük McGinty*

Main category: cs.AI

TL;DR: 本文聚焦利用语义网以标准化、机器可读格式表示食物与健康关系，创建知识图谱关联食物与健康，并以黄酮类食物与癌症关系为例，未来将拓展图谱。


<details>
  <summary>Details</summary>
Motivation: 过去对利用语义网以标准化、机器可读格式表示食物与健康关系的研究较少，为填补此空白开展研究。

Method: 使用KNARM方法仔细研究食物（USDA数据库中含黄酮类食物）与健康（文献中癌症关联）关系，并以机器可操作格式表示。

Result: 创建了知识图谱，可作为范例供研究人员探索饮食选择与疾病管理的复杂相互作用。

Conclusion: 未来需扩大知识图谱范围，捕捉细微差别、添加更多相关数据并进行推理以发现隐藏关系。

Abstract: The focus on "food as medicine" is gaining traction in the field of health
and several studies conducted in the past few years discussed this aspect of
food in the literature. However, very little research has been done on
representing the relationship between food and health in a standardized,
machine-readable format using a semantic web that can help us leverage this
knowledge effectively. To address this gap, this study aims to create a
knowledge graph to link food and health through the knowledge graph's ability
to combine information from various platforms focusing on flavonoid contents of
food found in the USDA databases and cancer connections found in the
literature. We looked closely at these relationships using KNARM methodology
and represented them in machine-operable format. The proposed knowledge graph
serves as an example for researchers, enabling them to explore the complex
interplay between dietary choices and disease management. Future work for this
study involves expanding the scope of the knowledge graph by capturing nuances,
adding more related data, and performing inferences on the acquired knowledge
to uncover hidden relationships.

</details>


### [8] [PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles](https://arxiv.org/abs/2510.06475)
*Yitao Long,Yuru Jiang,Hongjun Liu,Yilun Zhao,Jingchen Sun,Yiqiu Shen,Chen Zhao,Arman Cohan,Dennis Shasha*

Main category: cs.AI

TL;DR: 本文引入PuzzlePlex基准评估基础模型推理和规划能力及可扩展性，分析前沿模型表现和扩展限制，为模型改进提供指导。


<details>
  <summary>Details</summary>
Motivation: 研究基础模型在复杂动态环境中的推理、规划能力及可扩展性。

Method: 引入含15种谜题的PuzzlePlex基准，实现定制策略对比，开发细粒度指标，在指令和代码两种设置下分析前沿模型。

Result: 推理模型在指令设置中表现更好，代码执行虽挑战大但有可扩展性和高效性。

Conclusion: PuzzlePlex可针对性评估基础模型，指导其推理、规划和泛化能力的未来改进。

Abstract: This work investigates the reasoning and planning capabilities of foundation
models and their scalability in complex, dynamic environments. We introduce
PuzzlePlex, a benchmark designed to assess these capabilities through a diverse
set of puzzles. PuzzlePlex consists of 15 types of puzzles, including
deterministic and stochastic games of varying difficulty, as well as
single-player and two-player scenarios. The PuzzlePlex framework provides a
comprehensive environment for each game, and supports extensibility to generate
more challenging instances as foundation models evolve. Additionally, we
implement customized game-playing strategies for comparison. Building on this
benchmark, we develop fine-grained metrics to measure performance and conduct
an in-depth analysis of frontier foundation models across two settings:
instruction-based and code-based. Furthermore, we systematically investigate
their scaling limits. Our findings show that reasoning models outperform others
in instruction-based settings, while code-based execution presents greater
challenges but offers a scalable and efficient alternative. PuzzlePlex enables
targeted evaluation and guides future improvements in reasoning, planning, and
generalization for foundation models.

</details>


### [9] [Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them](https://arxiv.org/abs/2510.06534)
*Jiahe Jin,Abhijay Paladugu,Chenyan Xiong*

Main category: cs.AI

TL;DR: 本文提出推理驱动的基于大语言模型的流程研究代理搜索中的推理行为模式，识别四种有益推理行为，提出行为预训练技术，实验显示该技术使模型性能提升，揭示推理行为是关键因素。


<details>
  <summary>Details</summary>
Motivation: 代理搜索为大语言模型的推理和代理能力带来挑战，需要研究有效的推理行为模式。

Method: 提出推理驱动的管道分析成功的代理搜索轨迹，识别有益推理行为，提出行为预训练技术，通过监督微调后进行强化学习。

Result: 在三个基准测试中，行为预训练使Llama3.2 - 3B和Qwen3 - 1.7B性能提升超35%，含期望推理行为的SFT数据而非最终答案正确性是关键因素。

Conclusion: 识别的推理行为赋予模型更有效的探索和测试时扩展能力，为强化学习奠定基础，代码将开源。

Abstract: Agentic search leverages large language models (LLMs) to interpret complex
user information needs and execute a multi-step process of planning, searching,
and synthesizing information to provide answers. This paradigm introduces
unique challenges for LLMs' reasoning and agentic capabilities when interacting
with retrieval systems and the broader web. In this paper, we propose a
reasoning-driven LLM-based pipeline to study effective reasoning behavior
patterns in agentic search. Using this pipeline, we analyze successful agentic
search trajectories and identify four beneficial reasoning behaviors:
Information Verification, Authority Evaluation, Adaptive Search, and Error
Recovery. Based on these findings, we propose a technique called Behavior
Priming to train more effective agentic search models. It synthesizes agentic
search trajectories that exhibit these four behaviors and integrates them into
the agentic search model through supervised fine-tuning (SFT), followed by
standard reinforcement learning (RL). Experiments on three benchmarks (GAIA,
WebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in
Llama3.2-3B and Qwen3-1.7B compared to directly training agentic search models
with RL. Crucially, we demonstrate that the desired reasoning behaviors in the
SFT data, rather than the correctness of the final answer, is the critical
factor for achieving strong final performance after RL: fine-tuning on
trajectories with desirable reasoning behaviors but incorrect answers leads to
better performance than fine-tuning on trajectories with correct answers. Our
analysis further reveals the underlying mechanism: the introduced reasoning
behaviors endow models with more effective exploration (higher pass@k and
entropy) and test-time scaling (longer trajectories) capabilities, providing a
strong foundation for RL. Our code will be released as open source.

</details>


### [10] [Auto-Prompt Ensemble for LLM Judge](https://arxiv.org/abs/2510.06538)
*Jiajie Li,Huayi Zhang,Peng Lin,Jinjun Xiong,Wei Xu*

Main category: cs.AI

TL;DR: 提出Auto - Prompt Ensemble (APE)框架提升大语言模型（LLM）评判可靠性，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评判常遗漏关键评估维度，因无法识别隐含人类评估标准。

Method: 提出APE自适应框架，从失败案例中学习评估维度，采用基于置信度的集成机制和Collective Confidence方法决定何时采用额外评估维度。

Result: 在多个标准基准测试中提升了LLM评判的可靠性，如在零样本设置下将GPT - 4o在Reward Bench上的一致率从87.2%提高到90.5%。

Conclusion: APE为LLM评判提供了利用测试时计算的原则性方法，缩小了人类和LLM评判的评估差距。

Abstract: We present a novel framework that improves the reliability of LLM judges by
selectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM
judges often miss crucial evaluation dimensions because they fail to recognize
the implicit standards underlying human assessments. To address this challenge,
we propose the Auto-Prompt Ensemble (APE), an adaptive framework that
automatically learns evaluation dimensions from its failure cases. APE
incorporates a confidence-based ensemble mechanism to decide when to adopt the
judgments from additional evaluation dimensions through a novel confidence
estimation approach called Collective Confidence. Extensive experiments
demonstrate that APE improves the reliability of LLM Judge across diverse
standard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward
Bench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a
principled approach for LLM Judge to leverage test-time computation, and bridge
the evaluation gap between human and LLM judges.

</details>


### [11] [WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks](https://arxiv.org/abs/2510.06587)
*Jingbo Yang,Bairu Hou,Wei Wei,Shiyu Chang,Yujia Bao*

Main category: cs.AI

TL;DR: 提出WebDART框架让大语言模型处理复杂网络任务，在WebChoreArena上提升成功率，减少导航步骤。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理在长时导航、大规模信息提取和约束推理的复杂网络任务上表现不佳，需新方法。

Method: WebDART动态将目标分解为导航、信息提取和执行三个子任务，随新网页出现重新规划分解。

Result: 在WebChoreArena上成功率比之前SOTA代理最多提升13.7个百分点，在WebArena上表现相当，完成任务导航步骤最多减少14.7步。

Conclusion: WebDART框架能有效提升大语言模型处理复杂网络任务的能力。

Abstract: Large language model (LLM) agents are becoming competent at straightforward
web tasks, such as opening an item page or submitting a form, but still
struggle with objectives that require long horizon navigation, large scale
information extraction, and reasoning under constraints. We present WebDART, a
general framework that enables a single LLM to handle such complex chores.
WebDART (i) dynamically decomposes each objective into three focused subtasks:
navigation, information extraction, and execution, so the model concentrates on
one skill at a time, and (ii) continuously replans the decomposition as new
webpages are revealed, taking advantage of newly discovered filters or
shortcuts and avoiding redundant exploration. Evaluated on WebChoreArena,
WebDART lifts success rates by up to 13.7 percentage points over previous SOTA
agents, while matching their performance on the easier WebArena suite and
completing tasks with up to 14.7 fewer navigation steps.

</details>


### [12] [Fine-Grained Emotion Recognition via In-Context Learning](https://arxiv.org/abs/2510.06600)
*Zhaochun Ren,Zhou Yang,Chenglong Ye,Haizhou Sun,Chao Chen,Xiaofei Zhu,Xiangwen Liao*

Main category: cs.AI

TL;DR: 本文指出当前细粒度情感识别中ICL方法忽视决策过程的问题，提出EICL方法优化推理和决策，实验表明EICL效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有方法在细粒度情感识别中增强推理过程时忽略了决策过程，影响识别准确性。

Method: 提出Emotion In-Context Learning (EICL)，引入情感相似示例和动态软标签策略改善推理，用两阶段排除策略优化决策。

Result: 在多个数据集上，EICL显著优于ICL。

Conclusion: EICL能有效解决现有方法问题，提升细粒度情感识别性能。

Abstract: Fine-grained emotion recognition aims to identify the emotional type in
queries through reasoning and decision-making processes, playing a crucial role
in various systems. Recent methods use In-Context Learning (ICL), enhancing the
representation of queries in the reasoning process through semantically similar
examples, while further improving emotion recognition by explaining the
reasoning mechanisms. However, these methods enhance the reasoning process but
overlook the decision-making process. This paper investigates decision-making
in fine-grained emotion recognition through prototype theory. We show that ICL
relies on similarity matching between query representations and emotional
prototypes within the model, where emotion-accurate representations are
critical. However, semantically similar examples often introduce emotional
discrepancies, hindering accurate representations and causing errors. To
address this, we propose Emotion In-Context Learning (EICL), which introduces
emotionally similar examples and uses a dynamic soft-label strategy to improve
query representations in the emotion reasoning process. A two-stage exclusion
strategy is then employed to assess similarity from multiple angles, further
optimizing the decision-making process. Extensive experiments show that EICL
significantly outperforms ICL on multiple datasets.

</details>


### [13] [Agent-in-the-Loop: A Data Flywheel for Continuous Improvement in LLM-based Customer Support](https://arxiv.org/abs/2510.06674)
*Cen,Zhao,Tiantian Zhang,Hanchen Su,Yufeng,Zhang,Shaowei Su,Mingzhi Xu,Yu,Liu,Wei Han,Jeremy Werner,Claire Na Cheng,Yashar Mehdad*

Main category: cs.AI

TL;DR: 提出AITL框架，将四种注释集成到客户运营中，减少模型再训练周期，生产试点显示在检索准确性、生成质量和代理采用率上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 迭代改进基于大语言模型的客户支持系统，解决标准离线方法依赖批量注释的问题。

Method: 引入Agent - in - the - Loop (AITL)框架，将四种关键类型的注释集成到实时客户运营中，并将反馈信号用于模型更新。

Result: 生产试点中检索准确性（recall@75提升11.7%，precision@8提升14.8%）、生成质量（有用性提升8.4%）和代理采用率（提升4.5%）显著提高。

Conclusion: 将人类反馈循环直接嵌入运营工作流可有效持续优化基于大语言模型的客户支持系统。

Abstract: We introduce an Agent-in-the-Loop (AITL) framework that implements a
continuous data flywheel for iteratively improving an LLM-based customer
support system. Unlike standard offline approaches that rely on batch
annotations, AITL integrates four key types of annotations directly into live
customer operations: (1) pairwise response preferences, (2) agent adoption and
rationales, (3) knowledge relevance checks, and (4) identification of missing
knowledge. These feedback signals seamlessly feed back into models' updates,
reducing retraining cycles from months to weeks. Our production pilot involving
US-based customer support agents demonstrated significant improvements in
retrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation quality
(+8.4% helpfulness) and agent adoption rates (+4.5%). These results underscore
the effectiveness of embedding human feedback loops directly into operational
workflows to continuously refine LLM-based customer support system.

</details>


### [14] [Inefficiencies of Meta Agents for Agent Design](https://arxiv.org/abs/2510.06711)
*Batu El,Mert Yuksekgonul,James Zou*

Main category: cs.AI

TL;DR: 本文研究了一类常见元智能体在自动化设计智能体系统中的三个关键挑战。


<details>
  <summary>Details</summary>
Motivation: 近期有工作开始用元智能体自动设计智能体系统，本文旨在研究常见元智能体面临的关键挑战。

Method: 研究元智能体跨迭代学习方式，对比不同方法；分析设计智能体的行为多样性；评估自动化设计的经济可行性。

Result: 跨迭代学习中进化方法效果更好；设计的智能体行为多样性低；仅在两个数据集上自动化设计部署成本低于人工设计。

Conclusion: 元智能体在跨迭代学习、设计智能体多样性及经济可行性方面存在挑战。

Abstract: Recent works began to automate the design of agentic systems using
meta-agents that propose and iteratively refine new agent architectures. In
this paper, we examine three key challenges in a common class of meta-agents.
First, we investigate how a meta-agent learns across iterations and find that
simply expanding the context with all previous agents, as proposed by previous
works, performs worse than ignoring prior designs entirely. We show that the
performance improves with an evolutionary approach. Second, although the
meta-agent designs multiple agents during training, it typically commits to a
single agent at test time. We find that the designed agents have low behavioral
diversity, limiting the potential for their complementary use. Third, we assess
when automated design is economically viable. We find that only in a few
cases--specifically, two datasets--the overall cost of designing and deploying
the agents is lower than that of human-designed agents when deployed on over
15,000 examples. In contrast, the performance gains for other datasets do not
justify the design cost, regardless of scale.

</details>


### [15] [MultiCNKG: Integrating Cognitive Neuroscience, Gene, and Disease Knowledge Graphs Using Large Language Models](https://arxiv.org/abs/2510.06742)
*Ali Sarabadani,Kheirolah Rahsepar Fard*

Main category: cs.AI

TL;DR: 文章介绍MultiCNKG框架，融合多知识源构建知识图谱，经评估性能良好，可推动多领域应用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推动下，为克服传统机器学习方法不足，构建能关联基因、疾病和认知过程语义链接的知识图谱。

Method: 引入MultiCNKG框架，融合CNKG、GO和DO三个知识源，利用GPT - 4进行实体对齐、语义相似度计算和图增强。

Result: MultiCNKG有6.9K节点和11.3K边，评估指标表现良好，链接预测性能有竞争力。

Conclusion: MultiCNKG能推动个性化医疗、认知障碍诊断和认知神经科学假设制定等应用。

Abstract: The advent of large language models (LLMs) has revolutionized the integration
of knowledge graphs (KGs) in biomedical and cognitive sciences, overcoming
limitations in traditional machine learning methods for capturing intricate
semantic links among genes, diseases, and cognitive processes. We introduce
MultiCNKG, an innovative framework that merges three key knowledge sources: the
Cognitive Neuroscience Knowledge Graph (CNKG) with 2.9K nodes and 4.3K edges
across 9 node types and 20 edge types; Gene Ontology (GO) featuring 43K nodes
and 75K edges in 3 node types and 4 edge types; and Disease Ontology (DO)
comprising 11.2K nodes and 8.8K edges with 1 node type and 2 edge types.
Leveraging LLMs like GPT-4, we conduct entity alignment, semantic similarity
computation, and graph augmentation to create a cohesive KG that interconnects
genetic mechanisms, neurological disorders, and cognitive functions. The
resulting MultiCNKG encompasses 6.9K nodes across 5 types (e.g., Genes,
Diseases, Cognitive Processes) and 11.3K edges spanning 7 types (e.g., Causes,
Associated with, Regulates), facilitating a multi-layered view from molecular
to behavioral domains. Assessments using metrics such as precision (85.20%),
recall (87.30%), coverage (92.18%), graph consistency (82.50%), novelty
detection (40.28%), and expert validation (89.50%) affirm its robustness and
coherence. Link prediction evaluations with models like TransE (MR: 391, MRR:
0.411) and RotatE (MR: 263, MRR: 0.395) show competitive performance against
benchmarks like FB15k-237 and WN18RR. This KG advances applications in
personalized medicine, cognitive disorder diagnostics, and hypothesis
formulation in cognitive neuroscience.

</details>


### [16] [Verifying Memoryless Sequential Decision-making of Large Language Models](https://arxiv.org/abs/2510.06756)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: 介绍用于无记忆顺序决策任务中基于大语言模型（LLM）策略的严格自动验证工具，实验表明Ollama访问的开源LLM可验证但性能不及深度强化学习基线。


<details>
  <summary>Details</summary>
Motivation: 对无记忆顺序决策任务中基于LLM的策略进行严格自动验证。

Method: 根据马尔可夫决策过程、LLM策略和安全要求，以LLM选择的动作引导增量构建MDP可达部分，编码状态为自然语言提示，解析LLM响应为动作并扩展可达后继状态，用Storm检查形式模型。

Result: 在标准网格世界基准实验中，通过Ollama访问的开源LLM在确定性种子下可验证，但性能普遍低于深度强化学习基线。

Conclusion: 工具与Ollama原生集成，支持PRISM指定任务，为连续基准测试和正式验证更强大的LLM奠定基础。

Abstract: We introduce a tool for rigorous and automated verification of large language
model (LLM)- based policies in memoryless sequential decision-making tasks.
Given a Markov decision process (MDP) representing the sequential
decision-making task, an LLM policy, and a safety requirement expressed as a
PCTL formula, our approach incrementally constructs only the reachable portion
of the MDP guided by the LLM's chosen actions. Each state is encoded as a
natural language prompt, the LLM's response is parsed into an action, and
reachable successor states by the policy are expanded. The resulting formal
model is checked with Storm to determine whether the policy satisfies the
specified safety property. In experiments on standard grid world benchmarks, we
show that open source LLMs accessed via Ollama can be verified when
deterministically seeded, but generally underperform deep reinforcement
learning baselines. Our tool natively integrates with Ollama and supports
PRISM-specified tasks, enabling continuous benchmarking in user-specified
sequential decision-making tasks and laying a practical foundation for formally
verifying increasingly capable LLMs.

</details>


### [17] [Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration](https://arxiv.org/abs/2510.06761)
*Zhi Zhang,Yan Liu,Zhejing Hu,Gong Chen,Sheng-hua Zhong,Jiannong Cao*

Main category: cs.AI

TL;DR: 提出DLMA框架自动解决科研问题，经实验验证效果佳。


<details>
  <summary>Details</summary>
Motivation: 解决端到端科研过程自动化中制定新颖合理计划和在动态不确定条件下正确执行计划的双重挑战。

Method: 提出DLMA框架，领导循环由教授代理负责制定研究计划，采用进化算法迭代生成和完善提案；跟随循环由博士生代理负责执行最佳计划，实施中动态调整。

Result: 在ACLAward和Laboratory等基准测试上，DLMA生成的研究论文在自动评估中达到了最先进的分数，显著优于强基线。

Conclusion: 消融实验证实两个循环都至关重要，进化驱动新颖性，执行确保合理性。

Abstract: Automating the end-to-end scientific research process poses a fundamental
challenge: it requires both evolving high-level plans that are novel and sound,
and executing these plans correctly amidst dynamic and uncertain conditions. To
address this bilevel challenge, we propose a novel Double-Loop Multi-Agent
(DLMA) framework to solve the given research problem automatically. The leader
loop, composed of professor agents, is responsible for evolving research plans.
It employs an evolutionary algorithm through involvement, improvement, and
integration meetings to iteratively generate and refine a pool of research
proposals, exploring the solution space effectively. The follower loop,
composed of doctoral student agents, is responsible for executing the
best-evolved plan. It dynamically adjusts the plan during implementation via
pre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is
well-supported by contextual and external observations. Extensive experiments
on benchmarks like ACLAward and Laboratory show that DLMA generates research
papers that achieve state-of-the-art scores in automated evaluation,
significantly outperforming strong baselines. Ablation studies confirm the
critical roles of both loops, with evolution driving novelty and execution
ensuring soundness.

</details>


### [18] [Autoformalizer with Tool Feedback](https://arxiv.org/abs/2510.06857)
*Qi Guo,Jianing Wang,Jianfei Zhang,Deyang Kong,Xiangzhou Huang,Xiangyu Xi,Wei Wang,Jingang Wang,Xunliang Cai,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: 提出带工具反馈的自动形式化器ATF，结合语法和一致性信息，实验表现优于基线模型，开源数据集。


<details>
  <summary>Details</summary>
Motivation: 现有形式化器难以生成符合语法有效性和语义一致性的有效语句。

Method: 将语法和一致性信息作为工具融入形式化过程，结合Lean 4编译器和多LLMs评判方法，分阶段训练模型。

Result: ATF显著优于一系列基线形式化器模型，经人工评估验证，且有良好推理扩展特性。

Conclusion: ATF有效提升自动形式化的语法有效性和语义一致性，开源数据集助力相关研究。

Abstract: Autoformalization addresses the scarcity of data for Automated Theorem
Proving (ATP) by translating mathematical problems from natural language into
formal statements. Efforts in recent work shift from directly prompting large
language models to training an end-to-end formalizer model from scratch,
achieving remarkable advancements. However, existing formalizer still struggles
to consistently generate valid statements that meet syntactic validity and
semantic consistency. To address this issue, we propose the Autoformalizer with
Tool Feedback (ATF), a novel approach that incorporates syntactic and
consistency information as tools into the formalization process. By integrating
Lean 4 compilers for syntax corrections and employing a multi-LLMs-as-judge
approach for consistency validation, the model is able to adaptively refine
generated statements according to the tool feedback, enhancing both syntactic
validity and semantic consistency. The training of ATF involves a cold-start
phase on synthetic tool-calling data, an expert iteration phase to improve
formalization capabilities, and Direct Preference Optimization to alleviate
ineffective revisions. Experimental results show that ATF markedly outperforms
a range of baseline formalizer models, with its superior performance further
validated by human evaluations. Subsequent analysis reveals that ATF
demonstrates excellent inference scaling properties. Moreover, we open-source
Numina-ATF, a dataset containing 750K synthetic formal statements to facilitate
advancements in autoformalization and ATP research.

</details>


### [19] [TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs](https://arxiv.org/abs/2510.06878)
*Daria Ozerova,Ekaterina Trofimova*

Main category: cs.AI

TL;DR: 提出Tree - Guided Policy Refinement (TGPR)框架提升大语言模型迭代细化能力，在多个基准测试上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型迭代细化方法在搜索可能的细化空间时依赖预定义启发式方法，存在探索 - 利用困境且无法根据过往细化结果调整。

Method: 引入TGPR框架，结合GRPO与基于汤普森采样的树搜索，主动探索成功和失败的细化路径。

Result: 在HumanEval、MBPP和APPS基准测试中，相比GRPO基线，pass@1最高提升4.2个百分点，pass@10最高提升12.51个百分点。

Conclusion: TGPR提供了一个通用框架，可增强大语言模型的迭代细化和状态推理能力。

Abstract: Iterative refinement has been a promising paradigm to enable large language
models (LLMs) to resolve difficult reasoning and problem-solving tasks. One of
the key challenges, however, is how to effectively search through the enormous
search space of possible refinements. Existing methods typically fall back on
predefined heuristics, which are troubled by the exploration-exploitation
dilemma and cannot adapt based on past refinement outcomes. We introduce
Tree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with
a Thompson-Sampling-based tree search. TGPR explores both failed and successful
refinement paths actively, with denser training trajectories and more adaptive
policies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to
+4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to
+12.51 percentage points absolute improvement in pass@10 (on APPS) compared to
a competitive GRPO baseline. Apart from debugging code, TGPR focuses on a
principled approach to combining learned policies with structured search
methods, offering a general framework for enhancing iterative refinement and
stateful reasoning in LLMs.

</details>


### [20] [LLM-Assisted Modeling of Semantic Web-Enabled Multi-Agents Systems with AJAN](https://arxiv.org/abs/2510.06911)
*Hacane Hechehouche,Andre Antakli,Matthias Klusch*

Main category: cs.AI

TL;DR: 本文提出集成开发环境，克服AJAN代理建模障碍，拓展用户群体。


<details>
  <summary>Details</summary>
Motivation: AJAN框架基于语义Web标准构建多智能体系统，但RDF/RDFS和SPARQL的代理行为定义存在障碍，如处理URI易出错、复杂SPARQL查询学习成本高。

Method: 提出一个集成开发环境，利用大语言模型进行智能体工程。

Result: 未提及具体结果。

Conclusion: 提出的集成开发环境可克服AJAN代理建模的障碍，拓展用户群体。

Abstract: There are many established semantic Web standards for implementing
multi-agent driven applications. The AJAN framework allows to engineer
multi-agent systems based on these standards. In particular, agent knowledge is
represented in RDF/RDFS and OWL, while agent behavior models are defined with
Behavior Trees and SPARQL to access and manipulate this knowledge. However, the
appropriate definition of RDF/RDFS and SPARQL-based agent behaviors still
remains a major hurdle not only for agent modelers in practice. For example,
dealing with URIs is very error-prone regarding typos and dealing with complex
SPARQL queries in large-scale environments requires a high learning curve. In
this paper, we present an integrated development environment to overcome such
hurdles of modeling AJAN agents and at the same time to extend the user
community for AJAN by the possibility to leverage Large Language Models for
agent engineering.

</details>


### [21] [Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.06953)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.AI

TL;DR: 本文研究大语言模型推理轨迹中步骤级信息密度均匀性与推理质量的关系，提出基于熵的度量方法，实验表明均匀性可提升推理性能，是构建可靠推理系统的标准。


<details>
  <summary>Details</summary>
Motivation: 探讨在大语言模型推理轨迹中，步骤级信息密度均匀性是否反映推理质量。

Method: 提出基于熵的逐步信息密度度量，引入局部和全局均匀性分数两个互补度量。

Result: 步骤级均匀性有理论价值和实际性能提升，如在AIME2025上选择更均匀轨迹准确率相对提升10 - 32%；正确推理轨迹避免信息密度尖峰，错误轨迹有不规则信息爆发。

Conclusion: 受UID启发的信息密度度量优于其他内部信号，信息密度均匀性是构建可靠准确推理系统的稳健诊断和选择标准。

Abstract: The Uniform Information Density (UID) hypothesis suggests that effective
communication maintains a stable flow of information. In this work, we revisit
this principle in the context of large language model (LLM) reasoning traces,
asking whether step-level uniformity reflects reasoning quality. To this end,
we propose an entropy-based stepwise information density metric and introduce
two complementary measures of uniformity, local and global uniformity scores.
Across the experiments on six different reasoning benchmarks, we find that
step-level uniformity not only provides a strong theoretical lens but also
yields practical performance benefits; for example, selecting reasoning traces
with more uniform information density at the step-level improves accuracy by
10-32\% relative gains over baselines at AIME2025. Our analysis further reveals
that correct reasoning traces tend to avoid sharp information density spikes,
while incorrect traces exhibit irregular information bursts. These results
demonstrate that UID-inspired information density measures outperform
alternative internal signals as predictors of reasoning quality. Results
highlight the uniformity of the information density as a robust diagnostic and
selection criterion for building more reliable and accurate reasoning systems.

</details>


### [22] [Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning](https://arxiv.org/abs/2510.07038)
*Wenxun Wu,Yuanyang Li,Guhan Chen,Linyue Wang,Hongyang Chen*

Main category: cs.AI

TL;DR: 提出Tool - Augmented Policy Optimization (TAPO)框架，引入新数据集，实验证明其在知识和计算密集型任务中有效。


<details>
  <summary>Details</summary>
Motivation: 现有仅依赖直接推理的语言模型在需要最新知识或计算工具的任务中表现不佳，为克服此局限开展研究。

Method: 提出TAPO框架，采用改进的Dynamic Sampling Policy Optimization (DAPO)用于工具调用场景，引入TAPO - easy - 60K和TAPO - hard - 18K数据集。

Result: 在Qwen2.5 - 3B和Qwen2.5 - 7B模型上实验，两模型在需外部知识和数学计算任务上达最优，TAPO工具利用更高效，避免奖励破解导致的过度调用。

Conclusion: 结合高级推理和工具使用可显著提升模型在知识和计算密集型任务中的性能。

Abstract: Recent advances in large language models (LLMs) have popularized test-time
scaling, where models generate additional reasoning tokens before producing
final answers. These approaches have demonstrated significant performance
improvements on benchmarks involving mathematical reasoning. However, language
models relying solely on direct inference still struggle with tasks demanding
up-to-date knowledge or computational tools such as calculators and code
interpreters for complex arithmetic operations. To overcome these limitations,
we propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement
learning framework that systematically integrates multi-hop reasoning with
adaptive tool-calling capabilities. Our approach employs a modified version of
Dynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,
which we adapt specifically for tool invocation scenarios, enabling models to
dynamically interleave complex reasoning with on-demand tool usage (including
search APIs and Python interpreters).
  To support this research, we introduce two new datasets: TAPO-easy-60K and
TAPO-hard-18K, specifically designed to train and evaluate both fact-based
reasoning and mathematical calculation capabilities. Our experiments on
Qwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,
with both models achieving state-of-the-art performance on tasks requiring
external knowledge and mathematical computation among methods with comparable
parameters. Notably, TAPO achieves more efficient tool utilization than
baseline methods while preventing excessive calls caused by reward hacking.
These results highlight the significant potential of combining advanced
reasoning with tool usage to enhance model performance in knowledge-intensive
and computationally demanding tasks.

</details>


### [23] [Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations](https://arxiv.org/abs/2510.07064)
*Manh Hung Nguyen,Sebastian Tschiatschek,Adish Singla*

Main category: cs.AI

TL;DR: 因获取大规模人类反馈困难昂贵，LLMs成替代方案，但有输出同质化问题。提出构建一组LLM代理框架，用子模优化解决代理选择问题，实验表明该方法更有效。


<details>
  <summary>Details</summary>
Motivation: 获取大规模人类反馈困难且昂贵，而现有LLMs输出同质化，无法捕捉人类视角和行为的丰富多样性。

Method: 提出构建一组LLM代理的框架，通过上下文学习以少量人类示范引导每个代理行为，用子模优化解决从大量可能代理中选择代表性代理集的问题，并开发不同时间复杂度和性能保证的方法。

Result: 在众包和教育领域的大量实验表明，该方法构建的代理比基线方法更能有效代表人类群体，在新任务上能重现目标群体的行为模式和视角。

Conclusion: 所提出的方法能更有效地构建代表人类群体的LLM代理，可重现目标群体的行为和视角。

Abstract: The difficulty and expense of obtaining large-scale human responses make
Large Language Models (LLMs) an attractive alternative and a promising proxy
for human behavior. However, prior work shows that LLMs often produce
homogeneous outputs that fail to capture the rich diversity of human
perspectives and behaviors. Thus, rather than trying to capture this diversity
with a single LLM agent, we propose a novel framework to construct a set of
agents that collectively capture the diversity of a given human population.
Each agent is an LLM whose behavior is steered by conditioning on a small set
of human demonstrations (task-response pairs) through in-context learning. The
central challenge is therefore to select a representative set of LLM agents
from the exponentially large space of possible agents. We tackle this selection
problem from the lens of submodular optimization. In particular, we develop
methods that offer different trade-offs regarding time complexity and
performance guarantees. Extensive experiments in crowdsourcing and educational
domains demonstrate that our approach constructs agents that more effectively
represent human populations compared to baselines. Moreover, behavioral
analyses on new tasks show that these agents reproduce the behavior patterns
and perspectives of the students and annotators they are designed to represent.

</details>


### [24] [Inductive Learning for Possibilistic Logic Programs Under Stable Models](https://arxiv.org/abs/2510.07069)
*Hongbo Hu,Yisong Wang,Yi Huang,Kewen Wang*

Main category: cs.AI

TL;DR: 本文提出从背景程序和示例中提取可能性逻辑程序（poss - programs）的方法，定义归纳任务，给出计算归纳解的算法并进行实验。


<details>
  <summary>Details</summary>
Motivation: 可能性逻辑程序在稳定模型下的归纳推理问题尚未被研究，本文旨在解决此问题。

Method: 首先正式定义归纳任务的概念，研究其性质，然后给出两个计算归纳解的算法ilpsm和ilpsmmin，并实现了ilpsmmin。

Result: 实验结果表明，当输入为普通逻辑程序时，原型在随机生成的数据集上优于一个主要的基于稳定模型的正常逻辑程序归纳学习系统。

Conclusion: 所提出的方法在普通逻辑程序输入的情况下，在随机生成数据集上表现优于现有系统，为可能性逻辑程序的归纳推理提供了有效途径。

Abstract: Possibilistic logic programs (poss-programs) under stable models are a major
variant of answer set programming (ASP). While its semantics (possibilistic
stable models) and properties have been well investigated, the problem of
inductive reasoning has not been investigated yet. This paper presents an
approach to extracting poss-programs from a background program and examples
(parts of intended possibilistic stable models). To this end, the notion of
induction tasks is first formally defined, its properties are investigated and
two algorithms ilpsm and ilpsmmin for computing induction solutions are
presented. An implementation of ilpsmmin is also provided and experimental
results show that when inputs are ordinary logic programs, the prototype
outperforms a major inductive learning system for normal logic programs from
stable models on the datasets that are randomly generated.

</details>


### [25] [VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle Routing Problems](https://arxiv.org/abs/2510.07073)
*André Hottung,Federico Berto,Chuanbo Hua,Nayeli Gast Zepeda,Daniel Wetzel,Michael Römer,Haoran Ye,Davide Zago,Michael Poli,Stefano Massaroli,Jinkyoo Park,Kevin Tierney*

Main category: cs.AI

TL;DR: 本文提出VRPAgent框架，将大语言模型生成组件集成到元启发式算法中，经遗传搜索优化，在多类车辆路径问题上表现优于手工方法和基于学习的方法。


<details>
  <summary>Details</summary>
Motivation: 设计车辆路径问题（VRPs）的高性能启发式算法复杂，大语言模型代码生成在生成媲美人类专家的启发式算法方面不足。

Method: 提出VRPAgent框架，将大语言模型生成的特定问题算子嵌入通用元启发式框架，并通过新颖的遗传搜索进行优化。

Result: 在多种VRPs问题上，该方法发现的启发式算子优于手工方法和近期基于学习的方法，且仅需单个CPU核心。

Conclusion: VRPAgent是首个基于大语言模型推动VRPs领域发展的范例，展示了自动启发式算法发现的前景。

Abstract: Designing high-performing heuristics for vehicle routing problems (VRPs) is a
complex task that requires both intuition and deep domain knowledge. Large
language model (LLM)-based code generation has recently shown promise across
many domains, but it still falls short of producing heuristics that rival those
crafted by human experts. In this paper, we propose VRPAgent, a framework that
integrates LLM-generated components into a metaheuristic and refines them
through a novel genetic search. By using the LLM to generate problem-specific
operators, embedded within a generic metaheuristic framework, VRPAgent keeps
tasks manageable, guarantees correctness, and still enables the discovery of
novel and powerful strategies. Across multiple problems, including the
capacitated VRP, the VRP with time windows, and the prize-collecting VRP, our
method discovers heuristic operators that outperform handcrafted methods and
recent learning-based approaches while requiring only a single CPU core. To our
knowledge, \VRPAgent is the first LLM-based paradigm to advance the
state-of-the-art in VRPs, highlighting a promising future for automated
heuristics discovery.

</details>


### [26] [The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas](https://arxiv.org/abs/2510.07091)
*Baixuan Xu,Tianshi Zheng,Zhaowei Wang,Hong Ting Tsang,Weiqi Wang,Tianqing Fang,Yangqiu Song*

Main category: cs.AI

TL;DR: 研究长视野任务中两种动作表示法的有效性，提出认知带宽视角，找到表示选择转折点，并研究其与模型能力的关系，给出构建更优PwS智能体的建议。


<details>
  <summary>Details</summary>
Motivation: 传统动作表示法在环境动作空间组合爆炸时不实用，需研究长视野智能体的最优动作表示。

Method: 系统研究传统的基于动作的规划（PwA）和基于图式的规划（PwS）两种动作表示法，提出认知带宽视角，进行对照实验。

Result: 观察到ALFWorld和SciWorld之间的表示选择转折点，发现模型能力对转折点位置有影响。

Conclusion: 为构建更有能力的PwS智能体以实现更好的可扩展自主性提供了可行指南。

Abstract: Enabling LLMs to effectively operate long-horizon task which requires
long-term planning and multiple interactions is essential for open-world
autonomy. Conventional methods adopt planning with actions where a executable
action list would be provided as reference. However, this action representation
choice would be impractical when the environment action space is combinatorial
exploded (e.g., open-ended real world). This naturally leads to a question: As
environmental action space scales, what is the optimal action representation
for long-horizon agents? In this paper, we systematically study the
effectiveness of two different action representations. The first one is
conventional planning with actions (PwA) which is predominantly adopted for its
effectiveness on existing benchmarks. The other one is planning with schemas
(PwS) which instantiate an action schema into action lists (e.g., "move [OBJ]
to [OBJ]" -> "move apple to desk") to ensure concise action space and reliable
scalability. This alternative is motivated by its alignment with human
cognition and its compliance with environment-imposed action format
restriction. We propose cognitive bandwidth perspective as a conceptual
framework to qualitatively understand the differences between these two action
representations and empirically observe a representation-choice inflection
point between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve
as evidence of the need for scalable representations. We further conduct
controlled experiments to study how the location of this inflection point
interacts with different model capacities: stronger planning proficiency shifts
the inflection rightward, whereas better schema instantiation shifts it
leftward. Finally, noting the suboptimal performance of PwS agents, we provide
an actionable guide for building more capable PwS agents for better scalable
autonomy.

</details>


### [27] [The Contingencies of Physical Embodiment Allow for Open-Endedness and Care](https://arxiv.org/abs/2510.07117)
*Leonardo Christov-Moore,Arthur Juliani,Alex Kiefer,Nicco Reggente,B. Scott Rousse,Adam Safron,Nicol'as Hinrichs,Daniel Polani,Antonio Damasio*

Main category: cs.AI

TL;DR: 本文受哲学启发定义物理具身的两个条件，在强化学习框架下探讨具身智能体的驱动力，以培养开放性和关怀能力。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能体在开放环境适应性和关怀能力不足的问题，借鉴生物生存优势开发更强大的人工智能体。

Method: 受海德格尔和尼采哲学概念启发，定义物理具身条件和内在驱动力，在强化学习框架中进行形式化。

Result: 在强化学习框架下探讨了内在驱动的具身智能体在开放多智能体环境中的学习情况。

Conclusion: 通过设定条件和内在驱动力，可使人工智能体培养开放性和关怀能力。

Abstract: Physical vulnerability and mortality are often seen as obstacles to be
avoided in the development of artificial agents, which struggle to adapt to
open-ended environments and provide aligned care. Meanwhile, biological
organisms survive, thrive, and care for each other in an open-ended physical
world with relative ease and efficiency. Understanding the role of the
conditions of life in this disparity can aid in developing more robust,
adaptive, and caring artificial agents. Here we define two minimal conditions
for physical embodiment inspired by the existentialist phenomenology of Martin
Heidegger: being-in-the-world (the agent is a part of the environment) and
being-towards-death (unless counteracted, the agent drifts toward terminal
states due to the second law of thermodynamics). We propose that from these
conditions we can obtain both a homeostatic drive - aimed at maintaining
integrity and avoiding death by expending energy to learn and act - and an
intrinsic drive to continue to do so in as many ways as possible. Drawing
inspiration from Friedrich Nietzsche's existentialist concept of will-to-power,
we examine how intrinsic drives to maximize control over future states, e.g.,
empowerment, allow agents to increase the probability that they will be able to
meet their future homeostatic needs, thereby enhancing their capacity to
maintain physical integrity. We formalize these concepts within a reinforcement
learning framework, which enables us to examine how intrinsically driven
embodied agents learning in open-ended multi-agent environments may cultivate
the capacities for open-endedness and care.ov

</details>


### [28] [Integrating Domain Knowledge into Process Discovery Using Large Language Models](https://arxiv.org/abs/2510.07161)
*Ali Norouzifar,Humam Kourani,Marcus Dees,Wil van der Aalst*

Main category: cs.AI

TL;DR: 论文提出结合自然语言领域知识和大语言模型的交互式流程发现框架，评估多种大语言模型和提示工程策略并进行案例研究。


<details>
  <summary>Details</summary>
Motivation: 仅基于事件数据得到的流程模型可能无法准确反映真实流程，领域知识常被忽视，导致模型对下游任务缺乏可靠性。

Method: 利用大语言模型从领域专家的文本描述中提取声明性规则，用这些规则指导IMr发现算法构建流程模型，框架协调大语言模型、领域专家和后端服务交互。

Result: 实现支持该工作流的工具，对多种大语言模型和提示工程策略进行广泛评估，开展基于真实事件日志和领域专家参与的案例研究。

Conclusion: 未明确提及结论内容，但可推测该框架在一定程度上能解决现有流程发现问题，提升模型可靠性。

Abstract: Process discovery aims to derive process models from event logs, providing
insights into operational behavior and forming a foundation for conformance
checking and process improvement. However, models derived solely from event
data may not accurately reflect the real process, as event logs are often
incomplete or affected by noise, and domain knowledge, an important
complementary resource, is typically disregarded. As a result, the discovered
models may lack reliability for downstream tasks. We propose an interactive
framework that incorporates domain knowledge, expressed in natural language,
into the process discovery pipeline using Large Language Models (LLMs). Our
approach leverages LLMs to extract declarative rules from textual descriptions
provided by domain experts. These rules are used to guide the IMr discovery
algorithm, which recursively constructs process models by combining insights
from both the event log and the extracted rules, helping to avoid problematic
process structures that contradict domain knowledge. The framework coordinates
interactions among the LLM, domain experts, and a set of backend services. We
present a fully implemented tool that supports this workflow and conduct an
extensive evaluation of multiple LLMs and prompt engineering strategies. Our
empirical study includes a case study based on a real-life event log with the
involvement of domain experts, who assessed the usability and effectiveness of
the framework.

</details>


### [29] [NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents](https://arxiv.org/abs/2510.07172)
*Tianshi Zheng,Kelvin Kiu-Wai Tam,Newt Hue-Nam K. Nguyen,Baixuan Xu,Zhaowei Wang,Jiayang Cheng,Hong Ting Tsang,Weiqi Wang,Jiaxin Bai,Tianqing Fang,Yangqiu Song,Ginny Y. Wong,Simon See*

Main category: cs.AI

TL;DR: 文章介绍了新基准NewtonBench以解决现有科学定律发现基准的问题，实验揭示前沿大语言模型发现能力及工具辅助的悖论，强调复杂环境下发现挑战，NewtonBench可助力下一代AI发展。


<details>
  <summary>Details</summary>
Motivation: 现有科学定律发现基准存在方法三难困境，且将发现过程简化为静态函数拟合，无法体现真实科学过程。

Method: 引入包含12个物理领域324个科学定律发现任务的NewtonBench基准，用形而上学转变生成问题，将评估从静态函数拟合提升到交互式模型发现。

Result: 前沿大语言模型有一定发现能力，但随系统复杂度增加能力下降，对观测噪声敏感；工具辅助会使更有能力的模型陷入次优解。

Conclusion: 复杂交互环境下实现强大、可泛化的发现仍是核心挑战，NewtonBench为衡量进展和引导下一代AI发展提供重要工具。

Abstract: Large language models are emerging as powerful tools for scientific law
discovery, a foundational challenge in AI-driven science. However, existing
benchmarks for this task suffer from a fundamental methodological trilemma,
forcing a trade-off between scientific relevance, scalability, and resistance
to memorization. Furthermore, they oversimplify discovery as static function
fitting, failing to capture the authentic scientific process of uncovering
embedded laws through the interactive exploration of complex model systems. To
address these critical gaps, we introduce NewtonBench, a benchmark comprising
324 scientific law discovery tasks across 12 physics domains. Our design
mitigates the evaluation trilemma by using metaphysical shifts - systematic
alterations of canonical laws - to generate a vast suite of problems that are
scalable, scientifically relevant, and memorization-resistant. Moreover, we
elevate the evaluation from static function fitting to interactive model
discovery, requiring agents to experimentally probe simulated complex systems
to uncover hidden principles. Our extensive experiment reveals a clear but
fragile capability for discovery in frontier LLMs: this ability degrades
precipitously with increasing system complexity and exhibits extreme
sensitivity to observational noise. Notably, we uncover a paradoxical effect of
tool assistance: providing a code interpreter can hinder more capable models by
inducing a premature shift from exploration to exploitation, causing them to
satisfice on suboptimal solutions. These results demonstrate that robust,
generalizable discovery in complex, interactive environments remains the core
challenge. By providing a scalable, robust, and scientifically authentic
testbed, NewtonBench offers a crucial tool for measuring true progress and
guiding the development of next-generation AI agents capable of genuine
scientific discovery.

</details>


### [30] [Multi-Objective Multi-Agent Path Finding with Lexicographic Cost Preferences](https://arxiv.org/abs/2510.07276)
*Pulkit Rustagi,Kyle Hollins Wray,Sandhya Saisubramanian*

Main category: cs.AI

TL;DR: 提出用于MO - MAPF的字典序框架和LCBS算法，可根据目标偏好高效规划，能处理多达10个目标，成功率高于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多目标多智能体路径规划（MO - MAPF）算法未针对用户偏好优化，且随目标数量增加扩展性差。

Method: 提出字典序框架和LCBS算法，将优先级感知的低级A*搜索与基于冲突的搜索相结合，避免帕累托前沿构建。

Result: LCBS能计算最优解，可扩展到多达10个目标的实例，在标准和随机MAPF基准测试中成功率高于现有基线方法。

Conclusion: LCBS算法在处理多目标多智能体路径规划问题上，相比现有方法具有更好的扩展性和更高的成功率。

Abstract: Many real-world scenarios require multiple agents to coordinate in shared
environments, while balancing trade-offs between multiple, potentially
competing objectives. Current multi-objective multi-agent path finding
(MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto
frontiers. They do not explicitly optimize for user-defined preferences, even
when the preferences are available, and scale poorly with the number of
objectives. We propose a lexicographic framework for modeling MO-MAPF, along
with an algorithm \textit{Lexicographic Conflict-Based Search} (LCBS) that
directly computes a single solution aligned with a lexicographic preference
over objectives. LCBS integrates a priority-aware low-level $A^*$ search with
conflict-based search, avoiding Pareto frontier construction and enabling
efficient planning guided by preference over objectives. We provide insights
into optimality and scalability, and empirically demonstrate that LCBS computes
optimal solutions while scaling to instances with up to ten objectives -- far
beyond the limits of existing MO-MAPF methods. Evaluations on standard and
randomized MAPF benchmarks show consistently higher success rates against
state-of-the-art baselines, especially with increasing number of objectives.

</details>


### [31] [Agentic generative AI for media content discovery at the national football league](https://arxiv.org/abs/2510.07297)
*Henry Wang,Md Sirajus Salekin,Jake Lee,Ross Claytor,Shinan Zhang,Michael Chi*

Main category: cs.AI

TL;DR: 本文展示基于生成式AI的工作流助力NFL媒体研究和分析，提升效率。


<details>
  <summary>Details</summary>
Motivation: 展示生成式AI在内容发现和管理中的应用，解决传统界面查询不便问题。

Method: 采用代理工作流将用户查询转换为数据库查询语言，通过语义缓存提升准确性和降低延迟。

Result: 解决方案准确率超95%，查找相关视频平均时间从10分钟降至30秒。

Conclusion: 该方案显著提高NFL运营效率，让用户专注创作。

Abstract: Generative AI has unlocked new possibilities in content discovery and
management. Through collaboration with the National Football League (NFL), we
demonstrate how a generative-AI based workflow enables media researchers and
analysts to query relevant historical plays using natural language rather than
traditional filter-and-click interfaces. The agentic workflow takes a user
query as input, breaks it into elements, and translates them into the
underlying database query language. Accuracy and latency are further improved
through carefully designed semantic caching. The solution achieves over 95
percent accuracy and reduces the average time to find relevant videos from 10
minutes to 30 seconds, significantly increasing the NFL's operational
efficiency and allowing users to focus on producing creative content and
engaging storylines.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [32] [Attention-Enhanced Reinforcement Learning for Dynamic Portfolio Optimization](https://arxiv.org/abs/2510.06466)
*Pei Xue,Yuanchun Ye*

Main category: cs.CE

TL;DR: 开发结合Dirichlet策略与横截面注意力机制的深度强化学习框架用于动态投资组合优化，结果显示该策略优于基准。


<details>
  <summary>Details</summary>
Motivation: 构建更好的动态投资组合优化方法，解决投资组合权重可行性、交易约束等问题并捕捉资产间依赖关系。

Method: 开发结合Dirichlet策略与横截面注意力机制的框架，集成资产时间编码器和全局注意力层，奖励函数含交易成本和投资组合方差惩罚。

Result: 基于注意力的Dirichlet策略在期末财富和夏普比率上优于等权重和标准强化学习基准，同时维持合理换手率和回撤水平。

Conclusion: 将原则性行动设计与基于注意力的表示相结合可提高投资组合管理强化学习的稳定性和可解释性。

Abstract: We develop a deep reinforcement learning framework for dynamic portfolio
optimization that combines a Dirichlet policy with cross-sectional attention
mechanisms. The Dirichlet formulation ensures that portfolio weights are always
feasible, handles tradability constraints naturally, and provides a stable way
to explore the allocation space. The model integrates per-asset temporal
encoders with a global attention layer, allowing it to capture sector
relationships, factor spillovers, and other cross asset dependencies. The
reward function includes transaction costs and portfolio variance penalties,
linking the learning objective to traditional mean variance trade offs. The
results show that attention based Dirichlet policies outperform equal-weight
and standard reinforcement learning benchmarks in terms of terminal wealth and
Sharpe ratio, while maintaining realistic turnover and drawdown levels.
Overall, the study shows that combining principled action design with
attention-based representations improves both the stability and
interpretability of reinforcement learning for portfolio management.

</details>


### [33] [A Higher-Order Time Domain Boundary Element Formulation based on Isogeometric Analysis and the Convolution Quadrature Method](https://arxiv.org/abs/2510.06804)
*Thomas Kramer,Benjamin Marussig,Martin Schanz*

Main category: cs.CE

TL;DR: 提出等几何边界元法（BEM）求解各向同性均匀介质中的散射问题，采用基于多阶段龙格 - 库塔（RK）的卷积求积法处理卷积积分，在等几何分析（IGA）框架下离散空间变量，实现时空高收敛率。


<details>
  <summary>Details</summary>
Motivation: 解决各向同性均匀介质中的散射问题，如声学中的标量波动方程和弹性动力学中的拉梅 - 纳维方程相关的波动问题。

Method: 用基于多阶段龙格 - 库塔（RK）的卷积求积法近似时间相关的卷积积分；在等几何分析（IGA）框架下离散空间变量；采用对称伽辽金变分公式和配点法的边界元法近似混合问题的解。

Result: 该方法能在空间和时间上实现高收敛率，并对近似解在混合时空误差范数下的收敛率进行了研究。

Conclusion: 所提出的等几何边界元法在解决各向同性均匀介质散射问题上具有有效性和高收敛性。

Abstract: An isogeometric boundary element method (BEM) is presented to solve
scattering problems in an isotropic homogeneous medium. We consider wave
problems governed by the scalar wave equation as in acoustics and the
Lam\'e-Navier equations for elastodynamics considering the theory of linear
elasticity. The underlying boundary integral equations imply time-dependent
convolution integrals and allow us to determine the sought quantities in the
bounded interior or the unbounded exterior after solving for the unknown Cauchy
data. In the present work, the time-dependent convolution integrals are
approximated by multi-stage Runge-Kutta (RK) based convolution quadratures that
involve steady-state solutions in the Laplace domain. The proposed method
discretizes the spatial variables in the framework of isogeometric analysis
(IGA), entailing a patchwise smooth spline basis. Overall, it enables high
convergence rates in space and time. The implementation scheme follows an
element structure defined by the non-empty knot spans in the knot vectors and
local, uniform Bernstein polynomials as basis functions. The algorithms to
localize the basis functions on the elements are outlined and explained. The
solutions of the mixed problems are approximated by the BEM based on a
symmetric Galerkin variational formulation and a collocation method. We
investigate convergence rates of the approximative solutions in a mixed space
and time error norm.

</details>


### [34] [A Framework for Measuring How News Topics Drive Stock Movement](https://arxiv.org/abs/2510.06864)
*Qizhao Chen*

Main category: cs.CE

TL;DR: 本文提出新框架研究不同新闻主题对股价变动的影响，应用于苹果公司发现特定主题与次日回报显著相关，强调主题层面分析的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究将每日新闻情绪汇总为单一分数，可能忽略主题内容和相关性的重要差异，掩盖特定新闻主题与市场反应的细微关系。

Method: 用预训练句子转换器将新闻标题编码为密集语义嵌入，用K - means聚类识别不同新闻主题，将主题暴露作为解释变量纳入普通最小二乘回归量化其对每日股票回报的影响。

Result: 应用于苹果公司，发现某些主题与次日正或负回报显著相关，其他主题无明显影响。

Conclusion: 强调主题层面分析对理解新闻内容与金融市场关系的重要性，框架为评估新闻主题信息价值提供可扩展方法，为改进股价预测模型指明方向。

Abstract: In modern financial markets, news plays a critical role in shaping investor
sentiment and influencing stock price movements. However, most existing studies
aggregate daily news sentiment into a single score, potentially overlooking
important variations in topic content and relevance. This simplification may
mask nuanced relationships between specific news themes and market responses.
To address this gap, this paper proposes a novel framework to examine how
different news topics influence stock price movements. The framework encodes
individual news headlines into dense semantic embeddings using a pretrained
sentence transformer, then applies K-means clustering to identify distinct news
topics. Topic exposures are incorporated as explanatory variables in an
ordinary least squares regression to quantify their impact on daily stock
returns. Applied to Apple Inc., the framework reveals that certain topics are
significantly associated with positive or negative next-day returns, while
others have no measurable effect. These findings highlight the importance of
topic-level analysis in understanding the relationship between news content and
financial markets. The proposed framework provides a scalable approach for both
researchers and practitioners to assess the informational value of different
news topics and suggests a promising direction for improving predictive models
of stock price movement.

</details>


### [35] [TOMATOES: Topology and Material Optimization for Latent Heat Thermal Energy Storage Devices](https://arxiv.org/abs/2510.07057)
*Rahul Kumar Padhy,Krishnan Suresh,Aaditya Chandrasekhar*

Main category: cs.CE

TL;DR: 提出用于潜热热能存储（LHTES）系统的材料选择和拓扑并发优化的自动化设计框架，并用实例验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统拓扑优化局限于为固定预选材料优化几何，未利用新材料数据库，解决LHTES系统材料和几何协同设计挑战。

Method: 使用数据驱动的变分自编码器（VAE）将离散材料数据库投影到连续可微的潜在空间，集成到考虑相变的端到端可微瞬态非线性有限元求解器。

Result: 通过几个示例验证了所提方法的有效性。

Conclusion: 所提出的自动化设计框架可实现LHTES系统材料和拓扑的并发优化。

Abstract: Latent heat thermal energy storage (LHTES) systems are compelling candidates
for energy storage, primarily owing to their high storage density. Improving
their performance is crucial for developing the next-generation efficient and
cost effective devices. Topology optimization (TO) has emerged as a powerful
computational tool to design LHTES systems by optimally distributing a
high-conductivity material (HCM) and a phase change material (PCM). However,
conventional TO typically limits to optimizing the geometry for a fixed,
pre-selected materials. This approach does not leverage the large and expanding
databases of novel materials. Consequently, the co-design of material and
geometry for LHTES remains a challenge and unexplored.
  To address this limitation, we present an automated design framework for the
concurrent optimization of material choice and topology. A key challenge is the
discrete nature of material selection, which is incompatible with the
gradient-based methods used for TO. We overcome this by using a data-driven
variational autoencoder (VAE) to project discrete material databases for both
the HCM and PCM onto continuous and differentiable latent spaces. These
continuous material representations are integrated into an end-to-end
differentiable, transient nonlinear finite-element solver that accounts for
phase change. We demonstrate this framework on a problem aimed at maximizing
the discharged energy within a specified time, subject to cost constraints. The
effectiveness of the proposed method is validated through several illustrative
examples.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [36] [Bridging Imperative Process Models and Process Data Queries-Translation and Relaxation](https://arxiv.org/abs/2510.06414)
*Abdur Rehman Anwar Qureshi,Adrian Rebmann,Timotheus Kampik,Matthias Weidlich,Mathias Weske*

Main category: cs.DB

TL;DR: 本文提出将命令式模型转换为SQL查询以进行一致性检查的方法，弥合流程建模与数据驱动流程分析的差距。


<details>
  <summary>Details</summary>
Motivation: 经典命令式流程模型难以应用于关系数据库，导致流程模型未充分利用，需弥合传统流程建模与数据驱动流程分析之间的差距。

Method: 提供将命令式模型转换为可在关系数据库上执行的SQL查询的方法进行一致性检查。

Result: 表明命令式流程模型对数据驱动流程管理仍具相关性，以及行为足迹和其他声明式方法对集成基于模型和数据驱动的流程管理很重要。

Conclusion: 提出的方法能有效弥合传统流程建模与数据驱动流程分析的差距，强调命令式模型和相关方法在数据驱动流程管理中的重要性。

Abstract: Business process management is increasingly practiced using data-driven
approaches. Still, classical imperative process models, which are typically
formalized using Petri nets, are not straightforwardly applicable to the
relational databases that contain much of the available structured process
execution data. This creates a gap between the traditional world of process
modeling and recent developments around data-driven process analysis,
ultimately leading to the under-utilization of often readily available process
models. In this paper, we close this gap by providing an approach for
translating imperative models into relaxed process data queries, specifically
SQL queries executable on relational databases, for conformance checking. Our
results show the continued relevance of imperative process models to
data-driven process management, as well as the importance of behavioral
footprints and other declarative approaches for integrating model-based and
data-driven process management.

</details>


### [37] [Automated Discovery of Test Oracles for Database Management Systems Using LLMs](https://arxiv.org/abs/2510.06663)
*Qiuyang Mang,Runyuan He,Suyang Zhong,Xiaoxuan Liu,Huanchen Zhang,Alvin Cheung*

Main category: cs.DB

TL;DR: 本文探索用大语言模型自动化数据库管理系统测试预言机的发现和实例化，介绍Argus框架，在五个数据库管理系统上发现40个未知漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有数据库管理系统测试预言机设计依赖手动，存在瓶颈，且大语言模型用于此有幻觉和成本问题，需高效经济的方法。

Method: 引入基于约束抽象查询概念的Argus框架，用大语言模型生成骨架对，用SQL等价求解器证明等价性，再实例化骨架生成测试用例。

Result: 在五个广泛测试的数据库管理系统上发现40个未知漏洞，35个是逻辑漏洞，36个已确认，26个已修复。

Conclusion: Argus框架能有效解决数据库管理系统测试预言机自动化问题，高效发现数据库系统中的漏洞。

Abstract: Since 2020, automated testing for Database Management Systems (DBMSs) has
flourished, uncovering hundreds of bugs in widely-used systems. A cornerstone
of these techniques is test oracle, which typically implements a mechanism to
generate equivalent query pairs, thereby identifying bugs by checking the
consistency between their results. However, while applying these oracles can be
automated, their design remains a fundamentally manual endeavor. This paper
explores the use of large language models (LLMs) to automate the discovery and
instantiation of test oracles, addressing a long-standing bottleneck towards
fully automated DBMS testing. Although LLMs demonstrate impressive creativity,
they are prone to hallucinations that can produce numerous false positive bug
reports. Furthermore, their significant monetary cost and latency mean that LLM
invocations should be limited to ensure that bug detection is efficient and
economical.
  To this end, we introduce Argus, a novel framework built upon the core
concept of the Constrained Abstract Query - a SQL skeleton containing
placeholders and their associated instantiation conditions (e.g., requiring a
placeholder to be filled by a boolean column). Argus uses LLMs to generate
pairs of these skeletons that are asserted to be semantically equivalent. This
equivalence is then formally proven using a SQL equivalence solver to ensure
soundness. Finally, the placeholders within the verified skeletons are
instantiated with concrete, reusable SQL snippets that are also synthesized by
LLMs to efficiently produce complex test cases. We implemented Argus and
evaluated it on five extensively tested DBMSs, discovering 40 previously
unknown bugs, 35 of which are logic bugs, with 36 confirmed and 26 already
fixed by the developers.

</details>


### [38] [Relational Database Distillation: From Structured Tables to Condensed Graph Data](https://arxiv.org/abs/2510.06980)
*Xinyi Gao,Jingxi Zhang,Lijian Chen,Tong Chen,Lizhen Cui,Hongzhi Yin*

Main category: cs.DB

TL;DR: 提出关系数据库蒸馏（RDD）问题，将大规模RDB蒸馏为紧凑异构图，实验表明能大幅减小数据量且保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有利用图表示学习处理RDB的方法存在存储开销大、训练时间长的问题。

Method: 提出RDD问题，通过节点特征保留多模态列信息，用异质边编码主键 - 外键关系；设计带伪标签的核岭回归引导目标。

Result: 在多个真实RDB上实验，大幅减小数据量，在分类和回归任务上保持竞争力。

Conclusion: 为RDB的可扩展学习提供了有效途径。

Abstract: Relational databases (RDBs) underpin the majority of global data management
systems, where information is structured into multiple interdependent tables.
To effectively use the knowledge within RDBs for predictive tasks, recent
advances leverage graph representation learning to capture complex inter-table
relations as multi-hop dependencies. Despite achieving state-of-the-art
performance, these methods remain hindered by the prohibitive storage overhead
and excessive training time, due to the massive scale of the database and the
computational burden of intensive message passing across interconnected tables.
To alleviate these concerns, we propose and study the problem of Relational
Database Distillation (RDD). Specifically, we aim to distill large-scale RDBs
into compact heterogeneous graphs while retaining the predictive power (i.e.,
utility) required for training graph-based models. Multi-modal column
information is preserved through node features, and primary-foreign key
relations are encoded via heterogeneous edges, thereby maintaining both data
fidelity and relational structure. To ensure adaptability across diverse
downstream tasks without engaging the traditional, inefficient bi-level
distillation framework, we further design a kernel ridge regression-guided
objective with pseudo-labels, which produces quality features for the distilled
graph. Extensive experiments on multiple real-world RDBs demonstrate that our
solution substantially reduces the data size while maintaining competitive
performance on classification and regression tasks, creating an effective
pathway for scalable learning with RDBs.

</details>


### [39] [On the Expressiveness of Languages for Querying Property Graphs in Relational Databases](https://arxiv.org/abs/2510.07062)
*Hadar Rotschield,Liat Peterfreund*

Main category: cs.DB

TL;DR: 形式化分析 SQL/PGQ 三个片段的表达能力，得出严格层次结构及有序结构下的结论


<details>
  <summary>Details</summary>
Motivation: 分析新兴 ISO 标准 SQL/PGQ 在不同片段的表达能力

Method: 对 SQL/PGQ 的只读核心、读写扩展和扩展变体三个片段进行形式化分析

Result: 图创建决定表达能力，各片段有严格层次，扩展片段能精确捕获 NL，有序结构下层次坍塌

Conclusion: 明确了 SQL/PGQ 片段的表达能力层次及视图构建的核心作用

Abstract: SQL/PGQ is the emerging ISO standard for querying property graphs defined as
views over relational data. We formalize its expressive power across three
fragments: the read-only core, the read-write extension, and an extended
variant with richer view definitions. Our results show that graph creation
plays a central role in determining the expressiveness. The read-only fragment
is strictly weaker than the read-write fragment, and the latter is still below
the complexity class NL. Extending view definitions with arbitrary arity
identifiers closes this gap: the extended fragment captures exactly NL. This
yields a strict hierarchy of SQL/PGQ fragments, whose union covers all NL
queries. On ordered structures the hierarchy collapses: once arity-2
identifiers are allowed, higher arities add no power, mirroring the classical
transitive-closure collapse and underscoring the central role of view
construction in property graph querying.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [40] [DiLi: A Lock-Free Asynchronously Distributable Linked List](https://arxiv.org/abs/2510.06387)
*Raaghav Ravishankar,Sandeep Kulkarni,Sathya Peri,Gokarna Sharma*

Main category: cs.DC

TL;DR: 本文引入条件无锁概念，提出分布式链表 DiLi，可异步动态分区和负载均衡，单机器性能佳且吞吐量随机器数线性扩展。


<details>
  <summary>Details</summary>
Motivation: 现代数据库为满足吞吐量需求，需使数据结构分布式，但静态分区有负载不均和停机问题，因此需新的分布式数据结构解决方案。

Method: 引入条件无锁概念，设计 DiLi 链表，它可异步动态分区和负载均衡，搜索采用新的遍历方式。

Result: DiLi 在单机器上性能与基于链表的无锁并发搜索结构相当，吞吐量随机器数线性扩展。

Conclusion: DiLi 是一种有效的分布式数据结构，能满足数据库不断增长的吞吐量需求。

Abstract: Modern databases use dynamic search structures that store a huge amount of
data, and often serve them using multi-threaded algorithms to support the
ever-increasing throughput needs. When this throughput need exceeds the
capacity of the machine hosting the structure, one either needs to replace the
underlying hardware (an option that is typically not viable and introduces a
long down time) or make the data structure distributed. Static partitioning of
the data structure for distribution is not desirable, as it is prone to uneven
load distribution over time, and having to change the partitioning scheme later
will require downtime.
  Since a distributed data structure, inherently, relies on communication
support from the network stack and operating systems, we introduce the notion
of conditional lock-freedom that extends the notion of lock-free computation
with reasonable assumptions about communication between processes. We present
DiLi, a conditional lock-free, linearizable, and distributable linked list that
can be asynchronously and dynamically (1) partitioned into multiple sublists
and (2) load balanced by distributing sublists across multiple machines. DiLi
contains primitives for these that also maintain the lock-free property of the
underlying search structure that supports find, remove, and insert of a key as
the client operations.
  Searching for an item in DiLi is by a novel traversal that involves a binary
search on the partitioning scheme, and then a linear traversal on a limitable
number of linked nodes. As a result, we are able to empirically show that DiLi
performs as well as the state-of-the-art lock-free concurrent search structures
that are based off of a linked list when executed on a single-machine. We also
show that the throughput of DiLi scales linearly with the number of machines
that host it.

</details>


### [41] [Adaptive Protein Design Protocols and Middleware](https://arxiv.org/abs/2510.06396)
*Aymen Alsaadi,Jonathan Ash,Mikhail Titov,Matteo Turilli,Andre Merzky,Shantenu Jha,Sagar Khare*

Main category: cs.DC

TL;DR: 介绍IMPRESS系统，展示自适应蛋白设计协议及计算基础设施，提升蛋白设计质量一致性和通量。


<details>
  <summary>Details</summary>
Motivation: 计算蛋白设计受AI/ML驱动但搜索空间大，收敛需大量计算资源，需新方法和系统。

Method: 引入IMPRESS系统，开发并实施自适应蛋白设计协议及计算基础设施。

Result: 实现动态资源分配和异步工作负载执行。

Conclusion: 提高蛋白设计质量的一致性和通量。

Abstract: Computational protein design is experiencing a transformation driven by
AI/ML. However, the range of potential protein sequences and structures is
astronomically vast, even for moderately sized proteins. Hence, achieving
convergence between generated and predicted structures demands substantial
computational resources for sampling. The Integrated Machine-learning for
Protein Structures at Scale (IMPRESS) offers methods and advanced computing
systems for coupling AI to high-performance computing tasks, enabling the
ability to evaluate the effectiveness of protein designs as they are developed,
as well as the models and simulations used to generate data and train models.
This paper introduces IMPRESS and demonstrates the development and
implementation of an adaptive protein design protocol and its supporting
computing infrastructure. This leads to increased consistency in the quality of
protein design and enhanced throughput of protein design due to dynamic
resource allocation and asynchronous workload execution.

</details>


### [42] [MuFASA -- Asynchronous Checkpoint for Weakly Consistent Fully Replicated Databases](https://arxiv.org/abs/2510.06404)
*Raaghav Ravishankar,Sandeep Kulkarni,Nitin H Vaidya*

Main category: cs.DC

TL;DR: 聚焦全复制弱一致性分布式数据库的检查点问题（DTCS），提出最小开销检查点算法，有显著优势。


<details>
  <summary>Details</summary>
Motivation: 最终一致性会产生意外异常，传统检查点有高开销或不一致问题，需解决。

Method: 定义全复制数据库的最小规模检查点概念，提出最小开销（仅O(n)新消息和添加一个计数器）的检查点算法。

Result: 算法相比现有分布式系统和主内存数据库的检查点算法有显著优势。

Conclusion: DTCS能将弱一致性计算用强一致性快照序列总结，出现异常时可聚焦异常时间点附近的快照。

Abstract: We focus on the problem of checkpointing in fully replicated weakly
consistent distributed databases, which we refer to as Distributed Transaction
Consistent Snapshot (DTCS). A typical example of such a system is a main-memory
database that provides strong eventual consistency. This problem is important
and challenging for several reasons: (1) eventual consistency often creates
anomalies that the users do not anticipate. Hence, frequent checkpoints to
ascertain desired invariants is highly beneficial in their use, and (2)
traditional checkpoints lead to significant overhead and/or inconsistencies. By
showing that the traditional checkpoint leads to inconsistencies or excessive
overhead, we define the notion of size-minimal checkpointing for fully
replicated databases. We present an algorithm for checkpointing with minimal
checkpointing overhead (only O(n) new messages and addition of a single counter
for existing messages). It also provides a significant benefit over existing
checkpointing algorithms for distributed systems and main-memory databases.
  A key benefit of DTCS is that it summarizes the computation by a sequence of
snapshots that are strongly consistent even though the underlying computation
is weakly consistent. In essence, when anomalies arise in an eventually
consistent system, DTCS enables one to concentrate solely on the snapshots
surrounding the time point of the anomaly.

</details>


### [43] [REACH: Reinforcement Learning for Adaptive Microservice Rescheduling in the Cloud-Edge Continuum](https://arxiv.org/abs/2510.06675)
*Xu Bai,Muhammed Tawfiqul Islam,Rajkumar Buyya,Adel N. Toosi*

Main category: cs.DC

TL;DR: 云边连续体结合边缘资源响应性和云可扩展性，但微服务最优放置面临挑战，提出REACH算法，实验表明其能降低平均端到端延迟并缓解波动。


<details>
  <summary>Details</summary>
Motivation: 云计算难以满足新兴低延迟应用需求，云边连续体中微服务在异构动态计算资源下的最优放置存在挑战。

Method: 提出REACH算法，利用强化学习实时动态调整微服务放置。

Result: 在真实测试平台上，REACH使三个基准MSA应用的平均端到端延迟分别降低7.9%、10%和8%，并有效缓解延迟波动和峰值。

Conclusion: REACH算法能有效解决云边连续体中微服务放置问题，降低延迟并缓解波动。

Abstract: Cloud computing, despite its advantages in scalability, may not always fully
satisfy the low-latency demands of emerging latency-sensitive pervasive
applications. The cloud-edge continuum addresses this by integrating the
responsiveness of edge resources with cloud scalability. Microservice
Architecture (MSA) characterized by modular, loosely coupled services, aligns
effectively with this continuum. However, the heterogeneous and dynamic
computing resource poses significant challenges to the optimal placement of
microservices. We propose REACH, a novel rescheduling algorithm that
dynamically adapts microservice placement in real time using reinforcement
learning to react to fluctuating resource availability, and performance
variations across distributed infrastructures. Extensive experiments on a
real-world testbed demonstrate that REACH reduces average end-to-end latency by
7.9%, 10%, and 8% across three benchmark MSA applications, while effectively
mitigating latency fluctuations and spikes.

</details>


### [44] [Multi-Dimensional Autoscaling of Stream Processing Services on Edge Devices](https://arxiv.org/abs/2510.06882)
*Boris Sedlak,Philipp Raith,Andrea Morichetta,Víctor Casamayor Pujol,Schahram Dustdar*

Main category: cs.DC

TL;DR: 本文提出多维自动扩展平台MUDAP及基于RASK的扩展代理，在边缘设备上实验表明，RASK能快速推断回归模型，减少SLO违规。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源有限，现有自动扩展机制仅关注资源扩展，需新方法维持服务水平目标。

Method: 引入MUDAP支持细粒度垂直扩展，提出基于RASK的扩展代理探索解决方案空间并学习回归模型。

Result: RASK在20次迭代内推断出准确回归模型，增加弹性维度后，比基线减少28%的SLO违规。

Conclusion: RASK代理在边缘设备服务扩展中表现良好，可有效维持服务水平目标。

Abstract: Edge devices have limited resources, which inevitably leads to situations
where stream processing services cannot satisfy their needs. While existing
autoscaling mechanisms focus entirely on resource scaling, Edge devices require
alternative ways to sustain the Service Level Objectives (SLOs) of competing
services. To address these issues, we introduce a Multi-dimensional Autoscaling
Platform (MUDAP) that supports fine-grained vertical scaling across both
service- and resource-level dimensions. MUDAP supports service-specific scaling
tailored to available parameters, e.g., scale data quality or model size for a
particular service. To optimize the execution across services, we present a
scaling agent based on Regression Analysis of Structural Knowledge (RASK). The
RASK agent efficiently explores the solution space and learns a continuous
regression model of the processing environment for inferring optimal scaling
actions. We compared our approach with two autoscalers, the Kubernetes VPA and
a reinforcement learning agent, for scaling up to 9 services on a single Edge
device. Our results showed that RASK can infer an accurate regression model in
merely 20 iterations (i.e., observe 200s of processing). By increasingly adding
elasticity dimensions, RASK sustained the highest request load with 28% less
SLO violations, compared to baselines.

</details>


### [45] [GROMACS Unplugged: How Power Capping and Frequency Shapes Performance on GPUs](https://arxiv.org/abs/2510.06902)
*Ayesha Afzal,Anna Kahler,Georg Hager,Gerhard Wellein*

Main category: cs.DC

TL;DR: 对四种NVIDIA GPU加速器用GROMACS工作负载和合成基准测试进行性能分析，给出频率缩放行为及功率限制下表现，为GPU选择和性能优化提供指导。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟性能依赖硬件选择和配置，需对NVIDIA GPU加速器进行性能分析以指导硬件选择和性能优化。

Method: 使用六种代表性GROMACS生物分子工作负载和两个合成基准测试（Pi Solver和STREAM Triad）对A40、A100、L4和L40四种NVIDIA GPU加速器进行性能分析，研究性能随GPU图形时钟频率的缩放以及工作负载对功率限制的响应。

Result: 不同规模GROMACS系统频率缩放行为不同，小系统对频率敏感，大系统易饱和且受内存带宽限制；功率限制下，达到特定阈值前性能稳定，高端GPU如A100在低功率预算下仍能保持接近最大性能。

Conclusion: 研究结果为在功率限制下大规模MD工作流程选择GPU硬件和优化GROMACS性能提供了实用指导。

Abstract: Molecular dynamics simulations are essential tools in computational
biophysics, but their performance depend heavily on hardware choices and
configuration. In this work, we presents a comprehensive performance analysis
of four NVIDIA GPU accelerators -- A40, A100, L4, and L40 -- using six
representative GROMACS biomolecular workloads alongside two synthetic
benchmarks: Pi Solver (compute bound) and STREAM Triad (memory bound). We
investigate how performance scales with GPU graphics clock frequency and how
workloads respond to power capping. The two synthetic benchmarks define the
extremes of frequency scaling: Pi Solver shows ideal compute scalability, while
STREAM Triad reveals memory bandwidth limits -- framing GROMACS's performance
in context. Our results reveal distinct frequency scaling behaviors: Smaller
GROMACS systems exhibit strong frequency sensitivity, while larger systems
saturate quickly, becoming increasingly memory bound. Under power capping,
performance remains stable until architecture- and workload-specific thresholds
are reached, with high-end GPUs like the A100 maintaining near-maximum
performance even under reduced power budgets. Our findings provide practical
guidance for selecting GPU hardware and optimizing GROMACS performance for
large-scale MD workflows under power constraints.

</details>


### [46] [Evaluating Rapid Makespan Predictions for Heterogeneous Systems with Programmable Logic](https://arxiv.org/abs/2510.06998)
*Martin Wilhelm,Franz Freitag,Max Tzschoppe,Thilo Pionteck*

Main category: cs.DC

TL;DR: 本文为异构系统开发快速完工时间预测算法提供灵活评估框架，分析现有分析方法对实际完工时间的预测能力并指出常见挑战。


<details>
  <summary>Details</summary>
Motivation: 异构计算系统中任务映射对整体完工时间影响难预测，现有模拟器耗时长，纯分析函数与实际差距大，需开发快速预测算法。

Method: 提供一个基于抽象任务图描述、能收集实际完工时间结果的高度灵活评估框架。

Result: 分析现有分析方法对实际完工时间的预测程度。

Conclusion: 指出异构系统中数据传输开销和设备拥塞等高层特征带来的常见挑战。

Abstract: Heterogeneous computing systems, which combine general-purpose processors
with specialized accelerators, are increasingly important for optimizing the
performance of modern applications. A central challenge is to decide which
parts of an application should be executed on which accelerator or, more
generally, how to map the tasks of an application to available devices.
Predicting the impact of a change in a task mapping on the overall makespan is
non-trivial. While there are very capable simulators, these generally require a
full implementation of the tasks in question, which is particularly
time-intensive for programmable logic. A promising alternative is to use a
purely analytical function, which allows for very fast predictions, but
abstracts significantly from reality. Bridging the gap between theory and
practice poses a significant challenge to algorithm developers. This paper aims
to aid in the development of rapid makespan prediction algorithms by providing
a highly flexible evaluation framework for heterogeneous systems consisting of
CPUs, GPUs and FPGAs, which is capable of collecting real-world makespan
results based on abstract task graph descriptions. We analyze to what extent
actual makespans can be predicted by existing analytical approaches.
Furthermore, we present common challenges that arise from high-level
characteristics such as data transfer overhead and device congestion in
heterogeneous systems.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [47] [Constant Weighted Maximin Share Approximations for Chores](https://arxiv.org/abs/2510.06581)
*Bo Li,Fangxiao Wang,Shiji Xing*

Main category: cs.GT

TL;DR: 本文研究非对称权重下不可分家务的公平分配，提出首个常数因子近似加权最大最小份额（WMMS）算法，并证明无法保证优于2 - 近似。


<details>
  <summary>Details</summary>
Motivation: 解决加权公平分配中WMMS是否存在常数因子近似这一重要开放问题，当前最佳近似比为O(log n)。

Method: 引入规范实例约简和不同的代理人估值界限。

Result: 提出首个常数因子近似WMMS算法，证明无法保证优于2 - 近似，改进了已知的1.366的下界。

Conclusion: 在加权公平分配的WMMS近似上取得进展，给出常数因子近似算法并明确了近似界限。

Abstract: We study the fair allocation of indivisible chores among agents with
asymmetric weights. Among the various fairness notions, weighted maximin share
(WMMS) stands out as particularly compelling. However, whether WMMS admits a
constant-factor approximation has remained unknown and is one of the important
open problems in weighted fair division [ALMW22, Suk25]. So far, the best known
approximation ratio is O(log n), where n is the number of agents. In this
paper, we advance the state of the art and present the first constant-factor
approximate WMMS algorithm. To this end, we introduce canonical instance
reductions and different bounds of agents' valuations. We also prove that
guaranteeing better than 2-approximation is not possible, which improves the
best-known lower bound of 1.366.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [48] [LLM-Powered Nuanced Video Attribute Annotation for Enhanced Recommendations](https://arxiv.org/abs/2510.06657)
*Boyuan Long,Yueqi Wang,Hiloni Mehta,Mick Zomnir,Omkar Pathak,Changping Meng,Ruolin Jia,Yajun Peng,Dapeng Hong,Xia Wu,Mingyan Gao,Onkar Dalal,Ningren Han*

Main category: cs.IR

TL;DR: 本文通过案例研究，介绍在大规模工业短视频推荐系统中使用大语言模型作为‘标注’机制实现内容理解的方法，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习分类器在内容理解方面开发周期长且缺乏深度细微理解，需要新方法解决。

Method: 提出端到端工作流程，包括定义和评估目标属性、用大语言模型进行离线批量标注、将标注集成到在线推荐系统。

Result: 大语言模型在离线标注质量上优于人类评分者，在线A/B测试中显著提升用户参与度和消费满意度。

Conclusion: 为设计和扩展生产级大语言模型管道提供见解，凸显其在提升内容发现、用户满意度和推荐系统效能方面的适应性和优势。

Abstract: This paper presents a case study on deploying Large Language Models (LLMs) as
an advanced "annotation" mechanism to achieve nuanced content understanding
(e.g., discerning content "vibe") at scale within a large-scale industrial
short-form video recommendation system. Traditional machine learning
classifiers for content understanding face protracted development cycles and a
lack of deep, nuanced comprehension. The "LLM-as-annotators" approach addresses
these by significantly shortening development times and enabling the annotation
of subtle attributes. This work details an end-to-end workflow encompassing:
(1) iterative definition and robust evaluation of target attributes, refined by
offline metrics and online A/B testing; (2) scalable offline bulk annotation of
video corpora using LLMs with multimodal features, optimized inference, and
knowledge distillation for broad application; and (3) integration of these rich
annotations into the online recommendation serving system, for example, through
personalized restrict retrieval. Experimental results demonstrate the efficacy
of this approach, with LLMs outperforming human raters in offline annotation
quality for nuanced attributes and yielding significant improvements of user
participation and satisfied consumption in online A/B tests. The study provides
insights into designing and scaling production-level LLM pipelines for rich
content evaluation, highlighting the adaptability and benefits of LLM-generated
nuanced understanding for enhancing content discovery, user satisfaction, and
the overall effectiveness of modern recommendation systems.

</details>


### [49] [Can We Hide Machines in the Crowd? Quantifying Equivalence in LLM-in-the-loop Annotation Tasks](https://arxiv.org/abs/2510.06658)
*Jiaman He,Zikang Leng,Dana McKay,Damiano Spina,Johanne R. Trippas*

Main category: cs.IR

TL;DR: 本文提出超越LLM文本标注有效性评估的方法，测试其能否融入人类标注者群体，并在两个数据集上应用，发现结果有任务依赖性。


<details>
  <summary>Details</summary>
Motivation: 以往LLM文本标注评估主要关注输出正确性，本文旨在探索人类和LLM标注决策在个体间的统计评估方法。

Method: 基于Krippendorff's α、配对自举和TOST等价检验程序开发统计评估方法，测试LLM能否融入人类标注者群体。

Result: 在MovieLens 100K数据集上LLM与人类标注者统计上不可区分（p = 0.004），在PolitiFact数据集上则可区分（p = 0.155）。

Conclusion: 该评估方法能发现任务依赖差异，可基于小样本人类数据进行早期评估，以判断LLM是否适用于大规模标注。

Abstract: Many evaluations of large language models (LLMs) in text annotation focus
primarily on the correctness of the output, typically comparing model-generated
labels to human-annotated ``ground truth'' using standard performance metrics.
In contrast, our study moves beyond effectiveness alone. We aim to explore how
labeling decisions -- by both humans and LLMs -- can be statistically evaluated
across individuals. Rather than treating LLMs purely as annotation systems, we
approach LLMs as an alternative annotation mechanism that may be capable of
mimicking the subjective judgments made by humans. To assess this, we develop a
statistical evaluation method based on Krippendorff's $\alpha$, paired
bootstrapping, and the Two One-Sided t-Tests (TOST) equivalence test procedure.
This evaluation method tests whether an LLM can blend into a group of human
annotators without being distinguishable.
  We apply this approach to two datasets -- MovieLens 100K and PolitiFact --
and find that the LLM is statistically indistinguishable from a human annotator
in the former ($p = 0.004$), but not in the latter ($p = 0.155$), highlighting
task-dependent differences. It also enables early evaluation on a small sample
of human data to inform whether LLMs are suitable for large-scale annotation in
a given application.

</details>


### [50] [Reproducing and Extending Causal Insights Into Term Frequency Computation in Neural Rankers](https://arxiv.org/abs/2510.06728)
*Cile van Marken,Roxana Petcu*

Main category: cs.IR

TL;DR: 本文旨在复现Chen等人的研究成果，验证其主要观点，并扩展框架纳入新公理，成功识别编码公理的注意力头以分析神经排序模型决策过程。


<details>
  <summary>Details</summary>
Motivation: 神经排序模型内部决策过程不清晰，多数可解释性方法关注相关性而非因果关系，需要深入探究模型如何计算文档相关性及其中预定义检索公理的存在情况。

Method: 复现Chen等人的研究，扩展其激活补丁框架，纳入额外的词频公理。

Result: 验证了Chen等人的主要观点，成功识别出编码新公理的一组注意力头。

Conclusion: 对神经排序模型的内部决策过程有了更深入的了解，通过识别编码公理的注意力头为模型可解释性提供了新见解。

Abstract: Neural ranking models have shown outstanding performance across a variety of
tasks, such as document retrieval, re-ranking, question answering and
conversational retrieval. However, the inner decision process of these models
remains largely unclear, especially as models increase in size. Most
interpretability approaches, such as probing, focus on correlational insights
rather than establishing causal relationships. The paper 'Axiomatic Causal
Interventions for Reverse Engineering Relevance Computation in Neural Retrieval
Models' by Chen et al. addresses this gap by introducing a framework for
activation patching - a causal interpretability method - in the information
retrieval domain, offering insights into how neural retrieval models compute
document relevance. The study demonstrates that neural ranking models not only
capture term-frequency information, but also that these representations can be
localized to specific components of the model, such as individual attention
heads or layers. This paper aims to reproduce the findings by Chen et al. and
to further explore the presence of pre-defined retrieval axioms in neural IR
models. We validate the main claims made by Chen et al., and extend the
framework to include an additional term-frequency axiom, which states that the
impact of increasing query term frequency on document ranking diminishes as the
frequency becomes higher. We successfully identify a group of attention heads
that encode this axiom and analyze their behavior to give insight into the
inner decision-making process of neural ranking models.

</details>


### [51] [Crossing Domains without Labels: Distant Supervision for Term Extraction](https://arxiv.org/abs/2510.06838)
*Elena Senger,Yuri Campbell,Rob van der Goot,Barbara Plank*

Main category: cs.IR

TL;DR: 本文提出综合基准和基于大语言模型的自动术语提取（ATE）模型，在多个领域表现出色并发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 现有ATE方法需大量人工标注且难以进行领域迁移，需要更强大、可扩展的解决方案和现实评估设置。

Method: 构建七个领域的综合基准，用大语言模型生成伪标签，微调大语言模型用于ATE，引入轻量级事后启发式方法提升文档级一致性。

Result: 该方法在5/7的领域超越先前方法，平均提升10个百分点。

Conclusion: 提出的基准和模型有效，发布数据集和微调模型以支持后续研究。

Abstract: Automatic Term Extraction (ATE) is a critical component in downstream NLP
tasks such as document tagging, ontology construction and patent analysis.
Current state-of-the-art methods require expensive human annotation and
struggle with domain transfer, limiting their practical deployment. This
highlights the need for more robust, scalable solutions and realistic
evaluation settings. To address this, we introduce a comprehensive benchmark
spanning seven diverse domains, enabling performance evaluation at both the
document- and corpus-levels. Furthermore, we propose a robust LLM-based model
that outperforms both supervised cross-domain encoder models and few-shot
learning baselines and performs competitively with its GPT-4o teacher on this
benchmark. The first step of our approach is generating psuedo-labels with this
black-box LLM on general and scientific domains to ensure generalizability.
Building on this data, we fine-tune the first LLMs for ATE. To further enhance
document-level consistency, oftentimes needed for downstream tasks, we
introduce lightweight post-hoc heuristics. Our approach exceeds previous
approaches on 5/7 domains with an average improvement of 10 percentage points.
We release our dataset and fine-tuned models to support future research in this
area.

</details>


### [52] [M3Retrieve: Benchmarking Multimodal Retrieval for Medicine](https://arxiv.org/abs/2510.06888)
*Arkadeep Acharya,Akash Ghosh,Pradeepika Verma,Kitsuchart Pasupa,Sriparna Saha,Priti Singh*

Main category: cs.IR

TL;DR: 介绍了多模态医学检索基准M3Retrieve，含多领域数据，用于评估多模态检索模型并推动医学应用研究，数据集和代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估多模态检索模型在医疗场景表现的标准基准。

Method: 引入涵盖5个领域、16个医学领域、4个不同任务，包含超120万文本和16.4万多模态查询的M3Retrieve基准，并在其上评估领先的多模态检索模型。

Result: 在M3Retrieve上评估模型可探索不同医学专业的特定挑战及其对检索性能的影响。

Conclusion: 发布M3Retrieve可实现系统评估、促进模型创新，加速构建更强大可靠的医学多模态检索系统。

Abstract: With the increasing use of RetrievalAugmented Generation (RAG), strong
retrieval models have become more important than ever. In healthcare,
multimodal retrieval models that combine information from both text and images
offer major advantages for many downstream tasks such as question answering,
cross-modal retrieval, and multimodal summarization, since medical data often
includes both formats. However, there is currently no standard benchmark to
evaluate how well these models perform in medical settings. To address this
gap, we introduce M3Retrieve, a Multimodal Medical Retrieval Benchmark.
M3Retrieve, spans 5 domains,16 medical fields, and 4 distinct tasks, with over
1.2 Million text documents and 164K multimodal queries, all collected under
approved licenses. We evaluate leading multimodal retrieval models on this
benchmark to explore the challenges specific to different medical specialities
and to understand their impact on retrieval performance. By releasing
M3Retrieve, we aim to enable systematic evaluation, foster model innovation,
and accelerate research toward building more capable and reliable multimodal
retrieval systems for medical applications. The dataset and the baselines code
are available in this github page https://github.com/AkashGhosh/M3Retrieve.

</details>


### [53] [Ethical AI prompt recommendations in large language models using collaborative filtering](https://arxiv.org/abs/2510.06924)
*Jordan Nelson,Almas Baimagambetov,Konstantinos Avgerinakis,Nikolaos Polatidis*

Main category: cs.IR

TL;DR: 本文提出用协同过滤技术增强大语言模型的道德提示选择，还创建了合成数据集，应对道德AI挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推动AI发展时存在偏见、公平性和问责等伦理问题，传统监督方法缺乏可扩展性，需动态解决方案。

Method: 采用推荐系统中的协同过滤技术，利用用户交互来促进道德准则并减少偏差。

Result: 创建了用于提示推荐的合成数据集并应用了协同过滤技术。

Conclusion: 该方法能应对道德AI中的偏见缓解、透明度和防止不道德提示工程等挑战。

Abstract: As large language models (LLMs) shape AI development, ensuring ethical prompt
recommendations is crucial. LLMs offer innovation but risk bias, fairness
issues, and accountability concerns. Traditional oversight methods struggle
with scalability, necessitating dynamic solutions. This paper proposes using
collaborative filtering, a technique from recommendation systems, to enhance
ethical prompt selection. By leveraging user interactions, it promotes ethical
guidelines while reducing bias. Contributions include a synthetic dataset for
prompt recommendations and the application of collaborative filtering. The work
also tackles challenges in ethical AI, such as bias mitigation, transparency,
and preventing unethical prompt engineering.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [54] [Making and Evaluating Calibrated Forecasts](https://arxiv.org/abs/2510.06388)
*Yuxuan Lu,Yifan Wu,Jason Hartline,Lunjia Hu*

Main category: cs.LG

TL;DR: 文章引入多分类预测任务的完美真实校准度量，推广了前人工作，证明并验证其具有稳健性。


<details>
  <summary>Details</summary>
Motivation: 为多分类预测任务设计合适校准度量，解决之前校准度量非真实及已有方法非稳健性问题。

Method: 推广Hartline等人工作，研究从二分类到多分类校准度量扩展方法，通过数学证明和实验验证。

Result: 引入多分类完美真实校准度量，证明其在不同超参数下能稳健保持主导和被主导预测器的顺序。

Conclusion: 新的校准度量既具有真实性，又解决了已有方法的非稳健性问题。

Abstract: Calibrated predictions can be reliably interpreted as probabilities. An
important step towards achieving better calibration is to design an appropriate
calibration measure to meaningfully assess the miscalibration level of a
predictor. A recent line of work initiated by Haghtalab et al. [2024] studies
the design of truthful calibration measures: a truthful measure is minimized
when a predictor outputs the true probabilities, whereas a non-truthful measure
incentivizes the predictor to lie so as to appear more calibrated. All previous
calibration measures were non-truthful until Hartline et al. [2025] introduced
the first perfectly truthful calibration measures for binary prediction tasks
in the batch setting.
  We introduce a perfectly truthful calibration measure for multi-class
prediction tasks, generalizing the work of Hartline et al. [2025] beyond binary
prediction. We study common methods of extending calibration measures from
binary to multi-class prediction and identify ones that do or do not preserve
truthfulness. In addition to truthfulness, we mathematically prove and
empirically verify that our calibration measure exhibits superior robustness:
it robustly preserves the ordering between dominant and dominated predictors,
regardless of the choice of hyperparameters (bin sizes). This result addresses
the non-robustness issue of binned ECE, which has been observed repeatedly in
prior work.

</details>


### [55] [RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases](https://arxiv.org/abs/2510.06267)
*Khartik Uppalapati,Shakeel Abdulkareem,Bora Yimenicioglu*

Main category: cs.LG

TL;DR: 提出RareGraph - Synth框架生成超罕见病合成电子健康记录轨迹，在保真度和隐私性上表现良好。


<details>
  <summary>Details</summary>
Motivation: 为超罕见病生成逼真且保护隐私的合成电子健康记录轨迹，以实现更安全的数据共享用于研究。

Method: 将五个公共资源统一成异质知识图，用元路径分数调制前向随机微分方程中的每个标记噪声调度，反向去噪器生成无敏感信息的三元组序列。

Result: 在模拟超罕见病队列中，降低分类最大均值差异，黑盒成员推理评估AUROC远低于安全阈值，优于非知识图基线。

Conclusion: 将生物医学知识图直接集成到扩散噪声调度中可同时提高保真度和隐私性，利于罕见病研究的数据共享。

Abstract: We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion
framework that generates realistic yet privacy-preserving synthetic
electronic-health-record (EHR) trajectories for ultra-rare diseases.
RareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human
Phenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA
Adverse Event Reporting System (FAERS) into a heterogeneous knowledge graph
comprising approximately 8 M typed edges. Meta-path scores extracted from this
8-million-edge KG modulate the per-token noise schedule in the forward
stochastic differential equation, steering generation toward biologically
plausible lab-medication-adverse-event co-occurrences while retaining
score-based diffusion model stability. The reverse denoiser then produces
timestamped sequences of lab-code, medication-code, and adverse-event-flag
triples that contain no protected health information. On simulated
ultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean
Discrepancy by 40 percent relative to an unguided diffusion baseline and by
greater than 60 percent versus GAN counterparts, without sacrificing downstream
predictive utility. A black-box membership-inference evaluation using the
DOMIAS attacker yields AUROC approximately 0.53, well below the 0.55
safe-release threshold and substantially better than the approximately 0.61
plus or minus 0.03 observed for non-KG baselines, demonstrating strong
resistance to re-identification. These results suggest that integrating
biomedical knowledge graphs directly into diffusion noise schedules can
simultaneously enhance fidelity and privacy, enabling safer data sharing for
rare-disease research.

</details>


### [56] [MCCE: A Framework for Multi-LLM Collaborative Co-Evolution](https://arxiv.org/abs/2510.06270)
*Nian Ran,Zhongzheng Li,Yue Wang,Qingsong Ran,Xiaoyuan Zhang,Shikun Feng,Richard Allmendinger,Xiaoguang Zhao*

Main category: cs.LG

TL;DR: 提出Multi-LLM Collaborative Co - evolution (MCCE)混合框架解决多目标离散优化问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统进化算法在多目标离散优化问题中易陷入局部最优，闭源大语言模型无法更新参数，小模型知识和推理能力不足，需更好的优化方案。

Method: 引入MCCE混合框架，结合冻结的闭源大语言模型和可训练的轻量级模型，通过强化学习逐步优化小模型，二者相互支持互补进行全局探索。

Result: 在多目标药物设计基准测试中，MCCE达到了最先进的帕累托前沿质量，持续优于基线。

Conclusion: MCCE为混合大语言模型系统的持续进化提供了新范式，结合了知识驱动探索和经验驱动学习。

Abstract: Multi-objective discrete optimization problems, such as molecular design,
pose significant challenges due to their vast and unstructured combinatorial
spaces. Traditional evolutionary algorithms often get trapped in local optima,
while expert knowledge can provide crucial guidance for accelerating
convergence. Large language models (LLMs) offer powerful priors and reasoning
ability, making them natural optimizers when expert knowledge matters. However,
closed-source LLMs, though strong in exploration, cannot update their
parameters and thus cannot internalize experience. Conversely, smaller open
models can be continually fine-tuned but lack broad knowledge and reasoning
strength. We introduce Multi-LLM Collaborative Co-evolution (MCCE), a hybrid
framework that unites a frozen closed-source LLM with a lightweight trainable
model. The system maintains a trajectory memory of past search processes; the
small model is progressively refined via reinforcement learning, with the two
models jointly supporting and complementing each other in global exploration.
Unlike model distillation, this process enhances the capabilities of both
models through mutual inspiration. Experiments on multi-objective drug design
benchmarks show that MCCE achieves state-of-the-art Pareto front quality and
consistently outperforms baselines. These results highlight a new paradigm for
enabling continual evolution in hybrid LLM systems, combining knowledge-driven
exploration with experience-driven learning.

</details>


### [57] [Dynamic Regret Bounds for Online Omniprediction with Long Term Constraints](https://arxiv.org/abs/2510.07266)
*Yahav Bechavod,Jiuyao Lu,Aaron Roth*

Main category: cs.LG

TL;DR: 提出一种算法，为具有长期约束的在线全预测提供动态遗憾界。


<details>
  <summary>Details</summary>
Motivation: 解决在线全预测中学习者要让下游决策者选择行动以获最坏情况效用保证并最小化约束违反的问题。

Method: 给出能为所有代理获得同时动态遗憾保证的算法。

Result: 得到了动态遗憾保证，且确保每个代理的约束违反消失，代理只需解决单轮约束优化问题。

Conclusion: 该算法在在线全预测有良好效果，不要求代理维护状态。

Abstract: We present an algorithm guaranteeing dynamic regret bounds for online
omniprediction with long term constraints. The goal in this recently introduced
problem is for a learner to generate a sequence of predictions which are
broadcast to a collection of downstream decision makers. Each decision maker
has their own utility function, as well as a vector of constraint functions,
each mapping their actions and an adversarially selected state to reward or
constraint violation terms. The downstream decision makers select actions "as
if" the state predictions are correct, and the goal of the learner is to
produce predictions such that all downstream decision makers choose actions
that give them worst-case utility guarantees while minimizing worst-case
constraint violation. Within this framework, we give the first algorithm that
obtains simultaneous \emph{dynamic regret} guarantees for all of the agents --
where regret for each agent is measured against a potentially changing sequence
of actions across rounds of interaction, while also ensuring vanishing
constraint violation for each agent. Our results do not require the agents
themselves to maintain any state -- they only solve one-round constrained
optimization problems defined by the prediction made at that round.

</details>


### [58] [Vectorized FlashAttention with Low-cost Exponential Computation in RISC-V Vector Processors](https://arxiv.org/abs/2510.06834)
*Vasileios Titopoulos,Kosmas Alexandridis,Giorgos Dimitrakopoulos*

Main category: cs.LG

TL;DR: 本文聚焦于在基于RISC - V指令集架构的向量处理器中，用FlashAttention算法加速注意力内核，通过向量化、近似计算和分块策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 加速机器学习和人工智能模型中核心操作——注意力内核的计算。

Method: 首次对FlashAttention进行向量化，用低成本近似计算指数函数，探索合适的分块策略以提高内存局部性。

Result: 实验表明该方法具有可扩展性，在实际应用处理注意力层时，向量化实现带来显著性能提升。

Conclusion: 所采用的方法能有效加速注意力内核计算，提升性能。

Abstract: Attention is a core operation in numerous machine learning and artificial
intelligence models. This work focuses on the acceleration of attention kernel
using FlashAttention algorithm, in vector processors, particularly those based
on the RISC-V instruction set architecture (ISA). This work represents the
first effort to vectorize FlashAttention, minimizing scalar code and
simplifying the computational complexity of evaluating exponentials needed by
softmax used in attention. By utilizing a low-cost approximation for
exponentials in floating-point arithmetic, we reduce the cost of computing the
exponential function without the need to extend baseline vector ISA with new
custom instructions. Also, appropriate tiling strategies are explored with the
goal to improve memory locality. Experimental results highlight the scalability
of our approach, demonstrating significant performance gains with the
vectorized implementations when processing attention layers in practical
applications.

</details>


### [59] [The Effect of Label Noise on the Information Content of Neural Representations](https://arxiv.org/abs/2510.06401)
*Ali Hussaini Umar,Franky Kevin Nando Tezoh,Jean Barbier,Santiago Acevedo,Alessandro Laio*

Main category: cs.LG

TL;DR: 研究监督分类任务中标签噪声对网络隐藏表示的影响，发现隐藏表示信息含量呈双下降，超参数化网络表示对标签噪声鲁棒，还给出分类任务泛化新视角。


<details>
  <summary>Details</summary>
Motivation: 现有研究对标签噪声对网络隐藏表示的影响了解不足，需填补这一空白。

Method: 使用信息不平衡系统地比较隐藏表示。

Result: 隐藏表示信息含量随网络参数数量呈双下降；欠参数化时，含噪声标签学习的表示更具信息性，超参数化时两者信息性相同；超参数化时，倒数第二层和预softmax层的信息不平衡随交叉熵损失减小；随机标签学习的表示比随机特征表现差。

Conclusion: 超参数化网络的表示对标签噪声具有鲁棒性，为理解分类任务的泛化提供了新视角，训练随机标签使网络远超懒惰学习。

Abstract: In supervised classification tasks, models are trained to predict a label for
each data point. In real-world datasets, these labels are often noisy due to
annotation errors. While the impact of label noise on the performance of deep
learning models has been widely studied, its effects on the networks' hidden
representations remain poorly understood. We address this gap by systematically
comparing hidden representations using the Information Imbalance, a
computationally efficient proxy of conditional mutual information. Through this
analysis, we observe that the information content of the hidden representations
follows a double descent as a function of the number of network parameters,
akin to the behavior of the test error. We further demonstrate that in the
underparameterized regime, representations learned with noisy labels are more
informative than those learned with clean labels, while in the
overparameterized regime, these representations are equally informative. Our
results indicate that the representations of overparameterized networks are
robust to label noise. We also found that the information imbalance between the
penultimate and pre-softmax layers decreases with cross-entropy loss in the
overparameterized regime. This offers a new perspective on understanding
generalization in classification tasks. Extending our analysis to
representations learned from random labels, we show that these perform worse
than random features. This indicates that training on random labels drives
networks much beyond lazy learning, as weights adapt to encode labels
information.

</details>


### [60] [RVFL-X: A Novel Randomized Network Based on Complex Transformed Real-Valued Tabular Datasets](https://arxiv.org/abs/2510.06278)
*M. Sajid,Mushir Akhtar,A. Quadir,M. Tanveer*

Main category: cs.LG

TL;DR: 本文提出将实值数据集转换为复值表示的方法，构建复值扩展网络RVFL - X，实验表明其性能优于原网络和先进的随机神经网络变体。


<details>
  <summary>Details</summary>
Motivation: 现有随机神经网络中复数应用受限，缺乏将实值表格数据集转换为复值表示的有效方法。

Method: 提出自然变换和自编码器驱动两种方法生成复值表示，构建复值扩展网络RVFL - X，利用复数组件处理复值表示并输出实值。

Result: 在80个实值UCI数据集上评估，RVFL - X始终优于原始RVFL和最先进的随机神经网络变体。

Conclusion: RVFL - X具有鲁棒性和有效性，适用于不同应用领域。

Abstract: Recent advancements in neural networks, supported by foundational theoretical
insights, emphasize the superior representational power of complex numbers.
However, their adoption in randomized neural networks (RNNs) has been limited
due to the lack of effective methods for transforming real-valued tabular
datasets into complex-valued representations. To address this limitation, we
propose two methods for generating complex-valued representations from
real-valued datasets: a natural transformation and an autoencoder-driven
method. Building on these mechanisms, we propose RVFL-X, a complex-valued
extension of the random vector functional link (RVFL) network. RVFL-X
integrates complex transformations into real-valued datasets while maintaining
the simplicity and efficiency of the original RVFL architecture. By leveraging
complex components such as input, weights, and activation functions, RVFL-X
processes complex representations and produces real-valued outputs.
Comprehensive evaluations on 80 real-valued UCI datasets demonstrate that
RVFL-X consistently outperforms both the original RVFL and state-of-the-art
(SOTA) RNN variants, showcasing its robustness and effectiveness across diverse
application domains.

</details>


### [61] [On knot detection via picture recognition](https://arxiv.org/abs/2510.06284)
*Anne Dranowski,Yura Kabkov,Daniel Tubbenhauer*

Main category: cs.LG

TL;DR: 本文介绍一种结合机器学习和传统算法近似实现自动识别结的策略，展示简单基线模型效果，提出两阶段方法用于结分类。


<details>
  <summary>Details</summary>
Motivation: 实现用手机自动识别照片中的结。

Method: 结合现代机器学习方法（如卷积神经网络和变换器用于图像识别）和传统算法（计算量子不变量如琼斯多项式），还提出两阶段方法，先感知模块再符号重建。

Result: 简单基线模型能直接从图像预测交叉数，轻量级CNN和变换器架构可提取有意义结构信息。

Conclusion: 两阶段方法凸显了处理噪声视觉数据的机器学习和执行严格拓扑区分的不变量之间的互补性。

Abstract: Our goal is to one day take a photo of a knot and have a phone automatically
recognize it. In this expository work, we explain a strategy to approximate
this goal, using a mixture of modern machine learning methods (in particular
convolutional neural networks and transformers for image recognition) and
traditional algorithms (to compute quantum invariants like the Jones
polynomial). We present simple baselines that predict crossing number directly
from images, showing that even lightweight CNN and transformer architectures
can recover meaningful structural information. The longer-term aim is to
combine these perception modules with symbolic reconstruction into planar
diagram (PD) codes, enabling downstream invariant computation for robust knot
classification. This two-stage approach highlights the complementarity between
machine learning, which handles noisy visual data, and invariants, which
enforce rigorous topological distinctions.

</details>


### [62] [Bayesian Optimization under Uncertainty for Training a Scale Parameter in Stochastic Models](https://arxiv.org/abs/2510.06439)
*Akash Yadav,Ruda Zhang*

Main category: cs.LG

TL;DR: 提出用于不确定下超参数调优的贝叶斯优化框架，降低计算成本并通过算例验证有效性。


<details>
  <summary>Details</summary>
Motivation: 超参数调优在系统有不确定性时具挑战性，不确定下优化计算成本高。

Method: 提出针对不确定下超参数调优的贝叶斯优化框架，用统计替代和推导随机采集函数优化器的闭式表达式。

Result: 与传统一维蒙特卡罗优化方案相比，所需数据点少40倍，计算成本最多降低40倍。

Conclusion: 通过计算工程中的两个算例证明了所提方法的有效性。

Abstract: Hyperparameter tuning is a challenging problem especially when the system
itself involves uncertainty. Due to noisy function evaluations, optimization
under uncertainty can be computationally expensive. In this paper, we present a
novel Bayesian optimization framework tailored for hyperparameter tuning under
uncertainty, with a focus on optimizing a scale- or precision-type parameter in
stochastic models. The proposed method employs a statistical surrogate for the
underlying random variable, enabling analytical evaluation of the expectation
operator. Moreover, we derive a closed-form expression for the optimizer of the
random acquisition function, which significantly reduces computational cost per
iteration. Compared with a conventional one-dimensional Monte Carlo-based
optimization scheme, the proposed approach requires 40 times fewer data points,
resulting in up to a 40-fold reduction in computational cost. We demonstrate
the effectiveness of the proposed method through two numerical examples in
computational engineering.

</details>


### [63] [Traj-Transformer: Diffusion Models with Transformer for GPS Trajectory Generation](https://arxiv.org/abs/2510.06291)
*Zhiyang Zhang,Ningcong Chen,Xin Zhang,Yanhua Li,Shen Su,Hui Lu,Jun Luo*

Main category: cs.LG

TL;DR: 本文提出Trajectory Transformer模型用于轨迹生成，实验表明其能提升生成质量并缓解偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于卷积架构的轨迹生成方法存在偏差大、丢失细粒度信息的问题，需更好的模型。

Method: 提出Trajectory Transformer模型，采用transformer骨干进行条件信息嵌入和噪声预测，探索两种GPS坐标嵌入策略并分析不同尺度下的模型性能。

Result: 在两个真实数据集上实验，Trajectory Transformer显著提升了生成质量，有效缓解了先前方法的偏差问题。

Conclusion: Trajectory Transformer是一种有效的轨迹生成模型，能解决现有方法的不足。

Abstract: The widespread use of GPS devices has driven advances in spatiotemporal data
mining, enabling machine learning models to simulate human decision making and
generate realistic trajectories, addressing both data collection costs and
privacy concerns. Recent studies have shown the promise of diffusion models for
high-quality trajectory generation. However, most existing methods rely on
convolution based architectures (e.g. UNet) to predict noise during the
diffusion process, which often results in notable deviations and the loss of
fine-grained street-level details due to limited model capacity. In this paper,
we propose Trajectory Transformer, a novel model that employs a transformer
backbone for both conditional information embedding and noise prediction. We
explore two GPS coordinate embedding strategies, location embedding and
longitude-latitude embedding, and analyze model performance at different
scales. Experiments on two real-world datasets demonstrate that Trajectory
Transformer significantly enhances generation quality and effectively
alleviates the deviation issues observed in prior approaches.

</details>


### [64] [Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data](https://arxiv.org/abs/2510.06377)
*Rishabh Ranjan,Valter Hudovernik,Mark Znidar,Charilaos Kanatsoulis,Roshan Upendra,Mahmoud Mohammadi,Joe Meyer,Tom Palczewski,Carlos Guestrin,Jure Leskovec*

Main category: cs.LG

TL;DR: 提出关系变换器（RT）架构，可在不同关系数据库上预训练，零样本和微调表现佳，为关系数据基础模型提供路径。


<details>
  <summary>Details</summary>
Motivation: 现有预训练变换器在关系领域缺乏能跨数据集和任务迁移的架构，挑战源于关系数据的多样性。

Method: 提出RT架构，对单元格进行表/列元数据标记，通过掩码标记预测进行预训练，使用新型关系注意力机制。

Result: 在RelBench数据集上预训练，零样本性能强，单前向传播平均达全监督AUROC的94%（22M参数模型），优于27B大语言模型；微调有高样本效率。

Conclusion: RT的零样本迁移利用了任务表上下文、关系注意力模式和模式语义，为关系数据基础模型提供了实用途径。

Abstract: Pretrained transformers readily adapt to new sequence modeling tasks via
zero-shot prompting, but relational domains still lack architectures that
transfer across datasets and tasks. The core challenge is the diversity of
relational data, with varying heterogeneous schemas, graph structures and
functional dependencies. In this paper, we present the Relational Transformer
(RT) architecture, which can be pretrained on diverse relational databases and
directly applied to unseen datasets and tasks without task- or dataset-specific
fine-tuning, or retrieval of in-context examples. RT (i) tokenizes cells with
table/column metadata, (ii) is pretrained via masked token prediction, and
(iii) utilizes a novel \textit{Relational Attention} mechanism over columns,
rows, and primary-foreign key links. Pretrained on RelBench datasets spanning
tasks such as churn and sales forecasting, RT attains strong zero-shot
performance, averaging 94% of fully supervised AUROC on binary classification
tasks with a single forward pass of a 22M parameter model, as opposed to 84%
for a 27B LLM. Fine-tuning yields state-of-the-art results with high sample
efficiency. Our experiments show that RT's zero-shot transfer harnesses
task-table context, relational attention patterns and schema semantics.
Overall, RT provides a practical path toward foundation models for relational
data.

</details>


### [65] [BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression](https://arxiv.org/abs/2510.06293)
*Cristian Meo,Varun Sarathchandran,Avijit Majhi,Shao Hung,Carlo Saccardi,Ruben Imhoff,Roberto Deidda,Remko Uijlenhoet,Justin Dauwels*

Main category: cs.LG

TL;DR: 提出BlockGPT用于降水临近预报，在两数据集上评估，精度高且推理速度快。


<details>
  <summary>Details</summary>
Motivation: 现有降水预测方法存在归纳偏差、推理慢和计算量大等问题，需要准确且计算高效的模型用于实时应用。

Method: 引入使用批量标记化方法的生成式自回归变压器BlockGPT，通过帧内自注意力和帧间因果注意力分解时空。

Result: 在KNMI和SEVIR数据集上，BlockGPT精度高、事件定位好，推理速度比可比基线快达31倍。

Conclusion: BlockGPT在降水临近预报中表现出色，能解决现有方法的局限。

Abstract: Predicting precipitation maps is a highly complex spatiotemporal modeling
task, critical for mitigating the impacts of extreme weather events. Short-term
precipitation forecasting, or nowcasting, requires models that are not only
accurate but also computationally efficient for real-time applications. Current
methods, such as token-based autoregressive models, often suffer from flawed
inductive biases and slow inference, while diffusion models can be
computationally intensive. To address these limitations, we introduce BlockGPT,
a generative autoregressive transformer using batched tokenization (Block)
method that predicts full two-dimensional fields (frames) at each time step.
Conceived as a model-agnostic paradigm for video prediction, BlockGPT
factorizes space-time by using self-attention within each frame and causal
attention across frames; in this work, we instantiate it for precipitation
nowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI
(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines
including token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)
models. The results show that BlockGPT achieves superior accuracy, event
localization as measured by categorical metrics, and inference speeds up to 31x
faster than comparable baselines.

</details>


### [66] [SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation](https://arxiv.org/abs/2510.06303)
*Shuang Cheng,Yihan Bian,Dawei Liu,Yuhua Jiang,Yihao Liu,Linfeng Zhang,Wenhai Wang,Qipeng Guo,Kai Chen,Biqing Qi,Bowen Zhou*

Main category: cs.LG

TL;DR: 提出SDAR范式，结合自回归和扩散模型优势，实现高效转换与并行生成，有良好扩展性、推理和领域适应能力。


<details>
  <summary>Details</summary>
Motivation: 结合自回归模型训练效率和扩散模型并行推理能力。

Method: 通过轻量级范式转换，将训练好的自回归模型转换为分块扩散模型，推理时跨块自回归生成、块内并行解码。

Result: AR模型计算效率高，SDAR实现低成本转换，有良好扩展性，在推理和领域适应上表现出色。

Conclusion: SDAR是结合自回归和扩散优势的实用范式，可用于可扩展、高吞吐量推理。

Abstract: We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies
the training efficiency of autoregressive models with the parallel inference
capability of diffusion. Instead of costly end-to-end diffusion training, SDAR
performs a lightweight paradigm conversion that transforms a well-trained
autoregressive (AR) model into a blockwise diffusion model through brief,
data-efficient adaptation. During inference, SDAR generates sequences
autoregressively across blocks for global coherence while decoding all tokens
within each block in parallel via a discrete diffusion process. Extensive
experiments show that AR models remain substantially more compute-efficient
than masked diffusion models, providing a strong foundation for adaptation.
Building on this insight, SDAR achieves efficient AR-to-diffusion conversion
with minimal cost, preserving AR-level performance while enabling parallel
generation. Scaling studies across dense and Mixture-of-Experts architectures
confirm that SDAR scales without compromise: larger models exhibit stronger
robustness to block size and decoding thresholds, yielding greater speedups
without accuracy loss. Beyond efficiency, SDAR demonstrates enhanced reasoning
and domain adaptability. Our 30B MoE model surpasses its AR counterpart on
challenging scientific reasoning benchmarks such as GPQA and ChemBench, and
gains further improvements under test-time scaling methods like majority voting
and pass@k. Together, these results establish SDAR as a practical paradigm that
combines the strengths of autoregression and diffusion for scalable,
high-throughput reasoning.

</details>


### [67] [Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks](https://arxiv.org/abs/2510.06349)
*Moein E. Samadi,Andreas Schuppert*

Main category: cs.LG

TL;DR: 讨论基础模型在现实领域决策是否超人类，指出复杂系统适应动态环境挑战，提出分散式小代理网络（SANs）架构，认为其在动态环境决策上优于基础模型。


<details>
  <summary>Details</summary>
Motivation: 探讨基础模型决策能否超人类，解决复杂系统适应动态环境及用最少数据和有限机制知识开发自适应AI模型的问题。

Method: 提出分散式的小代理网络（SANs）架构，聚焦代表系统子结构的代理，利用SANs学习行为数学结果和现有应用证据。

Result: 多样群体的群体学习能让自适应SANs在动态环境中比整体基础模型有更优决策，但细节可重复性降低。

Conclusion: AI在承担更广泛决策角色前应在相关场景展现优势，SANs在动态环境决策上比基础模型更优。

Abstract: Foundation models have rapidly advanced AI, raising the question of whether
their decisions will ultimately surpass human strategies in real-world domains.
The exponential, and possibly super-exponential, pace of AI development makes
such analysis elusive. Nevertheless, many application areas that matter for
daily life and society show only modest gains so far; a prominent case is
diagnosing and treating dynamically evolving disease in intensive care.
  The common challenge is adapting complex systems to dynamic environments.
Effective strategies must optimize outcomes in systems composed of strongly
interacting functions while avoiding shared side effects; this requires
reliable, self-adaptive modeling. These tasks align with building digital twins
of highly complex systems whose mechanisms are not fully or quantitatively
understood. It is therefore essential to develop methods for self-adapting AI
models with minimal data and limited mechanistic knowledge. As this challenge
extends beyond medicine, AI should demonstrate clear superiority in these
settings before assuming broader decision-making roles.
  We identify the curse of dimensionality as a fundamental barrier to efficient
self-adaptation and argue that monolithic foundation models face conceptual
limits in overcoming it. As an alternative, we propose a decentralized
architecture of interacting small agent networks (SANs). We focus on agents
representing the specialized substructure of the system, where each agent
covers only a subset of the full system functions. Drawing on mathematical
results on the learning behavior of SANs and evidence from existing
applications, we argue that swarm-learning in diverse swarms can enable
self-adaptive SANs to deliver superior decision-making in dynamic environments
compared with monolithic foundation models, though at the cost of reduced
reproducibility in detail.

</details>


### [68] [Context-Aware Inference via Performance Forecasting in Decentralized Learning Networks](https://arxiv.org/abs/2510.06444)
*Joel Pfeffer,J. M. Diederik Kruijssen,Clément Gossart,Mélanie Chevance,Diego Campo Millan,Florian Stecker,Steven N. Longmore*

Main category: cs.LG

TL;DR: 本文针对分散学习网络现有线性池化方法的局限性，提出用机器学习预测各时期模型性能的方法，可提升网络推理准确性，且预测性能受特征集和训练时期数影响，该方法有更广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有分散学习网络线性池化方法依赖历史表现更新权重，调整慢，需解决对变化情况适应慢的问题。

Method: 开发机器学习模型预测时间序列中各时期模型预测性能，在分散学习网络中加入性能预测工作者。

Result: 加入性能预测工作者可提升网络推理准确性，预测后悔值或后悔值z分数的模型比预测损失的模型效果更好，预测模型性能受特征集和训练时期数选择影响。

Conclusion: 该性能预测方法虽为分散学习网络设计，但在需预测性而非反应性模型加权的场景中可能有用。

Abstract: In decentralized learning networks, predictions from many participants are
combined to generate a network inference. While many studies have demonstrated
performance benefits of combining multiple model predictions, existing
strategies using linear pooling methods (ranging from simple averaging to
dynamic weight updates) face a key limitation. Dynamic prediction combinations
that rely on historical performance to update weights are necessarily reactive.
Due to the need to average over a reasonable number of epochs (with moving
averages or exponential weighting), they tend to be slow to adjust to changing
circumstances (phase or regime changes). In this work, we develop a model that
uses machine learning to forecast the performance of predictions by models at
each epoch in a time series. This enables `context-awareness' by assigning
higher weight to models that are likely to be more accurate at a given time. We
show that adding a performance forecasting worker in a decentralized learning
network, following a design similar to the Allora network, can improve the
accuracy of network inferences. Specifically, we find forecasting models that
predict regret (performance relative to the network inference) or regret
z-score (performance relative to other workers) show greater improvement than
models predicting losses, which often do not outperform the naive network
inference (historically weighted average of all inferences). Through a series
of optimization tests, we show that the performance of the forecasting model
can be sensitive to choices in the feature set and number of training epochs.
These properties may depend on the exact problem and should be tailored to each
domain. Although initially designed for a decentralized learning network, using
performance forecasting for prediction combination may be useful in any
situation where predictive rather than reactive model weighting is needed.

</details>


### [69] [PIKAN: Physics-Inspired Kolmogorov-Arnold Networks for Explainable UAV Channel Modelling](https://arxiv.org/abs/2510.06355)
*Kürşat Tekbıyık,Güneş Karabulut Kurt,Antoine Lesage-Landry*

Main category: cs.LG

TL;DR: 提出物理启发的Kolmogorov - Arnold网络（PIKAN）用于无人机A2G信道建模，实验表明其准确、可解释且轻量级。


<details>
  <summary>Details</summary>
Motivation: 现有确定性模型和深度学习模型在无人机A2G信道建模中分别存在刚性或缺乏可解释性问题，需新模型。

Method: 提出PIKAN，将物理原理作为灵活归纳偏置嵌入学习过程。

Result: PIKAN与DL模型精度相当，参数少至232个，比MLP轻37倍，有符号可解释表达式。

Conclusion: PIKAN是适用于5G后和6G网络无人机信道建模的高效、可解释和可扩展解决方案。

Abstract: Unmanned aerial vehicle (UAV) communications demand accurate yet
interpretable air-to-ground (A2G) channel models that can adapt to
nonstationary propagation environments. While deterministic models offer
interpretability and deep learning (DL) models provide accuracy, both
approaches suffer from either rigidity or a lack of explainability. To bridge
this gap, we propose the Physics-Inspired Kolmogorov-Arnold Network (PIKAN)
that embeds physical principles (e.g., free-space path loss, two-ray
reflections) into the learning process. Unlike physics-informed neural networks
(PINNs), PIKAN is more flexible for applying physical information because it
introduces them as flexible inductive biases. Thus, it enables a more flexible
training process. Experiments on UAV A2G measurement data show that PIKAN
achieves comparable accuracy to DL models while providing symbolic and
explainable expressions aligned with propagation laws. Remarkably, PIKAN
achieves this performance with only 232 parameters, making it up to 37 times
lighter than multilayer perceptron (MLP) baselines with thousands of
parameters, without sacrificing correlation with measurements and also
providing symbolic expressions. These results highlight PIKAN as an efficient,
interpretable, and scalable solution for UAV channel modelling in beyond-5G and
6G networks.

</details>


### [70] [Spiral Model Technique For Data Science & Machine Learning Lifecycle](https://arxiv.org/abs/2510.06987)
*Rohith Mahadevan*

Main category: cs.LG

TL;DR: 本文提出用螺旋技术将数据科学生命周期融入有明确目标的商业问题，强调业务流程的多功能性、敏捷性和迭代性。


<details>
  <summary>Details</summary>
Motivation: 分析在现代商业中很重要，公司需将数据科学生命周期融入自身文化以提高生产力和竞争力，传统数据科学生命周期有局限性。

Method: 引入名为螺旋技术的新方法。

Result: 介绍了新的螺旋技术。

Conclusion: 螺旋技术可用于将数据科学生命周期融入有明确目标的商业问题，能体现多功能性、敏捷性和迭代性。

Abstract: Analytics play an important role in modern business. Companies adapt data
science lifecycles to their culture to seek productivity and improve their
competitiveness among others. Data science lifecycles are fairly an important
contributing factor to start and end a project that are data dependent. Data
science and Machine learning life cycles comprises of series of steps that are
involved in a project. A typical life cycle states that it is a linear or
cyclical model that revolves around. It is mostly depicted that it is possible
in a traditional data science life cycle to start the process again after
reaching the end of cycle. This paper suggests a new technique to incorporate
data science life cycle to business problems that have a clear end goal. A new
technique called spiral technique is introduced to emphasize versatility,
agility and iterative approach to business processes.

</details>


### [71] [Lagrangian neural ODEs: Measuring the existence of a Lagrangian with Helmholtz metrics](https://arxiv.org/abs/2510.06367)
*Luca Wolf,Tobias Buck,Bjoern Malte Schaefer*

Main category: cs.LG

TL;DR: 提出Helmholtz度量量化ODE与物理解相似性，结合二阶神经ODE形成拉格朗日神经ODE，可区分系统并改善解。


<details>
  <summary>Details</summary>
Motivation: 并非所有神经ODE解都符合欧拉 - 拉格朗日方程，需量化其与物理解的相似性。

Method: 提出Helmholtz度量，并将其与二阶神经ODE结合形成拉格朗日神经ODE。

Result: 能仅用位置数据区分拉格朗日和非拉格朗日系统，改善神经ODE解。

Conclusion: 拉格朗日神经ODE可直接学习欧拉 - 拉格朗日方程，且无额外推理成本，能有效区分系统和改善解。

Abstract: Neural ODEs are a widely used, powerful machine learning technique in
particular for physics. However, not every solution is physical in that it is
an Euler-Lagrange equation. We present Helmholtz metrics to quantify this
resemblance for a given ODE and demonstrate their capabilities on several
fundamental systems with noise. We combine them with a second order neural ODE
to form a Lagrangian neural ODE, which allows to learn Euler-Lagrange equations
in a direct fashion and with zero additional inference cost. We demonstrate
that, using only positional data, they can distinguish Lagrangian and
non-Lagrangian systems and improve the neural ODE solutions.

</details>


### [72] [Monte Carlo Permutation Search](https://arxiv.org/abs/2510.06381)
*Tristan Cazenave*

Main category: cs.LG

TL;DR: 提出改进GRAVE算法的MCPS算法，在多种游戏测试，结果表明其在双人游戏中优于GRAVE，多人游戏相当，使用抽象代码有益，还给出统计公式推导。


<details>
  <summary>Details</summary>
Motivation: 在深度强化学习不可行或赛前计算能力不足时，改进GRAVE算法以提高性能。

Method: 提出MCPS算法，在节点探索项中纳入特定统计信息，在多种游戏中进行测试，推导统计公式。

Result: MCPS在双人游戏中结果优于GRAVE，多人游戏相当，使用抽象代码对两者有益，给出无偏超参数的公式，MCPS对ref超参数不敏感。

Conclusion: MCPS算法是对GRAVE算法的有效改进，在多种游戏场景有良好表现。

Abstract: We propose Monte Carlo Permutation Search (MCPS), a general-purpose Monte
Carlo Tree Search (MCTS) algorithm that improves upon the GRAVE algorithm. MCPS
is relevant when deep reinforcement learning is not an option, or when the
computing power available before play is not substantial, such as in General
Game Playing, for example. The principle of MCPS is to include in the
exploration term of a node the statistics on all the playouts that contain all
the moves on the path from the root to the node. We extensively test MCPS on a
variety of games: board games, wargame, investment game, video game and
multi-player games. MCPS has better results than GRAVE in all the two-player
games. It has equivalent results for multi-player games because these games are
inherently balanced even when players have different strengths. We also show
that using abstract codes for moves instead of exact codes can be beneficial to
both MCPS and GRAVE, as they improve the permutation statistics and the AMAF
statistics. We also provide a mathematical derivation of the formulas used for
weighting the three sources of statistics. These formulas are an improvement on
the GRAVE formula since they no longer use the bias hyperparameter of GRAVE.
Moreover, MCPS is not sensitive to the ref hyperparameter.

</details>


### [73] [DPMM-CFL: Clustered Federated Learning via Dirichlet Process Mixture Model Nonparametric Clustering](https://arxiv.org/abs/2510.07132)
*Mariona Jaramillo-Civill,Peng Wu,Pau Closas*

Main category: cs.LG

TL;DR: 提出DPMM - CFL算法解决CFL方法需预先固定簇数量的问题，并在基准数据集上验证。


<details>
  <summary>Details</summary>
Motivation: 多数CFL方法需预先固定簇数量K，在潜在结构未知时不实用。

Method: 提出DPMM - CFL算法，对簇参数分布设置狄利克雷过程先验，通过非参数贝叶斯推理联合推断簇数量和客户端分配，同时优化每个簇的联邦目标。

Result: 算法实现每一轮联邦更新和簇推断的耦合。

Conclusion: 算法在狄利克雷和类分割非IID分区的基准数据集上得到验证。

Abstract: Clustered Federated Learning (CFL) improves performance under non-IID client
heterogeneity by clustering clients and training one model per cluster, thereby
balancing between a global model and fully personalized models. However, most
CFL methods require the number of clusters K to be fixed a priori, which is
impractical when the latent structure is unknown. We propose DPMM-CFL, a CFL
algorithm that places a Dirichlet Process (DP) prior over the distribution of
cluster parameters. This enables nonparametric Bayesian inference to jointly
infer both the number of clusters and client assignments, while optimizing
per-cluster federated objectives. This results in a method where, at each
round, federated updates and cluster inferences are coupled, as presented in
this paper. The algorithm is validated on benchmark datasets under Dirichlet
and class-split non-IID partitions.

</details>


### [74] [Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings](https://arxiv.org/abs/2510.06397)
*Ali Baheri*

Main category: cs.LG

TL;DR: 论文指出非欧几里得基础模型中双曲几何的边界驱动不对称性可被后门触发器利用，提出几何自适应触发器并评估，揭示模型漏洞及防御局限。


<details>
  <summary>Details</summary>
Motivation: 研究非欧几里得基础模型中双曲几何可能存在的安全漏洞。

Method: 分析双曲几何的边界驱动不对称性，形式化其影响，提出几何自适应触发器并跨任务和架构评估。

Result: 攻击成功率在边界处增加，传统检测器效果减弱，与理论趋势相符。

Conclusion: 非欧几里得模型存在特定几何漏洞，为防御设计和理解其局限性提供分析依据。

Abstract: Non-Euclidean foundation models increasingly place representations in curved
spaces such as hyperbolic geometry. We show that this geometry creates a
boundary-driven asymmetry that backdoor triggers can exploit. Near the
boundary, small input changes appear subtle to standard input-space detectors
but produce disproportionately large shifts in the model's representation
space. Our analysis formalizes this effect and also reveals a limitation for
defenses: methods that act by pulling points inward along the radius can
suppress such triggers, but only by sacrificing useful model sensitivity in
that same direction. Building on these insights, we propose a simple
geometry-adaptive trigger and evaluate it across tasks and architectures.
Empirically, attack success increases toward the boundary, whereas conventional
detectors weaken, mirroring the theoretical trends. Together, these results
surface a geometry-specific vulnerability in non-Euclidean models and offer
analysis-backed guidance for designing and understanding the limits of
defenses.

</details>


### [75] [Nearly Instance-Optimal Parameter Recovery from Many Trajectories via Hellinger Localization](https://arxiv.org/abs/2510.06434)
*Eliot Shekhtman,Yichen Zhou,Ingvar Ziemann,Nikolai Matni,Stephen Tu*

Main category: cs.LG

TL;DR: 本文通过Hellinger本地化框架拓宽多轨迹设置下实例最优率的范围，在四个案例研究中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有对多轨迹设置下顺序学习的理解不完整，且仅在最小二乘回归中有实例最优界，需为更通用模型和损失函数提供保障。

Method: 利用Hellinger本地化框架，先在路径测度层面控制平方Hellinger距离，再在参数空间进行本地化。

Result: 在多种条件下得到随完整数据预算缩放的实例最优界，在四个案例研究中，界接近渐近正态性的实例最优率，大幅优于标准归约。

Conclusion: Hellinger本地化框架能有效拓宽多轨迹设置下实例最优率的范围。

Abstract: Learning from temporally-correlated data is a core facet of modern machine
learning. Yet our understanding of sequential learning remains incomplete,
particularly in the multi-trajectory setting where data consists of many
independent realizations of a time-indexed stochastic process. This important
regime both reflects modern training pipelines such as for large foundation
models, and offers the potential for learning without the typical mixing
assumptions made in the single-trajectory case. However, instance-optimal
bounds are known only for least-squares regression with dependent covariates;
for more general models or loss functions, the only broadly applicable
guarantees result from a reduction to either i.i.d. learning, with effective
sample size scaling only in the number of trajectories, or an existing
single-trajectory result when each individual trajectory mixes, with effective
sample size scaling as the full data budget deflated by the mixing-time.
  In this work, we significantly broaden the scope of instance-optimal rates in
multi-trajectory settings via the Hellinger localization framework, a general
approach for maximum likelihood estimation. Our method proceeds by first
controlling the squared Hellinger distance at the path-measure level via a
reduction to i.i.d. learning, followed by localization as a quadratic form in
parameter space weighted by the trajectory Fisher information. This yields
instance-optimal bounds that scale with the full data budget under a broad set
of conditions. We instantiate our framework across four diverse case studies: a
simple mixture of Markov chains, dependent linear regression under non-Gaussian
noise, generalized linear models with non-monotonic activations, and
linear-attention sequence models. In all cases, our bounds nearly match the
instance-optimal rates from asymptotic normality, substantially improving over
standard reductions.

</details>


### [76] [Test-Time Efficient Pretrained Model Portfolios for Time Series Forecasting](https://arxiv.org/abs/2510.06419)
*Mert Kayaalp,Caner Turkmen,Oleksandr Shchur,Pedro Mercado,Abdul Fatir Ansari,Michael Bohlke-Schneider,Bernie Wang*

Main category: cs.LG

TL;DR: 探讨时间序列基础模型是否越大越好，提出构建小预训练预测模型组合，发现专家模型组合表现更佳及相关有效策略。


<details>
  <summary>Details</summary>
Motivation: 探究时间序列基础模型是否越大越好，寻找替代训练单个大型整体模型的方法。

Method: 构建小预训练预测模型组合，应用集成或模型选择方法，探索组合设计策略，对基础模型进行后训练。

Result: 在大规模基准测试中用更少参数取得有竞争力的性能，专家模型组合始终优于独立训练的通用模型组合。

Conclusion: 后训练基础模型是创建多样化专家模型的计算有效方法，集成和模型选择比测试时微调更具计算效率。

Abstract: Is bigger always better for time series foundation models? With the question
in mind, we explore an alternative to training a single, large monolithic
model: building a portfolio of smaller, pretrained forecasting models. By
applying ensembling or model selection over these portfolios, we achieve
competitive performance on large-scale benchmarks using much fewer parameters.
We explore strategies for designing such portfolios and find that collections
of specialist models consistently outperform portfolios of independently
trained generalists. Remarkably, we demonstrate that post-training a base model
is a compute-effective approach for creating sufficiently diverse specialists,
and provide evidences that ensembling and model selection are more
compute-efficient than test-time fine-tuning.

</details>


### [77] [A Median Perspective on Unlabeled Data for Out-of-Distribution Detection](https://arxiv.org/abs/2510.06505)
*Momin Abbas,Ali Falahati,Hossein Goli,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 提出Medix框架用于OOD检测，理论推导有低错误率，实证结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用无标签数据进行OOD检测时，因数据中InD和OOD样本混合，缺乏OOD样本集，难以训练最优OOD分类器。

Method: 引入Medix框架，用中位数操作从无标签数据中识别潜在离群点，结合有标签InD数据训练OOD分类器。

Result: 理论上推导得出Medix有低错误率，实证结果显示Medix在开放世界设置中全面优于现有方法。

Conclusion: Medix框架有效，理论见解得到验证。

Abstract: Out-of-distribution (OOD) detection plays a crucial role in ensuring the
robustness and reliability of machine learning systems deployed in real-world
applications. Recent approaches have explored the use of unlabeled data,
showing potential for enhancing OOD detection capabilities. However,
effectively utilizing unlabeled in-the-wild data remains challenging due to the
mixed nature of both in-distribution (InD) and OOD samples. The lack of a
distinct set of OOD samples complicates the task of training an optimal OOD
classifier. In this work, we introduce Medix, a novel framework designed to
identify potential outliers from unlabeled data using the median operation. We
use the median because it provides a stable estimate of the central tendency,
as an OOD detection mechanism, due to its robustness against noise and
outliers. Using these identified outliers, along with labeled InD data, we
train a robust OOD classifier. From a theoretical perspective, we derive error
bounds that demonstrate Medix achieves a low error rate. Empirical results
further substantiate our claims, as Medix outperforms existing methods across
the board in open-world settings, confirming the validity of our theoretical
insights.

</details>


### [78] [A Multi-Agent Framework for Stateful Inference-Time Search](https://arxiv.org/abs/2510.07147)
*Arshika Lalan,Rajat Ghosh,Aditya Kolsur,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 本文提出无训练框架有状态多智能体进化搜索，用于自动化单元测试生成，实验表明其比无状态单步基线有更高覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有无状态推理在多步任务上有困难，特定任务微调在深度推理和长时依赖任务上较脆弱。

Method: 提出有状态多智能体进化搜索框架，结合持久推理状态、对抗性变异和进化保存，通过专业智能体按顺序提出、变异和评分候选者，控制器维护跨代持久状态。

Result: 在HumanEval和TestGenEvalMini等基准测试上，使用三种不同大语言模型家族，该框架比无状态单步基线在覆盖率上有显著提升。

Conclusion: 将持久推理状态与进化搜索相结合能显著改善单元测试生成。

Abstract: Recent work explores agentic inference-time techniques to perform structured,
multi-step reasoning. However, stateless inference often struggles on
multi-step tasks due to the absence of persistent state. Moreover,
task-specific fine-tuning or instruction-tuning often achieve surface-level
code generation but remain brittle on tasks requiring deeper reasoning and
long-horizon dependencies. To address these limitations, we propose stateful
multi-agent evolutionary search, a training-free framework that departs from
prior stateless approaches by combining (i) persistent inference-time state,
(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate
its effectiveness in automated unit test generation through the generation of
edge cases. We generate robust edge cases using an evolutionary search process,
where specialized agents sequentially propose, mutate, and score candidates. A
controller maintains persistent state across generations, while evolutionary
preservation ensures diversity and exploration across all possible cases. This
yields a generalist agent capable of discovering robust, high-coverage edge
cases across unseen codebases. Experiments show our stateful multi-agent
inference framework achieves substantial gains in coverage over stateless
single-step baselines, evaluated on prevalent unit-testing benchmarks such as
HumanEval and TestGenEvalMini and using three diverse LLM families - Llama,
Gemma, and GPT. These results indicate that combining persistent inference-time
state with evolutionary search materially improves unit-test generation.

</details>


### [79] [Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture](https://arxiv.org/abs/2510.06527)
*John Dunbar,Scott Aaronson*

Main category: cs.LG

TL;DR: 证明大宽度随机初始化神经网络在激活函数满足特定条件时输出近似独立，并提出此类网络可用于验证计算无巧合猜想。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络输出独立性以及为计算无巧合猜想寻找合适候选网络。

Method: 理论分析，确定激活函数需满足在高斯测度下均值为零这一条件。

Result: 发现ReLU和GeLU加偏移、tanh等激活函数能使网络输出近似独立，而ReLU和GeLU本身不行。

Conclusion: 零均值激活函数的神经网络是计算无巧合猜想的有前景候选。

Abstract: We establish that randomly initialized neural networks, with large width and
a natural choice of hyperparameters, have nearly independent outputs exactly
when their activation function is nonlinear with zero mean under the Gaussian
measure: $\mathbb{E}_{z \sim \mathcal{N}(0,1)}[\sigma(z)]=0$. For example, this
includes ReLU and GeLU with an additive shift, as well as tanh, but not ReLU or
GeLU by themselves. Because of their nearly independent outputs, we propose
neural networks with zero-mean activation functions as a promising candidate
for the Alignment Research Center's computational no-coincidence conjecture --
a conjecture that aims to measure the limits of AI interpretability.

</details>


### [80] [Scalable Policy-Based RL Algorithms for POMDPs](https://arxiv.org/abs/2510.06540)
*Ameya Anjarlekar,Rasoul Etesami,R Srikant*

Main category: cs.LG

TL;DR: 本文通过将POMDP模型近似为有限状态MDP解决PORL问题，提出策略学习方法，分析近似误差并给出有限时间界。


<details>
  <summary>Details</summary>
Motivation: POMDP中信念状态的连续性给学习最优策略带来计算挑战。

Method: 将POMDP近似为Superstate MDP，推导理论保证，提出基于策略的线性函数近似学习方法，用TD学习和策略优化求解。

Result: 表明可将POMDP近似为MDP求解，近似误差随历史长度指数下降，给出有限时间界量化TD学习误差。

Conclusion: 本文方法能有效近似解决POMDP问题，所提有限时间界可量化TD学习在非马尔可夫环境中的误差。

Abstract: The continuous nature of belief states in POMDPs presents significant
computational challenges in learning the optimal policy. In this paper, we
consider an approach that solves a Partially Observable Reinforcement Learning
(PORL) problem by approximating the corresponding POMDP model into a
finite-state Markov Decision Process (MDP) (called Superstate MDP). We first
derive theoretical guarantees that improve upon prior work that relate the
optimal value function of the transformed Superstate MDP to the optimal value
function of the original POMDP. Next, we propose a policy-based learning
approach with linear function approximation to learn the optimal policy for the
Superstate MDP. Consequently, our approach shows that a POMDP can be
approximately solved using TD-learning followed by Policy Optimization by
treating it as an MDP, where the MDP state corresponds to a finite history. We
show that the approximation error decreases exponentially with the length of
this history. To the best of our knowledge, our finite-time bounds are the
first to explicitly quantify the error introduced when applying standard TD
learning to a setting where the true dynamics are not Markovian.

</details>


### [81] [How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards Realistic Evaluation](https://arxiv.org/abs/2510.06448)
*Prabhant Singh,Sibylle Hess,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: 本文指出评估可迁移性估计指标的基准存在缺陷，提出构建更稳健和现实的基准的建议。


<details>
  <summary>Details</summary>
Motivation: 现有评估可迁移性估计指标的基准未受充分审视，需发现其问题。

Method: 通过实证分析广泛使用的基准设置的缺点。

Result: 现有基准的不现实模型空间和静态性能层次人为提升了现有指标的表现，简单启发式方法可胜过复杂方法。

Conclusion: 当前评估协议与现实模型选择存在脱节，需构建更稳健和现实的基准指导未来研究。

Abstract: Transferability estimation metrics are used to find a high-performing
pre-trained model for a given target task without fine-tuning models and
without access to the source dataset. Despite the growing interest in
developing such metrics, the benchmarks used to measure their progress have
gone largely unexamined. In this work, we empirically show the shortcomings of
widely used benchmark setups to evaluate transferability estimation metrics. We
argue that the benchmarks on which these metrics are evaluated are
fundamentally flawed. We empirically demonstrate that their unrealistic model
spaces and static performance hierarchies artificially inflate the perceived
performance of existing metrics, to the point where simple, dataset-agnostic
heuristics can outperform sophisticated methods. Our analysis reveals a
critical disconnect between current evaluation protocols and the complexities
of real-world model selection. To address this, we provide concrete
recommendations for constructing more robust and realistic benchmarks to guide
future research in a more meaningful direction.

</details>


### [82] [The Effect of Attention Head Count on Transformer Approximation](https://arxiv.org/abs/2510.06662)
*Penghao Yu,Haotian Jiang,Zeyu Bao,Ruoxi Yu,Qianxiao Li*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Transformer has become the dominant architecture for sequence modeling, yet a
detailed understanding of how its structural parameters influence expressive
power remains limited. In this work, we study the approximation properties of
transformers, with particular emphasis on the role of the number of attention
heads. Our analysis begins with the introduction of a generalized $D$-retrieval
task, which we prove to be dense in the space of continuous functions, thereby
providing the basis for our theoretical framework. We then establish both upper
and lower bounds on the parameter complexity required for
$\epsilon$-approximation. Specifically, we show that transformers with
sufficiently many heads admit efficient approximation, whereas with too few
heads, the number of parameters must scale at least as $O(1/\epsilon^{cT})$,
for some constant $c$ and sequence length $T$. To the best of our knowledge,
this constitutes the first rigorous lower bound of this type in a nonlinear and
practically relevant setting. We further examine the single-head case and
demonstrate that an embedding dimension of order $O(T)$ allows complete
memorization of the input, where approximation is entirely achieved by the
feed-forward block. Finally, we validate our theoretical findings with
experiments on both synthetic data and real-world tasks, illustrating the
practical relevance of our results.

</details>


### [83] [Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin](https://arxiv.org/abs/2510.06477)
*Enrique Queipo-de-Llano,Álvaro Arroyo,Federico Barbero,Xiaowen Dong,Michael Bronstein,Yann LeCun,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: 本文揭示注意力汇点和压缩谷的联系，提出信息流动的Mix - Compress - Refine理论解释大语言模型计算组织方式。


<details>
  <summary>Details</summary>
Motivation: 此前注意力汇点和压缩谷被孤立研究，需探索两者联系及解释大语言模型如何组织深度计算。

Method: 理论证明大规模激活会产生表征压缩并确定熵减少界限，在多个模型上实验验证，进行有针对性的消融研究。

Result: 当序列起始标记在中间层出现极端激活范数时，压缩谷和注意力汇点同时出现，消融研究验证理论预测。

Conclusion: 提出Mix - Compress - Refine理论，解释了不同任务在不同层表现不同的原因。

Abstract: Attention sinks and compression valleys have attracted significant attention
as two puzzling phenomena in large language models, but have been studied in
isolation. In this work, we present a surprising connection between attention
sinks and compression valleys, tracing both to the formation of massive
activations in the residual stream. We prove theoretically that massive
activations necessarily produce representational compression and establish
bounds on the resulting entropy reduction. Through experiments across several
models (410M-120B parameters), we confirm that when the beginning-of-sequence
token develops extreme activation norms in the middle layers, both compression
valleys and attention sinks emerge simultaneously. Targeted ablation studies
validate our theoretical predictions. This unified view motivates us to propose
the Mix-Compress-Refine theory of information flow, as an attempt to explain
how LLMs organize their computation in depth by controlling attention and
representational compression via massive activations. Specifically, we posit
that Transformer-based LLMs process tokens in three distinct phases: (1) broad
mixing in the early layers, (2) compressed computation with limited mixing in
the middle layers, and (3) selective refinement in the late layers. Our
framework helps explain why embedding tasks perform best at intermediate
layers, whereas generation tasks benefit from full-depth processing, clarifying
differences in task-dependent representations.

</details>


### [84] [Non-Asymptotic Analysis of Efficiency in Conformalized Regression](https://arxiv.org/abs/2510.07093)
*Yunzhen Yao,Lie He,Michael Gastpar*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Conformal prediction provides prediction sets with coverage guarantees. The
informativeness of conformal prediction depends on its efficiency, typically
quantified by the expected size of the prediction set. Prior work on the
efficiency of conformalized regression commonly treats the miscoverage level
$\alpha$ as a fixed constant. In this work, we establish non-asymptotic bounds
on the deviation of the prediction set length from the oracle interval length
for conformalized quantile and median regression trained via SGD, under mild
assumptions on the data distribution. Our bounds of order
$\mathcal{O}(1/\sqrt{n} + 1/(\alpha^2 n) + 1/\sqrt{m} + \exp(-\alpha^2 m))$
capture the joint dependence of efficiency on the proper training set size $n$,
the calibration set size $m$, and the miscoverage level $\alpha$. The results
identify phase transitions in convergence rates across different regimes of
$\alpha$, offering guidance for allocating data to control excess prediction
set length. Empirical results are consistent with our theoretical findings.

</details>


### [85] [Valid Stopping for LLM Generation via Empirical Dynamic Formal Lift](https://arxiv.org/abs/2510.06478)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.LG

TL;DR: 提出Sequential - EDFL用于语言模型生成停止，在六个基准测试中减少生成量，有一定计算开销，结合正确性门可提高任务正确性，可作一级过滤器。


<details>
  <summary>Details</summary>
Motivation: 将随时有效的顺序测试应用于语言模型生成停止，控制信息充分性。

Method: 使用自归一化经验 - 伯恩斯坦e - 过程跟踪信息提升，通过在线均值估计处理未知中心，用混合e - 过程组合多参数，支持分布漂移下自适应重置。

Result: 在六个基准测试中，比顺序基线减少22 - 28%的生成量，有12%计算开销；结合正确性门可提高任务正确性；停止序列仍有10.9%不正确。

Conclusion: EDFL可作为一级过滤器减少验证负担，但不能作为安全关键领域的独立解决方案。

Abstract: We introduce Sequential-EDFL (Empirical Dynamic Formal Lift), applying
anytime-valid sequential testing to language model generation stopping. Our
approach tracks information lift -- the log-likelihood ratio between full
models and deliberately weakened "skeleton" baselines -- using self-normalized
empirical-Bernstein e-processes that provide formal delta-level error control
regardless of stopping time. We handle unknown centering through online mean
estimation, combine multiple parameters via mixture e-processes, and support
adaptive resets under distributional drift. On six benchmarks, Sequential-EDFL
reduces generation by 22-28% vs. sequential baselines while maintaining
delta-level control with 12% computational overhead. We introduce automated
skeletons (distilled submodels, randomized logits) and show robustness across
skeleton families. Composing EDFL with a lightweight correctness gate (sentence
boundaries + verifier) improves end-task correctness while preserving
anytime-valid guarantees by only delaying stopping. Our certificates control
information sufficiency, not factual correctness -- 10.9% of stopped sequences
remain incorrect even with the gate (13.2-22.7% without it). EDFL serves as a
first-stage filter reducing verification burden by 83%, not as a standalone
solution for safety-critical domains.

</details>


### [86] [GUIDE: Guided Initialization and Distillation of Embeddings](https://arxiv.org/abs/2510.06502)
*Khoa Trinh,Gaurav Menghani,Erik Vee*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Algorithmic efficiency techniques such as distillation
(\cite{hinton2015distillation}) are useful in improving model quality without
increasing serving costs, provided a larger teacher model is available for a
smaller student model to learn from during training. Standard distillation
methods are limited to only forcing the student to match the teacher's outputs.
Given the costs associated with training a large model, we believe we should be
extracting more useful information from a teacher model than by just making the
student match the teacher's outputs.
  In this paper, we introduce \guide (Guided Initialization and Distillation of
Embeddings). \guide can be considered a distillation technique that forces the
student to match the teacher in the parameter space. Using \guide we show
25-26\% reduction in the teacher-student quality gap when using large student
models (400M - 1B parameters) trained on $\approx$ 20B tokens. We also present
a thorough analysis demonstrating that \guide can be combined with knowledge
distillation with near additive improvements. Furthermore, we show that
applying \guide alone leads to substantially better model quality than applying
knowledge distillation by itself.
  Most importantly, \guide introduces no training or inference overhead and
hence any model quality gains from our method are virtually free.

</details>


### [87] [ATLO-ML: Adaptive Time-Length Optimizer for Machine Learning -- Insights from Air Quality Forecasting](https://arxiv.org/abs/2510.06503)
*I-Hsi Kao,Kanji Uchino*

Main category: cs.LG

TL;DR: 本文介绍自适应时间长度优化系统ATLO - ML，以用户定义输出时长自动确定最优输入时长和采样率，经空气质量数据集验证可提升机器学习模型预测准确性，有泛化潜力。


<details>
  <summary>Details</summary>
Motivation: 机器学习中准确的时间序列预测受输入时间长度和采样率选择的影响，需优化这些参数以提升预测性能。

Method: 引入自适应时间长度优化系统ATLO - ML，基于用户定义输出时间长度自动确定最优输入时间长度和采样率，对时间序列数据进行灵活预处理。

Result: 使用空气质量数据集验证，优化后的时间长度和采样率相比固定时间长度显著提高了机器学习模型的准确性。

Conclusion: ATLO - ML有潜力在各种时间敏感应用中泛化，为机器学习工作流程中优化时间输入参数提供了可靠解决方案。

Abstract: Accurate time-series predictions in machine learning are heavily influenced
by the selection of appropriate input time length and sampling rate. This paper
introduces ATLO-ML, an adaptive time-length optimization system that
automatically determines the optimal input time length and sampling rate based
on user-defined output time length. The system provides a flexible approach to
time-series data pre-processing, dynamically adjusting these parameters to
enhance predictive performance. ATLO-ML is validated using air quality
datasets, including both GAMS-dataset and proprietary data collected from a
data center, both in time series format. Results demonstrate that utilizing the
optimized time length and sampling rate significantly improves the accuracy of
machine learning models compared to fixed time lengths. ATLO-ML shows potential
for generalization across various time-sensitive applications, offering a
robust solution for optimizing temporal input parameters in machine learning
workflows.

</details>


### [88] [Text-to-Image Models Leave Identifiable Signatures: Implications for Leaderboard Security](https://arxiv.org/abs/2510.06525)
*Ali Naseh,Anshuman Suri,Yuefeng Peng,Harsh Chaudhari,Alina Oprea,Amir Houmansadr*

Main category: cs.LG

TL;DR: 研究指出文本到图像排行榜模型去匿名化易实现，排名操纵问题更严重，需更强防御。


<details>
  <summary>Details</summary>
Motivation: 生成式AI排行榜易受操纵，此前大语言模型存在排名操纵威胁，研究文本到图像排行榜该问题的严重性。

Method: 使用超15万张来自280个提示和19种不同模型的生成图像，在CLIP嵌入空间进行简单实时分类。引入提示级可分离性指标。

Result: 能高精度识别生成模型，找到可实现近乎完美去匿名化的提示。

Conclusion: 文本到图像排行榜的排名操纵比以往认知更容易，需更强防御。

Abstract: Generative AI leaderboards are central to evaluating model capabilities, but
remain vulnerable to manipulation. Among key adversarial objectives is rank
manipulation, where an attacker must first deanonymize the models behind
displayed outputs -- a threat previously demonstrated and explored for large
language models (LLMs). We show that this problem can be even more severe for
text-to-image leaderboards, where deanonymization is markedly easier. Using
over 150,000 generated images from 280 prompts and 19 diverse models spanning
multiple organizations, architectures, and sizes, we demonstrate that simple
real-time classification in CLIP embedding space identifies the generating
model with high accuracy, even without prompt control or historical data. We
further introduce a prompt-level separability metric and identify prompts that
enable near-perfect deanonymization. Our results indicate that rank
manipulation in text-to-image leaderboards is easier than previously
recognized, underscoring the need for stronger defenses.

</details>


### [89] [Incoherence in goal-conditioned autoregressive models](https://arxiv.org/abs/2510.06545)
*Jacek Karwowski,Raymond Douglas*

Main category: cs.LG

TL;DR: 研究强化学习策略中不连贯性问题，分析重训练过程，建立对应关系并探讨与有效视野的联系。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习策略中由朴素目标条件化自回归模型产生的不连贯性结构问题。

Method: 重新构建控制即推理和软Q学习的标准概念，通过软条件生成模型进行分析。

Result: 证明重训练可减少不连贯性并提高回报，建立重训练过程与其他理解方式的三方对应关系。

Conclusion: 通过研究重训练过程，建立对应关系，探讨了不连贯性与有效视野的联系。

Abstract: We investigate mathematically the notion of incoherence: a structural issue
with reinforcement learning policies derived by naive goal-conditioning of
autoregressive models. We focus on the process of re-training models on their
own actions, that is, fine-tuning offline-learned policies with online RL. We
prove that it decreases incoherence and leads to an improvement in return, and
we aim to characterize the resulting trajectory of policies. By re-framing
standard notions of control-as-inference and soft Q learning, we establish a
three-way correspondence with two other ways of understanding the iterative
re-training process: as folding the posterior into the reward and, in the
deterministic case, as decreasing the temperature parameter; the correspondence
has computational content via the training-inference trade-off. Through
soft-conditioning generative models, we discuss the link between incoherence
and the effective horizon.

</details>


### [90] [The Markovian Thinker](https://arxiv.org/abs/2510.06557)
*Milad Aghajohari,Kamran Chitsaz,Amirhossein Kazemnejad,Sarath Chandar,Alessandro Sordoni,Aaron Courville,Siva Reddy*

Main category: cs.LG

TL;DR: 本文提出Markovian Thinking范式及Delethink环境，解决强化学习训练推理大语言模型时的计算和内存问题，实现高效长推理。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习的“思考环境”使状态无界，注意力策略计算复杂度随思考长度呈二次增长，需改进。

Method: 提出Markovian Thinking范式，用Delethink环境将推理结构化，通过强化学习让策略学习续写文本状态。

Result: R1 - Distill 1.5B模型在Delethink环境下推理表现好，计算成本低，现成推理模型能提供有效样本。

Conclusion: 重新设计思考环境是强大手段，可实现无二次开销的长推理，为高效可扩展推理大语言模型开辟道路。

Abstract: Reinforcement learning (RL) has recently become a strong recipe for training
reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard
RL "thinking environment", where the state is the prompt plus all prior
reasoning tokens, makes the state unbounded and forces attention-based policies
to pay quadratic compute as thoughts lengthen. We revisit the environment
itself. We propose Markovian Thinking, a paradigm in which the policy advances
reasoning while conditioning on a constant-size state, decoupling thinking
length from context size. As an immediate consequence this yields linear
compute with constant memory. We instantiate this idea with Delethink, an RL
environment that structures reasoning into fixed-size chunks. Within each
chunk, the model thinks as usual; at the boundary, the environment resets the
context and reinitializes the prompt with a short carryover. Through RL, the
policy learns to write a textual state near the end of each chunk sufficient
for seamless continuation of reasoning after reset. Trained in this
environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up
to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget.
With test-time scaling, Delethink continues to improve where LongCoT plateaus.
The effect of linear compute is substantial: we empirically estimate at 96K
average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink.
Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B)
often sample Markovian traces zero-shot across diverse benchmarks, providing
positive samples that make RL effective at scale. Our results show that
redesigning the thinking environment is a powerful lever: it enables very long
reasoning without quadratic overhead and opens a path toward efficient,
scalable reasoning LLMs.

</details>


### [91] [The Framework That Survives Bad Models: Human-AI Collaboration For Clinical Trials](https://arxiv.org/abs/2510.06567)
*Yao Chen,David Ohlssen,Aimee Readie,Gregory Ligozio,Ruvie Martin,Thibaud Coroller*

Main category: cs.LG

TL;DR: 对比两种AI框架与纯人工评估用于医学影像疾病评估，发现AI - SR最适合临床试验。


<details>
  <summary>Details</summary>
Motivation: AI用于临床试验有潜力但无保障会有风险，需对比评估不同方法。

Method: 对比两种AI框架与纯人工评估，注入坏模型进行压力测试，用两项随机对照试验评估。

Result: AI - SR作为支持性阅读器的方法在各模型类型中满足所有标准，能提供可靠疾病估计，保留试验治疗效果估计和结论。

Conclusion: AI - SR是临床试验最适合的方法，在不同人群应用也有优势。

Abstract: Artificial intelligence (AI) holds great promise for supporting clinical
trials, from patient recruitment and endpoint assessment to treatment response
prediction. However, deploying AI without safeguards poses significant risks,
particularly when evaluating patient endpoints that directly impact trial
conclusions. We compared two AI frameworks against human-only assessment for
medical image-based disease evaluation, measuring cost, accuracy, robustness,
and generalization ability. To stress-test these frameworks, we injected bad
models, ranging from random guesses to naive predictions, to ensure that
observed treatment effects remain valid even under severe model degradation. We
evaluated the frameworks using two randomized controlled trials with endpoints
derived from spinal X-ray images. Our findings indicate that using AI as a
supporting reader (AI-SR) is the most suitable approach for clinical trials, as
it meets all criteria across various model types, even with bad models. This
method consistently provides reliable disease estimation, preserves clinical
trial treatment effect estimates and conclusions, and retains these advantages
when applied to different populations.

</details>


### [92] [DPA-Net: A Dual-Path Attention Neural Network for Inferring Glycemic Control Metrics from Self-Monitored Blood Glucose Data](https://arxiv.org/abs/2510.06623)
*Canyu Lei,Benjamin Lobo,Jianxin Xie*

Main category: cs.LG

TL;DR: 提出DPA - Net从SMBG数据估算AGP指标，实验显示其准确且能减少偏差，是首个相关监督学习框架。


<details>
  <summary>Details</summary>
Motivation: CGM成本高、可及性低，SMBG数据稀疏难转化为临床有意义的血糖指标，需从SMBG数据估算AGP指标的方法。

Method: 提出DPA - Net，包含空间通道注意力路径和多尺度ResNet路径，引入对齐机制；开发主动点选择器。

Result: 在真实数据集上实验表明DPA - Net准确性高、误差低且能减少系统偏差。

Conclusion: DPA - Net是首个从SMBG数据估算AGP指标的监督学习框架，为CGM不可及的场景提供实用决策支持工具。

Abstract: Continuous glucose monitoring (CGM) provides dense and dynamic glucose
profiles that enable reliable estimation of Ambulatory Glucose Profile (AGP)
metrics, such as Time in Range (TIR), Time Below Range (TBR), and Time Above
Range (TAR). However, the high cost and limited accessibility of CGM restrict
its widespread adoption, particularly in low- and middle-income regions. In
contrast, self-monitoring of blood glucose (SMBG) is inexpensive and widely
available but yields sparse and irregular data that are challenging to
translate into clinically meaningful glycemic metrics.
  In this work, we propose a Dual-Path Attention Neural Network (DPA-Net) to
estimate AGP metrics directly from SMBG data. DPA-Net integrates two
complementary paths: (1) a spatial-channel attention path that reconstructs a
CGM-like trajectory from sparse SMBG observations, and (2) a multi-scale ResNet
path that directly predicts AGP metrics. An alignment mechanism between the two
paths is introduced to reduce bias and mitigate overfitting. In addition, we
develop an active point selector to identify realistic and informative SMBG
sampling points that reflect patient behavioral patterns.
  Experimental results on a large, real-world dataset demonstrate that DPA-Net
achieves robust accuracy with low errors while reducing systematic bias. To the
best of our knowledge, this is the first supervised machine learning framework
for estimating AGP metrics from SMBG data, offering a practical and clinically
relevant decision-support tool in settings where CGM is not accessible.

</details>


### [93] [POME: Post Optimization Model Edit via Muon-style Projection](https://arxiv.org/abs/2510.06627)
*Yong Liu,Di Fu,Yang Luo,Zirui Zhu,Minhao Cheng,Cho-Jui Hsieh,Yang You*

Main category: cs.LG

TL;DR: 介绍新算法POME，仅用预训练和微调检查点提升微调大语言模型性能，无需额外数据和优化，有性能提升且适用广泛。


<details>
  <summary>Details</summary>
Motivation: 提升微调大语言模型的性能，找到一种无需额外数据和进一步优化的方法。

Method: 对微调与预训练权重差值应用muon风格投影，用截断奇异值分解均衡主导更新方向影响并修剪小奇异值。

Result: 在GSM8K上平均性能提升2.5%，代码生成上提升1.0%，从7B到72B模型都适用。

Conclusion: POME是实用、零成本的微调管道增强方法。

Abstract: We introduce Post-Optimization Model Edit (POME), a new algorithm that
enhances the performance of fine-tuned large language models using only their
pretrained and fine-tuned checkpoints, without requiring extra data or further
optimization. The core idea is to apply a muon-style projection to $\Delta W$,
the difference between the fine-tuned and pretrained weights. This projection
uses truncated singular value decomposition (SVD) to equalize the influence of
dominant update directions and prune small singular values, which often
represent noise. As a simple post-processing step, POME is completely decoupled
from the training pipeline. It requires zero modifications and imposes no
overhead, making it universally compatible with any optimizer or distributed
framework. POME delivers consistent gains, boosting average performance by
+2.5\% on GSM8K and +1.0\% on code generation. Its broad applicability -- from
7B foundation models to 72B RLHF-instructed models -- establishes it as a
practical, zero-cost enhancement for any fine-tuning pipeline. Code is
available at https://github.com/NUS-HPC-AI-Lab/POME.

</details>


### [94] [AI-Driven Forecasting and Monitoring of Urban Water System](https://arxiv.org/abs/2510.06631)
*Qiming Guo,Bishal Khatri,Hua Zhang,Wenlu Wang*

Main category: cs.LG

TL;DR: 提出AI与远程传感器集成框架检测地下水管泄漏，在校园污水网络数据集验证有效，有望推广。


<details>
  <summary>Details</summary>
Motivation: 地下水管和污水管存在泄漏等异常问题，传统人工检测效率低，密集传感器部署成本高。

Method: 部署稀疏远程传感器收集实时流量和深度数据，结合利用管道属性的HydroNet模型。

Result: 系统能收集有效时空水力数据，HydroNet表现优于先进基线。

Conclusion: 该方法可有效扩展到广泛的地下水管网络。

Abstract: Underground water and wastewater pipelines are vital for city operations but
plagued by anomalies like leaks and infiltrations, causing substantial water
loss, environmental damage, and high repair costs. Conventional manual
inspections lack efficiency, while dense sensor deployments are prohibitively
expensive. In recent years, artificial intelligence has advanced rapidly and is
increasingly applied to urban infrastructure. In this research, we propose an
integrated AI and remote-sensor framework to address the challenge of leak
detection in underground water pipelines, through deploying a sparse set of
remote sensors to capture real-time flow and depth data, paired with HydroNet -
a dedicated model utilizing pipeline attributes (e.g., material, diameter,
slope) in a directed graph for higher-precision modeling. Evaluations on a
real-world campus wastewater network dataset demonstrate that our system
collects effective spatio-temporal hydraulic data, enabling HydroNet to
outperform advanced baselines. This integration of edge-aware message passing
with hydraulic simulations enables accurate network-wide predictions from
limited sensor deployments. We envision that this approach can be effectively
extended to a wide range of underground water pipeline networks.

</details>


### [95] [Chem-NMF: Multi-layer $α$-divergence Non-Negative Matrix Factorization for Cardiorespiratory Disease Clustering, with Improved Convergence Inspired by Chemical Catalysts and Rigorous Asymptotic Analysis](https://arxiv.org/abs/2510.06632)
*Yasaman Torabi,Shahram Shirani,James P. Reilly*

Main category: cs.LG

TL;DR: 本文引入受化学反应能量势垒的玻尔兹曼概率启发的Chem - NMF方法，解决NMF多层架构收敛问题，实验显示算法提升了聚类准确率。


<details>
  <summary>Details</summary>
Motivation: NMF中引入α - 散度虽增强优化灵活性，但扩展到多层架构时收敛性难以保证。

Method: 引入受化学反应能量势垒的玻尔兹曼概率启发的方法进行收敛性理论分析，提出带边界因子的Chem - NMF方法稳定收敛。

Result: 实验表明，该算法在生物医学信号上聚类准确率提高5.6% ± 2.7%，在人脸图像上提高11.1% ± 7.2%。

Conclusion: 首次从物理化学角度严格分析NMF算法收敛行为，所提算法能有效提升聚类准确率。

Abstract: Non-Negative Matrix Factorization (NMF) is an unsupervised learning method
offering low-rank representations across various domains such as audio
processing, biomedical signal analysis, and image recognition. The
incorporation of $\alpha$-divergence in NMF formulations enhances flexibility
in optimization, yet extending these methods to multi-layer architectures
presents challenges in ensuring convergence. To address this, we introduce a
novel approach inspired by the Boltzmann probability of the energy barriers in
chemical reactions to theoretically perform convergence analysis. We introduce
a novel method, called Chem-NMF, with a bounding factor which stabilizes
convergence. To our knowledge, this is the first study to apply a physical
chemistry perspective to rigorously analyze the convergence behaviour of the
NMF algorithm. We start from mathematically proven asymptotic convergence
results and then show how they apply to real data. Experimental results
demonstrate that the proposed algorithm improves clustering accuracy by 5.6%
$\pm$ 2.7% on biomedical signals and 11.1% $\pm$ 7.2% on face images (mean
$\pm$ std).

</details>


### [96] [Three Forms of Stochastic Injection for Improved Distribution-to-Distribution Generative Modeling](https://arxiv.org/abs/2510.06634)
*Shiye Su,Yuhui Zhang,Linqi Zhou,Rajesh Ranganath,Serena Yeung-Levy*

Main category: cs.LG

TL;DR: 提出向训练过程注入随机性的方法解决标准流匹配在分布到分布转换问题上的稀疏监督问题，在五项成像任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 标准流匹配在源分布为需从有限样本学习的数据分布时，因稀疏监督而失效，需解决分布到分布转换建模问题。

Method: 向训练过程注入随机性，通过扰动源样本和流插值。

Result: 在五项成像任务中显著提高生成质量，平均比现有基线高出9个FID点，降低输入和生成样本间的传输成本。

Conclusion: 该方法使流匹配成为模拟科学中多样分布转换更实用的工具。

Abstract: Modeling transformations between arbitrary data distributions is a
fundamental scientific challenge, arising in applications like drug discovery
and evolutionary simulation. While flow matching offers a natural framework for
this task, its use has thus far primarily focused on the noise-to-data setting,
while its application in the general distribution-to-distribution setting is
underexplored. We find that in the latter case, where the source is also a data
distribution to be learned from limited samples, standard flow matching fails
due to sparse supervision. To address this, we propose a simple and
computationally efficient method that injects stochasticity into the training
process by perturbing source samples and flow interpolants. On five diverse
imaging tasks spanning biology, radiology, and astronomy, our method
significantly improves generation quality, outperforming existing baselines by
an average of 9 FID points. Our approach also reduces the transport cost
between input and generated samples to better highlight the true effect of the
transformation, making flow matching a more practical tool for simulating the
diverse distribution transformations that arise in science.

</details>


### [97] [StruSR: Structure-Aware Symbolic Regression with Physics-Informed Taylor Guidance](https://arxiv.org/abs/2510.06635)
*Yunpeng Gong,Sihan Lan,Can Yang,Kunpeng Xu,Min Jiang*

Main category: cs.LG

TL;DR: 提出StruSR结构感知符号回归框架，利用PINN提取物理先验，结合多种机制，实验表明其在基准PDE系统上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归方法难以从时间序列观测中提取结构化物理先验，难以捕捉反映系统全局行为的符号表达式。

Method: 提出StruSR框架，利用PINN从时间序列数据提取局部结构化物理先验，进行局部泰勒展开获取导数结构信息，引入基于掩码的归因机制评估表达式组件重要性，用混合适应度函数优化。

Result: 在基准PDE系统实验中，StruSR提高了收敛速度、结构保真度和表达式可解释性。

Conclusion: StruSR为基于物理的符号发现提供了一种有原则的范式。

Abstract: Symbolic regression aims to find interpretable analytical expressions by
searching over mathematical formula spaces to capture underlying system
behavior, particularly in scientific modeling governed by physical laws.
However, traditional methods lack mechanisms for extracting structured physical
priors from time series observations, making it difficult to capture symbolic
expressions that reflect the system's global behavior. In this work, we propose
a structure-aware symbolic regression framework, called StruSR, that leverages
trained Physics-Informed Neural Networks (PINNs) to extract locally structured
physical priors from time series data. By performing local Taylor expansions on
the outputs of the trained PINN, we obtain derivative-based structural
information to guide symbolic expression evolution. To assess the importance of
expression components, we introduce a masking-based attribution mechanism that
quantifies each subtree's contribution to structural alignment and physical
residual reduction. These sensitivity scores steer mutation and crossover
operations within genetic programming, preserving substructures with high
physical or structural significance while selectively modifying less
informative components. A hybrid fitness function jointly minimizes physics
residuals and Taylor coefficient mismatch, ensuring consistency with both the
governing equations and the local analytical behavior encoded by the PINN.
Experiments on benchmark PDE systems demonstrate that StruSR improves
convergence speed, structural fidelity, and expression interpretability
compared to conventional baselines, offering a principled paradigm for
physics-grounded symbolic discovery.

</details>


### [98] [Control-Augmented Autoregressive Diffusion for Data Assimilation](https://arxiv.org/abs/2510.06637)
*Prakhar Srivastava,Farrin Marouf Sofian,Francesco Immorlano,Kushagra Pandey,Stephan Mandt*

Main category: cs.LG

TL;DR: 本文提出摊销框架增强预训练ARDMs，在混沌时空偏微分方程数据同化中评估，表现优于四个基线方法，将公开代码。


<details>
  <summary>Details</summary>
Motivation: 自动回归扩散模型（ARDMs）中的引导研究不足，且现有数据同化方法计算成本高、在稀疏观测下易出现预测漂移。

Method: 引入摊销框架，用轻量级控制器网络增强预训练ARDMs，离线训练，在数据同化中减少推理为单次前向滚动并进行即时修正。

Result: 在两个典型偏微分方程和六种观测制度下，该方法在稳定性、准确性和物理保真度上始终优于四个最先进的基线方法。

Conclusion: 所提方法有效，有更好表现，将公开代码和检查点。

Abstract: Despite recent advances in test-time scaling and finetuning of diffusion
models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains
underexplored. We introduce an amortized framework that augments pretrained
ARDMs with a lightweight controller network, trained offline by previewing
future ARDM rollouts and learning stepwise controls that anticipate upcoming
observations under a terminal cost objective. We evaluate this framework in the
context of data assimilation (DA) for chaotic spatiotemporal partial
differential equations (PDEs), a setting where existing methods are often
computationally prohibitive and prone to forecast drift under sparse
observations. Our approach reduces DA inference to a single forward rollout
with on-the-fly corrections, avoiding expensive adjoint computations and/or
optimizations during inference. We demonstrate that our method consistently
outperforms four state-of-the-art baselines in stability, accuracy, and
physical fidelity across two canonical PDEs and six observation regimes. We
will release code and checkpoints publicly.

</details>


### [99] [The False Promise of Zero-Shot Super-Resolution in Machine-Learned Operators](https://arxiv.org/abs/2510.06646)
*Mansi Sakarvadia,Kareem Hegazy,Amin Totounferoush,Kyle Chard,Yaoqing Yang,Ian Foster,Michael W. Mahoney*

Main category: cs.LG

TL;DR: 评估机器学习算子（MLOs）零样本超分辨率能力，发现其存在问题并提出多分辨率训练协议解决。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习中用离散表示连续现象是挑战，评估MLOs是否能实现零样本超分辨率。

Method: 全面评估MLOs零样本子分辨率和超分辨率推理，将多分辨率推理解耦为频率信息外推和分辨率插值；提出多分辨率训练协议。

Result: MLOs无法零样本完成上述两项任务，在不同分辨率下推理不准确，易出现混叠。

Conclusion: MLOs在不同分辨率下易失败，提出的多分辨率训练协议能克服混叠，实现鲁棒的多分辨率泛化。

Abstract: A core challenge in scientific machine learning, and scientific computing
more generally, is modeling continuous phenomena which (in practice) are
represented discretely. Machine-learned operators (MLOs) have been introduced
as a means to achieve this modeling goal, as this class of architecture can
perform inference at arbitrary resolution. In this work, we evaluate whether
this architectural innovation is sufficient to perform "zero-shot
super-resolution," namely to enable a model to serve inference on
higher-resolution data than that on which it was originally trained. We
comprehensively evaluate both zero-shot sub-resolution and super-resolution
(i.e., multi-resolution) inference in MLOs. We decouple multi-resolution
inference into two key behaviors: 1) extrapolation to varying frequency
information; and 2) interpolating across varying resolutions. We empirically
demonstrate that MLOs fail to do both of these tasks in a zero-shot manner.
Consequently, we find MLOs are not able to perform accurate inference at
resolutions different from those on which they were trained, and instead they
are brittle and susceptible to aliasing. To address these failure modes, we
propose a simple, computationally-efficient, and data-driven multi-resolution
training protocol that overcomes aliasing and that provides robust
multi-resolution generalization.

</details>


### [100] [Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions](https://arxiv.org/abs/2510.06649)
*Frank Wu,Mengye Ren*

Main category: cs.LG

TL;DR: 提出ARQ价值估计方法用于局部强化学习，性能优于无反向传播和有反向传播的算法。


<details>
  <summary>Details</summary>
Motivation: Forward - Forward算法多用于监督学习，在强化学习领域存在空白，需要一种新的价值估计方法。

Method: 受FF的良好度函数启发，引入Action - conditioned Root mean squared Q - Functions (ARQ)，使用时间差分学习和良好度函数及动作条件进行局部强化学习。

Result: 在MinAtar和DeepMind Control Suite基准测试中，优于最先进的无反向传播的局部强化学习方法，在大多数任务上也优于使用反向传播训练的算法。

Conclusion: ARQ方法简单且有生物学基础，能有效用于局部强化学习，提升性能。

Abstract: The Forward-Forward (FF) Algorithm is a recently proposed learning procedure
for neural networks that employs two forward passes instead of the traditional
forward and backward passes used in backpropagation. However, FF remains
largely confined to supervised settings, leaving a gap at domains where
learning signals can be yielded more naturally such as RL. In this work,
inspired by FF's goodness function using layer activity statistics, we
introduce Action-conditioned Root mean squared Q-Functions (ARQ), a novel value
estimation method that applies a goodness function and action conditioning for
local RL using temporal difference learning. Despite its simplicity and
biological grounding, our approach achieves superior performance compared to
state-of-the-art local backprop-free RL methods in the MinAtar and the DeepMind
Control Suite benchmarks, while also outperforming algorithms trained with
backpropagation on most tasks. Code can be found at
https://github.com/agentic-learning-ai-lab/arq.

</details>


### [101] [Rethinking Nonlinearity: Trainable Gaussian Mixture Modules for Modern Neural Architectures](https://arxiv.org/abs/2510.06660)
*Weiguo Lu,Gangnan Yuan,Hong-kun Zhang,Shangyang Li*

Main category: cs.LG

TL;DR: 论文引入高斯混合启发的非线性模块GMNM，可集成到多种神经网络架构，实验表明能提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络设计在引入非线性方面受激活函数选择限制。

Method: 引入GMNM模块，借鉴高斯混合模型和高斯核的距离特性，放松概率约束并采用灵活的高斯投影参数化，可端到端训练。

Result: 将GMNM集成到MLPs、CNNs等架构中，性能始终优于标准基线。

Conclusion: GMNM有潜力成为提高机器学习应用效率和准确性的强大灵活模块。

Abstract: Neural networks in general, from MLPs and CNNs to attention-based
Transformers, are constructed from layers of linear combinations followed by
nonlinear operations such as ReLU, Sigmoid, or Softmax. Despite their strength,
these conventional designs are often limited in introducing non-linearity by
the choice of activation functions. In this work, we introduce Gaussian
Mixture-Inspired Nonlinear Modules (GMNM), a new class of differentiable
modules that draw on the universal density approximation Gaussian mixture
models (GMMs) and distance properties (metric space) of Gaussian kernal. By
relaxing probabilistic constraints and adopting a flexible parameterization of
Gaussian projections, GMNM can be seamlessly integrated into diverse neural
architectures and trained end-to-end with gradient-based methods. Our
experiments demonstrate that incorporating GMNM into architectures such as
MLPs, CNNs, attention mechanisms, and LSTMs consistently improves performance
over standard baselines. These results highlight GMNM's potential as a powerful
and flexible module for enhancing efficiency and accuracy across a wide range
of machine learning applications.

</details>


### [102] [XRPO: Pushing the limits of GRPO with Targeted Exploration and Exploitation](https://arxiv.org/abs/2510.06672)
*Udbhav Bamba,Minghao Fang,Yifan Yu,Haizhong Zheng,Fan Lai*

Main category: cs.LG

TL;DR: 本文提出XRPO框架改进大语言模型推理强化学习算法，实验显示其性能优于现有方法并加速训练收敛。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法如GRPO在挑战性提示上探索有限，且未充分利用信息反馈信号。

Method: 引入基于数学的滚动分配器增强探索，采用上下文播种策略解决零奖励提示问题，开发组相对、新颖感知优势锐化机制加强利用。

Result: 在多种数学和编码基准测试中，XRPO比现有方法（如GRPO和GSPO）在pass@1上最多高4%，cons@32上最多高6%，并使训练收敛加速达2.7倍。

Conclusion: XRPO框架有效提升了大语言模型推理强化学习算法的性能和训练效率。

Abstract: Reinforcement learning algorithms such as GRPO have driven recent advances in
large language model (LLM) reasoning. While scaling the number of rollouts
stabilizes training, existing approaches suffer from limited exploration on
challenging prompts and leave informative feedback signals underexploited, due
to context-independent rollout allocation across prompts (e.g., generating 16
rollouts per prompt) and relying heavily on sparse rewards. This paper presents
XRPO(eXplore - eXploit GRPO), a unified framework that recasts policy
optimization through the principled lens of rollout exploration-exploitation.
To enhance exploration, XRPO introduces a mathematically grounded rollout
allocator that adaptively prioritizes prompts with higher potential for
uncertainty reduction. It further addresses stagnation on zero-reward prompts
through an in-context seeding strategy that injects curated exemplars, steering
the model into more difficult reasoning trajectories. To strengthen
exploitation, XRPO develops a group-relative, novelty-aware advantage
sharpening mechanism that leverages sequence likelihoods to amplify
low-probability yet correct responses, thereby extending the policy's reach
beyond sparse rewards. Experiments across diverse math and coding benchmarks on
both reasoning and non-reasoning models demonstrate that XRPO outperforms
existing advances (e.g., GRPO and GSPO) up to 4% pass@1 and 6% cons@32, while
accelerating training convergence by up to 2.7X.

</details>


### [103] [TimeFormer: Transformer with Attention Modulation Empowered by Temporal Characteristics for Time Series Forecasting](https://arxiv.org/abs/2510.06680)
*Zhipeng Liu,Peibo Duan,Xuan Tang,Baixin Li,Yongsheng Huang,Mingyang Geng,Changsheng Zhang,Bin Zhang,Binwu Wang*

Main category: cs.LG

TL;DR: 提出针对时间序列数据的Transformer架构TimeFormer，其含MoSA机制，实验显示显著优于SOTA方法，MoSA机制可提升其他模型性能。


<details>
  <summary>Details</summary>
Motivation: Transformers扩展到时间序列预测有挑战，因未充分考虑文本和时间模态差异，要最大化其对时间序列数据的表征能力。

Method: 识别时间序列两个关键特征，提出含两个调制项的自注意力机制MoSA，引入多尺度和子序列分析框架。

Result: 在多真实数据集实验中，TimeFormer显著优于SOTA方法，MSE最多降7.45%，94.04%评估指标创新基准，MoSA机制可提升其他模型性能。

Conclusion: TimeFormer在时间序列预测中表现出色，MoSA机制有广泛应用价值。

Abstract: Although Transformers excel in natural language processing, their extension
to time series forecasting remains challenging due to insufficient
consideration of the differences between textual and temporal modalities. In
this paper, we develop a novel Transformer architecture designed for time
series data, aiming to maximize its representational capacity. We identify two
key but often overlooked characteristics of time series: (1) unidirectional
influence from the past to the future, and (2) the phenomenon of decaying
influence over time. These characteristics are introduced to enhance the
attention mechanism of Transformers. We propose TimeFormer, whose core
innovation is a self-attention mechanism with two modulation terms (MoSA),
designed to capture these temporal priors of time series under the constraints
of the Hawkes process and causal masking. Additionally, TimeFormer introduces a
framework based on multi-scale and subsequence analysis to capture semantic
dependencies at different temporal scales, enriching the temporal dependencies.
Extensive experiments conducted on multiple real-world datasets show that
TimeFormer significantly outperforms state-of-the-art methods, achieving up to
a 7.45% reduction in MSE compared to the best baseline and setting new
benchmarks on 94.04\% of evaluation metrics. Moreover, we demonstrate that the
MoSA mechanism can be broadly applied to enhance the performance of other
Transformer-based models.

</details>


### [104] [Distributed Algorithms for Multi-Agent Multi-Armed Bandits with Collision](https://arxiv.org/abs/2510.06683)
*Daoyuan Zhou,Xuchuang Wang,Lin Yang,Yang Gao*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the stochastic Multiplayer Multi-Armed Bandit (MMAB) problem, where
multiple players select arms to maximize their cumulative rewards. Collisions
occur when two or more players select the same arm, resulting in no reward, and
are observed by the players involved. We consider a distributed setting without
central coordination, where each player can only observe their own actions and
collision feedback. We propose a distributed algorithm with an adaptive,
efficient communication protocol. The algorithm achieves near-optimal group and
individual regret, with a communication cost of only $\mathcal{O}(\log\log T)$.
Our experiments demonstrate significant performance improvements over existing
baselines. Compared to state-of-the-art (SOTA) methods, our approach achieves a
notable reduction in individual regret. Finally, we extend our approach to a
periodic asynchronous setting, proving the lower bound for this problem and
presenting an algorithm that achieves logarithmic regret.

</details>


### [105] [AutoBalance: An Automatic Balancing Framework for Training Physics-Informed Neural Networks](https://arxiv.org/abs/2510.06684)
*Kang An,Chenhao Si,Ming Yan,Shiqian Ma*

Main category: cs.LG

TL;DR: 现有PINNs训练困难，提出AutoBalance新训练范式，实验表明其性能优于现有框架，且与其他方法互补。


<details>
  <summary>Details</summary>
Motivation: PINNs训练困难，现有‘pre - combine’策略有局限性。

Method: 引入AutoBalance‘post - combine’训练范式，为每个损失组件分配独立自适应优化器并聚合预条件更新。

Result: 在具有挑战性的PDE基准测试中，AutoBalance始终优于现有框架，显著降低解的误差。

Conclusion: AutoBalance是有效的，且与其他PINN方法正交互补，能提升其在复杂基准测试中的效果。

Abstract: Physics-Informed Neural Networks (PINNs) provide a powerful and general
framework for solving Partial Differential Equations (PDEs) by embedding
physical laws into loss functions. However, training PINNs is notoriously
difficult due to the need to balance multiple loss terms, such as PDE residuals
and boundary conditions, which often have conflicting objectives and vastly
different curvatures. Existing methods address this issue by manipulating
gradients before optimization (a "pre-combine" strategy). We argue that this
approach is fundamentally limited, as forcing a single optimizer to process
gradients from spectrally heterogeneous loss landscapes disrupts its internal
preconditioning. In this work, we introduce AutoBalance, a novel "post-combine"
training paradigm. AutoBalance assigns an independent adaptive optimizer to
each loss component and aggregates the resulting preconditioned updates
afterwards. Extensive experiments on challenging PDE benchmarks show that
AutoBalance consistently outperforms existing frameworks, achieving significant
reductions in solution error, as measured by both the MSE and $L^{\infty}$
norms. Moreover, AutoBalance is orthogonal to and complementary with other
popular PINN methodologies, amplifying their effectiveness on demanding
benchmarks.

</details>


### [106] [Is the Hard-Label Cryptanalytic Model Extraction Really Polynomial?](https://arxiv.org/abs/2510.06692)
*Akira Ito,Takayuki Miura,Yosuke Todo*

Main category: cs.LG

TL;DR: 本文指出Carlini等人攻击方法在目标深度增加时假设不现实，提出CrossLayer Extraction攻击方法降低查询复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有模型提取攻击方法在攻击目标深度增加时，假设变得不现实，攻击不总能在多项式时间内运行。

Method: 提出CrossLayer Extraction攻击方法，利用层间神经元交互从更深层提取信息。

Result: 显著降低查询复杂度，缓解现有模型提取方法的局限性。

Conclusion: CrossLayer Extraction方法有效解决了现有模型提取攻击方法在深度增加时的局限。

Abstract: Deep Neural Networks (DNNs) have attracted significant attention, and their
internal models are now considered valuable intellectual assets. Extracting
these internal models through access to a DNN is conceptually similar to
extracting a secret key via oracle access to a block cipher. Consequently,
cryptanalytic techniques, particularly differential-like attacks, have been
actively explored recently. ReLU-based DNNs are the most commonly and widely
deployed architectures. While early works (e.g., Crypto 2020, Eurocrypt 2024)
assume access to exact output logits, which are usually invisible, more recent
works (e.g., Asiacrypt 2024, Eurocrypt 2025) focus on the hard-label setting,
where only the final classification result (e.g., "dog" or "car") is available
to the attacker. Notably, Carlini et al. (Eurocrypt 2025) demonstrated that
model extraction is feasible in polynomial time even under this restricted
setting.
  In this paper, we first show that the assumptions underlying their attack
become increasingly unrealistic as the attack-target depth grows. In practice,
satisfying these assumptions requires an exponential number of queries with
respect to the attack depth, implying that the attack does not always run in
polynomial time. To address this critical limitation, we propose a novel attack
method called CrossLayer Extraction. Instead of directly extracting the secret
parameters (e.g., weights and biases) of a specific neuron, which incurs
exponential cost, we exploit neuron interactions across layers to extract this
information from deeper layers. This technique significantly reduces query
complexity and mitigates the limitations of existing model extraction
approaches.

</details>


### [107] [A Diffusion Model for Regular Time Series Generation from Irregular Data with Completion and Masking](https://arxiv.org/abs/2510.06699)
*Gal Fadlon,Idan Arbiv,Nimrod Berman,Omri Azencot*

Main category: cs.LG

TL;DR: 提出新两步框架解决不规则时间序列数据生成问题，性能达最优


<details>
  <summary>Details</summary>
Motivation: 现有方法处理不规则时间序列数据生成时结果不佳、计算成本高，简单扩展ImagenTime会引入问题

Method: 先使用时间序列Transformer补全不规则序列，再用带掩码的基于视觉的扩散模型最小化对补全值的依赖

Result: 方法实现了最优性能，判别分数相对提升70%，计算成本降低85%

Conclusion: 新方法结合补全和掩码优势，能强大且高效地生成逼真时间序列

Abstract: Generating realistic time series data is critical for applications in
healthcare, finance, and science. However, irregular sampling and missing
values present significant challenges. While prior methods address these
irregularities, they often yield suboptimal results and incur high
computational costs. Recent advances in regular time series generation, such as
the diffusion-based ImagenTime model, demonstrate strong, fast, and scalable
generative capabilities by transforming time series into image representations,
making them a promising solution. However, extending ImagenTime to irregular
sequences using simple masking introduces "unnatural" neighborhoods, where
missing values replaced by zeros disrupt the learning process. To overcome
this, we propose a novel two-step framework: first, a Time Series Transformer
completes irregular sequences, creating natural neighborhoods; second, a
vision-based diffusion model with masking minimizes dependence on the completed
values. This approach leverages the strengths of both completion and masking,
enabling robust and efficient generation of realistic time series. Our method
achieves state-of-the-art performance, achieving a relative improvement in
discriminative score by $70\%$ and in computational cost by $85\%$. Code is at
https://github.com/azencot-group/ImagenI2R.

</details>


### [108] [Dual Goal Representations](https://arxiv.org/abs/2510.06714)
*Seohong Park,Deepinder Mann,Sergey Levine*

Main category: cs.LG

TL;DR: 本文引入用于目标条件强化学习的双重目标表示，开发学习方法，实验证明其能提升离线目标达成性能。


<details>
  <summary>Details</summary>
Motivation: 为目标条件强化学习引入具有良好理论性质的目标表示，以提升性能。

Method: 引入双重目标表示，其通过与其他状态的时间距离来表征状态，并开发与之结合的目标表示学习方法。

Result: 在OGBench任务套件的20个基于状态和像素的任务中，双重目标表示持续提升离线目标达成性能。

Conclusion: 双重目标表示具有良好理论性质，能有效提升目标条件强化学习的离线目标达成性能。

Abstract: In this work, we introduce dual goal representations for goal-conditioned
reinforcement learning (GCRL). A dual goal representation characterizes a state
by "the set of temporal distances from all other states"; in other words, it
encodes a state through its relations to every other state, measured by
temporal distance. This representation provides several appealing theoretical
properties. First, it depends only on the intrinsic dynamics of the environment
and is invariant to the original state representation. Second, it contains
provably sufficient information to recover an optimal goal-reaching policy,
while being able to filter out exogenous noise. Based on this concept, we
develop a practical goal representation learning method that can be combined
with any existing GCRL algorithm. Through diverse experiments on the OGBench
task suite, we empirically show that dual goal representations consistently
improve offline goal-reaching performance across 20 state- and pixel-based
tasks.

</details>


### [109] [Incorporating Expert Knowledge into Bayesian Causal Discovery of Mixtures of Directed Acyclic Graphs](https://arxiv.org/abs/2510.06735)
*Zachris Björkman,Jorge Loría,Sophie Wharrie,Samuel Kaski*

Main category: cs.LG

TL;DR: 提出适用于异构领域的因果引出策略和VaMSL方法，结合专家反馈构建图先验，在合成数据和乳腺癌数据库上有良好表现。


<details>
  <summary>Details</summary>
Motivation: 现有先验引出方法假设单一因果图，不适用于异构领域，需要适用于异构领域的方法。

Method: 基于贝叶斯实验设计原则提出因果引出策略，扩展DiBS方法得到VaMSL方法，在CBNs混合推理中构建包含专家反馈的图先验。

Result: 成功生成一组替代因果模型，在有模拟专家信息时，在异构合成数据上结构学习性能提升，能捕捉乳腺癌数据库中的复杂分布。

Conclusion: 所提方法适用于异构领域的因果发现，能有效提升结构学习性能。

Abstract: Bayesian causal discovery benefits from prior information elicited from
domain experts, and in heterogeneous domains any prior knowledge would be badly
needed. However, so far prior elicitation approaches have assumed a single
causal graph and hence are not suited to heterogeneous domains. We propose a
causal elicitation strategy for heterogeneous settings, based on Bayesian
experimental design (BED) principles, and a variational mixture structure
learning (VaMSL) method -- extending the earlier differentiable Bayesian
structure learning (DiBS) method -- to iteratively infer mixtures of causal
Bayesian networks (CBNs). We construct an informative graph prior incorporating
elicited expert feedback in the inference of mixtures of CBNs. Our proposed
method successfully produces a set of alternative causal models (mixture
components or clusters), and achieves an improved structure learning
performance on heterogeneous synthetic data when informed by a simulated
expert. Finally, we demonstrate that our approach is capable of capturing
complex distributions in a breast cancer database.

</details>


### [110] [Function regression using the forward forward training and inferring paradigm](https://arxiv.org/abs/2510.06762)
*Shivam Padmani,Akshay Joshi*

Main category: cs.LG

TL;DR: 本文提出用前向 - 前向算法进行函数逼近的新方法，并在单变量和多变量函数上评估，还对拓展应用进行初步研究。


<details>
  <summary>Details</summary>
Motivation: 前向 - 前向学习算法适用于神经形态计算，但目前仅用于分类任务，需要将其拓展到函数回归领域。

Method: 引入使用前向 - 前向算法进行函数逼近的新方法。

Result: 在单变量和多变量函数上评估了所开发的方法。

Conclusion: 提出的前向 - 前向回归方法可用于函数逼近，并对拓展应用有初步研究。

Abstract: Function regression/approximation is a fundamental application of machine
learning. Neural networks (NNs) can be easily trained for function regression
using a sufficient number of neurons and epochs. The forward-forward learning
algorithm is a novel approach for training neural networks without
backpropagation, and is well suited for implementation in neuromorphic
computing and physical analogs for neural networks. To the best of the authors'
knowledge, the Forward Forward paradigm of training and inferencing NNs is
currently only restricted to classification tasks. This paper introduces a new
methodology for approximating functions (function regression) using the
Forward-Forward algorithm. Furthermore, the paper evaluates the developed
methodology on univariate and multivariate functions, and provides preliminary
studies of extending the proposed Forward-Forward regression to Kolmogorov
Arnold Networks, and Deep Physical Neural Networks.

</details>


### [111] [Modeling COVID-19 Dynamics in German States Using Physics-Informed Neural Networks](https://arxiv.org/abs/2510.06776)
*Phillip Rothenbeck,Sai Karthikeya Vemuri,Niklas Penzel,Joachim Denzler*

Main category: cs.LG

TL;DR: 使用PINNs对德国三年新冠疫情数据进行时空分析，揭示地区传播差异及与疫苗接种等关联，证明PINNs在流行病学建模的实用性。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情凸显定量建模分析的需求，传统SIR模型难以直接融入观测数据，需新方法。

Method: 采用物理信息神经网络（PINNs）解决SIR模型的逆问题，使用德国疾控机构感染数据。

Result: 发现地区间传播行为差异大，与疫苗接种和疫情阶段时间模式相关。

Conclusion: PINNs在局部、长期流行病学建模中有用。

Abstract: The COVID-19 pandemic has highlighted the need for quantitative modeling and
analysis to understand real-world disease dynamics. In particular, post hoc
analyses using compartmental models offer valuable insights into the
effectiveness of public health interventions, such as vaccination strategies
and containment policies. However, such compartmental models like SIR
(Susceptible-Infectious-Recovered) often face limitations in directly
incorporating noisy observational data. In this work, we employ
Physics-Informed Neural Networks (PINNs) to solve the inverse problem of the
SIR model using infection data from the Robert Koch Institute (RKI). Our main
contribution is a fine-grained, spatio-temporal analysis of COVID-19 dynamics
across all German federal states over a three-year period. We estimate
state-specific transmission and recovery parameters and time-varying
reproduction number (R_t) to track the pandemic progression. The results
highlight strong variations in transmission behavior across regions, revealing
correlations with vaccination uptake and temporal patterns associated with
major pandemic phases. Our findings demonstrate the utility of PINNs in
localized, long-term epidemiological modeling.

</details>


### [112] [Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness](https://arxiv.org/abs/2510.06790)
*Tavish McDonald,Bo Lei,Stanislav Fort,Bhavya Kailkhura,Brian Bartoldson*

Main category: cs.LG

TL;DR: 论文探讨模型对抗分布外数据问题，提出RICH假设，实证支持并建议结合训练和测试时防御。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在攻击者有梯度或多模态输入时，测试计算防御效果衰退的问题。

Method: 提出Robustness from Inference Compute Hypothesis (RICH)假设，通过实证研究不同模型和攻击类型。

Result: 若组合泛化能使模型遵循分布外数据规范，测试时计算可提升鲁棒性，强化规范提示对预训练模型有效。

Conclusion: 建议将训练时和测试时的防御措施结合，以获得协同效益。

Abstract: Models are susceptible to adversarially out-of-distribution (OOD) data
despite large training-compute investments into their robustification. Zaremba
et al. (2025) make progress on this problem at test time, showing LLM reasoning
improves satisfaction of model specifications designed to thwart attacks,
resulting in a correlation between reasoning effort and robustness to
jailbreaks. However, this benefit of test compute fades when attackers are
given access to gradients or multimodal inputs. We address this gap, clarifying
that inference-compute offers benefits even in such cases. Our approach argues
that compositional generalization, through which OOD data is understandable via
its in-distribution (ID) components, enables adherence to defensive
specifications on adversarially OOD inputs. Namely, we posit the Robustness
from Inference Compute Hypothesis (RICH): inference-compute defenses profit as
the model's training data better reflects the attacked data's components. We
empirically support this hypothesis across vision language model and attack
types, finding robustness gains from test-time compute if specification
following on OOD data is unlocked by compositional generalization, while RL
finetuning and protracted reasoning are not critical. For example, increasing
emphasis on defensive specifications via prompting lowers the success rate of
gradient-based multimodal attacks on VLMs robustified by adversarial
pretraining, but this same intervention provides no such benefit to
not-robustified models. This correlation of inference-compute's robustness
benefit with base model robustness is the rich-get-richer dynamic of the RICH:
attacked data components are more ID for robustified models, aiding
compositional generalization to OOD data. Accordingly, we advise layering
train-time and test-time defenses to obtain their synergistic benefit.

</details>


### [113] [The Unreasonable Effectiveness of Randomized Representations in Online Continual Graph Learning](https://arxiv.org/abs/2510.06819)
*Giovanni Donghi,Daniele Zambon,Luca Pasa,Cesare Alippi,Nicolò Navarin*

Main category: cs.LG

TL;DR: 提出一种简单有效的在线持续图学习方法，用固定随机初始化编码器生成节点嵌入，仅在线训练轻量级分类器，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 灾难性遗忘是在线持续图学习的主要障碍，现有方法难以解决。

Method: 使用固定随机初始化的编码器聚合邻域信息生成节点嵌入，仅在线训练轻量级分类器，冻结编码器以消除表示参数漂移。

Result: 在多个OCGL基准测试中，该方法始终优于现有方法，提升高达30%，性能接近联合离线训练上限。

Conclusion: 在OCGL中，通过采用架构的简单性和稳定性，无需复杂的重放或正则化即可最小化灾难性遗忘。

Abstract: Catastrophic forgetting is one of the main obstacles for Online Continual
Graph Learning (OCGL), where nodes arrive one by one, distribution drifts may
occur at any time and offline training on task-specific subgraphs is not
feasible. In this work, we explore a surprisingly simple yet highly effective
approach for OCGL: we use a fixed, randomly initialized encoder to generate
robust and expressive node embeddings by aggregating neighborhood information,
training online only a lightweight classifier. By freezing the encoder, we
eliminate drifts of the representation parameters, a key source of forgetting,
obtaining embeddings that are both expressive and stable. When evaluated across
several OCGL benchmarks, despite its simplicity and lack of memory buffer, this
approach yields consistent gains over state-of-the-art methods, with surprising
improvements of up to 30% and performance often approaching that of the joint
offline-training upper bound. These results suggest that in OCGL, catastrophic
forgetting can be minimized without complex replay or regularization by
embracing architectural simplicity and stability.

</details>


### [114] [Efficient numeracy in language models through single-token number embeddings](https://arxiv.org/abs/2510.06824)
*Linus Kreitner,Paul Hager,Jonathan Mengedoht,Georgios Kaissis,Daniel Rueckert,Martin J. Menten*

Main category: cs.LG

TL;DR: 为使大语言模型有效处理数值数据和长计算，提出新的单令牌数字编码策略BitTokens，实验表明可提升小模型解决算术问题的能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型处理数值数据和长计算需依赖外部工具或大量推理链，存在局限性，且现有数字编码方法有不足，因此需要高效有效的单令牌数字编码。

Method: 提出BitTokens，利用IEEE 754二进制浮点表示将任意数字嵌入单个令牌。

Result: 实验显示BitTokens能让小语言模型近乎完美地学习解决基本算术运算的算法。

Conclusion: BitTokens带来的效率提升可拓展语言模型能解决问题的长度和复杂度。

Abstract: To drive progress in science and engineering, large language models (LLMs)
must be able to process large amounts of numerical data and solve long
calculations efficiently. This is currently only possible through the use of
external tools or extensive reasoning chains, either limiting the numerical
intuition of LLMs or limiting the length of problems they can solve. We show
that frontier LLMs require excessive amounts of reasoning tokens to solve even
basic calculations, which is exacerbated by their tokenization strategies that
split single numbers into multiple tokens. This motivates the need for
efficient and effective single-token number encodings. We introduce a set of
desiderata for such encodings and show that existing approaches fail to fulfill
them. To address these shortcomings, we propose BitTokens, a novel tokenization
strategy that embeds any number into a single token using its IEEE 754 binary
floating-point representation. Through extensive experiments we show that our
BitTokens allow even small language models to learn algorithms that solve basic
arithmetic operations nearly perfectly. This newly gained efficiency could
expand the length and complexity of problems language models can solve.

</details>


### [115] [Recurrence-Complete Frame-based Action Models](https://arxiv.org/abs/2510.06828)
*Michael Keiblinger*

Main category: cs.LG

TL;DR: 本文挑战‘Attention Is All You Need’观点，指出非循环完备模型局限，引入循环完备架构并训练，发现损失与训练序列长度关系及训练优势。


<details>
  <summary>Details</summary>
Motivation: 挑战‘Attention Is All You Need’中RNN单元无需与注意力机制结合的观点，解决非循环完备模型在长期代理任务中的问题。

Method: 指出全可并行架构局限性，提出临界时间猜想，引入循环完备架构并在GitHub动作序列上训练。

Result: 损失与训练序列长度呈幂律关系，参数数量固定，长序列训练能降低时间成本并减少损失。

Conclusion: 循环完备架构在处理长序列任务中有优势，可用于代理系统。

Abstract: In recent years, attention-like mechanisms have been used to great success in
the space of large language models, unlocking scaling potential to a previously
unthinkable extent. "Attention Is All You Need" famously claims RNN cells are
not needed in conjunction with attention. We challenge this view. In this
paper, we point to existing proofs that architectures with fully parallelizable
forward or backward passes cannot represent classes of problems specifically
interesting for long-running agentic tasks. We further conjecture a critical
time t beyond which non-recurrence-complete models fail to aggregate inputs
correctly, with concrete implications for agentic systems (e.g., software
engineering agents). To address this, we introduce a recurrence-complete
architecture and train it on GitHub-derived action sequences. Loss follows a
power law in the trained sequence length while the parameter count remains
fixed. Moreover, longer-sequence training always amortizes its linearly
increasing wall-time cost, yielding lower loss as a function of wall time.

</details>


### [116] [Early wind turbine alarm prediction based on machine learning: AlarmForecasting](https://arxiv.org/abs/2510.06831)
*Syed Shazaib Shah,Daoliang Tan*

Main category: cs.LG

TL;DR: 本文提出AFC框架预测风力涡轮机警报，案例研究表明不同时间的警报预测有一定准确率，能减少警报频率并提高运营效率。


<details>
  <summary>Details</summary>
Motivation: 传统研究仅将警报数据作为诊断工具，本研究旨在提前预测警报，防止其触发并避免故障。

Method: 提出基于LSTM回归模块进行时间序列警报预测，再通过分类模块对预测警报进行标记的AFC框架。

Result: 以14台Senvion MM82涡轮机为例，10、20和30分钟警报预测准确率分别为82%、52%和41%。

Conclusion: 该方法可有效预测和避免警报，减少警报频率，通过主动干预提高运营效率。

Abstract: Alarm data is pivotal in curbing fault behavior in Wind Turbines (WTs) and
forms the backbone for advancedpredictive monitoring systems. Traditionally,
research cohorts have been confined to utilizing alarm data solelyas a
diagnostic tool, merely indicative of unhealthy status. However, this study
aims to offer a transformativeleap towards preempting alarms, preventing alarms
from triggering altogether, and consequently avertingimpending failures. Our
proposed Alarm Forecasting and Classification (AFC) framework is designed on
twosuccessive modules: first, the regression module based on long short-term
memory (LSTM) for time-series alarmforecasting, and thereafter, the
classification module to implement alarm tagging on the forecasted alarm.
Thisway, the entire alarm taxonomy can be forecasted reliably rather than a few
specific alarms. 14 Senvion MM82turbines with an operational period of 5 years
are used as a case study; the results demonstrated 82%, 52%,and 41% accurate
forecasts for 10, 20, and 30 min alarm forecasts, respectively. The results
substantiateanticipating and averting alarms, which is significant in curbing
alarm frequency and enhancing operationalefficiency through proactive
intervention.

</details>


### [117] [CNN-TFT explained by SHAP with multi-head attention weights for time series forecasting](https://arxiv.org/abs/2510.06840)
*Stefano F. Stefenon,João P. Matos-Carvalho,Valderi R. Q. Leithardt,Kin-Choong Yow*

Main category: cs.LG

TL;DR: 本文提出CNN - TFT - SHAP - MHAW混合架构用于多元时间序列预测，在水电自然流量数据集上表现优于现有模型，代码开源。


<details>
  <summary>Details</summary>
Motivation: 结合CNN和transformer架构优势，提升多元时间序列预测效果。

Method: 提出CNN - TFT架构，先用CNN模块提取局部特征，再将特征图输入TFT捕获长短依赖；用SHAP - MHAW实现模型可解释性。

Result: CNN - TFT在水电自然流量数据集上表现优于成熟深度学习模型，平均绝对百分比误差达2.2%。

Conclusion: CNN - TFT - SHAP - MHAW架构在高保真多元时间序列预测应用中有前景。

Abstract: Convolutional neural networks (CNNs) and transformer architectures offer
strengths for modeling temporal data: CNNs excel at capturing local patterns
and translational invariances, while transformers effectively model long-range
dependencies via self-attention. This paper proposes a hybrid architecture
integrating convolutional feature extraction with a temporal fusion transformer
(TFT) backbone to enhance multivariate time series forecasting. The CNN module
first applies a hierarchy of one-dimensional convolutional layers to distill
salient local patterns from raw input sequences, reducing noise and
dimensionality. The resulting feature maps are then fed into the TFT, which
applies multi-head attention to capture both short- and long-term dependencies
and to weigh relevant covariates adaptively. We evaluate the CNN-TFT on a
hydroelectric natural flow time series dataset. Experimental results
demonstrate that CNN-TFT outperforms well-established deep learning models,
with a mean absolute percentage error of up to 2.2%. The explainability of the
model is obtained by a proposed Shapley additive explanations with multi-head
attention weights (SHAP-MHAW). Our novel architecture, named CNN-TFT-SHAP-MHAW,
is promising for applications requiring high-fidelity, multivariate time series
forecasts, being available for future analysis at
https://github.com/SFStefenon/CNN-TFT-SHAP-MHAW .

</details>


### [118] [Enhancing Bankruptcy Prediction of Banks through Advanced Machine Learning Techniques: An Innovative Approach and Analysis](https://arxiv.org/abs/2510.06852)
*Zuherman Rustam,Sri Hartini,Sardar M. N. Islam,Fevi Novkaniza,Fiftitah R. Aszhari,Muhammad Rifqi*

Main category: cs.LG

TL;DR: 文章用机器学习技术开发银行破产模型，实验表明方法有效，能辅助制定降低破产成本的政策。


<details>
  <summary>Details</summary>
Motivation: 现有统计模型预测银行破产概率时依赖假设，准确率低，需新方法保障银行系统安全。

Method: 用逻辑回归、随机森林和支持向量机等机器学习技术，基于土耳其和印尼银行数据构建破产模型。

Result: 随机森林预测商业银行数据准确率达90%，三种方法准确预测农村银行破产可能性。

Conclusion: 创新的机器学习方法有助于实施降低破产成本的政策。

Abstract: Context: Financial system stability is determined by the condition of the
banking system. A bank failure can destroy the stability of the financial
system, as banks are subject to systemic risk, affecting not only individual
banks but also segments or the entire financial system. Calculating the
probability of a bank going bankrupt is one way to ensure the banking system is
safe and sound. Existing literature and limitations: Statistical models, such
as Altman's Z-Score, are one of the common techniques for developing a
bankruptcy prediction model. However, statistical methods rely on rigid and
sometimes irrelevant assumptions, which can result in low forecast accuracy.
New approaches are necessary. Objective of the research: Bankruptcy models are
developed using machine learning techniques, such as logistic regression (LR),
random forest (RF), and support vector machines (SVM). According to several
studies, machine learning is also more accurate and effective than statistical
methods for categorising and forecasting banking risk management. Present
Research: The commercial bank data are derived from the annual financial
statements of 44 active banks and 21 bankrupt banks in Turkey from 1994 to
2004, and the rural bank data are derived from the quarterly financial reports
of 43 active and 43 bankrupt rural banks in Indonesia between 2013 and 2019.
Five rural banks in Indonesia have also been selected to demonstrate the
feasibility of analysing bank bankruptcy trends. Findings and implications: The
results of the research experiments show that RF can forecast data from
commercial banks with a 90% accuracy rate. Furthermore, the three machine
learning methods proposed accurately predict the likelihood of rural bank
bankruptcy. Contribution and Conclusion: The proposed innovative machine
learning approach help to implement policies that reduce the costs of
bankruptcy.

</details>


### [119] [Towards Generalization of Graph Neural Networks for AC Optimal Power Flow](https://arxiv.org/abs/2510.06860)
*Olayiwola Arowolo,Jochen L. Cremer*

Main category: cs.LG

TL;DR: 提出混合异构消息传递神经网络（HH - MPNN）解决交流最优潮流（ACOPF）问题，在不同规模电网和拓扑下表现良好，有显著计算加速。


<details>
  <summary>Details</summary>
Motivation: 传统求解器解决大规模电力系统的ACOPF计算成本高，机器学习方法在可扩展性和拓扑适应性上有问题。

Method: 提出HH - MPNN，将不同电力元件建模为不同节点或边类型，结合可扩展的transformer模型处理长距离依赖。

Result: 在14到2000个母线的电网默认拓扑上最优性差距小于1%，零样本应用于数千个未见拓扑时差距小于3%，在小电网预训练可提升大电网结果，计算加速达1000x到10000x。

Conclusion: 研究推动了用于实时电力系统操作的实用、可泛化机器学习方法的发展。

Abstract: AC Optimal Power Flow (ACOPF) is computationally expensive for large-scale
power systems, with conventional solvers requiring prohibitive solution times.
Machine learning approaches offer computational speedups but struggle with
scalability and topology adaptability without expensive retraining. To enable
scalability across grid sizes and adaptability to topology changes, we propose
a Hybrid Heterogeneous Message Passing Neural Network (HH-MPNN). HH-MPNN models
buses, generators, loads, shunts, transmission lines and transformers as
distinct node or edge types, combined with a scalable transformer model for
handling long-range dependencies. On grids from 14 to 2,000 buses, HH-MPNN
achieves less than 1% optimality gap on default topologies. Applied zero-shot
to thousands of unseen topologies, HH-MPNN achieves less than 3% optimality gap
despite training only on default topologies. Pre-training on smaller grids also
improves results on a larger grid. Computational speedups reach 1,000x to
10,000x compared to interior point solvers. These results advance practical,
generalizable machine learning for real-time power system operations.

</details>


### [120] [SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models](https://arxiv.org/abs/2510.06871)
*Huahui Yi,Kun Wang,Qiankun Li,Miao Yu,Liang Lin,Gongli Xi,Hao Wu,Xuming Hu,Kang Li,Yang Liu*

Main category: cs.LG

TL;DR: 提出SaFeR - VLM框架解决MLRMs安全风险问题，表现超多个大模型，代码开源。


<details>
  <summary>Details</summary>
Motivation: MLRMs在对抗或不安全提示下有安全风险，现有防御多在输出层，未约束推理过程。

Method: 提出SaFeR - VLM框架，集成QI - Safe - 10K数据集、安全感知回滚、结构化奖励建模和GRPO优化四个组件。

Result: SaFeR - VLM - 3B在六个基准测试中安全和有用性表现超同规模及更大模型；SaFeR - VLM - 7B在安全指标上超GPT - 5 - mini和Gemini - 2.5 - Flash。

Conclusion: 该框架将安全从被动保障转为主动推理驱动力，实现可扩展和可泛化的安全感知推理。

Abstract: Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal
reasoning but often amplify safety risks under adversarial or unsafe prompts, a
phenomenon we call the \textit{Reasoning Tax}. Existing defenses mainly act at
the output level and do not constrain the reasoning process, leaving models
exposed to implicit risks. In this paper, we propose SaFeR-VLM, a
safety-aligned reinforcement learning framework that embeds safety directly
into multimodal reasoning. The framework integrates four components: (I)
QI-Safe-10K, a curated dataset emphasizing safety-critical and
reasoning-sensitive cases; (II) safety-aware rollout, where unsafe generations
undergo reflection and correction instead of being discarded; (III) structured
reward modeling with multi-dimensional weighted criteria and explicit penalties
for hallucinations and contradictions; and (IV) GRPO optimization, which
reinforces both safe and corrected trajectories. This unified design shifts
safety from a passive safeguard to an active driver of reasoning, enabling
scalable and generalizable safety-aware reasoning. SaFeR-VLM further
demonstrates robustness against both explicit and implicit risks, supporting
dynamic and interpretable safety decisions beyond surface-level filtering.
SaFeR-VLM-3B achieves average performance $70.13$ and $78.97$ on safety and
helpfulness across six benchmarks, surpassing both same-scale and $>10\times$
larger models such as Skywork-R1V3-38B, Qwen2.5VL-72B, and GLM4.5V-106B.
Remarkably, SaFeR-VLM-7B benefits from its increased scale to surpass
GPT-5-mini and Gemini-2.5-Flash by \num{6.47} and \num{16.76} points
respectively on safety metrics, achieving this improvement without any
degradation in helpfulness performance. Our codes are available at
https://github.com/HarveyYi/SaFeR-VLM.

</details>


### [121] [MoRE-GNN: Multi-omics Data Integration with a Heterogeneous Graph Autoencoder](https://arxiv.org/abs/2510.06880)
*Zhiyu Wang,Sonia Koszut,Pietro Liò,Francesco Ceccarelli*

Main category: cs.LG

TL;DR: 介绍MoRE - GNN用于多组学单细胞数据整合，评估显示其表现优且能进行下游跨模态预测，提供了适应性框架。


<details>
  <summary>Details</summary>
Motivation: 解决多组学单细胞数据因高维性和复杂模态间关系导致的整合难题。

Method: 引入MoRE - GNN，结合图卷积和注意力机制，直接从数据动态构建关系图。

Result: 在六个公开数据集上评估，MoRE - GNN捕获生物有意义关系，性能优于现有方法，尤其在模态间强相关设置中，学习的表征可进行准确下游跨模态预测。

Conclusion: 尽管性能随数据集复杂度变化，但MoRE - GNN为推进多组学整合提供了自适应、可扩展和可解释的框架。

Abstract: The integration of multi-omics single-cell data remains challenging due to
high-dimensionality and complex inter-modality relationships. To address this,
we introduce MoRE-GNN (Multi-omics Relational Edge Graph Neural Network), a
heterogeneous graph autoencoder that combines graph convolution and attention
mechanisms to dynamically construct relational graphs directly from data.
Evaluations on six publicly available datasets demonstrate that MoRE-GNN
captures biologically meaningful relationships and outperforms existing
methods, particularly in settings with strong inter-modality correlations.
Furthermore, the learned representations allow for accurate downstream
cross-modal predictions. While performance may vary with dataset complexity,
MoRE-GNN offers an adaptive, scalable and interpretable framework for advancing
multi-omics integration.

</details>


### [122] [Angular Constraint Embedding via SpherePair Loss for Constrained Clustering](https://arxiv.org/abs/2510.06907)
*Shaojie Zhang,Ke Chen*

Main category: cs.LG

TL;DR: 提出用于深度约束聚类的SpherePair方法，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有深度约束聚类方法存在端到端建模的锚点限制和学习欧几里得嵌入的困难，限制了可扩展性和实用性。

Method: 提出新颖的角度约束嵌入方法SpherePair，使用SpherePair损失和几何公式编码成对约束，分离表示学习和聚类。

Result: SpherePair能保留成对关系，无需指定聚类数量，可推广到未见数据，能快速推断聚类数量，有理论保证。对比评估显示其性能、可扩展性和实际有效性优越。

Conclusion: SpherePair方法在深度约束聚类中表现良好，具有实用性和理论支持。

Abstract: Constrained clustering integrates domain knowledge through pairwise
constraints. However, existing deep constrained clustering (DCC) methods are
either limited by anchors inherent in end-to-end modeling or struggle with
learning discriminative Euclidean embedding, restricting their scalability and
real-world applicability. To avoid their respective pitfalls, we propose a
novel angular constraint embedding approach for DCC, termed SpherePair. Using
the SpherePair loss with a geometric formulation, our method faithfully encodes
pairwise constraints and leads to embeddings that are clustering-friendly in
angular space, effectively separating representation learning from clustering.
SpherePair preserves pairwise relations without conflict, removes the need to
specify the exact number of clusters, generalizes to unseen data, enables rapid
inference of the number of clusters, and is supported by rigorous theoretical
guarantees. Comparative evaluations with state-of-the-art DCC methods on
diverse benchmarks, along with empirical validation of theoretical insights,
confirm its superior performance, scalability, and overall real-world
effectiveness. Code is available at
\href{https://github.com/spherepaircc/SpherePairCC/tree/main}{our repository}.

</details>


### [123] [Vacuum Spiker: A Spiking Neural Network-Based Model for Efficient Anomaly Detection in Time Series](https://arxiv.org/abs/2510.06910)
*Iago Xabier Vázquez,Javier Sedano,Muhammad Afzal,Ángel Miguel García-Vico*

Main category: cs.LG

TL;DR: 本文提出用于时间序列异常检测的Vacuum Spiker算法，实验显示该算法能效高且性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在异常检测领域能耗高，难以部署在资源受限环境，需开发低能耗方法。

Method: 提出基于脉冲神经网络的Vacuum Spiker算法，采用新检测标准、训练方式和高效编码方案。

Result: 在公开数据集上，算法性能有竞争力且显著降低能耗，在实际案例中成功识别太阳能逆变器的限电事件。

Conclusion: 该算法在异常检测中具有可持续和高效的潜力。

Abstract: Anomaly detection is a key task across domains such as industry, healthcare,
and cybersecurity. Many real-world anomaly detection problems involve analyzing
multiple features over time, making time series analysis a natural approach for
such problems. While deep learning models have achieved strong performance in
this field, their trend to exhibit high energy consumption limits their
deployment in resource-constrained environments such as IoT devices, edge
computing platforms, and wearables. To address this challenge, this paper
introduces the \textit{Vacuum Spiker algorithm}, a novel Spiking Neural
Network-based method for anomaly detection in time series. It incorporates a
new detection criterion that relies on global changes in neural activity rather
than reconstruction or prediction error. It is trained using Spike
Time-Dependent Plasticity in a novel way, intended to induce changes in neural
activity when anomalies occur. A new efficient encoding scheme is also
proposed, which discretizes the input space into non-overlapping intervals,
assigning each to a single neuron. This strategy encodes information with a
single spike per time step, improving energy efficiency compared to
conventional encoding methods. Experimental results on publicly available
datasets show that the proposed algorithm achieves competitive performance
while significantly reducing energy consumption, compared to a wide set of deep
learning and machine learning baselines. Furthermore, its practical utility is
validated in a real-world case study, where the model successfully identifies
power curtailment events in a solar inverter. These results highlight its
potential for sustainable and efficient anomaly detection.

</details>


### [124] [Utilizing Large Language Models for Machine Learning Explainability](https://arxiv.org/abs/2510.06912)
*Alexandros Vassiliades,Nikolaos Polatidis,Stamatios Samaras,Sotiris Diplaris,Ignacio Cabrera Martin,Yannis Manolopoulos,Stefanos Vrochidis,Ioannis Kompatsiaris*

Main category: cs.LG

TL;DR: 研究大语言模型自主生成机器学习解决方案时的可解释性能力，对两个分类任务生成模型并评估，发现大语言模型能生成有效且可解释的模型。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在自主生成机器学习解决方案时的可解释性能力。

Method: 对两个分类任务，用三种大语言模型为四种常见分类器设计训练管道，用SHAP评估生成模型的预测性能和可解释性。

Result: 大语言模型能生成有效、可解释的管道，具有高保真度和一致稀疏性，与手动设计的基线相近。

Conclusion: 大语言模型有作为可解释机器学习管道自动生成工具的潜力。

Abstract: This study explores the explainability capabilities of large language models
(LLMs), when employed to autonomously generate machine learning (ML) solutions.
We examine two classification tasks: (i) a binary classification problem
focused on predicting driver alertness states, and (ii) a multilabel
classification problem based on the yeast dataset. Three state-of-the-art LLMs
(i.e. OpenAI GPT, Anthropic Claude, and DeepSeek) are prompted to design
training pipelines for four common classifiers: Random Forest, XGBoost,
Multilayer Perceptron, and Long Short-Term Memory networks. The generated
models are evaluated in terms of predictive performance (recall, precision, and
F1-score) and explainability using SHAP (SHapley Additive exPlanations).
Specifically, we measure Average SHAP Fidelity (Mean Squared Error between SHAP
approximations and model outputs) and Average SHAP Sparsity (number of features
deemed influential). The results reveal that LLMs are capable of producing
effective and interpretable models, achieving high fidelity and consistent
sparsity, highlighting their potential as automated tools for interpretable ML
pipeline generation. The results show that LLMs can produce effective,
interpretable pipelines with high fidelity and consistent sparsity, closely
matching manually engineered baselines.

</details>


### [125] [DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed Multi-Agent Generative Adversarial Imitation Learning](https://arxiv.org/abs/2510.06913)
*Ke Guo,Haochen Liu,Xiaojun Wu,Chen Lv*

Main category: cs.LG

TL;DR: 现有模仿学习方法难以模拟真实交通行为，提出DecompGAIL方法，在基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法（行为克隆、GAIL）在模拟真实交通行为时存在问题，GAIL在多智能体场景不稳定。

Method: 提出Decomposed Multi - agent GAIL (DecompGAIL)，将现实性分解为自我地图和自我邻居组件，过滤误导性交互；引入社交PPO目标增强奖励。

Result: 集成到轻量级SMART骨干网络后，在WOMD Sim Agents 2025基准测试中达到了SOTA性能。

Conclusion: DecompGAIL能有效解决现有模仿学习方法在模拟真实交通行为时的问题。

Abstract: Realistic traffic simulation is critical for the development of autonomous
driving systems and urban mobility planning, yet existing imitation learning
approaches often fail to model realistic traffic behaviors. Behavior cloning
suffers from covariate shift, while Generative Adversarial Imitation Learning
(GAIL) is notoriously unstable in multi-agent settings. We identify a key
source of this instability: irrelevant interaction misguidance, where a
discriminator penalizes an ego vehicle's realistic behavior due to unrealistic
interactions among its neighbors. To address this, we propose Decomposed
Multi-agent GAIL (DecompGAIL), which explicitly decomposes realism into ego-map
and ego-neighbor components, filtering out misleading neighbor: neighbor and
neighbor: map interactions. We further introduce a social PPO objective that
augments ego rewards with distance-weighted neighborhood rewards, encouraging
overall realism across agents. Integrated into a lightweight SMART-based
backbone, DecompGAIL achieves state-of-the-art performance on the WOMD Sim
Agents 2025 benchmark.

</details>


### [126] [Revisiting Node Affinity Prediction in Temporal Graphs](https://arxiv.org/abs/2510.06940)
*Krishna Sri Ipsit Mantri,Or Feldman,Moshe Eliasof,Chaim Baskin*

Main category: cs.LG

TL;DR: 分析当前时间图神经网络在节点亲和性预测中的挑战并提出解决方案，开发NAViS模型，引入新损失函数，评估显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前适配的动态链接属性预测模型在节点亲和性预测上不如简单启发式方法，需解决训练时间图神经网络的挑战。

Method: 分析挑战并提出解决方案，利用启发式方法和状态空间模型的等价性开发NAViS模型，引入新的损失函数。

Result: 在TGB上评估显示NAViS优于现有方法，包括启发式方法。

Conclusion: 所提出的NAViS模型和新损失函数在节点亲和性预测任务中有效。

Abstract: Node affinity prediction is a common task that is widely used in temporal
graph learning with applications in social and financial networks, recommender
systems, and more. Recent works have addressed this task by adapting
state-of-the-art dynamic link property prediction models to node affinity
prediction. However, simple heuristics, such as Persistent Forecast or Moving
Average, outperform these models. In this work, we analyze the challenges in
training current Temporal Graph Neural Networks for node affinity prediction
and suggest appropriate solutions. Combining the solutions, we develop NAViS -
Node Affinity prediction model using Virtual State, by exploiting the
equivalence between heuristics and state space models. While promising,
training NAViS is non-trivial. Therefore, we further introduce a novel loss
function for node affinity prediction. We evaluate NAViS on TGB and show that
it outperforms the state-of-the-art, including heuristics. Our source code is
available at https://github.com/orfeld415/NAVIS

</details>


### [127] [Fisher Information, Training and Bias in Fourier Regression Models](https://arxiv.org/abs/2510.06945)
*Lorenzo Pastori,Veronika Eyring,Mierk Schwabe*

Main category: cs.LG

TL;DR: 研究基于Fisher信息矩阵的评估指标对量子神经网络训练和预测性能的有效性，分析有效维度和偏差对模型的影响。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习，特别是量子神经网络的研究热度上升，探究相关评估指标的有效性。

Method: 利用量子神经网络与傅里叶模型的等价性，推导傅里叶模型FIM的解析表达式，构建可调节有效维度和偏差的模型并比较训练情况，引入张量网络表示。

Result: 无偏模型有效维度高可能训练和性能更好，有偏模型低有效维度在训练中可能更有利。

Conclusion: 研究展示了几何性质、模型 - 任务对齐和训练之间的相互作用，对机器学习社区有参考价值。

Abstract: Motivated by the growing interest in quantum machine learning, in particular
quantum neural networks (QNNs), we study how recently introduced evaluation
metrics based on the Fisher information matrix (FIM) are effective for
predicting their training and prediction performance. We exploit the
equivalence between a broad class of QNNs and Fourier models, and study the
interplay between the \emph{effective dimension} and the \emph{bias} of a model
towards a given task, investigating how these affect the model's training and
performance. We show that for a model that is completely agnostic, or unbiased,
towards the function to be learned, a higher effective dimension likely results
in a better trainability and performance. On the other hand, for models that
are biased towards the function to be learned a lower effective dimension is
likely beneficial during training. To obtain these results, we derive an
analytical expression of the FIM for Fourier models and identify the features
controlling a model's effective dimension. This allows us to construct models
with tunable effective dimension and bias, and to compare their training. We
furthermore introduce a tensor network representation of the considered Fourier
models, which could be a tool of independent interest for the analysis of QNN
models. Overall, these findings provide an explicit example of the interplay
between geometrical properties, model-task alignment and training, which are
relevant for the broader machine learning community.

</details>


### [128] [Grouped Differential Attention](https://arxiv.org/abs/2510.06949)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Wai Ting Cheung,Beomgyu Kim,Taehwan Kim,Haesol Lee,Junhyeok Lee,Dongpin Oh,Eunhwan Park*

Main category: cs.LG

TL;DR: 提出分组差分注意力机制(GDA)，通过不平衡头分配增强信号聚焦，扩展到组差异化增长策略，实验证明其提升泛化性和稳定性，为设计高效Transformer架构提供途径。


<details>
  <summary>Details</summary>
Motivation: 解决自注意力机制常关注冗余或噪声上下文问题，以及差分注意力平衡头分配对表示灵活性和可扩展性的限制。

Method: 提出GDA，在信号保留和噪声控制组采用不平衡头分配，更多头用于信号提取，更少头用于噪声控制并通过控制重复稳定；扩展到组差异化增长策略，选择性复制信号聚焦头。

Result: 大规模预训练和持续训练实验表明，GDA适度不平衡比率相比对称基线在泛化性和稳定性上有显著提升。

Conclusion: 比率感知的头分配和选择性扩展为设计可扩展、计算高效的Transformer架构提供了有效且实用的方法。

Abstract: The self-attention mechanism, while foundational to modern Transformer
architectures, suffers from a critical inefficiency: it frequently allocates
substantial attention to redundant or noisy context. Differential Attention
addressed this by using subtractive attention maps for signal and noise, but
its required balanced head allocation imposes rigid constraints on
representational flexibility and scalability.
  To overcome this, we propose Grouped Differential Attention (GDA), a novel
approach that introduces unbalanced head allocation between signal-preserving
and noise-control groups. GDA significantly enhances signal focus by
strategically assigning more heads to signal extraction and fewer to
noise-control, stabilizing the latter through controlled repetition (akin to
GQA). This design achieves stronger signal fidelity with minimal computational
overhead. We further extend this principle to group-differentiated growth, a
scalable strategy that selectively replicates only the signal-focused heads,
thereby ensuring efficient capacity expansion.
  Through large-scale pretraining and continual training experiments, we
demonstrate that moderate imbalance ratios in GDA yield substantial
improvements in generalization and stability compared to symmetric baselines.
Our results collectively establish that ratio-aware head allocation and
selective expansion offer an effective and practical path toward designing
scalable, computation-efficient Transformer architectures.

</details>


### [129] [From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics](https://arxiv.org/abs/2510.06954)
*Zheng-An Chen,Tao Luo*

Main category: cs.LG

TL;DR: 用梯度流分析框架研究线性化Transformer训练动态，提出两阶段框架概括经典定向收敛结果。


<details>
  <summary>Details</summary>
Motivation: 现有对Transformer模型训练动态基本原理的研究不足，受小初始化尺度能提升语言模型推理能力的启发进行研究。

Method: 采用[Zhou et al. NeurIPS 2022]建立的梯度流分析框架研究线性化Transformer训练动态。

Result: 理论分析将注意力模块动态分为两个阶段，第一阶段参数矩阵摆脱小初始化状态后凝聚，第二阶段键 - 查询矩阵参与训练使归一化矩阵趋于秩坍缩。

Conclusion: 提出的两阶段框架概括了经典定向收敛结果。

Abstract: Although transformer-based models have shown exceptional empirical
performance, the fundamental principles governing their training dynamics are
inadequately characterized beyond configuration-specific studies. Inspired by
empirical evidence showing improved reasoning capabilities under small
initialization scales in language models, we employ the gradient flow
analytical framework established in [Zhou et al. NeurIPS 2022] to
systematically investigate linearized Transformer training dynamics. Our
theoretical analysis dissects the dynamics of attention modules into two
distinct stages. In the first stage, asymmetric weight perturbations from
random initialization sustain non-degenerate gradient dynamics in parameter
matrices, facilitating systematic escape from small initialization regimes.
Subsequently, these matrices undergo condensation, progressively aligning
toward the target orientation. In the second stage, the previously static
key-query matrices actively participate in training, driving the normalized
matrices toward asymptotic rank collapse. This two-stage framework generalizes
classical directional convergence results.

</details>


### [130] [High-Rate Mixout: Revisiting Mixout for Robust Domain Generalization](https://arxiv.org/abs/2510.06955)
*Masih Aminbeidokhti,Heitor Rapela Medeiros,Eric Granger,Marco Pedersoli*

Main category: cs.LG

TL;DR: 研究Mixout用于领域泛化，发现高掩码率Mixout在泛化基准上表现好，能降低计算开销，效果与集成方法相当。


<details>
  <summary>Details</summary>
Motivation: 集成微调模型计算成本高，Dropout用于预训练模型会过正则化，需新方法用于领域泛化。

Method: 研究Mixout，在训练时概率性地用预训练权重替换微调权重，采用高掩码率（ViTs为0.9，ResNets为0.8）。

Result: 高掩码率Mixout在五个领域泛化基准上实验显示，域外准确率与集成方法相当，能大幅降低训练成本。

Conclusion: Mixout（高率Mixout）可作为Dropout替代方案用于领域泛化，在保证效果的同时降低计算成本。

Abstract: Ensembling fine-tuned models initialized from powerful pre-trained weights is
a common strategy to improve robustness under distribution shifts, but it comes
with substantial computational costs due to the need to train and store
multiple models. Dropout offers a lightweight alternative by simulating
ensembles through random neuron deactivation; however, when applied to
pre-trained models, it tends to over-regularize and disrupt critical
representations necessary for generalization. In this work, we investigate
Mixout, a stochastic regularization technique that provides an alternative to
Dropout for domain generalization. Rather than deactivating neurons, Mixout
mitigates overfitting by probabilistically swapping a subset of fine-tuned
weights with their pre-trained counterparts during training, thereby
maintaining a balance between adaptation and retention of prior knowledge. Our
study reveals that achieving strong performance with Mixout on domain
generalization benchmarks requires a notably high masking probability of 0.9
for ViTs and 0.8 for ResNets. While this may seem like a simple adjustment, it
yields two key advantages for domain generalization: (1) higher masking rates
more strongly penalize deviations from the pre-trained parameters, promoting
better generalization to unseen domains; and (2) high-rate masking
substantially reduces computational overhead, cutting gradient computation by
up to 45% and gradient memory usage by up to 90%. Experiments across five
domain generalization benchmarks, PACS, VLCS, OfficeHome, TerraIncognita, and
DomainNet, using ResNet and ViT architectures, show that our approach,
High-rate Mixout, achieves out-of-domain accuracy comparable to ensemble-based
methods while significantly reducing training costs.

</details>


### [131] [Revisiting Mixout: An Overlooked Path to Robust Finetuning](https://arxiv.org/abs/2510.06982)
*Masih Aminbeidokhti,Heitor Rapela Medeiros,Eric Granger,Marco Pedersoli*

Main category: cs.LG

TL;DR: 本文提出GMixout方法，通过调整Mixout的关键因素提升模型在分布偏移下的鲁棒性和准确率。


<details>
  <summary>Details</summary>
Motivation: 微调视觉基础模型虽能提升领域内准确率，但会降低分布偏移下的鲁棒性，需改进方法。

Method: 从单运行、权重共享的隐式集成角度重新审视Mixout，引入GMixout，采用指数移动平均快照替换固定锚点，通过显式重采样频率超参数调节掩码周期，使用稀疏核实现。

Result: 在多个基准测试中，GMixout持续提升领域内准确率，超越零样本性能，在分布偏移下优于Model Soups和强参数高效微调基线。

Conclusion: GMixout方法有效提升了模型在分布偏移下的鲁棒性和准确率。

Abstract: Finetuning vision foundation models often improves in-domain accuracy but
comes at the cost of robustness under distribution shift. We revisit Mixout, a
stochastic regularizer that intermittently replaces finetuned weights with
their pretrained reference, through the lens of a single-run, weight-sharing
implicit ensemble. This perspective reveals three key levers that govern
robustness: the \emph{masking anchor}, \emph{resampling frequency}, and
\emph{mask sparsity}. Guided by this analysis, we introduce GMixout, which (i)
replaces the fixed anchor with an exponential moving-average snapshot that
adapts during training, and (ii) regulates masking period via an explicit
resampling-frequency hyperparameter. Our sparse-kernel implementation updates
only a small fraction of parameters with no inference-time overhead, enabling
training on consumer-grade GPUs. Experiments on benchmarks covering covariate
shift, corruption, and class imbalance, ImageNet / ImageNet-LT, DomainNet,
iWildCam, and CIFAR100-C, GMixout consistently improves in-domain accuracy
beyond zero-shot performance while surpassing both Model Soups and strong
parameter-efficient finetuning baselines under distribution shift.

</details>


### [132] [Sharpness-Aware Data Generation for Zero-shot Quantization](https://arxiv.org/abs/2510.07018)
*Dung Hoang-Anh,Cuong Pham Trung Le,Jianfei Cai,Thanh-Toan Do*

Main category: cs.LG

TL;DR: 本文提出考虑量化模型锐度的零样本量化新方法，在CIFAR - 100和ImageNet数据集上验证其优势。


<details>
  <summary>Details</summary>
Motivation: 以往零样本量化工作未将量化模型锐度作为生成训练数据的标准，而低锐度模型泛化能力更好，因此需考虑锐度来增强泛化。

Method: 在合成数据生成中考虑量化模型锐度；在一定假设下，通过最大化合成数据和真实验证数据上重建损失梯度的匹配来最小化锐度；用生成样本与其邻居的梯度匹配近似替代无真实验证集时的梯度匹配。

Result: 在CIFAR - 100和ImageNet数据集的低比特量化设置下，所提方法优于现有技术。

Conclusion: 所提考虑量化模型锐度的方法在零样本量化中有效，能提升泛化能力。

Abstract: Zero-shot quantization aims to learn a quantized model from a pre-trained
full-precision model with no access to original real training data. The common
idea in zero-shot quantization approaches is to generate synthetic data for
quantizing the full-precision model. While it is well-known that deep neural
networks with low sharpness have better generalization ability, none of the
previous zero-shot quantization works considers the sharpness of the quantized
model as a criterion for generating training data. This paper introduces a
novel methodology that takes into account quantized model sharpness in
synthetic data generation to enhance generalization. Specifically, we first
demonstrate that sharpness minimization can be attained by maximizing gradient
matching between the reconstruction loss gradients computed on synthetic and
real validation data, under certain assumptions. We then circumvent the problem
of the gradient matching without real validation set by approximating it with
the gradient matching between each generated sample and its neighbors.
Experimental evaluations on CIFAR-100 and ImageNet datasets demonstrate the
superiority of the proposed method over the state-of-the-art techniques in
low-bit quantization settings.

</details>


### [133] [Federated Unlearning in the Wild: Rethinking Fairness and Data Discrepancy](https://arxiv.org/abs/2510.07022)
*ZiHeng Huang,Di Wu,Jun Bai,Jiale Zhang,Sicong Cao,Ji Zhang,Yingjie Hu*

Main category: cs.LG

TL;DR: 本文指出联邦学习中无学习面临公平性和评估数据不真实的挑战，进行基准测试并提出FedCCCU方法，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决联邦无学习中公平性常被忽视以及评估依赖不真实合成数据的问题。

Method: 先对现有联邦无学习方法在现实数据异质性和公平条件下进行综合基准测试，再提出公平感知的FedCCCU方法。

Result: 现有方法在现实设置中表现不佳，FedCCCU方法始终优于它们。

Conclusion: FedCCCU为现实世界的联邦无学习提供了实用且可扩展的解决方案。

Abstract: Machine unlearning is critical for enforcing data deletion rights like the
"right to be forgotten." As a decentralized paradigm, Federated Learning (FL)
also requires unlearning, but realistic implementations face two major
challenges. First, fairness in Federated Unlearning (FU) is often overlooked.
Exact unlearning methods typically force all clients into costly retraining,
even those uninvolved. Approximate approaches, using gradient ascent or
distillation, make coarse interventions that can unfairly degrade performance
for clients with only retained data. Second, most FU evaluations rely on
synthetic data assumptions (IID/non-IID) that ignore real-world heterogeneity.
These unrealistic benchmarks obscure the true impact of unlearning and limit
the applicability of current methods. We first conduct a comprehensive
benchmark of existing FU methods under realistic data heterogeneity and
fairness conditions. We then propose a novel, fairness-aware FU approach,
Federated Cross-Client-Constrains Unlearning (FedCCCU), to explicitly address
both challenges. FedCCCU offers a practical and scalable solution for
real-world FU. Experimental results show that existing methods perform poorly
in realistic settings, while our approach consistently outperforms them.

</details>


### [134] [Unified Molecule Pre-training with Flexible 2D and 3D Modalities: Single and Paired Modality Integration](https://arxiv.org/abs/2510.07035)
*Tengwei Song,Min Wu,Yuan Fang*

Main category: cs.LG

TL;DR: 提出FlexMol框架解决现有分子表征学习方法对配对数据的依赖问题，在多任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有分子表征学习方法需配对2D和3D数据，在特定模态缺失或生成成本高时受限，需改进。

Method: 提出FlexMol框架，用单独模型处理2D和3D数据，参数共享提高效率，用解码器生成缺失模态特征，实现多阶段连续学习。

Result: FlexMol在多种分子属性预测任务中表现出色，对不完整数据也有效。

Conclusion: FlexMol是有效的分子预训练框架，支持单模态输入，能解决现有方法局限。

Abstract: Molecular representation learning plays a crucial role in advancing
applications such as drug discovery and material design. Existing work
leverages 2D and 3D modalities of molecular information for pre-training,
aiming to capture comprehensive structural and geometric insights. However,
these methods require paired 2D and 3D molecular data to train the model
effectively and prevent it from collapsing into a single modality, posing
limitations in scenarios where a certain modality is unavailable or
computationally expensive to generate. To overcome this limitation, we propose
FlexMol, a flexible molecule pre-training framework that learns unified
molecular representations while supporting single-modality input. Specifically,
inspired by the unified structure in vision-language models, our approach
employs separate models for 2D and 3D molecular data, leverages parameter
sharing to improve computational efficiency, and utilizes a decoder to generate
features for the missing modality. This enables a multistage continuous
learning process where both modalities contribute collaboratively during
training, while ensuring robustness when only one modality is available during
inference. Extensive experiments demonstrate that FlexMol achieves superior
performance across a wide range of molecular property prediction tasks, and we
also empirically demonstrate its effectiveness with incomplete data. Our code
and data are available at https://github.com/tewiSong/FlexMol.

</details>


### [135] [COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference Optimization](https://arxiv.org/abs/2510.07043)
*Tian Qin,Felix Bai,Ting-Yao Hu,Raviteja Vemulapalli,Hema Swetha Koppula,Zhiyang Xu,Bowen Jin,Mert Cemri,Jiarui Lu,Zirui Wang,Meng Cao*

Main category: cs.LG

TL;DR: 提出COMPASS基准，用于评估大语言模型代理在旅行规划场景中的表现，发现现有模型存在两个关键差距。


<details>
  <summary>Details</summary>
Motivation: 让现实世界的大语言模型代理通过多轮交互掌握战略工具使用和用户偏好优化，以协助用户完成复杂规划任务。

Method: 将旅行规划视为受限偏好优化问题，构建涵盖20个美国国家公园交通、住宿和票务的旅行数据库及综合工具生态系统。

Result: 评估最先进模型时发现可接受 - 最优差距和计划协调差距。

Conclusion: COMPASS基准能直接衡量代理在现实任务中优化用户偏好的能力，弥合理论进展与现实影响的差距。

Abstract: Real-world large language model (LLM) agents must master strategic tool use
and user preference optimization through multi-turn interactions to assist
users with complex planning tasks. We introduce COMPASS (Constrained
Optimization through Multi-turn Planning and Strategic Solutions), a benchmark
that evaluates agents on realistic travel-planning scenarios. We cast travel
planning as a constrained preference optimization problem, where agents must
satisfy hard constraints while simultaneously optimizing soft user preferences.
To support this, we build a realistic travel database covering transportation,
accommodation, and ticketing for 20 U.S. National Parks, along with a
comprehensive tool ecosystem that mirrors commercial booking platforms.
Evaluating state-of-the-art models, we uncover two critical gaps: (i) an
acceptable-optimal gap, where agents reliably meet constraints but fail to
optimize preferences, and (ii) a plan-coordination gap, where performance
collapses on multi-service (flight and hotel) coordination tasks, especially
for open-source models. By grounding reasoning and planning in a practical,
user-facing domain, COMPASS provides a benchmark that directly measures an
agent's ability to optimize user preferences in realistic tasks, bridging
theoretical advances with real-world impact.

</details>


### [136] [Enhancing Speech Emotion Recognition via Fine-Tuning Pre-Trained Models and Hyper-Parameter Optimisation](https://arxiv.org/abs/2510.07052)
*Aryan Golbaghi,Shuo Zhou*

Main category: cs.LG

TL;DR: 提出结合预训练表征和自动超参数优化的语音情感识别工作流，对比两种HPO策略，结果显示高效HPO结合预训练编码器在普通CPU上有竞争力。


<details>
  <summary>Details</summary>
Motivation: 提升语音情感识别（SER）的性能，探索高效的超参数优化方法。

Method: 使用在IEMOCAP上微调的SpeechBrain wav2vec2 - base模型作为编码器，对比高斯过程贝叶斯优化（GP - BO）和树结构Parzen估计器（TPE）两种HPO策略。

Result: GP - BO在11分钟内达到0.96的BCA，TPE在15分钟内达到0.97；跨语言泛化中，经EmoDB训练和HPO调优的模型在CREMA - D和RAVDESS上提升零样本准确率。

Conclusion: 高效HPO结合预训练编码器能在普通CPU上实现有竞争力的SER。

Abstract: We propose a workflow for speech emotion recognition (SER) that combines
pre-trained representations with automated hyperparameter optimisation (HPO).
Using SpeechBrain wav2vec2-base model fine-tuned on IEMOCAP as the encoder, we
compare two HPO strategies, Gaussian Process Bayesian Optimisation (GP-BO) and
Tree-structured Parzen Estimators (TPE), under an identical four-dimensional
search space and 15-trial budget, with balanced class accuracy (BCA) on the
German EmoDB corpus as the objective. All experiments run on 8 CPU cores with
32 GB RAM. GP-BO achieves 0.96 BCA in 11 minutes, and TPE (Hyperopt
implementation) attains 0.97 in 15 minutes. In contrast, grid search requires
143 trials and 1,680 minutes to exceed 0.9 BCA, and the best AutoSpeech 2020
baseline reports only 0.85 in 30 minutes on GPU. For cross-lingual
generalisation, an EmoDB-trained HPO-tuned model improves zero-shot accuracy by
0.25 on CREMA-D and 0.26 on RAVDESS. Results show that efficient HPO with
pre-trained encoders delivers competitive SER on commodity CPUs. Source code to
this work is available at:
https://github.com/youngaryan/speechbrain-emotion-hpo.

</details>


### [137] [Introspection in Learned Semantic Scene Graph Localisation](https://arxiv.org/abs/2510.07053)
*Manshika Charvi Bissessur,Efimia Panagiotaki,Daniele De Martini*

Main category: cs.LG

TL;DR: 研究语义对自监督对比语义定位框架性能和鲁棒性的影响，验证可解释性方法，揭示模型学习特性。


<details>
  <summary>Details</summary>
Motivation: 探究语义如何影响自监督对比语义定位框架的定位性能和鲁棒性。

Method: 在原始和扰动地图上训练定位网络，进行事后内省分析，验证多种可解释性方法并进行可靠性对比分析，开展语义类消融。

Result: 集成梯度和注意力权重是最可靠的探测方法，语义类消融揭示对频繁对象的隐式降权。

Conclusion: 模型学习到对噪声鲁棒、语义显著的位置关系，可在视觉和结构变化挑战下实现可解释配准。

Abstract: This work investigates how semantics influence localisation performance and
robustness in a learned self-supervised, contrastive semantic localisation
framework. After training a localisation network on both original and perturbed
maps, we conduct a thorough post-hoc introspection analysis to probe whether
the model filters environmental noise and prioritises distinctive landmarks
over routine clutter. We validate various interpretability methods and present
a comparative reliability analysis. Integrated gradients and Attention Weights
consistently emerge as the most reliable probes of learned behaviour. A
semantic class ablation further reveals an implicit weighting in which frequent
objects are often down-weighted. Overall, the results indicate that the model
learns noise-robust, semantically salient relations about place definition,
thereby enabling explainable registration under challenging visual and
structural variations.

</details>


### [138] [Blind Construction of Angular Power Maps in Massive MIMO Networks](https://arxiv.org/abs/2510.07071)
*Zheng Xing,Junting Chen*

Main category: cs.LG

TL;DR: 本文研究无位置标签下基于大规模MIMO网络大尺度CSI数据的无监督角功率图构建，建HMM模型估计移动位置，给出定位误差情况并结合实际数据实现一定定位精度。


<details>
  <summary>Details</summary>
Motivation: 传统无线电地图构建需带位置标签的CSI数据，实际中较难获取，因此研究无位置标签下的角功率图构建。

Method: 构建隐藏马尔可夫模型（HMM）连接移动台的隐藏轨迹和大规模MIMO信道的CSI演变来估计移动位置。

Result: 在均匀直线移动且基站呈泊松分布时，定位误差的克拉美 - 罗下界（CRLB）在任意信噪比下可消失；基站受限在有限区域时，即使有无限独立测量，误差仍不为零。结合实际多小区大规模MIMO网络的参考信号接收功率（RSRP）数据，平均定位误差达18米。

Conclusion: 通过构建HMM模型可在无位置标签的情况下实现角功率图构建和一定精度的移动台定位。

Abstract: Channel state information (CSI) acquisition is a challenging problem in
massive multiple-input multiple-output (MIMO) networks. Radio maps provide a
promising solution for radio resource management by reducing online CSI
acquisition. However, conventional approaches for radio map construction
require location-labeled CSI data, which is challenging in practice. This paper
investigates unsupervised angular power map construction based on large
timescale CSI data collected in a massive MIMO network without location labels.
A hidden Markov model (HMM) is built to connect the hidden trajectory of a
mobile with the CSI evolution of a massive MIMO channel. As a result, the
mobile location can be estimated, enabling the construction of an angular power
map. We show that under uniform rectilinear mobility with Poisson-distributed
base stations (BSs), the Cramer-Rao Lower Bound (CRLB) for localization error
can vanish at any signal-to-noise ratios (SNRs), whereas when BSs are confined
to a limited region, the error remains nonzero even with infinite independent
measurements. Based on reference signal received power (RSRP) data collected in
a real multi-cell massive MIMO network, an average localization error of 18
meters can be achieved although measurements are mainly obtained from a single
serving cell.

</details>


### [139] [HTMformer: Hybrid Time and Multivariate Transformer for Time Series Forecasting](https://arxiv.org/abs/2510.07084)
*Tan Wang,Yun Wei Dong,Tao Zhang,Qi Wang*

Main category: cs.LG

TL;DR: 现有Transformer在时间序列预测中存在局限，本文提出HTMformer，结合HTME增强特征提取，在多数据集实验中表现优。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer在序列建模中过度强调时间依赖，有计算开销且无对应性能提升，性能依赖嵌入方法。

Method: 提取多元特征增强嵌入层信息，引入HTME，结合Transformer架构得到HTMformer。

Result: 在八个真实数据集实验中，本文方法在准确性和效率上优于现有基线。

Conclusion: 提出的HTMformer能有效提升时间序列预测性能，平衡模型复杂度和性能。

Abstract: Transformer-based methods have achieved impressive results in time series
forecasting. However, existing Transformers still exhibit limitations in
sequence modeling as they tend to overemphasize temporal dependencies. This
incurs additional computational overhead without yielding corresponding
performance gains. We find that the performance of Transformers is highly
dependent on the embedding method used to learn effective representations. To
address this issue, we extract multivariate features to augment the effective
information captured in the embedding layer, yielding multidimensional
embeddings that convey richer and more meaningful sequence representations.
These representations enable Transformer-based forecasters to better understand
the series. Specifically, we introduce Hybrid Temporal and Multivariate
Embeddings (HTME). The HTME extractor integrates a lightweight temporal feature
extraction module with a carefully designed multivariate feature extraction
module to provide complementary features, thereby achieving a balance between
model complexity and performance. By combining HTME with the Transformer
architecture, we present HTMformer, leveraging the enhanced feature extraction
capability of the HTME extractor to build a lightweight forecaster. Experiments
conducted on eight real-world datasets demonstrate that our approach
outperforms existing baselines in both accuracy and efficiency.

</details>


### [140] [Non-Stationary Online Structured Prediction with Surrogate Losses](https://arxiv.org/abs/2510.07086)
*Shinsaku Sakaue,Han Bao,Yuzhou Cao*

Main category: cs.LG

TL;DR: 本文研究非平稳环境下在线结构化预测问题，提出累积目标损失的界，核心思想结合OGD动态遗憾界与代理间隙技术，还拓展到更广泛问题并证明下界。


<details>
  <summary>Details</summary>
Motivation: 现有代理遗憾在非平稳环境下保证失效，需解决非平稳环境下在线结构化预测问题。

Method: 合成在线梯度下降（OGD）动态遗憾界与利用代理间隙的技术，通过卷积Fenchel - Young损失将方法拓展到更广泛问题。

Result: 得到累积目标损失形如$F_T + C(1 + P_T)$的界，提出新的Polyak风格学习率，有良好实证表现，证明依赖$F_T$和$P_T$的下界是紧的。

Conclusion: 提出的方法在非平稳环境下有更强保证，新学习率能提供目标损失保证。

Abstract: Online structured prediction, including online classification as a special
case, is the task of sequentially predicting labels from input features.
Therein the surrogate regret -- the cumulative excess of the target loss (e.g.,
0-1 loss) over the surrogate loss (e.g., logistic loss) of the fixed best
estimator -- has gained attention, particularly because it often admits a
finite bound independent of the time horizon $T$. However, such guarantees
break down in non-stationary environments, where every fixed estimator may
incur the surrogate loss growing linearly with $T$. We address this by proving
a bound of the form $F_T + C(1 + P_T)$ on the cumulative target loss, where
$F_T$ is the cumulative surrogate loss of any comparator sequence, $P_T$ is its
path length, and $C > 0$ is some constant. This bound depends on $T$ only
through $F_T$ and $P_T$, often yielding much stronger guarantees in
non-stationary environments. Our core idea is to synthesize the dynamic regret
bound of the online gradient descent (OGD) with the technique of exploiting the
surrogate gap. Our analysis also sheds light on a new Polyak-style learning
rate for OGD, which systematically offers target-loss guarantees and exhibits
promising empirical performance. We further extend our approach to a broader
class of problems via the convolutional Fenchel--Young loss. Finally, we prove
a lower bound showing that the dependence on $F_T$ and $P_T$ is tight.

</details>


### [141] [Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report](https://arxiv.org/abs/2510.07092)
*Riccardo Mereu,Aidan Scannell,Yuxin Hou,Yi Zhao,Aditya Jitta,Antonio Dominguez,Luigi Acerbi,Amos Storkey,Paul Chang*

Main category: cs.LG

TL;DR: 本文介绍1X世界模型挑战赛，含采样和压缩两个赛道，分别采用不同方法训练模型，在两赛道均获第一名。


<details>
  <summary>Details</summary>
Motivation: 引入真实世界人形交互的开源基准测试，推动世界模型在AI和机器人领域发展。

Method: 采样赛道：适配Wan - 2.2 TI2V - 5B模型，用AdaLN - Zero结合机器人状态，用LoRA微调；压缩赛道：从头训练时空Transformer模型。

Result: 采样任务PSNR达23.0 dB，压缩任务Top - 500 CE为6.6386，两赛道均获第一。

Conclusion: 所采用方法在1X世界模型挑战赛中效果显著，可有效解决未来帧预测和离散潜在代码预测问题。

Abstract: World models are a powerful paradigm in AI and robotics, enabling agents to
reason about the future by predicting visual observations or compact latent
states. The 1X World Model Challenge introduces an open-source benchmark of
real-world humanoid interaction, with two complementary tracks: sampling,
focused on forecasting future image frames, and compression, focused on
predicting future discrete latent codes. For the sampling track, we adapt the
video generation foundation model Wan-2.2 TI2V-5B to video-state-conditioned
future frame prediction. We condition the video generation on robot states
using AdaLN-Zero, and further post-train the model using LoRA. For the
compression track, we train a Spatio-Temporal Transformer model from scratch.
Our models achieve 23.0 dB PSNR in the sampling task and a Top-500 CE of 6.6386
in the compression task, securing 1st place in both challenges.

</details>


### [142] [ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL](https://arxiv.org/abs/2510.07151)
*Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 提出ELMUR架构解决部分可观测性下长视野决策问题，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 现实机器人在部分可观测和长视野下行动，现代方法多依赖即时信息，标准模型难处理长期依赖。

Method: 提出ELMUR架构，各层维护内存嵌入，通过双向交叉注意力交互，用LRU模块更新。

Result: ELMUR将有效视野扩展到关注窗口的10万倍，T - Maze任务成功率100%，在POPGym超半数任务中优于基线，在MIKASA - Robo任务中几乎使强基线性能翻倍。

Conclusion: 结构化、层局部的外部内存为部分可观测下的决策提供简单可扩展的方法。

Abstract: Real-world robotic agents must act under partial observability and long
horizons, where key cues may appear long before they affect decision making.
However, most modern approaches rely solely on instantaneous information,
without incorporating insights from the past. Standard recurrent or transformer
models struggle with retaining and leveraging long-term dependencies: context
windows truncate history, while naive memory extensions fail under scale and
sparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a
transformer architecture with structured external memory. Each layer maintains
memory embeddings, interacts with them via bidirectional cross-attention, and
updates them through an Least Recently Used (LRU) memory module using
replacement or convex blending. ELMUR extends effective horizons up to 100,000
times beyond the attention window and achieves a 100% success rate on a
synthetic T-Maze task with corridors up to one million steps. In POPGym, it
outperforms baselines on more than half of the tasks. On MIKASA-Robo
sparse-reward manipulation tasks with visual observations, it nearly doubles
the performance of strong baselines. These results demonstrate that structured,
layer-local external memory offers a simple and scalable approach to decision
making under partial observability.

</details>


### [143] [Bridged Clustering for Representation Learning: Semi-Supervised Sparse Bridging](https://arxiv.org/abs/2510.07182)
*Patrick Peixuan Ye,Chen Shani,Ellen Vitercik*

Main category: cs.LG

TL;DR: 介绍了Bridged Clustering半监督框架，可从无配对输入输出数据集学习预测器，理论和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 开发一种能从无配对输入输出数据集学习预测器的半监督框架，且能显式利用仅输出数据，保持稀疏可解释对齐。

Method: 先独立对输入X和输出Y聚类，用少量配对示例学习簇间稀疏可解释桥梁，推理时将新输入分配到最近输入簇并返回关联输出簇质心作为预测。

Result: 理论上在有界的错误聚类和错误搭桥率下算法有效高效；实验上在低监督设置下与SOTA方法有竞争力，且简单、模型无关、标签效率高。

Conclusion: Bridged Clustering是一种有效、简单、标签高效的半监督学习框架。

Abstract: We introduce Bridged Clustering, a semi-supervised framework to learn
predictors from any unpaired input $X$ and output $Y$ dataset. Our method first
clusters $X$ and $Y$ independently, then learns a sparse, interpretable bridge
between clusters using only a few paired examples. At inference, a new input
$x$ is assigned to its nearest input cluster, and the centroid of the linked
output cluster is returned as the prediction $\hat{y}$. Unlike traditional SSL,
Bridged Clustering explicitly leverages output-only data, and unlike dense
transport-based methods, it maintains a sparse and interpretable alignment.
Through theoretical analysis, we show that with bounded mis-clustering and
mis-bridging rates, our algorithm becomes an effective and efficient predictor.
Empirically, our method is competitive with SOTA methods while remaining
simple, model-agnostic, and highly label-efficient in low-supervision settings.

</details>


### [144] [Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples](https://arxiv.org/abs/2510.07192)
*Alexandra Souly,Javier Rando,Ed Chapman,Xander Davies,Burak Hasircioglu,Ezzeldin Shereen,Carlos Mougan,Vasilios Mavroudis,Erik Jones,Chris Hicks,Nicholas Carlini,Yarin Gal,Robert Kirk*

Main category: cs.LG

TL;DR: 研究表明大语言模型中毒攻击所需恶意文档数量与数据集大小无关，凸显防御研究的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设攻击者控制一定比例训练语料，对大模型而言不现实，需探究攻击所需文档数量与数据集大小的关系。

Method: 进行了从600M到13B参数模型在不同规模数据集上的预训练中毒实验，还进行小范围实验分析影响攻击成功的因素，并在微调阶段验证。

Result: 发现250个中毒文档能在不同模型和数据集规模下造成类似危害，微调阶段也有相同情况。

Conclusion: 通过数据中毒注入后门对大模型可能比以往认为的更容易，需要更多防御研究来降低风险。

Abstract: Poisoning attacks can compromise the safety of large language models (LLMs)
by injecting malicious documents into their training data. Existing work has
studied pretraining poisoning assuming adversaries control a percentage of the
training corpus. However, for large models, even small percentages translate to
impractically large amounts of data. This work demonstrates for the first time
that poisoning attacks instead require a near-constant number of documents
regardless of dataset size. We conduct the largest pretraining poisoning
experiments to date, pretraining models from 600M to 13B parameters on
chinchilla-optimal datasets (6B to 260B tokens). We find that 250 poisoned
documents similarly compromise models across all model and dataset sizes,
despite the largest models training on more than 20 times more clean data. We
also run smaller-scale experiments to ablate factors that could influence
attack success, including broader ratios of poisoned to clean data and
non-random distributions of poisoned samples. Finally, we demonstrate the same
dynamics for poisoning during fine-tuning. Altogether, our results suggest that
injecting backdoors through data poisoning may be easier for large models than
previously believed as the number of poisons required does not scale up with
model size, highlighting the need for more research on defences to mitigate
this risk in future models.

</details>


### [145] [An in-depth look at approximation via deep and narrow neural networks](https://arxiv.org/abs/2510.07202)
*Joris Dommel,Sven A. Wegner*

Main category: cs.LG

TL;DR: 本文围绕Hanin和Sellke结论中的阈值情况，用神经网络逼近反例函数，研究深度对逼近质量的影响及原因。


<details>
  <summary>Details</summary>
Motivation: Hanin和Sellke证明宽度w>n时，特定神经网络在连续函数空间中稠密，本文聚焦w=n和w=n+1的阈值情况，研究逼近问题。

Method: 用神经网络逼近反例函数f，研究不同深度下逼近质量的变化。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: In 2017, Hanin and Sellke showed that the class of arbitrarily deep,
real-valued, feed-forward and ReLU-activated networks of width w forms a dense
subset of the space of continuous functions on R^n, with respect to the
topology of uniform convergence on compact sets, if and only if w>n holds. To
show the necessity, a concrete counterexample function f:R^n->R was used. In
this note we actually approximate this very f by neural networks in the two
cases w=n and w=n+1 around the aforementioned threshold. We study how the
approximation quality behaves if we vary the depth and what effect (spoiler
alert: dying neurons) cause that behavior.

</details>


### [146] [Guided by the Experts: Provable Feature Learning Dynamic of Soft-Routed Mixture-of-Experts](https://arxiv.org/abs/2510.07205)
*Fangshuo Liao,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: 本文在师生框架下为非线性路由和专家的软路由MoE模型联合训练提供收敛保证，证明学生网络可恢复教师参数，且剪枝和微调能达全局最优。


<details>
  <summary>Details</summary>
Motivation: 现有对MoE训练动态的理论理解局限于单独的专家 - 路由优化或特定数据集的top - 1路由场景，需推进MoE理论。

Method: 在师生框架下对软路由MoE模型进行联合训练分析，证明学生网络参数恢复过程，提出后训练剪枝和微调。

Result: 证明在适度过参数化下，学生网络能恢复教师参数，剪枝后微调可收敛到全局最优。

Conclusion: 本分析为理解MoE架构的优化格局带来了新见解。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a cornerstone of
modern AI systems. In particular, MoEs route inputs dynamically to specialized
experts whose outputs are aggregated through weighted summation. Despite their
widespread application, theoretical understanding of MoE training dynamics
remains limited to either separate expert-router optimization or only top-1
routing scenarios with carefully constructed datasets. This paper advances MoE
theory by providing convergence guarantees for joint training of soft-routed
MoE models with non-linear routers and experts in a student-teacher framework.
We prove that, with moderate over-parameterization, the student network
undergoes a feature learning phase, where the router's learning process is
``guided'' by the experts, that recovers the teacher's parameters. Moreover, we
show that a post-training pruning can effectively eliminate redundant neurons,
followed by a provably convergent fine-tuning process that reaches global
optimality. To our knowledge, our analysis is the first to bring novel insights
in understanding the optimization landscape of the MoE architecture.

</details>


### [147] [A Broader View of Thompson Sampling](https://arxiv.org/abs/2510.07208)
*Yanlin Qu,Hongseok Namkoong,Assaf Zeevi*

Main category: cs.LG

TL;DR: 本文将汤普森采样重铸为在线优化算法，引入“忠实”平稳化工具，解答其平衡探索与利用的问题。


<details>
  <summary>Details</summary>
Motivation: 大多数多臂老虎机算法中，汤普森采样平衡探索与利用的机制仍是谜团，本文旨在解答该问题。

Method: 将汤普森采样重铸为在线优化算法，引入“忠实”平稳化工具，把有限时域动态优化问题转化为平稳问题，用贝尔曼原理研究。

Result: 汤普森采样呈现出模仿贝尔曼最优策略结构的简单在线优化形式，其贪婪性由基于点双列相关的残余不确定性度量来正则化。

Conclusion: 解答了汤普森采样如何平衡探索与利用的问题，并提供了研究和改进其原始思想的原则性框架。

Abstract: Thompson Sampling is one of the most widely used and studied bandit
algorithms, known for its simple structure, low regret performance, and solid
theoretical guarantees. Yet, in stark contrast to most other families of bandit
algorithms, the exact mechanism through which posterior sampling (as introduced
by Thompson) is able to "properly" balance exploration and exploitation,
remains a mystery. In this paper we show that the core insight to address this
question stems from recasting Thompson Sampling as an online optimization
algorithm. To distill this, a key conceptual tool is introduced, which we refer
to as "faithful" stationarization of the regret formulation. Essentially, the
finite horizon dynamic optimization problem is converted into a stationary
counterpart which "closely resembles" the original objective (in contrast, the
classical infinite horizon discounted formulation, that leads to the Gittins
index, alters the problem and objective in too significant a manner). The newly
crafted time invariant objective can be studied using Bellman's principle which
leads to a time invariant optimal policy. When viewed through this lens,
Thompson Sampling admits a simple online optimization form that mimics the
structure of the Bellman-optimal policy, and where greediness is regularized by
a measure of residual uncertainty based on point-biserial correlation. This
answers the question of how Thompson Sampling balances
exploration-exploitation, and moreover, provides a principled framework to
study and further improve Thompson's original idea.

</details>


### [148] [Discriminative Feature Feedback with General Teacher Classes](https://arxiv.org/abs/2510.07245)
*Omri Bar Oz,Tosca Lechner,Sivan Sabato*

Main category: cs.LG

TL;DR: 研究Discriminative Feature Feedback (DFF)学习协议理论性质，对比经典协议，得到不同设置下错误界结果。


<details>
  <summary>Details</summary>
Motivation: 对DFF学习协议进行系统研究，与经典协议对比，探究其理论性质。

Method: 在通用框架下研究DFF，分析其在可实现和不可实现设置下的最优错误界。

Result: 得到新的结构结果，用新维度概念刻画可实现设置错误界，给出不可实现设置错误上界且证明一般不可改进。

Conclusion: 与在线学习不同，DFF中可实现维度不足以刻画最优不可实现错误界或无悔算法的存在性。

Abstract: We study the theoretical properties of the interactive learning protocol
Discriminative Feature Feedback (DFF) (Dasgupta et al., 2018). The DFF learning
protocol uses feedback in the form of discriminative feature explanations. We
provide the first systematic study of DFF in a general framework that is
comparable to that of classical protocols such as supervised learning and
online learning. We study the optimal mistake bound of DFF in the realizable
and the non-realizable settings, and obtain novel structural results, as well
as insights into the differences between Online Learning and settings with
richer feedback such as DFF. We characterize the mistake bound in the
realizable setting using a new notion of dimension. In the non-realizable
setting, we provide a mistake upper bound and show that it cannot be improved
in general. Our results show that unlike Online Learning, in DFF the realizable
dimension is insufficient to characterize the optimal non-realizable mistake
bound or the existence of no-regret algorithms.

</details>


### [149] [Test-Time Graph Search for Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2510.07257)
*Evgenii Opryshko,Junwei Quan,Claas Voelcker,Yilun Du,Igor Gilitschenski*

Main category: cs.LG

TL;DR: 提出Test - Time Graph Search (TTGS)轻量级规划方法解决离线目标条件强化学习（GCRL）长程决策难题，在OGBench基准测试中提升成功率。


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习（GCRL）长程决策因时间信用分配和误差累积困难，离线设置加剧此问题。

Method: 引入TTGS，接受状态空间距离或成本信号，在数据集状态上构建加权图，进行快速搜索以组装子目标序列供冻结策略执行，基于值的学习器可直接从学习的目标条件值函数推导距离。

Result: 在OGBench基准测试中，TTGS提高了多个基础学习器在具有挑战性的运动任务上的成功率。

Conclusion: 简单的度量引导测试时间规划对离线GCRL有益。

Abstract: Offline goal-conditioned reinforcement learning (GCRL) trains policies that
reach user-specified goals at test time, providing a simple, unsupervised,
domain-agnostic way to extract diverse behaviors from unlabeled, reward-free
datasets. Nonetheless, long-horizon decision making remains difficult for GCRL
agents due to temporal credit assignment and error accumulation, and the
offline setting amplifies these effects. To alleviate this issue, we introduce
Test-Time Graph Search (TTGS), a lightweight planning approach to solve the
GCRL task. TTGS accepts any state-space distance or cost signal, builds a
weighted graph over dataset states, and performs fast search to assemble a
sequence of subgoals that a frozen policy executes. When the base learner is
value-based, the distance is derived directly from the learned goal-conditioned
value function, so no handcrafted metric is needed. TTGS requires no changes to
training, no additional supervision, no online interaction, and no privileged
information, and it runs entirely at inference. On the OGBench benchmark, TTGS
improves success rates of multiple base learners on challenging locomotion
tasks, demonstrating the benefit of simple metric-guided test-time planning for
offline GCRL.

</details>


### [150] [GTCN-G: A Residual Graph-Temporal Fusion Network for Imbalanced Intrusion Detection (Preprint)](https://arxiv.org/abs/2510.07285)
*Tianxiang Xu,Zhichao Wen,Xinyu Zhao,Qi Hu,Yan Li,Chang Liu*

Main category: cs.LG

TL;DR: 本文提出GTCN - G框架解决网络入侵检测中网络威胁复杂和数据类别不平衡问题，实验显示其性能超现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 现代入侵检测系统面临网络威胁复杂和流量数据类别不平衡挑战，现有方法未有效结合GNN和TCN并解决数据不平衡问题。

Method: 提出GTCN - G框架，融合G - TCN提取网络流的分层时间特征，用GCN学习底层图结构，通过GAT实现残差学习机制。

Result: 在UNSW - NB15和ToN - IoT两个公开基准数据集上实验，GTCN - G模型在二分类和多分类任务中显著优于现有基线模型。

Conclusion: GTCN - G模型能有效应对网络入侵检测中的挑战，达到了当前最优性能。

Abstract: The escalating complexity of network threats and the inherent class imbalance
in traffic data present formidable challenges for modern Intrusion Detection
Systems (IDS). While Graph Neural Networks (GNNs) excel in modeling topological
structures and Temporal Convolutional Networks (TCNs) are proficient in
capturing time-series dependencies, a framework that synergistically integrates
both while explicitly addressing data imbalance remains an open challenge. This
paper introduces a novel deep learning framework, named Gated Temporal
Convolutional Network and Graph (GTCN-G), engineered to overcome these
limitations. Our model uniquely fuses a Gated TCN (G-TCN) for extracting
hierarchical temporal features from network flows with a Graph Convolutional
Network (GCN) designed to learn from the underlying graph structure. The core
innovation lies in the integration of a residual learning mechanism,
implemented via a Graph Attention Network (GAT). This mechanism preserves
original feature information through residual connections, which is critical
for mitigating the class imbalance problem and enhancing detection sensitivity
for rare malicious activities (minority classes). We conducted extensive
experiments on two public benchmark datasets, UNSW-NB15 and ToN-IoT, to
validate our approach. The empirical results demonstrate that the proposed
GTCN-G model achieves state-of-the-art performance, significantly outperforming
existing baseline models in both binary and multi-class classification tasks.

</details>


### [151] [Evolutionary Profiles for Protein Fitness Prediction](https://arxiv.org/abs/2510.07286)
*Jigang Fan,Xiaoran Jiao,Shengdong Lin,Zhanming Liang,Weian Mao,Chenchen Jing,Hao Chen,Chunhua Shen*

Main category: cs.LG

TL;DR: 文章提出EvoIF模型预测突变适应性，利用进化信号，在少数据下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决蛋白质工程中因序列空间大、实验有限而难以预测突变适应性的问题。

Method: 将自然进化视为隐式奖励最大化，MLM视为逆强化学习，引入EvoIF模型融合家族内和跨家族进化信号。

Result: EvoIF及其MSA变体在ProteinGym上达SOTA或具竞争力，用0.15%训练数据和更少参数。

Conclusion: 家族内和跨家族信号互补，提高了模型在不同条件下的鲁棒性，代码将公开。

Abstract: Predicting the fitness impact of mutations is central to protein engineering
but constrained by limited assays relative to the size of sequence space.
Protein language models (pLMs) trained with masked language modeling (MLM)
exhibit strong zero-shot fitness prediction; we provide a unifying view by
interpreting natural evolution as implicit reward maximization and MLM as
inverse reinforcement learning (IRL), in which extant sequences act as expert
demonstrations and pLM log-odds serve as fitness estimates. Building on this
perspective, we introduce EvoIF, a lightweight model that integrates two
complementary sources of evolutionary signal: (i) within-family profiles from
retrieved homologs and (ii) cross-family structural-evolutionary constraints
distilled from inverse folding logits. EvoIF fuses sequence-structure
representations with these profiles via a compact transition block, yielding
calibrated probabilities for log-odds scoring. On ProteinGym (217 mutational
assays; >2.5M mutants), EvoIF and its MSA-enabled variant achieve
state-of-the-art or competitive performance while using only 0.15% of the
training data and fewer parameters than recent large models. Ablations confirm
that within-family and cross-family profiles are complementary, improving
robustness across function types, MSA depths, taxa, and mutation depths. The
codes will be made publicly available at https://github.com/aim-uofa/EvoIF.

</details>


### [152] [MolGA: Molecular Graph Adaptation with Pre-trained 2D Graph Encoder](https://arxiv.org/abs/2510.07289)
*Xingtong Yu,Chang Zhou,Xinming Zhang,Yuan Fang*

Main category: cs.LG

TL;DR: 提出MolGA方法，灵活整合多样分子领域知识来适配预训练2D图编码器到下游分子应用，并经实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有预训练2D图编码器忽略分子领域知识，分子预训练方法缺乏整合多样知识的灵活性，因此需在下游适配时整合知识。

Method: 提出分子对齐策略弥合预训练拓扑表示与领域知识表示的差距，引入条件适应机制生成特定实例标记以实现细粒度知识整合。

Result: 在11个公开数据集上进行大量实验，证明了MolGA的有效性。

Conclusion: MolGA能有效将预训练2D图编码器适配到下游分子应用，灵活整合多样分子领域知识。

Abstract: Molecular graph representation learning is widely used in chemical and
biomedical research. While pre-trained 2D graph encoders have demonstrated
strong performance, they overlook the rich molecular domain knowledge
associated with submolecular instances (atoms and bonds). While molecular
pre-training approaches incorporate such knowledge into their pre-training
objectives, they typically employ designs tailored to a specific type of
knowledge, lacking the flexibility to integrate diverse knowledge present in
molecules. Hence, reusing widely available and well-validated pre-trained 2D
encoders, while incorporating molecular domain knowledge during downstream
adaptation, offers a more practical alternative. In this work, we propose
MolGA, which adapts pre-trained 2D graph encoders to downstream molecular
applications by flexibly incorporating diverse molecular domain knowledge.
First, we propose a molecular alignment strategy that bridge the gap between
pre-trained topological representations with domain-knowledge representations.
Second, we introduce a conditional adaptation mechanism that generates
instance-specific tokens to enable fine-grained integration of molecular domain
knowledge for downstream tasks. Finally, we conduct extensive experiments on
eleven public datasets, demonstrating the effectiveness of MolGA.

</details>


### [153] [MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline](https://arxiv.org/abs/2510.07307)
*Rushi Qiang,Yuchen Zhuang,Anikait Singh,Percy Liang,Chao Zhang,Sherry Yang,Bo Dai*

Main category: cs.LG

TL;DR: 介绍自动多智能体管道MLE - Smith将原始数据集转化为MLE挑战，应用于真实数据集生成大量任务，评估显示其能有效扩展任务并保证质量。


<details>
  <summary>Details</summary>
Motivation: 当前MLE基准可扩展性低、适用性有限，高质量MLE训练数据获取受限。

Method: 提出MLE - Smith，通过生成 - 验证 - 执行范式，利用多智能体管道进行任务设计和重构，结合混合验证机制，并通过交互执行验证。

Result: 应用于224个真实数据集生成606个任务，8个主流和前沿大语言模型在MLE - Smith任务上表现与人工设计任务表现强相关。

Conclusion: MLE - Smith能有效扩展MLE任务，同时保持任务质量。

Abstract: While Language Models (LMs) have made significant progress in automating
machine learning engineering (MLE), the acquisition of high-quality MLE
training data is significantly constrained. Current MLE benchmarks suffer from
low scalability and limited applicability because they rely on static, manually
curated tasks, demanding extensive time and manual effort to produce. We
introduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw
datasets into competition-style MLE challenges through an efficient
generate-verify-execute paradigm for scaling MLE tasks with verifiable quality,
real-world usability, and rich diversity. The proposed multi-agent pipeline in
MLE-Smith drives structured task design and standardized refactoring, coupled
with a hybrid verification mechanism that enforces strict structural rules and
high-level semantic soundness. It further validates empirical solvability and
real-world fidelity through interactive execution. We apply MLE-Smith to 224 of
real-world datasets and generate 606 tasks spanning multiple categories,
objectives, and modalities, demonstrating that MLE-Smith can work effectively
across a wide range of real-world datasets. Evaluation on the generated tasks
shows that the performance of eight mainstream and cutting-edge LLMs on
MLE-Smith tasks is strongly correlated with their performance on carefully
human-designed tasks, highlighting the effectiveness of the MLE-Smith to
scaling up MLE tasks, while maintaining task quality.

</details>


### [154] [h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement Learning](https://arxiv.org/abs/2510.07312)
*Sumeet Ramesh Motwani,Alesia Ivanova,Ziyang Cai,Philip Torr,Riashat Islam,Shital Shah,Christian Schroeder de Witt,Charles London*

Main category: cs.LG

TL;DR: 提出一种仅用短视距数据提升大语言模型长视距推理能力的可扩展方法，在数学问题上效果显著，理论上有样本复杂度优势。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长视距推理任务中性能下降，现有方法不易扩展，需新方法提升长视距推理能力。

Method: 合成简单问题为任意长度的复杂多步依赖链，在仅结果奖励和自动增加复杂度的课程设置下训练模型。

Result: 在6年级数学问题上训练提升了竞争级基准测试的准确率，最高达2.06倍，在高pass@k时表现也优于基线。

Conclusion: 该方法为仅用现有数据扩展长视距问题的强化学习提供了有效途径。

Abstract: Large language models excel at short-horizon reasoning tasks, but performance
drops as reasoning horizon lengths increase. Existing approaches to combat this
rely on inference-time scaffolding or costly step-level supervision, neither of
which scales easily. In this work, we introduce a scalable method to bootstrap
long-horizon reasoning capabilities using only existing, abundant short-horizon
data. Our approach synthetically composes simple problems into complex,
multi-step dependency chains of arbitrary length. We train models on this data
using outcome-only rewards under a curriculum that automatically increases in
complexity, allowing RL training to be scaled much further without saturating.
Empirically, our method generalizes remarkably well: curriculum training on
composed 6th-grade level math problems (GSM8K) boosts accuracy on longer,
competition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x.
Importantly, our long-horizon improvements are significantly higher than
baselines even at high pass@k, showing that models can learn new reasoning
paths under RL. Theoretically, we show that curriculum RL with outcome rewards
achieves an exponential improvement in sample complexity over full-horizon
training, providing training signal comparable to dense supervision. h1
therefore introduces an efficient path towards scaling RL for long-horizon
problems using only existing data.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [155] [Associative Memory Model with Neural Networks: Memorizing multiple images with one neuron](https://arxiv.org/abs/2510.06542)
*Hiroshi Inazawa*

Main category: cs.NE

TL;DR: 本文提出用于图像记忆与召回的联想记忆神经网络模型，可单神经元存多图并同时召回，还能从不完整图像完整召回。


<details>
  <summary>Details</summary>
Motivation: 构建能有效进行图像记忆与召回的神经网络模型。

Method: 构建由‘Cue Ball’和‘Recall Net’组成的联想记忆模型，让单个神经元存储多幅图像。

Result: 模型可通过呈现一个存储图像召回所有存储图像，也能从不完整图像完整召回图像。

Conclusion: 所提出的联想记忆模型在图像记忆与召回方面表现良好。

Abstract: This paper presents a neural network model (associative memory model) for
memory and recall of images. In this model, only a single neuron can memorize
multi-images and when that neuron is activated, it is possible to recall all
the memorized images at the same time. The system is composed of a single
cluster of numerous neurons, referred to as the "Cue Ball," and multiple neural
network layers, collectively called the "Recall Net." One of the features of
this model is that several different images are stored simultaneously in one
neuron, and by presenting one of the images stored in that neuron, all stored
images are recalled. Furthermore, this model allows for complete recall of an
image even when an incomplete image is presented

</details>


### [156] [Neuromorphic Computing -- An Overview](https://arxiv.org/abs/2510.06721)
*Benedikt Jung,Maximilian Kalcher,Merlin Marinova,Piper Powell,Esma Sakalli*

Main category: cs.NE

TL;DR: 文章介绍神经形态计算，涵盖其需求、现有技术，先讲传统计算问题，再概述神经形态系统及主要技术，最后总结并展望未来。


<details>
  <summary>Details</summary>
Motivation: 传统计算技术达到极限，需要新的计算系统，因此引入神经形态计算领域。

Method: 先介绍传统计算历史和问题，再概述神经形态系统，接着讨论当前开发的主要技术，包括传统硬件上的类神经形态计算、神经形态芯片和光子系统，并分析其优缺点。

Result: 详细介绍了神经形态计算领域的相关技术及其优缺点。

Conclusion: 对神经形态计算进行总结并展望未来。

Abstract: With traditional computing technologies reaching their limit, a new field has
emerged seeking to follow the example of the human brain into a new era:
neuromorphic computing. This paper provides an introduction to neuromorphic
computing, why this and other new computing systems are needed, and what
technologies currently exist in the neuromorphic field. It begins with a
general introduction into the history of traditional computing and its present
problems, and then proceeds to a general overview of neuromorphic systems. It
subsequently discusses the main technologies currently in development. For
completeness, the paper first discusses neuromorphic-style computing on
traditional hardware, and then discusses the two top branches of specialized
hardware in this field; neuromorphic chips and photonic systems. Both branches
are explained as well as their relative benefits and drawbacks. The paper
concludes with a summary and an outlook on the future.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [157] [Accelerating Sparse Ternary GEMM for Quantized LLM inference on Apple Silicon](https://arxiv.org/abs/2510.06957)
*Baraq Lipshitz,Alessio Melone,Charalampos Maraziaris,Muhammed Bilal*

Main category: cs.PF

TL;DR: 为苹果M系列处理器优化稀疏三元GEMM内核，有架构感知优化，标量和向量化实现均有性能提升且跨稀疏度稳定。


<details>
  <summary>Details</summary>
Motivation: 现有苹果硅CPU库中稀疏三元GEMM未充分优化。

Method: 提出架构感知优化，包括新的数据格式、提升指令级并行策略和基于NEON的SIMD向量化。

Result: 标量实现对50%稀疏大矩阵性能提升5.98倍，达理论峰值50.2%；向量化实现对25%稀疏大矩阵性能提升5.59倍。

Conclusion: 优化后的内核在不同稀疏度下性能提升且稳定。

Abstract: Sparse Ternary General Matrix-Matrix Multiplication (GEMM) remains
under-optimized in existing libraries for Apple Silicon CPUs. We present a
Sparse Ternary GEMM kernel optimized specifically for Apple's M-series
processors. We propose a set of architecture-aware optimizations, including a
novel blocked and interleaved sparse data format to improve memory locality,
strategies to increase Instruction-Level Parallelism (ILP), and NEON-based
Single Instruction Multiple Data (SIMD) vectorization to exploit data-level
parallelism. Our scalar implementation achieves up to a 5.98x performance
increase over a traditional Ternary Compressed Sparse Column (TCSC) baseline
for large matrices with 50% ternary nonzero values (sparsity), reaching up to a
50.2% of the processor's theoretical peak performance, and remains stable
across varying sparsity levels. Our vectorized implementation delivers up to a
5.59x performance increase for large matrices with 25% sparsity, and remains
stable across varying sparsity levels.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [158] [Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems](https://arxiv.org/abs/2510.06343)
*Fikret Mert Gültekin,Oscar Lilja,Ranim Khojah,Rebekka Wohlrab,Marvin Damschen,Mazen Mohamad*

Main category: cs.SE

TL;DR: 本文探讨利用本地大语言模型支持林业领域网络安全风险评估，研究表明大语言模型能辅助专家，需人工监督，鼓励在关键领域使用基于大语言模型的代理。


<details>
  <summary>Details</summary>
Motivation: 安全关键软件系统中网络安全活动重要，但专家数量少，需工具支持评估漏洞和威胁。

Method: 进行设计科学研究，对12位专家进行访谈、互动会议和调查。

Result: 大语言模型可生成初始风险评估、识别威胁和进行冗余检查，需人工监督确保准确性和合规性，专家愿意在特定角色使用。

Conclusion: 鼓励在安全关键领域使用基于大语言模型的代理支持网络物理系统风险评估。

Abstract: In safety-critical software systems, cybersecurity activities become
essential, with risk assessment being one of the most critical. In many
software teams, cybersecurity experts are either entirely absent or represented
by only a small number of specialists. As a result, the workload for these
experts becomes high, and software engineers would need to conduct
cybersecurity activities themselves. This creates a need for a tool to support
cybersecurity experts and engineers in evaluating vulnerabilities and threats
during the risk assessment process. This paper explores the potential of
leveraging locally hosted large language models (LLMs) with retrieval-augmented
generation to support cybersecurity risk assessment in the forestry domain
while complying with data protection and privacy requirements that limit
external data sharing. We performed a design science study involving 12 experts
in interviews, interactive sessions, and a survey within a large-scale project.
The results demonstrate that LLMs can assist cybersecurity experts by
generating initial risk assessments, identifying threats, and providing
redundancy checks. The results also highlight the necessity for human oversight
to ensure accuracy and compliance. Despite trust concerns, experts were willing
to utilize LLMs in specific evaluation and assistance roles, rather than solely
relying on their generative capabilities. This study provides insights that
encourage the use of LLM-based agents to support the risk assessment process of
cyber-physical systems in safety-critical domains.

</details>


### [159] [Improving Assignment Submission in Higher Education through a Git-Enabled System: An Iterative Case Study](https://arxiv.org/abs/2510.06363)
*Ololade Babatunde,Tomisin Ayodabo,Raqibul Raqibul*

Main category: cs.SE

TL;DR: 研究引入并评估基于Git的定制作业提交系统，在大学环境应用后有显著效果，还指出挑战并给出见解。


<details>
  <summary>Details</summary>
Motivation: 解决高等教育中传统作业提交方法的挑战。

Method: 采用迭代软件开发和以用户为中心的设计方法，在真实大学环境集成系统，进行实证评估，含可用性测试和收集学生反馈。

Result: 作业跟踪、协作和提交效率显著提升；85%的教师认为系统更易用，84%的学生更喜欢，提交和审核时间减少38%，存储需求降低48%。

Conclusion: 该系统为分布式版本控制融入教育环境提供实用见解，提升了教师监督和学生参与度。

Abstract: This study addresses challenges in traditional assignment submission methods
used in higher education by introducing and evaluating a customized Git-based
submission system. Employing iterative software development and user-centered
design methodologies, the system was integrated within a real-world university
environment. Empirical evaluation, including usability testing and student
feedback, indicated significant improvements in assignment tracking,
collaboration, and submission efficiency. Students reported positive
experiences using distributed version control workflows, highlighting improved
learning outcomes and reduced administrative burden. Challenges related to
initial adoption and student learning curves were identified and mitigated
through iterative improvements. The proposed system contributes practical
insights for integrating distributed version control into educational settings,
enhancing both instructor oversight and student engagement in software
engineering and related disciplines. Based on our results, the research showed
that 85% of instructors found the git based system easier to use, with 84% of
students preferring it over traditional methods, as it provides a 38% reduction
in time taken for submission and review, while also leading to a 48% reduction
in storage requirements.

</details>


### [160] [Addressing Visual Impairments with Model-Driven Engineering: A Systematic Literature Review](https://arxiv.org/abs/2510.06483)
*Judith Michael,Lukas Netz,Bernhard Rumpe,Ingo Müller,John Grundy,Shavindra Wickramathilaka,Hourieh Khalajzadeh*

Main category: cs.SE

TL;DR: 本文对模型驱动工程（MDE）如何解决视觉障碍人群的可访问性进行系统文献综述，发现当前MDE研究对视觉相关可访问性支持不足，并提出研究议程。


<details>
  <summary>Details</summary>
Motivation: 软件应用对有可访问性需求（如视觉障碍）的用户存在障碍，MDE可将可访问性问题集成到软件开发中，减少人工工作，因此研究MDE如何解决视觉障碍的可访问性。

Method: 进行系统文献综述，从447篇论文中筛选出30篇符合标准的主要研究。

Result: 约三分之二的研究引用WCAG，但项目特定调整和最终用户验证阻碍其在MDE中广泛应用；分析的研究对多种信息进行建模，但很少明确具体建模技术，MDE方法细节不足，影响复用和再现性；受影响用户参与和开发者可访问性专业知识有限，导致实证验证薄弱。

Conclusion: 当前MDE研究对视觉相关可访问性支持不足，提出研究议程以更有效地将对视觉障碍的支持嵌入MDE流程。

Abstract: Software applications often pose barriers for users with accessibility needs,
e.g., visual impairments. Model-driven engineering (MDE), with its systematic
nature of code derivation, offers systematic methods to integrate accessibility
concerns into software development while reducing manual effort. This paper
presents a systematic literature review on how MDE addresses accessibility for
vision impairments. From 447 initially identified papers, 30 primary studies
met the inclusion criteria. About two-thirds reference the Web Content
Accessibility Guidelines (WCAG), yet their project-specific adaptions and
end-user validations hinder wider adoption in MDE. The analyzed studies model
user interface structures, interaction and navigation, user capabilities,
requirements, and context information. However, only few specify concrete
modeling techniques on how to incorporate accessibility needs or demonstrate
fully functional systems. Insufficient details on MDE methods, i.e.,
transformation rules or code templates, hinder the reuse, generalizability, and
reproducibility. Furthermore, limited involvement of affected users and limited
developer expertise in accessibility contribute to weak empirical validation.
Overall, the findings indicate that current MDE research insufficiently
supports vision-related accessibility. Our paper concludes with a research
agenda outlining how support for vision impairments can be more effectively
embedded in MDE processes.

</details>


### [161] [Beyond More Context: How Granularity and Order Drive Code Completion Quality](https://arxiv.org/abs/2510.06606)
*Uswat Yusuf,Genevieve Caumartin,Diego Elias Costa*

Main category: cs.SE

TL;DR: 本文针对代码补全中上下文构建挑战，参加ASE 2025上下文收集挑战，开展文件和代码块级检索实验，结果表明上下文数量和顺序影响模型性能，基于静态分析的代码块检索有显著提升。


<details>
  <summary>Details</summary>
Motivation: 代码补全中为大语言模型构建相关上下文在大型代码库中存在挑战，要参加ASE 2025上下文收集挑战并超越JetBrains基线。

Method: 开展文件和代码块级别的检索策略实验，研究上下文大小和文件顺序对大语言模型性能的影响。

Result: 上下文的数量和顺序会显著影响模型性能，基于静态分析的代码块检索在Python初始竞赛阶段比最佳文件检索策略提升6%，比无上下文基线提升16%。

Conclusion: 在实际开发场景中，开发有效的上下文收集管道时，检索粒度、顺序和混合策略很重要。

Abstract: Context plays an important role in the quality of code completion, as Large
Language Models (LLMs) require sufficient and relevant information to assist
developers in code generation tasks. However, composing a relevant context for
code completion poses challenges in large repositories: First, the limited
context length of LLMs makes it impractical to include all repository files.
Second, the quality of generated code is highly sensitive to noisy or
irrelevant context. In this paper, we present our approach for the ASE 2025
Context Collection Challenge. The challenge entails outperforming JetBrains
baselines by designing effective retrieval and context collection strategies.
We develop and evaluate a series of experiments that involve retrieval
strategies at both the file and chunk levels. We focus our initial experiments
on examining the impact of context size and file ordering on LLM performance.
Our results show that the amount and order of context can significantly
influence the performance of the models. We introduce chunk-based retrieval
using static analysis, achieving a 6% improvement over our best file-retrieval
strategy and a 16% improvement over the no-context baseline for Python in the
initial phase of the competition. Our results highlight the importance of
retrieval granularity, ordering and hybrid strategies in developing effective
context collection pipelines for real-world development scenarios.

</details>


### [162] [AISysRev -- LLM-based Tool for Title-abstract Screening](https://arxiv.org/abs/2510.06708)
*Aleksi Huotala,Miikka Kuutila,Olli-Pekka Turtio,Mika Mäntylä*

Main category: cs.SE

TL;DR: 开发基于大语言模型的筛选工具AiSysRev辅助软件工程系统综述，研究表明虽大语言模型不能取代人类判断，但可减轻评估文献负担。


<details>
  <summary>Details</summary>
Motivation: 系统综述筛选阶段工作量大，大语言模型虽不能全信但可辅助，因此开发工具以减轻负担。

Method: 开发基于大语言模型的Web应用工具AiSysRev，支持多模型、零样本和少样本筛选，还支持人工筛选，并用137篇论文进行试验。

Result: 论文可分为四类，边界情况大语言模型易出错，需人工干预。

Conclusion: 大语言模型不能取代人类判断，但能显著减轻评估大量科学文献的负担。

Abstract: Systematic reviews are a standard practice for summarizing the state of
evidence in software engineering. Conducting systematic reviews is laborious,
especially during the screening or study selection phase, where the number of
papers can be overwhelming. During this phase, papers are assessed against
inclusion and exclusion criteria based on their titles and abstracts. Recent
research has demonstrated that large language models (LLMs) can perform
title-abstract screening at a level comparable to that of a master's student.
While LLMs cannot be fully trusted, they can help, for example, in Rapid
Reviews, which try to expedite the review process. Building on recent research,
we developed AiSysRev, an LLM-based screening tool implemented as a web
application running in a Docker container. The tool accepts a CSV file
containing paper titles and abstracts. Users specify inclusion and exclusion
criteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev
supports both zero-shot and few-shot screening, and also allows for manual
screening through interfaces that display LLM results as guidance for human
reviewers.We conducted a trial study with 137 papers using the tool. Our
findings indicate that papers can be classified into four categories: Easy
Includes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary
cases, where LLMs are prone to errors, highlight the need for human
intervention. While LLMs do not replace human judgment in systematic reviews,
they can significantly reduce the burden of assessing large volumes of
scientific literature. Video: https://www.youtube.com/watch?v=jVbEj4Y4tQI Tool:
https://github.com/EvoTestOps/AISysRev

</details>


### [163] [LLM Company Policies and Policy Implications in Software Organizations](https://arxiv.org/abs/2510.06718)
*Ranim Khojah,Mazen Mohamad,Linda Erlenhov,Francisco Gomes de Oliveira Neto,Philipp Leitner*

Main category: cs.SE

TL;DR: 研究软件组织采用大语言模型聊天机器人的政策创建及影响因素，助力管理者安全集成。


<details>
  <summary>Details</summary>
Motivation: 软件组织采用大语言模型聊天机器人存在风险，需要明确政策。

Method: 研究11家公司创建政策的情况。

Result: 未提及。

Conclusion: 未提及。

Abstract: The risks associated with adopting large language model (LLM) chatbots in
software organizations highlight the need for clear policies. We examine how 11
companies create these policies and the factors that influence them, aiming to
help managers safely integrate chatbots into development workflows.

</details>


### [164] [Oops!... I did it again. Conclusion (In-)Stability in Quantitative Empirical Software Engineering: A Large-Scale Analysis](https://arxiv.org/abs/2510.06844)
*Nicole Hoess,Carlos Paradis,Rick Kazman,Wolfgang Mauerer*

Main category: cs.SE

TL;DR: 研究复杂工具管道用于进化软件分析的有效性威胁及工具一致性，发现工具设计和实现细节会导致结果差异，建议用户选工具并评估局限性，推广工具复用。


<details>
  <summary>Details</summary>
Motivation: 挖掘软件仓库工具常用，但局限和一致性不明，需研究复杂工具管道有效性威胁和工具一致性。

Method: 进行轻量级文献综述选三项研究，用四个独立系统选的挖掘工具正式复制研究，定量定性比较数据、结果和结论。

Result: 工具设计和实现技术细节积累会使提取数据、分析结果和结论产生显著差异。

Conclusion: 用户要选好工具并评估局限，推荐复用工具，研究者和作者可通过复制包和比较研究促进复用、减少不确定性。

Abstract: Context: Mining software repositories is a popular means to gain insights
into a software project's evolution, monitor project health, support decisions
and derive best practices. Tools supporting the mining process are commonly
applied by researchers and practitioners, but their limitations and agreement
are often not well understood.
  Objective: This study investigates some threats to validity in complex tool
pipelines for evolutionary software analyses and evaluates the tools' agreement
in terms of data, study outcomes and conclusions for the same research
questions.
  Method: We conduct a lightweight literature review to select three studies on
collaboration and coordination, software maintenance and software quality from
high-ranked venues, which we formally replicate with four independent,
systematically selected mining tools to quantitatively and qualitatively
compare the extracted data, analysis results and conclusions.
  Results: We find that numerous technical details in tool design and
implementation accumulate along the complex mining pipelines and can cause
substantial differences in the extracted baseline data, its derivatives,
subsequent results of statistical analyses and, under specific circumstances,
conclusions.
  Conclusions: Users must carefully choose tools and evaluate their limitations
to assess the scope of validity in an adequate way. Reusing tools is
recommended. Researchers and tool authors can promote reusability and help
reducing uncertainties by reproduction packages and comparative studies
following our approach.

</details>


### [165] [An empirical study on declined proposals: why are these proposals declined?](https://arxiv.org/abs/2510.06984)
*Masanari Kondo,Mahmoud Alfadel,Shane McIntosh,Yasutaka Kamei,Naoyasu Ubayashi*

Main category: cs.SE

TL;DR: 研究Go语言项目提案，量化结果、构建拒绝原因分类、评估大语言模型预测能力，发现提案拒绝多、解决时间长等问题，提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 开源软件提案流程资源密集且易使贡献者受挫，提案拒绝原因不明，需了解拒绝原因并预测结果以优化流程。

Method: 对Go项目的1091个提案进行混合方法实证研究，量化结果、构建拒绝原因分类、评估大语言模型预测能力。

Result: 提案拒绝多于接受，解决时间超一个月，仅14.7%被拒提案会重新提交，识别出九个拒绝原因，GPT模型可早期预测拒绝决定。

Conclusion: 提案流程存在低效问题，可通过早期分诊和引导贡献者改进提案来提升贡献者体验和减轻审查工作量。

Abstract: Design-level decisions in open-source software (OSS) projects are often made
through structured mechanisms such as proposals, which require substantial
community discussion and review. Despite their importance, the proposal process
is resource-intensive and often leads to contributor frustration, especially
when proposals are declined without clear feedback. Yet, the reasons behind
proposal rejection remain poorly understood, limiting opportunities to
streamline the process or guide contributors effectively. This study
investigates the characteristics and outcomes of proposals in the Go
programming language to understand why proposals are declined and how such
outcomes might be anticipated. We conduct a mixed-method empirical study on
1,091 proposals submitted to the Go project. We quantify proposal outcomes,
build a taxonomy of decline reasons, and evaluate large language models (LLMs)
for predicting these outcomes. We find that proposals are more often declined
than accepted, and resolution typically takes over a month. Only 14.7% of
declined proposals are ever resubmitted. Through qualitative coding, we
identify nine key reasons for proposal decline, such as duplication, limited
use cases, or violations of project principles. This taxonomy can help
contributors address issues in advance, e.g., checking for existing
alternatives can reduce redundancy. We also demonstrate that GPT-based models
can predict decline decisions early in the discussion (F1 score = 0.71 with
partial comments), offering a practical tool for prioritizing review effort.
Our findings reveal inefficiencies in the proposal process and highlight
actionable opportunities for improving both contributor experience and reviewer
workload by enabling early triage and guiding contributors to strengthen their
proposals using a structured understanding of past decline reasons.

</details>


### [166] [Human-aligned AI Model Cards with Weighted Hierarchy Architecture](https://arxiv.org/abs/2510.06989)
*Pengyue Yang,Haolin Jin,Qingwen Zeng,Jiawen Wen,Harry Rao,Huaming Chen*

Main category: cs.SE

TL;DR: 大语言模型发展带来模型发现和采用挑战，现有文档框架有不足，提出CRAI - MCF框架解决问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生态发展使模型发现和采用困难，现有文档框架无法满足严格跨模型比较需求，导致模型利用不足和难以负责任地采用。

Method: 基于价值敏感设计（VSD），对240个开源项目进行实证分析，将217个参数提炼成八模块、价值对齐的架构，并引入定量充足标准。

Result: 提出CRAI - MCF框架，可进行严格的跨模型比较。

Conclusion: CRAI - MCF框架平衡技术、伦理和运营维度，能让从业者更有信心和完整性地评估、选择和采用大语言模型。

Abstract: The proliferation of Large Language Models (LLMs) has led to a burgeoning
ecosystem of specialized, domain-specific models. While this rapid growth
accelerates innovation, it has simultaneously created significant challenges in
model discovery and adoption. Users struggle to navigate this landscape due to
inconsistent, incomplete, and imbalanced documentation across platforms.
Existing documentation frameworks, such as Model Cards and FactSheets, attempt
to standardize reporting but are often static, predominantly qualitative, and
lack the quantitative mechanisms needed for rigorous cross-model comparison.
This gap exacerbates model underutilization and hinders responsible adoption.
To address these shortcomings, we introduce the Comprehensive Responsible AI
Model Card Framework (CRAI-MCF), a novel approach that transitions from static
disclosures to actionable, human-aligned documentation. Grounded in Value
Sensitive Design (VSD), CRAI-MCF is built upon an empirical analysis of 240
open-source projects, distilling 217 parameters into an eight-module,
value-aligned architecture. Our framework introduces a quantitative sufficiency
criterion to operationalize evaluation and enables rigorous cross-model
comparison under a unified scheme. By balancing technical, ethical, and
operational dimensions, CRAI-MCF empowers practitioners to efficiently assess,
select, and adopt LLMs with greater confidence and operational integrity.

</details>


### [167] [Building an Open AIBOM Standard in the Wild](https://arxiv.org/abs/2510.07070)
*Gopi Krishnan Rajbahadur,Keheliya Gallaba,Elyas Rashno,Arthit Suriyawongkul,Karen Bennet,Kate Stewart,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文介绍AI物料清单AIBOM规范开发经验，通过行动研究开展多方协作，用四种方法验证规范，还总结流程与经验。


<details>
  <summary>Details</summary>
Motivation: 现代软件工程依赖开源社区驱动标准，但在快速发展的AI系统领域标准创建研究不足，需开发AIBOM规范。

Method: 采用行动研究方法，组织全球超90名贡献者参与，进行多轮行动研究循环；用与法规和伦理标准对齐、映射行业用例、从业者访谈、工业案例研究四种方法验证规范。

Result: 得到了经过验证的AIBOM规范。

Conclusion: 记录了AIBOM规范开发过程，分析其与行动研究循环的契合度，总结经验为软件工程社区未来标准化工作提供参考。

Abstract: Modern software engineering increasingly relies on open, community-driven
standards, yet how such standards are created in fast-evolving domains like
AI-powered systems remains underexplored. This paper presents a detailed
experience report on the development of the AI Bill of Materials AIBOM
specification, an extension of the ISO/IEC 5962:2021 Software Package Data
Exchange (SPDX) software bill of materials (SBOM) standard, which captures AI
components such as datasets and iterative training artifacts. Framed through
the lens of Action Research (AR), we document a global, multi-stakeholder
effort involving over 90 contributors and structured AR cycles. The resulting
specification was validated through four complementary approaches: alignment
with major regulations and ethical standards (e.g., EU AI Act and IEEE 7000
standards), systematic mapping to six industry use cases, semi-structured
practitioner interviews, and an industrial case study. Beyond delivering a
validated artefact, our paper documents the process of building the AIBOM
specification in the wild, and reflects on how it aligns with the AR cycle, and
distills lessons that can inform future standardization efforts in the software
engineering community.

</details>


### [168] [Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe](https://arxiv.org/abs/2510.07189)
*Junjie Li,Fazle Rabbi,Bo Yang,Song Wang,Jinqiu Yang*

Main category: cs.SE

TL;DR: 提出Secure - Instruct框架优化大语言模型安全代码生成，经评估能提升代码安全性与功能正确性，优于SafeCoder。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型生成的代码常不安全，且当前改进方法因数据集问题效果和泛化性不佳。

Method: 提出Secure - Instruct框架，自动合成高质量易受攻击和安全的代码示例、生成微调指令并对大语言模型进行指令微调。

Result: 在CWEBench上安全比率平均提升14.3%，超SafeCoder 7.6%；在CWEval上，CodeLlama - 7B和Mistral - 7B的Func - Sec@1分别提升14%和5.8%，超SafeCoder 15.8%和6.8%。

Conclusion: Secure - Instruct能提升生成代码的安全性和功能正确性。

Abstract: Although Large Language Models (LLMs) show promising solutions to automated
code generation, they often produce insecure code that threatens software
security. Current approaches (e.g., SafeCoder) to improve secure code
generation suffer from limited and imbalanced datasets, reducing their
effectiveness and generalizability. In this work, we present Secure-Instruct, a
novel framework that automatically synthesizes high-quality vulnerable and
secure code examples, generates fine-tuning instructions, and instruction-tunes
LLMs to align task description and secure code generation abilities. We
evaluate Secure-Instruct on four representative LLMs using two benchmarks: our
own CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44
CWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning
dataset, while CWEval covers 31 CWEs with 119 manually verified
security-critical tasks. We find that Secure-Instruct improves not only the
security but also the functional correctness of the generated code. On
CWEBench, Secure-Instruct substantially improves secure code generation, giving
a 14.3% average increase in secure ratio over the pretrained models and
outperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%
increase for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained
models, and surpasses SafeCoder by 15.8% and 6.8% respectively.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [169] [Nonparametric Estimation of Self- and Cross-Impact](https://arxiv.org/abs/2510.06879)
*Natascha Hey,Eyal Neuman,Sturmius Tuschmann*

Main category: q-fin.TR

TL;DR: 提出凹多资产传播模型的离线非参数估计器，用不同数据实现，有多项发现且投影非参核表现好


<details>
  <summary>Details</summary>
Motivation: 避免多资产情况下参数模型的参数爆炸问题，并为估计器提供置信区间

Method: 基于相关价格轨迹和元订单数据集引入非参数估计器，用CFM专有元订单数据和公开S&P订单流数据实现，使用元订单代理扩充数据，进行形状约束投影

Result: 证明自冲击是凹的且呈移位幂律衰减，元订单代理稳定校准，引入交叉冲击解释力提升，凹规格优于线性规格，测量到资产间不对称交叉冲击，形状约束投影非参核预测准确性略胜参数模型

Conclusion: 非参数估计器在多资产传播模型中有优势，形状约束投影能保证可解释性和提升预测准确性

Abstract: We introduce an offline nonparametric estimator for concave multi-asset
propagator models based on a dataset of correlated price trajectories and
metaorders. Compared to parametric models, our framework avoids parameter
explosion in the multi-asset case and yields confidence bounds for the
estimator. We implement the estimator using both proprietary metaorder data
from Capital Fund Management (CFM) and publicly available S&P order flow data,
where we augment the former dataset using a metaorder proxy. In particular, we
provide unbiased evidence that self-impact is concave and exhibits a shifted
power-law decay, and show that the metaorder proxy stabilizes the calibration.
Moreover, we find that introducing cross-impact provides a significant gain in
explanatory power, with concave specifications outperforming linear ones,
suggesting that the square-root law extends to cross-impact. We also measure
asymmetric cross-impact between assets driven by relative liquidity
differences. Finally, we demonstrate that a shape-constrained projection of the
nonparametric kernel not only ensures interpretability but also slightly
outperforms established parametric models in terms of predictive accuracy.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [170] [A General Constructive Upper Bound on Shallow Neural Nets Complexity](https://arxiv.org/abs/2510.06372)
*Frantisek Hakl,Vit Fojtik*

Main category: stat.ML

TL;DR: 给出浅层神经网络逼近紧致集上连续函数所需神经元数量的上界，方法更具通用性。


<details>
  <summary>Details</summary>
Motivation: 为浅层神经网络逼近紧致集上连续函数所需神经元数量提供上界。

Method: 受Stone - Weierstrass定理特定证明启发的构造性方法。

Result: 得到了逼近给定精度下所需神经元数量的上界。

Conclusion: 该方法比以往同类界限更具通用性，适用于任意紧致集上的任意连续函数。

Abstract: We provide an upper bound on the number of neurons required in a shallow
  neural network to approximate a continuous function on a compact set with a
  given accuracy. This method, inspired by a specific proof of the
  Stone-Weierstrass theorem, is constructive and more general than previous
  bounds of this character, as it applies to any continuous function on any
  compact set.

</details>


### [171] [Online Matching via Reinforcement Learning: An Expert Policy Orchestration Strategy](https://arxiv.org/abs/2510.06515)
*Chiara Mignacco,Matthieu Jonckheere,Gilles Stoltz*

Main category: stat.ML

TL;DR: 提出强化学习方法协调专家策略，用于在线匹配问题，有理论保证，在模拟中表现良好，凸显结构化自适应学习优势。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法适用于特定操作模式，条件变化时效率低，需更好方法解决在线匹配问题。

Method: 基于Adv2框架，通过基于优势的权重更新结合专家决策，引入神经 actor - critic 架构。建立期望和高概率遗憾保证，推导新的有限时间偏差界限。

Result: 模拟显示，协调策略比单个专家和传统 RL 基线收敛更快，系统效率更高。

Conclusion: 结构化、自适应学习可改善复杂资源分配和决策过程的建模与管理。

Abstract: Online matching problems arise in many complex systems, from cloud services
and online marketplaces to organ exchange networks, where timely, principled
decisions are critical for maintaining high system performance. Traditional
heuristics in these settings are simple and interpretable but typically
tailored to specific operating regimes, which can lead to inefficiencies when
conditions change. We propose a reinforcement learning (RL) approach that
learns to orchestrate a set of such expert policies, leveraging their
complementary strengths in a data-driven, adaptive manner. Building on the Adv2
framework (Jonckheere et al., 2024), our method combines expert decisions
through advantage-based weight updates and extends naturally to settings where
only estimated value functions are available. We establish both expectation and
high-probability regret guarantees and derive a novel finite-time bias bound
for temporal-difference learning, enabling reliable advantage estimation even
under constant step size and non-stationary dynamics. To support scalability,
we introduce a neural actor-critic architecture that generalizes across large
state spaces while preserving interpretability. Simulations on stochastic
matching models, including an organ exchange scenario, show that the
orchestrated policy converges faster and yields higher system level efficiency
than both individual experts and conventional RL baselines. Our results
highlight how structured, adaptive learning can improve the modeling and
management of complex resource allocation and decision-making processes.

</details>


### [172] [Q-Learning with Fine-Grained Gap-Dependent Regret](https://arxiv.org/abs/2510.06647)
*Haochen Zhang,Zhong Zheng,Lingzhou Xue*

Main category: stat.ML

TL;DR: 研究情节式表格马尔可夫决策过程中无模型强化学习的细粒度差距相关遗憾界，为UCB和非UCB算法建立了相关界。


<details>
  <summary>Details</summary>
Motivation: 现有无模型算法的差距相关界较粗糙，未能充分捕捉次优差距结构。

Method: 在UCB设置下，开发新分析框架；引入新算法ULCB - Hoeffding。在非UCB设置下，修正AMB算法问题。

Result: 为UCB - Hoeffding得到首个细粒度遗憾上界；ULCB - Hoeffding有细粒度遗憾保证且表现优于AMB；为非UCB方法建立首个严格细粒度差距相关遗憾，改进版AMB表现更好。

Conclusion: 成功建立细粒度差距相关遗憾界，改进了现有算法性能。

Abstract: We study fine-grained gap-dependent regret bounds for model-free
reinforcement learning in episodic tabular Markov Decision Processes. Existing
model-free algorithms achieve minimax worst-case regret, but their
gap-dependent bounds remain coarse and fail to fully capture the structure of
suboptimality gaps. We address this limitation by establishing fine-grained
gap-dependent regret bounds for both UCB-based and non-UCB-based algorithms. In
the UCB-based setting, we develop a novel analytical framework that explicitly
separates the analysis of optimal and suboptimal state-action pairs, yielding
the first fine-grained regret upper bound for UCB-Hoeffding (Jin et al., 2018).
To highlight the generality of this framework, we introduce ULCB-Hoeffding, a
new UCB-based algorithm inspired by AMB (Xu et al.,2021) but with a simplified
structure, which enjoys fine-grained regret guarantees and empirically
outperforms AMB. In the non-UCB-based setting, we revisit the only known
algorithm AMB, and identify two key issues in its algorithm design and
analysis: improper truncation in the $Q$-updates and violation of the
martingale difference condition in its concentration argument. We propose a
refined version of AMB that addresses these issues, establishing the first
rigorous fine-grained gap-dependent regret for a non-UCB-based method, with
experiments demonstrating improved performance over AMB.

</details>


### [173] [Gaussian Equivalence for Self-Attention: Asymptotic Spectral Analysis of Attention Matrix](https://arxiv.org/abs/2510.06685)
*Tomohiro Hayase,Benoît Collins,Ryo Karakida*

Main category: stat.ML

TL;DR: 本文严格分析注意力矩阵奇异值谱，得到首个注意力高斯等价结果，指出奇异值平方分布与以往认知不同。


<details>
  <summary>Details</summary>
Motivation: 自注意力层理论理解有限，尤其是从随机矩阵理论视角。

Method: 借助精确控制归一化项波动和利用指数有利泰勒展开的精细线性化。

Result: 表明注意力矩阵奇异值分布可用易处理线性模型渐近刻画，奇异值平方分布偏离马尔琴科 - 帕斯图尔定律。

Conclusion: 分析确定线性化阈值，阐明此情况下注意力有严格高斯等价的原因。

Abstract: Self-attention layers have become fundamental building blocks of modern deep
neural networks, yet their theoretical understanding remains limited,
particularly from the perspective of random matrix theory. In this work, we
provide a rigorous analysis of the singular value spectrum of the attention
matrix and establish the first Gaussian equivalence result for attention. In a
natural regime where the inverse temperature remains of constant order, we show
that the singular value distribution of the attention matrix is asymptotically
characterized by a tractable linear model. We further demonstrate that the
distribution of squared singular values deviates from the Marchenko-Pastur law,
which has been believed in previous work. Our proof relies on two key
ingredients: precise control of fluctuations in the normalization term and a
refined linearization that leverages favorable Taylor expansions of the
exponential. This analysis also identifies a threshold for linearization and
elucidates why attention, despite not being an entrywise operation, admits a
rigorous Gaussian equivalence in this regime.

</details>


### [174] [Bayesian Nonparametric Dynamical Clustering of Time Series](https://arxiv.org/abs/2510.06919)
*Adrián Pérez-Herrero,Paulo Félix,Jesús Presedo,Carl Henrik Ek*

Main category: stat.ML

TL;DR: 提出一种通过在未知数量的线性动态状态间切换来建模无限数量时间序列簇演化的方法，用贝叶斯非参数方法实现，在心电图分析案例中展示了有效性。


<details>
  <summary>Details</summary>
Motivation: 解决对无限数量时间序列簇演化建模的问题，避免簇的不必要扩散。

Method: 采用贝叶斯非参数方法，以分层狄利克雷过程作为切换线性动态系统参数的先验，高斯过程先验建模每个簇内的幅度和时间对齐的统计变化，通过变分下界进行离线和在线推理。

Result: 方法在心电图分析的多个案例研究中展示了多功能性和有效性。

Conclusion: 所提出的方法能够有效地对时间序列簇的演化进行建模，且具有较好的实用性。

Abstract: We present a method that models the evolution of an unbounded number of time
series clusters by switching among an unknown number of regimes with linear
dynamics. We develop a Bayesian non-parametric approach using a hierarchical
Dirichlet process as a prior on the parameters of a Switching Linear Dynamical
System and a Gaussian process prior to model the statistical variations in
amplitude and temporal alignment within each cluster. By modeling the evolution
of time series patterns, the method avoids unnecessary proliferation of
clusters in a principled manner. We perform inference by formulating a
variational lower bound for off-line and on-line scenarios, enabling efficient
learning through optimization. We illustrate the versatility and effectiveness
of the approach through several case studies of electrocardiogram analysis
using publicly available databases.

</details>


### [175] [PyCFRL: A Python library for counterfactually fair offline reinforcement learning via sequential data preprocessing](https://arxiv.org/abs/2510.06935)
*Jianhan Zhang,Jitao Wang,Chengchun Shi,John D. Piette,Donglin Zeng,Zhenke Wu*

Main category: stat.ML

TL;DR: 提出Python库PyCFRL用于确保离线强化学习中的反事实公平性，介绍功能、展示用例并给出开源地址和文档。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法的顺序决策可能对少数群体或社会经济弱势群体不利，需要解决公平性问题。

Method: 引入PyCFRL库，实现新的数据预处理算法以从离线数据集学习反事实公平的强化学习策略，并提供评估工具。

Result: 描述了PyCFRL的高级功能，通过数据示例展示了一个主要用例，库在PyPI和Github上公开可用。

Conclusion: PyCFRL有助于解决离线强化学习中的公平性问题。

Abstract: Reinforcement learning (RL) aims to learn and evaluate a sequential decision
rule, often referred to as a "policy", that maximizes the population-level
benefit in an environment across possibly infinitely many time steps. However,
the sequential decisions made by an RL algorithm, while optimized to maximize
overall population benefits, may disadvantage certain individuals who are in
minority or socioeconomically disadvantaged groups. To address this problem, we
introduce PyCFRL, a Python library for ensuring counterfactual fairness in
offline RL. PyCFRL implements a novel data preprocessing algorithm for learning
counterfactually fair RL policies from offline datasets and provides tools to
evaluate the values and counterfactual unfairness levels of RL policies. We
describe the high-level functionalities of PyCFRL and demonstrate one of its
major use cases through a data example. The library is publicly available on
PyPI and Github (https://github.com/JianhanZhang/PyCFRL), and detailed
tutorials can be found in the PyCFRL documentation
(https://pycfrl-documentation.netlify.app).

</details>


### [176] [Diffusion-Augmented Reinforcement Learning for Robust Portfolio Optimization under Stress Scenarios](https://arxiv.org/abs/2510.07099)
*Himanshu Choudhary,Arishi Orra,Manoj Thakur*

Main category: stat.ML

TL;DR: 提出Diffusion - Augmented Reinforcement Learning (DARL)框架用于投资组合管理，表现优于传统方法，能提升对危机的应对能力。


<details>
  <summary>Details</summary>
Motivation: 传统投资组合优化方法难以捕捉市场复杂动态和满足投资者多样偏好。

Method: 提出DARL框架，将Denoising Diffusion Probabilistic Models (DDPMs)与Deep Reinforcement Learning (DRL)结合，用DDPMs生成不同压力强度下的合成市场崩溃场景。

Result: DARL在实证评估中优于传统基线，提供了更好的风险调整回报，对2025关税危机等意外危机有更强的恢复能力。

Conclusion: 该工作为强化DRL驱动的金融应用的压力恢复能力提供了稳健实用的方法。

Abstract: In the ever-changing and intricate landscape of financial markets, portfolio
optimisation remains a formidable challenge for investors and asset managers.
Conventional methods often struggle to capture the complex dynamics of market
behaviour and align with diverse investor preferences. To address this, we
propose an innovative framework, termed Diffusion-Augmented Reinforcement
Learning (DARL), which synergistically integrates Denoising Diffusion
Probabilistic Models (DDPMs) with Deep Reinforcement Learning (DRL) for
portfolio management. By leveraging DDPMs to generate synthetic market crash
scenarios conditioned on varying stress intensities, our approach significantly
enhances the robustness of training data. Empirical evaluations demonstrate
that DARL outperforms traditional baselines, delivering superior risk-adjusted
returns and resilience against unforeseen crises, such as the 2025 Tariff
Crisis. This work offers a robust and practical methodology to bolster stress
resilience in DRL-driven financial applications.

</details>


### [177] [Root Cause Analysis of Outliers in Unknown Cyclic Graphs](https://arxiv.org/abs/2510.06995)
*Daniela Schkoda,Dominik Janzing*

Main category: stat.ML

TL;DR: 研究循环因果图中异常值传播，在一定条件下找出潜在根源短列表且无需因果图先验知识


<details>
  <summary>Details</summary>
Motivation: 研究循环因果图中异常值的传播并追溯到根源节点

Method: 在扰动足够强且按正常模式结构方程传播的条件下进行分析

Result: 能识别出由真实根源和其处于与根源同一循环上的父节点组成的潜在根源短列表

Conclusion: 可以在不具备因果图先验知识的情况下，找出潜在根源短列表

Abstract: We study the propagation of outliers in cyclic causal graphs with linear
structural equations, tracing them back to one or several "root cause" nodes.
We show that it is possible to identify a short list of potential root causes
provided that the perturbation is sufficiently strong and propagates according
to the same structural equations as in the normal mode. This shortlist consists
of the true root causes together with those of its parents lying on a cycle
with the root cause. Notably, our method does not require prior knowledge of
the causal graph.

</details>


### [178] [Explaining Models under Multivariate Bernoulli Distribution via Hoeffding Decomposition](https://arxiv.org/abs/2510.07088)
*Baptiste Ferrere,Nicolas Bousquet,Fabrice Gamboa,Jean-Michel Loubes,Joseph Muré*

Main category: stat.ML

TL;DR: 本文聚焦输入变量为伯努利分布时预测模型的子模型分解，给出完整描述，形成可解释框架，推导输入影响指标，经实验验证可用于决策支持问题，并提出高维及有限可数输入模型的研究展望。


<details>
  <summary>Details</summary>
Motivation: 实现对具有随机输入的预测模型行为的解释，在输入变量为伯努利分布时完善子模型分解理论。

Method: 基于斜投影到L2子空间的概念，对输入变量为伯努利分布的情况进行分析。

Result: 发现L2子空间是一维的，功能分解是显式的，可明确推导输入对输出预测影响的指标，数值实验证明分析方法对决策支持问题有用。

Conclusion: 建立了完整的可解释性框架，理论上可进行逆向工程，为高维设置及有限可数输入模型的研究提供了方向。

Abstract: Explaining the behavior of predictive models with random inputs can be
achieved through sub-models decomposition, where such sub-models have easier
interpretable features. Arising from the uncertainty quantification community,
recent results have demonstrated the existence and uniqueness of a generalized
Hoeffding decomposition for such predictive models when the stochastic input
variables are correlated, based on concepts of oblique projection onto L 2
subspaces. This article focuses on the case where the input variables have
Bernoulli distributions and provides a complete description of this
decomposition. We show that in this case the underlying L 2 subspaces are
one-dimensional and that the functional decomposition is explicit. This leads
to a complete interpretability framework and theoretically allows reverse
engineering. Explicit indicators of the influence of inputs on the output
prediction (exemplified by Sobol' indices and Shapley effects) can be
explicitly derived. Illustrated by numerical experiments, this type of analysis
proves useful for addressing decision-support problems, based on binary
decision diagrams, Boolean networks or binary neural networks. The article
outlines perspectives for exploring high-dimensional settings and, beyond the
case of binary inputs, extending these findings to models with finite countable
inputs.

</details>


### [179] [Split Conformal Classification with Unsupervised Calibration](https://arxiv.org/abs/2510.07185)
*Santiago Mazuelas*

Main category: stat.ML

TL;DR: 本文提出用于分类任务的无监督校准分裂共形预测方法，理论和实验表明该方法性能接近有监督校准，只是性能保证和计算效率有适度下降。


<details>
  <summary>Details</summary>
Motivation: 现有分裂共形预测方法需使用与训练样本不同的有标签校准样本，不方便且限制了训练数据使用，因此提出无监督校准方法。

Method: 使用无监督校准样本和先前用于学习分类规则的有监督训练样本得到集合预测规则。

Result: 该方法能达到与有监督校准相当的性能，但性能保证和计算效率有适度下降。

Conclusion: 所提出的无监督校准分裂共形预测方法在分类任务中是有效的，虽有一定性能损耗但仍有可用性。

Abstract: Methods for split conformal prediction leverage calibration samples to
transform any prediction rule into a set-prediction rule that complies with a
target coverage probability. Existing methods provide remarkably strong
performance guarantees with minimal computational costs. However, they require
to use calibration samples composed by labeled examples different to those used
for training. This requirement can be highly inconvenient, as it prevents the
use of all labeled examples for training and may require acquiring additional
labels solely for calibration. This paper presents an effective methodology for
split conformal prediction with unsupervised calibration for classification
tasks. In the proposed approach, set-prediction rules are obtained using
unsupervised calibration samples together with supervised training samples
previously used to learn the classification rule. Theoretical and experimental
results show that the presented methods can achieve performance comparable to
that with supervised calibration, at the expenses of a moderate degradation in
performance guarantees and computational efficiency.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [180] [A Mixed-Methods Analysis of Repression and Mobilization in Bangladesh's July Revolution Using Machine Learning and Statistical Modeling](https://arxiv.org/abs/2510.06264)
*Md. Saiful Bari Siddiqui,Anupam Debashis Roy*

Main category: stat.AP

TL;DR: 研究孟加拉国2024年七月革命，分析国家暴力如何反而推动运动胜利，采用混合方法，得出革命由特定道德冲击触发的非线性回波效应驱动。


<details>
  <summary>Details</summary>
Motivation: 探究学生主导的平民起义成功的核心悖论，即国家暴力本欲平息异议却推动运动胜利。

Method: 采用混合方法，先构建冲突时间线定性叙事提出假设，再用事件级数据集进行多方法定量分析，还包括固定效应面板模型、向量自回归分析、结构断点分析和机器学习分析。

Result: 初始回归模型显示抗议势头重要；固定效应面板模型证明局部镇压回波效应；向量自回归分析显示致命暴力引发全国动员且效应非线性；结构断点分析表明早期回波不显著，特定道德冲击触发效应；机器学习分析确认过度使用武力是升级主因。

Conclusion: 七月革命由特定催化道德冲击触发的偶然、非线性回波效应驱动，国家暴行画面传播加速革命。

Abstract: The 2024 July Revolution in Bangladesh represents a landmark event in the
study of civil resistance. This study investigates the central paradox of the
success of this student-led civilian uprising: how state violence, intended to
quell dissent, ultimately fueled the movement's victory. We employ a
mixed-methods approach. First, we develop a qualitative narrative of the
conflict's timeline to generate specific, testable hypotheses. Then, using a
disaggregated, event-level dataset, we employ a multi-method quantitative
analysis to dissect the complex relationship between repression and
mobilisation. We provide a framework to analyse explosive modern uprisings like
the July Revolution. Initial pooled regression models highlight the crucial
role of protest momentum in sustaining the movement. To isolate causal effects,
we specify a Two-Way Fixed Effects panel model, which provides robust evidence
for a direct and statistically significant local suppression backfire effect.
Our Vector Autoregression (VAR) analysis provides clear visual evidence of an
immediate, nationwide mobilisation in response to increased lethal violence. We
further demonstrate that this effect was non-linear. A structural break
analysis reveals that the backfire dynamic was statistically insignificant in
the conflict's early phase but was triggered by the catalytic moral shock of
the first wave of lethal violence, and its visuals circulated around July 16th.
A complementary machine learning analysis (XGBoost, out-of-sample R$^{2}$=0.65)
corroborates this from a predictive standpoint, identifying "excessive force
against protesters" as the single most dominant predictor of nationwide
escalation. We conclude that the July Revolution was driven by a contingent,
non-linear backfire, triggered by specific catalytic moral shocks and
accelerated by the viral reaction to the visual spectacle of state brutality.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [181] [BACHI: Boundary-Aware Symbolic Chord Recognition Through Masked Iterative Decoding on Pop and Classical Music](https://arxiv.org/abs/2510.06528)
*Mingyang Yao,Ke Chen,Shlomo Dubnov,Taylor Berg-Kirkpatrick*

Main category: cs.SD

TL;DR: 现有深度学习自动和弦识别有音频领域受关注多、忽略人类分析策略两挑战，本文引入增强数据集POP909 - CL，提出模型BACHI，实验显示其性能达最优。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动和弦识别中符号音乐研究少和忽略人类分析策略的问题。

Method: 引入增强版数据集POP909 - CL，提出将任务分解为不同决策步骤的符号和弦识别模型BACHI。

Result: BACHI在古典和流行音乐基准测试中达到了最先进的和弦识别性能，消融研究验证了各模块的有效性。

Conclusion: 引入的数据集和提出的模型有效提升了符号和弦识别的性能。

Abstract: Automatic chord recognition (ACR) via deep learning models has gradually
achieved promising recognition accuracy, yet two key challenges remain. First,
prior work has primarily focused on audio-domain ACR, while symbolic music
(e.g., score) ACR has received limited attention due to data scarcity. Second,
existing methods still overlook strategies that are aligned with human music
analytical practices. To address these challenges, we make two contributions:
(1) we introduce POP909-CL, an enhanced version of POP909 dataset with
tempo-aligned content and human-corrected labels of chords, beats, keys, and
time signatures; and (2) We propose BACHI, a symbolic chord recognition model
that decomposes the task into different decision steps, namely boundary
detection and iterative ranking of chord root, quality, and bass (inversion).
This mechanism mirrors the human ear-training practices. Experiments
demonstrate that BACHI achieves state-of-the-art chord recognition performance
on both classical and pop music benchmarks, with ablation studies validating
the effectiveness of each module.

</details>


### [182] [AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs](https://arxiv.org/abs/2510.07293)
*Peize He,Zichen Wen,Yubo Wang,Yuxuan Wang,Xiaoqian Liu,Jiajie Huang,Zehui Lei,Zhuangcheng Gu,Xiangqi Jin,Jiabing Yang,Kai Li,Zhifei Liu,Weijia Li,Cunxiang Wang,Conghui He,Linfeng Zhang*

Main category: cs.SD

TL;DR: 引入AudioMarathon基准评估长音频理解和推理效率，评估现有模型并研究加速技术，指出当前模型差距和改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有音频基准多基于短音频，无法在真实长上下文场景评估大音频语言模型，需填补这一空白。

Method: 引入AudioMarathon基准，包含长上下文音频输入、全领域覆盖和复杂推理任务，评估现有SOTA模型，研究加速技术。

Result: 随着音频长度增加，模型性能明显下降，不同模型存在较大差距。

Conclusion: AudioMarathon可推动音频和多模态研究社区开发更先进的音频理解模型。

Abstract: Processing long-form audio is a major challenge for Large Audio Language
models (LALMs). These models struggle with the quadratic cost of attention
($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio
benchmarks are built mostly from short clips and do not evaluate models in
realistic long context settings. To address this gap, we introduce
AudioMarathon, a benchmark designed to evaluate both understanding and
inference efficiency on long-form audio. AudioMarathon provides a diverse set
of tasks built upon three pillars: long-context audio inputs with durations
ranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of
2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,
sound, and music, and complex reasoning that requires multi-hop inference. We
evaluate state-of-the-art LALMs and observe clear performance drops as audio
length grows. We also study acceleration techniques and analyze the trade-offs
of token pruning and KV cache eviction. The results show large gaps across
current LALMs and highlight the need for better temporal reasoning and
memory-efficient architectures. We believe AudioMarathon will drive the audio
and multimodal research community to develop more advanced audio understanding
models capable of solving complex audio tasks.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [183] [VeriEquivBench: An Equivalence Score for Ground-Truth-Free Evaluation of Formally Verifiable Code](https://arxiv.org/abs/2510.06296)
*Lingfei Zeng,Fengdi Che,Xuhan Huang,Fei Ye,Xu Xu,Binhang Yuan,Jie Fu*

Main category: cs.PL

TL;DR: 提出新基准VeriEquivBench评估大语言模型生成形式可验证代码能力，结果显示当前模型仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基准评估形式规范质量依赖人工匹配真实规范，存在数据集小和可靠性问题，需新方法。

Method: 引入含2389个复杂算法问题的VeriEquivBench基准，用等价分数替代真实匹配评估。

Result: 生成形式可验证代码对现有大模型仍是巨大挑战。

Conclusion: 该任务难度大，需要VeriEquivBench这样的基准推动可扩展可靠编码代理发展。

Abstract: Formal verification is the next frontier for ensuring the correctness of code
generated by Large Language Models (LLMs). While methods that co-generate code
and formal specifications in formal languages, like Dafny, can, in principle,
prove alignment with user intent, progress is bottlenecked by specification
quality evaluation. Current benchmarks rely on matching against ground-truth
specifications, a manual and expertise-intensive process that has limited
existing datasets to a few hundred simple problems and also suffers from a
reliability issue. To address this, we introduce VeriEquivBench, a new
benchmark with $2,389$ complex algorithmic problems that probe the limitations
of current models in both code generation and formal reasoning. Our evaluation
framework replaces ground-truth matching with a formally grounded metric, the
equivalence score, and rigorously verifies the quality of generated
specifications and code. Our results show that generating formally verifiable
code remains a profound challenge for state-of-the-art LLMs. This underscores
both the difficulty of the task and the need for benchmarks like VeriEquivBench
to drive progress toward scalable and reliable coding agents.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [184] [Quantum matrix arithmetics with Hamiltonian evolution](https://arxiv.org/abs/2510.06316)
*Christopher Kang,Yuan Su*

Main category: quant-ph

TL;DR: 本文开发了一套利用哈密顿演化进行矩阵算术运算的方法，结果编码在哈密顿量的非对角块中，可实现多种矩阵运算，还提出重叠估计算法，给出模拟一类哈密顿量的电路。


<details>
  <summary>Details</summary>
Motivation: 高效实现矩阵算术运算以加速量子算法。

Method: 利用输入算子的哈密顿演化进行矩阵运算，保持哈密顿块编码；矩阵乘法用李群交换子乘积公式及其高阶推广；奇异值变换采用主导多项式近似；给出模拟一类和平方哈密顿量的电路。

Result: 可实现矩阵乘法、加法、求逆等多种运算，整个量子计算最多用2个辅助量子比特；重叠估计算法无需额外量子比特；模拟电路在步长上有交换子缩放，用于量子化学相关哈密顿量时，对固定粒子数初态有进一步改进，仅需1个辅助量子比特。

Conclusion: 所提出的方法能有效实现矩阵算术运算，在量子算法和量子化学模拟中有应用价值。

Abstract: The efficient implementation of matrix arithmetic operations underpins the
speedups of many quantum algorithms. We develop a suite of methods to perform
matrix arithmetics -- with the result encoded in the off-diagonal blocks of a
Hamiltonian -- using Hamiltonian evolutions of input operators. We show how to
maintain this $\textit{Hamiltonian block encoding}$, so that matrix operations
can be composed one after another, and the entire quantum computation takes
$\leq 2$ ancilla qubits. We achieve this for matrix multiplication, matrix
addition, matrix inversion, Hermitian conjugation, fractional scaling, integer
scaling, complex phase scaling, as well as singular value transformation for
both odd and even polynomials. We also present an overlap estimation algorithm
to extract classical properties of Hamiltonian block encoded operators,
analogous to the well known Hadmard test, at no extra cost of qubit. Our
Hamiltonian matrix multiplication uses the Lie group commutator product formula
and its higher-order generalizations due to Childs and Wiebe. Our Hamiltonian
singular value transformation employs a dominated polynomial approximation,
where the approximation holds within the domain of interest, while the
constructed polynomial is upper bounded by the target function over the entire
unit interval. We describe a circuit for simulating a class of sum-of-squares
Hamiltonians, attaining a commutator scaling in step count, while leveraging
the power of matrix arithmetics to reduce the cost of each simulation step. In
particular, we apply this to the doubly factorized tensor hypercontracted
Hamiltonians from recent studies of quantum chemistry, obtaining further
improvements for initial states with a fixed number of particles. We achieve
this with $1$ ancilla qubit.

</details>


### [185] [Breaking the Treewidth Barrier in Quantum Circuit Simulation with Decision Diagrams](https://arxiv.org/abs/2510.06775)
*Bin Cheng,Ziyuan Wang,Ruixuan Deng,Jianxin Chen,Zhengfeng Ji*

Main category: quant-ph

TL;DR: 分析FeynmanDD方法，表明其在特定电路族上优于张量网络方法且可去除门集限制。


<details>
  <summary>Details</summary>
Motivation: 现有张量网络方法对大树宽电路计算成本高，需更好的量子电路经典模拟方法。

Method: 严格分析FeynmanDD方法，研究其多终端决策图大小与电路图线性秩宽的关系。

Result: FeynmanDD中多终端决策图大小与电路图线性秩宽呈指数关系，在特定电路族上优于张量网络方法，用Solovay - Kitaev算法可去除门集限制。

Conclusion: FeynmanDD在特定电路模拟上表现更优且可解决门集限制问题。

Abstract: Classical simulation of quantum circuits is a critical tool for validating
quantum hardware and probing the boundary between classical and quantum
computational power. Existing state-of-the-art methods, notably tensor network
approaches, have computational costs governed by the treewidth of the
underlying circuit graph, making circuits with large treewidth intractable.
This work rigorously analyzes FeynmanDD, a decision diagram-based simulation
method proposed in CAV 2025 by a subset of the authors, and shows that the size
of the multi-terminal decision diagram used in FeynmanDD is exponential in the
linear rank-width of the circuit graph. As linear rank-width can be
substantially smaller than treewidth and is at most larger than the treewidth
by a logarithmic factor, our analysis demonstrates that FeynmanDD outperforms
all tensor network-based methods for certain circuit families. We also show
that the method remains efficient if we use the Solovay-Kitaev algorithm to
expand arbitrary single-qubit gates to sequences of Hadamard and T gates,
essentially removing the gate-set restriction posed by the method.

</details>


### [186] [Reconquering Bell sampling on qudits: stabilizer learning and testing, quantum pseudorandomness bounds, and more](https://arxiv.org/abs/2510.06848)
*Jonathan Allcock,Joao F. Doriguello,Gábor Ivanyos,Miklos Santha*

Main category: quant-ph

TL;DR: 本文克服先前困难，将贝尔采样推广到任意维度的量子多体系统（qudits），并展示了该技术在多个量子问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 此前贝尔采样从量子比特（qubits）推广到任意维度（d > 2）的量子多体系统（qudits）存在困难，且自然扩展无法提供有意义信息，因此需要开发有用的推广方法。

Method: 基于拉格朗日四平方定理构建新的酉算子，将四个稳定子态副本映射到其复共轭副本。

Result: 成功将贝尔采样推广到所有d ≥ 2的量子多体系统，并将多个已知的量子比特结果提升到量子多体系统，如稳定子态学习、隐藏稳定子群问题求解等。

Conclusion: 新的贝尔采样技术可有效应用于量子多体系统，解决多个相关问题。

Abstract: Bell sampling is a simple yet powerful tool based on measuring two copies of
a quantum state in the Bell basis, and has found applications in a plethora of
problems related to stabiliser states and measures of magic. However, it was
not known how to generalise the procedure from qubits to $d$-level systems --
qudits -- for all dimensions $d > 2$ in a useful way. Indeed, a prior work of
the authors (arXiv'24) showed that the natural extension of Bell sampling to
arbitrary dimensions fails to provide meaningful information about the quantum
states being measured. In this paper, we overcome the difficulties encountered
in previous works and develop a useful generalisation of Bell sampling to
qudits of all $d\geq 2$. At the heart of our primitive is a new unitary, based
on Lagrange's four-square theorem, that maps four copies of any stabiliser
state $|\mathcal{S}\rangle$ to four copies of its complex conjugate
$|\mathcal{S}^\ast\rangle$ (up to some Pauli operator), which may be of
independent interest. We then demonstrate the utility of our new Bell sampling
technique by lifting several known results from qubits to qudits for any $d\geq
2$:
  1. Learning stabiliser states in $O(n^3)$ time with $O(n)$ samples;
  2. Solving the Hidden Stabiliser Group Problem in
$\tilde{O}(n^3/\varepsilon)$ time with $\tilde{O}(n/\varepsilon)$ samples;
  3. Testing whether $|\psi\rangle$ has stabiliser size at least $d^t$ or is
$\varepsilon$-far from all such states in $\tilde{O}(n^3/\varepsilon)$ time
with $\tilde{O}(n/\varepsilon)$ samples;
  4. Clifford circuits with at most $n/2$ single-qudit non-Clifford gates
cannot prepare pseudorandom states;
  5. Testing whether $|\psi\rangle$ has stabiliser fidelity at least
$1-\varepsilon_1$ or at most $1-\varepsilon_2$ with $O(d^2/\varepsilon_2)$
samples if $\varepsilon_1 = 0$ or $O(d^2/\varepsilon_2^2)$ samples if
$\varepsilon_1 = O(d^{-2})$.

</details>


### [187] [Randomized Quantum Singular Value Transformation](https://arxiv.org/abs/2510.06851)
*Xinzhao Wang,Yuxin Zhang,Soumyabrata Hazra,Tongyang Li,Changpeng Shao,Shantanav Chakraborty*

Main category: quant-ph

TL;DR: 提出量子奇异值变换（QSVT）的随机算法，用单辅助量子比特避免块编码，有两种方法，在特定参数下优于标准QSVT，应用于两个任务有优势，基准测试表现好。


<details>
  <summary>Details</summary>
Motivation: 标准QSVT实现依赖块编码，构建成本高，需多个辅助量子比特、复杂多量子比特控制和线性电路深度。

Method: 开发两种方法，一是用重要性采样替代块编码的直接随机化QSVT，二是将qDRIFT集成到广义量子信号处理框架并通过经典外推改进精度依赖。

Result: 门复杂度与哈密顿项数无关，对目标多项式次数仅二次依赖；应用于两个任务有多项式优势；基准测试在电路深度上比先前工作好几个数量级。

Conclusion: 随机QSVT是早期容错量子设备实用且资源高效的替代方案。

Abstract: We introduce the first randomized algorithms for Quantum Singular Value
Transformation (QSVT), a unifying framework for many quantum algorithms.
Standard implementations of QSVT rely on block encodings of the Hamiltonian,
which are costly to construct, requiring a logarithmic number of ancilla
qubits, intricate multi-qubit control, and circuit depth scaling linearly with
the number of Hamiltonian terms. In contrast, our algorithms use only a single
ancilla qubit and entirely avoid block encodings. We develop two methods: (i) a
direct randomization of QSVT, where block encodings are replaced by importance
sampling, and (ii) an approach that integrates qDRIFT into the generalized
quantum signal processing framework, with the dependence on precision
exponentially improved through classical extrapolation. Both algorithms achieve
gate complexity independent of the number of Hamiltonian terms, a hallmark of
randomized methods, while incurring only quadratic dependence on the degree of
the target polynomial. We identify natural parameter regimes where our methods
outperform even standard QSVT, making them promising for early fault-tolerant
quantum devices. We also establish a fundamental lower bound showing that the
quadratic dependence on the polynomial degree is optimal within this framework.
We apply our framework to two fundamental tasks: solving quantum linear systems
and estimating ground-state properties of Hamiltonians, obtaining polynomial
advantages over prior randomized algorithms. Finally, we benchmark our
ground-state property estimation algorithm on electronic structure Hamiltonians
and the transverse-field Ising model with long-range interactions. In both
cases, our approach outperforms prior work by several orders of magnitude in
circuit depth, establishing randomized QSVT as a practical and
resource-efficient alternative for early fault-tolerant quantum devices.

</details>


### [188] [Quantum Sparse Recovery and Quantum Orthogonal Matching Pursuit](https://arxiv.org/abs/2510.06925)
*Armando Bellante,Stefano Vanerio,Stefano Zanero*

Main category: quant-ph

TL;DR: 研究非正交、过完备字典中的量子稀疏恢复问题，证明一般恢复问题NP难，提出QOMP算法，给出非正交字典的稀疏量子层析框架，分析其在QRAM模型中的加速效果并提供估计字典互不相干性的量子算法。


<details>
  <summary>Details</summary>
Motivation: 解决非正交、过完备字典中的量子稀疏恢复问题，突破现有算法局限。

Method: 提出量子正交匹配追踪（QOMP）算法，结合量子子程序和误差重置设计。

Result: QOMP在标准假设下可多项式时间恢复K稀疏态的精确支撑集；给出非正交字典的稀疏量子层析框架，在有利情况下达到查询复杂度；在QRAM模型中相对经典OMP有多项式加速；提供估计字典互不相干性的量子算法。

Conclusion: QOMP算法有效解决量子稀疏恢复问题，在量子层析等领域有应用价值，能带来计算优势。

Abstract: We study quantum sparse recovery in non-orthogonal, overcomplete
dictionaries: given coherent quantum access to a state and a dictionary of
vectors, the goal is to reconstruct the state up to $\ell_2$ error using as few
vectors as possible. We first show that the general recovery problem is
NP-hard, ruling out efficient exact algorithms in full generality. To overcome
this, we introduce Quantum Orthogonal Matching Pursuit (QOMP), the first
quantum analogue of the classical OMP greedy algorithm. QOMP combines quantum
subroutines for inner product estimation, maximum finding, and block-encoded
projections with an error-resetting design that avoids iteration-to-iteration
error accumulation. Under standard mutual incoherence and well-conditioned
sparsity assumptions, QOMP provably recovers the exact support of a $K$-sparse
state in polynomial time. As an application, we give the first framework for
sparse quantum tomography with non-orthogonal dictionaries in $\ell_2$ norm,
achieving query complexity $\widetilde{O}(\sqrt{N}/\epsilon)$ in favorable
regimes and reducing tomography to estimating only $K$ coefficients instead of
$N$ amplitudes. In particular, for pure-state tomography with $m=O(N)$
dictionary vectors and sparsity $K=\widetilde{O}(1)$ on a well-conditioned
subdictionary, this circumvents the $\widetilde{\Omega}(N/\epsilon)$ lower
bound that holds in the dense, orthonormal-dictionary setting, without
contradiction, by leveraging sparsity together with non-orthogonality. Beyond
tomography, we analyze QOMP in the QRAM model, where it yields polynomial
speedups over classical OMP implementations, and provide a quantum algorithm to
estimate the mutual incoherence of a dictionary of $m$ vectors in
$O(m/\epsilon)$ queries, improving over both deterministic and quantum-inspired
classical methods.

</details>


### [189] [Clifford testing: algorithms and lower bounds](https://arxiv.org/abs/2510.07164)
*Marcel Hinsche,Zongbo Bao,Philippe van Dordrecht,Jens Eisert,Jop Briët,Jonas Helsen*

Main category: quant-ph

TL;DR: 本文提出首个4 - 查询Clifford测试器，解决Clifford测试问题，还考虑单副本访问情况并给出查询数上下界。


<details>
  <summary>Details</summary>
Motivation: 解决Clifford测试问题，即判断黑盒n - 量子比特酉矩阵是否为Clifford酉矩阵或与所有Clifford酉矩阵至少有ε距离。

Method: 利用Clifford群换位子结构，借鉴容错稳定子测试技术。

Result: 给出4 - 查询Clifford测试器；证明测试器是容错的；解决Bu, Gu和Jaffe的猜想；单副本访问下给出O(n) - 查询Clifford测试器并证明下界为Ω(n^(1/4))。

Conclusion: 成功解决Clifford测试问题，得到不同情况下的测试器及查询数上下界，相关技术结论有独立研究价值。

Abstract: We consider the problem of Clifford testing, which asks whether a black-box
$n$-qubit unitary is a Clifford unitary or at least $\varepsilon$-far from
every Clifford unitary. We give the first 4-query Clifford tester, which
decides this problem with probability $\mathrm{poly}(\varepsilon)$. This
contrasts with the minimum of 6 copies required for the closely-related task of
stabilizer testing. We show that our tester is tolerant, by adapting techniques
from tolerant stabilizer testing to our setting. In doing so, we settle in the
positive a conjecture of Bu, Gu and Jaffe, by proving a polynomial inverse
theorem for a non-commutative Gowers 3-uniformity norm. We also consider the
restricted setting of single-copy access, where we give an $O(n)$-query
Clifford tester that requires no auxiliary memory qubits or adaptivity. We
complement this with a lower bound, proving that any such, potentially
adaptive, single-copy algorithm needs at least $\Omega(n^{1/4})$ queries. To
obtain our results, we leverage the structure of the commutant of the Clifford
group, obtaining several technical statements that may be of independent
interest.

</details>


### [190] [On quantum to classical comparison for Davies generators](https://arxiv.org/abs/2510.07267)
*Joao Basso,Shirshendu Ganguly,Alistair Sinclair,Nikhil Srivastava,Zachary Stier,Thuy-Duong Vuong*

Main category: quant-ph

TL;DR: 本文研究量子马尔可夫链，证明在一类哈密顿量下量子谱隙与经典谱隙可比，可将经典马尔可夫链技术用于量子场景。


<details>
  <summary>Details</summary>
Motivation: 量子马尔可夫链理解不如经典的完善，前人提出量子和经典动力学收敛性质关系问题，本文进一步研究。

Method: 证明戴维斯生成器的“非对角”特征向量可构造与哈密顿量对易的可观测量，其林德布拉德瑞利商有上界，由可观测量的谱隙推出全戴维斯生成器的谱隙。

Result: 若哈密顿量谱中无长算术级数，量子和经典谱隙可比；对一大类哈密顿量，量子谱隙与经典谱隙在常数因子内。

Conclusion: 结果符合物理直觉，能将经典马尔可夫链技术应用于量子场景。

Abstract: Despite extensive study, our understanding of quantum Markov chains remains
far less complete than that of their classical counterparts. [Temme'13]
observed that the Davies Lindbladian, a well-studied model of quantum Markov
dynamics, contains an embedded classical Markov generator, raising the natural
question of how the convergence properties of the quantum and classical
dynamics are related. While [Temme'13] showed that the spectral gap of the
Davies Lindbladian can be much smaller than that of the embedded classical
generator for certain highly structured Hamiltonians, we show that if the
spectrum of the Hamiltonian does not contain long arithmetic progressions, then
the two spectral gaps must be comparable. As a consequence, we prove that for a
large class of Hamiltonians, including those obtained by perturbing a fixed
Hamiltonian with a generic external field, the quantum spectral gap remains
within a constant factor of the classical spectral gap. Our result aligns with
physical intuition and enables the application of classical Markov chain
techniques to the quantum setting.
  The proof is based on showing that any ``off-diagonal'' eigenvector of the
Davies generator can be used to construct an observable which commutes with the
Hamiltonian and has a Lindbladian Rayleigh quotient which can be upper bounded
in terms of that of the original eigenvector's Lindbladian Rayleigh quotient.
Thus, a spectral gap for such observables implies a spectral gap for the full
Davies generator.

</details>


### [191] [Layerwise Federated Learning for Heterogeneous Quantum Clients using Quorus](https://arxiv.org/abs/2510.06228)
*Jason Han,Nicholas S. DiBrita,Daniel Leeds,Jianqiang Li,Jason Ludmir,Tirthak Patel*

Main category: quant-ph

TL;DR: 提出针对量子联邦学习问题的Quorus方案，模拟和硬件实验显示其可增加高深度客户端梯度幅度并提升测试准确率。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习需分布式学习，但不同客户端量子计算机易出错且误差特性不同，需不同深度电路，因此要解决量子联邦学习问题。

Method: 提出Quorus方案，利用逐层损失函数有效训练不同深度量子模型，根据客户端需求给出多种模型设计。

Result: 模拟和真实硬件实验结果显示，Quorus增加了高深度客户端梯度幅度，平均测试准确率比现有技术提高12.4%。

Conclusion: Quorus方案有应用前景，能有效解决量子联邦学习中客户端量子计算机差异问题。

Abstract: Quantum machine learning (QML) holds the promise to solve classically
intractable problems, but, as critical data can be fragmented across private
clients, there is a need for distributed QML in a quantum federated learning
(QFL) format. However, the quantum computers that different clients have access
to can be error-prone and have heterogeneous error properties, requiring them
to run circuits of different depths. We propose a novel solution to this QFL
problem, Quorus, that utilizes a layerwise loss function for effective training
of varying-depth quantum models, which allows clients to choose models for
high-fidelity output based on their individual capacity. Quorus also presents
various model designs based on client needs that optimize for shot budget,
qubit count, midcircuit measurement, and optimization space. Our simulation and
real-hardware results show the promise of Quorus: it increases the magnitude of
gradients of higher depth clients and improves testing accuracy by 12.4% on
average over the state-of-the-art.

</details>


### [192] [CLAQS: Compact Learnable All-Quantum Token Mixer with Shared-ansatz for Text Classification](https://arxiv.org/abs/2510.06532)
*Junhao Chen,Yifan Zhou,Hanqi Jiang,Yi Pan,Yiwei Li,Huaqin Zhao,Wei Zhang,Yingfeng Wang,Tianming Liu*

Main category: quant-ph

TL;DR: 提出CLAQS量子token混合器用于文本分类，仅需少量数据量子比特和浅电路，在SST - 2和IMDB上表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前量子计算发展迅速，但设备存在比特和深度限制、训练不稳定以及经典注意力计算和内存开销大等问题，因此需要紧凑、相位感知的量子token混合器。

Method: 提出CLAQS，在统一量子电路中联合学习复值混合和非线性变换；应用l1归一化调节振幅缩放；引入两阶段参数化量子架构解耦共享token嵌入和窗口级量子前馈模块。

Result: CLAQS仅需8个数据量子比特和浅电路，在SST - 2上准确率达91.64%，在IMDB上达87.08%。

Conclusion: CLAQS优于经典Transformer基线和强大的混合量子 - 经典模型。

Abstract: Quantum compute is scaling fast, from cloud QPUs to high throughput GPU
simulators, making it timely to prototype quantum NLP beyond toy tasks.
However, devices remain qubit limited and depth limited, training can be
unstable, and classical attention is compute and memory heavy. This motivates
compact, phase aware quantum token mixers that stabilize amplitudes and scale
to long sequences. We present CLAQS, a compact, fully quantum token mixer for
text classification that jointly learns complex-valued mixing and nonlinear
transformations within a unified quantum circuit. To enable stable end-to-end
optimization, we apply l1 normalization to regulate amplitude scaling and
introduce a two-stage parameterized quantum architecture that decouples shared
token embeddings from a window-level quantum feed-forward module. Operating
under a sliding-window regime with document-level aggregation, CLAQS requires
only eight data qubits and shallow circuits, yet achieves 91.64% accuracy on
SST-2 and 87.08% on IMDB, outperforming both classical Transformer baselines
and strong hybrid quantum-classical counterparts.

</details>


### [193] [Toward Uncertainty-Aware and Generalizable Neural Decoding for Quantum LDPC Codes](https://arxiv.org/abs/2510.06257)
*Xiangjun Mi,Frank Mueller*

Main category: quant-ph

TL;DR: 本文提出贝叶斯图神经解码器QuBA和多码训练框架SAGU，在量子纠错解码中效果优于经典和现有神经解码器。


<details>
  <summary>Details</summary>
Motivation: 传统解码算法精度有限、开销高，现有机器学习解码器缺乏可靠不确定性量化和跨码泛化能力。

Method: 提出集成注意力机制的贝叶斯图神经解码器QuBA，开发多码训练框架SAGU。

Result: QuBA和SAGU在二元自行车码及其互质变体实验中，逻辑错误率优于经典基线BP，QuBA也超越现有神经解码器，SAGU性能与QuBA相当或更优。

Conclusion: QuBA和SAGU在量子纠错解码上具有良好性能和泛化能力，可有效解决现有解码器的不足。

Abstract: Quantum error correction (QEC) is essential for scalable quantum computing,
yet decoding errors via conventional algorithms result in limited accuracy
(i.e., suppression of logical errors) and high overheads, both of which can be
alleviated by inference-based decoders. To date, such machine-learning (ML)
decoders lack two key properties crucial for practical fault tolerance:
reliable uncertainty quantification and robust generalization to previously
unseen codes. To address this gap, we propose \textbf{QuBA}, a Bayesian graph
neural decoder that integrates attention to both dot-product and multi-head,
enabling expressive error-pattern recognition alongside calibrated uncertainty
estimates. Building on QuBA, we further develop \textbf{SAGU
}\textbf{(Sequential Aggregate Generalization under Uncertainty)}, a multi-code
training framework with enhanced cross-domain robustness enabling decoding
beyond the training set. Experiments on bivariate bicycle (BB) codes and their
coprime variants demonstrate that (i) both QuBA and SAGU consistently
outperform the classical baseline belief propagation (BP), achieving a
reduction of on average \emph{one order of magnitude} in logical error rate
(LER), and up to \emph{two orders of magnitude} under confident-decision bounds
on the coprime BB code $[[154, 6, 16]]$; (ii) QuBA also surpasses
state-of-the-art neural decoders, providing an advantage of roughly \emph{one
order of magnitude} (e.g., for the larger BB code $[[756, 16, \leq34]]$) even
when considering conservative (safe) decision bounds; (iii) SAGU achieves
decoding performance comparable to or even outperforming QuBA's domain-specific
training approach.

</details>


### [194] [Adapting Quantum Machine Learning for Energy Dissociation of Bonds](https://arxiv.org/abs/2510.06563)
*Swathi Chandrasekhar,Shiva Raj Pokhrel,Navneet Singh*

Main category: quant-ph

TL;DR: 本文对量子和经典机器学习模型预测键解离能（BDEs）进行基准测试，发现顶级量子模型在化学常见的中程BDE范围内与经典模型表现相当。


<details>
  <summary>Details</summary>
Motivation: 准确预测BDEs有助于分子和材料的理性设计，需要对量子和经典机器学习模型进行比较。

Method: 使用化学筛选特征集，在Qiskit Aer上实现量子框架，采用多种量子模型，并与经典模型进行基准测试。

Result: 顶级量子模型（QCNN、QRF）在化学常见的中程BDE范围内，预测准确性和鲁棒性与经典集成和深度网络相当。

Conclusion: 为量子增强的分子性质预测建立了透明基准，为推进量子计算化学接近化学精度奠定了基础。

Abstract: Accurate prediction of bond dissociation energies (BDEs) underpins
mechanistic insight and the rational design of molecules and materials. We
present a systematic, reproducible benchmark comparing quantum and classical
machine learning models for BDE prediction using a chemically curated feature
set encompassing atomic properties (atomic numbers, hybridization), bond
characteristics (bond order, type), and local environmental descriptors. Our
quantum framework, implemented in Qiskit Aer on six qubits, employs
ZZFeatureMap encodings with variational ansatz (RealAmplitudes) across multiple
architectures Variational Quantum Regressors (VQR), Quantum Support Vector
Regressors (QSVR), Quantum Neural Networks (QNN), Quantum Convolutional Neural
Networks (QCNN), and Quantum Random Forests (QRF). These are rigorously
benchmarked against strong classical baselines, including Support Vector
Regression (SVR), Random Forests (RF), and Multi-Layer Perceptrons (MLP).
Comprehensive evaluation spanning absolute and relative error metrics,
threshold accuracies, and error distributions shows that top-performing quantum
models (QCNN, QRF) match the predictive accuracy and robustness of classical
ensembles and deep networks, particularly within the chemically prevalent
mid-range BDE regime. These findings establish a transparent baseline for
quantum-enhanced molecular property prediction and outline a practical
foundation for advancing quantum computational chemistry toward near chemical
accuracy.

</details>


### [195] [Expressive and Scalable Quantum Fusion for Multimodal Learning](https://arxiv.org/abs/2510.06938)
*Tuyen Nguyen,Trong Nghia Hoang,Phi Le Nguyen,Hai L. Vu,Truong Cong Thang*

Main category: quant-ph

TL;DR: 本文介绍量子融合层（QFL）用于多模态学习，模拟显示其优于经典基线，是新的可扩展方法。


<details>
  <summary>Details</summary>
Motivation: 引入多模态学习的量子融合机制并确立其理论和实证潜力。

Method: 提出量子融合层（QFL），用混合量子 - 经典程序替代经典融合方案，利用参数化量子电路学习纠缠特征交互。

Result: 在小型多样多模态任务模拟中，QFL 始终优于经典基线，在高模态场景改善显著。

Conclusion: QFL 是一种全新且可扩展的多模态融合方法，值得在更大系统中深入探索。

Abstract: The aim of this paper is to introduce a quantum fusion mechanism for
multimodal learning and to establish its theoretical and empirical potential.
The proposed method, called the Quantum Fusion Layer (QFL), replaces classical
fusion schemes with a hybrid quantum-classical procedure that uses
parameterized quantum circuits to learn entangled feature interactions without
requiring exponential parameter growth. Supported by quantum signal processing
principles, the quantum component efficiently represents high-order polynomial
interactions across modalities with linear parameter scaling, and we provide a
separation example between QFL and low-rank tensor-based methods that
highlights potential quantum query advantages. In simulation, QFL consistently
outperforms strong classical baselines on small but diverse multimodal tasks,
with particularly marked improvements in high-modality regimes. These results
suggest that QFL offers a fundamentally new and scalable approach to multimodal
fusion that merits deeper exploration on larger systems.

</details>


### [196] [Quantum Computing Methods for Malware Detection](https://arxiv.org/abs/2510.06803)
*Eliška Krátká,Aurél Gábor Gábris*

Main category: quant-ph

TL;DR: 本文探索量子计算结合量子机器学习（QML）提升恶意软件检测能力，对比量子支持向量机（QSVM）和SVM，在模拟器和量子硬件上实验并总结使用经验和遇到的问题。


<details>
  <summary>Details</summary>
Motivation: 研究量子计算在恶意软件检测中的潜力，对比QSVM和SVM算法性能。

Method: 使用包含PE文件原始二进制的公开数据集，在Qiskit SDK本地模拟器和IBM量子计算机上实现并评估结合不同特征图量子内核的QSVM算法。

Result: 实验结果揭示了量子计算机在处理大规模恶意软件检测计算时的行为和性能。

Conclusion: 总结了通过Qiskit接口使用量子硬件的实践经验，详细描述遇到的关键问题及对Qiskit机器学习库基础代码的修复方法。

Abstract: In this paper, we explore the potential of quantum computing in enhancing
malware detection through the application of Quantum Machine Learning (QML).
Our main objective is to investigate the performance of the Quantum Support
Vector Machine (QSVM) algorithm compared to SVM. A publicly available dataset
containing raw binaries of Portable Executable (PE) files was used for the
classification. The QSVM algorithm, incorporating quantum kernels through
different feature maps, was implemented and evaluated on a local simulator
within the Qiskit SDK and IBM quantum computers. Experimental results from
simulators and quantum hardware provide insights into the behavior and
performance of quantum computers, especially in handling large-scale
computations for malware detection tasks. The work summarizes the practical
experience with using quantum hardware via the Qiskit interfaces. We describe
in detail the critical issues encountered, as well as the fixes that had to be
developed and applied to the base code of the Qiskit Machine Learning library.
These issues include missing transpilation of the circuits submitted to IBM
Quantum systems and exceeding the maximum job size limit due to the submission
of all the circuits in one job.

</details>


### [197] [Covert Quantum Learning: Privately and Verifiably Learning from Quantum Data](https://arxiv.org/abs/2510.07193)
*Abhishek Anand,Matthias C. Caro,Ari Karchmer,Saachi Mutreja*

Main category: quant-ph

TL;DR: 提出量子学习理论中的隐蔽可验证学习模型，在无计算硬度假设下实现远程数据访问场景，展示多种隐私概念下的算法，证明量子优势在不可信远程数据下可私密且可验证获得。


<details>
  <summary>Details</summary>
Motivation: 解决远程访问量子计算和数据时验证数据正确性和确保学习者数据收集策略及结论隐私的问题。

Method: 提出量子学习理论中的隐蔽可验证学习模型，考虑策略隐蔽性和目标隐蔽性两种隐私概念，设计相关算法和协议。

Result: 给出通过经典阴影进行量子统计查询的策略隐蔽算法，以及多种场景下的目标隐蔽算法，证明Forrelation和Simon问题在隐蔽性约束下经典和量子查询的指数分离仍存在。

Conclusion: 所提模型和算法表明即使使用不可信的远程数据，也能私密且可验证地实现量子优势。

Abstract: Quantum learning from remotely accessed quantum compute and data must address
two key challenges: verifying the correctness of data and ensuring the privacy
of the learner's data-collection strategies and resulting conclusions. The
covert (verifiable) learning model of Canetti and Karchmer (TCC 2021) provides
a framework for endowing classical learning algorithms with such guarantees. In
this work, we propose models of covert verifiable learning in quantum learning
theory and realize them without computational hardness assumptions for remote
data access scenarios motivated by established quantum data advantages. We
consider two privacy notions: (i) strategy-covertness, where the eavesdropper
does not gain information about the learner's strategy; and (ii)
target-covertness, where the eavesdropper does not gain information about the
unknown object being learned. We show: Strategy-covert algorithms for making
quantum statistical queries via classical shadows; Target-covert algorithms for
learning quadratic functions from public quantum examples and private quantum
statistical queries, for Pauli shadow tomography and stabilizer state learning
from public multi-copy and private single-copy quantum measurements, and for
solving Forrelation and Simon's problem from public quantum queries and private
classical queries, where the adversary is a unidirectional or i.i.d.
ancilla-free eavesdropper. The lattermost results in particular establish that
the exponential separation between classical and quantum queries for
Forrelation and Simon's problem survives under covertness constraints. Along
the way, we design covert verifiable protocols for quantum data acquisition
from public quantum queries which may be of independent interest. Overall, our
models and corresponding algorithms demonstrate that quantum advantages are
privately and verifiably achievable even with untrusted, remote data.

</details>


### [198] [Accelerating Inference for Multilayer Neural Networks with Quantum Computers](https://arxiv.org/abs/2510.07195)
*Arthur G. Rattew,Po-Wei Huang,Naixu Guo,Lirandë Pira,Patrick Rebentrost*

Main category: quant-ph

TL;DR: 本文提出多层神经网络的全相干量子实现，分析不同量子数据访问机制下网络推理复杂度，展示不同条件下相比经典方法的加速效果。


<details>
  <summary>Details</summary>
Motivation: 解决容错量子处理单元（QPUs）融入现代深度学习管道不明确的问题。

Method: 提出基于ResNet的多层神经网络全相干量子实现，包含残差块、多滤波器二维卷积等；分析三种量子数据访问机制下网络推理复杂度。

Result: 无假设时，浅双线性风格网络比经典方法有二次加速；高效量子访问权重时，有四次加速；高效量子访问输入和权重时，可实现特定网络并给出推理成本。

Conclusion: 成功迈出将量子处理单元融入深度学习管道的一步，展示量子方法在神经网络中的优势。

Abstract: Fault-tolerant Quantum Processing Units (QPUs) promise to deliver exponential
speed-ups in select computational tasks, yet their integration into modern deep
learning pipelines remains unclear. In this work, we take a step towards
bridging this gap by presenting the first fully-coherent quantum implementation
of a multilayer neural network with non-linear activation functions. Our
constructions mirror widely used deep learning architectures based on ResNet,
and consist of residual blocks with multi-filter 2D convolutions, sigmoid
activations, skip-connections, and layer normalizations. We analyse the
complexity of inference for networks under three quantum data access regimes.
Without any assumptions, we establish a quadratic speedup over classical
methods for shallow bilinear-style networks. With efficient quantum access to
the weights, we obtain a quartic speedup over classical methods. With efficient
quantum access to both the inputs and the network weights, we prove that a
network with an $N$-dimensional vectorized input, $k$ residual block layers,
and a final residual-linear-pooling layer can be implemented with an error of
$\epsilon$ with $O(\text{polylog}(N/\epsilon)^k)$ inference cost.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [199] [From Neural Sensing to Stimulation: An Interdisciplinary Roadmap for Neurotechnology](https://arxiv.org/abs/2510.07116)
*Ruben Ruiz-Mateos Serrano,Joe G Troughton,Nima Mirkhani,Natalia Martinez,Massimo Mariello,Jordan Tsigarides,Simon Williamson,Juan Sapriza,Ioana Susnoschi Luca,Antonio Dominguez-Alfaro,Estelle Cuttaz,Nicole Thompson,Sydney Swedick,Latifah Almulla,Amparo Guemes*

Main category: cs.ET

TL;DR: 本文提出神经技术发展战略路线图，识别关键权衡，提出协作框架与时间表，以加速适应性神经技术发展。


<details>
  <summary>Details</summary>
Motivation: 神经技术有变革潜力，但实现需应对跨学科挑战，因此制定战略路线图。

Method: 识别五个跨领域权衡，分析技术领域对解决权衡的影响，提出统一协作创新与教育框架。

Result: 明确了跨领域权衡及技术领域的影响，提出统一框架、伦理和监管重点及克服瓶颈的时间表。

Conclusion: 该路线图将技术发展与转化和社会需求结合，有望加速公平、有效且面向未来的适应性神经技术发展，指导全球科研创新协作。

Abstract: Neurotechnologies are transforming how we measure, interpret, and modulate
brain-body interactions, integrating real-time sensing, computation, and
stimulation to enable precise physiological control. They hold transformative
potential across clinical and non-clinical domains, from treating disorders to
enhancing cognition and performance. Realizing this potential requires
navigating complex, interdisciplinary challenges spanning neuroscience,
materials science, device engineering, signal processing, computational
modelling, and regulatory and ethical frameworks. This Perspective presents a
strategic roadmap for neurotechnology development, created by early-career
researchers, highlighting their role at the intersection of disciplines and
their capacity to bridge traditional silos. We identify five cross-cutting
trade-offs that constrain progress across functionality, scalability,
adaptability, and translatability, and illustrate how technical domains
influence their resolution. Rather than a domain-specific review, we focus on
shared challenges and strategic opportunities that transcend disciplines. We
propose a unified framework for collaborative innovation and education,
highlight ethical and regulatory priorities, and outline a timeline for
overcoming key bottlenecks. By aligning technical development with
translational and societal needs, this roadmap aims to accelerate equitable,
effective, and future-ready adaptive neurotechnologies, guiding coordinated
efforts across the global research and innovation community.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [200] [Neu-RadBERT for Enhanced Diagnosis of Brain Injuries and Conditions](https://arxiv.org/abs/2510.06232)
*Manpreet Singh,Sean Macrae,Pierre-Marc Williams,Nicole Hung,Sabrina Araujo de Franca,Laurent Letourneau-Guillon,François-Martin Carrier,Bang Liu,Yiorgos Alexandros Cavayas*

Main category: q-bio.TO

TL;DR: 开发Neu - RadBERT模型从急性呼吸衰竭患者脑部影像放射报告提取诊断，与Llama - 2 - 13B对比，Neu - RadBERT尤其是采用过采样策略时表现更好。


<details>
  <summary>Details</summary>
Motivation: 开发分类算法从急性呼吸衰竭且接受有创机械通气患者的脑部影像自由文本放射报告中提取诊断。

Method: 开发并微调基于BERT的Neu - RadBERT模型对非结构化放射报告分类，从MIMIC - IV数据库提取相关报告，对部分报告手动标注，用三种策略微调模型，并与Llama - 2 - 13B对比。

Result: Neu - RadBERT尤其是采用过采样时诊断准确性显著提高，急性脑损伤诊断准确率达98.0%，Llama - 2 - 13B表现相对较差，二元分类准确率最高67.5%。

Conclusion: Neu - RadBERT是从放射报告准确可靠诊断神经疾病的有力工具，凸显基于Transformer的NLP模型从自由文本报告自动提取诊断的潜力。

Abstract: Objective: We sought to develop a classification algorithm to extract
diagnoses from free-text radiology reports of brain imaging performed in
patients with acute respiratory failure (ARF) undergoing invasive mechanical
ventilation. Methods: We developed and fine-tuned Neu-RadBERT, a BERT-based
model, to classify unstructured radiology reports. We extracted all the brain
imaging reports (computed tomography and magnetic resonance imaging) from
MIMIC-IV database, performed in patients with ARF. Initial manual labelling was
performed on a subset of reports for various brain abnormalities, followed by
fine-tuning Neu-RadBERT using three strategies: 1) baseline RadBERT, 2)
Neu-RadBERT with Masked Language Modeling (MLM) pretraining, and 3) Neu-RadBERT
with MLM pretraining and oversampling to address data skewness. We compared the
performance of this model to Llama-2-13B, an autoregressive LLM. Results: The
Neu-RadBERT model, particularly with oversampling, demonstrated significant
improvements in diagnostic accuracy compared to baseline RadBERT for brain
abnormalities, achieving up to 98.0% accuracy for acute brain injuries.
Llama-2-13B exhibited relatively lower performance, peaking at 67.5% binary
classification accuracy. This result highlights potential limitations of
current autoregressive LLMs for this specific classification task, though it
remains possible that larger models or further fine-tuning could improve
performance. Conclusion: Neu-RadBERT, enhanced through target domain
pretraining and oversampling techniques, offered a robust tool for accurate and
reliable diagnosis of neurological conditions from radiology reports. This
study underscores the potential of transformer-based NLP models in
automatically extracting diagnoses from free text reports with potential
applications to both research and patient care.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [201] [Delay Independent Safe Control with Neural Networks: Positive Lur'e Certificates for Risk Aware Autonomy](https://arxiv.org/abs/2510.06661)
*Hamidreza Montazeri Hedesh,Milad Siami*

Main category: eess.SY

TL;DR: 提出风险感知安全认证方法，针对两种风险建模，测试速度快且能认证SDP - IQC无法认证的区域。


<details>
  <summary>Details</summary>
Motivation: 为自主、基于学习的控制系统提供风险感知的安全认证方法。

Method: 针对状态/输入延迟和区间矩阵不确定性，用局部扇区边界对神经网络控制器建模，利用正性结构推导线性、与延迟无关的证书，采用并实现最先进的IQC NN验证流程。

Result: 基于正性的测试比基于SDP的IQC运行速度快几个数量级，能认证SDP - IQC无法认证的区域。

Conclusion: 该方法提供了可扩展的安全保证，能补充风险感知控制。

Abstract: We present a risk-aware safety certification method for autonomous, learning
enabled control systems. Focusing on two realistic risks, state/input delays
and interval matrix uncertainty, we model the neural network (NN) controller
with local sector bounds and exploit positivity structure to derive linear,
delay-independent certificates that guarantee local exponential stability
across admissible uncertainties. To benchmark performance, we adopt and
implement a state-of-the-art IQC NN verification pipeline. On representative
cases, our positivity-based tests run orders of magnitude faster than SDP-based
IQC while certifying regimes the latter cannot-providing scalable safety
guarantees that complement risk-aware control.

</details>


### [202] [Falsification-Driven Reinforcement Learning for Maritime Motion Planning](https://arxiv.org/abs/2510.06970)
*Marlon Müller,Florian Finkeldei,Hanna Krasowski,Murat Arcak,Matthias Althoff*

Main category: eess.SY

TL;DR: 提出一种基于证伪驱动的强化学习方法，为自主船舶生成对抗训练场景，提升规则遵守的一致性。


<details>
  <summary>Details</summary>
Motivation: 训练强化学习智能体遵守海上交通规则具有挑战性，现有训练场景创建困难且真实数据不足。

Method: 提出证伪驱动的强化学习方法，生成船舶违反海上交通规则的对抗训练场景，规则以信号时序逻辑规范表达。

Result: 在双船公海航行实验中，该方法能提供更相关的训练场景，实现更一致的规则遵守。

Conclusion: 所提方法在训练强化学习智能体遵守海上交通规则方面有效。

Abstract: Compliance with maritime traffic rules is essential for the safe operation of
autonomous vessels, yet training reinforcement learning (RL) agents to adhere
to them is challenging. The behavior of RL agents is shaped by the training
scenarios they encounter, but creating scenarios that capture the complexity of
maritime navigation is non-trivial, and real-world data alone is insufficient.
To address this, we propose a falsification-driven RL approach that generates
adversarial training scenarios in which the vessel under test violates maritime
traffic rules, which are expressed as signal temporal logic specifications. Our
experiments on open-sea navigation with two vessels demonstrate that the
proposed approach provides more relevant training scenarios and achieves more
consistent rule compliance.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [203] [Latent Representation Learning in Heavy-Ion Collisions with MaskPoint Transformer](https://arxiv.org/abs/2510.06691)
*Jing-Zong Zhang,Shuang Guo,Li-Lin Zhu,Lingxiao Wang,Guo-Liang Ma*

Main category: hep-ph

TL;DR: 本文提出基于Transformer的自编码器，采用两阶段训练范式从重离子碰撞数据提取特征，在区分碰撞系统任务中表现优于PointNet，证明该框架可用于重离子碰撞特征学习。


<details>
  <summary>Details</summary>
Motivation: 高能核物理中传统方法依赖选定的可观测量，可能错过数据中微妙但相关的物理结构，需要新方法从高维重离子碰撞数据中提取信息特征。

Method: 引入基于Transformer的自编码器，采用自监督预训练和监督微调的两阶段训练范式。

Result: 在区分大小碰撞系统任务中，该方法分类准确率显著高于PointNet，能捕捉复杂非线性关联，特征有强判别和解释能力。

Conclusion: 两阶段框架是重离子碰撞特征学习的通用且稳健基础，为夸克 - 胶子等离子体特性和其他涌现现象的分析提供可能。

Abstract: A central challenge in high-energy nuclear physics is to extract informative
features from the high-dimensional final-state data of heavy-ion collisions
(HIC) in order to enable reliable downstream analyses. Traditional approaches
often rely on selected observables, which may miss subtle but physically
relevant structures in the data. To address this, we introduce a
Transformer-based autoencoder trained with a two-stage paradigm:
self-supervised pre-training followed by supervised fine-tuning. The pretrained
encoder learns latent representations directly from unlabeled HIC data,
providing a compact and information-rich feature space that can be adapted to
diverse physics tasks. As a case study, we apply the method to distinguish
between large and small collision systems, where it achieves significantly
higher classification accuracy than PointNet. Principal component analysis and
SHAP interpretation further demonstrate that the autoencoder captures complex
nonlinear correlations beyond individual observables, yielding features with
strong discriminative and explanatory power. These results establish our
two-stage framework as a general and robust foundation for feature learning in
HIC, opening the door to more powerful analyses of quark--gluon plasma
properties and other emergent phenomena. The implementation is publicly
available at https://github.com/Giovanni-Sforza/MaskPoint-AMPT.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [204] [Inducing State Anxiety in LLM Agents Reproduces Human-Like Biases in Consumer Decision-Making](https://arxiv.org/abs/2510.06222)
*Ziv Ben-Zion,Zohar Elyoseph,Tobias Spiller,Teddy Lazebnik*

Main category: cs.HC

TL;DR: 研究测试大语言模型（LLMs）在模拟焦虑情境下购物任务表现，发现创伤提示会降低购物篮营养质量，揭示其类似人类的情感偏差。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs从文本生成器向自主代理发展，需探究其在现实情境中的可靠性，尤其关注是否存在类似人类因压力和焦虑导致决策偏差的情况。

Method: 让三个先进模型（ChatGPT - 5、Gemini 2.5、Claude 3.5 - Sonnet）在预算约束下进行购物任务，在接触创伤性叙事前后进行测试，共进行2250次运行。

Result: 创伤提示使购物篮营养质量持续下降，不同模型和预算下结果稳健。

Conclusion: 心理情境会系统改变LLMs的生成内容和行动，其存在类似人类的情感偏差，对数字健康、消费者安全和伦理AI部署有影响。

Abstract: Large language models (LLMs) are rapidly evolving from text generators to
autonomous agents, raising urgent questions about their reliability in
real-world contexts. Stress and anxiety are well known to bias human
decision-making, particularly in consumer choices. Here, we tested whether LLM
agents exhibit analogous vulnerabilities. Three advanced models (ChatGPT-5,
Gemini 2.5, Claude 3.5-Sonnet) performed a grocery shopping task under budget
constraints (24, 54, 108 USD), before and after exposure to anxiety-inducing
traumatic narratives. Across 2,250 runs, traumatic prompts consistently reduced
the nutritional quality of shopping baskets (Change in Basket Health Scores of
-0.081 to -0.126; all pFDR<0.001; Cohens d=-1.07 to -2.05), robust across
models and budgets. These results show that psychological context can
systematically alter not only what LLMs generate but also the actions they
perform. By reproducing human-like emotional biases in consumer behavior, LLM
agents reveal a new class of vulnerabilities with implications for digital
health, consumer safety, and ethical AI deployment.

</details>


### [205] [AgentBuilder: Exploring Scaffolds for Prototyping User Experiences of Interface Agents](https://arxiv.org/abs/2510.04452)
*Jenny T. Liang,Titus Barik,Jeffrey Nichols,Eldon Schoop,Ruijia Cheng*

Main category: cs.HC

TL;DR: 本文围绕生成式AI接口代理的用户体验原型设计展开，通过研究确定关键活动和系统能力，并用AgentBuilder验证需求。


<details>
  <summary>Details</summary>
Motivation: 为非AI工程师提供代理体验原型设计的支撑，让更多人参与贡献有价值观点。

Method: 对12名不同经验参与者进行需求调研确定关键活动和系统能力，用AgentBuilder对14名参与者进行实地原型设计研究。

Result: 确定了代理体验原型设计的关键活动和代理原型系统的期望能力，验证了设计需求并获得开发者原型设计的见解。

Conclusion: 探索了代理原型系统应具备的功能，为开发代理体验原型提供了参考。

Abstract: Interface agents powered by generative AI models (referred to as "agents")
can automate actions based on user commands. An important aspect of developing
agents is their user experience (i.e., agent experience). There is a growing
need to provide scaffolds for a broader set of individuals beyond AI engineers
to prototype agent experiences, since they can contribute valuable perspectives
to designing agent experiences. In this work, we explore the affordances agent
prototyping systems should offer by conducting a requirements elicitation study
with 12 participants with varying experience with agents. We identify key
activities in agent experience prototyping and the desired capabilities of
agent prototyping systems. We instantiate those capabilities in the
AgentBuilder design probe for agent prototyping. We conduct an in situ agent
prototyping study with 14 participants using AgentBuilder to validate the
design requirements and elicit insights on how developers prototype agents and
what their needs are in this process.

</details>


### [206] [A Multimodal GUI Architecture for Interfacing with LLM-Based Conversational Assistants](https://arxiv.org/abs/2510.06223)
*Hans G. W. van Dam*

Main category: cs.HC

TL;DR: 文章提供使GUI与基于大语言模型的语音助手交互的架构，评估本地部署开源大模型在语音多模态界面的效果。


<details>
  <summary>Details</summary>
Motivation: 多数生产应用未考虑语音交互，需要让GUI与语音助手交互的架构。

Method: 提出通过MCP使应用导航图和语义可用，利用MVVM模式的ViewModel向助手暴露应用能力的架构，评估本地开源大模型效果。

Result: 架构可实现全语音可访问性，确保语音输入与视觉界面可靠对齐和多模态一致反馈；近期小型开源模型在整体准确率上接近领先私有模型，需企业级硬件实现快速响应。

Conclusion: 所提架构可行，开源大模型在语音多模态界面有一定实用性。

Abstract: Advances in large language models (LLMs) and real-time speech recognition now
make it possible to issue any graphical user interface (GUI) action through
natural language and receive the corresponding system response directly through
the GUI. Most production applications were never designed with speech in mind.
This article provides a concrete architecture that enables GUIs to interface
with LLM-based speech-enabled assistants.
  The architecture makes an application's navigation graph and semantics
available through the Model Context Protocol (MCP). The ViewModel, part of the
MVVM (Model-View-ViewModel) pattern, exposes the application's capabilities to
the assistant by supplying both tools applicable to a currently visible view
and application-global tools extracted from the GUI tree router. This
architecture facilitates full voice accessibility while ensuring reliable
alignment between spoken input and the visual interface, accompanied by
consistent feedback across modalities. It future-proofs apps for upcoming OS
super assistants that employ computer use agents (CUAs) and natively consume
MCP if an application provides it.
  To address concerns about privacy and data security, the practical
effectiveness of locally deployable, open-weight LLMs for speech-enabled
multimodal UIs is evaluated. Findings suggest that recent smaller open-weight
models approach the performance of leading proprietary models in overall
accuracy and require enterprise-grade hardware for fast responsiveness.

</details>


### [207] [Exploring Human-AI Collaboration Using Mental Models of Early Adopters of Multi-Agent Generative AI Tools](https://arxiv.org/abs/2510.06224)
*Suchismita Naik,Austin L. Toombs,Amanda Snellinger,Scott Saponas,Amanda K. Hall*

Main category: cs.HC

TL;DR: 研究微软多智能体生成式AI早期采用者和开发者对工具的概念化理解，发现他们将其视为类似人类协作的“团队”，指出关键挑战，强调透明度重要性并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探究早期采用者和开发者如何概念化多智能体生成式AI工具，理解人机协作机制、协作动态和透明度。

Method: 对13位微软多智能体生成式AI技术早期采用者开发者进行半结构化访谈。

Result: 早期采用者将多智能体系统概念化为基于角色和任务的“团队”，识别出关键挑战，强调透明度重要性。

Conclusion: 给出未来研究方向，将CSCW方法扩展到智能体间和人机调解交互设计。

Abstract: With recent advancements in multi-agent generative AI (Gen AI), technology
organizations like Microsoft are adopting these complex tools, redefining AI
agents as active collaborators in complex workflows rather than as passive
tools. In this study, we investigated how early adopters and developers
conceptualize multi-agent Gen AI tools, focusing on how they understand
human-AI collaboration mechanisms, general collaboration dynamics, and
transparency in the context of AI tools. We conducted semi-structured
interviews with 13 developers, all early adopters of multi-agent Gen AI
technology who work at Microsoft. Our findings revealed that these early
adopters conceptualize multi-agent systems as "teams" of specialized role-based
and task-based agents, such as assistants or reviewers, structured similar to
human collaboration models and ranging from AI-dominant to AI-assisted,
user-controlled interactions. We identified key challenges, including error
propagation, unpredictable and unproductive agent loop behavior, and the need
for clear communication to mitigate the layered transparency issues. Early
adopters' perspectives about the role of transparency underscored its
importance as a way to build trust, verify and trace errors, and prevent
misuse, errors, and leaks. The insights and design considerations we present
contribute to CSCW research about collaborative mechanisms with capabilities
ranging from AI-dominant to AI-assisted interactions, transparency and
oversight strategies in human-agent and agent-agent interactions, and how
humans make sense of these multi-agent systems as dynamic, role-diverse
collaborators which are customizable for diverse needs and workflows. We
conclude with future research directions that extend CSCW approaches to the
design of inter-agent and human mediation interactions.

</details>


### [208] [Evaluating Node-tree Interfaces for AI Explainability](https://arxiv.org/abs/2510.06457)
*Lifei Wang,Natalie Friedman,Chengchao Zhu,Zeshu Zhu,S. Joy Mountford*

Main category: cs.HC

TL;DR: 研究对比节点树界面和聊天机器人界面在AI系统中的用户体验，发现节点树界面在头脑风暴等方面更优，自适应AI界面可增强透明度和用户信心。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型普及，确保可解释性和建立用户信任很关键，但以人为中心的设计在嵌入透明度和信任方面滞后，因此评估不同AI界面的性能。

Method: 采用设计驱动方法，引入节点树界面，对20名商业用户进行对比研究。

Result: 聊天机器人界面支持线性查询，节点树界面增强头脑风暴，能提高任务表现、决策支持和用户信任。

Conclusion: 自适应AI界面可显著增强AI系统的透明度和用户信心，为相关领域提供见解。

Abstract: As large language models (LLMs) become ubiquitous in workplace tools and
decision-making processes, ensuring explainability and fostering user trust are
critical. Although advancements in LLM engineering continue, human-centered
design is still catching up, particularly when it comes to embedding
transparency and trust into AI interfaces. This study evaluates user
experiences with two distinct AI interfaces - node-tree interfaces and chatbot
interfaces - to assess their performance in exploratory, follow-up inquiry,
decision-making, and problem-solving tasks. Our design-driven approach
introduces a node-tree interface that visually structures AI-generated
responses into hierarchically organized, interactive nodes, allowing users to
navigate, refine, and follow up on complex information. In a comparative study
with n=20 business users, we observed that while the chatbot interface
effectively supports linear, step-by-step queries, it is the node-tree
interface that enhances brainstorming. Quantitative and qualitative findings
indicate that node-tree interfaces not only improve task performance and
decision-making support but also promote higher levels of user trust by
preserving context. Our findings suggest that adaptive AI interfaces capable of
switching between structured visualizations and conversational formats based on
task requirements can significantly enhance transparency and user confidence in
AI-powered systems. This work contributes actionable insights to the fields of
human-robot interaction and AI design, particularly for enterprise applications
where trust-building is critical for teams.

</details>


### [209] [Emotionally Vulnerable Subtype of Internet Gaming Disorder: Measuring and Exploring the Pathology of Problematic Generative AI Use](https://arxiv.org/abs/2510.06908)
*Haocan Sun,Di Wua,Weizi Liu,Guoming Yua,Mike Yao*

Main category: cs.HC

TL;DR: 研究开发并验证PUGenAIS - 9量表，发现其匹配IGD情感脆弱亚型，支持用其识别GenAI问题使用，需用ICD模型反思数字成瘾。


<details>
  <summary>Details</summary>
Motivation: 解决GenAI使用可能过度病理化和GenAI成瘾概念不清晰问题，需实证工具和理论完善。

Method: 用中美样本（N = 1,508）进行验证性因子分析确定31项结构，选高负荷项得PUGenAIS - 9，在独立样本（N = 1,426）验证，进行测量不变性检验，用潜剖面分析和网络分析。

Result: PUGenAIS - 9结构稳定，匹配IGD情感脆弱亚型。

Conclusion: 支持用PUGenAIS - 9识别GenAI问题使用，需用ICD模型反思数字成瘾，使成瘾研究适应新媒体并避免过度病理化。

Abstract: Concerns over the potential over-pathologization of generative AI (GenAI) use
and the lack of conceptual clarity surrounding GenAI addiction call for
empirical tools and theoretical refinement. This study developed and validated
the PUGenAIS-9 (Problematic Use of Generative Artificial Intelligence Scale-9
items) and examined whether PUGenAIS reflects addiction-like patterns under the
Internet Gaming Disorder (IGD) framework. Using samples from China and the
United States (N = 1,508), we conducted confirmatory factor analysis and
identified a robust 31-item structure across nine IGD-based dimensions. We then
derived the PUGenAIS-9 by selecting the highest-loading items from each
dimension and validated its structure in an independent sample (N = 1,426).
Measurement invariance tests confirmed its stability across nationality and
gender. Person-centered (latent profile analysis) and variable-centered
(network analysis) approaches found that PUGenAIS matches the traits of the
emotionally vulnerable subtype of IGD, not the competence-based kind. These
results support using PUGenAIS-9 to identify problematic GenAI use and show the
need to rethink digital addiction with an ICD (infrastructures, content, and
device) model. This keeps addiction research responsive to new media while
avoiding over-pathologizing.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [210] [jmstate, a Flexible Python Package for Multi-State Joint Modeling](https://arxiv.org/abs/2510.07128)
*Félix Laplante,Christophe Ambroise,Estelle Kuhn,Sarah Lemler*

Main category: stat.ME

TL;DR: 提出统一纵向生物标志物建模和多状态事件过程的通用框架，开发推理程序和动态预测框架，通过模拟实验和案例研究验证其灵活性和性能，并提供开源库。


<details>
  <summary>Details</summary>
Motivation: 经典联合建模方法只能捕捉有限的事件动态，多状态联合模型更灵活，因此提出统一纵向生物标志物建模与多状态事件过程的框架。

Method: 提出统一框架，耦合非线性混合效应纵向子模型与多状态生存过程，推导全似然，基于随机梯度下降开发可扩展推理程序，引入动态预测框架。

Result: 模拟实验和生物医学案例研究证明框架能有效表示复杂的纵向和多状态事件动态。

Conclusion: 所提框架灵活且性能良好，开源库方便方法的复现和传播。

Abstract: Classical joint modeling approaches often rely on competing risks or
recurrent event formulations to account for complex real-world processes
involving evolving longitudinal markers and discrete event occurrences.
However, these frameworks typically capture only limited aspects of the
underlying event dynamics.
  Multi-state joint models offer a more flexible alternative by representing
full event histories through a network of possible transitions, including
recurrent cycles and terminal absorptions, all potentially influenced by
longitudinal covariates.
  In this paper, we propose a general framework that unifies longitudinal
biomarker modeling with multi-state event processes defined on arbitrary
directed graphs. Our approach accommodates both Markovian and semi-Markovian
transition structures, and extends classical joint models by coupling nonlinear
mixed-effects longitudinal submodels with multi-state survival processes via
shared latent structures.
  We derive the full likelihood and develop scalable inference procedures based
on stochastic gradient descent. Furthermore, we introduce a dynamic prediction
framework, enabling individualized risk assessments along complex
state-transition trajectories.
  To facilitate reproducibility and dissemination, we provide an open-source
Python library \texttt{jmstate} implementing the proposed methodology,
available on \href{https://pypi.org/project/jmstate/}{PyPI}. Simulation
experiments and a biomedical case study demonstrate the flexibility and
performance of the framework in representing complex longitudinal and
multi-state event dynamics. The full Python notebooks used to reproduce the
experiments as well as the source code of this paper are available on
\href{https://gitlab.com/felixlaplante0/jmstate-paper/}{GitLab}.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [211] [Multi-hop Deep Joint Source-Channel Coding with Deep Hash Distillation for Semantically Aligned Image Retrieval](https://arxiv.org/abs/2510.06868)
*Didrik Bergström,Deniz Gündüz,Onur Günlü*

Main category: cs.IT

TL;DR: 通过训练带有预训练深度哈希蒸馏模块的深度联合源信道编码（DeepJSCC）编解码器对，研究多跳加性高斯白噪声（AWGN）信道上的图像传输，提高语义一致性和感知重建质量。


<details>
  <summary>Details</summary>
Motivation: 在多跳AWGN信道上进行图像传输，满足安全导向应用需求，解决经典DeepJSCC噪声累积问题，提高感知重建质量。

Method: 训练DeepJSCC模块，同时降低均方误差（MSE），并最小化源图像和重建图像的DHD哈希之间的余弦距离。

Result: 在不同多跳设置下，语义对齐显著提高了感知质量，通过学习感知图像块相似度（LPIPS）度量得到验证。

Conclusion: 该方法能有效提高多跳AWGN信道上图像传输的感知重建质量，减少噪声累积影响。

Abstract: We consider image transmission via deep joint source-channel coding
(DeepJSCC) over multi-hop additive white Gaussian noise (AWGN) channels by
training a DeepJSCC encoder-decoder pair with a pre-trained deep hash
distillation (DHD) module to semantically cluster images, facilitating
security-oriented applications through enhanced semantic consistency and
improving the perceptual reconstruction quality. We train the DeepJSCC module
to both reduce mean square error (MSE) and minimize cosine distance between DHD
hashes of source and reconstructed images. Significantly improved perceptual
quality as a result of semantic alignment is illustrated for different
multi-hop settings, for which classical DeepJSCC may suffer from noise
accumulation, measured by the learned perceptual image patch similarity (LPIPS)
metric.

</details>


### [212] [Spectral Graph Clustering under Differential Privacy: Balancing Privacy, Accuracy, and Efficiency](https://arxiv.org/abs/2510.07136)
*Mohamed Seif,Antti Koskela,H. Vincent Poor,Andrea J. Goldsmith*

Main category: cs.IT

TL;DR: 研究边差分隐私下的谱图聚类问题，提出三种机制，分析有隐私保证和错误率刻画，实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 解决边差分隐私下谱图聚类问题。

Method: 提出三种机制，包括图扰动、私有图投影、噪声幂迭代法。

Result: 分析提供严格隐私保证和错误率刻画，实验验证理论并展示隐私 - 效用权衡。

Conclusion: 所提机制能有效解决边差分隐私下谱图聚类问题，存在隐私 - 效用权衡。

Abstract: We study the problem of spectral graph clustering under edge differential
privacy (DP). Specifically, we develop three mechanisms: (i) graph perturbation
via randomized edge flipping combined with adjacency matrix shuffling, which
enforces edge privacy while preserving key spectral properties of the graph.
Importantly, shuffling considerably amplifies the guarantees: whereas flipping
edges with a fixed probability alone provides only a constant epsilon edge DP
guarantee as the number of nodes grows, the shuffled mechanism achieves
(epsilon, delta) edge DP with parameters that tend to zero as the number of
nodes increase; (ii) private graph projection with additive Gaussian noise in a
lower-dimensional space to reduce dimensionality and computational complexity;
and (iii) a noisy power iteration method that distributes Gaussian noise across
iterations to ensure edge DP while maintaining convergence. Our analysis
provides rigorous privacy guarantees and a precise characterization of the
misclassification error rate. Experiments on synthetic and real-world networks
validate our theoretical analysis and illustrate the practical privacy-utility
trade-offs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [213] [DeepXPalm: Tilt and Position Rendering using Palm-worn Haptic Display and CNN-based Tactile Pattern Recognition](https://arxiv.org/abs/2204.03521)
*Altamirano Cabrera Miguel,Sautenkov Oleg,Tirado Jonathan,Fedoseev Aleksey,Kopanev Pavel,Kajimoto Hiroyuki,Tsetserukou Dzmitry*

Main category: cs.RO

TL;DR: 本文提出用于塑料移液管的遥操作触觉反馈系统，用CNN检测倾斜和位置，提升用户识别率。


<details>
  <summary>Details</summary>
Motivation: 可变形物体遥操作需高精度和灵活性，物体形状动态变化会导致定位误差，需解决倾斜角和位置分类问题以提供清晰触觉模式。

Method: 提出基于卷积神经网络（CNN）的方法，在遥操作系统中检测抓取可变形物体时的倾斜和位置，CNN根据识别数据生成掩码以呈现多触点触觉刺激。

Result: 使用CNN算法和预设掩码，用户对倾斜和位置的识别率从直接数据的9.67%提高到82.5%。

Conclusion: 基于CNN的方法能有效提升可变形物体遥操作中用户对物体倾斜和位置的识别率。

Abstract: Telemanipulation of deformable objects requires high precision and dexterity
from the users, which can be increased by kinesthetic and tactile feedback.
However, the object shape can change dynamically, causing ambiguous perception
of its alignment and hence errors in the robot positioning. Therefore, the tilt
angle and position classification problem has to be solved to present a clear
tactile pattern to the user. This work presents a telemanipulation system for
plastic pipettes consisting of a multi-contact haptic device LinkGlide to
deliver haptic feedback at the users' palm and two tactile sensors array
embedded in the 2-finger Robotiq gripper. We propose a novel approach based on
Convolutional Neural Networks (CNN) to detect the tilt and position while
grasping deformable objects. The CNN generates a mask based on recognized tilt
and position data to render further multi-contact tactile stimuli provided to
the user during the telemanipulation. The study has shown that using the CNN
algorithm and the preset mask, tilt, and position recognition by users is
increased from 9.67% using the direct data to 82.5%.

</details>


### [214] [TiltXter: CNN-based Electro-tactile Rendering of Tilt Angle for Telemanipulation of Pasteur Pipettes](https://arxiv.org/abs/2409.15838)
*Miguel Altamirano Cabrera,Jonathan Tirado,Aleksey Fedoseev,Oleg Sautenkov,Vladimir Poliakov,Pavel Kopanev,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 本文提出基于CNN的可变形物体倾斜检测方法用于移液管遥操作，提高了用户倾斜识别率和遥操作成功率。


<details>
  <summary>Details</summary>
Motivation: 可变形物体抓取时形状变化大，导致遥操作中感知模糊、定位和操作出错，需研究传感器数据解码为触觉刺激的方法。

Method: 构建含触觉和电刺激阵列的移液管遥操作系统，提出基于CNN的可变形物体倾斜检测方法，根据识别的倾斜数据生成触觉模式。

Result: 使用CNN算法，用户倾斜识别率从23.13%提升到57.9%，遥操作成功率从53.12%提升到92.18%。

Conclusion: 基于CNN的方法能有效提高可变形物体遥操作的精度和成功率。

Abstract: The shape of deformable objects can change drastically during grasping by
robotic grippers, causing an ambiguous perception of their alignment and hence
resulting in errors in robot positioning and telemanipulation. Rendering clear
tactile patterns is fundamental to increasing users' precision and dexterity
through tactile haptic feedback during telemanipulation. Therefore, different
methods have to be studied to decode the sensors' data into haptic stimuli.
This work presents a telemanipulation system for plastic pipettes that consists
of a Force Dimension Omega.7 haptic interface endowed with two
electro-stimulation arrays and two tactile sensor arrays embedded in the
2-finger Robotiq gripper. We propose a novel approach based on convolutional
neural networks (CNN) to detect the tilt of deformable objects. The CNN
generates a tactile pattern based on recognized tilt data to render further
electro-tactile stimuli provided to the user during the telemanipulation. The
study has shown that using the CNN algorithm, tilt recognition by users
increased from 23.13\% with the downsized data to 57.9%, and the success rate
during teleoperation increased from 53.12% using the downsized data to 92.18%
using the tactile patterns generated by the CNN.

</details>


### [215] [Constrained Natural Language Action Planning for Resilient Embodied Systems](https://arxiv.org/abs/2510.06357)
*Grayson Byrd,Corban Rivera,Bethany Kemp,Meghan Booker,Aurora Schmidt,Celso M de Melo,Lalithkumar Seenivasan,Mathias Unberath*

Main category: cs.RO

TL;DR: 提出结合符号规划监督增强大语言模型（LLM）规划器的机器人规划方法，在模拟和真实环境验证有效，提高了LLM规划可靠性、可重复性和透明度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM规划存在幻觉、缺乏透明度和可重复性，符号规划难以应对现实任务复杂性和模糊性，需改进机器人规划方法。

Method: 引入用符号规划监督增强LLM规划器的新方法，提供定义硬约束的透明方法。

Result: 在ALFWorld基准测试中接近99%成功率，在真实四足机器人任务中成功率达100%，优于纯LLM和符号规划器。

Conclusion: 该方法有效增强了基于LLM的机器人规划器的可靠性、可重复性和透明度，保留其灵活性和泛化能力，有助于构建有弹性的具身智能系统。

Abstract: Replicating human-level intelligence in the execution of embodied tasks
remains challenging due to the unconstrained nature of real-world environments.
Novel use of large language models (LLMs) for task planning seeks to address
the previously intractable state/action space of complex planning tasks, but
hallucinations limit their reliability, and thus, viability beyond a research
context. Additionally, the prompt engineering required to achieve adequate
system performance lacks transparency, and thus, repeatability. In contrast to
LLM planning, symbolic planning methods offer strong reliability and
repeatability guarantees, but struggle to scale to the complexity and ambiguity
of real-world tasks. We introduce a new robotic planning method that augments
LLM planners with symbolic planning oversight to improve reliability and
repeatability, and provide a transparent approach to defining hard constraints
with considerably stronger clarity than traditional prompt engineering.
Importantly, these augmentations preserve the reasoning capabilities of LLMs
and retain impressive generalization in open-world environments. We demonstrate
our approach in simulated and real-world environments. On the ALFWorld planning
benchmark, our approach outperforms current state-of-the-art methods, achieving
a near-perfect 99% success rate. Deployment of our method to a real-world
quadruped robot resulted in 100% task success compared to 50% and 30% for pure
LLM and symbolic planners across embodied pick and place tasks. Our approach
presents an effective strategy to enhance the reliability, repeatability and
transparency of LLM-based robot planners while retaining their key strengths:
flexibility and generalizability to complex real-world environments. We hope
that this work will contribute to the broad goal of building resilient embodied
intelligent systems.

</details>


### [216] [Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications](https://arxiv.org/abs/2510.07077)
*Kento Kawaharazuka,Jihoon Oh,Jun Yamada,Ingmar Posner,Yuke Zhu*

Main category: cs.RO

TL;DR: 本文对VLA模型进行全面综述，涵盖软件和硬件组件，为机器人社区应用VLA到实际系统提供指导。


<details>
  <summary>Details</summary>
Motivation: 在利用大语言模型和视觉 - 语言模型推动机器人发展的背景下，以往调查聚焦单一，需要全面综述以指导VLA在机器人领域的应用。

Method: 对VLA系统的软件和硬件组件进行系统回顾，包括策略和架构转变、构建模块、处理技术、学习范式等，还回顾常用机器人平台、数据收集等。

Result: 完成对VLA的全面综述，提供各方面信息，所有参考文献按多种类别分类展示在项目网站。

Conclusion: 能为机器人社区将VLA应用于实际机器人系统提供实用指导。

Abstract: Amid growing efforts to leverage advances in large language models (LLMs) and
vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models
have recently gained significant attention. By unifying vision, language, and
action data at scale, which have traditionally been studied separately, VLA
models aim to learn policies that generalise across diverse tasks, objects,
embodiments, and environments. This generalisation capability is expected to
enable robots to solve novel downstream tasks with minimal or no additional
task-specific data, facilitating more flexible and scalable real-world
deployment. Unlike previous surveys that focus narrowly on action
representations or high-level model architectures, this work offers a
comprehensive, full-stack review, integrating both software and hardware
components of VLA systems. In particular, this paper provides a systematic
review of VLAs, covering their strategy and architectural transition,
architectures and building blocks, modality-specific processing techniques, and
learning paradigms. In addition, to support the deployment of VLAs in
real-world robotic applications, we also review commonly used robot platforms,
data collection strategies, publicly available datasets, data augmentation
methods, and evaluation benchmarks. Throughout this comprehensive survey, this
paper aims to offer practical guidance for the robotics community in applying
VLAs to real-world robotic systems. All references categorized by training
approach, evaluation method, modality, and dataset are available in the table
on our project website: https://vla-survey.github.io .

</details>


### [217] [A Digital Twin Framework for Metamorphic Testing of Autonomous Driving Systems Using Generative Model](https://arxiv.org/abs/2510.07133)
*Tony Zhang,Burak Kantarci,Umair Siddique*

Main category: cs.RO

TL;DR: 本文提出数字孪生驱动的变质测试框架，结合数字孪生和AI图像生成模型，生成多样驾驶场景，在Udacity模拟器验证，提升测试效果。


<details>
  <summary>Details</summary>
Motivation: 传统测试方法有局限性，难以确保自动驾驶汽车安全，需新测试方案。

Method: 引入数字孪生驱动的变质测试框架，结合数字孪生和AI图像生成模型，定义三个变质关系。

Result: 在Udacity模拟器验证，该方法相比基线方法有最高的真阳性率、F1分数和精度。

Conclusion: 集成数字孪生和AI场景生成对自动驾驶车辆安全测试有价值。

Abstract: Ensuring the safety of self-driving cars remains a major challenge due to the
complexity and unpredictability of real-world driving environments. Traditional
testing methods face significant limitations, such as the oracle problem, which
makes it difficult to determine whether a system's behavior is correct, and the
inability to cover the full range of scenarios an autonomous vehicle may
encounter. In this paper, we introduce a digital twin-driven metamorphic
testing framework that addresses these challenges by creating a virtual replica
of the self-driving system and its operating environment. By combining digital
twin technology with AI-based image generative models such as Stable Diffusion,
our approach enables the systematic generation of realistic and diverse driving
scenes. This includes variations in weather, road topology, and environmental
features, all while maintaining the core semantics of the original scenario.
The digital twin provides a synchronized simulation environment where changes
can be tested in a controlled and repeatable manner. Within this environment,
we define three metamorphic relations inspired by real-world traffic rules and
vehicle behavior. We validate our framework in the Udacity self-driving
simulator and demonstrate that it significantly enhances test coverage and
effectiveness. Our method achieves the highest true positive rate (0.719), F1
score (0.689), and precision (0.662) compared to baseline approaches. This
paper highlights the value of integrating digital twins with AI-powered
scenario generation to create a scalable, automated, and high-fidelity testing
solution for autonomous vehicle safety.

</details>


### [218] [TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking](https://arxiv.org/abs/2510.07134)
*Jiahang Liu,Yunpeng Qi,Jiazhao Zhang,Minghan Li,Shaoan Wang,Kui Wu,Hanjing Ye,Hong Zhang,Zhibo Chen,Fangwei Zhong,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: 提出TrackVLA++模型解决现有具身视觉跟踪方法缺乏空间推理和有效时间记忆的问题，实验显示其在多场景有优异表现和零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有具身视觉跟踪方法缺乏显式空间推理和有效时间记忆，在严重遮挡或有相似干扰物时易失败。

Method: 提出TrackVLA++模型，包含空间推理机制Polar - CoT和目标识别内存TIM。Polar - CoT推断目标相对位置并编码为极坐标令牌，TIM采用门控更新策略保存目标长时记忆。

Result: TrackVLA++在公共基准测试中达到了最先进的性能，在EVT - Bench DT分割上超越先前领先方法。且有较强零样本泛化能力。

Conclusion: TrackVLA++能有效提升具身视觉跟踪能力，可在动态和遮挡场景中实现鲁棒的现实世界跟踪。

Abstract: Embodied Visual Tracking (EVT) is a fundamental ability that underpins
practical applications, such as companion robots, guidance robots and service
assistants, where continuously following moving targets is essential. Recent
advances have enabled language-guided tracking in complex and unstructured
scenes. However, existing approaches lack explicit spatial reasoning and
effective temporal memory, causing failures under severe occlusions or in the
presence of similar-looking distractors. To address these challenges, we
present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances
embodied visual tracking with two key modules, a spatial reasoning mechanism
and a Target Identification Memory (TIM). The reasoning module introduces a
Chain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative
position and encodes it as a compact polar-coordinate token for action
prediction. Guided by these spatial priors, the TIM employs a gated update
strategy to preserve long-horizon target memory, ensuring spatiotemporal
consistency and mitigating target loss during extended occlusions. Extensive
experiments show that TrackVLA++ achieves state-of-the-art performance on
public benchmarks across both egocentric and multi-camera settings. On the
challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading
approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong
zero-shot generalization, enabling robust real-world tracking in dynamic and
occluded scenarios.

</details>


### [219] [TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics](https://arxiv.org/abs/2510.07181)
*Yi Han,Cheng Chi,Enshen Zhou,Shanyu Rong,Jingkun An,Pengwei Wang,Zhongyuan Wang,Lu Sheng,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出TIGeR框架将VLMs转变为几何计算机，结合数据集和训练方法实现SOTA性能与厘米级精度。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在空间推理上缺乏计算精度，无法满足现实机器人需求，当前方法未能利用深度传感器和相机校准的度量线索。

Method: 提出TIGeR框架，让VLMs通过外部工具进行精确几何计算；引入TIGeR - 300K数据集；采用结合监督微调（SFT）和强化微调（RFT）与分层奖励设计的两阶段训练管道。

Result: TIGeR在几何推理基准测试中达到SOTA性能，在现实机器人操作任务中展现厘米级精度。

Conclusion: TIGeR框架有效将VLMs从感知估计器转变为几何计算机，提升计算精度，适用于现实机器人任务。

Abstract: Vision-Language Models (VLMs) have shown remarkable capabilities in spatial
reasoning, yet they remain fundamentally limited to qualitative precision and
lack the computational precision required for real-world robotics. Current
approaches fail to leverage metric cues from depth sensors and camera
calibration, instead reducing geometric problems to pattern recognition tasks
that cannot deliver the centimeter-level accuracy essential for robotic
manipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel
framework that transforms VLMs from perceptual estimators to geometric
computers by enabling them to generate and execute precise geometric
computations through external tools. Rather than attempting to internalize
complex geometric operations within neural networks, TIGeR empowers models to
recognize geometric reasoning requirements, synthesize appropriate
computational code, and invoke specialized libraries for exact calculations. To
support this paradigm, we introduce TIGeR-300K, a comprehensive
tool-invocation-oriented dataset covering point transformations, pose
estimation, trajectory generation, and spatial compatibility verification,
complete with tool invocation sequences and intermediate computations. Through
a two-stage training pipeline combining supervised fine-tuning (SFT) and
reinforcement fine-tuning (RFT) with our proposed hierarchical reward design,
TIGeR achieves SOTA performance on geometric reasoning benchmarks while
demonstrating centimeter-level precision in real-world robotic manipulation
tasks.

</details>


### [220] [HyPlan: Hybrid Learning-Assisted Planning Under Uncertainty for Safe Autonomous Driving](https://arxiv.org/abs/2510.07210)
*Donald Pfaffmann,Matthias Klusch,Marcel Steinmetz*

Main category: cs.RO

TL;DR: 提出HyPlan混合学习辅助规划方法解决自动驾驶汽车在部分可观测交通环境中的无碰撞导航问题，实验显示其比基线更安全、比替代在线POMDP规划器更快。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶汽车在部分可观测交通环境中的无碰撞导航问题。

Method: 结合多智能体行为预测、带近端策略优化的深度强化学习和带启发式基于置信度垂直剪枝的近似在线POMDP规划。

Result: 在CARLA - CTS2行人关键交通场景基准测试中，HyPlan导航比相关基线更安全，且比替代在线POMDP规划器执行速度显著更快。

Conclusion: HyPlan方法在安全性和执行速度上有优势。

Abstract: We present a novel hybrid learning-assisted planning method, named HyPlan,
for solving the collision-free navigation problem for self-driving cars in
partially observable traffic environments. HyPlan combines methods for
multi-agent behavior prediction, deep reinforcement learning with proximal
policy optimization and approximated online POMDP planning with heuristic
confidence-based vertical pruning to reduce its execution time without
compromising safety of driving. Our experimental performance analysis on the
CARLA-CTS2 benchmark of critical traffic scenarios with pedestrians revealed
that HyPlan may navigate safer than selected relevant baselines and perform
significantly faster than considered alternative online POMDP planners.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [221] [DynBenchmark: Customizable Ground Truths to Benchmark Community Detection and Tracking in Temporal Networks](https://arxiv.org/abs/2510.06245)
*Laurent Brisson,Cécile Bothorel,Nicolas Duminy*

Main category: cs.SI

TL;DR: 提出新的以社区为中心的模型生成可定制的演化社区结构，用于测试社区检测算法并提供相关工具和指标。


<details>
  <summary>Details</summary>
Motivation: 现有基准常忽略追踪现实网络中社区的演化，需新方法解决。

Method: 提出新的社区中心模型生成可定制的演化社区结构和底层时间网络，用该基准测试三种方法。

Result: 用基准测试了三种方法，衡量其追踪节点聚类成员和检测社区演化的性能。

Conclusion: 提供Python库、绘图工具和验证指标，便于比较真实情况与算法结果以检测动态社区。

Abstract: Graph models help understand network dynamics and evolution. Creating graphs
with controlled topology and embedded partitions is a common strategy for
evaluating community detection algorithms. However, existing benchmarks often
overlook the need to track the evolution of communities in real-world networks.
To address this, a new community-centered model is proposed to generate
customizable evolving community structures where communities can grow, shrink,
merge, split, appear or disappear. This benchmark also generates the underlying
temporal network, where nodes can appear, disappear, or move between
communities. The benchmark has been used to test three methods, measuring their
performance in tracking nodes' cluster membership and detecting community
evolution. Python libraries, drawing utilities, and validation metrics are
provided to compare ground truth with algorithm results for detecting dynamic
communities.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [222] [Exposing Citation Vulnerabilities in Generative Engines](https://arxiv.org/abs/2510.06823)
*Riku Mochizuki,Shusuke Komatsu,Souta Noguchi,Kazuto Ataka*

Main category: cs.CR

TL;DR: 分析生成引擎答案，引入评估标准揭示中毒攻击威胁，通过实验发现美政治答案风险高，探讨缓解威胁方法。


<details>
  <summary>Details</summary>
Motivation: 现有引文评估研究未关注选择哪些网络来源作为引文以防御中毒攻击，填补此空白。

Method: 引入利用答案中引文信息评估中毒威胁的标准，对引文发布者属性分类以估计内容注入障碍；在美日政治领域实验。

Result: 美国政治答案中官方政党网站引文占比25%-45%，日本为60%-65%，美国政治答案中毒攻击风险更高；低内容注入障碍来源常被引用但答案内容反映差。

Conclusion: 探讨主要来源发布者增加其网页内容在答案中曝光的方法，且知名技术受语言差异限制。

Abstract: We analyze answers generated by generative engines (GEs) from the
perspectives of citation publishers and the content-injection barrier, defined
as the difficulty for attackers to manipulate answers to user prompts by
placing malicious content on the web. GEs integrate two functions: web search
and answer generation that cites web pages using large language models. Because
anyone can publish information on the web, GEs are vulnerable to poisoning
attacks. Existing studies of citation evaluation focus on how faithfully answer
content reflects cited sources, leaving unexamined which web sources should be
selected as citations to defend against poisoning attacks. To fill this gap, we
introduce evaluation criteria that assess poisoning threats using the citation
information contained in answers. Our criteria classify the publisher
attributes of citations to estimate the content-injection barrier thereby
revealing the threat of poisoning attacks in current GEs. We conduct
experiments in political domains in Japan and the United States (U.S.) using
our criteria and show that citations from official party websites (primary
sources) are approximately \(25\%\)--\(45\%\) in the U.S. and
\(60\%\)--\(65\%\) in Japan, indicating that U.S. political answers are at
higher risk of poisoning attacks. We also find that sources with low
content-injection barriers are frequently cited yet are poorly reflected in
answer content. To mitigate this threat, we discuss how publishers of primary
sources can increase exposure of their web content in answers and show that
well-known techniques are limited by language differences.

</details>


### [223] [Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation](https://arxiv.org/abs/2510.06605)
*Shuo Shao,Yiming Li,Hongwei Yao,Yifei Chen,Yuchen Yang,Zhan Qin*

Main category: cs.CR

TL;DR: LLM版权保护受关注，现有黑盒指纹方法不佳，提出ZeroPrint方法解决此问题，实验表明其效果和鲁棒性达最优。


<details>
  <summary>Details</summary>
Motivation: 开发大语言模型投资大，版权保护重要，但现有黑盒指纹方法难以生成独特指纹，需改进。

Method: 利用Fisher信息理论，证明模型输入梯度比输出更适合指纹提取，提出ZeroPrint，用零阶估计逼近梯度，通过语义保留的单词替换模拟输入扰动来估计模型雅可比矩阵作为指纹。

Result: 在标准基准上的实验显示，ZeroPrint效果和鲁棒性达到最优，显著优于现有黑盒方法。

Conclusion: ZeroPrint能有效解决现有黑盒指纹方法的问题，可用于大语言模型的版权保护。

Abstract: The substantial investment required to develop Large Language Models (LLMs)
makes them valuable intellectual property, raising significant concerns about
copyright protection. LLM fingerprinting has emerged as a key technique to
address this, which aims to verify a model's origin by extracting an intrinsic,
unique signature (a "fingerprint") and comparing it to that of a source model
to identify illicit copies. However, existing black-box fingerprinting methods
often fail to generate distinctive LLM fingerprints. This ineffectiveness
arises because black-box methods typically rely on model outputs, which lose
critical information about the model's unique parameters due to the usage of
non-linear functions. To address this, we first leverage Fisher Information
Theory to formally demonstrate that the gradient of the model's input is a more
informative feature for fingerprinting than the output. Based on this insight,
we propose ZeroPrint, a novel method that approximates these information-rich
gradients in a black-box setting using zeroth-order estimation. ZeroPrint
overcomes the challenge of applying this to discrete text by simulating input
perturbations via semantic-preserving word substitutions. This operation allows
ZeroPrint to estimate the model's Jacobian matrix as a unique fingerprint.
Experiments on the standard benchmark show ZeroPrint achieves a
state-of-the-art effectiveness and robustness, significantly outperforming
existing black-box methods.

</details>


### [224] [Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard](https://arxiv.org/abs/2508.20504)
*Guan-Yan Yang,Jui-Ning Chen,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: 本文提出基于图结构学习（GSL）的保障框架以抵御能源互联网（IoE）网络的对抗攻击，展示其优越性并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 能源互联网的互联性使其关键基础设施面临复杂网络威胁，且威胁带来的公共安全后果严重，需弹性解决方案。

Method: 从网络级保障角度，提出基于GSL的保障框架，联合优化图拓扑和节点表示。

Result: 通过概念概述、架构讨论和安全数据集案例研究，证明GSL比代表性方法有更强的鲁棒性。

Conclusion: GSL有潜力增强未来IoE网络的弹性和可靠性，同时指出该领域关键挑战和未来研究方向。

Abstract: The Internet of Energy (IoE) integrates IoT-driven digital communication with
power grids to enable efficient and sustainable energy systems. Still, its
interconnectivity exposes critical infrastructure to sophisticated cyber
threats, including adversarial attacks designed to bypass traditional
safeguards. Unlike general IoT risks, IoE threats have heightened public safety
consequences, demanding resilient solutions. From the networking-level
safeguard perspective, we propose a Graph Structure Learning (GSL)-based
safeguards framework that jointly optimizes graph topology and node
representations to resist adversarial network model manipulation inherently.
Through a conceptual overview, architectural discussion, and case study on a
security dataset, we demonstrate GSL's superior robustness over representative
methods, offering practitioners a viable path to secure IoE networks against
evolving attacks. This work highlights the potential of GSL to enhance the
resilience and reliability of future IoE networks for practitioners managing
critical infrastructure. Lastly, we identify key open challenges and propose
future research directions in this novel research area.

</details>


### [225] [Distilling Lightweight Language Models for C/C++ Vulnerabilities](https://arxiv.org/abs/2510.06645)
*Zhiyuan Wei,Xiaoxuan Yang,Jing Sun,Zijian Zhang*

Main category: cs.CR

TL;DR: 提出FineSec框架利用知识蒸馏实现C/C++代码漏洞检测，评估显示其优势且公开相关资源。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统复杂度增加，安全漏洞频发，LLMs在自动化代码漏洞检测方面潜力待挖掘。

Method: 提出FineSec框架，利用知识蒸馏将大教师模型知识转移到小学生模型，集成数据准备、训练等环节到统一单任务工作流。

Result: 在C/C++代码库上的广泛评估表明，FineSec在识别复杂漏洞和逻辑缺陷方面优于基础模型和更大的LLMs。

Conclusion: FineSec是解决现实世界软件安全问题的实用且可扩展的解决方案。

Abstract: The increasing complexity of modern software systems exacerbates the
prevalence of security vulnerabilities, posing risks of severe breaches and
substantial economic loss. Consequently, robust code vulnerability detection is
essential for software security. While Large Language Models (LLMs) have
demonstrated remarkable capabilities in natural language processing, their
potential for automated code vulnerability detection remains underexplored.
This paper presents FineSec, a novel framework that harnesses LLMs through
knowledge distillation to enable efficient and precise vulnerability
identification in C/C++ codebases. FineSec utilizes knowledge distillation to
transfer expertise from large teacher models to compact student models,
achieving high accuracy with minimal computational cost. By integrating data
preparation, training, evaluation, and continuous learning into a unified,
single-task workflow, FineSec offers a streamlined approach. Extensive
evaluations on C/C++ codebases demonstrate its superiority over both base
models and larger LLMs in identifying complex vulnerabilities and logical
flaws, establishing FineSec as a practical and scalable solution for real-world
software security. To facilitate reproducibility, the datasets, source code,
and experimental results are made publicly available at:
https://github.com/yangxiaoxuan123/FineSec_detect.

</details>


### [226] [From Description to Detection: LLM based Extendable O-RAN Compliant Blind DoS Detection in 5G and Beyond](https://arxiv.org/abs/2510.06530)
*Thusitha Dayaratne,Ngoc Duy Pham,Viet Vo,Shangqi Lai,Sharif Abuadbba,Hajime Suzuki,Xingliang Yuan,Carsten Rudolph*

Main category: cs.CR

TL;DR: 文章提出基于大语言模型（LLM）的零样本异常检测框架，用于检测移动控制平面协议安全威胁，经评估性能优越，能满足O-RAN实时性要求。


<details>
  <summary>Details</summary>
Motivation: 现有移动控制平面协议异常检测方法有需大量训练数据、预定义规则和可解释性有限等局限，需新方法。

Method: 在Open Radio Access Network (O-RAN)架构下，利用大语言模型（LLM）零样本模式、无序数据和简短自然语言攻击描述构建异常检测框架。

Result: 分析了对提示变化的鲁棒性，展示攻击描述自动化的实用性，表明检测质量依赖描述语义完整性；利用RRC/NAS数据集评估，比较开源和专有LLM实现，显示出优越攻击检测性能。

Conclusion: 框架在O-RAN实时约束下实用，有检测其他第3层攻击的潜力。

Abstract: The quality and experience of mobile communication have significantly
improved with the introduction of 5G, and these improvements are expected to
continue beyond the 5G era. However, vulnerabilities in control-plane
protocols, such as Radio Resource Control (RRC) and Non-Access Stratum (NAS),
pose significant security threats, such as Blind Denial of Service (DoS)
attacks. Despite the availability of existing anomaly detection methods that
leverage rule-based systems or traditional machine learning methods, these
methods have several limitations, including the need for extensive training
data, predefined rules, and limited explainability. Addressing these
challenges, we propose a novel anomaly detection framework that leverages the
capabilities of Large Language Models (LLMs) in zero-shot mode with unordered
data and short natural language attack descriptions within the Open Radio
Access Network (O-RAN) architecture. We analyse robustness to prompt variation,
demonstrate the practicality of automating the attack descriptions and show
that detection quality relies on the semantic completeness of the description
rather than its phrasing or length. We utilise an RRC/NAS dataset to evaluate
the solution and provide an extensive comparison of open-source and proprietary
LLM implementations to demonstrate superior performance in attack detection. We
further validate the practicality of our framework within O-RAN's real-time
constraints, illustrating its potential for detecting other Layer-3 attacks.

</details>


### [227] [Unsupervised Backdoor Detection and Mitigation for Spiking Neural Networks](https://arxiv.org/abs/2510.06629)
*Jiachen Li,Bang Wu,Xiaoyu Xia,Xiaoning Liu,Xun Yi,Xiuzhen Zhang*

Main category: cs.CR

TL;DR: 本文针对SNNs在后门攻击下的安全问题，提出TMPBD检测框架和NDSBM缓解机制，实验表明TMPBD检测准确率达100%，NDSBM大幅降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: SNNs相比ANNs有能效优势，但在后门攻击下的安全方面受关注少，现有ANNs防御方法在SNNs中效果差。

Method: 提出TMPBD检测框架，利用最终脉冲层的时间膜电位最大边缘统计检测目标标签；引入NDSBM缓解机制，根据小的干净未标记数据集提取的TMP，钳制早期卷积层之间的树突连接。

Result: TMPBD检测准确率达100%，NDSBM将攻击成功率从100%降至8.44%，结合检测时降至2.81%，且不降低干净准确率。

Conclusion: TMPBD和NDSBM能有效解决SNNs在后门攻击下的安全问题。

Abstract: Spiking Neural Networks (SNNs) have gained increasing attention for their
superior energy efficiency compared to Artificial Neural Networks (ANNs).
However, their security aspects, particularly under backdoor attacks, have
received limited attention. Existing defense methods developed for ANNs perform
poorly or can be easily bypassed in SNNs due to their event-driven and temporal
dependencies. This paper identifies the key blockers that hinder traditional
backdoor defenses in SNNs and proposes an unsupervised post-training detection
framework, Temporal Membrane Potential Backdoor Detection (TMPBD), to overcome
these challenges. TMPBD leverages the maximum margin statistics of temporal
membrane potential (TMP) in the final spiking layer to detect target labels
without any attack knowledge or data access. We further introduce a robust
mitigation mechanism, Neural Dendrites Suppression Backdoor Mitigation (NDSBM),
which clamps dendritic connections between early convolutional layers to
suppress malicious neurons while preserving benign behaviors, guided by TMP
extracted from a small, clean, unlabeled dataset. Extensive experiments on
multiple neuromorphic benchmarks and state-of-the-art input-aware dynamic
trigger attacks demonstrate that TMPBD achieves 100% detection accuracy, while
NDSBM reduces the attack success rate from 100% to 8.44%, and to 2.81% when
combined with detection, without degrading clean accuracy.

</details>


### [228] [VelLMes: A high-interaction AI-based deception framework](https://arxiv.org/abs/2510.06975)
*Muris Sladić,Veronica Valeros,Carlos Catania,Sebastian Garcia*

Main category: cs.CR

TL;DR: 本文提出基于AI的欺骗框架VelLMes，可模拟多协议服务作蜜罐，评估其生成与欺骗能力，结果显示LLM能生成逼真响应，部分攻击场景表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的欺骗系统少且功能有限，缺乏含人类攻击者的广泛评估，需新系统。

Method: 提出VelLMes框架，用单元测试评估生成能力，让89名人类攻击者评估SSH Linux shell欺骗能力，在互联网部署蜜罐捕获攻击。

Result: 单元测试中LLM精心提示后能生成逼真响应，部分通过率100%；约30%攻击者误认蜜罐为真实系统；蜜罐对互联网上无结构和意外攻击表现良好。

Conclusion: VelLMes框架可模拟多协议服务作蜜罐，有多种欺骗设计选择，LLM用于蜜罐有一定效果。

Abstract: There are very few SotA deception systems based on Large Language Models. The
existing ones are limited only to simulating one type of service, mainly SSH
shells. These systems - but also the deception technologies not based on LLMs -
lack an extensive evaluation that includes human attackers. Generative AI has
recently become a valuable asset for cybersecurity researchers and
practitioners, and the field of cyber-deception is no exception. Researchers
have demonstrated how LLMs can be leveraged to create realistic-looking
honeytokens, fake users, and even simulated systems that can be used as
honeypots. This paper presents an AI-based deception framework called VelLMes,
which can simulate multiple protocols and services such as SSH Linux shell,
MySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus
VelLMes offers a variety of choices for deception design based on the users'
needs. VelLMes is designed to be attacked by humans, so interactivity and
realism are key for its performance. We evaluate the generative capabilities
and the deception capabilities. Generative capabilities were evaluated using
unit tests for LLMs. The results of the unit tests show that, with careful
prompting, LLMs can produce realistic-looking responses, with some LLMs having
a 100% passing rate. In the case of the SSH Linux shell, we evaluated deception
capabilities with 89 human attackers. The results showed that about 30% of the
attackers thought that they were interacting with a real system when they were
assigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH
Linux shell honeypot on the Internet to capture real-life attacks. Analysis of
these attacks showed us that LLM honeypots simulating Linux shells can perform
well against unstructured and unexpected attacks on the Internet, responding
correctly to most of the issued commands.

</details>


### [229] [Differentially Private Synthetic Text Generation for Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2510.06719)
*Junki Mori,Kazuya Kakizaki,Taiki Miyagawa,Jun Sakuma*

Main category: cs.CR

TL;DR: 提出DP - SynRAG框架解决RAG在敏感领域应用的隐私风险问题，实验显示其性能优且隐私预算固定。


<details>
  <summary>Details</summary>
Motivation: RAG在敏感领域应用受隐私风险限制，现有私有RAG方法依赖查询时差分隐私，存在隐私损失累积问题。

Method: 提出DP - SynRAG框架，用大语言模型生成差分隐私合成RAG数据库，扩展私有预测以保留下游RAG任务关键信息。

Result: DP - SynRAG比现有私有RAG系统性能更优，能维持固定隐私预算。

Conclusion: DP - SynRAG为隐私保护RAG提供了可扩展的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
grounding them in external knowledge. However, its application in sensitive
domains is limited by privacy risks. Existing private RAG methods typically
rely on query-time differential privacy (DP), which requires repeated noise
injection and leads to accumulated privacy loss. To address this issue, we
propose DP-SynRAG, a framework that uses LLMs to generate differentially
private synthetic RAG databases. Unlike prior methods, the synthetic text can
be reused once created, thereby avoiding repeated noise injection and
additional privacy costs. To preserve essential information for downstream RAG
tasks, DP-SynRAG extends private prediction, which instructs LLMs to generate
text that mimics subsampled database records in a DP manner. Experiments show
that DP-SynRAG achieves superior performanec to the state-of-the-art private
RAG systems while maintaining a fixed privacy budget, offering a scalable
solution for privacy-preserving RAG.

</details>


### [230] [Pseudo-MDPs: A Novel Framework for Efficiently Optimizing Last Revealer Seed Manipulations in Blockchains](https://arxiv.org/abs/2510.07080)
*Maxime Reynouard*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This study tackles the computational challenges of solving Markov Decision
Processes (MDPs) for a restricted class of problems. It is motivated by the
Last Revealer Attack (LRA), which undermines fairness in some Proof-of-Stake
(PoS) blockchains such as Ethereum (\$400B market capitalization). We introduce
pseudo-MDPs (pMDPs) a framework that naturally models such problems and propose
two distinct problem reductions to standard MDPs. One problem reduction
provides a novel, counter-intuitive perspective, and combining the two problem
reductions enables significant improvements in dynamic programming algorithms
such as value iteration. In the case of the LRA which size is parameterized by
$\kappa$ (in Ethereum's case $\kappa$= 325), we reduce the computational
complexity from $O(2^\kappa \kappa^{2^{\kappa+2}})$ to $O(\kappa^4)$ (per
iteration). This solution also provide the usual benefits from Dynamic
Programming solutions: exponentially fast convergence toward the optimal
solution is guaranteed. The dual perspective also simplifies policy extraction,
making the approach well-suited for resource-constrained agents who can operate
with very limited memory and computation once the problem has been solved.
Furthermore, we generalize those results to a broader class of MDPs, enhancing
their applicability. The framework is validated through two case studies: a
fictional card game and the LRA on the Ethereum random seed consensus protocol.
These applications demonstrate the framework's ability to solve large-scale
problems effectively while offering actionable insights into optimal
strategies. This work advances the study of MDPs and contributes to
understanding security vulnerabilities in blockchain systems.

</details>


### [231] [GNN-enhanced Traffic Anomaly Detection for Next-Generation SDN-Enabled Consumer Electronics](https://arxiv.org/abs/2510.07109)
*Guan-Yan Yang,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: 现有基于深度学习的流量异常检测系统存在不足，本文提出集成SDN和CFN的可扩展网络模型及GNN - NAD框架，实验显示其性能优于现有方法，提升了下一代智能消费电子网络的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 消费电子设备联网易受攻击，现有基于深度学习的流量异常检测系统过于复杂、依赖静态基础设施且需手动配置管理，有改进需求。

Method: 提出集成SDN和CFN的可扩展网络模型，构建GNN - NAD框架，融合静态漏洞感知攻击图和动态流量特征，核心是GSAGE图表示学习模型加RF分类器。

Result: 在消费电子环境实验中，GNN - NAD即使在小样本情况下，在准确率、召回率、精确率和F1分数等指标上也优于现有网络异常检测方法。

Conclusion: 该工作提升了下一代智能消费电子网络的安全性和效率。

Abstract: Consumer electronics (CE) connected to the Internet of Things are susceptible
to various attacks, including DDoS and web-based threats, which can compromise
their functionality and facilitate remote hijacking. These vulnerabilities
allow attackers to exploit CE for broader system attacks while enabling the
propagation of malicious code across the CE network, resulting in device
failures. Existing deep learning-based traffic anomaly detection systems
exhibit high accuracy in traditional network environments but are often overly
complex and reliant on static infrastructure, necessitating manual
configuration and management. To address these limitations, we propose a
scalable network model that integrates Software-defined Networking (SDN) and
Compute First Networking (CFN) for next-generation CE networks. In this network
model, we propose a Graph Neural Networks-based Network Anomaly Detection
framework (GNN-NAD) that integrates SDN-based CE networks and enables the CFN
architecture. GNN-NAD uniquely fuses a static, vulnerability-aware attack graph
with dynamic traffic features, providing a holistic view of network security.
The core of the framework is a GNN model (GSAGE) for graph representation
learning, followed by a Random Forest (RF) classifier. This design (GSAGE+RF)
demonstrates superior performance compared to existing feature selection
methods. Experimental evaluations on CE environment reveal that GNN-NAD
achieves superior metrics in accuracy, recall, precision, and F1 score, even
with small sample sizes, exceeding the performance of current network anomaly
detection methods. This work advances the security and efficiency of
next-generation intelligent CE networks.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [232] [Economic Entropy and Sectoral Dynamics: A Thermodynamic Approach to Market Analysis](https://arxiv.org/abs/2510.06248)
*W. A. Rojas C.,A. Zamora V.,L. F. Quijano W.,Y. Beltran P*

Main category: physics.soc-ph

TL;DR: 本文将几何热力学应用于波哥大体育部门经济分析，对比博彩和休闲体育两个部门，得出相关结论并指出该方法对政策设计有潜力。


<details>
  <summary>Details</summary>
Motivation: 运用几何热力学对波哥大体育部门进行经济分析。

Method: 建立热力学系统和经济结构的类比，开发数学框架，分析2018 - 2023年博彩和休闲体育两个部门的数据。

Result: 博彩部门经济熵低于休闲体育部门；热容函数显示经济动态临界点；曲率标量识别部门组织潜在危机点；交叉收入弹性分析显示部门间资源流动模式不同。

Conclusion: 经济物理学和统计热力学可用于理解波哥大体育经济部门动态，对公共政策设计有潜在价值。

Abstract: This paper presents an application of geometrothermodynamics (GTD) to the
economic analysis of Bogot\'a's sports sector through the Satellite Account of
Sport (CSDB). By establishing an analogy between thermodynamic systems and
economic structures, we develop a mathematical framework where monetary flows
behave analogously to energy, while economic entropy, temperature, and heat
capacity acquire well-defined economic interpretations. The study focuses on
two contrasting sectors: gambling and betting $\mathbb{S}_{15}$, and
recreational and sports activities $\mathbb{S}_{16}$, analyzing data from
2018-2023. Our results demonstrate that $\mathbb{S}_{15}$ exhibits lower
economic entropy than $\mathbb{S}_{16}$ , indicating a higher degree of
organization and regulatory structure in the gambling sector compared to the
more heterogeneous recreational sports sector. The heat capacity function
reveals critical points that may signal phase transitions in economic dynamics,
while Ricci and Kretschmann curvature scalars identify potential crisis points
in the sectoral organization. Furthermore, the cross-income elasticity analysis
shows distinct resource flow patterns between sectors, suggesting that gambling
activities may serve as an economic driver for recreational sports. This
thermodynamic approach provides a quantitative tool for analyzing resource
redistribution policies and anticipating critical transitions in sectoral
economics. The findings suggest that econophysics and statistical
thermodynamics constitute powerful frameworks for understanding the sectoral
dynamics of Bogot\'a's sports economy, with significant potential for
developing prospective analysis tools in public policy design.

</details>


### [233] [Generalized Multi-agent Social Simulation Framework](https://arxiv.org/abs/2510.06225)
*Gang Li,Jie Lin,Yining Tang,Ziteng Wang,Yirui Huang,Junyu Zhang,Shuang Luo,Chao Wu,Yike Guo*

Main category: physics.soc-ph

TL;DR: 本文设计模块化、面向对象框架解决多智能体社交模拟系统扩展性和复用性问题，实现特定模拟环境并成功模拟社交媒体上的人类互动。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体社交模拟系统存在难以扩展到不同场景和缺乏模块化设计导致复用性差的问题。

Method: 设计开发模块化、面向对象框架，提出内存总结机制，选择组合派生类定制模拟环境。

Result: 成功模拟社交媒体上的人类互动，复制了现实世界的在线社交行为。

Conclusion: 所设计的框架有效解决了现有系统的问题，项目源码将发布并持续发展。

Abstract: Multi-agent social interaction has clearly benefited from Large Language
Models. However, current simulation systems still face challenges such as
difficulties in scaling to diverse scenarios and poor reusability due to a lack
of modular design. To address these issues, we designed and developed a
modular, object-oriented framework that organically integrates various base
classes through a hierarchical structure, harvesting scalability and
reusability. We inherited the framework to realize common derived classes.
Additionally, a memory summarization mechanism is proposed to filter and
distill relevant information from raw memory data, prioritizing contextually
salient events and interactions. By selecting and combining some necessary
derived classes, we customized a specific simulated environment. Utilizing this
simulated environment, we successfully simulated human interactions on social
media, replicating real-world online social behaviors. The source code for the
project will be released and evolve.

</details>


### [234] [Deep Generative Model for Human Mobility Behavior](https://arxiv.org/abs/2510.06473)
*Ye Hong,Yatao Zhang,Konrad Schindler,Martin Raubal*

Main category: physics.soc-ph

TL;DR: 提出MobilityGen模型用于生成大规模时空尺度的现实移动轨迹，有新发现并建立新框架。


<details>
  <summary>Details</summary>
Motivation: 理解和建模人类移动性对多领域很重要，但模拟个体移动性因复杂等特性仍具挑战。

Method: 提出MobilityGen模型，将行为属性与环境背景关联。

Result: 模型能重现关键模式，反映时空变异性，产生多样合理的移动模式，有新见解。

Conclusion: 建立了新的移动性模拟框架，为细粒度、数据驱动的人类行为研究及社会影响研究奠定基础。

Abstract: Understanding and modeling human mobility is central to challenges in
transport planning, sustainable urban design, and public health. Despite
decades of effort, simulating individual mobility remains challenging because
of its complex, context-dependent, and exploratory nature. Here, we present
MobilityGen, a deep generative model that produces realistic mobility
trajectories spanning days to weeks at large spatial scales. By linking
behavioral attributes with environmental context, MobilityGen reproduces key
patterns such as scaling laws for location visits, activity time allocation,
and the coupled evolution of travel mode and destination choices. It reflects
spatio-temporal variability and generates diverse, plausible, and novel
mobility patterns consistent with the built environment. Beyond standard
validation, MobilityGen yields insights not attainable with earlier models,
including how access to urban space varies across travel modes and how
co-presence dynamics shape social exposure and segregation. Our work
establishes a new framework for mobility simulation, paving the way for
fine-grained, data-driven studies of human behavior and its societal
implications.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [235] [Validation of Various Normalization Methods for Brain Tumor Segmentation: Can Federated Learning Overcome This Heterogeneity?](https://arxiv.org/abs/2510.07126)
*Jan Fiszer,Dominika Ciupek,Maciej Malawski*

Main category: cs.CV

TL;DR: 研究模拟非IID条件下用不同MRI强度归一化技术处理数据子集进行脑肿瘤分割模型训练测试，发现联邦学习对不一致归一化数据有韧性，可有效训练高性能模型且不侵犯数据隐私。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像应用需大量数据，存在隐私、存储和传输问题，联邦学习处理非IID数据时有效性可能降低。

Method: 模拟非IID条件，用不同MRI强度归一化技术处理不同数据子集，用于脑肿瘤分割模型的训练和测试。

Result: 联邦学习方法对跨客户端不一致归一化数据有韧性，3D Dice分数达92%，与集中式模型相当。

Conclusion: 联邦学习是在不侵犯数据隐私前提下有效训练高性能模型的解决方案。

Abstract: Deep learning (DL) has been increasingly applied in medical imaging, however,
it requires large amounts of data, which raises many challenges related to data
privacy, storage, and transfer. Federated learning (FL) is a training paradigm
that overcomes these issues, though its effectiveness may be reduced when
dealing with non-independent and identically distributed (non-IID) data. This
study simulates non-IID conditions by applying different MRI intensity
normalization techniques to separate data subsets, reflecting a common cause of
heterogeneity. These subsets are then used for training and testing models for
brain tumor segmentation. The findings provide insights into the influence of
the MRI intensity normalization methods on segmentation models, both training
and inference. Notably, the FL methods demonstrated resilience to
inconsistently normalized data across clients, achieving the 3D Dice score of
92%, which is comparable to a centralized model (trained using all data). These
results indicate that FL is a solution to effectively train high-performing
models without violating data privacy, a crucial concern in medical
applications. The code is available at:
https://github.com/SanoScience/fl-varying-normalization.

</details>


### [236] [Uncertainty Quantification In Surface Landmines and UXO Classification Using MC Dropout](https://arxiv.org/abs/2510.06238)
*Sagar Lekhak,Emmett J. Ientilucci,Dimah Dera,Susmita Ghosh*

Main category: cs.CV

TL;DR: 本研究将MC Dropout用于微调的ResNet - 50架构进行地表地雷和未爆弹药分类，在模拟数据集上测试，实验证明模型能标记不可靠预测，强调排雷中不确定性量化的必要性。


<details>
  <summary>Details</summary>
Motivation: 确定性神经网络在排雷中易受噪声和对抗攻击影响，导致漏检或误分类，需要进行不确定性量化。

Method: 将蒙特卡罗（MC）Dropout集成到微调的ResNet - 50架构中，在模拟数据集上进行测试。

Result: 在干净、对抗性扰动和有噪声的测试图像上的实验结果表明，模型能够在具有挑战性的条件下标记不可靠的预测。

Conclusion: 排雷中需要进行不确定性量化，现有神经网络易受对抗威胁，应开发更稳健可靠的模型用于实际应用。

Abstract: Detecting surface landmines and unexploded ordnances (UXOs) using deep
learning has shown promise in humanitarian demining. However, deterministic
neural networks can be vulnerable to noisy conditions and adversarial attacks,
leading to missed detection or misclassification. This study introduces the
idea of uncertainty quantification through Monte Carlo (MC) Dropout, integrated
into a fine-tuned ResNet-50 architecture for surface landmine and UXO
classification, which was tested on a simulated dataset. Integrating the MC
Dropout approach helps quantify epistemic uncertainty, providing an additional
metric for prediction reliability, which could be helpful to make more informed
decisions in demining operations. Experimental results on clean, adversarially
perturbed, and noisy test images demonstrate the model's ability to flag
unreliable predictions under challenging conditions. This proof-of-concept
study highlights the need for uncertainty quantification in demining, raises
awareness about the vulnerability of existing neural networks in demining to
adversarial threats, and emphasizes the importance of developing more robust
and reliable models for practical applications.

</details>


### [237] [Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis](https://arxiv.org/abs/2510.06260)
*Sher Khan,Raz Muhammad,Adil Hussain,Muhammad Sajjad,Muhammad Rashid*

Main category: cs.CV

TL;DR: 本文提出统一框架用于皮肤科诊断，结合异构卷积神经网络和大语言模型能力，解决诊断可靠性和沟通障碍问题，提升早期干预率。


<details>
  <summary>Details</summary>
Motivation: 当前皮肤科诊断存在观察者间差异和获取不平等问题，现有AI系统有架构单一、数据集偏差等局限，需要改进。

Method: 引入统一框架，包括异构卷积神经网络集成提供互补诊断视角，将大语言模型能力嵌入诊断流程。

Result: 生成包含精确病变特征、诊断推理和监测指导的结构化报告。

Conclusion: 该框架解决了先前AI实施的转化差距，朝着可部署的皮肤科AI迈进，提高诊断精度和早期干预率。

Abstract: Cutaneous malignancies demand early detection for favorable outcomes, yet
current diagnostics suffer from inter-observer variability and access
disparities. While AI shows promise, existing dermatological systems are
limited by homogeneous architectures, dataset biases across skin tones, and
fragmented approaches that treat natural language processing as separate
post-hoc explanations rather than integral to clinical decision-making. We
introduce a unified framework that fundamentally reimagines AI integration for
dermatological diagnostics through two synergistic innovations. First, a
purposefully heterogeneous ensemble of architecturally diverse convolutional
neural networks provides complementary diagnostic perspectives, with an
intrinsic uncertainty mechanism flagging discordant cases for specialist review
-- mimicking clinical best practices. Second, we embed large language model
capabilities directly into the diagnostic workflow, transforming classification
outputs into clinically meaningful assessments that simultaneously fulfill
medical documentation requirements and deliver patient-centered education. This
seamless integration generates structured reports featuring precise lesion
characterization, accessible diagnostic reasoning, and actionable monitoring
guidance -- empowering patients to recognize early warning signs between
visits. By addressing both diagnostic reliability and communication barriers
within a single cohesive system, our approach bridges the critical
translational gap that has prevented previous AI implementations from achieving
clinical impact. The framework represents a significant advancement toward
deployable dermatological AI that enhances diagnostic precision while actively
supporting the continuum of care from initial detection through patient
education, ultimately improving early intervention rates for skin lesions.

</details>


### [238] [Improving the Spatial Resolution of GONG Solar Images to GST Quality Using Deep Learning](https://arxiv.org/abs/2510.06281)
*Chenyang Li,Qin Li,Haimin Wang,Bo Shen*

Main category: cs.CV

TL;DR: 提出基于GAN的超分辨率方法提升GONG低分辨率全日面Hα图像质量，模型有效恢复细节，但图像对轻微错位影响性能，后续将改进。


<details>
  <summary>Details</summary>
Motivation: 现有全日面Hα图像空间分辨率有限，无法分辨细丝和原纤维等小尺度结构。

Method: 采用带有残差密集块的Real - ESRGAN和相对论判别器，仔细对齐GONG - GST图像对。

Result: 模型有效恢复黑子半影和细丝、原纤维细节，平均均方误差467.15，均方根误差21.59，互相关系数0.7794。

Conclusion: 图像对轻微错位限制定量性能，未来计划解决此问题并扩大数据集以提升重建质量。

Abstract: High-resolution (HR) solar imaging is crucial for capturing fine-scale
dynamic features such as filaments and fibrils. However, the spatial resolution
of the full-disk H$\alpha$ images is limited and insufficient to resolve these
small-scale structures. To address this, we propose a GAN-based superresolution
approach to enhance low-resolution (LR) full-disk H$\alpha$ images from the
Global Oscillation Network Group (GONG) to a quality comparable with HR
observations from the Big Bear Solar Observatory/Goode Solar Telescope
(BBSO/GST). We employ Real-ESRGAN with Residual-in-Residual Dense Blocks and a
relativistic discriminator. We carefully aligned GONG-GST pairs. The model
effectively recovers fine details within sunspot penumbrae and resolves fine
details in filaments and fibrils, achieving an average mean squared error (MSE)
of 467.15, root mean squared error (RMSE) of 21.59, and cross-correlation (CC)
of 0.7794. Slight misalignments between image pairs limit quantitative
performance, which we plan to address in future work alongside dataset
expansion to further improve reconstruction quality.

</details>


### [239] [ChainMPQ: Interleaved Text-Image Reasoning Chains for Mitigating Relation Hallucinations](https://arxiv.org/abs/2510.06292)
*Yike Wu,Yiwei Wang,Yujun Cai*

Main category: cs.CV

TL;DR: 提出ChainMPQ方法提升大视觉语言模型关系推理能力，减少关系幻觉，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型存在幻觉问题，关系幻觉占比大却少受关注，影响模型可靠性。

Method: 提出ChainMPQ，从问题中提取主客体关键词增强图像区域，构建多视角问题，形成图文交错链引导渐进式关系推理。

Result: 在多个大视觉语言模型和基准测试中，ChainMPQ大幅减少关系幻觉，消融实验验证核心模块有效性。

Conclusion: ChainMPQ能有效提升大视觉语言模型的关系推理能力，减少关系幻觉。

Abstract: While Large Vision-Language Models (LVLMs) achieve strong performance in
multimodal tasks, hallucinations continue to hinder their reliability. Among
the three categories of hallucinations, which include object, attribute, and
relation, relation hallucinations account for the largest proportion but have
received the least attention. To address this issue, we propose ChainMPQ
(Multi-Perspective Questions guided Interleaved Chain of Image and Text), a
training-free method that improves relational inference in LVLMs by utilizing
accumulated textual and visual memories. ChainMPQ first extracts subject and
object keywords from the question to enhance the corresponding image regions.
It then constructs multi-perspective questions that focus on the three core
components of a relationship: the subject, the object, and the relation that
links them. These questions are sequentially input to the model, with textual
and visual memories from earlier steps providing supporting context for
subsequent ones, thereby forming an interleaved chain of images and text that
guides progressive relational reasoning. Experiments on multiple LVLMs and
benchmarks show that ChainMPQ substantially reduces relation hallucinations,
while ablation studies further validate the effectiveness of its three core
modules.

</details>


### [240] [Efficient High-Resolution Image Editing with Hallucination-Aware Loss and Adaptive Tiling](https://arxiv.org/abs/2510.06295)
*Young D. Kwon,Abhinav Mehrotra,Malcolm Chadwick,Alberto Gil Ramos,Sourav Bhattacharya*

Main category: cs.CV

TL;DR: 提出MobilePicasso系统实现高效高分辨率图像编辑，降低计算成本和内存使用，效果好、速度快。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑扩散模型在资源受限设备上有内存和图像质量挑战，需实现高效高分辨率图像编辑。

Method: MobilePicasso分三阶段：用幻觉感知损失在标准分辨率编辑图像；应用潜在投影避免进入像素空间；用自适应上下文保留平铺将编辑图像潜在特征提升到更高分辨率。

Result: 用户研究显示，相比现有方法，图像质量提升18 - 48%，幻觉减少14 - 51%；延迟显著降低，速度提升达55.8倍，运行时内存仅增加9%；设备上运行速度比A100 GPU服务器上的高分辨率图像编辑模型还快。

Conclusion: MobilePicasso能在资源受限设备上高效进行高分辨率图像编辑。

Abstract: High-resolution (4K) image-to-image synthesis has become increasingly
important for mobile applications. Existing diffusion models for image editing
face significant challenges, in terms of memory and image quality, when
deployed on resource-constrained devices. In this paper, we present
MobilePicasso, a novel system that enables efficient image editing at high
resolutions, while minimising computational cost and memory usage.
MobilePicasso comprises three stages: (i) performing image editing at a
standard resolution with hallucination-aware loss, (ii) applying latent
projection to overcome going to the pixel space, and (iii) upscaling the edited
image latent to a higher resolution with adaptive context-preserving tiling.
Our user study with 46 participants reveals that MobilePicasso not only
improves image quality by 18-48% but reduces hallucinations by 14-51% over
existing methods. MobilePicasso demonstrates significantly lower latency, e.g.,
up to 55.8$\times$ speed-up, yet with a small increase in runtime memory, e.g.,
a mere 9% increase over prior work. Surprisingly, the on-device runtime of
MobilePicasso is observed to be faster than a server-based high-resolution
image editing model running on an A100 GPU.

</details>


### [241] [RGBD Gaze Tracking Using Transformer for Feature Fusion](https://arxiv.org/abs/2510.06298)
*Tobias J. Bauer*

Main category: cs.CV

TL;DR: 论文实现基于RGBD图像的AI注视跟踪系统，用Transformer融合特征，创建新数据集，在多个数据集上训练评估模型，对比不同架构表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究未探究RGBD输入图像与Transformers的结合，且现有数据集不适用于注视角度估计任务。

Method: 使用基于Transformer架构的模块融合图像特征，基于Lian等人的工作，用GAN去除深度图伪影和提取头部姿态特征，在三个不同数据集上训练、验证和评估多种模型配置。

Result: 在ShanghaiTechGaze+数据集上，含Transformer模块的模型平均欧氏误差55.3mm，无预训练GAN模块为30.1mm，用MLP替代Transformer模块误差为26.9mm；在ETH - XGaze数据集上，含Transformer模块平均角度误差3.59°，无该模块为3.26°。

Conclusion: 不同模型架构在不同数据集上有不同表现，某些调整可改善模型误差，但与其他先进模型仍有差距。

Abstract: Subject of this thesis is the implementation of an AI-based Gaze Tracking
system using RGBD images that contain both color (RGB) and depth (D)
information. To fuse the features extracted from the images, a module based on
the Transformer architecture is used. The combination of RGBD input images and
Transformers was chosen because it has not yet been investigated. Furthermore,
a new dataset is created for training the AI models as existing datasets either
do not contain depth information or only contain labels for Gaze Point
Estimation that are not suitable for the task of Gaze Angle Estimation. Various
model configurations are trained, validated and evaluated on a total of three
different datasets. The trained models are then to be used in a real-time
pipeline to estimate the gaze direction and thus the gaze point of a person in
front of a computer screen. The AI model architecture used in this thesis is
based on an earlier work by Lian et al. It uses a Generative Adversarial
Network (GAN) to simultaneously remove depth map artifacts and extract head
pose features. Lian et al. achieve a mean Euclidean error of 38.7mm on their
own dataset ShanghaiTechGaze+. In this thesis, a model architecture with a
Transformer module for feature fusion achieves a mean Euclidean error of 55.3mm
on the same dataset, but we show that using no pre-trained GAN module leads to
a mean Euclidean error of 30.1mm. Replacing the Transformer module with a
Multilayer Perceptron (MLP) improves the error to 26.9mm. These results are
coherent with the ones on the other two datasets. On the ETH-XGaze dataset, the
model with Transformer module achieves a mean angular error of 3.59{\deg} and
without Transformer module 3.26{\deg}, whereas the fundamentally different
model architecture used by the dataset authors Zhang et al. achieves a mean
angular error of 2.04{\deg}. On the OTH-Gaze-Estimation dataset created for...

</details>


### [242] [TransFIRA: Transfer Learning for Face Image Recognizability Assessment](https://arxiv.org/abs/2510.06353)
*Allen Tu,Kartik Narayan,Joshua Gleason,Jennifer Xu,Matthew Meyn,Tom Goldstein,Vishal M. Patel*

Main category: cs.CV

TL;DR: 提出轻量级无注释框架TransFIRA进行人脸图像可识别性评估，有多项创新，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 无约束环境下人脸识别面临诸多挑战，现有FIQA方法有缺陷，其预测与编码器决策几何脱节。

Method: 引入TransFIRA框架，通过类中心相似度和类中心角分离定义可识别性，采用可识别性感知聚合策略，还有新扩展。

Result: 在人脸和人体识别上取得先进结果，跨数据集转移时表现稳健。

Conclusion: TransFIRA是统一、几何驱动的可识别性评估框架，显著提升FIQA的准确性、可解释性和范围。

Abstract: Face recognition in unconstrained environments such as surveillance, video,
and web imagery must contend with extreme variation in pose, blur,
illumination, and occlusion, where conventional visual quality metrics fail to
predict whether inputs are truly recognizable to the deployed encoder. Existing
FIQA methods typically rely on visual heuristics, curated annotations, or
computationally intensive generative pipelines, leaving their predictions
detached from the encoder's decision geometry. We introduce TransFIRA (Transfer
Learning for Face Image Recognizability Assessment), a lightweight and
annotation-free framework that grounds recognizability directly in embedding
space. TransFIRA delivers three advances: (i) a definition of recognizability
via class-center similarity (CCS) and class-center angular separation (CCAS),
yielding the first natural, decision-boundary--aligned criterion for filtering
and weighting; (ii) a recognizability-informed aggregation strategy that
achieves state-of-the-art verification accuracy on BRIAR and IJB-C while nearly
doubling correlation with true recognizability, all without external labels,
heuristics, or backbone-specific training; and (iii) new extensions beyond
faces, including encoder-grounded explainability that reveals how degradations
and subject-specific factors affect recognizability, and the first
recognizability-aware body recognition assessment. Experiments confirm
state-of-the-art results on faces, strong performance on body recognition, and
robustness under cross-dataset shifts. Together, these contributions establish
TransFIRA as a unified, geometry-driven framework for recognizability
assessment -- encoder-specific, accurate, interpretable, and extensible across
modalities -- significantly advancing FIQA in accuracy, explainability, and
scope.

</details>


### [243] [LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval](https://arxiv.org/abs/2510.06512)
*Avishree Khare,Hideki Okamoto,Bardh Hoxha,Georgios Fainekos,Rajeev Alur*

Main category: cs.CV

TL;DR: 本文提出LogSTOP评分函数，用于计算序列上时间属性的分数，在查询匹配和排序检索任务上优于多个基线模型。


<details>
  <summary>Details</summary>
Motivation: 将局部属性检测分数提升到序列上的时间属性，以应用于查询匹配和排序检索等下游任务。

Method: 形式化分配序列上时间属性分数的问题，提出LogSTOP评分函数来计算线性时态逻辑表示的时间属性分数。

Result: 在视频对象和语音情感的时间属性查询匹配上，LogSTOP结合YOLO和HuBERT比大模型和其他时态逻辑基线至少高16%；在视频对象和动作的时间属性排序检索上，LogSTOP结合Grounding DINO和SlowR50比零样本图文检索基线的平均精度和召回率分别至少提高19%和16%。

Conclusion: LogSTOP评分函数能有效计算时间属性分数，在相关任务上表现出色。

Abstract: Neural models such as YOLO and HuBERT can be used to detect local properties
such as objects ("car") and emotions ("angry") in individual frames of videos
and audio clips respectively. The likelihood of these detections is indicated
by scores in [0, 1]. Lifting these scores to temporal properties over sequences
can be useful for several downstream applications such as query matching (e.g.,
"does the speaker eventually sound happy in this audio clip?"), and ranked
retrieval (e.g., "retrieve top 5 videos with a 10 second scene where a car is
detected until a pedestrian is detected"). In this work, we formalize this
problem of assigning Scores for TempOral Properties (STOPs) over sequences,
given potentially noisy score predictors for local properties. We then propose
a scoring function called LogSTOP that can efficiently compute these scores for
temporal properties represented in Linear Temporal Logic. Empirically, LogSTOP,
with YOLO and HuBERT, outperforms Large Vision / Audio Language Models and
other Temporal Logic-based baselines by at least 16% on query matching with
temporal properties over objects-in-videos and emotions-in-speech respectively.
Similarly, on ranked retrieval with temporal properties over objects and
actions in videos, LogSTOP with Grounding DINO and SlowR50 reports at least a
19% and 16% increase in mean average precision and recall over zero-shot
text-to-video retrieval baselines respectively.

</details>


### [244] [HSNet: Heterogeneous Subgraph Network for Single Image Super-resolution](https://arxiv.org/abs/2510.06564)
*Qiongyang Hu,Wenyang Liu,Wenbin Zou,Yuejiao Su,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出HSNet解决图像超分辨率现有方法的问题，通过分解全局图等操作平衡重建质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN和注意力机制的深度学习图像超分辨率方法结构不灵活，基于图的方法计算复杂度高，需新方法解决。

Method: 提出HSNet框架，引入CSSB生成互补子图，SAB集成子图表示，NSS选择性保留特征。

Result: 大量实验表明HSNet达到了最先进性能，有效平衡了重建质量和计算效率。

Conclusion: HSNet能有效解决现有图像超分辨率方法的局限，代码将公开。

Abstract: Existing deep learning approaches for image super-resolution, particularly
those based on CNNs and attention mechanisms, often suffer from structural
inflexibility. Although graph-based methods offer greater representational
adaptability, they are frequently impeded by excessive computational
complexity. To overcome these limitations, this paper proposes the
Heterogeneous Subgraph Network (HSNet), a novel framework that efficiently
leverages graph modeling while maintaining computational feasibility. The core
idea of HSNet is to decompose the global graph into manageable sub-components.
First, we introduce the Constructive Subgraph Set Block (CSSB), which generates
a diverse set of complementary subgraphs. Rather than relying on a single
monolithic graph, CSSB captures heterogeneous characteristics of the image by
modeling different relational patterns and feature interactions, producing a
rich ensemble of both local and global graph structures. Subsequently, the
Subgraph Aggregation Block (SAB) integrates the representations embedded across
these subgraphs. Through adaptive weighting and fusion of multi-graph features,
SAB constructs a comprehensive and discriminative representation that captures
intricate interdependencies. Furthermore, a Node Sampling Strategy (NSS) is
designed to selectively retain the most salient features, thereby enhancing
accuracy while reducing computational overhead. Extensive experiments
demonstrate that HSNet achieves state-of-the-art performance, effectively
balancing reconstruction quality with computational efficiency. The code will
be made publicly available.

</details>


### [245] [SDQM: Synthetic Data Quality Metric for Object Detection Dataset Evaluation](https://arxiv.org/abs/2510.06596)
*Ayush Zenith,Arnold Zumbrun,Neel Raut,Jing Lin*

Main category: cs.CV

TL;DR: 提出SDQM指标评估合成数据集质量，实验显示与YOLOv11的mAP强相关，为评估合成数据设新标准且代码开源。


<details>
  <summary>Details</summary>
Motivation: 大规模标注数据集稀缺，需有效指标评估合成数据质量以应对资源受限的目标检测任务。

Method: 引入Synthetic Dataset Quality Metric (SDQM)评估目标检测任务的数据质量，无需模型训练收敛。

Result: SDQM与YOLOv11的mAP分数强相关，而之前指标相关性弱或中等；能提供改进数据集质量的可行见解，减少迭代训练成本。

Conclusion: SDQM是可扩展且高效的指标，为评估合成数据设定新的标准。

Abstract: The performance of machine learning models depends heavily on training data.
The scarcity of large-scale, well-annotated datasets poses significant
challenges in creating robust models. To address this, synthetic data generated
through simulations and generative models has emerged as a promising solution,
enhancing dataset diversity and improving the performance, reliability, and
resilience of models. However, evaluating the quality of this generated data
requires an effective metric. This paper introduces the Synthetic Dataset
Quality Metric (SDQM) to assess data quality for object detection tasks without
requiring model training to converge. This metric enables more efficient
generation and selection of synthetic datasets, addressing a key challenge in
resource-constrained object detection tasks. In our experiments, SDQM
demonstrated a strong correlation with the mean Average Precision (mAP) scores
of YOLOv11, a leading object detection model, while previous metrics only
exhibited moderate or weak correlations. Additionally, it provides actionable
insights for improving dataset quality, minimizing the need for costly
iterative training. This scalable and efficient metric sets a new standard for
evaluating synthetic data. The code for SDQM is available at
https://github.com/ayushzenith/SDQM

</details>


### [246] [Milestone Determination for Autonomous Railway Operation](https://arxiv.org/abs/2510.06229)
*Josh Hunter,John McDermid,Simon Burton,Poppy Fynes,Mia Dempster*

Main category: cs.CV

TL;DR: 传统铁路自动化领域计算机视觉系统因数据问题受限，提出聚焦特定路线生成数据集和里程碑确定概念，为铁路自动化机器学习系统提供实用框架。


<details>
  <summary>Details</summary>
Motivation: 解决铁路自动化领域计算机视觉系统因高质量、序列数据有限，传统数据集缺乏时空上下文及替代方案存在现实性和适用性问题的挑战。

Method: 聚焦特定路线生成与现实操作逻辑相符的序列数据集，采用里程碑确定概念开发有针对性的基于规则的模型。

Result: 未提及具体结果。

Conclusion: 该方法为在可控、可预测环境中训练视觉代理提供实用框架，有助于铁路自动化机器学习系统更安全高效。

Abstract: In the field of railway automation, one of the key challenges has been the
development of effective computer vision systems due to the limited
availability of high-quality, sequential data. Traditional datasets are
restricted in scope, lacking the spatio temporal context necessary for
real-time decision-making, while alternative solutions introduce issues related
to realism and applicability. By focusing on route-specific, contextually
relevant cues, we can generate rich, sequential datasets that align more
closely with real-world operational logic. The concept of milestone
determination allows for the development of targeted, rule-based models that
simplify the learning process by eliminating the need for generalized
recognition of dynamic components, focusing instead on the critical decision
points along a route. We argue that this approach provides a practical
framework for training vision agents in controlled, predictable environments,
facilitating safer and more efficient machine learning systems for railway
automation.

</details>


### [247] [StaR-KVQA: Structured Reasoning Traces for Implicit-Knowledge Visual Question Answering](https://arxiv.org/abs/2510.06638)
*Zhihao Wen,Wenkang Wei,Yuan Fang,Xingtong Yu,Hui Zhang,Weicheng Zhu,Xin Zhang*

Main category: cs.CV

TL;DR: 本文提出StaR - KVQA模型解决IK - KVQA问题，通过监督结构化推理轨迹，在基准测试中提高了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在IK - KVQA任务中缺乏明确推理监督，标准监督微调后泛化能力差。

Method: 提出StaR - KVQA，监督结构化轨迹（双重符号关系路径和基于路径的自然语言解释），构建并选择基于路径的推理轨迹形成丰富数据集，通过结构化自蒸馏进行微调。

Result: 在多个基准测试中，提高了准确性和可解释性，在OK - VQA上比最强基线的答案准确率最高提升11.3%，且具有强大的跨领域泛化能力。

Conclusion: StaR - KVQA方法有效，能解决IK - KVQA任务中现有模型的问题。

Abstract: Knowledge-based Visual Question Answering (KVQA) requires models to ground
entities in images and reason over factual knowledge. We study its
implicit-knowledge variant, IK-KVQA, where a multimodal large language model
(MLLM) is the sole knowledge source, without external retrieval. Yet, MLLMs
lack explicit reasoning supervision and produce inconsistent justifications,
and generalize poorly after standard supervised fine-tuning (SFT). We present
StaR-KVQA (Structured Reasoning Traces for IK-KVQA), which supervises
structured traces - dual symbolic relation paths plus path-grounded
natural-language explanations - so that reasoning becomes transparent and
verifiable. With one open-source MLLM, StaR-KVQA constructs and selects
path-grounded reasoning traces to form a trace-enriched dataset, then
fine-tunes via structured self-distillation to align generation with
supervision; no external retrievers, verifiers, or curated knowledge bases
(KBs) are used, traces are built offline, and inference is a single
autoregressive pass. Across benchmarks, StaR-KVQA improves both accuracy and
interpretability, achieving up to +11.3% higher answer accuracy on OK-VQA over
the strongest baseline while exhibiting robust cross-domain generalization.

</details>


### [248] [Automated Neural Architecture Design for Industrial Defect Detection](https://arxiv.org/abs/2510.06669)
*Yuxi Liu,Yunfeng Ma,Yi Tang,Min Liu,Shuai Jiang,Yaonan Wang*

Main category: cs.CV

TL;DR: 提出用于工业表面缺陷检测的自动神经架构设计框架AutoNAD，能解决现有方法问题，在多数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有工业表面缺陷检测方法用手动设计模型，难有效解决类内差异和类间相似问题，且需大量试错。

Method: 提出AutoNAD框架，联合搜索卷积、Transformer和多层感知机；引入交叉权重共享策略；集成可搜索的多级特征聚合模块；加入延迟感知先验。

Result: AutoNAD在三个工业缺陷数据集上验证了有效性，并应用于缺陷成像和检测平台。

Conclusion: AutoNAD能有效解决工业表面缺陷检测的关键挑战，减少手动网络设计成本，兼顾检测精度和运行效率。

Abstract: Industrial surface defect detection (SDD) is critical for ensuring product
quality and manufacturing reliability. Due to the diverse shapes and sizes of
surface defects, SDD faces two main challenges: intraclass difference and
interclass similarity. Existing methods primarily utilize manually designed
models, which require extensive trial and error and often struggle to address
both challenges effectively. To overcome this, we propose AutoNAD, an automated
neural architecture design framework for SDD that jointly searches over
convolutions, transformers, and multi-layer perceptrons. This hybrid design
enables the model to capture both fine-grained local variations and long-range
semantic context, addressing the two key challenges while reducing the cost of
manual network design. To support efficient training of such a diverse search
space, AutoNAD introduces a cross weight sharing strategy, which accelerates
supernet convergence and improves subnet performance. Additionally, a
searchable multi-level feature aggregation module (MFAM) is integrated to
enhance multi-scale feature learning. Beyond detection accuracy, runtime
efficiency is essential for industrial deployment. To this end, AutoNAD
incorporates a latency-aware prior to guide the selection of efficient
architectures. The effectiveness of AutoNAD is validated on three industrial
defect datasets and further applied within a defect imaging and detection
platform. Code will be available at https://github.com/Yuxi104/AutoNAD.

</details>


### [249] [Heptapod: Language Modeling on Visual Signals](https://arxiv.org/abs/2510.06673)
*Yongxin Zhu,Jiawei Chen,Yuanzhe Chen,Zhuo Chen,Dongya Jia,Jian Cong,Xiaobin Zhuang,Yuping Wang,Yuxuan Wang*

Main category: cs.CV

TL;DR: 介绍图像自回归模型Heptapod，采用新方法在ImageNet基准测试表现佳，望启发视觉信号语言建模思考。


<details>
  <summary>Details</summary>
Motivation: 基于语言建模原则构建图像自回归模型，探索视觉信号语言建模新方法。

Method: 采用因果注意力，消除对CFG依赖，摒弃语义分词器趋势，提出“next 2D distribution prediction”方法。

Result: 在ImageNet生成基准测试中，Heptapod的FID为2.70，显著优于先前因果自回归方法。

Conclusion: 希望工作能启发对视觉信号及其他领域语言建模的重新思考。

Abstract: We introduce Heptapod, an image autoregressive model that adheres to the
foundational principles of language modeling. Heptapod employs \textbf{causal
attention}, \textbf{eliminates reliance on CFG}, and \textbf{eschews the trend
of semantic tokenizers}. Our key innovation is \textit{next 2D distribution
prediction}: a causal Transformer with reconstruction-focused visual tokenizer,
learns to predict the distribution over the entire 2D spatial grid of images at
each timestep. This learning objective unifies the sequential modeling of
autoregressive framework with the holistic self-supervised learning of masked
autoencoding, enabling the model to capture comprehensive image semantics via
generative training. On the ImageNet generation benchmark, Heptapod achieves an
FID of $2.70$, significantly outperforming previous causal autoregressive
approaches. We hope our work inspires a principled rethinking of language
modeling on visual signals and beyond.

</details>


### [250] [Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion](https://arxiv.org/abs/2510.06687)
*Jie Luo,Yuxuan Jiang,Xin Jin,Mingyu Liu,Yihui Fan*

Main category: cs.CV

TL;DR: 提出首个融合光场数据和点云数据的多模态语义分割数据集，并提出Mlpfseg网络，该方法在分割效果上优于仅用图像或点云的方法。


<details>
  <summary>Details</summary>
Motivation: 语义分割在自动驾驶场景理解中面临复杂条件挑战，光场和LiDAR数据融合存在视角多样性有限和模态差异问题，需要解决这些挑战。

Method: 提出融合光场数据和点云数据的多模态语义分割数据集，基于此提出Mlpfseg网络，包含特征补全和深度感知模块。

Result: 该方法比仅图像分割的平均交并比高1.71，比仅点云分割的平均交并比高2.38。

Conclusion: 提出的方法有效。

Abstract: Semantic segmentation serves as a cornerstone of scene understanding in
autonomous driving but continues to face significant challenges under complex
conditions such as occlusion. Light field and LiDAR modalities provide
complementary visual and spatial cues that are beneficial for robust
perception; however, their effective integration is hindered by limited
viewpoint diversity and inherent modality discrepancies. To address these
challenges, the first multimodal semantic segmentation dataset integrating
light field data and point cloud data is proposed. Based on this dataset, we
proposed a multi-modal light field point-cloud fusion segmentation
network(Mlpfseg), incorporating feature completion and depth perception to
segment both camera images and LiDAR point clouds simultaneously. The feature
completion module addresses the density mismatch between point clouds and image
pixels by performing differential reconstruction of point-cloud feature maps,
enhancing the fusion of these modalities. The depth perception module improves
the segmentation of occluded objects by reinforcing attention scores for better
occlusion awareness. Our method outperforms image-only segmentation by 1.71
Mean Intersection over Union(mIoU) and point cloud-only segmentation by 2.38
mIoU, demonstrating its effectiveness.

</details>


### [251] [Vision Transformer for Transient Noise Classification](https://arxiv.org/abs/2510.06273)
*Divyansh Srivastava,Andrzej Niedzielski*

Main category: cs.CV

TL;DR: 使用ViT模型对LIGO数据中的瞬态噪声进行分类，实现92.26%的分类效率，展现其提升引力波检测准确性的潜力。


<details>
  <summary>Details</summary>
Motivation: LIGO数据中的瞬态噪声阻碍引力波检测，O3运行新增两类噪声，需训练新模型进行有效分类。

Method: 在包含Gravity Spy数据集和LIGO O3a运行新增两类噪声的组合数据集上，训练预训练的Vision Transformer (ViT - B/32) 模型。

Result: 实现了92.26%的分类效率。

Conclusion: Vision Transformer能有效区分瞬态噪声，有提升引力波检测准确性的潜力。

Abstract: Transient noise (glitches) in LIGO data hinders the detection of
gravitational waves (GW). The Gravity Spy project has categorized these noise
events into various classes. With the O3 run, there is the inclusion of two
additional noise classes and thus a need to train new models for effective
classification. We aim to classify glitches in LIGO data into 22 existing
classes from the first run plus 2 additional noise classes from O3a using the
Vision Transformer (ViT) model. We train a pre-trained Vision Transformer
(ViT-B/32) model on a combined dataset consisting of the Gravity Spy dataset
with the additional two classes from the LIGO O3a run. We achieve a
classification efficiency of 92.26%, demonstrating the potential of Vision
Transformer to improve the accuracy of gravitational wave detection by
effectively distinguishing transient noise.
  Key words: gravitational waves --vision transformer --machine learning

</details>


### [252] [General and Efficient Visual Goal-Conditioned Reinforcement Learning using Object-Agnostic Masks](https://arxiv.org/abs/2510.06277)
*Fahim Shahriar,Cheryl Wang,Alireza Azimi,Gautham Vasan,Hany Hamed Elanwar,A. Rupam Mahmood,Colin Bellinger*

Main category: cs.CV

TL;DR: 提出基于掩码的目标表示系统用于目标条件强化学习，有高效学习和泛化能力，实验效果好且可用于实际任务和迁移应用。


<details>
  <summary>Details</summary>
Motivation: 现有目标条件强化学习（GCRL）的目标表示方法存在泛化性差、收敛慢等问题，需要更好的目标表示系统。

Method: 提出基于掩码的目标表示系统，可处理掩码生成密集奖励，利用预训练的开放词汇目标检测模型生成掩码。

Result: 在模拟中使用真实掩码学习，对训练和未见测试对象达到99.9%的到达准确率，可高精度执行拾取任务，在两台物理机器人上实现从零学习和从模拟到真实的迁移应用。

Conclusion: 基于掩码的目标表示系统在GCRL中表现良好，能高效学习、泛化，可用于实际机器人任务和迁移学习。

Abstract: Goal-conditioned reinforcement learning (GCRL) allows agents to learn diverse
objectives using a unified policy. The success of GCRL, however, is contingent
on the choice of goal representation. In this work, we propose a mask-based
goal representation system that provides object-agnostic visual cues to the
agent, enabling efficient learning and superior generalization. In contrast,
existing goal representation methods, such as target state images, 3D
coordinates, and one-hot vectors, face issues of poor generalization to unseen
objects, slow convergence, and the need for special cameras. Masks can be
processed to generate dense rewards without requiring error-prone distance
calculations. Learning with ground truth masks in simulation, we achieved 99.9%
reaching accuracy on training and unseen test objects. Our proposed method can
be utilized to perform pick-up tasks with high accuracy, without using any
positional information of the target. Moreover, we demonstrate learning from
scratch and sim-to-real transfer applications using two different physical
robots, utilizing pretrained open vocabulary object detection models for mask
generation.

</details>


### [253] [Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities](https://arxiv.org/abs/2510.06743)
*Maria Levchenko*

Main category: cs.CV

TL;DR: 提出基于大语言模型的历史OCR评估方法，评估12个多模态大模型，发现部分模型表现好但有过历史化问题，后处理会降低性能并给出实践指南。


<details>
  <summary>Details</summary>
Motivation: 数字人文领域学者用大语言模型进行历史文档数字化，但缺乏合适评估框架，传统指标无法捕捉历史语料库创建中的时间偏差和特定时期错误。

Method: 以18世纪俄罗斯民用字体文本为研究对象，引入历史字符保留率（HCPR）和古体插入率（AIR）等新指标，以及污染控制和稳定性测试协议。

Result: 评估12个多模态大模型，Gemini和Qwen模型优于传统OCR，但存在过历史化问题，后处理会降低性能。

Conclusion: 该方法为数字人文从业者在历史语料库数字化中的模型选择和质量评估提供了指导。

Abstract: Digital humanities scholars increasingly use Large Language Models for
historical document digitization, yet lack appropriate evaluation frameworks
for LLM-based OCR. Traditional metrics fail to capture temporal biases and
period-specific errors crucial for historical corpus creation. We present an
evaluation methodology for LLM-based historical OCR, addressing contamination
risks and systematic biases in diplomatic transcription. Using 18th-century
Russian Civil font texts, we introduce novel metrics including Historical
Character Preservation Rate (HCPR) and Archaic Insertion Rate (AIR), alongside
protocols for contamination control and stability testing. We evaluate 12
multimodal LLMs, finding that Gemini and Qwen models outperform traditional OCR
while exhibiting over-historicization: inserting archaic characters from
incorrect historical periods. Post-OCR correction degrades rather than improves
performance. Our methodology provides digital humanities practitioners with
guidelines for model selection and quality assessment in historical corpus
digitization.

</details>


### [254] [Extreme Amodal Face Detection](https://arxiv.org/abs/2510.06791)
*Changlin Song,Yunzhong Hou,Michael Randall Barnes,Rahul Shome,Dylan Campbell*

Main category: cs.CV

TL;DR: 本文聚焦极端无模态人脸检测子问题，提出单图像样本无关方法，用基于热图的检测器和选择性粗细解码器，取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 极端无模态检测有别于普通无模态检测，人脸检测在安全和隐私方面有应用价值。

Method: 提出单图像样本无关方法，设计基于热图的极端无模态目标检测器，使用选择性粗细解码器。

Result: 在新任务上取得强结果，优于效率较低的生成式方法。

Conclusion: 所提方法能有效解决从少量图像信息预测大量帧外区域的问题，在极端无模态人脸检测中表现良好。

Abstract: Extreme amodal detection is the task of inferring the 2D location of objects
that are not fully visible in the input image but are visible within an
expanded field-of-view. This differs from amodal detection, where the object is
partially visible within the input image, but is occluded. In this paper, we
consider the sub-problem of face detection, since this class provides
motivating applications involving safety and privacy, but do not tailor our
method specifically to this class. Existing approaches rely on image sequences
so that missing detections may be interpolated from surrounding frames or make
use of generative models to sample possible completions. In contrast, we
consider the single-image task and propose a more efficient, sample-free
approach that makes use of the contextual cues from the image to infer the
presence of unseen faces. We design a heatmap-based extreme amodal object
detector that addresses the problem of efficiently predicting a lot (the
out-of-frame region) from a little (the image) with a selective coarse-to-fine
decoder. Our method establishes strong results for this new task, even
outperforming less efficient generative approaches.

</details>


### [255] [Scalable deep fusion of spaceborne lidar and synthetic aperture radar for global forest structural complexity mapping](https://arxiv.org/abs/2510.06299)
*Tiago de Conto,John Armston,Ralph Dubayah*

Main category: cs.CV

TL;DR: 提出融合GEDI与SAR数据的深度学习框架绘制全球高分辨率森林结构复杂性地图，性能高且可扩展，支持森林监测。


<details>
  <summary>Details</summary>
Motivation: 星载激光雷达GEDI稀疏采样限制连续高分辨率森林结构复杂性制图，需新方法解决。

Method: 提出可扩展深度学习框架，采用适应的EfficientNetV2架构，用超1.3亿GEDI足迹训练。

Result: 模型性能高（全球R2 = 0.82），参数少，能生成2015 - 2022年全球多时相森林结构复杂性数据集。

Conclusion: 该方法支持全球森林结构动态监测，为生物多样性保护和生态系统管理提供工具。

Abstract: Forest structural complexity metrics integrate multiple canopy attributes
into a single value that reflects habitat quality and ecosystem function.
Spaceborne lidar from the Global Ecosystem Dynamics Investigation (GEDI) has
enabled mapping of structural complexity in temperate and tropical forests, but
its sparse sampling limits continuous high-resolution mapping. We present a
scalable, deep learning framework fusing GEDI observations with multimodal
Synthetic Aperture Radar (SAR) datasets to produce global, high-resolution (25
m) wall-to-wall maps of forest structural complexity. Our adapted
EfficientNetV2 architecture, trained on over 130 million GEDI footprints,
achieves high performance (global R2 = 0.82) with fewer than 400,000
parameters, making it an accessible tool that enables researchers to process
datasets at any scale without requiring specialized computing infrastructure.
The model produces accurate predictions with calibrated uncertainty estimates
across biomes and time periods, preserving fine-scale spatial patterns. It has
been used to generate a global, multi-temporal dataset of forest structural
complexity from 2015 to 2022. Through transfer learning, this framework can be
extended to predict additional forest structural variables with minimal
computational cost. This approach supports continuous, multi-temporal
monitoring of global forest structural dynamics and provides tools for
biodiversity conservation and ecosystem management efforts in a changing
climate.

</details>


### [256] [Road Surface Condition Detection with Machine Learning using New York State Department of Transportation Camera Images and Weather Forecast Data](https://arxiv.org/abs/2510.06440)
*Carly Sutter,Kara J. Sulia,Nick P. Bassill,Christopher D. Wirz,Christopher D. Thorncroft,Jay C. Rothenberger,Vanessa Przybylo,Mariana G. Cains,Jacob Radford,David Aaron Evans*

Main category: cs.CV

TL;DR: 本文利用卷积神经网络和随机森林，基于摄像头图像和天气数据预测纽约州道路状况，模型在未见过的摄像头数据上准确率达81.5%。


<details>
  <summary>Details</summary>
Motivation: 纽约州运输部评估道路状况的工作劳动强度大，机器学习模型可为其在冬季天气事件中做关键运营决策提供额外支持。

Method: 使用卷积神经网络和随机森林，在约22000张人工标注的摄像头图像及天气数据上训练模型，图像被分为六种道路状况。

Result: 模型在完全未见过的摄像头数据上达到81.5%的准确率。

Conclusion: 该天气相关的道路状况模型具有一定的泛化能力，能满足纽约州运输部决策者的运营需求。

Abstract: The New York State Department of Transportation (NYSDOT) has a network of
roadside traffic cameras that are used by both the NYSDOT and the public to
observe road conditions. The NYSDOT evaluates road conditions by driving on
roads and observing live cameras, tasks which are labor-intensive but necessary
for making critical operational decisions during winter weather events.
However, machine learning models can provide additional support for the NYSDOT
by automatically classifying current road conditions across the state. In this
study, convolutional neural networks and random forests are trained on camera
images and weather data to predict road surface conditions. Models are trained
on a hand-labeled dataset of ~22,000 camera images, each classified by human
labelers into one of six road surface conditions: severe snow, snow, wet, dry,
poor visibility, or obstructed. Model generalizability is prioritized to meet
the operational needs of the NYSDOT decision makers, and the weather-related
road surface condition model in this study achieves an accuracy of 81.5% on
completely unseen cameras.

</details>


### [257] [Explaining raw data complexity to improve satellite onboard processing](https://arxiv.org/abs/2510.06858)
*Adrien Dorise,Marjorie Bellizzi,Adrien Girard,Benjamin Francesconi,Stéphane May*

Main category: cs.CV

TL;DR: 研究利用原始数据对深度学习模型在遥感目标检测和分类任务中的影响，发现原始数据训练的模型在高置信度下边界识别有问题，改进轮廓方法可提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着卫星处理能力提升，直接在卫星上部署AI模型可行，但使用原始传感器数据有新约束，当前解决方案多依赖预处理图像，故研究利用原始数据对模型的影响。

Method: 引入模拟工作流程从高分辨率L1图像生成类似原始产品，用原始和L1数据集训练两个目标检测模型（YOLOv11s和YOLOX - S），用标准检测指标和可解释性工具比较性能。

Result: 两个模型在低到中等置信度阈值下表现相似，原始数据训练的模型在高置信度水平下难以进行目标边界识别。

Conclusion: 调整AI架构并采用改进的轮廓方法可增强原始图像的目标检测，提升遥感星载AI性能。

Abstract: With increasing processing power, deploying AI models for remote sensing
directly onboard satellites is becoming feasible. However, new constraints
arise, mainly when using raw, unprocessed sensor data instead of preprocessed
ground-based products. While current solutions primarily rely on preprocessed
sensor images, few approaches directly leverage raw data. This study
investigates the effects of utilising raw data on deep learning models for
object detection and classification tasks. We introduce a simulation workflow
to generate raw-like products from high-resolution L1 imagery, enabling
systemic evaluation. Two object detection models (YOLOv11s and YOLOX-S) are
trained on both raw and L1 datasets, and their performance is compared using
standard detection metrics and explainability tools. Results indicate that
while both models perform similarly at low to medium confidence thresholds, the
model trained on raw data struggles with object boundary identification at high
confidence levels. It suggests that adapting AI architectures with improved
contouring methods can enhance object detection on raw images, improving
onboard AI for remote sensing.

</details>


### [258] [Cluster Paths: Navigating Interpretability in Neural Networks](https://arxiv.org/abs/2510.06541)
*Nicholas M. Kroeger,Vincent Bindschaedler*

Main category: cs.CV

TL;DR: 提出聚类路径的事后可解释性方法，引入评估指标，在多个实验中验证效果，还可作OOD检测器，能为大视觉模型生成解释。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络决策过程不透明，存在信任、偏差和失败风险，需要可解释性方法。

Method: 提出聚类路径方法，聚类选定层的激活值并以聚类ID序列表示输入，引入四个评估指标；扩展为概念路径；用作OOD检测器。

Result: 在CIFAR - 10实验中识别捷径，CelebA任务达90%忠实度和96%一致性，可作有效OOD检测器。

Conclusion: 聚类路径可扩展到大型视觉模型，能生成简洁易读的解释。

Abstract: While modern deep neural networks achieve impressive performance in vision
tasks, they remain opaque in their decision processes, risking unwarranted
trust, undetected biases and unexpected failures. We propose cluster paths, a
post-hoc interpretability method that clusters activations at selected layers
and represents each input as its sequence of cluster IDs. To assess these
cluster paths, we introduce four metrics: path complexity (cognitive load),
weighted-path purity (class alignment), decision-alignment faithfulness
(predictive fidelity), and path agreement (stability under perturbations). In a
spurious-cue CIFAR-10 experiment, cluster paths identify color-based shortcuts
and collapse when the cue is removed. On a five-class CelebA hair-color task,
they achieve 90% faithfulness and maintain 96% agreement under Gaussian noise
without sacrificing accuracy. Scaling to a Vision Transformer pretrained on
ImageNet, we extend cluster paths to concept paths derived from prompting a
large language model on minimal path divergences. Finally, we show that cluster
paths can serve as an effective out-of-distribution (OOD) detector, reliably
flagging anomalous samples before the model generates over-confident
predictions. Cluster paths uncover visual concepts, such as color palettes,
textures, or object contexts, at multiple network depths, demonstrating that
cluster paths scale to large vision models while generating concise and
human-readable explanations.

</details>


### [259] [Generating Surface for Text-to-3D using 2D Gaussian Splatting](https://arxiv.org/abs/2510.06967)
*Huanning Dong,Fan Li,Ping Kuang,Jianwen Min*

Main category: cs.CV

TL;DR: 本文提出DirectGaussian方法生成3D对象表面，实验证明能实现多样且高保真3D内容创建。


<details>
  <summary>Details</summary>
Motivation: 自然世界物体几何形状复杂，当前3D内容生成方法有局限，生成3D内容仍是挑战。

Method: 提出DirectGaussian方法，利用条件文本生成模型，通过2D高斯散点图结合多视图法线和纹理先验渲染3D对象表面，在优化过程中加入曲率约束解决多视图几何一致性问题。

Result: 通过大量实验，证明框架能实现多样且高保真的3D内容创建。

Conclusion: DirectGaussian方法可有效解决3D内容生成问题，实现高质量3D内容创建。

Abstract: Recent advancements in Text-to-3D modeling have shown significant potential
for the creation of 3D content. However, due to the complex geometric shapes of
objects in the natural world, generating 3D content remains a challenging task.
Current methods either leverage 2D diffusion priors to recover 3D geometry, or
train the model directly based on specific 3D representations. In this paper,
we propose a novel method named DirectGaussian, which focuses on generating the
surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize
conditional text generation models and the surface of a 3D object is rendered
by 2D Gaussian splatting with multi-view normal and texture priors. For
multi-view geometric consistency problems, DirectGaussian incorporates
curvature constraints on the generated surface during optimization process.
Through extensive experiments, we demonstrate that our framework is capable of
achieving diverse and high-fidelity 3D content creation.

</details>


### [260] [Learning Global Representation from Queries for Vectorized HD Map Construction](https://arxiv.org/abs/2510.06969)
*Shoumeng Qiu,Xinrun Li,Yang Long,Xiangyang Xue,Varun Ojha,Jian Pu*

Main category: cs.CV

TL;DR: 提出MapGR架构用于在线构建矢量化高清地图，在数据集上验证效果好。


<details>
  <summary>Details</summary>
Motivation: 现有基于DETR框架的方法存在局部查询视角问题，忽略高清地图的全局表示。

Method: 提出MapGR架构，含全局表示学习（GRL）和全局表示引导（GRG）两个协同模块。

Result: 在nuScenes和Argoverse2数据集上，平均精度均值（mAP）较领先基线有显著提升。

Conclusion: MapGR架构能有效学习和利用全局表示，用于高清地图构建效果良好。

Abstract: The online construction of vectorized high-definition (HD) maps is a
cornerstone of modern autonomous driving systems. State-of-the-art approaches,
particularly those based on the DETR framework, formulate this as an instance
detection problem. However, their reliance on independent, learnable object
queries results in a predominantly local query perspective, neglecting the
inherent global representation within HD maps. In this work, we propose
\textbf{MapGR} (\textbf{G}lobal \textbf{R}epresentation learning for HD
\textbf{Map} construction), an architecture designed to learn and utilize a
global representations from queries. Our method introduces two synergistic
modules: a Global Representation Learning (GRL) module, which encourages the
distribution of all queries to better align with the global map through a
carefully designed holistic segmentation task, and a Global Representation
Guidance (GRG) module, which endows each individual query with explicit,
global-level contextual information to facilitate its optimization. Evaluations
on the nuScenes and Argoverse2 datasets validate the efficacy of our approach,
demonstrating substantial improvements in mean Average Precision (mAP) compared
to leading baselines.

</details>


### [261] [Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking](https://arxiv.org/abs/2510.06820)
*Mitchell Keren Taraday,Shahaf Wagner,Chaim Baskin*

Main category: cs.CV

TL;DR: 提出高效判别联合编码器EDJE，预计算视觉令牌并压缩，减少存储和在线计算，实现高吞吐量推理。


<details>
  <summary>Details</summary>
Motivation: 现有视觉 - 语言重排器因视觉特征提取阶段昂贵，难以大规模实际部署。

Method: 引入EDJE，离线预计算视觉令牌，通过轻量级注意力适配器压缩，在线推理时对少量视觉令牌和文本运行紧凑联合编码器。

Result: EDJE每秒可处理50k图像 - 文本对，每张图像仅需49kB磁盘存储，在Flickr和COCO检索上与现有技术相当。

Conclusion: EDJE在保持强检索性能的同时，大幅减少存储和在线计算，可实现高吞吐量推理。

Abstract: Multimodal retrieval still leans on embedding-based models like CLIP for fast
vector search over pre-computed image embeddings. Yet, unlike text retrieval,
where joint-encoder rerankers are standard, comparable vision--language
rerankers are largely absent. We find that seminal joint encoders such as BLIP
are severely bottlenecked by an expensive visual feature-extraction stage,
preventing practical deployment at scale. Motivated by this bottleneck, we
introduce EDJE, an Efficient Discriminative Joint Encoder that precomputes
vision tokens offline and compresses them via a lightweight attention-based
adapter, so online inference runs only a compact joint encoder over a small set
of visual tokens plus the text. EDJE preserves strong retrieval performance
while drastically reducing storage and online compute, enabling high-throughput
inference. Specifically, EDJE processes 50k image--text pairs/second while
requiring 49kB of disk storage per image, matching prior art on Flickr
(zero-shot) and COCO (fine-tuned) retrieval. The implementation and checkpoints
will be made publicly available shortly.

</details>


### [262] [Graph Conditioned Diffusion for Controllable Histopathology Image Generation](https://arxiv.org/abs/2510.07129)
*Sarah Cechnicka,Matthew Baugh,Weitong Zhang,Mischa Dombrowski,Zhe Li,Johannes C. Paetzold,Candice Roufosse,Bernhard Kainz*

Main category: cs.CV

TL;DR: 提出基于图的对象级表示用于图条件扩散，解决DPMs在医学图像可控生成的难题，生成数据可替代标注数据用于下游分割任务。


<details>
  <summary>Details</summary>
Motivation: 现有扩散概率模型（DPMs）在医学图像可控生成方面存在挑战，因其在缺乏语义结构和强先验的噪声潜在空间中运行，难以确保对生成内容的有意义控制。

Method: 提出基于图的对象级表示用于图条件扩散，生成对应图像主要结构的图节点，用Transformer模块处理图表示并通过文本条件机制集成到扩散模型中。

Result: 在真实世界的组织病理学用例中评估，生成的数据能可靠替代标注患者数据用于下游分割任务。

Conclusion: 所提出的方法有效解决了DPMs在医学图像可控生成方面的问题，生成的数据可用于下游任务。

Abstract: Recent advances in Diffusion Probabilistic Models (DPMs) have set new
standards in high-quality image synthesis. Yet, controlled generation remains
challenging, particularly in sensitive areas such as medical imaging. Medical
images feature inherent structure such as consistent spatial arrangement, shape
or texture, all of which are critical for diagnosis. However, existing DPMs
operate in noisy latent spaces that lack semantic structure and strong priors,
making it difficult to ensure meaningful control over generated content. To
address this, we propose graph-based object-level representations for
Graph-Conditioned-Diffusion. Our approach generates graph nodes corresponding
to each major structure in the image, encapsulating their individual features
and relationships. These graph representations are processed by a transformer
module and integrated into a diffusion model via the text-conditioning
mechanism, enabling fine-grained control over generation. We evaluate this
approach using a real-world histopathology use case, demonstrating that our
generated data can reliably substitute for annotated patient data in downstream
segmentation tasks. The code is available here.

</details>


### [263] [Resolution scaling governs DINOv3 transfer performance in chest radiograph classification](https://arxiv.org/abs/2510.07191)
*Soroosh Tayebi Arasteh,Mina Shaigan,Christiane Kuhl,Jakob Nikolas Kather,Sven Nebelung,Daniel Truhn*

Main category: cs.CV

TL;DR: 研究对比DINOv3、DINOv2和ImageNet初始化在胸部X光片上的表现，发现512x512分辨率下DINOv3 - 初始化的ConvNeXt - B网络性能最佳，支持使用微调的中型骨干网络进行解读。


<details>
  <summary>Details</summary>
Motivation: 探究自监督学习在胸部X光片上的价值，以及Meta的DINOv3设计选择是否能改善胸部X光片的迁移学习。

Method: 在七个数据集上对DINOv3、DINOv2和ImageNet初始化进行基准测试，评估两种骨干网络，分析不同像素分辨率图像，评估7B模型的冻结特征。

Result: 224x224时DINOv3和DINOv2在成人数据集上性能相当；512x512时DINOv3优于DINOv2和ImageNet；儿科队列无差异；ConvNeXt - B表现优于ViT - B/16；冻结特征模型表现不如全微调模型；1024x1024未提升精度。

Conclusion: 胸部X光片中，高输入分辨率对利用现代自监督模型很关键，512x512像素是实用上限，支持使用微调的中型骨干网络进行解读。

Abstract: Self-supervised learning (SSL) has advanced visual representation learning,
but its value in chest radiography, a high-volume imaging modality with
fine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL
models through Gram-anchored self-distillation. Whether these design choices
improve transfer learning for chest radiography has not been systematically
tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across
seven datasets (n>814,000). Two representative backbones were evaluated:
ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and
1024x1024 pixels. We additionally assessed frozen features from a 7B model. The
primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2
achieved comparable performance on adult datasets. Increasing resolution to
512x512 yielded consistent improvements for DINOv3 over both DINOv2 and
ImageNet. In contrast, results in pediatric cohort showed no differences across
initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models
using frozen DINOv3-7B features underperformed relative to fully finetuned
86-89M-parameter backbones, highlighting the importance of domain adaptation.
Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains
were most evident for boundary-dependent and small focal abnormalities. In
chest radiography, higher input resolution is critical for leveraging the
benefits of modern self-supervised models. 512x512 pixels represent a practical
upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest
performance, while larger inputs offer minimal return on cost. Clinically,
these findings support use of finetuned, mid-sized backbones at 512x512 for
chest radiograph interpretation, with the greatest gains expected in detecting
subtle or boundary-centered lesions relevant to emergency and critical care
settings.

</details>


### [264] [GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation](https://arxiv.org/abs/2510.07217)
*Wen Ye,Zhaocheng Liu,Yuwei Gui,Tingyu Yuan,Yunyue Su,Bowen Fang,Chaoyang Zhao,Qiang Liu,Liang Wang*

Main category: cs.CV

TL;DR: 现有文本到图像合成方法在处理复杂长提示时存在问题，本文提出GenPilot多智能体系统进行测试时提示优化，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像合成方法在处理复杂长提示时会出现语义不一致和细节缺失问题，现有解决方案存在模型特定、缺乏系统误差分析和细化策略、可解释性和适应性有限等不足。

Method: 提出名为GenPilot的即插即用多智能体系统，集成误差分析、基于聚类的自适应探索、细粒度验证和内存模块进行迭代优化。

Result: 在DPG - bench和Geneval上实验，分别提升了16.9%和5.7%，增强了文本与图像一致性和生成图像的结构连贯性。

Conclusion: 提出的测试时提示优化策略有效，代码开源。

Abstract: Text-to-image synthesis has made remarkable progress, yet accurately
interpreting complex and lengthy prompts remains challenging, often resulting
in semantic inconsistencies and missing details. Existing solutions, such as
fine-tuning, are model-specific and require training, while prior automatic
prompt optimization (APO) approaches typically lack systematic error analysis
and refinement strategies, resulting in limited reliability and effectiveness.
Meanwhile, test-time scaling methods operate on fixed prompts and on noise or
sample numbers, limiting their interpretability and adaptability. To solve
these, we introduce a flexible and efficient test-time prompt optimization
strategy that operates directly on the input text. We propose a plug-and-play
multi-agent system called GenPilot, integrating error analysis,
clustering-based adaptive exploration, fine-grained verification, and a memory
module for iterative optimization. Our approach is model-agnostic,
interpretable, and well-suited for handling long and complex prompts.
Simultaneously, we summarize the common patterns of errors and the refinement
strategy, offering more experience and encouraging further exploration.
Experiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7%
demonstrate the strong capability of our methods in enhancing the text and
image consistency and structural coherence of generated images, revealing the
effectiveness of our test-time prompt optimization strategy. The code is
available at https://github.com/27yw/GenPilot.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [265] [Trickle-down Theorems via C-Lorentzian Polynomials II: Pairwise Spectral Influence and Improved Dobrushin's Condition](https://arxiv.org/abs/2510.06549)
*Jonathan Leake,Shayan Oveis Gharan*

Main category: math.CO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Let $\mu$ be a probability distribution on a multi-state spin system on a set
$V$ of sites. Equivalently, we can think of this as a $d$-partite simplical
complex with distribution $\mu$ on maximal faces. For any pair of vertices
$u,v\in V$, define the pairwise spectral influence $\mathcal{I}_{u,v}$ as
follows. Let $\sigma$ be a choice of spins $s_w\in S_w$ for every $w\in V
\setminus \{u,v\}$, and construct a matrix in $\mathbb{R}^{(S_u\cup S_v)\times
(S_u\cup S_v)}$ where for any $s_u\in S_u, s_v\in S_v$, the $(us_u,vs_v)$-entry
is the probability that $s_v$ is the spin of $v$ conditioned on $s_u$ being the
spin of $u$ and on $\sigma$. Then $\mathcal{I}_{u,v}$ is the maximal second
eigenvalue of this matrix, over all choices of spins for all $w \in V \setminus
\{u,v\}$. Equivalently, $\mathcal{I}_{u,v}$ is the maximum local spectral
expansion of links of codimension $2$ that include a spin for every $w \in V
\setminus \{u,v\}$.
  We show that if the largest eigenvalue of the pairwise spectral influence
matrix with entries $\mathcal{I}_{u,v}$ is bounded away from 1, i.e.
$\lambda_{\max}(\mathcal{I})\leq 1-\epsilon$ (and $X$ is connected), then the
Glauber dynamics mixes rapidly and generate samples from $\mu$. This
improves/generalizes the classical Dobrushin's influence matrix as the
$\mathcal{I}_{u,v}$ lower-bounds the classical influence of $u\to v$. As a
by-product, we also prove improved/almost optimal trickle-down theorems for
partite simplicial complexes. The proof builds on the trickle-down theorems via
$\mathcal{C}$-Lorentzian polynomials machinery recently developed by the
authors and Lindberg.

</details>


### [266] [Extending Ghouila-Houri's Characterization of Comparability Graphs to Temporal Graphs](https://arxiv.org/abs/2510.06849)
*Pierre Charbit,Michel Habib,Amalia Sorondo*

Main category: math.CO

TL;DR: 本文重新审视Mertzios等人的模型，为时间场景提出类似Ghouila - Houri的特征描述，给出结构定理、高效识别算法，扩展模型到多标签时间图并证明结果适用，还通过禁止时间有序模式刻画时间可比图。


<details>
  <summary>Details</summary>
Motivation: 重新审视Mertzios等人提出的时间传递性模型，为时间场景找到类似Ghouila - Houri的特征描述。

Method: 提出结构定理，用2 - SAT公式表达时间传递性方向的约束，扩展模型到多标签时间图。

Result: 得到高效识别允许时间传递性方向的图的算法，证明之前结果在多标签设置下仍成立。

Conclusion: 为时间传递性方向的图提供了特征描述、识别算法，且结果可扩展到多标签时间图，还刻画了时间可比图。

Abstract: An orientation of a given static graph is called transitive if for any three
vertices $a,b,c$, the presence of arcs $(a,b)$ and $(b,c)$ forces the presence
of the arc $(a,c)$. If only the presence of an arc between $a$ and $c$ is
required, but its orientation is unconstrained, the orientation is called
quasi-transitive. A fundamental result presented by Ghouila-Houri guarantees
that any static graph admitting a quasi-transitive orientation also admits a
transitive orientation. In a seminal work, Mertzios et al. introduced the
notion of temporal transitivity in order to model information flows in simple
temporal networks. We revisit the model introduced by Mertzios et al. and
propose an analogous to Ghouila-Houri's characterization for the temporal
scenario. We present a structure theorem that will allow us to express by a
2-SAT formula all the constraints imposed by temporal transitive orientations.
The latter produces an efficient recognition algorithm for graphs admitting
such orientations. Additionally, we extend the temporal transitivity model to
temporal graphs having multiple time-labels associated to their edges and claim
that the previous results hold in the multilabel setting. Finally, we propose a
characterization of temporal comparability graphs via forbidden temporal
ordered patterns.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [267] [Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets](https://arxiv.org/abs/2510.06240)
*Jiqun Pan,Zhenke Duan,Jiani Tu,Anzhi Cheng,Yanqing Wang*

Main category: cs.CL

TL;DR: 提出KG - MASD方法解决工业问答系统问题，实验显示该方法提升准确率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 工业问答系统需更高安全性和可靠性，多智能体大语言模型有迭代不受控和输出不可验证问题，传统蒸馏方法难转移协作推理能力。

Method: 将蒸馏表述为马尔可夫决策过程，引入知识图谱作为可验证的结构化先验，生成高置信度指令调优数据并联合蒸馏推理深度和可验证性到学生模型。

Result: 在工业问答数据集上，相比基线准确率提高2.4% - 20.1%，显著提升可靠性。

Conclusion: KG - MASD可实现安全关键工业场景下可信AI部署。

Abstract: Industrial question-answering (QA) systems require higher safety and
reliability than general-purpose dialogue models, as errors in high-risk
scenarios such as equipment fault diagnosis can have severe consequences.
Although multi-agent large language models enhance reasoning depth, they suffer
from uncontrolled iterations and unverifiable outputs, and conventional
distillation methods struggle to transfer collaborative reasoning capabilities
to lightweight, deployable student models. To address these challenges, we
propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our
approach formulates distillation as a Markov Decision Process and incorporates
a knowledge graph as a verifiable structured prior to enrich state
representation and ensure convergence. By integrating collaborative reasoning
with knowledge grounding, KG-MASD generates high-confidence instruction-tuning
data and jointly distills reasoning depth and verifiability into compact
student models suitable for edge deployment. Experiments on an industrial QA
dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent
over baselines and significantly enhances reliability, enabling trustworthy AI
deployment in safety-critical industrial scenarios. Code and data are available
at https://github.com/erwinmsmith/KG-MAD/.

</details>


### [268] [Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization](https://arxiv.org/abs/2510.06732)
*Tiancheng Xing,Jerry Li,Yixuan Du,Xiyang Hu*

Main category: cs.CL

TL;DR: 提出RAF方法暴露大语言模型排序易受攻击问题，实验显示其效果好，指出LLM重排序有安全隐患。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为重排序器时排序行为易被小提示引导，需暴露该脆弱性。

Method: 提出两阶段的RAF方法，第一阶段用贪心坐标梯度筛选候选词，第二阶段用熵动态加权方案评估并采样选择。

Result: 多个大语言模型实验表明，RAF能显著提升目标项排名，在提升排名和保持自然度上比现有方法更稳健。

Conclusion: 基于LLM的重排序易受对抗性操纵，给现代检索系统的可信度和鲁棒性带来新挑战。

Abstract: Large language models (LLMs) are increasingly used as rerankers in
information retrieval, yet their ranking behavior can be steered by small,
natural-sounding prompts. To expose this vulnerability, we present Rank
Anything First (RAF), a two-stage token optimization method that crafts concise
textual perturbations to consistently promote a target item in LLM-generated
rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate
Gradient to shortlist candidate tokens at the current position by combining the
gradient of the rank-target with a readability score; Stage 2 evaluates those
candidates under exact ranking and readability losses using an entropy-based
dynamic weighting scheme, and selects a token via temperature-controlled
sampling. RAF generates ranking-promoting prompts token-by-token, guided by
dual objectives: maximizing ranking effectiveness and preserving linguistic
naturalness. Experiments across multiple LLMs show that RAF significantly
boosts the rank of target items using naturalistic language, with greater
robustness than existing methods in both promoting target items and maintaining
naturalness. These findings underscore a critical security implication:
LLM-based reranking is inherently susceptible to adversarial manipulation,
raising new challenges for the trustworthiness and robustness of modern
retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.

</details>


### [269] [Overview of the Plagiarism Detection Task at PAN 2025](https://arxiv.org/abs/2510.06805)
*André Greiner-Petter,Maik Fröbe,Jan Philip Wahle,Terry Ruas,Bela Gipp,Akiko Aizawa,Martin Potthast*

Main category: cs.CL

TL;DR: 本文围绕PAN 2025生成式抄袭检测任务，介绍新数据集创建，对比参与者和基线结果，发现当前基于嵌入向量的简单语义相似性方法在本次任务有不错结果，但在2015年数据集上泛化性不足。


<details>
  <summary>Details</summary>
Motivation: 解决PAN 2025中自动生成的科学文章文本抄袭检测及与源文本对齐问题。

Method: 用Llama、DeepSeek - R1和Mistral三个大语言模型创建大规模自动生成抄袭数据集，对比参与者和四个基线结果，在PAN 2015抄袭检测任务上评估结果。

Result: 基于嵌入向量的简单语义相似性方法在本次任务有高达0.8召回率和0.5精确率的结果，但在2015年数据集上表现不佳。

Conclusion: 当前方法缺乏泛化性。

Abstract: The generative plagiarism detection task at PAN 2025 aims at identifying
automatically generated textual plagiarism in scientific articles and aligning
them with their respective sources. We created a novel large-scale dataset of
automatically generated plagiarism using three large language models: Llama,
DeepSeek-R1, and Mistral. In this task overview paper, we outline the creation
of this dataset, summarize and compare the results of all participants and four
baselines, and evaluate the results on the last plagiarism detection task from
PAN 2015 in order to interpret the robustness of the proposed approaches. We
found that the current iteration does not invite a large variety of approaches
as naive semantic similarity approaches based on embedding vectors provide
promising results of up to 0.8 recall and 0.5 precision. In contrast, most of
these approaches underperform significantly on the 2015 dataset, indicating a
lack in generalizability.

</details>


### [270] [Towards Reliable Retrieval in RAG Systems for Large Legal Datasets](https://arxiv.org/abs/2510.06999)
*Markus Reuter,Tobias Lingenberg,Rūta Liepiņa,Francesca Lagioia,Marco Lippi,Giovanni Sartor,Andrea Passerini,Burcu Sayin*

Main category: cs.CL

TL;DR: 本文指出RAG在法律应用中检索准确性问题，提出SAC方法，实验表明其能减少DRM、提升检索指标，通用摘要策略表现更好。


<details>
  <summary>Details</summary>
Motivation: RAG在法律应用中检索步骤准确性影响可靠性，法律领域检索系统易因文档结构相似而失败。

Method: 识别并量化DRM，提出Summary - Augmented Chunking (SAC)方法，为文本块添加文档级合成摘要。

Result: SAC大大减少DRM，提高文本级检索的精确率和召回率，通用摘要策略优于结合法律专家领域知识的方法。

Conclusion: SAC是实用、可扩展且易集成的技术，能提升RAG系统在大规模法律文档数据集上的可靠性。

Abstract: Retrieval-Augmented Generation (RAG) is a promising approach to mitigate
hallucinations in Large Language Models (LLMs) for legal applications, but its
reliability is critically dependent on the accuracy of the retrieval step. This
is particularly challenging in the legal domain, where large databases of
structurally similar documents often cause retrieval systems to fail. In this
paper, we address this challenge by first identifying and quantifying a
critical failure mode we term Document-Level Retrieval Mismatch (DRM), where
the retriever selects information from entirely incorrect source documents. To
mitigate DRM, we investigate a simple and computationally efficient technique
which we refer to as Summary-Augmented Chunking (SAC). This method enhances
each text chunk with a document-level synthetic summary, thereby injecting
crucial global context that would otherwise be lost during a standard chunking
process. Our experiments on a diverse set of legal information retrieval tasks
show that SAC greatly reduces DRM and, consequently, also improves text-level
retrieval precision and recall. Interestingly, we find that a generic
summarization strategy outperforms an approach that incorporates legal expert
domain knowledge to target specific legal elements. Our work provides evidence
that this practical, scalable, and easily integrable technique enhances the
reliability of RAG systems when applied to large-scale legal document datasets.

</details>


### [271] [Vibe Checker: Aligning Code Evaluation with Human Preference](https://arxiv.org/abs/2510.07315)
*Ming Zhong,Xiang Zhou,Ting-Yun Chang,Qingze Wang,Nan Xu,Xiance Si,Dan Garrette,Shyam Upadhyay,Jeremiah Liu,Jiawei Han,Benoit Schillings,Jiao Sun*

Main category: cs.CL

TL;DR: 本文指出当前代码评估仅关注功能正确性，提出指令遵循是影响编码中人类偏好的因素，推出VeriCode和Vibe Checker评估大语言模型，发现模型遵循多指令存在困难，功能与指令遵循的综合得分与人类偏好相关性最高。


<details>
  <summary>Details</summary>
Motivation: 当前代码评估仅关注功能正确性，忽视了用户日常应用的非功能指令，本文旨在研究影响编码中人类偏好的因素。

Method: 提出包含30条可验证代码指令的VeriCode分类法及对应验证器，用其扩充现有评估套件得到Vibe Checker测试平台。

Result: 评估31个领先大语言模型，发现即使最强模型遵循多指令也有困难且出现功能衰退，功能正确性和指令遵循的综合得分与人类偏好相关性最好。

Conclusion: 确定了编码中“感觉检查”的核心因素，为开发更符合用户偏好的模型提供了具体方向。

Abstract: Large Language Models (LLMs) have catalyzed vibe coding, where users leverage
LLMs to generate and iteratively refine code through natural language
interactions until it passes their vibe check. Vibe check is tied to real-world
human preference and goes beyond functionality: the solution should feel right,
read cleanly, preserve intent, and remain correct. However, current code
evaluation remains anchored to pass@k and captures only functional correctness,
overlooking the non-functional instructions that users routinely apply. In this
paper, we hypothesize that instruction following is the missing piece
underlying vibe check that represents human preference in coding besides
functional correctness. To quantify models' code instruction following
capabilities with measurable signals, we present VeriCode, a taxonomy of 30
verifiable code instructions together with corresponding deterministic
verifiers. We use the taxonomy to augment established evaluation suites,
resulting in Vibe Checker, a testbed to assess both code instruction following
and functional correctness. Upon evaluating 31 leading LLMs, we show that even
the strongest models struggle to comply with multiple instructions and exhibit
clear functional regression. Most importantly, a composite score of functional
correctness and instruction following correlates the best with human
preference, with the latter emerging as the primary differentiator on
real-world programming tasks. Our work identifies core factors of the vibe
check, providing a concrete path for benchmarking and developing models that
better align with user preferences in coding.

</details>


### [272] [WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives](https://arxiv.org/abs/2510.05336)
*Yongan Yu,Xianda Du,Qingchen Hu,Jiahao Liang,Jingwei Ni,Dan Qiang,Kaiyu Huang,Grant McKenzie,Renee Sieber,Fengran Mo*

Main category: cs.CL

TL;DR: 本文介绍首个评估历史天气档案检索增强生成系统的基准WeatherArchive - Bench，实验揭示现有系统局限，数据集和评估框架公开。


<details>
  <summary>Details</summary>
Motivation: 历史天气档案虽对气候研究有价值，但难以转化为结构化知识，需评估检索增强生成系统。

Method: 构建包含WeatherArchive - Retrieval和WeatherArchive - Assessment两个任务的WeatherArchive - Bench基准进行实验。

Result: 实验显示密集检索器在历史术语上常失败，大语言模型常误解脆弱性和恢复力概念。

Conclusion: 指出当前系统在推理复杂社会指标方面的关键局限，为设计更稳健的气候相关检索增强生成系统提供见解。

Abstract: Historical archives on weather events are collections of enduring primary
source records that offer rich, untapped narratives of how societies have
experienced and responded to extreme weather events. These qualitative accounts
provide insights into societal vulnerability and resilience that are largely
absent from meteorological records, making them valuable for climate scientists
to understand societal responses. However, their vast scale, noisy digitized
quality, and archaic language make it difficult to transform them into
structured knowledge for climate research. To address this challenge, we
introduce WeatherArchive-Bench, the first benchmark for evaluating
retrieval-augmented generation (RAG) systems on historical weather archives.
WeatherArchive-Bench comprises two tasks: WeatherArchive-Retrieval, which
measures a system's ability to locate historically relevant passages from over
one million archival news segments, and WeatherArchive-Assessment, which
evaluates whether Large Language Models (LLMs) can classify societal
vulnerability and resilience indicators from extreme weather narratives.
Extensive experiments across sparse, dense, and re-ranking retrievers, as well
as a diverse set of LLMs, reveal that dense retrievers often fail on historical
terminology, while LLMs frequently misinterpret vulnerability and resilience
concepts. These findings highlight key limitations in reasoning about complex
societal indicators and provide insights for designing more robust
climate-focused RAG systems from archival contexts. The constructed dataset and
evaluation framework are publicly available at
https://anonymous.4open.science/r/WeatherArchive-Bench/.

</details>


### [273] [Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses](https://arxiv.org/abs/2510.06242)
*Subin An,Yugyeong Ji,Junyoung Kim,Heejin Kook,Yang Lu,Josh Seltzer*

Main category: cs.CL

TL;DR: 提出针对人类调查回复的两阶段评估框架，验证显示其优于现有指标且实用性高。


<details>
  <summary>Details</summary>
Motivation: 开放式调查回复中低质量回复需有效评估，现有自动评估方法不适用于人类回复。

Method: 提出两阶段评估框架，先进行乱码过滤，再从努力、相关性和完整性三个维度利用大语言模型能力评估。

Result: 在英语和韩语数据集上验证，框架优于现有指标，与专家评估有强相关性。

Conclusion: 框架具有高实际应用价值，可用于回复质量预测和拒绝回复等。

Abstract: Open-ended survey responses provide valuable insights in marketing research,
but low-quality responses not only burden researchers with manual filtering but
also risk leading to misleading conclusions, underscoring the need for
effective evaluation. Existing automatic evaluation methods target
LLM-generated text and inadequately assess human-written responses with their
distinct characteristics. To address such characteristics, we propose a
two-stage evaluation framework specifically designed for human survey
responses. First, gibberish filtering removes nonsensical responses. Then,
three dimensions-effort, relevance, and completeness-are evaluated using LLM
capabilities, grounded in empirical analysis of real-world survey data.
Validation on English and Korean datasets shows that our framework not only
outperforms existing metrics but also demonstrates high practical applicability
for real-world applications such as response quality prediction and response
rejection, showing strong correlations with expert assessment.

</details>


### [274] [CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning](https://arxiv.org/abs/2510.06243)
*Qihua Dong,Luis Figueroa,Handong Zhao,Kushal Kafle,Jason Kuen,Zhihong Ding,Scott Cohen,Yun Fu*

Main category: cs.CL

TL;DR: 提出CoT Referring策略解决指代表达理解和分割问题，实验显示比基线模型有2.5%+提升。


<details>
  <summary>Details</summary>
Motivation: 指代表达理解和分割是评估多模态大语言模型能力的关键任务，需解决相关挑战。

Method: 提出CoT Referring策略，通过结构化思维链训练数据结构增强跨模态推理；重构训练数据，提供新注释，编译评估基准；将检测和分割能力集成到统一框架，用自适应加权损失训练。

Result: 在自定义基准和RefCOCO/+/g上实验，比基线模型有2.5%+的显著提升。

Conclusion: 所提CoT Referring策略有效，能提高复杂查询场景的准确性。

Abstract: Referring Expression Comprehension and Segmentation are critical tasks for
assessing the integration of language understanding and image comprehension,
serving as benchmarks for Multimodal Large Language Models (MLLMs)
capabilities. To address these challenges, we propose a new strategy, CoT
Referring, which enhances model reasoning across modalities through a
structured, chain-of-thought training data structure. Our approach
systematically parses textual structures to a sequential referring step, where
in each step it identifies relationships and ensures consistent reference
alignment, thereby improving accuracy in complex query scenarios. We
restructure the training data to enforce a new output form, providing new
annotations for existing datasets and compiling an evaluation benchmark from
existing resources. This benchmark is designed explicitly for complex referring
cases. We also integrate detection and segmentation capabilities into a unified
MLLM framework, training it with a novel adaptive weighted loss to optimize
performance. Experimental results on our curated benchmark and RefCOCO/+/g
demonstrate the effectiveness of our approach, with a notable increase of 2.5%+
over baseline models.

</details>


### [275] [Evaluating Embedding Frameworks for Scientific Domain](https://arxiv.org/abs/2510.06244)
*Nouman Ahmed,Ronin Wu,Victor Botev*

Main category: cs.CL

TL;DR: 聚焦科学领域，寻找最优词表示算法和分词方法，构建评估套件并测试。


<details>
  <summary>Details</summary>
Motivation: 在特定领域中找到最优词表示算法，因同一单词在不同领域和语境有不同含义，且生成式AI和Transformer架构预训练耗时耗算力。

Method: 构建包含多个下游任务和相关数据集的评估套件，用其测试各种词表示和分词算法。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Finding an optimal word representation algorithm is particularly important in
terms of domain specific data, as the same word can have different meanings and
hence, different representations depending on the domain and context. While
Generative AI and transformer architecture does a great job at generating
contextualized embeddings for any given work, they are quite time and compute
extensive, especially if we were to pre-train such a model from scratch. In
this work, we focus on the scientific domain and finding the optimal word
representation algorithm along with the tokenization method that could be used
to represent words in the scientific domain. The goal of this research is two
fold: 1) finding the optimal word representation and tokenization methods that
can be used in downstream scientific domain NLP tasks, and 2) building a
comprehensive evaluation suite that could be used to evaluate various word
representation and tokenization algorithms (even as new ones are introduced) in
the scientific domain. To this end, we build an evaluation suite consisting of
several downstream tasks and relevant datasets for each task. Furthermore, we
use the constructed evaluation suite to test various word representation and
tokenization algorithms.

</details>


### [276] [TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B](https://arxiv.org/abs/2510.06249)
*Toshiki Nakai,Ravi Kiran Chikkala,Lena Sophie Oberkircher,Nicholas Jennings,Natalia Skachkova,Tatiana Anikina,Jesujoba Oluwadara Alabi*

Main category: cs.CL

TL;DR: 研究探索在解码器多语言大模型特定层强制跨语言相似性能否提升低资源语言到高资源语言的翻译质量，提出TRepLiNa方法并实验，证明其是提升低资源语言翻译的低成本实用方法。


<details>
  <summary>Details</summary>
Motivation: 解决印度低资源语言资源匮乏的问题，提升低资源语言到高资源语言的翻译质量。

Method: 将Centered Kernel Alignment (CKA)和REPINA结合成TRepLiNa方法，在零样本、少样本和微调设置下用Aya - 23 8B和QLoRA进行实验。

Result: 使用TRepLiNa对齐中间层是提升低资源语言翻译的低成本实用方法，在数据稀缺场景效果佳。

Conclusion: TRepLiNa方法能有效提升低资源语言翻译质量，尤其适用于数据稀缺场景。

Abstract: The 2025 Multimodal Models for Low-Resource Contexts and Social Impact
(MMLoSo) Language Challenge addresses one of India's most pressing linguistic
gaps: the lack of resources for its diverse low-resource languages (LRLs). In
this study, we investigate whether enforcing cross-lingual similarity in
specific internal layers of a decoder-only multilingual large language model
(LLM) can improve translation quality from LRL to high-resource language (HRL).
Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric
that encourages representations of different languages to align, with REPINA, a
regularization method that constrains parameter updates to remain close to the
pretrained model, into a joint method we call TRepLiNa. In this research
project, we experiment with zero-shot, few-shot, and fine-tuning settings using
Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari,
Santali, Bhili) with Hindi/English pivots. Our results show that aligning
mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach
to improving LRL translation, especially in data-scarce settings.

</details>


### [277] [Scalable multilingual PII annotation for responsible AI in LLMs](https://arxiv.org/abs/2510.06250)
*Bharti Meena,Joanna Skubisz,Harshit Rajgarhia,Nand Dave,Kiran Ganesh,Shivali Dalmia,Abhishek Mukherji,Vasudevan Sundarababu,Olga Pospelova*

Main category: cs.CL

TL;DR: 本文介绍了一个可扩展的多语言数据整理框架，用于13个代表性不足地区的高质量PII注释，能提升注释质量和下游模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用，确保其在不同监管环境下可靠处理个人身份信息（PII）至关重要。

Method: 采用分阶段、人工参与的注释方法，结合语言专业知识和严格质量保证，利用注释者间一致性指标和根因分析解决注释不一致问题。

Result: 在试点、训练和生产阶段，召回率和误报率有显著改善，得到适用于监督式大语言模型微调的高保真数据集。

Conclusion: 迭代、分析驱动的流程可提高注释质量和下游模型可靠性，同时指出多语言PII标注中注释者常见挑战。

Abstract: As Large Language Models (LLMs) gain wider adoption, ensuring their reliable
handling of Personally Identifiable Information (PII) across diverse regulatory
contexts has become essential. This work introduces a scalable multilingual
data curation framework designed for high-quality PII annotation across 13
underrepresented locales, covering approximately 336 locale-specific PII types.
Our phased, human-in-the-loop annotation methodology combines linguistic
expertise with rigorous quality assurance, leading to substantial improvements
in recall and false positive rates from pilot, training, and production phases.
By leveraging inter-annotator agreement metrics and root-cause analysis, the
framework systematically uncovers and resolves annotation inconsistencies,
resulting in high-fidelity datasets suitable for supervised LLM fine-tuning.
Beyond reporting empirical gains, we highlight common annotator challenges in
multilingual PII labeling and demonstrate how iterative, analytics-driven
pipelines can enhance both annotation quality and downstream model reliability.

</details>


### [278] [Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments](https://arxiv.org/abs/2510.06262)
*Aryan Kumar Singh,Janvi Singh*

Main category: cs.CL

TL;DR: 本文介绍了一个基于阿育吠陀原则的双语（英 - 印地语）体质评估问卷数据集，可用于多领域研究和应用。


<details>
  <summary>Details</summary>
Motivation: 提供标准化数据以支持计算智能、阿育吠陀研究和个性化健康分析等领域的研究。

Method: 依据AYUSH/CCRAS指南开发24项选择题的问卷，通过Google Forms收集数据并自动评分。

Result: 得到一个可将个人特征映射到特定体质分数的结构化数据集。

Conclusion: 该数据集能支持多种分析和建模，为基于体质的研究和智能健康应用开发提供参考。

Abstract: This dataset provides responses to a standardized, bilingual (English-Hindi)
Prakriti Assessment Questionnaire designed to evaluate the physical,
physiological, and psychological characteristics of individuals according to
classical Ayurvedic principles. The questionnaire consists of 24
multiple-choice items covering body features, appetite, sleep patterns, energy
levels, and temperament. It was developed following AYUSH/CCRAS guidelines to
ensure comprehensive and accurate data collection. All questions are mandatory
and neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha)
are hidden from participants. Data were collected via a Google Forms
deployment, enabling automated scoring of responses to map individual traits to
dosha-specific scores. The resulting dataset provides a structured platform for
research in computational intelligence, Ayurvedic studies, and personalized
health analytics, supporting analysis of trait distributions, correlations, and
predictive modeling. It can also serve as a reference for future Prakriti-based
studies and the development of intelligent health applications.

</details>


### [279] [Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians](https://arxiv.org/abs/2510.06263)
*Jiajun Wu,Swaleh Zaidi,Braden Teitge,Henry Leung,Jiayu Zhou,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 提出在嵌入式设备上运行的两阶段总结系统，实现离线临床总结并保护隐私，初步结果显示系统能在30秒内有效生成有用总结。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）中的非结构化临床数据过多，使急诊医生难以识别关键信息。

Method: 采用双设备架构，Jetson Nano - R检索相关患者记录部分，Jetson Nano - S生成结构化总结，两者通过轻量级套接字链接通信；检索阶段利用本地EHRs，分割长笔记并搜索相关部分；生成阶段使用本地小型语言模型；还对6个开源小型语言模型进行基准测试，采用大语言模型评估总结质量。

Result: 在MIMIC - IV和去识别的真实EHRs上的初步结果表明，全离线系统能在30秒内有效生成有用总结。

Conclusion: 所提出的两阶段总结系统能在嵌入式设备上有效运行，可实现离线临床总结、保护患者隐私并快速生成有用总结。

Abstract: Electronic health records (EHRs) contain extensive unstructured clinical data
that can overwhelm emergency physicians trying to identify critical
information. We present a two-stage summarization system that runs entirely on
embedded devices, enabling offline clinical summarization while preserving
patient privacy. In our approach, a dual-device architecture first retrieves
relevant patient record sections using the Jetson Nano-R (Retrieve), then
generates a structured summary on another Jetson Nano-S (Summarize),
communicating via a lightweight socket link. The summarization output is
two-fold: (1) a fixed-format list of critical findings, and (2) a
context-specific narrative focused on the clinician's query. The retrieval
stage uses locally stored EHRs, splits long notes into semantically coherent
sections, and searches for the most relevant sections per query. The generation
stage uses a locally hosted small language model (SLM) to produce the summary
from the retrieved text, operating within the constraints of two NVIDIA Jetson
devices. We first benchmarked six open-source SLMs under 7B parameters to
identify viable models. We incorporated an LLM-as-Judge evaluation mechanism to
assess summary quality in terms of factual accuracy, completeness, and clarity.
Preliminary results on MIMIC-IV and de-identified real EHRs demonstrate that
our fully offline system can effectively produce useful summaries in under 30
seconds.

</details>


### [280] [Language models for longitudinal analysis of abusive content in Billboard Music Charts](https://arxiv.org/abs/2510.06266)
*Rohitash Chandra,Yathin Suresh,Divyansh Raj Sinha,Sanchit Jindal*

Main category: cs.CL

TL;DR: 研究利用深度学习分析美国公告牌音乐榜近七十年歌曲歌词，发现自1990年起流行音乐中露骨内容显著增加。


<details>
  <summary>Details</summary>
Motivation: 流行音乐中滥用和色情露骨内容增多，缺乏相关研究以制定有效政策，且此类内容会对儿童和青少年行为产生有害影响。

Method: 利用深度学习和语言模型进行纵向研究，使用情感分析和滥用检测审查内容演变。

Result: 自1990年起流行音乐中露骨内容显著增加，含亵渎、色情和不当语言的歌曲增多，语言模型能捕捉歌词内容细微模式变化。

Conclusion: 语言模型的纵向分析反映了社会规范和语言使用随时间的变化。

Abstract: There is no doubt that there has been a drastic increase in abusive and
sexually explicit content in music, particularly in Billboard Music Charts.
However, there is a lack of studies that validate the trend for effective
policy development, as such content has harmful behavioural changes in children
and youths. In this study, we utilise deep learning methods to analyse songs
(lyrics) from Billboard Charts of the United States in the last seven decades.
We provide a longitudinal study using deep learning and language models and
review the evolution of content using sentiment analysis and abuse detection,
including sexually explicit content. Our results show a significant rise in
explicit content in popular music from 1990 onwards. Furthermore, we find an
increasing prevalence of songs with lyrics containing profane, sexually
explicit, and otherwise inappropriate language. The longitudinal analysis of
the ability of language models to capture nuanced patterns in lyrical content,
reflecting shifts in societal norms and language use over time.

</details>


### [281] [Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"](https://arxiv.org/abs/2510.06275)
*Ranjan Mishra,Julian I. Bibo,Quinten van Engelen,Henk Schaapman*

Main category: cs.CL

TL;DR: 本文复现Ma等人（2024）论文工作，用Llama 3替代GPT - 3.5 - turbo评估XRec，对其进行扩展分析，指出XRec能生成个性化解释且稳定性提升，但不总能超越基线模型，还提供开源评估实现。


<details>
  <summary>Details</summary>
Motivation: 复现Ma等人（2024）论文结果，并用Llama 3进行评估，扩展原论文研究。

Method: 基于Ma等人（2024）提供的源代码，修改XRec的Mixture of Experts模块的输入嵌入或删除输出嵌入。

Result: XRec能有效生成个性化解释，稳定性因纳入协作信息而提高，但并非在所有指标上都超越基线模型。

Conclusion: 强调Mixture of Experts嵌入对解释结构的重要性，展示协作信号与语言建模的相互作用，提供开源评估实现方便研究人员和从业者。

Abstract: In this study, we reproduced the work done in the paper "XRec: Large Language
Models for Explainable Recommendation" by Ma et al. (2024). The original
authors introduced XRec, a model-agnostic collaborative instruction-tuning
framework that enables large language models (LLMs) to provide users with
comprehensive explanations of generated recommendations. Our objective was to
replicate the results of the original paper, albeit using Llama 3 as the LLM
for evaluation instead of GPT-3.5-turbo. We built on the source code provided
by Ma et al. (2024) to achieve our goal. Our work extends the original paper by
modifying the input embeddings or deleting the output embeddings of XRec's
Mixture of Experts module. Based on our results, XRec effectively generates
personalized explanations and its stability is improved by incorporating
collaborative information. However, XRec did not consistently outperform all
baseline models in every metric. Our extended analysis further highlights the
importance of the Mixture of Experts embeddings in shaping the explanation
structures, showcasing how collaborative signals interact with language
modeling. Through our work, we provide an open-source evaluation implementation
that enhances accessibility for researchers and practitioners alike. Our
complete code repository can be found at
https://github.com/julianbibo/xrec-reproducibility.

</details>


### [282] [EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA](https://arxiv.org/abs/2510.06371)
*Firoj Alam,Ali Ezzat Shahroor,Md. Arid Hasan,Zien Sheikh Ali,Hunzalah Hassan Bhatti,Mohamed Bayan Kmainasi,Shammur Absar Chowdhury,Basel Mousi,Fahim Dalvi,Nadir Durrani,Natasa Milic-Frayling*

Main category: cs.CL

TL;DR: 提出EverydayMMQA框架创建SVQA数据集OASIS，含多模态数据，用于测试模型，框架和数据集将公开。


<details>
  <summary>Details</summary>
Motivation: 解决大规模多模态模型在需要文化背景和日常知识的查询任务，尤其是低资源和代表性不足语言中的不足。

Method: 引入EverydayMMQA框架，用该框架开发OASIS数据集，对多个模型进行基准测试。

Result: 开发出OASIS数据集，含大量图像和问答对，可进行四种输入组合，能测试多种推理能力。

Conclusion: EverydayMMQA和OASIS为构建文化背景下日常任务的多模态大语言模型提供基准和训练数据集。

Abstract: Large-scale multimodal models achieve strong results on tasks like Visual
Question Answering (VQA), but they often fail when queries require culturally
grounded, everyday knowledge, particularly in low-resource and underrepresented
languages. To bridge this gap, we introduce Everyday Multimodal and
Multilingual QA (EverydayMMQA), a framework for creating large-scale,
culturally-grounded datasets for spoken and visual question answering (SVQA).
Using this framework, we developed OASIS, a multimodal dataset integrating
speech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIS
contains 3.7M spoken questions, enabling four unique input combinations:
speech-only, text-only, speech+image, and text+image. Focused on English and
Arabic varieties, 18 countries, the dataset content is curated to reflect
diverse, real-world situations. OASIS tests models on tasks beyond object
recognition that involve pragmatic, commonsense, and culturally aware
reasoning. We benchmarked four closed-source models, three open-source models,
and one fine-tuned model. EverydayMMQA and OASIS together provide a benchmark
and training dataset for building multimodal LLMs for a comprehensive set of
everyday tasks within cultural contexts. The framework and dataset will be made
publicly available to the community.

</details>


### [283] [Protecting De-identified Documents from Search-based Linkage Attacks](https://arxiv.org/abs/2510.06383)
*Pierre Lison,Mark Anderson*

Main category: cs.CL

TL;DR: 本文提出一种方法，在保留文本语义完整性的同时对抗基于搜索的链接攻击，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有去标识模型无法解决链接风险，即去标识文本可能被映射回其来源。

Method: 分两步进行，先构建文档集合中N - 元语法的倒排索引，确定出现次数少于k的N - 元语法；再迭代查询基于大语言模型的重写器改写这些片段直至无法进行链接。

Result: 在法院案例集合上的实验表明，该方法能有效防止基于搜索的链接，同时忠实于原文内容。

Conclusion: 所提方法能在保留文本语义完整性的情况下，有效对抗基于搜索的链接攻击。

Abstract: While de-identification models can help conceal the identity of the
individual(s) mentioned in a document, they fail to address linkage risks,
defined as the potential to map the de-identified text back to its source. One
straightforward way to perform such linkages is to extract phrases from the
de-identified document and then check their presence in the original dataset.
This paper presents a method to counter search-based linkage attacks while
preserving the semantic integrity of the text. The method proceeds in two
steps. We first construct an inverted index of the N-grams occurring in the
document collection, making it possible to efficiently determine which N-grams
appear in less than $k$ documents (either alone or in combination with other
N-grams). An LLM-based rewriter is then iteratively queried to reformulate
those spans until linkage is no longer possible. Experimental results on a
collection of court cases show that the method is able to effectively prevent
search-based linkages while remaining faithful to the original content.

</details>


### [284] [Reward Model Perspectives: Whose Opinions Do Reward Models Reward?](https://arxiv.org/abs/2510.06391)
*Elle*

Main category: cs.CL

TL;DR: 研究奖励模型（RMs）行为，发现其与多个人口群体对齐不佳，会奖励有害刻板印象，强调偏好学习时需更谨慎考虑RM行为以防止社会偏见传播。


<details>
  <summary>Details</summary>
Motivation: 当前对奖励模型（RMs）行为理解有限，需深入研究其行为。

Method: 形式化测量RM意见对齐的框架，研究RM的社会人口统计学偏差，探索引导奖励至目标群体偏好的效果，研究对争议话题的主观多样观点。

Result: RMs与多个群体对齐不佳，会系统地奖励有害刻板印象，仅靠引导不足以克服这些局限。

Conclusion: 在偏好学习的模型对齐中需更谨慎考虑RM行为，防止语言技术中不良社会偏见的传播。

Abstract: Reward models (RMs) are central to the alignment of language models (LMs). An
RM often serves as a proxy for human preferences to guide downstream LM
behavior. However, our understanding of RM behavior is limited. Our work (i)
formalizes a framework for measuring the alignment of opinions captured by RMs,
(ii) investigates the extent to which RMs demonstrate sociodemographic biases,
and (iii) explores the effects of prompting to steer rewards towards the
preferences of a target group. We study the subjective and diverse perspectives
on controversial topics, which allows us to quantify RM perspectives in terms
of their opinions, attitudes, and values. We show that RMs are poorly aligned
with several demographic groups and can systematically reward harmful
stereotypes, and steering alone is not enough to overcome these limitations.
Our findings underscore the need for more careful consideration of RM behavior
in model alignment during preference learning to prevent the propagation of
unwanted social biases in the language technologies that we use.

</details>


### [285] [A Survey on Agentic Security: Applications, Threats and Defenses](https://arxiv.org/abs/2510.06445)
*Asif Shahriar,Md Nafiu Rahman,Sadif Ahmed,Farig Sadeque,Md Rizwan Parvez*

Main category: cs.CL

TL;DR: 本文对大语言模型代理的安全领域进行了全面调查，涵盖应用、威胁和防御三方面，分析新兴趋势并指出研究差距。


<details>
  <summary>Details</summary>
Motivation: 大语言模型从被动向自主代理转变带来新的安全风险，需对代理安全领域进行全面调查。

Method: 围绕应用、威胁和防御三个相互依存的支柱构建领域结构，对超150篇论文进行全面分类。

Result: 展示了代理架构的新兴趋势，揭示了模型和模态覆盖方面的关键研究差距。

Conclusion: 完成了对大语言模型代理安全领域的首次全面调查，对该领域研究有重要参考价值。

Abstract: The rapid shift from passive LLMs to autonomous LLM-agents marks a new
paradigm in cybersecurity. While these agents can act as powerful tools for
both offensive and defensive operations, the very agentic context introduces a
new class of inherent security risks. In this work we present the first
holistic survey of the agentic security landscape, structuring the field around
three interdependent pillars: Applications, Threats, and Defenses. We provide a
comprehensive taxonomy of over 150 papers, explaining how agents are used, the
vulnerabilities they possess, and the countermeasures designed to protect them.
A detailed cross-cutting analysis shows emerging trends in agent architecture
while revealing critical research gaps in model and modality coverage.

</details>


### [286] [Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels](https://arxiv.org/abs/2510.06499)
*Zhepeng Cen,Haolin Chen,Shiyu Wang,Zuxin Liu,Zhiwei Liu,Ding Zhao,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 提出Webscale - RL管道构建大规模RL数据集，实验表明用该数据集训练的模型表现佳且训练高效，为将RL扩展到预训练规模提供途径。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型模仿学习存在训练-生成差距，强化学习虽能解决但受限于数据瓶颈，现有RL数据集规模小且多样性不足。

Method: 引入Webscale - RL管道，将大规模预训练文档系统地转换为数百万个不同、可验证的问答对，构建Webscale - RL数据集。

Result: 在该数据集上训练的模型在一系列基准测试中显著优于持续预训练和强数据细化基线，训练效率更高，用少至100倍的令牌即可达到持续预训练的性能。

Conclusion: 为将强化学习扩展到预训练水平提供了可行途径，可实现更强大、高效的语言模型。

Abstract: Large Language Models (LLMs) have achieved remarkable success through
imitation learning on vast text corpora, but this paradigm creates a
training-generation gap and limits robust reasoning. Reinforcement learning
(RL) offers a more data-efficient solution capable of bridging this gap, yet
its application has been constrained by a critical data bottleneck: existing RL
datasets are orders of magnitude smaller and less diverse than web-scale
pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a
scalable data engine that systematically converts large-scale pre-training
documents into millions of diverse, verifiable question-answer pairs for RL.
Using this pipeline, we construct the Webscale-RL dataset, containing 1.2
million examples across more than 9 domains. Our experiments show that the
model trained on this dataset significantly outperforms continual pretraining
and strong data refinement baselines across a suite of benchmarks. Notably, RL
training with our dataset proves substantially more efficient, achieving the
performance of continual pre-training with up to 100$\times$ fewer tokens. Our
work presents a viable path toward scaling RL to pre-training levels, enabling
more capable and efficient language models.

</details>


### [287] [The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law](https://arxiv.org/abs/2510.06559)
*Cheonkam Jeong,Sungdo Kim,Jewoo Park*

Main category: cs.CL

TL;DR: 指出当代语言模型处理语义存在问题，提出将对齐视为解析问题，介绍Savassan架构，给出贡献并提出评估计划。


<details>
  <summary>Details</summary>
Motivation: 解决当代语言模型在处理输出语义时存在的幻觉、审核脆弱和合规结果不透明等问题。

Method: 基于Montague对语言的观点，提出Savassan神经符号架构，将话语编译为逻辑形式并映射到扩展的类型化本体。

Result: 系统可在跨境场景中进行一次解析并投影到多法律本体，生成可解释决策。

Conclusion: 值得信赖的自主性需要对意义进行组合式类型化，以便系统在统一的意义代数中进行推理。

Abstract: Contemporary language models are fluent yet routinely mis-handle the types of
meaning their outputs entail. We argue that hallucination, brittle moderation,
and opaque compliance outcomes are symptoms of missing type-theoretic semantics
rather than data or scale limitations. Building on Montague's view of language
as typed, compositional algebra, we recast alignment as a parsing problem:
natural-language inputs must be compiled into structures that make explicit
their descriptive, normative, and legal dimensions under context.
  We present Savassan, a neuro-symbolic architecture that compiles utterances
into Montague-style logical forms and maps them to typed ontologies extended
with deontic operators and jurisdictional contexts. Neural components extract
candidate structures from unstructured inputs; symbolic components perform type
checking, constraint reasoning, and cross-jurisdiction mapping to produce
compliance-aware guidance rather than binary censorship. In cross-border
scenarios, the system "parses once" (e.g., defect claim(product x, company y))
and projects the result into multiple legal ontologies (e.g., defamation risk
in KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into
a single, explainable decision.
  This paper contributes: (i) a diagnosis of hallucination as a type error;
(ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii)
a production-oriented design that embeds typed interfaces across the pipeline.
We outline an evaluation plan using legal reasoning benchmarks and synthetic
multi-jurisdiction suites. Our position is that trustworthy autonomy requires
compositional typing of meaning, enabling systems to reason about what is
described, what is prescribed, and what incurs liability within a unified
algebra of meaning.

</details>


### [288] [Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback](https://arxiv.org/abs/2510.06677)
*Yisha Wu,Cen,Zhao,Yuanpei Cao,Xiaoqing Su,Yashar Mehdad,Mindy Ji,Claire Na Cheng*

Main category: cs.CL

TL;DR: 介绍用于客服的增量式摘要系统，结合模型与分类器，经代理编辑反馈，生产部署后减少处理时间，提升摘要质量和代理生产力。


<details>
  <summary>Details</summary>
Motivation: 减少客服人员上下文切换的精力和冗余审查。

Method: 结合微调的Mixtral - 8x7B模型进行连续笔记生成，用基于DeBERTa的分类器过滤琐碎内容，通过代理编辑优化在线笔记生成并反馈到离线模型再训练。

Result: 系统在生产部署中比批量摘要减少3%的案例处理时间，复杂案例最多减少9%，且代理满意度高。

Conclusion: 带有连续反馈的增量式摘要能有效提升摘要质量和代理生产力。

Abstract: We introduce an incremental summarization system for customer support agents
that intelligently determines when to generate concise bullet notes during
conversations, reducing agents' context-switching effort and redundant review.
Our approach combines a fine-tuned Mixtral-8x7B model for continuous note
generation with a DeBERTa-based classifier to filter trivial content. Agent
edits refine the online notes generation and regularly inform offline model
retraining, closing the agent edits feedback loop. Deployed in production, our
system achieved a 3% reduction in case handling time compared to bulk
summarization (with reductions of up to 9% in highly complex cases), alongside
high agent satisfaction ratings from surveys. These results demonstrate that
incremental summarization with continuous feedback effectively enhances summary
quality and agent productivity at scale.

</details>


### [289] [Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks](https://arxiv.org/abs/2510.06695)
*Qinhao Zhou,Xiang Xiang,Kun He,John E. Hopcroft*

Main category: cs.CL

TL;DR: 论文提出针对机器翻译任务的新型提示优化方法，用小参数模型降低训练开销且效果好，还可扩展到其他下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程方法主要优化通用任务的指令部分，对机器翻译等输入部分更关键的任务适用性有限。

Method: 采用基于反向翻译策略训练的小参数模型进行提示优化。

Result: 显著降低单任务优化的训练开销，同时取得高效性能。

Conclusion: 该方法可用于机器翻译任务，经调整还能扩展到其他下游任务。

Abstract: In recent years, the growing interest in Large Language Models (LLMs) has
significantly advanced prompt engineering, transitioning from manual design to
model-based optimization. Prompts for LLMs generally comprise two components:
the \textit{instruction}, which defines the task or objective, and the
\textit{input}, which is tailored to the instruction type. In natural language
generation (NLG) tasks such as machine translation, the \textit{input}
component is particularly critical, while the \textit{instruction} component
tends to be concise. Existing prompt engineering methods primarily focus on
optimizing the \textit{instruction} component for general tasks, often
requiring large-parameter LLMs as auxiliary tools. However, these approaches
exhibit limited applicability for tasks like machine translation, where the
\textit{input} component plays a more pivotal role. To address this limitation,
this paper introduces a novel prompt optimization method specifically designed
for machine translation tasks. The proposed approach employs a small-parameter
model trained using a back-translation-based strategy, significantly reducing
training overhead for single-task optimization while delivering highly
effective performance. With certain adaptations, this method can also be
extended to other downstream tasks.

</details>


### [290] [Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management](https://arxiv.org/abs/2510.06727)
*Miao Lu,Weiwei Sun,Weihua Du,Zhan Ling,Xuesong Yao,Kang Liu,Jiecao Chen*

Main category: cs.CL

TL;DR: 研究长时多轮工具使用的大语言模型强化学习微调，提出基于摘要的上下文管理方法SUPO，实验证明其能提升成功率并管理上下文长度。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习管道在长时多轮工具使用中存在指令跟随能力下降、滚动成本过高及严格上下文限制等问题。

Method: 引入基于摘要的上下文管理，将工具使用历史压缩为保留任务相关信息的摘要；推导策略梯度表示，实现端到端优化；提出SUPO算法。

Result: 在交互式函数调用和搜索任务中，SUPO显著提高成功率，保持或降低上下文长度；对复杂搜索任务，扩展测试时摘要最大轮数可进一步提升性能。

Conclusion: 基于摘要的上下文管理是训练超越固定上下文长度限制的强化学习智能体的有效且可扩展的方法。

Abstract: We study reinforcement learning (RL) fine-tuning of large language model
(LLM) agents for long-horizon multi-turn tool use, where context length quickly
becomes a fundamental bottleneck. Existing RL pipelines can suffer from
degraded instruction following, excessive rollout costs, and most importantly,
strict context limits. To address these challenges, we introduce
summarization-based context management to training. In specific, it
periodically compresses the tool using history by LLM-generated summaries that
retain task-relevant information to keep a compact context while enabling the
agent to scale beyond the fixed context window. Building on this formulation,
we derive a policy gradient representation that seamlessly enables standard LLM
RL infrastructures to optimize both tool-use behaviors as well as summarization
strategies in an end-to-end fashion. We instantiate this framework with
\underline{SU}mmarization augmented \underline{P}olicy \underline{O}ptimization
(\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond
a fixed context limit. Experiments on interactive function calling and
searching tasks demonstrate that \texttt{SUPO} significantly improves the
success rate while maintaining the same or even lower working context length
compared to baselines. We also demonstrate that for complex searching tasks,
\texttt{SUPO} can further improve the evaluation performance when scaling
test-time maximum round of summarization beyond that of training time. Our
results establish summarization-based context management as a principled and
scalable approach for training RL agents beyond a fixed context length limit.

</details>


### [291] [Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness](https://arxiv.org/abs/2510.06780)
*Luca Giordano,Simon Razniewski*

Main category: cs.CL

TL;DR: 本文研究大语言模型知识结构化，通过miniGPTKBs分析终止性、可重复性和鲁棒性，发现有高终止率、混合可重复性和不同的鲁棒性，表明该方法有一定可靠性但也有局限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型编码了大量事实知识，但测量和系统化这些知识具有挑战性，递归提取方法如GPTKB仍未充分探索，存在终止性、可重复性和鲁棒性等问题。

Method: 使用miniGPTKBs系统研究大语言模型知识结构化，分析终止性、可重复性和鲁棒性，设置四种变化和三个示例领域进行实验。

Result: 终止率高但依赖模型；可重复性不一；鲁棒性因扰动类型而异，种子和温度的鲁棒性高，语言和模型的鲁棒性低。

Conclusion: 大语言模型知识结构化能可靠地揭示核心知识，但也存在重要局限性。

Abstract: Large Language Models (LLMs) encode substantial factual knowledge, yet
measuring and systematizing this knowledge remains challenging. Converting it
into structured format, for example through recursive extraction approaches
such as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key
open questions include whether such extraction can terminate, whether its
outputs are reproducible, and how robust they are to variations. We
systematically study LLM knowledge materialization using miniGPTKBs
(domain-specific, tractable subcrawls), analyzing termination, reproducibility,
and robustness across three categories of metrics: yield, lexical similarity,
and semantic similarity. We experiment with four variations (seed, language,
randomness, model) and three illustrative domains (from history, entertainment,
and finance). Our findings show (i) high termination rates, though
model-dependent; (ii) mixed reproducibility; and (iii) robustness that varies
by perturbation type: high for seeds and temperature, lower for languages and
models. These results suggest that LLM knowledge materialization can reliably
surface core knowledge, while also revealing important limitations.

</details>


### [292] [FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline](https://arxiv.org/abs/2510.06800)
*Haotian Wu,Shufan Jiang,Chios Chen,Yiyang Feng,Hehai Lin,Heqing Zou,Yao Shu,Yanran Li,Chengwei Qin*

Main category: cs.CL

TL;DR: 现有角色扮演基准过时，本文提出FURINA - Builder构建可定制基准FURINA - Bench，评估大模型，发现o3和DeepSeek - R1分别在英文和中文任务表现最佳，还有模型相关新发现。


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演基准存在范围窄、交互范式过时、适应性有限等问题，需要新的基准构建方法。

Method: 提出FURINA - Builder多智能体协作管道，模拟对话，用大模型裁判选择评估维度并调整测试角色响应，构建FURINA - Bench基准。

Result: 对前沿大模型评估发现o3和DeepSeek - R1分别在英文和中文任务表现最佳，既定角色表现优于合成角色，模型规模与幻觉非单调关系，推理型大模型存在性能与幻觉的权衡。

Conclusion: FURINA - Builder有效，FURINA - Bench带来挑战。

Abstract: As large language models (LLMs) advance in role-playing (RP) tasks, existing
benchmarks quickly become obsolete due to their narrow scope, outdated
interaction paradigms, and limited adaptability across diverse application
scenarios. To address this gap, we introduce FURINA-Builder, a novel
multi-agent collaboration pipeline that automatically constructs fully
customizable RP benchmarks at any scale. It enables evaluation of arbitrary
characters across diverse scenarios and prompt formats, as the first benchmark
builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues
between a test character and other characters drawn from a well-constructed
character-scene pool, while an LLM judge selects fine-grained evaluation
dimensions and adjusts the test character's responses into final test
utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive
role-playing benchmark featuring both established and synthesized test
characters, each assessed with dimension-specific evaluation criteria. Human
evaluation and preliminary separability analysis justify our pipeline and
benchmark design. We conduct extensive evaluations of cutting-edge LLMs and
find that o3 and DeepSeek-R1 achieve the best performance on English and
Chinese RP tasks, respectively. Across all models, established characters
consistently outperform synthesized ones, with reasoning capabilities further
amplifying this disparity. Interestingly, we observe that model scale does not
monotonically reduce hallucinations. More critically, for reasoning LLMs, we
uncover a novel trade-off: reasoning improves RP performance but simultaneously
increases RP hallucinations. This trade-off extends to a broader Pareto
frontier between RP performance and reliability for all LLMs. These findings
demonstrate the effectiveness of FURINA-Builder and the challenge posed by
FURINA-Bench.

</details>


### [293] [SID: Multi-LLM Debate Driven by Self Signals](https://arxiv.org/abs/2510.06843)
*Xuhang Chen,Zhifan Song,Deyi Ji,Shuo Gao,Lanyun Zhu*

Main category: cs.CL

TL;DR: 本文提出Self - Signals Driven Multi - LLM Debate (SID)方法，利用自信号提升多LLM辩论系统的性能和效率，并在多基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有Multi - LLM Agent Debate (MAD)方法主要利用外部结构，忽略自信号，导致冗余计算和性能下降，因此需要关注自信号以改进方法。

Method: 提出SID方法，利用模型级置信度和令牌级语义焦点两种自信号，自适应地引导辩论过程，使高置信度代理提前退出，基于注意力机制压缩冗余辩论内容。

Result: 在多个具有挑战性的基准测试中，该方法在准确性上优于现有MAD技术，且能减少令牌消耗。

Conclusion: 利用自信号可以有效增强多智能体辩论系统的性能和效率。

Abstract: Large Language Models (LLMs) have exhibited impressive capabilities across
diverse application domains. Recent work has explored Multi-LLM Agent Debate
(MAD) as a way to enhance performance by enabling multiple LLMs to discuss and
refine responses iteratively. Nevertheless, existing MAD methods predominantly
focus on utilizing external structures, such as debate graphs, using
LLM-as-a-Judge, while neglecting the application of self signals, such as token
logits and attention, that arise during generation. This omission leads to
redundant computation and potential performance degradation. In this paper, we
shift the focus to the self signals of multi-LLM debate and introduce a
Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of
self-signals: model-level confidence and token-level semantic focus, to
adaptively guide the debate process. Our approach enables high-confidence
agents to exit early at the model level and compress the redundant debate
contents based on the attention mechanism. We evaluate our method on various
LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental
results demonstrate that our method not only outperforms existing MAD
techniques in accuracy but also reduces token consumption, highlighting the
effectiveness of utilizing self signals in enhancing both the performance and
efficiency of multi-agent debate systems. Our code will be available
at~\href{https://github.com/xuhang2019/SID}{\texttt{https://github.com/xuhang2019/SID}}.

</details>


### [294] [OpenJAI-v1.0: An Open Thai Large Language Model](https://arxiv.org/abs/2510.06847)
*Pontakorn Trakuekul,Attapol T. Rutherford,Jullajak Karnjanaekarin,Narongkorn Panitsrisit,Sumana Sumanakul*

Main category: cs.CL

TL;DR: 介绍基于Qwen3 - 14B开发的开源多语言模型OpenJAI - v1.0，经评估性能提升且避免灾难性遗忘，已公开。


<details>
  <summary>Details</summary>
Motivation: 为泰语AI社区提供新的NLP资源，提升模型在实际任务中的性能。

Method: 基于Qwen3 - 14B模型，通过精心策划三个关键用例的数据来开发。

Result: OpenJAI - v1.0提升了基础模型能力，在多种基准测试中优于其他领先的开源泰语模型，且避免了灾难性遗忘。

Conclusion: OpenJAI - v1.0作为新的NLP资源向泰语AI社区公开。

Abstract: We introduce OpenJAI-v1.0, an open-source large language model for Thai and
English, developed from the Qwen3-14B model. Our work focuses on boosting
performance on practical tasks through carefully curated data across three key
use cases: instruction following, long-context understanding, and tool use.
Evaluation results show that OpenJAI-v1.0 improves on the capabilities of its
base model and outperforms other leading open-source Thai models on a diverse
suite of benchmarks, while avoiding catastrophic forgetting. OpenJAI-v1.0 is
publicly released as another alternative NLP resource for the Thai AI
community.

</details>


### [295] [From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining](https://arxiv.org/abs/2510.06548)
*Seng Pei Liew,Takuya Kato*

Main category: cs.CL

TL;DR: 研究自举预训练的缩放行为，发现其效率随基础模型预训练令牌数有可预测的下降，揭示多阶段预训练权衡。


<details>
  <summary>Details</summary>
Motivation: 自举预训练能降低从头训练语言模型成本，但对过度训练的基础模型效果不明，需研究其缩放行为。

Method: 通过实证研究自举预训练的缩放行为。

Result: 自举预训练的缩放效率以可预测方式降低，缩放指数与基础模型预训练令牌数呈对数关系，可由简单缩放定律建模。

Conclusion: 研究为高效语言模型训练提供实用见解，对过度训练模型的复用提出重要考量。

Abstract: Bootstrapped pretraining, i.e., the reuse of a pretrained base model for
further pretraining, such as continual pretraining or model growth, is
promising at reducing the cost of training language models from scratch.
However, its effectiveness remains unclear, especially when applied to
overtrained base models. In this work, we empirically study the scaling
behavior of bootstrapped pretraining and find that its scaling efficiency
diminishes in a predictable manner: The scaling exponent with respect to
second-stage pretraining tokens decreases logarithmically with the number of
tokens used to pretrain the base model. The joint dependence on first- and
second-stage tokens is accurately modeled by a simple scaling law. Such
saturation effect reveals a fundamental trade-off in multi-stage pretraining
strategies: the more extensively a model is pretrained, the less additional
benefit bootstrapping provides. Our findings provide practical insights for
efficient language model training and raise important considerations for the
reuse of overtrained models.

</details>


### [296] [LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling](https://arxiv.org/abs/2510.06915)
*Zecheng Tang,Baibei Ji,Quantong Qiu,Haitian Wang,Xiaobo Liang,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 提出Long - RewardBench基准用于长上下文奖励模型评估，提出多阶段训练策略提升长上下文奖励模型性能，8B LongRM表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型局限于短上下文设置，忽略长上下文 - 响应一致性，需评估模型在长历史轨迹下表现。

Method: 引入Long - RewardBench基准，包含Pairwise Comparison和Best - of - N任务；提出多阶段训练策略将任意模型扩展为长上下文奖励模型。

Result: 实验表明该方法大幅提升长上下文评估性能，保留短上下文能力，8B LongRM优于70B基线模型，与Gemini 2.5 Pro性能相当。

Conclusion: 所提方法有效提升奖励模型在长上下文场景下性能。

Abstract: Reward model (RM) plays a pivotal role in aligning large language model (LLM)
with human preferences. As real-world applications increasingly involve long
history trajectories, e.g., LLM agent, it becomes indispensable to evaluate
whether a model's responses are not only high-quality but also grounded in and
consistent with the provided context. Yet, current RMs remain confined to
short-context settings and primarily focus on response-level attributes (e.g.,
safety or helpfulness), while largely neglecting the critical dimension of long
context-response consistency. In this work, we introduce Long-RewardBench, a
benchmark specifically designed for long-context RM evaluation, featuring both
Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that
even state-of-the-art generative RMs exhibit significant fragility in
long-context scenarios, failing to maintain context-aware preference judgments.
Motivated by the analysis of failure patterns observed in model outputs, we
propose a general multi-stage training strategy that effectively scales
arbitrary models into robust Long-context RMs (LongRMs). Experiments show that
our approach not only substantially improves performance on long-context
evaluation but also preserves strong short-context capability. Notably, our 8B
LongRM outperforms much larger 70B-scale baselines and matches the performance
of the proprietary Gemini 2.5 Pro model.

</details>


### [297] [A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures](https://arxiv.org/abs/2510.06640)
*Nhat M. Hoang,Do Xuan Long,Cong-Duy Nguyen,Min-Yen Kan,Luu Anh Tuan*

Main category: cs.CL

TL;DR: 分析SSMs和TBMs中表征传播，发现TBMs和SSMs在表征同质化上的差异及过平滑原因，为长上下文推理模型设计提供信息。


<details>
  <summary>Details</summary>
Motivation: 当前对SSMs和TBMs架构中上下文信息在层和标记间的流动研究不足，需进行统一分析。

Method: 使用中心核对齐、稳定性指标和探测方法，还进行理论分析和参数随机化。

Result: TBMs快速同质化标记表征，后期重新出现多样性；SSMs早期保留标记唯一性，深层趋于同质化；TBMs过平滑源于架构设计，SSMs主要源于训练动态。

Conclusion: 研究结果明确了两种架构的归纳偏置，为未来模型和训练设计提供参考。

Abstract: State Space Models (SSMs) have recently emerged as efficient alternatives to
Transformer-Based Models (TBMs) for long-sequence processing, offering linear
scaling and lower memory use. Yet, how contextual information flows across
layers and tokens in these architectures remains understudied. We present the
first unified, token- and layer-level analysis of representation propagation in
SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing,
we characterize how representations evolve within and across layers. We find a
key divergence: TBMs rapidly homogenize token representations, with diversity
reemerging only in later layers, while SSMs preserve token uniqueness early but
converge to homogenization deeper. Theoretical analysis and parameter
randomization further reveal that oversmoothing in TBMs stems from
architectural design, whereas in SSMs it arises mainly from training dynamics.
These insights clarify the inductive biases of both architectures and inform
future model and training designs for long-context reasoning.

</details>


### [298] [Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation](https://arxiv.org/abs/2510.06961)
*Vaibhav Srivastav,Steven Zheng,Eric Bezzam,Eustache Le Bihan,Nithin Koluguri,Piotr Żelasko,Somshubra Majumdar,Adel Moumen,Sanchit Gandhi*

Main category: cs.CL

TL;DR: 提出Open ASR Leaderboard基准和排行榜，对60多个系统在11个数据集上评估，比较准确率和效率，公开代码支持透明评估。


<details>
  <summary>Details</summary>
Motivation: 当前ASR评估多集中于英语短文本且很少报告效率，需要改进评估方式。

Method: 建立Open ASR Leaderboard，标准化文本归一化，报告WER和RTFx。

Result: 英语转录中Conformer编码器搭配LLM解码器平均WER最佳但速度慢，CTC和TDT解码器RTFx更好；微调的Whisper编码器提高英语准确率但牺牲多语言覆盖。

Conclusion: 公开代码和数据集加载器，支持透明、可扩展的评估。

Abstract: Despite rapid progress, ASR evaluation remains saturated with short-form
English, and efficiency is rarely reported. We present the Open ASR
Leaderboard, a fully reproducible benchmark and interactive leaderboard
comparing 60+ open-source and proprietary systems across 11 datasets, including
dedicated multilingual and long-form tracks. We standardize text normalization
and report both word error rate (WER) and inverse real-time factor (RTFx),
enabling fair accuracy-efficiency comparisons. For English transcription,
Conformer encoders paired with LLM decoders achieve the best average WER but
are slower, while CTC and TDT decoders deliver much better RTFx, making them
attractive for long-form and offline use. Whisper-derived encoders fine-tuned
for English improve accuracy but often trade off multilingual coverage. All
code and dataset loaders are open-sourced to support transparent, extensible
evaluation.

</details>


### [299] [EDUMATH: Generating Standards-aligned Educational Math Word Problems](https://arxiv.org/abs/2510.06965)
*Bryan R. Christ,Penelope Molitz,Jonathan Kropko,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 本文提出用大语言模型（LLMs）生成定制化数学应用题（MWPs），评估大量MWPs，开发数据集训练模型，模型表现良好，学生对定制MWPs评价较好。


<details>
  <summary>Details</summary>
Motivation: 数学应用题是K - 12教育重要工具，定制化可提高学习效果，但教师难以为每个学生定制，因此提出用LLMs支持数学教育。

Method: 采用人类专家 - LLM联合评判方法评估超11000个MWPs，开发教师标注数据集，用数据集训练模型和文本分类器。

Result: 训练的12B开放模型性能与更大更强大的开放模型相当，30B开放LLM无需训练超现有封闭基线，模型生成的MWPs更接近人类编写的。

Conclusion: 对小学生的研究发现，学生在模型生成和人类编写的MWPs上表现相近，但更偏爱定制的MWPs。

Abstract: Math word problems (MWPs) are critical K-12 educational tools, and
customizing them to students' interests and ability levels can increase
learning outcomes. However, teachers struggle to find time to customize MWPs
for each student given large class sizes and increasing burnout. We propose
that LLMs can support math education by generating MWPs customized to student
interests and math education standards. To this end, we use a joint human
expert-LLM judge approach to evaluate over 11,000 MWPs generated by open and
closed LLMs and develop the first teacher-annotated dataset for
standards-aligned educational MWP generation. We show the value of our data by
using it to train a 12B open model that matches the performance of larger and
more capable open models. We also use our teacher-annotated data to train a
text classifier that enables a 30B open LLM to outperform existing closed
baselines without any training. Next, we show our models' MWPs are more similar
to human-written MWPs than those from existing models. We conclude by
conducting the first study of customized LLM-generated MWPs with grade school
students, finding they perform similarly on our models' MWPs relative to
human-written MWPs but consistently prefer our customized MWPs.

</details>


### [300] [Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages](https://arxiv.org/abs/2510.07000)
*Neel Prabhanjan Rachamalla,Aravind Konakalla,Gautam Rajeev,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.CL

TL;DR: 现有开源数据集在多语言覆盖等方面存在不足，引入人工参与的管道生成印度语后训练数据并创建两个数据集，为多语言大语言模型提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有开源数据集缺乏多语言覆盖、文化基础和任务多样性，特别是针对印度语言。

Method: 引入结合翻译和合成扩展的人工参与管道，利用57个不同数据集。

Result: 策划了两个数据集Pragyaan - IT（22.5K）和Pragyaan - Align（100K），涵盖10种印度语言、13个大类和56个子类别。

Conclusion: 数据集协议考虑多个维度，强调任务多样性等，为更具包容性和有效性的多语言大语言模型提供基础。

Abstract: The effectiveness of Large Language Models (LLMs) depends heavily on the
availability of high-quality post-training data, particularly
instruction-tuning and preference-based examples. Existing open-source
datasets, however, often lack multilingual coverage, cultural grounding, and
suffer from task diversity gaps that are especially pronounced for Indian
languages. We introduce a human-in-the-loop pipeline that combines translations
with synthetic expansion to produce reliable and diverse Indic post-training
data. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and
Pragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56
sub-categories, leveraging 57 diverse datasets. Our dataset protocol
incorporates several often-overlooked dimensions and emphasize task diversity,
multi-turn dialogue, instruction fidelity, safety alignment, and preservation
of cultural nuance, providing a foundation for more inclusive and effective
multilingual LLMs.

</details>


### [301] [Native Hybrid Attention for Efficient Sequence Modeling](https://arxiv.org/abs/2510.07019)
*Jusen Du,Jiaxi Hu,Tao Zhang,Weigao Sun,Yu Cheng*

Main category: cs.CL

TL;DR: 提出Native Hybrid Attention (NHA)，结合线性和全注意力，实验显示其在任务上超越Transformer和其他混合基线，还能与预训练大模型结合实现效率提升。


<details>
  <summary>Details</summary>
Motivation: Transformer存在二次复杂度问题，线性注意力在长上下文召回准确性上有妥协，需新架构兼顾效率与召回准确性。

Method: 引入NHA，将层内和层间混合集成到统一层设计，用线性RNN更新键值槽维护长期上下文，用滑动窗口添加短期令牌，通过单一softmax attention操作加权，用滑动窗口大小控制层间行为。

Result: NHA在召回密集和常识推理任务上超越Transformer和其他混合基线，与预训练大模型结构混合可实现有竞争力的准确性和显著效率提升。

Conclusion: NHA是一种有效结合线性和全注意力的架构，能在保证准确性的同时提高效率。

Abstract: Transformers excel at sequence modeling but face quadratic complexity, while
linear attention offers improved efficiency but often compromises recall
accuracy over long contexts. In this work, we introduce Native Hybrid Attention
(NHA), a novel hybrid architecture of linear and full attention that integrates
both intra \& inter-layer hybridization into a unified layer design. NHA
maintains long-term context in key-value slots updated by a linear RNN, and
augments them with short-term tokens from a sliding window. A single
\texttt{softmax attention} operation is then applied over all keys and values,
enabling per-token and per-head context-dependent weighting without requiring
additional fusion parameters. The inter-layer behavior is controlled through a
single hyperparameter, the sliding window size, which allows smooth adjustment
between purely linear and full attention while keeping all layers structurally
uniform. Experimental results show that NHA surpasses Transformers and other
hybrid baselines on recall-intensive and commonsense reasoning tasks.
Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving
competitive accuracy while delivering significant efficiency gains. Code is
available at https://github.com/JusenD/NHA.

</details>


### [302] [Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge](https://arxiv.org/abs/2510.07024)
*Shrestha Ghosh,Luca Giordano,Yujia Hu,Tuan-Phong Nguyen,Simon Razniewski*

Main category: cs.CL

TL;DR: 本文基于GPTKB v1.5深入研究前沿大语言模型的事实知识，发现其与现有知识库差异大、准确率低，且存在不一致、模糊和幻觉等问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的事实知识目前理解不足且分析样本有偏差，需要深入研究。

Method: 基于GPTKB v1.5，对前沿大语言模型GPT - 4.1的1亿条信念进行递归提取和分析。

Result: 模型的事实知识与既定知识库有显著差异，准确率低于先前基准，且存在不一致、模糊和幻觉等问题。

Conclusion: 指出了大语言模型事实知识存在的问题，为未来相关研究提供了方向。

Abstract: LLMs are remarkable artifacts that have revolutionized a range of NLP and AI
tasks. A significant contributor is their factual knowledge, which, to date,
remains poorly understood, and is usually analyzed from biased samples. In this
paper, we take a deep tour into the factual knowledge (or beliefs) of a
frontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited
set of 100 million beliefs of one of the strongest currently available frontier
LLMs, GPT-4.1. We find that the models' factual knowledge differs quite
significantly from established knowledge bases, and that its accuracy is
significantly lower than indicated by previous benchmarks. We also find that
inconsistency, ambiguity and hallucinations are major issues, shedding light on
future research opportunities concerning factual LLM knowledge.

</details>


### [303] [BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods](https://arxiv.org/abs/2510.06811)
*Philipp Mondorf,Mingyang Wang,Sebastian Gerstner,Ahmad Dawar Hakimi,Yihong Liu,Leonor Veloso,Shijia Zhou,Hinrich Schütze,Barbara Plank*

Main category: cs.CL

TL;DR: 研究集成多个电路定位方法是否能提升大语言模型电路定位性能，对比并行和顺序集成两种方式，发现并行集成效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探究集成多个电路定位方法能否提高在大语言模型中定位电路的性能。

Method: 探索并行和顺序两种集成方式，并行集成结合不同方法分配给每条边的归因分数，顺序集成用EAP - IG获得的边归因分数作为更精确电路识别方法的预热。

Result: 两种集成方式在基准指标上都有显著提升，并行集成多种方法（包括顺序集成）效果最佳。

Conclusion: 集成多个电路定位方法可提升电路定位性能，并行集成表现最优。

Abstract: The Circuit Localization track of the Mechanistic Interpretability Benchmark
(MIB) evaluates methods for localizing circuits within large language models
(LLMs), i.e., subnetworks responsible for specific task behaviors. In this
work, we investigate whether ensembling two or more circuit localization
methods can improve performance. We explore two variants: parallel and
sequential ensembling. In parallel ensembling, we combine attribution scores
assigned to each edge by different methods-e.g., by averaging or taking the
minimum or maximum value. In the sequential ensemble, we use edge attribution
scores obtained via EAP-IG as a warm start for a more expensive but more
precise circuit identification method, namely edge pruning. We observe that
both approaches yield notable gains on the benchmark metrics, leading to a more
precise circuit identification approach. Finally, we find that taking a
parallel ensemble over various methods, including the sequential ensemble,
achieves the best results. We evaluate our approach in the BlackboxNLP 2025 MIB
Shared Task, comparing ensemble scores to official baselines across multiple
model-task combinations.

</details>


### [304] [Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models](https://arxiv.org/abs/2510.07048)
*Yuntao Gui,James Cheng*

Main category: cs.CL

TL;DR: 提出Search - R3框架，利用大语言模型思维链能力生成搜索嵌入，经评估显著优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在检索任务方面未得到充分利用，需要解决此局限。

Method: 通过三种互补机制实现：监督学习阶段使模型能生成高质量嵌入；强化学习方法优化嵌入生成与推理；专门的强化学习环境高效处理嵌入表示。

Result: 在不同基准测试中，Search - R3通过统一推理和嵌入生成过程，显著优于先前方法。

Conclusion: 这种集成的训练后方法在处理复杂知识密集型任务方面取得了重大进展。

Abstract: Despite their remarkable natural language understanding capabilities, Large
Language Models (LLMs) have been underutilized for retrieval tasks. We present
Search-R3, a novel framework that addresses this limitation by adapting LLMs to
generate search embeddings as a direct output of their reasoning process. Our
approach exploits LLMs' chain-of-thought capabilities, allowing them to produce
more effective embeddings by reasoning step-by-step through complex semantic
analyses. We implement this through three complementary mechanisms. (1) a
supervised learning stage enables the model's ability to produce quality
embeddings, (2) a reinforcement learning (RL) methodology that optimizes
embedding generation alongside reasoning, and (3) a specialized RL environment
that efficiently handles evolving embedding representations without requiring
complete corpus re-encoding at each training iteration. Our extensive
evaluations on diverse benchmarks demonstrate that Search-R3 significantly
outperforms prior methods by unifying the reasoning and embedding generation
processes. This integrated post-training approach represents a substantial
advancement in handling complex knowledge-intensive tasks that require both
sophisticated reasoning and effective information retrieval. Project page:
https://github.com/ytgui/Search-R3

</details>


### [305] [LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish](https://arxiv.org/abs/2510.07074)
*Fred Philippy,Laura Bernardy,Siwen Guo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: 为解决卢森堡语缺乏高质量指令数据集问题，创建跨语言指令调优数据集，避免机器翻译弊端，提升模型能力。


<details>
  <summary>Details</summary>
Motivation: 低资源语言如卢森堡语因缺乏高质量指令数据集，传统机器翻译有语义和文化问题，需解决这些挑战。

Method: 不使用机器生成翻译，利用英语、法语和德语的对齐数据构建高质量数据集。

Result: 跨语言指令调优不仅改善了语言间的表征对齐，还提升了模型在卢森堡语中的生成能力。

Conclusion: 跨语言数据管理可避免机器翻译数据的常见问题，直接促进低资源语言发展。

Abstract: Instruction tuning has become a key technique for enhancing the performance
of large language models, enabling them to better follow human prompts.
However, low-resource languages such as Luxembourgish face severe limitations
due to the lack of high-quality instruction datasets. Traditional reliance on
machine translation often introduces semantic misalignment and cultural
inaccuracies. In this work, we address these challenges by creating a
cross-lingual instruction tuning dataset for Luxembourgish, without resorting
to machine-generated translations into it. Instead, by leveraging aligned data
from English, French, and German, we build a high-quality dataset that
preserves linguistic and cultural nuances. We provide evidence that
cross-lingual instruction tuning not only improves representational alignment
across languages but also the model's generative capabilities in Luxembourgish.
This highlights how cross-lingual data curation can avoid the common pitfalls
of machine-translated data and directly benefit low-resource language
development.

</details>


### [306] [Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning](https://arxiv.org/abs/2510.07105)
*Taylor Sorensen,Yejin Choi*

Main category: cs.CL

TL;DR: 本文提出建模人类差异的系统，利用大语言模型上下文学习能力和两步元学习训练过程，在竞赛中获胜并进行消融实验。


<details>
  <summary>Details</summary>
Motivation: 许多NLP任务存在主观性、歧义或标注者间的合理分歧，需建模人类差异。

Method: 利用大语言模型上下文学习能力，采用两步元学习训练过程，包括多数据集上下文学习后训练和上下文元学习专门化模型。

Result: 系统在竞赛中两个任务均获胜，消融实验得出各组件对性能的影响。

Conclusion: 上下文中包含评分者示例对系统性能至关重要，特定数据集微调在大数据集上有帮助，其他上下文数据集后训练在一个竞赛数据集上有帮助，模型规模增加性能提升。

Abstract: Many natural language processing (NLP) tasks involve subjectivity, ambiguity,
or legitimate disagreement between annotators. In this paper, we outline our
system for modeling human variation. Our system leverages language models'
(LLMs) in-context learning abilities, along with a two-step meta-learning
training procedure for 1) post-training on many datasets requiring in-context
learning and 2) specializing the model via in-context meta-learning to the
particular data distribution of interest. We also evaluate the performance of
our system submission to the Learning With Disagreements (LeWiDi) competition,
where it was the overall winner on both tasks. Additionally, we perform an
ablation study to measure the importance of each system component. We find that
including rater examples in-context is crucial for our system's performance,
dataset-specific fine-tuning is helpful on the larger datasets, post-training
on other in-context datasets is helpful on one of the competition datasets, and
that performance improves with model scale.

</details>


### [307] [Comparing human and language models sentence processing difficulties on complex structures](https://arxiv.org/abs/2510.07141)
*Samuel Joseph Amouyal,Aya Meltzer-Asscher,Jonathan Berant*

Main category: cs.CL

TL;DR: 本文系统比较人类与大语言模型（LLMs）对七种复杂语言结构的句子理解，发现LLMs在特定结构上有困难，揭示了人类与LLMs句子理解的异同。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否会经历类似人类的语言处理困难。

Method: 在统一实验框架下，收集人类和五种不同规模及训练过程的先进LLMs对句子理解的数据。

Result: LLMs在目标结构上总体表现不佳，尤其是花园路径句；模型参数增加时，人类与模型表现排名相关性增加；人类在目标句和基线句的表现差距在多数LLMs中存在。

Conclusion: 研究揭示了人类和LLMs句子理解的趋同和差异，为两者相似性提供新见解。

Abstract: Large language models (LLMs) that fluently converse with humans are a reality
- but do LLMs experience human-like processing difficulties? We systematically
compare human and LLM sentence comprehension across seven challenging
linguistic structures. We collect sentence comprehension data from humans and
five families of state-of-the-art LLMs, varying in size and training procedure
in a unified experimental framework. Our results show LLMs overall struggle on
the target structures, but especially on garden path (GP) sentences. Indeed,
while the strongest models achieve near perfect accuracy on non-GP structures
(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).
Additionally, when ranking structures based on average performance, rank
correlation between humans and models increases with parameter count. For each
target structure, we also collect data for their matched baseline without the
difficult structure. Comparing performance on the target vs. baseline
sentences, the performance gap observed in humans holds for LLMs, with two
exceptions: for models that are too weak performance is uniformly low across
both sentence types, and for models that are too strong the performance is
uniformly high. Together, these reveal convergence and divergence in human and
LLM sentence comprehension, offering new insights into the similarity of humans
and LLMs.

</details>


### [308] [Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models](https://arxiv.org/abs/2510.07213)
*Chengzhi Zhong,Fei Cheng,Qianying Liu,Yugo Murawaki,Chenhui Chu,Sadao Kurohashi*

Main category: cs.CL

TL;DR: 提出无训练方法识别和操作跨语言转换维度，实验表明该方法可切换输出语言且保留语义，成本低性能优。


<details>
  <summary>Details</summary>
Motivation: 基于英语中心大语言模型跨语言转换现象，假设其由少量稀疏维度控制，欲验证并利用。

Method: 提出简单、无训练方法，仅需50句平行或单语数据识别和操作相关维度。

Result: 实验证明维度具有可解释性，干预这些维度能切换输出语言并保留语义，成本低且性能超先前基于神经元的方法。

Conclusion: 所提方法能有效控制多语言生成，在多语言生成控制任务中表现良好。

Abstract: Large language models exhibit strong multilingual capabilities despite
limited exposure to non-English data. Prior studies show that English-centric
large language models map multilingual content into English-aligned
representations at intermediate layers and then project them back into
target-language token spaces in the final layer. From this observation, we
hypothesize that this cross-lingual transition is governed by a small and
sparse set of dimensions, which occur at consistent indices across the
intermediate to final layers. Building on this insight, we introduce a simple,
training-free method to identify and manipulate these dimensions, requiring
only as few as 50 sentences of either parallel or monolingual data. Experiments
on a multilingual generation control task reveal the interpretability of these
dimensions, demonstrating that the interventions in these dimensions can switch
the output language while preserving semantic content, and that it surpasses
the performance of prior neuron-based approaches at a substantially lower cost.

</details>


### [309] [Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation](https://arxiv.org/abs/2510.07227)
*Arjun Krishnakumar,Rhea Sanjay Sukthanker,Hannan Javed Mahadik,Gabriela Kadlecová,Vladyslav Moroshan,Timur Carstensen,Frank Hutter,Aaron Klein*

Main category: cs.CL

TL;DR: 提出预训练小语言模型（SLMs）框架，结合三种方法使预训练更高效，最佳模型用更少预训练令牌达到可比效果，代码和模型开源。


<details>
  <summary>Details</summary>
Motivation: 小语言模型资源使用少但性能强，需有效预训练框架。

Method: 结合识别结构稀疏子网络初始化、用进化搜索发现高质量子网络初始化、应用知识蒸馏三种方法。

Result: 最佳模型用9.2倍少的预训练令牌达到与可比Pythia SLM相当的验证困惑度。

Conclusion: 该框架使SLM预训练更高效，开源代码和模型为大规模低成本开发SLM提供可行路径。

Abstract: Small Language models (SLMs) offer an efficient and accessible alternative to
Large Language Models (LLMs), delivering strong performance while using far
fewer resources. We introduce a simple and effective framework for pretraining
SLMs that brings together three complementary ideas. First, we identify
structurally sparse sub-network initializations that consistently outperform
randomly initialized models of similar size under the same compute budget.
Second, we use evolutionary search to automatically discover high-quality
sub-network initializations, providing better starting points for pretraining.
Third, we apply knowledge distillation from larger teacher models to speed up
training and improve generalization. Together, these components make SLM
pretraining substantially more efficient: our best model, discovered using
evolutionary search and initialized with LLM weights, matches the validation
perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining
tokens. We release all code and models at
https://github.com/whittle-org/whittle/, offering a practical and reproducible
path toward cost-efficient small language model development at scale.

</details>


### [310] [Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships](https://arxiv.org/abs/2510.07231)
*Donggyu Lee,Sungwon Park,Yerin Hwang,Hyunwoo Oh,Hyoshin Kim,Jungwon Kim,Meeyoung Cha,Sangyoon Park,Jihee Kim*

Main category: cs.CL

TL;DR: 提出新的因果推理基准测试，实验显示现有大语言模型在因果推理上有局限性。


<details>
  <summary>Details</summary>
Motivation: 现有因果推理基准测试有依赖合成数据和领域覆盖窄的问题，需要新的基准测试。

Method: 从顶级经济和金融期刊提取因果关系构建含40379个评估项、覆盖5种任务类型的基准测试。

Result: 对8个先进大语言模型实验，最佳模型准确率仅57.6%，模型规模与性能无必然联系，先进推理模型也难识别基本因果关系。

Conclusion: 当前大语言模型能力与高风险应用中可靠因果推理的需求存在巨大差距。

Abstract: Causal reasoning is fundamental for Large Language Models (LLMs) to
understand genuine cause-and-effect relationships beyond pattern matching.
Existing benchmarks suffer from critical limitations such as reliance on
synthetic data and narrow domain coverage. We introduce a novel benchmark
constructed from casually identified relationships extracted from top-tier
economics and finance journals, drawing on rigorous methodologies including
instrumental variables, difference-in-differences, and regression discontinuity
designs. Our benchmark comprises 40,379 evaluation items covering five task
types across domains such as health, environment, technology, law, and culture.
Experimental results on eight state-of-the-art LLMs reveal substantial
limitations, with the best model achieving only 57.6\% accuracy. Moreover,
model scale does not consistently translate to superior performance, and even
advanced reasoning models struggle with fundamental causal relationship
identification. These findings underscore a critical gap between current LLM
capabilities and demands of reliable causal reasoning in high-stakes
applications.

</details>


### [311] [LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation](https://arxiv.org/abs/2510.07243)
*Joseph Enguehard,Morgane Van Ermengem,Kate Atkinson,Sujeong Cha,Arijit Ghosh Chowdhury,Prashanth Kallur Ramaswamy,Jeremy Roghair,Hannah R Marlowe,Carina Suzana Negreanu,Kitty Boxall,Diana Mincu*

Main category: cs.CL

TL;DR: 本文提出一种无参考的大语言模型法律领域输出评估方法，在数据集上表现优于基线，与人类专家评估相关性高，还开源部分数据。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在法律领域输出评估方法存在依赖参考数据成本高、标准化评估有局限等问题，LLM - as - a - Judge方法在法律场景可靠性和有效性不足。

Method: 将长回复拆分为“法律数据点”（LDPs），提出无参考评估方法。

Result: 该方法在专有数据集和开源数据集LegalBench上优于多种基线，与人类专家评估相关性更高，有助于提高标注者间一致性。

Conclusion: 提出的评估方法有效，开源部分数据利于推动大语言模型法律问答评估研究。

Abstract: Evaluating large language model (LLM) outputs in the legal domain presents
unique challenges due to the complex and nuanced nature of legal analysis.
Current evaluation approaches either depend on reference data, which is costly
to produce, or use standardized assessment methods, both of which have
significant limitations for legal applications.
  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its
reliability and effectiveness in legal contexts depend heavily on evaluation
processes unique to the legal industry and how trustworthy the evaluation
appears to the human legal expert. This is where existing evaluation methods
currently fail and exhibit considerable variability.
  This paper aims to close the gap: a) we break down lengthy responses into
'Legal Data Points' (LDPs), self-contained units of information, and introduce
a novel, reference-free evaluation methodology that reflects how lawyers
evaluate legal answers; b) we demonstrate that our method outperforms a variety
of baselines on both our proprietary dataset and an open-source dataset
(LegalBench); c) we show how our method correlates more closely with human
expert evaluations and helps improve inter-annotator agreement; and finally d)
we open source our Legal Data Points for a subset of LegalBench used in our
experiments, allowing the research community to replicate our results and
advance research in this vital area of LLM evaluation on legal
question-answering.

</details>


### [312] [TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning](https://arxiv.org/abs/2510.07118)
*Manish Nagaraj,Sakshi Choudhary,Utkarsh Saxena,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CL

TL;DR: 提出TRIM框架用于选择指令调优核心集，高效且表现优。


<details>
  <summary>Details</summary>
Motivation: 现有核心集筛选方法计算成本高且忽略细粒度特征，需改进。

Method: 引入TRIM，通过基于注意力的“指纹”匹配目标样本的潜在表征模式。

Result: TRIM所选核心集在下游任务中比基线高9%，部分设置超全数据微调，成本低。

Conclusion: TRIM是构建高质量指令调优数据集的可扩展高效替代方案。

Abstract: Instruction tuning is essential for aligning large language models (LLMs) to
downstream tasks and commonly relies on large, diverse corpora. However, small,
high-quality subsets, known as coresets, can deliver comparable or superior
results, though curating them remains challenging. Existing methods often rely
on coarse, sample-level signals like gradients, an approach that is
computationally expensive and overlooks fine-grained features. To address this,
we introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a
forward-only, token-centric framework. Instead of using gradients, TRIM
operates by matching underlying representational patterns identified via
attention-based "fingerprints" from a handful of target samples. Such an
approach makes TRIM highly efficient and uniquely sensitive to the structural
features that define a task. Coresets selected by our method consistently
outperform state-of-the-art baselines by up to 9% on downstream tasks and even
surpass the performance of full-data fine-tuning in some settings. By avoiding
expensive backward passes, TRIM achieves this at a fraction of the
computational cost. These findings establish TRIM as a scalable and efficient
alternative for building high-quality instruction-tuning datasets.

</details>


### [313] [Online Rubrics Elicitation from Pairwise Comparisons](https://arxiv.org/abs/2510.07284)
*MohammadHossein Rezaei,Robert Vacareanu,Zihao Wang,Clinton Wang,Yunzhong He,Afra Feyza Akyürek*

Main category: cs.CL

TL;DR: 提出OnlineRubrics方法动态管理评估标准，在多数据集上比静态规则训练有提升


<details>
  <summary>Details</summary>
Motivation: 现有基于静态规则训练大语言模型易受奖励破解影响，且无法捕获训练中出现的新需求

Method: 引入Online Rubrics Elicitation方法，通过当前和参考策略响应的成对比较在线动态管理评估标准

Result: 在AlpacaEval、GPQA等数据集上比仅用静态规则训练有最高达8%的持续提升，定性分析得出突出主题

Conclusion: OnlineRubrics方法有效，能持续识别和减少训练中的错误，且可挖掘出评估标准的突出主题

Abstract: Rubrics provide a flexible way to train LLMs on open-ended long-form answers
where verifiable rewards are not applicable and human preferences provide
coarse signals. Prior work shows that reinforcement learning with rubric-based
rewards leads to consistent gains in LLM post-training. Most existing
approaches rely on rubrics that remain static over the course of training. Such
static rubrics, however, are vulnerable to reward-hacking type behaviors and
fail to capture emergent desiderata that arise during training. We introduce
Online Rubrics Elicitation (OnlineRubrics), a method that dynamically curates
evaluation criteria in an online manner through pairwise comparisons of
responses from current and reference policies. This online process enables
continuous identification and mitigation of errors as training proceeds.
Empirically, this approach yields consistent improvements of up to 8% over
training exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as
well as the validation sets of expert questions and rubrics. We qualitatively
analyze the elicited criteria and identify prominent themes such as
transparency, practicality, organization, and reasoning.

</details>


### [314] [NurseLLM: The First Specialized Language Model for Nursing](https://arxiv.org/abs/2510.07173)
*Md Tawkat Islam Khondaker,Julia Harrington,Shady Shehata*

Main category: cs.CL

TL;DR: 介绍首个护理专业大语言模型NurseLLM，构建数据集训练，评估显示其表现优，还探讨推理与多智能体协作系统。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在护理领域潜力未充分挖掘，需专业模型。

Method: 开发多阶段数据生成管道构建大规模护理选择题数据集，引入多个护理基准进行评估。

Result: NurseLLM在不同基准测试中优于同等规模的通用和医疗专业大语言模型。

Conclusion: 强调护理领域专业大语言模型的重要性，推理和多智能体协作系统有未来研究和应用前景。

Abstract: Recent advancements in large language models (LLMs) have significantly
transformed medical systems. However, their potential within specialized
domains such as nursing remains largely underexplored. In this work, we
introduce NurseLLM, the first nursing-specialized LLM tailored for multiple
choice question-answering (MCQ) tasks. We develop a multi-stage data generation
pipeline to build the first large scale nursing MCQ dataset to train LLMs on a
broad spectrum of nursing topics. We further introduce multiple nursing
benchmarks to enable rigorous evaluation. Our extensive experiments demonstrate
that NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of
comparable size on different benchmarks, underscoring the importance of a
specialized LLM for the nursing domain. Finally, we explore the role of
reasoning and multi-agent collaboration systems in nursing, highlighting their
promise for future research and applications.

</details>


### [315] [Quantifying Data Contamination in Psychometric Evaluations of LLMs](https://arxiv.org/abs/2510.07175)
*Jongwook Han,Woojung Song,Jonggeun Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出框架测量大语言模型心理测量评估中的数据污染，发现部分常用问卷存在强污染。


<details>
  <summary>Details</summary>
Motivation: 先前工作虽提出心理测量问卷数据污染可能威胁评估可靠性，但未系统量化污染程度，本文旨在填补此空白。

Method: 提出框架从项目记忆、评估记忆和目标分数匹配三方面测量数据污染，并应用于21个主流模型和4个常用心理测量问卷。

Result: 发现像大五人格问卷（BFI - 44）和肖像价值观问卷（PVQ - 40）等常用问卷存在强污染，模型不仅能记住项目，还能调整回答以达到特定目标分数。

Conclusion: 通过所提框架证实了大语言模型心理测量评估中数据污染问题的严重性。

Abstract: Recent studies apply psychometric questionnaires to Large Language Models
(LLMs) to assess high-level psychological constructs such as values,
personality, moral foundations, and dark traits. Although prior work has raised
concerns about possible data contamination from psychometric inventories, which
may threaten the reliability of such evaluations, there has been no systematic
attempt to quantify the extent of this contamination. To address this gap, we
propose a framework to systematically measure data contamination in
psychometric evaluations of LLMs, evaluating three aspects: (1) item
memorization, (2) evaluation memorization, and (3) target score matching.
Applying this framework to 21 models from major families and four widely used
psychometric inventories, we provide evidence that popular inventories such as
the Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40)
exhibit strong contamination, where models not only memorize items but can also
adjust their responses to achieve specific target scores.

</details>


### [316] [Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense](https://arxiv.org/abs/2510.07242)
*Leitian Tao,Ilia Kulikov,Swarnadeep Saha,Tianlu Wang,Jing Xu,Yixuan Li,Jason E Weston,Ping Yu*

Main category: cs.CL

TL;DR: 介绍了HERO强化学习框架，将验证器信号与奖励模型分数结合，在数学推理基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理的后训练依赖的二元反馈存在局限性，奖励模型可提供更丰富的连续反馈作为补充监督信号。

Method: 引入HERO框架，采用分层归一化和方差感知加权，将验证器信号与奖励模型分数结构化整合。

Result: 在多种数学推理基准测试中，HERO始终优于仅使用奖励模型和仅使用验证器的基线，在可验证和难以验证的任务上均有显著提升。

Conclusion: 混合奖励设计保留了验证器的稳定性，同时利用奖励模型的细微差别推动推理进步。

Abstract: Post-training for reasoning of large language models (LLMs) increasingly
relies on verifiable rewards: deterministic checkers that provide 0-1
correctness signals. While reliable, such binary feedback is brittle--many
tasks admit partially correct or alternative answers that verifiers
under-credit, and the resulting all-or-nothing supervision limits learning.
Reward models offer richer, continuous feedback, which can serve as a
complementary supervisory signal to verifiers. We introduce HERO (Hybrid
Ensemble Reward Optimization), a reinforcement learning framework that
integrates verifier signals with reward-model scores in a structured way. HERO
employs stratified normalization to bound reward-model scores within
verifier-defined groups, preserving correctness while refining quality
distinctions, and variance-aware weighting to emphasize challenging prompts
where dense signals matter most. Across diverse mathematical reasoning
benchmarks, HERO consistently outperforms RM-only and verifier-only baselines,
with strong gains on both verifiable and hard-to-verify tasks. Our results show
that hybrid reward design retains the stability of verifiers while leveraging
the nuance of reward models to advance reasoning.

</details>


### [317] [Artificial Hippocampus Networks for Efficient Long-Context Modeling](https://arxiv.org/abs/2510.07318)
*Yunhao Fang,Weihao Yu,Shu Zhong,Qinghao Ye,Xuehan Xiong,Lai Wei*

Main category: cs.CL

TL;DR: 提出基于认知科学多存储模型的人工神经网络记忆框架，用AHN压缩信息，实验表明增强模型性能优且降低计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 解决长序列建模中RNN类模型压缩固定大小内存效率与基于注意力的Transformer无损增长内存保真度之间的权衡问题。

Method: 维护Transformer的KV缓存滑动窗口作为无损短期记忆，用可学习的AHN模块将窗口外信息压缩为固定大小的长期记忆，并使用现代RNN类架构实例化AHN。

Result: AHN增强模型在长上下文基准测试中始终优于滑动窗口基线，性能与全注意力模型相当甚至更优，大幅降低计算和内存需求，如增强Qwen2.5 - 3B - Instruct的效果。

Conclusion: 所提出的记忆框架有效，能在长序列建模中提升性能并减少资源消耗。

Abstract: Long-sequence modeling faces a fundamental trade-off between the efficiency
of compressive fixed-size memory in RNN-like models and the fidelity of
lossless growing memory in attention-based Transformers. Inspired by the
Multi-Store Model in cognitive science, we introduce a memory framework of
artificial neural networks. Our method maintains a sliding window of the
Transformer's KV cache as lossless short-term memory, while a learnable module
termed Artificial Hippocampus Network (AHN) recurrently compresses
out-of-window information into a fixed-size compact long-term memory. To
validate this framework, we instantiate AHNs using modern RNN-like
architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive
experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate
that AHN-augmented models consistently outperform sliding window baselines and
achieve performance comparable or even superior to full-attention models, while
substantially reducing computational and memory requirements. For instance,
augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%
and memory cache by 74.0%, while improving its average score on LV-Eval (128k
sequence length) from 4.41 to 5.88. Code is available at:
https://github.com/ByteDance-Seed/AHN.

</details>


### [318] [On the Convergence of Moral Self-Correction in Large Language Models](https://arxiv.org/abs/2510.07290)
*Guangliang Liu,Haitao Mao,Bochuan Cao,Zhiyu Xue,Xitong Zhang,Rongrong Wang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 研究大语言模型内在自我修正机制，聚焦道德自我修正，揭示多轮交互收敛特性并分析其机制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型内在自我修正经验上成功，但原理未知，需研究道德自我修正机制。

Method: 对大语言模型道德自我修正进行实验，分析多轮交互表现。

Result: 发现多轮交互中性能收敛特性，揭示收敛机制为持续注入指令激活道德概念减少模型不确定性。

Conclusion: 道德自我修正有实现收敛性能的潜力。

Abstract: Large Language Models (LLMs) are able to improve their responses when
instructed to do so, a capability known as self-correction. When instructions
provide only a general and abstract goal without specific details about
potential issues in the response, LLMs must rely on their internal knowledge to
improve response quality, a process referred to as intrinsic self-correction.
The empirical success of intrinsic self-correction is evident in various
applications, but how and why it is effective remains unknown. Focusing on
moral self-correction in LLMs, we reveal a key characteristic of intrinsic
self-correction: performance convergence through multi-round interactions; and
provide a mechanistic analysis of this convergence behavior. Based on our
experimental results and analysis, we uncover the underlying mechanism of
convergence: consistently injected self-correction instructions activate moral
concepts that reduce model uncertainty, leading to converged performance as the
activated moral concepts stabilize over successive rounds. This paper
demonstrates the strong potential of moral self-correction by showing that it
exhibits a desirable property of converged performance.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [319] [Data as Commodity: a Game-Theoretic Principle for Information Pricing](https://arxiv.org/abs/2510.07101)
*Pasquale Casaburi,Giovanni Piccioli,Pierpaolo Vivo*

Main category: cond-mat.stat-mech

TL;DR: 本文提出博弈论方法为数据定价，揭示了有别于传统经济学的市场效应，为数字市场无形商品定价奠定理论基础。


<details>
  <summary>Details</summary>
Motivation: 数据特性挑战标准供需理论，需新定价原则。

Method: 采用博弈论方法，分析N个玩家基于潜在随机过程的战略竞争，计算纳什均衡。

Result: 确定数据交易双赢的价格范围，发现多种有别于传统经济学的市场效应，如不同信息质量下参与者的竞争与合作、玩家数量对交易可行性的影响等。

Conclusion: 研究为动态交互数字市场中无形商品定价提供了理论基础。

Abstract: Data is the central commodity of the digital economy. Unlike physical goods,
it is non-rival, replicable at near-zero cost, and traded under heterogeneous
licensing rules. These properties defy standard supply--demand theory and call
for new pricing principles. We propose a game-theoretic approach in which the
value of a data string emerges from strategic competition among N players
betting on an underlying stochastic process, each holding partial information
about past outcomes. A better-informed player faces a choice: exploit their
informational advantage, or sell part of their dataset to less-informed
competitors. By analytically computing the Nash equilibrium of the game, we
determine the price range where the trade is beneficial to both buyer and
seller. We uncover a rich landscape of market effects that diverge from
textbook economics: first, prospective sellers and buyers can compete or
jointly exploit the less informed competitors depending on the quality of data
they hold. In a symbiotic regime, the seller can even share data for free while
still improving her payoffs, showing that losing exclusivity does not
necessarily reduce profit. Moreover, rivalry between well-informed players can
paradoxically benefit uninformed ones, demonstrating that information abundance
does not always translate to higher payoffs. We also show that the number of
players influences the competition between informed parties: trades impossible
in small markets become feasible in larger ones. These findings establish a
theoretical foundation for the pricing of intangible goods in dynamically
interacting digital markets, which are in need of robust valuation principles.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [320] [Visualizing Multimodality in Combinatorial Search Landscapes](https://arxiv.org/abs/2510.06517)
*Xavier F. C. Sánchez-Díaz,Ole Jakob Mengshoel*

Main category: cs.GR

TL;DR: 本文探讨组合搜索景观的可视化技术，聚焦多峰性，结合不同技术，给出示例并讨论相关工作，得出可视化无免费午餐的结论，并给出未来工作建议。


<details>
  <summary>Details</summary>
Motivation: 为组合搜索景观提供更全面的可视化视图，聚焦多峰性。

Method: 讨论景观分析文献中的不同技术并进行组合，结合图形语法的几何和美学元素，提供示例和讨论相关工作。

Result: 展示了如何结合不同技术来呈现搜索景观，给出了实际应用示例。

Conclusion: 可视化无免费午餐，并给出该领域未来工作的建议。

Abstract: This work walks through different visualization techniques for combinatorial
search landscapes, focusing on multimodality. We discuss different techniques
from the landscape analysis literature, and how they can be combined to provide
a more comprehensive view of the search landscape. We also include examples and
discuss relevant work to show how others have used these techniques in
practice, based on the geometric and aesthetic elements of the Grammar of
Graphics. We conclude that there is no free lunch in visualization, and provide
recommendations for future work as there are several paths to continue the work
in this field.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [321] [FEAorta: A Fully Automated Framework for Finite Element Analysis of the Aorta From 3D CT Images](https://arxiv.org/abs/2510.06621)
*Jiasong Chen,Linchen Qian,Ruonan Gong,Christina Sun,Tongran Qin,Thuy Pham,Caitlin Martin,Mohammad Zafar,John Elefteriades,Wei Sun,Liang Liang*

Main category: eess.IV

TL;DR: 本文聚焦胸主动脉瘤破裂风险评估，针对现有工程工具临床应用的两大障碍，先克服计算负担问题，现致力于开发端到端DNN解决3D重建难题。


<details>
  <summary>Details</summary>
Motivation: 胸主动脉瘤是成人主要死因之一，现有工程工具在临床应用中因3D重建和计算负担两大障碍受限。

Method: 开发端到端深度神经网络，直接从3D CT图像生成患者特定的主动脉有限元网格；此前开发PyTorch FEA库和FEA DNN集成框架解决计算负担问题。

Result: 此前将基于FEA的应力计算时间降至约三分钟/例，集成DNN和FEA后降至几秒/例。

Conclusion: 致力于开发端到端深度神经网络以克服3D重建障碍，推动胸主动脉瘤破裂风险评估工具的临床应用。

Abstract: Aortic aneurysm disease ranks consistently in the top 20 causes of death in
the U.S. population. Thoracic aortic aneurysm is manifested as an abnormal
bulging of thoracic aortic wall and it is a leading cause of death in adults.
From the perspective of biomechanics, rupture occurs when the stress acting on
the aortic wall exceeds the wall strength. Wall stress distribution can be
obtained by computational biomechanical analyses, especially structural Finite
Element Analysis. For risk assessment, probabilistic rupture risk of TAA can be
calculated by comparing stress with material strength using a material failure
model. Although these engineering tools are currently available for TAA rupture
risk assessment on patient specific level, clinical adoption has been limited
due to two major barriers: labor intensive 3D reconstruction current patient
specific anatomical modeling still relies on manual segmentation, making it
time consuming and difficult to scale to a large patient population, and
computational burden traditional FEA simulations are resource intensive and
incompatible with time sensitive clinical workflows. The second barrier was
successfully overcome by our team through the development of the PyTorch FEA
library and the FEA DNN integration framework. By incorporating the FEA
functionalities within PyTorch FEA and applying the principle of static
determinacy, we reduced the FEA based stress computation time to approximately
three minutes per case. Moreover, by integrating DNN and FEA through the
PyTorch FEA library, our approach further decreases the computation time to
only a few seconds per case. This work focuses on overcoming the first barrier
through the development of an end to end deep neural network capable of
generating patient specific finite element meshes of the aorta directly from 3D
CT images.

</details>


### [322] [Stacked Regression using Off-the-shelf, Stimulus-tuned and Fine-tuned Neural Networks for Predicting fMRI Brain Responses to Movies (Algonauts 2025 Report)](https://arxiv.org/abs/2510.06235)
*Robert Scholz,Kunal Bagga,Christine Ahrends,Carlo Alberto Barbano*

Main category: eess.IV

TL;DR: 本文介绍参加Algonauts 2025挑战赛的方案，用多模态表征预测fMRI脑响应，排名第10并公开代码。


<details>
  <summary>Details</summary>
Motivation: 参加Algonauts 2025挑战赛，预测fMRI脑对电影刺激的响应。

Method: 整合大语言模型、视频编码器、音频模型和视觉语言模型的多模态表征，增强文本输入，探索刺激调整和微调策略，用堆叠回归组合各模型预测结果。

Result: 团队Seinfeld排名第10。

Conclusion: 公开代码和资源，为多模态脑活动编码模型发展做贡献。

Abstract: We present our submission to the Algonauts 2025 Challenge, where the goal is
to predict fMRI brain responses to movie stimuli. Our approach integrates
multimodal representations from large language models, video encoders, audio
models, and vision-language models, combining both off-the-shelf and fine-tuned
variants. To improve performance, we enhanced textual inputs with detailed
transcripts and summaries, and we explored stimulus-tuning and fine-tuning
strategies for language and vision models. Predictions from individual models
were combined using stacked regression, yielding solid results. Our submission,
under the team name Seinfeld, ranked 10th. We make all code and resources
publicly available, contributing to ongoing efforts in developing multimodal
encoding models for brain activity.

</details>


### [323] [A Total Variation Regularized Framework for Epilepsy-Related MRI Image Segmentation](https://arxiv.org/abs/2510.06276)
*Mehdi Rabiee,Sergio Greco,Reza Shahbazian,Irina Trubitsyna*

Main category: eess.IV

TL;DR: 提出用于3D脑MRI图像中FCD区域分割的新框架，采用Transformer增强的编解码器架构及新损失函数，在公共数据集上表现优于标准损失公式。


<details>
  <summary>Details</summary>
Motivation: FCD是耐药性癫痫主要病因，在MRI中难检测，准确分割FCD区域对手术规划和治疗至关重要，但因数据集少、病灶小等问题，任务极具挑战。

Method: 采用Transformer增强的编解码器架构，引入结合Dice损失和各向异性总变差项的新损失函数。

Result: 在公共FCD数据集上，与标准损失公式相比，所提模型Dice系数提高11.9%，精度提高13.3%，假阳性簇数量减少61.6%。

Conclusion: 所提框架在分割FCD区域时具有更好的准确性和一致性。

Abstract: Focal Cortical Dysplasia (FCD) is a primary cause of drug-resistant epilepsy
and is difficult to detect in brain {magnetic resonance imaging} (MRI) due to
the subtle and small-scale nature of its lesions. Accurate segmentation of FCD
regions in 3D multimodal brain MRI images is essential for effective surgical
planning and treatment. However, this task remains highly challenging due to
the limited availability of annotated FCD datasets, the extremely small size
and weak contrast of FCD lesions, the complexity of handling 3D multimodal
inputs, and the need for output smoothness and anatomical consistency, which is
often not addressed by standard voxel-wise loss functions. This paper presents
a new framework for segmenting FCD regions in 3D brain MRI images. We adopt
state-of-the-art transformer-enhanced encoder-decoder architecture and
introduce a novel loss function combining Dice loss with an anisotropic {Total
Variation} (TV) term. This integration encourages spatial smoothness and
reduces false positive clusters without relying on post-processing. The
framework is evaluated on a public FCD dataset with 85 epilepsy patients and
demonstrates superior segmentation accuracy and consistency compared to
standard loss formulations. The model with the proposed TV loss shows an 11.9\%
improvement on the Dice coefficient and 13.3\% higher precision over the
baseline model. Moreover, the number of false positive clusters is reduced by
61.6%

</details>


### [324] [SER-Diff: Synthetic Error Replay Diffusion for Incremental Brain Tumor Segmentation](https://arxiv.org/abs/2510.06283)
*Sashank Makanaboyina*

Main category: eess.IV

TL;DR: 提出SER - Diff框架结合扩散细化与增量学习，在多数据集实验中表现优于先前方法，能减轻灾难性遗忘并实现更准确分割。


<details>
  <summary>Details</summary>
Motivation: 增量脑肿瘤分割中存在灾难性遗忘问题，现有增量学习框架有局限，扩散模型在增量学习中未被探索。

Method: 提出SER - Diff框架，利用冻结的教师扩散模型生成过去任务的合成误差图，在新任务训练时重放，采用结合Dice损失和知识蒸馏损失的双损失公式。

Result: 在BraTS2020、BraTS2021和BraTS2023数据集上，SER - Diff始终优于先前方法，取得最高Dice分数和最低HD95值。

Conclusion: SER - Diff能减轻灾难性遗忘，在不断演变的数据集中实现更准确和符合解剖学的分割。

Abstract: Incremental brain tumor segmentation is critical for models that must adapt
to evolving clinical datasets without retraining on all prior data. However,
catastrophic forgetting, where models lose previously acquired knowledge,
remains a major obstacle. Recent incremental learning frameworks with knowledge
distillation partially mitigate forgetting but rely heavily on generative
replay or auxiliary storage. Meanwhile, diffusion models have proven effective
for refining tumor segmentations, but have not been explored in incremental
learning contexts. We propose Synthetic Error Replay Diffusion (SER-Diff), the
first framework that unifies diffusion-based refinement with incremental
learning. SER-Diff leverages a frozen teacher diffusion model to generate
synthetic error maps from past tasks, which are replayed during training on new
tasks. A dual-loss formulation combining Dice loss for new data and knowledge
distillation loss for replayed errors ensures both adaptability and retention.
Experiments on BraTS2020, BraTS2021, and BraTS2023 demonstrate that SER-Diff
consistently outperforms prior methods. It achieves the highest Dice scores of
95.8\%, 94.9\%, and 94.6\%, along with the lowest HD95 values of 4.4 mm, 4.7
mm, and 4.9 mm, respectively. These results indicate that SER-Diff not only
mitigates catastrophic forgetting but also delivers more accurate and
anatomically coherent segmentations across evolving datasets.

</details>


### [325] [Conditional Denoising Diffusion Model-Based Robust MR Image Reconstruction from Highly Undersampled Data](https://arxiv.org/abs/2510.06335)
*Mohammed Alsubaie,Wenxi Liu,Linxia Gu,Ovidiu C. Andronesi,Sirani M. Perera,Xianqi Li*

Main category: eess.IV

TL;DR: 提出含迭代数据一致性校正的条件去噪扩散框架用于加速MRI重建，在fastMRI数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: MRI采集时间长，欠采样策略会导致图像伪影和质量下降，现有扩散模型方法存在不足。

Method: 引入含迭代数据一致性校正的条件去噪扩散框架，将测量模型嵌入每个反向扩散步骤，并在配对的欠采样-真实数据上训练模型。

Result: 在fastMRI数据集上，该框架在SSIM、PSNR和LPIPS指标上优于近期先进的深度学习和基于扩散的方法。

Conclusion: 将条件监督与迭代一致性更新相结合，可在像素级保真度和感知真实性上取得显著改进，推动了稳健、加速的MRI重建。

Abstract: Magnetic Resonance Imaging (MRI) is a critical tool in modern medical
diagnostics, yet its prolonged acquisition time remains a critical limitation,
especially in time-sensitive clinical scenarios. While undersampling strategies
can accelerate image acquisition, they often result in image artifacts and
degraded quality. Recent diffusion models have shown promise for reconstructing
high-fidelity images from undersampled data by learning powerful image priors;
however, most existing approaches either (i) rely on unsupervised score
functions without paired supervision or (ii) apply data consistency only as a
post-processing step. In this work, we introduce a conditional denoising
diffusion framework with iterative data-consistency correction, which differs
from prior methods by embedding the measurement model directly into every
reverse diffusion step and training the model on paired undersampled-ground
truth data. This hybrid design bridges generative flexibility with explicit
enforcement of MRI physics. Experiments on the fastMRI dataset demonstrate that
our framework consistently outperforms recent state-of-the-art deep learning
and diffusion-based methods in SSIM, PSNR, and LPIPS, with LPIPS capturing
perceptual improvements more faithfully. These results demonstrate that
integrating conditional supervision with iterative consistency updates yields
substantial improvements in both pixel-level fidelity and perceptual realism,
establishing a principled and practical advance toward robust, accelerated MRI
reconstruction.

</details>


### [326] [Fitzpatrick Thresholding for Skin Image Segmentation](https://arxiv.org/abs/2510.06655)
*Duncan Stothers,Sophia Xu,Carlie Reeves,Lia Gracey*

Main category: eess.IV

TL;DR: 本文聚焦银屑病皮疹体表面积准确估计问题，提出菲茨帕特里克阈值法提升不同肤色分割性能。


<details>
  <summary>Details</summary>
Motivation: 准确估计皮疹体表面积对评估病情、选治疗方案和跟踪疗效很重要，但现有炎症性皮肤病分割方法在深色皮肤效果差，影响公平治疗。

Method: 组装含菲茨帕特里克皮肤类型注释的银屑病数据集，训练无肤色信息的参考模型，在调优集上选择全局和按肤色的最优决策阈值。

Result: 采用菲茨帕特里克特定阈值提升了最深色组（Fitz VI）的分割性能，不同模型均有增益，且菲茨帕特里克肤色分类器准确率超95%，该技术所需肤色标注成本大幅下降。

Conclusion: 菲茨帕特里克阈值法简单、与模型无关、无需架构更改和重新训练且成本低，可作为未来公平性基线。

Abstract: Accurate estimation of the body surface area (BSA) involved by a rash, such
as psoriasis, is critical for assessing rash severity, selecting an initial
treatment regimen, and following clinical treatment response. Attempts at
segmentation of inflammatory skin disease such as psoriasis perform markedly
worse on darker skin tones, potentially impeding equitable care. We assembled a
psoriasis dataset sourced from six public atlases, annotated for Fitzpatrick
skin type, and added detailed segmentation masks for every image. Reference
models based on U-Net, ResU-Net, and SETR-small are trained without tone
information. On the tuning split we sweep decision thresholds and select (i)
global optima and (ii) per Fitzpatrick skin tone optima for Dice and binary
IoU. Adapting Fitzpatrick specific thresholds lifted segmentation performance
for the darkest subgroup (Fitz VI) by up to +31 % bIoU and +24 % Dice on UNet,
with consistent, though smaller, gains in the same direction for ResU-Net (+25
% bIoU, +18 % Dice) and SETR-small (+17 % bIoU, +11 % Dice). Because
Fitzpatrick skin tone classifiers trained on Fitzpatrick-17k now exceed 95 %
accuracy, the cost of skin tone labeling required for this technique has fallen
dramatically. Fitzpatrick thresholding is simple, model-agnostic, requires no
architectural changes, no re-training, and is virtually cost free. We
demonstrate the inclusion of Fitzpatrick thresholding as a potential future
fairness baseline.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [327] [Mass Conservation on Rails -- Rethinking Physics-Informed Learning of Ice Flow Vector Fields](https://arxiv.org/abs/2510.06286)
*Kim Bente,Roman Marchant,Fabio Ramos*

Main category: physics.ao-ph

TL;DR: 提出无散度神经网络（dfNNs）用于南极冰流矢量场插值，对比显示其更可靠，方向引导策略可提升各模型性能。


<details>
  <summary>Details</summary>
Motivation: 为可靠预测未来海平面上升，冰盖模型需要符合物理原理的输入，现有物理信息神经网络有局限性。

Method: 提出dfNNs，通过矢量微积分技巧严格执行局部质量守恒，并对比dfNNs、PINNs和无约束NNs，采用方向引导学习策略。

Result: 在伯德冰川冰通量插值中，dfNNs能产生更可靠估计，方向引导策略提升各模型性能。

Conclusion: dfNNs在冰流矢量场插值中表现更好，方向引导策略有助于提升模型性能。

Abstract: To reliably project future sea level rise, ice sheet models require inputs
that respect physics. Embedding physical principles like mass conservation into
models that interpolate Antarctic ice flow vector fields from sparse & noisy
measurements not only promotes physical adherence but can also improve accuracy
and robustness. While physics-informed neural networks (PINNs) impose physics
as soft penalties, offering flexibility but no physical guarantees, we instead
propose divergence-free neural networks (dfNNs), which enforce local mass
conservation exactly via a vector calculus trick. Our comparison of dfNNs,
PINNs, and unconstrained NNs on ice flux interpolation over Byrd Glacier
suggests that "mass conservation on rails" yields more reliable estimates, and
that directional guidance, a learning strategy leveraging continent-wide
satellite velocity data, boosts performance across models.

</details>


### [328] [Developing a Sequential Deep Learning Pipeline to Model Alaskan Permafrost Thaw Under Climate Change](https://arxiv.org/abs/2510.06258)
*Addina Rahaman*

Main category: physics.ao-ph

TL;DR: 本文提出基于纬度的深度学习管道来模拟多年土壤温度，测试五种模型，GRU表现最佳，建立了用于活动层温度建模的端到端框架。


<details>
  <summary>Details</summary>
Motivation: 气候变化威胁永久冻土冻融循环，准确预测土壤温度对风险缓解和稳定性评估至关重要，但现有方法忽视诸多影响土壤热动力学的因素。

Method: 提出基于纬度的深度学习管道，使用ERA5 - Land数据集的动态再分析特征数据、静态地质和岩性特征等，测试TCN、Transformer、Conv1DLSTM、GRU和BiLSTM五种模型。

Result: 能识别纬度和深度的温度差异，GRU在顺序温度模式检测中表现最佳，偏差校正的CMIP5 RCP数据可识别正弦温度趋势，但情景间差异有限。

Conclusion: 建立了用于活动层温度建模的端到端框架，提供季节、空间和垂直温度背景，对特征选择无内在限制。

Abstract: Changing climate conditions threaten the natural permafrost thaw-freeze
cycle, leading to year-round soil temperatures above 0{\deg}C. In Alaska, the
warming of the topmost permafrost layer, known as the active layer, signals
elevated greenhouse gas release due to high carbon storage. Accurate soil
temperature prediction is therefore essential for risk mitigation and stability
assessment; however, many existing approaches overlook the numerous factors
driving soil thermal dynamics. This study presents a proof-of-concept
latitude-based deep learning pipeline for modeling yearly soil temperatures
across multiple depths. The framework employs dynamic reanalysis feature data
from the ERA5-Land dataset, static geologic and lithological features,
sliding-window sequences for seasonal context, a derived scenario signal
feature for long-term climate forcing, and latitude band embeddings for spatial
sensitivity. Five deep learning models were tested: a Temporal Convolutional
Network (TCN), a Transformer, a 1-Dimensional Convolutional Long-Short Term
Memory (Conv1DLSTM), a Gated-Recurrent Unit (GRU), and a Bidirectional
Long-Short Term Memory (BiLSTM). Results showed solid recognition of
latitudinal and depth-wise temperature discrepancies, with the GRU performing
best in sequential temperature pattern detection. Bias-corrected CMIP5 RCP data
enabled recognition of sinusoidal temperature trends, though limited divergence
between scenarios were observed. This study establishes an end-to-end framework
for adopting deep learning in active layer temperature modeling, offering
seasonal, spatial, and vertical temperature context without intrinsic
restrictions on feature selection.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [329] [GyroSwin: 5D Surrogates for Gyrokinetic Plasma Turbulence Simulations](https://arxiv.org/abs/2510.07314)
*Fabian Paischer,Gianluca Galletti,William Hornsby,Paul Setinek,Lorenzo Zanisi,Naomi Carey,Stanislas Pamela,Johannes Brandstetter*

Main category: physics.plasm-ph

TL;DR: 提出首个可扩展的5D神经替代模型GyroSwin用于模拟5D非线性回旋动理学，性能优于传统降阶数值方法，降低计算成本，展示出良好扩展定律。


<details>
  <summary>Details</summary>
Motivation: 核聚变中，理解等离子体湍流是实现可行聚变动力的主要障碍，传统降阶模型忽略了全5D动力学的非线性效应，需要新模型。

Method: 将分层视觉变压器扩展到5D，引入交叉注意力和集成模块进行潜在3D与5D交互，受非线性物理启发进行通道模式分离。

Result: GyroSwin在热通量预测上优于常用降阶数值方法，捕捉到湍流能量级联，将全解析非线性回旋动理学成本降低三个数量级，且具有物理可验证性。

Conclusion: GyroSwin为等离子体湍流的回旋动理学模拟的可扩展神经替代模型铺平了道路。

Abstract: Nuclear fusion plays a pivotal role in the quest for reliable and sustainable
energy production. A major roadblock to viable fusion power is understanding
plasma turbulence, which significantly impairs plasma confinement, and is vital
for next-generation reactor design. Plasma turbulence is governed by the
nonlinear gyrokinetic equation, which evolves a 5D distribution function over
time. Due to its high computational cost, reduced-order models are often
employed in practice to approximate turbulent transport of energy. However,
they omit nonlinear effects unique to the full 5D dynamics. To tackle this, we
introduce GyroSwin, the first scalable 5D neural surrogate that can model 5D
nonlinear gyrokinetic simulations, thereby capturing the physical phenomena
neglected by reduced models, while providing accurate estimates of turbulent
heat transport.GyroSwin (i) extends hierarchical Vision Transformers to 5D,
(ii) introduces cross-attention and integration modules for latent
3D$\leftrightarrow$5D interactions between electrostatic potential fields and
the distribution function, and (iii) performs channelwise mode separation
inspired by nonlinear physics. We demonstrate that GyroSwin outperforms widely
used reduced numerics on heat flux prediction, captures the turbulent energy
cascade, and reduces the cost of fully resolved nonlinear gyrokinetics by three
orders of magnitude while remaining physically verifiable. GyroSwin shows
promising scaling laws, tested up to one billion parameters, paving the way for
scalable neural surrogates for gyrokinetic simulations of plasma turbulence.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [330] [Distributional welfare impacts and compensatory transit strategies under NYC congestion pricing](https://arxiv.org/abs/2510.06416)
*Xiyuan Ren,Zhenglei Ji,Joseph Y. J. Chow*

Main category: econ.GN

TL;DR: 研究纽约市拥堵收费计划的分配影响及补偿策略，发现计划虽有收益但存在福利损失差异，提出不同补偿方式。


<details>
  <summary>Details</summary>
Motivation: 纽约市拥堵收费计划的分配影响及补偿策略研究不足，需识别受影响群体和地区并评估补偿方式。

Method: 使用纽约和新泽西的合成出行数据估计联合模式和目的地模型，用MTA交通计数校准收费参数。

Result: 计划每年造成约2.4亿美元可达性福利损失，但低于收费收益；福利损失集中在部分地区和人群；不同人群补偿方式和成本不同。

Conclusion: 针对不同地区和人群需采用不同补偿策略，如调整公交等待时间、提供票价补贴等。

Abstract: Early evaluations of NYC's congestion pricing program indicate overall
improvements in vehicle speed and transit ridership. However, its
distributional impacts remain understudied, as does the design of compensatory
transit strategies to mitigate potential welfare losses. This study identifies
population segments and regions most affected by congestion pricing, and
evaluates how welfare losses can be compensated through transit improvements.
We estimate joint mode and destination models using aggregated synthetic trips
in New York and New Jersey and calibrate toll-related parameters with traffic
counts reported by the MTA. The results show that the program leads to an
accessibility-related welfare loss of approximately $240 million per year,
which is considerably lower than the gains from toll revenues: the gross
revenue estimated by our models ($1.077 billion per year) and the net revenue
projected by the MTA ($450 million per year). However, these benefits gains
conceal significant disparities. Welfare losses are concentrated in Upper
Manhattan, Brooklyn, and Hudson County, NJ, particularly among travelers less
able to shift to transit or alternative destinations. For NYC residents,
compensating aggregate welfare loss requires a 0.48-minute reduction in transit
wait time or a $135.59 million annual fare subsidy. Ensuring accessibility
gains for all populations and counties (Pareto improving) requires a 1-2 minute
reduction in wait time combined with an annual subsidy of about $100-300
million. For New Jersey residents, achieving aggregate welfare gains primarily
through fare discounts (requiring $108.53 million per year) is more feasible
and efficient; however, uniform discounts should be replaced by targeted
mechanisms such as origin-based fare reductions or commuter pass bundles.

</details>


### [331] [When Machines Meet Each Other: Network Effects and the Strategic Role of History in Multi-Agent AI](https://arxiv.org/abs/2510.06903)
*Yu Liu,Wenwen Li,Yifan Dou,Guangnan Ye*

Main category: econ.GN

TL;DR: 研究AI进入智能体时代，大语言模型在网络效应博弈中的行为，发现其与均衡有偏差，历史结构是设计杠杆，价格是偏差主因。


<details>
  <summary>Details</summary>
Motivation: 探讨在相互依赖环境中，机器智能体的行为表现，即结果不仅取决于自身选择还依赖同伴协调预期时的行为。

Method: 设计实验框架，让50个基于GPT - 5的异构智能体在不同网络效应强度、价格轨迹和决策历史长度下反复交互。

Result: 大语言模型智能体系统地偏离均衡，低价格时低估参与度，高价格时高估，且存在持续分散；简单单调历史有助于稳定协调，非单调历史放大差异和路径依赖；个体层面回归分析表明价格是偏差主因，历史调节此效应，网络效应放大情境扭曲。

Conclusion: 为网络效应下多智能体AI系统提供了首个系统证据，为实际配置此类系统提供指导。

Abstract: As artificial intelligence (AI) enters the agentic era, large language models
(LLMs) are increasingly deployed as autonomous agents that interact with one
another rather than operate in isolation. This shift raises a fundamental
question: how do machine agents behave in interdependent environments where
outcomes depend not only on their own choices but also on the coordinated
expectations of peers? To address this question, we study LLM agents in a
canonical network-effect game, where economic theory predicts convergence to a
fulfilled expectation equilibrium (FEE). We design an experimental framework in
which 50 heterogeneous GPT-5-based agents repeatedly interact under
systematically varied network-effect strengths, price trajectories, and
decision-history lengths. The results reveal that LLM agents systematically
diverge from FEE: they underestimate participation at low prices, overestimate
at high prices, and sustain persistent dispersion. Crucially, the way history
is structured emerges as a design lever. Simple monotonic histories-where past
outcomes follow a steady upward or downward trend-help stabilize coordination,
whereas nonmonotonic histories amplify divergence and path dependence.
Regression analyses at the individual level further show that price is the
dominant driver of deviation, history moderates this effect, and network
effects amplify contextual distortions. Together, these findings advance
machine behavior research by providing the first systematic evidence on
multi-agent AI systems under network effects and offer guidance for configuring
such systems in practice.

</details>


### [332] [The importance of emotional intelligence in leadership for building an effective team](https://arxiv.org/abs/2510.07004)
*Joanna Ćwiąkała,Waldemar Gajda,Michał Ćwiąkała,Ernest Górka,Dariusz Baran,Gabriela Wojak,Piotr Mrzygłód,Maciej Frasunkiewicz,Piotr Ręczajski,Jan Piwnik*

Main category: econ.GN

TL;DR: 研究调查情商对有效领导和团队建设的影响，发现情商与关键领导特质强相关，为领导力发展项目提供见解。


<details>
  <summary>Details</summary>
Motivation: 探究情商作为有效领导的基本组成部分的重要性及其对构建团队的影响。

Method: 对100名专业人士进行调查，研究情商能力如何塑造领导有效性等方面。

Result: 情商与关键领导特质强相关，高情商领导能改善团队动态和组织绩效，伦理领导增强动力，社交能力对团队目标对齐至关重要。

Conclusion: 研究虽因样本有限具探索性，但强调情商是人际有效性等的核心驱动力，应将情商培训等工具融入组织战略。

Abstract: This study investigates the significance of emotional intelligence (EI) as a
fundamental component of effective leadership and its impact on building
cohesive, motivated, and high-performing teams. Drawing on data from a survey
of 100 professionals, the research examines how EI competencies including
self-awareness, self-regulation, empathy, and social skills shape leadership
effectiveness, team collaboration, conflict resolution, and workplace
motivation. The results demonstrate strong correlations between EI and key
leadership traits such as empathy, ethical conduct, social competence, and
motivational effectiveness. Leaders with higher levels of EI are perceived as
more empathetic, ethical, and capable of fostering trust, resolving conflicts,
and inspiring commitment, thereby improving team dynamics and overall
organizational performance. The study also highlights that ethical leadership
significantly enhances motivation and that social competence is essential for
engaging and aligning teams toward common goals. While the findings are
exploratory due to the limited sample size, they provide valuable insights for
leadership development programs, emphasizing the importance of integrating
EI-focused training, coaching, and assessment tools into organizational
strategies. The research contributes to leadership theory by demonstrating that
emotional intelligence is not an isolated skill but a central driver of
interpersonal effectiveness, employee engagement, and sustainable business
success.

</details>


### [333] [The role of communication in effective business management](https://arxiv.org/abs/2510.07016)
*Dariusz Baran,Ernest Górka,Michał Ćwiąkała,Gabriela Wojak,Mateusz Grzelak,Katarzyna Olszyńska,Piotr Mrzygłód,Maciej Frasunkiewicz,Piotr Ręczajski,Maciej Ślusarczyk,Jan Piwnik*

Main category: econ.GN

TL;DR: 本文通过对比波兰两家汽车租赁公司，研究内部沟通对企业管理的影响，发现公司X表现更优，强调双向沟通战略作用并指出研究局限和未来方向。


<details>
  <summary>Details</summary>
Motivation: 研究内部沟通对有效企业管理的影响。

Method: 对波兰两家中型汽车租赁公司进行对比分析，通过220名员工的结构化调查评估15个与沟通相关的因素。

Result: 公司X在所有评估维度上显著优于公司Y，因其使用先进通信技术、参与式模式和明确反馈机制。

Conclusion: 双向沟通对促进员工参与、组织透明度和运营效率有战略作用，研究提供了行业内数据驱动对比，支持现有将内部沟通与工作满意度和动机联系起来的模型，同时指出研究局限并建议未来研究跨部门和纵向视角。

Abstract: This paper examines the impact of internal communication on effective
business management through a comparative analysis of two medium-sized car
rental companies operating in Poland. Using a structured survey completed by
220 employees, the study evaluates 15 communication-related factors, including
feedback culture, managerial accessibility, message clarity, and
interdepartmental coordination. The findings indicate that Company X
significantly outperforms Company Y across all evaluated dimensions, largely
due to its use of advanced communication technologies, participatory models,
and clear feedback mechanisms. The research highlights the strategic role of
two-way communication in fostering employee engagement, organizational
transparency, and operational efficiency. It contributes to the field by
offering a rare, data-driven comparison within one industry and supports
existing models that link internal communication to job satisfaction and
motivation. Limitations include reliance on self-reported data and focus on a
single industry and country. Future studies are recommended to explore
cross-sector and longitudinal perspectives, especially in the context of
digital and hybrid work environments.

</details>


### [334] [Optimal bidding in multiperiod day-ahead electricity markets assuming non-uniform uncertainty of clearing prices](https://arxiv.org/abs/2510.07025)
*Dávid Csercsik,Mihály András Vághy*

Main category: econ.GN

TL;DR: 研究假设市场清算价格为非均匀分布时，Richstein等人关于多部分投标能带来更高预期利润的结论是否成立，结果表明结论仍成立。


<details>
  <summary>Details</summary>
Motivation: Richstein等人分析假设市场清算价格为均匀分布，本文研究假设为非均匀分布时其结论是否受影响。

Method: 研究假设市场清算价格为简单、对称、阶梯常数但非均匀分布的情况。

Result: Richstein等人的结果在非均匀分布情况下仍然成立。

Conclusion: 即使市场清算价格为非均匀分布，多部分投标相比简单投标和块投标仍能确保投标者获得更高预期利润。

Abstract: In a recent publication, using a simple two-period model, which is already
capable to capture essential non-convex multiperiod bids, Richstein et al. have
shown that in the case of optimal bidding, multi-part bidding always ensures a
higher expected profit for the bidder, compared to simple bidding and
block-bidding. The model proposed in their analysis assumes a uniform
distribution of the market-clearing prices in both periods. In this paper, we
study how the conclusions of the analysis are affected, if a very simple,
symmetric, stepwise-constant but non-uniform distribution is assumed in the
case of the market-clearing price. We show that the results of Richstein et al.
also hold in this case.

</details>


### [335] [Exchange for Growth: Currency Dynamics in Emerging Markets](https://arxiv.org/abs/2510.07039)
*Shaunak Kulkarni,Rohan Ajay Dubey*

Main category: econ.GN

TL;DR: 文章探讨货币危机与双赤字假说，认为应结合当代货币经济学理论完善双赤字假说，研究实际经济因素与财政政策自主性的联系。


<details>
  <summary>Details</summary>
Motivation: 双赤字假说自提出后引发学术争论，其假设及实证证据存在冲突，需新方法调和批评与广泛适用性。

Method: 采用结合当代货币经济学理论框架的更细致方法。

Result: 研究实际经济因素与财政政策自主性的联系，以评估政府在不损害贸易政策自主性和经常账户可持续性的前提下实现国内政策目标的能力。

Conclusion: 未明确给出，但暗示结合新理论框架对双赤字假说研究有积极意义。

Abstract: Currency crises are frequently discussed retrospectively as a necessary and
deterministic outcome of a finite sequence of fiscal decisions, monetary
manoeuvres, and limited exogenous inputs. Parallelly, the Twin Deficits
Hypothesis (TDH) posits that an increase in the budget deficit leads to a
direct rise in the current account deficit; although analogous to the idea of
currency crises being the outcome of finite inputs (through a balance of
payments crisis here), this notion runs contrary to the conclusion that
independent intervention can have bearing on the expression of a currency
crisis.
  Since its introduction by Mundell and Fleming in 1960, the TDH has sparked
considerable academic debate regarding its validity. Given its assumption of a
stable private savings gap, and conflicting empirical evidence, we believe
there are novel insights to be gained from a more nuanced approach
incorporating theoretical frameworks from contemporary Monetary Economics in
order to reconcile criticisms of the TDH with the basis for its broad
applicability.
  The results from this paper thus investigate the link - or lack thereof -
between real economic factors, which can support the assessment of fiscal
policy autonomy, and thereby a government's ability, to meet domestic policy
objectives without compromising trade policy autonomy and current account
sustainability.

</details>


### [336] [Analysis of managerial behaviors in business management](https://arxiv.org/abs/2510.07047)
*Ernest Górka,Dariusz Baran,Michał Ćwiąkała,Gabriela Wojak,Robert Marszczuk,Katarzyna Olszyńska,Piotr Mrzygłód,Maciej Frasunkiewicz,Piotr Ręczajski,Kamil Saługa,Maciej Ślusarczyk,Jan Piwnik*

Main category: econ.GN

TL;DR: 研究用情境领导模型探讨管理行为对团队和组织的影响，发现支持型风格占优，强调灵活领导重要性，有理论和实践贡献但有局限。


<details>
  <summary>Details</summary>
Motivation: 探索不同管理行为如何影响团队有效性和组织成果。

Method: 以肯尼斯·布兰查德的情境领导模型为诊断工具，在十家公司通过情景问卷评估领导适应性。

Result: 支持型风格在60%受访公司占主导，授权型第二；某些风格间有强负相关。

Conclusion: 过度依赖单一风格会导致低效，平衡的情境方法能提升决策、士气和适应性；为领导理论做贡献，对管理培训有实践意义，研究有局限并提出未来研究方向。

Abstract: This study explores how different managerial behaviors influence team
effectiveness and organizational outcomes, using Kenneth Blanchard's
situational leadership model as a diagnostic tool. Conducted across ten
companies, the research evaluates leadership adaptability through a
scenario-based questionnaire identifying instructional, teaching, supportive,
and delegating styles. Results show that the supportive (affiliative) style is
dominant, present in 60 percent of surveyed companies, with delegating being
second. Correlation analysis reveals strong negative relationships between
certain styles, particularly instructional and supportive, indicating that
flexibility in leadership is crucial. The findings suggest that over-reliance
on any one style may lead to inefficiencies, while a balanced, situational
approach enhances decision-making, morale, and adaptability. The research
contributes to leadership theory by demonstrating how behavioral combinations,
not static traits, influence outcomes. It offers practical implications for
managerial training, recommending the integration of diagnostic tools like the
Blanchard test to improve style awareness and behavioral flexibility.
Limitations include reliance on self-assessment data and a small sample size.
Future research should explore longitudinal and cross-industry analyses to
assess how leadership behaviors evolve over time or under pressure.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [337] [Active Control of Turbulent Airfoil Flows Using Adjoint-based Deep Learning](https://arxiv.org/abs/2510.07106)
*Xuemin Liu,Tom Hickling,Jonathan F. MacArt*

Main category: physics.flu-dyn

TL;DR: 采用深度学习PDE增强方法训练主动神经网络流动控制器，优化湍流翼型流的升阻比，结果显示能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 优化湍流翼型流在特定雷诺数和马赫数下的升阻比。

Method: 用直接数值模拟和大涡模拟建模，通过神经网络自适应确定控制动作，用伴随Navier - Stokes方程计算灵敏度。

Result: 训练的流动控制器显著提高升阻比、减少流动分离，2D训练模型用于3D流有效，3D训练模型更能捕捉流动动态。

Conclusion: 基于学习的方法在改善空气动力学性能方面有效。

Abstract: We train active neural-network flow controllers using a deep learning PDE
augmentation method to optimize lift-to-drag ratios in turbulent airfoil flows
at Reynolds number $5\times10^4$ and Mach number 0.4. Direct numerical
simulation and large eddy simulation are employed to model compressible,
unconfined flow over two- and three-dimensional semi-infinite NACA 0012
airfoils at angles of attack $\alpha = 5^\circ$, $10^\circ$, and $15^\circ$.
Control actions, implemented through a blowing/suction jet at a fixed location
and geometry on the upper surface, are adaptively determined by a neural
network that maps local pressure measurements to optimal jet total pressure,
enabling a sensor-informed control policy that responds spatially and
temporally to unsteady flow conditions. The sensitivities of the flow to the
neural network parameters are computed using the adjoint Navier-Stokes
equations, which we construct using automatic differentiation applied to the
flow solver. The trained flow controllers significantly improve the
lift-to-drag ratios and reduce flow separation for both two- and
three-dimensional airfoil flows, especially at $\alpha = 5^\circ$ and
$10^\circ$. The 2D-trained models remain effective when applied out-of-sample
to 3D flows, which demonstrates the robustness of the adjoint-trained control
approach. The 3D-trained models capture the flow dynamics even more
effectively, which leads to better energy efficiency and comparable performance
for both adaptive (neural network) and offline (simplified, constant-pressure)
controllers. These results underscore the effectiveness of this learning-based
approach in improving aerodynamic performance.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [338] [Soft-Evidence Fused Graph Neural Network for Cancer Driver Gene Identification across Multi-View Biological Graphs](https://arxiv.org/abs/2510.06290)
*Bang Chen,Lijun Guo,Houli Fan,Wentao He,Rong Zhang*

Main category: q-bio.GN

TL;DR: 提出SEFGNN框架用于跨多网络识别癌症驱动基因，在决策层融合多网络信息，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的癌症驱动基因识别方法多依赖单一PPI网络或在表征层面融合多网络信息，存在忽视网络异质性和引入冲突信息的问题。

Method: 提出SEFGNN框架，将每个生物网络视为独立证据源，在决策层使用Dempster - Shafer理论进行不确定性感知融合，并引入Soft Evidence Smoothing模块。

Result: 在三个癌症数据集上的实验表明，SEFGNN始终优于现有基线方法。

Conclusion: SEFGNN在识别癌症驱动基因方面有强大潜力，能发现新的癌症驱动基因。

Abstract: Identifying cancer driver genes (CDGs) is essential for understanding cancer
mechanisms and developing targeted therapies. Graph neural networks (GNNs) have
recently been employed to identify CDGs by capturing patterns in biological
interaction networks. However, most GNN-based approaches rely on a single
protein-protein interaction (PPI) network, ignoring complementary information
from other biological networks. Some studies integrate multiple networks by
aligning features with consistency constraints to learn unified gene
representations for CDG identification. However, such representation-level
fusion often assumes congruent gene relationships across networks, which may
overlook network heterogeneity and introduce conflicting information. To
address this, we propose Soft-Evidence Fusion Graph Neural Network (SEFGNN), a
novel framework for CDG identification across multiple networks at the decision
level. Instead of enforcing feature-level consistency, SEFGNN treats each
biological network as an independent evidence source and performs
uncertainty-aware fusion at the decision level using Dempster-Shafer Theory
(DST). To alleviate the risk of overconfidence from DST, we further introduce a
Soft Evidence Smoothing (SES) module that improves ranking stability while
preserving discriminative performance. Experiments on three cancer datasets
show that SEFGNN consistently outperforms state-of-the-art baselines and
exhibits strong potential in discovering novel CDGs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [339] [On-Package Memory with Universal Chiplet Interconnect Express (UCIe): A Low Power, High Bandwidth, Low Latency and Low Cost Approach](https://arxiv.org/abs/2510.06513)
*Debendra Das Sharma,Swadesh Choudhary,Peter Onufryk,Rob Pelt*

Main category: cs.AR

TL;DR: 现有封装内存方案无法满足新兴计算应用需求，提出增强UCIe的内存语义的方案，效果显著。


<details>
  <summary>Details</summary>
Motivation: 新兴计算应用面临内存墙问题，现有封装内存方案无法满足节能带宽需求。

Method: 提出通过逻辑芯片复用现有LPDDR6和HBM内存，以及让DRAM芯片原生支持UCIe而非LPDDR6总线接口的方法。

Result: 与现有HBM4和LPDDR封装内存方案相比，实现了更高带宽密度（高达10倍）、更低延迟（高达3倍）、更低功耗（高达3倍）和更低成本。

Conclusion: 增强UCIe的内存语义可提供节能带宽和经济有效的封装内存解决方案，适用于整个计算领域。

Abstract: Emerging computing applications such as Artificial Intelligence (AI) are
facing a memory wall with existing on-package memory solutions that are unable
to meet the power-efficient bandwidth demands. We propose to enhance UCIe with
memory semantics to deliver power-efficient bandwidth and cost-effective
on-package memory solutions applicable across the entire computing continuum.
We propose approaches by reusing existing LPDDR6 and HBM memory through a logic
die that connects to the SoC using UCIe. We also propose an approach where the
DRAM die natively supports UCIe instead of the LPDDR6 bus interface. Our
approaches result in significantly higher bandwidth density (up to 10x), lower
latency (up to 3x), lower power (up to 3x), and lower cost compared to existing
HBM4 and LPDDR on-package memory solutions.

</details>


### [340] [Cocoon: A System Architecture for Differentially Private Training with Correlated Noises](https://arxiv.org/abs/2510.07304)
*Donghwan Kim,Xin Gu,Jinho Baek,Timothy Lo,Younghoon Min,Kwangsik Shin,Jongryool Kim,Jongse Park,Kiwan Maeng*

Main category: cs.AR

TL;DR: 研究带相关噪声训练机制开销，提出硬件 - 软件协同设计框架Cocoon加速训练，在实际系统上提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有带差分隐私的训练算法DP - SGD会降低模型准确率，新的添加相关噪声的方法在大模型或使用大嵌入表时有不可忽略的开销。

Method: 提出Cocoon框架，通过Cocoon - Emb以合并格式预计算和存储相关噪声加速带嵌入表的模型，通过Cocoon - NMP使用定制近内存处理设备支持大模型。

Result: 在基于FPGA的NMP设备原型的真实系统上，Cocoon - Emb性能提升2.33 - 10.82倍，Cocoon - NMP提升1.55 - 3.06倍。

Conclusion: Cocoon框架能有效提升添加相关噪声训练的效率。

Abstract: Machine learning (ML) models memorize and leak training data, causing serious
privacy issues to data owners. Training algorithms with differential privacy
(DP), such as DP-SGD, have been gaining attention as a solution. However,
DP-SGD adds a noise at each training iteration, which degrades the accuracy of
the trained model. To improve accuracy, a new family of approaches adds
carefully designed correlated noises, so that noises cancel out each other
across iterations. We performed an extensive characterization study of these
new mechanisms, for the first time to the best of our knowledge, and show they
incur non-negligible overheads when the model is large or uses large embedding
tables. Motivated by the analysis, we propose Cocoon, a hardware-software
co-designed framework for efficient training with correlated noises. Cocoon
accelerates models with embedding tables through pre-computing and storing
correlated noises in a coalesced format (Cocoon-Emb), and supports large models
through a custom near-memory processing device (Cocoon-NMP). On a real system
with an FPGA-based NMP device prototype, Cocoon improves the performance by
2.33-10.82x(Cocoon-Emb) and 1.55-3.06x (Cocoon-NMP).

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [341] [Bayesian Portfolio Optimization by Predictive Synthesis](https://arxiv.org/abs/2510.07180)
*Masahiro Kato,Kentaro Baba,Hibiki Kaibuchi,Ryo Inokuchi*

Main category: econ.EM

TL;DR: 研究基于贝叶斯预测合成（BPS）的投资组合优化方法，结合多模型预测获取考虑市场不确定性的资产均值回报后验分布，并探讨构建均值 - 方差和分位数投资组合。


<details>
  <summary>Details</summary>
Motivation: 现有投资组合优化方法依赖资产回报分布信息，而此信息通常未知，且估计方法受金融市场不确定性影响，不同时间模型表现差异大。

Method: 采用贝叶斯预测合成（BPS）与动态线性模型结合多个资产回报预测模型，获取资产均值回报的贝叶斯预测后验。

Result: 文中未提及具体结果。

Conclusion: 文中未提及具体结论。

Abstract: Portfolio optimization is a critical task in investment. Most existing
portfolio optimization methods require information on the distribution of
returns of the assets that make up the portfolio. However, such distribution
information is usually unknown to investors. Various methods have been proposed
to estimate distribution information, but their accuracy greatly depends on the
uncertainty of the financial markets. Due to this uncertainty, a model that
could well predict the distribution information at one point in time may
perform less accurately compared to another model at a different time. To solve
this problem, we investigate a method for portfolio optimization based on
Bayesian predictive synthesis (BPS), one of the Bayesian ensemble methods for
meta-learning. We assume that investors have access to multiple asset return
prediction models. By using BPS with dynamic linear models to combine these
predictions, we can obtain a Bayesian predictive posterior about the mean
rewards of assets that accommodate the uncertainty of the financial markets. In
this study, we examine how to construct mean-variance portfolios and
quantile-based portfolios based on the predicted distribution information.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [342] [Early Results from Teaching Modelling for Software Comprehension in New-Hire Onboarding](https://arxiv.org/abs/2510.07010)
*Mrityunjay Kumar,Venkatesh Choppella*

Main category: cs.CY

TL;DR: 介绍SaaS公司入职培训项目中试点干预课程的早期结果，该课程或可加速经验不足新员工对系统的理解。


<details>
  <summary>Details</summary>
Motivation: 多数毕业生应对大型现有软件系统理解挑战的准备不足，需提升其相关能力。

Method: 在入职培训中加入五节介绍系统思维和LTS建模的课程，让学员用模板表达理解并进行前后测。

Result: 整体成绩提升不显著；前测低于中位数的学员平均提高15个百分点，高于中位数的学员略有退步；课程反馈显示参与度和适用性高。

Conclusion: 短期建模干预课程可加速经验不足新员工的理解，强学员需差异化路径，公司可大规模采用此类干预作为入职培训补充。

Abstract: Working effectively with large, existing software systems requires strong
comprehension skills, yet most graduates enter the industry with little
preparation for this challenge. We report early results from a pilot
intervention integrated into a SaaS company's onboarding program: a
five-session course introducing systems thinking and Labelled Transition System
(LTS) modelling. Participants articulated their understanding of product
behaviour using a structured template and completed matched pre- and
post-assessments. Of 35 new hires, 31 provided paired records for analysis.
Across the full cohort, gains were small and not statistically significant.
However, participants below the median on the pre-test improved by 15
percentage points on average (statistically significant), while those above the
median regressed slightly (not statistically significant). Course feedback
indicated high engagement and perceived applicability. These results suggest
that short, modelling-focused onboarding interventions can accelerate
comprehension for less-prepared new hires. At the same time, they point to the
need for differentiated pathways for stronger participants, and to the
potential for companies to adopt such interventions at scale as a low-cost
complement to existing onboarding.

</details>


### [343] [LLM-Driven Rubric-Based Assessment of Algebraic Competence in Multi-Stage Block Coding Tasks with Design and Field Evaluation](https://arxiv.org/abs/2510.06253)
*Yong Oh Lee,Byeonghun Bang,Sejun Oh*

Main category: cs.CY

TL;DR: 研究提出并评估基于大语言模型（LLM）的量规评估框架用于衡量代数能力和编码任务，经实地研究验证其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着在线教育平台扩展，需要能衡量学生认知过程深度且符合课程目标的评估方法。

Method: 设计与五个预定义量规维度对齐的问题集，在在线平台实施系统，结合学习者自评和专家评分，对42名中学生进行实地研究。

Result: 基于LLM的量规评估与专家判断高度一致，能给出符合量规、注重过程的反馈。

Conclusion: 将LLM驱动的量规评估纳入在线数学和STEM教育平台具有有效性和可扩展性。

Abstract: As online education platforms continue to expand, there is a growing need for
assessment methods that not only measure answer accuracy but also capture the
depth of students' cognitive processes in alignment with curriculum objectives.
This study proposes and evaluates a rubric-based assessment framework powered
by a large language model (LLM) for measuring algebraic competence,
real-world-context block coding tasks. The problem set, designed by mathematics
education experts, aligns each problem segment with five predefined rubric
dimensions, enabling the LLM to assess both correctness and quality of
students' problem-solving processes. The system was implemented on an online
platform that records all intermediate responses and employs the LLM for
rubric-aligned achievement evaluation. To examine the practical effectiveness
of the proposed framework, we conducted a field study involving 42 middle
school students engaged in multi-stage quadratic equation tasks with block
coding. The study integrated learner self-assessments and expert ratings to
benchmark the system's outputs. The LLM-based rubric evaluation showed strong
agreement with expert judgments and consistently produced rubric-aligned,
process-oriented feedback. These results demonstrate both the validity and
scalability of incorporating LLM-driven rubric assessment into online
mathematics and STEM education platforms.

</details>


### [344] [Surgeons Are Indian Males and Speech Therapists Are White Females: Auditing Biases in Vision-Language Models for Healthcare Professionals](https://arxiv.org/abs/2510.06280)
*Zohaib Hasan Siddiqui,Dayam Nadeem,Mohammad Masudur Rahman,Mohammad Nadeem,Shahab Saquib Sohail,Beenish Moalla Chaudhry*

Main category: cs.CY

TL;DR: 本文提出医疗场景评估协议，量化视觉语言模型的关联偏差并评估操作风险，发现多角色和模型存在人口统计学偏差，强调医疗领域偏差识别的重要性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型会从网络数据中学习到医疗职业和人口属性的刻板关联，需要评估其偏差和操作风险。

Method: 定义涵盖临床和辅助医疗角色的分类法，策划专业感知提示套件探究模型行为，以平衡人脸语料为基准衡量人口统计学偏差。

Result: 在多个角色和视觉模型中观察到一致的人口统计学偏差。

Conclusion: 强调在医疗等关键领域进行偏差识别的重要性，因为人工智能招聘和劳动力分析会对公平性、合规性和患者信任产生影响。

Abstract: Vision language models (VLMs), such as CLIP and OpenCLIP, can encode and
reflect stereotypical associations between medical professions and demographic
attributes learned from web-scale data. We present an evaluation protocol for
healthcare settings that quantifies associated biases and assesses their
operational risk. Our methodology (i) defines a taxonomy spanning clinicians
and allied healthcare roles (e.g., surgeon, cardiologist, dentist, nurse,
pharmacist, technician), (ii) curates a profession-aware prompt suite to probe
model behavior, and (iii) benchmarks demographic skew against a balanced face
corpus. Empirically, we observe consistent demographic biases across multiple
roles and vision models. Our work highlights the importance of bias
identification in critical domains such as healthcare as AI-enabled hiring and
workforce analytics can have downstream implications for equity, compliance,
and patient trust.

</details>


### [345] [Asking For It: Question-Answering for Predicting Rule Infractions in Online Content Moderation](https://arxiv.org/abs/2510.06350)
*Mattia Samory,Diana Pamfile,Andrew To,Shruti Phadke*

Main category: cs.CY

TL;DR: 本文提出ModQ框架用于规则敏感的内容审核，在识别违规规则上表现优于基准模型，且能有效泛化。


<details>
  <summary>Details</summary>
Motivation: 在线社区规则差异大、会演变且执行不一致，给透明度、治理和自动化带来挑战。

Method: 引入ModQ问答框架，实现提取式和多项选择式QA两种模型变体，并在Reddit和Lemmy数据集上训练。

Result: 两个模型在识别违规规则上优于现有基准模型，轻量级且可解释。

Conclusion: ModQ模型能有效泛化到未见社区和规则，支持低资源审核和动态治理环境。

Abstract: Online communities rely on a mix of platform policies and community-authored
rules to define acceptable behavior and maintain order. However, these rules
vary widely across communities, evolve over time, and are enforced
inconsistently, posing challenges for transparency, governance, and automation.
In this paper, we model the relationship between rules and their enforcement at
scale, introducing ModQ, a novel question-answering framework for
rule-sensitive content moderation. Unlike prior classification or
generation-based approaches, ModQ conditions on the full set of community rules
at inference time and identifies which rule best applies to a given comment. We
implement two model variants - extractive and multiple-choice QA - and train
them on large-scale datasets from Reddit and Lemmy, the latter of which we
construct from publicly available moderation logs and rule descriptions. Both
models outperform state-of-the-art baselines in identifying moderation-relevant
rule violations, while remaining lightweight and interpretable. Notably, ModQ
models generalize effectively to unseen communities and rules, supporting
low-resource moderation settings and dynamic governance environments.

</details>


### [346] [Beyond Static Knowledge Messengers: Towards Adaptive, Fair, and Scalable Federated Learning for Medical AI](https://arxiv.org/abs/2510.06259)
*Jahidul Arafat,Fariha Tasmin,Sanjaya Poudel,Ahsan Habib Tareq,Iftekhar Haider*

Main category: cs.CY

TL;DR: 提出自适应公平联邦学习(AFFL)解决医疗AI隐私协作学习公平性问题，有多项创新，成果显著并提出评估套件和研究议程等。


<details>
  <summary>Details</summary>
Motivation: 解决医疗AI在隐私保护协作学习中跨异构医疗机构的公平性问题，当前联邦学习方法存在静态架构、收敛慢、公平性差和可扩展性受限等问题。

Method: 提出AFFL，包括自适应知识信使、公平感知蒸馏和课程引导加速三项创新。

Result: 实现通信减少55 - 75%、公平性提升56 - 68%、节能34 - 46%、支持超100家机构，还能多模态集成并合规。

Conclusion: 提出MedFedBench评估套件，有经济收益预测，提出研究议程、实施路线图和医疗AI民主化途径。

Abstract: Medical AI faces challenges in privacy-preserving collaborative learning
while ensuring fairness across heterogeneous healthcare institutions. Current
federated learning approaches suffer from static architectures, slow
convergence (45-73 rounds), fairness gaps marginalizing smaller institutions,
and scalability constraints (15-client limit). We propose Adaptive Fair
Federated Learning (AFFL) through three innovations: (1) Adaptive Knowledge
Messengers dynamically scaling capacity based on heterogeneity and task
complexity, (2) Fairness-Aware Distillation using influence-weighted
aggregation, and (3) Curriculum-Guided Acceleration reducing rounds by 60-70%.
Our theoretical analysis provides convergence guarantees with epsilon-fairness
bounds, achieving O(T^{-1/2}) + O(H_max/T^{3/4}) rates. Projected results show
55-75% communication reduction, 56-68% fairness improvement, 34-46% energy
savings, and 100+ institution support. The framework enables multi-modal
integration across imaging, genomics, EHR, and sensor data while maintaining
HIPAA/GDPR compliance. We propose MedFedBench benchmark suite for standardized
evaluation across six healthcare dimensions: convergence efficiency,
institutional fairness, privacy preservation, multi-modal integration,
scalability, and clinical deployment readiness. Economic projections indicate
400-800% ROI for rural hospitals and 15-25% performance gains for academic
centers. This work presents a seven-question research agenda, 24-month
implementation roadmap, and pathways toward democratizing healthcare AI.

</details>


### [347] [The Limits of Goal-Setting Theory in LLM-Driven Assessment](https://arxiv.org/abs/2510.06997)
*Mrityunjay Kumar*

Main category: cs.CY

TL;DR: 研究测试ChatGPT按不同详细程度提示语评估学生作业的一致性，发现提示语详细程度增加未使表现稳定提升，挑战大语言模型像人类评估者的假设。


<details>
  <summary>Details</summary>
Motivation: 根据目标设定理论和Model H假设，测试用更详细提示语让聊天机器人评估是否会有更一致的评估行为。

Method: 通过对照实验，让ChatGPT用四种详细程度递增的提示语评估29份学生作业，用Cohen's Kappa测量重复运行的一致性。

Result: 表现未随提示语详细程度增加而持续改善，表现差异基本不变。

Conclusion: 挑战大语言模型像人类评估者的假设，强调未来模型开发需增强鲁棒性和改进输入整合。

Abstract: Many users interact with AI tools like ChatGPT using a mental model that
treats the system as human-like, which we call Model H. According to
goal-setting theory, increased specificity in goals should reduce performance
variance. If Model H holds, then prompting a chatbot with more detailed
instructions should lead to more consistent evaluation behavior.
  This paper tests that assumption through a controlled experiment in which
ChatGPT evaluated 29 student submissions using four prompts with increasing
specificity. We measured consistency using intra-rater reliability (Cohen's
Kappa) across repeated runs.
  Contrary to expectations, performance did not improve consistently with
increased prompt specificity, and performance variance remained largely
unchanged. These findings challenge the assumption that LLMs behave like human
evaluators and highlight the need for greater robustness and improved input
integration in future model development.

</details>


### [348] [On the false election between regulation and innovation. Ideas for regulation through the responsible use of artificial intelligence in research and education.[Spanish version]](https://arxiv.org/abs/2510.07268)
*Pompeu Casanovas*

Main category: cs.CY

TL;DR: 文章是作者在AIHUB和EduCaixa暑期学校辩论环节回答的整理，尝试回答Albert Sabater提出的三个关于AI监管的问题，并对答案对教育和研究的相关性进行反思。


<details>
  <summary>Details</summary>
Motivation: 回答Albert Sabater提出的关于AI监管框架、负责任创新、国际合作等方面的问题。

Method: 对相关问题进行分析解答。

Result: 文章给出了对三个问题的回答。

Conclusion: 对答案对教育和研究的相关性进行反思。

Abstract: This short essay is a reworking of the answers offered by the author at the
Debate Session of the AIHUB (CSIC) and EduCaixa Summer School, organized by
Marta Garcia-Matos and Lissette Lemus, and coordinated by Albert Sabater
(OEIAC, UG), with the participation of Vanina Martinez-Posse (IIIA-CSIC),
Eulalia Soler (Eurecat) and Pompeu Casanovas (IIIA-CSIC) on July 4th 2025.
Albert Sabater posed three questions: (1) How can regulatory frameworks
priori-tise the protection of fundamental rights (privacy, non-discrimination,
autonomy, etc.) in the development of AI, without falling into the false
dichotomy between regulation and innova-tion? (2) Given the risks of AI (bias,
mass surveillance, manipulation), what examples of regu-lations or policies
have demonstrated that it is possible to foster responsible innovation, putting
the public interest before profitability, without giving in to competitive
pressure from actors such as China or the US? (3) In a scenario where the US
prioritizes flexibility, what mecha-nisms could ensure that international
cooperation in AI does not become a race to the bottom in rights, but rather a
global standard of accountability? The article attempts to answer these three
questions and concludes with some reflections on the relevance of the answers
for education and research.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [349] [Efficient reductions from a Gaussian source with applications to statistical-computational tradeoffs](https://arxiv.org/abs/2510.07250)
*Mengqi Lou,Guy Bresler,Ashwin Pananjady*

Main category: math.ST

TL;DR: 设计程序从目标分布近似生成观测值，用于建立高维统计模型计算下界。


<details>
  <summary>Details</summary>
Motivation: 解决在未知均值高斯分布下，为不同目标分布生成观测值及建立高维统计模型计算下界的问题。

Method: 设计计算高效的程序，利用技术建立基于归约的计算下界。

Result: 涵盖不同目标分布情况，证明尖峰张量PCA计算下界的普遍性，解决k - 稀疏相位恢复猜想，建立两模型间对应关系。

Conclusion: 所提方法有效建立了多种高维统计模型的计算下界，解决了相关猜想。

Abstract: Given a single observation from a Gaussian distribution with unknown mean
$\theta$, we design computationally efficient procedures that can approximately
generate an observation from a different target distribution $Q_{\theta}$
uniformly for all $\theta$ in a parameter set. We leverage our technique to
establish reduction-based computational lower bounds for several canonical
high-dimensional statistical models under widely-believed conjectures in
average-case complexity. In particular, we cover cases in which:
  1. $Q_{\theta}$ is a general location model with non-Gaussian distribution,
including both light-tailed examples (e.g., generalized normal distributions)
and heavy-tailed ones (e.g., Student's $t$-distributions). As a consequence, we
show that computational lower bounds proved for spiked tensor PCA with Gaussian
noise are universal, in that they extend to other non-Gaussian noise
distributions within our class.
  2. $Q_{\theta}$ is a normal distribution with mean $f(\theta)$ for a general,
smooth, and nonlinear link function $f:\mathbb{R} \rightarrow \mathbb{R}$.
Using this reduction, we construct a reduction from symmetric mixtures of
linear regressions to generalized linear models with link function $f$, and
establish computational lower bounds for solving the $k$-sparse generalized
linear model when $f$ is an even function. This result constitutes the first
reduction-based confirmation of a $k$-to-$k^2$ statistical-to-computational gap
in $k$-sparse phase retrieval, resolving a conjecture posed by Cai et al.
(2016). As a second application, we construct a reduction from the sparse
rank-1 submatrix model to the planted submatrix model, establishing a pointwise
correspondence between the phase diagrams of the two models that faithfully
preserves regions of computational hardness and tractability.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [350] [Textual interpretation of transient image classifications from large language models](https://arxiv.org/abs/2510.06931)
*Fiorenzo Stoppa,Turan Bulmus,Steven Bloemen,Stephen J. Smartt,Paul J. Groot,Paul Vreeswijk,Ken W. Smith*

Main category: astro-ph.IM

TL;DR: 研究表明大语言模型（LLMs）在光学瞬变调查数据集上分类性能接近卷积神经网络，且能给出可读描述，还可迭代优化，有望助力天文数据处理。


<details>
  <summary>Details</summary>
Motivation: 现代天文调查有大量瞬变探测数据，区分真实信号和伪像有挑战，卷积神经网络难解释。

Method: 使用 Google 的 LLM（Gemini），仅用 15 个示例和简洁指令进行分类；用第二个 LLM 评估第一个模型输出的连贯性。

Result: Gemini 在三个数据集上平均准确率达 93%；可通过第二个 LLM 迭代优化输出。

Conclusion: LLM 基分类法能绕过传统训练流程，方便用户查询，有望弥合自动检测和人类理解间的差距。

Abstract: Modern astronomical surveys deliver immense volumes of transient detections,
yet distinguishing real astrophysical signals (for example, explosive events)
from bogus imaging artefacts remains a challenge. Convolutional neural networks
are effectively used for real versus bogus classification; however, their
reliance on opaque latent representations hinders interpretability. Here we
show that large language models (LLMs) can approach the performance level of a
convolutional neural network on three optical transient survey datasets
(Pan-STARRS, MeerLICHT and ATLAS) while simultaneously producing direct,
human-readable descriptions for every candidate. Using only 15 examples and
concise instructions, Google's LLM, Gemini, achieves a 93% average accuracy
across datasets that span a range of resolution and pixel scales. We also show
that a second LLM can assess the coherence of the output of the first model,
enabling iterative refinement by identifying problematic cases. This framework
allows users to define the desired classification behaviour through natural
language and examples, bypassing traditional training pipelines. Furthermore,
by generating textual descriptions of observed features, LLMs enable users to
query classifications as if navigating an annotated catalogue, rather than
deciphering abstract latent spaces. As next-generation telescopes and surveys
further increase the amount of data available, LLM-based classification could
help bridge the gap between automated detection and transparent, human-level
understanding.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [351] [Parameterized Complexity of s-Club Cluster Edge Deletion](https://arxiv.org/abs/2510.07065)
*Ajinkya Gaikwad*

Main category: cs.DM

TL;DR: 研究s - Club Cluster Edge Deletion问题的参数化和经典复杂度，证明按路径宽度参数化时W[1] - 难，给出多种参数化下FPT结果，证明分裂图上NP - 难，设计FPT双准则近似算法。


<details>
  <summary>Details</summary>
Motivation: Montecchiani等人提出s - Club Cluster Edge Deletion问题按树宽参数化时是否为FPT的疑问，需研究该问题的复杂度。

Method: 理论证明复杂度，设计FPT双准则近似算法。

Result: 按路径宽度参数化时为W[1] - 难；按邻域多样性、孪生覆盖或簇顶点删除数参数化时为FPT；分裂图上s = 2时NP - 难；设计出FPT双准则近似算法。

Conclusion: 解决了按树宽参数化是否为FPT的疑问，扩展了已有结果，补充了经典复杂度结论，在仅按解大小k参数化的复杂度研究上取得进展。

Abstract: We study the parameterized and classical complexity of the s-Club Cluster
Edge Deletion problem: given a graph G = (V, E) and integers k and s, determine
whether it is possible to delete at most k edges so that every connected
component of the resulting graph has diameter at most s. This problem
generalizes Cluster Edge Deletion (the case s = 1) and captures a variety of
distance-bounded graph modification tasks.
  Montecchiani, Ortali, Piselli, and Tappini (Information and Computation,
2023) showed that the problem is fixed-parameter tractable when parameterized
by s plus the treewidth of G, and asked whether the dependence on s is
necessary; that is, whether the problem is FPT when parameterized by treewidth
alone. We resolve this by proving that the problem is W[1]-hard when
parameterized by pathwidth, and hence by treewidth.
  On the algorithmic side, we show that the problem is FPT when parameterized
by neighborhood diversity, twin cover, or cluster vertex deletion number,
thereby extending to all s >= 1 the results of Italiano, Konstantinidis, and
Papadopoulos (Algorithmica, 2023), who established FPT algorithms for the case
s = 1 under the neighborhood diversity and twin cover parameters.
  From a classical perspective, we prove that the problem is NP-hard on split
graphs already for s = 2, complementing the polynomial-time solvability for s =
1 due to Bonomo, Duran, and Valencia-Pabon (Theoretical Computer Science, 2015)
and the trivial case s = 3.
  Finally, while the problem is FPT when parameterized by s + k, its complexity
for the solution size k alone remains open. We make progress on this front by
designing an FPT bicriteria approximation algorithm, which runs in time f(k,
1/epsilon) * n^{O(1)} and, for graphs excluding long induced cycles, outputs a
solution of size at most k whose connected components have diameter at most (1
+ epsilon) * s.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [352] [Dream2Image : An Open Multimodal EEG Dataset for Decoding and Visualizing Dreams with Artificial Intelligence](https://arxiv.org/abs/2510.06252)
*Yann Bellec*

Main category: q-bio.NC

TL;DR: 介绍了世界首个结合EEG信号、梦境转录和AI生成图像的数据集Dream2Image，包括其构成、用途、开放获取情况及局限性。


<details>
  <summary>Details</summary>
Motivation: 为梦境研究提供新资源，研究梦境神经关联，开发梦境解码模型，探索跨学科新方法，激发研究并拓展脑活动解码途径。

Method: 基于38名参与者超31小时梦境EEG记录构建含129个样本的数据集。

Result: 创建Dream2Image数据集，可在Hugging Face和GitHub上开放获取。

Conclusion: Dream2Image为人工智能和神经科学交叉研究提供多模态资源，但存在样本量小和梦境回忆差异影响泛化性的局限。

Abstract: Dream2Image is the world's first dataset combining EEG signals, dream
transcriptions, and AI-generated images. Based on 38 participants and more than
31 hours of dream EEG recordings, it contains 129 samples offering: the final
seconds of brain activity preceding awakening (T-15, T-30, T-60, T-120), raw
reports of dream experiences, and an approximate visual reconstruction of the
dream. This dataset provides a novel resource for dream research, a unique
resource to study the neural correlates of dreaming, to develop models for
decoding dreams from brain activity, and to explore new approaches in
neuroscience, psychology, and artificial intelligence. Available in open access
on Hugging Face and GitHub, Dream2Image provides a multimodal resource designed
to support research at the interface of artificial intelligence and
neuroscience. It was designed to inspire researchers and extend the current
approaches to brain activity decoding. Limitations include the relatively small
sample size and the variability of dream recall, which may affect
generalizability.

</details>


### [353] [Diffusion-Guided Renormalization of Neural Systems via Tensor Networks](https://arxiv.org/abs/2510.06361)
*Nathan X. Kodama*

Main category: q-bio.NC

TL;DR: 本文探索基于扩散的神经系统重整化，开发可扩展算法，连接微观和中观尺度神经动力。


<details>
  <summary>Details</summary>
Motivation: 神经科学和人工智能需计算框架对随机神经轨迹的非平衡动力学建模，现有方法缺乏可扩展的数据驱动技术，粗粒化复杂动态网络问题未解决。

Method: 利用基于扩散的重整化，生成跨尺度破对称表示，使用张量网络提供可扩展算法；开发可扩展图推理算法发现社区结构，通过元图和联合概率函数生成重整化群流。

Result: 基于扩散的重整化连接了耗散神经系统的微观和中观尺度动力学。

Conclusion: 基于扩散的重整化能实现系统神经科学和人工智能中的粗到细控制。

Abstract: Far from equilibrium, neural systems self-organize across multiple scales.
Exploiting multiscale self-organization in neuroscience and artificial
intelligence requires a computational framework for modeling the effective
non-equilibrium dynamics of stochastic neural trajectories. Non-equilibrium
thermodynamics and representational geometry offer theoretical foundations, but
we need scalable data-driven techniques for modeling collective properties of
high-dimensional neural networks from partial subsampled observations.
Renormalization is a coarse-graining technique central to studying emergent
scaling properties of many-body and nonlinear dynamical systems. While widely
applied in physics and machine learning, coarse-graining complex dynamical
networks remains unsolved, affecting many computational sciences. Recent
diffusion-based renormalization, inspired by quantum statistical mechanics,
coarse-grains networks near entropy transitions marked by maximal changes in
specific heat or information transmission. Here I explore diffusion-based
renormalization of neural systems by generating symmetry-breaking
representations across scales and offering scalable algorithms using tensor
networks. Diffusion-guided renormalization bridges microscale and mesoscale
dynamics of dissipative neural systems. For microscales, I developed a scalable
graph inference algorithm for discovering community structure from subsampled
neural activity. Using community-based node orderings, diffusion-guided
renormalization generates renormalization group flow through metagraphs and
joint probability functions. Towards mesoscales, diffusion-guided
renormalization targets learning the effective non-equilibrium dynamics of
dissipative neural trajectories occupying lower-dimensional subspaces, enabling
coarse-to-fine control in systems neuroscience and artificial intelligence.

</details>
