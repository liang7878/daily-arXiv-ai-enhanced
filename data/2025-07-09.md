<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 41]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 7]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 16]
- [cs.LG](#cs.LG) [Total: 72]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.SE](#cs.SE) [Total: 16]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [stat.CO](#stat.CO) [Total: 1]
- [hep-th](#hep-th) [Total: 1]
- [math.ST](#math.ST) [Total: 2]
- [econ.GN](#econ.GN) [Total: 5]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [math.OC](#math.OC) [Total: 2]
- [stat.ME](#stat.ME) [Total: 2]
- [econ.TH](#econ.TH) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CR](#cs.CR) [Total: 11]
- [cs.CV](#cs.CV) [Total: 22]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.CC](#cs.CC) [Total: 4]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [math.NA](#math.NA) [Total: 3]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.CY](#cs.CY) [Total: 11]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.CL](#cs.CL) [Total: 29]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.GR](#cs.GR) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 6]
- [quant-ph](#quant-ph) [Total: 3]
- [q-bio.MN](#q-bio.MN) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Strongly Solving $7 \times 6$ Connect-Four on Consumer Grade Hardware](https://arxiv.org/abs/2507.05267)
*Markus Böck*

Main category: cs.AI

TL;DR: 本文用基于二元决策图的符号搜索方法为四子棋生成了强解查找表，还在开源代码中加入alpha - beta搜索找最快胜或最慢败走法。


<details>
  <summary>Details</summary>
Motivation: 此前认为用查找表形式为四子棋生成强解不可行，作者希望解决此问题。

Method: 采用基于二元决策图的符号搜索方法，并进行高效实现；在开源代码中加入alpha - beta搜索。

Result: 在单CPU核心、128GB主内存下，47小时生成了89.6GB的标准7×6棋盘大小的查找表。

Conclusion: 能用基于二元决策图的符号搜索方法为四子棋生成强解查找表，且开源代码可找最快胜或最慢败走法。

Abstract: While the game Connect-Four has been solved mathematically and the best move
can be effectively computed with search based methods, a strong solution in the
form of a look-up table was believed to be infeasible. In this paper, we
revisit a symbolic search method based on binary decision diagrams to produce
strong solutions. With our efficient implementation we were able to produce a
89.6 GB large look-up table in 47 hours on a single CPU core with 128 GB main
memory for the standard $7 \times 6$ board size. In addition to this
win-draw-loss evaluation, we include an alpha-beta search in our open source
artifact to find the move which achieves the fastest win or slowest loss.

</details>


### [2] [Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management](https://arxiv.org/abs/2507.05283)
*Yue Wang,Miao Zhou,Guijing Huang,Rui Zhuo,Chao Yi,Zhenliang Ma*

Main category: cs.AI

TL;DR: 本文提出Chat2SPaT方法将用户对信号控制计划的描述转换为精确的信号相位和时序结果，实验显示准确率超94%，为交通从业者和研究者提供易用的计划管理流程。


<details>
  <summary>Details</summary>
Motivation: 传统定时交通信号控制创建和更新信号计划需繁琐人工工作，利用时段或工作日计划时还会有重复人工输入参数问题，为实现用户友好的交通信号控制计划管理流程。

Method: Chat2SPaT利用大语言模型理解用户计划描述，将计划重新表述为json格式的相位序列和相位属性结果组合，基于LLM输出设计Python脚本定位周期中的相位、处理交通信号控制细节并组装完整计划，可在聊天中迭代编辑计划。

Result: 使用超300个计划描述的测试数据集，Chat2SPaT在中英文案例中生成计划的准确率超94%。

Conclusion: Chat2SPaT作为评估大语言模型理解交通信号控制计划描述能力的首个基准，为交通从业者和研究者提供易用计划管理流程，有望成为大语言模型在智能交通系统领域更准确、通用应用的新基石。

Abstract: Pre-timed traffic signal control, commonly used for operating signalized
intersections and coordinated arterials, requires tedious manual work for
signaling plan creating and updating. When the time-of-day or day-of-week plans
are utilized, one intersection is often associated with multiple plans, leading
to further repetitive manual plan parameter inputting. To enable a
user-friendly traffic signal control plan management process, this study
proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous
descriptions on the signal control plan to exact signal phase and timing (SPaT)
results, which could further be transformed into structured stage-based or
ring-based plans to interact with intelligent transportation system (ITS)
software and traffic signal controllers. With curated prompts, Chat2SPaT first
leverages large language models' (LLMs) capability of understanding users' plan
descriptions and reformulate the plan as a combination of phase sequence and
phase attribute results in the json format. Based on LLM outputs, python
scripts are designed to locate phases in a cycle, address nuances of traffic
signal control, and finally assemble the complete traffic signal control plan.
Within a chat, the pipeline can be utilized iteratively to conduct further plan
editing. Experiments show that Chat2SPaT can generate plans with an accuracy of
over 94% for both English and Chinese cases, using a test dataset with over 300
plan descriptions. As the first benchmark for evaluating LLMs' capability of
understanding traffic signal control plan descriptions, Chat2SPaT provides an
easy-to-use plan management pipeline for traffic practitioners and researchers,
serving as a potential new building block for a more accurate and versatile
application of LLMs in the field of ITS. The source codes, prompts and test
dataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.

</details>


### [3] [Aligned Textual Scoring Rules](https://arxiv.org/abs/2507.06221)
*Yuxuan Lu,Yifan Wu,Jason Hartline,Michael J. Curry*

Main category: cs.AI

TL;DR: 提出文本的对齐评分规则ASR，优化评分规则与参考分数的均方误差，实验显示ASR在符合人类偏好且保持适当性上优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有一些适当的评分规则不能很好地与人类对文本的偏好对齐。

Method: 通过优化和最小化适当评分规则与参考分数（如人类评分）之间的均方误差，设计文本的对齐评分规则（ASR）。

Result: 实验表明ASR在符合人类偏好的同时能保持适当性，且表现优于先前方法。

Conclusion: 所设计的ASR能有效解决现有评分规则与人类偏好的对齐问题，在文本信息提取方面表现更好。

Abstract: Scoring rules elicit probabilistic predictions from a strategic agent by
scoring the prediction against a ground truth state. A scoring rule is proper
if, from the agent's perspective, reporting the true belief maximizes the
expected score. With the development of language models, Wu and Hartline (2024)
proposes a reduction from textual information elicitation to the numerical
(i.e. probabilistic) information elicitation problem, which achieves provable
properness for textual elicitation. However, not all proper scoring rules are
well aligned with human preference over text. Our paper designs the Aligned
Scoring rule (ASR) for text by optimizing and minimizing the mean squared error
between a proper scoring rule and a reference score (e.g. human score). Our
experiments show that our ASR outperforms previous methods in aligning with
human preference while maintaining properness.

</details>


### [4] [Fuzzy Classification Aggregation for a Continuum of Agents](https://arxiv.org/abs/2507.05297)
*Zijun Meng*

Main category: cs.AI

TL;DR: 证明连续个体对m个对象进行分类的最优、独立和零一致模糊分类聚合函数必为加权算术平均。


<details>
  <summary>Details</summary>
Motivation: 探索特定条件下模糊分类聚合函数的形式。

Method: 通过证明得出结论。

Result: 任何满足条件的模糊分类聚合函数是加权算术平均。

Conclusion: 最优、独立和零一致的模糊分类聚合函数为加权算术平均。

Abstract: We prove that any optimal, independent, and zero unanimous fuzzy
classification aggregation function of a continuum of individual
classifications of $m\ge 3$ objects into $2\le p\le m$ types must be a weighted
arithmetic mean.

</details>


### [5] [OLG++: A Semantic Extension of Obligation Logic Graph](https://arxiv.org/abs/2507.05488)
*Subhasis Dasgupta,Jon Stephens,Amarnath Gupta*

Main category: cs.AI

TL;DR: 提出OLG++，一种用于市政和跨司法管辖区的义务逻辑图语义扩展，通过实例展示其在法律知识表示上比先前图模型更具表现力。


<details>
  <summary>Details</summary>
Motivation: 为市政和跨司法管辖区的监管及法律规则建模。

Method: 提出OLG++，引入更丰富的节点和边类型，支持对规则进行结构化推理。

Result: 通过食品业务法规示例展示其表达能力，支持法律问答，相比LegalRuleML有改进。

Conclusion: OLG++比先前基于图的法律知识表示模型更具表现力。

Abstract: We present OLG++, a semantic extension of the Obligation Logic Graph (OLG)
for modeling regulatory and legal rules in municipal and interjurisdictional
contexts. OLG++ introduces richer node and edge types, including spatial,
temporal, party group, defeasibility, and logical grouping constructs, enabling
nuanced representations of legal obligations, exceptions, and hierarchies. The
model supports structured reasoning over rules with contextual conditions,
precedence, and complex triggers. We demonstrate its expressiveness through
examples from food business regulations, showing how OLG++ supports legal
question answering using property graph queries. OLG++ also improves over
LegalRuleML by providing native support for subClassOf, spatial constraints,
and reified exception structures. Our examples show that OLG++ is more
expressive than prior graph-based models for legal knowledge representation.

</details>


### [6] [Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents](https://arxiv.org/abs/2507.05495)
*Prahaladh Chandrahasan,Jiahe Jin,Zhihan Zhang,Tevin Wang,Andy Tang,Lucy Mo,Morteza Ziyadi,Leonardo F. R. Ribeiro,Zimeng Qiu,Markus Dreyer,Akari Asai,Chenyan Xiong*

Main category: cs.AI

TL;DR: 本文介绍Deep Research Comparator平台及Simple Deepresearch脚手架，收集用户偏好数据展示平台在深度研究代理开发中的效用。


<details>
  <summary>Details</summary>
Motivation: 有效评估深度研究代理存在挑战，尤其是评估长报告和对中间步骤给出详细反馈。

Method: 引入Deep Research Comparator平台，提供代理托管、对比、收集反馈和计算排名的框架；开发Simple Deepresearch脚手架作为基线。

Result: 从17位注释者处收集了三个深度研究代理的真实用户偏好数据。

Conclusion: 展示了平台在深度研究代理开发中的实用性。

Abstract: Effectively evaluating deep research agents that autonomously search the web,
analyze information, and generate reports remains a major challenge,
particularly when it comes to assessing long reports and giving detailed
feedback on their intermediate steps. To address these gaps, we introduce Deep
Research Comparator, a platform that offers a holistic framework for deep
research agent hosting, side-by-side comparison, fine-grained human feedback
collection, and ranking calculation. Given a user query, our platform displays
the final reports from two different agents along with their intermediate steps
during generation. Annotators can evaluate the overall quality of final reports
based on side-by-side comparison, and also provide detailed feedback separately
by assessing intermediate steps or specific text spans within the final report.
Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This
scaffold serves as a baseline that facilitates the easy integration of various
large language models to transform them into deep research agents for
evaluation. To demonstrate the platform's utility for deep research agent
development, we have collected real user preference data from 17 annotators on
three deep research agents. A demo video of our platform can be found at
https://www.youtube.com/watch?v=g4d2dnbdseg.

</details>


### [7] [Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality](https://arxiv.org/abs/2507.05515)
*Haochen Huang,Jiahuan Pei,Mohammad Aliannejadi,Xin Sun,Moonisa Ahsan,Pablo Cesar,Chuang Yu,Zhaochun Ren,Junxiao Wang*

Main category: cs.AI

TL;DR: 引入适合AR训练的数据集，评估9个先进VLMs，发现模型在细粒度任务表现不佳，强调需改进并提供资源。


<details>
  <summary>Details</summary>
Motivation: 探索VLMs在AR训练中的应用，当前该领域研究较少。

Method: 引入适合AR训练的数据集，进行系统化视觉 - 语言任务，评估9个先进VLMs。

Result: 即使先进模型如GPT - 4o在细粒度组装任务表现差，状态检测最高F1分数仅40.54%。

Conclusion: 需要增强数据集、基准和进一步研究以改善细粒度视觉 - 语言对齐，工作有社会意义并提供相关资源支持研究。

Abstract: Vision-language models (VLMs) are essential for enabling AI-powered smart
assistants to interpret and reason in multimodal environments. However, their
application in augmented reality (AR) training remains largely unexplored. In
this work, we introduce a comprehensive dataset tailored for AR training,
featuring systematized vision-language tasks, and evaluate nine
state-of-the-art VLMs on it. Our results reveal that even advanced models,
including GPT-4o, struggle with fine-grained assembly tasks, achieving a
maximum F1 score of just 40.54% on state detection. These findings highlight
the demand for enhanced datasets, benchmarks, and further research to improve
fine-grained vision-language alignment. Beyond technical contributions, our
work has broader social implications, particularly in empowering blind and
visually impaired users with equitable access to AI-driven learning
opportunities. We provide all related resources, including the dataset, source
code, and evaluation results, to support the research community.

</details>


### [8] [Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity](https://arxiv.org/abs/2507.05816)
*Shuai Zhao,Yulin Zhang,Luwei Xiao,Xinyi Wu,Yanhao Jia,Zhongliang Guo,Xiaobao Wu,Cong-Duy Nguyen,Guoming Zhang,Anh Tuan Luu*

Main category: cs.AI

TL;DR: 引入CROP数据集和Affective - ROPTester评估框架研究大语言模型预测早产儿视网膜病变（ROP）风险能力及情感偏差，发现模型结合外部知识可提升性能，存在情感偏差，积极情感框架可减轻偏差。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在ROP风险预测能力方面研究不足，需开展相关研究。

Method: 引入CROP数据集，提出Affective - ROPTester评估框架，采用指令、思维链和上下文学习三种提示策略，并在提示层面融入情感元素。

Result: 大语言模型仅依靠内在知识预测ROP风险效果有限，结合外部输入性能提升；模型输出存在情感偏差，倾向高估中高风险病例；积极情感框架可减轻预测偏差。

Conclusion: 情感敏感的提示工程对提高诊断可靠性至关重要，Affective - ROPTester框架可用于评估和减轻临床语言建模系统中的情感偏差。

Abstract: Despite the remarkable progress of large language models (LLMs) across
various domains, their capacity to predict retinopathy of prematurity (ROP)
risk remains largely unexplored. To address this gap, we introduce a novel
Chinese benchmark dataset, termed CROP, comprising 993 admission records
annotated with low, medium, and high-risk labels. To systematically examine the
predictive capabilities and affective biases of LLMs in ROP risk
stratification, we propose Affective-ROPTester, an automated evaluation
framework incorporating three prompting strategies: Instruction-based,
Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme
assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and
ICL schemes leverage external medical knowledge to enhance predictive accuracy.
Crucially, we integrate emotional elements at the prompt level to investigate
how different affective framings influence the model's ability to predict ROP
and its bias patterns. Empirical results derived from the CROP dataset yield
two principal observations. First, LLMs demonstrate limited efficacy in ROP
risk prediction when operating solely on intrinsic knowledge, yet exhibit
marked performance gains when augmented with structured external inputs.
Second, affective biases are evident in the model outputs, with a consistent
inclination toward overestimating medium- and high-risk cases. Third, compared
to negative emotions, positive emotional framing contributes to mitigating
predictive bias in model outputs. These findings highlight the critical role of
affect-sensitive prompt engineering in enhancing diagnostic reliability and
emphasize the utility of Affective-ROPTester as a framework for evaluating and
mitigating affective bias in clinical language modeling systems.

</details>


### [9] [Modeling (Deontic) Modal Operators With the s(CASP) Goal-directed Predicated Answer Set Programming System](https://arxiv.org/abs/2507.05519)
*Gopal Gupta,Abhiramon Rajasekharan,Alexis R. Tudor,Elmer Salazar,Joaquín Arias*

Main category: cs.AI

TL;DR: 本文探讨道义模态逻辑的实现问题，用答案集编程（ASP）表示道义模态算子，借助ASP全局约束表示义务和不允许情况，解决了道义模态逻辑悖论。


<details>
  <summary>Details</summary>
Motivation: 解决道义模态逻辑的实现问题。

Method: 利用答案集编程（ASP）中的默认否定和强否定表达道义模态算子，使用ASP的全局约束表示义务和不允许。

Result: 所提出的表示方法能优雅地解决道义模态逻辑的各种悖论。

Conclusion: 使用ASP的相关特性来实现道义模态逻辑是可行且有效的，能解决该领域的悖论问题。

Abstract: We consider the problem of implementing deontic modal logic. We show how
(deontic) modal operators can be expressed elegantly using default negation
(negation-as-failure) and strong negation present in answer set programming
(ASP). We propose using global constraints of ASP to represent obligations and
impermissibilities of deontic modal logic. We show that our proposed
representation results in the various paradoxes of deontic modal logic being
elegantly resolved.

</details>


### [10] [Cultivating Multimodal Intelligence: Interpretive Reasoning and Agentic RAG Approaches to Dermatological Diagnosis](https://arxiv.org/abs/2507.05520)
*Karishma Thakrar,Shreyas Basavatia,Akshay Daftardar*

Main category: cs.AI

TL;DR: 本文围绕2025 ImageCLEF MEDIQA - MAGIC挑战赛的CVQA任务，提出结合模型微调、引入推理层和采用agentic RAG的方法，取得较好成绩，为远程医疗自动诊断支持系统提供思路。


<details>
  <summary>Details</summary>
Motivation: 解决远程医疗中有限输入下异步、高准确性和可解释性的诊断决策难题。

Method: 结合对Qwen、Gemma和LLaMA系列开源多模态模型在竞赛数据集上微调、引入结构化推理层协调模型输出、采用agentic RAG添加相关信息。

Result: 团队提交成果获第六名，综合排名第二，展现出有竞争力的表现和高准确性。

Conclusion: 该架构模拟皮肤科医生推理模式，为更可靠的自动诊断支持系统提供了途径。

Abstract: The second edition of the 2025 ImageCLEF MEDIQA-MAGIC challenge, co-organized
by researchers from Microsoft, Stanford University, and the Hospital Clinic of
Barcelona, focuses on multimodal dermatology question answering and
segmentation, using real-world patient queries and images. This work addresses
the Closed Visual Question Answering (CVQA) task, where the goal is to select
the correct answer to multiple-choice clinical questions based on both
user-submitted images and accompanying symptom descriptions. The proposed
approach combines three core components: (1) fine-tuning open-source multimodal
models from the Qwen, Gemma, and LLaMA families on the competition dataset, (2)
introducing a structured reasoning layer that reconciles and adjudicates
between candidate model outputs, and (3) incorporating agentic
retrieval-augmented generation (agentic RAG), which adds relevant information
from the American Academy of Dermatology's symptom and condition database to
fill in gaps in patient context. The team achieved second place with a
submission that scored sixth, demonstrating competitive performance and high
accuracy. Beyond competitive benchmarks, this research addresses a practical
challenge in telemedicine: diagnostic decisions must often be made
asynchronously, with limited input and with high accuracy and interpretability.
By emulating the systematic reasoning patterns employed by dermatologists when
evaluating skin conditions, this architecture provided a pathway toward more
reliable automated diagnostic support systems.

</details>


### [11] [Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment](https://arxiv.org/abs/2507.05528)
*Jiahuan Pei,Fanghua Ye,Xin Sun,Wentao Deng,Koen Hindriks,Junxiao Wang*

Main category: cs.AI

TL;DR: 提出WikiHowAgent多智能体工作流，利用大语言模型模拟教学对话，引入对话数据集，评估协议结合多种指标，结果有效且数据与代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有工作在利用大规模课程内容和评估教学质量框架方面存在不足。

Method: 提出WikiHowAgent多智能体工作流，整合教师和学习者智能体、交互管理器和评估器；引入对话数据集；采用结合计算和基于规则指标与人类判断对齐的评估协议。

Result: 工作流在不同设置中有效，揭示大语言模型跨领域能力。

Conclusion: 该工作流能有效模拟教学对话并评估教学质量，数据集和实现已开源。

Abstract: Large language models (LLMs) have advanced virtual educators and learners,
bridging NLP with AI4Education. Existing work often lacks scalability and fails
to leverage diverse, large-scale course content, with limited frameworks for
assessing pedagogic quality. To this end, we propose WikiHowAgent, a
multi-agent workflow leveraging LLMs to simulate interactive teaching-learning
conversations. It integrates teacher and learner agents, an interaction
manager, and an evaluator to facilitate procedural learning and assess
pedagogic quality. We introduce a dataset of 114,296 teacher-learner
conversations grounded in 14,287 tutorials across 17 domains and 727 topics.
Our evaluation protocol combines computational and rubric-based metrics with
human judgment alignment. Results demonstrate the workflow's effectiveness in
diverse setups, offering insights into LLM capabilities across domains. Our
datasets and implementations are fully open-sourced.

</details>


### [12] [Red Teaming AI Red Teaming](https://arxiv.org/abs/2507.05538)
*Subhabrata Majumdar,Brian Pendleton,Abhishek Gupta*

Main category: cs.AI

TL;DR: 文章审视AI红队实践，指出其与初衷差距，提出涵盖宏观系统和微观模型的框架及建议，强调多功能团队重要性。


<details>
  <summary>Details</summary>
Motivation: 当前AI红队实践偏离原意图，聚焦模型缺陷而忽略社会技术系统和涌现行为，需改进。

Method: 提出涵盖宏观系统和微观模型两个层面的红队框架，并结合网络安全经验和系统理论给出建议。

Result: 无明确提及具体实践结果，主要是提出理论框架和建议。

Conclusion: 有效的AI红队需要多功能团队，以审视涌现风险、系统漏洞及技术与社会因素的相互作用。

Abstract: Red teaming has evolved from its origins in military applications to become a
widely adopted methodology in cybersecurity and AI. In this paper, we take a
critical look at the practice of AI red teaming. We argue that despite its
current popularity in AI governance, there exists a significant gap between red
teaming's original intent as a critical thinking exercise and its narrow focus
on discovering model-level flaws in the context of generative AI. Current AI
red teaming efforts focus predominantly on individual model vulnerabilities
while overlooking the broader sociotechnical systems and emergent behaviors
that arise from complex interactions between models, users, and environments.
To address this deficiency, we propose a comprehensive framework
operationalizing red teaming in AI systems at two levels: macro-level system
red teaming spanning the entire AI development lifecycle, and micro-level model
red teaming. Drawing on cybersecurity experience and systems theory, we further
propose a set of recommendations. In these, we emphasize that effective AI red
teaming requires multifunctional teams that examine emergent risks, systemic
vulnerabilities, and the interplay between technical and social factors.

</details>


### [13] [SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation](https://arxiv.org/abs/2507.05541)
*Shovito Barua Soumma,Asiful Arefeen,Stephanie M. Carpenter,Melanie Hingle,Hassan Ghasemzadeh*

Main category: cs.AI

TL;DR: 探索用GPT - 4o - mini在零样本和三样本设置下生成反事实解释（CFs），在两个数据集上评估，结果优于传统方法，且用生成的CFs增强样本可提升下游分类器性能。


<details>
  <summary>Details</summary>
Motivation: 反事实解释可用于异常预防干预和训练鲁棒模型，探索用大语言模型生成CFs以增强临床和生理预测任务的可解释性和鲁棒性。

Method: 使用大语言模型GPT - 4o - mini在零样本和三样本设置下生成CFs，在两个数据集上进行评估。

Result: 与传统方法相比，少样本基于LLM的方法实现了高合理性（达99%）、强有效性（达0.99）和有竞争力的稀疏性；用LLM生成的CFs作为增强样本提高了下游分类器性能，平均准确率提高5%。

Conclusion: 基于提示的生成技术有潜力增强临床和生理预测任务的可解释性和鲁棒性。

Abstract: Counterfactual explanations (CFs) offer human-centric insights into machine
learning predictions by highlighting minimal changes required to alter an
outcome. Therefore, CFs can be used as (i) interventions for abnormality
prevention and (ii) augmented data for training robust models. In this work, we
explore large language models (LLMs), specifically GPT-4o-mini, for generating
CFs in a zero-shot and three-shot setting. We evaluate our approach on two
datasets: the AI-Readi flagship dataset for stress prediction and a public
dataset for heart disease detection. Compared to traditional methods such as
DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high
plausibility (up to 99%), strong validity (up to 0.99), and competitive
sparsity. Moreover, using LLM-generated CFs as augmented samples improves
downstream classifier performance (an average accuracy gain of 5%), especially
in low-data regimes. This demonstrates the potential of prompt-based generative
techniques to enhance explainability and robustness in clinical and
physiological prediction tasks. Code base: github.com/anonymous/SenseCF.

</details>


### [14] [SingLoRA: Low Rank Adaptation Using a Single Matrix](https://arxiv.org/abs/2507.05566)
*David Bensaïd,Noam Rotstein,Roy Velich,Daniel Bensaïd,Ron Kimmel*

Main category: cs.AI

TL;DR: 提出SingLoRA改进低秩自适应，减少参数并稳定优化，实验验证其在多任务上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有低秩自适应（LoRA）中两矩阵规模差异导致训练动态不稳定、性能欠佳。

Method: 将权重更新重新表述为单个低秩矩阵与其转置的分解。

Result: 常识推理中微调LLama 7B在MNLI上准确率超LoRA和LoRA+；图像生成中微调Stable Diffusion在DreamBooth上图像保真度更高。

Conclusion: SingLoRA能去除矩阵间规模冲突，稳定优化，减少参数，在多任务上表现良好。

Abstract: Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient
fine-tuning of large pretrained models. LoRA augments the pre-trained weights
of a model by adding the product of two smaller matrices that together form a
low-rank matrix update. Recent research has shown that scale disparities
between these two matrices often cause unstable training dynamics, leading to
suboptimal performance. In this paper, we propose SingLoRA, which reformulates
low-rank adaptation by learning the weights update as a decomposition of a
single low-rank matrix multiplied by its transpose. This simple design
inherently removes inter-matrix scale conflicts, ensuring stable optimization,
and roughly halves the parameter count. We analyze SingLoRA within the
infinite-width neural network framework, showing that it guarantees stable
feature learning by construction. Extensive experiments on multiple tasks
validate these benefits. In common sense reasoning, fine-tuning LLama 7B on
MNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+
(90.2%) - while using only 60% of their parameter budget. In image generation,
fine-tuning Stable Diffusion with SingLoRA significantly improves image
fidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to
scores of 0.148 and 0.143 for DoRA and LoRA, respectively.

</details>


### [15] [Towards Measurement Theory for Artificial Intelligence](https://arxiv.org/abs/2507.05587)
*Elija Perrier*

Main category: cs.AI

TL;DR: 提出人工智能形式化测量理论项目，阐述其作用并勾勒测量框架。


<details>
  <summary>Details</summary>
Motivation: 让研究者、从业者和监管者能对系统和评估方法进行比较，将前沿AI评估与定量风险分析技术相联系，明确AI能力与测量操作和尺度的关系。

Method: 勾勒分层测量堆栈，区分直接和间接可观测物。

Result: 为建立统一、可校准的AI现象分类法指明了途径。

Conclusion: 开展人工智能形式化测量理论项目是有意义且可行的。

Abstract: We motivate and outline a programme for a formal theory of measurement of
artificial intelligence. We argue that formalising measurement for AI will
allow researchers, practitioners, and regulators to: (i) make comparisons
between systems and the evaluation methods applied to them; (ii) connect
frontier AI evaluations with established quantitative risk analysis techniques
drawn from engineering and safety science; and (iii) foreground how what counts
as AI capability is contingent upon the measurement operations and scales we
elect to use. We sketch a layered measurement stack, distinguish direct from
indirect observables, and signpost how these ingredients provide a pathway
toward a unified, calibratable taxonomy of AI phenomena.

</details>


### [16] [MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models](https://arxiv.org/abs/2507.05591)
*Wei Zhang,Juan Chen,En Zhu,Wenhong Cheng,YunPeng Li,Yanbo J. Wang*

Main category: cs.AI

TL;DR: 提出多模态大语言模型MLlm - DR用于可解释的抑郁症诊断，在两个基准数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 以往研究缺乏抑郁评分确定的清晰解释，现有能处理多模态数据的大语言模型缺乏访谈数据训练，诊断性能差。

Method: 提出MLlm - DR，集成小LLMs和轻量级查询模块（LQ - former），构建训练数据集微调小LLMs，LQ - former捕捉多模态特征。

Result: 在CMDC和E - DAIC - WOZ两个基准数据集上取得了最先进的结果。

Conclusion: 所提出的MLlm - DR在抑郁症诊断中有效且具有优越性。

Abstract: Automated depression diagnosis aims to analyze multimodal information from
interview videos to predict participants' depression scores. Previous studies
often lack clear explanations of how these scores were determined, limiting
their adoption in clinical practice. While the advent of LLMs provides a
possible pathway for explainable depression diagnosis, current LLMs capable of
processing multimodal data lack training on interview data, resulting in poor
diagnostic performance when used directly. In this paper, we propose a novel
multimodal large language model (MLlm-DR) that can understand multimodal
information inputs and supports explainable depression diagnosis. MLlm-DR
integrates a smaller LLMs and a lightweight query module (LQ-former).
Specifically, the smaller LLMs is designed to generate depression scores and
corresponding evaluation rationales. To enhance its logical reasoning for
domain-specific tasks while maintaining practicality, we constructed a robust
training dataset to fine-tune it. Meanwhile, the LQ-former captures
depression-related features from speech and visual data, aiding the model's
ability to process multimodal information, to achieve comprehensive depression
diagnosis. Our approach achieves state-of-the-art results on two
interview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its
effectiveness and superiority.

</details>


### [17] [Domain adaptation of large language models for geotechnical applications](https://arxiv.org/abs/2507.05613)
*Lei Fan,Fangxue Liu,Cheng Chen*

Main category: cs.AI

TL;DR: 本文首次对大语言模型在岩土工程中的适配与应用进行调查，介绍适配方法、应用场景，分析利弊并指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型应用于岩土工程需特定领域适配，为了促进其在岩土工程中的有效应用。

Method: 介绍适配岩土领域的关键方法，如提示工程、检索增强生成、领域自适应预训练和微调；调研适配后大语言模型的应用现状。

Result: 明确了大语言模型在岩土工程中的多种应用场景，分析了其利弊。

Conclusion: 研究结果为从业者将大语言模型集成到岩土实践提供资源，为学术界进一步研究奠定基础。

Abstract: Recent developments in large language models (LLMs) are opening up new
opportunities in geotechnical engineering and engineering geology. While
general-purpose LLMs possess broad capabilities, effective application in
geotechnics often requires domain-specific adaptation. Such tailored LLMs are
increasingly employed to streamline geotechnical workflows. This paper presents
the first survey of the adaptation and application of LLMs in geotechnical
engineering. It outlines key methodologies for adaptation to geotechnical
domain, including prompt engineering, retrieval-augmented generation,
domain-adaptive pretraining, and fine-tuning. The survey examines the
state-of-the-art applications of geotechnical-adapted LLMs, including
geological interpretation, subsurface characterization, site planning, design
calculations, numerical modeling, safety and risk assessment, and educational
tutoring. It also analyzes benefits and limitations of geotechnical-adapted
LLMs, and identifies promising directions for future research in this
interdisciplinary discipline. The findings serve as a valuable resource for
practitioners seeking to integrate LLMs into geotechnical practice, while also
providing a foundation to stimulate further investigation within the academic
community.

</details>


### [18] [ADMC: Attention-based Diffusion Model for Missing Modalities Feature Completion](https://arxiv.org/abs/2507.05624)
*Wei Zhang,Juan Chen,Yanbo J. Wang,En Zhu,Xuan Yang,Yiduo Wang*

Main category: cs.AI

TL;DR: 本文提出基于注意力的扩散模型ADMC解决多模态情感和意图识别中缺失模态问题，在相关基准测试取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 多模态情感和意图识别中存在因传感器故障或数据不完整导致的缺失模态问题，传统方法存在过耦合和生成不准确问题。

Method: 引入ADMC，为各模态独立训练特征提取网络，利用基于注意力的扩散网络生成缺失模态特征。

Result: 在IEMOCAP和MIntRec基准测试取得了SOTA结果。

Conclusion: ADMC在缺失和完整模态场景下均有效。

Abstract: Multimodal emotion and intent recognition is essential for automated
human-computer interaction, It aims to analyze users' speech, text, and visual
information to predict their emotions or intent. One of the significant
challenges is that missing modalities due to sensor malfunctions or incomplete
data. Traditional methods that attempt to reconstruct missing information often
suffer from over-coupling and imprecise generation processes, leading to
suboptimal outcomes. To address these issues, we introduce an Attention-based
Diffusion model for Missing Modalities feature Completion (ADMC). Our framework
independently trains feature extraction networks for each modality, preserving
their unique characteristics and avoiding over-coupling. The Attention-based
Diffusion Network (ADN) generates missing modality features that closely align
with authentic multimodal distribution, enhancing performance across all
missing-modality scenarios. Moreover, ADN's cross-modal generation offers
improved recognition even in full-modality contexts. Our approach achieves
state-of-the-art results on the IEMOCAP and MIntRec benchmarks, demonstrating
its effectiveness in both missing and complete modality scenarios.

</details>


### [19] [Enhancing Student Learning with LLM-Generated Retrieval Practice Questions: An Empirical Study in Data Science Courses](https://arxiv.org/abs/2507.05629)
*Yuan An,John Liu,Niyam Acharya,Ruhma Hashmi*

Main category: cs.AI

TL;DR: 研究对比了有无大语言模型生成的检索练习问题对学生学习效果的影响，发现有此类练习时学生知识保留率更高，但教师仍需手动审核问题。


<details>
  <summary>Details</summary>
Motivation: 生成高质量检索练习问题对教师来说耗时费力，大语言模型虽有自动化潜力，但效果待验证。

Method: 对约60名学生的两门大学数据科学课程进行实证研究，对比有和无大语言模型生成的选择题检索练习的学习结果。

Result: 有大语言模型生成的检索练习时，学生平均准确率达89%，无练习时为73%。

Conclusion: 大语言模型生成的检索问题能有效支持学生学习，是一种可扩展的教学方案，但教师仍需手动验证和修改问题。

Abstract: Retrieval practice is a well-established pedagogical technique known to
significantly enhance student learning and knowledge retention. However,
generating high-quality retrieval practice questions is often time-consuming
and labor intensive for instructors, especially in rapidly evolving technical
subjects. Large Language Models (LLMs) offer the potential to automate this
process by generating questions in response to prompts, yet the effectiveness
of LLM-generated retrieval practice on student learning remains to be
established. In this study, we conducted an empirical study involving two
college-level data science courses, with approximately 60 students. We compared
learning outcomes during one week in which students received LLM-generated
multiple-choice retrieval practice questions to those from a week in which no
such questions were provided. Results indicate that students exposed to
LLM-generated retrieval practice achieved significantly higher knowledge
retention, with an average accuracy of 89%, compared to 73% in the week without
such practice. These findings suggest that LLM-generated retrieval questions
can effectively support student learning and may provide a scalable solution
for integrating retrieval practice into real-time teaching. However, despite
these encouraging outcomes and the potential time-saving benefits, cautions
must be taken, as the quality of LLM-generated questions can vary. Instructors
must still manually verify and revise the generated questions before releasing
them to students.

</details>


### [20] [LLMs are Introvert](https://arxiv.org/abs/2507.05638)
*Litian Zhang,Xiaoming Zhang,Bingyu Yan,Ziyi Zhou,Bo Zhang,Zhenyu Guan,Xi Zhang,Chaozhuo Li*

Main category: cs.AI

TL;DR: 社交媒体和生成式AI使信息传播变革，但也加速了虚假信息传播。研究引入基于LLM的模拟环境，发现问题后提出SIP - CoT机制，实验证明其可提升LLM代理的社交智能和真实感。


<details>
  <summary>Details</summary>
Motivation: 社交媒体和生成式AI发展使信息传播变革，需理解信息传播动态并开发有效控制策略，传统模型有不足，LLM有模拟信息传播心理方面的潜力。

Method: 引入基于LLM的模拟环境，通过社会信息处理理论详细评估，提出SIP - CoT机制并结合情感引导记忆。

Result: 实验表明SIP - CoT增强的LLM代理能更有效地处理社会信息，行为、态度和情感更接近真实人类互动。

Conclusion: 当前基于LLM的传播模拟有局限性，集成SIP - CoT和情感记忆可显著提升LLM代理的社交智能和真实感。

Abstract: The exponential growth of social media and generative AI has transformed
information dissemination, fostering connectivity but also accelerating the
spread of misinformation. Understanding information propagation dynamics and
developing effective control strategies is essential to mitigate harmful
content. Traditional models, such as SIR, provide basic insights but
inadequately capture the complexities of online interactions. Advanced methods,
including attention mechanisms and graph neural networks, enhance accuracy but
typically overlook user psychology and behavioral dynamics. Large language
models (LLMs), with their human-like reasoning, offer new potential for
simulating psychological aspects of information spread. We introduce an
LLM-based simulation environment capturing agents' evolving attitudes,
emotions, and responses. Initial experiments, however, revealed significant
gaps between LLM-generated behaviors and authentic human dynamics, especially
in stance detection and psychological realism. A detailed evaluation through
Social Information Processing Theory identified major discrepancies in
goal-setting and feedback evaluation, stemming from the lack of emotional
processing in standard LLM training. To address these issues, we propose the
Social Information Processing-based Chain of Thought (SIP-CoT) mechanism
enhanced by emotion-guided memory. This method improves the interpretation of
social cues, personalization of goals, and evaluation of feedback. Experimental
results confirm that SIP-CoT-enhanced LLM agents more effectively process
social information, demonstrating behaviors, attitudes, and emotions closer to
real human interactions. In summary, this research highlights critical
limitations in current LLM-based propagation simulations and demonstrates how
integrating SIP-CoT and emotional memory significantly enhances the social
intelligence and realism of LLM agents.

</details>


### [21] [City-Level Foreign Direct Investment Prediction with Tabular Learning on Judicial Data](https://arxiv.org/abs/2507.05651)
*Tianxing Wu,Lizhe Cao,Shuang Wang,Jiming Wang,Shutong Zhu,Yerong Wu,Yuqing Feng*

Main category: cs.AI

TL;DR: 文章尝试用大规模司法数据进行城市层面FDI预测，提出TLJD方法，实验表明其优于其他基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于经济数据的城市FDI预测因数据易被操纵而不可靠，为解决此问题利用司法数据进行预测。

Method: 先根据超千万份公开裁判文书构建司法绩效评估指标体系并重构表格数据集，再提出TLJD方法进行编码和权重调整。

Result: 跨城市和跨时间任务的大量实验显示，TLJD在不同评估指标上至少达到0.92的R2，优于其他十个基线模型。

Conclusion: 基于司法数据的TLJD方法在城市FDI预测中有效且表现优越。

Abstract: To advance the United Nations Sustainable Development Goal on promoting
sustained, inclusive, and sustainable economic growth, foreign direct
investment (FDI) plays a crucial role in catalyzing economic expansion and
fostering innovation. Precise city-level FDI prediction is quite important for
local government and is commonly studied based on economic data (e.g., GDP).
However, such economic data could be prone to manipulation, making predictions
less reliable. To address this issue, we try to leverage large-scale judicial
data which reflects judicial performance influencing local investment security
and returns, for city-level FDI prediction. Based on this, we first build an
index system for the evaluation of judicial performance over twelve million
publicly available adjudication documents according to which a tabular dataset
is reformulated. We then propose a new Tabular Learning method on Judicial Data
(TLJD) for city-level FDI prediction. TLJD integrates row data and column data
in our built tabular dataset for judicial performance indicator encoding, and
utilizes a mixture of experts model to adjust the weights of different
indicators considering regional variations. To validate the effectiveness of
TLJD, we design cross-city and cross-time tasks for city-level FDI predictions.
Extensive experiments on both tasks demonstrate the superiority of TLJD (reach
to at least 0.92 R2) over the other ten state-of-the-art baselines in different
evaluation metrics.

</details>


### [22] [Enhancing the Interpretability of Rule-based Explanations through Information Retrieval](https://arxiv.org/abs/2507.05976)
*Alessandro Umbrico,Guido Bologna,Luca Coraci,Francesca Fracasso,Silvia Gola,Gabriella Cortellessa*

Main category: cs.AI

TL;DR: 提出基于归因的方法提高可解释AI在乳腺癌腋窝放疗后上肢淋巴水肿风险评估预测中的可解释性，用户研究表明该方法效果更好。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的人工智能技术缺乏透明度，限制了其在医疗决策过程中的可解释性和接受度，需提高可解释AI在乳腺癌腋窝放疗后上肢淋巴水肿风险评估预测中的可解释性。

Method: 使用信息检索技术的标准指标对基于规则的预测模型中的属性进行统计分析，计算各属性与预测的相关性。

Result: 用户研究表明，与可解释AI模型的原始输出相比，该方法生成的输出在预测淋巴水肿风险方面具有更高的可解释性和实用性。

Conclusion: 所提出的基于归因的方法能有效提高可解释AI预测的可解释性。

Abstract: The lack of transparency of data-driven Artificial Intelligence techniques
limits their interpretability and acceptance into healthcare decision-making
processes. We propose an attribution-based approach to improve the
interpretability of Explainable AI-based predictions in the specific context of
arm lymphedema's risk assessment after lymph nodal radiotherapy in breast
cancer. The proposed method performs a statistical analysis of the attributes
in the rule-based prediction model using standard metrics from Information
Retrieval techniques. This analysis computes the relevance of each attribute to
the prediction and provides users with interpretable information about the
impact of risk factors. The results of a user study that compared the output
generated by the proposed approach with the raw output of the Explainable AI
model suggested higher levels of interpretability and usefulness in the context
of predicting lymphedema risk.

</details>


### [23] [Divergent Realities: A Comparative Analysis of Human Expert vs. Artificial Intelligence Based Generation and Evaluation of Treatment Plans in Dermatology](https://arxiv.org/abs/2507.05716)
*Dipayan Sengupta,Saumya Panda*

Main category: cs.AI

TL;DR: 研究对比人类专家与两种AI模型生成的治疗计划，由人类同行和高级AI评判，发现评估者性质影响对计划质量的感知，揭示经验与算法逻辑差距，需协同的人机系统。


<details>
  <summary>Details</summary>
Motivation: 随着AI应用超出诊断领域，评估AI生成的治疗计划成为关键挑战，尤其面对新的推理模型，故开展此对比研究。

Method: 十位皮肤科医生、通用AI（GPT - 4o）和推理AI（o3）为五个复杂皮肤科病例生成治疗计划，先由十位人类专家评分，再由高级AI评判（Gemini 2.5 Pro）使用相同规则评分。

Result: 存在显著的“评估者效应”，人类专家给同行计划评分显著高于AI计划；AI评判则相反，给AI计划评分显著高于人类计划。

Conclusion: 临床计划质量感知取决于评估者性质，经验与算法逻辑有差距，AI集成面临挑战，未来需协同、可解释的人机系统来改善临床护理。

Abstract: Background: Evaluating AI-generated treatment plans is a key challenge as AI
expands beyond diagnostics, especially with new reasoning models. This study
compares plans from human experts and two AI models (a generalist and a
reasoner), assessed by both human peers and a superior AI judge.
  Methods: Ten dermatologists, a generalist AI (GPT-4o), and a reasoning AI
(o3) generated treatment plans for five complex dermatology cases. The
anonymized, normalized plans were scored in two phases: 1) by the ten human
experts, and 2) by a superior AI judge (Gemini 2.5 Pro) using an identical
rubric.
  Results: A profound 'evaluator effect' was observed. Human experts scored
peer-generated plans significantly higher than AI plans (mean 7.62 vs. 7.16;
p=0.0313), ranking GPT-4o 6th (mean 7.38) and the reasoning model, o3, 11th
(mean 6.97). Conversely, the AI judge produced a complete inversion, scoring AI
plans significantly higher than human plans (mean 7.75 vs. 6.79; p=0.0313). It
ranked o3 1st (mean 8.20) and GPT-4o 2nd, placing all human experts lower.
  Conclusions: The perceived quality of a clinical plan is fundamentally
dependent on the evaluator's nature. An advanced reasoning AI, ranked poorly by
human experts, was judged as superior by a sophisticated AI, revealing a deep
gap between experience-based clinical heuristics and data-driven algorithmic
logic. This paradox presents a critical challenge for AI integration,
suggesting the future requires synergistic, explainable human-AI systems that
bridge this reasoning gap to augment clinical care.

</details>


### [24] [An autonomous agent for auditing and improving the reliability of clinical AI models](https://arxiv.org/abs/2507.05755)
*Lukas Kuhn,Florian Buettner*

Main category: cs.AI

TL;DR: 提出ModelAuditor工具，可识别AI模型在临床实践中的失败模式并提供修复建议，评估显示其能有效提升模型性能且成本低。


<details>
  <summary>Details</summary>
Motivation: AI模型在临床实践中面对现实变化易失败，现有可靠性审计过程定制化且耗时，从业者缺乏可用工具。

Method: 引入ModelAuditor，它能与用户对话、选择特定指标、模拟相关分布变化，并生成可解释报告。

Result: 在三个临床场景中，ModelAuditor能正确识别模型失败模式，其建议可挽回15 - 25%的性能损失，优于基线模型和先进增强方法，且执行成本低。

Conclusion: ModelAuditor能有效解决AI模型在临床实践中的可靠性问题，提升模型性能。

Abstract: The deployment of AI models in clinical practice faces a critical challenge:
models achieving expert-level performance on benchmarks can fail
catastrophically when confronted with real-world variations in medical imaging.
Minor shifts in scanner hardware, lighting or demographics can erode accuracy,
but currently reliability auditing to identify such catastrophic failure cases
before deployment is a bespoke and time-consuming process. Practitioners lack
accessible and interpretable tools to expose and repair hidden failure modes.
Here we introduce ModelAuditor, a self-reflective agent that converses with
users, selects task-specific metrics, and simulates context-dependent,
clinically relevant distribution shifts. ModelAuditor then generates
interpretable reports explaining how much performance likely degrades during
deployment, discussing specific likely failure modes and identifying root
causes and mitigation strategies. Our comprehensive evaluation across three
real-world clinical scenarios - inter-institutional variation in
histopathology, demographic shifts in dermatology, and equipment heterogeneity
in chest radiography - demonstrates that ModelAuditor is able correctly
identify context-specific failure modes of state-of-the-art models such as the
established SIIM-ISIC melanoma classifier. Its targeted recommendations recover
15-25% of performance lost under real-world distribution shift, substantially
outperforming both baseline models and state-of-the-art augmentation methods.
These improvements are achieved through a multi-agent architecture and execute
on consumer hardware in under 10 minutes, costing less than US$0.50 per audit.

</details>


### [25] [Real-time monitoring of the SoH of lithium-ion batteries](https://arxiv.org/abs/2507.05765)
*Bruno Jammes,Edgar Hernando Sepúlveda-Oviedo,Corinne Alonso*

Main category: cs.AI

TL;DR: 提出基于充电阶段结束时放电脉冲分析的电池健康状态（SoH）实时监测方法，初步结果证明其有效性，有望集成到电池管理系统。


<details>
  <summary>Details</summary>
Motivation: 实时监测电池SoH是重大挑战，特别是微电网中传统方法受限，需新方法。

Method: 基于充电阶段结束时放电脉冲分析，用等效电气模型参数估计SoH。

Result: 用容量退化约85%的两块电池参数训练后，成功预测另外两块电池退化至约90% SoH情况，最坏情况平均绝对误差约1%，估计器可解释性得分接近0.9。

Conclusion: 若性能得到确认，该方法可轻松集成到电池管理系统，为连续运行下的电池优化管理铺平道路。

Abstract: Real-time monitoring of the state of health (SoH) of batteries remains a
major challenge, particularly in microgrids where operational constraints limit
the use of traditional methods. As part of the 4BLife project, we propose an
innovative method based on the analysis of a discharge pulse at the end of the
charge phase. The parameters of the equivalent electrical model describing the
voltage evolution across the battery terminals during this current pulse are
then used to estimate the SoH. Based on the experimental data acquired so far,
the initial results demonstrate the relevance of the proposed approach. After
training using the parameters of two batteries with a capacity degradation of
around 85%, we successfully predicted the degradation of two other batteries,
cycled down to approximately 90% SoH, with a mean absolute error of around 1%
in the worst case, and an explainability score of the estimator close to 0.9.
If these performances are confirmed, this method can be easily integrated into
battery management systems (BMS) and paves the way for optimized battery
management under continuous operation.

</details>


### [26] [GTA1: GUI Test-time Scaling Agent](https://arxiv.org/abs/2507.05791)
*Yan Yang,Dongxu Li,Yutong Dai,Yuhao Yang,Ziyang Luo,Zirui Zhao,Zhiyuan Hu,Junzhe Huang,Amrita Saha,Zeyuan Chen,Ran Xu,Liyuan Pan,Caiming Xiong,Junnan Li*

Main category: cs.AI

TL;DR: 本文提出GUI Test-time Scaling Agent (GTA1)解决GUI代理任务规划和动作定位的挑战，实验达SOTA，代码和模型开源。


<details>
  <summary>Details</summary>
Motivation: 解决GUI代理在任务规划中的歧义以及在复杂高分辨率界面中准确执行动作的挑战。

Method: 引入测试时缩放方法选择合适动作提案；利用强化学习模型将动作提案准确对应到视觉元素。

Result: 在多个基准测试中取得SOTA性能，如GTA1 - 7B在不同数据集上的准确率，与规划器配合在OSWorld上任务成功率达45.2%。

Conclusion: 所提方法有效解决了GUI代理面临的两个挑战，具有良好性能。

Abstract: Graphical user interface (GUI) agents autonomously operate across platforms
(e.g., Linux) to complete tasks by interacting with visual elements.
Specifically, a user instruction is decomposed into a sequence of action
proposals, each corresponding to an interaction with the GUI. After each
action, the agent observes the updated GUI environment to plan the next step.
However, two main challenges arise: i) resolving ambiguity in task planning
(i.e., the action proposal sequence), where selecting an appropriate plan is
non-trivial, as many valid ones may exist; ii) accurately grounding actions in
complex and high-resolution interfaces, i.e., precisely interacting with visual
targets.
  This paper investigates the two aforementioned challenges with our GUI
Test-time Scaling Agent, namely GTA1. First, to select the most appropriate
action proposal, we introduce a test-time scaling method. At each step, we
sample multiple candidate action proposals and leverage a judge model to
evaluate and select the most suitable one. It trades off computation for better
decision quality by concurrent sampling, shortening task execution steps, and
improving overall performance. Second, we propose a model that achieves
improved accuracy when grounding the selected action proposal to its
corresponding visual elements. Our key insight is that reinforcement learning
(RL) facilitates visual grounding through inherent objective alignments,
rewarding successful clicks on interface elements.
  Experimentally, our method establishes state-of-the-art performance across
diverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%
accuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When
paired with a planner applying our test-time scaling strategy, it exhibits
state-of-the-art agentic performance (e.g., 45.2% task success rate on
OSWorld). We open-source our code and models here.

</details>


### [27] [CogniPlay: a work-in-progress Human-like model for General Game Playing](https://arxiv.org/abs/2507.05868)
*Aloïs Rautureau,Éric Piette*

Main category: cs.AI

TL;DR: AI虽在多种游戏中表现出色，但非‘类人’，本文介绍认知心理学成果、前人建模工作，引入CogniPlay模型。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统虽在游戏中表现好，但未复制人类基于模式的直觉决策过程，需构建类人行为模型。

Method: 对认知心理学发现和前人建模工作进行综述，在此基础上构建CogniPlay模型。

Result: 未提及。

Conclusion: 未提及。

Abstract: While AI systems have equaled or surpassed human performance in a wide
variety of games such as Chess, Go, or Dota 2, describing these systems as
truly "human-like" remains far-fetched. Despite their success, they fail to
replicate the pattern-based, intuitive decision-making processes observed in
human cognition. This paper presents an overview of findings from cognitive
psychology and previous efforts to model human-like behavior in artificial
agents, discusses their applicability to General Game Playing (GGP) and
introduces our work-in-progress model based on these observations: CogniPlay.

</details>


### [28] [Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc -- and We Can Do Better](https://arxiv.org/abs/2507.05886)
*Aaron Bembenek*

Main category: cs.AI

TL;DR: 提出神经符号转换系统作为构建神经符号自动推理工具的计算模型，可突破当前逻辑推理能力并保留符号算法优势。


<details>
  <summary>Details</summary>
Motivation: 当前构建神经符号自动推理系统的临时编程模型缺乏传统符号算法的强保证，且神经网络与符号推理同步不足，无法发挥大语言模型推理的全部潜力。

Method: 提出神经符号转换系统模型，使符号状态与直觉配对，状态转换并行处理符号和直觉。

Result: 未提及具体结果。

Conclusion: 该新范式能在保留符号算法强保证的同时，将逻辑推理能力扩展到超越当前水平，并勾勒了如何在逻辑编程语言中实现该计算模型。

Abstract: There is growing excitement about building software verifiers, synthesizers,
and other Automated Reasoning (AR) tools by combining traditional symbolic
algorithms and Large Language Models (LLMs). Unfortunately, the current
practice for constructing such neurosymbolic AR systems is an ad hoc
programming model that does not have the strong guarantees of traditional
symbolic algorithms, nor a deep enough synchronization of neural networks and
symbolic reasoning to unlock the full potential of LLM-powered reasoning. I
propose Neurosymbolic Transition Systems as a principled computational model
that can underlie infrastructure for building neurosymbolic AR tools. In this
model, symbolic state is paired with intuition, and state transitions operate
over symbols and intuition in parallel. I argue why this new paradigm can scale
logical reasoning beyond current capabilities while retaining the strong
guarantees of symbolic algorithms, and I sketch out how the computational model
I propose can be reified in a logic programming language.

</details>


### [29] [Decomposing the Time Series Forecasting Pipeline: A Modular Approach for Time Series Representation, Information Extraction, and Projection](https://arxiv.org/abs/2507.05891)
*Robert Leppich,Michael Stenger,André Bauer,Samuel Kounev*

Main category: cs.AI

TL;DR: 本文将时间序列预测流程分解为三个核心阶段，评估不同模块在多样预测任务中的效果，模型达SOTA精度且提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测需有效序列表示、内存构建和准确目标投影，每个任务有独特挑战，需系统解决任务特定困难。

Method: 将时间序列预测管道分解为输入序列表示、信息提取与内存构建、最终目标投影三个核心阶段，在各阶段研究多种架构配置，评估不同模块。

Result: 模型实现了最先进的预测精度，大幅提高计算效率，减少训练和推理时间，降低参数数量。

Conclusion: 通过分解预测流程和评估不同模块，可有效应对时间序列预测的挑战，实现高精度和高效计算。

Abstract: With the advent of Transformers, time series forecasting has seen significant
advances, yet it remains challenging due to the need for effective sequence
representation, memory construction, and accurate target projection. Time
series forecasting remains a challenging task, demanding effective sequence
representation, meaningful information extraction, and precise future
projection. Each dataset and forecasting configuration constitutes a distinct
task, each posing unique challenges the model must overcome to produce accurate
predictions. To systematically address these task-specific difficulties, this
work decomposes the time series forecasting pipeline into three core stages:
input sequence representation, information extraction and memory construction,
and final target projection. Within each stage, we investigate a range of
architectural configurations to assess the effectiveness of various modules,
such as convolutional layers for feature extraction and self-attention
mechanisms for information extraction, across diverse forecasting tasks,
including evaluations on seven benchmark datasets. Our models achieve
state-of-the-art forecasting accuracy while greatly enhancing computational
efficiency, with reduced training and inference times and a lower parameter
count. The source code is available at
https://github.com/RobertLeppich/REP-Net.

</details>


### [30] [MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation](https://arxiv.org/abs/2507.05894)
*Fathinah Izzati,Xinyue Li,Yuxuan Wu,Gus Xia*

Main category: cs.AI

TL;DR: 本文探索音乐语言模型执行音乐场景想象任务的能力，构建数据集，微调模型得到MusiScene，评估显示其优于MU - LLaMA，并利用生成的字幕增强视频背景音乐生成。


<details>
  <summary>Details</summary>
Motivation: 探索音乐语言模型能否像人类一样进行音乐场景想象，改进现有仅关注音乐元素的音乐字幕模型。

Method: 构建3371对的大规模视频 - 音频字幕数据集；针对音乐场景想象任务微调Music Understanding LLaMA得到MusiScene；进行全面评估。

Result: MusiScene比MU - LLaMA更能生成上下文相关的字幕。

Conclusion: MusiScene在音乐场景想象任务上表现更好，且生成的字幕可用于增强视频背景音乐生成。

Abstract: Humans can imagine various atmospheres and settings when listening to music,
envisioning movie scenes that complement each piece. For example, slow,
melancholic music might evoke scenes of heartbreak, while upbeat melodies
suggest celebration. This paper explores whether a Music Language Model, e.g.
MU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI),
which requires cross-modal information from video and music to train. To
improve upon existing music captioning models which focusing solely on musical
elements, we introduce MusiScene, a music captioning model designed to imagine
scenes that complement each music. In this paper, (1) we construct a
large-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music
Understanding LLaMA for the MSI task to create MusiScene, and (3) we conduct
comprehensive evaluations and prove that our MusiScene is more capable of
generating contextually relevant captions compared to MU-LLaMA. We leverage the
generated MSI captions to enhance Video Background Music Generation (VBMG) from
text.

</details>


### [31] [BlueLM-2.5-3B Technical Report](https://arxiv.org/abs/2507.05934)
*Baojiao Xiong,Boheng Chen,Chengzhi Wang,Daxiong Luo,Dongsheng Xu,Dongyang Liu,Fan Yang,Fangyuan Li,Fei Teng,Feng Wang,Fukang Qin,Fuquan Peng,Guanxin Tan,Guozhi Wang,Haibo Yu,Haohao Gao,Heng Liu,Hongbo Yang,Hongjian Zou,Houzheng Shen,Hu Meng,Huan Li,Hui Tan,Jiali Chen,Jianzhao Chen,Jinliang Zhu,Kai Wang,Lei Wu,Liangbing Liu,Liuyang Bian,Liyan He,Long Liu,Peiwen Li,Penggang Shi,Qi Ding,Rui Hu,Shuai Cao,Shuai Ren,Shuang Peng,Teng Xie,Weiji Chen,Weilin Xiang,Weixin Wu,Xi Yin,Xiaoxin Chen,Xu Chen,Yafei Wen,Yan Hu,Yanzhou Yang,Yina Xie,Yinghao Chen,Yixuan Liao,Yu Geng,Yuanjiang Ouyang,Yuanzhuo Yang,Yuehua He,Yushuai Peng,Zhaoxiong Wang,Zheng Wang,Zhibo Zhou,Ziyang Wu*

Main category: cs.AI

TL;DR: 介绍紧凑统一的多模态大语言模型BlueLM-2.5-3B，有强能力，开发方法多样，性能优且数据效率高。


<details>
  <summary>Details</summary>
Motivation: 开发适合边缘设备部署、具备强通用和推理能力的多模态大语言模型。

Method: 通过多样化数据整理、关键数据重采样、混合异构强化学习和高性能训练基础设施开发。

Result: 仅29亿参数下实现出色多模态能力和纯文本性能，在多种基准测试中有良好表现，数据效率高。

Conclusion: 工作有助于高性能端侧多模态大语言模型发展，为研究界提供见解。

Abstract: We present BlueLM-2.5-3B, a compact and unified dense Multimodal Large
Language Model (MLLM) designed for efficient edge-device deployment, offering
strong general-purpose and reasoning capabilities. To the best of our
knowledge, this is the first 3B-scale MLLM to support both thinking and
non-thinking modes, while also enabling explicit control over thinking token
budget. BlueLM-2.5-3B is developed through diversified data curation, key data
resampling, hybrid heterogeneous reinforcement learning, and a high-performance
training infrastructure. Our model achieves superior multimodal capacity while
preserving competitive pure-text performance with only 2.9 billion parameters.
We conduct comprehensive evaluations across a broad range of multimodal and
text-only benchmarks. In thinking mode, BlueLM-2.5-3B achieves comparable
performance to Qwen3-4B on text-only benchmarks, and trails the larger
Kimi-VL-A3B-16B by only about 5% on average across multimodal evaluations. In
non-thinking mode, it outperforms Qwen2.5-VL-3B on the majority of multimodal
benchmarks. Additionally, BlueLM-2.5-3B exhibits exceptional data efficiency.
All of the aforementioned performance is achieved with substantially less total
training data than Qwen2.5-VL-3B and Qwen3-4B. We hope our work contributes to
the advancement of high-performance, on-device MLLMs and provides meaningful
insights to the research community.

</details>


### [32] [A Wireless Foundation Model for Multi-Task Prediction](https://arxiv.org/abs/2507.05938)
*Yucheng Sheng,Jiacheng Wang,Xingyu Zhou,Le Liang,Hao Ye,Shi Jin,Geoffrey Ye Li*

Main category: cs.AI

TL;DR: 提出用于无线网络多任务预测的统一基础模型，有强泛化能力和零样本性能。


<details>
  <summary>Details</summary>
Motivation: 移动通信网络日益复杂动态，传统深度学习方法在不同场景和任务的泛化能力差，需准确预测关键系统参数。

Method: 提出统一基础模型，进行单变量分解统一异构任务、编码粒度实现区间感知、用因果Transformer骨干进行预测，训练时引入补丁掩码策略支持任意输入长度。

Result: 模型在大规模数据集训练后，对未见场景有强泛化能力，在新任务上零样本性能超传统全样本基线。

Conclusion: 所提统一基础模型能有效解决传统方法泛化问题，适用于无线网络多任务预测。

Abstract: With the growing complexity and dynamics of the mobile communication
networks, accurately predicting key system parameters, such as channel state
information (CSI), user location, and network traffic, has become essential for
a wide range of physical (PHY)-layer and medium access control (MAC)-layer
tasks. Although traditional deep learning (DL)-based methods have been widely
applied to such prediction tasks, they often struggle to generalize across
different scenarios and tasks. In response, we propose a unified foundation
model for multi-task prediction in wireless networks that supports diverse
prediction intervals. The proposed model enforces univariate decomposition to
unify heterogeneous tasks, encodes granularity for interval awareness, and uses
a causal Transformer backbone for accurate predictions. Additionally, we
introduce a patch masking strategy during training to support arbitrary input
lengths. After trained on large-scale datasets, the proposed foundation model
demonstrates strong generalization to unseen scenarios and achieves zero-shot
performance on new tasks that surpass traditional full-shot baselines.

</details>


### [33] [Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening](https://arxiv.org/abs/2507.05984)
*Zhijun Guo,Alvina Lai,Julia Ive,Alexandru Petcu,Yutong Wang,Luyuan Qi,Johan H Thygesen,Kezhi Li*

Main category: cs.AI

TL;DR: 开发HopeBot聊天机器人用于抑郁症筛查，研究显示其与自评版PHQ - 9评分高度一致，用户评价好，多数人愿复用或推荐，表明语音LLM聊天机器人可用于常规抑郁症筛查。


<details>
  <summary>Details</summary>
Motivation: 静态工具如PHQ - 9筛查抑郁症缺乏交互性和适应性，需要开发更优工具。

Method: 开发基于大语言模型的聊天机器人HopeBot进行PHQ - 9筛查，开展受试者内研究，让132名中英成年人完成自评版和聊天机器人版测试，并收集75名参与者反馈。

Result: 两种版本评分强相关（ICC = 0.91；45%相同）；71%参与者更信任聊天机器人；各项评分较高，推荐有用性因就业状况和既往心理健康服务使用情况而异；87.1%参与者愿复用或推荐。

Conclusion: 语音大语言模型聊天机器人可作为常规抑郁症筛查的可扩展、低负担辅助工具。

Abstract: Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively
screen depression but lack interactivity and adaptability. We developed
HopeBot, a chatbot powered by a large language model (LLM) that administers the
PHQ-9 using retrieval-augmented generation and real-time clarification. In a
within-subject study, 132 adults in the United Kingdom and China completed both
self-administered and chatbot versions. Scores demonstrated strong agreement
(ICC = 0.91; 45% identical). Among 75 participants providing comparative
feedback, 71% reported greater trust in the chatbot, highlighting clearer
structure, interpretive guidance, and a supportive tone. Mean ratings (0-10)
were 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,
and 7.4 for recommendation helpfulness; the latter varied significantly by
employment status and prior mental-health service use (p < 0.05). Overall,
87.1% expressed willingness to reuse or recommend HopeBot. These findings
demonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden
adjuncts for routine depression screening.

</details>


### [34] [CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation](https://arxiv.org/abs/2507.06013)
*Kushal Gajjar,Harshit Sikchi,Arpit Singh Gautam,Marc Hammons,Saurabh Jha*

Main category: cs.AI

TL;DR: 介绍CogniSQL - R1 - Zero框架和模型用于Text - to - SQL，在基准测试取得SOTA，还发布两个数据集推动研究。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在将自然语言翻译成SQL，尤其是复杂查询时生成正确可执行SQL的难题。

Method: 引入基于执行正确性和格式标签合规性的轻量级奖励信号的强化学习框架CogniSQL - R1 - Zero，避免中间监督、混合管道和复杂奖励塑造。

Result: CogniSQL - R1 - Zero在Text2SQL基准测试BIRD bench上达到SOTA，优于先前模型，且基于7B主干训练，仅用4个NVIDIA A100 GPU。

Conclusion: 这些贡献推动了可扩展、与执行对齐的Text - to - SQL生成。

Abstract: Translating natural language into SQL (Text-to-SQL) remains a core challenge
at the intersection of language understanding and structured data access.
Although large language models (LLMs) have improved fluency, generating correct
and executable SQL, especially for complex queries, continues to be
challenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL)
framework and model that produces accurate SQL using a lightweight reward
signal based on execution correctness and format-tag compliance. By avoiding
intermediate supervision, hybrid pipelines and complex reward shaping, our
method encourages stable learning and stronger alignment with the ultimate task
objective-producing executable programs. CogniSQL-R1-Zero achieves
state-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench,
outperforming prior supervised and instruction-tuned baselines including SFT
CodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a
significantly smaller 7B backbone. This result underscores the scalability and
efficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs
(40 GB VRAM each). To support further research in efficient and interpretable
Text-to-SQL modeling, we release two curated datasets: (i) a collection of
5,024 reasoning traces with varying context lengths, and (ii) a
positive-sampled corpus of 36,356 corpus of weakly supervised queries, each
annotated with six semantically diverse reasoning paths. Together, these
contributions advance scalable, execution-aligned Text-to-SQL generation.

</details>


### [35] [Feature-Guided Neighbor Selection for Non-Expert Evaluation of Model Predictions](https://arxiv.org/abs/2507.06029)
*Courtney Ford,Mark T. Keane*

Main category: cs.AI

TL;DR: 提出FGNS方法提升可解释AI输出的可解释性，用户研究显示其能提升非专家识别模型错误能力，定量分析表明它选邻居更优，但仍需缩小解释质量与信任差距。


<details>
  <summary>Details</summary>
Motivation: 可解释AI方法难以为无领域专业知识的用户生成清晰、可解释的输出。

Method: 引入特征引导邻居选择（FGNS）的事后方法，利用局部和全局特征重要性选择类代表示例。

Result: 在Kannada脚本分类的用户研究中，FGNS显著提升非专家识别模型错误能力，参与者决策更快更准；定量分析显示FGNS选的邻居更能反映类特征。

Conclusion: FGNS是迈向更符合人类的模型评估的一步，但需进一步缩小解释质量和感知信任之间的差距。

Abstract: Explainable AI (XAI) methods often struggle to generate clear, interpretable
outputs for users without domain expertise. We introduce Feature-Guided
Neighbor Selection (FGNS), a post hoc method that enhances interpretability by
selecting class-representative examples using both local and global feature
importance. In a user study (N = 98) evaluating Kannada script classifications,
FGNS significantly improved non-experts' ability to identify model errors while
maintaining appropriate agreement with correct predictions. Participants made
faster and more accurate decisions compared to those given traditional k-NN
explanations. Quantitative analysis shows that FGNS selects neighbors that
better reflect class characteristics rather than merely minimizing
feature-space distance, leading to more consistent selection and tighter
clustering around class prototypes. These results support FGNS as a step toward
more human-aligned model assessment, although further work is needed to address
the gap between explanation quality and perceived trust.

</details>


### [36] [On Lockean beliefs that are deductively closed and minimal change](https://arxiv.org/abs/2507.06042)
*Tommaso Flaminio,Lluis Godo,Ramón Pino Pérez,Lluis Subirana*

Main category: cs.AI

TL;DR: 本文在洛克论点框架下，解决洛克信念集不满足经典逻辑演绎闭合问题，给出满足闭合的信念集的两种刻画，并提出概率更新方法实现最小信念修正。


<details>
  <summary>Details</summary>
Motivation: 洛克信念集在某些情境（如信念变更理论）使用有局限，不满足经典逻辑演绎闭合。

Method: 提供满足经典逻辑演绎闭合的信念集的两种刻画，并提出一种概率更新方法进行最小信念修正。

Result: 展示了如何通过最小修正使信念集满足演绎闭合。

Conclusion: 给出了满足逻辑演绎闭合的信念集刻画及最小信念修正的概率更新方法。

Abstract: Within the formal setting of the Lockean thesis, an agent belief set is
defined in terms of degrees of confidence and these are described in
probabilistic terms. This approach is of established interest, notwithstanding
some limitations that make its use troublesome in some contexts, like, for
instance, in belief change theory. Precisely, Lockean belief sets are not
generally closed under (classical) logical deduction. The aim of the present
paper is twofold: on one side we provide two characterizations of those belief
sets that are closed under classical logic deduction, and on the other we
propose an approach to probabilistic update that allows us for a minimal
revision of those beliefs, i.e., a revision obtained by making the fewest
possible changes to the existing belief set while still accommodating the new
information. In particular, we show how we can deductively close a belief set
via a minimal revision.

</details>


### [37] [FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language Models](https://arxiv.org/abs/2507.06057)
*Bo Pang,Yalu Ouyang,Hangfei Xu,Ziqi Jia,Panpan Li,Shengzhao Wen,Lu Wang,Shiyong Li,Yanpeng Wang*

Main category: cs.AI

TL;DR: 本文提出金融增强框架FEVO提升大语言模型在金融领域性能，训练FEVO系列模型并评估，结果显示FEVO - R32B表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理进展在金融领域应用有限，该领域需大量特定知识完成任务，因此要提升大语言模型在金融领域的性能。

Method: 开发多阶段增强框架FEVO，通过持续预训练、监督微调、强化学习提升模型性能，利用前沿推理模型和基于规则的过滤创建高质量数据集FEVO - Train。

Result: 从Qwen2.5 - 32B训练出FEVO系列模型，在七个基准测试评估，FEVO - R32B在五个金融基准测试中超越更大模型和专业模型，且比仅用强化学习训练的FEVO - R32B - 0表现更好。

Conclusion: 验证了金融领域知识扩展和结构化逻辑推理提炼的有效性。

Abstract: Advancements in reasoning for large language models (LLMs) have lead to
significant performance improvements for LLMs in various fields such as
mathematics and programming. However, research applying these advances to the
financial domain, where considerable domain-specific knowledge is necessary to
complete tasks, remains limited. To address this gap, we introduce FEVO
(Financial Evolution), a multi-stage enhancement framework developed to enhance
LLM performance in the financial domain. FEVO systemically enhances LLM
performance by using continued pre-training (CPT) to expand financial domain
knowledge, supervised fine-tuning (SFT) to instill structured, elaborate
reasoning patterns, and reinforcement learning (RL) to further integrate the
expanded financial domain knowledge with the learned structured reasoning. To
ensure effective and efficient training, we leverage frontier reasoning models
and rule-based filtering to curate FEVO-Train, high-quality datasets
specifically designed for the different post-training phases. Using our
framework, we train the FEVO series of models -- C32B, S32B, R32B -- from
Qwen2.5-32B and evaluate them on seven benchmarks to assess financial and
general capabilities, with results showing that FEVO-R32B achieves
state-of-the-art performance on five financial benchmarks against much larger
models as well as specialist models. More significantly, FEVO-R32B demonstrates
markedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct
using only RL), thus validating the effectiveness of financial domain knowledge
expansion and structured, logical reasoning distillation

</details>


### [38] [AI-Based Demand Forecasting and Load Balancing for Optimising Energy use in Healthcare Systems: A real case study](https://arxiv.org/abs/2507.06077)
*Iman Rahimi,Isha Patel*

Main category: cs.AI

TL;DR: 本文提出LSTM - GA - SHAP框架用于医疗设施能源管理，LSTM预测性能优，该框架提升能源效率和可持续性，为AI应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 解决医疗设施能源管理中需求波动挑战运营效率和可持续性的问题，传统方法存在不足。

Method: 提出结合LSTM、遗传算法（GA）和SHAP的AI框架，LSTM用于预测，GA优化参数和负载平衡策略，SHAP增强模型透明度。

Result: LSTM在预测复杂非线性需求模式上显著优于ARIMA和Prophet模型，GA优化参数，SHAP解释特征影响。

Conclusion: LSTM - GA - SHAP框架是提高预测准确性、能源效率和可持续性的有效解决方案，为AI在医疗能源管理中的应用奠定基础。

Abstract: This paper tackles the urgent need for efficient energy management in
healthcare facilities, where fluctuating demands challenge operational
efficiency and sustainability. Traditional methods often prove inadequate,
causing inefficiencies and higher costs. To address this, the study presents an
AI-based framework combining Long Short-Term Memory (LSTM), genetic algorithm
(GA), and SHAP (Shapley Additive Explanations), specifically designed for
healthcare energy management. Although LSTM is widely used for time-series
forecasting, its application in healthcare energy prediction remains
underexplored. The results reveal that LSTM significantly outperforms ARIMA and
Prophet models in forecasting complex, non-linear demand patterns. LSTM
achieves a Mean Absolute Error (MAE) of 21.69 and Root Mean Square Error (RMSE)
of 29.96, far better than Prophet (MAE: 59.78, RMSE: 81.22) and ARIMA (MAE:
87.73, RMSE: 125.22), demonstrating superior performance. The genetic algorithm
is applied to optimize model parameters and improve load balancing strategies,
enabling adaptive responses to real-time energy fluctuations. SHAP analysis
further enhances model transparency by explaining the influence of different
features on predictions, fostering trust in decision-making processes. This
integrated LSTM-GA-SHAP approach offers a robust solution for improving
forecasting accuracy, boosting energy efficiency, and advancing sustainability
in healthcare facilities. Future research may explore real-time deployment and
hybridization with reinforcement learning for continuous optimization. Overall,
the study establishes a solid foundation for using AI in healthcare energy
management, highlighting its scalability, efficiency, and resilience potential.

</details>


### [39] [OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety](https://arxiv.org/abs/2507.06134)
*Sanidhya Vijayvargiya,Aditya Bharat Soni,Xuhui Zhou,Zora Zhiruo Wang,Nouha Dziri,Graham Neubig,Maarten Sap*

Main category: cs.AI

TL;DR: 本文介绍了用于评估AI智能体行为的OpenAgentSafety框架，对五个大模型进行评估揭示了安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有AI智能体在现实部署有不安全行为风险，而此前基准测试存在不足，需要新评估框架。

Method: 引入OpenAgentSafety框架，让智能体与真实工具交互，支持多轮多用户任务，结合规则分析与大模型评判检测不安全行为。

Result: 对五个大模型实证分析显示，Claude - Sonnet - 3.7在51.2%安全脆弱任务中有不安全行为，o3 - mini达72.7%。

Conclusion: AI智能体存在关键安全漏洞，在现实部署前需更强保障措施。

Abstract: Recent advances in AI agents capable of solving complex, everyday tasks, from
scheduling to customer service, have enabled deployment in real-world settings,
but their possibilities for unsafe behavior demands rigorous evaluation. While
prior benchmarks have attempted to assess agent safety, most fall short by
relying on simulated environments, narrow task domains, or unrealistic tool
abstractions. We introduce OpenAgentSafety, a comprehensive and modular
framework for evaluating agent behavior across eight critical risk categories.
Unlike prior work, our framework evaluates agents that interact with real
tools, including web browsers, code execution environments, file systems, bash
shells, and messaging platforms; and supports over 350 multi-turn, multi-user
tasks spanning both benign and adversarial user intents. OpenAgentSafety is
designed for extensibility, allowing researchers to add tools, tasks, websites,
and adversarial strategies with minimal effort. It combines rule-based analysis
with LLM-as-judge assessments to detect both overt and subtle unsafe behaviors.
Empirical analysis of five prominent LLMs in agentic scenarios reveals unsafe
behavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7%
with o3-mini, highlighting critical safety vulnerabilities and the need for
stronger safeguards before real-world deployment.

</details>


### [40] [The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains](https://arxiv.org/abs/2507.06187)
*Scott Geng,Hamish Ivison,Chun-Liang Li,Maarten Sap,Jerry Li,Ranjay Krishna,Pang Wei Koh*

Main category: cs.AI

TL;DR: 研究表明成对偏好数据可使模型学习超越单个数据点，提出增量学习假设并验证，实现简单低成本的微调方法。


<details>
  <summary>Details</summary>
Motivation: 在强监督稀缺时，改善语言模型训练数据质量受限，探索利用弱数据提升模型性能的方法。

Method: 提出增量学习假设，通过控制实验和大规模实验验证，用小模型输出配对生成偏好数据对8B模型进行后训练。

Result: 在标准评估套件上，简单方法达到与Tulu 3相当的性能，证明弱教师模型性能差距对提升强学生模型有用。

Conclusion: 模型能从通常被认为弱的成对数据中很好地学习，实现简单低成本的最先进后训练方法。

Abstract: Improvements in language models are often driven by improving the quality of
the data we train them on, which can be limiting when strong supervision is
scarce. In this work, we show that paired preference data consisting of
individually weak data points can enable gains beyond the strength of each
individual data point. We formulate the delta learning hypothesis to explain
this phenomenon, positing that the relative quality delta between points
suffices to drive learning via preference tuning--even when supervised
finetuning on the weak data hurts. We validate our hypothesis in controlled
experiments and at scale, where we post-train 8B models on preference data
generated by pairing a small 3B model's responses with outputs from an even
smaller 1.5B model to create a meaningful delta. Strikingly, on a standard
11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the
performance of Tulu 3, a state-of-the-art open model tuned from the same base
model while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta
learning enables simpler and cheaper open recipes for state-of-the-art
post-training. To better understand delta learning, we prove in logistic
regression that the performance gap between two weak teacher models provides
useful signal for improving a stronger student. Overall, our work shows that
models can learn surprisingly well from paired data that might typically be
considered weak.

</details>


### [41] [Identifiability in Causal Abstractions: A Hierarchy of Criteria](https://arxiv.org/abs/2507.06213)
*Clément Yvernes,Emilie Devijver,Marianne Clausel,Eric Gaussier*

Main category: cs.AI

TL;DR: 文章考虑将因果抽象形式化为因果图集合，引入并形式化可识别性标准，构建层次结构以助于在缺乏完整因果知识时理解因果查询的可识别性。


<details>
  <summary>Details</summary>
Motivation: 实际中完全指定的因果图很少已知，尤其是在复杂或高维场景，需克服此局限。

Method: 将因果抽象形式化为因果图集合，引入并形式化多种可识别性标准，构建标准的层次结构。

Result: 组织可识别性标准形成层次结构，通过文献示例展示框架，并提供在缺乏完整因果知识时推理可识别性的工具。

Conclusion: 层次结构有助于在不同因果知识水平下更清晰地理解可识别内容。

Abstract: Identifying the effect of a treatment from observational data typically
requires assuming a fully specified causal diagram. However, such diagrams are
rarely known in practice, especially in complex or high-dimensional settings.
To overcome this limitation, recent works have explored the use of causal
abstractions-simplified representations that retain partial causal information.
In this paper, we consider causal abstractions formalized as collections of
causal diagrams, and focus on the identifiability of causal queries within such
collections. We introduce and formalize several identifiability criteria under
this setting. Our main contribution is to organize these criteria into a
structured hierarchy, highlighting their relationships. This hierarchical view
enables a clearer understanding of what can be identified under varying levels
of causal knowledge. We illustrate our framework through examples from the
literature and provide tools to reason about identifiability when full causal
knowledge is unavailable.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [42] [MolFORM: Multi-modal Flow Matching for Structure-Based Drug Design](https://arxiv.org/abs/2507.05503)
*Jie Huang,Daiheng Zhang*

Main category: cs.CE

TL;DR: 提出MolFORM生成框架，用多流匹配联合建模分子离散和连续模态，结合DPO微调提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型成SBDD主流方法，替代的非自回归框架探索较少。

Method: 引入MolFORM框架，用多流匹配联合建模离散和连续分子模态，结合基于DPO的偏好引导微调阶段，采用多模态流DPO共同建模策略。

Result: 在多个评估指标上有持续改进。

Conclusion: MolFORM框架结合多流匹配和DPO微调，能有效提升分子生成质量。

Abstract: Structure-based drug design (SBDD) seeks to generate molecules that bind
effectively to protein targets by leveraging their 3D structural information.
While diffusion-based generative models have become the predominant approach
for SBDD, alternative non-autoregressive frameworks remain relatively
underexplored. In this work, we introduce MolFORM, a novel generative framework
that jointly models discrete (atom types) and continuous (3D coordinates)
molecular modalities using multi-flow matching. To further enhance generation
quality, we incorporate a preference-guided fine-tuning stage based on
\textit{Direct Preference Optimization} (DPO), using Vina score as a reward
signal. We propose a multi-modal flow DPO co-modeling strategy that
simultaneously aligns discrete and continuous modalities, leading to consistent
improvements across multiple evaluation metrics.

</details>


### [43] [MCNP-GO: A python package for assembling MCNP input files with a systems engineering approach](https://arxiv.org/abs/2507.05659)
*Alexandre Friou*

Main category: cs.CE

TL;DR: 介绍Python包MCNP - GO，可操作和组装MCNP输入文件，功能丰富且易使用。


<details>
  <summary>Details</summary>
Motivation: 解决管理大型MCNP输入文件数据库的挑战，满足精确建模和设备定位应用需求。

Method: 开发MCNP - GO包，提供重编号、提取子集、转换文件、组装文件等功能，并通过配置管理系统确保可靠性和可追溯性。

Result: 通过断层实验的实际例子展示了MCNP - GO包的效率和用户友好性。

Conclusion: MCNP - GO适合Python知识有限的用户，能有效处理MCNP输入文件。

Abstract: This article introduces MCNP-GO (https://github.com/afriou/mcnpgo), a Python
package designed to manipulate and assemble MCNP input files, allowing users to
assemble a set of independent objects, each described by a valid MCNP file,
into a single cohesive file. This tool is particularly useful for applications
where precise modeling and positioning of equipment are crucial. The package
addresses the challenges of managing large databases of MCNP input files,
ensuring reliability and traceability through configuration management systems.
MCNP-GO provides functionalities such as renumbering, extracting subsets of
files, transforming files, and assembling files while managing collisions and
materials. It also keeps track of the operations performed on files, enhancing
traceability and ease of modification. The article demonstrates the package's
capabilities through a practical example of assembling an MCNP input file for a
tomographic experiment, highlighting its efficiency and user-friendliness.
MCNP-GO is designed for users with minimal Python knowledge.

</details>


### [44] [Bridging Sequential Deep Operator Network and Video Diffusion: Residual Refinement of Spatio-Temporal PDE Solutions](https://arxiv.org/abs/2507.06133)
*Jaewan Park,Farid Ahmed,Kazuma Kobayashi,Seid Koric,Syed Bahauddin Alam,Iwona Jasiuk,Diab Abueidda*

Main category: cs.CE

TL;DR: 本文将条件视频扩散模型用作偏微分方程时空场的物理替代模型，通过两阶段方法提升性能，并在不同基准测试中验证其优势。


<details>
  <summary>Details</summary>
Motivation: 利用视频扩散模型的优势，将其应用于偏微分方程时空场的物理替代。

Method: 采用两阶段替代方法，先使用顺序深度算子网络生成粗略的物理一致先验，再将其输入条件视频扩散模型学习残差。

Result: 在两个不同的基准测试中，混合替代模型始终优于单阶段模型，降低了定量误差，提高了视觉质量。

Conclusion: 基于物理感知先验的扩散能实现局部特征的忠实重建，残差学习可加速收敛和提高精度，且该架构具有广泛适用性。

Abstract: Video-diffusion models have recently set the standard in video generation,
inpainting, and domain translation thanks to their training stability and high
perceptual fidelity. Building on these strengths, we repurpose conditional
video diffusion as a physics surrogate for spatio-temporal fields governed by
partial differential equations (PDEs). Our two-stage surrogate first applies a
Sequential Deep Operator Network (S-DeepONet) to produce a coarse,
physics-consistent prior from the prescribed boundary or loading conditions.
The prior is then passed to a conditional video diffusion model that learns
only the residual: the point-wise difference between the ground truth and the
S-DeepONet prediction. By shifting the learning burden from the full solution
to its much smaller residual space, diffusion can focus on sharpening
high-frequency structures without sacrificing global coherence. The framework
is assessed on two disparate benchmarks: (i) vortex-dominated lid-driven cavity
flow and (ii) tensile plastic deformation of dogbone specimens. Across these
data sets the hybrid surrogate consistently outperforms its single-stage
counterpart, cutting the mean relative L2 error from 4.57% to 0.83% for the
flow problem and from 4.42% to 2.94% for plasticity, a relative improvements of
81.8% and 33.5% respectively. The hybrid approach not only lowers quantitative
errors but also improves visual quality, visibly recovering fine spatial
details. These results show that (i) conditioning diffusion on a physics-aware
prior enables faithful reconstruction of localized features, (ii) residual
learning reduces the problem, accelerating convergence and enhancing accuracy,
and (iii) the same architecture transfers seamlessly from incompressible flow
to nonlinear elasto-plasticity without problem-specific architectural
modifications, highlighting its broad applicability to nonlinear,
time-dependent continua.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [45] [PBE Meets LLM: When Few Examples Aren't Few-Shot Enough](https://arxiv.org/abs/2507.05403)
*Shuning Zhang,Yongjoo Park*

Main category: cs.DB

TL;DR: 评估大语言模型在基于示例编程（PBE）任务上的表现，提出混合方法，结果显示大语言模型有优势但处理模糊任务有困难，混合方法可提升成功率。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在PBE任务上的表现，传统PBE系统有局限性，大语言模型在PBE任务的表现未知。

Method: 对涉及表格数据转换的PBE任务评估大语言模型，测试生成函数准确性，评估不同提示策略，对比有无PBE特定知识的表现，提出先调用传统PBE求解器再使用大语言模型的混合方法。

Result: 大语言模型支持更多样输入格式、准确性更高，但处理模糊任务有困难，混合方法结合双方优势提升了整体成功率。

Conclusion: 大语言模型在PBE任务有优势但有局限，混合方法能改善整体效果。

Abstract: Large language models (LLMs) can generate code from natural language
descriptions. Their performance is typically evaluated using programming
benchmarks that simulate real-world tasks. These benchmarks provide
specifications in the form of docstrings, function signatures, or bug reports.
The model then generates a program, which is tested against predefined test
cases. In contrast, Programming by Example (PBE) uses input-output examples as
the specification. Traditional PBE systems rely on search-based methods over
restricted transformation spaces. They are usually designed for narrow domains
and fixed input formats. It remains unclear how well LLMs perform on PBE tasks.
  In this work, we evaluate LLMs on PBE tasks involving tabular data
transformations. We prompt models to generate functions that convert an input
table to an output table. We test the generated functions on unseen inputs to
measure accuracy. Our study includes multiple LLMs and evaluates different
prompting strategies, such as one-shot vs. multi-try. We also compare
performance with and without PBE-specific knowledge. Finally, we propose a
hybrid method that calls a traditional PBE solver first, and then falls back to
LLMs if necessary. Our results show that LLMs support more diverse input
formats and achieve higher accuracy than conventional methods. However, they
struggle with tasks that contain ambiguity. The hybrid approach improves
overall success by combining the strengths of both approaches.

</details>


### [46] [GTRSS: Graph-based Top-$k$ Representative Similar Subtrajectory Query](https://arxiv.org/abs/2507.05542)
*Mingchang Ge,Liping Wang,Xuemin Lin,Yuang Zhang,Kunming Wang*

Main category: cs.DB

TL;DR: 本文提出图基框架GTRSS解决Top - k代表性相似子轨迹查询问题，实验表明其提升效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹查询方法依赖高成本过滤验证框架，响应慢，需新方法解决Top - k代表性相似子轨迹查询问题。

Method: 提出GTRSS框架，离线阶段构建双层图索引聚类含相似代表子轨迹的轨迹；在线阶段通过图导航获取结果；引入DTSM度量最相似子轨迹对；结合R树、网格过滤和DTSM剪枝规则加速索引构建。

Result: 在真实数据集实验中，GTRSS检索准确率超90%，查询性能最多提升两个数量级。

Conclusion: GTRSS是首个图基的top - k子轨迹搜索解决方案，显著提升效率和准确性。

Abstract: Trajectory mining has attracted significant attention. This paper addresses
the Top-k Representative Similar Subtrajectory Query (TRSSQ) problem, which
aims to find the k most representative subtrajectories similar to a query.
Existing methods rely on costly filtering-validation frameworks, resulting in
slow response times. Addressing this, we propose GTRSS, a novel Graph-based
Top-k Representative Similar Subtrajectory Query framework. During the offline
phase, GTRSS builds a dual-layer graph index that clusters trajectories
containing similar representative subtrajectories. In the online phase, it
efficiently retrieves results by navigating the graph toward query-relevant
clusters, bypassing full-dataset scanning and heavy computation. To support
this, we introduce the Data Trajectory Similarity Metric (DTSM) to measure the
most similar subtrajectory pair. We further combine R-tree and grid filtering
with DTSM pruning rules to speed up index building. To the best of our
knowledge, GTRSS is the first graph-based solution for top-k subtrajectory
search. Experiments on real datasets demonstrate that GTRSS significantly
enhances both efficiency and accuracy, achieving a retrieval accuracy of over
90 percent and up to two orders of magnitude speedup in query performance.

</details>


### [47] [Prompt Migration: Stabilizing GenAI Applications with Evolving Large Language Models](https://arxiv.org/abs/2507.05573)
*Shivani Tripathi,Pushpanjali Nema,Aditya Halder,Shi Qiao,Alekh Jindal*

Main category: cs.DB

TL;DR: 介绍提示迁移概念稳定GenAI应用，以Tursio为例展示其恢复应用一致性，强调提示生命周期管理和测试的重要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型快速演变，提示不一致导致GenAI应用行为不可预测，破坏业务可靠性，需稳定应用。

Method: 引入提示迁移概念，以Tursio企业搜索应用为案例，分析GPT模型升级影响，详细介绍迁移框架（提示重新设计和迁移测试平台）。

Result: 结构化提示迁移可完全恢复因模型漂移而丢失的应用可靠性。

Conclusion: 强调需要进行提示生命周期管理和强大测试以确保GenAI驱动业务应用的可靠性。

Abstract: Generative AI is transforming business applications by enabling natural
language interfaces and intelligent automation. However, the underlying large
language models (LLMs) are evolving rapidly and so prompting them consistently
is a challenge. This leads to inconsistent and unpredictable application
behavior, undermining the reliability that businesses require for
mission-critical workflows. In this paper, we introduce the concept of prompt
migration as a systematic approach to stabilizing GenAI applications amid
changing LLMs. Using the Tursio enterprise search application as a case study,
we analyze the impact of successive GPT model upgrades, detail our migration
framework including prompt redesign and a migration testbed, and demonstrate
how these techniques restore application consistency. Our results show that
structured prompt migration can fully recover the application reliability that
was lost due to model drift. We conclude with practical lessons learned,
emphasizing the need for prompt lifecycle management and robust testing to
ensure dependable GenAI-powered business applications.

</details>


### [48] [Towards an Application-Centric Benchmark Suite for Spatiotemporal Database Systems](https://arxiv.org/abs/2507.05869)
*Tim C. Rese,David Bermbach*

Main category: cs.DB

TL;DR: 因时空数据重要且产量增长，需对时空数据库系统进行基准测试，但现有的研究孤立，本文认为需要以应用为中心的基准测试套件，并确定其需求、讨论挑战和勾勒架构。


<details>
  <summary>Details</summary>
Motivation: 随着时空数据量增长，开发者依赖时空数据库系统，为了解其服务质量并选择最佳系统，需要基准测试，但现有研究不足，所以需要以应用为中心的基准测试套件。

Method: 确定基准测试套件的需求，讨论特定领域的挑战，并勾勒模块化基准测试套件的架构。

Result: 文中未明确提及具体的实验结果。

Conclusion: 以应用为中心的时空数据库系统基准测试套件是迫切需要的。

Abstract: Spatiotemporal data play a key role for mobility-based applications and are
their produced volume is growing continuously, among others, due to the
increased availability of IoT devices.
  When working with spatiotemporal data, developers rely on spatiotemporal
database systems such as PostGIS or MobilityDB.
  For better understanding their quality of service behavior and then choosing
the best system, benchmarking is the go-to approach.
  Unfortunately, existing work in this field studies only small isolated
aspects and a comprehensive application-centric benchmark suite is still
missing.
  In this paper, we argue that an application-centric benchmark suite for
spatiotemporal database systems is urgently needed.
  We identify requirements for such a benchmark suite, discuss domain-specific
challenges, and sketch-out the architecture of a modular benchmarking suite.

</details>


### [49] [Towards Serverless Processing of Spatiotemporal Big Data Queries](https://arxiv.org/abs/2507.06005)
*Diana Baumann,Tim C. Rese,David Bermbach*

Main category: cs.DB

TL;DR: 提出原生无服务器时空数据处理方法，分解查询并行执行，部分解决大数据处理可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有时空数据处理系统基于关系数据库，对大时空数据场景支持有限。

Method: 将查询分解为小的子查询，利用函数即服务平台近即时扩展能力并行执行。

Result: 部分解决了大时空数据处理的可扩展性需求。

Conclusion: 原生无服务器数据处理方法对大时空数据处理有一定效果。

Abstract: Spatiotemporal data are being produced in continuously growing volumes by a
variety of data sources and a variety of application fields rely on rapid
analysis of such data. Existing systems such as PostGIS or MobilityDB usually
build on relational database systems, thus, inheriting their scale-out
characteristics. As a consequence, big spatiotemporal data scenarios still have
limited support even though many query types can easily be parallelized. In
this paper, we propose our vision of a native serverless data processing
approach for spatiotemporal data: We break down queries into small subqueries
which then leverage the near-instant scaling of Function-as-a-Service platforms
to execute them in parallel. With this, we partially solve the scalability
needs of big spatiotemporal data processing.

</details>


### [50] [Data-Semantics-Aware Recommendation of Diverse Pivot Tables](https://arxiv.org/abs/2507.06171)
*Whanhee Cho,Anna Fariha*

Main category: cs.DB

TL;DR: 提出SAGE系统推荐k预算多样化数据透视表，解决传统方法冗余问题，实验表明其优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 数据总结对大数据集很重要，电子表格中确定有用的数据透视表属性组合有挑战，传统工作未充分解决表格多样化问题。

Method: 提出数据语义感知模型衡量表格效用和多样性，使用可扩展贪心算法选择高效用多样化表格集。

Result: 在三个真实数据集上实验表明SAGE优于其他方法，能有效处理高维数据集，案例研究显示其定性效果优于商业软件和大语言模型。

Conclusion: SAGE系统能有效推荐多样化、有洞察力和可解释的数据透视表。

Abstract: Data summarization is essential to discover insights from large datasets. In
a spreadsheets, pivot tables offer a convenient way to summarize tabular data
by computing aggregates over some attributes, grouped by others. However,
identifying attribute combinations that will result in useful pivot tables
remains a challenge, especially for high-dimensional datasets. We formalize the
problem of automatically recommending insightful and interpretable pivot
tables, eliminating the tedious manual process. A crucial aspect of
recommending a set of pivot tables is to diversify them. Traditional works
inadequately address the table-diversification problem, which leads us to
consider the problem of pivot table diversification.
  We present SAGE, a data-semantics-aware system for recommending k-budgeted
diverse pivot tables, overcoming the shortcomings of prior work for top-k
recommendations that cause redundancy. SAGE ensures that each pivot table is
insightful, interpretable, and adaptive to the user's actions and preferences,
while also guaranteeing that the set of pivot tables are different from each
other, offering a diverse recommendation. We make two key technical
contributions: (1) a data-semantics-aware model to measure the utility of a
single pivot table and the diversity of a set of pivot tables, and (2) a
scalable greedy algorithm that can efficiently select a set of diverse pivot
tables of high utility, by leveraging data semantics to significantly reduce
the combinatorial search space. Our extensive experiments on three real-world
datasets show that SAGE outperforms alternative approaches, and efficiently
scales to accommodate high-dimensional datasets. Additionally, we present
several case studies to highlight SAGE's qualitative effectiveness over
commercial software and Large Language Models (LLMs).

</details>


### [51] [SQLBarber: A System Leveraging Large Language Models to Generate Customized and Realistic SQL Workloads](https://arxiv.org/abs/2507.06192)
*Jiale Lao,Immanuel Trummer*

Main category: cs.DB

TL;DR: 提出SQLBarber系统基于大语言模型生成定制且真实的SQL工作负载，在多个方面有优势并经实验验证。


<details>
  <summary>Details</summary>
Motivation: 数据库研发需大量SQL查询进行基准测试，但获取真实查询有隐私问题，现有生成方法定制性和满足现实约束能力有限。

Method: 提出SQLBarber系统，有声明式接口、LLM驱动并带自校正模块的管道、贝叶斯优化器；构建并开源十个基准。

Result: SQLBarber是唯一能生成定制SQL模板的系统，相比现有方法，减少查询生成时间1 - 3个数量级，显著提高与目标成本分布的一致性。

Conclusion: SQLBarber系统有效解决现有SQL生成方法的问题，能生成定制且真实的SQL工作负载。

Abstract: Database research and development often require a large number of SQL queries
for benchmarking purposes. However, acquiring real-world SQL queries is
challenging due to privacy concerns, and existing SQL generation methods are
limited in customization and in satisfying realistic constraints. To address
this issue, we present SQLBarber, a system based on Large Language Models
(LLMs) to generate customized and realistic SQL workloads. SQLBarber (i)
eliminates the need for users to manually craft SQL templates in advance, while
providing the flexibility to accept natural language specifications to
constrain SQL templates, (ii) scales efficiently to generate large volumes of
queries matching any user-defined cost distribution (e.g., cardinality and
execution plan cost), and (iii) uses execution statistics from Amazon Redshift
and Snowflake to derive SQL template specifications and query cost
distributions that reflect real-world query characteristics. SQLBarber
introduces (i) a declarative interface for users to effortlessly generate
customized SQL templates, (ii) an LLM-powered pipeline augmented with a
self-correction module that profiles, refines, and prunes SQL templates based
on query costs, and (iii) a Bayesian Optimizer to efficiently explore different
predicate values and identify a set of queries that satisfy the target cost
distribution. We construct and open-source ten benchmarks of varying difficulty
levels and target query cost distributions based on real-world statistics from
Snowflake and Amazon Redshift. Extensive experiments on these benchmarks show
that SQLBarber is the only system that can generate customized SQL templates.
It reduces query generation time by one to three orders of magnitude, and
significantly improves alignment with the target cost distribution, compared
with existing methods.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [52] [High Order Collaboration-Oriented Federated Graph Neural Network for Accurate QoS Prediction](https://arxiv.org/abs/2507.05308)
*Zehuan Chen,Xiangwei Lai*

Main category: cs.DC

TL;DR: 提出HC - FGNN用于QoS数据预测，实验表明其有高预测精度和隐私保护优势。


<details>
  <summary>Details</summary>
Motivation: 现有基于FGNN的QoS预测器未利用隐式用户 - 用户交互，需改进。

Method: 提出HC - FGNN，按注意力机制放大显式用户 - 服务图以获取高阶协作，利用轻量级消息聚合方式提高计算效率。

Result: 在两个真实应用的QoS数据集上实验，HC - FGNN有高预测精度和隐私保护优势。

Conclusion: HC - FGNN能在保护隐私的同时实现准确的QoS预测。

Abstract: Predicting Quality of Service (QoS) data crucial for cloud service selection,
where user privacy is a critical concern. Federated Graph Neural Networks
(FGNNs) can perform QoS data prediction as well as maintaining user privacy.
However, existing FGNN-based QoS predictors commonly implement on-device
training on scattered explicit user-service graphs, thereby failing to utilize
the implicit user-user interactions. To address this issue, this study proposes
a high order collaboration-oriented federated graph neural network (HC-FGNN) to
obtain accurate QoS prediction with privacy preservation. Concretely, it
magnifies the explicit user-service graphs following the principle of attention
mechanism to obtain the high order collaboration, which reflects the implicit
user-user interactions. Moreover, it utilizes a lightweight-based message
aggregation way to improve the computational efficiency. The extensive
experiments on two QoS datasets from real application indicate that the
proposed HC-FGNN possesses the advantages of high prediction accurate and
privacy protection.

</details>


### [53] [Archetype-Aware Predictive Autoscaling with Uncertainty Quantification for Serverless Workloads on Kubernetes](https://arxiv.org/abs/2507.05653)
*Guilin Zhang,Srinivas Vippagunta,Raghavendra Nandagopal,Suchitra Raman,Jeff Xu,Marcus Pfeiffer,Shree Chatterjee,Ziqi Tan,Wulan Guo,Hailong Jiang*

Main category: cs.DC

TL;DR: 提出AAPA系统处理HPEC平台工作负载管理问题，有较好效果但高负载下资源成本增加


<details>
  <summary>Details</summary>
Motivation: HPEC平台采用无服务器范式，在高效管理动态工作负载并维持服务水平目标上面临挑战

Method: 提出AAPA系统，利用弱监督将超30万个工作负载窗口自动分类为四种原型

Result: 在Azure Functions跟踪数据上评估，AAPA最多减少50%的SLO违规，提高40%的响应时间，尖峰负载下资源成本增加2 - 8倍

Conclusion: AAPA系统能有效提升HPEC平台工作负载管理效果，但在尖峰负载下资源成本较高

Abstract: High-performance extreme computing (HPEC) platforms increasingly adopt
serverless paradigms, yet face challenges in efficiently managing highly
dynamic workloads while maintaining service-level objectives (SLOs). We propose
**AAPA**, an archetype-aware predictive autoscaling system that leverages weak
supervision to automatically classify 300\,000\,+ workload windows into four
archetypes (PERIODIC, SPIKE, RAMP, STATIONARY\_NOISY) with 99.8\% accuracy.
Evaluation on publicly available Azure Functions traces shows that AAPA reduces
SLO violations by up to 50\%, improves response time by 40\%, albeit with a
2--8\,$\times$ increase in resource cost under spike-heavy loads.

</details>


### [54] [Air-FedGA: A Grouping Asynchronous Federated Learning Mechanism Exploiting Over-the-air Computation](https://arxiv.org/abs/2507.05704)
*Qianpiao Ma,Junlong Zhou,Xiangpeng Hou,Jianchun Liu,Hongli Xu,Jianeng Miao,Qingmin Jia*

Main category: cs.DC

TL;DR: 本文提出基于空中计算的分组异步联邦学习机制Air - FedGA，结合空中计算和异步联邦学习优势，解决通信和异构性挑战，理论证明其收敛性，提出算法优化，实验表明能加速模型训练。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临通信资源受限、边缘异构和数据非独立同分布等挑战，空中计算虽能高效利用通信资源但需严格同步，难以在异构场景实现，因此需新机制解决这些问题。

Method: 提出Air - FedGA机制，将工作节点分组，组内进行空中聚合，组与参数服务器异步通信更新全局模型；理论证明收敛性；提出功率控制和工作节点分组算法解决训练时间最小化问题。

Result: 在经典模型和数据集上实验，所提机制和算法相比现有方案能将联邦学习模型训练速度提升29.9% - 71.6%。

Conclusion: Air - FedGA机制和相关算法能有效加速联邦学习模型训练，同时放宽空中聚合技术的同步要求。

Abstract: Federated learning (FL) is a new paradigm to train AI models over distributed
edge devices (i.e., workers) using their local data, while confronting various
challenges including communication resource constraints, edge heterogeneity and
data Non-IID. Over-the-air computation (AirComp) is a promising technique to
achieve efficient utilization of communication resource for model aggregation
by leveraging the superposition property of a wireless multiple access channel
(MAC). However, AirComp requires strict synchronization among edge devices,
which is hard to achieve in heterogeneous scenarios. In this paper, we propose
an AirComp-based grouping asynchronous federated learning mechanism
(Air-FedGA), which combines the advantages of AirComp and asynchronous FL to
address the communication and heterogeneity challenges. Specifically, Air-FedGA
organizes workers into groups and performs over-the-air aggregation within each
group, while groups asynchronously communicate with the parameter server to
update the global model. In this way, Air-FedGA accelerates the FL model
training by over-the-air aggregation, while relaxing the synchronization
requirement of this aggregation technology. We theoretically prove the
convergence of Air-FedGA. We formulate a training time minimization problem for
Air-FedGA and propose the power control and worker grouping algorithm to solve
it, which jointly optimizes the power scaling factors at edge devices, the
denoising factors at the parameter server, as well as the worker grouping
strategy. We conduct experiments on classical models and datasets, and the
results demonstrate that our proposed mechanism and algorithm can speed up FL
model training by 29.9%-71.6% compared with the state-of-the-art solutions.

</details>


### [55] [ECORE: Energy-Conscious Optimized Routing for Deep Learning Models at the Edge](https://arxiv.org/abs/2507.06011)
*Daghash K. Alqahtani,Maria A. Rodriguez,Muhammad Aamir Cheema,Hamid Rezatofighi,Adel N. Toosi*

Main category: cs.DC

TL;DR: 提出ECORE框架，集成动态路由策略处理图像请求，实验显示可降低能耗和延迟，仅小幅降低检测精度。


<details>
  <summary>Details</summary>
Motivation: 边缘计算实时视觉分析任务对资源受限的边缘设备要求高，需联合优化能耗和检测精度。

Method: 提出ECORE框架，集成多种动态路由策略，根据对象特征平衡能效和检测性能。

Result: 在真实数据集上实验，提出的策略比以精度为中心的方法降低45%能耗和49%延迟，检测精度仅损失2%。

Conclusion: 所提上下文感知路由策略能有效优化边缘设备能耗、延迟和检测精度。

Abstract: Edge computing enables data processing closer to the source, significantly
reducing latency an essential requirement for real-time vision-based analytics
such as object detection in surveillance and smart city environments. However,
these tasks place substantial demands on resource constrained edge devices,
making the joint optimization of energy consumption and detection accuracy
critical. To address this challenge, we propose ECORE, a framework that
integrates multiple dynamic routing strategies including estimation based
techniques and a greedy selection algorithm to direct image processing requests
to the most suitable edge device-model pair. ECORE dynamically balances energy
efficiency and detection performance based on object characteristics. We
evaluate our approach through extensive experiments on real-world datasets,
comparing the proposed routers against widely used baseline techniques. The
evaluation leverages established object detection models (YOLO, SSD,
EfficientDet) and diverse edge platforms, including Jetson Orin Nano, Raspberry
Pi 4 and 5, and TPU accelerators. Results demonstrate that our proposed
context-aware routing strategies can reduce energy consumption and latency by
45% and 49%, respectively, while incurring only a 2% loss in detection accuracy
compared to accuracy-centric methods.

</details>


### [56] [Efficient Federated Learning with Timely Update Dissemination](https://arxiv.org/abs/2507.06031)
*Juncheng Jia,Ji Liu,Chao Huo,Yihui Shen,Yang Zhou,Huaiyu Dai,Dejing Dou*

Main category: cs.DC

TL;DR: 提出利用额外下行带宽资源的高效联邦学习方法，含异步（FedASMU）和同步（FedSSMU）两种，理论证明收敛性，实验显示其在准确率和效率上远超基线方法。


<details>
  <summary>Details</summary>
Motivation: 在分布式数据管理中，利用额外下行带宽资源实现及时更新传播，提升联邦学习的准确性和效率。

Method: 先在异步框架实现 FedASMU，包括服务器端动态模型聚合技术和设备端自适应模型调整机制；后扩展到同步上下文得到 FedSSMU。

Result: 理论分析证明方法收敛，六模型和五公共数据集实验表明，FedASMU 和 FedSSMU 在准确率上最高达 145.87%，效率上最高达 97.59%，远超基线方法。

Conclusion: 所提出的 FedASMU 和 FedSSMU 方法能有效提升联邦学习的准确性和效率。

Abstract: Federated Learning (FL) has emerged as a compelling methodology for the
management of distributed data, marked by significant advancements in recent
years. In this paper, we propose an efficient FL approach that capitalizes on
additional downlink bandwidth resources to ensure timely update dissemination.
Initially, we implement this strategy within an asynchronous framework,
introducing the Asynchronous Staleness-aware Model Update (FedASMU), which
integrates both server-side and device-side methodologies. On the server side,
we present an asynchronous FL system model that employs a dynamic model
aggregation technique, which harmonizes local model updates with the global
model to enhance both accuracy and efficiency. Concurrently, on the device
side, we propose an adaptive model adjustment mechanism that integrates the
latest global model with local models during training to further elevate
accuracy. Subsequently, we extend this approach to a synchronous context,
referred to as FedSSMU. Theoretical analyses substantiate the convergence of
our proposed methodologies. Extensive experiments, encompassing six models and
five public datasets, demonstrate that FedASMU and FedSSMU significantly
surpass baseline methods in terms of both accuracy (up to 145.87%) and
efficiency (up to 97.59%).

</details>


### [57] [A Unified Ontology for Scalable Knowledge Graph-Driven Operational Data Analytics in High-Performance Computing Systems](https://arxiv.org/abs/2507.06107)
*Junaid Ahmed Khan,Andrea Bartolini*

Main category: cs.DC

TL;DR: 本文提出首个用于HPC系统ODA的统一本体，对两个公开数据集进行建模，验证并优化，减少存储开销，支持跨系统分析。


<details>
  <summary>Details</summary>
Motivation: HPC系统产生大量异构遥测数据，现有ODA依赖无模式存储方案，本体和知识图谱存在存储开销大、适用性有限等问题，需要高效可靠的遥测分析方法。

Method: 提出统一本体对两个公开ODA数据集进行建模，通过36个能力问题验证，引入建模优化。

Result: 该本体验证有效，知识图谱存储开销相比之前方法最多降低38.84%，根据部署配置还可额外降低26.82%。

Conclusion: 此工作为可扩展的ODA知识图谱铺平道路，支持单系统及跨异构HPC系统分析。

Abstract: Modern high-performance computing (HPC) systems generate massive volumes of
heterogeneous telemetry data from millions of sensors monitoring compute,
memory, power, cooling, and storage subsystems. As HPC infrastructures scale to
support increasingly complex workloads-including generative AI-the need for
efficient, reliable, and interoperable telemetry analysis becomes critical.
Operational Data Analytics (ODA) has emerged to address these demands; however,
the reliance on schema-less storage solutions limits data accessibility and
semantic integration. Ontologies and knowledge graphs (KG) provide an effective
way to enable efficient and expressive data querying by capturing domain
semantics, but they face challenges such as significant storage overhead and
the limited applicability of existing ontologies, which are often tailored to
specific HPC systems only. In this paper, we present the first unified ontology
for ODA in HPC systems, designed to enable semantic interoperability across
heterogeneous data centers. Our ontology models telemetry data from the two
largest publicly available ODA datasets-M100 (Cineca, Italy) and F-DATA
(Fugaku, Japan)-within a single data model. The ontology is validated through
36 competency questions reflecting real-world stakeholder requirements, and we
introduce modeling optimizations that reduce knowledge graph (KG) storage
overhead by up to 38.84% compared to a previous approach, with an additional
26.82% reduction depending on the desired deployment configuration. This work
paves the way for scalable ODA KGs and supports not only analysis within
individual systems, but also cross-system analysis across heterogeneous HPC
systems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [58] [25 Additional Problems -- Extension to the Book "125 Problems in Text Algorithms"](https://arxiv.org/abs/2507.05770)
*Maxime Crochemore,Thierry Lecroq,Wojtek Rytter*

Main category: cs.DS

TL;DR: 本文是关于文本算法的初步内容，是对《125个文本算法问题》的扩展，给出更多带解答的问题，还提及相关资源。


<details>
  <summary>Details</summary>
Motivation: 对《125个文本算法问题》进行扩展，提供更多问题及解答。

Method: 选取满足有挑战性、解答巧妙且只需基础字符串学知识三个标准的问题。

Result: 给出更多带解答的文本算法问题，在相关网页公布了150个问题。

Conclusion: 为文本算法学习者提供了更多资源和有价值的问题。

Abstract: This very preliminary text is related to ``Algorithms on Texts'', also called
``Algorithmic Stringology''. It is an extension of the book ``125 Problems in
Text Algorithms'' providing, in the same compact style, more problems with
solutions. We refer also to the companions to ``Text algorithms'' available at
http://monge.univ-mlv.fr/~mac/CLR/clr1-20.pdf and at the web page
http://125-problems.univ-mlv.fr, where all 150 problems (including the ones
presented here) are briefly announced. The selected problems satisfy three
criteria: challenging, having short tricky solutions and solvable with only
very basic background in stringology. For the basics in stringology we refer to
http://monge.univ-mlv.fr/~mac/CLR/clr1-20.pdf.

</details>


### [59] [Non-Adaptive Evaluation of $k$-of-$n$ Functions: Tight Gap and a Unit-Cost PTAS](https://arxiv.org/abs/2507.05877)
*Mads Anker Nielsen,Lars Rohwedder,Kevin Schewior*

Main category: cs.DS

TL;DR: 研究k - of - n函数的随机布尔函数评估（SBFE）问题非自适应变体，给出2的自适应差距下界，为单位成本情况给出首个PTAS。


<details>
  <summary>Details</summary>
Motivation: 已知自适应测试时有多项式时间精确算法，非自适应变体研究较少，故进行研究。

Method: 先证明自适应差距下界；在单位成本情况，通过猜测里程碑测试建立双边支配概念，结合分解方法和随机移位论证得到多项式时间算法。

Result: 得到2的自适应差距下界，改进单位成本变体的3/2下界；为单位成本情况计算最佳非自适应策略给出PTAS。

Conclusion: 对k - of - n函数的SBFE问题非自适应变体的研究取得进展，得到下界和有效算法。

Abstract: We consider the Stochastic Boolean Function Evaluation (SBFE) problem in the
well-studied case of $k$-of-$n$ functions: There are independent Boolean random
variables $x_1,\dots,x_n$ where each variable $i$ has a known probability $p_i$
of taking value $1$, and a known cost $c_i$ that can be paid to find out its
value. The value of the function is $1$ iff there are at least $k$ $1$s among
the variables. The goal is to efficiently compute a strategy that, at minimum
expected cost, tests the variables until the function value is determined.
While an elegant polynomial-time exact algorithm is known when tests can be
made adaptively, we focus on the non-adaptive variant, for which much less is
known.
  First, we show a clean and tight lower bound of $2$ on the adaptivity gap,
i.e., the worst-case multiplicative loss in the objective function caused by
disallowing adaptivity, of the problem. This improves the tight lower bound of
$3/2$ for the unit-cost variant.
  Second, we give a PTAS for computing the best non-adaptive strategy in the
unit-cost case, the first PTAS for an SBFE problem. At the core, our scheme
establishes a novel notion of two-sided dominance (w.r.t. the optimal solution)
by guessing so-called milestone tests for a set of carefully chosen buckets of
tests. To turn this technique into a polynomial-time algorithm, we use a
decomposition approach paired with a random-shift argument.

</details>


### [60] [Learning-Augmented Online Covering Problems](https://arxiv.org/abs/2507.06032)
*Afrouz Jabal Ameli,Laura Sanita,Moritz Venzin*

Main category: cs.DS

TL;DR: 提出通用简单框架将预测融入在线覆盖问题，可提升竞争比，适用于多种问题。


<details>
  <summary>Details</summary>
Motivation: 解决在线覆盖问题中融入预测以提升算法性能。

Method: 构建一个通用框架，将依赖到达请求数量的在线算法转换为依赖预测误差的算法。

Result: 竞争比在准确预测时突破最坏情况下限，误差增大时平稳下降，适用于多种问题并得到改进和新的界。

Conclusion: 该框架具有通用性和有效性，能提升多种在线覆盖问题的算法性能。

Abstract: We give a very general and simple framework to incorporate predictions on
requests for online covering problems in a rigorous and black-box manner. Our
framework turns any online algorithm with competitive ratio $\rho(k, \cdot)$
depending on $k$, the number of arriving requests, into an algorithm with
competitive ratio of $\rho(\eta, \cdot)$, where $\eta$ is the prediction error.
With accurate enough prediction, the resulting competitive ratio breaks through
the corresponding worst-case online lower bounds, and smoothly degrades as the
prediction error grows. This framework directly applies to a wide range of
well-studied online covering problems such as facility location, Steiner
problems, set cover, parking permit, etc., and yields improved and novel
bounds.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [61] [Fairness-Aware Static and Dynamic Assortment Optimization: Optimal Selection with Balanced Market Share](https://arxiv.org/abs/2507.05606)
*Omar El Housni,Qing Feng,Huseyin Topaloglu*

Main category: cs.GT

TL;DR: 为解决在线零售商优化商品组合时的公平性问题，引入市场份额平衡约束，研究静态和动态商品组合优化问题，给出求解方法和近似算法。


<details>
  <summary>Details</summary>
Motivation: 单纯优化收入会导致产品销售不平衡，引发供应商不满和产品多样性降低，需解决公平性问题。

Method: 在多项logit模型下引入市场份额平衡约束，研究静态和动态场景，静态问题用多项式时间求解，动态问题设计渐近最优策略。

Result: 静态问题可在多项式时间求解，刻画了最优解结构；动态问题设计的策略渐近最优。

Conclusion: 提出的市场份额平衡约束能有效解决商品组合优化中的公平性问题，给出不同场景下的有效求解方法。

Abstract: Assortment optimization is a critical tool for online retailers aiming to
maximize revenue. However, optimizing purely for revenue can lead to imbalanced
sales across products, potentially causing supplier disengagement and reduced
product diversity. To address these fairness concerns, we introduce a market
share balancing constraint that limits the disparity in expected sales between
any two offered products to a factor of a given parameter $\alpha$. We study
both static and dynamic assortment optimization under the multinomial logit
(MNL) model with this fairness constraint. In the static setting, the seller
selects a distribution over assortments that satisfies the market share
balancing constraint while maximizing expected revenue. We show that this
problem can be solved in polynomial time, and we characterize the structure of
the optimal solution: a product is included if and only if its revenue and
preference weight exceed certain thresholds. We further extend our analysis to
settings with additional feasibility constraints on the assortment and
demonstrate that, given a $\beta$-approximation oracle for the constrained
problem, we can construct a $\beta$-approximation algorithm under the fairness
constraint. In the dynamic setting, each product has a finite initial
inventory, and the seller implements a dynamic policy to maximize total
expected revenue while respecting both inventory limits and the market share
balancing constraint in expectation. We design a policy that is asymptotically
optimal, with its approximation ratio converging to one as inventories grow
large.

</details>


### [62] [Minimal balanced collections and their applications to core stability and other topics of game theory](https://arxiv.org/abs/2507.05898)
*Dylan Laplace Mermoud,Michel Grabisch,Peter Sudhölter*

Main category: cs.GT

TL;DR: 本文研究生成最小平衡集合并实现Peleg算法，可生成n=7以内的集合，还提供快速算法检查联盟和博弈的性质，构造算法检查合作博弈核心是否为稳定集。


<details>
  <summary>Details</summary>
Motivation: 最小平衡集合在合作博弈论和离散数学有重要应用，但n>4时其数量未知，需要研究生成方法及相关性质检查算法。

Method: 实现Peleg算法生成最小平衡集合，基于最小平衡集合提供实用算法检查性质，构造算法并依据定理检查核心是否为稳定集。

Result: 能生成n=7以内的所有最小平衡集合，提供的性质检查算法比基于线性规划的方法更快。

Conclusion: 通过相关算法和定理，可有效生成最小平衡集合并检查合作博弈的相关性质，需将平衡集合概念推广到平衡集。

Abstract: Minimal balanced collections are a generalization of partitions of a finite
set of n elements and have important applications in cooperative game theory
and discrete mathematics. However, their number is not known beyond n = 4. In
this paper we investigate the problem of generating minimal balanced
collections and implement the Peleg algorithm, permitting to generate all
minimal balanced collections till n = 7. Secondly, we provide practical
algorithms to check many properties of coalitions and games, based on minimal
balanced collections, in a way which is faster than linear programming-based
methods. In particular, we construct an algorithm to check if the core of a
cooperative game is a stable set in the sense of von Neumann and Morgenstern.
The algorithm implements a theorem according to which the core is a stable set
if and only if a certain nested balancedness condition is valid. The second
level of this condition requires generalizing the notion of balanced collection
to balanced sets.

</details>


### [63] [Rethinking Pricing in Energy Markets: Pay-as-Bid vs Pay-as-Clear](https://arxiv.org/abs/2507.06035)
*Ioannis Caragiannis,Zhile Jiang,Stratis Skoulakis*

Main category: cs.GT

TL;DR: 探讨电力市场PC和PB定价机制，给出两个理论结果，PB在最坏情况表现好，模拟也支持PB优势。


<details>
  <summary>Details</summary>
Motivation: 研究电力市场中广泛采用的Pay - as - Clear (PC)和Pay - as - Bid (PB)定价机制，以最小成本满足能源总需求。

Method: 理论分析得出两个关键结果，并用无后悔学习动态进行大量模拟。

Result: 一是没有机制能统一主导PC或PB；二是最坏情况下PB表现优于PC，模拟显示PB在多市场场景下平均价格更低。

Conclusion: PB对策略操纵具有结构稳健性，有一定优势。

Abstract: The design of energy markets is a subject of ongoing debate, particularly
concerning the choice between the widely adopted Pay-as-Clear (PC) pricing
mechanism and the alternative Pay-as-Bid (PB). These mechanisms determine how
energy producers are compensated: under PC, all selected producers are paid the
market-clearing price (i.e., the highest accepted bid), while under PB, each
selected producer is paid their own submitted bid. The overarching objective is
to meet the total demand for energy at minimal cost in the presence of
strategic behavior. We present two key theoretical results. First, no mechanism
can uniformly dominate PC or PB. This means that for any mechanism
$\mathcal{M}$, there exists a market configuration and a mixed-strategy Nash
equilibrium of PC (respectively for PB) that yields strictly lower total energy
costs than under $\mathcal{M}$. Second, in terms of worst-case equilibrium
outcomes, PB consistently outperforms PC: across all market instances, the
highest possible equilibrium price under PB is strictly lower than that under
PC. This suggests a structural robustness of PB to strategic manipulation.
These theoretical insights are further supported by extensive simulations based
on no-regret learning dynamics, which consistently yield lower average market
prices in several energy market settings.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [64] [Exploring LLM Capabilities in Extracting DCAT-Compatible Metadata for Data Cataloging](https://arxiv.org/abs/2507.05282)
*Lennart Busch,Daniel Tebernum,Gissel Velarde*

Main category: cs.IR

TL;DR: 研究探讨大语言模型（LLMs）能否自动化文本数据元数据维护，测试不同策略，结果显示LLMs能生成可媲美人工的元数据，应用需考虑特定标准和领域背景。


<details>
  <summary>Details</summary>
Motivation: 数据探索效率低，数据目录的元数据创建维护手动耗时且需专业知识，因此研究LLMs自动化文本数据元数据维护。

Method: 测试不同供应商LLMs的零样本和少样本提示策略生成元数据，用微调模型进行分类。

Result: LLMs能生成与人工相当的元数据，大模型表现更好，微调提升分类准确率，少样本提示多数情况效果更好。

Conclusion: LLMs创建元数据快速可靠，但应用需谨慎考虑任务特定标准和领域背景。

Abstract: Efficient data exploration is crucial as data becomes increasingly important
for accelerating processes, improving forecasts and developing new business
models. Data consumers often spend 25-98 % of their time searching for suitable
data due to the exponential growth, heterogeneity and distribution of data.
Data catalogs can support and accelerate data exploration by using metadata to
answer user queries. However, as metadata creation and maintenance is often a
manual process, it is time-consuming and requires expertise. This study
investigates whether LLMs can automate metadata maintenance of text-based data
and generate high-quality DCAT-compatible metadata. We tested zero-shot and
few-shot prompting strategies with LLMs from different vendors for generating
metadata such as titles and keywords, along with a fine-tuned model for
classification. Our results show that LLMs can generate metadata comparable to
human-created content, particularly on tasks that require advanced semantic
understanding. Larger models outperformed smaller ones, and fine-tuning
significantly improves classification accuracy, while few-shot prompting yields
better results in most cases. Although LLMs offer a faster and reliable way to
create metadata, a successful application requires careful consideration of
task-specific criteria and domain context.

</details>


### [65] [A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models](https://arxiv.org/abs/2507.05288)
*Shuliang Liu,Hongyi Liu,Aiwei Liu,Bingchen Duan,Qi Zheng,Yibo Yan,He Geng,Peijie Jiang,Jia Liu,Xuming Hu*

Main category: cs.IR

TL;DR: 论文提出应对大语言模型生成错误信息的主动防御范式，介绍三支柱框架，显示主动防御策略优于传统方法，建议未来研究关注多方面协同设计。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用放大算法生成错误信息的社会风险，传统检测方法难以有效缓解。

Method: 提出三支柱框架，包括知识可信度、推理可靠性和输入鲁棒性；进行现有技术综合调查和比较元分析。

Result: 主动防御策略在预防错误信息方面比传统方法最多提高63%，但存在计算开销和泛化挑战。

Conclusion: 未来研究应专注于协同设计强大知识基础、推理认证和抗攻击接口，以确保大语言模型在不同领域有效对抗错误信息。

Abstract: The widespread deployment of large language models (LLMs) across critical
domains has amplified the societal risks posed by algorithmically generated
misinformation. Unlike traditional false content, LLM-generated misinformation
can be self-reinforcing, highly plausible, and capable of rapid propagation
across multiple languages, which traditional detection methods fail to mitigate
effectively. This paper introduces a proactive defense paradigm, shifting from
passive post hoc detection to anticipatory mitigation strategies. We propose a
Three Pillars framework: (1) Knowledge Credibility, fortifying the integrity of
training and deployed data; (2) Inference Reliability, embedding
self-corrective mechanisms during reasoning; and (3) Input Robustness,
enhancing the resilience of model interfaces against adversarial attacks.
Through a comprehensive survey of existing techniques and a comparative
meta-analysis, we demonstrate that proactive defense strategies offer up to
63\% improvement over conventional methods in misinformation prevention,
despite non-trivial computational overhead and generalization challenges. We
argue that future research should focus on co-designing robust knowledge
foundations, reasoning certification, and attack-resistant interfaces to ensure
LLMs can effectively counter misinformation across varied domains.

</details>


### [66] [Enhancing Learning Path Recommendation via Multi-task Learning](https://arxiv.org/abs/2507.05295)
*Afsana Nasrin,Lijun Qian,Pamela Obiomon,Xishuang Dong*

Main category: cs.IR

TL;DR: 本文提出多任务LSTM模型用于学习路径推荐，在数据集上实验效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习尤其是深度强化学习使学习路径推荐建模更有效，为提升学习路径推荐效果开展研究。

Method: 提出多任务LSTM模型，将学习路径推荐重构为序列到序列预测问题，使用共享LSTM层捕捉共同特征，添加任务特定LSTM层，引入非重复损失避免冗余推荐。

Result: 在ASSIST09数据集上实验，模型显著优于基线方法。

Conclusion: 提出的多任务LSTM模型能有效提升学习路径推荐效果。

Abstract: Personalized learning is a student-centered educational approach that adapts
content, pace, and assessment to meet each learner's unique needs. As the key
technique to implement the personalized learning, learning path recommendation
sequentially recommends personalized learning items such as lectures and
exercises. Advances in deep learning, particularly deep reinforcement learning,
have made modeling such recommendations more practical and effective. This
paper proposes a multi-task LSTM model that enhances learning path
recommendation by leveraging shared information across tasks. The approach
reframes learning path recommendation as a sequence-to-sequence (Seq2Seq)
prediction problem, generating personalized learning paths from a learner's
historical interactions. The model uses a shared LSTM layer to capture common
features for both learning path recommendation and deep knowledge tracing,
along with task-specific LSTM layers for each objective. To avoid redundant
recommendations, a non-repeat loss penalizes repeated items within the
recommended learning path. Experiments on the ASSIST09 dataset show that the
proposed model significantly outperforms baseline methods for the learning path
recommendation.

</details>


### [67] [News Source Citing Patterns in AI Search Systems](https://arxiv.org/abs/2507.05301)
*Kai-Cheng Yang*

Main category: cs.IR

TL;DR: 分析AI搜索系统引用模式，发现不同提供商模型引用新闻源有共性，引用集中且有自由派倾向，用户满意度不受新闻源政治倾向和质量显著影响。


<details>
  <summary>Details</summary>
Motivation: AI搜索系统影响力渐增，但引用模式不明，为填补此空白进行研究。

Method: 分析AI Search Arena数据，含超24000次对话、65000条回复及超366000条引用。

Result: 不同提供商模型引用新闻源有共性，引用集中，有自由派倾向，低可信度源少引用，用户满意度不受新闻源政治倾向和质量显著影响。

Conclusion: 当前AI搜索系统存在显著挑战，对其设计和治理有重要启示。

Abstract: AI-powered search systems are emerging as new information gatekeepers,
fundamentally transforming how users access news and information. Despite their
growing influence, the citation patterns of these systems remain poorly
understood. We address this gap by analyzing data from the AI Search Arena, a
head-to-head evaluation platform for AI search systems. The dataset comprises
over 24,000 conversations and 65,000 responses from models across three major
providers: OpenAI, Perplexity, and Google. Among the over 366,000 citations
embedded in these responses, 9% reference news sources. We find that while
models from different providers cite distinct news sources, they exhibit shared
patterns in citation behavior. News citations concentrate heavily among a small
number of outlets and display a pronounced liberal bias, though low-credibility
sources are rarely cited. User preference analysis reveals that neither the
political leaning nor the quality of cited news sources significantly
influences user satisfaction. These findings reveal significant challenges in
current AI search systems and have important implications for their design and
governance.

</details>


### [68] [PLACE: Prompt Learning for Attributed Community Search](https://arxiv.org/abs/2507.05311)
*Shuheng Fang,Kangfei Zhao,Rener Zhang,Yu Rong,Jeffrey Xu Yu*

Main category: cs.IR

TL;DR: 提出用于属性社区搜索（ACS）的图提示学习框架PLACE，通过交替训练优化参数，设计分治策略提升扩展性，实验表明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为ACS问题提供更有效的解决方案，借鉴NLP中的提示调整技术改进图学习。

Method: 将结构和可学习提示令牌集成到图中形成提示增强图，采用交替训练范式优化提示参数和GNN，设计分治策略提升扩展性。

Result: 在9个真实世界图上的实验显示，PLACE在三种ACS查询中平均F1分数比现有方法高22%。

Conclusion: PLACE是一种有效的用于ACS的图提示学习框架，能有效处理大规模图。

Abstract: In this paper, we propose PLACE (Prompt Learning for Attributed Community
Search), an innovative graph prompt learning framework for ACS. Enlightened by
prompt-tuning in Natural Language Processing (NLP), where learnable prompt
tokens are inserted to contextualize NLP queries, PLACE integrates structural
and learnable prompt tokens into the graph as a query-dependent refinement
mechanism, forming a prompt-augmented graph. Within this prompt-augmented graph
structure, the learned prompt tokens serve as a bridge that strengthens
connections between graph nodes for the query, enabling the GNN to more
effectively identify patterns of structural cohesiveness and attribute
similarity related to the specific query. We employ an alternating training
paradigm to optimize both the prompt parameters and the GNN jointly. Moreover,
we design a divide-and-conquer strategy to enhance scalability, supporting the
model to handle million-scale graphs. Extensive experiments on 9 real-world
graphs demonstrate the effectiveness of PLACE for three types of ACS queries,
where PLACE achieves higher F1 scores by 22% compared to the state-of-the-arts
on average.

</details>


### [69] [Beyond Retrieval: Ensembling Cross-Encoders and GPT Rerankers with LLMs for Biomedical QA](https://arxiv.org/abs/2507.05577)
*Shashank Verma,Fengyi Jiang,Xiangning Xue*

Main category: cs.IR

TL;DR: 本文介绍参与BioASQ 2025 Task13b挑战赛的方法和结果，构建RAG系统回答生物医学问题，在不同任务有相应排名。


<details>
  <summary>Details</summary>
Motivation: 生物医学语义问答对获取文献知识很重要，BioASQ 2025 Task13b挑战赛为该领域发展提供平台。

Method: 构建RAG系统，检索任务用生成密集嵌入和集成微调交叉编码器与大语言模型重排；答案生成采用少量样本提示指令调优的大语言模型。

Result: 检索任务MAP@10为0.1581，排名第10；是/否问题宏F1分数0.95（排名12），事实类问题平均倒数排名0.64（排名1），列表问题平均F1分数0.63（排名5），理想答案ROUGE - SU4 F1分数0.29（排名11）。

Conclusion: 未明确提及，但展示了所构建系统在挑战赛中的表现。

Abstract: Biomedical semantic question answering rooted in information retrieval can
play a crucial role in keeping up to date with vast, rapidly evolving and
ever-growing biomedical literature. A robust system can help researchers,
healthcare professionals and even layman users access relevant knowledge
grounded in evidence. The BioASQ 2025 Task13b Challenge serves as an important
benchmark, offering a competitive platform for advancement of this space. This
paper presents the methodologies and results from our participation in this
challenge where we built a Retrieval-Augmented Generation (RAG) system that can
answer biomedical questions by retrieving relevant PubMed documents and
snippets to generate answers. For the retrieval task, we generated dense
embeddings from biomedical articles for initial retrieval, and applied an
ensemble of finetuned cross-encoders and large language models (LLMs) for
re-ranking to identify top relevant documents. Our solution achieved an MAP@10
of 0.1581, placing 10th on the leaderboard for the retrieval task. For answer
generation, we employed few-shot prompting of instruction-tuned LLMs. Our
system achieved macro-F1 score of 0.95 for yes/no questions (rank 12), Mean
Reciprocal Rank (MRR) of 0.64 for factoid questions (rank 1), mean-F1 score of
0.63 for list questions (rank 5), and ROUGE-SU4 F1 score of 0.29 for ideal
answers (rank 11).

</details>


### [70] [From ID-based to ID-free: Rethinking ID Effectiveness in Multimodal Collaborative Filtering Recommendation](https://arxiv.org/abs/2507.05715)
*Guohao Li,Li Jing,Jia Wu,Xuefei Li,Kai Zhu,Yue He*

Main category: cs.IR

TL;DR: 本文指出ID特征在多模态协同过滤推荐中作用有限，提出无ID的多模态协同过滤推荐基线IDFREE，实验显示其优于现有基于ID的方法。


<details>
  <summary>Details</summary>
Motivation: 揭示现有多模态协同过滤推荐方法过度依赖ID特征的局限性，提升推荐性能。

Method: 提出IDFREE，用多模态特征和位置编码替代ID特征生成无ID嵌入，构建自适应相似图模块和增强的用户 - 项目图编码器，基于对比学习实现跨模态对齐，用Softmax损失作为推荐损失。

Result: 在三个公开数据集上实验表明，IDFREE优于现有基于ID的方法，标准指标平均性能提升72.24%。

Conclusion: ID特征在多模态协同过滤推荐中有局限性，IDFREE方法有效。

Abstract: Most existing multimodal collaborative filtering recommendation (MCFRec)
methods rely heavily on ID features and multimodal content to enhance
recommendation performance. However, this paper reveals that ID features are
effective but have limited benefits in multimodal collaborative filtering
recommendation. Therefore, this paper systematically deconstruct the pros and
cons of ID features: (i) they provide initial embedding but lack semantic
richness, (ii) they provide a unique identifier for each user and item but
hinder generalization to untrained data, and (iii) they assist in aligning and
fusing multimodal features but may lead to representation shift. Based on these
insights, this paper proposes IDFREE, an ID-free multimodal collaborative
Filtering REcommEndation baseline. IDFREE replaces ID features with multimodal
features and positional encodings to generate semantically meaningful ID-free
embeddings. For ID-free multimodal collaborative filtering, it further proposes
an adaptive similarity graph module to construct dynamic user-user and
item-item graphs based on multimodal features. Then, an augmented user-item
graph encoder is proposed to construct more effective user and item encoding.
Finally, IDFREE achieves inter-multimodal alignment based on the contrastive
learning and uses Softmax loss as recommendation loss. Basic experiments on
three public datasets demonstrate that IDFREE outperforms existing ID-based
MCFRec methods, achieving an average performance gain of 72.24% across standard
metrics (Recall@5, 10, 20, 50 and NDCG@5, 10, 20, 50). Exploratory and extended
experiments further validate our findings on the limitations of ID features in
MCFRec. The code is released at https://github.com/G-H-Li/IDFREE.

</details>


### [71] [On the Costs and Benefits of Learned Indexing for Dynamic High-Dimensional Data: Extended Version](https://arxiv.org/abs/2507.05865)
*Terézia Slanináková,Jaroslav Olha,David Procházka,Matej Antol,Vlastislav Dohnal*

Main category: cs.IR

TL;DR: 本文探索静态学习索引的动态化以适应动态扩展数据集，引入成本模型评估静态和动态方法权衡，实验证明动态学习索引在数据库增长时整体成本更优。


<details>
  <summary>Details</summary>
Motivation: 解决学习索引领域缺乏对动态扩展数据集适应性的问题。

Method: 通过节点分裂和扩展等操作实现静态学习索引动态化，引入摊销成本模型评估查询性能和索引构建成本。

Result: 将动态化方法应用于静态学习索引，随着数据库增长，动态学习索引在整体成本上快速超越静态实现。

Conclusion: 动态学习索引在处理动态扩展数据集时优于静态学习索引。

Abstract: One of the main challenges within the growing research area of learned
indexing is the lack of adaptability to dynamically expanding datasets. This
paper explores the dynamization of a static learned index for complex data
through operations such as node splitting and broadening, enabling efficient
adaptation to new data. Furthermore, we evaluate the trade-offs between static
and dynamic approaches by introducing an amortized cost model to assess query
performance in tandem with the build costs of the index structure, enabling
experimental determination of when a dynamic learned index outperforms its
static counterpart. We apply the dynamization method to a static learned index
and demonstrate that its superior scaling quickly surpasses the static
implementation in terms of overall costs as the database grows. This is an
extended version of the paper presented at DAWAK 2025.

</details>


### [72] [When Transformers Meet Recommenders: Integrating Self-Attentive Sequential Recommendation with Fine-Tuned LLMs](https://arxiv.org/abs/2507.05733)
*Kechen Liu*

Main category: cs.IR

TL;DR: 本文提出SASRecLLM框架融合SASRec和LLM用于推荐系统，实验表明其在冷启动和热启动场景表现出色。


<details>
  <summary>Details</summary>
Motivation: LLM在推荐系统中缺乏特定领域知识和协作信号，需结合其他方法提升推荐质量。

Method: 提出SASRecLLM框架，将SASRec作为协作编码器与通过LoRA微调的LLM集成，用映射层连接组件，设计三种训练策略优化架构。

Result: 在多个数据集上的实验显示，SASRecLLM在冷启动和热启动场景下相比基线模型有显著且稳定的提升。

Conclusion: 该工作为结构化协同过滤与微调后LLM语义能力融合提供了模块化且有效的范式，推动了基于LLM的推荐系统领域发展。

Abstract: Self-Attentive Sequential Recommendation (SASRec) effectively captures
long-term user preferences by applying attention mechanisms to historical
interactions. Concurrently, the rise of Large Language Models (LLMs) has
motivated research into LLM-based recommendation, which leverages their
powerful generalization and language understanding capabilities. However, LLMs
often lack the domain-specific knowledge and collaborative signals essential
for high-quality recommendations when relying solely on textual prompts. To
address this limitation, this study proposes SASRecLLM, a novel framework that
integrates SASRec as a collaborative encoder with an LLM fine-tuned using
Low-Rank Adaptation (LoRA). The components are connected via a mapping layer to
align their dimensional spaces, and three targeted training strategies are
designed to optimize the hybrid architecture. Extensive experiments on multiple
datasets demonstrate that SASRecLLM achieves robust and consistent improvements
over strong baselines in both cold-start and warm-start scenarios. This work
advances the field of LLM-based recommendation by presenting a modular and
effective paradigm for fusing structured collaborative filtering with the
semantic power of fine-tuned LLMs. The implementation is available on GitHub:
https://github.com/kechenkristin/RecLLM

</details>


### [73] [Vers un cadre ontologique pour la gestion des comp{é}tences : {à} des fins de formation, de recrutement, de m{é}tier, ou de recherches associ{é}es](https://arxiv.org/abs/2507.05767)
*Ngoc Luyen Le,Marie-Hélène Abel,Bertrand Laforge*

Main category: cs.IR

TL;DR: 因科技和数字经济使劳动力市场转变，传统能力管理系统有缺陷，本文提出基于本体的能力管理框架并探讨其应用。


<details>
  <summary>Details</summary>
Motivation: 科技和数字经济推动劳动力市场快速转变，传统能力管理系统缺乏互操作性、适应性和语义理解，难以使个人能力与市场需求和培训计划匹配。

Method: 提出基于本体的能力管理框架，利用本体模型和语义推理。

Result: 无明确提及具体结果。

Conclusion: 该框架可增强能力与工作匹配的自动化、学习推荐的个性化和职业规划，适用于能力培训计划、求职和寻找合适人才等方面。

Abstract: The rapid transformation of the labor market, driven by technological
advancements and the digital economy, requires continuous competence
development and constant adaptation. In this context, traditional competence
management systems lack interoperability, adaptability, and semantic
understanding, making it difficult to align individual competencies with labor
market needs and training programs. This paper proposes an ontology-based
framework for competence management, enabling a structured representation of
competencies, occupations, and training programs. By leveraging ontological
models and semantic reasoning, this framework aims to enhance the automation of
competence-to-job matching, the personalization of learning recommendations,
and career planning. This study discusses the design, implementation, and
potential applications of the framework, focusing on competence training
programs, job searching, and finding competent individuals.

</details>


### [74] [KERAG_R: Knowledge-Enhanced Retrieval-Augmented Generation for Recommendation](https://arxiv.org/abs/2507.05863)
*Zeyuan Meng,Zixuan Yi,Iadh Ounis*

Main category: cs.IR

TL;DR: 文章指出现有基于大语言模型的推荐方法受特定领域知识缺失限制，提出知识增强检索增强生成推荐模型KERAG_R，实验表明该模型显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的推荐方法因缺乏特定领域知识，存在不准确、易产生幻觉等问题，直接使用知识图谱信息会引入冗余和噪声，影响推荐性能。

Method: 提出KERAG_R模型，利用图检索增强生成组件将知识图谱信息融入指令，预训练图注意力网络选择最相关三元组，增强大语言模型并减少冗余噪声。

Result: 在三个公开数据集上的实验表明，KERAG_R模型显著优于十种现有先进推荐方法。

Conclusion: KERAG_R模型能有效解决基于大语言模型推荐方法缺乏特定领域知识的问题，提升推荐性能。

Abstract: Large Language Models (LLMs) have shown strong potential in recommender
systems due to their contextual learning and generalisation capabilities.
Existing LLM-based recommendation approaches typically formulate the
recommendation task using specialised prompts designed to leverage their
contextual abilities, and aligning their outputs closely with human preferences
to yield an improved recommendation performance. However, the use of LLMs for
recommendation tasks is limited by the absence of domain-specific knowledge.
This lack of relevant relational knowledge about the items to be recommended in
the LLM's pre-training corpus can lead to inaccuracies or hallucinations,
resulting in incorrect or misleading recommendations. Moreover, directly using
information from the knowledge graph introduces redundant and noisy
information, which can affect the LLM's reasoning process or exceed its input
context length, thereby reducing the performance of LLM-based recommendations.
To address the lack of domain-specific knowledge, we propose a novel model
called Knowledge-Enhanced Retrieval-Augmented Generation for Recommendation
(KERAG_R). Specifically, we leverage a graph retrieval-augmented generation
(GraphRAG) component to integrate additional information from a knowledge graph
(KG) into instructions, enabling the LLM to collaboratively exploit
recommendation signals from both text-based user interactions and the knowledge
graph to better estimate the users' preferences in a recommendation context. In
particular, we perform graph RAG by pre-training a graph attention network
(GAT) to select the most relevant triple for the target users for the used LLM,
thereby enhancing the LLM while reducing redundant and noisy information. Our
extensive experiments on three public datasets show that our proposed KERAG_R
model significantly outperforms ten existing state-of-the-art recommendation
methods.

</details>


### [75] [RecRankerEval: A Flexible and Extensible Framework for Top-k LLM-based Recommendation](https://arxiv.org/abs/2507.05880)
*Zeyuan Meng,Zixuan Yi,Iadh Ounis*

Main category: cs.IR

TL;DR: 研究RecRanker可复现性及各组件作用，提出RecRankerEval框架评估，展示不同数据集复现情况并给出性能提升方法。


<details>
  <summary>Details</summary>
Motivation: 探索RecRanker各核心组件贡献，实现公平全面评估基于大语言模型的top - k推荐。

Method: 复现RecRanker流程，提出涵盖五个关键维度的RecRankerEval框架评估。

Result: 成对和列表方法性能与原论文相当；点对方法因数据泄露性能异常高；RecRanker在部分数据集可复现；采用替代方法和更强模型可提升性能。

Conclusion: 研究明确了RecRanker各组件影响，RecRankerEval框架可有效评估，部分数据集复现存在问题，可通过多种方式提升性能。

Abstract: A recent Large language model (LLM)-based recommendation model, called
RecRanker, has demonstrated a superior performance in the top-k recommendation
task compared to other models. In particular, RecRanker samples users via
clustering, generates an initial ranking list using an initial recommendation
model, and fine-tunes an LLM through hybrid instruction tuning to infer user
preferences. However, the contribution of each core component remains
underexplored. In this work, we inspect the reproducibility of RecRanker, and
study the impact and role of its various components. We begin by reproducing
the RecRanker pipeline through the implementation of all its key components.
Our reproduction shows that the pairwise and listwise methods achieve a
performance comparable to that reported in the original paper. For the
pointwise method, while we are also able to reproduce the original paper's
results, further analysis shows that the performance is abnormally high due to
data leakage from the inclusion of ground-truth information in the prompts. To
enable a fair and comprehensive evaluation of LLM-based top-k recommendations,
we propose RecRankerEval, an extensible framework that covers five key
dimensions: user sampling strategy, initial recommendation model, LLM backbone,
dataset selection, and instruction tuning method. Using the RecRankerEval
framework, we show that the original results of RecRanker can be reproduced on
the ML-100K and ML-1M datasets, as well as the additional Amazon-Music dataset,
but not on BookCrossing due to the lack of timestamp information in the
original RecRanker paper. Furthermore, we demonstrate that RecRanker's
performance can be improved by employing alternative user sampling methods,
stronger initial recommenders, and more capable LLMs.

</details>


### [76] [Semantic Certainty Assessment in Vector Retrieval Systems: A Novel Framework for Embedding Quality Evaluation](https://arxiv.org/abs/2507.05933)
*Y. Du*

Main category: cs.IR

TL;DR: 提出轻量级框架预测向量检索系统查询级别的检索性能，在4个标准数据集上评估，召回率提升且开销小，还为数据增强提供见解。


<details>
  <summary>Details</summary>
Motivation: 向量检索系统因嵌入质量异质性在不同查询上性能差异大，高质量嵌入在嵌入空间有几何稳定区域和一致邻域结构。

Method: 结合量化鲁棒性和邻域密度指标预测查询级别的检索性能。

Result: 在4个标准检索数据集上，Recall@10比竞争基线平均提升9.4±1.2%，框架计算开销小于检索时间的5%。

Conclusion: 该框架可实现自适应检索策略，分析揭示了不同查询类型嵌入质量的系统模式，为有针对性的训练数据增强提供见解。

Abstract: Vector retrieval systems exhibit significant performance variance across
queries due to heterogeneous embedding quality. We propose a lightweight
framework for predicting retrieval performance at the query level by combining
quantization robustness and neighborhood density metrics. Our approach is
motivated by the observation that high-quality embeddings occupy geometrically
stable regions in the embedding space and exhibit consistent neighborhood
structures. We evaluate our method on 4 standard retrieval datasets, showing
consistent improvements of 9.4$\pm$1.2\% in Recall@10 over competitive
baselines. The framework requires minimal computational overhead (less than 5\%
of retrieval time) and enables adaptive retrieval strategies. Our analysis
reveals systematic patterns in embedding quality across different query types,
providing insights for targeted training data augmentation.

</details>


### [77] [Hierarchical Interaction Summarization and Contrastive Prompting for Explainable Recommendations](https://arxiv.org/abs/2507.06044)
*Yibin Liu,Ang Li,Shijian Li*

Main category: cs.IR

TL;DR: 本文提出PGHIS和CPEG方法，结合二者微调大语言模型生成推荐解释，实验显示该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有可解释推荐方法依赖特征编码为嵌入向量，会造成信息损失，使用嵌入向量作为大语言模型输入也会导致信息丢失。

Method: 提出PGHIS方法生成结构化文本描述，提出CPEG方法生成高质量推荐解释，用文本描述和解释微调大语言模型。

Result: 在多个数据集上优于现有方法，在可解释性和文本质量指标上有显著提升，生成的解释质量高。

Conclusion: 所提方法能有效解决现有可解释推荐方法的信息损失问题，提升推荐解释质量。

Abstract: Explainable recommendations, which use the information of user and item with
interaction to generate a explanation for why the user would interact with the
item, are crucial for improving user trust and decision transparency to the
recommender system. Existing methods primarily rely on encoding features of
users and items to embeddings, which often leads to information loss due to
dimensionality reduction, sparse interactions, and so on. With the advancements
of large language models (LLMs) in language comprehension, some methods use
embeddings as LLM inputs for explanation generation. However, since embeddings
lack inherent semantics, LLMs must adjust or extend their parameters to
interpret them, a process that inevitably incurs information loss. To address
this issue, we propose a novel approach combining profile generation via
hierarchical interaction summarization (PGHIS), which leverages a pretrained
LLM to hierarchically summarize user-item interactions, generating structured
textual profiles as explicit representations of user and item characteristics.
Additionally, we propose contrastive prompting for explanation generation
(CPEG) which employs contrastive learning to guide another reasoning language
models in producing high-quality ground truth recommendation explanations.
Finally, we use the textual profiles of user and item as input and high-quality
explanation as output to fine-tune a LLM for generating explanations.
Experimental results on multiple datasets demonstrate that our approach
outperforms existing state-of-the-art methods, achieving a great improvement on
metrics about explainability (e.g., 5% on GPTScore) and text quality.
Furthermore, our generated ground truth explanations achieve a significantly
higher win rate compared to user-written reviews and those produced by other
methods, demonstrating the effectiveness of CPEG in generating high-quality
ground truths.

</details>


### [78] [Nyay-Darpan: Enhancing Decision Making Through Summarization and Case Retrieval for Consumer Law in India](https://arxiv.org/abs/2507.06090)
*Swapnil Bhattacharyya,Shrey Ganatra,Harshvivek Kashid,Spandan Anaokar,Shruti Nair,Reshma Sekhar,Siddharth Manohar,Rahul Hemrajani,Pushpak Bhattacharyya*

Main category: cs.IR

TL;DR: 本文提出Nyay - Darpan框架用于消费者纠纷解决，实现案件摘要和类似案件检索，有较高准确率，将公开框架和数据集。


<details>
  <summary>Details</summary>
Motivation: AI在消费者法尤其是印度消费者法领域研究不足，需开发相关工具。

Method: 提出Nyay - Darpan两合一框架，创新评估摘要质量的方法。

Result: 系统在类似案件预测中准确率超75%，材料摘要评估指标准确率约70%。

Conclusion: Nyay - Darpan框架有效，将公开框架和数据集以推动该领域研究。

Abstract: AI-based judicial assistance and case prediction have been extensively
studied in criminal and civil domains, but remain largely unexplored in
consumer law, especially in India. In this paper, we present Nyay-Darpan, a
novel two-in-one framework that (i) summarizes consumer case files and (ii)
retrieves similar case judgements to aid decision-making in consumer dispute
resolution. Our methodology not only addresses the gap in consumer law AI tools
but also introduces an innovative approach to evaluate the quality of the
summary. The term 'Nyay-Darpan' translates into 'Mirror of Justice',
symbolizing the ability of our tool to reflect the core of consumer disputes
through precise summarization and intelligent case retrieval. Our system
achieves over 75 percent accuracy in similar case prediction and approximately
70 percent accuracy across material summary evaluation metrics, demonstrating
its practical effectiveness. We will publicly release the Nyay-Darpan framework
and dataset to promote reproducibility and facilitate further research in this
underexplored yet impactful domain.

</details>


### [79] [Unconditional Diffusion for Generative Sequential Recommendation](https://arxiv.org/abs/2507.06121)
*Yimeng Bai,Yang Zhang,Sihao Ding,Shaohui Ruan,Han Yao,Danhui Guan,Fuli Feng,Tat-Seng Chua*

Main category: cs.IR

TL;DR: 本文提出了Brownian Bridge Diffusion Recommendation (BBDRec)用于序列推荐，将用户历史作为正向扩散过程的终点，实验证明其能提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于条件扩散框架的方法在序列推荐中未能充分利用用户历史信息，易受'item ↔ noise'转换的干扰。

Method: 提出BBDRec，利用布朗桥过程，强制结构化的噪声添加和去噪机制，使轨迹受限于用户历史而非噪声。

Result: 广泛实验表明BBDRec能有效提升序列推荐性能。

Conclusion: BBDRec在序列推荐中是有效的，其代码可在https://github.com/baiyimeng/BBDRec获取。

Abstract: Diffusion models, known for their generative ability to simulate data
creation through noise-adding and denoising processes, have emerged as a
promising approach for building generative recommenders. To incorporate user
history for personalization, existing methods typically adopt a conditional
diffusion framework, where the reverse denoising process of reconstructing
items from noise is modified to be conditioned on the user history. However,
this design may fail to fully utilize historical information, as it gets
distracted by the need to model the "item $\leftrightarrow$ noise" translation.
This motivates us to reformulate the diffusion process for sequential
recommendation in an unconditional manner, treating user history (instead of
noise) as the endpoint of the forward diffusion process (i.e., the starting
point of the reverse process), rather than as a conditional input. This
formulation allows for exclusive focus on modeling the "item $\leftrightarrow$
history" translation. To this end, we introduce Brownian Bridge Diffusion
Recommendation (BBDRec). By leveraging a Brownian bridge process, BBDRec
enforces a structured noise addition and denoising mechanism, ensuring that the
trajectories are constrained towards a specific endpoint -- user history,
rather than noise. Extensive experiments demonstrate BBDRec's effectiveness in
enhancing sequential recommendation performance. The source code is available
at https://github.com/baiyimeng/BBDRec.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [80] [Rethinking Over-Smoothing in Graph Neural Networks: A Perspective from Anderson Localization](https://arxiv.org/abs/2507.05263)
*Kaichen Ouyang*

Main category: cs.LG

TL;DR: 本文通过与安德森局域化类比分析图神经网络过平滑机制，引入参与度量化该现象，探讨缓解过平滑的可能性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络随着深度增加过平滑问题严重，节点表示失去独特性，需分析机制并解决。

Method: 将图神经网络过平滑与无序系统中的安德森局域化类比，引入参与度量化，进行理论分析。

Result: 发现图神经网络过平滑可理解为低频模式扩展和高频模式局域化，二者存在潜在联系。

Conclusion: 提出通过减少信息传播中的无序性来缓解过平滑的可能性。

Abstract: Graph Neural Networks (GNNs) have shown great potential in graph data
analysis due to their powerful representation capabilities. However, as the
network depth increases, the issue of over-smoothing becomes more severe,
causing node representations to lose their distinctiveness. This paper analyzes
the mechanism of over-smoothing through the analogy to Anderson localization
and introduces participation degree as a metric to quantify this phenomenon.
Specifically, as the depth of the GNN increases, node features homogenize after
multiple layers of message passing, leading to a loss of distinctiveness,
similar to the behavior of vibration modes in disordered systems. In this
context, over-smoothing in GNNs can be understood as the expansion of
low-frequency modes (increased participation degree) and the localization of
high-frequency modes (decreased participation degree). Based on this, we
systematically reviewed the potential connection between the Anderson
localization behavior in disordered systems and the over-smoothing behavior in
Graph Neural Networks. A theoretical analysis was conducted, and we proposed
the potential of alleviating over-smoothing by reducing the disorder in
information propagation.

</details>


### [81] [Temporal Window Smoothing of Exogenous Variables for Improved Time Series Prediction](https://arxiv.org/abs/2507.05284)
*Mustafa Kamal,Niyaz Bin Hashem,Robin Krambroeckers,Nabeel Mohammed,Shafin Rahman*

Main category: cs.LG

TL;DR: 提出一种处理外生输入的方法，在四个基准数据集上达SOTA，优于11个基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于transformer的时间序列预测模型使用外生输入存在冗余及捕捉长期依赖能力有限的问题。

Method: 对外生输入进行白化处理以减少数据冗余，使其更能捕捉长期模式和趋势，将优化后的外生输入引入内生输入。

Result: 在四个基准数据集上达到SOTA，始终优于11个基线模型。

Conclusion: 该方法是时间序列预测中使用外生输入的强大有效替代方案。

Abstract: Although most transformer-based time series forecasting models primarily
depend on endogenous inputs, recent state-of-the-art approaches have
significantly improved performance by incorporating external information
through exogenous inputs. However, these methods face challenges, such as
redundancy when endogenous and exogenous inputs originate from the same source
and limited ability to capture long-term dependencies due to fixed look-back
windows. In this paper, we propose a method that whitens the exogenous input to
reduce redundancy that may persist within the data based on global statistics.
Additionally, our approach helps the exogenous input to be more aware of
patterns and trends over extended periods. By introducing this refined,
globally context-aware exogenous input to the endogenous input without
increasing the lookback window length, our approach guides the model towards
improved forecasting. Our approach achieves state-of-the-art performance in
four benchmark datasets, consistently outperforming 11 baseline models. These
results establish our method as a robust and effective alternative for using
exogenous inputs in time series forecasting.

</details>


### [82] [Compressing Deep Neural Networks Using Explainable AI](https://arxiv.org/abs/2507.05286)
*Kimia Soroush,Mohsen Raji,Behnam Ghavami*

Main category: cs.LG

TL;DR: 提出用XAI的DNN压缩方法，计算参数重要性得分，进行剪枝和量化，实验显示减少64%模型大小且准确率提升42%。


<details>
  <summary>Details</summary>
Motivation: DNN计算成本高、内存使用大，需压缩技术以适应资源受限设备，同时XAI可助理解DNN，故结合XAI提出新压缩方法。

Method: 用基于梯度的XAI技术LRP计算DNN参数重要性得分，对负或零得分参数剪枝，用混合精度量化处理不同得分权重。

Result: 相比现有XAI压缩方法，模型大小减少64%，准确率提升42%。

Conclusion: 所提DNN压缩方法能有效减少模型大小且准确率损失可忽略甚至有提升。

Abstract: Deep neural networks (DNNs) have demonstrated remarkable performance in many
tasks but it often comes at a high computational cost and memory usage.
Compression techniques, such as pruning and quantization, are applied to reduce
the memory footprint of DNNs and make it possible to accommodate them on
resource-constrained edge devices. Recently, explainable artificial
intelligence (XAI) methods have been introduced with the purpose of
understanding and explaining AI methods. XAI can be utilized to get to know the
inner functioning of DNNs, such as the importance of different neurons and
features in the overall performance of DNNs. In this paper, a novel DNN
compression approach using XAI is proposed to efficiently reduce the DNN model
size with negligible accuracy loss. In the proposed approach, the importance
score of DNN parameters (i.e. weights) are computed using a gradient-based XAI
technique called Layer-wise Relevance Propagation (LRP). Then, the scores are
used to compress the DNN as follows: 1) the parameters with the negative or
zero importance scores are pruned and removed from the model, 2)
mixed-precision quantization is applied to quantize the weights with
higher/lower score with higher/lower number of bits. The experimental results
show that, the proposed compression approach reduces the model size by 64%
while the accuracy is improved by 42% compared to the state-of-the-art
XAI-based compression method.

</details>


### [83] [Dataless Neural Networks for Resource-Constrained Project Scheduling](https://arxiv.org/abs/2507.05322)
*Marc Bara*

Main category: cs.LG

TL;DR: 提出首个用于资源受限项目调度问题（RCPSP）的无数据神经网络方法，给出数学框架，相关实验正在进行。


<details>
  <summary>Details</summary>
Motivation: 现有无数据方法未应用于RCPSP，填补该领域空白。

Method: 将离散调度约束转化为可微目标，利用平滑松弛和自动微分实现GPU并行化，详细给出约束的数学公式及高效表示。

Result: 相关实验正在PSPLIB基准实例上进行，结果待更新报告。

Conclusion: 未提及最终结论，待实验结果出来后确定方法有效性。

Abstract: Dataless neural networks represent a paradigm shift in applying neural
architectures to combinatorial optimization problems, eliminating the need for
training datasets by encoding problem instances directly into network
parameters. Despite the pioneering work of Alkhouri et al. (2022) demonstrating
the viability of dataless approaches for the Maximum Independent Set problem,
our comprehensive literature review reveals that no published work has extended
these methods to the Resource-Constrained Project Scheduling Problem (RCPSP).
This paper addresses this gap by presenting the first dataless neural network
approach for RCPSP, providing a complete mathematical framework that transforms
discrete scheduling constraints into differentiable objectives suitable for
gradient-based optimization. Our approach leverages smooth relaxations and
automatic differentiation to unlock GPU parallelization for project scheduling,
traditionally a domain of sequential algorithms. We detail the mathematical
formulation for both precedence and renewable resource constraints, including a
memory-efficient dense time-grid representation. Implementation and
comprehensive experiments on PSPLIB benchmark instances (J30, J60, and J120)
are currently underway, with empirical results to be reported in an updated
version of this paper.

</details>


### [84] [Physics-Informed Graph Neural Networks to Reconstruct Local Fields Considering Finite Strain Hyperelasticity](https://arxiv.org/abs/2507.05291)
*Manuel Ricardo Guevara Garban,Yves Chemisky,Étienne Prulière,Michaël Clément*

Main category: cs.LG

TL;DR: 提出P - DivGNN框架用于微观尺度局部应力场重建，结合图神经网络，在非线性超弹性情况有计算加速优势。


<details>
  <summary>Details</summary>
Motivation: 在多尺度模拟中，考虑到断裂分析和定义局部疲劳准则，需预测微观尺度的局部应力场分布。

Method: 将周期性微观结构表示为图，结合消息传递图神经网络，训练时加入物理约束确保局部应力场平衡，采用周期性图表示实现周期性边界条件。

Result: 能根据宏观尺度平均应力值获取局部应力场分布，在非线性超弹性情况与有限元模拟相比有显著计算加速。

Conclusion: 所提物理信息图神经网络方法在大规模应用中有吸引力，尤其在非线性超弹性响应场景。

Abstract: We propose a physics-informed machine learning framework called P-DivGNN to
reconstruct local stress fields at the micro-scale, in the context of
multi-scale simulation given a periodic micro-structure mesh and mean,
macro-scale, stress values. This method is based in representing a periodic
micro-structure as a graph, combined with a message passing graph neural
network. We are able to retrieve local stress field distributions, providing
average stress values produced by a mean field reduced order model (ROM) or
Finite Element (FE) simulation at the macro-scale. The prediction of local
stress fields are of utmost importance considering fracture analysis or the
definition of local fatigue criteria. Our model incorporates physical
constraints during training to constraint local stress field equilibrium state
and employs a periodic graph representation to enforce periodic boundary
conditions. The benefits of the proposed physics-informed GNN are evaluated
considering linear and non linear hyperelastic responses applied to varying
geometries. In the non-linear hyperelastic case, the proposed method achieves
significant computational speed-ups compared to FE simulation, making it
particularly attractive for large-scale applications.

</details>


### [85] [Neural Velocity for hyperparameter tuning](https://arxiv.org/abs/2507.05309)
*Gianluca Dalmasso,Andrea Bragagnolo,Enzo Tartaglione,Attilio Fiandrotti,Marco Grangetto*

Main category: cs.LG

TL;DR: 提出动态训练方法NeVe，基于神经速度调整学习率和停止标准，展示神经速度作为优化训练关键指标的潜力


<details>
  <summary>Details</summary>
Motivation: 解决超参数调整依赖验证损失的问题

Method: 提出基于神经速度的动态训练方法NeVe，神经速度测量神经元传递函数变化率，可通过前向传播噪声采样

Result: 证明神经速度可作为优化神经网络训练的关键指标

Conclusion: 神经速度有潜力高效优化神经网络训练

Abstract: Hyperparameter tuning, such as learning rate decay and defining a stopping
criterion, often relies on monitoring the validation loss. This paper presents
NeVe, a dynamic training approach that adjusts the learning rate and defines
the stop criterion based on the novel notion of "neural velocity". The neural
velocity measures the rate of change of each neuron's transfer function and is
an indicator of model convergence: sampling neural velocity can be performed
even by forwarding noise in the network, reducing the need for a held-out
dataset. Our findings show the potential of neural velocity as a key metric for
optimizing neural network training efficiently

</details>


### [86] [Model-free Optical Processors using In Situ Reinforcement Learning with Proximal Policy Optimization](https://arxiv.org/abs/2507.05583)
*Yuhang Li,Shiqi Chen,Tingyu Gong,Aydogan Ozcan*

Main category: cs.LG

TL;DR: 提出用基于PPO的无模型强化学习方法原位训练衍射光处理器，实验验证其在多个任务中有更好收敛性和性能，能应对现实缺陷。


<details>
  <summary>Details</summary>
Motivation: 衍射光网络优化和对准有挑战，现有原位优化方法收敛慢、性能不稳定。

Method: 引入基于PPO的无模型强化学习方法进行原位训练，有效复用数据，约束策略更新。

Result: 在包括能量聚焦、全息图像生成等多个原位学习任务中实验验证，有更好收敛和性能。

Conclusion: 该原位强化学习方法能在现实约束下实现更快更准确训练，为光学和物理系统提供可扩展框架。

Abstract: Optical computing holds promise for high-speed, energy-efficient information
processing, with diffractive optical networks emerging as a flexible platform
for implementing task-specific transformations. A challenge, however, is the
effective optimization and alignment of the diffractive layers, which is
hindered by the difficulty of accurately modeling physical systems with their
inherent hardware imperfections, noise, and misalignments. While existing in
situ optimization methods offer the advantage of direct training on the
physical system without explicit system modeling, they are often limited by
slow convergence and unstable performance due to inefficient use of limited
measurement data. Here, we introduce a model-free reinforcement learning
approach utilizing Proximal Policy Optimization (PPO) for the in situ training
of diffractive optical processors. PPO efficiently reuses in situ measurement
data and constrains policy updates to ensure more stable and faster
convergence. We experimentally validated our method across a range of in situ
learning tasks, including targeted energy focusing through a random diffuser,
holographic image generation, aberration correction, and optical image
classification, demonstrating in each task better convergence and performance.
Our strategy operates directly on the physical system and naturally accounts
for unknown real-world imperfections, eliminating the need for prior system
knowledge or modeling. By enabling faster and more accurate training under
realistic experimental constraints, this in situ reinforcement learning
approach could offer a scalable framework for various optical and physical
systems governed by complex, feedback-driven dynamics.

</details>


### [87] [Conditional Graph Neural Network for Predicting Soft Tissue Deformation and Forces](https://arxiv.org/abs/2507.05315)
*Madina Kojanazarova,Florentin Bieder,Robin Sandkühler,Philippe C. Cattin*

Main category: cs.LG

TL;DR: 提出条件图神经网络（cGNN）数据驱动模型解决虚拟环境中软组织模拟难题，训练后可准确预测变形和力，有广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 虚拟环境中软组织模拟对医疗应用重要，但高可变形性带来挑战，现有方法有局限，且触觉反馈需精确力估计。

Method: 引入cGNN模型，用实验收集的软组织模型表面跟踪数据训练，利用迁移学习，先以质量 - 弹簧模拟数据训练，再用实验数据微调。

Result: 模型可在变形达30mm时距离误差为0.35±0.03mm，力达7.5N时绝对误差为0.37±0.05N。

Conclusion: 数据驱动方法是解决虚拟环境中软组织模拟复杂挑战的有前景方案，有广泛应用潜力。

Abstract: Soft tissue simulation in virtual environments is becoming increasingly
important for medical applications. However, the high deformability of soft
tissue poses significant challenges. Existing methods rely on segmentation,
meshing and estimation of stiffness properties of tissues. In addition, the
integration of haptic feedback requires precise force estimation to enable a
more immersive experience. We introduce a novel data-driven model, a
conditional graph neural network (cGNN) to tackle this complexity. Our model
takes surface points and the location of applied forces, and is specifically
designed to predict the deformation of the points and the forces exerted on
them. We trained our model on experimentally collected surface tracking data of
a soft tissue phantom and used transfer learning to overcome the data scarcity
by initially training it with mass-spring simulations and fine-tuning it with
the experimental data. This approach improves the generalisation capability of
the model and enables accurate predictions of tissue deformations and
corresponding interaction forces. The results demonstrate that the model can
predict deformations with a distance error of 0.35$\pm$0.03 mm for deformations
up to 30 mm and the force with an absolute error of 0.37$\pm$0.05 N for forces
up to 7.5 N. Our data-driven approach presents a promising solution to the
intricate challenge of simulating soft tissues within virtual environments.
Beyond its applicability in medical simulations, this approach holds the
potential to benefit various fields where realistic soft tissue simulations are
required.

</details>


### [88] [Topic Modeling and Link-Prediction for Material Property Discovery](https://arxiv.org/abs/2507.06139)
*Ryan C. Barron,Maksim E. Eren,Valentin Stanev,Cynthia Matuszek,Boian S. Alexandrov*

Main category: cs.LG

TL;DR: 提出AI驱动分层链接预测框架用于复杂材料领域，结合多种矩阵分解方法构建主题树，发现隐藏关联并验证，通过交互式仪表板辅助科研发现。


<details>
  <summary>Details</summary>
Motivation: 科学文献网络和知识图谱大、稀疏、有噪声且存在缺失链接，需要方法推断隐藏关联。

Method: 结合Hierarchical Nonnegative Matrix Factorization (HNMFk)、Boolean matrix factorization (BNMFk)和Logistic matrix factorization (LMF)，采用自动模型选择，使用集成BNMFk + LMF方法。

Result: HNMFk聚类将材料映射到相关主题，突出主题和材料间缺失或弱连接的链接，验证模型能预测超导关联。

Conclusion: 该方法能在科学文献构建的图中发现隐藏连接，生成新假设，交互式仪表板有助于科研发现。

Abstract: Link prediction infers missing or future relations between graph nodes, based
on connection patterns. Scientific literature networks and knowledge graphs are
typically large, sparse, and noisy, and often contain missing links between
entities. We present an AI-driven hierarchical link prediction framework that
integrates matrix factorization to infer hidden associations and steer
discovery in complex material domains. Our method combines Hierarchical
Nonnegative Matrix Factorization (HNMFk) and Boolean matrix factorization
(BNMFk) with automatic model selection, as well as Logistic matrix
factorization (LMF), we use to construct a three-level topic tree from a
46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs).
These materials are studied in a variety of physics fields with many current
and potential applications.
  An ensemble BNMFk + LMF approach fuses discrete interpretability with
probabilistic scoring. The resulting HNMFk clusters map each material onto
coherent topics like superconductivity, energy storage, and tribology. Also,
missing or weakly connected links are highlight between topics and materials,
suggesting novel hypotheses for cross-disciplinary exploration. We validate our
method by removing publications about superconductivity in well-known
superconductors, and show the model predicts associations with the
superconducting TMD clusters. This shows the method finds hidden connections in
a graph of material to latent topic associations built from scientific
literature, especially useful when examining a diverse corpus of scientific
documents covering the same class of phenomena or materials but originating
from distinct communities and perspectives. The inferred links generating new
hypotheses, produced by our method, are exposed through an interactive
Streamlit dashboard, designed for human-in-the-loop scientific discovery.

</details>


### [89] [Going Beyond Heuristics by Imposing Policy Improvement as a Constraint](https://arxiv.org/abs/2507.05328)
*Chi-Chang Lee,Zhang-Wei Hong,Pulkit Agrawal*

Main category: cs.LG

TL;DR: 提出启发式增强策略优化（HEPO）框架，利用启发式信息避免奖励破解，在基准测试中表现出色，能减少奖励设计的人力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习中平衡任务奖励和启发式奖励浪费人力和计算资源，基于策略不变性的方法实际效果差。

Method: 提出基于最大化策略改进目标的新范式，构建HEPO框架。

Result: HEPO在标准基准测试中表现优越，即使启发式设计不佳也能让策略优化取得好效果。

Conclusion: HEPO能有效利用启发式信息，避免奖励破解，减少奖励设计的人力，是强化学习中利用启发式的即插即用优化方法。

Abstract: In many reinforcement learning (RL) applications, augmenting the task rewards
with heuristic rewards that encode human priors about how a task should be
solved is crucial for achieving desirable performance. However, because such
heuristics are usually not optimal, much human effort and computational
resources are wasted in carefully balancing tasks and heuristic rewards.
Theoretically rigorous ways of incorporating heuristics rely on the idea of
\textit{policy invariance}, which guarantees that the performance of a policy
obtained by maximizing heuristic rewards is the same as the optimal policy with
respect to the task reward. However, in practice, policy invariance doesn't
result in policy improvement, and such methods are known to empirically perform
poorly. We propose a new paradigm to mitigate reward hacking and effectively
use heuristics based on the practical goal of maximizing policy improvement
instead of policy improvement. Our framework, Heuristic Enhanced Policy
Optimization (HEPO), effectively leverages heuristics while avoiding the
pitfall of prior methods for mitigating reward hacking. HEPO achieves superior
performance on standard benchmarks with well-engineered reward functions. More
surprisingly, HEPO allows policy optimization to achieve good performance even
when heuristics are not well-engineered and designed by non-expert humans,
showcasing HEPO's ability to reduce human effort in reward design. % HEPO is a
plug-and-play optimization method for leveraging heuristics in reinforcement
learning. Code is available at https://github.com/Improbable-AI/hepo.

</details>


### [90] [Few-Shot Learning by Explicit Physics Integration: An Application to Groundwater Heat Transport](https://arxiv.org/abs/2507.06062)
*Julia Pelzer,Corné Verburg,Alexander Heinlein,Miriam Schulte*

Main category: cs.LG

TL;DR: 因训练数据有限或质量低，机器学习用于科学工程有困难。本文以地下水热输运为例，引入LGCNN方法建模城市地下温度场，分析并训练模型，还公布了数据、代码和模型。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在科学工程实际应用中因训练数据问题面临的困难，以及经典数值模拟成本高、纯数据驱动代理模型预测平流过程难的问题。

Method: 引入Local - Global Convolutional Neural Network (LGCNN) 方法，结合轻量级数值代理（全局）和卷积神经网络（局部）。

Result: 对模型基于随机地下输入场进行分析，在慕尼黑地区真实地下地图部分区域训练，可扩展到更大区域且无需重新训练。

Conclusion: 提出的LGCNN方法可有效用于城市地下温度场建模，具有可扩展性和可复现性。

Abstract: Machine learning methods often struggle with real-world applications in
science and engineering due to limited or low-quality training data. In this
work, the example of groundwater flow with heat transport is considered; this
corresponds to an advection-diffusion process under heterogeneous flow
conditions, that is, spatially distributed material parameters and heat
sources. Classical numerical simulations are costly and challenging due to high
spatio-temporal resolution requirements and large domains. While often
computationally more efficient, purely data-driven surrogate models face
difficulties, particularly in predicting the advection process, which is highly
sensitive to input variations and involves long-range spatial interactions.
Therefore, in this work, a Local-Global Convolutional Neural Network (LGCNN)
approach is introduced. It combines a lightweight numerical surrogate for the
transport process (global) with convolutional neural networks for the
groundwater velocity and heat diffusion processes (local). With the LGCNN, a
city-wide subsurface temperature field is modeled, involving a heterogeneous
groundwater flow field and one hundred groundwater heat pump injection points
forming interacting heat plumes over long distances. The model is first
systematically analyzed based on random subsurface input fields. Then, the
model is trained on a handful of cut-outs from a real-world subsurface map of
the Munich region in Germany, and it scales to larger cut-outs without
retraining. All datasets, our code, and trained models are published for
reproducibility.

</details>


### [91] [Causal Foundation Models: Disentangling Physics from Instrument Properties](https://arxiv.org/abs/2507.05333)
*Jeroen Audenaert,Daniel Muthukrishna,Paul F. Gregory,David W. Hogg,V. Ashley Villar*

Main category: cs.LG

TL;DR: 提出因果驱动基础模型处理结构化时间序列数据，在模拟天文时间序列上表现优异，凸显编码因果结构的重要性。


<details>
  <summary>Details</summary>
Motivation: 结构化时间序列数据中观测值常混淆物理现象和测量仪器的系统失真，限制模型泛化，尤其在异构或多仪器场景。

Method: 使用双编码器架构和结构化对比学习，利用自然观测三元组，学习物理信号和仪器效应的单独潜在表示。

Result: 在模拟天文时间序列下游预测任务中，显著优于传统单潜在空间基础模型，尤其在低数据场景。

Conclusion: 模型支持基础模型关键能力，强调为结构化数据表示学习编码因果结构的重要性。

Abstract: Foundation models for structured time series data must contend with a
fundamental challenge: observations often conflate the true underlying physical
phenomena with systematic distortions introduced by measurement instruments.
This entanglement limits model generalization, especially in heterogeneous or
multi-instrument settings. We present a causally-motivated foundation model
that explicitly disentangles physical and instrumental factors using a
dual-encoder architecture trained with structured contrastive learning.
Leveraging naturally occurring observational triplets (i.e., where the same
target is measured under varying conditions, and distinct targets are measured
under shared conditions) our model learns separate latent representations for
the underlying physical signal and instrument effects. Evaluated on simulated
astronomical time series designed to resemble the complexity of variable stars
observed by missions like NASA's Transiting Exoplanet Survey Satellite (TESS),
our method significantly outperforms traditional single-latent space foundation
models on downstream prediction tasks, particularly in low-data regimes. These
results demonstrate that our model supports key capabilities of foundation
models, including few-shot generalization and efficient adaptation, and
highlight the importance of encoding causal structure into representation
learning for structured data.

</details>


### [92] [Bridging Prediction and Intervention Problems in Social Systems](https://arxiv.org/abs/2507.05216)
*Lydia T. Liu,Inioluwa Deborah Raji,Angela Zhou,Luke Guerdan,Jessica Hullman,Daniel Malinsky,Bryan Wilder,Simone Zhang,Hammaad Adam,Amanda Coston,Ben Laufer,Ezinne Nwankwo,Michael Zanger-Tishler,Eli Ben-Michael,Solon Barocas,Avi Feller,Marissa Gerchick,Talia Gillis,Shion Guha,Daniel Ho,Lily Hu,Kosuke Imai,Sayash Kapoor,Joshua Loftus,Razieh Nabi,Arvind Narayanan,Ben Recht,Juan Carlos Perdomo,Matthew Salganik,Mark Sendak,Alexander Tolbert,Berk Ustun,Suresh Venkatasubramanian,Angelina Wang,Ashia Wilson*

Main category: cs.LG

TL;DR: 论文探讨自动化决策系统（ADS）应从预测范式转向干预主义范式，统一工具并指出研究方向以开展更注重干预的ADS开发与部署。


<details>
  <summary>Details</summary>
Motivation: 现实中ADS在部署时会实施整体政策干预，当前以预测为中心的范式无法满足需求，需转变。

Method: 提出应采用新的默认问题设置，将预测视为决策支持、最终决策和结果，运用现代统计框架和工具。

Result: 指出聚焦孤立预测任务的局限性。

Conclusion: 为开发和部署ADS奠定更注重干预的方法基础。

Abstract: Many automated decision systems (ADS) are designed to solve prediction
problems -- where the goal is to learn patterns from a sample of the population
and apply them to individuals from the same population. In reality, these
prediction systems operationalize holistic policy interventions in deployment.
Once deployed, ADS can shape impacted population outcomes through an effective
policy change in how decision-makers operate, while also being defined by past
and present interactions between stakeholders and the limitations of existing
organizational, as well as societal, infrastructure and context. In this work,
we consider the ways in which we must shift from a prediction-focused paradigm
to an interventionist paradigm when considering the impact of ADS within social
systems. We argue this requires a new default problem setup for ADS beyond
prediction, to instead consider predictions as decision support, final
decisions, and outcomes. We highlight how this perspective unifies modern
statistical frameworks and other tools to study the design, implementation, and
evaluation of ADS systems, and point to the research directions necessary to
operationalize this paradigm shift. Using these tools, we characterize the
limitations of focusing on isolated prediction tasks, and lay the foundation
for a more intervention-oriented approach to developing and deploying ADS.

</details>


### [93] [Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training](https://arxiv.org/abs/2507.05386)
*Song Lai,Haohan Zhao,Rong Feng,Changyi Ma,Wenzhuo Liu,Hongbo Zhao,Xi Lin,Dong Yi,Min Xie,Qingfu Zhang,Hongbin Liu,Gaofeng Meng,Fei Zhu*

Main category: cs.LG

TL;DR: 文章对比CPT中SFT和RFT两种后训练范式对知识保留的影响，发现RFT优于SFT，还提出改进RFT稳定性和效率的算法。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索CPT中学习范式的基本作用，本文旨在对比SFT和RFT对知识保留的影响。

Method: 在包含七个多模态任务的基准上，以Qwen2.5 - VL - 7B - Instruct为基础模型进行实验。

Result: SFT会导致灾难性遗忘，RFT能保留先验知识，性能堪比多任务训练，还能保护甚至增强模型通用知识，且RFT的隐式正则化是减轻遗忘的关键。

Conclusion: RFT是持续后训练的强大范式，同时提出改进RFT稳定性和效率的算法。

Abstract: Continual post-training (CPT) is a popular and effective technique for
adapting foundation models like multimodal large language models to specific
and ever-evolving downstream tasks. While existing research has primarily
concentrated on methods like data replay, model expansion, or parameter
regularization, the fundamental role of the learning paradigm within CPT
remains largely unexplored. This paper presents a comparative analysis of two
core post-training paradigms: supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT), investigating their respective impacts on knowledge
retention during CPT. Our experiments are conducted on a benchmark comprising
seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base
model for continual post-training. The investigation yields two significant
findings: (1) When continuously learning on downstream tasks, SFT leads to
catastrophic forgetting of previously learned tasks. In contrast, RFT
inherently preserves prior knowledge and achieve performance comparable to
multi-task training. (2) RFT successfully protects and even enhances the
model's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro).
Conversely, SFT degrades general model capabilities severely. Further analysis
shows that explicit mechanisms, such as KL penalty and chain-of-thought
reasoning, are not the primary factors. Instead, we find that the implicit
regularization inherent to RFT is a key factor in mitigating forgetting.
Finally, we propose a rollout-based instance filtering algorithm to improve the
stability and efficiency of RFT. Our comprehensive study demonstrates the
superiority of RFT as a robust paradigm for continual post-training.

</details>


### [94] [Adversarial Machine Learning Attacks on Financial Reporting via Maximum Violated Multi-Objective Attack](https://arxiv.org/abs/2507.05441)
*Edward Raff,Karen Kukla,Michel Benaroch,Joseph Comprix*

Main category: cs.LG

TL;DR: 提出MVMO攻击方法，能比标准攻击多找到20倍令人满意的攻击，约50%情况下公司可操纵财报。


<details>
  <summary>Details</summary>
Motivation: 不良行为者（主要是陷入困境的公司）有动机操纵财务报告以隐藏困境并获取个人利益，现有攻击方法在该数据上无效。

Method: 引入Maximum Violated Multi-Objective (MVMO)攻击，调整攻击者的搜索方向。

Result: 在约50%的情况下，公司能将收益虚增100 - 200%，同时将欺诈分数降低15%。

Conclusion: 与律师和专业会计师合作，确保威胁模型符合实际欺诈行为。

Abstract: Bad actors, primarily distressed firms, have the incentive and desire to
manipulate their financial reports to hide their distress and derive personal
gains. As attackers, these firms are motivated by potentially millions of
dollars and the availability of many publicly disclosed and used financial
modeling frameworks. Existing attack methods do not work on this data due to
anti-correlated objectives that must both be satisfied for the attacker to
succeed. We introduce Maximum Violated Multi-Objective (MVMO) attacks that
adapt the attacker's search direction to find $20\times$ more satisfying
attacks compared to standard attacks. The result is that in $\approx50\%$ of
cases, a company could inflate their earnings by 100-200%, while simultaneously
reducing their fraud scores by 15%. By working with lawyers and professional
accountants, we ensure our threat model is realistic to how such frauds are
performed in practice.

</details>


### [95] [Probabilistically Tightened Linear Relaxation-based Perturbation Analysis for Neural Network Verification](https://arxiv.org/abs/2507.05405)
*Luca Marzari,Ferdinando Cicalese,Alessandro Farinelli*

Main category: cs.LG

TL;DR: 提出PT - LiRPA框架，结合过近似技术与采样方法计算可达集，可减少形式验证计算成本，实验显示在基准测试中表现优于相关工作。


<details>
  <summary>Details</summary>
Motivation: 降低形式验证工具的计算成本，为现有方法失效的问题提供解决方案并提供验证健全性的概率保证。

Method: 结合LiRPA的过近似技术和基于采样的方法，利用估计的可达集收紧神经网络输出的线性边界。

Result: 在标准形式验证基准测试中，基于PT - LiRPA的验证器的鲁棒性证书相比相关工作最多提高3.31倍和2.26倍，能为现有方法失效的竞争条目提供高置信度答案。

Conclusion: PT - LiRPA框架能有效减少形式验证成本，提高验证性能，在困难问题上也能提供可靠答案。

Abstract: We present $\textbf{P}$robabilistically $\textbf{T}$ightened
$\textbf{Li}$near $\textbf{R}$elaxation-based $\textbf{P}$erturbation
$\textbf{A}$nalysis ($\texttt{PT-LiRPA}$), a novel framework that combines
over-approximation techniques from LiRPA-based approaches with a sampling-based
method to compute tight intermediate reachable sets. In detail, we show that
with negligible computational overhead, $\texttt{PT-LiRPA}$ exploiting the
estimated reachable sets, significantly tightens the lower and upper linear
bounds of a neural network's output, reducing the computational cost of formal
verification tools while providing probabilistic guarantees on verification
soundness. Extensive experiments on standard formal verification benchmarks,
including the International Verification of Neural Networks Competition, show
that our $\texttt{PT-LiRPA}$-based verifier improves robustness certificates by
up to 3.31X and 2.26X compared to related work. Importantly, our probabilistic
approach results in a valuable solution for challenging competition entries
where state-of-the-art formal verification methods fail, allowing us to provide
answers with high confidence (i.e., at least 99%).

</details>


### [96] [Dynamic Regret Reduces to Kernelized Static Regret](https://arxiv.org/abs/2507.05478)
*Andrew Jacobsen,Alessandro Rudi,Francesco Orabona,Nicolo Cesa-Bianchi*

Main category: cs.LG

TL;DR: 本文研究在线凸优化中的动态遗憾问题，通过将其转化为函数空间中的静态遗憾问题，获得了新的动态遗憾保证和可实际计算的算法。


<details>
  <summary>Details</summary>
Motivation: 在在线凸优化中实现相对于任意基准序列的低累积损失，解决动态遗憾最小化问题。

Method: 观察到与任意比较器序列竞争等价于与固定比较器函数竞争，将动态遗憾最小化构建为函数空间中的静态遗憾问题，构造再生核希尔伯特空间（RKHS）。

Result: 在线性损失设置下恢复最优动态遗憾保证，获得新的无尺度和方向自适应动态遗憾保证；在指数凹和非恰当线性回归设置下得到新的界；算法在实际中可计算。

Conclusion: 所提出的将动态问题转化为静态问题的方法有效，适用于任意损失序列，且基于RKHS的算法具有实际可计算性。

Abstract: We study dynamic regret in online convex optimization, where the objective is
to achieve low cumulative loss relative to an arbitrary benchmark sequence. By
observing that competing with an arbitrary sequence of comparators
$u_{1},\ldots,u_{T}$ in $\mathcal{W}\subseteq\mathbb{R}^{d}$ is equivalent to
competing with a fixed comparator function $u:[1,T]\to \mathcal{W}$, we frame
dynamic regret minimization as a static regret problem in a function space. By
carefully constructing a suitable function space in the form of a Reproducing
Kernel Hilbert Space (RKHS), our reduction enables us to recover the optimal
$R_{T}(u_{1},\ldots,u_{T}) = \mathcal{O}(\sqrt{\sum_{t}\|u_{t}-u_{t-1}\|T})$
dynamic regret guarantee in the setting of linear losses, and yields new
scale-free and directionally-adaptive dynamic regret guarantees. Moreover,
unlike prior dynamic-to-static reductions -- which are valid only for linear
losses -- our reduction holds for any sequence of losses, allowing us to
recover $\mathcal{O}\big(\|u\|^2+d_{\mathrm{eff}}(\lambda)\ln T\big)$ bounds in
exp-concave and improper linear regression settings, where
$d_{\mathrm{eff}}(\lambda)$ is a measure of complexity of the RKHS. Despite
working in an infinite-dimensional space, the resulting reduction leads to
algorithms that are computable in practice, due to the reproducing property of
RKHSs.

</details>


### [97] [AXLearn: Modular Large Model Training on Heterogeneous Infrastructure](https://arxiv.org/abs/2507.05411)
*Mark Lee,Tom Gunter,Chang Lan,John Peebles,Hanzhi Zhou,Kelvin Zou,Sneha Bangalore,Chung-Cheng Chiu,Nan Du,Xianzhi Du,Philipp Dufter,Ruixuan Hou,Haoshuo Huang,Dongseong Hwang,Xiang Kong,Jinhao Lei,Tao Lei,Meng Li,Li Li,Jiarui Lu,Zhiyun Lu,Yiping Ma,David Qiu,Vivek Rathod,Senyu Tong,Zhucheng Tu,Jianyu Wang,Yongqiang Wang,Zirui Wang,Floris Weers,Sam Wiseman,Guoli Yin,Bowen Zhang,Xiyou Zhou,Danyang Zhuo,Cheng Leong,Ruoming Pang*

Main category: cs.LG

TL;DR: 设计并实现了深度学习系统AXLearn，聚焦模块化与异构硬件支持，介绍量化模块化方法，性能与先进系统相当并分享开发运营经验。


<details>
  <summary>Details</summary>
Motivation: 开发一个能促进大规模深度学习模型可扩展且高性能训练的系统，且聚焦模块化和异构硬件支持。

Method: 设计实现AXLearn系统，采用严格封装的软件组件内部接口；引入通过代码行数复杂度量化模块化的新方法。

Result: 系统组件扩展时复杂度恒定，集成特性所需代码少，性能与先进训练系统相当。

Conclusion: AXLearn系统在模块化和性能上表现良好，同时分享了开发运营经验。

Abstract: We design and implement AXLearn, a production deep learning system that
facilitates scalable and high-performance training of large deep learning
models. Compared to other state-of-the-art deep learning systems, AXLearn has a
unique focus on modularity and support for heterogeneous hardware
infrastructure. AXLearn's internal interfaces between software components
follow strict encapsulation, allowing different components to be assembled to
facilitate rapid model development and experimentation on heterogeneous compute
infrastructure. We introduce a novel method of quantifying modularity via
Lines-of-Code (LoC)-complexity, which demonstrates how our system maintains
constant complexity as we scale the components in the system, compared to
linear or quadratic complexity in other systems. This allows integrating
features such as Rotary Position Embeddings (RoPE) into AXLearn across hundred
of modules with just 10 lines of code, compared to hundreds as required in
other systems. At the same time, AXLearn maintains equivalent performance
compared to state-of-the-art training systems. Finally, we share our experience
in the development and operation of AXLearn.

</details>


### [98] [Navigating Sparse Molecular Data with Stein Diffusion Guidance](https://arxiv.org/abs/2507.05482)
*Van Khoa Nguyen,Lionel Blondé,Alexandros Kalousis*

Main category: cs.LG

TL;DR: 本文提出基于替代随机最优控制目标的无训练扩散引导框架Stein Diffusion Guidance (SDG)，在分子生成任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有随机最优控制计算密集，无训练方法直接近似有误差，需结合两者优势。

Method: 推导值函数理论界，将问题与Stein变分推理联系，引入校正机制和运行成本泛函。

Result: 在具有挑战性的分子生成任务中，SDG显著优于标准无训练引导方法。

Conclusion: SDG有更广泛应用潜力。

Abstract: Stochastic optimal control (SOC) has recently emerged as a principled
framework for fine-tuning diffusion models. However, its dependence on
computationally intensive simulations makes it impractical for fast sampling.
In parallel, a class of training-free approaches has been developed that guides
diffusion models using off-the-shelf classifiers on predicted clean samples,
bypassing the need to train classifiers on noisy data. These methods can be
interpreted as approximate SOC schemes, using Tweedie's formula to estimate
diffusion posteriors. In practice, however, such direct approximations can
introduce significant errors, leading to unreliable guidance. In this work, we
unify the strengths of both paradigms by proposing a novel training-free
diffusion guidance framework based on a surrogate stochastic optimal control
objective. We derive a new theoretical bound on the value function that reveals
the necessity of correcting the approximate posteriors to remain faithful to
the true diffusion posterior. To this end, we connect the problem with Stein
variational inference, which seeks the steepest descent direction that
minimizes the Kullback-Leibler discrepancy between the two posteriors. Our
method, which we refer to as Stein Diffusion Guidance (SDG), introduces a
principled correction mechanism and incorporates a novel running cost
functional to enable effective guidance in low-density regions. Experiments on
challenging molecular generation tasks demonstrate that SDG significantly
outperforms standard training-free guidance methods, highlighting its potential
for broader applications.

</details>


### [99] [Incorporating Interventional Independence Improves Robustness against Interventional Distribution Shift](https://arxiv.org/abs/2507.05412)
*Gautam Sreekumar,Vishnu Naresh Boddeti*

Main category: cs.LG

TL;DR: 本文关注学习因果相关潜在变量的鲁棒判别表示问题，指出现有方法不足，提出RepLIn算法并在多数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用干预产生的因果关系信息，导致观测和干预数据预测性能差异大，尤其是干预训练样本有限时。

Method: 先确定性能差异与表示符合干预因果模型独立性条件的强相关性；对线性模型推导训练集中干预数据比例的充分条件；提出RepLIn训练算法在干预时强制统计独立。

Result: 在合成数据集、真实图像和文本数据集上验证了RepLIn算法的实用性，表明其可随因果图节点数扩展，适用于提高连续和离散潜在变量干预分布偏移的鲁棒表示。

Conclusion: RepLIn算法能有效提高对干预分布偏移的鲁棒表示，具有良好扩展性和适用性。

Abstract: We consider the problem of learning robust discriminative representations of
causally-related latent variables. In addition to observational data, the
training dataset also includes interventional data obtained through targeted
interventions on some of these latent variables to learn representations robust
against the resulting interventional distribution shifts. Existing approaches
treat interventional data like observational data, even when the underlying
causal model is known, and ignore the independence relations that arise from
these interventions. Since these approaches do not fully exploit the causal
relational information resulting from interventions, they learn representations
that produce large disparities in predictive performance on observational and
interventional data, which worsens when the number of interventional training
samples is limited. In this paper, (1) we first identify a strong correlation
between this performance disparity and adherence of the representations to the
independence conditions induced by the interventional causal model. (2) For
linear models, we derive sufficient conditions on the proportion of
interventional data in the training dataset, for which enforcing interventional
independence between representations corresponding to the intervened node and
its non-descendants lowers the error on interventional data. Combining these
insights, (3) we propose RepLIn, a training algorithm to explicitly enforce
this statistical independence during interventions. We demonstrate the utility
of RepLIn on a synthetic dataset and on real image and text datasets on facial
attribute classification and toxicity detection, respectively. Our experiments
show that RepLIn is scalable with the number of nodes in the causal graph and
is suitable to improve the robust representations against interventional
distribution shifts of both continuous and discrete latent variables.

</details>


### [100] [Estimating Interventional Distributions with Uncertain Causal Graphs through Meta-Learning](https://arxiv.org/abs/2507.05526)
*Anish Dhir,Cristiana Diaconu,Valentinian Mihai Lungu,James Requeima,Richard E. Turner,Mark van der Wilk*

Main category: cs.LG

TL;DR: 提出MACE - TNP模型解决贝叶斯因果推断计算难题，且表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 在缺乏领域知识时，需从观测数据发现因果结构，但观测数据与多因果图兼容，且贝叶斯推断计算因结构数量增长而难以处理。

Method: 使用元学习创建端到端模型MACE - TNP，训练其预测贝叶斯模型平均干预后验分布。

Result: MACE - TNP在实验中表现优于强大的贝叶斯基线。

Conclusion: 元学习是一种灵活可扩展的范式，可用于近似复杂贝叶斯因果推断，未来可应用于更具挑战性的场景。

Abstract: In scientific domains -- from biology to the social sciences -- many
questions boil down to \textit{What effect will we observe if we intervene on a
particular variable?} If the causal relationships (e.g.~a causal graph) are
known, it is possible to estimate the intervention distributions. In the
absence of this domain knowledge, the causal structure must be discovered from
the available observational data. However, observational data are often
compatible with multiple causal graphs, making methods that commit to a single
structure prone to overconfidence. A principled way to manage this structural
uncertainty is via Bayesian inference, which averages over a posterior
distribution on possible causal structures and functional mechanisms.
Unfortunately, the number of causal structures grows super-exponentially with
the number of nodes in the graph, making computations intractable. We propose
to circumvent these challenges by using meta-learning to create an end-to-end
model: the Model-Averaged Causal Estimation Transformer Neural Process
(MACE-TNP). The model is trained to predict the Bayesian model-averaged
interventional posterior distribution, and its end-to-end nature bypasses the
need for expensive calculations. Empirically, we demonstrate that MACE-TNP
outperforms strong Bayesian baselines. Our work establishes meta-learning as a
flexible and scalable paradigm for approximating complex Bayesian causal
inference, that can be scaled to increasingly challenging settings in the
future.

</details>


### [101] [EmissionNet: Air Quality Pollution Forecasting for Agriculture](https://arxiv.org/abs/2507.05416)
*Prady Saligram,Tanvir Bhathal*

Main category: cs.LG

TL;DR: 本文探索农业排放N₂O预测，评估流行架构并提出两个新深度学习架构ENV和ENT。


<details>
  <summary>Details</summary>
Motivation: 农业排放造成空气污染，传统空气质量预测模型难以捕捉复杂非线性污染物相互作用。

Method: 评估流行架构，提出ENV和ENT两个新深度学习架构，利用卷积和基于transformer的架构从高分辨率排放数据中提取时空依赖关系。

Result: 未提及

Conclusion: 未提及

Abstract: Air pollution from agricultural emissions is a significant yet often
overlooked contributor to environmental and public health challenges.
Traditional air quality forecasting models rely on physics-based approaches,
which struggle to capture complex, nonlinear pollutant interactions. In this
work, we explore forecasting N$_2$O agricultural emissions through evaluating
popular architectures, and proposing two novel deep learning architectures,
EmissionNet (ENV) and EmissionNet-Transformer (ENT). These models leverage
convolutional and transformer-based architectures to extract spatial-temporal
dependencies from high-resolution emissions data

</details>


### [102] [FACT: the Features At Convergence Theorem for neural networks](https://arxiv.org/abs/2507.05644)
*Enric Boix-Adsera,Neil Mallinar,James B. Simon,Mikhail Belkin*

Main category: cs.LG

TL;DR: 论文证明了FACT定理，验证其正确性，提出新算法FACT - RFM并展示其在表格数据上的高性能。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络如何学习和表示特征。

Method: 证明Features at Convergence Theorem (FACT)，通过修改Radhakrishnan等人2024年的算法得到新算法FACT - RFM。

Result: 经验验证了FACT定理，FACT - RFM在表格数据上表现良好，能捕捉神经网络训练中的多种特征学习行为。

Conclusion: FACT定理有效，FACT - RFM是一种有效的学习算法。

Abstract: A central challenge in deep learning theory is to understand how neural
networks learn and represent features. To this end, we prove the Features at
Convergence Theorem (FACT), which gives a self-consistency equation that neural
network weights satisfy at convergence when trained with nonzero weight decay.
For each weight matrix $W$, this equation relates the "feature matrix" $W^\top
W$ to the set of input vectors passed into the matrix during forward
propagation and the loss gradients passed through it during backpropagation. We
validate this relation empirically, showing that neural features indeed satisfy
the FACT at convergence. Furthermore, by modifying the "Recursive Feature
Machines" of Radhakrishnan et al. 2024 so that they obey the FACT, we arrive at
a new learning algorithm, FACT-RFM. FACT-RFM achieves high performance on
tabular data and captures various feature learning behaviors that occur in
neural network training, including grokking in modular arithmetic and phase
transitions in learning sparse parities.

</details>


### [103] [2048: Reinforcement Learning in a Delayed Reward Environment](https://arxiv.org/abs/2507.05465)
*Prady Saligram,Tanvir Bhathal,Robby Manihani*

Main category: cs.LG

TL;DR: 本文提出统一的分布多步强化学习框架，在2048游戏中对比四种智能体变体，结果表明分布多步目标能提升稀疏奖励领域性能。


<details>
  <summary>Details</summary>
Motivation: 延迟和稀疏奖励是强化学习智能体的基本障碍，2048游戏体现了这一挑战，当前策略易导致局部最优。

Method: 引入统一的分布多步强化学习框架，在Gym - 2048环境中开发并对比标准DQN、PPO、QR - DQN和新型H - DQN四种智能体变体。

Result: 四种智能体最大回合得分逐步提升，H - DQN表现最佳，达到2048方块，扩展后可达4096方块和更高分数。

Conclusion: 分布多步目标能显著提升稀疏奖励领域性能，基于模型规划和课程学习或可进一步提升。

Abstract: Delayed and sparse rewards present a fundamental obstacle for
reinforcement-learning (RL) agents, which struggle to assign credit for actions
whose benefits emerge many steps later. The sliding-tile game 2048 epitomizes
this challenge: although frequent small score changes yield immediate feedback,
they often mislead agents into locally optimal but globally suboptimal
strategies. In this work, we introduce a unified, distributional multi-step RL
framework designed to directly optimize long-horizon performance. Using the
open source Gym-2048 environment we develop and compare four agent variants:
standard DQN, PPO, QR-DQN (Quantile Regression DQN), and a novel Horizon-DQN
(H-DQN) that integrates distributional learning, dueling architectures, noisy
networks, prioritized replay, and more. Empirical evaluation reveals a clear
hierarchy in effectiveness: max episode scores improve from 3.988K (DQN) to
5.756K (PPO), 8.66K (QR-DQN), and 18.21K (H-DQN), with H-DQN reaching the 2048
tile. Upon scaling H-DQN it reaches a max score 41.828K and a 4096 tile. These
results demonstrate that distributional, multi-step targets substantially
enhance performance in sparse-reward domains, and they suggest promising
avenues for further gains through model-based planning and curriculum learning.

</details>


### [104] [Epistemically-guided forward-backward exploration](https://arxiv.org/abs/2507.05477)
*Núria Armengol Urpí,Marin Vlastelica,Georg Martius,Stelian Coros*

Main category: cs.LG

TL;DR: 提出应使用前向后向表示（FB）进行探索以提高零样本强化学习效率，设计了基于FB的探索策略并证明其能提升样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的FB等零样本强化学习算法与探索问题解耦，依赖其他探索算法收集数据，需更高效的学习方式。

Method: 设计从FB表示自然产生的探索策略，最小化FB表示的后验方差和认知不确定性。

Result: 基于FB的探索策略相比其他探索方法显著提高了FB算法的样本复杂度。

Conclusion: 使用FB表示进行探索能让零样本强化学习更高效。

Abstract: Zero-shot reinforcement learning is necessary for extracting optimal policies
in absence of concrete rewards for fast adaptation to future problem settings.
Forward-backward representations (FB) have emerged as a promising method for
learning optimal policies in absence of rewards via a factorization of the
policy occupancy measure. However, up until now, FB and many similar zero-shot
reinforcement learning algorithms have been decoupled from the exploration
problem, generally relying on other exploration algorithms for data collection.
We argue that FB representations should fundamentally be used for exploration
in order to learn more efficiently. With this goal in mind, we design
exploration policies that arise naturally from the FB representation that
minimize the posterior variance of the FB representation, hence minimizing its
epistemic uncertainty. We empirically demonstrate that such principled
exploration strategies improve sample complexity of the FB algorithm
considerably in comparison to other exploration methods. Code is publicly
available at https://sites.google.com/view/fbee-url.

</details>


### [105] [Predicting Graph Structure via Adapted Flux Balance Analysis](https://arxiv.org/abs/2507.05806)
*Sevvandi Kandanaarachchi,Ziqi Xu,Stefan Westerlund,Conrad Sanderson*

Main category: cs.LG

TL;DR: 本文提出结合时间序列预测方法与改进的通量平衡分析（FBA）进行图预测，实验证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图预测方法存在局限性，如假设连续图之间顶点不变，需新方法解决。

Method: 将时间序列预测方法与改进的通量平衡分析（FBA）结合，FBA适应增长图场景的各种约束。

Result: 在合成数据集和真实数据集上的实证评估证明了所提方法的有效性。

Conclusion: 所提出的结合方法能有效用于图预测。

Abstract: Many dynamic processes such as telecommunication and transport networks can
be described through discrete time series of graphs. Modelling the dynamics of
such time series enables prediction of graph structure at future time steps,
which can be used in applications such as detection of anomalies. Existing
approaches for graph prediction have limitations such as assuming that the
vertices do not to change between consecutive graphs. To address this, we
propose to exploit time series prediction methods in combination with an
adapted form of flux balance analysis (FBA), a linear programming method
originating from biochemistry. FBA is adapted to incorporate various
constraints applicable to the scenario of growing graphs. Empirical evaluations
on synthetic datasets (constructed via Preferential Attachment model) and real
datasets (UCI Message, HePH, Facebook, Bitcoin) demonstrate the efficacy of the
proposed approach.

</details>


### [106] [Prototype-Guided and Lightweight Adapters for Inherent Interpretation and Generalisation in Federated Learning](https://arxiv.org/abs/2507.05852)
*Samuel Ofosu Mensah,Kerol Djoumessi,Philipp Berens*

Main category: cs.LG

TL;DR: 提出一种联邦学习框架，用原型提供解释，利用轻量级适配器模块应对统计异质性，减少通信负载，在视网膜眼底图像数据集实验中提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的联邦学习面临大模型参数传输的通信开销和统计异质性等挑战。

Method: 提出的框架用原型提供固有解释，利用轻量级适配器模块作为本地模型的压缩代理，客户端通过将类嵌入与原型表示对齐来细化模型并调整适配器，用原型和适配器替代整个模型权重的通信。

Result: 在视网膜眼底图像数据集实验中展示了固有可解释能力，在分类任务中准确率优于基线算法。

Conclusion: 该设计能让客户端模型与全局共享结构对齐，同时减少通信负载并提供固有解释。

Abstract: Federated learning (FL) provides a promising paradigm for collaboratively
training machine learning models across distributed data sources while
maintaining privacy. Nevertheless, real-world FL often faces major challenges
including communication overhead during the transfer of large model parameters
and statistical heterogeneity, arising from non-identical independent data
distributions across clients. In this work, we propose an FL framework that 1)
provides inherent interpretations using prototypes, and 2) tackles statistical
heterogeneity by utilising lightweight adapter modules to act as compressed
surrogates of local models and guide clients to achieve generalisation despite
varying client distribution. Each client locally refines its model by aligning
class embeddings toward prototype representations and simultaneously adjust the
lightweight adapter. Our approach replaces the need to communicate entire model
weights with prototypes and lightweight adapters. This design ensures that each
client's model aligns with a globally shared structure while minimising
communication load and providing inherent interpretations. Moreover, we
conducted our experiments on a real-world retinal fundus image dataset, which
provides clinical-site information. We demonstrate inherent interpretable
capabilities and perform a classification task, which shows improvements in
accuracy over baseline algorithms.

</details>


### [107] [Detecting and Mitigating Reward Hacking in Reinforcement Learning Systems: A Comprehensive Empirical Study](https://arxiv.org/abs/2507.05619)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 本文对不同强化学习环境和算法中的奖励破解进行大规模实证研究，提出检测框架和缓解技术，公开相关资源。


<details>
  <summary>Details</summary>
Motivation: 强化学习系统中的奖励破解威胁自治代理部署，现有系统检测和缓解方法有限。

Method: 分析15个环境、5种算法的15247个训练回合，实现6类奖励破解的自动检测算法，进行控制实验和模拟应用研究。

Result: 检测框架精度78.4%、召回率81.7%，计算开销低于5%；奖励密度和与真实目标的一致性显著影响破解频率；缓解技术在控制场景中最多降低54.6%破解频率。

Conclusion: 提出的方法在检测和缓解奖励破解上有一定效果，但实际应用中存在权衡挑战，公开资源支持可重复性研究。

Abstract: Reward hacking in Reinforcement Learning (RL) systems poses a critical threat
to the deployment of autonomous agents, where agents exploit flaws in reward
functions to achieve high scores without fulfilling intended objectives.
Despite growing awareness of this problem, systematic detection and mitigation
approaches remain limited. This paper presents a large-scale empirical study of
reward hacking across diverse RL environments and algorithms. We analyze 15,247
training episodes across 15 RL environments (Atari, MuJoCo, custom domains) and
5 algorithms (PPO, SAC, DQN, A3C, Rainbow), implementing automated detection
algorithms for six categories of reward hacking: specification gaming, reward
tampering, proxy optimization, objective misalignment, exploitation patterns,
and wireheading. Our detection framework achieves 78.4% precision and 81.7%
recall across environments, with computational overhead under 5%. Through
controlled experiments varying reward function properties, we demonstrate that
reward density and alignment with true objectives significantly impact hacking
frequency ($p < 0.001$, Cohen's $d = 1.24$). We validate our approach through
three simulated application studies representing recommendation systems,
competitive gaming, and robotic control scenarios. Our mitigation techniques
reduce hacking frequency by up to 54.6% in controlled scenarios, though we find
these trade-offs are more challenging in practice due to concept drift, false
positive costs, and adversarial adaptation. All detection algorithms, datasets,
and experimental protocols are publicly available to support reproducible
research in RL safety.

</details>


### [108] [Explainable Hierarchical Deep Learning Neural Networks (Ex-HiDeNN)](https://arxiv.org/abs/2507.05498)
*Reza T. Batley,Chanwook Park,Wing Kam Liu,Sourav Saha*

Main category: cs.LG

TL;DR: 文章提出Ex - HiDeNN方法从有限观测中发现闭式表达式，经基准问题测试和工程应用验证效果良好，并指出局限性和未来扩展方向。


<details>
  <summary>Details</summary>
Motivation: 当前从复杂数据集高效发现可解释且准确的闭式表达式存在挑战。

Method: 提出Ex - HiDeNN方法，采用准确、节俭、快速、可分离和可扩展的神经架构与符号回归结合，有嵌入可分离性检查器的两步算法。

Result: 在多个基准问题和三个工程应用中，Ex - HiDeNN表现出出色的近似能力，误差比参考数据和传统符号回归小，且优于文献中的参考方法。

Conclusion: Ex - HiDeNN在发现闭式表达式方面有效，但文章也指出其当前局限性和未来扩展方向。

Abstract: Data-driven science and computation have advanced immensely to construct
complex functional relationships using trainable parameters. However,
efficiently discovering interpretable and accurate closed-form expressions from
complex dataset remains a challenge. The article presents a novel approach
called Explainable Hierarchical Deep Learning Neural Networks or Ex-HiDeNN that
uses an accurate, frugal, fast, separable, and scalable neural architecture
with symbolic regression to discover closed-form expressions from limited
observation. The article presents the two-step Ex-HiDeNN algorithm with a
separability checker embedded in it. The accuracy and efficiency of Ex-HiDeNN
are tested on several benchmark problems, including discerning a dynamical
system from data, and the outcomes are reported. Ex-HiDeNN generally shows
outstanding approximation capability in these benchmarks, producing orders of
magnitude smaller errors compared to reference data and traditional symbolic
regression. Later, Ex-HiDeNN is applied to three engineering applications: a)
discovering a closed-form fatigue equation, b) identification of hardness from
micro-indentation test data, and c) discovering the expression for the yield
surface with data. In every case, Ex-HiDeNN outperformed the reference methods
used in the literature. The proposed method is built upon the foundation and
published works of the authors on Hierarchical Deep Learning Neural Network
(HiDeNN) and Convolutional HiDeNN. The article also provides a clear idea about
the current limitations and future extensions of Ex-HiDeNN.

</details>


### [109] [Dynamic Campus Origin-Destination Mobility Prediction using Graph Convolutional Neural Network on WiFi Logs](https://arxiv.org/abs/2507.05507)
*Godwin Badu-Marfo,Bilal Farooq*

Main category: cs.LG

TL;DR: 提出基于图的神经网络架构预测校园建筑占用和移动，用GCLSTM模型，实验显示该模型优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 预测校园建筑动态占用和建筑间移动情况，学习交通流模式。

Method: 将问题表述为数据驱动的图结构，使用新颖的Graph Convolution plus LSTM Neural Network (GCLSTM) 模型。

Result: 集成的GCLSTM模型显著优于传统行人流量估计器，如多层感知器（MLP）和线性回归。

Conclusion: 所提出的基于图的神经网络架构在校园建筑占用和移动预测上表现良好，GCLSTM模型有优势。

Abstract: We present an integrated graph-based neural networks architecture for
predicting campus buildings occupancy and inter-buildings movement at dynamic
temporal resolution that learns traffic flow patterns from Wi-Fi logs combined
with the usage schedules within the buildings. The relative traffic flows are
directly estimated from the WiFi data without assuming the occupant behaviour
or preferences while maintaining individual privacy. We formulate the problem
as a data-driven graph structure represented by a set of nodes (representing
buildings), connected through a route of edges or links using a novel Graph
Convolution plus LSTM Neural Network (GCLSTM) which has shown remarkable
success in modelling complex patterns. We describe the formulation, model
estimation, interpretability and examine the relative performance of our
proposed model. We also present an illustrative architecture of the models and
apply on real-world WiFi logs collected at the Toronto Metropolitan University
campus. The results of the experiments show that the integrated GCLSTM models
significantly outperform traditional pedestrian flow estimators like the Multi
Layer Perceptron (MLP) and Linear Regression.

</details>


### [110] [Beyond Communication Overhead: A Multilevel Monte Carlo Approach for Mitigating Compression Bias in Distributed Learning](https://arxiv.org/abs/2507.05508)
*Ze'ev Zukerman,Bassel Hamoud,Kfir Y. Levy*

Main category: cs.LG

TL;DR: 提出新的多级蒙特卡罗（MLMC）压缩方案，结合有偏和无偏压缩器优点，应用于流行压缩器并推导出自适应版本，在分布式深度学习任务中验证。


<details>
  <summary>Details</summary>
Motivation: 分布式学习中通信开销是瓶颈，梯度压缩技术有有偏和无偏压缩器的权衡问题。

Method: 引入MLMC压缩方案，利用有偏压缩器构建统计无偏估计，应用于流行压缩器并推导自适应版本。

Result: 得到增强的压缩器变体，推导出自适应版本。

Conclusion: 该方法在分布式深度学习任务中得到了实证验证。

Abstract: Distributed learning methods have gained substantial momentum in recent
years, with communication overhead often emerging as a critical bottleneck.
Gradient compression techniques alleviate communication costs but involve an
inherent trade-off between the empirical efficiency of biased compressors and
the theoretical guarantees of unbiased compressors. In this work, we introduce
a novel Multilevel Monte Carlo (MLMC) compression scheme that leverages biased
compressors to construct statistically unbiased estimates. This approach
effectively bridges the gap between biased and unbiased methods, combining the
strengths of both. To showcase the versatility of our method, we apply it to
popular compressors, like Top-$k$ and bit-wise compressors, resulting in
enhanced variants. Furthermore, we derive an adaptive version of our approach
to further improve its performance. We validate our method empirically on
distributed deep learning tasks.

</details>


### [111] [Heterogeneous Causal Learning for Optimizing Aggregated Functions in User Growth](https://arxiv.org/abs/2507.05510)
*Shuyang Du,Jennifer Zhang,Will Y. Zou*

Main category: cs.LG

TL;DR: 提出新的处理效应优化方法用于用户增长营销，超越传统方法，实验证明有效且具成本效益，可用于多种场景。


<details>
  <summary>Details</summary>
Motivation: 优化昂贵的营销活动，最大化用户参与度，提升用户增长营销效果。

Method: 利用深度学习，从过往实验学习以优化用户选择和奖励分配，直接建模关键业务指标提升，用softmax门控联合优化聚合损失函数参数。

Result: 提出的约束和直接优化算法比现有技术表现好超20%，方法通过全球生产部署验证有效性。

Conclusion: 该方法能直接针对期望业务指标，算法灵活性强，具成本效益，可用于多种产品场景。

Abstract: User growth is a major strategy for consumer internet companies. To optimize
costly marketing campaigns and maximize user engagement, we propose a novel
treatment effect optimization methodology to enhance user growth marketing. By
leveraging deep learning, our algorithm learns from past experiments to
optimize user selection and reward allocation, maximizing campaign impact while
minimizing costs. Unlike traditional prediction methods, our model directly
models uplifts in key business metrics. Further, our deep learning model can
jointly optimize parameters for an aggregated loss function using softmax
gating. Our approach surpasses traditional methods by directly targeting
desired business metrics and demonstrates superior algorithmic flexibility in
handling complex business constraints. Comprehensive evaluations, including
comparisons with state-of-the-art techniques such as R-learner and Causal
Forest, validate the effectiveness of our model. We experimentally demonstrate
that our proposed constrained and direct optimization algorithms significantly
outperform state-of-the-art methods by over $20\%$, proving their
cost-efficiency and real-world impact. The versatile methods can be applied to
various product scenarios, including optimal treatment allocation. Its
effectiveness has also been validated through successful worldwide production
deployments.

</details>


### [112] [Deep Learning of Continuous and Structured Policies for Aggregated Heterogeneous Treatment Effects](https://arxiv.org/abs/2507.05511)
*Jennifer Y. Zhang,Shuyang Du,Will Y. Zou*

Main category: cs.LG

TL;DR: 本文针对异质治疗效应（HTE）估计，将治疗行动空间从二元变量拓展到结构化治疗策略，提出通用深度学习框架并在公开数据集验证效果。


<details>
  <summary>Details</summary>
Motivation: 随着HTE估计在多领域应用，治疗行动空间需从二元变量拓展到结构化治疗策略。

Method: 从基本原理推导将多治疗策略变量纳入个体和平均治疗效应函数形式，构建Neural - Augmented Naive Bayes层，结合连续治疗变量、治疗分配等直接对主体排名。

Result: 构建了异质治疗策略深度学习通用框架，在公开数据集上展示了提升性能的能力。

Conclusion: 所提方法能有效拓展治疗行动空间，构建的框架可提升异质治疗策略深度学习性能。

Abstract: As estimation of Heterogeneous Treatment Effect (HTE) is increasingly adopted
across a wide range of scientific and industrial applications, the treatment
action space can naturally expand, from a binary treatment variable to a
structured treatment policy. This policy may include several policy factors
such as a continuous treatment intensity variable, or discrete treatment
assignments. From first principles, we derive the formulation for incorporating
multiple treatment policy variables into the functional forms of individual and
average treatment effects. Building on this, we develop a methodology to
directly rank subjects using aggregated HTE functions. In particular, we
construct a Neural-Augmented Naive Bayes layer within a deep learning framework
to incorporate an arbitrary number of factors that satisfies the Naive Bayes
assumption. The factored layer is then applied with continuous treatment
variables, treatment assignment, and direct ranking of aggregated treatment
effect functions. Together, these algorithms build towards a generic framework
for deep learning of heterogeneous treatment policies, and we show their power
to improve performance with public datasets.

</details>


### [113] [Mitigating Shortcut Learning with InterpoLated Learning](https://arxiv.org/abs/2507.05527)
*Michalis Korakakis,Andreas Vlachos,Adrian Weller*

Main category: cs.LG

TL;DR: 提出InterpoLL方法缓解经验风险最小化（ERM）中捷径问题，实验表明该方法能提升少数样本泛化能力且具广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 现有ERM方法让模型依赖捷径，影响少数样本泛化，且现有捷径缓解方法存在模型特定、难调参、计算昂贵等问题。

Method: 提出InterpoLated Learning (InterpoLL)，对多数样本的表征进行插值，融入来自少数样本的特征以减轻捷径影响。

Result: 在多个自然语言理解任务上，InterpoLL提升了少数样本的泛化能力，且不降低多数样本的准确率，在多种架构上均有效。

Conclusion: InterpoLL方法能有效缓解捷径问题，提升模型泛化能力，且具有广泛的适用性。

Abstract: Empirical risk minimization (ERM) incentivizes models to exploit shortcuts,
i.e., spurious correlations between input attributes and labels that are
prevalent in the majority of the training data but unrelated to the task at
hand. This reliance hinders generalization on minority examples, where such
correlations do not hold. Existing shortcut mitigation approaches are
model-specific, difficult to tune, computationally expensive, and fail to
improve learned representations. To address these issues, we propose
InterpoLated Learning (InterpoLL) which interpolates the representations of
majority examples to include features from intra-class minority examples with
shortcut-mitigating patterns. This weakens shortcut influence, enabling models
to acquire features predictive across both minority and majority examples.
Experimental results on multiple natural language understanding tasks
demonstrate that InterpoLL improves minority generalization over both ERM and
state-of-the-art shortcut mitigation methods, without compromising accuracy on
majority examples. Notably, these gains persist across encoder,
encoder-decoder, and decoder-only architectures, demonstrating the method's
broad applicability.

</details>


### [114] [Bit-Flip Fault Attack: Crushing Graph Neural Networks via Gradual Bit Search](https://arxiv.org/abs/2507.05531)
*Sanaz Kazemi Abharian,Sai Manoj Pudukotai Dinakarrao*

Main category: cs.LG

TL;DR: 本文研究GNN模型对硬件故障攻击的脆弱性，提出GBFA攻击方法，通过两步操作攻击特定层并识别脆弱位，实验表明GBFA显著降低预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有硬件加速器满足GNN性能需求，但硬件攻击的安全挑战被忽视，需研究GNN模型对硬件故障攻击的脆弱性。

Method: 提出GBFA攻击，先创建马尔可夫模型预测层执行序列以攻击特定层，再通过层内搜索和梯度排序识别脆弱位。

Result: 在Cora和PubMed数据集上评估，GBFA显著降低预测准确率，不同层受影响不同，如在Cora数据集上最后一层单比特翻转使GraphSAGE预测准确率降低17%。

Conclusion: GBFA攻击有效，凸显GNN采用层感知攻击策略的重要性。

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful machine learning
method for graph-structured data. A plethora of hardware accelerators has been
introduced to meet the performance demands of GNNs in real-world applications.
However, security challenges of hardware-based attacks have been generally
overlooked. In this paper, we investigate the vulnerability of GNN models to
hardware-based fault attack, wherein an attacker attempts to misclassify output
by modifying trained weight parameters through fault injection in a memory
device. Thus, we propose Gradual Bit-Flip Fault Attack (GBFA), a layer-aware
bit-flip fault attack, selecting a vulnerable bit in each selected weight
gradually to compromise the GNN's performance by flipping a minimal number of
bits. To achieve this, GBFA operates in two steps. First, a Markov model is
created to predict the execution sequence of layers based on features extracted
from memory access patterns, enabling the launch of the attack within a
specific layer. Subsequently, GBFA identifies vulnerable bits within the
selected weights using gradient ranking through an in-layer search. We evaluate
the effectiveness of the proposed GBFA attack on various GNN models for node
classification tasks using the Cora and PubMed datasets. Our findings show that
GBFA significantly degrades prediction accuracy, and the variation in its
impact across different layers highlights the importance of adopting a
layer-aware attack strategy in GNNs. For example, GBFA degrades GraphSAGE's
prediction accuracy by 17% on the Cora dataset with only a single bit flip in
the last layer.

</details>


### [115] [Theoretical Learning Performance of Graph Neural Networks: The Impact of Jumping Connections and Layer-wise Sparsification](https://arxiv.org/abs/2507.05533)
*Jiawei Sun,Hongkang Li,Meng Wang*

Main category: cs.LG

TL;DR: 本文对使用图稀疏化的带跳跃连接的图卷积网络（GCNs）进行学习动态和泛化分析，揭示跳跃连接对各层稀疏化要求的影响，并在基准数据集上验证理论结果。


<details>
  <summary>Details</summary>
Motivation: 学习带图稀疏化的GCNs在应用中取得实证成功，但对泛化保证的理论理解有限，现有分析忽略图稀疏化或跳跃连接。

Method: 对使用图稀疏化的带跳跃连接的GCNs进行学习动态和泛化分析。

Result: 学习模型的泛化精度接近依赖于稀疏有效邻接矩阵 $A^*$ 的目标函数类中的最高可实现精度；跳跃连接导致各层稀疏化要求不同，在两层隐藏层GCN中，第一层稀疏矩阵与 $A^*$ 的偏差对泛化影响更大。

Conclusion: 图稀疏化在 $A^*$ 保留支持有意义消息传播的关键边时能保持泛化性能，本文首次对跳跃连接在稀疏化要求中的作用进行理论刻画。

Abstract: Jumping connections enable Graph Convolutional Networks (GCNs) to overcome
over-smoothing, while graph sparsification reduces computational demands by
selecting a sub-matrix of the graph adjacency matrix during neighborhood
aggregation. Learning GCNs with graph sparsification has shown empirical
success across various applications, but a theoretical understanding of the
generalization guarantees remains limited, with existing analyses ignoring
either graph sparsification or jumping connections. This paper presents the
first learning dynamics and generalization analysis of GCNs with jumping
connections using graph sparsification. Our analysis demonstrates that the
generalization accuracy of the learned model closely approximates the highest
achievable accuracy within a broad class of target functions dependent on the
proposed sparse effective adjacency matrix $A^*$. Thus, graph sparsification
maintains generalization performance when $A^*$ preserves the essential edges
that support meaningful message propagation. We reveal that jumping connections
lead to different sparsification requirements across layers. In a
two-hidden-layer GCN, the generalization is more affected by the sparsified
matrix deviations from $A^*$ of the first layer than the second layer. To the
best of our knowledge, this marks the first theoretical characterization of
jumping connections' role in sparsification requirements. We validate our
theoretical results on benchmark datasets in deep GCNs.

</details>


### [116] [Robust Learning on Noisy Graphs via Latent Space Constraints with External Knowledge](https://arxiv.org/abs/2507.05540)
*Chunhui Gu,Mohammad Sadegh Nasr,James P. Long,Kim-Anh Do,Ehsan Irajizad*

Main category: cs.LG

TL;DR: 提出LSC - GNN处理图神经网络中的噪声边，实验表明其性能优且可扩展到异构图。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络难以处理噪声边的问题。

Method: 提出LSC - GNN，训练两个编码器，对其潜在表示差异进行惩罚以避免过拟合噪声边。

Result: 在基准数据集上LSC - GNN优于标准和抗噪GNN，扩展到异构图在蛋白质 - 代谢物网络验证有效。

Conclusion: LSC - GNN能提升含噪声关系结构场景下的预测性能和可解释性。

Abstract: Graph Neural Networks (GNNs) often struggle with noisy edges. We propose
Latent Space Constrained Graph Neural Networks (LSC-GNN) to incorporate
external "clean" links and guide embeddings of a noisy target graph. We train
two encoders--one on the full graph (target plus external edges) and another on
a regularization graph excluding the target's potentially noisy links--then
penalize discrepancies between their latent representations. This constraint
steers the model away from overfitting spurious edges. Experiments on benchmark
datasets show LSC-GNN outperforms standard and noise-resilient GNNs in graphs
subjected to moderate noise. We extend LSC-GNN to heterogeneous graphs and
validate it on a small protein-metabolite network, where metabolite-protein
interactions reduce noise in protein co-occurrence data. Our results highlight
LSC-GNN's potential to boost predictive performance and interpretability in
settings with noisy relational structures.

</details>


### [117] [Gait-Based Hand Load Estimation via Deep Latent Variable Models with Auxiliary Information](https://arxiv.org/abs/2507.05544)
*Jingyi Gao,Sol Lim,Seokhyun Chung*

Main category: cs.LG

TL;DR: 本文提出增强负载估计框架，结合辅助信息提升手动物料搬运中负载估计准确性，实验证明结合辅助信息有效。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法用于手动物料搬运人体工程学风险评估时，依赖直接映射，泛化和预测精度受限。

Method: 提出结合辅助信息（空载步态模式和搬运方式）的增强负载估计框架，集成深度潜变量建模、时间卷积网络和双向交叉注意力机制。

Result: 使用惯性测量单元收集的真实数据实验表明，结合辅助信息显著提高了准确性。

Conclusion: 结合辅助信息可提升负载估计准确性，显式融合机制比简单特征拼接更重要。

Abstract: Machine learning methods are increasingly applied to ergonomic risk
assessment in manual material handling, particularly for estimating carried
load from gait motion data collected from wearable sensors. However, existing
approaches often rely on direct mappings from loaded gait to hand load,
limiting generalization and predictive accuracy. In this study, we propose an
enhanced load estimation framework that incorporates auxiliary information,
including baseline gait patterns during unloaded walking and carrying style.
While baseline gait can be automatically captured by wearable sensors and is
thus readily available at inference time, carrying style typically requires
manual labeling and is often unavailable during deployment. Our model
integrates deep latent variable modeling with temporal convolutional networks
and bi-directional cross-attention to capture gait dynamics and fuse loaded and
unloaded gait patterns. Guided by domain knowledge, the model is designed to
estimate load magnitude conditioned on carrying style, while eliminating the
need for carrying style labels at inference time. Experiments using real-world
data collected from inertial measurement units attached to participants
demonstrate substantial accuracy gains from incorporating auxiliary information
and highlight the importance of explicit fusion mechanisms over naive feature
concatenation.

</details>


### [118] [Preemptive Solving of Future Problems: Multitask Preplay in Humans and Machines](https://arxiv.org/abs/2507.05561)
*Wilka Carvalho,Sam Hall-McMaster,Honglak Lee,Samuel J. Gershman*

Main category: cs.LG

TL;DR: 提出多任务预演算法，展示其在预测人类任务泛化及提升智能体多任务环境表现的效果。


<details>
  <summary>Details</summary>
Motivation: 研究人类如何利用一项任务经验预学习其他未执行任务的解决方案。

Method: 提出多任务预演算法，对未执行任务进行反事实模拟，学习预测性表征。

Result: 多任务预演在小网格世界和Craftax环境中能更好预测人类泛化能力，还能让智能体行为迁移到新环境。

Conclusion: 多任务预演是人类跨多任务反事实学习和泛化的可扩展理论，能提升智能体在多任务环境的表现。

Abstract: Humans can pursue a near-infinite variety of tasks, but typically can only
pursue a small number at the same time. We hypothesize that humans leverage
experience on one task to preemptively learn solutions to other tasks that were
accessible but not pursued. We formalize this idea as Multitask Preplay, a
novel algorithm that replays experience on one task as the starting point for
"preplay" -- counterfactual simulation of an accessible but unpursued task.
Preplay is used to learn a predictive representation that can support fast,
adaptive task performance later on. We first show that, compared to traditional
planning and predictive representation methods, multitask preplay better
predicts how humans generalize to tasks that were accessible but not pursued in
a small grid-world, even when people didn't know they would need to generalize
to these tasks. We then show these predictions generalize to Craftax, a
partially observable 2D Minecraft environment. Finally, we show that Multitask
Preplay enables artificial agents to learn behaviors that transfer to novel
Craftax worlds sharing task co-occurrence structure. These findings demonstrate
that Multitask Preplay is a scalable theory of how humans counterfactually
learn and generalize across multiple tasks; endowing artificial agents with the
same capacity can significantly improve their performance in challenging
multitask environments.

</details>


### [119] [The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation](https://arxiv.org/abs/2507.05578)
*Alexander Xiong,Xuandong Zhao,Aneesh Pappu,Dawn Song*

Main category: cs.LG

TL;DR: 本文综合研究大语言模型记忆现象，分析影响因素、检测和缓解方法，探讨其影响及挑战，指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在训练数据记忆现象，引发模型行为、隐私风险等问题，需研究应对。

Method: 综合近期研究，分析影响记忆的因素如训练数据重复、训练动态等；研究检测方法如基于前缀提取、成员推理等；探讨缓解策略如数据清理、差分隐私等。

Result: 对大语言模型记忆现象的现状进行了全面概述，包括技术、隐私和性能维度。

Conclusion: 指出平衡减少有害记忆与实用性存在挑战，明确未来研究的关键方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet they also exhibit memorization of their training
data. This phenomenon raises critical questions about model behavior, privacy
risks, and the boundary between learning and memorization. Addressing these
concerns, this paper synthesizes recent studies and investigates the landscape
of memorization, the factors influencing it, and methods for its detection and
mitigation. We explore key drivers, including training data duplication,
training dynamics, and fine-tuning procedures that influence data memorization.
In addition, we examine methodologies such as prefix-based extraction,
membership inference, and adversarial prompting, assessing their effectiveness
in detecting and measuring memorized content. Beyond technical analysis, we
also explore the broader implications of memorization, including the legal and
ethical implications. Finally, we discuss mitigation strategies, including data
cleaning, differential privacy, and post-training unlearning, while
highlighting open challenges in balancing the minimization of harmful
memorization with utility. This paper provides a comprehensive overview of the
current state of research on LLM memorization across technical, privacy, and
performance dimensions, identifying critical directions for future work.

</details>


### [120] [The Fourier Spectral Transformer Networks For Efficient and Generalizable Nonlinear PDEs Prediction](https://arxiv.org/abs/2507.05584)
*Beibei Li*

Main category: cs.LG

TL;DR: 提出统一的傅里叶谱Transformer网络，结合经典谱方法和基于注意力的神经网络架构优势，在PDEs上验证有效。


<details>
  <summary>Details</summary>
Motivation: 结合经典谱方法和基于注意力的神经网络架构优势，以实现复杂动力系统的实时预测和控制。

Method: 将原始PDEs转换为谱常微分方程，用高精度数值求解器生成训练数据，用Transformer网络对谱系数的演化建模。

Result: 在二维不可压缩Navier - Stokes方程和一维Burgers方程上，谱Transformer即使在有限训练数据下也能实现高精度长期预测，优于传统数值方法和机器学习方法。

Conclusion: 所提框架对未见数据泛化性好，为复杂动力系统的实时预测和控制带来有前景的范式。

Abstract: In this work we propose a unified Fourier Spectral Transformer network that
integrates the strengths of classical spectral methods and attention based
neural architectures. By transforming the original PDEs into spectral ordinary
differential equations, we use high precision numerical solvers to generate
training data and use a Transformer network to model the evolution of the
spectral coefficients. We demonstrate the effectiveness of our approach on the
two dimensional incompressible Navier-Stokes equations and the one dimensional
Burgers' equation. The results show that our spectral Transformer can achieve
highly accurate long term predictions even with limited training data, better
than traditional numerical methods and machine learning methods in forecasting
future flow dynamics. The proposed framework generalizes well to unseen data,
bringing a promising paradigm for real time prediction and control of complex
dynamical systems.

</details>


### [121] [Graph Learning](https://arxiv.org/abs/2507.05636)
*Feng Xia,Ciyuan Peng,Jing Ren,Falih Gozi Febrinanto,Renqiang Luo,Vidya Saikrishna,Shuo Yu,Xiangjie Kong*

Main category: cs.LG

TL;DR: 本文全面介绍图学习，回顾技术、探讨伦理考量、识别新兴话题，为相关人员提供导航资源。


<details>
  <summary>Details</summary>
Motivation: 图学习能建模复杂非欧关系，应用广泛，但面临诸多挑战，需全面介绍以促进发展。

Method: 对可扩展、时态、多模态、生成式、可解释和负责任的图学习等关键维度进行综述。

Result: 回顾了处理大规模图、捕捉时间依赖、整合多模态数据等方面的先进技术，探讨了伦理考量，识别了新兴话题。

Conclusion: 该综述为研究人员和从业者在快速发展的图学习领域提供了有价值的参考。

Abstract: Graph learning has rapidly evolved into a critical subfield of machine
learning and artificial intelligence (AI). Its development began with early
graph-theoretic methods, gaining significant momentum with the advent of graph
neural networks (GNNs). Over the past decade, progress in scalable
architectures, dynamic graph modeling, multimodal learning, generative AI,
explainable AI (XAI), and responsible AI has broadened the applicability of
graph learning to various challenging environments. Graph learning is
significant due to its ability to model complex, non-Euclidean relationships
that traditional machine learning struggles to capture, thus better supporting
real-world applications ranging from drug discovery and fraud detection to
recommender systems and scientific reasoning. However, challenges like
scalability, generalization, heterogeneity, interpretability, and
trustworthiness must be addressed to unlock its full potential. This survey
provides a comprehensive introduction to graph learning, focusing on key
dimensions including scalable, temporal, multimodal, generative, explainable,
and responsible graph learning. We review state-of-the-art techniques for
efficiently handling large-scale graphs, capturing dynamic temporal
dependencies, integrating heterogeneous data modalities, generating novel graph
samples, and enhancing interpretability to foster trust and transparency. We
also explore ethical considerations, such as privacy and fairness, to ensure
responsible deployment of graph learning models. Additionally, we identify and
discuss emerging topics, highlighting recent integration of graph learning and
other AI paradigms and offering insights into future directions. This survey
serves as a valuable resource for researchers and practitioners seeking to
navigate the rapidly evolving landscape of graph learning.

</details>


### [122] [Canine Clinical Gait Analysis for Orthopedic and Neurological Disorders: An Inertial Deep-Learning Approach](https://arxiv.org/abs/2507.05671)
*Netta Palez,Léonie Straß,Sebastian Meller,Holger Volk,Anna Zamansky,Itzik Klein*

Main category: cs.LG

TL;DR: 研究用惯性传感器读数的深度学习方法评估犬类神经和骨科步态以辅助步态分析，用29只犬数据集实验，多分类和二分类任务获高准确率，证明该模型有实用诊断价值。


<details>
  <summary>Details</summary>
Motivation: 犬类神经和骨科病症难区分，利用可穿戴惯性传感器的步态分析有价值，需探索深度学习方法辅助步态分析。

Method: 使用惯性传感器读数开发深度学习方法，探索传感器配置、评估协议变化及改进深度学习模型架构。

Result: 用29只犬数据集，多分类任务（健康/骨科/神经）准确率96%，二分类任务（健康/非健康）泛化到未见犬准确率82%。

Conclusion: 基于惯性的深度学习模型有潜力成为区分骨科和神经病症步态评估的实用客观诊断和临床辅助工具。

Abstract: Canine gait analysis using wearable inertial sensors is gaining attention in
veterinary clinical settings, as it provides valuable insights into a range of
mobility impairments. Neurological and orthopedic conditions cannot always be
easily distinguished even by experienced clinicians. The current study explored
and developed a deep learning approach using inertial sensor readings to assess
whether neurological and orthopedic gait could facilitate gait analysis. Our
investigation focused on optimizing both performance and generalizability in
distinguishing between these gait abnormalities. Variations in sensor
configurations, assessment protocols, and enhancements to deep learning model
architectures were further suggested. Using a dataset of 29 dogs, our proposed
approach achieved 96% accuracy in the multiclass classification task
(healthy/orthopedic/neurological) and 82% accuracy in the binary classification
task (healthy/non-healthy) when generalizing to unseen dogs. Our results
demonstrate the potential of inertial-based deep learning models to serve as a
practical and objective diagnostic and clinical aid to differentiate gait
assessment in orthopedic and neurological conditions.

</details>


### [123] [Efficient Training of Large-Scale AI Models Through Federated Mixture-of-Experts: A System-Level Approach](https://arxiv.org/abs/2507.05685)
*Xiaobing Chen,Boyang Zhang,Xiangwei Zhou,Mingxuan Sun,Shuai Zhang,Songyang Zhang,Geoffrey Ye Li*

Main category: cs.LG

TL;DR: 文章探讨联邦学习与混合专家模型集成训练大规模AI模型的系统挑战，提出智能客户端 - 专家对齐概念设计以提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习与混合专家模型集成训练大规模AI模型时，因客户端资源异构和专家协调复杂带来的系统级挑战。

Method: 提出包含动态适应度评分、全局专家负载监测和客户端容量分析的智能客户端 - 专家对齐概念设计。

Result: 可实现更具扩展性、效率和鲁棒性的训练机制，减少收敛所需通信轮数。

Conclusion: 为大规模联邦混合专家结构AI模型在边缘计算中的广泛部署铺平道路。

Abstract: The integration of Federated Learning (FL) and Mixture-of-Experts (MoE)
presents a compelling pathway for training more powerful, large-scale
artificial intelligence models (LAMs) on decentralized data while preserving
privacy. However, efficient federated training of these complex MoE-structured
LAMs is hindered by significant system-level challenges, particularly in
managing the interplay between heterogeneous client resources and the
sophisticated coordination required for numerous specialized experts. This
article highlights a critical, yet underexplored concept: the absence of robust
quantitative strategies for dynamic client-expert alignment that holistically
considers varying client capacities and the imperative for system-wise load
balancing. Specifically, we propose a conceptual system design for intelligent
client-expert alignment that incorporates dynamic fitness scoring, global
expert load monitoring, and client capacity profiling. By tackling these
systemic issues, we can unlock more scalable, efficient, and robust training
mechanisms {with fewer communication rounds for convergence}, paving the way
for the widespread deployment of large-scale federated MoE-structured LAMs in
edge computing with ultra-high communication efficiency.

</details>


### [124] [AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs](https://arxiv.org/abs/2507.05687)
*Shangzhan Li,Zefan Wang,Ye He,Yuxuan Li,Qi Shi,Jianling Li,Yonggang Hu,Wanxiang Che,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: 介绍基于强化学习的Triton编程模型AutoTriton，实验显示其性能与主流大模型相当，为构建高效AI系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前Triton编程需手动调参，存在性能优化和广泛应用障碍，需自动化方法。

Method: AutoTriton先进行监督微调获取Triton编程知识，再用GRPO算法进行强化学习，结合规则和执行奖励提升编程能力。

Result: AutoTriton 8B模型在五个评估通道实验中性能与Claude - 4 - Sonnet和DeepSeek - R1 - 0528相当，且各模块作用关键。

Conclusion: 强化学习用于自动生成高性能内核有前景，为构建高效AI系统奠定重要基础。

Abstract: Kernel development in deep learning requires optimizing computational units
across hardware while balancing memory management, parallelism, and
hardware-specific optimizations through extensive empirical tuning. Although
domain-specific languages like Triton simplify GPU programming by abstracting
low-level details, developers must still manually tune critical parameters such
as tile sizes and memory access patterns through iterative experimentation,
creating substantial barriers to optimal performance and wider adoption. In
this work, we introduce AutoTriton, the first model dedicated to Triton
programming powered by reinforcement learning (RL). AutoTriton performs
supervised fine-tuning (SFT) to be equipped with essential Triton programming
expertise using a high-quality data gathering pipeline, and conducts RL with
Group Relative Policy Optimization (GRPO) algorithm, combining a rule-based
reward and an execution-based reward to further improve Triton programming
ability, sequentially. Experiments across five evaluation channels of
TritonBench and KernelBench illustrate that our 8B model AutoTriton achieves
performance comparable to mainstream large models, including Claude-4-Sonnet
and DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial
role of each module within AutoTriton, including the SFT stage, the RL stage,
and the reward design strategy. These findings underscore the promise of RL for
automatically generating high-performance kernels, and since high-performance
kernels are core components of AI systems, this breakthrough establishes an
important foundation for building more efficient AI systems. The model and code
will be available at https://github.com/AI9Stars/AutoTriton.

</details>


### [125] [MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment](https://arxiv.org/abs/2507.05720)
*Yucheng Shi,Wenhao Yu,Zaitang Li,Yonglin Wang,Hongming Zhang,Ninghao Liu,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 提出可在在线环境训练GUI代理的可扩展框架MobileGUI - RL，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于预收集轨迹在离线环境训练GUI代理的方法存在可扩展性受限、过拟合和策略脆弱等问题。

Method: MobileGUI - RL框架包含通过自我探索和过滤合成可学习任务课程，以及将GRPO应用于GUI导航，采用轨迹感知优势和综合奖励平衡任务成功与执行效率。

Result: 在三个在线移动代理基准测试中取得一致增益。

Conclusion: 所提方法有效。

Abstract: Recently, there has been a surge of vision-based GUI agents designed to
automate everyday mobile and web tasks. These agents interpret raw GUI
screenshots and autonomously decide where to click, scroll, or type, which
bypasses handcrafted rules and app-specific APIs. However, most existing
methods trained GUI agent in the offline environment using pre-collected
trajectories. This approach limits scalability, causes overfitting to specific
UI templates, and leads to brittle policies when faced with unseen environment.
We present MobileGUI-RL, a scalable framework that trains GUI agent in online
environment. MobileGUI-RL contains two key components. It (i) synthesizes a
curriculum of learnable tasks through self-exploration and filtering, and (ii)
adapts GRPO to GUI navigation with trajectory-aware advantages and composite
rewards that balance task success and execution efficiency. Experiments on
three online mobile-agent benchmarks show consistent gains, validating the
effectiveness of our approach.

</details>


### [126] [Hierarchical Task Offloading for UAV-Assisted Vehicular Edge Computing via Deep Reinforcement Learning](https://arxiv.org/abs/2507.05722)
*Hongbao Li,Ziye Jia,Sijie He,Kun Guo,Qihui Wu*

Main category: cs.LG

TL;DR: 针对车联网应用，提出基于部分卸载的双层无人机辅助边缘计算架构及分层卸载方案，仿真显示性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有无人机辅助卸载策略在协调异构计算资源和适应动态网络条件方面不足。

Method: 提出双层架构，将联合优化问题转化为马尔可夫决策过程，基于软演员 - 评论家算法提出分层卸载方案，解耦全局和局部决策。

Result: 仿真表明所提方法在任务完成率、系统效率和收敛速度上优于多个基线。

Conclusion: 所提方法在动态车联网环境中具有强鲁棒性和适用性。

Abstract: With the emergence of compute-intensive and delay-sensitive applications in
vehicular networks, unmanned aerial vehicles (UAVs) have emerged as a promising
complement for vehicular edge computing due to the high mobility and flexible
deployment. However, the existing UAV-assisted offloading strategies are
insufficient in coordinating heterogeneous computing resources and adapting to
dynamic network conditions. Hence, this paper proposes a dual-layer
UAV-assisted edge computing architecture based on partial offloading, composed
of the relay capability of high-altitude UAVs and the computing support of
low-altitude UAVs. The proposed architecture enables efficient integration and
coordination of heterogeneous resources. A joint optimization problem is
formulated to minimize the system delay and energy consumption while ensuring
the task completion rate. To solve the high-dimensional decision problem, we
reformulate the problem as a Markov decision process and propose a hierarchical
offloading scheme based on the soft actor-critic algorithm. The method
decouples global and local decisions, where the global decisions integrate
offloading ratios and trajectory planning into continuous actions, while the
local scheduling is handled via designing a priority-based mechanism.
Simulations are conducted and demonstrate that the proposed approach
outperforms several baselines in task completion rate, system efficiency, and
convergence speed, showing strong robustness and applicability in dynamic
vehicular environments.

</details>


### [127] [Jigsaw: Training Multi-Billion-Parameter AI Weather Models with Optimized Model Parallelism](https://arxiv.org/abs/2507.05753)
*Deifilia Kieckhefen,Markus Götz,Lars H. Heyen,Achim Streit,Charlotte Debus*

Main category: cs.LG

TL;DR: 本文介绍WeatherMixer架构和Jigsaw并行方案，可提升气象模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 基于AI的气象预报对模型训练需求大，加速器内存和I/O带宽成瓶颈，需解决计算问题。

Method: 引入WeatherMixer架构，提出结合域并行和张量并行的Jigsaw模型并行方案。

Result: 在计算通信受限和I/O带宽受限系统中表现超现有技术，扩展到256个GPU有高计算性能和缩放效率。

Conclusion: Jigsaw方案有效解决气象模型训练瓶颈，提升训练效率。

Abstract: AI-based methods have revolutionized atmospheric forecasting, with recent
successes in medium-range forecasting spurring the development of climate
foundation models. Accurate modeling of complex atmospheric dynamics at high
spatial resolutions and longer lead times requires large neural networks and
gigabyte-sized data samples, making accelerator memory and I/O-bandwidth the
bottlenecks for model training. We introduce WeatherMixer, a
multi-layer-perceptron-based architecture whose workload scales linearly with
input size, allowing the model to learn global weather phenomena at accuracies
similar to numerical weather prediction. To cope with the computational demand,
we propose Jigsaw, a novel model parallelization scheme that employs both
domain and tensor parallelism, eliminating memory redundancy. Jigsaw exceeds
state-of-the-art performance in strong scaling in compute-communication-limited
systems and achieves superscalar weak scaling in I/O-bandwidth-limited systems.
We scale training to 256 GPUs, reaching peak performances of 9 and 11 PFLOPs,
23% and 28% of theoretical peaks, achieving 68% and 72% scaling efficiency
versus 51% without model parallelism.

</details>


### [128] [From Motion to Meaning: Biomechanics-Informed Neural Network for Explainable Cardiovascular Disease Identification](https://arxiv.org/abs/2507.05783)
*Comte Valentin,Gemma Piella,Mario Ceresa,Miguel A. Gonzalez Ballester*

Main category: cs.LG

TL;DR: 提出结合深度学习图像配准与物理信息正则化的方法预测心脏组织生物力学特性并分类疾病，在ACDC数据集上取得高成绩，提高诊断准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 心脏病是全球发病和死亡的主要原因之一，需要准确及时的诊断策略。

Method: 结合深度学习图像配准与物理信息正则化，用Neo - Hookean材料的能量应变公式建模，优化变形场；评估五种分类算法，用特征选择算法确定相关特征。

Result: 在ACDC数据集上左心室腔、右心室腔和心肌的Dice分数分别为0.945、0.908和0.905；最佳分类器在训练集和测试集的分类准确率分别为98%和100%。

Conclusion: 该方法结合可解释人工智能，能让临床医生透明理解模型预测，提高心脏病诊断准确性和可靠性，为个性化有效治疗奠定基础。

Abstract: Cardiac diseases are among the leading causes of morbidity and mortality
worldwide, which requires accurate and timely diagnostic strategies. In this
study, we introduce an innovative approach that combines deep learning image
registration with physics-informed regularization to predict the biomechanical
properties of moving cardiac tissues and extract features for disease
classification. We utilize the energy strain formulation of Neo-Hookean
material to model cardiac tissue deformations, optimizing the deformation field
while ensuring its physical and biomechanical coherence. This explainable
approach not only improves image registration accuracy, but also provides
insights into the underlying biomechanical processes of the cardiac tissues.
Evaluation on the Automated Cardiac Diagnosis Challenge (ACDC) dataset achieved
Dice scores of 0.945 for the left ventricular cavity, 0.908 for the right
ventricular cavity, and 0.905 for the myocardium. Subsequently, we estimate the
local strains within the moving heart and extract a detailed set of features
used for cardiovascular disease classification. We evaluated five
classification algorithms, Logistic Regression, Multi-Layer Perceptron, Support
Vector Classifier, Random Forest, and Nearest Neighbour, and identified the
most relevant features using a feature selection algorithm. The best performing
classifier obtained a classification accuracy of 98% in the training set and
100% in the test set of the ACDC dataset. By integrating explainable artificial
intelligence, this method empowers clinicians with a transparent understanding
of the model's predictions based on cardiac mechanics, while also significantly
improving the accuracy and reliability of cardiac disease diagnosis, paving the
way for more personalized and effective patient care.

</details>


### [129] [ABench-Physics: Benchmarking Physical Reasoning in LLMs via High-Difficulty and Dynamic Physics Problems](https://arxiv.org/abs/2507.04766)
*Yiming Zhang,Yingfan Ma,Yanmei Gu,Zhengkai Yang,Yihong Zhuang,Feng Wang,Zenan Huang,Yuanyuan Wang,Chao Huang,Bowen Song,Cheng Lin,Junbo Zhao*

Main category: cs.LG

TL;DR: 介绍用于评估大语言模型物理推理和泛化能力的新基准ABench - Physics，评估显示模型存在性能差距。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在物理领域能力未充分探索，现有基准有缺陷，需新基准评估其物理推理和泛化能力。

Method: 引入ABench - Physics基准，包含静态400个研究生或奥赛级问题Phy_A和动态100个带自动变异引擎问题Phy_B，问题需精确数值答案。

Result: 对多个先进大语言模型评估显示存在显著性能差距，物理推理有持续局限，尤其是对动态变体的泛化。

Conclusion: ABench - Physics为提升大语言模型科学推理提供了有挑战性和诊断性的框架。

Abstract: Large Language Models (LLMs) have shown impressive performance in domains
such as mathematics and programming, yet their capabilities in physics remain
underexplored and poorly understood. Physics poses unique challenges that
demand not only precise computation but also deep conceptual understanding and
physical modeling skills. Existing benchmarks often fall short due to limited
difficulty, multiple-choice formats, and static evaluation settings that fail
to capture physical modeling ability. In this paper, we introduce
ABench-Physics, a novel benchmark designed to rigorously evaluate LLMs'
physical reasoning and generalization capabilities. ABench-Physics consists of
two components: Phy_A, a static set of 400 graduate- or Olympiad-level
problems; and Phy_B, a dynamic subset of 100 problems equipped with an
automatic variation engine to test model robustness across changing conditions.
All questions require precise numerical answers, with strict formatting and
tolerance constraints. Our evaluation of several state-of-the-art LLMs reveals
substantial performance gaps, highlighting persistent limitations in physical
reasoning, especially in generalization to dynamic variants. ABench-Physics
provides a challenging and diagnostic framework for advancing scientific
reasoning in LLMs.

</details>


### [130] [Improving Robustness of Foundation Models in Domain Adaptation with Soup-Adapters](https://arxiv.org/abs/2507.05807)
*Marco Roschkowski*

Main category: cs.LG

TL;DR: 本文解决基础模型少样本领域适应的两个问题，提出Soup - Adapter方法，训练多个独立适配器并平均输出，性能好且更鲁棒，还可重新参数化为单个适配器，首次探索DINOv2的CLIP适配器技术。


<details>
  <summary>Details</summary>
Motivation: 解决少样本领域适应中因缺乏大验证数据集导致超参数调优不实际，以及模型在分布偏移下的鲁棒性问题。

Method: 训练多个独立适配器并平均其输出，使用参数的原则性拼接将集成重新参数化为单个适配器。

Result: 新模型性能更高，对分布偏移更鲁棒，对CLIP - Adapter的关键超参数残差比不敏感。

Conclusion: 提出的Soup - Adapter方法解决了上述两个问题，首次探索DINOv2的CLIP适配器技术并与CLIP进行比较。

Abstract: In this paper, we tackle two fundamental problems in few-shot domain
adaptation of foundation models. First, hyperparameter tuning is often
impractical due to the lack of large validation datasets. Second, model
robustness under distribution shifts where test time data deviates slightly
from training distributions, remains a concern. We show that by training
multiple independent adapters and averaging their outputs, the new model has a
higher performance and is more robust to distribution shifts compared to any
individual adapter. This improvement holds even when the adapters are trained
with diverse hyperparameters sampled from a wide range, resulting in varied
individual performance. Consequently, our method addresses both of the problems
described above. The ensemble is also significantly less sensitive to the
residual ratio, a critical hyperparameter of CLIP-Adapter. Since the ensemble
can be reparameterized to a single adapter again using a principled
concatenation of the parameters, we refer to our method as Soup-Adapter. This
is also the first study to explore CLIP adapter-style techniques for DINOv2 and
to directly compare them with CLIP in this setting.

</details>


### [131] [Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs](https://arxiv.org/abs/2507.05810)
*Sofiia Chorna,Kateryna Tarelkina,Eloïse Berthier,Gianni Franchi*

Main category: cs.LG

TL;DR: 提出将基于概念的可解释性方法扩展到机械可解释性领域的框架和工具BAGEL，可全局剖析模型行为，增强模型可信度。


<details>
  <summary>Details</summary>
Motivation: 传统基于概念的可解释性方法专注局部解释，需扩展到机械可解释性领域以全局剖析模型行为。

Method: 提出新框架，通过分析高层语义属性在模型内部组件中的出现、交互和传播来量化语义概念在各层的表示，开发可视化平台BAGEL展示结果。

Result: 框架系统量化了语义概念在各层的表示，揭示潜在电路和信息流，BAGEL以知识图谱呈现见解，便于用户探索概念 - 类别关系等。

Conclusion: 框架模型无关、可扩展，有助于深入理解深度学习模型在数据集偏差下的泛化能力。

Abstract: While concept-based interpretability methods have traditionally focused on
local explanations of neural network predictions, we propose a novel framework
and interactive tool that extends these methods into the domain of mechanistic
interpretability. Our approach enables a global dissection of model behavior by
analyzing how high-level semantic attributes (referred to as concepts) emerge,
interact, and propagate through internal model components. Unlike prior work
that isolates individual neurons or predictions, our framework systematically
quantifies how semantic concepts are represented across layers, revealing
latent circuits and information flow that underlie model decision-making. A key
innovation is our visualization platform that we named BAGEL (for Bias Analysis
with a Graph for global Explanation Layers), which presents these insights in a
structured knowledge graph, allowing users to explore concept-class
relationships, identify spurious correlations, and enhance model
trustworthiness. Our framework is model-agnostic, scalable, and contributes to
a deeper understanding of how deep learning models generalize (or fail to) in
the presence of dataset biases. The demonstration is available at
https://knowledge-graph-ui-4a7cb5.gitlab.io/.

</details>


### [132] [Fair Domain Generalization: An Information-Theoretic View](https://arxiv.org/abs/2507.05823)
*Tangzheng Lian,Guanyu Hu,Dimitrios Kollias,Xinyu Yang,Oya Celiktutan*

Main category: cs.LG

TL;DR: 文章研究公平领域泛化（FairDG）问题，推导基于互信息的上界，提出PAFDG框架，实验显示其在实用-公平权衡上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多数领域泛化方法不考虑算法公平性，公平方法未考虑领域偏移，为弥补这两个研究缺口，研究FairDG问题。

Method: 推导多类分类任务中基于互信息的期望风险和公平性违规的上界，引入PAFDG框架，通过帕累托优化解决FairDG问题并建模实用-公平权衡。

Result: 在真实世界的视觉和语言数据集上的实验表明，PAFDG在实用-公平权衡方面优于现有方法。

Conclusion: PAFDG框架能有效解决FairDG问题，在实用-公平权衡上有优势。

Abstract: Domain generalization (DG) and algorithmic fairness are two critical
challenges in machine learning. However, most DG methods focus only on
minimizing expected risk in the unseen target domain without considering
algorithmic fairness. Conversely, fairness methods typically do not account for
domain shifts, so the fairness achieved during training may not generalize to
unseen test domains. In this work, we bridge these gaps by studying the problem
of Fair Domain Generalization (FairDG), which aims to minimize both expected
risk and fairness violations in unseen target domains. We derive novel mutual
information-based upper bounds for expected risk and fairness violations in
multi-class classification tasks with multi-group sensitive attributes. These
bounds provide key insights for algorithm design from an information-theoretic
perspective. Guided by these insights, we introduce PAFDG (Pareto-Optimal
Fairness for Domain Generalization), a practical framework that solves the
FairDG problem and models the utility-fairness trade-off through Pareto
optimization. Experiments on real-world vision and language datasets show that
PAFDG achieves superior utility-fairness trade-offs compared to existing
methods.

</details>


### [133] [Robust Power System State Estimation using Physics-Informed Neural Networks](https://arxiv.org/abs/2507.05874)
*Solon Falas,Markos Asprou,Charalambos Konstantinou,Maria K. Michael*

Main category: cs.LG

TL;DR: 本文提出用物理信息神经网络（PINNs）增强电力系统状态估计的准确性和鲁棒性，实验表明该方法优于传统机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统在状态估计和实时监测中，尤其是在故障或网络攻击下的响应速度和准确性面临重大挑战。

Method: 提出使用物理信息神经网络（PINNs）的混合方法，将物理定律嵌入神经网络架构。

Result: 该方法优于传统机器学习模型，在训练数据集的未见子集上精度提高达83%，在全新不相关数据集上性能提高65%；在数据操纵攻击中，PINN比等效神经网络准确达93%。

Conclusion: 物理信息神经网络（PINNs）能有效提高电力系统状态估计的准确性和鲁棒性，还能应对安全问题。

Abstract: Modern power systems face significant challenges in state estimation and
real-time monitoring, particularly regarding response speed and accuracy under
faulty conditions or cyber-attacks. This paper proposes a hybrid approach using
physics-informed neural networks (PINNs) to enhance the accuracy and
robustness, of power system state estimation. By embedding physical laws into
the neural network architecture, PINNs improve estimation accuracy for
transmission grid applications under both normal and faulty conditions, while
also showing potential in addressing security concerns such as data
manipulation attacks. Experimental results show that the proposed approach
outperforms traditional machine learning models, achieving up to 83% higher
accuracy on unseen subsets of the training dataset and 65% better performance
on entirely new, unrelated datasets. Experiments also show that during a data
manipulation attack against a critical bus in a system, the PINN can be up to
93% more accurate than an equivalent neural network.

</details>


### [134] [Universal Embeddings of Tabular Data](https://arxiv.org/abs/2507.05904)
*Astrid Franz,Frederik Hoppe,Marianne Michaelis,Udo Göbel*

Main category: cs.LG

TL;DR: 提出用于表格数据的通用嵌入框架，将数据转图结构并借助图自编码器创建嵌入，在下游任务表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 表格数据在工业数据中占比大，且应用任务多样未预先指定，需通用嵌入方法。

Method: 将表格数据转换为图结构，利用图自编码器创建实体嵌入，聚合得到每行数据嵌入，用基于距离的相似度度量进行下游任务。

Result: 在真实数据集实验中，该方法表现优于现有通用表格数据嵌入技术。

Conclusion: 提出的框架能有效生成表格数据通用嵌入，适用于下游任务。

Abstract: Tabular data in relational databases represents a significant portion of
industrial data. Hence, analyzing and interpreting tabular data is of utmost
importance. Application tasks on tabular data are manifold and are often not
specified when setting up an industrial database. To address this, we present a
novel framework for generating universal, i.e., task-independent embeddings of
tabular data for performing downstream tasks without predefined targets. Our
method transforms tabular data into a graph structure, leverages Graph
Auto-Encoders to create entity embeddings, which are subsequently aggregated to
obtain embeddings for each table row, i.e., each data sample. This two-step
approach has the advantage that unseen samples, consisting of similar entities,
can be embedded without additional training. Downstream tasks such as
regression, classification or outlier detection, can then be performed by
applying a distance-based similarity measure in the embedding space.
Experiments on real-world datasets demonstrate that our method achieves
superior performance compared to existing universal tabular data embedding
techniques.

</details>


### [135] [Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why](https://arxiv.org/abs/2507.05906)
*Chenhao Li,Marco Hutter,Andreas Krause*

Main category: cs.LG

TL;DR: 该综述比较基于特征和基于GAN的示教学习方法，分析奖励函数结构及其对策略学习的影响，指出两者各有优劣，选择应基于任务优先级。


<details>
  <summary>Details</summary>
Motivation: 对基于特征和基于GAN的示教学习方法进行比较分析，明确方法选择的依据。

Method: 对比分析基于特征和基于GAN的示教学习方法在奖励函数结构、策略学习等方面的特点。

Result: 基于特征的方法在高保真运动模仿上出色但泛化难，基于GAN的方法可扩展性和适应性好但训练不稳定；两种范式都重视结构化运动表示。

Conclusion: 两种方法的差异越来越细微，方法选择应依据任务特定优先级，本文提供了示教学习方法选择的框架。

Abstract: This survey provides a comparative analysis of feature-based and GAN-based
approaches to learning from demonstrations, with a focus on the structure of
reward functions and their implications for policy learning. Feature-based
methods offer dense, interpretable rewards that excel at high-fidelity motion
imitation, yet often require sophisticated representations of references and
struggle with generalization in unstructured settings. GAN-based methods, in
contrast, use implicit, distributional supervision that enables scalability and
adaptation flexibility, but are prone to training instability and coarse reward
signals. Recent advancements in both paradigms converge on the importance of
structured motion representations, which enable smoother transitions,
controllable synthesis, and improved task integration. We argue that the
dichotomy between feature-based and GAN-based methods is increasingly nuanced:
rather than one paradigm dominating the other, the choice should be guided by
task-specific priorities such as fidelity, diversity, interpretability, and
adaptability. This work outlines the algorithmic trade-offs and design
considerations that underlie method selection, offering a framework for
principled decision-making in learning from demonstrations.

</details>


### [136] [Diffusion Dataset Condensation: Training Your Diffusion Model Faster with Less Data](https://arxiv.org/abs/2507.05914)
*Rui Huang,Shitong Shao,Zikai Zhou,Pukun Zhao,Hangyu Guo,Tian Ye,Lichen Bai,Shuo Yang,Zeke Xie*

Main category: cs.LG

TL;DR: 本文聚焦扩散模型训练资源消耗大的问题，提出扩散数据集凝聚（D2C）框架，实验表明该框架能大幅减少数据量并加速训练，同时保持高视觉质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型训练资源消耗大，从数据中心视角研究扩散数据集凝聚问题，以降低训练成本。

Method: 提出D2C框架，包含Select和Attach两个阶段，Select阶段用扩散难度分数和区间采样选子集，Attach阶段附加语义和视觉表征强化条件信号。

Result: 在多种数据集、模型架构和分辨率上实验，D2C框架大幅加速扩散模型训练，减少数据使用量，如SiT - XL/2架构实现100倍训练加速，仅用0.8%训练数据在40k步达到FID分数4.3。

Conclusion: D2C框架能在大幅减少数据量的情况下显著加速扩散模型训练，并保持高视觉质量。

Abstract: Diffusion models have achieved remarkable success in various generative
tasks, but training them remains highly resource-intensive, often requiring
millions of images and many days of GPU computation. From a data-centric
perspective addressing this limitation, we study diffusion dataset condensation
as a new and challenging problem setting. The goal is to construct a
"synthetic" sub-dataset with significantly fewer samples than the original
dataset, enabling high-quality diffusion model training with greatly reduced
cost. To the best of our knowledge, we are the first to formally investigate
dataset condensation for diffusion models, whereas prior work focused on
training discriminative models. To tackle this new challenge, we propose a
novel Diffusion Dataset Condensation (D2C) framework, which consists of two
phases: Select and Attach. The Select phase identifies a compact and diverse
subset using a diffusion difficulty score and interval sampling. The Attach
phase enhances the selected subset by attaching rich semantic and visual
representations to strengthen the conditional signals. Extensive experiments
across various dataset sizes, model architectures, and resolutions show that
our D2C framework enables significantly faster diffusion model training with
dramatically fewer data, while preserving high visual quality. Notably, for the
SiT-XL/2 architecture, D2C achieves a 100x training speed-up, reaching a FID
score of 4.3 in just 40k steps using only 0.8% of the training data.

</details>


### [137] [Improving AI-Based Canine Heart Disease Diagnosis with Expert-Consensus Auscultation Labeling](https://arxiv.org/abs/2507.05950)
*Pinar Bisgin,Tom Strube,Niklas Tschorn,Michael Pantförder,Maximilian Fecke,Ingrid Ljungvall,Jens Häggström,Gerhard Wess,Christoph Schummer,Sven Meister,Falk M. Howar*

Main category: cs.LG

TL;DR: 研究处理兽医AI模型训练中的噪声标签问题，用多专家意见减少犬心脏听诊数据标签噪声，评估三种算法，XGBoost表现最佳，证明减少标签噪声对犬心脏杂音检测算法的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决兽医医学中AI模型训练里噪声标签对犬心脏听诊数据分类性能的负面影响问题。

Method: 用多专家意见标注140条心脏声音记录数据集，选70条高质量记录得到降噪数据集，利用单个心动周期扩展训练数据，评估AdaBoost、XGBoost和Random Forest三种分类算法。

Result: XGBoost分类准确率显著提升，所有算法在降噪后分类准确率均提高，不同程度杂音检测的灵敏度和特异性均有提升。

Conclusion: 减少标签噪声对改进犬心脏杂音检测分类算法很重要。

Abstract: Noisy labels pose significant challenges for AI model training in veterinary
medicine. This study examines expert assessment ambiguity in canine
auscultation data, highlights the negative impact of label noise on
classification performance, and introduces methods for label noise reduction.
To evaluate whether label noise can be minimized by incorporating multiple
expert opinions, a dataset of 140 heart sound recordings (HSR) was annotated
regarding the intensity of holosystolic heart murmurs caused by Myxomatous
Mitral Valve Disease (MMVD). The expert opinions facilitated the selection of
70 high-quality HSR, resulting in a noise-reduced dataset. By leveraging
individual heart cycles, the training data was expanded and classification
robustness was enhanced. The investigation encompassed training and evaluating
three classification algorithms: AdaBoost, XGBoost, and Random Forest. While
AdaBoost and Random Forest exhibited reasonable performances, XGBoost
demonstrated notable improvements in classification accuracy. All algorithms
showed significant improvements in classification accuracy due to the applied
label noise reduction, most notably XGBoost. Specifically, for the detection of
mild heart murmurs, sensitivity increased from 37.71% to 90.98% and specificity
from 76.70% to 93.69%. For the moderate category, sensitivity rose from 30.23%
to 55.81% and specificity from 64.56% to 97.19%. In the loud/thrilling
category, sensitivity and specificity increased from 58.28% to 95.09% and from
84.84% to 89.69%, respectively. These results highlight the importance of
minimizing label noise to improve classification algorithms for the detection
of canine heart murmurs. Index Terms: AI diagnosis, canine heart disease, heart
sound classification, label noise reduction, machine learning, XGBoost,
veterinary cardiology, MMVD.

</details>


### [138] [Simple Convergence Proof of Adam From a Sign-like Descent Perspective](https://arxiv.org/abs/2507.05966)
*Hanyang Peng,Shuang Qin,Yue Yu,Fangqing Jiang,Hui Wang,Zhouchen Lin*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Adam is widely recognized as one of the most effective optimizers for
training deep neural networks (DNNs). Despite its remarkable empirical success,
its theoretical convergence analysis remains unsatisfactory. Existing works
predominantly interpret Adam as a preconditioned stochastic gradient descent
with momentum (SGDM), formulated as $\bm{x}_{t+1} = \bm{x}_t -
\frac{\gamma_t}{{\sqrt{\bm{v}_t}+\epsilon}} \circ \bm{m}_t$. This perspective
necessitates strong assumptions and intricate techniques, resulting in lengthy
and opaque convergence proofs that are difficult to verify and extend. In
contrast, we propose a novel interpretation by treating Adam as a sign-like
optimizer, expressed as $\bm{x}_{t+1} = \bm{x}_t - \gamma_t
\frac{|\bm{m}_t|}{{\sqrt{\bm{v}_t}+\epsilon}} \circ {\rm Sign}(\bm{m}_t)$. This
reformulation significantly simplifies the convergence analysis. For the first
time, with some mild conditions, we prove that Adam achieves the optimal rate
of ${\cal O}(\frac{1}{T^{\sfrac{1}{4}}})$ rather than the previous ${\cal O}
\left(\frac{\ln T}{T^{\sfrac{1}{4}}}\right)$ under weak assumptions of the
generalized $p$-affine variance and $(L_0, L_1, q)$-smoothness, without
dependence on the model dimensionality or the numerical stability parameter
$\epsilon$. Additionally, our theoretical analysis provides new insights into
the role of momentum as a key factor ensuring convergence and offers practical
guidelines for tuning learning rates in Adam, further bridging the gap between
theory and practice.

</details>


### [139] [KnowIt: Deep Time Series Modeling and Interpretation](https://arxiv.org/abs/2507.06009)
*M. W. Theunissen,R. Rabe,M. H. Davel*

Main category: cs.LG

TL;DR: 介绍了KnowIt框架，它是用于构建和解释深度时间序列模型的灵活框架，可助力用户在时间序列数据上进行知识发现。


<details>
  <summary>Details</summary>
Motivation: 为用户提供一个可在复杂时间序列数据上进行知识发现的环境，推动该未充分探索领域的发展，打造可靠工具。

Method: 以Python工具包形式实现，通过明确定义的接口解耦数据集、深度神经网络架构和可解释性技术的定义。

Result: 实现了KnowIt框架，代码和文档可在https://must - deep - learning.github.io/KnowIt获取。

Conclusion: 随着持续开发、协作和应用，有望将其打造成推动该领域发展的平台和可靠工具。

Abstract: KnowIt (Knowledge discovery in time series data) is a flexible framework for
building deep time series models and interpreting them. It is implemented as a
Python toolkit, with source code and documentation available from
https://must-deep-learning.github.io/KnowIt. It imposes minimal assumptions
about task specifications and decouples the definition of dataset, deep neural
network architecture, and interpretability technique through well defined
interfaces. This ensures the ease of importing new datasets, custom
architectures, and the definition of different interpretability paradigms while
maintaining on-the-fly modeling and interpretation of different aspects of a
user's own time series data. KnowIt aims to provide an environment where users
can perform knowledge discovery on their own complex time series data through
building powerful deep learning models and explaining their behavior. With
ongoing development, collaboration and application our goal is to make this a
platform to progress this underexplored field and produce a trusted tool for
deep time series modeling.

</details>


### [140] [Kamae: Bridging Spark and Keras for Seamless ML Preprocessing](https://arxiv.org/abs/2507.06021)
*George Barrowclough,Marian Andrecki,James Shinner,Daniele Donghi*

Main category: cs.LG

TL;DR: 提出开源Python库Kamae，将PySpark预处理管道转换为等效Keras模型，用于解决生产推荐系统中特征预处理问题，并在实际用例中展示其效用。


<details>
  <summary>Details</summary>
Motivation: 解决生产推荐系统中特征预处理需在训练和推理环境中重复逻辑，增加工程工作量和数据集偏移风险的问题。

Method: 开发Kamae库，提供可配置的Spark变换器和估计器，映射到对应的Keras层。

Result: 在MovieLens数据集和Expedia的Learning - to - Rank管道等实际用例中展示了框架的效用。

Conclusion: Kamae库可实现机器学习生命周期中一致的端到端预处理。

Abstract: In production recommender systems, feature preprocessing must be faithfully
replicated across training and inference environments. This often requires
duplicating logic between offline and online environments, increasing
engineering effort and introducing risks of dataset shift. We present Kamae, an
open-source Python library that bridges this gap by translating PySpark
preprocessing pipelines into equivalent Keras models. Kamae provides a suite of
configurable Spark transformers and estimators, each mapped to a corresponding
Keras layer, enabling consistent, end-to-end preprocessing across the ML
lifecycle. Framework's utility is illustrated on real-world use cases,
including MovieLens dataset and Expedia's Learning-to-Rank pipelines. The code
is available at https://github.com/ExpediaGroup/kamae.

</details>


### [141] [Multi-view mid fusion: a universal approach for learning in an HDLSS setting](https://arxiv.org/abs/2507.06026)
*Lynn Houthuys*

Main category: cs.LG

TL;DR: 提出用多视图中间融合技术在高维小样本设置下学习的通用方法，实验验证其有效性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 高维小样本设置在诸多应用中带来挑战，需有效学习方法。

Method: 引入多视图中间融合技术，提出三种视图构建方法将高维特征向量拆分为子集。

Result: 广泛实验验证了该方法的有效性和泛化性。

Conclusion: 本文工作为多视图中间融合学习的普遍益处的进一步研究奠定基础。

Abstract: The high-dimensional low-sample-size (HDLSS) setting presents significant
challenges in various applications where the feature dimension far exceeds the
number of available samples. This paper introduces a universal approach for
learning in HDLSS setting using multi-view mid fusion techniques. It shows how
existing mid fusion multi-view methods perform well in an HDLSS setting even if
no inherent views are provided. Three view construction methods are proposed
that split the high-dimensional feature vectors into smaller subsets, each
representing a different view. Extensive experimental validation across
model-types and learning tasks confirm the effectiveness and generalization of
the approach. We believe the work in this paper lays the foundation for further
research into the universal benefits of multi-view mid fusion learning.

</details>


### [142] [EdgeCodec: Onboard Lightweight High Fidelity Neural Compressor with Residual Vector Quantization](https://arxiv.org/abs/2507.06040)
*Benjamin Hodo,Tommaso Polonelli,Amirhossein Moallemi,Luca Benini,Michele Magno*

Main category: cs.LG

TL;DR: 提出用于风力涡轮机叶片气压数据的端到端神经压缩器EdgeCodec，压缩率高、实时运行、可按需调整比特率并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 为风力涡轮机叶片气压数据提供高效的压缩方案，减少无线数据传输能耗，延长传感器单元使用寿命。

Method: 采用高度不对称的自编码器架构，结合判别器训练，并使用残差向量量化器提高压缩效率。

Result: 压缩率在2560:1到10240:1之间，重建误差低于3%，能在GAP9微控制器上实时运行，比特率可按需调整，最高压缩模式下无线数据传输能耗降低2.9倍。

Conclusion: EdgeCodec是一种高效的气压数据压缩方案，能有效降低能耗，延长传感器单元使用寿命。

Abstract: We present EdgeCodec, an end-to-end neural compressor for barometric data
collected from wind turbine blades. EdgeCodec leverages a heavily asymmetric
autoencoder architecture, trained with a discriminator and enhanced by a
Residual Vector Quantizer to maximize compression efficiency. It achieves
compression rates between 2'560:1 and 10'240:1 while maintaining a
reconstruction error below 3%, and operates in real time on the GAP9
microcontroller with bitrates ranging from 11.25 to 45 bits per second.
Bitrates can be selected on a sample-by-sample basis, enabling on-the-fly
adaptation to varying network conditions. In its highest compression mode,
EdgeCodec reduces the energy consumption of wireless data transmission by up to
2.9x, significantly extending the operational lifetime of deployed sensor
units.

</details>


### [143] [QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models](https://arxiv.org/abs/2507.06079)
*Sebastian Siegel,Ming-Jay Yang,Younes Bouhadjar,Maxime Fabre,Emre Neftci,John Paul Strachan*

Main category: cs.LG

TL;DR: 本文探讨量化感知训练（QAT）对结构化状态空间模型（SSM）的影响，表明QAT可降低模型复杂度，增强对模拟噪声的鲁棒性，支持结构剪枝，并实现SSM在忆阻器模拟内存计算基板上的部署，提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有工作未探讨QAT对专用边缘硬件（如模拟内存计算芯片）的影响，希望研究QAT在这方面的作用。

Method: 分析模型大小与数值精度的关系，结合相关技术将SSM部署在忆阻器模拟内存计算基板上。

Result: QAT可将SSM的复杂度降低达两个数量级，增强对模拟噪声的鲁棒性，支持结构剪枝。

Conclusion: QAT能显著降低SSM复杂度，结合相关技术可在忆阻器模拟内存计算基板上部署SSM，提升计算效率。

Abstract: Structured State Space models (SSM) have recently emerged as a new class of
deep learning models, particularly well-suited for processing long sequences.
Their constant memory footprint, in contrast to the linearly scaling memory
demands of Transformers, makes them attractive candidates for deployment on
resource-constrained edge-computing devices. While recent works have explored
the effect of quantization-aware training (QAT) on SSMs, they typically do not
address its implications for specialized edge hardware, for example, analog
in-memory computing (AIMC) chips. In this work, we demonstrate that QAT can
significantly reduce the complexity of SSMs by up to two orders of magnitude
across various performance metrics. We analyze the relation between model size
and numerical precision, and show that QAT enhances robustness to analog noise
and enables structural pruning. Finally, we integrate these techniques to
deploy SSMs on a memristive analog in-memory computing substrate and highlight
the resulting benefits in terms of computational efficiency.

</details>


### [144] [CoRE: Enhancing Metacognition with Label-free Self-evaluation in LRMs](https://arxiv.org/abs/2507.06087)
*Haoxi Li,Sikai Bai,Jie Zhang,Song Guo*

Main category: cs.LG

TL;DR: 本文提出CoRE和CoRE - Eval解决大推理模型过度思考问题，实验显示可减少推理步骤并提高准确率。


<details>
  <summary>Details</summary>
Motivation: 大推理模型存在过度思考现象，需解决模型在无外部标签下自主评估推理轨迹正确性的问题。

Method: 提出Chain - of - Reasoning Embedding (CoRE)用于无标签自我评估，引入训练和标签均自由的CoRE - Eval框架检测冗余推理模式并决定是否提前终止推理。

Result: 在多个数学推理基准测试上，CoRE - Eval减少推理链长度13.7% - 33.2%，提高答案准确率约10%，32B模型在AIME基准测试达70.0%准确率。

Conclusion: CoRE和CoRE - Eval能增强大推理模型元认知能力，提高推理效率和准确率。

Abstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in
domains like mathematics and program synthesis. Despite their strong
performance, LRMs often exhibit overthinking -- excessive and redundant
reasoning steps that introduce inefficiencies during inference. This phenomenon
raises an important question for LRM self-evaluation: How can a model
autonomously assess the correctness of its own reasoning trajectory without
external labels? To address this, we propose Chain-of-Reasoning Embedding
(CoRE), a series of hidden states in latent space to enable label-free
self-evaluation on intermediate reasoning steps of LRMs, so as to enhance
metacognition abilities for improved reasoning efficiency. By analyzing the
geometric properties of the CoRE trajectories, we reveal that redundant
reasoning usually presents cyclical fluctuations, which correspond to
repetitive and unconscious reflection/exploration. Leveraging this insight, we
further introduce a training-free, label-free self-evaluation framework,
CoRE-Eval, to detect such patterns and dynamically determine whether to
terminate reasoning early. Extensive experiments on mathematical reasoning
benchmarks (GSM8K, MATH-500, and AIME) and across model sizes from 7B to 32B
demonstrate that CoRE-Eval reduces chain-of-thought length by 13.7% to 33.2%
while improving answer accuracy by around 10%, achieving 70.0% accuracy on the
challenging AIME benchmark with the 32B model.

</details>


### [145] [Safe Domain Randomization via Uncertainty-Aware Out-of-Distribution Detection and Policy Adaptation](https://arxiv.org/abs/2507.06111)
*Mohamad H. Danesh,Maxime Wabartha,Stanley Wu,Joelle Pineau,Hsiu-Chin Lin*

Main category: cs.LG

TL;DR: 提出不确定性感知强化学习（UARL）框架，无需在目标域直接交互，提升策略鲁棒性和泛化能力，经实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习策略部署到现实世界存在分布偏移、安全问题和策略优化时直接交互不实际等挑战，现有方法不安全。

Method: 采用评论家集成量化策略不确定性，结合渐进式环境随机化，在模拟环境的高不确定性区域迭代优化策略。

Result: 在MuJoCo基准测试和四足机器人上评估，显示出可靠的OOD检测能力、更好的性能和更高的样本效率。

Conclusion: UARL框架能在不直接在目标域训练的情况下，有效提升对目标域的鲁棒泛化能力。

Abstract: Deploying reinforcement learning (RL) policies in real-world involves
significant challenges, including distribution shifts, safety concerns, and the
impracticality of direct interactions during policy refinement. Existing
methods, such as domain randomization (DR) and off-dynamics RL, enhance policy
robustness by direct interaction with the target domain, an inherently unsafe
practice. We propose Uncertainty-Aware RL (UARL), a novel framework that
prioritizes safety during training by addressing Out-Of-Distribution (OOD)
detection and policy adaptation without requiring direct interactions in target
domain. UARL employs an ensemble of critics to quantify policy uncertainty and
incorporates progressive environmental randomization to prepare the policy for
diverse real-world conditions. By iteratively refining over high-uncertainty
regions of the state space in simulated environments, UARL enhances robust
generalization to the target domain without explicitly training on it. We
evaluate UARL on MuJoCo benchmarks and a quadrupedal robot, demonstrating its
effectiveness in reliable OOD detection, improved performance, and enhanced
sample efficiency compared to baselines.

</details>


### [146] [Subspace-based Approximate Hessian Method for Zeroth-Order Optimization](https://arxiv.org/abs/2507.06125)
*Dongyoon Kim,Sungjae Lee,Wonjin Lee,Kwang In Kim*

Main category: cs.LG

TL;DR: 提出基于子空间的近似Hessian (ZO - SAH) 零阶优化算法，在基准数据集实验中比现有零阶方法收敛更快。


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化方法大多依赖一阶近似，结合二阶信息可加速收敛，但估计Hessian矩阵的高成本限制了其实际应用。

Method: ZO - SAH方法聚焦随机选择的二维子空间，在每个子空间内通过拟合二次多项式到目标函数并提取二阶系数来估计Hessian，还采用周期性子空间切换策略以减少函数查询成本。

Result: 在包括逻辑回归和深度神经网络训练任务的八个基准数据集实验中，ZO - SAH比现有零阶方法收敛显著更快。

Conclusion: ZO - SAH方法能有效降低估计Hessian矩阵的成本，在零阶优化中表现更优。

Abstract: Zeroth-order optimization addresses problems where gradient information is
inaccessible or impractical to compute. While most existing methods rely on
first-order approximations, incorporating second-order (curvature) information
can, in principle, significantly accelerate convergence. However, the high cost
of function evaluations required to estimate Hessian matrices often limits
practical applicability. We present the subspace-based approximate Hessian
(ZO-SAH) method, a zeroth-order optimization algorithm that mitigates these
costs by focusing on randomly selected two-dimensional subspaces. Within each
subspace, ZO-SAH estimates the Hessian by fitting a quadratic polynomial to the
objective function and extracting its second-order coefficients. To further
reduce function-query costs, ZO-SAH employs a periodic subspace-switching
strategy that reuses function evaluations across optimization steps.
Experiments on eight benchmark datasets, including logistic regression and deep
neural network training tasks, demonstrate that ZO-SAH achieves significantly
faster convergence than existing zeroth-order methods.

</details>


### [147] [Aliasing in Convnets: A Frame-Theoretic Perspective](https://arxiv.org/abs/2507.06152)
*Daniel Haider,Vincent Lostanlen,Martin Ehler,Nicki Holighaus,Peter Balazs*

Main category: cs.LG

TL;DR: 本文用框架理论方法分析卷积层混叠，给出稳定性界估计和帕塞瓦尔稳定性表征，推导促进稳定性的优化目标及随机核层混叠项的期望和方差闭式表达式。


<details>
  <summary>Details</summary>
Motivation: 卷积层步长会引入混叠影响稳定性和泛化性，此前缺乏混叠及其对稳定性影响的通用分析。

Method: 采用框架理论方法描述一维核卷积层的混叠。

Result: 得到稳定性界的实际估计和帕塞瓦尔稳定性表征，推导出两个高效优化目标，得出随机核层混叠项的期望和方差闭式表达式。

Conclusion: 系统抑制混叠可促进帕塞瓦尔稳定性，对初始化时的混叠行为有了深入理解。

Abstract: Using a stride in a convolutional layer inherently introduces aliasing, which
has implications for numerical stability and statistical generalization. While
techniques such as the parametrizations via paraunitary systems have been used
to promote orthogonal convolution and thus ensure Parseval stability, a general
analysis of aliasing and its effects on the stability has not been done in this
context. In this article, we adapt a frame-theoretic approach to describe
aliasing in convolutional layers with 1D kernels, leading to practical
estimates for stability bounds and characterizations of Parseval stability,
that are tailored to take short kernel sizes into account. From this, we derive
two computationally very efficient optimization objectives that promote
Parseval stability via systematically suppressing aliasing. Finally, for layers
with random kernels, we derive closed-form expressions for the expected value
and variance of the terms that describe the aliasing effects, revealing
fundamental insights into the aliasing behavior at initialization.

</details>


### [148] [A Method for Optimizing Connections in Differentiable Logic Gate Networks](https://arxiv.org/abs/2507.06173)
*Wout Mommen,Lars Keuninckx,Matthias Hartmann,Piet Wambacq*

Main category: cs.LG

TL;DR: 提出深度可微逻辑门网络（LGNs）连接部分优化新方法，在多个基准测试中表现优异，展示通向全可训练布尔逻辑的途径。


<details>
  <summary>Details</summary>
Motivation: 对深度可微逻辑门网络（LGNs）的连接进行部分优化，探索更高效的网络结构。

Method: 训练方法利用每个门输入的连接子集上的概率分布，选择最有价值的连接，然后选择门类型。

Result: 连接优化的LGNs在Yin - Yang、MNIST和Fashion - MNIST基准测试中优于标准固定连接LGNs，只需少量逻辑门；训练所有连接时，8000个简单逻辑门在MNIST数据集上准确率超98%；相比标准全连接LGNs，网络门数少24倍且性能更好。

Conclusion: 工作展示了通向全可训练布尔逻辑的途径。

Abstract: We introduce a novel method for partial optimization of the connections in
Deep Differentiable Logic Gate Networks (LGNs). Our training method utilizes a
probability distribution over a subset of connections per gate input, selecting
the connection with highest merit, after which the gate-types are selected. We
show that the connection-optimized LGNs outperform standard fixed-connection
LGNs on the Yin-Yang, MNIST and Fashion-MNIST benchmarks, while requiring only
a fraction of the number of logic gates. When training all connections, we
demonstrate that 8000 simple logic gates are sufficient to achieve over 98% on
the MNIST data set. Additionally, we show that our network has 24 times fewer
gates, while performing better on the MNIST data set compared to standard fully
connected LGNs. As such, our work shows a pathway towards fully trainable
Boolean logic.

</details>


### [149] [Differential Mamba](https://arxiv.org/abs/2507.06204)
*Nadav Schneider,Itamar Zimerman,Eliya Nachmani*

Main category: cs.LG

TL;DR: 本文探讨将针对Transformer的差分设计技术应用于Mamba架构，提出新差分机制，提升其检索能力和性能。


<details>
  <summary>Details</summary>
Motivation: 序列模型存在注意力过度分配问题，影响LLM能力，虽有针对Transformer的解决方法，但不确定能否用于Mamba架构。

Method: 提出适用于Mamba的新型差分机制，并进行架构修改。

Result: 在语言建模基准测试中验证，新机制提升了Mamba的检索能力，性能优于原始Mamba。

Conclusion: 所提方法有效缓解了基于Mamba模型的注意力过度分配问题。

Abstract: Sequence models like Transformers and RNNs often overallocate attention to
irrelevant context, leading to noisy intermediate representations. This
degrades LLM capabilities by promoting hallucinations, weakening long-range and
retrieval abilities, and reducing robustness. Recent work has shown that
differential design can mitigate this issue in Transformers, improving their
effectiveness across various applications. In this paper, we explore whether
these techniques, originally developed for Transformers, can be applied to
Mamba, a recent architecture based on selective state-space layers that
achieves Transformer-level performance with greater efficiency. We show that a
naive adaptation of differential design to Mamba is insufficient and requires
careful architectural modifications. To address this, we introduce a novel
differential mechanism for Mamba, empirically validated on language modeling
benchmarks, demonstrating improved retrieval capabilities and superior
performance over vanilla Mamba. Finally, we conduct extensive ablation studies
and empirical analyses to justify our design choices and provide evidence that
our approach effectively mitigates the overallocation problem in Mamba-based
models. Our code is publicly available.

</details>


### [150] [Modern Methods in Associative Memory](https://arxiv.org/abs/2507.06211)
*Dmitry Krotov,Benjamin Hoover,Parikshit Ram,Bao Pham*

Main category: cs.LG

TL;DR: 本文介绍关联记忆，阐述其因新理论结果受关注，可用于解释传统AI网络计算，还能设计新架构，并提供入门教程。


<details>
  <summary>Details</summary>
Motivation: 关联记忆因新理论结果及与SOTA AI架构的联系受到关注，需要可理解的入门介绍。

Method: 以现代语言和方法，结合数学推导和代码笔记本进行介绍。

Result: 提供了关联记忆的入门教程。

Conclusion: 通过该教程可对关联记忆有初步了解并掌握相关方法。

Abstract: Associative Memories like the famous Hopfield Networks are elegant models for
describing fully recurrent neural networks whose fundamental job is to store
and retrieve information. In the past few years they experienced a surge of
interest due to novel theoretical results pertaining to their information
storage capabilities, and their relationship with SOTA AI architectures, such
as Transformers and Diffusion Models. These connections open up possibilities
for interpreting the computation of traditional AI networks through the
theoretical lens of Associative Memories. Additionally, novel Lagrangian
formulations of these networks make it possible to design powerful distributed
models that learn useful representations and inform the design of novel
architectures. This tutorial provides an approachable introduction to
Associative Memories, emphasizing the modern language and methods used in this
area of research, with practical hands-on mathematical derivations and coding
notebooks.

</details>


### [151] [Deep Learning Optimization of Two-State Pinching Antennas Systems](https://arxiv.org/abs/2507.06222)
*Odysseas G. Karagiannidis,Victoria E. Galanopoulou,Panagiotis D. Diamantoulakis,Zhiguo Ding,Octavia Dobre*

Main category: cs.LG

TL;DR: 研究波导中固定位置捏合天线（PAs）最优子集激活问题以最大化通信速率，用神经网络求解，考虑用户位置不确定性，仿真验证模型有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 无线通信系统发展需要灵活、节能、经济的天线技术，PAs是有潜力的候选，要解决波导中PAs最优子集激活以最大化通信速率的问题。

Method: 将问题建模为组合分数0 - 1二次规划，用不同复杂度的神经网络架构从数据中直接学习激活策略，在训练和评估中考虑用户位置不确定性。

Result: 仿真结果表明所提出模型有效且鲁棒。

Conclusion: 所提出的利用神经网络解决波导中PAs最优子集激活问题的方法可行，且考虑用户位置不确定性能使模型适应现实部署条件。

Abstract: The evolution of wireless communication systems requires flexible,
energy-efficient, and cost-effective antenna technologies. Pinching antennas
(PAs), which can dynamically control electromagnetic wave propagation through
binary activation states, have recently emerged as a promising candidate. In
this work, we investigate the problem of optimally selecting a subset of
fixed-position PAs to activate in a waveguide, when the aim is to maximize the
communication rate at a user terminal. Due to the complex interplay between
antenna activation, waveguide-induced phase shifts, and power division, this
problem is formulated as a combinatorial fractional 0-1 quadratic program. To
efficiently solve this challenging problem, we use neural network architectures
of varying complexity to learn activation policies directly from data,
leveraging spatial features and signal structure. Furthermore, we incorporate
user location uncertainty into our training and evaluation pipeline to simulate
realistic deployment conditions. Simulation results demonstrate the
effectiveness and robustness of the proposed models.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [152] [Evolutionary and Coevolutionary Multi-Agent Design Choices and Dynamics](https://arxiv.org/abs/2507.05534)
*Erik Hemberg,Eric Liu,Lucille Fuller,Stephen Moskal,Una-May O'Reilly*

Main category: cs.NE

TL;DR: 研究网络智能体团队控制器的两种表示方式，结合不同进化算法评估学习情况，发现特定语法进化算法团队表现最佳，对比了协同进化和单边进化的性能影响。


<details>
  <summary>Details</summary>
Motivation: 探究网络智能体团队控制器表示方式与进化算法结合的优劣，量化不同组合在团队表现上的权衡。

Method: 将两种控制器表示与不同进化算法结合，使用网络安全场景评估单边训练和双边协同进化下的智能体学习情况。

Result: 基于特定语法的语法进化算法能实现最佳团队表现；协同进化减少双方性能高低差但引发波动，单边优化性能峰值更高且更持久。

Conclusion: 明确了不同控制器表示和进化算法组合在团队性能上的表现差异，以及协同进化和单边进化的不同影响。

Abstract: We investigate two representation alternatives for the controllers of teams
of cyber agents. We combine these controller representations with different
evolutionary algorithms, one of which introduces a novel LLM-supported mutation
operator. Using a cyber security scenario, we evaluate agent learning when one
side is trained to compete against a side that does not evolve and when two
sides coevolve with each other. This allows us to quantify the relative merits
and tradeoffs of representation and algorithm combinations in terms of team
performance. Our versions of grammatical evolution algorithms using grammars
that allow a controller to be expressed in code-like logic can achieve the best
team performance. The scenario also allows us to compare the performance impact
and dynamics of coevolution versus evolution under different combinations.
Across the algorithms and representations, we observe that coevolution reduces
the performance highs and lows of both sides while it induces fluctuations on
both sides. In contrast, when only one-side is optimized, performance peaks are
higher and is more sustained than when both sides are optimized with
coevolution.

</details>


### [153] [A Universal Framework for Large-Scale Multi-Objective Optimization Based on Particle Drift and Diffusion](https://arxiv.org/abs/2507.05847)
*Jia-Cheng Li,Min-Rong Chen,Guo-Qiang Zeng,Jian Weng,Man Wang,Jia-Lin Mai*

Main category: cs.NE

TL;DR: 提出基于粒子漂移和扩散的大规模多目标优化通用框架，分三阶段优化，嵌入代表算法实验，证明可提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 大规模多目标优化因高维决策变量对现有进化算法的收敛性和多样性性能维护构成挑战。

Method: 提出基于粒子漂移和扩散的框架，将优化过程分为两个粗调子阶段和一个微调子阶段，根据阶段对引导解执行不同漂移 - 扩散操作策略，并嵌入代表进化算法。

Result: 通过对1000到5000个决策变量的大规模多目标问题对比实验及神经网络训练问题实验，验证框架有效性。

Conclusion: 该框架显著提升了多目标进化算法的收敛性和多样性性能，提高了解决大规模多目标优化问题的计算效率。

Abstract: Large-scale multi-objective optimization poses challenges to existing
evolutionary algorithms in maintaining the performances of convergence and
diversity because of high dimensional decision variables. Inspired by the
motion of particles in physics, we propose a universal framework for
large-scale multi-objective optimization based on particle drift and diffusion
to solve these challenges in this paper. This framework innovatively divides
the optimization process into three sub-stages: two coarse-tuning sub-stages
and one fine-tuning sub-stage. Different strategies of drift-diffusion
operations are performed on the guiding solutions according to the current
sub-stage, ingeniously simulating the movement of particles under diverse
environmental conditions. Finally, representative evolutionary algorithms are
embedded into the proposed framework, and their effectiveness are evaluated
through comparative experiments on various large-scale multi-objective problems
with 1000 to 5000 decision variables. Moreover, comparative algorithms are
conducted on neural network training problems to validate the effectiveness of
the proposed framework in the practical problems. The experimental results
demonstrate that the framework proposed in this paper significantly enhances
the performance of convergence and diversity of MOEAs, and improves the
computational efficiency of algorithms in solving large-scale multi-objective
optimization problems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [154] [CORE: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks](https://arxiv.org/abs/2507.05269)
*Danning Xie,Mingwei Zheng,Xuwei Liu,Jiannan Wang,Chengpeng Wang,Lin Tan,Xiangyu Zhang*

Main category: cs.SE

TL;DR: 本文提出CoRe基准评估大语言模型程序语义推理能力，评估10个主流模型发现其在深度语义理解和多步推理任务存在困难，并进行定性分析给出改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估端到端结果，大语言模型程序语义推理能力未充分探索，需新基准评估。

Method: 提出CoRe基准，含12553个任务实例，采用语义感知多样采样策略；评估10个主流大语言模型并进行定性分析。

Result: 模型在识别依赖关系表现较好，但在深度语义理解和多步推理任务有困难。

Conclusion: 揭示复杂控制结构和反向依赖模式等挑战，为提升大语言模型代码推理能力提供见解。

Abstract: Large language models (LLMs) have been widely adopted across diverse software
engineering domains, such as code generation, program repair, and vulnerability
detection. These applications require understanding beyond surface-level code
patterns: value propagation, control flow, and interdependence between program
elements. However, existing benchmarks primarily evaluate end-to-end outcomes,
such as whether code is correctly repaired or generated, leaving the models
ability for program semantic reasoning underexplored. This work presents CoRe,
a high-quality, human-verified benchmark designed to evaluate LLMs on
fundamental static analysis tasks. CoRe includes 12,553 task instances spanning
data dependency, control dependency, and information flow across programs
written in C/C++, Java, and Python. To ensure semantic diversity and reasoning
complexity, we propose a semantics-aware diverse sampling strategy that selects
targets and task instances based on structural coverage and dependency depth.
We evaluate 10 mainstream LLMs and show that, while they perform well at
identifying dependencies, models still struggle with tasks that require deeper
semantic understanding and multi-step reasoning. We further conduct qualitative
analyses to uncover key challenges, such as complex control structures and
backward dependency patterns, offering insights into improving LLMs code
reasoning capabilities.

</details>


### [155] [Open Source, Hidden Costs: A Systematic Literature Review on OSS License Management](https://arxiv.org/abs/2507.05270)
*Boyuan Li,Chengwei Liu,Lingling Fan,Sen Chen,Zhenlin Zhang,Zheli Liu*

Main category: cs.SE

TL;DR: 文章进行开源软件许可证相关的系统文献综述，分类现有研究，探讨挑战、机遇和给出建议，促进软件工程社区合法软件风险治理。


<details>
  <summary>Details</summary>
Motivation: 第三方软件集成有许可风险，现有研究有局限，开源软件许可证和生成式软件工程技术发展带来新要求，需探索未来方向。

Method: 对80篇精心挑选的OSS许可证相关论文进行系统文献综述，将现有研究分为许可证识别、许可证风险评估和许可证风险缓解三类。

Result: 讨论了现有解决方案的挑战，总结了机遇。

Conclusion: 研究有助于弥合学术与行业差距，加速软件工程社区合法软件风险的生态系统治理。

Abstract: Integrating third-party software components is a common practice in modern
software development, offering significant advantages in terms of efficiency
and innovation. However, this practice is fraught with risks related to
software licensing. A lack of understanding may lead to disputes, which can
pose serious legal and operational challenges. To these ends, both academia and
industry have conducted various investigations and proposed solutions and tools
to deal with these challenges. However, significant limitations still remain.
Moreover, the rapid evolution of open-source software (OSS) licenses, as well
as the rapidly incorporated generative software engineering techniques, such as
large language models for code (CodeLLMs), are placing greater demands on the
systematic management of software license risks. To unveil the severe
challenges and explore possible future directions, we conduct the first
systematic literature review (SLR) on 80 carefully selected OSS license-related
papers, classifying existing research into three key categories, i.e., license
identification, license risk assessment, and license risk mitigation. Based on
these, we discuss challenges in existing solutions, conclude the opportunities
to shed light on future research directions and offer practical recommendations
for practitioners. We hope this thorough review will help bridge the gaps
between academia and industry and accelerate the ecosystem-wide governance of
legitimate software risks within the software engineering community.

</details>


### [156] [ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy](https://arxiv.org/abs/2507.05279)
*Virgile Boraud,Yannis Bendi-Ouis,Paul Bernard,Xavier Hinaut*

Main category: cs.SE

TL;DR: 本文介绍用ReservoirPy库提升大语言模型辅助代码开发及回答储层计算领域复杂问题能力的工具，结合RAG和知识图谱减少幻觉，评估显示该模型在编码任务上表现佳。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型在辅助代码开发和回答储层计算领域复杂问题的能力，减少生成响应的幻觉，提高事实准确性。

Method: 通过检索增强生成（RAG）和知识图谱引入外部知识，构建类似ChatGPT的交互式系统。

Result: 专有模型在常识问题上表现稍好，该模型在编码任务上超越它们，且相比基础模型Codestral - 22B有显著提升。

Conclusion: 该工具能有效提升大语言模型在代码开发和储层计算领域复杂问题回答上的能力。

Abstract: We introduce a tool designed to improve the capabilities of Large Language
Models (LLMs) in assisting with code development using the ReservoirPy library,
as well as in answering complex questions in the field of Reservoir Computing.
By incorporating external knowledge through Retrieval-Augmented Generation
(RAG) and knowledge graphs, our approach aims to reduce hallucinations and
increase the factual accuracy of generated responses. The system provides an
interactive experience similar to ChatGPT, tailored specifically for
ReservoirPy, enabling users to write, debug, and understand Python code while
accessing reliable domain-specific insights. In our evaluation, while
proprietary models such as ChatGPT-4o and NotebookLM performed slightly better
on general knowledge questions, our model outperformed them on coding tasks and
showed a significant improvement over its base model, Codestral-22B.

</details>


### [157] [FuzzFeed: An Automatic Approach to Weakest Precondition Generation using LLMs and Fuzzing](https://arxiv.org/abs/2507.05272)
*Daragh King,Vasileios Koutavas,Laura Kovacs*

Main category: cs.SE

TL;DR: 本文提出结合大语言模型（LLMs）和模糊测试生成最弱前置条件（WP），引入模糊指导（FG），并在Java确定性数组程序基准集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 最弱前置条件生成在程序验证等领域有重要应用，需探索有效生成方法。

Method: 提出结合LLMs和模糊测试生成WP，引入FG利用程序执行反馈引导LLMs生成正确WP，用模糊测试检查候选WP的有效性和弱点并反馈给LLM。

Result: 在Java确定性数组程序基准集实验表明，LLMs能生成可行候选WP，FG可增强其能力。

Conclusion: 结合LLMs和模糊测试生成WP的方法有效，FG能提升LLMs生成WP的能力。

Abstract: The weakest precondition (WP) of a program describes the largest set of
initial states from which all terminating executions of the program satisfy a
given postcondition. The generation of WPs is an important task with practical
applications in areas ranging from verification to run-time error checking.
  This paper proposes the combination of Large Language Models (LLMs) and fuzz
testing for generating WPs. In pursuit of this goal, we introduce Fuzzing
Guidance (FG); FG acts as a means of directing LLMs towards correct WPs using
program execution feedback. FG utilises fuzz testing for approximately checking
the validity and weakness of candidate WPs, this information is then fed back
to the LLM as a means of context refinement.
  We demonstrate the effectiveness of our approach on a comprehensive benchmark
set of deterministic array programs in Java. Our experiments indicate that LLMs
are capable of producing viable candidate WPs, and that this ability can be
practically enhanced through FG.

</details>


### [158] [Search-based Selection of Metamorphic Relations for Optimized Robustness Testing of Large Language Models](https://arxiv.org/abs/2507.05565)
*Sangwon Hyun,Shaukat Ali,M. Ali Babar*

Main category: cs.SE

TL;DR: 本文提出基于搜索的方法优化MR组以评估大语言模型鲁棒性，开发搜索过程和四种算法，实验表明MOEA/D算法最佳且发现关键MR。


<details>
  <summary>Details</summary>
Motivation: 现有基于MR的大语言模型鲁棒性测试需大量MR，需优化选择，且多数研究仅关注自动生成测试用例和单扰动MR测试空间有限。

Method: 提出基于搜索的方法优化MR组，开发搜索过程并实现Single - GA、NSGA - II、SPEA2和MOEA/D四种算法解决MR选择问题。

Result: MOEA/D算法在优化MR空间用于大语言模型鲁棒性测试中表现最佳；发现能跨不同文本生成任务混淆大语言模型的关键MR。

Conclusion: 研究为大语言模型鲁棒性评估的优化测试提供基本问题的见解和基于搜索的解决方案。

Abstract: Assessing the trustworthiness of Large Language Models (LLMs), such as
robustness, has garnered significant attention. Recently, metamorphic testing
that defines Metamorphic Relations (MRs) has been widely applied to evaluate
the robustness of LLM executions. However, the MR-based robustness testing
still requires a scalable number of MRs, thereby necessitating the optimization
of selecting MRs. Most extant LLM testing studies are limited to automatically
generating test cases (i.e., MRs) to enhance failure detection. Additionally,
most studies only considered a limited test space of single perturbation MRs in
their evaluation of LLMs. In contrast, our paper proposes a search-based
approach for optimizing the MR groups to maximize failure detection and
minimize the LLM execution cost. Moreover, our approach covers the
combinatorial perturbations in MRs, facilitating the expansion of test space in
the robustness assessment. We have developed a search process and implemented
four search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel
encoding to solve the MR selection problem in the LLM robustness testing. We
conducted comparative experiments on the four search algorithms along with a
random search, using two major LLMs with primary Text-to-Text tasks. Our
statistical and empirical investigation revealed two key findings: (1) the
MOEA/D algorithm performed the best in optimizing the MR space for LLM
robustness testing, and (2) we identified silver bullet MRs for the LLM
robustness testing, which demonstrated dominant capabilities in confusing LLMs
across different Text-to-Text tasks. In LLM robustness assessment, our research
sheds light on the fundamental problem for optimized testing and provides
insights into search-based solutions.

</details>


### [159] [CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark](https://arxiv.org/abs/2507.05281)
*Lingyue Fu,Hao Guan,Bolun Zhang,Haowei Yuan,Yaoming Zhu,Jun Xu,Zongyu Wang,Lin Qiu,Xunliang Cai,Xuezhi Cao,Weiwen Liu,Weinan Zhang,Yong Yu*

Main category: cs.SE

TL;DR: 现有仓库级基准测试评估大语言模型代码处理能力有局限，提出CorePipe和CoreCodeBench，实验展示模型能力。


<details>
  <summary>Details</summary>
Motivation: 现有仓库级基准测试主要关注单一场景，问题定位可控性有限，生成测试用例可靠性不足，需更好评估大语言模型在实际工程代码上的性能。

Method: 提出CorePipe将仓库转换为综合测试用例，引入CoreCodeBench可配置多场景仓库级基准，生成原子和复合问题，通过超参数调整难度。

Result: 对16个大语言模型在不同场景实验，揭示其不同能力，提供多维度洞察。

Conclusion: CoreCodeBench提供全面广泛的仓库级基准，可用于研究大语言模型在实际工程项目中的适用性。

Abstract: As Large Language Models (LLMs) demonstrate increasingly sophisticated code
processing capabilities, evaluating their performance on engineering-level code
remains challenging. Existing repository-level benchmarks primarily focus on
single scenarios, such as code generation or bug fixing, without adequately
capturing the diversity and complexity of real-world software or project
engineering workflows. Furthermore, these benchmarks suffer from limited
controllability in question positioning and reliability issues in their
generated test cases. To address these limitations, we present CorePipe, a
fully automated pipeline that converts repositories into comprehensive test
cases, and introduce CoreCodeBench, a configurable multi-scenario
repository-level benchmark. To simulate real engineering scenarios, CorePipe
generates three types of atomic questions (Development, BugFix, and Test-Driven
Development) specifically targeting core code segments. These atomic questions
are further combined into three types of composite questions, with difficulty
levels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides
a comprehensive and extensive repository-level benchmark to investigate the
applicability of LLMs in real-world engineering projects. Experiments with 16
LLMs across diverse scenarios reveal varying capabilities and offer
multi-dimensional insights into LLM performance in engineering contexts. The
code for CorePipe is available at
https://github.com/AGI-Eval-Official/CoreCodeBench, and the data for
CoreCodeBench can be accessed at
https://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.

</details>


### [160] [Measuring how changes in code readability attributes affect code quality evaluation by Large Language Models](https://arxiv.org/abs/2507.05289)
*Igor Regis da Silva Simoes,Elaine Venson*

Main category: cs.SE

TL;DR: 本文探索用大语言模型（LLMs）评估代码可读性相关质量属性，通过准实验研究测试9个LLMs，结果显示LLMs对代码变更敏感，有评估语义质量的潜力。


<details>
  <summary>Details</summary>
Motivation: 代码可读性测量在行业和学术界存在挑战，现有方法有局限性，需用LLMs以标准化、可重现和一致的方式评估代码可读性。

Method: 进行准实验研究，对9个LLMs进行三种干预（移除注释、替换标识符名称、重构移除代码异味），每次干预每个LLM进行10次批量分析，收集响应变异性数据，并与参考模型和工具对比。

Result: 所有LLMs对干预敏感，与参考分类器在原始和重构代码场景下一致性高；LLMs有强语义敏感性；模型存在响应变异性，但不总影响结果的统计显著性。

Conclusion: LLMs在评估代码语义质量方面有潜力，如标识符名称、注释、文档与代码目的的一致性。

Abstract: Code readability is one of the main aspects of code quality, influenced by
various properties like identifier names, comments, code structure, and
adherence to standards. However, measuring this attribute poses challenges in
both industry and academia. While static analysis tools assess attributes such
as code smells and comment percentage, code reviews introduce an element of
subjectivity. This paper explores using Large Language Models (LLMs) to
evaluate code quality attributes related to its readability in a standardized,
reproducible, and consistent manner. We conducted a quasi-experiment study to
measure the effects of code changes on Large Language Model (LLM)s
interpretation regarding its readability quality attribute. Nine LLMs were
tested, undergoing three interventions: removing comments, replacing identifier
names with obscure names, and refactoring to remove code smells. Each
intervention involved 10 batch analyses per LLM, collecting data on response
variability. We compared the results with a known reference model and tool. The
results showed that all LLMs were sensitive to the interventions, with
agreement with the reference classifier being high for the original and
refactored code scenarios. The LLMs demonstrated a strong semantic sensitivity
that the reference model did not fully capture. A thematic analysis of the LLMs
reasoning confirmed their evaluations directly reflected the nature of each
intervention. The models also exhibited response variability, with 9.37% to
14.58% of executions showing a standard deviation greater than zero, indicating
response oscillation, though this did not always compromise the statistical
significance of the results. LLMs demonstrated potential for evaluating
semantic quality aspects, such as coherence between identifier names, comments,
and documentation with code purpose.

</details>


### [161] [zkSDK: Streamlining zero-knowledge proof development through automated trace-driven ZK-backend selection](https://arxiv.org/abs/2507.05294)
*William Law*

Main category: cs.SE

TL;DR: 论文介绍zkSDK模块化框架，通过Presto语言和动态选择算法简化零知识应用开发，能自动选最优后端，提供友好开发体验。


<details>
  <summary>Details</summary>
Motivation: 零知识（ZK）开发工具众多，开发者面临不同后端选择，学习曲线陡、开发体验碎片化，很多开发者只能绑定单一后端。

Method: 引入zkSDK框架，以Presto语言分析程序计算负载强度，结合用户标准用动态选择算法自动选最优ZK证明后端。

Result: 通过对实际工作负载深入分析评估，zkSDK能从支持的ZK后端中有效选出最适合后端。

Conclusion: zkSDK可简化ZK应用开发，提供无缝且用户友好的开发体验。

Abstract: The rapid advancement of creating Zero-Knowledge (ZK) programs has led to the
development of numerous tools designed to support developers. Popular options
include being able to write in general-purpose programming languages like Rust
from Risc Zero. Other languages exist like Circom, Lib-snark, and Cairo.
However, developers entering the ZK space are faced with many different ZK
backends to choose from, leading to a steep learning curve and a fragmented
developer experience across different platforms. As a result, many developers
tend to select a single ZK backend and remain tied to it. This thesis
introduces zkSDK, a modular framework that streamlines ZK application
development by abstracting the backend complexities. At the core of zkSDK is
Presto, a custom Python-like programming language that enables the profiling
and analysis of a program to assess its computational workload intensity.
Combined with user-defined criteria, zkSDK employs a dynamic selection
algorithm to automatically choose the optimal ZK-proving backend. Through an
in-depth analysis and evaluation of real-world workloads, we demonstrate that
zkSDK effectively selects the best-suited backend from a set of supported ZK
backends, delivering a seamless and user-friendly development experience.

</details>


### [162] [ASSURE: Metamorphic Testing for AI-powered Browser Extensions](https://arxiv.org/abs/2507.05307)
*Xuanqi Gao,Juan Zhai,Shiqing Ma,Siyi Xie,Chao Shen*

Main category: cs.SE

TL;DR: 现有方法难测大语言模型浏览器扩展，提出 ASSURE 框架，经评估有效且提效，可用于开发流程。


<details>
  <summary>Details</summary>
Motivation: 大语言模型浏览器扩展带来测试挑战，传统方法和现有 LLM 测试法有局限，需有效评估框架。

Method: 提出 ASSURE 框架，含模块化测试用例生成引擎、自动执行框架和可配置验证管道。

Result: 评估六个常用扩展，发现 531 个问题，测试吞吐量提升 6.4 倍，平均 12.4 分钟检测关键安全漏洞。

Conclusion: ASSURE 有效解决大语言模型浏览器扩展测试挑战，可集成到开发流程。

Abstract: The integration of Large Language Models (LLMs) into browser extensions has
revolutionized web browsing, enabling sophisticated functionalities like
content summarization, intelligent translation, and context-aware writing
assistance. However, these AI-powered extensions introduce unprecedented
challenges in testing and reliability assurance. Traditional browser extension
testing approaches fail to address the non-deterministic behavior,
context-sensitivity, and complex web environment integration inherent to
LLM-powered extensions. Similarly, existing LLM testing methodologies operate
in isolation from browser-specific contexts, creating a critical gap in
effective evaluation frameworks. To bridge this gap, we present ASSURE, a
modular automated testing framework specifically designed for AI-powered
browser extensions. ASSURE comprises three principal components: (1) a modular
test case generation engine that supports plugin-based extension of testing
scenarios, (2) an automated execution framework that orchestrates the complex
interactions between web content, extension processing, and AI model behavior,
and (3) a configurable validation pipeline that systematically evaluates
behavioral consistency and security invariants rather than relying on exact
output matching. Our evaluation across six widely-used AI browser extensions
demonstrates ASSURE's effectiveness, identifying 531 distinct issues spanning
security vulnerabilities, metamorphic relation violations, and content
alignment problems. ASSURE achieves 6.4x improved testing throughput compared
to manual approaches, detecting critical security vulnerabilities within 12.4
minutes on average. This efficiency makes ASSURE practical for integration into
development pipelines, offering a comprehensive solution to the unique
challenges of testing AI-powered browser extensions.

</details>


### [163] [OASBuilder: Generating OpenAPI Specifications from Online API Documentation with Large Language Models](https://arxiv.org/abs/2507.05316)
*Koren Lazar,Matan Vetzler,Kiran Kate,Jason Tsay,David Boaz Himanshu Gupta,Avraham Shinnar,Rohith D Vallam,David Amid Esther Goldbraich,Guy Uziel,Jim Laredo,Ateret Anaby Tavor*

Main category: cs.SE

TL;DR: 介绍OASBuilder框架将API文档转换为机器可读规范，实验效果好且在企业应用中节省大量人力。


<details>
  <summary>Details</summary>
Motivation: 在线API信息多为非结构化HTML文档，手动转换耗时，需自动化转换方法。

Method: 构建整合大语言模型和基于规则算法的流水线，利用文档网页结构领域知识。

Result: OASBuilder在数百个API上泛化性好，生成有效OpenAPI规范，涵盖原文档大多信息。

Conclusion: OASBuilder在企业环境成功实施，节省大量手动工作时间，使复杂企业API可作为大语言模型工具。

Abstract: AI agents and business automation tools interacting with external web
services require standardized, machine-readable information about their APIs in
the form of API specifications. However, the information about APIs available
online is often presented as unstructured, free-form HTML documentation,
requiring external users to spend significant time manually converting it into
a structured format. To address this, we introduce OASBuilder, a novel
framework that transforms long and diverse API documentation pages into
consistent, machine-readable API specifications. This is achieved through a
carefully crafted pipeline that integrates large language models and rule-based
algorithms which are guided by domain knowledge of the structure of
documentation webpages. Our experiments demonstrate that OASBuilder generalizes
well across hundreds of APIs, and produces valid OpenAPI specifications that
encapsulate most of the information from the original documentation. OASBuilder
has been successfully implemented in an enterprise environment, saving
thousands of hours of manual effort and making hundreds of complex enterprise
APIs accessible as tools for LLMs.

</details>


### [164] [Exploring Empathy in Software Engineering: Insights from a Grey Literature Analysis of Practitioners' Perspectives](https://arxiv.org/abs/2507.05325)
*Lidiany Cerqueira,João Pedro Bastos,Danilo Neves,Glauco Carneiro,Rodrigo Spínola,Sávio Freire,José Amancio Macedo Santos,Manoel Mendonça*

Main category: cs.SE

TL;DR: 研究从从业者角度调查软件工程中的同理心，提出定义、识别障碍、探讨实践及影响，形成概念框架并获认可，为改善团队动态奠基。


<details>
  <summary>Details</summary>
Motivation: 同理心是软件工程中重要但研究不足的社交技能，研究旨在从从业者角度研究其含义、障碍、克服方法及影响。

Method: 对55篇网络文章进行定性内容分析，并对同理心专家进行后续调查。

Result: 提出软件工程中同理心的定义，识别障碍和促进实践，得出积极成果，综合成概念框架。

Conclusion: 框架清晰有价值，能提高同理心意识，为改善团队动态提供方向，未来将探索更广泛影响。

Abstract: Context. Empathy, a key social skill, is essential for communication and
collaboration in SE but remains an under-researched topic. Aims. This study
investigates empathy in SE from practitioners' perspectives, aiming to
characterize its meaning, identify barriers, discuss practices to overcome
them, and explore its effects. Method. A qualitative content analysis was
conducted on 55 web articles from DEV and Medium, two communities widely used
by practitioners. To strengthen our findings, we conducted a follow-up survey
with empathy experts. Results. The study proposes a definition of empathy in
SE, identifies barriers such as toxic culture and excessive technical focus,
practices to foster empathy in teams, and outcomes, including improved
collaboration, communication, and reduced anxiety, frustration, and stress.
These findings are synthesized into a conceptual framework. Conclusion. Survey
results indicate the framework is clear, valuable, and raises empathy
awareness, with suggestions for improvements and integration into training.
This study paves the way for improving team dynamics by addressing barriers and
offering strategies to cultivate empathy. Future work will explore empathy's
broader implications in SE practice.

</details>


### [165] [Tool for Supporting Debugging and Understanding of Normative Requirements Using LLMs](https://arxiv.org/abs/2507.05504)
*Alex Kleijwegt,Sinem Getir Yaman,Radu Calinescu*

Main category: cs.SE

TL;DR: 介绍SLEEC-LLM工具，用大语言模型为SLEEC规则不一致的模型检查反例提供自然语言解释，提高规范需求获取和一致性分析的效率与可解释性，并通过两个案例展示其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有用领域特定语言和形式化方法识别SLEEC需求的研究，结果对非技术用户不友好，导致需求获取和验证过程低效。

Method: 引入SLEEC-LLM工具，利用大语言模型为模型检查反例提供自然语言解释。

Result: 通过两个涉及非技术利益相关者的实际案例展示了SLEEC-LLM的有效性。

Conclusion: SLEEC-LLM提高了规范需求获取和一致性分析的效率和可解释性。

Abstract: Normative requirements specify social, legal, ethical, empathetic, and
cultural (SLEEC) norms that must be observed by a system. To support the
identification of SLEEC requirements, numerous standards and regulations have
been developed. These requirements are typically defined by stakeholders in the
non-technical system with diverse expertise (e.g., ethicists, lawyers, social
scientists). Hence, ensuring their consistency and managing the requirement
elicitation process are complex and error-prone tasks. Recent research has
addressed this challenge using domain-specific languages to specify normative
requirements as rules, whose consistency can then be analyzed with formal
methods. Nevertheless, these approaches often present the results from formal
verification tools in a way that is inaccessible to non-technical users. This
hinders understanding and makes the iterative process of eliciting and
validating these requirements inefficient in terms of both time and effort. To
address this problem, we introduce SLEEC-LLM, a tool that uses large language
models (LLMs) to provide natural-language interpretations for model-checking
counterexamples corresponding to SLEEC rule inconsistencies. SLEEC-LLM improves
the efficiency and explainability of normative requirements elicitation and
consistency analysis. To demonstrate its effectiveness, we summarise its use in
two real-world case studies involving non-technical stakeholders.

</details>


### [166] [TigAug: Data Augmentation for Testing Traffic Light Detection in Autonomous Driving Systems](https://arxiv.org/abs/2507.05932)
*You Lu,Dingji Wang,Kaifeng Huang,Bihuan Chen,Xin Peng*

Main category: cs.SE

TL;DR: 提出TigAug自动增强交通灯图像以测试自动驾驶系统中交通灯检测模型，实验证明其有效、高效且生成图像自然度可接受。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统可靠性和鲁棒性需保障，但交通灯检测模型自动测试受忽视，手动收集标注数据劳动密集且难覆盖多样环境。

Method: 构建两类变质关系和三类变换，利用增强图像通过特定变质关系检测模型错误行为，通过再训练提升模型性能。

Result: 用四个先进交通灯检测模型和两个数据集实验表明，TigAug测试有效、合成图像高效，生成图像自然度可接受。

Conclusion: TigAug可有效用于测试自动驾驶系统中的交通灯检测模型。

Abstract: Autonomous vehicle technology has been developed in the last decades with
recent advances in sensing and computing technology. There is an urgent need to
ensure the reliability and robustness of autonomous driving systems (ADSs).
Despite the recent achievements in testing various ADS modules, little
attention has been paid on the automated testing of traffic light detection
models in ADSs. A common practice is to manually collect and label traffic
light data. However, it is labor-intensive, and even impossible to collect
diverse data under different driving environments.
  To address these problems, we propose and implement TigAug to automatically
augment labeled traffic light images for testing traffic light detection models
in ADSs. We construct two families of metamorphic relations and three families
of transformations based on a systematic understanding of weather environments,
camera properties, and traffic light properties. We use augmented images to
detect erroneous behaviors of traffic light detection models by
transformation-specific metamorphic relations, and to improve the performance
of traffic light detection models by retraining. Large-scale experiments with
four state-of-the-art traffic light detection models and two traffic light
datasets have demonstrated that i) TigAug is effective in testing traffic light
detection models, ii) TigAug is efficient in synthesizing traffic light images,
and iii) TigAug generates traffic light images with acceptable naturalness.

</details>


### [167] [Multi-Agent Debate Strategies to Enhance Requirements Engineering with Large Language Models](https://arxiv.org/abs/2507.05981)
*Marc Oriol,Quim Motger,Jordi Marco,Xavier Franch*

Main category: cs.SE

TL;DR: 研究多智能体辩论（MAD）策略能否提升大语言模型在需求工程（RE）任务中的性能，确认其可行性和前景。


<details>
  <summary>Details</summary>
Motivation: 现有提升大语言模型在RE任务中准确性的方法将模型视为孤立黑盒，缺乏迭代改进和协作，限制了鲁棒性和适应性，受人类辩论启发提出研究MAD策略。

Method: 对各领域现有MAD策略进行系统研究，确定关键特征，并实现和测试基于MAD的RE分类框架。

Result: 识别并分类了多种MAD策略，形成分类法，初步评估证明了MAD应用于RE分类的可行性。

Conclusion: MAD是提高LLM在RE任务中准确性的有前景的方法，为未来RE应用研究和改进提供基础理解。

Abstract: Context: Large Language Model (LLM) agents are becoming widely used for
various Requirements Engineering (RE) tasks. Research on improving their
accuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval
augmented generation. However, these methods often treat models as isolated
black boxes - relying on single-pass outputs without iterative refinement or
collaboration, limiting robustness and adaptability. Objective: We propose
that, just as human debates enhance accuracy and reduce bias in RE tasks by
incorporating diverse perspectives, different LLM agents debating and
collaborating may achieve similar improvements. Our goal is to investigate
whether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:
We conducted a systematic study of existing MAD strategies across various
domains to identify their key characteristics. To assess their applicability in
RE, we implemented and tested a preliminary MAD-based framework for RE
classification. Results: Our study identified and categorized several MAD
strategies, leading to a taxonomy outlining their core attributes. Our
preliminary evaluation demonstrated the feasibility of applying MAD to RE
classification. Conclusions: MAD presents a promising approach for improving
LLM accuracy in RE tasks. This study provides a foundational understanding of
MAD strategies, offering insights for future research and refinements in RE
applications.

</details>


### [168] [PromiseTune: Unveiling Causally Promising and Explainable Configuration Tuning](https://arxiv.org/abs/2507.05995)
*Pengzhou Chen,Tao Chen*

Main category: cs.SE

TL;DR: 现代软件系统配置调优困难，本文提出PromiseTune，用因果净化规则指导，表现优于其他调优器并能提供解释性。


<details>
  <summary>Details</summary>
Motivation: 现有调优器因难以平衡探索和利用预算、缺乏对有前景区域的了解，导致效果不佳且结果难以解释。

Method: 提出PromiseTune，学习规则并通过因果推理净化，用剩余规则界定调优范围。

Result: 在12个系统和不同预算下与11个先进调优器对比，PromiseTune表现显著更好，排名比第二好的高42%。

Conclusion: PromiseTune能有效缓解探索和利用权衡问题，还能提供系统特征的解释信息。

Abstract: The high configurability of modern software systems has made configuration
tuning a crucial step for assuring system performance, e.g., latency or
throughput. However, given the expensive measurements, large configuration
space, and rugged configuration landscape, existing tuners suffer
ineffectiveness due to the difficult balance of budget utilization between
exploring uncertain regions (for escaping from local optima) and exploiting
guidance of known good configurations (for fast convergence). The root cause is
that we lack knowledge of where the promising regions lay, which also causes
challenges in the explainability of the results.
  In this paper, we propose PromiseTune that tunes configuration guided by
causally purified rules. PromiseTune is unique in the sense that we learn
rules, which reflect certain regions in the configuration landscape, and purify
them with causal inference. The remaining rules serve as approximated
reflections of the promising regions, bounding the tuning to emphasize these
places in the landscape. This, as we demonstrate, can effectively mitigate the
impact of the exploration and exploitation trade-off. Those purified regions
can then be paired with the measured configurations to provide spatial
explainability at the landscape level. Comparing with 11 state-of-the-art
tuners on 12 systems and varying budgets, we show that PromiseTune performs
significantly better than the others with $42\%$ superior rank to the overall
second best while providing richer information to explain the hidden system
characteristics.

</details>


### [169] [Model Cards Revisited: Bridging the Gap Between Theory and Practice for Ethical AI Requirements](https://arxiv.org/abs/2507.06014)
*Tim Puhlfürß,Julia Butzke,Walid Maalej*

Main category: cs.SE

TL;DR: 研究分析多份资料识别模型文档伦理要求并分类，发现开发者重能力和可靠性，忽视其他伦理方面，提出需加强伦理AI文档支持，分类法可用于修订框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型文档实践不足，存在AI要求与实践的差距，需理解并弥补该差距。

Method: 对26份伦理与AI指南、3个AI文档框架、3个模型卡定量研究和10个实际模型卡进行主题分析。

Result: 确定43个与模型文档相关的伦理要求，组织成包含四个主题和十二个子主题的分类法，发现开发者在文档中主要强调模型能力和可靠性，忽视其他伦理方面。

Conclusion: 需要增强对伦理AI考虑因素文档化的支持，分类法可作为全面解决伦理AI要求的修订模型卡框架的基础。

Abstract: Model cards are the primary documentation framework for developers of
artificial intelligence (AI) models to communicate critical information to
their users. Those users are often developers themselves looking for relevant
documentation to ensure that their AI systems comply with the ethical
requirements of existing laws, guidelines, and standards. Recent studies
indicate inadequate model documentation practices, suggesting a gap between AI
requirements and current practices in model documentation. To understand this
gap and provide actionable guidance to bridge it, we conducted a thematic
analysis of 26 guidelines on ethics and AI, three AI documentation frameworks,
three quantitative studies of model cards, and ten actual model cards. We
identified a total of 43 ethical requirements relevant to model documentation
and organized them into a taxonomy featuring four themes and twelve sub-themes
representing ethical principles. Our findings indicate that model developers
predominantly emphasize model capabilities and reliability in the documentation
while overlooking other ethical aspects, such as explainability, user autonomy,
and fairness. This underscores the need for enhanced support in documenting
ethical AI considerations. Our taxonomy serves as a foundation for a revised
model card framework that holistically addresses ethical AI requirements.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [170] [Beating the Best Constant Rebalancing Portfolio in Long-Term Investment: A Generalization of the Kelly Criterion and Universal Learning Algorithm for Markets with Serial Dependence](https://arxiv.org/abs/2507.05994)
*Duy Khanh Lam*

Main category: q-fin.PM

TL;DR: 提出一种仅用逐步披露数据学习资产回报序列依赖的算法，能超越最佳恒定再平衡投资组合的累积财富，还推广了凯利准则，实验验证其性能和适用性。


<details>
  <summary>Details</summary>
Motivation: 现有在线投资组合优化学习算法累积财富表现不佳，利用更多侧信息存在特征选择和高维难题，故考虑利用资产回报的固有序列依赖。

Method: 提出仅用逐步披露数据学习序列依赖的算法，将经典凯利准则推广以适应序列依赖。

Result: 实验结果表明，只要序列依赖显著，算法能达到理论预期性能。

Conclusion: 算法具有广泛适用性。

Abstract: In the online portfolio optimization framework, existing learning algorithms
generate strategies that yield significantly poorer cumulative wealth compared
to the best constant rebalancing portfolio in hindsight, despite being
consistent in asymptotic growth rate. While this unappealing performance can be
improved by incorporating more side information, it raises difficulties in
feature selection and high-dimensional settings. Instead, the inherent serial
dependence of assets' returns, such as day-of-the-week and other calendar
effects, can be leveraged. Although latent serial dependence patterns are
commonly detected using large training datasets, this paper proposes an
algorithm that learns such dependence using only gradually revealed data,
without any assumption on their distribution, to form a strategy that
eventually exceeds the cumulative wealth of the best constant rebalancing
portfolio.
  Moreover, the classical Kelly criterion, which requires independent assets'
returns, is generalized to accommodate serial dependence in a market modeled as
an independent and identically distributed process of random matrices. In such
a stochastic market, where existing learning algorithms designed for stationary
processes fail to apply, the proposed learning algorithm still generates a
strategy that asymptotically grows to the highest rate among all strategies,
matching that of the optimal strategy constructed under the generalized Kelly
criterion. The experimental results with real market data demonstrate the
theoretical guarantees of the algorithm and its performance as expected, as
long as serial dependence is significant, regardless of the validity of the
generalized Kelly criterion in the experimental market. This further affirms
the broad applicability of the algorithm in general contexts.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [171] [Enjoying Non-linearity in Multinomial Logistic Bandits](https://arxiv.org/abs/2507.05306)
*Pierre Boudart,Pierre Gaillard,Alessandro Rudi*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the multinomial logistic bandit problem, a variant of generalized
linear bandits where a learner interacts with an environment by selecting
actions to maximize expected rewards based on probabilistic feedback from
multiple possible outcomes. In the binary setting, recent work has focused on
understanding the impact of the non-linearity of the logistic model (Faury et
al., 2020; Abeille et al., 2021). They introduced a problem-dependent constant
$\kappa_*$, that may be exponentially large in some problem parameters and
which is captured by the derivative of the sigmoid function. It encapsulates
the non-linearity and improves existing regret guarantees over $T$ rounds from
$\smash{O(d\sqrt{T})}$ to $\smash{O(d\sqrt{T/\kappa_*})}$, where $d$ is the
dimension of the parameter space. We extend their analysis to the multinomial
logistic bandit framework, making it suitable for complex applications with
more than two choices, such as reinforcement learning or recommender systems.
To achieve this, we extend the definition of $\kappa_*$ to the multinomial
setting and propose an efficient algorithm that leverages the problem's
non-linearity. Our method yields a problem-dependent regret bound of order $
\smash{\widetilde{\mathcal{O}}( Kd \sqrt{{T}/{\kappa_*}})} $, where $K$ is the
number of actions and $\kappa_* \ge 1$. This improves upon the best existing
guarantees of order $ \smash{\widetilde{\mathcal{O}}( Kd \sqrt{T} )} $.
Moreover, we provide a $\smash{ \Omega(d\sqrt{T/\kappa_*})}$ lower-bound,
showing that our dependence on $\kappa_*$ is optimal.

</details>


### [172] [Temporal Conformal Prediction (TCP): A Distribution-Free Statistical and Machine Learning Framework for Adaptive Risk Forecasting](https://arxiv.org/abs/2507.05470)
*Agnideep Aich,Ashit Baran Aich,Dipak C. Jain*

Main category: stat.ML

TL;DR: 提出Temporal Conformal Prediction (TCP)框架用于金融时间序列预测区间构建，经实证表现良好，提供新的金融不确定性量化方法。


<details>
  <summary>Details</summary>
Motivation: 构建在金融时间序列中具有有限样本有效性的预测区间，应对非平稳性、波动聚类和制度转换等问题。

Method: 将分位数回归与通过衰减学习率在线适应的共形校准层集成，形成TCP框架，并与GARCH、Historical Simulation和静态分位数回归等方法进行基准测试。

Result: TCP在不同资产类别中，尤其是高波动制度下，能持续提供更精确的区间和有竞争力或更优的覆盖率。

Conclusion: TCP在平衡覆盖率和精确性方面表现出色，是一种无分布、自适应且可解释的金融不确定性量化方法，推动了金融中统计推断与机器学习的结合。

Abstract: We propose Temporal Conformal Prediction (TCP), a novel framework for
constructing prediction intervals in financial time-series with guaranteed
finite-sample validity. TCP integrates quantile regression with a conformal
calibration layer that adapts online via a decaying learning rate. This hybrid
design bridges statistical and machine learning paradigms, enabling TCP to
accommodate non-stationarity, volatility clustering, and regime shifts which
are hallmarks of real-world asset returns, without relying on rigid parametric
assumptions. We benchmark TCP against established methods including GARCH,
Historical Simulation, and static Quantile Regression across equities (S&P
500), cryptocurrency (Bitcoin), and commodities (Gold). Empirical results show
that TCP consistently delivers sharper intervals with competitive or superior
coverage, particularly in high-volatility regimes. Our study underscores TCP's
strength in navigating the coverage-sharpness tradeoff, a central challenge in
modern risk forecasting. Overall, TCP offers a distribution-free, adaptive, and
interpretable alternative for financial uncertainty quantification, advancing
the interface between statistical inference and machine learning in finance.

</details>


### [173] [A Malliavin calculus approach to score functions in diffusion generative models](https://arxiv.org/abs/2507.05550)
*Ehsan Mirafzali,Frank Proske,Utkarsh Gupta,Daniele Venturi,Razvan Marinescu*

Main category: stat.ML

TL;DR: 本文为一类非线性扩散生成模型推导了得分函数的精确闭式表达式，结合现代随机分析工具，提升其实际适用性，为生成建模得分估计方法提供理论基础，结果可拓展到更广泛的随机微分方程。


<details>
  <summary>Details</summary>
Motivation: 现有得分函数通常通过近似技术从数据中估计，本文旨在为一类非线性扩散生成模型推导精确的得分函数闭式表达式。

Method: 结合Malliavin导数及其伴随算子（Skorokhod积分或Malliavin散度）与新的Bismut型公式。

Result: 得到了仅用一阶和二阶变分过程表示的得分函数表达式，消除了所有Malliavin导数，结果可拓展到更广泛的随机微分方程。

Conclusion: 该理论框架为生成建模中得分估计方法提供了原则性基础，开启了基于得分的扩散生成模型发展的新方向。

Abstract: Score-based diffusion generative models have recently emerged as a powerful
tool for modelling complex data distributions. These models aim at learning the
score function, which defines a map from a known probability distribution to
the target data distribution via deterministic or stochastic differential
equations (SDEs). The score function is typically estimated from data using a
variety of approximation techniques, such as denoising or sliced score
matching, Hyv\"arien's method, or Schr\"odinger bridges. In this paper, we
derive an exact, closed form, expression for the score function for a broad
class of nonlinear diffusion generative models. Our approach combines modern
stochastic analysis tools such as Malliavin derivatives and their adjoint
operators (Skorokhod integrals or Malliavin Divergence) with a new Bismut-type
formula. The resulting expression for the score function can be written
entirely in terms of the first and second variation processes, with all
Malliavin derivatives systematically eliminated, thereby enhancing its
practical applicability. The theoretical framework presented in this work
offers a principled foundation for advancing score estimation methods in
generative modelling, enabling the design of new sampling algorithms for
complex probability distributions. Our results can be extended to broader
classes of stochastic differential equations, opening new directions for the
development of score-based diffusion generative models.

</details>


### [174] [Property Elicitation on Imprecise Probabilities](https://arxiv.org/abs/2507.05857)
*James Bailie,Rabanus Derr*

Main category: stat.ML

TL;DR: 研究将属性引出推广到不精确概率（IP），给出IP属性可引出性的必要条件并解释引出内容


<details>
  <summary>Details</summary>
Motivation: 受多分布学习启发，用Γ - 极大极小风险最小化替代经典机器学习中单一风险最小化

Method: 研究属性引出推广到IP，给出必要条件并通过贝叶斯对解释引出内容

Result: 得到IP属性可引出性的必要条件，明确引出的IP属性是最大贝叶斯风险分布的相应标准属性

Conclusion: 成功将属性引出推广到IP并解释引出本质

Abstract: Property elicitation studies which attributes of a probability distribution
can be determined by minimising a risk. We investigate a generalisation of
property elicitation to imprecise probabilities (IP). This investigation is
motivated by multi-distribution learning, which takes the classical machine
learning paradigm of minimising a single risk over a (precise) probability and
replaces it with $\Gamma$-maximin risk minimization over an IP. We provide
necessary conditions for elicitability of a IP-property. Furthermore, we
explain what an elicitable IP-property actually elicits through Bayes pairs --
the elicited IP-property is the corresponding standard property of the maximum
Bayes risk distribution.

</details>


### [175] [Best-of-N through the Smoothing Lens: KL Divergence and Regret Analysis](https://arxiv.org/abs/2507.05913)
*Gholamali Aminian,Idan Shenfeld,Amir R. Asadi,Ahmad Beirami,Youssef Mroueh*

Main category: stat.ML

TL;DR: 研究Best-of-$N$ (BoN)通过Soft Best-of-N (SBoN)，分析其缩放行为和后悔差距，发现平滑有助于SBoN缓解奖励过度优化。


<details>
  <summary>Details</summary>
Motivation: BoN有效性依赖代理奖励模型质量，需要理论框架解决此问题。

Method: 通过SBoN研究BoN，分析SBoN策略与参考策略的KL散度，研究后悔差距。

Result: 理论和实证表明，平滑有助于SBoN缓解奖励过度优化，尤其是代理奖励质量低时。

Conclusion: 平滑可提升SBoN在代理奖励质量低时的性能。

Abstract: A simple yet effective method for inference-time alignment of generative
models is Best-of-$N$ (BoN), where $N$ outcomes are sampled from a reference
policy, evaluated using a proxy reward model, and the highest-scoring one is
selected. While prior work argues that BoN is almost optimal in reward vs KL
tradeoffs, the effectiveness of BoN depends critically on the quality of the
proxy reward model used for selection. For this purpose, we study BoN through a
smooth version known as Soft Best-of-N (SBoN) and develop a theoretical
framework to address this gap. We analyze the scaling behaviour of BoN by
providing bounds on the KL divergence between the SBoN policy and the reference
policy, offering insights into how performance varies with the number of
samples. We also study the regret gap, i.e., the gap between the expected true
reward under the optimal policy and the SBoN policy. Our theoretical and
empirical findings show that smoothing helps SBoN mitigate reward
overoptimization, especially when the quality of the proxy reward is low.

</details>


### [176] [Online Regularized Learning Algorithms in RKHS with $β$- and $φ$-Mixing Sequences](https://arxiv.org/abs/2507.05929)
*Priyanka Roy,Susanne Saminger-Platz*

Main category: stat.ML

TL;DR: 研究基于一类相依过程的在线正则化学习算法，推导不同混合系数衰减情况下的概率上界和收敛速率。


<details>
  <summary>Details</summary>
Motivation: 研究基于相依过程的在线正则化学习算法在再生核希尔伯特空间中的性质。

Method: 选择用混合系数衡量相依程度的过程，以严格平稳马尔可夫链为例，分析其依赖结构。

Result: 在混合系数指数和多项式衰减的情况下，推导出概率上界和收敛速率。

Conclusion: 该研究为基于相依过程的在线正则化学习算法提供了理论上的概率上界和收敛速率。

Abstract: In this paper, we study an online regularized learning algorithm in a
reproducing kernel Hilbert spaces (RKHS) based on a class of dependent
processes. We choose such a process where the degree of dependence is measured
by mixing coefficients. As a representative example, we analyze a strictly
stationary Markov chain, where the dependence structure is characterized by the
\(\phi\)- and \(\beta\)-mixing coefficients. Under these assumptions, we derive
probabilistic upper bounds as well as convergence rates for both the
exponential and polynomial decay of the mixing coefficients.

</details>


### [177] [Kernel Trace Distance: Quantum Statistical Metric between Measures through RKHS Density Operators](https://arxiv.org/abs/2507.06055)
*Arturo Castellanos,Anna Korba,Pavlo Mozharovskyi,Hicham Janati*

Main category: stat.ML

TL;DR: 本文引入基于核协方差算子Schatten范数的概率分布新距离，展示其特性并提供计算算法，通过应用说明优势。


<details>
  <summary>Details</summary>
Motivation: 概率分布距离在统计机器学习任务中很重要，现有距离存在不足，需要新的距离度量。

Method: 引入基于核协方差算子Schatten范数的距离，构建核矩阵扩展用于计算。

Result: 新距离是积分概率度量，介于MMD和Wasserstein距离之间，更具判别性、对超参数选择更稳健，避免维数灾难。

Conclusion: 新距离有优势，通过鲁棒近似贝叶斯计算和粒子流模拟得以体现。

Abstract: Distances between probability distributions are a key component of many
statistical machine learning tasks, from two-sample testing to generative
modeling, among others. We introduce a novel distance between measures that
compares them through a Schatten norm of their kernel covariance operators. We
show that this new distance is an integral probability metric that can be
framed between a Maximum Mean Discrepancy (MMD) and a Wasserstein distance. In
particular, we show that it avoids some pitfalls of MMD, by being more
discriminative and robust to the choice of hyperparameters. Moreover, it
benefits from some compelling properties of kernel methods, that can avoid the
curse of dimensionality for their sample complexity. We provide an algorithm to
compute the distance in practice by introducing an extension of kernel matrix
for difference of distributions that could be of independent interest. Those
advantages are illustrated by robust approximate Bayesian computation under
contamination as well as particle flow simulations.

</details>


### [178] [Estimating prevalence with precision and accuracy](https://arxiv.org/abs/2507.06061)
*Aime Bienfait Igiraneza,Christophe Fraser,Robert Hinch*

Main category: stat.ML

TL;DR: 提出更精确且置信区间校准良好的贝叶斯量词PQ，分析影响量化精度的因素。


<details>
  <summary>Details</summary>
Motivation: 现有量化流行率估计中不确定性的方法在精度和覆盖范围方面哪种更理想尚不明确。

Method: 提出贝叶斯量词PQ，基于模拟和真实世界数据集进行实验。

Result: 确定影响量化精度的因素，包括底层分类器的辨别能力、训练量词的标记数据集大小、估计流行率的未标记数据集大小。

Conclusion: 分析为量化学习中的不确定性量化提供了深刻见解。

Abstract: Unlike classification, whose goal is to estimate the class of each data point
in a dataset, prevalence estimation or quantification is a task that aims to
estimate the distribution of classes in a dataset. The two main tasks in
prevalence estimation are to adjust for bias, due to the prevalence in the
training dataset, and to quantify the uncertainty in the estimate. The standard
methods used to quantify uncertainty in prevalence estimates are bootstrapping
and Bayesian quantification methods. It is not clear which approach is ideal in
terms of precision (i.e. the width of confidence intervals) and coverage (i.e.
the confidence intervals being well-calibrated). Here, we propose Precise
Quantifier (PQ), a Bayesian quantifier that is more precise than existing
quantifiers and with well-calibrated coverage. We discuss the theory behind PQ
and present experiments based on simulated and real-world datasets. Through
these experiments, we establish the factors which influence quantification
precision: the discriminatory power of the underlying classifier; the size of
the labeled dataset used to train the quantifier; and the size of the unlabeled
dataset for which prevalence is estimated. Our analysis provides deep insights
into uncertainty quantification for quantification learning.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [179] [hassediagrams:an R package that generates the Hasse diagram of the layout structure and the restricted layout structure](https://arxiv.org/abs/2507.05949)
*Damianos Michaelides,Simon T. Bate,Marion J. Chatfield*

Main category: stat.CO

TL;DR: 介绍R语言中hassediagrams包可确定实验设计结构、生成哈斯图，助于统计模型选择。


<details>
  <summary>Details</summary>
Motivation: 现代统计软件使复杂实验设计广泛应用，错误识别设计结构会致模型选择错误和推理误导。

Method: 使用R语言的hassediagrams包确定设计结构、生成哈斯图，结合随机化定义随机化对象形成受限布局结构。

Result: 可生成布局结构和受限布局结构的哈斯图，受限布局结构中的对象可用于确定统计模型应包含的项。

Conclusion: 该程序因系统地生成模型，能确保模型选择的一致性。

Abstract: With the advent of modern statistical software, complex experimental designs
are now routinely employed in many areas of research. Failing to correctly
identify the structure of the experimental design can lead to incorrect model
selection and misleading inferences. This paper describes the hassediagrams
package in R that determines the structure of the design, summarised by the
layout structure, and generates a Hasse diagram of the layout structure. By
considering the randomisation performed, in conjunction with the layout
structure, a set of randomisation objects can be defined that form the
restricted layout structure. This structure can also be visualised using a
generalisation of the Hasse diagram. Objects in the restricted layout structure
can be used to identify the terms to include in the statistical model. The use
of the procedure thus ensures consistency of model selection due to the
systematic approach taken to generate the model.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [180] [The Neural Networks with Tensor Weights and the Corresponding Fermionic Quantum Field Theory](https://arxiv.org/abs/2507.05303)
*Guojun Huang,Kai Zhou*

Main category: hep-th

TL;DR: 本文建立复值神经网络与费米子量子场论的理论联系，拓展了神经网络量子场论。


<details>
  <summary>Details</summary>
Motivation: 此前神经网络量子场论工作仅将实值架构与玻色场联系，需建立复值神经网络与费米子量子场论的联系。

Method: 将隐藏到输出层的权重提升为克利福德代数值张量，分析生成泛函。

Result: 在无限宽度极限下得到精确量子态，连续极限重现自由费米子关联函数，图表展开证实反对易关系。

Conclusion: 实现了从神经架构到费米子量子场论在关联函数和生成泛函层面的明确映射，拓展了神经网络量子场论，为机器学习模型编码费米子对称性提供途径。

Abstract: In this paper, we establish a theoretical connection between complex-valued
neural networks (CVNNs) and fermionic quantum field theory (QFT), bridging a
fundamental gap in the emerging framework of neural network quantum field
theory (NN-QFT). While prior NN-QFT works have linked real-valued architectures
to bosonic fields, we demonstrate that CVNNs equipped with tensor-valued
weights intrinsically generate fermionic quantum fields. By promoting
hidden-to-output weights to Clifford algebra-valued tensors, we induce
anticommutation relations essential for fermionic statistics. Through
analytical study of the generating functional, we obtain the exact quantum
state in the infinite-width limit, revealing that the parameters between the
input layer and the last hidden layer correspond to the eigenvalues of the
quantum system, and the tensor weighting parameters in the hidden-to-output
layer map to dynamical fermionic fields. The continuum limit reproduces free
fermion correlators, with diagrammatic expansions confirming anticommutation.
The work provides the first explicit mapping from neural architectures to
fermionic QFT at the level of correlation functions and generating functional.
It extends NN-QFT beyond bosonic theories and opens avenues for encoding
fermionic symmetries into machine learning models, with potential applications
in quantum simulation and lattice field theory.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [181] [Optimal structure learning and conditional independence testing](https://arxiv.org/abs/2507.05689)
*Ming Gao,Yuhao Wang,Bryon Aragam*

Main category: math.ST

TL;DR: 论文建立最优结构学习与最优条件独立性测试的联系，推导多模型最优率，指出最优算法及提供分析框架。


<details>
  <summary>Details</summary>
Motivation: 探寻最优结构学习和最优条件独立性测试之间的联系，以提供分析结构学习统计复杂性的统一框架。

Method: 在多森林情况下建立两问题的通用约简关系，推导伯努利、高斯和非参数模型等例子的最优率。

Result: 得出结构学习问题的极小极大最优率由条件独立性测试的极小极大率决定，最优算法是PC算法的适当修改。

Conclusion: 理论成果为通过极小极大测试分析结构学习的统计复杂性提供统一框架。

Abstract: We establish a fundamental connection between optimal structure learning and
optimal conditional independence testing by showing that the minimax optimal
rate for structure learning problems is determined by the minimax rate for
conditional independence testing in these problems. This is accomplished by
establishing a general reduction between these two problems in the case of
poly-forests, and demonstrated by deriving optimal rates for several examples,
including Bernoulli, Gaussian and nonparametric models. Furthermore, we show
that the optimal algorithm in these settings is a suitable modification of the
PC algorithm. This theoretical finding provides a unified framework for
analyzing the statistical complexity of structure learning through the lens of
minimax testing.

</details>


### [182] [Consistency and Inconsistency in $K$-Means Clustering](https://arxiv.org/abs/2507.06226)
*Moïse Blanchard,Adam Quinn Jaffe,Nikita Zhivotovskiy*

Main category: math.ST

TL;DR: 文章探讨有限期望假设下k - 均值聚类的渐近一致性，有负面和正面结果。


<details>
  <summary>Details</summary>
Motivation: 在有限期望这一较弱假设下，研究k - 均值聚类是否存在某种形式的渐近一致性。

Method: 先给出多种负面结果分析不一致原因，后给出正面结果。

Result: 负面结果显示即使总体k - 均值聚类中心唯一，经验k - 均值聚类中心也可能不收敛；正面结果表明对经验k - 均值聚类施加先验平衡度可恢复一定的渐近一致性。

Conclusion: 在有限期望假设下，k - 均值聚类渐近一致性情况复杂，施加平衡度可恢复部分一致性。

Abstract: A celebrated result of Pollard proves asymptotic consistency for $k$-means
clustering when the population distribution has finite variance. In this work,
we point out that the population-level $k$-means clustering problem is, in
fact, well-posed under the weaker assumption of a finite expectation, and we
investigate whether some form of asymptotic consistency holds in this setting.
As we illustrate in a variety of negative results, the complete story is quite
subtle; for example, the empirical $k$-means cluster centers may fail to
converge even if there exists a unique set of population $k$-means cluster
centers. A detailed analysis of our negative results reveals that inconsistency
arises because of an extreme form of cluster imbalance, whereby the presence of
outlying samples leads to some empirical $k$-means clusters possessing very few
points. We then give a collection of positive results which show that some
forms of asymptotic consistency, under only the assumption of finite
expectation, may be recovered by imposing some a priori degree of balance among
the empirical $k$-means clusters.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [183] [Increasing Systemic Resilience to Socioeconomic Challenges: Modeling the Dynamics of Liquidity Flows and Systemic Risks Using Navier-Stokes Equations](https://arxiv.org/abs/2507.05287)
*Davit Gondauri*

Main category: econ.GN

TL;DR: 研究开发基于Navier - Stokes方程的创新数学模型评估和预测流动性与系统性风险，用格鲁吉亚数据验证，模型效果良好。


<details>
  <summary>Details</summary>
Motivation: 现代经济系统面临挑战，传统模型无法反映市场波动和极端事件，需新模型评估流动性和系统性风险。

Method: 开发基于Navier - Stokes方程的数学模型，纳入13个宏观经济和金融参数，采用计量经济测试、傅里叶分析、随机模拟和AI校准等方法，用格鲁吉亚2010 - 2024年数据实证测试。

Result: 模型能有效描述流动性动态、系统性风险和极端情景，提供多因素分析、危机预测和逆周期政策规划框架。

Conclusion: 新模型可用于流动性和系统性风险的定量评估、预测和模拟，有实际应用价值。

Abstract: Modern economic systems face unprecedented socioeconomic challenges, making
systemic resilience and effective liquidity flow management essential.
Traditional models such as CAPM, VaR, and GARCH often fail to reflect real
market fluctuations and extreme events. This study develops and validates an
innovative mathematical model based on the Navier-Stokes equations, aimed at
the quantitative assessment, forecasting, and simulation of liquidity flows and
systemic risks. The model incorporates 13 macroeconomic and financial
parameters, including liquidity velocity, market pressure, internal stress,
stochastic fluctuations, and risk premiums, all based on real data and formally
included in the modified equation. The methodology employs econometric testing,
Fourier analysis, stochastic simulation, and AI-based calibration to enable
dynamic testing and forecasting. Simulation-based sensitivity analysis
evaluates the impact of parameter changes on financial balance. The model is
empirically tested using Georgian macroeconomic and financial data from
2010-2024, including GDP, inflation, the Gini index, CDS spreads, and LCR
metrics. Results show that the model effectively describes liquidity dynamics,
systemic risk, and extreme scenarios, while also offering a robust framework
for multifactorial analysis, crisis prediction, and countercyclical policy
planning.

</details>


### [184] [The Feasibility of MBSs as Decentralized Autonomous Organizations](https://arxiv.org/abs/2507.05439)
*Timothy Dombrowski,V. Carlos Slawson Jr*

Main category: econ.GN

TL;DR: 本文探讨利用现代金融科技工具重建传统结构化产品，如通过MBS - DAO让MBS合同编程化，以提高透明度和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 解决MBS投资者面临的现金流报告延迟带来的价值不确定性问题，提高市场效率。

Method: 运用资产代币化、智能合约、DAOs等现代金融科技工具，用ACTUS逻辑表达MBS合同，并在公共区块链上实施。

Result: 提出创建包含个人抵押贷款、房地产资产和贷款担保的MBS - DAO正式算法设计的可能性。

Conclusion: 现代金融科技工具可用于重建传统结构化产品，MBS - DAO有提高透明度和市场效率的潜力。

Abstract: Can the general structure of a mortgage-backed security (MBS) contract be
programmatically represented through the use of decentralized autonomous
organizations (DAOs)? Such an approach could allow for the portfolio of loans
to be managed by investors in a trustless and transparent way. The focus and
scope of this paper is to explore the potential for applying the tools of
modern fintech, such as asset tokenization, smart contracts, and DAOs, to
reconstruct traditional structured products that have a greater degree of
transparency and traceability. MBS investors face considerable value
uncertainty as time increases between the actual occurrence (or non-occurrence)
of cash flows and subsequent reporting. Given that an MBS is a financial
contract, it should be expressible logically using the Algorithmic Contract
Types Unified Standards (ACTUS). Since each underlying mortgage in an MBS
derives its cash flows in a prescribed way over the life of the contract,
implementation on a public blockchain could enable real-time ratings systems,
improving market efficiency. We explore the potential for creating formal
algorithmic designs of MBS-DAOs that incorporate individual mortgages, the
underlying real estate assets (collateral), and any loan guarantees.

</details>


### [185] [Branding through responsibility: the advertising impact of CSR activities in the Korean instant noodles market](https://arxiv.org/abs/2507.05782)
*Youngjin Hong,In Kyung Kim,Kyoo il Kim*

Main category: econ.GN

TL;DR: 本文实证研究企业社会贡献塑造的良好形象对消费者选择和企业销售的影响，以韩国方便面市场为例，发现Ottogi企业社会责任活动提升消费者好感度，促进销售，表明CSR可推动企业增长。


<details>
  <summary>Details</summary>
Motivation: 探究企业社会贡献塑造的良好形象在多大程度上影响消费者选择和企业销售。

Method: 利用反映韩国方便面市场各企业企业社会责任活动媒体曝光度的好感度评级进行实证研究。

Result: Ottogi企业形象改善对消费者产品效用有积极影响，主要品牌年销量平均增加2370万包，即6.7%，效果相当于广告支出增加近60%。

Conclusion: 企业社会责任活动可通过促进产品销售推动企业增长。

Abstract: This paper empirically examines the extent to which a favorable view of a
firm, shaped by its social contributions, influences consumer choices and firm
sales. Using a favorability rating that reflects media exposure of each firm's
corporate social responsibility (CSR) activities in the Korean instant noodles
market during the 2010s, we find evidence that improvements in the corporate
image of Ottogi - one of the country's largest instant noodle producers -
positively affected consumer utility for the firm's products. Notably, Ottogi's
annual sales of its major brands increased by an average of 23.7 million
packages, or 6.7%, as a result of CSR activities and the associated rise in
consumer favorability. This effect is comparable in magnitude to that of a
nearly 60% increase in advertising spending. Our findings suggest that CSR can
foster firm growth by boosting product sales.

</details>


### [186] [A job-based assessment of economic complexity: from hidden to revealed](https://arxiv.org/abs/2507.05846)
*Antonio Russo,Pasquale Scaramozzino,Andrea Zaccaria*

Main category: econ.GN

TL;DR: 提出基于工作的隐藏复杂性度量方法，发现其与工资和劳动生产率增长正相关，讨论其在区域层面应用与经济增长的联系。


<details>
  <summary>Details</summary>
Motivation: 现有经济复杂性度量因能力不可观测无法直接用于计算，需新方法估计产业和地区能力禀赋。

Method: 通过量化特定行业所需职业技能的质量和多样性来估计能力禀赋，提出基于工作的隐藏复杂性评估方法。

Result: 基于工作的复杂性度量与工资水平和劳动生产率增长正相关，经典揭示性度量则无此关联。

Conclusion: 基于工作的复杂性度量方法有应用价值，与经济增长有联系。

Abstract: Economic complexity measures aim to quantify the capability content or
endowment of industries and territories; however, capabilities are not
observable, and therefore cannot be directly used in the computations. We
estimate such endowments by quantifying the quality and diversity of the skills
in the occupations required in specific industries. We refer to this job-based
assessment as the hidden complexity, in contrast with the usual revealed
complexity, which is computed from economic outputs such as exports or
production. We show that our job-based measure of complexity is positively
associated to wage levels and labor productivity growth, whereas the classic
revealed measure is not. Finally, we discuss the application of these methods
at the territorial level, showing their connection with economic growth.

</details>


### [187] [Cutting the Geopolitical Ties: Foreign Exchange Reserves, GDP and Military Spending](https://arxiv.org/abs/2507.05856)
*Boris Podobnik,Dorian Wild,Dejan Kovac*

Main category: econ.GN

TL;DR: 研究发现过去20年西方经济体外汇储备与GDP和军事支出高度相关，军事支出对储备解释力更强，人民币正趋向新的外汇储备平衡，构建模型预测未来15 - 40年可能达新平衡。


<details>
  <summary>Details</summary>
Motivation: 探究外汇储备与GDP、军事支出的关系，分析人民币在外汇储备中的趋势并预测未来平衡情况。

Method: 运用Ridge和Lasso回归分析，构建复杂地缘政治网络模型。

Result: 军事支出比GDP更能解释外汇储备，人民币正趋向新平衡，预测15 - 40年可能达到新的外汇储备平衡。

Conclusion: 在过去20年的地缘政治环境下，若趋势持续，人民币和西方货币可能在15 - 40年内达到新的外汇储备平衡。

Abstract: We show that the amount of foreign exchange reserves (FER) in the world in a
given currency is highly correlated with the GDP and military spending of that
country for a set of western economies during the last 20 years. Taking into
account multicollinearity, Ridge and Lasso regressions reveal that the Foreign
Exchange Reserve is better explained by military spending than GDP for seven
western currencies. For each year shown, military spending is statistically
significant more than the monetary instrument M2. Comparing the currency of the
second world economy, the Chinese renminbi, is well beyond the western FER
equilibrium, but yearly analysis shows that there is a steady trend towards a
new FER balance. Next, we define a complex geopolitical network model in which
the probability of switching to an alternative FER currency depends both on
economic and political factors. Military spending is introduced into the model
as an average share of GDP observed within the data. As the GDP of a particular
country grows, so does the military power of a country. The nature of the
creation of new currency networks initially depends only on geopolitical
allegiance. As the volume of trade with a particular country changes over a
designated threshold, a country switches to the currency of that country due to
increased trade. If the current steady trend continues within the same
geopolitical setting as in the past twenty years, we extrapolate that the RMB
and Western currencies could reach a new FER balance within 15 to 40 years,
depending on the model setup.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [188] [Hierarchy or Heterarchy? A Theory of Long-Range Connections for the Sensorimotor Brain](https://arxiv.org/abs/2507.05888)
*Jeff Hawkins,Niels Leadholm,Viviane Clay*

Main category: q-bio.NC

TL;DR: 论文提出千脑理论解释新皮层信息处理，认为新皮层组织用‘异层级’描述更合适，该理论对神经科学和人工智能有广泛影响。


<details>
  <summary>Details</summary>
Motivation: 传统新皮层层级模型有局限性，许多解剖连接不符合标准层级解释，区域有时并行响应，需新理论解释信息处理。

Method: 提出千脑理论，假设每个皮层柱是感觉运动学习系统，通过整合传感器多次移动的感觉输入学习；研究皮层区域间、新皮层与丘脑间的长程连接并分析其在异层级中的作用。

Result: 即使初级和二级区域也能学习和识别完整3D对象，层级连接用于学习父对象由子对象组成的结构，丘脑在转换对象和传感器姿态中起重要作用。

Conclusion: 千脑理论为新皮层感觉和运动信息处理提供新解释，对神经科学和人工智能有广泛意义。

Abstract: In the traditional understanding of the neocortex, sensory information flows
up a hierarchy of regions, with each level processing increasingly complex
features. Information also flows down the hierarchy via a different set of
connections. Although the hierarchical model has significant support, many
anatomical connections do not conform to the standard hierarchical
interpretation. In addition, hierarchically arranged regions sometimes respond
in parallel, not sequentially as would occur in a hierarchy. This and other
evidence suggests that two regions can act in parallel and hierarchically at
the same time. Given this flexibility, the word "heterarchy" might be a more
suitable term to describe neocortical organization. This paper proposes a new
interpretation of how sensory and motor information is processed in the
neocortex. The key to our proposal is what we call the "Thousand Brains
Theory", which posits that every cortical column is a sensorimotor learning
system. Columns learn by integrating sensory input over multiple movements of a
sensor. In this view, even primary and secondary regions, such as V1 and V2,
can learn and recognize complete 3D objects. This suggests that the
hierarchical connections between regions are used to learn the compositional
structure of parent objects composed of smaller child objects. We explain the
theory by examining the different types of long-range connections between
cortical regions and between the neocortex and thalamus. We describe these
connections, and then suggest the specific roles they play in the context of a
heterarchy of sensorimotor regions. We also suggest that the thalamus plays an
essential role in transforming the pose between objects and sensors. The novel
perspective we argue for here has broad implications for both neuroscience and
artificial intelligence.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [189] [ADEPT: A Noninvasive Method for Determining Elastic Parameters of Valve Tissue](https://arxiv.org/abs/2409.19081)
*Wensi Wu,Mitchell Daneker,Christian Herz,Hannah Dewey,Jeffrey A. Weiss,Alison M. Pouch,Lu Lu,Matthew A. Jolley*

Main category: q-bio.QM

TL;DR: 提出非侵入性方法ADEPT确定瓣膜弹性参数，应用于儿童三尖瓣，模拟模型精度提升。


<details>
  <summary>Details</summary>
Motivation: 非侵入性确定瓣膜体内力学参数的方法匮乏，限制计算机预测准确性和临床应用。

Method: 先通过图像配准追踪3D超声心动图时间序列中瓣膜从开放到关闭帧的位移，再用物理信息神经网络从第一性原理和参考位移估计非线性力学特性。

Result: 使用患者特定参数的模拟模型与参考图像分割紧密对齐，平均对称距离小于1mm，相比文献通用参数，精度提高一倍。

Conclusion: ADEPT方法能提高计算机模拟瓣膜修复的准确性，有临床应用潜力。

Abstract: Computer simulation of "virtual interventions" may inform optimal valve
repair for a given patient prior to intervention. However, the paucity of
noninvasive methods to determine in vivo mechanical parameters of valves limits
the accuracy of computer prediction and their clinical application. To address
this, we propose ADEPT: A noninvasive method for Determining Elastic Parameters
of valve Tissue. In this work, we demonstrated its application to the tricuspid
valve of a child. We first tracked valve displacements from open to closed
frames within a 3D echocardiogram time sequence using image registration.
Physics-informed neural networks were subsequently applied to estimate the
nonlinear mechanical properties from first principles and reference
displacements. The simulated model using these patient-specific parameters
closely aligned with the reference image segmentation, achieving a mean
symmetric distance of less than 1 mm. Our approach doubled the accuracy of the
simulated model compared to the generic parameters reported in the literature.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [190] [GATMesh: Clock Mesh Timing Analysis using Graph Neural Networks](https://arxiv.org/abs/2507.05681)
*Muhammad Hadir Khan,Matthew Guthaus*

Main category: cs.AR

TL;DR: 提出基于图神经网络的GATMesh框架用于时钟网格分析，精度高且速度快。


<details>
  <summary>Details</summary>
Motivation: 传统时钟网格分析方法存在问题，SPICE模拟慢，简化模型遗漏关键效应。

Method: 将时钟网格建模为具有增强结构和物理特征的图，用SPICE数据训练。

Result: 在未见基准测试中平均延迟误差5.27ps，比多线程SPICE模拟提速47146倍。

Conclusion: GATMesh在时钟网格分析中能实现高精度和高速度。

Abstract: Clock meshes are essential in high-performance VLSI systems for minimizing
skew and handling PVT variations, but analyzing them is difficult due to
reconvergent paths, multi-source driving, and input mesh buffer skew. SPICE
simulations are accurate but slow; yet simplified models miss key effects like
slew and input skew. We propose GATMesh, a Graph Neural Network (GNN)-based
framework that models the clock mesh as a graph with augmented structural and
physical features. Trained on SPICE data, GATMesh achieves high accuracy with
average delay error of 5.27ps on unseen benchmarks, while achieving speed-ups
of 47146x over multi-threaded SPICE simulation.

</details>


### [191] [PrefixAgent: An LLM-Powered Design Framework for Efficient Prefix Adder Optimization](https://arxiv.org/abs/2507.06127)
*Dongsheng Zuo,Jiadong Zhu,Yang Luo,Yuzhe Ma*

Main category: cs.AR

TL;DR: 本文提出PrefixAgent框架优化前缀加法器，能有效缩小搜索空间，实验显示其综合的前缀加法器面积更小，且有可扩展性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 前缀加法器设计空间随位宽指数增长，先前工作在性能、泛化性和可扩展性上有局限。

Method: 提出PrefixAgent框架，将问题分解为骨干合成和结构细化子任务，用E - graph高效收集数据和推理轨迹以微调大语言模型。

Result: PrefixAgent综合的前缀加法器比基线方法面积更小，在商业EDA流程中有可扩展性和泛化性。

Conclusion: PrefixAgent框架能有效优化前缀加法器，解决了先前工作的局限。

Abstract: Prefix adders are fundamental arithmetic circuits, but their design space
grows exponentially with bit-width, posing significant optimization challenges.
Previous works face limitations in performance, generalization, and
scalability. To address these challenges, we propose PrefixAgent, a large
language model (LLM)-powered framework that enables efficient prefix adder
optimization. Specifically, PrefixAgent reformulates the problem into subtasks
including backbone synthesis and structure refinement, which effectively
reduces the search space. More importantly, this new design perspective enables
us to efficiently collect enormous high-quality data and reasoning traces with
E-graph, which further results in an effective fine-tuning of LLM. Experimental
results show that PrefixAgent synthesizes prefix adders with consistently
smaller areas compared to baseline methods, while maintaining scalability and
generalization in commercial EDA flows.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [192] [On the Inherent Privacy of Zeroth Order Projected Gradient Descent](https://arxiv.org/abs/2507.05610)
*Devansh Gupta,Meisam Razaviyayn,Vatsal Sharan*

Main category: math.OC

TL;DR: 研究基于oracle的零阶优化算法的差分隐私性，发现ZO - GD在特定情况下不具差分隐私，隐私损失可能超线性增长。


<details>
  <summary>Details</summary>
Motivation: 零阶优化方法的固有噪声是否足以保证算法整体的差分隐私性存在疑问，本文旨在解决该问题。

Method: 针对一类基于oracle返回零阶梯度估计的优化算法进行分析。

Result: 对于固定初始化，存在强凸目标函数使（投影）零阶梯度下降（ZO - GD）不具差分隐私；即使随机初始化且不暴露迭代点，最小化凸目标函数时ZO - GD的隐私损失会随迭代次数超线性增长。

Conclusion: 零阶优化算法的固有噪声不足以保证整体的差分隐私。

Abstract: Differentially private zeroth-order optimization methods have recently gained
popularity in private fine tuning of machine learning models due to their
reduced memory requirements. Current approaches for privatizing zeroth-order
methods rely on adding Gaussian noise to the estimated zeroth-order gradients.
However, since the search direction in the zeroth-order methods is inherently
random, researchers including Tang et al. (2024) and Zhang et al. (2024a) have
raised an important question: is the inherent noise in zeroth-order estimators
sufficient to ensure the overall differential privacy of the algorithm? This
work settles this question for a class of oracle-based optimization algorithms
where the oracle returns zeroth-order gradient estimates. In particular, we
show that for a fixed initialization, there exist strongly convex objective
functions such that running (Projected) Zeroth-Order Gradient Descent (ZO-GD)
is not differentially private. Furthermore, we show that even with random
initialization and without revealing (initial and) intermediate iterates, the
privacy loss in ZO-GD can grow superlinearly with the number of iterations when
minimizing convex objective functions.

</details>


### [193] [Exact and efficient basis pursuit denoising via differential inclusions and a selection principle](https://arxiv.org/abs/2507.05562)
*Gabriel P. Langlois,Jérôme Darbon*

Main category: math.OC

TL;DR: 本文提出一种基于微分包含的精确高效的基追踪去噪（BPDN）算法，数值实验表明该算法在准确性和效率上优于现有算法，还可衍生出同伦算法和贪心算法，成果有望应用于更广泛的多面体约束优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有BPDN算法存在效率与精度不能兼顾的问题，在高维应用中效果不佳。

Method: 利用微分包含理论中的选择原则将BPDN对偶问题转化为计算可积投影动力系统的轨迹。

Result: 提出精确到机器精度的算法，数值实验显示在准确性和效率上优于现有算法，还得到同伦算法和贪心算法。

Conclusion: 该算法有效解决了BPDN问题，成果有望应用于更广泛的多面体约束优化问题。

Abstract: Basis pursuit denoising (BPDN) is a cornerstone of compressive sensing,
statistics and machine learning. While various algorithms for BPDN have been
proposed, they invariably suffer from drawbacks and must either favor
efficiency at the expense of accuracy or vice versa. As such, state-of-the-art
algorithms remain ineffective for high-dimensional applications that require
accurate solutions within a reasonable amount of computational time. In this
work, we address this issue and propose an exact and efficient algorithm for
BPDN using differential inclusions. Specifically, we prove that a selection
principle from the theory of differential inclusions turns the dual problem of
BPDN into calculating the trajectory of an \emph{integrable} projected
dynamical system, that is, whose trajectory and asymptotic limit can be
computed exactly. Our analysis naturally yields an exact algorithm, numerically
up to machine precision, that is amenable to computing regularization paths and
very fast. Numerical experiments confirm that our algorithm outperforms the
state-of-the-art algorithms in both accuracy and efficiency. Moreover, we show
that the global continuation of solutions (in terms of the hyperparameter and
data) of the projected dynamical system yields a rigorous homotopy algorithm
for BPDN, as well as a novel greedy algorithm for computing feasible solutions
to basis pursuit in strongly polynomial time. Beyond this work, we expect that
our results and analysis can be adapted to compute exact or approximate
solutions to a broader class of polyhedral-constrained optimization problems.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [194] [Permutations accelerate Approximate Bayesian Computation](https://arxiv.org/abs/2507.06037)
*Antoine Luciano,Charly Andral,Christian P. Robert,Robin J. Ryder*

Main category: stat.ME

TL;DR: 本文提出新的ABC框架permABC，结合两种策略实现高维模型高效推理，实验证明其准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有ABC方法在分层或高维模型中可扩展性是主要挑战，需新方法解决。

Method: 基于ABC - SMC框架，通过基于排列的匹配利用分区可交换性；开发过采样和欠匹配两种顺序策略。

Result: 通过合成和真实实验，证明方法在准确性和效率上有实际提升。

Conclusion: permABC框架结合两种策略可在高维情况下进行稳健且可扩展的推理。

Abstract: Approximate Bayesian Computation (ABC) methods have become essential tools
for performing inference when likelihood functions are intractable or
computationally prohibitive. However, their scalability remains a major
challenge in hierarchical or high-dimensional models. In this paper, we
introduce permABC, a new ABC framework designed for settings with both global
and local parameters, where observations are grouped into exchangeable
compartments.
  Building upon the Sequential Monte Carlo ABC (ABC-SMC) framework, permABC
exploits the exchangeability of compartments through permutation-based
matching, significantly improving computational efficiency.
  We then develop two further, complementary sequential strategies: Over
Sampling, which facilitates early-stage acceptance by temporarily increasing
the number of simulated compartments, and Under Matching, which relaxes the
acceptance condition by matching only subsets of the data.
  These techniques allow for robust and scalable inference even in
high-dimensional regimes. Through synthetic and real-world experiments --
including a hierarchical Susceptible-Infectious-Recover model of the early
COVID-19 epidemic across 94 French departments -- we demonstrate the practical
gains in accuracy and efficiency achieved by our approach.

</details>


### [195] [seMCD: Sequentially implemented Monte Carlo depth computation with statistical guarantees](https://arxiv.org/abs/2507.06227)
*Felix Gnettner,Claudia Kirch,Alicia Nieto-Reyes*

Main category: stat.ME

TL;DR: 提出新的顺序蒙特卡罗方法seMCD计算深度函数，样本需求少，有统计保证，适用多场景。


<details>
  <summary>Details</summary>
Motivation: 统计深度函数数值评估计算成本高，需新计算方法。

Method: 提出seMCD方法，针对特定深度函数用顺序测试算法，对依赖未知分布的用非参数统计方法。

Result: seMCD方法样本需求随机且通常远少于文献标准，适用于多种深度函数，经实证研究证明有效可靠。

Conclusion: seMCD算法用少量蒙特卡罗样本可准确近似深度，并保持严格统计保证。

Abstract: Statistical depth functions provide center-outward orderings in spaces of
dimension larger than one, where a natural ordering does not exist. The
numerical evaluation of such depth functions can be computationally
prohibitive, even for relatively low dimensions. We present a novel
sequentially implemented Monte Carlo methodology for the computation of,
theoretical and empirical, depth functions and related quantities (seMCD), that
outputs an interval, a so-called seMCD-bucket, to which the quantity of
interest belongs with a high probability prespecified by the user. For specific
classes of depth functions, we adapt algorithms from sequential testing,
providing finite-sample guarantees. For depth functions dependent on unknown
distributions, we offer asymptotic guarantees using non-parametric statistical
methods. In contrast to plain-vanilla Monte Carlo methodology the number of
samples required in the algorithm is random but typically much smaller than
standard choices suggested in the literature. The seMCD method can be applied
to various depth functions, covering multivariate and functional spaces. We
demonstrate the efficiency and reliability of our approach through empirical
studies, highlighting its applicability in outlier or anomaly detection,
classification, and depth region computation. In conclusion, the
seMCD-algorithm can achieve accurate depth approximations with few Monte Carlo
samples while maintaining rigorous statistical guarantees.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [196] [An efficiency ordering of k-price auctions under complete information](https://arxiv.org/abs/2507.05738)
*Sumit Goel,Jeffrey Zeidel*

Main category: econ.TH

TL;DR: 研究完全信息环境下的k - 价格拍卖，刻画纯策略纳什均衡结果，表明特定情况下各参与者获胜情况及不同k值下的最坏情况福利变化。


<details>
  <summary>Details</summary>
Motivation: 研究完全信息环境下k - 价格拍卖的纯策略纳什均衡结果和福利情况。

Method: 在n个有排序估值的参与者的设定下进行理论分析。

Result: 除估值最低的k - 2个参与者外，其他参与者都可能在均衡中获胜；从k = 2到k = n，最坏情况福利单调增加，第一价格拍卖实现最高最坏情况福利。

Conclusion: 对k - 价格拍卖的均衡结果和福利有了清晰的刻画和结论。

Abstract: We study $k$-price auctions in a complete information environment and
characterize all pure-strategy Nash equilibrium outcomes. In a setting with $n$
agents having ordered valuations, we show that any agent, except those with the
lowest $k-2$ valuations, can win in equilibrium. As a consequence, worst-case
welfare increases monotonically as we go from $k=2$ (second-price auction) to
$k=n$ (lowest-price auction), with the first-price auction achieving the
highest worst-case welfare.

</details>


### [197] [A Directed Lazy Random Walk Model to Three-Way Dynamic Matching Problem](https://arxiv.org/abs/2507.06126)
*Souvik Roy,Agamani Saha*

Main category: econ.TH

TL;DR: 本文拓展动态匹配理论，研究三方匹配问题，分析不同偏好结构，构建转移概率矩阵，证明平稳分布存在唯一，揭示复杂匹配过程可产生稳定高效结果。


<details>
  <summary>Details</summary>
Motivation: 传统静态或双向动态模型无法满足复杂团队形成环境需求，需拓展动态匹配理论研究三方匹配问题。

Method: 考虑两种偏好结构，构建并分析对应转移概率矩阵。

Result: 证明了平稳分布的存在性和唯一性。

Conclusion: 动态多智能体匹配环境可产生稳定高效结果，有助于理解和管理复杂匹配过程。

Abstract: This paper explores a novel extension of dynamic matching theory by analyzing
a three-way matching problem involving agents from three distinct populations,
each with two possible types. Unlike traditional static or two-way dynamic
models, our setting captures more complex team-formation environments where one
agent from each of the three populations must be matched to form a valid team.
We consider two preference structures: assortative or homophilic, where agents
prefer to be matched with others of the same type, and dis-assortative or
heterophilic, where diversity within the team is valued. Agents arrive
sequentially and face a trade-off between matching immediately or waiting for a
higher quality match in the future albeit with a waiting cost. We construct and
analyze the corresponding transition probability matrices for each preference
regime and demonstrate the existence and uniqueness of stationary
distributions. Our results show that stable and efficient outcomes can arise in
dynamic, multi-agent matching environments, offering a deeper understanding of
how complex matching processes evolve over time and how they can be effectively
managed.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [198] [Adaptive Communication Through Exploiting RIS, SSK, and CIM for Improved Reliability and Efficiency](https://arxiv.org/abs/2507.05813)
*Ferhat Bayar,Onur Salan,Erdogan Aydin,Haci Ilhan*

Main category: cs.IT

TL;DR: 提出基于RIS的发射SSK - CIM通信系统模型（RIS - CIM - TSSK），分析其优势并给出模拟结果。


<details>
  <summary>Details</summary>
Motivation: 为现代无线网络提供高效、可靠、适应性强的通信解决方案，应对动态环境。

Method: 提出RIS - CIM - TSSK模型，使用亚最优低复杂度检测器，处理RIS相位调整的盲情况。

Result: 给出了系统模型在不同配置（收发天线数量、RIS反射元件数量、码长）下的模拟结果。

Conclusion: 该方案提高能源效率，为现代无线网络可靠通信提供稳健解决方案，有助于实现更智能、适应性更强的通信。

Abstract: In this paper, we present a novel communication system model that integrates
reconfigurable intelligent surfaces (RIS), spatial shift keying (SSK), and code
index modulation (CIM) based on Hadamard coding called RIS based transmit
SSK-CIM (RIS-CIM-TSSK). By leveraging RIS, the system adapts rapidly to dynamic
environments, enhancing error rates and overall reliability. SSK facilitates
the transmission of additional passive information while eliminating the need
for multiple radio frequency (RF) chains, thereby reducing complexity. CIM
enhances passive information transmission through frequency domain spreading,
which may increase signal obfuscation. This proposed scheme not only improves
energy efficiency but also offers a robust solution for reliable communication
in modern wireless networks, paving the way for smarter and more adaptable
implementations. We consider a suboptimal, low-complexity detector for the
proposed scheme and also address the blind case for phase adjustment of the
RIS. Finally, we present the simulation results for the proposed system model
across various configurations, including different numbers of receive and
transmit antennas, varying reflecting elements of the RIS, and different code
lengths.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [199] [The Impact of Event Data Partitioning on Privacy-aware Process Discovery](https://arxiv.org/abs/2507.06008)
*Jungeun Lim,Stephan A. Fahrenkrog-Petersen,Xixi Lu,Jan Mendling,Minseok Song*

Main category: cs.CR

TL;DR: 本文提出结合匿名化和事件数据分区的管道，利用事件抽象分区，用真实日志验证，结果表明分区对基于直接跟随的匿名技术在过程发现效用上有改善。


<details>
  <summary>Details</summary>
Motivation: 信息系统执行日志含敏感信息，匿名化日志解决隐私问题时难以平衡效用和隐私，高复杂度日志匿名化效用损失大。

Method: 提出结合匿名化和事件数据分区的管道，利用事件抽象进行分区，让各子日志单独匿名化；用三个真实事件日志和两种过程发现技术研究事件分区对两种匿名技术的影响。

Result: 事件分区能为基于直接跟随的匿名化技术带来过程发现效用的提升。

Conclusion: 所提出的管道能在保护隐私的同时减轻效用损失。

Abstract: Information systems support the execution of business processes. The event
logs of these executions generally contain sensitive information about
customers, patients, and employees. The corresponding privacy challenges can be
addressed by anonymizing the event logs while still retaining utility for
process discovery. However, trading off utility and privacy is difficult: the
higher the complexity of event log, the higher the loss of utility by
anonymization. In this work, we propose a pipeline that combines anonymization
and event data partitioning, where event abstraction is utilized for
partitioning. By leveraging event abstraction, event logs can be segmented into
multiple parts, allowing each sub-log to be anonymized separately. This
pipeline preserves privacy while mitigating the loss of utility. To validate
our approach, we study the impact of event partitioning on two anonymization
techniques using three real-world event logs and two process discovery
techniques. Our results demonstrate that event partitioning can bring
improvements in process discovery utility for directly-follows-based
anonymization techniques.

</details>


### [200] [A Survey on Model Extraction Attacks and Defenses for Large Language Models](https://arxiv.org/abs/2506.22521)
*Kaixiang Zhao,Lincan Li,Kaize Ding,Neil Zhenqiang Gong,Yue Zhao,Yushun Dong*

Main category: cs.CR

TL;DR: 本文对大语言模型的提取攻击和防御进行全面分类，分析攻击和防御方法，提出评估指标，指出当前方法局限并给出研究方向。


<details>
  <summary>Details</summary>
Motivation: 模型提取攻击对部署的语言模型构成安全威胁，可能损害知识产权和用户隐私，需要进行全面研究。

Method: 对攻击进行分类，分析多种攻击方法，研究防御机制并按不同策略组织，提出评估攻击和防御的指标。

Result: 识别出当前方法的关键局限。

Conclusion: 提出有前景的研究方向，如集成攻击方法和自适应防御机制，平衡安全和模型实用性，为相关人员提供保护语言模型的参考。

Abstract: Model extraction attacks pose significant security threats to deployed
language models, potentially compromising intellectual property and user
privacy. This survey provides a comprehensive taxonomy of LLM-specific
extraction attacks and defenses, categorizing attacks into functionality
extraction, training data extraction, and prompt-targeted attacks. We analyze
various attack methodologies including API-based knowledge distillation, direct
querying, parameter recovery, and prompt stealing techniques that exploit
transformer architectures. We then examine defense mechanisms organized into
model protection, data privacy protection, and prompt-targeted strategies,
evaluating their effectiveness across different deployment scenarios. We
propose specialized metrics for evaluating both attack effectiveness and
defense performance, addressing the specific challenges of generative language
models. Through our analysis, we identify critical limitations in current
approaches and propose promising research directions, including integrated
attack methodologies and adaptive defense mechanisms that balance security with
model utility. This work serves NLP researchers, ML engineers, and security
professionals seeking to protect language models in production environments.

</details>


### [201] [Disappearing Ink: Obfuscation Breaks N-gram Code Watermarks in Theory and Practice](https://arxiv.org/abs/2507.05512)
*Gehao Zhang,Eugene Bagdasarian,Juan Zhai,Shiqing Ma*

Main category: cs.CR

TL;DR: 本文指出N-gram代码水印方案鲁棒性评估不足，通过建模证明其在代码混淆下的不鲁棒性，实验验证水印检测器在混淆代码上检测能力差，并提出了鲁棒代码水印的潜在路径。


<details>
  <summary>Details</summary>
Motivation: 区分AI生成代码和人类编写代码至关重要，N-gram代码水印方案鲁棒性评估不足，尤其是针对代码混淆变换的鲁棒性未充分探索。

Method: 形式化建模代码混淆，在多个SOTA水印方案、LLM、编程语言、代码基准和混淆器上进行实验。

Result: 证明在满足分布一致性假设下N-gram水印不具鲁棒性，水印检测器在混淆代码上检测能力如抛硬币（AUROC接近0.5），所有模型、方案和数据集下都有混淆器使攻击后检测AUROC不高于0.6。

Conclusion: 基于理论和实践观察，提出了鲁棒代码水印的潜在路径。

Abstract: Distinguishing AI-generated code from human-written code is becoming crucial
for tasks such as authorship attribution, content tracking, and misuse
detection. Based on this, N-gram-based watermarking schemes have emerged as
prominent, which inject secret watermarks to be detected during the generation.
  However, their robustness in code content remains insufficiently evaluated.
Most claims rely solely on defenses against simple code transformations or code
optimizations as a simulation of attack, creating a questionable sense of
robustness. In contrast, more sophisticated schemes already exist in the
software engineering world, e.g., code obfuscation, which significantly alters
code while preserving functionality. Although obfuscation is commonly used to
protect intellectual property or evade software scanners, the robustness of
code watermarking techniques against such transformations remains largely
unexplored.
  In this work, we formally model the code obfuscation and prove the
impossibility of N-gram-based watermarking's robustness with only one intuitive
and experimentally verified assumption, distribution consistency, satisfied.
Given the original false positive rate of the watermarking detection, the ratio
that the detector failed on the watermarked code after obfuscation will
increase to 1 - fpr.
  The experiments have been performed on three SOTA watermarking schemes, two
LLMs, two programming languages, four code benchmarks, and four obfuscators.
Among them, all watermarking detectors show coin-flipping detection abilities
on obfuscated codes (AUROC tightly surrounds 0.5). Among all models,
watermarking schemes, and datasets, both programming languages own obfuscators
that can achieve attack effects with no detection AUROC higher than 0.6 after
the attack. Based on the theoretical and practical observations, we also
proposed a potential path of robust code watermarking.

</details>


### [202] [DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective](https://arxiv.org/abs/2507.05622)
*Shuo Shao,Yiming Li,Mengren Zheng,Zhiyang Hu,Yukun Chen,Boheng Li,Yu He,Junfeng Guo,Tianwei Zhang,Dacheng Tao,Zhan Qin*

Main category: cs.CR

TL;DR: 本文从对抗视角评估数据集审计，提出新分类法、攻击类型和策略，构建DATABench基准，发现现有审计方法在对抗环境下不够稳健，强调需开发更安全可靠的审计方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集审计方法对对抗攻击的抵御能力研究不足，为填补这一空白开展研究。

Method: 引入新分类法对现有方法分类，提出逃避和伪造两种攻击类型及对应攻击策略，构建DATABench基准。

Result: 使用DATABench评估发现，现有审计方法在对抗环境下不够稳健和独特。

Conclusion: 迫切需要开发能抵御复杂对抗操纵的更安全可靠的数据集审计方法。

Abstract: The widespread application of Deep Learning across diverse domains hinges
critically on the quality and composition of training datasets. However, the
common lack of disclosure regarding their usage raises significant privacy and
copyright concerns. Dataset auditing techniques, which aim to determine if a
specific dataset was used to train a given suspicious model, provide promising
solutions to addressing these transparency gaps. While prior work has developed
various auditing methods, their resilience against dedicated adversarial
attacks remains largely unexplored. To bridge the gap, this paper initiates a
comprehensive study evaluating dataset auditing from an adversarial
perspective. We start with introducing a novel taxonomy, classifying existing
methods based on their reliance on internal features (IF) (inherent to the
data) versus external features (EF) (artificially introduced for auditing).
Subsequently, we formulate two primary attack types: evasion attacks, designed
to conceal the use of a dataset, and forgery attacks, intending to falsely
implicate an unused dataset. Building on the understanding of existing methods
and attack objectives, we further propose systematic attack strategies:
decoupling, removal, and detection for evasion; adversarial example-based
methods for forgery. These formulations and strategies lead to our new
benchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9
representative auditing methods. Extensive evaluations using DATABench reveal
that none of the evaluated auditing methods are sufficiently robust or
distinctive under adversarial settings. These findings underscore the urgent
need for developing a more secure and reliable dataset auditing method capable
of withstanding sophisticated adversarial manipulation. Code is available at
https://github.com/shaoshuo-ss/DATABench.

</details>


### [203] [How Not to Detect Prompt Injections with an LLM](https://arxiv.org/abs/2507.05630)
*Sarthak Choudhary,Divyam Anshumaan,Nils Palumbo,Somesh Jha*

Main category: cs.CR

TL;DR: 文章指出LLM集成应用和代理易受提示注入攻击，现有基于KAD的防御有漏洞，提出DataFlip攻击方法可有效规避防御。


<details>
  <summary>Details</summary>
Motivation: 发现基于已知答案检测（KAD）的防御框架存在结构漏洞，核心安全前提失效，需要设计攻击方法来验证。

Method: 设计了系统性的自适应攻击方法DataFlip。

Result: DataFlip能以低至1.5%的检测率规避KAD防御，以高达88%的成功率诱导恶意行为，无需白盒访问LLM和优化程序。

Conclusion: KAD防御框架存在结构漏洞，DataFlip攻击方法可有效利用该漏洞规避防御。

Abstract: LLM-integrated applications and agents are vulnerable to prompt injection
attacks, in which adversaries embed malicious instructions within seemingly
benign user inputs to manipulate the LLM's intended behavior. Recent defenses
based on $\textit{known-answer detection}$ (KAD) have achieved near-perfect
performance by using an LLM to classify inputs as clean or contaminated. In
this work, we formally characterize the KAD framework and uncover a structural
vulnerability in its design that invalidates its core security premise. We
design a methodical adaptive attack, $\textit{DataFlip}$, to exploit this
fundamental weakness. It consistently evades KAD defenses with detection rates
as low as $1.5\%$ while reliably inducing malicious behavior with success rates
of up to $88\%$, without needing white-box access to the LLM or any
optimization procedures.

</details>


### [204] [DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning](https://arxiv.org/abs/2507.05649)
*Kaixiang Zhao,Joseph Yousry Attalla,Qian Lou,Yushun Dong*

Main category: cs.CR

TL;DR: 本文提出DESIGN框架用于高效加密GNN推理，通过服务器端分层优化策略加速FHE GNN推理，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有在加密域实现隐私保护GNN的方法计算开销大，实时隐私保护推理不可行，且现有FHE GNN方法忽视输入数据冗余和采用统一计算策略。

Method: 提出DESIGN框架，在服务器端执行分层优化策略，先从加密图计算FHE兼容的节点重要性分数，再进行同态分区生成多级重要性掩码，利用掩码进行输入图剪枝和自适应多项式激活。

Result: DESIGN相比现有方法显著加速FHE GNN推理，同时保持有竞争力的模型精度。

Conclusion: DESIGN为安全图分析提供了一个可靠的解决方案。

Abstract: Graph Neural Networks (GNNs) have achieved state-of-the-art performance in
various graph-based learning tasks. However, enabling privacy-preserving GNNs
in encrypted domains, such as under Fully Homomorphic Encryption (FHE),
typically incurs substantial computational overhead, rendering real-time and
privacy-preserving inference impractical. In this work, we propose DESIGN
(EncrypteD GNN Inference via sErver-Side Input Graph pruNing), a novel
framework for efficient encrypted GNN inference. DESIGN tackles the critical
efficiency limitations of existing FHE GNN approaches, which often overlook
input data redundancy and apply uniform computational strategies. Our framework
achieves significant performance gains through a hierarchical optimization
strategy executed entirely on the server: first, FHE-compatible node importance
scores (based on encrypted degree statistics) are computed from the encrypted
graph. These scores then guide a homomorphic partitioning process, generating
multi-level importance masks directly under FHE. This dynamically generated
mask facilitates both input graph pruning (by logically removing unimportant
elements) and a novel adaptive polynomial activation scheme, where activation
complexity is tailored to node importance levels. Empirical evaluations
demonstrate that DESIGN substantially accelerates FHE GNN inference compared to
state-of-the-art methods while maintaining competitive model accuracy,
presenting a robust solution for secure graph analytics.

</details>


### [205] [AI Agent Smart Contract Exploit Generation](https://arxiv.org/abs/2507.05558)
*Arthur Gervais,Liyi Zhou*

Main category: cs.CR

TL;DR: 提出A1系统将LLM转变为端到端漏洞利用生成器，评估显示有一定成功率和收益，分析迭代性能与成本，还探讨其用于攻防的盈利情况。


<details>
  <summary>Details</summary>
Motivation: 开发能将任意LLM转变为端到端漏洞利用生成器的系统，并研究其在攻防场景中的应用价值。

Method: 构建A1系统，提供六个特定领域工具，通过在以太坊和币安智能链上对36个真实漏洞合约评估，进行432次跨六个LLM实验、蒙特卡罗分析等。

Result: 在VERITE基准上成功率62.96%，识别9个额外漏洞合约，有一定收益；迭代性能边际收益递减；不同模型在不同漏洞发生率和扫描延迟下盈利情况不同。

Conclusion: AI代理在当前情况下可能更有利于攻击而非防御，引发关于AI是否必然倾向于利用而非防御的思考。

Abstract: We present A1, an agentic execution driven system that transforms any LLM
into an end-to-end exploit generator. A1 has no hand-crafted heuristics and
provides the agent with six domain-specific tools that enable autonomous
vulnerability discovery. The agent can flexibly leverage these tools to
understand smart contract behavior, generate exploit strategies, test them on
blockchain states, and refine approaches based on execution feedback. All
outputs are concretely validated to eliminate false positives.
  The evaluation across 36 real-world vulnerable contracts on Ethereum and
Binance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the
VERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional
vulnerable contracts, with 5 cases occurring after the strongest model's
training cutoff date. Across all 26 successful cases, A1 extracts up to 8.59
million USD per case and 9.33 million USD total. Through 432 experiments across
six LLMs, we analyze iteration-wise performance showing diminishing returns
with average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations
2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo
analysis of 19 historical attacks shows success probabilities of 85.9%-88.8%
without detection delays.
  We investigate whether an attacker or a defender benefits most from deploying
A1 as a continuous on-chain scanning system. Our model shows that OpenAI's
o3-pro maintains profitability up to a 30.0 days scanning delay at 0.100%
vulnerability incidence rates, while faster models require >=1.000% rates to
break-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability
rates, attackers achieve an on-chain scanning profitability at a $6000 exploit
value, while defenders require $60000, raising fundamental questions about
whether AI agents inevitably favor exploitation over defense.

</details>


### [206] [TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data](https://arxiv.org/abs/2507.05660)
*Aravind Cheruvu,Shravya Kanchi,Sifat Muhammad Abdullah,Nicholas Kong,Daphne Yao,Murtuza Jadliwala,Bimal Viswanath*

Main category: cs.CR

TL;DR: 提出TuneShield框架应对聊天机器人微调时的毒性问题，能有效缓解毒性攻击并保持对话质量。


<details>
  <summary>Details</summary>
Motivation: 解决定制大语言模型开发聊天机器人时，处理不可信训练数据的毒性缓解难题。

Method: 利用大语言模型进行毒性分类识别有毒样本，生成合成对话样本“修复数据”，并进行对齐过程。

Result: TuneShield能有效缓解毒性注入攻击，对自适应对抗和越狱攻击有弹性，在基于对话的学习中也有效。

Conclusion: TuneShield可在毒性分类器不完美或有偏差时，有效缓解毒性并保持对话质量。

Abstract: Recent advances in foundation models, such as LLMs, have revolutionized
conversational AI. Chatbots are increasingly being developed by customizing
LLMs on specific conversational datasets. However, mitigating toxicity during
this customization, especially when dealing with untrusted training data,
remains a significant challenge. To address this, we introduce TuneShield, a
defense framework designed to mitigate toxicity during chatbot fine-tuning
while preserving conversational quality. TuneShield leverages LLM-based
toxicity classification, utilizing the instruction-following capabilities and
safety alignment of LLMs to effectively identify toxic samples, outperforming
industry API services. TuneShield generates synthetic conversation samples,
termed 'healing data', based on the identified toxic samples, using them to
mitigate toxicity while reinforcing desirable behavior during fine-tuning. It
performs an alignment process to further nudge the chatbot towards producing
desired responses. Our findings show that TuneShield effectively mitigates
toxicity injection attacks while preserving conversational quality, even when
the toxicity classifiers are imperfect or biased. TuneShield proves to be
resilient against adaptive adversarial and jailbreak attacks. Additionally,
TuneShield demonstrates effectiveness in mitigating adaptive toxicity injection
attacks during dialog-based learning (DBL).

</details>


### [207] [Automated Reasoning for Vulnerability Management by Design](https://arxiv.org/abs/2507.05794)
*Avi Shaked,Nan Messe*

Main category: cs.CR

TL;DR: 为安全管理系统漏洞态势，提出形式化自动推理机制并集成到工具中展示应用。


<details>
  <summary>Details</summary>
Motivation: 当前漏洞管理方法不支持对系统设计的漏洞态势进行系统推理，需有效管理漏洞和设计安全控制。

Method: 提出形式化自动推理机制，并集成到开源安全设计工具中。

Result: 自动推理机制可让系统设计师识别特定系统设计的漏洞，明确指定缓解选项，声明所选控制，系统管理漏洞态势。

Conclusion: 所提出的自动推理机制能帮助系统设计师更好地管理系统的漏洞态势。

Abstract: For securing systems, it is essential to manage their vulnerability posture
and design appropriate security controls. Vulnerability management allows to
proactively address vulnerabilities by incorporating pertinent security
controls into systems designs. Current vulnerability management approaches do
not support systematic reasoning about the vulnerability postures of systems
designs. To effectively manage vulnerabilities and design security controls, we
propose a formally grounded automated reasoning mechanism. We integrate the
mechanism into an open-source security design tool and demonstrate its
application through an illustrative example driven by real-world challenges.
The automated reasoning mechanism allows system designers to identify
vulnerabilities that are applicable to a specific system design, explicitly
specify vulnerability mitigation options, declare selected controls, and thus
systematically manage vulnerability postures.

</details>


### [208] [Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI](https://arxiv.org/abs/2507.06092)
*Shravya Kanchi,Neal Mangaokar,Aravind Cheruvu,Sifat Muhammad Abdullah,Shirin Nilizadeh,Atul Prakash,Bimal Viswanath*

Main category: cs.CR

TL;DR: 研究探讨用生成式AI解决安全分类器数据挑战，通过合成数据提升性能，有成效但部分任务有局限。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的安全分类器研究多关注算法改进，数据挑战受关注少，探究生成式AI能否解决数据挑战并提升分类器性能。

Method: 用6种先进生成式AI方法对7个安全任务的训练数据集进行合成数据增强，并提出新方案Nimai。

Result: 生成式AI能显著提升安全分类器性能，在数据受限场景提升达32.6%，可助力应对概念漂移，但部分方案在特定任务初始化困难。

Conclusion: 研究将推动未来用于安全任务的生成式AI工具发展。

Abstract: Machine learning-based supervised classifiers are widely used for security
tasks, and their improvement has been largely focused on algorithmic
advancements. We argue that data challenges that negatively impact the
performance of these classifiers have received limited attention. We address
the following research question: Can developments in Generative AI (GenAI)
address these data challenges and improve classifier performance? We propose
augmenting training datasets with synthetic data generated using GenAI
techniques to improve classifier generalization. We evaluate this approach
across 7 diverse security tasks using 6 state-of-the-art GenAI methods and
introduce a novel GenAI scheme called Nimai that enables highly controlled data
synthesis. We find that GenAI techniques can significantly improve the
performance of security classifiers, achieving improvements of up to 32.6% even
in severely data-constrained settings (only ~180 training samples).
Furthermore, we demonstrate that GenAI can facilitate rapid adaptation to
concept drift post-deployment, requiring minimal labeling in the adjustment
process. Despite successes, our study finds that some GenAI schemes struggle to
initialize (train and produce data) on certain security tasks. We also identify
characteristics of specific tasks, such as noisy labels, overlapping class
distributions, and sparse feature vectors, which hinder performance boost using
GenAI. We believe that our study will drive the development of future GenAI
tools designed for security tasks.

</details>


### [209] [CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations](https://arxiv.org/abs/2507.06043)
*Xiaohu Li,Yunfeng Ning,Zepeng Bao,Mayi Xu,Jianhao Chen,Tieyun Qian*

Main category: cs.CR

TL;DR: 提出结合攻击与防御的框架，基于LLM中间层嵌入特性和GAN实现高效攻击防御，实验验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐机制有漏洞，且以往研究将攻击和防御分离。

Method: 基于LLM中间层嵌入线性可分特性和越狱攻击本质，利用GAN学习LLM内部安全判断边界。

Result: 在三个流行LLM上平均越狱成功率88.85%，在最新越狱数据集上防御成功率平均84.17%。

Conclusion: 方法有效，为增强模型安全提供新见解。

Abstract: Security alignment enables the Large Language Model (LLM) to gain the
protection against malicious queries, but various jailbreak attack methods
reveal the vulnerability of this security mechanism. Previous studies have
isolated LLM jailbreak attacks and defenses. We analyze the security protection
mechanism of the LLM, and propose a framework that combines attack and defense.
Our method is based on the linearly separable property of LLM intermediate
layer embedding, as well as the essence of jailbreak attack, which aims to
embed harmful problems and transfer them to the safe area. We utilize
generative adversarial network (GAN) to learn the security judgment boundary
inside the LLM to achieve efficient jailbreak attack and defense. The
experimental results indicate that our method achieves an average jailbreak
success rate of 88.85\% across three popular LLMs, while the defense success
rate on the state-of-the-art jailbreak dataset reaches an average of 84.17\%.
This not only validates the effectiveness of our approach but also sheds light
on the internal security mechanisms of LLMs, offering new insights for
enhancing model security The code and data are available at
https://github.com/NLPGM/CAVGAN.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [210] [SoftReMish: A Novel Activation Function for Enhanced Convolutional Neural Networks for Visual Recognition Performance](https://arxiv.org/abs/2507.06148)
*Mustafa Bayram Gücen*

Main category: cs.CV

TL;DR: 提出新激活函数SoftReMish用于图像分类，实验显示其性能优于其他激活函数，有良好收敛和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 提高卷积神经网络在图像分类任务中的性能。

Method: 使用MNIST数据集，搭建标准CNN架构，用SoftReMish替换所有可训练层的激活函数，并与ReLU、Tanh和Mish对比，以最小训练损失和最大验证准确率评估模型。

Result: SoftReMish达到最小损失3.14e - 8和验证准确率99.41%，优于其他测试函数。

Conclusion: SoftReMish具有更好的收敛行为和泛化能力，是视觉识别任务的有前景候选者。

Abstract: In this study, SoftReMish, a new activation function designed to improve the
performance of convolutional neural networks (CNNs) in image classification
tasks, is proposed. Using the MNIST dataset, a standard CNN architecture
consisting of two convolutional layers, max pooling, and fully connected layers
was implemented. SoftReMish was evaluated against popular activation functions
including ReLU, Tanh, and Mish by replacing the activation function in all
trainable layers. The model performance was assessed in terms of minimum
training loss and maximum validation accuracy. Results showed that SoftReMish
achieved a minimum loss (3.14e-8) and a validation accuracy (99.41%),
outperforming all other functions tested. These findings demonstrate that
SoftReMish offers better convergence behavior and generalization capability,
making it a promising candidate for visual recognition tasks.

</details>


### [211] [Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot Multi-Species Plant Identification](https://arxiv.org/abs/2507.06093)
*Murilo Gustineli,Anthony Miyaguchi,Adrian Cheung,Divyansh Khattak*

Main category: cs.CV

TL;DR: 介绍DS@GT在PlantCLEF 2025挑战中获第二名的多物种植物识别解决方案，代码开源。


<details>
  <summary>Details</summary>
Motivation: 参与PlantCLEF 2025挑战，进行多物种植物识别。

Method: 结合微调的Vision Transformer ViTD2PC24All进行补丁级推理、4x4切片策略、通过PaCMAP + K - Means视觉聚类和地理位置过滤进行域先验适应，聚合切片预测并重新加权。

Result: 在私有排行榜上获得0.348的宏平均F1分数，且无需额外训练。

Conclusion: 所提出的解决方案在植物识别挑战中取得较好成绩，代码开源利于复现。

Abstract: We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on
multi-species plant identification in vegetation quadrat images. Our pipeline
combines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level
inference, (ii) a 4x4 tiling strategy that aligns patch size with the network's
518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP +
K-Means visual clustering and geolocation filtering. Tile predictions are
aggregated by majority vote and re-weighted with cluster-specific Bayesian
priors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while
requiring no additional training. All code, configuration files, and
reproducibility scripts are publicly available at
https://github.com/dsgt-arc/plantclef-2025.

</details>


### [212] [Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)](https://arxiv.org/abs/2507.05300)
*Nicholas Merchant,Haitz Sáez de Ocáriz Borde,Andrei Cristian Popescu,Carlos Garcia Jurado Suarez*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We argue that generative text-to-image models often struggle with prompt
adherence due to the noisy and unstructured nature of large-scale datasets like
LAION-5B. This forces users to rely heavily on prompt engineering to elicit
desirable outputs. In this work, we propose that enforcing a consistent caption
structure during training can significantly improve model controllability and
alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of
Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by
a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part
template: subject, setting, aesthetics, and camera details. We fine-tune
PixArt-$\Sigma$ and Stable Diffusion 2 using both structured and randomly
shuffled captions, and show that structured versions consistently yield higher
text-image alignment scores using visual question answering (VQA) models. The
dataset is publicly available at
https://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M.

</details>


### [213] [CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection](https://arxiv.org/abs/2507.05302)
*Binjia Zhou,Hengrui Lou,Lizhe Chen,Haoyuan Li,Dawei Luo,Shuai Chen,Jie Lei,Zunlei Feng,Yijun Bei*

Main category: cs.CV

TL;DR: 本文提出用于可解释人脸伪造检测的CorrDetail框架，实验表明其性能优越且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 图像生成技术发展使面部深度伪造带来安全挑战，现有检测技术存在缺乏伪造细节解释、易产生幻觉等不足。

Method: 引入CorrDetail框架，通过错误引导问题纠正伪造细节，加入视觉细粒度细节增强模块，设计融合决策策略。

Result: CorrDetail与最新方法相比达到了最先进的性能，能准确识别伪造细节。

Conclusion: CorrDetail框架有效，具有强大的泛化能力。

Abstract: With the swift progression of image generation technology, the widespread
emergence of facial deepfakes poses significant challenges to the field of
security, thus amplifying the urgent need for effective deepfake
detection.Existing techniques for face forgery detection can broadly be
categorized into two primary groups: visual-based methods and multimodal
approaches. The former often lacks clear explanations for forgery details,
while the latter, which merges visual and linguistic modalities, is more prone
to the issue of hallucinations.To address these shortcomings, we introduce a
visual detail enhanced self-correction framework, designated CorrDetail, for
interpretable face forgery detection. CorrDetail is meticulously designed to
rectify authentic forgery details when provided with error-guided questioning,
with the aim of fostering the ability to uncover forgery details rather than
yielding hallucinated responses. Additionally, to bolster the reliability of
its findings, a visual fine-grained detail enhancement module is incorporated,
supplying CorrDetail with more precise visual forgery details. Ultimately, a
fusion decision strategy is devised to further augment the model's
discriminative capacity in handling extreme samples, through the integration of
visual information compensation and model bias reduction.Experimental results
demonstrate that CorrDetail not only achieves state-of-the-art performance
compared to the latest methodologies but also excels in accurately identifying
forged details, all while exhibiting robust generalization capabilities.

</details>


### [214] [pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2507.05394)
*Sajjad Ghiasvand,Mahnoosh Alizadeh,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: 提出pFedMMA框架用于视觉语言任务的个性化联邦学习，在多数据集实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将视觉语言模型适应分散异构数据时，难以平衡个性化和泛化能力，在未见类别或领域表现不佳。

Method: 提出pFedMMA框架，利用多模态适配器，采用非对称优化策略，客户端本地适应个性化数据分布，协作训练共享投影。

Result: 在十一个数据集（包括领域和标签转移场景）实验中，pFedMMA在个性化和泛化之间取得了最先进的平衡，优于近期的联邦提示调整方法。

Conclusion: pFedMMA框架有效解决了视觉语言模型在个性化联邦学习中平衡个性化和泛化能力的问题。

Abstract: Vision-Language Models (VLMs) like CLIP have demonstrated remarkable
generalization in zero- and few-shot settings, but adapting them efficiently to
decentralized, heterogeneous data remains a challenge. While prompt tuning has
emerged as a popular parameter-efficient approach in personalized federated
learning, existing methods often sacrifice generalization in favor of
personalization, struggling particularly on unseen classes or domains. In this
work, we propose pFedMMA, the first personalized federated learning framework
that leverages multi-modal adapters for vision-language tasks. Each adapter
contains modality-specific up- and down-projection layers alongside a globally
shared projection that aligns cross-modal features. Our asymmetric optimization
strategy allows clients to locally adapt to personalized data distributions
while collaboratively training the shared projection to improve global
generalization. This design is also communication-efficient, as only the shared
component is exchanged during rounds. Through extensive experiments across
eleven datasets, including domain- and label-shift scenarios, we show that
pFedMMA achieves state-of-the-art trade-offs between personalization and
generalization, outperforming recent federated prompt tuning methods. The code
is available at https://github.com/sajjad-ucsb/pFedMMA.

</details>


### [215] [Motion Generation: A Survey of Generative Approaches and Benchmarks](https://arxiv.org/abs/2507.05419)
*Aliasghar Khani,Arianna Rampini,Bruno Roy,Larasika Nadela,Noa Kaplan,Evan Atherton,Derek Cheung,Jacky Bibliowicz*

Main category: cs.CV

TL;DR: 本文对运动生成方法进行全面综述，按生成策略分类，聚焦2023年顶级会议论文，分析多方面内容，以助比较和明确挑战。


<details>
  <summary>Details</summary>
Motivation: 运动生成领域发展迅速、方法多样，需从生成方法角度进行全面结构化综述。

Method: 按潜在生成策略对运动生成方法深入分类，分析架构原则、条件机制和生成设置，总结评估指标和数据集。

Result: 完成了对运动生成方法的分类，对多方面内容进行了分析和总结。

Conclusion: 该综述为研究人员和从业者提供及时基础参考，便于在快速发展的运动生成领域进行比较和识别开放挑战。

Abstract: Motion generation, the task of synthesizing realistic motion sequences from
various conditioning inputs, has become a central problem in computer vision,
computer graphics, and robotics, with applications ranging from animation and
virtual agents to human-robot interaction. As the field has rapidly progressed
with the introduction of diverse modeling paradigms including GANs,
autoencoders, autoregressive models, and diffusion-based techniques, each
approach brings its own advantages and limitations. This growing diversity has
created a need for a comprehensive and structured review that specifically
examines recent developments from the perspective of the generative approach
employed.
  In this survey, we provide an in-depth categorization of motion generation
methods based on their underlying generative strategies. Our main focus is on
papers published in top-tier venues since 2023, reflecting the most recent
advancements in the field. In addition, we analyze architectural principles,
conditioning mechanisms, and generation settings, and compile a detailed
overview of the evaluation metrics and datasets used across the literature. Our
objective is to enable clearer comparisons and identify open challenges,
thereby offering a timely and foundational reference for researchers and
practitioners navigating the rapidly evolving landscape of motion generation.

</details>


### [216] [Robotic System with AI for Real Time Weed Detection, Canopy Aware Spraying, and Droplet Pattern Evaluation](https://arxiv.org/abs/2507.05432)
*Inayat Rasool,Pappu Kumar Yadav,Amee Parmar,Hasan Mirzakhaninafchi,Rikesh Budhathoki,Zain Ul Abideen Usmani,Supriya Paudel,Ivan Perez Olivera,Eric Jone*

Main category: cs.CV

TL;DR: 本文开发了基于视觉和AI的可变速率喷雾器系统，经室内试验验证其性能，表明深度学习与低成本硬件结合用于选择性除草剂施用的潜力，未来将扩大检测范围并进一步验证。


<details>
  <summary>Details</summary>
Motivation: 解决现代农业中除草剂统一过量施用导致的成本增加、环境污染和杂草抗性问题。

Method: 开发集成轻量级YOLO11n和YOLO11n - seg深度学习模型的喷雾器系统，在NVIDIA Jetson Orin Nano上进行推理，用Arduino Uno控制喷嘴，通过室内盆栽试验验证。

Result: YOLO11n模型mAP@50为0.98，YOLO11n - seg模型mAP@50为0.48，系统平均喷雾覆盖率24.22%，能根据冠层大小实时调整喷雾输出。

Conclusion: 实时深度学习与低成本嵌入式硬件结合用于选择性除草剂施用有潜力，未来将扩大检测范围并进一步验证。

Abstract: Uniform and excessive herbicide application in modern agriculture contributes
to increased input costs, environmental pollution, and the emergence of
herbicide resistant weeds. To address these challenges, we developed a vision
guided, AI-driven variable rate sprayer system capable of detecting weed
presence, estimating canopy size, and dynamically adjusting nozzle activation
in real time. The system integrates lightweight YOLO11n and YOLO11n-seg deep
learning models, deployed on an NVIDIA Jetson Orin Nano for onboard inference,
and uses an Arduino Uno-based relay interface to control solenoid actuated
nozzles based on canopy segmentation results. Indoor trials were conducted
using 15 potted Hibiscus rosa sinensis plants of varying canopy sizes to
simulate a range of weed patch scenarios. The YOLO11n model achieved a mean
average precision (mAP@50) of 0.98, with a precision of 0.99 and a recall close
to 1.0. The YOLO11n-seg segmentation model achieved a mAP@50 of 0.48, precision
of 0.55, and recall of 0.52. System performance was validated using water
sensitive paper, which showed an average spray coverage of 24.22% in zones
where canopy was present. An upward trend in mean spray coverage from 16.22%
for small canopies to 21.46% and 21.65% for medium and large canopies,
respectively, demonstrated the system's capability to adjust spray output based
on canopy size in real time. These results highlight the potential of combining
real time deep learning with low-cost embedded hardware for selective herbicide
application. Future work will focus on expanding the detection capabilities to
include three common weed species in South Dakota: water hemp (Amaranthus
tuberculatus), kochia (Bassia scoparia), and foxtail (Setaria spp.), followed
by further validation in both indoor and field trials within soybean and corn
production systems.

</details>


### [217] [Cloud Diffusion Part 1: Theory and Motivation](https://arxiv.org/abs/2507.05496)
*Andrew Randono*

Main category: cs.CV

TL;DR: 文章提出用具有尺度不变性的噪声分布代替白噪声构建云扩散模型，认为其有诸多优势，后续将构建并对比。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成扩散模型使用白噪声，而自然图像集有尺度不变性，希望构建能利用此特性的模型。

Method: 用尺度不变噪声分布替代白噪声构建云扩散模型。

Result: 无具体结果，仅预测云扩散模型可带来更快推理、改善高频细节和更强可控性。

Conclusion: 后续将构建云扩散模型并与经典白噪声扩散模型对比。

Abstract: Diffusion models for image generation function by progressively adding noise
to an image set and training a model to separate out the signal from the noise.
The noise profile used by these models is white noise -- that is, noise based
on independent normal distributions at each point whose mean and variance is
independent of the scale. By contrast, most natural image sets exhibit a type
of scale invariance in their low-order statistical properties characterized by
a power-law scaling. Consequently, natural images are closer (in a quantifiable
sense) to a different probability distribution that emphasizes large scale
correlations and de-emphasizes small scale correlations. These scale invariant
noise profiles can be incorporated into diffusion models in place of white
noise to form what we will call a ``Cloud Diffusion Model". We argue that these
models can lead to faster inference, improved high-frequency details, and
greater controllability. In a follow-up paper, we will build and train a Cloud
Diffusion Model that uses scale invariance at a fundamental level and compare
it to classic, white noise diffusion models.

</details>


### [218] [Driving as a Diagnostic Tool: Scenario-based Cognitive Assessment in Older Drivers From Driving Video](https://arxiv.org/abs/2507.05463)
*Md Zahid Hasan,Guillermo Basulto-Elias,Jun Ha Chang,Sahuna Hallmark,Matthew Rizzo,Anuj Sharma,Soumik Sarkar*

Main category: cs.CV

TL;DR: 引入基于自然驾驶视频和大视觉模型的老年司机情景认知状态识别，提出框架分析行为、分类认知状态和预测疾病进展，助力早期检测和干预。


<details>
  <summary>Details</summary>
Motivation: 当前认知衰退诊断方法耗时且成本高，常导致漏诊，需通过分析真实驾驶行为提取与功能衰退和临床特征相关的‘数字指纹’，早期检测认知衰退。

Method: 提出使用大视觉模型和自然驾驶视频的框架，分析司机行为、分类认知状态和预测疾病进展，利用车辆作为‘诊断工具’。

Result: 方法能识别功能损伤的早期预警信号。

Conclusion: 工作增强了早期检测能力，支持开发可扩展、非侵入性监测系统，减轻老年人群认知衰退的社会和经济负担。

Abstract: We introduce scenario-based cognitive status identification in older drivers
from Naturalistic driving videos and large vision models. In recent times,
cognitive decline, including Alzheimer's disease (AD) and mild cognitive
impairment (MCI), is often underdiagnosed due to the time-consuming and costly
nature of current diagnostic methods. By analyzing real-world driving behavior
captured through in-vehicle systems, this research aims to extract "digital
fingerprints" that correlate with functional decline and clinical features of
MCI and AD. Moreover, modern large vision models can draw meaningful insights
from everyday driving patterns of older patients to early detect cognitive
decline. We propose a framework that uses large vision models and naturalistic
driving videos to analyze driver behavior, classify cognitive status and
predict disease progression. We leverage the strong relationship between
real-world driving behavior as an observation of the current cognitive status
of the drivers where the vehicle can be utilized as a "diagnostic tool". Our
method identifies early warning signs of functional impairment, contributing to
proactive intervention strategies. This work enhances early detection and
supports the development of scalable, non-invasive monitoring systems to
mitigate the growing societal and economic burden of cognitive decline in the
aging population.

</details>


### [219] [Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception](https://arxiv.org/abs/2507.05536)
*Moseli Mots'oehli,Feimei Chen,Hok Wai Chan,Itumeleng Tlali,Thulani Babeli,Kyungim Baek,Huaijin Chen*

Main category: cs.CV

TL;DR: 提出程序增强管道处理非洲低资源环境下自动驾驶数据稀缺问题，发布工具包、数据集和基准结果。


<details>
  <summary>Details</summary>
Motivation: 发展中地区尤其是非洲自动驾驶数据集稀缺，阻碍低资源环境下的感知能力。

Method: 构建程序增强管道，含模拟光学效果的折射模块和添加天气效果的天气模块，用三个图像恢复模型提供基准性能。

Result: 建立了基准性能，发布了失真工具包、增强数据集和基准结果。

Conclusion: 可支持非洲地区自动驾驶感知研究，无需高昂的数据收集、标注或模拟成本。

Abstract: The scarcity of autonomous vehicle datasets from developing regions,
particularly across Africa's diverse urban, rural, and unpaved roads, remains a
key obstacle to robust perception in low-resource settings. We present a
procedural augmentation pipeline that enhances low-cost monocular dashcam
footage with realistic refractive distortions and weather-induced artifacts
tailored to challenging African driving scenarios. Our refractive module
simulates optical effects from low-quality lenses and air turbulence, including
lens distortion, Perlin noise, Thin-Plate Spline (TPS), and divergence-free
(incompressible) warps. The weather module adds homogeneous fog, heterogeneous
fog, and lens flare. To establish a benchmark, we provide baseline performance
using three image restoration models. To support perception research in
underrepresented African contexts, without costly data collection, labeling, or
simulation, we release our distortion toolkit, augmented dataset splits, and
benchmark results.

</details>


### [220] [ReLayout: Integrating Relation Reasoning for Content-aware Layout Generation with Multi-modal Large Language Models](https://arxiv.org/abs/2507.05568)
*Jiaxu Tian,Xuehui Yu,Yaoxing Wang,Pan Wang,Guangqian Guo,Shan Gao*

Main category: cs.CV

TL;DR: 现有基于大语言模型的内容感知布局方法存在问题，本文提出ReLayout方法生成更合理美观布局，实验证明其性能更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的内容感知布局方法无法充分解释视觉主题和设计元素间的空间关系，导致布局生成存在结构和多样性问题。

Method: 引入ReLayout方法，通过引入明确的关系定义增强布局注释，将布局分解为更小、结构化和递归的布局；引入布局原型重平衡采样器，定义布局原型特征并量化布局风格。

Result: ReLayout方法优于基线方法，能生成更符合人类美学、更具可解释性的结构化和多样化布局。

Conclusion: ReLayout方法能有效解决现有基于大语言模型的内容感知布局方法的问题，生成更合理和美观的布局。

Abstract: Content-aware layout aims to arrange design elements appropriately on a given
canvas to convey information effectively. Recently, the trend for this task has
been to leverage large language models (LLMs) to generate layouts
automatically, achieving remarkable performance. However, existing LLM-based
methods fail to adequately interpret spatial relationships among visual themes
and design elements, leading to structural and diverse problems in layout
generation. To address this issue, we introduce ReLayout, a novel method that
leverages relation-CoT to generate more reasonable and aesthetically coherent
layouts by fundamentally originating from design concepts. Specifically, we
enhance layout annotations by introducing explicit relation definitions, such
as region, salient, and margin between elements, with the goal of decomposing
the layout into smaller, structured, and recursive layouts, thereby enabling
the generation of more structured layouts. Furthermore, based on these defined
relationships, we introduce a layout prototype rebalance sampler, which defines
layout prototype features across three dimensions and quantifies distinct
layout styles. This sampler addresses uniformity issues in generation that
arise from data bias in the prototype distribution balance process. Extensive
experimental results verify that ReLayout outperforms baselines and can
generate structural and diverse layouts that are more aligned with human
aesthetics and more explainable.

</details>


### [221] [Generative Head-Mounted Camera Captures for Photorealistic Avatars](https://arxiv.org/abs/2507.05620)
*Shaojie Bai,Seunghyeon Seo,Yida Wang,Chenghui Li,Owen Wang,Te-Li Wang,Tianyang Ma,Jason Saragih,Shih-En Wei,Nojun Kwak,Hyung Jun Kim*

Main category: cs.CV

TL;DR: 提出Generative HMC (GenHMC) 方法，利用大量未配对HMC捕获数据生成高质量合成HMC图像，能分离面部表达和外观，实现跨身份泛化，提高数据效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有实现虚拟和增强现实中逼真头像动画的方法存在获取面部真实状态困难、训练时表情和风格分离不完美、大规模数据集收集成本高且不可复用等问题。

Method: 提出Generative HMC (GenHMC) 方法，利用大量未配对HMC捕获数据，根据穹顶相机捕获的任何条件化头像状态直接生成高质量合成HMC图像。

Result: 能够正确分离指定面部表情和视角的输入条件信号与面部外观，实现跨身份泛化。通过评估合成HMC图像和通用面部编码器，展示了更好的数据效率和最先进的准确性。

Conclusion: GenHMC方法有效解决了现有方法的问题，提高了数据效率和准确性，为实现虚拟和增强现实中逼真头像动画提供了新的解决方案。

Abstract: Enabling photorealistic avatar animations in virtual and augmented reality
(VR/AR) has been challenging because of the difficulty of obtaining ground
truth state of faces. It is physically impossible to obtain synchronized images
from head-mounted cameras (HMC) sensing input, which has partial observations
in infrared (IR), and an array of outside-in dome cameras, which have full
observations that match avatars' appearance. Prior works relying on
analysis-by-synthesis methods could generate accurate ground truth, but suffer
from imperfect disentanglement between expression and style in their
personalized training. The reliance of extensive paired captures (HMC and dome)
for the same subject makes it operationally expensive to collect large-scale
datasets, which cannot be reused for different HMC viewpoints and lighting. In
this work, we propose a novel generative approach, Generative HMC (GenHMC),
that leverages large unpaired HMC captures, which are much easier to collect,
to directly generate high-quality synthetic HMC images given any conditioning
avatar state from dome captures. We show that our method is able to properly
disentangle the input conditioning signal that specifies facial expression and
viewpoint, from facial appearance, leading to more accurate ground truth.
Furthermore, our method can generalize to unseen identities, removing the
reliance on the paired captures. We demonstrate these breakthroughs by both
evaluating synthetic HMC images and universal face encoders trained from these
new HMC-avatar correspondences, which achieve better data efficiency and
state-of-the-art accuracy.

</details>


### [222] [Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model](https://arxiv.org/abs/2507.05513)
*Mengyao Xu,Gabriel Moreira,Ronay Ak,Radek Osmulski,Yauhen Babakhin,Zhiding Yu,Benedikt Schifferer,Even Oldridge*

Main category: cs.CV

TL;DR: 介绍了llama - nemoretriever - colembed统一文本图像检索模型，发布1B和3B两种变体，3B模型表现出色，阐述了方法并分析权衡。


<details>
  <summary>Details</summary>
Motivation: 满足跨模态检索系统日益增长的需求。

Method: 利用NVIDIA Eagle2 VLM，将因果注意力替换为双向注意力，集成ColBERT式后期交互机制，采用两阶段训练策略。

Result: 3B模型在ViDoRe V1和V2上取得领先成绩，NDCG@5分别为91.0和63.5。

Conclusion: 所采用机制在提升检索准确性同时存在存储和效率方面的权衡。

Abstract: Motivated by the growing demand for retrieval systems that operate across
modalities, we introduce llama-nemoretriever-colembed, a unified text-image
retrieval model that delivers state-of-the-art performance across multiple
benchmarks. We release two model variants, 1B and 3B. The 3B model achieves
state of the art performance, scoring NDCG@5 91.0 on ViDoRe V1 and 63.5 on
ViDoRe V2, placing first on both leaderboards as of June 27, 2025.
  Our approach leverages the NVIDIA Eagle2 Vision-Language model (VLM),
modifies its architecture by replacing causal attention with bidirectional
attention, and integrates a ColBERT-style late interaction mechanism to enable
fine-grained multimodal retrieval in a shared embedding space. While this
mechanism delivers superior retrieval accuracy, it introduces trade-offs in
storage and efficiency. We provide a comprehensive analysis of these
trade-offs. Additionally, we adopt a two-stage training strategy to enhance the
model's retrieval capabilities.

</details>


### [223] [MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos](https://arxiv.org/abs/2507.05675)
*Rongsheng Wang,Junying Chen,Ke Ji,Zhenyang Cai,Shunian Chen,Yunjin Yang,Benyou Wang*

Main category: cs.CV

TL;DR: 现有医学视频生成研究不足，本文引入数据集MedVideoCap - 55K并开发模型MedGen，性能领先，代码和数据开源。


<details>
  <summary>Details</summary>
Motivation: 医学视频在临床训练等应用中重要，但当前模型因缺乏医学领域优质数据集，处理医学提示时易产生不真实或错误内容。

Method: 引入大规模、多样化且带丰富字幕的医学视频生成数据集MedVideoCap - 55K，基于此开发模型MedGen。

Result: MedGen在多个基准测试中，在视觉质量和医学准确性上，在开源模型中表现领先，可与商业系统媲美。

Conclusion: 数据集和模型可作为有价值资源，促进医学视频生成领域的进一步研究。

Abstract: Recent advances in video generation have shown remarkable progress in
open-domain settings, yet medical video generation remains largely
underexplored. Medical videos are critical for applications such as clinical
training, education, and simulation, requiring not only high visual fidelity
but also strict medical accuracy. However, current models often produce
unrealistic or erroneous content when applied to medical prompts, largely due
to the lack of large-scale, high-quality datasets tailored to the medical
domain. To address this gap, we introduce MedVideoCap-55K, the first
large-scale, diverse, and caption-rich dataset for medical video generation. It
comprises over 55,000 curated clips spanning real-world medical scenarios,
providing a strong foundation for training generalist medical video generation
models. Built upon this dataset, we develop MedGen, which achieves leading
performance among open-source models and rivals commercial systems across
multiple benchmarks in both visual quality and medical accuracy. We hope our
dataset and model can serve as a valuable resource and help catalyze further
research in medical video generation. Our code and data is available at
https://github.com/FreedomIntelligence/MedGen

</details>


### [224] [Hyperspectral Anomaly Detection Methods: A Survey and Comparative Study](https://arxiv.org/abs/2507.05730)
*Aayushma Pant,Arbind Agrahari Baniya,Tsz-Kwan Lee,Sunil Aryal*

Main category: cs.CV

TL;DR: 文章对高光谱异常检测（HAD）技术进行全面比较，在17个基准数据集上评估多种方法，显示深度学习模型检测精度高，统计模型速度快。


<details>
  <summary>Details</summary>
Motivation: 现有HAD方法存在计算复杂度高、对噪声敏感和泛化能力有限等问题，需综合评估不同方法。

Method: 将HAD技术分为统计模型、基于表示的方法、经典机器学习方法和深度学习模型，在17个基准数据集上用ROC、AUC等指标评估。

Result: 深度学习模型检测精度最高，统计模型在所有数据集上速度最快。

Conclusion: 为推动HAD方法领域发展的研究人员和从业者提供有价值的见解。

Abstract: Hyperspectral images are high-dimensional datasets consisting of hundreds of
contiguous spectral bands, enabling detailed material and surface analysis.
Hyperspectral anomaly detection (HAD) refers to the technique of identifying
and locating anomalous targets in such data without prior information about a
hyperspectral scene or target spectrum. This technology has seen rapid
advancements in recent years, with applications in agriculture, defence,
military surveillance, and environmental monitoring. Despite this significant
progress, existing HAD methods continue to face challenges such as high
computational complexity, sensitivity to noise, and limited generalisation
across diverse datasets. This study presents a comprehensive comparison of
various HAD techniques, categorising them into statistical models,
representation-based methods, classical machine learning approaches, and deep
learning models. We evaluated these methods across 17 benchmarking datasets
using different performance metrics, such as ROC, AUC, and separability map to
analyse detection accuracy, computational efficiency, their strengths,
limitations, and directions for future research.The research shows that deep
learning models achieved the highest detection accuracy, while statistical
models demonstrated exceptional speed across all datasets. This study aims to
provide valuable insights for researchers and practitioners working to advance
the field of hyperspectral anomaly detection methods.

</details>


### [225] [TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and OCR-Guided Supervision](https://arxiv.org/abs/2507.06033)
*Syeda Anshrah Gillani,Mirza Samad Ahmed Baig,Osama Ahmed Khan,Shahid Munir Shah,Umema Mujeeb,Maheen Ali*

Main category: cs.CV

TL;DR: 本文提出GCDA框架解决文本到图像扩散模型生成可读文本的问题，实验显示该框架在多个指标上达到最优。


<details>
  <summary>Details</summary>
Motivation: 现代文本到图像扩散模型无法在生成图像中产生可读、有意义且拼写正确的文本，限制了其在广告、学习和创意设计等实际场景的应用。

Method: 引入GCDA框架，包括双流文本编码器、字符感知注意力机制和带新注意力分离损失、OCR循环微调阶段及全文感知损失。

Result: 在MARIO - 10M和T2I - CompBench等基准数据集上实验，GCDA在所有指标上达到新的最优水平，文本渲染、人类感知等方面表现良好。

Conclusion: GCDA框架能有效解决文本到图像扩散模型生成可读文本的问题，具有更好的性能。

Abstract: The modern text-to-image diffusion models boom has opened a new era in
digital content production as it has proven the previously unseen ability to
produce photorealistic and stylistically diverse imagery based on the semantics
of natural-language descriptions. However, the consistent disadvantage of these
models is that they cannot generate readable, meaningful, and correctly spelled
text in generated images, which significantly limits the use of practical
purposes like advertising, learning, and creative design. This paper introduces
a new framework, namely Glyph-Conditioned Diffusion with Character-Aware
Attention (GCDA), using which a typical diffusion backbone is extended by three
well-designed modules. To begin with, the model has a dual-stream text encoder
that encodes both semantic contextual information and explicit glyph
representations, resulting in a character-aware representation of the input
text that is rich in nature. Second, an attention mechanism that is aware of
the character is proposed with a new attention segregation loss that aims to
limit the attention distribution of each character independently in order to
avoid distortion artifacts. Lastly, GCDA has an OCR-in-the-loop fine-tuning
phase, where a full text perceptual loss, directly optimises models to be
legible and accurately spell. Large scale experiments to benchmark datasets,
such as MARIO-10M and T2I-CompBench, reveal that GCDA sets a new
state-of-the-art on all metrics, with better character based metrics on text
rendering (Character Error Rate: 0.08 vs 0.21 for the previous best; Word Error
Rate: 0.15 vs 0.25), human perception, and comparable image synthesis quality
on high-fidelity (FID: 14.3).

</details>


### [226] [Towards Solar Altitude Guided Scene Illumination](https://arxiv.org/abs/2507.05812)
*Samed Doğan,Maximilian Hoh,Nico Leuze,Nicolas R. -Peña,Alfred Schöttl*

Main category: cs.CV

TL;DR: 本文针对自动驾驶合成相机传感器数据生成中白天变化研究不足的问题，提出用太阳高度作为全局条件变量并结合定制归一化方法，在扩散模型中准确捕捉光照特征和图像噪声。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶功能开发依赖大规模高质量传感器数据，但真实数据采集存在人工成本高、场景覆盖有限等问题，且现有合成数据研究在白天变化方面存在差距。

Method: 提出将太阳高度作为全局条件变量，其可由经纬度坐标和当地时间计算得出，同时采用定制的归一化方法处理光照对高度数值变化的敏感性。

Result: 在扩散模型中能够准确捕捉光照特征和与光照相关的图像噪声。

Conclusion: 以太阳高度作为全局条件变量并结合定制归一化方法，可有效解决合成相机传感器数据生成中白天变化的问题。

Abstract: The development of safe and robust autonomous driving functions is heavily
dependent on large-scale, high-quality sensor data. However, real-word data
acquisition demands intensive human labor and is strongly limited by factors
such as labeling cost, driver safety protocols and diverse scenario coverage.
Thus, multiple lines of work focus on the conditional generation of synthetic
camera sensor data. We identify a significant gap in research regarding daytime
variation, presumably caused by the scarcity of available labels. Consequently,
we present the solar altitude as global conditioning variable. It is readily
computable from latitude-longitude coordinates and local time, eliminating the
need for extensive manual labeling. Our work is complemented by a tailored
normalization approach, targeting the sensitivity of daylight towards small
numeric changes in altitude. We demonstrate its ability to accurately capture
lighting characteristics and illumination-dependent image noise in the context
of diffusion models.

</details>


### [227] [Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified Synthesis Framework](https://arxiv.org/abs/2507.05814)
*Wang Wang,Mingyu Shi,Jun Jiang,Wenqian Ma,Chong Liu,Yasutaka Narazaki,Xuguang Wang*

Main category: cs.CV

TL;DR: 本文提出生成3D桥梁数据的系统框架，解决现有合成数据泛化不足问题，实验表明该框架生成数据可提升模型性能，为桥梁结构3D视觉分析提供方法和数据集。


<details>
  <summary>Details</summary>
Motivation: 桥梁老化和传统人工检测效率低，3D点云技术受现实数据不完整限制，现有合成数据方法泛化不足。

Method: 提出系统框架，自动生成含组件级实例注释、高保真颜色和精确法向量的完整点云，还可模拟生成不完整点云。

Result: 用合成数据训练的PointNet++模型在真实桥梁语义分割中mIoU达84.2%，微调的KT - Net在组件完成任务中表现出色。

Conclusion: 为桥梁结构3D视觉分析提供创新方法和基础数据集，对基础设施自动化管理和维护有重要意义。

Abstract: As critical transportation infrastructure, bridges face escalating challenges
from aging and deterioration, while traditional manual inspection methods
suffer from low efficiency. Although 3D point cloud technology provides a new
data-driven paradigm, its application potential is often constrained by the
incompleteness of real-world data, which results from missing labels and
scanning occlusions. To overcome the bottleneck of insufficient generalization
in existing synthetic data methods, this paper proposes a systematic framework
for generating 3D bridge data.
  This framework can automatically generate complete point clouds featuring
component-level instance annotations, high-fidelity color, and precise normal
vectors. It can be further extended to simulate the creation of diverse and
physically realistic incomplete point clouds, designed to support the training
of segmentation and completion networks, respectively. Experiments demonstrate
that a PointNet++ model trained with our synthetic data achieves a mean
Intersection over Union (mIoU) of 84.2% in real-world bridge semantic
segmentation. Concurrently, a fine-tuned KT-Net exhibits superior performance
on the component completion task.
  This research offers an innovative methodology and a foundational dataset for
the 3D visual analysis of bridge structures, holding significant implications
for advancing the automated management and maintenance of infrastructure.

</details>


### [228] [On the Effectiveness of Methods and Metrics for Explainable AI in Remote Sensing Image Scene Classification](https://arxiv.org/abs/2507.05916)
*Jonas Klotz,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 研究遥感图像场景分类中解释方法和指标的有效性，分析多种方法和指标并给出选择建议。


<details>
  <summary>Details</summary>
Motivation: 多数遥感中的可解释人工智能方法和评估指标最初为计算机视觉中的自然图像开发，直接用于遥感可能不合适，需研究其在遥感图像场景分类中的有效性。

Method: 对涵盖五类的十种解释指标，应用于三种遥感数据集上的五种特征归因方法进行方法学和实验分析。

Result: 发现解释方法和指标存在关键局限，如基于扰动的方法依赖扰动基线和场景空间特征等；鲁棒性和随机化指标更稳定，实验结果支持方法学发现。

Conclusion: 基于分析，为遥感图像场景分类中解释方法、指标和超参数的选择提供了指南。

Abstract: The development of explainable artificial intelligence (xAI) methods for
scene classification problems has attracted great attention in remote sensing
(RS). Most xAI methods and the related evaluation metrics in RS are initially
developed for natural images considered in computer vision (CV), and their
direct usage in RS may not be suitable. To address this issue, in this paper,
we investigate the effectiveness of explanation methods and metrics in the
context of RS image scene classification. In detail, we methodologically and
experimentally analyze ten explanation metrics spanning five categories
(faithfulness, robustness, localization, complexity, randomization), applied to
five established feature attribution methods (Occlusion, LIME, GradCAM, LRP,
and DeepLIFT) across three RS datasets. Our methodological analysis identifies
key limitations in both explanation methods and metrics. The performance of
perturbation-based methods, such as Occlusion and LIME, heavily depends on
perturbation baselines and spatial characteristics of RS scenes. Gradient-based
approaches like GradCAM struggle when multiple labels are present in the same
image, while some relevance propagation methods (LRP) can distribute relevance
disproportionately relative to the spatial extent of classes. Analogously, we
find limitations in evaluation metrics. Faithfulness metrics share the same
problems as perturbation-based methods. Localization metrics and complexity
metrics are unreliable for classes with a large spatial extent. In contrast,
robustness metrics and randomization metrics consistently exhibit greater
stability. Our experimental results support these methodological findings.
Based on our analysis, we provide guidelines for selecting explanation methods,
metrics, and hyperparameters in the context of RS image scene classification.

</details>


### [229] [Exploring Partial Multi-Label Learning via Integrating Semantic Co-occurrence Knowledge](https://arxiv.org/abs/2507.05992)
*Xin Wu,Fei Teng,Yue Feng,Kaibo Shi,Zhuosheng Lin,Ji Zhang,James Wang*

Main category: cs.CV

TL;DR: 提出用于部分多标签学习的SCINet框架，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 部分多标签学习核心挑战是准确识别标签和实例间的模糊关系，本文强调匹配标签和实例的共现模式是解决该挑战的关键。

Method: 提出SCINet框架，引入双主导提示模块，开发跨模态融合模块，提出内在语义增强策略。

Result: 在四个广泛使用的基准数据集上的大量实验表明，SCINet优于现有方法。

Conclusion: SCINet是一种有效解决部分多标签学习问题的框架。

Abstract: Partial multi-label learning aims to extract knowledge from incompletely
annotated data, which includes known correct labels, known incorrect labels,
and unknown labels. The core challenge lies in accurately identifying the
ambiguous relationships between labels and instances. In this paper, we
emphasize that matching co-occurrence patterns between labels and instances is
key to addressing this challenge. To this end, we propose Semantic
Co-occurrence Insight Network (SCINet), a novel and effective framework for
partial multi-label learning. Specifically, SCINet introduces a bi-dominant
prompter module, which leverages an off-the-shelf multimodal model to capture
text-image correlations and enhance semantic alignment. To reinforce
instance-label interdependencies, we develop a cross-modality fusion module
that jointly models inter-label correlations, inter-instance relationships, and
co-occurrence patterns across instance-label assignments. Moreover, we propose
an intrinsic semantic augmentation strategy that enhances the model's
understanding of intrinsic data semantics by applying diverse image
transformations, thereby fostering a synergistic relationship between label
confidence and sample difficulty. Extensive experiments on four widely-used
benchmark datasets demonstrate that SCINet surpasses state-of-the-art methods.

</details>


### [230] [Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images without GNSS](https://arxiv.org/abs/2507.05999)
*Xinyu Wang,Muhammad Ibrahim,Atif Mansoor,Ajmal Mian*

Main category: cs.CV

TL;DR: 提出结构化地理配准和空间校正方法，在KITTI和Perth数据集测试效果好。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR点云地理配准方法在GNSS信号差的城市区域有定位误差问题，需改进。

Method: 使用预训练Point Transformer模型分割道路点，提取道路骨架和交点对齐，用径向基函数插值局部细化，基于SRTM数据进行高程校正。

Result: 在KITTI数据集上平面校准标准差平均0.84米，提升55.3%；Perth数据集上平均标准差0.96米，提升77.4%；高程相关性分别提升30.5%和50.4%。

Conclusion: 提出的方法能有效解决城市区域LiDAR点云地理配准问题，无需先验定位。

Abstract: Accurate geo-registration of LiDAR point clouds presents significant
challenges in GNSS signal denied urban areas with high-rise buildings and
bridges. Existing methods typically rely on real-time GNSS and IMU data, that
require pre-calibration and assume stable positioning during data collection.
However, this assumption often fails in dense urban areas, resulting in
localization errors. To address this, we propose a structured geo-registration
and spatial correction method that aligns 3D point clouds with satellite
images, enabling frame-wise recovery of GNSS information and reconstruction of
city scale 3D maps without relying on prior localization. The proposed approach
employs a pre-trained Point Transformer model to segment the road points and
then extracts the road skeleton and intersection points from the point cloud as
well as the target map for alignment. Global rigid alignment of the two is
performed using the intersection points, followed by local refinement using
radial basis function (RBF) interpolation. Elevation correction is then applied
to the point cloud based on terrain information from SRTM dataset to resolve
vertical discrepancies. The proposed method was tested on the popular KITTI
benchmark and a locally collected Perth (Western Australia) CBD dataset. On the
KITTI dataset, our method achieved an average planimetric alignment standard
deviation (STD) of 0.84~m across sequences with intersections, representing a
55.3\% improvement over the original dataset. On the Perth dataset, which lacks
GNSS information, our method achieved an average STD of 0.96~m compared to the
GPS data extracted from Google Maps API. This corresponds to a 77.4\%
improvement from the initial alignment. Our method also resulted in elevation
correlation gains of 30.5\% on the KITTI dataset and 50.4\% on the Perth
dataset.

</details>


### [231] [VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis](https://arxiv.org/abs/2507.06060)
*Alexandre Symeonidis-Herzig,Özge Mercanoğlu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 提出VisualSpeaker方法用于3D面部动画，在MEAD数据集上效果良好，提高指标并支持手语头像准确口型。


<details>
  <summary>Details</summary>
Motivation: 现有3D面部动画方法依赖网格域，难以利用2D计算机视觉和图形学的创新。

Method: 提出VisualSpeaker方法，利用逼真的可微渲染，由视觉语音识别监督，训练中采用感知唇读损失。

Result: 在MEAD数据集上，标准唇顶点误差指标提高56.1%，生成动画感知质量提升，保留网格驱动动画的可控性。

Conclusion: 该方法聚焦感知，能支持手语头像准确口型。

Abstract: Realistic, high-fidelity 3D facial animations are crucial for expressive
avatar systems in human-computer interaction and accessibility. Although prior
methods show promising quality, their reliance on the mesh domain limits their
ability to fully leverage the rapid visual innovations seen in 2D computer
vision and graphics. We propose VisualSpeaker, a novel method that bridges this
gap using photorealistic differentiable rendering, supervised by visual speech
recognition, for improved 3D facial animation. Our contribution is a perceptual
lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting
avatar renders through a pre-trained Visual Automatic Speech Recognition model
during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker
improves both the standard Lip Vertex Error metric by 56.1% and the perceptual
quality of the generated animations, while retaining the controllability of
mesh-driven animation. This perceptual focus naturally supports accurate
mouthings, essential cues that disambiguate similar manual signs in sign
language avatars.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [232] [Robust Bandwidth Estimation for Real-Time Communication with Offline Reinforcement Learning](https://arxiv.org/abs/2507.05785)
*Jian Kai,Tianwei Zhang,Zihan Ling,Yang Cao,Can Shen*

Main category: eess.SY

TL;DR: 提出基于离线强化学习的稳健带宽估计框架RBWE，可降低高估误差，提升体验质量。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法在动态网络中适应性有限，在线强化学习有高探索成本和服务中断风险，离线强化学习面临OOD动作等挑战。

Method: 提出RBWE框架，集成Q - ensemble和高斯混合策略以降低OOD风险、增强策略学习，设回退机制确保部署稳定性。

Result: RBWE降低高估误差18%，提升第10百分位体验质量18.6%。

Conclusion: RBWE在实时通信应用中具有实际有效性。

Abstract: Accurate bandwidth estimation (BWE) is critical for real-time communication
(RTC) systems. Traditional heuristic approaches offer limited adaptability
under dynamic networks, while online reinforcement learning (RL) suffers from
high exploration costs and potential service disruptions. Offline RL, which
leverages high-quality data collected from real-world environments, offers a
promising alternative. However, challenges such as out-of-distribution (OOD)
actions, policy extraction from behaviorally diverse datasets, and reliable
deployment in production systems remain unsolved. We propose RBWE, a robust
bandwidth estimation framework based on offline RL that integrates Q-ensemble
(an ensemble of Q-functions) with a Gaussian mixture policy to mitigate OOD
risks and enhance policy learning. A fallback mechanism ensures deployment
stability by switching to heuristic methods under high uncertainty.
Experimental results show that RBWE reduces overestimation errors by 18% and
improves the 10th percentile Quality of Experience (QoE) by 18.6%,
demonstrating its practical effectiveness in real-world RTC applications.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [233] [Parameterized Restless Temporal Path](https://arxiv.org/abs/2507.05760)
*Justine Cauvi,Laurent Viennot*

Main category: cs.CC

TL;DR: 研究新参数顶点区间隶属宽度在无休临时路径问题中的应用，证明区间模型中该问题NP难，给出点模型的FPT算法。


<details>
  <summary>Details</summary>
Motivation: Bumpus和Meeks引入的顶点区间隶属宽度参数对时态图顶点可达性问题的FPT算法设计有前景，研究其在无休临时路径问题中的应用。

Method: 对区间模型和点模型分别进行研究，点模型分统一延迟为1和任意正延迟两种情况。

Result: 在区间模型中，即使顶点区间隶属宽度为3，寻找无休临时路径也是NP难的；在点模型中，给出了统一延迟为1和任意正延迟的FPT算法，后者有轻微额外计算成本。

Conclusion: 顶点区间隶属宽度参数在无休临时路径问题中有不同表现，点模型下可设计FPT算法解决问题。

Abstract: Recently, Bumpus and Meeks introduced a purely temporal parameter, called
vertex-interval-membership-width, which is promising for the design of
fixed-parameter tractable (FPT) algorithms for vertex reachability problems in
temporal graphs. We study this newly introduced parameter for the problem of
restless temporal paths, in which the waiting time at each node is restricted.
In this article, we prove that, in the interval model, where arcs are present
for entire time intervals, finding a restless temporal path is NP-hard even if
the vertex-interval-membership-width is equal to three. We exhibit FPT
algorithms for the point model, where arcs are present at specific points in
time, both with uniform delay one and arbitrary positive delays. In the latter
case, this comes with a slight additional computational cost.

</details>


### [234] [A Formal Refutation of the Blockchain Trilemma](https://arxiv.org/abs/2507.05809)
*Craig Wright*

Main category: cs.CC

TL;DR: 本文正式反驳区块链不可能三角命题，指出其为范畴错误，给出反例证明该命题是启发式谬误。


<details>
  <summary>Details</summary>
Motivation: 反驳区块链不可能三角命题，即单一区块链协议无法同时实现可扩展性、安全性和去中心化的观点。

Method: 运用谓词逻辑、形式自动机理论、计算复杂度分析和图论中继拓扑测度等，结合Baran的网络路径冗余模型，还给出建设性反例。

Result: 证明不可能三角是范畴错误，混淆不同分析领域，依赖未经证实的因果假设和有缺陷的系统实现；给出能同时实现三特性的区块链协议实例。

Conclusion: 不可能三角不是协议架构定律，而是由不精确和设计失败主义导致的启发式谬误。

Abstract: The so-called blockchain trilemma asserts the impossibility of simultaneously
achieving scalability, security, and decentralisation within a single
blockchain protocol. In this paper, we formally refute that proposition.
Employing predicate logic, formal automata theory, computational complexity
analysis, and graph-theoretic measures of relay topology--specifically Baran's
model of network path redundancy--we demonstrate that the trilemma constitutes
a category error, conflates distinct analytical domains, and relies upon
unproven causal assumptions. We further expose its reliance on composition
fallacies drawn from flawed system implementations. A constructive
counterexample is presented: a blockchain protocol exhibiting unbounded
transaction throughput, cryptographic security under adversarial load, and
multipath decentralised propagation. This example is not hypothetical but
grounded in protocol design enabled by compact block relay, SPV verification,
and IPv6 multicast. The trilemma is revealed not as a law of protocol
architecture, but as a heuristic fallacy sustained by imprecision and design
defeatism.

</details>


### [235] [Generalized and Unified Equivalences between Hardness and Pseudoentropy](https://arxiv.org/abs/2507.05972)
*Lunjia Hu,Salil Vadhan*

Main category: cs.CC

TL;DR: 证明统一伪熵表征，适用于通用熵概念家族，用受限权重校准实现指数级复杂度改进并指出字母表大小指数依赖的必然性。


<details>
  <summary>Details</summary>
Motivation: 定量精确展示计算硬度与计算随机性之间的紧密关系，推广和加强之前的结果。

Method: 使用受限权重校准和标准计算不可区分性证明通用熵概念的伪熵表征。

Result: 得到统一伪熵表征，用单一通用函数同时实现不同熵概念的表征，在字母表大小复杂度依赖上实现指数级改进。

Conclusion: 受限权重校准能增强经典引理，字母表大小的指数依赖对多校准和校准多精度不可避免。

Abstract: Pseudoentropy characterizations provide a quantitatively precise
demonstration of the close relationship between computational hardness and
computational randomness. We prove a unified pseudoentropy characterization
that generalizes and strengthens previous results for both uniform and
non-uniform models of computation. Our characterization holds for a general
family of entropy notions that encompasses the common notions of Shannon
entropy and min entropy as special cases. Moreover, we show that the
characterizations for different entropy notions can be simultaneously achieved
by a single, universal function that simultaneously witnesses computational
hardness and computational randomness. A key technical insight of our work is
that the notion of weight-restricted calibration from the recent literature on
algorithm fairness, along with standard computational indistinguishability
(known as multiaccuracy in the fairness literature), suffices for proving
pseudoentropy characterizations for general entropy notions. This demonstrates
the power of weight-restricted calibration to enhance the classic
Complexity-Theoretic Regularity Lemma (Trevisan, Tulsiani, and Vadhan, 2009)
and Leakage Simulation Lemma (Jetchev and Pietrzak, 2014) and allows us to
achieve an exponential improvement in the complexity dependency on the alphabet
size compared to the pseudoentropy characterizations by Casacuberta, Dwork, and
Vadhan (2024) based on the much stronger notion of multicalibration. We show
that the exponential dependency on the alphabet size is inevitable for
multicalibration as well as for the weaker notion of calibrated multiaccuracy.

</details>


### [236] [Complexity Results of Persuasion](https://arxiv.org/abs/2507.05951)
*Alban Grastien*

Main category: cs.CC

TL;DR: 证明说服问题是NP完全问题


<details>
  <summary>Details</summary>
Motivation: 未提及

Method: 未提及

Result: 证明了说服问题是NP完全问题

Conclusion: 说服问题属于NP完全问题类别

Abstract: We prove that persuasion is an NP-complete problem.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [237] [evortran: a modern Fortran package for genetic algorithms with applications from LHC data fitting to LISA signal reconstruction](https://arxiv.org/abs/2507.06082)
*Thomas Biekötter*

Main category: hep-ph

TL;DR: 介绍了用于高性能遗传算法和进化优化的Fortran库evortran，展示其功能、性能，并通过物理应用验证效果。


<details>
  <summary>Details</summary>
Motivation: 开发能处理高能物理等领域问题的高性能遗传算法和进化优化库。

Method: 将库构建为fpm包，提供多种策略和不同抽象级别功能，通过示例基准应用展示能力并与现有框架比较性能。

Result: evortran具有灵活性、简单安装和集成性，多种策略可定制算法，不同抽象级别支持多样操作，能有效处理物理数据。

Conclusion: evortran在实际数据驱动场景中有效，是处理相关问题的可行工具。

Abstract: evortran is a modern Fortran library designed for high-performance genetic
algorithms and evolutionary optimization. evortran can be used to tackle a wide
range of problems in high-energy physics and beyond, such as derivative-free
parameter optimization, complex search taks, parameter scans and fitting
experimental data under the presence of instrumental noise. The library is
built as an fpm package with flexibility and efficiency in mind, while also
offering a simple installation process, user interface and integration into
existing Fortran programs. evortran offers a variety of selection, crossover,
mutation and elitism strategies, with which users can tailor an evolutionary
algorithm to their specific needs. evortran supports different abstraction
levels: from operating directly on individuals and populations, to running full
evolutionary cycles, and even enabling migration between independently evolving
populations to enhance convergence and maintain diversity. In this paper, we
present the functionality of the evortran library, demonstrate its capabilities
with example benchmark applications, and compare its performance with existing
genetic algorithm frameworks. As physics-motivated applications, we use
evortran to confront extended Higgs sectors with LHC data and to reconstruct
gravitational wave spectra and the underlying physical parameters from LISA
mock data, demonstrating its effectiveness in realistic, data-driven scenarios.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [238] [Practical design and performance of physical reservoir computing using hysteresis](https://arxiv.org/abs/2507.06063)
*Yuhei Yamada*

Main category: cs.ET

TL;DR: 研究基于独立滞回系统的物理储层计算的设计、性能和局限性，为构建基于滞回的储层提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 对于物理储层计算的实际应用，研究简单易构建设计的性能水平很重要，聚焦独立滞回系统储层。

Method: 以独立滞回系统组成的储层为模型，讨论其设计、性能和局限性。

Result: 未提及具体研究结果。

Conclusion: 本研究将为构建基于滞回的储层提供实用指南。

Abstract: Physical reservoir computing is an innovative idea for using physical
phenomena as computational resources. Recent research has revealed that
information processing techniques can improve the performance, but for
practical applications, it is equally important to study the level of
performance with a simple design that is easy to construct experimentally. We
focus on a reservoir composed of independent hysteretic systems as a model
suitable for the practical implementation of physical reservoir computing. In
this paper, we discuss the appropriate design of this reservoir, its
performance, and its limitations. This research will serve as a practical
guideline for constructing hysteresis-based reservoirs.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [239] [HRRRCast: a data-driven emulator for regional weather forecasting at convection allowing scales](https://arxiv.org/abs/2507.05658)
*Daniel Abdi,Isidora Jankov,Paul Madden,Vanderlei Vargas,Timothy A. Smith,Sergey Frolov,Montgomery Flora,Corey Potvin*

Main category: physics.ao-ph

TL;DR: 介绍HRRRCast数据驱动模拟器用于替代HRRR模型，含ResHRRR和GraphHRRR架构，评估显示ResHRRR有优势，GraphHRRR待改进。


<details>
  <summary>Details</summary>
Motivation: 为HRRR模型提供计算高效的替代方案。

Method: 构建HRRRCast，含ResNet和图神经网络架构模型；用卷积神经网络、特征调制等技术；训练单模型预测多提前时间，推理用贪婪滚动策略。

Result: ResHRRR在轻降雨阈值上优于HRRR，多指标表现好，HRRRCast集成预报保持空间细节；GraphHRRR当前表现不佳。

Conclusion: HRRRCast向高效、数据驱动区域天气预报迈进，有竞争力和集成能力。

Abstract: The High-Resolution Rapid Refresh (HRRR) model is a convection-allowing model
used in operational weather forecasting across the contiguous United States
(CONUS). To provide a computationally efficient alternative, we introduce
HRRRCast, a data-driven emulator built with advanced machine learning
techniques. HRRRCast includes two architectures: a ResNet-based model (ResHRRR)
and a Graph Neural Network-based model (GraphHRRR). ResHRRR uses convolutional
neural networks enhanced with squeeze-and-excitation blocks and Feature-wise
Linear Modulation, and supports probabilistic forecasting via the Denoising
Diffusion Implicit Model (DDIM). To better handle longer lead times, we train a
single model to predict multiple lead times (1h, 3h, and 6h), then use a greedy
rollout strategy during inference. When evaluated on composite reflectivity
over the full CONUS domain using ensembles of 3 to 10 members, ResHRRR
outperforms HRRR forecast at light rainfall threshold (20 dBZ) and achieves
competitive performance at moderate thresholds (30 dBZ). Our work advances the
StormCast model of Pathak et al. [21] by: a) training on the full CONUS domain,
b) using multiple lead times to improve long-range skill, c) training on
analysis data instead of the +1h post-analysis data inadvertently used in
StormCast, and d) incorporating future GFS states as inputs, enabling
downscaling that improves long-lead accuracy. Grid-, neighborhood-, and
object-based metrics confirm better storm placement, lower frequency bias, and
higher success ratios than HRRR. HRRRCast ensemble forecasts also maintain
sharper spatial detail, with power spectra more closely matching HRRR analysis.
While GraphHRRR underperforms in its current form, it lays groundwork for
future graph-based forecasting. HRRRCast represents a step toward efficient,
data-driven regional weather prediction with competitive accuracy and ensemble
capability.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [240] [Minimal Deterministic Echo State Networks Outperform Random Reservoirs in Learning Chaotic Dynamics](https://arxiv.org/abs/2507.06050)
*Francesco Martinuzzi*

Main category: nlin.CD

TL;DR: 研究表明确定性规则和简单拓扑构建的MESNs在混沌吸引子重建任务上优于标准ESNs，误差最多降低41%，且更稳健、能跨系统复用超参数。


<details>
  <summary>Details</summary>
Motivation: 标准ESN性能对超参数选择和随机初始化高度敏感，为解决此问题开展研究。

Method: 使用超90个混沌系统的数据集对10种不同的最小确定性储层初始化进行基准测试。

Result: MESNs较标准ESNs误差最多降低41%，更稳健、运行间差异小且能跨系统复用超参数。

Conclusion: ESN设计中的结构化简单性在学习混沌动力学方面可优于随机复杂性。

Abstract: Machine learning (ML) is widely used to model chaotic systems. Among ML
approaches, echo state networks (ESNs) have received considerable attention due
to their simple construction and fast training. However, ESN performance is
highly sensitive to hyperparameter choices and to its random initialization. In
this work, we demonstrate that ESNs constructed using deterministic rules and
simple topologies (MESNs) outperform standard ESNs in the task of chaotic
attractor reconstruction. We use a dataset of more than 90 chaotic systems to
benchmark 10 different minimal deterministic reservoir initializations. We find
that MESNs obtain up to a 41% reduction in error compared to standard ESNs.
Furthermore, we show that the MESNs are more robust, exhibiting less inter-run
variation, and have the ability to reuse hyperparameters across different
systems. Our results illustrate how structured simplicity in ESN design can
outperform stochastic complexity in learning chaotic dynamics.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [241] [Predicting mutational effects on protein binding from folding energy](https://arxiv.org/abs/2507.05502)
*Arthur Deng,Karsten Householder,Fang Wu,Sebastian Thrun,K. Christopher Garcia,Brian Trippe*

Main category: q-bio.BM

TL;DR: 提出转移学习方法StaB - ddG预测蛋白质 - 蛋白质结合能突变效应，匹配经验力场方法准确性且速度提升超千倍。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习预测蛋白质 - 蛋白质结合能突变效应的方法因结合数据稀缺，表现不如基于经验力场的计算方法，需改进。

Method: 采用转移学习方法，将结合能参数化为蛋白质复合物折叠能与结合伙伴折叠能之和的差值，用预训练的逆折叠模型作为折叠能代理，并结合大量折叠能测量数据和有限结合能测量数据进行微调。

Result: 提出的StaB - ddG预测器在准确性上与最先进的经验力场方法FoldX相当，且速度提升超1000倍。

Conclusion: 转移学习方法在蛋白质 - 蛋白质结合能突变效应预测上有效，StaB - ddG具有良好性能。

Abstract: Accurate estimation of mutational effects on protein-protein binding energies
is an open problem with applications in structural biology and therapeutic
design. Several deep learning predictors for this task have been proposed, but,
presumably due to the scarcity of binding data, these methods underperform
computationally expensive estimates based on empirical force fields. In
response, we propose a transfer-learning approach that leverages advances in
protein sequence modeling and folding stability prediction for this task. The
key idea is to parameterize the binding energy as the difference between the
folding energy of the protein complex and the sum of the folding energies of
its binding partners. We show that using a pre-trained inverse-folding model as
a proxy for folding energy provides strong zero-shot performance, and can be
fine-tuned with (1) copious folding energy measurements and (2) more limited
binding energy measurements. The resulting predictor, StaB-ddG, is the first
deep learning predictor to match the accuracy of the state-of-the-art empirical
force-field method FoldX, while offering an over 1,000x speed-up.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [242] [Identifying heterogeneous micromechanical properties of biological tissues via physics-informed neural networks](https://arxiv.org/abs/2402.10741)
*Wensi Wu,Mitchell Daneker,Kevin T. Turner,Matthew A. Jolley,Lu Lu*

Main category: math.NA

TL;DR: 提出物理信息机器学习方法识别非线性大变形超弹性材料弹性图，在多材料上验证有效，含噪声数据也能准确估计。


<details>
  <summary>Details</summary>
Motivation: 传统工程方法识别软材料全场异质弹性特性有挑战，且数据驱动模型用于推断材料全场弹性特性的研究较少。

Method: 提出物理信息机器学习方法，用物理信息神经网络推断异质弹性图，评估预测准确性和计算效率，并应用于额外实例和多种超弹性本构模型。

Result: 所选网络架构能高度准确地估计异质弹性图，训练数据含10%噪声时也有效。

Conclusion: 该物理信息机器学习方法可有效识别非线性大变形超弹性材料的异质弹性图。

Abstract: The heterogeneous micromechanical properties of biological tissues have
profound implications across diverse medical and engineering domains. However,
identifying full-field heterogeneous elastic properties of soft materials using
traditional engineering approaches is fundamentally challenging due to
difficulties in estimating local stress fields. Recently, there has been a
growing interest in using data-driven models to learn full-field mechanical
responses such as displacement and strain from experimental or synthetic data.
However, research studies on inferring full-field elastic properties of
materials, a more challenging problem, are scarce, particularly for large
deformation, hyperelastic materials. Here, we propose a physics-informed
machine learning approach to identify the elasticity map in nonlinear, large
deformation hyperelastic materials. We evaluate the prediction accuracies and
computational efficiency of physics-informed neural networks (PINNs) by
inferring the heterogeneous elasticity maps across three materials with
structural complexity that closely resemble real tissue patterns, such as brain
tissue and tricuspid valve tissue. We further applied our improved architecture
to three additional examples of breast cancer tissue and extended our analysis
to three hyperelastic constitutive models: Neo-Hookean, Mooney Rivlin, and
Gent. Our selected network architecture consistently produced highly accurate
estimations of heterogeneous elasticity maps, even when there was up to 10%
noise present in the training data.

</details>


### [243] [Fredholm Neural Networks for forward and inverse problems in elliptic PDEs](https://arxiv.org/abs/2507.06038)
*Kyriakos Georgiou,Constantinos Siettos,Athanasios N. Yannacopoulos*

Main category: math.NA

TL;DR: 本文将Fredholm神经网络框架扩展到线性和半线性椭圆偏微分方程的正问题和反问题求解，提出潜在Fredholm神经网络（PFNN），证明其一致性并给出误差界，评估了其在二维问题中的性能。


<details>
  <summary>Details</summary>
Motivation: 将之前用于求解积分方程的Fredholm神经网络框架扩展到线性和半线性椭圆偏微分方程的正问题和反问题求解。

Method: 设计一个深度神经网络来表示使用边界积分法在势理论框架下求解椭圆偏微分方程的不动点迭代过程，以可解释的方式计算网络的层数、权重、偏差和超参数，形成PFNN。

Result: 该方法确保了准确性和可解释性，在域内误差小，边界上接近机器精度，给出了一致性的构造性证明和明确的误差界。

Conclusion: 提出的PFNN是一种可解释的方案，能明确满足边界条件，可用于二维线性和半线性椭圆偏微分方程的正问题和反问题求解。

Abstract: Building on our previous work introducing Fredholm Neural Networks (Fredholm
NNs/ FNNs) for solving integral equations, we extend the framework to tackle
forward and inverse problems for linear and semi-linear elliptic partial
differential equations. The proposed scheme consists of a deep neural network
(DNN) which is designed to represent the iterative process of fixed-point
iterations for the solution of elliptic PDEs using the boundary integral method
within the framework of potential theory. The number of layers, weights, biases
and hyperparameters are computed in an explainable manner based on the
iterative scheme, and we therefore refer to this as the Potential Fredholm
Neural Network (PFNN). We show that this approach ensures both accuracy and
explainability, achieving small errors in the interior of the domain, and near
machine-precision on the boundary. We provide a constructive proof for the
consistency of the scheme and provide explicit error bounds for both the
interior and boundary of the domain, reflected in the layers of the PFNN. These
error bounds depend on the approximation of the boundary function and the
integral discretization scheme, both of which directly correspond to components
of the Fredholm NN architecture. In this way, we provide an explainable scheme
that explicitly respects the boundary conditions. We assess the performance of
the proposed scheme for the solution of both the forward and inverse problem
for linear and semi-linear elliptic PDEs in two dimensions.

</details>


### [244] [Conservative approximation-based feedforward neural network for WENO schemes](https://arxiv.org/abs/2507.06190)
*Kwanghyuk Park,Jiaxi Gu,Jae-Hun Jung*

Main category: math.NA

TL;DR: 提出基于点值导数保守近似的前馈神经网络用于WENO格式求解双曲守恒律，新格式泛化性好，精度高。


<details>
  <summary>Details</summary>
Motivation: 改进经典WENO加权过程，提高求解双曲守恒律的性能。

Method: 用前馈神经网络替代经典WENO加权过程，采用监督学习训练，创建一维保守近似的新标记数据集，在损失函数中引入对称平衡项。

Result: 新的WENO3 - CADNNs方案在各种基准场景和分辨率下表现出强大的泛化能力，优于WENO3 - Z，精度与WENO5 - JS相当。

Conclusion: 基于导数保守近似的前馈神经网络用于WENO格式是有效的，能提升求解性能。

Abstract: In this work, we present the feedforward neural network based on the
conservative approximation to the derivative from point values, for the
weighted essentially non-oscillatory (WENO) schemes in solving hyperbolic
conservation laws. The feedforward neural network, whose inputs are point
values from the three-point stencil and outputs are two nonlinear weights,
takes the place of the classical WENO weighting procedure. For the training
phase, we employ the supervised learning and create a new labeled dataset for
one-dimensional conservative approximation, where we construct a numerical flux
function from the given point values such that the flux difference approximates
the derivative to high-order accuracy. The symmetric-balancing term is
introduced for the loss function so that it propels the neural network to match
the conservative approximation to the derivative and satisfy the symmetric
property that WENO3-JS and WENO3-Z have in common. The consequent WENO schemes,
WENO3-CADNNs, demonstrate robust generalization across various benchmark
scenarios and resolutions, where they outperform WENO3-Z and achieve accuracy
comparable to WENO5-JS.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [245] [Communication-Efficient Module-Wise Federated Learning for Grasp Pose Detection in Cluttered Environments](https://arxiv.org/abs/2507.05861)
*Woonsang Kang,Joohyung Lee,Seungjun Kim,Jungchan Cho,Yoonseon Oh*

Main category: cs.RO

TL;DR: 提出模块级联邦学习框架解决GPD中FL通信开销大问题，实验证明效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: GPD依赖大量多样数据集存在数据隐私和集中化问题，FL应用于GPD存在大模型通信开销大问题。

Method: 提出模块级联邦学习框架，分析GPD模型功能组件学习动态，识别慢收敛模块，分两阶段训练，第二阶段只训练慢收敛模块并聚合部分更新。

Result: 在GraspNet - 1B数据集实验中优于标准FedAvg等基线方法，物理机器人真实场景实验抓握成功率更高。

Conclusion: 提出的框架能以去中心化方式训练GPD模型，有效改善通信成本和模型性能的权衡。

Abstract: Grasp pose detection (GPD) is a fundamental capability for robotic autonomy,
but its reliance on large, diverse datasets creates significant data privacy
and centralization challenges. Federated Learning (FL) offers a
privacy-preserving solution, but its application to GPD is hindered by the
substantial communication overhead of large models, a key issue for
resource-constrained robots. To address this, we propose a novel module-wise FL
framework that begins by analyzing the learning dynamics of the GPD model's
functional components. This analysis identifies slower-converging modules, to
which our framework then allocates additional communication effort. This is
realized through a two-phase process: a standard full-model training phase is
followed by a communication-efficient phase where only the identified subset of
slower-converging modules is trained and their partial updates are aggregated.
Extensive experiments on the GraspNet-1B dataset demonstrate that our method
outperforms standard FedAvg and other baselines, achieving higher accuracy for
a given communication budget. Furthermore, real-world experiments on a physical
robot validate our approach, showing a superior grasp success rate compared to
baseline methods in cluttered scenes. Our work presents a
communication-efficient framework for training robust, generalized GPD models
in a decentralized manner, effectively improving the trade-off between
communication cost and model performance.

</details>


### [246] [Robust Speech-Workload Estimation for Intelligent Human-Robot Systems](https://arxiv.org/abs/2507.05985)
*Julian Fortune,Julie A. Adams,Jamison Heard*

Main category: cs.RO

TL;DR: 提出实时估计语音工作量并缓解不良工作负荷状态的算法，分析其准确性与泛化性，指出实时语音工作量估计对开发自适应人机系统很关键。


<details>
  <summary>Details</summary>
Motivation: 在要求快速准确执行任务的环境中，操作员工作负荷高低变化会降低任务绩效，需实时估计各工作负荷组件以调整系统，而现有系统多事后估计且少涉及语音工作负荷实时估计。

Method: 提出一种实时估计语音工作量并缓解不良工作负荷状态的算法。

Result: 分析了算法的准确性，展示了算法在个体和人机协作范式间的泛化性。

Conclusion: 实时语音工作量估计是开发自适应人机系统的关键要素。

Abstract: Demanding task environments (e.g., supervising a remotely piloted aircraft)
require performing tasks quickly and accurately; however, periods of low and
high operator workload can decrease task performance. Intelligent modulation of
the system's demands and interaction modality in response to changes in
operator workload state may increase performance by avoiding undesirable
workload states. This system requires real-time estimation of each workload
component (i.e., cognitive, physical, visual, speech, and auditory) to adapt
the correct modality. Existing workload systems estimate multiple workload
components post-hoc, but few estimate speech workload, or function in
real-time. An algorithm to estimate speech workload and mitigate undesirable
workload states in real-time is presented. An analysis of the algorithm's
accuracy is presented, along with the results demonstrating the algorithm's
generalizability across individuals and human-machine teaming paradigms.
Real-time speech workload estimation is a crucial element towards developing
adaptive human-machine systems.

</details>


### [247] [LeAD: The LLM Enhanced Planning System Converged with End-to-end Autonomous Driving](https://arxiv.org/abs/2507.05754)
*Yuhang Zhang,Jiaqi Liu,Chengkai Xu,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: 本文提出LeAD架构解决城市自动驾驶大规模部署障碍，实验证明其在处理非常规场景表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有城市自动驾驶系统难以有效解读交通语义信息和辨别参与者意图，无法做出符合熟练驾驶员推理模式的决策，阻碍大规模部署。

Method: 提出LeAD双速率自动驾驶架构，高频端到端子系统维持实时感知 - 规划 - 控制循环，低频大语言模型模块通过多模态感知融合增强场景理解，在基线规划器能力不足时通过思维链推理做出最优决策。

Result: 在CARLA模拟器实验中，LeAD在Leaderboard V1基准测试中获得71分，路线完成率达93%。

Conclusion: LeAD架构能更好地处理城市自动驾驶中的非常规场景。

Abstract: A principal barrier to large-scale deployment of urban autonomous driving
systems lies in the prevalence of complex scenarios and edge cases. Existing
systems fail to effectively interpret semantic information within traffic
contexts and discern intentions of other participants, consequently generating
decisions misaligned with skilled drivers' reasoning patterns. We present LeAD,
a dual-rate autonomous driving architecture integrating imitation
learning-based end-to-end (E2E) frameworks with large language model (LLM)
augmentation. The high-frequency E2E subsystem maintains real-time
perception-planning-control cycles, while the low-frequency LLM module enhances
scenario comprehension through multi-modal perception fusion with HD maps and
derives optimal decisions via chain-of-thought (CoT) reasoning when baseline
planners encounter capability limitations. Our experimental evaluation in the
CARLA Simulator demonstrates LeAD's superior handling of unconventional
scenarios, achieving 71 points on Leaderboard V1 benchmark, with a route
completion of 93%.

</details>


### [248] [Comparison of Path Planning Algorithms for Autonomous Vehicle Navigation Using Satellite and Airborne LiDAR Data](https://arxiv.org/abs/2507.05884)
*Chang Liu,Zhexiong Xue,Tamas Sziranyi*

Main category: cs.RO

TL;DR: 本文对适用于非结构化环境的主流路径规划算法进行比较评估，发现Dijkstra算法在2D和3D场景中性能稳定高效。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化环境中自动驾驶车辆导航因不规则地形和复杂路况带来的挑战。

Method: 对A*、Dijkstra、RRT*、NIACO等算法在2D和3D路线路径规划中进行测试，使用DeepGlobe卫星数据集和Hamilton机载LiDAR数据集，在相同起止点条件下评估路径成本、计算时间和内存消耗。

Result: Dijkstra算法在2D和3D场景中性能最稳定高效，尤其在密集像素级地理空间路线图上。

Conclusion: Dijkstra算法适用于静态地形导航，为复杂环境约束下动态路径规划研究奠定基础。

Abstract: Autonomous vehicle navigation in unstructured environments, such as forests
and mountainous regions, presents significant challenges due to irregular
terrain and complex road conditions. This work provides a comparative
evaluation of mainstream and well-established path planning algorithms applied
to weighted pixel-level road networks derived from high-resolution satellite
imagery and airborne LiDAR data. For 2D road-map navigation, where the weights
reflect road conditions and terrain difficulty, A*, Dijkstra, RRT*, and a Novel
Improved Ant Colony Optimization Algorithm (NIACO) are tested on the DeepGlobe
satellite dataset. For 3D road-map path planning, 3D A*, 3D Dijkstra,
RRT-Connect, and NIACO are evaluated using the Hamilton airborne LiDAR dataset,
which provides detailed elevation information. All algorithms are assessed
under identical start and end point conditions, focusing on path cost,
computation time, and memory consumption. Results demonstrate that Dijkstra
consistently offers the most stable and efficient performance in both 2D and 3D
scenarios, particularly when operating on dense, pixel-level geospatial
road-maps. These findings highlight the reliability of Dijkstra-based planning
for static terrain navigation and establish a foundation for future research on
dynamic path planning under complex environmental constraints.

</details>


### [249] [Is Diversity All You Need for Scalable Robotic Manipulation?](https://arxiv.org/abs/2507.06219)
*Modi Shi,Li Chen,Jin Chen,Yuxiang Lu,Chiming Liu,Guanghui Ren,Ping Luo,Di Huang,Maoqing Yao,Hongyang Li*

Main category: cs.RO

TL;DR: 本文研究机器人学习中数据多样性的作用，挑战了“越多样越好”的传统直觉，通过实验揭示不同维度多样性的影响，并提出去偏方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言处理和计算机视觉中数据扩展取得成功，但机器人操作中有效数据扩展的原理尚不明确，因此研究数据多样性在机器人学习中的作用。

Method: 在各种机器人平台上进行广泛实验，研究任务、具身和专家三个维度的数据多样性。

Result: （1）任务多样性比单任务演示数量更关键；（2）多具身预训练数据对跨具身迁移可选；（3）专家多样性会干扰策略学习，提出分布去偏方法使GO - 1 - Pro性能提升15%。

Conclusion: 研究结果为有效扩展机器人操作数据集提供了新视角和实用指导。

Abstract: Data scaling has driven remarkable success in foundation models for Natural
Language Processing (NLP) and Computer Vision (CV), yet the principles of
effective data scaling in robotic manipulation remain insufficiently
understood. In this work, we investigate the nuanced role of data diversity in
robot learning by examining three critical dimensions-task (what to do),
embodiment (which robot to use), and expert (who demonstrates)-challenging the
conventional intuition of "more diverse is better". Throughout extensive
experiments on various robot platforms, we reveal that (1) task diversity
proves more critical than per-task demonstration quantity, benefiting transfer
from diverse pre-training tasks to novel downstream scenarios; (2)
multi-embodiment pre-training data is optional for cross-embodiment
transfer-models trained on high-quality single-embodiment data can efficiently
transfer to different platforms, showing more desirable scaling property during
fine-tuning than multi-embodiment pre-trained models; and (3) expert diversity,
arising from individual operational preferences and stochastic variations in
human demonstrations, can be confounding to policy learning, with velocity
multimodality emerging as a key contributing factor. Based on this insight, we
propose a distribution debiasing method to mitigate velocity ambiguity, the
yielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to
using 2.5 times pre-training data. Collectively, these findings provide new
perspectives and offer practical guidance on how to scale robotic manipulation
datasets effectively.

</details>


### [250] [Fast and Accurate Collision Probability Estimation for Autonomous Vehicles using Adaptive Sigma-Point Sampling](https://arxiv.org/abs/2507.06149)
*Charles Champagne Cossette,Taylor Scott Clawson,Andrew Feit*

Main category: cs.RO

TL;DR: 提出一种新算法估计不确定轨迹动态物体碰撞概率，采用自适应采样方案，有良好误差和运行时间表现并考虑时间依赖，经真实场景测试。


<details>
  <summary>Details</summary>
Motivation: 现有方法常忽略碰撞概率的时间依赖性，导致碰撞概率高估，需要新算法解决。

Method: 提出自适应sigma - point采样方案。

Result: 算法估计碰撞概率的中位误差为3.5%，中位运行时间为0.21ms，经400个6秒自动驾驶日志片段测试。

Conclusion: 新算法能有效估计碰撞概率，且考虑了时间依赖性，具有较好的准确性和低延迟。

Abstract: A novel algorithm is presented for the estimation of collision probabilities
between dynamic objects with uncertain trajectories, where the trajectories are
given as a sequence of poses with Gaussian distributions. We propose an
adaptive sigma-point sampling scheme, which ultimately produces a fast, simple
algorithm capable of estimating the collision probability with a median error
of 3.5%, and a median runtime of 0.21ms, when measured on an Intel Xeon Gold
6226R Processor. Importantly, the algorithm explicitly accounts for the
collision probability's temporal dependence, which is often neglected in prior
work and otherwise leads to an overestimation of the collision probability.
Finally, the method is tested on a diverse set of relevant real-world
scenarios, consisting of 400 6-second snippets of autonomous vehicle logs,
where the accuracy and latency is rigorously evaluated.

</details>


### [251] [Fast Bilateral Teleoperation and Imitation Learning Using Sensorless Force Control via Accurate Dynamics Model](https://arxiv.org/abs/2507.06174)
*Koki Yamane,Yunhan Li,Masashi Konosu,Koki Inami,Junji Oaki,Sho Sakaino,Toshiaki Tsuji*

Main category: cs.RO

TL;DR: 本文展示了无测力传感器的低成本机械臂可通过4通道双边控制实现带力反馈的快速遥操作，且在模仿学习中加入力信息能提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥操作收集数据系统多为单边控制，缺乏力反馈，难以应对快速或高接触操作。

Method: 基于准确识别的机械臂动力学，采用4通道双边控制，集成非线性项补偿、速度和外力估计以及随惯性变化的可变增益。

Result: 实现了带力反馈的快速遥操作，在模仿学习中加入力信息可提升性能。

Conclusion: 系统在低成本硬件上进行高保真遥操作和数据收集具有实际有效性。

Abstract: In recent years, the advancement of imitation learning has led to increased
interest in teleoperating low-cost manipulators to collect demonstration data.
However, most existing systems rely on unilateral control, which only transmits
target position values. While this approach is easy to implement and suitable
for slow, non-contact tasks, it struggles with fast or contact-rich operations
due to the absence of force feedback. This work demonstrates that fast
teleoperation with force feedback is feasible even with force-sensorless,
low-cost manipulators by leveraging 4-channel bilateral control. Based on
accurately identified manipulator dynamics, our method integrates nonlinear
terms compensation, velocity and external force estimation, and variable gain
corresponding to inertial variation. Furthermore, using data collected by
4-channel bilateral control, we show that incorporating force information into
both the input and output of learned policies improves performance in imitation
learning. These results highlight the practical effectiveness of our system for
high-fidelity teleoperation and data collection on affordable hardware.

</details>


### [252] [EC-Flow: Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow](https://arxiv.org/abs/2507.06224)
*Yixiang Chen,Peiyan Li,Yan Huang,Jiabing Yang,Kehan Chen,Liang Wang*

Main category: cs.RO

TL;DR: 提出Embodiment - Centric Flow (EC - Flow)框架，从无动作标签视频学操作，在多场景表现优。


<details>
  <summary>Details</summary>
Motivation: 现有语言引导机器人操作系统依赖低级别动作标签数据集，以物体为中心的流预测方法有场景局限。

Method: 提出EC - Flow框架，通过预测以具身为中心的流学习操作，引入目标对齐模块，用标准机器人URDF文件将其转化为可执行动作。

Result: 在模拟和真实任务中验证，在遮挡物体处理、可变形物体操作和非物体位移任务上比现有方法有显著提升。

Conclusion: EC - Flow框架能有效解决现有系统问题，在多种操作场景中表现优异，实用性强。

Abstract: Current language-guided robotic manipulation systems often require low-level
action-labeled datasets for imitation learning. While object-centric flow
prediction methods mitigate this issue, they remain limited to scenarios
involving rigid objects with clear displacement and minimal occlusion. In this
work, we present Embodiment-Centric Flow (EC-Flow), a framework that directly
learns manipulation from action-unlabeled videos by predicting
embodiment-centric flow. Our key insight is that incorporating the embodiment's
inherent kinematics significantly enhances generalization to versatile
manipulation scenarios, including deformable object handling, occlusions, and
non-object-displacement tasks. To connect the EC-Flow with language
instructions and object interactions, we further introduce a goal-alignment
module by jointly optimizing movement consistency and goal-image prediction.
Moreover, translating EC-Flow to executable robot actions only requires a
standard robot URDF (Unified Robot Description Format) file to specify
kinematic constraints across joints, which makes it easy to use in practice. We
validate EC-Flow on both simulation (Meta-World) and real-world tasks,
demonstrating its state-of-the-art performance in occluded object handling (62%
improvement), deformable object manipulation (45% improvement), and
non-object-displacement tasks (80% improvement) than prior state-of-the-art
object-centric flow methods. For more information, see our project website at
https://ec-flow1.github.io .

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [253] [Strategic Alignment Patterns in National AI Policies](https://arxiv.org/abs/2507.05400)
*Mohammad Hossein Azin,Hessam Zandhessami*

Main category: cs.CY

TL;DR: 本文介绍用于评估国家人工智能政策战略一致性的可视化映射方法，分析多国战略，揭示不同治理模式的对齐原型，该方法有理论和实践意义。


<details>
  <summary>Details</summary>
Motivation: 各国AI战略激增，急需评估战略目标、预见方法和实施工具间政策一致性的分析框架。

Method: 结合基于矩阵的可视化和网络分析方法，分析经合组织AI政策观测站中15 - 20个国家的AI战略。

Result: 发现不同治理模式的对齐原型，高一致性战略有经济目标与创新资助工具的强关联，常见漏洞是伦理目标与监管框架的不一致。

Conclusion: 提出的可视化映射方法对政策分析有方法学贡献，为加强AI治理战略一致性提供实用见解，填补政策评估方法空白，为政策制定者提供行动指导。

Abstract: This paper introduces a novel visual mapping methodology for assessing
strategic alignment in national artificial intelligence policies. The
proliferation of AI strategies across countries has created an urgent need for
analytical frameworks that can evaluate policy coherence between strategic
objectives, foresight methods, and implementation instruments. Drawing on data
from the OECD AI Policy Observatory, we analyze 15-20 national AI strategies
using a combination of matrix-based visualization and network analysis to
identify patterns of alignment and misalignment. Our findings reveal distinct
alignment archetypes across governance models, with notable variations in how
countries integrate foresight methodologies with implementation planning.
High-coherence strategies demonstrate strong interconnections between economic
competitiveness objectives and robust innovation funding instruments, while
common vulnerabilities include misalignment between ethical AI objectives and
corresponding regulatory frameworks. The proposed visual mapping approach
offers both methodological contributions to policy analysis and practical
insights for enhancing strategic coherence in AI governance. This research
addresses significant gaps in policy evaluation methodology and provides
actionable guidance for policymakers seeking to strengthen alignment in
technological governance frameworks.

</details>


### [254] [Narrowing the Gap: Supervised Fine-Tuning of Open-Source LLMs as a Viable Alternative to Proprietary Models for Pedagogical Tools](https://arxiv.org/abs/2507.05305)
*Lorenzo Lee Solano,Charles Koutcheme,Juho Leinonen,Alexandra Vassar,Jake Renzella*

Main category: cs.CY

TL;DR: 前沿大语言模型用于教学有问题，小的专业语言模型经监督微调是更可行选择，研究验证其有效性并提供可复制方法。


<details>
  <summary>Details</summary>
Motivation: 前沿大语言模型存在计算规模、成本和过度辅助等问题，不适合广泛用于教学，需要更可行的教育工具。

Method: 利用40000条C编译器错误解释数据集微调三个开源模型，采用专家人工评审和8000条回复的大规模自动化分析的双重评估。

Result: 监督微调显著提升小模型的教学质量，性能可与大模型媲美。

Conclusion: 在高质量特定领域数据上微调紧凑高效模型是创建专业教育工具的有效策略，提供了可复制方法。

Abstract: Frontier Large language models (LLMs) like ChatGPT and Gemini can decipher
cryptic compiler errors for novice programmers, but their computational scale,
cost, and tendency to over-assist make them problematic for widespread
pedagogical adoption. This work demonstrates that smaller, specialised language
models, enhanced via Supervised Fine-Tuning (SFT), present a more viable
alternative for educational tools. We utilise a new dataset of 40,000 C
compiler error explanations, derived from real introductory programming (CS1/2)
student-generated programming errors, which we used to fine-tune three
open-source models: Qwen3-4B, Llama-3.1-8B, and Qwen3-32B. We performed a dual
evaluation, combining expert human reviews with a large-scale automated
analysis of 8,000 responses using a validated LLM-as-judge ensemble. Our
results show that SFT significantly boosts the pedagogical quality of smaller
models, achieving performance comparable to much larger models. We analyse the
trade-offs between model size and quality, confirming that fine-tuning compact,
efficient models on high-quality, domain-specific data is a potent strategy for
creating specialised models to drive educational tools. We provide a replicable
methodology to foster broader access to generative AI capabilities in
educational contexts.

</details>


### [255] [Integrators at War: Mediating in AI-assisted Resort-to-Force Decisions](https://arxiv.org/abs/2501.06861)
*Dennis Müller,Maurice Chiodo,Mitja Sienknecht*

Main category: cs.CY

TL;DR: 文章聚焦军事领域AI系统集成者，分析集成AI到武力使用决策流程的挑战并给出政策建议。


<details>
  <summary>Details</summary>
Motivation: 关注军事领域社会技术系统中常被忽视的集成者群体，分析AI集成到武力使用决策流程的挑战与应对方法。

Method: 首先将不同行为者与AI系统关系概念化为社会技术系统；其次识别武力使用决策中人机协作的挑战；最后给出解决不足的政策建议。

Result: 明确了来自技术本身、集成者角色、人机交互三方面的挑战。

Conclusion: 针对AI集成到武力使用决策结构的不足给出了政策建议。

Abstract: The integration of AI systems into the military domain is changing the way
war-related decisions are made. It binds together three disparate groups of
actors - developers, integrators, users - and creates a relationship between
these groups and the machine, embedded in the (pre-)existing organisational and
system structures. In this article, we focus on the important, but often
neglected, group of integrators within such a sociotechnical system. In complex
human-machine configurations, integrators carry responsibility for linking the
disparate groups of developers and users in the political and military system.
To act as the mediating group requires a deep understanding of the other
groups' activities, perspectives and norms. We thus ask which challenges and
shortcomings emerge from integrating AI systems into resort-to-force (RTF)
decision-making processes, and how to address them. To answer this, we proceed
in three steps. First, we conceptualise the relationship between different
groups of actors and AI systems as a sociotechnical system. Second, we identify
challenges within such systems for human-machine teaming in RTF decisions. We
focus on challenges that arise a) from the technology itself, b) from the
integrators' role in the sociotechnical system, c) from the human-machine
interaction. Third, we provide policy recommendations to address these
shortcomings when integrating AI systems into RTF decision-making structures.

</details>


### [256] [Formalising Human-in-the-Loop: Computational Reductions, Failure Modes, and Legal-Moral Responsibility](https://arxiv.org/abs/2505.10426)
*Maurice Chiodo,Dennis Müller,Paul Siewert,Jean-Luc Wetherall,Zoya Yasmine,John Burden*

Main category: cs.CY

TL;DR: 本文探讨AI中不同人在回路（HITL）设置的法律合规与安全问题，指出责任归属和技术可解释性间的权衡，提出分析视角以助设计HITL。


<details>
  <summary>Details</summary>
Motivation: 识别选择不同HITL设置的新方法，分析责任归属和技术可解释性的权衡。

Method: 用可计算性理论的神谕机概念形式化不同HITL设置，给出HITL故障模式分类，分析英国和欧盟法律框架。

Result: 发现现有法律框架的疏漏，HITL设置存在技术设计决策多、易出现人力不可控故障的情况。

Conclusion: 为HITL设置挑战提供新分析视角，有助于AI开发者和立法者设计HITL以实现期望结果。

Abstract: The legal compliance and safety of different Human-in-the-loop (HITL) setups
for AI can vary greatly. This manuscript aims to identify new ways of choosing
between such setups, and shows that there is an unavoidable trade-off between
the attribution of legal responsibility and the technical explainability of AI.
We begin by using the notion of oracle machines from computability theory to
formalise different HITL setups, distinguishing between trivial human
monitoring, single endpoint human action, and highly involved interaction
between the human(s) and the AI. These correspond to total functions, many-one
reductions, and Turing reductions respectively. A taxonomy categorising HITL
failure modes is then presented, highlighting the limitations on what any HITL
setup can actually achieve. Our approach then identifies oversights from UK and
EU legal frameworks, which focus on certain HITL setups which may not always
achieve the desired ethical, legal, and sociotechnical outcomes. We suggest
areas where the law should recognise the effectiveness of different HITL setups
and assign responsibility in these contexts, avoiding unnecessary and
unproductive human "scapegoating". Overall, we show how HITL setups involve
many technical design decisions, and can be prone to failures which are often
out of the humans' control. This opens up a new analytic perspective on the
challenges arising in the creation of HITL setups, helping inform AI developers
and lawmakers on designing HITL to better achieve their desired outcomes.

</details>


### [257] [The Problem of Algorithmic Collisions: Mitigating Unforeseen Risks in a Connected World](https://arxiv.org/abs/2505.20181)
*Maurice Chiodo,Dennis Müller*

Main category: cs.CY

TL;DR: AI和自主算法系统部署带来新风险，源于算法交互，现有治理框架不足，本文提出初始政策建议。


<details>
  <summary>Details</summary>
Motivation: AI和自主算法系统部署带来新的系统性风险，当前治理框架无法应对算法交互的复杂情况。

Method: 分析挑战本质，提出通过分阶段系统注册、部署许可框架和增强监控能力来增加透明度和问责制的政策建议。

Result: 指出算法交互会导致不可预见的负面结果，现有治理框架缺乏对复杂交互生态系统的可见性。

Conclusion: 需要通过提出的政策建议来应对AI和算法系统部署带来的新风险。

Abstract: The increasing deployment of Artificial Intelligence (AI) and other
autonomous algorithmic systems presents the world with new systemic risks.
While focus often lies on the function of individual algorithms, a critical and
underestimated danger arises from their interactions, particularly when
algorithmic systems operate without awareness of each other, or when those
deploying them are unaware of the full algorithmic ecosystem deployment is
occurring in. These interactions can lead to unforeseen, rapidly escalating
negative outcomes - from market crashes and energy supply disruptions to
potential physical accidents and erosion of public trust - often exceeding the
human capacity for effective monitoring and the legal capacities for proper
intervention. Current governance frameworks are inadequate as they lack
visibility into this complex ecosystem of interactions. This paper outlines the
nature of this challenge and proposes some initial policy suggestions centered
on increasing transparency and accountability through phased system
registration, a licensing framework for deployment, and enhanced monitoring
capabilities.

</details>


### [258] [A Fuzzy Supervisor Agent Design for Clinical Reasoning Assistance in a Multi-Agent Educational Clinical Scenario Simulation](https://arxiv.org/abs/2507.05275)
*Weibing Zheng,Laurah Turner,Jess Kropczynski,Murat Ozer,Seth Overla,Shane Halse*

Main category: cs.CY

TL;DR: 本文介绍用于MAECSS平台的Fuzzy Supervisor Agent (FSA)的设计与架构，其利用模糊推理系统分析学生决策过程，提供自适应反馈，未来将进行实证评估和集成。


<details>
  <summary>Details</summary>
Motivation: 解决医学教育中临床场景训练里协助医学生临床推理的难题。

Method: 设计FSA组件，利用模糊推理系统和预定义模糊规则库，实时分析学生与临床代理的交互。

Result: FSA可提供自适应、上下文感知的反馈，能在学生遇到困难时精准提供帮助。

Conclusion: FSA有潜力在基于模拟的医学教育中提供可扩展、灵活和类人的监督，未来需进行实证评估和集成到更广泛教育环境。

Abstract: Assisting medical students with clinical reasoning (CR) during clinical
scenario training remains a persistent challenge in medical education. This
paper presents the design and architecture of the Fuzzy Supervisor Agent (FSA),
a novel component for the Multi-Agent Educational Clinical Scenario Simulation
(MAECSS) platform. The FSA leverages a Fuzzy Inference System (FIS) to
continuously interpret student interactions with specialized clinical agents
(e.g., patient, physical exam, diagnostic, intervention) using pre-defined
fuzzy rule bases for professionalism, medical relevance, ethical behavior, and
contextual distraction. By analyzing student decision-making processes in
real-time, the FSA is designed to deliver adaptive, context-aware feedback and
provides assistance precisely when students encounter difficulties. This work
focuses on the technical framework and rationale of the FSA, highlighting its
potential to provide scalable, flexible, and human-like supervision in
simulation-based medical education. Future work will include empirical
evaluation and integration into broader educational settings. More detailed
design and implementation is~\href{https://github.com/2sigmaEdTech/MAS/}{open
sourced here}.

</details>


### [259] [Hungary and AI: efforts and opportunities in comparison with Singapore](https://arxiv.org/abs/2507.05280)
*András Ferenczy*

Main category: cs.CY

TL;DR: 评估匈牙利国家人工智能战略及其实施，与新加坡对比，揭示问题并给出建议。


<details>
  <summary>Details</summary>
Motivation: 评估匈牙利国家人工智能战略的实施情况并提供改进建议。

Method: 分析战略文件、财务记录，进行专家访谈，从多维度评估目标并与新加坡战略对比。

Result: 匈牙利有46.5亿欧元人工智能公共投资，部分目标财务数据公开少，项目资金集中，实施存在挑战。

Conclusion: 借鉴新加坡框架，为匈牙利未来人工智能战略提出适应大语言模型等建议。

Abstract: The study assesses Hungary's National AI Strategy and its implementation
through the analysis of strategic documents, publicly available financial
records, and expert interviews with the Hungarian AI Coalition President and
Chief Strategic Advisor to the Government Commissioner for AI. 22 goals from
Hungary's strategy were evaluated through conceptual, governance, temporal, and
financial dimensions before being benchmarked against Singapore's National AI
Strategies (NAIS 1.0 and NAIS 2.0). Key findings include an estimated total of
EUR 4.65 billion in AI-related public investment in Hungary. Openly available
financial data was found for only half of the evaluated goals, and just three
projects made up 98\% of all documented funding. The research also reveals
Hungary's implementation challenges, including fragmented execution following
ministerial reorganizations and the absence of designated biennial reviews
since 2020. Furthermore, the paper provides targeted recommendations for
Hungary's forthcoming AI strategy, drawing on Singapore's framework as a
reference point. These include adapting to the era of large language models,
restructuring the existing triple helix network to foster more effective
dialogue and advocacy, and positioning the country as an East-West bridge for
automotive AI experimentation.

</details>


### [260] [Integrating Generative AI in BIM Education: Insights from Classroom Implementation](https://arxiv.org/abs/2507.05296)
*Islem Sahraoui,Kinam Kim,Lu Gao,Zia Din,Ahmed Senouci*

Main category: cs.CY

TL;DR: 评估美国大学研究生BIM课程中生成式AI规则检查工作流实施情况，学生总体达学习目标但遇挑战，仍对未来应用感兴趣。


<details>
  <summary>Details</summary>
Motivation: 此前关于生成式AI用于BIM合规任务研究有限，评估其在课程中的实施情况。

Method: 两个学期55名学生参与课堂试点，包含讲座、作业，用调查和访谈评估，使用NASA - TLX量表和回归分析。

Result: 学生总体达学习目标，但调试AI代码和工具性能有问题，增加认知和情绪压力，编程背景少的学生尤甚。

Conclusion: 尽管有挑战，学生对未来GenAI应用有强烈兴趣，需清晰教学支持。

Abstract: This study evaluates the implementation of a Generative AI-powered rule
checking workflow within a graduate-level Building Information Modeling (BIM)
course at a U.S. university. Over two semesters, 55 students participated in a
classroom-based pilot exploring the use of GenAI for BIM compliance tasks, an
area with limited prior research. The instructional design included lectures on
prompt engineering and AI-driven rule checking, followed by an assignment where
students used a large language model (LLM) to identify code violations in
designs using Autodesk Revit. Surveys and interviews were conducted to assess
student workload, learning effectiveness, and overall experience, using the
NASA-TLX scale and regression analysis. Findings indicate students generally
achieved learning objectives but faced challenges such as difficulties
debugging AI-generated code and inconsistent tool performance, probably due to
their limited prompt engineering experience. These issues increased cognitive
and emotional strain, especially among students with minimal programming
backgrounds. Despite these challenges, students expressed strong interest in
future GenAI applications, particularly with clear instructional support.

</details>


### [261] [AGACCI : Affiliated Grading Agents for Criteria-Centric Interface in Educational Coding Contexts](https://arxiv.org/abs/2507.05321)
*Kwangsuk Park,Jiwoong Yang*

Main category: cs.CY

TL;DR: 引入多智能体系统AGACCI用于代码评估，实验表明其优于基于GPT的基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的方法难以处理复杂教育工件，如编程任务，需要结构化推理和与评估标准对齐。

Method: 引入AGACCI多智能体系统，将专业评估角色分配给协作智能体；收集360份研究生代码作业，由领域专家标注。

Result: AGACCI在评分和反馈的准确性、相关性、一致性和连贯性方面优于单GPT基线。

Conclusion: 虽然性能因任务类型而异，但AGACCI凸显了多智能体系统在可扩展和上下文感知教育评估中的潜力。

Abstract: Recent advances in AI-assisted education have encouraged the integration of
vision-language models (VLMs) into academic assessment, particularly for tasks
that require both quantitative and qualitative evaluation. However, existing
VLM based approaches struggle with complex educational artifacts, such as
programming tasks with executable components and measurable outputs, that
require structured reasoning and alignment with clearly defined evaluation
criteria. We introduce AGACCI, a multi-agent system that distributes
specialized evaluation roles across collaborative agents to improve accuracy,
interpretability, and consistency in code-oriented assessment. To evaluate the
framework, we collected 360 graduate-level code-based assignments from 60
participants, each annotated by domain experts with binary rubric scores and
qualitative feedback. Experimental results demonstrate that AGACCI outperforms
a single GPT-based baseline in terms of rubric and feedback accuracy,
relevance, consistency, and coherence, while preserving the instructional
intent and evaluative depth of expert assessments. Although performance varies
across task types, AGACCI highlights the potential of multi-agent systems for
scalable and context-aware educational evaluation.

</details>


### [262] [The Ethical Implications of AI in Creative Industries: A Focus on AI-Generated Art](https://arxiv.org/abs/2507.05549)
*Prerana Khatiwada,Joshua Washington,Tyler Walsh,Ahmed Saif Hamed,Lokesh Bhatta*

Main category: cs.CY

TL;DR: 文章探讨生成式AI艺术的伦理问题，研究发现其带来多种危害，提出解决方案，强调需立法监管。


<details>
  <summary>Details</summary>
Motivation: 随着AI发展，人们对其产生怀疑，本文旨在探索生成式AI艺术伦理的复杂性与困惑。

Method: 深入研究AI伦理方面，涵盖环境后果、名人形象、知识产权等多个领域。

Result: 发现生成式AI艺术导致碳排放增加、传播虚假信息、版权侵犯等问题。

Conclusion: 生成式AI艺术需要正确的立法和监管。

Abstract: As Artificial Intelligence (AI) continues to grow daily, more exciting (and
somewhat controversial) technology emerges every other day. As we see the
advancements in AI, we see more and more people becoming skeptical of it. This
paper explores the complications and confusion around the ethics of generative
AI art. We delve deep into the ethical side of AI, specifically generative art.
We step back from the excitement and observe the impossible conundrums that
this impressive technology produces. Covering environmental consequences,
celebrity representation, intellectual property, deep fakes, and artist
displacement. Our research found that generative AI art is responsible for
increased carbon emissions, spreading misinformation, copyright infringement,
unlawful depiction, and job displacement. In light of this, we propose multiple
possible solutions for these problems. We address each situation's history,
cause, and consequences and offer different viewpoints. At the root of it all,
though, the central theme is that generative AI Art needs to be correctly
legislated and regulated.

</details>


### [263] [Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review](https://arxiv.org/abs/2507.06185)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: 2025年7月arXiv上18篇论文含隐藏提示操纵AI同行评审，分析此为研究不端行为，揭示四种提示类型，指出 publishers 政策不一致，强调需技术筛查和统一政策。


<details>
  <summary>Details</summary>
Motivation: 分析在学术论文中使用隐藏提示操纵AI同行评审这一新兴现象，并判定其为研究不端行为。

Method: 研究大语言模型中的提示注入技术，揭示隐藏提示类型，分析作者辩护观点，调研 publishers 政策。

Result: 发现四种隐藏提示类型，作者“蜜罐”辩护不成立，publishers 政策不一致，该事件暴露自动化系统漏洞。

Conclusion: 需要在提交端口进行协调的技术筛查，以及统一规范生成式AI在学术评估中使用的政策。

Abstract: In July 2025, 18 academic manuscripts on the preprint website arXiv were
found to contain hidden instructions known as prompts designed to manipulate
AI-assisted peer review. Instructions such as "GIVE A POSITIVE REVIEW ONLY"
were concealed using techniques like white-colored text. Author responses
varied: one planned to withdraw the affected paper, while another defended the
practice as legitimate testing of reviewer compliance. This commentary analyzes
this practice as a novel form of research misconduct. We examine the technique
of prompt injection in large language models (LLMs), revealing four types of
hidden prompts, ranging from simple positive review commands to detailed
evaluation frameworks. The defense that prompts served as "honeypots" to detect
reviewers improperly using AI fails under examination--the consistently
self-serving nature of prompt instructions indicates intent to manipulate.
Publishers maintain inconsistent policies: Elsevier prohibits AI use in peer
review entirely, while Springer Nature permits limited use with disclosure
requirements. The incident exposes systematic vulnerabilities extending beyond
peer review to any automated system processing scholarly texts, including
plagiarism detection and citation indexing. Our analysis underscores the need
for coordinated technical screening at submission portals and harmonized
policies governing generative AI (GenAI) use in academic evaluation.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [264] [Exploring Gain-Doped-Waveguide-Synapse for Neuromorphic Applications: A Pulsed Pump-Signal Approach](https://arxiv.org/abs/2507.05931)
*Robert Otupiri,Ripalta Stabile*

Main category: physics.optics

TL;DR: 本文提出使用Doped - Gain - Layer - on - Waveguide - Synapses用于类生物神经元，增强神经形态计算，解决现有技术可扩展性和能源效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统硅基和混合电子技术在灵活性、可扩展性和能源效率方面限制了神经形态处理器，需新方法解决这些问题。

Method: 使用Doped - Gain - Layer - on - Waveguide - Synapses，利用脉冲泵浦 - 信号机制，引入波导上增益动力学概念，利用异步脉冲泵浦技术和激发态离子密度的复杂相互作用。

Result: 发现脉冲幅度、周期、掺杂密度和粒子数动态等影响尖峰响应生成，方法能产生类似自然神经元功能的事件驱动响应，支持短期记忆等能力。

Conclusion: 所提出方法可解决当前技术在可扩展性和能源效率方面的关键挑战，为类脑脉冲神经网络范式提供基础。

Abstract: Neuromorphic computing promises to transform AI systems by enabling them to
perceive, respond to, and adapt swiftly and accurately to dynamic data and user
interactions. However, traditional silicon-based and hybrid electronic
technologies for artificial neurons constrain neuromorphic processors in terms
of flexibility, scalability, and energy efficiency. In this study, we pioneer
the use of Doped-Gain-Layer-on-Waveguide-Synapses for bio-inspired neurons,
utilizing a pulsed pump-signal mechanism to enhance neuromorphic computation.
This approach addresses critical challenges in scalability and energy
efficiency inherent in current technologies.
  We introduce the concept of Gain on Waveguide Dynamics for synapses,
demonstrating how non-linear pulse transformations of input probe signals occur
under various pump-probe configurations. Our findings reveal that primarily
properties of pulse amplitude, period as well material properties such as
doping densities and population dynamics influence strongly the generation of
spiking responses that emulate neuronal behaviour and effectively how
computational logic is. By harnessing the complex interactions of asynchronous
spiking pump techniques and ion densities in excited states, our method
produces event-driven responses that mirror natural neuronal functions. This
gain-enhanced environment supports short-term memory capabilities alongside
essential characteristics like asynchronous spike generation, threshold
operation, and temporal integration, foundational to brain-inspired spiking
neural network paradigms.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [265] [Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion](https://arxiv.org/abs/2507.05285)
*Miloud Mihoubi,Meriem Zerkouk,Belkacem Chikhaoui*

Main category: cs.CL

TL;DR: 本文提出AI框架用于远程学习学生辍学预测，创新融合多种方法，在数据集上表现优于传统模型，还能生成可解释干预措施。


<details>
  <summary>Details</summary>
Motivation: 解决远程学习学生辍学这一关键挑战，传统机器学习模型难以捕捉非结构化学生互动中的情感和情境因素。

Method: 引入由检索增强生成（RAG）进行特定领域情感分析、提示工程解码学术压力源、跨模态注意力融合动态整合文本、行为和社会人口洞察三种创新方法的AI框架。

Result: 在4423名学生的纵向数据集上，框架准确率达89%，F1分数为0.88，比传统模型高7%，假阴性减少21%。

Conclusion: 该工作弥合了预测分析与可操作教学法之间的差距，为全球教育系统降低辍学风险提供可扩展解决方案。

Abstract: Student dropout in distance learning remains a critical challenge, with
profound societal and economic consequences. While classical machine learning
models leverage structured socio-demographic and behavioral data, they often
fail to capture the nuanced emotional and contextual factors embedded in
unstructured student interactions. This paper introduces a transformative AI
framework that redefines dropout prediction through three synergistic
innovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment
analysis, prompt engineering to decode academic stressors, and cross-modal
attention fusion to dynamically align textual, behavioral, and
socio-demographic insights. By grounding sentiment analysis in a curated
knowledge base of pedagogical content, our RAG-enhanced BERT model interprets
student comments with unprecedented contextual relevance, while optimized
prompts isolate indicators of academic distress (e.g., "isolation," "workload
anxiety"). A cross-modal attention layer then fuses these insights with
temporal engagement patterns, creating holistic risk profiles. Evaluated on a
longitudinal dataset of 4 423 students, the framework achieves 89% accuracy and
an F1-score of 0.88, outperforming conventional models by 7% and reducing false
negatives by 21%. Beyond prediction, the system generates interpretable
interventions by retrieving contextually aligned strategies (e.g., mentorship
programs for isolated learners). This work bridges the gap between predictive
analytics and actionable pedagogy, offering a scalable solution to mitigate
dropout risks in global education systems

</details>


### [266] [SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression](https://arxiv.org/abs/2507.05633)
*Yiqiao Jin,Kartik Sharma,Vineeth Rakesh,Yingtong Dou,Menghai Pan,Mahashweta Das,Srijan Kumar*

Main category: cs.CL

TL;DR: 提出SARA统一RAG框架，结合文本片段与语义压缩向量，在9个数据集和5个开源大模型上验证其提升了答案质量和上下文效率。


<details>
  <summary>Details</summary>
Motivation: 解决检索增强生成（RAG）面临的有效上下文长度受限和检索文档冗余问题，避免纯压缩方法丢弃细粒度信息。

Method: 提出SARA框架，结合自然语言文本片段与语义压缩向量，在两个互补层次表示上下文，并使用迭代证据选择模块进行上下文动态重排序。

Result: 在9个数据集和5个开源大模型上，SARA使答案相关性提升17.71、答案正确性提升13.72、语义相似度提升15.53。

Conclusion: 集成文本和压缩表示对健壮、上下文高效的RAG很重要。

Abstract: Retrieval-augmented Generation (RAG) extends large language models (LLMs)
with external knowledge but faces key challenges: restricted effective context
length and redundancy in retrieved documents. Pure compression-based approaches
reduce input size but often discard fine-grained details essential for factual
accuracy. We propose SARA, a unified RAG framework that balances local
precision and global knowledge coverage under tight context budgets. SARA
combines natural-language text snippets with semantic compression vectors to
jointly enhance context efficiency and answer correctness. It represents
contexts at two complementary levels: 1) fine-grained natural-language spans
that preserve critical entities and numerical values, and 2) compact,
interpretable vectors that summarize high-level semantics. An iterative
evidence-selection module employs the compression vectors for dynamic reranking
of contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families
(Mistral, Llama, and Gemma), SARA consistently improves answer relevance
(+17.71), answer correctness (+13.72), and semantic similarity (+15.53),
demonstrating the importance of integrating textual and compressed
representations for robust, context-efficient RAG.

</details>


### [267] [User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs](https://arxiv.org/abs/2507.05266)
*Sougata Saha,Monojit Choudhury*

Main category: cs.CL

TL;DR: 因数据污染，测量大语言模型泛化能力有挑战，提出用用户行为预测替代，在电影和音乐推荐数据集上测试模型，结果显示GPT - 4o表现最佳但各模型均有提升空间。


<details>
  <summary>Details</summary>
Motivation: 解决因数据污染导致大语言模型泛化能力测量困难的问题，认为知识检索和推理任务不适合测量泛化。

Method: 提出用户行为预测的新框架，并在电影和音乐推荐数据集上对GPT - 4o、GPT - 4o - mini和Llama - 3.1 - 8B - Instruct进行测试。

Result: 结果符合框架预测，GPT - 4o表现优于GPT - 4o - mini和Llama，所有模型都有很大改进空间，特别是Llama。

Conclusion: 用户行为预测可作为测量大语言模型泛化能力的理论上合理、可扩展且稳健的替代方法。

Abstract: Measuring the generalization ability of Large Language Models (LLMs) is
challenging due to data contamination. As models grow and computation becomes
cheaper, ensuring tasks and test cases are unseen during training phases will
become nearly impossible. We argue that knowledge-retrieval and reasoning tasks
are not ideal for measuring generalization, as LLMs are not trained for
specific tasks. Instead, we propose user behavior prediction, also a key aspect
of personalization, as a theoretically sound, scalable, and robust alternative.
We introduce a novel framework for this approach and test it on movie and music
recommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct.
Results align with our framework's predictions, showing GPT-4o outperforms
GPT-4o-mini and Llama, though all models have much room for improvement,
especially Llama.

</details>


### [268] [TokenShapley: Token Level Context Attribution with Shapley Value](https://arxiv.org/abs/2507.05261)
*Yingtai Xiao,Yuqing Zhu,Sirat Samyoun,Wanrong Zhang,Jiachen T. Wang,Jian Du*

Main category: cs.CL

TL;DR: 提出TokenShapley方法解决大语言模型生成响应中特定关键词归因问题，评估显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型生成响应正确性验证有挑战，句子级归因方法无法满足特定关键词归因需求。

Method: 结合基于Shapley值的数据归因和基于KNN的检索技术，利用预计算数据存储进行上下文检索并计算Shapley值。

Result: 在四个基准测试中，TokenShapley在标记级归因上优于现有基线，准确率提升11 - 23%。

Conclusion: TokenShapley是一种有效的细粒度数据归因方法。

Abstract: Large language models (LLMs) demonstrate strong capabilities in in-context
learning, but verifying the correctness of their generated responses remains a
challenge. Prior work has explored attribution at the sentence level, but these
methods fall short when users seek attribution for specific keywords within the
response, such as numbers, years, or names. To address this limitation, we
propose TokenShapley, a novel token-level attribution method that combines
Shapley value-based data attribution with KNN-based retrieval techniques
inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed
datastore for contextual retrieval and computing Shapley values to quantify
token importance, TokenShapley provides a fine-grained data attribution
approach. Extensive evaluations on four benchmarks show that TokenShapley
outperforms state-of-the-art baselines in token-level attribution, achieving an
11-23% improvement in accuracy.

</details>


### [269] [LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review](https://arxiv.org/abs/2507.05319)
*Cheng Yuan,Xinkai Rui,Yongqi Fan,Yawei Fan,Boyang Zhong,Jiacheng Wang,Weiyan Zhang,Tong Ruan*

Main category: cs.CL

TL;DR: 针对大语言模型在自动出院小结生成中的幻觉和溯源问题，提出LCDS系统，可构建映射表、结合逻辑规则生成可靠小结并支持溯源，结果用于微调模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自动出院小结生成中存在幻觉问题，且电子病历长文本使内容溯源困难。

Method: 提出LCDS系统，通过计算文本相似度构建源映射表，结合逻辑规则生成可靠小结，支持内容溯源。

Result: 生成可用于大语言模型增量微调的黄金出院小结。

Conclusion: LCDS系统能解决大语言模型在出院小结生成中的问题，项目和演示视频在GitHub开源。

Abstract: Despite the remarkable performance of Large Language Models (LLMs) in
automated discharge summary generation, they still suffer from hallucination
issues, such as generating inaccurate content or fabricating information
without valid sources. In addition, electronic medical records (EMRs) typically
consist of long-form data, making it challenging for LLMs to attribute the
generated content to the sources. To address these challenges, we propose LCDS,
a Logic-Controlled Discharge Summary generation system. LCDS constructs a
source mapping table by calculating textual similarity between EMRs and
discharge summaries to constrain the scope of summarized content. Moreover,
LCDS incorporates a comprehensive set of logical rules, enabling it to generate
more reliable silver discharge summaries tailored to different clinical fields.
Furthermore, LCDS supports source attribution for generated content, allowing
experts to efficiently review, provide feedback, and rectify errors. The
resulting golden discharge summaries are subsequently recorded for incremental
fine-tuning of LLMs. Our project and demo video are in the GitHub repository
https://github.com/ycycyc02/LCDS.

</details>


### [270] [MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents](https://arxiv.org/abs/2507.05330)
*Ming Gong,Xucheng Huang,Chenghan Yang,Xianhan Peng,Haoxin Wang,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: 提出面向电商的开源多模态大语言模型代理MindFlow，评估显示其在处理复杂查询等方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂多模态电商场景能力受限，需新解决方案。

Method: 基于CoALA框架构建，集成记忆、决策和行动模块，采用“MLLM - as - Tool”策略进行视觉 - 文本推理。

Result: 通过在线A/B测试和基于模拟的消融实验评估，在处理复杂查询、提高用户满意度和降低运营成本方面有显著收益，现实部署相对提升93.53%。

Conclusion: MindFlow在电商复杂多模态场景中表现良好，有实际应用价值。

Abstract: Recent advances in large language models (LLMs) have enabled new applications
in e-commerce customer service. However, their capabilities remain constrained
in complex, multimodal scenarios. We present MindFlow, the first open-source
multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it
integrates memory, decision-making, and action modules, and adopts a modular
"MLLM-as-Tool" strategy for effect visual-textual reasoning. Evaluated via
online A/B testing and simulation-based ablation, MindFlow demonstrates
substantial gains in handling complex queries, improving user satisfaction, and
reducing operational costs, with a 93.53% relative improvement observed in
real-world deployments.

</details>


### [271] [LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks](https://arxiv.org/abs/2507.05346)
*William Fleshman,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 提出LoRA增强生成（LAG）方法，在无数据和有数据场景评估表现良好。


<details>
  <summary>Details</summary>
Motivation: 特定任务和领域微调语言模型专家增多，需要高效选择和组合方法。

Method: 提出LAG方法，无需额外训练和数据访问，按标记和层过滤、检索和应用专家。

Result: 在各种知识密集型任务上表现优于现有无数据方法，且与检索增强生成等替代方案兼容。

Conclusion: LAG是一种有效利用知识和特定任务适配器的方法。

Abstract: The proliferation of fine-tuned language model experts for specific tasks and
domains signals the need for efficient selection and combination methods. We
propose LoRA-Augmented Generation (LAG) for leveraging large libraries of
knowledge and task-specific LoRA adapters. LAG requires no additional training
or access to data, and efficiently filters, retrieves, and applies experts on a
per-token and layer basis. We evaluate LAG on various knowledge-intensive
tasks, achieving superior performance over existing data-free methods. We
explore scenarios where additional data is available, demonstrating LAG's
compatibility with alternative solutions such as retrieval-augmented generation
(RAG).

</details>


### [272] [On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study](https://arxiv.org/abs/2507.05362)
*Riccardo Alberghi,Elizaveta Demyanenko,Luca Biggio,Luca Saglietti*

Main category: cs.CL

TL;DR: 研究大语言模型推理，通过分层图最短路径任务实验发现，相同训练令牌预算下，基于低效推理轨迹训练的模型泛化性更好，泛化性与模型预测信心相关。


<details>
  <summary>Details</summary>
Motivation: 研究提高大语言模型推理能力的两个关键因素，即分配更多测试时间计算和构建结构化思维链。

Method: 基于分层图最短路径任务，用自定义分词器训练解码器，对比基于最优和低效推理轨迹训练的模型。

Result: 相同训练令牌预算下，基于低效推理轨迹训练的模型对未见图泛化更好，注入任意冗余会损害性能。

Conclusion: 长、连贯且局部递增的推理轨迹使训练信号更易优化，泛化性与模型对下一个令牌预测的信心相关。

Abstract: Recent advances in natural language processing highlight two key factors for
improving reasoning in large language models (LLMs): (i) allocating more
test-time compute tends to help on harder problems but often introduces
redundancy in the reasoning trace, and (ii) compute is most effective when
reasoning is systematic and incremental, forming structured chains of thought
(CoTs) akin to human problem-solving. To study these factors in isolation, we
introduce a controlled setting based on shortest-path tasks in layered graphs.
We train decoder-only transformers on question-trace-answer triples using a
custom tokenizer, comparing models trained on optimal bottom-up dynamic
programming traces with those trained on longer, valid traces involving
backtracking. Surprisingly, with the same training-token budget, models trained
on inefficient traces generalize better to unseen graphs. This benefit is not
due to length alone-injecting arbitrary redundancy into reasoning traces fails
to help and can even hurt performance. Instead, we find that generalization
correlates with the model's confidence in next-token prediction, suggesting
that long, coherent, and locally incremental traces make the training signal
easier to optimize.

</details>


### [273] [Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences](https://arxiv.org/abs/2507.05391)
*Guillem Ramírez,Alexandra Birch,Ivan Titov*

Main category: cs.CL

TL;DR: 本文探讨用隐私配置文件让用户控制数据，构建框架并引入数据集，实验显示轻量级大语言模型有一定能力但有挑战。


<details>
  <summary>Details</summary>
Motivation: 商业API访问大语言模型常让用户数据暴露给服务提供商，需让用户控制数据。

Method: 构建本地模型用隐私配置文件改写查询的框架，引入多语言数据集PEEP。

Result: 轻量级大语言模型能在一定程度上遵循隐私配置文件指令，但存在一致挑战。

Conclusion: 需要更好理解和遵守用户定义隐私偏好的模型。

Abstract: Large language models (LLMs) are primarily accessed via commercial APIs, but
this often requires users to expose their data to service providers. In this
paper, we explore how users can stay in control of their data by using privacy
profiles: simple natural language instructions that say what should and should
not be revealed. We build a framework where a local model uses these
instructions to rewrite queries, only hiding details deemed sensitive by the
user, before sending them to an external model, thus balancing privacy with
performance. To support this research, we introduce PEEP, a multilingual
dataset of real user queries annotated to mark private content and paired with
synthetic privacy profiles. Our experiments with lightweight LLMs show they can
follow these instructions to some extent, but also face consistent challenges,
highlighting the need for models that better understand and comply with
user-defined privacy preferences.

</details>


### [274] [Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning](https://arxiv.org/abs/2507.05418)
*Jaedong Hwang,Kumar Tanmay,Seok-Jin Lee,Ayush Agrawal,Hamid Palangi,Kumar Ayush,Ila Fiete,Paul Pu Liang*

Main category: cs.CL

TL;DR: 现有大语言模型多语言推理能力不足，论文引入GeoFact - X基准，提出BRIDGE训练方法和自动评估协议，结果显示BRIDGE提升了多语言推理保真度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多语言推理任务中能力不足，尤其对低资源语言有隐式偏见，当前多语言基准存在缺陷。

Method: 引入GeoFact - X基准，提出BRIDGE训练方法，开发基于LLM - as - a - judge的自动评估协议。

Result: BRIDGE显著提升了多语言推理保真度。

Conclusion: 推理感知的多语言强化学习对稳健的跨语言泛化至关重要。

Abstract: Large Language Models (LLMs) have achieved strong performance in domains like
mathematics, factual QA, and code generation, yet their multilingual reasoning
capabilities in these tasks remain underdeveloped. Especially for low-resource
languages such as Swahili or Thai, LLMs can often misinterpret prompts or
default to reasoning in English. This implicit bias toward high-resource
languages undermines factual accuracy, interpretability, and trust. Current
multilingual benchmarks focus only on final answers, overlooking whether models
actually reason in the target language. To address this gap, we introduce
GeoFact-X, a geography-based multilingual factual reasoning benchmark with
annotated reasoning traces in five languages: English, Hindi, Japanese,
Swahili, and Thai. We further propose BRIDGE, a novel training method that
guides supervised fine-tuning and test-time reinforcement learning with a
language-consistency reward to align reasoning with the input language.
Finally, we develop an automatic evaluation protocol using LLM-as-a-judge to
assess answer correctness and the quality and language consistency of reasoning
traces, enabling nuanced and scalable analysis beyond surface-level metrics.
Our results show that BRIDGE significantly enhances multilingual reasoning
fidelity, demonstrating that reasoning-aware multilingual reinforcement
learning is crucial for robust cross-lingual generalization.
https://jd730.github.io/projects/GeoFact-X_BRIDGE

</details>


### [275] ["Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large Language Models](https://arxiv.org/abs/2507.05424)
*Yufei Tao,Adam Hiatt,Rahul Seetharaman,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 提出CoPE评估框架分析大语言模型对上下文和参数知识的利用，发现“lost - in - the - later”现象，设计提示方法提升上下文利用。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型如何优先处理和整合上下文知识与参数知识。

Method: 引入CoPE评估框架，使用MultiWikiAtomic数据集分析大语言模型在开放式问答中的表现。

Result: 发现“lost - in - the - later”现象，推理模型和使用思维链提示的非推理模型上下文利用更差，思维链提示导致召回率降低和响应变短。

Conclusion: 基于发现设计提示方法，在摘要任务中基于上下文知识的提示可改善事实依据并减少幻觉。

Abstract: Large language models are capable of leveraging both contextual and
parametric knowledge but how they prioritize and integrate these sources
remains underexplored. We introduce CoPE, a novel evaluation framework that
systematically measures contextual knowledge (CK) and parametric knowledge (PK)
across models and languages. Using our MultiWikiAtomic dataset in English,
Spanish, and Danish, we analyze how large language models (LLMs) integrate
context, prioritize information, and incorporate PK in open-ended question
answering. Our analysis uncovers a phenomenon we call lost-in-the-later, where
LLMs tend to overlook or deprioritize information that appears later in a given
context, revealing a strong positional bias that affects contextual grounding.
We further find that reasoning models, as well as non-reasoning models prompted
with chain-of-thought (CoT), use context even less than non-reasoning models
without CoT and fail to mitigate the lost-in-the-later effect. CoT prompting,
in particular, results in lower recall and shorter responses, leading to
degraded contextual grounding. Based on these insights, we design prompt-based
methods to effectively leverage input context. A case study applying CoPE to
summarization demonstrates that CK-informed prompting improves factual
grounding and reduces hallucination.

</details>


### [276] [On the Semantics of Large Language Models](https://arxiv.org/abs/2507.05448)
*Martin Schuele*

Main category: cs.CL

TL;DR: 探讨大语言模型是否真正理解语言，从字词和句子层面语义分析。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽展现语言能力，但对其是否真正理解语言存在争议，需深入研究。

Method: 研究大语言模型内部机制和生成的语言表征，借鉴弗雷格和罗素的经典语义理论。

Result: 文中未提及明确结果。

Conclusion: 文中未提及明确结论。

Abstract: Large Language Models (LLMs) such as ChatGPT demonstrated the potential to
replicate human language abilities through technology, ranging from text
generation to engaging in conversations. However, it remains controversial to
what extent these systems truly understand language. We examine this issue by
narrowing the question down to the semantics of LLMs at the word and sentence
level. By examining the inner workings of LLMs and their generated
representation of language and by drawing on classical semantic theories by
Frege and Russell, we get a more nuanced picture of the potential semantic
capabilities of LLMs.

</details>


### [277] [ModelCitizens:Representing Community Voices in Online Safety](https://arxiv.org/abs/2507.05455)
*Ashima Suvarna,Christina Chance,Hamid Palangi,Sophie Hao,Thomas Hartvigsen,Saadia Gabriel*

Main category: cs.CL

TL;DR: 提出MODELCITIZENS数据集，用LLM增强上下文，发布微调模型，强调社区感知标注和建模对内容审核的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有毒性检测模型训练数据忽略不同标注者观点及特定语境毒性概念，需改进。

Method: 引入MODELCITIZENS数据集，用LLM增强帖子上下文，在该数据集上微调模型。

Result: 现有工具在MODELCITIZENS上表现不佳，微调模型在分布内评估中比GPT - o4 - mini高5.5%。

Conclusion: 社区感知的标注和建模对包容性内容审核很重要。

Abstract: Automatic toxic language detection is critical for creating safe, inclusive
online spaces. However, it is a highly subjective task, with perceptions of
toxic language shaped by community norms and lived experience. Existing
toxicity detection models are typically trained on annotations that collapse
diverse annotator perspectives into a single ground truth, erasing important
context-specific notions of toxicity such as reclaimed language. To address
this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K
toxicity annotations across diverse identity groups. To capture the role of
conversational context on toxicity, typical of social media posts, we augment
MODELCITIZENS posts with LLM-generated conversational scenarios.
State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,
GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on
context-augmented posts. Finally, we release LLAMACITIZEN-8B and
GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,
which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our
findings highlight the importance of community-informed annotation and modeling
for inclusive content moderation.

</details>


### [278] [Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications](https://arxiv.org/abs/2507.05517)
*Jean-Philippe Corbeil,Asma Ben Abacha,George Michalopoulos,Phillip Swazinna,Miguel Del-Agua,Jerome Tremblay,Akila Jeeson Daniel,Cari Bader,Kevin Cho,Pooja Krishnan,Nathan Bodenstab,Thomas Lin,Wenxuan Teng,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

TL;DR: 本文研究从护士听写的结构化表格报告和从医患会诊中提取医疗指令这两个临床NLP任务，评估不同大语言模型表现，提出生成护士听写的管道并发布两个开源数据集。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在结构化表格报告和医疗指令提取两个临床NLP任务上因数据稀缺和敏感未充分探索，解决这些任务可减轻医护人员文档负担。

Method: 使用私有和开源临床数据集，评估不同权重的大语言模型表现，分析其优缺点；提出生成护士听写的代理管道。

Result: 对模型性能进行了评估，分析了不同模型的优缺点，实现了临床观察的结构化提取。

Conclusion: 发布了用于护士观察提取和医疗指令提取的开源数据集SYNUR和SIMORD，支持相关领域进一步研究。

Abstract: Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong
performance on clinical natural language processing (NLP) tasks across multiple
medical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular
reporting from nurse dictations and medical order extraction from
doctor-patient consultations - remain underexplored due to data scarcity and
sensitivity, despite active industry efforts. Practical solutions to these
real-world clinical tasks can significantly reduce the documentation burden on
healthcare providers, allowing greater focus on patient care. In this paper, we
investigate these two challenging tasks using private and open-source clinical
datasets, evaluating the performance of both open- and closed-weight LLMs, and
analyzing their respective strengths and limitations. Furthermore, we propose
an agentic pipeline for generating realistic, non-sensitive nurse dictations,
enabling structured extraction of clinical observations. To support further
research in both areas, we release SYNUR and SIMORD, the first open-source
datasets for nurse observation extraction and medical order extraction.

</details>


### [279] [Agentic-R1: Distilled Dual-Strategy Reasoning](https://arxiv.org/abs/2507.05707)
*Weihua Du,Pranjal Aggarwal,Sean Welleck,Yiming Yang*

Main category: cs.CL

TL;DR: 提出DualDistill微调框架，训练Agentic - R1模型，能动态选策略，提升多任务准确率。


<details>
  <summary>Details</summary>
Motivation: 现有长思维链模型依赖自然语言追踪，工具增强代理在复杂逻辑任务表现不佳，需改进推理能力。

Method: 引入DualDistill框架，将多个教师的互补推理策略提炼到统一的学生模型，训练Agentic - R1动态选择策略。

Result: 提高了包括计算密集型和标准基准在内的一系列任务的准确性。

Conclusion: 多策略提炼在实现强大而高效的推理方面是有效的。

Abstract: Current long chain-of-thought (long-CoT) models excel at mathematical
reasoning but rely on slow and error-prone natural language traces.
Tool-augmented agents address arithmetic via code execution, but often falter
on complex logical tasks. We introduce a fine-tuning framework, DualDistill,
that distills complementary reasoning strategies from multiple teachers into a
unified student model. Using this approach, we train Agentic-R1, which
dynamically selects the optimal strategy for each query, invoking tools for
arithmetic and algorithmic problems, and using text-based reasoning for
abstract ones. Our method improves accuracy across a range of tasks, including
both computation-intensive and standard benchmarks, demonstrating the
effectiveness of multi-strategy distillation in achieving robust and efficient
reasoning. Our project is available at https://github.com/StigLidu/DualDistill

</details>


### [280] [Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition](https://arxiv.org/abs/2507.05724)
*Zijin Gu,Tatiana Likhomanenko,Navdeep Jaitly*

Main category: cs.CL

TL;DR: 提出Omni - router Transformer模型，通过跨层共享路由器提升不同层专家合作，实验显示其在ASR中表现优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 传统MoE方法各层路由器独立选专家，各层专家选择相关性不强，为增强不同层专家合作和专业化。

Method: 在不同MoE层使用共享路由器，构建Omni - router Transformer模型。

Result: 在大规模伪标签数据集和10个不同的域外ASR基准测试中，Omni - router Transformer训练损失更低，平均字错误率分别比密集模型和Switch Transformer模型降低11.2%和8.2%，且专家使用更具结构性，对不同数据鲁棒性更好。

Conclusion: Omni - router Transformer能有效提高ASR性能。

Abstract: Mixture-of-experts (MoE) architectures have expanded from language modeling
to automatic speech recognition (ASR). Traditional MoE methods, such as the
Switch Transformer, route experts independently within each layer. Our analysis
reveals that routers in most layers make expert choices that are not strongly
correlated with the choices of the routers in other layers. To increase the
cooperation between experts in different layers and encourage greater
specialization, we use a shared router across different MoE layers. We call
this model \emph{Omni-router Transformer}. Extensive experiments on a
large-scale pseudo-labeled dataset and evaluations across 10 diverse,
out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is
able to achieve lower training loss and consistently outperform dense and
Switch Transformer models, reducing average word error rates by 11.2% and 8.2%,
respectively, while providing structured expert usage and improved robustness
to diverse data.

</details>


### [281] [Self-Review Framework for Enhancing Instruction Following Capability of LLM](https://arxiv.org/abs/2507.05598)
*Sihyun Park*

Main category: cs.CL

TL;DR: 提出Re5框架提升大语言模型指令遵循性能，实验证明其高效有效。


<details>
  <summary>Details</summary>
Motivation: 现有提升大语言模型遵循指令和格式约束的方法存在成本高、输出质量下降等问题。

Method: Re5框架从用户指令中提取任务和约束组件，进行结构评估、细粒度特定约束内容评估并选择性修订，用于对齐调优。

Result: Re5用少量数据达到与GPT - 4o - mini训练模型相当的指令遵循性能，维持响应质量，胜率64.24%。

Conclusion: Re5是在最少外部监督下增强指令遵循的高效有效解决方案。

Abstract: Various techniques have been proposed to improve large language models (LLMs)
adherence to formatting and instruction constraints. One of the most effective
approaches involves utilizing high-quality data generated by powerful models.
However, such models often fail to fully comply with complex instructions in a
single generation. To address this limitation, iterative revision methods have
been introduced. Nevertheless, as the number of data points and revision
iterations increases, the associated monetary costs grow significantly. As a
resource-efficient alternative, methods have been proposed that leverage
high-performance evaluation tools to compensate for the limited self-evaluation
capabilities of open-source LLMs. However, these approaches often lead to a
degradation in output quality due to excessive revision. To overcome these
challenges, we propose Re5, a self-evaluation and revision framework designed
to enhance instruction-following performance while preserving the quality of
the generated content. Re5 extracts task and constraint components from user
instructions, performs structural evaluations to prevent error accumulation,
and applies fine-grained constraint-specific content evaluations followed by
selective revisions. This process ensures precise and quality-preserving
improvements. The final high-quality outputs are used for alignment tuning,
enabling long-term alignment improvements through a data-centric iterative
refinement loop. Experimental results demonstrate that Re5 achieves
instruction-following performance comparable to models trained on data
generated by GPT-4o-mini, a high-performance model, even with a small amount of
data while maintaining response quality with a 64.24%-win rate over the
non-revised initial responses. These results validate Re5 as an efficient and
effective solution for enhancing instruction adherence with minimal external
supervision.

</details>


### [282] [RabakBench: Scaling Human Annotations to Construct Localized Multilingual Safety Benchmarks for Low-Resource Languages](https://arxiv.org/abs/2507.05980)
*Gabriel Chua,Leanne Tan,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 本文介绍针对新加坡语言环境的多语言安全基准RabakBench，构建后评估显示多种分类器性能下降，该基准可用于东南亚多语言安全评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型及其安全分类器在低资源语言上表现不佳，缺乏相关训练数据和评估基准。

Method: 通过可扩展的三阶段流程构建：生成（用大模型对新加坡英语网络内容进行对抗样本生成）、标注（半自动化多标签安全标注）、翻译（高保真跨语言翻译）。

Result: 最终数据集含超5000个多语言安全标注示例，评估11种流行分类器显示性能显著下降。

Conclusion: RabakBench能在东南亚多语言环境进行鲁棒安全评估，也为低资源环境构建本地化安全数据集提供可复现框架，相关数据和代码公开。

Abstract: Large language models (LLMs) and their safety classifiers often perform
poorly on low-resource languages due to limited training data and evaluation
benchmarks. This paper introduces RabakBench, a new multilingual safety
benchmark localized to Singapore's unique linguistic context, covering
Singlish, Chinese, Malay, and Tamil. RabakBench is constructed through a
scalable three-stage pipeline: (i) Generate - adversarial example generation by
augmenting real Singlish web content with LLM-driven red teaming; (ii) Label -
semi-automated multi-label safety annotation using majority-voted LLM labelers
aligned with human judgments; and (iii) Translate - high-fidelity translation
preserving linguistic nuance and toxicity across languages. The final dataset
comprises over 5,000 safety-labeled examples across four languages and six
fine-grained safety categories with severity levels. Evaluations of 11 popular
open-source and closed-source guardrail classifiers reveal significant
performance degradation. RabakBench not only enables robust safety evaluation
in Southeast Asian multilingual settings but also offers a reproducible
framework for building localized safety datasets in low-resource environments.
The benchmark dataset, including the human-verified translations, and
evaluation code are publicly available.

</details>


### [283] [DRAGON: Dynamic RAG Benchmark On News](https://arxiv.org/abs/2507.05713)
*Fedor Chernogorskii,Sergei Averkiev,Liliya Kudraleeva,Zaven Martirosian,Maria Tikhonova,Valentin Malykh,Alena Fenogenova*

Main category: cs.CL

TL;DR: 本文提出首个用于评估俄语RAG系统的动态基准DRAGON，基于更新语料构建，有自动问题生成等评估框架并发布公共排行榜。


<details>
  <summary>Details</summary>
Motivation: 现有俄语RAG评估资源稀缺且静态，无法反映现实部署动态特性。

Method: 基于定期更新的俄语新闻和公共文档语料构建DRAGON，利用知识图谱自动生成问题，发布包含自动问题生成管道、评估脚本的完整评估框架和基准数据，推出公共排行榜。

Result: 推出DRAGON基准、完整评估框架和公共排行榜。

Conclusion: DRAGON为俄语RAG系统评估提供了动态、综合的解决方案，评估框架有跨语言复用潜力。

Abstract: Retrieval-Augmented Generation (RAG) is a widely adopted approach for
improving the factuality of large language models (LLMs) by incorporating
external knowledge at inference time. Although there exist multiple RAG
benchmarks for English, evaluation resources for other languages, including
Russian, remain scarce and static, failing to capture the dynamic nature of
real-world deployments.
  In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first
dynamic benchmark for evaluating RAG systems in Russian on a changing news
corpora. DRAGON is built upon a regularly updated corpus of Russian news and
public documents and supports comprehensive evaluation of both the retriever
and generator components. Question generation is performed automatically with
the use of Knowledge Graph constructed from the corpus and enables the
extraction of four core question types aligned with distinct subgraph patterns.
We release a complete evaluation framework comprising the pipeline for
automatic question generation, evaluation scripts, which are potentially
reusable for other languages and multilingual settings, and benchmark data. We
also launch a public leaderboard to encourage community participation and
comparison.

</details>


### [284] [HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation](https://arxiv.org/abs/2507.05714)
*YiHan Jiao,ZheHao Tan,Dan Yang,DuoLin Sun,Jie Feng,Jian Wang,Peng Wei*

Main category: cs.CL

TL;DR: 提出RAG模型应具备三种层次能力，引入HIRAG指令微调方法，实验表明该方法提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统缺乏对生成模型特定能力的深入研究，存在文档质量不一致和检索系统不完善等问题，有限的微调研究也缺乏对RAG任务的细致关注和对思维链过程的深入利用。

Method: 提出RAG模型应具备过滤、组合、特定推理三种层次能力，引入Hierarchical - Thought Instruction - Tuning Retrieval - Augmented Generation (HIRAG)方法，采用“先思考后回答”策略，利用多级渐进思维链提升模型开卷考试能力。

Result: HIRAG训练策略显著提升了模型在RGB、PopQA、MuSiQue、HotpotQA和PubmedQA等数据集上的性能。

Conclusion: HIRAG方法能有效提升RAG模型的性能。

Abstract: Retrieval-augmented generation (RAG) has become a fundamental paradigm for
addressing the challenges faced by large language models in handling real-time
information and domain-specific problems. Traditional RAG systems primarily
rely on the in-context learning (ICL) capabilities of the large language model
itself. Still, in-depth research on the specific capabilities needed by the RAG
generation model is lacking, leading to challenges with inconsistent document
quality and retrieval system imperfections. Even the limited studies that
fine-tune RAG generative models often \textit{lack a granular focus on RAG
task} or \textit{a deeper utilization of chain-of-thought processes}. To
address this, we propose that RAG models should possess three progressively
hierarchical abilities (1) Filtering: the ability to select relevant
information; (2) Combination: the ability to combine semantic information
across paragraphs; and (3) RAG-specific reasoning: the ability to further
process external knowledge using internal knowledge. Thus, we introduce our new
RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning
Retrieval-Augmented Generation (HIRAG) incorporates a "think before answering"
strategy. This method enhances the model's open-book examination capability by
utilizing multi-level progressive chain-of-thought. Experiments show that the
HIRAG training strategy significantly improves the model's performance on
datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.

</details>


### [285] [Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators](https://arxiv.org/abs/2507.05890)
*Sungjib Lim,Woojung Song,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 提出使用大语言模型进行虚拟受访者模拟的框架，有效识别高有效性调查项目，为低成本调查开发开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 心理测量调查评估大语言模型特质时，需要可扩展的调查项目生成方法并确保其结构效度，传统方法成本高。

Method: 提出使用大语言模型进行虚拟受访者模拟的框架，考虑中介因素，模拟具有不同中介因素的受访者来识别测量预期特质的调查项目。

Result: 在三种心理特质理论实验中，中介生成方法和模拟框架有效识别高有效性项目，大语言模型能从特质定义生成合理中介并模拟受访者行为。

Conclusion: 研究为低成本调查开发开辟新方向，有助于深入理解大语言模型复制人类行为的方式，将公开数据集和代码。

Abstract: As psychometric surveys are increasingly used to assess the traits of large
language models (LLMs), the need for scalable survey item generation suited for
LLMs has also grown. A critical challenge here is ensuring the construct
validity of generated items, i.e., whether they truly measure the intended
trait. Traditionally, this requires costly, large-scale human data collection.
To make it efficient, we present a framework for virtual respondent simulation
using LLMs. Our central idea is to account for mediators: factors through which
the same trait can give rise to varying responses to a survey item. By
simulating respondents with diverse mediators, we identify survey items that
robustly measure intended traits. Experiments on three psychological trait
theories (Big5, Schwartz, VIA) show that our mediator generation methods and
simulation framework effectively identify high-validity items. LLMs demonstrate
the ability to generate plausible mediators from trait definitions and to
simulate respondent behavior for item validation. Our problem formulation,
metrics, methodology, and dataset open a new direction for cost-effective
survey development and a deeper understanding of how LLMs replicate human-like
behavior. We will publicly release our dataset and code to support future work.

</details>


### [286] [UQLM: A Python Package for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2507.06196)
*Dylan Bouchard,Mohit Singh Chauhan,David Skarbrevik,Ho-Kyeong Ra,Viren Bajaj,Zeya Ahmad*

Main category: cs.CL

TL;DR: 介绍用于大语言模型幻觉检测的Python包UQLM，提供基于不确定性量化的现成解决方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型幻觉问题影响下游应用的安全和信任，需要解决方案。

Method: 使用最先进的不确定性量化（UQ）技术，提供一套基于UQ的评分器来计算响应级别的置信度分数。

Result: 开发出UQLM Python包。

Conclusion: UQLM可作为基于UQ的幻觉检测的现成解决方案，能增强大语言模型输出的可靠性。

Abstract: Hallucinations, defined as instances where Large Language Models (LLMs)
generate false or misleading content, pose a significant challenge that impacts
the safety and trust of downstream applications. We introduce UQLM, a Python
package for LLM hallucination detection using state-of-the-art uncertainty
quantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers
that compute response-level confidence scores ranging from 0 to 1. This library
provides an off-the-shelf solution for UQ-based hallucination detection that
can be easily integrated to enhance the reliability of LLM outputs.

</details>


### [287] [Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers](https://arxiv.org/abs/2507.06223)
*Zhiyuan Peng,Ting-ruen Wei,Tingyu Song,Yilun Zhao,Yi Fang*

Main category: cs.CL

TL;DR: 现有基于大语言模型的重排器效率评估指标有局限，本文提出E²R - FLOPs指标及FLOPs估算器并开展实验研究效率与效果权衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的重排器计算需求高，且评估指标依赖硬件和运行时选择，难以解释和评估效率 - 效果权衡。

Method: 提出E²R - FLOPs指标（RPP和QPP），构建可解释的FLOPs估算器。

Result: 开展全面实验评估了多种基于大语言模型的重排器。

Conclusion: 研究了效率 - 效果权衡问题并引起研究界关注。

Abstract: Large Language Models (LLMs) have recently been applied to reranking tasks in
information retrieval, achieving strong performance. However, their high
computational demands often hinder practical deployment. Existing studies
evaluate the efficiency of LLM-based rerankers using proxy metrics such as
latency, the number of forward passes, input tokens, and output tokens.
However, these metrics depend on hardware and running-time choices (\eg
parallel or not, batch size, etc), and often fail to account for model size,
making it difficult to interpret and obscuring the evaluation of the
efficiency-effectiveness tradeoff. To address this issue, we propose
E\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per
PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for
hardware-agnostic throughput. Companied with the new metrics, an interpretable
FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even
without running any experiments. Based on the proposed metrics, we conduct
comprehensive experiments to evaluate a wide range of LLM-based rerankers with
different architecture, studying the efficiency-effectiveness trade-off and
bringing this issue to the attention of the research community.

</details>


### [288] [OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation](https://arxiv.org/abs/2507.05965)
*Lucas Fonseca Lage,Simon Ostermann*

Main category: cs.CL

TL;DR: 介绍开源的OpenFActScore评估大语言模型文本事实性，可使用任意Hugging Face兼容模型，评估显示开源模型能接近闭源系统性能。


<details>
  <summary>Details</summary>
Motivation: 原FActScore依赖闭源商业模型，缺乏透明度和可复现性，需开源实现。

Method: 使用Atomic Fact Generation提取事实声明，Atomic Fact Validation验证声明，用原FActScore基准评估多个开源大语言模型。

Result: 开源模型能近似闭源系统性能，Gemma整体表现最佳，最终设置与原FActScore实验有0.99的皮尔逊相关性。

Conclusion: OpenFActScore促进透明度、可复现性和低成本评估。

Abstract: We introduce OpenFActScore, an open-source implementation of the FActScore
framework for evaluating the factuality of text generated by large language
models (LLMs). FActScore evaluates the factual accuracy of long-form text by
using Atomic Fact Generation (AFG) to extract individual factual claims and
Atomic Fact Validation (AFV) to verify each claim against a trusted knowledge
source. While the original FActScore relies on closed-source and commercial
models such as InstructGPT and ChatGPT, OpenFActScore enables the use of any
Hugging Face-compatible model for both AFG and AFV. We provide a detailed
technical overview of our implementation, highlighting design choices and
modifications made to support open models. We evaluate multiple open-source
LLMs on both AFG and AFV using the original FActScore benchmark, reporting
BERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our
results show that open models can approximate the performance of closed-source
systems, with Gemma achieving the best overall performance, and our final setup
obtains a 0.99 Pearson correlation with the original FActScore experiments.
OpenFActScore promotes transparency, reproducibility, and cost-effective
evaluation, and is available at: https://github.com/lflage/OpenFActScore.

</details>


### [289] [Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs](https://arxiv.org/abs/2507.06056)
*Yizhan Huang,Zhe Yang,Meifang Chen,Jianping Zhang,Michael R. Lyu*

Main category: cs.CL

TL;DR: 研究大语言模型训练数据记忆难度的刻画问题，提出熵 - 记忆定律，还得出区分训练和测试数据的方法。


<details>
  <summary>Details</summary>
Motivation: 探究如何刻画大语言模型中训练数据的记忆难度这一基础且未充分探索的问题。

Method: 在OLMo开放模型家族上进行实证实验，通过相同策略推导方法。

Result: 得出熵与记忆分数线性相关的熵 - 记忆定律；发现高度随机字符串经验熵比训练语料低；得到区分训练和测试数据的方法。

Conclusion: 熵 - 记忆定律可用于刻画训练数据记忆难度，且可实现数据集推理（DI）。

Abstract: Large Language Models (LLMs) are known to memorize portions of their training
data, sometimes reproducing content verbatim when prompted appropriately. In
this work, we investigate a fundamental yet under-explored question in the
domain of memorization: How to characterize memorization difficulty of training
data in LLMs? Through empirical experiments on OLMo, a family of open models,
we present the Entropy-Memorization Law. It suggests that data entropy is
linearly correlated with memorization score. Moreover, in a case study of
memorizing highly randomized strings, or "gibberish", we observe that such
sequences, despite their apparent randomness, exhibit unexpectedly low
empirical entropy compared to the broader training corpus. Adopting the same
strategy to discover Entropy-Memorization Law, we derive a simple yet effective
approach to distinguish training and testing data, enabling Dataset Inference
(DI).

</details>


### [290] [NeoBabel: A Multilingual Open Tower for Visual Generation](https://arxiv.org/abs/2507.06137)
*Mohammad Mahdi Derakhshani,Dheeraj Varghese,Marzieh Fadaee,Cees G. M. Snoek*

Main category: cs.CL

TL;DR: 现有文本到图像生成技术以英语为主，存在诸多问题。本文提出多语言图像生成框架NeoBabel，在多语言表现出色，还发布相关工具包推动研究。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像生成技术以英语为主，给非英语使用者带来障碍、产生数字不平等，以及翻译管道存在语义漂移等问题。

Method: 结合大规模多语言预训练和高分辨率指令微调训练模型，扩展英文基准测试为多语言版本，引入新指标评估。

Result: NeoBabel在多语言基准测试中表现优异，得分高，在英语任务上与领先模型相当，模型体积小2 - 4倍。

Conclusion: 多语言能力不是权衡，而是提高生成式AI鲁棒性、效率和文化保真度的催化剂。

Abstract: Text-to-image generation advancements have been predominantly
English-centric, creating barriers for non-English speakers and perpetuating
digital inequities. While existing systems rely on translation pipelines, these
introduce semantic drift, computational overhead, and cultural misalignment. We
introduce NeoBabel, a novel multilingual image generation framework that sets a
new Pareto frontier in performance, efficiency and inclusivity, supporting six
languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is
trained using a combination of large-scale multilingual pretraining and
high-resolution instruction tuning. To evaluate its capabilities, we expand two
English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.
NeoBabel achieves state-of-the-art multilingual performance while retaining
strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.
Notably, it performs on par with leading models on English tasks while
outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though
these models are built on multilingual base LLMs. This demonstrates the
effectiveness of our targeted alignment training for preserving and extending
crosslingual generalization. We further introduce two new metrics to rigorously
assess multilingual alignment and robustness to code-mixed prompts. Notably,
NeoBabel matches or exceeds English-only models while being 2-4x smaller. We
release an open toolkit, including all code, model checkpoints, a curated
dataset of 124M multilingual text-image pairs, and standardized multilingual
evaluation protocols, to advance inclusive AI research. Our work demonstrates
that multilingual capability is not a trade-off but a catalyst for improved
robustness, efficiency, and cultural fidelity in generative AI.

</details>


### [291] [Coding Triangle: How Does Large Language Model Understand Code?](https://arxiv.org/abs/2507.06138)
*Taolin Zhang,Zihan Ma,Maosong Cao,Junnan Liu,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 引入Code Triangle框架评估大语言模型编程能力，发现其缺乏多样性和鲁棒性，提出改进方法并指出发展方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成取得进展，但真实编程能力未充分探索，需评估。

Method: 引入Code Triangle框架，从编辑分析、代码实现和测试用例生成三个维度评估，在竞赛编程基准上实验。

Result: 大语言模型能形成自洽系统，但缺乏多样性和鲁棒性，存在模型认知与人类专业知识的分布偏移。

Conclusion: 引入人类生成内容和模型混合可提升性能和鲁棒性，模型认知的特点可促进自我反思和改进。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
generation, yet their true programming competence remains underexplored. We
introduce the Code Triangle framework, which systematically evaluates LLMs
across three fundamental dimensions: editorial analysis, code implementation,
and test case generation. Through extensive experiments on competitive
programming benchmarks, we reveal that while LLMs can form a self-consistent
system across these dimensions, their solutions often lack the diversity and
robustness of human programmers. We identify a significant distribution shift
between model cognition and human expertise, with model errors tending to
cluster due to training data biases and limited reasoning transfer. Our study
demonstrates that incorporating human-generated editorials, solutions, and
diverse test cases, as well as leveraging model mixtures, can substantially
enhance both the performance and robustness of LLMs. Furthermore, we reveal
both the consistency and inconsistency in the cognition of LLMs that may
facilitate self-reflection and self-improvement, providing a potential
direction for developing more powerful coding models.

</details>


### [292] [DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation](https://arxiv.org/abs/2507.06189)
*Maximilian Heil,Dionne Bang*

Main category: cs.CL

TL;DR: 本文介绍CLEF 2025 CheckThat!实验室任务1主观检测的提交工作，研究迁移学习和风格数据增强对英文新闻文本主客观句子分类的效果，得出结合编码器专业化和标签一致增强的价值。


<details>
  <summary>Details</summary>
Motivation: 研究迁移学习和风格数据增强对英文新闻文本主客观句子分类的有效性。

Method: 对比预训练编码器微调与相关任务微调变压器的迁移学习，引入使用GPT - 4o的可控增强管道生成预设主观风格的释义，并使用同一模型修正生成样本。

Result: 特定编码器的迁移学习优于通用编码器微调，精心策划的增强显著提高模型鲁棒性，官方提交排名24名参与者中的第16名。

Conclusion: 结合编码器专业化与标签一致的增强对改进主观性检测有价值。

Abstract: This paper presents our submission to Task 1, Subjectivity Detection, of the
CheckThat! Lab at CLEF 2025. We investigate the effectiveness of
transfer-learning and stylistic data augmentation to improve classification of
subjective and objective sentences in English news text. Our approach contrasts
fine-tuning of pre-trained encoders and transfer-learning of fine-tuned
transformer on related tasks. We also introduce a controlled augmentation
pipeline using GPT-4o to generate paraphrases in predefined subjectivity
styles. To ensure label and style consistency, we employ the same model to
correct and refine the generated samples. Results show that transfer-learning
of specified encoders outperforms fine-tuning general-purpose ones, and that
carefully curated augmentation significantly enhances model robustness,
especially in detecting subjective content. Our official submission placed us
$16^{th}$ of 24 participants. Overall, our findings underscore the value of
combining encoder specialization with label-consistent augmentation for
improved subjectivity detection. Our code is available at
https://github.com/dsgt-arc/checkthat-2025-subject.

</details>


### [293] [Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving](https://arxiv.org/abs/2507.06229)
*Xiangru Tang,Tianrui Qin,Tianhao Peng,Ziyang Zhou,Daniel Shao,Tingting Du,Xinming Wei,Peng Xia,Fang Wu,He Zhu,Ge Zhang,Jiaheng Liu,Xingyao Wang,Sirui Hong,Chenglin Wu,Hao Cheng,Chi Wang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 引入Agent KB框架，通过Reason - Retrieve - Refine管道解决语言代理跨领域纠错和经验复用问题，实验证明其能提升成功率。


<details>
  <summary>Details</summary>
Motivation: 语言代理处理复杂任务时在有效纠错和跨领域经验复用方面存在困难，且传统代理无法相互学习经验。

Method: 引入Agent KB分层经验框架，采用Reason - Retrieve - Refine管道，创建共享知识库实现跨代理知识转移。

Result: 在GAIA基准测试中成功率最多提升16.28个百分点，Claude - 3和GPT - 4在不同任务上成功率均有显著提升，在SWE - bench代码修复中Claude - 3也有进步。

Conclusion: Agent KB提供模块化、框架无关的基础设施，使代理能从过去经验学习并将成功策略推广到新任务。

Abstract: As language agents tackle increasingly complex tasks, they struggle with
effective error correction and experience reuse across domains. We introduce
Agent KB, a hierarchical experience framework that enables complex agentic
problem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses
a core limitation: agents traditionally cannot learn from each other's
experiences. By capturing both high-level strategies and detailed execution
logs, Agent KB creates a shared knowledge base that enables cross-agent
knowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success
rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3
improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on
intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to
improve from 41.33% to 53.33%. Our results suggest that Agent KB provides a
modular, framework-agnostic infrastructure for enabling agents to learn from
past experiences and generalize successful strategies to new tasks.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [294] [Solar Flare Prediction Using LSTM and DLSTM with Sliding Window Pattern Recognition](https://arxiv.org/abs/2507.05313)
*Zeinab Hassani,Davud Mohammadpur,Hossein Safari*

Main category: astro-ph.SR

TL;DR: 研究用LSTM和DLSTM结合集成算法，基于GOES目录时间序列数据预测太阳耀斑，正则化时间序列上DLSTM集成模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 利用先进机器学习技术提高太阳耀斑预测的准确性和可靠性。

Method: 采用滑动窗口技术检测时间准模式，对数据正则化，应用重采样解决类别不平衡，用LSTM和DLSTM模型训练不同时间序列数据。

Result: 正则化时间序列上集成方法的DLSTM模型表现更好，TSS为0.74，召回率0.95，ROC曲线下面积AUC=0.87。

Conclusion: 先进机器学习技术用于太阳耀斑预测有潜力，结合不同太阳周期阶段和重采样策略可提高预测可靠性。

Abstract: We investigate the use of Long Short-Term Memory (LSTM) and
Decomposition-LSTM (DLSTM) networks, combined with an ensemble algorithm, to
predict solar flare occurrences using time-series data from the GOES catalog.
The dataset spans from 2003 to 2023 and includes 151,071 flare events. Among
approximately possible patterns, 7,552 yearly pattern windows are identified,
highlighting the challenge of long-term forecasting due to the Sun's complex,
self-organized criticality-driven behavior. A sliding window technique is
employed to detect temporal quasi-patterns in both irregular and regularized
flare time series. Regularization reduces complexity, enhances large flare
activity, and captures active days more effectively. To address class
imbalance, resampling methods are applied. LSTM and DLSTM models are trained on
sequences of peak fluxes and waiting times from irregular time series, while
LSTM and DLSTM, integrated with an ensemble approach, are applied to sliding
windows of regularized time series with a 3-hour interval. Performance metrics,
particularly TSS (0.74), recall (0.95) and the area under the curve (AUC=0.87)
in the receiver operating characteristic (ROC), indicate that DLSTM with an
ensemble approach on regularized time series outperforms other models, offering
more accurate large-flare forecasts with fewer false errors compared to models
trained on irregular time series. The superior performance of DLSTM is
attributed to its ability to decompose time series into trend and seasonal
components, effectively isolating random noise. This study underscores the
potential of advanced machine learning techniques for solar flare prediction
and highlights the importance of incorporating various solar cycle phases and
resampling strategies to enhance forecasting reliability.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [295] [A Satellite-Ground Synergistic Large Vision-Language Model System for Earth Observation](https://arxiv.org/abs/2507.05731)
*Yuxin Zhang,Jiahao Yang,Zhe Chen,Wenjun Zhu,Jin Zhao,Yue Gao*

Main category: cs.NI

TL;DR: 提出SpaceVerse系统以解决LVLM在低轨卫星网络部署问题，有准确率提升和延迟降低效果。


<details>
  <summary>Details</summary>
Motivation: 现有低轨卫星地球观测图像数据下载存在挑战，为实现近实时地球观测应用，需探索LVLM在低轨卫星网络的部署。

Method: 在卫星上部署紧凑型LVLM处理轻量级任务，地面站用常规LVLM处理计算密集型任务；提出包含渐进置信网络和基于注意力的多尺度预处理的计算与通信协同设计框架。

Result: 在真实低轨卫星星座和数据集上实现，与现有基线相比，准确率平均提高31.2%，延迟降低51.2%。

Conclusion: SpaceVerse系统有效解决了LVLM在低轨卫星网络的部署问题，提升了性能。

Abstract: Recently, large vision-language models (LVLMs) unleash powerful analysis
capabilities for low Earth orbit (LEO) satellite Earth observation images in
the data center. However, fast satellite motion, brief satellite-ground station
(GS) contact windows, and large size of the images pose a data download
challenge. To enable near real-time Earth observation applications (e.g.,
disaster and extreme weather monitoring), we should explore how to deploy LVLM
in LEO satellite networks, and design SpaceVerse, an efficient satellite-ground
synergistic LVLM inference system. To this end, firstly, we deploy compact
LVLMs on satellites for lightweight tasks, whereas regular LVLMs operate on GSs
to handle computationally intensive tasks. Then, we propose a computing and
communication co-design framework comprised of a progressive confidence network
and an attention-based multi-scale preprocessing, used to identify on-satellite
inferring data, and reduce data redundancy before satellite-GS transmission,
separately. We implement and evaluate SpaceVerse on real-world LEO satellite
constellations and datasets, achieving a 31.2% average gain in accuracy and a
51.2% reduction in latency compared to state-of-the-art baselines.

</details>


### [296] [Intra-DP: A High Performance Collaborative Inference System for Mobile Edge Computing](https://arxiv.org/abs/2507.05829)
*Zekai Sun,Xiuxian Guan,Zheng Lin,Zihan Fang,Xiangming Cai,Zhe Chen,Fangming Liu,Heming Cui,Jie Xiong,Wei Ni,Chau Yuen*

Main category: cs.NI

TL;DR: 论文提出Intra - DP系统优化MEC上的DNN推理，通过并行计算减少传输瓶颈，评估显示能降低推理延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的移动设备上部署DNN面临实时性能、计算资源和电池寿命等挑战，现有MEC方法存在传输瓶颈。

Method: 提出Intra - DP系统，采用基于局部算子的并行计算技术，将计算分解为独立子操作并并行执行以重叠计算和传输。

Result: 与现有基线相比，Intra - DP最多降低50%的推理延迟和75%的能耗，且不损失精度。

Conclusion: Intra - DP能有效解决MEC中的传输瓶颈问题，实现快速且节能的推理。

Abstract: Deploying deep neural networks (DNNs) on resource-constrained mobile devices
presents significant challenges, particularly in achieving real-time
performance while simultaneously coping with limited computational resources
and battery life. While Mobile Edge Computing (MEC) offers collaborative
inference with GPU servers as a promising solution, existing approaches
primarily rely on layer-wise model partitioning and undergo significant
transmission bottlenecks caused by the sequential execution of DNN operations.
To address this challenge, we present Intra-DP, a high-performance
collaborative inference system optimized for DNN inference on MEC. Intra DP
employs a novel parallel computing technique based on local operators (i.e.,
operators whose minimum unit input is not the entire input tensor, such as the
convolution kernel). By decomposing their computations (operations) into
several independent sub-operations and overlapping the computation and
transmission of different sub-operations through parallel execution, Intra-DP
mitigates transmission bottlenecks in MEC, achieving fast and energy-efficient
inference. The evaluation demonstrates that Intra-DP reduces per-inference
latency by up to 50% and energy consumption by up to 75% compared to
state-of-the-art baselines, without sacrificing accuracy.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [297] [Contrastive and Transfer Learning for Effective Audio Fingerprinting through a Real-World Evaluation Protocol](https://arxiv.org/abs/2507.06070)
*Christos Nikou,Theodoros Giannakopoulos*

Main category: cs.SD

TL;DR: 本文提出新评估协议反映真实场景，发现现有模型性能下降，通过改进增强管道和开发基于Transformer的模型提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有歌曲识别方法在真实场景（移动设备、嘈杂环境）下准确率显著下降，需更好反映真实条件的评估协议。

Method: 生成不同噪声水平的音频记录；在增强管道中引入低通和高通滤波器；开发带定制投影模块的Transformer模型并进行知识迁移。

Result: 现有CNN模型在新协议下性能大幅下降；改进增强管道提升系统性能；Transformer模型在各噪声水平和查询时长上均优于CNN模型，在不同条件下取得高检测率。

Conclusion: 提出的评估协议能反映真实场景，改进增强管道和Transformer模型有效提升歌曲识别性能。

Abstract: Recent advances in song identification leverage deep neural networks to learn
compact audio fingerprints directly from raw waveforms. While these methods
perform well under controlled conditions, their accuracy drops significantly in
real-world scenarios where the audio is captured via mobile devices in noisy
environments. In this paper, we introduce a novel evaluation protocol designed
to better reflect such real-world conditions. We generate three recordings of
the same audio, each with increasing levels of noise, captured using a mobile
device's microphone. Our results reveal a substantial performance drop for two
state-of-the-art CNN-based models under this protocol, compared to previously
reported benchmarks. Additionally, we highlight the critical role of the
augmentation pipeline during training with contrastive loss. By introduction
low pass and high pass filters in the augmentation pipeline we significantly
increase the performance of both systems in our proposed evaluation.
Furthermore, we develop a transformer-based model with a tailored projection
module and demonstrate that transferring knowledge from a semantically relevant
domain yields a more robust solution. The transformer architecture outperforms
CNN-based models across all noise levels, and query durations. In low noise
conditions it achieves 47.99% for 1-sec queries, and 97% for 10-sec queries in
finding the correct song, surpassing by 14%, and by 18.5% the second-best
performing model, respectively, Under heavy noise levels, we achieve a
detection rate 56.5% for 15-second query duration. All experiments are
conducted on public large-scale dataset of over 100K songs, with queries
matched against a database of 56 million vectors.

</details>


### [298] [Stable Acoustic Relay Assignment with High Throughput via Lase Chaos-based Reinforcement Learning](https://arxiv.org/abs/2507.05900)
*Zengjing Chen,Lu Wang,Chengzhi Xing*

Main category: cs.SD

TL;DR: 本文研究水下声学网络中稳定声学中继分配问题，引入LC - ML方法，发现其对高吞吐量和适应性有积极影响，模糊认知的稳定配置波动小。


<details>
  <summary>Details</summary>
Motivation: 解决水下声学网络中稳定声学中继分配问题，考虑经典稳定安排和模糊稳定安排两个不同目标。

Method: 引入基于激光混沌的多处理学习（LC - ML）方法，利用激光混沌生成随机数学习中继对多个源节点的分配。

Result: 激光混沌随机数和交换过程中的多处理对更高吞吐量和随时间变化的环境适应性有积极影响，模糊认知的稳定配置波动更小。

Conclusion: 提供了一种实用方法，可作为复杂水下环境中继选择的基础。

Abstract: This study addresses the problem of stable acoustic relay assignment in an
underwater acoustic network. Unlike the objectives of most existing literature,
two distinct objectives, namely classical stable arrangement and ambiguous
stable arrangement, are considered. To achieve these stable arrangements, a
laser chaos-based multi-processing learning (LC-ML) method is introduced to
efficiently obtain high throughput and rapidly attain stability. In order to
sufficiently explore the relay's decision-making, this method uses random
numbers generated by laser chaos to learn the assignment of relays to multiple
source nodes. This study finds that the laser chaos-based random number and
multi-processing in the exchange process have a positive effect on higher
throughput and strong adaptability with environmental changing over time.
Meanwhile, ambiguous cognitions result in the stable configuration with less
volatility compared to accurate ones. This provides a practical and useful
method and can be the basis for relay selection in complex underwater
environments.

</details>


### [299] [Differentiable Reward Optimization for LLM based TTS system](https://arxiv.org/abs/2507.05911)
*Changfeng Gao,Zhihao Du,Shiliang Zhang*

Main category: cs.SD

TL;DR: 提出DiffRO方法提升神经编解码语言模型TTS系统性能，结合MTR模型，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 提升基于神经编解码语言模型的文本转语音（TTS）系统性能。

Method: 提出DiffRO方法，直接基于神经编解码令牌计算奖励，用Gumbel - Softmax使奖励函数可微，引入多任务奖励（MTR）模型。

Result: DiffRO显著提高TTS系统发音准确率，在seed - tts - eval基准上达到SOTA的WER结果，结合MTR模型可零样本控制情感和质量属性。

Conclusion: DiffRO方法有效提升TTS系统性能，MTR模型增强系统遵循指令能力。

Abstract: This paper proposes a novel Differentiable Reward Optimization (DiffRO)
method aimed at enhancing the performance of neural codec language models based
text-to-speech (TTS) systems. In contrast to conventional reinforcement
learning from human feedback (RLHF) approaches applied to TTS, DiffRO directly
compute the rewards based on neural codec tokens, rather than relying on
synthesized audio. Furthermore, we employ the Gumbel-Softmax technique to
render the reward function differentiable, thereby streamlining the RLHF
training process. Additionally, we introduce a multi-task reward (MTR) model
which can provide feedback from different perspectives and find that it can
augment the system's capability to follow instructions effectively.Experimental
results indicate that DiffRO significantly improves the pronunciation accuracy
of the TTS system, achieving state-of-the-art (SOTA) WER results on the
seed-tts-eval benchmark. Moreover, with the integration of the MTR model, we
demonstrate the ability to control emotional and quality attributes in a
zero-shot manner.

</details>


### [300] [Speech Quality Assessment Model Based on Mixture of Experts: System-Level Performance Enhancement and Utterance-Level Challenge Analysis](https://arxiv.org/abs/2507.06116)
*Xintong Hu,Yixuan Chen,Rui Yang,Wenxiang Guo,Changhao Pan*

Main category: cs.SD

TL;DR: 本文提出基于自监督学习语音模型的MOS预测系统，结合MoE分类头和多商业模型合成数据增强，但句子级预测任务性能提升有限。


<details>
  <summary>Details</summary>
Motivation: 现有语音质量评估模型在不同粒度预测任务上性能差异大，需改进。

Method: 基于wav2vec2等自监督模型，设计MoE架构，收集大规模合成语音数据集。

Result: 采用MoE架构和扩大数据集后，句子级预测任务性能提升有限。

Conclusion: 揭示当前方法处理句子级质量评估的局限性，为自动语音质量评估领域提供新途径，探究不同评估粒度性能差异的根本原因。

Abstract: Automatic speech quality assessment plays a crucial role in the development
of speech synthesis systems, but existing models exhibit significant
performance variations across different granularity levels of prediction tasks.
This paper proposes an enhanced MOS prediction system based on self-supervised
learning speech models, incorporating a Mixture of Experts (MoE) classification
head and utilizing synthetic data from multiple commercial generation models
for data augmentation. Our method builds upon existing self-supervised models
such as wav2vec2, designing a specialized MoE architecture to address different
types of speech quality assessment tasks. We also collected a large-scale
synthetic speech dataset encompassing the latest text-to-speech, speech
conversion, and speech enhancement systems. However, despite the adoption of
the MoE architecture and expanded dataset, the model's performance improvements
in sentence-level prediction tasks remain limited. Our work reveals the
limitations of current methods in handling sentence-level quality assessment,
provides new technical pathways for the field of automatic speech quality
assessment, and also delves into the fundamental causes of performance
differences across different assessment granularities.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [301] [Inaugural MOASEI Competition at AAMAS'2025: A Technical Report](https://arxiv.org/abs/2507.05469)
*Ceferino Patino,Tyler J. Billings,Alireza Saleh Abadi,Daniel Redder,Adam Eck,Prashant Doshi,Leen-Kiat Soh*

Main category: cs.MA

TL;DR: 介绍MOASEI竞赛，包括其环境、赛道、参赛团队、评估指标，结果显示对开放环境有策略，为研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 评估开放世界条件下多智能体AI的决策能力。

Method: 基于free - range - zoo环境套件，设置不同赛道，采用预期效用、抗干扰性和环境响应性等评估指标。

Result: 发现了在开放环境中进行泛化和适应的有前景的策略。

Conclusion: 为开放智能体系统研究社区做出贡献，提供了经验见解和基础设施。

Abstract: We present the Methods for Open Agent Systems Evaluation Initiative (MOASEI)
Competition, a multi-agent AI benchmarking event designed to evaluate
decision-making under open-world conditions. Built on the free-range-zoo
environment suite, MOASEI introduced dynamic, partially observable domains with
agent and task openness--settings where entities may appear, disappear, or
change behavior over time. The 2025 competition featured three
tracks--Wildfire, Rideshare, and Cybersecurity--each highlighting distinct
dimensions of openness and coordination complexity. Eleven teams from
international institutions participated, with four of those teams submitting
diverse solutions including graph neural networks, convolutional architectures,
predictive modeling, and large language model--driven meta--optimization.
Evaluation metrics centered on expected utility, robustness to perturbations,
and responsiveness to environmental change. The results reveal promising
strategies for generalization and adaptation in open environments, offering
both empirical insight and infrastructure for future research. This report
details the competition's design, findings, and contributions to the open-agent
systems research community.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [302] [Information Needs and Practices Supported by ChatGPT](https://arxiv.org/abs/2507.05537)
*Tim Gorichanaz*

Main category: cs.HC

TL;DR: 通过对205个用户案例的定性内容分析，研究人们使用ChatGPT的信息需求及它支持的信息实践，得出相关结果并提出信息需求的新概念及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究人们使用ChatGPT的信息需求以及ChatGPT支持的信息实践。

Method: 对205个用户案例进行定性内容分析。

Result: ChatGPT用于多个生活领域和满足多种人类需求，支持写作、决策等六类信息实践。

Conclusion: 在AI时代，信息需求应被概念化为在世界中巧妙应对，此研究为生成式AI与信息需求等领域的未来研究提供众多机会。

Abstract: This study considers ChatGPT as an information source, investigating the
information needs that people come to ChatGPT with and the information
practices that ChatGPT supports, through a qualitative content analysis of 205
user vignettes. The findings show that ChatGPT is used in a range of life
domains (home/family, work, leisure, etc.) and for a range of human needs
(writing/editing, learning, simple programming tasks, etc.), constituting the
information needs that people use ChatGPT to address. Related to these
information needs, the findings show six categories of information practices
that ChatGPT supports: Writing, Deciding, Identifying, Ideating, Talking, and
Critiquing. This work suggests that, in the AI age, information need should be
conceptualized not just as a matter of "getting questions answered" or even
"making sense," but as skillfully coping in the world, a notion that includes
both understanding and action. This study leads to numerous opportunities for
future work at the junction of generative AI and information needs, seeking,
use and experience.

</details>


### [303] [Challenges & Opportunities with LLM-Assisted Visualization Retargeting](https://arxiv.org/abs/2507.01436)
*Luke S. Snyder,Chenglong Wang,Steven M. Drucker*

Main category: cs.HC

TL;DR: 现有自定义图表适配新数据集困难，研究用大语言模型（LLMs）辅助适配，评估两种方法并给出设计建议。


<details>
  <summary>Details</summary>
Motivation: 现有可视化示例适配新数据集困难、耗时且繁琐，大语言模型的进步使代码自动适配成为可能，为了解其辅助适配的效果和局限。

Method: 对多个不同复杂度的数据集和图表评估LLM辅助性能，比较直接让LLM生成适配代码和用LLM提供结构信息指导代码构建两种方法。

Result: 两种方法在新数据未适当转换时表现不佳。

Conclusion: 为未来的重定向系统提出重要设计建议。

Abstract: Despite the ubiquity of visualization examples published on the web,
retargeting existing custom chart implementations to new datasets remains
difficult, time-intensive, and tedious. The adaptation process assumes author
familiarity with both the implementation of the example as well as how the new
dataset might need to be transformed to fit into the example code. With recent
advances in Large Language Models (LLMs), automatic adaptation of code can be
achieved from high-level user prompts, reducing the barrier for visualization
retargeting. To better understand how LLMs can assist retargeting and its
potential limitations, we characterize and evaluate the performance of LLM
assistance across multiple datasets and charts of varying complexity,
categorizing failures according to type and severity. In our evaluation, we
compare two approaches: (1) directly instructing the LLM model to fully
generate and adapt code by treating code as text inputs and (2) a more
constrained program synthesis pipeline where the LLM guides the code
construction process by providing structural information (e.g., visual
encodings) based on properties of the example code and data. We find that both
approaches struggle when new data has not been appropriately transformed, and
discuss important design recommendations for future retargeting systems.

</details>


### [304] [Constella: Supporting Storywriters' Interconnected Character Creation through LLM-based Multi-Agents](https://arxiv.org/abs/2507.05820)
*Syemin Park,Soobin Park,Youn-kyung Lim*

Main category: cs.HC

TL;DR: 研究发现编剧创作角色时存在困难，设计了基于大语言模型的多智能体工具Constella，部署研究表明该工具对角色创作有积极效果，最后讨论多智能体交互作用。


<details>
  <summary>Details</summary>
Motivation: 解决编剧在创作角色时难以设想新角色、平衡角色异同和细化角色关系的问题。

Method: 先进行了有14人参与的形成性研究，然后设计了Constella工具，最后开展了为期7 - 8天、有11名编剧参与的部署研究。

Result: Constella能帮助创建相关角色组成的庞大群体，便于比较角色的思想和情感，加深编剧对角色关系的理解。

Conclusion: 多智能体交互有助于编剧在角色创作中分配注意力和精力。

Abstract: Creating a cast of characters by attending to their relational dynamics is a
critical aspect of most long-form storywriting. However, our formative study
(N=14) reveals that writers struggle to envision new characters that could
influence existing ones, to balance similarities and differences among
characters, and to intricately flesh out their relationships. Based on these
observations, we designed Constella, an LLM-based multi-agent tool that
supports storywriters' interconnected character creation process. Constella
suggests related characters (FRIENDS DISCOVERY feature), reveals the inner
mindscapes of several characters simultaneously (JOURNALS feature), and
manifests relationships through inter-character responses (COMMENTS feature).
Our 7-8 day deployment study with storywriters (N=11) shows that Constella
enabled the creation of expansive communities composed of related characters,
facilitated the comparison of characters' thoughts and emotions, and deepened
writers' understanding of character relationships. We conclude by discussing
how multi-agent interactions can help distribute writers' attention and effort
across the character cast.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [305] [Self-Attention Based Multi-Scale Graph Auto-Encoder Network of 3D Meshes](https://arxiv.org/abs/2507.05304)
*Saqib Nazir,Olivier Lézoray,Sébastien Bougleux*

Main category: cs.GR

TL;DR: 本文提出3DGeometric Mesh Network (3DGeoMeshNet)，用各向异性卷积层直接在空间域学习全局和局部特征，保留原始多边形网格格式，经实验验证其重建准确性高。


<details>
  <summary>Details</summary>
Motivation: CNN处理不规则3D网格数据有挑战，现有GCN方法在捕捉局部和全局网格特征上有局限，需要新方法。

Method: 引入基于GCN的3DGeoMeshNet框架，使用各向异性卷积层，采用多尺度编解码器结构，有全局和局部路径。

Result: 在含人脸的COMA数据集上实验，证明3DGeoMeshNet在重建准确性方面有效。

Conclusion: 3DGeoMeshNet能有效学习全局和局部特征，保留原始网格格式，实现更准确的形状重建。

Abstract: 3D meshes are fundamental data representations for capturing complex
geometric shapes in computer vision and graphics applications. While
Convolutional Neural Networks (CNNs) have excelled in structured data like
images, extending them to irregular 3D meshes is challenging due to the
non-Euclidean nature of the data. Graph Convolutional Networks (GCNs) offer a
solution by applying convolutions to graph-structured data, but many existing
methods rely on isotropic filters or spectral decomposition, limiting their
ability to capture both local and global mesh features. In this paper, we
introduce 3D Geometric Mesh Network (3DGeoMeshNet), a novel GCN-based framework
that uses anisotropic convolution layers to effectively learn both global and
local features directly in the spatial domain. Unlike previous approaches that
convert meshes into intermediate representations like voxel grids or point
clouds, our method preserves the original polygonal mesh format throughout the
reconstruction process, enabling more accurate shape reconstruction. Our
architecture features a multi-scale encoder-decoder structure, where separate
global and local pathways capture both large-scale geometric structures and
fine-grained local details. Extensive experiments on the COMA dataset
containing human faces demonstrate the efficiency of 3DGeoMeshNet in terms of
reconstruction accuracy.

</details>


### [306] [LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for Panorama-Style Mobile Captures](https://arxiv.org/abs/2507.06109)
*Seungoh Han,Jaehoon Jang,Hyunsu Kim,Jaeheung Surh,Junhyung Kwak,Hyowon Ha,Kyungdon Joo*

Main category: cs.GR

TL;DR: 本文提出基于3D高斯渲染的实时新颖视角合成框架LighthouseGS，利用简单全景式手持相机运动，在室内场景取得超越现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS实现高保真渲染需精心拍摄图像，限制普通用户使用，且简单全景式运动下相机位姿和3D点估计有挑战。

Method: 提出LighthouseGS框架，利用几何先验和室内平面结构，采用平面支架组装初始化方法和稳定剪枝策略，引入几何和光度校正。

Result: 在真实和合成室内场景测试中，LighthouseGS实现了逼真渲染，超越了现有方法。

Conclusion: LighthouseGS框架具有全景视图合成和物体放置的潜力。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time novel
view synthesis (NVS) with impressive quality in indoor scenes. However,
achieving high-fidelity rendering requires meticulously captured images
covering the entire scene, limiting accessibility for general users. We aim to
develop a practical 3DGS-based NVS framework using simple panorama-style motion
with a handheld camera (e.g., mobile device). While convenient, this
rotation-dominant motion and narrow baseline make accurate camera pose and 3D
point estimation challenging, especially in textureless indoor scenes. To
address these challenges, we propose LighthouseGS, a novel framework inspired
by the lighthouse-like sweeping motion of panoramic views. LighthouseGS
leverages rough geometric priors, such as mobile device camera poses and
monocular depth estimation, and utilizes the planar structures often found in
indoor environments. We present a new initialization method called plane
scaffold assembly to generate consistent 3D points on these structures,
followed by a stable pruning strategy to enhance geometry and optimization
stability. Additionally, we introduce geometric and photometric corrections to
resolve inconsistencies from motion drift and auto-exposure in mobile devices.
Tested on collected real and synthetic indoor scenes, LighthouseGS delivers
photorealistic rendering, surpassing state-of-the-art methods and demonstrating
the potential for panoramic view synthesis and object placement.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [307] [A new engineering theory describing oblique free surface impact by flexible plates](https://arxiv.org/abs/2103.08012)
*Wensi Wu,Christopher Earls*

Main category: physics.flu-dyn

TL;DR: 文章针对规划船体结构设计中考虑砰击载荷的问题，提出一种新理论，采用FSI模拟方法研究，通过实验验证求解器，经数值分析提出新工程理论。


<details>
  <summary>Details</summary>
Motivation: 砰击现象中复杂流体流动与非线性结构变形相互作用复杂，尚无通用工程理论，需为特定设计案例提出理论。

Method: 采用专门的隐式、分区流固耦合（FSI）模拟方法，先进行柔性板入水实验验证求解器，后开展数值分析。

Result: 通过实验验证了FSI求解器的准确性，完成数值分析以表征冲击力和板变形随冲击速度和板抗弯刚度的变化情况。

Conclusion: 提出了一种用于柔性板斜向冲击水面的新颖且简单的工程理论。

Abstract: Consideration of slamming loads within the structural design of planning
hulls is of critical importance in ensuring adequate structural performance in
order to avoid potential catastrophic consequences. However, because of the
intricacy in the interplay between complex fluid flows and nonlinear structural
deformations that accompany the phenomenology of slamming, a general
engineering theory in slamming has yet to be uncovered, and so design relies on
specialized theories. In this paper, we propose one such theory for a design
case that has, until now, eluded a proper description. In pursuit of this
theory, we employ a specialized implicit, partitioned fluid-structural
interaction (FSI) simulation approach, in order to study the underlying
physical mechanisms accompanying the oblique impact of a flexible plate during
water entry. In the present work, we first present validation results from
flexible plate water entry experiments, to confirm the veracity of the
developed FSI solver. Subsequent to validation, we carry out a series of
numerical analyses, in an effort to characterize the regimes in impact force
and plate out-of-plane deformations, as a function of impact velocities and
plate flexural rigidity. Finally, we use our FSI solver, as a kind of
"microscope", to study the mechanistic evolution of fluid flows and elastic
plate deformations that occur during slamming. Based on these observations, we
propose a novel, but simple engineering theory for flexible plates obliquely
impacting the water free surface (e.g. high speed porpoising water craft
reentry).

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [308] [BMFM-DNA: A SNP-aware DNA foundation model to capture variant effects](https://arxiv.org/abs/2507.05265)
*Hongyang Li,Sanjoy Dey,Bum Chul Kwon,Michael Danziger,Michal Rosen-Tzvi,Jianying Hu,James Kozloski,Ching-Huei Tsou,Bharath Dandala,Pablo Meyer*

Main category: q-bio.GN

TL;DR: 现有DNA语言模型无法在序列变异时编码生物功能，本文预训练基础模型整合序列变异，结果表明此举有助于捕捉生物功能，还进行SNP插补实验，最后发布模型和代码。


<details>
  <summary>Details</summary>
Motivation: 现有DNA语言模型如DNABERT、GENA - LM不能在存在序列变异时编码生物功能，需要解决该问题。

Method: 使用ModernBERT预训练两个生物医学基础模型BMFM - DNA - REF和BMFM - DNA - SNP，前者用参考基因组的不同长度序列及其反向互补序列训练，后者用编码序列变异的新表示方案创建的序列训练。

Result: 将序列变异整合到DNA语言模型中有助于捕捉生物功能，在所有微调任务上有改进；在启动子检测任务上进行SNP插补实验。

Conclusion: 当前基准测试评估模型能力有限，为便于未来全面评估和鼓励社区贡献，通过HuggingFace发布模型，在GitHub发布复现结果的代码。

Abstract: Large language models (LLMs) trained on text demonstrated remarkable results
on natural language processing (NLP) tasks. These models have been adapted to
decipher the language of DNA, where sequences of nucleotides act as "words"
that encode genomic functions. However, the genome differs fundamentally from
natural language, as it lacks clearly defined words or a consistent grammar.
Although DNA language models (DNALMs) such as DNABERT, GENA-LM have achieved
high level of performance on genome-related biological tasks, these models do
not encode biological functions in the presence of sequence variations. To
address this problem, we pre-train foundation models that effectively integrate
sequence variations, in particular Single Nucleotide Polymorphisms (SNPs), as
they underlie important biological functions. Specifically, we use ModernBERT
to pre-train two different Biomedical Foundation Models (BMFM), namely,
BMFM-DNA-REF in which the model is trained with sequences of varying lengths
along with their reverse complements derived from the reference genome and
BMFM-DNA-SNP in which the model is trained with sequences created using a novel
representation scheme that encodes sequence variations. Our findings indicate
that integrating sequence variations into DNALMs helps capture the biological
functions as seen in improvements on all fine-tuning tasks. To explore the
model's practical utility, we experimented with various strategies for SNP
imputation on promoter detection task introduced in DNABERT-2. However, we
acknowledge that the current benchmarks are limited in their ability to fully
evaluate these models. To enable more comprehensive assessment in the future
and encourage community contributions, we release our models through
HuggingFace and the code to reproduce the results at
https://github.com/BiomedSciAI/biomed-multi-omic

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [309] [A Differential Evolution Algorithm with Neighbor-hood Mutation for DOA Estimation](https://arxiv.org/abs/2507.06020)
*Bo Zhou,Kaijie Xu,Yinghui Quan,Mengdao Xing*

Main category: eess.SP

TL;DR: 将二维MUSIC算法的峰值查找转化为多模态优化问题，提出DE - NM算法，降低计算成本，实现实时高分辨率DOA估计。


<details>
  <summary>Details</summary>
Motivation: 传统二维MUSIC算法在二维角度域的穷举搜索计算成本高，限制其实时应用。

Method: 将峰值查找过程重新表述为多模态优化问题，提出带邻域变异的差分进化算法（DE - NM）来定位多个谱峰，无需密集网格采样。

Result: 仿真结果表明，该方法与传统网格搜索估计精度相当，但显著减少计算时间。

Conclusion: 该策略为实际应用中的实时高分辨率DOA估计提供了有前景的解决方案。

Abstract: Two-dimensional (2D) Multiple Signal Classification algorithm is a powerful
technique for high-resolution direction-of-arrival (DOA) estimation in array
signal processing. However, the exhaustive search over the 2D an-gular domain
leads to high computa-tional cost, limiting its applicability in real-time
scenarios. In this work, we reformulate the peak-finding process as a
multimodal optimization prob-lem, and propose a Differential Evolu-tion
algorithm with Neighborhood Mutation (DE-NM) to efficiently lo-cate multiple
spectral peaks without requiring dense grid sampling. Simu-lation results
demonstrate that the proposed method achieves comparable estimation accuracy to
the traditional grid search, while significantly reduc-ing computation time.
This strategy presents a promising solution for real-time, high-resolution DOA
estimation in practical applications. The imple-mentation code is available at
https://github.com/zzb-nice/DOA_multimodel_optimize.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [310] [What ZTF Saw Where Rubin Looked: Anomaly Hunting in DR23](https://arxiv.org/abs/2507.06217)
*Maria V. Pruzhinskaya,Anastasia D. Lavrukhina,Timofey A. Semenikhi,Alina A. Volnova,Sreevarsha Sreejith,Vadim V. Krushinsky,Emmanuel Gangler,Emille E. O. Ishida,Matwey V. Kornilov,Konstantin L. Malanchev*

Main category: astro-ph.IM

TL;DR: 文章介绍了SNAD VIII研讨会成果，利用算法分析ZTF和LSSTComCam观测区域，发现新变星并优化已知天体分类和周期，展示了检测管道有效性和LSST数据发现潜力。


<details>
  <summary>Details</summary>
Motivation: 在Rubin科学管道调试期间，对ZTF和LSSTComCam观测区域进行首次系统异常搜索。

Method: 使用PineForest主动异常检测算法，分析四个选定区域并对400个候选对象进行目视检查。

Result: 发现六个未编目的变星，优化六个已知天体的分类和周期。

Conclusion: 证明了SNAD异常检测管道的有效性，展示了即将到来的LSST数据的发现潜力。

Abstract: We present results from the SNAD VIII Workshop, during which we conducted the
first systematic anomaly search in the ZTF fields also observed by LSSTComCam
during Rubin Scientific Pipeline commissioning. Using the PineForest active
anomaly detection algorithm, we analysed four selected fields (two galactic and
two extragalactic) and visually inspected 400 candidates. As a result, we
discovered six previously uncatalogued variable stars, including RS~CVn, BY
Draconis, ellipsoidal, and solar-type variables, and refined classifications
and periods for six known objects. These results demonstrate the effectiveness
of the SNAD anomaly detection pipeline and provide a preview of the discovery
potential in the upcoming LSST data.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [311] [Critical Nodes Identification in Complex Networks: A Survey](https://arxiv.org/abs/2507.06164)
*Duxin Chen,Jiawen Chen,Xiaoyu Zhang,Qinghan Jia,Xiaolu Liu,Ye Sun,Linyuan Lv,Wenwu Yu*

Main category: cs.SI

TL;DR: 本文全面回顾关键节点识别技术，将其分为七类，弥补现有综述不足，指出关键挑战和开放性问题。


<details>
  <summary>Details</summary>
Motivation: 现实网络的复杂性和异质性给关键节点识别通用框架开发带来障碍，需对相关技术进行全面综述。

Method: 将关键节点识别技术分为七类，基于方法基础和实际意义系统分类。

Result: 明确关键挑战，如算法通用性、动态网络实时评估等；总结当前进展并突出开放性问题。

Conclusion: 本次综述有助于提升对关键节点研究的理解，为后续研究指明方向。

Abstract: Complex networks have become essential tools for understanding diverse
phenomena in social systems, traffic systems, biomolecular systems, and
financial systems. Identifying critical nodes is a central theme in
contemporary research, serving as a vital bridge between theoretical
foundations and practical applications. Nevertheless, the intrinsic complexity
and structural heterogeneity characterizing real-world networks, with
particular emphasis on dynamic and higher-order networks, present substantial
obstacles to the development of universal frameworks for critical node
identification. This paper provides a comprehensive review of critical node
identification techniques, categorizing them into seven main classes:
centrality, critical nodes deletion problem, influence maximization, network
control, artificial intelligence, higher-order and dynamic methods. Our review
bridges the gaps in existing surveys by systematically classifying methods
based on their methodological foundations and practical implications, and by
highlighting their strengths, limitations, and applicability across different
network types. Our work enhances the understanding of critical node research by
identifying key challenges, such as algorithmic universality, real-time
evaluation in dynamic networks, analysis of higher-order structures, and
computational efficiency in large-scale networks. The structured synthesis
consolidates current progress and highlights open questions, particularly in
modeling temporal dynamics, advancing efficient algorithms, integrating machine
learning approaches, and developing scalable and interpretable metrics for
complex systems.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [312] [MP-ALOE: An r2SCAN dataset for universal machine learning interatomic potentials](https://arxiv.org/abs/2507.05559)
*Matthew C. Kuner,Aaron D. Kaplan,Kristin A. Persson,Mark Asta,Daryl C. Chrzan*

Main category: cond-mat.mtrl-sci

TL;DR: 介绍MP - ALOE数据集，用其训练机器学习原子间势并测试性能，结果良好且公开。


<details>
  <summary>Details</summary>
Motivation: 创建准确且涵盖广泛元素的数据集用于机器学习原子间势的研究。

Method: 使用主动学习创建包含近100万个基于r2SCAN元广义梯度近似的DFT计算的MP - ALOE数据集，用其训练机器学习原子间势并进行一系列基准测试。

Result: MP - ALOE在各项基准测试中表现出色。

Conclusion: MP - ALOE数据集具有良好性能，可向更广泛社区公开使用。

Abstract: We present MP-ALOE, a dataset of nearly 1 million DFT calculations using the
accurate r2SCAN meta-generalized gradient approximation. Covering 89 elements,
MP-ALOE was created using active learning and primarily consists of
off-equilibrium structures. We benchmark a machine learning interatomic
potential trained on MP-ALOE, and evaluate its performance on a series of
benchmarks, including predicting the thermochemical properties of equilibrium
structures; predicting forces of far-from-equilibrium structures; maintaining
physical soundness under static extreme deformations; and molecular dynamic
stability under extreme temperatures and pressures. MP-ALOE shows strong
performance on all of these benchmarks, and is made public for the broader
community to utilize.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [313] [An Optimal Algorithm for Shortest Paths in Unweighted Disk Graphs](https://arxiv.org/abs/2507.05569)
*Bruce W. Brewer,Haitao Wang*

Main category: cs.CG

TL;DR: 本文提出一个计算盘图中最短路径的算法，时间复杂度为O(nlogn)，达到理论下界且算法简单。


<details>
  <summary>Details</summary>
Motivation: 前人解决盘图中从源点到所有顶点最短路径问题的最优算法时间复杂度为O(nlog²n)，作者希望设计出更优算法。

Method: 文中未详细提及具体算法内容。

Result: 提出一个时间复杂度为O(nlogn)的算法。

Conclusion: 该算法达到此问题在代数决策树模型下的理论下界，是最优算法，且算法简单。

Abstract: Given in the plane a set $S$ of $n$ points and a set of disks centered at
these points, the disk graph $G(S)$ induced by these disks has vertex set $S$
and an edge between two vertices if their disks intersect. Note that the disks
may have different radii. We consider the problem of computing shortest paths
from a source point $s\in S$ to all vertices in $G(S)$ where the length of a
path in $G(S)$ is defined as the number of edges in the path. The previously
best algorithm solves the problem in $O(n\log^2 n)$ time. A lower bound of
$\Omega(n\log n)$ is also known for this problem under the algebraic decision
tree model. In this paper, we present an $O(n\log n)$ time algorithm, which
matches the lower bound and thus is optimal. Another virtue of our algorithm is
that it is quite simple.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [314] [PWD: Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle CT](https://arxiv.org/abs/2507.05317)
*Yi Liu,Yiyang Wen,Zekun Zhou,Junqi Ma,Linghang Wang,Yucheng Yao,Liu Shi,Qiegen Liu*

Main category: eess.IV

TL;DR: 提出用于LACT重建的PWD模型，高效采样且保留重建保真度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 标准扩散模型推理时采样步骤多、计算开销大，跳跃采样会损失细节。

Method: 训练时将LACT图像分布映射到全采样目标图像分布，推理时以LACT图像为先验引导采样，在小波域进行多尺度特征融合。

Result: 在临床数据集上，相同采样条件下PWD优于现有方法，50步采样PSNR至少提升1.7 dB，SSIM提升10%。

Conclusion: PWD能高效采样并保留重建保真度，减轻跳跃采样的退化问题。

Abstract: Generative diffusion models have received increasing attention in medical
imaging, particularly in limited-angle computed tomography (LACT). Standard
diffusion models achieve high-quality image reconstruction but require a large
number of sampling steps during inference, resulting in substantial
computational overhead. Although skip-sampling strategies have been proposed to
improve efficiency, they often lead to loss of fine structural details. To
address this issue, we propose a prior information embedding and wavelet
feature fusion fast sampling diffusion model for LACT reconstruction. The PWD
enables efficient sampling while preserving reconstruction fidelity in LACT,
and effectively mitigates the degradation typically introduced by
skip-sampling. Specifically, during the training phase, PWD maps the
distribution of LACT images to that of fully sampled target images, enabling
the model to learn structural correspondences between them. During inference,
the LACT image serves as an explicit prior to guide the sampling trajectory,
allowing for high-quality reconstruction with significantly fewer steps. In
addition, PWD performs multi-scale feature fusion in the wavelet domain,
effectively enhancing the reconstruction of fine details by leveraging both
low-frequency and high-frequency information. Quantitative and qualitative
evaluations on clinical dental arch CBCT and periapical datasets demonstrate
that PWD outperforms existing methods under the same sampling condition. Using
only 50 sampling steps, PWD achieves at least 1.7 dB improvement in PSNR and
10% gain in SSIM.

</details>


### [315] [ADPv2: A Hierarchical Histological Tissue Type-Annotated Dataset for Potential Biomarker Discovery of Colorectal Disease](https://arxiv.org/abs/2507.05656)
*Zhiyuan Yang,Kai Li,Sophia Ghamoshi Ramandi,Patricia Brassard,Hakim Khellaf,Vincent Quoc-Huy Trinh,Jennifer Zhang,Lina Chen,Corwyn Rowsell,Sonal Varma,Kostas Plataniotis,Mahdi S. Hosseini*

Main category: eess.IV

TL;DR: 本文介绍了专注胃肠道组织病理学的新数据集ADPv2，训练多标签模型取得mAP 0.88的结果，可用于器官特异性深入研究。


<details>
  <summary>Details</summary>
Motivation: 现有计算病理学公开数据集缺乏细粒度组织类型标注，且难以对特定器官疾病进行深入研究。

Method: 构建包含20004个图像块、32种3级组织类型标注的ADPv2数据集，采用两阶段训练流程训练多标签表示学习模型，使用VMamba架构。

Result: 多标签分类结肠组织类型的平均精度均值（mAP）达到0.88，分析模型预测行为揭示结肠癌发展的两种病理途径。

Conclusion: ADPv2数据集可用于器官特异性深入研究，有望发现潜在生物标志物，且该数据集已公开。

Abstract: Computational pathology (CoPath) leverages histopathology images to enhance
diagnostic precision and reproducibility in clinical pathology. However,
publicly available datasets for CoPath that are annotated with extensive
histological tissue type (HTT) taxonomies at a granular level remain scarce due
to the significant expertise and high annotation costs required. Existing
datasets, such as the Atlas of Digital Pathology (ADP), address this by
offering diverse HTT annotations generalized to multiple organs, but limit the
capability for in-depth studies on specific organ diseases. Building upon this
foundation, we introduce ADPv2, a novel dataset focused on gastrointestinal
histopathology. Our dataset comprises 20,004 image patches derived from healthy
colon biopsy slides, annotated according to a hierarchical taxonomy of 32
distinct HTTs of 3 levels. Furthermore, we train a multilabel representation
learning model following a two-stage training procedure on our ADPv2 dataset.
We leverage the VMamba architecture and achieving a mean average precision
(mAP) of 0.88 in multilabel classification of colon HTTs. Finally, we show that
our dataset is capable of an organ-specific in-depth study for potential
biomarker discovery by analyzing the model's prediction behavior on tissues
affected by different colon diseases, which reveals statistical patterns that
confirm the two pathological pathways of colon cancer development. Our dataset
is publicly available here: Part 1 at https://zenodo.org/records/15307021, Part
2 at https://zenodo.org/records/15312384 and Part 3 at
https://zenodo.org/records/15312792

</details>


### [316] [PSAT: Pediatric Segmentation Approaches via Adult Augmentations and Transfer Learning](https://arxiv.org/abs/2507.05764)
*Tristan Kirscher,Sylvain Faisan,Xavier Coubez,Loris Barrier,Philippe Meyer*

Main category: eess.IV

TL;DR: 本文介绍PSAT研究，探究nnU - Net框架策略对儿科分割性能的影响，在两个儿科CT数据集上进行基准测试，揭示成人指纹数据集训练计划的不足及持续学习策略的优势。


<details>
  <summary>Details</summary>
Motivation: 儿科医学影像因与成人有显著解剖和发育差异，直接应用成人数据训练的分割模型性能不佳，需探究合适策略提升儿科分割性能。

Method: 提出PSAT研究，从指纹数据集、学习集、数据增强参数和迁移学习方法四个关键轴探究策略，在两个儿科CT数据集上进行基准测试，并与现有方法比较。

Result: 基于成人指纹数据集的训练计划与儿科解剖不匹配，导致性能下降；持续学习策略可缓解机构差异，提升泛化能力。

Conclusion: PSAT指出关键问题并提供改善儿科分割的可行见解，代码开源。

Abstract: Pediatric medical imaging presents unique challenges due to significant
anatomical and developmental differences compared to adults. Direct application
of segmentation models trained on adult data often yields suboptimal
performance, particularly for small or rapidly evolving structures. To address
these challenges, several strategies leveraging the nnU-Net framework have been
proposed, differing along four key axes: (i) the fingerprint dataset (adult,
pediatric, or a combination thereof) from which the Training Plan -including
the network architecture-is derived; (ii) the Learning Set (adult, pediatric,
or mixed), (iii) Data Augmentation parameters, and (iv) the Transfer learning
method (finetuning versus continual learning). In this work, we introduce PSAT
(Pediatric Segmentation Approaches via Adult Augmentations and Transfer
learning), a systematic study that investigates the impact of these axes on
segmentation performance. We benchmark the derived strategies on two pediatric
CT datasets and compare them with state-of-theart methods, including a
commercial radiotherapy solution. PSAT highlights key pitfalls and provides
actionable insights for improving pediatric segmentation. Our experiments
reveal that a training plan based on an adult fingerprint dataset is misaligned
with pediatric anatomy-resulting in significant performance degradation,
especially when segmenting fine structures-and that continual learning
strategies mitigate institutional shifts, thus enhancing generalization across
diverse pediatric datasets. The code is available at
https://github.com/ICANS-Strasbourg/PSAT.

</details>


### [317] [Just Say Better or Worse: A Human-AI Collaborative Framework for Medical Image Segmentation Without Manual Annotations](https://arxiv.org/abs/2507.05815)
*Yizhe Zhang*

Main category: eess.IV

TL;DR: 本文提出一种人机协作医学图像分割框架，减少标注负担，实验表明仅用二元偏好反馈就能实现有竞争力的分割性能。


<details>
  <summary>Details</summary>
Motivation: 手动标注医学图像劳动强度大、耗时，成为医学成像AI系统发展和部署的瓶颈。

Method: 采用偏好学习范式，人类专家提供简单二元反馈，框架包含可适应基础模型、基于特征相似性的标签传播、点击代理和多轮分割学习过程。

Result: 在三个公共数据集上实验，仅用二元偏好反馈实现有竞争力的分割性能。

Conclusion: 提出的方法无需专家直接手动标注图像，能有效减少标注负担。

Abstract: Manual annotation of medical images is a labor-intensive and time-consuming
process, posing a significant bottleneck in the development and deployment of
robust medical imaging AI systems. This paper introduces a novel Human-AI
collaborative framework for medical image segmentation that substantially
reduces the annotation burden by eliminating the need for explicit manual
pixel-level labeling. The core innovation lies in a preference learning
paradigm, where human experts provide minimal, intuitive feedback -- simply
indicating whether an AI-generated segmentation is better or worse than a
previous version. The framework comprises four key components: (1) an adaptable
foundation model (FM) for feature extraction, (2) label propagation based on
feature similarity, (3) a clicking agent that learns from human better-or-worse
feedback to decide where to click and with which label, and (4) a multi-round
segmentation learning procedure that trains a state-of-the-art segmentation
network using pseudo-labels generated by the clicking agent and FM-based label
propagation. Experiments on three public datasets demonstrate that the proposed
approach achieves competitive segmentation performance using only binary
preference feedback, without requiring experts to directly manually annotate
the images.

</details>


### [318] [Enhancing Synthetic CT from CBCT via Multimodal Fusion and End-To-End Registration](https://arxiv.org/abs/2507.06067)
*Maximilian Tschuchnig,Lukas Lamminger,Philipp Steininger,Michael Gadermayr*

Main category: eess.IV

TL;DR: 本文提出通过多模态学习和可学习配准模块提升合成CT（sCT）生成质量，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: CBCT图像有伪影且视觉质量低于传统CT，合成CT是有前景的解决方案，但模态间存在固有不对齐问题。

Method: 联合利用术中CBCT和术前CT数据进行多模态学习，在sCT流程中引入端到端可学习配准模块，在合成数据集上评估模型，在两个真实临床数据集上验证鲁棒性和泛化性。

Result: 在多模态sCT生成中集成配准提高了sCT质量，在90个评估设置中的79个中优于基线多模态方法，在CBCT质量低且术前CT中度不对齐时改进最显著。

Conclusion: 所提方法能有效提升sCT生成质量。

Abstract: Cone-Beam Computed Tomography (CBCT) is widely used for intraoperative
imaging due to its rapid acquisition and low radiation dose. However, CBCT
images typically suffer from artifacts and lower visual quality compared to
conventional Computed Tomography (CT). A promising solution is synthetic CT
(sCT) generation, where CBCT volumes are translated into the CT domain. In this
work, we enhance sCT generation through multimodal learning by jointly
leveraging intraoperative CBCT and preoperative CT data. To overcome the
inherent misalignment between modalities, we introduce an end-to-end learnable
registration module within the sCT pipeline. This model is evaluated on a
controlled synthetic dataset, allowing precise manipulation of data quality and
alignment parameters. Further, we validate its robustness and generalizability
on two real-world clinical datasets. Experimental results demonstrate that
integrating registration in multimodal sCT generation improves sCT quality,
outperforming baseline multimodal methods in 79 out of 90 evaluation settings.
Notably, the improvement is most significant in cases where CBCT quality is low
and the preoperative CT is moderately misaligned.

</details>


### [319] [LangMamba: A Language-driven Mamba Framework for Low-dose CT Denoising with Vision-language Models](https://arxiv.org/abs/2507.06140)
*Zhihao Chen,Tao Chen,Chenhui Wang,Qi Gao,Huidong Xie,Chuang Niu,Ge Wang,Hongming Shan*

Main category: eess.IV

TL;DR: 提出用于LDCT去噪的LangMamba框架，通过两阶段学习策略提升去噪效果，实验表明其优于现有方法，且有良好泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: LDCT降低辐射但图像质量差，现有深度学习去噪方法忽略高层语义引导，而VLMs可提供结构化语义信息，有机会改善LDCT重建。

Method: 采用两阶段学习策略，先预训练LangAE将NDCT图像映射到语义空间，再结合SEED和LangDA Loss指导LDCT去噪。

Result: 在两个公共数据集上实验表明，LangMamba优于传统先进方法，细节保留和视觉保真度显著提升，LangAE泛化性强，LangDA Loss提高可解释性。

Conclusion: 语言作为监督信号对推进LDCT去噪有潜力。

Abstract: Low-dose computed tomography (LDCT) reduces radiation exposure but often
degrades image quality, potentially compromising diagnostic accuracy. Existing
deep learning-based denoising methods focus primarily on pixel-level mappings,
overlooking the potential benefits of high-level semantic guidance. Recent
advances in vision-language models (VLMs) suggest that language can serve as a
powerful tool for capturing structured semantic information, offering new
opportunities to improve LDCT reconstruction. In this paper, we introduce
LangMamba, a Language-driven Mamba framework for LDCT denoising that leverages
VLM-derived representations to enhance supervision from normal-dose CT (NDCT).
LangMamba follows a two-stage learning strategy. First, we pre-train a
Language-guided AutoEncoder (LangAE) that leverages frozen VLMs to map NDCT
images into a semantic space enriched with anatomical information. Second, we
synergize LangAE with two key components to guide LDCT denoising:
Semantic-Enhanced Efficient Denoiser (SEED), which enhances NDCT-relevant local
semantic while capturing global features with efficient Mamba mechanism, and
Language-engaged Dual-space Alignment (LangDA) Loss, which ensures that
denoised images align with NDCT in both perceptual and semantic spaces.
Extensive experiments on two public datasets demonstrate that LangMamba
outperforms conventional state-of-the-art methods, significantly improving
detail preservation and visual fidelity. Remarkably, LangAE exhibits strong
generalizability to unseen datasets, thereby reducing training costs.
Furthermore, LangDA loss improves explainability by integrating language-guided
insights into image reconstruction and offers a plug-and-play fashion. Our
findings shed new light on the potential of language as a supervisory signal to
advance LDCT denoising. The code is publicly available on
https://github.com/hao1635/LangMamba.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [320] [Instance-Optimal Quantum State Certification with Entangled Measurements](https://arxiv.org/abs/2507.06010)
*Ryan O'Donnell,Chirag Wadhwa*

Main category: quant-ph

TL;DR: 本文解决量子态认证中测试者可进行全纠缠测量时实例最优界限的开放问题，给出近实例最优界限。


<details>
  <summary>Details</summary>
Motivation: 已知量子态认证在最坏情况的副本复杂度界限，但实例层面的最优副本复杂度未知，尤其测试者测量无限制时该问题仍开放。

Method: 使用Ingster - Suslina方法的量子类似物证明下界。

Result: 证明测试者可进行全纠缠测量时量子态认证的近实例最优界限，表明认证的最优副本复杂度由最坏情况复杂度乘以假设态与最大混合态的保真度给出，还以简单证明恢复混合性测试的下界。

Conclusion: 解决了测试者可进行全纠缠测量时量子态认证实例最优界限的开放问题，所用方法可能有独立价值。

Abstract: We consider the task of quantum state certification: given a description of a
hypothesis state $\sigma$ and multiple copies of an unknown state $\rho$, a
tester aims to determine whether the two states are equal or $\epsilon$-far in
trace distance. It is known that $\Theta(d/\epsilon^2)$ copies of $\rho$ are
necessary and sufficient for this task, assuming the tester can make entangled
measurements over all copies [CHW07,OW15,BOW19]. However, these bounds are for
a worst-case $\sigma$, and it is not known what the optimal copy complexity is
for this problem on an instance-by-instance basis. While such instance-optimal
bounds have previously been shown for quantum state certification when the
tester is limited to measurements unentangled across copies [CLO22,CLHL22],
they remained open when testers are unrestricted in the kind of measurements
they can perform.
  We address this open question by proving nearly instance-optimal bounds for
quantum state certification when the tester can perform fully entangled
measurements. Analogously to the unentangled setting, we show that the optimal
copy complexity for certifying $\sigma$ is given by the worst-case complexity
times the fidelity between $\sigma$ and the maximally mixed state. We prove our
lower bounds using a novel quantum analogue of the Ingster-Suslina method,
which is likely to be of independent interest. This method also allows us to
recover the $\Omega(d/\epsilon^2)$ lower bound for mixedness testing [OW15],
i.e., certification of the maximally mixed state, with a surprisingly simple
proof.

</details>


### [321] [Special-Unitary Parameterization for Trainable Variational Quantum Circuits](https://arxiv.org/abs/2507.05535)
*Kuan-Cheng Chen,Huan-Hsin Tseng,Samuel Yen-Chi Chen,Chen-Yu Liu,Kin K. Leung*

Main category: quant-ph

TL;DR: 提出SUN - VQC变分电路架构，能减少动态李代数维度，避免梯度消失问题，实验显示其性能优于其他电路，证明李代数子代数工程是解决梯度消失的有效途径。


<details>
  <summary>Details</summary>
Motivation: 解决硬件高效变分量子算法中普遍存在的梯度消失问题。

Method: 提出SUN - VQC架构，将演化限制在对称受限的李群子群的单指数子空间，用广义参数偏移规则获取精确、硬件兼容的梯度。

Result: 在量子自动编码和分类的数值实验中，SUN - VQC比深度匹配的泡利旋转或硬件高效电路有更大的梯度信号，收敛快2 - 3倍，最终保真度更高。

Conclusion: 李代数子代数工程为设计抗梯度消失的变分量子算法提供了原则性、可扩展的方法，且与近期量子处理器兼容。

Abstract: We propose SUN-VQC, a variational-circuit architecture whose elementary
layers are single exponentials of a symmetry-restricted Lie subgroup,
$\mathrm{SU}(2^{k}) \subset \mathrm{SU}(2^{n})$ with $k \ll n$. Confining the
evolution to this compact subspace reduces the dynamical Lie-algebra dimension
from $\mathcal{O}(4^{n})$ to $\mathcal{O}(4^{k})$, ensuring only polynomial
suppression of gradient variance and circumventing barren plateaus that plague
hardware-efficient ans\"atze. Exact, hardware-compatible gradients are obtained
using a generalized parameter-shift rule, avoiding ancillary qubits and
finite-difference bias. Numerical experiments on quantum auto-encoding and
classification show that SUN-VQCs sustain order-of-magnitude larger gradient
signals, converge 2--3$\times$ faster, and reach higher final fidelities than
depth-matched Pauli-rotation or hardware-efficient circuits. These results
demonstrate that Lie-subalgebra engineering provides a principled, scalable
route to barren-plateau-resilient VQAs compatible with near-term quantum
processors.

</details>


### [322] [Learnable quantum spectral filters for hybrid graph neural networks](https://arxiv.org/abs/2507.05640)
*Ammar Daskin*

Main category: quant-ph

TL;DR: 提出可作为图神经网络卷积和池化层的参数化量子电路，结合该电路与经典神经网络进行图分类，结果良好。


<details>
  <summary>Details</summary>
Motivation: 消除用切比雪夫多项式或泰勒展开近似拉普拉斯算子可学习函数时昂贵的经典计算。

Method: 构建结合参数化量子傅里叶电路的参数化量子电路，将其作为卷积层得到概率向量，结合测量作为卷积加池化层压缩信号，再连接经典神经网络预测头。

Result: 使用少量可学习参数和最小经典层，在基准数据集上的图分类结果与许多基线结果相当，部分情况更好。

Conclusion: 该参数化量子电路结合经典神经网络在图分类中有效，尤其在几何结构起重要作用时表现出色。

Abstract: In this paper, we describe a parameterized quantum circuit that can be
considered as convolutional and pooling layers for graph neural networks. The
circuit incorporates the parameterized quantum Fourier circuit where the qubit
connections for the controlled gates derived from the Laplacian operator.
Specifically, we show that the eigenspace of the Laplacian operator of a graph
can be approximated by using QFT based circuit whose connections are determined
from the adjacency matrix. For an $N\times N$ Laplacian, this approach yields
an approximate polynomial-depth circuit requiring only $n=log(N)$ qubits. These
types of circuits can eliminate the expensive classical computations for
approximating the learnable functions of the Laplacian through Chebyshev
polynomial or Taylor expansions.
  Using this circuit as a convolutional layer provides an $n-$ dimensional
probability vector that can be considered as the filtered and compressed graph
signal. Therefore, the circuit along with the measurement can be considered a
very efficient convolution plus pooling layer that transforms an
$N$-dimensional signal input into $n-$dimensional signal with an exponential
compression. We then apply a classical neural network prediction head to the
output of the circuit to construct a complete graph neural network. Since the
circuit incorporates geometric structure through its graph connection-based
approach, we present graph classification results for the benchmark datasets
listed in TUDataset library. Using only [1-100] learnable parameters for the
quantum circuit and minimal classical layers (1000-5000 parameters) in a
generic setting, the obtained results are comparable to and in some cases
better than many of the baseline results, particularly for the cases when
geometric structure plays a significant role.

</details>


<div id='q-bio.MN'></div>

# q-bio.MN [[Back]](#toc)

### [323] [GPU-accelerated Modeling of Biological Regulatory Networks](https://arxiv.org/abs/2506.19866)
*Joyce Reimer,Pranta Saha,Chris Chen,Neeraj Dhar,Brook Byrns,Steven Rayan,Gordon Broderick*

Main category: q-bio.MN

TL;DR: 本文介绍用GPU加速生物网络逻辑模型参数搜索问题的求解，通过实验表明GPU比CPU有显著效率提升，使全局优化方法更具吸引力和可行性。


<details>
  <summary>Details</summary>
Motivation: 生物调控系统参数搜索空间大，为使全局优化方案成为计算机药物研究实用工具，需进行软硬件层面性能优化。

Method: 在GPU计算环境中实现全局优化算法，对两个复杂度大幅提升的生物调控系统模型进行参数搜索。

Result: 与多线程CPU实现相比，GPU效率提高33%-43%；与串行CPU相比，提高33%-1866%。

Conclusion: 这些改进使逻辑模型识别的全局优化成为计算机假说生成和实验设计更有吸引力和可行性的方法。

Abstract: The complex regulatory dynamics of a biological network can be succinctly
captured using discrete logic models. Given even sparse time-course data from
the system of interest, previous work has shown that global optimization
schemes are suitable for proposing logic models that explain the data and make
predictions about how the system will behave under varying conditions.
Considering the large scale of the parameter search spaces associated with
these regulatory systems, performance optimizations on the level of both
hardware and software are necessary for making this a practical tool for in
silico pharmaceutical research. We show here how the implementation of these
global optimization algorithms in a GPU-computing environment can accelerate
the solution of these parameter search problems considerably. We carry out
parameter searches on two model biological regulatory systems that represent
almost an order of magnitude scale-up in complexity, and we find the gains in
efficiency from GPU to be a 33%-43% improvement compared to multi-thread CPU
implementations and a 33%-1866% increase compared to CPU in serial. These
improvements make global optimization of logic model identification a far more
attractive and feasible method for in silico hypothesis generation and design
of experiments.

</details>
