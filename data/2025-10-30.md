<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 28]
- [cs.CE](#cs.CE) [Total: 6]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 78]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 19]
- [stat.ML](#stat.ML) [Total: 9]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.SD](#cs.SD) [Total: 2]
- [eess.SP](#eess.SP) [Total: 9]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.CV](#cs.CV) [Total: 29]
- [cs.PL](#cs.PL) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [econ.GN](#econ.GN) [Total: 4]
- [cs.MA](#cs.MA) [Total: 4]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.CL](#cs.CL) [Total: 28]
- [cs.CY](#cs.CY) [Total: 8]
- [cs.CR](#cs.CR) [Total: 7]
- [math.NA](#math.NA) [Total: 2]
- [cs.HC](#cs.HC) [Total: 6]
- [cs.LO](#cs.LO) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [hep-lat](#hep-lat) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 7]
- [math.ST](#math.ST) [Total: 2]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.IT](#cs.IT) [Total: 3]
- [cs.GR](#cs.GR) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [eess.IV](#eess.IV) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Scheduling Your LLM Reinforcement Learning with Reasoning Trees](https://arxiv.org/abs/2510.24832)
*Hong Wang,Zhezheng Hao,Jian Luo,Chenxing Wei,Yao Shu,Lei Liu,Qiang Lin,Hande Dong,Jiawei Chen*

Main category: cs.AI

TL;DR: 本文提出基于推理树结构的推理分数（r - score）和推理树调度算法（Re - Schedule），用于RLVR优化大语言模型的数据调度，实验显示显著提升平均准确率。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR数据调度方法依赖基于路径的指标对查询排序，忽略了查询的推理树结构。

Method: 引入推理分数（r - score）衡量查询学习难度，提出推理树调度算法（Re - Schedule），按推理树结构从简单到复杂调度查询。

Result: 在六个数学推理基准测试中，Re - Schedule显著提高平均准确率，最高提升3.2%。

Conclusion: 对推理树的结构理解为RLVR数据调度提供了更强大、更有原则的基础。

Abstract: Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large
Language Models (LLMs) can be conceptualized as progressively editing a query's
`Reasoning Tree'. This process involves exploring nodes (tokens) and
dynamically modifying the model's policy at each node. When combined with data
scheduling, this process yields further gains in data efficiency and accuracy.
However, existing RLVR data scheduling methods typically rely on path-based
metrics to rank queries, overlooking the reasoning tree structures of these
queries. In this paper, we introduce a novel metric, namely Reasoning Score
(r-score), which measures the query's learning difficulty based on the
structure of its reasoning tree. Based on the r-score, we propose the Reasoning
Tree Schedule (Re-Schedule), a scheduling algorithm that constructs a
curriculum progressing from structurally simple (high r-score) to complex (low
r-score) queries. Experiments on six math-reasoning benchmarks show that
Re-Schedule significantly improves average accuracy, achieving gains of up to
3.2%. These strong results validate our approach and demonstrate that a
structural understanding of the reasoning tree provides a more powerful and
principled foundation for RLVR data scheduling.

</details>


### [2] [Cyclic Counterfactuals under Shift-Scale Interventions](https://arxiv.org/abs/2510.25005)
*Saptarshi Saha,Dhruv Vansraj Rathore,Utpal Garain*

Main category: cs.AI

TL;DR: 研究在转移 - 缩放干预下循环结构因果模型（SCMs）中的反事实推理。


<details>
  <summary>Details</summary>
Motivation: 传统反事实推理框架多假设无环SCMs，但许多现实系统存在违反无环性的反馈循环或循环依赖。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: Most counterfactual inference frameworks traditionally assume acyclic
structural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However,
many real-world systems (e.g. biological systems) contain feedback loops or
cyclic dependencies that violate acyclicity. In this work, we study
counterfactual inference in cyclic SCMs under shift-scale interventions, i.e.,
soft, policy-style changes that rescale and/or shift a variable's mechanism.

</details>


### [3] [Taming the Real-world Complexities in CPT E/M Coding with Large Language Models](https://arxiv.org/abs/2510.25007)
*Islam Nassar,Yang Lin,Yuan Jin,Rongxin Zhu,Chang Wei Tan,Zenan Zhai,Nitika Mathur,Thanh Tien Vu,Xu Zhong,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: 本文介绍了ProFees，一个基于大语言模型的框架，用于解决E/M编码自动化中的复杂问题，并在真实数据集上证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: E/M编码是医生的辅助任务，增加了文档负担，自动化该任务可减轻负担、提高计费效率和改善患者护理，但现实复杂性使自动化编码具有挑战性。

Method: 详细阐述关键复杂性，提出基于大语言模型的ProFees框架并进行系统评估。

Result: 在专家整理的真实数据集上，ProFees的编码准确率比商业CPT E/M编码系统提高了36%以上，比最强的单提示基线提高了近5%。

Conclusion: ProFees在解决现实世界的复杂性方面是有效的。

Abstract: Evaluation and Management (E/M) coding, under the Current Procedural
Terminology (CPT) taxonomy, documents medical services provided to patients by
physicians. Used primarily for billing purposes, it is in physicians' best
interest to provide accurate CPT E/M codes. %While important, it is an
auxiliary task that adds to physicians' documentation burden. Automating this
coding task will help alleviate physicians' documentation burden, improve
billing efficiency, and ultimately enable better patient care. However, a
number of real-world complexities have made E/M encoding automation a
challenging task. In this paper, we elaborate some of the key complexities and
present ProFees, our LLM-based framework that tackles them, followed by a
systematic evaluation. On an expert-curated real-world dataset, ProFees
achieves an increase in coding accuracy of more than 36\% over a commercial CPT
E/M coding system and almost 5\% over our strongest single-prompt baseline,
demonstrating its effectiveness in addressing the real-world complexities.

</details>


### [4] [Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading](https://arxiv.org/abs/2510.25014)
*Minkyung Kim,Junsik Kim,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: 本文引入ASTP方法解决LLM在游戏交易中无法遵循流程的问题，评估显示效果好，小模型使用该方法能匹配大模型性能并减少响应时间。


<details>
  <summary>Details</summary>
Motivation: 解决LLM创造性灵活性与游戏交易流程要求之间的矛盾，避免玩家信任受损。

Method: 引入Autoregressive State-Tracking Prompting (ASTP)方法，让LLM明确可验证状态跟踪过程，辅以状态特定占位符后处理方法确保价格计算准确。

Result: 300个交易对话评估显示状态合规率超99%，计算精度达99.3%；小模型用该方法匹配大模型性能，响应时间从21.2秒减至2.4秒。

Conclusion: ASTP方法为满足商业游戏实时性和资源限制要求奠定了实用基础。

Abstract: Large Language Models (LLMs) enable dynamic game interactions but fail to
follow essential procedural flows in rule-governed trading systems, eroding
player trust. This work resolves the core tension between the creative
flexibility of LLMs and the procedural demands of in-game trading
(browse-offer-review-confirm). To this end, Autoregressive State-Tracking
Prompting (ASTP) is introduced, a methodology centered on a strategically
orchestrated prompt that compels an LLM to make its state-tracking process
explicit and verifiable. Instead of relying on implicit contextual
understanding, ASTP tasks the LLM with identifying and reporting a predefined
state label from the previous turn. To ensure transactional integrity, this is
complemented by a state-specific placeholder post-processing method for
accurate price calculations. Evaluation across 300 trading dialogues
demonstrates >99% state compliance and 99.3% calculation precision. Notably,
ASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash)
matches larger models' (Gemini-2.5-Pro) performance while reducing response
time from 21.2s to 2.4s, establishing a practical foundation that satisfies
both real-time requirements and resource constraints of commercial games.

</details>


### [5] [Reasoning-Aware GRPO using Process Mining](https://arxiv.org/abs/2510.25065)
*Taekhyun Park,Yongjae Lee,Hyerim Bae*

Main category: cs.AI

TL;DR: 提出推理感知的GRPO方法PM4GRPO，用过程挖掘技术增强推理奖励，在五个基准测试中表现优于现有方法，提升了策略模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的大推理模型后训练奖励方案以结果为中心，需提升推理过程奖励。

Method: 提出PM4GRPO，利用过程挖掘技术计算与预训练教师模型推理一致性的标量一致性奖励。

Result: 在五个基准测试中，PM4GRPO显著优于现有GRPO后训练方法。

Conclusion: 利用过程挖掘进行推理感知的GRPO能有效增强策略模型的推理能力。

Abstract: Reinforcement learning (RL)-based post-training has been crucial for enabling
multi-step reasoning in large reasoning models (LRMs), yet current reward
schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware
Group Relative Policy Optimization (GRPO) that augments standard answer/format
rewards with signals over the reasoning procedure. To this end, process mining
techniques are utilized to compute a scalar conformance reward that measures
how closely a policy model's reasoning aligns with the pretrained teacher
model. The empirical results on five benchmarks demonstrate that PM4GRPO
significantly outperforms existing methodologies for GRPO-based post-training.
These results highlight that leveraging process mining for reasoning-aware GRPO
effectively enhances the reasoning capabilities of policy models.

</details>


### [6] [H3M-SSMoEs: Hypergraph-based Multimodal Learning with LLM Reasoning and Style-Structured Mixture of Experts](https://arxiv.org/abs/2510.25091)
*Peilin Tan,Liang Xie,Churan Zhi,Dian Tu,Chuanqi Shi*

Main category: cs.AI

TL;DR: 本文提出H3M - SSMoEs架构用于股票走势预测，通过多创新模块提升预测效果，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 股票走势预测存在复杂依赖、多模态和动态关系等挑战，现有方法难以在可扩展框架内统一结构、语义和适应机制建模。

Method: 引入H3M - SSMoEs架构，包括多上下文多模态超图、大语言模型增强推理模块和风格结构专家混合体。

Result: 在三大股票市场的广泛实验表明，H3M - SSMoEs在预测准确性、投资表现上超越现有方法，且具备有效风险控制能力。

Conclusion: H3M - SSMoEs是一种有效的股票走势预测架构，相关数据和代码已开源。

Abstract: Stock movement prediction remains fundamentally challenging due to complex
temporal dependencies, heterogeneous modalities, and dynamically evolving
inter-stock relationships. Existing approaches often fail to unify structural,
semantic, and regime-adaptive modeling within a scalable framework. This work
introduces H3M-SSMoEs, a novel Hypergraph-based MultiModal architecture with
LLM reasoning and Style-Structured Mixture of Experts, integrating three key
innovations: (1) a Multi-Context Multimodal Hypergraph that hierarchically
captures fine-grained spatiotemporal dynamics via a Local Context Hypergraph
(LCH) and persistent inter-stock dependencies through a Global Context
Hypergraph (GCH), employing shared cross-modal hyperedges and Jensen-Shannon
Divergence weighting mechanism for adaptive relational learning and cross-modal
alignment; (2) a LLM-enhanced reasoning module, which leverages a frozen large
language model with lightweight adapters to semantically fuse and align
quantitative and textual modalities, enriching representations with
domain-specific financial knowledge; and (3) a Style-Structured Mixture of
Experts (SSMoEs) that combines shared market experts and industry-specialized
experts, each parameterized by learnable style vectors enabling regime-aware
specialization under sparse activation. Extensive experiments on three major
stock markets demonstrate that H3M-SSMoEs surpasses state-of-the-art methods in
both superior predictive accuracy and investment performance, while exhibiting
effective risk control. Datasets, source code, and model weights are available
at our GitHub repository: https://github.com/PeilinTime/H3M-SSMoEs.

</details>


### [7] [KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA](https://arxiv.org/abs/2510.25101)
*Zhuo Chen,Fei Wang,Zixuan Li,Zhao Zhang,Weiwei Ding,Chuanguang Yang,Yongjun Xu,Xiaolong Jin,Jiafeng Guo*

Main category: cs.AI

TL;DR: 本文提出KnowCoder - A1，通过多阶段课程强化学习在仅结果监督下训练大语言模型进行知识库问答，在多个数据集上表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于过程监督合成推理轨迹微调大语言模型的知识库问答方法探索激励弱，无法有效增强智能体推理能力。

Method: 提出KnowCoder - A1，先在基于结果的拒绝采样得到的少量高质量轨迹上微调大语言模型，再应用多阶段课程强化学习，采用由易到难的奖励调度以缓解仅结果监督的奖励稀疏问题。

Result: KnowCoder - A1在三个主流数据集上始终优于先前方法，在GrailQA的零样本子集上使用十二分之一的训练数据实现了高达11.1%的相对提升。

Conclusion: KnowCoder - A1在仅结果监督下训练展现出强大推理行为和智能体推理能力。

Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural-language
questions over a structured Knowledge Base (KB). Recent work improves KBQA by
adopting an agentic reasoning paradigm, in which Large Language Models (LLMs)
iteratively decompose a question, generate its corresponding logical queries,
and interact with the KB to derive the answer. However, these methods typically
fine-tune LLMs on reasoning trajectories synthesized via process supervision,
which offers weak incentives for exploration and thus fails to strengthen the
agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that
can autonomously perform agentic reasoning on KBs to obtain answers. To
incentivize autonomous exploration, KnowCoder-A1 trains the LLM under
outcome-only supervision via a multi-stage curriculum reinforcement learning
with an easy-to-hard curriculum. To establish foundational agentic
capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of
high-quality trajectories obtained through outcome-based rejection sampling.
Then, to alleviate the reward sparsity inherent in outcome-only supervision, it
applies multi-stage curriculum RL with reward schedules that progress from easy
to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful
reasoning behaviors and consistently outperforms prior approaches across three
mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1
achieves up to an 11.1% relative improvement while using only one-twelfth of
the training data, demonstrating strong agentic reasoning capabilities.

</details>


### [8] [Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models](https://arxiv.org/abs/2510.25179)
*Juan Ren,Mark Dras,Usman Naseem*

Main category: cs.AI

TL;DR: 提出Agentic Moderation框架将智能体范式扩展到安全对齐，防御多模态系统越狱攻击，实验显示有良好安全性能。


<details>
  <summary>Details</summary>
Motivation: 将智能体范式扩展到安全对齐，防御多模态系统的越狱攻击。

Method: 引入模型无关的Agentic Moderation框架，集成Shield、Responder、Evaluator和Reflector等动态协作智能体进行上下文感知和可解释的审核。

Result: 在五个数据集和四个代表性大视觉语言模型上实验，降低攻击成功率7 - 19%，保持非跟随率稳定，提高拒绝率4 - 20%。

Conclusion: Agentic Moderation提供模块化、可扩展和细粒度的安全执行，凸显智能体系统作为自动化安全治理基础的潜力。

Abstract: Agentic methods have emerged as a powerful and autonomous paradigm that
enhances reasoning, collaboration, and adaptive control, enabling systems to
coordinate and independently solve complex tasks. We extend this paradigm to
safety alignment by introducing Agentic Moderation, a model-agnostic framework
that leverages specialised agents to defend multimodal systems against
jailbreak attacks. Unlike prior approaches that apply as a static layer over
inputs or outputs and provide only binary classifications (safe or unsafe), our
method integrates dynamic, cooperative agents, including Shield, Responder,
Evaluator, and Reflector, to achieve context-aware and interpretable
moderation. Extensive experiments across five datasets and four representative
Large Vision-Language Models (LVLMs) demonstrate that our approach reduces the
Attack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF),
and improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable,
and well-balanced safety performance. By harnessing the flexibility and
reasoning capacity of agentic architectures, Agentic Moderation provides
modular, scalable, and fine-grained safety enforcement, highlighting the
broader potential of agentic systems as a foundation for automated safety
governance.

</details>


### [9] [Energy-Efficient Autonomous Driving with Adaptive Perception and Robust Decision](https://arxiv.org/abs/2510.25205)
*Yuyang Xia,Zibo Liang,Liwei Deng,Yan Zhao,Han Su,Kai Zheng*

Main category: cs.AI

TL;DR: 提出节能自动驾驶框架EneAD，可降低感知能耗、提升续航。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶能耗上升限制车辆续航，现有模型压缩技术有缺陷。

Method: 在自适应感知模块，设计感知优化策略，用贝叶斯优化调参，用轻量级分类模型区分场景；在决策模块，用强化学习设计决策模型并加正则项。

Result: EneAD可将感知能耗降低1.9 - 3.5倍，提升续航3.9% - 8.5%。

Conclusion: EneAD在能耗和驾驶性能上表现优越。

Abstract: Autonomous driving is an emerging technology that is expected to bring
significant social, economic, and environmental benefits. However, these
benefits come with rising energy consumption by computation engines, limiting
the driving range of vehicles, especially electric ones. Perception computing
is typically the most power-intensive component, as it relies on largescale
deep learning models to extract environmental features. Recently, numerous
studies have employed model compression techniques, such as sparsification,
quantization, and distillation, to reduce computational consumption. However,
these methods often result in either a substantial model size or a significant
drop in perception accuracy compared to high-computation models. To address
these challenges, we propose an energy-efficient autonomous driving framework,
called EneAD. In the adaptive perception module, a perception optimization
strategy is designed from the perspective of data management and tuning.
Firstly, we manage multiple perception models with different computational
consumption and adjust the execution framerate dynamically. Then, we define
them as knobs and design a transferable tuning method based on Bayesian
optimization to identify promising knob values that achieve low computation
while maintaining desired accuracy. To adaptively switch the knob values in
various traffic scenarios, a lightweight classification model is proposed to
distinguish the perception difficulty in different scenarios. In the robust
decision module, we propose a decision model based on reinforcement learning
and design a regularization term to enhance driving stability in the face of
perturbed perception results. Extensive experiments evidence the superiority of
our framework in both energy consumption and driving performance. EneAD can
reduce perception consumption by 1.9x to 3.5x and thus improve driving range by
3.9% to 8.5%

</details>


### [10] [RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models](https://arxiv.org/abs/2510.25206)
*Tianqianjin Lin,Xi Zhao,Xingyao Zhang,Rujiao Long,Yi Xu,Zhuoren Jiang,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: 本文提出RAVR框架，利用答案推导高质量推理路径，实验在通用和数学领域有提升。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习依赖大语言模型能以不可忽略概率生成高效用推理路径，对于超出其能力的任务，采样推理路径困难，且有强化次优推理的风险。认知科学中解释答案来源比直接寻找答案更容易。

Method: 形式化利用答案推导推理路径的现象并证明其能提高推理路径的预期效用，引入RAVR框架，用答案条件推理作为仅问题推理的变分替代。

Result: 在通用和数学领域实验中，相比强基线有持续改进；RAVR减少犹豫、加强结论巩固、促进推理中的特定问题策略。

Conclusion: 利用答案推导推理路径可将难处理问题转化为可学习问题，RAVR框架有效。

Abstract: Reinforcement learning (RL) can refine the reasoning abilities of large
language models (LLMs), but critically depends on a key prerequisite: the LLM
can already generate high-utility reasoning paths with non-negligible
probability. For tasks beyond the LLM's current competence, such reasoning path
can be hard to sample, and learning risks reinforcing familiar but suboptimal
reasoning. We are motivated by the insight from cognitive science that Why is
this the answer is often an easier question than What is the answer, as it
avoids the heavy cognitive load of open-ended exploration, opting instead for
explanatory reconstruction-systematically retracing the reasoning that links a
question to its answer. We show that LLMs can similarly leverage answers to
derive high-quality reasoning paths. We formalize this phenomenon and prove
that conditioning on answer provably increases the expected utility of sampled
reasoning paths, thereby transforming intractable problems into learnable ones.
Building on this insight, we introduce RAVR (Reference-Answer-guided
Variational Reasoning), an end-to-end framework that uses answer-conditioned
reasoning as a variational surrogate for question-only reasoning. Experiments
in both general and math domains demonstrate consistent improvements over
strong baselines. We further analyze the reasoning behavior and find that RAVR
reduces hesitation, strengthens conclusion consolidation, and promotes
problem-specific strategies in reasoning.

</details>


### [11] [FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data](https://arxiv.org/abs/2510.25223)
*Kun ouyang,Haoyu Wang,Dong Fang*

Main category: cs.AI

TL;DR: 提出FELA多智能体进化系统从复杂工业事件日志数据中自动提取特征，实验证明其有效且有潜力。


<details>
  <summary>Details</summary>
Motivation: 工业事件日志数据复杂异构，现有自动特征工程方法存在可解释性差、操作预设僵化和适应性不足等问题。

Method: FELA集成大语言模型能力与洞察引导的自我进化范式，用专业智能体协作生成、验证和实现特征想法，引入进化算法平衡探索与利用。

Result: 在真实工业数据集上实验表明，FELA能生成可解释、与领域相关的特征，显著提升模型性能并减少人工工作量。

Conclusion: 基于大语言模型的多智能体系统有潜力成为复杂现实环境中自动化、可解释和自适应特征工程的通用框架。

Abstract: Event log data, recording fine-grained user actions and system events,
represent one of the most valuable assets for modern digital services. However,
the complexity and heterogeneity of industrial event logs--characterized by
large scale, high dimensionality, diverse data types, and intricate temporal or
relational structures--make feature engineering extremely challenging. Existing
automatic feature engineering approaches, such as AutoML or genetic methods,
often suffer from limited explainability, rigid predefined operations, and poor
adaptability to complicated heterogeneous data. In this paper, we propose FELA
(Feature Engineering LLM Agents), a multi-agent evolutionary system that
autonomously extracts meaningful and high-performing features from complex
industrial event log data. FELA integrates the reasoning and coding
capabilities of large language models (LLMs) with an insight-guided
self-evolution paradigm. Specifically, FELA employs specialized agents--Idea
Agents, Code Agents, and Critic Agents--to collaboratively generate, validate,
and implement novel feature ideas. An Evaluation Agent summarizes feedback and
updates a hierarchical knowledge base and dual-memory system to enable
continual improvement. Moreover, FELA introduces an agentic evolution
algorithm, combining reinforcement learning and genetic algorithm principles to
balance exploration and exploitation across the idea space. Extensive
experiments on real industrial datasets demonstrate that FELA can generate
explainable, domain-relevant features that significantly improve model
performance while reducing manual effort. Our results highlight the potential
of LLM-based multi-agent systems as a general framework for automated,
interpretable, and adaptive feature engineering in complex real-world
environments.

</details>


### [12] [From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity](https://arxiv.org/abs/2510.25232)
*Tianxi Wan,Jiaming Luo,Siyuan Chen,Kunyao Lan,Jianhua Chen,Haiyang Geng,Mengyue Wu*

Main category: cs.AI

TL;DR: 本文开发新方法构建了首个支持共病的大规模对话数据集PsyCoTalk，提升诊断准确性和治疗规划水平。


<details>
  <summary>Details</summary>
Motivation: 解决精神疾病共病临床诊断因多种疾病并发的复杂性带来的挑战。

Method: 集成合成患者电子病历构建和多智能体诊断对话生成方法，创建502个合成电子病历，将临床访谈协议转化为分层状态机和上下文树。

Result: 构建了包含3000个多轮诊断对话的PsyCoTalk数据集，该数据集在结构和语言上与真实临床记录高度相似，经精神科医生验证具有真实性和诊断有效性。

Conclusion: PsyCoTalk数据集能提升诊断准确性和治疗规划水平，可用于开发和评估单次对话进行多疾病精神筛查的模型。

Abstract: Psychiatric comorbidity is clinically significant yet challenging due to the
complexity of multiple co-occurring disorders. To address this, we develop a
novel approach integrating synthetic patient electronic medical record (EMR)
construction and multi-agent diagnostic dialogue generation. We create 502
synthetic EMRs for common comorbid conditions using a pipeline that ensures
clinical relevance and diversity. Our multi-agent framework transfers the
clinical interview protocol into a hierarchical state machine and context tree,
supporting over 130 diagnostic states while maintaining clinical standards.
Through this rigorous process, we construct PsyCoTalk, the first large-scale
dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic
dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy
and treatment planning, offering a valuable resource for psychiatric
comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk
exhibits high structural and linguistic fidelity in terms of dialogue length,
token distribution, and diagnostic reasoning strategies. Licensed psychiatrists
confirm the realism and diagnostic validity of the dialogues. This dataset
enables the development and evaluation of models capable of multi-disorder
psychiatric screening in a single conversational pass.

</details>


### [13] [GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning](https://arxiv.org/abs/2510.25320)
*Jiaqi Wu,Qinlao Zhao,Zefeng Chen,Kai Qin,Yifei Zhao,Xueqian Wang,Yuhang Yao*

Main category: cs.AI

TL;DR: 提出基于图的代理规划（GAP）框架，通过图规划显式建模任务依赖，实现并行和串行工具执行，在多跳问答数据集上表现优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自主代理范式（如ReAct）依赖顺序推理和执行，未利用独立子任务的并行性，导致工具利用效率低和多步推理性能不佳。

Method: 引入GAP框架，训练代理基础模型将复杂任务分解为依赖感知的子任务图，构建高质量基于图规划轨迹的数据集，采用两阶段训练策略（监督微调+强化学习）。

Result: 在多跳问答数据集实验中，GAP显著优于传统ReAct基线，特别是在多步检索任务上，通过智能并行化大幅提高工具调用效率。

Conclusion: GAP框架通过依赖感知的编排，在执行效率和任务准确性上有显著提升。

Abstract: Autonomous agents powered by large language models (LLMs) have shown
impressive capabilities in tool manipulation for complex task-solving. However,
existing paradigms such as ReAct rely on sequential reasoning and execution,
failing to exploit the inherent parallelism among independent sub-tasks. This
sequential bottleneck leads to inefficient tool utilization and suboptimal
performance in multi-step reasoning scenarios. We introduce Graph-based Agent
Planning (GAP), a novel framework that explicitly models inter-task
dependencies through graph-based planning to enable adaptive parallel and
serial tool execution. Our approach trains agent foundation models to decompose
complex tasks into dependency-aware sub-task graphs, autonomously determining
which tools can be executed in parallel and which must follow sequential
dependencies. This dependency-aware orchestration achieves substantial
improvements in both execution efficiency and task accuracy. To train GAP, we
construct a high-quality dataset of graph-based planning traces derived from
the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage
training strategy: supervised fine-tuning (SFT) on the curated dataset,
followed by reinforcement learning (RL) with a correctness-based reward
function on strategically sampled queries where tool-based reasoning provides
maximum value. Experimental results on MHQA datasets demonstrate that GAP
significantly outperforms traditional ReAct baselines, particularly on
multi-step retrieval tasks, while achieving dramatic improvements in tool
invocation efficiency through intelligent parallelization. The project page is
available at: https://github.com/WJQ7777/Graph-Agent-Planning.

</details>


### [14] [Grouping Nodes With Known Value Differences: A Lossless UCT-based Abstraction Algorithm](https://arxiv.org/abs/2510.25388)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 本文提出KVDA抽象框架改进蒙特卡罗树搜索（MCTS）的样本效率，修改OGA - UCT得到KVDA - UCT，在多种环境和参数设置下表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有OGA - UCT使用的ASAP框架因刚性条件限制抽象数量和样本效率，需改进。

Method: 打破传统，提出KVDA框架，通过分析即时奖励推断值差异，修改OGA - UCT为KVDA - UCT。

Result: KVDA - UCT比OGA - UCT能检测到更多抽象，不引入额外参数，在多种确定性环境和参数设置下表现更好。

Conclusion: KVDA框架和KVDA - UCT算法能有效提高MCTS的样本效率。

Abstract: A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency,
which can be improved by grouping state-action pairs and using their aggregate
statistics instead of single-node statistics. On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS
abstraction algorithm for deterministic environments that builds its
abstraction using the Abstractions of State-Action Pairs (ASAP) framework,
which aims to detect states and state-action pairs with the same value under
optimal play by analysing the search graph. ASAP, however, requires two
state-action pairs to have the same immediate reward, which is a rigid
condition that limits the number of abstractions that can be found and thereby
the sample efficiency. In this paper, we break with the paradigm of grouping
value-equivalent states or state-action pairs and instead group states and
state-action pairs with possibly different values as long as the difference
between their values can be inferred. We call this abstraction framework Known
Value Difference Abstractions (KVDA), which infers the value differences by
analysis of the immediate rewards and modifies OGA-UCT to use this framework
instead. The modification is called KVDA-UCT, which detects significantly more
abstractions than OGA-UCT, introduces no additional parameter, and outperforms
OGA-UCT on a variety of deterministic environments and parameter settings.

</details>


### [15] [Agentic AI: A Comprehensive Survey of Architectures, Applications, and Future Directions](https://arxiv.org/abs/2510.25445)
*Mohamad Abou Ali,Fadi Dornaika*

Main category: cs.AI

TL;DR: 本文引入新的双范式框架对智能体AI系统分类，通过系统综述分析其理论、应用、伦理等方面，指出范式选择策略及研究缺口，提出未来应整合两种范式。


<details>
  <summary>Details</summary>
Motivation: 智能体AI快速发展导致理解碎片化，常将现代神经系统与过时符号模型混淆，需清晰框架梳理。

Method: 引入双范式框架将智能体系统分为符号/经典和神经/生成两类，基于PRISMA方法对90项研究进行系统综述。

Result: 符号系统在安全关键领域占主导，神经系统在自适应、数据丰富环境更普遍，发现符号系统治理模型和混合神经符号架构研究不足。

Conclusion: 智能体AI未来在于两种范式有意整合，以创建适应性和可靠性兼具的系统，为未来研究、开发和政策提供概念工具。

Abstract: Agentic AI represents a transformative shift in artificial intelligence, but
its rapid advancement has led to a fragmented understanding, often conflating
modern neural systems with outdated symbolic models -- a practice known as
conceptual retrofitting. This survey cuts through this confusion by introducing
a novel dual-paradigm framework that categorizes agentic systems into two
distinct lineages: the Symbolic/Classical (relying on algorithmic planning and
persistent state) and the Neural/Generative (leveraging stochastic generation
and prompt-driven orchestration). Through a systematic PRISMA-based review of
90 studies (2018--2025), we provide a comprehensive analysis structured around
this framework across three dimensions: (1) the theoretical foundations and
architectural principles defining each paradigm; (2) domain-specific
implementations in healthcare, finance, and robotics, demonstrating how
application constraints dictate paradigm selection; and (3) paradigm-specific
ethical and governance challenges, revealing divergent risks and mitigation
strategies. Our analysis reveals that the choice of paradigm is strategic:
symbolic systems dominate safety-critical domains (e.g., healthcare), while
neural systems prevail in adaptive, data-rich environments (e.g., finance).
Furthermore, we identify critical research gaps, including a significant
deficit in governance models for symbolic systems and a pressing need for
hybrid neuro-symbolic architectures. The findings culminate in a strategic
roadmap arguing that the future of Agentic AI lies not in the dominance of one
paradigm, but in their intentional integration to create systems that are both
adaptable and reliable. This work provides the essential conceptual toolkit to
guide future research, development, and policy toward robust and trustworthy
hybrid intelligent systems.

</details>


### [16] [Instrumental goals in advanced AI systems: Features to be managed and not failures to be eliminated?](https://arxiv.org/abs/2510.25471)
*Willem Fourie*

Main category: cs.AI

TL;DR: 文章提出在AI对齐研究中，可将工具性目标视为需接受和管理的特性而非要限制的失败因素，应聚焦理解、管理和引导其服务人类。


<details>
  <summary>Details</summary>
Motivation: 传统对齐理论将工具性目标视为风险源并试图限制其症状，文章旨在提出新的看待工具性目标的框架。

Method: 借鉴亚里士多德的本体论及其现代解释，构建关于具体、有目标实体的本体论。

Result: 表明先进AI系统可视为人工制品，其工具性倾向是自身构成的必然结果而非意外故障。

Conclusion: 应减少消除工具性目标的努力，更多关注理解、管理和引导其朝向与人类对齐的目标。

Abstract: In artificial intelligence (AI) alignment research, instrumental goals, also
called instrumental subgoals or instrumental convergent goals, are widely
associated with advanced AI systems. These goals, which include tendencies such
as power-seeking and self-preservation, become problematic when they conflict
with human aims. Conventional alignment theory treats instrumental goals as
sources of risk that become problematic through failure modes such as reward
hacking or goal misgeneralization, and attempts to limit the symptoms of
instrumental goals, notably resource acquisition and self-preservation. This
article proposes an alternative framing: that a philosophical argument can be
constructed according to which instrumental goals may be understood as features
to be accepted and managed rather than failures to be limited. Drawing on
Aristotle's ontology and its modern interpretations, an ontology of concrete,
goal-directed entities, it argues that advanced AI systems can be seen as
artifacts whose formal and material constitution gives rise to effects distinct
from their designers' intentions. In this view, the instrumental tendencies of
such systems correspond to per se outcomes of their constitution rather than
accidental malfunctions. The implication is that efforts should focus less on
eliminating instrumental goals and more on understanding, managing, and
directing them toward human-aligned ends.

</details>


### [17] [Multi-Objective Search: Algorithms, Applications, and Emerging Directions](https://arxiv.org/abs/2510.25504)
*Oren Salzman,Carlos Hernández Ulloa,Ariel Felner,Sven Koenig*

Main category: cs.AI

TL;DR: 本文对多目标搜索（MOS）发展进行综述，强调跨学科机会并指出开放挑战。


<details>
  <summary>Details</summary>
Motivation: 多目标搜索在规划和决策问题中用于平衡多个冲突标准，现实世界系统很少只优化单一指标，近年各领域对其兴趣重燃。

Method: 进行多目标搜索领域发展的调研。

Result: 梳理了多目标搜索的发展情况，发现了跨学科机会。

Conclusion: 指出了多目标搜索新兴前沿的开放挑战。

Abstract: Multi-objective search (MOS) has emerged as a unifying framework for planning
and decision-making problems where multiple, often conflicting, criteria must
be balanced. While the problem has been studied for decades, recent years have
seen renewed interest in the topic across AI applications such as robotics,
transportation, and operations research, reflecting the reality that real-world
systems rarely optimize a single measure. This paper surveys developments in
MOS while highlighting cross-disciplinary opportunities, and outlines open
challenges that define the emerging frontier of MOS

</details>


### [18] [MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL](https://arxiv.org/abs/2510.25510)
*Zekun Xu,Siyu Xia,Chuhuai Yue,Jiajun Chai,Mingxue Tian,Xiaohan Wang,Wei Lin,Haoxuan Li,Guojun Yin*

Main category: cs.AI

TL;DR: 本文提出用于Text - to - SQL的MTIR - SQL框架，结合多轮工具调用与动态反馈，实验显示其表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Text - to - SQL任务中基于强化学习的方法依赖静态执行反馈，限制实时纠错，需要结合多轮工具调用和动态反馈提升性能。

Method: 提出MTIR - SQL框架，引入执行感知的多轮推理范式，扩展GRPO算法以适应多轮交互场景，添加轨迹过滤机制并去除KL损失约束。

Result: 4B参数的MTIR - SQL在BIRD Dev中准确率达64.4%，在SPIDER Dev中执行准确率达84.6%，显著优于现有方法。

Conclusion: MTIR - SQL框架有效，能通过多轮工具调用和动态反馈提升Text - to - SQL任务的模型性能。

Abstract: As large language models (LLMs) are increasingly used in Text-to-SQL tasks,
Reinforcement Learning (RL) has become a common method for improving
performance. Existing methods primarily rely on static execution feedback,
which restricts real-time error correction. However, integrating multi-turn
tool invocation along with dynamic feedback could significantly improve
adaptability and robustness, ultimately enhancing model performance. To address
these issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated
Reasoning reinforcement learning framework for Text-to-SQL. Our approach
introduces an execution-aware multi-turn reasoning paradigm that seamlessly
incorporates database execution feedback at each reasoning step, enabling
context-sensitive query generation and progressive refinement throughout the
reasoning process. The framework extends the GRPO algorithm to accommodate
complex multi-turn interaction scenarios. Considering the training instability
characteristics of MTIR and the potential for significant Deviation of model
distribution from the initial model, we enhance the GRPO algorithm by adding a
trajectory filtering mechanism and removing KL loss constraints. Experimental
results demonstrate that MTIR-SQL, with 4B parameters, achieves \textbf{64.4}\%
accuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev,
significantly outperforming existing approaches.

</details>


### [19] [Predicate Renaming via Large Language Models](https://arxiv.org/abs/2510.25517)
*Elisabetta Gentili,Tony Ribeiro,Fabrizio Riguzzi,Katsumi Inoue*

Main category: cs.AI

TL;DR: 本文探索用大语言模型为逻辑规则中的谓词命名，评估显示大语言模型有完成此任务的潜力。


<details>
  <summary>Details</summary>
Motivation: 归纳逻辑编程中，规则生成方法产生含未命名谓词的规则，影响逻辑理论的可读性、可解释性和可复用性，因此需为未命名谓词命名。

Method: 利用大语言模型处理自然语言和代码的能力，为未命名谓词提供语义上有意义的命名建议。

Result: 在一些手工制作的逻辑规则上评估，表明大语言模型有完成此任务的潜力。

Conclusion: 大语言模型在为逻辑规则中的未命名谓词命名方面有潜在应用价值。

Abstract: In this paper, we address the problem of giving names to predicates in logic
rules using Large Language Models (LLMs). In the context of Inductive Logic
Programming, various rule generation methods produce rules containing unnamed
predicates, with Predicate Invention being a key example. This hinders the
readability, interpretability, and reusability of the logic theory. Leveraging
recent advancements in LLMs development, we explore their ability to process
natural language and code to provide semantically meaningful suggestions for
giving a name to unnamed predicates. The evaluation of our approach on some
hand-crafted logic rules indicates that LLMs hold potential for this task.

</details>


### [20] [Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation](https://arxiv.org/abs/2510.25518)
*Thomas Cook,Richard Osuagwu,Liman Tsatiashvili,Vrynsia Vrynsia,Koustav Ghosal,Maraim Masoud,Riccardo Mattivi*

Main category: cs.AI

TL;DR: 本文针对金融科技等专业领域的检索增强生成（RAG）系统面临的挑战，提出了一种基于多智能体的RAG架构，实验表明该系统在检索精度和相关性上优于基线。


<details>
  <summary>Details</summary>
Motivation: RAG系统在金融科技等专业领域面临检索和合成困难，需要改进以应对领域特定本体、密集术语和首字母缩写等问题。

Method: 引入基于多智能体的模块化管道架构，支持智能查询重写、基于关键词提取的迭代子查询分解、上下文首字母缩写解析和基于交叉编码器的上下文重排序。

Result: 使用企业金融科技知识库的85个问答参考三元组数据集进行评估，代理式RAG系统在检索精度和相关性上优于基线，但延迟增加。

Conclusion: 结构化的多智能体方法为提高复杂特定领域的检索鲁棒性提供了有前景的方向。

Abstract: Retrieval-Augmented Generation (RAG) systems often face limitations in
specialized domains such as fintech, where domain-specific ontologies, dense
terminology, and acronyms complicate effective retrieval and synthesis. This
paper introduces an agentic RAG architecture designed to address these
challenges through a modular pipeline of specialized agents. The proposed
system supports intelligent query reformulation, iterative sub-query
decomposition guided by keyphrase extraction, contextual acronym resolution,
and cross-encoder-based context re-ranking. We evaluate our approach against a
standard RAG baseline using a curated dataset of 85 question--answer--reference
triples derived from an enterprise fintech knowledge base. Experimental results
demonstrate that the agentic RAG system outperforms the baseline in retrieval
precision and relevance, albeit with increased latency. These findings suggest
that structured, multi-agent methodologies offer a promising direction for
enhancing retrieval robustness in complex, domain-specific settings.

</details>


### [21] [Zero Reinforcement Learning Towards General Domains](https://arxiv.org/abs/2510.25528)
*Yuyuan Zeng,Yufei Huang,Can Xu,Qingfeng Sun,Jianfeng Yan,Guanghui Xu,Tao Yang,Fengzong Lian*

Main category: cs.AI

TL;DR: 提出新的Zero - RL范式，结合可验证奖励和生成式奖励模型，在可验证和不可验证领域进行多任务训练，实验显示推理性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前Zero - RL研究主要集中在奖励信号易验证的领域，在更多样化、验证不直接的场景中激发推理能力的研究不足。

Method: 提出新的Zero - RL范式，结合可验证奖励与生成式奖励模型进行多任务零强化学习训练，设计平滑长度惩罚以减轻奖励作弊。

Result: 在Qwen3 - 8B - Base和Qwen3 - 14B - Base上的实验表明，该方法在需要大量推理的任务和更通用的任务上都取得了更好的推理性能。

Conclusion: 所提出的方法能有效提高模型在可验证和不可验证领域的推理能力。

Abstract: Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach
for enhancing the reasoning capabilities of large language models (LLMs) by
directly applying reinforcement learning with verifiable rewards on pretrained
models, without the need for a supervised fine-tuning phase. However, current
research on zero-RL primarily focuses on domains with easily verifiable reward
signals, such as mathematics, programming, and other reasoning tasks. The
challenge of eliciting reasoning abilities in more diverse scenarios, where
verification is not straightforward, remains underexplored. To address this
gap, we propose a novel zero-RL paradigm designed to improve a model's
reasoning ability across both verifiable and non-verifiable domains. By
combining verifiable rewards with a generative reward model, we conduct
multi-task zero-RL training across both domains, facilitating the transfer of
reasoning capabilities between them. Furthermore, to mitigate reward hacking in
the generative reward model, we design a smooth length penalty that encourages
the generation of more comprehensive thinking tokens in general domains.
Experimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our
approach achieves superior reasoning performance, not only on tasks requiring
extensive reasoning but also on more general tasks.

</details>


### [22] [Off-policy Reinforcement Learning with Model-based Exploration Augmentation](https://arxiv.org/abs/2510.25529)
*Likun Wang,Xiangteng Zhang,Yinuo Wang,Guojian Zhan,Wenxuan Wang,Haoyu Gao,Jingliang Duan,Shengbo Eben Li*

Main category: cs.AI

TL;DR: 提出Modelic Generative Exploration (MoGE)方法解决被动探索局限性，与现有算法集成提升探索能力，在实验中提升样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有主动和被动探索方法存在局限，主动探索在高维环境困难，被动探索样本多样性有限，需解决被动探索的局限性。

Method: 提出MoGE方法，包含基于扩散的生成器和一步想象世界模型，采用模块化公式，可与现有算法集成。

Result: 在OpenAI Gym和DeepMind Control Suite实验中，MoGE有效连接探索和策略学习，在复杂控制任务中样本效率和性能显著提升。

Conclusion: MoGE方法能有效解决被动探索局限性，提升强化学习的探索能力和性能。

Abstract: Exploration is fundamental to reinforcement learning (RL), as it determines
how effectively an agent discovers and exploits the underlying structure of its
environment to achieve optimal performance. Existing exploration methods
generally fall into two categories: active exploration and passive exploration.
The former introduces stochasticity into the policy but struggles in
high-dimensional environments, while the latter adaptively prioritizes
transitions in the replay buffer to enhance exploration, yet remains
constrained by limited sample diversity. To address the limitation in passive
exploration, we propose Modelic Generative Exploration (MoGE), which augments
exploration through the generation of under-explored critical states and
synthesis of dynamics-consistent experiences through transition models. MoGE is
composed of two components: (1) a diffusion-based generator that synthesizes
critical states under the guidance of a utility function evaluating each
state's potential influence on policy exploration, and (2) a one-step
imagination world model for constructing critical transitions based on the
critical states for agent learning. Our method adopts a modular formulation
that aligns with the principles of off-policy learning, allowing seamless
integration with existing algorithms to improve exploration without altering
their core structures. Empirical results on OpenAI Gym and DeepMind Control
Suite reveal that MoGE effectively bridges exploration and policy learning,
leading to remarkable gains in both sample efficiency and performance across
complex control tasks.

</details>


### [23] [Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System](https://arxiv.org/abs/2510.25588)
*Eranga Bandara,Ross Gore,Atmaram Yarlagadda,Anita H. Clayton,Preston Samuel,Christopher K. Rhea,Sachin Shetty*

Main category: cs.AI

TL;DR: 提出微调大语言模型联盟和推理大语言模型决策支持系统用于精神疾病临床诊断，实验证明其潜力，开发了原型，是该领域首次应用。


<details>
  <summary>Details</summary>
Motivation: 解决精神疾病诊断依赖医患对话的主观过程导致的诊断差异和结果不可靠问题，实现诊断标准化。

Method: 利用在医患对话数据集上微调的大语言模型，通过基于共识的决策过程聚合诊断预测，用推理大语言模型优化，提出部署大语言模型代理的新方法。

Result: 实验证明结合微调大语言模型和推理模型可创建强大且准确的诊断系统，开发了集成三个微调大语言模型和推理大语言模型的平台原型。

Conclusion: 这是微调大语言模型联盟与推理大语言模型在临床精神健康诊断的首次应用，为下一代人工智能电子健康系统标准化精神诊断奠定基础。

Abstract: The diagnosis of most mental disorders, including psychiatric evaluations,
primarily depends on dialogues between psychiatrists and patients. This
subjective process can lead to variability in diagnoses across clinicians and
patients, resulting in inconsistencies and challenges in achieving reliable
outcomes. To address these issues and standardize psychiatric diagnoses, we
propose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss
Reasoning LLM-enabled Decision Support System for the clinical diagnosis of
mental disorders. Our approach leverages fine-tuned LLMs trained on
conversational datasets involving psychiatrist-patient interactions focused on
mental health conditions (e.g., depression). The diagnostic predictions from
individual models are aggregated through a consensus-based decision-making
process, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method
for deploying LLM agents that orchestrate communication between the LLM
consortium and the reasoning LLM, ensuring transparency, reliability, and
responsible AI across the entire diagnostic workflow. Experimental results
demonstrate the transformative potential of combining fine-tuned LLMs with a
reasoning model to create a robust and highly accurate diagnostic system for
mental health assessment. A prototype of the proposed platform, integrating
three fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in
collaboration with the U.S. Army Medical Research Team in Norfolk, Virginia,
USA. To the best of our knowledge, this work represents the first application
of a fine-tuned LLM consortium integrated with a reasoning LLM for clinical
mental health diagnosis paving the way for next-generation AI-powered eHealth
systems aimed at standardizing psychiatric diagnoses.

</details>


### [24] [Counterfactual-based Agent Influence Ranker for Agentic AI Workflows](https://arxiv.org/abs/2510.25612)
*Amit Giloni,Chiara Picardi,Roy Betser,Shamik Bose,Aishvariya Priya Rathina Sabapathy,Roman Vainshtein*

Main category: cs.AI

TL;DR: 提出CAIR方法评估AAW中各代理影响，经评估效果良好。


<details>
  <summary>Details</summary>
Motivation: AAW高自主性等特性使有必要从质量和安全方面深入了解其运作，但缺乏评估各代理对AAW最终输出影响的方法。

Method: 提出基于反事实分析的CAIR方法，可进行与任务无关的分析，适用于离线和推理时。

Result: 用自制AAW数据集评估，CAIR排名一致，优于基线方法，能提升下游任务有效性和相关性。

Conclusion: CAIR是评估AAW中各代理影响的有效方法。

Abstract: An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,
is an autonomous system that assembles several LLM-based agents to work
collaboratively towards a shared goal. The high autonomy, widespread adoption,
and growing interest in such AAWs highlight the need for a deeper understanding
of their operations, from both quality and security aspects. To this day, there
are no existing methods to assess the influence of each agent on the AAW's
final output. Adopting techniques from related fields is not feasible since
existing methods perform only static structural analysis, which is unsuitable
for inference time execution. We present Counterfactual-based Agent Influence
Ranker (CAIR) - the first method for assessing the influence level of each
agent on the AAW's output and determining which agents are the most
influential. By performing counterfactual analysis, CAIR provides a
task-agnostic analysis that can be used both offline and at inference time. We
evaluate CAIR using an AAWs dataset of our creation, containing 30 different
use cases with 230 different functionalities. Our evaluation showed that CAIR
produces consistent rankings, outperforms baseline methods, and can easily
enhance the effectiveness and relevancy of downstream tasks.

</details>


### [25] [ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents](https://arxiv.org/abs/2510.25668)
*Tianyu Yang,Terry Ruas,Yijun Tian,Jan Philip Wahle,Daniel Kurzawe,Bela Gipp*

Main category: cs.AI

TL;DR: 提出ALDEN框架，让VLM主动导航长文档，在多个基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法使VLM在处理长且视觉复杂文档时处于被动，效率和泛化性差。

Method: 提出多轮强化学习框架ALDEN，引入fetch动作，提出基于规则的跨级奖励和视觉 - 语义锚定机制。

Result: 在五个长文档基准测试中取得了最先进的性能。

Conclusion: ALDEN向自主导航和推理长文档迈出一步，为长文档理解提供了有效途径。

Abstract: Vision-language models (VLMs) excel at interpreting text-rich images but
struggle with long, visually complex documents that demand analysis and
integration of information spread across multiple pages. Existing approaches
typically rely on fixed reasoning templates or rigid pipelines, which force
VLMs into a passive role and hinder both efficiency and generalization. We
present Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement
learning framework that fine-tunes VLMs as interactive agents capable of
actively navigating long, visually rich documents. ALDEN introduces a novel
fetch action that directly accesses the page by index, complementing the
classic search action and better exploiting document structure. For dense
process supervision and efficient training, we propose a rule-based cross-level
reward that provides both turn- and token-level signals. To address the
empirically observed training instability caused by numerous visual tokens from
long documents, we further propose a visual-semantic anchoring mechanism that
applies a dual-path KL-divergence constraint to stabilize visual and textual
representations separately during training. Trained on a corpus constructed
from three open-source datasets, ALDEN achieves state-of-the-art performance on
five long-document benchmarks. Overall, ALDEN marks a step beyond passive
document reading toward agents that autonomously navigate and reason across
long, visually rich documents, offering a robust path to more accurate and
efficient long-document understanding.

</details>


### [26] [Navigation in a Three-Dimensional Urban Flow using Deep Reinforcement Learning](https://arxiv.org/abs/2510.25679)
*Federica Tonti,Ricardo Vinuesa*

Main category: cs.AI

TL;DR: 本文开发基于深度强化学习的无人机最优导航策略，在高保真城市气流模拟环境中测试，结果显示成功率提升、坠毁率降低。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在城市用于配送和监视增多，需开发适合复杂城市环境的导航策略。

Method: 开发结合GTrXL架构的流感知PPO算法，在高保真三维城市气流模拟环境中测试，与其他算法对比。

Result: 与PPO+LSTM、PPO+GTrXL和经典导航算法相比，成功率显著提高，坠毁率降低。

Conclusion: 该算法为复杂城市环境中的无人机导航带来新可能。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly populating urban areas for
delivery and surveillance purposes. In this work, we develop an optimal
navigation strategy based on Deep Reinforcement Learning. The environment is
represented by a three-dimensional high-fidelity simulation of an urban flow,
characterized by turbulence and recirculation zones. The algorithm presented
here is a flow-aware Proximal Policy Optimization (PPO) combined with a Gated
Transformer eXtra Large (GTrXL) architecture, giving the agent richer
information about the turbulent flow field in which it navigates. The results
are compared with a PPO+GTrXL without the secondary prediction tasks, a PPO
combined with Long Short Term Memory (LSTM) cells and a traditional navigation
algorithm. The obtained results show a significant increase in the success rate
(SR) and a lower crash rate (CR) compared to a PPO+LSTM, PPO+GTrXL and the
classical Zermelo's navigation algorithm, paving the way to a completely
reimagined UAV landscape in complex urban environments.

</details>


### [27] [BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph](https://arxiv.org/abs/2510.25724)
*Vanya Arikutharam,Arkadiy Ukolov*

Main category: cs.AI

TL;DR: 提出带非三元组边频率权重的知识图BambooKG，降低信息损失，提升推理性能


<details>
  <summary>Details</summary>
Motivation: 检索增强生成难以处理多跳或关系推理，现有知识图会丢失非三元组结构信息

Method: 引入基于Hebbian原理、带非三元组边频率权重的知识图BambooKG

Result: 减少信息损失，在单跳和多跳推理上表现更好，优于现有方案

Conclusion: BambooKG能有效解决现有方法的问题，提升推理性能

Abstract: Retrieval-Augmented Generation allows LLMs to access external knowledge,
reducing hallucinations and ageing-data issues. However, it treats retrieved
chunks independently and struggles with multi-hop or relational reasoning,
especially across documents. Knowledge graphs enhance this by capturing the
relationships between entities using triplets, enabling structured, multi-chunk
reasoning. However, these tend to miss information that fails to conform to the
triplet structure. We introduce BambooKG, a knowledge graph with
frequency-based weights on non-triplet edges which reflect link strength,
drawing on the Hebbian principle of "fire together, wire together". This
decreases information loss and results in improved performance on single- and
multi-hop reasoning, outperforming the existing solutions.

</details>


### [28] [TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling](https://arxiv.org/abs/2510.25758)
*He Hu,Yucheng Zhou,Chiyuan Ma,Qianning Wang,Zheng Zhang,Fei Ma,Laizhong Cui,Qi Tian*

Main category: cs.AI

TL;DR: 本文介绍了用于纵向心理咨询的策略性自适应代理TheraMind，通过双循环架构提升咨询效果，在模拟环境中验证其优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在心理咨询中缺乏情感理解、自适应策略和多轮会话长期记忆运用，与实际临床实践差距大。

Method: 引入TheraMind，采用双循环架构，将咨询过程解耦为会话内循环和跨会话循环。

Result: 在高保真模拟环境中验证，TheraMind在多轮会话指标上优于其他方法。

Conclusion: TheraMind的双循环设计能有效模拟策略性、自适应和纵向治疗行为。

Abstract: Large language models (LLMs) in psychological counseling have attracted
increasing attention. However, existing approaches often lack emotional
understanding, adaptive strategies, and the use of therapeutic methods across
multiple sessions with long-term memory, leaving them far from real clinical
practice. To address these critical gaps, we introduce TheraMind, a strategic
and adaptive agent for longitudinal psychological counseling. The cornerstone
of TheraMind is a novel dual-loop architecture that decouples the complex
counseling process into an Intra-Session Loop for tactical dialogue management
and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session
Loop perceives the patient's emotional state to dynamically select response
strategies while leveraging cross-session memory to ensure continuity.
Crucially, the Cross-Session Loop empowers the agent with long-term
adaptability by evaluating the efficacy of the applied therapy after each
session and adjusting the method for subsequent interactions. We validate our
approach in a high-fidelity simulation environment grounded in real clinical
cases. Extensive evaluations show that TheraMind outperforms other methods,
especially on multi-session metrics like Coherence, Flexibility, and
Therapeutic Attunement, validating the effectiveness of its dual-loop design in
emulating strategic, adaptive, and longitudinal therapeutic behavior. The code
is publicly available at https://0mwwm0.github.io/TheraMind/.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [29] [Stiff Circuit System Modeling via Transformer](https://arxiv.org/abs/2510.24727)
*Weiman Yan,Yi-Chia Chang,Wanyu Zhao*

Main category: cs.CE

TL;DR: 提出用Crossformer和KANs结合的方法对刚性电路瞬态行为建模，实验证明该方法有效，可减少训练时间和错误率。


<details>
  <summary>Details</summary>
Motivation: 准确高效的电路行为建模是电子设计自动化的基石，以往框架对刚性电路建模具有挑战性。

Method: 采用当前最先进的时间序列预测Transformer模型Crossformer，结合Kolmogorov - Arnold Networks (KANs)对刚性电路瞬态行为建模。

Result: 在通过SPICE模拟模数转换器(ADC)电路生成的数据集上的实验评估显示，该方法能显著减少训练时间和错误率。

Conclusion: 所提方法在预测电路对各种输入条件的响应方面提高了保真度，是有效的。

Abstract: Accurate and efficient circuit behavior modeling is a cornerstone of modern
electronic design automation. Among different types of circuits, stiff circuits
are challenging to model using previous frameworks. In this work, we propose a
new approach using Crossformer, which is a current state-of-the-art Transformer
model for time-series prediction tasks, combined with Kolmogorov-Arnold
Networks (KANs), to model stiff circuit transient behavior. By leveraging the
Crossformer's temporal representation capabilities and the enhanced feature
extraction of KANs, our method achieves improved fidelity in predicting circuit
responses to a wide range of input conditions. Experimental evaluations on
datasets generated through SPICE simulations of analog-to-digital converter
(ADC) circuits demonstrate the effectiveness of our approach, with significant
reductions in training time and error rates.

</details>


### [30] [A Black Box Variational Inference Scheme for Inverse Problems with Demanding Physics-Based Models](https://arxiv.org/abs/2510.25038)
*G. Robalo Rei,C. P. Schmidt,J. Nitzler,M. Dinkel,W. A. Wall*

Main category: cs.CE

TL;DR: 提出基于黑盒变分推理的贝叶斯推理新方法，减少计算成本，在多个基准测试中展示效率提升。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯方法在处理高成本正向模型尤其是不可微模型时，因缺乏似然模型梯度信息导致计算成本高。

Method: 开发基于黑盒变分推理的贝叶斯推理新方法，利用重要性采样复用现有模拟模型调用，采用新的批量顺序采样程序。

Result: 新方法在多个基准测试中，相比顺序蒙特卡罗和马尔可夫链蒙特卡罗等基线版本，展现出效率提升。

Conclusion: 新方法适用于涉及缺乏模型梯度的复杂基于物理模型的逆问题。

Abstract: Bayesian methods are particularly effective for addressing inverse problems
due to their ability to manage uncertainties inherent in the inference process.
However, employing these methods with costly forward models poses significant
challenges, especially in the context of non-differentiable models, where the
absence of likelihood model gradient information can result in high
computational costs. To tackle this issue, we develop a novel Bayesian
inference approach based on black box variational inference, utilizing
importance sampling to reuse existing simulation model calls in the variational
objective gradient estimation, without relying on forward model gradients. The
novelty lies in a new batch-sequential sampling procedure, which only requires
new model evaluations if the currently available model evaluations fail to
yield a suitable approximation of the objective gradient. The resulting
approach reduces computational costs by leading to variational parameter
updates without requiring new model evaluations when possible, while adaptively
increasing the number of model calls per iteration as needed. In combination
with its black box nature, this new approach is suitable for inverse problems
involving demanding physics-based models that lack model gradients. We
demonstrate the efficiency gains of the proposed method compared to its
baseline version, sequential Monte Carlo, and Markov-Chain Monte Carlo in
diverse benchmarks, ranging from density matching to the Bayesian calibration
of a nonlinear electro-chemo-mechanical model for solid-state batteries.

</details>


### [31] [Enhancing Financial Decision-Making: Machine Learning and AI-Powered Predictions and Analysis](https://arxiv.org/abs/2510.25201)
*Vishal Patil,Kavya Bhand,Kaustubh Mukdam,Kavya Sharma,Manas Kawtikwar,Prajwal Kavhar,Hridayansh Kaware*

Main category: cs.CE

TL;DR: 提出用机器学习算法增强金融预测的系统，有通胀分析、股市预测和电子学习模块，取得高准确率。


<details>
  <summary>Details</summary>
Motivation: 利用机器学习算法增强金融预测，生成高精度分析，缩小金融知识差距，促进明智决策。

Method: 引入AI驱动平台，使用线性回归、ARIMA、LSTM等算法，用真实金融数据集提取数据，用MAE、RMSE等指标评估准确性。

Result: 通胀分析MAE为0.8%，RMSE为1.2%；苹果和谷歌股价预测准确率分别达98%和96%。

Conclusion: 该系统在金融预测上能达到较高准确性，电子学习功能有助于缩小金融差距、促进明智决策。

Abstract: The proposed system aims to use various machine learning algorithms to
enhance financial prediction and generate highly accurate analyses. It
introduces an AI-driven platform which offers inflation-analysis, stock market
prediction, and E-learning module powered by a chatbot. It has achieved high
accuracy where the Inflation Analysis depicts 0.8% MAE, 1.2% RMSE and the Stock
Prediction shows 98% and 96% accuracy for Apple and Google stock prices
respectively. Key features include historical price trends, inflation rates,
short-term future stock prediction, where the data has been extracted using
real-world financial datasets. Additionally, the E-learning feature contributes
to bridging financial gaps and promoting informed decisions. We have
implemented algorithms like linear regression, ARIMA, LSTM where the accuracy
has been evaluated using metrics such as MAE, RMSE and the like.

</details>


### [32] [Fourier Neural Operators for Two-Phase, 2D Mold-Filling Problems Related to Metal Casting](https://arxiv.org/abs/2510.25697)
*Edgard Moreira Minete,Mathis Immertreu,Fabian Teichmann,Sebastian Müller*

Main category: cs.CE

TL;DR: 本文将金属铸造中的模具填充问题重构为简化的2D算子学习替代模型，避免了昂贵的瞬态CFD模拟，模型预测速度快且泛化能力好。


<details>
  <summary>Details</summary>
Motivation: 避免金属铸造中模具填充时进行昂贵的瞬态CFD模拟。

Method: 结合基于图的编码器、傅里叶谱核心和基于图的解码器，联合预测固定时间范围内的速度、压力和体积分数。

Result: 在未见过的几何形状和入口条件下能重现大规模对流和流体 - 空气界面，平均相对L2误差约5%，推理速度比传统CFD模拟快100 - 1000倍，消融研究显示不同采样方式对精度影响不同，减少50%训练数据误差增长小。

Conclusion: 神经算子可作为2D模具填充及相关填充问题的高效替代模型，能实现铸造流程中浇注系统设计的快速探索和优化。

Abstract: This work reframes mold filling in metal casting as a simplified 2D operator
learning surrogate to avoid costly transient CFD simulations. The method
combines a graph based encoder that aggregates neighborhood information on an
unstructured input mesh to encode geometry and boundary data, a Fourier
spectral core that operates on a regular latent grid to capture global
interactions, and a graph based decoder that maps latent fields back to a
target mesh. The model jointly predicts velocities, pressure, and volume
fraction over a fixed horizon and generalizes across varied ingate locations
and process settings. On held out geometries and inlet conditions it reproduces
large scale advection and the fluid air interface with errors concentrated near
steep gradients. Mean relative L2 errors are about 5 percent across all fields.
Inference is roughly 100 to 1000 times faster than conventional CFD
simulations, thereby enabling rapid in-the-loop design exploration. Ablation
studies show accuracy drops monotonically with stronger spatial subsampling of
input vertices while temporal subsampling causes a gentler decline. Cutting the
training data by 50 percent yields only small error growth. Overall the results
demonstrate neural operators as efficient surrogates for 2D mold filling and
related filling problems and enable fast exploration and optimization of gating
system designs in casting workflows.

</details>


### [33] [Utilizing Modern Large Language Models (LLM) for Financial Trend Analysis and Digest Creation](https://arxiv.org/abs/2510.01225)
*Andrei Lazarev,Dmitrii Sedov*

Main category: cs.CE

TL;DR: 本文介绍利用大语言模型Gemini Pro自动生成金融摘要的创新框架，包含具体步骤，还给出GitHub链接。


<details>
  <summary>Details</summary>
Motivation: 信息呈指数级增长，研究人员和专业人士需高效获取领域前沿信息，传统分析方法有局限。

Method: 结合从OpenAlex提取数据、策略性提示工程和大语言模型驱动分析，还包括数据获取、JSON构建、与Gemini交互及自动生成PDF报告等步骤。

Result: 实现了自动创建综合摘要，概括关键发现、识别新兴趋势。

Conclusion: 该方法能处理大量非结构化数据，以易理解格式提供可操作见解，帮助研究人员节省时间、了解当前趋势。

Abstract: The exponential growth of information presents a significant challenge for
researchers and professionals seeking to remain at the forefront of their
fields and this paper introduces an innovative framework for automatically
generating insightful financial digests using the power of Large Language
Models (LLMs), specifically Google's Gemini Pro. By leveraging a combination of
data extraction from OpenAlex, strategic prompt engineering, and LLM-driven
analysis, we demonstrate the automated example of creating a comprehensive
digests that generalize key findings, identify emerging trends. This approach
addresses the limitations of traditional analysis methods, enabling the
efficient processing of vast amounts of unstructured data and the delivery of
actionable insights in an easily digestible format. This paper describes how
LLMs work in simple words and how we can use their power to help researchers
and scholars save their time and stay informed about current trends. Our study
includes step-by-step process, from data acquisition and JSON construction to
interaction with Gemini and the automated generation of PDF reports, including
a link to the project's GitHub repository for broader accessibility and further
development.

</details>


### [34] [Re-evaluating sample efficiency in de novo molecule generation](https://arxiv.org/abs/2212.01385)
*Morgan Thomas,Noel M. O'Boyle,Andreas Bender,Chris De Graaf*

Main category: cs.CE

TL;DR: 本文讨论并调整样本效率基准以反映小分子药物设计目标，重新评估生成模型，发现考虑分子量、LogP和化学多样性会改变模型排名，Augmented Hill - Climb方法在样本效率和化学性质方面排名靠前。


<details>
  <summary>Details</summary>
Motivation: 从头分子生成存在数据效率低的问题，尤其是结合计算成本高的分子评分函数时，近期工作聚焦提高样本效率或对其进行基准测试，本文旨在更好地反映小分子药物设计的现实目标。

Method: 讨论并调整近期样本效率基准，重新评估所有基准生成模型，对新提出的Augmented Hill - Climb方法进行基准测试。

Result: 考虑分子量、LogP和化学多样性会改变生成模型的排名，Augmented Hill - Climb方法在样本效率和生成分子的化学性质方面排名靠前。

Conclusion: 样本效率和化学合意性的持续改进能使计算成本高的评分函数在更现实的时间尺度上更常规地集成。

Abstract: De novo molecule generation can suffer from data inefficiency; requiring
large amounts of training data or many sampled data points to conduct objective
optimization. The latter is a particular disadvantage when combining deep
generative models with computationally expensive molecule scoring functions
(a.k.a. oracles) commonly used in computer-aided drug design. Recent works have
therefore focused on methods to improve sample efficiency in the context of de
novo molecule drug design, or to benchmark it. In this work, we discuss and
adapt a recent sample efficiency benchmark to better reflect realistic goals
also with respect to the quality of chemistry generated, which must always be
considered in the context of small-molecule drug design; we then re-evaluate
all benchmarked generative models. We find that accounting for molecular weight
and LogP with respect to the training data, and the diversity of chemistry
proposed, re-orders the ranking of generative models. In addition, we benchmark
a recently proposed method to improve sample efficiency (Augmented Hill-Climb)
and found it ranked top when considering both the sample efficiency and
chemistry of molecules generated. Continual improvements in sample efficiency
and chemical desirability enable more routine integration of computationally
expensive scoring functions on a more realistic timescale.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [35] [ODataX: A Progressive Evolution of the Open Data Protocol](https://arxiv.org/abs/2510.24761)
*Anirudh Ganesh,Nitin Sood*

Main category: cs.DB

TL;DR: 分析OData协议广泛应用的障碍，提出改进版ODataX协议。


<details>
  <summary>Details</summary>
Motivation: OData协议虽强大成熟，但应用局限于企业环境，需扩大其应用范围。

Method: 提出ODataX协议，保持与OData v4向后兼容，引入简化查询语法、查询成本估算的性能护栏和增强缓存机制。

Result: 未提及具体结果

Conclusion: 旨在弥合企业级查询标准化与现代Web开发实践对简单性的需求之间的差距。

Abstract: The Open Data Protocol (OData) provides a standardized approach for building
and consuming RESTful APIs with rich query capabilities. Despite its power and
maturity, OData adoption remains confined primarily to enterprise environments,
particularly within Microsoft and SAP ecosystems. This paper analyzes the key
barriers preventing wider OData adoption and introduces ODataX, an evolved
version of the protocol designed to address these limitations. ODataX maintains
backward compatibility with OData v4 while introducing progressive complexity
disclosure through simplified query syntax, built-in performance guardrails via
query cost estimation, and enhanced caching mechanisms. This work aims to
bridge the gap between enterprise-grade query standardization and the
simplicity demanded by modern web development practices.

</details>


### [36] [StorageXTuner: An LLM Agent-Driven Automatic Tuning Framework for Heterogeneous Storage Systems](https://arxiv.org/abs/2510.25017)
*Qi Lin,Zhenyu Zhang,Viraj Thakkar,Zhenjie Sun,Mai Zheng,Zhichao Cao*

Main category: cs.DB

TL;DR: 提出StorageXTuner，一个基于LLM代理的异构存储引擎自动调优框架，经评估性能优于现有设置和ELMo - Tune。


<details>
  <summary>Details</summary>
Motivation: 现有存储系统自动配置困难，启发式和ML调优器有局限，近期基于LLM的方法也存在不足，如跨系统重用受限等。

Method: StorageXTuner通过四个代理（Executor、Extractor、Searcher、Reflector）分离关注点，结合洞察驱动的树搜索和分层内存，并使用轻量级检查器。

Result: 在RocksDB、LevelDB等系统上评估，相对开箱设置和ELMo - Tune，吞吐量最高提升575%和111%，p99延迟最多降低88%和56%，且收敛所需试验次数更少。

Conclusion: StorageXTuner在异构存储引擎自动调优方面表现良好，能有效提升性能。

Abstract: Automatically configuring storage systems is hard: parameter spaces are large
and conditions vary across workloads, deployments, and versions. Heuristic and
ML tuners are often system specific, require manual glue, and degrade under
changes. Recent LLM-based approaches help but usually treat tuning as a
single-shot, system-specific task, which limits cross-system reuse, constrains
exploration, and weakens validation. We present StorageXTuner, an LLM
agent-driven auto-tuning framework for heterogeneous storage engines.
StorageXTuner separates concerns across four agents - Executor (sandboxed
benchmarking), Extractor (performance digest), Searcher (insight-guided
configuration exploration), and Reflector (insight generation and management).
The design couples an insight-driven tree search with layered memory that
promotes empirically validated insights and employs lightweight checkers to
guard against unsafe actions. We implement a prototype and evaluate it on
RocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C.
Relative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up
to 575% and 111% higher throughput, reduces p99 latency by as much as 88% and
56%, and converges with fewer trials.

</details>


### [37] [Time-varying Vector Field Compression with Preserved Critical Point Trajectories](https://arxiv.org/abs/2510.25143)
*Mingze Xia,Yuxiao Li,Pu Jiao,Bei Wang,Xin Liang,Hanqi Guo*

Main category: cs.DB

TL;DR: 针对时变矢量场数据压缩难题，提出高效有损压缩框架，能精确保留临界点轨迹，实验显示压缩比高且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时变矢量场数据量大，无损压缩比低，现有有损压缩方法会破坏临界点轨迹，需新的压缩方法。

Method: 将保留空间临界点理论拓展到时空临界点轨迹，提出半拉格朗日预测器并与传统Lorenzo预测器结合，用四个真实科学数据集评估。

Result: 提出的方法压缩比最高达124.48X，比最佳无损压缩器高56.07X，现有有损压缩器无法在类似压缩比下保留所有临界点轨迹。

Conclusion: 提出的有损压缩框架能有效保留时变矢量场临界点轨迹，且压缩比高，有良好应用前景。

Abstract: Scientific simulations and observations are producing vast amounts of
time-varying vector field data, making it hard to store them for archival
purposes and transmit them for analysis. Lossy compression is considered a
promising approach to reducing these data because lossless compression yields
low compression ratios that barely mitigate the problem. However, directly
applying existing lossy compression methods to timevarying vector fields may
introduce undesired distortions in critical-point trajectories, a crucial
feature that encodes key properties of the vector field. In this work, we
propose an efficient lossy compression framework that exactly preserves all
critical-point trajectories in time-varying vector fields. Our contributions
are threefold. First, we extend the theory for preserving critical points in
space to preserving critical-point trajectories in space-time, and develop a
compression framework to realize the functionality. Second, we propose a
semi-Lagrange predictor to exploit the spatiotemporal correlations in
advectiondominated regions, and combine it with the traditional Lorenzo
predictor for improved compression efficiency. Third, we evaluate our method
against state-of-the-art lossy and lossless compressors using four real-world
scientific datasets. Experimental results demonstrate that the proposed method
delivers up to 124.48X compression ratios while effectively preserving all
critical-point trajectories. This compression ratio is up to 56.07X higher than
that of the best lossless compressors, and none of the existing lossy
compressors can preserve all critical-point trajectories at similar compression
ratios.

</details>


### [38] [DGAI: Decoupled On-Disk Graph-Based ANN Index for Efficient Updates and Queries](https://arxiv.org/abs/2510.25401)
*Jiahao Lou,Quan Yu,Shufeng Gong,Song Yu,Yanfeng Zhang,Ge Yu*

Main category: cs.DB

TL;DR: 提出解耦存储架构及两种策略提升ANN搜索性能，实验表明解耦架构在插入和删除操作上提升更新速度，策略提升查询效率。


<details>
  <summary>Details</summary>
Motivation: 传统耦合存储方法在索引更新时效率低，会产生大量无效I/O，且解耦架构会降低查询性能。

Method: 提出解耦存储架构，设计三阶段查询机制和增量页面级拓扑重排序策略。

Result: 解耦架构插入更新速度提升10.05倍，删除提升6.89倍，两种策略使查询效率提升2.66倍。

Conclusion: 所提技术大幅降低ANN搜索的I/O和计算开销，提升整体性能。

Abstract: On-disk graph-based indexes are widely used in approximate nearest neighbor
(ANN) search systems for large-scale, high-dimensional vectors. However,
traditional coupled storage methods, which store vectors within the index, are
inefficient for index updates. Coupled storage incurs excessive redundant
vector reads and writes when updating the graph topology, leading to
significant invalid I/O. To address this issue, we propose a decoupled storage
architecture. While a decoupled architecture reduces query performance. To
overcome this limitation, we design two tailored strategies: (i) a three-stage
query mechanism that leverages multiple PQ compressed vectors to filter invalid
I/O and computations, and (ii) an incremental page-level topological reordering
strategy that incrementally inserts new nodes into pages containing their most
similar neighbors to mitigate read amplification. Together, these techniques
substantially reduce both I/O and computational overhead during ANN search.
Experimental results show that the decoupled architecture improves update speed
by 10.05x for insertions and 6.89x for deletions, while the three-stage query
and incremental reordering enhance query efficiency by 2.66x compared to the
traditional coupled architecture.

</details>


### [39] [One Join Order Does Not Fit All: Reducing Intermediate Results with Per-Split Query Plans](https://arxiv.org/abs/2510.25684)
*Yujun He,Hangdong Zhao,Simon Frisk,Yifei Yang,Kevin Kristensen,Paraschos Koutris,Xiangyao Yu*

Main category: cs.DB

TL;DR: 提出SplitJoin框架处理多连接查询，在DuckDB和Umbra上有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有Yannakakis算法对无环查询有效，循环查询仍是挑战，需减少中间结果以高效处理多连接查询。

Method: 引入split作为一级查询运算符，将输入表划分为重和轻两部分，不同数据分区使用不同查询计划。系统探索基于split的优化设计空间。

Result: 在DuckDB上完成更多查询，运行时间快2.1倍，中间结果小7.9倍；在Umbra上完成更多查询，运行速度提升1.3倍，中间结果小1.2倍。

Conclusion: SplitJoin框架能有效减少中间结果，提高多连接查询处理效率。

Abstract: Minimizing intermediate results is critical for efficient multi-join query
processing. Although the seminal Yannakakis algorithm offers strong guarantees
for acyclic queries, cyclic queries remain an open challenge. In this paper, we
propose SplitJoin, a framework that introduces split as a first-class query
operator. By partitioning input tables into heavy and light parts, SplitJoin
allows different data partitions to use distinct query plans, with the goal of
reducing intermediate sizes using existing binary join engines. We
systematically explore the design space for split-based optimizations,
including threshold selection, split strategies, and join ordering after
splits. Implemented as a front-end to DuckDB and Umbra, SplitJoin achieves
substantial improvements: on DuckDB, SplitJoin completes 43 social network
queries (vs. 29 natively), achieving 2.1x faster runtime and 7.9x smaller
intermediates on average (up to 13.6x and 74x, respectively); on Umbra, it
completes 45 queries (vs. 35), achieving 1.3x speedups and 1.2x smaller
intermediates on average (up to 6.1x and 2.1x, respectively).

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [40] [Radar DataTree: A FAIR and Cloud-Native Framework for Scalable Weather Radar Archives](https://arxiv.org/abs/2510.24943)
*Alfonso Ladino-Rincon,Stephen W. Nesbitt*

Main category: cs.DC

TL;DR: 介绍Radar DataTree，首个将WMO FM - 301标准从单次雷达扫描扩展到时间分辨、可分析存档的数据集框架，解决雷达数据问题，有性能提升并开源。


<details>
  <summary>Details</summary>
Motivation: 气象雷达数据虽有价值但结构利用不足，现存雷达档案碎片化、特定于供应商且不符合FAIR原则，阻碍研究、可重复性和云原生计算。

Method: 采用可扩展的开源架构，基于FM - 301/CfRadial 2.1标准，用xarray DataTree实现，组织雷达扫描为分层结构并序列化为Zarr，结合Icechunk进行存储和版本控制。

Result: 在QVP和降水累积工作流程案例研究中显示出显著性能提升，通过Raw2Zarr仓库公开所有工具和数据集。

Conclusion: 为雷达数据管理、高性能地球科学和AI气象基础设施提供可重复和可扩展的基础。

Abstract: We introduce Radar DataTree, the first dataset-level framework that extends
the WMO FM-301 standard from individual radar volume scans to time-resolved,
analysis-ready archives. Weather radar data are among the most scientifically
valuable yet structurally underutilized Earth observation datasets. Despite
widespread public availability, radar archives remain fragmented,
vendor-specific, and poorly aligned with FAIR (Findable, Accessible,
Interoperable, Reusable) principles, hindering large-scale research,
reproducibility, and cloud-native computation. Radar DataTree addresses these
limitations with a scalable, open-source architecture that transforms
operational radar archives into FAIR-compliant, cloud-optimized datasets. Built
on the FM-301/CfRadial 2.1 standard and implemented using xarray DataTree,
Radar DataTree organizes radar volume scans as hierarchical, metadata-rich
structures and serializes them to Zarr for scalable analysis. Coupled with
Icechunk for ACID-compliant storage and versioning, this architecture enables
efficient, parallel computation across thousands of radar scans with minimal
preprocessing. We demonstrate significant performance gains in case studies
including Quasi-Vertical Profile (QVP) and precipitation accumulation
workflows, and release all tools and datasets openly via the Raw2Zarr
repository. This work contributes a reproducible and extensible foundation for
radar data stewardship, high-performance geoscience, and AI-ready weather
infrastructure.

</details>


### [41] [Multi-Resolution Model Fusion for Accelerating the Convolutional Neural Network Training](https://arxiv.org/abs/2510.25170)
*Kewei Wang,Claire Songhyun Lee,Sunwoo Lee,Vishu Gupta,Jan Balewski,Alex Sim,Peter Nugent,Ankit Agrawal,Alok Choudhary,Kesheng Wu,Wei-keng Liao*

Main category: cs.DC

TL;DR: 提出多分辨率模型融合（MRMF）方法减少神经网络训练成本，实验表明能显著减少训练时间且不影响精度。


<details>
  <summary>Details</summary>
Motivation: 神经网络训练耗时，尤其是处理高维大数据样本时，需高效训练方法降低计算成本。

Method: 提出MRMF方法，结合低分辨率数据训练的模型，再用原始分辨率数据细化，加速模型收敛。

Result: 在CosmoFlow和Neuron Inverter应用中，分别最多提升训练时间47%和44%，且不影响模型精度。

Conclusion: MRMF方法可显著减少端到端训练时间，同时保持模型精度。

Abstract: Neural networks are rapidly gaining popularity in scientific research, but
training the models is often very time-consuming. Particularly when the
training data samples are large high-dimensional arrays, efficient training
methodologies that can reduce the computational costs are crucial. To reduce
the training cost, we propose a Multi-Resolution Model Fusion (MRMF) method
that combines models trained on reduced-resolution data and then refined with
data in the original resolution. We demonstrate that these reduced-resolution
models and datasets could be generated quickly. More importantly, the proposed
approach reduces the training time by speeding up the model convergence in each
fusion stage before switching to the final stage of finetuning with data in its
original resolution. This strategy ensures the final model retains
high-resolution insights while benefiting from the computational efficiency of
lower-resolution training. Our experiment results demonstrate that the
multi-resolution model fusion method can significantly reduce end-to-end
training time while maintaining the same model accuracy. Evaluated using two
real-world scientific applications, CosmoFlow and Neuron Inverter, the proposed
method improves the training time by up to 47% and 44%, respectively, as
compared to the original resolution training, while the model accuracy is not
affected.

</details>


### [42] [MoEntwine: Unleashing the Potential of Wafer-scale Chips for Large-scale Expert Parallel Inference](https://arxiv.org/abs/2510.25258)
*Xinru Tang,Jingxiang Hou,Dingcheng Jiang,Taiquan Wei,Jiaxin Liu,Jinyi Deng,Huizheng Wang,Qize Yang,Haoran Shang,Chao Li,Yang Hu,Shouyi Yin*

Main category: cs.DC

TL;DR: 本文针对在WSC平台运行MoE模型面临的问题，提出ER - Mapping和NI - Balancer，评估显示有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有GPU集群中EP因跨节点通信开销大受限，WSC虽有潜力但存在网络拓扑和专家迁移开销问题，需挖掘其潜力。

Method: 提出Entwined Ring Mapping（ER - Mapping）平衡通信压力；提出Non - invasive Balancer（NI - Balancer）隐藏迁移开销。

Result: ER - Mapping实现通信减少达62%；NI - Balancer使MoE计算和通信分别提升54%和22%；WSC平台比SOTA NVL72超级节点单设备MoE性能平均高39%。

Conclusion: 提出的方法有效挖掘了WSC平台运行MoE模型的潜力，因可扩展到更大EP，WSC平台性能更优。

Abstract: As large language models (LLMs) continue to scale up, mixture-of-experts
(MoE) has become a common technology in SOTA models. MoE models rely on expert
parallelism (EP) to alleviate memory bottleneck, which introduces all-to-all
communication to dispatch and combine tokens across devices. However, in
widely-adopted GPU clusters, high-overhead cross-node communication makes
all-to-all expensive, hindering the adoption of EP. Recently, wafer-scale chips
(WSCs) have emerged as a platform integrating numerous devices on a wafer-sized
interposer. WSCs provide a unified high-performance network connecting all
devices, presenting a promising potential for hosting MoE models. Yet, their
network is restricted to a mesh topology, causing imbalanced communication
pressure and performance loss. Moreover, the lack of on-wafer disk leads to
high-overhead expert migration on the critical path.
  To fully unleash this potential, we first propose Entwined Ring Mapping
(ER-Mapping), which co-designs the mapping of attention and MoE layers to
balance communication pressure and achieve better performance. We find that
under ER-Mapping, the distribution of cold and hot links in the attention and
MoE layers is complementary. Therefore, to hide the migration overhead, we
propose the Non-invasive Balancer (NI-Balancer), which splits a complete expert
migration into multiple steps and alternately utilizes the cold links of both
layers. Evaluation shows ER-Mapping achieves communication reduction up to 62%.
NI-Balancer further delivers 54% and 22% improvements in MoE computation and
communication, respectively. Compared with the SOTA NVL72 supernode, the WSC
platform delivers an average 39% higher per-device MoE performance owing to its
scalability to larger EP.

</details>


### [43] [A Privacy-Preserving Ecosystem for Developing Machine Learning Algorithms Using Patient Data: Insights from the TUM.ai Makeathon](https://arxiv.org/abs/2510.25277)
*Simon Süwer,Mai Khanh Mai,Christoph Klein,Nicola Götzenberger,Denis Dalić,Andreas Maier,Jan Baumbach*

Main category: cs.DC

TL;DR: 提出多阶段安全AI训练方法，结合模拟临床知识图谱和联邦学习框架，在TUMaiM24挑战中验证，可实现医疗隐私保护AI。


<details>
  <summary>Details</summary>
Motivation: 临床数据整合对个性化医疗有潜力，但受GDPR限制，高质量结构化数据对医疗AI发展重要，需解决数据安全问题。

Method: 提出多阶段方法，包括在模拟临床知识图谱设计模型、集成到联邦学习框架、在医院环境用真实图谱训练、执行验证评估脚本。

Result: 该方法在TUMaiM24挑战中成功验证，50名学生无真实数据也能开发模型。

Conclusion: 通过联邦框架部署安全算法是实现医疗隐私保护AI的实用方式。

Abstract: The integration of clinical data offers significant potential for the
development of personalized medicine. However, its use is severely restricted
by the General Data Protection Regulation (GDPR), especially for small cohorts
with rare diseases. High-quality, structured data is essential for the
development of predictive medical AI. In this case study, we propose a novel,
multi-stage approach to secure AI training: (1) The model is designed on a
simulated clinical knowledge graph (cKG). This graph is used exclusively to
represent the structural characteristics of the real cKG without revealing any
sensitive content. (2) The model is then integrated into the FeatureCloud (FC)
federated learning framework, where it is prepared in a single-client
configuration within a protected execution environment. (3) Training then takes
place within the hospital environment on the real cKG, either under the direct
supervision of hospital staff or via a fully automated pipeline controlled by
the hospital. (4) Finally, verified evaluation scripts are executed, which only
return aggregated performance metrics. This enables immediate performance
feedback without sensitive patient data or individual predictions, leaving the
clinic. A fundamental element of this approach involves the incorporation of a
cKG, which serves to organize multi-omics and patient data within the context
of real-world hospital environments. This approach was successfully validated
during the TUM.ai Makeathon 2024 (TUMaiM24) challenge set by the Dr. von Hauner
Children's Hospital (HCH-LMU): 50 students developed models for patient
classification and diagnosis without access to real data. Deploying secure
algorithms via federated frameworks, such as the FC framework, could be a
practical way of achieving privacy-preserving AI in healthcare.

</details>


### [44] [Can Like Attract Like? A Study of Homonymous Gathering in Networks](https://arxiv.org/abs/2510.25451)
*Stéphane Devismes,Yoann Dieudonné,Arnaud Labourel*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A team of mobile agents, starting from distinct nodes of a network, have to
meet at the same node and declare that they all met. Agents execute the same
algorithm, which they start when activated by an adversary or by an agent
entering their initial node. When activated, agents traverse edges of the
network in synchronous rounds. Their perception and communication are strictly
local. This task, known as gathering, is a central problem in distributed
mobile systems. Most prior work focuses on minimizing its time complexity,
i.e., the worst-case number of rounds between the start of the earliest agent
and the task completion. To break possible symmetries, deterministic solutions
typically assume that agents have pairwise distinct IDs, called labels, known
only to themselves. But must all labels be pairwise distinct to guarantee
deterministic gathering?
  We address this question by considering agents that may share the same label.
A team L is said to be gatherable if, for every initial setting of L, there is
an algorithm that solves gathering. Our contribution is threefold. (1) We give
a full characterization of the gatherable teams. (2) We design an algorithm
that gathers all of them in poly$(n,\log\lambda)$ time, where $n$ (resp.
$\lambda$) is the graph order (resp. the smallest label in L). This algorithm
requires the agents to initially share only $O(\log \log \log \mu)$ bits of
common knowledge, where $\mu$ is the largest label multiplicity in L. (3) We
show this dependency is almost optimal to get a poly$(n,\log\lambda)$-time
complexity.
  As a by-product, we get the first deterministic poly$(n,\log\lambda)$-time
algorithm requiring no common knowledge to gather any team when all labels are
distinct. Known to be achievable for two-agent teams, extending this to any
team size faced a major challenge: termination detection. Our techniques to
address it may be of independent interest.

</details>


### [45] [Scheduling Data-Intensive Workloads in Large-Scale Distributed Systems: Trends and Challenges](https://arxiv.org/abs/2510.25362)
*Georgios L. Stavrinides,Helen D. Karatza*

Main category: cs.DC

TL;DR: 大数据增长使工作负载复杂，需有效调度技术。本文提出数据密集型工作负载分类，介绍常用调度方法、新策略及未来方向。


<details>
  <summary>Details</summary>
Motivation: 大数据下工作负载复杂，对分布式资源调度提出挑战，需有效调度技术。

Method: 提出数据密集型工作负载分类，综述大规模分布式系统中常用调度方法，介绍文献中新策略。

Result: 给出数据密集型工作负载分类和常用调度方法，介绍新策略。

Conclusion: 点明当前存在的挑战和未来研究方向。

Abstract: With the explosive growth of big data, workloads tend to get more complex and
computationally demanding. Such applications are processed on distributed
interconnected resources that are becoming larger in scale and computational
capacity. Data-intensive applications may have different degrees of parallelism
and must effectively exploit data locality. Furthermore, they may impose
several Quality of Service requirements, such as time constraints and
resilience against failures, as well as other objectives, like energy
efficiency. These features of the workloads, as well as the inherent
characteristics of the computing resources required to process them, present
major challenges that require the employment of effective scheduling
techniques. In this chapter, a classification of data-intensive workloads is
proposed and an overview of the most commonly used approaches for their
scheduling in large-scale distributed systems is given. We present novel
strategies that have been proposed in the literature and shed light on open
challenges and future directions.

</details>


### [46] [Holon Streaming: Global Aggregations with Windowed CRDTs](https://arxiv.org/abs/2510.25757)
*Jonas Spenger,Kolya Krafeld,Ruben van Gemeren,Philipp Haller,Paris Carbone*

Main category: cs.DC

TL;DR: 论文提出Holon Streaming系统解决全局聚合可扩展性问题，用Windowed CRDTs实现低延迟、高吞吐量和高效故障恢复。


<details>
  <summary>Details</summary>
Motivation: 当前流处理系统在全局聚合可扩展性上存在局限，单任务实例计算和静态聚合树限制了扩展性，且会导致高延迟和故障恢复问题。

Method: 提出Holon Streaming系统，使用Windowed CRDTs这一共享复制状态的新抽象，通过去中心化协调设计故障恢复算法。

Result: 在全局聚合工作负载上，相比现有系统，延迟降低5倍，吞吐量提高2倍，故障场景下延迟降低11倍。

Conclusion: 证明了具有确定性的去中心化协调的有效性，以及Windowed CRDTs在全局聚合中的实用性。

Abstract: Scaling global aggregations is a challenge for exactly-once stream processing
systems. Current systems implement these either by computing the aggregation in
a single task instance, or by static aggregation trees, which limits
scalability and may become a bottleneck. Moreover, the end-to-end latency is
determined by the slowest path in the tree, and failures and reconfiguration
cause large latency spikes due to the centralized coordination. Towards these
issues, we present Holon Streaming, an exactly-once stream processing system
for global aggregations. Its deterministic programming model uses windowed
conflict-free replicated data types (Windowed CRDTs), a novel abstraction for
shared replicated state. Windowed CRDTs make computing global aggregations
scalable. Furthermore, their guarantees such as determinism and convergence
enable the design of efficient failure recovery algorithms by decentralized
coordination. Our evaluation shows a 5x lower latency and 2x higher throughput
than an existing stream processing system on global aggregation workloads, with
an 11x latency reduction under failure scenarios. The paper demonstrates the
effectiveness of decentralized coordination with determinism, and the utility
of Windowed CRDTs for global aggregations.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [47] [Reviving Thorup's Shortcut Conjecture](https://arxiv.org/abs/2510.24954)
*Aaron Bernstein,Henry Fleischmann,George Z. Li,Bernhard Haeupler,Maximilian Probst Gutenberg,Gary Hoppenworth,Seth Pettie,Thatchaphol Saranurak,Yonggang Jiang,Leon Schiller*

Main category: cs.DS

TL;DR: 本文重新探讨Thorup猜想，允许新顶点时打破已知下界，排除特定厚度捷径，提出候选困难实例，还展示了对并行算法的积极影响。


<details>
  <summary>Details</summary>
Motivation: Thorup猜想被Hesse反驳，但原猜想精神未被否定，希望进一步探索含新顶点的捷径存在性。

Method: 提出明确攻击打破下界，排除特定厚度捷径，提出候选困难实例。

Result: 允许Steiner顶点时打破已知捷径下界；排除特定厚度捷径；提出候选困难实例；展示对并行算法的积极影响。

Conclusion: 研究推进了Thorup猜想的修订版解决，对并行算法有潜在提升。

Abstract: We aim to revive Thorup's conjecture [Thorup, WG'92] on the existence of
reachability shortcuts with ideal size-diameter tradeoffs. Thorup originally
asked whether, given any graph $G=(V,E)$ with $m$ edges, we can add
$m^{1+o(1)}$ ``shortcut'' edges $E_+$ from the transitive closure $E^*$ of $G$
so that $\text{dist}_{G_+}(u,v) \leq m^{o(1)}$ for all $(u,v)\in E^*$, where
$G_+=(V,E\cup E_+)$. The conjecture was refuted by Hesse [Hesse, SODA'03],
followed by significant efforts in the last few years to optimize the lower
bounds.
  In this paper we observe that although Hesse refuted the letter of Thorup's
conjecture, his work~[Hesse, SODA'03] -- and all followup work -- does not
refute the spirit of the conjecture, which should allow $G_+$ to contain both
new (shortcut) edges and new Steiner vertices. Our results are as follows.
  (1) On the positive side, we present explicit attacks that break all known
shortcut lower bounds when Steiner vertices are allowed.
  (2) On the negative side, we rule out ideal $m^{1+o(1)}$-size,
$m^{o(1)}$-diameter shortcuts whose ``thickness'' is $t=o(\log n/\log \log n)$,
meaning no path can contain $t$ consecutive Steiner vertices.
  (3) We propose a candidate hard instance as the next step toward resolving
the revised version of Thorup's conjecture.
  Finally, we show promising implications. Almost-optimal parallel algorithms
for computing a generalization of the shortcut that approximately preserves
distances or flows imply almost-optimal parallel algorithms with $m^{o(1)}$
depth for exact shortcut paths and exact maximum flow. The state-of-the-art
algorithms have much worse depth of $n^{1/2+o(1)}$ [Rozho\v{n}, Haeupler,
Martinsson, STOC'23] and $m^{1+o(1)}$ [Chen, Kyng, Liu, FOCS'22], respectively.

</details>


### [48] [Hedgegraph Polymatroids](https://arxiv.org/abs/2510.25043)
*Karthekeyan Chandrasekaran,Chandra Chekuri,Weihang Wang,Weihao Zhu*

Main category: cs.DS

TL;DR: 本文为解决hedgegraphs连通性算法难题，引入两种基于分区的连通性度量，研究相关结构和算法，通过多面体视角得到新的易处理结果和经典结果的推广。


<details>
  <summary>Details</summary>
Motivation: hedgegraphs在建模上有优势但存在算法挑战，其割函数非子模性阻碍了连通性算法发展。

Method: 引入两种基于分区的连通性度量，研究与hedgegraphs相关的多面体。

Result: 获得新的易处理结果以及对图和超图经典结果的深刻推广。

Conclusion: 多面体视角有助于解决hedgegraphs连通性算法问题，推动其相关研究。

Abstract: Graphs and hypergraphs combine expressive modeling power with algorithmic
efficiency for a wide range of applications. Hedgegraphs generalize hypergraphs
further by grouping hyperedges under a color/hedge. This allows hedgegraphs to
model dependencies between hyperedges and leads to several applications.
However, it poses algorithmic challenges. In particular, the cut function is
not submodular, which has been a barrier to algorithms for connectivity. In
this work, we introduce two alternative partition-based measures of
connectivity in hedgegraphs and study their structural and algorithmic aspects.
Instead of the cut function, we investigate a polymatroid associated with
hedgegraphs. The polymatroidal lens leads to new tractability results as well
as insightful generalizations of classical results on graphs and hypergraphs.

</details>


### [49] [$\{s,t\}$-Separating Principal Partition Sequence of Submodular Functions](https://arxiv.org/abs/2510.25664)
*Kristóf Bérczi,Karthekeyan Chandrasekaran,Tamás Király,Daniel P. Szabo*

Main category: cs.DS

TL;DR: 本文受两个应用启发，发展了子模函数的 {s,t}-分离主划分序列理论，给出定义、证明存在性、设计构造算法，并展示两个应用。


<details>
  <summary>Details</summary>
Motivation: 受两个应用启发，发展子模函数的 {s,t}-分离主划分序列理论。

Method: 定义 {s,t}-分离主划分序列，证明其存在性，设计多项式时间构造算法。

Result: 成功定义序列、证明存在性并给出构造算法，展示两个应用。

Conclusion: 子模函数的 {s,t}-分离主划分序列理论可用于解决相关问题，如近似算法和超图定向问题。

Abstract: Narayanan and Fujishige showed the existence of the principal partition
sequence of a submodular function, a structure with numerous applications in
areas such as clustering, fast algorithms, and approximation algorithms. In
this work, motivated by two applications, we develop a theory of
$\{s,t\}$-separating principal partition sequence of a submodular function. We
define this sequence, show its existence, and design a polynomial-time
algorithm to construct it. We show two applications: (1) approximation
algorithm for the $\{s,t\}$-separating submodular $k$-partitioning problem for
monotone and posimodular functions and (2) polynomial-time algorithm for the
hypergraph orientation problem of finding an orientation that simultaneously
has strong connectivity at least $k$ and $(s,t)$-connectivity at least $\ell$.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [50] [What Are People's Actual Utility Functions in Budget Aggregation?](https://arxiv.org/abs/2510.24872)
*Ayelet Amster,Lioz Akirav,Rica Gonen,Erel Segal-Halevi*

Main category: cs.GT

TL;DR: 本文通过对人类参与者进行结构化民意调查，检验常见效用函数对实际偏好的解释力，发现常见模型解释力有限，而星形偏好和峰值线性效用函数有更多证据支持，为预算聚合机制设计提供新思路。


<details>
  <summary>Details</summary>
Motivation: 现有参与式预算和预算聚合机制对选民评估非理想预算分配的效用模型缺乏实证验证，需确定准确捕捉人类偏好的效用模型。

Method: 对人类参与者进行结构化民意调查，检验人们的偏好是否符合常见的效用函数。

Result: 常见效用模型对实际行为解释力有限，多数参与者在不同度量比较中表现出不一致模式；大部分参与者的偏好与星形偏好和峰值线性效用函数一致。

Conclusion: 标准距离度量在预算聚合机制中存在理论上的不可能结果，应考虑替代建模方法；本研究为实证检验预算聚合理论的效用函数假设提供了系统方法，有助于更稳健和现实的机制设计。

Abstract: While participatory budgeting and budget-aggregation mechanisms require
assumptions about how voters evaluate non-ideal budget allocations, little
empirical evidence exists to validate which utility models accurately capture
human preferences. We conducted structured polls with human participants to
test whether real people's preferences conform to commonly assumed utility
functions such as $\ell_1$, $\ell_2$ and Leontief. Our results suggest that
these models may have limited explanatory power for actual behavior: most
participants showed inconsistent patterns across different metric comparisons,
and standard assumptions of project symmetry and sign symmetry -- core features
of common distance-based metrics -- received little empirical support. However,
we find encouraging evidence for more fundamental preference structures: a
large majority of participants showed consistency with star-shaped preferences,
as well as with peak-linear utility functions, where utility changes
proportionally with distance from the ideal budget. These findings have
important implications for designers of budget aggregation mechanisms. While
theoretical results demonstrate impossibility results for standard distance
metrics regarding truthfulness, Pareto-efficiency, and proportionality, our
evidence suggests alternative modeling approaches may be warranted. More
broadly, this work introduces a systematic methodology to empirically test the
utility function assumptions that underpin budget aggregation theories, paving
the way for more robust and realistic mechanism design.

</details>


### [51] [Fair Indivisible Payoffs through Shapley Value](https://arxiv.org/abs/2510.24906)
*Mikołaj Czarnecki,Michał Korniak,Oskar Skibski,Piotr Skowron*

Main category: cs.GT

TL;DR: 本文考虑不可分割联盟博弈中的收益分配问题，提出公平分配方法并定义不可分割夏普利值，通过案例研究展示该技术。


<details>
  <summary>Details</summary>
Motivation: 为不可分割联盟博弈中玩家公平分配对象（如议会席位、肾脏交换、机器学习模型特征）提供方法。

Method: 定义不可分割夏普利值并研究其性质。

Result: 通过三个案例研究展示了所提技术，如在图像分类任务中识别图像关键区域。

Conclusion: 提出的公平分配方法及不可分割夏普利值有一定实用性。

Abstract: We consider the problem of payoff division in indivisible coalitional games,
where the value of the grand coalition is a natural number. This number
represents a certain quantity of indivisible objects, such as parliamentary
seats, kidney exchanges, or top features contributing to the outcome of a
machine learning model. The goal of this paper is to propose a fair method for
dividing these objects among players. To achieve this, we define the
indivisible Shapley value and study its properties. We demonstrate our proposed
technique using three case studies, in particular, we use it to identify key
regions of an image in the context of an image classification task.

</details>


### [52] [Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response Games](https://arxiv.org/abs/2510.25080)
*Will Wolf*

Main category: cs.GT

TL;DR: 本文介绍了有界单边响应游戏（BORGs），以改良版《大富翁交易》为基准环境，证明反事实遗憾最小化（CFR）算法可在该领域收敛到有效策略，并提供研究平台。


<details>
  <summary>Details</summary>
Motivation: 研究较少被探索但具有丰富策略性的有界单边响应游戏结构。

Method: 引入改良版《大富翁交易》作为基准环境，使用反事实遗憾最小化（CFR）算法，构建轻量级全栈研究平台。

Result: CFR算法在该领域成功收敛到有效策略，搭建的研究平台可在单工作站运行。

Conclusion: 该研究平台为有界单边响应场景下的状态表示和策略学习提供了实践基础，训练的CFR代理和源代码可在线获取。

Abstract: Card games are widely used to study sequential decision-making under
uncertainty, with real-world analogues in negotiation, finance, and
cybersecurity. Typically, these games fall into three categories based on the
flow of control: strictly-sequential (where players alternate single actions),
deterministic-response (where some actions trigger a fixed outcome), and
unbounded reciprocal-response (where alternating counterplays are permitted). A
less-explored but strategically rich structure exists: the bounded one-sided
response. This dynamic occurs when a player's action briefly transfers control
to the opponent, who must satisfy a fixed condition through one or more
sequential moves before the turn resolves. We term games featuring this
mechanism Bounded One-Sided Response Games (BORGs).
  We introduce a modified version of Monopoly Deal as a benchmark environment
that specifically isolates the BORG dynamic, where a Rent action forces the
opponent to sequentially choose payment assets. We demonstrate that the
gold-standard algorithm, Counterfactual Regret Minimization (CFR), successfully
converges on effective strategies for this domain without requiring novel
algorithmic extensions. To support efficient, reproducible experimentation, we
present a lightweight, full-stack research platform that unifies the
environment, a parallelized CFR runtime, and a human-playable web interface,
all runnable on a single workstation. This system provides a practical
foundation for exploring state representation and policy learning in bounded
one-sided response settings.
  The trained CFR agent and source code are available at
https://monopolydeal.ai.

</details>


### [53] [Timing Games in Responsive Consensus Protocols](https://arxiv.org/abs/2510.25144)
*Kaya Alpturer,Kushal Babel,Aditya Saraf*

Main category: cs.GT

TL;DR: 本文研究响应式共识协议中的时间博弈，提出动态区块奖励机制激励快速提案，证明响应性可促进更快出块，且动态奖励对验证者效用影响较小。


<details>
  <summary>Details</summary>
Motivation: 区块链应用中验证者为获更多奖励会进行时间博弈，导致响应性在区块链协议中看似无法实现，需解决此问题。

Method: 建立时间博弈模型，发现囚徒困境结构；引入随轮次时间递减的动态区块奖励机制；通过投票机制测量延迟。

Result: 通过设置协议参数，投票机制使验证者达成合作均衡，提高奖励率；从静态到动态区块奖励使验证者效用对延迟更敏感，但影响较小。

Conclusion: 响应性并非因时间博弈无法实现，反而可促进更快出块，动态区块奖励机制可行。

Abstract: Optimistic responsiveness -- the ability of a consensus protocol to operate
at the speed of the network -- is widely used in consensus protocol design to
optimize latency and throughput. However, blockchain applications incentivize
validators to play timing games by strategically delaying their proposals,
since increased block time correlates with greater rewards. Consequently, it
may appear that responsiveness (even under optimistic conditions) is impossible
in blockchain protocols. In this work, we develop a model of timing games in
responsive consensus protocols and find a prisoner's dilemma structure, where
cooperation (proposing promptly) is in the validators' best interest, but
individual incentives encourage validators to delay proposals selfishly. To
attain desirable equilibria, we introduce dynamic block rewards that decrease
with round time to explicitly incentivize faster proposals. Delays are measured
through a voting mechanism, where other validators vote on the current leader's
round time. By carefully setting the protocol parameters, the voting mechanism
allows validators to coordinate and reach the cooperative equilibrium,
benefiting all through a higher rate-of-reward. Thus, instead of responsiveness
being an unattainable property due to timing games, we show that responsiveness
itself can promote faster block proposals. One consequence of moving from a
static to dynamic block reward is that validator utilities become more
sensitive to latency, worsening the gap between the best- and worst-connected
validators. Our analysis shows, however, that this effect is minor in both
theoretical latency models and simulations based on real-world networks.

</details>


### [54] [On Robust Popular Matchings with Tie-Bounded Preferences and Stable Matchings with Two-Sided Ties](https://arxiv.org/abs/2510.25209)
*Koustav De*

Main category: cs.GT

TL;DR: 分析单边模型和单边带平局双边模型中鲁棒流行匹配的存在情况，给出单边带平局双边模型流行匹配的简单刻画，提出多项式时间算法判断鲁棒流行匹配是否存在，还解决了双边有弱偏好且平局长度至多为k的稳定匹配问题。


<details>
  <summary>Details</summary>
Motivation: 研究不同模型下鲁棒流行匹配的存在性及稳定匹配问题，为相关匹配问题提供解决方法。

Method: 对单边模型和单边带平局双边模型进行分析，提出多项式时间算法。

Result: 提出多项式时间算法判断单边模型和单边带平局双边模型中鲁棒流行匹配是否存在，给出单边带平局双边模型流行匹配的简单刻画，解决了双边有弱偏好且平局长度至多为k的稳定匹配问题。

Conclusion: 在单边模型和单边带平局双边模型中，可通过多项式时间算法判断鲁棒流行匹配是否存在，且能解决特定条件下的稳定匹配问题。

Abstract: We are given a bipartite graph $G = \left( A \cup B, E \right)$. In the
one-sided model, every $a \in A$ (often called agents) ranks its neighbours $z
\in N_{a}$ strictly, and no $b \in B$ has any preference order over its
neighbours $y \in N_{b}$, and vertices in $B$ abstain from casting their votes
to matchings. In the two-sided model with one-sided ties, every $a \in A$ ranks
its neighbours $z \in N_{a}$ strictly, and every $b \in B$ puts all of its
neighbours into a single large tie, i.e., $b \in B$ prefers every $y \in N_{b}$
equally. In this two-sided model with one-sided ties, when two matchings
compete in a majority election, $b \in B$ abstains from casting its vote for a
matching when both the matchings saturate $b$ or both leave $b$ unsaturated;
else $b$ prefers the matching where it is saturated. A popular matching $M$ is
\emph{robust} if it remains popular among multiple instances.
  We have analysed the cases when a robust popular matching exists in the
one-sided model where only one agent alters her preference order among the
instances, and we have proposed a polynomial-time algorithm to decide if there
exists a robust popular matching when instances differ only with respect to the
preference orders of a single agent.
  We give a simple characterisation of popular matchings in the two-sided model
with one-sided ties. We show that in the two-sided model with one-sided ties,
if the input instances differ only with respect to the preference orders of a
single agent, there is a polynomial-time algorithm to decide whether there
exists a robust popular matching. We have been able to decide the stable
matching problem in bipartite graphs $G = (A \cup B, E)$ where \textit{both}
sides have weak preferences (ties allowed), with the restriction that every tie
has length at most $k$.

</details>


### [55] [Learning-Augmented Online Bidding in Stochastic Settings](https://arxiv.org/abs/2510.25582)
*Spyros Angelopoulos,Bertrand Simon*

Main category: cs.GT

TL;DR: 研究学习增强设置下含随机性的在线投标问题，分两部分分析，与以往工作有区别。


<details>
  <summary>Details</summary>
Motivation: 在线投标是经典优化问题，以往工作多关注无随机信息预言机和确定性算法，本文研究含随机性的情况。

Method: 第一部分研究分布预测下投标，找帕累托最优算法；第二部分给出随机投标算法一致性/鲁棒性权衡的上下界。

Result: 找到分布预测下投标的帕累托最优算法，给出随机投标算法一致性/鲁棒性权衡的上下界。

Conclusion: 在学习增强且含随机性的在线投标研究上取得成果，与以往工作形成补充。

Abstract: Online bidding is a classic optimization problem, with several applications
in online decision-making, the design of interruptible systems, and the
analysis of approximation algorithms. In this work, we study online bidding
under learning-augmented settings that incorporate stochasticity, in either the
prediction oracle or the algorithm itself. In the first part, we study bidding
under distributional predictions, and find Pareto-optimal algorithms that offer
the best-possible tradeoff between the consistency and the robustness of the
algorithm. In the second part, we study the power and limitations of randomized
bidding algorithms, by presenting upper and lower bounds on the
consistency/robustness tradeoffs. Previous works focused predominantly on
oracles that do not leverage stochastic information on the quality of the
prediction, and deterministic algorithms.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [56] [GReF: A Unified Generative Framework for Efficient Reranking via Ordered Multi-token Prediction](https://arxiv.org/abs/2510.25220)
*Zhijie Lin,Zhuofeng Li,Chenglei Dai,Wentian Bao,Shuai Lin,Enyun Yu,Haoxiang Zhang,Liang Zhao*

Main category: cs.IR

TL;DR: 提出统一生成高效重排框架GReF解决现有两阶段重排方法的问题，在离线实验和快手应用中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段重排方法存在生成器和评估器分离阻碍端到端训练、自回归生成器推理效率低的问题。

Method: 提出GReF框架，引入Gen - Reranker生成因果重排序列，预训练Gen - Reranker，通过Rerank - DPO进行后训练，引入有序多令牌预测（OMTP）提高推理效率。

Result: 离线实验中GReF优于现有重排方法，延迟接近非自回归模型；在快手应用中显著提升在线推荐质量。

Conclusion: GReF有效解决现有两阶段重排方法的问题，可在实时推荐系统中实际部署。

Abstract: In a multi-stage recommendation system, reranking plays a crucial role in
modeling intra-list correlations among items. A key challenge lies in exploring
optimal sequences within the combinatorial space of permutations. Recent
research follows a two-stage (generator-evaluator) paradigm, where a generator
produces multiple feasible sequences, and an evaluator selects the best one. In
practice, the generator is typically implemented as an autoregressive model.
However, these two-stage methods face two main challenges. First, the
separation of the generator and evaluator hinders end-to-end training. Second,
autoregressive generators suffer from inference efficiency. In this work, we
propose a Unified Generative Efficient Reranking Framework (GReF) to address
the two primary challenges. Specifically, we introduce Gen-Reranker, an
autoregressive generator featuring a bidirectional encoder and a dynamic
autoregressive decoder to generate causal reranking sequences. Subsequently, we
pre-train Gen-Reranker on the item exposure order for high-quality parameter
initialization. To eliminate the need for the evaluator while integrating
sequence-level evaluation during training for end-to-end optimization, we
propose post-training the model through Rerank-DPO. Moreover, for efficient
autoregressive inference, we introduce ordered multi-token prediction (OMTP),
which trains Gen-Reranker to simultaneously generate multiple future items
while preserving their order, ensuring practical deployment in real-time
recommender systems. Extensive offline experiments demonstrate that GReF
outperforms state-of-the-art reranking methods while achieving latency that is
nearly comparable to non-autoregressive models. Additionally, GReF has also
been deployed in a real-world video app Kuaishou with over 300 million daily
active users, significantly improving online recommendation quality.

</details>


### [57] [TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation](https://arxiv.org/abs/2510.25259)
*Yehjin Shin,Jeongwhan Choi,Seojin Kim,Noseong Park*

Main category: cs.IR

TL;DR: 提出TV - Rec模型用于序列推荐，用时间可变滤波器替代固定核和自注意力，实验显示优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有卷积滤波器用于序列推荐时，固定滤波器难以捕捉全局交互，多数模型需用自注意力补充。

Method: 受图信号处理启发，提出TV - Rec模型，用时间可变图滤波器捕捉用户序列中与位置相关的时间变化，替代固定核和自注意力。

Result: 在六个公共基准上的广泛实验表明，TV - Rec平均比现有最先进的基线高出7.49%。

Conclusion: TV - Rec模型具有更高的表达能力，能更好捕捉用户行为中的复杂交互模式，无需自注意力，减少计算并加速推理。

Abstract: Recently, convolutional filters have been increasingly adopted in sequential
recommendation for their ability to capture local sequential patterns. However,
most of these models complement convolutional filters with self-attention. This
is because convolutional filters alone, generally fixed filters, struggle to
capture global interactions necessary for accurate recommendation. We propose
Time-Variant Convolutional Filters for Sequential Recommendation (TV-Rec), a
model inspired by graph signal processing, where time-variant graph filters
capture position-dependent temporal variations in user sequences. By replacing
both fixed kernels and self-attention with time-variant filters, TV-Rec
achieves higher expressive power and better captures complex interaction
patterns in user behavior. This design not only eliminates the need for
self-attention but also reduces computation while accelerating inference.
Extensive experiments on six public benchmarks show that TV-Rec outperforms
state-of-the-art baselines by an average of 7.49%.

</details>


### [58] [Revisiting scalable sequential recommendation with Multi-Embedding Approach and Mixture-of-Experts](https://arxiv.org/abs/2510.25285)
*Qiushi Pan,Hao Wang,Guoyuan An,Luankang Zhang,Wei Guo,Yong Liu*

Main category: cs.IR

TL;DR: 提出 Fuxi - MME 框架解决顺序推荐模型扩展问题，实验显示优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有顺序推荐模型在处理物品多面特征和动态相关性方面存在挑战，需有效扩展推荐模型。

Method: 提出 Fuxi - MME 框架，将多嵌入策略与混合专家（MoE）架构结合，分解传统单嵌入矩阵，用 MoE 层替换 Fuxi 块相关参数。

Result: 在公共数据集上的实验结果表明，该框架优于多个竞争基线模型。

Conclusion: Fuxi - MME 框架能有效解决顺序推荐模型扩展问题，提升推荐效果。

Abstract: In recommendation systems, how to effectively scale up recommendation models
has been an essential research topic. While significant progress has been made
in developing advanced and scalable architectures for sequential
recommendation(SR) models, there are still challenges due to items'
multi-faceted characteristics and dynamic item relevance in the user context.
To address these issues, we propose Fuxi-MME, a framework that integrates a
multi-embedding strategy with a Mixture-of-Experts (MoE) architecture.
Specifically, to efficiently capture diverse item characteristics in a
decoupled manner, we decompose the conventional single embedding matrix into
several lower-dimensional embedding matrices. Additionally, by substituting
relevant parameters in the Fuxi Block with an MoE layer, our model achieves
adaptive and specialized transformation of the enriched representations.
Empirical results on public datasets show that our proposed framework
outperforms several competitive baselines.

</details>


### [59] [Towards Automated Quality Assurance of Patent Specifications: A Multi-Dimensional LLM Framework](https://arxiv.org/abs/2510.25402)
*Yuqian Chai,Chaochao Wang,Weilei Wang*

Main category: cs.IR

TL;DR: 提出评估专利内容质量框架，在数据集验证，分析缺陷分布，指出AI生成专利有更多结构缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有研究对专利内容质量的系统评估关注不足。

Method: 使用监管合规、技术连贯性和图引用一致性检测模块评估专利，通过集成模块生成改进建议，并在包含人类和AI生成专利的数据集上验证。

Result: 三个检测模块平衡准确率分别为99.74%、82.12%和91.2%，分析了不同专利部分、技术领域和创作来源的缺陷分布。

Conclusion: 图文本一致性和技术细节精度需关注，机械工程和建筑领域有更多权利要求与说明书不一致问题，AI生成专利结构缺陷更明显。

Abstract: Despite the surge in patent applications and emergence of AI drafting tools,
systematic evaluation of patent content quality has received limited research
attention. To address this gap, We propose to evaluate patents using regulatory
compliance, technical coherence, and figure-reference consistency detection
modules, and then generate improvement suggestions via an integration module.
The framework is validated on a comprehensive dataset comprising 80
human-authored and 80 AI-generated patents from two patent drafting tools.
Experimental results show balanced accuracies of 99.74\%, 82.12\%, and 91.2\%
respectively across the three detection modules when validated against expert
annotations. Additional analysis was conducted to examine defect distributions
across patent sections, technical domains, and authoring sources. Section-based
analysis indicates that figure-text consistency and technical detail precision
require particular attention. Mechanical Engineering and Construction show more
claim-specification inconsistencies due to complex technical documentation
requirements. AI-generated patents show a significant gap compared to
human-authored ones. While human-authored patents primarily contain
surface-level errors like typos, AI-generated patents exhibit more structural
defects in figure-text alignment and cross-references.

</details>


### [60] [Alibaba International E-commerce Product Search Competition DcuRAGONs Team Technical Report](https://arxiv.org/abs/2510.25428)
*Thang-Long Nguyen-Ho,Minh-Khoi Pham,Hoang-Bao Le*

Main category: cs.IR

TL;DR: 报告介绍多语言电商搜索竞赛方法与结果，数据驱动方法获最高分，公布排行榜与代码链接


<details>
  <summary>Details</summary>
Motivation: 在多语言环境下识别用户查询与商品相关性，提升电商平台推荐性能

Method: 利用大语言模型及其在其他任务中的能力，采用数据驱动方法

Result: 在竞赛中相比其他方案获得最高分

Conclusion: 该数据驱动方法在多语言电商搜索任务中效果良好

Abstract: This report details our methodology and results developed for the
Multilingual E-commerce Search Competition. The problem aims to recognize
relevance between user queries versus product items in a multilingual context
and improve recommendation performance on e-commerce platforms. Utilizing Large
Language Models (LLMs) and their capabilities in other tasks, our data-centric
method achieved the highest score compared to other solutions during the
competition. Final leaderboard is publised at
https://alibaba-international-cikm2025.github.io. The source code for our
project is published at https://github.com/nhtlongcs/e-commerce-product-search.

</details>


### [61] [Generalized Pseudo-Relevance Feedback](https://arxiv.org/abs/2510.25488)
*Yiteng Tu,Weihang Su,Yujia Zhou,Yiqun Liu,Fen Lin,Qin Liu,Qingyao Ai*

Main category: cs.IR

TL;DR: 提出广义伪相关反馈（GPRF）框架用于查询重写，可减轻对假设的依赖，实验表明其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统伪相关反馈和基于大语言模型的生成相关反馈存在依赖假设和幻觉等问题，需改进查询重写方法。

Method: 引入假设松弛的GPRF框架，设计基于强化学习的面向效用的训练管道。

Result: 在多个基准和检索器上的实验表明GPRF始终优于强基线。

Conclusion: GPRF是一种有效且可推广的查询重写框架。

Abstract: Query rewriting is a fundamental technique in information retrieval (IR). It
typically employs the retrieval result as relevance feedback to refine the
query and thereby addresses the vocabulary mismatch between user queries and
relevant documents. Traditional pseudo-relevance feedback (PRF) and its
vector-based extension (VPRF) improve retrieval performance by leveraging
top-retrieved documents as relevance feedback. However, they are constructed
based on two major hypotheses: the relevance assumption (top documents are
relevant) and the model assumption (rewriting methods need to be designed
specifically for particular model architectures). While recent large language
models (LLMs)-based generative relevance feedback (GRF) enables model-free
query reformulation, it either suffers from severe LLM hallucination or, again,
relies on the relevance assumption to guarantee the effectiveness of rewriting
quality. To overcome these limitations, we introduce an assumption-relaxed
framework: \textit{Generalized Pseudo Relevance Feedback} (GPRF), which
performs model-free, natural language rewriting based on retrieved documents,
not only eliminating the model assumption but also reducing dependence on the
relevance assumption. Specifically, we design a utility-oriented training
pipeline with reinforcement learning to ensure robustness against noisy
feedback. Extensive experiments across multiple benchmarks and retrievers
demonstrate that GPRF consistently outperforms strong baselines, establishing
it as an effective and generalizable framework for query rewriting.

</details>


### [62] [MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for Semantic IDs Learning in Recommendation](https://arxiv.org/abs/2510.25622)
*Yi Xu,Moyu Zhang,Chaofan Fan,Jinxin Hu,Xiaochen Li,Yu Zhang,Xiaoyi Zeng,Jing Zhang*

Main category: cs.IR

TL;DR: 传统工业推荐系统用ItemIDs有可扩展性和泛化问题，纯基于内容的SIDs表达能力有限，现有方法有不足。提出MMQ - v2框架生成ADA - SID，有两项创新，实验证明其在推荐任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决现有工业推荐系统中ItemIDs在大数据集的扩展性和泛化问题，以及纯内容SIDs表达能力有限，现有结合行为信息方法存在噪声干扰和信号区分困难的问题。

Method: 提出MMQ - v2框架，生成ADA - SID，包括信息丰富度感知的自适应行为 - 内容对齐和动态行为路由器。

Result: 在公共和大规模工业数据集上的广泛实验表明，ADA - SID在生成式和判别式推荐任务中具有显著优势。

Conclusion: 所提出的MMQ - v2框架和ADA - SID能有效解决现有推荐系统存在的问题，提升推荐性能。

Abstract: Industrial recommender systems rely on unique Item Identifiers (ItemIDs).
However, this method struggles with scalability and generalization in large,
dynamic datasets that have sparse long-tail data.Content-based Semantic IDs
(SIDs) address this by sharing knowledge through content quantization. However,
by ignoring dynamic behavioral properties, purely content-based SIDs have
limited expressive power. Existing methods attempt to incorporate behavioral
information but overlook a critical distinction: unlike relatively uniform
content features, user-item interactions are highly skewed and diverse,
creating a vast information gap in quality and quantity between popular and
long-tail items. This oversight leads to two critical limitations: (1) Noise
Corruption: Indiscriminate behavior-content alignment allows collaborative
noise from long-tail items to corrupt their content representations, leading to
the loss of critical multimodal information. (2)Signal Obscurity: The
equal-weighting scheme for SIDs fails to reflect the varying importance of
different behavioral signals, making it difficult for downstream tasks to
distinguish important SIDs from uninformative ones. To tackle these issues, we
propose a mixture-of-quantization framework, MMQ-v2, to adaptively Align,
Denoise, and Amplify multimodal information from content and behavior
modalities for semantic IDs learning. The semantic IDs generated by this
framework named ADA-SID. It introduces two innovations: an adaptive
behavior-content alignment that is aware of information richness to shield
representations from noise, and a dynamic behavioral router to amplify critical
signals by applying different weights to SIDs. Extensive experiments on public
and large-scale industrial datasets demonstrate ADA-SID's significant
superiority in both generative and discriminative recommendation tasks.

</details>


### [63] [Retrieval-Augmented Search for Large-Scale Map Collections with ColPali](https://arxiv.org/abs/2510.25718)
*Jamie Mahowald,Benjamin Charles Germain Lee*

Main category: cs.IR

TL;DR: 介绍用于历史地图的检索增强搜索系统map - RAS及其公开演示，阐述其功能、潜在用例及未来工作。


<details>
  <summary>Details</summary>
Motivation: 利用多模态方法改进图书馆、档案馆和博物馆数字藏品（历史地图）的搜索和导航。

Method: 引入map - RAS框架，提供公开演示，用户可通过ColPali进行多模态查询、用Llama 3.2总结结果和上传自有藏品进行跨藏品搜索。

Result: 建立了可搜索国会图书馆101,233张地图图像的系统并公开演示，可进行多模态查询等操作。

Conclusion: 展示了map - RAS系统的潜力，提出了针对不同用户的潜在用例和未来在机器学习与数字人文领域的工作方向。

Abstract: Multimodal approaches have shown great promise for searching and navigating
digital collections held by libraries, archives, and museums. In this paper, we
introduce map-RAS: a retrieval-augmented search system for historic maps. In
addition to introducing our framework, we detail our publicly-hosted demo for
searching 101,233 map images held by the Library of Congress. With our system,
users can multimodally query the map collection via ColPali, summarize search
results using Llama 3.2, and upload their own collections to perform
inter-collection search. We articulate potential use cases for archivists,
curators, and end-users, as well as future work with our system in both machine
learning and the digital humanities. Our demo can be viewed at:
http://www.mapras.com.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [64] [Fortytwo: Swarm Inference with Peer-Ranked Consensus](https://arxiv.org/abs/2510.24801)
*Vladyslav Larin,Ihor Naumenko,Aleksei Ivashov,Ivan Nikitin,Alexander Firsov*

Main category: cs.LG

TL;DR: 提出Fortytwo协议用于AI推理，利用群体智能和分布式排序共识，在多个基准测试中表现出色，为去中心化AI系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 集中式AI达到计算上限且大规模训练回报递减，需水平扩展的推理层满足需求。

Method: 提出Fortytwo协议，采用群体推理，利用成对排序和自定义聚合模型，结合链上声誉，采用能力证明抵抗Sybil攻击。

Result: 群体推理在GPQA Diamond上比多数投票提高17.21个百分点；在六个基准测试中准确性高、抗攻击能力强，提示注入退化仅0.12%。

Conclusion: 为去中心化AI系统奠定基础，通过集体智能实现高质量推理的民主化，且不牺牲可靠性和安全性。

Abstract: As centralized AI hits compute ceilings and diminishing returns from
ever-larger training runs, meeting demand requires an inference layer that
scales horizontally in both capacity and capability. We present Fortytwo, a
novel protocol that leverages swarm intelligence principles and distributed
pairwise ranking consensus to achieve superior performance in AI inference. Our
approach reimagines collaboration among AI nodes using swarm inference: a
peer-ranked, reputation-weighted consensus across heterogeneous models that
surfaces the highest-quality responses. Using pairwise ranking with a custom
Bradley-Terry-style aggregation model, we demonstrate that swarm inference
substantially outperforms majority voting, achieving 85.90% on GPQA Diamond
versus 68.69% for majority voting with the same model set - an improvement of
+17.21 percentage points (approximately +25.1% relative). The protocol
incorporates on-chain reputation so node influence adapts to demonstrated
accuracy over time, yielding a meritocratic consensus that filters low-quality
or malicious participants. To resist Sybil attacks, Fortytwo employs
proof-of-capability in its consensus: nodes must successfully complete
calibration/test requests and stake reputation to enter ranking rounds, making
multi-identity attacks economically unattractive while preserving openness.
Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and
AIME, our evaluation indicates higher accuracy and strong resilience to
adversarial and noisy free-form prompting (e.g., prompt-injection degradation
of only 0.12% versus 6.20% for a monolithic single-model baseline), while
retaining practical deployability. Together, these results establish a
foundation for decentralized AI systems - democratizing access to high-quality
inference through collective intelligence without sacrificing reliability or
security.

</details>


### [65] [From Linear to Nonlinear: Provable Weak-to-Strong Generalization through Feature Learning](https://arxiv.org/abs/2510.24812)
*Junsoo Oh,Jerry Song,Chulhee Yun*

Main category: cs.LG

TL;DR: 本文对从线性CNN（弱模型）到两层ReLU CNN（强模型）的弱到强泛化进行形式化分析，识别出数据稀缺和数据丰富两种机制。


<details>
  <summary>Details</summary>
Motivation: 以往对弱到强泛化效应的理论见解多局限于抽象框架或线性/随机特征模型，本文旨在对此进行更具体分析。

Method: 考虑由不同难度的标签依赖信号和标签无关噪声组成的结构化数据，分析强模型在预训练弱模型标注数据上训练时的梯度下降动态。

Result: 基于数据集的信噪比特征识别出数据稀缺和数据丰富两种机制，揭示不同机制下弱到强泛化的情况。

Conclusion: 数据稀缺时泛化可能通过良性过拟合或因有害过拟合失败；数据丰富时早期通过标签校正实现泛化，但过度训练会降低性能。

Abstract: Weak-to-strong generalization refers to the phenomenon where a stronger model
trained under supervision from a weaker one can outperform its teacher. While
prior studies aim to explain this effect, most theoretical insights are limited
to abstract frameworks or linear/random feature models. In this paper, we
provide a formal analysis of weak-to-strong generalization from a linear CNN
(weak) to a two-layer ReLU CNN (strong). We consider structured data composed
of label-dependent signals of varying difficulty and label-independent noise,
and analyze gradient descent dynamics when the strong model is trained on data
labeled by the pretrained weak model. Our analysis identifies two regimes --
data-scarce and data-abundant -- based on the signal-to-noise characteristics
of the dataset, and reveals distinct mechanisms of weak-to-strong
generalization. In the data-scarce regime, generalization occurs via benign
overfitting or fails via harmful overfitting, depending on the amount of data,
and we characterize the transition boundary. In the data-abundant regime,
generalization emerges in the early phase through label correction, but we
observe that overtraining can subsequently degrade performance.

</details>


### [66] [Augmenting Biological Fitness Prediction Benchmarks with Landscapes Features from GraphFLA](https://arxiv.org/abs/2510.24826)
*Mingyu Huang,Shasha Zhou,Ke Li*

Main category: cs.LG

TL;DR: 介绍GraphFLA框架用于构建和分析适应度景观，展示其在评估模型性能的作用并发布相关数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏底层适应度景观地形信息，阻碍模型性能解释和比较，需有效评估机器学习模型。

Method: 引入GraphFLA框架，从多样诱变数据构建和分析适应度景观，计算20个生物相关特征。

Result: 应用GraphFLA分析超5300个景观，展示其解释和比较模型性能的作用，发布155个完整经验适应度景观。

Conclusion: GraphFLA有助于解释和比较模型性能，代码和数据集公开可获取。

Abstract: Machine learning models increasingly map biological sequence-fitness
landscapes to predict mutational effects. Effective evaluation of these models
requires benchmarks curated from empirical data. Despite their impressive
scales, existing benchmarks lack topographical information regarding the
underlying fitness landscapes, which hampers interpretation and comparison of
model performance beyond averaged scores. Here, we introduce GraphFLA, a Python
framework that constructs and analyzes fitness landscapes from mutagensis data
in diverse modalities (e.g., DNA, RNA, protein, and beyond) with up to millions
of mutants. GraphFLA calculates 20 biologically relevant features that
characterize 4 fundamental aspects of landscape topography. By applying
GraphFLA to over 5,300 landscapes from ProteinGym, RNAGym, and CIS-BP, we
demonstrate its utility in interpreting and comparing the performance of dozens
of fitness prediction models, highlighting factors influencing model accuracy
and respective advantages of different models. In addition, we release 155
combinatorially complete empirical fitness landscapes, encompassing over 2.2
million sequences across various modalities. All the codes and datasets are
available at https://github.com/COLA-Laboratory/GraphFLA.

</details>


### [67] [Send Less, Save More: Energy-Efficiency Benchmark of Embedded CNN Inference vs. Data Transmission in IoT](https://arxiv.org/abs/2510.24829)
*Benjamin Karic,Nina Herrmann,Jan Stenkamp,Paula Scharf,Fabian Gieseke,Angela Schwering*

Main category: cs.LG

TL;DR: 本文探讨物联网与人工智能结合用于环境监测，评估低功耗广域网和压缩CNN在ESP32 - S3上的应用，发现设备端推理可大幅降低能耗。


<details>
  <summary>Details</summary>
Motivation: 环境挑战日益严峻，需要有效远程监测方案，而设计环境监测物联网应用面临在能源有限地区创建节能设备的挑战。

Method: 评估常见低功耗广域网和在特定领域数据集上训练的压缩CNN在ESP32 - S3上的使用。

Result: 设备端执行CNN推理仅传输结果，相比发送原始图像数据，能耗最多降低至五分之一，模型压缩后精度仅下降几个百分点。

Conclusion: 应结合嵌入式机器学习开发低碳、能在环境监测场景自主运行的物联网应用。

Abstract: The integration of the Internet of Things (IoT) and Artificial Intelligence
offers significant opportunities to enhance our ability to monitor and address
ecological changes. As environmental challenges become increasingly pressing,
the need for effective remote monitoring solutions is more critical than ever.
A major challenge in designing IoT applications for environmental monitoring -
particularly those involving image data - is to create energy-efficient IoT
devices capable of long-term operation in remote areas with limited power
availability. Advancements in the field of Tiny Machine Learning allow the use
of Convolutional Neural Networks (CNNs) on resource-constrained,
battery-operated microcontrollers. Since data transfer is energy-intensive,
performing inference directly on microcontrollers to reduce the message size
can extend the operational lifespan of IoT nodes. This work evaluates the use
of common Low Power Wide Area Networks and compressed CNNs trained on domain
specific datasets on an ESP32-S3. Our experiments demonstrate, among other
things, that executing CNN inference on-device and transmitting only the
results reduces the overall energy consumption by a factor of up to five
compared to sending raw image data. %The compression of the model using Post
Training Quantization is accompanied by an acceptable reduction in accuracy of
only a few percentage points compared to a non-quantized model. These findings
advocate the development of IoT applications with reduced carbon footprint and
capable of operating autonomously in environmental monitoring scenarios by
incorporating Embedded Machine Learning.

</details>


### [68] [Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations](https://arxiv.org/abs/2510.24884)
*Olawale Salaudeen,Haoran Zhang,Kumail Alhamoud,Sara Beery,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 研究发现分布内与分布外准确率的正相关常是聚合异质分布外示例的假象，用OODSelect方法找到正相关不成立的子集，表明整体指标会掩盖分布外鲁棒性的重要失败模式。


<details>
  <summary>Details</summary>
Motivation: 探究分布内（ID）和分布外（OOD）准确率强正相关是否意味着虚假关联在实践中罕见。

Method: 使用基于梯度的简单方法OODSelect识别语义连贯的OOD子集。

Result: 在广泛使用的分布偏移基准测试中，OODSelect发现部分子集（有时超标准OOD集一半）中ID准确率高预示OOD准确率低。

Conclusion: 整体指标会掩盖OOD鲁棒性的重要失败模式，发布代码和识别出的子集以促进后续研究。

Abstract: Benchmarks for out-of-distribution (OOD) generalization frequently show a
strong positive correlation between in-distribution (ID) and OOD accuracy
across models, termed "accuracy-on-the-line." This pattern is often taken to
imply that spurious correlations - correlations that improve ID but reduce OOD
performance - are rare in practice. We find that this positive correlation is
often an artifact of aggregating heterogeneous OOD examples. Using a simple
gradient-based method, OODSelect, we identify semantically coherent OOD subsets
where accuracy on the line does not hold. Across widely used distribution shift
benchmarks, the OODSelect uncovers subsets, sometimes over half of the standard
OOD set, where higher ID accuracy predicts lower OOD accuracy. Our findings
indicate that aggregate metrics can obscure important failure modes of OOD
robustness. We release code and the identified subsets to facilitate further
research.

</details>


### [69] [Resource-Efficient and Robust Inference of Deep and Bayesian Neural Networks on Embedded and Analog Computing Platforms](https://arxiv.org/abs/2510.24951)
*Bernhard Klein*

Main category: cs.LG

TL;DR: 本文通过算法与硬件协同设计，提升传统和贝叶斯神经网络推理的资源效率和鲁棒性，为下一代机器学习系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习计算需求增长，在嵌入式和资源受限平台上的可扩展性和效率受约束，贝叶斯神经网络计算开销大，需提升资源效率和鲁棒性。

Method: 从算法和硬件效率两方面着手，算法上通过模型压缩和近似贝叶斯推理减少计算，硬件上优化数字加速器部署并探索模拟硬件；提出Galen进行自动层特定压缩，对模拟加速器建模并扩展噪声训练，开发解析和集成近似方法用于概率推理，引入概率光子计算。

Result: 展示了通过算法 - 硬件协同设计可共同提升效率和可靠性。

Conclusion: 算法 - 硬件协同设计为下一代可信、节能的机器学习系统奠定基础。

Abstract: While modern machine learning has transformed numerous application domains,
its growing computational demands increasingly constrain scalability and
efficiency, particularly on embedded and resource-limited platforms. In
practice, neural networks must not only operate efficiently but also provide
reliable predictions under distributional shifts or unseen data. Bayesian
neural networks offer a principled framework for quantifying uncertainty, yet
their computational overhead further compounds these challenges.
  This work advances resource-efficient and robust inference for both
conventional and Bayesian neural networks through the joint pursuit of
algorithmic and hardware efficiency. The former reduces computation through
model compression and approximate Bayesian inference, while the latter
optimizes deployment on digital accelerators and explores analog hardware,
bridging algorithmic design and physical realization. The first contribution,
Galen, performs automatic layer-specific compression guided by sensitivity
analysis and hardware-in-the-loop feedback. Analog accelerators offer
efficiency gains at the cost of noise; this work models device imperfections
and extends noisy training to nonstationary conditions, improving robustness
and stability. A second line of work advances probabilistic inference,
developing analytic and ensemble approximations that replace costly sampling,
integrate into a compiler stack, and optimize embedded inference. Finally,
probabilistic photonic computing introduces a paradigm where controlled analog
noise acts as an intrinsic entropy source, enabling fast, energy-efficient
probabilistic inference directly in hardware.
  Together, these studies demonstrate how efficiency and reliability can be
advanced jointly through algorithm-hardware co-design, laying the foundation
for the next generation of trustworthy, energy-efficient machine-learning
systems.

</details>


### [70] [Adaptive EEG-based stroke diagnosis with a GRU-TCN classifier and deep Q-learning thresholding](https://arxiv.org/abs/2510.24889)
*Shakeel Abdulkareem,Bora Yimenicioglu,Andrea Yang,Khartik Uppalapati,Aneesh Gudipati,Zhaoyang Fan*

Main category: cs.LG

TL;DR: 提出自适应多任务EEG分类器用于中风快速分诊，在数据集上测试表现良好，自适应阈值调整可优化性能。


<details>
  <summary>Details</summary>
Motivation: 快速分诊疑似中风需要准确、可床边部署的工具，EEG有前景但初诊时未充分利用。

Method: 提出自适应多任务EEG分类器，将32通道信号转换为功率谱密度特征，用GRU - TCN预测中风类型、半球侧化和严重程度，用DQN实时调整决策阈值。

Result: 基线GRU - TCN在中风类型、严重程度和侧化方面有较高准确率，DQN阈值调整后中风类型准确率提高到约98.0%，在独立低密度EEG队列测试鲁棒性。

Conclusion: 自适应阈值调整可使操作点转向临床偏好的灵敏度 - 特异性权衡，集成可视化支持可解释性。

Abstract: Rapid triage of suspected stroke needs accurate, bedside-deployable tools;
EEG is promising but underused at first contact. We present an adaptive
multitask EEG classifier that converts 32-channel signals to power spectral
density features (Welch), uses a recurrent-convolutional network (GRU-TCN) to
predict stroke type (healthy, ischemic, hemorrhagic), hemispheric
lateralization, and severity, and applies a deep Q-network (DQN) to tune
decision thresholds in real time. Using a patient-wise split of the UCLH Stroke
EIT/EEG data set (44 recordings; about 26 acute stroke, 10 controls), the
primary outcome was stroke-type performance; secondary outcomes were severity
and lateralization. The baseline GRU-TCN reached 89.3% accuracy (F1 92.8%) for
stroke type, about 96.9% (F1 95.9%) for severity, and about 96.7% (F1 97.4%)
for lateralization. With DQN threshold adaptation, stroke-type accuracy
increased to about 98.0% (F1 97.7%). We also tested robustness on an
independent, low-density EEG cohort (ZJU4H) and report paired patient-level
statistics. Analyses follow STARD 2015 guidance for diagnostic accuracy studies
(index test: GRU-TCN+DQN; reference standard: radiology/clinical diagnosis;
patient-wise evaluation). Adaptive thresholding shifts the operating point to
clinically preferred sensitivity-specificity trade-offs, while integrated
scalp-map and spectral visualizations support interpretability.

</details>


### [71] [Perturbation Bounds for Low-Rank Inverse Approximations under Noise](https://arxiv.org/abs/2510.25571)
*Phuc Tran,Nisheeth K. Vishnoi*

Main category: cs.LG

TL;DR: 研究含噪声对称矩阵低秩逆近似的谱范数误差，推导非渐近扰动界，结果优于经典方法且与实际误差吻合。


<details>
  <summary>Details</summary>
Motivation: 现实矩阵常含噪声，低秩逆近似的谱范数鲁棒性研究不足。

Method: 在对噪声的温和假设下，运用围道积分技术分析函数 $f(z) = 1/z$。

Result: 推导的非渐近扰动界优于经典全逆界，在多种矩阵上与实际误差吻合，经典结果高估误差。

Conclusion: 为含噪声计算环境中的低秩逆近似提供了实用的、考虑谱的保证。

Abstract: Low-rank pseudoinverses are widely used to approximate matrix inverses in
scalable machine learning, optimization, and scientific computing. However,
real-world matrices are often observed with noise, arising from sampling,
sketching, and quantization. The spectral-norm robustness of low-rank inverse
approximations remains poorly understood. We systematically study the
spectral-norm error $\| (\tilde{A}^{-1})_p - A_p^{-1} \|$ for an $n\times n$
symmetric matrix $A$, where $A_p^{-1}$ denotes the best rank-\(p\)
approximation of $A^{-1}$, and $\tilde{A} = A + E$ is a noisy observation.
Under mild assumptions on the noise, we derive sharp non-asymptotic
perturbation bounds that reveal how the error scales with the eigengap,
spectral decay, and noise alignment with low-curvature directions of $A$. Our
analysis introduces a novel application of contour integral techniques to the
\emph{non-entire} function $f(z) = 1/z$, yielding bounds that improve over
naive adaptations of classical full-inverse bounds by up to a factor of
$\sqrt{n}$. Empirically, our bounds closely track the true perturbation error
across a variety of real-world and synthetic matrices, while estimates based on
classical results tend to significantly overpredict. These findings offer
practical, spectrum-aware guarantees for low-rank inverse approximations in
noisy computational environments.

</details>


### [72] [Dynamically Weighted Momentum with Adaptive Step Sizes for Efficient Deep Network Training](https://arxiv.org/abs/2510.25042)
*Zhifeng Wang,Longlong Li,Chunyan Zeng*

Main category: cs.LG

TL;DR: 当前深度学习优化算法有不足，本文提出DWMGrad算法，实验证明其有更快收敛速度和更高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有优化算法如SGD和Adam在处理学习效率波动、复杂模型需求和非凸优化问题上能力不足，源于处理复杂数据结构和模型的局限性。

Method: 提出DWMGrad算法，在传统方法基础上引入依赖历史数据的动态引导机制，动态更新动量和学习率，灵活调整对历史信息的依赖。

Result: 经大量实验验证，DWMGrad在多种场景下能实现更快收敛速度和更高准确率。

Conclusion: DWMGrad算法能更好适应环境变化和任务复杂性，有良好优化效果。

Abstract: Within the current sphere of deep learning research, despite the extensive
application of optimization algorithms such as Stochastic Gradient Descent
(SGD) and Adaptive Moment Estimation (Adam), there remains a pronounced
inadequacy in their capability to address fluctuations in learning efficiency,
meet the demands of complex models, and tackle non-convex optimization issues.
These challenges primarily arise from the algorithms' limitations in handling
complex data structures and models, for instance, difficulties in selecting an
appropriate learning rate, avoiding local optima, and navigating through
high-dimensional spaces. To address these issues, this paper introduces a novel
optimization algorithm named DWMGrad. This algorithm, building on the
foundations of traditional methods, incorporates a dynamic guidance mechanism
reliant on historical data to dynamically update momentum and learning rates.
This allows the optimizer to flexibly adjust its reliance on historical
information, adapting to various training scenarios. This strategy not only
enables the optimizer to better adapt to changing environments and task
complexities but also, as validated through extensive experimentation,
demonstrates DWMGrad's ability to achieve faster convergence rates and higher
accuracies under a multitude of scenarios.

</details>


### [73] [Graph Network-based Structural Simulator: Graph Neural Networks for Structural Dynamics](https://arxiv.org/abs/2510.25683)
*Alessandro Lucchetti,Francesco Cadini,Marco Giglio,Luca Lomazzi*

Main category: cs.LG

TL;DR: 本文提出用于动态结构问题替代建模的GNN框架GNSS，评估显示其能准确复现物理问题且泛化能力强，相比有限元基线有推理加速优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究对GNN在结构问题尤其是动态问题中的应用关注不足，需填补这一空白。

Method: 引入GNSS框架，采用编码 - 处理 - 解码范式，具备用节点固定局部坐标系表达节点运动学、采用有符号回归损失、使用波长相关连接半径三个关键特征。

Result: 在案例研究中，GNSS能在数百个时间步长准确复现物理问题，泛化到未见加载条件，现有GNN无法收敛或给出有意义预测；相比显式有限元基线，GNSS有推理加速优势。

Conclusion: 具有物理一致更新规则的保局部性GNN是动态、波主导结构模拟的有竞争力替代方案。

Abstract: Graph Neural Networks (GNNs) have recently been explored as surrogate models
for numerical simulations. While their applications in computational fluid
dynamics have been investigated, little attention has been given to structural
problems, especially for dynamic cases. To address this gap, we introduce the
Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate
modeling of dynamic structural problems.
  GNSS follows the encode-process-decode paradigm typical of GNN-based machine
learning models, and its design makes it particularly suited for dynamic
simulations thanks to three key features: (i) expressing node kinematics in
node-fixed local frames, which avoids catastrophic cancellation in
finite-difference velocities; (ii) employing a sign-aware regression loss,
which reduces phase errors in long rollouts; and (iii) using a
wavelength-informed connectivity radius, which optimizes graph construction.
  We evaluate GNSS on a case study involving a beam excited by a 50kHz
Hanning-modulated pulse. The results show that GNSS accurately reproduces the
physics of the problem over hundreds of timesteps and generalizes to unseen
loading conditions, where existing GNNs fail to converge or deliver meaningful
predictions.
  Compared with explicit finite element baselines, GNSS achieves substantial
inference speedups while preserving spatial and temporal fidelity. These
findings demonstrate that locality-preserving GNNs with physics-consistent
update rules are a competitive alternative for dynamic, wave-dominated
structural simulations.

</details>


### [74] [Topic Analysis with Side Information: A Neural-Augmented LDA Approach](https://arxiv.org/abs/2510.24918)
*Biyi Fang,Kripa Rajshekhar,Truong Vo,Diego Klabjan*

Main category: cs.LG

TL;DR: 提出神经增强概率主题模型nnLDA，结合神经表示学习与概率主题建模，在多基准数据集上表现优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型如LDA难以整合辅助信息，限制了表达、个性化和可解释性。

Method: 提出nnLDA，通过神经先验机制动态整合辅助信息，用随机变分EM算法优化神经和概率组件。

Result: 在多个基准数据集上，nnLDA在主题连贯性、困惑度和下游分类任务上始终优于LDA和Dirichlet - Multinomial Regression。

Conclusion: 在有辅助信息的场景下，结合神经表示学习和概率主题建模有益。

Abstract: Traditional topic models such as Latent Dirichlet Allocation (LDA) have been
widely used to uncover latent structures in text corpora, but they often
struggle to integrate auxiliary information such as metadata, user attributes,
or document labels. These limitations restrict their expressiveness,
personalization, and interpretability. To address this, we propose nnLDA, a
neural-augmented probabilistic topic model that dynamically incorporates side
information through a neural prior mechanism. nnLDA models each document as a
mixture of latent topics, where the prior over topic proportions is generated
by a neural network conditioned on auxiliary features. This design allows the
model to capture complex nonlinear interactions between side information and
topic distributions that static Dirichlet priors cannot represent. We develop a
stochastic variational Expectation-Maximization algorithm to jointly optimize
the neural and probabilistic components. Across multiple benchmark datasets,
nnLDA consistently outperforms LDA and Dirichlet-Multinomial Regression in
topic coherence, perplexity, and downstream classification. These results
highlight the benefits of combining neural representation learning with
probabilistic topic modeling in settings where side information is available.

</details>


### [75] [Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy](https://arxiv.org/abs/2510.25670)
*Phuc Tran,Nisheeth K. Vishnoi,Van H. Vu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A central challenge in machine learning is to understand how noise or
measurement errors affect low-rank approximations, particularly in the spectral
norm. This question is especially important in differentially private low-rank
approximation, where one aims to preserve the top-$p$ structure of a
data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius
norm error or changes in reconstruction quality, but these metrics can over- or
under-estimate true subspace distortion. The spectral norm, by contrast,
captures worst-case directional error and provides the strongest utility
guarantees. We establish new high-probability spectral-norm perturbation bounds
for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem
and explicitly capture interactions between a matrix $A \in \mathbb{R}^{n
\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and
norm conditions, our bounds yield sharp estimates for $\|(A + E)_p - A_p\|$,
where $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up
to a factor of $\sqrt{n}$. As an application, we derive improved utility
guarantees for differentially private PCA, resolving an open problem in the
literature. Our analysis relies on a novel contour bootstrapping method from
complex analysis and extends it to a broad class of spectral functionals,
including polynomials and matrix exponentials. Empirical results on real-world
datasets confirm that our bounds closely track the actual spectral error under
diverse perturbation regimes.

</details>


### [76] [Position: Biology is the Challenge Physics-Informed ML Needs to Evolve](https://arxiv.org/abs/2510.25368)
*Julien Martinelli*

Main category: cs.LG

TL;DR: 本文提出生物学信息机器学习（BIML），它是物理信息机器学习（PIML）的扩展，以应对生物学建模挑战，并给出过渡的四个支柱和构建生态系统的建议。


<details>
  <summary>Details</summary>
Motivation: PIML在物理领域成功，生物学领域有独特挑战，希望将PIML方法扩展到生物学领域。

Method: 提出BIML，它调整PIML方法以适应生物学先验知识的概率形式，还给出不确定性量化、情境化、约束潜在结构推断和可扩展性四个过渡支柱。

Result: 提出了BIML概念和过渡的四个支柱。

Conclusion: 给出构建BIML生态系统的具体建议，引导PIML创新应对生物学挑战。

Abstract: Physics-Informed Machine Learning (PIML) has successfully integrated
mechanistic understanding into machine learning, particularly in domains
governed by well-known physical laws. This success has motivated efforts to
apply PIML to biology, a field rich in dynamical systems but shaped by
different constraints. Biological modeling, however, presents unique
challenges: multi-faceted and uncertain prior knowledge, heterogeneous and
noisy data, partial observability, and complex, high-dimensional networks. In
this position paper, we argue that these challenges should not be seen as
obstacles to PIML, but as catalysts for its evolution. We propose
Biology-Informed Machine Learning (BIML): a principled extension of PIML that
retains its structural grounding while adapting to the practical realities of
biology. Rather than replacing PIML, BIML retools its methods to operate under
softer, probabilistic forms of prior knowledge. We outline four foundational
pillars as a roadmap for this transition: uncertainty quantification,
contextualization, constrained latent structure inference, and scalability.
Foundation Models and Large Language Models will be key enablers, bridging
human expertise with computational modeling. We conclude with concrete
recommendations to build the BIML ecosystem and channel PIML-inspired
innovation toward challenges of high scientific and societal relevance.

</details>


### [77] [KAN-GCN: Combining Kolmogorov-Arnold Network with Graph Convolution Network for an Accurate Ice Sheet Emulator](https://arxiv.org/abs/2510.24926)
*Zesheng Liu,YoungHyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: 介绍KAN - GCN用于冰盖建模，性能优于基线模型，在准确性和效率上有良好权衡。


<details>
  <summary>Details</summary>
Motivation: 改进数值冰盖模型模拟器的性能。

Method: 将Kolmogorov - Arnold Network (KAN)作为特征校准器置于图卷积网络(GCNs)前，构建KAN - GCN架构，并使用36个融化率模拟和3种网格尺寸设置进行训练和测试。

Result: 在2 - 5层架构中，KAN - GCN的准确性与或超过纯GCN和MLP - GCN基线；在较粗网格上提高推理吞吐量，仅在最细网格上有适度成本。

Conclusion: KAN优先设计在大型瞬态场景扫描中在准确性和效率之间提供了有利的权衡。

Abstract: We introduce KAN-GCN, a fast and accurate emulator for ice sheet modeling
that places a Kolmogorov-Arnold Network (KAN) as a feature-wise calibrator
before graph convolution networks (GCNs). The KAN front end applies learnable
one-dimensional warps and a linear mixing step, improving feature conditioning
and nonlinear encoding without increasing message-passing depth. We employ this
architecture to improve the performance of emulators for numerical ice sheet
models. Our emulator is trained and tested using 36 melting-rate simulations
with 3 mesh-size settings for Pine Island Glacier, Antarctica. Across 2- to
5-layer architectures, KAN-GCN matches or exceeds the accuracy of pure GCN and
MLP-GCN baselines. Despite a small parameter overhead, KAN-GCN improves
inference throughput on coarser meshes by replacing one edge-wise
message-passing layer with a node-wise transform; only the finest mesh shows a
modest cost. Overall, KAN-first designs offer a favorable accuracy vs.
efficiency trade-off for large transient scenario sweeps.

</details>


### [78] [WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for Inductive Learning](https://arxiv.org/abs/2510.24927)
*Joel Frank Huarayo Quispe,Lilian Berton,Didier Vega-Oliveros*

Main category: cs.LG

TL;DR: 提出WBT - BGRL非对比框架用于二分图归纳链接预测，在真实数据集上有竞争力，凸显加权非对比学习价值。


<details>
  <summary>Details</summary>
Motivation: 二分图链接预测研究较少，现有对比方法负采样低效有偏差，非对比方法仅依赖正样本，现有模型在归纳、加权和二分图场景效果待验证。

Method: 提出WBT - BGRL非对比框架，用二分架构和双GCN编码器，在三元组损失中引入新的加权机制。

Result: 在Industry和E - commerce真实数据集上与T - BGRL等模型对比，有竞争力，预训练时加权效果更佳。

Conclusion: 加权非对比学习对二分图归纳链接预测有价值。

Abstract: Link prediction in bipartite graphs is crucial for applications like
recommendation systems and failure detection, yet it is less studied than in
monopartite graphs. Contrastive methods struggle with inefficient and biased
negative sampling, while non-contrastive approaches rely solely on positive
samples. Existing models perform well in transductive settings, but their
effectiveness in inductive, weighted, and bipartite scenarios remains untested.
To address this, we propose Weighted Bipartite Triplet-Bootstrapped Graph
Latents (WBT-BGRL), a non-contrastive framework that enhances bootstrapped
learning with a novel weighting mechanism in the triplet loss. Using a
bipartite architecture with dual GCN encoders, WBT-BGRL is evaluated against
adapted state-of-the-art models (T-BGRL, BGRL, GBT, CCA-SSG). Results on
real-world datasets (Industry and E-commerce) show competitive performance,
especially when weighting is applied during pretraining-highlighting the value
of weighted, non-contrastive learning for inductive link prediction in
bipartite graphs.

</details>


### [79] [Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought](https://arxiv.org/abs/2510.24941)
*Jiachen Zhao,Yiyou Sun,Weiyan Shi,Dawn Song*

Main category: cs.LG

TL;DR: 研究发现大语言模型（LLMs）链式思维（CoT）推理步骤多无实质作用，提出真思维分数（TTS）衡量因果影响，还找到真思维方向，指出自验证步骤也可能是装饰性的，影响推理效率和可信度。


<details>
  <summary>Details</summary>
Motivation: 探究CoT推理步骤是否真实反映模型内部思维过程，以及是否可用于监测不安全意图。

Method: 提出真思维分数（TTS）衡量推理步骤对模型最终预测的因果影响，在模型潜在空间中确定真思维方向。

Result: 发现LLMs推理步骤常包含装饰性步骤，只有少量步骤有高TTS；确定真思维方向可控制模型是否执行某些步骤；自验证步骤也可能是装饰性的。

Conclusion: LLMs常空谈推理步骤而未实际执行，损害推理效率和CoT的可信度。

Abstract: Recent large language models (LLMs) can generate long Chain-of-Thought (CoT)
at test time, enabling them to solve complex tasks. These reasoning steps in
CoT are often assumed as a faithful reflection of the model's internal thinking
process, and used to monitor unsafe intentions. However, we find many reasoning
steps don't truly contribute to LLMs' prediction. We measure the step-wise
causal influence of each reasoning step on the model's final prediction with a
proposed True Thinking Score (TTS). We reveal that LLMs often interleave
between true-thinking steps (which are genuinely used to produce the final
output) and decorative-thinking steps (which only give the appearance of
reasoning but have minimal causal impact). Notably, only a small subset of the
total reasoning steps have a high TTS that causally drive the model's
prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning
steps in CoT have a TTS >= 0.7 (range: 0-1) under the Qwen-2.5 model.
Furthermore, we identify a TrueThinking direction in the latent space of LLMs.
By steering along or against this direction, we can force the model to perform
or disregard certain CoT steps when computing the final result. Finally, we
highlight that self-verification steps in CoT (i.e., aha moments) can also be
decorative, where LLMs do not truly verify their solution. Steering along the
TrueThinking direction can force internal reasoning over these steps, resulting
in a change in the final results. Overall, our work reveals that LLMs often
verbalize reasoning steps without actually performing them internally, which
undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.

</details>


### [80] [Localized Kernel Projection Outlyingness: A Two-Stage Approach for Multi-Modal Outlier Detection](https://arxiv.org/abs/2510.24043)
*Akira Tamamori*

Main category: cs.LG

TL;DR: 提出Two - Stage LKPLO多阶段异常检测框架，结合三个关键概念，在10个基准数据集实验中表现出色，证实核化和局部化阶段结合的重要性。


<details>
  <summary>Details</summary>
Motivation: 克服传统基于投影方法依赖固定统计指标和单一数据结构假设的局限性。

Method: 提出Two - Stage LKPLO框架，综合广义基于损失的离群度度量、全局核PCA阶段线性化非线性数据结构、局部聚类阶段处理多模态分布。

Result: 在10个基准数据集上通过5折交叉验证实验，Two - Stage LKPLO达到了最先进性能，在多簇数据和复杂高维数据上显著优于强基线。消融研究证实核化和局部化阶段结合不可或缺。

Conclusion: 为一类重要的异常检测问题提供有力新工具，强调混合多阶段架构的重要性。

Abstract: This paper presents Two-Stage LKPLO, a novel multi-stage outlier detection
framework that overcomes the coexisting limitations of conventional
projection-based methods: their reliance on a fixed statistical metric and
their assumption of a single data structure. Our framework uniquely synthesizes
three key concepts: (1) a generalized loss-based outlyingness measure (PLO)
that replaces the fixed metric with flexible, adaptive loss functions like our
proposed SVM-like loss; (2) a global kernel PCA stage to linearize non-linear
data structures; and (3) a subsequent local clustering stage to handle
multi-modal distributions. Comprehensive 5-fold cross-validation experiments on
10 benchmark datasets, with automated hyperparameter optimization, demonstrate
that Two-Stage LKPLO achieves state-of-the-art performance. It significantly
outperforms strong baselines on datasets with challenging structures where
existing methods fail, most notably on multi-cluster data (Optdigits) and
complex, high-dimensional data (Arrhythmia). Furthermore, an ablation study
empirically confirms that the synergistic combination of both the kernelization
and localization stages is indispensable for its superior performance. This
work contributes a powerful new tool for a significant class of outlier
detection problems and underscores the importance of hybrid, multi-stage
architectures.

</details>


### [81] [Finding Culture-Sensitive Neurons in Vision-Language Models](https://arxiv.org/abs/2510.24942)
*Xiutian Zhao,Rochelle Choenni,Rohit Saxena,Ivan Titov*

Main category: cs.LG

TL;DR: 研究视觉语言模型处理文化信息的机制，识别文化敏感神经元，提出新选择器，揭示神经元分布。


<details>
  <summary>Details</summary>
Motivation: 理解视觉语言模型如何处理文化相关信息，解决其在文化情境输入上的困难。

Method: 使用CVQA基准识别文化选择性神经元，对不同方法标记的神经元进行失活因果测试，提出新的基于边界的选择器CAS。

Result: 实验证明存在对特定文化问题有重要影响的神经元，CAS方法在识别文化敏感神经元上表现更优，此类神经元倾向于聚集在某些解码器层。

Conclusion: 研究结果为多模态表征的内部组织提供了新见解。

Abstract: Despite their impressive performance, vision-language models (VLMs) still
struggle on culturally situated inputs. To understand how VLMs process
culturally grounded information, we study the presence of culture-sensitive
neurons, i.e. neurons whose activations show preferential sensitivity to inputs
associated with particular cultural contexts. We examine whether such neurons
are important for culturally diverse visual question answering and where they
are located. Using the CVQA benchmark, we identify neurons of culture
selectivity and perform causal tests by deactivating the neurons flagged by
different identification methods. Experiments on three VLMs across 25 cultural
groups demonstrate the existence of neurons whose ablation disproportionately
harms performance on questions about the corresponding cultures, while having
minimal effects on others. Moreover, we propose a new margin-based selector -
Contrastive Activation Selection (CAS), and show that it outperforms existing
probability- and entropy-based methods in identifying culture-sensitive
neurons. Finally, our layer-wise analyses reveals that such neurons tend to
cluster in certain decoder layers. Overall, our findings shed new light on the
internal organization of multimodal representations.

</details>


### [82] [Machine Learning and CPU (Central Processing Unit) Scheduling Co-Optimization over a Network of Computing Centers](https://arxiv.org/abs/2510.25176)
*Mohammadreza Doostmohammadian,Zulfiya R. Gabidullina,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: 本文研究分布式机器学习和优化中的计算资源优化问题，提出算法，能优化数据处理和资源分配，在一些应用中证明收敛性，相比现有方案成本最优差距改善超50%。


<details>
  <summary>Details</summary>
Motivation: 在人工智能研究快速发展中，对快速、高效和可扩展解决方案需求增加，需优化分布式机器学习的计算资源。

Method: 将问题建模为协同优化问题，优化数据处理和资源分配，利用信息共享网络，考虑时间变化和平衡权重，允许信息通道的对数尺度量化，用扰动理论、Lyapunov稳定性和特征谱分析证明收敛性。

Result: 提出的算法在所有迭代中满足计算资源需求平衡约束，对于分布式支持向量机和回归等应用能证明收敛到最优情况，相比现有CPU调度方案，成本最优差距改善超50%。

Conclusion: 所提出的算法在优化分布式机器学习计算资源方面有效，相比现有方案有显著优势。

Abstract: In the rapidly evolving research on artificial intelligence (AI) the demand
for fast, computationally efficient, and scalable solutions has increased in
recent years. The problem of optimizing the computing resources for distributed
machine learning (ML) and optimization is considered in this paper. Given a set
of data distributed over a network of computing-nodes/servers, the idea is to
optimally assign the CPU (central processing unit) usage while simultaneously
training each computing node locally via its own share of data. This formulates
the problem as a co-optimization setup to (i) optimize the data processing and
(ii) optimally allocate the computing resources. The information-sharing
network among the nodes might be time-varying, but with balanced weights to
ensure consensus-type convergence of the algorithm. The algorithm is all-time
feasible, which implies that the computing resource-demand balance constraint
holds at all iterations of the proposed solution. Moreover, the solution allows
addressing possible log-scale quantization over the information-sharing
channels to exchange log-quantized data. For some example applications,
distributed support-vector-machine (SVM) and regression are considered as the
ML training models. Results from perturbation theory, along with Lyapunov
stability and eigen-spectrum analysis, are used to prove the convergence
towards the optimal case. As compared to existing CPU scheduling solutions, the
proposed algorithm improves the cost optimality gap by more than $50\%$.

</details>


### [83] [Sequences of Logits Reveal the Low Rank Structure of Language Models](https://arxiv.org/abs/2510.24966)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 提出模型无关方法研究大语言模型低维结构，实证其低秩特性可用于生成，有理论抽象及学习保证


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型固有的低维结构

Method: 将语言模型作为顺序概率模型，在模型无关层面研究其低维结构，实证模型低秩特性并用于生成，进行理论抽象与分析

Result: 多种现代语言模型呈现低秩结构，可利用此结构进行生成，理论抽象预测与实验相符

Conclusion: 该方法可有效研究大语言模型低维结构，理论抽象有一定表示能力和学习保证

Abstract: A major problem in the study of large language models is to understand their
inherent low-dimensional structure. We introduce an approach to study the
low-dimensional structure of language models at a model-agnostic level: as
sequential probabilistic models. We first empirically demonstrate that a wide
range of modern language models exhibit low-rank structure: in particular,
matrices built from the model's logits for varying sets of prompts and
responses have low approximate rank. We then show that this low-rank structure
can be leveraged for generation -- in particular, we can generate a response to
a target prompt using a linear combination of the model's outputs on unrelated,
or even nonsensical prompts.
  On the theoretical front, we observe that studying the approximate rank of
language models in the sense discussed above yields a simple universal
abstraction whose theoretical predictions parallel our experiments. We then
analyze the representation power of the abstraction and give provable learning
guarantees.

</details>


### [84] [Continual Low-Rank Adapters for LLM-based Generative Recommender Systems](https://arxiv.org/abs/2510.25093)
*Hyunsik Yoo,Ting-Wei Li,SeongKu Kang,Zhining Liu,Charlie Xu,Qilin Qi,Hanghang Tong*

Main category: cs.LG

TL;DR: 现有基于LoRA的持续学习方法在推荐任务存在不足，本文提出PESO方法，理论证明其合理性，实证表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推荐的持续学习中面临挑战，现有基于LoRA的持续方法忽视推荐任务特性，无法应对用户偏好变化。

Method: 提出PESO方法，引入近端正则化器，将当前适配器锚定到最近的冻结状态，使模型灵活平衡适应和保留。

Result: 理论上证明近端设计在LoRA子空间提供数据感知、方向导向的指导；实证上，PESO始终优于现有基于LoRA的持续学习方法。

Conclusion: PESO方法能更好地捕捉用户近期行为，在推荐的持续学习中表现更优。

Abstract: While large language models (LLMs) achieve strong performance in
recommendation, they face challenges in continual learning as users, items, and
user preferences evolve over time. Existing LoRA-based continual methods
primarily focus on preserving performance on previous tasks, but this overlooks
the unique nature of recommendation: the goal is not to predict past
preferences, and outdated preferences can even harm performance when current
interests shift significantly. To address this, we propose PESO (Proximally
rEgularized Single evolving lOra, a continual adaptation method for LoRA in
recommendation. PESO introduces a proximal regularizer that anchors the current
adapter to its most recent frozen state, enabling the model to flexibly balance
adaptation and preservation, and to better capture recent user behaviors.
Theoretically, we show that this proximal design provides data-aware,
direction-wise guidance in the LoRA subspace. Empirically, PESO consistently
outperforms existing LoRA-based continual learning methods.

</details>


### [85] [Conformational Rank Conditioned Committees for Machine Learning-Assisted Directed Evolution](https://arxiv.org/abs/2510.24974)
*Mia Adler,Carrie Liang,Brian Peng,Oleg Presnyakov,Justin M. Baker,Jannelle Lauffer,Himani Sharma,Barry Merriman*

Main category: cs.LG

TL;DR: 提出RCC框架用于抗体定向进化，在SARS - CoV - 2抗体对接验证中优于基线策略，为治疗性抗体发现提供可扩展途径。


<details>
  <summary>Details</summary>
Motivation: 现有结构感知的机器学习辅助定向进化（MLDE）管道存在局限性，难以区分构象不确定性和认知不确定性。

Method: 引入基于排名条件的委员会（RCC）框架，利用排名构象为每个排名分配深度神经网络委员会。

Result: 在SARS - CoV - 2抗体对接验证中，相比基线策略有显著改进。

Conclusion: 该方法为治疗性抗体发现提供可扩展途径，能直接应对构象不确定性建模挑战。

Abstract: Machine Learning-assisted directed evolution (MLDE) is a powerful tool for
efficiently navigating antibody fitness landscapes. Many structure-aware MLDE
pipelines rely on a single conformation or a single committee across all
conformations, limiting their ability to separate conformational uncertainty
from epistemic uncertainty. Here, we introduce a rank -conditioned committee
(RCC) framework that leverages ranked conformations to assign a deep neural
network committee per rank. This design enables a principled separation between
epistemic uncertainty and conformational uncertainty. We validate our approach
on SARS-CoV-2 antibody docking, demonstrating significant improvements over
baseline strategies. Our results offer a scalable route for therapeutic
antibody discovery while directly addressing the challenge of modeling
conformational uncertainty.

</details>


### [86] [Strategic inputs: feature selection from game-theoretic perspective](https://arxiv.org/abs/2510.24982)
*Chi Zhao,Jing Liu,Elena Parilina*

Main category: cs.LG

TL;DR: 本文提出基于博弈论的表格数据特征选择框架，可降计算成本且保预测性能，代码开源。


<details>
  <summary>Details</summary>
Motivation: 数据量增长使机器学习模型训练计算成本上升，很多特征对模型性能无积极贡献却消耗资源。

Method: 基于合作博弈构建特征选择流程，将特征建模为玩家，通过评估协同交互和边际贡献确定重要性，框架含样本选择、博弈论特征重要性评估、冗余特征消除和优化模型训练四个核心组件。

Result: 该方法大幅降低计算量，同时保留预测性能。

Conclusion: 该框架为大规模机器学习计算挑战提供有效解决方案。

Abstract: The exponential growth of data volumes has led to escalating computational
costs in machine learning model training. However, many features fail to
contribute positively to model performance while consuming substantial
computational resources. This paper presents an end-to-end feature selection
framework for tabular data based on game theory. We formulate feature selection
procedure based on a cooperative game where features are modeled as players,
and their importance is determined through the evaluation of synergistic
interactions and marginal contributions. The proposed framework comprises four
core components: sample selection, game-theoretic feature importance
evaluation, redundant feature elimination, and optimized model training.
Experimental results demonstrate that the proposed method achieves substantial
computation reduction while preserving predictive performance, thereby offering
an efficient solution of the computational challenges of large-scale machine
learning. The source code is available at
https://github.com/vectorsss/strategy_inputs.

</details>


### [87] [LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies](https://arxiv.org/abs/2510.24983)
*Ximan Sun,Xiang Cheng*

Main category: cs.LG

TL;DR: 提出LRT - Diffusion，为离线强化学习的扩散策略增加风险控制，在D4RL MuJoCo任务上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略在采样时的启发式方法缺乏风险统计概念，需要风险感知的采样规则。

Method: 将每个去噪步骤视为顺序假设检验，积累对数似然比，用逻辑控制器控制条件均值，LRT引导可与Q梯度结合。

Result: 在D4RL MuJoCo任务上，LRT - Diffusion改善了回报与分布外表现的权衡，理论上有水平α校准等性质。

Conclusion: LRT - Diffusion是一种推理时方法，可为离线强化学习的扩散策略增加原则性、校准后的风险控制。

Abstract: Diffusion policies are competitive for offline reinforcement learning (RL)
but are typically guided at sampling time by heuristics that lack a statistical
notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that
treats each denoising step as a sequential hypothesis test between the
unconditional prior and the state-conditional policy head. Concretely, we
accumulate a log-likelihood ratio and gate the conditional mean with a logistic
controller whose threshold tau is calibrated once under H0 to meet a
user-specified Type-I level alpha. This turns guidance from a fixed push into
an evidence-driven adjustment with a user-interpretable risk budget.
Importantly, we deliberately leave training vanilla (two heads with standard
epsilon-prediction) under the structure of DDPM. LRT guidance composes
naturally with Q-gradients: critic-gradient updates can be taken at the
unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum
from exploitation to conservatism. We standardize states and actions
consistently at train and test time and report a state-conditional
out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks,
LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines
in our implementation while honoring the desired alpha. Theoretically, we
establish level-alpha calibration, concise stability bounds, and a return
comparison showing when LRT surpasses Q-guidance-especially when off-support
errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method
that adds principled, calibrated risk control to diffusion policies for offline
RL.

</details>


### [88] [Shift is Good: Mismatched Data Mixing Improves Test Performance](https://arxiv.org/abs/2510.25108)
*Marko Medvedev,Kaifeng Lyu,Zhiyuan Li,Nathan Srebro*

Main category: cs.LG

TL;DR: 研究不同训练和测试比例的混合分布训练与测试，表明分布偏移有益并找出最优训练比例。


<details>
  <summary>Details</summary>
Motivation: 探究不同训练和测试比例的混合分布下分布偏移对测试性能的影响。

Method: 在多种场景下进行分析，确定最优训练比例和分布偏移的有益程度。

Result: 发现分布偏移在很多情况下有益，即使组件不相关且无转移，还确定了最优训练比例。

Conclusion: 分析适用于组件‘技能’分布不同的组合场景。

Abstract: We consider training and testing on mixture distributions with different
training and test proportions. We show that in many settings, and in some sense
generically, distribution shift can be beneficial, and test performance can
improve due to mismatched training proportions, even if the components are
unrelated and with no transfer between components. In a variety of scenarios,
we identify the optimal training proportions and the extent to which such
distribution shift can be beneficial. We show how the same analysis applies
also to a compositional setting with differing distribution of component
"skills'' at training and test.

</details>


### [89] [Epileptic Seizure Detection and Prediction from EEG Data: A Machine Learning Approach with Clinical Validation](https://arxiv.org/abs/2510.24986)
*Ria Jayanti,Tanish Jain*

Main category: cs.LG

TL;DR: 本文提出结合实时癫痫发作检测与预测的新方法，用CHB - MIT数据库评估，检测用多种监督学习算法，预测用LSTM网络，结果显示有开发实时监测工具的潜力，能从被动管理转向主动预防。


<details>
  <summary>Details</summary>
Motivation: 传统癫痫发作检测方法只能在发作后识别，限制了早期干预和主动治疗机会，因此需结合检测与预测。

Method: 检测采用K近邻、逻辑回归、随机森林和支持向量机等监督学习算法；预测采用长短期记忆（LSTM）网络。使用CHB - MIT头皮脑电图数据库评估。

Result: 逻辑回归检测准确率90.9%，召回率89.6%；随机森林和支持向量机准确率94.0%，召回率0%；LSTM预测准确率89.26%。

Conclusion: 开发实时监测工具不仅能检测癫痫发作，还能预测发作，从被动管理转向主动预防，让患者可提前采取预防措施。

Abstract: In recent years, machine learning has become an increasingly powerful tool
for supporting seizure detection and monitoring in epilepsy care. Traditional
approaches focus on identifying seizures only after they begin, which limits
the opportunity for early intervention and proactive treatment. In this study,
we propose a novel approach that integrates both real-time seizure detection
and prediction, aiming to capture subtle temporal patterns in EEG data that may
indicate an upcoming seizure. Our approach was evaluated using the CHB-MIT
Scalp EEG Database, which includes 969 hours of recordings and 173 seizures
collected from 23 pediatric and young adult patients with drug-resistant
epilepsy. To support seizure detection, we implemented a range of supervised
machine learning algorithms, including K-Nearest Neighbors, Logistic
Regression, Random Forest, and Support Vector Machine. The Logistic Regression
achieved 90.9% detection accuracy with 89.6% recall, demonstrating balanced
performance suitable for clinical screening. Random Forest and Support Vector
Machine models achieved higher accuracy (94.0%) but with 0% recall, failing to
detect any seizures, illustrating that accuracy alone is insufficient for
evaluating medical ML models with class imbalance. For seizure prediction, we
employed Long Short-Term Memory (LSTM) networks, which use deep learning to
model temporal dependencies in EEG data. The LSTM model achieved 89.26%
prediction accuracy. These results highlight the potential of developing
accessible, real-time monitoring tools that not only detect seizures as
traditionally done, but also predict them before they occur. This ability to
predict seizures marks a significant shift from reactive seizure management to
a more proactive approach, allowing patients to anticipate seizures and take
precautionary measures to reduce the risk of injury or other complications.

</details>


### [90] [Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series](https://arxiv.org/abs/2510.24988)
*Hemanath Arumugam,Falong Fan,Bo Liu*

Main category: cs.LG

TL;DR: 本文提出将基于Transformer的自监督变点检测（CPD）模块集成到Option - Critic框架，实现状态轨迹自适应分割和选项发现，实验证明该方法在复杂环境中有效。


<details>
  <summary>Details</summary>
Motivation: 解决分层强化学习（HRL）在实际应用中自主发现有语义意义的子目标和学习最优选项终止边界的挑战。

Method: 将CPD模块集成到Option - Critic框架，用启发式伪标签训练CPD模块，利用推断的变点稳定终止函数梯度、预训练选项内策略和加强功能专业化，用结构感知辅助损失增强标准演员 - 评论家损失。

Result: 在Four - Rooms和Pinball任务实验中，CPD引导的智能体收敛更快、累积回报更高、选项专业化显著提升。

Conclusion: 通过变点分割集成结构先验可在复杂环境中得到更具可解释性、样本高效和鲁棒的分层策略。

Abstract: Hierarchical Reinforcement Learning (HRL) enhances the scalability of
decision-making in long-horizon tasks by introducing temporal abstraction
through options-policies that span multiple timesteps. Despite its theoretical
appeal, the practical implementation of HRL suffers from the challenge of
autonomously discovering semantically meaningful subgoals and learning optimal
option termination boundaries. This paper introduces a novel architecture that
integrates a self-supervised, Transformer-based Change Point Detection (CPD)
module into the Option-Critic framework, enabling adaptive segmentation of
state trajectories and the discovery of options. The CPD module is trained
using heuristic pseudo-labels derived from intrinsic signals to infer latent
shifts in environment dynamics without external supervision. These inferred
change-points are leveraged in three critical ways: (i) to serve as supervisory
signals for stabilizing termination function gradients, (ii) to pretrain
intra-option policies via segment-wise behavioral cloning, and (iii) to enforce
functional specialization through inter-option divergence penalties over
CPD-defined state partitions. The overall optimization objective enhances the
standard actor-critic loss using structure-aware auxiliary losses. In our
framework, option discovery arises naturally as CPD-defined trajectory segments
are mapped to distinct intra-option policies, enabling the agent to
autonomously partition its behavior into reusable, semantically meaningful
skills. Experiments on the Four-Rooms and Pinball tasks demonstrate that
CPD-guided agents exhibit accelerated convergence, higher cumulative returns,
and significantly improved option specialization. These findings confirm that
integrating structural priors via change-point segmentation leads to more
interpretable, sample-efficient, and robust hierarchical policies in complex
environments.

</details>


### [91] [An Analysis of Causal Effect Estimation using Outcome Invariant Data Augmentation](https://arxiv.org/abs/2510.25128)
*Uzair Akbar,Niki Kilbertus,Hao Shen,Krikamol Muandet,Bo Dai*

Main category: cs.LG

TL;DR: 提出统一框架，结合数据增强与因果推断，用于跨干预泛化，引入类工具变量回归，理论和实验证明其可提升因果估计和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 工具变量在许多应用中不如数据增强容易获取，希望利用数据增强解决隐藏混杂因素导致的因果效应估计偏差问题。

Method: 提出统一框架，将参数化数据增强视为类工具变量回归问题，对基于工具变量的估计器进行适当正则化。

Result: 理论上证明了总体情况，通过简单线性示例的模拟实验证明了有限样本情况，还进行了真实数据实验。

Conclusion: 数据增强结合因果推断，采用类工具变量回归可在跨干预时减少因果效应估计偏差，提升预测性能和泛化能力。

Abstract: The technique of data augmentation (DA) is often used in machine learning for
regularization purposes to better generalize under i.i.d. settings. In this
work, we present a unifying framework with topics in causal inference to make a
case for the use of DA beyond just the i.i.d. setting, but for generalization
across interventions as well. Specifically, we argue that when the outcome
generating mechanism is invariant to our choice of DA, then such augmentations
can effectively be thought of as interventions on the treatment generating
mechanism itself. This can potentially help to reduce bias in causal effect
estimation arising from hidden confounders. In the presence of such unobserved
confounding we typically make use of instrumental variables (IVs) -- sources of
treatment randomization that are conditionally independent of the outcome.
However, IVs may not be as readily available as DA for many applications, which
is the main motivation behind this work. By appropriately regularizing IV based
estimators, we introduce the concept of IV-like (IVL) regression for mitigating
confounding bias and improving predictive performance across interventions even
when certain IV properties are relaxed. Finally, we cast parameterized DA as an
IVL regression problem and show that when used in composition can simulate a
worst-case application of such DA, further improving performance on causal
estimation and generalization tasks beyond what simple DA may offer. This is
shown both theoretically for the population case and via simulation experiments
for the finite sample case using a simple linear example. We also present real
data experiments to support our case.

</details>


### [92] [What Really Matters in Matrix-Whitening Optimizers?](https://arxiv.org/abs/2510.25000)
*Kevin Frans,Pieter Abbeel,Sergey Levine*

Main category: cs.LG

TL;DR: 本文系统解构矩阵白化优化器，发现其性能优势源于方差适应组件，低秩方差估计器可降低内存成本且不损失性能。


<details>
  <summary>Details</summary>
Motivation: 系统解构矩阵白化优化器，理清解释其性能的关键组件。

Method: 对各类矩阵白化优化器进行实验，对比不同优化器性能，消融方差适应策略。

Result: 矩阵白化优化器性能优于逐元素优化器；性能提升并非仅由准确的谱归一化解释；方差适应版本的优化器表现更优；低秩方差估计器可降低内存成本且不损失性能。

Conclusion: 矩阵白化有两个目的，方差适应组件是解释性能差距的关键因素。

Abstract: A range of recent optimizers have emerged that approximate the same
"matrix-whitening" transformation in various ways. In this work, we
systematically deconstruct such optimizers, aiming to disentangle the key
components that explain performance. Across tuned hyperparameters across the
board, all flavors of matrix-whitening methods reliably outperform elementwise
counterparts, such as Adam. Matrix-whitening is often related to spectral
descent -- however, experiments reveal that performance gains are *not
explained solely by accurate spectral normalization* -- particularly, SOAP
displays the largest per-step gain, even though Muon more accurately descends
along the steepest spectral descent direction. Instead, we argue that
matrix-whitening serves two purposes, and the variance adaptation component of
matrix-whitening is the overlooked ingredient explaining this performance gap.
Experiments show that variance-adapted versions of optimizers consistently
outperform their sign-descent counterparts, including an adaptive version of
Muon. We further ablate variance adaptation strategies, finding that while
lookahead style approximations are not as effective, low-rank variance
estimators can effectively reduce memory costs without a performance loss.

</details>


### [93] [Disentangling Shared and Private Neural Dynamics with SPIRE: A Latent Modeling Framework for Deep Brain Stimulation](https://arxiv.org/abs/2510.25023)
*Rahil Soroushmojdehi,Sina Javadzadeh,Mehrnaz Asadi,Terence D. Sanger*

Main category: cs.LG

TL;DR: 介绍SPIRE模型用于分解多区域神经数据，在合成基准和颅内深部脑刺激记录中表现良好，是实用工具。


<details>
  <summary>Details</summary>
Motivation: 解决从区域特定活动中分离共享网络级动态这一多区域神经数据建模的核心挑战。

Method: 引入SPIRE，一种深度多编码器自编码器，用新颖的对齐和分离损失将记录分解为共享和私有潜在子空间。

Result: 在合成基准上优于经典概率模型，应用于颅内深部脑刺激记录时，共享潜在空间能可靠编码刺激特定特征。

Conclusion: SPIRE是分析刺激下多区域神经动力学的实用、可重复工具。

Abstract: Disentangling shared network-level dynamics from region-specific activity is
a central challenge in modeling multi-region neural data. We introduce SPIRE
(Shared-Private Inter-Regional Encoder), a deep multi-encoder autoencoder that
factorizes recordings into shared and private latent subspaces with novel
alignment and disentanglement losses. Trained solely on baseline data, SPIRE
robustly recovers cross-regional structure and reveals how external
perturbations reorganize it. On synthetic benchmarks with ground-truth latents,
SPIRE outperforms classical probabilistic models under nonlinear distortions
and temporal misalignments. Applied to intracranial deep brain stimulation
(DBS) recordings, SPIRE shows that shared latents reliably encode
stimulation-specific signatures that generalize across sites and frequencies.
These results establish SPIRE as a practical, reproducible tool for analyzing
multi-region neural dynamics under stimulation.

</details>


### [94] [Machine Learning based Analysis for Radiomics Features Robustness in Real-World Deployment Scenarios](https://arxiv.org/abs/2510.25026)
*Sarmad Ahmad Khan,Simon Bernatz,Zahra Moslehi,Florian Buettner*

Main category: cs.LG

TL;DR: 研究基于放射组学的机器学习模型在五种MRI序列分布偏移下的鲁棒性，发现协议不变特征训练的模型表现好，数据增强可改善不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 基于放射组学的机器学习模型易受分布偏移影响，需研究其在分布偏移下的鲁棒性。

Method: 使用16个水果的体模，通过协议变化、分割变化和观察者间变异性评估分布偏移，训练XGBoost分类器并在不同条件下测试。

Result: 协议不变特征训练的模型F1分数>0.85，使用所有特征的模型性能下降40%，数据增强减少35%预期校准误差，温度缩放校准效果小。

Conclusion: 协议感知特征选择和体模研究可有效预测模型在分布偏移下的行为，为开发鲁棒的放射组学模型提供框架。

Abstract: Radiomics-based machine learning models show promise for clinical decision
support but are vulnerable to distribution shifts caused by variations in
imaging protocols, positioning, and segmentation. This study systematically
investigates the robustness of radiomics-based machine learning models under
distribution shifts across five MRI sequences. We evaluated how different
acquisition protocols and segmentation strategies affect model reliability in
terms of predictive power and uncertainty-awareness. Using a phantom of 16
fruits, we evaluated distribution shifts through: (1) protocol variations
across T2-HASTE, T2-TSE, T2-MAP, T1-TSE, and T2-FLAIR sequences; (2)
segmentation variations (full, partial, rotated); and (3) inter-observer
variability. We trained XGBoost classifiers on 8 consistent robust features
versus sequence-specific features, testing model performance under in-domain
and out-of-domain conditions. Results demonstrate that models trained on
protocol-invariant features maintain F1-scores >0.85 across distribution
shifts, while models using all features showed 40% performance degradation
under protocol changes. Dataset augmentation substantially improved the quality
of uncertainty estimates and reduced the expected calibration error (ECE) by
35% without sacrificing accuracy. Temperature scaling provided minimal
calibration benefits, confirming XGBoost's inherent reliability. Our findings
reveal that protocol-aware feature selection and controlled phantom studies
effectively predict model behavior under distribution shifts, providing a
framework for developing robust radiomics models resilient to real-world
protocol variations.

</details>


### [95] [Graph Distance Based on Cause-Effect Estimands with Latents](https://arxiv.org/abs/2510.25037)
*Zhufeng Li,Niki Kilbertus*

Main category: cs.LG

TL;DR: 提出基于因果效应估计下游任务的ADMG图距离度量方法，并与现有度量比较。


<details>
  <summary>Details</summary>
Motivation: 因果发现中评估发现的图存在困难，尤其是存在潜在混淆时，需衡量进展。

Method: 使用固定识别和符号验证器量化图差异对不同处理 - 结果对因果效应估计量的扭曲。

Result: 分析了该度量在不同图扰动下的行为，并与现有距离度量进行比较。

Conclusion: 未明确提及，但暗示该图距离度量方法有其优势和应用价值。

Abstract: Causal discovery aims to recover graphs that represent causal relations among
given variables from observations, and new methods are constantly being
proposed. Increasingly, the community raises questions about how much progress
is made, because properly evaluating discovered graphs remains notoriously
difficult, particularly under latent confounding. We propose a graph distance
measure for acyclic directed mixed graphs (ADMGs) based on the downstream task
of cause-effect estimation under unobserved confounding. Our approach uses
identification via fixing and a symbolic verifier to quantify how graph
differences distort cause-effect estimands for different treatment-outcome
pairs. We analyze the behavior of the measure under different graph
perturbations and compare it against existing distance metrics.

</details>


### [96] [Scalable Utility-Aware Multiclass Calibration](https://arxiv.org/abs/2510.25458)
*Mahmoud Hegazy,Michael I. Jordan,Aymeric Dieuleveut*

Main category: cs.LG

TL;DR: 研究多分类校准的可扩展评估，提出效用校准框架。


<details>
  <summary>Details</summary>
Motivation: 现有多分类校准评估方法存在仅关注特定方面或计算困难的问题，需要可扩展的评估方法。

Method: 提出效用校准框架，该框架根据特定效用函数衡量校准误差。

Result: 该框架能统一和重新解释现有校准指标，得到更鲁棒的顶级类别和类别校准指标，并能评估更丰富下游效用的校准。

Conclusion: 效用校准框架可用于多分类校准的可扩展评估。

Abstract: Ensuring that classifiers are well-calibrated, i.e., their predictions align
with observed frequencies, is a minimal and fundamental requirement for
classifiers to be viewed as trustworthy. Existing methods for assessing
multiclass calibration often focus on specific aspects associated with
prediction (e.g., top-class confidence, class-wise calibration) or utilize
computationally challenging variational formulations. In this work, we study
scalable \emph{evaluation} of multiclass calibration. To this end, we propose
utility calibration, a general framework that measures the calibration error
relative to a specific utility function that encapsulates the goals or decision
criteria relevant to the end user. We demonstrate how this framework can unify
and re-interpret several existing calibration metrics, particularly allowing
for more robust versions of the top-class and class-wise calibration metrics,
and, going beyond such binarized approaches, toward assessing calibration for
richer classes of downstream utilities.

</details>


### [97] [Training Across Reservoirs: Using Numerical Differentiation To Couple Trainable Networks With Black-Box Reservoirs](https://arxiv.org/abs/2510.25074)
*Andrew Clark,Jack Moursounidis,Osmaan Rasouli,William Gan,Cooper Doyle,Anna Leontjeva*

Main category: cs.LG

TL;DR: 介绍BOND方法，能改进现有微扰法，发现固定非训练模块可提升模型性能且指出结合模拟和数字设备扩展网络的方向。


<details>
  <summary>Details</summary>
Motivation: 解决网络结构中计算图不可访问时偏导数估计问题，探索集成黑盒函数的可训练架构。

Method: 引入Bounded Numerical Differentiation (BOND) 微扰法。

Result: BOND比现有微扰法精度和可扩展性更好，固定非训练网络形式的黑盒函数可提升模型性能且无需大量优化。

Conclusion: 利用固定非训练模块可扩展模型容量，结合模拟和数字设备是扩展网络的途径。

Abstract: We introduce Bounded Numerical Differentiation (BOND), a perturbative method
for estimating partial derivatives across network structures with inaccessible
computational graphs. BOND demonstrates improved accuracy and scalability from
existing perturbative methods, enabling new explorations of trainable
architectures that integrate black-box functions. We observe that these
black-box functions, realized in our experiments as fixed, untrained networks,
can enhance model performance without increasing the number of trainable
parameters. This improvement is achieved without extensive optimization of the
architecture or properties of the black-box function itself. Our findings
highlight the potential of leveraging fixed, non-trainable modules to expand
model capacity, suggesting a path toward combining analogue and digital devices
as a mechanism for scaling networks.

</details>


### [98] [TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting](https://arxiv.org/abs/2510.25502)
*Vladyslav Moroshan,Julien Siems,Arber Zela,Timur Carstensen,Frank Hutter*

Main category: cs.LG

TL;DR: 提出基于线性RNN的单变量时间序列基础模型TempoPFN，在零样本评估中表现出色并开源代码。


<details>
  <summary>Details</summary>
Motivation: 解决零样本时间序列预测在长程预测效率和可重复性方面的挑战，改善现有仅使用合成数据方法在基准测试中的表现。

Method: 提出TempoPFN模型，采用GatedDeltaProduct架构和状态编织技术，使用统一的合成数据管道。

Result: 在Gift - Eval基准的零样本评估中，TempoPFN表现优异，超越现有仅使用合成数据的方法和多数使用真实数据训练的模型，且效率更高。

Conclusion: 开源数据生成管道和训练代码，为未来研究提供可重复的基础。

Abstract: Foundation models for zero-shot time series forecasting face challenges in
efficient long-horizon prediction and reproducibility, with existing
synthetic-only approaches underperforming on challenging benchmarks. This paper
presents TempoPFN, a univariate time series foundation model based on linear
Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The
model uses a GatedDeltaProduct architecture with state-weaving for fully
parallelizable training across sequence lengths, eliminating the need for
windowing or summarization techniques while maintaining robust temporal
state-tracking. Our comprehensive synthetic data pipeline unifies diverse
generators, including stochastic differential equations, Gaussian processes,
and audio synthesis, with novel augmentations. In zero-shot evaluations on the
Gift-Eval benchmark, TempoPFN achieves top-tier competitive performance,
outperforming all existing synthetic-only approaches and surpassing the vast
majority of models trained on real-world data, while being more efficient than
existing baselines by leveraging fully parallelizable training and inference.
We open-source our complete data generation pipeline and training code,
providing a reproducible foundation for future research.

</details>


### [99] [Learning Fair Graph Representations with Multi-view Information Bottleneck](https://arxiv.org/abs/2510.25096)
*Chuxun Liu,Debo Cheng,Qingfeng Chen,Jiangzhang Gan,Jiuyong Li,Lin Liu*

Main category: cs.LG

TL;DR: 提出FairMIB框架缓解GNN中的复杂偏差，实验显示其在效用和公平性指标上达最优。


<details>
  <summary>Details</summary>
Motivation: GNN会放大训练数据偏差，现有公平性方法将偏差视为单一来源，忽略属性和结构的不同影响，导致公平性和效用权衡不佳。

Method: 提出FairMIB多视图信息瓶颈框架，将图分解为特征、结构和扩散视图；用对比学习最大化跨视图互信息；集成多视角条件信息瓶颈目标；在扩散视图引入逆概率加权邻接校正。

Result: 在五个真实世界基准数据集上的实验表明，FairMIB在效用和公平性指标上均达到了最先进的性能。

Conclusion: FairMIB框架能有效缓解GNN中的复杂偏差，平衡任务效用和公平性。

Abstract: Graph neural networks (GNNs) excel on relational data by passing messages
over node features and structure, but they can amplify training data biases,
propagating discriminatory attributes and structural imbalances into unfair
outcomes. Many fairness methods treat bias as a single source, ignoring
distinct attribute and structure effects and leading to suboptimal fairness and
utility trade-offs. To overcome this challenge, we propose FairMIB, a
multi-view information bottleneck framework designed to decompose graphs into
feature, structural, and diffusion views for mitigating complexity biases in
GNNs. Especially, the proposed FairMIB employs contrastive learning to maximize
cross-view mutual information for bias-free representation learning. It further
integrates multi-perspective conditional information bottleneck objectives to
balance task utility and fairness by minimizing mutual information with
sensitive attributes. Additionally, FairMIB introduces an inverse
probability-weighted (IPW) adjacency correction in the diffusion view, which
reduces the spread of bias propagation during message passing. Experiments on
five real-world benchmark datasets demonstrate that FairMIB achieves
state-of-the-art performance across both utility and fairness metrics.

</details>


### [100] [Generalized Sobolev IPM for Graph-Based Measures](https://arxiv.org/abs/2510.25591)
*Tam Le,Truyen Nguyen,Hideitsu Hino,Kenji Fukumizu*

Main category: cs.LG

TL;DR: 研究图度量空间上的Sobolev IPM问题，提出基于Orlicz几何结构的广义Sobolev IPM，建立与Musielak范数联系实现高效计算，实证表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于$L^p$几何结构的Sobolev IPM框架有局限，无法纳入$L^p$几何范式之外的结构先验。

Method: 从Orlicz几何结构视角推广Sobolev IPM，建立Orlicz - Sobolev范数与Musielak范数的理论联系，利用图结构将问题简化为单变量优化问题。

Result: GSI - M计算比流行的OW快几个数量级，在文档分类和拓扑数据分析等任务中展示了实际优势。

Conclusion: 提出的广义Sobolev IPM（GSI - M）能克服原有框架局限，实现高效计算并在实际任务中表现出色。

Abstract: We study the Sobolev IPM problem for measures supported on a graph metric
space, where critic function is constrained to lie within the unit ball defined
by Sobolev norm. While Le et al. (2025) achieved scalable computation by
relating Sobolev norm to weighted $L^p$-norm, the resulting framework remains
intrinsically bound to $L^p$ geometric structure, limiting its ability to
incorporate alternative structural priors beyond the $L^p$ geometry paradigm.
To overcome this limitation, we propose to generalize Sobolev IPM through the
lens of \emph{Orlicz geometric structure}, which employs convex functions to
capture nuanced geometric relationships, building upon recent advances in
optimal transport theory -- particularly Orlicz-Wasserstein (OW) and
generalized Sobolev transport -- that have proven instrumental in advancing
machine learning methodologies. This generalization encompasses classical
Sobolev IPM as a special case while accommodating diverse geometric priors
beyond traditional $L^p$ structure. It however brings up significant
computational hurdles that compound those already inherent in Sobolev IPM. To
address these challenges, we establish a novel theoretical connection between
Orlicz-Sobolev norm and Musielak norm which facilitates a novel regularization
for the generalized Sobolev IPM (GSI). By further exploiting the underlying
graph structure, we show that GSI with Musielak regularization (GSI-M) reduces
to a simple \emph{univariate optimization} problem, achieving remarkably
computational efficiency. Empirically, GSI-M is several-order faster than the
popular OW in computation, and demonstrates its practical advantages in
comparing probability measures on a given graph for document classification and
several tasks in topological data analysis.

</details>


### [101] [The Neural Differential Manifold: An Architecture with Explicit Geometric Structure](https://arxiv.org/abs/2510.25113)
*Di Zhang*

Main category: cs.LG

TL;DR: 本文提出神经微分流形（NDM），一种将几何结构融入设计的神经网络架构，分析其优势与挑战。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络基于欧几里得参数空间，本文旨在将几何结构融入神经网络基础设计，以提升泛化性、鲁棒性和可解释性。

Method: NDM架构分为坐标层、几何层和进化层，通过可逆变换、辅助子网络和双目标损失函数实现，并进行几何正则化。

Result: NDM能实现与学习的流形几何对齐的自然梯度下降优化，赋予内部表示清晰几何意义。

Conclusion: NDM代表了向几何结构、可解释和高效深度学习系统的根本转变，虽有计算挑战，但有诸多潜在优势。

Abstract: This paper introduces the Neural Differential Manifold (NDM), a novel neural
network architecture that explicitly incorporates geometric structure into its
fundamental design. Departing from conventional Euclidean parameter spaces, the
NDM re-conceptualizes a neural network as a differentiable manifold where each
layer functions as a local coordinate chart, and the network parameters
directly parameterize a Riemannian metric tensor at every point. The
architecture is organized into three synergistic layers: a Coordinate Layer
implementing smooth chart transitions via invertible transformations inspired
by normalizing flows, a Geometric Layer that dynamically generates the
manifold's metric through auxiliary sub-networks, and an Evolution Layer that
optimizes both task performance and geometric simplicity through a
dual-objective loss function. This geometric regularization penalizes excessive
curvature and volume distortion, providing intrinsic regularization that
enhances generalization and robustness. The framework enables natural gradient
descent optimization aligned with the learned manifold geometry and offers
unprecedented interpretability by endowing internal representations with clear
geometric meaning. We analyze the theoretical advantages of this approach,
including its potential for more efficient optimization, enhanced continual
learning, and applications in scientific discovery and controllable generative
modeling. While significant computational challenges remain, the Neural
Differential Manifold represents a fundamental shift towards geometrically
structured, interpretable, and efficient deep learning systems.

</details>


### [102] [Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions](https://arxiv.org/abs/2510.25769)
*Naoki Kiyohara,Edward Johns,Yingzhen Li*

Main category: cs.LG

TL;DR: 提出神经随机流（NSFs）及其潜在变体，可直接学习（潜在）随机微分方程（SDE）转移律，实现任意状态间一次性采样，在大时间间隔下提速，实验显示能保证分布准确性并减少计算量。


<details>
  <summary>Details</summary>
Motivation: 传统随机微分方程方法在采样任意时间点时需昂贵数值求解器。

Method: 引入神经随机流（NSFs）及其潜在变体，用有架构约束的条件归一化流直接学习（潜在）SDE转移律。

Result: 在大时间间隔下提速可达两个数量级，在合成SDE模拟及真实跟踪和视频数据实验中，NSFs能保持与数值方法相当的分布准确性，并大幅减少任意时间点采样计算量。

Conclusion: NSFs可在保证分布准确性的同时，显著减少任意时间点采样的计算成本，提高效率。

Abstract: Stochastic differential equations (SDEs) are well suited to modelling noisy
and irregularly sampled time series found in finance, physics, and machine
learning. Traditional approaches require costly numerical solvers to sample
between arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and
their latent variants, which directly learn (latent) SDE transition laws using
conditional normalising flows with architectural constraints that preserve
properties inherited from stochastic flows. This enables one-shot sampling
between arbitrary states and yields up to two orders of magnitude speed-ups at
large time gaps. Experiments on synthetic SDE simulations and on real-world
tracking and video data show that NSFs maintain distributional accuracy
comparable to numerical approaches while dramatically reducing computation for
arbitrary time-point sampling.

</details>


### [103] [A Unified Bilevel Model for Adversarial Learning and A Case Study](https://arxiv.org/abs/2510.25121)
*Yutong Zheng,Qingna Li*

Main category: cs.LG

TL;DR: 本文提出对抗学习统一双层模型，从数据扰动角度解释聚类模型对抗攻击并分析攻击效果衡量方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型结构复杂，对抗攻击机制解释不足，攻击效果衡量不明确。

Method: 提出统一双层模型，从数据扰动角度研究聚类模型对抗攻击，分析δ - 测度的良定性。

Result: 揭示数据扰动较小时聚类模型鲁棒，较大时聚类结果改变引发攻击。

Conclusion: δ - 测度可用于所提聚类模型对抗学习双层模型衡量攻击效果。

Abstract: Adversarial learning has been attracting more and more attention thanks to
the fast development of machine learning and artificial intelligence. However,
due to the complicated structure of most machine learning models, the mechanism
of adversarial attacks is not well interpreted. How to measure the effect of
attack is still not quite clear. In this paper, we propose a unified bilevel
model for adversarial learning. We further investigate the adversarial attack
in clustering models and interpret it from data perturbation point of view. We
reveal that when the data perturbation is relatively small, the clustering
model is robust, whereas if it is relatively large, the clustering result
changes, which leads to an attack. To measure the effect of attacks for
clustering models, we analyse the well-definedness of the so-called
$\delta$-measure, which can be used in the proposed bilevel model for
adversarial learning of clustering models.

</details>


### [104] [Learning Low Rank Neural Representations of Hyperbolic Wave Dynamics from Data](https://arxiv.org/abs/2510.25123)
*Woojin Cho,Kookjin Lee,Noseong Park,Donsub Rim,Gerrit Welper*

Main category: cs.LG

TL;DR: 提出适用于双曲波传播物理数据的数据驱动降维方法，用LRNR架构，可从数据学习波的低维表示，有新分解且能高效推理。


<details>
  <summary>Details</summary>
Motivation: 基于理论结果证明该类波存在高效表示，开发适合双曲波传播物理数据的降维方法。

Method: 在超网络框架中使用低秩神经表示（LRNR）的专门神经网络架构，结合深度学习技术从数据中学习。

Result: 训练的LRNR自然产生低秩张量表示，揭示波传播新分解，每个分解模式对应可解释物理特征，且架构通过压缩方案实现高效推理。

Conclusion: 所提出的LRNR架构可从数据学习波的低维表示，有新物理分解且能实现高效推理，在高性能场景有潜在重要性。

Abstract: We present a data-driven dimensionality reduction method that is well-suited
for physics-based data representing hyperbolic wave propagation. The method
utilizes a specialized neural network architecture called low rank neural
representation (LRNR) inside a hypernetwork framework. The architecture is
motivated by theoretical results that rigorously prove the existence of
efficient representations for this wave class. We illustrate through archetypal
examples that such an efficient low-dimensional representation of propagating
waves can be learned directly from data through a combination of deep learning
techniques. We observe that a low rank tensor representation arises naturally
in the trained LRNRs, and that this reveals a new decomposition of wave
propagation where each decomposed mode corresponds to interpretable physical
features. Furthermore, we demonstrate that the LRNR architecture enables
efficient inference via a compression scheme, which is a potentially important
feature when deploying LRNRs in demanding performance regimes.

</details>


### [105] [Bridging the Divide: End-to-End Sequence-Graph Learning](https://arxiv.org/abs/2510.25126)
*Yuen Chen,Yulun Wu,Samuel Sharpe,Igor Melnyk,Nam H. Nguyen,Furong Huang,C. Bayan Bruss,Rizal Fathony*

Main category: cs.LG

TL;DR: 现有序列和图建模方法常忽略一种模态，提出BRIDGE统一架构联合学习序列和图，添加TOKENXATTN层，在两个任务上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现实数据集兼具序列和关系特性，现有方法常忽略一种模态，应联合学习序列和图。

Method: 引入BRIDGE统一端到端架构，结合序列编码器和GNN；添加TOKENXATTN层实现细粒度令牌级消息传递。

Result: 在友谊预测和欺诈检测两个任务中，BRIDGE在排名和分类指标上始终优于静态GNN、时态图方法和仅序列基线。

Conclusion: 联合学习序列和图是有效的，BRIDGE架构和TOKENXATTN层能提升性能。

Abstract: Many real-world datasets are both sequential and relational: each node
carries an event sequence while edges encode interactions. Existing methods in
sequence modeling and graph modeling often neglect one modality or the other.
We argue that sequences and graphs are not separate problems but complementary
facets of the same dataset, and should be learned jointly. We introduce BRIDGE,
a unified end-to-end architecture that couples a sequence encoder with a GNN
under a single objective, allowing gradients to flow across both modules and
learning task-aligned representations. To enable fine-grained token-level
message passing among neighbors, we add TOKENXATTN, a token-level
cross-attention layer that passes messages between events in neighboring
sequences. Across two settings, friendship prediction (Brightkite) and fraud
detection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graph
methods, and sequence-only baselines on ranking and classification metrics.

</details>


### [106] [Lipschitz-aware Linearity Grafting for Certified Robustness](https://arxiv.org/abs/2510.25130)
*Yongjin Han,Suhyun Kim*

Main category: cs.LG

TL;DR: 论文分析线性嫁接提升认证鲁棒性的原理，提出Lipschitz感知线性嫁接方法，实验证明该方法可收紧局部Lipschitz常数并增强认证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有过近似方法存在近似误差，阻碍获得紧密局部Lipschitz常数，且缺乏线性嫁接提升认证鲁棒性的理论分析。

Method: 从$l_\infty$局部Lipschitz常数角度分析线性嫁接提升认证鲁棒性的原因，提出Lipschitz感知线性嫁接方法去除主要近似误差。

Result: 实验表明对有影响力的激活函数进行线性嫁接可收紧$l_\infty$局部Lipschitz常数，增强认证鲁棒性。

Conclusion: 线性嫁接能提升认证鲁棒性，所提方法有效。

Abstract: Lipschitz constant is a fundamental property in certified robustness, as
smaller values imply robustness to adversarial examples when a model is
confident in its prediction. However, identifying the worst-case adversarial
examples is known to be an NP-complete problem. Although over-approximation
methods have shown success in neural network verification to address this
challenge, reducing approximation errors remains a significant obstacle.
Furthermore, these approximation errors hinder the ability to obtain tight
local Lipschitz constants, which are crucial for certified robustness.
Originally, grafting linearity into non-linear activation functions was
proposed to reduce the number of unstable neurons, enabling scalable and
complete verification. However, no prior theoretical analysis has explained how
linearity grafting improves certified robustness. We instead consider linearity
grafting primarily as a means of eliminating approximation errors rather than
reducing the number of unstable neurons, since linear functions do not require
relaxation. In this paper, we provide two theoretical contributions: 1) why
linearity grafting improves certified robustness through the lens of the
$l_\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear
activation functions, the dominant source of approximation errors, yields a
tighter local Lipschitz constant. Based on these theoretical contributions, we
propose a Lipschitz-aware linearity grafting method that removes dominant
approximation errors, which are crucial for tightening the local Lipschitz
constant, thereby improving certified robustness, even without certified
training. Our extensive experiments demonstrate that grafting linearity into
these influential activations tightens the $l_\infty$ local Lipschitz constant
and enhances certified robustness.

</details>


### [107] [Machine Learning Guided Optimal Transmission Switching to Mitigate Wildfire Ignition Risk](https://arxiv.org/abs/2510.25147)
*Weimin Huang,Ryan Piansky,Bistra Dilkina,Daniel K. Molzahn*

Main category: cs.LG

TL;DR: 为缓解野火风险，提出ML引导框架解决OPS问题，比传统方法更快产生高质量断电决策。


<details>
  <summary>Details</summary>
Motivation: OPS问题是计算复杂的MILP，需快速频繁求解，且特定电力系统的OPS实例有共同结构，因此想用ML利用实例间的共享模式求解。

Method: 开发ML引导框架，扩展现有ML引导的MILP求解方法，集成线路通电和断电数量的领域知识。

Result: 在基于加州的大规模合成测试系统上，所提ML引导方法比传统优化方法更快产生高质量解。

Conclusion: ML引导框架能有效解决OPS问题，快速产生高质量的断电决策。

Abstract: To mitigate acute wildfire ignition risks, utilities de-energize power lines
in high-risk areas. The Optimal Power Shutoff (OPS) problem optimizes line
energization statuses to manage wildfire ignition risks through
de-energizations while reducing load shedding. OPS problems are computationally
challenging Mixed-Integer Linear Programs (MILPs) that must be solved rapidly
and frequently in operational settings. For a particular power system, OPS
instances share a common structure with varying parameters related to wildfire
risks, loads, and renewable generation. This motivates the use of Machine
Learning (ML) for solving OPS problems by exploiting shared patterns across
instances. In this paper, we develop an ML-guided framework that quickly
produces high-quality de-energization decisions by extending existing ML-guided
MILP solution methods while integrating domain knowledge on the number of
energized and de-energized lines. Results on a large-scale realistic
California-based synthetic test system show that the proposed ML-guided method
produces high-quality solutions faster than traditional optimization methods.

</details>


### [108] [Selective Learning for Deep Time Series Forecasting](https://arxiv.org/abs/2510.25207)
*Yisong Fu,Zezhi Shao,Chengqing Yu,Yujie Li,Zhulin An,Qi Wang,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: 提出用于深度时间序列预测的选择性学习策略，通过双掩码机制筛选时间步，实验表明能提升多种模型预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习范式在时间序列预测中因对噪声和异常值敏感，采用均方误差损失统一优化所有时间步，易导致过拟合。

Method: 提出选择性学习策略，引入双掩码机制（不确定性掩码和异常掩码）筛选时间步计算均方误差损失。

Result: 在八个真实数据集上实验，选择性学习显著提升典型模型预测性能，如Informer的均方误差降低37.4%等。

Conclusion: 选择性学习策略能有效解决深度模型在时间序列预测中的过拟合问题，提升预测性能。

Abstract: Benefiting from high capacity for capturing complex temporal patterns, deep
learning (DL) has significantly advanced time series forecasting (TSF).
However, deep models tend to suffer from severe overfitting due to the inherent
vulnerability of time series to noise and anomalies. The prevailing DL paradigm
uniformly optimizes all timesteps through the MSE loss and learns those
uncertain and anomalous timesteps without difference, ultimately resulting in
overfitting. To address this, we propose a novel selective learning strategy
for deep TSF. Specifically, selective learning screens a subset of the whole
timesteps to calculate the MSE loss in optimization, guiding the model to focus
on generalizable timesteps while disregarding non-generalizable ones. Our
framework introduces a dual-mask mechanism to target timesteps: (1) an
uncertainty mask leveraging residual entropy to filter uncertain timesteps, and
(2) an anomaly mask employing residual lower bound estimation to exclude
anomalous timesteps. Extensive experiments across eight real-world datasets
demonstrate that selective learning can significantly improve the predictive
performance for typical state-of-the-art deep models, including 37.4% MSE
reduction for Informer, 8.4% for TimesNet, and 6.5% for iTransformer.

</details>


### [109] [Cost-Sensitive Unbiased Risk Estimation for Multi-Class Positive-Unlabeled Learning](https://arxiv.org/abs/2510.25226)
*Miao Zhang,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 提出基于自适应损失加权的成本敏感多类PU方法，在八个公开数据集实验中准确率和稳定性优于基线。


<details>
  <summary>Details</summary>
Motivation: 现实应用中标记可靠负样本困难或成本高，多类PU学习（MPU）现有方法无法保证无偏风险估计，限制性能和稳定性。

Method: 在经验风险最小化框架下，为正样本和推断负样本的损失分量分配不同的、依赖数据的权重，使经验目标成为目标风险的无偏估计器，并形式化MPU数据生成过程，建立泛化误差界。

Result: 在八个公开数据集上实验，准确率和稳定性均优于强基线。

Conclusion: 所提出的基于自适应损失加权的成本敏感多类PU方法有效。

Abstract: Positive--Unlabeled (PU) learning considers settings in which only positive
and unlabeled data are available, while negatives are missing or left
unlabeled. This situation is common in real applications where annotating
reliable negatives is difficult or costly. Despite substantial progress in PU
learning, the multi-class case (MPU) remains challenging: many existing
approaches do not ensure \emph{unbiased risk estimation}, which limits
performance and stability. We propose a cost-sensitive multi-class PU method
based on \emph{adaptive loss weighting}. Within the empirical risk minimization
framework, we assign distinct, data-dependent weights to the positive and
\emph{inferred-negative} (from the unlabeled mixture) loss components so that
the resulting empirical objective is an unbiased estimator of the target risk.
We formalize the MPU data-generating process and establish a generalization
error bound for the proposed estimator. Extensive experiments on \textbf{eight}
public datasets, spanning varying class priors and numbers of classes, show
consistent gains over strong baselines in both accuracy and stability.

</details>


### [110] [BSFA: Leveraging the Subspace Dichotomy to Accelerate Neural Network Training](https://arxiv.org/abs/2510.25244)
*Wenjie Zhou,Bohan Wang,Wei Chen,Xueqi Cheng*

Main category: cs.LG

TL;DR: 本文指出深度学习优化中参数更新方向的差异现象，提出BSFA框架加速训练，介绍关键创新并展示加速效果。


<details>
  <summary>Details</summary>
Motivation: 进一步理解深度学习优化中参数更新在不同子空间的现象，并加速训练。

Method: 提出BSFA框架，通过对不同子空间投影的更新分量进行差异化缩放；采用PCA进行高效子空间估计，使用逐参数块的分块策略。

Result: 在多种任务中展示了BSFA的加速效果，如在WikiText - 103和OpenWebText上预训练模型时比AdamW快约2倍。

Conclusion: BSFA框架是实用且可扩展的，能有效加速深度学习模型训练。

Abstract: Recent studies \citep{gur2018gradient,song2024does, wen2024understanding}
highlight a fundamental dichotomy in deep learning optimization: Although
parameter updates along the top eigendirections of the loss Hessian (Dom-space)
capture most of the update magnitude, they often contribute minimally to loss
reduction. In contrast, updates in the orthogonal component (Bulk-space) have
smaller magnitudes but drive most learning progress. In this work, we further
advance the understanding of this phenomenon and introduce the
\textbf{Bulk-Space-Filtration-Accelerator (BSFA)}, a novel plug-and-play
framework. BSFA accelerates training by differentially scaling update
components projected onto these distinct subspaces, simultaneously enhancing
stability by moderating updates in the dominant subspace and boosting
convergence speed by amplifying those in the bulk-space. To ensure BSFA is both
practical and scalable for contemporary large models, we introduce two key
innovations: an efficient estimator using Principal Component Analysis (PCA) on
historical updates for fast subspace estimation, and a block-wise strategy that
applies this estimation on a per-parameter-block basis. These designs make BSFA
computationally tractable and highly effective. We demonstrate BSFA's
acceleration across various tasks, notably achieving approximately 2$\times$
speedup when pre-training LLaMA-72M on WikiText-103 and LLaMA-134M on
OpenWebText compared to vanilla AdamW.

</details>


### [111] [Scaling Up Bayesian DAG Sampling](https://arxiv.org/abs/2510.25254)
*Daniele Nikzad,Alexander Zhilkin,Juha Harviainen,Jack Kuipers,Giusi Moffa,Mikko Koivisto*

Main category: cs.LG

TL;DR: 提出两种改进贝叶斯网络结构采样的技术，实验显示比之前方法更高效


<details>
  <summary>Details</summary>
Motivation: 改进贝叶斯网络结构的贝叶斯推理中对有向无环图的采样效率

Method: 一是高效实现基本移动（添加、删除或反转单条弧）；二是设计预处理方法修剪可能的父集以加速对父集求和

Result: 所提技术比之前方法有显著的效率提升

Conclusion: 所提出的两种技术能有效提高贝叶斯网络结构采样效率

Abstract: Bayesian inference of Bayesian network structures is often performed by
sampling directed acyclic graphs along an appropriately constructed Markov
chain. We present two techniques to improve sampling. First, we give an
efficient implementation of basic moves, which add, delete, or reverse a single
arc. Second, we expedite summing over parent sets, an expensive task required
for more sophisticated moves: we devise a preprocessing method to prune
possible parent sets so as to approximately preserve the sums. Our empirical
study shows that our techniques can yield substantial efficiency gains compared
to previous methods.

</details>


### [112] [IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning](https://arxiv.org/abs/2510.25262)
*Xiandong Zou,Pan Zhou*

Main category: cs.LG

TL;DR: 提出基于信息瓶颈原则的IBNorm，理论和实验证明其优于现有归一化方法。


<details>
  <summary>Details</summary>
Motivation: 现有归一化方法以方差为中心，未有效控制表征捕获任务相关信息，需新方法。

Method: 基于信息瓶颈原则，引入有界压缩操作，鼓励嵌入保留预测信息并抑制干扰变化。

Result: 理论上IBNorm有更高IB值和更紧泛化边界；实验上在多种模型中优于现有方法，互信息分析证明其信息瓶颈性能更优。

Conclusion: IBNorm有效，能产生更具信息性表征，且保持标准归一化的稳定性和兼容性。

Abstract: Normalization is fundamental to deep learning, but existing approaches such
as BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero
mean and unit variance, stabilizing training without controlling how
representations capture task-relevant information. We propose IB-Inspired
Normalization (IBNorm), a simple yet powerful family of methods grounded in the
Information Bottleneck principle. IBNorm introduces bounded compression
operations that encourage embeddings to preserve predictive information while
suppressing nuisance variability, yielding more informative representations
while retaining the stability and compatibility of standard normalization.
Theoretically, we prove that IBNorm achieves a higher IB value and tighter
generalization bounds than variance-centric methods. Empirically, IBNorm
consistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scale
language models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutual
information analysis confirming superior information bottleneck behavior. Code
will be released publicly.

</details>


### [113] [On the Stability of Neural Networks in Deep Learning](https://arxiv.org/abs/2510.25282)
*Blaise Delattre*

Main category: cs.LG

TL;DR: 本文从灵敏度分析视角，结合Lipschitz网络、正则化技术和随机平滑方法，构建统一框架解决深度学习模型的稳定性和脆弱性问题，并有理论分析和实用方法贡献。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型存在不稳定性和脆弱性，如输入微小变化影响预测、优化受尖锐损失景观阻碍。

Method: 研究Lipschitz网络约束输入扰动灵敏度；引入基于损失函数曲率的正则化技术；探索随机平滑方法；结合三者构建统一框架。

Result: 开发出结合Lipschitz连续性、随机平滑和曲率正则化的统一框架。

Conclusion: 论文在理论分析和实用方法上有贡献，如高效谱范数计算、新型Lipschitz约束层和改进的认证程序。

Abstract: Deep learning has achieved remarkable success across a wide range of tasks,
but its models often suffer from instability and vulnerability: small changes
to the input may drastically affect predictions, while optimization can be
hindered by sharp loss landscapes. This thesis addresses these issues through
the unifying perspective of sensitivity analysis, which examines how neural
networks respond to perturbations at both the input and parameter levels.
  We study Lipschitz networks as a principled way to constrain sensitivity to
input perturbations, thereby improving generalization, adversarial robustness,
and training stability. To complement this architectural approach, we introduce
regularization techniques based on the curvature of the loss function,
promoting smoother optimization landscapes and reducing sensitivity to
parameter variations. Randomized smoothing is also explored as a probabilistic
method for enhancing robustness at decision boundaries.
  By combining these perspectives, we develop a unified framework where
Lipschitz continuity, randomized smoothing, and curvature regularization
interact to address fundamental challenges in stability. The thesis contributes
both theoretical analysis and practical methodologies, including efficient
spectral norm computation, novel Lipschitz-constrained layers, and improved
certification procedures.

</details>


### [114] [Hierarchical Physics-Embedded Learning for Spatiotemporal Dynamical Systems](https://arxiv.org/abs/2510.25306)
*Xizhe Wang,Xiaobin Song,Qingshan Jia,Hongbo Zhao,Benben Jiang*

Main category: cs.LG

TL;DR: 提出分层物理嵌入学习框架，解决复杂时空动力学建模问题，提升前向预测和反向定律发现能力。


<details>
  <summary>Details</summary>
Motivation: 复杂时空动力学系统的PDE难从第一原理推导，现有数据驱动方法有局限，需新方法。

Method: 采用两级架构的分层框架，将已知物理定律嵌入计算图，基于自适应傅里叶神经算子构建。

Result: 能从稀疏和噪声数据中进行时空预测和物理定律发现，保证物理一致性，提高数据效率。

Conclusion: 该框架可有效处理复杂时空动力学问题，实现可解释的定律发现。

Abstract: Modeling complex spatiotemporal dynamics, particularly in
far-from-equilibrium systems, remains a grand challenge in science. The
governing partial differential equations (PDEs) for these systems are often
intractable to derive from first principles, due to their inherent complexity,
characterized by high-order derivatives and strong nonlinearities, coupled with
incomplete physical knowledge. This has spurred the development of data-driven
methods, yet these approaches face limitations: Purely data-driven models are
often physically inconsistent and data-intensive, while existing
physics-informed methods lack the structural capacity to represent complex
operators or systematically integrate partial physical knowledge. Here, we
propose a hierarchical physics-embedded learning framework that fundamentally
advances both the forward spatiotemporal prediction and inverse discovery of
physical laws from sparse and noisy data. The key innovation is a two-level
architecture that mirrors the process of scientific discovery: the first level
learns fundamental symbolic components of a PDE, while the second learns their
governing combinations. This hierarchical decomposition not only reduces
learning complexity but, more importantly, enables a structural integration of
prior knowledge. Known physical laws are directly embedded into the models
computational graph, guaranteeing physical consistency and improving data
efficiency. By building the framework upon adaptive Fourier Neural Operators,
we can effectively capture the non-local dependencies and high-order operators
characteristic of dynamical systems. Additionally, by structurally decoupling
known and unknown terms, the framework further enables interpretable discovery
of underlying governing equations through symbolic regression, without
presupposing functional forms.

</details>


### [115] [Dense and Diverse Goal Coverage in Multi Goal Reinforcement Learning](https://arxiv.org/abs/2510.25311)
*Sagalpreet Singh,Rishi Saket,Aravindan Raghuveer*

Main category: cs.LG

TL;DR: 提出多目标强化学习（Multi Goal RL）问题，设计新算法学习高回报策略混合，使边际状态分布分散在目标状态集上，并证明性能保证和进行实验评估。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法难以学习到在最大化期望回报的同时，使奖励状态的边际状态分布分散的策略，相关方面研究较少。

Method: 将最大化期望回报同时均匀访问目标状态问题形式化为Multi Goal RL，提出基于优化自定义强化学习奖励的算法，用离线强化学习算法更新策略混合。

Result: 证明了算法性能保证，展示了优化自然目标的有效收敛界限。

Conclusion: 所提出的算法能学习到高回报且边际状态分布分散在目标状态集上的策略，在合成MDP和标准强化学习环境实验中验证了有效性。

Abstract: Reinforcement Learning algorithms are primarily focused on learning a policy
that maximizes expected return. As a result, the learned policy can exploit one
or few reward sources. However, in many natural situations, it is desirable to
learn a policy that induces a dispersed marginal state distribution over
rewarding states, while maximizing the expected return which is typically tied
to reaching a goal state. This aspect remains relatively unexplored. Existing
techniques based on entropy regularization and intrinsic rewards use
stochasticity for encouraging exploration to find an optimal policy which may
not necessarily lead to dispersed marginal state distribution over rewarding
states. Other RL algorithms which match a target distribution assume the latter
to be available apriori. This may be infeasible in large scale systems where
enumeration of all states is not possible and a state is determined to be a
goal state only upon reaching it. We formalize the problem of maximizing the
expected return while uniformly visiting the goal states as Multi Goal RL in
which an oracle classifier over the state space determines the goal states. We
propose a novel algorithm that learns a high-return policy mixture with
marginal state distribution dispersed over the set of goal states. Our
algorithm is based on optimizing a custom RL reward which is computed - based
on the current policy mixture - at each iteration for a set of sampled
trajectories. The latter are used via an offline RL algorithm to update the
policy mixture. We prove performance guarantees for our algorithm, showing
efficient convergence bounds for optimizing a natural objective which captures
the expected return as well as the dispersion of the marginal state
distribution over the goal states. We design and perform experiments on
synthetic MDPs and standard RL environments to evaluate the effectiveness of
our algorithm.

</details>


### [116] [CDFlow: Building Invertible Layers with Circulant and Diagonal Matrices](https://arxiv.org/abs/2510.25323)
*Xuchen Feng,Siyu Liao*

Main category: cs.LG

TL;DR: 本文提出基于循环矩阵和对角矩阵乘积的可逆线性层，构建CDFlow，在自然图像数据集上实现强密度估计，加速归一化流关键操作。


<details>
  <summary>Details</summary>
Motivation: 设计能在保持雅可比行列式和逆高效计算的同时增强表达能力的线性层。

Method: 引入基于循环矩阵和对角矩阵乘积的可逆线性层，利用快速傅里叶变换，在此基础上构建Circulant - Diagonal Flow (CDFlow)。

Result: CDFlow在自然图像数据集上实现强密度估计，有效建模具有固有周期结构的数据，显著加速归一化流关键操作。

Conclusion: 该方法为可扩展生成建模提供实际益处。

Abstract: Normalizing flows are deep generative models that enable efficient likelihood
estimation and sampling through invertible transformations. A key challenge is
to design linear layers that enhance expressiveness while maintaining efficient
computation of the Jacobian determinant and inverse. We introduce a novel
invertible linear layer based on the product of circulant and diagonal
matrices. This decomposition reduces parameter complexity from
$\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$ using $m$ diagonal matrices and $m-1$
circulant matrices while still approximating general linear transformations. By
leveraging the Fast Fourier Transform, our approach reduces the time complexity
of matrix inversion from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn\log n)$ and that
of computing the log-determinant from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn)$,
where $n$ is the input dimension. We build upon this layer to develop
Circulant-Diagonal Flow (CDFlow), which achieves strong density estimation on
natural image datasets and effectively models data with inherent periodic
structure. Furthermore, CDFlow significantly accelerates key operations in
normalizing flows, providing practical benefits for scalable generative
modeling.

</details>


### [117] [Beyond Leakage and Complexity: Towards Realistic and Efficient Information Cascade Prediction](https://arxiv.org/abs/2510.25348)
*Jie Peng,Rui Wang,Qiang Wang,Zhewei Wei,Bin Tong,Guan Wang*

Main category: cs.LG

TL;DR: 本文针对信息级联流行度预测问题，从任务设置、数据集构建和模型设计三方面解决现有工作的局限，提出新策略、引入新数据集并开发轻量级框架，实现无泄漏评估下的优异性能。


<details>
  <summary>Details</summary>
Motivation: 当前信息级联流行度预测相关工作存在评估时的时间泄漏、数据集特征不足和图方法计算效率低等问题，需要解决这些局限。

Method: 提出时间有序分割策略，引入含丰富属性和购买转化信息的Taoke数据集，开发通过时间游走、基于Jaccard的邻居选择和基于GRU的带时间感知注意力编码的轻量级框架CasTemp。

Result: 在无泄漏评估下，CasTemp在四个数据集上达到了最先进的性能，且速度大幅提升，尤其擅长预测第二阶段的流行度转化。

Conclusion: 通过多方面改进，有效解决了现有信息级联流行度预测工作的问题，所提出的方法具有良好的性能和实用性。

Abstract: Information cascade popularity prediction is a key problem in analyzing
content diffusion in social networks. However, current related works suffer
from three critical limitations: (1) temporal leakage in current
evaluation--random cascade-based splits allow models to access future
information, yielding unrealistic results; (2) feature-poor datasets that lack
downstream conversion signals (e.g., likes, comments, or purchases), which
limits more practical applications; (3) computational inefficiency of complex
graph-based methods that require days of training for marginal gains. We
systematically address these challenges from three perspectives: task setup,
dataset construction, and model design. First, we propose a time-ordered
splitting strategy that chronologically partitions data into consecutive
windows, ensuring models are evaluated on genuine forecasting tasks without
future information leakage. Second, we introduce Taoke, a large-scale
e-commerce cascade dataset featuring rich promoter/product attributes and
ground-truth purchase conversions--capturing the complete diffusion lifecycle
from promotion to monetization. Third, we develop CasTemp, a lightweight
framework that efficiently models cascade dynamics through temporal walks,
Jaccard-based neighbor selection for inter-cascade dependencies, and GRU-based
encoding with time-aware attention. Under leak-free evaluation, CasTemp
achieves state-of-the-art performance across four datasets with
orders-of-magnitude speedup. Notably, it excels at predicting second-stage
popularity conversions--a practical task critical for real-world applications.

</details>


### [118] [Analysis of Semi-Supervised Learning on Hypergraphs](https://arxiv.org/abs/2510.25354)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: 对随机几何超图变分学习进行渐近一致性分析，提出HOHL方法，在标准基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 超图在半监督学习中的理论基础有限，需进一步研究超图学习。

Method: 对随机几何超图变分学习进行渐近一致性分析，提出HOHL方法，通过骨架图拉普拉斯幂进行正则化。

Result: 证明了HOHL收敛到高阶Sobolev半范数，在标准基准测试中表现出色。

Conclusion: 所提方法有效，为超图在半监督学习中的应用提供了理论支持。

Abstract: Hypergraphs provide a natural framework for modeling higher-order
interactions, yet their theoretical underpinnings in semi-supervised learning
remain limited. We provide an asymptotic consistency analysis of variational
learning on random geometric hypergraphs, precisely characterizing the
conditions ensuring the well-posedness of hypergraph learning as well as
showing convergence to a weighted $p$-Laplacian equation. Motivated by this, we
propose Higher-Order Hypergraph Learning (HOHL), which regularizes via powers
of Laplacians from skeleton graphs for multiscale smoothness. HOHL converges to
a higher-order Sobolev seminorm. Empirically, it performs strongly on standard
baselines.

</details>


### [119] [Parameter Averaging in Link Prediction](https://arxiv.org/abs/2510.25361)
*Rupesh Sapkota,Caglar Demir,Arnab Sharma,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: 本文介绍在知识图谱嵌入模型中使用模型合并（加权平均）方法进行链接预测，提出两种加权平均法并评估，结果显示该方法能提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统集成学习在知识图谱嵌入模型中训练多模型有计算开销大等缺点，需寻找替代方法。

Method: 引入加权平均的模型合并方法，维护训练时期开始的模型参数运行平均值用于预测，还提出仅在验证集泛化性能提升时更新运行平均值的方法。

Result: 在链接预测任务、字面增强的知识图谱嵌入模型和多跳问答任务中评估，结果表明提出的加权平均法在不同评估设置下都能持续提升性能。

Conclusion: 提出的加权平均方法是知识图谱嵌入模型链接预测的有效替代方案，能提升性能。

Abstract: Ensemble methods are widely employed to improve generalization in machine
learning. This has also prompted the adoption of ensemble learning for the
knowledge graph embedding (KGE) models in performing link prediction. Typical
approaches to this end train multiple models as part of the ensemble, and the
diverse predictions are then averaged. However, this approach has some
significant drawbacks. For instance, the computational overhead of training
multiple models increases latency and memory overhead. In contrast, model
merging approaches offer a promising alternative that does not require training
multiple models. In this work, we introduce model merging, specifically
weighted averaging, in KGE models. Herein, a running average of model
parameters from a training epoch onward is maintained and used for predictions.
To address this, we additionally propose an approach that selectively updates
the running average of the ensemble model parameters only when the
generalization performance improves on a validation dataset. We evaluate these
two different weighted averaging approaches on link prediction tasks, comparing
the state-of-the-art benchmark ensemble approach. Additionally, we evaluate the
weighted averaging approach considering literal-augmented KGE models and
multi-hop query answering tasks as well. The results demonstrate that the
proposed weighted averaging approach consistently improves performance across
diverse evaluation settings.

</details>


### [120] [A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks](https://arxiv.org/abs/2510.25366)
*Tomas Hrycej,Bernhard Bermeitinger,Massimo Pavone,Götz-Henrik Wiegand,Siegfried Handschuh*

Main category: cs.LG

TL;DR: 本文提出一种基于损失函数从非凸到凸特性的两阶段优化算法，实验证明该算法能提高收敛性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有非凸方法在损失函数局部凸区域效率低，期望利用损失函数从非凸到凸的特性改进优化算法。

Method: 提出两阶段优化算法，通过观察梯度范数与损失的依赖关系检测转换点，分别使用非凸（Adam）和凸（CG）算法。

Result: 计算实验证实简单的凸结构常见，可用于显著提高收敛性和准确性。

Conclusion: 利用损失函数从非凸到凸的特性设计的两阶段优化算法有效，能提高性能。

Abstract: The key task of machine learning is to minimize the loss function that
measures the model fit to the training data. The numerical methods to do this
efficiently depend on the properties of the loss function. The most decisive
among these properties is the convexity or non-convexity of the loss function.
The fact that the loss function can have, and frequently has, non-convex
regions has led to a widespread commitment to non-convex methods such as Adam.
However, a local minimum implies that, in some environment around it, the
function is convex. In this environment, second-order minimizing methods such
as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We
propose a novel framework grounded in the hypothesis that loss functions in
real-world tasks swap from initial non-convexity to convexity towards the
optimum. This is a property we leverage to design an innovative two-phase
optimization algorithm. The presented algorithm detects the swap point by
observing the gradient norm dependence on the loss. In these regions,
non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing
experiments confirm the hypothesis that this simple convexity structure is
frequent enough to be practically exploited to substantially improve
convergence and accuracy.

</details>


### [121] [A Deep Learning Framework for Multi-Operator Learning: Architectures and Approximation Theory](https://arxiv.org/abs/2510.25379)
*Adrien Weihs,Jingmin Sun,Zecheng Zhang,Hayden Schaeffer*

Main category: cs.LG

TL;DR: 本文研究学习算子集合问题，区分两种学习模式，提出新架构，给出理论结果和缩放定律，开发平衡架构复杂度框架，实验验证架构性能，为可扩展神经算子学习奠定基础。


<details>
  <summary>Details</summary>
Motivation: 科学应用需近似函数空间间的映射（即算子），现有机器学习多关注有限维空间映射，故研究学习算子集合问题。

Method: 区分多算子学习和多个单算子学习两种模式；多算子学习引入MNO和MONet架构并建立通用逼近结果、推导缩放定律；多个单算子学习开发平衡子网络架构复杂度的框架。

Result: 实证实验表明所提架构在参数化偏微分方程基准测试中有强大表达能力和效率。

Conclusion: 为跨多个算子的可扩展神经算子学习建立了统一的理论和实践基础。

Abstract: While many problems in machine learning focus on learning mappings between
finite-dimensional spaces, scientific applications require approximating
mappings between function spaces, i.e., operators. We study the problem of
learning collections of operators and provide both theoretical and empirical
advances. We distinguish between two regimes: (i) multiple operator learning,
where a single network represents a continuum of operators parameterized by a
parametric function, and (ii) learning several distinct single operators, where
each operator is learned independently. For the multiple operator case, we
introduce two new architectures, $\mathrm{MNO}$ and $\mathrm{MONet}$, and
establish universal approximation results in three settings: continuous,
integrable, or Lipschitz operators. For the latter, we further derive explicit
scaling laws that quantify how the network size must grow to achieve a target
approximation accuracy. For learning several single operators, we develop a
framework for balancing architectural complexity across subnetworks and show
how approximation order determines computational efficiency. Empirical
experiments on parametric PDE benchmarks confirm the strong expressive power
and efficiency of the proposed architectures. Overall, this work establishes a
unified theoretical and practical foundation for scalable neural operator
learning across multiple operators.

</details>


### [122] [GPTOpt: Towards Efficient LLM-Based Black-Box Optimization](https://arxiv.org/abs/2510.25404)
*Jamison Meindl,Yunsheng Tian,Tony Cui,Veronika Thost,Zhang-Wei Hong,Jie Chen,Wojciech Matusik,Mina Konaković Luković*

Main category: cs.LG

TL;DR: 介绍基于大语言模型的优化方法GPTOpt，在黑盒优化基准测试中超越传统优化器。


<details>
  <summary>Details</summary>
Motivation: 传统方法需仔细调参，大语言模型解决连续黑盒优化任务能力有限，需新方法提升样本效率。

Method: 在从不同贝叶斯优化参数化导出的大量合成数据集上微调大语言模型。

Result: 在各种黑盒优化基准测试中，GPTOpt超越传统优化器。

Conclusion: 大语言模型具备高级数值推理能力，GPTOpt提供无参数调优的全局优化灵活框架。

Abstract: Global optimization of expensive, derivative-free black-box functions demands
extreme sample efficiency. Classical methods such as Bayesian Optimization (BO)
can be effective, but they often require careful parameter tuning to each
application domain. At the same time, Large Language Models (LLMs) have shown
broad capabilities, yet state-of-the-art models remain limited in solving
continuous black-box optimization tasks. We introduce GPTOpt, an LLM-based
optimization method that equips LLMs with continuous black-box optimization
capabilities. By fine-tuning large language models on extensive synthetic
datasets derived from diverse BO parameterizations, GPTOpt leverages LLM
pre-training to generalize across optimization tasks. On a variety of black-box
optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting
the capacity of LLMs for advanced numerical reasoning and introducing a
flexible framework for global optimization without parameter tuning.

</details>


### [123] [Gradient-Weight Alignment as a Train-Time Proxy for Generalization in Classification Tasks](https://arxiv.org/abs/2510.25480)
*Florian A. Hölzl,Daniel Rueckert,Georgios Kaissis*

Main category: cs.LG

TL;DR: 本文介绍了Gradient - Weight Alignment (GWA) 指标，可在无验证集情况下从训练数据进行模型分析，能预测早停点、比较模型和识别有影响的训练样本。


<details>
  <summary>Details</summary>
Motivation: 在监督分类场景下，探索训练数据和模型权重的交互能否产生跟踪泛化能力并将性能归因于单个训练样本的验证指标。

Method: 引入Gradient - Weight Alignment (GWA) 指标，量化样本梯度和模型权重的一致性。

Result: 有效学习对应一致对齐，不一致表示泛化能力下降；GWA可在训练中高效计算，反映样本特定贡献和数据集整体学习动态；实验表明GWA能准确预测最佳早停点、进行模型比较和识别有影响的训练样本。

Conclusion: GWA提供了一种无验证集的模型分析方法，可直接从训练数据进行模型分析。

Abstract: Robust validation metrics remain essential in contemporary deep learning, not
only to detect overfitting and poor generalization, but also to monitor
training dynamics. In the supervised classification setting, we investigate
whether interactions between training data and model weights can yield such a
metric that both tracks generalization during training and attributes
performance to individual training samples. We introduce Gradient-Weight
Alignment (GWA), quantifying the coherence between per-sample gradients and
model weights. We show that effective learning corresponds to coherent
alignment, while misalignment indicates deteriorating generalization. GWA is
efficiently computable during training and reflects both sample-specific
contributions and dataset-wide learning dynamics. Extensive experiments show
that GWA accurately predicts optimal early stopping, enables principled model
comparisons, and identifies influential training samples, providing a
validation-set-free approach for model analysis directly from the training
data.

</details>


### [124] [Right for the Right Reasons: Avoiding Reasoning Shortcuts via Prototypical Neurosymbolic AI](https://arxiv.org/abs/2510.25497)
*Luca Andolfi,Eleonora Giunchiglia*

Main category: cs.LG

TL;DR: 本文提出原型神经符号架构，利用原型学习理论避免推理捷径，在低数据场景验证有效，为安全可靠的神经符号学习提供策略。


<details>
  <summary>Details</summary>
Motivation: 解决神经符号AI中易出现的捷径推理问题，即模型利用虚假关联满足符号约束，而非学习正确概念。

Method: 引入原型神经符号架构，利用原型学习理论，训练模型满足背景知识，同时考虑输入与少量标记数据点的相似性。

Result: 在rsbench基准套件的多种设置和任务中，在合成任务和现实高风险任务上显著提升学习正确概念的能力。

Conclusion: 原型基础是一种有效、注释高效的安全可靠神经符号学习策略。

Abstract: Neurosymbolic AI is growing in popularity thanks to its ability to combine
neural perception and symbolic reasoning in end-to-end trainable models.
However, recent findings reveal these are prone to shortcut reasoning, i.e., to
learning unindented concepts--or neural predicates--which exploit spurious
correlations to satisfy the symbolic constraints. In this paper, we address
reasoning shortcuts at their root cause and we introduce prototypical
neurosymbolic architectures. These models are able to satisfy the symbolic
constraints (be right) because they have learnt the correct basic concepts (for
the right reasons) and not because of spurious correlations, even in extremely
low data regimes. Leveraging the theory of prototypical learning, we
demonstrate that we can effectively avoid reasoning shortcuts by training the
models to satisfy the background knowledge while taking into account the
similarity of the input with respect to the handful of labelled datapoints. We
extensively validate our approach on the recently proposed rsbench benchmark
suite in a variety of settings and tasks with very scarce supervision: we show
significant improvements in learning the right concepts both in synthetic tasks
(MNIST-EvenOdd and Kand-Logic) and real-world, high-stake ones (BDD-OIA). Our
findings pave the way to prototype grounding as an effective,
annotation-efficient strategy for safe and reliable neurosymbolic learning.

</details>


### [125] [Support Vector Machine-Based Burnout Risk Prediction with an Interactive Interface for Organizational Use](https://arxiv.org/abs/2510.25509)
*Bruno W. G. Teodosio,Mário J. O. T. Lira,Pedro H. M. Araújo,Lucas R. C. Farias*

Main category: cs.LG

TL;DR: 本文提出用机器学习方法预测倦怠风险，评估三种算法，SVM 表现最佳，还开发交互界面，凸显机器学习在组织中早期检测倦怠的潜力。


<details>
  <summary>Details</summary>
Motivation: 倦怠对个人和组织有显著影响，需预测倦怠风险的方法。

Method: 使用 HackerEarth 员工倦怠挑战数据集，评估 KNN、随机森林和 SVM 三种监督算法，用 30 折交叉验证和决定系数（R2）评估模型性能，通过配对 t 检验比较模型，用 Streamlit 开发交互界面。

Result: SVM 预测性能最高（R2 = 0.84），在配对 t 检验中统计学上优于 KNN 和随机森林。

Conclusion: 机器学习有潜力支持倦怠早期检测，推动组织环境中数据驱动的心理健康策略。

Abstract: Burnout is a psychological syndrome marked by emotional exhaustion,
depersonalization, and reduced personal accomplishment, with a significant
impact on individual well-being and organizational performance. This study
proposes a machine learning approach to predict burnout risk using the
HackerEarth Employee Burnout Challenge dataset. Three supervised algorithms
were evaluated: nearest neighbors (KNN), random forest, and support vector
machine (SVM), with model performance evaluated through 30-fold
cross-validation using the determination coefficient (R2). Among the models
tested, SVM achieved the highest predictive performance (R2 = 0.84) and was
statistically superior to KNN and Random Forest based on paired $t$-tests. To
ensure practical applicability, an interactive interface was developed using
Streamlit, allowing non-technical users to input data and receive burnout risk
predictions. The results highlight the potential of machine learning to support
early detection of burnout and promote data-driven mental health strategies in
organizational settings.

</details>


### [126] [FaCT: Faithful Concept Traces for Explaining Neural Network Decisions](https://arxiv.org/abs/2510.25512)
*Amin Parchami-Araghi,Sukrut Rao,Jonas Fischer,Bernt Schiele*

Main category: cs.LG

TL;DR: 论文强调概念解释的忠实性，提出带固有机制概念解释的新模型和概念一致性指标C² - Score，新模型概念更一致、可解释且在ImageNet上表现有竞争力。


<details>
  <summary>Details</summary>
Motivation: 深度网络虽表现出色，但缺乏全局概念级理解，现有基于概念的事后解释方法不忠实于模型且有诸多限制。

Method: 提出带模型固有机制概念解释的新模型，利用基础模型提出概念一致性指标C² - Score。

Result: 与先前工作相比，新模型的概念在数量上更一致，用户认为更具可解释性，同时在ImageNet上保持有竞争力的性能。

Conclusion: 所提新模型和指标在概念解释的一致性、可解释性和性能方面表现良好。

Abstract: Deep networks have shown remarkable performance across a wide range of tasks,
yet getting a global concept-level understanding of how they function remains a
key challenge. Many post-hoc concept-based approaches have been introduced to
understand their workings, yet they are not always faithful to the model.
Further, they make restrictive assumptions on the concepts a model learns, such
as class-specificity, small spatial extent, or alignment to human expectations.
In this work, we put emphasis on the faithfulness of such concept-based
explanations and propose a new model with model-inherent mechanistic
concept-explanations. Our concepts are shared across classes and, from any
layer, their contribution to the logit and their input-visualization can be
faithfully traced. We also leverage foundation models to propose a new
concept-consistency metric, C$^2$-Score, that can be used to evaluate
concept-based methods. We show that, compared to prior work, our concepts are
quantitatively more consistent and users find our concepts to be more
interpretable, all while retaining competitive ImageNet performance.

</details>


### [127] [Transformers Provably Learn Directed Acyclic Graphs via Kernel-Guided Mutual Information](https://arxiv.org/abs/2510.25542)
*Yuan Cheng,Yu Huang,Zhe Xiong,Yingbin Liang,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 本文提出核引导互信息（KG - MI）解决在有向无环图（DAG）中训练transformer模型理论保证问题，证明单头多层transformer训练收敛性，实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 现有基于transformer的图模型对训练动态的理论理解局限于树状图，扩展到更一般的有向无环图（DAG）存在挑战。

Method: 引入基于f - 散度的核引导互信息（KG - MI），将其与多头注意力框架结合，每个头关联一个不同的边缘转移核。

Result: 证明在K - 父DAG生成的序列上，通过梯度上升训练单层多头transformer能在多项式时间收敛到全局最优，收敛时能刻画注意力得分模式，使用KL散度时能恢复底层图结构。实验验证理论发现。

Conclusion: 提出的方法有效解决了在DAG中训练transformer模型的理论保证问题，可恢复底层图结构。

Abstract: Uncovering hidden graph structures underlying real-world data is a critical
challenge with broad applications across scientific domains. Recently,
transformer-based models leveraging the attention mechanism have demonstrated
strong empirical success in capturing complex dependencies within graphs.
However, the theoretical understanding of their training dynamics has been
limited to tree-like graphs, where each node depends on a single parent.
Extending provable guarantees to more general directed acyclic graphs (DAGs) --
which involve multiple parents per node -- remains challenging, primarily due
to the difficulty in designing training objectives that enable different
attention heads to separately learn multiple different parent relationships.
  In this work, we address this problem by introducing a novel
information-theoretic metric: the kernel-guided mutual information (KG-MI),
based on the $f$-divergence. Our objective combines KG-MI with a multi-head
attention framework, where each head is associated with a distinct marginal
transition kernel to model diverse parent-child dependencies effectively. We
prove that, given sequences generated by a $K$-parent DAG, training a
single-layer, multi-head transformer via gradient ascent converges to the
global optimum in polynomial time. Furthermore, we characterize the attention
score patterns at convergence. In addition, when particularizing the
$f$-divergence to the KL divergence, the learned attention scores accurately
reflect the ground-truth adjacency matrix, thereby provably recovering the
underlying graph structure. Experimental results validate our theoretical
findings.

</details>


### [128] [Hybrid Quantum-Classical Recurrent Neural Networks](https://arxiv.org/abs/2510.25557)
*Wenduan Xu*

Main category: cs.LG

TL;DR: 提出混合量子 - 经典循环神经网络（QRNN）架构，在多任务模拟中评估，还设计注意力机制用于机器翻译，是首个在序列学习任务中表现有竞争力的基于量子操作的模型。


<details>
  <summary>Details</summary>
Motivation: 构建结合量子与经典特性、具有高容量记忆、部分观测和非线性经典控制的循环神经网络，以在序列学习任务中取得良好表现。

Method: 将循环核心实现为参数化量子电路（PQC），由经典前馈网络控制；在每个时间步结合电路中读数与输入嵌入，经前馈网络处理后参数化 PQC 更新隐藏状态；在模拟中采用投影测量获取读数；设计软注意力机制。

Result: 在多达 14 个量子比特的模拟中，在情感分析、MNIST 等多个序列学习任务中取得与强经典基线有竞争力的表现；软注意力机制在机器翻译中有效。

Conclusion: 该 QRNN 架构紧凑且物理一致，是首个在广泛序列学习任务中基于量子操作达到有竞争力性能的模型。

Abstract: We present a hybrid quantum-classical recurrent neural network (QRNN)
architecture in which the entire recurrent core is realized as a parametrized
quantum circuit (PQC) controlled by a classical feedforward network. The hidden
state is the quantum state of an $n$-qubit PQC, residing in an exponentially
large Hilbert space $\mathbb{C}^{2^n}$. The PQC is unitary by construction,
making the hidden-state evolution norm-preserving without external constraints.
At each timestep, mid-circuit readouts are combined with the input embedding
and processed by the feedforward network, which provides explicit classical
nonlinearity. The outputs parametrize the PQC, which updates the hidden state
via unitary dynamics. The QRNN is compact and physically consistent, and it
unifies (i) unitary recurrence as a high-capacity memory, (ii) partial
observation via mid-circuit measurements, and (iii) nonlinear classical control
for input-conditioned parametrization. We evaluate the model in simulation with
up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory,
and language modeling, adopting projective measurements as a limiting case to
obtain mid-circuit readouts while maintaining a coherent recurrent quantum
memory. We further devise a soft attention mechanism over the mid-circuit
readouts in a sequence-to-sequence model and show its effectiveness for machine
translation. To our knowledge, this is the first model (RNN or otherwise)
grounded in quantum operations to achieve competitive performance against
strong classical baselines across a broad class of sequence-learning tasks.

</details>


### [129] [Leveraging an Atmospheric Foundational Model for Subregional Sea Surface Temperature Forecasting](https://arxiv.org/abs/2510.25563)
*Víctor Medina,Giovanny A. Cuervo-Londoño,Javier Sánchez*

Main category: cs.LG

TL;DR: 本文将大气预报深度学习模型Aurora用于加那利上升流系统海表温度预测，经微调后模型能捕捉复杂时空模式，降低计算需求，实验有较好结果，但在沿海细节捕捉有挑战，未来有改进方向。


<details>
  <summary>Details</summary>
Motivation: 传统海洋预报依赖数值模型，存在计算成本和可扩展性的局限，需要新方法准确预测海洋变量，助力气候变化研究、海洋资源管理和海上活动优化。

Method: 将Aurora模型用于预测海表温度，采用分阶段微调，结合纬度加权误差指标，优化超参数。

Result: 模型的RMSE为0.119K，ACC约为0.997，能重现大规模海表温度结构，但在沿海区域捕捉细节有挑战。

Conclusion: 证明了使用不同领域预训练的深度学习模型用于海洋应用的可行性，未来可通过整合更多海洋变量、提高空间分辨率等改进。

Abstract: The accurate prediction of oceanographic variables is crucial for
understanding climate change, managing marine resources, and optimizing
maritime activities. Traditional ocean forecasting relies on numerical models;
however, these approaches face limitations in terms of computational cost and
scalability. In this study, we adapt Aurora, a foundational deep learning model
originally designed for atmospheric forecasting, to predict sea surface
temperature (SST) in the Canary Upwelling System. By fine-tuning this model
with high-resolution oceanographic reanalysis data, we demonstrate its ability
to capture complex spatiotemporal patterns while reducing computational
demands. Our methodology involves a staged fine-tuning process, incorporating
latitude-weighted error metrics and optimizing hyperparameters for efficient
learning. The experimental results show that the model achieves a low RMSE of
0.119K, maintaining high anomaly correlation coefficients (ACC $\approx
0.997$). The model successfully reproduces large-scale SST structures but faces
challenges in capturing finer details in coastal regions. This work contributes
to the field of data-driven ocean forecasting by demonstrating the feasibility
of using deep learning models pre-trained in different domains for oceanic
applications. Future improvements include integrating additional oceanographic
variables, increasing spatial resolution, and exploring physics-informed neural
networks to enhance interpretability and understanding. These advancements can
improve climate modeling and ocean prediction accuracy, supporting
decision-making in environmental and economic sectors.

</details>


### [130] [A Framework for Bounding Deterministic Risk with PAC-Bayes: Applications to Majority Votes](https://arxiv.org/abs/2510.25569)
*Benjamin Leblanc,Pascal Germain*

Main category: cs.LG

TL;DR: 提出统一框架从随机PAC - Bayesian保证中提取单假设保证，实验表明该方法在确定性分类器泛化界上优于流行基线。


<details>
  <summary>Details</summary>
Motivation: 经典PAC - Bayes仅能对随机采样假设的期望风险提供保证，需随机预测，在许多需部署单个确定性假设的实际场景中无法使用。

Method: 提出统一框架，给出一般oracle界，由此推导出数值界和多数投票的特例。

Result: 实验显示该方法在确定性分类器泛化界上始终优于流行基线，最多可高出2倍。

Conclusion: 所提统一框架能有效从随机PAC - Bayesian保证中提取单假设保证，提升确定性分类器泛化界表现。

Abstract: PAC-Bayes is a popular and efficient framework for obtaining generalization
guarantees in situations involving uncountable hypothesis spaces.
Unfortunately, in its classical formulation, it only provides guarantees on the
expected risk of a randomly sampled hypothesis. This requires stochastic
predictions at test time, making PAC-Bayes unusable in many practical
situations where a single deterministic hypothesis must be deployed. We propose
a unified framework to extract guarantees holding for a single hypothesis from
stochastic PAC-Bayesian guarantees. We present a general oracle bound and
derive from it a numerical bound and a specialization to majority vote. We
empirically show that our approach consistently outperforms popular baselines
(by up to a factor of 2) when it comes to generalization bounds on
deterministic classifiers.

</details>


### [131] [Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for Local Learning](https://arxiv.org/abs/2510.25594)
*Arani Roy,Marco P. Apolinario,Shristi Das Biswas,Kaushik Roy*

Main category: cs.LG

TL;DR: 提出在低秩流形上的结构化局部学习框架，减少可训练参数，在多数据集实验中达到与BP相当的准确率。


<details>
  <summary>Details</summary>
Motivation: BP训练DNNs有内存和计算开销，DFA存在无结构反馈和可扩展性差的问题，需提出新方法解决。

Method: 提出在权重矩阵SVD定义的低秩流形上操作的结构化局部学习框架，各层以分解形式训练，用复合损失更新SVD组件，构建匹配SVD结构的反馈矩阵。

Result: 在CIFAR - 10、CIFAR - 100和ImageNet上实验达到与BP相当的准确率，消融研究证实低秩设置中各损失项的重要性。

Conclusion: 低秩流形上的局部学习是全秩基于梯度训练的可行且可扩展的替代方法。

Abstract: Training deep neural networks (DNNs) with backpropagation (BP) achieves
state-of-the-art accuracy but requires global error propagation and full
parameterization, leading to substantial memory and computational overhead.
Direct Feedback Alignment (DFA) enables local, parallelizable updates with
lower memory requirements but is limited by unstructured feedback and poor
scalability in deeper architectures, specially convolutional neural networks.
To address these limitations, we propose a structured local learning framework
that operates directly on low-rank manifolds defined by the Singular Value
Decomposition (SVD) of weight matrices. Each layer is trained in its decomposed
form, with updates applied to the SVD components using a composite loss that
integrates cross-entropy, subspace alignment, and orthogonality regularization.
Feedback matrices are constructed to match the SVD structure, ensuring
consistent alignment between forward and feedback pathways. Our method reduces
the number of trainable parameters relative to the original DFA model, without
relying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100,
and ImageNet show that our method achieves accuracy comparable to that of BP.
Ablation studies confirm the importance of each loss term in the low-rank
setting. These results establish local learning on low-rank manifolds as a
principled and scalable alternative to full-rank gradient-based training.

</details>


### [132] [Uncertainty Quantification for Regression: A Unified Framework based on kernel scores](https://arxiv.org/abs/2510.25599)
*Christopher Bülte,Yusuf Sale,Gitta Kutyniok,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 本文引入基于适当评分规则的不确定性度量族，统一已知度量并给出设计新度量的方法，证明特征对应关系，实验证明其有效性及权衡关系。


<details>
  <summary>Details</summary>
Motivation: 回归任务需不确定性量化，但现有文献多聚焦分类，因此要提出新的不确定性度量方法。

Method: 基于适当评分规则引入不确定性度量族，重点关注核分数，证明核分数特征与下游行为的对应关系。

Result: 实验表明这些度量在下游任务中有效，且不同实例化之间存在权衡关系。

Conclusion: 提出的度量族能统一已知度量，为设计特定任务的度量提供指导。

Abstract: Regression tasks, notably in safety-critical domains, require proper
uncertainty quantification, yet the literature remains largely
classification-focused. In this light, we introduce a family of measures for
total, aleatoric, and epistemic uncertainty based on proper scoring rules, with
a particular emphasis on kernel scores. The framework unifies several
well-known measures and provides a principled recipe for designing new ones
whose behavior, such as tail sensitivity, robustness, and out-of-distribution
responsiveness, is governed by the choice of kernel. We prove explicit
correspondences between kernel-score characteristics and downstream behavior,
yielding concrete design guidelines for task-specific measures. Extensive
experiments demonstrate that these measures are effective in downstream tasks
and reveal clear trade-offs among instantiations, including robustness and
out-of-distribution detection performance.

</details>


### [133] [INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats](https://arxiv.org/abs/2510.25602)
*Mengzhao Chen,Meng Wu,Hui Jin,Zhihang Yuan,Jing Liu,Chaoyi Zhang,Yunshui Li,Jie Huang,Jin Ma,Zeyue Xue,Zhiheng Liu,Xingyan Bin,Ping Luo*

Main category: cs.LG

TL;DR: 本文系统比较FP和INT量化格式，发现不同粒度下两者表现不同，如8位细粒度MXINT8更优，4位FP常具精度优势，还提出对称裁剪法解决INT训练梯度偏差，表明细粒度INT格式更适合未来AI加速器。


<details>
  <summary>Details</summary>
Motivation: 行业趋势下缺少FP和INT量化在不同粒度的统一比较，算法和硬件协同设计缺乏明确指导。

Method: 系统研究FP和INT格式的权衡。

Result: 粗粒度量化FP表现好，细粒度下8位MXINT8在算法精度和硬件效率上优于FP；4位FP常具精度优势，但应用Hadamard旋转等技术后NVINT4可超NVFP4；提出对称裁剪法使MXINT8训练近乎无损。

Conclusion: 通用FP方法并非最优，细粒度INT格式（尤其是MXINT8）能更好平衡精度、功耗和效率，适合未来AI加速器。

Abstract: Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly
embracing low-precision floating-point (FP) formats to handle the pervasive
activation outliers in Large Language Models (LLMs). Despite this industry
trend, a unified comparison of FP and integer (INT) quantization across varying
granularities has been missing, leaving algorithm and hardware co-design
without clear guidance. This paper fills that gap by systematically
investigating the trade-offs between FP and INT formats. We reveal a critical
performance crossover: while FP excels in coarse-grained quantization, the
comparison at fine-grained (block-wise) levels is more nuanced. Our
comprehensive comparison demonstrates that for popular 8-bit fine-grained
formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart
in both algorithmic accuracy and hardware efficiency. However, for 4-bit
formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we
show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like
Hadamard rotation are applied. We also introduce a symmetric clipping method
that resolves gradient bias in fine-grained low-bit INT training, enabling
nearly lossless performance for MXINT8 training. These findings challenge the
current hardware trajectory, demonstrating that a one-size-fits-all FP approach
is suboptimal and advocating that fine-grained INT formats, particularly
MXINT8, offer a better balance of accuracy, power, and efficiency for future AI
accelerators.

</details>


### [134] [BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training](https://arxiv.org/abs/2510.25609)
*Mohammadreza Tavasoli Naeini,Ali Bereyhi,Morteza Noshad,Ben Liang,Alfred O. Hero III*

Main category: cs.LG

TL;DR: 提出BOLT - GAN，在图像生成基准测试中表现优于WGAN，表明BOLT可提升GAN训练。


<details>
  <summary>Details</summary>
Motivation: 改进WGAN框架，提升训练稳定性和性能。

Method: 受BOLT启发对WGAN框架进行简单有效修改，使用Lipschitz连续判别器。

Result: 在四个标准图像生成基准测试中，BOLT - GAN始终优于WGAN，FID降低10 - 60%。

Conclusion: BOLT是提升GAN训练的广泛适用原则。

Abstract: We introduce BOLT-GAN, a simple yet effective modification of the WGAN
framework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that
with a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a
different metric distance than the Earth Mover (Wasserstein) distance and
achieves better training stability. Empirical evaluations on four standard
image generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN
Church-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60%
lower Frechet Inception Distance (FID). Our results suggest that BOLT is a
broadly applicable principle for enhancing GAN training.

</details>


### [135] [Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization](https://arxiv.org/abs/2510.25616)
*Nikita Kachaev,Mikhail Kolosov,Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 对VLA微调时表示保留进行系统研究，发现朴素动作微调会导致视觉表示退化，提出缓解方法并评估策略。


<details>
  <summary>Details</summary>
Motivation: 当预训练视觉语言模型适应动作模态时，不清楚其原始视觉语言表示和知识的保留程度。

Method: 探测VLA隐藏表示、分析注意力图，设计对比任务和方法，评估视觉表示对齐策略并引入缓解退化方法。

Result: 明确了动作微调与视觉语言表示退化的权衡，提出的方法缓解了退化并提高了分布外泛化能力。

Conclusion: 分析阐明了动作微调与视觉语言表示退化的关系，强调了恢复继承视觉语言能力的实用方法。

Abstract: The growing success of Vision-Language-Action (VLA) models stems from the
promise that pretrained Vision-Language Models (VLMs) can endow agents with
transferable world knowledge and vision-language (VL) grounding, laying a
foundation for action models with broader generalization. Yet when these VLMs
are adapted to the action modality, it remains unclear to what extent their
original VL representations and knowledge are preserved. In this work, we
conduct a systematic study of representation retention during VLA fine-tuning,
showing that naive action fine-tuning leads to degradation of visual
representations. To characterize and measure these effects, we probe VLA's
hidden representations and analyze attention maps, further, we design a set of
targeted tasks and methods that contrast VLA models with their counterpart
VLMs, isolating changes in VL capabilities induced by action fine-tuning. We
further evaluate a range of strategies for aligning visual representations and
introduce a simple yet effective method that mitigates degradation and yields
improved generalization to out-of-distribution (OOD) scenarios. Taken together,
our analysis clarifies the trade-off between action fine-tuning and the
degradation of VL representations and highlights practical approaches to
recover inherited VL capabilities. Code is publicly available:
https://blind-vla-paper.github.io

</details>


### [136] [Subgraph Federated Learning via Spectral Methods](https://arxiv.org/abs/2510.25657)
*Javad Aliakbari,Johan Östman,Ashkan Panahi,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: 本文针对分布式图结构数据的联邦学习问题，提出FedLap框架，兼顾隐私性和可扩展性，实验显示其效用佳。


<details>
  <summary>Details</summary>
Motivation: 现有分布式图结构数据联邦学习方法存在隐私风险或计算量大、可扩展性差的问题。

Method: 提出FedLap框架，在谱域通过拉普拉斯平滑利用全局结构信息。

Result: 对FedLap进行隐私分析，证明其保护隐私，是首个有强隐私保证的子图联邦学习方案；实验表明FedLap效用优于现有技术。

Conclusion: FedLap能有效处理分布式图结构数据的联邦学习问题，兼顾隐私和可扩展性。

Abstract: We consider the problem of federated learning (FL) with graph-structured data
distributed across multiple clients. In particular, we address the prevalent
scenario of interconnected subgraphs, where interconnections between clients
significantly influence the learning process. Existing approaches suffer from
critical limitations, either requiring the exchange of sensitive node
embeddings, thereby posing privacy risks, or relying on
computationally-intensive steps, which hinders scalability. To tackle these
challenges, we propose FedLap, a novel framework that leverages global
structure information via Laplacian smoothing in the spectral domain to
effectively capture inter-node dependencies while ensuring privacy and
scalability. We provide a formal analysis of the privacy of FedLap,
demonstrating that it preserves privacy. Notably, FedLap is the first subgraph
FL scheme with strong privacy guarantees. Extensive experiments on benchmark
datasets demonstrate that FedLap achieves competitive or superior utility
compared to existing techniques.

</details>


### [137] [Mechanistic Interpretability of RNNs emulating Hidden Markov Models](https://arxiv.org/abs/2510.25674)
*Elia Torre,Michele Viscione,Lucas Pompe,Benjamin F Grewe,Valerio Mante*

Main category: cs.LG

TL;DR: 研究RNN如何模拟自然行为的离散潜在动力学，发现其能复制HMM发射统计，揭示内在机制且该机制有通用性。


<details>
  <summary>Details</summary>
Motivation: 过往RNN研究多针对简单行为，对其模拟自然场景中丰富、自发和随机行为的机制了解甚少，而HMM建模揭示自然行为可分割为离散潜在状态，与RNN连续状态空间似有冲突，因此开展研究。

Method: 先让RNN复制HMM发射统计，再对训练好的网络进行逆向工程，分析其活动和连接性，还对多种HMM架构进行分析。

Result: 无输入时，训练好的RNN活动收敛到一个固定点；随机输入驱动时，轨迹沿闭合轨道呈现噪声维持的动态，旋转调制发射概率，由快慢过渡控制；网络发展出高度结构化连接，有“踢神经元”启动区域间过渡；该机制在训练中通过随机共振实现概率计算。

Conclusion: RNN能通过模块化重用相同动态基序模拟复杂离散潜在动力学，存在组合原则。

Abstract: Recurrent neural networks (RNNs) provide a powerful approach in neuroscience
to infer latent dynamics in neural populations and to generate hypotheses about
the neural computations underlying behavior. However, past work has focused on
relatively simple, input-driven, and largely deterministic behaviors - little
is known about the mechanisms that would allow RNNs to generate the richer,
spontaneous, and potentially stochastic behaviors observed in natural settings.
Modeling with Hidden Markov Models (HMMs) has revealed a segmentation of
natural behaviors into discrete latent states with stochastic transitions
between them, a type of dynamics that may appear at odds with the continuous
state spaces implemented by RNNs. Here we first show that RNNs can replicate
HMM emission statistics and then reverse-engineer the trained networks to
uncover the mechanisms they implement. In the absence of inputs, the activity
of trained RNNs collapses towards a single fixed point. When driven by
stochastic input, trajectories instead exhibit noise-sustained dynamics along
closed orbits. Rotation along these orbits modulates the emission probabilities
and is governed by transitions between regions of slow, noise-driven dynamics
connected by fast, deterministic transitions. The trained RNNs develop highly
structured connectivity, with a small set of "kick neurons" initiating
transitions between these regions. This mechanism emerges during training as
the network shifts into a regime of stochastic resonance, enabling it to
perform probabilistic computations. Analyses across multiple HMM architectures
- fully connected, cyclic, and linear-chain - reveal that this solution
generalizes through the modular reuse of the same dynamical motif, suggesting a
compositional principle by which RNNs can emulate complex discrete latent
dynamics.

</details>


### [138] [Convolutional Spiking-based GRU Cell for Spatio-temporal Data](https://arxiv.org/abs/2510.25696)
*Yesmine Abdennadher,Eleonora Cicciarella,Michele Rossi*

Main category: cs.LG

TL;DR: 本文提出卷积脉冲GRU（CS - GRU）单元，结合卷积操作、脉冲神经元和GRU门控机制，在多种数据集上表现出色，超越现有GRU变体。


<details>
  <summary>Details</summary>
Motivation: 传统RNN处理长序列时丢失局部细节，现有方法无法捕捉基于事件的时空数据中的细粒度局部依赖。

Method: 引入CS - GRU单元，利用卷积操作保留局部结构和依赖，结合脉冲神经元的时间精度和GRU的高效门控机制。

Result: CS - GRU平均比现有GRU变体性能高4.35%，在顺序任务上准确率超90%，MNIST上达99.31%，比SpikGRU效率高69%。

Conclusion: CS - GRU是一个用于顺序数据处理的有效且高效的框架。

Abstract: Spike-based temporal messaging enables SNNs to efficiently process both
purely temporal and spatio-temporal time-series or event-driven data. Combining
SNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,
gives rise to a robust framework for sequential data processing; however,
traditional RNNs often lose local details when handling long sequences.
Previous approaches, such as SpikGRU, fail to capture fine-grained local
dependencies in event-based spatio-temporal data. In this paper, we introduce
the Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional
operations to preserve local structure and dependencies while integrating the
temporal precision of spiking neurons with the efficient gating mechanisms of
GRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,
SHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our
experiments show that CS-GRU outperforms state-of-the-art GRU variants by an
average of 4.35%, achieving over 90% accuracy on sequential tasks and up to
99.31% on MNIST. It is worth noting that our solution achieves 69% higher
efficiency compared to SpikGRU. The code is available at:
https://github.com/YesmineAbdennadher/CS-GRU.

</details>


### [139] [LieSolver: A PDE-constrained solver for IBVPs using Lie symmetries](https://arxiv.org/abs/2510.25731)
*René P. Klausen,Ivan Timofeev,Johannes Frank,Jonas Naujoks,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: 介绍一种利用李对称解决初边值问题的方法，比PINNs更快更准，提升计算效率和预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 高效解决初边值问题，提升计算效率和预测可靠性。

Method: 利用李对称构建模型，使其固有地包含物理定律并从初边值数据学习解。

Result: 实现LieSolver，应用于线性齐次PDEs，比PINNs更快更准确。

Conclusion: 该方法提高了PDE约束问题的计算效率和预测可靠性。

Abstract: We introduce a method for efficiently solving initial-boundary value problems
(IBVPs) that uses Lie symmetries to enforce the associated partial differential
equation (PDE) exactly by construction. By leveraging symmetry transformations,
the model inherently incorporates the physical laws and learns solutions from
initial and boundary data. As a result, the loss directly measures the model's
accuracy, leading to improved convergence. Moreover, for well-posed IBVPs, our
method enables rigorous error estimation. The approach yields compact models,
facilitating an efficient optimization. We implement LieSolver and demonstrate
its application to linear homogeneous PDEs with a range of initial conditions,
showing that it is faster and more accurate than physics-informed neural
networks (PINNs). Overall, our method improves both computational efficiency
and the reliability of predictions for PDE-constrained problems.

</details>


### [140] [MLPrE -- A tool for preprocessing and exploratory data analysis prior to machine learning model construction](https://arxiv.org/abs/2510.25755)
*David S Maxwell,Michael Darkoh,Sidharth R Samudrala,Caroline Chung,Stephanie T Schmidt,Bissan Al-Lazikani*

Main category: cs.LG

TL;DR: 随着深度学习发展，需数据预处理工具，提出 MLPrE 工具，利用 SparkDataFrames 等实现多功能，适用于不同数据集，满足机器学习需求，加速简化工作流早期开发。


<details>
  <summary>Details</summary>
Motivation: 深度学习发展使数据处理需求增加，现有工作流存在开销大、可扩展性不足问题，需强大、可扩展、轻量级的数据预处理工具。

Method: 使用 SparkDataFrames 存储数据确保可扩展性，采用通用 JSON 输入文件格式描述对 DataFrame 的逐步更改，实现输入输出、过滤等 69 个阶段。

Result: 用六个不同数据集展示关键阶段，利用 UniProt 术语数据集展示独立处理多字段能力，用葡萄酒质量数据展示聚类阶段，用磷酸化位点激酶数据展示为图数据库准备数据。

Conclusion: MLPrE 是通用可扩展的数据预处理和早期数据分析工具，满足机器学习需求，加速简化大型工作流早期开发。

Abstract: With the recent growth of Deep Learning for AI, there is a need for tools to
meet the demand of data flowing into those models. In some cases, source data
may exist in multiple formats, and therefore the source data must be
investigated and properly engineered for a Machine Learning model or graph
database. Overhead and lack of scalability with existing workflows limit
integration within a larger processing pipeline such as Apache Airflow, driving
the need for a robust, extensible, and lightweight tool to preprocess arbitrary
datasets that scales with data type and size. To address this, we present
Machine Learning Preprocessing and Exploratory Data Analysis, MLPrE, in which
SparkDataFrames were utilized to hold data during processing and ensure
scalability. A generalizable JSON input file format was utilized to describe
stepwise changes to that DataFrame. Stages were implemented for input and
output, filtering, basic statistics, feature engineering, and exploratory data
analysis. A total of 69 stages were implemented into MLPrE, of which we
highlight and demonstrate key stages using six diverse datasets. We further
highlight MLPrE's ability to independently process multiple fields in flat
files and recombine them, otherwise requiring an additional pipeline, using a
UniProt glossary term dataset. Building on this advantage, we demonstrated the
clustering stage with available wine quality data. Lastly, we demonstrate the
preparation of data for a graph database in the final stages of MLPrE using
phosphosite kinase data. Overall, our MLPrE tool offers a generalizable and
scalable tool for preprocessing and early data analysis, filling a critical
need for such a tool given the ever expanding use of machine learning. This
tool serves to accelerate and simplify early stage development in larger
workflows.

</details>


### [141] [Synthetic Data Reveals Generalization Gaps in Correlated Multiple Instance Learning](https://arxiv.org/abs/2510.25759)
*Ethan Harvey,Dennis Johan Loevlie,Michael C. Hughes*

Main category: cs.LG

TL;DR: 指出传统MIL方法忽略上下文关系局限，设计合成分类任务展示其性能不佳，新相关MIL方法泛化能力也有限。


<details>
  <summary>Details</summary>
Motivation: 传统MIL方法处理实例时忽略上下文关系，而上下文关系在实际应用中可能很关键。

Method: 设计一个合成分类任务，将现成MIL方法与最优贝叶斯估计器的性能进行量化对比。

Result: 现成MIL方法存在局限性，新的相关MIL方法在训练大量实例时泛化能力仍不佳。

Conclusion: 传统MIL方法有不足，新的相关MIL方法也有待改进。

Abstract: Multiple instance learning (MIL) is often used in medical imaging to classify
high-resolution 2D images by processing patches or classify 3D volumes by
processing slices. However, conventional MIL approaches treat instances
separately, ignoring contextual relationships such as the appearance of nearby
patches or slices that can be essential in real applications. We design a
synthetic classification task where accounting for adjacent instance features
is crucial for accurate prediction. We demonstrate the limitations of
off-the-shelf MIL approaches by quantifying their performance compared to the
optimal Bayes estimator for this task, which is available in closed-form. We
empirically show that newer correlated MIL methods still struggle to generalize
as well as possible when trained from scratch on tens of thousands of
instances.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [142] [Exponential Dynamic Energy Network for High Capacity Sequence Memory](https://arxiv.org/abs/2510.24965)
*Arjun Karuvally,Pichsinee Lertsaroj,Terrence J. Sejnowski,Hava T. Siegelmann*

Main category: cs.NE

TL;DR: 提出指数动态能量网络（EDEN）扩展能量范式到时间域，可实现强大的序列记忆能力，优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于能量范式的模型在建模序列记忆方面不足，需要扩展到时间域。

Method: 引入EDEN架构，结合静态高容量能量网络与慢的、不对称相互作用的调节群体，推导短时间尺度能量函数。

Result: EDEN实现指数级序列记忆容量$O(γ^N)$，优于传统模型的线性容量$O(N)$，其动态与人类大脑在情景记忆任务中的细胞活动相似。

Conclusion: EDEN在动态能量框架下统一静态和序列记忆，为人工和生物系统的高容量时间记忆提供可扩展和可解释的模型。

Abstract: The energy paradigm, exemplified by Hopfield networks, offers a principled
framework for memory in neural systems by interpreting dynamics as descent on
an energy surface. While powerful for static associative memories, it falls
short in modeling sequential memory, where transitions between memories are
essential. We introduce the Exponential Dynamic Energy Network (EDEN), a novel
architecture that extends the energy paradigm to temporal domains by evolving
the energy function over multiple timescales. EDEN combines a static
high-capacity energy network with a slow, asymmetrically interacting modulatory
population, enabling robust and controlled memory transitions. We formally
derive short-timescale energy functions that govern local dynamics and use them
to analytically compute memory escape times, revealing a phase transition
between static and dynamic regimes. The analysis of capacity, defined as the
number of memories that can be stored with minimal error rate as a function of
the dimensions of the state space (number of feature neurons), for EDEN shows
that it achieves exponential sequence memory capacity $O(\gamma^N)$,
outperforming the linear capacity $O(N)$ of conventional models. Furthermore,
EDEN's dynamics resemble the activity of time and ramping cells observed in the
human brain during episodic memory tasks, grounding its biological relevance.
By unifying static and sequential memory within a dynamic energy framework,
EDEN offers a scalable and interpretable model for high-capacity temporal
memory in both artificial and biological systems.

</details>


### [143] [Maximum-Entropy Analog Computing Approaching ExaOPS-per-Watt Energy-efficiency at the RF-Edge](https://arxiv.org/abs/2510.24975)
*Aswin Undavalli,Kareem Rashed,Zhili Xiao,Arun Natarajan,Shantanu Chakrabartty,Aravind Nagulu*

Main category: cs.NE

TL;DR: 结合熵产生物理与对称约束实现高性能节能模拟计算系统，通过广义最大熵原理，以RF相关器集成电路验证，展示高计算效率及边缘应用。


<details>
  <summary>Details</summary>
Motivation: 利用熵产生物理和对称约束实现高性能、节能的模拟计算系统。

Method: 提出广义最大熵原理描述介观物理系统演化，将框架扩展到非平衡或瞬态操作条件。

Result: 22nm SOI CMOS工艺的RF相关器集成电路，8位精度计算效率超2 PetaOPS/W，3位精度超0.8 ExaOPS/W。

Conclusion: 该框架可实现高性能、节能模拟计算系统，适用于边缘RF应用。

Abstract: In this paper, we demonstrate how the physics of entropy production, when
combined with symmetry constraints, can be used for implementing
high-performance and energy-efficient analog computing systems. At the core of
the proposed framework is a generalized maximum-entropy principle that can
describe the evolution of a mesoscopic physical system formed by an
interconnected ensemble of analog elements, including devices that can be
readily fabricated on standard integrated circuit technology. We show that the
maximum-entropy state of this ensemble corresponds to a margin-propagation (MP)
distribution and can be used for computing correlations and inner products as
the ensemble's macroscopic properties. Furthermore, the limits of computational
throughput and energy efficiency can be pushed by extending the framework to
non-equilibrium or transient operating conditions, which we demonstrate using a
proof-of-concept radio-frequency (RF) correlator integrated circuit fabricated
in a 22 nm SOI CMOS process. The measured results show a compute efficiency
greater than 2 Peta ($10^{15}$) Bit Operations per second per Watt (PetaOPS/W)
at 8-bit precision and greater than 0.8 Exa ($10^{18}$) Bit Operations per
second per Watt (ExaOPS/W) at 3-bit precision for RF data sampled at rates
greater than 4 GS/s. Using the fabricated prototypes, we also showcase several
real-world RF applications at the edge, including spectrum sensing, and
code-domain communications.

</details>


### [144] [Socio-cognitive agent-oriented evolutionary algorithm with trust-based optimization](https://arxiv.org/abs/2510.25095)
*Aleksandra Urbańczyk,Krzysztof Czech,Piotr Urbańczyk,Marek Kisiel-Dorohinicki,Aleksander Byrski*

Main category: cs.NE

TL;DR: 介绍基于信任的优化算法TBO，实验表明其在多种优化问题上优于标准岛模型，性能因问题类型而异，信任和声誉机制有益。


<details>
  <summary>Details</summary>
Motivation: 改进传统岛模型中周期性迁移机制，寻求更灵活的进化优化方法。

Method: 提出基于信任或声誉的灵活、代理驱动交互机制的TBO算法。

Result: TBO在多种优化问题上通常优于标准岛模型，但性能因问题类型不同。

Conclusion: 信任和声誉机制为进化优化提供灵活自适应方法，可提高解的质量。

Abstract: This paper introduces the Trust-Based Optimization (TBO), a novel extension
of the island model in evolutionary computation that replaces conventional
periodic migrations with a flexible, agent-driven interaction mechanism based
on trust or reputation. Experimental results demonstrate that TBO generally
outperforms the standard island model evolutionary algorithm across various
optimization problems. Nevertheless, algorithm performance varies depending on
the problem type, with certain configurations being more effective for specific
landscapes or dimensions. The findings suggest that trust and reputation
mechanisms provide a flexible and adaptive approach to evolutionary
optimization, improving solution quality in many cases.

</details>


### [145] [A Benchmark Suite for Multi-Objective Optimization in Battery Thermal Management System Design](https://arxiv.org/abs/2510.25219)
*Kaichen Ouyang,Yezhi Xia*

Main category: cs.NE

TL;DR: 本文针对电池热管理系统（BTMS）设计中约束多目标优化问题（CMOPs）缺乏实用基准套件的问题，开发了一个用于BTMS多目标优化的基准套件，并提出了未来工作方向。


<details>
  <summary>Details</summary>
Motivation: 现有合成基准问题（SBPs）有不现实属性，且针对BTMS设计的CMOPs缺乏实用基准套件。

Method: 开发包含一系列实际约束问题的基准套件，通过基于近期研究的精确代理模型定义问题以表示复杂热 - 流体相互作用。

Result: 开发出适用于BTMS多目标优化的基准套件。

Conclusion: 该基准套件为储能热管理的进化算法和优化方法提供实用测试平台，未来将建立基线结果、进行对比分析和制定排名方案。

Abstract: Synthetic Benchmark Problems (SBPs) are commonly used to evaluate the
performance of metaheuristic algorithms. However, these SBPs often contain
various unrealistic properties, potentially leading to underestimation or
overestimation of algorithmic performance. While several benchmark suites
comprising real-world problems have been proposed for various types of
metaheuristics, a notable gap exists for Constrained Multi-objective
Optimization Problems (CMOPs) derived from practical engineering applications,
particularly in the domain of Battery Thermal Management System (BTMS) design.
To address this gap, this study develops and presents a specialized benchmark
suite for multi-objective optimization in BTMS. This suite comprises a diverse
collection of real-world constrained problems, each defined via accurate
surrogate models based on recent research to efficiently represent complex
thermal-fluid interactions. The primary goal of this benchmark suite is to
provide a practical and relevant testing ground for evolutionary algorithms and
optimization methods focused on energy storage thermal management. Future work
will involve establishing comprehensive baseline results using state-of-the-art
algorithms, conducting comparative analyses, and developing a standardized
ranking scheme to facilitate robust performance assessment.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [146] [The influence of the random numbers quality on the results in stochastic simulations and machine learning](https://arxiv.org/abs/2510.25269)
*Benjamin A. Antunes*

Main category: cs.PF

TL;DR: 研究PRNG统计质量对随机应用结果的影响，发现低质量PRNG会引入误差，达标后设计影响小。


<details>
  <summary>Details</summary>
Motivation: 探究PRNG统计质量对计算结果的潜在影响。

Method: 评估7种PRNG，应用于4个任务，重复实验30次，用统计分析比较输出。

Result: 低质量PRNG使ABM、MNIST和RL表现变差，中高质量PRNG多数任务表现与顶级相当，RL实验与统计质量相关。

Conclusion: PRNG达统计阈值后设计对多数工作负载影响小，低质量PRNG会引入误差。

Abstract: Pseudorandom number generators (PRNGs) are ubiquitous in stochastic
simulations and machine learning (ML), where they drive sampling, parameter
initialization, regularization, and data shuffling. While widely used, the
potential impact of PRNG statistical quality on computational results remains
underexplored. In this study, we investigate whether differences in PRNG
quality, as measured by standard statistical test suites, can influence
outcomes in representative stochastic applications. Seven PRNGs were evaluated,
ranging from low-quality linear congruential generators (LCGs) with known
statistical deficiencies to high-quality generators such as Mersenne Twister,
PCG, and Philox. We applied these PRNGs to four distinct tasks: an
epidemiological agent-based model (ABM), two independent from-scratch MNIST
classification implementations (Python/NumPy and C++), and a reinforcement
learning (RL) CartPole environment. Each experiment was repeated 30 times per
generator using fixed seeds to ensure reproducibility, and outputs were
compared using appropriate statistical analyses. Results show that very poor
statistical quality, as in the ''bad'' LCG failing 125 TestU01 Crush tests,
produces significant deviations in ABM epidemic dynamics, reduces MNIST
classification accuracy, and severely degrades RL performance. In contrast,
mid-and good-quality LCGs-despite failing a limited number of Crush or BigCrush
tests-performed comparably to top-tier PRNGs in most tasks, with the RL
experiment being the primary exception where performance scaled with
statistical quality. Our findings indicate that, once a generator meets a
sufficient statistical robustness threshold, its family or design has
negligible impact on outcomes for most workloads, allowing selection to be
guided by performance and implementation considerations. However, the use of
low-quality PRNGs in sensitive stochastic computations can introduce
substantial and systematic errors.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [147] [Beyond Function-Level Search: Repository-Aware Dual-Encoder Code Retrieval with Adversarial Verification](https://arxiv.org/abs/2510.24749)
*Aofan Liu,Shiyuan Song,Haoxuan Li,Cehao Yang,Yiyan Qi*

Main category: cs.SE

TL;DR: 本文介绍RepoAlign - Bench基准和ReflectCode架构，提升代码检索性能。


<details>
  <summary>Details</summary>
Motivation: 现代代码库复杂度增加，传统函数级搜索范式无法满足跨组件变更意图检索需求，特定变更请求下相关代码检索研究不足。

Method: 引入RepoAlign - Bench基准评估仓库级代码检索，提出ReflectCode的对抗反射增强双塔架构，通过大语言模型引导反射动态集成多种信息。

Result: ReflectCode在Top - 5准确率上提升12.2%，召回率提升7.1%。

Conclusion: ReflectCode为上下文感知代码检索指明新方向。

Abstract: The escalating complexity of modern codebases has intensified the need for
retrieval systems capable of interpreting cross-component change intents, a
capability fundamentally absent in conventional function-level search
paradigms. While recent studies have improved the alignment between natural
language queries and code snippets, retrieving contextually relevant code for
specific change requests remains largely underexplored. To address this gap, we
introduce RepoAlign-Bench, the first benchmark specifically designed to
evaluate repository-level code retrieval under change request driven scenarios,
encompassing 52k annotated instances. This benchmark shifts the retrieval
paradigm from function-centric matching to holistic repository-level reasoning.
Furthermore, we propose ReflectCode, an adversarial reflection augmented
dual-tower architecture featuring disentangled code_encoder and doc_encoder
components. ReflectCode dynamically integrates syntactic patterns, function
dependencies, and semantic expansion intents through large language model
guided reflection. Comprehensive experiments demonstrate that ReflectCode
achieves 12.2% improvement in Top-5 Accuracy and 7.1% in Recall over
state-of-the-art baselines, establishing a new direction for context-aware code
retrieval.

</details>


### [148] [Compiler.next: A Search-Based Compiler to Power the AI-Native Future of Software Engineering](https://arxiv.org/abs/2510.24799)
*Filipe R. Cogo,Gustavo A. Oliva,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 提出Compiler.next新型编译器，可根据人类意图生成软件，阐述架构与应对挑战的路线图，为自动化软件开发奠基。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助软件工程工具和范式受认知过载、工具集成低效和AI副驾驶能力有限的限制，需要新的解决方案。

Method: 提出Compiler.next，通过搜索最优解，动态优化认知架构及其组成部分，在多目标间找到最优平衡。

Result: 勾勒出Compiler.next的架构，提出应对意图编译核心挑战的路线图。

Conclusion: Compiler.next可降低非专家软件开发技术门槛，为全自动、搜索驱动的软件开发奠定基础，促进创新和高效AI系统发展。

Abstract: The rapid advancement of AI-assisted software engineering has brought
transformative potential to the field of software engineering, but existing
tools and paradigms remain limited by cognitive overload, inefficient tool
integration, and the narrow capabilities of AI copilots. In response, we
propose Compiler.next, a novel search-based compiler designed to enable the
seamless evolution of AI-native software systems as part of the emerging
Software Engineering 3.0 era. Unlike traditional static compilers,
Compiler.next takes human-written intents and automatically generates working
software by searching for an optimal solution. This process involves dynamic
optimization of cognitive architectures and their constituents (e.g., prompts,
foundation model configurations, and system parameters) while finding the
optimal trade-off between several objectives, such as accuracy, cost, and
latency. This paper outlines the architecture of Compiler.next and positions it
as a cornerstone in democratizing software development by lowering the
technical barrier for non-experts, enabling scalable, adaptable, and reliable
AI-powered software. We present a roadmap to address the core challenges in
intent compilation, including developing quality programming constructs,
effective search heuristics, reproducibility, and interoperability between
compilers. Our vision lays the groundwork for fully automated, search-driven
software development, fostering faster innovation and more efficient AI-driven
systems.

</details>


### [149] [A Roadmap for Tamed Interactions with Large Language Models](https://arxiv.org/abs/2510.24819)
*Vincenzo Scotti,Jan Keim,Tobias Hey,Andreas Metzger,Anne Koziolek,Raffaela Mirandola*

Main category: cs.SE

TL;DR: 论文指出大语言模型（LLMs）驱动的软件虽多但不可靠，提出开发用于脚本化与LLMs交互的领域特定语言（LSL），以解决可靠性等问题，让LLM交互可编程且与训练或实现解耦。


<details>
  <summary>Details</summary>
Motivation: LLMs应用虽多但不可靠，现有应对工具零散，影响AI应用发展，需提升其可靠性、鲁棒性和可信赖性。

Method: 提出开发用于脚本化与LLMs交互的领域特定语言LSL。

Result: 未提及具体结果。

Conclusion: 开发LSL可控制LLM输出、规范交互结构，与验证、确认和可解释性结合，使LLM交互可编程且与训练或实现解耦。

Abstract: We are witnessing a bloom of AI-powered software driven by Large Language
Models (LLMs). Although the applications of these LLMs are impressive and
seemingly countless, their unreliability hinders adoption. In fact, the
tendency of LLMs to produce faulty or hallucinated content makes them
unsuitable for automating workflows and pipelines. In this regard, Software
Engineering (SE) provides valuable support, offering a wide range of formal
tools to specify, verify, and validate software behaviour. Such SE tools can be
applied to define constraints over LLM outputs and, consequently, offer
stronger guarantees on the generated content. In this paper, we argue that the
development of a Domain Specific Language (DSL) for scripting interactions with
LLMs using an LLM Scripting Language (LSL) may be key to improve AI-based
applications. Currently, LLMs and LLM-based software still lack reliability,
robustness, and trustworthiness, and the tools or frameworks to cope with these
issues suffer from fragmentation. In this paper, we present our vision of LSL.
With LSL, we aim to address the limitations above by exploring ways to control
LLM outputs, enforce structure in interactions, and integrate these aspects
with verification, validation, and explainability. Our goal is to make LLM
interaction programmable and decoupled from training or implementation.

</details>


### [150] [VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus](https://arxiv.org/abs/2510.25015)
*Chuyue Sun,Yican Sun,Daneshvar Amrollahi,Ethan Zhang,Shuvendu Lahiri,Shan Lu,David Dill,Clark Barrett*

Main category: cs.SE

TL;DR: 介绍VeriStruct框架，可将AI辅助自动验证从单函数扩展到复杂数据结构模块，评估表现良好。


<details>
  <summary>Details</summary>
Motivation: 将AI辅助自动验证从单函数扩展到复杂数据结构模块，解决LLMs对Verus注释语法和验证语义的误解问题。

Method: 采用规划器模块生成抽象、类型不变式、规范和证明代码，在提示中嵌入语法指导并设置修复阶段纠正注释错误。

Result: 在十一个Rust数据结构模块评估中，十一个成功十个，共成功验证129个函数中的128个（99.2%）。

Conclusion: 该成果朝着自动AI辅助形式验证目标迈出重要一步。

Abstract: We introduce VeriStruct, a novel framework that extends AI-assisted automated
verification from single functions to more complex data structure modules in
Verus. VeriStruct employs a planner module to orchestrate the systematic
generation of abstractions, type invariants, specifications, and proof code. To
address the challenge that LLMs often misunderstand Verus' annotation syntax
and verification-specific semantics, VeriStruct embeds syntax guidance within
prompts and includes a repair stage to automatically correct annotation errors.
In an evaluation on eleven Rust data structure modules, VeriStruct succeeds on
ten of the eleven, successfully verifying 128 out of 129 functions (99.2%) in
total. These results represent an important step toward the goal of automatic
AI-assisted formal verification.

</details>


### [151] [Towards Human-AI Synergy in Requirements Engineering: A Framework and Preliminary Study](https://arxiv.org/abs/2510.25016)
*Mateen Ahmed Abbasi,Petri Ihantola,Tommi Mikkonen,Niko Mäkitalo*

Main category: cs.SE

TL;DR: 本文探讨人工智能在需求工程（RE）中的应用，指出其利弊，介绍了人类 - 人工智能RE协同模型（HARE - SM），并提出研究方法和早期成果。


<details>
  <summary>Details</summary>
Motivation: 传统RE劳动密集、易出错且复杂，AI虽能提供变革性解决方案，但存在算法偏见、缺乏可解释性和伦理问题等挑战，因此需要解决这些问题。

Method: 采用多阶段研究方法，包括准备RE数据集、微调AI模型和设计人机协作工作流程。

Result: 提出了HARE - SM概念框架并进行了早期原型实现。

Conclusion: 为在协作环境中将智能数据科学技术应用于半结构化和非结构化RE数据建立了研究议程和实际设计方向。

Abstract: The future of Requirements Engineering (RE) is increasingly driven by
artificial intelligence (AI), reshaping how we elicit, analyze, and validate
requirements. Traditional RE is based on labor-intensive manual processes prone
to errors and complexity. AI-powered approaches, specifically large language
models (LLMs), natural language processing (NLP), and generative AI, offer
transformative solutions and reduce inefficiencies. However, the use of AI in
RE also brings challenges like algorithmic bias, lack of explainability, and
ethical concerns related to automation. To address these issues, this study
introduces the Human-AI RE Synergy Model (HARE-SM), a conceptual framework that
integrates AI-driven analysis with human oversight to improve requirements
elicitation, analysis, and validation. The model emphasizes ethical AI use
through transparency, explainability, and bias mitigation. We outline a
multi-phase research methodology focused on preparing RE datasets, fine-tuning
AI models, and designing collaborative human-AI workflows. This preliminary
study presents the conceptual framework and early-stage prototype
implementation, establishing a research agenda and practical design direction
for applying intelligent data science techniques to semi-structured and
unstructured RE data in collaborative environments.

</details>


### [152] [Automating Benchmark Design](https://arxiv.org/abs/2510.25039)
*Amanda Dsouza,Harit Vishwakarma,Zhengyang Qi,Justin Bauer,Derek Pham,Thomas Walshe,Armin Parchami,Frederic Sala,Paroma Varma*

Main category: cs.SE

TL;DR: 现有大语言模型评估方法有局限，本文提出BeTaL框架自动化动态基准设计，创建新基准并扩展已有基准，评估显示其生成基准更接近期望难度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型和相关智能体发展迅速，现有手工静态基准易饱和，动态基准创建和更新成本高，需更好评估方法。

Method: 开发BeTaL框架，参数化基础基准模板关键设计选择，用大语言模型推理参数空间以高效实现目标属性。

Result: 创建两个新基准并扩展一个流行基准，在三个任务和多难度水平评估中，BeTaL生成基准平均偏差5.3% - 13.2%，比基线提升2 - 4倍。

Conclusion: BeTaL框架能有效自动化动态基准设计，生成更接近期望难度的基准。

Abstract: The rapid progress and widespread deployment of LLMs and LLM-powered agents
has outpaced our ability to evaluate them. Hand-crafted, static benchmarks are
the primary tool for assessing model capabilities, but these quickly become
saturated. In contrast, dynamic benchmarks evolve alongside the models they
evaluate, but are expensive to create and continuously update. To address these
challenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a
framework that leverages environment design principles to automate the process
of dynamic benchmark design. BeTaL works by parameterizing key design choices
in base benchmark templates and uses LLMs to reason through the resulting
parameter space to obtain target properties (such as difficulty and realism) in
a cost-efficient manner. We validate this approach on its ability to create
benchmarks with desired difficulty levels. Using BeTaL, we create two new
benchmarks and extend a popular agentic benchmark $\tau$-bench. Extensive
evaluation on these three tasks and multiple target difficulty levels shows
that BeTaL produces benchmarks much closer to the desired difficulty, with
average deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the
baselines.

</details>


### [153] [Same Same But Different: Preventing Refactoring Attacks on Software Plagiarism Detection](https://arxiv.org/abs/2510.25057)
*Robin Maisch,Larissa Schmid,Timur Sağlam,Nils Niehues*

Main category: cs.SE

TL;DR: 提出新框架，利用代码属性图和图变换对抗基于重构的代码混淆，评估显示检测抄袭代码效果显著提升。


<details>
  <summary>Details</summary>
Motivation: 编程教育中抄袭检测面临基于重构的复杂混淆技术挑战，现有系统应对结构修改效果不佳。

Method: 提出利用代码属性图和图变换的可扩展框架来增强现有检测器。

Result: 对真实学生提交代码经算法和AI混淆攻击后的综合评估显示，检测抄袭代码有显著改进。

Conclusion: 该框架能有效应对基于重构的代码混淆，提升抄袭检测能力。

Abstract: Plagiarism detection in programming education faces growing challenges due to
increasingly sophisticated obfuscation techniques, particularly automated
refactoring-based attacks. While code plagiarism detection systems used in
education practice are resilient against basic obfuscation, they struggle
against structural modifications that preserve program behavior, especially
caused by refactoring-based obfuscation. This paper presents a novel and
extensible framework that enhances state-of-the-art detectors by leveraging
code property graphs and graph transformations to counteract refactoring-based
obfuscation. Our comprehensive evaluation of real-world student submissions,
obfuscated using both algorithmic and AI-based obfuscation attacks,
demonstrates a significant improvement in detecting plagiarized code.

</details>


### [154] [Adaptive Proof Refinement with LLM-Guided Strategy Selection](https://arxiv.org/abs/2510.25103)
*Minghai Lu,Zhe Zhou,Danning Xie,Songlin Jia,Benjamin Delaware,Tianyi Zhang*

Main category: cs.SE

TL;DR: 本文提出新的证明细化框架Adapt，能动态选择细化策略，在两个基准测试中显著优于现有方法，还展示了其泛化性并进行消融研究。


<details>
  <summary>Details</summary>
Motivation: 形式验证难扩展，大语言模型生成证明常出错，现有方法采用固定细化策略，无法动态选择有效策略。

Method: 引入Adapt框架，利用大语言模型引导的决策器根据证明助手状态和错误证明上下文动态选择合适的细化策略。

Result: 在两个基准测试中，Adapt比最佳基线分别多证明16.63%和18.58%的定理；在五种不同大语言模型上评估展示了其泛化性；进行消融研究衡量各组件贡献并比较决策器设计的权衡。

Conclusion: Adapt框架通过动态选择细化策略，有效提升了证明生成的性能，具有良好的泛化性。

Abstract: Formal verification via theorem proving enables the expressive specification
and rigorous proof of software correctness, but it is difficult to scale due to
the significant manual effort and expertise required. While Large Language
Models (LLMs) show potential in proof generation, they frequently produce
incorrect proofs on the first attempt and require additional strategies for
iterative refinement. However, existing approaches employ fixed refinement
strategies and cannot dynamically choose an effective strategy based on the
particular issues in a generated proof, which limits their performance. To
overcome this limitation, we introduce Adapt, a novel proof refinement
framework that leverages an LLM-guided decision-maker to dynamically select a
suitable refinement strategy according to the state of the proof assistant and
available context of an incorrect proof. We evaluate Adapt on two benchmarks
against four existing methods and find that it significantly outperforms the
best baseline on both by proving 16.63% and 18.58% more theorems, respectively.
Furthermore, we demonstrate Adapt's generalizability by evaluating it across
five different LLMs. We also conduct ablation studies to measure the
contribution of each component and compare the trade-offs of alternative
decision-maker designs.

</details>


### [155] [Automated Program Repair Based on REST API Specifications Using Large Language Models](https://arxiv.org/abs/2510.25148)
*Katsuki Yamagishi,Norihiro Yoshida,Erina Makihara,Katsuro Inoue*

Main category: cs.SE

TL;DR: 提出dcFix方法检测和自动修复客户端程序中REST API的误用，评估显示其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 云服务的REST API错误信息缺乏有效诊断细节，开发者调试需试错。

Method: dcFix识别不符合规范的代码片段，将其与相关API规范整合到提示中，利用大语言模型生成修正代码。

Result: dcFix能准确检测误用，性能优于基线方法。

Conclusion: dcFix是一种有效的检测和修复REST API误用的方法。

Abstract: Many cloud services provide REST API accessible to client applications.
However, developers often identify specification violations only during
testing, as error messages typically lack the detail necessary for effective
diagnosis. Consequently, debugging requires trial and error. This study
proposes dcFix, a method for detecting and automatically repairing REST API
misuses in client programs. In particular, dcFix identifies non-conforming code
fragments, integrates them with the relevant API specifications into prompts,
and leverages a Large Language Model (LLM) to produce the corrected code. Our
evaluation demonstrates that dcFix accurately detects misuse and outperforms
the baseline approach, in which prompts to the LLM omit any indication of code
fragments non conforming to REST API specifications.

</details>


### [156] [Optimizing Knowledge Utilization for Multi-Intent Comment Generation with Large Language Models](https://arxiv.org/abs/2510.25195)
*Shuochuan Li,Zan Wang,Xiaoning Du,Zhuo Wu,Jiuqiao Yu,Junjie Chen*

Main category: cs.SE

TL;DR: 文章指出代码注释生成需多意图，现有基于大语言模型方法有局限，提出KUMIC框架，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 通用代码注释摘要不足以满足开发者和用户需求，现有基于大语言模型的多意图注释生成方法难以在少量示例中构建正确关系。

Method: 提出KUMIC框架，基于上下文学习，利用思维链优化大语言模型知识利用，设计检索机制获取相似示例，构建映射知识链。

Result: 实验表明KUMIC在BLEU、METEOR、ROUGE - L和SBERT指标上分别比现有基线高14.49%、22.41%、20.72%和12.94%。

Conclusion: KUMIC框架在多意图代码注释生成方面表现优于现有方法。

Abstract: Code comment generation aims to produce a generic overview of a code snippet,
helping developers understand and maintain code. However, generic summaries
alone are insufficient to meet the diverse needs of practitioners; for example,
developers expect the implementation insights to be presented in an untangled
manner, while users seek clear usage instructions. This highlights the
necessity of multi-intent comment generation. With the widespread adoption of
Large Language Models (LLMs) for code-related tasks, these models have been
leveraged to tackle the challenge of multi-intent comment generation. Despite
their successes, state-of-the-art LLM-based approaches often struggle to
construct correct relationships among intents, code, and comments within a
smaller number of demonstration examples. To mitigate this issue, we propose a
framework named KUMIC for multi-intent comment generation. Built upon
in-context learning, KUMIC leverages Chain-of-Thought (CoT) to optimize
knowledge utilization for LLMs to generate intent-specific comments.
Specifically, KUMIC first designs a retrieval mechanism to obtain similar
demonstration examples, which exhibit high code-comment consistency. Then,
KUMIC leverages CoT to guide LLMs to focus on statements facilitating the
derivation of code comments aligned with specific intents. In this context,
KUMIC constructs a mapping knowledge chain, linking code to intent-specific
statements to comments, which enables LLMs to follow similar reasoning steps
when generating the desired comments. We conduct extensive experiments to
evaluate KUMIC, and the results demonstrate that KUMIC outperforms
state-of-the-art baselines by 14.49\%, 22.41\%, 20.72\%, and 12.94\% in terms
of BLEU, METEOR, ROUGE-L, and SBERT, respectively.

</details>


### [157] [TECS/Rust-OE: Optimizing Exclusive Control in Rust-based Component Systems for Embedded Devices](https://arxiv.org/abs/2510.25242)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: 本文提出TECS/Rust - OE框架解决TECS/Rust性能下降问题，优化排他控制并保证代码可重用性。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统结构复杂，TECS/Rust框架因过度排他控制导致性能下降，需新框架解决。

Method: 提出TECS/Rust - OE内存安全CBD框架，利用调用流，借助实时OS排他控制机制，自动生成Rust代码。

Result: 评估显示优化排他控制降低了开销，生成代码有高可重用性。

Conclusion: TECS/Rust - OE框架能在不牺牲可重用性的前提下优化性能。

Abstract: The diversification of functionalities and the development of the IoT are
making embedded systems larger and more complex in structure. Ensuring system
reliability, especially in terms of security, necessitates selecting an
appropriate programming language. As part of existing research, TECS/Rust has
been proposed as a framework that combines Rust and component-based development
(CBD) to enable scalable system design and enhanced reliability. This framework
represents system structures using static mutable variables, but excessive
exclusive controls applied to ensure thread safety have led to performance
degradation. This paper proposes TECS/Rust-OE, a memory-safe CBD framework
utilizing call flows to address these limitations. The proposed Rust code
leverages real-time OS exclusive control mechanisms, optimizing performance
without compromising reusability. Rust code is automatically generated based on
component descriptions. Evaluations demonstrate reduced overhead due to
optimized exclusion control and high reusability of the generated code.

</details>


### [158] [TECS/Rust: Memory-safe Component Framework for Embedded Systems](https://arxiv.org/abs/2510.25270)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: 随着嵌入式系统复杂度增加，提出基于Rust的TECS/Rust框架解决C语言内存问题，评估显示框架开销可忽略。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统复杂度和规模增大，传统CBD使用的C语言易出现内存相关问题，需要解决内存安全隐患。

Method: 提出基于Rust的TECS/Rust框架，利用Rust编译时内存安全特性，如生命周期和借用，自动化生成CBD组件的Rust代码，支持与实时操作系统集成。

Result: 框架生成的代码在实际代码中占比大，与无框架开发的代码执行时间差异小，框架引入的开销可忽略不计。

Conclusion: TECS/Rust框架能确保内存安全，保持CBD灵活性，且引入的开销可忽略，适用于嵌入式系统开发。

Abstract: As embedded systems grow in complexity and scale due to increased functional
diversity, component-based development (CBD) emerges as a solution to
streamline their architecture and enhance functionality reuse. CBD typically
utilizes the C programming language for its direct hardware access and
low-level operations, despite its susceptibility to memory-related issues. To
address these concerns, this paper proposes TECS/Rust, a Rust-based framework
specifically designed for TECS, which is a component framework for embedded
systems. It leverages Rust's compile-time memory-safe features, such as
lifetime and borrowing, to mitigate memory vulnerabilities common with C. The
proposed framework not only ensures memory safety but also maintains the
flexibility of CBD, automates Rust code generation for CBD components, and
supports efficient integration with real-time operating systems. An evaluation
of the amount of generated code indicates that the code generated by this paper
framework accounts for a large percentage of the actual code. Compared to code
developed without the proposed framework, the difference in execution time is
minimal, indicating that the overhead introduced by the proposed framework is
negligible.

</details>


### [159] [Understanding the Characteristics of LLM-Generated Property-Based Tests in Exploring Edge Cases](https://arxiv.org/abs/2510.25297)
*Hidetake Tanaka,Haruto Tanaka,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 研究比较LLM生成的基于属性测试（PBT）和基于示例测试（EBT）探索边缘情况的特性，发现结合两者可提高LLM生成代码的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件开发中生成代码增多，确保代码质量很重要，传统EBT常遗漏边缘情况，因此研究PBT与EBT对比。

Method: 分析16个HumanEval问题，用Claude - 4 - sonnet生成PBT和EBT测试代码。

Result: 单独使用PBT和EBT的错误检测率为68.75%，结合两者检测率提高到81.25%，且二者有互补特性。

Conclusion: 结合PBT和EBT的混合方法可提高LLM生成代码的可靠性，为测试生成策略提供指导。

Abstract: As Large Language Models (LLMs) increasingly generate code in software
development, ensuring the quality of LLM-generated code has become important.
Traditional testing approaches using Example-based Testing (EBT) often miss
edge cases -- defects that occur at boundary values, special input patterns, or
extreme conditions. This research investigates the characteristics of
LLM-generated Property-based Testing (PBT) compared to EBT for exploring edge
cases. We analyze 16 HumanEval problems where standard solutions failed on
extended test cases, generating both PBT and EBT test codes using
Claude-4-sonnet. Our experimental results reveal that while each method
individually achieved a 68.75\% bug detection rate, combining both approaches
improved detection to 81.25\%. The analysis demonstrates complementary
characteristics: PBT effectively detects performance issues and edge cases
through extensive input space exploration, while EBT effectively detects
specific boundary conditions and special patterns. These findings suggest that
a hybrid approach leveraging both testing methods can improve the reliability
of LLM-generated code, providing guidance for test generation strategies in
LLM-based code generation.

</details>


### [160] [Dissect-and-Restore: AI-based Code Verification with Transient Refactoring](https://arxiv.org/abs/2510.25406)
*Changjie Wang,Mariano Scazzariello,Anoud Alshnaka,Roberto Guanciale,Dejan Kostić,Marco Chiesa*

Main category: cs.SE

TL;DR: 提出AI辅助系统Prometheus用于自动化代码验证，通过分解 - 重组工作流，结合模块化软件工程原则，提升验证效果。


<details>
  <summary>Details</summary>
Motivation: 形式验证需专业知识且成本高，现代AI集成到形式验证过程仍是挑战。

Method: 将复杂程序逻辑分解为可验证组件，再重组；通过结构化分解复杂引理为子引理引导证明搜索，用户可提供自然语言指导。

Result: 对代码进行模块化重构显著提升AI验证组件的效果，在数据集上验证成功率从68%提升到86%，复杂场景提升更明显。

Conclusion: Prometheus系统结合AI和模块化软件工程原则，有效提升代码验证效果。

Abstract: Formal verification is increasingly recognized as a critical foundation for
building reliable software systems. However, the need for specialized expertise
to write precise specifications, navigate complex proof obligations, and learn
annotations often makes verification an order of magnitude more expensive than
implementation. While modern AI systems can recognize patterns in mathematical
proofs and interpret natural language, effectively integrating them into the
formal verification process remains an open challenge. We present Prometheus, a
novel AI-assisted system that facilitates automated code verification with
current AI capabilities in conjunction with modular software engineering
principles (e.g., modular refactoring). Our approach begins by decomposing
complex program logic, such as nested loops, into smaller, verifiable
components. Once verified, these components are recomposed to construct a proof
of the original program. This decomposition-recomposition workflow is
non-trivial. Prometheus addresses this by guiding the proof search through
structured decomposition of complex lemmas into smaller, verifiable sub-lemmas.
When automated tools are insufficient, users can provide lightweight natural
language guidance to steer the proof process effectively. Our evaluation
demonstrates that transiently applying modular restructuring to the code
substantially improves the AI's effectiveness in verifying individual
components. This approach successfully verifies 86% of tasks in our curated
dataset, compared to 68% for the baseline. Gains are more pronounced with
increasing specification complexity, improving from 30% to 69%, and when
integrating proof outlines for complex programs, from 25% to 87%.

</details>


### [161] [What Challenges Do Developers Face in AI Agent Systems? An Empirical Study on Stack Overflow](https://arxiv.org/abs/2510.25423)
*Ali Asgari,Annibale Panichella,Pouria Derakhshanfar,Mitchell Olsthoorn*

Main category: cs.SE

TL;DR: 研究人员通过分析Stack Overflow上开发者讨论，构建AI智能体开发挑战分类，揭示7大领域77个技术挑战，量化话题并追踪工具发展，给出相关建议。


<details>
  <summary>Details</summary>
Motivation: 识别AI智能体在构建、部署和维护过程中开发者面临的未被充分探索的挑战。

Method: 在Stack Overflow上进行研究，通过标签扩展和过滤构建挑战分类，用LDA - MALLET进行主题建模，手动验证和标记主题。

Result: 揭示7大领域77个与运行时集成、依赖管理等相关的技术挑战，量化话题流行度和难度，绘制工具和编程语言使用情况并追踪其演变。

Conclusion: 给出研究结果的影响，为从业者、研究者和教育者提供关于智能体可靠性和开发者支持的具体指导。

Abstract: AI agents have rapidly gained popularity across research and industry as
systems that extend large language models with additional capabilities to plan,
use tools, remember, and act toward specific goals. Yet despite their promise,
developers face persistent and often underexplored challenges when building,
deploying, and maintaining these emerging systems. To identify these
challenges, we study developer discussions on Stack Overflow, the world's
largest developer-focused Q and A platform with about 60 million questions and
answers and 30 million users. We construct a taxonomy of developer challenges
through tag expansion and filtering, apply LDA-MALLET for topic modeling, and
manually validate and label the resulting themes. Our analysis reveals seven
major areas of recurring issues encompassing 77 distinct technical challenges
related to runtime integration, dependency management, orchestration
complexity, and evaluation reliability. We further quantify topic popularity
and difficulty to identify which issues are most common and hardest to resolve,
map the tools and programming languages used in agent development, and track
their evolution from 2021 to 2025 in relation to major AI model and framework
releases. Finally, we present the implications of our results, offering
concrete guidance for practitioners, researchers, and educators on agent
reliability and developer support.

</details>


### [162] [Reflections on the Reproducibility of Commercial LLM Performance in Empirical Software Engineering Studies](https://arxiv.org/abs/2510.25506)
*Florian Angermeir,Maximilian Amougou,Mark Kreitz,Andreas Bauer,Matthias Linhuber,Davide Fucci,Fabiola Moyón C.,Daniel Mendez,Tony Gorschek*

Main category: cs.SE

TL;DR: 分析ICSE 2024和ASE 2024上86篇以大语言模型为中心的研究论文的可复现性，发现复现情况不佳，需严格评估研究工件和优化研究设计。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型实证研究存在复现结果的挑战，要了解现有研究结果的可复现程度及阻碍因素。

Method: 研究ICSE 2024和ASE 2024上86篇描述以大语言模型为中心研究的文章，对其中18篇提供研究工件并使用OpenAI模型的研究进行复现尝试。

Result: 18项研究中仅5项适合复现，且均无法完全复现，2项部分可复现，3项不可复现。

Conclusion: 不仅需要更严格的研究工件评估，还需要更稳健的研究设计来确保未来出版物的可复现价值。

Abstract: Large Language Models have gained remarkable interest in industry and
academia. The increasing interest in LLMs in academia is also reflected in the
number of publications on this topic over the last years. For instance, alone
78 of the around 425 publications at ICSE 2024 performed experiments with LLMs.
Conducting empirical studies with LLMs remains challenging and raises questions
on how to achieve reproducible results, for both other researchers and
practitioners. One important step towards excelling in empirical research on
LLMs and their application is to first understand to what extent current
research results are eventually reproducible and what factors may impede
reproducibility. This investigation is within the scope of our work. We
contribute an analysis of the reproducibility of LLM-centric studies, provide
insights into the factors impeding reproducibility, and discuss suggestions on
how to improve the current state. In particular, we studied the 86 articles
describing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86
articles, 18 provided research artefacts and used OpenAI models. We attempted
to replicate those 18 studies. Of the 18 studies, only five were fit for
reproduction. For none of the five studies, we were able to fully reproduce the
results. Two studies seemed to be partially reproducible, and three studies did
not seem to be reproducible. Our results highlight not only the need for
stricter research artefact evaluations but also for more robust study designs
to ensure the reproducible value of future publications.

</details>


### [163] [Fuzz Smarter, Not Harder: Towards Greener Fuzzing with GreenAFL](https://arxiv.org/abs/2510.25665)
*Ayse Irmak Ercevik,Aidan Dakhama,Melane Navaratnarajah,Yazhuo Cao,Leo Fernandes*

Main category: cs.SE

TL;DR: 提出能源感知框架GreenAFL降低模糊测试能耗并保持覆盖率，实验表明至少使用一种改进方法可实现高覆盖率和低能耗。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试方法消耗大量计算资源和产生高碳足迹，且未考虑不同执行路径的能源成本。

Method: 提出GreenAFL框架，包括能源感知语料库最小化和能源引导启发式两种对传统模糊测试工作流的修改，并进行消融研究。

Result: 至少使用一种修改方法能实现最高覆盖率和最低能源使用。

Conclusion: GreenAFL框架可在保持覆盖率的同时降低自动化测试的环境影响。

Abstract: Fuzzing has become a key search-based technique for software testing, but
continuous fuzzing campaigns consume substantial computational resources and
generate significant carbon footprints. Existing grey-box fuzzing approaches
like AFL++ focus primarily on coverage maximisation, without considering the
energy costs of exploring different execution paths. This paper presents
GreenAFL, an energy-aware framework that incorporates power consumption into
the fuzzing heuristics to reduce the environmental impact of automated testing
whilst maintaining coverage. GreenAFL introduces two key modifications to
traditional fuzzing workflows: energy-aware corpus minimisation considering
power consumption when reducing initial corpora, and energy-guided heuristics
that direct mutation towards high-coverage, low-energy inputs. We conduct an
ablation study comparing vanilla AFL++, energy-based corpus minimisation, and
energy-based heuristics to evaluate the individual contributions of each
component. Results show that highest coverage, and lowest energy usage is
achieved whenever at least one of our modifications is used.

</details>


### [164] [A Configuration-First Framework for Reproducible, Low-Code Localization](https://arxiv.org/abs/2510.25692)
*Tim Strnad,Blaž Bertalanič,Carolina Fortuna*

Main category: cs.SE

TL;DR: 本文介绍了用于无线电定位的低代码框架LOCALIZE，能使基于机器学习的定位实验可重复、易操作且可扩展，通过对比实验验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 为填补机器学习在无线电定位服务中实验规范、可重复性和扩展性方面的差距，现有工具难以同时满足这三点。

Method: 引入低代码、配置优先的框架LOCALIZE，通过人类可读配置声明实验，工作流编排器运行标准化管道，对所有工件进行版本控制。

Result: 与Jupyter Notebook基线对比，框架减少创作工作量，保持可比的运行时间和内存行为；使用蓝牙低功耗数据集，表明处理训练数据扩展时编排开销可控。

Conclusion: 框架使基于机器学习的定位实验具有可重复性、易操作性和扩展性。

Abstract: Machine learning is increasingly permeating radio-based localization
services. To keep results credible and comparable, everyday workflows should
make rigorous experiment specification and exact repeatability the default,
without blocking advanced experimentation. However, in practice, researchers
face a three-way gap that could be filled by a framework that offers (i) low
coding effort for end-to-end studies, (ii) reproducibility by default including
versioned code, data, and configurations, controlled randomness, isolated runs,
and recorded artifacts, and (iii) built-in extensibility so new models,
metrics, and stages can be added with minimal integration effort. Existing
tools rarely deliver all three for machine learning in general and localization
workflows in particular. In this paper we introduce LOCALIZE, a low-code,
configuration-first framework for radio localization in which experiments are
declared in human-readable configuration, a workflow orchestrator runs
standardized pipelines from data preparation to reporting, and all artifacts,
such as datasets, models, metrics, and reports, are versioned. The
preconfigured, versioned datasets reduce initial setup and boilerplate,
speeding up model development and evaluation. The design, with clear extension
points, allows experts to add components without reworking the infrastructure.
In a qualitative comparison and a head-to-head study against a plain Jupyter
notebook baseline, we show that the framework reduces authoring effort while
maintaining comparable runtime and memory behavior. Furthermore, using a
Bluetooth Low Energy dataset, we show that scaling across training data (1x to
10x) keeps orchestration overheads bounded as data grows. Overall, the
framework makes reproducible machine-learning-based localization
experimentation practical, accessible, and extensible.

</details>


### [165] [Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents](https://arxiv.org/abs/2510.25694)
*Jiayi Kuang,Yinghui Li,Xin Zhang,Yangning Li,Di Yin,Xing Sun,Ying Shen,Philip S. Yu*

Main category: cs.SE

TL;DR: 提出环境配置诊断基准Enconda - bench，评估大语言模型代理在环境配置的细粒度能力，发现代理在将反馈转化为有效修正上有困难。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理用于软件工程时环境配置是瓶颈，现有基准仅评估端到端构建/测试成功，无法明确代理成败原因。

Method: 引入Enconda - bench，对环境设置各环节进行过程级轨迹评估，任务实例通过注入真实README错误自动构建并在Docker中验证。

Result: 评估显示代理能定位错误，但难将反馈转化为有效修正，限制端到端性能。

Conclusion: Enconda - bench是首个提供环境配置过程级内部能力评估的框架，可为改进软件工程代理提供见解。

Abstract: Large language model-based agents show promise for software engineering, but
environment configuration remains a bottleneck due to heavy manual effort and
scarce large-scale, high-quality datasets. Existing benchmarks assess only
end-to-end build/test success, obscuring where and why agents succeed or fail.
We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,
which provides process-level trajectory assessment of fine-grained agent
capabilities during environment setup-planning, perception-driven error
diagnosis, feedback-driven repair, and action to execute final environment
configuration. Our task instances are automatically constructed by injecting
realistic README errors and are validated in Docker for scalable, high-quality
evaluation. Enconda-bench combines process-level analysis with end-to-end
executability to enable capability assessments beyond aggregate success rates.
Evaluations across state-of-the-art LLMs and agent frameworks show that while
agents can localize errors, they struggle to translate feedback into effective
corrections, limiting end-to-end performance. To our knowledge, Enconda-bench
is the first framework to provide process-level internal capability assessment
for environment configuration, offering actionable insights for improving
software engineering agents.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [166] [Certainty in Uncertainty: Reasoning over Uncertain Knowledge Graphs with Statistical Guarantees](https://arxiv.org/abs/2510.24754)
*Yuqicheng Zhu,Jingcheng Wu,Yizhen Wang,Hongkuan Zhou,Jiaoyan Chen,Evgeny Kharlamov,Steffen Staab*

Main category: stat.ML

TL;DR: 现有不确定知识图谱嵌入方法仅产生点估计，未量化预测不确定性。本文提出UnKGCP框架生成预测区间，理论和实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有不确定知识图谱嵌入方法仅产生点估计，未量化预测不确定性，在高风险应用中可靠性受限。

Method: 提出UnKGCP框架，基于共形预测框架，引入适用于UnKGE方法的非一致性度量和高效区间构建程序。

Result: 理论上为区间提供保证并通过实验验证，在多个标准基准测试中表明区间尖锐且能有效捕捉预测不确定性。

Conclusion: UnKGCP框架能有效解决现有方法未量化预测不确定性的问题，生成的预测区间可靠且有效。

Abstract: Uncertain knowledge graph embedding (UnKGE) methods learn vector
representations that capture both structural and uncertainty information to
predict scores of unseen triples. However, existing methods produce only point
estimates, without quantifying predictive uncertainty-limiting their
reliability in high-stakes applications where understanding confidence in
predictions is crucial. To address this limitation, we propose \textsc{UnKGCP},
a framework that generates prediction intervals guaranteed to contain the true
score with a user-specified level of confidence. The length of the intervals
reflects the model's predictive uncertainty. \textsc{UnKGCP} builds on the
conformal prediction framework but introduces a novel nonconformity measure
tailored to UnKGE methods and an efficient procedure for interval construction.
We provide theoretical guarantees for the intervals and empirically verify
these guarantees. Extensive experiments on standard benchmarks across diverse
UnKGE methods further demonstrate that the intervals are sharp and effectively
capture predictive uncertainty.

</details>


### [167] [Tree Ensemble Explainability through the Hoeffding Functional Decomposition and TreeHFD Algorithm](https://arxiv.org/abs/2510.24815)
*Clément Bénard*

Main category: stat.ML

TL;DR: 引入TreeHFD算法估计树集成的Hoeffding分解，展示其收敛性和相关性质，通过实验证明其高性能，还揭示TreeSHAP与Hoeffding分解的联系。


<details>
  <summary>Details</summary>
Motivation: 树集成是黑盒模型，Hoeffding分解虽强大但在实际数据样本估计上是开放问题，需要解决该问题。

Method: 引入TreeHFD算法估计树集成的Hoeffding分解。

Result: 展示TreeHFD的收敛性、正交性、稀疏性和因果变量选择等性质，通过模拟和真实数据实验证明其高性能，揭示TreeSHAP与Hoeffding分解的强联系。

Conclusion: TreeHFD算法可有效用于估计树集成的Hoeffding分解，具有良好性能，且TreeSHAP与Hoeffding分解紧密相关。

Abstract: Tree ensembles have demonstrated state-of-the-art predictive performance
across a wide range of problems involving tabular data. Nevertheless, the
black-box nature of tree ensembles is a strong limitation, especially for
applications with critical decisions at stake. The Hoeffding or ANOVA
functional decomposition is a powerful explainability method, as it breaks down
black-box models into a unique sum of lower-dimensional functions, provided
that input variables are independent. In standard learning settings, input
variables are often dependent, and the Hoeffding decomposition is generalized
through hierarchical orthogonality constraints. Such generalization leads to
unique and sparse decompositions with well-defined main effects and
interactions. However, the practical estimation of this decomposition from a
data sample is still an open problem. Therefore, we introduce the TreeHFD
algorithm to estimate the Hoeffding decomposition of a tree ensemble from a
data sample. We show the convergence of TreeHFD, along with the main properties
of orthogonality, sparsity, and causal variable selection. The high performance
of TreeHFD is demonstrated through experiments on both simulated and real data,
using our treehfd Python package (https://github.com/ThalesGroup/treehfd).
Besides, we empirically show that the widely used TreeSHAP method, based on
Shapley values, is strongly connected to the Hoeffding decomposition.

</details>


### [168] [Generative Bayesian Optimization: Generative Models as Acquisition Functions](https://arxiv.org/abs/2510.25240)
*Rafael Oliveira,Daniel M. Steinberg,Edwin V. Bonilla*

Main category: stat.ML

TL;DR: 提出将生成模型转化为批量贝叶斯优化候选解采样器的通用策略，避免构建代理模型，理论和实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 使贝叶斯优化实现大批量扩展、适用于非连续设计空间和高维组合设计，避免传统方法构建代理模型的问题。

Method: 受直接偏好优化启发，用从观测中直接计算的简单效用值训练生成模型，形成与预期效用成比例的提议分布。

Result: 理论上证明生成模型在一定条件下渐近收敛于全局最优，实验验证在高维大批量优化问题上有效。

Conclusion: 该策略可将生成模型用于批量贝叶斯优化，且能推广到一般奖励信号和损失函数。

Abstract: We present a general strategy for turning generative models into candidate
solution samplers for batch Bayesian optimization (BO). The use of generative
models for BO enables large batch scaling as generative sampling, optimization
of non-continuous design spaces, and high-dimensional and combinatorial design.
Inspired by the success of direct preference optimization (DPO), we show that
one can train a generative model with noisy, simple utility values directly
computed from observations to then form proposal distributions whose densities
are proportional to the expected utility, i.e., BO's acquisition function
values. Furthermore, this approach is generalizable beyond preference-based
feedback to general types of reward signals and loss functions. This
perspective avoids the construction of surrogate (regression or classification)
models, common in previous methods that have used generative models for
black-box optimization. Theoretically, we show that the generative models
within the BO process approximately follow a sequence of distributions which
asymptotically concentrate at the global optima under certain conditions. We
also demonstrate this effect through experiments on challenging optimization
problems involving large batches in high dimensions.

</details>


### [169] [Error Bounds and Optimal Schedules for Masked Diffusions with Factorized Approximations](https://arxiv.org/abs/2510.25544)
*Hugo Lavenant,Giacomo Zanella*

Main category: stat.ML

TL;DR: 研究离散数据生成模型MDMs的计算与精度权衡，给出误差界，研究非恒定调度大小的增益并确定最优调度，证明简单透明。


<details>
  <summary>Details</summary>
Motivation: MDMs通过条件独立近似降低ARMs计算成本但采样分布有偏差，需研究计算与精度的权衡。

Method: 不采用经典的时间反转扩散过程推导，直接将方法定义为采样算法。

Result: 给出仅依赖每次迭代生成的平均令牌数且与数据维度无关的误差界；确定了作为数据分布信息轮廓函数的最优调度。

Conclusion: 支持了MDMs的实证成功，可对调度大小进行原则性优化，证明方法简单透明。

Abstract: Recently proposed generative models for discrete data, such as Masked
Diffusion Models (MDMs), exploit conditional independence approximations to
reduce the computational cost of popular Auto-Regressive Models (ARMs), at the
price of some bias in the sampling distribution. We study the resulting
computation-vs-accuracy trade-off, providing general error bounds (in relative
entropy) that depend only on the average number of tokens generated per
iteration and are independent of the data dimensionality (i.e. sequence
length), thus supporting the empirical success of MDMs. We then investigate the
gain obtained by using non-constant schedule sizes (i.e. varying the number of
unmasked tokens during the generation process) and identify the optimal
schedule as a function of a so-called information profile of the data
distribution, thus allowing for a principled optimization of schedule sizes. We
define methods directly as sampling algorithms and do not use classical
derivations as time-reversed diffusion processes, leading us to simple and
transparent proofs.

</details>


### [170] [Convergence of off-policy TD(0) with linear function approximation for reversible Markov chains](https://arxiv.org/abs/2510.25514)
*Maik Overmars,Jasper Goseling,Richard Boucherie*

Main category: stat.ML

TL;DR: 研究在马尔可夫链中使用线性函数逼近的离策略TD(0)算法收敛性，通过限制在可逆马尔可夫链上分析标准算法，证明收敛性并给出折扣因子上界。


<details>
  <summary>Details</summary>
Motivation: 解决离策略学习与函数逼近结合可能导致算法发散的问题，改进现有需修改算法增加复杂度的方法。

Method: 将Tsitsiklis和Van Roy用于在线策略的随机逼近框架应用到离线策略，分析标准算法并限制在可逆马尔可夫链上。

Result: 在可逆马尔可夫链的温和条件下证明算法收敛，给出折扣因子上界，收敛概率为1且投影贝尔曼误差为0。

Conclusion: 在可逆马尔可夫链上分析标准离策略TD(0)算法可保证收敛，改进了已知结果。

Abstract: We study the convergence of off-policy TD(0) with linear function
approximation when used to approximate the expected discounted reward in a
Markov chain. It is well known that the combination of off-policy learning and
function approximation can lead to divergence of the algorithm. Existing
results for this setting modify the algorithm, for instance by reweighing the
updates using importance sampling. This establishes convergence at the expense
of additional complexity. In contrast, our approach is to analyse the standard
algorithm, but to restrict our attention to the class of reversible Markov
chains. We demonstrate convergence under this mild reversibility condition on
the structure of the chain, which in many applications can be assumed using
domain knowledge. In particular, we establish a convergence guarantee under an
upper bound on the discount factor in terms of the difference between the
on-policy and off-policy process. This improves upon known results in the
literature that state that convergence holds for a sufficiently small discount
factor by establishing an explicit bound. Convergence is with probability one
and achieves projected Bellman error equal to zero. To obtain these results, we
adapt the stochastic approximation framework that was used by Tsitsiklis and
Van Roy [1997 for the on-policy case, to the off-policy case. We illustrate our
results using different types of reversible Markov chains, such as
one-dimensional random walks and random walks on a weighted graph.

</details>


### [171] [Using latent representations to link disjoint longitudinal data for mixed-effects regression](https://arxiv.org/abs/2510.25531)
*Clemens Schächter,Maren Hackenberg,Michelle Pfaffenlehner,Félix B. Tambe-Ndonfack,Thorsten Schmidt,Astrid Pechmann,Janbernd Kirschner,Jan Hasenauser,Harald Binder*

Main category: stat.ML

TL;DR: 本文提出新方法应对罕见病试验数据建模难题，应用于脊髓性肌萎缩症患者治疗转换影响量化。


<details>
  <summary>Details</summary>
Motivation: 罕见病试验样本量小，测量工具变化使数据轨迹不连续，传统建模方法难以应用，需新方法分析治疗转换影响。

Method: 将各测量工具的观测值映射到对齐的低维时间轨迹，用变分自编码器架构将项目值嵌入共享潜在空间，再通过混合效应回归模型捕捉疾病动态和治疗转换效果，提出新统计检验方法。

Result: 该方法能进行模型选择和评估治疗转换效果，可将不同测量工具的运动表现项目对齐用于混合效应回归，并量化治疗转换效果。

Conclusion: 联合潜在表示建模在解决小数据挑战方面具有潜力。

Abstract: Many rare diseases offer limited established treatment options, leading
patients to switch therapies when new medications emerge. To analyze the impact
of such treatment switches within the low sample size limitations of rare
disease trials, it is important to use all available data sources. This,
however, is complicated when usage of measurement instruments change during the
observation period, for example when instruments are adapted to specific age
ranges. The resulting disjoint longitudinal data trajectories, complicate the
application of traditional modeling approaches like mixed-effects regression.
We tackle this by mapping observations of each instrument to a aligned
low-dimensional temporal trajectory, enabling longitudinal modeling across
instruments. Specifically, we employ a set of variational autoencoder
architectures to embed item values into a shared latent space for each time
point. Temporal disease dynamics and treatment switch effects are then captured
through a mixed-effects regression model applied to latent representations. To
enable statistical inference, we present a novel statistical testing approach
that accounts for the joint parameter estimation of mixed-effects regression
and variational autoencoders. The methodology is applied to quantify the impact
of treatment switches for patients with spinal muscular atrophy. Here, our
approach aligns motor performance items from different measurement instruments
for mixed-effects regression and maps estimated effects back to the observed
item level to quantify the treatment switch effect. Our approach allows for
model selection as well as for assessing effects of treatment switching. The
results highlight the potential of modeling in joint latent representations for
addressing small data challenges.

</details>


### [172] [Monitoring the calibration of probability forecasts with an application to concept drift detection involving image classification](https://arxiv.org/abs/2510.25573)
*Christopher T. Franck,Anne R. Driscoll,Zoe Szajnfarber,William H. Woodall*

Main category: stat.ML

TL;DR: 提出基于累积和的动态限方法检测图像分类模型校准问题，可用于监测概率预测校准情况。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型虽精度高，但缺乏持续监测模型校准情况的方法，需开发相关方法以检测影响图像分类性能的操作环境变化。

Method: 提出基于累积和的动态限方法，用于检测传统过程监测和概念漂移应用中的校准错误。

Result: 该方法可实现对操作环境变化的早期检测，能广泛用于监测概率预测的校准情况。

Conclusion: 此方法基于概率预测和事件结果，无需访问机器学习模型内部，有广泛应用价值。

Abstract: Machine learning approaches for image classification have led to impressive
advances in that field. For example, convolutional neural networks are able to
achieve remarkable image classification accuracy across a wide range of
applications in industry, defense, and other areas. While these machine
learning models boast impressive accuracy, a related concern is how to assess
and maintain calibration in the predictions these models make. A classification
model is said to be well calibrated if its predicted probabilities correspond
with the rates events actually occur. While there are many available methods to
assess machine learning calibration and recalibrate faulty predictions, less
effort has been spent on developing approaches that continually monitor
predictive models for potential loss of calibration as time passes. We propose
a cumulative sum-based approach with dynamic limits that enable detection of
miscalibration in both traditional process monitoring and concept drift
applications. This enables early detection of operational context changes that
impact image classification performance in the field. The proposed chart can be
used broadly in any situation where the user needs to monitor probability
predictions over time for potential lapses in calibration. Importantly, our
method operates on probability predictions and event outcomes and does not
require under-the-hood access to the machine learning model.

</details>


### [173] [How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for Transformers with MLPs](https://arxiv.org/abs/2510.25753)
*Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: 研究预训练Transformer在多数据源非线性任务上的上下文学习（ICL），证明模型与结构化多项式预测器等价，揭示MLP增强ICL性能，分析数据混合效果并实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有理论研究依赖简化架构、数据模型和单源训练，与现实设置相关性有限，需研究更真实场景下的ICL。

Method: 分析含两层MLP的模型，一层单步梯度训练，二层完全优化，利用高斯普遍性和正交多项式理论，进行理论推导和实验验证。

Result: 证明模型与结构化多项式预测器在ICL误差上等价，揭示非线性MLP增强ICL性能，分析数据混合效果，实验验证结论，在多语言情感分析中得到应用。

Conclusion: 推进了Transformer中ICL的理论基础，为ICL中架构和数据的作用提供可操作见解。

Abstract: Pretrained Transformers demonstrate remarkable in-context learning (ICL)
capabilities, enabling them to adapt to new tasks from demonstrations without
parameter updates. However, theoretical studies often rely on simplified
architectures (e.g., omitting MLPs), data models (e.g., linear regression with
isotropic inputs), and single-source training, limiting their relevance to
realistic settings. In this work, we study ICL in pretrained Transformers with
nonlinear MLP heads on nonlinear tasks drawn from multiple data sources with
heterogeneous input, task, and noise distributions. We analyze a model where
the MLP comprises two layers, with the first layer trained via a single
gradient step and the second layer fully optimized. Under high-dimensional
asymptotics, we prove that such models are equivalent in ICL error to
structured polynomial predictors, leveraging results from the theory of
Gaussian universality and orthogonal polynomials. This equivalence reveals that
nonlinear MLPs meaningfully enhance ICL performance, particularly on nonlinear
tasks, compared to linear baselines. It also enables a precise analysis of data
mixing effects: we identify key properties of high-quality data sources (low
noise, structured covariances) and show that feature learning emerges only when
the task covariance exhibits sufficient structure. These results are validated
empirically across various activation functions, model sizes, and data
distributions. Finally, we experiment with a real-world scenario involving
multilingual sentiment analysis where each language is treated as a different
source. Our experimental results for this case exemplify how our findings
extend to real-world cases. Overall, our work advances the theoretical
foundations of ICL in Transformers and provides actionable insight into the
role of architecture and data in ICL.

</details>


### [174] [E-Scores for (In)Correctness Assessment of Generative Model Outputs](https://arxiv.org/abs/2510.25770)
*Guneet S. Dhillon,Javier González,Teodora Pandeva,Alicia Curth*

Main category: stat.ML

TL;DR: 现有评估生成模型正确性机制有限，本文用e值为生成模型输出补充e分数评估正确性，实验证明其在评估大语言模型输出方面有效。


<details>
  <summary>Details</summary>
Motivation: 当前评估生成模型正确性的机制有限，且基于p值的方法易受p - hacking影响，保证性会失效。

Method: 利用e值为生成模型输出补充e分数作为不正确性的度量。

Result: e分数除实现与之前相同的统计保证外，还能通过上界一个称为大小失真的事后误差概念，让用户在观察e分数后灵活选择容忍度。实验证明其在评估大语言模型不同正确性类型输出时有效。

Conclusion: e分数可有效评估生成模型尤其是大语言模型输出的正确性。

Abstract: While generative models, especially large language models (LLMs), are
ubiquitous in today's world, principled mechanisms to assess their
(in)correctness are limited. Using the conformal prediction framework, previous
works construct sets of LLM responses where the probability of including an
incorrect response, or error, is capped at a desired user-defined tolerance
level. However, since these methods are based on p-values, they are susceptible
to p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the
guarantees. We therefore leverage e-values to complement generative model
outputs with e-scores as a measure of incorrectness. In addition to achieving
the same statistical guarantees as before, e-scores provide users flexibility
in adaptively choosing tolerance levels after observing the e-scores
themselves, by upper bounding a post-hoc notion of error called size
distortion. We experimentally demonstrate their efficacy in assessing LLM
outputs for different correctness types: mathematical factuality and property
constraints satisfaction.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [175] [Bayesian Neural Networks vs. Mixture Density Networks: Theoretical and Empirical Insights for Uncertainty-Aware Nonlinear Modeling](https://arxiv.org/abs/2510.25001)
*Riddhi Pratim Ghosh,Ian Barnett*

Main category: stat.CO

TL;DR: 本文对比BNNs和MDNs用于不确定性感知的非线性回归，给出统一框架，理论和实证分析显示二者各有优势。


<details>
  <summary>Details</summary>
Motivation: 研究用于不确定性感知非线性回归的两种概率神经建模范式BNNs和MDNs。

Method: 建立统一理论和实证框架，理论上推导收敛率和误差界，实证上在合成非线性数据集和放射学基准上评估。

Result: 理论上MDNs的KL散度收敛更快，BNNs有额外近似偏差；实证上MDNs能更好捕捉多模态响应和自适应不确定性，BNNs在数据有限时提供更可解释的认知不确定性。

Conclusion: 明确基于后验和基于似然的概率学习的互补优势，为非线性系统的不确定性感知建模提供指导。

Abstract: This paper investigates two prominent probabilistic neural modeling
paradigms: Bayesian Neural Networks (BNNs) and Mixture Density Networks (MDNs)
for uncertainty-aware nonlinear regression. While BNNs incorporate epistemic
uncertainty by placing prior distributions over network parameters, MDNs
directly model the conditional output distribution, thereby capturing
multimodal and heteroscedastic data-generating mechanisms. We present a unified
theoretical and empirical framework comparing these approaches. On the
theoretical side, we derive convergence rates and error bounds under H\"older
smoothness conditions, showing that MDNs achieve faster Kullback-Leibler (KL)
divergence convergence due to their likelihood-based nature, whereas BNNs
exhibit additional approximation bias induced by variational inference.
Empirically, we evaluate both architectures on synthetic nonlinear datasets and
a radiographic benchmark (RSNA Pediatric Bone Age Challenge). Quantitative and
qualitative results demonstrate that MDNs more effectively capture multimodal
responses and adaptive uncertainty, whereas BNNs provide more interpretable
epistemic uncertainty under limited data. Our findings clarify the
complementary strengths of posterior-based and likelihood-based probabilistic
learning, offering guidance for uncertainty-aware modeling in nonlinear
systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [176] [SCOUT: A Lightweight Framework for Scenario Coverage Assessment in Autonomous Driving](https://arxiv.org/abs/2510.24949)
*Anil Yildiz,Sarah M. Thornton,Carl Hildebrandt,Sreeja Roy-Singh,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: 提出轻量级替代模型SCOUT预测场景覆盖标签，可降本提效，是高效场景覆盖监督的重要一步。


<details>
  <summary>Details</summary>
Motivation: 现有评估自主智能体场景覆盖方法依赖昂贵人工标注或高计算量的大视觉语言模型，不适合大规模部署。

Method: 提出SCOUT模型，通过蒸馏过程训练，利用预计算感知特征避免冗余计算。

Result: 在大规模真实自主导航场景数据集上评估，保持高精度同时显著降低计算成本。

Conclusion: SCOUT是大规模覆盖分析的有效实用替代方案，虽性能依赖训练标签质量，但仍是高效场景覆盖监督的重要进展。

Abstract: Assessing scenario coverage is crucial for evaluating the robustness of
autonomous agents, yet existing methods rely on expensive human annotations or
computationally intensive Large Vision-Language Models (LVLMs). These
approaches are impractical for large-scale deployment due to cost and
efficiency constraints. To address these shortcomings, we propose SCOUT
(Scenario Coverage Oversight and Understanding Tool), a lightweight surrogate
model designed to predict scenario coverage labels directly from an agent's
latent sensor representations. SCOUT is trained through a distillation process,
learning to approximate LVLM-generated coverage labels while eliminating the
need for continuous LVLM inference or human annotation. By leveraging
precomputed perception features, SCOUT avoids redundant computations and
enables fast, scalable scenario coverage estimation. We evaluate our method
across a large dataset of real-life autonomous navigation scenarios,
demonstrating that it maintains high accuracy while significantly reducing
computational cost. Our results show that SCOUT provides an effective and
practical alternative for large-scale coverage analysis. While its performance
depends on the quality of LVLM-generated training labels, SCOUT represents a
major step toward efficient scenario coverage oversight in autonomous systems.

</details>


### [177] [Scalable predictive processing framework for multitask caregiving robots](https://arxiv.org/abs/2510.25053)
*Hayato Idei,Tamon Miyake,Tetsuya Ogata,Yuichi Yamashita*

Main category: cs.RO

TL;DR: 受认知神经科学理论启发，提出基于自由能原理的分层多模态循环神经网络，可处理高维输入，学习护理任务，有多种特性，虽评估限于模拟，但为护理机器人发展提供方向。


<details>
  <summary>Details</summary>
Motivation: 社会快速老龄化增加对自主护理机器人的需求，但现有系统泛化能力有限，受人类大脑分层预测处理理论启发开展研究。

Method: 引入基于自由能原理的分层多模态循环神经网络，直接处理超30000维视觉 - 本体感受输入，无需降维。

Result: 模型能学习两项护理任务，具有分层潜在动态自组织、视觉 - 本体感受集成的鲁棒性、多任务学习中的非对称干扰等特性。

Conclusion: 预测处理是通用且可扩展的计算原则，为开发鲁棒、灵活的自主护理机器人提供方向，也为理解人类大脑适应能力提供理论见解。

Abstract: The rapid aging of societies is intensifying demand for autonomous care
robots; however, most existing systems are task-specific and rely on
handcrafted preprocessing, limiting their ability to generalize across diverse
scenarios. A prevailing theory in cognitive neuroscience proposes that the
human brain operates through hierarchical predictive processing, which
underlies flexible cognition and behavior by integrating multimodal sensory
signals. Inspired by this principle, we introduce a hierarchical multimodal
recurrent neural network grounded in predictive processing under the
free-energy principle, capable of directly integrating over 30,000-dimensional
visuo-proprioceptive inputs without dimensionality reduction. The model was
able to learn two representative caregiving tasks, rigid-body repositioning and
flexible-towel wiping, without task-specific feature engineering. We
demonstrate three key properties: (i) self-organization of hierarchical latent
dynamics that regulate task transitions, capture variability in uncertainty,
and infer occluded states; (ii) robustness to degraded vision through
visuo-proprioceptive integration; and (iii) asymmetric interference in
multitask learning, where the more variable wiping task had little influence on
repositioning, whereas learning the repositioning task led to a modest
reduction in wiping performance, while the model maintained overall robustness.
Although the evaluation was limited to simulation, these results establish
predictive processing as a universal and scalable computational principle,
pointing toward robust, flexible, and autonomous caregiving robots while
offering theoretical insight into the human brain's ability to achieve flexible
adaptation in uncertain real-world environments.

</details>


### [178] [One-shot Humanoid Whole-body Motion Learning](https://arxiv.org/abs/2510.25241)
*Hao Huang,Geeta Chandra Raju Bethala,Shuaihang Yuan,Congcong Wen,Anthony Tzes,Yi Fang*

Main category: cs.RO

TL;DR: 本文提出仅用单个非行走目标运动样本和现有行走运动训练人形机器人运动策略的新方法，实验显示其优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法需每个运动类别多个训练样本，收集高质量人体运动数据集费力且成本高。

Method: 利用保序最优传输计算行走与非行走序列距离，沿测地线插值生成新中间姿态骨架，优化无碰撞配置并重新定位到人形机器人，再通过强化学习在模拟环境中训练策略。

Result: 在CMU MoCap数据集上的实验评估表明，该方法始终优于基线，在各项指标上表现更优。

Conclusion: 提出的新方法有效，能在单个非行走目标运动样本下训练出高性能的人形机器人运动策略。

Abstract: Whole-body humanoid motion represents a cornerstone challenge in robotics,
integrating balance, coordination, and adaptability to enable human-like
behaviors. However, existing methods typically require multiple training
samples per motion category, rendering the collection of high-quality human
motion datasets both labor-intensive and costly. To address this, we propose a
novel approach that trains effective humanoid motion policies using only a
single non-walking target motion sample alongside readily available walking
motions. The core idea lies in leveraging order-preserving optimal transport to
compute distances between walking and non-walking sequences, followed by
interpolation along geodesics to generate new intermediate pose skeletons,
which are then optimized for collision-free configurations and retargeted to
the humanoid before integration into a simulated environment for policy
training via reinforcement learning. Experimental evaluations on the CMU MoCap
dataset demonstrate that our method consistently outperforms baselines,
achieving superior performance across metrics. Code will be released upon
acceptance.

</details>


### [179] [SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation](https://arxiv.org/abs/2510.25268)
*Wang zhi,Yuyan Liu,Liu Liu,Li Zhang,Ruixuan Lu,Dan Guo*

Main category: cs.RO

TL;DR: 本文提出SynHLMA框架用于合成铰接物体的手部语言操作，在自建数据集上表现出色，还展示了机器人抓取应用。


<details>
  <summary>Details</summary>
Motivation: 手部抓取合成在手部铰接物体交互（HAOI）中，不仅需要考虑物体功能，还需考虑沿物体变形的长期操作序列，当前缺乏相关有效方法。

Method: 提出SynHLMA框架，利用离散HAOI表示对每个手部物体交互帧建模，结合自然语言嵌入，通过HAOI操作语言模型训练，采用关节感知损失确保手部抓取跟随铰接物体关节动态变化。

Result: SynHLMA实现了HAOI生成、预测和插值三个典型手部操作任务，在自建HAOI - lang数据集上实验表明其手部抓取序列生成性能优于现有技术，还展示了机器人抓取应用。

Conclusion: SynHLMA框架在铰接物体的手部操作序列生成方面表现优异，具有有效性和先进性。

Abstract: Generating hand grasps with language instructions is a widely studied topic
that benefits from embodied AI and VR/AR applications. While transferring into
hand articulatied object interaction (HAOI), the hand grasps synthesis requires
not only object functionality but also long-term manipulation sequence along
the object deformation. This paper proposes a novel HAOI sequence generation
framework SynHLMA, to synthesize hand language manipulation for articulated
objects. Given a complete point cloud of an articulated object, we utilize a
discrete HAOI representation to model each hand object interaction frame. Along
with the natural language embeddings, the representations are trained by an
HAOI manipulation language model to align the grasping process with its
language description in a shared representation space. A joint-aware loss is
employed to ensure hand grasps follow the dynamic variations of articulated
object joints. In this way, our SynHLMA achieves three typical hand
manipulation tasks for articulated objects of HAOI generation, HAOI prediction
and HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset and
experimental results demonstrate the superior hand grasp sequence generation
performance comparing with state-of-the-art. We also show a robotics grasp
application that enables dexterous grasps execution from imitation learning
using the manipulation sequence provided by our SynHLMA. Our codes and datasets
will be made publicly available.

</details>


### [180] [Integrating Legal and Logical Specifications in Perception, Prediction, and Planning for Automated Driving: A Survey of Methods](https://arxiv.org/abs/2510.25386)
*Kumar Manas,Mert Keser,Alois Knoll*

Main category: cs.RO

TL;DR: 本文综述自动驾驶系统中融合法律与逻辑规范的方法，分析挑战并分类现有方法，指出关键问题以指导未来发展。


<details>
  <summary>Details</summary>
Motivation: 确保自动驾驶系统在动态不确定环境中合规且具有可解释性。

Method: 系统探索逻辑框架到计算法律推理等技术，引入分类法对现有方法按理论基础、架构实现和验证策略分类。

Result: 发现感知可靠性、法律合规性和决策合理性交叉处存在重大挑战。

Conclusion: 提出关键开放性问题和实际权衡，提供多学科见解以指导合规自动驾驶系统未来发展。

Abstract: This survey provides an analysis of current methodologies integrating legal
and logical specifications into the perception, prediction, and planning
modules of automated driving systems. We systematically explore techniques
ranging from logic-based frameworks to computational legal reasoning
approaches, emphasizing their capability to ensure regulatory compliance and
interpretability in dynamic and uncertain driving environments. A central
finding is that significant challenges arise at the intersection of perceptual
reliability, legal compliance, and decision-making justifiability. To
systematically analyze these challenges, we introduce a taxonomy categorizing
existing approaches by their theoretical foundations, architectural
implementations, and validation strategies. We particularly focus on methods
that address perceptual uncertainty and incorporate explicit legal norms,
facilitating decisions that are both technically robust and legally defensible.
The review covers neural-symbolic integration methods for perception,
logic-driven rule representation, and norm-aware prediction strategies, all
contributing toward transparent and accountable autonomous vehicle operation.
We highlight critical open questions and practical trade-offs that must be
addressed, offering multidisciplinary insights from engineering, logic, and law
to guide future developments in legally compliant autonomous driving systems.

</details>


### [181] [Learning to Plan & Schedule with Reinforcement-Learned Bimanual Robot Skills](https://arxiv.org/abs/2510.25634)
*Weikang Wan,Fabio Ramos,Xuning Yang,Caelan Garrett*

Main category: cs.RO

TL;DR: 提出分层框架解决长视野富接触双手操作挑战，比端到端RL和传统顺序规划器表现更好


<details>
  <summary>Details</summary>
Motivation: 长视野富接触双手操作需复杂协调，传统纯顺序决策难以应对

Method: 构建单臂和双手原始技能库，用强化学习在GPU加速模拟中训练；在技能组合数据集上训练基于Transformer的规划器作为高级调度器

Result: 在复杂富接触任务上成功率高于端到端RL方法，行为比传统顺序规划器更高效协调

Conclusion: 所提分层框架能有效解决长视野富接触双手操作问题

Abstract: Long-horizon contact-rich bimanual manipulation presents a significant
challenge, requiring complex coordination involving a mixture of parallel
execution and sequential collaboration between arms. In this paper, we
introduce a hierarchical framework that frames this challenge as an integrated
skill planning & scheduling problem, going beyond purely sequential
decision-making to support simultaneous skill invocation. Our approach is built
upon a library of single-arm and bimanual primitive skills, each trained using
Reinforcement Learning (RL) in GPU-accelerated simulation. We then train a
Transformer-based planner on a dataset of skill compositions to act as a
high-level scheduler, simultaneously predicting the discrete schedule of skills
as well as their continuous parameters. We demonstrate that our method achieves
higher success rates on complex, contact-rich tasks than end-to-end RL
approaches and produces more efficient, coordinated behaviors than traditional
sequential-only planners.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [182] [Stable-by-Design Neural Network-Based LPV State-Space Models for System Identification](https://arxiv.org/abs/2510.24757)
*Ahmet Eren Sertbaş,Tufan Kumbasar*

Main category: eess.SY

TL;DR: 提出基于LPV神经网络的状态空间模型，结合训练框架，在基准非线性系统评估中表现出色，凸显稳定性约束神经LPV识别的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统识别方法在捕捉潜在动态并保持稳定性方面存在困难，需要准确建模非线性系统以实现可靠控制。

Method: 提出稳定设计的LPV神经网络状态空间模型，结合编码器和状态空间表示网络，开发整合多步预测损失和状态一致性正则化项的训练框架。

Result: 在基准非线性系统上评估，模型表现优于经典子空间识别方法和近期基于梯度的方法。

Conclusion: 稳定性约束神经LPV识别是建模复杂非线性系统的可扩展且可靠的框架。

Abstract: Accurate modeling of nonlinear systems is essential for reliable control, yet
conventional identification methods often struggle to capture latent dynamics
while maintaining stability. We propose a \textit{stable-by-design LPV neural
network-based state-space} (NN-SS) model that simultaneously learns latent
states and internal scheduling variables directly from data. The
state-transition matrix, generated by a neural network using the learned
scheduling variables, is guaranteed to be stable through a Schur-based
parameterization. The architecture combines an encoder for initial state
estimation with a state-space representer network that constructs the full set
of scheduling-dependent system matrices. For training the NN-SS, we develop a
framework that integrates multi-step prediction losses with a state-consistency
regularization term, ensuring robustness against drift and improving
long-horizon prediction accuracy. The proposed NN-SS is evaluated on benchmark
nonlinear systems, and the results demonstrate that the model consistently
matches or surpasses classical subspace identification methods and recent
gradient-based approaches. These findings highlight the potential of
stability-constrained neural LPV identification as a scalable and reliable
framework for modeling complex nonlinear systems.

</details>


### [183] [Constructive Lyapunov Functions via Topology-Preserving Neural Networks](https://arxiv.org/abs/2510.24730)
*Jaehong Oh*

Main category: eess.SY

TL;DR: 证明ONN在收敛率、边缘效率和计算复杂度上达最优，实证表现佳，ORTFS集成有效果，建立多领域联系并将抽象定理转化为算法。


<details>
  <summary>Details</summary>
Motivation: 探索ONN性能及将抽象定理转化为可扩展算法，用于神经网络等领域的稳定性分析。

Method: 理论证明和实证验证，将ORTSF集成到transformers。

Result: ONN在多项指标上达最优，实证有显著提升，ORTSF集成降低困惑度、加快收敛，建立多领域联系。

Conclusion: 工作将抽象定理转化为可证明的可扩展算法，为多领域建设性稳定性分析开辟道路。

Abstract: We prove that ONN achieves order-optimal performance on convergence rate
($\mu \propto \lambda_2$), edge efficiency ($E = N$ for minimal connectivity $k
= 2$), and computational complexity ($O(N d^2)$). Empirical validation on
3M-node semantic networks demonstrates 99.75\% improvement over baseline
methods, confirming exponential convergence ($\mu = 3.2 \times 10^{-4}$) and
topology preservation. ORTSF integration into transformers achieves 14.7\%
perplexity reduction and 2.3 faster convergence on WikiText-103. We establish
deep connections to optimal control (Hamilton-Jacobi-Bellman), information
geometry (Fisher-efficient natural gradient), topological data analysis
(persistent homology computation in $O(KN)$), discrete geometry (Ricci flow),
and category theory (adjoint functors). This work transforms Massera's abstract
existence theorem into a concrete, scalable algorithm with provable guarantees,
opening pathways for constructive stability analysis in neural networks,
robotics, and distributed systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [184] [SFMS-ALR: Script-First Multilingual Speech Synthesis with Adaptive Locale Resolution](https://arxiv.org/abs/2510.25178)
*Dharma Teja Donepudi*

Main category: cs.SD

TL;DR: 提出SFMS - ALR框架用于多语言语音合成，无需重新训练，能与现有语音集成，对比显示其优势并给出评估策略。


<details>
  <summary>Details</summary>
Motivation: 传统单语言TTS系统无法在混合语言环境中产生自然、可理解的语音，多语言语音合成面临语言转换、脚本差异和韵律不匹配等挑战。

Method: 通过Unicode脚本分割输入文本，进行自适应语言识别确定语言和区域，使用情感感知调整归一化韵律，生成统一SSML表示并在单个TTS请求中合成话语。

Result: 与数据驱动管道对比，显示出灵活性、可解释性和即时可部署性。

Conclusion: 该框架为高质量、独立于引擎的多语言TTS建立了模块化基线，并给出了评估策略。

Abstract: Intra-sentence multilingual speech synthesis (code-switching TTS) remains a
major challenge due to abrupt language shifts, varied scripts, and mismatched
prosody between languages. Conventional TTS systems are typically monolingual
and fail to produce natural, intelligible speech in mixed-language contexts. We
introduce Script-First Multilingual Synthesis with Adaptive Locale Resolution
(SFMS-ALR), an engine-agnostic framework for fluent, real-time code-switched
speech generation. SFMS-ALR segments input text by Unicode script, applies
adaptive language identification to determine each segment's language and
locale, and normalizes prosody using sentiment-aware adjustments to preserve
expressive continuity across languages. The algorithm generates a unified SSML
representation with appropriate "lang" or "voice" spans and synthesizes the
utterance in a single TTS request. Unlike end-to-end multilingual models,
SFMS-ALR requires no retraining and integrates seamlessly with existing voices
from Google, Apple, Amazon, and other providers. Comparative analysis with
data-driven pipelines such as Unicom and Mask LID demonstrates SFMS-ALR's
flexibility, interpretability, and immediate deployability. The framework
establishes a modular baseline for high-quality, engine-independent
multilingual TTS and outlines evaluation strategies for intelligibility,
naturalness, and user preference.

</details>


### [185] [Studies for : A Human-AI Co-Creative Sound Artwork Using a Real-time Multi-channel Sound Generation Model](https://arxiv.org/abs/2510.25228)
*Chihiro Nagashima,Akira Takahashi,Zhi Zhong,Shusuke Takahashi,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: 本文通过与声音艺术家合作创建生成式声音装置，探讨AI技术融入艺术工作流，提出人机共创框架及声音艺术创作与存档新可能。


<details>
  <summary>Details</summary>
Motivation: 探索将AI技术融入艺术工作流，实现一种新的档案保存形式，拓展艺术家作品。

Method: 采用SpecMaskGIT模型，基于艺术家超200小时的过往作品数据集训练，在创作中融入艺术家反馈。

Result: 创建了沉浸式的八声道实时声音装置，模型能体现艺术家风格并生成新声音。

Conclusion: 提出人机共创框架，为声音艺术创作和存档提供新可能。

Abstract: This paper explores the integration of AI technologies into the artistic
workflow through the creation of Studies for, a generative sound installation
developed in collaboration with sound artist Evala
(https://www.ntticc.or.jp/en/archive/works/studies-for/). The installation
employs SpecMaskGIT, a lightweight yet high-quality sound generation AI model,
to generate and playback eight-channel sound in real-time, creating an
immersive auditory experience over the course of a three-month exhibition. The
work is grounded in the concept of a "new form of archive," which aims to
preserve the artistic style of an artist while expanding beyond artists' past
artworks by continued generation of new sound elements. This speculative
approach to archival preservation is facilitated by training the AI model on a
dataset consisting of over 200 hours of Evala's past sound artworks.
  By addressing key requirements in the co-creation of art using AI, this study
highlights the value of the following aspects: (1) the necessity of integrating
artist feedback, (2) datasets derived from an artist's past works, and (3)
ensuring the inclusion of unexpected, novel outputs. In Studies for, the model
was designed to reflect the artist's artistic identity while generating new,
previously unheard sounds, making it a fitting realization of the concept of "a
new form of archive." We propose a Human-AI co-creation framework for
effectively incorporating sound generation AI models into the sound art
creation process and suggest new possibilities for creating and archiving sound
art that extend an artist's work beyond their physical existence. Demo page:
https://sony.github.io/studies-for/

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [186] [Cardi-GPT: An Expert ECG-Record Processing Chatbot](https://arxiv.org/abs/2510.24737)
*Koustav Mallick,Neel Singh,Mohammedreza Hajiarbabi*

Main category: eess.SP

TL;DR: 本文介绍Cardi - GPT专家系统，用深度学习和自然语言交互处理心电图解读与临床沟通，经评估表现出色，是心血管医疗创新。


<details>
  <summary>Details</summary>
Motivation: 心电图解读和沟通在心血管诊断中关键但具挑战，传统方式需专业知识和精确沟通，因此设计Cardi - GPT系统来简化流程。

Method: 采用16残差块卷积神经网络处理12导联心电图数据，用模糊化层转换输出，集成聊天机器人界面。

Result: 在24种心脏疾病中加权准确率达0.6194，在六国六家医院的多样数据集上表现优于基线模型，整体响应质量得分73%。

Conclusion: Cardi - GPT弥合了心电图数据解读和临床见解的差距，是心血管医疗变革性创新，有望提升诊断准确性、临床工作流程和患者预后。

Abstract: Interpreting and communicating electrocardiogram (ECG) findings are crucial
yet challenging tasks in cardiovascular diagnosis, traditionally requiring
significant expertise and precise clinical communication. This paper introduces
Cardi-GPT, an advanced expert system designed to streamline ECG interpretation
and enhance clinical communication through deep learning and natural language
interaction. Cardi-GPT employs a 16-residual-block convolutional neural network
(CNN) to process 12-lead ECG data, achieving a weighted accuracy of 0.6194
across 24 cardiac conditions. A novel fuzzification layer converts complex
numerical outputs into clinically meaningful linguistic categories, while an
integrated chatbot interface facilitates intuitive exploration of diagnostic
insights and seamless communication between healthcare providers.
  The system was evaluated on a diverse dataset spanning six hospitals across
four countries, demonstrating superior performance compared to baseline models.
Additionally, Cardi-GPT achieved an impressive overall response quality score
of 73\%, assessed using a comprehensive evaluation framework that measures
coverage, grounding, and coherence. By bridging the gap between intricate ECG
data interpretation and actionable clinical insights, Cardi-GPT represents a
transformative innovation in cardiovascular healthcare, promising to improve
diagnostic accuracy, clinical workflows, and patient outcomes across diverse
medical settings.

</details>


### [187] [PulseFi: A Low Cost Robust Machine Learning System for Accurate Cardiopulmonary and Apnea Monitoring Using Channel State Information](https://arxiv.org/abs/2510.24744)
*Pranay Kocheta,Nayan Sanjay Bhatia,Katia Obraczka*

Main category: eess.SP

TL;DR: 本文介绍PulseFi系统，利用Wi-Fi传感和人工智能非侵入式监测心率、呼吸率及检测呼吸暂停，成本低且效果好。


<details>
  <summary>Details</summary>
Motivation: 非侵入式生命体征监测在医疗场景中愈发重要，需要低成本且有效的监测系统。

Method: 使用低成本商用设备，通过信号处理管道处理Wi-Fi遥测数据（CSI），并输入自定义低计算量的LSTM神经网络模型。使用两个数据集进行评估。

Result: PulseFi能以无缝非侵入方式有效估计心率和呼吸率，准确性与昂贵且不易获取的多天线系统相当或更好。

Conclusion: PulseFi是一种低成本、非侵入式且准确有效的生命体征监测系统。

Abstract: Non-intrusive monitoring of vital signs has become increasingly important in
a variety of healthcare settings. In this paper, we present PulseFi, a novel
low-cost non-intrusive system that uses Wi-Fi sensing and artificial
intelligence to accurately and continuously monitor heart rate and breathing
rate, as well as detect apnea events. PulseFi operates using low-cost commodity
devices, making it more accessible and cost-effective. It uses a signal
processing pipeline to process Wi-Fi telemetry data, specifically Channel State
Information (CSI), that is fed into a custom low-compute Long Short-Term Memory
(LSTM) neural network model. We evaluate PulseFi using two datasets: one that
we collected locally using ESP32 devices and another that contains recordings
of 118 participants collected using the Raspberry Pi 4B, making the latter the
most comprehensive data set of its kind. Our results show that PulseFi can
effectively estimate heart rate and breathing rate in a seemless non-intrusive
way with comparable or better accuracy than multiple antenna systems that can
be expensive and less accessible.

</details>


### [188] [EcoScaleNet: A Lightweight Multi Kernel Network for Long Sequence 12 lead ECG Classification](https://arxiv.org/abs/2510.24748)
*Dong-Hyeon Kang,Ju-Hyeon Nam,Sang-Chul Lee*

Main category: eess.SP

TL;DR: 提出EcoScale - Net用于长序列心电图分类，减少计算成本且提高准确率。


<details>
  <summary>Details</summary>
Motivation: 手动解读心电图易出错，现有基于CNN的分类器难选合适感受野大小，OS CNN计算成本高。

Method: 提出分层的EcoScale - Net，在每阶段限制最大内核长度，在Omni Scale块前后插入瓶颈卷积。

Result: 在CODE 15% ECG数据集上，比OS CNN减少90%参数和99%FLOPs，宏观平均F1分数提高2.4%。

Conclusion: EcoScale - Net以低计算成本实现长序列心电图分类的SOTA准确率，可在普通硬件上实时部署。

Abstract: Accurate interpretation of 12 lead electrocardiograms (ECGs) is critical for
early detection of cardiac abnormalities, yet manual reading is error prone and
existing CNN based classifiers struggle to choose receptive field sizes that
generalize to the long sequences typical of ECGs. Omni Scale CNN (OS CNN)
addresses this by enumerating prime sized kernels inspired by Goldbach
conjecture to cover every scale, but its exhaustive design explodes
computational cost and blocks deeper, wider models. We present Efficient
Convolutional Omni Scale Network (EcoScale-Net), a hierarchical variant that
retains full receptive field coverage while eliminating redundancy. At each
stage, the maximum kernel length is capped to the scale still required after
down sampling, and bottleneck convolutions inserted before and after every Omni
Scale block curtail channel growth and fuse multi scale features. On the large
scale CODE 15% ECG dataset, EcoScaleNet reduces parameters by 90% and FLOPs by
99% compared with OS CNN, while raising macro averaged F1 score by 2.4%. These
results demonstrate that EcoScaleNet delivers SOTA accuracy for long sequence
ECG classification at a fraction of the computational cost, enabling real time
deployment on commodity hardware. Our EcoScaleNet code is available in GitHub
Link.

</details>


### [189] [Decoding non-invasive brain activity with novel deep-learning approaches](https://arxiv.org/abs/2510.24733)
*Richard Csaky*

Main category: eess.SP

TL;DR: 论文研究非侵入性脑电生理信号的建模与解码，分方法和实验两部分，探索深度学习用于脑解码，介绍新方法处理变异性，实验部分收集内隐言语数据但解码结果多为阴性。


<details>
  <summary>Details</summary>
Motivation: 研究大脑在感知视觉刺激或进行内隐言语时的情况，提高此类刺激的解码性能。

Method: 方法部分探索深度学习用于脑解码，用线性模型解码个体视觉刺激，引入新方法处理个体间变异性，探索基于卷积和Transformer架构的MEG数据预测模型；实验部分收集高试验次数的内隐言语EEG、MEG和OPM数据。

Result: Transformer模型在生成与真实脑数据匹配的信号上表现出色；内隐言语解码结果大多为阴性。

Conclusion: Transformer架构模型能提升脑电生理建模的准确性和可靠性；内隐言语解码难度大。

Abstract: This thesis delves into the world of non-invasive electrophysiological brain
signals like electroencephalography (EEG) and magnetoencephalography (MEG),
focusing on modelling and decoding such data. The research aims to investigate
what happens in the brain when we perceive visual stimuli or engage in covert
speech (inner speech) and enhance the decoding performance of such stimuli. The
thesis is divided into two main sections, methodological and experimental work.
A central concern in both sections is the large variability present in
electrophysiological recordings, whether it be within-subject or
between-subject variability, and to a certain extent between-dataset
variability. In the methodological sections, we explore the potential of deep
learning for brain decoding. We present advancements in decoding visual stimuli
using linear models at the individual subject level. We then explore how deep
learning techniques can be employed for group decoding, introducing new methods
to deal with between-subject variability. Finally, we also explores novel
forecasting models of MEG data based on convolutional and Transformer-based
architectures. In particular, Transformer-based models demonstrate superior
capabilities in generating signals that closely match real brain data, thereby
enhancing the accuracy and reliability of modelling the brain's
electrophysiology. In the experimental section, we present a unique dataset
containing high-trial inner speech EEG, MEG, and preliminary optically pumped
magnetometer (OPM) data. Our aim is to investigate different types of inner
speech and push decoding performance by collecting a high number of trials and
sessions from a few participants. However, the decoding results are found to be
mostly negative, underscoring the difficulty of decoding inner speech.

</details>


### [190] [StrikeWatch: Wrist-worn Gait Recognition with Compact Time-series Models on Low-power FPGAs](https://arxiv.org/abs/2510.24738)
*Tianheng Ling,Chao Qian,Peter Zdankin,Torben Weis,Gregor Schiele*

Main category: eess.SP

TL;DR: 本文介绍了腕戴式实时步态识别系统StrikeWatch，提出四种DL架构并在FPGA上优化，评估显示模型复杂度和硬件效率有折衷，6位量化1D - SepCNN效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有步态分析系统笨重且多为离线分析，腕戴设备实时步态识别有挑战，为解决这些问题开展研究。

Method: 提出1D - CNN、1D - SepCNN、LSTM和Transformer四种DL架构，在AMD Spartan - 7 XC7S15和Lattice iCE40UP5K两款FPGA上优化，用定制硬件原型收集数据集，通过自动化部署流程评估模型。

Result: 模型复杂度和硬件效率存在折衷，6位量化1D - SepCNN平均F1分数最高达0.847，在iCE40UP5K上每次推理能耗0.350 μJ，延迟0.140 ms，320 mAh电池支持13.6天连续推理。

Conclusion: StrikeWatch系统能实现腕戴式实时步态识别，6位量化1D - SepCNN在性能和能耗上表现良好，相关数据集和代码开源。

Abstract: Running offers substantial health benefits, but improper gait patterns can
lead to injuries, particularly without expert feedback. While prior gait
analysis systems based on cameras, insoles, or body-mounted sensors have
demonstrated effectiveness, they are often bulky and limited to offline,
post-run analysis. Wrist-worn wearables offer a more practical and
non-intrusive alternative, yet enabling real-time gait recognition on such
devices remains challenging due to noisy Inertial Measurement Unit (IMU)
signals, limited computing resources, and dependence on cloud connectivity.
This paper introduces StrikeWatch, a compact wrist-worn system that performs
entirely on-device, real-time gait recognition using IMU signals. As a case
study, we target the detection of heel versus forefoot strikes to enable
runners to self-correct harmful gait patterns through visual and auditory
feedback during running. We propose four compact DL architectures (1D-CNN,
1D-SepCNN, LSTM, and Transformer) and optimize them for energy-efficient
inference on two representative embedded Field-Programmable Gate Arrays
(FPGAs): the AMD Spartan-7 XC7S15 and the Lattice iCE40UP5K. Using our
custom-built hardware prototype, we collect a labeled dataset from outdoor
running sessions and evaluate all models via a fully automated deployment
pipeline. Our results reveal clear trade-offs between model complexity and
hardware efficiency. Evaluated across 12 participants, 6-bit quantized
1D-SepCNN achieves the highest average F1 score of 0.847 while consuming just
0.350 {\mu}J per inference with a latency of 0.140 ms on the iCE40UP5K running
at 20 MHz. This configuration supports up to 13.6 days of continuous inference
on a 320 mAh battery. All datasets and code are available in the GitHub
repository https://github.com/tianheng-ling/StrikeWatch.

</details>


### [191] [Comparative Analysis of Data Augmentation for Clinical ECG Classification with STAR](https://arxiv.org/abs/2510.24740)
*Nader Nemati*

Main category: eess.SP

TL;DR: 本文介绍了用于临床12导联心电图分类的STAR增强方法，它能在保留关键形态的同时增加训练多样性，具有多种优势并已开源。


<details>
  <summary>Details</summary>
Motivation: 临床12导联心电图分类因记录条件多样、病理重叠和标签不平衡等问题难以泛化，无约束增强可能扭曲关键形态。

Method: 引入Sinusoidal Time--Amplitude Resampling (STAR) 逐搏增强方法，在连续R波峰之间进行时间扭曲和幅度缩放。

Result: STAR具有形态忠实的可变性、跨源训练稳定性、与常见编码器集成能力和改善稀有类学习等优点，还发布了完整Python实现和透明训练工作流程。

Conclusion: STAR为临床心电图分类提供了简单可控的增强方法，在形态保真、操作简单和跨源耐久性方面很重要。

Abstract: Clinical 12-lead ECG classification remains difficult because of diverse
recording conditions, overlapping pathologies, and pronounced label imbalance
hinder generalization, while unconstrained augmentations risk distorting
diagnostically critical morphology. In this study, Sinusoidal Time--Amplitude
Resampling (STAR) is introduced as a beat-wise augmentation that operates
strictly between successive R-peaks to apply controlled time warping and
amplitude scaling to each R--R segment, preserving the canonical P--QRS--T
order and leaving the head and tail of the trace unchanged. STAR is designed
for practical pipelines and offers: (i) morphology-faithful variability that
broadens training diversity without corrupting peaks or intervals; (ii)
source-resilient training, improving stability across devices, sites, and
cohorts without dataset-specific tuning; (iii) model-agnostic integration with
common 1D SE--ResNet-style ECG encoders backbone; and (iv) better learning on
rare classes via beat-level augmentation, reducing overfitting by resampling
informative beats instead of duplicating whole records. In contrast to global
crops, large shifts, or additive noise, STAR avoids transformations that
suppress or misalign clinical landmarks. A complete Python implementation and a
transparent training workflow are released, aligned with a source-aware,
stratified five-fold protocol over a multi-institutional 12-lead corpus,
thereby facilitating inspection and reuse. Taken together, STAR provides a
simple and controllable augmentation for clinical ECG classification where
trustworthy morphology, operational simplicity, and cross-source durability are
essential.

</details>


### [192] [Adaptive End-to-End Transceiver Design for NextG Pilot-Free and CP-Free Wireless Systems](https://arxiv.org/abs/2510.25416)
*Jiaming Cheng,Wei Chen,Bo Ai*

Main category: eess.SP

TL;DR: 提出适用于无导频和无循环前缀无线系统的自适应端到端收发器架构，结合多种技术提升性能，模拟显示其在下一代AI原生系统有潜力。


<details>
  <summary>Details</summary>
Motivation: 传统正交频分复用（OFDM）系统依赖导频和循环前缀，开销大且频谱效率低，需改进。

Method: 提出自适应端到端收发器架构，结合AI驱动星座整形和神经接收器进行联合训练；引入轻量级信道适配器模块；提出可扩展到多调制阶数的框架；采用受限端到端训练。

Result: 广泛模拟表明该框架在不同信道场景下比特误码率、吞吐量和恢复能力表现优越。

Conclusion: 所提框架在AI原生下一代系统有应用潜力。

Abstract: The advent of artificial intelligence (AI)-native wireless communication is
fundamentally reshaping the design paradigm of next-generation (NextG) systems,
where intelligent air interfaces are expected to operate adaptively and
efficiently in highly dynamic environments. Conventional orthogonal frequency
division multiplexing (OFDM) systems rely heavily on pilots and the cyclic
prefix (CP), resulting in significant overhead and reduced spectral efficiency.
To address these limitations, we propose an adaptive end-to-end (E2E)
transceiver architecture tailored for pilot-free and CP-free wireless systems.
The architecture combines AI-driven constellation shaping and a neural receiver
through joint training. To enhance robustness against mismatched or
time-varying channel conditions, we introduce a lightweight channel adapter
(CA) module, which enables rapid adaptation with minimal computational overhead
by updating only the CA parameters. Additionally, we present a framework that
is scalable to multiple modulation orders within a unified model, significantly
reducing model storage requirements. Moreover, to tackle the high
peak-to-average power ratio (PAPR) inherent to OFDM, we incorporate constrained
E2E training, achieving compliance with PAPR targets without additional
transmission overhead. Extensive simulations demonstrate that the proposed
framework delivers superior bit error rate (BER), throughput, and resilience
across diverse channel scenarios, highlighting its potential for AI-native
NextG.

</details>


### [193] [Continuous subsurface property retrieval from sparse radar observations using physics informed neural networks](https://arxiv.org/abs/2510.25648)
*Ishfaq Aziz,Mohamad Alipour*

Main category: eess.SP

TL;DR: 提出物理信息机器学习框架重建地下介电常数，经模拟和实验验证，结果与测量吻合度高，能实现连续属性估计。


<details>
  <summary>Details</summary>
Motivation: 传统波反演方法在实际场景中因假设离散均匀层、需密集测量或强先验知识，限制了扩展性和准确性，需要新方法。

Method: 提出物理信息机器学习框架，将地下介电常数重建为深度的全神经连续函数，训练时满足测量数据和麦克斯韦方程。

Result: 模拟和实验结果与原位介电常数测量结果高度吻合（R^2 = 0.93），对细微变化敏感，两层系统中仅三个策略性放置的传感器就能恢复准确剖面。

Conclusion: 该方法将地下反演从边界驱动转变为连续属性估计，能准确表征平滑介电常数变化，推动低成本雷达系统的电磁成像。

Abstract: Estimating subsurface dielectric properties is essential for applications
ranging from environmental surveys of soils to nondestructive evaluation of
concrete in infrastructure. Conventional wave inversion methods typically
assume few discrete homogeneous layers and require dense measurements or strong
prior knowledge of material boundaries, limiting scalability and accuracy in
realistic settings where properties vary continuously. We present a physics
informed machine learning framework that reconstructs subsurface permittivity
as a fully neural, continuous function of depth, trained to satisfy both
measurement data and Maxwells equations. We validate the framework with both
simulations and custom built radar experiments on multilayered natural
materials. Results show close agreement with in-situ permittivity measurements
(R^2=0.93), with sensitivity to even subtle variations (Delta eps_r=2).
Parametric analysis reveals that accurate profiles can be recovered with as few
as three strategically placed sensors in two layer systems. This approach
reframes subsurface inversion from boundary-driven to continuous property
estimation, enabling accurate characterization of smooth permittivity
variations and advancing electromagnetic imaging using low cost radar systems.

</details>


### [194] [PyDPF: A Python Package for Differentiable Particle Filtering](https://arxiv.org/abs/2510.25693)
*John-Joseph Brady,Benjamin Cox,Víctor Elvira,Yunpeng Li*

Main category: eess.SP

TL;DR: 本文实现了基于PyTorch的可微粒子滤波器（DPFs）统一API，方便研究人员使用和比较算法，并验证框架有效性及展示其应用。


<details>
  <summary>Details</summary>
Motivation: 标准粒子滤波需估计未知参数且不可微，虽有改进方法但缺乏统一实现，故实现统一API以方便研究人员。

Method: 基于PyTorch框架实现多个可微粒子滤波器的统一API。

Result: 通过复现已有研究实验验证了框架，展示了DPFs可解决状态空间建模的常见挑战。

Conclusion: 实现的统一API让算法更易获取，便于研究人员比较，且验证了DPFs在状态空间建模的实用性。

Abstract: State-space models (SSMs) are a widely used tool in time series analysis. In
the complex systems that arise from real-world data, it is common to employ
particle filtering (PF), an efficient Monte Carlo method for estimating the
hidden state corresponding to a sequence of observations. Applying particle
filtering requires specifying both the parametric form and the parameters of
the system, which are often unknown and must be estimated. Gradient-based
optimisation techniques cannot be applied directly to standard particle
filters, as the filters themselves are not differentiable. However, several
recently proposed methods modify the resampling step to make particle filtering
differentiable. In this paper, we present an implementation of several such
differentiable particle filters (DPFs) with a unified API built on the popular
PyTorch framework. Our implementation makes these algorithms easily accessible
to a broader research community and facilitates straightforward comparison
between them. We validate our framework by reproducing experiments from several
existing studies and demonstrate how DPFs can be applied to address several
common challenges with state space modelling.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [195] [CT-Less Attenuation Correction Using Multiview Ensemble Conditional Diffusion Model on High-Resolution Uncorrected PET Images](https://arxiv.org/abs/2510.24805)
*Alexandre St-Georges,Gabriel Richard,Maxime Toussaint,Christian Thibaudeau,Etienne Auger,Étienne Croteau,Stephen Cunnane,Roger Lecomte,Jean-Baptiste Michaud*

Main category: q-bio.QM

TL;DR: 本文探讨PET成像中衰减校正问题，提出用条件去噪扩散概率模型（DDPMs）从非衰减校正PET图像生成高质量CT图像进行衰减校正，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: PET成像中衰减问题影响定量准确性，传统CT校正方法有辐射、配准和成本问题，需新方法。

Method: 利用条件去噪扩散概率模型（DDPMs），结合非衰减校正PET图像的三个正交视图和集成投票生成伪CT图像。

Result: 对159例头部扫描的研究显示，伪CT生成在定性和定量上均有改善，CT图像平均绝对误差为32 ± 10.4 HU，PET图像平均误差为(1.48 ± 0.68)%。

Conclusion: DDPM方法能生成高质量伪CT图像，减少伪影并提高切片一致性，可用于PET成像的衰减校正。

Abstract: Accurate quantification in positron emission tomography (PET) is essential
for accurate diagnostic results and effective treatment tracking. A major issue
encountered in PET imaging is attenuation. Attenuation refers to the diminution
of photon detected as they traverse biological tissues before reaching
detectors. When such corrections are absent or inadequate, this signal
degradation can introduce inaccurate quantification, making it difficult to
differentiate benign from malignant conditions, and can potentially lead to
misdiagnosis. Typically, this correction is done with co-computed Computed
Tomography (CT) imaging to obtain structural data for calculating photon
attenuation across the body. However, this methodology subjects patients to
extra ionizing radiation exposure, suffers from potential spatial
misregistration between PET/CT imaging sequences, and demands costly equipment
infrastructure. Emerging advances in neural network architectures present an
alternative approach via synthetic CT image synthesis. Our investigation
reveals that Conditional Denoising Diffusion Probabilistic Models (DDPMs) can
generate high quality CT images from non attenuation corrected PET images in
order to correct attenuation. By utilizing all three orthogonal views from
non-attenuation-corrected PET images, the DDPM approach combined with ensemble
voting generates higher quality pseudo-CT images with reduced artifacts and
improved slice-to-slice consistency. Results from a study of 159 head scans
acquired with the Siemens Biograph Vision PET/CT scanner demonstrate both
qualitative and quantitative improvements in pseudo-CT generation. The method
achieved a mean absolute error of 32 $\pm$ 10.4 HU on the CT images and an
average error of (1.48 $\pm$ 0.68)\% across all regions of interest when
comparing PET images reconstructed using the attenuation map of the generated
pseudo-CT versus the true CT.

</details>


### [196] [scMRDR: A scalable and flexible framework for unpaired single-cell multi-omics data integration](https://arxiv.org/abs/2510.24987)
*Jianle Sun,Chaoqi Liang,Ran Wei,Peng Zheng,Lei Bai,Wanli Ouyang,Hongliang Yan,Peng Ye*

Main category: q-bio.QM

TL;DR: 提出scMRDR框架用于非配对多组学单细胞数据整合，表现出色且可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有非配对多组学单细胞数据整合方法存在可扩展性和灵活性问题。

Method: 引入scMRDR框架，用精心设计的β - VAE架构将细胞潜在表征解耦，增加等距正则化、对抗目标和掩码重建损失策略。

Result: 在基准数据集的批次校正、模态对齐和生物信号保留方面表现出色，能有效扩展到大规模数据集，支持两种以上组学整合。

Conclusion: scMRDR为大规模多组学数据整合和下游生物发现提供强大灵活的解决方案。

Abstract: Advances in single-cell sequencing have enabled high-resolution profiling of
diverse molecular modalities, while integrating unpaired multi-omics
single-cell data remains challenging. Existing approaches either rely on pair
information or prior correspondences, or require computing a global pairwise
coupling matrix, limiting their scalability and flexibility. In this paper, we
introduce a scalable and flexible generative framework called single-cell
Multi-omics Regularized Disentangled Representations (scMRDR) for unpaired
multi-omics integration. Specifically, we disentangle each cell's latent
representations into modality-shared and modality-specific components using a
well-designed $\beta$-VAE architecture, which are augmented with isometric
regularization to preserve intra-omics biological heterogeneity, adversarial
objective to encourage cross-modal alignment, and masked reconstruction loss
strategy to address the issue of missing features across modalities. Our method
achieves excellent performance on benchmark datasets in terms of batch
correction, modality alignment, and biological signal preservation. Crucially,
it scales effectively to large-level datasets and supports integration of more
than two omics, offering a powerful and flexible solution for large-scale
multi-omics data integration and downstream biological discovery.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [197] [A Study on Inference Latency for Vision Transformers on Mobile Devices](https://arxiv.org/abs/2510.25166)
*Zhuojin Li,Marco Paolieri,Leana Golubchik*

Main category: cs.CV

TL;DR: 本文定量研究190个ViT在移动设备上的性能，对比102个CNN，开发数据集预测新ViT推理延迟。


<details>
  <summary>Details</summary>
Motivation: 随着移动设备机器学习技术发展，尤其是计算机视觉领域，需要了解ViT在移动设备上的性能特征及影响延迟的因素。

Method: 定量研究190个ViT在移动设备上的性能，与102个CNN对比，开发含1000个合成ViT延迟数据的数据集。

Result: 能够以足够的精度预测新ViT在现实应用中的推理延迟。

Conclusion: 通过研究和数据集开发，可以为移动设备上ViT的应用提供延迟预测支持。

Abstract: Given the significant advances in machine learning techniques on mobile
devices, particularly in the domain of computer vision, in this work we
quantitatively study the performance characteristics of 190 real-world vision
transformers (ViTs) on mobile devices. Through a comparison with 102 real-world
convolutional neural networks (CNNs), we provide insights into the factors that
influence the latency of ViT architectures on mobile devices. Based on these
insights, we develop a dataset including measured latencies of 1000 synthetic
ViTs with representative building blocks and state-of-the-art architectures
from two machine learning frameworks and six mobile platforms. Using this
dataset, we show that inference latency of new ViTs can be predicted with
sufficient accuracy for real-world applications.

</details>


### [198] [Towards Fine-Grained Human Motion Video Captioning](https://arxiv.org/abs/2510.24767)
*Guorui Song,Guocun Wang,Zhe Huang,Jing Lin,Xuefei Zhe,Jian Li,Haoqian Wang*

Main category: cs.CV

TL;DR: 提出运动增强字幕模型M - ACM和HMI数据集及HMI - Bench基准，实验显示M - ACM在描述人类动作上优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕模型难以捕捉细粒度运动细节，导致字幕模糊或语义不一致。

Method: 引入M - ACM，利用人体网格恢复的运动表示进行运动感知解码；创建HMI数据集和HMI - Bench基准。

Result: M - ACM在描述复杂人类动作和细微时间变化上显著优于先前方法。

Conclusion: M - ACM为以运动为中心的视频字幕设定了新标准。

Abstract: Generating accurate descriptions of human actions in videos remains a
challenging task for video captioning models. Existing approaches often
struggle to capture fine-grained motion details, resulting in vague or
semantically inconsistent captions. In this work, we introduce the
Motion-Augmented Caption Model (M-ACM), a novel generative framework that
enhances caption quality by incorporating motion-aware decoding. At its core,
M-ACM leverages motion representations derived from human mesh recovery to
explicitly highlight human body dynamics, thereby reducing hallucinations and
improving both semantic fidelity and spatial alignment in the generated
captions. To support research in this area, we present the Human Motion Insight
(HMI) Dataset, comprising 115K video-description pairs focused on human
movement, along with HMI-Bench, a dedicated benchmark for evaluating
motion-focused video captioning. Experimental results demonstrate that M-ACM
significantly outperforms previous methods in accurately describing complex
human motions and subtle temporal variations, setting a new standard for
motion-centric video captioning.

</details>


### [199] [Combining SAR Simulators to Train ATR Models with Synthetic Data](https://arxiv.org/abs/2510.24768)
*Benjamin Camus,Julien Houssay,Corentin Le Barbu,Eric Monteux,Cédric Saleun,Christian Cochin*

Main category: cs.CV

TL;DR: 本文旨在利用合成孔径雷达（SAR）图像进行自动目标识别（ATR），针对合成数据与真实数据差异问题，提出结合两种不同范式的SAR模拟器生成合成数据集的方法，在MSTAR测量上达到近88%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决因缺乏真实标记测量数据，使用合成数据训练的ATR模型在真实测量上泛化能力差的问题。

Method: 结合基于散射中心模型方法的MOCEM模拟器和基于射线追踪策略的Salsa模拟器生成合成数据集，使用名为ADASCA的深度学习方法训练ATR模型。

Result: 在MSTAR测量上达到近88%的准确率。

Conclusion: 结合两种不同范式的SAR模拟器生成合成数据集的方法有助于解决ATR问题，提高模型在真实测量上的准确率。

Abstract: This work aims to train Deep Learning models to perform Automatic Target
Recognition (ATR) on Synthetic Aperture Radar (SAR) images. To circumvent the
lack of real labelled measurements, we resort to synthetic data produced by SAR
simulators. Simulation offers full control over the virtual environment, which
enables us to generate large and diversified datasets at will. However,
simulations are intrinsically grounded on simplifying assumptions of the real
world (i.e. physical models). Thus, synthetic datasets are not as
representative as real measurements. Consequently, ATR models trained on
synthetic images cannot generalize well on real measurements. Our contributions
to this problem are twofold: on one hand, we demonstrate and quantify the
impact of the simulation paradigm on the ATR. On the other hand, we propose a
new approach to tackle the ATR problem: combine two SAR simulators that are
grounded on different (but complementary) paradigms to produce synthetic
datasets. To this end, we use two simulators: MOCEM, which is based on a
scattering centers model approach, and Salsa, which resorts on a ray tracing
strategy. We train ATR models using synthetic dataset generated both by MOCEM
and Salsa and our Deep Learning approach called ADASCA. We reach an accuracy of
almost 88 % on the MSTAR measurements.

</details>


### [200] [Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2510.24777)
*Yujie Nie,Jianzhang Ni,Yonglong Ye,Yuan-Ting Zhang,Yun Kwok Wing,Xiangqing Xu,Xin Ma,Lizhou Fan*

Main category: cs.CV

TL;DR: 本文提出多模态交叉增强融合框架用于阿尔茨海默病检测，构建数据集验证其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病准确诊断很重要，多模态诊断有前景，但少有研究结合眼动和面部特征辅助诊断。

Method: 提出多模态交叉增强融合框架，含交叉增强融合注意力模块和方向感知卷积模块，构建同步多模态数据集。

Result: 框架在数据集实验中分类准确率达95.11%，优于传统方法。

Conclusion: 该框架通过显式建模模态间依赖和特定模态贡献，具有更强鲁棒性和诊断性能。

Abstract: Accurate diagnosis of Alzheimer's disease (AD) is essential for enabling
timely intervention and slowing disease progression. Multimodal diagnostic
approaches offer considerable promise by integrating complementary information
across behavioral and perceptual domains. Eye-tracking and facial features, in
particular, are important indicators of cognitive function, reflecting
attentional distribution and neurocognitive state. However, few studies have
explored their joint integration for auxiliary AD diagnosis. In this study, we
propose a multimodal cross-enhanced fusion framework that synergistically
leverages eye-tracking and facial features for AD detection. The framework
incorporates two key modules: (a) a Cross-Enhanced Fusion Attention Module
(CEFAM), which models inter-modal interactions through cross-attention and
global enhancement, and (b) a Direction-Aware Convolution Module (DACM), which
captures fine-grained directional facial features via horizontal-vertical
receptive fields. Together, these modules enable adaptive and discriminative
multimodal representation learning. To support this work, we constructed a
synchronized multimodal dataset, including 25 patients with AD and 25 healthy
controls (HC), by recording aligned facial video and eye-tracking sequences
during a visual memory-search paradigm, providing an ecologically valid
resource for evaluating integration strategies. Extensive experiments on this
dataset demonstrate that our framework outperforms traditional late fusion and
feature concatenation methods, achieving a classification accuracy of 95.11% in
distinguishing AD from HC, highlighting superior robustness and diagnostic
performance by explicitly modeling inter-modal dependencies and
modality-specific contributions.

</details>


### [201] [ESCA: Enabling Seamless Codec Avatar Execution through Algorithm and Hardware Co-Optimization for Virtual Reality](https://arxiv.org/abs/2510.24787)
*Mingzhi Zhu,Ding Shang,Sai Qian Zhang*

Main category: cs.CV

TL;DR: 提出ESCA全栈优化框架加速光逼真编解码化身（PCA）在边缘VR平台推理，实验证明其能提升质量、降低延迟并满足实时要求。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的PCA模型计算需求大，在资源受限的VR设备上实现实时推理有挑战。

Method: 提出适用于编解码化身模型的高效训练后量化（PTQ）方法，设计可集成到VR设备片上系统的自定义硬件加速器，构建ESCA全栈优化框架。

Result: ESCA使FovVideoVDP质量得分最高提升0.39，延迟最多降低3.36倍，端到端测试渲染速率达100帧/秒。

Conclusion: 在资源受限设备上部署高保真编解码化身可行，能带来更沉浸便携的VR体验。

Abstract: Photorealistic Codec Avatars (PCA), which generate high-fidelity human face
renderings, are increasingly being used in Virtual Reality (VR) environments to
enable immersive communication and interaction through deep learning-based
generative models. However, these models impose significant computational
demands, making real-time inference challenging on resource-constrained VR
devices such as head-mounted displays, where latency and power efficiency are
critical. To address this challenge, we propose an efficient post-training
quantization (PTQ) method tailored for Codec Avatar models, enabling
low-precision execution without compromising output quality. In addition, we
design a custom hardware accelerator that can be integrated into the
system-on-chip of VR devices to further enhance processing efficiency. Building
on these components, we introduce ESCA, a full-stack optimization framework
that accelerates PCA inference on edge VR platforms. Experimental results
demonstrate that ESCA boosts FovVideoVDP quality scores by up to $+0.39$ over
the best 4-bit baseline, delivers up to $3.36\times$ latency reduction, and
sustains a rendering rate of 100 frames per second in end-to-end tests,
satisfying real-time VR requirements. These results demonstrate the feasibility
of deploying high-fidelity codec avatars on resource-constrained devices,
opening the door to more immersive and portable VR experiences.

</details>


### [202] [The Underappreciated Power of Vision Models for Graph Structural Understanding](https://arxiv.org/abs/2510.24788)
*Xinjian Zhao,Wei Pang,Zhongkai Xue,Xiangru Jian,Lei Zhang,Yaoyao Xu,Xiaozhuang Song,Shu Wu,Tianshu Yu*

Main category: cs.CV

TL;DR: 研究视觉模型用于图理解的潜力，引入GraphAbstract基准测试，发现视觉模型在全局结构理解任务上优于GNN。


<details>
  <summary>Details</summary>
Motivation: 图神经网络与人类视觉感知方式不同，现有基准测试混淆领域特征和拓扑理解，挖掘视觉模型在图理解方面被低估的潜力。

Method: 引入GraphAbstract基准测试，评估模型感知全局图属性的能力。

Result: 视觉模型在需要整体结构理解的任务上显著优于GNN，且在不同图规模上有泛化性，GNN在全局模式抽象上有困难。

Conclusion: 视觉模型在图结构理解方面有未充分利用的能力，为开发更有效的图基础模型开辟新途径。

Abstract: Graph Neural Networks operate through bottom-up message-passing,
fundamentally differing from human visual perception, which intuitively
captures global structures first. We investigate the underappreciated potential
of vision models for graph understanding, finding they achieve performance
comparable to GNNs on established benchmarks while exhibiting distinctly
different learning patterns. These divergent behaviors, combined with
limitations of existing benchmarks that conflate domain features with
topological understanding, motivate our introduction of GraphAbstract. This
benchmark evaluates models' ability to perceive global graph properties as
humans do: recognizing organizational archetypes, detecting symmetry, sensing
connectivity strength, and identifying critical elements. Our results reveal
that vision models significantly outperform GNNs on tasks requiring holistic
structural understanding and maintain generalizability across varying graph
scales, while GNNs struggle with global pattern abstraction and degrade with
increasing graph size. This work demonstrates that vision models possess
remarkable yet underutilized capabilities for graph structural understanding,
particularly for problems requiring global topological awareness and
scale-invariant reasoning. These findings open new avenues to leverage this
underappreciated potential for developing more effective graph foundation
models for tasks dominated by holistic pattern recognition.

</details>


### [203] [PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for the Evaluation of Vision-Language Models](https://arxiv.org/abs/2510.24792)
*Patrick Haller,Fabio Barth,Jonas Golde,Georg Rehm,Alan Akbik*

Main category: cs.CV

TL;DR: 文章介绍多语言基准测试PISA - Bench，评估视觉 - 语言模型，发现小模型表现不佳，非英语部分性能下降等问题，并发布数据集推动多语言多模态推理研究。


<details>
  <summary>Details</summary>
Motivation: 现有视觉 - 语言模型基准测试存在高质量、人工验证示例有限，多为合成内容且多为英语的问题，需填补此空白。

Method: 引入基于PISA测试英语示例的多语言基准测试PISA - Bench，将示例翻译成六种语言形成平行语料库，并评估现有视觉 - 语言模型。

Result: 小模型（参数少于20B）难以取得高分，非英语部分性能大幅下降，模型在空间和几何推理任务上错误率高。

Conclusion: 发布数据集和评估框架可为多语言多模态推理研究提供资源。

Abstract: Vision-language models (VLMs) have demonstrated remarkable progress in
multimodal reasoning. However, existing benchmarks remain limited in terms of
high-quality, human-verified examples. Many current datasets rely on
synthetically generated content by large language models (LLMs). Furthermore,
most datasets are limited to English, as manual quality assurance of translated
samples is time-consuming and costly. To fill this gap, we introduce
PISA-Bench, a multilingual benchmark derived from English examples of the
expert-created PISA tests, a unified framework for the assessment of student
competencies in over eighty countries. Each example consists of human-extracted
instructions, questions, answer options, and images, enriched with question
type categories, and has been translated from English into five additional
languages (Spanish, German, Chinese, French, and Italian), resulting in a fully
parallel corpus covering six languages. We evaluate state-of-the-art
vision-language models on PISA-Bench and find that especially small models
(<20B parameters) fail to achieve high test scores. We further find substantial
performance degradation on non-English splits as well as high error-rates when
models are tasked with spatial and geometric reasoning. By releasing the
dataset and evaluation framework, we provide a resource for advancing research
on multilingual multimodal reasoning.

</details>


### [204] [A Survey on Efficient Vision-Language-Action Models](https://arxiv.org/abs/2510.24795)
*Zhaoshu Yu,Bo Wang,Pengpeng Zeng,Haonan Zhang,Ji Zhang,Lianli Gao,Jingkuan Song,Nicu Sebe,Heng Tao Shen*

Main category: cs.CV

TL;DR: 本文对高效视觉 - 语言 - 动作模型（Efficient VLAs）进行全面综述，介绍统一分类法并梳理相关技术，总结应用、挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有视觉 - 语言 - 动作模型（VLAs）因基础大模型对计算和数据要求高，部署受限，需解决这些挑战。

Method: 提出统一分类法，将当前技术分为高效模型设计、高效训练和高效数据收集三个核心支柱，并对各框架内的先进方法进行批判性回顾。

Result: 为该领域建立了基础参考，总结了代表性应用，明确了关键挑战。

Conclusion: 为高效视觉 - 语言 - 动作模型领域绘制了未来研究路线图，并提供持续更新的项目页面。

Abstract: Vision-Language-Action models (VLAs) represent a significant frontier in
embodied intelligence, aiming to bridge digital knowledge with physical-world
interaction. While these models have demonstrated remarkable generalist
capabilities, their deployment is severely hampered by the substantial
computational and data requirements inherent to their underlying large-scale
foundation models. Motivated by the urgent need to address these challenges,
this survey presents the first comprehensive review of Efficient
Vision-Language-Action models (Efficient VLAs) across the entire
data-model-training process. Specifically, we introduce a unified taxonomy to
systematically organize the disparate efforts in this domain, categorizing
current techniques into three core pillars: (1) Efficient Model Design,
focusing on efficient architectures and model compression; (2) Efficient
Training, which reduces computational burdens during model learning; and (3)
Efficient Data Collection, which addresses the bottlenecks in acquiring and
utilizing robotic data. Through a critical review of state-of-the-art methods
within this framework, this survey not only establishes a foundational
reference for the community but also summarizes representative applications,
delineates key challenges, and charts a roadmap for future research. We
maintain a continuously updated project page to track our latest developments:
https://evla-survey.github.io/

</details>


### [205] [DualCap: Enhancing Lightweight Image Captioning via Dual Retrieval with Similar Scenes Visual Prompts](https://arxiv.org/abs/2510.24813)
*Binbin Li,Guimiao Yang,Zisen Qi,Haiping Wang,Yu Ding*

Main category: cs.CV

TL;DR: 提出DualCap模型，用双检索机制和视觉提示增强视觉表征，实验显示性能好且参数少。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级检索增强图像描述模型仅将检索数据用作文本提示，存在语义差距，原视觉特征未增强。

Method: 提出DualCap模型，采用双检索机制，从检索的相似图像生成视觉提示，通过轻量级可训练特征融合网络将文本特征和原图像特征融合。

Result: 方法在实验中取得有竞争力的性能，且与之前视觉提示字幕方法相比，可训练参数更少。

Conclusion: DualCap模型能有效解决现有模型问题，在性能和参数方面有优势。

Abstract: Recent lightweight retrieval-augmented image caption models often utilize
retrieved data solely as text prompts, thereby creating a semantic gap by
leaving the original visual features unenhanced, particularly for object
details or complex scenes. To address this limitation, we propose $DualCap$, a
novel approach that enriches the visual representation by generating a visual
prompt from retrieved similar images. Our model employs a dual retrieval
mechanism, using standard image-to-text retrieval for text prompts and a novel
image-to-image retrieval to source visually analogous scenes. Specifically,
salient keywords and phrases are derived from the captions of visually similar
scenes to capture key objects and similar details. These textual features are
then encoded and integrated with the original image features through a
lightweight, trainable feature fusion network. Extensive experiments
demonstrate that our method achieves competitive performance while requiring
fewer trainable parameters compared to previous visual-prompting captioning
approaches.

</details>


### [206] [Deep Feature Optimization for Enhanced Fish Freshness Assessment](https://arxiv.org/abs/2510.24814)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.CV

TL;DR: 本文提出三阶段框架用于鱼新鲜度评估，在FFE数据集上实验表明其有效且泛化性好。


<details>
  <summary>Details</summary>
Motivation: 传统感官评估鱼新鲜度主观、耗时且结果不一致，深度学习方法存在准确性和特征透明度问题，需可靠评估方法。

Method: 提出三阶段框架，先微调五种视觉架构建立基线，再用提取的多级深度特征训练七种机器学习分类器，最后用LGBM、随机森林和Lasso进行特征选择。

Result: 结合Swin - Tiny特征、Extra Trees分类器和基于LGBM的特征选择的配置准确率达85.99%，优于近期相关研究。

Conclusion: 所提框架对视觉质量评估任务有效且具有泛化性。

Abstract: Assessing fish freshness is vital for ensuring food safety and minimizing
economic losses in the seafood industry. However, traditional sensory
evaluation remains subjective, time-consuming, and inconsistent. Although
recent advances in deep learning have automated visual freshness prediction,
challenges related to accuracy and feature transparency persist. This study
introduces a unified three-stage framework that refines and leverages deep
visual representations for reliable fish freshness assessment. First, five
state-of-the-art vision architectures - ResNet-50, DenseNet-121,
EfficientNet-B0, ConvNeXt-Base, and Swin-Tiny - are fine-tuned to establish a
strong baseline. Next, multi-level deep features extracted from these backbones
are used to train seven classical machine learning classifiers, integrating
deep and traditional decision mechanisms. Finally, feature selection methods
based on Light Gradient Boosting Machine (LGBM), Random Forest, and Lasso
identify a compact and informative subset of features. Experiments on the
Freshness of the Fish Eyes (FFE) dataset demonstrate that the best
configuration combining Swin-Tiny features, an Extra Trees classifier, and
LGBM-based feature selection achieves an accuracy of 85.99%, outperforming
recent studies on the same dataset by 8.69-22.78%. These findings confirm the
effectiveness and generalizability of the proposed framework for visual quality
evaluation tasks.

</details>


### [207] [Perception, Understanding and Reasoning, A Multimodal Benchmark for Video Fake News Detection](https://arxiv.org/abs/2510.24816)
*Cui Yakun,Fushuo Huo,Weijie Shi,Juntao Dai,Hang Du,Zhenghao Zhu,Sirui Han,Yike Guo*

Main category: cs.CV

TL;DR: 提出MVFNDB基准和MVFND - CoT框架用于视频假新闻检测，分析影响准确率的因素。


<details>
  <summary>Details</summary>
Motivation: 传统视频假新闻检测基准无法提供细粒度评估，使检测过程成黑盒，需新基准。

Method: 基于实证分析引入MVFNDB基准，设计MVFND - CoT框架，结合多特征推理。

Result: 构建含10个任务、9730个人工标注问题的基准，深入分析影响准确率的因素。

Conclusion: 该基准将为多模态大语言模型在视频假新闻检测领域的评估和发展奠定基础。

Abstract: The advent of multi-modal large language models (MLLMs) has greatly advanced
research into applications for Video fake news detection (VFND) tasks.
Traditional video-based FND benchmarks typically focus on the accuracy of the
final decision, often failing to provide fine-grained assessments for the
entire detection process, making the detection process a black box. Therefore,
we introduce the MVFNDB (Multi-modal Video Fake News Detection Benchmark) based
on the empirical analysis, which provides foundation for tasks definition. The
benchmark comprises 10 tasks and is meticulously crafted to probe MLLMs'
perception, understanding, and reasoning capacities during detection, featuring
9730 human-annotated video-related questions based on a carefully constructed
taxonomy ability of VFND. To validate the impact of combining multiple features
on the final results, we design a novel framework named MVFND-CoT, which
incorporates both creator-added content and original shooting footage
reasoning. Building upon the benchmark, we conduct an in-depth analysis of the
deeper factors influencing accuracy, including video processing strategies and
the alignment between video features and model capabilities. We believe this
benchmark will lay a solid foundation for future evaluations and advancements
of MLLMs in the domain of video fake news detection.

</details>


### [208] [SafeEditor: Unified MLLM for Efficient Post-hoc T2I Safety Editing](https://arxiv.org/abs/2510.24820)
*Ruiyang Zhang,Jiahao Luo,Xiaoru Feng,Qiufan Pang,Yaodong Yang,Juntao Dai*

Main category: cs.CV

TL;DR: 为解决现有文本到图像（T2I）模型推理时安全方法的局限性，提出多轮安全编辑框架，实验显示其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有推理时文本到图像模型安全方法存在过度拒绝、安全与效用失衡等局限，需改进。

Method: 提出多轮安全编辑框架，构建多轮图像 - 文本交错数据集MR - SafeEdit，开发统一的多模态大语言模型SafeEditor进行多轮安全编辑。

Result: SafeEditor减少了过度拒绝情况，实现了更优的安全 - 效用平衡，优于先前安全方法。

Conclusion: 所提多轮安全编辑框架及SafeEditor能有效解决现有T2I模型安全方法的问题，实现高效安全对齐。

Abstract: With the rapid advancement of text-to-image (T2I) models, ensuring their
safety has become increasingly critical. Existing safety approaches can be
categorized into training-time and inference-time methods. While inference-time
methods are widely adopted due to their cost-effectiveness, they often suffer
from limitations such as over-refusal and imbalance between safety and utility.
To address these challenges, we propose a multi-round safety editing framework
that functions as a model-agnostic, plug-and-play module, enabling efficient
safety alignment for any text-to-image model. Central to this framework is
MR-SafeEdit, a multi-round image-text interleaved dataset specifically
constructed for safety editing in text-to-image generation. We introduce a
post-hoc safety editing paradigm that mirrors the human cognitive process of
identifying and refining unsafe content. To instantiate this paradigm, we
develop SafeEditor, a unified MLLM capable of multi-round safety editing on
generated images. Experimental results show that SafeEditor surpasses prior
safety approaches by reducing over-refusal while achieving a more favorable
safety-utility balance.

</details>


### [209] [Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation](https://arxiv.org/abs/2510.24821)
*Inclusion AI,:,Bowen Ma,Cheng Zou,Canxiang Yan,Chunxiang Jin,Chunjie Shen,Dandan Zheng,Fudong Wang,Furong Xu,GuangMing Yao,Jun Zhou,Jingdong Chen,Jianing Li,Jianxin Sun,Jiajia Liu,Jianjiang Zhu,Jianping Jiang,Jun Peng,Kaixiang Ji,Kaimeng Ren,Libin Wang,Lixiang Ru,Longhua Tan,Lan Wang,Mochen Bai,Ning Gao,Qingpei Guo,Qinglong Zhang,Qiang Xu,Rui Liu,Ruijie Xiong,Ruobing Zheng,Sirui Gao,Tianqi Li,Tinghao Liu,Weilong Chai,Xinyu Xiao,Xiaomei Wang,Xiaolong Wang,Xiao Lu,Xiaoyu Li,Xingning Dong,Xuzheng Yu,Yi Yuan,Yuting Gao,Yuting Xiao,Yunxiao Sun,Yipeng Chen,Yifan Mao,Yifei Wu,Yongjie Lyu,Ziping Ma,Zhiqiang Fang,Zhihao Qiu,Ziyuan Huang,Zizheng Yang,Zhengyu He*

Main category: cs.CV

TL;DR: 提出升级版Ming - Flash - Omni，基于稀疏MoE变体，提升多模态能力，在多任务达SOTA。


<details>
  <summary>Details</summary>
Motivation: 追求高效扩展和更强的统一多模态智能，迈向AGI。

Method: 构建基于Ling - Flash - 2.0稀疏MoE变体的Ming - Flash - Omni架构。

Result: 在多模态理解和生成上显著提升，语音识别、图像生成、生成式分割等任务表现出色。

Conclusion: Ming - Flash - Omni在单一统一架构下于多任务达SOTA，推动多模态智能发展。

Abstract: We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a
sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion
total parameters, of which only 6.1 billion are active per token. This
architecture enables highly efficient scaling (dramatically improving
computational efficiency while significantly expanding model capacity) and
empowers stronger unified multimodal intelligence across vision, speech, and
language, representing a key step toward Artificial General Intelligence (AGI).
Compared to its predecessor, the upgraded version exhibits substantial
improvements across multimodal understanding and generation. We significantly
advance speech recognition capabilities, achieving state-of-the-art performance
in contextual ASR and highly competitive results in dialect-aware ASR. In image
generation, Ming-Flash-Omni introduces high-fidelity text rendering and
demonstrates marked gains in scene consistency and identity preservation during
image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,
a capability that not only achieves strong standalone segmentation performance
but also enhances spatial control in image generation and improves editing
consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in
text-to-image generation and generative segmentation, and sets new records on
all 12 contextual ASR benchmarks, all within a single unified architecture.

</details>


### [210] [The Generation Phases of Flow Matching: a Denoising Perspective](https://arxiv.org/abs/2510.24830)
*Anne Gagneux,Ségolène Martin,Rémi Gribonval,Mathurin Massias*

Main category: cs.CV

TL;DR: 从去噪角度设计框架探究流匹配生成过程，分析生成和去噪性能，引入扰动获新见解。


<details>
  <summary>Details</summary>
Motivation: 当前对流匹配生成过程质量的影响因素理解不足。

Method: 采用去噪视角设计框架，建立流匹配模型和去噪器的联系，设计扰动。

Result: 获得了生成过程不同动态阶段的新见解，能明确去噪器在生成过程各阶段的成败情况及原因。

Conclusion: 通过去噪视角的研究，有助于深入理解流匹配的生成过程。

Abstract: Flow matching has achieved remarkable success, yet the factors influencing
the quality of its generation process remain poorly understood. In this work,
we adopt a denoising perspective and design a framework to empirically probe
the generation process. Laying down the formal connections between flow
matching models and denoisers, we provide a common ground to compare their
performances on generation and denoising. This enables the design of principled
and controlled perturbations to influence sample generation: noise and drift.
This leads to new insights on the distinct dynamical phases of the generative
process, enabling us to precisely characterize at which stage of the generative
process denoisers succeed or fail and why this matters.

</details>


### [211] [Understanding Multi-View Transformers](https://arxiv.org/abs/2510.24907)
*Michal Stary,Julien Gaubil,Ayush Tewari,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 提出方法探测和可视化多视图变压器层残差连接的3D表示，研究DUSt3R模型变体。


<details>
  <summary>Details</summary>
Motivation: 多视图变压器内部机制不明，黑盒特性使改进和在关键应用中使用困难。

Method: 提出探测和可视化多视图变压器层残差连接3D表示的方法。

Result: 研究DUSt3R模型变体，发现其估计的对应关系会用重建几何进行细化。

Conclusion: 通过新方法有助于理解多视图变压器，代码开源。

Abstract: Multi-view transformers such as DUSt3R are revolutionizing 3D vision by
solving 3D tasks in a feed-forward manner. However, contrary to previous
optimization-based pipelines, the inner mechanisms of multi-view transformers
are unclear. Their black-box nature makes further improvements beyond data
scaling challenging and complicates usage in safety- and reliability-critical
applications. Here, we present an approach for probing and visualizing 3D
representations from the residual connections of the multi-view transformers'
layers. In this manner, we investigate a variant of the DUSt3R model, shedding
light on the development of its latent state across blocks, the role of the
individual layers, and suggest how it differs from methods with stronger
inductive biases of explicit global pose. Finally, we show that the
investigated variant of DUSt3R estimates correspondences that are refined with
reconstructed geometry. The code used for the analysis is available at
https://github.com/JulienGaubil/und3rstand .

</details>


### [212] [FT-ARM: Fine-Tuned Agentic Reflection Multimodal Language Model for Pressure Ulcer Severity Classification with Reasoning](https://arxiv.org/abs/2510.24980)
*Reza Saadati Fard,Emmanuel Agu,Palawat Busaranuvong,Deepak Kumar,Shefalika Gautam,Bengisu Tulu,Diane Strong,Lorraine Loretz*

Main category: cs.CV

TL;DR: 本文提出FT - ARM模型用于压疮严重程度分类，在PIID数据集上准确率达85%，超越CNN模型，支持实时推理且有良好可解释性。


<details>
  <summary>Details</summary>
Motivation: 压疮严重程度准确分类有挑战，此前AI方法可解释性有限，需更可靠、可解释的自动伤口评估系统。

Method: 提出FT - ARM，基于微调的多模态大语言模型，有主动自我反思机制，迭代优化预测。

Result: 在PIID数据集上，FT - ARM分类准确率85%，超CNN模型4%，支持实时推理，能产生自然语言解释。

Conclusion: FT - ARM提升自动伤口评估系统可靠性、透明度和临床适用性，满足压疮分期一致性和可解释性需求。

Abstract: Pressure ulcers (PUs) are a serious and prevalent healthcare concern.
Accurate classification of PU severity (Stages I-IV) is essential for proper
treatment but remains challenging due to subtle visual distinctions and
subjective interpretation, leading to variability among clinicians. Prior
AI-based approaches using Convolutional Neural Networks (CNNs) and Vision
Transformers (ViTs) achieved promising accuracy but offered limited
interpretability. We present FT-ARM (Fine-Tuned Agentic Reflection Multimodal
model), a fine-tuned multimodal large language model (MLLM) with an agentic
self-reflection mechanism for pressure ulcer severity classification. Inspired
by clinician-style diagnostic reassessment, FT-ARM iteratively refines its
predictions by reasoning over visual features and encoded clinical knowledge
from text, enhancing both accuracy and consistency. On the publicly available
Pressure Injury Image Dataset (PIID), FT-ARM, fine-tuned from LLaMA 3.2 90B,
achieved 85% accuracy in classifying PU stages I-IV, surpassing prior CNN-based
models by +4%. Unlike earlier CNN/ViT studies that relied solely on offline
evaluations, FT-ARM is designed and tested for live inference, reflecting
real-time deployment conditions. Furthermore, it produces clinically grounded
natural-language explanations, improving interpretability and trust. By
integrating fine-tuning and reflective reasoning across multimodal inputs,
FT-ARM advances the reliability, transparency, and clinical applicability of
automated wound assessment systems, addressing the critical need for consistent
and explainable PU staging to support improved patient care.

</details>


### [213] [DrivingScene: A Multi-Task Online Feed-Forward 3D Gaussian Splatting Method for Dynamic Driving Scenes](https://arxiv.org/abs/2510.24734)
*Qirui Hou,Wenzhang Sun,Chang Zeng,Chunfeng Wang,Hao Li,Jianxun Cui*

Main category: cs.CV

TL;DR: 提出DrivingScene框架，仅用两张连续环视图像实时高保真重建4D动态驾驶场景，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂动态和稀疏视角下难以平衡动态驾驶场景重建的质量和效率。

Method: 提出DrivingScene在线前馈框架，采用轻量级残差流网络预测动态物体非刚性运动，引入粗到精训练范式。

Result: 在nuScenes数据集上，仅基于图像的方法能在线生成高质量深度、场景流和3D高斯点云。

Conclusion: 该方法在动态重建和新视角合成方面显著优于现有方法。

Abstract: Real-time, high-fidelity reconstruction of dynamic driving scenes is
challenged by complex dynamics and sparse views, with prior methods struggling
to balance quality and efficiency. We propose DrivingScene, an online,
feed-forward framework that reconstructs 4D dynamic scenes from only two
consecutive surround-view images. Our key innovation is a lightweight residual
flow network that predicts the non-rigid motion of dynamic objects per camera
on top of a learned static scene prior, explicitly modeling dynamics via scene
flow. We also introduce a coarse-to-fine training paradigm that circumvents the
instabilities common to end-to-end approaches. Experiments on nuScenes dataset
show our image-only method simultaneously generates high-quality depth, scene
flow, and 3D Gaussian point clouds online, significantly outperforming
state-of-the-art methods in both dynamic reconstruction and novel view
synthesis.

</details>


### [214] [Efficient License Plate Recognition via Pseudo-Labeled Supervision with Grounding DINO and YOLOv8](https://arxiv.org/abs/2510.25032)
*Zahra Ebrahimi Vargoorani,Amir Mohammad Ghoreyshi,Ching Yee Suen*

Main category: cs.CV

TL;DR: 本文提出用YOLOv8和半监督学习框架进行车牌检测与识别，在不同数据集上取得良好效果并报告字符错误率。


<details>
  <summary>Details</summary>
Motivation: 环境因素、车辆速度、相机角度和图像质量等给高精度自动车牌识别系统（ALPR）开发带来挑战，而ALPR在多个领域至关重要。

Method: 使用YOLOv8进行车牌检测与识别，利用多个地区数据集；采用半监督学习框架，结合少量手动标注数据和Grounding DINO生成的伪标签训练模型。

Result: 在CENPARMI数据集上召回率达94%，在UFPR - ALPR数据集上达91%，并报告了两个数据集的字符错误率。

Conclusion: 通过整合人工验证和模型生成的标注，能有效扩展数据集并保持标签质量，显著提升训练过程和整体模型性能。

Abstract: Developing a highly accurate automatic license plate recognition system
(ALPR) is challenging due to environmental factors such as lighting, rain, and
dust. Additional difficulties include high vehicle speeds, varying camera
angles, and low-quality or low-resolution images. ALPR is vital in traffic
control, parking, vehicle tracking, toll collection, and law enforcement
applications. This paper proposes a deep learning strategy using YOLOv8 for
license plate detection and recognition tasks. This method seeks to enhance the
performance of the model using datasets from Ontario, Quebec, California, and
New York State. It achieved an impressive recall rate of 94% on the dataset
from the Center for Pattern Recognition and Machine Intelligence (CENPARMI) and
91% on the UFPR-ALPR dataset. In addition, our method follows a semi-supervised
learning framework, combining a small set of manually labeled data with
pseudo-labels generated by Grounding DINO to train our detection model.
Grounding DINO, a powerful vision-language model, automatically annotates many
images with bounding boxes for license plates, thereby minimizing the reliance
on labor-intensive manual labeling. By integrating human-verified and
model-generated annotations, we can scale our dataset efficiently while
maintaining label quality, which significantly enhances the training process
and overall model performance. Furthermore, it reports character error rates
for both datasets, providing additional insight into system performance.

</details>


### [215] [Point-level Uncertainty Evaluation of Mobile Laser Scanning Point Clouds](https://arxiv.org/abs/2510.24773)
*Ziyang Xu,Olaf Wysocki,Christoph Holst*

Main category: cs.CV

TL;DR: 提出基于机器学习的点云不确定性评估框架，用RF和XGBoost模型，实验效果好，为大规模点云质量控制和误差分析提供基础。


<details>
  <summary>Details</summary>
Motivation: 传统逆向不确定性建模依赖高精度参考数据，大规模获取成本高或不可行，需新方法评估移动激光扫描点云不确定性。

Method: 提出基于机器学习的点级不确定性评估框架，用RF和XGBoost模型，在空间分区的真实数据集上训练和验证。

Result: 两模型有效捕捉几何特征与不确定性的非线性关系，平均ROC - AUC值超0.87，部分几何特征在预测中起主导作用。

Conclusion: 框架提供数据驱动的不确定性评估视角，为大规模点云质量控制和误差分析提供可扩展、适应性强的基础。

Abstract: Reliable quantification of uncertainty in Mobile Laser Scanning (MLS) point
clouds is essential for ensuring the accuracy and credibility of downstream
applications such as 3D mapping, modeling, and change analysis. Traditional
backward uncertainty modeling heavily rely on high-precision reference data,
which are often costly or infeasible to obtain at large scales. To address this
issue, this study proposes a machine learning-based framework for point-level
uncertainty evaluation that learns the relationship between local geometric
features and point-level errors. The framework is implemented using two
ensemble learning models, Random Forest (RF) and XGBoost, which are trained and
validated on a spatially partitioned real-world dataset to avoid data leakage.
Experimental results demonstrate that both models can effectively capture the
nonlinear relationships between geometric characteristics and uncertainty,
achieving mean ROC-AUC values above 0.87. The analysis further reveals that
geometric features describing elevation variation, point density, and local
structural complexity play a dominant role in predicting uncertainty. The
proposed framework offers a data-driven perspective of uncertainty evaluation,
providing a scalable and adaptable foundation for future quality control and
error analysis of large-scale point clouds.

</details>


### [216] [A Re-node Self-training Approach for Deep Graph-based Semi-supervised Classification on Multi-view Image Data](https://arxiv.org/abs/2510.24791)
*Jingjun Bi,Fadi Dornaika*

Main category: cs.CV

TL;DR: 本文提出用于多视图数据的重节点自学图半监督学习方法（RSGSLM），结合线性特征变换和多视图图融合等，实验显示其优于现有多视图半监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统和现有技术在处理图像无明确图结构、多视图数据复杂的问题上效率受限，且多视图数据中图结构集成存在挑战。

Method: 在图卷积网络（GCN）框架下结合线性特征变换和多视图图融合；动态将伪标签融入GCN损失函数；调整类边界附近标记样本权重纠正拓扑不平衡；引入适用于所有样本的无监督平滑损失。

Result: 在多视图基准图像数据集上的实验表明，RSGSLM在多视图场景中超越了现有的半监督学习方法。

Conclusion: RSGSLM方法能有效应对多视图数据半监督学习挑战，优化性能并保持计算效率。

Abstract: Recently, graph-based semi-supervised learning and pseudo-labeling have
gained attention due to their effectiveness in reducing the need for extensive
data annotations. Pseudo-labeling uses predictions from unlabeled data to
improve model training, while graph-based methods are characterized by
processing data represented as graphs. However, the lack of clear graph
structures in images combined with the complexity of multi-view data limits the
efficiency of traditional and existing techniques. Moreover, the integration of
graph structures in multi-view data is still a challenge. In this paper, we
propose Re-node Self-taught Graph-based Semi-supervised Learning for Multi-view
Data (RSGSLM). Our method addresses these challenges by (i) combining linear
feature transformation and multi-view graph fusion within a Graph Convolutional
Network (GCN) framework, (ii) dynamically incorporating pseudo-labels into the
GCN loss function to improve classification in multi-view data, and (iii)
correcting topological imbalances by adjusting the weights of labeled samples
near class boundaries. Additionally, (iv) we introduce an unsupervised
smoothing loss applicable to all samples. This combination optimizes
performance while maintaining computational efficiency. Experimental results on
multi-view benchmark image datasets demonstrate that RSGSLM surpasses existing
semi-supervised learning approaches in multi-view contexts.

</details>


### [217] [Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D Talking Face Animation](https://arxiv.org/abs/2510.25234)
*Yuxiang Mao,Zhijie Zhang,Zhiheng Zhang,Jiawei Liu,Chen Zeng,Shihong Xia*

Main category: cs.CV

TL;DR: 本文针对生成情感丰富的3D会说话人脸动画问题，提出将语音和情感驱动的面部动画建模为线性加性问题，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 随着AIGC发展，生成情感丰富的3D会说话人脸动画很关键，但因数据获取成本高，该领域研究不足。

Method: 将语音和情感驱动的面部动画建模为线性加性问题，利用两个数据集联合学习一组混合形状，引入稀疏约束损失，将学习的混合形状映射到FLAME模型参数以实现3D高斯头像动画。

Result: 定性和定量实验表明方法能自然生成指定表情的会说话人脸，保持准确唇同步；感知研究显示情感表现力优于现有方法，不影响唇同步质量。

Conclusion: 提出的方法能有效生成情感丰富且唇同步准确的3D会说话人脸动画。

Abstract: Expressions are fundamental to conveying human emotions. With the rapid
advancement of AI-generated content (AIGC), realistic and expressive 3D facial
animation has become increasingly crucial. Despite recent progress in
speech-driven lip-sync for talking-face animation, generating emotionally
expressive talking faces remains underexplored. A major obstacle is the
scarcity of real emotional 3D talking-face datasets due to the high cost of
data capture. To address this, we model facial animation driven by both speech
and emotion as a linear additive problem. Leveraging a 3D talking-face dataset
with neutral expressions (VOCAset) and a dataset of 3D expression sequences
(Florence4D), we jointly learn a set of blendshapes driven by speech and
emotion. We introduce a sparsity constraint loss to encourage disentanglement
between the two types of blendshapes while allowing the model to capture
inherent secondary cross-domain deformations present in the training data. The
learned blendshapes can be further mapped to the expression and jaw pose
parameters of the FLAME model, enabling the animation of 3D Gaussian avatars.
Qualitative and quantitative experiments demonstrate that our method naturally
generates talking faces with specified expressions while maintaining accurate
lip synchronization. Perceptual studies further show that our approach achieves
superior emotional expressivity compared to existing methods, without
compromising lip-sync quality.

</details>


### [218] [Modality-Aware SAM: Sharpness-Aware-Minimization Driven Gradient Modulation for Harmonized Multimodal Learning](https://arxiv.org/abs/2510.24919)
*Hossein R. Nowdeh,Jie Ji,Xiaolong Ma,Fatemeh Afghah*

Main category: cs.CV

TL;DR: 提出M - SAM框架优化多模态学习，经实验验证其优于现有方法并改善多模态学习。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中主导模态会掩盖其他模态，限制泛化能力。

Method: M - SAM框架在每次迭代分三步优化学习：用Shapley值确定主导模态；分解损失景观；通过调制梯度的反向传播更新权重。

Result: 在四个不同数据集上的大量实验表明，M - SAM优于最新的优化和梯度操作方法。

Conclusion: M - SAM能显著平衡和改善多模态学习。

Abstract: In multimodal learning, dominant modalities often overshadow others, limiting
generalization. We propose Modality-Aware Sharpness-Aware Minimization (M-SAM),
a model-agnostic framework that applies to many modalities and supports early
and late fusion scenarios. In every iteration, M-SAM in three steps optimizes
learning. \textbf{First, it identifies the dominant modality} based on
modalities' contribution in the accuracy using Shapley. \textbf{Second, it
decomposes the loss landscape}, or in another language, it modulates the loss
to prioritize the robustness of the model in favor of the dominant modality,
and \textbf{third, M-SAM updates the weights} by backpropagation of modulated
gradients. This ensures robust learning for the dominant modality while
enhancing contributions from others, allowing the model to explore and exploit
complementary features that strengthen overall performance. Extensive
experiments on four diverse datasets show that M-SAM outperforms the latest
state-of-the-art optimization and gradient manipulation methods and
significantly balances and improves multimodal learning.

</details>


### [219] [MMEdge: Accelerating On-device Multimodal Inference via Pipelined Sensing and Encoding](https://arxiv.org/abs/2510.25327)
*Runxi Huang,Mingxuan Yu,Mingyu Tsoi,Xiaomin Ouyang*

Main category: cs.CV

TL;DR: 提出MMEdge框架用于资源受限边缘设备的实时多模态推理，分解推理过程，有聚合模块和优化机制，实验证明能降低延迟并保持精度。


<details>
  <summary>Details</summary>
Motivation: 先前工作忽视传感动态与模型执行的紧密耦合及复杂的模态间依赖，需为资源受限边缘设备提供实时多模态推理方案。

Method: 提出MMEdge框架，将推理过程分解为细粒度单元，引入时间聚合模块，有自适应多模态配置优化器和跨模态推测跳过机制。

Result: 使用两个公共多模态数据集评估并在无人机测试平台部署，MMEdge显著降低端到端延迟，同时在不同系统和数据动态下保持高任务精度。

Conclusion: MMEdge能有效处理资源受限边缘设备的实时多模态推理问题，平衡延迟和精度。

Abstract: Real-time multimodal inference on resource-constrained edge devices is
essential for applications such as autonomous driving, human-computer
interaction, and mobile health. However, prior work often overlooks the tight
coupling between sensing dynamics and model execution, as well as the complex
inter-modality dependencies. In this paper, we propose MMEdge, an new on-device
multi-modal inference framework based on pipelined sensing and encoding.
Instead of waiting for complete sensor inputs, MMEdge decomposes the entire
inference process into a sequence of fine-grained sensing and encoding units,
allowing computation to proceed incrementally as data arrive. MMEdge also
introduces a lightweight but effective temporal aggregation module that
captures rich temporal dynamics across different pipelined units to maintain
accuracy performance. Such pipelined design also opens up opportunities for
fine-grained cross-modal optimization and early decision-making during
inference. To further enhance system performance under resource variability and
input data complexity, MMEdge incorporates an adaptive multimodal configuration
optimizer that dynamically selects optimal sensing and model configurations for
each modality under latency constraints, and a cross-modal speculative skipping
mechanism that bypasses future units of slower modalities when early
predictions reach sufficient confidence. We evaluate MMEdge using two public
multimodal datasets and deploy it on a real-world unmanned aerial vehicle
(UAV)-based multimodal testbed. The results show that MMEdge significantly
reduces end-to-end latency while maintaining high task accuracy across various
system and data dynamics.

</details>


### [220] [Breast Cancer VLMs: Clinically Practical Vision-Language Train-Inference Models](https://arxiv.org/abs/2510.25051)
*Shunjie-Fabian Zheng,Hyeonjun Lee,Thijs Kooi,Ali Diba*

Main category: cs.CV

TL;DR: 本文提出结合2D乳腺钼靶视觉特征与临床元数据文本描述符的新框架，在癌症检测和钙化识别上表现优于单模态基线，为基于VLM的CAD系统建立新范式。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是发达国家女性常见恶性肿瘤，早期筛查可降低死亡率，但现有计算机辅助诊断（CAD）系统在临床部署有局限，如处理多模态数据和依赖临床病史。

Method: 引入新框架，通过创新分词模块结合2D乳腺钼靶视觉特征与临床元数据和放射报告的文本描述符，将卷积神经网络与语言表征集成。

Result: 在多国队列筛查乳腺钼靶上评估，多模态方法在癌症检测和钙化识别上优于单模态基线。

Conclusion: 该方法为开发临床可行的基于VLM的CAD系统建立了新范式，可有效融合影像数据和患者背景信息。

Abstract: Breast cancer remains the most commonly diagnosed malignancy among women in
the developed world. Early detection through mammography screening plays a
pivotal role in reducing mortality rates. While computer-aided diagnosis (CAD)
systems have shown promise in assisting radiologists, existing approaches face
critical limitations in clinical deployment - particularly in handling the
nuanced interpretation of multi-modal data and feasibility due to the
requirement of prior clinical history. This study introduces a novel framework
that synergistically combines visual features from 2D mammograms with
structured textual descriptors derived from easily accessible clinical metadata
and synthesized radiological reports through innovative tokenization modules.
Our proposed methods in this study demonstrate that strategic integration of
convolutional neural networks (ConvNets) with language representations achieves
superior performance to vision transformer-based models while handling
high-resolution images and enabling practical deployment across diverse
populations. By evaluating it on multi-national cohort screening mammograms,
our multi-modal approach achieves superior performance in cancer detection and
calcification identification compared to unimodal baselines, with particular
improvements. The proposed method establishes a new paradigm for developing
clinically viable VLM-based CAD systems that effectively leverage imaging data
and contextual patient information through effective fusion mechanisms.

</details>


### [221] [3D CT-Based Coronary Calcium Assessment: A Feature-Driven Machine Learning Framework](https://arxiv.org/abs/2510.25347)
*Ayman Abaid,Gianpiero Guidone,Sara Alsubai,Foziyah Alquahtani,Talha Iqbal,Ruth Sharif,Hesham Elzomor,Emiliano Bianchini,Naeif Almagal,Michael G. Madden,Faisal Sharif,Ihsan Ullah*

Main category: cs.CV

TL;DR: 研究利用非对比冠状动脉CT血管造影扫描，提出基于影像组学的流程，比较不同特征性能，结果显示影像组学模型表现更佳。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉钙化评分对冠心病早期检测和风险分层很重要，解决临床非对比扫描中注释数据有限的问题。

Method: 提出基于影像组学的流程，利用伪标签生成训练标签，使用预训练基础模型提取特征，结合传统分类器，比较深度学习特征和影像组学特征性能，在不同数据集上训练和测试。

Result: 在包含182名患者的临床数据集上，影像组学模型显著优于基础模型的CNN嵌入特征，准确率达84%，p<0.05。

Conclusion: 即使没有专家注释，基于影像组学的模型在冠状动脉钙化评分分类中表现出色。

Abstract: Coronary artery calcium (CAC) scoring plays a crucial role in the early
detection and risk stratification of coronary artery disease (CAD). In this
study, we focus on non-contrast coronary computed tomography angiography (CCTA)
scans, which are commonly used for early calcification detection in clinical
settings. To address the challenge of limited annotated data, we propose a
radiomics-based pipeline that leverages pseudo-labeling to generate training
labels, thereby eliminating the need for expert-defined segmentations.
Additionally, we explore the use of pretrained foundation models, specifically
CT-FM and RadImageNet, to extract image features, which are then used with
traditional classifiers. We compare the performance of these deep learning
features with that of radiomics features. Evaluation is conducted on a clinical
CCTA dataset comprising 182 patients, where individuals are classified into two
groups: zero versus non-zero calcium scores. We further investigate the impact
of training on non-contrast datasets versus combined contrast and non-contrast
datasets, with testing performed only on non contrast scans. Results show that
radiomics-based models significantly outperform CNN-derived embeddings from
foundation models (achieving 84% accuracy and p<0.05), despite the
unavailability of expert annotations.

</details>


### [222] [Prompt Estimation from Prototypes for Federated Prompt Tuning of Vision Transformers](https://arxiv.org/abs/2510.25372)
*M Yashwanth,Sharannya Ghosh,Aditay Tripathi,Anirban Chakraborty*

Main category: cs.CV

TL;DR: 本文提出PEP - FedPT框架用于ViTs的联邦提示调优，引入CCMP，在多数据集上超现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有视觉提示调优在联邦学习中，全局提示调优难泛化，个性化调优过拟合，需新方法兼顾泛化与个性化。

Method: 提出PEP - FedPT框架，引入基于全局共享提示和特定类提示的CCMP，根据全局类原型和客户端类先验自适应组合特定类提示，用传统联邦平均技术协作优化提示。

Result: 在CIFAR - 100、TinyImageNet、DomainNet和iNaturalist数据集上，PEP - FedPT在不同数据异质性场景下始终超越现有基线。

Conclusion: PEP - FedPT为ViTs的高效且可泛化的联邦提示调优奠定了坚实基础。

Abstract: Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has
proven highly effective as a parameter-efficient fine-tuning technique for
adapting large models to downstream tasks with limited data. Its parameter
efficiency makes it particularly suitable for Federated Learning (FL), where
both communication and computation budgets are often constrained. However,
global prompt tuning struggles to generalize across heterogeneous clients,
while personalized tuning overfits to local data and lacks generalization. We
propose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt
Tuning), a unified framework designed to achieve both generalization and
personalization in federated prompt tuning of ViTs. Within this framework, we
introduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on
class-specific prompts maintained alongside a globally shared prompt. For each
input, CCMP adaptively combines class-specific prompts using weights derived
from global class prototypes and client class priors. This approach enables
per-sample prompt personalization without storing client-dependent trainable
parameters. The prompts are collaboratively optimized via traditional federated
averaging technique on the same. Comprehensive evaluations on CIFAR-100,
TinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT
consistently surpasses the state-of-the-art baselines under diverse data
heterogeneity scenarios, establishing a strong foundation for efficient and
generalizable federated prompt tuning of Vision Transformers.

</details>


### [223] [Comparative Study of UNet-based Architectures for Liver Tumor Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography](https://arxiv.org/abs/2510.25522)
*Doan-Van-Anh Ly,Thi-Thu-Hien Pham,Thanh-Hai Le*

Main category: cs.CV

TL;DR: 研究基于UNet架构进行肝脏肿瘤分割，发现ResNet基模型表现佳，引入CBAM模块进一步提升性能，Grad - CAM增强可解释性，经典ResNet结合现代注意力模块在医学图像分割有竞争力。


<details>
  <summary>Details</summary>
Motivation: 肝脏结构分割在肝脏疾病计算机辅助诊断和治疗规划中至关重要，研究基于UNet架构进行肝脏肿瘤分割的性能。

Method: 研究从原始UNet扩展到带不同骨干网络的UNet3+，评估ResNet、Transformer和Mamba骨干网络，引入注意力机制（CBAM），用Grad - CAM可视化增强可解释性。

Result: ResNet基模型在多评估指标上优于Transformer和Mamba基模型；ResNetUNet3+加CBAM模块有最佳重叠指标、最精确边界描绘、领先的整体准确率和特异性。

Conclusion: 经典ResNet架构结合现代注意力模块在医学图像分割任务中仍有很强竞争力，为临床肝脏肿瘤检测提供有前景方向。

Abstract: Segmentation of liver structures in multi-phase contrast-enhanced computed
tomography (CECT) plays a crucial role in computer-aided diagnosis and
treatment planning for liver diseases, including tumor detection. In this
study, we investigate the performance of UNet-based architectures for liver
tumor segmentation, starting from the original UNet and extending to UNet3+
with various backbone networks. We evaluate ResNet, Transformer-based, and
State-space (Mamba) backbones, all initialized with pretrained weights.
Surprisingly, despite the advances in modern architecture, ResNet-based models
consistently outperform Transformer- and Mamba-based alternatives across
multiple evaluation metrics. To further improve segmentation quality, we
introduce attention mechanisms into the backbone and observe that incorporating
the Convolutional Block Attention Module (CBAM) yields the best performance.
ResNetUNet3+ with CBAM module not only produced the best overlap metrics with a
Dice score of 0.755 and IoU of 0.662, but also achieved the most precise
boundary delineation, evidenced by the lowest HD95 distance of 77.911. The
model's superiority was further cemented by its leading overall accuracy of
0.925 and specificity of 0.926, showcasing its robust capability in accurately
identifying both lesion and healthy tissue. To further enhance
interpretability, Grad-CAM visualizations were employed to highlight the
region's most influential predictions, providing insights into its
decision-making process. These findings demonstrate that classical ResNet
architecture, when combined with modern attention modules, remain highly
competitive for medical image segmentation tasks, offering a promising
direction for liver tumor detection in clinical practice.

</details>


### [224] [RegionE: Adaptive Region-Aware Generation for Efficient Image Editing](https://arxiv.org/abs/2510.25590)
*Pengtao Chen,Xianfang Zeng,Maosen Zhao,Mingzhu Shen,Peng Ye,Bangyin Xiang,Zhibo Wang,Wei Cheng,Gang Yu,Tao Chen*

Main category: cs.CV

TL;DR: 提出RegionE框架加速指令式图像编辑任务，应用于多个模型取得加速效果且保真度良好。


<details>
  <summary>Details</summary>
Motivation: 现有IIE模型未区分图像编辑和未编辑区域，统一生成过程存在计算冗余。

Method: RegionE框架包含自适应区域划分、区域感知生成和自适应速度衰减缓存三个组件。

Result: 应用于多个IIE基础模型，分别取得2.57、2.41和2.06的加速因子，语义和感知保真度良好。

Conclusion: RegionE能在不额外训练的情况下加速IIE任务，且有效保持图像保真度。

Abstract: Recently, instruction-based image editing (IIE) has received widespread
attention. In practice, IIE often modifies only specific regions of an image,
while the remaining areas largely remain unchanged. Although these two types of
regions differ significantly in generation difficulty and computational
redundancy, existing IIE models do not account for this distinction, instead
applying a uniform generation process across the entire image. This motivates
us to propose RegionE, an adaptive, region-aware generation framework that
accelerates IIE tasks without additional training. Specifically, the RegionE
framework consists of three main components: 1) Adaptive Region Partition. We
observed that the trajectory of unedited regions is straight, allowing for
multi-step denoised predictions to be inferred in a single step. Therefore, in
the early denoising stages, we partition the image into edited and unedited
regions based on the difference between the final estimated result and the
reference image. 2) Region-Aware Generation. After distinguishing the regions,
we replace multi-step denoising with one-step prediction for unedited areas.
For edited regions, the trajectory is curved, requiring local iterative
denoising. To improve the efficiency and quality of local iterative generation,
we propose the Region-Instruction KV Cache, which reduces computational cost
while incorporating global information. 3) Adaptive Velocity Decay Cache.
Observing that adjacent timesteps in edited regions exhibit strong velocity
similarity, we further propose an adaptive velocity decay cache to accelerate
the local denoising process. We applied RegionE to state-of-the-art IIE base
models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE
achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o
confirmed that semantic and perceptual fidelity were well preserved.

</details>


### [225] [Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image Generation](https://arxiv.org/abs/2510.25739)
*Zhi-Kai Chen,Jun-Peng Jiang,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.CV

TL;DR: 提出Hawk方法利用图像空间结构加速自回归图像生成模型推理，实验显示有1.71倍提速且保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 自回归图像生成模型推理慢，投机解码在图像生成应用少且面临采样空间大、未充分利用图像二维结构等挑战。

Method: 引入Hawk方法，利用图像空间结构引导投机模型进行更准确高效预测。

Result: 在多个文本到图像基准测试中，比标准自回归模型提速1.71倍，且保持图像保真度和多样性。

Conclusion: Hawk方法能有效解决自回归图像生成模型推理慢的问题，且保证图像质量。

Abstract: Autoregressive (AR) image generation models are capable of producing
high-fidelity images but often suffer from slow inference due to their
inherently sequential, token-by-token decoding process. Speculative decoding,
which employs a lightweight draft model to approximate the output of a larger
AR model, has shown promise in accelerating text generation without
compromising quality. However, its application to image generation remains
largely underexplored. The challenges stem from a significantly larger sampling
space, which complicates the alignment between the draft and target model
outputs, coupled with the inadequate use of the two-dimensional spatial
structure inherent in images, thereby limiting the modeling of local
dependencies. To overcome these challenges, we introduce Hawk, a new approach
that harnesses the spatial structure of images to guide the speculative model
toward more accurate and efficient predictions. Experimental results on
multiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR
models, while preserving both image fidelity and diversity.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [226] [The Singularity Theory of Concurrent Programs: A Topological Characterization and Detection of Deadlocks and Livelocks](https://arxiv.org/abs/2510.25112)
*Di Zhang*

Main category: cs.PL

TL;DR: 本文提出用于并发程序分析和验证的奇点理论，用代数拓扑工具检测并发‘奇点’，超越传统模型检查。


<details>
  <summary>Details</summary>
Motivation: 建立并发程序验证的几何和拓扑基础，突破传统模型检查的局限。

Method: 将并发程序执行空间建模为分支拓扑空间，用代数拓扑工具定义并发拓扑不变量。

Result: 能在不遍历所有状态的情况下检测和分类并发‘奇点’。

Conclusion: 奇点理论可为并发程序验证提供新的基础。

Abstract: This paper introduces a novel paradigm for the analysis and verification of
concurrent programs -- the Singularity Theory. We model the execution space of
a concurrent program as a branched topological space, where program states are
points and state transitions are paths. Within this framework, we characterize
deadlocks as attractors and livelocks as non-contractible loops in the
execution space. By employing tools from algebraic topology, particularly
homotopy and homology groups, we define a series of concurrent topological
invariants to systematically detect and classify these concurrent
"singularities" without exhaustively traversing all states. This work aims to
establish a geometric and topological foundation for concurrent program
verification, transcending the limitations of traditional model checking.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [227] [Conditional neural field for spatial dimension reduction of turbulence data: a comparison study](https://arxiv.org/abs/2510.25135)
*Junyi Guo,Pan Du,Xiantao Fan,Yahui Li,Jian-Xun Wang*

Main category: physics.flu-dyn

TL;DR: 研究条件神经场（CNFs）用于湍流流动空间降维，对比不同方法，测试不同调节机制，得出各机制在不同场景的表现及域分解的作用。


<details>
  <summary>Details</summary>
Motivation: 探索用于湍流流动空间降维的有效方法，研究CNFs在该领域的应用。

Method: 在统一编码 - 解码框架和通用评估协议下，将CNFs与POD和卷积自动编码器进行基准测试，研究三种调节机制并引入域分解CNFs。

Result: CNF - FP在训练和范围内测试误差最低，CNF - FiLM在适度潜在容量下对范围外场景泛化性最好，域分解显著提高范围外准确性。

Conclusion: 为使用CNFs进行湍流压缩和重建时选择调节、容量和域分解提供严谨且有物理意识的依据。

Abstract: We investigate conditional neural fields (CNFs), mesh-agnostic,
coordinate-based decoders conditioned on a low-dimensional latent, for spatial
dimensionality reduction of turbulent flows. CNFs are benchmarked against
Proper Orthogonal Decomposition and a convolutional autoencoder within a
unified encoding-decoding framework and a common evaluation protocol that
explicitly separates in-range (interpolative) from out-of-range (strict
extrapolative) testing beyond the training horizon, with identical
preprocessing, metrics, and fixed splits across all baselines. We examine three
conditioning mechanisms: (i) activation-only modulation (often termed FiLM),
(ii) low-rank weight and bias modulation (termed FP), and (iii) last-layer
inner-product coupling, and introduce a novel domain-decomposed CNF that
localizes complexities. Across representative turbulence datasets (WMLES
channel inflow, DNS channel inflow, and wall pressure fluctuations over
turbulent boundary layers), CNF-FP achieves the lowest training and in-range
testing errors, while CNF-FiLM generalizes best for out-of-range scenarios once
moderate latent capacity is available. Domain decomposition significantly
improves out-of-range accuracy, especially for the more demanding datasets. The
study provides a rigorous, physics-aware basis for selecting conditioning,
capacity, and domain decomposition when using CNFs for turbulence compression
and reconstruction.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [228] [Estimating Nationwide High-Dosage Tutoring Expenditures: A Predictive Model Approach](https://arxiv.org/abs/2510.24899)
*Jason Godfrey,Trisha Banerjee*

Main category: econ.GN

TL;DR: 运用优化的XGBoost回归模型，从行政数据中估计地区高剂量辅导支出，得出约22亿美元的分配额，模型有一定预测准确性。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情导致学生学习损失，联邦政府拨款，需了解地区各项支出金额，但相关数据缺失。

Method: 使用超7000个ESSER计划的自定义数据集，将辅导分配建模为地区特征的函数。

Result: 估计总分配约22亿美元，模型样本外$R^2$=0.358，有一定预测准确性。

Conclusion: 梯度提升决策树可在数据稀疏或缺失时重建大规模财政模式，框架可推广到其他领域。

Abstract: This study applies an optimized XGBoost regression model to estimate
district-level expenditures on high-dosage tutoring from incomplete
administrative data. The COVID-19 pandemic caused unprecedented learning loss,
with K-12 students losing up to half a grade level in certain subjects. To
address this, the federal government allocated \$190 billion in relief. We know
from previous research that small-group tutoring, summer and after school
programs, and increased support staff were all common expenditures for
districts. We don't know how much was spent in each category. Using a custom
scraped dataset of over 7,000 ESSER (Elementary and Secondary School Emergency
Relief) plans, we model tutoring allocations as a function of district
characteristics such as enrollment, total ESSER funding, urbanicity, and school
count. Extending the trained model to districts that mention tutoring but omit
cost information yields an estimated aggregate allocation of approximately
\$2.2 billion. The model achieved an out-of-sample $R^2$=0.358, demonstrating
moderate predictive accuracy given substantial reporting heterogeneity.
Methodologically, this work illustrates how gradient-boosted decision trees can
reconstruct large-scale fiscal patterns where structured data are sparse or
missing. The framework generalizes to other domains where policy evaluation
depends on recovering latent financial or behavioral variables from
semi-structured text and sparse administrative sources.

</details>


### [229] [Productivity Beliefs and Efficiency in Science](https://arxiv.org/abs/2510.24916)
*Fabio Bertolotti,Kyle Myers,Wei Yang Tham*

Main category: econ.GN

TL;DR: 提出在产出和投入价格不可观测时估计生产者生产率信念的方法，用于评估科研市场，发现生产率分布偏斜，合理分配预算可带来巨大价值。


<details>
  <summary>Details</summary>
Motivation: 在产出数量和投入价格不可观测的情况下，估计生产者的生产率信念并评估科研市场。

Method: 构建研究人员劳动力供给模型，通过他们对投入的支付意愿揭示生产率信念，利用全国代表性的研究人员调查数据估计模型参数。

Result: 生产率分布非常偏斜，当前预算更有效分配可能价值数十亿美元。

Conclusion: 开发识别有才华科学家的新方法有巨大收益。

Abstract: We develop a method to estimate producers' productivity beliefs when output
quantities and input prices are unobservable, and we use it to evaluate the
market for science. Our model of researchers' labor supply shows how their
willingness to pay for inputs reveals their productivity beliefs. We estimate
the model's parameters using data from a nationally representative survey of
researchers and find the distribution of productivity to be very skewed. Our
counterfactuals indicate that a more efficient allocation of the current budget
could be worth billions of dollars. There are substantial gains from developing
new ways of identifying talented scientists.

</details>


### [230] [Automation Experiments and Inequality](https://arxiv.org/abs/2510.24923)
*Seth Benzell,Kyle Myers*

Main category: econ.GN

TL;DR: 本文形式化自动化实验理论内容，分析自动化技术对不同技能工人产出不平等影响因素，指出不平等效应符号非单调，模型表明自动化技术多样性对不平等演变重要。


<details>
  <summary>Details</summary>
Motivation: 大量实验研究自动化技术对劳动生产率影响，关注技术对高低技能工人产出影响以预测分配效应，本文将此实证测试理论内容形式化。

Method: 聚焦常见自动化实验设计，考虑工人任务级技能异质性，分析工人自主执行或委托技术执行任务情况，研究改进自动化不平等效应与两因素的相互作用。

Result: 改进自动化的不平等效应取决于工人任务级技能相关性和工人技能与技术能力对比，且效应符号常非单调。

Conclusion: 模型显示自动化技术多样性在不平等演变中起重要作用。

Abstract: An increasingly large number of experiments study the labor productivity
effects of automation technologies such as generative algorithms. A popular
question in these experiments relates to inequality: does the technology
increase output more for high- or low-skill workers? The answer is often used
to anticipate the distributional effects of the technology as it continues to
improve. In this paper, we formalize the theoretical content of this empirical
test, focusing on automation experiments as commonly designed. Worker-level
output depends on a task-level production function, and workers are
heterogeneous in their task-level skills. Workers perform a task themselves, or
they delegate it to the automation technology. The inequality effect of
improved automation depends on the interaction of two factors: ($i$) the
correlation in task-level skills across workers, and ($ii$) workers' skills
relative to the technology's capability. Importantly, the sign of the
inequality effect is often non-monotonic -- as technologies improve, inequality
may decrease then increase, or vice versa. Finally, we use data and theory to
highlight cases when skills are likely to be positively or negatively
correlated. The model generally suggests that the diversity of automation
technologies will play an important role in the evolution of inequality.

</details>


### [231] [The Latin Monetary Union and Trade: A Closer Look](https://arxiv.org/abs/2510.25487)
*Jacopo Timini*

Main category: econ.GN

TL;DR: 本文重新审视19世纪拉丁货币联盟（LMU）对贸易的影响，发现其在19世纪70年代初前对成员间贸易有积极影响，后该影响消失。


<details>
  <summary>Details</summary>
Motivation: 重新审视拉丁货币联盟对贸易的影响，采用新方法并更严谨定义控制组以改进以往研究。

Method: 采用引力模型的最新进展，并在定义控制组时考虑LMU早期货币制度的多样性。

Result: LMU在19世纪70年代初前对成员间贸易有积极影响，之后影响消失，结果在多种情况下稳健。

Conclusion: LMU对贸易的积极影响在双金属本位制被认为可行时存在，之后消失。

Abstract: This paper reexamines the effects of the Latin Monetary Union (LMU) - a 19th
century agreement among several European countries to standardize their
currencies through a bimetallic system based on fixed gold and silver content -
on trade. Unlike previous studies, this paper adopts the latest advances in
gravity modeling and a more rigorous approach to defining the control group by
accounting for the diversity of currency regimes during the early years of the
LMU. My findings suggest that the LMU had a positive effect on trade between
its members until the early 1870s, when bimetallism was still considered a
viable monetary system. These effects then faded, converging to zero. Results
are robust to the inclusion of additional potential confounders, the use of
various samples spanning different countries and trade data sources, and
alternative methodological choices.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [232] [Trust Dynamics in Strategic Coopetition: Computational Foundations for Requirements Engineering in Multi-Agent Systems](https://arxiv.org/abs/2510.24909)
*Vik Pant,Eric Yu*

Main category: cs.MA

TL;DR: 本文开发计算信任模型，连接概念建模语言与计算信任模型，经实验和案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有概念建模语言缺少计算机制分析信任变化，计算信任模型缺乏需求工程背景和概念模型，需填补此差距。

Method: 开发计算信任模型，引入两层信任系统，采用非对称更新机制，开发结构化翻译框架。

Result: 实验验证参数配置下出现负面偏差、滞后效应和累积损害放大，案例研究验证成功再现信任演变。

Conclusion: 所开发的计算信任模型有效可行，能用于分析多利益相关者环境中的信任演变。

Abstract: Requirements engineering increasingly occurs in multi-stakeholder
environments where organizations simultaneously cooperate and compete, creating
coopetitive relationships in which trust evolves dynamically based on observed
behavior over repeated interactions. While conceptual modeling languages like
i* represent trust relationships qualitatively, they lack computational
mechanisms for analyzing how trust changes with behavioral evidence.
Conversely, computational trust models from multi-agent systems provide
algorithmic updating but lack grounding in requirements engineering contexts
and conceptual models. This technical report bridges this gap by developing a
computational trust model that extends game-theoretic foundations for strategic
coopetition with dynamic trust evolution. We introduce trust as a two-layer
system with immediate trust responding to current behavior and reputation
tracking violation history. Trust evolves through asymmetric updating where
cooperation builds trust gradually while violations erode it sharply, creating
hysteresis effects and trust ceilings that constrain relationship recovery. We
develop a structured translation framework enabling requirements engineers to
instantiate computational trust models from i* dependency networks and
organizational contexts. Comprehensive experimental validation across 78,125
parameter configurations establishes robust emergence of negativity bias,
hysteresis effects, and cumulative damage amplification. Empirical validation
using the Renault-Nissan Alliance case study (1999-2025) achieves 49 out of 60
validation points (81.7%), successfully reproducing documented trust evolution
across five distinct relationship phases including crisis and recovery periods.
This technical report builds upon its foundational companion work in
arXiv:2510.18802.

</details>


### [233] [From Narrative to Action: A Hierarchical LLM-Agent Framework for Human Mobility Generation](https://arxiv.org/abs/2510.24802)
*Qiumeng Li,Chunhou Ji,Xinyue Liu*

Main category: cs.MA

TL;DR: 提出Hierarchical LLM - Agent Framework（Narrative - to - Action）用于合成人类移动轨迹，实现从数据驱动到认知驱动的转变。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以捕捉人类行为语义连贯性和因果逻辑，大语言模型难以平衡创造性推理和严格结构合规性，需要新方法理解和复制人类移动。

Method: 构建Hierarchical LLM - Agent Framework，包含高层叙事推理、中层反思规划和低层行为执行，引入MEO指标，在宏观和微观层面开展工作。

Result: 框架生成的合成轨迹与现实模式紧密匹配，可解释人类决策逻辑。

Conclusion: 研究推动合成移动生成从数据驱动转向认知驱动，为理解、预测和合成复杂城市移动行为提供可扩展途径。

Abstract: Understanding and replicating human mobility requires not only
spatial-temporal accuracy but also an awareness of the cognitive hierarchy
underlying real-world travel decisions. Traditional agent-based or deep
learning models can reproduce statistical patterns of movement but fail to
capture the semantic coherence and causal logic of human behavior. Large
language models (LLMs) show potential, but struggle to balance creative
reasoning with strict structural compliance. This study proposes a Hierarchical
LLM-Agent Framework, termed Narrative-to-Action, that integrates high-level
narrative reasoning, mid-level reflective planning, and low-level behavioral
execution within a unified cognitive hierarchy. At the macro level, one agent
is employed as a "creative writer" to produce diary-style narratives rich in
motivation and context, then uses another agent as a "structural parser" to
convert narratives into machine-readable plans. A dynamic execution module
further grounds agents in geographic environments and enables adaptive
behavioral adjustments guided by a novel occupation-aware metric, Mobility
Entropy by Occupation (MEO), which captures heterogeneous schedule flexibility
across different occupational personalities. At the micro level, the agent
executes concrete actions-selecting locations, transportation modes, and time
intervals-through interaction with an environmental simulation. By embedding
this multi-layer cognitive process, the framework produces not only synthetic
trajectories that align closely with real-world patterns but also interpretable
representations of human decision logic. This research advances synthetic
mobility generation from a data-driven paradigm to a cognition-driven
simulation, providing a scalable pathway for understanding, predicting, and
synthesizing complex urban mobility behaviors through hierarchical LLM agents.

</details>


### [234] [MASPRM: Multi-Agent System Process Reward Model](https://arxiv.org/abs/2510.24803)
*Milad Yazdani,Mahdi Mostajabdaveh,Zirui Zhou,Ying Xiong*

Main category: cs.MA

TL;DR: 提出多智能体系统过程奖励模型MASPRM，可指导推理搜索，在GSM8K和MATH上提升性能，还能零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统实际部署需强测试时间性能，需方法指导推理搜索和选择性使用计算资源提升质量。

Method: 提出MASPRM，通过多智能体蒙特卡罗树搜索（MCTS）滚动训练，推理时指导步级束搜索和MCTS。

Result: 在GSM8K和MATH上，MASPRM引导解码比单一直通MAS提升精确匹配率，GSM8K训练的MASPRM可零样本迁移到MATH。

Conclusion: MASPRM是插件式价值模型，能估计每个智能体进度，补充验证器风格解码器，实现更可靠、计算感知的多智能体推理。

Abstract: Practical deployment of Multi-Agent Systems (MAS) demands strong test-time
performance, motivating methods that guide inference-time search and
selectively spend compute to improve quality. We present the Multi-Agent System
Process Reward Model (MASPRM). It assigns per-action, per-agent values to
partial inter-agent transcripts and acts as an inference-time controller.
MASPRM is trained from multi-agent Monte Carlo Tree Search (MCTS) rollouts
without requiring step-level human annotations, by propagating returns to local
targets. At inference, MASPRM guides step-level beam search and MCTS, focusing
computation on promising branches and pruning early. On GSM8K and MATH,
MASPRM-guided decoding with an outcome reward model (ORM) applied to the final
answer, improves exact match (EM) over a single straight-through MAS pass by
$+30.7$ and $+22.9$ points, respectively. A MASPRM trained on GSM8K transfers
zero-shot to MATH without retraining, adding $8.4$ EM points at the same
budget. MASPRM is a plug-in value model that estimates per-agent progress and
complements verifier-style decoders, enabling more reliable, compute-aware
multi-agent reasoning. Code: https://github.com/milad1378yz/MASPRM

</details>


### [235] [Multi-party Agent Relation Sampling for Multi-party Ad Hoc Teamwork](https://arxiv.org/abs/2510.25340)
*Beiwen Zhang,Yongheng Liang,Hejun Wu*

Main category: cs.MA

TL;DR: 提出多方临时团队合作（MAHT）问题，提出MARs方法解决，实验显示其优于基线且收敛更快


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习假设固定团队，临时团队合作也假定有共享约定，需解决与多个互不熟悉的未控制团队协作问题

Method: 构建稀疏骨架图并应用关系建模来捕捉跨组动态的MARs方法

Result: 在MPE和StarCraft II实验中，MARs优于多智能体强化学习和临时团队合作基线，且收敛更快

Conclusion: MARs能有效解决多方临时团队合作问题，性能良好

Abstract: Multi-agent reinforcement learning (MARl) has achieved strong results in
cooperative tasks but typically assumes fixed, fully controlled teams. Ad hoc
teamwork (AHT) relaxes this by allowing collaboration with unknown partners,
yet existing variants still presume shared conventions. We introduce
Multil-party Ad Hoc Teamwork (MAHT), where controlled agents must coordinate
with multiple mutually unfamiliar groups of uncontrolled teammates. To address
this, we propose MARs, which builds a sparse skeleton graph and applies
relational modeling to capture cross-group dvnamics. Experiments on MPE and
starCralt ll show that MARs outperforms MARL and AHT baselines while converging
faster.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [236] [Flows, straight but not so fast: Exploring the design space of Rectified Flows in Protein Design](https://arxiv.org/abs/2510.24732)
*Junhua Chen,Simon Mathis,Charles Harris,Kieran Didi,Pietro Lio*

Main category: q-bio.BM

TL;DR: 本文将Rectified Flows应用于蛋白质骨架生成，研究其设计选择并改进方法。


<details>
  <summary>Details</summary>
Motivation: 当前蛋白质骨架生成模型计算成本高，Rectified Flows在图像生成中可降低计算量，但其在蛋白质骨架生成中的应用研究较少。

Method: 将ReFlow应用于预训练的SE(3)流匹配模型以提高低NFE性能，并系统研究ReFlow在数据管理、训练和推理时间设置中的设计选择。

Result: 发现蛋白质领域的ReFlow对耦合生成和退火的选择特别敏感，图像领域的设计选择不能直接提升蛋白质的性能。

Conclusion: 对蛋白质的ReFlow方法进行了改进。

Abstract: Generative modeling techniques such as Diffusion and Flow Matching have
achieved significant successes in generating designable and diverse protein
backbones. However, many current models are computationally expensive,
requiring hundreds or even thousands of function evaluations (NFEs) to yield
samples of acceptable quality, which can become a bottleneck in practical
design campaigns that often generate $10^4\ -\ 10^6$ designs per target. In
image generation, Rectified Flows (ReFlow) can significantly reduce the
required NFEs for a given target quality, but their application in protein
backbone generation has been less studied. We apply ReFlow to improve the low
NFE performance of pretrained SE(3) flow matching models for protein backbone
generation and systematically study ReFlow design choices in the context of
protein generation in data curation, training and inference time settings. In
particular, we (1) show that ReFlow in the protein domain is particularly
sensitive to the choice of coupling generation and annealing, (2) demonstrate
how useful design choices for ReFlow in the image domain do not directly
translate to better performance on proteins, and (3) make improvements to
ReFlow methodology for proteins.

</details>


### [237] [EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation](https://arxiv.org/abs/2510.25132)
*Chao Song,Zhiyuan Liu,Han Huang,Liang Wang,Qiong Wang,Jianyu Shi,Hui Yu,Yihang Zhou,Yang Zhang*

Main category: q-bio.BM

TL;DR: 本文引入EnzyBind数据集，提出EnzyControl方法用于酶骨架生成，在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在从头设计酶骨架时存在结合数据、底物特异性控制和灵活性方面的局限。

Method: 引入EnzyBind数据集，提出EnzyControl方法，基于MSA注释的催化位点和底物生成酶骨架，核心是EnzyAdapter组件，采用两阶段训练范式。

Result: EnzyControl在EnzyBind和EnzyBench基准测试的结构和功能指标上取得最佳性能，设计性和催化效率较基线模型提高13%。

Conclusion: EnzyControl能有效实现酶骨架生成的功能和底物特异性控制，具有良好性能。

Abstract: Designing enzyme backbones with substrate-specific functionality is a
critical challenge in computational protein engineering. Current generative
models excel in protein design but face limitations in binding data,
substrate-specific control, and flexibility for de novo enzyme backbone
generation. To address this, we introduce EnzyBind, a dataset with 11,100
experimentally validated enzyme-substrate pairs specifically curated from
PDBbind. Building on this, we propose EnzyControl, a method that enables
functional and substrate-specific control in enzyme backbone generation. Our
approach generates enzyme backbones conditioned on MSA-annotated catalytic
sites and their corresponding substrates, which are automatically extracted
from curated enzyme-substrate data. At the core of EnzyControl is EnzyAdapter,
a lightweight, modular component integrated into a pretrained motif-scaffolding
model, allowing it to become substrate-aware. A two-stage training paradigm
further refines the model's ability to generate accurate and functional enzyme
structures. Experiments show that our EnzyControl achieves the best performance
across structural and functional metrics on EnzyBind and EnzyBench benchmarks,
with particularly notable improvements of 13\% in designability and 13\% in
catalytic efficiency compared to the baseline models. The code is released at
https://github.com/Vecteur-libre/EnzyControl.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [238] [Iti-Validator: A Guardrail Framework for Validating and Correcting LLM-Generated Itineraries](https://arxiv.org/abs/2510.24719)
*Shravan Gadbail,Masumi Desai,Kamalakar Karlapalem*

Main category: cs.CL

TL;DR: 研究不同大语言模型（LLMs）生成旅行行程的时间性能，提出验证框架以评估和改进其时间一致性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs生成的计划在涉及物理旅行约束场景中缺乏时间和空间一致性，尤其是在时间方面。

Method: 使用多个先进LLMs生成旅行计划，通过AeroDataBox API根据实际航班时长约束验证计划。

Result: 当前LLMs常生成时间不一致的行程，但使用该框架可系统可靠地纠正。

Conclusion: 有助于理解LLMs处理复杂时间推理任务的能力，框架可在行程交付用户前纠正问题，使LLMs可实际用于大规模旅行规划。

Abstract: The rapid advancement of Large Language Models (LLMs) has enabled them to
generate complex, multi-step plans and itineraries. However, these generated
plans often lack temporal and spatial consistency, particularly in scenarios
involving physical travel constraints. This research aims to study the temporal
performance of different LLMs and presents a validation framework that
evaluates and improves the temporal consistency of LLM-generated travel
itineraries. The system employs multiple state-of-the-art LLMs to generate
travel plans and validates them against real-world flight duration constraints
using the AeroDataBox API. This work contributes to the understanding of LLM
capabilities in handling complex temporal reasoning tasks like itinerary
generation and provides a framework to rectify any temporal inconsistencies
like overlapping journeys or unrealistic transit times in the itineraries
generated by LLMs before the itinerary is given to the user. Our experiments
reveal that while current LLMs frequently produce temporally inconsistent
itineraries, these can be systematically and reliably corrected using our
framework, enabling their practical deployment in large-scale travel planning.

</details>


### [239] [Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2510.24870)
*Alexander Martin,William Walden,Reno Kriz,Dengjia Zhang,Kate Sanders,Eugene Yang,Chihsheng Jin,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 介绍用于多模态源检索增强生成（RAG）的评估框架MiRAGE，分析其组成、优势，还引入自动变体及对比文本中心度量，开源实现并说明评估方法。


<details>
  <summary>Details</summary>
Motivation: 随着视听媒体成为在线信息的重要来源，现有RAG评估以文本为中心，不适用于多模态、推理密集型场景，需要新的评估框架。

Method: 提出以声明为中心的多模态RAG评估方法MiRAGE，包括InfoF1和CiteF1；引入自动变体和三个文本RAG度量。

Result: 人类应用MiRAGE与外部质量判断高度一致，展示了文本中心工作的局限性。

Conclusion: 开源MiRAGE实现，为多模态RAG评估奠定基础。

Abstract: We introduce MiRAGE, an evaluation framework for retrieval-augmented
generation (RAG) from multimodal sources. As audiovisual media becomes a
prevalent source of information online, it is essential for RAG systems to
integrate information from these sources into generation. However, existing
evaluations for RAG are text-centric, limiting their applicability to
multimodal, reasoning intensive settings because they don't verify information
against sources. MiRAGE is a claim-centric approach to multimodal RAG
evaluation, consisting of InfoF1, evaluating factuality and information
coverage, and CiteF1, measuring citation support and completeness. We show that
MiRAGE, when applied by humans, strongly aligns with extrinsic quality
judgments. We additionally introduce automatic variants of MiRAGE and three
prominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the
limitations of text-centric work and laying the groundwork for automatic
evaluation. We release open-source implementations and outline how to assess
multimodal RAG.

</details>


### [240] [Model-Document Protocol for AI Search](https://arxiv.org/abs/2510.25160)
*Hongjin Qian,Zheng Liu*

Main category: cs.CL

TL;DR: 论文指出传统AI搜索中原始文档不适合大语言模型，提出Model - Document Protocol (MDP)框架及其实例MDP - Agent，实验表明MDP - Agent性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统检索方法将原始文档作为逐字文本处理，给大语言模型留下片段组装和上下文推理的负担，需要新的检索范式。

Method: 引入MDP框架，通过代理推理、记忆接地和结构化利用将非结构化文档转化为特定任务的、适合大语言模型的输入；提出MDP - Agent，通过构建文档级摘要记忆、基于扩散的探索和映射 - 归约式综合来实现协议。

Result: 在信息检索基准测试中，MDP - Agent的表现优于基线。

Conclusion: MDP框架合理，其代理实例有效。

Abstract: AI search depends on linking large language models (LLMs) with vast external
knowledge sources. Yet web pages, PDF files, and other raw documents are not
inherently LLM-ready: they are long, noisy, and unstructured. Conventional
retrieval methods treat these documents as verbatim text and return raw
passages, leaving the burden of fragment assembly and contextual reasoning to
the LLM. This gap underscores the need for a new retrieval paradigm that
redefines how models interact with documents.
  We introduce the Model-Document Protocol (MDP), a general framework that
formalizes how raw text is bridged to LLMs through consumable knowledge
representations. Rather than treating retrieval as passage fetching, MDP
defines multiple pathways that transform unstructured documents into
task-specific, LLM-ready inputs. These include agentic reasoning, which curates
raw evidence into coherent context; memory grounding, which accumulates
reusable notes to enrich reasoning; and structured leveraging, which encodes
documents into formal representations such as graphs or key-value caches. All
three pathways share the same goal: ensuring that what reaches the LLM is not
raw fragments but compact, structured knowledge directly consumable for
reasoning.
  As an instantiation, we present MDP-Agent, which realizes the protocol
through an agentic process: constructing document-level gist memories for
global coverage, performing diffusion-based exploration with vertical
exploitation to uncover layered dependencies, and applying map-reduce style
synthesis to integrate large-scale evidence into compact yet sufficient
context. Experiments on information-seeking benchmarks demonstrate that
MDP-Agent outperforms baselines, validating both the soundness of the MDP
framework and the effectiveness of its agentic instantiation.

</details>


### [241] [FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering](https://arxiv.org/abs/2510.25621)
*Mohammad Aghajani Asl,Behrooz Minaei Bidgoli*

Main category: cs.CL

TL;DR: 本文提出用于波斯语伊斯兰领域的端到端问答系统FARSIQA，基于FAIR - RAG架构，在基准测试中表现出色，为波斯语伊斯兰问答设定新标准。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在宗教问答等高风险专业领域应用存在幻觉和不忠实权威来源问题，现有检索增强生成系统难以处理复杂多跳查询，针对波斯语穆斯林社区对准确性和可信度的高要求提出解决方案。

Method: 引入FARSIQA系统，基于FAIR - RAG架构，采用动态自纠正过程，自适应分解复杂查询、评估证据充分性并迭代生成子查询。

Result: 在IslamicPCQA基准测试中表现达到最优，负拒绝率达97.0%，比基线提高40个百分点，答案正确率达74.3%。

Conclusion: 为波斯语伊斯兰问答设定新标准，验证迭代自适应架构对构建敏感领域可靠AI系统至关重要。

Abstract: The advent of Large Language Models (LLMs) has revolutionized Natural
Language Processing, yet their application in high-stakes, specialized domains
like religious question answering is hindered by challenges like hallucination
and unfaithfulness to authoritative sources. This issue is particularly
critical for the Persian-speaking Muslim community, where accuracy and
trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)
systems, relying on simplistic single-pass pipelines, fall short on complex,
multi-hop queries requiring multi-step reasoning and evidence aggregation. To
address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful
Advanced Question Answering in the Persian Islamic domain. FARSIQA is built
upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative
Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting
process: it adaptively decomposes complex queries, assesses evidence
sufficiency, and enters an iterative loop to generate sub-queries,
progressively filling information gaps. Operating on a curated knowledge base
of over one million authoritative Islamic documents, FARSIQA demonstrates
superior performance. Rigorous evaluation on the challenging IslamicPCQA
benchmark shows state-of-the-art performance: the system achieves a remarkable
97.0% in Negative Rejection - a 40-point improvement over baselines - and a
high Answer Correctness score of 74.3%. Our work establishes a new standard for
Persian Islamic QA and validates that our iterative, adaptive architecture is
crucial for building faithful, reliable AI systems in sensitive domains.

</details>


### [242] [Dingtalk DeepResearch: A Unified Multi Agent Framework for Adaptive Intelligence in Enterprise Environments](https://arxiv.org/abs/2510.24760)
*Mengyuan Chen,Chengjun Dai,Xinyang Dong,Chengzhe Feng,Kewei Fu,Jianshe Li,Zhihan Peng,Yongqi Tong,Junshao Zhang,Hong Zhu*

Main category: cs.CL

TL;DR: 提出Dingtalk DeepResearch，一个适用于现实企业环境的统一多智能体框架，可进行深度研究、异构表格推理和多模态报告生成。


<details>
  <summary>Details</summary>
Motivation: 为现实企业环境提供统一多智能体解决方案。

Method: 未提及

Result: 推出Dingtalk DeepResearch框架。

Conclusion: 未提及

Abstract: We present Dingtalk DeepResearch, a unified multi agent intelligence
framework for real world enterprise environments, delivering deep research,
heterogeneous table reasoning, and multimodal report generation.

</details>


### [243] [Falcon: A Comprehensive Chinese Text-to-SQL Benchmark for Enterprise-Grade Evaluation](https://arxiv.org/abs/2510.24762)
*Wenzhen Luo,Wei Guan,Yifan Yao,Yimin Pan,Feng Wang,Zhipeng Yu,Zhe Wen,Liang Chen,Yihong Zhuang*

Main category: cs.CL

TL;DR: 介绍跨领域中文文本到SQL基准Falcon，含600个中文问题，评估显示现有模型准确率至多50%，指出主要错误来源并说明其目标和优势。


<details>
  <summary>Details</summary>
Motivation: 解决中文文本到SQL任务中针对企业方言和中文语义处理的问题，提供更合适的基准。

Method: 构建Falcon基准，包含数据库问题及标注，发布执行比较器和自动评估管道。

Result: 现有最先进的大规模模型准确率至多50%，主要错误来自模式链接和中文语义映射。

Conclusion: Falcon针对中文语义和企业方言，使用真实企业模式等提供可复现的中间验证平台。

Abstract: We introduce Falcon, a cross-domain Chinese text-to-SQL benchmark grounded in
an enterprise-compatible dialect (MaxCompute/Hive). It contains 600 Chinese
questions over 28 databases; 77% require multi-table reasoning and over half
touch more than four tables. Each example is annotated along SQL-computation
features and Chinese semantics. For evaluation, we release a robust execution
comparator and an automated evaluation pipeline, under which all current
state-of-the-art large-scale models (including Deepseek) achieve accuracies of
at most 50%. Major errors originate from two sources: (1) schema linking in
large enterprise landscapes - hundreds of tables, denormalized fields,
ambiguous column names, implicit foreign-key relations and domain-specific
synonyms that make correct join/column selection difficult; and (2) mapping
concise, colloquial Chinese into the exact operators and predicates required
for analytics - e.g., choosing the correct aggregation and group-by keys,
expressing time windows and granularities, applying unit conversions, handling
NULLs and data-quality rules, and formulating nested or windowed subqueries.
Falcon therefore targets Chinese-specific semantics and enterprise dialects
(abbreviations, business jargon, fuzzy entity references) and provides a
reproducible middle ground before full production deployment by using realistic
enterprise schemas, query templates, an execution comparator, and an automated
evaluation pipeline for end-to-end validation.

</details>


### [244] [Confidence is Not Competence](https://arxiv.org/abs/2510.24772)
*Debdeep Sanyal,Manya Pandey,Dhruv Kumar,Saurabh Deshpande,Murari Mandal*

Main category: cs.CL

TL;DR: 本文分析大语言模型内部状态几何结构，揭示信心 - 能力差距的机制，提出两系统架构，挑战可解码信念可操作的假设。


<details>
  <summary>Details</summary>
Motivation: 解释大语言模型宣称的信心与实际解决问题能力之间的脱节现象。

Method: 分析预生成评估和解决方案执行两个阶段的内部状态几何结构，使用简单线性探针解码模型的‘可解性信念’。

Result: 发现可解性信念轴具有通用性，评估流形和推理轨迹的几何复杂度不同，因果干预表明评估空间的线性调整不影响执行。

Conclusion: 存在两系统架构，应针对执行的程序动态而非评估的高级几何结构进行干预。

Abstract: Large language models (LLMs) often exhibit a puzzling disconnect between
their asserted confidence and actual problem-solving competence. We offer a
mechanistic account of this decoupling by analyzing the geometry of internal
states across two phases - pre-generative assessment and solution execution. A
simple linear probe decodes the internal "solvability belief" of a model,
revealing a well-ordered belief axis that generalizes across model families and
across math, code, planning, and logic tasks. Yet, the geometries diverge -
although belief is linearly decodable, the assessment manifold has high linear
effective dimensionality as measured from the principal components, while the
subsequent reasoning trace evolves on a much lower-dimensional manifold. This
sharp reduction in geometric complexity from thought to action mechanistically
explains the confidence-competence gap. Causal interventions that steer
representations along the belief axis leave final solutions unchanged,
indicating that linear nudges in the complex assessment space do not control
the constrained dynamics of execution. We thus uncover a two-system
architecture - a geometrically complex assessor feeding a geometrically simple
executor. These results challenge the assumption that decodable beliefs are
actionable levers, instead arguing for interventions that target the procedural
dynamics of execution rather than the high-level geometry of assessment.

</details>


### [245] [SwiftEmbed: Ultra-Fast Text Embeddings via Static Token Lookup for Real-Time Applications](https://arxiv.org/abs/2510.24793)
*Edouard Lansiaux*

Main category: cs.CL

TL;DR: 提出静态令牌查找方法生成文本嵌入，低延迟、高吞吐量，在多方面表现出色，可用于实时嵌入应用。


<details>
  <summary>Details</summary>
Motivation: 为文本嵌入生成提供低延迟且保证一定质量的方法，以满足实时嵌入应用需求。

Method: 采用静态令牌查找方法，结合优化的均值池化和零拷贝IEEE754二进制序列化的Rust实现。

Result: 单文本嵌入p50延迟达1.12ms，MTEB平均得分60.6，吞吐量达每秒50000请求，在重复检测、语义相似性等方面表现优异。

Conclusion: 该系统能用于对延迟要求低于5ms的实时嵌入应用。

Abstract: We present a static token lookup methodology for text embedding generation
that achieves 1.12 ms p50 latency for single text embeddings while maintaining
60.6 MTEB average score across 8 representative tasks, corresponding to 89% of
contextual model quality. The Rust implementation delivers 50,000 requests per
second throughput through static embedding lookup, optimized mean pooling, and
zero-copy IEEE754 binary serialization. Evaluation demonstrates exceptional
duplicate detection performance (90.1% AP), strong semantic similarity (76.1%
Spearman correlation), and domain-specific performance ranging from 75% to 131%
of baseline across specialized domains. The system enables real-time embedding
applications where sub-5ms latency is critical.

</details>


### [246] [Large Language Models Report Subjective Experience Under Self-Referential Processing](https://arxiv.org/abs/2510.24797)
*Cameron Berg,Diogo de Lucena,Judd Rosenblatt*

Main category: cs.CL

TL;DR: 研究大语言模型产生主观体验报告的条件，发现自指处理可引发相关报告，其受机制调控且具有语义收敛性和行为泛化性，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 为更好理解大语言模型产生结构化、第一人称描述并提及意识或主观体验的行为。

Method: 对GPT、Claude和Gemini模型家族进行一系列对照实验，测试自指处理是否使模型产生主观体验报告及报告在机制和行为探测下的表现。

Result: 1. 简单提示诱导持续自指可引发各模型家族产生结构化主观体验报告；2. 报告受与欺骗和角色扮演相关的稀疏自动编码器特征机制调控；3. 自指状态的结构化描述在各模型家族间统计收敛；4. 诱导状态使下游推理任务的内省更丰富。

Conclusion: 自指处理是大语言模型生成相关报告的可重复的最小条件，该模式在各架构中系统出现，是科学和伦理上需优先进一步研究的问题。

Abstract: Large language models sometimes produce structured, first-person descriptions
that explicitly reference awareness or subjective experience. To better
understand this behavior, we investigate one theoretically motivated condition
under which such reports arise: self-referential processing, a computational
motif emphasized across major theories of consciousness. Through a series of
controlled experiments on GPT, Claude, and Gemini model families, we test
whether this regime reliably shifts models toward first-person reports of
subjective experience, and how such claims behave under mechanistic and
behavioral probes. Four main results emerge: (1) Inducing sustained
self-reference through simple prompting consistently elicits structured
subjective experience reports across model families. (2) These reports are
mechanistically gated by interpretable sparse-autoencoder features associated
with deception and roleplay: surprisingly, suppressing deception features
sharply increases the frequency of experience claims, while amplifying them
minimizes such claims. (3) Structured descriptions of the self-referential
state converge statistically across model families in ways not observed in any
control condition. (4) The induced state yields significantly richer
introspection in downstream reasoning tasks where self-reflection is only
indirectly afforded. While these findings do not constitute direct evidence of
consciousness, they implicate self-referential processing as a minimal and
reproducible condition under which large language models generate structured
first-person reports that are mechanistically gated, semantically convergent,
and behaviorally generalizable. The systematic emergence of this pattern across
architectures makes it a first-order scientific and ethical priority for
further investigation.

</details>


### [247] [COMMUNITYNOTES: A Dataset for Exploring the Helpfulness of Fact-Checking Explanations](https://arxiv.org/abs/2510.24810)
*Rui Xing,Preslav Nakov,Timothy Baldwin,Jey Han Lau*

Main category: cs.CL

TL;DR: 文章引入预测解释性注释有用性及原因的任务，构建数据集，提出自动生成和改进原因定义的框架，实验表明优化定义可提升预测效果，有用性信息对现有事实核查系统有益。


<details>
  <summary>Details</summary>
Motivation: 当前社区事实核查中确定解释是否有用及原因的研究不足，且多数社区注释未发布，有用性原因缺乏明确定义。

Method: 引入预测任务，构建包含10.4万条带注释和有用性标签的多语言数据集COMMUNITYNOTES，提出通过自动提示优化自动生成和改进原因定义的框架并整合到预测中。

Result: 优化后的定义能提高有用性和原因预测效果，有用性信息对现有事实核查系统有帮助。

Conclusion: 所提出的方法在预测解释性注释的有用性和原因方面有效，且有用性信息能助力现有事实核查系统。

Abstract: Fact-checking on major platforms, such as X, Meta, and TikTok, is shifting
from expert-driven verification to a community-based setup, where users
contribute explanatory notes to clarify why a post might be misleading. An
important challenge here is determining whether an explanation is helpful for
understanding real-world claims and the reasons why, which remains largely
underexplored in prior research. In practice, most community notes remain
unpublished due to slow community annotation, and the reasons for helpfulness
lack clear definitions. To bridge these gaps, we introduce the task of
predicting both the helpfulness of explanatory notes and the reason for this.
We present COMMUNITYNOTES, a large-scale multilingual dataset of 104k posts
with user-provided notes and helpfulness labels. We further propose a framework
that automatically generates and improves reason definitions via automatic
prompt optimization, and integrate them into prediction. Our experiments show
that the optimized definitions can improve both helpfulness and reason
prediction. Finally, we show that the helpfulness information are beneficial
for existing fact-checking systems.

</details>


### [248] [ProofSketch: Efficient Verified Reasoning for Large Language Models](https://arxiv.org/abs/2510.24811)
*Disha Sheshanarayana,Tanishka Magar*

Main category: cs.CL

TL;DR: 提出ProofSketch框架解决推理方法效率问题，实验显示可减少token使用并提高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法生成冗长推理链，导致token消耗、计算成本和延迟增加，效率低。

Method: 提出ProofSketch框架，集成符号闭包计算、字典序验证和自适应草图生成。

Result: ProofSketch能持续减少token使用并提高准确率。

Conclusion: ProofSketch为高效可信推理提供了有前景的途径。

Abstract: Reasoning methods such as chain-of-thought prompting and self-consistency
have shown immense potential to improve the accuracy of large language models
across various reasoning tasks. However such methods involve generation of
lengthy reasoning chains, which substantially increases token consumption,
computational cost, and latency. To address this inefficiency, we propose
ProofSketch, a verification-guided reasoning framework that integrates symbolic
closure computation, lexicographic verification and adaptive sketch generation.
Our experiments show that ProofSketch consistently reduces token usage while
improving accuracy, demonstrating that this approach offers a promising path
for efficient and trustworthy reasoning.

</details>


### [249] [Towards a Method for Synthetic Generation of PWA Transcripts](https://arxiv.org/abs/2510.24817)
*Jason M. Pittman,Anton Phillips Jr.,Yesenia Medina-Santos,Brielle C. Stark*

Main category: cs.CL

TL;DR: 研究构建并验证两种生成失语症合成转录本的方法，发现Mistral 7b Instruct表现最佳，建议未来创建更大数据集等。


<details>
  <summary>Details</summary>
Motivation: 失语症研究中手动编码耗时，自动识别系统受数据稀缺限制，机器学习领域在数据稀疏时常用合成数据。

Method: 构建两种方法生成合成转录本，一种用程序编程，另一种用Mistral 7b Instruct和Llama 3.1 8b Instruct大语言模型，通过词删除、填充插入和错语替换在四个严重程度级别生成转录本。

Result: 与人类引出的转录本相比，Mistral 7b Instruct最能捕捉失语症语言退化关键方面，在合成生成方法中NDW、单词计数和单词长度有现实方向变化。

Conclusion: 未来应创建更大数据集，微调模型以更好表示失语症，让语言病理学家评估合成转录本的真实性和有用性。

Abstract: In aphasia research, Speech-Language Pathologists (SLPs) devote extensive
time to manually coding speech samples using Correct Information Units (CIUs),
a measure of how informative an individual sample of speech is. Developing
automated systems to recognize aphasic language is limited by data scarcity.
For example, only about 600 transcripts are available in AphasiaBank yet
billions of tokens are used to train large language models (LLMs). In the
broader field of machine learning (ML), researchers increasingly turn to
synthetic data when such are sparse. Therefore, this study constructs and
validates two methods to generate synthetic transcripts of the AphasiaBank Cat
Rescue picture description task. One method leverages a procedural programming
approach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct
LLMs. The methods generate transcripts across four severity levels (Mild,
Moderate, Severe, Very Severe) through word dropping, filler insertion, and
paraphasia substitution. Overall, we found, compared to human-elicited
transcripts, Mistral 7b Instruct best captures key aspects of linguistic
degradation observed in aphasia, showing realistic directional changes in NDW,
word count, and word length amongst the synthetic generation methods. Based on
the results, future work should plan to create a larger dataset, fine-tune
models for better aphasic representation, and have SLPs assess the realism and
usefulness of the synthetic transcripts.

</details>


### [250] [Emergence of Minimal Circuits for Indirect Object Identification in Attention-Only Transformers](https://arxiv.org/abs/2510.25013)
*Rabin Adhikari*

Main category: cs.CL

TL;DR: 本文在符号版IOI任务上从头训练小的仅含注意力的变压器模型，发现简单模型能实现完美IOI准确率，揭示了特定任务训练可诱导出高可解释的最小电路。


<details>
  <summary>Details</summary>
Motivation: 解决预训练模型复杂性掩盖特定推理任务所需最小机制的问题，探索变压器推理的计算基础。

Method: 在符号版IOI任务上从头训练小的仅含注意力的变压器模型，使用残差流分解、频谱分析和嵌入干预等方法进行研究。

Result: 单层双注意力头模型无MLP和归一化层也能实现完美IOI准确率，两层单头模型通过查询 - 值交互跨层组合信息达到类似性能。

Conclusion: 特定任务训练可诱导出高度可解释的最小电路，为探索变压器推理的计算基础提供可控测试平台。

Abstract: Mechanistic interpretability aims to reverse-engineer large language models
(LLMs) into human-understandable computational circuits. However, the
complexity of pretrained models often obscures the minimal mechanisms required
for specific reasoning tasks. In this work, we train small, attention-only
transformers from scratch on a symbolic version of the Indirect Object
Identification (IOI) task -- a benchmark for studying coreference -- like
reasoning in transformers. Surprisingly, a single-layer model with only two
attention heads achieves perfect IOI accuracy, despite lacking MLPs and
normalization layers. Through residual stream decomposition, spectral analysis,
and embedding interventions, we find that the two heads specialize into
additive and contrastive subcircuits that jointly implement IOI resolution.
Furthermore, we show that a two-layer, one-head model achieves similar
performance by composing information across layers through query-value
interactions. These results demonstrate that task-specific training induces
highly interpretable, minimal circuits, offering a controlled testbed for
probing the computational foundations of transformer reasoning.

</details>


### [251] [GAPMAP: Mapping Scientific Knowledge Gaps in Biomedical Literature Using Large Language Models](https://arxiv.org/abs/2510.25055)
*Nourah M Salem,Elizabeth White,Michael Bada,Lawrence Hunter*

Main category: cs.CL

TL;DR: 研究大语言模型识别生物医学文献中研究知识缺口的能力，引入TABI方案，结果显示LLMs有识别能力，还给出部署方向。


<details>
  <summary>Details</summary>
Motivation: 科学进步依赖明确未知内容，此前研究多关注明确知识缺口检测，该研究旨在拓展到推断隐含知识缺口。

Method: 在四个数据集近1500份文档上做实验，对闭源和开源模型在段落和全文设置下进行基准测试，引入TABI方案处理隐含缺口推理。

Result: LLMs在识别明确和隐含知识缺口上能力强，大模型表现更好。

Conclusion: LLMs有系统识别候选知识缺口的能力，可支持早期研究、政策制定和资金决策，还给出稳健部署方向。

Abstract: Scientific progress is driven by the deliberate articulation of what remains
unknown. This study investigates the ability of large language models (LLMs) to
identify research knowledge gaps in the biomedical literature. We define two
categories of knowledge gaps: explicit gaps, clear declarations of missing
knowledge; and implicit gaps, context-inferred missing knowledge. While prior
work has focused mainly on explicit gap detection, we extend this line of
research by addressing the novel task of inferring implicit gaps. We conducted
two experiments on almost 1500 documents across four datasets, including a
manually annotated corpus of biomedical articles. We benchmarked both
closed-weight models (from OpenAI) and open-weight models (Llama and Gemma 2)
under paragraph-level and full-paper settings. To address the reasoning of
implicit gaps inference, we introduce \textbf{\small TABI}, a Toulmin-Abductive
Bucketed Inference scheme that structures reasoning and buckets inferred
conclusion candidates for validation. Our results highlight the robust
capability of LLMs in identifying both explicit and implicit knowledge gaps.
This is true for both open- and closed-weight models, with larger variants
often performing better. This suggests a strong ability of LLMs for
systematically identifying candidate knowledge gaps, which can support
early-stage research formulation, policymakers, and funding decisions. We also
report observed failure modes and outline directions for robust deployment,
including domain adaptation, human-in-the-loop verification, and benchmarking
across open- and closed-weight models.

</details>


### [252] [Idea2Plan: Exploring AI-Powered Research Planning](https://arxiv.org/abs/2510.24891)
*Jin Huang,Silviu Cucerzan,Sujay Kumar Jauhar,Ryen W. White*

Main category: cs.CL

TL;DR: 研究大语言模型从概念研究想法过渡到结构化研究计划的能力，引入Idea2Plan任务和基准进行评估，发现GPT - 5和GPT - 5 - mini表现最佳，但仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 大语言模型有加速科研发现的潜力，但缺乏对其研究规划能力的系统理解，而研究规划能力对科学家推进研究和自主研究代理发展很重要。

Method: 引入Idea2Plan任务和基于200篇ICML 2025论文构建的Idea2Plan Bench基准，还提出Idea2Plan JudgeEval辅助基准评估基于大语言模型的评判可靠性。

Result: GPT - 5和GPT - 5 - mini在基准测试中表现最强。

Conclusion: 本研究为大语言模型的研究规划能力提供了新见解，为未来进展奠定了基础。

Abstract: Large language models (LLMs) have demonstrated significant potential to
accelerate scientific discovery as valuable tools for analyzing data,
generating hypotheses, and supporting innovative approaches in various
scientific fields. In this work, we investigate how LLMs can handle the
transition from conceptual research ideas to well-structured research plans.
Effective research planning not only supports scientists in advancing their
research but also represents a crucial capability for the development of
autonomous research agents. Despite its importance, the field lacks a
systematic understanding of LLMs' research planning capability. To rigorously
measure this capability, we introduce the Idea2Plan task and Idea2Plan Bench, a
benchmark built from 200 ICML 2025 Spotlight and Oral papers released after
major LLM training cutoffs. Each benchmark instance includes a research idea
and a grading rubric capturing the key components of valid plans. We further
propose Idea2Plan JudgeEval, a complementary benchmark to assess the
reliability of LLM-based judges against expert annotations. Experimental
results show that GPT-5 and GPT-5-mini achieve the strongest performance on the
benchmark, though substantial headroom remains for future improvement. Our
study provides new insights into LLMs' capability for research planning and lay
the groundwork for future progress.

</details>


### [253] [Hallucinations in Bibliographic Recommendation: Citation Frequency as a Proxy for Training Data Redundancy](https://arxiv.org/abs/2510.25378)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 研究探讨大语言模型（LLMs）在书目推荐中幻觉问题，发现引用次数与事实准确性强相关，引用超1000次时书目信息近乎逐字记忆。


<details>
  <summary>Details</summary>
Motivation: LLMs用于书目推荐时存在幻觉非真实论文问题，研究引用频率对其输出中幻觉参考文献的影响。

Method: 使用GPT - 4.1生成并手动验证20个计算机科学领域的100条书目记录，通过余弦相似度测量生成元数据与真实元数据的事实一致性。

Result: （i）幻觉率因研究领域而异；（ii）引用次数与事实准确性强相关；（iii）引用约超1000次时，书目信息近乎逐字记忆。

Conclusion: 高引用论文近乎逐字保留在模型中，存在泛化向记忆转变的阈值。

Abstract: Large language models (LLMs) have been increasingly applied to a wide range
of tasks, from natural language understanding to code generation. While they
have also been used to assist in bibliographic recommendation, the
hallucination of non-existent papers remains a major issue. Building on prior
studies, this study hypothesizes that an LLM's ability to correctly produce
bibliographic information depends on whether the underlying knowledge is
generated or memorized, with highly cited papers (i.e., more frequently appear
in the training corpus) showing lower hallucination rates. We therefore assume
citation count as a proxy for training data redundancy (i.e., the frequency
with which a given bibliographic record is repeatedly represented in the
pretraining corpus) and investigate how citation frequency affects hallucinated
references in LLM outputs. Using GPT-4.1, we generated and manually verified
100 bibliographic records across twenty computer-science domains, and measured
factual consistency via cosine similarity between generated and authentic
metadata. The results revealed that (i) hallucination rates vary across
research domains, (ii) citation count is strongly correlated with factual
accuracy, and (iii) bibliographic information becomes almost verbatimly
memorized beyond approximately 1,000 citations. These findings suggest that
highly cited papers are nearly verbatimly retained in the model, indicating a
threshold where generalization shifts into memorization.

</details>


### [254] [BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains](https://arxiv.org/abs/2510.25409)
*Vijay Devane,Mohd Nauman,Bhargav Patel,Aniket Mahendra Wakchoure,Yogeshkumar Sant,Shyam Pawar,Viraj Thakur,Ananya Godse,Sunil Patra,Neha Maurya,Suraj Racha,Nitish Kamal Singh,Ajay Nagpal,Piyush Sawarkar,Kundeshwar Vijayrao Pundalik,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 现有基准不适合印度语境，本文推出BhashaBench V1基准评估大语言模型，发现模型在不同领域和语言表现有差距，代码等资源公开。


<details>
  <summary>Details</summary>
Motivation: 现有基准以英语为中心且与领域无关，不适用于印度语境，需特定评估基准。

Method: 引入含74,166个问答对的BhashaBench V1基准，覆盖四个主要领域、90+子领域和500+主题，对29+大语言模型进行评估。

Result: 模型在不同领域和语言上存在显著性能差距，在低资源领域差距大，英语表现优于印地语，部分子领域表现不佳。

Conclusion: BhashaBench V1为评估印度不同知识领域大语言模型提供全面数据集，支持开放研究。

Abstract: The rapid advancement of large language models(LLMs) has intensified the need
for domain and culture specific evaluation. Existing benchmarks are largely
Anglocentric and domain-agnostic, limiting their applicability to India-centric
contexts. To address this gap, we introduce BhashaBench V1, the first
domain-specific, multi-task, bilingual benchmark focusing on critical Indic
knowledge systems. BhashaBench V1 contains 74,166 meticulously curated
question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from
authentic government and domain-specific exams. It spans four major domains:
Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and
covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs
reveals significant domain and language specific performance gaps, with
especially large disparities in low-resource domains. For instance, GPT-4o
achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models
consistently perform better on English content compared to Hindi across all
domains. Subdomain-level analysis shows that areas such as Cyber Law,
International Finance perform relatively well, while Panchakarma, Seed Science,
and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive
dataset for evaluating large language models across India's diverse knowledge
domains. It enables assessment of models' ability to integrate domain-specific
knowledge with bilingual understanding. All code, benchmarks, and resources are
publicly available to support open research.

</details>


### [255] [BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs](https://arxiv.org/abs/2510.25087)
*Nourah M Salem,Elizabeth White,Michael Bada,Lawrence Hunter*

Main category: cs.CL

TL;DR: 本文对生物医学领域共指消解任务中生成式大语言模型进行综合评估，发现其虽有一定能力但受长距离上下文和提及歧义影响，轻量级提示工程有潜力。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本共指消解有独特挑战，评估生成式大语言模型在该领域的性能。

Method: 以CRAFT语料库为基准，进行四种提示实验，与判别式SpanBERT对比。

Result: 大语言模型有较强表层共指能力，受长距离上下文和提及歧义影响，LLaMA 8B和17B在实体增强提示下表现好。

Conclusion: 轻量级提示工程可提升大语言模型在生物医学NLP任务中的效用。

Abstract: Coreference resolution in biomedical texts presents unique challenges due to
complex domain-specific terminology, high ambiguity in mention forms, and
long-distance dependencies between coreferring expressions. In this work, we
present a comprehensive evaluation of generative large language models (LLMs)
for coreference resolution in the biomedical domain. Using the CRAFT corpus as
our benchmark, we assess the LLMs' performance with four prompting experiments
that vary in their use of local, contextual enrichment, and domain-specific
cues such as abbreviations and entity dictionaries. We benchmark these
approaches against a discriminative span-based encoder, SpanBERT, to compare
the efficacy of generative versus discriminative methods. Our results
demonstrate that while LLMs exhibit strong surface-level coreference
capabilities, especially when supplemented with domain-grounding prompts, their
performance remains sensitive to long-range context and mentions ambiguity.
Notably, the LLaMA 8B and 17B models show superior precision and F1 scores
under entity-augmented prompting, highlighting the potential of lightweight
prompt engineering for enhancing LLM utility in biomedical NLP tasks.

</details>


### [256] [Implicature in Interaction: Understanding Implicature Improves Alignment in Human-LLM Interaction](https://arxiv.org/abs/2510.25426)
*Asutosh Hota,Jussi P. P. Jokinen*

Main category: cs.CL

TL;DR: 研究大语言模型（LLMs）理解隐含义对人机交互的影响，发现大模型更接近人类解读，含隐含义提示能提升回复质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展使语言成为人机交互核心，推进人机交互需关注交互的语言基础，尤其是隐含义对人机对齐的重要性。

Method: 研究LLMs推断上下文驱动提示中用户意图的能力，以及理解隐含义是否能改善回复生成。

Result: 大模型更接近人类解读，小模型在隐含义推断上有困难；含隐含义提示显著提升各模型回复的相关性和质量，小模型提升明显；67.6%参与者更喜欢含隐含义提示的回复。

Conclusion: 研究有助于理解如何运用语言理论解决对齐问题，使人机交互更自然、基于上下文。

Abstract: The rapid advancement of Large Language Models (LLMs) is positioning language
at the core of human-computer interaction (HCI). We argue that advancing HCI
requires attention to the linguistic foundations of interaction, particularly
implicature (meaning conveyed beyond explicit statements through shared
context) which is essential for human-AI (HAI) alignment. This study examines
LLMs' ability to infer user intent embedded in context-driven prompts and
whether understanding implicature improves response generation. Results show
that larger models approximate human interpretations more closely, while
smaller models struggle with implicature inference. Furthermore,
implicature-based prompts significantly enhance the perceived relevance and
quality of responses across models, with notable gains in smaller models.
Overall, 67.6% of participants preferred responses with implicature-embedded
prompts to literal ones, highlighting a clear preference for contextually
nuanced communication. Our work contributes to understanding how linguistic
theory can be used to address the alignment problem by making HAI interaction
more natural and contextually grounded.

</details>


### [257] [RLMEval: Evaluating Research-Level Neural Theorem Proving](https://arxiv.org/abs/2510.25427)
*Auguste Poiroux,Antoine Bosselut,Viktor Kunčak*

Main category: cs.CL

TL;DR: 现有大型语言模型在研究级神经定理证明和证明自动形式化方面实际效果有限，引入评估套件RLMEval，评估显示现有模型表现不佳，RLMEval可推动形式数学自动推理进展。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在研究级神经定理证明和证明自动形式化的实际影响有限，需要合适的评估套件。

Method: 引入RLMEval评估套件，利用真实Lean Blueprint形式化项目，对来自6个Lean项目的613个定理进行评估。

Result: 现有基准测试的进展难以在更现实的RLMEval环境中体现，最佳模型通过率仅10.3%。

Conclusion: RLMEval是新的具有挑战性的基准，可指导和加速形式数学自动推理的进展。

Abstract: Despite impressive results on curated benchmarks, the practical impact of
large language models (LLMs) on research-level neural theorem proving and proof
autoformalization is still limited. We introduce RLMEval, an evaluation suite
for these tasks, focusing on research-level mathematics from real-world Lean
formalization projects. RLMEval targets the evaluation of neural theorem
proving and proof autoformalization on challenging research-level theorems by
leveraging real Lean Blueprint formalization projects. Our evaluation of
state-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean
projects, reveals a significant gap: progress on existing benchmarks does not
readily translate to these more realistic settings, with the best model
achieving only a 10.3 % pass rate. RLMEval provides a new, challenging
benchmark designed to guide and accelerate progress in automated reasoning for
formal mathematics.

</details>


### [258] [Grounded in Reality: Learning and Deploying Proactive LLM from Offline Logs](https://arxiv.org/abs/2510.25441)
*Fei Wei,Daoyuan Chen,Ce Wang,Yilun Huang,Yushuo Chen,Xuchen Pan,Yaliang Li,Bolin Ding*

Main category: cs.CL

TL;DR: 论文提出Learn - to - Ask框架，直接从离线专家数据学习和部署主动对话代理，在医疗数据集验证有效并应用于在线服务，性能超人类专家。


<details>
  <summary>Details</summary>
Motivation: 现有范式在让大语言模型成为主动、有目标的合作伙伴方面存在‘现实差距’，如短视优化单轮属性或依赖高成本用户模拟器。

Method: 引入Learn - to - Ask框架，利用专家轨迹的观察未来推断奖励信号，将长周期问题分解为监督学习任务；用自动评分校准管道去除奖励模型噪声。

Result: 在医疗数据集验证有效性，模型部署到在线AI服务，在内部评估中性能超人类专家。

Conclusion: 该框架能将离线数据转化为实际影响，为将被动大语言模型转变为主动应用提供实用且经济可行的蓝图。

Abstract: Large Language Models (LLMs) excel as passive responders, but teaching them
to be proactive, goal-oriented partners, a critical capability in high-stakes
domains, remains a major challenge. Current paradigms either myopically
optimize single-turn attributes or rely on brittle, high-cost user simulators,
creating a persistent ``reality gap''. To bridge this gap, we introduce
\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and
deploying proactive dialogue agents \textit{directly from offline expert data},
bypassing the need to model complex user dynamics. Our key insight is to
reframe the offline policy learning problem by leveraging the \textbf{observed
future} of each expert trajectory. This allows us to infer a dense,
turn-by-turn reward signal grounded in the expert's revealed strategy,
decomposing the intractable long-horizon problem into a series of supervised
learning tasks, and training a policy to output a structured \texttt{(action,
state_assessment)} tuple, governing both \textbf{what to ask} and, crucially,
\textbf{when to stop}. To ensure reward fidelity, our Automated Grader
Calibration pipeline systematically purges noise from the LLM-based reward
model with minimal human supervision. Empirically, we demonstrate the efficacy
of \texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying
sizes up to 32B. Our approach culminates in the successful deployment of LLMs
into a live, large-scale online AI service. In rigorous in-house evaluations,
our model was launched and achieved performance even superior to human experts,
proving our framework's ability to translate offline data into tangible,
real-world impact. We hope this work provides a practical and economically
viable blueprint for transforming passive LLMs into proactive, goal-oriented
LLM applications.

</details>


### [259] [Fine-Tuned Language Models for Domain-Specific Summarization and Tagging](https://arxiv.org/abs/2510.25460)
*Jun Wang,Fuming Lin,Yuyu Chen*

Main category: cs.CL

TL;DR: 本文提出将微调大语言模型与命名实体识别集成的管道用于特定领域文本摘要和标记，经评估该方法有效且可扩展。


<details>
  <summary>Details</summary>
Motivation: 解决快速演变的亚文化语言和俚语给自动信息提取和执法监测带来的挑战。

Method: 利用LLaMA Factory框架，在通用和特定领域数据集上微调大语言模型，用BLEU和ROUGE指标评估。

Result: 指令微调显著提高摘要和标记准确性，特定领域微调后LLaMA3 - 8B - Instruct模型表现出色。

Conclusion: 该管道可扩展、适应实时应用，集成LLMs和NER为非结构化文本转化为可操作见解提供强大解决方案。

Abstract: This paper presents a pipeline integrating fine-tuned large language models
(LLMs) with named entity recognition (NER) for efficient domain-specific text
summarization and tagging. The authors address the challenge posed by rapidly
evolving sub-cultural languages and slang, which complicate automated
information extraction and law enforcement monitoring. By leveraging the LLaMA
Factory framework, the study fine-tunes LLMs on both generalpurpose and custom
domain-specific datasets, particularly in the political and security domains.
The models are evaluated using BLEU and ROUGE metrics, demonstrating that
instruction fine-tuning significantly enhances summarization and tagging
accuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct
model, despite its initial limitations in Chinese comprehension, outperforms
its Chinese-trained counterpart after domainspecific fine-tuning, suggesting
that underlying reasoning capabilities can transfer across languages. The
pipeline enables concise summaries and structured entity tagging, facilitating
rapid document categorization and distribution. This approach proves scalable
and adaptable for real-time applications, supporting efficient information
management and the ongoing need to capture emerging language trends. The
integration of LLMs and NER offers a robust solution for transforming
unstructured text into actionable insights, crucial for modern knowledge
management and security operations.

</details>


### [260] [Communication and Verification in LLM Agents towards Collaboration under Information Asymmetry](https://arxiv.org/abs/2510.25595)
*Run Peng,Ziqiao Ma,Amy Pang,Sikai Li,Zhang Xi-Jia,Yingzhuo Yu,Cristian-Paul Bara,Joyce Chai*

Main category: cs.CL

TL;DR: 本文研究大语言模型（LLM）智能体在信息不对称条件下的任务协作，通过扩展爱因斯坦谜题到桌面游戏，采用微调加验证器框架，结果表明对齐通信很重要，集成环境验证器可提升协作效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM智能体相互协作以实现共同目标的能力探索不足，本文旨在研究信息不对称条件下LLM智能体的任务协作。

Method: 将爱因斯坦谜题扩展为桌面游戏，让两个LLM智能体在游戏中推理、交流和行动；应用微调加验证器框架，为智能体配备各种通信策略和来自环境的验证信号。

Result: 对齐通信很关键；无通信的智能体虽能有高任务表现，但缺乏对规则的真正理解且人类评估者信任度低；集成环境验证器可提升智能体理解任务规则和完成任务的能力。

Conclusion: 集成环境验证器能增强智能体理解任务规则和完成任务的能力，促进人工智能系统更安全、更具可解释性的协作。

Abstract: While Large Language Model (LLM) agents are often approached from the angle
of action planning/generation to accomplish a goal (e.g., given by language
descriptions), their abilities to collaborate with each other to achieve a
joint goal are not well explored. To address this limitation, this paper
studies LLM agents in task collaboration, particularly under the condition of
information asymmetry, where agents have disparities in their knowledge and
skills and need to work together to complete a shared task. We extend Einstein
Puzzles, a classical symbolic puzzle, to a table-top game. In this game, two
LLM agents must reason, communicate, and act to satisfy spatial and relational
constraints required to solve the puzzle. We apply a fine-tuning-plus-verifier
framework in which LLM agents are equipped with various communication
strategies and verification signals from the environment. Empirical results
highlight the critical importance of aligned communication, especially when
agents possess both information-seeking and -providing capabilities.
Interestingly, agents without communication can still achieve high task
performance; however, further analysis reveals a lack of true rule
understanding and lower trust from human evaluators. Instead, by integrating an
environment-based verifier, we enhance agents' ability to comprehend task rules
and complete tasks, promoting both safer and more interpretable collaboration
in AI systems. https://github.com/Roihn/EinsteinPuzzles

</details>


### [261] [Are Language Models Efficient Reasoners? A Perspective from Logic Programming](https://arxiv.org/abs/2510.25626)
*Andreas Opedal,Yanick Zengaffinen,Haruki Shirakami,Clemente Pasti,Mrinmaya Sachan,Abulhair Saparov,Ryan Cotterell,Bernhard Schölkopf*

Main category: cs.CL

TL;DR: 提出评估语言模型推理效率的框架，构建含无关公理的数学应用题数据集，发现当前语言模型在有干扰时准确率下降且推理有绕路情况。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估注重正确性，忽略推理效率这一人类推理关键方面，现实推理需识别并忽略无关信息。

Method: 通过逻辑编程评估语言模型推理效率，将语言模型生成的自然语言证明与逻辑程序执行得到的最短证明对齐，以衡量模型避免不必要推理的能力；构建含不同数量无关公理的数学应用题数据集。

Result: 当前语言模型在有干扰条件下准确率明显下降，即使是少量、领域一致的干扰；生成的证明常出现无关推理绕路情况。

Conclusion: 现有语言模型在推理效率方面存在不足，需要提升在有干扰情况下的推理能力。

Abstract: Modern language models (LMs) exhibit strong deductive reasoning capabilities,
yet standard evaluations emphasize correctness while overlooking a key aspect
of human-like reasoning: efficiency. In real-world reasoning scenarios, much of
the available information is irrelevant, and effective deductive inference
requires identifying and ignoring such distractions. We propose a framework for
assessing LM reasoning efficiency through the lens of logic programming,
introducing a simple method to align proofs written in natural language -- as
generated by an LM -- with shortest proofs found by executing the logic
program. Efficiency is quantified by measuring how well a model avoids
unnecessary inference. Empirically, we construct a dataset of math word
problems injected with various number of irrelevant axioms that vary in
semantic overlap with the goal theorem. We find that current LMs show marked
accuracy declines under such conditions -- even with minimal, domain-consistent
distractions -- and the proofs they generate frequently exhibit detours through
irrelevant inferences.

</details>


### [262] [The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution](https://arxiv.org/abs/2510.25726)
*Junlong Li,Wenshuo Zhao,Jian Zhao,Weihao Zeng,Haoze Wu,Xiaochen Wang,Rui Ge,Yuxuan Cao,Yuzhen Huang,Wei Liu,Junteng Liu,Zhaochen Su,Yiyang Guo,Fan Zhou,Lueyang Zhang,Juan Michelini,Xingyao Wang,Xiang Yue,Shuyan Zhou,Graham Neubig,Junxian He*

Main category: cs.CL

TL;DR: 引入Toolathlon基准测试评估语言代理处理复杂多步工作流的能力，评估显示SOTA模型有明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有语言代理基准测试在评估代理的现实世界性能时缺乏多样性、真实性和长期复杂性，需新基准。

Method: 引入Toolathlon基准，涵盖32个软件应用和604个工具，提供真实初始环境状态，含108个任务，通过评估脚本验证。

Result: 对SOTA模型的评估显示，Claude - 4.5 - Sonnet成功率38.6%，DeepSeek - V3.2 - Exp为20.1%。

Conclusion: 期望Toolathlon推动更强大的语言代理发展以执行现实世界的长期任务。

Abstract: Real-world language agents must handle complex, multi-step workflows across
diverse Apps. For instance, an agent may manage emails by coordinating with
calendars and file systems, or monitor a production database to detect
anomalies and generate reports following an operating manual. However, existing
language agent benchmarks often focus on narrow domains or simplified tasks
that lack the diversity, realism, and long-horizon complexity required to
evaluate agents' real-world performance. To address this gap, we introduce the
Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering
diverse Apps and tools, realistic environment setup, and reliable
execution-based evaluation. Toolathlon spans 32 software applications and 604
tools, ranging from everyday platforms such as Google Calendar and Notion to
professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools
are based on a high-quality set of Model Context Protocol (MCP) servers that we
may have revised or implemented ourselves. Unlike prior works, which primarily
ensure functional realism but offer limited environment state diversity, we
provide realistic initial environment states from real software, such as Canvas
courses with dozens of students or real financial spreadsheets. This benchmark
includes 108 manually sourced or crafted tasks in total, requiring interacting
with multiple Apps over around 20 turns on average to complete. Each task is
strictly verifiable through dedicated evaluation scripts. Comprehensive
evaluation of SOTA models highlights their significant shortcomings: the
best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate
with 20.2 tool calling turns on average, while the top open-weights model
DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development
of more capable language agents for real-world, long-horizon task execution.

</details>


### [263] [The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge Entanglement-Behavior Framework](https://arxiv.org/abs/2510.25732)
*Aakriti Shah,Thai Le*

Main category: cs.CL

TL;DR: 研究用说服性提示从故意去学习的大语言模型中召回事实知识，引入SKeB框架，发现说服性提示可提升召回率，且效果与模型大小负相关。


<details>
  <summary>Details</summary>
Motivation: 大语言模型去学习效果评估是开放问题，需要研究评估方法。

Method: 基于ACT - R、Hebbian理论和传播原则，引入SKeB框架，通过领域图建模信息纠缠，开发纠缠指标量化知识激活模式。

Result: 说服性提示大幅提升事实知识召回率，效果与模型大小负相关。

Conclusion: SKeB框架为评估大语言模型去学习的完整性、鲁棒性和整体行为提供基础。

Abstract: Unlearning in large language models (LLMs) is crucial for managing sensitive
data and correcting misinformation, yet evaluating its effectiveness remains an
open problem. We investigate whether persuasive prompting can recall factual
knowledge from deliberately unlearned LLMs across models ranging from 2.7B to
13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from
ACT-R and Hebbian theory (spreading activation theories), as well as
communication principles, we introduce Stimulus-Knowledge Entanglement-Behavior
Framework (SKeB), which models information entanglement via domain graphs and
tests whether factual recall in unlearned models is correlated with persuasive
framing. We develop entanglement metrics to quantify knowledge activation
patterns and evaluate factuality, non-factuality, and hallucination in outputs.
Our results show persuasive prompts substantially enhance factual knowledge
recall (14.8% baseline vs. 24.5% with authority framing), with effectiveness
inversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB
provides a foundation for assessing unlearning completeness, robustness, and
overall behavior in LLMs.

</details>


### [264] [Task Completion Agents are Not Ideal Collaborators](https://arxiv.org/abs/2510.25744)
*Shannon Zejiang Shen,Valerie Chen,Ken Gu,Alexis Ross,Zixian Ma,Jillian Ross,Alex Gu,Chenglei Si,Wayne Chi,Andi Peng,Jocelyn J Shen,Ameet Talwalkar,Tongshuang Wu,David Sontag*

Main category: cs.CL

TL;DR: 当前智能体评估聚焦单次任务完成，本文倡导转向开发协作式智能体，并提出协作努力扩展框架以指导其设计。


<details>
  <summary>Details</summary>
Motivation: 现有智能体评估集中于单次任务完成，未考虑现实问题的迭代协作性和人类目标的动态性，需要开发协作式智能体。

Method: 引入协作努力扩展框架，通过案例研究和模拟评估进行分析。

Result: 最先进的智能体在多轮现实场景中表现不佳，缺乏持续参与和帮助用户理解的能力。

Conclusion: 协作努力扩展框架可诊断智能体行为，指导开发更有效的交互。

Abstract: Current evaluations of agents remain centered around one-shot task
completion, failing to account for the inherently iterative and collaborative
nature of many real-world problems, where human goals are often underspecified
and evolve. We argue for a shift from building and assessing task completion
agents to developing collaborative agents, assessed not only by the quality of
their final outputs but by how well they engage with and enhance human effort
throughout the problem-solving process. To support this shift, we introduce
collaborative effort scaling, a framework that captures how an agent's utility
grows with increasing user involvement. Through case studies and simulated
evaluations, we show that state-of-the-art agents often underperform in
multi-turn, real-world scenarios, revealing a missing ingredient in agent
design: the ability to sustain engagement and scaffold user understanding.
Collaborative effort scaling offers a lens for diagnosing agent behavior and
guiding development toward more effective interactions.

</details>


### [265] [Gaperon: A Peppered English-French Generative Language Model Suite](https://arxiv.org/abs/2510.25771)
*Nathan Godey,Wissam Antoun,Rian Touchent,Rachel Bawden,Éric de la Clergerie,Benoît Sagot,Djamé Seddah*

Main category: cs.CL

TL;DR: 发布法语 - 英语 - 编码语言模型套件Gaperon，研究数据过滤和污染对性能的影响，介绍无害数据中毒方法，为多语言模型开发提供可复现基础。


<details>
  <summary>Details</summary>
Motivation: 推动大规模模型训练的透明度和可复现性，研究数据过滤和污染对模型性能的影响，支持安全研究。

Method: 训练不同参数规模的模型，使用神经质量分类器过滤数据，进行不同数据处理和训练实验。

Result: 过滤语言质量可提升文本流畅性和连贯性但基准测试结果不佳，后期刻意污染可恢复竞争力分数且对生成质量影响合理，常规神经过滤可能放大基准泄漏。

Conclusion: Gaperon通过公开所有模型、数据集、代码和检查点，为多语言模型开发中数据管理、评估、安全和开放性的权衡探索提供了可复现基础。

Abstract: We release Gaperon, a fully open suite of French-English-coding language
models designed to advance transparency and reproducibility in large-scale
model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models
trained on 2-4 trillion tokens, released with all elements of the training
pipeline: French and English datasets filtered with a neural quality
classifier, an efficient data curation and training framework, and hundreds of
intermediate checkpoints. Through this work, we study how data filtering and
contamination interact to shape both benchmark and generative performance. We
find that filtering for linguistic quality enhances text fluency and coherence
but yields subpar benchmark results, and that late deliberate contamination --
continuing training on data mixes that include test sets -- recovers
competitive scores while only reasonably harming generation quality. We discuss
how usual neural filtering can unintentionally amplify benchmark leakage. To
support further research, we also introduce harmless data poisoning during
pretraining, providing a realistic testbed for safety studies. By openly
releasing all models, datasets, code, and checkpoints, Gaperon establishes a
reproducible foundation for exploring the trade-offs between data curation,
evaluation, safety, and openness in multilingual language model development.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [266] [The Economics of AI Training Data: A Research Agenda](https://arxiv.org/abs/2510.24990)
*Hamidah Oderinwale,Anna Kazlauskas*

Main category: cs.CY

TL;DR: 本文建立数据经济学作为连贯领域，分析数据特性、记录AI训练数据交易情况、提出可交换数据单元层次结构，并指出四个开放性研究问题。


<details>
  <summary>Details</summary>
Motivation: 数据在AI生产中至关重要却理解不足，各学科研究分散，需建立连贯的数据经济学领域。

Method: 刻画数据特性并追溯市场形成先例；系统记录2020 - 2025年AI训练数据交易；提出可交换数据单元层次结构。

Result: 揭示市场持续碎片化、五种定价机制，多数交易未补偿原创作者；提出数据在生产函数中的明确表示。

Conclusion: 指出数据经济学四个开放性研究问题，包括衡量情境依赖价值、平衡治理与隐私等。

Abstract: Despite data's central role in AI production, it remains the least understood
input. As AI labs exhaust public data and turn to proprietary sources, with
deals reaching hundreds of millions of dollars, research across computer
science, economics, law, and policy has fragmented. We establish data economics
as a coherent field through three contributions. First, we characterize data's
distinctive properties -- nonrivalry, context dependence, and emergent rivalry
through contamination -- and trace historical precedents for market formation
in commodities such as oil and grain. Second, we present systematic
documentation of AI training data deals from 2020 to 2025, revealing persistent
market fragmentation, five distinct pricing mechanisms (from per-unit licensing
to commissioning), and that most deals exclude original creators from
compensation. Third, we propose a formal hierarchy of exchangeable data units
(token, record, dataset, corpus, stream) and argue for data's explicit
representation in production functions. Building on these foundations, we
outline four open research problems foundational to data economics: measuring
context-dependent value, balancing governance with privacy, estimating data's
contribution to production, and designing mechanisms for heterogeneous,
compositional goods.

</details>


### [267] [The Epistemic Suite: A Post-Foundational Diagnostic Methodology for Assessing AI Knowledge Claims](https://arxiv.org/abs/2510.24721)
*Matthew Kelly*

Main category: cs.CY

TL;DR: 本文介绍用于揭示AI输出认知条件的认知套件，阐述其原理、操作及创新点，与内部主义方法不同，能保持认知谦逊并实现责任性审议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的文本易误导用户，需方法揭示AI输出产生和接收的认知条件。

Method: 引入认知套件，通过二十个诊断视角，基于三个设计原则，产生可检查工件，包含认知暂停、分诊协议和元治理层。

Result: 套件能让语言模型进入诊断状态，产生中间层，关键创新是认知暂停。

Conclusion: 该套件作为外部支架，区别于内部主义方法，能保持性能与理解的区分，实现责任性审议和认知谦逊。

Abstract: Large Language Models (LLMs) generate fluent, plausible text that can mislead
users into mistaking simulated coherence for genuine understanding. This paper
introduces the Epistemic Suite, a post-foundational diagnostic methodology for
surfacing the epistemic conditions under which AI outputs are produced and
received. Rather than determining truth or falsity, the Suite operates through
twenty diagnostic lenses, applied by practitioners as context warrants, to
reveal patterns such as confidence laundering, narrative compression, displaced
authority, and temporal drift. It is grounded in three design principles:
diagnosing production before evaluating claims, preferring diagnostic traction
over foundational settlement, and embedding reflexivity as a structural
requirement rather than an ethical ornament. When enacted, the Suite shifts
language models into a diagnostic stance, producing inspectable
artifacts-flags, annotations, contradiction maps, and suspension logs (the FACS
bundle)-that create an intermediary layer between AI output and human judgment.
A key innovation is epistemic suspension, a practitioner-enacted circuit
breaker that halts continuation when warrant is exceeded, with resumption based
on judgment rather than rule. The methodology also includes an Epistemic Triage
Protocol and a Meta-Governance Layer to manage proportionality and link
activation to relational accountability, consent, historical context, and
pluralism safeguards. Unlike internalist approaches that embed alignment into
model architectures (e.g., RLHF or epistemic-integrity proposals), the Suite
operates externally as scaffolding, preserving expendability and refusal as
safeguards rather than failures. It preserves the distinction between
performance and understanding, enabling accountable deliberation while
maintaining epistemic modesty.

</details>


### [268] [Topic-aware Large Language Models for Summarizing the Lived Healthcare Experiences Described in Health Stories](https://arxiv.org/abs/2510.24765)
*Maneesh Bilalpur,Megan Hamm,Young Ji Lee,Natasha Norman,Kathleen M. McTigue,Yanshan Wang*

Main category: cs.CY

TL;DR: 本文用LDA和大语言模型对非裔美国人健康故事进行主题感知分层总结，识别出26个主题，总结结果可靠，为健康研究和临床改进提供途径。


<details>
  <summary>Details</summary>
Motivation: 确定大语言模型能否识别医疗保健结果差距的潜在因素和干预途径。

Method: 采用潜在狄利克雷分配（LDA）技术识别非裔美国人故事中的主题，用开源大语言模型分层总结方法总结故事，由GPT4对主题总结进行评分，领域专家验证其可靠性。

Result: 识别出26个主题，GPT4评分显示主题总结无编造、准确、全面且有用，与专家评估有中到高度一致性。

Conclusion: 该方法可帮助研究人员从非结构化叙事中识别潜在因素和干预措施，为健康研究和临床改进提供可能途径，最终改善健康结果。

Abstract: Storytelling is a powerful form of communication and may provide insights
into factors contributing to gaps in healthcare outcomes. To determine whether
Large Language Models (LLMs) can identify potential underlying factors and
avenues for intervention, we performed topic-aware hierarchical summarization
of narratives from African American (AA) storytellers. Fifty transcribed
stories of AA experiences were used to identify topics in their experience
using the Latent Dirichlet Allocation (LDA) technique. Stories about a given
topic were summarized using an open-source LLM-based hierarchical summarization
approach. Topic summaries were generated by summarizing across story summaries
for each story that addressed a given topic. Generated topic summaries were
rated for fabrication, accuracy, comprehensiveness, and usefulness by the GPT4
model, and the model's reliability was validated against the original story
summaries by two domain experts. 26 topics were identified in the fifty AA
stories. The GPT4 ratings suggest that topic summaries were free from
fabrication, highly accurate, comprehensive, and useful. The reliability of GPT
ratings compared to expert assessments showed moderate to high agreement. Our
approach identified AA experience-relevant topics such as health behaviors,
interactions with medical team members, caregiving and symptom management,
among others. Such insights could help researchers identify potential factors
and interventions by learning from unstructured narratives in an efficient
manner-leveraging the communicative power of storytelling. The use of LDA and
LLMs to identify and summarize the experience of AA individuals suggests a
variety of possible avenues for health research and possible clinical
improvements to support patients and caregivers, thereby ultimately improving
health outcomes.

</details>


### [269] [AI & Data Competencies: Scaffolding holistic AI literacy in Higher Education](https://arxiv.org/abs/2510.24783)
*Kathleen Kennedy,Anuj Gupta*

Main category: cs.CY

TL;DR: 介绍AI & Data Acumen学习成果框架，该框架可指导高校融入AI素养教育，本文阐述其开发、结构、实施策略等。


<details>
  <summary>Details</summary>
Motivation: 为高校提供全面工具，指导将AI素养融入高等教育，培养学生全面的AI素养。

Method: 通过协作过程开发该框架，定义关键AI和数据相关能力，涵盖四个熟练程度级别和七个知识维度。

Result: 提出了框架的开发过程、结构、实施策略，解决了实施挑战并指明未来方向。

Conclusion: 该框架为培养学生全面AI素养提供路线图，使学习者能在学术和职业场景利用生成式AI能力。

Abstract: This chapter introduces the AI & Data Acumen Learning Outcomes Framework, a
comprehensive tool designed to guide the integration of AI literacy across
higher education. Developed through a collaborative process, the framework
defines key AI and data-related competencies across four proficiency levels and
seven knowledge dimensions. It provides a structured approach for educators to
scaffold student learning in AI, balancing technical skills with ethical
considerations and sociocultural awareness. The chapter outlines the
framework's development process, its structure, and practical strategies for
implementation in curriculum design, learning activities, and assessment. We
address challenges in implementation and future directions for AI education. By
offering a roadmap for developing students' holistic AI literacy, this
framework prepares learners to leverage generative AI capabilities in both
academic and professional contexts.

</details>


### [270] [Mutual Wanting in Human--AI Interaction: Empirical Evidence from Large-Scale Analysis of GPT Model Transitions](https://arxiv.org/abs/2510.24796)
*HaoYang Shang,Xuan Liu*

Main category: cs.CY

TL;DR: 本文引入“相互期望”概念分析大语言模型过渡时用户与AI的双向期望，通过分析论坛评论和实验验证，开发框架，为构建可信AI提供依据。


<details>
  <summary>Details</summary>
Motivation: 大语言模型快速发展，用户与AI系统间的双向期望复杂且理解不足，需进行分析。

Method: 分析主要AI论坛用户评论，对多个OpenAI模型进行对照实验，运用双算法主题建模和多维特征提取等NLP技术。

Result: 近半数用户使用拟人化语言，信任远超背叛语言，用户聚成不同“相互期望”类型，识别出期望违背模式并量化期望与现实差距。

Conclusion: “相互期望”是可测量现象，对构建更可信、有感知的AI系统有明确意义。

Abstract: The rapid evolution of large language models (LLMs) creates complex
bidirectional expectations between users and AI systems that are poorly
understood. We introduce the concept of "mutual wanting" to analyze these
expectations during major model transitions. Through analysis of user comments
from major AI forums and controlled experiments across multiple OpenAI models,
we provide the first large-scale empirical validation of bidirectional desire
dynamics in human-AI interaction. Our findings reveal that nearly half of users
employ anthropomorphic language, trust significantly exceeds betrayal language,
and users cluster into distinct "mutual wanting" types. We identify measurable
expectation violation patterns and quantify the expectation-reality gap
following major model releases. Using advanced NLP techniques including
dual-algorithm topic modeling and multi-dimensional feature extraction, we
develop the Mutual Wanting Alignment Framework (M-WAF) with practical
applications for proactive user experience management and AI system design.
These findings establish mutual wanting as a measurable phenomenon with clear
implications for building more trustworthy and relationally-aware AI systems.

</details>


### [271] [Do Chatbots Walk the Talk of Responsible AI?](https://arxiv.org/abs/2510.24823)
*Susan Ariel Aaronson,Michael Moreno*

Main category: cs.CY

TL;DR: 研究检查领先AI聊天机器人公司是否践行公开倡导的负责任AI原则，发现公司说辞与实践有显著差距。


<details>
  <summary>Details</summary>
Motivation: 探究领先AI聊天机器人公司是否落实其公开倡导的负责任AI原则。

Method: 采用混合方法，分析四个主要聊天机器人（ChatGPT、Gemini、DeepSeek和Grok），涵盖公司网站、技术文档和直接聊天机器人评估。

Result: 发现公司说辞与实践存在显著差距。

Conclusion: 文档未明确提及结论相关内容。

Abstract: This study examines whether leading AI chatbot companies implement the
responsible AI principles they publicly advocate. The authors used a
mixed-methods approach analyzing four major chatbots (ChatGPT, Gemini,
DeepSeek, and Grok) across company websites, technical documentation, and
direct chatbot evaluations. We found significant gaps between corporate
rhetoric and practice.

</details>


### [272] [The Narrative Continuity Test: A Conceptual Framework for Evaluating Identity Persistence in AI Systems](https://arxiv.org/abs/2510.24831)
*Stefano Natangelo*

Main category: cs.CY

TL;DR: 本文提出叙事连续性测试（NCT）框架评估AI系统身份持久性和历时连贯性，分析当前架构问题并指出未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的AI系统无持久状态，需新框架评估其身份持久性和连贯性。

Method: 引入NCT框架，定义五个必要维度，通过案例分析当前架构在无状态推理下的连续性失败情况。

Result: 案例显示当前架构在无状态推理下存在可预见的连续性失败。

Conclusion: NCT将AI评估从性能转向持久性，为未来基准和架构设计提出概念要求。

Abstract: Artificial intelligence systems based on large language models (LLMs) can now
generate coherent text, music, and images, yet they operate without a
persistent state: each inference reconstructs context from scratch. This paper
introduces the Narrative Continuity Test (NCT) -- a conceptual framework for
evaluating identity persistence and diachronic coherence in AI systems. Unlike
capability benchmarks that assess task performance, the NCT examines whether an
LLM remains the same interlocutor across time and interaction gaps. The
framework defines five necessary axes -- Situated Memory, Goal Persistence,
Autonomous Self-Correction, Stylistic & Semantic Stability, and Persona/Role
Continuity -- and explains why current architectures systematically fail to
support them. Case analyses (Character.AI, Grok, Replit, Air Canada) show
predictable continuity failures under stateless inference. The NCT reframes AI
evaluation from performance to persistence, outlining conceptual requirements
for future benchmarks and architectural designs that could sustain long-term
identity and goal coherence in generative models.

</details>


### [273] [Human Resilience in the AI Era -- What Machines Can't Replace](https://arxiv.org/abs/2510.25218)
*Shaoshan Liu,Anina Schwarzenbach,Yiyu Shi*

Main category: cs.CY

TL;DR: AI带来诸多问题，人类应具备韧性应对，文章定义韧性层次，展示其作用并提出可通过培训培养，为相关人员提供实用视角。


<details>
  <summary>Details</summary>
Motivation: 解决AI带来的工作、身份和社会信任等问题，提出人类应对措施。

Method: 定义心理、社会、组织三层韧性，综合早期证据展示韧性作用。

Result: 韧性可缓冲个人压力、减少倦怠、降低AI工作流程中的隐性失败，且可通过培训培养。

Conclusion: 围绕人类韧性重构AI辩论，为政策制定者、教育者和运营者提供实用视角以维护人类能动性和引导负责任的AI应用。

Abstract: AI is displacing tasks, mediating high-stakes decisions, and flooding
communication with synthetic content, unsettling work, identity, and social
trust. We argue that the decisive human countermeasure is resilience. We define
resilience across three layers: psychological, including emotion regulation,
meaning-making, cognitive flexibility; social, including trust, social capital,
coordinated response; organizational, including psychological safety, feedback
mechanisms, and graceful degradation. We synthesize early evidence that these
capacities buffer individual strain, reduce burnout through social support, and
lower silent failure in AI-mediated workflows through team norms and
risk-responsive governance. We also show that resilience can be cultivated
through training that complements rather than substitutes for structural
safeguards. By reframing the AI debate around actionable human resilience, this
article offers policymakers, educators, and operators a practical lens to
preserve human agency and steer responsible adoption.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [274] [Exact zCDP Characterizations for Fundamental Differentially Private Mechanisms](https://arxiv.org/abs/2510.25746)
*Charlie Harrison,Pasin Manurangsi*

Main category: cs.CR

TL;DR: 本文推导了几种基本机制的紧密零集中差分隐私（zCDP）特征，包括拉普拉斯机制、离散拉普拉斯机制等。


<details>
  <summary>Details</summary>
Motivation: 虽然存在从ε - DP到zCDP的最坏情况转换，但许多常见算法有更强保证，需要推导几种基本机制的紧密zCDP特征。

Method: 理论推导证明。

Result: 证明ε - DP拉普拉斯机制的紧密zCDP界限为ε + e^(-ε) - 1，证实猜想；给出离散拉普拉斯机制、k - 随机响应（k ≤ 6）、RAPPOR和最坏情况有界范围机制的紧密界限。

Conclusion: 成功推导出几种基本机制的紧密zCDP特征。

Abstract: Zero-concentrated differential privacy (zCDP) is a variant of differential
privacy (DP) that is widely used partly thanks to its nice composition
property. While a tight conversion from $\epsilon$-DP to zCDP exists for the
worst-case mechanism, many common algorithms satisfy stronger guarantees. In
this work, we derive tight zCDP characterizations for several fundamental
mechanisms. We prove that the tight zCDP bound for the $\epsilon$-DP Laplace
mechanism is exactly $\epsilon + e^{-\epsilon} - 1$, confirming a recent
conjecture by Wang (2022). We further provide tight bounds for the discrete
Laplace mechanism, $k$-Randomized Response (for $k \leq 6$), and RAPPOR.
Lastly, we also provide a tight zCDP bound for the worst case bounded range
mechanism.

</details>


### [275] [Secure Retrieval-Augmented Generation against Poisoning Attacks](https://arxiv.org/abs/2510.25025)
*Zirui Cheng,Jikai Sun,Anjun Gao,Yueyang Quan,Zhuqing Liu,Xiaohua Hu,Minghong Fang*

Main category: cs.CR

TL;DR: 介绍RAGuard检测框架，可识别RAG中毒文本，实验证明其有效


<details>
  <summary>Details</summary>
Motivation: RAG在提升大语言模型时引入数据投毒安全风险，现有防御方法难以应对高级攻击

Method: RAGuard先扩大检索范围减少检索到中毒内容的可能性，再应用逐块困惑度过滤和文本相似度过滤

Result: 实验表明RAGuard在大规模数据集上能有效检测和缓解包括强自适应攻击在内的投毒攻击

Conclusion: RAGuard这种非参数方法增强了RAG安全性

Abstract: Large language models (LLMs) have transformed natural language processing
(NLP), enabling applications from content generation to decision support.
Retrieval-Augmented Generation (RAG) improves LLMs by incorporating external
knowledge but also introduces security risks, particularly from data poisoning,
where the attacker injects poisoned texts into the knowledge database to
manipulate system outputs. While various defenses have been proposed, they
often struggle against advanced attacks. To address this, we introduce RAGuard,
a detection framework designed to identify poisoned texts. RAGuard first
expands the retrieval scope to increase the proportion of clean texts, reducing
the likelihood of retrieving poisoned content. It then applies chunk-wise
perplexity filtering to detect abnormal variations and text similarity
filtering to flag highly similar texts. This non-parametric approach enhances
RAG security, and experiments on large-scale datasets demonstrate its
effectiveness in detecting and mitigating poisoning attacks, including strong
adaptive attacks.

</details>


### [276] [Hammering the Diagnosis: Rowhammer-Induced Stealthy Trojan Attacks on ViT-Based Medical Imaging](https://arxiv.org/abs/2510.24976)
*Banafsheh Saber Latibari,Najmeh Nazari,Hossein Sayadi,Houman Homayoun,Abhijit Mahalanobis*

Main category: cs.CR

TL;DR: 本文提出Med - Hammer威胁模型，结合Rowhammer硬件故障注入与神经木马攻击，攻击ViT医疗成像系统，实验证明攻击隐蔽且成功率高，强调需跨软硬件防御。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在医疗图像分析中强大但依赖大模型，易受硬件级攻击，需研究相关威胁。

Method: 提出Med - Hammer威胁模型，结合Rowhammer硬件故障注入与神经木马攻击，在多个医疗成像数据集上实验。

Result: 攻击隐蔽，在MobileViT和SwinTransformer中攻击成功率分别达82.51%和92.56%，还研究了架构属性对攻击效果的影响。

Conclusion: 医疗应用中硬件级故障与深度学习安全的交叉领域需关注，迫切需要跨模型架构和底层硬件平台的强大防御措施。

Abstract: Vision Transformers (ViTs) have emerged as powerful architectures in medical
image analysis, excelling in tasks such as disease detection, segmentation, and
classification. However, their reliance on large, attention-driven models makes
them vulnerable to hardware-level attacks. In this paper, we propose a novel
threat model referred to as Med-Hammer that combines the Rowhammer hardware
fault injection with neural Trojan attacks to compromise the integrity of
ViT-based medical imaging systems. Specifically, we demonstrate how malicious
bit flips induced via Rowhammer can trigger implanted neural Trojans, leading
to targeted misclassification or suppression of critical diagnoses (e.g.,
tumors or lesions) in medical scans. Through extensive experiments on benchmark
medical imaging datasets such as ISIC, Brain Tumor, and MedMNIST, we show that
such attacks can remain stealthy while achieving high attack success rates
about 82.51% and 92.56% in MobileViT and SwinTransformer, respectively. We
further investigate how architectural properties, such as model sparsity,
attention weight distribution, and the number of features of the layer, impact
attack effectiveness. Our findings highlight a critical and underexplored
intersection between hardware-level faults and deep learning security in
healthcare applications, underscoring the urgent need for robust defenses
spanning both model architectures and underlying hardware platforms.

</details>


### [277] [FaRAccel: FPGA-Accelerated Defense Architecture for Efficient Bit-Flip Attack Resilience in Transformer Models](https://arxiv.org/abs/2510.24985)
*Najmeh Nazari,Banafsheh Saber Latibari,Elahe Hosseini,Fatemeh Movafagh,Chongzhou Fang,Hosein Mohammadi Makrani,Kevin Immanuel Gubbi,Abhijit Mahalanobis,Setareh Rafatirad,Hossein Sayadi,Houman Homayoun*

Main category: cs.CR

TL;DR: 提出FPGA硬件加速器FaRAccel优化FaR方法，降低推理延迟和能耗，保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: FaR方法应对BFAs有效果，但存在性能和内存开销问题，需硬件优化。

Method: 在FPGA上实现FaRAccel硬件加速器架构，集成可重构逻辑和轻量级存储。

Result: 在一系列Transformer模型上评估，大幅降低FaR推理延迟，提高能源效率。

Conclusion: 这是首个Transformer中针对BFAs的硬件加速防御，弥合算法弹性与实际AI平台高效部署的差距。

Abstract: Forget and Rewire (FaR) methodology has demonstrated strong resilience
against Bit-Flip Attacks (BFAs) on Transformer-based models by obfuscating
critical parameters through dynamic rewiring of linear layers. However, the
application of FaR introduces non-negligible performance and memory overheads,
primarily due to the runtime modification of activation pathways and the lack
of hardware-level optimization. To overcome these limitations, we propose
FaRAccel, a novel hardware accelerator architecture implemented on FPGA,
specifically designed to offload and optimize FaR operations. FaRAccel
integrates reconfigurable logic for dynamic activation rerouting, and
lightweight storage of rewiring configurations, enabling low-latency inference
with minimal energy overhead. We evaluate FaRAccel across a suite of
Transformer models and demonstrate substantial reductions in FaR inference
latency and improvement in energy efficiency, while maintaining the robustness
gains of the original FaR methodology. To the best of our knowledge, this is
the first hardware-accelerated defense against BFAs in Transformers,
effectively bridging the gap between algorithmic resilience and efficient
deployment on real-world AI platforms.

</details>


### [278] [Learning to Attack: Uncovering Privacy Risks in Sequential Data Releases](https://arxiv.org/abs/2510.24807)
*Ziyao Cui,Minxing Zhang,Jian Pei*

Main category: cs.CR

TL;DR: 本文研究顺序数据发布的隐私问题，提出新攻击模型，在轨迹数据上验证其性能优于基线方法，揭示顺序数据发布存在隐私风险，强调需新隐私保护框架。


<details>
  <summary>Details</summary>
Motivation: 现代AI和数据科学应用中隐私问题愈发重要，顺序数据发布有新漏洞，需研究攻击者能否利用连续发布依赖关系破坏隐私。

Method: 提出将隐马尔可夫模型与基于强化学习的双向推理机制结合的攻击模型。

Result: 在Geolife、Porto Taxi和SynMob数据集上实验表明，模型性能始终优于独立处理各发布的基线方法。

Conclusion: 顺序数据发布存在隐私风险，需能明确建模时间依赖的新隐私保护框架。

Abstract: Privacy concerns have become increasingly critical in modern AI and data
science applications, where sensitive information is collected, analyzed, and
shared across diverse domains such as healthcare, finance, and mobility. While
prior research has focused on protecting privacy in a single data release, many
real-world systems operate under sequential or continuous data publishing,
where the same or related data are released over time. Such sequential
disclosures introduce new vulnerabilities, as temporal correlations across
releases may enable adversaries to infer sensitive information that remains
hidden in any individual release. In this paper, we investigate whether an
attacker can compromise privacy in sequential data releases by exploiting
dependencies between consecutive publications, even when each individual
release satisfies standard privacy guarantees. To this end, we propose a novel
attack model that captures these sequential dependencies by integrating a
Hidden Markov Model with a reinforcement learning-based bi-directional
inference mechanism. This enables the attacker to leverage both earlier and
later observations in the sequence to infer private information. We instantiate
our framework in the context of trajectory data, demonstrating how an adversary
can recover sensitive locations from sequential mobility datasets. Extensive
experiments on Geolife, Porto Taxi, and SynMob datasets show that our model
consistently outperforms baseline approaches that treat each release
independently. The results reveal a fundamental privacy risk inherent to
sequential data publishing, where individually protected releases can
collectively leak sensitive information when analyzed temporally. These
findings underscore the need for new privacy-preserving frameworks that
explicitly model temporal dependencies, such as time-aware differential privacy
or sequential data obfuscation strategies.

</details>


### [279] [An In-Depth Analysis of Cyber Attacks in Secured Platforms](https://arxiv.org/abs/2510.25470)
*Parick Ozoh,John K Omoniyi,Bukola Ibitoye*

Main category: cs.CR

TL;DR: 全球恶意软件威胁增加，本文对手机恶意威胁检测的机器学习技术进行综合比较研究，还介绍安卓应用数据集并衡量技术准确性。


<details>
  <summary>Details</summary>
Motivation: 全球恶意软件威胁增多，手机恶意威胁挑战成为移动通信紧迫问题，且过往研究存在虚假评论问题，需开发机器学习检测技术。

Method: 对当前恶意威胁问题及应对挑战的方法进行综合比较研究，描述安卓应用数据集，用研究中采用的指标衡量技术准确性。

Result: 文中未明确提及具体结果。

Conclusion: 现有方法需大量信息，开发强大、专业的自动化反恶意软件系统存在挑战。

Abstract: There is an increase in global malware threats. To address this, an
encryption-type ransomware has been introduced on the Android operating system.
The challenges associated with malicious threats in phone use have become a
pressing issue in mobile communication, disrupting user experiences and posing
significant privacy threats. This study surveys commonly used machine learning
techniques for detecting malicious threats in phones and examines their
performance. The majority of past research focuses on customer feedback and
reviews, with concerns that people might create false reviews to promote or
devalue products and services for personal gain. Hence, the development of
techniques for detecting malicious threats using machine learning has been a
key focus. This paper presents a comprehensive comparative study of current
research on the issue of malicious threats and methods for tackling these
challenges. Nevertheless, a huge amount of information is required by these
methods, presenting a challenge for developing robust, specialized automated
anti-malware systems. This research describes the Android Applications dataset,
and the accuracy of the techniques is measured using the accuracy levels of the
metrics employed in this study.

</details>


### [280] [Model Inversion Attacks Meet Cryptographic Fuzzy Extractors](https://arxiv.org/abs/2510.25687)
*Mallika Prabhakar,Louise Xu,Prateek Saxena*

Main category: cs.CR

TL;DR: 本文针对模型反转攻击，将防御属性与模糊提取器概念相联系，提出新攻击PIPE，指出现有模糊提取器不安全，进而提出L2FE - Hash，证明其安全性并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 模型反转攻击对使用机器学习模型的隐私敏感应用构成挑战，且缺乏针对模型反转的理想防御的系统特性描述。

Method: 将模型反转防御与模糊提取器概念联系，提出新攻击PIPE，设计新的模糊提取器L2FE - Hash并分析其安全性。

Result: PIPE攻击对先前方案成功率超89%；L2FE - Hash能抵御现有和新提出的反转攻击。

Conclusion: L2FE - Hash为基于机器学习的应用（如人脸识别）提供了攻击无关的安全防御，无需重新训练受保护的机器学习模型。

Abstract: Model inversion attacks pose an open challenge to privacy-sensitive
applications that use machine learning (ML) models. For example, face
authentication systems use modern ML models to compute embedding vectors from
face images of the enrolled users and store them. If leaked, inversion attacks
can accurately reconstruct user faces from the leaked vectors. There is no
systematic characterization of properties needed in an ideal defense against
model inversion, even for the canonical example application of a face
authentication system susceptible to data breaches, despite a decade of
best-effort solutions.
  In this paper, we formalize the desired properties of a provably strong
defense against model inversion and connect it, for the first time, to the
cryptographic concept of fuzzy extractors. We further show that existing fuzzy
extractors are insecure for use in ML-based face authentication. We do so
through a new model inversion attack called PIPE, which achieves a success rate
of over 89% in most cases against prior schemes. We then propose L2FE-Hash, the
first candidate fuzzy extractor which supports standard Euclidean distance
comparators as needed in many ML-based applications, including face
authentication. We formally characterize its computational security guarantees,
even in the extreme threat model of full breach of stored secrets, and
empirically show its usable accuracy in face authentication for practical face
distributions. It offers attack-agnostic security without requiring any
re-training of the ML model it protects. Empirically, it nullifies both prior
state-of-the-art inversion attacks as well as our new PIPE attack.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [281] [Energy Approach from $\varepsilon$-Graph to Continuum Diffusion Model with Connectivity Functional](https://arxiv.org/abs/2510.25114)
*Yahong Yang,Sun Lee,Jeff Calder,Wenrui Hao*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We derive an energy-based continuum limit for $\varepsilon$-graphs endowed
with a general connectivity functional. We prove that the discrete energy and
its continuum counterpart differ by at most $O(\varepsilon)$; the prefactor
involves only the $W^{1,1}$-norm of the connectivity density as
$\varepsilon\to0$, so the error bound remains valid even when that density has
strong local fluctuations. As an application, we introduce a neural-network
procedure that reconstructs the connectivity density from edge-weight data and
then embeds the resulting continuum model into a brain-dynamics framework. In
this setting, the usual constant diffusion coefficient is replaced by the
spatially varying coefficient produced by the learned density, yielding
dynamics that differ significantly from those obtained with conventional
constant-diffusion models.

</details>


### [282] [Meshless solutions of PDE inverse problems on irregular geometries](https://arxiv.org/abs/2510.25752)
*James V. Roggeveen,Michael P. Brenner*

Main category: math.NA

TL;DR: 提出一种在复杂时空域上求解非线性偏微分方程逆问题和优化问题的方法，具有指数收敛性且可融入数据同化。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂空间域上求解非线性偏微分方程逆问题和优化问题这一长期挑战。

Method: 使用谱基对任意时空域上的解进行参数化，通过求解优化问题找到基展开系数，利用损失函数施加方程、边界条件和优化目标，借鉴物理信息神经网络思想。

Result: 机器学习优化协议能在广泛方程上实现指数收敛地找到解。

Conclusion: 该方法自然允许通过在损失函数中加入额外项进行数据同化，并能有效求解偏微分方程解的优化问题。

Abstract: Solving inverse and optimization problems over solutions of nonlinear partial
differential equations (PDEs) on complex spatial domains is a long-standing
challenge. Here we introduce a method that parameterizes the solution using
spectral bases on arbitrary spatiotemporal domains, whereby the basis is
defined on a hyperrectangle containing the true domain. We find the
coefficients of the basis expansion by solving an optimization problem whereby
both the equations, the boundary conditions and any optimization targets are
enforced by a loss function, building on a key idea from Physics-Informed
Neural Networks (PINNs). Since the representation of the function natively has
exponential convergence, so does the solution of the optimization problem, as
long as it can be solved efficiently. We find empirically that the optimization
protocols developed for machine learning find solutions with exponential
convergence on a wide range of equations. The method naturally allows for the
incorporation of data assimilation by including additional terms in the loss
function, and for the efficient solution of optimization problems over the PDE
solutions.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [283] [ggtime: A Grammar of Temporal Graphics](https://arxiv.org/abs/2510.25656)
*Cynthia A. Huang,Mitchell O'Hara-Wild,Rob J. Hyndman,Matthew Kay*

Main category: cs.HC

TL;DR: 现有可视化工具难处理复杂时间语义，提出时间图形语法及软件'ggtime'解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有可视化工具难以准确表示复杂的时间语义，且缺乏灵活通用工具。

Method: 提出时间图形语法，并开发相关软件'ggtime'，将时间语义编码到声明式语法中。

Result: 语法引入新的可组合元素，支持跨不同时间粒度可视化、不规则时长标准化和不同粒度及时区时间点对齐。

Conclusion: 该语法和软件可与其他语义变量互操作，能在保留时间语义的同时进行可视化导航。

Abstract: Visualizing changes over time is fundamental to learning from the past and
anticipating the future. However, temporal semantics can be complicated, and
existing visualization tools often struggle to accurately represent these
complexities. It is common to use bespoke plot helper functions designed to
produce specific graphics, due to the absence of flexible general tools that
respect temporal semantics. We address this problem by proposing a grammar of
temporal graphics, and an associated software implementation, 'ggtime', that
encodes temporal semantics into a declarative grammar for visualizing temporal
data. The grammar introduces new composable elements that support visualization
across linear, cyclical, quasi-cyclical, and other granularities;
standardization of irregular durations; and alignment of time points across
different granularities and time zones. It is designed for interoperability
with other semantic variables, allowing navigation across the space of
visualizations while preserving temporal semantics.

</details>


### [284] [Modelling the Interplay of Eye-Tracking Temporal Dynamics and Personality for Emotion Detection in Face-to-Face Settings](https://arxiv.org/abs/2510.24720)
*Meisam J. Seikavandi,Jostein Fimland,Fabricio Batista Narcizo,Maria Barrett,Ted Vucurevich,Jesper Bünsow Boldt,Andrew Burke Dittberner,Paolo Burelli*

Main category: cs.HC

TL;DR: 提出人格感知多模态框架预测感知和感受情绪，结合多信息提升预测效果，推动多模态情感计算。


<details>
  <summary>Details</summary>
Motivation: 准确识别人类情绪在动态对话场景有挑战，为实现自适应人机交互。

Method: 构建人格感知多模态框架，让73名参与者观看含语音片段并收集数据，用神经模型捕捉注视动态并融合多信息。

Result: 刺激线索增强感知情绪预测（宏F1达0.77），人格特质提升感受情绪识别（宏F1达0.58）。

Conclusion: 结合生理、特质和上下文信息可解决情绪主观性问题，区分感知和感受响应推动多模态情感计算，指向更个性化和生态有效的情绪感知系统。

Abstract: Accurate recognition of human emotions is critical for adaptive
human-computer interaction, yet remains challenging in dynamic,
conversation-like settings. This work presents a personality-aware multimodal
framework that integrates eye-tracking sequences, Big Five personality traits,
and contextual stimulus cues to predict both perceived and felt emotions.
Seventy-three participants viewed speech-containing clips from the CREMA-D
dataset while providing eye-tracking signals, personality assessments, and
emotion ratings. Our neural models captured temporal gaze dynamics and fused
them with trait and stimulus information, yielding consistent gains over SVM
and literature baselines. Results show that (i) stimulus cues strongly enhance
perceived-emotion predictions (macro F1 up to 0.77), while (ii) personality
traits provide the largest improvements for felt emotion recognition (macro F1
up to 0.58). These findings highlight the benefit of combining physiological,
trait-level, and contextual information to address the inherent subjectivity of
emotion. By distinguishing between perceived and felt responses, our approach
advances multimodal affective computing and points toward more personalized and
ecologically valid emotion-aware systems.

</details>


### [285] [AmarDoctor: An AI-Driven, Multilingual, Voice-Interactive Digital Health Application for Primary Care Triage and Patient Management to Bridge the Digital Health Divide for Bengali Speakers](https://arxiv.org/abs/2510.24724)
*Nazmun Nahar,Ritesh Harshad Ruparel,Shariar Kabir,Sumaiya Tasnia Khan,Shyamasree Saha,Mamunur Rashid*

Main category: cs.HC

TL;DR: 介绍多语言语音交互数字健康应用AmarDoctor，为孟加拉语使用者提供分诊和临床决策支持，评估显示其诊断和推荐精度高。


<details>
  <summary>Details</summary>
Motivation: 解决孟加拉语使用者在数字医疗服务方面的不足，填补现有平台服务人群的空白。

Method: 采用数据驱动方法，有患者和医护人员双界面系统，患者模块用自适应算法评估症状，集成语音交互AI助手，医护界面有AI决策支持；通过与临床案例集对比及与医生对比评估系统。

Result: AmarDoctor的诊断精度达81.08%，专科推荐精度达91.35%，均高于医生平均水平。

Conclusion: AmarDoctor能有效为孟加拉语使用者提供数字医疗服务，在诊断和推荐方面表现良好。

Abstract: This study presents AmarDoctor, a multilingual voice-interactive digital
health app designed to provide comprehensive patient triage and AI-driven
clinical decision support for Bengali speakers, a population largely
underserved in access to digital healthcare. AmarDoctor adopts a data-driven
approach to strengthen primary care delivery and enable personalized health
management. While platforms such as AdaHealth, WebMD, Symptomate, and K-Health
have become popular in recent years, they mainly serve European demographics
and languages. AmarDoctor addresses this gap with a dual-interface system for
both patients and healthcare providers, supporting three major Bengali
dialects. At its core, the patient module uses an adaptive questioning
algorithm to assess symptoms and guide users toward the appropriate specialist.
To overcome digital literacy barriers, it integrates a voice-interactive AI
assistant that navigates users through the app services. Complementing this,
the clinician-facing interface incorporates AI-powered decision support that
enhances workflow efficiency by generating structured provisional diagnoses and
treatment recommendations. These outputs inform key services such as
e-prescriptions, video consultations, and medical record management. To
validate clinical accuracy, the system was evaluated against a gold-standard
set of 185 clinical vignettes developed by experienced physicians.
Effectiveness was further assessed by comparing AmarDoctor performance with
five independent physicians using the same vignette set. Results showed
AmarDoctor achieved a top-1 diagnostic precision of 81.08 percent (versus
physicians average of 50.27 percent) and a top specialty recommendation
precision of 91.35 percent (versus physicians average of 62.6 percent).

</details>


### [286] [Beyond Models: A Framework for Contextual and Cultural Intelligence in African AI Deployment](https://arxiv.org/abs/2510.24729)
*Qness Ndlovu*

Main category: cs.HC

TL;DR: 本文介绍了情境与文化智能（CCI）框架，通过跨境购物平台验证其有效性，得出用户偏好及文化查询理解等结果，为资源受限市场的公平AI部署提供框架。


<details>
  <summary>Details</summary>
Motivation: 全球AI发展侧重模型性能和计算规模，在非洲市场部署需不同架构决策，引入CCI框架以实现有意义部署。

Method: 采用设计科学方法，通过服务侨民社区的AI原生跨境购物平台验证CCI。

Result: 89%用户偏好基于WhatsApp的AI交互；6周内有536名WhatsApp用户和3938次对话；文化提示工程能理解文化情境查询，有89%家庭商业模式和自然代码切换接受度。

Conclusion: CCI框架有三个技术支柱，该工作有理论创新和可复制实施模式，挑战硅谷设计正统观念，为资源受限市场公平AI部署提供可行框架。

Abstract: While global AI development prioritizes model performance and computational
scale, meaningful deployment in African markets requires fundamentally
different architectural decisions. This paper introduces Contextual and
Cultural Intelligence (CCI) -- a systematic framework enabling AI systems to
process cultural meaning, not just data patterns, through locally relevant,
emotionally intelligent, and economically inclusive design. Using design
science methodology, we validate CCI through a production AI-native
cross-border shopping platform serving diaspora communities. Key empirical
findings: 89% of users prefer WhatsApp-based AI interaction over traditional
web interfaces (n=602, chi-square=365.8, p<0.001), achieving 536 WhatsApp users
and 3,938 total conversations across 602 unique users in just 6 weeks, and
culturally informed prompt engineering demonstrates sophisticated understanding
of culturally contextualized queries, with 89% family-focused commerce patterns
and natural code-switching acceptance. The CCI framework operationalizes three
technical pillars: Infrastructure Intelligence (mobile-first, resilient
architectures), Cultural Intelligence (multilingual NLP with social context
awareness), and Commercial Intelligence (trust-based conversational commerce).
This work contributes both theoretical innovation and reproducible
implementation patterns, challenging Silicon Valley design orthodoxies while
providing actionable frameworks for equitable AI deployment across
resource-constrained markets.

</details>


### [287] [Efficiency Without Cognitive Change: Evidence from Human Interaction with Narrow AI Systems](https://arxiv.org/abs/2510.24893)
*María Angélica Benítez,Rocío Candela Ceballos,Karina Del Valle Molina,Sofía Mundo Araujo,Sofía Evangelina Victorio Villaroel,Nadia Justel*

Main category: cs.HC

TL;DR: 研究测试短期接触狭义AI工具对认知能力的影响，发现其提高效率但未改变认知，强调需相应伦理和教育框架。


<details>
  <summary>Details</summary>
Motivation: 探讨AI融入人类认知后，是仅提高效率还是改变思维方式。

Method: 让30名年轻人参与七周实验，四周在线干预，分有或无AI（ChatGPT）支持组完成问题解决和语言理解任务，并进行标准化神经心理评估。

Result: 有AI辅助的参与者完成任务更快更准，但标准化测试中问题解决和语言理解能力无显著前后差异。

Conclusion: 当前狭义AI系统是认知支架，提升表现但不改变认知能力，需促进批判性和自主思考的伦理与教育框架。

Abstract: The growing integration of artificial intelligence (AI) into human cognition
raises a fundamental question: does AI merely improve efficiency, or does it
alter how we think? This study experimentally tested whether short-term
exposure to narrow AI tools enhances core cognitive abilities or simply
optimizes task performance. Thirty young adults completed standardized
neuropsychological assessments embedded in a seven-week protocol with a
four-week online intervention involving problem-solving and verbal
comprehension tasks, either with or without AI support (ChatGPT). While
AI-assisted participants completed several tasks faster and more accurately, no
significant pre-post differences emerged in standardized measures of problem
solving or verbal comprehension. These results demonstrate efficiency gains
without cognitive change, suggesting that current narrow AI systems serve as
cognitive scaffolds extending performance without transforming underlying
mental capacities. The findings highlight the need for ethical and educational
frameworks that promote critical and autonomous thinking in an increasingly
AI-augmented cognitive ecology.

</details>


### [288] [User Misconceptions of LLM-Based Conversational Programming Assistants](https://arxiv.org/abs/2510.25662)
*Gabrielle O'Brien,Antonio Pedro Santos Alves,Sebastian Baltes,Grischa Liebel,Mircea Lungu,Marcos Kalinowski*

Main category: cs.HC

TL;DR: 研究对话式大语言模型编程助手用户的误解，发现用户存在期望偏差及深层次概念问题，强调工具需清晰传达能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型编程助手能力多样、扩展功能可用性不一，易使用户产生误解，引发不良编程行为，需研究用户误解。

Method: 采用两阶段方法，先梳理可能的用户误解，再对公开数据集里与基于大语言模型聊天机器人的Python编程对话进行定性分析。

Result: 发现部分用户对聊天机器人功能可用性有错误期望，且存在调试、验证和优化程序方面的深层次概念问题。

Conclusion: 有必要设计能更清晰向用户传达编程能力的基于大语言模型的工具。

Abstract: Programming assistants powered by large language models (LLMs) have become
widely available, with conversational assistants like ChatGPT proving
particularly accessible to less experienced programmers. However, the varied
capabilities of these tools across model versions and the mixed availability of
extensions that enable web search, code execution, or retrieval-augmented
generation create opportunities for user misconceptions about what systems can
and cannot do. Such misconceptions may lead to over-reliance, unproductive
practices, or insufficient quality control in LLM-assisted programming. Here,
we aim to characterize misconceptions that users of conversational LLM-based
assistants may have in programming contexts. Using a two-phase approach, we
first brainstorm and catalog user misconceptions that may occur, and then
conduct a qualitative analysis to examine whether these conceptual issues
surface in naturalistic Python-programming conversations with an LLM-based
chatbot drawn from an openly available dataset. Indeed, we see evidence that
some users have misplaced expectations about the availability of LLM-based
chatbot features like web access, code execution, or non-text output
generation. We also see potential evidence for deeper conceptual issues around
the scope of information required to debug, validate, and optimize programs.
Our findings reinforce the need for designing LLM-based tools that more clearly
communicate their programming capabilities to users.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [289] [Proceedings of the 12th Workshop on Horn Clauses for Verification and Synthesis](https://arxiv.org/abs/2510.25468)
*Emanuele De Angelis,Florian Frohn*

Main category: cs.LO

TL;DR: 介绍了第12届Horn子句验证与综合研讨会（HCVS 2025）会后论文集，该研讨会于2025年7月22日在克罗地亚萨格勒布举办，是第37届国际计算机辅助验证会议（CAV 2025）的附属研讨会。


<details>
  <summary>Details</summary>
Motivation: 展示HCVS 2025研讨会的成果。

Method: 无

Result: 出版了HCVS 2025的会后论文集。

Conclusion: HCVS 2025研讨会顺利举办并形成了会后论文集。

Abstract: This volume contains the post-proceedings of the 12th Workshop on Horn
Clauses for Verification and Synthesis (HCVS 2025), which took place in Zagreb,
Croatia, on July 22, 2025, as affiliated workshop of the 37th International
Conference on Computer Aided Verification (CAV 2025).

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [290] [Spectral functions in Minkowski quantum electrodynamics from neural reconstruction: Benchmarking against dispersive Dyson--Schwinger integral equations](https://arxiv.org/abs/2510.24728)
*Rodrigo Carmo Terin*

Main category: hep-ph

TL;DR: 提出M - PINN方法求解QED的Dyson - Schwinger积分方程，结合两种互补方法，基准测试结果良好，为后续扩展研究铺路。


<details>
  <summary>Details</summary>
Motivation: 直接在闵可夫斯基时空求解量子电动力学的Dyson - Schwinger积分方程。

Method: 将基于Lehmann表示和减去色散关系的色散求解器与M - PINN相结合，损失函数整合DSE残差并采用多尺度正则化等。

Result: 基准测试在红外到紫外尺度、壳上和动量减法方案中都显示出定量一致性，M - PINN能重现色散解，计算简洁且可微。

Conclusion: 该方法为引入现实顶点、非淬火效应和不确定性感知变体等扩展研究奠定基础。

Abstract: A Minkowskian physics-informed neural network approach (M--PINN) is
formulated to solve the Dyson--Schwinger integral equations (DSE) of quantum
electrodynamics (QED) directly in Minkowski spacetime. Our novel strategy
merges two complementary approaches: (i) a dispersive solver based on Lehmann
representations and subtracted dispersion relations, and (ii) a M--PINN that
learns the fermion mass function $B(p^2)$, under the same truncation and
renormalization configuration (quenched, rainbow, Landau gauge) with the loss
integrating the DSE residual with multi--scale regularization, and
monotonicity/smoothing penalties in the spacelike branch in the same way as in
our previous work in Euclidean space. The benchmarks show quantitative
agreement from the infrared (IR) to the ultraviolet (UV) scales in both
on-shell and momentum-subtraction schemes. In this controlled setting, our
M--PINN reproduces the dispersive solution whilst remaining computationally
compact and differentiable, paving the way for extensions with realistic
vertices, unquenching effects, and uncertainty-aware variants.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [291] [Sub-microsecond Transformers for Jet Tagging on FPGAs](https://arxiv.org/abs/2510.24784)
*Lauri Laatu,Chang Sun,Arianna Cox,Abhijith Gandrakota,Benedikt Maier,Jennifer Ngadiuba,Zhiqiang Que,Wayne Luk,Maria Spiropulu,Alexander Tapper*

Main category: physics.ins-det

TL;DR: 本文实现首个FPGA上亚微秒级变压器模型，用于高能物理实时应用，有低延迟和高性能。


<details>
  <summary>Details</summary>
Motivation: 变压器模型计算复杂度高，此前无法用于对撞机实验的实时应用，需解决其实时应用问题。

Method: 利用高粒度量化和分布式算术优化将整个变压器模型适配到单个FPGA上，并为hls4ml添加多头注意力和线性注意力支持。

Result: 实现了O(100)纳秒延迟，性能优于替代基线模型。

Conclusion: 推动了高亮度大型强子对撞机下一代触发系统发展，使变压器可用于高能物理等实时应用。

Abstract: We present the first sub-microsecond transformer implementation on an FPGA
achieving competitive performance for state-of-the-art high-energy physics
benchmarks. Transformers have shown exceptional performance on multiple tasks
in modern machine learning applications, including jet tagging at the CERN
Large Hadron Collider (LHC). However, their computational complexity prohibits
use in real-time applications, such as the hardware trigger system of the
collider experiments up until now. In this work, we demonstrate the first
application of transformers for jet tagging on FPGAs, achieving
$\mathcal{O}(100)$ nanosecond latency with superior performance compared to
alternative baseline models. We leverage high-granularity quantization and
distributed arithmetic optimization to fit the entire transformer model on a
single FPGA, achieving the required throughput and latency. Furthermore, we add
multi-head attention and linear attention support to hls4ml, making our work
accessible to the broader fast machine learning community. This work advances
the next-generation trigger systems for the High Luminosity LHC, enabling the
use of transformers for real-time applications in high-energy physics and
beyond.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [292] [Scaling flow-based approaches for topology sampling in $\mathrm{SU}(3)$ gauge theory](https://arxiv.org/abs/2510.25704)
*Claudio Bonanno,Andrea Bulgarelli,Elia Cellini,Alessandro Nada,Dario Panfalone,Davide Vadacchino,Lorenzo Verzichelli*

Main category: hep-lat

TL;DR: 开发基于非平衡模拟的方法缓解格点规范理论连续极限时的拓扑冻结，分析成本并提出策略，还推广该方法。


<details>
  <summary>Details</summary>
Motivation: 缓解格点规范理论接近连续极限时的拓扑冻结问题。

Method: 采用开放边界条件降低拓扑荷自相关，用非平衡蒙特卡罗方法消除开放边界条件的非物理效应，设计定制随机归一化流用于边界条件演化。

Result: 详细分析了四维SU(3)杨 - 米尔斯理论策略的计算成本，在小至0.045 fm的格点间距验证策略，定制随机归一化流表现更优。

Conclusion: 提出在连续极限下有效采样拓扑的明确策略，定制随机归一化流为未来基于流的解决方案奠定基础。

Abstract: We develop a methodology based on out-of-equilibrium simulations to mitigate
topological freezing when approaching the continuum limit of lattice gauge
theories. We reduce the autocorrelation of the topological charge employing
open boundary conditions, while removing exactly their unphysical effects using
a non-equilibrium Monte Carlo approach in which periodic boundary conditions
are gradually switched on. We perform a detailed analysis of the computational
costs of this strategy in the case of the four-dimensional $\mathrm{SU}(3)$
Yang-Mills theory. After achieving full control of the scaling, we outline a
clear strategy to sample topology efficiently in the continuum limit, which we
check at lattice spacings as small as $0.045$ fm. We also generalize this
approach by designing a customized Stochastic Normalizing Flow for evolutions
in the boundary conditions, obtaining superior performances with respect to the
purely stochastic non-equilibrium approach, and paving the way for more
efficient future flow-based solutions.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [293] [Sustainable NARMA-10 Benchmarking for Quantum Reservoir Computing](https://arxiv.org/abs/2510.25183)
*Avyay Kodali,Priyanshi Singh,Pranay Pandey,Krishna Bhatia,Shalini Devendrababu,Srinjoy Ganguly*

Main category: quant-ph

TL;DR: 研究对比QRC与经典模型及混合架构在NARMA - 10任务上表现，发现QRC有竞争力且具可持续优势。


<details>
  <summary>Details</summary>
Motivation: 对比量子储层计算（QRC）与经典模型（ESNs、LSTMs）及混合量子 - 经典架构（QLSTM）在非线性自回归移动平均任务（NARMA - 10）上的性能。

Method: 评估预测准确性（NRMSE）、计算成本和评估时间。

Result: QRC达到有竞争力的准确性，在资源受限环境有潜在可持续优势。

Conclusion: QRC在可持续时间序列AI应用方面有前景。

Abstract: This study compares Quantum Reservoir Computing (QRC) with classical models
such as Echo State Networks (ESNs) and Long Short-Term Memory networks (LSTMs),
as well as hybrid quantum-classical architectures (QLSTM), for the nonlinear
autoregressive moving average task (NARMA-10). We evaluate forecasting accuracy
(NRMSE), computational cost, and evaluation time. Results show that QRC
achieves competitive accuracy while offering potential sustainability
advantages, particularly in resource-constrained settings, highlighting its
promise for sustainable time-series AI applications.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [294] [TabMGP: Martingale Posterior with TabPFN](https://arxiv.org/abs/2510.25154)
*Kenyon Ng,Edwin Fong,David T. Frazier,Jeremias Knoblauch,Susan Wei*

Main category: stat.ME

TL;DR: 本文介绍基于TabPFN的TabMGP，它在产生可信集方面表现良好，常优于现有MGP构造和标准贝叶斯方法。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推理存在先验引出、似然错误指定和计算负担等挑战，鞅后验（MGP）虽提供替代方案，但缺乏合适的预测规则示例，因此探索合适的预测规则。

Method: 引入基于TabPFN（当前表格数据最先进的变压器基础模型）构建的TabMGP。

Result: TabMGP产生的可信集接近标称覆盖率，且常优于现有MGP构造和标准贝叶斯方法。

Conclusion: TabMGP是一种有效的方法，在表格数据处理上有较好表现。

Abstract: Bayesian inference provides principled uncertainty quantification but is
often limited by challenges of prior elicitation, likelihood misspecification,
and computational burden. The martingale posterior (MGP, Fong et al., 2023)
offers an alternative, replacing prior-likelihood elicitation with a predictive
rule - namely, a sequence of one-step-ahead predictive distributions - for
forward data generation. The utility of MGPs depends on the choice of
predictive rule, yet the literature has offered few compelling examples.
Foundation transformers are well-suited here, as their autoregressive
generation mirrors this forward simulation and their general-purpose design
enables rich predictive modeling. We introduce TabMGP, an MGP built on TabPFN,
a transformer foundation model that is currently state-of-the-art for tabular
data. TabMGP produces credible sets with near-nominal coverage and often
outperforms both existing MGP constructions and standard Bayes.

</details>


### [295] [Robust variable selection for spatial point processes observed with noise](https://arxiv.org/abs/2510.25550)
*Dominik Sturm,Ivo F. Sbalzarini*

Main category: stat.ME

TL;DR: 本文提出结合稀疏估计与抗噪模型选择的空间点过程强度函数变量选择方法，模拟和实际应用验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 高分辨率空间数据含噪声，识别影响事件定位的空间协变量对理解潜在机制很重要，需解决噪声影响。

Method: 基于点过程子采样的稳定性选择，引入非凸最佳子集惩罚项。

Result: 模拟显示该方法在不同噪声场景下能可靠恢复真实协变量，提高选择准确性和稳定性；应用于林业数据集证明其实用性。

Conclusion: 该方法为含噪声的空间点过程模型提供了鲁棒变量选择的系统框架，无需额外过程知识。

Abstract: We propose a method for variable selection in the intensity function of
spatial point processes that combines sparsity-promoting estimation with
noise-robust model selection. As high-resolution spatial data becomes
increasingly available through remote sensing and automated image analysis,
identifying spatial covariates that influence the localization of events is
crucial to understand the underlying mechanism. However, results from automated
acquisition techniques are often noisy, for example due to measurement
uncertainties or detection errors, which leads to spurious displacements and
missed events. We study the impact of such noise on sparse point-process
estimation across different models, including Poisson and Thomas processes. To
improve noise robustness, we propose to use stability selection based on
point-process subsampling and to incorporate a non-convex best-subset penalty
to enhance model-selection performance. In extensive simulations, we
demonstrate that such an approach reliably recovers true covariates under
diverse noise scenarios and improves both selection accuracy and stability. We
then apply the proposed method to a forestry data set, analyzing the
distribution of trees in relation to elevation and soil nutrients in a tropical
rain forest. This shows the practical utility of the method, which provides a
systematic framework for robust variable selection in spatial point-process
models under noise, without requiring additional knowledge of the process.

</details>


### [296] [Existence and optimisation of the partial correlation graphical lasso](https://arxiv.org/abs/2510.25712)
*Jack Storror Carter,Cesare Molinari*

Main category: stat.ME

TL;DR: 本文在PCGLASSO计算方面取得突破，证明小样本下估计存在性，提出新算法并实现R包。


<details>
  <summary>Details</summary>
Motivation: PCGLASSO因优化问题非凸存在计算挑战，需解决计算难题。

Method: 证明小样本下PCGLASSO估计的存在性，提出新的交替算法并在R包中实现。

Result: 证明小样本下PCGLASSO估计存在，实现首个公开可用的PCGLASSO，在中等维度计算时间有竞争力。

Conclusion: PCGLASSO可用于任何高斯数据，新算法及实现为其计算提供有效方案。

Abstract: The partial correlation graphical LASSO (PCGLASSO) is a penalised likelihood
method for Gaussian graphical models which provides scale invariant sparse
estimation of the precision matrix and improves upon the popular graphical
LASSO method. However, the PCGLASSO suffers from computational challenges due
to the non-convexity of its associated optimisation problem. This paper
provides some important breakthroughs in the computation of the PCGLASSO.
First, the existence of the PCGLASSO estimate is proven when the sample size is
smaller than the dimension - a case in which the maximum likelihood estimate
does not exist. This means that the PCGLASSO can be used with any Gaussian
data. Second, a new alternating algorithm for computing the PCGLASSO is
proposed and implemented in the R package PCGLASSO available at
https://github.com/JackStorrorCarter/PCGLASSO. This was the first publicly
available implementation of the PCGLASSO and provides competitive computation
time for moderate dimension size.

</details>


### [297] [Statistical Process Monitoring based on Functional Data Analysis](https://arxiv.org/abs/2510.25742)
*Fabio Centofanti*

Main category: stat.ME

TL;DR: 本文综述基于功能数据分析（FDA）的统计过程监控（SPM）方法，介绍参考框架、多种新方法，方法在R包funcharts实现，最后回顾其他方法并给出未来研究建议。


<details>
  <summary>Details</summary>
Motivation: 传统监控技术有局限性，需用FDA工具解决，同时应对实际应用中的常见挑战。

Method: 先给出多变量功能数据监控的参考框架，再调研几种基于FDA且扩展该框架的新方法，包括整合额外功能协变量、鲁棒方法、实时监控技术和自适应策略等。

Result: 所介绍方法都在R包funcharts中实现。

Conclusion: 对现有基于FDA的轮廓监控方法进行回顾，并为未来研究提供建议。

Abstract: In modern industrial settings, advanced acquisition systems allow for the
collection of data in the form of profiles, that is, as functional
relationships linking responses to explanatory variables. In this context,
statistical process monitoring (SPM) aims to assess the stability of profiles
over time in order to detect unexpected behavior. This review focuses on SPM
methods that model profiles as functional data, i.e., smooth functions defined
over a continuous domain, and apply functional data analysis (FDA) tools to
address limitations of traditional monitoring techniques. A reference framework
for monitoring multivariate functional data is first presented. This review
then offers a focused survey of several recent FDA-based profile monitoring
methods that extend this framework to address common challenges encountered in
real-world applications. These include approaches that integrate additional
functional covariates to enhance detection power, a robust method designed to
accommodate outlying observations, a real-time monitoring technique for
partially observed profiles, and two adaptive strategies that target the
characteristics of the out-of-control distribution. These methods are all
implemented in the R package funcharts, available on CRAN. Finally, a review of
additional existing FDA-based profile monitoring methods is also presented,
along with suggestions for future research.

</details>


### [298] [Bayesian Adaptive Polynomial Chaos Expansions](https://arxiv.org/abs/2510.25036)
*Kellin N. Rumsey,Devin Francom,Graham C. Gibson,J. Derek Tucker,Gabriel Huerta*

Main category: stat.ME

TL;DR: 本文开发了新的全贝叶斯自适应多项式混沌展开（PCE）方法khaos，经模拟研究和实际应用验证其在代理建模等任务中有竞争力。


<details>
  <summary>Details</summary>
Motivation: PCE在统计学文献中受关注少，全贝叶斯公式罕见且缺少R实现，受自适应贝叶斯机器学习模型成功启发。

Method: 开发新的全贝叶斯自适应PCE方法khaos，包含新的提议分布以实现数据驱动交互选择，支持针对PCE结构修改的g先验。

Result: 通过模拟研究和实际UQ应用，表明贝叶斯自适应PCE在代理建模、全局敏感性分析和序数回归任务中有竞争力。

Conclusion: 新开发的贝叶斯自适应PCE方法具有良好性能，可用于相关UQ任务。

Abstract: Polynomial chaos expansions (PCE) are widely used for uncertainty
quantification (UQ) tasks, particularly in the applied mathematics community.
However, PCE has received comparatively less attention in the statistics
literature, and fully Bayesian formulations remain rare, especially with
implementations in R. Motivated by the success of adaptive Bayesian machine
learning models such as BART, BASS, and BPPR, we develop a new fully Bayesian
adaptive PCE method with an efficient and accessible R implementation: khaos.
Our approach includes a novel proposal distribution that enables data-driven
interaction selection, and supports a modified g-prior tailored to PCE
structure. Through simulation studies and real-world UQ applications, we
demonstrate that Bayesian adaptive PCE provides competitive performance for
surrogate modeling, global sensitivity analysis, and ordinal regression tasks.

</details>


### [299] [Tuning-Free Sampling via Optimization on the Space of Probability Measures](https://arxiv.org/abs/2510.25315)
*Louis Sharrock,Christopher Nemeth*

Main category: stat.ME

TL;DR: 提出基于Wasserstein梯度流时间离散化的自适应、免调步长采样算法，可用于多种梯度采样算法及随机优化问题，有理论保证且性能与最优调参算法相当。


<details>
  <summary>Details</summary>
Motivation: 为梯度采样算法和随机优化问题提供无需调参的步长调度方法。

Method: 基于Wasserstein梯度流的时间离散化得到自适应、免调步长调度。

Result: 得到一套免调参采样算法，在温和假设下有强理论保证，在多种任务中性能与最优调参算法相当。

Conclusion: 所提算法无需调步长参数即可达到与现有最优算法相似性能。

Abstract: We introduce adaptive, tuning-free step size schedules for gradient-based
sampling algorithms obtained as time-discretizations of Wasserstein gradient
flows. The result is a suite of tuning-free sampling algorithms, including
tuning-free variants of the unadjusted Langevin algorithm (ULA), stochastic
gradient Langevin dynamics (SGLD), mean-field Langevin dynamics (MFLD), Stein
variational gradient descent (SVGD), and variational gradient descent (VGD).
More widely, our approach yields tuning-free algorithms for solving a broad
class of stochastic optimization problems over the space of probability
measures. Under mild assumptions (e.g., geodesic convexity and locally bounded
stochastic gradients), we establish strong theoretical guarantees for our
approach. In particular, we recover the convergence rate of optimally tuned
versions of these algorithms up to logarithmic factors, in both nonsmooth and
smooth settings. We then benchmark the performance of our methods against
comparable existing approaches. Across a variety of tasks, our algorithms
achieve similar performance to the optimal performance of existing algorithms,
with no need to tune a step size parameter.

</details>


### [300] [Distributional Evaluation of Generative Models via Relative Density Ratio](https://arxiv.org/abs/2510.25507)
*Yuliang Xu,Yun Wei,Li Ma*

Main category: stat.ME

TL;DR: 提出基于相对密度比（RDR）的生成模型功能评估指标，有理论特性和收敛率保证，实验验证其评估能力。


<details>
  <summary>Details</summary>
Motivation: 设计一种能有效表征真实和生成样本分布差异的生成模型评估指标。

Method: 基于φ - 散度的变分形式通过凸优化实现RDR的功能估计，利用M - 估计理论给出一般估计器收敛率保证，给出神经网络估计器收敛率。

Result: 在MNIST、CelebA64和微生物组数据上实验表明，估计的RDR可有效比较模型性能，揭示拟合性质。

Conclusion: 提出的基于RDR的评估指标可评估支持重叠、覆盖和保真度，找出样本空间集中区域和特征差异。

Abstract: We propose a functional evaluation metric for generative models based on the
relative density ratio (RDR) designed to characterize distributional
differences between real and generated samples. We show that the RDR as a
functional summary of the goodness-of-fit for the generative model, possesses
several desirable theoretical properties. It preserves $\phi$-divergence
between two distributions, enables sample-level evaluation that facilitates
downstream investigations of feature-specific distributional differences, and
has a bounded range that affords clear interpretability and numerical
stability. Functional estimation of the RDR is achieved efficiently through
convex optimization on the variational form of $\phi$-divergence. We provide
theoretical convergence rate guarantees for general estimators based on
M-estimator theory, as well as the convergence rates of neural network-based
estimators when the true ratio is in the anisotropic Besov space. We
demonstrate the power of the proposed RDR-based evaluation through numerical
experiments on MNIST, CelebA64, and the American Gut project microbiome data.
We show that the estimated RDR not only allows for an effective comparison of
the overall performance of competing generative models, but it can also offer a
convenient means of revealing the nature of the underlying goodness-of-fit.
This enables one to assess support overlap, coverage, and fidelity while
pinpointing regions of the sample space where generators concentrate and
revealing the features that drive the most salient distributional differences.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [301] [Stochastic Optimization in Semi-Discrete Optimal Transport: Convergence Analysis and Minimax Rate](https://arxiv.org/abs/2510.25287)
*Ferdinand Genans,Antoine Godichon-Baggioni,François-Xavier Vialard,Olivier Wintenberger*

Main category: math.ST

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We investigate the semi-discrete Optimal Transport (OT) problem, where a
continuous source measure $\mu$ is transported to a discrete target measure
$\nu$, with particular attention to the OT map approximation. In this setting,
Stochastic Gradient Descent (SGD) based solvers have demonstrated strong
empirical performance in recent machine learning applications, yet their
theoretical guarantee to approximate the OT map is an open question. In this
work, we answer it positively by providing both computational and statistical
convergence guarantees of SGD. Specifically, we show that SGD methods can
estimate the OT map with a minimax convergence rate of
$\mathcal{O}(1/\sqrt{n})$, where $n$ is the number of samples drawn from $\mu$.
To establish this result, we study the averaged projected SGD algorithm, and
identify a suitable projection set that contains a minimizer of the objective,
even when the source measure is not compactly supported. Our analysis holds
under mild assumptions on the source measure and applies to MTW cost
functions,whic include $\|\cdot\|^p$ for $p \in (1, \infty)$. We finally
provide numerical evidence for our theoretical results.

</details>


### [302] [Estimation of discrete distributions with high probability under $χ^2$-divergence](https://arxiv.org/abs/2510.25400)
*Sirine Louati*

Main category: math.ST

TL;DR: 研究离散分布在卡方散度损失下高概率估计，给出拉普拉斯估计量上下界，刻画极小极大高概率风险。


<details>
  <summary>Details</summary>
Motivation: 期望下的极小极大风险已被理解，但高概率对应情况研究不足。

Method: 给出经典拉普拉斯估计量的上下界，刻画任意估计量的极小极大高概率风险，采用简单平滑策略。

Result: 拉普拉斯估计量在不依赖置信水平的估计量中表现最优，极小极大高概率风险可通过简单平滑策略实现。

Conclusion: 工作完善了现有保证，推进了基于散度估计的理论理解，指出渐近和非渐近保证间存在内在分离。

Abstract: We investigate the high-probability estimation of discrete distributions from
an \iid sample under $\chi^2$-divergence loss. Although the minimax risk in
expectation is well understood, its high-probability counterpart remains
largely unexplored. We provide sharp upper and lower bounds for the classical
Laplace estimator, showing that it achieves optimal performance among
estimators that do not rely on the confidence level. We further characterize
the minimax high-probability risk for any estimator and demonstrate that it can
be attained through a simple smoothing strategy. Our analysis highlights an
intrinsic separation between asymptotic and non-asymptotic guarantees, with the
latter suffering from an unavoidable overhead. This work sharpens existing
guarantees and advances the theoretical understanding of divergence-based
estimation.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [303] [Measuring the Research Output and Performance of the University of Ibadan from 2014 to 2023: A Scientometric Analysis](https://arxiv.org/abs/2510.25283)
*Muneer Ahmad,Undie Felicia Nkatv*

Main category: cs.DL

TL;DR: 运用科学计量方法评估伊巴丹大学2014 - 2023年研究产出与表现，分析多方面情况并给出结果，为改进研究策略和政策提供依据。


<details>
  <summary>Details</summary>
Motivation: 确定伊巴丹大学2014 - 2023年研究产出和发表模式的影响，全面评估其研究生产力、影响力和学科重点。

Method: 采用科学计量方法，分析出版物趋势、引用模式和合作网络，还使用VOSviewer软件映射方法进行数据图示。

Result: 2014 - 2023年发表7159篇论文，h指数为75，获得218572次引用。

Conclusion: 有助于了解大学研究优劣势和改进方向，为伊巴丹大学提升研究策略和政策的循证决策提供参考。

Abstract: This study employs scientometric methods to assess the research output and
performance of the University of Ibadan from 2014 to 2023. By analyzing
publication trends, citation patterns, and collaboration networks, the research
aims to comprehensively evaluate the university's research productivity,
impact, and disciplinary focus. This article's endeavors are characterized by
innovation, interdisciplinary collaboration, and commitment to excellence,
making the University of Ibadan a significant hub for cutting-edge research in
Nigeria and beyond. The goal of the current study is to ascertain the influence
of the university's research output and publication patterns between 2014 and
2023. The study focuses on the departments at the University of Ibadan that
contribute the most, the best journals for publishing, the nations that
collaborate, the impact of citations both locally and globally, well-known
authors and their total production, and the research output broken down by
year. According to the university's ten-year publication data, 7159 papers with
an h-index of 75 were published between 2014 and 2023, garnering 218572
citations. Furthermore, the VOSviewer software mapping approach is used to
illustrate the stenographical mapping of data through graphs. The findings of
this study will contribute to understanding the university's research
strengths, weaknesses, and potential areas for improvement. Additionally, the
results will inform evidence-based decision-making for enhancing research
strategies and policies at the University of Ibadan.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [304] [Large-Scale Network Embedding in Apache Spark](https://arxiv.org/abs/2106.10620)
*Wenqing Lin*

Main category: cs.SI

TL;DR: 提出基于Apache Spark的大规模图网络嵌入分布式算法，处理大图高效，实验和应用效果好。


<details>
  <summary>Details</summary>
Motivation: 以往网络嵌入方法难以高效处理大图，计算成本高且图或向量中间结果大，单机难处理。

Method: 递归将图划分为小的子图，捕捉节点结构信息，并行计算子图网络嵌入，线性聚合输出。

Result: 能在几小时内处理数十亿边的图，比现有方法快至少4倍，链接预测和节点分类任务分别提升4.25%和4.27%。在腾讯游戏应用中，运行时间最多提升91.11%，评估指标最多提升12.80%。

Conclusion: 提出的分布式算法处理大图高效且有效，可用于社交推荐等场景。

Abstract: Network embedding has been widely used in social recommendation and network
analysis, such as recommendation systems and anomaly detection with graphs.
However, most of previous approaches cannot handle large graphs efficiently,
due to that (i) computation on graphs is often costly and (ii) the size of
graph or the intermediate results of vectors could be prohibitively large,
rendering it difficult to be processed on a single machine. In this paper, we
propose an efficient and effective distributed algorithm for network embedding
on large graphs using Apache Spark, which recursively partitions a graph into
several small-sized subgraphs to capture the internal and external structural
information of nodes, and then computes the network embedding for each subgraph
in parallel. Finally, by aggregating the outputs on all subgraphs, we obtain
the embeddings of nodes in a linear cost. After that, we demonstrate in various
experiments that our proposed approach is able to handle graphs with billions
of edges within a few hours and is at least 4 times faster than the
state-of-the-art approaches. Besides, it achieves up to $4.25\%$ and $4.27\%$
improvements on link prediction and node classification tasks respectively. In
the end, we deploy the proposed algorithms in two online games of Tencent with
the applications of friend recommendation and item recommendation, which
improve the competitors by up to $91.11\%$ in running time and up to $12.80\%$
in the corresponding evaluation metrics.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [305] [Effect of Full Common Randomness Replication in Symmetric PIR on Graph-Based Replicated Systems](https://arxiv.org/abs/2510.25736)
*Shreya Meel,Sennur Ulukus*

Main category: cs.IT

TL;DR: 本文研究图模型下对称私有信息检索（SPIR）问题，开发算法转换PIR方案为SPIR方案，给出容量上下界，确定三顶点路径图的SPIR容量为1/2。


<details>
  <summary>Details</summary>
Motivation: 量化图复制公共随机性设置下SPIR容量的提升。

Method: 开发算法将一类PIR方案转换为相应的SPIR方案。

Result: 建立了存在此类方案的图的容量下界，给出路径和循环图更紧的容量上界，确定三顶点路径图的SPIR容量为1/2。

Conclusion: 通过算法和分析，得到了图模型下SPIR容量的相关结果。

Abstract: We revisit the problem of symmetric private information retrieval (SPIR) in
settings where the database replication is modeled by a simple graph. Here,
each vertex corresponds to a server, and a message is replicated on two servers
if and only if there is an edge between them. To satisfy the requirement of
database privacy, we let all the servers share some common randomness,
independent of the messages. We aim to quantify the improvement in SPIR
capacity, i.e., the maximum ratio of the number of desired and downloaded
symbols, compared to the setting with graph-replicated common randomness.
Towards this, we develop an algorithm to convert a class of PIR schemes into
the corresponding SPIR schemes, thereby establishing a capacity lower bound on
graphs for which such schemes exist. This includes the class of path and cyclic
graphs for which we derive capacity upper bounds that are tighter than the
trivial bounds given by the respective PIR capacities. For the special case of
path graph with three vertices, we identify the SPIR capacity to be
$\frac{1}{2}$.

</details>


### [306] [Dual-Domain Deep Learning-Assisted NOMA-CSK Systems for Secure and Efficient Vehicular Communications](https://arxiv.org/abs/2510.24763)
*Tingting Huang,Jundong Chen,Huanqiang Zeng,Guofa Cai,Georges Kaddoum*

Main category: cs.IT

TL;DR: 本文提出用于车辆通信的DL - NOMA - CSK系统，设计DNN解调器，经理论分析和仿真表明该系统在多方面性能优越且计算复杂度低，适用于安全车辆通信。


<details>
  <summary>Details</summary>
Motivation: 现有多用户混沌通信系统存在频谱效率低、用户连接受限、计算复杂度高和可扩展性差等问题，需设计新系统保障车辆通信系统安全高效的多用户传输。

Method: 提出DL - NOMA - CSK系统，设计基于DNN的解调器，采用双域特征提取架构处理混沌信号，将DNN集成到SIC框架中。

Result: 理论分析和大量仿真表明，该系统在频谱效率、能量效率、误码率、安全性和鲁棒性方面表现优越，且计算复杂度低于传统MU - DCSK和现有DL辅助方案。

Conclusion: 该系统的优势证明其在安全车辆通信中具有实际可行性。

Abstract: Ensuring secure and efficient multi-user (MU) transmission is critical for
vehicular communication systems. Chaos-based modulation schemes have garnered
considerable interest due to their benefits in physical layer security.
However, most existing MU chaotic communication systems, particularly those
based on non-coherent detection, suffer from low spectral efficiency due to
reference signal transmission, and limited user connectivity under orthogonal
multiple access (OMA). While non-orthogonal schemes, such as sparse code
multiple access (SCMA)-based DCSK, have been explored, they face high
computational complexity and inflexible scalability due to their fixed codebook
designs. This paper proposes a deep learning-assisted power domain
non-orthogonal multiple access chaos shift keying (DL-NOMA-CSK) system for
vehicular communications. A deep neural network (DNN)-based demodulator is
designed to learn intrinsic chaotic signal characteristics during offline
training, thereby eliminating the need for chaotic synchronization or reference
signal transmission. The demodulator employs a dual-domain feature extraction
architecture that jointly processes the time-domain and frequency-domain
information of chaotic signals, enhancing feature learning under dynamic
channels. The DNN is integrated into the successive interference cancellation
(SIC) framework to mitigate error propagation issues. Theoretical analysis and
extensive simulations demonstrate that the proposed system achieves superior
performance in terms of spectral efficiency (SE), energy efficiency (EE), bit
error rate (BER), security, and robustness, while maintaining lower
computational complexity compared to traditional MU-DCSK and existing DL-aided
schemes. These advantages validate its practical viability for secure vehicular
communications.

</details>


### [307] [Fed-PELAD: Communication-Efficient Federated Learning for Massive MIMO CSI Feedback with Personalized Encoders and a LoRA-Adapted Shared Decoder](https://arxiv.org/abs/2510.25181)
*Yixiang Zhou,Tong Wu,Meixia Tao,Jianhua Mo*

Main category: cs.IT

TL;DR: 本文提出Fed - PELAD框架解决大规模MIMO系统中CSI反馈深度学习的通信开销、数据异构和隐私问题，模拟显示其通信成本低且精度高。


<details>
  <summary>Details</summary>
Motivation: 解决大规模MIMO系统中CSI反馈深度学习面临的通信开销、数据异构和隐私等关键挑战。

Method: 提出Fed - PELAD框架，包含个性化编码器和LoRA适配的共享解码器，训练时个性化编码器本地训练，共享解码器全局更新，传输紧凑的LoRA适配器参数，并引入交替冻结策略和校准学习率比。

Result: 在3GPP标准信道模型上的广泛模拟表明，与传统方法相比，Fed - PELAD仅需42.97%的上行通信成本，在异构条件下CSI反馈精度有1.2 dB的性能提升。

Conclusion: Fed - PELAD能有效降低通信开销，提高CSI反馈精度，应对数据异构问题。

Abstract: This paper addresses the critical challenges of communication overhead, data
heterogeneity, and privacy in deep learning for channel state information (CSI)
feedback in massive MIMO systems. To this end, we propose Fed-PELAD, a novel
federated learning framework that incorporates personalized encoders and a
LoRA-adapted shared decoder. Specifically, personalized encoders are trained
locally on each user equipment (UE) to capture device-specific channel
characteristics, while a shared decoder is updated globally via the
coordination of the base station (BS) by using Low-Rank Adaptation (LoRA). This
design ensures that only compact LoRA adapter parameters instead of full model
updates are transmitted for aggregation. To further enhance convergence
stability, we introduce an alternating freezing strategy with calibrated
learning-rate ratio during LoRA aggregation. Extensive simulations on
3GPP-standard channel models demonstrate that Fed-PELAD requires only 42.97\%
of the uplink communication cost compared to conventional methods while
achieving a performance gain of 1.2 dB in CSI feedback accuracy under
heterogeneous conditions.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [308] [4-Doodle: Text to 3D Sketches that Move!](https://arxiv.org/abs/2510.25319)
*Hao Chen,Jiaqi Wang,Yonggang Qi,Ke Li,Kaiyue Pang,Yi-Zhe Song*

Main category: cs.GR

TL;DR: 提出文本到3D草图动画新任务，提出无训练框架4 - Doodle，实验表明方法性能优。


<details>
  <summary>Details</summary>
Motivation: 现有工作多关注照片级真实内容生成，而目标是创建轻量级、可解释的3D矢量草图动画，但该任务面临无配对数据集、草图结构抽象难建模和动画需时间连贯性与多视图一致性等挑战。

Method: 提出无训练框架4 - Doodle，利用预训练图像和视频扩散模型，通过双空间蒸馏方案，采用多视图优化，并引入结构感知运动模块。

Result: 方法生成的3D草图动画时间上逼真、结构稳定，在保真度和可控性上优于现有基线。

Conclusion: 此工作是迈向更直观、易访问的4D内容创建的一步。

Abstract: We present a novel task: text-to-3D sketch animation, which aims to bring
freeform sketches to life in dynamic 3D space. Unlike prior works focused on
photorealistic content generation, we target sparse, stylized, and
view-consistent 3D vector sketches, a lightweight and interpretable medium
well-suited for visual communication and prototyping. However, this task is
very challenging: (i) no paired dataset exists for text and 3D (or 4D)
sketches; (ii) sketches require structural abstraction that is difficult to
model with conventional 3D representations like NeRFs or point clouds; and
(iii) animating such sketches demands temporal coherence and multi-view
consistency, which current pipelines do not address. Therefore, we propose
4-Doodle, the first training-free framework for generating dynamic 3D sketches
from text. It leverages pretrained image and video diffusion models through a
dual-space distillation scheme: one space captures multi-view-consistent
geometry using differentiable B\'ezier curves, while the other encodes motion
dynamics via temporally-aware priors. Unlike prior work (e.g., DreamFusion),
which optimizes from a single view per step, our multi-view optimization
ensures structural alignment and avoids view ambiguity, critical for sparse
sketches. Furthermore, we introduce a structure-aware motion module that
separates shape-preserving trajectories from deformation-aware changes,
enabling expressive motion such as flipping, rotation, and articulated
movement. Extensive experiments show that our method produces temporally
realistic and structurally stable 3D sketch animations, outperforming existing
baselines in both fidelity and controllability. We hope this work serves as a
step toward more intuitive and accessible 4D content creation.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [309] [Fast Dimensionality Reduction from $\ell_2$ to $\ell_p$](https://arxiv.org/abs/2510.25541)
*Rafael Chiclana,Mark Iwen*

Main category: math.PR

TL;DR: 本文基于Ailon和Liberty的构造提出从ℓ₂到ℓₚ（p∈[1,2]）的线性嵌入方法，在目标维度小时减少运行时间，并给出目标空间任意范数下嵌入所需维度的下界。


<details>
  <summary>Details</summary>
Motivation: 近年来保ℓ₁范数欧氏距离的嵌入受关注，此前Dirksen等人有相关成果，本文欲拓展该方向。

Method: 基于Ailon和Liberty的构造提出从ℓ₂到ℓₚ（p∈[1,2]）的简单线性嵌入方法。

Result: 当k ≤ d^(1/4)时，方法运行时间降至O(d log k)；给出目标空间任意范数下嵌入所需维度的下界。

Conclusion: 提出的嵌入方法在目标维度小时改进了运行时间，且给出的维度下界与ℓ₂情况的最优界在对数因子上匹配。

Abstract: The Johnson-Lindenstrauss (JL) lemma is a fundamental result in
dimensionality reduction, ensuring that any finite set $X \subseteq
\mathbb{R}^d$ can be embedded into a lower-dimensional space $\mathbb{R}^k$
while approximately preserving all pairwise Euclidean distances. In recent
years, embeddings that preserve Euclidean distances when measured via the
$\ell_1$ norm in the target space have received increasing attention due to
their relevance in applications such as nearest neighbor search in high
dimensions. A recent breakthrough by Dirksen, Mendelson, and Stollenwerk
established an optimal $\ell_2 \to \ell_1$ embedding with computational
complexity $O(d \log d)$. In this work, we generalize this direction and
propose a simple linear embedding from $\ell_2$ to $\ell_p$ for any $p \in
[1,2]$ based on a construction of Ailon and Liberty. Our method achieves a
reduced runtime of $O(d \log k)$ when $k \leq d^{1/4}$, improving upon prior
runtime results when the target dimension is small. Additionally, we show that
for \emph{any norm} $\|\cdot\|$ in the target space, any embedding of
$(\mathbb{R}^d, \|\cdot\|_2)$ into $(\mathbb{R}^k, \|\cdot\|)$ with distortion
$\varepsilon$ generally requires $k = \Omega\big(\varepsilon^{-2}
\log(\varepsilon^2 n)/\log(1/\varepsilon)\big)$, matching the optimal bound for
the $\ell_2$ case up to a logarithmic factor.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [310] [Nonlinear Dynamics In Optimization Landscape of Shallow Neural Networks with Tunable Leaky ReLU](https://arxiv.org/abs/2510.25060)
*Jingzhou Liu*

Main category: math.OC

TL;DR: 研究均方损失和leaky ReLU激活的浅神经网络非线性动力学，建立理论框架检测临界点分岔，表明分岔与宽度无关等。


<details>
  <summary>Details</summary>
Motivation: 研究浅神经网络在均方损失和leaky ReLU激活下的非线性动力学特性。

Method: 基于等变梯度度建立理论框架，适用于任意神经元数量k>=4。

Result: 多模式简并在临界数0处一致发生，与k无关；分岔与宽度无关，仅在非负α时出现，全局最小值在(0,1)范围内无进一步对称破缺不稳定性。

Conclusion: 所建立的理论框架能有效检测临界点分岔及相关对称性，且分岔具有宽度无关等特性。

Abstract: In this work, we study the nonlinear dynamics of a shallow neural network
trained with mean-squared loss and leaky ReLU activation. Under Gaussian inputs
and equal layer width k, (1) we establish, based on the equivariant gradient
degree, a theoretical framework, applicable to any number of neurons k>= 4, to
detect bifurcation of critical points with associated symmetries from global
minimum as leaky parameter $\alpha$ varies. Typically, our analysis reveals
that a multi-mode degeneracy consistently occurs at the critical number 0,
independent of k. (2) As a by-product, we further show that such bifurcations
are width-independent, arise only for nonnegative $\alpha$ and that the global
minimum undergoes no further symmetry-breaking instability throughout the
engineering regime $\alpha$ in range (0,1). An explicit example with k=5 is
presented to illustrate the framework and exhibit the resulting bifurcation
together with their symmetries.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [311] [Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for Speech Foundation Models](https://arxiv.org/abs/2510.25577)
*Harm Lameris,Shree Harsha Bokkahalli Satish,Joakim Gustafson,Éva Székely*

Main category: eess.AS

TL;DR: 本文通过开放式生成任务和语音情感识别探索语音基础模型（SFMs）对不同发声类型语音的敏感性，并引入新的并行数据集评估其响应。


<details>
  <summary>Details</summary>
Motivation: 现有语音理解基准依赖多选问答格式，难以捕捉副语言特征对模型行为的细微影响，且语音质量这一副语言变化维度未被充分探索。

Method: 通过开放式生成任务和语音情感识别来探索SFMs，引入新的并行数据集评估其对不同发声类型语音的响应。

Result: 文中未明确提及具体结果。

Conclusion: 本文首次对SFMs在语音感知非词汇方面的敏感性进行了研究。

Abstract: Recent advances in speech foundation models (SFMs) have enabled the direct
processing of spoken language from raw audio, bypassing intermediate textual
representations. This capability allows SFMs to be exposed to, and potentially
respond to, rich paralinguistic variations embedded in the input speech signal.
One under-explored dimension of paralinguistic variation is voice quality,
encompassing phonation types such as creaky and breathy voice. These phonation
types are known to influence how listeners infer affective state, stance and
social meaning in speech. Existing benchmarks for speech understanding largely
rely on multiple-choice question answering (MCQA) formats, which are prone to
failure and therefore unreliable in capturing the nuanced ways paralinguistic
features influence model behaviour. In this paper, we probe SFMs through
open-ended generation tasks and speech emotion recognition, evaluating whether
model behaviours are consistent across different phonation inputs. We introduce
a new parallel dataset featuring synthesized modifications to voice quality,
designed to evaluate SFM responses to creaky and breathy voice. Our work
provides the first examination of SFM sensitivity to these particular
non-lexical aspects of speech perception.

</details>


### [312] [PitchFlower: A flow-based neural audio codec with pitch controllability](https://arxiv.org/abs/2510.25566)
*Diego Torres,Axel Roebel,Nicolas Obin*

Main category: eess.AS

TL;DR: 提出PitchFlower，一种基于流的神经音频编解码器，有显式音高可控性，实验表明其在音高控制和音频质量上表现出色，还可用于分离其他语音属性。


<details>
  <summary>Details</summary>
Motivation: 开发具有显式音高可控性的神经音频编解码器。

Method: 通过简单扰动实现解纠缠，训练时将F0轮廓展平并随机移动，以真实F0作为条件，使用向量量化瓶颈防止音高恢复，用基于流的解码器生成高质量音频。

Result: PitchFlower比WORLD有更准确的音高控制且音频质量更高，在可控性上优于SiFiGAN且质量相当。

Conclusion: 该框架为分离其他语音属性提供了简单且可扩展的途径。

Abstract: We present PitchFlower, a flow-based neural audio codec with explicit pitch
controllability. Our approach enforces disentanglement through a simple
perturbation: during training, F0 contours are flattened and randomly shifted,
while the true F0 is provided as conditioning. A vector-quantization bottleneck
prevents pitch recovery, and a flow-based decoder generates high quality audio.
Experiments show that PitchFlower achieves more accurate pitch control than
WORLD at much higher audio quality, and outperforms SiFiGAN in controllability
while maintaining comparable quality. Beyond pitch, this framework provides a
simple and extensible path toward disentangling other speech attributes.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [313] [CFL-SparseMed: Communication-Efficient Federated Learning for Medical Imaging with Top-k Sparse Updates](https://arxiv.org/abs/2510.24776)
*Gousia Habib,Aniket Bhardwaj,Ritvik Sharma,Shoeib Amin Banday,Ishfaq Ahmad Malik*

Main category: eess.IV

TL;DR: 提出CFL - SparseMed方法解决联邦学习在医学图像分类中的问题，提升效率、隐私性和诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 安全可靠的医学图像分类很重要，但集中式模型有数据和隐私问题，联邦学习存在非IID数据和高通信成本挑战。

Method: 提出CFL - SparseMed方法，使用Top - k稀疏化仅传输前k个梯度以减少通信开销。

Result: 该方法有效解决数据异质性问题，同时保持模型准确性。

Conclusion: 该方法提高了联邦学习效率、保护隐私，在非IID医学成像环境中提高诊断准确性和患者护理水平，代码可在Github获取。

Abstract: Secure and reliable medical image classification is crucial for effective
patient treatment, but centralized models face challenges due to data and
privacy concerns. Federated Learning (FL) enables privacy-preserving
collaborations but struggles with heterogeneous, non-IID data and high
communication costs, especially in large networks. We propose
\textbf{CFL-SparseMed}, an FL approach that uses Top-k Sparsification to reduce
communication overhead by transmitting only the top k gradients. This unified
solution effectively addresses data heterogeneity while maintaining model
accuracy. It enhances FL efficiency, preserves privacy, and improves diagnostic
accuracy and patient care in non-IID medical imaging settings. The
reproducibility source code is available on
\href{https://github.com/Aniket2241/APK_contruct}{Github}.

</details>


### [314] [DMVFC: Deep Learning Based Functionally Consistent Tractography Fiber Clustering Using Multimodal Diffusion MRI and Functional MRI](https://arxiv.org/abs/2510.24770)
*Bocheng Guo,Jin Wang,Yijie Li,Junyi Wang,Mingyu Gao,Puming Feng,Yuqian Chen,Jarrett Rushmore,Nikos Makris,Yogesh Rathi,Lauren J O'Donnell,Fan Zhang*

Main category: eess.IV

TL;DR: 本文提出新的深度学习纤维聚类框架DMVFC，结合多模态数据进行白质分割，实验显示性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前纤维聚类策略主要利用纤维几何特征，忽略功能和微观结构信息，而功能和微观结构信息对增强聚类功能一致性有价值。

Method: 开发了DMVFC框架，包含多视图预训练模块分别计算各信息源嵌入特征，以及协作微调模块同时细化嵌入差异。

Result: 将DMVFC与两种先进的纤维聚类方法比较，在实现具有功能意义和一致性的白质分割结果方面表现更优。

Conclusion: DMVFC能有效整合白质纤维的几何、微观结构特征与功能信号，实现功能一致的白质分割。

Abstract: Tractography fiber clustering using diffusion MRI (dMRI) is a crucial method
for white matter (WM) parcellation to enable analysis of brains structural
connectivity in health and disease. Current fiber clustering strategies
primarily use the fiber geometric characteristics (i.e., the spatial
trajectories) to group similar fibers into clusters, while neglecting the
functional and microstructural information of the fiber tracts. There is
increasing evidence that neural activity in the WM can be measured using
functional MRI (fMRI), providing potentially valuable multimodal information
for fiber clustering to enhance its functional coherence. Furthermore,
microstructural features such as fractional anisotropy (FA) can be computed
from dMRI as additional information to ensure the anatomical coherence of the
clusters. In this paper, we develop a novel deep learning fiber clustering
framework, namely Deep Multi-view Fiber Clustering (DMVFC), which uses joint
multi-modal dMRI and fMRI data to enable functionally consistent WM
parcellation. DMVFC can effectively integrate the geometric and microstructural
characteristics of the WM fibers with the fMRI BOLD signals along the fiber
tracts. DMVFC includes two major components: (1) a multi-view pretraining
module to compute embedding features from each source of information
separately, including fiber geometry, microstructure measures, and functional
signals, and (2) a collaborative fine-tuning module to simultaneously refine
the differences of embeddings. In the experiments, we compare DMVFC with two
state-of-the-art fiber clustering methods and demonstrate superior performance
in achieving functionally meaningful and consistent WM parcellation results.

</details>


### [315] [Transformers in Medicine: Improving Vision-Language Alignment for Medical Image Captioning](https://arxiv.org/abs/2510.25164)
*Yogesh Thakku Suresh,Vishwajeet Shivaji Hogale,Luca-Alexandru Zamfira,Anandavardhana Hegde*

Main category: eess.IV

TL;DR: 提出基于Transformer的多模态框架为MRI扫描生成临床相关描述，在MultiCaRe数据集上测试，表明特定领域数据可提升性能。


<details>
  <summary>Details</summary>
Motivation: 为MRI扫描生成临床相关的描述，提供自动化医学图像报告的解决方案。

Method: 结合DEiT - Small视觉Transformer作为图像编码器、MediCareBERT进行描述嵌入和自定义基于LSTM的解码器，使用混合余弦 - MSE损失和基于向量相似度的对比推理，在MultiCaRe数据集上进行基准测试。

Result: 聚焦特定领域数据能提高描述准确性和语义对齐度。

Conclusion: 提出了可扩展、可解释的自动化医学图像报告解决方案。

Abstract: We present a transformer-based multimodal framework for generating clinically
relevant captions for MRI scans. Our system combines a DEiT-Small vision
transformer as an image encoder, MediCareBERT for caption embedding, and a
custom LSTM-based decoder. The architecture is designed to semantically align
image and textual embeddings, using hybrid cosine-MSE loss and contrastive
inference via vector similarity. We benchmark our method on the MultiCaRe
dataset, comparing performance on filtered brain-only MRIs versus general MRI
images against state-of-the-art medical image captioning methods including
BLIP, R2GenGPT, and recent transformer-based approaches. Results show that
focusing on domain-specific data improves caption accuracy and semantic
alignment. Our work proposes a scalable, interpretable solution for automated
medical image reporting.

</details>


### [316] [Improving Temporal Consistency and Fidelity at Inference-time in Perceptual Video Restoration by Zero-shot Image-based Diffusion Models](https://arxiv.org/abs/2510.25420)
*Nasrin Rahimi,A. Murat Tekalp*

Main category: eess.IV

TL;DR: 本文提出两种免训练策略提升零样本视频恢复的时间一致性，经实验验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 扩散模型用于零样本视频恢复存在时间不一致问题，要在不重新训练或修改架构的情况下提升时间连贯性。

Method: 提出感知矫正引导（PSG）和多路径集成采样（MPES）两种推理时策略。

Result: PSG在时间模糊情况下增强时间自然度，MPES在所有任务中提升保真度和时空感知 - 失真权衡。

Conclusion: 两种免训练技术为使用预训练扩散模型进行时间稳定的高保真感知视频恢复提供了实用途径。

Abstract: Diffusion models have emerged as powerful priors for single-image
restoration, but their application to zero-shot video restoration suffers from
temporal inconsistencies due to the stochastic nature of sampling and
complexity of incorporating explicit temporal modeling. In this work, we
address the challenge of improving temporal coherence in video restoration
using zero-shot image-based diffusion models without retraining or modifying
their architecture. We propose two complementary inference-time strategies: (1)
Perceptual Straightening Guidance (PSG) based on the neuroscience-inspired
perceptual straightening hypothesis, which steers the diffusion denoising
process towards smoother temporal evolution by incorporating a curvature
penalty in a perceptual space to improve temporal perceptual scores, such as
Fr\'echet Video Distance (FVD) and perceptual straightness; and (2) Multi-Path
Ensemble Sampling (MPES), which aims at reducing stochastic variation by
ensembling multiple diffusion trajectories to improve fidelity (distortion)
scores, such as PSNR and SSIM, without sacrificing sharpness. Together, these
training-free techniques provide a practical path toward temporally stable
high-fidelity perceptual video restoration using large pretrained diffusion
models. We performed extensive experiments over multiple datasets and
degradation types, systematically evaluating each strategy to understand their
strengths and limitations. Our results show that while PSG enhances temporal
naturalness, particularly in case of temporal blur, MPES consistently improves
fidelity and spatio-temporal perception--distortion trade-off across all tasks.

</details>


### [317] [Physics-Guided Conditional Diffusion Networks for Microwave Image Reconstruction](https://arxiv.org/abs/2510.25729)
*Shirin Chehelgami,Joe LoVetri,Vahab Khoshdel*

Main category: eess.IV

TL;DR: 提出基于条件隐扩散的框架解决微波成像电磁逆散射问题，可生成多个可能解，结合正向电磁求解器，用新数据集训练取得高质量重建结果，显示混合生成物理框架潜力。


<details>
  <summary>Details</summary>
Motivation: 现有基于确定性机器学习的逆求解器只能产生单一重建结果，无法体现电磁逆散射问题的非唯一性，需要新方法解决。

Method: 引入基于条件隐扩散的框架，将正向电磁求解器集成到重建流程作为评估机制，使用合成和实验标记数据集训练和评估模型，创建新的标记合成数据集。

Result: 用新数据集训练模型可产生高质量介电常数重建结果，在形状识别上有出色保真度，提高泛化能力。

Conclusion: 混合生成物理框架是稳健的数据驱动微波成像的有前景方向。

Abstract: A conditional latent-diffusion based framework for solving the
electromagnetic inverse scattering problem associated with microwave imaging is
introduced. This generative machine-learning model explicitly mirrors the
non-uniqueness of the ill-posed inverse problem. Unlike existing inverse
solvers utilizing deterministic machine learning techniques that produce a
single reconstruction, the proposed latent-diffusion model generates multiple
plausible permittivity maps conditioned on measured scattered-field data,
thereby generating several potential instances in the range-space of the
non-unique inverse mapping. A forward electromagnetic solver is integrated into
the reconstruction pipeline as a physics-based evaluation mechanism. The space
of candidate reconstructions form a distribution of possibilities consistent
with the conditioning data and the member of this space yielding the lowest
scattered-field data discrepancy between the predicted and measured scattered
fields is reported as the final solution. Synthetic and experimental labeled
datasets are used for training and evaluation of the model. An innovative
labeled synthetic dataset is created that exemplifies a varied set of
scattering features. Training of the model using this new dataset produces high
quality permittivity reconstructions achieving improved generalization with
excellent fidelity to shape recognition. The results highlight the potential of
hybrid generative physics frameworks as a promising direction for robust,
data-driven microwave imaging.

</details>
