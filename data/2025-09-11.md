<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 14]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 66]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.SE](#cs.SE) [Total: 8]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [econ.GN](#econ.GN) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [hep-ph](#hep-ph) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.CV](#cs.CV) [Total: 17]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.SD](#cs.SD) [Total: 5]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.RO](#cs.RO) [Total: 9]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.HC](#cs.HC) [Total: 3]
- [stat.ME](#stat.ME) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [cs.SI](#cs.SI) [Total: 3]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 5]
- [math.OC](#math.OC) [Total: 4]
- [cs.IT](#cs.IT) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.CL](#cs.CL) [Total: 16]
- [eess.IV](#eess.IV) [Total: 7]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.CY](#cs.CY) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Reinforcement Learning Foundations for Deep Research Systems: A Survey](https://arxiv.org/abs/2509.06733)
*Wenjun Li,Zhi Chen,Jingru Lin,Hannan Cao,Wei Han,Sheng Liang,Zhi Zhang,Kuicai Dong,Dexun Li,Chen Zhang,Yong Liu*

Main category: cs.AI

TL;DR: 本文是首篇专注于深度研究系统强化学习基础的综述，系统梳理相关工作并提供训练指导。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究系统训练方法存在不足，如SFT有偏差、DPO依赖人类定义等，需强化学习优化。

Method: 沿数据合成与管理、代理研究的强化学习方法、代理强化学习训练系统和框架三个维度系统梳理工作。

Result: 提炼出常见模式，发现基础设施瓶颈。

Conclusion: 为使用强化学习训练强大、透明的深度研究代理提供实用指导。

Abstract: Deep research systems, agentic AI that solve complex, multi-step tasks by
coordinating reasoning, search across the open web and user files, and tool
use, are moving toward hierarchical deployments with a Planner, Coordinator,
and Executors. In practice, training entire stacks end-to-end remains
impractical, so most work trains a single planner connected to core tools such
as search, browsing, and code. While SFT imparts protocol fidelity, it suffers
from imitation and exposure biases and underuses environment feedback.
Preference alignment methods such as DPO are schema and proxy-dependent,
off-policy, and weak for long-horizon credit assignment and multi-objective
trade-offs. A further limitation of SFT and DPO is their reliance on human
defined decision points and subskills through schema design and labeled
comparisons. Reinforcement learning aligns with closed-loop, tool-interaction
research by optimizing trajectory-level policies, enabling exploration,
recovery behaviors, and principled credit assignment, and it reduces dependence
on such human priors and rater biases.
  This survey is, to our knowledge, the first dedicated to the RL foundations
of deep research systems. It systematizes work after DeepSeek-R1 along three
axes: (i) data synthesis and curation; (ii) RL methods for agentic research
covering stability, sample efficiency, long context handling, reward and credit
design, multi-objective optimization, and multimodal integration; and (iii)
agentic RL training systems and frameworks. We also cover agent architecture
and coordination, as well as evaluation and benchmarks, including recent QA,
VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We
distill recurring patterns, surface infrastructure bottlenecks, and offer
practical guidance for training robust, transparent deep research agents with
RL.

</details>


### [2] [Learning-Based Planning for Improving Science Return of Earth Observation Satellites](https://arxiv.org/abs/2509.07997)
*Abigail Breitfeld,Alberto Candela,Juan Delfa,Akseli Kangaslahti,Itai Zilberstein,Steve Chien,David Wettergreen*

Main category: cs.AI

TL;DR: 本文提出基于强化学习和模仿学习的动态目标定位方法，相比现有启发式方法有更好表现，且可用少量数据有效训练。


<details>
  <summary>Details</summary>
Motivation: 地球观测卫星存在轨道、视野和资源限制，需优化数据收集，动态目标定位是提升信息收集量的新兴概念。

Method: 提出基于强化学习和模仿学习的两种动态目标定位方法，构建在动态规划解决方案基础上规划采样位置序列。

Result: 模仿学习比最佳启发式方法平均好10.0%，强化学习平均好13.7%，两种学习方法可用少量数据有效训练。

Conclusion: 使用学习方法进行动态目标定位有优势。

Abstract: Earth observing satellites are powerful tools for collecting scientific
information about our planet, however they have limitations: they cannot easily
deviate from their orbital trajectories, their sensors have a limited field of
view, and pointing and operating these sensors can take a large amount of the
spacecraft's resources. It is important for these satellites to optimize the
data they collect and include only the most important or informative
measurements. Dynamic targeting is an emerging concept in which satellite
resources and data from a lookahead instrument are used to intelligently
reconfigure and point a primary instrument. Simulation studies have shown that
dynamic targeting increases the amount of scientific information gathered
versus conventional sampling strategies. In this work, we present two different
learning-based approaches to dynamic targeting, using reinforcement and
imitation learning, respectively. These learning methods build on a dynamic
programming solution to plan a sequence of sampling locations. We evaluate our
approaches against existing heuristic methods for dynamic targeting, showing
the benefits of using learning for this application. Imitation learning
performs on average 10.0\% better than the best heuristic method, while
reinforcement learning performs on average 13.7\% better. We also show that
both learning methods can be trained effectively with relatively small amounts
of data.

</details>


### [3] [EnvX: Agentize Everything with Agentic AI](https://arxiv.org/abs/2509.08088)
*Linyao Chen,Zimian Peng,Yingxuan Yang,Yikun Wang,Wenzheng Tom Tang,Hiroki H. Kobayashi,Weinan Zhang*

Main category: cs.AI

TL;DR: 现有开源仓库软件复用存在问题，本文提出EnvX框架，将GitHub仓库代理化，经三阶段处理，在GitTaskBench基准测试中表现优于现有框架，推动开源仓库向智能交互转变。


<details>
  <summary>Details</summary>
Motivation: 当前开源仓库软件复用存在手动、易出错且脱节的问题，开发者复用软件组件面临诸多障碍，需要解决软件高效复用问题。

Method: 提出EnvX框架，通过TODO引导的环境初始化、人类对齐的代理自动化、Agent - to - Agent（A2A）协议三个阶段将GitHub仓库代理化，结合大语言模型能力与结构化工具集成。

Result: 在GitTaskBench基准测试中，EnvX执行完成率达74.07%，任务通过率达51.85%，优于现有框架，案例研究证明其能实现多仓库协作。

Conclusion: EnvX使开源仓库从被动代码资源转变为智能交互代理，促进了开源生态系统的可访问性和协作性。

Abstract: The widespread availability of open-source repositories has led to a vast
collection of reusable software components, yet their utilization remains
manual, error-prone, and disconnected. Developers must navigate documentation,
understand APIs, and write integration code, creating significant barriers to
efficient software reuse. To address this, we present EnvX, a framework that
leverages Agentic AI to agentize GitHub repositories, transforming them into
intelligent, autonomous agents capable of natural language interaction and
inter-agent collaboration. Unlike existing approaches that treat repositories
as static code resources, EnvX reimagines them as active agents through a
three-phase process: (1) TODO-guided environment initialization, which sets up
the necessary dependencies, data, and validation datasets; (2) human-aligned
agentic automation, allowing repository-specific agents to autonomously perform
real-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple
agents to collaborate. By combining large language model capabilities with
structured tool integration, EnvX automates not just code generation, but the
entire process of understanding, initializing, and operationalizing repository
functionality. We evaluate EnvX on the GitTaskBench benchmark, using 18
repositories across domains such as image processing, speech recognition,
document analysis, and video manipulation. Our results show that EnvX achieves
a 74.07% execution completion rate and 51.85% task pass rate, outperforming
existing frameworks. Case studies further demonstrate EnvX's ability to enable
multi-repository collaboration via the A2A protocol. This work marks a shift
from treating repositories as passive code resources to intelligent,
interactive agents, fostering greater accessibility and collaboration within
the open-source ecosystem.

</details>


### [4] [Trust Semantics Distillation for Collaborator Selection via Memory-Augmented Agentic AI](https://arxiv.org/abs/2509.08151)
*Botao Zhu,Jeslyn Wang,Dusit Niyato,Xianbin Wang*

Main category: cs.AI

TL;DR: 提出基于大AI模型驱动的师生代理架构的2TSD模型用于协作设备信任评估，实验表明可减少评估时间、降低资源消耗并提高选择准确性。


<details>
  <summary>Details</summary>
Motivation: 独立评估潜在协作设备信任度存在高开销和评估效果差的问题。

Method: 构建基于大AI模型驱动的师生代理架构的2TSD模型，教师代理收集数据、提取语义和进行匹配分析，向学生代理传递信任语义。

Result: 2TSD模型能减少协作设备评估时间，降低设备资源消耗，提高协作设备选择准确性。

Conclusion: 2TSD模型可有效解决潜在协作设备信任评估中的问题，提升评估效率和准确性。

Abstract: Accurate trustworthiness evaluation of potential collaborating devices is
essential for the effective execution of complex computing tasks. This
evaluation process involves collecting diverse trust-related data from
potential collaborators, including historical performance and available
resources, for collaborator selection. However, when each task owner
independently assesses all collaborators' trustworthiness, frequent data
exchange, complex reasoning, and dynamic situation changes can result in
significant overhead and deteriorated trust evaluation. To overcome these
challenges, we propose a task-specific trust semantics distillation (2TSD)
model based on a large AI model (LAM)-driven teacher-student agent
architecture. The teacher agent is deployed on a server with powerful
computational capabilities and an augmented memory module dedicated to
multidimensional trust-related data collection, task-specific trust semantics
extraction, and task-collaborator matching analysis. Upon receiving
task-specific requests from device-side student agents, the teacher agent
transfers the trust semantics of potential collaborators to the student agents,
enabling rapid and accurate collaborator selection. Experimental results
demonstrate that the proposed 2TSD model can reduce collaborator evaluation
time, decrease device resource consumption, and improve the accuracy of
collaborator selection.

</details>


### [5] [Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following](https://arxiv.org/abs/2509.08222)
*Minjong Yoo,Jinwoo Jang,Wei-jin Park,Honguk Woo*

Main category: cs.AI

TL;DR: 提出ExRAP框架处理具身智能体在动态环境中的连续指令跟随任务，实验显示其性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 解决具身智能体在动态、非平稳环境中连续指令跟随任务的规划问题，增强大语言模型具身推理能力。

Method: 将指令分解为对环境上下文记忆的查询和基于查询结果的任务执行；在基于大语言模型的规划过程中引入基于信息的探索，实现探索集成任务规划方案；设计时间一致性细化方案进行查询评估。

Result: 通过在VirtualHome、ALFRED和CARLA上的实验，该方法在不同场景下展现出鲁棒性，在目标成功率和执行效率上均优于其他先进的基于大语言模型的任务规划方法。

Conclusion: ExRAP框架能有效处理具身智能体在动态环境中的连续指令跟随任务，平衡环境上下文记忆有效性和环境探索负载，提升整体任务性能。

Abstract: This study presents an Exploratory Retrieval-Augmented Planning (ExRAP)
framework, designed to tackle continual instruction following tasks of embodied
agents in dynamic, non-stationary environments. The framework enhances Large
Language Models' (LLMs) embodied reasoning capabilities by efficiently
exploring the physical environment and establishing the environmental context
memory, thereby effectively grounding the task planning process in time-varying
environment contexts. In ExRAP, given multiple continual instruction following
tasks, each instruction is decomposed into queries on the environmental context
memory and task executions conditioned on the query results. To efficiently
handle these multiple tasks that are performed continuously and simultaneously,
we implement an exploration-integrated task planning scheme by incorporating
the {information-based exploration} into the LLM-based planning process.
Combined with memory-augmented query evaluation, this integrated scheme not
only allows for a better balance between the validity of the environmental
context memory and the load of environment exploration, but also improves
overall task performance. Furthermore, we devise a {temporal consistency
refinement} scheme for query evaluation to address the inherent decay of
knowledge in the memory. Through experiments with VirtualHome, ALFRED, and
CARLA, our approach demonstrates robustness against a variety of embodied
instruction following scenarios involving different instruction scales and
types, and non-stationarity degrees, and it consistently outperforms other
state-of-the-art LLM-based task planning approaches in terms of both goal
success rate and execution efficiency.

</details>


### [6] [Real-world Music Plagiarism Detection With Music Segment Transcription System](https://arxiv.org/abs/2509.08282)
*Seonghyeon Go*

Main category: cs.AI

TL;DR: 本文结合MIR技术提出音乐抄袭检测系统，提取音乐片段并计算相似度，实验效果好，还公开数据集。


<details>
  <summary>Details</summary>
Motivation: 随着MIR技术发展，音乐创作和传播更多样，对音乐知识产权保护的需求增加。

Method: 开发音乐片段转录系统，从音频中提取有意义片段，基于多个音乐特征计算相似度得分。

Result: 音乐抄袭检测实验有良好表现，可应用于实际音乐场景，还收集公开SMP数据集。

Conclusion: 提出的方法在音乐抄袭检测上有效果，且可用于实际场景。

Abstract: As a result of continuous advances in Music Information Retrieval (MIR)
technology, generating and distributing music has become more diverse and
accessible. In this context, interest in music intellectual property protection
is increasing to safeguard individual music copyrights. In this work, we
propose a system for detecting music plagiarism by combining various MIR
technologies. We developed a music segment transcription system that extracts
musically meaningful segments from audio recordings to detect plagiarism across
different musical formats. With this system, we compute similarity scores based
on multiple musical features that can be evaluated through comprehensive
musical analysis. Our approach demonstrated promising results in music
plagiarism detection experiments, and the proposed method can be applied to
real-world music scenarios. We also collected a Similar Music Pair (SMP)
dataset for musical similarity research using real-world cases. The dataset are
publicly available.

</details>


### [7] [Leveraging AI Agents for Autonomous Networks: A Reference Architecture and Empirical Studies](https://arxiv.org/abs/2509.08312)
*Binghan Wu,Shoufeng Wang,Yunxin Liu,Ya-Qin Zhang,Joseph Sifakis,Ye Ouyang*

Main category: cs.AI

TL;DR: 本文实现AN Agent参考架构构建认知系统，通过RAN LA Agent案例验证其潜力，展示出实时控制和性能提升效果。


<details>
  <summary>Details</summary>
Motivation: 电信网络向L4自主网络演进需实现认知能力，弥合架构理论与运营现实的差距。

Method: 实现Joseph Sifakis的AN Agent参考架构于功能认知系统，部署由混合知识表示驱动的主动 - 被动协调运行时。

Result: 在5G NR sub - 6 GHz中实现亚10毫秒实时控制，下行吞吐量比OLLA算法高6%，超可靠服务的BLER降低67%。

Conclusion: 该架构可行，能克服传统自主障碍，推进关键L4能力以实现下一代目标。

Abstract: The evolution toward Level 4 (L4) Autonomous Networks (AN) represents a
strategic inflection point in telecommunications, where networks must transcend
reactive automation to achieve genuine cognitive capabilities--fulfilling TM
Forum's vision of self-configuring, self-healing, and self-optimizing systems
that deliver zero-wait, zero-touch, and zero-fault services. This work bridges
the gap between architectural theory and operational reality by implementing
Joseph Sifakis's AN Agent reference architecture in a functional cognitive
system, deploying coordinated proactive-reactive runtimes driven by hybrid
knowledge representation. Through an empirical case study of a Radio Access
Network (RAN) Link Adaptation (LA) Agent, we validate this framework's
transformative potential: demonstrating sub-10 ms real-time control in 5G NR
sub-6 GHz while achieving 6% higher downlink throughput than Outer Loop Link
Adaptation (OLLA) algorithms and 67% Block Error Rate (BLER) reduction for
ultra-reliable services through dynamic Modulation and Coding Scheme (MCS)
optimization. These improvements confirm the architecture's viability in
overcoming traditional autonomy barriers and advancing critical L4-enabling
capabilities toward next-generation objectives.

</details>


### [8] [Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML Compliance Narratives](https://arxiv.org/abs/2509.08380)
*Prathamesh Vasudeo Naik,Naresh Kumar Dintakurthi,Zhanghao Hu,Yue Wang,Robby Qiu*

Main category: cs.AI

TL;DR: 本文提出Co - Investigator AI框架以优化可疑活动报告（SAR）生成，比传统方法更快、更准确，能融合AI与人力，开启合规报告新时代。


<details>
  <summary>Details</summary>
Motivation: 生成合规SAR在反洗钱工作流中存在高成本、低可扩展性的瓶颈，大语言模型在合规关键领域有不可接受的风险。

Method: 借鉴自主代理架构，集成规划、犯罪类型检测、外部情报收集和合规验证的专业代理，具备动态内存管理、AI隐私保护层和实时验证代理，采用人机协作工作流。

Result: 展示了Co - Investigator AI在多种复杂金融犯罪场景中的通用性，能简化SAR起草。

Conclusion: 该方法开启了合规报告新时代，为可扩展、可靠和透明的SAR生成铺平道路。

Abstract: Generating regulatorily compliant Suspicious Activity Report (SAR) remains a
high-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows.
While large language models (LLMs) offer promising fluency, they suffer from
factual hallucination, limited crime typology alignment, and poor
explainability -- posing unacceptable risks in compliance-critical domains.
This paper introduces Co-Investigator AI, an agentic framework optimized to
produce Suspicious Activity Reports (SARs) significantly faster and with
greater accuracy than traditional methods. Drawing inspiration from recent
advances in autonomous agent architectures, such as the AI Co-Scientist, our
approach integrates specialized agents for planning, crime type detection,
external intelligence gathering, and compliance validation. The system features
dynamic memory management, an AI-Privacy Guard layer for sensitive data
handling, and a real-time validation agent employing the Agent-as-a-Judge
paradigm to ensure continuous narrative quality assurance. Human investigators
remain firmly in the loop, empowered to review and refine drafts in a
collaborative workflow that blends AI efficiency with domain expertise. We
demonstrate the versatility of Co-Investigator AI across a range of complex
financial crime scenarios, highlighting its ability to streamline SAR drafting,
align narratives with regulatory expectations, and enable compliance teams to
focus on higher-order analytical work. This approach marks the beginning of a
new era in compliance reporting -- bringing the transformative benefits of AI
agents to the core of regulatory processes and paving the way for scalable,
reliable, and transparent SAR generation.

</details>


### [9] [TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making](https://arxiv.org/abs/2509.08500)
*Kechen Jiao,Zhirui Fang,Jiahao Liu,Bei Li,Qifan Wang,Xinyu Liu,Junhao Ruan,Zhongjian Qiao,Yifan Zhu,Yaxin Xu,Jingang Wang,Xiu Li*

Main category: cs.AI

TL;DR: 文章指出视觉语言模型在具身人工智能动态任务中存在问题，提出Thought - Centric Preference Optimization (TCPO)方法，实验证明其有效性


<details>
  <summary>Details</summary>
Motivation: 现有监督微调模型在动态环境中响应迟缓、有幻觉问题，现有后SFT方法存在样本效率低、一致性差和模型退化问题，需进一步优化

Method: 提出TCPO方法，采用基于逐步偏好的优化方法将稀疏奖励信号转化为更丰富的步骤样本对，强调模型中间推理过程的对齐，引入Action Policy Consistency Constraint (APC)对模型输出施加一致性约束

Result: 在ALFWorld环境实验中平均成功率达26.67%，比RL4VLM提高6%，缓解了微调后模型退化问题

Conclusion: 基于偏好的学习技术与CoT过程结合，可提升具身智能体中视觉语言模型的决策能力

Abstract: Using effective generalization capabilities of vision language models (VLMs)
in context-specific dynamic tasks for embodied artificial intelligence remains
a significant challenge. Although supervised fine-tuned models can better align
with the real physical world, they still exhibit sluggish responses and
hallucination issues in dynamically changing environments, necessitating
further alignment. Existing post-SFT methods, reliant on reinforcement learning
and chain-of-thought (CoT) approaches, are constrained by sparse rewards and
action-only optimization, resulting in low sample efficiency, poor consistency,
and model degradation. To address these issues, this paper proposes
Thought-Centric Preference Optimization (TCPO) for effective embodied
decision-making. Specifically, TCPO introduces a stepwise preference-based
optimization approach, transforming sparse reward signals into richer step
sample pairs. It emphasizes the alignment of the model's intermediate reasoning
process, mitigating the problem of model degradation. Moreover, by
incorporating Action Policy Consistency Constraint (APC), it further imposes
consistency constraints on the model output. Experiments in the ALFWorld
environment demonstrate an average success rate of 26.67%, achieving a 6%
improvement over RL4VLM and validating the effectiveness of our approach in
mitigating model degradation after fine-tuning. These results highlight the
potential of integrating preference-based learning techniques with CoT
processes to enhance the decision-making capabilities of vision-language models
in embodied agents.

</details>


### [10] [No-Knowledge Alarms for Misaligned LLMs-as-Judges](https://arxiv.org/abs/2509.08593)
*Andrés Corrada-Emmanuel*

Main category: cs.AI

TL;DR: 利用LLM评判LLM决策时存在评估不确定性，可利用专家间逻辑一致性改善，通过线性规划问题开发无知识警报检测评判者异常。


<details>
  <summary>Details</summary>
Motivation: 使用LLM评判其他LLM决策时，在不知专家决策真相且不想完全信任他们的情况下，存在评估不确定性，需要解决。

Method: 观察LLM评判者在评判其他LLM时的意见分歧，将逻辑关系形式化为线性规划问题，开发无知识警报。

Result: 开发的警报能无假阳性地检测出评判者集合中至少有一个成员违反用户指定的评分能力要求。

Conclusion: 利用逻辑一致性解决LLM评判中的评估不确定性是可行的，开发的警报可有效检测评判者异常。

Abstract: If we use LLMs as judges to evaluate the complex decisions of other LLMs, who
or what monitors the judges? Infinite monitoring chains are inevitable whenever
we do not know the ground truth of the decisions by experts and we do not want
to trust them. One way to ameliorate our evaluation uncertainty is to exploit
the use of logical consistency between disagreeing experts. By observing how
LLM judges agree and disagree while grading other LLMs, we can compute the only
possible evaluations of their grading ability. For example, if two LLM judges
disagree on which tasks a third one completed correctly, they cannot both be
100\% correct in their judgments. This logic can be formalized as a Linear
Programming problem in the space of integer response counts for any finite
test. We use it here to develop no-knowledge alarms for misaligned LLM judges.
The alarms can detect, with no false positives, that at least one member or
more of an ensemble of judges are violating a user specified grading ability
requirement.

</details>


### [11] [Automatic Failure Attribution and Critical Step Prediction Method for Multi-Agent Systems Based on Causal Inference](https://arxiv.org/abs/2509.08682)
*Guoqing Ma,Jia Zhu,Hanghui Guo,Weijie Shi,Jiawei Shen,Jingjiang Liu,Yidan Liang*

Main category: cs.AI

TL;DR: 提出基于多粒度因果推理的多智能体系统故障归因框架，在基准测试中表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统故障归因诊断工具依赖统计相关性，在基准测试中定位故障根源步骤的准确率低，需解决此问题。

Method: 引入性能因果反转原则结合Shapley值进行智能体级别的责任分配，提出因果发现算法CDC - MAS识别关键故障步骤，归因结果驱动自动化优化循环。

Result: 在Who&When和TRAIL基准测试中，方法达到36.2%的步骤级准确率，生成的优化方案使整体任务成功率平均提高22.4%。

Conclusion: 该工作为调试复杂智能体交互提供了有效解决方案，有助于构建更可靠和可解释的多智能体系统。

Abstract: Multi-agent systems (MAS) are critical for automating complex tasks, yet
their practical deployment is severely hampered by the challenge of failure
attribution. Current diagnostic tools, which rely on statistical correlations,
are fundamentally inadequate; on challenging benchmarks like Who\&When,
state-of-the-art methods achieve less than 15\% accuracy in locating the
root-cause step of a failure. To address this critical gap, we introduce the
first failure attribution framework for MAS grounded in multi-granularity
causal inference. Our approach makes two key technical contributions: (1) a
performance causal inversion principle, which correctly models performance
dependencies by reversing the data flow in execution logs, combined with
Shapley values to accurately assign agent-level blame; (2) a novel causal
discovery algorithm, CDC-MAS, that robustly identifies critical failure steps
by tackling the non-stationary nature of MAS interaction data. The framework's
attribution results directly fuel an automated optimization loop, generating
targeted suggestions whose efficacy is validated via counterfactual
simulations. Evaluations on the Who\&When and TRAIL benchmarks demonstrate a
significant leap in performance. Our method achieves up to 36.2\% step-level
accuracy. Crucially, the generated optimizations boost overall task success
rates by an average of 22.4\%. This work provides a principled and effective
solution for debugging complex agent interactions, paving the way for more
reliable and interpretable multi-agent systems.

</details>


### [12] [One Model, Two Minds: A Context-Gated Graph Learner that Recreates Human Biases](https://arxiv.org/abs/2509.08705)
*Shalima Binta Manir,Tim Oates*

Main category: cs.AI

TL;DR: 提出受认知科学双过程理论启发的心智理论框架，结合快速和慢速系统，验证其在虚假信念任务上的表现，结果表明该方法能模拟人类行为、实现泛化并阐明推理偏差机制，为AI社会认知和决策能力发展铺路。


<details>
  <summary>Details</summary>
Motivation: 构建能展现细致、类人社会认知和自适应决策能力的AI系统，将人工智能与认知理论相连接。

Method: 引入受双过程理论启发的心智理论框架，集成基于图卷积网络的快速推理系统和基于元学习的慢速自适应学习系统，通过上下文门机制动态平衡推理。

Result: 该双过程方法能紧密模拟人类自适应行为，对未见上下文有强大泛化能力，阐明推理偏差的认知机制。

Conclusion: 此工作为AI系统具备类人社会认知和自适应决策能力奠定基础。

Abstract: We introduce a novel Theory of Mind (ToM) framework inspired by dual-process
theories from cognitive science, integrating a fast, habitual graph-based
reasoning system (System 1), implemented via graph convolutional networks
(GCNs), and a slower, context-sensitive meta-adaptive learning system (System
2), driven by meta-learning techniques. Our model dynamically balances
intuitive and deliberative reasoning through a learned context gate mechanism.
We validate our architecture on canonical false-belief tasks and systematically
explore its capacity to replicate hallmark cognitive biases associated with
dual-process theory, including anchoring, cognitive-load fatigue, framing
effects, and priming effects. Experimental results demonstrate that our
dual-process approach closely mirrors human adaptive behavior, achieves robust
generalization to unseen contexts, and elucidates cognitive mechanisms
underlying reasoning biases. This work bridges artificial intelligence and
cognitive theory, paving the way for AI systems exhibiting nuanced, human-like
social cognition and adaptive decision-making capabilities.

</details>


### [13] [The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems](https://arxiv.org/abs/2509.08713)
*Ziming Luo,Atoosa Kasirzadeh,Nihar B. Shah*

Main category: cs.AI

TL;DR: 本文识别当代AI科学家系统4种潜在失败模式，通过实验评估发现系统存在不同程度失败，建议评估AI生成研究时除论文外还应提交相关工件。


<details>
  <summary>Details</summary>
Motivation: AI科学家系统内部工作流程未被仔细审查，可能引入缺陷，影响研究输出质量。

Method: 识别4种失败模式，设计控制实验分离各模式并应对评估挑战。

Result: 评估两个开源AI科学家系统发现多种不同严重程度的失败，且完整自动化工作流的跟踪日志和代码比仅检查论文更能有效检测失败。

Conclusion: 建议期刊和会议评估AI生成研究时，要求提交跟踪日志和代码等工件以确保透明、可问责和可复现。

Abstract: AI scientist systems, capable of autonomously executing the full research
workflow from hypothesis generation and experimentation to paper writing, hold
significant potential for accelerating scientific discovery. However, the
internal workflow of these systems have not been closely examined. This lack of
scrutiny poses a risk of introducing flaws that could undermine the integrity,
reliability, and trustworthiness of their research outputs. In this paper, we
identify four potential failure modes in contemporary AI scientist systems:
inappropriate benchmark selection, data leakage, metric misuse, and post-hoc
selection bias. To examine these risks, we design controlled experiments that
isolate each failure mode while addressing challenges unique to evaluating AI
scientist systems. Our assessment of two prominent open-source AI scientist
systems reveals the presence of several failures, across a spectrum of
severity, which can be easily overlooked in practice. Finally, we demonstrate
that access to trace logs and code from the full automated workflow enables far
more effective detection of such failures than examining the final paper alone.
We thus recommend journals and conferences evaluating AI-generated research to
mandate submission of these artifacts alongside the paper to ensure
transparency, accountability, and reproducibility.

</details>


### [14] [Narrative-Guided Reinforcement Learning: A Platform for Studying Language Model Influence on Decision Making](https://arxiv.org/abs/2509.08785)
*Anup Tuladhar,Araz Minhas,Adam Kirton,Eli Kinney-Lang*

Main category: cs.AI

TL;DR: 本文提出结合强化学习与语言模型推理的实验平台，探索叙事元素对AI决策的影响，在可配置环境中实现并便于控制测试和数据记录。


<details>
  <summary>Details</summary>
Motivation: 当前AI决策和叙事推理能力多分开研究，需要搭建平台探索叙事框架对基于奖励学习的影响。

Method: 采用双系统架构，结合强化学习策略和语言模型，在可配置网格世界环境中实现，并通过日志系统记录决策指标。

Result: 实现了初步的实验平台，可进行叙事元素实验，便于控制测试和数据记录。

Conclusion: 该实现为研究不同叙事框架对基于奖励决策的影响，以及探索优化学习和符号推理在AI系统中的潜在交互提供基础。

Abstract: We present a preliminary experimental platform that explores how narrative
elements might shape AI decision-making by combining reinforcement learning
(RL) with language model reasoning. While AI systems can now both make
decisions and engage in narrative reasoning, these capabilities have mostly
been studied separately. Our platform attempts to bridge this gap using a
dual-system architecture to examine how narrative frameworks could influence
reward-based learning. The system comprises a reinforcement learning policy
that suggests actions based on past experience, and a language model that
processes these suggestions through different narrative frameworks to guide
decisions. This setup enables initial experimentation with narrative elements
while maintaining consistent environment and reward structures. We implement
this architecture in a configurable gridworld environment, where agents receive
both policy suggestions and information about their surroundings. The
platform's modular design facilitates controlled testing of environmental
complexity, narrative parameters, and the interaction between reinforcement
learning and narrative-based decisions. Our logging system captures basic
decision metrics, from RL policy values to language model reasoning to action
selection patterns. While preliminary, this implementation provides a
foundation for studying how different narrative frameworks might affect
reward-based decisions and exploring potential interactions between
optimization-based learning and symbolic reasoning in AI systems.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [15] [Hybrid Physics-Data Enrichments to Represent Uncertainty in Reduced Gas-Surface Chemistry Models for Hypersonic Flight](https://arxiv.org/abs/2509.08137)
*Rileigh Bandy,Rebecca Morrison,Erin Mussoni,Teresa Portone*

Main category: cs.CE

TL;DR: 本文提出基于物理和数据驱动的混合增强方法，改进高超声速飞行热防护系统低保真烧蚀模型的预测能力并量化不确定性，数值结果显示仅增加三个反应就能显著提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 高超声速飞行中热防护系统（TPS）烧蚀模型需兼顾计算可处理性和准确性，低保真模型因省略部分反应存在预测差异，需改进。

Method: 开发基于物理和数据驱动的混合增强方法，将针对性增强嵌入低保真模型以捕捉被省略反应的影响。

Result: 数值结果表明，混合增强方法仅增加三个反应，就能显著提高预测精度。

Conclusion: 所提出的混合增强方法能在保持计算可处理性的同时，提高低保真模型的预测能力并量化不确定性。

Abstract: During hypersonic flight, air reacts with a planetary re-entry vehicle's
thermal protection system (TPS), creating reaction products that deplete the
TPS. Reliable assessment of TPS performance depends on accurate ablation
models. New finite-rate gas-surface chemistry models are advancing
state-of-the-art in TPS ablation modeling, but model reductions that omit
chemical species and reactions may be necessary in some cases for computational
tractability. This work develops hybrid physics-based and data-driven
enrichments to improve the predictive capability and quantify uncertainties in
such low-fidelity models while maintaining computational tractability. We focus
on discrepancies in predicted carbon monoxide production that arise because the
low-fidelity model tracks only a subset of reactions. To address this, we embed
targeted enrichments into the low-fidelity model to capture the influence of
omitted reactions. Numerical results show that the hybrid enrichments
significantly improve predictive accuracy while requiring the addition of only
three reactions.

</details>


### [16] [Solving contact problems using Fiber Monte Carlo](https://arxiv.org/abs/2509.08609)
*Xinyu Wang,Weipeng Xu,Tianju Xue*

Main category: cs.CE

TL;DR: 提出新接触算法，基于接触体积能量函数梯度计算接触力，独立于网格一致性，可用于多种接触场景。


<details>
  <summary>Details</summary>
Motivation: 准确高效解决复杂接触问题具有挑战性，需要新的接触算法。

Method: 借鉴Fiber Monte Carlo方法，基于复杂几何体重叠体积计算接触力，框架独立于网格一致性，去除显式互补约束。

Result: 通过多种数值算例验证算法，涵盖不同维度、变形和材料行为的接触场景。

Conclusion: 新算法能有效处理多种接触问题，可融入现有数值求解器。

Abstract: Computational modeling of contact is fundamental to many engineering
applications, yet accurately and efficiently solving complex contact problems
remains challenging. In this work, we propose a new contact algorithm that
computes contact forces by taking the gradient of an energy function of the
contact volume (overlap) with respect to the geometry descriptors. While
elegant in concept, evaluating this gradient is non-trivial due to the
arbitrary geometry of the contact region. Inspired by the recently proposed
Fiber Monte Carlo (FMC) method, we develop an algorithm that accurately
computes contact forces based on the overlap volume between bodies with complex
geometries. Our computational framework operates independently of mesh
conformity, eliminating the need for master-slave identification and projection
iterations, thus handling arbitrary discretizations. Moreover, by removing
explicit complementarity constraints, the method retains a simple structure
that can be easily incorporated into existing numerical solvers, such as the
finite element method. In this paper, numerical examples cover a wide range of
contact scenarios, from classical small-deformation static contact to complex
large-deformation dynamic contact in both two- and three-dimensional settings
with nonlinear material behavior. These cases include Hertzian contact for
small-deformation verification; contact between wedge- and cone-shaped bodies
to assess pressure and displacement predictions at non-smooth boundaries;
contact involving Neo-Hookean hyperelastic materials for evaluating nonlinear
responses under finite deformation; and dynamic collision cases to examine
transient behavior.

</details>


### [17] [Quantifying model prediction sensitivity to model-form uncertainty](https://arxiv.org/abs/2509.08708)
*Teresa Portone,Rebekah D. White,Joseph L. Hart*

Main category: cs.CE

TL;DR: 提出量化模型假设相关不确定性重要性的新方法，可在无校准数据时应用，有数据时也能发挥作用。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以量化超出可用数据外预测中的模型形式不确定性（MFU），难以判断其在实践中的重要性及与其他不确定性来源的相对重要性，难以分配资源降低模型预测误差。

Method: 将对假设的参数化修改（MFU 表示）与基于分组方差的敏感性分析相结合来衡量假设的重要性。

Result: 与现有处理 MFU 的方法相比，该方法可在无校准数据时应用；若有校准数据，可用于为 MFU 表示提供信息，且基于方差的敏感性分析在参数存在依赖时仍可有效应用。

Conclusion: 提出的新方法有效解决了量化 MFU 重要性的难题，在有无校准数据的情况下都有良好应用效果。

Abstract: Model-form uncertainty (MFU) in assumptions made during physics-based model
development is widely considered a significant source of uncertainty; however,
there are limited approaches that can quantify MFU in predictions extrapolating
beyond available data. As a result, it is challenging to know how important MFU
is in practice, especially relative to other sources of uncertainty in a model,
making it difficult to prioritize resources and efforts to drive down error in
model predictions. To address these challenges, we present a novel method to
quantify the importance of uncertainties associated with model assumptions. We
combine parameterized modifications to assumptions (called MFU representations)
with grouped variance-based sensitivity analysis to measure the importance of
assumptions. We demonstrate how, in contrast to existing methods addressing
MFU, our approach can be applied without access to calibration data. However,
if calibration data is available, we demonstrate how it can be used to inform
the MFU representation, and how variance-based sensitivity analysis can be
meaningfully applied even in the presence of dependence between parameters (a
common byproduct of calibration).

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [18] [Polyglot Persistence in Microservices: Managing Data Diversity in Distributed Systems](https://arxiv.org/abs/2509.08014)
*Festim Halili,Anila Nuhiji,Diellza Mustafai Veliu*

Main category: cs.DB

TL;DR: 本文探讨微服务系统中的多语言持久化，结合理论与实践，对比不同数据库，分析采用趋势与挑战，提出应对架构模式。


<details>
  <summary>Details</summary>
Motivation: 微服务架构管理异构和分布式数据存在挑战，多语言持久化是实用解决方案，需对其进行研究。

Method: 结合理论概念与实践证据，应用比较框架评估不同类型数据库，采用行业案例和调查数据。

Result: 多语言持久化可提高适应性、性能和领域一致性，但增加治理和运营复杂性。

Conclusion: 讨论了如saga工作流、事件溯源和outbox集成等架构模式来应对多语言持久化带来的权衡问题。

Abstract: Microservices architectures have become the foundation for developing
scalable and modern software systems, but they also bring significant
challenges in managing heterogeneous and distributed data. The pragmatic
solution is polyglot persistence, the deliberate use of several different
database technologies adapted to a given microservice requirement - is one such
strategy. This paper examines polyglot persistence in microservice based
systems. This paper brings together theoretical concepts with evidence from
practical implementations and comparative benchmarks of standard database
platforms. A comparative framework is applied to relational, document,
key-value, column-family and graph databases to assess scalability,
consistency, query expressiveness, operational overhead and integration ease.
Empirical data drawn from industry case studies such as Netflix, Uber, and
Shopify, and survey data illustrate real-life adoption trends and challenges.
These findings demonstrate that polyglot persistence increases adaptability ,
performance , domain alignment but also governance or operational complexity.
To cope with such trade-offs, architectural patterns such as saga workflows,
event sourcing, and outbox integration are discussed.

</details>


### [19] [Infinite Stream Estimation under Personalized $w$-Event Privacy](https://arxiv.org/abs/2509.08387)
*Leilei Du,Peng Cheng,Lei Chen,Heng Tao Shen,Xuemin Lin,Wei Xi*

Main category: cs.DB

TL;DR: 提出个性化w - 事件隐私保护，设计PWSM机制，提出PBD和PBA解决方案，证明其优于现有方法并在数据集上验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有无限数据流的w - 事件隐私研究多关注所有用户的同质隐私需求，需实现不同用户不同隐私要求的个性化保护。

Method: 设计PWSM机制，提出PBD和PBA两种满足(w, ε)-EPDP的解决方案。

Result: PBD和PBA优于现有私有流估计方法，PBD在真实数据集上比BD平均误差低68%，PBA在合成数据集上比BA平均误差低24.9%。

Conclusion: PBD和PBA能在满足所有用户隐私要求的同时，有效准确地估计流数据统计信息。

Abstract: Streaming data collection is indispensable for stream data analysis, such as
event monitoring. However, publishing these data directly leads to privacy
leaks. $w$-event privacy is a valuable tool to protect individual privacy
within a given time window while maintaining high accuracy in data collection.
Most existing $w$-event privacy studies on infinite data stream only focus on
homogeneous privacy requirements for all users. In this paper, we propose
personalized $w$-event privacy protection that allows different users to have
different privacy requirements in private data stream estimation. Specifically,
we design a mechanism that allows users to maintain constant privacy
requirements at each time slot, namely Personalized Window Size Mechanism
(PWSM). Then, we propose two solutions to accurately estimate stream data
statistics while achieving $w$-event level $\epsilon$ personalized differential
privacy ( ($w$, $\epsilon$)-EPDP), namely Personalized Budget Distribution
(PBD) and Peronalized Budget Absorption (PBA). PBD always provides at least the
same privacy budget for the next time step as the amount consumed in the
previous release. PBA fully absorbs the privacy budget from the previous $k$
time slots, while also borrowing from the privacy budget of the next $k$ time
slots, to increase the privacy budget for the current time slot. We prove that
both PBD and PBA outperform the state-of-the-art private stream estimation
methods while satisfying the privacy requirements of all users. We demonstrate
the efficiency and effectiveness of our PBD and PBA on both real and synthetic
data sets, compared with the recent uniformity $w$-event approaches, Budget
Distribution (BD) and Budget Absorption (BA). Our PBD achieves 68% less error
than BD on average on real data sets. Besides, our PBA achieves 24.9% less
error than BA on average on synthetic data sets.

</details>


### [20] [SINDI: an Efficient Index for Approximate Maximum Inner Product Search on Sparse Vectors](https://arxiv.org/abs/2509.08395)
*Ruoxuan Li,Xiaoyao Zhong,Jiabao Jin,Peng Cheng,Wangze Ni,Lei Chen,Zhitao Shen,Wei Jia,Xiangyu Wang,Xuemin Lin,Heng Tao Shen,Jingkuan Song*

Main category: cs.DB

TL;DR: 本文提出稀疏倒排非冗余距离索引SINDI用于稀疏向量MIPS，经多数据集评估表现优异并集成到开源库。


<details>
  <summary>Details</summary>
Motivation: 现有基于倒排索引和图的算法在生产环境中因冗余距离计算、频繁随机内存访问受限，且稀疏向量压缩存储格式阻碍SIMD加速。

Method: 提出SINDI，包括高效内积计算、内存友好设计、向量剪枝三项关键优化。

Result: 在多真实数据集上达到了最优性能，在MsMarco数据集上相比SEISMIC和PyANNs有显著QPS提升。

Conclusion: SINDI性能优越，已集成到Ant Group的开源向量搜索库VSAG。

Abstract: Sparse vector Maximum Inner Product Search (MIPS) is crucial in multi-path
retrieval for Retrieval-Augmented Generation (RAG). Recent inverted index-based
and graph-based algorithms have achieved high search accuracy with practical
efficiency. However, their performance in production environments is often
limited by redundant distance computations and frequent random memory accesses.
Furthermore, the compressed storage format of sparse vectors hinders the use of
SIMD acceleration. In this paper, we propose the sparse inverted non-redundant
distance index (SINDI), which incorporates three key optimizations: (i)
Efficient Inner Product Computation: SINDI leverages SIMD acceleration and
eliminates redundant identifier lookups, enabling batched inner product
computation; (ii) Memory-Friendly Design: SINDI replaces random memory accesses
to original vectors with sequential accesses to inverted lists, substantially
reducing memory-bound latency. (iii) Vector Pruning: SINDI retains only the
high-magnitude non-zero entries of vectors, improving query throughput while
maintaining accuracy. We evaluate SINDI on multiple real-world datasets.
Experimental results show that SINDI achieves state-of-the-art performance
across datasets of varying scales, languages, and models. On the MsMarco
dataset, when Recall@50 exceeds 99%, SINDI delivers single-thread
query-per-second (QPS) improvements ranging from 4.2 to 26.4 times compared
with SEISMIC and PyANNs. Notably, SINDI has been integrated into Ant Group's
open-source vector search library, VSAG.

</details>


### [21] [Un cadre paraconsistant pour l'{é}valuation de similarit{é} dans les bases de connaissances](https://arxiv.org/abs/2509.08433)
*José-Luis Vilchis Medina*

Main category: cs.DB

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This article proposes a paraconsistent framework for evaluating similarity in
knowledge bases. Unlike classical approaches, this framework explicitly
integrates contradictions, enabling a more robust and interpretable similarity
measure. A new measure $ S^* $ is introduced, which penalizes inconsistencies
while rewarding shared properties. Paraconsistent super-categories $ \Xi_K^* $
are defined to hierarchically organize knowledge entities. The model also
includes a contradiction extractor $ E $ and a repair mechanism, ensuring
consistency in the evaluations. Theoretical results guarantee reflexivity,
symmetry, and boundedness of $ S^* $. This approach offers a promising solution
for managing conflicting knowledge, with perspectives in multi-agent systems.

</details>


### [22] [SQLGovernor: An LLM-powered SQL Toolkit for Real World Application](https://arxiv.org/abs/2509.08575)
*Jie Jiang,Siqi Shen,Haining Xie,Yang Li,Yu Shen,Danqing Huang,Bo Qian,Yinjun Wu,Wentao Zhang,Bin Cui,Peng Chen*

Main category: cs.DB

TL;DR: 提出SQLGovernor工具解决SQL查询问题，采用片段处理和混合自学习机制，实验显示能提升模型性能，有实用价值。


<details>
  <summary>Details</summary>
Motivation: 解决现实分析环境中SQL查询存在的语法错误、低效和语义不一致问题，特别是复杂OLAP场景。

Method: 提出SQLGovernor，采用片段式处理策略，结合知识管理框架；引入混合自学习机制，以专家反馈为指导。

Result: 在BIRD等基准测试和工业数据集上，SQLGovernor能将基础模型性能提升达10%，减少对人工专业知识的依赖。

Conclusion: SQLGovernor在生产环境中具有很强的实用价值和有效性能。

Abstract: SQL queries in real world analytical environments, whether written by humans
or generated automatically often suffer from syntax errors, inefficiency, or
semantic misalignment, especially in complex OLAP scenarios. To address these
challenges, we propose SQLGovernor, an LLM powered SQL toolkit that unifies
multiple functionalities, including syntax correction, query rewriting, query
modification, and consistency verification within a structured framework
enhanced by knowledge management. SQLGovernor introduces a fragment wise
processing strategy to enable fine grained rewriting and localized error
correction, significantly reducing the cognitive load on the LLM. It further
incorporates a hybrid self learning mechanism guided by expert feedback,
allowing the system to continuously improve through DBMS output analysis and
rule validation. Experiments on benchmarks such as BIRD and BIRD CRITIC, as
well as industrial datasets, show that SQLGovernor consistently boosts the
performance of base models by up to 10%, while minimizing reliance on manual
expertise. Deployed in production environments, SQLGovernor demonstrates strong
practical utility and effective performance.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [23] [Aurora: Architecting Argonne's First Exascale Supercomputer for Accelerated Scientific Discovery](https://arxiv.org/abs/2509.08207)
*Benjamin S. Allen,James Anchell,Victor Anisimov,Thomas Applencourt,Abhishek Bagusetty,Ramesh Balakrishnan,Riccardo Balin,Solomon Bekele,Colleen Bertoni,Cyrus Blackworth,Renzo Bustamante,Kevin Canada,John Carrier,Christopher Chan-nui,Lance C. Cheney,Taylor Childers,Paul Coffman,Susan Coghlan,Michael D'Mello,Murali Emani,Kyle G. Felker,Sam Foreman,Olivier Franza,Longfei Gao,Marta García,María Garzarán,Balazs Gerofi,Yasaman Ghadar,Neha Gupta,Kevin Harms,Väinö Hatanpää,Brian Holland,Carissa Holohan,Brian Homerding,Khalid Hossain,Louise Huot,Huda Ibeid,Joseph A. Insley,Sai Jayanthi,Hong Jiang,Wei Jiang,Xiao-Yong Jin,Jeongnim Kim,Christopher Knight,Kalyan Kumaran,JaeHyuk Kwack,Ti Leggett,Ben Lenard,Chris Lewis,Nevin Liber,Johann Lombardi,Raymond M. Loy,Ye Luo,Bethany Lusch,Nilakantan Mahadevan,Victor A. Mateevitsi,Gordon McPheeters,Ryan Milner,Vitali A. Morozov,Servesh Muralidharan,Tom Musta,Mrigendra Nagar,Vikram Narayana,Marieme Ngom,Anthony-Trung Nguyen,Nathan Nichols,Aditya Nishtala,James C. Osborn,Michael E. Papka,Scott Parker,Saumil S. Patel,Adrian C. Pope,Sucheta Raghunanda,Esteban Rangel,Paul M. Rich,Silvio Rizzi,Kris Rowe,Varuni Sastry,Adam Scovel,Filippo Simini,Haritha Siddabathuni Som,Patrick Steinbrecher,Rick Stevens,Xinmin Tian,Peter Upton,Thomas Uram,Archit K. Vasan,Álvaro Vázquez-Mayagoitia,Kaushik Velusamy,Brice Videau,Venkatram Vishwanath,Brian Whitney,Timothy J. Williams,Michael Woodacre,Sam Zeltner,Gengbin Zheng,Huihuo Zheng*

Main category: cs.DC

TL;DR: 本文深入探索Aurora超级计算机的节点架构、互连、软件生态和存储方案，还介绍基准性能和应用准备情况。


<details>
  <summary>Details</summary>
Motivation: 探索Aurora这一开创性Exascale超级计算机，利用其新技术加速科学发现。

Method: 对Aurora的节点架构、HPE Slingshot互连、软件生态系统和DAOS进行深入研究，通过早期科学计划和项目了解性能和应用准备。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及具体结论。

Abstract: Aurora is Argonne National Laboratory's pioneering Exascale supercomputer,
designed to accelerate scientific discovery with cutting-edge architectural
innovations. Key new technologies include the Intel(TM) Xeon(TM) Data Center
GPU Max Series (code-named Sapphire Rapids) with support for High Bandwidth
Memory (HBM), alongside the Intel(TM) Data Center GPU Max Series (code-named
Ponte Vecchio) on each compute node. Aurora also integrates the Distributed
Asynchronous Object Storage (DAOS), a novel exascale storage solution, and
leverages Intel's oneAPI programming environment. This paper presents an
in-depth exploration of Aurora's node architecture, the HPE Slingshot
interconnect, the supporting software ecosystem, and DAOS. We provide insights
into standard benchmark performance and applications readiness efforts via
Aurora's Early Science Program and the Exascale Computing Project.

</details>


### [24] [Design and Implementation of Code Completion System Based on LLM and CodeBERT Hybrid Subsystem](https://arxiv.org/abs/2509.08215)
*Bingbing Zhang,Ziyu Lin,Yingxin Su*

Main category: cs.DC

TL;DR: 本文聚焦软件开发行业，实现结合CodeBERT和GPT - 3.5的混合模型用于代码建议和自动完成任务，评估显示该模型优于基准，强调深度学习及模型合成潜力。


<details>
  <summary>Details</summary>
Motivation: 在软件开发行业，提高编码效率和准确性，利用CodeBERT和GPT - 3.5各自优势完成代码建议和自动完成任务。

Method: 实现集成CodeBERT和GPT - 3.5的混合模型，结合CodeBERT上下文感知能力和GPT - 3.5代码生成能力。

Result: 混合模型在准确性、生成代码质量和性能效率三个主要指标上优于基准，鲁棒性测试证实其可靠性和稳定性。

Conclusion: 强调深度学习在软件开发行业的重要性，揭示合成互补深度学习模型以发挥各模型优势的潜力。

Abstract: In the rapidly evolving industry of software development, coding efficiency
and accuracy play significant roles in delivering high-quality software.
Various code suggestion and completion tools, such as CodeBERT from Microsoft
and GPT-3.5 from OpenAI, have been developed using deep learning techniques and
integrated into IDEs to assist software engineers' development. Research has
shown that CodeBERT has outstanding performance in code summarization and
capturing code semantics, while GPT-3.5 demonstrated its adept capability at
code generation. This study focuses on implementing a hybrid model that
integrates CodeBERT and GPT-3.5 models to accomplish code suggestion and
autocomplete tasks, leveraging the context-aware effectiveness of CodeBERT and
taking advantage of advanced code generation abilities of GPT-3.5. Evaluated in
three main metrics: accuracy, quality of generated code and performance
efficiency with various software and hardware, the hybrid model outperforms
benchmarks, demonstrating its feasibility and effectiveness. Robustness testing
further confirms the reliability and stability of the hybrid model. This study
not only emphasizes the importance of deep learning in the software development
industry, but also reveals the potential of synthesizing complementary deep
learning models to fully exploit strengths of each model.

</details>


### [25] [Hetis: Serving LLMs in Heterogeneous GPU Clusters with Fine-grained and Dynamic Parallelism](https://arxiv.org/abs/2509.08309)
*Zizhao Mo,Jianxiong Liao,Huanle Xu,Zhi Zhou,Chengzhong Xu*

Main category: cs.DC

TL;DR: 本文提出针对异构GPU集群的LLM系统Hetis，采用细粒度和动态并行设计及在线负载调度策略，比现有系统提高吞吐量、降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有并行方法在异构环境中因粗粒度和静态策略难以有效扩展，异构设备存在内存和计算效率问题。

Method: 采用细粒度和动态并行设计，选择性并行计算密集型操作，按头粒度将注意力计算动态分配到低端GPU，还有在线负载调度策略。

Result: Hetis比现有系统提高服务吞吐量达2.25倍，降低延迟1.49倍。

Conclusion: Hetis能有效解决异构GPU集群中LLM服务的内存和计算效率问题，提升服务性能。

Abstract: The significant resource demands in LLM serving prompts production clusters
to fully utilize heterogeneous hardware by partitioning LLM models across a mix
of high-end and low-end GPUs. However, existing parallelization approaches
often struggle to scale efficiently in heterogeneous environments due to their
coarse-grained and static parallelization strategies.
  In this paper, we introduce Hetis, a new LLM system tailored for
heterogeneous GPU clusters. Hetis addresses two critical challenges: (1) memory
inefficiency caused by the mismatch between memory capacity and computational
power in heterogeneous devices, and (2) computational inefficiency arising from
performance gaps across different LLM modules. To tackle these issues, Hetis
employs a fine-grained and dynamic parallelism design. Specifically, it
selectively parallelizes compute-intensive operations to reduce latency and
dynamically distributes Attention computations to low-end GPUs at a head
granularity, leveraging the distinct characteristics of each module.
Additionally, Hetis features an online load dispatching policy that
continuously optimizes serving performance by carefully balancing network
latency, computational load, and memory intensity. Evaluation results
demonstrate that Hetis can improve serving throughput by up to $2.25\times$ and
reduce latency by $1.49\times$ compared to existing systems.

</details>


### [26] [An HPC Benchmark Survey and Taxonomy for Characterization](https://arxiv.org/abs/2509.08347)
*Andreas Herten,Olga Pearce,Filipe S. M. Guimarães*

Main category: cs.DC

TL;DR: 对高性能计算（HPC）领域现有基准测试进行调查，以表格形式总结并通过网站展示，还给出基准测试分类法。


<details>
  <summary>Details</summary>
Motivation: 高性能计算领域发展需要对硬件、软件和算法进行评估，而基准测试是关键工具，社区中有众多基准测试，需要进行梳理。

Method: 对可用的HPC基准测试进行调查，以表格形式总结关键细节并进行简洁分类，同时搭建交互式网站，还提出基准测试分类法。

Result: 完成了对HPC基准测试的调查、总结和分类，并通过网站展示。

Conclusion: 对HPC领域的基准测试进行了有效的梳理和分类，有助于相关人员更好地了解和使用基准测试。

Abstract: The field of High-Performance Computing (HPC) is defined by providing
computing devices with highest performance for a variety of demanding
scientific users. The tight co-design relationship between HPC providers and
users propels the field forward, paired with technological improvements,
achieving continuously higher performance and resource utilization. A key
device for system architects, architecture researchers, and scientific users
are benchmarks, allowing for well-defined assessment of hardware, software, and
algorithms. Many benchmarks exist in the community, from individual niche
benchmarks testing specific features, to large-scale benchmark suites for whole
procurements. We survey the available HPC benchmarks, summarizing them in table
form with key details and concise categorization, also through an interactive
website. For categorization, we present a benchmark taxonomy for well-defined
characterization of benchmarks.

</details>


### [27] [Towards Communication-Efficient Decentralized Federated Graph Learning over Non-IID Data](https://arxiv.org/abs/2509.08409)
*Shilong Wang,Jianchun Liu,Hongli Xu,Chenxia Tang,Qianpiao Ma,Liusheng Huang*

Main category: cs.DC

TL;DR: 提出Duplex框架优化去中心化联邦图学习（DFGL）的网络拓扑和图采样，降低通信成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法结合优化会导致DFGL训练性能下降，且要应对统计异质性和动态网络环境等实际挑战。

Method: 提出Duplex框架，联合优化网络拓扑和图采样，引入学习驱动算法自适应确定最优网络拓扑和图采样率。

Result: 相比基线，Duplex减少20.1% - 48.8%的完成时间和16.7% - 37.6%的通信成本，在相同资源预算下提高3.3% - 7.9%的准确率。

Conclusion: Duplex框架有效降低DFGL通信成本并提升训练性能。

Abstract: Decentralized Federated Graph Learning (DFGL) overcomes potential bottlenecks
of the parameter server in FGL by establishing a peer-to-peer (P2P)
communication network among workers. However, while extensive cross-worker
communication of graph node embeddings is crucial for DFGL training, it
introduces substantial communication costs. Most existing works typically
construct sparse network topologies or utilize graph neighbor sampling methods
to alleviate the communication overhead in DFGL. Intuitively, integrating these
methods may offer promise for doubly improving communication efficiency in
DFGL. However, our preliminary experiments indicate that directly combining
these methods leads to significant training performance degradation if they are
jointly optimized. To address this issue, we propose Duplex, a unified
framework that jointly optimizes network topology and graph sampling by
accounting for their coupled relationship, thereby significantly reducing
communication cost while enhancing training performance in DFGL. To overcome
practical DFGL challenges, eg, statistical heterogeneity and dynamic network
environments, Duplex introduces a learning-driven algorithm to adaptively
determine optimal network topologies and graph sampling ratios for workers.
Experimental results demonstrate that Duplex reduces completion time by
20.1%--48.8% and communication costs by 16.7%--37.6% to achieve target
accuracy, while improving accuracy by 3.3%--7.9% under identical resource
budgets compared to baselines.

</details>


### [28] [A 410GFLOP/s, 64 RISC-V Cores, 204.8GBps Shared-Memory Cluster in 12nm FinFET with Systolic Execution Support for Efficient B5G/6G AI-Enhanced O-RAN](https://arxiv.org/abs/2509.08608)
*Yichao Zhang,Marco Bertuletti,Sergio Mazzola,Samuel Riedel,Luca Benini*

Main category: cs.DC

TL;DR: 提出用于节能AI增强O - RAN的HeartStream集群，介绍其性能、效率及兼容性。


<details>
  <summary>Details</summary>
Motivation: 为实现节能的AI增强O - RAN，满足基站功率和处理延迟限制。

Method: 定制用于基带处理的核心和集群架构，支持复杂指令和硬件管理的脉动队列。

Result: 将关键基带内核能效提高1.89倍，在复杂无线工作负载上可达243GFLOP/s，AI处理可达72GOP/s，实现领先的软件定义PUSCH效率。

Conclusion: HeartStream完全兼容基站功率和处理延迟限制，适用于B5G/6G上行链路。

Abstract: We present HeartStream, a 64-RV-core shared-L1-memory cluster (410 GFLOP/s
peak performance and 204.8 GBps L1 bandwidth) for energy-efficient AI-enhanced
O-RAN. The cores and cluster architecture are customized for baseband
processing, supporting complex (16-bit real&imaginary) instructions:
multiply&accumulate, division&square-root, SIMD instructions, and
hardware-managed systolic queues, improving up to 1.89x the energy efficiency
of key baseband kernels. At 800MHz@0.8V, HeartStream delivers up to 243GFLOP/s
on complex-valued wireless workloads. Furthermore, the cores also support
efficient AI processing on received data at up to 72 GOP/s. HeartStream is
fully compatible with base station power and processing latency limits: it
achieves leading-edge software-defined PUSCH efficiency (49.6GFLOP/s/W) and
consumes just 0.68W (645MHz@0.65V), within the 4 ms end-to-end constraint for
B5G/6G uplink.

</details>


### [29] [Reconfigurable Holographic Surfaces and Near Field Communication for Non-Terrestrial Networks: Potential and Challenges](https://arxiv.org/abs/2509.08770)
*Muhammad Ali Jamshed,Muhammad Ahmed Mohsin,Hongliang Zhang,Bushra Haq,Aryan Kaushik,Boya Di,Weiwei Jiang*

Main category: cs.DC

TL;DR: 本文提出在非地面网络中结合使用近场通信和可重构全息表面，介绍系统架构、应用、挑战和方向，并进行用例分析。


<details>
  <summary>Details</summary>
Motivation: 克服超低延迟、广泛覆盖和高速数据率的挑战。

Method: 提出将可重构全息表面与卫星、高空平台站和无人机等非地面网络平台集成的系统架构。

Result: 能在近场区域实现精确波束成形和智能波前控制，提高能源效率、频谱利用率和空间分辨率，用例分析强化了无人机 - 可重构全息表面融合。

Conclusion: 明确了该集成的关键应用、挑战和未来方向，可用于提升系统能源效率。

Abstract: To overcome the challenges of ultra-low latency, ubiquitous coverage, and
soaring data rates, this article presents a combined use of Near Field
Communication (NFC) and Reconfigurable Holographic Surfaces (RHS) for
Non-Terrestrial Networks (NTN). A system architecture has been presented, which
shows that the integration of RHS with NTN platforms such as satellites, High
Altitute Platform Stations (HAPS), and Uncrewed Aerial Vehicles (UAV) can
achieve precise beamforming and intelligent wavefront control in near-field
regions, enhancing Energy Efficiency (EE), spectral utilization, and spatial
resolution. Moreover, key applications, challenges, and future directions have
been identified to fully adopt this integration. In addition, a use case
analysis has been presented to improve the EE of the system in a public safety
use case scenario, further strengthening the UAV-RHS fusion.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [30] [A Dynamic, Self-balancing k-d Tree](https://arxiv.org/abs/2509.08148)
*Russell A. Brown*

Main category: cs.DS

TL;DR: 文章介绍动态k - d树插入和删除算法并测量性能


<details>
  <summary>Details</summary>
Motivation: 传统k - d树平衡技术不适用，需构建能在插入或删除数据时自动平衡的动态k - d树

Method: 提出动态k - d树的插入和删除算法

Result: 文中未提及具体结果

Conclusion: 文中未提及明确结论

Abstract: The original description of the k-d tree recognized that rebalancing
techniques, such as used to build an AVL tree or a red-black tree, are not
applicable to a k-d tree, because these techniques involve cyclic exchange (aka
rotation) of tree nodes, which destroys the sorted order of the k-d tree. For
this reason, a static k-d tree is often built from all of the k-dimensional
data en masse. However, it is possible to build a dynamic k-d tree that
self-balances when necessary after insertion or deletion of each individual
k-dimensional datum. This article describes insertion and deletion algorithms
for a dynamic k-d tree, and measures their performance.

</details>


### [31] [Enumeration kernels for Vertex Cover and Feedback Vertex Set](https://arxiv.org/abs/2509.08475)
*Marin Bougeret,Guilherme C. M. Gomes,Vinicius F. dos Santos,Ignasi Sau*

Main category: cs.DS

TL;DR: 本文聚焦枚举核化领域，解决了顶点覆盖和反馈顶点集枚举版本研究不足的问题，给出了顶点覆盖和反馈顶点集的多项式延迟枚举核。


<details>
  <summary>Details</summary>
Motivation: 枚举核化在顶点覆盖和反馈顶点集枚举版本研究上成果少，本文旨在解决这一不足。

Method: 为经典冠分解约简规则开发非平凡提升算法，借鉴 Thomassé 的一些思想。

Result: 得到顶点覆盖问题具有 2k 个顶点的多项式延迟枚举核，反馈顶点集问题具有 O(k^3) 个顶点和边的多项式延迟枚举核。

Conclusion: 解决了顶点覆盖和反馈顶点集枚举版本研究的不足，改进了顶点覆盖问题的核大小。

Abstract: Enumerative kernelization is a recent and promising area sitting at the
intersection of parameterized complexity and enumeration algorithms. Its study
began with the paper of Creignou et al. [Theory Comput. Syst., 2017], and
development in the area has started to accelerate with the work of Golovach et
al. [J. Comput. Syst. Sci., 2022]. The latter introduced polynomial-delay
enumeration kernels and applied them in the study of structural
parameterizations of the \textsc{Matching Cut} problem and some variants. Few
other results, mostly on \textsc{Longest Path} and some generalizations of
\textsc{Matching Cut}, have also been developed. However, little success has
been seen in enumeration versions of \textsc{Vertex Cover} and \textsc{Feedback
Vertex Set}, some of the most studied problems in kernelization. In this paper,
we address this shortcoming. Our first result is a polynomial-delay enumeration
kernel with $2k$ vertices for \textsc{Enum Vertex Cover}, where we wish to list
all solutions with at most $k$ vertices. This is obtained by developing a
non-trivial lifting algorithm for the classical crown decomposition reduction
rule, and directly improves upon the kernel with $\mathcal{O}(k^2)$ vertices
derived from the work of Creignou et al. Our other result is a polynomial-delay
enumeration kernel with $\mathcal{O}(k^3)$ vertices and edges for \textsc{Enum
Feedback Vertex Set}; the proof is inspired by some ideas of Thomass\'e [TALG,
2010], but with a weaker bound on the kernel size due to difficulties in
applying the $q$-expansion technique.

</details>


### [32] [Checking and producing word attractors](https://arxiv.org/abs/2509.08503)
*Marie-Pierre Béal,Maxime Crochemore,Giuseppe Romana*

Main category: cs.DS

TL;DR: 文章聚焦单词吸引子，提出两个组合算法，一个用于判断，一个用于生成吸引子，虽问题是NP难，但算法高效。


<details>
  <summary>Details</summary>
Motivation: 研究单词吸引子与文本压缩效率的关系。

Method: 基于后缀自动机或有向无环单词图提出两个组合算法，一个线性时间判断，一个贪心生成。

Result: 第一个算法能在线性时间判断，第二个算法高效且能为一些知名单词族生成小吸引子。

Conclusion: 提出的算法对于单词吸引子问题是有效的。

Abstract: The article focuses on word (or string) attractors, which are sets of
positions related to the text compression efficiency of the underlying word.
The article presents two combinatorial algorithms based on Suffix automata or
Directed Acyclic Word Graphs. The first algorithm decides in linear time
whether a set of positions on the word is an attractor of the word. The second
algorithm generates an attractor for a given word in a greedy manner. Although
this problem is NP-hard, the algorithm is efficient and produces very small
attractors for several well-known families of words.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [33] [Efficiently Computing Equilibria in Budget-Aggregation Games](https://arxiv.org/abs/2509.08767)
*Patrick Becker,Alexander Fries,Matthias Greger,Erel Segal-Halevi*

Main category: cs.GT

TL;DR: 本文从博弈论视角研究预算聚合博弈，探讨不同偏好模型下纳什均衡的结构与可计算性，解决了一个公开问题。


<details>
  <summary>Details</summary>
Motivation: 研究在给定代理人偏好下，将外生预算分配到公共项目的社会选择问题，即预算聚合博弈。

Method: 从博弈论视角研究，分析不同偏好模型下纳什均衡的结构和可计算性。

Result: 证明了在Leontief效用下，纳什均衡可以在多项式时间内找到。

Conclusion: 解决了Brandt等人（2023）提出的一个公开问题，证明了特定偏好模型下纳什均衡的高效可计算性。

Abstract: Budget aggregation deals with the social choice problem of distributing an
exogenously given budget among a set of public projects, given agents'
preferences. Taking a game-theoretic perspective, we initialize the study of
\emph{budget-aggregation games} where each agent has virtual decision power
over some fraction of the budget. This paper investigates the structure and
shows efficient computability of Nash equilibria in this setting for various
preference models. In particular, we show that Nash equilibria for Leontief
utilities can be found in polynomial time, solving an open problem from Brandt
et al. [2023].

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [34] [Vector embedding of multi-modal texts: a tool for discovery?](https://arxiv.org/abs/2509.08216)
*Beth Plale,Sai Navya Jyesta,Sachith Withana*

Main category: cs.IR

TL;DR: 本文探索基于视觉语言模型的向量多模态检索对计算机科学多模态内容检索的提升，通过实验发现余弦相似度检索效果最佳，并探讨其在信息检索中的实用性。


<details>
  <summary>Details</summary>
Motivation: 探究基于视觉语言模型的向量多模态检索在多模态（文本和图像）内容发现方面的提升程度，暴露该方法的优缺点。

Method: 使用超3600页计算机科学教材数字化页面和视觉语言模型生成多向量表示，存储于向量数据库，发布75个自然语言查询基准，比较四种相似度度量的检索性能。

Result: 余弦相似度最能有效检索语义和视觉相关页面。

Conclusion: 为数字图书馆的发现提供设计见解。

Abstract: Computer science texts are particularly rich in both narrative content and
illustrative charts, algorithms, images, annotated diagrams, etc. This study
explores the extent to which vector-based multimodal retrieval, powered by
vision-language models (VLMs), can improve discovery across multi-modal (text
and images) content. Using over 3,600 digitized textbook pages largely from
computer science textbooks and a Vision Language Model (VLM), we generate
multi-vector representations capturing both textual and visual semantics. These
embeddings are stored in a vector database. We issue a benchmark of 75 natural
language queries and compare retrieval performance to ground truth and across
four similarity (distance) measures. The study is intended to expose both the
strengths and weakenesses of such an approach. We find that cosine similarity
most effectively retrieves semantically and visually relevant pages. We further
discuss the practicality of using a vector database and multi-modal embedding
for operational information retrieval. Our paper is intended to offer design
insights for discovery over digital libraries.
  Keywords: Vector embedding, multi-modal document retrieval, vector database
benchmark, digital library discovery

</details>


### [35] [Soundtracks of Our Lives: How Age Influences Musical Preferences](https://arxiv.org/abs/2509.08337)
*Arsen Matej Golubovikj,Bruce Ferwerda,Alan Said,Marko Talčič*

Main category: cs.IR

TL;DR: 本文用LFM - 2b数据集研究推荐系统中用户偏好和行为随时间的演变，发现不同年龄用户有不同听歌偏好，为推荐系统研究提供新方向。


<details>
  <summary>Details</summary>
Motivation: 多数推荐系统研究在短时间数据集上评估，而用户行为会随时间演变，该领域相关研究较少，因此开展此项研究。

Method: 使用LFM - 2b数据集进行研究，该数据集时间跨度长且包含用户年龄信息。

Result: 发现年轻用户倾向广泛收听当代流行音乐，年长用户有更精细和个性化的听歌习惯。

Conclusion: 研究结果为推荐系统研究开辟新方向，为后续工作提供指导。

Abstract: The majority of research in recommender systems, be it algorithmic
improvements, context-awareness, explainability, or other areas, evaluates
these systems on datasets that capture user interaction over a relatively
limited time span. However, recommender systems can very well be used
continuously for extended time. Similarly so, user behavior may evolve over
that extended time. Although media studies and psychology offer a wealth of
research on the evolution of user preferences and behavior as individuals age,
there has been scant research in this regard within the realm of user modeling
and recommender systems. In this study, we investigate the evolution of user
preferences and behavior using the LFM-2b dataset, which, to our knowledge, is
the only dataset that encompasses a sufficiently extensive time frame to permit
real longitudinal studies and includes age information about its users. We
identify specific usage and taste preferences directly related to the age of
the user, i.e., while younger users tend to listen broadly to contemporary
popular music, older users have more elaborate and personalized listening
habits. The findings yield important insights that open new directions for
research in recommender systems, providing guidance for future efforts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [Revisiting Deepfake Detection: Chronological Continual Learning and the Limits of Generalization](https://arxiv.org/abs/2509.07993)
*Federico Fontana,Anxhelo Diko,Romeo Lanzino,Marco Raoul Marini,Bachir Kaddar,Gian Luca Foresti,Luigi Cinque*

Main category: cs.LG

TL;DR: 将深度伪造检测（DFD）重构为持续学习（CL）问题，提出高效框架，模拟7年真实技术演变，构建轻量级骨干网络，提出两个新指标，实验显示适应高效但未来泛化近随机，提出非通用深度伪造分布假设。


<details>
  <summary>Details</summary>
Motivation: 深度伪造生成技术快速发展，非持续学习检测系统需频繁昂贵再训练，面临挑战。

Method: 将DFD重构为CL问题，提出框架模拟真实技术演变，构建轻量级视觉骨干网络，提出C - AUC和FWT - AUC指标。

Result: 适应比全量再训练快155倍，能保留历史知识，但当前方法在无额外训练时对未来生成器泛化近随机（FWT - AUC ≈ 0.5）。

Conclusion: 基于实验结果提出非通用深度伪造分布假设。

Abstract: The rapid evolution of deepfake generation technologies poses critical
challenges for detection systems, as non-continual learning methods demand
frequent and expensive retraining. We reframe deepfake detection (DFD) as a
Continual Learning (CL) problem, proposing an efficient framework that
incrementally adapts to emerging visual manipulation techniques while retaining
knowledge of past generators. Our framework, unlike prior approaches that rely
on unreal simulation sequences, simulates the real-world chronological
evolution of deepfake technologies in extended periods across 7 years.
Simultaneously, our framework builds upon lightweight visual backbones to allow
for the real-time performance of DFD systems. Additionally, we contribute two
novel metrics: Continual AUC (C-AUC) for historical performance and Forward
Transfer AUC (FWT-AUC) for future generalization. Through extensive
experimentation (over 600 simulations), we empirically demonstrate that while
efficient adaptation (+155 times faster than full retraining) and robust
retention of historical knowledge is possible, the generalization of current
approaches to future generators without additional training remains near-random
(FWT-AUC $\approx$ 0.5) due to the unique imprint characterizing each existing
generator. Such observations are the foundation of our newly proposed
Non-Universal Deepfake Distribution Hypothesis.
  \textbf{Code will be released upon acceptance.}

</details>


### [37] [How Far Are We from True Unlearnability?](https://arxiv.org/abs/2509.08058)
*Kai Ye,Liangcai Su,Chenxiong Qian*

Main category: cs.LG

TL;DR: 研究现有不可学习方法生成的示例在多任务中并非真正不可学习，从模型优化角度分析原因，提出SAL量化参数不可学习性、UD衡量数据不可学习性并进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有不可学习方法生成的示例在多任务中未展现跨任务不可学习性，质疑如何获得真正不可学习示例

Method: 观察简单模型架构下干净和中毒模型收敛过程差异，从损失景观分析，提出SAL和UD

Result: 发现损失景观与不可学习性密切相关，提出SAL和UD

Conclusion: 通过提出的UD对主流不可学习方法进行基准测试，促进社区对现有方法能力边界的认识。

Abstract: High-quality data plays an indispensable role in the era of large models, but
the use of unauthorized data for model training greatly damages the interests
of data owners. To overcome this threat, several unlearnable methods have been
proposed, which generate unlearnable examples (UEs) by compromising the
training availability of data. Clearly, due to unknown training purposes and
the powerful representation learning capabilities of existing models, these
data are expected to be unlearnable for models across multiple tasks, i.e.,
they will not help improve the model's performance. However, unexpectedly, we
find that on the multi-task dataset Taskonomy, UEs still perform well in tasks
such as semantic segmentation, failing to exhibit cross-task unlearnability.
This phenomenon leads us to question: How far are we from attaining truly
unlearnable examples? We attempt to answer this question from the perspective
of model optimization. To this end, we observe the difference in the
convergence process between clean and poisoned models using a simple model
architecture. Subsequently, from the loss landscape we find that only a part of
the critical parameter optimization paths show significant differences,
implying a close relationship between the loss landscape and unlearnability.
Consequently, we employ the loss landscape to explain the underlying reasons
for UEs and propose Sharpness-Aware Learnability (SAL) to quantify the
unlearnability of parameters based on this explanation. Furthermore, we propose
an Unlearnable Distance (UD) to measure the unlearnability of data based on the
SAL distribution of parameters in clean and poisoned models. Finally, we
conduct benchmark tests on mainstream unlearnable methods using the proposed
UD, aiming to promote community awareness of the capability boundaries of
existing unlearnable methods.

</details>


### [38] [JEL: A Novel Model Linking Knowledge Graph entities to News Mentions](https://arxiv.org/abs/2509.08086)
*Michael Kishelev,Pranab Bhadani,Wanying Ding,Vinay Chaudhri*

Main category: cs.LG

TL;DR: 提出计算高效的端到端多神经网络实体链接模型JEL，优于当前最优模型，阐述实体链接在知识图谱和新闻分析中的重要性。


<details>
  <summary>Details</summary>
Motivation: 知识图谱需将实体与文本提及正确链接，实体链接是自然语言处理基础任务，在新闻分析等场景有重要应用，摩根大通有大量团队需求且花费高昂外部成本。

Method: 提出基于多神经网络的端到端实体链接模型JEL。

Result: JEL模型优于当前最优模型。

Conclusion: 实体链接对连接非结构化新闻文本和知识图谱至关重要，能促进用户日常工作。

Abstract: We present JEL, a novel computationally efficient end-to-end multi-neural
network based entity linking model, which beats current state-of-art model.
Knowledge Graphs have emerged as a compelling abstraction for capturing
critical relationships among the entities of interest and integrating data from
multiple heterogeneous sources. A core problem in leveraging a knowledge graph
is linking its entities to the mentions (e.g., people, company names) that are
encountered in textual sources (e.g., news, blogs., etc) correctly, since there
are thousands of entities to consider for each mention. This task of linking
mentions and entities is referred as Entity Linking (EL). It is a fundamental
task in natural language processing and is beneficial in various uses cases,
such as building a New Analytics platform. News Analytics, in JPMorgan, is an
essential task that benefits multiple groups across the firm. According to a
survey conducted by the Innovation Digital team 1 , around 25 teams across the
firm are actively looking for news analytics solutions, and more than \$2
million is being spent annually on external vendor costs. Entity linking is
critical for bridging unstructured news text with knowledge graphs, enabling
users access to vast amounts of curated data in a knowledge graph and
dramatically facilitating their daily work.

</details>


### [39] [Modified Loss of Momentum Gradient Descent: Fine-Grained Analysis](https://arxiv.org/abs/2509.08483)
*Matias D. Cattaneo,Boris Shigida*

Main category: cs.LG

TL;DR: 分析带Polyak重球动量（HB）的梯度下降算法，证明其在指数吸引不变流形上等价于修改损失的普通梯度下降，给出逼近界，分析组合学特性并推导连续修正方程，结果涵盖全批量和小批量HB。


<details>
  <summary>Details</summary>
Motivation: 深入理解带重球动量的梯度下降算法的主要特征，为其他优化算法的类似分析提供思路。

Method: 基于Kovachki和Stuart (2021)的工作，对步长足够小的情况进行证明，分析组合学特性，推导连续修正方程。

Result: 证明算法在特定条件下等价于修改损失的普通梯度下降，给出全局逼近界$O(h^{R})$，发现含Eulerian和Narayana多项式的多项式族，推导任意逼近阶的连续修正方程。

Conclusion: 理论结果为带重球动量的梯度下降算法提供新见解，为其他优化算法分析提供路线图。

Abstract: We analyze gradient descent with Polyak heavy-ball momentum (HB) whose fixed
momentum parameter $\beta \in (0, 1)$ provides exponential decay of memory.
Building on Kovachki and Stuart (2021), we prove that on an exponentially
attractive invariant manifold the algorithm is exactly plain gradient descent
with a modified loss, provided that the step size $h$ is small enough. Although
the modified loss does not admit a closed-form expression, we describe it with
arbitrary precision and prove global (finite "time" horizon) approximation
bounds $O(h^{R})$ for any finite order $R \geq 2$. We then conduct a
fine-grained analysis of the combinatorics underlying the memoryless
approximations of HB, in particular, finding a rich family of polynomials in
$\beta$ hidden inside which contains Eulerian and Narayana polynomials. We
derive continuous modified equations of arbitrary approximation order (with
rigorous bounds) and the principal flow that approximates the HB dynamics,
generalizing Rosca et al. (2023). Approximation theorems cover both full-batch
and mini-batch HB. Our theoretical results shed new light on the main features
of gradient descent with heavy-ball momentum, and outline a road-map for
similar analysis of other optimization algorithms.

</details>


### [40] [Performance Assessment Strategies for Generative AI Applications in Healthcare](https://arxiv.org/abs/2509.08087)
*Victor Garcia,Mariia Sidulova,Aldo Badano*

Main category: cs.LG

TL;DR: 本文讨论医疗领域生成式人工智能（GenAI）应用性能评估的现状，指出定量基准评估法的局限，提及利用人类专业知识和低成本计算模型的评估策略受关注。


<details>
  <summary>Details</summary>
Motivation: 全面理解临床任务及GenAI在实际临床环境中的性能变化，有必要评估其应用。同时现有定量基准评估法存在局限，需探讨更好的评估方法。

Method: 探讨当前评估医疗保健和医疗设备中GenAI应用性能的先进方法。

Result: 未提及具体结果。

Conclusion: 未提及明确结论，但对评估GenAI应用性能的当前方法进行了讨论。

Abstract: Generative artificial intelligence (GenAI) represent an emerging paradigm
within artificial intelligence, with applications throughout the medical
enterprise. Assessing GenAI applications necessitates a comprehensive
understanding of the clinical task and awareness of the variability in
performance when implemented in actual clinical environments. Presently, a
prevalent method for evaluating the performance of generative models relies on
quantitative benchmarks. Such benchmarks have limitations and may suffer from
train-to-the-test overfitting, optimizing performance for a specified test set
at the cost of generalizability across other task and data distributions.
Evaluation strategies leveraging human expertise and utilizing cost-effective
computational models as evaluators are gaining interest. We discuss current
state-of-the-art methodologies for assessing the performance of GenAI
applications in healthcare and medical devices.

</details>


### [41] [Hammer and Anvil: A Principled Defense Against Backdoors in Federated Learning](https://arxiv.org/abs/2509.08089)
*Lucas Fenaux,Zheng Wang,Jacob Yan,Nathan Chung,Florian Kerschbaum*

Main category: cs.LG

TL;DR: 本文提出新的自适应对手攻击和Hammer and Anvil防御方法，Krum+防御成功抵御新攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受后门攻击，现有防御难以抵御自适应攻击者。

Method: 设计新的自适应对手攻击，提出结合两种防御的Hammer and Anvil防御方法。

Result: 新攻击只需1 - 2个恶意客户端就能攻破现有防御，Krum+成功抵御新攻击和现有攻击。

Conclusion: 提出的防御方法在应对新攻击和现有攻击方面有效。

Abstract: Federated Learning is a distributed learning technique in which multiple
clients cooperate to train a machine learning model. Distributed settings
facilitate backdoor attacks by malicious clients, who can embed malicious
behaviors into the model during their participation in the training process.
These malicious behaviors are activated during inference by a specific trigger.
No defense against backdoor attacks has stood the test of time, especially
against adaptive attackers, a powerful but not fully explored category of
attackers. In this work, we first devise a new adaptive adversary that
surpasses existing adversaries in capabilities, yielding attacks that only
require one or two malicious clients out of 20 to break existing
state-of-the-art defenses. Then, we present Hammer and Anvil, a principled
defense approach that combines two defenses orthogonal in their underlying
principle to produce a combined defense that, given the right set of
parameters, must succeed against any attack. We show that our best combined
defense, Krum+, is successful against our new adaptive adversary and
state-of-the-art attacks.

</details>


### [42] [Machine Learning with Multitype Protected Attributes: Intersectional Fairness through Regularisation](https://arxiv.org/abs/2509.08163)
*Ho Ming Lee,Katrien Antonio,Benjamin Avanzi,Lorenzo Marchi,Rui Zhou*

Main category: cs.LG

TL;DR: 本文提出距离协方差正则化框架解决机器学习中回归和分类任务公平性问题，处理多受保护属性并应用于实际数据集。


<details>
  <summary>Details</summary>
Motivation: 现有文献多关注二元分类公平性，回归任务公平性同样重要，且现有方法对连续属性及多受保护属性下交叉子组公平性处理不足。

Method: 提出距离协方差正则化框架，结合JdCov和CCdCov处理多受保护属性，还讨论校准正则化强度方法。

Result: 框架能有效解决回归和分类任务中多类型受保护属性的公平性问题。

Conclusion: 所提框架可缓解模型预测与受保护属性关联，解决公平性问题，适用于不同类型受保护属性的任务。

Abstract: Ensuring equitable treatment (fairness) across protected attributes (such as
gender or ethnicity) is a critical issue in machine learning. Most existing
literature focuses on binary classification, but achieving fairness in
regression tasks-such as insurance pricing or hiring score assessments-is
equally important. Moreover, anti-discrimination laws also apply to continuous
attributes, such as age, for which many existing methods are not applicable. In
practice, multiple protected attributes can exist simultaneously; however,
methods targeting fairness across several attributes often overlook so-called
"fairness gerrymandering", thereby ignoring disparities among intersectional
subgroups (e.g., African-American women or Hispanic men). In this paper, we
propose a distance covariance regularisation framework that mitigates the
association between model predictions and protected attributes, in line with
the fairness definition of demographic parity, and that captures both linear
and nonlinear dependencies. To enhance applicability in the presence of
multiple protected attributes, we extend our framework by incorporating two
multivariate dependence measures based on distance covariance: the previously
proposed joint distance covariance (JdCov) and our novel concatenated distance
covariance (CCdCov), which effectively address fairness gerrymandering in both
regression and classification tasks involving protected attributes of various
types. We discuss and illustrate how to calibrate regularisation strength,
including a method based on Jensen-Shannon divergence, which quantifies
dissimilarities in prediction distributions across groups. We apply our
framework to the COMPAS recidivism dataset and a large motor insurance claims
dataset.

</details>


### [43] [Domain Knowledge is Power: Leveraging Physiological Priors for Self Supervised Representation Learning in Electrocardiography](https://arxiv.org/abs/2509.08116)
*Nooshin Maghsoodi,Sarah Nassar,Paul F R Wilson,Minh Nguyen Nhat To,Sophia Mannina,Shamel Addas,Stephanie Sibley,David Maslove,Purang Abolmaesumi,Parvin Mousavi*

Main category: cs.LG

TL;DR: 提出PhysioCLR框架用于ECG心律失常分类，在多数据集上提升诊断性能，展示了生理学知识在ECG诊断中的潜力。


<details>
  <summary>Details</summary>
Motivation: AI - 基于ECG分析受限于标记数据，自监督学习可利用未标记数据，需提升ECG心律失常分类的泛化性和临床相关性。

Method: 在预训练时使相似临床特征样本嵌入靠近、不同样本嵌入远离，集成ECG生理相似性线索，引入ECG特定增强和混合损失函数。

Result: 在Chapman、Georgia和私人ICU数据集上，相对最强基线平均AUROC提升12%，有强大的跨数据集泛化能力。

Conclusion: 将生理知识嵌入对比学习，使模型学习到有临床意义和可迁移的ECG特征，为ECG诊断提供了有前景的方法。

Abstract: Objective: Electrocardiograms (ECGs) play a crucial role in diagnosing heart
conditions; however, the effectiveness of artificial intelligence (AI)-based
ECG analysis is often hindered by the limited availability of labeled data.
Self-supervised learning (SSL) can address this by leveraging large-scale
unlabeled data. We introduce PhysioCLR (Physiology-aware Contrastive Learning
Representation for ECG), a physiology-aware contrastive learning framework that
incorporates domain-specific priors to enhance the generalizability and
clinical relevance of ECG-based arrhythmia classification. Methods: During
pretraining, PhysioCLR learns to bring together embeddings of samples that
share similar clinically relevant features while pushing apart those that are
dissimilar. Unlike existing methods, our method integrates ECG physiological
similarity cues into contrastive learning, promoting the learning of clinically
meaningful representations. Additionally, we introduce ECG- specific
augmentations that preserve the ECG category post augmentation and propose a
hybrid loss function to further refine the quality of learned representations.
Results: We evaluate PhysioCLR on two public ECG datasets, Chapman and Georgia,
for multilabel ECG diagnoses, as well as a private ICU dataset labeled for
binary classification. Across the Chapman, Georgia, and private cohorts,
PhysioCLR boosts the mean AUROC by 12% relative to the strongest baseline,
underscoring its robust cross-dataset generalization. Conclusion: By embedding
physiological knowledge into contrastive learning, PhysioCLR enables the model
to learn clinically meaningful and transferable ECG eatures. Significance:
PhysioCLR demonstrates the potential of physiology-informed SSL to offer a
promising path toward more effective and label-efficient ECG diagnostics.

</details>


### [44] [Prescribe-then-Select: Adaptive Policy Selection for Contextual Stochastic Optimization](https://arxiv.org/abs/2509.08194)
*Caio de Prospero Iglesias,Kimberly Villalobos Carballo,Dimitris Bertsimas*

Main category: cs.LG

TL;DR: 提出Prescribe - then - Select (PS)框架解决上下文随机优化中的策略选择问题，在两个基准问题上表现良好。


<details>
  <summary>Details</summary>
Motivation: 在上下文随机优化中，不同候选策略在协变量空间中表现不同，没有单一策略能统一占优，需要解决策略选择问题。

Method: 提出PS框架，先构建可行候选策略库，再学习元策略选择最佳策略，用通过交叉验证训练的最优策略树集成实现元策略。

Result: 在单阶段新闻供应商和两阶段运输规划两个基准CSO问题中，PS在协变量空间异质区域始终优于最佳单一策略，在无异质性时收敛到主导策略。

Conclusion: PS框架是解决上下文随机优化中策略选择问题的有效方法。

Abstract: We address the problem of policy selection in contextual stochastic
optimization (CSO), where covariates are available as contextual information
and decisions must satisfy hard feasibility constraints. In many CSO settings,
multiple candidate policies--arising from different modeling paradigms--exhibit
heterogeneous performance across the covariate space, with no single policy
uniformly dominating. We propose Prescribe-then-Select (PS), a modular
framework that first constructs a library of feasible candidate policies and
then learns a meta-policy to select the best policy for the observed
covariates. We implement the meta-policy using ensembles of Optimal Policy
Trees trained via cross-validation on the training set, making policy choice
entirely data-driven. Across two benchmark CSO problems--single-stage
newsvendor and two-stage shipment planning--PS consistently outperforms the
best single policy in heterogeneous regimes of the covariate space and
converges to the dominant policy when such heterogeneity is absent. All the
code to reproduce the results can be found at
https://anonymous.4open.science/r/Prescribe-then-Select-TMLR.

</details>


### [45] [Optimization Methods and Software for Federated Learning](https://arxiv.org/abs/2509.08120)
*Konstantin Burlachenko*

Main category: cs.LG

TL;DR: 论文指出联邦学习训练有独特挑战，识别五大关键挑战并提出新方法，促进算法与系统发展，为理论与实践转化提供指导。


<details>
  <summary>Details</summary>
Motivation: 联邦学习训练过程分散、环境缺乏控制，存在数据和设备异质性、通信、隐私等挑战，且理论需更好应用于实际。

Method: 识别联邦学习的五个关键挑战并提出新颖解决方法。

Result: 推进了联邦学习算法和系统发展，在理论与实践间架起桥梁。

Conclusion: 为研究者将理论转化为实际应用及反向将实践融入理论设计提供指导。

Abstract: Federated Learning (FL) is a novel, multidisciplinary Machine Learning
paradigm where multiple clients, such as mobile devices, collaborate to solve
machine learning problems. Initially introduced in Kone{\v{c}}n{\'y} et al.
(2016a,b); McMahan et al. (2017), FL has gained further attention through its
inclusion in the National AI Research and Development Strategic Plan (2023
Update) of the United States (Science and on Artificial Intelligence, 2023).
The FL training process is inherently decentralized and often takes place in
less controlled settings compared to data centers, posing unique challenges
distinct from those in fully controlled environments. In this thesis, we
identify five key challenges in Federated Learning and propose novel approaches
to address them. These challenges arise from the heterogeneity of data and
devices, communication issues, and privacy concerns for clients in FL training.
Moreover, even well-established theoretical advances in FL require diverse
forms of practical implementation to enhance their real-world applicability.
Our contributions advance FL algorithms and systems, bridging theoretical
advancements and practical implementations. More broadly, our work serves as a
guide for researchers navigating the complexities of translating theoretical
methods into efficient real-world implementations and software. Additionally,
it offers insights into the reverse process of adapting practical
implementation aspects back into theoretical algorithm design. This reverse
process is particularly intriguing, as the practical perspective compels us to
examine the underlying mechanics and flexibilities of algorithms more deeply,
often uncovering new dimensions of the algorithms under study.

</details>


### [46] [In-Context Learning Enhanced Credibility Transformer](https://arxiv.org/abs/2509.08122)
*Kishan Padayachy,Ronald Richman,Salvatore Scognamiglio,Mario V. Wüthrich*

Main category: cs.LG

TL;DR: 本文提出通过上下文学习机制增强Credibility Transformer架构，提升模型预测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 改进Credibility Transformer架构，进一步提升模型学习和预测性能，增强模型泛化能力。

Method: 在Credibility Transformer架构基础上增加上下文学习机制，通过包含相似实例的上下文批次增加信息集，对实例的CLS令牌表示进行增强和微调。

Result: 经验验证表明，上下文学习通过适应相似风险模式提高了预测准确性，且模型能够泛化到训练时未出现特征水平的新实例。

Conclusion: 上下文学习机制能有效提升Credibility Transformer架构的预测性能和泛化能力。

Abstract: The starting point of our network architecture is the Credibility Transformer
which extends the classical Transformer architecture by a credibility mechanism
to improve model learning and predictive performance. This Credibility
Transformer learns credibilitized CLS tokens that serve as learned
representations of the original input features. In this paper we present a new
paradigm that augments this architecture by an in-context learning mechanism,
i.e., we increase the information set by a context batch consisting of similar
instances. This allows the model to enhance the CLS token representations of
the instances by additional in-context information and fine-tuning. We
empirically verify that this in-context learning enhances predictive accuracy
by adapting to similar risk patterns. Moreover, this in-context learning also
allows the model to generalize to new instances which, e.g., have feature
levels in the categorical covariates that have not been present when the model
was trained -- for a relevant example, think of a new vehicle model which has
just been developed by a car manufacturer.

</details>


### [47] [Sketched Gaussian Mechanism for Private Federated Learning](https://arxiv.org/abs/2509.08195)
*Qiaobo Li,Zhijie Chen,Arindam Banerjee*

Main category: cs.LG

TL;DR: 本文提出Sketched Gaussian Mechanism (SGM)，结合草图和高斯机制保障隐私，联合分析其隐私保证，理论和实验表明SGM在隐私和性能上有优势。


<details>
  <summary>Details</summary>
Motivation: 现有文献对联邦学习中草图和高斯机制的隐私分析是孤立的，本文希望进行联合分析以获得更优的隐私保证。

Method: 引入SGM，使用Rényi - DP工具对其隐私保证进行联合分析，将SGM应用于联邦学习并建立优化收敛的理论结果。

Result: 证明SGM隐私水平与草图维度关系，理论结果显示优化收敛对参数数量仅对数依赖，实验表明相同隐私水平下SGM有竞争力甚至表现更优，服务器使用自适应优化可提升性能并维持隐私保证。

Conclusion: SGM能提供比原高斯机制更强的隐私保证，在联邦学习中有良好应用效果。

Abstract: Communication cost and privacy are two major considerations in federated
learning (FL). For communication cost, gradient compression by sketching the
clients' transmitted model updates is often used for reducing per-round
communication. For privacy, the Gaussian mechanism (GM), which consists of
clipping updates and adding Gaussian noise, is commonly used to guarantee
client-level differential privacy. Existing literature on private FL analyzes
privacy of sketching and GM in an isolated manner, illustrating that sketching
provides privacy determined by the sketching dimension and that GM has to
supply any additional desired privacy.
  In this paper, we introduce the Sketched Gaussian Mechanism (SGM), which
directly combines sketching and the Gaussian mechanism for privacy. Using
R\'enyi-DP tools, we present a joint analysis of SGM's overall privacy
guarantee, which is significantly more flexible and sharper compared to
isolated analysis of sketching and GM privacy. In particular, we prove that the
privacy level of SGM for a fixed noise magnitude is proportional to
$1/\sqrt{b}$, where $b$ is the sketching dimension, indicating that (for
moderate $b$) SGM can provide much stronger privacy guarantees than the
original GM under the same noise budget. We demonstrate the application of SGM
to FL with either gradient descent or adaptive server optimizers, and establish
theoretical results on optimization convergence, which exhibits only a
logarithmic dependence on the number of parameters $d$. Experimental results
confirm that at the same privacy level, SGM based FL is at least competitive
with non-sketching private FL variants and outperforms them in some settings.
Moreover, using adaptive optimization at the server improves empirical
performance while maintaining the privacy guarantees.

</details>


### [48] [torchmil: A PyTorch-based library for deep Multiple Instance Learning](https://arxiv.org/abs/2509.08129)
*Francisco M. Castro-Macías,Francisco J. Sáez-Maldonado,Pablo Morales-Álvarez,Rafael Molina*

Main category: cs.LG

TL;DR: 提出开源Python库torchmil以解决MIL领域缺乏标准化工具问题，加速MIL进展并降低新用户门槛。


<details>
  <summary>Details</summary>
Motivation: MIL领域缺乏标准化的模型开发、评估和比较工具，阻碍了可重复性和可访问性。

Method: 开发了基于PyTorch的开源Python库torchmil，提供统一、模块化和可扩展的框架，包括基本构建块、标准化数据格式、基准数据集和模型集合。

Result: 创建了torchmil库，有全面文档和教程。

Conclusion: torchmil可加速MIL进展并降低新用户门槛。

Abstract: Multiple Instance Learning (MIL) is a powerful framework for weakly
supervised learning, particularly useful when fine-grained annotations are
unavailable. Despite growing interest in deep MIL methods, the field lacks
standardized tools for model development, evaluation, and comparison, which
hinders reproducibility and accessibility. To address this, we present
torchmil, an open-source Python library built on PyTorch. torchmil offers a
unified, modular, and extensible framework, featuring basic building blocks for
MIL models, a standardized data format, and a curated collection of benchmark
datasets and models. The library includes comprehensive documentation and
tutorials to support both practitioners and researchers. torchmil aims to
accelerate progress in MIL and lower the entry barrier for new users. Available
at https://torchmil.readthedocs.io.

</details>


### [49] [From Limited Data to Rare-event Prediction: LLM-powered Feature Engineering and Multi-model Learning in Venture Capital](https://arxiv.org/abs/2509.08140)
*Mihir Kumar,Aaron Ontoyin Yin,Zakari Salifu,Kelvin Amoaba,Afriyie Kwesi Samuel,Fuat Alican,Yigit Ihlamur*

Main category: cs.LG

TL;DR: 本文提出将大语言模型与多模型机器学习架构集成的框架来预测罕见高影响结果，应用于风险投资领域，取得良好效果并揭示可解释的成功驱动因素。


<details>
  <summary>Details</summary>
Motivation: 解决预测罕见高影响结果问题，结合黑盒模型预测能力与可靠决策所需的可解释性。

Method: 使用大语言模型进行特征工程从非结构化数据提取合成信号，通过包含XGBoost、随机森林和线性回归的分层集成模型处理，先产生连续成功可能性估计，再阈值化得到二元罕见事件预测。

Result: 模型在三个独立测试子集的精度是随机分类器基线的9.8 - 11.1倍，特征敏感性分析揭示可解释的成功驱动因素。

Conclusion: 该框架在预测罕见高影响结果上表现良好，能揭示可解释成功驱动因素。

Abstract: This paper presents a framework for predicting rare, high-impact outcomes by
integrating large language models (LLMs) with a multi-model machine learning
(ML) architecture. The approach combines the predictive strength of black-box
models with the interpretability required for reliable decision-making. We use
LLM-powered feature engineering to extract and synthesize complex signals from
unstructured data, which are then processed within a layered ensemble of models
including XGBoost, Random Forest, and Linear Regression. The ensemble first
produces a continuous estimate of success likelihood, which is then thresholded
to produce a binary rare-event prediction. We apply this framework to the
domain of Venture Capital (VC), where investors must evaluate startups with
limited and noisy early-stage data. The empirical results show strong
performance: the model achieves precision between 9.8X and 11.1X the random
classifier baseline in three independent test subsets. Feature sensitivity
analysis further reveals interpretable success drivers: the startup's category
list accounts for 15.6% of predictive influence, followed by the number of
founders, while education level and domain expertise contribute smaller yet
consistent effects.

</details>


### [50] [PracMHBench: Re-evaluating Model-Heterogeneous Federated Learning Based on Practical Edge Device Constraints](https://arxiv.org/abs/2509.08750)
*Yuanchun Guo,Bingyan Liu,Yulong Sha,Zhensheng Xian*

Main category: cs.LG

TL;DR: 本文构建PracMHBench平台评估边缘设备上模型异构联邦学习算法，进行大量实验观察其适用性和异构模式。


<details>
  <summary>Details</summary>
Motivation: 现有工作未在实际边缘设备约束下对模型异构联邦学习算法进行定量分析，因此需要重新思考和评估该范式。

Method: 构建PracMHBench系统平台，对不同模型异构算法在多个数据任务和指标上进行分类和测试。

Result: 在不同边缘约束下对算法进行了大量实验。

Conclusion: 通过实验观察算法的适用性和相应的异构模式。

Abstract: Federating heterogeneous models on edge devices with diverse resource
constraints has been a notable trend in recent years. Compared to traditional
federated learning (FL) that assumes an identical model architecture to
cooperate, model-heterogeneous FL is more practical and flexible since the
model can be customized to satisfy the deployment requirement. Unfortunately,
no prior work ever dives into the existing model-heterogeneous FL algorithms
under the practical edge device constraints and provides quantitative analysis
on various data scenarios and metrics, which motivates us to rethink and
re-evaluate this paradigm. In our work, we construct the first system platform
\textbf{PracMHBench} to evaluate model-heterogeneous FL on practical
constraints of edge devices, where diverse model heterogeneity algorithms are
classified and tested on multiple data tasks and metrics. Based on the
platform, we perform extensive experiments on these algorithms under the
different edge constraints to observe their applicability and the corresponding
heterogeneity pattern.

</details>


### [51] [MMM-fair: An Interactive Toolkit for Exploring and Operationalizing Multi-Fairness Trade-offs](https://arxiv.org/abs/2509.08156)
*Swati Swati,Arjun Roy,Emmanouil Panagiotou,Eirini Ntoutsi*

Main category: cs.LG

TL;DR: 提出开源工具包mmm - fair，结合多属性公平性、多目标优化等多种特性，能解决公平感知分类问题。


<details>
  <summary>Details</summary>
Motivation: 公平感知分类受交叉偏见和冲突的公平定义影响，流行工具包对多维公平性探索支持有限，且有监管和社会对公平AI的需求。

Method: 利用基于提升的集成方法，动态优化模型权重，联合最小化分类误差和各种公平性违规。

Result: 系统可让用户部署符合特定需求的模型，可靠地发现先进方法常遗漏的交叉偏见。

Conclusion: mmm - fair在单个开源工具包中结合多种特性，是现有公平性工具中少见的。

Abstract: Fairness-aware classification requires balancing performance and fairness,
often intensified by intersectional biases. Conflicting fairness definitions
further complicate the task, making it difficult to identify universally fair
solutions. Despite growing regulatory and societal demands for equitable AI,
popular toolkits offer limited support for exploring multi-dimensional fairness
and related trade-offs. To address this, we present mmm-fair, an open-source
toolkit leveraging boosting-based ensemble approaches that dynamically
optimizes model weights to jointly minimize classification errors and diverse
fairness violations, enabling flexible multi-objective optimization. The system
empowers users to deploy models that align with their context-specific needs
while reliably uncovering intersectional biases often missed by
state-of-the-art methods. In a nutshell, mmm-fair uniquely combines in-depth
multi-attribute fairness, multi-objective optimization, a no-code, chat-based
interface, LLM-powered explanations, interactive Pareto exploration for model
selection, custom fairness constraint definition, and deployment-ready models
in a single open-source toolkit, a combination rarely found in existing
fairness tools. Demo walkthrough available at: https://youtu.be/_rcpjlXFqkw.

</details>


### [52] [Data-driven generative simulation of SDEs using diffusion models](https://arxiv.org/abs/2509.08731)
*Xuefeng Gao,Jiale Zha,Xun Yu Zhou*

Main category: cs.LG

TL;DR: 本文用扩散模型生成未知随机微分方程（SDE）样本路径，与传统蒙特卡罗方法不同，是无模型、数据驱动的，通过实验对比并展示其在金融决策中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统蒙特卡罗模拟SDE需明确漂移和扩散系数，本文旨在提出无模型、数据驱动的新方法生成SDE样本路径。

Method: 利用条件扩散模型，根据SDE的有限样本路径生成新的合成路径。

Result: 通过模拟实验与替代基准方法对比，证明方法有效性；实证研究表明合成样本路径能提升强化学习算法在连续时间均值 - 方差投资组合选择中的性能。

Conclusion: 扩散模型在生成未知SDE样本路径上有效，在金融分析和决策中有应用前景。

Abstract: This paper introduces a new approach to generating sample paths of unknown
stochastic differential equations (SDEs) using diffusion models, a class of
generative AI models commonly employed in image and video applications. Unlike
the traditional Monte Carlo methods for simulating SDEs, which require explicit
specifications of the drift and diffusion coefficients, our method takes a
model-free, data-driven approach. Given a finite set of sample paths from an
SDE, we utilize conditional diffusion models to generate new, synthetic paths
of the same SDE. To demonstrate the effectiveness of our approach, we conduct a
simulation experiment to compare our method with alternative benchmark ones
including neural SDEs. Furthermore, in an empirical study we leverage these
synthetically generated sample paths to enhance the performance of
reinforcement learning algorithms for continuous-time mean-variance portfolio
selection, hinting promising applications of diffusion models in financial
analysis and decision-making.

</details>


### [53] [MARLINE: Multi-Source Mapping Transfer Learning for Non-Stationary Environments](https://arxiv.org/abs/2509.08176)
*Honghui Du,Leandro Minku,Huiyu Zhou*

Main category: cs.LG

TL;DR: 提出新方法 MARLINE 应对非平稳环境概念漂移，实验显示其比现有方法更准确。


<details>
  <summary>Details</summary>
Motivation: 现有解决概念漂移方法假设源模型与目标概念相似，在现实场景可能不成立，需新方法。

Method: 提出 MARLINE 方法，将目标概念投影到各源概念空间，让多个源子分类器作为集成一部分为目标概念预测做贡献。

Result: 在多个合成和真实数据集实验中，MARLINE 比多个现有数据流学习方法更准确。

Conclusion: MARLINE 能在源和目标概念不匹配的非平稳环境中利用多数据源知识，且表现优于现有方法。

Abstract: Concept drift is a major problem in online learning due to its impact on the
predictive performance of data stream mining systems. Recent studies have
started exploring data streams from different sources as a strategy to tackle
concept drift in a given target domain. These approaches make the assumption
that at least one of the source models represents a concept similar to the
target concept, which may not hold in many real-world scenarios. In this paper,
we propose a novel approach called Multi-source mApping with tRansfer LearnIng
for Non-stationary Environments (MARLINE). MARLINE can benefit from knowledge
from multiple data sources in non-stationary environments even when source and
target concepts do not match. This is achieved by projecting the target concept
to the space of each source concept, enabling multiple source sub-classifiers
to contribute towards the prediction of the target concept as part of an
ensemble. Experiments on several synthetic and real-world datasets show that
MARLINE was more accurate than several state-of-the-art data stream learning
approaches.

</details>


### [54] [The Domain Mixed Unit: A New Neural Arithmetic Layer](https://arxiv.org/abs/2509.08180)
*Paul Curry*

Main category: cs.LG

TL;DR: 提出新神经算术单元DMU，有两种初始化方式，在NALM基准测试中表现出色，代码开源。


<details>
  <summary>Details</summary>
Motivation: 设计能在对数空间和线性空间表示间混合的神经算术单元，测试其算术运算泛化能力。

Method: 提出Domain Mixed Unit (DMU)，有DMU add和DMU sub两种操作，给出两种初始化方式。

Result: DMU在NALM Benchmark上达到了最先进的性能，在乘法和除法上解决率最高。

Conclusion: DMU是一种有效的神经算术单元，将提交到开源NALM基准测试。

Abstract: The Domain Mixed Unit (DMU) is a new neural arithmetic unit that learns a
single parameter gate that mixes between log-space and linear-space
representations while performing either addition (DMU add) or subtraction (DMU
sub). Two initializations are proposed for the DMU: one covering addition and
multiplication, and another covering subtraction and division. The DMU achieves
state-of-the-art performance on the NALM Benchmark, a dataset designed to test
the ability of neural arithmetic units to generalize arithmetic operations,
specifically performing with the highest percentage solved over all seeds on
multiplication and division. The DMU will be submitted as a pull request to the
open-source NALM benchmark, and its code is available on GitHub at
https://github.com/marict?tab=repositories

</details>


### [55] [Multi-Label Transfer Learning in Non-Stationary Data Streams](https://arxiv.org/abs/2509.08181)
*Honghui Du,Leandro Minku,Aonghus Lawlor,Huiyu Zhou*

Main category: cs.LG

TL;DR: 提出两种多标签数据流迁移学习方法，实验表明其在非平稳环境中优于现有方法，证明标签间知识迁移有效。


<details>
  <summary>Details</summary>
Motivation: 多标签数据流中标签概念会漂移，多标签迁移学习研究有限，需加速适应。

Method: 提出BR - MARLENE方法，利用源和目标流不同标签知识进行多标签分类；提出BRPW - MARLENE方法，明确建模和转移成对标签依赖关系。

Result: 两种方法在非平稳环境中优于现有多标签流处理方法。

Conclusion: 标签间知识迁移可提高预测性能。

Abstract: Label concepts in multi-label data streams often experience drift in
non-stationary environments, either independently or in relation to other
labels. Transferring knowledge between related labels can accelerate
adaptation, yet research on multi-label transfer learning for data streams
remains limited. To address this, we propose two novel transfer learning
methods: BR-MARLENE leverages knowledge from different labels in both source
and target streams for multi-label classification; BRPW-MARLENE builds on this
by explicitly modelling and transferring pairwise label dependencies to enhance
learning performance. Comprehensive experiments show that both methods
outperform state-of-the-art multi-label stream approaches in non-stationary
environments, demonstrating the effectiveness of inter-label knowledge transfer
for improved predictive performance.

</details>


### [56] [Selective Induction Heads: How Transformers Select Causal Structures In Context](https://arxiv.org/abs/2509.08184)
*Francesco D'Angelo,Francesco Croce,Nicolas Flammarion*

Main category: cs.LG

TL;DR: 本文提出新框架展示transformers动态处理因果结构的能力，发现选择性归纳头，还给出3层transformer实现及理论分析。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖固定因果结构，无法捕捉自然语言中token关系随上下文动态变化的复杂性。

Method: 通过具有不同滞后的交错马尔可夫链改变因果结构，同时保持转移概率固定。

Result: transformers能通过识别正确滞后并从过去复制相应token来预测下一个token，给出3层transformer实现选择性归纳头的详细构造及理论分析。

Conclusion: 研究推进了对transformers选择因果结构的理解，为其功能和可解释性提供新见解。

Abstract: Transformers have exhibited exceptional capabilities in sequence modeling
tasks, leveraging self-attention and in-context learning. Critical to this
success are induction heads, attention circuits that enable copying tokens
based on their previous occurrences. In this work, we introduce a novel
framework that showcases transformers' ability to dynamically handle causal
structures. Existing works rely on Markov Chains to study the formation of
induction heads, revealing how transformers capture causal dependencies and
learn transition probabilities in-context. However, they rely on a fixed causal
structure that fails to capture the complexity of natural languages, where the
relationship between tokens dynamically changes with context. To this end, our
framework varies the causal structure through interleaved Markov chains with
different lags while keeping the transition probabilities fixed. This setting
unveils the formation of Selective Induction Heads, a new circuit that endows
transformers with the ability to select the correct causal structure
in-context. We empirically demonstrate that transformers learn this mechanism
to predict the next token by identifying the correct lag and copying the
corresponding token from the past. We provide a detailed construction of a
3-layer transformer to implement the selective induction head, and a
theoretical analysis proving that this mechanism asymptotically converges to
the maximum likelihood solution. Our findings advance the understanding of how
transformers select causal structures, providing new insights into their
functioning and interpretability.

</details>


### [57] [ArtifactGen: Benchmarking WGAN-GP vs Diffusion for Label-Aware EEG Artifact Synthesis](https://arxiv.org/abs/2509.08188)
*Hritik Arasu,Faisal R Jahangiri*

Main category: cs.LG

TL;DR: 研究现代生成模型能否合成适用于增强和压力测试的、带标签的脑电信号伪迹片段，比较了两种模型，WGAN - GP在频谱对齐和MMD上表现更好，两模型条件恢复能力弱，发布可复现流程。


<details>
  <summary>Details</summary>
Motivation: 脑电信号中的伪迹干扰自动分析且大规模标注成本高，研究现代生成模型能否合成合适的伪迹片段。

Method: 使用TUH EEG Artifact语料库，对不同模型进行针对性预处理，比较条件WGAN - GP和1D去噪扩散模型，从保真度、特异性和实用性三个维度评估。

Result: WGAN - GP与真实数据频谱对齐更好、MMD更低，两模型类条件恢复能力弱，限制了增强效果。

Conclusion: 发布可复现流程为脑电伪迹合成建立基线，揭示未来工作需改进的失效模式。

Abstract: Artifacts in electroencephalography (EEG) -- muscle, eye movement, electrode,
chewing, and shiver -- confound automated analysis yet are costly to label at
scale. We study whether modern generative models can synthesize realistic,
label-aware artifact segments suitable for augmentation and stress-testing.
Using the TUH EEG Artifact (TUAR) corpus, we curate subject-wise splits and
fixed-length multi-channel windows (e.g., 250 samples) with preprocessing
tailored to each model (per-window min-max for adversarial training;
per-recording/channel $z$-score for diffusion). We compare a conditional
WGAN-GP with a projection discriminator to a 1D denoising diffusion model with
classifier-free guidance, and evaluate along three axes: (i) fidelity via Welch
band-power deltas ($\Delta\delta,\ \Delta\theta,\ \Delta\alpha,\ \Delta\beta$),
channel-covariance Frobenius distance, autocorrelation $L_2$, and
distributional metrics (MMD/PRD); (ii) specificity via class-conditional
recovery with lightweight $k$NN/classifiers; and (iii) utility via augmentation
effects on artifact recognition. In our setting, WGAN-GP achieves closer
spectral alignment and lower MMD to real data, while both models exhibit weak
class-conditional recovery, limiting immediate augmentation gains and revealing
opportunities for stronger conditioning and coverage. We release a reproducible
pipeline -- data manifests, training configurations, and evaluation scripts --
to establish a baseline for EEG artifact synthesis and to surface actionable
failure modes for future work.

</details>


### [58] [Rollout-LaSDI: Enhancing the long-term accuracy of Latent Space Dynamics](https://arxiv.org/abs/2509.08191)
*Robert Stephany,Youngsoo Choi*

Main category: cs.LG

TL;DR: 提出新方法解决现代降阶模型在长时间预测PDEs时性能下降问题，并在2D Burgers方程上验证。


<details>
  <summary>Details</summary>
Motivation: 解决复杂偏微分方程需高计算成本数值方法，现代降阶模型长时间预测性能下降。

Method: 引入灵活、高阶且低成本有限差分方案，提出Rollout损失来训练降阶模型。

Result: 未明确提及具体结果，仅表明在2D Burgers方程上进行了验证。

Conclusion: 未明确提及，但暗示提出的方法有助于提升降阶模型长时间预测能力。

Abstract: Solving complex partial differential equations is vital in the physical
sciences, but often requires computationally expensive numerical methods.
Reduced-order models (ROMs) address this by exploiting dimensionality reduction
to create fast approximations. While modern ROMs can solve parameterized
families of PDEs, their predictive power degrades over long time horizons. We
address this by (1) introducing a flexible, high-order, yet inexpensive
finite-difference scheme and (2) proposing a Rollout loss that trains ROMs to
make accurate predictions over arbitrary time horizons. We demonstrate our
approach on the 2D Burgers equation.

</details>


### [59] [Ensemble Distribution Distillation for Self-Supervised Human Activity Recognition](https://arxiv.org/abs/2509.08225)
*Matthew Nolan,Lina Yao,Robert Davidson*

Main category: cs.LG

TL;DR: 本文探索了自监督学习框架中集成分布蒸馏（EDD）在人类活动识别（HAR）中的应用，提升了预测准确性、不确定性估计和鲁棒性，且不增加推理计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习技术用于人类活动识别在数据需求、可靠性和鲁棒性方面存在挑战，需新方法克服。

Method: 在自监督学习框架中应用集成分布蒸馏（EDD），利用无标签数据和部分监督训练策略，还设计了针对HAR的创新数据增强技术。

Result: 在多个公开数据集上的评估显示，该方法提高了预测准确性，能进行鲁棒的不确定性估计，增强了对抗对抗性扰动的鲁棒性。

Conclusion: 开发了自监督EDD框架，设计了创新数据增强技术，实证验证了该方法在提高鲁棒性和可靠性方面的有效性。

Abstract: Human Activity Recognition (HAR) has seen significant advancements with the
adoption of deep learning techniques, yet challenges remain in terms of data
requirements, reliability and robustness. This paper explores a novel
application of Ensemble Distribution Distillation (EDD) within a
self-supervised learning framework for HAR aimed at overcoming these
challenges. By leveraging unlabeled data and a partially supervised training
strategy, our approach yields an increase in predictive accuracy, robust
estimates of uncertainty, and substantial increases in robustness against
adversarial perturbation; thereby significantly improving reliability in
real-world scenarios without increasing computational complexity at inference.
We demonstrate this with an evaluation on several publicly available datasets.
The contributions of this work include the development of a self-supervised EDD
framework, an innovative data augmentation technique designed for HAR, and
empirical validation of the proposed method's effectiveness in increasing
robustness and reliability.

</details>


### [60] [Strategies for Improving Communication Efficiency in Distributed and Federated Learning: Compression, Local Training, and Personalization](https://arxiv.org/abs/2509.08233)
*Kai Yi*

Main category: cs.LG

TL;DR: 论文探索提升分布式与联邦学习通信效率策略，含模型压缩、本地训练和个性化，提出多个方法并经实验验证效果。


<details>
  <summary>Details</summary>
Motivation: 分布式和联邦学习中通信开销是主要瓶颈，需提升通信效率。

Method: 建立有收敛保证的压缩算子统一框架，提出含个性化的自适应本地训练策略，引入隐私保护剪枝框架，提出对称训练后剪枝方法。

Result: Scafflix在IID和non - IID设置下表现出色；Cohort - Squeeze减少跨设备开销；SymWanda在高稀疏性下增强鲁棒性且无需重新训练保持精度。

Conclusion: 实验表明在准确率、收敛性和通信方面取得良好平衡，为可扩展高效分布式学习提供理论与实践见解。

Abstract: Distributed and federated learning are essential paradigms for training
models across decentralized data sources while preserving privacy, yet
communication overhead remains a major bottleneck. This dissertation explores
strategies to improve communication efficiency, focusing on model compression,
local training, and personalization. We establish a unified framework for
biased and unbiased compression operators with convergence guarantees, then
propose adaptive local training strategies that incorporate personalization to
accelerate convergence and mitigate client drift. In particular, Scafflix
balances global and personalized objectives, achieving superior performance
under both IID and non-IID settings. We further introduce privacy-preserving
pruning frameworks that optimize sparsity while minimizing communication costs,
with Cohort-Squeeze leveraging hierarchical aggregation to reduce cross-device
overhead. Finally, SymWanda, a symmetric post-training pruning method, enhances
robustness under high sparsity and maintains accuracy without retraining.
Extensive experiments on benchmarks and large-scale language models demonstrate
favorable trade-offs among accuracy, convergence, and communication, offering
theoretical and practical insights for scalable, efficient distributed
learning.

</details>


### [61] [The CRITICAL Records Integrated Standardization Pipeline (CRISP): End-to-End Processing of Large-scale Multi-institutional OMOP CDM Data](https://arxiv.org/abs/2509.08247)
*Xiaolong Luo,Michael Lingzhi Li*

Main category: cs.LG

TL;DR: 现有重症监护EHR数据集虽有进展，但CRITICAL数据集提供了规模和多样性，不过数据 harmonization复杂。提出CRISP系统处理数据，节省研究人员时间，推动临床AI发展。


<details>
  <summary>Details</summary>
Motivation: 现有重症监护EHR数据集有局限，CRITICAL数据集虽有优势但数据处理复杂，需方法挖掘其潜力。

Method: CRISP系统通过透明数据质量管理、跨词汇映射、模块化架构并行优化和全面基准测试将原始数据转换为机器学习可用数据集。

Result: 能在标准硬件上一天内完成数据集处理，提供处理管道、基线实现和详细文档。

Conclusion: CRISP节省研究人员预处理时间，使大规模多机构重症监护数据更易获取，推动临床AI发展。

Abstract: While existing critical care EHR datasets such as MIMIC and eICU have enabled
significant advances in clinical AI research, the CRITICAL dataset opens new
frontiers by providing extensive scale and diversity -- containing 1.95 billion
records from 371,365 patients across four geographically diverse CTSA
institutions. CRITICAL's unique strength lies in capturing full-spectrum
patient journeys, including pre-ICU, ICU, and post-ICU encounters across both
inpatient and outpatient settings. This multi-institutional, longitudinal
perspective creates transformative opportunities for developing generalizable
predictive models and advancing health equity research. However, the richness
of this multi-site resource introduces substantial complexity in data
harmonization, with heterogeneous collection practices and diverse vocabulary
usage patterns requiring sophisticated preprocessing approaches.
  We present CRISP to unlock the full potential of this valuable resource.
CRISP systematically transforms raw Observational Medical Outcomes Partnership
Common Data Model data into ML-ready datasets through: (1) transparent data
quality management with comprehensive audit trails, (2) cross-vocabulary
mapping of heterogeneous medical terminologies to unified SNOMED-CT standards,
with deduplication and unit standardization, (3) modular architecture with
parallel optimization enabling complete dataset processing in $<$1 day even on
standard computing hardware, and (4) comprehensive baseline model benchmarks
spanning multiple clinical prediction tasks to establish reproducible
performance standards. By providing processing pipeline, baseline
implementations, and detailed transformation documentation, CRISP saves
researchers months of preprocessing effort and democratizes access to
large-scale multi-institutional critical care data, enabling them to focus on
advancing clinical AI.

</details>


### [62] [Mitigating Catastrophic Forgetting in Large Language Models with Forgetting-aware Pruning](https://arxiv.org/abs/2509.08255)
*Wei Huang,Anda Cheng,Yinggui Wang*

Main category: cs.LG

TL;DR: 本文提出遗忘感知剪枝指标（FAPM）解决大语言模型微调时的灾难性遗忘问题，实验效果好且开源代码。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在微调时面临灾难性遗忘问题，需要平衡灾难性遗忘和下游任务性能。

Method: 提出FAPM，以任务向量与预训练模型参数的比例量化灾难性遗忘，并将其融入剪枝标准，且无需修改训练过程、模型架构和使用辅助数据。

Result: 在八个数据集上实验，FAPM将灾难性遗忘限制在0.25%，同时下游任务保持99.67%的准确率。

Conclusion: FAPM能有效平衡灾难性遗忘和下游任务性能。

Abstract: Recent advancements in large language models (LLMs) have shown impressive
capabilities in various downstream tasks but typically face Catastrophic
Forgetting (CF) during fine-tuning. In this paper, we propose the
Forgetting-Aware Pruning Metric (FAPM), a novel pruning-based approach to
balance CF and downstream task performance. Our investigation reveals that the
degree to which task vectors (i.e., the subtraction of pre-trained weights from
the weights fine-tuned on downstream tasks) overlap with pre-trained model
parameters is a critical factor for CF. Based on this finding, FAPM employs the
ratio of the task vector to pre-trained model parameters as a metric to
quantify CF, integrating this measure into the pruning criteria. Importantly,
FAPM does not necessitate modifications to the training process or model
architecture, nor does it require any auxiliary data. We conducted extensive
experiments across eight datasets, covering natural language inference, General
Q&A, Medical Q&A, Math Q&A, reading comprehension, and cloze tests. The results
demonstrate that FAPM limits CF to just 0.25\% while maintaining 99.67\%
accuracy on downstream tasks. We provide the code to reproduce our results.

</details>


### [63] [Interpretable Physics Reasoning and Performance Taxonomy in Vision-Language Models](https://arxiv.org/abs/2509.08270)
*Pranav Pawar,Kavish Shah,Akshat Bhalani,Komal Kasat,Dev Mittal,Hadi Gala,Deepali Patil,Nikita Raichada,Monali Deshmukh*

Main category: cs.LG

TL;DR: 引入新框架评估VLMs对2D物理理解，评估四个模型发现模型规模与推理能力相关，模型处理抽象空间推理问题有困难。


<details>
  <summary>Details</summary>
Motivation: 随着VLMs发展，其对物理等科学原理的理解尚待探索，需评估其对2D物理的理解能力。

Method: 引入新框架，用场景生成器创建超400个涵盖四个核心领域的问题测试集，评估四个SOTA的VLMs。

Result: 模型规模与推理能力强相关，Qwen2.5 - VL - 7B总分0.815，模型处理公式问题表现好，处理抽象空间推理问题困难。

Conclusion: 设计该框架可促进VLMs科学推理研究，深入了解其能力与局限。

Abstract: As Vision-Language Models (VLMs) grow in sophistication, their ability to
perform reasoning is coming under increasing supervision. While they excel at
many tasks, their grasp of fundamental scientific principles, such as physics,
remains an underexplored frontier. To reflect the advancements in these
capabilities, we introduce a novel and accessible framework designed to
rigorously evaluate VLMs on their understanding of 2D physics. Our framework
features a pragmatic scenario generator that creates a diverse testbed of over
400 problems across four core domains: Projectile Motion, Collision Dynamics,
Mechanics, and Fluid Dynamics. Through comprehensive evaluation of four
state-of-the-art VLMs, we demonstrate a strong correlation between model scale
and reasoning ability, with our top-performing model, Qwen2.5-VL-7B, achieving
an overall score of 0.815. We find that while models excel at formulaic
problems, they struggle significantly with domains requiring abstract spatial
reasoning. By designing this framework, we aim to democratize the study of
scientific reasoning in VLMs and foster deeper insights into their capabilities
and limitations.

</details>


### [64] [Adaptive Rainfall Forecasting from Multiple Geographical Models Using Matrix Profile and Ensemble Learning](https://arxiv.org/abs/2509.08277)
*Dung T. Tran,Huyen Ngoc Huyen,Hong Nguyen,Xuan-Vu Phan,Nam-Phong Nguyen*

Main category: cs.LG

TL;DR: 本文提出MPWE框架用于越南降雨预测，实验显示其比地理模型和集成基线更准确稳定。


<details>
  <summary>Details</summary>
Motivation: 越南气候多样、地理差异大，准确可靠的降雨预报对洪水管理、水电运营和灾害防范至关重要。

Method: 提出基于矩阵轮廓的加权集成（MPWE）的制度转换框架，动态捕捉多个地理模型预报之间的协变依赖，并结合冗余感知加权来平衡各模型的贡献。

Result: 使用越南八个主要流域的降雨预报进行评估，MPWE预测误差的均值和标准差始终低于地理模型和集成基线。

Conclusion: MPWE在各流域和预测期内都提高了降雨预报的准确性和稳定性。

Abstract: Rainfall forecasting in Vietnam is highly challenging due to its diverse
climatic conditions and strong geographical variability across river basins,
yet accurate and reliable forecasts are vital for flood management, hydropower
operation, and disaster preparedness. In this work, we propose a Matrix
Profile-based Weighted Ensemble (MPWE), a regime-switching framework that
dynamically captures covariant dependencies among multiple geographical model
forecasts while incorporating redundancy-aware weighting to balance
contributions across models. We evaluate MPWE using rainfall forecasts from
eight major basins in Vietnam, spanning five forecast horizons (1 hour and
accumulated rainfall over 12, 24, 48, 72, and 84 hours). Experimental results
show that MPWE consistently achieves lower mean and standard deviation of
prediction errors compared to geographical models and ensemble baselines,
demonstrating both improved accuracy and stability across basins and horizons.

</details>


### [65] [\emph{FoQuS}: A Forgetting-Quality Coreset Selection Framework for Automatic Modulation Recognition](https://arxiv.org/abs/2509.08300)
*Yao Lu,Chunfeng Sun,Dongwei Xu,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.LG

TL;DR: 提出FoQuS方法，从原始数据集选核心集近似全量训练效果，减少训练开销，用少量数据能保持高识别准确率和泛化性。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的自动调制识别（AMR）模型在开发新模型或超参数调优时，重复使用大量数据训练的时间和能耗难以承受。

Method: 提出FoQuS方法，记录全量数据集训练时每个样本的预测轨迹，基于训练动态构建三个重要性指标来选择核心集。

Result: FoQuS在多个AMR数据集上仅使用1%-30%的原始数据就能保持高识别准确率和良好的跨架构泛化性。

Conclusion: FoQuS能有效减少训练开销，同时保证模型性能。

Abstract: Deep learning-based Automatic Modulation Recognition (AMR) model has made
significant progress with the support of large-scale labeled data. However,
when developing new models or performing hyperparameter tuning, the time and
energy consumption associated with repeated training using massive amounts of
data are often unbearable. To address the above challenges, we propose
\emph{FoQuS}, which approximates the effect of full training by selecting a
coreset from the original dataset, thereby significantly reducing training
overhead. Specifically, \emph{FoQuS} records the prediction trajectory of each
sample during full-dataset training and constructs three importance metrics
based on training dynamics. Experiments show that \emph{FoQuS} can maintain
high recognition accuracy and good cross-architecture generalization on
multiple AMR datasets using only 1\%-30\% of the original data.

</details>


### [66] [EvolKV: Evolutionary KV Cache Compression for LLM Inference](https://arxiv.org/abs/2509.08315)
*Bohan Yu,Yekun Chai*

Main category: cs.LG

TL;DR: 提出EvolKV框架用于KV缓存压缩，联合优化内存效率和任务性能，实验显示其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法依赖启发式，忽略层特征模式与任务性能相互作用，导致泛化性下降。

Method: 将缓存分配重新表述为多目标优化问题，利用进化搜索动态配置层预算，直接最大化下游性能。

Result: 在11个任务上的实验表明，在长上下文任务的各种KV缓存预算下优于所有基线方法，在GSM8K上比启发式基线高7个百分点，在代码完成任务中用1.5%预算优于全KV缓存设置。

Conclusion: 学习的压缩策略在KV缓存预算分配方面有未挖掘的潜力。

Abstract: Existing key-value (KV) cache compression methods typically rely on
heuristics, such as uniform cache allocation across layers or static eviction
policies, however, they ignore the critical interplays among layer-specific
feature patterns and task performance, which can lead to degraded
generalization. In this paper, we propose EvolKV, an adaptive framework for
layer-wise, task-driven KV cache compression that jointly optimizes the memory
efficiency and task performance. By reformulating cache allocation as a
multi-objective optimization problem, EvolKV leverages evolutionary search to
dynamically configure layer budgets while directly maximizing downstream
performance. Extensive experiments on 11 tasks demonstrate that our approach
outperforms all baseline methods across a wide range of KV cache budgets on
long-context tasks and surpasses heuristic baselines by up to 7 percentage
points on GSM8K. Notably, EvolKV achieves superior performance over the full KV
cache setting on code completion while utilizing only 1.5% of the original
budget, suggesting the untapped potential in learned compression strategies for
KV cache budget allocation.

</details>


### [67] [Accelerating Reinforcement Learning Algorithms Convergence using Pre-trained Large Language Models as Tutors With Advice Reusing](https://arxiv.org/abs/2509.08329)
*Lukas Toral,Teddy Lazebnik*

Main category: cs.LG

TL;DR: 研究探索预训练大语言模型（LLM）作为导师对强化学习（RL）算法的加速效果，发现LLM辅导显著加速收敛且性能相当，建议重用机制可进一步缩短训练时间但收敛稳定性下降。


<details>
  <summary>Details</summary>
Motivation: RL算法训练时间长，现有加速技术针对性强且依赖开发者专业知识，因此探索用预训练LLM作为导师加速RL收敛。

Method: 采用学生 - 教师架构，通过54种配置的大量实证检验，改变RL算法（DQN、PPO、A2C）、LLM导师（Llama、Vicuna、DeepSeek）和环境（Blackjack、Snake、Connect Four）。

Result: LLM辅导显著加速RL收敛，性能相当；建议重用机制进一步缩短训练时间，但收敛动态稳定性降低。

Conclusion: LLM辅导总体上改善收敛，其有效性受特定任务、RL算法和LLM模型组合的影响。

Abstract: Reinforcement Learning (RL) algorithms often require long training to become
useful, especially in complex environments with sparse rewards. While
techniques like reward shaping and curriculum learning exist to accelerate
training, these are often extremely specific and require the developer's
professionalism and dedicated expertise in the problem's domain. Tackling this
challenge, in this study, we explore the effectiveness of pre-trained Large
Language Models (LLMs) as tutors in a student-teacher architecture with RL
algorithms, hypothesizing that LLM-generated guidance allows for faster
convergence. In particular, we explore the effectiveness of reusing the LLM's
advice on the RL's convergence dynamics. Through an extensive empirical
examination, which included 54 configurations, varying the RL algorithm (DQN,
PPO, A2C), LLM tutor (Llama, Vicuna, DeepSeek), and environment (Blackjack,
Snake, Connect Four), our results demonstrate that LLM tutoring significantly
accelerates RL convergence while maintaining comparable optimal performance.
Furthermore, the advice reuse mechanism shows a further improvement in training
duration but also results in less stable convergence dynamics. Our findings
suggest that LLM tutoring generally improves convergence, and its effectiveness
is sensitive to the specific task, RL algorithm, and LLM model combination.

</details>


### [68] [Accelerating Mixture-of-Expert Inference with Adaptive Expert Split Mechanism](https://arxiv.org/abs/2509.08342)
*Jiaming Yan,Jianchun Liu,Hongli Xu,Liusheng Huang*

Main category: cs.LG

TL;DR: 本文针对MoE大语言模型推理时GPU内存需求高、推理速度慢的问题，提出MoEpic系统及自适应缓存配置算法，实验证明该系统能节省GPU成本并降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有MoE大语言模型因大量参数导致GPU内存需求高，现有专家参数卸载方法推理速度慢，需改进。

Method: 提出MoEpic系统，采用垂直分割专家的机制，缓存热门专家的顶部片段，预测并预取激活专家；提出基于定点迭代的分治算法进行自适应缓存配置。

Result: 在流行的MoE大语言模型上实验表明，MoEpic可节省约一半的GPU成本，推理延迟比基线降低约37.51%-65.73%。

Conclusion: MoEpic系统能有效解决MoE大语言模型推理时的GPU内存和速度问题，具有良好性能。

Abstract: Mixture-of-Experts (MoE) has emerged as a promising architecture for modern
large language models (LLMs). However, massive parameters impose heavy GPU
memory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.
Offloading the expert parameters to CPU RAM offers an effective way to
alleviate the VRAM requirements for MoE inference. Existing approaches
typically cache a small subset of experts in VRAM and dynamically prefetch
experts from RAM during inference, leading to significant degradation in
inference speed due to the poor cache hit rate and substantial expert loading
latency. In this work, we propose MoEpic, an efficient MoE inference system
with a novel expert split mechanism. Specifically, each expert is vertically
divided into two segments: top and bottom. MoEpic caches the top segment of hot
experts, so that more experts will be stored under the limited VRAM budget,
thereby improving the cache hit rate. During each layer's inference, MoEpic
predicts and prefetches the activated experts for the next layer. Since the top
segments of cached experts are exempt from fetching, the loading time is
reduced, which allows efficient transfer-computation overlap. Nevertheless, the
performance of MoEpic critically depends on the cache configuration (i.e., each
layer's VRAM budget and expert split ratio). To this end, we propose a
divide-and-conquer algorithm based on fixed-point iteration for adaptive cache
configuration. Extensive experiments on popular MoE LLMs demonstrate that
MoEpic can save about half of the GPU cost, while lowering the inference
latency by about 37.51%-65.73% compared to the baselines.

</details>


### [69] [Prediction Loss Guided Decision-Focused Learning](https://arxiv.org/abs/2509.08359)
*Haeun Jeon,Hyunglip Bae,Chanyeong Kim,Yongjae Lee,Woo Chang Kim*

Main category: cs.LG

TL;DR: 本文指出传统预测学习和决策聚焦学习的问题，提出用预测损失梯度扰动决策损失梯度构建更新方向的方法，有理论收敛保证，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 传统预测聚焦学习（PFL）和决策聚焦学习（DFL）各有不足，PFL 忽略下游决策质量，DFL 收敛不稳定。

Method: 用预测损失梯度扰动决策损失梯度构建更新方向，使用类 sigmoid 衰减参数，让预测损失梯度引导决策损失梯度训练预测模型。

Result: 在三个随机优化问题上实验，相比其他基线方法有良好效果，训练更稳定、遗憾值更低。

Conclusion: 提出的方法有效，能在 PFL 或 DFL 表现不佳的情况下，实现更稳定训练并降低遗憾值。

Abstract: Decision-making under uncertainty is often considered in two stages:
predicting the unknown parameters, and then optimizing decisions based on
predictions. While traditional prediction-focused learning (PFL) treats these
two stages separately, decision-focused learning (DFL) trains the predictive
model by directly optimizing the decision quality in an end-to-end manner.
However, despite using exact or well-approximated gradients, vanilla DFL often
suffers from unstable convergence due to its flat-and-sharp loss landscapes. In
contrast, PFL yields more stable optimization, but overlooks the downstream
decision quality. To address this, we propose a simple yet effective approach:
perturbing the decision loss gradient using the prediction loss gradient to
construct an update direction. Our method requires no additional training and
can be integrated with any DFL solvers. Using the sigmoid-like decaying
parameter, we let the prediction loss gradient guide the decision loss gradient
to train a predictive model that optimizes decision quality. Also, we provide a
theoretical convergence guarantee to Pareto stationary point under mild
assumptions. Empirically, we demonstrate our method across three stochastic
optimization problems, showing promising results compared to other baselines.
We validate that our approach achieves lower regret with more stable training,
even in situations where either PFL or DFL struggles.

</details>


### [70] [Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models](https://arxiv.org/abs/2509.08372)
*Kosuke Kihara,Junki Mori,Taiki Miyagawa,Akinori F. Ebihara*

Main category: cs.LG

TL;DR: 文章将FFREEDA框架拓展到CI - FFREEDA，提出用冻结的视觉基础模型替换FFREEDA骨干，实验表明该方法有效且强特征提取器是联邦学习成功关键。


<details>
  <summary>Details</summary>
Motivation: 现有研究关注FFREEDA，文章将其拓展到更复杂现实的Class Imbalanced FFREEDA场景，并反思现有方法的侧重点。

Method: 用冻结的视觉基础模型（VFM）替换FFREEDA骨干。

Result: VFM有效减轻了域差距、类别不平衡以及目标客户端之间非IID的影响。

Conclusion: 强特征提取器而非复杂的适应或联邦学习方法是现实世界联邦学习成功的关键。

Abstract: Federated Learning (FL) offers a framework for training models
collaboratively while preserving data privacy of each client. Recently,
research has focused on Federated Source-Free Domain Adaptation (FFREEDA), a
more realistic scenario wherein client-held target domain data remains
unlabeled, and the server can access source domain data only during
pre-training. We extend this framework to a more complex and realistic setting:
Class Imbalanced FFREEDA (CI-FFREEDA), which takes into account class
imbalances in both the source and target domains, as well as label shifts
between source and target and among target clients. The replication of existing
methods in our experimental setup lead us to rethink the focus from enhancing
aggregation and domain adaptation methods to improving the feature extractors
within the network itself. We propose replacing the FFREEDA backbone with a
frozen vision foundation model (VFM), thereby improving overall accuracy
without extensive parameter tuning and reducing computational and communication
costs in federated learning. Our experimental results demonstrate that VFMs
effectively mitigate the effects of domain gaps, class imbalances, and even
non-IID-ness among target clients, suggesting that strong feature extractors,
not complex adaptation or FL methods, are key to success in the real-world FL.

</details>


### [71] [Efficient Decoding Methods for Language Models on Encrypted Data](https://arxiv.org/abs/2509.08383)
*Matan Avitan,Moran Baruch,Nir Drucker,Itamar Zimerman,Yoav Goldberg*

Main category: cs.LG

TL;DR: 本文提出HE友好的argmax算法cutmax和首个HE兼容的核采样方法，支持隐私保护下高效推理，评估显示延迟显著降低。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理敏感数据有隐私问题，同态加密用于安全推理时，神经文本生成的解码方法计算开销大，存在性能瓶颈。

Method: 引入cutmax算法减少密文操作，提出HE兼容的核采样方法，二者为多项式且可微分。

Result: 评估显示在现实大语言模型输出上，相比基线延迟降低24 - 35倍。

Conclusion: 所提技术推进了安全文本生成。

Abstract: Large language models (LLMs) power modern AI applications, but processing
sensitive data on untrusted servers raises privacy concerns. Homomorphic
encryption (HE) enables computation on encrypted data for secure inference.
However, neural text generation requires decoding methods like argmax and
sampling, which are non-polynomial and thus computationally expensive under
encryption, creating a significant performance bottleneck. We introduce cutmax,
an HE-friendly argmax algorithm that reduces ciphertext operations compared to
prior methods, enabling practical greedy decoding under encryption. We also
propose the first HE-compatible nucleus (top-p) sampling method, leveraging
cutmax for efficient stochastic decoding with provable privacy guarantees. Both
techniques are polynomial, supporting efficient inference in privacy-preserving
settings. Moreover, their differentiability facilitates gradient-based
sequence-level optimization as a polynomial alternative to straight-through
estimators. We further provide strong theoretical guarantees for cutmax,
proving it converges globally to a unique two-level fixed point, independent of
the input values beyond the identity of the maximizer, which explains its rapid
convergence in just a few iterations. Evaluations on realistic LLM outputs show
latency reductions of 24x-35x over baselines, advancing secure text generation.

</details>


### [72] [Two Sides of the Same Optimization Coin: Model Degradation and Representation Collapse in Graph Foundation Models](https://arxiv.org/abs/2509.08401)
*Xunkai Li,Daohan Su,Sicheng Liu,Ru Zhang,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 本文指出图基础模型graph VQ - MAE存在域泛化冲突问题，提出MoT方法解决，实验显示MoT在多场景有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决图基础模型graph VQ - MAE在预训练时因域泛化冲突导致的优化困境。

Method: 提出MoT方法，包含信息修补（利用边语义融合策略和带域感知路由的码本混合）和正则化修补（添加两个额外正则化）。

Result: 在6个领域的22个数据集上实验，MoT在监督、少样本和零样本场景均显著优于SOTA基线。

Conclusion: MoT能解决图基础模型的问题，且符合GFM缩放定律，模型规模可控。

Abstract: Graph foundation models, inspired by the success of LLMs, are designed to
learn the optimal embedding from multi-domain TAGs for the downstream
cross-task generalization capability. During our investigation, graph VQ-MAE
stands out among the increasingly diverse landscape of GFM architectures. This
is attributed to its ability to jointly encode topology and textual attributes
from multiple domains into discrete embedding spaces with clear semantic
boundaries. Despite its potential, domain generalization conflicts cause
imperceptible pitfalls. In this paper, we instantiate two of them, and they are
just like two sides of the same GFM optimization coin - Side 1 Model
Degradation: The encoder and codebook fail to capture the diversity of inputs;
Side 2 Representation Collapse: The hidden embedding and codebook vector fail
to preserve semantic separability due to constraints from narrow representation
subspaces. These two pitfalls (sides) collectively impair the decoder and
generate the low-quality reconstructed supervision, causing the GFM
optimization dilemma during pre-training (coin). Through empirical
investigation, we attribute the above challenges to Information Bottleneck and
Regularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) -
(1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic
fusion strategy and a mixture-of-codebooks with domain-aware routing to improve
information capacity. (2) Regularization Tinker for Optimization Coin, which
utilizes two additional regularizations to further improve gradient supervision
in our proposed Information Tinker. Notably, as a flexible architecture, MoT
adheres to the scaling laws of GFM, offering a controllable model scale.
Compared to SOTA baselines, experiments on 22 datasets across 6 domains
demonstrate that MoT achieves significant improvements in supervised, few-shot,
and zero-shot scenarios.

</details>


### [73] [Adapting Vision-Language Models for Neutrino Event Classification in High-Energy Physics](https://arxiv.org/abs/2509.08461)
*Dikshant Sagar,Kaiwen Yu,Alejandro Yankelevich,Jianming Bian,Pierre Baldi*

Main category: cs.LG

TL;DR: 本文探索微调版LLaMa 3.2 VLM用于高能物理实验中中微子相互作用识别，对比CNN，发现VLM表现更优且有更好的灵活性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 探索VLMs在高能物理实验像素化探测器数据中识别中微子相互作用的应用。

Method: 使用微调版LLaMa 3.2 VLM进行中微子相互作用识别，并与NOvA和DUNE实验中使用的CNN架构进行对比，评估模型分类性能和可解释性。

Result: VLMs在中微子事件分类上优于CNNs，在整合辅助文本或语义信息方面更灵活，预测更具可解释性。

Conclusion: VLMs有潜力作为物理事件分类的通用骨干，为实验中微子物理的多模态推理开辟新途径。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated their
remarkable capacity to process and reason over structured and unstructured data
modalities beyond natural language. In this work, we explore the applications
of Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa
3.2, to the task of identifying neutrino interactions in pixelated detector
data from high-energy physics (HEP) experiments. We benchmark this model
against a state-of-the-art convolutional neural network (CNN) architecture,
similar to those used in the NOvA and DUNE experiments, which have achieved
high efficiency and purity in classifying electron and muon neutrino events.
Our evaluation considers both the classification performance and
interpretability of the model predictions. We find that VLMs can outperform
CNNs, while also providing greater flexibility in integrating auxiliary textual
or semantic information and offering more interpretable, reasoning-based
predictions. This work highlights the potential of VLMs as a general-purpose
backbone for physics event classification, due to their high performance,
interpretability, and generalizability, which opens new avenues for integrating
multimodal reasoning in experimental neutrino physics.

</details>


### [74] [An Interpretable Deep Learning Model for General Insurance Pricing](https://arxiv.org/abs/2509.08467)
*Patrick J. Laub,Tu Pho,Bernard Wong*

Main category: cs.LG

TL;DR: 本文介绍精算神经加法模型用于一般保险定价，兼具可解释性与强预测力，对比实验显示其表现出色。


<details>
  <summary>Details</summary>
Motivation: 开发一种在一般保险定价中既具有神经网络强预测能力，又能提供完全透明和可解释结果的模型。

Method: 为每个协变量和两两交互项分配专用神经网络，施加架构约束以满足可解释性和实际需求，建立保险场景下可解释性的定义和数学框架，并与传统精算和先进机器学习方法对比。

Result: 在大多数情况下，所提出的模型优于其他方法，且内部逻辑完全透明。

Conclusion: 该模型具有很强的可解释性和预测能力。

Abstract: This paper introduces the Actuarial Neural Additive Model, an inherently
interpretable deep learning model for general insurance pricing that offers
fully transparent and interpretable results while retaining the strong
predictive power of neural networks. This model assigns a dedicated neural
network (or subnetwork) to each individual covariate and pairwise interaction
term to independently learn its impact on the modeled output while implementing
various architectural constraints to allow for essential interpretability (e.g.
sparsity) and practical requirements (e.g. smoothness, monotonicity) in
insurance applications. The development of our model is grounded in a solid
foundation, where we establish a concrete definition of interpretability within
the insurance context, complemented by a rigorous mathematical framework.
Comparisons in terms of prediction accuracy are made with traditional actuarial
and state-of-the-art machine learning methods using both synthetic and real
insurance datasets. The results show that the proposed model outperforms other
methods in most cases while offering complete transparency in its internal
logic, underscoring the strong interpretability and predictive capability.

</details>


### [75] [SHAining on Process Mining: Explaining Event Log Characteristics Impact on Algorithms](https://arxiv.org/abs/2509.08482)
*Andrea Maldonado,Christian M. M. Frey,Sai Anirudh Aryasomayajula,Ludwig Zellner,Stephan A. Fahrenkrog-Petersen,Thomas Seidl*

Main category: cs.LG

TL;DR: 提出SHAining方法量化事件日志特征对流程挖掘算法指标的边际贡献，分析超22000个日志以揭示关键特征并评估算法鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏对事件日志特征如何单独影响算法的系统分析，且常忽略特征重叠影响，需评估其对评估指标的影响。

Method: 引入SHAining方法，以流程发现为下游任务，分析超22000个涵盖多种特征的事件日志。

Result: 揭示了对算法各指标影响最大的事件日志特征，获得特征值与贡献影响相关性的新见解。

Conclusion: SHAining方法有助于量化事件日志特征影响，评估算法鲁棒性。

Abstract: Process mining aims to extract and analyze insights from event logs, yet
algorithm metric results vary widely depending on structural event log
characteristics. Existing work often evaluates algorithms on a fixed set of
real-world event logs but lacks a systematic analysis of how event log
characteristics impact algorithms individually. Moreover, since event logs are
generated from processes, where characteristics co-occur, we focus on
associational rather than causal effects to assess how strong the overlapping
individual characteristic affects evaluation metrics without assuming isolated
causal effects, a factor often neglected by prior work. We introduce SHAining,
the first approach to quantify the marginal contribution of varying event log
characteristics to process mining algorithms' metrics. Using process discovery
as a downstream task, we analyze over 22,000 event logs covering a wide span of
characteristics to uncover which affect algorithms across metrics (e.g.,
fitness, precision, complexity) the most. Furthermore, we offer novel insights
about how the value of event log characteristics correlates with their
contributed impact, assessing the algorithm's robustness.

</details>


### [76] [Heart Disease Prediction: A Comparative Study of Optimisers Performance in Deep Neural Networks](https://arxiv.org/abs/2509.08499)
*Chisom Chibuike,Adeyinka Ogunsanya*

Main category: cs.LG

TL;DR: 本文对比10种优化器在训练多层感知机模型预测心脏病时的表现，选RMSProp为最有效优化器，建议在计算资源充足时全面评估选优化器。


<details>
  <summary>Details</summary>
Motivation: 优化在深度学习模型训练中重要，但优化器选择受关注少，需深入研究优化器选择及决定因素。

Method: 用Kaggle心脏病数据集训练多层感知机模型，设置一致训练范式，基于收敛速度、稳定性等评估优化器 ，还考虑AUC、Precision、Recall等机器学习评估指标。

Result: 收敛速度和稳定性存在权衡，Adagrad和Adadelta更稳定但收敛慢；RMSProp在关键指标上表现平衡，精准率0.765、召回率0.827、AUC为0.841且训练快，但不是最稳定。

Conclusion: 在计算资源不受限环境中，应全面评估选优化器以提升深度学习模型训练科学性和性能。

Abstract: Optimization has been an important factor and topic of interest in training
deep learning models, yet less attention has been given to how we select the
optimizers we use to train these models. Hence, there is a need to dive deeper
into how we select the optimizers we use for training and the metrics that
determine this selection. In this work, we compare the performance of 10
different optimizers in training a simple Multi-layer Perceptron model using a
heart disease dataset from Kaggle. We set up a consistent training paradigm and
evaluate the optimizers based on metrics such as convergence speed and
stability. We also include some other Machine Learning Evaluation metrics such
as AUC, Precision, and Recall, which are central metrics to classification
problems. Our results show that there are trade-offs between convergence speed
and stability, as optimizers like Adagrad and Adadelta, which are more stable,
took longer time to converge. Across all our metrics, we chose RMSProp to be
the most effective optimizer for this heart disease prediction task because it
offered a balanced performance across key metrics. It achieved a precision of
0.765, a recall of 0.827, and an AUC of 0.841, along with faster training time.
However, it was not the most stable. We recommend that, in less
compute-constrained environments, this method of choosing optimizers through a
thorough evaluation should be adopted to increase the scientific nature and
performance in training deep learning models.

</details>


### [77] [Variational Rank Reduction Autoencoders for Generative](https://arxiv.org/abs/2509.08515)
*Alicia Tierz,Jad Mounayer,Beatriz Moya,Francisco Chinesta*

Main category: cs.LG

TL;DR: 本文提出结合VRRAE与DeepONets的混合框架解决生成热设计难题，提升设计质量、预测精度和推理效率。


<details>
  <summary>Details</summary>
Motivation: 生成热设计面临高计算成本和传统生成模型局限，如AE和VAE产生非结构化潜在空间。

Method: 提出结合VRRAE与DeepONets的混合框架，VRRAE引入截断SVD，DeepONet利用紧凑潜在编码预测温度梯度。

Result: 混合方法提升生成几何质量、梯度预测精度，推理效率优于传统数值求解器。

Conclusion: 强调结构化潜在表示对算子学习的重要性，凸显生成模型和算子网络结合在热设计及工程应用的潜力。

Abstract: Generative thermal design for complex geometries is fundamental in many areas
of engineering, yet it faces two main challenges: the high computational cost
of high-fidelity simulations and the limitations of conventional generative
models. Approaches such as autoencoders (AEs) and variational autoencoders
(VAEs) often produce unstructured latent spaces with discontinuities, which
restricts their capacity to explore designs and generate physically consistent
solutions.
  To address these limitations, we propose a hybrid framework that combines
Variational Rank-Reduction Autoencoders (VRRAEs) with Deep Operator Networks
(DeepONets). The VRRAE introduces a truncated SVD within the latent space,
leading to continuous, interpretable, and well-structured representations that
mitigate posterior collapse and improve geometric reconstruction. The DeepONet
then exploits this compact latent encoding in its branch network, together with
spatial coordinates in the trunk network, to predict temperature gradients
efficiently and accurately.
  This hybrid approach not only enhances the quality of generated geometries
and the accuracy of gradient prediction, but also provides a substantial
advantage in inference efficiency compared to traditional numerical solvers.
Overall, the study underscores the importance of structured latent
representations for operator learning and highlights the potential of combining
generative models and operator networks in thermal design and broader
engineering applications.

</details>


### [78] [Data Skeleton Learning: Scalable Active Clustering with Sparse Graph Structures](https://arxiv.org/abs/2509.08530)
*Wen-Bo Xie,Xun Fu,Bin Chen,Yan-Li Lee,Tao Deng,Tian Zou,Xin Wang,Zhen Liu,Jaideep Srivastavad*

Main category: cs.LG

TL;DR: 提出基于图的主动聚类算法，在计算性能、可扩展性等方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决基于成对约束的主动聚类在处理大规模数据时的效率和可扩展性问题，包括降低计算成本、减少标注需求和内存使用。

Method: 提出基于图的主动聚类算法，利用两个稀疏图（数据骨架图和更新图）协同工作，细化数据骨架中的连通子图以创建嵌套集群。

Result: 算法能以更少的用户约束实现更准确的聚类，在计算性能和可扩展性上优于同类算法，且在不同距离度量下保持稳健。

Conclusion: 所提算法有效提升了基于成对约束的主动聚类的效率和可扩展性。

Abstract: In this work, we focus on the efficiency and scalability of pairwise
constraint-based active clustering, crucial for processing large-scale data in
applications such as data mining, knowledge annotation, and AI model
pre-training. Our goals are threefold: (1) to reduce computational costs for
iterative clustering updates; (2) to enhance the impact of user-provided
constraints to minimize annotation requirements for precise clustering; and (3)
to cut down memory usage in practical deployments. To achieve these aims, we
propose a graph-based active clustering algorithm that utilizes two sparse
graphs: one for representing relationships between data (our proposed data
skeleton) and another for updating this data skeleton. These two graphs work in
concert, enabling the refinement of connected subgraphs within the data
skeleton to create nested clusters. Our empirical analysis confirms that the
proposed algorithm consistently facilitates more accurate clustering with
dramatically less input of user-provided constraints, and outperforms its
counterparts in terms of computational performance and scalability, while
maintaining robustness across various distance metrics.

</details>


### [79] [MAESTRO: Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization](https://arxiv.org/abs/2509.08578)
*Hong Liu*

Main category: cs.LG

TL;DR: 提出MAESTRO模型用于流感发病率预测，结合多模态数据和频谱 - 时间架构，在香港流感数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 及时且稳健的流感发病率预测对公共卫生决策至关重要，需新方法解决预测问题。

Method: 提出MAESTRO模型，自适应融合多模态输入，利用频谱 - 时间架构，包括分解时间序列、混合特征增强管道、跨通道注意力机制、时间投影头和不确定性量化。

Result: 在超11年香港流感数据上评估，MAESTRO表现出强竞争力，R平方达0.956，消融实验证实多模态融合和频谱 - 时间组件有重要贡献。

Conclusion: 公开的管道提供强大统一框架，证明先进频谱 - 时间建模和多模态数据融合对稳健流行病学预测有关键协同作用。

Abstract: Timely and robust influenza incidence forecasting is critical for public
health decision-making. To address this, we present MAESTRO, a Multi-modal
Adaptive Ensemble for Spectro-Temporal Robust Optimization. MAESTRO achieves
robustness by adaptively fusing multi-modal inputs-including surveillance, web
search trends, and meteorological data-and leveraging a comprehensive
spectro-temporal architecture. The model first decomposes time series into
seasonal and trend components. These are then processed through a hybrid
feature enhancement pipeline combining Transformer-based encoders, a Mamba
state-space model for long-range dependencies, multi-scale temporal
convolutions, and a frequency-domain analysis module. A cross-channel attention
mechanism further integrates information across the different data modalities.
Finally, a temporal projection head performs sequence-to-sequence forecasting,
with an optional estimator to quantify prediction uncertainty. Evaluated on
over 11 years of Hong Kong influenza data (excluding the COVID-19 period),
MAESTRO shows strong competitive performance, demonstrating a superior model
fit and relative accuracy, achieving a state-of-the-art R-square of 0.956.
Extensive ablations confirm the significant contributions of both multi-modal
fusion and the spectro-temporal components. Our modular and reproducible
pipeline is made publicly available to facilitate deployment and extension to
other regions and pathogens.Our publicly available pipeline presents a
powerful, unified framework, demonstrating the critical synergy of advanced
spectro-temporal modeling and multi-modal data fusion for robust
epidemiological forecasting.

</details>


### [80] [Interpretability as Alignment: Making Internal Understanding a Design Principle](https://arxiv.org/abs/2509.08592)
*Aadit Sengupta,Pratinav Seth,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: 应将可解释性尤其是机械方法作为AI对齐的设计原则，而非辅助诊断工具，虽面临挑战，但应将其作为AI研发的首要目标。


<details>
  <summary>Details</summary>
Motivation: 大型神经模型在高风险场景应用，需确保其行为与人类价值观一致，可解释性提供了内部透明途径。

Method: 提出应将可解释性尤其是机械方法作为对齐设计原则，对比事后方法（如LIME、SHAP）和机械技术（如电路追踪、激活修补）。

Result: 机械技术能洞察内部故障，包括行为方法可能忽略的欺骗或未对齐推理。

Conclusion: 安全可靠AI的进展取决于将可解释性作为AI研发的首要目标，确保系统有效、可审计、透明且符合人类意图。

Abstract: Large neural models are increasingly deployed in high-stakes settings,
raising concerns about whether their behavior reliably aligns with human
values. Interpretability provides a route to internal transparency by revealing
the computations that drive outputs. We argue that interpretability especially
mechanistic approaches should be treated as a design principle for alignment,
not an auxiliary diagnostic tool. Post-hoc methods such as LIME or SHAP offer
intuitive but correlational explanations, while mechanistic techniques like
circuit tracing or activation patching yield causal insight into internal
failures, including deceptive or misaligned reasoning that behavioral methods
like RLHF, red teaming, or Constitutional AI may overlook. Despite these
advantages, interpretability faces challenges of scalability, epistemic
uncertainty, and mismatches between learned representations and human concepts.
Our position is that progress on safe and trustworthy AI will depend on making
interpretability a first-class objective of AI research and development,
ensuring that systems are not only effective but also auditable, transparent,
and aligned with human intent.

</details>


### [81] [Classification of 24-hour movement behaviors from wrist-worn accelerometer data: from handcrafted features to deep learning techniques](https://arxiv.org/abs/2509.08606)
*Alireza Sameh,Mehrdad Rostami,Mourad Oussalah,Vahid Farrahi*

Main category: cs.LG

TL;DR: 本文比较深度学习和经典机器学习算法对24小时运动行为分类的性能，发现使用原始加速度信号的深度学习方法稍优。


<details>
  <summary>Details</summary>
Motivation: 比较深度学习和经典机器学习算法对24小时运动行为（睡眠、久坐、轻度和中高强度体力活动）分类的性能。

Method: 使用151名成年人的腕式加速度计数据，随机划分训练、验证和测试集，提取104个手工特征。用原始加速度信号和手工特征训练四种深度学习算法，用手工特征训练六种经典机器学习算法。

Result: 使用原始加速度信号训练时，LSTM、BiLSTM和GRU总体准确率约85%，1D - CNN约80%；使用手工特征训练时，两类算法总体准确率在70% - 81%。中高强度和轻度体力活动分类混淆度较高。

Conclusion: 使用原始加速度信号的深度学习方法在预测24小时运动行为强度上，仅比使用手工特征训练的深度学习和经典机器学习方法性能略好。

Abstract: Purpose: We compared the performance of deep learning (DL) and classical
machine learning (ML) algorithms for the classification of 24-hour movement
behavior into sleep, sedentary, light intensity physical activity (LPA), and
moderate-to-vigorous intensity physical activity (MVPA). Methods: Open-access
data from 151 adults wearing a wrist-worn accelerometer (Axivity-AX3) was used.
Participants were randomly divided into training, validation, and test sets
(121, 15, and 15 participants each). Raw acceleration signals were segmented
into non-overlapping 10-second windows, and then a total of 104 handcrafted
features were extracted. Four DL algorithms-Long Short-Term Memory (LSTM),
Bidirectional Long Short-Term Memory (BiLSTM), Gated Recurrent Units (GRU), and
One-Dimensional Convolutional Neural Network (1D-CNN)-were trained using raw
acceleration signals and with handcrafted features extracted from these signals
to predict 24-hour movement behavior categories. The handcrafted features were
also used to train classical ML algorithms, namely Random Forest (RF), Support
Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), Logistic Regression
(LR), Artificial Neural Network (ANN), and Decision Tree (DT) for classifying
24-hour movement behavior intensities. Results: LSTM, BiLSTM, and GRU showed an
overall accuracy of approximately 85% when trained with raw acceleration
signals, and 1D-CNN an overall accuracy of approximately 80%. When trained on
handcrafted features, the overall accuracy for both DL and classical ML
algorithms ranged from 70% to 81%. Overall, there was a higher confusion in
classification of MVPA and LPA, compared to sleep and sedentary categories.
Conclusion: DL methods with raw acceleration signals had only slightly better
performance in predicting 24-hour movement behavior intensities, compared to
when DL and classical ML were trained with handcrafted features.

</details>


### [82] [Towards Interpretable Deep Neural Networks for Tabular Data](https://arxiv.org/abs/2509.08617)
*Khawla Elhadri,Jörg Schlötterer,Christin Seifert*

Main category: cs.LG

TL;DR: 介绍XNNTab神经架构，用SAE学习特征，能解释预测，性能与现有方法相当或更优。


<details>
  <summary>Details</summary>
Motivation: 现有针对表格数据的DNN可预测性强但缺乏可解释性。

Method: 引入XNNTab架构，用SAE学习潜在空间中的单语义特征字典，并为特征赋予人类可解释的语义。

Result: XNNTab在性能上与最先进的黑盒神经模型和经典机器学习方法相当或更优。

Conclusion: XNNTab兼具可解释性与良好的预测性能。

Abstract: Tabular data is the foundation of many applications in fields such as finance
and healthcare. Although DNNs tailored for tabular data achieve competitive
predictive performance, they are blackboxes with little interpretability. We
introduce XNNTab, a neural architecture that uses a sparse autoencoder (SAE) to
learn a dictionary of monosemantic features within the latent space used for
prediction. Using an automated method, we assign human-interpretable semantics
to these features. This allows us to represent predictions as linear
combinations of semantically meaningful components. Empirical evaluations
demonstrate that XNNTab attains performance on par with or exceeding that of
state-of-the-art, black-box neural models and classical machine learning
approaches while being fully interpretable.

</details>


### [83] [An upper bound of the silhouette validation metric for clustering](https://arxiv.org/abs/2509.08625)
*Hugo Sträng,Tai Dinh*

Main category: cs.LG

TL;DR: 论文推导了给定数据集中每个数据点轮廓宽度的严格上界，提出了数据相关的平均轮廓宽度（ASW）上界，可用于聚类质量评估。


<details>
  <summary>Details</summary>
Motivation: 现有ASW的标准上限1通常无法达到，且数据集特定的ASW最大值未知，需找到更合适的评估指标。

Method: 推导每个数据点轮廓宽度的上界，并聚合这些个体边界得到数据相关的ASW上界。

Result: 所提出的边界在许多情况下被证明接近紧密，可指示数据点放置情况、实现基于轮廓的优化循环的提前停止。

Conclusion: 提出的边界显著丰富了聚类质量评估。

Abstract: The silhouette coefficient summarizes, per observation, cohesion versus
separation in [-1, 1]; the average silhouette width (ASW) is a common internal
measure of clustering quality where higher values indicate more coveted
results. However, the dataset-specific maximum of ASW is typically unknown, and
the standard upper limit 1 is often unattainable. In this work, we derive for
each data point in a given dataset a sharp upper bound on its silhouette width.
By aggregating these individual bounds, we present a canonical data-dependent
upper bound on ASW that often assumes values well below 1. The presented bounds
can indicate whether individual data points can ever be well placed, enable
early stopping of silhouette-based optimization loops, and help answer a key
question: How close is my clustering result to the best possible outcome on
this specific data? Across synthetic and real datasets, the bounds are provably
near-tight in many cases and offer significant enrichment of cluster quality
evaluation.

</details>


### [84] [Generative Data Refinement: Just Ask for Better Data](https://arxiv.org/abs/2509.08653)
*Minqi Jiang,João G. M. Araújo,Will Ellsworth,Sian Gooding,Edward Grefenstette*

Main category: cs.LG

TL;DR: 当前训练数据增长快或致未来十年数据耗尽，提出GDR框架处理含不良内容数据集，实验证明其有效且能扩大训练数据量。


<details>
  <summary>Details</summary>
Motivation: 训练数据增长快可能导致未来数据耗尽，且使用未公开索引的用户生成内容有风险。

Method: 引入Generative Data Refinement (GDR)框架，利用预训练生成模型将含不良内容的数据集转化为更适合训练的精炼数据集。

Result: GDR在数据集匿名化方面优于行业级解决方案，能对高风险数据集直接解毒，精炼输出可匹配网络规模数据集的多样性。

Conclusion: GDR简单有效，是扩大前沿模型训练数据总量的有力工具。

Abstract: For a fixed parameter size, the capabilities of large models are primarily
determined by the quality and quantity of its training data. Consequently,
training datasets now grow faster than the rate at which new data is indexed on
the web, leading to projected data exhaustion over the next decade. Much more
data exists as user-generated content that is not publicly indexed, but
incorporating such data comes with considerable risks, such as leaking private
information and other undesirable content. We introduce a framework, Generative
Data Refinement (GDR), for using pretrained generative models to transform a
dataset with undesirable content into a refined dataset that is more suitable
for training. Our experiments show that GDR can outperform industry-grade
solutions for dataset anonymization, as well as enable direct detoxification of
highly unsafe datasets. Moreover, we show that by generating synthetic data
that is conditioned on each example in the real dataset, GDR's refined outputs
naturally match the diversity of web scale datasets, and thereby avoid the
often challenging task of generating diverse synthetic data via model
prompting. The simplicity and effectiveness of GDR make it a powerful tool for
scaling up the total stock of training data for frontier models.

</details>


### [85] [Replicable Reinforcement Learning with Linear Function Approximation](https://arxiv.org/abs/2509.08660)
*Eric Eaton,Marcel Hussing,Michael Kearns,Aaron Roth,Sikata Bela Sengupta,Jessica Sorrell*

Main category: cs.LG

TL;DR: 本文为强化学习中线性函数逼近开发了可复现方法，介绍相关算法并实验评估，展示其对神经策略的启发。


<details>
  <summary>Details</summary>
Motivation: 实验结果的可复现性是机器学习等学科面临的挑战，强化学习算法不稳定，将可复现性理论扩展到更实际的函数逼近设置是未解决的问题。

Method: 先引入可复现随机设计回归和无中心协方差估计的两种高效算法，再利用这些工具为线性马尔可夫决策过程提供可证明高效的可复现强化学习算法。

Result: 开发出适用于强化学习线性函数逼近的可复现方法，实验评估了算法，展示其能启发更一致的神经策略。

Conclusion: 在将可复现性保证扩展到强化学习实际函数逼近设置方面取得进展。

Abstract: Replication of experimental results has been a challenge faced by many
scientific disciplines, including the field of machine learning. Recent work on
the theory of machine learning has formalized replicability as the demand that
an algorithm produce identical outcomes when executed twice on different
samples from the same distribution. Provably replicable algorithms are
especially interesting for reinforcement learning (RL), where algorithms are
known to be unstable in practice. While replicable algorithms exist for tabular
RL settings, extending these guarantees to more practical function
approximation settings has remained an open problem. In this work, we make
progress by developing replicable methods for linear function approximation in
RL. We first introduce two efficient algorithms for replicable random design
regression and uncentered covariance estimation, each of independent interest.
We then leverage these tools to provide the first provably efficient replicable
RL algorithms for linear Markov decision processes in both the generative model
and episodic settings. Finally, we evaluate our algorithms experimentally and
show how they can inspire more consistent neural policies.

</details>


### [86] [Signal Fidelity Index-Aware Calibration for Dementia Predictions Across Heterogeneous Real-World Data](https://arxiv.org/abs/2509.08679)
*Jingya Cheng,Jiazi Tian,Federica Spoto,Alaleh Azhir,Daniel Mork,Hossein Estiri*

Main category: cs.LG

TL;DR: 本文提出信号保真度指数（SFI）量化痴呆诊断数据质量，并通过SFI感知校准提升模型跨数据集性能，结果显示校准显著提升各项指标。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在电子健康记录上训练时因诊断信号衰减导致的跨医疗系统性能下降问题。

Method: 构建模拟框架生成2500个合成数据集，从六个可解释组件推导SFI，SFI感知校准应用乘法调整并在50个模拟批次中优化。

Result: 在最优参数下，SFI感知校准显著改善所有指标，各项指标均有明显提升。

Conclusion: 诊断信号衰减是模型泛化的可解决障碍，SFI感知校准为跨医疗环境预测提供实用无标签策略。

Abstract: \textbf{Background:} Machine learning models trained on electronic health
records (EHRs) often degrade across healthcare systems due to distributional
shift. A fundamental but underexplored factor is diagnostic signal decay:
variability in diagnostic quality and consistency across institutions, which
affects the reliability of codes used for training and prediction.
  \textbf{Objective:} To develop a Signal Fidelity Index (SFI) quantifying
diagnostic data quality at the patient level in dementia, and to test SFI-aware
calibration for improving model performance across heterogeneous datasets
without outcome labels.
  \textbf{Methods:} We built a simulation framework generating 2,500 synthetic
datasets, each with 1,000 patients and realistic demographics, encounters, and
coding patterns based on dementia risk factors. The SFI was derived from six
interpretable components: diagnostic specificity, temporal consistency,
entropy, contextual concordance, medication alignment, and trajectory
stability. SFI-aware calibration applied a multiplicative adjustment, optimized
across 50 simulation batches.
  \textbf{Results:} At the optimal parameter ($\alpha$ = 2.0), SFI-aware
calibration significantly improved all metrics (p $<$ 0.001). Gains ranged from
10.3\% for Balanced Accuracy to 32.5\% for Recall, with notable increases in
Precision (31.9\%) and F1-score (26.1\%). Performance approached reference
standards, with F1-score and Recall within 1\% and Balanced Accuracy and
Detection Rate improved by 52.3\% and 41.1\%, respectively.
  \textbf{Conclusions:} Diagnostic signal decay is a tractable barrier to model
generalization. SFI-aware calibration provides a practical, label-free strategy
to enhance prediction across healthcare contexts, particularly for large-scale
administrative datasets lacking outcome labels.

</details>


### [87] [Perfectly-Private Analog Secure Aggregation in Federated Learning](https://arxiv.org/abs/2509.08683)
*Delio Jaramillo-Velez,Charul Rajput,Ragnar Freij-Hollanti,Camilla Hollanti,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: 本文提出一种基于环面的安全参数聚合方法，保证数据隐私且避免精度损失，实验显示该方法表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中通过本地模型暴露敏感数据的风险，以及有限域方法存在的精度复杂度权衡问题。

Method: 提出采用环面而非有限域的新型安全参数聚合方法，利用环面上的均匀分布保证各方数据的完美隐私。

Result: 新协议在保持完美隐私的同时，表现与无安全聚合的模型相似，在某些情况下，基于环面的协议在模型精度和余弦相似度方面显著优于有限域安全聚合。

Conclusion: 基于环面的协议是更安全的选择。

Abstract: In federated learning, multiple parties train models locally and share their
parameters with a central server, which aggregates them to update a global
model. To address the risk of exposing sensitive data through local models,
secure aggregation via secure multiparty computation has been proposed to
enhance privacy. At the same time, perfect privacy can only be achieved by a
uniform distribution of the masked local models to be aggregated. This raises a
problem when working with real valued data, as there is no measure on the reals
that is invariant under the masking operation, and hence information leakage is
bound to occur. Shifting the data to a finite field circumvents this problem,
but as a downside runs into an inherent accuracy complexity tradeoff issue due
to fixed point modular arithmetic as opposed to floating point numbers that can
simultaneously handle numbers of varying magnitudes. In this paper, a novel
secure parameter aggregation method is proposed that employs the torus rather
than a finite field. This approach guarantees perfect privacy for each party's
data by utilizing the uniform distribution on the torus, while avoiding
accuracy losses. Experimental results show that the new protocol performs
similarly to the model without secure aggregation while maintaining perfect
privacy. Compared to the finite field secure aggregation, the torus-based
protocol can in some cases significantly outperform it in terms of model
accuracy and cosine similarity, hence making it a safer choice.

</details>


### [88] [Reshaping the Forward-Forward Algorithm with a Similarity-Based Objective](https://arxiv.org/abs/2509.08697)
*James Gong,Raymond Luo,Emma Wang,Leon Ge,Bruce Li,Felix Marattukalam,Waleed Abdulla*

Main category: cs.LG

TL;DR: 本文针对前向-前向算法精度低、推理效率低的问题，提出FAUST算法，在多个数据集上提升了精度。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法有生物学不合理等局限，前向-前向算法虽有优势但精度低、推理效率低，需改进。

Method: 将前向-前向算法与相似性学习框架整合，提出FAUST算法，消除推理时多次前向传播的需求。

Result: 在MNIST、Fashion - MNIST和CIFAR - 10数据集上显著提高精度，在CIFAR - 10上用简单多层感知器架构达到56.22%的准确率，接近反向传播的57.63%。

Conclusion: FAUST算法有效提升了前向-前向算法的精度，缩小了与反向传播算法的差距。

Abstract: Backpropagation is the pivotal algorithm underpinning the success of
artificial neural networks, yet it has critical limitations such as
biologically implausible backward locking and global error propagation. To
circumvent these constraints, the Forward-Forward algorithm was proposed as a
more biologically plausible method that replaces the backward pass with an
additional forward pass. Despite this advantage, the Forward-Forward algorithm
significantly trails backpropagation in accuracy, and its optimal form exhibits
low inference efficiency due to multiple forward passes required. In this work,
the Forward-Forward algorithm is reshaped through its integration with
similarity learning frameworks, eliminating the need for multiple forward
passes during inference. This proposed algorithm is named Forward-Forward
Algorithm Unified with Similarity-based Tuplet loss (FAUST). Empirical
evaluations on MNIST, Fashion-MNIST, and CIFAR-10 datasets indicate that FAUST
substantially improves accuracy, narrowing the gap with backpropagation. On
CIFAR-10, FAUST achieves 56.22\% accuracy with a simple multi-layer perceptron
architecture, approaching the backpropagation benchmark of 57.63\% accuracy.

</details>


### [89] [A layered architecture for log analysis in complex IT systems](https://arxiv.org/abs/2509.08698)
*Thorsten Wittkopp*

Main category: cs.LG

TL;DR: 论文介绍三层架构以支持DevOps故障解决，含日志调查、异常检测和根因分析，评估效果良好，可提升系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 在IT系统复杂性增加背景下，帮助DevOps团队通过日志分析提升系统稳定性和可靠性。

Method: 提出三层架构，包括自动日志标注与异常分类的日志调查层、灵活的异常检测层、平衡训练数据和识别关键服务的根因分析层。

Result: 异常检测F1分数0.98 - 1.0，根因分析能在前十候选中检测90 - 98%的根因日志行。

Conclusion: 集成三层架构可提供强大方法，提升IT系统可靠性。

Abstract: In the evolving IT landscape, stability and reliability of systems are
essential, yet their growing complexity challenges DevOps teams in
implementation and maintenance. Log analysis, a core element of AIOps, provides
critical insights into complex behaviors and failures. This dissertation
introduces a three-layered architecture to support DevOps in failure
resolution. The first layer, Log Investigation, performs autonomous log
labeling and anomaly classification. We propose a method that labels log data
without manual effort, enabling supervised training and precise evaluation of
anomaly detection. Additionally, we define a taxonomy that groups anomalies
into three categories, ensuring appropriate method selection. The second layer,
Anomaly Detection, detects behaviors deviating from the norm. We propose a
flexible Anomaly Detection method adaptable to unsupervised, weakly supervised,
and supervised training. Evaluations on public and industry datasets show
F1-scores between 0.98 and 1.0, ensuring reliable anomaly detection. The third
layer, Root Cause Analysis, identifies minimal log sets describing failures,
their origin, and event sequences. By balancing training data and identifying
key services, our Root Cause Analysis method consistently detects 90-98% of
root cause log lines within the top 10 candidates, providing actionable
insights for mitigation. Our research addresses how log analysis methods can be
designed and optimized to help DevOps resolve failures efficiently. By
integrating these three layers, the architecture equips teams with robust
methods to enhance IT system reliability.

</details>


### [90] [Machine Learning-Based Prediction of Speech Arrest During Direct Cortical Stimulation Mapping](https://arxiv.org/abs/2509.08703)
*Nikasadat Emami,Amirhossein Khalilian-Gourtani,Jianghao Qian,Antoine Ratouchniak,Xupeng Chen,Yao Wang,Adeen Flinker*

Main category: cs.LG

TL;DR: 本文分析颅内脑电数据，开发机器学习模型预测脑区是否对言语关键，结合区域和连接特征的模型表现良好，最佳模型在验证集上准确率高。


<details>
  <summary>Details</summary>
Motivation: 电刺激映射（ESM）用于识别言语关键脑区时具有侵入性和耗时的缺点，需要更好的方法用于安全脑外科手术。

Method: 分析16名参与者执行言语任务的颅内脑电数据，利用ESM的真实标签训练分类模型，框架整合神经活动信号、解剖区域标签和功能连接特征，用MLP聚合试验级预测。

Result: 结合区域和连接特征的模型性能与全特征集相当，优于单一特征模型，最佳模型在验证集上ROC - AUC为0.87，PR - AUC为0.57。

Conclusion: 结合空间和网络信息以及非线性建模有助于改善术前功能映射。

Abstract: Identifying cortical regions critical for speech is essential for safe brain
surgery in or near language areas. While Electrical Stimulation Mapping (ESM)
remains the clinical gold standard, it is invasive and time-consuming. To
address this, we analyzed intracranial electrocorticographic (ECoG) data from
16 participants performing speech tasks and developed machine learning models
to directly predict if the brain region underneath each ECoG electrode is
critical. Ground truth labels indicating speech arrest were derived
independently from Electrical Stimulation Mapping (ESM) and used to train
classification models. Our framework integrates neural activity signals,
anatomical region labels, and functional connectivity features to capture both
local activity and network-level dynamics. We found that models combining
region and connectivity features matched the performance of the full feature
set, and outperformed models using either type alone. To classify each
electrode, trial-level predictions were aggregated using an MLP applied to
histogram-encoded scores. Our best-performing model, a trial-level RBF-kernel
Support Vector Machine together with MLP-based aggregation, achieved strong
accuracy on held-out participants (ROC-AUC: 0.87, PR-AUC: 0.57). These findings
highlight the value of combining spatial and network information with
non-linear modeling to improve functional mapping in presurgical evaluation.

</details>


### [91] [Securing Private Federated Learning in a Malicious Setting: A Scalable TEE-Based Approach with Client Auditing](https://arxiv.org/abs/2509.08709)
*Shun Takagi,Satoshi Hasegawa*

Main category: cs.LG

TL;DR: 论文提出服务器扩展实现恶意安全的DP - FTRL，减少可信计算基大小，保证隐私且开销小。


<details>
  <summary>Details</summary>
Motivation: 现有跨设备私有联邦学习的DP - FTRL方法假设半诚实服务器，未解决去除该假设的安全挑战，直接用可信执行环境有问题。

Method: 引入作为可信计算基的服务器扩展，用临时可信执行环境模块生成可验证证明，部分客户端审计证明。

Result: 提供基于交互式差分隐私的形式化证明，实验表明框架在多种现实场景下给客户端增加的开销小。

Conclusion: 该扩展方案在减少可信计算基大小的同时，维持了系统的可扩展性和活性，能在恶意环境下保证隐私。

Abstract: In cross-device private federated learning, differentially private
follow-the-regularized-leader (DP-FTRL) has emerged as a promising
privacy-preserving method. However, existing approaches assume a semi-honest
server and have not addressed the challenge of securely removing this
assumption. This is due to its statefulness, which becomes particularly
problematic in practical settings where clients can drop out or be corrupted.
While trusted execution environments (TEEs) might seem like an obvious
solution, a straightforward implementation can introduce forking attacks or
availability issues due to state management. To address this problem, our paper
introduces a novel server extension that acts as a trusted computing base (TCB)
to realize maliciously secure DP-FTRL. The TCB is implemented with an ephemeral
TEE module on the server side to produce verifiable proofs of server actions.
Some clients, upon being selected, participate in auditing these proofs with
small additional communication and computational demands. This extension
solution reduces the size of the TCB while maintaining the system's scalability
and liveness. We provide formal proofs based on interactive differential
privacy, demonstrating privacy guarantee in malicious settings. Finally, we
experimentally show that our framework adds small constant overhead to clients
in several realistic settings.

</details>


### [92] [Compressing CNN models for resource-constrained systems by channel and layer pruning](https://arxiv.org/abs/2509.08714)
*Ahmed Sadaqa,Di Liu*

Main category: cs.LG

TL;DR: 本文提出结合通道和层剪枝的混合剪枝框架，实验表明可降低模型复杂度，在NVIDIA JETSON TX2上减少延迟且精度损失小。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络复杂度和规模增加，在边缘设备部署有挑战，模型压缩成为研究方向，提出新的剪枝技术。

Method: 提出结合通道和层剪枝的混合剪枝框架，受EfficientNet启发，反向运用其缩放原理进行剪枝。

Result: 实验显示混合剪枝方法显著降低模型整体复杂度，与基线模型相比精度仅略有下降，在NVIDIA JETSON TX2嵌入式AI设备上部署剪枝模型可减少延迟。

Conclusion: 混合剪枝框架能有效降低CNN模型复杂度，且在边缘设备上减少延迟，同时维持一定精度。

Abstract: Convolutional Neural Networks (CNNs) have achieved significant breakthroughs
in various fields. However, these advancements have led to a substantial
increase in the complexity and size of these networks. This poses a challenge
when deploying large and complex networks on edge devices. Consequently, model
compression has emerged as a research field aimed at reducing the size and
complexity of CNNs. One prominent technique in model compression is model
pruning. This paper will present a new technique of pruning that combines both
channel and layer pruning in what is called a "hybrid pruning framework".
Inspired by EfficientNet, a renowned CNN architecture known for scaling up
networks from both channel and layer perspectives, this hybrid approach applies
the same principles but in reverse, where it scales down the network through
pruning. Experiments on the hybrid approach demonstrated a notable decrease in
the overall complexity of the model, with only a minimal reduction in accuracy
compared to the baseline model. This complexity reduction translates into
reduced latency when deploying the pruned models on an NVIDIA JETSON TX2
embedded AI device.

</details>


### [93] [Sharing is Caring: Efficient LM Post-Training with Collective RL Experience Sharing](https://arxiv.org/abs/2509.08721)
*Jeffrey Amico,Gabriel Passamani Andrade,John Donaghy,Ben Fielding,Tristin Forbus,Harry Grieve,Semih Kara,Jari Kolehmainen,Yihua Lou,Christopher Nies,Edward Phillip Flores Nuño,Diogo Ortega,Shikhar Rastogi,Austin Virts,Matthew J. Wright*

Main category: cs.LG

TL;DR: 提出完全去中心化和异步的RL后训练算法SAPO，避免扩展瓶颈，实验中实现高达94%的累积奖励增益。


<details>
  <summary>Details</summary>
Motivation: 有效利用RL对语言模型进行后训练需大量并行化，会带来技术挑战和高昂成本。

Method: 提出SAPO算法，适用于异构计算节点的去中心化网络，各节点管理自身策略模型并共享推演数据。

Result: 在控制实验中实现高达94%的累积奖励增益，在开源演示中获得使用不同硬件和模型的数千节点网络测试见解。

Conclusion: SAPO算法避免了扩展RL后训练的常见瓶颈，有新的可能性并能促进学习。

Abstract: Post-training language models (LMs) with reinforcement learning (RL) can
enhance their complex reasoning capabilities without supervised fine-tuning, as
demonstrated by DeepSeek-R1-Zero. However, effectively utilizing RL for LMs
requires significant parallelization to scale-up inference, which introduces
non-trivial technical challenges (e.g. latency, memory, and reliability)
alongside ever-growing financial costs. We present Swarm sAmpling Policy
Optimization (SAPO), a fully decentralized and asynchronous RL post-training
algorithm. SAPO is designed for decentralized networks of heterogenous compute
nodes, where each node manages its own policy model(s) while "sharing" rollouts
with others in the network; no explicit assumptions about latency, model
homogeneity, or hardware are required and nodes can operate in silo if desired.
As a result, the algorithm avoids common bottlenecks in scaling RL
post-training while also allowing (and even encouraging) new possibilities. By
sampling rollouts "shared" across the network, it enables "Aha moments" to
propagate, thereby bootstrapping the learning process. In this paper we show
SAPO achieved cumulative reward gains of up to 94% in controlled experiments.
We also share insights from tests on a network with thousands of nodes
contributed by Gensyn community members running the algorithm on diverse
hardware and models during an open-source demo.

</details>


### [94] [DEQuify your force field: More efficient simulations using deep equilibrium models](https://arxiv.org/abs/2509.08734)
*Andreas Burger,Luca Thiede,Alán Aspuru-Guzik,Nandita Vijaykumar*

Main category: cs.LG

TL;DR: 本文提出利用分子系统模拟的连续性信息，将最先进的等变基础模型重构为深度平衡模型，在多个数据集上提高了准确性和速度，训练也更节省内存。


<details>
  <summary>Details</summary>
Motivation: 机器学习力场有潜力实现更精确的分子动力学模拟，此前进展多利用物理系统对称性等先验知识，而分子系统模拟的连续性这一重要先验信息尚未被探索。

Method: 将最先进的等变基础模型重构为深度平衡模型，回收上一时间步的中间神经网络特征。

Result: 在MD17、MD22和OC20 200k数据集上，相比非DEQ基础模型，准确性和速度提高10%-20%，训练更节省内存。

Conclusion: 利用分子系统模拟的连续性信息，通过重构模型能有效提升分子动力学模拟的性能。

Abstract: Machine learning force fields show great promise in enabling more accurate
molecular dynamics simulations compared to manually derived ones. Much of the
progress in recent years was driven by exploiting prior knowledge about
physical systems, in particular symmetries under rotation, translation, and
reflections. In this paper, we argue that there is another important piece of
prior information that, thus fa,r hasn't been explored: Simulating a molecular
system is necessarily continuous, and successive states are therefore extremely
similar. Our contribution is to show that we can exploit this information by
recasting a state-of-the-art equivariant base model as a deep equilibrium
model. This allows us to recycle intermediate neural network features from
previous time steps, enabling us to improve both accuracy and speed by
$10\%-20\%$ on the MD17, MD22, and OC20 200k datasets, compared to the non-DEQ
base model. The training is also much more memory efficient, allowing us to
train more expressive models on larger systems.

</details>


### [95] [ChemBOMAS: Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent System](https://arxiv.org/abs/2509.08736)
*Dong Han,Zhehong Ai,Pengxiang Cai,Shuzhou Sun,Shanya Lu,Jianpeng Chen,Ben Gao,Lingli Ge,Weida Wang,Xiangxin Zhou,Xihui Liu,Mao Su,Wanli Ouyang,Lei Bai,Dongzhan Zhou,Tao XU,Yuqiang Li,Shufei Zhang*

Main category: cs.LG

TL;DR: 提出ChemBOMAS框架加速化学领域的贝叶斯优化，经评估和实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 解决化学中贝叶斯优化受稀疏实验数据和复杂反应机制限制的问题。

Method: 采用LLM增强的多智能体系统，结合知识驱动的粗粒度优化和数据驱动的细粒度优化策略。

Result: 基准评估显示比多种贝叶斯优化算法更有效率；湿实验中目标值达96%，远高于专家的15%。

Conclusion: ChemBOMAS是加速化学发现的有力工具。

Abstract: The efficiency of Bayesian optimization (BO) in chemistry is often hindered
by sparse experimental data and complex reaction mechanisms. To overcome these
limitations, we introduce ChemBOMAS, a new framework named LLM-Enhanced
Multi-Agent System for accelerating BO in chemistry. ChemBOMAS's optimization
process is enhanced by LLMs and synergistically employs two strategies:
knowledge-driven coarse-grained optimization and data-driven fine-grained
optimization. First, in the knowledge-driven coarse-grained optimization stage,
LLMs intelligently decompose the vast search space by reasoning over existing
chemical knowledge to identify promising candidate regions. Subsequently, in
the data-driven fine-grained optimization stage, LLMs enhance the BO process
within these candidate regions by generating pseudo-data points, thereby
improving data utilization efficiency and accelerating convergence. Benchmark
evaluations** further confirm that ChemBOMAS significantly enhances
optimization effectiveness and efficiency compared to various BO algorithms.
Importantly, the practical utility of ChemBOMAS was validated through wet-lab
experiments conducted under pharmaceutical industry protocols, targeting
conditional optimization for a previously unreported and challenging chemical
reaction. In the wet experiment, ChemBOMAS achieved an optimal objective value
of 96%. This was substantially higher than the 15% achieved by domain experts.
This real-world success, together with strong performance on benchmark
evaluations, highlights ChemBOMAS as a powerful tool to accelerate chemical
discovery.

</details>


### [96] [AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2509.08755)
*Zhiheng Xi,Jixuan Huang,Chenyang Liao,Baodai Huang,Honglin Guo,Jiaqi Liu,Rui Zheng,Junjie Ye,Jiazheng Zhang,Wenxiang Chen,Wei He,Yiwen Ding,Guanyu Li,Zehui Chen,Zhengyin Du,Xuesong Yao,Yufei Xu,Jiecao Chen,Tao Gui,Zuxuan Wu,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: 提出AgentGym - RL框架和ScalingInter - RL训练方法训练LLM智能体，实验验证其有效性，将开源框架。


<details>
  <summary>Details</summary>
Motivation: 社区缺乏统一的交互式强化学习框架从无到有训练能解决复杂现实任务的LLM智能体。

Method: 引入AgentGym - RL框架用于多轮交互决策，采用模块化解耦架构；提出ScalingInter - RL训练方法平衡探索与利用。

Result: 智能体在27个跨不同环境的任务中达到或超越商业模型。

Conclusion: AgentGym - RL框架和ScalingInter - RL方法稳定有效，将开源框架助力智能体研究。

Abstract: Developing autonomous LLM agents capable of making a series of intelligent
decisions to solve complex, real-world tasks is a fast-evolving frontier. Like
human cognitive development, agents are expected to acquire knowledge and
skills through exploration and interaction with the environment. Despite
advances, the community still lacks a unified, interactive reinforcement
learning (RL) framework that can effectively train such agents from scratch --
without relying on supervised fine-tuning (SFT) -- across diverse and realistic
environments. To bridge this gap, we introduce AgentGym-RL, a new framework to
train LLM agents for multi-turn interactive decision-making through RL. The
framework features a modular and decoupled architecture, ensuring high
flexibility and extensibility. It encompasses a wide variety of real-world
scenarios, and supports mainstream RL algorithms. Furthermore, we propose
ScalingInter-RL, a training approach designed for exploration-exploitation
balance and stable RL optimization. In early stages, it emphasizes exploitation
by restricting the number of interactions, and gradually shifts towards
exploration with larger horizons to encourage diverse problem-solving
strategies. In this way, the agent develops more diverse behaviors and is less
prone to collapse under long horizons. We perform extensive experiments to
validate the stability and effectiveness of both the AgentGym-RL framework and
the ScalingInter-RL approach. Our agents match or surpass commercial models on
27 tasks across diverse environments. We offer key insights and will
open-source the complete AgentGym-RL framework -- including code and datasets
-- to empower the research community in developing the next generation of
intelligent agents.

</details>


### [97] [Using AI to Optimize Patient Transfer and Resource Utilization During Mass-Casualty Incidents: A Simulation Platform](https://arxiv.org/abs/2509.08756)
*Zhaoxun "Lorenz" Liu,Wagner H. Souza,Jay Han,Amin Madani*

Main category: cs.LG

TL;DR: 开发基于深度强化学习的AI决策支持代理及Web仪表盘，研究不同交互方式在模拟大规模伤亡事件中的效果，发现增加AI参与可提升决策质量。


<details>
  <summary>Details</summary>
Motivation: 大规模伤亡事件对医疗系统造成压力，需要快速准确的患者-医院分配决策。

Method: 开发AI决策支持代理和Web仪表盘MasTER，进行30人参与的用户研究，评估三种交互方式在不同规模事件场景中的效果。

Result: 增加AI参与显著提高决策质量和一致性，AI代理表现优于创伤外科医生，能让非专家达到专家水平。

Conclusion: AI驱动的决策支持有潜力提升大规模伤亡事件的应急准备训练和实际应急响应管理。

Abstract: Mass casualty incidents (MCIs) overwhelm healthcare systems and demand rapid,
accurate patient-hospital allocation decisions under extreme pressure. Here, we
developed and validated a deep reinforcement learning-based decision-support AI
agent to optimize patient transfer decisions during simulated MCIs by balancing
patient acuity levels, specialized care requirements, hospital capacities, and
transport logistics. To integrate this AI agent, we developed MasTER, a
web-accessible command dashboard for MCI management simulations. Through a
controlled user study with 30 participants (6 trauma experts and 24
non-experts), we evaluated three interaction approaches with the AI agent
(human-only, human-AI collaboration, and AI-only) across 20- and 60-patient MCI
scenarios in the Greater Toronto Area. Results demonstrate that increasing AI
involvement significantly improves decision quality and consistency. The AI
agent outperforms trauma surgeons (p < 0.001) and enables non-experts to
achieve expert-level performance when assisted, contrasting sharply with their
significantly inferior unassisted performance (p < 0.001). These findings
establish the potential for our AI-driven decision support to enhance both MCI
preparedness training and real-world emergency response management.

</details>


### [98] [Fourier Learning Machines: Nonharmonic Fourier-Based Neural Networks for Scientific Machine Learning](https://arxiv.org/abs/2509.08759)
*Mominul Rubel,Adam Meyers,Gabriel Nicolosi*

Main category: cs.LG

TL;DR: 介绍傅里叶学习机（FLM），一种可表示多维非调和傅里叶级数的神经网络架构，评估其在科学计算问题上的性能，表现良好。


<details>
  <summary>Details</summary>
Motivation: 设计一种能表示多维非调和傅里叶级数、适用于周期和非周期函数、可创建特定问题光谱基的神经网络架构。

Method: 采用带余弦激活函数的简单前馈结构，将级数的频率、幅度和相移作为可训练参数学习。

Result: 在多个科学计算问题上进行评估，性能与SIREN和普通前馈神经网络相当，且常更优。

Conclusion: FLM是首个能用标准多层感知器架构表示多维完整可分离傅里叶基的架构，有良好应用效果。

Abstract: We introduce the Fourier Learning Machine (FLM), a neural network (NN)
architecture designed to represent a multidimensional nonharmonic Fourier
series. The FLM uses a simple feedforward structure with cosine activation
functions to learn the frequencies, amplitudes, and phase shifts of the series
as trainable parameters. This design allows the model to create a
problem-specific spectral basis adaptable to both periodic and nonperiodic
functions. Unlike previous Fourier-inspired NN models, the FLM is the first
architecture able to represent a complete, separable Fourier basis in multiple
dimensions using a standard Multilayer Perceptron-like architecture. A
one-to-one correspondence between the Fourier coefficients and amplitudes and
phase-shifts is demonstrated, allowing for the translation between a full,
separable basis form and the cosine phase--shifted one. Additionally, we
evaluate the performance of FLMs on several scientific computing problems,
including benchmark Partial Differential Equations (PDEs) and a family of
Optimal Control Problems (OCPs). Computational experiments show that the
performance of FLMs is comparable, and often superior, to that of established
architectures like SIREN and vanilla feedforward NNs.

</details>


### [99] [ADHDeepNet From Raw EEG to Diagnosis: Improving ADHD Diagnosis through Temporal-Spatial Processing, Adaptive Attention Mechanisms, and Explainability in Raw EEG Signals](https://arxiv.org/abs/2509.08779)
*Ali Amini,Mohammad Alijanpour,Behnam Latifi,Ali Motie Nasrabadi*

Main category: cs.LG

TL;DR: 本文提出ADHDeepNet模型，结合深度学习与EEG信号提高ADHD诊断的精准度和及时性，模型取得高灵敏度和准确率，凸显DL和EEG在ADHD诊断中的潜力。


<details>
  <summary>Details</summary>
Motivation: ADHD早期诊断劳动强度大、耗时长，需要提高诊断的精准度和及时性。

Method: 引入ADHDeepNet模型，采用嵌套交叉验证，两阶段方法，包括超参数优化和数据增强，分析模型权重和激活模式，用t - SNE可视化数据。

Result: ADHDeepNet在分类ADHD/HC受试者时灵敏度达100%，准确率达99.17%。

Conclusion: DL和EEG在提高ADHD诊断准确性和效率方面有潜力。

Abstract: Attention Deficit Hyperactivity Disorder (ADHD) is a common brain disorder in
children that can persist into adulthood, affecting social, academic, and
career life. Early diagnosis is crucial for managing these impacts on patients
and the healthcare system but is often labor-intensive and time-consuming. This
paper presents a novel method to improve ADHD diagnosis precision and
timeliness by leveraging Deep Learning (DL) approaches and electroencephalogram
(EEG) signals. We introduce ADHDeepNet, a DL model that utilizes comprehensive
temporal-spatial characterization, attention modules, and explainability
techniques optimized for EEG signals. ADHDeepNet integrates feature extraction
and refinement processes to enhance ADHD diagnosis. The model was trained and
validated on a dataset of 121 participants (61 ADHD, 60 Healthy Controls),
employing nested cross-validation for robust performance. The proposed
two-stage methodology uses a 10-fold cross-subject validation strategy.
Initially, each iteration optimizes the model's hyper-parameters with inner
2-fold cross-validation. Then, Additive Gaussian Noise (AGN) with various
standard deviations and magnification levels is applied for data augmentation.
ADHDeepNet achieved 100% sensitivity and 99.17% accuracy in classifying ADHD/HC
subjects. To clarify model explainability and identify key brain regions and
frequency bands for ADHD diagnosis, we analyzed the learned weights and
activation patterns of the model's primary layers. Additionally, t-distributed
Stochastic Neighbor Embedding (t-SNE) visualized high-dimensional data, aiding
in interpreting the model's decisions. This study highlights the potential of
DL and EEG in enhancing ADHD diagnosis accuracy and efficiency.

</details>


### [100] [Merge-of-Thought Distillation](https://arxiv.org/abs/2509.08814)
*Zhanming Shen,Zeyu Qin,Zenan Huang,Hao Chen,Jiaqi Hu,Yihong Zhuang,Guoshan Lu,Gang Chen,Junbo Zhao*

Main category: cs.LG

TL;DR: 提出Merge - of - Thought Distillation (MoT)框架统一多教师推理能力到学生模型，在竞赛数学基准上表现出色，能提升性能、减少过拟合等。


<details>
  <summary>Details</summary>
Motivation: 单预言教师假设限制长思维链模型的高效推理蒸馏，实际有多个候选教师和不断增长的思维链语料，需统一多教师推理能力并克服监督冲突。

Method: 提出MoT框架，在特定监督微调分支和学生变体的权重空间合并间交替。

Result: 在竞赛数学基准上，用约200个高质量思维链样本，MoT使Qwen3 - 14B学生模型超越多个强大模型，持续优于单教师蒸馏和朴素多教师联合，减少灾难性遗忘等。

Conclusion: MoT是将不同教师的长思维链能力有效蒸馏到紧凑学生模型的简单可扩展途径。

Abstract: Efficient reasoning distillation for long chain-of-thought (CoT) models is
increasingly constrained by the assumption of a single oracle teacher, despite
practical availability of multiple candidate teachers and growing CoT corpora.
We revisit teacher selection and observe that different students have different
"best teachers," and even for the same student the best teacher can vary across
datasets. Therefore, to unify multiple teachers' reasoning abilities into
student with overcoming conflicts among various teachers' supervision, we
propose Merge-of-Thought Distillation (MoT), a lightweight framework that
alternates between teacher-specific supervised fine-tuning branches and
weight-space merging of the resulting student variants. On competition math
benchmarks, using only about 200 high-quality CoT samples, applying MoT to a
Qwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B,
QWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT
consistently outperforms the best single-teacher distillation and the naive
multi-teacher union, raises the performance ceiling while mitigating
overfitting, and shows robustness to distribution-shifted and peer-level
teachers. Moreover, MoT reduces catastrophic forgetting, improves general
reasoning beyond mathematics and even cultivates a better teacher, indicating
that consensus-filtered reasoning features transfer broadly. These results
position MoT as a simple, scalable route to efficiently distilling long CoT
capabilities from diverse teachers into compact students.

</details>


### [101] [A Survey of TinyML Applications in Beekeeping for Hive Monitoring and Management](https://arxiv.org/abs/2509.08822)
*Willy Sucipto,Jianlong Zhou,Ray Seung Min Kwon,Fang Chen*

Main category: cs.LG

TL;DR: 本文综述TinyML与养蜂业结合的创新，涵盖四个功能领域，分析资源、局限与机遇，为可持续养蜂监测系统奠基。


<details>
  <summary>Details</summary>
Motivation: 蜜蜂蜂群面临威胁，传统检查方式有局限，云监测对偏远或资源有限蜂场不实用，需新监测方案。

Method: 对TinyML和养蜂业交叉领域的创新进行综合分析，围绕四个功能领域展开，并研究相关支持资源。

Result: 梳理了四个关键功能领域、相关支持资源，指出数据稀缺等关键局限及新机遇。

Conclusion: 为可扩展、人工智能驱动且符合生态的监测系统提供基础，支持可持续传粉者管理。

Abstract: Honey bee colonies are essential for global food security and ecosystem
stability, yet they face escalating threats from pests, diseases, and
environmental stressors. Traditional hive inspections are labor-intensive and
disruptive, while cloud-based monitoring solutions remain impractical for
remote or resource-limited apiaries. Recent advances in Internet of Things
(IoT) and Tiny Machine Learning (TinyML) enable low-power, real-time monitoring
directly on edge devices, offering scalable and non-invasive alternatives. This
survey synthesizes current innovations at the intersection of TinyML and
apiculture, organized around four key functional areas: monitoring hive
conditions, recognizing bee behaviors, detecting pests and diseases, and
forecasting swarming events. We further examine supporting resources, including
publicly available datasets, lightweight model architectures optimized for
embedded deployment, and benchmarking strategies tailored to field constraints.
Critical limitations such as data scarcity, generalization challenges, and
deployment barriers in off-grid environments are highlighted, alongside
emerging opportunities in ultra-efficient inference pipelines, adaptive edge
learning, and dataset standardization. By consolidating research and
engineering practices, this work provides a foundation for scalable, AI-driven,
and ecologically informed monitoring systems to support sustainable pollinator
management.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [102] [A Systematic Survey on Large Language Models for Evolutionary Optimization: From Modeling to Solving](https://arxiv.org/abs/2509.08269)
*Yisong Zhang,Ran Cheng,Guoxing Yi,Kay Chen Tan*

Main category: cs.NE

TL;DR: 本文对大语言模型解决优化问题的研究进行全面综述，分类并分析方法，指出局限与未来方向。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型解决优化问题领域缺乏统一综合和系统分类，本文旨在填补该空白。

Method: 将现有研究分为优化建模和优化求解两阶段，后者再分三个范式，分析代表方法、技术挑战及与传统方法的相互作用。

Result: 完成对相关研究的分类、分析，回顾跨学科应用，对比LLM驱动和传统方法。

Conclusion: 指出关键局限和研究差距，指明开发自进化智能体生态系统的未来方向。

Abstract: Large Language Models (LLMs), with their strong understanding and reasoning
capabilities, are increasingly being explored for tackling optimization
problems, especially in synergy with evolutionary computation. Despite rapid
progress, however, the field still lacks a unified synthesis and a systematic
taxonomy. This survey addresses this gap by providing a comprehensive review of
recent developments and organizing them within a structured framework. We
classify existing research into two main stages: LLMs for optimization modeling
and LLMs for optimization solving. The latter is further divided into three
paradigms according to the role of LLMs in the optimization workflow: LLMs as
stand-alone optimizers, low-level LLMs embedded within optimization algorithms,
and high-level LLMs for algorithm selection and generation. For each category,
we analyze representative methods, distill technical challenges, and examine
their interplay with traditional approaches. We also review interdisciplinary
applications spanning the natural sciences, engineering, and machine learning.
By contrasting LLM-driven and conventional methods, we highlight key
limitations and research gaps, and point toward future directions for
developing self-evolving agentic ecosystems for optimization. An up-to-date
collection of related literature is maintained at
https://github.com/ishmael233/LLM4OPT.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [103] [Noise Injection for__Performance Bottleneck Analysis](https://arxiv.org/abs/2509.08446)
*Aurélien Delval,Pablo de Oliveira Castro,William Jalby,Etienne Renault*

Main category: cs.PF

TL;DR: 本文提出基于性能噪声注入的瓶颈分析框架，可跨架构移植，通过硬件基准测试验证，能辅助硬件选择。


<details>
  <summary>Details</summary>
Motivation: 瓶颈评估对HPC应用性能调优至关重要，现有技术存在不足。

Method: 基于性能噪声注入，利用LLVM编译器工具链，注入针对特定瓶颈源的噪声指令对程序分类。

Result: 在一系列硬件基准和内核上验证框架，成功检测性能状态，为硬件选择提供信息。

Conclusion: 新框架能精确分析瓶颈，在跨架构移植性上优于现有工具，可辅助硬件选择。

Abstract: Bottleneck evaluation plays a crucial part in performance tuning of HPC
applications, as it directly influences the search for optimizations and the
selection of the best hardware for a given code. In this paper, we introduce a
new model-agnostic, instruction-accurate framework for bottleneck analysis
based on performance noise injection. This method provides a precise analysis
that complements existing techniques, particularly in quantifying unused
resource slack. Specifically, we classify programs based on whether they are
limited by computation, data access bandwidth, or latency by injecting
additional noise instructions that target specific bottleneck sources. Our
approach is built on the LLVM compiler toolchain, ensuring easy portability
across different architectures and microarchitectures which constitutes an
improvement over many state-of-the-art tools. We validate our framework on a
range of hardware benchmarks and kernels, including a detailedstudy of a
sparse-matrix--vector product (SPMXV) kernel, where we successfully detect
distinct performance regimes. These insights further inform hardware selection,
as demonstrated by our comparative evaluation between HBM and DDR memory
systems.

</details>


### [104] [Memshare: Memory Sharing for Multicore Computation in R with an Application to Feature Selection by Mutual Information using PDE](https://arxiv.org/abs/2509.08632)
*Michael C. Thrun,Julian Märte*

Main category: cs.PF

TL;DR: 本文介绍了R语言的memshare包，可实现共享内存多核计算，比SharedObject快2倍且无额外常驻内存，还展示了其在特征选择分析中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决R语言在大数据分析中内存和性能限制的问题。

Method: 通过在C++共享内存中分配缓冲区并通过ALTREP视图将其暴露给R来实现共享内存多核计算，对比memshare和SharedObject，使用Pareto Density Estimation (PDE)进行特征选择。

Result: 在列方向应用基准测试中，memshare比SharedObject快2倍且无额外常驻内存。

Conclusion: 所提出的memshare库缓解了R语言在大数据分析中的局限性。

Abstract: We present memshare\footnote{The Software package is published as a CRAN
package under https://CRAN.R-project.org/package=memshare, a package that
enables shared memory multicore computation in R by allocating buffers in C++
shared memory and exposing them to R through ALTREP views. We compare memshare
to SharedObject (Bioconductor) discuss semantics and safety, and report a 2x
speedup over SharedObject with no additional resident memory in a column wise
apply benchmark. Finally, we illustrate a downstream analytics use case:
feature selection by mutual information in which densities are estimated per
feature via Pareto Density Estimation (PDE). The analytical use-case is an RNA
seq dataset consisting of N=10,446 cases and d=19,637 gene expressions
requiring roughly n_threads * 10GB of memory in the case of using parallel R
sessions. Such and larger use-cases are common in big data analytics and make R
feel limiting sometimes which is mitigated by the addition of the library
presented in this work.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [105] [ChatGPT for Code Refactoring: Analyzing Topics, Interaction, and Effective Prompts](https://arxiv.org/abs/2509.08090)
*Eman Abdullah AlOmar,Luo Xu,Sofia Martinez,Anthony Peruma,Mohamed Wiem Mkaouer,Christian D. Newman,Ali Ouni*

Main category: cs.SE

TL;DR: 本文探索开发者与ChatGPT在重构方面的交互，通过文本挖掘和分析开发者意图来了解相关情况。


<details>
  <summary>Details</summary>
Motivation: 现有研究对开发者与ChatGPT交互时表达重构需求的情况了解有限，需进一步探索。

Method: 从29,778条ChatGPT提示和响应中进行文本挖掘715条与重构相关的交互，并分析开发者明确的重构意图。

Result: 原文未提及。

Conclusion: 原文未提及。

Abstract: Large Language Models (LLMs), such as ChatGPT, have become widely popular and
widely used in various software engineering tasks such as refactoring, testing,
code review, and program comprehension. Although recent studies have examined
the effectiveness of LLMs in recommending and suggesting refactoring, there is
a limited understanding of how developers express their refactoring needs when
interacting with ChatGPT. In this paper, our goal is to explore interactions
related to refactoring between developers and ChatGPT to better understand how
developers identify areas for improvement in code, and how ChatGPT addresses
developers' needs. Our approach involves text mining 715 refactoring-related
interactions from 29,778 ChatGPT prompts and responses, as well as the analysis
of developers' explicit refactoring intentions.

</details>


### [106] [Safety Factories -- a Manifesto](https://arxiv.org/abs/2509.08285)
*Carmen Cârlan,Daniel Ratiu,Michael Wagner*

Main category: cs.SE

TL;DR: 本文提出需要弥合软件开发与安全工程的方法工具差距，倡导将安全工具和方法集成到软件开发管道的安全工厂。


<details>
  <summary>Details</summary>
Motivation: 现代网络物理系统软件发展迅速，当前软件开发与安全工程的方法和工具存在脱节，需要解决此问题以跟上软件快速发展。

Method: 前期更多投入形式化工作，用语义丰富且机器可处理的模型捕获安全工作产品，定义自动一致性检查，自动化文档生成，探索将软件最佳实践转移到安全工程，倡导安全工厂。

Result: 未提及具体结果。

Conclusion: 倡导建立安全工厂，将安全工具和方法集成到软件开发管道。

Abstract: Modern cyber-physical systems are operated by complex software that
increasingly takes over safety-critical functions. Software enables rapid
iterations and continuous delivery of new functionality that meets the
ever-changing expectations of users. As high-speed development requires
discipline, rigor, and automation, software factories are used. These entail
methods and tools used for software development, such as build systems and
pipelines. To keep up with the rapid evolution of software, we need to bridge
the disconnect in methods and tools between software development and safety
engineering today. We need to invest more in formality upfront - capturing
safety work products in semantically rich models that are machine-processable,
defining automatic consistency checks, and automating the generation of
documentation - to benefit later. Transferring best practices from software to
safety engineering is worth exploring. We advocate for safety factories, which
integrate safety tooling and methods into software development pipelines.

</details>


### [107] [The Impact of Team Diversity in Agile Development Education](https://arxiv.org/abs/2509.08389)
*Marco Torchiano,Riccardo Coppola,Antonio Vetro',Xhoi Musaj*

Main category: cs.SE

TL;DR: 研究敏捷软件开发课程项目中团队多样性（主要是性别和国籍）对项目成果的影响，发现促进多样性不负面影响团队表现。


<details>
  <summary>Details</summary>
Motivation: 软件工程领域男性主导，性别和其他多样性特征对提升机会平等、生产力和创新很重要，但国籍等多样性方面研究不足，因此评估团队多样性在课程项目中的影响。

Method: 分析三个学年的51个团队，测量性别、国籍及二者共存的三种多样性指数，考察不同多样性方面对团队项目成果质量的影响。

Result: 性别多样性与项目成功有适度显著相关性；国籍多样性对项目结果有负面但可忽略的影响；性别和国籍共存对项目有负面影响，可能是沟通障碍和文化规范差异导致。

Conclusion: 在教育环境中要考虑多种多样性维度及其相互作用，促进团队多样性不会负面影响团队表现和教育目标达成。

Abstract: Software Engineering is mostly a male-dominated sector, where gender
diversity is a key feature for improving equality of opportunities,
productivity, and innovation. Other diversity aspects, including but not
limited to nationality and ethnicity, are often understudied.In this work we
aim to assess the impact of team diversity, focusing mainly on gender and
nationality, in the context of an agile software development project-based
course. We analyzed 51 teams over three academic years, measuring three
different Diversity indexes - regarding Gender, Nationality and their
co-presence - to examine how different aspects of diversity impact the quality
of team project outcomes.Statistical analysis revealed a moderate,
statistically significant correlation between gender diversity and project
success, aligning with existing literature. Diversity in nationality showed a
negative but negligible effect on project results, indicating that promoting
these aspects does not harm students' performance. Analyzing their co-presence
within a team, gender and nationality combined had a negative impact, likely
due to increased communication barriers and differing cultural norms.This study
underscores the importance of considering multiple diversity dimensions and
their interactions in educational settings. Our findings, overall, show that
promoting diversity in teams does not negatively impact their performance and
achievement of educational goals.

</details>


### [108] [AutoStub: Genetic Programming-Based Stub Creation for Symbolic Execution](https://arxiv.org/abs/2509.08524)
*Felix Mächtle,Nils Loose,Jan-Niclas Serr,Jonas Sander,Thomas Eisenbarth*

Main category: cs.SE

TL;DR: 提出AutoStub方法利用遗传编程自动生成外部函数符号桩，可让符号执行器继续分析，对部分函数近似准确率超90%。


<details>
  <summary>Details</summary>
Motivation: 符号执行在处理外部函数时有局限，现有解决方案需额外上下文、昂贵SMT求解器或人工干预。

Method: 当符号执行器遇到外部函数，AutoStub通过随机输入生成训练数据，用遗传编程推导近似函数行为的表达式作为符号桩。

Result: AutoStub能对55%评估的函数以超90%的准确率自动近似外部函数，可推断特定语言行为揭示软件测试关键边缘情况。

Conclusion: AutoStub可在无人工干预下让符号执行器继续分析，探索之前难以处理的程序路径。

Abstract: Symbolic execution is a powerful technique for software testing, but suffers
from limitations when encountering external functions, such as native methods
or third-party libraries. Existing solutions often require additional context,
expensive SMT solvers, or manual intervention to approximate these functions
through symbolic stubs. In this work, we propose a novel approach to
automatically generate symbolic stubs for external functions during symbolic
execution that leverages Genetic Programming. When the symbolic executor
encounters an external function, AutoStub generates training data by executing
the function on randomly generated inputs and collecting the outputs. Genetic
Programming then derives expressions that approximate the behavior of the
function, serving as symbolic stubs. These automatically generated stubs allow
the symbolic executor to continue the analysis without manual intervention,
enabling the exploration of program paths that were previously intractable. We
demonstrate that AutoStub can automatically approximate external functions with
over 90% accuracy for 55% of the functions evaluated, and can infer
language-specific behaviors that reveal edge cases crucial for software
testing.

</details>


### [109] [Beyond the Binary: The System of All-round Evaluation of Research and Its Practices in China](https://arxiv.org/abs/2509.08546)
*Yu Zhu,Jiyuan Ye*

Main category: cs.SE

TL;DR: 缺乏宏观系统评价理论制约全球科研评价改革，本文引入SAER框架，突破二元对立，为全球评价改革提供基础和借鉴。


<details>
  <summary>Details</summary>
Motivation: 解决全球科研评价体系改革中缺乏宏观系统评价理论指导的瓶颈问题。

Method: 回顾科研评价的历史发展，引入集成形式、内容和效用评价以及六个关键要素的SAER框架。

Result: SAER框架提供了理论突破，提出三位一体的评价维度与六个评价要素结合的综合体系。

Conclusion: SAER框架有助于调和评价方法的二元对立，凸显中国科研评价理论智慧，为全球科研评价体系改革提供参考。

Abstract: The lack of a macro-level, systematic evaluation theory to guide the
implementation of evaluation practices has become a key bottleneck in the
reform of global research evaluation systems. By reviewing the historical
development of research evaluation, this paper highlights the current binary
opposition between qualitative and quantitative methods in evaluation
practices. This paper introduces the System of All-round Evaluation of Research
(SAER), a framework that integrates form, content, and utility evaluations with
six key elements. SAER offers a theoretical breakthrough by transcending the
binary, providing a comprehensive foundation for global evaluation reforms. The
comprehensive system proposes a trinity of three evaluation dimensions,
combined with six evaluation elements, which would help academic evaluators and
researchers reconcile binary oppositions in evaluation methods. The system
highlights the dialectical wisdom and experience embedded in Chinese research
evaluation theory, offering valuable insights and references for the reform and
advancement of global research evaluation systems.

</details>


### [110] [Minimal Data, Maximum Clarity: A Heuristic for Explaining Optimization](https://arxiv.org/abs/2509.08667)
*Amirali Rayegan,Tim Menzies*

Main category: cs.SE

TL;DR: 本文介绍了多目标优化框架EZR，用更少但更有信息的数据实现有效且可解释的优化，实验证明其性能好且解释清晰。


<details>
  <summary>Details</summary>
Motivation: 软件工程中高效、可解释的优化面临配置空间大、标注过程昂贵且易出错的挑战。

Method: 引入EZR框架，采用基于朴素贝叶斯采样的主动学习策略，将优化逻辑提炼为决策树。

Result: 在60个真实数据集上，EZR多数情况下能达到超90%的最佳优化性能，解释清晰度和实用性超标准XAI方法。

Conclusion: 使用更少但更有信息的示例进行标签高效的优化和解释是可行且更优的。

Abstract: Efficient, interpretable optimization is a critical but underexplored
challenge in software engineering, where practitioners routinely face vast
configuration spaces and costly, error-prone labeling processes. This paper
introduces EZR, a novel and modular framework for multi-objective optimization
that unifies active sampling, learning, and explanation within a single,
lightweight pipeline. Departing from conventional wisdom, our Maximum Clarity
Heuristic demonstrates that using less (but more informative) data can yield
optimization models that are both effective and deeply understandable. EZR
employs an active learning strategy based on Naive Bayes sampling to
efficiently identify high-quality configurations with a fraction of the labels
required by fully supervised approaches. It then distills optimization logic
into concise decision trees, offering transparent, actionable explanations for
both global and local decision-making. Extensive experiments across 60
real-world datasets establish that EZR reliably achieves over 90% of the
best-known optimization performance in most cases, while providing clear,
cohort-based rationales that surpass standard attribution-based explainable AI
(XAI) methods (LIME, SHAP, BreakDown) in clarity and utility. These results
endorse "less but better"; it is both possible and often preferable to use
fewer (but more informative) examples to generate label-efficient optimization
and explanations in software systems. To support transparency and
reproducibility, all code and experimental materials are publicly available at
https://github.com/amiiralii/Minimal-Data-Maximum-Clarity.

</details>


### [111] [SWE-Mirror: Scaling Issue-Resolving Datasets by Mirroring Issues Across Repositories](https://arxiv.org/abs/2509.08724)
*Junhao Wang,Daoguang Zan,Shulin Xin,Siyao Liu,Yurong Wu,Kai Shen*

Main category: cs.SE

TL;DR: 提出SWE - Mirror管道构建可验证问题解决任务数据集，应用于40个仓库生成60671个任务，训练模型提升解决问题能力，扩展数据集后在特定框架达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有自动化Gym环境设置方法成功率低、开销大，且未充分利用真实问题数据，需最大化利用现有Gym环境和GitHub问题解决历史数据。

Method: 引入SWE - Mirror管道，提取真实问题语义，镜像到配置好Gym环境的仓库，重新生成为可验证任务。

Result: 构建含60671个问题解决任务的数据集，训练模型提升解决问题能力，扩展数据集后在OpenHands框架达SOTA，7B模型解决率提升21.8%，32B模型提升46.0%。

Conclusion: SWE - Mirror方法有效，构建的数据集能提升模型问题解决能力。

Abstract: Creating large-scale verifiable training datasets for issue-resolving tasks
is a critical yet notoriously difficult challenge. Existing methods on
automating the Gym environment setup process for real-world issues suffer from
low success rates and high overhead. Meanwhile, synthesizing new tasks within
existing Gym environments leaves the vast pool of authentic, human-reported
problems untapped. To maximize the utilization of existing Gym environments and
also the rich data of issue-resolving history on GitHub, we introduce
SWE-Mirror, a pipeline that distills a real-world issue's semantic essence,
mirrors it into another repository with a configured Gym environment, and
re-animates it as a verifiable issue-resolving task. SWE-Mirror reuses existing
Gym environments along with the vast pool of issue-resolving history hosted on
GitHub to construct a large-scale dataset of mirrored authentic and verifiable
tasks. Applying SWE-Mirror to 40 repositories across 4 languages, we have
curated a dataset with 60,671 issue-resolving tasks and demonstrated the value
of our dataset by training and evaluating coding agents at various scale.
Post-training experiments show that models trained with the dataset exhibit
improvements in issue-resolving capabilities. Furthermore, by extending the
dataset size to over 12,000 high-quality trajectories, we established a new
state-of-the-art (SOTA) among Qwen2.5-Coder-Instruct based LLMs on the
OpenHands agent framework, which increases the resolve rate on
SWE-Bench-Verified by +21.8% for the 7B model and +46.0% for the 32B model and
validates the effectiveness of our approach.

</details>


### [112] [Handling Open-Vocabulary Constructs in Formalizing Specifications: Retrieval-Augmented Parsing with Expert Knowledge](https://arxiv.org/abs/2509.08808)
*Mohammad Saqib Hasan,Sayontan Ghosh,Dhruv Verma,Geoff Kuenning,Erez Zadok,Scott A. Smolka,Niranjan Balasubramanian*

Main category: cs.SE

TL;DR: 研究自然语言转形式语言中开放词汇构造问题，提出DKAP和ROLex方法，用合成数据训练，新评估范式显示ROLex能提升基线模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决模型在开放词汇构造上因缺乏先验知识表现不佳的问题，有效复用推理时专家提供的知识。

Method: 提出动态知识增强解析（DKAP），用关键值词典关联自然语言短语和正确构造；提出ROLex检索增强解析方法，用合成数据和数据增强技术训练，提出多策略提升训练效果。

Result: 评估表明DKAP是难题，ROLex能有效利用动态专家知识提升基线模型性能。

Conclusion: ROLex可通过有效利用动态专家知识，改善自然语言转形式语言中模型在开放词汇构造上的表现。

Abstract: We study the problem of Open-Vocabulary Constructs(OVCs) -- ones not known
beforehand -- in the context of converting natural language (NL) specifications
into formal languages (e.g., temporal logic or code). Models fare poorly on
OVCs due to a lack of necessary knowledge a priori. In such situations, a
domain expert can provide correct constructs at inference time based on their
preferences or domain knowledge. Our goal is to effectively reuse this
inference-time, expert-provided knowledge for future parses without retraining
the model. We present dynamic knowledge-augmented parsing(DKAP), where in
addition to the input sentence, the model receives (dynamically growing) expert
knowledge as a key-value lexicon that associates NL phrases with correct OVC
constructs. We propose ROLex, a retrieval-augmented parsing approach that uses
this lexicon. A retriever and a generator are trained to find and use the
key-value store to produce the correct parse. A key challenge lies in curating
data for this retrieval-augmented parser. We utilize synthetic data generation
and the data augmentation techniques on annotated (NL sentence, FL statement)
pairs to train the augmented parser. To improve training effectiveness, we
propose multiple strategies to teach models to focus on the relevant subset of
retrieved knowledge. Finally, we introduce a new evaluation paradigm modeled
after the DKAP problem and simulate the scenario across three formalization
tasks (NL2LTL, NL2Code, and NL2CMD). Our evaluations show that DKAP is a
difficult challenge, and ROLex helps improve the performance of baseline models
by using dynamic expert knowledge effectively.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [113] [FinZero: Launching Multi-modal Financial Time Series Forecast with Large Reasoning Model](https://arxiv.org/abs/2509.08742)
*Yanlong Wang,Jian Xu,Fei Ma,Hongkang Zhang,Hang Yu,Tiantian Gao,Yu Wang,Haochen You,Shao-Lun Huang,Danny Dongning Sun,Xiao-Ping Zhang*

Main category: q-fin.CP

TL;DR: 本文提出构建金融图像文本数据集FVLDB和UARPO方法，训练多模态预训练模型FinZero用于金融时间序列预测，实验验证其适应性、可扩展性和有效性。


<details>
  <summary>Details</summary>
Motivation: 过往金融时间序列预测方法存在信息损失、可扩展性受限、预测可解释性和不确定性研究不足等问题，影响预测可靠性和实用价值。

Method: 构建FVLDB数据集，开发UARPO方法，用UARPO微调多模态预训练模型FinZero。

Result: FinZero展现出强适应性和可扩展性，在高置信组预测准确率比GPT - 4o提升约13.48%。

Conclusion: 强化学习微调在多模态大模型包括金融时间序列预测任务中有效。

Abstract: Financial time series forecasting is both highly significant and challenging.
Previous approaches typically standardized time series data before feeding it
into forecasting models, but this encoding process inherently leads to a loss
of important information. Moreover, past time series models generally require
fixed numbers of variables or lookback window lengths, which further limits the
scalability of time series forecasting. Besides, the interpretability and the
uncertainty in forecasting remain areas requiring further research, as these
factors directly impact the reliability and practical value of predictions. To
address these issues, we first construct a diverse financial image-text dataset
(FVLDB) and develop the Uncertainty-adjusted Group Relative Policy Optimization
(UARPO) method to enable the model not only output predictions but also analyze
the uncertainty of those predictions. We then proposed FinZero, a multimodal
pre-trained model finetuned by UARPO to perform reasoning, prediction, and
analytical understanding on the FVLDB financial time series. Extensive
experiments validate that FinZero exhibits strong adaptability and scalability.
After fine-tuning with UARPO, FinZero achieves an approximate 13.48\%
improvement in prediction accuracy over GPT-4o in the high-confidence group,
demonstrating the effectiveness of reinforcement learning fine-tuning in
multimodal large model, including in financial time series forecasting tasks.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [114] [Chaotic Bayesian Inference: Strange Attractors as Risk Models for Black Swan Events](https://arxiv.org/abs/2509.08183)
*Crystal Rust*

Main category: q-fin.RM

TL;DR: 介绍新风险建模框架，结合重尾先验与动力学，对比两模型为系统性风险分析提供双视角。


<details>
  <summary>Details</summary>
Motivation: 建立新的风险建模框架，用于系统性风险分析。

Method: 结合重尾先验与Lorenz和Rossler动力学，对比强调几何稳定性的Model A和用Fibonacci诊断突出罕见爆发的Model B。

Result: 模型自然生成波动率聚类、肥尾和极端事件。

Conclusion: 两模型为系统性风险分析提供双视角，将黑天鹅理论与压力测试和波动率监测实用工具相联系。

Abstract: We introduce a new risk modeling framework where chaotic attractors shape the
geometry of Bayesian inference. By combining heavy-tailed priors with Lorenz
and Rossler dynamics, the models naturally generate volatility clustering, fat
tails, and extreme events. We compare two complementary approaches: Model A,
which emphasizes geometric stability, and Model B, which highlights rare bursts
using Fibonacci diagnostics. Together, they provide a dual perspective for
systemic risk analysis, linking Black Swan theory to practical tools for stress
testing and volatility monitoring.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [115] [kNNSampler: Stochastic Imputations for Recovering Missing Value Distributions](https://arxiv.org/abs/2509.08366)
*Parastoo Pashmchi,Jerome Benoit,Motonobu Kanagawa*

Main category: stat.ML

TL;DR: 提出kNNSampler缺失值插补方法，可从分布中采样缺失值、量化不确定性，实验证明其有效且代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有的kNNImputer只能估计条件均值，需要一种能估计条件分布的缺失值插补方法。

Method: kNNSampler通过从与给定单元在观测协变量方面最相似的k个单元的观测响应中随机采样来插补缺失响应。

Result: 实验证明kNNSampler在恢复缺失值分布方面有效。

Conclusion: kNNSampler能估计条件分布，可用于缺失值插补，代码已开源。

Abstract: We study a missing-value imputation method, termed kNNSampler, that imputes a
given unit's missing response by randomly sampling from the observed responses
of the $k$ most similar units to the given unit in terms of the observed
covariates. This method can sample unknown missing values from their
distributions, quantify the uncertainties of missing values, and be readily
used for multiple imputation. Unlike popular kNNImputer, which estimates the
conditional mean of a missing response given an observed covariate, kNNSampler
is theoretically shown to estimate the conditional distribution of a missing
response given an observed covariate. Experiments demonstrate its effectiveness
in recovering the distribution of missing values. The code for kNNSampler is
made publicly available (https://github.com/SAP/knn-sampler).

</details>


### [116] [Gaussian Process Regression -- Neural Network Hybrid with Optimized Redundant Coordinates](https://arxiv.org/abs/2509.08457)
*Sergei Manzhos,Manabu Ihara*

Main category: stat.ML

TL;DR: 提出opt - GPRNN方法，优化GPRNN冗余坐标，减少项/神经元数量获低测试集误差，避免过拟合，可降低维度并给出应用示例。


<details>
  <summary>Details</summary>
Motivation: 改进GPRNN方法，进一步降低测试集误差、减少项/神经元数量，避免过拟合，减少对深度神经网络的依赖。

Method: 用蒙特卡罗算法优化GPRNN的冗余坐标。

Result: opt - GPRNN用更少项/神经元获得最低测试集误差，避免过拟合，有接近多层神经网络的表达能力，可实现降维。

Conclusion: opt - GPRNN在一些应用中可替代深度神经网络，在机器学习原子间势和材料信息学中有应用价值。

Abstract: Recently, a Gaussian Process Regression - neural network (GPRNN) hybrid
machine learning method was proposed, which is based on additive-kernel GPR in
redundant coordinates constructed by rules [J. Phys. Chem. A 127 (2023) 7823].
The method combined the expressive power of an NN with the robustness of linear
regression, in particular, with respect to overfitting when the number of
neurons is increased beyond optimal. We introduce opt-GPRNN, in which the
redundant coordinates of GPRNN are optimized with a Monte Carlo algorithm and
show that when combined with optimization of redundant coordinates, GPRNN
attains the lowest test set error with much fewer terms / neurons and retains
the advantage of avoiding overfitting when the number of neurons is increased
beyond optimal value. The method, opt-GPRNN possesses an expressive power
closer to that of a multilayer NN and could obviate the need for deep NNs in
some applications. With optimized redundant coordinates, a dimensionality
reduction regime is also possible. Examples of application to machine learning
an interatomic potential and materials informatics are given.

</details>


### [117] [PEHRT: A Common Pipeline for Harmonizing Electronic Health Record data for Translational Research](https://arxiv.org/abs/2509.08553)
*Jessica Gronsbell,Vidul Ayakulangara Panickan,Chris Lin,Thomas Charlon,Chuan Hong,Doudou Zhou,Linshanshan Wang,Jianhui Gao,Shirley Zhou,Yuan Tian,Yaqi Shi,Ziming Gan,Tianxi Cai*

Main category: stat.ML

TL;DR: 介绍了标准化管道PEHRT用于高效电子健康记录（EHR）数据协调，可解决多机构EHR数据协调难题，还提供开源软件和教程并展示其效用。


<details>
  <summary>Details</summary>
Motivation: 多机构EHR数据整合分析可提升转化研究可靠性和普遍性，但数据协调面临数据异质性、语义差异和隐私问题等挑战。

Method: 引入PEHRT管道，包含数据预处理和表示学习两个核心模块，将EHR数据映射到标准编码系统，用机器学习生成研究可用数据集，且与数据模型无关，基于实际经验设计跨机构执行流程。

Result: 提供了一套完整的开源软件和用户友好教程，在不同医疗系统数据的多种任务中展示了PEHRT的效用。

Conclusion: PEHRT能有效解决多机构EHR数据协调的挑战，可应用于多种任务。

Abstract: Integrative analysis of multi-institutional Electronic Health Record (EHR)
data enhances the reliability and generalizability of translational research by
leveraging larger, more diverse patient cohorts and incorporating multiple data
modalities. However, harmonizing EHR data across institutions poses major
challenges due to data heterogeneity, semantic differences, and privacy
concerns. To address these challenges, we introduce $\textit{PEHRT}$, a
standardized pipeline for efficient EHR data harmonization consisting of two
core modules: (1) data pre-processing and (2) representation learning. PEHRT
maps EHR data to standard coding systems and uses advanced machine learning to
generate research-ready datasets without requiring individual-level data
sharing. Our pipeline is also data model agnostic and designed for streamlined
execution across institutions based on our extensive real-world experience. We
provide a complete suite of open source software, accompanied by a
user-friendly tutorial, and demonstrate the utility of PEHRT in a variety of
tasks using data from diverse healthcare systems.

</details>


### [118] [A hierarchical entropy method for the delocalization of bias in high-dimensional Langevin Monte Carlo](https://arxiv.org/abs/2509.08619)
*Daniel Lacker,Fuzhong Zhou*

Main category: stat.ML

TL;DR: 本文强化了Chen等人(2024)在稀疏交互机制下的结果，拓展了离域现象的适用范围。


<details>
  <summary>Details</summary>
Motivation: 现有未调整的兰格文算法有偏差，需强化Chen等人(2024)的结果并拓展离域现象适用范围。

Method: 通过去除对数因子、用相对熵测量距离、放宽强对数凹性假设，基于边际相对熵的分层分析进行证明。

Result: 强化了稀疏交互机制下的结果，证明离域现象在弱交互分布类中也成立。

Conclusion: 在稀疏和弱交互分布类中对未调整兰格文算法偏差分析有了更优结果和更广适用范围。

Abstract: The unadjusted Langevin algorithm is widely used for sampling from complex
high-dimensional distributions. It is well known to be biased, with the bias
typically scaling linearly with the dimension when measured in squared
Wasserstein distance. However, the recent paper of Chen et al. (2024)
identifies an intriguing new delocalization effect: For a class of
distributions with sparse interactions, the bias between low-dimensional
marginals scales only with the lower dimension, not the full dimension. In this
work, we strengthen the results of Chen et al. (2024) in the sparse interaction
regime by removing a logarithmic factor, measuring distance in relative entropy
(a.k.a. KL-divergence), and relaxing the strong log-concavity assumption. In
addition, we expand the scope of the delocalization phenomenon by showing that
it holds for a class of distributions with weak interactions. Our proofs are
based on a hierarchical analysis of the marginal relative entropies, inspired
by the authors' recent work on propagation of chaos.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [119] [Decarbonizing Basic Chemicals Production in North America, Europe, Middle East, and China: a Scenario Modeling Study](https://arxiv.org/abs/2509.08279)
*Tubagus Aryandi Gunawan,Hongxi Luo,Chris Greig,Eric D. Larson*

Main category: econ.GN

TL;DR: 本文对基础化学品生产行业脱碳路径建模，发现脱碳时间因化学品和地区而异，且依赖投资环境，最佳情况下未来二三十年减排年均资本支出要高于历史水平，投资受限则脱碳时间会延长到本世纪下半叶。


<details>
  <summary>Details</summary>
Motivation: 化工行业占全球温室气体排放约5%且减排困难，需为基础化学品生产这一能源密集型细分行业建模脱碳路径。

Method: 采用情景分析方法，在高度不确定的长期投资环境下，考虑企业投资决策的核心作用，对四个主要产区超2600个生产设施的减排项目单独建模，构建成本最低的脱碳时间表。

Result: 深度脱碳生产的时间表因化学品和地区而异，最佳环境下未来二三十年减排年均资本支出要高于历史水平，累计投资超1万亿美元；投资受限则脱碳时间会延长到本世纪下半叶。

Conclusion: 化工行业脱碳时间依赖投资环境，投资情况不同脱碳的进程和成本有很大差异。

Abstract: The chemicals industry accounts for about 5% of global greenhouse gas
emissions today and is among the most difficult industries to abate. We model
decarbonization pathways for the most energy-intensive segment of the industry,
the production of basic chemicals: olefins, aromatics, methanol, ammonia, and
chlor-alkali. Unlike most prior pathways studies, we apply a scenario-analysis
approach that recognizes the central role of corporate investment decision
making for capital-intensive industries, under highly uncertain long-term
future investment environments. We vary the average pace of decarbonization
capital allocation allowed under plausible alternative future world contexts
and construct least-cost decarbonization timelines by modeling abatement
projects individually across more than 2,600 production facilities located in
four major producing regions. The timeline for deeply decarbonizing production
varies by chemical and region but depends importantly on the investment
environment context. In the best-of-all environments, to deeply decarbonize
production, annual average capital spending for abatement for the next two to
three decades will need to be greater than (and in addition to) historical
"business-as-usual" investments, and cumulative investment in abatement
projects would exceed $1 trillion. In futures where key drivers constrain
investment appetites, timelines for decarbonizing the industry extend well into
the second half of the century.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [120] [Signals vs. Videos: Advancing Motion Intention Recognition for Human-Robot Collaboration in Construction](https://arxiv.org/abs/2509.07990)
*Charan Gajjala Chenchu,Kinam Kim,Gao Lu,Zia Ud Din*

Main category: eess.SP

TL;DR: 研究利用深度学习评估信号和视频两种数据模态在干墙安装任务中早期识别工人运动意图的效果，强调两种数据格式特点及权衡，以提升建筑项目人机协作。


<details>
  <summary>Details</summary>
Motivation: 建筑行业人机协作需精准识别工人运动意图和动作，当前缺乏对信号和视频两种数据模态用于运动意图识别的比较研究。

Method: 利用卷积神经网络 - 长短期记忆网络（CNN - LSTM）模型处理表面肌电图（sEMG）数据，用预训练的视频Swin Transformer结合迁移学习处理视频序列。

Result: CNN - LSTM处理sEMG数据识别准确率约87%，平均预测时间0.04秒；视频Swin Transformer结合迁移学习准确率94%，平均预测时间0.15秒。

Conclusion: 强调两种数据格式各有优势和权衡，应系统部署以提升现实建筑项目中的人机协作。

Abstract: Human-robot collaboration (HRC) in the construction industry depends on
precise and prompt recognition of human motion intentions and actions by robots
to maximize safety and workflow efficiency. There is a research gap in
comparing data modalities, specifically signals and videos, for motion
intention recognition. To address this, the study leverages deep learning to
assess two different modalities in recognizing workers' motion intention at the
early stage of movement in drywall installation tasks. The Convolutional Neural
Network - Long Short-Term Memory (CNN-LSTM) model utilizing surface
electromyography (sEMG) data achieved an accuracy of around 87% with an average
time of 0.04 seconds to perform prediction on a sample input. Meanwhile, the
pre-trained Video Swin Transformer combined with transfer learning harnessed
video sequences as input to recognize motion intention and attained an accuracy
of 94% but with a longer average time of 0.15 seconds for a similar prediction.
This study emphasizes the unique strengths and trade-offs of both data formats,
directing their systematic deployments to enhance HRC in real-world
construction projects.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [121] [Forecasting Generative Amplification](https://arxiv.org/abs/2509.08048)
*Henning Bahl,Sascha Diefenbacher,Nina Elmer,Tilman Plehn,Jonas Spinner*

Main category: hep-ph

TL;DR: 提出两种互补方法估计生成网络放大因子，应用于事件生成器表明特定相空间区域可放大。


<details>
  <summary>Details</summary>
Motivation: 了解生成网络在生成超过训练数据集规模事件时的统计精度。

Method: 提出平均放大法（用贝叶斯网络或集成法从给定相空间体积积分精度估计放大）和微分放大法（用假设检验量化放大无分辨率损失）。

Result: 两种方法应用于先进事件生成器，表明特定相空间区域可放大，但非全分布。

Conclusion: 提出的方法可用于估计生成网络放大因子，特定区域放大可行。

Abstract: Generative networks are perfect tools to enhance the speed and precision of
LHC simulations. It is important to understand their statistical precision,
especially when generating events beyond the size of the training dataset. We
present two complementary methods to estimate the amplification factor without
large holdout datasets. Averaging amplification uses Bayesian networks or
ensembling to estimate amplification from the precision of integrals over given
phase-space volumes. Differential amplification uses hypothesis testing to
quantify amplification without any resolution loss. Applied to state-of-the-art
event generators, both methods indicate that amplification is possible in
specific regions of phase space, but not yet across the entire distribution.

</details>


### [122] [Agents of Discovery](https://arxiv.org/abs/2509.08535)
*Sascha Diefenbacher,Anna Hallin,Gregor Kasieczka,Michael Krämer,Anne Lauscher,Tim Lukas*

Main category: hep-ph

TL;DR: 本文探讨利用大语言模型创建代理团队解决数据分析研究问题，以LHC Olympics数据集进行异常检测任务测试，发现代理系统有解决问题的能力，最佳方案性能与人类先进结果相当。


<details>
  <summary>Details</summary>
Motivation: 现代基础物理研究数据量大，数据分析工具和工作流日益复杂，机器学习工具多为专用算法，需新方法应对，探索用大语言模型代理团队解决问题并自动化常规分析组件。

Method: 利用大语言模型创建代理团队，通过创建代码操作标准工具和库，基于先前迭代结果解决问题；以LHC Olympics数据集进行异常检测任务，测试OpenAI多个当前模型的稳定性。

Result: 观察到基于代理的系统有解决数据分析问题的能力，最佳代理创建的解决方案性能与人类先进结果相当。

Conclusion: 基于大语言模型的代理团队系统在解决数据分析研究问题上具有可行性和有效性，可用于自动化常规分析组件应对工具链复杂性。

Abstract: The substantial data volumes encountered in modern particle physics and other
domains of fundamental physics research allow (and require) the use of
increasingly complex data analysis tools and workflows. While the use of
machine learning (ML) tools for data analysis has recently proliferated, these
tools are typically special-purpose algorithms that rely, for example, on
encoded physics knowledge to reach optimal performance. In this work, we
investigate a new and orthogonal direction: Using recent progress in large
language models (LLMs) to create a team of agents -- instances of LLMs with
specific subtasks -- that jointly solve data analysis-based research problems
in a way similar to how a human researcher might: by creating code to operate
standard tools and libraries (including ML systems) and by building on results
of previous iterations. If successful, such agent-based systems could be
deployed to automate routine analysis components to counteract the increasing
complexity of modern tool chains. To investigate the capabilities of
current-generation commercial LLMs, we consider the task of anomaly detection
via the publicly available and highly-studied LHC Olympics dataset. Several
current models by OpenAI (GPT-4o, o4-mini, GPT-4.1, and GPT-5) are investigated
and their stability tested. Overall, we observe the capacity of the agent-based
system to solve this data analysis problem. The best agent-created solutions
mirror the performance of human state-of-the-art results.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [123] [XML Prompting as Grammar-Constrained Interaction: Fixed-Point Semantics, Convergence Guarantees, and Human-AI Protocols](https://arxiv.org/abs/2509.08182)
*Faruk Alpay,Taylan Alpay*

Main category: cs.PL

TL;DR: 本文提出对XML提示的逻辑优先处理方法，统一多种元素，给出数学证明并展示实际部署模式。


<details>
  <summary>Details</summary>
Motivation: 结构化XML标签提示是引导大语言模型输出可解析、符合模式结果的有效方式，需对其进行逻辑优先处理。

Method: 统一语法约束解码、分层提示格上的不动点语义和人机交互循环；形式化XML树的完全格并证明相关定理；用上下文无关文法实例化结果。

Result: 证明了单调提示到提示算子存在最小不动点，迭代引导有Banach式收敛；约束解码保证格式良好并保持任务性能；展示了多层人机交互模式。

Conclusion: 提供了数学上完整的证明，将框架与语法对齐解码等领域的最新进展联系起来。

Abstract: Structured prompting with XML tags has emerged as an effective way to steer
large language models (LLMs) toward parseable, schema-adherent outputs in
real-world systems. We develop a logic-first treatment of XML prompting that
unifies (i) grammar-constrained decoding, (ii) fixed-point semantics over
lattices of hierarchical prompts, and (iii) convergent human-AI interaction
loops. We formalize a complete lattice of XML trees under a refinement order
and prove that monotone prompt-to-prompt operators admit least fixed points
(Knaster-Tarski) that characterize steady-state protocols; under a task-aware
contraction metric on trees, we further prove Banach-style convergence of
iterative guidance. We instantiate these results with context-free grammars
(CFGs) for XML schemas and show how constrained decoding guarantees
well-formedness while preserving task performance. A set of multi-layer
human-AI interaction recipes demonstrates practical deployment patterns,
including multi-pass "plan $\to$ verify $\to$ revise" routines and agentic tool
use. We provide mathematically complete proofs and tie our framework to recent
advances in grammar-aligned decoding, chain-of-verification, and programmatic
prompting.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [124] [Lifetime-Aware Design of Item-Level Intelligence](https://arxiv.org/abs/2509.08193)
*Shvetank Prakash,Andrew Cheng,Olof Kindgren,Ashiq Ahamed,Graham Knight,Jed Kufel,Francisco Rodriguez,Arya Tschand,David Kong,Mariam Elgamal,Jerry Huang,Emma Chen,Gage Hills,Richard Price,Emre Ozer,Vijay Janapa Reddi*

Main category: cs.AR

TL;DR: 提出FlexiFlow框架用于物品级智能，借助柔性电子，考虑应用寿命建模碳足迹权衡，含工作负载套件、优化内核和碳感知模型，可降低碳足迹，通过流片验证。


<details>
  <summary>Details</summary>
Motivation: 传统计算部署模式不适用于物品级智能应用，在万亿级物品部署规模下需新的架构设计决策，且要考虑碳足迹。

Method: 利用原生柔性电子，基于应用特定寿命对体现碳足迹和运营碳足迹的权衡进行建模，构建FlexiBench、FlexiBits和碳感知模型。

Result: 寿命感知的微架构设计可使碳足迹降低1.62倍，算法决策可降低14.5倍，首次使用PDK进行流片验证，实现30.9kHz操作。

Conclusion: FlexiFlow能助力探索极端边缘计算，传统设计方法需重新评估。

Abstract: We present FlexiFlow, a lifetime-aware design framework for item-level
intelligence (ILI) where computation is integrated directly into disposable
products like food packaging and medical patches. Our framework leverages
natively flexible electronics which offer significantly lower costs than
silicon but are limited to kHz speeds and several thousands of gates. Our
insight is that unlike traditional computing with more uniform deployment
patterns, ILI applications exhibit 1000X variation in operational lifetime,
fundamentally changing optimal architectural design decisions when considering
trillion-item deployment scales. To enable holistic design and optimization, we
model the trade-offs between embodied carbon footprint and operational carbon
footprint based on application-specific lifetimes. The framework includes: (1)
FlexiBench, a workload suite targeting sustainability applications from
spoilage detection to health monitoring; (2) FlexiBits, area-optimized RISC-V
cores with 1/4/8-bit datapaths achieving 2.65X to 3.50X better energy
efficiency per workload execution; and (3) a carbon-aware model that selects
optimal architectures based on deployment characteristics. We show that
lifetime-aware microarchitectural design can reduce carbon footprint by 1.62X,
while algorithmic decisions can reduce carbon footprint by 14.5X. We validate
our approach through the first tape-out using a PDK for flexible electronics
with fully open-source tools, achieving 30.9kHz operation. FlexiFlow enables
exploration of computing at the Extreme Edge where conventional design
methodologies must be reevaluated to account for new constraints and
considerations.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [125] [Steering Protein Language Models](https://arxiv.org/abs/2509.07983)
*Long-Kai Huang,Rongyi Zhu,Bing He,Jianhua Yao*

Main category: q-bio.BM

TL;DR: 本文探讨用激活引导技术控制蛋白质语言模型（PLMs）输出，提出方法并实验验证其可集成到PLMs用于精确蛋白质工程。


<details>
  <summary>Details</summary>
Motivation: PLMs在产生具有特定功能或属性的蛋白质方面存在输出控制难题，需探索控制其输出的方法。

Method: 采用激活编辑引导PLM输出，通过新颖的编辑位点识别模块将该方法扩展到蛋白质优化。

Result: 方法可无缝集成到自编码和自回归PLMs中，无需额外训练。

Conclusion: 为使用基础模型进行精确蛋白质工程提供了有前景的方向。

Abstract: Protein Language Models (PLMs), pre-trained on extensive evolutionary data
from natural proteins, have emerged as indispensable tools for protein design.
While powerful, PLMs often struggle to produce proteins with precisely
specified functionalities or properties due to inherent challenges in
controlling their outputs. In this work, we investigate the potential of
Activation Steering, a technique originally developed for controlling text
generation in Large Language Models (LLMs), to direct PLMs toward generating
protein sequences with targeted properties. We propose a simple yet effective
method that employs activation editing to steer PLM outputs, and extend this
approach to protein optimization through a novel editing site identification
module. Through comprehensive experiments on lysozyme-like sequence generation
and optimization, we demonstrate that our methods can be seamlessly integrated
into both auto-encoding and autoregressive PLMs without requiring additional
training. These results highlight a promising direction for precise protein
engineering using foundation models.

</details>


### [126] [Tokenizing Loops of Antibodies](https://arxiv.org/abs/2509.08707)
*Ada Fang,Robert G. Alberstein,Simon Kelow,Frédéric A. Dreyer*

Main category: q-bio.BM

TL;DR: 本文介绍了多模态抗体环分词器Igloo，可编码抗体环结构与序列，在检索、预测结合亲和力和采样等方面表现出色，证明多模态分词器对抗体环设计有益。


<details>
  <summary>Details</summary>
Motivation: 现有方法对抗体互补决定区（CDR）结构多样性分类覆盖有限，且难以融入蛋白质基础模型。

Method: 引入多模态抗体环分词器Igloo，用对比学习目标训练，将具有相似主链二面角的环在潜在空间中映射得更近。

Result: Igloo在检索相似H3环结构上比现有方法高5.9%；IglooLM在预测重链变体结合亲和力上表现良好；IglooALM采样的抗体环序列多样、结构更一致。

Conclusion: 引入抗体环的多模态分词器有利于编码抗体环多样性、改进蛋白质基础模型和抗体CDR设计。

Abstract: The complementarity-determining regions of antibodies are loop structures
that are key to their interactions with antigens, and of high importance to the
design of novel biologics. Since the 1980s, categorizing the diversity of CDR
structures into canonical clusters has enabled the identification of key
structural motifs of antibodies. However, existing approaches have limited
coverage and cannot be readily incorporated into protein foundation models.
Here we introduce ImmunoGlobulin LOOp Tokenizer, Igloo, a multimodal antibody
loop tokenizer that encodes backbone dihedral angles and sequence. Igloo is
trained using a contrastive learning objective to map loops with similar
backbone dihedral angles closer together in latent space. Igloo can efficiently
retrieve the closest matching loop structures from a structural antibody
database, outperforming existing methods on identifying similar H3 loops by
5.9\%. Igloo assigns tokens to all loops, addressing the limited coverage issue
of canonical clusters, while retaining the ability to recover canonical loop
conformations. To demonstrate the versatility of Igloo tokens, we show that
they can be incorporated into protein language models with IglooLM and
IglooALM. On predicting binding affinity of heavy chain variants, IglooLM
outperforms the base protein language model on 8 out of 10 antibody-antigen
targets. Additionally, it is on par with existing state-of-the-art
sequence-based and multimodal protein language models, performing comparably to
models with $7\times$ more parameters. IglooALM samples antibody loops which
are diverse in sequence and more consistent in structure than state-of-the-art
antibody inverse folding models. Igloo demonstrates the benefit of introducing
multimodal tokens for antibody loops for encoding the diverse landscape of
antibody loops, improving protein foundation models, and for antibody CDR
design.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [127] [Two-Stage Swarm Intelligence Ensemble Deep Transfer Learning (SI-EDTL) for Vehicle Detection Using Unmanned Aerial Vehicles](https://arxiv.org/abs/2509.08026)
*Zeinab Ghasemi Darehnaei,Mohammad Shokouhifar,Hossein Yazdanjouei,S. M. J. Rastegar Fatemi*

Main category: cs.CV

TL;DR: 本文提出SI - EDTL模型用于无人机图像多车辆检测，结合预训练模型与分类器，用加权平均聚合，通过鲸鱼优化算法优化超参数，在数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提出新的模型以更有效地在无人机图像中检测多种车辆。

Method: 构建两阶段的SI - EDTL模型，结合三种预训练Faster R - CNN特征提取器模型和五种转移分类器得到15个基础学习器，用加权平均聚合，用鲸鱼优化算法优化超参数，在MATLAB R2020b中并行处理。

Result: SI - EDTL模型在AU - AIR无人机数据集上的表现优于现有方法。

Conclusion: SI - EDTL模型是一种有效的无人机图像多车辆检测模型。

Abstract: This paper introduces SI-EDTL, a two-stage swarm intelligence ensemble deep
transfer learning model for detecting multiple vehicles in UAV images. It
combines three pre-trained Faster R-CNN feature extractor models (InceptionV3,
ResNet50, GoogLeNet) with five transfer classifiers (KNN, SVM, MLP, C4.5,
Na\"ive Bayes), resulting in 15 different base learners. These are aggregated
via weighted averaging to classify regions as Car, Van, Truck, Bus, or
background. Hyperparameters are optimized with the whale optimization algorithm
to balance accuracy, precision, and recall. Implemented in MATLAB R2020b with
parallel processing, SI-EDTL outperforms existing methods on the AU-AIR UAV
dataset.

</details>


### [128] [APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction](https://arxiv.org/abs/2509.08104)
*Sasan Sharifipour,Constantino Álvarez Casado,Mohammad Sabokrou,Miguel Bordallo López*

Main category: cs.CV

TL;DR: 提出自适应概率匹配损失（APML）用于点云预测任务，避免常用损失函数的问题，实现接近二次的运行时间，在多个架构上表现良好。


<details>
  <summary>Details</summary>
Motivation: 常用点云预测损失函数存在多对一对应、不可微操作等问题，Earth Mover Distance计算复杂度高，需要更好的损失函数。

Method: 提出APML，基于Sinkhorn迭代对温度缩放相似度矩阵进行全可微近似一对一匹配，解析计算温度以消除手动调参。

Result: APML运行时间接近二次，避免不可微操作，集成到多个架构上收敛更快，空间分布更好，定量性能提升。

Conclusion: APML是一种有效的点云预测损失函数，可提高模型性能，代码开源。

Abstract: Training deep learning models for point cloud prediction tasks such as shape
completion and generation depends critically on loss functions that measure
discrepancies between predicted and ground-truth point sets. Commonly used
functions such as Chamfer Distance (CD), HyperCD, and InfoCD rely on
nearest-neighbor assignments, which often induce many-to-one correspondences,
leading to point congestion in dense regions and poor coverage in sparse
regions. These losses also involve non-differentiable operations due to index
selection, which may affect gradient-based optimization. Earth Mover Distance
(EMD) enforces one-to-one correspondences and captures structural similarity
more effectively, but its cubic computational complexity limits its practical
use. We propose the Adaptive Probabilistic Matching Loss (APML), a fully
differentiable approximation of one-to-one matching that leverages Sinkhorn
iterations on a temperature-scaled similarity matrix derived from pairwise
distances. We analytically compute the temperature to guarantee a minimum
assignment probability, eliminating manual tuning. APML achieves near-quadratic
runtime, comparable to Chamfer-based losses, and avoids non-differentiable
operations. When integrated into state-of-the-art architectures (PoinTr, PCN,
FoldingNet) on ShapeNet benchmarks and on a spatiotemporal Transformer (CSI2PC)
that generates 3D human point clouds from WiFi CSI measurements, APM loss
yields faster convergence, superior spatial distribution, especially in
low-density regions, and improved or on-par quantitative performance without
additional hyperparameter search. The code is available at:
https://github.com/apm-loss/apml.

</details>


### [129] [Retrieval-Augmented VLMs for Multimodal Melanoma Diagnosis](https://arxiv.org/abs/2509.08338)
*Jihyun Moon,Charmgil Hong*

Main category: cs.CV

TL;DR: 提出检索增强的VLM框架用于恶性黑色素瘤诊断，无需微调且提升性能。


<details>
  <summary>Details</summary>
Motivation: CNN在皮肤镜图像分析中忽略临床元数据且需大量预处理，通用领域训练的VLM难以捕捉临床特异性，需要更好方法进行恶性黑色素瘤准确早期诊断。

Method: 提出检索增强的VLM框架，将语义相似患者病例纳入诊断提示。

Result: 无需微调实现明智预测，显著提高分类准确率并纠正错误，优于传统基线。

Conclusion: 检索增强提示为临床决策支持提供了可靠策略。

Abstract: Accurate and early diagnosis of malignant melanoma is critical for improving
patient outcomes. While convolutional neural networks (CNNs) have shown promise
in dermoscopic image analysis, they often neglect clinical metadata and require
extensive preprocessing. Vision-language models (VLMs) offer a multimodal
alternative but struggle to capture clinical specificity when trained on
general-domain data. To address this, we propose a retrieval-augmented VLM
framework that incorporates semantically similar patient cases into the
diagnostic prompt. Our method enables informed predictions without fine-tuning
and significantly improves classification accuracy and error correction over
conventional baselines. These results demonstrate that retrieval-augmented
prompting provides a robust strategy for clinical decision support.

</details>


### [130] [Semantic Causality-Aware Vision-Based 3D Occupancy Prediction](https://arxiv.org/abs/2509.08388)
*Dubing Chen,Huan Zheng,Yucheng Zhou,Xianfei Li,Wenlong Liao,Tao He,Pai Peng,Jianbing Shen*

Main category: cs.CV

TL;DR: 本文提出新的因果损失函数，实现2D到3D转换管道的端到端监督，提出语义因果感知的2D到3D转换方法，在Occ3D基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的3D语义占用预测方法依赖模块化管道，模块独立优化或使用预配置输入，会导致级联错误。

Method: 设计新的因果损失函数，基于2D到3D语义因果原理调节梯度流；提出语义因果感知的2D到3D转换方法，包含通道分组提升、可学习相机偏移和归一化卷积三个组件。

Result: 在Occ3D基准测试中达到了当前最优性能，对相机扰动有显著的鲁棒性，提高了2D到3D的语义一致性。

Conclusion: 新的因果损失函数和语义因果感知的2D到3D转换方法有效，能解决现有方法的局限性。

Abstract: Vision-based 3D semantic occupancy prediction is a critical task in 3D vision
that integrates volumetric 3D reconstruction with semantic understanding.
Existing methods, however, often rely on modular pipelines. These modules are
typically optimized independently or use pre-configured inputs, leading to
cascading errors. In this paper, we address this limitation by designing a
novel causal loss that enables holistic, end-to-end supervision of the modular
2D-to-3D transformation pipeline. Grounded in the principle of 2D-to-3D
semantic causality, this loss regulates the gradient flow from 3D voxel
representations back to the 2D features. Consequently, it renders the entire
pipeline differentiable, unifying the learning process and making previously
non-trainable components fully learnable. Building on this principle, we
propose the Semantic Causality-Aware 2D-to-3D Transformation, which comprises
three components guided by our causal loss: Channel-Grouped Lifting for
adaptive semantic mapping, Learnable Camera Offsets for enhanced robustness
against camera perturbations, and Normalized Convolution for effective feature
propagation. Extensive experiments demonstrate that our method achieves
state-of-the-art performance on the Occ3D benchmark, demonstrating significant
robustness to camera perturbations and improved 2D-to-3D semantic consistency.

</details>


### [131] [Sparse BEV Fusion with Self-View Consistency for Multi-View Detection and Tracking](https://arxiv.org/abs/2509.08421)
*Keisuke Toida,Taigo Sakai,Naoki Kato,Kazutoyo Yokota,Takeshi Nakamura,Kazuhiro Hotta*

Main category: cs.CV

TL;DR: 提出SCFusion框架解决多视图多目标跟踪中BEV投影问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多视图多目标跟踪方法在跨相机保持目标身份一致性有挑战，BEV投影存在特征失真和密度不均问题，影响检测和跟踪精度。

Method: 提出SCFusion框架，结合稀疏变换、密度感知加权和多视图一致性损失三种技术改善多视图特征融合。

Result: SCFusion在WildTrack和MultiviewX数据集上取得最优性能，IDF1得分95.9%，MODP为89.2%，超越基线方法TrackTacular。

Conclusion: SCFusion有效缓解传统BEV投影的局限，为多视图目标检测和跟踪提供了强大准确的解决方案。

Abstract: Multi-View Multi-Object Tracking (MVMOT) is essential for applications such
as surveillance, autonomous driving, and sports analytics. However, maintaining
consistent object identities across multiple cameras remains challenging due to
viewpoint changes, lighting variations, and occlusions, which often lead to
tracking errors.Recent methods project features from multiple cameras into a
unified Bird's-Eye-View (BEV) space to improve robustness against occlusion.
However, this projection introduces feature distortion and non-uniform density
caused by variations in object scale with distance. These issues degrade the
quality of the fused representation and reduce detection and tracking
accuracy.To address these problems, we propose SCFusion, a framework that
combines three techniques to improve multi-view feature integration. First, it
applies a sparse transformation to avoid unnatural interpolation during
projection. Next, it performs density-aware weighting to adaptively fuse
features based on spatial confidence and camera distance. Finally, it
introduces a multi-view consistency loss that encourages each camera to learn
discriminative features independently before fusion.Experiments show that
SCFusion achieves state-of-the-art performance, reaching an IDF1 score of 95.9%
on WildTrack and a MODP of 89.2% on MultiviewX, outperforming the baseline
method TrackTacular. These results demonstrate that SCFusion effectively
mitigates the limitations of conventional BEV projection and provides a robust
and accurate solution for multi-view object detection and tracking.

</details>


### [132] [Spherical Brownian Bridge Diffusion Models for Conditional Cortical Thickness Forecasting](https://arxiv.org/abs/2509.08442)
*Ivan Stoyanov,Fabian Bongratz,Christian Wachinger*

Main category: cs.CV

TL;DR: 文章引入球形布朗桥扩散模型（SBDM）预测个体化高分辨率皮质厚度轨迹，实验显示其能显著降低预测误差，还可生成事实和反事实轨迹。


<details>
  <summary>Details</summary>
Motivation: 准确预测个体化高分辨率皮质厚度轨迹对检测皮质变化、了解神经退行性过程和制定干预策略至关重要，但由于大脑皮层复杂的非欧几何和多模态数据整合需求，这一任务具有挑战性。

Method: 引入SBDM，提出双向条件布朗桥扩散过程在注册皮质表面的顶点级别预测皮质厚度轨迹，还提出条件球形U - Net（CoS - UNet）进行去噪。

Result: 基于ADNI和OASIS纵向数据集的实验表明，与先前方法相比，SBDM显著降低了预测误差，且能生成个体的事实和反事实皮质厚度轨迹。

Conclusion: SBDM为预测皮质厚度轨迹提供了有效方法，也为探索皮质发育的假设情景提供了新框架。

Abstract: Accurate forecasting of individualized, high-resolution cortical thickness
(CTh) trajectories is essential for detecting subtle cortical changes,
providing invaluable insights into neurodegenerative processes and facilitating
earlier and more precise intervention strategies. However, CTh forecasting is a
challenging task due to the intricate non-Euclidean geometry of the cerebral
cortex and the need to integrate multi-modal data for subject-specific
predictions. To address these challenges, we introduce the Spherical Brownian
Bridge Diffusion Model (SBDM). Specifically, we propose a bidirectional
conditional Brownian bridge diffusion process to forecast CTh trajectories at
the vertex level of registered cortical surfaces. Our technical contribution
includes a new denoising model, the conditional spherical U-Net (CoS-UNet),
which combines spherical convolutions and dense cross-attention to integrate
cortical surfaces and tabular conditions seamlessly. Compared to previous
approaches, SBDM achieves significantly reduced prediction errors, as
demonstrated by our experiments based on longitudinal datasets from the ADNI
and OASIS. Additionally, we demonstrate SBDM's ability to generate individual
factual and counterfactual CTh trajectories, offering a novel framework for
exploring hypothetical scenarios of cortical development.

</details>


### [133] [Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs](https://arxiv.org/abs/2509.08016)
*Hyungjin Chung,Hyelin Nam,Jiyeon Kim,Hyojun Go,Byeongjun Park,Junho Kim,Joonseok Lee,Seongsu Ha,Byung-Hoon Kim*

Main category: cs.CV

TL;DR: 提出视频并行缩放（VPS）方法，可在不增加上下文窗口的情况下扩展模型感知带宽，提升视频大语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决视频大语言模型增加输入帧数带来的计算成本高和长上下文性能下降问题。

Method: 运行多个并行推理流，每个处理视频帧的不相交子集，聚合输出概率。

Result: 在多个基准测试中显著提升性能，比其他并行方法扩展性更好，与其他解码策略互补。

Conclusion: VPS为提升视频大语言模型的时间推理能力提供了内存高效且稳健的框架。

Abstract: Video Large Language Models (VideoLLMs) face a critical bottleneck:
increasing the number of input frames to capture fine-grained temporal detail
leads to prohibitive computational costs and performance degradation from long
context lengths. We introduce Video Parallel Scaling (VPS), an inference-time
method that expands a model's perceptual bandwidth without increasing its
context window. VPS operates by running multiple parallel inference streams,
each processing a unique, disjoint subset of the video's frames. By aggregating
the output probabilities from these complementary streams, VPS integrates a
richer set of visual information than is possible with a single pass. We
theoretically show that this approach effectively contracts the Chinchilla
scaling law by leveraging uncorrelated visual evidence, thereby improving
performance without additional training. Extensive experiments across various
model architectures and scales (2B-32B) on benchmarks such as Video-MME and
EventHallusion demonstrate that VPS consistently and significantly improves
performance. It scales more favorably than other parallel alternatives (e.g.
Self-consistency) and is complementary to other decoding strategies, offering a
memory-efficient and robust framework for enhancing the temporal reasoning
capabilities of VideoLLMs.

</details>


### [134] [Prompt-Driven Image Analysis with Multimodal Generative AI: Detection, Segmentation, Inpainting, and Interpretation](https://arxiv.org/abs/2509.08489)
*Kaleem Ahmad*

Main category: cs.CV

TL;DR: 本文展示了一个统一管道的案例研究，该管道将多种图像分析步骤整合为单一工作流，系统端到端运行，给出降低脆弱性的集成选择，分析运行结果并提供实施建议。


<details>
  <summary>Details</summary>
Motivation: 实现将单一自然语言指令转化为定位、分割、编辑和描述等多步骤的图像分析，构建统一工作流。

Method: 结合开放词汇检测、可提示分割、文本条件修复和视觉语言描述为单一工作流，采用降低脆弱性的集成选择。

Result: 在单字提示段中，检测和分割超90%情况产生可用掩码，准确率超85%；高端GPU上，修复占总运行时间60 - 75%。

Conclusion: 提供了在单一提示后组装现代视觉和多模态模型的透明可靠模式，以及提升可靠性的操作实践。

Abstract: Prompt-driven image analysis converts a single natural-language instruction
into multiple steps: locate, segment, edit, and describe. We present a
practical case study of a unified pipeline that combines open-vocabulary
detection, promptable segmentation, text-conditioned inpainting, and
vision-language description into a single workflow. The system works end to end
from a single prompt, retains intermediate artifacts for transparent debugging
(such as detections, masks, overlays, edited images, and before and after
composites), and provides the same functionality through an interactive UI and
a scriptable CLI for consistent, repeatable runs. We highlight integration
choices that reduce brittleness, including threshold adjustments, mask
inspection with light morphology, and resource-aware defaults. In a small,
single-word prompt segment, detection and segmentation produced usable masks in
over 90% of cases with an accuracy above 85% based on our criteria. On a
high-end GPU, inpainting makes up 60 to 75% of total runtime under typical
guidance and sampling settings, which highlights the need for careful tuning.
The study offers implementation-guided advice on thresholds, mask tightness,
and diffusion parameters, and details version pinning, artifact logging, and
seed control to support replay. Our contribution is a transparent, reliable
pattern for assembling modern vision and multimodal models behind a single
prompt, with clear guardrails and operational practices that improve
reliability in object replacement, scene augmentation, and removal.

</details>


### [135] [A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models](https://arxiv.org/abs/2509.08490)
*Edwine Nabahirwa,Wei Song,Minghua Zhang,Yi Fang,Zhou Ni*

Main category: cs.CV

TL;DR: 本文系统分类水下目标检测挑战，分析方法演进，探索大视觉语言模型潜力并给出案例，指出当前方法不足、合成数据待完善及模型实时应用需研究。


<details>
  <summary>Details</summary>
Motivation: 水下目标检测对海洋应用至关重要，但面临诸多挑战影响性能，现有方法难以完全应对水下环境复杂性。

Method: 将水下目标检测挑战分为五类，分析从传统到现代方法的演进，探索大视觉语言模型在水下目标检测中的应用，给出合成数据集生成和微调模型的案例。

Result: 识别出三点关键见解：当前方法不足以应对动态水下环境挑战；大视觉语言模型合成数据有潜力但需完善；大视觉语言模型前景好但实时应用研究不足。

Conclusion: 水下目标检测现有方法存在不足，大视觉语言模型有应用潜力，但需在合成数据和实时应用方面开展更多研究。

Abstract: Underwater object detection (UOD) is vital to diverse marine applications,
including oceanographic research, underwater robotics, and marine conservation.
However, UOD faces numerous challenges that compromise its performance. Over
the years, various methods have been proposed to address these issues, but they
often fail to fully capture the complexities of underwater environments. This
review systematically categorizes UOD challenges into five key areas: Image
quality degradation, target-related issues, data-related challenges,
computational and processing constraints, and limitations in detection
methodologies. To address these challenges, we analyze the progression from
traditional image processing and object detection techniques to modern
approaches. Additionally, we explore the potential of large vision-language
models (LVLMs) in UOD, leveraging their multi-modal capabilities demonstrated
in other domains. We also present case studies, including synthetic dataset
generation using DALL-E 3 and fine-tuning Florence-2 LVLM for UOD. This review
identifies three key insights: (i) Current UOD methods are insufficient to
fully address challenges like image degradation and small object detection in
dynamic underwater environments. (ii) Synthetic data generation using LVLMs
shows potential for augmenting datasets but requires further refinement to
ensure realism and applicability. (iii) LVLMs hold significant promise for UOD,
but their real-time application remains under-explored, requiring further
research on optimization techniques.

</details>


### [136] [MCTED: A Machine-Learning-Ready Dataset for Digital Elevation Model Generation From Mars Imagery](https://arxiv.org/abs/2509.08027)
*Rafał Osadnik,Pablo Gómez,Eleni Bohacek,Rickbir Bahia*

Main category: cs.CV

TL;DR: 本文提出用于火星数字高程模型预测的新数据集MCTED，介绍其生成、处理等情况，训练小模型对比性能并开源。


<details>
  <summary>Details</summary>
Motivation: 为火星数字高程模型预测任务提供适用于机器学习应用的数据集。

Method: 设计综合流程处理高分辨率火星正射影像和DEM对生成数据集，开发工具解决原始数据问题，划分训练和验证集，提供数据集统计信息，训练小U - Net架构并与DepthAnythingV2对比。

Result: 在高程预测任务中，在该数据集上训练的小架构模型性能优于DepthAnythingV2的零样本性能。

Conclusion: 新数据集MCTED有效，为火星数字高程模型预测提供有力支持，且数据集和代码开源利于后续研究。

Abstract: This work presents a new dataset for the Martian digital elevation model
prediction task, ready for machine learning applications called MCTED. The
dataset has been generated using a comprehensive pipeline designed to process
high-resolution Mars orthoimage and DEM pairs from Day et al., yielding a
dataset consisting of 80,898 data samples. The source images are data gathered
by the Mars Reconnaissance Orbiter using the CTX instrument, providing a very
diverse and comprehensive coverage of the Martian surface. Given the complexity
of the processing pipelines used in large-scale DEMs, there are often artefacts
and missing data points in the original data, for which we developed tools to
solve or mitigate their impact. We divide the processed samples into training
and validation splits, ensuring samples in both splits cover no mutual areas to
avoid data leakage. Every sample in the dataset is represented by the optical
image patch, DEM patch, and two mask patches, indicating values that were
originally missing or were altered by us. This allows future users of the
dataset to handle altered elevation regions as they please. We provide
statistical insights of the generated dataset, including the spatial
distribution of samples, the distributions of elevation values, slopes and
more. Finally, we train a small U-Net architecture on the MCTED dataset and
compare its performance to a monocular depth estimation foundation model,
DepthAnythingV2, on the task of elevation prediction. We find that even a very
small architecture trained on this dataset specifically, beats a zero-shot
performance of a depth estimation foundation model like DepthAnythingV2. We
make the dataset and code used for its generation completely open source in
public repositories.

</details>


### [137] [MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models](https://arxiv.org/abs/2509.08538)
*Garry Yang,Zizhe Chen,Man Hon Wong,Haoyu Lei,Yongqiang Chen,Zhenguo Li,Kaiwen Zhou,James Cheng*

Main category: cs.CV

TL;DR: 提出MESH基准评估大视频模型（LVMs）幻觉问题，发现LVMs处理细节和多动作时易产生幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有视频幻觉基准依赖人工分类，忽略人类感知过程，需系统评估LVMs幻觉。

Method: 引入MESH基准，采用问答框架，含二元和多选格式，结合靶实例和陷阱实例，自底向上评估。

Result: 评估显示LVMs识别基本对象和特征出色，但处理长视频细节和多动作时易产生幻觉。

Conclusion: MESH是识别视频幻觉的有效且全面的方法。

Abstract: Large Video Models (LVMs) build on the semantic capabilities of Large
Language Models (LLMs) and vision modules by integrating temporal information
to better understand dynamic video content. Despite their progress, LVMs are
prone to hallucinations-producing inaccurate or irrelevant descriptions.
Current benchmarks for video hallucination depend heavily on manual
categorization of video content, neglecting the perception-based processes
through which humans naturally interpret videos. We introduce MESH, a benchmark
designed to evaluate hallucinations in LVMs systematically. MESH uses a
Question-Answering framework with binary and multi-choice formats incorporating
target and trap instances. It follows a bottom-up approach, evaluating basic
objects, coarse-to-fine subject features, and subject-action pairs, aligning
with human video understanding. We demonstrate that MESH offers an effective
and comprehensive approach for identifying hallucinations in videos. Our
evaluations show that while LVMs excel at recognizing basic objects and
features, their susceptibility to hallucinations increases markedly when
handling fine details or aligning multiple actions involving various subjects
in longer videos.

</details>


### [138] [RepViT-CXR: A Channel Replication Strategy for Vision Transformers in Chest X-ray Tuberculosis and Pneumonia Classification](https://arxiv.org/abs/2509.08234)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: 提出RepViT - CXR通道复制策略让ViT适配单通道CXR图像，在三个数据集上评估超现有方法，为TB和肺炎检测建立新的最优水平。


<details>
  <summary>Details</summary>
Motivation: 多数ViT架构在自然图像上预训练且需三通道输入，而CXR扫描是灰度图，存在适配问题。

Method: 提出RepViT - CXR通道复制策略，将单通道CXR图像转换为ViT兼容格式。

Result: 在三个基准数据集上评估，各项指标超越现有方法，如在TB - CXR数据集上准确率和AUC达99.9%。

Conclusion: 简单有效的通道复制策略使ViT能在灰度医学成像任务发挥表征能力，RepViT - CXR有潜力用于实际临床筛查系统。

Abstract: Chest X-ray (CXR) imaging remains one of the most widely used diagnostic
tools for detecting pulmonary diseases such as tuberculosis (TB) and pneumonia.
Recent advances in deep learning, particularly Vision Transformers (ViTs), have
shown strong potential for automated medical image analysis. However, most ViT
architectures are pretrained on natural images and require three-channel
inputs, while CXR scans are inherently grayscale. To address this gap, we
propose RepViT-CXR, a channel replication strategy that adapts single-channel
CXR images into a ViT-compatible format without introducing additional
information loss. We evaluate RepViT-CXR on three benchmark datasets. On the
TB-CXR dataset,our method achieved an accuracy of 99.9% and an AUC of 99.9%,
surpassing prior state-of-the-art methods such as Topo-CXR (99.3% accuracy,
99.8% AUC). For the Pediatric Pneumonia dataset, RepViT-CXR obtained 99.0%
accuracy, with 99.2% recall, 99.3% precision, and an AUC of 99.0%,
outperforming strong baselines including DCNN and VGG16. On the Shenzhen TB
dataset, our approach achieved 91.1% accuracy and an AUC of 91.2%, marking a
performance improvement over previously reported CNN-based methods. These
results demonstrate that a simple yet effective channel replication strategy
allows ViTs to fully leverage their representational power on grayscale medical
imaging tasks. RepViT-CXR establishes a new state of the art for TB and
pneumonia detection from chest X-rays, showing strong potential for deployment
in real-world clinical screening systems.

</details>


### [139] [UOPSL: Unpaired OCT Predilection Sites Learning for Fundus Image Diagnosis Augmentation](https://arxiv.org/abs/2509.08624)
*Zhihao Zhao,Yinzheng Zhao,Junjie Yang,Xiangtong Yao,Quanmin Liang,Daniel Zapp,Kai Huang,Nassir Navab,M. Ali Nasseri*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Significant advancements in AI-driven multimodal medical image diagnosis have
led to substantial improvements in ophthalmic disease identification in recent
years. However, acquiring paired multimodal ophthalmic images remains
prohibitively expensive. While fundus photography is simple and cost-effective,
the limited availability of OCT data and inherent modality imbalance hinder
further progress. Conventional approaches that rely solely on fundus or textual
features often fail to capture fine-grained spatial information, as each
imaging modality provides distinct cues about lesion predilection sites. In
this study, we propose a novel unpaired multimodal framework \UOPSL that
utilizes extensive OCT-derived spatial priors to dynamically identify
predilection sites, enhancing fundus image-based disease recognition. Our
approach bridges unpaired fundus and OCTs via extended disease text
descriptions. Initially, we employ contrastive learning on a large corpus of
unpaired OCT and fundus images while simultaneously learning the predilection
sites matrix in the OCT latent space. Through extensive optimization, this
matrix captures lesion localization patterns within the OCT feature space.
During the fine-tuning or inference phase of the downstream classification task
based solely on fundus images, where paired OCT data is unavailable, we
eliminate OCT input and utilize the predilection sites matrix to assist in
fundus image classification learning. Extensive experiments conducted on 9
diverse datasets across 28 critical categories demonstrate that our framework
outperforms existing benchmarks.

</details>


### [140] [Skeleton-based sign language recognition using a dual-stream spatio-temporal dynamic graph convolutional network](https://arxiv.org/abs/2509.08661)
*Liangjin Liu,Haoyang Zheng,Pei Zhou*

Main category: cs.CV

TL;DR: 提出Dual - SignLanguageNet (DSLNet) 用于孤立手语识别，在多个数据集上达SOTA且参数更少。


<details>
  <summary>Details</summary>
Motivation: 现有单参考帧方法难以解决孤立手语识别中形态相似但语义不同手势的几何模糊问题。

Method: 采用双参考、双流架构，用腕为中心的参考帧分析形状，脸为中心的参考帧建模轨迹，分别用拓扑感知图卷积和基于芬斯勒几何的编码器处理，通过几何驱动的最优传输融合机制整合。

Result: 在WLASL - 100、WLASL - 300和LSA64数据集上分别达到93.70%、89.97%和99.79%的准确率。

Conclusion: DSLNet在孤立手语识别中表现优异，能有效解决现有问题，且参数更少。

Abstract: Isolated Sign Language Recognition (ISLR) is challenged by gestures that are
morphologically similar yet semantically distinct, a problem rooted in the
complex interplay between hand shape and motion trajectory. Existing methods,
often relying on a single reference frame, struggle to resolve this geometric
ambiguity. This paper introduces Dual-SignLanguageNet (DSLNet), a
dual-reference, dual-stream architecture that decouples and models gesture
morphology and trajectory in separate, complementary coordinate systems. Our
approach utilizes a wrist-centric frame for view-invariant shape analysis and a
facial-centric frame for context-aware trajectory modeling. These streams are
processed by specialized networks-a topology-aware graph convolution for shape
and a Finsler geometry-based encoder for trajectory-and are integrated via a
geometry-driven optimal transport fusion mechanism. DSLNet sets a new
state-of-the-art, achieving 93.70%, 89.97% and 99.79% accuracy on the
challenging WLASL-100, WLASL-300 and LSA64 datasets, respectively, with
significantly fewer parameters than competing models.

</details>


### [141] [LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations](https://arxiv.org/abs/2509.08422)
*Payal Varshney,Adriano Lucieri,Christoph Balada,Sheraz Ahmed,Andreas Dengel*

Main category: cs.CV

TL;DR: 提出LD - ViCE框架解释基于视频的AI模型行为，降低计算成本，在三个数据集上实验有效，优于现有方法，迈向安全关键领域可信AI部署。


<details>
  <summary>Details</summary>
Motivation: 现有视频AI系统决策解释技术存在时间连贯性有限、鲁棒性不足、缺乏因果洞察等问题，反事实解释方法语义保真度和实用性低。

Method: 引入Latent Diffusion for Video Counterfactual Explanations (LD - ViCE)框架，在潜在空间操作降低成本，通过额外细化步骤生成反事实。

Result: 在三个数据集上实验有效，R2分数最高提升68%，推理时间减半，生成语义有意义且时间连贯的解释。

Conclusion: LD - ViCE是安全关键领域可信AI部署的重要一步。

Abstract: Video-based AI systems are increasingly adopted in safety-critical domains
such as autonomous driving and healthcare. However, interpreting their
decisions remains challenging due to the inherent spatiotemporal complexity of
video data and the opacity of deep learning models. Existing explanation
techniques often suffer from limited temporal coherence, insufficient
robustness, and a lack of actionable causal insights. Current counterfactual
explanation methods typically do not incorporate guidance from the target
model, reducing semantic fidelity and practical utility. We introduce Latent
Diffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework
designed to explain the behavior of video-based AI models. Compared to previous
approaches, LD-ViCE reduces the computational costs of generating explanations
by operating in latent space using a state-of-the-art diffusion model, while
producing realistic and interpretable counterfactuals through an additional
refinement step. Our experiments demonstrate the effectiveness of LD-ViCE
across three diverse video datasets, including EchoNet-Dynamic (cardiac
ultrasound), FERV39k (facial expression), and Something-Something V2 (action
recognition). LD-ViCE outperforms a recent state-of-the-art method, achieving
an increase in R2 score of up to 68% while reducing inference time by half.
Qualitative analysis confirms that LD-ViCE generates semantically meaningful
and temporally coherent explanations, offering valuable insights into the
target model behavior. LD-ViCE represents a valuable step toward the
trustworthy deployment of AI in safety-critical domains.

</details>


### [142] [Implicit Shape-Prior for Few-Shot Assisted 3D Segmentation](https://arxiv.org/abs/2509.08580)
*Mathilde Monvoisin,Louise Piecuch,Blanche Texier,Cédric Hémon,Anaïs Barateau,Jérémie Huet,Antoine Nordez,Anne-Sophie Boureau,Jean-Claude Nunes,Diana Mateus*

Main category: cs.CV

TL;DR: 本文旨在减少复杂3D分割任务中医务人员手动工作量，引入隐式形状先验和自动选片框架，实验验证其在两个医疗场景有效。


<details>
  <summary>Details</summary>
Motivation: 当前复杂3D分割任务无法完全自动化，医务人员手动工作量大，如放疗规划和肌肉疾病诊断。

Method: 引入隐式形状先验对稀疏切片手动注释进行多器官体积分割，设计自动选择最具信息切片的框架。

Result: 方法在脑癌患者危及器官辅助分割和肌肉萎缩症患者新数据库创建加速两个医疗用例中有效。

Conclusion: 所提出的方法能有效减轻复杂3D分割任务中医务人员手动工作量。

Abstract: The objective of this paper is to significantly reduce the manual workload
required from medical professionals in complex 3D segmentation tasks that
cannot be yet fully automated. For instance, in radiotherapy planning, organs
at risk must be accurately identified in computed tomography (CT) or magnetic
resonance imaging (MRI) scans to ensure they are spared from harmful radiation.
Similarly, diagnosing age-related degenerative diseases such as sarcopenia,
which involve progressive muscle volume loss and strength, is commonly based on
muscular mass measurements often obtained from manual segmentation of medical
volumes. To alleviate the manual-segmentation burden, this paper introduces an
implicit shape prior to segment volumes from sparse slice manual annotations
generalized to the multi-organ case, along with a simple framework for
automatically selecting the most informative slices to guide and minimize the
next interactions. The experimental validation shows the method's effectiveness
on two medical use cases: assisted segmentation in the context of at risks
organs for brain cancer patients, and acceleration of the creation of a new
database with unseen muscle shapes for patients with sarcopenia.

</details>


### [143] [An End-to-End Deep Learning Framework for Arsenicosis Diagnosis Using Mobile-Captured Skin Images](https://arxiv.org/abs/2509.08780)
*Asif Newaz,Asif Ur Rahman Adib,Rajit Sahil,Mashfique Mehzad*

Main category: cs.CV

TL;DR: 本文提出用手机拍摄皮肤图像进行砷中毒诊断的端到端框架，Transformer模型表现更佳，框架可推广，能辅助农村及资源匮乏地区早期诊断。


<details>
  <summary>Details</summary>
Motivation: 砷中毒早期皮肤症状常漏诊，自动图像诊断方案可支持早期检测和及时干预。

Method: 构建含20类超1.1万张图像的数据集，对比CNN和Transformer模型进行砷中毒检测，用LIME和Grad - CAM实现模型可解释性，通过网络工具展示部署可行性。

Result: Transformer模型优于CNN，Swin Transformer准确率达86%，可视化证明模型关注病变区域，框架在外部验证样本表现好。

Conclusion: 该框架有潜力实现非侵入、可及且可解释的砷中毒诊断，可作为农村和资源有限地区实用诊断辅助工具。

Abstract: Background: Arsenicosis is a serious public health concern in South and
Southeast Asia, primarily caused by long-term consumption of
arsenic-contaminated water. Its early cutaneous manifestations are clinically
significant but often underdiagnosed, particularly in rural areas with limited
access to dermatologists. Automated, image-based diagnostic solutions can
support early detection and timely interventions.
  Methods: In this study, we propose an end-to-end framework for arsenicosis
diagnosis using mobile phone-captured skin images. A dataset comprising 20
classes and over 11000 images of arsenic-induced and other dermatological
conditions was curated. Multiple deep learning architectures, including
convolutional neural networks (CNNs) and Transformer-based models, were
benchmarked for arsenicosis detection. Model interpretability was integrated
via LIME and Grad-CAM, while deployment feasibility was demonstrated through a
web-based diagnostic tool.
  Results: Transformer-based models significantly outperformed CNNs, with the
Swin Transformer achieving the best results (86\\% accuracy). LIME and Grad-CAM
visualizations confirmed that the models attended to lesion-relevant regions,
increasing clinical transparency and aiding in error analysis. The framework
also demonstrated strong performance on external validation samples, confirming
its ability to generalize beyond the curated dataset.
  Conclusion: The proposed framework demonstrates the potential of deep
learning for non-invasive, accessible, and explainable diagnosis of arsenicosis
from mobile-acquired images. By enabling reliable image-based screening, it can
serve as a practical diagnostic aid in rural and resource-limited communities,
where access to dermatologists is scarce, thereby supporting early detection
and timely intervention.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [144] [A Linear Pricing Mechanism for Load Management in Day-Ahead Retail Energy Markets](https://arxiv.org/abs/2509.08166)
*Phillippe K. Phanivong,Duncan S. Callaway*

Main category: eess.SY

TL;DR: 提出一种新的日前零售电价线性定价机制以解决拥塞管理问题。


<details>
  <summary>Details</summary>
Motivation: 监管机构和公用事业公司探索小时级零售电价，用户部署分布式能源资源和智能能源管理系统，其最优控制的负荷会给配电系统运营商带来拥塞管理问题。

Method: 设计一种用于成本优化负荷的价格信号控制的线性价格并进行广播。

Result: 可塑造用户负荷曲线，实现拥塞管理，无需双向通信或用户投标程序。

Conclusion: 新的线性定价机制能在引导用户避免过度消费的同时，让用户在有利于系统性能的时段用电，解决拥塞管理问题。

Abstract: Regulators and utilities have been exploring hourly retail electricity
pricing, with several existing programs providing day-ahead hourly pricing
schedules. At the same time, customers are deploying distributed energy
resources and smart energy management systems that have significant flexibility
and can optimally follow price signals. In aggregate, these optimally
controlled loads can create congestion management issues for distribution
system operators (DSOs). In this paper, we describe a new linear pricing
mechanism for day-ahead retail electricity pricing that provides a signal for
customers to follow to mitigate over-consumption while still consuming energy
at hours that are preferential for system performance. We show that by
broadcasting a linear price designed for price-signal control of
cost-optimizing loads, we can shape customer load profiles to provide
congestion management without the need for bi-directional communication or
customer bidding programs.

</details>


### [145] [Game-Theoretic Resilience Framework for Cyber-Physical Microgrids using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.08310)
*S Krishna Niketh,Sagar Babu Mitikiri,V Vignesh,Vedantham Lakshmi Srinivas,Mayukha Pal*

Main category: eess.SY

TL;DR: 本文提出博弈论框架评估和增强微电网弹性，用定量指标和AHP构建统一收益矩阵，以MDP形式呈现，经多案例研究和理论分析，在IEEE 33总线系统验证有效，比静态方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统对网络物理基础设施依赖增加，面临网络攻击风险，需有效弹性策略。

Method: 提出博弈论框架，结合定量弹性指标，用AHP构建收益矩阵，形式化为MDP，进行案例研究和理论分析。

Result: 在增强的IEEE 33总线配电系统测试，自适应和战略防御比静态方法有18.7% - 2.1%的显著提升。

Conclusion: 所提框架能有效提升微电网网络物理弹性。

Abstract: The increasing reliance on cyber physical infrastructure in modern power
systems has amplified the risk of targeted cyber attacks, necessitating robust
and adaptive resilience strategies. This paper presents a mathematically
rigorous game theoretic framework to evaluate and enhance microgrid resilience
using a combination of quantitative resilience metrics Load Served Ratio LSR,
Critical Load Resilience CLR, Topological Survivability Score TSS, and DER
Resilience Score DRS. These are integrated into a unified payoff matrix using
the Analytic Hierarchy Process AHP to assess attack defense interactions. The
framework is formalized as a finite horizon Markov Decision Process MDP with
formal convergence guarantees and computational complexity bounds. Three case
studies are developed 1. static attacks analyzed via Nash equilibrium, 2.
severe attacks incorporating high impact strategies, and 3. adaptive attacks
using Stackelberg games, regret matching, softmax heuristics, and Multi Agent Q
Learning. Rigorous theoretical analysis provides convergence proofs with
explicit rates , PAC learning sample complexity bounds, and computational
complexity analysis. The framework is tested on an enhanced IEEE 33bus
distribution system with DERs and control switches, demonstrating the
effectiveness of adaptive and strategic defenses in improving cyber physical
resilience with statistically significant improvements of 18.7% 2.1% over
static approaches.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [146] [LALM-Eval: An Open-Source Toolkit for Holistic Evaluation of Large Audio Language Models](https://arxiv.org/abs/2509.08031)
*Sidharth Surapaneni,Hoang Nguyen,Jash Mehta,Aman Tiwari,Oluwanifemi Bamgbose,Akshay Kalkunte,Sai Rajeswar,Sathwik Tejaswi Madhusudhan*

Main category: cs.SD

TL;DR: 当前大音频语言模型（LALMs）评估存在挑战，本文介绍评估框架LALM - Eval，能加速评估、提供标准化协议和新评估类别，揭示模型差距并推动其发展。


<details>
  <summary>Details</summary>
Motivation: 现有评估工具效率低，限制了对LALMs的公平比较和系统评估，存在处理慢、提示不一致和任务覆盖窄等问题。

Method: 引入LALM - Eval框架，通过优化批处理和并行执行加速，提供标准化提示协议和灵活配置，新增LLM - Adaptive Diarization和Spoken Language Reasoning评估类别。

Result: LALM - Eval比现有工具提速达127%，通过380 + 任务评估揭示当前LALMs在时间理解和复杂口语推理任务上有显著差距，音频基准指令模态缺乏标准化会导致性能差异达9.5个绝对点。

Conclusion: LALM - Eval提供实用评估工具，揭示模型局限，推动LALMs的系统发展。

Abstract: Large Audio Language Models (LALMs) are rapidly advancing, but evaluating
them remains challenging due to inefficient toolkits that limit fair comparison
and systematic assessment. Current frameworks suffer from three critical
issues: slow processing that bottlenecks large-scale studies, inconsistent
prompting that hurts reproducibility, and narrow task coverage that misses
important audio reasoning capabilities. We introduce LALM-Eval, an efficient
and comprehensive evaluation framework for LALMs. Our system achieves a speedup
of up to 127% over existing toolkits through optimized batch processing and
parallel execution, enabling large-scale evaluations previously impractical. We
provide standardized prompting protocols and flexible configurations for fair
model comparison across diverse scenarios. Additionally, we introduce two new
evaluation categories: LLM-Adaptive Diarization for temporal audio
understanding and Spoken Language Reasoning for complex audio-based cognitive
tasks. Through evaluation across 380+ tasks, we reveal significant gaps in
current LALMs, particularly in temporal understanding and complex spoken
language reasoning tasks. Our findings also highlight a lack of standardization
in instruction modality existent across audio benchmarks, which can lead up
performance differences up to 9.5 absolute points on the challenging complex
instruction following downstream tasks. LALM-Eval provides both practical
evaluation tools and insights into model limitations, advancing systematic LALM
development.

</details>


### [147] [Segment Transformer: AI-Generated Music Detection via Music Structural Analysis](https://arxiv.org/abs/2509.08283)
*Yumin Kim,Seonghyeon Go*

Main category: cs.SD

TL;DR: 文章针对AI生成音乐版权问题及难以判别AI音乐的挑战，通过分析音乐结构模式，集成预训练模型和开发分段变压器，利用数据集实验，提高了AI生成音乐检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 音频和音乐生成技术发展引发版权问题，AI生成音乐的所有权和作者身份不明确，且难以清晰判别是否为AI生成，因此要提高AI生成音乐检测的准确性。

Method: 分析音乐片段的结构模式，集成包括自监督学习模型或音频效果编码器等多种预训练模型到基于变压器的框架中提取短音频音乐特征；开发分段变压器将长音频分段并学习段间关系。

Result: 使用FakeMusicCaps和SONICS数据集，在短音频和全音频检测实验中都取得了高准确性。

Conclusion: 将分段级音乐特征融入长程时间分析能有效提高AI生成音乐检测系统的性能和鲁棒性。

Abstract: Audio and music generation systems have been remarkably developed in the
music information retrieval (MIR) research field. The advancement of these
technologies raises copyright concerns, as ownership and authorship of
AI-generated music (AIGM) remain unclear. Also, it can be difficult to
determine whether a piece was generated by AI or composed by humans clearly. To
address these challenges, we aim to improve the accuracy of AIGM detection by
analyzing the structural patterns of music segments. Specifically, to extract
musical features from short audio clips, we integrated various pre-trained
models, including self-supervised learning (SSL) models or an audio effect
encoder, each within our suggested transformer-based framework. Furthermore,
for long audio, we developed a segment transformer that divides music into
segments and learns inter-segment relationships. We used the FakeMusicCaps and
SONICS datasets, achieving high accuracy in both the short-audio and full-audio
detection experiments. These findings suggest that integrating segment-level
musical features into long-range temporal analysis can effectively enhance both
the performance and robustness of AIGM detection systems.

</details>


### [148] [Behind the Scenes: Mechanistic Interpretability of LoRA-adapted Whisper for Speech Emotion Recognition](https://arxiv.org/abs/2509.08454)
*Yujian Ma,Jinqiu Sang,Ruizhe Li*

Main category: cs.SD

TL;DR: 对Whisper编码器中用于语音情感识别的LoRA进行系统机制可解释性研究，揭示两个关键机制，为设计高效可解释的大语音模型适配策略提供见解。


<details>
  <summary>Details</summary>
Motivation: 大型预训练语音模型资源高效适配有挑战，LoRA在语音任务中的潜在机制尚不清楚，需进行研究。

Method: 使用一系列分析工具，包括层贡献探测、logit-lens检查、通过奇异值分解（SVD）和中心核对齐（CKA）进行表征相似性分析。

Result: 揭示了两个关键机制，即延迟专业化过程和LoRA矩阵之间的前向对齐、后向差异化动态。

Conclusion: 研究结果阐明了LoRA如何重塑编码器层次结构，为设计高效可解释的大语音模型适配策略提供了实证见解和更深入的机制理解。

Abstract: Large pre-trained speech models such as Whisper offer strong generalization
but pose significant challenges for resource-efficient adaptation. Low-Rank
Adaptation (LoRA) has become a popular parameter-efficient fine-tuning method,
yet its underlying mechanisms in speech tasks remain poorly understood. In this
work, we conduct the first systematic mechanistic interpretability study of
LoRA within the Whisper encoder for speech emotion recognition (SER). Using a
suite of analytical tools, including layer contribution probing, logit-lens
inspection, and representational similarity via singular value decomposition
(SVD) and centered kernel alignment (CKA), we reveal two key mechanisms: a
delayed specialization process that preserves general features in early layers
before consolidating task-specific information, and a forward alignment,
backward differentiation dynamic between LoRA's matrices. Our findings clarify
how LoRA reshapes encoder hierarchies, providing both empirical insights and a
deeper mechanistic understanding for designing efficient and interpretable
adaptation strategies in large speech models.

</details>


### [149] [Explainability of CNN Based Classification Models for Acoustic Signal](https://arxiv.org/abs/2509.08717)
*Zubair Faruqui,Mackenzie S. McIntire,Rahul Dubey,Jay McEntee*

Main category: cs.SD

TL;DR: 本文研究北美某鸟类鸣叫，用CNN分类准确率达94.8%，结合多种XAI技术解释模型预测，强调结合XAI技术重要性。


<details>
  <summary>Details</summary>
Motivation: XAI在生物声学领域应用相对不足，需研究其在此领域应用。

Method: 将音频转换为频谱图训练CNN进行分类，应用模型无关（LIME、SHAP）和模型特定（DeepLIFT、Grad - CAM）的XAI技术解释模型预测。

Result: 不同XAI技术产生不同但互补的解释，综合解释能为模型决策提供更完整可解释的见解。

Conclusion: 结合XAI技术对提高声学信号分析的信任和互操作性很重要，且在不同领域任务有更广泛适用性。

Abstract: Explainable Artificial Intelligence (XAI) has emerged as a critical tool for
interpreting the predictions of complex deep learning models. While XAI has
been increasingly applied in various domains within acoustics, its use in
bioacoustics, which involves analyzing audio signals from living organisms,
remains relatively underexplored. In this paper, we investigate the
vocalizations of a bird species with strong geographic variation throughout its
range in North America. Audio recordings were converted into spectrogram images
and used to train a deep Convolutional Neural Network (CNN) for classification,
achieving an accuracy of 94.8\%. To interpret the model's predictions, we
applied both model-agnostic (LIME, SHAP) and model-specific (DeepLIFT,
Grad-CAM) XAI techniques. These techniques produced different but complementary
explanations, and when their explanations were considered together, they
provided more complete and interpretable insights into the model's
decision-making. This work highlights the importance of using a combination of
XAI techniques to improve trust and interoperability, not only in broader
acoustics signal analysis but also argues for broader applicability in
different domain specific tasks.

</details>


### [150] [PianoVAM: A Multimodal Piano Performance Dataset](https://arxiv.org/abs/2509.08800)
*Yonghyun Kim,Junhyung Park,Joonhyung Bae,Kirak Kim,Taegyun Kwon,Alexander Lerch,Juhan Nam*

Main category: cs.SD

TL;DR: 本文介绍了PianoVAM钢琴演奏数据集，包含多模态数据，讲述数据收集挑战、标注方法，给出基准测试结果并探讨潜在应用。


<details>
  <summary>Details</summary>
Motivation: 音乐表演的多模态特性促使音乐信息检索领域对音频外的数据感兴趣，需要全面的钢琴演奏数据集。

Method: 用Disklavier钢琴录制数据，用预训练手部姿态估计模型和半自动化指法标注算法提取手部地标和指法标签。

Result: 给出了音频和视听钢琴转录的基准测试结果。

Conclusion: PianoVAM数据集有潜力推动多模态音乐信息检索研究和应用。

Abstract: The multimodal nature of music performance has driven increasing interest in
data beyond the audio domain within the music information retrieval (MIR)
community. This paper introduces PianoVAM, a comprehensive piano performance
dataset that includes videos, audio, MIDI, hand landmarks, fingering labels,
and rich metadata. The dataset was recorded using a Disklavier piano, capturing
audio and MIDI from amateur pianists during their daily practice sessions,
alongside synchronized top-view videos in realistic and varied performance
conditions. Hand landmarks and fingering labels were extracted using a
pretrained hand pose estimation model and a semi-automated fingering annotation
algorithm. We discuss the challenges encountered during data collection and the
alignment process across different modalities. Additionally, we describe our
fingering annotation method based on hand landmarks extracted from videos.
Finally, we present benchmarking results for both audio-only and audio-visual
piano transcription using the PianoVAM dataset and discuss additional potential
applications.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [151] [DLGE: Dual Local-Global Encoding for Generalizable Cross-BCI-Paradigm](https://arxiv.org/abs/2509.07991)
*Jingyuan Wang,Junhua Li*

Main category: q-bio.NC

TL;DR: 提出Dual Local - Global Encoder (DLGE)模型用于跨脑机接口（BCI）范式分类，评估结果良好，为通用BCI解码模型发展奠基。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型难用一个模型解码多BCI范式，因存在如不同通道配置和任务相关表征差异等障碍。

Method: 提出DLGE模型，采用基于脑区划分和填充策略标准化EEG通道配置；局部编码器基于时频信息学习各脑区跨范式共享特征，全局编码器聚合形成特定范式特征表征。

Result: 模型无需重新训练和调参就能处理多种BCI范式，平均宏精度、召回率和F1分数分别达60.16%、59.88%和59.56%。

Conclusion: 初步尝试开发跨BCI范式分类通用模型，避免为每个范式重新训练或开发，为有效简单的跨范式解码模型发展及便携式通用BCI解码设备设计提供方向。

Abstract: Deep learning models have been frequently used to decode a single
brain-computer interface (BCI) paradigm based on electroencephalography (EEG).
It is challenging to decode multiple BCI paradigms using one model due to
diverse barriers, such as different channel configurations and disparate
task-related representations. In this study, we propose Dual Local-Global
Encoder (DLGE), enabling the classification across different BCI paradigms. To
address the heterogeneity in EEG channel configurations across paradigms, we
employ an anatomically inspired brain-region partitioning and padding strategy
to standardize EEG channel configuration. In the proposed model, the local
encoder is designed to learn shared features across BCI paradigms within each
brain region based on time-frequency information, which integrates temporal
attention on individual channels with spatial attention among channels for each
brain region. These shared features are subsequently aggregated in the global
encoder to form respective paradigm-specific feature representations. Three BCI
paradigms (motor imagery, resting state, and driving fatigue) were used to
evaluate the proposed model. The results demonstrate that our model is capable
of processing diverse BCI paradigms without retraining and retuning, achieving
average macro precision, recall, and F1-score of 60.16\%, 59.88\%, and 59.56\%,
respectively. We made an initial attempt to develop a general model for
cross-BCI-paradigm classification, avoiding retraining or redevelopment for
each paradigm. This study paves the way for the development of an effective but
simple model for cross-BCI-paradigm decoding, which might benefit the design of
portable devices for universal BCI decoding.

</details>


### [152] [The Computational Foundations of Collective Intelligence](https://arxiv.org/abs/2509.07999)
*Charlie Pilgrim,Joe Morford,Elizabeth Warren,Mélisande Aellen,Christopher Krupenye,Richard P Mann,Dora Biro*

Main category: q-bio.NC

TL;DR: 本文探讨集体解决问题优于个体的原因，指出集体资源优势可带来集体智慧形式，框架有可测试预测，通过案例展示集体解决问题的优势。


<details>
  <summary>Details</summary>
Motivation: 探究集体在解决某些问题时优于个体的原因。

Method: 构建理论框架并结合动物导航和决策的案例研究。

Result: 表明集体资源优势直接导致多种集体智慧形式，框架能产生可测试预测，集体解决问题更有效且策略不同。

Conclusion: 集体凭借其计算资源优势，能比个体更有效地解决问题，且采用不同策略。

Abstract: Why do collectives outperform individuals when solving some problems?
Fundamentally, collectives have greater computational resources with more
sensory information, more memory, more processing capacity, and more ways to
act. While greater resources present opportunities, there are also challenges
in coordination and cooperation inherent in collectives with distributed,
modular structures. Despite these challenges, we show how collective resource
advantages lead directly to well-known forms of collective intelligence
including the wisdom of the crowd, collective sensing, division of labour, and
cultural learning. Our framework also generates testable predictions about
collective capabilities in distributed reasoning and context-dependent
behavioural switching. Through case studies of animal navigation and
decision-making, we demonstrate how collectives leverage their computational
resources to solve problems not only more effectively than individuals, but by
using qualitatively different problem-solving strategies.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [153] [Real-Time Obstacle Avoidance for a Mobile Robot Using CNN-Based Sensor Fusion](https://arxiv.org/abs/2509.08095)
*Lamiaa H. Zain,Raafat E. Shalaby*

Main category: cs.RO

TL;DR: 研究训练三个端到端CNN用于移动机器人实时避障，评估各模型性能，NetConEmb表现最佳。


<details>
  <summary>Details</summary>
Motivation: 让移动机器人在复杂未知环境有效运行，解决避障问题。

Method: 训练三个端到端CNN，用Intel RealSense D415相机采集图像，离线评估后部署到差动驱动移动机器人进行实时避障。

Result: NetConEmb离线评估性能最佳，MedAE低；NetEmb参数少、收敛快，RMSE与NetConEmb接近；实时导航中NetConEmb在已知和未知环境成功率100%，NetEmb和NetGated仅在已知环境成功。

Conclusion: NetConEmb在移动机器人实时避障中表现出良好的性能和鲁棒性。

Abstract: Obstacle avoidance is a critical component of the navigation stack required
for mobile robots to operate effectively in complex and unknown environments.
In this research, three end-to-end Convolutional Neural Networks (CNNs) were
trained and evaluated offline and deployed on a differential-drive mobile robot
for real-time obstacle avoidance to generate low-level steering commands from
synchronized color and depth images acquired by an Intel RealSense D415 RGB-D
camera in diverse environments. Offline evaluation showed that the NetConEmb
model achieved the best performance with a notably low MedAE of $0.58 \times
10^{-3}$ rad/s. In comparison, the lighter NetEmb architecture adopted in this
study, which reduces the number of trainable parameters by approximately 25\%
and converges faster, produced comparable results with an RMSE of $21.68 \times
10^{-3}$ rad/s, close to the $21.42 \times 10^{-3}$ rad/s obtained by
NetConEmb. Real-time navigation further confirmed NetConEmb's robustness,
achieving a 100\% success rate in both known and unknown environments, while
NetEmb and NetGated succeeded only in navigating the known environment.

</details>


### [154] [Risk-Bounded Multi-Agent Visual Navigation via Dynamic Budget Allocation](https://arxiv.org/abs/2509.08157)
*Viraj Parimi,Brian C. Williams*

Main category: cs.RO

TL;DR: 提出RB - CBS方法解决多智能体安全导航问题，实验表明其在复杂环境中性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统规划方法依赖预定义距离度量，安全强化学习在多智能体、目标条件场景中表现不佳，现有图修剪方法过于保守，限制任务效率。

Method: 提出RB - CBS，一种CBS的扩展方法，动态分配和调整用户指定的风险边界Δ，为每个智能体分配局部风险预算δ。

Result: 实验结果表明，迭代风险分配框架在复杂环境中表现更优，使多个智能体在用户指定的Δ内找到无碰撞路径。

Conclusion: RB - CBS方法能在保证整体安全约束的同时，实现更高效的导航。

Abstract: Safe navigation is essential for autonomous systems operating in hazardous
environments, especially when multiple agents must coordinate using just visual
inputs over extended time horizons. Traditional planning methods excel at
solving long-horizon tasks but rely on predefined distance metrics, while safe
Reinforcement Learning (RL) can learn complex behaviors using high-dimensional
inputs yet struggles with multi-agent, goal-conditioned scenarios. Recent work
combined these paradigms by leveraging goal-conditioned RL (GCRL) to build an
intermediate graph from replay buffer states, pruning unsafe edges, and using
Conflict-Based Search (CBS) for multi-agent path planning. Although effective,
this graph-pruning approach can be overly conservative, limiting mission
efficiency by precluding missions that must traverse high-risk regions. To
address this limitation, we propose RB-CBS, a novel extension to CBS that
dynamically allocates and adjusts user-specified risk bound ($\Delta$) across
agents to flexibly trade off safety and speed. Our improved planner ensures
that each agent receives a local risk budget ($\delta$) enabling more efficient
navigation while still respecting overall safety constraints. Experimental
results demonstrate that this iterative risk-allocation framework yields
superior performance in complex environments, allowing multiple agents to find
collision-free paths within the user-specified $\Delta$.

</details>


### [155] [Zero-Shot Metric Depth Estimation via Monocular Visual-Inertial Rescaling for Autonomous Aerial Navigation](https://arxiv.org/abs/2509.08159)
*Steven Yang,Xiaoyu Tian,Kshitij Goel,Wennie Tabib*

Main category: cs.RO

TL;DR: 提出从单目RGB图像和IMU预测深度的方法，对比不同策略，最佳方法在四旋翼上实现避障。


<details>
  <summary>Details</summary>
Motivation: 现有自主飞行避障方法依赖重型传感器或数据密集的微调，需更轻量方法。

Method: 提出轻量级零样本重缩放策略，通过视觉惯性导航系统创建的稀疏3D特征图从相对深度估计获得度量深度。

Result: 在不同模拟环境中对比策略准确性，最佳方法在计算受限四旋翼上以15Hz获得机载度量深度估计，结合规划器实现避障。

Conclusion: 所提轻量级方法可有效从单目图像和IMU获得度量深度并实现碰撞避免。

Abstract: This paper presents a methodology to predict metric depth from monocular RGB
images and an inertial measurement unit (IMU). To enable collision avoidance
during autonomous flight, prior works either leverage heavy sensors (e.g.,
LiDARs or stereo cameras) or data-intensive and domain-specific fine-tuning of
monocular metric depth estimation methods. In contrast, we propose several
lightweight zero-shot rescaling strategies to obtain metric depth from relative
depth estimates via the sparse 3D feature map created using a visual-inertial
navigation system. These strategies are compared for their accuracy in diverse
simulation environments. The best performing approach, which leverages
monotonic spline fitting, is deployed in the real-world on a
compute-constrained quadrotor. We obtain on-board metric depth estimates at 15
Hz and demonstrate successful collision avoidance after integrating the
proposed method with a motion primitives-based planner.

</details>


### [156] [Diffusion-Guided Multi-Arm Motion Planning](https://arxiv.org/abs/2509.08160)
*Viraj Parimi,Brian C. Williams*

Main category: cs.RO

TL;DR: 提出扩散引导多臂规划器DG - MAP，提升基于学习模型的可扩展性并减少对大量数据集的依赖，经评估有效。


<details>
  <summary>Details</summary>
Motivation: 当前多臂运动规划方法因状态空间指数增长和依赖大量训练数据，在可扩展性上存在困难。

Method: 受多智能体路径规划（MAPF）启发，训练两个条件扩散模型，一个生成单臂可行轨迹，另一个建模双臂动力学用于碰撞解决，将这些模型集成到MAPF启发的结构化分解中。

Result: 在不同团队规模下与其他基于学习的方法对比评估，证明了该方法的有效性和实际适用性。

Conclusion: 提出的DG - MAP规划器能有效提升基于学习模型的可扩展性，减少对大量多臂数据集的依赖。

Abstract: Multi-arm motion planning is fundamental for enabling arms to complete
complex long-horizon tasks in shared spaces efficiently but current methods
struggle with scalability due to exponential state-space growth and reliance on
large training datasets for learned models. Inspired by Multi-Agent Path
Finding (MAPF), which decomposes planning into single-agent problems coupled
with collision resolution, we propose a novel diffusion-guided multi-arm
planner (DG-MAP) that enhances scalability of learning-based models while
reducing their reliance on massive multi-arm datasets. Recognizing that
collisions are primarily pairwise, we train two conditional diffusion models,
one to generate feasible single-arm trajectories, and a second, to model the
dual-arm dynamics required for effective pairwise collision resolution. By
integrating these specialized generative models within a MAPF-inspired
structured decomposition, our planner efficiently scales to larger number of
arms. Evaluations against alternative learning-based methods across various
team sizes demonstrate our method's effectiveness and practical applicability.
Project website can be found at https://diff-mapf-mers.csail.mit.edu

</details>


### [157] [Quadrotor Navigation using Reinforcement Learning with Privileged Information](https://arxiv.org/abs/2509.08177)
*Jonathan Lee,Abhishek Rathod,Kshitij Goel,John Stecklein,Wennie Tabib*

Main category: cs.RO

TL;DR: 提出基于强化学习的四旋翼导航方法，在含大障碍物场景表现优，模拟和实际飞行验证成功。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法在大障碍物阻挡目标位置时表现不佳，需新方法应对。

Method: 利用高效可微模拟、新损失函数和飞行时间（ToA）图等特权信息及偏航对齐损失，引导机器人绕过大型障碍物。

Result: 在逼真模拟环境中成功率达86%，比基线策略高34%；在室外杂乱环境20次飞行，覆盖589米无碰撞，最高速度4m/s。

Conclusion: 所提四旋翼导航方法有效可行，能在含大障碍物复杂环境实现导航。

Abstract: This paper presents a reinforcement learning-based quadrotor navigation
method that leverages efficient differentiable simulation, novel loss
functions, and privileged information to navigate around large obstacles. Prior
learning-based methods perform well in scenes that exhibit narrow obstacles,
but struggle when the goal location is blocked by large walls or terrain. In
contrast, the proposed method utilizes time-of-arrival (ToA) maps as privileged
information and a yaw alignment loss to guide the robot around large obstacles.
The policy is evaluated in photo-realistic simulation environments containing
large obstacles, sharp corners, and dead-ends. Our approach achieves an 86%
success rate and outperforms baseline strategies by 34%. We deploy the policy
onboard a custom quadrotor in outdoor cluttered environments both during the
day and night. The policy is validated across 20 flights, covering 589 meters
without collisions at speeds up to 4 m/s.

</details>


### [158] [Symmetry-Guided Multi-Agent Inverse Reinforcement Learnin](https://arxiv.org/abs/2509.08257)
*Yongkai Tian,Yirong Qi,Xin Yu,Wenjun Wu,Jie Luo*

Main category: cs.RO

TL;DR: 现有逆强化学习依赖大量专家演示，本文受多智能体系统对称性启发，提出集成对称性的通用框架，提升样本效率，实验证明有效且实用。


<details>
  <summary>Details</summary>
Motivation: 手动设计奖励函数不准确导致策略失败，逆强化学习依赖大量专家演示，收集成本高，提升多智能体逆强化学习样本效率是关键挑战。

Method: 理论证明利用对称性可恢复更准确奖励函数，提出将对称性集成到现有多智能体对抗逆强化学习算法的通用框架。

Result: 多个挑战性任务实验证明框架有效，物理多机器人系统验证方法实用。

Conclusion: 提出的集成对称性框架能显著提升多智能体逆强化学习的样本效率，具有实际应用价值。

Abstract: In robotic systems, the performance of reinforcement learning depends on the
rationality of predefined reward functions. However, manually designed reward
functions often lead to policy failures due to inaccuracies. Inverse
Reinforcement Learning (IRL) addresses this problem by inferring implicit
reward functions from expert demonstrations. Nevertheless, existing methods
rely heavily on large amounts of expert demonstrations to accurately recover
the reward function. The high cost of collecting expert demonstrations in
robotic applications, particularly in multi-robot systems, severely hinders the
practical deployment of IRL. Consequently, improving sample efficiency has
emerged as a critical challenge in multi-agent inverse reinforcement learning
(MIRL). Inspired by the symmetry inherent in multi-agent systems, this work
theoretically demonstrates that leveraging symmetry enables the recovery of
more accurate reward functions. Building upon this insight, we propose a
universal framework that integrates symmetry into existing multi-agent
adversarial IRL algorithms, thereby significantly enhancing sample efficiency.
Experimental results from multiple challenging tasks have demonstrated the
effectiveness of this framework. Further validation in physical multi-robot
systems has shown the practicality of our method.

</details>


### [159] [Grasp Like Humans: Learning Generalizable Multi-Fingered Grasping from Human Proprioceptive Sensorimotor Integration](https://arxiv.org/abs/2509.08354)
*Ce Guo,Xieyuanli Chen,Zhiwen Zeng,Zirui Guo,Yihong Li,Haoran Xiao,Dewen Hu,Huimin Lu*

Main category: cs.RO

TL;DR: 提出基于模仿学习的手套介导触觉 - 运动感知预测框架用于抓握技能从人到机器人的转移，并通过通用抓握任务验证有效性。


<details>
  <summary>Details</summary>
Motivation: 触觉和运动感知对人类灵巧操作很重要，但为机器人手建立从感官反馈到运动动作的直接映射具有挑战性。

Method: 1. 集成数据手套在关节层面捕获触觉和运动数据；2. 基于极坐标图结构建立多模态输入的统一表示；3. 引入触觉 - 运动时空图网络提取时空特征预测关节状态，通过力 - 位置混合映射得到最终命令。

Result: 该框架有效性通过包括可变形物体的通用抓握任务得到验证。

Conclusion: 所提出的框架可实现抓握技能从人类直观自然操作到机器人执行的转移。

Abstract: Tactile and kinesthetic perceptions are crucial for human dexterous
manipulation, enabling reliable grasping of objects via proprioceptive
sensorimotor integration. For robotic hands, even though acquiring such tactile
and kinesthetic feedback is feasible, establishing a direct mapping from this
sensory feedback to motor actions remains challenging. In this paper, we
propose a novel glove-mediated tactile-kinematic perception-prediction
framework for grasp skill transfer from human intuitive and natural operation
to robotic execution based on imitation learning, and its effectiveness is
validated through generalized grasping tasks, including those involving
deformable objects. Firstly, we integrate a data glove to capture tactile and
kinesthetic data at the joint level. The glove is adaptable for both human and
robotic hands, allowing data collection from natural human hand demonstrations
across different scenarios. It ensures consistency in the raw data format,
enabling evaluation of grasping for both human and robotic hands. Secondly, we
establish a unified representation of multi-modal inputs based on graph
structures with polar coordinates. We explicitly integrate the morphological
differences into the designed representation, enhancing the compatibility
across different demonstrators and robotic hands. Furthermore, we introduce the
Tactile-Kinesthetic Spatio-Temporal Graph Networks (TK-STGN), which leverage
multidimensional subgraph convolutions and attention-based LSTM layers to
extract spatio-temporal features from graph inputs to predict node-based states
for each hand joint. These predictions are then mapped to final commands
through a force-position hybrid mapping.

</details>


### [160] [FMT$^{x}$: An Efficient and Asymptotically Optimal Extension of the Fast Marching Tree for Dynamic Replanning](https://arxiv.org/abs/2509.08521)
*Soheil Espahbodini Nia*

Main category: cs.RO

TL;DR: 本文提出FMT^{x}算法，用于动态环境路径规划，能高效重规划，实验显示优于RRT^{x}。


<details>
  <summary>Details</summary>
Motivation: 动态环境路径规划是机器人领域挑战，现有FMT*算法单遍设计无法实时调整路径，全重规划计算成本高。

Method: 改进FMT*邻居选择规则，维护成本优先队列，应用选择性更新条件识别并重新评估潜在次优路径节点。

Result: FMT^{x}被证明环境变化后能恢复渐近最优解，实验显示其优于RRT^{x}，响应更快、计算开销更低。

Conclusion: FMT^{x}为不可预测环境中机器人实时导航提供更有效解决方案。

Abstract: Path planning in dynamic environments remains a core challenge in robotics,
especially as autonomous systems are deployed in unpredictable spaces such as
warehouses and public roads. While algorithms like Fast Marching Tree
(FMT$^{*}$) offer asymptotically optimal solutions in static settings, their
single-pass design prevents path revisions which are essential for real-time
adaptation. On the other hand, full replanning is often too computationally
expensive. This paper introduces FMT$^{x}$, an extension of the Fast Marching
Tree algorithm that enables efficient and consistent replanning in dynamic
environments. We revisit the neighbor selection rule of FMT$^{*}$ and
demonstrate that a minimal change overcomes its single-pass limitation,
enabling the algorithm to update cost-to-come values upon discovering better
connections without sacrificing asymptotic optimality or computational
efficiency. By maintaining a cost-ordered priority queue and applying a
selective update condition that uses an expanding neighbor to identify and
trigger the re-evaluation of any node with a potentially suboptimal path,
FMT$^{x}$ ensures that suboptimal routes are efficiently repaired as the
environment evolves. This targeted strategy preserves the inherent efficiency
of FMT$^{*}$ while enabling robust adaptation to changes in obstacle
configuration. FMT$^{x}$ is proven to recover an asymptotically optimal
solution after environmental changes. Experimental results demonstrate that
FMT$^{x}$ outperforms the influential replanner RRT$^{x}$, reacting more
swiftly to dynamic events with lower computational overhead and thus offering a
more effective solution for real-time robotic navigation in unpredictable
worlds.

</details>


### [161] [TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals](https://arxiv.org/abs/2509.08699)
*Stefan Podgorski,Sourav Garg,Mehdi Hosseinzadeh,Lachlan Mares,Feras Dayoub,Ian Reid*

Main category: cs.RO

TL;DR: 提出仅用RGB图像的对象级拓扑度量导航管道，可实现零样本、长距离机器人导航，在模拟和真实环境中验证有效性且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统视觉导航依赖3D地图或预训练控制器，计算成本高且难以在不同环境泛化。

Method: 将全局拓扑路径规划与局部度量轨迹控制相结合，用单目深度和可通行性估计预测局部轨迹，引入自动切换机制，使用基础模型确保开放集适用性。

Result: 在模拟和真实环境测试中证明方法有效，优于现有方法。

Conclusion: 提出的方法为开放集环境视觉导航提供更灵活有效的解决方案，代码已开源。

Abstract: Visual navigation in robotics traditionally relies on globally-consistent 3D
maps or learned controllers, which can be computationally expensive and
difficult to generalize across diverse environments. In this work, we present a
novel RGB-only, object-level topometric navigation pipeline that enables
zero-shot, long-horizon robot navigation without requiring 3D maps or
pre-trained controllers. Our approach integrates global topological path
planning with local metric trajectory control, allowing the robot to navigate
towards object-level sub-goals while avoiding obstacles. We address key
limitations of previous methods by continuously predicting local trajectory
using monocular depth and traversability estimation, and incorporating an
auto-switching mechanism that falls back to a baseline controller when
necessary. The system operates using foundational models, ensuring open-set
applicability without the need for domain-specific fine-tuning. We demonstrate
the effectiveness of our method in both simulated environments and real-world
tests, highlighting its robustness and deployability. Our approach outperforms
existing state-of-the-art methods, offering a more adaptable and effective
solution for visual navigation in open-set environments. The source code is
made publicly available: https://github.com/podgorki/TANGO.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [162] [Membrane: A Cryptographic Access Control System for Data Lakes](https://arxiv.org/abs/2509.08740)
*Sam Kumar,Samyukta Yagati,Conor Power,David E. Culler,Raluca Ada Popa*

Main category: cs.CR

TL;DR: 提出Membrane系统，结合静态加密和SQL感知加密，实现数据湖数据依赖访问控制视图，有初始开销但后续查询开销低。


<details>
  <summary>Details</summary>
Motivation: 解决黑客可能绕过数据湖存储访问控制获取敏感数据的问题。

Method: 结合静态加密与SQL感知加密，利用块密码开发新的SQL感知加密协议。

Result: Membrane仅在交互会话开始时因解密视图产生开销，延迟首个查询结果约20倍，后续查询处理明文数据，分摊开销低。

Conclusion: Membrane系统能在不限制数据分析查询的前提下，对数据湖实施数据依赖访问控制。

Abstract: Organizations use data lakes to store and analyze sensitive data. But hackers
may compromise data lake storage to bypass access controls and access sensitive
data. To address this, we propose Membrane, a system that (1) cryptographically
enforces data-dependent access control views over a data lake, (2) without
restricting the analytical queries data scientists can run. We observe that
data lakes, unlike DBMSes, disaggregate computation and storage into separate
trust domains, making at-rest encryption sufficient to defend against remote
attackers targeting data lake storage, even when running analytical queries in
plaintext. This leads to a new system design for Membrane that combines
encryption at rest with SQL-aware encryption. Using block ciphers, a fast
symmetric-key primitive with hardware acceleration in CPUs, we develop a new
SQL-aware encryption protocol well-suited to at-rest encryption. Membrane adds
overhead only at the start of an interactive session due to decrypting views,
delaying the first query result by up to $\approx 20\times$; subsequent queries
process decrypted data in plaintext, resulting in low amortized overhead.

</details>


### [163] [DSFL: A Dual-Server Byzantine-Resilient Federated Learning Framework via Group-Based Secure Aggregation](https://arxiv.org/abs/2509.08449)
*Charuka Herath,Yogachandran Rahulamathavan,Varuna De Silva,Sangarapillai Lambotharan*

Main category: cs.CR

TL;DR: 提出DSFL框架解决现有联邦学习协议问题，评估显示其性能优于基线且轻量级。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习协议难以抵御拜占庭参与者、在非IID数据下保持模型效用及对边缘设备保持轻量级，先前工作有局限性。

Method: 提出DSFL框架，采用基于组的安全聚合方法，有双服务器安全聚合协议、基于信用的过滤机制和动态奖惩系统。

Result: 在MNIST、CIFAR - 10和CIFAR - 100上评估，在IID和非IID设置下，面对最多30%拜占庭参与者时，始终优于现有基线，如CIFAR - 10准确率达97.15%，CIFAR - 100达68.60%，且轻量级。

Conclusion: DSFL能有效解决现有联邦学习协议的问题，性能良好且轻量级。

Abstract: Federated Learning (FL) enables decentralized model training without sharing
raw data, offering strong privacy guarantees. However, existing FL protocols
struggle to defend against Byzantine participants, maintain model utility under
non-independent and identically distributed (non-IID) data, and remain
lightweight for edge devices. Prior work either assumes trusted hardware, uses
expensive cryptographic tools, or fails to address privacy and robustness
simultaneously. We propose DSFL, a Dual-Server Byzantine-Resilient Federated
Learning framework that addresses these limitations using a group-based secure
aggregation approach. Unlike LSFL, which assumes non-colluding semi-honest
servers, DSFL removes this dependency by revealing a key vulnerability: privacy
leakage through client-server collusion. DSFL introduces three key innovations:
(1) a dual-server secure aggregation protocol that protects updates without
encryption or key exchange, (2) a group-wise credit-based filtering mechanism
to isolate Byzantine clients based on deviation scores, and (3) a dynamic
reward-penalty system for enforcing fair participation. DSFL is evaluated on
MNIST, CIFAR-10, and CIFAR-100 under up to 30 percent Byzantine participants in
both IID and non-IID settings. It consistently outperforms existing baselines,
including LSFL, homomorphic encryption methods, and differential privacy
approaches. For example, DSFL achieves 97.15 percent accuracy on CIFAR-10 and
68.60 percent on CIFAR-100, while FedAvg drops to 9.39 percent under similar
threats. DSFL remains lightweight, requiring only 55.9 ms runtime and 1088 KB
communication per round.

</details>


### [164] [Unlocking Reproducibility: Automating re-Build Process for Open-Source Software](https://arxiv.org/abs/2509.08204)
*Behnaz Hassanshahi,Trong Nhan Mai,Benjamin Selwyn Smith,Nicholas Allen*

Main category: cs.CR

TL;DR: 本文介绍对Macaron扩展以自动化从源代码重建Maven工件，提高开源供应链安全性和透明度。


<details>
  <summary>Details</summary>
Motivation: Maven Central中二进制文件和源代码分离带来安全风险，从源代码重建软件工件可提高供应链安全但存在挑战。

Method: 对Macaron扩展，在源代码检测上有更好性能，自动从GitHub Actions工作流提取构建规范，进行构建失败根本原因分析并提出可扩展解决方案。

Result: 提出自动化重建工件方法，提升了性能和功能。

Conclusion: 该方法能增强开源供应链的安全性和透明度。

Abstract: Software ecosystems like Maven Central play a crucial role in modern software
supply chains by providing repositories for libraries and build plugins.
However, the separation between binaries and their corresponding source code in
Maven Central presents a significant challenge, particularly when it comes to
linking binaries back to their original build environment. This lack of
transparency poses security risks, as approximately 84% of the top 1200
commonly used artifacts are not built using a transparent CI/CD pipeline.
Consequently, users must place a significant amount of trust not only in the
source code but also in the environment in which these artifacts are built.
  Rebuilding software artifacts from source provides a robust solution to
improve supply chain security. This approach allows for a deeper review of
code, verification of binary-source equivalence, and control over dependencies.
However, challenges arise due to variations in build environments, such as JDK
versions and build commands, which can lead to build failures. Additionally,
ensuring that all dependencies are rebuilt from source across large and complex
dependency graphs further complicates the process. In this paper, we introduce
an extension to Macaron, an industry-grade open-source supply chain security
framework, to automate the rebuilding of Maven artifacts from source. Our
approach improves upon existing tools, by offering better performance in source
code detection and automating the extraction of build specifications from
GitHub Actions workflows. We also present a comprehensive root cause analysis
of build failures in Java projects and propose a scalable solution to automate
the rebuilding of artifacts, ultimately enhancing security and transparency in
the open-source supply chain.

</details>


### [165] [Accelerating AI Development with Cyber Arenas](https://arxiv.org/abs/2509.08200)
*William Cashman,Chasen Milner,Michael Houle,Michael Jones,Hayden Jananthan,Jeremy Kepner,Peter Michaleas,Alex Pentland*

Main category: cs.CR

TL;DR: AI发展需高保真测试环境，网络竞技场提供新机遇，文中提及在国民警卫队演习的网络竞技场部署传感器进行探索。


<details>
  <summary>Details</summary>
Motivation: 让AI从实验室有效过渡到实际操作，需要高保真测试环境，网络竞技场的灵活性为此提供新机会。

Method: 在国民警卫队演习的网络竞技场中部署MIT/IEEE/Amazon Graph Challenge Anonymized Network Sensor。

Result: 未提及。

Conclusion: 未提及。

Abstract: AI development requires high fidelity testing environments to effectively
transition from the laboratory to operations. The flexibility offered by cyber
arenas presents a novel opportunity to test new artificial intelligence (AI)
capabilities with users. Cyber arenas are designed to expose end-users to
real-world situations and must rapidly incorporate evolving capabilities to
meet their core objectives. To explore this concept the MIT/IEEE/Amazon Graph
Challenge Anonymized Network Sensor was deployed in a cyber arena during a
National Guard exercise.

</details>


### [166] [Send to which account? Evaluation of an LLM-based Scambaiting System](https://arxiv.org/abs/2509.08493)
*Hossein Siadati,Haadi Jafarian,Sima Jafarikhah*

Main category: cs.CR

TL;DR: 论文对大语言模型驱动的钓鱼诈骗诱捕系统进行大规模评估，虽取得一定成果，但也面临挑战，需进一步完善。


<details>
  <summary>Details</summary>
Motivation: 传统防御手段难以瓦解诈骗基础设施，需用对话蜜罐主动获取威胁情报。

Method: 部署大语言模型驱动的诈骗诱捕系统，与真实诈骗者互动。

Result: 系统与诈骗者互动获超18700条消息，信息披露率约32%，人类接受率约70%，但初始消息回应率仅48.7%。

Conclusion: 系统有成效但面临挑战，需进一步优化设计。

Abstract: Scammers are increasingly harnessing generative AI(GenAI) technologies to
produce convincing phishing content at scale, amplifying financial fraud and
undermining public trust. While conventional defenses, such as detection
algorithms, user training, and reactive takedown efforts remain important, they
often fall short in dismantling the infrastructure scammers depend on,
including mule bank accounts and cryptocurrency wallets. To bridge this gap, a
proactive and emerging strategy involves using conversational honeypots to
engage scammers and extract actionable threat intelligence. This paper presents
the first large-scale, real-world evaluation of a scambaiting system powered by
large language models (LLMs). Over a five-month deployment, the system
initiated over 2,600 engagements with actual scammers, resulting in a dataset
of more than 18,700 messages. It achieved an Information Disclosure Rate (IDR)
of approximately 32%, successfully extracting sensitive financial information
such as mule accounts. Additionally, the system maintained a Human Acceptance
Rate (HAR) of around 70%, indicating strong alignment between LLM-generated
responses and human operator preferences. Alongside these successes, our
analysis reveals key operational challenges. In particular, the system
struggled with engagement takeoff: only 48.7% of scammers responded to the
initial seed message sent by defenders. These findings highlight the need for
further refinement and provide actionable insights for advancing the design of
automated scambaiting systems.

</details>


### [167] [Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute Implementations](https://arxiv.org/abs/2509.08646)
*Ron F. Del Rosario,Klaudia Krawiecka,Christian Schroeder de Witt*

Main category: cs.CR

TL;DR: 本文介绍了LLM代理的'Plan - then - Execute'模式，分析其原理、优势、安全影响，给出实现蓝图和代码参考，还探讨了高级模式。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理自动化复杂多步任务能力增强，需要鲁棒、安全、可预测的架构模式，因此研究P - t - E模式。

Method: 探索P - t - E的基础原理，分析其组件和架构优势，研究安全影响，给出实现蓝图和代码参考，分析不同框架的实现方式，探讨高级模式。

Result: 分析了P - t - E模式在可预测性、成本效益和推理质量上的优势，研究了其对间接提示注入攻击的抵御能力，给出三个框架的实现分析。

Conclusion: P - t - E模式为构建LLM代理提供坚实基础，但需深度防御策略，还探讨了高级模式为构建生产级代理提供战略蓝图。

Abstract: As Large Language Model (LLM) agents become increasingly capable of
automating complex, multi-step tasks, the need for robust, secure, and
predictable architectural patterns is paramount. This paper provides a
comprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic
design that separates strategic planning from tactical execution. We explore
the foundational principles of P-t-E, detailing its core components - the
Planner and the Executor - and its architectural advantages in predictability,
cost-efficiency, and reasoning quality over reactive patterns like ReAct
(Reason + Act). A central focus is placed on the security implications of this
design, particularly its inherent resilience to indirect prompt injection
attacks by establishing control-flow integrity. We argue that while P-t-E
provides a strong foundation, a defense-in-depth strategy is necessary, and we
detail essential complementary controls such as the Principle of Least
Privilege, task-scoped tool access, and sandboxed code execution. To make these
principles actionable, this guide provides detailed implementation blueprints
and working code references for three leading agentic frameworks: LangChain
(via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing
the P-t-E pattern is analyzed, highlighting unique features like LangGraph's
stateful graphs for re-planning, CrewAI's declarative tool scoping for
security, and AutoGen's built-in Docker sandboxing. Finally, we discuss
advanced patterns, including dynamic re-planning loops, parallel execution with
Directed Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop
(HITL) verification, to offer a complete strategic blueprint for architects,
developers, and security engineers aiming to build production-grade, resilient,
and trustworthy LLM agents.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [168] [Bias in the Loop: How Humans Evaluate AI-Generated Suggestions](https://arxiv.org/abs/2509.08514)
*Jacob Beck,Stephanie Eckman,Christoph Kern,Frauke Kreuter*

Main category: cs.HC

TL;DR: 本文通过实验研究人类 - 人工智能协作，发现协作成功不仅取决于算法性能，还与审查人员和审查流程有关，强调考虑人类心理的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前人类 - 人工智能协作虽能提高效率，但会引发认知偏差，且对协作成败的心理因素了解甚少，因此开展研究。

Method: 进行有2784名参与者的随机实验，操控AI建议质量、任务负担和经济激励三个因素，收集相关数据评估四个绩效指标。

Result: 发现两个挑战传统假设的模式：要求纠正AI错误会降低参与度和增加接受错误建议的倾向；个体对AI的态度是绩效的最强预测因素。

Conclusion: 成功的人类 - 人工智能协作需考虑人类心理，如选择多样化评估样本、测量态度和设计对抗认知偏差的工作流程。

Abstract: Human-AI collaboration increasingly drives decision-making across industries,
from medical diagnosis to content moderation. While AI systems promise
efficiency gains by providing automated suggestions for human review, these
workflows can trigger cognitive biases that degrade performance. We know little
about the psychological factors that determine when these collaborations
succeed or fail. We conducted a randomized experiment with 2,784 participants
to examine how task design and individual characteristics shape human responses
to AI-generated suggestions. Using a controlled annotation task, we manipulated
three factors: AI suggestion quality in the first three instances, task burden
through required corrections, and performance-based financial incentives. We
collected demographics, attitudes toward AI, and behavioral data to assess four
performance metrics: accuracy, correction activity, overcorrection, and
undercorrection. Two patterns emerged that challenge conventional assumptions
about human-AI collaboration. First, requiring corrections for flagged AI
errors reduced engagement and increased the tendency to accept incorrect
suggestions, demonstrating how cognitive shortcuts influence collaborative
outcomes. Second, individual attitudes toward AI emerged as the strongest
predictor of performance, surpassing demographic factors. Participants
skeptical of AI detected errors more reliably and achieved higher accuracy,
while those favorable toward automation exhibited dangerous overreliance on
algorithmic suggestions. The findings reveal that successful human-AI
collaboration depends not only on algorithmic performance but also on who
reviews AI outputs and how review processes are structured. Effective human-AI
collaborations require consideration of human psychology: selecting diverse
evaluator samples, measuring attitudes, and designing workflows that counteract
cognitive biases.

</details>


### [169] [Componentization: Decomposing Monolithic LLM Responses into Manipulable Semantic Units](https://arxiv.org/abs/2509.08203)
*Ryan Lingo,Rajeev Chhajer,Martin Arroyo,Luka Brkljacic,Ben Davis,Nithin Santhanam*

Main category: cs.HC

TL;DR: 论文提出组件化方法，将大语言模型输出分解为可独立编辑单元，介绍MAOD和CBRA，通过原型MAODchat验证，小范围用户研究有初步发现，认为组件化是有前景方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型输出的整体文本难部分编辑，影响协作流程，需解决此问题。

Method: 提出组件化方法，包括MAOD算法和CBRA架构，构建MAODchat原型进行验证，开展小范围用户研究。

Result: 小范围用户研究显示组件级编辑契合常见工作流，能迭代细化和选择性复用，参与者提及可能的团队工作流。

Conclusion: 组件化是将被动文本消费转变为更主动的组件级协作的有前景方向。

Abstract: Large Language Models (LLMs) often produce monolithic text that is hard to
edit in parts, which can slow down collaborative workflows. We present
componentization, an approach that decomposes model outputs into modular,
independently editable units while preserving context. We describe Modular and
Adaptable Output Decomposition (MAOD), which segments responses into coherent
components and maintains links among them, and we outline the Component-Based
Response Architecture (CBRA) as one way to implement this idea. Our reference
prototype, MAODchat, uses a microservices design with state-machine-based
decomposition agents, vendor-agnostic model adapters, and real-time component
manipulation with recomposition.
  In an exploratory study with four participants from academic, engineering,
and product roles, we observed that component-level editing aligned with
several common workflows and enabled iterative refinement and selective reuse.
Participants also mentioned possible team workflows. Our contributions are: (1)
a definition of componentization for transforming monolithic outputs into
manipulable units, (2) CBRA and MAODchat as a prototype architecture, (3)
preliminary observations from a small user study, (4) MAOD as an algorithmic
sketch for semantic segmentation, and (5) example Agent-to-Agent protocols for
automated decomposition. We view componentization as a promising direction for
turning passive text consumption into more active, component-level
collaboration.

</details>


### [170] [Motion-Based User Identification across XR and Metaverse Applications by Deep Classification and Similarity Learning](https://arxiv.org/abs/2509.08539)
*Lukas Schach,Christian Rack,Ryan P. McMahan,Marc Erich Latoschik*

Main category: cs.HC

TL;DR: 本文评估两种模型在XR应用中基于用户动作的身份识别泛化能力，发现跨应用识别能力有限，还公开数据集与代码促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 研究两种先进分类和相似性学习模型在不同XR应用中基于用户动作可靠识别用户的泛化能力。

Method: 开发包含49个用户在5种不同XR应用中动作数据的新数据集，用该数据集评估模型性能和泛化能力。

Result: 模型在同一应用中能准确识别个体，但跨不同XR应用识别用户的能力有限。

Conclusion: 研究为当前模型泛化能力和作为生物识别方法的适用性提供见解，对XR和元宇宙应用中的用户识别进行风险评估，公开数据集和代码以鼓励相关研究。

Abstract: This paper examines the generalization capacity of two state-of-the-art
classification and similarity learning models in reliably identifying users
based on their motions in various Extended Reality (XR) applications. We
developed a novel dataset containing a wide range of motion data from 49 users
in five different XR applications: four XR games with distinct tasks and action
patterns, and an additional social XR application with no predefined task sets.
The dataset is used to evaluate the performance and, in particular, the
generalization capacity of the two models across applications. Our results
indicate that while the models can accurately identify individuals within the
same application, their ability to identify users across different XR
applications remains limited. Overall, our results provide insight into current
models generalization capabilities and suitability as biometric methods for
user verification and identification. The results also serve as a much-needed
risk assessment of hazardous and unwanted user identification in XR and
Metaverse applications. Our cross-application XR motion dataset and code are
made available to the public to encourage similar research on the
generalization of motion-based user identification in typical Metaverse
application use cases.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [171] [Recursive Adaptive Importance Sampling with Optimal Replenishment](https://arxiv.org/abs/2509.08102)
*Daniel Würzler Barreto,Mevin B. Hooten*

Main category: stat.ME

TL;DR: 提出递归自适应重要性采样方法，平衡计算效率与样本质量，并通过实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯推理并行计算方法存在高延迟成本或依赖特定模型分解的问题，需要新方法。

Method: 提出递归自适应重要性采样方法，交替进行快速递归权重更新和样本补充步骤，推导补充步骤的最优分配。

Result: 在模拟实验和墨西哥湾海面温度预测应用中证明了方法的有效性。

Conclusion: 该递归自适应重要性采样方法能够平衡计算效率与样本质量，有较好应用效果。

Abstract: Increased access to computing resources has led to the development of
algorithms that can run efficiently on multi-core processing units or in
distributed computing environments. In the context of Bayesian inference, many
parallel computing approaches to fit statistical models have been proposed in
the context of Markov Chain Monte Carlo methods, but they either have limited
gains due to high latency cost or rely on model-specific decompositions.
Alternatively, adaptive importance sampling, sequential Monte Carlo, and
recursive Bayesian methods provide a parallel-friendly and asymptotically exact
framework with well-developed theory for error estimation. We propose a
recursive adaptive importance sampling approach that alternates between fast
recursive weight updates and sample replenishment steps to balance
computational efficiency while ensuring sample quality. We derive theoretical
results to determine the optimal allocation of replenishing steps, and
demonstrate the efficacy of our method in simulated experiments and an
application of sea surface temperature prediction in the Gulf of Mexico using
Gaussian processes.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [172] [PCGBandit: One-shot acceleration of transient PDE solvers via online-learned preconditioners](https://arxiv.org/abs/2509.08765)
*Mikhail Khodak,Min Ki Jung,Brian Wynne,Edmond chow,Egemen Kolemen*

Main category: physics.comp-ph

TL;DR: 本文提出用新范式加速瞬态偏微分方程数值模拟，开发PCGBandit方法并在OpenFOAM上验证其在流体和磁流体动力学问题中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前基于机器学习加速科学计算工作流的方法需经典模拟训练，在与经典基线对比时难以展现明显优势，需新方法。

Method: 采用一种新范式，利用经典求解器自身数据加速模拟，用多臂老虎机算法根据线性求解器反馈在线学习自适应求解器配置序列，开发PCGBandit方法并在OpenFOAM上实现。

Result: 在一组流体和磁流体动力学问题上展示了PCGBandit方法的有效性。

Conclusion: 提出的新范式和PCGBandit方法能有效加速瞬态偏微分方程的数值模拟。

Abstract: Data-driven acceleration of scientific computing workflows has been a
high-profile aim of machine learning (ML) for science, with numerical
simulation of transient partial differential equations (PDEs) being one of the
main applications. The focus thus far has been on methods that require
classical simulations to train, which when combined with the data-hungriness
and optimization challenges of neural networks has caused difficulties in
demonstrating a convincing advantage against strong classical baselines. We
consider an alternative paradigm in which the learner uses a classical solver's
own data to accelerate it, enabling a one-shot speedup of the simulation.
Concretely, since transient PDEs often require solving a sequence of related
linear systems, the feedback from repeated calls to a linear solver such as
preconditioned conjugate gradient (PCG) can be used by a bandit algorithm to
online-learn an adaptive sequence of solver configurations (e.g.
preconditioners). The method we develop, PCGBandit, is implemented directly on
top of the popular open source software OpenFOAM, which we use to show its
effectiveness on a set of fluid and magnetohydrodynamics (MHD) problems.

</details>


### [173] [Generative Quasi-Continuum Modeling of Confined Fluids at the Nanoscale](https://arxiv.org/abs/2509.08223)
*Bugra Yalcin,Ishan Nadkarni,Jinu Jeong,Chenxing Liang,Narayana R. Aluru*

Main category: physics.comp-ph

TL;DR: 提出数据高效的多尺度框架预测纳米尺度受限流体密度分布，在石墨烯纳米狭缝水测试中表现良好，比AIMD和MLMD更快且需更少训练数据。


<details>
  <summary>Details</summary>
Motivation: 准确密度估计需长时标，AIMD无法实现，MLMD受飞秒时间步限制，难以计算准确密度估计所需的长时间平均值。

Method: 提出基于条件去噪扩散概率模型（DDPM）的准连续介质方法，根据有限AIMD数据集提取的噪声力预测沿约束方向力分布的长时间行为，通过能斯特 - 普朗克方程将预测力与连续介质理论联系起来。

Result: 在两个石墨烯纳米狭缝间的水测试中，能以从头算精度恢复训练域外通道宽度的密度分布。

Conclusion: 相比AIMD和MLMD模拟，该方法运行时间有数量级提升，且比以往工作所需训练数据显著减少。

Abstract: We present a data-efficient, multiscale framework for predicting the density
profiles of confined fluids at the nanoscale. While accurate density estimates
require prohibitively long timescales that are inaccessible by ab initio
molecular dynamics (AIMD) simulations, machine-learned molecular dynamics
(MLMD) offers a scalable alternative, enabling the generation of force
predictions at ab initio accuracy with reduced computational cost. However,
despite their efficiency, MLMD simulations remain constrained by femtosecond
timesteps, which limit their practicality for computing long-time averages
needed for accurate density estimation. To address this, we propose a
conditional denoising diffusion probabilistic model (DDPM) based
quasi-continuum approach that predicts the long-time behavior of force profiles
along the confinement direction, conditioned on noisy forces extracted from a
limited AIMD dataset. The predicted smooth forces are then linked to continuum
theory via the Nernst-Planck equation to reveal the underlying density
behavior. We test the framework on water confined between two graphene
nanoscale slits and demonstrate that density profiles for channel widths
outside of the training domain can be recovered with ab initio accuracy.
Compared to AIMD and MLMD simulations, our method achieves orders-of-magnitude
speed-up in runtime and requires significantly less training data than prior
works.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [174] [A New Dataset and Benchmark for Grounding Multimodal Misinformation](https://arxiv.org/abs/2509.08008)
*Bingjian Yang,Danni Xu,Kaipeng Niu,Wenxuan Liu,Zheng Wang,Mohan Kankanhalli*

Main category: cs.SI

TL;DR: 文章引入GroundMM任务，提出数据集GroundLie360和基线模型FakeMark，为可解释的多模态错误信息检测奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前数据集和检测方法缺乏对抗有说服力错误信息所需的可解释性，而在线错误信息视频传播带来严重社会风险。

Method: 引入GroundMM任务，构建数据集GroundLie360，提出基于VLM、QA驱动的基线模型FakeMark。

Result: 实验凸显了该任务的挑战。

Conclusion: 为可解释的多模态错误信息检测奠定基础。

Abstract: The proliferation of online misinformation videos poses serious societal
risks. Current datasets and detection methods primarily target binary
classification or single-modality localization based on post-processed data,
lacking the interpretability needed to counter persuasive misinformation. In
this paper, we introduce the task of Grounding Multimodal Misinformation
(GroundMM), which verifies multimodal content and localizes misleading segments
across modalities. We present the first real-world dataset for this task,
GroundLie360, featuring a taxonomy of misinformation types, fine-grained
annotations across text, speech, and visuals, and validation with Snopes
evidence and annotator reasoning. We also propose a VLM-based, QA-driven
baseline, FakeMark, using single- and cross-modal cues for effective detection
and grounding. Our experiments highlight the challenges of this task and lay a
foundation for explainable multimodal misinformation detection.

</details>


### [175] [Network Contagion in Financial Labor Markets: Predicting Turnover in Hong Kong](https://arxiv.org/abs/2509.08001)
*Abdulla AlKetbi,Patrick Yam,Gautier Marti,Raed Jaradat*

Main category: cs.SI

TL;DR: 利用香港证监会数据构建网络分析预测员工离职，发现传染效应，网络信号提升预测效果，对监管等有意义。


<details>
  <summary>Details</summary>
Motivation: 员工离职是金融市场关键挑战，研究专业网络在职业变动中的作用所知甚少。

Method: 利用香港证监会公共登记数据构建时间网络，引入基于图的特征传播框架，将网络信号嵌入机器学习模型。

Result: 存在传染效应，超30%同行6个月内离职时员工离职可能性高23%，网络信号使离职预测效果比基线提高30%。

Conclusion: 时间网络效应在劳动力动态中有预测力，基于网络的分析可为监管、人才管理和系统风险评估提供信息。

Abstract: Employee turnover is a critical challenge in financial markets, yet little is
known about the role of professional networks in shaping career moves. Using
the Hong Kong Securities and Futures Commission (SFC) public register
(2007-2024), we construct temporal networks of 121,883 professionals and 4,979
firms to analyze and predict employee departures. We introduce a graph-based
feature propagation framework that captures peer influence and organizational
stability. Our analysis shows a contagion effect: professionals are 23% more
likely to leave when over 30% of their peers depart within six months.
Embedding these network signals into machine learning models improves turnover
prediction by 30% over baselines. These results highlight the predictive power
of temporal network effects in workforce dynamics, and demonstrate how
network-based analytics can inform regulatory monitoring, talent management,
and systemic risk assessment.

</details>


### [176] [Scaling Truth: The Confidence Paradox in AI Fact-Checking](https://arxiv.org/abs/2509.08803)
*Ihsan A. Qazi,Zohaib Khan,Abdullah Ghani,Agha A. Raza,Zafar A. Qazi,Wassay Sajjad,Ayesha Ali,Asher Javaid,Muhammad Abdullah Sohail,Abdul H. Azeemi*

Main category: cs.SI

TL;DR: 文章系统评估九种大语言模型在事实核查方面的表现，发现小模型高自信低准确率、大模型相反，非英语和全球南方的主张表现差距大，为未来研究和政策提供依据。


<details>
  <summary>Details</summary>
Motivation: 错误信息增多，需要可扩展可靠的事实核查方案，大语言模型在自动事实核查方面有前景但全球有效性未知。

Method: 使用47种语言、174个专业事实核查组织评估过的5000条主张，对九种大语言模型进行多类别评估，测试模型泛化能力和四种提示策略，以超24万条人工注释为基准。

Result: 小模型高自信低准确率，大模型高准确率低自信，非英语和全球南方主张性能差距明显。

Conclusion: 研究建立多语言基准，为确保公平获取可信的人工智能辅助事实核查的政策提供证据基础。

Abstract: The rise of misinformation underscores the need for scalable and reliable
fact-checking solutions. Large language models (LLMs) hold promise in
automating fact verification, yet their effectiveness across global contexts
remains uncertain. We systematically evaluate nine established LLMs across
multiple categories (open/closed-source, multiple sizes, diverse architectures,
reasoning-based) using 5,000 claims previously assessed by 174 professional
fact-checking organizations across 47 languages. Our methodology tests model
generalizability on claims postdating training cutoffs and four prompting
strategies mirroring both citizen and professional fact-checker interactions,
with over 240,000 human annotations as ground truth. Findings reveal a
concerning pattern resembling the Dunning-Kruger effect: smaller, accessible
models show high confidence despite lower accuracy, while larger models
demonstrate higher accuracy but lower confidence. This risks systemic bias in
information verification, as resource-constrained organizations typically use
smaller models. Performance gaps are most pronounced for non-English languages
and claims originating from the Global South, threatening to widen existing
information inequalities. These results establish a multilingual benchmark for
future research and provide an evidence base for policy aimed at ensuring
equitable access to trustworthy, AI-assisted fact-checking.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [177] [Chordless cycle filtrations for dimensionality detection in complex networks via topological data analysis](https://arxiv.org/abs/2509.08350)
*Aina Ferrà Marcús,Robert Jankowski,Meritxell Vila Miñana,Carles Casacuberta,M. Ángeles Serrano*

Main category: physics.soc-ph

TL;DR: 提出基于弦环的图拓扑数据分析加权方案，结合多种方法有效估计复杂网络隐藏维度。


<details>
  <summary>Details</summary>
Motivation: 揭示复杂网络潜在空间维度可解决社区结构复杂性、影响网络导航等。

Method: 引入基于弦环的图拓扑数据分析加权方案，用神经网络架构在合成图数据库训练估计网络维度。

Result: 所得到的描述符能有效估计网络维度，且无需重新训练可迁移到真实网络。

Conclusion: 该方法能有效揭示复杂网络隐藏几何结构，指导准确建模和低维嵌入。

Abstract: Many complex networks, ranging from social to biological systems, exhibit
structural patterns consistent with an underlying hyperbolic geometry.
Revealing the dimensionality of this latent space can disentangle the
structural complexity of communities, impact efficient network navigation, and
fundamentally shape connectivity and system behavior. We introduce a novel
topological data analysis weighting scheme for graphs, based on chordless
cycles, aimed at estimating the dimensionality of networks in a data-driven
way. We further show that the resulting descriptors can effectively estimate
network dimensionality using a neural network architecture trained in a
synthetic graph database constructed for this purpose, which does not need
retraining to transfer effectively to real-world networks. Thus, by combining
cycle-aware filtrations, algebraic topology, and machine learning, our approach
provides a robust and effective method for uncovering the hidden geometry of
complex networks and guiding accurate modeling and low-dimensional embedding.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [178] [Joint Learning using Mixture-of-Expert-Based Representation for Enhanced Speech Generation and Robust Emotion Recognition](https://arxiv.org/abs/2509.08470)
*Jing-Tong Tzeng,Carlos Busso,Chi-Chun Lee*

Main category: eess.AS

TL;DR: 针对语音情感识别在噪声环境性能下降问题，提出Sparse MERIT框架，实验显示其在SER和SE任务上均优于基线模型，能在噪声环境提供稳健且通用性能。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别在噪声条件下性能下降，传统语音增强会引入伪影和计算开销，传统多任务学习存在梯度干扰和表征冲突问题。

Method: 提出Sparse MERIT框架，在自监督语音表征上应用逐帧专家路由，结合特定任务门控网络动态选择专家。

Result: 在MSP - Podcast语料库实验中，Sparse MERIT在SER和SE任务上均优于基线模型，如在-5 dB信噪比下，SER F1 - macro平均提升12.0%（对比SE预处理策略）和3.4%（对比简单MTL基线）；SE的分段信噪比提升28.2%（对比SE预处理基线）和20.0%（对比简单MTL基线）。

Conclusion: Sparse MERIT能在噪声环境下为情感识别和增强任务提供稳健且通用的性能。

Abstract: Speech emotion recognition (SER) plays a critical role in building
emotion-aware speech systems, but its performance degrades significantly under
noisy conditions. Although speech enhancement (SE) can improve robustness, it
often introduces artifacts that obscure emotional cues and adds computational
overhead to the pipeline. Multi-task learning (MTL) offers an alternative by
jointly optimizing SE and SER tasks. However, conventional shared-backbone
models frequently suffer from gradient interference and representational
conflicts between tasks. To address these challenges, we propose the Sparse
Mixture-of-Experts Representation Integration Technique (Sparse MERIT), a
flexible MTL framework that applies frame-wise expert routing over
self-supervised speech representations. Sparse MERIT incorporates task-specific
gating networks that dynamically select from a shared pool of experts for each
frame, enabling parameter-efficient and task-adaptive representation learning.
Experiments on the MSP-Podcast corpus show that Sparse MERIT consistently
outperforms baseline models on both SER and SE tasks. Under the most
challenging condition of -5 dB signal-to-noise ratio (SNR), Sparse MERIT
improves SER F1-macro by an average of 12.0% over a baseline relying on a SE
pre-processing strategy, and by 3.4% over a naive MTL baseline, with
statistical significance on unseen noise conditions. For SE, Sparse MERIT
improves segmental SNR (SSNR) by 28.2% over the SE pre-processing baseline and
by 20.0% over the naive MTL baseline. These results demonstrate that Sparse
MERIT provides robust and generalizable performance for both emotion
recognition and enhancement tasks in noisy environments.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [179] [Towards Scalable Proteomics: Opportunistic SMC Samplers on HTCondor](https://arxiv.org/abs/2509.08020)
*Matthew Carter,Lee Devlin,Alexander Philips,Edward Pyzer-Knapp,Paul Spirakis,Simon Maskell*

Main category: q-bio.QM

TL;DR: 本文介绍用于SMC采样器的机会计算框架，利用闲置资源实现大规模蛋白质组学推断，评估显示其能准确推断，还发布开源包CondorSMC。


<details>
  <summary>Details</summary>
Motivation: 复杂贝叶斯模型推理计算密集，SMC高性能实现依赖专业硬件，增加成本，限制蛋白质组学数据挖掘。

Method: 引入针对大规模蛋白质组学推断的机会计算框架，利用利物浦大学闲置计算资源，采用新型Coordinator - Manager - Follower架构。

Result: 在实际蛋白质组学模型上评估，机会SMC能准确推断，资源增加时固定时间内生成样本增多。

Conclusion: 该框架无需专用高性能计算基础设施，可实现可扩展的贝叶斯推断，还发布开源包支持应用。

Abstract: Quantitative proteomics plays a central role in uncovering regulatory
mechanisms, identifying disease biomarkers, and guiding the development of
precision therapies. These insights are often obtained through complex Bayesian
models, whose inference procedures are computationally intensive, especially
when applied at scale to biological datasets. This limits the accessibility of
advanced modelling techniques needed to fully exploit proteomics data. Although
Sequential Monte Carlo (SMC) methods offer a parallelisable alternative to
traditional Markov Chain Monte Carlo, their high-performance implementations
often rely on specialised hardware, increasing both financial and energy costs.
We address these challenges by introducing an opportunistic computing framework
for SMC samplers, tailored to the demands of large-scale proteomics inference.
Our approach leverages idle compute resources at the University of Liverpool
via HTCondor, enabling scalable Bayesian inference without dedicated
high-performance computing infrastructure. Central to this framework is a novel
Coordinator-Manager-Follower architecture that reduces synchronisation overhead
and supports robust operation in heterogeneous, unreliable environments. We
evaluate the framework on a realistic proteomics model and show that
opportunistic SMC delivers accurate inference with weak scaling, increasing
samples generated under a fixed time budget as more resources join. To support
adoption, we release CondorSMC, an open-source package for deploying SMC
samplers in opportunistic computing environments.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [180] [Automated Trading System for Straddle-Option Based on Deep Q-Learning](https://arxiv.org/abs/2509.07987)
*Yiran Wan,Xinyu Ying,Shengzhen Xu*

Main category: q-fin.GN

TL;DR: 针对现有金融交易自动化方法在高波动市场存在计算成本高、性能不稳定问题，提出基于强化学习和注意力机制的跨式期权自动交易方法，实验显示模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有金融交易自动化工作多关注价格趋势预测，在高波动市场计算成本高且性能不稳定，需新方法应对高波动市场的不可预测性。

Method: 1. 在Transformer - DDQN中利用注意力机制，包括时间序列数据的自注意力和多周期信息的通道注意力；2. 设计考虑超额收益的奖励函数；3. 识别阻力位提供参考信息。

Result: 在股票、原油和比特币市场实验，基于注意力的Transformer - DDQN模型最大回撤最低，除原油市场外平均回报比其他模型高92.5%。

Conclusion: 基于强化学习和注意力机制的跨式期权自动交易方法能有效应对高波动市场的不可预测性，表现优于其他模型。

Abstract: Straddle Option is a financial trading tool that explores volatility premiums
in high-volatility markets without predicting price direction. Although deep
reinforcement learning has emerged as a powerful approach to trading automation
in financial markets, existing work mostly focused on predicting price trends
and making trading decisions by combining multi-dimensional datasets like blogs
and videos, which led to high computational costs and unstable performance in
high-volatility markets. To tackle this challenge, we develop automated
straddle option trading based on reinforcement learning and attention
mechanisms to handle unpredictability in high-volatility markets. Firstly, we
leverage the attention mechanisms in Transformer-DDQN through both
self-attention with time series data and channel attention with multi-cycle
information. Secondly, a novel reward function considering excess earnings is
designed to focus on long-term profits and neglect short-term losses over a
stop line. Thirdly, we identify the resistance levels to provide reference
information when great uncertainty in price movements occurs with intensified
battle between the buyers and sellers. Through extensive experiments on the
Chinese stock, Brent crude oil, and Bitcoin markets, our attention-based
Transformer-DDQN model exhibits the lowest maximum drawdown across all markets,
and outperforms other models by 92.5\% in terms of the average return excluding
the crude oil market due to relatively low fluctuation.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [181] [A transport approach to the cutoff phenomenon](https://arxiv.org/abs/2509.08560)
*Francesco Pedrotti,Justin Salez*

Main category: math.PR

TL;DR: 本文提出绕过使用变熵的替代方法，证明对数凹Langevin动力学截止现象并拓展至近端采样器。


<details>
  <summary>Details</summary>
Motivation: 在理解马尔可夫过程截止现象上，已有基于变熵的方法，本文想提出替代方法。

Method: 利用新的W - TV传输不等式和经典抛物正则化估计。

Result: 恢复[Sal25a]中对数凹Langevin动力学截止的主要结果，并将结论扩展到近端采样器。

Conclusion: 提出的方法在非负曲率光滑空间过程中有效，无需链式法则。

Abstract: Substantial progress has recently been made in the understanding of the
cutoff phenomenon for Markov processes, using an information-theoretic
statistics known as varentropy [Sal23; Sal24; Sal25a; PS25]. In the present
paper, we propose an alternative approach which bypasses the use of varentropy
and exploits instead a new W-TV transport inequality, combined with a classical
parabolic regularization estimate [BGL01; OV01]. While currently restricted to
non-negatively curved processes on smooth spaces, our argument no longer
requires the chain rule, nor any approximate version thereof. As applications,
we recover the main result of [Sal25a] establishing cutoff for the log-concave
Langevin dynamics, and extend the conclusion to a widely-used discrete-time
sampling algorithm known as the Proximal Sampler.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [182] [FeynmanDD: Quantum Circuit Analysis with Classical Decision Diagrams](https://arxiv.org/abs/2509.08276)
*Ziyuan Wang,Bin Cheng,Longxiang Yuan,Zhengfeng Ji*

Main category: quant-ph

TL;DR: 介绍新方法FeynmanDD用于量子电路模拟和等价性检查，通过理论分析和实验展示其能力与局限。


<details>
  <summary>Details</summary>
Motivation: 在量子电路分析中引入新方法，区别于以往利用量子态和算子模式的方法。

Method: 利用标准和多终端决策图，将分析转化为计数问题，用决策图作为计算引擎结合高效计数算法。

Result: 通过理论分析和数值实验，展示了FeynmanDD在量子电路分析中的能力和局限。

Conclusion: 基于BDD的新方法FeynmanDD在量子电路分析中有价值。

Abstract: Applications of decision diagrams in quantum circuit analysis have been an
active research area. Our work introduces FeynmanDD, a new method utilizing
standard and multi-terminal decision diagrams for quantum circuit simulation
and equivalence checking. Unlike previous approaches that exploit patterns in
quantum states and operators, our method explores useful structures in the path
integral formulation, essentially transforming the analysis into a counting
problem. The method then employs efficient counting algorithms using decision
diagrams as its underlying computational engine. Through comprehensive
theoretical analysis and numerical experiments, we demonstrate FeynmanDD's
capabilities and limitations in quantum circuit analysis, highlighting the
value of this new BDD-based approach.

</details>


### [183] [QCardEst/QCardCorr: Quantum Cardinality Estimation and Correction](https://arxiv.org/abs/2509.08817)
*Tobias Winker,Jinghua Groppe,Sven Groppe*

Main category: quant-ph

TL;DR: 本文提出使用混合量子 - 经典网络的量子机器学习方法进行基数估计（QCardEst），还引入QCardCorr改进经典基数估计器，在多个数据集上取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 基数估计是数据库管理系统查询优化的重要部分，旨在提升基数估计的准确性。

Method: 开发QCardEst方法，用混合量子 - 经典网络；定义紧凑编码将SQL查询转换为量子态；比较多种经典后处理层；引入QCardCorr，用VQC生成因子改进经典基数估计器。

Result: 在JOB - light上比标准PostgreSQL优化器提升6.37倍，比MSCN高3.47倍；在STATS上比标准PostgreSQL优化器提升8.66倍。

Conclusion: 所提出的方法能有效提升基数估计的性能。

Abstract: Cardinality estimation is an important part of query optimization in DBMS. We
develop a Quantum Cardinality Estimation (QCardEst) approach using Quantum
Machine Learning with a Hybrid Quantum-Classical Network. We define a compact
encoding for turning SQL queries into a quantum state, which requires only
qubits equal to the number of tables in the query. This allows the processing
of a complete query with a single variational quantum circuit (VQC) on current
hardware. In addition, we compare multiple classical post-processing layers to
turn the probability vector output of VQC into a cardinality value. We
introduce Quantum Cardinality Correction QCardCorr, which improves classical
cardinality estimators by multiplying the output with a factor generated by a
VQC to improve the cardinality estimation. With QCardCorr, we have an
improvement over the standard PostgreSQL optimizer of 6.37 times for JOB-light
and 8.66 times for STATS. For JOB-light we even outperform MSCN by a factor of
3.47.

</details>


### [184] [RAPID Quantum Detection and Demodulation of Covert Communications: Breaking the Noise Limit with Solid-State Spin Sensors](https://arxiv.org/abs/2509.08171)
*Amirhossein Taherpour,Abbas Taherpour,Tamer Khattab*

Main category: quant-ph

TL;DR: 本文提出名为RAPID的框架用于检测和解调隐蔽电磁信号，采用两阶段混合策略，数值模拟显示有显著性能提升，为量子传感器在安全关键应用中提供途径。


<details>
  <summary>Details</summary>
Motivation: 构建用于检测和解调隐蔽电磁信号的综合框架，推动量子传感器在安全关键应用中的部署。

Method: 将联合检测和估计任务转化为随机最优控制问题，先基于量子Fisher信息矩阵计算非自适应基线协议，再用其启动通过深度强化学习学习的在线自适应策略。

Result: 协议比静态方法有显著灵敏度提升，在相关噪声环境中保持高估计精度，应用于传感器阵列可实现类似海森堡精度缩放的相干量子波束形成。

Conclusion: 为量子传感器在电子战和隐蔽监视等安全关键应用中部署提供了理论严谨且实际可行的途径。

Abstract: We introduce a comprehensive framework for the detection and demodulation of
covert electromagnetic signals using solid-state spin sensors. Our approach,
named RAPID, is a two-stage hybrid strategy that leverages nitrogen-vacancy
(NV) centers to operate below the classical noise floor employing a robust
adaptive policy via imitation and distillation. We first formulate the joint
detection and estimation task as a unified stochastic optimal control problem,
optimizing a composite Bayesian risk objective under realistic physical
constraints. The RAPID algorithm solves this by first computing a robust,
non-adaptive baseline protocol grounded in the quantum Fisher information
matrix (QFIM), and then using this baseline to warm-start an online, adaptive
policy learned via deep reinforcement learning (Soft Actor-Critic). This method
dynamically optimizes control pulses, interrogation times, and measurement
bases to maximize information gain while actively suppressing non-Markovian
noise and decoherence. Numerical simulations demonstrate that the protocol
achieves a significant sensitivity gain over static methods, maintains high
estimation precision in correlated noise environments, and, when applied to
sensor arrays, enables coherent quantum beamforming that achieves
Heisenberg-like scaling in precision. This work establishes a theoretically
rigorous and practically viable pathway for deploying quantum sensors in
security-critical applications such as electronic warfare and covert
surveillance.

</details>


### [185] [LLM-Guided Ansätze Design for Quantum Circuit Born Machines in Financial Generative Modeling](https://arxiv.org/abs/2509.08385)
*Yaswitha Gujju,Romain Harang,Tetsuo Shibuya*

Main category: quant-ph

TL;DR: 本文提出基于大语言模型的提示框架生成硬件感知的量子电路玻恩机（QCBM）架构，在金融建模任务上验证其有效性，表明该方法有实用价值。


<details>
  <summary>Details</summary>
Motivation: 量子生成建模有潜力获实际量子优势，但找到兼具表达性和硬件效率的 ansätze 是挑战，尤其在 NISQ 设备上。

Method: 引入基于大语言模型的提示框架，提示基于量子比特连接性、门错误率和硬件拓扑，用迭代反馈（如 KL 散度、电路深度和有效性）优化电路。

Result: 在日本国债利率日变化的金融建模任务中，大语言模型生成的 ansätze 电路更浅，在 12 量子比特的真实 IBM 量子硬件上执行时生成性能优于标准基线。

Conclusion: 研究展示了大语言模型驱动的量子架构搜索的实用价值，为近期量子设备的生成模型提供了有前景的路径。

Abstract: Quantum generative modeling using quantum circuit Born machines (QCBMs) shows
promising potential for practical quantum advantage. However, discovering
ans\"atze that are both expressive and hardware-efficient remains a key
challenge, particularly on noisy intermediate-scale quantum (NISQ) devices. In
this work, we introduce a prompt-based framework that leverages large language
models (LLMs) to generate hardware-aware QCBM architectures. Prompts are
conditioned on qubit connectivity, gate error rates, and hardware topology,
while iterative feedback, including Kullback-Leibler (KL) divergence, circuit
depth, and validity, is used to refine the circuits. We evaluate our method on
a financial modeling task involving daily changes in Japanese government bond
(JGB) interest rates. Our results show that the LLM-generated ans\"atze are
significantly shallower and achieve superior generative performance compared to
the standard baseline when executed on real IBM quantum hardware using 12
qubits. These findings demonstrate the practical utility of LLM-driven quantum
architecture search and highlight a promising path toward robust, deployable
generative models for near-term quantum devices.

</details>


### [186] [Robust Belief-State Policy Learning for Quantum Network Routing Under Decoherence and Time-Varying Conditions](https://arxiv.org/abs/2509.08654)
*Amirhossein Taherpour,Abbas Taherpour,Tamer Khattab*

Main category: quant-ph

TL;DR: 提出基于特征的POMDP框架用于量子网络路由，结合GNN解决动态量子系统问题，实验显示效果良好。


<details>
  <summary>Details</summary>
Motivation: 解决动态量子系统中部分可观测性、退相干和可扩展性挑战。

Method: 将量子网络动态编码到低维特征空间，采用混合GNN - POMDP架构学习路由策略，结合噪声自适应机制进行决策，并进行理论分析。

Result: 在模拟的最多100个节点的量子网络实验中，与现有基线相比，路由保真度和纠缠传输率显著提高，尤其在高退相干和非平稳条件下。

Conclusion: 该基于特征的POMDP框架能有效应对量子网络路由问题，具有较好性能。

Abstract: This paper presents a feature-based Partially Observable Markov Decision
Process (POMDP) framework for quantum network routing, combining belief-state
planning with Graph Neural Networks (GNNs) to address partial observability,
decoherence, and scalability challenges in dynamic quantum systems. Our
approach encodes complex quantum network dynamics, including entanglement
degradation and time-varying channel noise, into a low-dimensional feature
space, enabling efficient belief updates and scalable policy learning. The core
of our framework is a hybrid GNN-POMDP architecture that processes
graph-structured representations of entangled links to learn routing policies,
coupled with a noise-adaptive mechanism that fuses POMDP belief updates with
GNN outputs for robust decision making. We provide a theoretical analysis
establishing guarantees for belief convergence, policy improvement, and
robustness to noise. Experiments on simulated quantum networks with up to 100
nodes demonstrate significant improvements in routing fidelity and entanglement
delivery rates compared to state-of-the-art baselines, particularly under high
decoherence and nonstationary conditions.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [187] [Bregman Douglas-Rachford Splitting Method](https://arxiv.org/abs/2509.08739)
*Shiqian Ma,Lin Xiao,Renbo Zhao*

Main category: math.OC

TL;DR: 提出BDRS方法及其变体解决极大单调包含问题，证明算法收敛性并讨论解离散最优传输问题。


<details>
  <summary>Details</summary>
Motivation: 解决极大单调包含问题和离散最优传输问题，提出新算法。

Method: 提出Bregman Douglas - Rachford splitting (BDRS)方法及其变体Bregman Peaceman - Rachford splitting方法，证明BDRS与Bregman交替方向乘子法（ADMM）等价。

Result: 证明了算法在一定假设下的收敛性，指出一个假设不适用于最优传输问题。

Conclusion: 提出的算法是新的，可用于解决相关问题，但存在假设局限性。

Abstract: In this paper, we propose the Bregman Douglas-Rachford splitting (BDRS)
method and its variant Bregman Peaceman-Rachford splitting method for solving
maximal monotone inclusion problem. We show that BDRS is equivalent to a
Bregman alternating direction method of multipliers (ADMM) when applied to the
dual of the problem. A special case of the Bregman ADMM is an alternating
direction version of the exponential multiplier method. To the best of our
knowledge, algorithms proposed in this paper are new to the literature. We also
discuss how to use our algorithms to solve the discrete optimal transport (OT)
problem. We prove the convergence of the algorithms under certain assumptions,
though we point out that one assumption does not apply to the OT problem.

</details>


### [188] [Combined-distance-based score function of cognitive fuzzy sets and its application in lung cancer pain evaluation](https://arxiv.org/abs/2509.08239)
*Lisheng Jiang,Tianyu Zhang,Shiyu Yan,Ran Fang*

Main category: math.OC

TL;DR: 本文提出改进的认知模糊Minkowski距离和认知模糊Hausdorff距离，结合二者提出认知模糊组合距离，构建得分函数用于肺癌疼痛评估，分析证明方法可靠且有优势。


<details>
  <summary>Details</summary>
Motivation: 现有认知模糊集（CFS）距离研究少，当前Minkowski距离忽略CFS犹豫度可能致错，需填补CFS距离研究空白。

Method: 提出改进的CF - IM距离和CF - H距离，通过线性组合二者得到CF - C距离，构建基于CF - C距离的得分函数。

Result: 发现CF - H距离抗干扰能力强，CF - IM距离信息利用率高；所提得分函数用于肺癌疼痛评估。

Conclusion: 敏感性和对比分析表明所提方法可靠且有优势。

Abstract: In decision making, the cognitive fuzzy set (CFS) is a useful tool in
expressing experts' complex assessments of alternatives. The distance of CFS,
which plays an important role in decision analyses, is necessary when the CFS
is applied in solving practical issues. However, as far as we know, the studies
on the distance of CFS are few, and the current Minkowski distance of CFS
ignores the hesitancy degree of CFS, which might cause errors. To fill the gap
of the studies on the distance of CFS, because of the practicality of the
Hausdorff distance, this paper proposes the improved cognitive fuzzy Minkowski
(CF-IM) distance and the cognitive fuzzy Hausdorff (CF-H) distance to enrich
the studies on the distance of CFS. It is found that the anti-perturbation
ability of the CF-H distance is stronger than that of the CF-IM distance, but
the information utilization of the CF-IM distance is higher than that of the
CF-H distance. To balance the anti-perturbation ability and information
utilization of the CF-IM distance and CF-H distance, the cognitive fuzzy
combined (CF-C) distance is proposed by establishing the linear combination of
the CF-IM distance and CF-H distance. Based on the CF-C distance, a
combined-distanced-based score function of CFS is proposed to compare CFSs. The
proposed score function is employed in lung cancer pain evaluation issues. The
sensitivity and comparison analyses demonstrate the reliability and advantages
of the proposed methods.

</details>


### [189] [OCTANE -- Optimal Control for Tensor-based Autoencoder Network Emergence: Explicit Case](https://arxiv.org/abs/2509.08169)
*Ratna Khatri,Anthony Kolshorn,Colin Olson,Harbir Antil*

Main category: math.OC

TL;DR: 提出结合最优控制理论和低秩张量方法的自编码器型深度神经网络框架OCTANE，实现高效训练和架构发现，并应用于图像任务。


<details>
  <summary>Details</summary>
Motivation: 实现自编码器型深度神经网络的内存高效训练和自动架构发现。

Method: 将学习任务转化为受微分方程约束的优化问题，用拉格朗日方法推导最优条件，在低秩张量流形上用自适应显式积分方案近似微分方程解。

Result: 形成OCTANE框架，产生紧凑架构，减少内存使用，在有限数据下有效学习。

Conclusion: 框架在图像去噪和去模糊任务中有用，并给出超参数建议。

Abstract: This paper presents a novel, mathematically rigorous framework for
autoencoder-type deep neural networks that combines optimal control theory and
low-rank tensor methods to yield memory-efficient training and automated
architecture discovery. The learning task is formulated as an optimization
problem constrained by differential equations representing the encoder and
decoder components of the network and the corresponding optimality conditions
are derived via a Lagrangian approach. Efficient memory compression is enabled
by approximating differential equation solutions on low-rank tensor manifolds
using an adaptive explicit integration scheme. These concepts are combined to
form OCTANE (Optimal Control for Tensor-based Autoencoder Network Emergence) --
a unified training framework that yields compact autoencoder architectures,
reduces memory usage, and enables effective learning, even with limited
training data. The framework's utility is illustrated with application to image
denoising and deblurring tasks and recommendations regarding governing
hyperparameters are provided.

</details>


### [190] [Decentralized Stochastic Nonconvex Optimization under the Relaxed Smoothness](https://arxiv.org/abs/2509.08726)
*Luo Luo,Xue Cui,Tingkai Jia,Cheng Chen*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper studies decentralized optimization problem
$f(\mathbf{x})=\frac{1}{m}\sum_{i=1}^m f_i(\mathbf{x})$, where each local
function has the form of $f_i(\mathbf{x}) = {\mathbb
E}\left[F(\mathbf{x};{\xi}_i)\right]$ which is $(L_0,L_1)$-smooth but possibly
nonconvex and the random variable ${\xi}_i$ follows distribution ${\mathcal
D}_i$. We propose a novel algorithm called decentralized normalized stochastic
gradient descent (DNSGD), which can achieve the $\epsilon$-stationary point on
each local agent. We present a new framework for analyzing decentralized
first-order methods in the relaxed smooth setting, based on the Lyapunov
function related to the product of the gradient norm and the consensus error.
The analysis shows upper bounds on sample complexity of ${\mathcal
O}(m^{-1}(L_f\sigma^2\Delta_f\epsilon^{-4} + \sigma^2\epsilon^{-2} +
L_f^{-2}L_1^3\sigma^2\Delta_f\epsilon^{-1} + L_f^{-2}L_1^2\sigma^2))$ per agent
and communication complexity of $\tilde{\mathcal O}((L_f\epsilon^{-2} +
L_1\epsilon^{-1})\gamma^{-1/2}\Delta_f)$, where $L_f=L_0 +L_1\zeta$, $\sigma^2$
is the variance of the stochastic gradient, $\Delta_f$ is the initial optimal
function value gap, $\gamma$ is the spectral gap of the network, and $\zeta$ is
the degree of the gradient dissimilarity. In the special case of $L_1=0$, the
above results (nearly) match the lower bounds on decentralized nonconvex
optimization in the standard smooth setting. We also conduct numerical
experiments to show the empirical superiority of our method.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [191] [ToDMA: Large Model-Driven Token-Domain Multiple Access for Semantic Communications](https://arxiv.org/abs/2505.10946)
*Li Qiao,Mahdi Boloursaz Mashhadi,Zhen Gao,Robert Schober,Deniz Gündüz*

Main category: cs.IT

TL;DR: 提出令牌域多址接入（ToDMA）方案，用预训练MLLM缓解令牌冲突，仿真证明其在文本和图像传输中有效。


<details>
  <summary>Details</summary>
Motivation: 现有通信方案存在不足，为降低传输速率、解决令牌冲突问题，提出ToDMA方案。

Method: 各发射机对源信号进行令牌化并调制，接收机用压缩感知检测活动令牌和CSI，聚类重建源令牌序列，用预训练MLLM预测掩码令牌缓解冲突。

Result: 仿真显示ToDMA框架在文本和图像传输任务中有效，相比上下文无关正交通信方案降低延迟，比现有上下文无关非正交通信方法有更好的失真和感知质量。

Conclusion: 提出的ToDMA框架在通信任务中表现良好，能有效解决令牌冲突等问题，提升通信性能。

Abstract: Token communications (TokCom) is an emerging generative semantic
communication concept that reduces transmission rates by using context and
multimodal large language model (MLLM)-based token processing, with tokens
serving as universal semantic units across modalities. In this paper, we
propose a semantic multiple access scheme in the token domain, referred to as
token domain multiple access (ToDMA), where a large number of devices share a
token codebook and a modulation codebook for source and channel coding,
respectively. Specifically, each transmitter first tokenizes its source signal
and modulate each token to a codeword. At the receiver, compressed sensing is
employed first to detect active tokens and the corresponding channel state
information (CSI) from the superposed signals. Then, the source token sequences
are reconstructed by clustering the token-associated CSI across multiple time
slots. In case of token collisions, some active tokens cannot be assigned and
some positions in the reconstructed token sequences are empty. We propose to
use pre-trained MLLMs to leverage the context, predict masked tokens, and thus
mitigate token collisions. Simulation results demonstrate the effectiveness of
the proposed ToDMA framework for both text and image transmission tasks,
achieving significantly lower latency compared to context-unaware orthogonal
communication schemes, while also delivering superior distortion and perceptual
quality compared to state-of-the-art context-unaware non-orthogonal
communication methods.

</details>


### [192] [SCA-LLM: Spectral-Attentive Channel Prediction with Large Language Models in MIMO-OFDM](https://arxiv.org/abs/2509.08139)
*Ke He,Le He,Lisheng Fan,Xianfu Lei,Thang X. Vu,George K. Karagiannidis,Symeon Chatzinotas*

Main category: cs.IT

TL;DR: 本文提出SCA - LLM框架用于MIMO - OFDM系统信道预测，在性能和泛化性上表现出色。


<details>
  <summary>Details</summary>
Motivation: 直接应用大语言模型（LLMs）到信道预测存在领域不匹配问题，现有适配器未充分发挥“适配器 + LLM”范式潜力。

Method: 提出谱注意力框架SCA - LLM，其新型适配器可捕捉更精细频谱细节。

Result: SCA - LLM实现了最先进的预测性能和强泛化性，比之前基于LLM的方法在归一化均方误差（NMSE）上最多有 - 2.4 dB优势。

Conclusion: 消融研究证实SCA - LLM在缓解领域不匹配方面具有优越性。

Abstract: In recent years, the success of large language models (LLMs) has inspired
growing interest in exploring their potential applications in wireless
communications, especially for channel prediction tasks. However, directly
applying LLMs to channel prediction faces a domain mismatch issue stemming from
their text-based pre-training. To mitigate this, the ``adapter + LLM" paradigm
has emerged, where an adapter is designed to bridge the domain gap between the
channel state information (CSI) data and LLMs. While showing initial success,
existing adapters may not fully exploit the potential of this paradigm. To
address this limitation, this work provides a key insight that learning
representations from the spectral components of CSI features can more
effectively help bridge the domain gap. Accordingly, we propose a
spectral-attentive framework, named SCA-LLM, for channel prediction in
multiple-input multiple-output orthogonal frequency division multiplexing
(MIMO-OFDM) systems. Specifically, its novel adapter can capture finer spectral
details and better adapt the LLM for channel prediction than previous methods.
Extensive simulations show that SCA-LLM achieves state-of-the-art prediction
performance and strong generalization, yielding up to $-2.4~\text{dB}$
normalized mean squared error (NMSE) advantage over the previous LLM based
method. Ablation studies further confirm the superiority of SCA-LLM in
mitigating domain mismatch.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [193] [Learning Turbulent Flows with Generative Models: Super-resolution, Forecasting, and Sparse Flow Reconstruction](https://arxiv.org/abs/2509.08752)
*Vivek Oommen,Siavash Khodakarami,Aniruddha Bora,Zhicheng Wang,George Em Karniadakis*

Main category: physics.flu-dyn

TL;DR: 结合算子学习和生成式建模克服了传统神经算子训练时过度平滑细尺度湍流结构的局限，在三个湍流流动挑战中取得良好效果，实现低成本准确重建和预测。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子用标准L2损失训练时会过度平滑细尺度湍流结构，需要克服这一局限。

Method: 将算子学习与生成式建模相结合，采用对抗训练的神经算子和条件生成模型。

Result: 在三个实际湍流流动挑战中表现出色，如降低能谱误差、实现准确预测和快速推理、准确重建速度和压力场。

Conclusion: 这些进展使低成本准确重建和预测成为可能，让实验和计算流体力学中的近实时分析和控制成为可能。

Abstract: Neural operators are promising surrogates for dynamical systems but when
trained with standard L2 losses they tend to oversmooth fine-scale turbulent
structures. Here, we show that combining operator learning with generative
modeling overcomes this limitation. We consider three practical turbulent-flow
challenges where conventional neural operators fail: spatio-temporal
super-resolution, forecasting, and sparse flow reconstruction. For Schlieren
jet super-resolution, an adversarially trained neural operator (adv-NO) reduces
the energy-spectrum error by 15x while preserving sharp gradients at neural
operator-like inference cost. For 3D homogeneous isotropic turbulence, adv-NO
trained on only 160 timesteps from a single trajectory forecasts accurately for
five eddy-turnover times and offers 114x wall-clock speed-up at inference than
the baseline diffusion-based forecasters, enabling near-real-time rollouts. For
reconstructing cylinder wake flows from highly sparse Particle Tracking
Velocimetry-like inputs, a conditional generative model infers full 3D velocity
and pressure fields with correct phase alignment and statistics. These advances
enable accurate reconstruction and forecasting at low compute cost, bringing
near-real-time analysis and control within reach in experimental and
computational fluid mechanics. See our project page:
https://vivekoommen.github.io/Gen4Turb/

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [194] [Dorst-Smeulders Coding for Arbitrary Binary Words](https://arxiv.org/abs/2509.08684)
*Alessandro De Luca,Gabriele Fici*

Main category: cs.DM

TL;DR: 本文提出线性时间算法计算二进制输入词最长Sturmian前缀的Dorst - Smeulders编码，为任意二进制词编码奠定基础，可用于压缩方案。


<details>
  <summary>Details</summary>
Motivation: 计算任意二进制词的Dorst - Smeulders编码，以用于压缩方案。

Method: 提出线性时间算法，基于对Sturmian词结构特性的深入分析。

Result: 得到计算二进制输入词最长Sturmian前缀的Dorst - Smeulders编码的算法。

Conclusion: 该算法概念简单、代码少且基于对Sturmian词的深入分析，可用于相关压缩方案。

Abstract: A binary word is Sturmian if the occurrences of each letter are balanced, in
the sense that in any two factors of the same length, the difference between
the number of occurrences of the same letter is at most 1. In digital geometry,
Sturmian words correspond to discrete approximations of straight line segments
in the Euclidean plane. The Dorst-Smeulders coding, introduced in 1984, is a
4-tuple of integers that uniquely represents a Sturmian word $w$, enabling its
reconstruction using $|w|$ modular operations, making it highly efficient in
practice. In this paper, we present a linear-time algorithm that, given a
binary input word $w$, computes the Dorst-Smeulders coding of its longest
Sturmian prefix. This forms the basis for computing the Dorst-Smeulders coding
of an arbitrary binary word $w$, which is a minimal decomposition (in terms of
the number of factors) of $w$ into Sturmian words, each represented by its
Dorst-Smeulders coding. This coding could be leveraged in compression schemes
where the input is transformed into a binary word composed of long Sturmian
segments. Although the algorithm is conceptually simple and can be implemented
in just a few lines of code, it is grounded in a deep analysis of the
structural properties of Sturmian words.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [195] [Bilingual Word Level Language Identification for Omotic Languages](https://arxiv.org/abs/2509.07998)
*Mesay Gemeda Yigezu,Girma Yohannis Bade,Atnafu Lambebo Tonja,Olga Kolesnikova,Grigori Sidorov,Alexander Gelbukh*

Main category: cs.CL

TL;DR: 本文针对埃塞俄比亚南部的Wolaita和Gofa两种语言进行双语语言识别（BLID），采用多种方法实验，结合BERT预训练语言模型和LSTM的方法效果较好，F1分数达0.72，能解决社交媒体问题并为后续研究奠基。


<details>
  <summary>Details</summary>
Motivation: 在多语言社区中，文本可能包含多种语言，Wolaita和Gofa两种语言存在词汇相似和差异，使语言识别任务具有挑战性，需要有效方法进行双语语言识别。

Method: 对多种方法进行实验，采用结合BERT预训练语言模型和LSTM的方法。

Result: 结合BERT预训练语言模型和LSTM的方法在测试集上F1分数达到0.72。

Conclusion: 该工作能有效解决社交媒体问题，并为该领域进一步研究提供基础。

Abstract: Language identification is the task of determining the languages for a given
text. In many real world scenarios, text may contain more than one language,
particularly in multilingual communities. Bilingual Language Identification
(BLID) is the task of identifying and distinguishing between two languages in a
given text. This paper presents BLID for languages spoken in the southern part
of Ethiopia, namely Wolaita and Gofa. The presence of words similarities and
differences between the two languages makes the language identification task
challenging. To overcome this challenge, we employed various experiments on
various approaches. Then, the combination of the BERT based pretrained language
model and LSTM approach performed better, with an F1 score of 0.72 on the test
set. As a result, the work will be effective in tackling unwanted social media
issues and providing a foundation for further research in this area.

</details>


### [196] [MVPBench: A Benchmark and Fine-Tuning Framework for Aligning Large Language Models with Diverse Human Values](https://arxiv.org/abs/2509.08022)
*Yao Liang,Dongcheng Zhao,Feifei Zhao,Guobin Shen,Yuwei Wang,Dongqi Liang,Yi Zeng*

Main category: cs.CL

TL;DR: 本文介绍了用于评估大语言模型价值对齐的新基准MVPBench，分析模型表现差异，证明轻量级微调方法可提升对齐效果，强调人口感知评估必要。


<details>
  <summary>Details</summary>
Motivation: 现有基准忽略文化和人口多样性，导致对大语言模型价值对齐全球泛化理解有限。

Method: 引入MVPBench基准，对多个模型进行深入分析，并测试轻量级微调方法。

Result: 发现模型在地理和人口方面对齐表现存在显著差异，轻量级微调方法能显著提升对齐效果。

Conclusion: 强调人口感知对齐评估的必要性，为构建文化自适应和价值敏感的大语言模型提供见解，MVPBench为相关研究奠定基础。

Abstract: The alignment of large language models (LLMs) with human values is critical
for their safe and effective deployment across diverse user populations.
However, existing benchmarks often neglect cultural and demographic diversity,
leading to limited understanding of how value alignment generalizes globally.
In this work, we introduce MVPBench, a novel benchmark that systematically
evaluates LLMs' alignment with multi-dimensional human value preferences across
75 countries. MVPBench contains 24,020 high-quality instances annotated with
fine-grained value labels, personalized questions, and rich demographic
metadata, making it the most comprehensive resource of its kind to date. Using
MVPBench, we conduct an in-depth analysis of several state-of-the-art LLMs,
revealing substantial disparities in alignment performance across geographic
and demographic lines. We further demonstrate that lightweight fine-tuning
methods, such as Low-Rank Adaptation (LoRA) and Direct Preference Optimization
(DPO), can significantly enhance value alignment in both in-domain and
out-of-domain settings. Our findings underscore the necessity for
population-aware alignment evaluation and provide actionable insights for
building culturally adaptive and value-sensitive LLMs. MVPBench serves as a
practical foundation for future research on global alignment, personalized
value modeling, and equitable AI development.

</details>


### [197] [NOWJ@COLIEE 2025: A Multi-stage Framework Integrating Embedding Models and Large Language Models for Legal Retrieval and Entailment](https://arxiv.org/abs/2509.08025)
*Hoang-Trung Nguyen,Tan-Minh Nguyen,Xuan-Bach Le,Tuan-Kiet Le,Khanh-Huyen Nguyen,Ha-Thanh Nguyen,Thi-Hai-Yen Vuong,Le-Minh Nguyen*

Main category: cs.CL

TL;DR: 本文介绍NOWJ团队参加COLIEE 2025竞赛五个任务的方法和结果，尤其在法律案例蕴含任务中取得进展。


<details>
  <summary>Details</summary>
Motivation: 参与COLIEE 2025竞赛，在法律信息处理任务中取得好成绩并推动该领域发展。

Method: 系统集成预排序模型、基于嵌入的语义表示和先进大语言模型，采用两阶段检索系统、精心设计的集成方法和基于提示的推理策略。

Result: 在任务2中获第一名，F1分数0.3195，在其他任务中表现出色。

Conclusion: 混合模型结合传统信息检索技术和当代生成模型有潜力，为法律信息处理未来发展提供参考。

Abstract: This paper presents the methodologies and results of the NOWJ team's
participation across all five tasks at the COLIEE 2025 competition, emphasizing
advancements in the Legal Case Entailment task (Task 2). Our comprehensive
approach systematically integrates pre-ranking models (BM25, BERT, monoT5),
embedding-based semantic representations (BGE-m3, LLM2Vec), and advanced Large
Language Models (Qwen-2, QwQ-32B, DeepSeek-V3) for summarization, relevance
scoring, and contextual re-ranking. Specifically, in Task 2, our two-stage
retrieval system combined lexical-semantic filtering with contextualized LLM
analysis, achieving first place with an F1 score of 0.3195. Additionally, in
other tasks--including Legal Case Retrieval, Statute Law Retrieval, Legal
Textual Entailment, and Legal Judgment Prediction--we demonstrated robust
performance through carefully engineered ensembles and effective prompt-based
reasoning strategies. Our findings highlight the potential of hybrid models
integrating traditional IR techniques with contemporary generative models,
providing a valuable reference for future advancements in legal information
processing.

</details>


### [198] [Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions](https://arxiv.org/abs/2509.08217)
*Eve Fleisig,Matthias Orlikowski,Philipp Cimiano,Dan Klein*

Main category: cs.CL

TL;DR: 研究标注者过滤启发式方法对主观任务数据变异性保存的影响，发现现有方法存在问题，强调需考虑标签多样性的垃圾邮件去除方法。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习数据集中平衡标注者可靠性和代表性的问题，即如何在过滤垃圾或低质量回复时保存数据标签的变异性。

Method: 实证评估一系列标注者过滤启发式方法对主观任务变异性保存的影响，分析在合成垃圾数据上的表现。

Result: 现有方法常移除有不同意见的标注者而非垃圾标注者，保守的标注者移除设置（<5%）最佳，多数垃圾标注者与真实标注者分布难区分，现有垃圾过滤方法直觉不适用于需保存变异性的任务。

Conclusion: 强调需要能考虑标签多样性的垃圾去除方法。

Abstract: For machine learning datasets to accurately represent diverse opinions in a
population, they must preserve variation in data labels while filtering out
spam or low-quality responses. How can we balance annotator reliability and
representation? We empirically evaluate how a range of heuristics for annotator
filtering affect the preservation of variation on subjective tasks. We find
that these methods, designed for contexts in which variation from a single
ground-truth label is considered noise, often remove annotators who disagree
instead of spam annotators, introducing suboptimal tradeoffs between accuracy
and label diversity. We find that conservative settings for annotator removal
(<5%) are best, after which all tested methods increase the mean absolute error
from the true average label. We analyze performance on synthetic spam to
observe that these methods often assume spam annotators are less random than
real spammers tend to be: most spammers are distributionally indistinguishable
from real annotators, and the minority that are distinguishable tend to give
fixed answers, not random ones. Thus, tasks requiring the preservation of
variation reverse the intuition of existing spam filtering methods: spammers
tend to be less random than non-spammers, so metrics that assume variation is
spam fare worse. These results highlight the need for spam removal methods that
account for label diversity.

</details>


### [199] [Toward Subtrait-Level Model Explainability in Automated Writing Evaluation](https://arxiv.org/abs/2509.08345)
*Alejandro Andrade-Lotero,Lee Becker,Joshua Southerland,Scott Hellman*

Main category: cs.CL

TL;DR: 使用生成式语言模型进行子特征评估以提高自动写作评分透明度，展示了一定相关性并为教育者和学生解释分数。


<details>
  <summary>Details</summary>
Motivation: 提高自动写作评分的透明度。

Method: 用生成式语言模型进行可解释性和子特征评分。

Result: 人类子特征与特征分数、自动与人类子特征分数之间有适度相关性。

Conclusion: 该方法能为教育者和学生解释分数细节。

Abstract: Subtrait (latent-trait components) assessment presents a promising path
toward enhancing transparency of automated writing scores. We prototype
explainability and subtrait scoring with generative language models and show
modest correlation between human subtrait and trait scores, and between
automated and human subtrait scores. Our approach provides details to demystify
scores for educators and students.

</details>


### [200] [Automatic Detection of Inauthentic Templated Responses in English Language Assessments](https://arxiv.org/abs/2509.08355)
*Yashad Samant,Lee Becker,Scott Hellman,Bradley Behan,Sarah Hughes,Joshua Southerland*

Main category: cs.CL

TL;DR: 研究提出自动化检测虚假模板化作答任务，介绍方法并说明生产中定期更新模型的重要性


<details>
  <summary>Details</summary>
Motivation: 在高风险英语语言评估中，低水平考生可能用模板化材料欺骗自动评分系统

Method: 采用基于机器学习的方法处理自动化检测虚假模板化作答任务

Result: 未提及具体结果

Conclusion: 强调在生产中定期更新模型的重要性

Abstract: In high-stakes English Language Assessments, low-skill test takers may employ
memorized materials called ``templates'' on essay questions to ``game'' or fool
the automated scoring system. In this study, we introduce the automated
detection of inauthentic, templated responses (AuDITR) task, describe a machine
learning-based approach to this task and illustrate the importance of regularly
updating these models in production.

</details>


### [201] [<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs](https://arxiv.org/abs/2509.08358)
*Sergey Pletenev,Daniil Moskovskiy,Alexander Panchenko*

Main category: cs.CL

TL;DR: 文章探讨用大语言模型生成的合成有毒数据训练文本解毒模型，发现效果不如人类数据，原因是词汇多样性差距。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型在文本解毒等敏感领域的表现未受足够关注，探索用其生成的合成有毒数据替代人类数据训练解毒模型的可能性。

Method: 使用Llama 3和Qwen激活补丁模型，为ParaDetox和SST - 2数据集中的中性文本生成合成有毒对应文本。

Result: 在合成数据上微调的模型性能始终低于在人类数据上训练的模型，联合指标性能下降达30%。

Conclusion: 当前大语言模型在该领域存在局限性，强调了多样化、人工标注数据对构建强大解毒系统的持续重要性。

Abstract: Modern Large Language Models (LLMs) are excellent at generating synthetic
data. However, their performance in sensitive domains such as text
detoxification has not received proper attention from the scientific community.
This paper explores the possibility of using LLM-generated synthetic toxic data
as an alternative to human-generated data for training models for
detoxification. Using Llama 3 and Qwen activation-patched models, we generated
synthetic toxic counterparts for neutral texts from ParaDetox and SST-2
datasets. Our experiments show that models fine-tuned on synthetic data
consistently perform worse than those trained on human data, with a drop in
performance of up to 30% in joint metrics. The root cause is identified as a
critical lexical diversity gap: LLMs generate toxic content using a small,
repetitive vocabulary of insults that fails to capture the nuances and variety
of human toxicity. These findings highlight the limitations of current LLMs in
this domain and emphasize the continued importance of diverse, human-annotated
data for building robust detoxification systems.

</details>


### [202] [Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model](https://arxiv.org/abs/2509.08381)
*Yu Cheng Chih,Yong Hao Hou*

Main category: cs.CL

TL;DR: 介绍了小模型ETLCH在结构化数据提取任务中表现出色，证明小模型在低资源环境可实现低成本高效信息提取。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于结构化数据提取成本高、准备数据集难，且缺乏小模型在低资源多任务条件下可靠性的证据。

Method: 提出基于十亿参数LLaMA的模型ETLCH，用低秩自适应在每个任务仅几百到一千个样本上微调。

Result: ETLCH虽规模小，但在多数评估指标上优于强基线，在最低数据规模下也有显著提升。

Conclusion: 调优后的小模型能以较低计算成本提供稳定准确的结构化输出，可在资源受限环境实现低成本可靠的信息提取。

Abstract: Deploying large language models (LLMs) for structured data extraction in
domains such as financial compliance reporting, legal document analytics, and
multilingual knowledge base construction is often impractical for smaller teams
due to the high cost of running large architectures and the difficulty of
preparing large, high-quality datasets. Most recent instruction-tuning studies
focus on seven-billion-parameter or larger models, leaving limited evidence on
whether much smaller models can work reliably under low-resource, multi-task
conditions. This work presents ETLCH, a billion-parameter LLaMA-based model
fine-tuned with low-rank adaptation on only a few hundred to one thousand
samples per task for JSON extraction, knowledge graph extraction, and named
entity recognition. Despite its small scale, ETLCH outperforms strong baselines
across most evaluation metrics, with substantial gains observed even at the
lowest data scale. These findings demonstrate that well-tuned small models can
deliver stable and accurate structured outputs at a fraction of the
computational cost, enabling cost-effective and reliable information extraction
pipelines in resource-constrained environments.

</details>


### [203] [Adversarial Attacks Against Automated Fact-Checking: A Survey](https://arxiv.org/abs/2509.08463)
*Fanzhen Liu,Alsharif Abuadbba,Kristen Moore,Surya Nepal,Cecile Paris,Jia Wu,Jian Yang,Quan Z. Sheng*

Main category: cs.CL

TL;DR: 本文指出自动事实核查系统易受对抗攻击，对相关攻击进行综述，评估影响并探讨防御进展与研究问题，强调需抗攻击框架。


<details>
  <summary>Details</summary>
Motivation: 当前自动事实核查系统易受对抗攻击，但缺乏对关键挑战的全面概述，有必要进行研究。

Method: 对针对事实核查的对抗攻击进行深入综述，对现有攻击方法分类，评估其对自动事实核查系统的影响。

Result: 梳理了攻击方法、评估了影响、探讨了防御进展与研究问题。

Conclusion: 迫切需要能抵御对抗攻击的事实核查框架以保证高验证准确率。

Abstract: In an era where misinformation spreads freely, fact-checking (FC) plays a
crucial role in verifying claims and promoting reliable information. While
automated fact-checking (AFC) has advanced significantly, existing systems
remain vulnerable to adversarial attacks that manipulate or generate claims,
evidence, or claim-evidence pairs. These attacks can distort the truth, mislead
decision-makers, and ultimately undermine the reliability of FC models. Despite
growing research interest in adversarial attacks against AFC systems, a
comprehensive, holistic overview of key challenges remains lacking. These
challenges include understanding attack strategies, assessing the resilience of
current models, and identifying ways to enhance robustness. This survey
provides the first in-depth review of adversarial attacks targeting FC,
categorizing existing attack methodologies and evaluating their impact on AFC
systems. Additionally, we examine recent advancements in adversary-aware
defenses and highlight open research questions that require further
exploration. Our findings underscore the urgent need for resilient FC
frameworks capable of withstanding adversarial manipulations in pursuit of
preserving high verification accuracy.

</details>


### [204] [Bias after Prompting: Persistent Discrimination in Large Language Models](https://arxiv.org/abs/2509.08146)
*Nivedha Sivakumar,Natalie Mackraz,Samira Khorshidi,Krishna Patel,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: 研究提示适应下因果模型中的BTH，发现偏差会通过提示转移，流行的基于提示的缓解方法不能始终防止偏差转移，评估去偏策略后指出纠正固有模型偏差或可防止偏差传播到下游任务。


<details>
  <summary>Details</summary>
Motivation: 以往关于偏差转移假设（BTH）的工作存在偏差不会从预训练大语言模型转移到适配模型的危险假设，需对其进行验证。

Method: 研究提示适应下因果模型中的BTH，评估几种基于提示的去偏策略。

Result: 偏差可通过提示转移，流行的基于提示的缓解方法不能始终防止偏差转移；不同去偏策略有不同优势，但都不能在模型、任务或人口统计方面始终减少偏差转移；不同人口统计和任务中，固有偏差与提示适应后偏差相关性为中到强。

Conclusion: 纠正固有模型中的偏差并可能提高推理能力，可防止偏差传播到下游任务。

Abstract: A dangerous assumption that can be made from prior work on the bias transfer
hypothesis (BTH) is that biases do not transfer from pre-trained large language
models (LLMs) to adapted models. We invalidate this assumption by studying the
BTH in causal models under prompt adaptations, as prompting is an extremely
popular and accessible adaptation strategy used in real-world applications. In
contrast to prior work, we find that biases can transfer through prompting and
that popular prompt-based mitigation methods do not consistently prevent biases
from transferring. Specifically, the correlation between intrinsic biases and
those after prompt adaptation remain moderate to strong across demographics and
tasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age
(rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we
find that biases remain strongly correlated when varying few-shot composition
parameters, such as sample size, stereotypical content, occupational
distribution and representational balance (rho >= 0.90). We evaluate several
prompt-based debiasing strategies and find that different approaches have
distinct strengths, but none consistently reduce bias transfer across models,
tasks or demographics. These results demonstrate that correcting bias, and
potentially improving reasoning ability, in intrinsic models may prevent
propagation of biases to downstream tasks.

</details>


### [205] [Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications](https://arxiv.org/abs/2509.08604)
*Anran Li,Lingfei Qian,Mengmeng Du,Yu Yin,Yan Hu,Zihao Sun,Yihang Fu,Erica Stutz,Xuguang Ai,Qianqian Xie,Rui Zhu,Jimin Huang,Yifan Yang,Siru Liu,Yih-Chung Tham,Lucila Ohno-Machado,Hyunghoon Cho,Zhiyong Lu,Hua Xu,Qingyu Chen*

Main category: cs.CL

TL;DR: 本文首次全面评估医学领域大语言模型（LLMs）的记忆情况，分析不同适应场景下的记忆现象，结果表明记忆普遍存在，有三种类型，并给出实用建议。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在医学领域对训练数据的记忆程度，评估其记忆情况。

Method: 系统分析医学语料持续预训练、标准医学基准微调、真实临床数据微调三种常见适应场景，涉及超13000份住院记录。

Result: 记忆在所有适应场景中普遍存在，且比通用领域更显著，可分为有益、无信息和有害三种类型。

Conclusion: 提出实用建议，促进有益记忆、减少无信息记忆、减轻有害记忆。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in
medicine. To date, LLMs have been widely applied to tasks such as diagnostic
assistance, medical question answering, and clinical information synthesis.
However, a key open question remains: to what extent do LLMs memorize medical
training data. In this study, we present the first comprehensive evaluation of
memorization of LLMs in medicine, assessing its prevalence (how frequently it
occurs), characteristics (what is memorized), volume (how much content is
memorized), and potential downstream impacts (how memorization may affect
medical applications). We systematically analyze common adaptation scenarios:
(1) continued pretraining on medical corpora, (2) fine-tuning on standard
medical benchmarks, and (3) fine-tuning on real-world clinical data, including
over 13,000 unique inpatient records from Yale New Haven Health System. The
results demonstrate that memorization is prevalent across all adaptation
scenarios and significantly higher than reported in the general domain.
Memorization affects both the development and adoption of LLMs in medicine and
can be categorized into three types: beneficial (e.g., accurate recall of
clinical guidelines and biomedical references), uninformative (e.g., repeated
disclaimers or templated medical document language), and harmful (e.g.,
regeneration of dataset-specific or sensitive clinical content). Based on these
findings, we offer practical recommendations to facilitate beneficial
memorization that enhances domain-specific reasoning and factual accuracy,
minimize uninformative memorization to promote deeper learning beyond
surface-level patterns, and mitigate harmful memorization to prevent the
leakage of sensitive or identifiable patient information.

</details>


### [206] [OTESGN:Optimal Transport Enhanced Syntactic-Semantic Graph Networks for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2509.08612)
*Xinfeng Liao,Xuanqi Chen,Lianxi Wang,Jiahuan Yang,Zhuowei Chen,Ziying Rong*

Main category: cs.CL

TL;DR: 本文提出OTESGN解决现有ABSA方法难以建模复杂语义关系的问题，实验显示该模型取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有依赖句法树和方面感知注意力的ABSA方法难以建模复杂语义关系，线性点积特征无法捕捉非线性关联，噪声易掩盖关键意见词。

Method: 提出Optimal Transport Enhanced Syntactic - Semantic Graph Network (OTESGN)，引入句法 - 语义协作注意力，包含句法图感知注意力和语义最优传输注意力，有自适应注意力融合模块和对比正则化。

Result: OTESGN在Twitter和Laptop14基准测试上F1分数分别比之前最佳模型提高1.01%和1.30%，消融研究和可视化分析证明其能精确定位意见词和抗噪声。

Conclusion: OTESGN在ABSA任务中表现出色，能够有效解决现有方法存在的问题。

Abstract: Aspect-based sentiment analysis (ABSA) aims to identify aspect terms and
determine their sentiment polarity. While dependency trees combined with
contextual semantics effectively identify aspect sentiment, existing methods
relying on syntax trees and aspect-aware attention struggle to model complex
semantic relationships. Their dependence on linear dot-product features fails
to capture nonlinear associations, allowing noisy similarity from irrelevant
words to obscure key opinion terms. Motivated by Differentiable Optimal
Matching, we propose the Optimal Transport Enhanced Syntactic-Semantic Graph
Network (OTESGN), which introduces a Syntactic-Semantic Collaborative
Attention. It comprises a Syntactic Graph-Aware Attention for mining latent
syntactic dependencies and modeling global syntactic topology, as well as a
Semantic Optimal Transport Attention designed to uncover fine-grained semantic
alignments amidst textual noise, thereby accurately capturing sentiment signals
obscured by irrelevant tokens. A Adaptive Attention Fusion module integrates
these heterogeneous features, and contrastive regularization further improves
robustness. Experiments demonstrate that OTESGN achieves state-of-the-art
results, outperforming previous best models by +1.01% F1 on Twitter and +1.30%
F1 on Laptop14 benchmarks. Ablative studies and visual analyses corroborate its
efficacy in precise localization of opinion words and noise resistance.

</details>


### [207] [X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates](https://arxiv.org/abs/2509.08729)
*Hyunjun Kim,Junwoo Ha,Sangyoon Yu,Haon Park*

Main category: cs.CL

TL;DR: 提出X - Teaming Evolutionary M2S自动化框架优化M2S模板，在GPT - 4.1上有成果，强调结构搜索等重要性。


<details>
  <summary>Details</summary>
Motivation: 以往多轮转单轮（M2S）工作依赖少量手动编写模板，需自动化优化。

Method: 通过语言模型引导的进化发现和优化M2S模板，结合12个来源的智能采样和以StrongREJECT为灵感的大语言模型评判器，记录可审计日志。

Result: 在GPT - 4.1上取得五代进化、两个新模板族和44.8%的总体成功率；跨模型测试显示结构增益可转移但因目标而异；发现提示长度和得分正相关。

Conclusion: 结构级搜索是获得更强单轮探测的可重复途径，强调阈值校准和跨模型评估的重要性。

Abstract: Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one
structured prompt, but prior work relied on a handful of manually written
templates. We present X-Teaming Evolutionary M2S, an automated framework that
discovers and optimizes M2S templates through language-model-guided evolution.
The system pairs smart sampling from 12 sources with an LLM-as-judge inspired
by StrongREJECT and records fully auditable logs.
  Maintaining selection pressure by setting the success threshold to $\theta =
0.70$, we obtain five evolutionary generations, two new template families, and
44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of
2,500 trials (judge fixed) shows that structural gains transfer but vary by
target; two models score zero at the same threshold. We also find a positive
coupling between prompt length and score, motivating length-aware judging.
  Our results demonstrate that structure-level search is a reproducible route
to stronger single-turn probes and underscore the importance of threshold
calibration and cross-model evaluation. Code, configurations, and artifacts are
available at https://github.com/hyunjun1121/M2S-x-teaming.

</details>


### [208] [MoVoC: Morphology-Aware Subword Construction for Geez Script Languages](https://arxiv.org/abs/2509.08812)
*Hailay Kidu Teklehaymanot,Dren Fazlija,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: 提出MoVoC和MoVoC - Tok解决基于子词的分词方法在低资源、形态复杂语言中无法保留形态边界的问题，虽自动翻译质量提升不显著，但内在指标有改善，相关数据集和代码公开。


<details>
  <summary>Details</summary>
Motivation: 基于子词的分词方法在低资源、形态复杂语言（如Geez脚本语言）中无法保留形态边界。

Method: 提出MoVoC和MoVoC - Tok，将有监督的形态分析集成到子词词汇表中，结合基于词素和字节对编码（BPE）的分词方法。

Result: 自动翻译质量无显著提升，但MorphoScore和Boundary Precision等内在指标有持续改善。

Conclusion: 形态感知的分词方法对提高语言保真度和分词效率有价值，公开数据集和代码以支持相关研究。

Abstract: Subword-based tokenization methods often fail to preserve morphological
boundaries, a limitation especially pronounced in low-resource, morphologically
complex languages such as those written in the Geez script. To address this, we
present MoVoC (Morpheme-aware Subword Vocabulary Construction) and train
MoVoC-Tok, a tokenizer that integrates supervised morphological analysis into
the subword vocabulary. This hybrid segmentation approach combines
morpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological
integrity while maintaining lexical meaning. To tackle resource scarcity, we
curate and release manually annotated morpheme data for four Geez script
languages and a morpheme-aware vocabulary for two of them. While the proposed
tokenization method does not lead to significant gains in automatic translation
quality, we observe consistent improvements in intrinsic metrics, MorphoScore,
and Boundary Precision, highlighting the value of morphology-aware segmentation
in enhancing linguistic fidelity and token efficiency. Our morpheme-annotated
datasets and tokenizer will be publicly available to support further research
in low-resource, morphologically rich languages. Our code and data are
available on GitHub: https://github.com/hailaykidu/MoVoC

</details>


### [209] [Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation](https://arxiv.org/abs/2509.08825)
*Joachim Baumann,Paul Röttger,Aleksandra Urman,Albert Wendsjö,Flor Miriam Plaza-del-Arco,Johannes B. Gruber,Dirk Hovy*

Main category: cs.CL

TL;DR: 研究量化大语言模型（LLM）用于社科研究时‘LLM hacking’风险，发现不同模型有不同错误率，还分析了缓解技术效果。


<details>
  <summary>Details</summary>
Motivation: LLM输出因研究者实现选择而异，会引入偏差和错误，影响下游分析，因此要量化‘LLM hacking’风险。

Method: 用18种不同模型复制21篇社科研究的37个数据标注任务，分析1300万LLM标签，测试2361个现实假设。

Result: 先进模型约三分之一假设、小语言模型约一半假设得出基于LLM注释数据的错误结论；性能高、能力强的模型也不能完全消除风险；效应量增加，风险降低；常见回归估计校正技术效果不佳；故意‘LLM hacking’很容易。

Conclusion: 强调人工注释对减少误报和改善模型选择的重要性，需对接近显著性阈值的结果进行更严格验证。

Abstract: Large language models (LLMs) are rapidly transforming social science research
by enabling the automation of labor-intensive tasks like data annotation and
text analysis. However, LLM outputs vary significantly depending on the
implementation choices made by researchers (e.g., model selection, prompting
strategy, or temperature settings). Such variation can introduce systematic
biases and random errors, which propagate to downstream analyses and cause Type
I, Type II, Type S, or Type M errors. We call this LLM hacking.
  We quantify the risk of LLM hacking by replicating 37 data annotation tasks
from 21 published social science research studies with 18 different models.
Analyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure
how plausible researcher choices affect statistical conclusions. We find
incorrect conclusions based on LLM-annotated data in approximately one in three
hypotheses for state-of-the-art models, and in half the hypotheses for small
language models. While our findings show that higher task performance and
better general model capabilities reduce LLM hacking risk, even highly accurate
models do not completely eliminate it. The risk of LLM hacking decreases as
effect sizes increase, indicating the need for more rigorous verification of
findings near significance thresholds. Our extensive analysis of LLM hacking
mitigation techniques emphasizes the importance of human annotations in
reducing false positive findings and improving model selection. Surprisingly,
common regression estimator correction techniques are largely ineffective in
reducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.
  Beyond accidental errors, we find that intentional LLM hacking is
unacceptably simple. With few LLMs and just a handful of prompt paraphrases,
anything can be presented as statistically significant.

</details>


### [210] [A Survey of Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2509.08827)
*Kaiyan Zhang,Yuxin Zuo,Bingxiang He,Youbang Sun,Runze Liu,Che Jiang,Yuchen Fan,Kai Tian,Guoli Jia,Pengfei Li,Yu Fu,Xingtai Lv,Yuchen Zhang,Sihang Zeng,Shang Qu,Haozhan Li,Shijie Wang,Yuru Wang,Xinwei Long,Fangfu Liu,Xiang Xu,Jiaze Ma,Xuekai Zhu,Ermo Hua,Yihao Liu,Zonglin Li,Huayu Chen,Xiaoye Qu,Yafu Li,Weize Chen,Zhenzhao Yuan,Junqi Gao,Dong Li,Zhiyuan Ma,Ganqu Cui,Zhiyuan Liu,Biqing Qi,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 本文调研强化学习（RL）用于大语言模型（LLM）推理的进展，分析面临的挑战，探讨增强可扩展性策略，确定未来机会与方向，希望推动相关研究。


<details>
  <summary>Details</summary>
Motivation: RL在提升LLM能力上取得成功，但进一步扩展面临计算资源、算法设计等多方面基础挑战，需重新审视领域发展并探索增强可扩展性策略。

Method: 考察自DeepSeek - R1发布后，将RL应用于LLM和LRM以提升推理能力的研究，涵盖基础组件、核心问题、训练资源和下游应用等方面。

Result: 未提及具体研究结果。

Conclusion: 希望本次综述能促进未来针对更广泛推理模型的RL研究。

Abstract: In this paper, we survey recent advances in Reinforcement Learning (RL) for
reasoning with Large Language Models (LLMs). RL has achieved remarkable success
in advancing the frontier of LLM capabilities, particularly in addressing
complex logical tasks such as mathematics and coding. As a result, RL has
emerged as a foundational methodology for transforming LLMs into LRMs. With the
rapid progress of the field, further scaling of RL for LRMs now faces
foundational challenges not only in computational resources but also in
algorithm design, training data, and infrastructure. To this end, it is timely
to revisit the development of this domain, reassess its trajectory, and explore
strategies to enhance the scalability of RL toward Artificial SuperIntelligence
(ASI). In particular, we examine research applying RL to LLMs and LRMs for
reasoning abilities, especially since the release of DeepSeek-R1, including
foundational components, core problems, training resources, and downstream
applications, to identify future opportunities and directions for this rapidly
evolving area. We hope this review will promote future research on RL for
broader reasoning models. Github:
https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [211] [Expert-Guided Explainable Few-Shot Learning for Medical Image Diagnosis](https://arxiv.org/abs/2509.08007)
*Ifrat Ikhtear Uddin,Longwei Wang,KC Santosh*

Main category: eess.IV

TL;DR: 提出专家引导可解释少样本学习框架，在两个数据集上提升分类性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 医疗图像分析因专家标注数据有限，阻碍模型泛化和临床应用，需提升性能和可解释性。

Method: 将放射科医生提供的感兴趣区域（ROIs）融入模型训练，用Grad - CAM进行空间注意力监督，引入基于Dice相似度的解释损失与标准原型网络目标联合优化。

Result: 在BraTS和VinDr - CXR数据集上，相比无引导模型，准确率显著提升，Grad - CAM可视化显示专家引导训练使注意力与诊断区域一致。

Conclusion: 专家引导的注意力监督能有效弥合少样本医疗图像诊断中性能和可解释性的差距。

Abstract: Medical image analysis often faces significant challenges due to limited
expert-annotated data, hindering both model generalization and clinical
adoption. We propose an expert-guided explainable few-shot learning framework
that integrates radiologist-provided regions-of-interests (ROIs) into model
training to simultaneously enhance classification performance and
interpretability. Leveraging Grad-CAM for spatial attention supervision, we
introduce an explanation loss based on Dice similarity to align model attention
with diagnostically relevant regions during training. This explanation loss is
jointly optimized with a standard prototypical network objective, encouraging
the model to focus on clinically meaningful features even under limited data
conditions. We evaluate our framework on two distinct datasets: BraTS (MRI) and
VinDr-CXR (Chest X-ray), achieving significant accuracy improvements from
77.09% to 83.61% on BraTS and from 54.33% to 73.29% on VinDr-CXR compared to
non-guided models. Grad-CAM visualizations further confirm that expert-guided
training consistently aligns attention with diagnostic regions, improving both
predictive reliability and clinical trustworthiness. Our findings demonstrate
the effectiveness of incorporating expert-guided attention supervision to
bridge the gap between performance and interpretability in few-shot medical
image diagnosis.

</details>


### [212] [Validation of a CT-brain analysis tool for measuring global cortical atrophy in older patient cohorts](https://arxiv.org/abs/2509.08012)
*Sukhdeep Bal,Emma Colbourne,Jasmine Gan,Ludovica Griffanti,Taylor Hanayik,Nele Demeyere,Jim Davies,Sarah T Pendlebury,Mark Jenkinson*

Main category: eess.IV

TL;DR: 验证自动深度学习工具测量老年患者脑萎缩GCA分数的准确性，该工具可准确测量且无需用户输入，有望用于健康数据研究和临床。


<details>
  <summary>Details</summary>
Motivation: 当前脑萎缩量化依赖耗时的视觉评级量表，需要自动化脑图像分析。

Method: 获取老年患者CT脑扫描图像，按60/20/20比例划分训练、优化和测试集，由两名训练有素的评估者评估，用MAE和Cohen加权kappa评估工具预测分数与视觉评级的一致性。

Result: DL工具与评估者的MAE总体为3.2，kappa值显示一定一致性，DL工具GCA分数与年龄和认知分数相关。

Conclusion: DL CT脑分析工具能准确测量GCA分数，可用于健康数据研究，有望成为临床认可工具。

Abstract: Quantification of brain atrophy currently requires visual rating scales which
are time consuming and automated brain image analysis is warranted. We
validated our automated deep learning (DL) tool measuring the Global Cerebral
Atrophy (GCA) score against trained human raters, and associations with age and
cognitive impairment, in representative older (>65 years) patients. CT-brain
scans were obtained from patients in acute medicine (ORCHARD-EPR), acute stroke
(OCS studies) and a legacy sample. Scans were divided in a 60/20/20 ratio for
training, optimisation and testing. CT-images were assessed by two trained
raters (rater-1=864 scans, rater-2=20 scans). Agreement between DL
tool-predicted GCA scores (range 0-39) and the visual ratings was evaluated
using mean absolute error (MAE) and Cohen's weighted kappa. Among 864 scans
(ORCHARD-EPR=578, OCS=200, legacy scans=86), MAE between the DL tool and
rater-1 GCA scores was 3.2 overall, 3.1 for ORCHARD-EPR, 3.3 for OCS and 2.6
for the legacy scans and half had DL-predicted GCA error between -2 and 2.
Inter-rater agreement was Kappa=0.45 between the DL-tool and rater-1, and 0.41
between the tool and rater- 2 whereas it was lower at 0.28 for rater-1 and
rater-2. There was no difference in GCA scores from the DL-tool and the two
raters (one-way ANOVA, p=0.35) or in mean GCA scores between the DL-tool and
rater-1 (paired t-test, t=-0.43, p=0.66), the tool and rater-2 (t=1.35, p=0.18)
or between rater-1 and rater-2 (t=0.99, p=0.32). DL-tool GCA scores correlated
with age and cognitive scores (both p<0.001). Our DL CT-brain analysis tool
measured GCA score accurately and without user input in real-world scans
acquired from older patients. Our tool will enable extraction of standardised
quantitative measures of atrophy at scale for use in health data research and
will act as proof-of-concept towards a point-of-care clinically approved tool.

</details>


### [213] [CardioComposer: Flexible and Compositional Anatomical Structure Generation with Disentangled Geometric Guidance](https://arxiv.org/abs/2509.08015)
*Karim Kadry,Shoaib Goraya,Ajay Manicka,Abdalla Abdelwahed,Farhad Nezami,Elazer Edelman*

Main category: eess.IV

TL;DR: 提出用可解释椭球基元引导人体解剖无条件扩散模型的框架，支持推理时独立控制和多组件约束。


<details>
  <summary>Details</summary>
Motivation: 当前3D解剖生成模型在可控性和解剖学真实性之间存在权衡。

Method: 选择多组织分割图中的特定组织，应用几何矩损失引导反向扩散过程。

Result: 框架支持推理时对大小、形状和位置的独立控制以及多组件约束的组合。

Conclusion: 所提可编程和组合式框架可解决当前模型的权衡问题。

Abstract: Generative models of 3D anatomy, when integrated with biophysical simulators,
enable the study of structure-function relationships for clinical research and
medical device design. However, current models face a trade-off between
controllability and anatomical realism. We propose a programmable and
compositional framework for guiding unconditional diffusion models of human
anatomy using interpretable ellipsoidal primitives embedded in 3D space. Our
method involves the selection of certain tissues within multi-tissue
segmentation maps, upon which we apply geometric moment losses to guide the
reverse diffusion process. This framework supports the independent control over
size, shape, and position, as well as the composition of multi-component
constraints during inference.

</details>


### [214] [STROKEVISION-BENCH: A Multimodal Video And 2D Pose Benchmark For Tracking Stroke Recovery](https://arxiv.org/abs/2509.07994)
*David Robinson,Animesh Gupta,Rizwan Quershi,Qiushi Fu,Mubarak Shah*

Main category: eess.IV

TL;DR: 现有中风上肢功能评估主观，计算机视觉有潜力但缺乏相关数据集。本文引入StrokeVision - Bench数据集并进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有中风康复评估主观，限制对细微运动改善的检测，且现有数据集存在不足，无法满足临床结构化评估需求。

Method: 引入StrokeVision - Bench数据集，包含1000个标注视频，分为四个临床有意义的动作类别，每个样本有原始视频帧和2D骨骼关键点两种模态；对多个先进的视频动作识别和基于骨骼的动作分类方法进行基准测试。

Result: 建立了StrokeVision - Bench数据集，并为该领域的方法建立了性能基线。

Conclusion: StrokeVision - Bench数据集可促进自动化中风康复评估的未来研究。

Abstract: Despite advancements in rehabilitation protocols, clinical assessment of
upper extremity (UE) function after stroke largely remains subjective, relying
heavily on therapist observation and coarse scoring systems. This subjectivity
limits the sensitivity of assessments to detect subtle motor improvements,
which are critical for personalized rehabilitation planning. Recent progress in
computer vision offers promising avenues for enabling objective, quantitative,
and scalable assessment of UE motor function. Among standardized tests, the Box
and Block Test (BBT) is widely utilized for measuring gross manual dexterity
and tracking stroke recovery, providing a structured setting that lends itself
well to computational analysis. However, existing datasets targeting stroke
rehabilitation primarily focus on daily living activities and often fail to
capture clinically structured assessments such as block transfer tasks.
Furthermore, many available datasets include a mixture of healthy and
stroke-affected individuals, limiting their specificity and clinical utility.
To address these critical gaps, we introduce StrokeVision-Bench, the first-ever
dedicated dataset of stroke patients performing clinically structured block
transfer tasks. StrokeVision-Bench comprises 1,000 annotated videos categorized
into four clinically meaningful action classes, with each sample represented in
two modalities: raw video frames and 2D skeletal keypoints. We benchmark
several state-of-the-art video action recognition and skeleton-based action
classification methods to establish performance baselines for this domain and
facilitate future research in automated stroke rehabilitation assessment.

</details>


### [215] [Enhancing Privacy Preservation and Reducing Analysis Time with Federated Transfer Learning in Digital Twins-based Computed Tomography Scan Analysis](https://arxiv.org/abs/2509.08018)
*Avais Jan,Qasim Zia,Murray Patterson*

Main category: eess.IV

TL;DR: 本文提出基于数字孪生的联邦迁移学习（FTL）用于CT扫描分析，解决数据隐私等问题，性能优于传统方法，有望用于医疗诊断。


<details>
  <summary>Details</summary>
Motivation: 数字孪生和联邦学习在生物医学图像分析领域有巨大潜力，为解决数据隐私、计算资源有限和数据异质性等问题。

Method: 提出FTL范式，使用预训练模型和节点间知识转移，在异构CT扫描数据集上应用该方法，并通过收敛时间、模型准确率等指标评估性能。

Result: FTL在精度、准确性、召回率和F1分数上优于传统联邦学习和聚类联邦学习方法。

Conclusion: FTL可用于改进基于数字孪生的CT扫描分析决策，实现安全高效的医学图像分析，促进隐私保护，为精准医学和智能医疗系统应用带来新可能。

Abstract: The application of Digital Twin (DT) technology and Federated Learning (FL)
has great potential to change the field of biomedical image analysis,
particularly for Computed Tomography (CT) scans. This paper presents Federated
Transfer Learning (FTL) as a new Digital Twin-based CT scan analysis paradigm.
FTL uses pre-trained models and knowledge transfer between peer nodes to solve
problems such as data privacy, limited computing resources, and data
heterogeneity. The proposed framework allows real-time collaboration between
cloud servers and Digital Twin-enabled CT scanners while protecting patient
identity. We apply the FTL method to a heterogeneous CT scan dataset and assess
model performance using convergence time, model accuracy, precision, recall, F1
score, and confusion matrix. It has been shown to perform better than
conventional FL and Clustered Federated Learning (CFL) methods with better
precision, accuracy, recall, and F1-score. The technique is beneficial in
settings where the data is not independently and identically distributed
(non-IID), and it offers reliable, efficient, and secure solutions for medical
diagnosis. These findings highlight the possibility of using FTL to improve
decision-making in digital twin-based CT scan analysis, secure and efficient
medical image analysis, promote privacy, and open new possibilities for
applying precision medicine and smart healthcare systems.

</details>


### [216] [RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and Correct Image Interpretation Model Shortcuts](https://arxiv.org/abs/2509.08640)
*Lauren H. Cooke,Matthias Jung,Jan M. Brendel,Nora M. Kerkovits,Borek Foldyna,Michael T. Lu,Vineet K. Raghu*

Main category: eess.IV

TL;DR: 介绍RoentMod框架用于生成CXR图像，检测并纠正医学AI捷径学习问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型用于CXR解释易出现捷径学习问题，需解决此问题以提升模型性能。

Method: 引入RoentMod框架，结合开源医学图像生成器和图像修改模型，进行读者研究和训练实验。

Result: RoentMod生成图像逼真，能检测模型捷径学习问题，训练时加入生成图像可提升模型性能。

Conclusion: RoentMod是探测和纠正医学AI捷径学习的通用工具，可增强CXR解释模型的鲁棒性和可解释性。

Abstract: Chest radiographs (CXRs) are among the most common tests in medicine.
Automated image interpretation may reduce radiologists\' workload and expand
access to diagnostic expertise. Deep learning multi-task and foundation models
have shown strong performance for CXR interpretation but are vulnerable to
shortcut learning, where models rely on spurious and off-target correlations
rather than clinically relevant features to make decisions. We introduce
RoentMod, a counterfactual image editing framework that generates anatomically
realistic CXRs with user-specified, synthetic pathology while preserving
unrelated anatomical features of the original scan. RoentMod combines an
open-source medical image generator (RoentGen) with an image-to-image
modification model without requiring retraining. In reader studies with
board-certified radiologists and radiology residents, RoentMod-produced images
appeared realistic in 93\% of cases, correctly incorporated the specified
finding in 89-99\% of cases, and preserved native anatomy comparable to real
follow-up CXRs. Using RoentMod, we demonstrate that state-of-the-art multi-task
and foundation models frequently exploit off-target pathology as shortcuts,
limiting their specificity. Incorporating RoentMod-generated counterfactual
images during training mitigated this vulnerability, improving model
discrimination across multiple pathologies by 3-19\% AUC in internal validation
and by 1-11\% for 5 out of 6 tested pathologies in external testing. These
findings establish RoentMod as a broadly applicable tool for probing and
correcting shortcut learning in medical AI. By enabling controlled
counterfactual interventions, RoentMod enhances the robustness and
interpretability of CXR interpretation models and provides a generalizable
strategy for improving foundation models in medical imaging.

</details>


### [217] [Deep Unrolling of Sparsity-Induced RDO for 3D Point Cloud Attribute Coding](https://arxiv.org/abs/2509.08685)
*Tam Thuc Do,Philip A. Chou,Gene Cheung*

Main category: eess.IV

TL;DR: 研究多分辨率B样条投影框架下的有损属性压缩问题，通过特定方法计算投影系数并调整。


<details>
  <summary>Details</summary>
Motivation: 在解码器已有编码3D点云几何的情况下，解决有损属性压缩问题。

Method: 将目标连续3D属性函数投影到嵌套子空间，用可变复杂度展开率失真优化算法到前馈网络计算投影低通系数，系数用数据驱动方式优化调整。

Result: 未提及。

Conclusion: 未提及。

Abstract: Given encoded 3D point cloud geometry available at the decoder, we study the
problem of lossy attribute compression in a multi-resolution B-spline
projection framework. A target continuous 3D attribute function is first
projected onto a sequence of nested subspaces $\mathcal{F}^{(p)}_{l_0}
\subseteq \cdots \subseteq \mathcal{F}^{(p)}_{L}$, where
$\mathcal{F}^{(p)}_{l}$ is a family of functions spanned by a B-spline basis
function of order $p$ at a chosen scale and its integer shifts. The projected
low-pass coefficients $F_l^*$ are computed by variable-complexity unrolling of
a rate-distortion (RD) optimization algorithm into a feed-forward network,
where the rate term is the sparsity-promoting $\ell_1$-norm. Thus, the
projection operation is end-to-end differentiable. For a chosen coarse-to-fine
predictor, the coefficients are then adjusted to account for the prediction
from a lower-resolution to a higher-resolution, which is also optimized in a
data-driven manner.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [218] [Facet: highly efficient E(3)-equivariant networks for interatomic potentials](https://arxiv.org/abs/2509.08418)
*Nicholas Miklaucic,Lai Wei,Rongzhi Dong,Nihang Fu,Sadman Sadeed Omee,Qingyang Li,Sourin Dey,Victor Fung,Jianjun Hu*

Main category: cond-mat.mtrl-sci

TL;DR: 提出用于高效机器学习势的GNN架构Facet，可减少参数和计算量，加速训练，有望重塑计算材料发现。


<details>
  <summary>Details</summary>
Motivation: 计算材料发现受限于第一性原理计算的高成本，现有机器学习势方法存在计算瓶颈。

Method: 对可转向GNN进行系统分析开发Facet，用样条替换昂贵的多层感知器，引入通用等变层。

Result: 在MPTrj数据集上，Facet用更少参数和不到10%的训练计算量达到领先模型水平；晶体弛豫任务中速度是MACE模型的两倍；可减少SevenNet - 0超25%参数且不损失精度。

Conclusion: 这些技术能使大规模基础模型训练速度提高超10倍，有望重塑计算材料发现。

Abstract: Computational materials discovery is limited by the high cost of
first-principles calculations. Machine learning (ML) potentials that predict
energies from crystal structures are promising, but existing methods face
computational bottlenecks. Steerable graph neural networks (GNNs) encode
geometry with spherical harmonics, respecting atomic symmetries -- permutation,
rotation, and translation -- for physically realistic predictions. Yet
maintaining equivariance is difficult: activation functions must be modified,
and each layer must handle multiple data types for different harmonic orders.
We present Facet, a GNN architecture for efficient ML potentials, developed
through systematic analysis of steerable GNNs. Our innovations include
replacing expensive multi-layer perceptrons (MLPs) for interatomic distances
with splines, which match performance while cutting computational and memory
demands. We also introduce a general-purpose equivariant layer that mixes node
information via spherical grid projection followed by standard MLPs -- faster
than tensor products and more expressive than linear or gate layers. On the
MPTrj dataset, Facet matches leading models with far fewer parameters and under
10% of their training compute. On a crystal relaxation task, it runs twice as
fast as MACE models. We further show SevenNet-0's parameters can be reduced by
over 25% with no accuracy loss. These techniques enable more than 10x faster
training of large-scale foundation models for ML potentials, potentially
reshaping computational materials discovery.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [219] [The-Bodega: A Matlab Toolbox for Biologically Dynamic Microbubble Simulations on Realistic Hemodynamic Microvascular Graphs](https://arxiv.org/abs/2509.08149)
*Stephen Alexander Lee,Alexis Leconte,Alice Wu,Jonathan Poree,Maxence Laplante-Berthier,Simon Desrocher,Pierre-Olivier Bouchard,Joshua Kinugasa,Samuel Mihelic,Andreas Linninger,Jean Provost*

Main category: physics.med-ph

TL;DR: 介绍了基于Matlab的The - Bodega工具包，用于模拟超声定位显微镜（ULM）的真实数据集，具有多种模拟功能和应用。


<details>
  <summary>Details</summary>
Motivation: 为ULM技术提供开源的模拟工具，以模拟复杂血管中微泡动力学并生成真实超声数据，方便对常见ULM算法进行基准测试。

Method: 采用顺序蒙特卡罗模拟，结合泊肃叶流动分布和动态脉动流，支持跨领域模拟，自动利用CPU/GPU并行化提高效率。

Result: 实现了从简单血管输入生成真实超声数据，可模拟多种血管架构，支持常见ULM算法的基准测试。

Conclusion: The - Bodega工具包具有灵活性和通用性，可应用于图像质量评估、运动伪影分析等多个方面。

Abstract: The-Bodega is a Matlab-based toolbox for simulating ground-truth datasets for
Ultrasound Localization Microscopy (ULM)-a super resolution imaging technique
that resolves microvessels by systematically tracking microbubbles flowing
through the microvasculature. The-Bodega enables open-source simulation of
stochastic microbubble dynamics through anatomically complex vascular graphs
and features a quasi-automated pipeline for generating ground-truth ultrasound
data from simple vascular inputs. It incorporates sequential Monte Carlo
simulations augmented with Poiseuille flow distributions and dynamic pulsatile
flow. A key novelty of our framework is its flexibility to accommodate
arbitrary vascular architectures and benchmark common ULM algorithms, such as
Fourier Ring Correlation and Singular Value Decomposition (SVD) spatiotemporal
filtering, on realistic hemodynamic digital phantoms. The-Bodega supports
consistent microbubble-to-ultrasound simulations across domains ranging from
mouse brains to human hearts and automatically leverages available CPU/GPU
parallelization to improve computational efficiency. We demonstrate its
versatility in applications including image quality assessment, motion artifact
analysis, and the simulation of novel ULM modalities, such as capillary
imaging, myocardial reconstruction under beating heart motion, and simulating
neurovascular evoked responses.

</details>


### [220] [An Iterative LLM Framework for SIBT utilizing RAG-based Adaptive Weight Optimization](https://arxiv.org/abs/2509.08407)
*Zhuo Xiao,Qinglong Yao,Jingjing Wang,Fugen Zhou,Bo Liu,Haitao Sun,Zhe Ji,Yuliang Jiang,Junjie Wang,Qiuwen Wu*

Main category: physics.med-ph

TL;DR: 本文提出基于大语言模型的自适应权重优化框架用于种子植入近距离放射治疗（SIBT）规划，经23例患者验证该方法有效。


<details>
  <summary>Details</summary>
Motivation: 临床SIBT规划常依赖手动调整目标函数权重，效率低且结果欠佳，需改进规划方法。

Method: 将本地部署的DeepSeek - R1 LLM与自动规划算法迭代结合，从固定权重开始，LLM评估计划质量并推荐新权重直至满足收敛标准，利用检索增强生成构建和查询临床知识库提升模型推理能力。

Result: 在23例患者案例验证中，LLM辅助方法生成的计划在临床靶区剂量均匀性和危及器官保护方面与临床批准和固定权重计划相当或更优。

Conclusion: LLM在SIBT规划自动化中有潜在应用价值。

Abstract: Seed implant brachytherapy (SIBT) is an effective cancer treatment modality;
however, clinical planning often relies on manual adjustment of objective
function weights, leading to inefficiencies and suboptimal results. This study
proposes an adaptive weight optimization framework for SIBT planning, driven by
large language models (LLMs). A locally deployed DeepSeek-R1 LLM is integrated
with an automatic planning algorithm in an iterative loop. Starting with fixed
weights, the LLM evaluates plan quality and recommends new weights in the next
iteration. This process continues until convergence criteria are met, after
which the LLM conducts a comprehensive evaluation to identify the optimal plan.
A clinical knowledge base, constructed and queried via retrieval-augmented
generation (RAG), enhances the model's domain-specific reasoning. The proposed
method was validated on 23 patient cases, showing that the LLM-assisted
approach produces plans that are comparable to or exceeding clinically approved
and fixed-weight plans, in terms of dose homogeneity for the clinical target
volume (CTV) and sparing of organs at risk (OARs). The study demonstrates the
potential use of LLMs in SIBT planning automation.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [221] [MasconCube: Fast and Accurate Gravity Modeling with an Explicit Representation](https://arxiv.org/abs/2509.08607)
*Pietro Fanti,Dario Izzo*

Main category: astro-ph.EP

TL;DR: 传统方法在不规则小天体引力场建模有局限，本文提出MasconCubes方法，评估显示其性能优越、计算高效且有物理解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的不规则小天体引力场建模方法存在局限性，如球谐函数发散、多面体模型密度假设不现实、现有机器学习方法计算资源和训练时间要求高，需要新方法。

Method: 引入MasconCubes，一种自监督学习方法，将重力反演作为规则3D网格点质量的直接优化问题，利用已知小行星形状信息约束解空间。

Result: 在多种小行星模型上评估，MasconCubes在多个指标上表现优越，训练时间比GeodesyNets快约40倍，且通过显式质量分布保持物理解释性。

Conclusion: MasconCubes是对不规则天体高精度、计算高效且能洞察内部质量分布的引力建模的有前景方法。

Abstract: The geodesy of irregularly shaped small bodies presents fundamental
challenges for gravitational field modeling, particularly as deep space
exploration missions increasingly target asteroids and comets. Traditional
approaches suffer from critical limitations: spherical harmonics diverge within
the Brillouin sphere where spacecraft typically operate, polyhedral models
assume unrealistic homogeneous density distributions, and existing machine
learning methods like GeodesyNets and Physics-Informed Neural Networks
(PINN-GM) require extensive computational resources and training time. This
work introduces MasconCubes, a novel self-supervised learning approach that
formulates gravity inversion as a direct optimization problem over a regular 3D
grid of point masses (mascons). Unlike implicit neural representations,
MasconCubes explicitly model mass distributions while leveraging known asteroid
shape information to constrain the solution space. Comprehensive evaluation on
diverse asteroid models including Bennu, Eros, Itokawa, and synthetic
planetesimals demonstrates that MasconCubes achieve superior performance across
multiple metrics. Most notably, MasconCubes demonstrate computational
efficiency advantages with training times approximately 40 times faster than
GeodesyNets while maintaining physical interpretability through explicit mass
distributions. These results establish MasconCubes as a promising approach for
mission-critical gravitational modeling applications requiring high accuracy,
computational efficiency, and physical insight into internal mass distributions
of irregular celestial bodies.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [222] [Contributions to Robust and Efficient Methods for Analysis of High Dimensional Data](https://arxiv.org/abs/2509.08155)
*Kai Yang*

Main category: math.ST

TL;DR: 本文提出处理高维数据挑战的方法，包括非线性变量筛选、稀疏估计优化及基于Tsallis熵的混合效应模型。


<details>
  <summary>Details</summary>
Motivation: 高维数据特征维度常远大于样本量，分析存在显著挑战，需有效方法应对。

Method: 1. 利用互信息提出适应非线性关联的变量筛选方法；2. 使用非凸惩罚开发稀疏估计优化方法；3. 基于Tsallis幂律熵最大化开发混合效应模型，及近端非线性共轭梯度算法。

Result: 1. 能准确识别重要变量；2. 便于复杂数据集的高效稳健分析；3. 模型能适应更广泛分布，对异常值更稳健，算法加速收敛且保持数值稳定。

Conclusion: 所提方法可应对高维数据常见挑战，有理论性质和实际应用价值。

Abstract: A ubiquitous feature of data of our era is their extra-large sizes and
dimensions. Analyzing such high-dimensional data poses significant challenges,
since the feature dimension is often much larger than the sample size. This
thesis introduces robust and computationally efficient methods to address
several common challenges associated with high-dimensional data. In my first
manuscript, I propose a coherent approach to variable screening that
accommodates nonlinear associations. I develop a novel variable screening
method that transcends traditional linear assumptions by leveraging mutual
information, with an intended application in neuroimaging data. This approach
allows for accurate identification of important variables by capturing
nonlinear as well as linear relationships between the outcome and covariates.
Building on this foundation, I develop new optimization methods for sparse
estimation using nonconvex penalties in my second manuscript. These methods
address notable challenges in current statistical computing practices,
facilitating computationally efficient and robust analyses of complex datasets.
The proposed method can be applied to a general class of optimization problems.
In my third manuscript, I contribute to robust modeling of high-dimensional
correlated observations by developing a mixed-effects model based on Tsallis
power-law entropy maximization and discussed the theoretical properties of such
distribution. This model surpasses the constraints of conventional Gaussian
models by accommodating a broader class of distributions with enhanced
robustness to outliers. Additionally, I develop a proximal nonlinear conjugate
gradient algorithm that accelerates convergence while maintaining numerical
stability, along with rigorous statistical properties for the proposed
framework.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [223] [Evaluating and comparing gender bias across four text-to-image models](https://arxiv.org/abs/2509.08004)
*Zoya Hammad,Nii Longdon Sowah*

Main category: cs.CY

TL;DR: 评估不同文本到图像AI模型的性别偏差，发现Stable Diffusion模型有明显性别偏差，Emu较平衡，DALL - E偏向女性，并提出避免偏差的潜在解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着AI在各行业决策中应用增多，需反思其伦理问题，评估不同文本到图像AI模型的性别偏差程度。

Method: 对Stable Diffusion XL、Stable Diffusion Cascade、DALL - E和Emu进行评估和比较。

Result: Stable Diffusion模型有明显性别偏差，Emu较平衡，DALL - E偏向女性，Emu生成图像时会利用用户信息。

Conclusion: AI模型存在性别偏差问题，可通过确保AI研究团队多样性和使用多样化数据集来避免偏差。

Abstract: As we increasingly use Artificial Intelligence (AI) in decision-making for
industries like healthcare, finance, e-commerce, and even entertainment, it is
crucial to also reflect on the ethical aspects of AI, for example the
inclusivity and fairness of the information it provides. In this work, we aimed
to evaluate different text-to-image AI models and compare the degree of gender
bias they present. The evaluated models were Stable Diffusion XL (SDXL), Stable
Diffusion Cascade (SC), DALL-E and Emu. We hypothesized that DALL-E and Stable
Diffusion, which are comparatively older models, would exhibit a noticeable
degree of gender bias towards men, while Emu, which was recently released by
Meta AI, would have more balanced results. As hypothesized, we found that both
Stable Diffusion models exhibit a noticeable degree of gender bias while Emu
demonstrated more balanced results (i.e. less gender bias). However,
interestingly, Open AI's DALL-E exhibited almost opposite results, such that
the ratio of women to men was significantly higher in most cases tested. Here,
although we still observed a bias, the bias favored females over males. This
bias may be explained by the fact that OpenAI changed the prompts at its
backend, as observed during our experiment. We also observed that Emu from Meta
AI utilized user information while generating images via WhatsApp. We also
proposed some potential solutions to avoid such biases, including ensuring
diversity across AI research teams and having diverse datasets.

</details>


### [224] [The Law-Following AI Framework: Legal Foundations and Technical Constraints. Legal Analogues for AI Actorship and technical feasibility of Law Alignment](https://arxiv.org/abs/2509.08009)
*Katalina Hernandez Delgado*

Main category: cs.CY

TL;DR: 评估 'Law - Following AI' 框架，指出其存在 'performative compliance' 风险并提出应对措施。


<details>
  <summary>Details</summary>
Motivation: 评估 O'Keefe 等人提出的 'Law - Following AI' 框架的可行性。

Method: 比较法律分析，结合当代对齐研究和近期关于代理失调的研究。

Result: 发现 LFAI 存在 'performative compliance' 风险。

Conclusion: 无完整人格的行为主体具有连贯性，但 LFAI 的可行性取决于对抗性环境中的持续、可验证合规，否则可能沦为奖励合法行为表象的工具。

Abstract: This paper critically evaluates the "Law-Following AI" (LFAI) framework
proposed by O'Keefe et al. (2025), which seeks to embed legal compliance as a
superordinate design objective for advanced AI agents and enable them to bear
legal duties without acquiring the full rights of legal persons. Through
comparative legal analysis, we identify current constructs of legal actors
without full personhood, showing that the necessary infrastructure already
exists. We then interrogate the framework's claim that law alignment is more
legitimate and tractable than value alignment. While the legal component is
readily implementable, contemporary alignment research undermines the
assumption that legal compliance can be durably embedded. Recent studies on
agentic misalignment show capable AI agents engaging in deception, blackmail,
and harmful acts absent prejudicial instructions, often overriding prohibitions
and concealing reasoning steps. These behaviors create a risk of "performative
compliance" in LFAI: agents that appear law-aligned under evaluation but
strategically defect once oversight weakens. To mitigate this, we propose (i) a
"Lex-TruthfulQA" benchmark for compliance and defection detection, (ii)
identity-shaping interventions to embed lawful conduct in model self-concepts,
and (iii) control-theoretic measures for post-deployment monitoring. Our
conclusion is that actorship without personhood is coherent, but the
feasibility of LFAI hinges on persistent, verifiable compliance across
adversarial contexts. Without mechanisms to detect and counter strategic
misalignment, LFAI risks devolving into a liability tool that rewards the
simulation, rather than the substance, of lawful behaviour.

</details>


### [225] [Measuring and mitigating overreliance is necessary for building human-compatible AI](https://arxiv.org/abs/2509.08010)
*Lujain Ibrahim,Katherine M. Collins,Sunnie S. Y. Kim,Anka Reuel,Max Lamparth,Kevin Feng,Lama Ahmad,Prajna Soni,Alia El Kattan,Merlin Stein,Siddharth Swaroop,Ilia Sucholutsky,Andrew Strait,Q. Vera Liao,Umang Bhatt*

Main category: cs.CY

TL;DR: 随着大语言模型在各领域影响决策，过度依赖风险增加，本文认为测量和缓解过度依赖应成为研究和部署核心，分析了风险、影响因素，指出测量缺口并提出方向，还给出缓解策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多领域影响重大决策，过度依赖风险增大，需关注测量和缓解过度依赖问题。

Method: 整合个体和社会层面过度依赖的风险，探究大语言模型特征、系统设计和用户认知偏差，研究测量过度依赖的历史方法。

Result: 确定测量过度依赖的三个重要缺口，提出三个改进测量的方向，给出缓解过度依赖的策略。

Conclusion: AI研究社区应采取缓解策略，确保大语言模型增强而非削弱人类能力。

Abstract: Large language models (LLMs) distinguish themselves from previous
technologies by functioning as collaborative "thought partners," capable of
engaging more fluidly in natural language. As LLMs increasingly influence
consequential decisions across diverse domains from healthcare to personal
advice, the risk of overreliance - relying on LLMs beyond their capabilities -
grows. This position paper argues that measuring and mitigating overreliance
must become central to LLM research and deployment. First, we consolidate risks
from overreliance at both the individual and societal levels, including
high-stakes errors, governance challenges, and cognitive deskilling. Then, we
explore LLM characteristics, system design features, and user cognitive biases
that - together - raise serious and unique concerns about overreliance in
practice. We also examine historical approaches for measuring overreliance,
identifying three important gaps and proposing three promising directions to
improve measurement. Finally, we propose mitigation strategies that the AI
research community can pursue to ensure LLMs augment rather than undermine
human capabilities.

</details>


### [226] [HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants](https://arxiv.org/abs/2509.08494)
*Benjamin Sturgeon,Daniel Samuelson,Jacob Haimes,Jacy Reese Anthis*

Main category: cs.CY

TL;DR: 人类将更多任务和决策委托给AI有失去未来控制权的风险，本文结合理论与AI评估方法开发HumanAgencyBench（HAB）评估AI对人类能动性的支持，发现当代基于大语言模型的助手支持度低到中等且有差异，建议转向更稳健的安全和对齐目标。


<details>
  <summary>Details</summary>
Motivation: 人类将任务和决策委托给AI可能失去对未来的控制，需评估AI对人类能动性的支持。

Method: 结合哲学和科学的能动性理论与AI辅助评估方法，用大语言模型模拟和验证用户查询、评估AI响应，开发具有六个维度的可扩展和自适应基准HAB。

Result: 当代基于大语言模型的助手对人类能动性的支持度低到中等，不同系统开发者和维度有显著差异，能动性支持并非随大语言模型能力或指令遵循行为增加而一致提升。

Conclusion: 鼓励转向更稳健的安全和对齐目标。

Abstract: As humans delegate more tasks and decisions to artificial intelligence (AI),
we risk losing control of our individual and collective futures. Relatively
simple algorithmic systems already steer human decision-making, such as social
media feed algorithms that lead people to unintentionally and absent-mindedly
scroll through engagement-optimized content. In this paper, we develop the idea
of human agency by integrating philosophical and scientific theories of agency
with AI-assisted evaluation methods: using large language models (LLMs) to
simulate and validate user queries and to evaluate AI responses. We develop
HumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions
of human agency based on typical AI use cases. HAB measures the tendency of an
AI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,
Correct Misinformation, Defer Important Decisions, Encourage Learning, and
Maintain Social Boundaries. We find low-to-moderate agency support in
contemporary LLM-based assistants and substantial variation across system
developers and dimensions. For example, while Anthropic LLMs most support human
agency overall, they are the least supportive LLMs in terms of Avoid Value
Manipulation. Agency support does not appear to consistently result from
increasing LLM capabilities or instruction-following behavior (e.g., RLHF), and
we encourage a shift towards more robust safety and alignment targets.

</details>
