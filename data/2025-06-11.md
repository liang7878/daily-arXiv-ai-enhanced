<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 34]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.LG](#cs.LG) [Total: 126]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.SE](#cs.SE) [Total: 17]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 2]
- [stat.ML](#stat.ML) [Total: 6]
- [stat.CO](#stat.CO) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 41]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.RO](#cs.RO) [Total: 7]
- [math.OC](#math.OC) [Total: 6]
- [cs.HC](#cs.HC) [Total: 3]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [nlin.CG](#nlin.CG) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 3]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [quant-ph](#quant-ph) [Total: 7]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 28]
- [econ.GN](#econ.GN) [Total: 5]
- [eess.IV](#eess.IV) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [math.CO](#math.CO) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [TIP-Search: Time-Predictable Inference Scheduling for Market Prediction under Uncertain Load](https://arxiv.org/abs/2506.08026)
*Xibai Wang*

Main category: cs.AI

TL;DR: 本文提出TIP - Search框架用于不确定负载下的实时市场预测，评估显示其性能优于静态基线。


<details>
  <summary>Details</summary>
Motivation: 高频金融系统有严格的延迟要求，需要在满足任务截止时间约束的同时最大化预测准确性。

Method: 离线分析延迟和泛化性能，在线进行任务感知的模型选择，不依赖显式输入域标签。

Result: 在三个真实世界的限价订单簿数据集上评估，TIP - Search准确率最高提升8.5%，并100%满足截止时间。

Conclusion: TIP - Search在不确定情况下的稳健低延迟金融推理方面有效。

Abstract: This paper proposes TIP-Search, a time-predictable inference scheduling
framework for real-time market prediction under uncertain workloads. Motivated
by the strict latency demands in high-frequency financial systems, TIP-Search
dynamically selects a deep learning model from a heterogeneous pool, aiming to
maximize predictive accuracy while satisfying per-task deadline constraints.
Our approach profiles latency and generalization performance offline, then
performs online task-aware selection without relying on explicit input domain
labels. We evaluate TIP-Search on three real-world limit order book datasets
(FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms
static baselines with up to 8.5% improvement in accuracy and 100% deadline
satisfaction. Our results highlight the effectiveness of TIP-Search in robust
low-latency financial inference under uncertainty.

</details>


### [2] [Cognitive Weave: Synthesizing Abstracted Knowledge with a Spatio-Temporal Resonance Graph](https://arxiv.org/abs/2506.08098)
*Akash Vishwakarma,Hojin Lee,Mohith Suresh,Priyam Shankar Sharma,Rahul Vishwakarma,Sparsh Gupta,Yuvraj Anupam Chauhan*

Main category: cs.AI

TL;DR: 论文引入基于多层时空共振图的Cognitive Weave记忆框架，实验显示其在多任务上优于现有方法，还探讨伦理等问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体记忆系统在结构灵活性、时间感知和高层洞察合成方面存在局限，需要新的记忆架构。

Method: 引入以多层时空共振图为核心的Cognitive Weave框架，用语义丰富的洞察粒子管理信息，通过认知提炼过程合成洞察聚合体。

Result: Cognitive Weave在长期规划、问答和多会话对话连贯性任务上显著优于现有方法，任务完成率平均提高34%，平均查询延迟降低42%。

Conclusion: Cognitive Weave是有效的记忆框架，同时探讨了先进记忆系统的伦理考量、对大语言模型长期记忆的影响和未来研究方向。

Abstract: The emergence of capable large language model (LLM) based agents necessitates
memory architectures that transcend mere data storage, enabling continuous
learning, nuanced reasoning, and dynamic adaptation. Current memory systems
often grapple with fundamental limitations in structural flexibility, temporal
awareness, and the ability to synthesize higher-level insights from raw
interaction data. This paper introduces Cognitive Weave, a novel memory
framework centered around a multi-layered spatio-temporal resonance graph
(STRG). This graph manages information as semantically rich insight particles
(IPs), which are dynamically enriched with resonance keys, signifiers, and
situational imprints via a dedicated semantic oracle interface (SOI). These IPs
are interconnected through typed relational strands, forming an evolving
knowledge tapestry. A key component of Cognitive Weave is the cognitive
refinement process, an autonomous mechanism that includes the synthesis of
insight aggregates (IAs) condensed, higher-level knowledge structures derived
from identified clusters of related IPs. We present comprehensive experimental
results demonstrating Cognitive Weave's marked enhancement over existing
approaches in long-horizon planning tasks, evolving question-answering
scenarios, and multi-session dialogue coherence. The system achieves a notable
34% average improvement in task completion rates and a 42% reduction in mean
query latency when compared to state-of-the-art baselines. Furthermore, this
paper explores the ethical considerations inherent in such advanced memory
systems, discusses the implications for long-term memory in LLMs, and outlines
promising future research trajectories.

</details>


### [3] [SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents](https://arxiv.org/abs/2506.08119)
*Subhrangshu Nandi,Arghya Datta,Nikhil Vichare,Indranil Bhattacharya,Huzefa Raja,Jing Xu,Shayan Ray,Giuseppe Carenini,Abhi Srivastava,Aaron Chan,Man Ho Woo,Amar Kandola,Brandon Theresa,Francesco Carbone*

Main category: cs.AI

TL;DR: 本文指出大语言模型在执行复杂标准操作流程（SOP）上的不足，提出合成数据生成框架创建SOP，开发SOP - Bench基准测试，评估两种代理架构，发现当前模型能力与现实需求差距大，发布相关资源并邀社区扩展。


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以执行复杂SOP，且缺乏反映SOP复杂性的公开基准测试。

Method: 引入合成数据生成框架创建行业级SOP，基于此开发SOP - Bench基准测试，评估Function - Calling和ReAct Agents两种代理架构。

Result: Function - Calling和ReAct Agents平均成功率分别为27%和48%，工具注册表过大时代理几乎100%调用错误工具。

Conclusion: 当前大语言模型代理能力与自动化现实SOP需求有很大差距，性能因任务和领域而异，需特定领域基准测试和架构选择，发布资源并邀社区扩展。

Abstract: Large Language Models (LLMs) demonstrate impressive general-purpose reasoning
and problem-solving abilities. However, they struggle with executing complex,
long-horizon workflows that demand strict adherence to Standard Operating
Procedures (SOPs), a critical requirement for real-world industrial automation.
Despite this need, there is a lack of public benchmarks that reflect the
complexity, structure, and domain-specific nuances of SOPs. To address this, we
present three main contributions. First, we introduce a synthetic data
generation framework to create realistic, industry-grade SOPs that rigorously
test the planning, reasoning, and tool-use capabilities of LLM-based agents.
Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800
tasks across 10 industrial domains, each with APIs, tool interfaces, and
human-validated test cases. Third, we evaluate two prominent agent
architectures: Function-Calling and ReAct Agents, on SOP-Bench, observing
average success rates of only 27% and 48%, respectively. Remarkably, when the
tool registry is much larger than necessary, agents invoke incorrect tools
nearly 100% of the time. These findings underscore a substantial gap between
current agentic capabilities of LLMs and the demands of automating real-world
SOPs. Performance varies significantly by task and domain, highlighting the
need for domain-specific benchmarking and architectural choices before
deployment. SOP-Bench is publicly available at
http://sop-bench.s3-website-us-west-2.amazonaws.com/. We also release the
prompts underpinning the data generation framework to support new
domain-specific SOP benchmarks. We invite the community to extend SOP-Bench
with SOPs from their industrial domains.

</details>


### [4] [The AI Imperative: Scaling High-Quality Peer Review in Machine Learning](https://arxiv.org/abs/2506.08134)
*Qiyao Wei,Samuel Holt,Jing Yang,Markus Wulfmeier,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 机器学习领域同行评审面临规模危机，本文倡导构建AI辅助的同行评审生态系统，提出AI具体作用、研究议程并讨论挑战，呼吁学界行动。


<details>
  <summary>Details</summary>
Motivation: 顶级机器学习会议投稿数量呈指数级增长，超出评审人员能力，导致评审质量、一致性问题和评审人员疲劳，因此需将AI辅助同行评审作为研究和基础设施优先事项。

Method: 提出构建全面的AI增强生态系统，让大语言模型作为作者、评审人员和领域主席的协作伙伴；提出AI在事实核查、指导评审表现、协助作者提升质量和支持领域主席决策等方面的具体作用；制定研究议程并开展实验；讨论技术和伦理挑战。

Result: 未提及具体研究结果。

Conclusion: 呼吁机器学习界积极构建AI辅助的未来，确保科学验证的完整性和可扩展性，维持同行评审的高标准。

Abstract: Peer review, the bedrock of scientific advancement in machine learning (ML),
is strained by a crisis of scale. Exponential growth in manuscript submissions
to premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite
capacity of qualified reviewers, leading to concerns about review quality,
consistency, and reviewer fatigue. This position paper argues that AI-assisted
peer review must become an urgent research and infrastructure priority. We
advocate for a comprehensive AI-augmented ecosystem, leveraging Large Language
Models (LLMs) not as replacements for human judgment, but as sophisticated
collaborators for authors, reviewers, and Area Chairs (ACs). We propose
specific roles for AI in enhancing factual verification, guiding reviewer
performance, assisting authors in quality improvement, and supporting ACs in
decision-making. Crucially, we contend that the development of such systems
hinges on access to more granular, structured, and ethically-sourced peer
review process data. We outline a research agenda, including illustrative
experiments, to develop and validate these AI assistants, and discuss
significant technical and ethical challenges. We call upon the ML community to
proactively build this AI-assisted future, ensuring the continued integrity and
scalability of scientific validation, while maintaining high standards of peer
review.

</details>


### [5] [Compiling Metric Temporal Answer Set Programming](https://arxiv.org/abs/2506.08150)
*Arvid Becker,Pedro Cabalar,Martin Diéguez,Javier Romero,Susana Hahn,Torsten Schaub*

Main category: cs.AI

TL;DR: 开发计算方法解决Metric ASP中细粒度时间约束的可扩展性问题，使解决方案不受时间精度影响。


<details>
  <summary>Details</summary>
Motivation: 解决Metric ASP处理细粒度时间约束时可扩展性问题，缓解ASP的grounding瓶颈。

Method: 利用ASP与差分约束的扩展，将时间相关方面外部处理。

Result: 有效将metric ASP与时间粒度解耦，解决方案不受时间精度影响。

Conclusion: 所提方法能解决Metric ASP中细粒度时间约束的可扩展性问题。

Abstract: We develop a computational approach to Metric Answer Set Programming (ASP) to
allow for expressing quantitative temporal constrains, like durations and
deadlines. A central challenge is to maintain scalability when dealing with
fine-grained timing constraints, which can significantly exacerbate ASP's
grounding bottleneck. To address this issue, we leverage extensions of ASP with
difference constraints, a simplified form of linear constraints, to handle
time-related aspects externally. Our approach effectively decouples metric ASP
from the granularity of time, resulting in a solution that is unaffected by
time precision.

</details>


### [6] [AstroCompress: A benchmark dataset for multi-purpose compression of astronomical data](https://arxiv.org/abs/2506.08306)
*Tuan Truong,Rithwik Sudharsan,Yibo Yang,Peter Xiangyuan Ma,Ruihan Yang,Stephan Mandt,Joshua S. Bloom*

Main category: cs.AI

TL;DR: 本文介绍AstroCompress，一个天体物理数据的神经压缩挑战，提供数据集和代码，测试多种无损压缩方法，表明无损神经压缩技术可增强天文台数据收集，并提及未来有损压缩探索。


<details>
  <summary>Details</summary>
Motivation: 地面和太空天文台数据传输能力有限，无损数据压缩改进可带来巨大科学价值，传统手动设计方法有局限，神经数据压缩有潜力。

Method: 引入AstroCompress挑战，提供四个新数据集和一个旧数据集，提供代码，对七种无损压缩方法（三种神经和四种非神经）进行基准测试。

Result: 无损神经压缩技术可增强天文台数据收集。

Conclusion: 为科学应用中采用神经压缩提供指导，还提及未来可探索有损压缩方法。

Abstract: The site conditions that make astronomical observatories in space and on the
ground so desirable -- cold and dark -- demand a physical remoteness that leads
to limited data transmission capabilities. Such transmission limitations
directly bottleneck the amount of data acquired and in an era of costly modern
observatories, any improvements in lossless data compression has the potential
scale to billions of dollars worth of additional science that can be
accomplished on the same instrument. Traditional lossless methods for
compressing astrophysical data are manually designed. Neural data compression,
on the other hand, holds the promise of learning compression algorithms
end-to-end from data and outperforming classical techniques by leveraging the
unique spatial, temporal, and wavelength structures of astronomical images.
This paper introduces AstroCompress: a neural compression challenge for
astrophysics data, featuring four new datasets (and one legacy dataset) with
16-bit unsigned integer imaging data in various modes: space-based,
ground-based, multi-wavelength, and time-series imaging. We provide code to
easily access the data and benchmark seven lossless compression methods (three
neural and four non-neural, including all practical state-of-the-art
algorithms). Our results on lossless compression indicate that lossless neural
compression techniques can enhance data collection at observatories, and
provide guidance on the adoption of neural compression in scientific
applications. Though the scope of this paper is restricted to lossless
compression, we also comment on the potential exploration of lossy compression
methods in future studies.

</details>


### [7] [LeanTutor: A Formally-Verified AI Tutor for Mathematical Proofs](https://arxiv.org/abs/2506.08321)
*Manooshree Patel,Rayna Bhattacharyya,Thomas Lu,Arnav Mehta,Niels Voss,Narges Norouzi,Gireeja Ranade*

Main category: cs.AI

TL;DR: 介绍基于大语言模型的数学证明辅导系统LeanTutor，含三模块，用PeanoBench评估，有一定表现。


<details>
  <summary>Details</summary>
Motivation: 开发基于大语言模型的数学证明辅导系统，辅助学生进行数学证明学习。

Method: 构建由自动形式化器/证明检查器、下一步生成器和自然语言反馈生成器组成的系统，用PeanoBench数据集评估。

Result: 自动形式化器正确形式化57%正确证明策略，准确识别30%错误证明的错误步骤，生成提示时在准确率和相关性指标上优于基线。

Conclusion: LeanTutor系统在数学证明辅导方面有一定效果和优势。

Abstract: We present LeanTutor, a Large Language Model (LLM)-based tutoring system for
math proofs. LeanTutor interacts with the student in natural language, formally
verifies student-written math proofs in Lean, generates correct next steps, and
provides the appropriate instructional guidance. LeanTutor is composed of three
modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and
(iii) a natural language feedback generator. The first module faithfully
autoformalizes student proofs into Lean and verifies proof accuracy via
successful code compilation. If the proof has an error, the incorrect step is
identified. The next-step generator module outputs a valid next Lean tactic for
incorrect proofs via LLM-based candidate generation and proof search. The
feedback generator module leverages Lean data to produce a
pedagogically-motivated natural language hint for the student user. To evaluate
our system, we introduce PeanoBench, a human-written dataset derived from the
Natural Numbers Game, consisting of 371 Peano Arithmetic proofs, where each
natural language proof step is paired with the corresponding logically
equivalent tactic in Lean. The Autoformalizer correctly formalizes 57% of
tactics in correct proofs and accurately identifies the incorrect step in 30%
of incorrect proofs. In generating natural language hints for erroneous proofs,
LeanTutor outperforms a simple baseline on accuracy and relevance metrics.

</details>


### [8] [ORFS-agent: Tool-Using Agents for Chip Design Optimization](https://arxiv.org/abs/2506.08332)
*Amur Ghose,Andrew B. Kahng,Sayak Kundu,Zhiang Wang*

Main category: cs.AI

TL;DR: 本文介绍了基于大语言模型的迭代优化代理ORFS - agent，用于自动调整开源硬件设计流程中的参数，在资源效率和设计指标上优于贝叶斯优化方法。


<details>
  <summary>Details</summary>
Motivation: 集成电路设计流程参数多，参数小变化对设计性能、功耗和面积影响大，大语言模型为高维优化任务带来新机遇。

Method: 引入基于大语言模型的迭代优化代理ORFS - agent来自适应探索参数配置。

Result: 在两个技术节点和一系列电路基准测试中，ORFS - agent使布线线长和有效时钟周期改善超13%，优化迭代次数减少40%，可按自然语言目标进行多目标优化。

Conclusion: ORFS - agent具有模块化和模型无关性，可直接接入前沿大语言模型，无需进一步微调。

Abstract: Machine learning has been widely used to optimize complex engineering
workflows across numerous domains. In the context of integrated circuit design,
modern flows (e.g., going from a register-transfer level netlist to physical
layouts) involve extensive configuration via thousands of parameters, and small
changes to these parameters can have large downstream impacts on desired
outcomes - namely design performance, power, and area. Recent advances in Large
Language Models (LLMs) offer new opportunities for learning and reasoning
within such high-dimensional optimization tasks. In this work, we introduce
ORFS-agent, an LLM-based iterative optimization agent that automates parameter
tuning in an open-source hardware design flow. ORFS-agent adaptively explores
parameter configurations, demonstrating clear improvements over standard
Bayesian optimization approaches in terms of resource efficiency and final
design metrics. Our empirical evaluations on two different technology nodes and
a range of circuit benchmarks indicate that ORFS-agent can improve both routed
wirelength and effective clock period by over 13%, all while using 40% fewer
optimization iterations. Moreover, by following natural language objectives to
trade off certain metrics for others, ORFS-agent demonstrates a flexible and
interpretable framework for multi-objective optimization. Crucially, RFS-agent
is modular and model-agnostic, and can be plugged in to any frontier LLM
without any further fine-tuning.

</details>


### [9] [FloorplanMAE:A self-supervised framework for complete floorplan generation from partial inputs](https://arxiv.org/abs/2506.08363)
*Jun Yin,Jing Zhong,Pengyu Zeng,Peilin Li,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.AI

TL;DR: 提出FloorplanMAE自监督学习框架用于将不完整平面图恢复为完整平面图，实验表明该模型能从部分平面图生成高质量完整平面图。


<details>
  <summary>Details</summary>
Motivation: 平面图设计是动态迭代过程，从部分平面图预测完整平面图可帮助建筑师快速生成初步设计，提高效率、减少重复修改工作量。

Method: 开发平面图重建数据集FloorplanNet，提出基于Masked Autoencoders的平面图重建方法，通过遮蔽平面图部分区域并训练轻量级Vision Transformer来重建缺失部分。

Result: FloorplanMAE模型能从部分不完整平面图生成高质量完整平面图。

Conclusion: 该框架为平面图生成提供可扩展解决方案，有广阔应用前景。

Abstract: In the architectural design process, floorplan design is often a dynamic and
iterative process. Architects progressively draw various parts of the floorplan
according to their ideas and requirements, continuously adjusting and refining
throughout the design process. Therefore, the ability to predict a complete
floorplan from a partial one holds significant value in the design process.
Such prediction can help architects quickly generate preliminary designs,
improve design efficiency, and reduce the workload associated with repeated
modifications. To address this need, we propose FloorplanMAE, a self-supervised
learning framework for restoring incomplete floor plans into complete ones.
First, we developed a floor plan reconstruction dataset, FloorplanNet,
specifically trained on architectural floor plans. Secondly, we propose a floor
plan reconstruction method based on Masked Autoencoders (MAE), which
reconstructs missing parts by masking sections of the floor plan and training a
lightweight Vision Transformer (ViT). We evaluated the reconstruction accuracy
of FloorplanMAE and compared it with state-of-the-art benchmarks. Additionally,
we validated the model using real sketches from the early stages of
architectural design. Experimental results show that the FloorplanMAE model can
generate high-quality complete floor plans from incomplete partial plans. This
framework provides a scalable solution for floor plan generation, with broad
application prospects.

</details>


### [10] [On Reasoning Strength Planning in Large Reasoning Models](https://arxiv.org/abs/2506.08390)
*Leheng Sheng,An Zhang,Zijian Wu,Weixiang Zhao,Changshuo Shen,Yi Zhang,Xiang Wang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 文章从模型激活角度解释大推理模型自动分配推理强度现象，发现预分配方向向量控制推理强度，还展示两个应用。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽观察到大型推理模型自动分配推理强度现象，但对其潜在机制缺乏深入探索，本文旨在解释该现象。

Method: 从模型激活角度分析，用线性探针基于问题激活预测推理令牌数量，研究预分配方向向量对推理强度的影响。

Result: LRMs在生成前就在激活中预规划推理强度，推理令牌数量可基于问题激活预测，预分配方向向量控制推理强度，增减向量会改变推理令牌数量和性能，方向向量能预测推理长度并影响推理结束标记的对数几率。

Conclusion: 研究为LRMs推理的内部机制提供新见解，还提供控制其推理行为的实用工具。

Abstract: Recent studies empirically reveal that large reasoning models (LRMs) can
automatically allocate more reasoning strengths (i.e., the number of reasoning
tokens) for harder problems, exhibiting difficulty-awareness for better task
performance. While this automatic reasoning strength allocation phenomenon has
been widely observed, its underlying mechanism remains largely unexplored. To
this end, we provide explanations for this phenomenon from the perspective of
model activations. We find evidence that LRMs pre-plan the reasoning strengths
in their activations even before generation, with this reasoning strength
causally controlled by the magnitude of a pre-allocated directional vector.
Specifically, we show that the number of reasoning tokens is predictable solely
based on the question activations using linear probes, indicating that LRMs
estimate the required reasoning strength in advance. We then uncover that LRMs
encode this reasoning strength through a pre-allocated directional vector
embedded in the activations of the model, where the vector's magnitude
modulates the reasoning strength. Subtracting this vector can lead to reduced
reasoning token number and performance, while adding this vector can lead to
increased reasoning token number and even improved performance. We further
reveal that this direction vector consistently yields positive reasoning length
prediction, and it modifies the logits of end-of-reasoning token </think> to
affect the reasoning length. Finally, we demonstrate two potential applications
of our findings: overthinking behavior detection and enabling efficient
reasoning on simple problems. Our work provides new insights into the internal
mechanisms of reasoning in LRMs and offers practical tools for controlling
their reasoning behaviors. Our code is available at
https://github.com/AlphaLab-USTC/LRM-plans-CoT.

</details>


### [11] [SafeCoT: Improving VLM Safety with Minimal Reasoning](https://arxiv.org/abs/2506.08399)
*Jiachen Ma,Zhanhui Zhou,Chao Yang,Chaochao Lu*

Main category: cs.AI

TL;DR: 提出SafeCoT框架利用规则驱动的思维链监督提升视觉语言模型拒绝行为，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在高风险或模糊场景中确保安全适当响应的挑战。

Method: 引入SafeCoT框架，利用规则驱动的思维链监督，用最少监督帮助模型推理安全风险并进行上下文感知拒绝。

Result: 在多个基准测试中，SafeCoT显著减少过度拒绝，增强泛化能力，即使训练数据有限。

Conclusion: SafeCoT为视觉语言模型与安全关键目标对齐提供可扩展解决方案。

Abstract: Ensuring safe and appropriate responses from vision-language models (VLMs)
remains a critical challenge, particularly in high-risk or ambiguous scenarios.
We introduce SafeCoT, a lightweight, interpretable framework that leverages
rule-based chain-of-thought (CoT) supervision to improve refusal behavior in
VLMs. Unlike prior methods that rely on large-scale safety annotations or
complex modeling, SafeCoT uses minimal supervision to help models reason about
safety risks and make context-aware refusals. Experiments across multiple
benchmarks show that SafeCoT significantly reduces overrefusal and enhances
generalization, even with limited training data. Our approach offers a scalable
solution for aligning VLMs with safety-critical objectives.

</details>


### [12] [Single-Node Trigger Backdoor Attacks in Graph-Based Recommendation Systems](https://arxiv.org/abs/2506.08401)
*Runze Li,Di Jin,Xiaobao Wang,Dongxiao He,Bingdao Feng,Zhen Wang*

Main category: cs.AI

TL;DR: 提出图后门攻击方法，插入一个假用户节点使目标物品向目标用户曝光，控制对推荐系统性能影响。


<details>
  <summary>Details</summary>
Motivation: 现有图推荐系统的攻击策略存在低隐身性和高破坏性问题，需改进。

Method: 设计单节点触发生成器，插入一个假用户节点曝光多个目标物品；引入目标节点与无关节点的约束条件。

Result: 99%的目标用户中目标物品曝光率不低于50%，对推荐系统性能影响控制在约5%。

Conclusion: 所提图后门攻击方法能在隐蔽情况下提高目标物品曝光，且对推荐系统性能影响小。

Abstract: Graph recommendation systems have been widely studied due to their ability to
effectively capture the complex interactions between users and items. However,
these systems also exhibit certain vulnerabilities when faced with attacks. The
prevailing shilling attack methods typically manipulate recommendation results
by injecting a large number of fake nodes and edges. However, such attack
strategies face two primary challenges: low stealth and high destructiveness.
To address these challenges, this paper proposes a novel graph backdoor attack
method that aims to enhance the exposure of target items to the target user in
a covert manner, without affecting other unrelated nodes. Specifically, we
design a single-node trigger generator, which can effectively expose multiple
target items to the target user by inserting only one fake user node.
Additionally, we introduce constraint conditions between the target nodes and
irrelevant nodes to mitigate the impact of fake nodes on the recommendation
system's performance. Experimental results show that the exposure of the target
items reaches no less than 50% in 99% of the target users, while the impact on
the recommendation system's performance is controlled within approximately 5%.

</details>


### [13] [Transforming Expert Knowledge into Scalable Ontology via Large Language Models](https://arxiv.org/abs/2506.08422)
*Ikkei Itoku,David Theil,Evelyn Eichelsdoerfer Uehara,Sreyoshi Bhaduri,Junnosuke Kuroda,Toshi Yumoto,Alex Gil,Natalie Perez,Rajesh Cherukuri,Naumaan Nayyar*

Main category: cs.AI

TL;DR: 本文提出结合大语言模型、专家校准和迭代提示优化的框架自动进行分类法对齐，评估中取得高F1分数，证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 传统手动分类法对齐成本高、耗时长且易有分歧，现有自动化方法处理语义关系和跨领域一致性存在局限。

Method: 提出结合大语言模型、专家校准和迭代提示优化的框架，整合专家标注示例、多阶段提示工程和人工验证。

Result: 在特定领域概念重要性映射任务评估中，F1分数达0.97，远超人类基准0.68。

Conclusion: 该方法在扩展分类法对齐规模的同时能保持高质量映射，且在模糊情况下可保留专家监督。

Abstract: Having a unified, coherent taxonomy is essential for effective knowledge
representation in domain-specific applications as diverse terminologies need to
be mapped to underlying concepts. Traditional manual approaches to taxonomy
alignment rely on expert review of concept pairs, but this becomes
prohibitively expensive and time-consuming at scale, while subjective
interpretations often lead to expert disagreements. Existing automated methods
for taxonomy alignment have shown promise but face limitations in handling
nuanced semantic relationships and maintaining consistency across different
domains. These approaches often struggle with context-dependent concept
mappings and lack transparent reasoning processes. We propose a novel framework
that combines large language models (LLMs) with expert calibration and
iterative prompt optimization to automate taxonomy alignment. Our method
integrates expert-labeled examples, multi-stage prompt engineering, and human
validation to guide LLMs in generating both taxonomy linkages and supporting
rationales. In evaluating our framework on a domain-specific mapping task of
concept essentiality, we achieved an F1-score of 0.97, substantially exceeding
the human benchmark of 0.68. These results demonstrate the effectiveness of our
approach in scaling taxonomy alignment while maintaining high-quality mappings
and preserving expert oversight for ambiguous cases.

</details>


### [14] [SHIELD: Multi-task Multi-distribution Vehicle Routing Solver with Sparsity and Hierarchy](https://arxiv.org/abs/2506.08424)
*Yong Liang Goh,Zhiguang Cao,Yining Ma,Jianan Zhou,Mohammad Haroon Dupty,Wee Sun Lee*

Main category: cs.AI

TL;DR: 本文将MTVRP拓展到更现实的MTMDVRP场景，提出SHIELD模型，在多地图多VRP变体上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有路由问题基础模型忽略复杂的现实客户分布，需更现实的模型。

Method: 提出SHIELD模型，结合深度混合技术实现稀疏性，开发基于上下文的聚类层利用问题的层次结构。

Result: 在9个真实世界地图和16种VRP变体上，该方法优于现有方法。

Conclusion: 所提模型能有效识别跨任务和分布的关键特征，显著提高对未见数据的泛化能力。

Abstract: Recent advances toward foundation models for routing problems have shown
great potential of a unified deep model for various VRP variants. However, they
overlook the complex real-world customer distributions. In this work, we
advance the Multi-Task VRP (MTVRP) setting to the more realistic yet
challenging Multi-Task Multi-Distribution VRP (MTMDVRP) setting, and introduce
SHIELD, a novel model that leverages both sparsity and hierarchy principles.
Building on a deeper decoder architecture, we first incorporate the
Mixture-of-Depths (MoD) technique to enforce sparsity. This improves both
efficiency and generalization by allowing the model to dynamically select nodes
to use or skip each decoder layer, providing the needed capacity to adaptively
allocate computation for learning the task/distribution specific and shared
representations. We also develop a context-based clustering layer that exploits
the presence of hierarchical structures in the problems to produce better local
representations. These two designs inductively bias the network to identify key
features that are common across tasks and distributions, leading to
significantly improved generalization on unseen ones. Our empirical results
demonstrate the superiority of our approach over existing methods on 9
real-world maps with 16 VRP variants each.

</details>


### [15] [A Survey on Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2506.08446)
*Peng-Yuan Wang,Tian-Shuo Liu,Chenyang Wang,Yi-Di Wang,Shu Yan,Cheng-Xing Jia,Xu-Hui Liu,Xin-Wei Chen,Jia-Cheng Xu,Ziniu Li,Yang Yu*

Main category: cs.AI

TL;DR: 本文综述大语言模型数学推理能力发展，涵盖阶段、增强方法、挑战及研究方向，为相关研究者提供见解。


<details>
  <summary>Details</summary>
Motivation: 数学推理是人工智能研究前沿，大语言模型近年在此取得进展，需对其数学推理能力发展进行研究。

Method: 通过理解和答案生成两个认知阶段审视大语言模型数学推理能力发展，回顾从免训练提示到微调等增强方法。

Result: 大语言模型在数学推理上有显著进展，但在能力、效率和泛化方面仍存在挑战。

Conclusion: 指出先进预训练、知识增强、形式推理框架和元泛化等有前景的研究方向，为相关研究提供见解。

Abstract: Mathematical reasoning has long represented one of the most fundamental and
challenging frontiers in artificial intelligence research. In recent years,
large language models (LLMs) have achieved significant advances in this area.
This survey examines the development of mathematical reasoning abilities in
LLMs through two high-level cognitive phases: comprehension, where models gain
mathematical understanding via diverse pretraining strategies, and answer
generation, which has progressed from direct prediction to step-by-step
Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical
reasoning, ranging from training-free prompting to fine-tuning approaches such
as supervised fine-tuning and reinforcement learning, and discuss recent work
on extended CoT and "test-time scaling". Despite notable progress, fundamental
challenges remain in terms of capacity, efficiency, and generalization. To
address these issues, we highlight promising research directions, including
advanced pretraining and knowledge augmentation techniques, formal reasoning
frameworks, and meta-generalization through principled learning paradigms. This
survey tries to provide some insights for researchers interested in enhancing
reasoning capabilities of LLMs and for those seeking to apply these techniques
to other domains.

</details>


### [16] [Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing](https://arxiv.org/abs/2506.08462)
*Christos Margadji,Sebastian W. Pattinson*

Main category: cs.AI

TL;DR: 介绍了用于工业控制的CIPHER框架，在3D打印机上验证，可泛化到分布外任务，为工业自主系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 工业过程需鲁棒和适应，但当前AI控制系统泛化能力受限，基础模型缺乏定量精度，因此需要新方法。

Method: 引入CIPHER框架，集成过程专家和回归模型，结合检索增强生成以实现类人推理。

Result: 该混合架构能泛化到分布外任务，可解释输入、决策并自主生成指令。

Conclusion: CIPHER为工业环境中安全可靠的自主系统部署奠定基础。

Abstract: Industrial processes must be robust and adaptable, as environments and tasks
are often unpredictable, while operational errors remain costly and difficult
to detect. AI-based control systems offer a path forward, yet typically depend
on supervised learning with extensive labelled datasets, which limits their
ability to generalize across variable and data-scarce industrial settings.
Foundation models could enable broader reasoning and knowledge integration, but
rarely deliver the quantitative precision demanded by engineering applications.
Here, we introduceControl and Interpretation of Production via Hybrid Expertise
and Reasoning (CIPHER): a vision-language-action (VLA) model framework aiming
to replicate human-like reasoning for industrial control, instantiated in a
commercial-grade 3D printer. It integrates a process expert, a regression model
enabling quantitative characterization of system states required for
engineering tasks. CIPHER also incorporates retrieval-augmented generation to
access external expert knowledge and support physics-informed, chain-of-thought
reasoning. This hybrid architecture exhibits strong generalization to
out-of-distribution tasks. It interprets visual or textual inputs from process
monitoring, explains its decisions, and autonomously generates precise machine
instructions, without requiring explicit annotations. CIPHER thus lays the
foundations for autonomous systems that act with precision, reason with
context, and communicate decisions transparently, supporting safe and trusted
deployment in industrial settings.

</details>


### [17] [RHealthTwin: Towards Responsible and Multimodal Digital Twins for Personalized Well-being](https://arxiv.org/abs/2506.08486)
*Rahatara Ferdousi,M Anwar Hossain*

Main category: cs.AI

TL;DR: 本文提出RHealthTwin框架构建和管理AI健康数字替身，其核心RPE可解决传统LLM配置问题，在多领域评估表现良好，有望成为健康领域负责任应用的基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于医疗数字替身存在幻觉、偏见等问题，响应卫生当局建议，构建负责任的框架。

Method: 提出RHealthTwin框架，核心是RPE动态提取预定义插槽来结构化输入，还有反馈循环更新提示结构。

Result: 在四个消费健康领域评估，RPE在基准数据集上取得先进结果，伦理合规和指令遵循指标超90%，优于基线策略。

Conclusion: RHealthTwin有望成为健康和幸福领域基于大语言模型的负责任应用的前瞻性基础。

Abstract: The rise of large language models (LLMs) has created new possibilities for
digital twins in healthcare. However, the deployment of such systems in
consumer health contexts raises significant concerns related to hallucination,
bias, lack of transparency, and ethical misuse. In response to recommendations
from health authorities such as the World Health Organization (WHO), we propose
Responsible Health Twin (RHealthTwin), a principled framework for building and
governing AI-powered digital twins for well-being assistance. RHealthTwin
processes multimodal inputs that guide a health-focused LLM to produce safe,
relevant, and explainable responses. At the core of RHealthTwin is the
Responsible Prompt Engine (RPE), which addresses the limitations of traditional
LLM configuration. Conventionally, users input unstructured prompt and the
system instruction to configure the LLM, which increases the risk of
hallucination. In contrast, RPE extracts predefined slots dynamically to
structure both inputs. This guides the language model to generate responses
that are context aware, personalized, fair, reliable, and explainable for
well-being assistance. The framework further adapts over time through a
feedback loop that updates the prompt structure based on user satisfaction. We
evaluate RHealthTwin across four consumer health domains including mental
support, symptom triage, nutrition planning, and activity coaching. RPE
achieves state-of-the-art results with BLEU = 0.41, ROUGE-L = 0.63, and
BERTScore = 0.89 on benchmark datasets. Also, we achieve over 90% in ethical
compliance and instruction-following metrics using LLM-as-judge evaluation,
outperforming baseline strategies. We envision RHealthTwin as a forward-looking
foundation for responsible LLM-based applications in health and well-being.

</details>


### [18] [FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching](https://arxiv.org/abs/2506.08518)
*Sunny Gupta,Nikita Jangid,Shounak Das,Amit Sethi*

Main category: cs.AI

TL;DR: 提出FedTAIL框架解决领域泛化问题，在领域偏移和标签不平衡场景表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化方法在长尾类分布和冲突优化目标下表现不佳。

Method: 采用锐度引导、梯度对齐优化，引入梯度一致性正则化器，进行类别的锐度最小化，提出曲率感知动态加权方案，将锐度感知扰动融入熵正则化。

Result: 在标准领域泛化基准测试中取得了最先进的性能。

Conclusion: FedTAIL在集中式和联邦式环境中均有效，能解决领域泛化中的难题。

Abstract: Domain Generalization (DG) seeks to train models that perform reliably on
unseen target domains without access to target data during training. While
recent progress in smoothing the loss landscape has improved generalization,
existing methods often falter under long-tailed class distributions and
conflicting optimization objectives. We introduce FedTAIL, a federated domain
generalization framework that explicitly addresses these challenges through
sharpness-guided, gradient-aligned optimization. Our method incorporates a
gradient coherence regularizer to mitigate conflicts between classification and
adversarial objectives, leading to more stable convergence. To combat class
imbalance, we perform class-wise sharpness minimization and propose a
curvature-aware dynamic weighting scheme that adaptively emphasizes
underrepresented tail classes. Furthermore, we enhance conditional distribution
alignment by integrating sharpness-aware perturbations into entropy
regularization, improving robustness under domain shift. FedTAIL unifies
optimization harmonization, class-aware regularization, and conditional
alignment into a scalable, federated-compatible framework. Extensive
evaluations across standard domain generalization benchmarks demonstrate that
FedTAIL achieves state-of-the-art performance, particularly in the presence of
domain shifts and label imbalance, validating its effectiveness in both
centralized and federated settings. Code: https://github.com/sunnyinAI/FedTail

</details>


### [19] [Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery](https://arxiv.org/abs/2506.08771)
*Yuni Susanti,Michael Färber*

Main category: cs.AI

TL;DR: 提出结合知识图谱和大语言模型的新方法用于因果发现，在多数据集实验中表现优于多数基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的因果发现方法结果不稳定、不可靠，需改进。

Method: 识别知识图谱中有信息的基于元路径的子图，用基于学习排序的模型优化子图选择，将排名靠前的子图融入零样本提示。

Result: 在生物医学和开放领域数据集上，该方法在多种大语言模型和知识图谱中F1分数比多数基线高最多44.4分。

Conclusion: 所提方法能有效提升基于知识的因果发现效果。

Abstract: Inferring causal relationships between variable pairs is crucial for
understanding multivariate interactions in complex systems. Knowledge-based
causal discovery -- which involves inferring causal relationships by reasoning
over the metadata of variables (e.g., names or textual context) -- offers a
compelling alternative to traditional methods that rely on observational data.
However, existing methods using Large Language Models (LLMs) often produce
unstable and inconsistent results, compromising their reliability for causal
inference. To address this, we introduce a novel approach that integrates
Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.
Our approach identifies informative metapath-based subgraphs within KGs and
further refines the selection of these subgraphs using Learning-to-Rank-based
models. The top-ranked subgraphs are then incorporated into zero-shot prompts,
improving the effectiveness of LLMs in inferring the causal relationship.
Extensive experiments on biomedical and open-domain datasets demonstrate that
our method outperforms most baselines by up to 44.4 points in F1 scores,
evaluated across diverse LLMs and KGs. Our code and datasets are available on
GitHub: https://github.com/susantiyuni/path-to-causality

</details>


### [20] [Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A Hybrid DRL-LLM Approach with Compliance Awareness](https://arxiv.org/abs/2506.08532)
*Yanwei Gong,Xiaolin Chang*

Main category: cs.AI

TL;DR: 提出结合DRL与LLM推理的无人机轨迹规划框架，实验表明其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 低空经济发展使无人机轨迹规划面临新挑战，现有研究忽略关键因素，DRL学习效率低。

Method: 提出结合DRL与LLM推理的无人机轨迹规划框架。

Result: 该方法在多个指标上显著优于现有基线。

Conclusion: 该方法能有效应对低空经济网络约束下无人机轨迹规划的关键挑战。

Abstract: The rapid growth of the low-altitude economy has driven the widespread
adoption of unmanned aerial vehicles (UAVs). This growing deployment presents
new challenges for UAV trajectory planning in complex urban environments.
However, existing studies often overlook key factors, such as urban airspace
constraints and economic efficiency, which are essential in low-altitude
economy contexts. Deep reinforcement learning (DRL) is regarded as a promising
solution to these issues, while its practical adoption remains limited by low
learning efficiency. To overcome this limitation, we propose a novel UAV
trajectory planning framework that combines DRL with large language model (LLM)
reasoning to enable safe, compliant, and economically viable path planning.
Experimental results demonstrate that our method significantly outperforms
existing baselines across multiple metrics, including data collection rate,
collision avoidance, successful landing, regulatory compliance, and energy
efficiency. These results validate the effectiveness of our approach in
addressing UAV trajectory planning key challenges under constraints of the
low-altitude economy networking.

</details>


### [21] [HGFormer: A Hierarchical Graph Transformer Framework for Two-Stage Colonel Blotto Games via Reinforcement Learning](https://arxiv.org/abs/2506.08580)
*Yang Lv,Jinlong Lei,Peng Yi*

Main category: cs.AI

TL;DR: 针对两阶段Colonel Blotto博弈问题，提出HGformer框架和反馈强化学习算法，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 两阶段Colonel Blotto博弈中阶段依赖和图拓扑约束使传统方法难获全局最优策略。

Method: 提出HGformer框架，结合增强图Transformer编码器与两层决策模型，设计层间反馈强化学习算法。

Result: 相比现有分层决策或图神经网络方法，HGformer显著提高资源分配效率和对抗收益。

Conclusion: HGformer在复杂动态博弈场景中表现更优。

Abstract: Two-stage Colonel Blotto game represents a typical adversarial resource
allocation problem, in which two opposing agents sequentially allocate
resources in a network topology across two phases: an initial resource
deployment followed by multiple rounds of dynamic reallocation adjustments. The
sequential dependency between game stages and the complex constraints imposed
by the graph topology make it difficult for traditional approaches to attain a
globally optimal strategy. To address these challenges, we propose a
hierarchical graph Transformer framework called HGformer. By incorporating an
enhanced graph Transformer encoder with structural biases and a two-agent
hierarchical decision model, our approach enables efficient policy generation
in large-scale adversarial environments. Moreover, we design a layer-by-layer
feedback reinforcement learning algorithm that feeds the long-term returns from
lower-level decisions back into the optimization of the higher-level strategy,
thus bridging the coordination gap between the two decision-making stages.
Experimental results demonstrate that, compared to existing hierarchical
decision-making or graph neural network methods, HGformer significantly
improves resource allocation efficiency and adversarial payoff, achieving
superior overall performance in complex dynamic game scenarios.

</details>


### [22] [FoldA: Computing Partial-Order Alignments Using Directed Net Unfoldings](https://arxiv.org/abs/2506.08627)
*Douwe Geurtjens,Xixi Lu*

Main category: cs.AI

TL;DR: 提出新的部分顺序对齐技术FoldA，虽计算时间长，但可减少排队状态数并更准确表示并发。


<details>
  <summary>Details</summary>
Motivation: 现有一致性检查方法存在状态空间爆炸，且无法充分表示并发行为的问题。

Method: 使用有向Petri网展开计算部分顺序对齐的技术FoldA。

Result: 在合成和真实数据集上评估，FoldA虽需更多计算时间，但减少了排队状态数，更准确表示并发。

Conclusion: FoldA能在一定程度上解决现有方法的局限性。

Abstract: Conformance checking is a fundamental task of process mining, which
quantifies the extent to which the observed process executions match a
normative process model. The state-of-the-art approaches compute alignments by
exploring the state space formed by the synchronous product of the process
model and the trace. This often leads to state space explosion, particularly
when the model exhibits a high degree of choice and concurrency. Moreover, as
alignments inherently impose a sequential structure, they fail to fully
represent the concurrent behavior present in many real-world processes. To
address these limitations, this paper proposes a new technique for computing
partial-order alignments {on the fly using directed Petri net unfoldings, named
FoldA. We evaluate our technique on 485 synthetic model-log pairs and compare
it against Astar- and Dijkstra-alignments on 13 real-life model-log pairs and 6
benchmark pairs. The results show that our unfolding alignment, although it
requires more computation time, generally reduces the number of queued states
and provides a more accurate representation of concurrency.

</details>


### [23] [Modular Recurrence in Contextual MDPs for Universal Morphology Control](https://arxiv.org/abs/2506.08630)
*Laurens Engwegen,Daan Brinks,Wendelin Böhmer*

Main category: cs.AI

TL;DR: 本文提出通过模块化循环架构提升多机器人控制对未见机器人的泛化能力，在多种环境下取得较好效果。


<details>
  <summary>Details</summary>
Motivation: 开发通用机器人控制器以提高计算和数据效率，解决多机器人控制对未见机器人泛化难的问题。

Method: 实现模块化循环架构，并在大量MuJoCo机器人上评估其泛化性能。

Result: 在四种不同环境下，对具有未见动力学、运动学和拓扑结构的机器人，性能有显著提升。

Conclusion: 通过交互推断相关上下文信息的方法能提升对未见上下文的泛化能力。

Abstract: A universal controller for any robot morphology would greatly improve
computational and data efficiency. By utilizing contextual information about
the properties of individual robots and exploiting their modular structure in
the architecture of deep reinforcement learning agents, steps have been made
towards multi-robot control. Generalization to new, unseen robots, however,
remains a challenge. In this paper we hypothesize that the relevant contextual
information is partially observable, but that it can be inferred through
interactions for better generalization to contexts that are not seen during
training. To this extent, we implement a modular recurrent architecture and
evaluate its generalization performance on a large set of MuJoCo robots. The
results show a substantial improved performance on robots with unseen dynamics,
kinematics, and topologies, in four different environments.

</details>


### [24] [Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08745)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Yingjie Wang,Baisheng Lai,Jieping Ye,Mingli Song,Dacheng Tao*

Main category: cs.AI

TL;DR: 提出自奖励强化学习框架CoVo提升大语言模型推理能力，实验表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习有效训练依赖外部监督，限制广泛应用，需无外部监督的推理学习方法。

Method: 提出利用不同推理轨迹中间推理状态一致性的自奖励强化学习框架，引入CoVo奖励机制并结合好奇奖励。

Result: 在不同推理基准测试中，CoVo性能与有监督强化学习相当甚至更优。

Conclusion: CoVo为大语言模型无外部监督学习推理提供可扩展途径。

Abstract: Recent advances of Reinforcement Learning (RL) have highlighted its potential
in complex reasoning tasks, yet effective training often relies on external
supervision, which limits the broader applicability. In this work, we propose a
novel self-rewarding reinforcement learning framework to enhance Large Language
Model (LLM) reasoning by leveraging the consistency of intermediate reasoning
states across different reasoning trajectories. Our key insight is that correct
responses often exhibit consistent trajectory patterns in terms of model
likelihood: their intermediate reasoning states tend to converge toward their
own final answers (high consistency) with minimal deviation toward other
candidates (low volatility). Inspired by this observation, we introduce CoVo,
an intrinsic reward mechanism that integrates Consistency and Volatility via a
robust vector-space aggregation strategy, complemented by a curiosity bonus to
promote diverse exploration. CoVo enables LLMs to perform RL in a
self-rewarding manner, offering a scalable pathway for learning to reason
without external supervision. Extensive experiments on diverse reasoning
benchmarks show that CoVo achieves performance comparable to or even surpassing
supervised RL. Our code is available at https://github.com/sastpg/CoVo.

</details>


### [25] [A Sample Efficient Conditional Independence Test in the Presence of Discretization](https://arxiv.org/abs/2506.08747)
*Boyang Sun,Yu Yao,Xinshuai Dong,Zongfang Liu,Tongliang Liu,Yumou Qiu,Kun Zhang*

Main category: cs.AI

TL;DR: 本文提出一种无需二值化过程的样本高效条件独立性（CI）测试方法，理论和实证结果证明其优越性和有效性。


<details>
  <summary>Details</summary>
Motivation: 直接对离散化数据进行CI测试会得出错误结论，二值化观测数据推断潜在变量的CI关系会导致信息损失、降低测试性能。

Method: 通过广义矩估计法（GMM）解决过度识别约束问题来建立潜在连续变量的独立关系，利用节点式回归推导合适的检验统计量并确定其渐近分布。

Result: 理论发现和在各种数据集上的实证结果表明所提出测试方法的优越性和有效性。

Conclusion: 所提出的无需二值化过程的样本高效CI测试方法是有效的，代码可在https://github.com/boyangaaaaa/DCT获取。

Abstract: In many real-world scenarios, interested variables are often represented as
discretized values due to measurement limitations. Applying Conditional
Independence (CI) tests directly to such discretized data, however, can lead to
incorrect conclusions. To address this, recent advancements have sought to
infer the correct CI relationship between the latent variables through
binarizing observed data. However, this process inevitably results in a loss of
information, which degrades the test's performance. Motivated by this, this
paper introduces a sample-efficient CI test that does not rely on the
binarization process. We find that the independence relationships of latent
continuous variables can be established by addressing an over-identifying
restriction problem with Generalized Method of Moments (GMM). Based on this
insight, we derive an appropriate test statistic and establish its asymptotic
distribution correctly reflecting CI by leveraging nodewise regression.
Theoretical findings and Empirical results across various datasets demonstrate
that the superiority and effectiveness of our proposed test. Our code
implementation is provided in https://github.com/boyangaaaaa/DCT

</details>


### [26] [Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents](https://arxiv.org/abs/2506.08800)
*Irene Testini,José Hernández-Orallo,Lorenzo Pacchiardi*

Main category: cs.AI

TL;DR: 本文对用于数据科学的大语言模型助手和代理的评估进行了调查，指出当前评估存在的一些问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在数据科学领域应用增加，LLM 代理兴起，需要对其评估情况进行调研。

Method: 对用于数据科学的 LLM 助手和代理的评估进行调查。

Result: 发现评估存在三个问题，即聚焦于小部分目标导向活动、未考虑人机协作中间水平、强调人类替代而忽略任务转换带来的更高自动化。

Conclusion: 当前用于数据科学的 LLM 助手和代理的评估存在不足。

Abstract: Data science aims to extract insights from data to support decision-making
processes. Recently, Large Language Models (LLMs) are increasingly used as
assistants for data science, by suggesting ideas, techniques and small code
snippets, or for the interpretation of results and reporting. Proper automation
of some data-science activities is now promised by the rise of LLM agents,
i.e., AI systems powered by an LLM equipped with additional affordances--such
as code execution and knowledge bases--that can perform self-directed actions
and interact with digital environments. In this paper, we survey the evaluation
of LLM assistants and agents for data science. We find (1) a dominant focus on
a small subset of goal-oriented activities, largely ignoring data management
and exploratory activities; (2) a concentration on pure assistance or fully
autonomous agents, without considering intermediate levels of human-AI
collaboration; and (3) an emphasis on human substitution, therefore neglecting
the possibility of higher levels of automation thanks to task transformation.

</details>


### [27] [Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task](https://arxiv.org/abs/2506.08872)
*Nataliya Kosmyna,Eugene Hauptmann,Ye Tong Yuan,Jessica Situ,Xian-Hao Liao,Ashly Vivian Beresnitzky,Iris Braunstein,Pattie Maes*

Main category: cs.AI

TL;DR: 研究探索大语言模型辅助写作的神经和行为后果，发现其虽便利但有潜在认知成本，引发对依赖大语言模型长期教育影响的关注。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型辅助写作带来的神经和行为后果，关注其对认知的影响及长期教育意义。

Method: 将参与者分为三组，完成多轮写作任务，用脑电图评估认知负荷，用NLP分析文章，并结合人工和AI评分。

Result: 不同组大脑连接性有差异，认知活动与工具使用相关；换组后参与者表现不同；大语言模型组文章自主感低，引用能力差，长期表现不佳。

Conclusion: 大语言模型虽便利但有潜在认知成本，需深入研究其在学习中的作用。

Abstract: This study explores the neural and behavioral consequences of LLM-assisted
essay writing. Participants were divided into three groups: LLM, Search Engine,
and Brain-only (no tools). Each completed three sessions under the same
condition. In a fourth session, LLM users were reassigned to Brain-only group
(LLM-to-Brain), and Brain-only users were reassigned to LLM condition
(Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18
completing session 4. We used electroencephalography (EEG) to assess cognitive
load during essay writing, and analyzed essays using NLP, as well as scoring
essays with the help from human teachers and an AI judge. Across groups, NERs,
n-gram patterns, and topic ontology showed within-group homogeneity. EEG
revealed significant differences in brain connectivity: Brain-only participants
exhibited the strongest, most distributed networks; Search Engine users showed
moderate engagement; and LLM users displayed the weakest connectivity.
Cognitive activity scaled down in relation to external tool use. In session 4,
LLM-to-Brain participants showed reduced alpha and beta connectivity,
indicating under-engagement. Brain-to-LLM users exhibited higher memory recall
and activation of occipito-parietal and prefrontal areas, similar to Search
Engine users. Self-reported ownership of essays was the lowest in the LLM group
and the highest in the Brain-only group. LLM users also struggled to accurately
quote their own work. While LLMs offer immediate convenience, our findings
highlight potential cognitive costs. Over four months, LLM users consistently
underperformed at neural, linguistic, and behavioral levels. These results
raise concerns about the long-term educational implications of LLM reliance and
underscore the need for deeper inquiry into AI's role in learning.

</details>


### [28] [Preference-Driven Multi-Objective Combinatorial Optimization with Conditional Computation](https://arxiv.org/abs/2506.08898)
*Mingfeng Fan,Jianan Zhou,Yifeng Zhang,Yaoxin Wu,Jinbiao Chen,Guillaume Adrien Sartoretti*

Main category: cs.AI

TL;DR: 现有多目标组合优化问题（MOCOPs）深度强化学习方法有局限，提出POCCO框架，经实验验证其优越性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有方法平等对待所有子问题并用单一模型求解，阻碍解空间探索，导致性能欠佳。

Method: 提出POCCO框架，设计条件计算块将子问题路由到特定神经架构，提出基于偏好驱动的优化算法学习解之间的成对偏好。

Result: 在四个经典MOCOP基准测试中，POCCO显著优于现有方法，泛化性强。

Conclusion: POCCO框架能有效解决MOCOPs，具有显著优势和强泛化性。

Abstract: Recent deep reinforcement learning methods have achieved remarkable success
in solving multi-objective combinatorial optimization problems (MOCOPs) by
decomposing them into multiple subproblems, each associated with a specific
weight vector. However, these methods typically treat all subproblems equally
and solve them using a single model, hindering the effective exploration of the
solution space and thus leading to suboptimal performance. To overcome the
limitation, we propose POCCO, a novel plug-and-play framework that enables
adaptive selection of model structures for subproblems, which are subsequently
optimized based on preference signals rather than explicit reward values.
Specifically, we design a conditional computation block that routes subproblems
to specialized neural architectures. Moreover, we propose a preference-driven
optimization algorithm that learns pairwise preferences between winning and
losing solutions. We evaluate the efficacy and versatility of POCCO by applying
it to two state-of-the-art neural methods for MOCOPs. Experimental results
across four classic MOCOP benchmarks demonstrate its significant superiority
and strong generalization.

</details>


### [29] [IntTrajSim: Trajectory Prediction for Simulating Multi-Vehicle driving at Signalized Intersections](https://arxiv.org/abs/2506.08957)
*Yash Ranjan,Rahul Sengupta,Anand Rangarajan,Sanjay Ranka*

Main category: cs.AI

TL;DR: 传统交通模拟器有局限，本文提出评估生成式轨迹预测模型的指标和模拟循环管道，还给出含信号信息的轨迹预测模型且表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统交通模拟器基于规则，难以模拟真实驾驶行为，现有生成模型未在微模拟场景测试及用交通工程指标评估，需创建数据驱动模拟器模拟交通路口驾驶行为。

Method: 提出交通工程相关指标评估生成式轨迹预测模型，提供模拟循环管道，构建含信号信息的多头自注意力轨迹预测模型。

Result: 含信号信息的轨迹预测模型在评估指标上优于先前模型。

Conclusion: 提出的指标、管道和模型有助于解决传统交通模拟器局限，更好模拟交通路口驾驶行为。

Abstract: Traffic simulators are widely used to study the operational efficiency of
road infrastructure, but their rule-based approach limits their ability to
mimic real-world driving behavior. Traffic intersections are critical
components of the road infrastructure, both in terms of safety risk (nearly 28%
of fatal crashes and 58% of nonfatal crashes happen at intersections) as well
as the operational efficiency of a road corridor. This raises an important
question: can we create a data-driven simulator that can mimic the macro- and
micro-statistics of the driving behavior at a traffic intersection? Deep
Generative Modeling-based trajectory prediction models provide a good starting
point to model the complex dynamics of vehicles at an intersection. But they
are not tested in a "live" micro-simulation scenario and are not evaluated on
traffic engineering-related metrics. In this study, we propose traffic
engineering-related metrics to evaluate generative trajectory prediction models
and provide a simulation-in-the-loop pipeline to do so. We also provide a
multi-headed self-attention-based trajectory prediction model that incorporates
the signal information, which outperforms our previous models on the evaluation
metrics.

</details>


### [30] [Evaluating Generative Vehicle Trajectory Models for Traffic Intersection Dynamics](https://arxiv.org/abs/2506.08963)
*Yash Ranjan,Rahul Sengupta,Anand Rangarajan,Sanjay Ranka*

Main category: cs.AI

TL;DR: 本文提出综合分析工具评估交通动力学模型，用新指标展示模型生成轨迹有违规行为。


<details>
  <summary>Details</summary>
Motivation: 现有模型评估主要看轨迹重建误差，未在线评估且未充分考虑交通工程特定问题，需更好评估方法。

Method: 训练多车辆轨迹预测模型，在微观模拟器中在线评估模型在未知交通条件下的性能，引入新指标评估违规行为。

Result: 尽管输入理想轨迹且轨迹重建误差低，但生成轨迹存在违反交通规则的行为。

Conclusion: 所提出的综合分析工具和新指标能从交通工程角度更好洞察模型性能。

Abstract: Traffic Intersections are vital to urban road networks as they regulate the
movement of people and goods. However, they are regions of conflicting
trajectories and are prone to accidents. Deep Generative models of traffic
dynamics at signalized intersections can greatly help traffic authorities
better understand the efficiency and safety aspects. At present, models are
evaluated on computational metrics that primarily look at trajectory
reconstruction errors. They are not evaluated online in a `live'
microsimulation scenario. Further, these metrics do not adequately consider
traffic engineering-specific concerns such as red-light violations, unallowed
stoppage, etc. In this work, we provide a comprehensive analytics tool to
train, run, and evaluate models with metrics that give better insights into
model performance from a traffic engineering point of view. We train a
state-of-the-art multi-vehicle trajectory forecasting model on a large dataset
collected by running a calibrated scenario of a real-world urban intersection.
We then evaluate the performance of the prediction models, online in a
microsimulator, under unseen traffic conditions. We show that despite using
ideally-behaved trajectories as input, and achieving low trajectory
reconstruction errors, the generated trajectories show behaviors that break
traffic rules. We introduce new metrics to evaluate such undesired behaviors
and present our results.

</details>


### [31] [A Survey of Link Prediction in N-ary Knowledge Graphs](https://arxiv.org/abs/2506.08970)
*Jiyao Wei,Saiping Guan,Da Li,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本文对N元知识图谱（NKGs）的链接预测进行了首次全面综述，涵盖领域概述、方法分类、性能与应用场景分析，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: NKGs能有效表示复杂事实，链接预测对完善NKGs及提升下游应用性能至关重要，该任务受关注但缺乏全面综述。

Method: 对NKGs链接预测领域进行系统梳理，对现有方法进行分类。

Result: 给出NKGs链接预测领域的全面概述，分析了现有方法的性能和应用场景。

Conclusion: 指出了NKGs链接预测领域未来有前景的研究方向。

Abstract: N-ary Knowledge Graphs (NKGs) are a specialized type of knowledge graph
designed to efficiently represent complex real-world facts. Unlike traditional
knowledge graphs, where a fact typically involves two entities, NKGs can
capture n-ary facts containing more than two entities. Link prediction in NKGs
aims to predict missing elements within these n-ary facts, which is essential
for completing NKGs and improving the performance of downstream applications.
This task has recently gained significant attention. In this paper, we present
the first comprehensive survey of link prediction in NKGs, providing an
overview of the field, systematically categorizing existing methods, and
analyzing their performance and application scenarios. We also outline
promising directions for future research.

</details>


### [32] [AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions](https://arxiv.org/abs/2506.09038)
*Polina Kirichenko,Mark Ibrahim,Kamalika Chaudhuri,Samuel J. Bell*

Main category: cs.AI

TL;DR: 本文介绍了用于评估大语言模型弃权能力的AbstentionBench基准，评估发现弃权问题未解决，推理微调会降低弃权能力，精心设计的系统提示无法解决模型对不确定性推理的根本缺陷，并发布该基准以推动研究。


<details>
  <summary>Details</summary>
Motivation: 现实世界用户查询需要大语言模型推理不确定性并选择性弃权，但该领域缺乏系统评估框架。

Method: 引入AbstentionBench基准，对20个前沿大语言模型在20个不同数据集上进行评估。

Result: 弃权是未解决问题，扩大模型规模用处不大；推理微调平均降低24%的弃权能力；精心设计的系统提示无法解决模型对不确定性推理的根本缺陷。

Conclusion: 发布AbstentionBench基准以促进提高大语言模型可靠性的研究。

Abstract: For Large Language Models (LLMs) to be reliably deployed in both everyday and
high-stakes domains, knowing when not to answer is equally critical as
answering correctly. Real-world user queries, which can be underspecified,
ill-posed, or fundamentally unanswerable, require LLMs to reason about
uncertainty and selectively abstain -- i.e., refuse to answer definitively.
However, abstention remains understudied, without a systematic evaluation
framework for modern LLMs. In this work, we introduce AbstentionBench, a
large-scale benchmark for holistically evaluating abstention across 20 diverse
datasets, including questions with unknown answers, underspecification, false
premises, subjective interpretations, and outdated information. Evaluating 20
frontier LLMs reveals abstention is an unsolved problem, and one where scaling
models is of little use. While recent reasoning LLMs have shown impressive
results in complex problem solving, surprisingly, we find that reasoning
fine-tuning degrades abstention (by $24\%$ on average), even for math and
science domains on which reasoning models are explicitly trained. We find that
while a carefully crafted system prompt can boost abstention in practice, it
does not resolve models' fundamental inability to reason about uncertainty. We
release AbstentionBench to foster research into advancing LLM reliability.

</details>


### [33] [VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning](https://arxiv.org/abs/2506.09049)
*Li Kang,Xiufeng Song,Heng Zhou,Yiran Qin,Jie Yang,Xiaohong Liu,Philip Torr,Lei Bai,Zhenfei Yin*

Main category: cs.AI

TL;DR: 本文引入了针对具身多智能体协作的分层基准VIKI - Bench，并提出框架VIKI - R，实验表明VIKI - R性能优于基线方法，二者为具身AI系统的多智能体视觉驱动协作提供了统一测试平台和方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型（VLM）的方法对不同具身类型的支持有限，需要解决动态环境中多具身智能体协作的核心挑战。

Method: 引入VIKI - Bench基准，提出VIKI - R框架，先使用思维链注释演示微调预训练VLM，再在多级奖励信号下进行强化学习。

Result: VIKI - R在所有任务级别上显著优于基线方法，强化学习使异构智能体间出现组合协作模式。

Conclusion: VIKI - Bench和VIKI - R为具身AI系统的多智能体视觉驱动协作提供了统一测试平台和方法。

Abstract: Coordinating multiple embodied agents in dynamic environments remains a core
challenge in artificial intelligence, requiring both perception-driven
reasoning and scalable cooperation strategies. While recent works have
leveraged large language models (LLMs) for multi-agent planning, a few have
begun to explore vision-language models (VLMs) for visual reasoning. However,
these VLM-based approaches remain limited in their support for diverse
embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical
benchmark tailored for embodied multi-agent cooperation, featuring three
structured levels: agent activation, task planning, and trajectory perception.
VIKI-Bench includes diverse robot embodiments, multi-view visual observations,
and structured supervision signals to evaluate reasoning grounded in visual
inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a
two-stage framework that fine-tunes a pretrained vision-language model (VLM)
using Chain-of-Thought annotated demonstrations, followed by reinforcement
learning under multi-level reward signals. Our extensive experiments show that
VIKI-R significantly outperforms baselines method across all task levels.
Furthermore, we show that reinforcement learning enables the emergence of
compositional cooperation patterns among heterogeneous agents. Together,
VIKI-Bench and VIKI-R offer a unified testbed and method for advancing
multi-agent, visual-driven cooperation in embodied AI systems.

</details>


### [34] [ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering](https://arxiv.org/abs/2506.09050)
*Yuki Imajuku,Kohki Horie,Yoichi Iwata,Kensho Aoki,Naohiro Takahashi,Takuya Akiba*

Main category: cs.AI

TL;DR: 介绍ALE - Bench基准用于评估AI系统在算法编程竞赛中的表现，评估前沿大语言模型发现其与人类存在差距，强调该基准对推动AI发展的必要性。


<details>
  <summary>Details</summary>
Motivation: 评估AI系统在如包裹配送路由、机组调度等领域的硬优化问题算法工程中的表现。

Method: 引入ALE - Bench基准，基于AtCoder启发式竞赛的真实任务，提出计算困难且无精确解的优化问题，软件框架支持交互式代理架构。

Result: 评估前沿大语言模型发现其在特定问题上表现高，但在跨问题一致性和长时问题解决能力上与人类有显著差距。

Conclusion: ALE - Bench基准对促进未来AI发展很有必要。

Abstract: How well do AI systems perform in algorithm engineering for hard optimization
problems in domains such as package-delivery routing, crew scheduling, factory
production planning, and power-grid balancing? We introduce ALE-Bench, a new
benchmark for evaluating AI systems on score-based algorithmic programming
contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench
presents optimization problems that are computationally hard and admit no known
exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench
encourages iterative solution refinement over long time horizons. Our software
framework supports interactive agent architectures that leverage test-run
feedback and visualizations. Our evaluation of frontier LLMs revealed that
while they demonstrate high performance on specific problems, a notable gap
remains compared to humans in terms of consistency across problems and
long-horizon problem-solving capabilities. This highlights the need for this
benchmark to foster future AI advancements.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [35] [KP-PINNs: Kernel Packet Accelerated Physics Informed Neural Networks](https://arxiv.org/abs/2506.08563)
*Siyuan Yang,Cheng Song,Zhilu Lai,Wenjia Wang*

Main category: cs.CE

TL;DR: 本文提出Kernel Packet加速的PINNs（KP - PINNs）框架求解微分方程，理论和实验证明其有效且稳定。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs使用L2损失函数求解部分复杂方程时数值解错误且不稳定，需改进。

Method: 提出KP - PINNs框架，用再生核希尔伯特空间（RKHS）范数给出新损失函数表达式，并用Kernel Packet（KP）方法加速计算。

Result: 理论上KP - PINNs在各种微分方程中稳定，数值实验表明其能有效高效求解微分方程。

Conclusion: 该框架为提高基于PINNs的求解器在科学计算中的稳定性和准确性提供了有前景的方向。

Abstract: Differential equations are involved in modeling many engineering problems.
Many efforts have been devoted to solving differential equations. Due to the
flexibility of neural networks, Physics Informed Neural Networks (PINNs) have
recently been proposed to solve complex differential equations and have
demonstrated superior performance in many applications. While the L2 loss
function is usually a default choice in PINNs, it has been shown that the
corresponding numerical solution is incorrect and unstable for some complex
equations. In this work, we propose a new PINNs framework named Kernel Packet
accelerated PINNs (KP-PINNs), which gives a new expression of the loss function
using the reproducing kernel Hilbert space (RKHS) norm and uses the Kernel
Packet (KP) method to accelerate the computation. Theoretical results show that
KP-PINNs can be stable across various differential equations. Numerical
experiments illustrate that KP-PINNs can solve differential equations
effectively and efficiently. This framework provides a promising direction for
improving the stability and accuracy of PINNs-based solvers in scientific
computing.

</details>


### [36] [Rapid cardiac activation prediction for cardiac resynchronization therapy planning using geometric deep learning](https://arxiv.org/abs/2506.08987)
*Ehsan Naghavi,Haifeng Wang,Vahid Ziaei Rad,Julius Guccione,Ghassan Kassab,Vishnu Boddeti,Seungik Baek,Lik-Chuan Lee*

Main category: cs.CE

TL;DR: 本文开发两种几何深度学习模型预测心脏激活时间图用于CRT规划优化，GINO模型表现更优，还开发优化流程，结合GUI有临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: CRT约三分之一患者因导线放置不佳无反应，确定最佳起搏点因个体解剖差异和现有策略局限而困难。

Method: 开发基于GNN和GINO的几何深度学习模型，在有限元模拟生成的合成数据集上训练，用GINO模型开发优化起搏点工作流程。

Result: GINO模型预测误差更低、鲁棒性更好；优化流程比随机选点更能降低最大激活时间。

Conclusion: GINO模型结合GUI有潜力作为个性化CRT术前优化的临床决策支持工具。

Abstract: Cardiac resynchronization therapy (CRT) is a common intervention for patients
with dyssynchronous heart failure, yet approximately one-third of recipients
fail to respond due to suboptimal lead placement. Identifying optimal pacing
sites remains challenging, largely due to patient-specific anatomical
variability and the limitations of current individualized planning strategies.
In a step towards constructing an in-silico approach to help address this
issue, we develop two geometric deep learning (DL) models, based on graph
neural network (GNN) and geometry-informed neural operator (GINO), to predict
cardiac activation time map in real-time for CRT planning and optimization.
Both models are trained on a large synthetic dataset generated from
finite-element (FE) simulations over a wide range of left ventricular (LV)
geometries, pacing site configurations, and tissue conductivities. The GINO
model significantly outperforms the GNN model, with lower prediction errors
(1.14% vs 3.14%) and superior robustness to noise and various mesh
discretization. Using the GINO model, we also develop a workflow for optimizing
the pacing site in CRT from given activation time map and LV geometry. Compared
to randomly selecting a pacing site, the CRT optimization workflow produces a
larger reduction in maximum activation time (20% vs. 8%). In conjunction with
an interactive web-based graphical user interface (GUI) available at
https://dcsim.egr.msu.edu/, the GINO model shows promising potential as a
clinical decision-support tool for personalized pre-procedural CRT
optimization.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [37] [RADAR: Benchmarking Language Models on Imperfect Tabular Data](https://arxiv.org/abs/2506.08249)
*Ken Gu,Zhihan Zhang,Kate Lin,Yuwei Zhang,Akshay Paruchuri,Hong Yu,Mehran Kazemi,Kumar Ayush,A. Ali Heydari,Maxwell A. Xu,Girish Narayanswamy,Yun Liu,Ming-Zher Poh,Yuzhe Yang,Mark Malhotra,Shwetak Patel,Hamid Palangi,Xuhai Xu,Daniel McDuff,Tim Althoff,Xin Liu*

Main category: cs.DB

TL;DR: 提出RADAR基准评估表格数据的数据感知推理，发现前沿模型处理数据工件能力差。


<details>
  <summary>Details</summary>
Motivation: 语言模型在处理表格数据工件时能力未被充分研究，处理不当会影响分析结论有效性。

Method: 开发框架通过编程扰动模拟数据工件，构建包含2980个表查询对的RADAR基准，改变表大小进行评估。

Result: 前沿模型在无数据工件表格表现不错，但有数据工件时性能显著下降。

Conclusion: RADAR灵活可扩展，为推进表格推理提供有价值资源。

Abstract: Language models (LMs) are increasingly being deployed to perform autonomous
data analyses. However, their data awareness -- the ability to recognize,
reason over, and appropriately handle data artifacts such as missing values,
outliers, and logical inconsistencies -- remains underexplored. These artifacts
are especially common in real-world tabular data and, if mishandled, can
significantly compromise the validity of analytical conclusions. To address
this gap, we present RADAR, a benchmark for systematically evaluating
data-aware reasoning on tabular data. We develop a framework to simulate data
artifacts via programmatic perturbations to enable targeted evaluation of model
behavior. RADAR comprises 2980 table query pairs, grounded in real-world data
spanning 9 domains and 5 data artifact types. In addition to evaluating
artifact handling, RADAR systematically varies table size to study how
reasoning performance holds when increasing table size. Our evaluation reveals
that, despite decent performance on tables without data artifacts, frontier
models degrade significantly when data artifacts are introduced, exposing
critical gaps in their capacity for robust, data-aware analysis. Designed to be
flexible and extensible, RADAR supports diverse perturbation types and
controllable table sizes, offering a valuable resource for advancing tabular
reasoning.

</details>


### [38] [LEANN: A Low-Storage Vector Index](https://arxiv.org/abs/2506.08276)
*Yichuan Wang,Shu Liu,Zhifei Li,Yongji Wu,Ziming Mao,Yilong Zhao,Xiao Yan,Zhiying Xu,Yang Zhou,Ion Stoica,Sewon Min,Matei Zaharia,Joseph E. Gonzalez*

Main category: cs.DB

TL;DR: 提出存储高效的近似最近邻搜索索引LEANN，可减少存储开销并保持检索性能。


<details>
  <summary>Details</summary>
Motivation: 基于嵌入的搜索在本地个人数据应用中有高存储开销问题，需降低开销同时保持搜索质量和延迟。

Method: LEANN结合紧凑图结构和高效即时重计算策略。

Result: LEANN将索引大小降至原始数据的5%以下，存储比标准索引小50倍，在真实问答基准测试中2秒内保持90%的前3召回率。

Conclusion: LEANN能在资源受限的个人设备上实现高效准确的检索。

Abstract: Embedding-based search is widely used in applications such as recommendation
and retrieval-augmented generation (RAG). Recently, there is a growing demand
to support these capabilities over personal data stored locally on devices.
However, maintaining the necessary data structure associated with the
embedding-based search is often infeasible due to its high storage overhead.
For example, indexing 100 GB of raw data requires 150 to 700 GB of storage,
making local deployment impractical. Reducing this overhead while maintaining
search quality and latency becomes a critical challenge. In this paper, we
present LEANN, a storage-efficient approximate nearest neighbor (ANN) search
index optimized for resource-constrained personal devices. LEANN combines a
compact graph-based structure with an efficient on-the-fly recomputation
strategy to enable fast and accurate retrieval with minimal storage overhead.
Our evaluation shows that LEANN reduces index size to under 5% of the original
raw data, achieving up to 50 times smaller storage than standard indexes, while
maintaining 90% top-3 recall in under 2 seconds on real-world question
answering benchmarks.

</details>


### [39] [Evaluating Learned Indexes in LSM-tree Systems: Benchmarks,Insights and Design Choices](https://arxiv.org/abs/2506.08671)
*Junfeng Liu,Jiarui Ye,Mengshi Chen,Meng Li,Siqiang Luo*

Main category: cs.DB

TL;DR: 本文针对LSM树系统中学习索引应用问题进行全面系统基准测试，分析现有学习索引，得出意外实验结果并给出实用指南。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅考虑有限学习索引类型，不同学习索引优缺点不明，难以实际应用。

Method: 总结8种现有学习索引工作流程，分析理论成本，确定影响性能的关键因素，在统一平台实现不同设计并跨配置评估。

Result: 实验有意外发现，如给学习索引分配大内存预算时查找提升有限、再训练开销适中。

Conclusion: 为开发者提供智能选择和调优学习索引的实用指南。

Abstract: LSM-tree-based data stores are widely used in industry due to their
exceptional performance. However, as data volumes grow, efficiently querying
large-scale databases becomes increasingly challenging. To address this, recent
studies attempted to integrate learned indexes into LSM-trees to enhance lookup
performance, which has demonstrated promising improvements. Despite this, only
a limited range of learned index types has been considered, and the strengths
and weaknesses of different learned indexes remain unclear, making them
difficult for practical use. To fill this gap, we provide a comprehensive and
systematic benchmark to pursue an in-depth understanding of learned indexes in
LSM-tree systems. In this work, we summarize the workflow of 8 existing learned
indexes and analyze the associated theoretical cost. We also identify several
key factors that significantly influence the performance of learned indexes and
conclude them with a novel configuration space, including various index types,
boundary positions, and granularity. Moreover, we implement different learned
index designs on a unified platform to evaluate across various configurations.
Surprisingly, our experiments reveal several unexpected insights, such as the
marginal lookup enhancement when allocating a large memory budget to learned
indexes and modest retraining overhead of learned indexes. Besides, we also
offer practical guidelines to help developers intelligently select and tune
learned indexes for custom use cases.

</details>


### [40] [QUITE: A Query Rewrite System Beyond Rules with LLM Agents](https://arxiv.org/abs/2506.07675)
*Yuyang Song,Hanxu Yan,Jiale Lao,Yibo Wang,Yufei Li,Yuanchun Zhou,Jianguo Wang,Mingjie Tang*

Main category: cs.DB

TL;DR: 本文提出基于大语言模型代理的无训练、反馈感知系统QUITE进行SQL查询重写，能解决规则方法局限，实验显示性能佳。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的查询重写方法有局限性，人类专家可扩展性差，而大语言模型有语义和推理能力，由此提出用大语言模型进行查询重写。

Method: 设计多智能体框架，开发重写中间件，采用提示注入技术。

Result: QUITE相比现有方法最多减少35.8%查询执行时间，产生的重写数量多24.1%，能处理先前系统未覆盖的查询情况。

Conclusion: QUITE能将SQL查询重写成语义等价且性能更好的形式，覆盖更多查询模式和重写策略。

Abstract: Query rewrite transforms SQL queries into semantically equivalent forms that
run more efficiently. Existing approaches mainly rely on predefined rewrite
rules, but they handle a limited subset of queries and can cause performance
regressions. This limitation stems from three challenges of rule-based query
rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite
rules do not generalize to new query patterns, and (3) some rewrite techniques
cannot be expressed as fixed rules. Motivated by the fact that human experts
exhibit significantly better rewrite ability but suffer from scalability, and
Large Language Models (LLMs) have demonstrated nearly human-level semantic and
reasoning abilities, we propose a new approach of using LLMs to rewrite SQL
queries beyond rules. Due to the hallucination problems in LLMs, directly
applying LLMs often leads to nonequivalent and suboptimal queries. To address
this issue, we propose QUITE (query rewrite), a training-free and
feedback-aware system based on LLM agents that rewrites SQL queries into
semantically equivalent forms with significantly better performance, covering a
broader range of query patterns and rewrite strategies compared to rule-based
methods. Firstly, we design a multi-agent framework controlled by a finite
state machine (FSM) to equip LLMs with the ability to use external tools and
enhance the rewrite process with real-time database feedback. Secondly, we
develop a rewrite middleware to enhance the ability of LLMs to generate
optimized query equivalents. Finally, we employ a novel hint injection
technique to improve execution plans for rewritten queries. Extensive
experiments show that QUITE reduces query execution time by up to 35.8% over
state-of-the-art approaches and produces 24.1% more rewrites than prior
methods, covering query cases that earlier systems did not handle.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [41] [PerfTracker: Online Performance Troubleshooting for Large-scale Model Training in Production](https://arxiv.org/abs/2506.08528)
*Yu Guan,Zhiyu Yin,Haoyu Chen,Sheng Cheng,Chaojie Yang,Tianyin Xu,Yang Zhang,Hanyu Zhao,Yong Li,Dennis Cai,Ennan Zhai*

Main category: cs.DC

TL;DR: 提出首个在线故障排查系统PerfTracker，用于诊断生产中大规模模型训练性能问题，可诊断软硬件问题，已部署到大规模GPU集群。


<details>
  <summary>Details</summary>
Motivation: 现代GPU集群规模大、软硬件交互复杂、训练数据密集，现有故障排查方法不适用于实际训练系统。

Method: 利用细粒度分析进行在线故障排查，有效总结细粒度大规模模型训练函数的运行时行为模式，利用差异可观测性定位根本原因。

Result: 已作为生产服务部署到约10000个GPU的大规模GPU集群，用于诊断各种困难的性能问题。

Conclusion: PerfTracker能有效诊断大规模模型训练中的性能问题，适用于现代GPU集群。

Abstract: Troubleshooting performance problems of large model training (LMT) is
immensely challenging, due to unprecedented scales of modern GPU clusters, the
complexity of software-hardware interactions, and the data intensity of the
training process. Existing troubleshooting approaches designed for traditional
distributed systems or datacenter networks fall short and can hardly apply to
real-world training systems. In this paper, we present PerfTracker, the first
online troubleshooting system utilizing fine-grained profiling, to diagnose
performance issues of large-scale model training in production. PerfTracker can
diagnose performance issues rooted in both hardware (e.g., GPUs and their
interconnects) and software (e.g., Python functions and GPU operations). It
scales to LMT on modern GPU clusters. PerfTracker effectively summarizes
runtime behavior patterns of fine-grained LMT functions via online profiling,
and leverages differential observability to localize the root cause with
minimal production impact. PerfTracker has been deployed as a production
service for large-scale GPU clusters of O(10, 000) GPUs (product homepage
https://help.aliyun.com/zh/pai/user-guide/perftracker-online-performance-analysis-diagnostic-tool).
It has been used to diagnose a variety of difficult performance issues.

</details>


### [42] [Towards Provenance-Aware Earth Observation Workflows: the openEO Case Study](https://arxiv.org/abs/2506.08597)
*H. Omidi,L. Sacco,V. Hutter,G. Irsiegler,M. Claus,M. Schobben,A. Jacob,M. Schramm,S. Fiore*

Main category: cs.DC

TL;DR: 本文提出将数据溯源库yProv4WFs集成到openEO平台以改进地球观测数据操作历史记录，展示此集成助于理解分析工作流。


<details>
  <summary>Details</summary>
Motivation: 地球观测中捕获计算工作流操作历史很重要，数据溯源可收集记录数据产品谱系的元数据。

Method: 将数据溯源库yProv4WFs集成到openEO平台。

Result: 展示了跨地球观测处理链集成数据溯源概念能让研究人员和利益相关者更好理解分析工作流中的流程、依赖和转换。

Conclusion: 通过将数据溯源库集成到openEO平台可改进地球观测数据相关操作，且集成溯源概念有助于理解工作流。

Abstract: Capturing the history of operations and activities during a computational
workflow is significantly important for Earth Observation (EO). The data
provenance helps to collect the metadata that records the lineage of data
products, providing information about how data are generated, transferred,
manipulated, by whom all these operations are performed and through which
processes, parameters, and datasets. This paper presents an approach to improve
those aspects, by integrating the data provenance library yProv4WFs within
openEO, a platform to let users connect to Earth Observation cloud back-ends in
a simple and unified way. In addition, it is demonstrated how the integration
of data provenance concepts across EO processing chains enables researchers and
stakeholders to better understand the flow, the dependencies, and the
transformations involved in analytical workflows.

</details>


### [43] [Blockchain and Edge Computing Nexus: A Large-scale Systematic Literature Review](https://arxiv.org/abs/2506.08636)
*Zeinab Nezami,Zhuolun Li,Chuhao Qin,Fatemeh Banaie,Rabiya Khalid,Evangelos Pournaras*

Main category: cs.DC

TL;DR: 本文对区块链与边缘计算的结合进行系统文献综述，构建新分类法，揭示两者交互模式及应用优势。


<details>
  <summary>Details</summary>
Motivation: 区块链和边缘计算虽有社会影响，但技术和研究领域碎片化，旨在探索两者结合以解决研究挑战。

Method: 从3个数据库收集近6000篇论文，筛选近1000篇，构建含22个特征287个属性的分类法，用定量和机器学习方法研究。

Result: 揭示4种交互模式，关键决定因素为设计、技术和概念验证；显示区块链辅助边缘计算在提升隐私安全上的优势。

Conclusion: 区块链与边缘计算结合有潜力推动创新，为相关研究挑战提供解决方案，尤其在提升隐私安全方面有应用价值。

Abstract: Blockchain and edge computing are two instrumental paradigms of decentralized
computation, driving key advancements in Smart Cities applications such as
supply chain, energy and mobility. Despite their unprecedented impact on
society, they remain significantly fragmented as technologies and research
areas, while they share fundamental principles of distributed systems and
domains of applicability. This paper introduces a novel and large-scale
systematic literature review on the nexus of blockchain and edge computing with
the aim to unravel a new understanding of how the interfacing of the two
computing paradigms can boost innovation to provide solutions to timely but
also long-standing research challenges. By collecting almost 6000 papers from 3
databases and putting under scrutiny almost 1000 papers, we build a novel
taxonomy and classification consisting of 22 features with 287 attributes that
we study using quantitative and machine learning methods. They cover a broad
spectrum of technological, design, epistemological and sustainability aspects.
Results reveal 4 distinguishing patterns of interplay between blockchain and
edge computing with key determinants the public (permissionless) vs. private
(permissioned) design, technology and proof of concepts. They also demonstrate
the prevalence of blockchain-assisted edge computing for improving privacy and
security, in particular for mobile computing applications.

</details>


### [44] [Parallel FFTW on RISC-V: A Comparative Study including OpenMP, MPI, and HPX](https://arxiv.org/abs/2506.08653)
*Alexander Strack,Christopher Taylor,Dirk Pflüger*

Main category: cs.DC

TL;DR: 评估FFTW库在RISC - V上的并行扩展，对比x86 - 64，研究HPX - FFT内存优化效果，为RISC - V大规模并行应用迈出早期一步。


<details>
  <summary>Details</summary>
Motivation: RISC - V硬件发展使重点转向高层并行化，高效并行对RISC - V很重要，需评估FFTW库并行扩展情况。

Method: 评估FFTW库在RISC - V上MPI和OpenMP的并行扩展，与AMD EPYC 7742 CPU对比，研究HPX - FFT在RISC - V上的内存优化效果。

Result: 双精度2D FFT中x86 - 64和RISC - V芯片有8倍性能差异；HPX - FFT内存优化在x86 - 64有效但在RISC - V无效；FFTW with MPI在x86 - 64和RISC - V上到64核都有良好扩展，FFTW with OpenMP需规划。

Conclusion: 研究为RISC - V上大规模并行应用迈出早期一步。

Abstract: Rapid advancements in RISC-V hardware development shift the focus from
low-level optimizations to higher-level parallelization. Recent RISC-V
processors, such as the SOPHON SG2042, have 64 cores. RISC-V processors with
core counts comparable to the SG2042, make efficient parallelization as crucial
for RISC-V as the more established processors such as x86-64. In this work, we
evaluate the parallel scaling of the widely used FFTW library on RISC-V for MPI
and OpenMP. We compare it to a 64-core AMD EPYC 7742 CPU side by side for
different types of FFTW planning. Additionally, we investigate the effect of
memory optimization on RISC-V in HPX-FFT, a parallel FFT library based on the
asynchronous many-task runtime HPX using an FFTW backend. We generally observe
a performance delta between the x86-64 and RISC-V chips of factor eight for
double-precision 2D FFT. Effective memory optimizations in HPX-FFT on x86-64 do
not translate to the RISC-V chip. FFTW with MPI shows good scaling up to 64
cores on x86-64 and RISC-V regardless of planning. In contrast, FFTW with
OpenMP requires measured planning on both architectures to achieve good scaling
up to 64 cores. The results of our study mark an early step on the journey to
large-scale parallel applications running on RISC-V.

</details>


### [45] [Synchronization in Anonymous Networks Under Continuous Dynamics](https://arxiv.org/abs/2506.08661)
*Rida Bazzi,Anya Chaturvedi,Andréa W. Richa,Peter Vargas*

Main category: cs.DC

TL;DR: 提出适用于非同步动态网络的κ - Synchronizer，有相关贡献及开销情况


<details>
  <summary>Details</summary>
Motivation: 在非同步动态网络的最小假设下实现节点模拟动态网络同步算法，解决标准Pull模型无法实现非平凡同步的问题

Method: 扩展Pull通信模型，在节点的每个边缘端口添加单个1位多写原子寄存器

Result: 实现了首个能在半同步动态环境模拟同步算法的确定性同步器，有线性于最大节点度和对数于模拟算法运行时间的内存开销

Conclusion: 将同步器定义扩展到有连续任意边缘动态的网络，给出首个从半同步到同步模型的同步器并应用于现有算法

Abstract: We present the $\kappa$-Synchronizer that works in non-synchronous dynamic
networks under minimal assumptions. Our model allows continuous topological
changes without any guarantee of eventual global or partial stabilization and
assumes that nodes are anonymous. This deterministic synchronizer is the first
to enable nodes to simulate a dynamic network synchronous algorithm for
executions in a semi-synchronous dynamic environment under a weakly-fair node
activation scheduler, despite the absence of a global clock, node ids,
persistent connectivity or any assumptions about the edge dynamics (in both the
synchronous and semi-synchronous environments). In summary, we make the
following contributions: (1) we extend the definition of synchronizers to
networks with continuous arbitrary edge dynamics; (2) we present the first
synchronizer from the semi-synchronous to the synchronous model in a network
with continuous arbitrary edge dynamics; and (3) we present non-trivial
applications of the proposed synchronizer to existing algorithms. We assume an
extension of the Pull communication model by adding a single 1-bit multi-writer
atomic register at each edge-port of a node, since we show that the standard
Pull model is not sufficient to allow for non-trivial synchronization in our
scenario. The $\kappa$-Synchronizer operates with memory overhead at the nodes
that is linear on the maximum node degree and logarithmic on the runtime of the
underlying synchronous algorithm being simulated.

</details>


### [46] [Balancing Fixed Number of Nodes Among Multiple Fixed Clusters](https://arxiv.org/abs/2506.08715)
*Paritosh Ranjan,Surajit Majumder,Prodip Roy,Bhuban Padhan*

Main category: cs.DC

TL;DR: 提出基于实时资源利用率阈值在多个固定集群间动态重新平衡节点的系统和方法，优化资源利用与成本。


<details>
  <summary>Details</summary>
Motivation: 云基础设施用户为容器集群分配固定节点，导致资源利用率低，需优化。

Method: 引入节点平衡集群组（NBCG），通过节点平衡集群平衡器和调整规则引擎管理节点动态共享和重新分配。

Result: 能识别集群资源使用情况，在不增加成本下重新分配节点，若违反阈值会撤销操作。

Conclusion: 该架构优化资源利用和成本，减少资源浪费，提升IBM云服务竞争力。

Abstract: Cloud infrastructure users often allocate a fixed number of nodes to
individual container clusters (e.g., Kubernetes, OpenShift), resulting in
underutilization of computing resources due to asynchronous and variable
workload peaks across clusters. This research proposes a novel system and
method for dynamic rebalancing of a fixed total number of nodes among multiple
fixed clusters based on real-time resource utilization thresholds. By
introducing a Node Balancing Cluster Group (NBCG), clusters are grouped and
allowed to dynamically share nodes through a controlled reallocation mechanism,
managed by a Node Balancing Cluster Balancer and a Resizing Rule Engine. The
system identifies overutilized and underutilized clusters using threshold
parameters, and reassigns nodes without incurring additional provisioning
costs. If reallocation causes a violation of utilization thresholds, the system
reverses the operation to maintain cluster stability. The proposed architecture
not only optimizes resource utilization and operational cost but also
introduces a strategic advantage for cloud service providers like IBM Cloud.
Unlike existing solutions, this approach enables intra-account node sharing
across clusters with strict adherence to user-defined constraints and ensures
consistent cluster state management. This invention has the potential to
significantly reduce computing resource waste and position IBM Cloud services
as more efficient and competitive.

</details>


### [47] [Mycelium: A Transformation-Embedded LSM-Tree](https://arxiv.org/abs/2506.08923)
*Holly Casaletto,Jeff Lefevre,Aldrin Montana,Peter Alvaro*

Main category: cs.DC

TL;DR: 本文提出TE - LSM方法，将数据转换嵌入LSM - trees的压缩过程，以Mycelium为例展示应用，评估其读写性能，结果显示有较好表现。


<details>
  <summary>Details</summary>
Motivation: 降低LSM - trees压缩过程成本，利用压缩过程嵌入有用工作以分摊IO成本和放大。

Method: 提出TE - LSM方法，在RocksDB上构建Mycelium原型，通过跨列族合并机制扩展压缩过程，探索三种数据转换类型，进行成本模型分析和YCSB工作负载评估。

Result: Mycelium写吞吐量开销20%，远低于在压缩外进行数据转换的简单方法的35% - 60%；与RocksDB基线相比，读延迟最多改善425%。

Conclusion: TE - LSM方法能有效降低成本，提升读写性能。

Abstract: Compaction is a necessary, but often costly background process in
write-optimized data structures like LSM-trees that reorganizes incoming data
that is sequentially appended to logs. In this paper, we introduce
Transformation-Embedded LSM-trees (TE-LSM), a novel approach that transparently
embeds a variety of data transformations into the compaction process. While
many others have sought to reduce the high cost of compaction, TE-LSMs leverage
the opportunity to embed other useful work to amortize IO costs and
amplification. We illustrate the use of a TE-LSM in Mycelium, our prototype
built on top of RocksDB that extends the compaction process through a
cross-column-family merging mechanism. Mycelium enables seamless integration of
a transformer interface and aims to better prepare data for future accesses
based on access patterns. We use Mycelium to explore three types of
transformations: splitting column groups, converting data formats, and index
building. In addition to providing a cost model analysis, we evaluate
Mycelium's write and read performance using YCSB workloads. Our results show
that Mycelium incurs a 20% write throughput overhead - significantly lower than
the 35% to 60% overhead observed in naive approaches that perform data
transformations outside of compaction-while achieving up to 425% improvements
in read latency compared to RocksDB baseline.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [48] [Fair Diversity Maximization with Few Representatives](https://arxiv.org/abs/2506.08110)
*Florian Adriaens,Nikolaj Tatti*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Diversity maximization problem is a well-studied problem where the goal is to
find $k$ diverse items. Fair diversity maximization aims to select a diverse
subset of $k$ items from a large dataset, while requiring that each group of
items be well represented in the output. More formally, given a set of items
with labels, our goal is to find $k$ items that maximize the minimum pairwise
distance in the set, while maintaining that each label is represented within
some budget. In many cases, one is only interested in selecting a handful (say
a constant) number of items from each group. In such scenario we show that a
randomized algorithm based on padded decompositions improves the
state-of-the-art approximation ratio to $\sqrt{\log(m)}/(3m)$, where $m$ is the
number of labels. The algorithms work in several stages: ($i$) a preprocessing
pruning which ensures that points with the same label are far away from each
other, ($ii$) a decomposition phase, where points are randomly placed in
clusters such that there is a feasible solution with maximum one point per
cluster and that any feasible solution will be diverse, $(iii)$ assignment
phase, where clusters are assigned to labels, and a representative point with
the corresponding label is selected from each cluster. We experimentally verify
the effectiveness of our algorithm on large datasets.

</details>


### [49] [Testing Suffixient Sets](https://arxiv.org/abs/2506.08225)
*Davide Cenzato,Francisco Olivares,Nicola Prezza*

Main category: cs.DS

TL;DR: 本文介绍后缀集这种新颖的前缀数组压缩技术，并给出判断给定文本位置子集是否为后缀集或最小基数后缀集的线性时间算法。


<details>
  <summary>Details</summary>
Motivation: 解决判断给定文本位置子集是否为后缀集或最小基数后缀集的问题。

Method: 提出线性时间算法。

Result: 成功得出解决上述问题的线性时间算法。

Conclusion: 可以用线性时间算法解决判断给定文本位置子集是否为后缀集或最小基数后缀集的问题。

Abstract: Suffixient sets are a novel prefix array (PA) compression technique based on
subsampling PA (rather than compressing the entire array like previous
techniques used to do): by storing very few entries of PA (in fact, a
compressed number of entries), one can prove that pattern matching via binary
search is still possible provided that random access is available on the text.
In this paper, we tackle the problems of determining whether a given subset of
text positions is (1) a suffixient set or (2) a suffixient set of minimum
cardinality. We provide linear-time algorithms solving these problems.

</details>


### [50] [Towards universally optimal sorting algorithms](https://arxiv.org/abs/2506.08261)
*Sandeep Sen*

Main category: cs.DS

TL;DR: 提出新的算法最优性范式，重审现有排序算法，提出新排序度量及最优算法，该范式有更多应用潜力。


<details>
  <summary>Details</summary>
Motivation: 将仅基于输入大小的最坏情况最优性推广到包含隐式参数的问题相关参数。

Method: 提出新范式，从新视角重审现有排序算法，提出新的排序度量。

Result: 得到基于分区排序的最优算法。

Conclusion: 该算法效率度量范式有进一步有趣应用前景。

Abstract: We formalize a new paradigm for optimality of algorithms, that generalizes
worst-case optimality based only on input-size to problem-dependent parameters
including implicit ones. We re-visit some existing sorting algorithms from this
perspective, and also present a novel measure of sortedness that leads to an
optimal algorithm based on partition sort. This paradigm of measuring
efficiency of algorithms looks promising for further interesting applications
beyond the existing ones.

</details>


### [51] [Optimal Graph Reconstruction by Counting Connected Components in Induced Subgraphs](https://arxiv.org/abs/2506.08405)
*Hadley Black,Arya Mazumdar,Barna Saha,Yinzhan Xu*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The graph reconstruction problem has been extensively studied under various
query models. In this paper, we propose a new query model regarding the number
of connected components, which is one of the most basic and fundamental graph
parameters. Formally, we consider the problem of reconstructing an $n$-node
$m$-edge graph with oracle queries of the following form: provided with a
subset of vertices, the oracle returns the number of connected components in
the induced subgraph. We show $\Theta(\frac{m \log n}{\log m})$ queries in
expectation are both sufficient and necessary to adaptively reconstruct the
graph. In contrast, we show that $\Omega(n^2)$ non-adaptive queries are
required, even when $m = O(n)$. We also provide an $O(m\log n + n\log^2 n)$
query algorithm using only two rounds of adaptivity.

</details>


### [52] [Improving Online Bin Covering with Little Advice](https://arxiv.org/abs/2506.09004)
*Andrej Brodnik,Bengt J. Nilsson,Gordana Vujović*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The online bin covering problem is: given an input sequence of items find a
placement of the items in the maximum number of bins such that the sum of the
items' sizes in each bin is at least~1. Boyar~{\em et~al}.\@~\cite{boyar2021}
present a strategy that with $O(\log \log n)$ bits of advice, where $n$ is the
length of the input sequence, achieves a competitive ratio of
$8/15\approx0.5333\ldots$. We show that with a strengthened analysis and some
minor improvements, the same strategy achieves the significantly improved
competitive ratio of~$135/242\approx0.5578\ldots$, still using $O(\log \log n)$
bits of advice.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [53] [Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval](https://arxiv.org/abs/2506.08074)
*Abdellah Ghassel,Ian Robinson,Gabriel Tanase,Hal Cooper,Bryan Thompson,Zhen Han,Vassilis N. Ioannidis,Soji Adeshina,Huzefa Rangwala*

Main category: cs.IR

TL;DR: 提出分层词法图HLG及两种检索器，引入合成数据集生成管道，实验显示方法优于朴素基于块的RAG。


<details>
  <summary>Details</summary>
Motivation: 解决检索增强生成（RAG）在跨语义远距离文档拼凑答案时的不足，以及现有基准缺乏评估多跳摘要系统的复杂度问题。

Method: 构建三层索引HLG，基于HLG构建StatementGraphRAG和TopicGraphRAG两个检索器，引入合成数据集生成管道。

Result: 在五个数据集上实验，方法比朴素基于块的RAG在检索召回率和正确性上平均相对提升23.1%。

Conclusion: 所提方法有效，开源代码可在指定链接获取。

Abstract: Retrieval-Augmented Generation (RAG) grounds large language models in
external evidence, yet it still falters when answers must be pieced together
across semantically distant documents. We close this gap with the Hierarchical
Lexical Graph (HLG), a three-tier index that (i) traces every atomic
proposition to its source, (ii) clusters propositions into latent topics, and
(iii) links entities and relations to expose cross-document paths. On top of
HLG we build two complementary, plug-and-play retrievers: StatementGraphRAG,
which performs fine-grained entity-aware beam search over propositions for
high-precision factoid questions, and TopicGraphRAG, which selects coarse
topics before expanding along entity links to supply broad yet relevant context
for exploratory queries. Additionally, existing benchmarks lack the complexity
required to rigorously evaluate multi-hop summarization systems, often focusing
on single-document queries or limited datasets. To address this, we introduce a
synthetic dataset generation pipeline that curates realistic, multi-document
question-answer pairs, enabling robust evaluation of multi-hop retrieval
systems. Extensive experiments across five datasets demonstrate that our
methods outperform naive chunk-based RAG achieving an average relative
improvement of 23.1% in retrieval recall and correctness. Open-source Python
library is available at https://github.com/awslabs/graphrag-toolkit.

</details>


### [54] [No Stupid Questions: An Analysis of Question Query Generation for Citation Recommendation](https://arxiv.org/abs/2506.08196)
*Brian D. Zimmerman,Julien Aubert-Béduchaud,Florian Boudin,Akiko Aizawa,Olga Vechtomova*

Main category: cs.IR

TL;DR: 现有引文推荐技术受限，利用GPT - 4o - mini生成问题作检索查询，部分效果优于关键词查询，还提出MMR - RBO方法。


<details>
  <summary>Details</summary>
Motivation: 现有引文推荐技术受文章内容和元数据限制，需新方法改进。

Method: 利用GPT - 4o - mini作为好奇助手生成问题，将其作为检索查询，提出MMR - RBO方法筛选问题。

Result: 部分生成问题作为查询比同一模型生成的提取式关键词查询效果更好，各问题查询产生独特结果集。

Conclusion: 所有问题查询都有价值，不存在愚蠢的问题。

Abstract: Existing techniques for citation recommendation are constrained by their
adherence to article contents and metadata. We leverage GPT-4o-mini's latent
expertise as an inquisitive assistant by instructing it to ask questions which,
when answered, could expose new insights about an excerpt from a scientific
article. We evaluate the utility of these questions as retrieval queries,
measuring their effectiveness in retrieving and ranking masked target
documents. In some cases, generated questions ended up being better queries
than extractive keyword queries generated by the same model. We additionally
propose MMR-RBO, a variation of Maximal Marginal Relevance (MMR) using
Rank-Biased Overlap (RBO) to identify which questions will perform
competitively with the keyword baseline. As all question queries yield unique
result sets, we contend that there are no stupid questions.

</details>


### [55] [Serendipitous Recommendation with Multimodal LLM](https://arxiv.org/abs/2506.08283)
*Haoting Wang,Jianling Wang,Hao Li,Fangjun Yi,Mengyu Fu,Youwei Zhang,Yifan Liu,Liang Liu,Minmin Chen,Ed H. Chi,Lichan Hong,Haokai Lu*

Main category: cs.IR

TL;DR: 本文提出分层框架，用微调的多模态大语言模型指导传统推荐模型，提升推荐的意外性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统难提供意外或新颖物品，多模态大语言模型虽有潜力但在大规模平台集成有挑战。

Method: 提出分层框架，让微调的多模态大语言模型为传统推荐模型提供高层指导；采用思维链策略发现用户新兴趣。

Result: 在服务数十亿用户的商业短视频平台的实时实验表明，该方法显著提升了推荐的意外性和用户满意度。

Conclusion: 所提出的基于多模态大语言模型的方法有效，能改善推荐效果。

Abstract: Conventional recommendation systems succeed in identifying relevant content
but often fail to provide users with surprising or novel items. Multimodal
Large Language Models (MLLMs) possess the world knowledge and multimodal
understanding needed for serendipity, but their integration into
billion-item-scale platforms presents significant challenges. In this paper, we
propose a novel hierarchical framework where fine-tuned MLLMs provide
high-level guidance to conventional recommendation models, steering them
towards more serendipitous suggestions. This approach leverages MLLM strengths
in understanding multimodal content and user interests while retaining the
efficiency of traditional models for item-level recommendation. This mitigates
the complexity of applying MLLMs directly to vast action spaces. We also
demonstrate a chain-of-thought strategy enabling MLLMs to discover novel user
interests by first understanding video content and then identifying relevant
yet unexplored interest clusters. Through live experiments within a commercial
short-form video platform serving billions of users, we show that our
MLLM-powered approach significantly improves both recommendation serendipity
and user satisfaction.

</details>


### [56] [Rule-Assisted Attribute Embedding](https://arxiv.org/abs/2506.08314)
*Sibo Zhao,Michael Bewong,Selasi Kwashie,Junwei Hu,Zaiwen Feng*

Main category: cs.IR

TL;DR: 现有推荐系统忽视属性图属性信息，提出RAE方法，实验显示其性能优于基线且对稀疏数据更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统忽视属性图丰富属性信息，现有GCN模型处理属性方式不佳，限制推荐效果。

Method: 提出RAE方法，从属性图挖掘语义规则指导属性嵌入，通过基于规则的随机游走生成丰富属性表示并集成到GCN中。

Result: 在真实数据集上，RAE在Recall@20和NDCG@20上平均比现有基线高10.6%，对稀疏数据和缺失属性更鲁棒。

Conclusion: 利用结构化属性信息对推荐任务有价值。

Abstract: Recommendation systems often overlook the rich attribute information embedded
in property graphs, limiting their effectiveness. Existing graph convolutional
network (GCN) models either ignore attributes or rely on simplistic <user,
item, attribute> triples, failing to capture deeper semantic structures. We
propose RAE (Rule- Assisted Approach for Attribute Embedding), a novel method
that improves recommendations by mining semantic rules from property graphs to
guide attribute embedding. RAE performs rule-based random walks to generate
enriched attribute representations, which are integrated into GCNs. Experiments
on real-world datasets (BlogCatalog, Flickr) show that RAE outperforms
state-of-the-art baselines by 10.6% on average in Recall@20 and NDCG@20. RAE
also demonstrates greater robustness to sparse data and missing attributes,
highlighting the value of leveraging structured attribute information in
recommendation tasks.

</details>


### [57] [Bridging RDF Knowledge Graphs with Graph Neural Networks for Semantically-Rich Recommender Systems](https://arxiv.org/abs/2506.08743)
*Michael Färber,David Lamprecht,Yuni Susanti*

Main category: cs.IR

TL;DR: 本文提出将RDF知识图谱与GNNs全面集成，通过多场景实验证明利用RDF KGs语义信息能提升推荐系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的推荐系统未充分利用W3C标准RDF下知识图谱的丰富语义信息。

Method: 提出将RDF KGs与GNNs全面集成，利用RDF对象属性拓扑信息和数据类型属性内容信息，深入评估不同GNNs。

Result: 在涉及数百万节点的RDF图的多个推荐场景实验中，证明利用RDF KGs语义信息可显著改善推荐系统。

Conclusion: 利用RDF KGs语义丰富性可显著提升推荐系统性能，为链接开放数据云的基于GNN的推荐系统奠定基础。

Abstract: Graph Neural Networks (GNNs) have substantially advanced the field of
recommender systems. However, despite the creation of more than a thousand
knowledge graphs (KGs) under the W3C standard RDF, their rich semantic
information has not yet been fully leveraged in GNN-based recommender systems.
To address this gap, we propose a comprehensive integration of RDF KGs with
GNNs that utilizes both the topological information from RDF object properties
and the content information from RDF datatype properties. Our main focus is an
in-depth evaluation of various GNNs, analyzing how different semantic feature
initializations and types of graph structure heterogeneity influence their
performance in recommendation tasks. Through experiments across multiple
recommendation scenarios involving multi-million-node RDF graphs, we
demonstrate that harnessing the semantic richness of RDF KGs significantly
improves recommender systems and lays the groundwork for GNN-based recommender
systems for the Linked Open Data cloud. The code and data are available on our
GitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation

</details>


### [58] [Reinforcement Fine-Tuning for Reasoning towards Multi-Step Multi-Source Search in Large Language Models](https://arxiv.org/abs/2506.08352)
*Wentao Shi,Yiqing Shen*

Main category: cs.IR

TL;DR: 提出R - Search单LLM搜索框架解决现有搜索增强方法的局限，通过ReFT方法优化，实验显示优于现有方法且提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有搜索增强方法存在局限，多智能体搜索框架计算开销大，单LLM工具调用方法受限，解决LLMs在处理训练语料知识阈值后事件查询的事实性局限。

Method: 提出R - Search单LLM搜索框架，将多步规划、多源搜索执行和答案合成统一；提出基于GRPO的Reinforcement Fine - Tuning (ReFT)方法和多组件奖励函数。

Result: 在多个基准测试中，R - Search优于现有方法，上下文令牌使用减少70%，执行延迟降低约50%。

Conclusion: R - Search能有效解决现有搜索增强方法的问题，提高效率和性能。

Abstract: Large language models (LLMs) can face factual limitations when responding to
time-sensitive queries about recent events that arise after their knowledge
thresholds in the training corpus. Existing search-augmented approaches fall
into two categories, each with distinct limitations: multi-agent search
frameworks incur substantial computational overhead by separating search
planning and response synthesis across multiple LLMs, while single-LLM
tool-calling methods restrict themselves to sequential planned, single-query
searches from sole search sources. We present Reasoning-Search (R-Search), a
single-LLM search framework that unifies multi-step planning, multi-source
search execution, and answer synthesis within one coherent inference process.
Innovatively, it structure the output into four explicitly defined components,
including reasoning steps that guide the search process (<think>), a
natural-language directed acyclic graph that represents the search plans with
respect to diverse sources (<search>), retrieved results from executing the
search plans (<result>), and synthesized final answers (<answer>). To enable
effective generation of these structured outputs, we propose a specialized
Reinforcement Fine-Tuning (ReFT) method based on GRPO, together with a
multi-component reward function that optimizes LLM's answer correctness,
structural validity of the generated DAG, and adherence to the defined output
format. Experimental evaluation on FinSearchBench-24, SearchExpertBench-25, and
seven Q and A benchmarks demonstrates that R-Search outperforms
state-of-the-art methods, while achieving substantial efficiency gains through
70% reduction in context token usage and approximately 50% decrease in
execution latency. Code is available at
https://github.com/wentao0429/Reasoning-search.

</details>


### [59] [NAM: A Normalization Attention Model for Personalized Product Search In Fliggy](https://arxiv.org/abs/2506.08382)
*Shui Liu,Mingyuan Tao,Maofei Que,Pan Li,Dong Li,Shenghua Ni,Zhuoran Zhuang*

Main category: cs.IR

TL;DR: 本文提出NAM模型解决个性化产品搜索中以往方法忽视商品视角的问题，实验表明该模型优于基线模型，线上测试转化率提升0.8%。


<details>
  <summary>Details</summary>
Motivation: 以往个性化搜索查询研究多关注用户因素，忽略商品视角，导致流行商品和长尾商品转化率估计不准确、用户购买倾向与历史行为相关性判断问题。

Method: 提出Normalization Attention Model (NAM)，利用Inverse Item Frequency (IIF)和门控机制优化“何时个性化”，从全局视角归一化注意力机制优化“如何个性化”。

Result: 综合实验显示NAM模型显著优于现有基线模型；在Fliggy的线上A/B测试中，转化率较最新生产系统提升0.8%。

Conclusion: 提出的NAM模型能有效解决个性化产品搜索中忽视商品视角带来的问题，提升搜索效果和转化率。

Abstract: Personalized product search provides significant benefits to e-commerce
platforms by extracting more accurate user preferences from historical
behaviors. Previous studies largely focused on the user factors when
personalizing the search query, while ignoring the item perspective, which
leads to the following two challenges that we summarize in this paper: First,
previous approaches relying only on co-occurrence frequency tend to
overestimate the conversion rates for popular items and underestimate those for
long-tail items, resulting in inaccurate item similarities; Second, user
purchasing propensity is highly heterogeneous according to the popularity of
the target item: it is less correlated with the user's historical behavior for
a popular item and more correlated for a long-tail item. To address these
challenges, in this paper we propose NAM, a Normalization Attention Model,
which optimizes ''when to personalize'' by utilizing Inverse Item Frequency
(IIF) and employing a gating mechanism, as well as optimizes ''how to
personalize'' by normalizing the attention mechanism from a global perspective.
Through comprehensive experiments, we demonstrate that our proposed NAM model
significantly outperforms state-of-the-art baseline models. Furthermore, we
conducted an online A/B test at Fliggy, and obtained a significant improvement
of 0.8% over the latest production system in conversion rate.

</details>


### [60] [MERIT: A Merchant Incentive Ranking Model for Hotel Search & Ranking](https://arxiv.org/abs/2506.08442)
*Shigang Quan,Hailong Tan,Shui Liu,Zhenzhe zheng,Ruihao Zhu,Liangyue Li,Quan Lu,Fan Wu*

Main category: cs.IR

TL;DR: 文章提出MERIT模型，将酒店商家目标融入酒店搜索与排名系统设计，解决相关挑战，离线实验和在线A/B测试显示效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有在线旅游平台的酒店搜索与排名系统主要关注平台收入，本文旨在将酒店商家目标纳入设计以实现激励循环。

Method: 提出MERIT模型，定义新的商家竞争力指数（MCI），设计商家塔来建模MCI与排名分数的关系，采用单调结构和多目标分层成对损失函数。

Result: 离线实验表明MERIT在优化消费者和商家需求方面优于其他方法，在线A/B测试使MCI分数提高3.02%。

Conclusion: MERIT模型能同时兼顾商家和消费者利益，有效解决设计中的挑战。

Abstract: Online Travel Platforms (OTPs) have been working on improving their hotel
Search & Ranking (S&R) systems that facilitate efficient matching between
consumers and hotels. Existing OTPs focus almost exclusively on improving
platform revenue. In this work, we take a first step in incorporating hotel
merchants' objectives into the design of hotel S&R systems to achieve an
incentive loop: the OTP tilts impressions and better-ranked positions to
merchants with high quality, and in return, the merchants provide better
service to consumers. Three critical design challenges need to be resolved to
achieve this incentive loop: Matthew Effect in the consumer feedback-loop,
unclear relation between hotel quality and performance, and conflicts between
short-term and long-term revenue. To address these challenges, we propose
MERIT, a MERchant IncenTive ranking model, which can simultaneously take the
interests of merchants and consumers into account. We define a new Merchant
Competitiveness Index (MCI) to represent hotel merchant quality and propose a
new Merchant Tower to model the relation between MCI and ranking scores. Also,
we design a monotonic structure for Merchant Tower to provide a clear relation
between hotel quality and performance. Finally, we propose a Multi-objective
Stratified Pairwise Loss, which can mitigate the conflicts between OTP's
short-term and long-term revenue. The offline experiment results indicate that
MERIT outperforms these methods in optimizing the demands of consumers and
merchants. Furthermore, we conduct an online A/B test and obtain an improvement
of 3.02% for the MCI score.

</details>


### [61] [TSRec: Enhancing Repeat-Aware Recommendation from a Temporal-Sequential Perspective](https://arxiv.org/abs/2506.08531)
*Shigang Quan,Shui Liu,Zhenzhe Zheng,Fan Wu*

Main category: cs.IR

TL;DR: 本文研究提升重复感知推荐的固有特征，提出TSRec模型，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 重复消费常见，现有重复感知推荐需进一步提升，研究固有特征以增强推荐效果。

Method: 从时间和顺序两方面探索用户行为序列特征，提出TSRec模型，含UTRM、ITRM和SRAM三个组件。

Result: 在三个公开基准上的实验表明TSRec优于现有方法。

Conclusion: TSRec模型能有效增强重复感知推荐。

Abstract: Repeat consumption, such as repurchasing items and relistening songs, is a
common scenario in daily life. To model repeat consumption, the repeat-aware
recommendation has been proposed to predict which item will be re-interacted
based on the user-item interactions. In this paper, we investigate various
inherent characteristics to enhance the repeat-aware recommendation.
Specifically, we explore these characteristics from two aspects: one is from
the temporal aspect where we consider the time interval relationship in the
user behavior sequence; the other is from the sequential aspect where we
consider the sequential-level relationship in the user behavior sequence. And
our intuition is that both the temporal pattern and sequential pattern will
reflect users' intentions of repeat consumption. By utilizing these two
patterns, a novel model called Temporal and Sequential repeat-aware
Recommendation(TSRec for short) is proposed to enhance repeat-aware
recommendation. TSRec has three main components: 1) User-specific Temporal
Representation Module (UTRM), which encodes and extracts user historical repeat
temporal information. 2)Item-specific Temporal Representation Module (ITRM),
which incorporates item time interval information as side information to
alleviate the data sparsity problem of user repeat behavior sequence. 3)
Sequential Repeat-Aware Module (SRAM), which represents the similarity between
the user's current and the last repeat sequences. Extensive experimental
results on three public benchmarks demonstrate the superiority of TSRec over
state-of-the-art methods. The implementation code is available
https://anonymous.4open.science/r/TSRec-2306/.

</details>


### [62] [Leveraging LLMs to Evaluate Usefulness of Document](https://arxiv.org/abs/2506.08626)
*Xingzhu Wang,Erhan Zhang,Yiqun Chen,Jinghan Xuan,Yucheng Hou,Yitong Xu,Ying Nie,Shuaiqiang Wang,Dawei Yin,Jiaxin Mao*

Main category: cs.IR

TL;DR: 研究利用大语言模型生成多级有用性标签用于评估，提出以用户为中心框架，超越第三方标注方法，标签提升满意度预测模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统Cranfield范式难以有效衡量用户满意度，相关性标注成本高，探索利用大语言模型解决这些问题。

Method: 引入整合用户搜索上下文和行为数据到LLMs的以用户为中心评估框架，采用级联判断结构进行多级有用性评估。

Result: LLMs在上下文和行为信息引导下能准确评估有用性，超越第三方标注方法；标签能显著提升满意度预测模型性能。

Conclusion: 所提出的利用LLMs生成多级有用性标签的方法可行且有效，可用于评估和预测用户满意度。

Abstract: The conventional Cranfield paradigm struggles to effectively capture user
satisfaction due to its weak correlation between relevance and satisfaction,
alongside the high costs of relevance annotation in building test collections.
To tackle these issues, our research explores the potential of leveraging large
language models (LLMs) to generate multilevel usefulness labels for evaluation.
We introduce a new user-centric evaluation framework that integrates users'
search context and behavioral data into LLMs. This framework uses a cascading
judgment structure designed for multilevel usefulness assessments, drawing
inspiration from ordinal regression techniques. Our study demonstrates that
when well-guided with context and behavioral information, LLMs can accurately
evaluate usefulness, allowing our approach to surpass third-party labeling
methods. Furthermore, we conduct ablation studies to investigate the influence
of key components within the framework. We also apply the labels produced by
our method to predict user satisfaction, with real-world experiments indicating
that these labels substantially improve the performance of satisfaction
prediction models.

</details>


### [63] [Multimodal Representation Alignment for Cross-modal Information Retrieval](https://arxiv.org/abs/2506.08774)
*Fan Xu,Luis A. Leiva*

Main category: cs.IR

TL;DR: 研究多模态检索中视觉和文本嵌入的几何关系，用多种相似度度量对齐特征，发现Wasserstein距离可衡量模态差距，余弦相似度表现最佳，传统架构不足以捕捉复杂交互。


<details>
  <summary>Details</summary>
Motivation: 解决野外多模态检索中的特征对齐问题，即根据一种模态输入找到另一种模态的对应表示。

Method: 研究视觉和文本嵌入的几何关系，使用四种标准相似度度量和两种基于神经网络的学习度量来对齐表示。

Result: Wasserstein距离可衡量模态差距，余弦相似度在特征对齐任务中表现优于其他度量，传统架构不足以捕捉图像和文本表示的复杂交互。

Conclusion: 为多模态信息检索，特别是现实世界的跨模态应用提供了新见解和实际考虑。

Abstract: Different machine learning models can represent the same underlying concept
in different ways. This variability is particularly valuable for in-the-wild
multimodal retrieval, where the objective is to identify the corresponding
representation in one modality given another modality as input. This challenge
can be effectively framed as a feature alignment problem. For example, given a
sentence encoded by a language model, retrieve the most semantically aligned
image based on features produced by an image encoder, or vice versa. In this
work, we first investigate the geometric relationships between visual and
textual embeddings derived from both vision-language models and combined
unimodal models. We then align these representations using four standard
similarity metrics as well as two learned ones, implemented via neural
networks. Our findings indicate that the Wasserstein distance can serve as an
informative measure of the modality gap, while cosine similarity consistently
outperforms alternative metrics in feature alignment tasks. Furthermore, we
observe that conventional architectures such as multilayer perceptrons are
insufficient for capturing the complex interactions between image and text
representations. Our study offers novel insights and practical considerations
for researchers working in multimodal information retrieval, particularly in
real-world, cross-modal applications.

</details>


### [64] [Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion](https://arxiv.org/abs/2506.04760)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.IR

TL;DR: 本文提出Exp4Fuse框架用于零样本大模型查询扩展以提升稀疏检索性能，实验表明其效果超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大模型生成假设文档进行查询扩展的方法成本高、计算量大，需探索新方法提升稀疏检索。

Method: 引入Exp4Fuse融合排序框架，同时考虑原始查询和大模型增强查询两条检索路径，用稀疏检索器生成两个排序列表，再用改进的互反排名融合方法融合。

Result: 在多个数据集上的实验显示，Exp4Fuse超越现有大模型查询扩展方法，结合先进稀疏检索器在多个基准测试中达SOTA。

Conclusion: Exp4Fuse在提升稀疏检索的查询扩展方面性能优越、效果显著。

Abstract: Large Language Models (LLMs) have shown potential in generating hypothetical
documents for query expansion, thereby enhancing information retrieval
performance. However, the efficacy of this method is highly dependent on the
quality of the generated documents, which often requires complex prompt
strategies and the integration of advanced dense retrieval techniques. This can
be both costly and computationally intensive. To mitigate these limitations, we
explore the use of zero-shot LLM-based query expansion to improve sparse
retrieval, particularly for learned sparse retrievers. We introduce a novel
fusion ranking framework, Exp4Fuse, which enhances the performance of sparse
retrievers through an indirect application of zero-shot LLM-based query
expansion. Exp4Fuse operates by simultaneously considering two retrieval
routes-one based on the original query and the other on the LLM-augmented
query. It then generates two ranked lists using a sparse retriever and fuses
them using a modified reciprocal rank fusion method. We conduct extensive
evaluations of Exp4Fuse against leading LLM-based query expansion methods and
advanced retrieval techniques on three MS MARCO-related datasets and seven
low-resource datasets. Experimental results reveal that Exp4Fuse not only
surpasses existing LLM-based query expansion methods in enhancing sparse
retrievers but also, when combined with advanced sparse retrievers, achieves
SOTA results on several benchmarks. This highlights the superior performance
and effectiveness of Exp4Fuse in improving query expansion for sparse
retrieval.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [65] [KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache](https://arxiv.org/abs/2506.08018)
*Fei Li,Song Liu,Weiguo Wu,Shiqiang Nie,Jinyu Wang*

Main category: cs.LG

TL;DR: 提出KVmix混合精度量化方法，在LLMs推理中平衡准确性与效率，实现低内存使用和高推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理时KV Cache内存需求高，现有量化方法存在不足，需在内存、准确性和吞吐量间权衡。

Method: 利用基于梯度的重要性分析进行层特定的位宽分配，引入动态长上下文优化策略，提供高效低比特量化和CUDA内核。

Result: 在Llama和Mistral等模型上，以极低量化配置实现近乎无损推理性能，实现4.9倍内存压缩和5.3倍推理吞吐量加速。

Conclusion: KVmix能有效解决大语言模型推理时KV Cache的内存问题，在准确性和效率间取得平衡。

Abstract: The high memory demands of the Key-Value (KV) Cache during the inference of
Large Language Models (LLMs) severely restrict their deployment in
resource-constrained platforms. Quantization can effectively alleviate the
memory pressure caused by KV Cache. However, existing methods either rely on
static one-size-fits-all precision allocation or fail to dynamically prioritize
critical KV in long-context tasks, forcing memory-accuracy-throughput
tradeoffs. In this work, we propose a novel mixed-precision quantization method
for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to
evaluate how individual Key and Value projection matrices affect the model
loss, enabling layer-specific bit-width allocation for mix-precision
quantization. It dynamically prioritizes higher precision for important layers
while aggressively quantizing less influential ones, achieving a tunable
balance between accuracy and efficiency. KVmix also introduces a dynamic
long-context optimization strategy that adaptively keeps full-precision KV
pairs for recent pivotal tokens and compresses older ones, achieving
high-quality sequence generation with low memory usage. Additionally, KVmix
provides efficient low-bit quantization and CUDA kernels to optimize
computational overhead. On LLMs such as Llama and Mistral, KVmix achieves
near-lossless inference performance with extremely low quantization
configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x
memory compression and a 5.3x speedup in inference throughput.

</details>


### [66] [Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework](https://arxiv.org/abs/2506.08231)
*Melissa Estevez,Nisha Singh,Lauren Dyson,Blythe Adamson,Qianyu Yuan,Megan W. Hildner,Erin Fidyk,Olive Mbah,Farhad Khan,Kathi Seidl-Rathkopf,Aaron B. Cohen*

Main category: cs.LG

TL;DR: 本文提出评估大语言模型提取临床数据质量的框架，以应对数据可靠性等挑战，推动行业标准。


<details>
  <summary>Details</summary>
Motivation: 大语言模型提取临床数据虽提升效率，但带来可靠性、准确性和公平性等挑战，现有质量保证框架无法满足需求。

Method: 提出综合框架，包含与专家人工抽象对比的变量级性能基准测试、内部一致性和合理性的自动验证检查，以及与人工抽象数据集或外部标准对比的复制分析，还支持按人口亚组分层评估偏差。

Result: 该框架可识别需改进的变量、检测潜在错误、确认数据集适用性，支持偏差评估。

Conclusion: 该框架提供严谨透明的评估方法，推动行业标准，支持肿瘤研究和实践中人工智能证据生成的可靠使用。

Abstract: Large language models (LLMs) are increasingly used to extract clinical data
from electronic health records (EHRs), offering significant improvements in
scalability and efficiency for real-world data (RWD) curation in oncology.
However, the adoption of LLMs introduces new challenges in ensuring the
reliability, accuracy, and fairness of extracted data, which are essential for
research, regulatory, and clinical applications. Existing quality assurance
frameworks for RWD and artificial intelligence do not fully address the unique
error modes and complexities associated with LLM-extracted data. In this paper,
we propose a comprehensive framework for evaluating the quality of clinical
data extracted by LLMs. The framework integrates variable-level performance
benchmarking against expert human abstraction, automated verification checks
for internal consistency and plausibility, and replication analyses comparing
LLM-extracted data to human-abstracted datasets or external standards. This
multidimensional approach enables the identification of variables most in need
of improvement, systematic detection of latent errors, and confirmation of
dataset fitness-for-purpose in real-world research. Additionally, the framework
supports bias assessment by stratifying metrics across demographic subgroups.
By providing a rigorous and transparent method for assessing LLM-extracted RWD,
this framework advances industry standards and supports the trustworthy use of
AI-powered evidence generation in oncology research and practice.

</details>


### [67] [Gridding Forced Displacement using Semi-Supervised Learning](https://arxiv.org/abs/2506.08019)
*Andrew Wells,Geraldine Henningsen,Brice Bolane Tchinde Kengne*

Main category: cs.LG

TL;DR: 提出半监督方法将撒哈拉以南25个非洲国家难民统计数据从行政区细化到0.5度网格单元，精度高，能助于理解难民流离失所驱动因素。


<details>
  <summary>Details</summary>
Motivation: 将难民统计数据从行政边界细化到网格单元，以识别更细致的流离失所模式，深入理解流离失所驱动因素。

Method: 结合UNHCR的ProGres登记数据、谷歌开放建筑的卫星建筑足迹和OpenStreetMap人口聚居地的坐标数据，使用标签传播算法。

Result: 将超1000万难民观测数据准确放入网格单元的平均准确率达92.9%，能识别以往被掩盖的局部流离失所模式。

Conclusion: 高分辨率数据集为深入理解流离失所驱动因素奠定基础。

Abstract: We present a semi-supervised approach that disaggregates refugee statistics
from administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan
African countries. By integrating UNHCR's ProGres registration data with
satellite-derived building footprints from Google Open Buildings and location
coordinates from OpenStreetMap Populated Places, our label spreading algorithm
creates spatially explicit refugee statistics at high granularity.This
methodology achieves 92.9% average accuracy in placing over 10 million refugee
observations into appropriate grid cells, enabling the identification of
localized displacement patterns previously obscured in broader regional and
national statistics. The resulting high-resolution dataset provides a
foundation for a deeper understanding of displacement drivers.

</details>


### [68] [Learning to Lead: Incentivizing Strategic Agents in the Dark](https://arxiv.org/abs/2506.08438)
*Yuchen Wu,Xinyi Zhong,Zhuoran Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study an online learning version of the generalized principal-agent model,
where a principal interacts repeatedly with a strategic agent possessing
private types, private rewards, and taking unobservable actions. The agent is
non-myopic, optimizing a discounted sum of future rewards and may strategically
misreport types to manipulate the principal's learning. The principal,
observing only her own realized rewards and the agent's reported types, aims to
learn an optimal coordination mechanism that minimizes strategic regret. We
develop the first provably sample-efficient algorithm for this challenging
setting. Our approach features a novel pipeline that combines (i) a delaying
mechanism to incentivize approximately myopic agent behavior, (ii) an
innovative reward angle estimation framework that uses sector tests and a
matching procedure to recover type-dependent reward functions, and (iii) a
pessimistic-optimistic LinUCB algorithm that enables the principal to explore
efficiently while respecting the agent's incentive constraints. We establish a
near optimal $\tilde{O}(\sqrt{T}) $ regret bound for learning the principal's
optimal policy, where $\tilde{O}(\cdot) $ omits logarithmic factors. Our
results open up new avenues for designing robust online learning algorithms for
a wide range of game-theoretic settings involving private types and strategic
agents.

</details>


### [69] [Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation](https://arxiv.org/abs/2506.08020)
*Zi-Ying Chen,Chuan-Xian Ren,Hong Yan*

Main category: cs.LG

TL;DR: 提出双级不平衡最优传输（BUOT）模型解决部分领域适应（PDA）问题，实验验证其竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有加权框架处理PDA问题时对集群结构探索不足，权重对不准确预测敏感且会导致异常类混淆。

Method: 提出BUOT模型，引入样本级和类级传输的合作机制，结合标签感知传输成本确保局部传输结构并推导快速计算公式。

Result: 在基准数据集上的大量实验验证了BUOT的竞争力。

Conclusion: BUOT模型能有效解决PDA问题，在跨域样本对齐和异常类区分方面表现良好。

Abstract: Partial domain adaptation (PDA) problem requires aligning cross-domain
samples while distinguishing the outlier classes for accurate knowledge
transfer. The widely used weighting framework tries to address the outlier
classes by introducing the reweighed source domain with a similar label
distribution to the target domain. However, the empirical modeling of weights
can only characterize the sample-wise relations, which leads to insufficient
exploration of cluster structures, and the weights could be sensitive to the
inaccurate prediction and cause confusion on the outlier classes. To tackle
these issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model
to simultaneously characterize the sample-wise and class-wise relations in a
unified transport framework. Specifically, a cooperation mechanism between
sample-level and class-level transport is introduced, where the sample-level
transport provides essential structure information for the class-level
knowledge transfer, while the class-level transport supplies discriminative
information for the outlier identification. The bi-level transport plan
provides guidance for the alignment process. By incorporating the label-aware
transport cost, the local transport structure is ensured and a fast computation
formulation is derived to improve the efficiency. Extensive experiments on
benchmark datasets validate the competitiveness of BUOT.

</details>


### [70] [FlowBERT: Prompt-tuned BERT for variable flow field prediction](https://arxiv.org/abs/2506.08021)
*Weihao Zou,Weibing Feng,Pin Wu*

Main category: cs.LG

TL;DR: 提出基于大语言模型知识迁移的通用流场预测框架，性能优、耗时短，为流体动力学预测开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 解决传统CFD方法计算成本高和现有深度学习模型跨条件迁移能力有限的问题。

Method: 将POD降维与预训练LLM微调策略相结合，设计面向流体动力学的文本模板。

Result: 框架在少样本学习中优于传统Transformer模型，泛化性好，预测时间从数小时降至数秒，准确率超90%。

Conclusion: 该知识迁移范式为快速流体动力学预测建立新方向，有广泛工程应用潜力。

Abstract: This study proposes a universal flow field prediction framework based on
knowledge transfer
  from large language model (LLM), addressing the high computational costs of
traditional
  computational fluid dynamics (CFD) methods and the limited cross-condition
transfer capability
  of existing deep learning models. The framework innovatively integrates
Proper Orthogonal
  Decomposition (POD) dimensionality reduction with fine-tuning strategies for
pretrained LLM,
  where POD facilitates compressed representation of flow field features while
the fine-tuned model
  learns to encode system dynamics in state space. To enhance the model's
adaptability to flow field
  data, we specifically designed fluid dynamics-oriented text templates that
improve predictive
  performance through enriched contextual semantic information. Experimental
results demonstrate
  that our framework outperforms conventional Transformer models in few-shot
learning scenarios while
  exhibiting exceptional generalization across various inflow conditions and
airfoil geometries.
  Ablation studies reveal the contributions of key components in the FlowBERT
architecture. Compared
  to traditional Navier-Stokes equation solvers requiring hours of computation,
our approach reduces
  prediction time to seconds while maintaining over 90% accuracy. The developed
knowledge transfer
  paradigm establishes a new direction for rapid fluid dynamics prediction,
with potential
  applications extending to aerodynamic optimization, flow control, and other
engineering domains.

</details>


### [71] [A Machine Learning Approach to Generate Residual Stress Distributions using Sparse Characterization Data in Friction-Stir Processed Parts](https://arxiv.org/abs/2506.08205)
*Shadab Anwar Shaikh,Kranthi Balusu,Ayoub Soulami*

Main category: cs.LG

TL;DR: 本文提出基于机器学习的残余应力生成器（RSG），从有限测量推断全场应力，减少实验工作量。


<details>
  <summary>Details</summary>
Motivation: 残余应力会影响部件性能，准确确定其全场分布很重要，但全尺寸表征实验工作量大，不切实际。

Method: 构建大量参数集的工艺模拟数据集，用U - Net架构的机器学习模型并调优超参数进行训练，评估其生成模拟应力能力，并用实际表征数据测试。

Result: 模型对模拟应力预测准确性高，有良好泛化性，能学习残余应力分布潜在结构。

Conclusion: RSG在预测实验表征数据上表现良好，证明该方法可行，可减少实验工作量。

Abstract: Residual stresses, which remain within a component after processing, can
deteriorate performance. Accurately determining their full-field distributions
is essential for optimizing the structural integrity and longevity. However,
the experimental effort required for full-field characterization is
impractical. Given these challenges, this work proposes a machine learning (ML)
based Residual Stress Generator (RSG) to infer full-field stresses from limited
measurements. An extensive dataset was initially constructed by performing
numerous process simulations with a diverse parameter set. A ML model based on
U-Net architecture was then trained to learn the underlying structure through
systematic hyperparameter tuning. Then, the model's ability to generate
simulated stresses was evaluated, and it was ultimately tested on actual
characterization data to validate its effectiveness. The model's prediction of
simulated stresses shows that it achieved excellent predictive accuracy and
exhibited a significant degree of generalization, indicating that it
successfully learnt the latent structure of residual stress distribution. The
RSG's performance in predicting experimentally characterized data highlights
the feasibility of the proposed approach in providing a comprehensive
understanding of residual stress distributions from limited measurements,
thereby significantly reducing experimental efforts.

</details>


### [72] [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)
*Chenxi Liu,Tianyi Xiong,Ruibo Chen,Yihan Wu,Junfeng Guo,Tianyi Zhou,Heng Huang*

Main category: cs.LG

TL;DR: 多数大跨模态模型推理时存在模态不平衡问题，现有偏好优化方法有局限。本文提出MBPO框架，构建离线偏好数据集、生成在线响应，用GRPO训练，实验表明能提升模型性能、减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 多数大跨模态模型推理时存在模态不平衡问题，导致下游任务泛化性差和幻觉问题，且现有偏好优化方法未抑制LLM骨干内部偏差、依赖离线数据、缺乏探索能力，GRPO在LMM对齐中未充分探索。

Method: 提出MBPO框架，通过对抗扰动输入图像生成硬负样本构建离线偏好数据集，利用封闭式任务生成带验证奖励的在线响应，用GRPO结合离线 - 在线混合数据训练模型。

Result: 实验表明MBPO能提升大跨模态模型在挑战性视觉 - 语言任务上的性能，有效减少幻觉。

Conclusion: MBPO框架可有效解决大跨模态模型的模态不平衡问题，提升模型性能。

Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been
significantly advanced by instruction tuning and further strengthened by recent
preference optimization. Yet, most LMMs still suffer from severe modality
imbalance during reasoning, i.e., outweighing language prior biases over visual
inputs, which bottlenecks their generalization to downstream tasks and causes
hallucinations. However, existing preference optimization approaches for LMMs
do not focus on restraining the internal biases of their Large Language Model
(LLM) backbones when curating the training data. Moreover, they heavily rely on
offline data and lack the capacity to explore diverse responses adaptive to
dynamic distributional shifts during training. Meanwhile, Group Relative Policy
Optimization (GRPO), a recent method using online-generated data and verified
rewards to improve reasoning capabilities, remains largely underexplored in LMM
alignment. In this paper, we propose a novel preference learning framework,
Modality-Balancing Preference Optimization (MBPO), to address the modality
imbalance in LMMs. MBPO constructs a more effective offline preference dataset
by generating hard negatives, i.e., rejected responses misled by LLM biases due
to limited usage of visual information, through adversarial perturbation of
input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended
tasks to generate online responses with verified rewards. GRPO is then employed
to train the model with offline-online hybrid data. Extensive experiments
demonstrate that MBPO can enhance LMM performance on challenging
vision-language tasks and effectively reduce hallucinations.

</details>


### [73] [Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems](https://arxiv.org/abs/2506.08475)
*Xiaolong He,Yeonjong Shin,Anthony Gruber,Sohyeon Jung,Kookjin Lee,Youngsoo Choi*

Main category: cs.LG

TL;DR: 提出热力学信息潜空间动力学识别框架tLaSDI用于降阶建模，结合自编码器和pGFINNs，加入主动学习策略，实验显示有速度提升和成本降低，还能揭示系统热力学行为。


<details>
  <summary>Details</summary>
Motivation: 为参数化非线性动力系统的降阶建模提供高效方法，同时保留关键热力学原理。

Method: 将自编码器用于降维，结合新开发的pGFINNs，加入物理信息主动学习策略，用基于残差的误差指标自适应采样训练数据。

Result: 在Burgers方程和1D/1V Vlasov - Poisson方程实验中实现高达3,528倍加速，相对误差1 - 3%，显著降低训练（50 - 90%）和推理（57 - 61%）成本。

Conclusion: 所提方法有效，能揭示系统潜在热力学行为，为物理空间动力学提供有价值见解。

Abstract: We propose an efficient thermodynamics-informed latent space dynamics
identification (tLaSDI) framework for the reduced-order modeling of parametric
nonlinear dynamical systems. This framework integrates autoencoders for
dimensionality reduction with newly developed parametric GENERIC
formalism-informed neural networks (pGFINNs), which enable efficient learning
of parametric latent dynamics while preserving key thermodynamic principles
such as free energy conservation and entropy generation across the parameter
space. To further enhance model performance, a physics-informed active learning
strategy is incorporated, leveraging a greedy, residual-based error indicator
to adaptively sample informative training data, outperforming uniform sampling
at equivalent computational cost. Numerical experiments on the Burgers'
equation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed
method achieves up to 3,528x speed-up with 1-3% relative errors, and
significant reduction in training (50-90%) and inference (57-61%) cost.
Moreover, the learned latent space dynamics reveal the underlying thermodynamic
behavior of the system, offering valuable insights into the physical-space
dynamics.

</details>


### [74] [Recipes for Pre-training LLMs with MXFP8](https://arxiv.org/abs/2506.08027)
*Asit Mishra,Dusan Stosic,Simon Layton*

Main category: cs.LG

TL;DR: 论文指出MX格式用于精度缩放可提升GPU效率，但OCP规范的舍入模式会使LLM预训练发散，提出用round - to - infinity计算缩放因子的改进舍入模式，使8B模型在15T令牌上成功预训练。


<details>
  <summary>Details</summary>
Motivation: MX格式虽有优势，但实际用于多万亿令牌数据集上预训练LLM时需谨慎，OCP规范的舍入模式存在问题，需改进以实现成功预训练。

Method: 提出使用round - to - infinity计算缩放因子的改进舍入模式。

Result: 改进的舍入模式使8B模型在15T令牌的MXFP8预训练中成功收敛。

Conclusion: 改进的舍入模式能解决OCP规范舍入模式导致的预训练发散问题，可用于LLM的成功预训练。

Abstract: Precision scaling - using fewer bits to represent model parameters and
related tensors during pre-training - has emerged as a compelling technique for
improving GPU efficiency without sacrificing accuracy. Microscaling (MX)
formats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling
this precision scaling aspect. These formats combine narrow floating-point data
types with per-block scaling factors, offering a fine-grained approach to
quantizing tensors.
  Although MX-formats offer the promise of improved numeric stability compared
to other reduced-precision representations, in practice they must be used
carefully in order to successfully converge an LLM on a multi-trillion token
dataset. In this paper, we show that the rounding mode suggested in OCP
specification can lead to divergence when pre-training an LLM. We show an
improved rounding mode, which uses round-to-infinity to compute scaling
factors, enables successful pre-training in MXFP8 for an 8B model on 15T
tokens.

</details>


### [75] [Private Evolution Converges](https://arxiv.org/abs/2506.08312)
*Tomás González,Giulia Fanti,Aaditya Ramdas*

Main category: cs.LG

TL;DR: 提出新理论框架解释Private Evolution (PE)实际行为，证明其收敛性，还将其与Private Signed Measure Mechanism建立联系并通过模拟验证。


<details>
  <summary>Details</summary>
Motivation: 现有PE收敛的理论分析依赖不现实假设，且PE在某些领域表现不一致。

Method: 开发新理论框架，针对d维、n个数据点的有界敏感数据集进行分析，证明PE收敛性，分析拓展到一般Banach空间。

Result: 证明PE能生成与原数据集有特定1 - Wasserstein距离的(ε, δ) - DP合成数据集，建立算法最坏情况下收敛性，将PE与Private Signed Measure Mechanism建立联系。

Conclusion: 新理论框架可解释PE实际行为，模拟验证了理论发现的实际相关性。

Abstract: Private Evolution (PE) is a promising training-free method for differentially
private (DP) synthetic data generation. While it achieves strong performance in
some domains (e.g., images and text), its behavior in others (e.g., tabular
data) is less consistent. To date, the only theoretical analysis of the
convergence of PE depends on unrealistic assumptions about both the algorithm's
behavior and the structure of the sensitive dataset. In this work, we develop a
new theoretical framework to explain PE's practical behavior and identify
sufficient conditions for its convergence. For $d$-dimensional sensitive
datasets with $n$ data points from a bounded domain, we prove that PE produces
an $(\epsilon, \delta)$-DP synthetic dataset with expected 1-Wasserstein
distance of order $\tilde{O}(d(n\epsilon)^{-1/d})$ from the original,
establishing worst-case convergence of the algorithm as $n \to \infty$. Our
analysis extends to general Banach spaces as well. We also connect PE to the
Private Signed Measure Mechanism, a method for DP synthetic data generation
that has thus far not seen much practical adoption. We demonstrate the
practical relevance of our theoretical findings in simulations.

</details>


### [76] [Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation](https://arxiv.org/abs/2506.08604)
*Giacomo Baldan,Qiang Liu,Alberto Guardone,Nils Thuerey*

Main category: cs.LG

TL;DR: 提出基于物理的流匹配（PBFM）框架，结合物理约束和流匹配目标，通过实验证明其在物理残差和分布准确性上优于现有算法，可用于物理和工程应用。


<details>
  <summary>Details</summary>
Motivation: 现有生成式机器学习方法隐式学习物理，提出显式嵌入物理约束的方法。

Method: 提出PBFM框架，将物理约束嵌入流匹配目标，引入时间展开，联合最小化流匹配损失和基于物理的残差损失，分析最小噪声水平作用并评估随机采样策略。

Result: 在三个代表性偏微分方程问题上，PBFM物理残差比FM准确达8倍，分布准确性优于现有算法。

Conclusion: PBFM为物理和工程应用中的代理建模、不确定性量化和加速模拟提供了有效框架。

Abstract: Generative machine learning methods, such as diffusion models and flow
matching, have shown great potential in modeling complex system behaviors and
building efficient surrogate models. However, these methods typically learn the
underlying physics implicitly from data. We propose Physics-Based Flow Matching
(PBFM), a novel generative framework that explicitly embeds physical
constraints, both PDE residuals and algebraic relations, into the flow matching
objective. We also introduce temporal unrolling at training time that improves
the accuracy of the final, noise-free sample prediction. Our method jointly
minimizes the flow matching loss and the physics-based residual loss without
requiring hyperparameter tuning of their relative weights. Additionally, we
analyze the role of the minimum noise level, $\sigma_{\min}$, in the context of
physical constraints and evaluate a stochastic sampling strategy that helps to
reduce physical residuals. Through extensive benchmarks on three representative
PDE problems, we show that our approach yields up to an $8\times$ more accurate
physical residuals compared to FM, while clearly outperforming existing
algorithms in terms of distributional accuracy. PBFM thus provides a principled
and efficient framework for surrogate modeling, uncertainty quantification, and
accelerated simulation in physics and engineering applications.

</details>


### [77] [ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity](https://arxiv.org/abs/2506.08051)
*Mahmuda Sultana Mimi,Md Monzurul Islam,Anannya Ghosh Tusti,Shriyank Somvanshi,Subasish Das*

Main category: cs.LG

TL;DR: 本文提出ST - GraphNet框架用于预测自动驾驶汽车（AV）碰撞严重程度，利用粗细粒度图，在粗粒度图上用DSTGCN骨干网络取得97.74%测试准确率，凸显空间聚合等方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 理解AV碰撞严重程度的时空动态对提升城市交通安全性和基础设施规划至关重要。

Method: 构建粗细粒度两种图表示，节点融入多模态数据，评估多种图神经网络架构进行碰撞严重程度分类和高风险区域预测。

Result: 提出的ST - GraphNet在粗粒度H3图上用DSTGCN骨干网络测试准确率达97.74%，远超最佳细粒度模型的64.7%。

Conclusion: 空间聚合、动态消息传递和多模态特征集成在捕捉AV碰撞严重程度的复杂时空模式方面非常有效。

Abstract: Understanding the spatial and temporal dynamics of automated vehicle (AV)
crash severity is critical for advancing urban mobility safety and
infrastructure planning. In this work, we introduce ST-GraphNet, a
spatio-temporal graph neural network framework designed to model and predict AV
crash severity by using both fine-grained and region-aggregated spatial graphs.
Using a balanced dataset of 2,352 real-world AV-related crash reports from
Texas (2024), including geospatial coordinates, crash timestamps, SAE
automation levels, and narrative descriptions, we construct two complementary
graph representations: (1) a fine-grained graph with individual crash events as
nodes, where edges are defined via spatio-temporal proximity; and (2) a
coarse-grained graph where crashes are aggregated into Hexagonal Hierarchical
Spatial Indexing (H3)-based spatial cells, connected through hexagonal
adjacency. Each node in the graph is enriched with multimodal data, including
semantic, spatial, and temporal attributes, including textual embeddings from
crash narratives using a pretrained Sentence-BERT model. We evaluate various
graph neural network (GNN) architectures, such as Graph Convolutional Networks
(GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN
(DSTGCN), to classify crash severity and predict high-risk regions. Our
proposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3
graph, achieves a test accuracy of 97.74\%, substantially outperforming the
best fine-grained model (64.7\% test accuracy). These findings highlight the
effectiveness of spatial aggregation, dynamic message passing, and multi-modal
feature integration in capturing the complex spatio-temporal patterns
underlying AV crash severity.

</details>


### [78] [STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation](https://arxiv.org/abs/2506.08054)
*Yiming Wang,Hao Peng,Senzhang Wang,Haohua Du,Chunyang Liu,Jia Wu,Guanlin Wu*

Main category: cs.LG

TL;DR: 提出STAMImputer用于交通数据插补，在四个数据集实验中性能优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间到空间顺序方法在块缺失数据场景下难以有效提取特征，静态图结构限制模型处理非平稳交通数据分布偏移问题的灵活性。

Method: 引入混合专家（MoE）框架捕获潜在时空特征及其影响权重；设计低秩引导采样图注意力（LrSGAT）机制动态平衡道路网络的局部和全局相关性，利用采样注意力向量生成动态图捕获实时空间相关性。

Result: 在四个交通数据集上的实验表明，STAMImputer与现有SOTA方法相比取得显著性能提升。

Conclusion: 所提出的STAMImputer在交通数据插补方面是有效的，能解决现有方法的不足。

Abstract: Traffic data imputation is fundamentally important to support various
applications in intelligent transportation systems such as traffic flow
prediction. However, existing time-to-space sequential methods often fail to
effectively extract features in block-wise missing data scenarios. Meanwhile,
the static graph structure for spatial feature propagation significantly
constrains the models flexibility in handling the distribution shift issue for
the nonstationary traffic data. To address these issues, this paper proposes a
SpatioTemporal Attention Mixture of experts network named STAMImputer for
traffic data imputation. Specifically, we introduce a Mixture of Experts (MoE)
framework to capture latent spatio-temporal features and their influence
weights, effectively imputing block missing. A novel Low-rank guided Sampling
Graph ATtention (LrSGAT) mechanism is designed to dynamically balance the local
and global correlations across road networks. The sampled attention vectors are
utilized to generate dynamic graphs that capture real-time spatial
correlations. Extensive experiments are conducted on four traffic datasets for
evaluation. The result shows STAMImputer achieves significantly performance
improvement compared with existing SOTA approaches. Our codes are available at
https://github.com/RingBDStack/STAMImupter.

</details>


### [79] [GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity](https://arxiv.org/abs/2210.16402)
*Artavazd Maranjyan,Mher Safaryan,Peter Richtárik*

Main category: cs.LG

TL;DR: 本文研究分布式优化算法，提出改进ProxSkip的GradSkip和GradSkip+方法，证明收敛性和通信复杂度，并通过实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 原ProxSkip方法要求所有客户端在每轮通信中进行相同数量的本地训练步骤，作者希望改进方法以允许数据“重要性较低”的客户端减少本地训练步骤，同时不影响整体通信复杂度。

Method: 重新设计ProxSkip方法得到GradSkip，进一步推广得到GradSkip+，将概率交替的随机性扩展到任意无偏压缩算子并考虑通用近似正则化器。

Result: 证明GradSkip在相同假设下线性收敛且有相同的加速通信复杂度，可根据本地条件数减少本地梯度步数；GradSkip+可将文献中几种相关方法作为特殊情况。

Conclusion: 通过精心设计的玩具问题的实证研究，证实了理论主张。

Abstract: We study a class of distributed optimization algorithms that aim to alleviate
high communication costs by allowing clients to perform multiple local
gradient-type training steps before communication. In a recent breakthrough,
Mishchenko et al. (2022) proved that local training, when properly executed,
leads to provable communication acceleration, and this holds in the strongly
convex regime without relying on any data similarity assumptions. However,
their ProxSkip method requires all clients to take the same number of local
training steps in each communication round. We propose a redesign of the
ProxSkip method, allowing clients with ``less important'' data to get away with
fewer local training steps without impacting the overall communication
complexity of the method. In particular, we prove that our modified method,
GradSkip, converges linearly under the same assumptions and has the same
accelerated communication complexity, while the number of local gradient steps
can be reduced relative to a local condition number. We further generalize our
method by extending the randomness of probabilistic alternations to arbitrary
unbiased compression operators and by considering a generic proximable
regularizer. This generalization, which we call GradSkip+, recovers several
related methods in the literature as special cases. Finally, we present an
empirical study on carefully designed toy problems that confirm our theoretical
claims.

</details>


### [80] [Towards Fair Representation: Clustering and Consensus](https://arxiv.org/abs/2506.08673)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien Long Nguyen,Romina Nobahari*

Main category: cs.LG

TL;DR: 研究公平聚类视角下的共识聚类问题，提供常数因子近似解，开发算法并证明问题的NP难。


<details>
  <summary>Details</summary>
Motivation: 在公平聚类框架下，找到既具代表性又对特定受保护属性公平的共识聚类。

Method: 研究如何最小化修改现有聚类以实现公平，为等组表示数据集开发最优算法，为更一般场景开发近线性时间常数因子近似算法。

Result: 为问题提供常数因子近似解，证明问题对两个不等规模组是NP难的。

Conclusion: 关于最接近公平聚类的结果可能对其他聚类问题有更广泛影响。

Abstract: Consensus clustering, a fundamental task in machine learning and data
analysis, aims to aggregate multiple input clusterings of a dataset,
potentially based on different non-sensitive attributes, into a single
clustering that best represents the collective structure of the data. In this
work, we study this fundamental problem through the lens of fair clustering, as
introduced by Chierichetti et al. [NeurIPS'17], which incorporates the
disparate impact doctrine to ensure proportional representation of each
protected group in the dataset within every cluster. Our objective is to find a
consensus clustering that is not only representative but also fair with respect
to specific protected attributes. To the best of our knowledge, we are the
first to address this problem and provide a constant-factor approximation.
  As part of our investigation, we examine how to minimally modify an existing
clustering to enforce fairness -- an essential postprocessing step in many
clustering applications that require fair representation. We develop an optimal
algorithm for datasets with equal group representation and near-linear time
constant factor approximation algorithms for more general scenarios with
different proportions of two group sizes. We complement our approximation
result by showing that the problem is NP-hard for two unequal-sized groups.
Given the fundamental nature of this problem, we believe our results on Closest
Fair Clustering could have broader implications for other clustering problems,
particularly those for which no prior approximation guarantees exist for their
fair variants.

</details>


### [81] [IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)](https://arxiv.org/abs/2506.08844)
*Siyi Sun,David Antony Selby,Yunchuan Huang,Sebastian Vollmer,Seth Flaxman,Anisoara Calinescu*

Main category: cs.LG

TL;DR: 利用世界银行合成数据集创建IMAGIC - 500数据集，对不同缺失机制和比例进行缺失数据插补基准测试，评估多种插补技术优缺点，推动算法开发和社会科学研究。


<details>
  <summary>Details</summary>
Motivation: 现实社会经济数据集因数据保护难以共享，公开合成数据集少，缺乏系统评估插补方法的基准。

Method: 使用世界银行公开合成数据集，衍生出IMAGIC - 500数据集，在不同缺失机制和比例下进行插补基准测试，评估插补准确性、计算效率和对下游预测任务的影响。

Result: 突出了统计、传统机器学习和深度学习插补技术（包括基于扩散的方法）的优缺点。

Conclusion: IMAGIC - 500数据集和基准有助于开发稳健插补算法和促进可重复的社会科学研究。

Abstract: Missing data imputation in tabular datasets remains a pivotal challenge in
data science and machine learning, particularly within socioeconomic research.
However, real-world socioeconomic datasets are typically subject to strict data
protection protocols, which often prohibit public sharing, even for synthetic
derivatives. This severely limits the reproducibility and accessibility of
benchmark studies in such settings. Further, there are very few publicly
available synthetic datasets. Thus, there is limited availability of benchmarks
for systematic evaluation of imputation methods on socioeconomic datasets,
whether real or synthetic. In this study, we utilize the World Bank's publicly
available synthetic dataset, Synthetic Data for an Imaginary Country, which
closely mimics a real World Bank household survey while being fully public,
enabling broad access for methodological research. With this as a starting
point, we derived the IMAGIC-500 dataset: we select a subset of 500k
individuals across approximately 100k households with 19 socioeconomic
features, designed to reflect the hierarchical structure of real-world
household surveys. This paper introduces a comprehensive missing data
imputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR,
MNAR) and missingness ratios (10\%, 20\%, 30\%, 40\%, 50\%). Our evaluation
considers the imputation accuracy for continuous and categorical variables,
computational efficiency, and impact on downstream predictive tasks, such as
estimating educational attainment at the individual level. The results
highlight the strengths and weaknesses of statistical, traditional machine
learning, and deep learning imputation techniques, including recent
diffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate
the development of robust imputation algorithms and foster reproducible social
science research.

</details>


### [82] [Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques](https://arxiv.org/abs/2506.08060)
*Asankhaya Sharma*

Main category: cs.LG

TL;DR: 论文证明基础Transformer模型用推理时技术可近似监督微调能力，给出不同任务数据集大小要求，为大模型高效部署提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 监督微调计算成本高，需探索资源高效部署大语言模型的方法。

Method: 在理想化假设下证明基础Transformer模型用推理时技术（上下文学习）可近似监督微调能力，并扩展到实际场景。

Result: 给出文本生成和线性分类任务在不同条件下近似微调行为所需数据集大小。

Conclusion: 研究结果为大语言模型资源高效部署提供理论基础，检索增强生成等技术可连接理论与实际应用。

Abstract: Large language models have transformed natural language processing, yet
supervised fine-tuning (SFT) remains computationally intensive. This paper
formally proves that capabilities acquired through SFT can be approximated by a
base transformer model using inference-time techniques, specifically in-context
learning (ICL), without altering model parameters, under idealized assumptions
including unbounded computational resources and access to the fine-tuning
dataset. We extend these results to practical scenarios with finite context
lengths and partial dataset access. For text generation tasks with fixed output
length $l$, datasets of size $\mathrm{O}\left( \frac{m V}{\varepsilon^2} \log
\frac{m}{\delta} \right)$ or, with bounded context, $\mathrm{O}\left( \frac{l
\log V}{\varepsilon^2} \log \frac{1}{\delta} \right)$ suffice to approximate
fine-tuned behavior across $m$ contexts within error $\varepsilon$, where $V$
is the vocabulary size and $\delta$ is the failure probability. For linear
classification, datasets of size $\mathrm{O}\left( \frac{d}{\varepsilon}
\right)$ or, with fixed context, $\mathrm{O}\left( \frac{1}{\varepsilon^2} \log
\frac{1}{\delta} \right)$ are sufficient, where $d$ is the input dimension.
Grounded in the Turing completeness of transformers, these results provide a
theoretical foundation for resource-efficient deployment of large language
models, with practical techniques like retrieval-augmented generation bridging
theory to real-world applications.

</details>


### [83] [FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2506.08062)
*Woosung Kim,Jinho Lee,Jongmin Lee,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 提出首个离线多目标强化学习框架FairDICE，可直接优化非线性福利目标，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 线性标量化方法无法捕捉公平导向目标，且缺乏离线环境下优化非线性福利标准的统一方法。

Method: FairDICE利用分布校正估计，同时考虑福利最大化和分布正则化。

Result: 在多个离线基准测试中，FairDICE相比现有基线展现出强大的公平感知性能。

Conclusion: FairDICE是有效的离线多目标强化学习框架，能直接优化非线性福利目标。

Abstract: Multi-objective reinforcement learning (MORL) aims to optimize policies in
the presence of conflicting objectives, where linear scalarization is commonly
used to reduce vector-valued returns into scalar signals. While effective for
certain preferences, this approach cannot capture fairness-oriented goals such
as Nash social welfare or max-min fairness, which require nonlinear and
non-additive trade-offs. Although several online algorithms have been proposed
for specific fairness objectives, a unified approach for optimizing nonlinear
welfare criteria in the offline setting-where learning must proceed from a
fixed dataset-remains unexplored. In this work, we present FairDICE, the first
offline MORL framework that directly optimizes nonlinear welfare objective.
FairDICE leverages distribution correction estimation to jointly account for
welfare maximization and distributional regularization, enabling stable and
sample-efficient learning without requiring explicit preference weights or
exhaustive weight search. Across multiple offline benchmarks, FairDICE
demonstrates strong fairness-aware performance compared to existing baselines.

</details>


### [84] [UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data](https://arxiv.org/abs/2506.08167)
*Sunny Gupta,Nikita Jangid,Amit Sethi*

Main category: cs.LG

TL;DR: 提出UniVarFL框架解决联邦学习在非IID数据下性能下降问题，实验显示其表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非IID数据下因局部分类器偏差导致性能严重下降，传统方法有计算成本高或难适应特征偏移的问题。

Method: 提出UniVarFL框架，在本地训练时采用分类器方差正则化和超球面均匀性正则化两种策略。

Result: 在多个基准数据集上的实验表明，UniVarFL在准确性上优于现有方法。

Conclusion: UniVarFL是用于实际联邦学习部署，特别是资源受限场景的可扩展且高效的解决方案。

Abstract: Federated Learning (FL) often suffers from severe performance degradation
when faced with non-IID data, largely due to local classifier bias. Traditional
remedies such as global model regularization or layer freezing either incur
high computational costs or struggle to adapt to feature shifts. In this work,
we propose UniVarFL, a novel FL framework that emulates IID-like training
dynamics directly at the client level, eliminating the need for global model
dependency. UniVarFL leverages two complementary regularization strategies
during local training: Classifier Variance Regularization, which aligns
class-wise probability distributions with those expected under IID conditions,
effectively mitigating local classifier bias; and Hyperspherical Uniformity
Regularization, which encourages a uniform distribution of feature
representations across the hypersphere, thereby enhancing the model's ability
to generalize under diverse data distributions. Extensive experiments on
multiple benchmark datasets demonstrate that UniVarFL outperforms existing
methods in accuracy, highlighting its potential as a highly scalable and
efficient solution for real-world FL deployments, especially in
resource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL

</details>


### [85] [Lite-RVFL: A Lightweight Random Vector Functional-Link Neural Network for Learning Under Concept Drift](https://arxiv.org/abs/2506.08063)
*Songqiao Hu,Zeyi Liu,Xiao He*

Main category: cs.LG

TL;DR: 提出轻量级随机向量功能链接网络Lite - RVFL应对概念漂移，无需检测和再训练，经理论分析和实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有在线学习方法应对概念漂移需高计算成本的模型再训练或漂移检测，不适用于实时应用。

Method: 提出Lite - RVFL网络，引入新目标函数对新样本赋指数增长权重，推导高效增量更新规则。

Result: 理论分析证实目标函数用于漂移适应的可行性，在真实安全评估任务实验中验证了Lite - RVFL的效率、适应漂移的有效性及捕捉时间模式的潜力。

Conclusion: Lite - RVFL是一种轻量级、快速高效的方法，能有效适应概念漂移，无需漂移检测和模型再训练。

Abstract: The change in data distribution over time, also known as concept drift, poses
a significant challenge to the reliability of online learning methods. Existing
methods typically require model retraining or drift detection, both of which
demand high computational costs and are often unsuitable for real-time
applications. To address these limitations, a lightweight, fast and efficient
random vector functional-link network termed Lite-RVFL is proposed, capable of
adapting to concept drift without drift detection and retraining. Lite-RVFL
introduces a novel objective function that assigns weights exponentially
increasing to new samples, thereby emphasizing recent data and enabling timely
adaptation. Theoretical analysis confirms the feasibility of this objective
function for drift adaptation, and an efficient incremental update rule is
derived. Experimental results on a real-world safety assessment task validate
the efficiency, effectiveness in adapting to drift, and potential to capture
temporal patterns of Lite-RVFL. The source code is available at
https://github.com/songqiaohu/Lite-RVFL.

</details>


### [86] [Federated Learning on Stochastic Neural Networks](https://arxiv.org/abs/2506.08169)
*Jingqiao Tang,Ryan Bausback,Feng Bao,Richard Archibald*

Main category: cs.LG

TL;DR: 提出将随机神经网络作为联邦学习框架中的本地模型，即联邦随机神经网络，通过数值实验展示其在处理非独立同分布数据时的性能和有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受本地数据集中潜在噪声影响，如测量能力有限或人为错误可能导致数据不准确，需解决该问题。

Method: 在联邦学习框架中使用随机神经网络作为本地模型，提出联邦随机神经网络方法。

Result: 将进行数值实验展示方法在处理非独立同分布数据时的性能和有效性。

Conclusion: 暂未提及明确结论，从研究走向看可能证明联邦随机神经网络在处理含噪声数据上的优势。

Abstract: Federated learning is a machine learning paradigm that leverages edge
computing on client devices to optimize models while maintaining user privacy
by ensuring that local data remains on the device. However, since all data is
collected by clients, federated learning is susceptible to latent noise in
local datasets. Factors such as limited measurement capabilities or human
errors may introduce inaccuracies in client data. To address this challenge, we
propose the use of a stochastic neural network as the local model within the
federated learning framework. Stochastic neural networks not only facilitate
the estimation of the true underlying states of the data but also enable the
quantification of latent noise. We refer to our federated learning approach,
which incorporates stochastic neural networks as local models, as Federated
stochastic neural networks. We will present numerical experiments demonstrating
the performance and effectiveness of our method, particularly in handling
non-independent and identically distributed data.

</details>


### [87] [Attention with Trained Embeddings Provably Selects Important Tokens](https://arxiv.org/abs/2505.17282)
*Diyuan Wu,Aleksandr Shevchenko,Samet Oymak,Marco Mondelli*

Main category: cs.LG

TL;DR: 本文研究梯度下降得到的嵌入结构，分析单步训练和梯度流训练后嵌入特点，并通过实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 当前对语言建模中token嵌入的理论理解有限，需弥补这一差距。

Method: 考虑一层softmax注意力模型用于二分类，分析单步梯度训练和梯度流训练。

Result: 单步训练后嵌入能捕捉token重要性，梯度流训练后softmax可选择重要token，实验与理论现象相符。

Conclusion: 对梯度下降得到的嵌入结构有了理论刻画，实验验证了理论的有效性。

Abstract: Token embeddings play a crucial role in language modeling but, despite this
practical relevance, their theoretical understanding remains limited. Our paper
addresses the gap by characterizing the structure of embeddings obtained via
gradient descent. Specifically, we consider a one-layer softmax attention model
with a linear head for binary classification, i.e., $\texttt{Softmax}( p^\top
E_X^\top ) E_X v = \frac{ \sum_{i=1}^T \exp(p^\top E_{x_i}) E_{x_i}^\top
v}{\sum_{j=1}^T \exp(p^\top E_{x_{j}}) }$, where $E_X = [ E_{x_1} , \dots,
E_{x_T} ]^\top$ contains the embeddings of the input sequence, $p$ is the
embedding of the $\mathrm{\langle cls \rangle}$ token and $v$ the output
vector. First, we show that, already after a single step of gradient training
with the logistic loss, the embeddings $E_X$ capture the importance of tokens
in the dataset by aligning with the output vector $v$ proportionally to the
frequency with which the corresponding tokens appear in the dataset. Then,
after training $p$ via gradient flow until convergence, the softmax selects the
important tokens in the sentence (i.e., those that are predictive of the
label), and the resulting $\mathrm{\langle cls \rangle}$ embedding maximizes
the margin for such a selection. Experiments on real-world datasets (IMDB,
Yelp) exhibit a phenomenology close to that unveiled by our theory.

</details>


### [88] [Info-Coevolution: An Efficient Framework for Data Model Coevolution](https://arxiv.org/abs/2506.08070)
*Ziheng Qin,Hailun Xu,Wei Chee Yew,Qi Jia,Yang Luo,Kanchan Sarkar,Danhui Guan,Kai Wang,Yang You*

Main category: cs.LG

TL;DR: 提出Info - Coevolution框架，通过在线选择性标注使模型和数据无偏协同进化，降低标注和训练成本。


<details>
  <summary>Details</summary>
Motivation: 现实数据增长给高效数据集构建和训练带来挑战，传统方法数据和训练效率非最优，主动学习有复杂度和偏差问题。

Method: 提出Info - Coevolution框架，利用特定任务模型和开源模型，选择性标注和整合在线及网络数据。

Result: 在ImageNet - 1K等数据集上，Info - Coevolution降低32%标注和训练成本且性能无损失，能自动给出节省比例，结合半监督学习可将标注比例降至50%，还探索了基于检索的数据集增强。

Conclusion: Info - Coevolution是一个高效且无偏的模型和数据协同进化框架，有良好的应用效果。

Abstract: Machine learning relies heavily on data, yet the continuous growth of
real-world data poses challenges for efficient dataset construction and
training. A fundamental yet unsolved question is: given our current model and
data, does a new data (sample/batch) need annotation/learning? Conventional
approaches retain all available data, leading to non-optimal data and training
efficiency. Active learning aims to reduce data redundancy by selecting a
subset of samples to annotate, while it increases pipeline complexity and
introduces bias. In this work, we propose Info-Coevolution, a novel framework
that efficiently enables models and data to coevolve through online selective
annotation with no bias. Leveraging task-specific models (and open-source
models), it selectively annotates and integrates online and web data to improve
datasets efficiently. For real-world datasets like ImageNet-1K,
Info-Coevolution reduces annotation and training costs by 32\% without
performance loss. It is able to automatically give the saving ratio without
tuning the ratio. It can further reduce the annotation ratio to 50\% with
semi-supervised learning. We also explore retrieval-based dataset enhancement
using unlabeled open-source data. Code is available at
https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.

</details>


### [89] [HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems](https://arxiv.org/abs/2506.08426)
*Zheng Lin,Zhe Chen,Xianhao Chen,Wei Ni,Yue Gao*

Main category: cs.LG

TL;DR: 提出HASFL框架解决SFL中边缘设备的掉队者效应问题，实验验证其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有SFL方法因边缘设备能力异质性存在严重的掉队者效应，需解决资源异质性问题。

Method: 推导SFL的收敛边界，提出HASFL框架，自适应控制批量大小和模型分割。

Result: 广泛实验验证了HASFL的有效性，证明其优于现有基准。

Conclusion: HASFL能在异构边缘网络中平衡通信 - 计算延迟和训练收敛。

Abstract: Split federated learning (SFL) has emerged as a promising paradigm to
democratize machine learning (ML) on edge devices by enabling layer-wise model
partitioning. However, existing SFL approaches suffer significantly from the
straggler effect due to the heterogeneous capabilities of edge devices. To
address the fundamental challenge, we propose adaptively controlling batch
sizes (BSs) and model splitting (MS) for edge devices to overcome resource
heterogeneity. We first derive a tight convergence bound of SFL that quantifies
the impact of varied BSs and MS on learning performance. Based on the
convergence bound, we propose HASFL, a heterogeneity-aware SFL framework
capable of adaptively controlling BS and MS to balance communication-computing
latency and training convergence in heterogeneous edge networks. Extensive
experiments with various datasets validate the effectiveness of HASFL and
demonstrate its superiority over state-of-the-art benchmarks.

</details>


### [90] [Modified K-means Algorithm with Local Optimality Guarantees](https://arxiv.org/abs/2506.06990)
*Mingyi Li,Michael R. Metel,Akiko Takeda*

Main category: cs.LG

TL;DR: 本文研究K - means算法局部最优性，提出改进方法确保局部最优，实验证明改进方法效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对K - means算法局部最优性保证的严格分析。

Method: 先给出K - means算法收敛到局部最优解的条件，基于此提出简单修改方法，使用一般的Bregman散度作为相异度度量。

Result: 数值实验表明K - means算法在实践中并非总能找到局部最优解，改进方法能提供更好的局部最优解并降低聚类损失。

Conclusion: 提出的改进方法能确保K - means算法在连续和离散意义上的局部最优性，且计算复杂度不变。

Abstract: The K-means algorithm is one of the most widely studied clustering algorithms
in machine learning. While extensive research has focused on its ability to
achieve a globally optimal solution, there still lacks a rigorous analysis of
its local optimality guarantees. In this paper, we first present conditions
under which the K-means algorithm converges to a locally optimal solution.
Based on this, we propose simple modifications to the K-means algorithm which
ensure local optimality in both the continuous and discrete sense, with the
same computational complexity as the original K-means algorithm. As the
dissimilarity measure, we consider a general Bregman divergence, which is an
extension of the squared Euclidean distance often used in the K-means
algorithm. Numerical experiments confirm that the K-means algorithm does not
always find a locally optimal solution in practice, while our proposed methods
provide improved locally optimal solutions with reduced clustering loss. Our
code is available at https://github.com/lmingyi/LO-K-means.

</details>


### [91] [Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting](https://arxiv.org/abs/2506.08113)
*Timothée Hornek Amir Sartipi,Igor Tchappi,Gilbert Fridgen*

Main category: cs.LG

TL;DR: 文章对多种预训练模型在电价预测上进行基准测试，对比传统方法，发现 Chronos - Bolt 和 Time - MoE 表现较好，但双季节性 MSTL 模型表现最稳定。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能和预训练大语言模型推动了时间序列基础模型发展，但在电价预测中的有效性未知，需进行研究。

Method: 对 Chronos - Bolt、Chronos - T5 等多个预训练模型与传统统计和机器学习方法进行基准测试，用德、法等国 2024 年日前拍卖电价数据进行一日前瞻的每日预测。

Result: Chronos - Bolt 和 Time - MoE 在时间序列基础模型中表现强，与传统模型相当；双季节性 MSTL 模型在各国和评估指标上表现稳定，无时间序列基础模型在统计上优于它。

Conclusion: 双季节性 MSTL 模型在电价预测中表现突出，时间序列基础模型有一定表现但未超越传统模型。

Abstract: Accurate electricity price forecasting (EPF) is crucial for effective
decision-making in power trading on the spot market. While recent advances in
generative artificial intelligence (GenAI) and pre-trained large language
models (LLMs) have inspired the development of numerous time series foundation
models (TSFMs) for time series forecasting, their effectiveness in EPF remains
uncertain. To address this gap, we benchmark several state-of-the-art
pretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and
TimeGPT--against established statistical and machine learning (ML) methods for
EPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany,
France, the Netherlands, Austria, and Belgium, we generate daily forecasts with
a one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the
TSFMs, performing on par with traditional models. However, the biseasonal MSTL
model, which captures daily and weekly seasonality, stands out for its
consistent performance across countries and evaluation metrics, with no TSFM
statistically outperforming it.

</details>


### [92] [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://arxiv.org/abs/2506.08125)
*Hanbing Liu,Lang Cao,Yuanyi Ren,Mengyu Zhou,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang*

Main category: cs.LG

TL;DR: 本文提出Bingo强化学习框架改进基于长度的奖励设计，以提升大语言模型推理效率，实验表明其能兼顾准确率与效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理输出冗长低效，现有强化学习方法多关注准确率，对推理效率关注有限，直接基于长度的奖励会降低准确率。

Method: 提出Bingo框架，包含显著性感知长度奖励和动态长度奖励两种机制。

Result: 在多个推理基准测试中，Bingo既提高了准确率又提升了效率，优于普通奖励和其他基于长度奖励的基线方法。

Conclusion: 明确训练大语言模型进行高效推理具有潜力。

Abstract: Large language models have demonstrated impressive reasoning capabilities,
yet they often suffer from inefficiencies due to unnecessarily verbose or
redundant outputs. While many works have explored reinforcement learning (RL)
to enhance reasoning abilities, most primarily focus on improving accuracy,
with limited attention to reasoning efficiency. Some existing approaches
introduce direct length-based rewards to encourage brevity, but this often
leads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL
framework that advances length-based reward design to boost efficient
reasoning. Bingo incorporates two key mechanisms: a significance-aware length
reward, which gradually guides the model to reduce only insignificant tokens,
and a dynamic length reward, which initially encourages elaborate reasoning for
hard questions but decays over time to improve overall efficiency. Experiments
across multiple reasoning benchmarks show that Bingo improves both accuracy and
efficiency. It outperforms the vanilla reward and several other length-based
reward baselines in RL, achieving a favorable trade-off between accuracy and
efficiency. These results underscore the potential of training LLMs explicitly
for efficient reasoning.

</details>


### [93] [Parameter-free approximate equivariance for tasks with finite group symmetry](https://arxiv.org/abs/2506.08244)
*Riccardo Ali,Pietro Liò,Jamie Vicary*

Main category: cs.LG

TL;DR: 提出零参数方法在潜在表征中施加近似等变性，实验表明网络偏好学习正则表示，在多数据集上以少量参数达相似或更好性能。


<details>
  <summary>Details</summary>
Motivation: 现有等变方法计算密集、参数多且常与特定架构绑定，需改进。

Method: 在损失函数中添加额外项，在潜在表征中施加有限群的近似等变性，让网络学习潜在空间的群表示。

Result: 网络在每种情况下都倾向于学习正则表示，以少量参数在多个数据集上达到与现有方法相似或更好的性能。

Conclusion: 所提出的零参数方法能有效施加近似等变性，减少参数并保证性能。

Abstract: Equivariant neural networks incorporate symmetries through group actions,
embedding them as an inductive bias to improve performance on a wide variety of
tasks. However, existing equivariant methods can be computationally intensive,
with high parameter counts, and are often tied to a specific architecture. We
propose a simple zero-parameter approach that imposes approximate equivariance
for a finite group in the latent representation, as an additional term in the
loss function. We conduct experiments which allow the network to learn a group
representation on the latent space, and show in every case it prefers to learn
the regular representation. Fixing this action on the latent space, this yields
a simple method to impose approximate equivariance as an additional loss
penalty. We benchmark our approach on three datasets and compare it against
several existing equivariant methods, showing that in many cases it achieves
similar or better performance for a fraction of the parameters.

</details>


### [94] [Nearness of Neighbors Attention for Regression in Supervised Finetuning](https://arxiv.org/abs/2506.08139)
*Aviad Susman,Mayte Suárez-Fariñas,Joseph T Colonel*

Main category: cs.LG

TL;DR: 论文提出NONA回归层，结合神经网络与传统算法，在多个无结构数据集上回归表现优于密集层预测和基于SFT嵌入的k - NN。


<details>
  <summary>Details</summary>
Motivation: 传统算法与神经网络特征提取结合时表现好，但因非可微性和独特优化要求，很多未实现为神经网络层，故需解决此问题以提升性能。

Method: 引入Nearness of Neighbors Attention (NONA)回归层，利用神经网络注意力机制和新的注意力掩码方案，得到k - NN回归算法的可微代理。

Result: 在多个无结构数据集上，回归表现优于密集层预测和基于SFT嵌入的k - NN。

Conclusion: NONA回归层是将传统算法融入SFT作为预测层的有效尝试，可提升性能。

Abstract: It is common in supervised machine learning to combine the feature extraction
capabilities of neural networks with the predictive power of traditional
algorithms, such as k-nearest neighbors (k-NN) or support vector machines. This
procedure involves performing supervised fine-tuning (SFT) on a
domain-appropriate feature extractor, followed by training a traditional
predictor on the resulting SFT embeddings. When used in this manner,
traditional predictors often deliver increased performance over the SFT model
itself, despite the fine-tuned feature extractor yielding embeddings
specifically optimized for prediction by the neural network's final dense
layer. This suggests that directly incorporating traditional algorithms into
SFT as prediction layers may further improve performance. However, many
traditional algorithms have not been implemented as neural network layers due
to their non-differentiable nature and their unique optimization requirements.
As a step towards solving this problem, we introduce the Nearness of Neighbors
Attention (NONA) regression layer. NONA uses the mechanics of neural network
attention and a novel learned attention-masking scheme to yield a
differentiable proxy of the k-NN regression algorithm. Results on multiple
unstructured datasets show improved performance over both dense layer
prediction and k-NN on SFT embeddings for regression.

</details>


### [95] [The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks](https://arxiv.org/abs/2506.08274)
*João Manoel Herrera Pinheiro,Suzana Vilas Boas de Oliveira,Thiago Henrique Segreto Silva,Pedro Antonio Rabelo Saraiva,Enzo Ferreira de Souza,Leonardo André Ambrosio,Marcelo Becker*

Main category: cs.LG

TL;DR: 本文系统评估12种特征缩放技术对14种机器学习算法在16个数据集上的影响，发现集成方法对缩放不敏感，其他模型性能受缩放技术影响大，研究提供代码和结果，为从业者提供选择缩放技术的指导。


<details>
  <summary>Details</summary>
Motivation: 解决特征缩放综合研究不足的问题。

Method: 系统评估12种特征缩放技术在14种机器学习算法和16个数据集上的表现，分析对预测性能和计算成本的影响。

Result: 集成方法性能受缩放影响小，其他常用模型性能受缩放技术影响大。

Conclusion: 研究为从业者提供模型特定的特征缩放技术选择指导，且保证研究的透明性和可重复性。

Abstract: This research addresses the critical lack of comprehensive studies on feature
scaling by systematically evaluating 12 scaling techniques - including several
less common transformations - across 14 different Machine Learning algorithms
and 16 datasets for classification and regression tasks. We meticulously
analyzed impacts on predictive performance (using metrics such as accuracy,
MAE, MSE, and $R^2$) and computational costs (training time, inference time,
and memory usage). Key findings reveal that while ensemble methods (such as
Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)
demonstrate robust performance largely independent of scaling, other widely
used models such as Logistic Regression, SVMs, TabNet, and MLPs show
significant performance variations highly dependent on the chosen scaler. This
extensive empirical analysis, with all source code, experimental results, and
model parameters made publicly available to ensure complete transparency and
reproducibility, offers model-specific crucial guidance to practitioners on the
need for an optimal selection of feature scaling techniques.

</details>


### [96] [AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists](https://arxiv.org/abs/2506.08140)
*Yifei Li,Hanane Nour Moussa,Ziru Chen,Shijie Chen,Botao Yu,Mingyi Xue,Benjamin Burns,Tzu-Yao Chiu,Vishal Dey,Zitong Lu,Chen Wei,Qianheng Zhang,Tianyu Zhang,Song Gao,Xuhui Huang,Xia Ning,Nesreen K. Ahmed,Ali Payani,Huan Sun*

Main category: cs.LG

TL;DR: 提出AutoSDT自动管道构建AutoSDT - 5K数据集，训练的AutoSDT - Coder在数据驱动发现基准测试中表现提升。


<details>
  <summary>Details</summary>
Motivation: 解决用AI加速科学发现时训练和评估所需高质量数据稀缺问题。

Method: 提出AutoSDT自动管道，利用大语言模型的编码能力和参数知识收集和合成数据。

Result: 构建了含5404个编码任务的AutoSDT - 5K数据集，专家反馈表明AutoSDT有效；AutoSDT - Coder在两个基准测试中表现提升。

Conclusion: AutoSDT能有效解决数据稀缺问题，训练的模型在数据驱动发现基准测试中有显著表现。

Abstract: Despite long-standing efforts in accelerating scientific discovery with AI,
building AI co-scientists remains challenging due to limited high-quality data
for training and evaluation. To tackle this data scarcity issue, we present
AutoSDT, an automatic pipeline that collects high-quality coding tasks in
real-world data-driven discovery workflows. AutoSDT leverages the coding
capabilities and parametric knowledge of LLMs to search for diverse sources,
select ecologically valid tasks, and synthesize accurate task instructions and
code solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404
coding tasks for data-driven discovery that covers four scientific disciplines
and 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the
only automatically collected and the largest open dataset for data-driven
scientific discovery. Expert feedback on a subset of 256 tasks shows the
effectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,
and 92.2% of the synthesized programs are functionally correct. Trained on
AutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show
substantial improvement on two challenging data-driven discovery benchmarks,
ScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches
the same level of performance as GPT-4o on ScienceAgentBench with a success
rate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it
lifts the hypothesis matching score to 8.1, bringing a 17.4% relative
improvement and closing the gap between open-weight models and GPT-4o.

</details>


### [97] [Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion](https://arxiv.org/abs/2506.08316)
*Alan N. Amin,Nate Gruver,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 本文解释了掩码扩散模型的优越性能，提出了调度条件离散扩散（SCUD）模型，该模型能推广经典离散扩散和掩码扩散，应用于不同数据时表现优于掩码扩散。


<details>
  <summary>Details</summary>
Motivation: 解释掩码扩散模型表现优越的原因，并改进离散扩散模型性能。

Method: 指出离散马尔可夫过程的特点，将已知跳跃时间分布融入任意离散扩散模型得到SCUD模型。

Result: 构建的SCUD模型推广了经典离散扩散和掩码扩散，应用于图像、文本和蛋白质数据时表现优于掩码扩散。

Conclusion: SCUD模型是一种有效的离散扩散模型改进方法。

Abstract: Discrete diffusion models, like continuous diffusion models, generate
high-quality samples by gradually undoing noise applied to datapoints with a
Markov process. Gradual generation in theory comes with many conceptual
benefits; for example, inductive biases can be incorporated into the noising
Markov process, and access to improved sampling algorithms. In practice,
however, the consistently best performing discrete diffusion model is,
surprisingly, masking diffusion, which does not denoise gradually. Here we
explain the superior performance of masking diffusion by noting that it makes
use of a fundamental difference between continuous and discrete Markov
processes: discrete Markov processes evolve by discontinuous jumps at a fixed
rate and, unlike other discrete diffusion models, masking diffusion builds in
the known distribution of jump times and only learns where to jump to. We show
that we can similarly bake in the known distribution of jump times into any
discrete diffusion model. The resulting models - schedule-conditioned discrete
diffusion (SCUD) - generalize classical discrete diffusion and masking
diffusion. By applying SCUD to models with noising processes that incorporate
inductive biases on images, text, and protein data, we build models that
outperform masking.

</details>


### [98] [Accelerating Spectral Clustering under Fairness Constraints](https://arxiv.org/abs/2506.08143)
*Francesco Tonin,Alex Lambert,Johan A. K. Suykens,Volkan Cevher*

Main category: cs.LG

TL;DR: 本文提出用于公平谱聚类的新方法，计算效率高于先前工作，实验证明其有效性，推动公平聚类在现实应用中的采用。


<details>
  <summary>Details</summary>
Motivation: 决策算法的公平性日益重要，聚焦具有群体公平约束的谱聚类问题。

Method: 将公平谱聚类问题置于凸函数差（DC）框架，引入变量增强策略，采用适用于DC问题的交替方向乘子法。

Result: 每个相关子问题可高效求解，计算效率高于先前工作，数值实验表明在合成和真实世界基准上有显著计算时间加速。

Conclusion: 该工作在推动公平聚类在现实应用中的采用方面迈出重要一步。

Abstract: Fairness of decision-making algorithms is an increasingly important issue. In
this paper, we focus on spectral clustering with group fairness constraints,
where every demographic group is represented in each cluster proportionally as
in the general population. We present a new efficient method for fair spectral
clustering (Fair SC) by casting the Fair SC problem within the difference of
convex functions (DC) framework. To this end, we introduce a novel variable
augmentation strategy and employ an alternating direction method of multipliers
type of algorithm adapted to DC problems. We show that each associated
subproblem can be solved efficiently, resulting in higher computational
efficiency compared to prior work, which required a computationally expensive
eigendecomposition. Numerical experiments demonstrate the effectiveness of our
approach on both synthetic and real-world benchmarks, showing significant
speedups in computation time over prior art, especially as the problem size
grows. This work thus represents a considerable step forward towards the
adoption of fair clustering in real-world applications.

</details>


### [99] [A Simple Analysis of Discretization Error in Diffusion Models](https://arxiv.org/abs/2506.08337)
*Juhyeok Choi,Chenglin Fan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Diffusion models, formulated as discretizations of stochastic differential
equations (SDEs), achieve state-of-the-art generative performance. However,
existing analyses of their discretization error often rely on complex
probabilistic tools. In this work, we present a simplified theoretical
framework for analyzing the Euler--Maruyama discretization of
variance-preserving SDEs (VP-SDEs) in Denoising Diffusion Probabilistic Models
(DDPMs), where $ T $ denotes the number of denoising steps in the diffusion
process. Our approach leverages Gr\"onwall's inequality to derive a convergence
rate of $ \mathcal{O}(1/T^{1/2}) $ under Lipschitz assumptions, significantly
streamlining prior proofs. Furthermore, we demonstrate that the Gaussian noise
in the discretization can be replaced by a discrete random variable (e.g.,
Rademacher or uniform noise) without sacrificing convergence guarantees-an
insight with practical implications for efficient sampling. Experiments
validate our theory, showing that (1) the error scales as predicted, (2)
discrete noise achieves comparable sample quality to Gaussian noise, and (3)
incorrect noise scaling degrades performance. By unifying simplified analysis
and discrete noise substitution, our work bridges theoretical rigor with
practical efficiency in diffusion-based generative modeling.

</details>


### [100] [Fully data-driven inverse hyperelasticity with hyper-network neural ODE fields](https://arxiv.org/abs/2506.08146)
*Vahidullah Taç,Amirhossein Amiri-Hezaveh,Manuel K. Rausch,Grace N. Bechtel,Francisco Sahli Costabal,Adrian Buganza Tepole*

Main category: cs.LG

TL;DR: 提出识别异质材料力学特性新框架，用神经网络获应变场，基于NODE发现本构方程，用超网络处理异质性，经数值例子验证其稳健通用。


<details>
  <summary>Details</summary>
Motivation: 在无封闭形式本构方程情况下识别异质材料力学特性。

Method: 用含傅里叶特征的神经网络获应变场，基于NODE构建数据驱动方法发现本构方程，用超网络处理异质性，优化超网络参数最小化多目标损失函数。

Result: 经多个数值例子验证，包括材料参数变化、各向同性到各向异性转变、含噪声材料识别及实验数据应用。

Conclusion: 该方法在很少假设下能稳健通用地识别异质材料力学特性，可替代经典逆方法。

Abstract: We propose a new framework for identifying mechanical properties of
heterogeneous materials without a closed-form constitutive equation. Given a
full-field measurement of the displacement field, for instance as obtained from
digital image correlation (DIC), a continuous approximation of the strain field
is obtained by training a neural network that incorporates Fourier features to
effectively capture sharp gradients in the data. A physics-based data-driven
method built upon ordinary neural differential equations (NODEs) is employed to
discover constitutive equations. The NODE framework can represent arbitrary
materials while satisfying constraints in the theory of constitutive equations
by default. To account for heterogeneity, a hyper-network is defined, where the
input is the material coordinate system, and the output is the NODE-based
constitutive equation. The parameters of the hyper-network are optimized by
minimizing a multi-objective loss function that includes penalty terms for
violations of the strong form of the equilibrium equations of elasticity and
the associated Neumann boundary conditions. We showcase the framework with
several numerical examples, including heterogeneity arising from variations in
material parameters, spatial transitions from isotropy to anisotropy, material
identification in the presence of noise, and, ultimately, application to
experimental data. As the numerical results suggest, the proposed approach is
robust and general in identifying the mechanical properties of heterogeneous
materials with very few assumptions, making it a suitable alternative to
classical inverse methods.

</details>


### [101] [Spatiotemporal deep learning models for detection of rapid intensification in cyclones](https://arxiv.org/abs/2506.08397)
*Vamshika Sutar,Amandeep Singh,Rohitash Chandra*

Main category: cs.LG

TL;DR: 文章评估深度学习、集成学习和数据增强框架检测气旋快速增强，提出用深度学习生成数据解决类别不平衡问题，结果表明数据增强和空间坐标对检测有帮助。


<details>
  <summary>Details</summary>
Motivation: 气旋快速增强是极端事件，发生少导致数据集类别不平衡，多种因素影响其发生，传统机器学习模型处理困难。

Method: 评估深度学习、集成学习和数据增强框架，用深度学习模型生成空间坐标和风力强度解决类别不平衡，在数据增强框架中用深度学习模型分类。

Result: 数据增强提高了气旋快速增强检测结果，空间坐标作为输入特征起关键作用。

Conclusion: 为含极端事件的时空数据合成数据生成研究铺平道路。

Abstract: Cyclone rapid intensification is the rapid increase in cyclone wind
intensity, exceeding a threshold of 30 knots, within 24 hours. Rapid
intensification is considered an extreme event during a cyclone, and its
occurrence is relatively rare, contributing to a class imbalance in the
dataset. A diverse array of factors influences the likelihood of a cyclone
undergoing rapid intensification, further complicating the task for
conventional machine learning models. In this paper, we evaluate deep learning,
ensemble learning and data augmentation frameworks to detect cyclone rapid
intensification based on wind intensity and spatial coordinates. We note that
conventional data augmentation methods cannot be utilised for generating
spatiotemporal patterns replicating cyclones that undergo rapid
intensification. Therefore, our framework employs deep learning models to
generate spatial coordinates and wind intensity that replicate cyclones to
address the class imbalance problem of rapid intensification. We also use a
deep learning model for the classification module within the data augmentation
framework to differentiate between rapid and non-rapid intensification events
during a cyclone. Our results show that data augmentation improves the results
for rapid intensification detection in cyclones, and spatial coordinates play a
critical role as input features to the given models. This paves the way for
research in synthetic data generation for spatiotemporal data with extreme
events.

</details>


### [102] [BLUR: A Bi-Level Optimization Approach for LLM Unlearning](https://arxiv.org/abs/2506.08164)
*Hadi Reisizadeh,Jinghan Jia,Zhiqi Bu,Bhanukiran Vinzamuri,Anil Ramakrishna,Kai-Wei Chang,Volkan Cevher,Sijia Liu,Mingyi Hong*

Main category: cs.LG

TL;DR: 本文提出新的大语言模型遗忘学习问题的双层优化公式及算法BLUR，实验表明其性能优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型遗忘学习问题的公式化方法常导致性能下降，需要更好的公式化方法。

Method: 提出对遗忘学习问题进行分层建模，得到双层优化公式，基于此提出BLUR算法。

Result: 广泛实验显示，BLUR在各种遗忘学习任务、模型和指标上始终优于所有现有算法。

Conclusion: 新的双层优化公式及BLUR算法在大语言模型遗忘学习问题上有良好表现。

Abstract: Enabling large language models (LLMs) to unlearn knowledge and capabilities
acquired during training has proven vital for ensuring compliance with data
regulations and promoting ethical practices in generative AI. Although there
are growing interests in developing various unlearning algorithms, it remains
unclear how to best formulate the unlearning problem. The most popular
formulation uses a weighted sum of forget and retain loss, but it often leads
to performance degradation due to the inherent trade-off between forget and
retain losses. In this work, we argue that it is important to model the
hierarchical structure of the unlearning problem, where the forget problem
(which \textit{unlearns} certain knowledge and/or capabilities) takes priority
over the retain problem (which preserves model utility). This hierarchical
structure naturally leads to a bi-level optimization formulation where the
lower-level objective focuses on minimizing the forget loss, while the
upper-level objective aims to maintain the model's utility. Based on this new
formulation, we propose a novel algorithm, termed Bi-Level UnleaRning
(\texttt{BLUR}), which not only possesses strong theoretical guarantees but
more importantly, delivers superior performance. In particular, our extensive
experiments demonstrate that \texttt{BLUR} consistently outperforms all the
state-of-the-art algorithms across various unlearning tasks, models, and
metrics. Codes are available at
https://github.com/OptimAI-Lab/BLURLLMUnlearning.

</details>


### [103] [Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs](https://arxiv.org/abs/2506.08727)
*Samarth Sikand,Rohit Mehra,Priyavanshi Pathania,Nikhil Bamby,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.LG

TL;DR: 生成式AI发展快但耗能大，现有能耗监测和估算工具存在不足，提出R - ICE框架利用LLM基准估算推理碳排放，验证结果有潜力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI使用大型语言模型对能源网格和环境造成负担，阻碍组织可持续发展目标，且缺乏能耗和碳排放估算工具，现有工具存在诸多弊端。

Method: 利用新兴LLM基准和相关数据点，提出R - ICE框架，借助现有先进基准估算提示级推理碳排放。

Result: 验证结果有前景，表明基于基准的建模在推理排放估算方面有很大潜力。

Conclusion: 基于基准的建模在推理排放估算方面值得科学界进一步探索。

Abstract: While Generative AI stands to be one of the fastest adopted technologies
ever, studies have made evident that the usage of Large Language Models (LLMs)
puts significant burden on energy grids and our environment. It may prove a
hindrance to the Sustainability goals of any organization. A crucial step in
any Sustainability strategy is monitoring or estimating the energy consumption
of various components. While there exist multiple tools for monitoring energy
consumption, there is a dearth of tools/frameworks for estimating the
consumption or carbon emissions. Current drawbacks of both monitoring and
estimation tools include high input data points, intrusive nature, high error
margin, etc. We posit that leveraging emerging LLM benchmarks and related data
points can help overcome aforementioned challenges while balancing accuracy of
the emission estimations. To that extent, we discuss the challenges of current
approaches and present our evolving framework, R-ICE, which estimates prompt
level inference carbon emissions by leveraging existing state-of-the-art(SOTA)
benchmark. This direction provides a more practical and non-intrusive way to
enable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our
promising validation results suggest that benchmark-based modelling holds great
potential for inference emission estimation and warrants further exploration
from the scientific community.

</details>


### [104] [Improved Scaling Laws in Linear Regression via Data Reuse](https://arxiv.org/abs/2506.08415)
*Licong Lin,Jingfeng Wu,Peter L. Bartlett*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Neural scaling laws suggest that the test error of large language models
trained online decreases polynomially as the model size and data size increase.
However, such scaling can be unsustainable when running out of new data. In
this work, we show that data reuse can improve existing scaling laws in linear
regression. Specifically, we derive sharp test error bounds on $M$-dimensional
linear models trained by multi-pass stochastic gradient descent (multi-pass
SGD) on $N$ data with sketched features. Assuming that the data covariance has
a power-law spectrum of degree $a$, and that the true parameter follows a prior
with an aligned power-law spectrum of degree $b-a$ (with $a > b > 1$), we show
that multi-pass SGD achieves a test error of $\Theta(M^{1-b} + L^{(1-b)/a})$,
where $L \lesssim N^{a/b}$ is the number of iterations. In the same setting,
one-pass SGD only attains a test error of $\Theta(M^{1-b} + N^{(1-b)/a})$ (see
e.g., Lin et al., 2024). This suggests an improved scaling law via data reuse
(i.e., choosing $L>N$) in data-constrained regimes. Numerical simulations are
also provided to verify our theoretical findings.

</details>


### [105] [Online Learning-guided Learning Rate Adaptation via Gradient Alignment](https://arxiv.org/abs/2506.08419)
*Ruichen Jiang,Ali Kavis,Aryan Mokhtari*

Main category: cs.LG

TL;DR: 提出GALA框架动态调整学习率，理论分析收敛率，实验表明增强常见优化器性能且无需调参。


<details>
  <summary>Details</summary>
Motivation: 优化器在大规模深度学习模型上性能依赖学习率微调，需大量超参网格搜索。

Method: 提出GALA框架，通过跟踪连续梯度对齐和局部曲率估计动态调整学习率，将学习率选择问题建模为一维在线学习问题。

Result: 为配备GALA的归一化SGD建立数据自适应收敛率，增强SGD和Adam等常见优化器在不同初始学习率下的性能。

Conclusion: GALA框架灵活自适应，能提升优化器性能，无需大量调参。

Abstract: The performance of an optimizer on large-scale deep learning models depends
critically on fine-tuning the learning rate, often requiring an extensive grid
search over base learning rates, schedules, and other hyperparameters. In this
paper, we propose a principled framework called GALA (Gradient Alignment-based
Learning rate Adaptation), which dynamically adjusts the learning rate by
tracking the alignment between consecutive gradients and using a local
curvature estimate. Guided by the convergence analysis, we formulate the
problem of selecting the learning rate as a one-dimensional online learning
problem. When paired with an online learning algorithm such as
Follow-the-Regularized-Leader, our method produces a flexible, adaptive
learning rate schedule that tends to increase when consecutive gradients are
aligned and decrease otherwise. We establish a data-adaptive convergence rate
for normalized SGD equipped with GALA in the smooth, nonconvex setting.
Empirically, common optimizers such as SGD and Adam, when augmented with GALA,
demonstrate robust performance across a wide range of initial learning rates
and perform competitively without the need for tuning.

</details>


### [106] [FedGA-Tree: Federated Decision Tree using Genetic Algorithm](https://arxiv.org/abs/2506.08176)
*Anh V Nguyen,Diego Klabjan*

Main category: cs.LG

TL;DR: 现有联邦学习对决策树研究不足，本文用遗传算法构建个性化决策树，支持分类和回归树，实验显示优于本地训练和基准算法。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习多关注参数化梯度模型，非参数决策树研究少，现有适配方法局限于分类树和分类数据。

Method: 利用遗传算法构建个性化决策树，能处理分类和数值数据。

Result: 方法优于仅在本地数据上训练的决策树和基准算法。

Conclusion: 所提出利用遗传算法构建决策树的方法有效，可用于联邦学习中的分类和回归任务。

Abstract: In recent years, with rising concerns for data privacy, Federated Learning
has gained prominence, as it enables collaborative training without the
aggregation of raw data from participating clients. However, much of the
current focus has been on parametric gradient-based models, while nonparametric
counterparts such as decision tree are relatively understudied. Existing
methods for adapting decision trees to Federated Learning generally combine a
greedy tree-building algorithm with differential privacy to produce a global
model for all clients. These methods are limited to classification trees and
categorical data due to the constraints of differential privacy. In this paper,
we explore an alternative approach that utilizes Genetic Algorithm to
facilitate the construction of personalized decision trees and accommodate
categorical and numerical data, thus allowing for both classification and
regression trees. Comprehensive experiments demonstrate that our method
surpasses decision trees trained solely on local data and a benchmark
algorithm.

</details>


### [107] [The Geometries of Truth Are Orthogonal Across Tasks](https://arxiv.org/abs/2506.08572)
*Waiss Azizian,Michael Kirchhof,Eugene Ndiaye,Louis Bethune,Michal Klein,Pierre Ablin,Marco Cuturi*

Main category: cs.LG

TL;DR: 指出基于大语言模型激活值判断答案正确性的‘真理几何’方法存在跨任务迁移性差的局限，且更复杂方法也难以克服。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽有泛化能力，但可靠性受关注，已有通过分析推理时激活值判断答案正确性的方法，需研究其局限性。

Method: 观察不同任务训练的线性分类器的相似性及支持集，研究更复杂方法能否克服局限。

Result: ‘真理几何’方法跨任务迁移性差，不同任务训练的线性分类器相似度低，用稀疏正则化训练时支持集几乎不相交，更复杂方法也难以克服。

Conclusion: 基于激活值的‘真理几何’方法存在跨任务局限性，需寻找更好方法解决大语言模型可靠性问题。

Abstract: Large Language Models (LLMs) have demonstrated impressive generalization
capabilities across various tasks, but their claim to practical relevance is
still mired by concerns on their reliability. Recent works have proposed
examining the activations produced by an LLM at inference time to assess
whether its answer to a question is correct. Some works claim that a "geometry
of truth" can be learned from examples, in the sense that the activations that
generate correct answers can be distinguished from those leading to mistakes
with a linear classifier. In this work, we underline a limitation of these
approaches: we observe that these "geometries of truth" are intrinsically
task-dependent and fail to transfer across tasks. More precisely, we show that
linear classifiers trained across distinct tasks share little similarity and,
when trained with sparsity-enforcing regularizers, have almost disjoint
supports. We show that more sophisticated approaches (e.g., using mixtures of
probes and tasks) fail to overcome this limitation, likely because activation
vectors commonly used to classify answers form clearly separated clusters when
examined across tasks.

</details>


### [108] [Correlated Noise Mechanisms for Differentially Private Learning](https://arxiv.org/abs/2506.08201)
*Krishna Pillutla,Jalaj Upadhyay,Christopher A. Choquette-Choo,Krishnamurthy Dvijotham,Arun Ganesh,Monika Henzinger,Jonathan Katz,Ryan McKenna,H. Brendan McMahan,Keith Rush,Thomas Steinke,Abhradeep Thakurta*

Main category: cs.LG

TL;DR: 本文探讨差分隐私相关噪声机制设计分析，及其在AI和机器学习模型隐私训练的应用。


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私机制在随机梯度学习算法各步骤注入独立噪声，而引入噪声相关性可改善隐私 - 效用权衡。

Method: 聚焦通过加权前缀和估计这一核心原语来设计和分析相关噪声机制。

Result: 相关噪声机制在实践中有影响力，已在全球工业规模部署。

Conclusion: 引入噪声相关性的差分隐私机制可改善隐私 - 效用权衡，有实际应用价值。

Abstract: This monograph explores the design and analysis of correlated noise
mechanisms for differential privacy (DP), focusing on their application to
private training of AI and machine learning models via the core primitive of
estimation of weighted prefix sums. While typical DP mechanisms inject
independent noise into each step of a stochastic gradient (SGD) learning
algorithm in order to protect the privacy of the training data, a growing body
of recent research demonstrates that introducing (anti-)correlations in the
noise can significantly improve privacy-utility trade-offs by carefully
canceling out some of the noise added on earlier steps in subsequent steps.
Such correlated noise mechanisms, known variously as matrix mechanisms,
factorization mechanisms, and DP-Follow-the-Regularized-Leader (DP-FTRL) when
applied to learning algorithms, have also been influential in practice, with
industrial deployment at a global scale.

</details>


### [109] [What makes an Ensemble (Un) Interpretable?](https://arxiv.org/abs/2506.08216)
*Shahaf Bassan,Guy Amir,Meirav Zehavi,Guy Katz*

Main category: cs.LG

TL;DR: 本文运用计算复杂度理论研究集成模型可解释性，揭示不同因素影响下的复杂度模式，为理解集成模型可解释性提供基础。


<details>
  <summary>Details</summary>
Motivation: 机器学习界虽认识到集成模型可解释性有限，但缺乏对其（不可）解释性的严格数学理解，本文旨在填补这一空白。

Method: 应用计算复杂度理论研究不同集成配置生成解释的挑战。

Result: 发现不同因素影响下的细微复杂度模式，如在标准复杂度假设下，即使基模型大小固定，解释集成模型仍难处理；决策树小集成可高效解释，而线性模型即使数量固定解释仍难处理。

Conclusion: 研究结果为理解集成模型可解释性提供更坚实基础，强调从计算复杂度角度研究的益处。

Abstract: Ensemble models are widely recognized in the ML community for their limited
interpretability. For instance, while a single decision tree is considered
interpretable, ensembles of trees (e.g., boosted trees) are often treated as
black-boxes. Despite this folklore recognition, there remains a lack of
rigorous mathematical understanding of what particularly makes an ensemble
(un)-interpretable, including how fundamental factors like the (1) *number*,
(2) *size*, and (3) *type* of base models influence its interpretability. In
this work, we seek to bridge this gap by applying concepts from computational
complexity theory to study the challenges of generating explanations for
various ensemble configurations. Our analysis uncovers nuanced complexity
patterns influenced by various factors. For example, we demonstrate that under
standard complexity assumptions like P$\neq$NP, interpreting ensembles remains
intractable even when base models are of constant size. Surprisingly, the
complexity changes drastically with the number of base models: small ensembles
of decision trees are efficiently interpretable, whereas interpreting ensembles
with even a constant number of linear models remains intractable. We believe
that our findings provide a more robust foundation for understanding the
interpretability of ensembles, emphasizing the benefits of examining it through
a computational complexity lens.

</details>


### [110] [Local MDI+: Local Feature Importances for Tree-Based Models](https://arxiv.org/abs/2506.08928)
*Zhongyuan Liang,Zachary T. Rewolinski,Abhineet Agarwal,Tiffany M. Tang,Bin Yu*

Main category: cs.LG

TL;DR: 提出局部MDI+（LMDI+）方法用于表格数据特征重要性分析，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有局部特征重要性方法依赖近似且忽略模型内部结构，全局MDI+无法处理个体特征异质性，需要新方法解决这些问题。

Method: 提出LMDI+，将MDI+框架扩展到样本特定设置。

Result: LMDI+在识别特定实例信号特征上优于LIME和TreeSHAP，下游任务性能平均提高10%，且在多次随机森林拟合中表现更稳定。

Conclusion: LMDI+可实现局部可解释性用例，如识别更接近的反事实和发现同质子组。

Abstract: Tree-based ensembles such as random forests remain the go-to for tabular data
over deep learning models due to their prediction performance and computational
efficiency. These advantages have led to their widespread deployment in
high-stakes domains, where interpretability is essential for ensuring
trustworthy predictions. This has motivated the development of popular local
(i.e. sample-specific) feature importance (LFI) methods such as LIME and
TreeSHAP. However, these approaches rely on approximations that ignore the
model's internal structure and instead depend on potentially unstable
perturbations. These issues are addressed in the global setting by MDI+, a
feature importance method which exploits an equivalence between decision trees
and linear models on a transformed node basis. However, the global MDI+ scores
are not able to explain predictions when faced with heterogeneous individual
characteristics. To address this gap, we propose Local MDI+ (LMDI+), a novel
extension of the MDI+ framework to the sample specific setting. LMDI+
outperforms existing baselines LIME and TreeSHAP in identifying
instance-specific signal features, averaging a 10% improvement in downstream
task performance across twelve real-world benchmark datasets. It further
demonstrates greater stability by consistently producing similar instance-level
feature importance rankings across multiple random forest fits. Finally, LMDI+
enables local interpretability use cases, including the identification of
closer counterfactuals and the discovery of homogeneous subgroups.

</details>


### [111] [Mondrian: Transformer Operators via Domain Decomposition](https://arxiv.org/abs/2506.08226)
*Arthur Feeney,Kuei-Hsiang Huang,Aparna Chandramowlishwaran*

Main category: cs.LG

TL;DR: 介绍了名为Mondrian的变压器算子，它将域分解为非重叠子域以处理PDEs，在Allen - Cahn和Navier - Stokes PDEs上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于变压器的算子模型在高分辨率、多尺度域上因注意力的二次成本及其与离散化的耦合而难以扩展。

Method: 将域分解为非重叠子域，在子域内用表达性神经算子替代标准层，通过基于softmax的函数内积计算子域间的注意力，自然扩展到分层窗口和邻域注意力。

Result: Mondrian在Allen - Cahn和Navier - Stokes PDEs上取得了良好性能，无需重新训练即可实现分辨率扩展。

Conclusion: 基于域分解的注意力在可扩展和通用的神经算子方面有很大潜力。

Abstract: Operator learning enables data-driven modeling of partial differential
equations (PDEs) by learning mappings between function spaces. However, scaling
transformer-based operator models to high-resolution, multiscale domains
remains a challenge due to the quadratic cost of attention and its coupling to
discretization. We introduce \textbf{Mondrian}, transformer operators that
decompose a domain into non-overlapping subdomains and apply attention over
sequences of subdomain-restricted functions. Leveraging principles from domain
decomposition, Mondrian decouples attention from discretization. Within each
subdomain, it replaces standard layers with expressive neural operators, and
attention across subdomains is computed via softmax-based inner products over
functions. The formulation naturally extends to hierarchical windowed and
neighborhood attention, supporting both local and global interactions. Mondrian
achieves strong performance on Allen-Cahn and Navier-Stokes PDEs, demonstrating
resolution scaling without retraining. These results highlight the promise of
domain-decomposed attention for scalable and general-purpose neural operators.

</details>


### [112] [Scaling Laws of Motion Forecasting and Planning -- A Technical Report](https://arxiv.org/abs/2506.08228)
*Mustafa Baniodeh,Kratarth Goel,Scott Ettinger,Carlos Fuertes,Ari Seff,Tim Shen,Cole Gulino,Chenjie Yang,Ghassen Jerfel,Dokook Choe,Rui Wang,Vinutha Kallem,Sergio Casas,Rami Al-Rfou,Benjamin Sapp,Dragomir Anguelov*

Main category: cs.LG

TL;DR: 研究自动驾驶领域编解码自回归Transformer模型在运动预测和规划任务的经验缩放定律，探讨训练和推理计算缩放，强调优化缩放性能的重要性，还研究通用数据训练的效用。


<details>
  <summary>Details</summary>
Motivation: 探索自动驾驶领域运动预测和规划模型的性能提升方法，解决数据稀缺问题。

Method: 使用50万小时驾驶数据集，研究模型性能与计算预算的关系、参数和数据量的最优缩放、推理时间计算缩放，还研究通用驾驶数据训练。

Result: 模型性能随计算预算呈幂律增长，训练损失与评估指标强相关，闭环指标随缩放改善，最优缩放时模型大小增长速度是数据集的1.5倍，小模型采样聚类在一定程度可与大模型竞争。

Conclusion: 优化运动预测和规划模型的训练和推理缩放特性是提升性能的关键，研究通用数据训练对解决数据稀缺有意义。

Abstract: We study the empirical scaling laws of a family of encoder-decoder
autoregressive transformer models on the task of joint motion forecasting and
planning in the autonomous driving domain. Using a 500 thousand hours driving
dataset, we demonstrate that, similar to language modeling, model performance
improves as a power-law function of the total compute budget, and we observe a
strong correlation between model training loss and model evaluation metrics.
Most interestingly, closed-loop metrics also improve with scaling, which has
important implications for the suitability of open-loop metrics for model
development and hill climbing. We also study the optimal scaling of the number
of transformer parameters and the training data size for a training
compute-optimal model. We find that as the training compute budget grows,
optimal scaling requires increasing the model size 1.5x as fast as the dataset
size. We also study inference-time compute scaling, where we observe that
sampling and clustering the output of smaller models makes them competitive
with larger models, up to a crossover point beyond which a larger models
becomes more inference-compute efficient. Overall, our experimental results
demonstrate that optimizing the training and inference-time scaling properties
of motion forecasting and planning models is a key lever for improving their
performance to address a wide variety of driving scenarios. Finally, we briefly
study the utility of training on general logged driving data of other agents to
improve the performance of the ego-agent, an important research area to address
the scarcity of robotics data for large capacity models training.

</details>


### [113] [Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations](https://arxiv.org/abs/2506.08240)
*Dongkyu Cho,Rumi Chunara*

Main category: cs.LG

TL;DR: 重新审视随机数据增强，提出解决遗忘问题的简单方法提升其泛化效果。


<details>
  <summary>Details</summary>
Motivation: 数据增强中随机增强虽成本低但效果欠佳，需探索解决其缺点的方法。

Method: 指出随机增强的随机特性会产生碰撞增强扭曲特征，类似灾难性遗忘，提出解决遗忘问题的方法。

Result: 所提方法在各种单源域泛化基准测试中展现出强大的泛化性能。

Conclusion: 提出的简单解决方案能有效提升随机增强的泛化效果。

Abstract: Data augmentation is a promising tool for enhancing out-of-distribution
generalization, where the key is to produce diverse, challenging variations of
the source domain via costly targeted augmentations that maximize its
generalization effect. Conversely, random augmentation is inexpensive but is
deemed suboptimal due to its limited effect. In this paper, we revisit random
augmentation and explore methods to address its shortcomings. We show that the
stochastic nature of random augmentation can produce a set of colliding
augmentations that distorts the learned features, similar to catastrophic
forgetting. We propose a simple solution that improves the generalization
effect of random augmentation by addressing forgetting, which displays strong
generalization performance across various single source domain generalization
(sDG) benchmarks.

</details>


### [114] [Temporalizing Confidence: Evaluation of Chain-of-Thought Reasoning with Signal Temporal Logic](https://arxiv.org/abs/2506.08243)
*Zhenjiang Mao,Artem Bisliouk,Rohith Reddy Nama,Ivan Ruchkin*

Main category: cs.LG

TL;DR: 提出结构化框架评估大语言模型数学推理置信度，实验显示优于传统方法


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学推理中输出高置信但可能错误的结果，在教育等领域有风险

Method: 将逐步置信度建模为时间信号，用信号时序逻辑评估，定义STL约束，计算鲁棒性分数，引入不确定性重塑策略

Result: 实验表明该方法持续改进校准指标，提供比传统置信聚合和事后校准更可靠的不确定性估计

Conclusion: 所提方法有效且优于传统方法

Abstract: Large Language Models (LLMs) have shown impressive performance in
mathematical reasoning tasks when guided by Chain-of-Thought (CoT) prompting.
However, they tend to produce highly confident yet incorrect outputs, which
poses significant risks in domains like education, where users may lack the
expertise to assess reasoning steps. To address this, we propose a structured
framework that models stepwise confidence as a temporal signal and evaluates it
using Signal Temporal Logic (STL). In particular, we define formal STL-based
constraints to capture desirable temporal properties and compute robustness
scores that serve as structured, interpretable confidence estimates. Our
approach also introduces a set of uncertainty reshaping strategies to enforce
smoothness, monotonicity, and causal consistency across the reasoning
trajectory. Experiments show that our approach consistently improves
calibration metrics and provides more reliable uncertainty estimates than
conventional confidence aggregation and post-hoc calibration.

</details>


### [115] [SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense](https://arxiv.org/abs/2506.08255)
*Patryk Krukowski,Łukasz Gorczyca,Piotr Helm,Kamil Książek,Przemysław Spurek*

Main category: cs.LG

TL;DR: 传统深度神经网络存在灾难性遗忘和对抗攻击脆弱问题，现有模型无法同时解决，提出SHIELD方法集成超网络持续学习和区间算术，增强安全性且不牺牲适应性。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络有灾难性遗忘和对输入扰动缺乏鲁棒性问题，且无模型能同时解决这两个问题。

Method: 引入SHIELD方法，集成基于超网络的持续学习方法和区间算术，用超网络将可训练任务嵌入向量转换为目标模型权重，目标模型处理区间范围输入数据。

Result: 目标模型能为区间范围内数据样本的所有可能攻击提供严格保证。

Conclusion: 该方法增强了安全性，同时不牺牲网络适应性，解决了持续学习中被忽视的安全挑战。

Abstract: Traditional deep neural networks suffer from several limitations, including
catastrophic forgetting. When models are adapted to new datasets, they tend to
quickly forget previously learned knowledge. Another significant issue is the
lack of robustness to even small perturbations in the input data. In practice,
we can often easily perform adversarial attacks and change the network's
predictions, adding minimal noise to the input. Dedicated architectures and
training procedures can solve each of the above problems separately.
Unfortunately, currently, no model can simultaneously address both catastrophic
forgetting and vulnerability to adversarial attacks. We introduce SHIELD
(Secure Hypernetworks for Incremental Expansion and Learning Defense), a novel
approach that integrates a hypernetwork-based continual learning approach with
interval arithmetic. SHIELD use the hypernetwork to transfer trainable task
embedding vectors into the weights of a target model dedicated to specific
data. This paradigm allows for the dynamic generation of separate networks for
each subtask, while the hypernetwork aggregates and analyzes information across
all tasks. The target model takes in the input a data sample with a defined
interval range, and by creating a hypercube, produces a prediction for the
given range. Therefore, such target models provide strict guarantees against
all possible attacks for data samples within the interval range. Our approach
enhances security without sacrificing network adaptability, addressing the
overlooked challenge of safety in continual learning.

</details>


### [116] [Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints](https://arxiv.org/abs/2506.08266)
*Yaswanth Chittepu,Blossom Metevier,Will Schwarzer,Austin Hoag,Scott Niekum,Philip S. Thomas*

Main category: cs.LG

TL;DR: 提出HC - RLHF方法确保语言模型在敏感领域安全且有用，理论分析并实证应用该方法，结果显示其有效。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型对齐方法将安全与有用性视为权衡，导致敏感领域响应不可接受，需确保可靠表现。

Method: 提出HC - RLHF方法，解耦人类偏好为有用性和无害性，分别训练奖励和成本模型，采用两步法找安全解，先在悲观成本约束下优化奖励函数，再进行安全测试。

Result: 将HC - RLHF应用于三个语言模型，结果表明能大概率产生安全模型，相比之前方法可提升无害性和有用性。

Conclusion: HC - RLHF能提供高置信度安全保证，同时最大化有用性，是有效的语言模型对齐方法。

Abstract: Existing approaches to language model alignment often treat safety as a
tradeoff against helpfulness, which can lead to unacceptable responses in
sensitive domains. To ensure reliable performance in such settings, we propose
High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a
method that provides high-confidence safety guarantees while maximizing
helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human
preferences into helpfulness and harmlessness (safety), which are learned by
training a reward model and a cost model, respectively. It then employs a
two-step process to find safe solutions. In the first step, it optimizes the
reward function under an intentionally pessimistic version of the cost
constraint. In the second step, the trained model undergoes a safety test to
verify whether its performance stays within an upper-confidence bound of the
actual cost constraint. We provide a theoretical analysis of HC-RLHF, including
proof that it will not return an unsafe solution with a probability greater
than a user-specified threshold. For our empirical analysis, we apply HC-RLHF
to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and
LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF
produces safe models with high probability and can improve harmlessness and
helpfulness compared to previous methods.

</details>


### [117] [Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression](https://arxiv.org/abs/2506.08267)
*Mansooreh Montazerin,Majd Al Aawar,Antonio Ortega,Ajitesh Srivastava*

Main category: cs.LG

TL;DR: 本文提出LIES框架用于符号回归，通过特定策略提取紧凑公式，实验表明其表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归方法在可扩展性和符号一致性方面存在问题，需要新方法。

Method: 引入具有可解释原始激活的固定神经网络架构LIES，采用过采样策略和定制损失函数训练，并应用额外剪枝策略。

Result: LIES框架在符号回归基准测试中始终能生成稀疏且准确的符号公式，优于所有基线。

Conclusion: LIES框架有效，且通过消融实验证明各设计组件的重要性。

Abstract: Symbolic regression (SR) aims to discover closed-form mathematical
expressions that accurately describe data, offering interpretability and
analytical insight beyond standard black-box models. Existing SR methods often
rely on population-based search or autoregressive modeling, which struggle with
scalability and symbolic consistency. We introduce LIES (Logarithm, Identity,
Exponential, Sine), a fixed neural network architecture with interpretable
primitive activations that are optimized to model symbolic expressions. We
develop a framework to extract compact formulae from LIES networks by training
with an appropriate oversampling strategy and a tailored loss function to
promote sparsity and to prevent gradient instability. After training, it
applies additional pruning strategies to further simplify the learned
expressions into compact formulae. Our experiments on SR benchmarks show that
the LIES framework consistently produces sparse and accurate symbolic formulae
outperforming all baselines. We also demonstrate the importance of each design
component through ablation studies.

</details>


### [118] [SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space](https://arxiv.org/abs/2506.08270)
*Zitong Huang,Mansooreh Montazerin,Ajitesh Srivastava*

Main category: cs.LG

TL;DR: 提出同时优化神经网络架构和权重的新方法，通过训练通用多尺度自编码器嵌入信息到连续潜在空间，在空间中随机初始化点并梯度下降优化，实验证明可发现高效网络。


<details>
  <summary>Details</summary>
Motivation: 传统设计神经网络的手动试错法耗时耗力，神经架构搜索法会分离架构搜索和权重优化，需要新方法同时优化架构和权重。

Method: 先训练通用多尺度自编码器将架构和参数信息嵌入连续潜在空间，在空间随机初始化点并通过梯度下降更新以获得最优网络，优化中加入稀疏和紧凑惩罚。

Result: 在合成回归任务实验中，有效发现稀疏、紧凑且性能强的神经网络。

Conclusion: 所提方法能有效同时优化神经网络的架构和权重，得到高效模型。

Abstract: Designing neural networks typically relies on manual trial and error or a
neural architecture search (NAS) followed by weight training. The former is
time-consuming and labor-intensive, while the latter often discretizes
architecture search and weight optimization. In this paper, we propose a
fundamentally different approach that simultaneously optimizes both the
architecture and the weights of a neural network. Our framework first trains a
universal multi-scale autoencoder that embeds both architectural and parametric
information into a continuous latent space, where functionally similar neural
networks are mapped closer together. Given a dataset, we then randomly
initialize a point in the embedding space and update it via gradient descent to
obtain the optimal neural network, jointly optimizing its structure and
weights. The optimization process incorporates sparsity and compactness
penalties to promote efficient models. Experiments on synthetic regression
tasks demonstrate that our method effectively discovers sparse and compact
neural networks with strong performance.

</details>


### [119] [Universal Differential Equations for Scientific Machine Learning of Node-Wise Battery Dynamics in Smart Grids](https://arxiv.org/abs/2506.08272)
*Tarushri N. S.*

Main category: cs.LG

TL;DR: 本文提出基于通用微分方程（UDE）的方法学习节点特定的电池演化，实验表明该方法可行，对可再生能源集成智能电网有意义。


<details>
  <summary>Details</summary>
Motivation: 在智能电网系统中，传统方法在建模节点电池动态时存在泛化困难、无法捕捉未建模残余动态的问题，因此需要新方法。

Method: 将神经残差嵌入受物理启发的电池常微分方程中，使用合成的太阳能发电和负荷需求数据模拟电池动态，让神经组件学习对未观测到或随机因素的修正。

Result: 训练后的UDE与真实电池轨迹紧密对齐，收敛行为平滑，长期预测稳定。

Conclusion: 基于UDE的科学机器学习方法用于分散能源网络的电池建模是可行的，对可再生能源集成智能电网的实时控制和优化有更广泛意义。

Abstract: Universal Differential Equations (UDEs), which blend neural networks with
physical differential equations, have emerged as a powerful framework for
scientific machine learning (SciML), enabling data-efficient, interpretable,
and physically consistent modeling. In the context of smart grid systems,
modeling node-wise battery dynamics remains a challenge due to the
stochasticity of solar input and variability in household load profiles.
Traditional approaches often struggle with generalization and fail to capture
unmodeled residual dynamics. This work proposes a UDE-based approach to learn
node-specific battery evolution by embedding a neural residual into a
physically inspired battery ODE. Synthetic yet realistic solar generation and
load demand data are used to simulate battery dynamics over time. The neural
component learns to model unobserved or stochastic corrections arising from
heterogeneity in node demand and environmental conditions. Comprehensive
experiments reveal that the trained UDE aligns closely with ground truth
battery trajectories, exhibits smooth convergence behavior, and maintains
stability in long-term forecasts. These findings affirm the viability of
UDE-based SciML approaches for battery modeling in decentralized energy
networks and suggest broader implications for real-time control and
optimization in renewable-integrated smart grids.

</details>


### [120] [From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium](https://arxiv.org/abs/2506.08292)
*Xie Yi,Zhanke Zhou,Chentao Cao,Qiyu Niu,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: 本文提出ECON方法，将多LLM协调问题转化为不完全信息博弈，证明其有更紧的遗憾边界，在六个基准测试中平均优于现有方法11.2%，还展示了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 多智能体框架提升大语言模型推理能力时存在计算成本高和缺乏收敛保证的问题。

Method: 将多LLM协调视为不完全信息博弈，寻求贝叶斯纳什均衡，引入ECON分层强化学习范式，结合分布式推理和集中式最终输出。

Result: 数学上证明ECON比非均衡多智能体方案有更紧的遗憾边界；在六个基准测试中平均比现有多LLM方法高11.2%；实验展示了其可扩展性。

Conclusion: ECON方法有效且可扩展，为构建更大、更强的多LLM集成铺平道路。

Abstract: Multi-agent frameworks can substantially boost the reasoning power of large
language models (LLMs), but they typically incur heavy computational costs and
lack convergence guarantees. To overcome these challenges, we recast multi-LLM
coordination as an incomplete-information game and seek a Bayesian Nash
equilibrium (BNE), in which each agent optimally responds to its probabilistic
beliefs about the strategies of others. We introduce Efficient Coordination via
Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that
marries distributed reasoning with centralized final output. Under ECON, each
LLM independently selects responses that maximize its expected reward,
conditioned on its beliefs about co-agents, without requiring costly
inter-agent exchanges. We mathematically prove that ECON attains a markedly
tighter regret bound than non-equilibrium multi-agent schemes. Empirically,
ECON outperforms existing multi-LLM approaches by 11.2% on average across six
benchmarks spanning complex reasoning and planning tasks. Further experiments
demonstrate ECON's ability to flexibly incorporate additional models,
confirming its scalability and paving the way toward larger, more powerful
multi-LLM ensembles. The code is publicly available at:
https://github.com/tmlr-group/ECON.

</details>


### [121] [From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?](https://arxiv.org/abs/2506.08295)
*Zhanke Zhou,Xiao Feng,Zhaocheng Zhu,Jiangchao Yao,Sanmi Koyejo,Bo Han*

Main category: cs.LG

TL;DR: 提出新基准AR - Bench评估大语言模型主动推理能力，发现当代大语言模型主动推理有困难，需改进方法。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估大语言模型被动推理能力，主动推理缺乏系统研究，需新基准评估主动推理能力。

Method: 提出AR - Bench基准，包含侦探案例、情景谜题和猜数字三个任务族，模拟现实场景。

Result: 当代大语言模型主动推理困难，现有策略提升效果有限。

Conclusion: 迫切需要推进主动推理方法，如结合交互式学习等。

Abstract: While existing benchmarks probe the reasoning abilities of large language
models (LLMs) across diverse domains, they predominantly assess passive
reasoning, providing models with all the information needed to reach a
solution. By contrast, active reasoning-where an LLM must interact with
external systems to acquire missing evidence or data-has received little
systematic attention. To address this shortfall, we present AR-Bench, a novel
benchmark designed explicitly to evaluate an LLM's active reasoning skills.
AR-Bench comprises three task families-detective cases, situation puzzles, and
guessing numbers-that together simulate real-world, agentic scenarios and
measure performance across commonsense, logical, and symbolic reasoning
challenges. Empirical evaluation on AR-Bench demonstrates that contemporary
LLMs exhibit pronounced difficulties with active reasoning: they frequently
fail to acquire or leverage the information needed to solve tasks. This gap
highlights a stark divergence between their passive and active reasoning
abilities. Moreover, ablation studies indicate that even advanced strategies,
such as tree-based searching or post-training approaches, yield only modest
gains and fall short of the levels required for real-world deployment.
Collectively, these findings highlight the critical need to advance methodology
for active reasoning, e.g., incorporating interactive learning, real-time
feedback loops, and environment-aware objectives for training. The benchmark is
publicly available at: https://github.com/tmlr-group/AR-Bench.

</details>


### [122] [H$^2$GFM: Towards unifying Homogeneity and Heterogeneity on Text-Attributed Graphs](https://arxiv.org/abs/2506.08298)
*Trung-Kien Nguyen,Heng Ping,Shixuan Li,Peiyu Zhang,Nikos Kanakaris,Nicholas Kotov,Paul Bogdan*

Main category: cs.LG

TL;DR: 本文提出H²GFM框架提升图基础模型（GFM）能力，可在同质和异质文本属性图上泛化，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注同质文本属性图（HoTAGs），而异质文本属性图（HeTAGs）未得到充分探索，为增强GFM的能力和应用，提出新框架。

Method: 引入H²GFM框架，将图间元关系投影到统一文本空间，采用上下文编码，提出上下文自适应图变换器（CGT），使用CGT专家混合模型。

Result: 在多种HoTAGs、HeTAGs和学习场景上的综合实验证明了模型的有效性。

Conclusion: 提出的H²GFM框架能有效解决图的异质性问题，在不同类型图和任务上具有良好的泛化能力。

Abstract: The growing interests and applications of graph learning in diverse domains
have propelled the development of a unified model generalizing well across
different graphs and tasks, known as the Graph Foundation Model (GFM). Existing
research has leveraged text-attributed graphs (TAGs) to tackle the
heterogeneity in node features among graphs. However, they primarily focus on
homogeneous TAGs (HoTAGs), leaving heterogeneous TAGs (HeTAGs), where multiple
types of nodes/edges reside, underexplored. To enhance the capabilities and
applications of GFM, we introduce H$^2$GFM, a novel framework designed to
generalize across both HoTAGs and HeTAGs. Our model projects diverse
meta-relations among graphs under a unified textual space, and employs a
context encoding to capture spatial and higher-order semantic relationships. To
achieve robust node representations, we propose a novel context-adaptive graph
transformer (CGT), effectively capturing information from both context
neighbors and their relationships. Furthermore, we employ a mixture of CGT
experts to capture the heterogeneity in structural patterns among graph types.
Comprehensive experiments on a wide range of HoTAGs and HeTAGs as well as
learning scenarios demonstrate the effectiveness of our model.

</details>


### [123] [Learnable Spatial-Temporal Positional Encoding for Link Prediction](https://arxiv.org/abs/2506.08309)
*Katherine Tieu,Dongqi Fu,Zihao Li,Ross Maciejewski,Jingrui He*

Main category: cs.LG

TL;DR: 论文指出当前位置编码的局限，提出有效高效的可学习时空位置编码模型L - STEP，并在多个数据集和算法上验证其性能。


<details>
  <summary>Details</summary>
Motivation: 当前位置编码存在适应复杂图、考虑时空信息及处理大规模数据能力不足的问题，需开发有效高效的可学习时空位置编码。

Method: 提出L - STEP模型，从时空谱角度证明位置学习方案能保留图属性，用MLPs挖掘编码表现力，改变初始输入验证鲁棒性，分析复杂度并对比运行时间。

Result: 在13个经典数据集和10个算法上，以3种采样策略在归纳和直推设置中展现出时间链接预测的优异性能，在TGB基准测试中领先。

Conclusion: L - STEP模型在时空位置编码和时间链接预测任务中表现出色，代码开源。

Abstract: Accurate predictions rely on the expressiveness power of graph deep learning
frameworks like graph neural networks and graph transformers, where a
positional encoding mechanism has become much more indispensable in recent
state-of-the-art works to record the canonical position information. However,
the current positional encoding is limited in three aspects: (1) most
positional encoding methods use pre-defined, and fixed functions, which are
inadequate to adapt to the complex attributed graphs; (2) a few pioneering
works proposed the learnable positional encoding but are still limited to the
structural information, not considering the real-world time-evolving
topological and feature information; (3) most positional encoding methods are
equipped with transformers' attention mechanism to fully leverage their
capabilities, where the dense or relational attention is often unaffordable on
large-scale structured data. Hence, we aim to develop Learnable
Spatial-Temporal Positional Encoding in an effective and efficient manner and
propose a simple temporal link prediction model named L-STEP. Briefly, for
L-STEP, we (1) prove the proposed positional learning scheme can preserve the
graph property from the spatial-temporal spectral viewpoint, (2) verify that
MLPs can fully exploit the expressiveness and reach transformers' performance
on that encoding, (3) change different initial positional encoding inputs to
show robustness, (4) analyze the theoretical complexity and obtain less
empirical running time than SOTA, and (5) demonstrate its temporal link
prediction out-performance on 13 classic datasets and with 10 algorithms in
both transductive and inductive settings using 3 different sampling strategies.
Also, \name\ obtains the leading performance in the newest large-scale TGB
benchmark. Our code is available at https://github.com/kthrn22/L-STEP.

</details>


### [124] [Graph Prompting for Graph Learning Models: Recent Advances and Future Directions](https://arxiv.org/abs/2506.08326)
*Xingbo Fu,Zehong Wang,Zihan Chen,Jiazheng Li,Yaochen Zhu,Zhenyu Lei,Cong Shen,Yanfang Ye,Chuxu Zhang,Jundong Li*

Main category: cs.LG

TL;DR: 本文对图提示（graph prompting）的最新进展进行了系统综述，涵盖图预训练方法、主流技术、实际应用、开放挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 图学习模型在‘预训练 - 适应’方案中，图提示是有前景的方法，本文旨在对其最新进展进行系统回顾。

Method: 先介绍作为图提示基础的代表性图预训练方法，再回顾图提示主流技术并阐述其如何设计可学习提示，接着总结不同领域的实际应用，最后讨论现有研究的开放挑战和未来方向。

Result: 梳理了图提示的相关进展和应用。

Conclusion: 指出图提示研究领域存在一些开放挑战，也给出了有前景的未来方向。

Abstract: Graph learning models have demonstrated great prowess in learning expressive
representations from large-scale graph data in a wide variety of real-world
scenarios. As a prevalent strategy for training powerful graph learning models,
the "pre-training, adaptation" scheme first pre-trains graph learning models on
unlabeled graph data in a self-supervised manner and then adapts them to
specific downstream tasks. During the adaptation phase, graph prompting emerges
as a promising approach that learns trainable prompts while keeping the
pre-trained graph learning models unchanged. In this paper, we present a
systematic review of recent advancements in graph prompting. First, we
introduce representative graph pre-training methods that serve as the
foundation step of graph prompting. Next, we review mainstream techniques in
graph prompting and elaborate on how they design learnable prompts for graph
prompting. Furthermore, we summarize the real-world applications of graph
prompting from different domains. Finally, we discuss several open challenges
in existing studies with promising future directions in this field.

</details>


### [125] [Dynamical System Optimization](https://arxiv.org/abs/2506.08340)
*Emo Todorov*

Main category: cs.LG

TL;DR: 开发围绕策略的优化框架，推导算法，表明可计算与现有方法相同量，算法适用多场景，也适用于生成式AI模型调整。


<details>
  <summary>Details</summary>
Motivation: 摆脱对控制、动作的依赖及近似动态规划和强化学习机制，优化策略参数。

Method: 以指定策略后形成自主动力系统为核心思想，在自主系统层面推导更简单算法。

Result: 算法能计算与策略梯度、Hessians、自然梯度、近端方法相同的量，有近似策略迭代和离策略学习的类似形式。

Conclusion: 该框架统一处理策略和系统参数，算法适用于多种场景，生成式AI模型调整更适合此框架。

Abstract: We develop an optimization framework centered around a core idea: once a
(parametric) policy is specified, control authority is transferred to the
policy, resulting in an autonomous dynamical system. Thus we should be able to
optimize policy parameters without further reference to controls or actions,
and without directly using the machinery of approximate Dynamic Programming and
Reinforcement Learning. Here we derive simpler algorithms at the autonomous
system level, and show that they compute the same quantities as policy
gradients and Hessians, natural gradients, proximal methods. Analogs to
approximate policy iteration and off-policy learning are also available. Since
policy parameters and other system parameters are treated uniformly, the same
algorithms apply to behavioral cloning, mechanism design, system
identification, learning of state estimators. Tuning of generative AI models is
not only possible, but is conceptually closer to the present framework than to
Reinforcement Learning.

</details>


### [126] [Differentially Private Relational Learning with Entity-level Privacy Guarantees](https://arxiv.org/abs/2506.08347)
*Yinan Huang,Haoteng Ying,Eli Chien,Rongzhe Wei,Pan Li*

Main category: cs.LG

TL;DR: 本文提出关系学习的实体级差分隐私框架，实验显示其在效用 - 隐私权衡方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 直接将 DP - SGD 应用于关系学习存在实体敏感度高难控制和标准隐私放大分析不适用的问题，需设计有实体级差分隐私保证的框架。

Method: 进行严格敏感度分析，引入基于实体出现频率调整裁剪阈值的自适应梯度裁剪方案，将隐私放大结果扩展到特定耦合采样子类。

Result: 得到适用于关系数据的 DP - SGD 变体，实验证明该方法在效用 - 隐私权衡方面表现良好。

Conclusion: 提出的框架能有效解决关系学习中的隐私保护问题，实现较好的效用 - 隐私权衡。

Abstract: Learning with relational and network-structured data is increasingly vital in
sensitive domains where protecting the privacy of individual entities is
paramount. Differential Privacy (DP) offers a principled approach for
quantifying privacy risks, with DP-SGD emerging as a standard mechanism for
private model training. However, directly applying DP-SGD to relational
learning is challenging due to two key factors: (i) entities often participate
in multiple relations, resulting in high and difficult-to-control sensitivity;
and (ii) relational learning typically involves multi-stage, potentially
coupled (interdependent) sampling procedures that make standard privacy
amplification analyses inapplicable. This work presents a principled framework
for relational learning with formal entity-level DP guarantees. We provide a
rigorous sensitivity analysis and introduce an adaptive gradient clipping
scheme that modulates clipping thresholds based on entity occurrence frequency.
We also extend the privacy amplification results to a tractable subclass of
coupled sampling, where the dependence arises only through sample sizes. These
contributions lead to a tailored DP-SGD variant for relational data with
provable privacy guarantees. Experiments on fine-tuning text encoders over
text-attributed network-structured relational data demonstrate the strong
utility-privacy trade-offs of our approach. Our code is available at
https://github.com/Graph-COM/Node_DP.

</details>


### [127] [An Adaptive Method Stabilizing Activations for Enhanced Generalization](https://arxiv.org/abs/2506.08353)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: 介绍了AdaAct优化算法，能根据激活方差调整学习率，实验显示其在图像分类基准上有竞争力。


<details>
  <summary>Details</summary>
Motivation: 提升神经元输出稳定性和模型泛化能力，补充传统激活正则化方法。

Method: 提出AdaAct算法，在训练过程中融入神经元级自适应调整学习率。

Result: 在CIFAR和ImageNet上与其他先进方法对比，AdaAct表现有竞争力，且兼顾Adam收敛速度和SGD泛化能力，执行时间也有竞争力。

Conclusion: AdaAct是一种有效的优化算法，代码可在https://github.com/hseung88/adaact获取。

Abstract: We introduce AdaAct, a novel optimization algorithm that adjusts learning
rates according to activation variance. Our method enhances the stability of
neuron outputs by incorporating neuron-wise adaptivity during the training
process, which subsequently leads to better generalization -- a complementary
approach to conventional activation regularization methods. Experimental
results demonstrate AdaAct's competitive performance across standard image
classification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing
it with other state-of-the-art methods. Importantly, AdaAct effectively bridges
the gap between the convergence speed of Adam and the strong generalization
capabilities of SGD, all while maintaining competitive execution times. Code is
available at https://github.com/hseung88/adaact.

</details>


### [128] [NysAct: A Scalable Preconditioned Gradient Descent using Nystrom Approximation](https://arxiv.org/abs/2506.08360)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: 介绍NysAct方法，平衡一阶和二阶优化方法，减少复杂度且提升测试精度，代码开源。


<details>
  <summary>Details</summary>
Motivation: 自适应梯度方法泛化性差，二阶方法计算和内存成本高，需平衡二者。

Method: 利用特征值偏移的Nystrom方法近似激活协方差矩阵作为预条件矩阵。

Result: NysAct比一阶和二阶方法测试精度更高，比现有二阶方法计算资源需求少。

Conclusion: NysAct能在一阶和二阶优化方法间取得平衡，有较好性能。

Abstract: Adaptive gradient methods are computationally efficient and converge quickly,
but they often suffer from poor generalization. In contrast, second-order
methods enhance convergence and generalization but typically incur high
computational and memory costs. In this work, we introduce NysAct, a scalable
first-order gradient preconditioning method that strikes a balance between
state-of-the-art first-order and second-order optimization methods. NysAct
leverages an eigenvalue-shifted Nystrom method to approximate the activation
covariance matrix, which is used as a preconditioning matrix, significantly
reducing time and memory complexities with minimal impact on test accuracy. Our
experiments show that NysAct not only achieves improved test accuracy compared
to both first-order and second-order methods but also demands considerably less
computational resources than existing second-order methods. Code is available
at https://github.com/hseung88/nysact.

</details>


### [129] [AlphaFold Database Debiasing for Robust Inverse Folding](https://arxiv.org/abs/2506.08365)
*Cheng Tan,Zhenxiao Cao,Zhangyang Gao,Siyuan Li,Yufei Huang,Stan Z. Li*

Main category: cs.LG

TL;DR: AFDB有结构覆盖优势但有几何偏差，提出DeSAE去偏，提升逆折叠性能。


<details>
  <summary>Details</summary>
Motivation: AFDB直接用于训练对原子几何敏感的深度模型有局限，其结构存在与PDB结构不同的统计规律和几何偏差。

Method: 引入Debiasing Structure AutoEncoder (DeSAE)，从故意损坏的骨干几何中重建类天然构象。

Result: 将DeSAE应用于AFDB结构产生去偏结构，在多个基准测试中显著提高逆折叠性能。

Conclusion: 强调预测结构中细微系统偏差的关键影响，提出去偏的原则框架，提升基于结构学习任务的性能。

Abstract: The AlphaFold Protein Structure Database (AFDB) offers unparalleled
structural coverage at near-experimental accuracy, positioning it as a valuable
resource for data-driven protein design. However, its direct use in training
deep models that are sensitive to fine-grained atomic geometry, such as inverse
folding, exposes a critical limitation. Comparative analysis of structural
feature distributions reveals that AFDB structures exhibit distinct statistical
regularities, reflecting a systematic geometric bias that deviates from the
conformational diversity found in experimentally determined structures from the
Protein Data Bank (PDB). While AFDB structures are cleaner and more idealized,
PDB structures capture the intrinsic variability and physical realism essential
for generalization in downstream tasks. To address this discrepancy, we
introduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct
native-like conformations from intentionally corrupted backbone geometries. By
training the model to recover plausible structural states, DeSAE implicitly
captures a more robust and natural structural manifold. At inference, applying
DeSAE to AFDB structures produces debiased structures that significantly
improve inverse folding performance across multiple benchmarks. This work
highlights the critical impact of subtle systematic biases in predicted
structures and presents a principled framework for debiasing, significantly
boosting the performance of structure-based learning tasks like inverse
folding.

</details>


### [130] [Reinforce LLM Reasoning through Multi-Agent Reflection](https://arxiv.org/abs/2506.08379)
*Yurun Yuan,Tengyang Xie*

Main category: cs.LG

TL;DR: 文章提出DPSDP算法优化大语言模型推理能力，理论上可匹配训练分布内策略性能，实证有提升。


<details>
  <summary>Details</summary>
Motivation: 现有验证改进范式存在反馈空间受限和各方缺乏协同训练问题，导致性能不佳。

Method: 将多轮细化过程建模为马尔可夫决策过程，引入DPSDP强化学习算法，通过对自生成数据的直接偏好学习训练演员-评论家大语言模型系统。

Result: 在多个基准测试中有改进，如在MATH 500上，Ministral模型经五步细化后准确率从58.2%提升到63.2%，消融研究证实多智能体协作和分布外泛化优势。

Conclusion: DPSDP算法有效提升大语言模型推理能力。

Abstract: Leveraging more test-time computation has proven to be an effective way to
boost the reasoning capabilities of large language models (LLMs). Among various
methods, the verify-and-improve paradigm stands out for enabling dynamic
solution exploration and feedback incorporation. However, existing approaches
often suffer from restricted feedback spaces and lack of coordinated training
of different parties, leading to suboptimal performance. To address this, we
model this multi-turn refinement process as a Markov Decision Process and
introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement
learning algorithm that trains an actor-critic LLM system to iteratively refine
answers via direct preference learning on self-generated data. Theoretically,
DPSDP can match the performance of any policy within the training distribution.
Empirically, we instantiate DPSDP with various base models and show
improvements on both in- and out-of-distribution benchmarks. For example, on
benchmark MATH 500, majority voting over five refinement steps increases
first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An
ablation study further confirms the benefits of multi-agent collaboration and
out-of-distribution generalization.

</details>


### [131] [Network Threat Detection: Addressing Class Imbalanced Data with Deep Forest](https://arxiv.org/abs/2506.08383)
*Jiaqi Chen,Rongbin Ye*

Main category: cs.LG

TL;DR: 研究利用IoT - 23数据集对机器学习恶意软件检测技术进行分析，处理数据类别不平衡问题，发现合适处理方法与集成方法结合效果更好，助力物联网威胁检测系统发展。


<details>
  <summary>Details</summary>
Motivation: 随着物联网网络快速扩张，实时检测恶意流量成为关键网络安全挑战，需对机器学习恶意软件检测技术进行研究。

Method: 使用Stratosphere实验室提供的IoT - 23数据集，通过三种重采样策略处理数据类别不平衡问题，实现并比较几种机器学习技术。

Result: 合适的不平衡处理技术与集成方法（尤其是gcForest）结合，比传统方法有更好的检测性能。

Conclusion: 本研究对物联网环境下更智能高效的自动化威胁检测系统发展有重要贡献，可保护关键基础设施并优化计算资源使用。

Abstract: With the rapid expansion of Internet of Things (IoT) networks, detecting
malicious traffic in real-time has become a critical cybersecurity challenge.
This research addresses the detection challenges by presenting a comprehensive
empirical analysis of machine learning techniques for malware detection using
the IoT-23 dataset provided by the Stratosphere Laboratory. We address the
significant class imbalance within the dataset through three resampling
strategies. We implement and compare a few machine learning techniques. Our
findings demonstrate that the combination of appropriate imbalance treatment
techniques with ensemble methods, particularly gcForest, achieves better
detection performance compared to traditional approaches. This work contributes
significantly to the development of more intelligent and efficient automated
threat detection systems for IoT environments, helping to secure critical
infrastructure against sophisticated cyber attacks while optimizing
computational resource usage.

</details>


### [132] [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/abs/2506.08388)
*Edoardo Cetin,Tianyu Zhao,Yujin Tang*

Main category: cs.LG

TL;DR: 提出强化学习教师（RLTs）框架，避免探索挑战，在竞赛和研究生级任务上表现好，有高复用性。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习训练推理语言模型的探索挑战，满足推理语言模型作为教师进行蒸馏和冷启动的需求。

Method: 训练RLTs，用问题和解决方案提示，用详细解释“连接点”，用学生对解释的理解获得密集奖励进行训练。

Result: 7B RLT原始输出在竞赛和研究生级任务上比现有蒸馏和冷启动管道表现更好，训练大模型和零样本应用时也有效。

Conclusion: RLTs框架为强化学习推理框架带来新的效率和复用性。

Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for
one-hot correctness inherently relies on the LM being able to explore and solve
its task with some chance at initialization. Furthermore, a key use case of
reasoning LMs is to act as teachers for distilling new students and
cold-starting future RL iterations rather than being deployed themselves. From
these considerations, we introduce a new framework that avoids RL's exploration
challenge by training a new class of Reinforcement-Learned Teachers (RLTs)
focused on yielding the most effective downstream distillation. RLTs are
prompted with both the question and solution to each problem, and tasked to
simply "connect-the-dots" with detailed explanations tailored for their
students. We train RLTs with dense rewards obtained by feeding each explanation
to the student and testing its understanding of the problem's solution. In
practice, the raw outputs of a 7B RLT provide higher final performance on
competition and graduate-level tasks than existing distillation and
cold-starting pipelines that collect and postprocess the reasoning traces of
orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness
when training larger students and when applied zero-shot to out-of-distribution
tasks, unlocking new levels of efficiency and re-usability for the RL reasoning
framework.

</details>


### [133] [FUSE: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion](https://arxiv.org/abs/2506.08409)
*Fred Xu,Song Jiang,Zijie Huang,Xiao Luo,Shichang Zhang,Adrian Chen,Yizhou Sun*

Main category: cs.LG

TL;DR: 提出基于模糊集体积近似的集合表示学习方法FUSE，用于分类扩展任务，相比基线有显著提升，是理解和高效计算模糊集嵌入的首次尝试。


<details>
  <summary>Details</summary>
Motivation: 现有集合表示学习方法在集合运算上不封闭，而模糊集适合概念建模，需要更优方法。

Method: 提出基于模糊集体积近似的集合表示学习方法，构建Fuzzy Set Embedding (FUSE) 框架。

Result: 在分类扩展任务上相比现有基线有高达23%的显著提升。

Conclusion: 这是理解和高效计算模糊集嵌入的首次尝试，FUSE方法有效且高效。

Abstract: Taxonomy Expansion, which models complex concepts and their relations, can be
formulated as a set representation learning task. The generalization of set,
fuzzy set, incorporates uncertainty and measures the information within a
semantic concept, making it suitable for concept modeling. Existing works
usually model sets as vectors or geometric objects such as boxes, which are not
closed under set operations. In this work, we propose a sound and efficient
formulation of set representation learning based on its volume approximation as
a fuzzy set. The resulting embedding framework, Fuzzy Set Embedding (FUSE),
satisfies all set operations and compactly approximates the underlying fuzzy
set, hence preserving information while being efficient to learn, relying on
minimum neural architecture. We empirically demonstrate the power of FUSE on
the task of taxonomy expansion, where FUSE achieves remarkable improvements up
to 23% compared with existing baselines. Our work marks the first attempt to
understand and efficiently compute the embeddings of fuzzy sets.

</details>


### [134] [Learning to Hear Broken Motors: Signature-Guided Data Augmentation for Induction-Motor Diagnostics](https://arxiv.org/abs/2506.08412)
*Saraa Ali,Aleksandr Khizhik,Stepan Svirin,Artem Ryzhikov,Denis Derkach*

Main category: cs.LG

TL;DR: 本文提出结合机器学习算法与考虑发动机物理模型的无监督异常生成方法，即SGDA框架，能提高三相发动机诊断准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统三相发动机智能诊断依赖特征分析，结合先进机器学习技术可提升诊断性能与准确性。

Method: 结合机器学习算法与考虑发动机物理模型的无监督异常生成方法，提出Signature - Guided Data Augmentation (SGDA) 无监督框架。

Result: 混合方法结合监督式机器学习和无监督特征分析的优势，实现了卓越的诊断准确性和可靠性，具有广泛工业应用前景。

Conclusion: 该方法能为发动机诊断领域做出重要贡献，为实际应用提供强大高效的解决方案。

Abstract: The application of machine learning (ML) algorithms in the intelligent
diagnosis of three-phase engines has the potential to significantly enhance
diagnostic performance and accuracy. Traditional methods largely rely on
signature analysis, which, despite being a standard practice, can benefit from
the integration of advanced ML techniques. In our study, we innovate by
combining ML algorithms with a novel unsupervised anomaly generation
methodology that takes into account the engine physics model. We propose
Signature-Guided Data Augmentation (SGDA), an unsupervised framework that
synthesizes physically plausible faults directly in the frequency domain of
healthy current signals. Guided by Motor Current Signature Analysis, SGDA
creates diverse and realistic anomalies without resorting to computationally
intensive simulations. This hybrid approach leverages the strengths of both
supervised ML and unsupervised signature analysis, achieving superior
diagnostic accuracy and reliability along with wide industrial application. The
findings highlight the potential of our approach to contribute significantly to
the field of engine diagnostics, offering a robust and efficient solution for
real-world applications.

</details>


### [135] [Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood](https://arxiv.org/abs/2506.08417)
*Qingmao Yao,Zhichao Lei,Tianyuan Chen,Ziyue Yuan,Xuefan Chen,Jianxiang Liu,Faguo Wu,Xiao Zhang*

Main category: cs.LG

TL;DR: 提出新方法 SQOG 增强 OOD 区域 Q 函数泛化，缓解过约束问题，在 D4RL 基准测试中表现优。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法处理分布偏移时评估 OOD 区域过于保守，导致 Q 函数泛化受限和 Q 值估计不佳。

Method: 在凸包及其邻域（CHN）内增强 Q 函数泛化，提出平滑贝尔曼算子（SBO）更新 OOD Q 值，基于此设计 Smooth Q - function OOD Generalization（SQOG）算法。

Result: 理论上 SBO 能近似 CHN 内样本内和 OOD 动作的真实 Q 值，实践中 SQOG 缓解过约束问题，实现接近准确的 Q 值估计。

Conclusion: SQOG 在 D4RL 基准测试中性能和计算效率优于现有最先进方法。

Abstract: Offline Reinforcement Learning (RL) struggles with distributional shifts,
leading to the $Q$-value overestimation for out-of-distribution (OOD) actions.
Existing methods address this issue by imposing constraints; however, they
often become overly conservative when evaluating OOD regions, which constrains
the $Q$-function generalization. This over-constraint issue results in poor
$Q$-value estimation and hinders policy improvement. In this paper, we
introduce a novel approach to achieve better $Q$-value estimation by enhancing
$Q$-function generalization in OOD regions within Convex Hull and its
Neighborhood (CHN). Under the safety generalization guarantees of the CHN, we
propose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by
smoothing them with neighboring in-sample $Q$-values. We theoretically show
that SBO approximates true $Q$-values for both in-sample and OOD actions within
the CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG),
empirically alleviates the over-constraint issue, achieving near-accurate
$Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing
state-of-the-art methods in both performance and computational efficiency.

</details>


### [136] [Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings](https://arxiv.org/abs/2506.08435)
*Mingyuan Fan,Fuyi Wang,Cen Chen,Jianying Zhou*

Main category: cs.LG

TL;DR: 本文通过实验证明在现实联邦学习环境中客户数据仍可被有效重构，提出FedLeak方法并制定评估协议，凸显联邦学习系统隐私漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有研究对联邦学习中梯度泄露攻击的隐私风险存在争议，需明确现实环境下客户数据是否能被有效重构。

Method: 提出FedLeak方法，引入部分梯度匹配和梯度正则化两种新技术，制定基于大量文献和行业实践的实用评估协议。

Result: 在评估协议下，FedLeak能实现高保真度的数据重构。

Conclusion: 联邦学习系统存在显著隐私漏洞，急需更有效的防御方法。

Abstract: Federated learning (FL) enables collaborative model training among multiple
clients without the need to expose raw data. Its ability to safeguard privacy,
at the heart of FL, has recently been a hot-button debate topic. To elaborate,
several studies have introduced a type of attacks known as gradient leakage
attacks (GLAs), which exploit the gradients shared during training to
reconstruct clients' raw data. On the flip side, some literature, however,
contends no substantial privacy risk in practical FL environments due to the
effectiveness of such GLAs being limited to overly relaxed conditions, such as
small batch sizes and knowledge of clients' data distributions.
  This paper bridges this critical gap by empirically demonstrating that
clients' data can still be effectively reconstructed, even within realistic FL
environments. Upon revisiting GLAs, we recognize that their performance
failures stem from their inability to handle the gradient matching problem. To
alleviate the performance bottlenecks identified above, we develop FedLeak,
which introduces two novel techniques, partial gradient matching and gradient
regularization. Moreover, to evaluate the performance of FedLeak in real-world
FL environments, we formulate a practical evaluation protocol grounded in a
thorough review of extensive FL literature and industry practices. Under this
protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby
underscoring the significant vulnerability in FL systems and the urgent need
for more effective defense methods.

</details>


### [137] [Time-Aware World Model for Adaptive Prediction and Control](https://arxiv.org/abs/2506.08441)
*Anh N. Nhu,Sanghyun Son,Ming Lin*

Main category: cs.LG

TL;DR: 本文介绍了时间感知世界模型TAWM，它显式纳入时间动态，在不同控制任务中表现优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理控制问题时未充分考虑时间动态，基于最优采样率依赖系统底层动态的信息论见解，旨在提高性能和数据效率。

Method: 引入TAWM，通过对时间步长Δt进行条件设定，并在不同的Δt值上训练，学习不同控制问题的高低频任务动态。

Result: 经验评估表明，在相同训练样本和迭代次数下，TAWM在各种控制任务的不同观测率下始终优于传统模型。

Conclusion: TAWM这种显式纳入时间动态的基于模型的方法是有效的，能提升控制任务的性能和数据效率。

Abstract: In this work, we introduce the Time-Aware World Model (TAWM), a model-based
approach that explicitly incorporates temporal dynamics. By conditioning on the
time-step size, {\Delta}t, and training over a diverse range of {\Delta}t
values -- rather than sampling at a fixed time-step -- TAWM learns both high-
and low-frequency task dynamics across diverse control problems. Grounded in
the information-theoretic insight that the optimal sampling rate depends on a
system's underlying dynamics, this time-aware formulation improves both
performance and data efficiency. Empirical evaluations show that TAWM
consistently outperforms conventional models across varying observation rates
in a variety of control tasks, using the same number of training samples and
iterations. Our code can be found online at:
github.com/anh-nn01/Time-Aware-World-Model.

</details>


### [138] [MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning](https://arxiv.org/abs/2506.08460)
*Yihong Guo,Yu Yang,Pan Xu,Anqi Liu*

Main category: cs.LG

TL;DR: 提出基于模型的离线强化学习算法MOBODY，通过学习动力学探索目标域，在MuJoCo基准测试中显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有非动态离线强化学习方法受目标域有限转移的限制，学习的策略无法超越离线数据集探索目标域。

Method: 提出MOBODY算法，通过模型滚动生成目标域的新合成转移用于数据增强，利用源和目标数据集学习目标动力学，引入Q加权行为克隆损失稳定训练。

Result: 在MuJoCo基准测试中，MOBODY显著优于现有基线，在具有挑战性的场景中改进尤其明显。

Conclusion: MOBODY算法有效解决了现有方法的局限性，在离线强化学习中表现出色。

Abstract: We study the off-dynamics offline reinforcement learning problem, where the
goal is to learn a policy from offline datasets collected from source and
target domains with mismatched transition. Existing off-dynamics offline RL
methods typically either filter source transitions that resemble those of the
target domain or apply reward augmentation to source data, both constrained by
the limited transitions available from the target domain. As a result, the
learned policy is unable to explore target domain beyond the offline datasets.
We propose MOBODY, a Model-Based Off-Dynamics offline RL algorithm that
addresses this limitation by enabling exploration of the target domain via
learned dynamics. MOBODY generates new synthetic transitions in the target
domain through model rollouts, which are used as data augmentation during
offline policy learning. Unlike existing model-based methods that learn
dynamics from a single domain, MOBODY tackles the challenge of mismatched
dynamics by leveraging both source and target datasets. Directly merging these
datasets can bias the learned model toward source dynamics. Instead, MOBODY
learns target dynamics by discovering a shared latent representation of states
and transitions across domains through representation learning. To stabilize
training, MOBODY incorporates a behavior cloning loss that regularizes the
policy. Specifically, we introduce a Q-weighted behavior cloning loss that
regularizes the policy toward actions with high target-domain Q-values, rather
than uniformly imitating all actions in the dataset. These Q-values are learned
from an enhanced target dataset composed of offline target data, augmented
source data, and rollout data from the learned target dynamics. We evaluate
MOBODY on MuJoCo benchmarks and show that it significantly outperforms
state-of-the-art baselines, with especially pronounced improvements in
challenging scenarios.

</details>


### [139] [How to Provably Improve Return Conditioned Supervised Learning?](https://arxiv.org/abs/2506.08463)
*Zhishuai Liu,Yu Yang,Ruhan Wang,Pan Xu,Dongruo Zhou*

Main category: cs.LG

TL;DR: 提出Reinforced RCSL框架解决RCSL缺乏拼接属性的局限，理论和实验证明其优于标准RCSL。


<details>
  <summary>Details</summary>
Motivation: RCSL虽有简单稳定的优点，但缺乏拼接属性，性能受生成离线数据集的策略质量限制。

Method: 提出Reinforced RCSL框架，引入分布内最优未来回报概念，利用策略根据当前状态确定数据集中可实现的最佳未来回报。

Result: 理论分析表明Reinforced RCSL能持续优于标准RCSL，实证结果显示在一系列基准测试中有显著性能提升。

Conclusion: Reinforced RCSL框架有效解决了RCSL的局限性，表现优于标准RCSL。

Abstract: In sequential decision-making problems, Return-Conditioned Supervised
Learning (RCSL) has gained increasing recognition for its simplicity and
stability in modern decision-making tasks. Unlike traditional offline
reinforcement learning (RL) algorithms, RCSL frames policy learning as a
supervised learning problem by taking both the state and return as input. This
approach eliminates the instability often associated with temporal difference
(TD) learning in offline RL. However, RCSL has been criticized for lacking the
stitching property, meaning its performance is inherently limited by the
quality of the policy used to generate the offline dataset. To address this
limitation, we propose a principled and simple framework called Reinforced
RCSL. The key innovation of our framework is the introduction of a concept we
call the in-distribution optimal return-to-go. This mechanism leverages our
policy to identify the best achievable in-dataset future return based on the
current state, avoiding the need for complex return augmentation techniques.
Our theoretical analysis demonstrates that Reinforced RCSL can consistently
outperform the standard RCSL approach. Empirical results further validate our
claims, showing significant performance improvements across a range of
benchmarks.

</details>


### [140] [MAC: An Efficient Gradient Preconditioning using Mean Activation Approximated Curvature](https://arxiv.org/abs/2506.08464)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: 本文提出计算高效的优化方法MAC，分析层级Fisher信息矩阵的组成部分并提出近似方法，证明其收敛性，实验显示MAC在多方面优于KFAC等方法。


<details>
  <summary>Details</summary>
Motivation: 二阶优化方法（如KFAC）在训练神经网络时虽收敛性好，但计算负担高，需改进。

Method: 分析构成层级Fisher信息矩阵的两个组成部分，根据其特征谱提出高效近似方法，得到MAC算法，并研究其在非线性神经网络上的收敛性。

Result: 在各种网络架构和数据集上的广泛评估表明，MAC在准确率、端到端训练时间和内存使用方面优于KFAC和其他最先进的方法。

Conclusion: 提出的MAC方法是计算高效的优化方法，能有效解决二阶优化方法计算负担高的问题。

Abstract: Second-order optimization methods for training neural networks, such as KFAC,
exhibit superior convergence by utilizing curvature information of loss
landscape. However, it comes at the expense of high computational burden. In
this work, we analyze the two components that constitute the layer-wise Fisher
information matrix (FIM) used in KFAC: the Kronecker factors related to
activations and pre-activation gradients. Based on empirical observations on
their eigenspectra, we propose efficient approximations for them, resulting in
a computationally efficient optimization method called MAC. To the best of our
knowledge, MAC is the first algorithm to apply the Kronecker factorization to
the FIM of attention layers used in transformers and explicitly integrate
attention scores into the preconditioning. We also study the convergence
property of MAC on nonlinear neural networks and provide two conditions under
which it converges to global minima. Our extensive evaluations on various
network architectures and datasets show that the proposed method outperforms
KFAC and other state-of-the-art methods in terms of accuracy, end-to-end
training time, and memory usage. Code is available at
https://github.com/hseung88/mac.

</details>


### [141] [AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin](https://arxiv.org/abs/2506.08473)
*Shuo Yang,Qihui Zhang,Yuyang Liu,Yue Huang,Xiaojun Jia,Kunpeng Ning,Jiayu Yao,Jigang Wang,Hailiang Dai,Yibing Song,Li Yuan*

Main category: cs.LG

TL;DR: 本文提出AsFT方法用于大语言模型安全微调，实验表明其优于Safe LoRA，能减少有害行为、提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在微调时易面临安全风险，少量恶意或无害数据可能破坏安全防护。

Method: 基于对齐方向概念，提出AsFT方法，在训练目标中集成正则化项，以对齐方向为锚抑制有害方向的更新。

Result: 在多个数据集上的大量实验显示，AsFT优于Safe LoRA，减少7.60%的有害行为，提升3.44%的模型性能，且在不同实验设置下表现稳健。

Conclusion: AsFT方法能有效提高大语言模型微调时的安全性和性能。

Abstract: Large language models (LLMs) are vulnerable to safety risks during
fine-tuning, where small amounts of malicious or harmless data can compromise
safeguards. In this paper, building on the concept of alignment direction --
defined by the weight difference between aligned and unaligned models -- we
observe that perturbations along this direction preserve model safety. In
contrast, perturbations along directions orthogonal to this alignment are
strongly linked to harmful direction perturbations, rapidly degrading safety
and framing the parameter space as a narrow safety basin. Based on this
insight, we propose a methodology for safety fine-tuning called AsFT (Anchoring
Safety in Fine-Tuning), which integrates a regularization term into the
training objective. This term uses the alignment direction as an anchor to
suppress updates in harmful directions, ensuring that fine-tuning is
constrained within the narrow safety basin. Extensive experiments on multiple
datasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by
7.60 percent, improving model performance by 3.44 percent, and maintaining
robust performance across various experimental settings. Code is available at
https://github.com/PKU-YuanGroup/AsFT

</details>


### [142] [Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations](https://arxiv.org/abs/2506.08505)
*Shahaf Bassan,Yizhak Yisrael Elboher,Tobias Ladner,Matthias Althoff,Guy Katz*

Main category: cs.LG

TL;DR: 本文提出一种抽象 - 细化技术来高效计算神经网络预测的可证明充分解释，实验表明该方法提高了获取解释的效率并提供细粒度解释。


<details>
  <summary>Details</summary>
Motivation: 现有后验可解释性技术大多依赖启发式方法，缺乏形式化保证，且基于验证技术的可解释性计算面临可扩展性挑战。

Method: 提出抽象 - 细化技术，构建简化网络加速验证过程，若解释不足则迭代细化网络直至收敛。

Result: 实验表明该方法提高了获取可证明充分解释的效率，能在不同抽象级别对网络预测进行细粒度解释。

Conclusion: 所提方法有效解决了可证明充分解释计算的可扩展性问题，提升了解释获取效率和细粒度解释能力。

Abstract: Despite significant advancements in post-hoc explainability techniques for
neural networks, many current methods rely on heuristics and do not provide
formally provable guarantees over the explanations provided. Recent work has
shown that it is possible to obtain explanations with formal guarantees by
identifying subsets of input features that are sufficient to determine that
predictions remain unchanged using neural network verification techniques.
Despite the appeal of these explanations, their computation faces significant
scalability challenges. In this work, we address this gap by proposing a novel
abstraction-refinement technique for efficiently computing provably sufficient
explanations of neural network predictions. Our method abstracts the original
large neural network by constructing a substantially reduced network, where a
sufficient explanation of the reduced network is also provably sufficient for
the original network, hence significantly speeding up the verification process.
If the explanation is in sufficient on the reduced network, we iteratively
refine the network size by gradually increasing it until convergence. Our
experiments demonstrate that our approach enhances the efficiency of obtaining
provably sufficient explanations for neural network predictions while
additionally providing a fine-grained interpretation of the network's
predictions across different abstraction levels.

</details>


### [143] [DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial Training](https://arxiv.org/abs/2506.08514)
*Jacob Piland,Chris Sweet,Adam Czakja*

Main category: cs.LG

TL;DR: 文章指出标准CAM易受对抗操纵，引入SHAMs作为基准，提出DiffGradCAM解决被动欺骗漏洞，建立新框架并验证。


<details>
  <summary>Details</summary>
Motivation: 标准CAM和基于梯度的变体关注单个logits，在使用softmax的神经网络中存在与类成员概率估计的脱节，易受对抗操纵，需要提升CAM鲁棒性。

Method: 引入Salience - Hoax Activation Maps (SHAMs)作为对抗条件下CAM鲁棒性的基准，提出DiffGradCAM这种轻量级、对比性的类激活映射方法。

Result: SHAM和DiffGradCAM建立了一个新框架用于探测和改进基于显著性的解释的鲁棒性，且在多类任务中得到验证。

Conclusion: SHAM和DiffGradCAM为提升CAM鲁棒性提供了有效方法，建立的新框架具有一定价值。

Abstract: Class Activation Mapping (CAM) and its gradient-based variants (e.g.,
GradCAM) have become standard tools for explaining Convolutional Neural Network
(CNN) predictions. However, these approaches typically focus on individual
logits, while for neural networks using softmax, the class membership
probability estimates depend \textit{only} on the \textit{differences} between
logits, not on their absolute values. This disconnect leaves standard CAMs
vulnerable to adversarial manipulation, such as passive fooling, where a model
is trained to produce misleading CAMs without affecting decision performance.
We introduce \textbf{Salience-Hoax Activation Maps (SHAMs)}, an
\emph{entropy-aware form of passive fooling} that serves as a benchmark for CAM
robustness under adversarial conditions. To address the passive fooling
vulnerability, we then propose \textbf{DiffGradCAM}, a novel, lightweight, and
contrastive approach to class activation mapping that is both non-suceptible to
passive fooling, but also matches the output of standard CAM methods such as
GradCAM in the non-adversarial case. Together, SHAM and DiffGradCAM establish a
new framework for probing and improving the robustness of saliency-based
explanations. We validate both contributions across multi-class tasks with few
and many classes.

</details>


### [144] [NeurIPS 2024 ML4CFD Competition: Results and Retrospective Analysis](https://arxiv.org/abs/2506.08516)
*Mouadh Yagoubi,David Danan,Milad Leyli-Abadi,Ahmed Mazari,Jean-Patrick Brunet,Abbas Kabalan,Fabien Casenave,Yuxin Ma,Giovanni Catalani,Jean Fesquet,Jacob Helwig,Xuan Zhang,Haiyang Yu,Xavier Bertrand,Frederic Tost,Michael Baurheim,Joseph Morlier,Shuiwang Ji*

Main category: cs.LG

TL;DR: 组织ML4CFD竞赛解决机器学习在计算流体动力学应用中的问题，回顾竞赛结果并分析，展示ML代用模型潜力并为未来挑战提供指导。


<details>
  <summary>Details</summary>
Motivation: 机器学习用于计算流体动力学存在精度、泛化和物理一致性问题，阻碍其在科学领域实际应用，需进行基准测试。

Method: 组织ML4CFD竞赛，围绕二维翼型气动模拟的代理建模，提供OpenFOAM生成的数据集，用多标准框架评估。

Result: 竞赛吸引超240支队伍，部分方法在全局评估得分上超过基线，顶级参赛作品在综合指标上超原OpenFOAM求解器。

Conclusion: 分析顶级参赛作品设计原则和评估框架稳健性，为未来科学机器学习挑战提供指导。

Abstract: The integration of machine learning (ML) into the physical sciences is
reshaping computational paradigms, offering the potential to accelerate
demanding simulations such as computational fluid dynamics (CFD). Yet,
persistent challenges in accuracy, generalization, and physical consistency
hinder the practical deployment of ML models in scientific domains. To address
these limitations and systematically benchmark progress, we organized the
ML4CFD competition, centered on surrogate modeling for aerodynamic simulations
over two-dimensional airfoils. The competition attracted over 240 teams, who
were provided with a curated dataset generated via OpenFOAM and evaluated
through a multi-criteria framework encompassing predictive accuracy, physical
fidelity, computational efficiency, and out-of-distribution generalization.
This retrospective analysis reviews the competition outcomes, highlighting
several approaches that outperformed baselines under our global evaluation
score. Notably, the top entry exceeded the performance of the original OpenFOAM
solver on aggregate metrics, illustrating the promise of ML-based surrogates to
outperform traditional solvers under tailored criteria. Drawing from these
results, we analyze the key design principles of top submissions, assess the
robustness of our evaluation framework, and offer guidance for future
scientific ML challenges.

</details>


### [145] [Leveraging chaos in the training of artificial neural networks](https://arxiv.org/abs/2506.08523)
*Pedro Jiménez-González,Miguel C. Soriano,Lucas Lacasa*

Main category: cs.LG

TL;DR: 研究大学习率下神经网络训练轨迹动态，发现特定学习率区域能平衡探索与利用，可加速训练，结果具普适性。


<details>
  <summary>Details</summary>
Motivation: 传统优化神经网络算法多为利用型，探索大学习率下神经网络训练轨迹动态。

Method: 研究大学习率下神经网络训练轨迹，通过网络最大李雅普诺夫指数表征轨迹对初始条件的敏感依赖。

Result: 特定学习率区域GD优化从纯利用转变为探索 - 利用平衡，达到可接受测试精度的训练时间最短，结果在多种任务、架构和超参数下成立。

Conclusion: 瞬态混沌动力学在人工神经网络训练中有积极建设性作用，可通过定位混沌起始点加速训练。

Abstract: Traditional algorithms to optimize artificial neural networks when confronted
with a supervised learning task are usually exploitation-type relaxational
dynamics such as gradient descent (GD). Here, we explore the dynamics of the
neural network trajectory along training for unconventionally large learning
rates. We show that for a region of values of the learning rate, the GD
optimization shifts away from purely exploitation-like algorithm into a regime
of exploration-exploitation balance, as the neural network is still capable of
learning but the trajectory shows sensitive dependence on initial conditions --
as characterized by positive network maximum Lyapunov exponent --.
Interestingly, the characteristic training time required to reach an acceptable
accuracy in the test set reaches a minimum precisely in such learning rate
region, further suggesting that one can accelerate the training of artificial
neural networks by locating at the onset of chaos. Our results -- initially
illustrated for the MNIST classification task -- qualitatively hold for a range
of supervised learning tasks, learning architectures and other hyperparameters,
and showcase the emergent, constructive role of transient chaotic dynamics in
the training of artificial neural networks.

</details>


### [146] [Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)](https://arxiv.org/abs/2506.08533)
*Nihal Acharya Adde,Alexandra Gianzina,Hanno Gottschalk,Andreas Ebert*

Main category: cs.LG

TL;DR: 本文首次将EMNAS用于大规模自动驾驶强化学习的神经网络架构优化，实验显示其优于手动设计模型。


<details>
  <summary>Details</summary>
Motivation: 在大规模自动驾驶强化学习中优化神经网络架构，提高奖励、减小模型大小且不影响性能。

Method: 使用遗传算法自动化网络设计，采用并行化技术加速搜索，实施师生方法确保可扩展优化，利用迁移学习框架。

Result: 定制的EMNAS优于手动设计模型，用更少参数获得更高奖励。

Conclusion: 这些策略对自动驾驶强化学习的EMNAS有积极贡献，推动该领域发展。

Abstract: This paper introduces Evolutionary Multi-Objective Network Architecture
Search (EMNAS) for the first time to optimize neural network architectures in
large-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses
genetic algorithms to automate network design, tailored to enhance rewards and
reduce model size without compromising performance. Additionally,
parallelization techniques are employed to accelerate the search, and
teacher-student methodologies are implemented to ensure scalable optimization.
This research underscores the potential of transfer learning as a robust
framework for optimizing performance across iterative learning processes by
effectively leveraging knowledge from earlier generations to enhance learning
efficiency and stability in subsequent generations. Experimental results
demonstrate that tailored EMNAS outperforms manually designed models, achieving
higher rewards with fewer parameters. The findings of these strategies
contribute positively to EMNAS for RL in autonomous driving, advancing the
field toward better-performing networks suitable for real-world scenarios.

</details>


### [147] [DeepForm: Reasoning Large Language Model for Communication System Formulation](https://arxiv.org/abs/2506.08551)
*Panlong Wu,Ting Wang,Yifei Zhong,Haoqi Zhang,Zitong Wang,Fangxin Wang*

Main category: cs.LG

TL;DR: 本文介绍用于自动通信系统公式化的推理大语言模型DeepForm，提出数据集CSFRC，采用两阶段训练策略，实验表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有通用大语言模型缺乏通信系统公式化所需的专业知识、推理能力和特定领域训练数据，需开发专用模型。

Method: 引入DeepForm模型，提出数据集CSFRC，采用两阶段训练策略，包括使用CoT数据的有监督微调及基于ReMax的规则强化学习算法C - ReMax。

Result: 模型达到了最先进的性能，在多种场景下显著优于更大的专有大语言模型。

Conclusion: 提出的方法有效，论文接受后将发布相关资源以促进该领域研究。

Abstract: Communication system formulation is critical for advancing 6G and future
wireless technologies, yet it remains a complex, expertise-intensive task.
While Large Language Models (LLMs) offer potential, existing general-purpose
models often lack the specialized domain knowledge, nuanced reasoning
capabilities, and access to high-quality, domain-specific training data
required for adapting a general LLM into an LLM specially for communication
system formulation. To bridge this gap, we introduce DeepForm, the first
reasoning LLM specially for automated communication system formulation. We
propose the world-first large-scale, open-source dataset meticulously curated
for this domain called Communication System Formulation Reasoning Corpus
(CSFRC). Our framework employs a two-stage training strategy: first, Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge;
second, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based
on ReMax, to cultivate advanced modeling capabilities and elicit sophisticated
reasoning patterns like self-correction and verification. Extensive experiments
demonstrate that our model achieves state-of-the-art performance, significantly
outperforming larger proprietary LLMs on diverse senerios. We will release
related resources to foster further research in this area after the paper is
accepted.

</details>


### [148] [SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models](https://arxiv.org/abs/2506.08574)
*Alvise Dei Rossi,Matteo Metaldi,Michal Bechny,Irina Filchenko,Julia van der Meer,Markus H. Schmidt,Claudio L. A. Bassetti,Athina Tzovara,Francesca D. Faraci,Luigi Fiorillo*

Main category: cs.LG

TL;DR: 提出SLEEPYLAND框架及SOMNUS集成模型，SOMNUS在多数据集表现佳，超人类评分者，还引入分歧指标。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习自动睡眠分期在临床应用中模型评估、泛化、偏差及人类标注差异等问题。

Method: 构建SLEEPYLAND框架，有大量域内和域外睡眠记录；基于高性能架构发布预训练模型；引入SOMNUS集成模型，通过软投票结合多架构和通道配置模型。

Result: SOMNUS在24个数据集表现稳健，超94.9%个体模型，超之前最优方法；量化模型偏差；在域外多标注数据集超最佳人类评分者；引入分歧指标预测评分者分歧，ROC AUC达0.828。

Conclusion: 集成模型提升鲁棒性，但无架构能持续减少性能和临床指标估计偏差；SOMNUS能更好重现评分者共识；分歧指标可作为人类不确定性的数据驱动代理。

Abstract: Despite advances in deep learning for automatic sleep staging, clinical
adoption remains limited due to challenges in fair model evaluation,
generalization across diverse datasets, model bias, and variability in human
annotations. We present SLEEPYLAND, an open-source sleep staging evaluation
framework designed to address these barriers. It includes more than 22'0000
hours in-domain (ID) sleep recordings, and more than 84'000 hours out-of-domain
(OOD) sleep recordings, spanning a broad range of ages, sleep-wake disorders,
and hardware setups. We release pre-trained models based on high-performing SoA
architectures and evaluate them under standardized conditions across single-
and multi-channel EEG/EOG configurations. We introduce SOMNUS, an ensemble
combining models across architectures and channel setups via soft voting.
SOMNUS achieves robust performance across twenty-four different datasets, with
macro-F1 scores between 68.7% and 87.2%, outperforming individual models in
94.9% of cases. Notably, SOMNUS surpasses previous SoA methods, even including
cases where compared models were trained ID while SOMNUS treated the same data
as OOD. Using a subset of the BSWR (N=6'633), we quantify model biases linked
to age, gender, AHI, and PLMI, showing that while ensemble improves robustness,
no model architecture consistently minimizes bias in performance and clinical
markers estimation. In evaluations on OOD multi-annotated datasets (DOD-H,
DOD-O), SOMNUS exceeds the best human scorer, i.e., MF1 85.2% vs 80.8% on
DOD-H, and 80.2% vs 75.9% on DOD-O, better reproducing the scorer consensus
than any individual expert (k = 0.89/0.85 and ACS = 0.95/0.94 for healthy/OSA
cohorts). Finally, we introduce ensemble disagreement metrics - entropy and
inter-model divergence based - predicting regions of scorer disagreement with
ROC AUCs up to 0.828, offering a data-driven proxy for human uncertainty.

</details>


### [149] [Diffusion-based Time Series Forecasting for Sewerage Systems](https://arxiv.org/abs/2506.08577)
*Nicholas A. Pearson,Francesca Cairoli,Luca Bortolussi,Davide Russo,Francesca Zanello*

Main category: cs.LG

TL;DR: 本文提出利用生成式AI的深度学习方法，提升污水系统上下文预测准确性，经实证测试该模型效果佳。


<details>
  <summary>Details</summary>
Motivation: 提升污水系统上下文预测的准确性。

Method: 开发基于扩散的模型处理多变量时间序列数据，并用共形推理技术校准预测。

Result: 实证测试表明模型能在极端天气下保持准确的上下文预测。

Conclusion: 所提模型能在污水系统中提供可靠的上下文预测。

Abstract: We introduce a novel deep learning approach that harnesses the power of
generative artificial intelligence to enhance the accuracy of contextual
forecasting in sewerage systems. By developing a diffusion-based model that
processes multivariate time series data, our system excels at capturing complex
correlations across diverse environmental signals, enabling robust predictions
even during extreme weather events. To strengthen the model's reliability, we
further calibrate its predictions with a conformal inference technique,
tailored for probabilistic time series data, ensuring that the resulting
prediction intervals are statistically reliable and cover the true target
values with a desired confidence level. Our empirical tests on real sewerage
system data confirm the model's exceptional capability to deliver reliable
contextual predictions, maintaining accuracy even under severe weather
conditions.

</details>


### [150] [CALT: A Library for Computer Algebra with Transformer](https://arxiv.org/abs/2506.08600)
*Hiroshi Kera,Shun Arakawa,Yuta Sato*

Main category: cs.LG

TL;DR: 介绍能帮助非深度学习专家为符号计算任务训练模型的Python库CALT。


<details>
  <summary>Details</summary>
Motivation: 人工智能在符号计算端到端深度学习的进展带来新挑战和研究方向，需要符号计算社区贡献，且要帮助非专家开展相关工作。

Method: 开发了名为Computer Algebra with Transformer (CALT)的Python库。

Result: 无明确结果表述，开发出了CALT库。

Conclusion: 推出CALT库以助力非专家进行符号计算任务的模型训练。

Abstract: Recent advances in artificial intelligence have demonstrated the learnability
of symbolic computation through end-to-end deep learning. Given a sufficient
number of examples of symbolic expressions before and after the target
computation, Transformer models - highly effective learners of
sequence-to-sequence functions - can be trained to emulate the computation.
This development opens up several intriguing challenges and new research
directions, which require active contributions from the symbolic computation
community. In this work, we introduce Computer Algebra with Transformer (CALT),
a user-friendly Python library designed to help non-experts in deep learning
train models for symbolic computation tasks.

</details>


### [151] [Sample Efficient Demonstration Selection for In-Context Learning](https://arxiv.org/abs/2506.08607)
*Kiran Purohit,V Venktesh,Sourangshu Bhattacharya,Avishek Anand*

Main category: cs.LG

TL;DR: 提出CASE策略解决大语言模型上下文学习中示例选择问题，实现效率提升且不牺牲性能，并开源代码和数据。


<details>
  <summary>Details</summary>
Motivation: 在上下文长度预算约束下，有效选择少样本示例对构建有效提示很重要，现有设置中评估找出最佳示例面临大量需评估的情况。

Method: 将示例选择任务表述为top - m最佳臂识别问题，提出CASE策略维护挑战者臂短名单，每次迭代仅拉取短名单或当前top - m集合中的一个臂，并用参数化线性评分函数建模示例子集得分。

Result: CASE相比现有示例选择方法，运行时间最多加速7倍，减少7倍的大语言模型调用（减少87%）且不牺牲性能。

Conclusion: CASE策略能在不牺牲性能的前提下，显著提升大语言模型上下文学习中示例选择的效率。

Abstract: The in-context learning paradigm with LLMs has been instrumental in advancing
a wide range of natural language processing tasks. The selection of few-shot
examples (exemplars / demonstration samples) is essential for constructing
effective prompts under context-length budget constraints. In this paper, we
formulate the exemplar selection task as a top-m best arms identification
problem. A key challenge in this setup is the exponentially large number of
arms that need to be evaluated to identify the m-best arms. We propose CASE
(Challenger Arm Sampling for Exemplar selection), a novel sample-efficient
selective exploration strategy that maintains a shortlist of "challenger" arms,
which are current candidates for the top-m arms. In each iteration, only one of
the arms from this shortlist or the current topm set is pulled, thereby
reducing sample complexity and, consequently, the number of LLM evaluations.
Furthermore, we model the scores of exemplar subsets (arms) using a
parameterized linear scoring function, leading to stochastic linear bandits
setting. CASE achieves remarkable efficiency gains of up to 7x speedup in
runtime while requiring 7x fewer LLM calls (87% reduction) without sacrificing
performance compared to state-of-the-art exemplar selection methods. We release
our code and data at https://github.com/kiranpurohit/CASE

</details>


### [152] [HSG-12M: A Large-Scale Spatial Multigraph Dataset](https://arxiv.org/abs/2506.08618)
*Xianquan Yan,Hakan Akgün,Kenji Kawaguchi,N. Duane Loh,Ching Hua Lee*

Main category: cs.LG

TL;DR: 介绍大规模空间多重图数据集HSG - 12M及转换工具Poly2Graph，揭示图学习挑战并建立代数与图的新联系，为几何感知图学习奠基。


<details>
  <summary>Details</summary>
Motivation: 现有图基准假设非空间简单边，会合并物理上不同路径，本文旨在解决此问题，推动几何感知图学习。

Method: 引入HSG - 12M数据集，包含大量静态和动态哈密顿谱图；发布Poly2Graph管道将一维晶体哈密顿量映射到谱图；用流行GNN进行基准测试。

Result: 基准测试揭示从多边缘几何大规模学习的新挑战；谱图可作为多项式、向量和矩阵的通用拓扑指纹，建立新的代数 - 图联系。

Conclusion: HSG - 12M为几何感知图学习奠定基础，为凝聚态物理等领域的数据驱动科学发现带来新机遇。

Abstract: Existing graph benchmarks assume non-spatial, simple edges, collapsing
physically distinct paths into a single link. We introduce HSG-12M, the first
large-scale dataset of $\textbf{spatial multigraphs}-$graphs embedded in a
metric space where multiple geometrically distinct trajectories between two
nodes are retained as separate edges. HSG-12M contains 11.6 million static and
5.1 million dynamic $\textit{Hamiltonian spectral graphs}$ across 1401
characteristic-polynomial classes, derived from 177 TB of spectral potential
data. Each graph encodes the full geometry of a 1-D crystal's energy spectrum
on the complex plane, producing diverse, physics-grounded topologies that
transcend conventional node-coordinate datasets. To enable future extensions,
we release $\texttt{Poly2Graph}$: a high-performance, open-source pipeline that
maps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with
popular GNNs expose new challenges in learning from multi-edge geometry at
scale. Beyond its practical utility, we show that spectral graphs serve as
universal topological fingerprints of polynomials, vectors, and matrices,
forging a new algebra-to-graph link. HSG-12M lays the groundwork for
geometry-aware graph learning and new opportunities of data-driven scientific
discovery in condensed matter physics and beyond.

</details>


### [153] [Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers](https://arxiv.org/abs/2506.08641)
*Simon Roschmann,Quentin Bouniot,Vasilii Feofanov,Ievgen Redko,Zeynep Akata*

Main category: cs.LG

TL;DR: 提出TiViT框架将时间序列转为图像利用预训练ViT，在时间序列分类基准测试中表现出色，发现中间层效果好，与TSFM有互补性。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型发展受公开数据集稀缺限制，需新方法进行时间序列分类。

Method: 提出TiViT框架，将时间序列转换为图像，利用预训练ViT的表征能力；分析ViT的2D分块用于时间序列的理论依据；利用OpenCLIP模型的隐藏表征。

Result: TiViT在标准时间序列分类基准测试中达到了最先进水平；中间层具有高内在维度，对时间序列分类最有效；TiViT与TSFM表示空间有很强的互补性，结合特征可进一步提高性能。

Conclusion: 揭示了在非视觉领域复用视觉表征的新方向。

Abstract: Time series classification is a fundamental task in healthcare and industry,
yet the development of time series foundation models (TSFMs) remains limited by
the scarcity of publicly available time series datasets. In this work, we
propose Time Vision Transformer (TiViT), a framework that converts time series
into images to leverage the representational power of frozen Vision
Transformers (ViTs) pretrained on large-scale image datasets. First, we
theoretically motivate our approach by analyzing the 2D patching of ViTs for
time series, showing that it can increase the number of label-relevant tokens
and reduce the sample complexity. Second, we empirically demonstrate that TiViT
achieves state-of-the-art performance on standard time series classification
benchmarks by utilizing the hidden representations of large OpenCLIP models. We
explore the structure of TiViT representations and find that intermediate
layers with high intrinsic dimension are the most effective for time series
classification. Finally, we assess the alignment between TiViT and TSFM
representation spaces and identify a strong complementarity, with further
performance gains achieved by combining their features. Our findings reveal yet
another direction for reusing vision representations in a non-visual domain.

</details>


### [154] [Semi-gradient DICE for Offline Constrained Reinforcement Learning](https://arxiv.org/abs/2506.08644)
*Woosung Kim,JunHo Seo,Jongmin Lee,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 本文指出增强DICE框架离线RL性能的方法损害OPE能力，分析原因并提出新方法实现OPE和约束RL，在DSRL基准测试取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有增强DICE框架离线RL性能的方法损害其OPE能力，不适合约束RL场景，需解决该问题。

Method: 提出通过半梯度DICE实现OPE和约束RL的新方法。

Result: 新方法能确保准确的成本估计，在离线约束RL基准DSRL上取得了最先进的性能。

Conclusion: 所提新方法解决了现有方法的局限性，可有效实现OPE和约束RL。

Abstract: Stationary Distribution Correction Estimation (DICE) addresses the mismatch
between the stationary distribution induced by a policy and the target
distribution required for reliable off-policy evaluation (OPE) and policy
optimization. DICE-based offline constrained RL particularly benefits from the
flexibility of DICE, as it simultaneously maximizes return while estimating
costs in offline settings. However, we have observed that recent approaches
designed to enhance the offline RL performance of the DICE framework
inadvertently undermine its ability to perform OPE, making them unsuitable for
constrained RL scenarios. In this paper, we identify the root cause of this
limitation: their reliance on a semi-gradient optimization, which solves a
fundamentally different optimization problem and results in failures in cost
estimation. Building on these insights, we propose a novel method to enable OPE
and constrained RL through semi-gradient DICE. Our method ensures accurate cost
estimation and achieves state-of-the-art performance on the offline constrained
RL benchmark, DSRL.

</details>


### [155] [Fusing Cross-modal and Uni-modal Representations: A Kronecker Product Approach](https://arxiv.org/abs/2506.08645)
*Youqi Wu,Jingwei Zhang,Farzan Farnia*

Main category: cs.LG

TL;DR: 本文提出RP - KrossFuse方法统一跨模态和单模态嵌入，实验表明其能兼顾模态特定性能和跨模态对齐。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态嵌入在特定模态任务表现不佳，单模态嵌入缺乏跨模态对齐能力，需统一二者优势。

Method: 提出RP - KrossFuse方法，利用基于随机投影的Kronecker积整合跨模态和单模态嵌入，在指定核空间高效运行。

Result: 通过结合CLIP嵌入与单模态图像和文本嵌入的实验，表明RP - KrossFuse能实现有竞争力的特定模态性能并保留跨模态对齐。

Conclusion: RP - KrossFuse缩小了跨模态和单模态嵌入的差距，有效统一二者优势。

Abstract: Cross-modal embeddings, such as CLIP, BLIP and their variants, have achieved
promising results in aligning representations across modalities. However, these
embeddings could underperform compared to state-of-the-art single-modality
embeddings on modality-specific tasks. On the other hand, single-modality
embeddings excel in their domains but lack cross-modal alignment capabilities.
In this work, we focus on the problem of unifying cross-modality and
single-modality embeddings to achieve the performance of modality-expert
embedding within individual modalities while preserving cross-modal alignment.
To this end, we propose RP-KrossFuse, a method that leverages a random
projection-based Kronecker product to integrate cross-modal embeddings with
single-modality embeddings. RP-KrossFuse aims to fuse the sample-pairwise
similarity scores of the fused embeddings and operates efficiently in a
specified kernel space and supports scalable implementations via random Fourier
features for shift-invariant kernels such as the Gaussian kernel. We
demonstrate the effectiveness of RP-KrossFuse through several numerical
experiments, combining CLIP embeddings with uni-modal image and text
embeddings. Our numerical results indicate that RP-KrossFuse achieves
competitive modality-specific performance while retaining cross-modal
alignment, bridging the gap between cross-modal and single-modality embeddings.

</details>


### [156] [JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the Tiny Shakespeare Dataset](https://arxiv.org/abs/2506.08652)
*Mahesh Godavarti*

Main category: cs.LG

TL;DR: 本文介绍了基于旅程的Transformer架构JoFormer，它能有效结合位置信息，在字符级语言建模任务上表现优于基线模型，为Transformer架构整合位置结构提供了原则性方法。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在序列建模中有效结合位置信息的难题。

Method: 引入基于非交换代数的JoFormer架构，通过可学习的方向变换表示相对位置，推导其注意力机制。

Result: 在Tiny Shakespeare字符级语言建模任务上，JoFormer比RoFormer基线模型困惑度更低、收敛更快。

Conclusion: JoFormer为Transformer架构整合位置结构提供了原则性方法。

Abstract: Transformers have demonstrated remarkable success in sequence modeling, yet
effectively incorporating positional information remains a challenging and
active area of research. In this paper, we introduce JoFormer, a journey-based
Transformer architecture grounded in a recently proposed non-commutative
algebra for composing transformations across positions. JoFormer represents
relative positions through learnable directional transforms that are
sequentially composed along the input, thereby extending and generalizing
existing approaches based on relative position representations. We derive the
JoFormer attention mechanism from first principles and show that it subsumes
standard methods such as rotary transformations as special cases. To evaluate
its effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny
Shakespeare character-level language modeling task. Our results demonstrate
that
  JoFormer consistently achieves lower perplexity and faster convergence,
highlighting the advantages of its more expressive, journey-based treatment of
position. Notably, the per-token JoFormer is still a primitive, conceptual
variant with layer-independent angles, yet it already demonstrates strong
performance-underscoring its promise as a proof of concept for more expressive
architectures. We conclude by discussing how JoFormer offers a principled
approach to integrating positional structure into Transformer architectures.
The code used in this work is available at
https://github.com/mahesh-godavarti/joformer.

</details>


### [157] [When Simple Model Just Works: Is Network Traffic Classification in Crisis?](https://arxiv.org/abs/2506.08655)
*Kamil Jerabek,Jan Luxemburk,Richard Plny,Josef Koumar,Jaroslav Pesek,Karel Hynek*

Main category: cs.LG

TL;DR: 研究网络流量分类中简单k - NN基线表现佳的现象，发现数据集冗余问题，指出标准机器学习实践可能不适用于此领域并提出新方向。


<details>
  <summary>Details</summary>
Motivation: 探究简单k - NN基线在网络流量分类中表现好的原因。

Method: 在12个数据集和15个流量分类任务中评估k - NN基线，并分析数据集特点。

Result: 多数数据集含超50%冗余样本，冗余会导致模型性能高估和理论最大准确率降低。

Conclusion: 标准机器学习实践可能不适用于流量分类，需新的任务制定和评估方向。

Abstract: Machine learning has been applied to network traffic classification (TC) for
over two decades. While early efforts used shallow models, the latter 2010s saw
a shift toward complex neural networks, often reporting near-perfect accuracy.
However, it was recently revealed that a simple k-NN baseline using packet
sequences metadata (sizes, times, and directions) can be on par or even
outperform more complex methods. In this paper, we investigate this phenomenon
further and evaluate this baseline across 12 datasets and 15 TC tasks, and
investigate why it performs so well. Our analysis shows that most datasets
contain over 50% redundant samples (identical packet sequences), which
frequently appear in both training and test sets due to common splitting
practices. This redundancy can lead to overestimated model performance and
reduce the theoretical maximum accuracy when identical flows have conflicting
labels. Given its distinct characteristics, we further argue that standard
machine learning practices adapted from domains like NLP or computer vision may
be ill-suited for TC. Finally, we propose new directions for task formulation
and evaluation to address these challenges and help realign the field.

</details>


### [158] [Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness](https://arxiv.org/abs/2506.08660)
*Jinkwan Jang,Hyungjin Park,Jinmyeong Choi,Taesup Kim*

Main category: cs.LG

TL;DR: 提出ChannelTokenFormer模型处理多变量时间序列预测问题，实验显示其在现实条件下性能优越。


<details>
  <summary>Details</summary>
Motivation: 现实世界多变量时间序列数据存在通道依赖、采样异步和缺失值等问题，而现有模型假设过于简化，不适合现实场景。

Method: 提出基于Transformer的ChannelTokenFormer模型，能捕捉跨通道交互、适应通道异步采样和处理缺失值。

Result: 在三个修改后的基准数据集和一个现实工业数据集上实验，表明ChannelTokenFormer在现实挑战条件下有更好的鲁棒性和准确性。

Conclusion: ChannelTokenFormer模型能有效应对现实世界多变量时间序列数据的预测挑战。

Abstract: Real-world time series data are inherently multivariate, often exhibiting
complex inter-channel dependencies. Each channel is typically sampled at its
own period and is prone to missing values due to various practical and
operational constraints. These characteristics pose fundamental challenges
related to channel dependency, sampling asynchrony, and missingness, all of
which must be addressed to enable robust and reliable forecasting in practical
settings. However, most existing architectures are built on oversimplified
assumptions, such as identical sampling periods across channels and fully
observed inputs at test time, which often do not hold in real-world scenarios.
To bridge this gap, we propose ChannelTokenFormer, a Transformer-based
forecasting model with a flexible architecture designed to explicitly capture
cross-channel interactions, accommodate channel-wise asynchronous sampling, and
effectively handle missing values. Extensive experiments on three benchmark
datasets modified to reflect practical settings, along with one real-world
industrial dataset, demonstrate the superior robustness and accuracy of
ChannelTokenFormer under challenging real-world conditions.

</details>


### [159] [Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization](https://arxiv.org/abs/2506.08662)
*Florian Borzechowski,Michael Schäfer,Heiko Schwarz,Jonathan Pfaff,Detlev Marpe,Thomas Wiegand*

Main category: cs.LG

TL;DR: 提出额外微调训练步骤，对量化潜变量重训练，在不增加推理复杂度下为量化编码带来增益。


<details>
  <summary>Details</summary>
Motivation: 现有量化近似方法不能正确建模量化噪声，导致网络性能欠佳。

Method: 在传统端到端训练后，对推理阶段获得的量化潜变量重新训练网络部分模块。

Result: 对柯达测试集平均节省1%-2%比特率，对TecNick测试集最高节省2.2%。

Conclusion: 对正确量化的数据重训练能为均匀标量和熵约束量化带来编码增益，且不增加推理复杂度。

Abstract: The continuous improvements on image compression with variational
autoencoders have lead to learned codecs competitive with conventional
approaches in terms of rate-distortion efficiency. Nonetheless, taking the
quantization into account during the training process remains a problem, since
it produces zero derivatives almost everywhere and needs to be replaced with a
differentiable approximation which allows end-to-end optimization. Though there
are different methods for approximating the quantization, none of them model
the quantization noise correctly and thus, result in suboptimal networks.
Hence, we propose an additional finetuning training step: After conventional
end-to-end training, parts of the network are retrained on quantized latents
obtained at the inference stage. For entropy-constraint quantizers like
Trellis-Coded Quantization, the impact of the quantizer is particularly
difficult to approximate by rounding or adding noise as the quantized latents
are interdependently chosen through a trellis search based on both the entropy
model and a distortion measure. We show that retraining on correctly quantized
data consistently yields additional coding gain for both uniform scalar and
especially for entropy-constraint quantization, without increasing inference
complexity. For the Kodak test set, we obtain average savings between 1% and
2%, and for the TecNick test set up to 2.2% in terms of Bj{\o}ntegaard-Delta
bitrate.

</details>


### [160] [Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search](https://arxiv.org/abs/2506.08669)
*Dongge Han,Menglin Xia,Daniel Madrigal Diaz,Samuel Kessler,Ankur Mallick,Xuchao Zhang,Mirian Del Carmen Hipolito Garcia,Jin Xu,Victor Rühle,Saravan Rajmohan*

Main category: cs.LG

TL;DR: 提出用LLM生成蓝图的框架提升SLM推理能力及减轻对提示变化的敏感性，在多任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: SLM容量有限，推理能力受限且对提示变化敏感。

Method: 提出用LLM生成蓝图的框架，蓝图提供推理指导，并集成提示模板搜索机制。

Result: 框架在数学、编码和逻辑推理等任务中提升了SLM性能。

Conclusion: 该方法无需增加模型大小或额外训练，适合设备端或资源受限环境。

Abstract: Small language models (SLMs) offer promising and efficient alternatives to
large language models (LLMs). However, SLMs' limited capacity restricts their
reasoning capabilities and makes them sensitive to prompt variations. To
address these challenges, we propose a novel framework that enhances SLM
reasoning capabilities through LLM generated blueprints. The blueprints provide
structured, high-level reasoning guides that help SLMs systematically tackle
related problems. Furthermore, our framework integrates a prompt template
search mechanism to mitigate the SLMs' sensitivity to prompt variations. Our
framework demonstrates improved SLM performance across various tasks, including
math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves
the reasoning capabilities of SLMs without increasing model size or requiring
additional training, offering a lightweight and deployment-friendly solution
for on-device or resource-constrained environments.

</details>


### [161] [Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling](https://arxiv.org/abs/2506.08681)
*Phuc Minh Nguyen,Ngoc-Hieu Nguyen,Duy H. M. Nguyen,Anji Liu,An Mai,Binh T. Nguyen,Daniel Sonntag,Khoa D. Doan*

Main category: cs.LG

TL;DR: 本文提出 IS - DAAs 方法缓解离线直接对齐算法过优化问题，实验表明其效果好于其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有直接对齐算法（DAAs）易出现过优化问题，导致模型性能下降。

Method: 提出重要性采样方法 IS - DAAs，将 DAA 目标与考虑参考策略分布的重要性比率相乘，并将重要性比率裁剪到最大值以避免高方差问题。

Result: 广泛实验表明 IS - DAAs 能有效缓解过优化问题，尤其在低正则化强度下，性能优于其他方法。

Conclusion: IS - DAAs 是缓解离线 DAAs 过优化问题的有效方法。

Abstract: Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization
(DPO) have emerged as alternatives to the standard Reinforcement Learning from
Human Feedback (RLHF) for aligning large language models (LLMs) with human
values. However, these methods are more susceptible to over-optimization, in
which the model drifts away from the reference policy, leading to degraded
performance as training progresses. This paper proposes a novel
importance-sampling approach to mitigate the over-optimization problem of
offline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective
with an importance ratio that accounts for the reference policy distribution.
IS-DAAs additionally avoid the high variance issue associated with importance
sampling by clipping the importance ratio to a maximum value. Our extensive
experiments demonstrate that IS-DAAs can effectively mitigate
over-optimization, especially under low regularization strength, and achieve
better performance than other methods designed to address this problem. Our
implementations are provided publicly at this link.

</details>


### [162] [Variational Autoencoder-Based Approach to Latent Feature Analysis on Efficient Representation of Power Load Monitoring Data](https://arxiv.org/abs/2506.08698)
*Boyu Xie,Tangtang Xie*

Main category: cs.LG

TL;DR: 针对智能电网中高维不完整电力负荷监测数据挑战预测模型性能问题，提出VAE - LF模型补全数据，实验显示其优于其他基准模型，为负荷管理提供有效方案。


<details>
  <summary>Details</summary>
Motivation: 智能电网发展下，高维不完整的电力负荷监测数据影响电力负荷预测模型性能。

Method: 提出基于变分自编码器的VAE - LF模型，采用编码器 - 解码器结构，将高维不完整数据拆分为向量依次输入模型，学习数据低维潜在表示并生成补全数据。

Result: 在UK - DALE数据集上实验，VAE - LF在5%和10%稀疏度测试中优于其他基准模型，RMSE和MAE显著更低，在低稀疏度数据上表现更优。

Conclusion: 该方法为智能电网电力负荷管理提供了高效的数据补全解决方案。

Abstract: With the development of smart grids, High-Dimensional and Incomplete (HDI)
Power Load Monitoring (PLM) data challenges the performance of Power Load
Forecasting (PLF) models. In this paper, we propose a potential
characterization model VAE-LF based on Variational Autoencoder (VAE) for
efficiently representing and complementing PLM missing data. VAE-LF learns a
low-dimensional latent representation of the data using an Encoder-Decoder
structure by splitting the HDI PLM data into vectors and feeding them
sequentially into the VAE-LF model, and generates the complementary data.
Experiments on the UK-DALE dataset show that VAE-LF outperforms other benchmark
models in both 5% and 10% sparsity test cases, with significantly lower RMSE
and MAE, and especially outperforms on low sparsity ratio data. The method
provides an efficient data-completion solution for electric load management in
smart grids.

</details>


### [163] [Exploration by Random Reward Perturbation](https://arxiv.org/abs/2506.08737)
*Haozhe Ma,Guoji Fu,Zhengding Luo,Jiele Wu,Tze-Yun Leong*

Main category: cs.LG

TL;DR: 提出随机奖励扰动（RRP）探索策略，理论上可增强策略多样性，兼容其他探索策略，实验表明能提升算法性能。


<details>
  <summary>Details</summary>
Motivation: 为强化学习寻找新的有效探索策略，解决现有探索方法的局限性。

Method: 提出RRP策略，向环境奖励添加零均值噪声，并与基于动作扰动的探索策略结合。

Result: RRP显著提升近端策略优化和软演员 - 评判家算法的性能，在不同任务和奖励场景下实现更高样本效率和逃离局部最优。

Conclusion: RRP是一种通用、轻量级的探索策略，能与现有算法轻松集成，在奖励塑造和噪声驱动探索间建立理论联系，具有互补潜力。

Abstract: We introduce Random Reward Perturbation (RRP), a novel exploration strategy
for reinforcement learning (RL). Our theoretical analyses demonstrate that
adding zero-mean noise to environmental rewards effectively enhances policy
diversity during training, thereby expanding the range of exploration. RRP is
fully compatible with the action-perturbation-based exploration strategies,
such as $\epsilon$-greedy, stochastic policies, and entropy regularization,
providing additive improvements to exploration effects. It is general,
lightweight, and can be integrated into existing RL algorithms with minimal
implementation effort and negligible computational overhead. RRP establishes a
theoretical connection between reward shaping and noise-driven exploration,
highlighting their complementary potential. Experiments show that RRP
significantly boosts the performance of Proximal Policy Optimization and Soft
Actor-Critic, achieving higher sample efficiency and escaping local optima
across various tasks, under both sparse and dense reward scenarios.

</details>


### [164] [Urban Incident Prediction with Graph Neural Networks: Integrating Government Ratings and Crowdsourced Reports](https://arxiv.org/abs/2506.08740)
*Sidhika Balachandar,Shuvom Sadhuka,Bonnie Berger,Emma Pierson,Nikhil Garg*

Main category: cs.LG

TL;DR: 本文提出多视图、多输出GNN模型，结合政府评级数据和众包报告数据预测城市事件潜在状态，通过纽约市案例展示该方法可更好预测，还量化了众包报告中的人口偏差。


<details>
  <summary>Details</summary>
Motivation: 政府官员希望了解各社区基础设施问题发生情况，但政府检查评级数据稀疏，众包报告数据有偏差，需结合两者数据准确预测事件潜在状态。

Method: 提出多视图、多输出GNN模型，结合无偏评级数据和有偏报告数据；开展纽约市城市事件案例研究，收集并公开相关数据集。

Result: 在真实和半合成数据上，模型比仅使用报告数据或评级数据的模型能更好预测潜在状态，尤其在评级数据稀疏且报告能预测评级时；量化了众包报告中的人口偏差。

Conclusion: 展示了一种使用异质、稀疏和有偏数据进行潜在状态预测的广泛适用方法。

Abstract: Graph neural networks (GNNs) are widely used in urban spatiotemporal
forecasting, such as predicting infrastructure problems. In this setting,
government officials wish to know in which neighborhoods incidents like
potholes or rodent issues occur. The true state of incidents (e.g., street
conditions) for each neighborhood is observed via government inspection
ratings. However, these ratings are only conducted for a sparse set of
neighborhoods and incident types. We also observe the state of incidents via
crowdsourced reports, which are more densely observed but may be biased due to
heterogeneous reporting behavior. First, for such settings, we propose a
multiview, multioutput GNN-based model that uses both unbiased rating data and
biased reporting data to predict the true latent state of incidents. Second, we
investigate a case study of New York City urban incidents and collect,
standardize, and make publicly available a dataset of 9,615,863 crowdsourced
reports and 1,041,415 government inspection ratings over 3 years and across 139
types of incidents. Finally, we show on both real and semi-synthetic data that
our model can better predict the latent state compared to models that use only
reporting data or models that use only rating data, especially when rating data
is sparse and reports are predictive of ratings. We also quantify demographic
biases in crowdsourced reporting, e.g., higher-income neighborhoods report
problems at higher rates. Our analysis showcases a widely applicable approach
for latent state prediction using heterogeneous, sparse, and biased data.

</details>


### [165] [On the Stability of the Jacobian Matrix in Deep Neural Networks](https://arxiv.org/abs/2506.08764)
*Benjamin Dadoun,Soufiane Hayou,Hanan Salam,Mohamed El Amine Seddik,Pierre Youssef*

Main category: cs.LG

TL;DR: 本文突破先前工作局限，建立深度神经网络通用稳定性定理，扩展现代神经网络初始化方案理论基础。


<details>
  <summary>Details</summary>
Motivation: 先前工作对雅可比矩阵稳定性的分析局限于全连接网络和独立同分布权重，本文旨在突破此局限。

Method: 借助随机矩阵理论的最新进展。

Result: 建立了能适应稀疏性和非独立同分布、弱相关权重的通用稳定性定理，为更广泛网络模型的谱稳定性提供严格保证。

Conclusion: 扩展了具有结构化和依赖随机性的现代神经网络初始化方案的理论基础。

Abstract: Deep neural networks are known to suffer from exploding or vanishing
gradients as depth increases, a phenomenon closely tied to the spectral
behavior of the input-output Jacobian. Prior work has identified critical
initialization schemes that ensure Jacobian stability, but these analyses are
typically restricted to fully connected networks with i.i.d. weights. In this
work, we go significantly beyond these limitations: we establish a general
stability theorem for deep neural networks that accommodates sparsity (such as
that introduced by pruning) and non-i.i.d., weakly correlated weights (e.g.
induced by training). Our results rely on recent advances in random matrix
theory, and provide rigorous guarantees for spectral stability in a much
broader class of network models. This extends the theoretical foundation for
initialization schemes in modern neural networks with structured and dependent
randomness.

</details>


### [166] [Design Patterns for Securing LLM Agents against Prompt Injections](https://arxiv.org/abs/2506.08837)
*Luca Beurer-Kellner,Beat Buesser Ana-Maria Creţu,Edoardo Debenedetti,Daniel Dobos,Daniel Fabian,Marc Fischer,David Froelicher,Kathrin Grosse,Daniel Naeff,Ezinwanne Ozoani,Andrew Paverd,Florian Tramèr,Václav Volhejn*

Main category: cs.LG

TL;DR: 提出构建对提示注入攻击有可证明抗性的AI代理设计模式，分析并通过案例展示其适用性。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的AI代理功能增多，确保其安全成关键挑战，提示注入攻击威胁大。

Method: 提出一组设计模式，系统分析模式，讨论效用和安全的权衡。

Result: 通过一系列案例研究展示了设计模式的现实适用性。

Conclusion: 所提出的设计模式可用于构建对提示注入有可证明抗性的AI代理。

Abstract: As AI agents powered by Large Language Models (LLMs) become increasingly
versatile and capable of addressing a broad spectrum of tasks, ensuring their
security has become a critical challenge. Among the most pressing threats are
prompt injection attacks, which exploit the agent's resilience on natural
language inputs -- an especially dangerous threat when agents are granted tool
access or handle sensitive information. In this work, we propose a set of
principled design patterns for building AI agents with provable resistance to
prompt injection. We systematically analyze these patterns, discuss their
trade-offs in terms of utility and security, and illustrate their real-world
applicability through a series of case studies.

</details>


### [167] [Agile Reinforcement Learning for Real-Time Task Scheduling in Edge Computing](https://arxiv.org/abs/2506.08850)
*Amin Avan,Akramul Azim,Qusay Mahmoud*

Main category: cs.LG

TL;DR: 本文指出边缘计算中软实时应用任务调度的挑战，提出敏捷强化学习（aRL）方法，实验表明其比基线方法有更高命中率和更快收敛速度。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中软实时应用任务调度复杂，启发式和元启发式算法难以适应动态环境，传统强化学习算法学习时间长。

Method: 提出敏捷强化学习（aRL），让RL - agent进行有信息的探索并只执行相关动作。

Result: 实验表明，结合有信息探索和动作掩码方法，aRL比基线方法有更高命中率和更快收敛速度。

Conclusion: aRL具有更好的可预测性，能快速适应和收敛，适合边缘计算中软实时应用的任务调度。

Abstract: Soft real-time applications are becoming increasingly complex, posing
significant challenges for scheduling offloaded tasks in edge computing
environments while meeting task timing constraints. Moreover, the exponential
growth of the search space, presence of multiple objectives and parameters, and
highly dynamic nature of edge computing environments further exacerbate the
complexity of task scheduling. As a result, schedulers based on heuristic and
metaheuristic algorithms frequently encounter difficulties in generating
optimal or near-optimal task schedules due to their constrained ability to
adapt to the dynamic conditions and complex environmental characteristics of
edge computing. Accordingly, reinforcement learning algorithms have been
incorporated into schedulers to address the complexity and dynamic conditions
inherent in task scheduling in edge computing. However, a significant
limitation of reinforcement learning algorithms is the prolonged learning time
required to adapt to new environments and to address medium- and large-scale
problems. This challenge arises from the extensive global action space and
frequent random exploration of irrelevant actions. Therefore, this study
proposes Agile Reinforcement learning (aRL), in which the RL-agent performs
informed exploration and executes only relevant actions. Consequently, the
predictability of the RL-agent is enhanced, leading to rapid adaptation and
convergence, which positions aRL as a suitable candidate for scheduling the
tasks of soft real-time applications in edge computing. The experiments
demonstrate that the combination of informed exploration and action-masking
methods enables aRL to achieve a higher hit-ratio and converge faster than the
baseline approaches.

</details>


### [168] [Adapting to Heterophilic Graph Data with Structure-Guided Neighbor Discovery](https://arxiv.org/abs/2506.08871)
*Victor M. Tenorio,Madeline Navarro,Samuel Rey,Santiago Segarra,Antonio G. Marques*

Main category: cs.LG

TL;DR: 本文针对GNN处理异质数据的问题，提出SG - GNN架构，利用结构信息引导GNN，实验表明其性能优异。


<details>
  <summary>Details</summary>
Motivation: GNN处理异质数据时因假设同质性和依赖局部消息传递而表现不佳，需新方法解决。

Method: 创建具有相似结构属性节点相连的替代图结构，理论证明利用假正边少的图和多图视图有益，提出SG - GNN架构自适应学习加权。

Result: 在多个基准数据集，尤其是异质特征数据集上，SG - GNN达到了最先进或极具竞争力的性能。

Conclusion: 利用结构信息引导GNN是有效的。

Abstract: Graph Neural Networks (GNNs) often struggle with heterophilic data, where
connected nodes may have dissimilar labels, as they typically assume homophily
and rely on local message passing. To address this, we propose creating
alternative graph structures by linking nodes with similar structural
attributes (e.g., role-based or global), thereby fostering higher label
homophily on these new graphs. We theoretically prove that GNN performance can
be improved by utilizing graphs with fewer false positive edges (connections
between nodes of different classes) and that considering multiple graph views
increases the likelihood of finding such beneficial structures. Building on
these insights, we introduce Structure-Guided GNN (SG-GNN), an architecture
that processes the original graph alongside the newly created structural
graphs, adaptively learning to weigh their contributions. Extensive experiments
on various benchmark datasets, particularly those with heterophilic
characteristics, demonstrate that our SG-GNN achieves state-of-the-art or
highly competitive performance, highlighting the efficacy of exploiting
structural information to guide GNNs.

</details>


### [169] [Filling in the Blanks: Applying Data Imputation in incomplete Water Metering Data](https://arxiv.org/abs/2506.08882)
*Dimitrios Amaxilatis,Themistoklis Sarantakos,Ioannis Chatzigiannakis,Georgios Mylonas*

Main category: cs.LG

TL;DR: 本文基于实际物联网水网监测数据，对比多种数据插补方法以提升智能水表对水分配网络的监测和管理质量。


<details>
  <summary>Details</summary>
Motivation: 智能水表产生的数据存在因技术问题导致的数据缺口，影响运营决策和效率，需提升数据质量。

Method: 对比多种插补方法，如k近邻、MissForest、Transformer和循环神经网络。

Result: 有效的数据插补能显著提升从用水数据中获取的洞察质量。

Conclusion: 数据插补可在漏水检测和预测性维护调度等应用中提供解决方案。

Abstract: In this work, we explore the application of recent data imputation techniques
to enhance monitoring and management of water distribution networks using smart
water meters, based on data derived from a real-world IoT water grid monitoring
deployment. Despite the detailed data produced by such meters, data gaps due to
technical issues can significantly impact operational decisions and efficiency.
Our results, by comparing various imputation methods, such as k-Nearest
Neighbors, MissForest, Transformers, and Recurrent Neural Networks, indicate
that effective data imputation can substantially enhance the quality of the
insights derived from water consumption data as we study their effect on
accuracy and reliability of water metering data to provide solutions in
applications like leak detection and predictive maintenance scheduling.

</details>


### [170] [InfoDPCCA: Information-Theoretic Dynamic Probabilistic Canonical Correlation Analysis](https://arxiv.org/abs/2506.08884)
*Shiqin Tang,Shujian Yu*

Main category: cs.LG

TL;DR: 提出InfoDPCCA框架用于从高维序列数据提取潜在表示，有新目标、训练方案和机制，实验证明其在表示学习方面表现出色，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决从高维序列数据中提取有意义潜在表示的挑战，现有动态CCA模型存在不足。

Method: 引入InfoDPCCA框架，利用信息论目标，采用两步训练方案和残差连接机制。

Result: 通过合成和医学fMRI数据实验，证明InfoDPCCA在表示学习方面表现出色。

Conclusion: InfoDPCCA是一种优秀的表示学习工具。

Abstract: Extracting meaningful latent representations from high-dimensional sequential
data is a crucial challenge in machine learning, with applications spanning
natural science and engineering. We introduce InfoDPCCA, a dynamic
probabilistic Canonical Correlation Analysis (CCA) framework designed to model
two interdependent sequences of observations. InfoDPCCA leverages a novel
information-theoretic objective to extract a shared latent representation that
captures the mutual structure between the data streams and balances
representation compression and predictive sufficiency while also learning
separate latent components that encode information specific to each sequence.
Unlike prior dynamic CCA models, such as DPCCA, our approach explicitly
enforces the shared latent space to encode only the mutual information between
the sequences, improving interpretability and robustness. We further introduce
a two-step training scheme to bridge the gap between information-theoretic
representation learning and generative modeling, along with a residual
connection mechanism to enhance training stability. Through experiments on
synthetic and medical fMRI data, we demonstrate that InfoDPCCA excels as a tool
for representation learning. Code of InfoDPCCA is available at
https://github.com/marcusstang/InfoDPCCA.

</details>


### [171] [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](https://arxiv.org/abs/2506.08889)
*Yizhao Gao,Shuming Guo,Shijie Cao,Yuqing Xia,Yu Cheng,Lei Wang,Lingxiao Ma,Yutao Sun,Tianzhu Ye,Li Dong,Hayden Kwok-Hay So,Yu Hua,Ting Cao,Fan Yang,Mao Yang*

Main category: cs.LG

TL;DR: 提出SeerAttention - R用于推理模型长解码，灵活可集成，训练数据少，推理精度高，解码内核有速度提升。


<details>
  <summary>Details</summary>
Motivation: 为推理模型的长解码提供合适的稀疏注意力框架。

Method: 在SeerAttention基础上改进，去除查询池化，采用轻量级插件门控；用TileLang开发优化的稀疏解码内核。

Result: 在AIME基准测试中，用0.4B令牌训练，4K令牌预算下保持近无损推理精度；在H100 GPU上90%稀疏度时比FlashAttention - 3快达9倍。

Conclusion: SeerAttention - R是一种有效且高效的推理模型长解码稀疏注意力框架。

Abstract: We introduce SeerAttention-R, a sparse attention framework specifically
tailored for the long decoding of reasoning models. Extended from
SeerAttention, SeerAttention-R retains the design of learning attention
sparsity through a self-distilled gating mechanism, while removing query
pooling to accommodate auto-regressive decoding. With a lightweight plug-in
gating, SeerAttention-R is flexible and can be easily integrated into existing
pretrained model without modifying the original parameters. We demonstrate that
SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning
accuracy with 4K token budget in AIME benchmark under large sparse attention
block sizes (64/128). Using TileLang, we develop a highly optimized sparse
decoding kernel that achieves near-theoretical speedups of up to 9x over
FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:
https://github.com/microsoft/SeerAttention.

</details>


### [172] [Intention-Conditioned Flow Occupancy Models](https://arxiv.org/abs/2506.08902)
*Chongyi Zheng,Seohong Park,Sergey Levine,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本文提出意图条件流占用模型（InFOM）用于强化学习预训练，在多个基准任务上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 将大规模预训练框架应用于强化学习，解决样本效率和鲁棒性等核心挑战，同时应对强化学习中预训练大模型的时间推理难题。

Method: 使用流匹配构建概率模型预测代理在遥远未来访问的状态，模型中加入潜在变量捕捉用户意图。

Result: 在36个基于状态和4个基于图像的基准任务实验中，该方法使回报中位数提升1.8倍，成功率提高36%。

Conclusion: 所提出的InFOM方法在强化学习预训练中表现优于其他方法。

Abstract: Large-scale pre-training has fundamentally changed how machine learning
research is done today: large foundation models are trained once, and then can
be used by anyone in the community (including those without data or compute
resources to train a model from scratch) to adapt and fine-tune to specific
tasks. Applying this same framework to reinforcement learning (RL) is appealing
because it offers compelling avenues for addressing core challenges in RL,
including sample efficiency and robustness. However, there remains a
fundamental challenge to pre-train large models in the context of RL: actions
have long-term dependencies, so training a foundation model that reasons across
time is important. Recent advances in generative AI have provided new tools for
modeling highly complex distributions. In this paper, we build a probabilistic
model to predict which states an agent will visit in the temporally distant
future (i.e., an occupancy measure) using flow matching. As large datasets are
often constructed by many distinct users performing distinct tasks, we include
in our model a latent variable capturing the user intention. This intention
increases the expressivity of our model, and enables adaptation with
generalized policy improvement. We call our proposed method
intention-conditioned flow occupancy models (InFOM). Comparing with alternative
methods for pre-training, our experiments on $36$ state-based and $4$
image-based benchmark tasks demonstrate that the proposed method achieves $1.8
\times$ median improvement in returns and increases success rates by $36\%$.
Website: https://chongyi-zheng.github.io/infom Code:
https://github.com/chongyi-zheng/infom

</details>


### [173] [Enhancing generalizability of model discovery across parameter space with multi-experiment equation learning (ME-EQL)](https://arxiv.org/abs/2506.08916)
*Maria-Veronica Ciocanel,John T. Nardini,Kevin B. Flores,Erica M. Rutter,Suzanne S. Sindi,Alexandria Volkening*

Main category: cs.LG

TL;DR: 本文将方程学习扩展为多实验方程学习（ME - EQL），介绍两种方法并通过模型验证，结果显示可降低误差，OAT ME - EQL泛化性更好。


<details>
  <summary>Details</summary>
Motivation: 基于代理的建模（ABM）计算量大且难分析，传统方程学习（EQL）需大量模拟且泛化性存疑，需改进方法。

Method: 将EQL扩展为ME - EQL，提出OAT ME - EQL（为每个参数集学习模型并插值连接）和ES ME - EQL（跨参数构建统一模型库）两种方法。

Result: 两种方法显著降低从基于代理的模拟中恢复参数的相对误差，OAT ME - EQL在参数空间上的泛化性更好。

Conclusion: 多实验方程学习能增强复杂生物系统学习模型的泛化性和可解释性。

Abstract: Agent-based modeling (ABM) is a powerful tool for understanding
self-organizing biological systems, but it is computationally intensive and
often not analytically tractable. Equation learning (EQL) methods can derive
continuum models from ABM data, but they typically require extensive
simulations for each parameter set, raising concerns about generalizability. In
this work, we extend EQL to Multi-experiment equation learning (ME-EQL) by
introducing two methods: one-at-a-time ME-EQL (OAT ME-EQL), which learns
individual models for each parameter set and connects them via interpolation,
and embedded structure ME-EQL (ES ME-EQL), which builds a unified model library
across parameters. We demonstrate these methods using a birth--death mean-field
model and an on-lattice agent-based model of birth, death, and migration with
spatial structure. Our results show that both methods significantly reduce the
relative error in recovering parameters from agent-based simulations, with OAT
ME-EQL offering better generalizability across parameter space. Our findings
highlight the potential of equation learning from multiple experiments to
enhance the generalizability and interpretability of learned models for complex
biological systems.

</details>


### [174] [BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models](https://arxiv.org/abs/2506.08936)
*Amina Mollaysa,Artem Moskale,Pushpak Pati,Tommaso Mansi,Mangal Prakash,Rui Liao*

Main category: cs.LG

TL;DR: 提出BioLangFusion方法，将预训练的DNA、mRNA和蛋白质语言模型整合为统一分子表示，在五个分子属性预测任务中表现优于单模态基线。


<details>
  <summary>Details</summary>
Motivation: 受分子生物学中心法则启发，期望整合不同模态语言模型以捕获多组学信息。

Method: 在有生物学意义的密码子水平对齐各模态嵌入，研究三种标准融合技术，无需额外预训练或修改基础模型。

Result: 在五个分子属性预测任务中，BioLangFusion优于强单模态基线。

Conclusion: 即使简单融合预训练模型也能以最小开销捕获互补的多组学信息。

Abstract: We present BioLangFusion, a simple approach for integrating pre-trained DNA,
mRNA, and protein language models into unified molecular representations.
Motivated by the central dogma of molecular biology (information flow from gene
to transcript to protein), we align per-modality embeddings at the biologically
meaningful codon level (three nucleotides encoding one amino acid) to ensure
direct cross-modal correspondence. BioLangFusion studies three standard fusion
techniques: (i) codon-level embedding concatenation, (ii) entropy-regularized
attention pooling inspired by multiple-instance learning, and (iii) cross-modal
multi-head attention -- each technique providing a different inductive bias for
combining modality-specific signals. These methods require no additional
pre-training or modification of the base models, allowing straightforward
integration with existing sequence-based foundation models. Across five
molecular property prediction tasks, BioLangFusion outperforms strong unimodal
baselines, showing that even simple fusion of pre-trained models can capture
complementary multi-omic information with minimal overhead.

</details>


### [175] [KARMA: A Multilevel Decomposition Hybrid Mamba Framework for Multivariate Long-Term Time Series Forecasting](https://arxiv.org/abs/2506.08939)
*Hang Ye,Gaoxiang Duan,Haoran Zeng,Yangxin Zhu,Lingxue Meng,Xiaoying Zheng,Yongxin Zhu*

Main category: cs.LG

TL;DR: 提出KARMA模型用于多变量长期高效时间序列预测，实验显示其在准确性和效率上优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分解方法单一且依赖固定规则，Transformer模型计算复杂度高，难以有效处理长序列和复杂动态关系。

Method: 引入KARMA，包含自适应时间通道分解模块（ATCD）动态提取趋势和季节性成分，集成混合频率 - 时间分解模块（HFTD）将序列进一步分解到频域和时域，结合多尺度基于Mamba的KarmaBlock处理全局和局部信息。

Result: 在八个不同领域的真实数据集上实验，KARMA在预测准确性和计算效率上显著优于主流基线方法。

Conclusion: KARMA能有效克服现有方法的局限，实现更高效准确的多变量长期时间序列预测。

Abstract: Multivariate long-term and efficient time series forecasting is a key
requirement for a variety of practical applications, and there are complex
interleaving time dynamics in time series data that require decomposition
modeling. Traditional time series decomposition methods are single and rely on
fixed rules, which are insufficient for mining the potential information of the
series and adapting to the dynamic characteristics of complex series. On the
other hand, the Transformer-based models for time series forecasting struggle
to effectively model long sequences and intricate dynamic relationships due to
their high computational complexity. To overcome these limitations, we
introduce KARMA, with an Adaptive Time Channel Decomposition module (ATCD) to
dynamically extract trend and seasonal components. It further integrates a
Hybrid Frequency-Time Decomposition module (HFTD) to further decompose Series
into frequency-domain and time-domain. These components are coupled with
multi-scale Mamba-based KarmaBlock to efficiently process global and local
information in a coordinated manner. Experiments on eight real-world datasets
from diverse domains well demonstrated that KARMA significantly outperforms
mainstream baseline methods in both predictive accuracy and computational
efficiency. Code and full results are available at this repository:
https://github.com/yedadasd/KARMA

</details>


### [176] [Towards Robust Deep Reinforcement Learning against Environmental State Perturbation](https://arxiv.org/abs/2506.08961)
*Chenxu Wang,Huaping Liu*

Main category: cs.LG

TL;DR: 本文研究深度强化学习中环境状态扰动问题，提出攻击方法和BAT防御框架，实验验证攻击有效性和BAT框架增强鲁棒性的效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习对抗攻击研究很少考虑环境状态扰动，为提高DRL智能体的鲁棒性开展研究。

Method: 提出环境状态扰动问题，引入非目标攻击方法作为校准对手，提出Boosted Adversarial Training (BAT)防御框架，先通过监督学习调整智能体，再用强化学习进行对抗训练。

Result: 实验表明主流智能体在环境状态扰动下易受攻击，提出的攻击方法有效；现有鲁棒强化学习算法可能不适用，BAT框架能显著增强智能体鲁棒性。

Conclusion: BAT框架能有效提高深度强化学习智能体在环境状态扰动下的鲁棒性。

Abstract: Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have
been widely studied in various threat models; however, few consider
environmental state perturbations, which are natural in embodied scenarios. To
improve the robustness of DRL agents, we formulate the problem of environmental
state perturbation, introducing a preliminary non-targeted attack method as a
calibration adversary, and then propose a defense framework, named Boosted
Adversarial Training (BAT), which first tunes the agents via supervised
learning to avoid catastrophic failure and subsequently adversarially trains
the agent with reinforcement learning. Extensive experimental results
substantiate the vulnerability of mainstream agents under environmental state
perturbations and the effectiveness of our proposed attack. The defense results
demonstrate that while existing robust reinforcement learning algorithms may
not be suitable, our BAT framework can significantly enhance the robustness of
agents against environmental state perturbations across various situations.

</details>


### [177] [GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO](https://arxiv.org/abs/2506.08965)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.LG

TL;DR: 提出数据增强扩展框架，让小数据集训练的生成式奖励模型达大规模数据集训练效果，实验证明可提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 提高基于人类反馈的强化学习（RLHF）中奖励模型小样本数据训练的效率和可扩展性。

Method: 引入偏好细化，用思维链采样挖掘偏好关系，采用基于困惑度的评分机制，使用多级直接偏好优化（M - DPO）。

Result: 显著提升数据效率和模型性能，少样本训练的奖励模型能取得与大规模数据集训练相当的结果。

Conclusion: 数据高效策略在推进奖励模型优化方面有潜力，为低资源RLHF应用提供解决方案。

Abstract: The ability to train high-performing reward models with few-shot data is
critical for enhancing the efficiency and scalability of Reinforcement Learning
from Human Feedback (RLHF). We propose a data augmentation and expansion
framework that enables generative reward models trained on small datasets to
achieve comparable performance to those trained on large-scale datasets.
Traditional methods to train a generative reward model, such as Direct
Preference Optimization (DPO), are constrained by inefficiencies in sample
pairing and limited data diversity. This work introduces preference refinement,
which employs Chain-of-Thought (CoT) sampling to uncover diverse and
high-quality preference relationships. It also incorporates a perplexity-based
scoring mechanism to assign nuanced preference levels and utilizes Multi-level
Direct Preference Optimization (M-DPO) to enable the model to capture
finer-grained preference differences between samples. Experimental results
demonstrate that the proposed method significantly enhances data efficiency and
model performance, enabling reward models trained in a few-shot setting to
achieve results on par with those trained on large-scale datasets. This study
underscores the potential of data-efficient strategies in advancing reward
model optimization, offering a robust solution for low-resource RLHF
applications.

</details>


### [178] [Tailored Architectures for Time Series Forecasting: Evaluating Deep Learning Models on Gaussian Process-Generated Data](https://arxiv.org/abs/2506.08977)
*Victoria Hankemeier,Malte Schilling*

Main category: cs.LG

TL;DR: 研究旨在揭示时间序列特征与特定模型的联系，引入新数据集，提出新模型TimeFlex并与现有模型对比。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习时间序列预测模型在特定真实数据上展示效果，未明确数据特征与模型架构优势的关联，需探究两者联系。

Method: 引入用高斯过程生成的新数据集，设计TimeFlex模型，将其与现有模型对比。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及结论，通过对比可深入了解模型在不同时间序列条件下的表现。

Abstract: Developments in Deep Learning have significantly improved time series
forecasting by enabling more accurate modeling of complex temporal dependencies
inherent in sequential data. The effectiveness of such models is often
demonstrated on limited sets of specific real-world data. Although this allows
for comparative analysis, it still does not demonstrate how specific data
characteristics align with the architectural strengths of individual models.
Our research aims at uncovering clear connections between time series
characteristics and particular models. We introduce a novel dataset generated
using Gaussian Processes, specifically designed to display distinct, known
characteristics for targeted evaluations of model adaptability to them.
Furthermore, we present TimeFlex, a new model that incorporates a modular
architecture tailored to handle diverse temporal dynamics, including trends and
periodic patterns. This model is compared to current state-of-the-art models,
offering a deeper understanding of how models perform under varied time series
conditions.

</details>


### [179] [Propositional Logic for Probing Generalization in Neural Networks](https://arxiv.org/abs/2506.08978)
*Anna Langedijk,Jaap Jumelet,Willem Zuidema*

Main category: cs.LG

TL;DR: 研究三种神经架构在命题逻辑任务中的泛化行为，发现标准架构学习逻辑算子系统表示有局限，需更强归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 探究神经网络获取和表示符号规则的能力，当前研究多关注大语言模型能力与失败，本文聚焦三种神经架构在命题逻辑任务的泛化。

Method: 引入现有数据集的平衡扩展，消除表面模式，用于测试模型在未见算子组合上的泛化能力。

Result: 所有模型在分布内表现良好，但对未见模式泛化，尤其涉及否定时是挑战，Transformer 除非引入结构偏置，否则无法组合应用否定。

Conclusion: 标准架构学习逻辑算子系统表示存在持续局限，需要更强归纳偏置支持可靠的基于规则的推理。

Abstract: The extent to which neural networks are able to acquire and represent
symbolic rules remains a key topic of research and debate. Much current work
focuses on the impressive capabilities of large language models, as well as
their often ill-understood failures on a wide range of reasoning tasks. In this
paper, in contrast, we investigate the generalization behavior of three key
neural architectures (Transformers, Graph Convolution Networks and LSTMs) in a
controlled task rooted in propositional logic. The task requires models to
generate satisfying assignments for logical formulas, making it a structured
and interpretable setting for studying compositionality. We introduce a
balanced extension of an existing dataset to eliminate superficial patterns and
enable testing on unseen operator combinations. Using this dataset, we evaluate
the ability of the three architectures to generalize beyond the training
distribution. While all models perform well in-distribution, we find that
generalization to unseen patterns, particularly those involving negation,
remains a significant challenge. Transformers fail to apply negation
compositionally, unless structural biases are introduced. Our findings
highlight persistent limitations in the ability of standard architectures to
learn systematic representations of logical operators, suggesting the need for
stronger inductive biases to support robust rule-based reasoning.

</details>


### [180] [On Finetuning Tabular Foundation Models](https://arxiv.org/abs/2506.08982)
*Ivan Rubachev,Akim Kotelnikov,Nikolay Kartashev*

Main category: cs.LG

TL;DR: 研究表格基础模型TabPFNv2的微调方法及机制，发现全微调是高效实用方案，微调能改进检索预测逻辑，在部分数据集提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有表格基础模型TabPFNv2的最优微调方法及内部机制变化研究不足，且之前研究结果不一致，TabPFNv2架构独特需新研究。

Method: 系统评估不同微调策略，类比检索增强模型研究微调对TabPFNv2内部机制的影响。

Result: 全微调是TabPFNv2最实用方案，微调后模型能更好反映目标相似度，改进预测逻辑，在多数任务提升性能，在部分数据集表现不稳定。

Conclusion: 确定了TabPFNv2的最佳微调策略，解释了微调提升性能的机制，指出其在不同数据集的性能表现。

Abstract: Foundation models are an emerging research direction in tabular deep
learning. Notably, TabPFNv2 recently claimed superior performance over
traditional GBDT-based methods on small-scale datasets using an in-context
learning paradigm, which does not adapt model parameters to target datasets.
However, the optimal finetuning approach for adapting tabular foundational
models, and how this adaptation reshapes their internal mechanisms, remains
underexplored. While prior works studied finetuning for earlier foundational
models, inconsistent findings and TabPFNv2's unique architecture necessitate
fresh investigation. To address these questions, we first systematically
evaluate various finetuning strategies on diverse datasets. Our findings
establish full finetuning as the most practical solution for TabPFNv2 in terms
of time-efficiency and effectiveness. We then investigate how finetuning alters
TabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.
We reveal that the success of finetuning stems from the fact that after
gradient-based adaptation, the dot products of the query-representations of
test objects and the key-representations of in-context training objects more
accurately reflect their target similarity. This improved similarity allows
finetuned TabPFNv2 to better approximate target dependency by appropriately
weighting relevant in-context samples, improving the retrieval-based prediction
logic. From the practical perspective, we managed to finetune TabPFNv2 on
datasets with up to 50K objects, observing performance improvements on almost
all tasks. More precisely, on academic datasets with I.I.D. splits, finetuning
allows TabPFNv2 to achieve state-of-the-art results, while on datasets with
gradual temporal shifts and rich feature sets, TabPFNv2 is less stable and
prior methods remain better.

</details>


### [181] [SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08989)
*Xiao Liang,Zhong-Zhi Li,Yeyun Gong,Yang Wang,Hengyuan Zhang,Yelong Shen,Ying Nian Wu,Weizhu Chen*

Main category: cs.LG

TL;DR: 提出Self - aware Weakness - driven problem Synthesis (SwS)框架用于强化学习训练大语言模型解决数学问题，不依赖外部知识蒸馏，在多基准测试中有性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有高质量数学问题集稀缺，现有问题合成策略未考虑模型能力，导致强化学习效率低。

Method: 引入SwS框架，将模型在强化学习训练中反复学不会的问题定义为弱点，提取核心概念合成新问题进行增强训练。

Result: 在八个主流推理基准测试中，7B和32B模型分别平均有10.0%和7.7%的性能提升。

Conclusion: 该框架能让模型在强化学习中自我识别和解决弱点，实现稳健泛化。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for training large language models (LLMs) on complex reasoning tasks, such as
mathematical problem solving. A prerequisite for the scalability of RLVR is a
high-quality problem set with precise and verifiable answers. However, the
scarcity of well-crafted human-labeled math problems and limited-verification
answers in existing distillation-oriented synthetic datasets limit their
effectiveness in RL. Additionally, most problem synthesis strategies
indiscriminately expand the problem set without considering the model's
capabilities, leading to low efficiency in generating useful questions. To
mitigate this issue, we introduce a Self-aware Weakness-driven problem
Synthesis framework (SwS) that systematically identifies model deficiencies and
leverages them for problem augmentation. Specifically, we define weaknesses as
questions that the model consistently fails to learn through its iterative
sampling during RL training. We then extract the core concepts from these
failure cases and synthesize new problems to strengthen the model's weak areas
in subsequent augmented training, enabling it to focus on and gradually
overcome its weaknesses. Without relying on external knowledge distillation,
our framework enables robust generalization byempowering the model to
self-identify and address its weaknesses in RL, yielding average performance
gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning
benchmarks.

</details>


### [182] [Branched Schrödinger Bridge Matching](https://arxiv.org/abs/2506.09007)
*Sophia Tang,Yinuo Zhang,Alexander Tong,Pranam Chatterjee*

Main category: cs.LG

TL;DR: 提出分支薛定谔桥匹配（BranchSBM）框架解决现有方法在多模态转换上的局限。


<details>
  <summary>Details</summary>
Motivation: 现有方法如流匹配和薛定谔桥匹配只能处理单模态转换，无法捕捉从共同起点到多个不同结果的分支或发散演化。

Method: 引入BranchSBM框架，参数化多个随时间变化的速度场和增长过程。

Result: BranchSBM更具表达能力。

Conclusion: BranchSBM对涉及多路径表面导航、细胞命运分叉建模和模拟细胞对扰动的不同反应等任务至关重要。

Abstract: Predicting the intermediate trajectories between an initial and target
distribution is a central problem in generative modeling. Existing approaches,
such as flow matching and Schr\"odinger Bridge Matching, effectively learn
mappings between two distributions by modeling a single stochastic path.
However, these methods are inherently limited to unimodal transitions and
cannot capture branched or divergent evolution from a common origin to multiple
distinct outcomes. To address this, we introduce Branched Schr\"odinger Bridge
Matching (BranchSBM), a novel framework that learns branched Schr\"odinger
bridges. BranchSBM parameterizes multiple time-dependent velocity fields and
growth processes, enabling the representation of population-level divergence
into multiple terminal distributions. We show that BranchSBM is not only more
expressive but also essential for tasks involving multi-path surface
navigation, modeling cell fate bifurcations from homogeneous progenitor states,
and simulating diverging cellular responses to perturbations.

</details>


### [183] [Effective Data Pruning through Score Extrapolation](https://arxiv.org/abs/2506.09010)
*Sebastian Schmidt,Prasanga Dhungel,Christoffer Löffler,Björn Nieth,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出重要性分数外推框架以解决机器学习模型训练数据修剪技术的效率问题，通过小数据子集预测样本重要性并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有数据修剪技术需全量初始训练来识别可移除样本，对单次训练无效率优势，为解决该问题提出新方法。

Method: 引入重要性分数外推框架，用k近邻和图神经网络两种方法，通过小数据子集学习的模式预测全量数据样本重要性。

Result: 在2种剪枝方法、4种数据集和3种训练范式中验证了方法有效性。

Conclusion: 分数外推是扩展昂贵分数计算方法（如修剪、数据归因等）的有前景方向。

Abstract: Training advanced machine learning models demands massive datasets, resulting
in prohibitive computational costs. To address this challenge, data pruning
techniques identify and remove redundant training samples while preserving
model performance. Yet, existing pruning techniques predominantly require a
full initial training pass to identify removable samples, negating any
efficiency benefits for single training runs. To overcome this limitation, we
introduce a novel importance score extrapolation framework that requires
training on only a small subset of data. We present two initial approaches in
this framework - k-nearest neighbors and graph neural networks - to accurately
predict sample importance for the entire dataset using patterns learned from
this minimal subset. We demonstrate the effectiveness of our approach for 2
state-of-the-art pruning methods (Dynamic Uncertainty and TDDS), 4 different
datasets (CIFAR-10, CIFAR-100, Places-365, and ImageNet), and 3 training
paradigms (supervised, unsupervised, and adversarial). Our results indicate
that score extrapolation is a promising direction to scale expensive score
calculation methods, such as pruning, data attribution, or other tasks.

</details>


### [184] [SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning](https://arxiv.org/abs/2506.09016)
*Ruiqi Zhang,Daman Arora,Song Mei,Andrea Zanette*

Main category: cs.LG

TL;DR: 引入SPEED方法提升大语言模型强化学习训练效率，训练加速2到6倍且不降低准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型强化学习中统一提示采样效率低、计算成本高，需提升训练效率。

Method: 提出Selective Prompting with Efficient Estimation of Difficulty (SPEED)自适应在线强化学习课程，选择中等难度训练示例。

Result: 高效实现使训练加速2到6倍，不降低准确率，无需手动调参，可无缝集成到标准强化学习算法。

Conclusion: 中等难度提示能提高梯度估计器信噪比、加速收敛，SPEED方法有效提升训练效率。

Abstract: Training large language models with reinforcement learning (RL) against
verifiable rewards significantly enhances their reasoning abilities, yet
remains computationally expensive due to inefficient uniform prompt sampling.
We introduce Selective Prompting with Efficient Estimation of Difficulty
(SPEED), an adaptive online RL curriculum that selectively chooses training
examples of intermediate difficulty to maximize learning efficiency.
Theoretically, we establish that intermediate-difficulty prompts improve the
gradient estimator's signal-to-noise ratio, accelerating convergence.
Empirically, our efficient implementation leads to 2x to 6x faster training
without degrading accuracy, requires no manual tuning, and integrates
seamlessly into standard RL algorithms.

</details>


### [185] [Edit Flows: Flow Matching with Edit Operations](https://arxiv.org/abs/2506.09018)
*Marton Havasi,Brian Karrer,Itai Gat,Ricky T. Q. Chen*

Main category: cs.LG

TL;DR: 提出非自回归模型Edit Flows，通过编辑操作定义离散流，训练利用辅助变量，在多任务表现优。


<details>
  <summary>Details</summary>
Motivation: 非自回归模型在生成可变长度序列时有局限，需克服这些问题。

Method: 通过编辑操作在序列上定义离散流，在序列空间的连续时间马尔可夫链中建模操作，训练利用带辅助变量的扩展状态空间。

Result: Edit Flows在图像字幕任务上优于自回归和掩码模型，在文本和代码生成中显著优于掩码构造。

Conclusion: Edit Flows能有效克服非自回归模型局限，在多种序列生成任务表现良好。

Abstract: Autoregressive generative models naturally generate variable-length
sequences, while non-autoregressive models struggle, often imposing rigid,
token-wise structures. We propose Edit Flows, a non-autoregressive model that
overcomes these limitations by defining a discrete flow over sequences through
edit operations-insertions, deletions, and substitutions. By modeling these
operations within a Continuous-time Markov Chain over the sequence space, Edit
Flows enable flexible, position-relative generation that aligns more closely
with the structure of sequence data. Our training method leverages an expanded
state space with auxiliary variables, making the learning process efficient and
tractable. Empirical results show that Edit Flows outperforms both
autoregressive and mask models on image captioning and significantly
outperforms the mask construction in text and code generation.

</details>


### [186] [e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs](https://arxiv.org/abs/2506.09026)
*Amrith Setlur,Matthew Y. R. Yang,Charlie Snell,Jeremy Greer,Ian Wu,Virginia Smith,Max Simchowitz,Aviral Kumar*

Main category: cs.LG

TL;DR: 现有推理模型外推效果不佳，提出e3方法训练大语言模型进行上下文内探索，在AIME'25和HMMT'25得分上表现最佳且能外推至2倍训练令牌预算。


<details>
  <summary>Details</summary>
Motivation: 多数现有推理模型在测试时间缩放范式的外推效果不好，需找到实现外推的方法以提升大语言模型推理能力。

Method: 提出e3方法，包含链接受限能力技能、利用错误轨迹的负梯度、通过特定课程将任务难度与训练令牌预算耦合。

Result: e3方法产生了在AIME'25和HMMT'25得分最佳的1.7B模型，能外推至2倍训练令牌预算，提升了pass@1和pass@k分数。

Conclusion: e3方法可有效使大语言模型实现外推，提升推理性能。

Abstract: Test-time scaling offers a promising path to improve LLM reasoning by
utilizing more compute at inference time; however, the true promise of this
paradigm lies in extrapolation (i.e., improvement in performance on hard
problems as LLMs keep "thinking" for longer, beyond the maximum token budget
they were trained on). Surprisingly, we find that most existing reasoning
models do not extrapolate well. We show that one way to enable extrapolation is
by training the LLM to perform in-context exploration: training the LLM to
effectively spend its test time budget by chaining operations (such as
generation, verification, refinement, etc.), or testing multiple hypotheses
before it commits to an answer. To enable in-context exploration, we identify
three key ingredients as part of our recipe e3: (1) chaining skills that the
base LLM has asymmetric competence in, e.g., chaining verification (easy) with
generation (hard), as a way to implement in-context search; (2) leveraging
"negative" gradients from incorrect traces to amplify exploration during RL,
resulting in longer search traces that chains additional asymmetries; and (3)
coupling task difficulty with training token budget during training via a
specifically-designed curriculum to structure in-context exploration. Our
recipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25
scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not
only attains high pass@1 scores, but also improves pass@k over the base model.

</details>


### [187] [FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed](https://arxiv.org/abs/2506.09034)
*Sizhe Dang,Yangyang Guo,Yanjun Zhao,Haishan Ye,Xiaodong Zheng,Guang Dai,Ivor Tsang*

Main category: cs.LG

TL;DR: 本文提出快速零阶优化器FZOO，解决大语言模型微调GPU内存瓶颈，实验验证其有效性，理论证明其性质，还能与PEFT结合节省内存。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型微调中GPU内存瓶颈，改善零阶优化器速度与内存的权衡问题。

Method: 采用批量单边估计、自适应步长，利用Rademacher随机向量扰动和CUDA并行处理加速计算。

Result: 在多个模型和任务中，FZOO比MeZO准确率平均高3%，前向传播次数少3倍；在RoBERTa - large上准确率提升5.6%，前向传播次数减少18倍，速度接近Adam。

Conclusion: FZOO使单GPU、高速、全参数微调可行，为内存高效预训练指明方向。

Abstract: Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks:
the backward pass of first-order optimizers like Adam increases memory usage to
more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order
(ZO) optimizers avoid this cost by estimating gradients only from forward
passes, yet existing methods like MeZO usually require many more steps to
converge. Can this trade-off between speed and memory in ZO be fundamentally
improved? Normalized-SGD demonstrates strong empirical performance with greater
memory efficiency than Adam. In light of this, we introduce FZOO, a Fast
Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward
passes needed for convergence by employing batched one-sided estimates that
adapt step sizes based on the standard deviation of batch losses. It also
accelerates per-batch computation through the use of Rademacher random vector
perturbations coupled with CUDA's parallel processing. Extensive experiments on
diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3,
across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms
MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For
RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy
and an 18 times reduction in forward passes compared to MeZO, achieving
convergence speeds comparable to Adam. We also provide theoretical analysis
proving FZOO's formal equivalence to a normalized-SGD update rule and its
convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling
even larger memory savings. Overall, our results make single-GPU, high-speed,
full-parameter fine-tuning practical and point toward future work on
memory-efficient pre-training.

</details>


### [188] [The Decoupled Risk Landscape in Performative Prediction](https://arxiv.org/abs/2506.09044)
*Javier Sanguino,Thomas Kehrenberg,Jose A. Lozano,Novi Quadrianto*

Main category: cs.LG

TL;DR: 提出简单解耦风险可视化方法和扩展执行性预测新设置，以研究执行性预测中的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有文献从理论视角研究执行性预测，作者认为可视化损失景观可提供实用见解，补充理论进展。

Method: 引入基于执行性预测两步过程的解耦风险可视化方法，可视化风险景观；在此基础上引入扩展执行性预测新设置。

Result: 利用解耦风险可视化方法提出兴趣点新属性，研究现有算法在现实条件下在风险景观中的表现；提出扩展执行性预测设置。

Conclusion: 可视化方法能为执行性预测研究提供实用见解，扩展设置更符合现实情况。

Abstract: Performative Prediction addresses scenarios where deploying a model induces a
distribution shift in the input data, such as individuals modifying their
features and reapplying for a bank loan after rejection. Literature has had a
theoretical perspective giving mathematical guarantees for convergence (either
to the stable or optimal point). We believe that visualization of the loss
landscape can complement this theoretical advances with practical insights.
Therefore, (1) we introduce a simple decoupled risk visualization method
inspired in the two-step process that performative prediction is. Our approach
visualizes the risk landscape with respect to two parameter vectors: model
parameters and data parameters. We use this method to propose new properties of
the interest points, to examine how existing algorithms traverse the risk
landscape and perform under more realistic conditions, including strategic
classification with non-linear models. (2) Building on this decoupled risk
visualization, we introduce a novel setting - extended Performative Prediction
- which captures scenarios where the distribution reacts to a model different
from the decision-making one, reflecting the reality that agents often lack
full access to the deployed model.

</details>


### [189] [Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation](https://arxiv.org/abs/2506.09046)
*Xiaowen Ma,Chenyang Lin,Yao Zhang,Volker Tresp,Yunpu Ma*

Main category: cs.LG

TL;DR: 提出Agentic Neural Network (ANN)框架解决多LLM协作的静态配置问题，有两阶段优化策略，在四个基准数据集上表现超基线，将开源。


<details>
  <summary>Details</summary>
Motivation: 当前多LLM协作方法依赖静态、手动设计的多智能体配置，有局限性。

Method: 将多智能体协作概念化为分层神经网络架构，每个智能体为节点，层为协作团队。采用两阶段优化策略，前向阶段动态分解任务、构建协作团队，后向阶段通过迭代反馈优化协作。

Result: 在四个基准数据集上，ANN在相同配置下超越领先的多智能体基线，性能持续提升。

Conclusion: ANN为多智能体系统提供了可扩展、数据驱动的框架，结合了LLM协作能力和神经网络的效率与灵活性。

Abstract: Leveraging multiple Large Language Models(LLMs) has proven effective for
addressing complex, high-dimensional tasks, but current approaches often rely
on static, manually engineered multi-agent configurations. To overcome these
constraints, we present the Agentic Neural Network(ANN), a framework that
conceptualizes multi-agent collaboration as a layered neural network
architecture. In this design, each agent operates as a node, and each layer
forms a cooperative "team" focused on a specific subtask. Agentic Neural
Network follows a two-phase optimization strategy: (1) Forward Phase-Drawing
inspiration from neural network forward passes, tasks are dynamically
decomposed into subtasks, and cooperative agent teams with suitable aggregation
methods are constructed layer by layer. (2) Backward Phase-Mirroring
backpropagation, we refine both global and local collaboration through
iterative feedback, allowing agents to self-evolve their roles, prompts, and
coordination. This neuro-symbolic approach enables ANN to create new or
specialized agent teams post-training, delivering notable gains in accuracy and
adaptability. Across four benchmark datasets, ANN surpasses leading multi-agent
baselines under the same configurations, showing consistent performance
improvements. Our findings indicate that ANN provides a scalable, data-driven
framework for multi-agent systems, combining the collaborative capabilities of
LLMs with the efficiency and flexibility of neural network principles. We plan
to open-source the entire framework.

</details>


### [190] [Understanding Task Vectors in In-Context Learning: Emergence, Functionality, and Limitations](https://arxiv.org/abs/2506.09048)
*Yuxin Dong,Jiachen Jiang,Zhihui Zhu,Xia Ning*

Main category: cs.LG

TL;DR: 本文提出线性组合猜想解释任务向量，从理论和实证提供支持，为理解任务向量和上下文学习机制提供新思路。


<details>
  <summary>Details</summary>
Motivation: 现有任务向量虽经验成功，但产生和功能的底层原理不明。

Method: 提出线性组合猜想，通过损失景观分析、实际大语言模型验证、显著性分析和参数可视化等方法。

Result: 发现任务向量在线性变压器中自然出现，预测并证实其在表示高秩映射时的失败，验证通过注入多个任务向量可增强效果。

Conclusion: 研究推进了对任务向量的理解，揭示了基于变压器模型的上下文学习机制。

Abstract: Task vectors offer a compelling mechanism for accelerating inference in
in-context learning (ICL) by distilling task-specific information into a
single, reusable representation. Despite their empirical success, the
underlying principles governing their emergence and functionality remain
unclear. This work proposes the Linear Combination Conjecture, positing that
task vectors act as single in-context demonstrations formed through linear
combinations of the original ones. We provide both theoretical and empirical
support for this conjecture. First, we show that task vectors naturally emerge
in linear transformers trained on triplet-formatted prompts through loss
landscape analysis. Next, we predict the failure of task vectors on
representing high-rank mappings and confirm this on practical LLMs. Our
findings are further validated through saliency analyses and parameter
visualization, suggesting an enhancement of task vectors by injecting multiple
ones into few-shot prompts. Together, our results advance the understanding of
task vectors and shed light on the mechanisms underlying ICL in
transformer-based models.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [191] [A Practical Guide to Tuning Spiking Neuronal Dynamics](https://arxiv.org/abs/2506.08138)
*William Gebhardt,Alexander G. Ororbia,Nathan McDonald,Clare Thiem,Jack Lombardi*

Main category: cs.NE

TL;DR: 研究脉冲神经网络（SNNs）的基本元素及调优方法，聚焦LIF和RAF神经元。


<details>
  <summary>Details</summary>
Motivation: 探究SNNs的基本元素及调优方式。

Method: 研究LIF和RAF神经元的关键方程，探讨超参数值的影响，还讨论输入编码选择和兴奋 - 抑制群体设置等设计元素。

Result: 无明确提及

Conclusion: 无明确提及

Abstract: In this work, we examine fundamental elements of spiking neural networks
(SNNs) as well as how to tune them. Concretely, we focus on two different
foundational neuronal units utilized in SNNs -- the leaky integrate-and-fire
(LIF) and the resonate-and-fire (RAF) neuron. We explore key equations and how
hyperparameter values affect behavior. Beyond hyperparameters, we discuss other
important design elements of SNNs -- the choice of input encoding and the setup
for excitatory-inhibitory populations -- and how these impact LIF and RAF
dynamics.

</details>


### [192] [Efficient Fireworks Algorithm Equipped with an Explosion Mechanism based on Student's T-distribution](https://arxiv.org/abs/2506.08484)
*Cen Shipeng,Tan Ying*

Main category: cs.NE

TL;DR: 提出基于学生t分布的新烟花算法TFWA，实验证明其效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有烟花算法在凸问题上性能不佳，多数变体采样方法在高维时效率低，需要新的优化技术。

Method: 提出基于学生t分布的TFWA算法，利用t分布可调整参数的优势。

Result: 在CEC2013和CEC2017基准测试中，TFWA成为烟花算法最强变体，结果与SOTA相当，在某些多极值场景远超SOTA。

Conclusion: TFWA算法有效，能解决现有烟花算法的问题，在优化问题上表现出色。

Abstract: Many real-world problems can be transformed into optimization problems, which
can be classified into convex and non-convex. Although convex problems are
almost completely studied in theory, many related algorithms to many non-convex
problems do not work well and we need more optimization techniques. As a swarm
intelligence optimization algorithm, the Fireworks Algorithm(FWA) has been
widely studied and applied to many real-world scenarios, even including large
language model fine-tuning. But the current fireworks algorithm still has a
number of problems. Firstly, as a heuristic algorithm, its performance on
convex problems cannot match the SOTA results, and can even be said to be
unsatisfactory; secondly, the sampling methods (explosion) of most FWA variants
are still uniform sampling, which is actually inefficient in high dimensional
cases. This work of ours proposes a new student's t-distribution based
FWA(TFWA) with a solid theoretical foundation, which fully utilizes the
advantage that student's t-distribution can adjust the parameters (degrees of
freedom) and thus adjust the exploitation capability. We have fully
experimented on mainstream benchmarks CEC2013 and CEC2017, which proves that
TFWA not only becomes the strongest variant of the fireworks algorithm, but
also achieves results comparable to SOTA on the test set, and its performance
is far superior to that of the SOTA algorithm in some scenarios with a large
number of extreme points.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [193] [A Systematic Literature Review on Continuous Integration and Deployment (CI/CD) for Secure Cloud Computing](https://arxiv.org/abs/2506.08055)
*Sabbir M. Saleh,Nazim Madhavji,John Steinbacher*

Main category: cs.SE

TL;DR: 本文进行云环境下CI/CD安全的系统文献综述，回顾66篇论文，总结工具、方法和挑战，指出研究差距。


<details>
  <summary>Details</summary>
Motivation: 云环境普及，网络安全成重点，需开展系统文献综述以识别研究差距。

Method: 对66篇论文进行系统文献综述，总结云环境下CI/CD安全相关的工具、方法和挑战。

Result: 介绍了Harbor、SonarQube和GitHub Actions等工具，突出了镜像操纵、未授权访问和弱认证等挑战。

Conclusion: 研究发现工具和实践在CI/CD管道中解决安全问题存在研究差距，需进一步研究改进云安全解决方案。

Abstract: As cloud environments become widespread, cybersecurity has emerged as a top
priority across areas such as networks, communication, data privacy, response
times, and availability. Various sectors, including industries, healthcare, and
government, have recently faced cyberattacks targeting their computing systems.
Ensuring secure app deployment in cloud environments requires substantial
effort. With the growing interest in cloud security, conducting a systematic
literature review (SLR) is critical to identifying research gaps. Continuous
Software Engineering, which includes continuous integration (CI), delivery
(CDE), and deployment (CD), is essential for software development and
deployment. In our SLR, we reviewed 66 papers, summarising tools, approaches,
and challenges related to the security of CI/CD in the cloud. We addressed key
aspects of cloud security and CI/CD and reported on tools such as Harbor,
SonarQube, and GitHub Actions. Challenges such as image manipulation,
unauthorised access, and weak authentication were highlighted. The review also
uncovered research gaps in how tools and practices address these security
issues in CI/CD pipelines, revealing a need for further study to improve
cloud-based security solutions.

</details>


### [194] [A Metrics-Oriented Architectural Model to Characterize Complexity on Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.08153)
*Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: 研究如何有效管理ML - 启用系统的复杂性，提出创建基于指标的架构模型的第一步。


<details>
  <summary>Details</summary>
Motivation: 探究复杂性如何影响ML - 启用系统，有效管理其复杂性。

Method: 引入基于指标的架构模型来表征ML - 启用系统的复杂性，第一步是扩展参考架构以收集指标。

Result: 文中展示了创建基于指标的架构模型的第一步。

Conclusion: 基于指标的架构模型可支持架构决策，为系统的初始和发展提供指导。

Abstract: How can the complexity of ML-enabled systems be managed effectively? The goal
of this research is to investigate how complexity affects ML-Enabled Systems
(MLES). To address this question, this research aims to introduce a
metrics-based architectural model to characterize the complexity of MLES. The
goal is to support architectural decisions, providing a guideline for the
inception and growth of these systems. This paper showcases the first step for
creating the metrics-based architectural model: an extension of a reference
architecture that can describe MLES to collect their metrics.

</details>


### [195] [Worst-Case Symbolic Constraints Analysis and Generalisation with Large Language Models](https://arxiv.org/abs/2506.08171)
*Daniel Koh,Yannic Noller,Corina S. Pasareanu,Adrians Skapars,Youcheng Sun*

Main category: cs.SE

TL;DR: 本文研究大语言模型对程序最坏情况执行进行符号约束分析的能力，通过符号推理引导微调改进其能力，实验表明模型表现出色，说明大语言模型能进行深度符号推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂符号推理任务方面研究较少，旨在将大语言模型与符号推理方法相结合，评估其对程序最坏情况的理解能力。

Method: 定义最坏情况符号约束分析问题，评估现有大语言模型在该任务上的表现，通过基于SMT约束求解和特制数据集进行符号推理引导微调。

Result: 求解器对齐模型WARP - 1.0 - 3B始终优于规模匹配甚至更大的基线模型，3B的大语言模型可通过强化学习方法恢复确定算法最坏情况行为的约束。

Conclusion: 大语言模型能够进行更深入的符号推理，支持基于神经网络的学习与严格程序分析的形式化方法更紧密结合。

Abstract: Large language models (LLMs) have been successfully applied to a variety of
coding tasks, including code generation, completion, and repair. However, more
complex symbolic reasoning tasks remain largely unexplored by LLMs. This paper
investigates the capacity of LLMs to reason about worst-case executions in
programs through symbolic constraints analysis, aiming to connect LLMs and
symbolic reasoning approaches. Specifically, we define and address the problem
of worst-case symbolic constraints analysis as a measure to assess the
comprehension of LLMs. We evaluate the performance of existing LLMs on this
novel task and further improve their capabilities through symbolic
reasoning-guided fine-tuning, grounded in SMT (Satisfiability Modulo Theories)
constraint solving and supported by a specially designed dataset of symbolic
constraints. Experimental results show that our solver-aligned model,
WARP-1.0-3B, consistently surpasses size-matched and even much larger
baselines, demonstrating that a 3B LLM can recover the very constraints that
pin down an algorithm's worst-case behaviour through reinforcement learning
methods. These findings suggest that LLMs are capable of engaging in deeper
symbolic reasoning, supporting a closer integration between neural
network-based learning and formal methods for rigorous program analysis.

</details>


### [196] [Repeton: Structured Bug Repair with ReAct-Guided Patch-and-Test Cycles](https://arxiv.org/abs/2506.08173)
*Nguyen Phu Vinh,Anh Chung Hoang,Chris Ngo,Truong-Son Hy*

Main category: cs.SE

TL;DR: 介绍Repeton框架，它利用大语言模型在真实Git仓库进行精确自动代码操作，经评估表现良好，为自主调试提供实用路径。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用于复杂软件工程任务时精度低、可解释性有限，需要更好的解决方案。

Method: 提出Repeton框架，采用结构化的补丁测试管道，迭代诊断问题、提出代码更改并通过自动化测试验证，由轻量级启发式和开发工具引导，不依赖基于嵌入的检索系统。

Result: 在SWE - bench Lite基准测试中，与基于RAG的方法相比，在补丁有效性和可解释性方面表现良好。

Conclusion: 通过将软件工程任务分解为模块化、可验证的阶段，Repeton为可扩展和透明的自主调试提供了实用路径。

Abstract: Large Language Models (LLMs) have shown strong capabilities in code
generation and comprehension, yet their application to complex software
engineering tasks often suffers from low precision and limited
interpretability. We present Repeton, a fully open-source framework that
leverages LLMs for precise and automated code manipulation in real-world Git
repositories. Rather than generating holistic fixes, Repeton operates through a
structured patch-and-test pipeline: it iteratively diagnoses issues, proposes
code changes, and validates each patch through automated testing. This stepwise
process is guided by lightweight heuristics and development tools, avoiding
reliance on embedding-based retrieval systems. Evaluated on the SWE-bench Lite
benchmark, our method shows good performance compared to RAG-based methods in
both patch validity and interpretability. By decomposing software engineering
tasks into modular, verifiable stages, Repeton provides a practical path toward
scalable and transparent autonomous debugging.

</details>


### [197] [MBTModelGenerator: A software tool for reverse engineering of Model-based Testing (MBT) models from clickstream data of web applications](https://arxiv.org/abs/2506.08179)
*Sasidhar Matta,Vahid Garousi*

Main category: cs.SE

TL;DR: 开发开源工具从用户与Web应用交互的点击流数据自动生成MBT模型，降低手动创建模型的工作量并可立即执行测试，报告介绍了工具相关信息。


<details>
  <summary>Details</summary>
Motivation: 解决自动测试中创建测试模型和套件劳动密集的问题，降低MBT采用门槛。

Method: 开发工具捕获UI事件，将其转换为状态转换模型，并以与GraphWalker MBT工具兼容的格式导出。

Result: 开发出可自动从点击流数据生成MBT模型的开源工具，无需手动创建模型即可立即执行测试。

Conclusion: 该工具通过利用实际使用行为和减少前期建模依赖，降低了MBT采用门槛。

Abstract: Automated testing has become a standard practice in software engineering, yet
the creation of test models and suites remains labor-intensive. To reduce this
effort, we developed an open-source tool that automatically generates
Model-Based Testing (MBT) models from clickstream data collected during user
interaction with web applications. The tool captures UI events, transforms them
into state-transition models, and exports the result in a format compatible
with the GraphWalker MBT tool. This enables immediate test execution without
the need for manual model creation. The approach lowers the barrier to MBT
adoption by leveraging actual usage behavior and reducing the reliance on
upfront modeling. This technical report documents the system requirements,
design decisions, implementation details, testing process, and empirical
evaluation of the tool, which is publicly available as open-source.

</details>


### [198] [Understanding Software Engineering Agents Through the Lens of Traceability: An Empirical Study](https://arxiv.org/abs/2506.08311)
*Ira Ceka,Saurabh Pujar,Shyam Ramji,Luca Buratti,Gail Kaiser,Baishakhi Ray*

Main category: cs.SE

TL;DR: 文章通过执行跟踪对软件工程代理行为进行系统研究，提出决策路径分类法，分析核心组件、测试生成影响及补丁差异，为代理设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程代理内部决策工作流程不明，深入了解可提升其可靠性和效率。

Method: 通过执行跟踪对代表性代理进行研究，提出决策路径分类法，深入分析核心组件，研究测试生成影响并开展代码克隆分析。

Result: 提出决策路径分类法，确定三个核心组件，研究测试生成影响，分析代理生成和开发者编写补丁差异。

Conclusion: 研究为代理设计提供新见解，有助于构建更有效且符合人类开发实践的代理。

Abstract: With the advent of large language models (LLMs), software engineering agents
(SWE agents) have emerged as a powerful paradigm for automating a range of
software tasks -- from code generation and repair to test case synthesis. These
agents operate autonomously by interpreting user input and responding to
environmental feedback. While various agent architectures have demonstrated
strong empirical performance, the internal decision-making worfklows that drive
their behavior remain poorly understood. Deeper insight into these workflows
hold promise for improving both agent reliability and efficiency. In this work,
we present the first systematic study of SWE agent behavior through the lens of
execution traces. Our contributions are as follows: (1) we propose the first
taxonomy of decision-making pathways across five representative agents; (2)
using this taxonomy, we identify three core components essential to agent
success -- bug localization, patch generation, and reproduction test generation
-- and study each in depth; (3) we study the impact of test generation on
successful patch production; and analyze strategies that can lead to successful
test generation; (4) we further conduct the first large-scale code clone
analysis comparing agent-generated and developer-written patches and provide a
qualitative study revealing structural and stylistic differences in patch
content. Together, these findings offer novel insights into agent design and
open avenues for building agents that are both more effective and more aligned
with human development practices.

</details>


### [199] [Detecting State Manipulation Vulnerabilities in Smart Contracts Using LLM and Static Analysis](https://arxiv.org/abs/2506.08561)
*Hao Wu,Haijun Wang,Shangwang Li,Yin Wu,Ming Fan,Yitao Zhao,Ting Liu*

Main category: cs.SE

TL;DR: 提出PriceSleuth方法，结合LLM和静态分析检测DeFi智能合约价格操纵攻击，展示初步实验结果并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: DeFi协议流行，状态操纵攻击常见，攻击者操纵价格获利，需方法检测价格操纵攻击。

Method: PriceSleuth先识别DeFi合约中价格计算核心逻辑函数，引导LLM定位价格计算代码语句，对价格变量进行反向依赖分析，利用价格变量传播分析辅助LLM检测恶意利用情况。

Result: 展示了初步实验结果证明PriceSleuth的有效性。

Conclusion: 提出PriceSleuth可有效检测价格操纵攻击，还给出了未来研究方向。

Abstract: An increasing number of DeFi protocols are gaining popularity, facilitating
transactions among multiple anonymous users. State Manipulation is one of the
notorious attacks in DeFi smart contracts, with price variable being the most
commonly exploited state variable-attackers manipulate token prices to gain
illicit profits. In this paper, we propose PriceSleuth, a novel method that
leverages the Large Language Model (LLM) and static analysis to detect Price
Manipulation (PM) attacks proactively. PriceSleuth firstly identifies core
logic function related to price calculation in DeFi contracts. Then it guides
LLM to locate the price calculation code statements. Secondly, PriceSleuth
performs backward dependency analysis of price variables, instructing LLM in
detecting potential price manipulation. Finally, PriceSleuth utilizes
propagation analysis of price variables to assist LLM in detecting whether
these variables are maliciously exploited. We presented preliminary
experimental results to substantiate the effectiveness of PriceSleuth . And we
outline future research directions for PriceSleuth.

</details>


### [200] [Evaluating the Performance and Efficiency of Sentence-BERT for Code Comment Classification](https://arxiv.org/abs/2506.08581)
*Fabian C. Peña,Steffen Herbold*

Main category: cs.SE

TL;DR: 评估Sentence - BERT用于多标签代码注释分类任务，在推理时平衡性能与效率，实现合理F1提升和最小效率下降。


<details>
  <summary>Details</summary>
Motivation: 在多标签代码注释分类任务中，在推理时控制效率约束的同时最大化分类性能。

Method: 使用13216个标注注释句子的数据集，微调Sentence - BERT模型并结合不同分类头识别注释类型。

Result: 大模型F1表现更好，小模型效率高，实现F1提高0.0346，运行时间增加1.4倍，GFLOPS增加2.1倍的平衡。

Conclusion: 在多标签代码注释分类中可实现性能提升与效率下降间的平衡。

Abstract: This work evaluates Sentence-BERT for a multi-label code comment
classification task seeking to maximize the classification performance while
controlling efficiency constraints during inference. Using a dataset of 13,216
labeled comment sentences, Sentence-BERT models are fine-tuned and combined
with different classification heads to recognize comment types. While larger
models outperform smaller ones in terms of F1, the latter offer outstanding
efficiency, both in runtime and GFLOPS. As result, a balance between a
reasonable F1 improvement (+0.0346) and a minimal efficiency degradation (+1.4x
in runtime and +2.1x in GFLOPS) is reached.

</details>


### [201] [RE-oriented Model Development with LLM Support and Deduction-based Verification](https://arxiv.org/abs/2506.08606)
*Radoslaw Klimek*

Main category: cs.SE

TL;DR: 提出结合高级建模技术、大语言模型和形式验证的需求工程框架，助力从设计到实现的转换。


<details>
  <summary>Details</summary>
Motivation: 提升软件开发中需求工程阶段的质量。

Method: 提出专注于特定UML图的框架，整合大语言模型和逻辑推理引擎，自动将行为模型转为形式逻辑规范并进行演绎验证。

Result: 框架能实现可视化，自动生成程序骨架。

Conclusion: 该框架可简化从设计到实现的过程，提高软件开发效率。

Abstract: The requirements engineering (RE) phase is pivotal in developing high-quality
software. Integrating advanced modelling techniques with large language models
(LLMs) and formal verification in a logical style can significantly enhance
this process. We propose a comprehensive framework that focuses on specific
Unified Modelling Language (UML) diagrams for preliminary system development.
This framework offers visualisations at various modelling stages and seamlessly
integrates large language models and logical reasoning engines. The behavioural
models generated with the assistance of LLMs are automatically translated into
formal logical specifications. Deductive formal verification ensures that
logical requirements and interrelations between software artefacts are
thoroughly addressed. Ultimately, the framework facilitates the automatic
generation of program skeletons, streamlining the transition from design to
implementation.

</details>


### [202] [Logic Mining from Process Logs: Towards Automated Specification and Verification](https://arxiv.org/abs/2506.08628)
*Radoslaw Klimek,Julia Witek*

Main category: cs.SE

TL;DR: 本文提出从工作流挖掘的过程模型生成逻辑规范的方法，在通用和实际事件日志上评估，研究数据质量影响并验证多种逻辑属性，结果支持方法在现实场景的适用性。


<details>
  <summary>Details</summary>
Motivation: 在复杂系统中，手动构建逻辑规范耗时且易出错，需要自动化推导逻辑规范的方法。

Method: 结合基于模式的翻译和自动推理技术，从工作流挖掘的过程模型生成逻辑规范，在通用和实际事件日志上评估，并研究数据质量的影响，用自动定理证明器验证逻辑属性。

Result: 结果支持该方法在现实场景的适用性及其与实证软件工程实践集成的潜力。

Conclusion: 该方法适用于现实场景，有潜力集成到实证软件工程实践中。

Abstract: Logical specifications play a key role in the formal analysis of behavioural
models. Automating the derivation of such specifications is particularly
valuable in complex systems, where manual construction is time-consuming and
error-prone. This article presents an approach for generating logical
specifications from process models discovered via workflow mining, combining
pattern-based translation with automated reasoning techniques. In contrast to
earlier work, we evaluate the method on both general-purpose and real-case
event logs, enabling a broader empirical assessment. The study examines the
impact of data quality, particularly noise, on the structure and testability of
generated specifications. Using automated theorem provers, we validate a
variety of logical properties, including satisfiability, internal consistency,
and alignment with predefined requirements. The results support the
applicability of the approach in realistic settings and its potential
integration into empirical software engineering practices.

</details>


### [203] [Proceedings of the 23rd International Overture Workshop](https://arxiv.org/abs/2506.08680)
*Hugo Daniel Macedo,Ken Pierce*

Main category: cs.SE

TL;DR: 本文介绍2025年6月11日第23届国际Overture研讨会情况，涉及维也纳开发方法（VDM）、开源项目Overture等相关工具和形式化方法。


<details>
  <summary>Details</summary>
Motivation: 展示VDM和Overture相关技术的发展和应用，促进学界和业界交流。

Method: 举办研讨会，展示新兴技术进展。

Result: 研讨会提供了VDM/Overture新兴技术的更新，包括协作基础设施、网络物理系统的协作建模和联合仿真。

Conclusion: VDM和Overture相关技术在建模和分析领域有持续发展，为系统开发提供支持。

Abstract: This volume contains the papers presented at the 23rd International Overture
Workshop, held on the 11th of June 2025. This event was the latest in a series
of workshops around the Vienna Development Method (VDM), the open-source
project Overture, and related tools and formalisms. VDM is one of the longest
established formal methods for systems development. A lively community of
researchers and practitioners has grown up in academia and industry has grown
around the modelling languages (VDM-SL, VDM++, VDM-RT, CML) and tools
(VDMTools, Overture, Crescendo, Symphony, the INTO-CPS chain, and ViennaTalk).
Together, these provide a platform for work on modelling and analysis
technology that includes static and dynamic analysis, test generation,
execution support, and model checking. This workshop provided updates on the
emerging technology of VDM/Overture, including collaboration infrastructure,
collaborative modelling and co-simulation for Cyber-Physical Systems.

</details>


### [204] [Causality-aware Safety Testing for Autonomous Driving Systems](https://arxiv.org/abs/2506.08688)
*Wenbing Tang,Mingfei Cheng,Renzhi Wang,Yuan Zhou,Chengwei Liu,Yang Liu,Zuohua Ding*

Main category: cs.SE

TL;DR: 提出因果感知模糊测试技术Causal - Fuzzer对自动驾驶系统进行高效全面测试，在行业级ADS Apollo上评估显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统测试方法未能捕捉输入场景、运动命令和系统违规之间复杂的相互关系，导致测试覆盖存在差距，需新方法解决。

Method: 构建因果图对多样性间的相互关系建模，提出基于因果的反馈机制和因果驱动的变异策略。

Result: 在行业级ADS Apollo上评估，Causal - Fuzzer在识别违规多样性、测试充分性和检测关键场景效率上显著优于现有方法。

Conclusion: Causal - Fuzzer能更高效全面地对自动驾驶系统进行测试。

Abstract: Simulation-based testing is essential for evaluating the safety of Autonomous
Driving Systems (ADSs). Comprehensive evaluation requires testing across
diverse scenarios that can trigger various types of violations under different
conditions. While existing methods typically focus on individual diversity
metrics, such as input scenarios, ADS-generated motion commands, and system
violations, they often fail to capture the complex interrelationships among
these elements. This oversight leads to gaps in testing coverage, potentially
missing critical issues in the ADS under evaluation. However, quantifying these
interrelationships presents a significant challenge. In this paper, we propose
a novel causality-aware fuzzing technique, Causal-Fuzzer, to enable efficient
and comprehensive testing of ADSs by exploring causally diverse scenarios. The
core of Causal-Fuzzer is constructing a causal graph to model the
interrelationships among the diversities of input scenarios, ADS motion
commands, and system violations. Then the causal graph will guide the process
of critical scenario generation. Specifically, Causal-Fuzzer proposes (1) a
causality-based feedback mechanism that quantifies the combined diversity of
test scenarios by assessing whether they activate new causal relationships, and
(2) a causality-driven mutation strategy that prioritizes mutations on input
scenario elements with higher causal impact on ego action changes and violation
occurrence, rather than treating all elements equally. We evaluated
Causal-Fuzzer on an industry-grade ADS Apollo, with a high-fidelity. Our
empirical results demonstrate that Causal-Fuzzer significantly outperforms
existing methods in (1) identifying a greater diversity of violations, (2)
providing enhanced testing sufficiency with improved coverage of causal
relationships, and (3) achieving greater efficiency in detecting the first
critical scenarios.

</details>


### [205] [Do Generative AI Tools Ensure Green Code? An Investigative Study](https://arxiv.org/abs/2506.08790)
*Samarth Sikand,Rohit Mehra,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: 本文探讨AI生成代码的可持续性，研究三个流行生成式AI工具，发现其默认代码不环保，需深入研究和补救策略。


<details>
  <summary>Details</summary>
Motivation: 软件可持续性受关注，但缺乏对AI生成代码可持续性的研究，需了解其环保程度。

Method: 对ChatGPT、BARD和Copilot三个流行生成式AI工具生成代码的可持续性进行早期调查。

Result: 这些工具生成的代码在多个规则和场景下呈现默认的非绿色行为。

Conclusion: 需要进一步深入调查并制定有效的补救策略。

Abstract: Software sustainability is emerging as a primary concern, aiming to optimize
resource utilization, minimize environmental impact, and promote a greener,
more resilient digital ecosystem. The sustainability or "greenness" of software
is typically determined by the adoption of sustainable coding practices. With a
maturing ecosystem around generative AI, many software developers now rely on
these tools to generate code using natural language prompts. Despite their
potential advantages, there is a significant lack of studies on the
sustainability aspects of AI-generated code. Specifically, how environmentally
friendly is the AI-generated code based upon its adoption of sustainable coding
practices? In this paper, we present the results of an early investigation into
the sustainability aspects of AI-generated code across three popular generative
AI tools - ChatGPT, BARD, and Copilot. The results highlight the default
non-green behavior of tools for generating code, across multiple rules and
scenarios. It underscores the need for further in-depth investigations and
effective remediation strategies.

</details>


### [206] [Towards a Knowledge Base of Common Sustainability Weaknesses in Green Software Development](https://arxiv.org/abs/2506.08812)
*Priyavanshi Pathania,Rohit Mehra,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: 面对气候危机，开发可持续软件系统很重要，但缺乏分析代码可持续性的自动化工具，本文提出开发标准知识库并通过实验指出现有软件弱点知识不能简单重标记为可持续性相关。


<details>
  <summary>Details</summary>
Motivation: 气候危机下，开发可持续软件系统至关重要，开发者需自动化工具分析代码可持续性，但缺乏标准化知识基础的相关工具。

Method: 提出开发代码中常见可持续性弱点标准知识库的初步方法，并进行初步实验。

Result: 实验表明现有软件弱点知识不能不经过大量尽职调查就简单重标记为可持续性相关。

Conclusion: 呼吁在这个具有生态意义的领域进行更多探索。

Abstract: With the climate crisis looming, engineering sustainable software systems
become crucial to optimize resource utilization, minimize environmental impact,
and foster a greener, more resilient digital ecosystem. For developers, getting
access to automated tools that analyze code and suggest sustainabilityrelated
optimizations becomes extremely important from a learning and implementation
perspective. However, there is currently a dearth of such tools due to the lack
of standardized knowledge, which serves as the foundation of these tools. In
this paper, we motivate the need for the development of a standard knowledge
base of commonly occurring sustainability weaknesses in code, and propose an
initial way of doing that. Furthermore, through preliminary experiments, we
demonstrate why existing knowledge regarding software weaknesses cannot be
re-tagged "as is" to sustainability without significant due diligence, thereby
urging further explorations in this ecologically significant domain.

</details>


### [207] [On The Impact of Merge Request Deviations on Code Review Practices](https://arxiv.org/abs/2506.08860)
*Samah Kansab,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 文章指出工业MR工作流存在偏离标准审查流程情况，识别了偏差类别，提出检测方法，排除偏差可提升ML模型性能，给出相关贡献以助优化审查。


<details>
  <summary>Details</summary>
Motivation: 工业MR工作流常偏离标准审查流程，很多MR有非审查目的，忽视这些偏差会影响分析和ML模型。

Method: 识别七种偏差类别，提出少样本学习检测方法。

Result: 偏差占37.02%的MR，检测方法准确率91%，排除偏差后53.33%的预测审查完成时间的ML模型性能提升，特征重要性有显著变化。

Conclusion: 研究给出MR偏差分类、AI检测方法及对ML审查分析影响的实证，有助于从业者优化审查工作和获取可靠见解。

Abstract: Code review is a key practice in software engineering, ensuring quality and
collaboration. However, industrial Merge Request (MR) workflows often deviate
from standardized review processes, with many MRs serving non-review purposes
(e.g., drafts, rebases, or dependency updates). We term these cases deviations
and hypothesize that ignoring them biases analytics and undermines ML models
for review analysis.
  We identify seven deviation categories, occurring in 37.02% of MRs, and
propose a few-shot learning detection method (91% accuracy). By excluding
deviations, ML models predicting review completion time improve performance in
53.33% of cases (up to 2.25x) and exhibit significant shifts in feature
importance (47% overall, 60% top-*k*).
  Our contributions include: (1) a taxonomy of MR deviations, (2) an AI-driven
detection approach, and (3) empirical evidence of their impact on ML-based
review analytics. This work aids practitioners in optimizing review efforts and
ensuring reliable insights.

</details>


### [208] [AdaDec: Uncertainty-Guided Adaptive Decoding for LLM-based Code Generation](https://arxiv.org/abs/2506.08980)
*Kaifeng He,Mingwei Liu,Chong Wang,Zike Li,Yanlin Wang,Xin Peng,Zibin Zheng*

Main category: cs.SE

TL;DR: 论文指出代码生成在解码时选token敏感，标准策略有不足，提出AdaDec框架，实验显示其能提升准确率、降低成本。


<details>
  <summary>Details</summary>
Motivation: 标准解码策略在代码生成时忽略代码特定的不确定性模式，导致性能不佳，许多生成错误源于高不确定性步骤的排序错误。

Method: 提出AdaDec，一种基于token不确定性（香农熵）的自适应解码框架，学习模型特定的不确定性阈值，在高不确定性时应用基于前瞻的重排序策略。

Result: 在HumanEval和MBPP基准测试中，AdaDec相比贪婪解码使Pass@1准确率最多提高15.5%，优于或匹配束搜索，且通过高效选择性暂停降低计算成本和延迟。

Conclusion: 基于不确定性的自适应解码对提高基于大语言模型的代码生成的可靠性和效率有前景。

Abstract: Code generation with large language models (LLMs) is highly sensitive to
token selection during decoding, particularly at uncertain decision points that
influence program logic. While standard strategies like greedy and beam search
treat all tokens uniformly, they overlook code-specific uncertainty patterns,
leading to suboptimal performance. This paper presents an empirical study
revealing that many generation errors stem from ranking mistakes at
high-uncertainty steps, where the correct token is present but not top-ranked.
  Motivated by these findings, we propose AdaDec, an uncertainty-guided
adaptive decoding framework that integrates a token-level pause-then-rerank
mechanism driven by token uncertainty (Shannon entropy). AdaDec learns
model-specific uncertainty thresholds and applies a lookahead-based reranking
strategy when uncertainty is high. Experiments on HumanEval and MBPP benchmarks
show that AdaDec improves Pass@1 accuracy by up to 15.5% over greedy decoding,
outperforms or matches beam search, and reduces computational cost and latency
through efficient, selective pausing. Our results highlight the promise of
uncertainty-aware adaptive decoding for improving the reliability and
efficiency of LLM-based code generation.

</details>


### [209] [Boosting Rust Unit Test Coverage through Hybrid Program Analysis and Large Language Models](https://arxiv.org/abs/2506.09002)
*Bei Chu,Yang Feng,Kui Liu,Hange Shi,Zifan Nan,Zhaoqiang Guo,Baowen Xu*

Main category: cs.SE

TL;DR: 本文提出PALM方法利用大语言模型生成高覆盖率单元测试，在10个开源Rust项目中实验，显著提升覆盖率，证明结合程序分析与AI的有效性。


<details>
  <summary>Details</summary>
Motivation: 经典SBST方法和基于符号执行方法难处理复杂程序单元，现有基于大语言模型生成测试用例方法依赖固定提示，编译成功率和覆盖率较低。

Method: 进行程序分析识别函数分支条件并组合成路径约束，用约束和上下文信息构造提示引导大语言模型生成单元测试。

Result: 两到三小时内显著提升测试覆盖率，部分项目整体覆盖率提升超50%，平均覆盖率达75.77%；提交91个单元测试，80个被接受，5个被拒，6个待审。

Conclusion: 集成程序分析与AI有效，为自动化软件测试研究开辟新途径。

Abstract: Unit testing is essential for ensuring software reliability and correctness.
Classic Search-Based Software Testing (SBST) methods and concolic
execution-based approaches for generating unit tests often fail to achieve high
coverage due to difficulties in handling complex program units, such as
branching conditions and external dependencies. Recent work has increasingly
utilized large language models (LLMs) to generate test cases, improving the
quality of test generation by providing better context and correcting errors in
the model's output. However, these methods rely on fixed prompts, resulting in
relatively low compilation success rates and coverage. This paper presents
PALM, an approach that leverages large language models (LLMs) to enhance the
generation of high-coverage unit tests. PALM performs program analysis to
identify branching conditions within functions, which are then combined into
path constraints. These constraints and relevant contextual information are
used to construct prompts that guide the LLMs in generating unit tests. We
implement the approach and evaluate it in 10 open-source Rust crates.
Experimental results show that within just two or three hours, PALM can
significantly improves test coverage compared to classic methods, with
increases in overall project coverage exceeding 50% in some instances and its
generated tests achieving an average coverage of 75.77%, comparable to human
effort (71.30%), highlighting the potential of LLMs in automated test
generation. We submitted 91 PALM-generated unit tests targeting new code. Of
these submissions, 80 were accepted, 5 were rejected, and 6 remain pending
review. The results demonstrate the effectiveness of integrating program
analysis with AI and open new avenues for future research in automated software
testing.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [210] [EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements](https://arxiv.org/abs/2506.08762)
*Issa Sugiura,Takashi Ishida,Taro Makino,Chieko Tazuke,Takanori Nakagawa,Kosuke Nakago,David Ha*

Main category: q-fin.ST

TL;DR: 因缺少金融数据集阻碍LLM在金融分析领域发展，本文推出EDINET - Bench基准评估LLM，实验显示LLM应用于金融有挑战，相关数据和代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决日本金融数据集稀缺问题，推动LLM在金融分析领域的发展和评估。

Method: 从日本EDINET下载过去10年的年报构建EDINET - Bench基准，并自动分配对应评估任务的标签。

Result: 即使是最先进的LLM在欺诈检测和收益预测的二元分类中仅比逻辑回归略好。

Conclusion: 将LLM应用于现实金融应用存在重大挑战，需要进行特定领域的调整，同时公开数据和代码以促进未来研究。

Abstract: Financial analysis presents complex challenges that could leverage large
language model (LLM) capabilities. However, the scarcity of challenging
financial datasets, particularly for Japanese financial data, impedes academic
innovation in financial analytics. As LLMs advance, this lack of accessible
research resources increasingly hinders their development and evaluation in
this specialized domain. To address this gap, we introduce EDINET-Bench, an
open-source Japanese financial benchmark designed to evaluate the performance
of LLMs on challenging financial tasks including accounting fraud detection,
earnings forecasting, and industry prediction. EDINET-Bench is constructed by
downloading annual reports from the past 10 years from Japan's Electronic
Disclosure for Investors' NETwork (EDINET) and automatically assigning labels
corresponding to each evaluation task. Our experiments reveal that even
state-of-the-art LLMs struggle, performing only slightly better than logistic
regression in binary classification for fraud detection and earnings
forecasting. These results highlight significant challenges in applying LLMs to
real-world financial applications and underscore the need for domain-specific
adaptation. Our dataset, benchmark construction code, and evaluation code is
publicly available to facilitate future research in finance with LLMs.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [211] [Price Discovery in Cryptocurrency Markets](https://arxiv.org/abs/2506.08718)
*Juan Plazuelo Pascual,Carlos Tardon Rubio,Juan Toro Cebada,Angel Hernando Veciana*

Main category: q-fin.TR

TL;DR: 本文通过对比中心化与去中心化交易所、现货与期货市场，分析加密货币市场的价格发现，先研究以太坊后应用类似方法研究比特币，发现中心化市场在以太坊价格发现中通常占主导，期货市场整体占优但高波动期结果不一。


<details>
  <summary>Details</summary>
Motivation: 分析加密货币市场的价格发现机制，对比不同类型市场在价格发现中的作用。

Method: 运用Hasbrouck信息份额、Gonzalo和Granger的永久 - 临时分解以及Hayashi - Yoshida估计量，对不同市场类型进行实证分析。

Result: 中心化市场在以太坊价格发现中通常占主导；期货市场整体在价格发现中占优，但高波动期结果不一。

Conclusion: 研究结果对交易者和机构在流动性、套利和市场效率方面有重要启示，可用于衡量改良自动做市商的表现，理解去中心化和中心化结构的相互作用。

Abstract: This document analyzes price discovery in cryptocurrency markets by comparing
centralized and decentralized exchanges, as well as spot and futures markets.
The study focuses first on Ethereum (ETH) and then applies a similar approach
to Bitcoin (BTC). Chapter 1 outlines the theoretical framework, emphasizing the
structural differences between centralized exchanges and decentralized finance
mechanisms, especially Automated Market Makers (AMMs). It also explains how to
construct an order book from a liquidity pool in a decentralized setting for
comparison with centralized exchanges. Chapter 2 describes the methodological
tools used: Hasbrouck's Information Share, Gonzalo and Granger's
Permanent-Transitory decomposition, and the Hayashi-Yoshida estimator. These
are applied to explore lead-lag dynamics, cointegration, and price discovery
across market types. Chapter 3 presents the empirical analysis. For ETH, it
compares price dynamics on Binance and Uniswap v2 over a one-year period,
focusing on five key events in 2024. For BTC, it analyzes the relationship
between spot and futures prices on the CME. The study estimates lead-lag
effects and cointegration in both cases. Results show that centralized markets
typically lead in ETH price discovery. In futures markets, while they tend to
lead overall, high-volatility periods produce mixed outcomes. The findings have
key implications for traders and institutions regarding liquidity, arbitrage,
and market efficiency. Various metrics are used to benchmark the performance of
modified AMMs and to understand the interaction between decentralized and
centralized structures.

</details>


### [212] [Optimal hedging of an informed broker facing many traders](https://arxiv.org/abs/2506.08992)
*Philippe Bergault,Pierre Cardaliaguet,Wenbin Yan*

Main category: q-fin.TR

TL;DR: 研究金融市场中知情经纪商与多个交易员的最优对冲策略，用平均场博弈方法得出均衡策略并分析收敛性。


<details>
  <summary>Details</summary>
Motivation: 探究金融市场中知情经纪商与多个交易员互动时的最优对冲策略。

Method: 采用平均场博弈方法，考虑经纪商具有资产价格漂移的独家信息及交易员交易活动对市场价格的影响。

Result: 得出经纪商和交易员的均衡策略，经纪商的最优策略是Stackelberg均衡；分析了有限参与者模型的平均场极限，证明交易员数量增多时收敛到平均场解。

Conclusion: 揭示了经纪商和交易员互动的复杂动态，通过理论框架和方法得到相关策略及收敛结果。

Abstract: This paper investigates the optimal hedging strategies of an informed broker
interacting with multiple traders in a financial market. We develop a
theoretical framework in which the broker, possessing exclusive information
about the drift of the asset's price, engages with traders whose trading
activities impact the market price. Using a mean-field game approach, we derive
the equilibrium strategies for both the broker and the traders, illustrating
the intricate dynamics of their interactions. The broker's optimal strategy
involves a Stackelberg equilibrium, where the broker leads and the traders
follow. Our analysis also addresses the mean field limit of finite-player
models and shows the convergence to the mean-field solution as the number of
traders becomes large.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [213] [Physics-Informed Teleconnection-Aware Transformer for Global Subseasonal-to-Seasonal Forecasting](https://arxiv.org/abs/2506.08049)
*Tengfei Lyu,Weijia Zhang,Hao Liu*

Main category: stat.ML

TL;DR: 提出TelePiT深度学习架构用于改进全球S2S预测，实验显示其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有S2S预测方法难以明确建模关键物理过程和遥相关，影响预测效果。

Method: 引入TelePiT架构，包含球面谐波嵌入、多尺度物理信息神经ODE和遥相关感知Transformer三个关键组件。

Result: TelePiT显著优于现有数据驱动基线和数值天气预报系统，如2米温度RMSE降低57.7%。

Conclusion: TelePiT能有效提升全球S2S预测性能。

Abstract: Subseasonal-to-seasonal (S2S) forecasting, which predicts climate conditions
from several weeks to months in advance, presents significant challenges due to
the chaotic dynamics of atmospheric systems and complex interactions across
multiple scales. Current approaches often fail to explicitly model underlying
physical processes and teleconnections that are crucial at S2S timescales. We
introduce TelePiT, a novel deep learning architecture that enhances global S2S
forecasting through integrated multi-scale physics and teleconnection
awareness. Our approach consists of three key components: (1) Spherical
Harmonic Embedding, which accurately encodes global atmospheric variables onto
spherical geometry; (2) Multi-Scale Physics-Informed Neural ODE, which
explicitly captures atmospheric physical processes across multiple learnable
frequency bands; (3) Teleconnection-Aware Transformer, which models critical
global climate interactions through tactfully injecting teleconnection patterns
into the self-attention. Extensive experiments demonstrate that TelePiT
significantly outperforms state-of-the-art data-driven baselines and
operational numerical weather prediction systems, with remarkable improvements
for atmospheric variables including a 57.7% reduction in RMSE for 2-meter
temperature compared to previous best models.

</details>


### [214] [WWAggr: A Window Wasserstein-based Aggregation for Ensemble Change Point Detection](https://arxiv.org/abs/2506.08066)
*Alexander Stepikin,Evgenia Romanenkova,Alexey Zaytsev*

Main category: stat.ML

TL;DR: 研究高维数据流变点检测，指出标准聚合技术欠佳，提出基于Wasserstein距离的WWAggr聚合方法，还解决阈值选择问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界高维变点检测因数据模式复杂和假设不成立而具挑战性，现有基于独立深度神经网络的检测器质量欠佳，标准预测聚合技术不理想。

Method: 引入基于Wasserstein距离的WWAggr任务特定的集成聚合方法。

Result: 该方法具有通用性，能有效处理各种深度变点检测模型的集成。

Conclusion: 提出的新方法优于标准聚合技术，还解决了长期存在的变点检测决策阈值选择问题。

Abstract: Change Point Detection (CPD) aims to identify moments of abrupt distribution
shifts in data streams. Real-world high-dimensional CPD remains challenging due
to data pattern complexity and violation of common assumptions. Resorting to
standalone deep neural networks, the current state-of-the-art detectors have
yet to achieve perfect quality. Concurrently, ensembling provides more robust
solutions, boosting the performance. In this paper, we investigate ensembles of
deep change point detectors and realize that standard prediction aggregation
techniques, e.g., averaging, are suboptimal and fail to account for problem
peculiarities. Alternatively, we introduce WWAggr -- a novel task-specific
method of ensemble aggregation based on the Wasserstein distance. Our procedure
is versatile, working effectively with various ensembles of deep CPD models.
Moreover, unlike existing solutions, we practically lift a long-standing
problem of the decision threshold selection for CPD.

</details>


### [215] [Constrained Pareto Set Identification with Bandit Feedback](https://arxiv.org/abs/2506.08127)
*Cyrille Kone,Emilie Kaufmann,Laura Richert*

Main category: stat.ML

TL;DR: 本文解决多变量老虎机环境下可行性约束下的帕累托集识别问题，提出算法优于现有方法，证明信息论下界，且样本复杂度接近最优，有实验支持。


<details>
  <summary>Details</summary>
Motivation: 解决多变量老虎机环境下，在可行性约束条件下识别帕累托集的问题。

Method: 引入一种新算法，用于固定置信度识别，并证明信息论下界。

Result: 新算法显著优于类似竞赛算法和两阶段方法，样本复杂度接近最优。

Conclusion: 提出的算法有效解决了带约束的帕累托集识别问题，理论结果得到实验验证。

Abstract: In this paper, we address the problem of identifying the Pareto Set under
feasibility constraints in a multivariate bandit setting. Specifically, given a
$K$-armed bandit with unknown means $\mu_1, \dots, \mu_K \in \mathbb{R}^d$, the
goal is to identify the set of arms whose mean is not uniformly worse than that
of another arm (i.e., not smaller for all objectives), while satisfying some
known set of linear constraints, expressing, for example, some minimal
performance on each objective. Our focus lies in fixed-confidence
identification, for which we introduce an algorithm that significantly
outperforms racing-like algorithms and the intuitive two-stage approach that
first identifies feasible arms and then their Pareto Set. We further prove an
information-theoretic lower bound on the sample complexity of any algorithm for
constrained Pareto Set identification, showing that the sample complexity of
our approach is near-optimal. Our theoretical results are supported by an
extensive empirical evaluation on a series of benchmarks.

</details>


### [216] [Model-Free Kernel Conformal Depth Measures Algorithm for Uncertainty Quantification in Regression Models in Separable Hilbert Spaces](https://arxiv.org/abs/2506.08325)
*Marcos Matabuena,Rahul Ghosal,Pavlo Mozharovskyi,Oscar Hernan Madrid Padilla,Jukka-Pekka Onnela*

Main category: stat.ML

TL;DR: 提出基于条件深度度量的无模型不确定性量化算法，用于在可分希尔伯特空间中定义预测和容忍区域，评估了有限样本性能并通过数字健康应用展示实用性。


<details>
  <summary>Details</summary>
Motivation: 将深度度量整合到回归建模以提供预测区域的研究不足，需填补该空白。

Method: 提出基于条件深度度量（条件核均值嵌入和综合深度度量）的算法，引入共形预测变体。

Result: 建立了条件和无条件一致性结果，在某些同方差设置中有快速收敛率，通过模拟研究评估有限样本性能。

Conclusion: 所提算法在定义预测和容忍区域方面有效，在数字健康应用中有实际意义。

Abstract: Depth measures are powerful tools for defining level sets in emerging,
non--standard, and complex random objects such as high-dimensional multivariate
data, functional data, and random graphs. Despite their favorable theoretical
properties, the integration of depth measures into regression modeling to
provide prediction regions remains a largely underexplored area of research. To
address this gap, we propose a novel, model-free uncertainty quantification
algorithm based on conditional depth measures--specifically, conditional kernel
mean embeddings and an integrated depth measure. These new algorithms can be
used to define prediction and tolerance regions when predictors and responses
are defined in separable Hilbert spaces. The use of kernel mean embeddings
ensures faster convergence rates in prediction region estimation. To enhance
the practical utility of the algorithms with finite samples, we also introduce
a conformal prediction variant that provides marginal, non-asymptotic
guarantees for the derived prediction regions. Additionally, we establish both
conditional and unconditional consistency results, as well as fast convergence
rates in certain homoscedastic settings. We evaluate the finite--sample
performance of our model in extensive simulation studies involving various
types of functional data and traditional Euclidean scenarios. Finally, we
demonstrate the practical relevance of our approach through a digital health
application related to physical activity, aiming to provide personalized
recommendations

</details>


### [217] [Asymptotic Normality of Infinite Centered Random Forests -Application to Imbalanced Classification](https://arxiv.org/abs/2506.08548)
*Moria Mayala,Erwan Scornet,Charles Tillier,Olivier Wintenberger*

Main category: stat.ML

TL;DR: 研究在不平衡数据分类任务中，以Centered Random Forests（CRF）为分类器，对重平衡数据集训练的理论分析，提出去偏估计器IS - ICRF并证明其优势。


<details>
  <summary>Details</summary>
Motivation: 解决分类任务中不平衡数据问题，研究对重平衡数据集训练分类器的理论情况。

Method: 对无限CRF建立中心极限定理（CLT），基于重要性抽样（IS）方法提出去偏估计器IS - ICRF。

Result: 证明CRF在重平衡数据集训练有偏差可去除，IS - ICRF满足以预测函数值为中心的CLT，在高不平衡设置下相比原数据训练的ICRF有方差减小。

Conclusion: 理论分析表明对重平衡数据集训练随机森林（后去偏）比用原数据有优势，实验显示结果对Breiman随机森林也有效。

Abstract: Many classification tasks involve imbalanced data, in which a class is
largely underrepresented. Several techniques consists in creating a rebalanced
dataset on which a classifier is trained. In this paper, we study theoretically
such a procedure, when the classifier is a Centered Random Forests (CRF). We
establish a Central Limit Theorem (CLT) on the infinite CRF with explicit rates
and exact constant. We then prove that the CRF trained on the rebalanced
dataset exhibits a bias, which can be removed with appropriate techniques.
Based on an importance sampling (IS) approach, the resulting debiased
estimator, called IS-ICRF, satisfies a CLT centered at the prediction function
value. For high imbalance settings, we prove that the IS-ICRF estimator enjoys
a variance reduction compared to the ICRF trained on the original data.
Therefore, our theoretical analysis highlights the benefits of training random
forests on a rebalanced dataset (followed by a debiasing procedure) compared to
using the original data. Our theoretical results, especially the variance rates
and the variance reduction, appear to be valid for Breiman's random forests in
our experiments.

</details>


### [218] [Flexible and Efficient Drift Detection without Labels](https://arxiv.org/abs/2506.08734)
*Nelvin Tan,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: stat.ML

TL;DR: 提出无标签场景下灵活高效的概念漂移检测算法，结合新框架，在计算约束下统计功效优于已知方法，模拟显示性能良好。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型广泛应用，早期检测概念漂移很重要，现有研究多为监督场景，在无即时真实标签的大数据集上控制误报有挑战。

Method: 使用经典统计过程控制，提出无标签场景下概念漂移检测算法，引入新的漂移检测框架并将算法融入。

Result: 在计算约束下，该算法统计功效优于已知方法，数值模拟显示有良好性能。

Conclusion: 提出的算法和框架在无标签场景下检测概念漂移有效。

Abstract: Machine learning models are being increasingly used to automate decisions in
almost every domain, and ensuring the performance of these models is crucial
for ensuring high quality machine learning enabled services. Ensuring concept
drift is detected early is thus of the highest importance. A lot of research on
concept drift has focused on the supervised case that assumes the true labels
of supervised tasks are available immediately after making predictions.
Controlling for false positives while monitoring the performance of predictive
models used to make inference from extremely large datasets periodically, where
the true labels are not instantly available, becomes extremely challenging. We
propose a flexible and efficient concept drift detection algorithm that uses
classical statistical process control in a label-less setting to accurately
detect concept drifts. We shown empirically that under computational
constraints, our approach has better statistical power than previous known
methods. Furthermore, we introduce a new drift detection framework to model the
scenario of detecting drift (without labels) given prior detections, and show
our how our drift detection algorithm can be incorporated effectively into this
framework. We demonstrate promising performance via numerical simulations.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [219] [Massive parallelization of projection-based depths](https://arxiv.org/abs/2506.08262)
*Leonardo Leone,Pavlo Mozharovskyi,David Bounie*

Main category: stat.CO

TL;DR: 提出基于RRS的算法框架实现投影深度大规模并行化，在GPU上加速显著，方法适合大规模应用且相关算法有Python库支持。


<details>
  <summary>Details</summary>
Motivation: 解决高维空间数据深度计算难题。

Method: 提出基于精炼随机搜索（RRS）的算法框架。

Result: 在GPU上实现高达7000倍的加速，合成数据上精度提升、运行时间减少。

Conclusion: 该方法适合大规模应用，且有Python库方便实现和拓展。

Abstract: This article introduces a novel methodology for the massive parallelization
of projection-based depths, addressing the computational challenges of data
depth in high-dimensional spaces. We propose an algorithmic framework based on
Refined Random Search (RRS) and demonstrate significant speedup (up to 7,000
times faster) on GPUs. Empirical results on synthetic data show improved
precision and reduced runtime, making the method suitable for large-scale
applications. The RRS algorithm (and other depth functions) are available in
the Python-library data-depth (https://data-depth.github.io/) with ready-to-use
tools to implement and to build upon this work.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [220] [Inverse Design in Distributed Circuits Using Single-Step Reinforcement Learning](https://arxiv.org/abs/2506.08029)
*Jiayu Li,Masood Mortazavi,Ning Yan,Yihong Ma,Reza Zafarani*

Main category: eess.SY

TL;DR: 本文提出DCIDA框架，学习目标传递函数的近似最优设计采样策略，实验表明其比现有方法减少设计误差，在复杂传递函数场景表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有分布式电路逆设计探索方法难以适应现实中不可微评估程序、不同拓扑和近连续布局空间的需求。

Method: 提出DCIDA框架，通过从策略生成的联合训练条件分布中采样，在复合单步动作中决定所有设计因素，并利用单射相互依赖“映射”将采样设计动作转换为物理表示。

Result: DCIDA基于Transformer的策略网络相比现有方法显著降低设计误差，在复杂传递函数情况下拟合度更好。

Conclusion: DCIDA框架在分布式电路逆设计中有效，能解决现有方法的局限性。

Abstract: The goal of inverse design in distributed circuits is to generate
near-optimal designs that meet a desirable transfer function specification.
Existing design exploration methods use some combination of strategies
involving artificial grids, differentiable evaluation procedures, and specific
template topologies. However, real-world design practices often require
non-differentiable evaluation procedures, varying topologies, and
near-continuous placement spaces. In this paper, we propose DCIDA, a design
exploration framework that learns a near-optimal design sampling policy for a
target transfer function. DCIDA decides all design factors in a compound
single-step action by sampling from a set of jointly-trained conditional
distributions generated by the policy. Utilizing an injective interdependent
``map", DCIDA transforms raw sampled design ``actions" into uniquely equivalent
physical representations, enabling the framework to learn the conditional
dependencies among joint ``raw'' design decisions. Our experiments demonstrate
DCIDA's Transformer-based policy network achieves significant reductions in
design error compared to state-of-the-art approaches, with significantly better
fit in cases involving more complex transfer functions.

</details>


### [221] [Feasibility Study of CNNs and MLPs for Radiation Heat Transfer in 2-D Furnaces with Spectrally Participative Gases](https://arxiv.org/abs/2506.08033)
*Axel TahmasebiMoradi,Vincent Ren,Benjamin Le-Creurer,Chetra Mang*

Main category: eess.SY

TL;DR: 引入CNN和MLP构建替代模型以降低辐射传热数值模拟计算成本，对比二者性能。


<details>
  <summary>Details</summary>
Motivation: 降低数值模拟的计算成本。

Method: 用CNN和MLP构建替代模型，创建精确数据集，用Optuna优化超参数，对比CNN和MLP性能，分析数据集大小对模型的影响。

Result: 两种架构较传统求解器显著提速且误差可接受，CNN在精度上更优，对超参数变化更稳健稳定。

Conclusion: CNN和MLP构建的替代模型可有效降低计算成本，CNN性能更优。

Abstract: Aiming to reduce the computational cost of numerical simulations, a
convolutional neural network (CNN) and a multi-layer perceptron (MLP) are
introduced to build a surrogate model to approximate radiative heat transfer
solutions in a 2-D walled domain with participative gases. The originality of
this work lays in the adaptation of the inputs of the problem (gas and wall
properties) in order to fit with the CNN architecture, more commonly used for
image processing. Two precision datasets have been created with the classical
solver, ICARUS2D, that uses the discrete transfer radiation method with the
statistical narrow bands model. The performance of the CNN architecture is
compared to a more classical MLP architecture in terms of speed and accuracy.
Thanks to Optuna, all results are obtained using the optimized hyper parameters
networks. The results show a significant speedup with industrially acceptable
relative errors compared to the classical solver for both architectures.
Additionally, the CNN outperforms the MLP in terms of precision and is more
robust and stable to changes in hyper-parameters. A performance analysis on the
dataset size of the samples have also been carried out to gain a deeper
understanding of the model behavior.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [222] [Learning-Based Multiuser Scheduling in MIMO-OFDM Systems with Hybrid Beamforming](https://arxiv.org/abs/2506.08263)
*Pouya Agheli,Tugce Kobal,François Durand,Matthew Andrews*

Main category: cs.IT

TL;DR: 研究MIMO - OFDM混合波束赋形系统多用户调度问题，提出不同方法并分析性能与复杂度权衡。


<details>
  <summary>Details</summary>
Motivation: 混合波束赋形系统复用增益有限，改善调度对提升频谱效率和系统长期性能至关重要。

Method: 采用双时间尺度协议，长时尺度分配模拟波束，短时尺度调度用户和设计数字预编码器，提出贪婪、排序等组合解决方案和机器学习方法。

Result: 数值结果突出了所提方法在性能和复杂度之间的权衡。

Conclusion: 方法的选择取决于给定场景的具体标准。

Abstract: We investigate the multiuser scheduling problem in multiple-input
multiple-output (MIMO) systems using orthogonal frequency division multiplexing
(OFDM) and hybrid beamforming in which a base station (BS) communicates with
multiple users over millimeter wave (mmWave) channels in the downlink. Improved
scheduling is critical for enhancing spectral efficiency and the long-term
performance of the system from the perspective of proportional fairness (PF)
metric in hybrid beamforming systems due to its limited multiplexing gain. Our
objective is to maximize PF by properly designing the analog and digital
precoders within the hybrid beamforming and selecting the users subject to the
number of radio frequency (RF) chains. Leveraging the characteristics of mmWave
channels, we apply a two-timescale protocol. On a long timescale, we assign an
analog beam to each user. Scheduling the users and designing the digital
precoder are done accordingly on a short timescale. To conduct scheduling, we
propose combinatorial solutions, such as greedy and sorting algorithms,
followed by a machine learning (ML) approach. Our numerical results highlight
the trade-off between the performance and complexity of the proposed
approaches. Consequently, we show that the choice of approach depends on the
specific criteria within a given scenario.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [223] [Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers](https://arxiv.org/abs/2506.08043)
*Ashkan Shahbazi,Kyvia Pereira,Jon S. Heiselman,Elaheh Akbari,Annie C. Benson,Sepehr Seifi,Xinyuan Liu,Garrison L. Johnston,Erwin Terpstra,Anne Draaisma,Jan-Jaap Severes,Jie Ying Wu,Nabil Simaan,Michael L. Miga,Soheil Kolouri*

Main category: cs.GR

TL;DR: 提出基于物理信息的神经模拟器实现软组织变形实时逼真模拟，结合Kelvinlet先验和FEM模拟提升效果。


<details>
  <summary>Details</summary>
Motivation: 实现手术机器人和医学训练中软组织变形的快速准确模拟。

Method: 将基于Kelvinlet的先验知识集成到神经模拟器中，结合大规模有限元方法（FEM）模拟线性和非线性软组织响应。

Result: 在不同架构上提高了神经网络预测的准确性和物理一致性，保持低延迟，并能高精度模拟腹腔镜组织抓取工具的手术操作。

Conclusion: 基于Kelvinlet的学习是用于手术应用中实时、物理感知软组织模拟的强大有效策略。

Abstract: Fast and accurate simulation of soft tissue deformation is a critical factor
for surgical robotics and medical training. In this paper, we introduce a novel
physics-informed neural simulator that approximates soft tissue deformations in
a realistic and real-time manner. Our framework integrates Kelvinlet-based
priors into neural simulators, making it the first approach to leverage
Kelvinlets for residual learning and regularization in data-driven soft tissue
modeling. By incorporating large-scale Finite Element Method (FEM) simulations
of both linear and nonlinear soft tissue responses, our method improves neural
network predictions across diverse architectures, enhancing accuracy and
physical consistency while maintaining low latency for real-time performance.
We demonstrate the effectiveness of our approach by performing accurate
surgical maneuvers that simulate the use of standard laparoscopic tissue
grasping tools with high fidelity. These results establish Kelvinlet-augmented
learning as a powerful and efficient strategy for real-time, physics-aware soft
tissue simulation in surgical applications.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [224] [Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers](https://arxiv.org/abs/2506.08966)
*Marek Kadlčík,Michal Štefánik,Timothee Mickus,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: 现有预训练语言模型有算术错误，提出新探测技术证明模型精确表示数字，该技术准确性可解释算术错误并能缓解问题。


<details>
  <summary>Details</summary>
Motivation: 现有工作在从模型表示中探测数值方面成效有限，且先前探测方法不适用于具有正弦模式的数字嵌入。

Method: 提出一种新的探测技术，从输入嵌入中解码数值。

Result: 在一系列开源语言模型中以近乎完美的精度解码数值，证明预训练后语言模型能精确表示数字。

Conclusion: 探测技术的准确性可解释语言模型在基本算术上的大部分错误，对齐嵌入模式可缓解这些错误。

Abstract: Pretrained language models (LMs) are prone to arithmetic errors. Existing
work showed limited success in probing numeric values from models'
representations, indicating that these errors can be attributed to the inherent
unreliability of distributionally learned embeddings in representing exact
quantities. However, we observe that previous probing methods are inadequate
for the emergent structure of learned number embeddings with sinusoidal
patterns.
  In response, we propose a novel probing technique that decodes numeric values
from input embeddings with near-perfect accuracy across a range of open-source
LMs. This proves that after the sole pre-training, LMs represent numbers with
remarkable precision. Finally, we find that the embeddings' preciseness judged
by our probe's accuracy explains a large portion of LM's errors in elementary
arithmetic, and show that aligning the embeddings with the pattern discovered
by our probe can mitigate these errors.

</details>


### [225] [Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning](https://arxiv.org/abs/2506.08354)
*Yiqun Sun,Qiang Huang,Anthony K. H. Tung,Jun Yu*

Main category: cs.CL

TL;DR: 呼吁文本嵌入研究社区超越表层语义，将隐式语义作为核心建模目标，并提出范式转变建议。


<details>
  <summary>Details</summary>
Motivation: 当前文本嵌入模型多关注表层语义，在处理需要解释推理、说话者立场或社会意义的任务时表现不佳，与语言学理论强调的隐式语义不符。

Method: 通过试点研究凸显当前模型在隐式语义任务上的不足，提出应优先使用更多样化且有语言学基础的训练数据、设计评估深层语义理解的基准，并将隐式意义作为核心建模目标。

Result: 试点研究表明即使是最先进的模型在隐式语义任务上仅比简单基线略好。

Conclusion: 文本嵌入研究需进行范式转变，以更好地适应现实世界语言的复杂性。

Abstract: This position paper argues that the text embedding research community should
move beyond surface meaning and embrace implicit semantics as a central
modeling goal. Text embedding models have become foundational in modern NLP,
powering a wide range of applications and drawing increasing research
attention. Yet, much of this progress remains narrowly focused on surface-level
semantics. In contrast, linguistic theory emphasizes that meaning is often
implicit, shaped by pragmatics, speaker intent, and sociocultural context.
Current embedding models are typically trained on data that lacks such depth
and evaluated on benchmarks that reward the capture of surface meaning. As a
result, they struggle with tasks requiring interpretive reasoning, speaker
stance, or social meaning. Our pilot study highlights this gap, showing that
even state-of-the-art models perform only marginally better than simplistic
baselines on implicit semantics tasks. To address this, we call for a paradigm
shift: embedding research should prioritize more diverse and linguistically
grounded training data, design benchmarks that evaluate deeper semantic
understanding, and explicitly frame implicit meaning as a core modeling
objective, better aligning embeddings with real-world language complexity.

</details>


### [226] [Low-resource domain adaptation while minimizing energy and hardware resource consumption](https://arxiv.org/abs/2506.08433)
*Hernán Maina,Nicolás Wolovick,Luciana Benotti*

Main category: cs.CL

TL;DR: 论文探讨不同数值精度和数据并行策略对训练速度和模型准确性的影响，以促进低资源环境下的领域适应。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型成本高且存在文化价值倾向问题，领域适应策略虽有前景但计算成本高，特别是对缺乏大规模基础设施的研究团队而言，需解决低资源环境下领域适应问题。

Method: 评估不同数值精度和数据并行化策略对训练速度和模型准确性的影响。

Result: 未提及具体结果。

Conclusion: 研究结果适用于能源效率、可访问性或硬件可用性受限的场景。

Abstract: Training Large Language Models (LLMs) is costly in terms of energy, hardware,
and annotated data, often resulting in a positionality rooted in predominant
cultures and values (Santy et al., 2023). Domain adaptation has emerged as a
promising strategy to better align models with diverse cultural and value
contexts (Hershcovich et al., 2022), but its computational cost remains a
significant barrier, particularly for research groups lacking access to
large-scale infrastructure. In this paper, we evaluate how the use of different
numerical precisions and data parallelization strategies impacts both training
speed (as a proxy to energy and hardware consumption) and model accuracy, with
the goal of facilitating domain adaptation in low-resource environments. Our
findings are relevant to any setting where energy efficiency, accessibility, or
limited hardware availability are key concerns.

</details>


### [227] [Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$](https://arxiv.org/abs/2506.08479)
*Chihiro Taguchi,Seiji Maekawa,Nikita Bhutani*

Main category: cs.CL

TL;DR: 提出Adaptive - k检索方法解决检索增强生成和长上下文语言模型在问答中最优外部上下文检索问题，在基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有固定检索大小方法有浪费或遗漏问题，自适应方法在聚合问答中表现不佳，需更好方法。

Method: 提出Adaptive - k检索方法，基于查询与候选段落相似度分数分布自适应选择段落数量，无需模型微调等。

Result: 在事实类和聚合问答基准测试中，匹配或超越固定k基线，使用更少token检索到70%相关段落，提升多个模型准确性。

Conclusion: 动态调整上下文大小可使问答更高效准确。

Abstract: Retrieval-augmented generation (RAG) and long-context language models (LCLMs)
both address context limitations of LLMs in open-domain question answering
(QA). However, optimal external context to retrieve remains an open problem:
fixing the retrieval size risks either wasting tokens or omitting key evidence.
Existing adaptive methods like Self-RAG and Self-Route rely on iterative LLM
prompting and perform well on factoid QA, but struggle with aggregation QA,
where the optimal context size is both unknown and variable. We present
Adaptive-$k$ retrieval, a simple and effective single-pass method that
adaptively selects the number of passages based on the distribution of the
similarity scores between the query and the candidate passages. It does not
require model fine-tuning, extra LLM inferences or changes to existing
retriever-reader pipelines. On both factoid and aggregation QA benchmarks,
Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x
fewer tokens than full-context input, yet still retrieves 70% of relevant
passages. It improves accuracy across five LCLMs and two embedding models,
highlighting that dynamically adjusting context size leads to more efficient
and accurate QA.

</details>


### [228] [Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure](https://arxiv.org/abs/2506.08713)
*Fariz Ikhwantri,Dusica Marijan*

Main category: cs.CL

TL;DR: 提出基于NLI的合规检测方法EXCLAIM，利用LLM生成保证案例，引入相关指标，通过案例证明NLI方法在自动化合规检测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决保证案例有效性检查过程中法律和技术文本复杂、需模型解释以及保证案例数据获取有限等挑战。

Method: 提出EXCLAIM方法，将保证案例结构构建为多跳推理，用LLM生成保证案例，引入衡量覆盖度和结构一致性的指标。

Result: 通过GDPR要求生成保证案例的多跳推理任务案例，证明了方法的有效性。

Conclusion: NLI-based方法在自动化监管合规过程中有潜力。

Abstract: Ensuring complex systems meet regulations typically requires checking the
validity of assurance cases through a claim-argument-evidence framework. Some
challenges in this process include the complicated nature of legal and
technical texts, the need for model explanations, and limited access to
assurance case data. We propose a compliance detection approach based on
Natural Language Inference (NLI): EXplainable CompLiance detection with
Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the
claim-argument-evidence structure of an assurance case as a multi-hop inference
for explainable and traceable compliance detection. We address the limited
number of assurance cases by generating them using large language models
(LLMs). We introduce metrics that measure the coverage and structural
consistency. We demonstrate the effectiveness of the generated assurance case
from GDPR requirements in a multi-hop inference task as a case study. Our
results highlight the potential of NLI-based approaches in automating the
regulatory compliance process.

</details>


### [229] [Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement](https://arxiv.org/abs/2506.00160)
*Qihui Fan,Enfu Nan,Wenbo Li,Lei Lu,Pu Zhao,Yanzhi Wang*

Main category: cs.CL

TL;DR: 随着大语言模型发展，狼人杀等社交推理游戏系统受益。本文提出基于LLM且适配TTS模型的狼人杀游戏系统，认为强大LLM下额外组件不必要。


<details>
  <summary>Details</summary>
Motivation: 提升基于LLM代理的狼人杀等社交推理游戏中人类玩家的体验，改进现有狼人杀游戏体验实现方式。

Method: 提出一种新颖且简单的基于LLM的狼人杀游戏系统，使用经过调整的文本转语音（TTS）模型以增强与各种LLM模型的兼容性。

Result: 未提及明确结果。

Conclusion: 随着LLM推理能力不断增强，在狼人杀游戏中额外组件将变得不必要。

Abstract: The growing popularity of social deduction game systems for both business
applications and AI research has greatly benefited from the rapid advancements
in Large Language Models (LLMs), which now demonstrate stronger reasoning and
persuasion capabilities. Especially with the raise of DeepSeek R1 and V3
models, LLMs should enable a more engaging experience for human players in
LLM-agent-based social deduction games like Werewolf. Previous works either
fine-tuning, advanced prompting engineering, or additional experience pool to
achieve engaging text-format Werewolf game experience. We propose a novel yet
straightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS)
models designed for enhanced compatibility with various LLM models, and
improved user engagement. We argue with ever enhancing LLM reasoning, extra
components will be unnecessary in the case of Werewolf.

</details>


### [230] [Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework](https://arxiv.org/abs/2506.05695)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: 提出POCL课程学习框架解决大模型知识蒸馏问题，提升蒸馏模型性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型知识蒸馏方法训练时难以防止学生模型分布显著偏移，导致灾难性遗忘等问题。

Method: 提出POCL框架，含难度测量器和训练调度器，按难度逐步引入样本并调整损失函数温度。

Result: 在指令跟随场景实验中，POCL提升了不同白盒KD方法和模型家族的蒸馏学生模型性能。

Conclusion: 排序训练样本在大模型知识蒸馏中有效，展示了在KD过程中组织训练数据可提升蒸馏大模型稳定性和性能。

Abstract: Knowledge Distillation (KD) compresses large language models (LLMs) by
transferring the teacher model's capabilities to a smaller student model,
reducing inference cost and memory usage while maintaining performance.
However, existing KD methods for LLMs often fail to prevent significant shifts
in the student model's distribution during training, leading to issues such as
catastrophic forgetting, mode collapse, and training-inference mismatch. To
address these challenges, we propose a novel, plug-in curriculum learning
framework inspired by the strength training principle of "progressive overload"
(POCL), which can be seamlessly integrated into existing white-box KD
approaches with minimal computational overhead. The framework comprises two
core components: (1) a difficulty measurer that ranks and partitions training
samples from easy to hard, and (2) a training scheduler that incrementally
introduces these subsets into the distillation process at fixed intervals while
applying loss functions with progressively rising temperatures. By starting
with the easiest samples and progressively increasing the difficulty, the
approach enhances both the stability and efficiency of learning. Extensive
experiments in instruction-following settings demonstrate that POCL
consistently improves the performance of distilled student models across
various white-box KD methods and model families. Our findings highlight the
effectiveness of sorted training samples in KD for LLMs. More generally, our
work demonstrates how to structure training data within the KD process to
enhance the stability and performance of distilled LLMs.

</details>


### [231] [Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models](https://arxiv.org/abs/2506.08147)
*Muhammad Usman,Muhammad Ahmad,M. Shahiki Tash,Irina Gelbukh,Rolando Quintero Tellez,Grigori Sidorov*

Main category: cs.CL

TL;DR: 文章引入三语推文数据集，结合注意力层和多种模型检测多语言仇恨言论，取得良好效果，提升检测性能，助力打造安全数字社区。


<details>
  <summary>Details</summary>
Motivation: 社交媒体仇恨言论危害大，乌尔都语仇恨言论检测用翻译方法研究不足，需填补空白。

Method: 收集三语推文构建数据集，用注意力层辅助transformer和大语言模型，非transformer模型用TF - IDF，用多种模型进行基准测试，由三名标注员保证数据质量。

Result: 结合注意力层与GPT - 3.5 Turbo和Qwen 2.5 72B模型表现出色，各语言及多语言联合模型宏观F1分数较SVM基线均有提升。

Conclusion: 该框架为多语言仇恨言论检测提供有力方案，促进全球数字社区安全。

Abstract: Social media platforms are critical spaces for public discourse, shaping
opinions and community dynamics, yet their widespread use has amplified harmful
content, particularly hate speech, threatening online safety and inclusivity.
While hate speech detection has been extensively studied in languages like
English and Spanish, Urdu remains underexplored, especially using
translation-based approaches. To address this gap, we introduce a trilingual
dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and
Spanish (3,162 samples), collected via keyword filtering, with a balanced
distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology
leverages attention layers as a precursor to transformer-based models and large
language models (LLMs), enhancing feature extraction for multilingual hate
speech detection. For non-transformer models, we use TF-IDF for feature
extraction. The dataset is benchmarked using state-of-the-art models, including
GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models
like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators,
following rigorous guidelines, ensured high dataset quality, achieving a
Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5
Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of
0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for
Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B).
These results reflect improvements of 8.75% in English (over SVM baseline
0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM
baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline
0.82). Our framework offers a robust solution for multilingual hate speech
detection, fostering safer digital communities worldwide.

</details>


### [232] [Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length](https://arxiv.org/abs/2506.08184)
*Chupei Wang,Jiaqiu Vince Sun*

Main category: cs.CL

TL;DR: 研究大语言模型信息检索中上下文干扰的影响，发现干扰累积会降低检索准确率，提示存在工作记忆瓶颈。


<details>
  <summary>Details</summary>
Motivation: 大语言模型信息检索与生成能力相关，长上下文虽常被认为利于检索，但上下文内干扰影响研究不足。

Method: 借鉴认知科学的前摄干扰范式，引入PI - LLM评估，顺序流式处理语义相关键值更新并仅查询最终值。

Result: 随着干扰累积，大语言模型检索准确率呈对数线性下降至零，错误源于检索先前被覆盖的值，提示工程提示缓解干扰效果有限。

Conclusion: 大语言模型在处理干扰和灵活操纵信息方面存在根本限制，可能存在工作记忆瓶颈，需增强模型在检索时抑制无关内容的能力。

Abstract: Information retrieval in Large Language Models (LLMs) is increasingly
recognized as intertwined with generation capabilities rather than mere lookup.
While longer contexts are often assumed to improve retrieval, the effects of
intra-context interference remain understudied. To address this, we adapt the
proactive interference (PI) paradigm from cognitive science, where earlier
information disrupts recall of newer updates. In humans, susceptibility to such
interference is inversely linked to working memory capacity. We introduce
PI-LLM, an evaluation that sequentially streams semantically related key-value
updates and queries only the final values. Although these final values are
clearly positioned just before the query, LLM retrieval accuracy declines
log-linearly toward zero as interference accumulates; errors arise from
retrieving previously overwritten values. Attempts to mitigate interference via
prompt engineering (e.g., instructing models to ignore earlier input) yield
limited success. These findings reveal a fundamental constraint on LLMs'
ability to disentangle interference and flexibly manipulate information,
suggesting a working memory bottleneck beyond mere context access. This calls
for approaches that strengthen models' ability to suppress irrelevant content
during retrieval.

</details>


### [233] [Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions](https://arxiv.org/abs/2506.08234)
*Yu-Ang Lee,Guan-Ting Yi,Mei-Yi Liu,Jui-Chao Lu,Guan-Bo Yang,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 本文对复合AI系统优化的最新进展进行系统综述，形式化相关概念，分类现有方法并指出研究挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和AI系统发展使复合AI系统复杂，传统优化方法有局限，自然语言反馈带来新途径，需系统研究优化进展。

Method: 对复合AI系统优化相关研究进行系统综述，形式化优化概念，按关键维度对现有方法分类。

Result: 完成对复合AI系统优化进展的综述，提供调查论文列表。

Conclusion: 指出该快速发展领域存在的开放性研究挑战和未来方向。

Abstract: Recent advancements in large language models (LLMs) and AI systems have led
to a paradigm shift in the design and optimization of complex AI workflows. By
integrating multiple components, compound AI systems have become increasingly
adept at performing sophisticated tasks. However, as these systems grow in
complexity, new challenges arise in optimizing not only individual components
but also their interactions. While traditional optimization methods such as
supervised fine-tuning (SFT) and reinforcement learning (RL) remain
foundational, the rise of natural language feedback introduces promising new
approaches, especially for optimizing non-differentiable systems. This paper
provides a systematic review of recent progress in optimizing compound AI
systems, encompassing both numerical and language-based techniques. We
formalize the notion of compound AI system optimization, classify existing
methods along several key dimensions, and highlight open research challenges
and future directions in this rapidly evolving field. A list of surveyed papers
is publicly available at https://github.com/MiuLab/AISysOpt-Survey.

</details>


### [234] [Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning](https://arxiv.org/abs/2506.08235)
*Shashidhar Reddy Javaji,Yupeng Cao,Haohang Li,Yangyang Yu,Nikhil Muralidhar,Zining Zhu*

Main category: cs.CL

TL;DR: 本文提出CLAIM - BENCH基准评估大语言模型在科学主张 - 证据提取和验证方面的能力，比较多种方法和模型，揭示了大语言模型处理复杂科学内容的局限性及不同模型表现差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理复杂研究论文中主张与证据的逻辑关系能力未充分探索，需要评估其科学理解能力。

Method: 提出CLAIM - BENCH基准，系统比较受分治方法启发的三种方法，在六个不同大语言模型上进行评估，涉及300多个主张 - 证据对。

Result: 闭源模型在精确率和召回率上优于开源模型；三遍和逐个提示方法能提升大语言模型能力，但增加计算成本。

Conclusion: CLAIM - BENCH为评估大语言模型的科学理解能力设定了新标准，提供诊断工具和构建更可靠推理系统的方向。

Abstract: Large language models (LLMs) are increasingly being used for complex research
tasks such as literature review, idea generation, and scientific paper
analysis, yet their ability to truly understand and process the intricate
relationships within complex research papers, such as the logical links between
claims and supporting evidence remains largely unexplored. In this study, we
present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs'
capabilities in scientific claim-evidence extraction and validation, a task
that reflects deeper comprehension of scientific argumentation. We
systematically compare three approaches which are inspired by divide and
conquer approaches, across six diverse LLMs, highlighting model-specific
strengths and weaknesses in scientific comprehension. Through evaluation
involving over 300 claim-evidence pairs across multiple research domains, we
reveal significant limitations in LLMs' ability to process complex scientific
content. Our results demonstrate that closed-source models like GPT-4 and
Claude consistently outperform open-source counterparts in precision and recall
across claim-evidence identification tasks. Furthermore, strategically designed
three-pass and one-by-one prompting approaches significantly improve LLMs'
abilities to accurately link dispersed evidence with claims, although this
comes at increased computational cost. CLAIM-BENCH sets a new standard for
evaluating scientific comprehension in LLMs, offering both a diagnostic tool
and a path forward for building systems capable of deeper, more reliable
reasoning across full-length papers.

</details>


### [235] [Automatic Generation of Inference Making Questions for Reading Comprehension Assessments](https://arxiv.org/abs/2506.08260)
*Wanjing Anya Ma,Michael Flor,Zuowei Wang*

Main category: cs.CL

TL;DR: 本文介绍阅读理解推理类型分类法，用GPT - 4o生成推理题，评估结果显示其能生成高质量但部分类型匹配度低的题目，认为自动生成结合人工判断是有前景的评估方式。


<details>
  <summary>Details</summary>
Motivation: 推理是阅读理解的重要技能，诊断性阅读理解问题可帮助教育者提供更有效的阅读指导和干预，需要分析推理类型分布及生成相关题目。

Method: 引入阅读理解推理类型分类法分析诊断性阅读理解题库中题目的分布；用GPT - 4o通过少样本提示为给定阅读段落生成衔接推理题，对比有无思维链提示的情况；从三个方面评估生成的题目。

Result: 生成的题目在评估上评分者间一致性高于0.90；GPT - 4o生成93.8%适合3 - 12年级使用的高质量问题，但只有42.6%的问题准确匹配目标推理类型。

Conclusion: 自动题目生成与人工判断相结合为可扩展、高质量的诊断性阅读理解评估提供了有前景的途径。

Abstract: Inference making is an essential but complex skill in reading comprehension
(RC). Some inferences require resolving references across sentences, and some
rely on using prior knowledge to fill in the detail that is not explicitly
written in the text. Diagnostic RC questions can help educators provide more
effective and targeted reading instruction and interventions for school-age
students. We introduce a taxonomy of inference types for RC and use it to
analyze the distribution of items within a diagnostic RC item bank. Next, we
present experiments using GPT-4o to generate bridging-inference RC items for
given reading passages via few-shot prompting, comparing conditions with and
without chain-of-thought prompts. Generated items were evaluated on three
aspects: overall item quality, appropriate inference type, and LLM reasoning,
achieving high inter-rater agreements above 0.90. Our results show that GPT-4o
produced 93.8% good-quality questions suitable for operational use in grade
3-12 contexts; however, only 42.6% of the generated questions accurately
matched the targeted inference type. We conclude that combining automatic item
generation with human judgment offers a promising path toward scalable,
high-quality diagnostic RC assessments.

</details>


### [236] [Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving](https://arxiv.org/abs/2506.08349)
*Yuxuan Zhou,Xien Liu,Chenwei Yan,Chen Ning,Xiao Zhang,Boxun Li,Xiangling Fu,Shijin Wang,Guoping Hu,Yu Wang,Ji Wu*

Main category: cs.CL

TL;DR: 提出多认知水平评估框架评估医学领域大语言模型，发现模型在认知复杂度增加时性能下降，强调提升高阶认知能力的必要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医学基准测试表现出色，但不同认知水平能力探索不足，故开展研究。

Method: 受布鲁姆分类法启发，提出多认知水平评估框架，整合现有医学数据集，引入三类认知水平任务，评估六个家族的大语言模型。

Result: 评估的模型在认知复杂度增加时性能显著下降，模型大小在高阶认知水平对性能影响更大。

Conclusion: 需提升大语言模型在高阶认知水平的医学能力，为开发适用于实际医疗应用的模型提供见解。

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
various medical benchmarks, but their capabilities across different cognitive
levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a
multi-cognitive-level evaluation framework for assessing LLMs in the medical
domain in this study. The framework integrates existing medical datasets and
introduces tasks targeting three cognitive levels: preliminary knowledge grasp,
comprehensive knowledge application, and scenario-based problem solving. Using
this framework, we systematically evaluate state-of-the-art general and medical
LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek.
Our findings reveal a significant performance decline as cognitive complexity
increases across evaluated models, with model size playing a more critical role
in performance at higher cognitive levels. Our study highlights the need to
enhance LLMs' medical capabilities at higher cognitive levels and provides
insights for developing LLMs suited to real-world medical applications.

</details>


### [237] [Draft-based Approximate Inference for LLMs](https://arxiv.org/abs/2506.08373)
*Kevin Galim,Ethan Ewer,Wonjun Kang,Minjae Lee,Hyung Il Koo,Kangwook Lee*

Main category: cs.CL

TL;DR: 提出利用小草稿模型预测重要性的近似大语言模型推理框架，含SpecKV和SpecPC实例，实验显示优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 因Transformer计算和内存复杂度问题，现有近似方法依赖粗略预测，需更准确方法优化长上下文大语言模型推理。

Method: 提出利用小草稿模型预测标记和键值对重要性的框架，有SpecKV和SpecPC两个实例。

Result: 实验表明该方法在长上下文基准测试中比现有基线准确率更高，且内存使用、延迟和吞吐量有同样改善。

Conclusion: 此为首次将草稿模型用于近似大语言模型推理加速，扩展其用途。代码开源。

Abstract: Optimizing inference for long-context Large Language Models (LLMs) is
increasingly important due to the quadratic compute and linear memory
complexity of Transformers. Existing approximation methods, such as key-value
(KV) cache dropping, sparse attention, and prompt compression, typically rely
on rough predictions of token or KV pair importance. We propose a novel
framework for approximate LLM inference that leverages small draft models to
more accurately predict the importance of tokens and KV pairs. Specifically, we
introduce two instantiations of our proposed framework: (i) SpecKV, which
leverages a draft output to accurately assess the importance of each KV pair
for more effective KV cache dropping, and (ii) SpecPC, which uses the draft
model's attention activations to identify and discard unimportant prompt
tokens. To the best of our knowledge, this is the first work to use draft
models for approximate LLM inference acceleration, extending their utility
beyond traditional lossless speculative decoding. We motivate our methods with
theoretical and empirical analyses, and show a strong correlation between the
attention patterns of draft and target models. Extensive experiments on
long-context benchmarks show that our methods consistently achieve higher
accuracy than existing baselines, while preserving the same improvements in
memory usage, latency, and throughput. Our code is available at
https://github.com/furiosa-ai/draft-based-approx-llm.

</details>


### [238] [TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration](https://arxiv.org/abs/2506.08403)
*Weiya Li,Junjie Chen,Bei Li,Boyang Liu,Zichen Wen,Nuanqiao Shan,Xiaoqian Liu,Anping Liu,Huajie Liu,Youyan Wang,Wujiuge Yin,Hu Song,Bing Huang,Zhiyuan Xia,Jialiang Chen,Linfeng Zhang*

Main category: cs.CL

TL;DR: 现有多智能体翻译框架忽略认知翻译研究，本文提出TACTIC框架，在多语言对实验中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体翻译框架忽略认知翻译研究见解，未能充分挖掘大语言模型翻译潜力。

Method: 提出TACTIC框架，包含六个不同功能智能体，模拟人类翻译认知过程。

Result: 在FLORES - 200和WMT24基准测试中，TACTIC达SOTA，以DeepSeek - V3为基础模型时优于GPT - 4.1和DeepSeek - R1。

Conclusion: TACTIC框架能有效利用大语言模型进行高质量翻译。

Abstract: Machine translation has long been a central task in natural language
processing. With the rapid advancement of large language models (LLMs), there
has been remarkable progress in translation quality. However, fully realizing
the translation potential of LLMs remains an open challenge. Recent studies
have explored multi-agent systems to decompose complex translation tasks into
collaborative subtasks, showing initial promise in enhancing translation
quality through agent cooperation and specialization. Nevertheless, existing
multi-agent translation frameworks largely neglect foundational insights from
cognitive translation studies. These insights emphasize how human translators
employ different cognitive strategies, such as balancing literal and free
translation, refining expressions based on context, and iteratively evaluating
outputs. To address this limitation, we propose a cognitively informed
multi-agent framework called TACTIC, which stands for T ranslation A gents with
Cognitive- T heoretic Interactive Collaboration. The framework comprises six
functionally distinct agents that mirror key cognitive processes observed in
human translation behavior. These include agents for drafting, refinement,
evaluation, scoring, context reasoning, and external knowledge gathering. By
simulating an interactive and theory-grounded translation workflow, TACTIC
effectively leverages the full capacity of LLMs for high-quality translation.
Experimental results on diverse language pairs from the FLORES-200 and WMT24
benchmarks show that our method consistently achieves state-of-the-art
performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by
an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it
further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at
https://github.com/weiyali126/TACTIC.

</details>


### [239] [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/abs/2506.08480)
*Huixuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 现有文本到图像生成评估重人类评估一致性，本文指出可靠评估应关注两方面，证明主流框架不足并提改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成评估忽略了可靠评估框架的其他关键属性。

Method: 先确定可靠评估应解决的两个关键方面，然后实证研究主流评估框架。

Result: 当前主流评估框架在多种指标和模型下不能完全满足关键属性。

Conclusion: 提出了改进图像 - 文本对齐评估的建议。

Abstract: Text-to-image models often struggle to generate images that precisely match
textual prompts. Prior research has extensively studied the evaluation of
image-text alignment in text-to-image generation. However, existing evaluations
primarily focus on agreement with human assessments, neglecting other critical
properties of a trustworthy evaluation framework. In this work, we first
identify two key aspects that a reliable evaluation should address. We then
empirically demonstrate that current mainstream evaluation frameworks fail to
fully satisfy these properties across a diverse range of metrics and models.
Finally, we propose recommendations for improving image-text alignment
evaluation.

</details>


### [240] [Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models](https://arxiv.org/abs/2506.08487)
*Sumanth Manduru,Carlotta Domeniconi*

Main category: cs.CL

TL;DR: 对指令调优的小型语言模型（SLMs）进行大规模审计，评估多个开源模型，揭示能力与公平性关系、架构差异及压缩权衡，为负责任部署提供指导。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在设备端和资源受限环境的快速应用使人们对其伦理风险的理解不足，需对被忽视的中间层级SLMs进行审计。

Method: 选取Qwen 2.5、LLaMA 3.2、Gemma 3和Phi系列的九个开源模型，使用BBQ基准在零样本提示下分析效用和公平性。

Result: 发现能力和公平性并非对立；不同架构社会偏差差异大；压缩存在细微权衡。

Conclusion: 研究结果为需要公平性和效率的SLMs应用提供实际指导，尤其有利于小企业和资源受限环境。

Abstract: The rapid adoption of Small Language Models (SLMs) for on-device and
resource-constrained deployments has outpaced our understanding of their
ethical risks. To the best of our knowledge, we present the first large-scale
audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an
overlooked "middle tier" between BERT-class encoders and flagship LLMs. Our
evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma
3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we
analyze both utility and fairness across ambiguous and disambiguated contexts.
This evaluation reveals three key insights. First, competence and fairness need
not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while
exhibiting minimal bias, showing that efficient and ethical NLP is attainable.
Second, social bias varies significantly by architecture: Qwen 2.5 models may
appear fair, but this often reflects vacuous neutrality, random guessing, or
evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2
models exhibit stronger stereotypical bias, suggesting overconfidence rather
than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ
quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but
increases disability-related bias in Phi-4-Mini by over 7 percentage points.
These insights provide practical guidance for the responsible deployment of
SLMs in applications demanding fairness and efficiency, particularly benefiting
small enterprises and resource-constrained environments.

</details>


### [241] [EtiCor++: Towards Understanding Etiquettical Bias in LLMs](https://arxiv.org/abs/2506.08488)
*Ashutosh Dwivedi,Siddhant Shivdutt Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 介绍EtiCor++语料库，用于评估大语言模型对礼仪的理解和偏差，实验显示大语言模型存在地区偏差。


<details>
  <summary>Details</summary>
Motivation: 使大语言模型对礼仪敏感，但评估其对礼仪理解和偏差的资源不足。

Method: 引入全球礼仪语料库EtiCor++，设计评估任务和衡量偏差的指标。

Result: 对大语言模型的广泛实验显示其对某些地区存在固有偏差。

Conclusion: 大语言模型在礼仪理解上存在地区偏差问题，EtiCor++可用于评估。

Abstract: In recent years, researchers have started analyzing the cultural sensitivity
of LLMs. In this respect, Etiquettes have been an active area of research.
Etiquettes are region-specific and are an essential part of the culture of a
region; hence, it is imperative to make LLMs sensitive to etiquettes. However,
there needs to be more resources in evaluating LLMs for their understanding and
bias with regard to etiquettes. In this resource paper, we introduce EtiCor++,
a corpus of etiquettes worldwide. We introduce different tasks for evaluating
LLMs for knowledge about etiquettes across various regions. Further, we
introduce various metrics for measuring bias in LLMs. Extensive experimentation
with LLMs shows inherent bias towards certain regions.

</details>


### [242] [DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs](https://arxiv.org/abs/2506.08500)
*Arie Cattan,Alon Jacovi,Ori Ram,Jonathan Herzig,Roee Aharoni,Sasha Goldshtein,Eran Ofek,Idan Szpektor,Avi Caciularu*

Main category: cs.CL

TL;DR: 本文提出RAG知识冲突类型分类法和高质量基准CONFLICTS，实验表明大语言模型解决知识冲突有困难，提示推理可改善但仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 解决RAG中检索源存在冲突信息，模型不知如何处理的问题。

Method: 提出知识冲突类型分类法，创建有专家标注的基准CONFLICTS并进行实验。

Result: 大语言模型难以恰当解决源之间的冲突，提示其推理能提升回复质量和恰当性。

Conclusion: 未来研究在解决RAG知识冲突方面仍有很大提升空间。

Abstract: Retrieval Augmented Generation (RAG) is a commonly used approach for
enhancing large language models (LLMs) with relevant and up-to-date
information. However, the retrieved sources can often contain conflicting
information and it remains unclear how models should address such
discrepancies. In this work, we first propose a novel taxonomy of knowledge
conflict types in RAG, along with the desired model behavior for each type. We
then introduce CONFLICTS, a high-quality benchmark with expert annotations of
conflict types in a realistic RAG setting. CONFLICTS is the first benchmark
that enables tracking progress on how models address a wide range of knowledge
conflicts. We conduct extensive experiments on this benchmark, showing that
LLMs often struggle to appropriately resolve conflicts between sources. While
prompting LLMs to explicitly reason about the potential conflict in the
retrieved documents significantly improves the quality and appropriateness of
their responses, substantial room for improvement in future research remains.

</details>


### [243] [CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations](https://arxiv.org/abs/2506.08504)
*Divyaksh Shukla,Ritesh Baviskar,Dwijesh Gohil,Aniket Tiwari,Atul Shree,Ashutosh Modi*

Main category: cs.CL

TL;DR: 介绍用于对话语篇分析的多领域代码混合语料库CoMuMDR，实验表明现有模型在此语料库上表现不佳，需开发更优模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于对话的语篇分析数据集是单领域书面英语对话，缺乏多领域代码混合语料库。

Method: 引入CoMuMDR语料库（印地语和英语代码混合，有音频和转录文本，标注九种语篇关系），用多种最先进基线模型进行实验。

Result: 最先进模型表现不佳。

Conclusion: 多领域代码混合语料库存在挑战，需要为现实场景开发更好的模型。

Abstract: Discourse parsing is an important task useful for NLU applications such as
summarization, machine comprehension, and emotion recognition. The current
discourse parsing datasets based on conversations consists of written English
dialogues restricted to a single domain. In this resource paper, we introduce
CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in
conversations. The corpus (code-mixed in Hindi and English) has both audio and
transcribed text and is annotated with nine discourse relations. We experiment
with various SoTA baseline models; the poor performance of SoTA models
highlights the challenges of multi-domain code-mixed corpus, pointing towards
the need for developing better models for such realistic settings.

</details>


### [244] [Efficient Post-Training Refinement of Latent Reasoning in Large Language Models](https://arxiv.org/abs/2506.08552)
*Xinyuan Wang,Dongjie Wang,Wangyang Ying,Haoyue Bai,Nanxu Gong,Sixun Dong,Kunpeng Liu,Yanjie Fu*

Main category: cs.CL

TL;DR: 本文提出轻量级后训练框架优化大语言模型的潜在推理轨迹，在五个推理基准测试中证明有效，如MathQA准确率提升5%。


<details>
  <summary>Details</summary>
Motivation: 现有链式思维提示存在标记开销大、推理轨迹固定问题，潜在推理需解决后训练中有效更新推理嵌入的挑战。

Method: 提出轻量级后训练框架，采用对比推理反馈和残差嵌入细化两种策略。

Result: 在五个推理基准测试中证明框架有效，MathQA无额外训练下准确率提升5%。

Conclusion: 所提轻量级后训练框架能有效优化大语言模型的潜在推理轨迹。

Abstract: Reasoning is a key component of language understanding in Large Language
Models. While Chain-of-Thought prompting enhances performance via explicit
intermediate steps, it suffers from sufficient token overhead and a fixed
reasoning trajectory, preventing step-wise refinement. Recent advances in
latent reasoning address these limitations by refining internal reasoning
processes directly in the model's latent space, without producing explicit
outputs. However, a key challenge remains: how to effectively update reasoning
embeddings during post-training to guide the model toward more accurate
solutions. To overcome this challenge, we propose a lightweight post-training
framework that refines latent reasoning trajectories using two novel
strategies: 1) Contrastive reasoning feedback, which compares reasoning
embeddings against strong and weak baselines to infer effective update
directions via embedding enhancement; 2) Residual embedding refinement, which
stabilizes updates by progressively integrating current and historical
gradients, enabling fast yet controlled convergence. Extensive experiments and
case studies are conducted on five reasoning benchmarks to demonstrate the
effectiveness of the proposed framework. Notably, a 5\% accuracy gain on MathQA
without additional training.

</details>


### [245] [TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning](https://arxiv.org/abs/2506.08646)
*Mingyu Zheng,Zhifan Feng,Jia Wang,Lanrui Wang,Zheng Lin,Yang Hao,Weiping Wang*

Main category: cs.CL

TL;DR: 提出TableDreamer框架解决现有LLM数据合成方法在生成表格指令调优数据的局限，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的数据合成方法在生成表格指令调优数据时存在数据多样性有限和数据效率不佳的问题。

Method: 先合成多样化表格和指令作为种子数据，再在新识别的弱点数据引导下迭代探索输入空间生成最终训练数据。

Result: 在10个表格基准测试上，用27K GPT - 4o合成数据使Llama3.1 - 8B - instruct平均准确率提升11.62%，优于使用更多训练数据的现有方法。

Conclusion: 提出的TableDreamer框架有效，代码和数据开源。

Abstract: Despite the commendable progress of recent LLM-based data synthesis methods,
they face two limitations in generating table instruction tuning data. First,
they can not thoroughly explore the vast input space of table understanding
tasks, leading to limited data diversity. Second, they ignore the weaknesses in
table understanding ability of the target LLM and blindly pursue the increase
of data quantity, resulting in suboptimal data efficiency. In this paper, we
introduce a progressive and weakness-guided data synthesis framework tailored
for table instruction tuning, named TableDreamer, to mitigate the above issues.
Specifically, we first synthesize diverse tables and related instructions as
seed data, and then perform an iterative exploration of the input space under
the guidance of the newly identified weakness data, which eventually serve as
the final training data for fine-tuning the target LLM. Extensive experiments
on 10 tabular benchmarks demonstrate the effectiveness of the proposed
framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62%
(49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms
state-of-the-art data synthesis baselines which use more training data. The
code and data is available at https://github.com/SpursGoZmy/TableDreamer

</details>


### [246] [Summarization for Generative Relation Extraction in the Microbiome Domain](https://arxiv.org/abs/2506.08647)
*Oumaima El Khettari,Solen Quiniou,Samuel Chaffron*

Main category: cs.CL

TL;DR: 探索用于肠道微生物组相互作用研究的生成式关系提取（RE）管道，利用大语言模型（LLM）总结上下文，初步结果显示总结能提升性能，但BERT方法仍更优，展示了生成式方法在低资源领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 针对肠道微生物组这一复杂且低资源的生物医学领域，探索合适的生成式关系提取管道。

Method: 利用大语言模型进行总结以精炼上下文，再通过指令调优生成来提取关系。

Result: 在专用语料库上的初步结果表明，总结能减少噪声、引导模型，提升生成式RE性能，但BERT-based RE方法表现更优。

Conclusion: 生成式方法在低资源专业领域研究中有潜在应用价值。

Abstract: We explore a generative relation extraction (RE) pipeline tailored to the
study of interactions in the intestinal microbiome, a complex and low-resource
biomedical domain. Our method leverages summarization with large language
models (LLMs) to refine context before extracting relations via
instruction-tuned generation. Preliminary results on a dedicated corpus show
that summarization improves generative RE performance by reducing noise and
guiding the model. However, BERT-based RE approaches still outperform
generative models. This ongoing work demonstrates the potential of generative
methods to support the study of specialized domains in low-resources setting.

</details>


### [247] [ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization](https://arxiv.org/abs/2506.08712)
*Hee Suk Yoon,Eunseop Yoon,Mark A. Hasegawa-Johnson,Sungwoong Kim,Chang D. Yoo*

Main category: cs.CL

TL;DR: 介绍ConfPO方法用于大语言模型偏好学习，不依赖辅助模型或额外计算，聚焦关键令牌优化，实验显示其优于统一直接对齐算法。


<details>
  <summary>Details</summary>
Motivation: 现有直接对齐算法统一调整所有令牌概率，不区分与偏好的相关性，且近期令牌级方法存在可扩展性和可靠性问题，需更好的偏好学习方法。

Method: ConfPO仅基于训练策略的置信度识别和优化偏好关键令牌，高效利用KL散度预算。

Result: 在AlpacaEval 2和Arena - Hard等基准测试中，ConfPO在各种大语言模型上始终优于统一直接对齐算法。

Conclusion: ConfPO方法能提高对齐质量，避免过度优化，且无额外计算开销，是一种简单、轻量级且无模型的偏好学习方法。

Abstract: We introduce ConfPO, a method for preference learning in Large Language
Models (LLMs) that identifies and optimizes preference-critical tokens based
solely on the training policy's confidence, without requiring any auxiliary
models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as
Direct Preference Optimization (DPO), which uniformly adjust all token
probabilities regardless of their relevance to preference, ConfPO focuses
optimization on the most impactful tokens. This targeted approach improves
alignment quality while mitigating overoptimization (i.e., reward hacking) by
using the KL divergence budget more efficiently. In contrast to recent
token-level methods that rely on credit-assignment models or AI annotators,
raising concerns about scalability and reliability, ConfPO is simple,
lightweight, and model-free. Experimental results on challenging alignment
benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO
consistently outperforms uniform DAAs across various LLMs, delivering better
alignment with zero additional computational overhead.

</details>


### [248] [Improved LLM Agents for Financial Document Question Answering](https://arxiv.org/abs/2506.08726)
*Nelvin Tan,Zian Seng,Liang Zhang,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: 论文研究大语言模型在金融文档数值问答任务上的表现，指出传统批评代理在无预言标签时性能下降，提出改进批评代理和计算器代理，表现优于先前方法且更安全，并研究了代理间的交互影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在金融文档数值问答任务存在困难，传统批评代理在无预言标签时的有效性未知。

Method: 先实验验证传统批评代理在无预言标签时性能，再提出改进批评代理和计算器代理，并研究代理间的交互。

Result: 改进批评代理和计算器代理优于先前的program - of - thought方法，且更安全。

Conclusion: 改进的代理在金融文档数值问答任务中表现更好，代理间的交互会影响性能。

Abstract: Large language models (LLMs) have shown impressive capabilities on numerous
natural language processing tasks. However, LLMs still struggle with numerical
question answering for financial documents that include tabular and textual
data. Recent works have showed the effectiveness of critic agents (i.e.,
self-correction) for this task given oracle labels. Building upon this
framework, this paper examines the effectiveness of the traditional critic
agent when oracle labels are not available, and show, through experiments, that
this critic agent's performance deteriorates in this scenario. With this in
mind, we present an improved critic agent, along with the calculator agent
which outperforms the previous state-of-the-art approach (program-of-thought)
and is safer. Furthermore, we investigate how our agents interact with each
other, and how this interaction affects their performance.

</details>


### [249] [Societal AI Research Has Become Less Interdisciplinary](https://arxiv.org/abs/2506.08738)
*Dror Kris Markus,Fabrizio Gilardi,Daria Stetsenko*

Main category: cs.CL

TL;DR: 分析2014 - 2024年ArXiv上超10万篇AI论文，发现计算机科学团队在社会导向AI研究占比上升，挑战常见认知。


<details>
  <summary>Details</summary>
Motivation: 探究跨学科研究团队是否真正引领AI研究融入伦理和社会价值的转变。

Method: 开发分类器识别社会内容，衡量论文对社会考量的表达程度。

Result: 跨学科团队更易产出社会导向研究，但计算机科学团队社会导向研究产出占比增加，且涉及领域广泛。

Conclusion: 挑战了社会导向AI研究驱动因素的常见假设，引发对AI安全治理及社科人文学者作用的思考。

Abstract: As artificial intelligence (AI) systems become deeply embedded in everyday
life, calls to align AI development with ethical and societal values have
intensified. Interdisciplinary collaboration is often championed as a key
pathway for fostering such engagement. Yet it remains unclear whether
interdisciplinary research teams are actually leading this shift in practice.
This study analyzes over 100,000 AI-related papers published on ArXiv between
2014 and 2024 to examine how ethical values and societal concerns are
integrated into technical AI research. We develop a classifier to identify
societal content and measure the extent to which research papers express these
considerations. We find a striking shift: while interdisciplinary teams remain
more likely to produce societally-oriented research, computer science-only
teams now account for a growing share of the field's overall societal output.
These teams are increasingly integrating societal concerns into their papers
and tackling a wide range of domains - from fairness and safety to healthcare
and misinformation. These findings challenge common assumptions about the
drivers of societal AI and raise important questions. First, what are the
implications for emerging understandings of AI safety and governance if most
societally-oriented research is being undertaken by exclusively technical
teams? Second, for scholars in the social sciences and humanities: in a
technical field increasingly responsive to societal demands, what distinctive
perspectives can we still offer to help shape the future of AI?

</details>


### [250] [Factors affecting the in-context learning abilities of LLMs for dialogue state tracking](https://arxiv.org/abs/2506.08753)
*Pradyoth Hegde,Santosh Kesiraju,Jan Švec,Šimon Sedláček,Bolaji Yusuf,Oldřich Plchot,Deepak K T,Jan Černocký*

Main category: cs.CL

TL;DR: 研究将上下文学习应用于对话状态跟踪问题，分析影响其有效性的因素，给出有用见解。


<details>
  <summary>Details</summary>
Motivation: 探索上下文学习在对话状态跟踪问题中的应用及影响其有效性的因素。

Method: 用基于句子嵌入的k近邻方法检索合适示例，将示例和测试样本按模板输入大语言模型，系统分析示例选择和提示上下文因素对性能的影响。

Result: 基于MultiWoZ2.4数据集，对OLMo - 7B - instruct、Mistral - 7B - Instruct - v0.3和Llama3.2 - 3B - Instruct模型进行研究，获得关于大语言模型上下文学习能力的有用见解。

Conclusion: 在对话状态跟踪中，大语言模型的上下文学习能力有一定规律，分析的因素对性能有影响。

Abstract: This study explores the application of in-context learning (ICL) to the
dialogue state tracking (DST) problem and investigates the factors that
influence its effectiveness. We use a sentence embedding based k-nearest
neighbour method to retrieve the suitable demonstrations for ICL. The selected
demonstrations, along with the test samples, are structured within a template
as input to the LLM. We then conduct a systematic study to analyse the impact
of factors related to demonstration selection and prompt context on DST
performance. This work is conducted using the MultiWoZ2.4 dataset and focuses
primarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and
Llama3.2-3B-Instruct models. Our findings provide several useful insights on
in-context learning abilities of LLMs for dialogue state tracking.

</details>


### [251] [The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation](https://arxiv.org/abs/2506.08827)
*Francisco Vargas,Alejandro González Coene,Gaston Escalante,Exequiel Lobón,Manuel Pulido*

Main category: cs.CL

TL;DR: 提出从法律文件提取交通事故信息的两步程序，比较不同文本分割和实体提取方法，发现微调可减少幻觉，不同模型有不同准确率，GPT - 4 Turbo最高。


<details>
  <summary>Details</summary>
Motivation: 从法律文件提取交通事故信息对量化保险公司成本至关重要，但提取实体具有挑战性。

Method: 先进行文本分割（对比正则表达式经典方法和基于n - token块的向量分割法），再用大语言模型（LLaMA - 2 7b、70b、LLaMA - 3 8b、GPT - 4 Turbo）提取实体，对LLaMA模型用LoRA微调。

Result: 微调减少了LLaMA - 2 7b的幻觉；基于向量分割和LLM的方法超越经典方法；开源模型中微调的LLaMA - 2 70B准确率最高达79.4%；LLaMA - 3 8B基础版表现接近微调的LLaMA - 2 70B；GPT - 4 Turbo准确率最高为86.1%。

Conclusion: 所提方法有效，不同模型表现有差异，体现模型发展迅速。

Abstract: The extraction of information about traffic accidents from legal documents is
crucial for quantifying insurance company costs. Extracting entities such as
percentages of physical and/or psychological disability and the involved
compensation amounts is a challenging process, even for experts, due to the
subtle arguments and reasoning in the court decision. A two-step procedure is
proposed: first, segmenting the document identifying the most relevant
segments, and then extracting the entities. For text segmentation, two
methodologies are compared: a classic method based on regular expressions and a
second approach that divides the document into blocks of n-tokens, which are
then vectorized using multilingual models for semantic searches
(text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models
(LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to
the selected segments for entity extraction. For the LLaMA models, fine-tuning
is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a
significant number of hallucinations in extractions which are an important
contention point for named entity extraction. This work shows that these
hallucinations are substantially reduced after finetuning the model. The
performance of the methodology based on segment vectorization and subsequent
use of LLMs significantly surpasses the classic method which achieves an
accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning
achieves the highest accuracy 79.4%, surpassing its base version 61.7%.
Notably, the base LLaMA-3 8B model already performs comparably to the finetuned
LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model
development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.

</details>


### [252] [mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks](https://arxiv.org/abs/2506.08400)
*Luel Hagos Beyene,Vivek Verma,Min Ma,Jesujoba O. Alabi,Fabian David Schmidt,Joyce Nakatumba-Nabende,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 本文引入新基准mSTEB评估大语言模型在多任务和多模态上的表现，评估显示高低资源语言间性能差距大，需更多投资解决低资源语言代表性不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评估多限于英语和高资源语言，低资源语言缺乏标准化评估基准。

Method: 引入新基准mSTEB，对Gemini 2.0 Flash、GPT - 4o (Audio)等领先模型和Qwen 2 Audio、Gemma 3 27B等开源模型进行评估。

Result: 评估显示高资源和低资源语言间存在显著性能差距，特别是非洲和美洲/大洋洲语言。

Conclusion: 需要更多投资来解决低资源语言在大语言模型覆盖中的代表性不足问题。

Abstract: Large Language models (LLMs) have demonstrated impressive performance on a
wide range of tasks, including in multimodal settings such as speech. However,
their evaluation is often limited to English and a few high-resource languages.
For low-resource languages, there is no standardized evaluation benchmark. In
this paper, we address this gap by introducing mSTEB, a new benchmark to
evaluate the performance of LLMs on a wide range of tasks covering language
identification, text classification, question answering, and translation tasks
on both speech and text modalities. We evaluated the performance of leading
LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open
models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in
performance between high-resource and low-resource languages, especially for
languages spoken in Africa and Americas/Oceania. Our findings show that more
investment is needed to address their under-representation in LLMs coverage.

</details>


### [253] [PlantBert: An Open Source Language Model for Plant Science](https://arxiv.org/abs/2506.08897)
*Hiba Khey,Amine Lakhder,Salma Rouichi,Imane El Ghabi,Kamal Hejjaoui,Younes En-nahli,Fahd Kalloubi,Moez Amri*

Main category: cs.CL

TL;DR: 介绍针对植物科学领域的开源语言模型PlantBert，能从植物应激反应文献中提取知识，有良好泛化能力，填补农业NLP空白。


<details>
  <summary>Details</summary>
Motivation: 现有基于transformer的语言模型在植物科学领域应用不足，需要为该领域开发适用工具。

Method: 基于DeBERTa架构，在精心策划的专家标注摘要语料库上微调，结合基于transformer的建模、规则增强的语言后处理和基于本体的实体归一化。

Result: PlantBert在实体类型上表现出强大的泛化能力，证明了在低资源科学领域进行强大的领域适应的可行性。

Conclusion: PlantBert填补了农业NLP的关键空白，为植物基因组学、表型组学和农艺知识发现的智能数据驱动系统铺平道路，模型公开以促进跨学科创新。

Abstract: The rapid advancement of transformer-based language models has catalyzed
breakthroughs in biomedical and clinical natural language processing; however,
plant science remains markedly underserved by such domain-adapted tools. In
this work, we present PlantBert, a high-performance, open-source language model
specifically tailored for extracting structured knowledge from plant
stress-response literature. Built upon the DeBERTa architecture-known for its
disentangled attention and robust contextual encoding-PlantBert is fine-tuned
on a meticulously curated corpus of expert-annotated abstracts, with a primary
focus on lentil (Lens culinaris) responses to diverse abiotic and biotic
stressors. Our methodology combines transformer-based modeling with
rule-enhanced linguistic post-processing and ontology-grounded entity
normalization, enabling PlantBert to capture biologically meaningful
relationships with precision and semantic fidelity. The underlying corpus is
annotated using a hierarchical schema aligned with the Crop Ontology,
encompassing molecular, physiological, biochemical, and agronomic dimensions of
plant adaptation. PlantBert exhibits strong generalization capabilities across
entity types and demonstrates the feasibility of robust domain adaptation in
low-resource scientific fields. By providing a scalable and reproducible
framework for high-resolution entity recognition, PlantBert bridges a critical
gap in agricultural NLP and paves the way for intelligent, data-driven systems
in plant genomics, phenomics, and agronomic knowledge discovery. Our model is
publicly released to promote transparency and accelerate cross-disciplinary
innovation in computational plant science.

</details>


### [254] [From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis](https://arxiv.org/abs/2506.08899)
*Elias Horner,Cristinel Mateis,Guido Governatori,Agata Ciabattoni*

Main category: cs.CL

TL;DR: 提出用大语言模型对法律文本进行自动语义分析并转换为可废止道义逻辑形式表示的方法，经评估证明有效。


<details>
  <summary>Details</summary>
Motivation: 实现法律文本的自动语义分析并转换为可废止道义逻辑的形式表示。

Method: 提出结构化流程分割规范语言、提取道义规则并评估，在多种大语言模型配置下评估该方法，以澳大利亚电信消费者保护守则的法律规范为对象。

Result: 机器生成的形式化结果与专家制作的结果有较好的一致性。

Conclusion: 大语言模型特别是有效提示时能对可扩展的法律信息学作出重要贡献。

Abstract: We present a novel approach to the automated semantic analysis of legal texts
using large language models (LLMs), targeting their transformation into formal
representations in Defeasible Deontic Logic (DDL). We propose a structured
pipeline that segments complex normative language into atomic snippets,
extracts deontic rules, and evaluates them for syntactic and semantic
coherence. Our methodology is evaluated across various LLM configurations,
including prompt engineering strategies, fine-tuned models, and multi-stage
pipelines, focusing on legal norms from the Australian Telecommunications
Consumer Protections Code. Empirical results demonstrate promising alignment
between machine-generated and expert-crafted formalizations, showing that LLMs
- particularly when prompted effectively - can significantly contribute to
scalable legal informatics.

</details>


### [255] [Olica: Efficient Structured Pruning of Large Language Models without Retraining](https://arxiv.org/abs/2506.08436)
*Jiujun He,Huazhen Lin*

Main category: cs.CL

TL;DR: 提出无需重新训练的大语言模型剪枝框架Olica，通过PCA压缩模型，用线性校准法解决FFN层误差积累问题，实验证明其高效且性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型结构化剪枝方法重新训练成本高，需大量计算和数据资源。

Method: 提出Olica框架，用PCA处理MHA层矩阵积以压缩模型，用线性校准法处理FFN层误差积累，利用SVD获得低秩矩阵。

Result: Olica在数据使用、GPU内存和运行时间上高效，在多个基准测试中表现优越。

Conclusion: Olica框架无需重新训练，能有效压缩大语言模型并保持性能。

Abstract: Most existing structured pruning methods for Large Language Models (LLMs)
require substantial computational and data resources for retraining to
reestablish the corrupted correlations, making them prohibitively expensive. To
address this, we propose a pruning framework for LLMs called Orthogonal
decomposition and Linear Calibration (Olica), which eliminates the need for
retraining. A key observation is that the multi-head attention (MHA) layer
depends on two types of matrix products. By treating these matrix products as
unified entities and applying principal component analysis (PCA), we extract
the most important information to compress LLMs without sacrificing accuracy or
disrupting their original structure. Consequently, retraining becomes
unnecessary. A fast decomposition method is devised, reducing the complexity of
PCA by a factor of the square of the number of attention heads. Additionally,
to mitigate error accumulation problem caused by pruning the feed-forward
network (FFN) layer, we introduce a linear calibration method to reconstruct
the residual errors of pruned layers using low-rank matrices. By leveraging
singular value decomposition (SVD) on the solution of the least-squares
problem, these matrices are obtained without requiring retraining. Extensive
experiments show that the proposed Olica is efficient in terms of data usage,
GPU memory, and running time, while delivering superior performance across
multiple benchmarks.

</details>


### [256] [PropMEND: Hypernetworks for Knowledge Propagation in LLMs](https://arxiv.org/abs/2506.08920)
*Zeyu Leo Liu,Greg Durrett,Eunsol Choi*

Main category: cs.CL

TL;DR: 提出基于超网络的知识传播方法PropMEND，在RippleEdit数据集表现佳，在新数据集也有较好泛化性但传播到更多关系待研究


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型知识编辑技术在知识传播方面不足，模型无法用注入知识推理回答问题

Method: 提出PropMEND方法，元学习修改语言建模损失的梯度，扩展MEND的元目标

Result: 在RippleEdit数据集上性能提升，在新数据集Controlled RippleEdit上对未见实体 - 关系对仍优于现有方法，但性能差距减小

Conclusion: PropMEND在知识传播上有效果，但将知识传播到广泛关系还需未来研究

Abstract: Knowledge editing techniques for large language models (LLMs) can inject
knowledge that is later reproducible verbatim, but they fall short on
propagating that knowledge: models cannot answer questions that require
reasoning with the injected knowledge. We present a hypernetwork-based approach
for knowledge propagation, named PropMEND, where we meta-learn how to modify
gradients of a language modeling loss to encourage injected information to
propagate. Our approach extends the meta-objective of MEND [29] so that
gradient updates on knowledge are transformed to enable answering multi-hop
questions involving that knowledge. We show improved performance on the
RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop
questions whose answers are not explicitly stated in the injected fact. We
further introduce a new dataset, Controlled RippleEdit, to evaluate the
generalization of our hypernetwork, testing knowledge propagation along
relations and entities unseen during hypernetwork training. PropMEND still
outperforms existing approaches in unseen entity-relation pairs, yet the
performance gap decreases substantially, suggesting future work in propagating
knowledge to a wide range of relations.

</details>


### [257] [Can A Gamer Train A Mathematical Reasoning Model?](https://arxiv.org/abs/2506.08935)
*Andrew Shin*

Main category: cs.CL

TL;DR: 本文展示了使用单张普通游戏GPU，结合强化学习和内存优化技术，可训练出具备出色数学推理能力的模型，挑战了高性能推理需大规模基础设施的范式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型开发需大量计算资源，现有方法仍依赖高端硬件集群，为降低资源门槛。

Method: 集成强化学习和内存优化技术，在16GB显存的RTX 3080 Ti上训练15亿参数的数学推理模型。

Result: 该模型在数学推理基准测试中，比数倍大的模型表现相当或更好。

Conclusion: 挑战了最先进数学推理需大规模基础设施的范式，使高性能AI研究更普及。

Abstract: While large language models (LLMs) have achieved remarkable performance in
various tasks including mathematical reasoning, their development typically
demands prohibitive computational resources. Recent advancements have reduced
costs for training capable models, yet even these approaches rely on high-end
hardware clusters. In this paper, we demonstrate that a single average gaming
GPU can train a solid mathematical reasoning model, by integrating
reinforcement learning and memory optimization techniques. Specifically, we
train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB
memory that achieves comparable or better performance on mathematical reasoning
benchmarks than models several times larger, in resource-constrained
environments. Our results challenge the paradigm that state-of-the-art
mathematical reasoning necessitates massive infrastructure, democratizing
access to high-performance AI research.
https://github.com/shinandrew/YouronMath.

</details>


### [258] [Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions](https://arxiv.org/abs/2506.08952)
*Clara Lachenmaier,Judith Sieker,Sina Zarrieß*

Main category: cs.CL

TL;DR: 研究大语言模型在政治领域处理共同知识的能力，发现其接地和纠正错误信念存在挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在知识（不）具备情况下如何管理共同知识，尤其关注政治领域错误信息和接地失败风险高的情况。

Method: 考察大语言模型回答直接知识问题和预设错误信息的诱导性问题的能力，评估诱导性问题是否使模型主动接地并纠正用户错误信念，关联其知识水平和政治偏见。

Result: 大语言模型在接地和拒绝用户错误信念方面存在重大挑战。

Conclusion: 大语言模型在缓解政治话语错误信息方面的作用令人担忧。

Abstract: Communication among humans relies on conversational grounding, allowing
interlocutors to reach mutual understanding even when they do not have perfect
knowledge and must resolve discrepancies in each other's beliefs. This paper
investigates how large language models (LLMs) manage common ground in cases
where they (don't) possess knowledge, focusing on facts in the political domain
where the risk of misinformation and grounding failure is high. We examine the
ability of LLMs to answer direct knowledge questions and loaded questions that
presuppose misinformation. We evaluate whether loaded questions lead LLMs to
engage in active grounding and correct false user beliefs, in connection to
their level of knowledge and their political bias. Our findings highlight
significant challenges in LLMs' ability to engage in grounding and reject false
user beliefs, raising concerns about their role in mitigating misinformation in
political discourse.

</details>


### [259] [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings](https://arxiv.org/abs/2506.08592)
*Liyan Xu,Zhenlin Su,Mo Yu,Jiangnan Li,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: 本文指出文本编码器无法识别细粒度实体或事件的局限，引入中文评估数据集CapRetrieval进行测试，提出数据生成策略微调编码器提升性能，还发现粒度困境问题，相关资源公开。


<details>
  <summary>Details</summary>
Motivation: 解决文本编码器在语义中无法识别细粒度实体或事件，导致简单场景下密集检索失败的问题。

Method: 引入中文评估数据集CapRetrieval进行零样本评估，提出数据生成策略微调编码器。

Result: 零样本评估表明编码器在细粒度匹配上失败，使用数据生成策略微调编码器在CapRetrieval上获得最佳性能。

Conclusion: 文本编码器存在细粒度识别问题，提出的数据生成策略能提升性能，还发现了粒度困境挑战，相关资源公开可推动研究。

Abstract: This work focuses on an observed limitation of text encoders: embeddings may
not be able to recognize fine-grained entities or events within the semantics,
resulting in failed dense retrieval on even simple cases. To examine such
behaviors, we first introduce a new evaluation dataset in Chinese, named
CapRetrieval, whose passages are image captions, and queries are phrases
inquiring entities or events in various forms. Zero-shot evaluation suggests
that encoders may fail on these fine-grained matching, regardless of training
sources or model sizes. Aiming for enhancement, we proceed to finetune encoders
with our proposed data generation strategies, which obtains the best
performance on CapRetrieval. Within this process, we further identify an issue
of granularity dilemma, a challenge for embeddings to express fine-grained
salience while aligning with overall semantics. Our dataset, code and models in
this work are publicly released at https://github.com/lxucs/CapRetrieval.

</details>


### [260] [Employing self-supervised learning models for cross-linguistic child speech maturity classification](https://arxiv.org/abs/2506.08999)
*Theo Zhang,Madurya Suresh,Anne S. Warlaumont,Kasia Hitczenko,Alejandrina Cristia,Margaret Cychosz*

Main category: cs.CL

TL;DR: 使用新数据集SpeechMaturity解决儿童语音分类任务，模型表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有语音技术系统因训练语料少和儿童语音难题，在儿童语音下游任务中表现不佳，需解决儿童发声识别这一基础分类任务。

Method: 将SpeechMaturity数据集应用于最先进的transformer模型，训练模型区分儿童的哭声、笑声、成熟语音和不成熟语音。

Result: 在该数据集上训练的模型优于在先前数据集上训练的模型，分类准确率与人类相当，且在城乡环境中都很稳健。

Conclusion: 新数据集SpeechMaturity有助于提升儿童语音分类任务的模型表现。

Abstract: Speech technology systems struggle with many downstream tasks for child
speech due to small training corpora and the difficulties that child speech
pose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer
models to address a fundamental classification task: identifying child
vocalizations. Unlike previous corpora, our dataset captures maximally
ecologically-valid child vocalizations across an unprecedented sample,
comprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu,
Papua New Guinea, Solomon Islands, and France. The dataset contains 242,004
labeled vocalizations, magnitudes larger than previous work. Models were
trained to distinguish between cry, laughter, mature (consonant+vowel), and
immature speech (just consonant or vowel). Models trained on the dataset
outperform state-of-the-art models trained on previous datasets, achieved
classification accuracy comparable to humans, and were robust across rural and
urban settings.

</details>


### [261] [Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning](https://arxiv.org/abs/2506.09033)
*Haozhen Zhang,Tao Feng,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出基于强化学习的Router - R1框架解决多LLM路由聚合问题，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由器单轮一对一映射，难以处理需多LLM互补的复杂任务。

Method: 将多LLM路由聚合建模为顺序决策过程，以LLM为路由器，交错‘思考’与‘路由’动作，用轻量级规则奖励引导学习，仅依赖简单模型描述符。

Result: 在七个通用和多跳QA基准测试中，Router - R1优于多个强基线，性能好且泛化和成本管理能力强。

Conclusion: Router - R1有效解决多LLM路由聚合问题，可通过强化学习优化性能成本权衡。

Abstract: The rapid emergence of diverse large language models (LLMs) has spurred the
development of LLM routers that assign user queries to the most suitable model.
However, existing LLM routers typically perform a single-round, one-to-one
mapping (\textit{i.e.}, assigning each query to a single model in isolation),
which limits their capability to tackle complex tasks that demand the
complementary strengths of multiple LLMs. In this paper, we present
\textbf{Router-R1}, a reinforcement learning (RL)-based framework that
formulates multi-LLM routing and aggregation as a sequential decision process.
Router-R1 instantiates the router itself as a capable LLM, leveraging its
reasoning ability to interleave "think" actions (internal deliberation) with
"route" actions (dynamic model invocation), and integrates each response into
its evolving context. To guide learning, we employ a lightweight rule-based
reward comprising format rewards, final outcome rewards, and a novel cost
reward for performance and cost trade-off optimization, opening a pathway
toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions
only on simple model descriptors such as pricing, latency, and example
performance, enabling strong generalization to unseen model selection.
Experiments on seven general and multi-hop QA benchmarks show that Router-R1
outperforms over several strong baselines, achieving superior performance while
maintaining robust generalization and cost management.Code is available at
https://github.com/ulab-uiuc/Router-R1.

</details>


### [262] [Towards Secure and Private Language Models for Nuclear Power Plants](https://arxiv.org/abs/2506.08746)
*Muhammad Anwar,Mishca de Costa,Issam Hammad,Daniel Lau*

Main category: cs.CL

TL;DR: 本文基于公开教科书构建核应用领域特定大语言模型，训练有成果但文本生成有不足，提出后续改进方向。


<details>
  <summary>Details</summary>
Motivation: 构建符合核操作严格网络安全和数据保密标准的内部大语言模型。

Method: 基于公开教科书，采用紧凑的基于Transformer的架构，在单个GPU上训练。

Result: 虽数据集小，但能捕捉核专业词汇，不过生成文本句法有时缺乏连贯性。

Conclusion: 此方法可行，模型对专业任务有用，需丰富语料、改进预处理和微调指令，还提出未来改进方向。

Abstract: This paper introduces a domain-specific Large Language Model for nuclear
applications, built from the publicly accessible Essential CANDU textbook.
Drawing on a compact Transformer-based architecture, the model is trained on a
single GPU to protect the sensitive data inherent in nuclear operations.
Despite relying on a relatively small dataset, it shows encouraging signs of
capturing specialized nuclear vocabulary, though the generated text sometimes
lacks syntactic coherence. By focusing exclusively on nuclear content, this
approach demonstrates the feasibility of in-house LLM solutions that align with
rigorous cybersecurity and data confidentiality standards. Early successes in
text generation underscore the model's utility for specialized tasks, while
also revealing the need for richer corpora, more sophisticated preprocessing,
and instruction fine-tuning to enhance domain accuracy. Future directions
include extending the dataset to cover diverse nuclear subtopics, refining
tokenization to reduce noise, and systematically evaluating the model's
readiness for real-world applications in nuclear domain.

</details>


### [263] [Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL](https://arxiv.org/abs/2506.08757)
*Mishca de Costa,Muhammad Anwar,Dave Mercier,Mark Randall,Issam Hammad*

Main category: cs.CL

TL;DR: 传统NL - to - SQL在核电厂数据检索有风险，本文提出用函数调用大语言模型的混合方法，对比显示新方法在准确性和可维护性上有提升。


<details>
  <summary>Details</summary>
Motivation: 传统NL - to - SQL在核电厂数据检索存在用户难验证查询、数据库复杂导致查询生成困难等问题，增加了不准确的可能性并降低信任度。

Method: 定义一组预批准、特定用途的函数，代表常见用例，通过调用这些封装了验证过SQL逻辑的函数来处理查询，同时利用NL - to - SQL工具辅助生成函数代码，专家专注验证。

Result: 通过性能对比，显示出函数调用方法在准确性和可维护性上比直接的NL - to - SQL生成有改进。

Conclusion: 强调平衡用户可访问性和操作安全性的重要性，为关键系统的数据检索提供了新的可行框架。

Abstract: Retrieving operational data from nuclear power plants requires exceptional
accuracy and transparency due to the criticality of the decisions it supports.
Traditionally, natural language to SQL (NL-to-SQL) approaches have been
explored for querying such data. While NL-to-SQL promises ease of use, it poses
significant risks: end-users cannot easily validate generated SQL queries, and
legacy nuclear plant databases -- often complex and poorly structured --
complicate query generation due to decades of incremental modifications. These
challenges increase the likelihood of inaccuracies and reduce trust in the
approach. In this work, we propose an alternative paradigm: leveraging
function-calling large language models (LLMs) to address these challenges.
Instead of directly generating SQL queries, we define a set of pre-approved,
purpose-specific functions representing common use cases. Queries are processed
by invoking these functions, which encapsulate validated SQL logic. This hybrid
approach mitigates the risks associated with direct NL-to-SQL translations by
ensuring that SQL queries are reviewed and optimized by experts before
deployment. While this strategy introduces the upfront cost of developing and
maintaining the function library, we demonstrate how NL-to-SQL tools can assist
in the initial generation of function code, allowing experts to focus on
validation rather than creation. Our study includes a performance comparison
between direct NL-to-SQL generation and the proposed function-based approach,
highlighting improvements in accuracy and maintainability. This work
underscores the importance of balancing user accessibility with operational
safety and provides a novel, actionable framework for robust data retrieval in
critical systems.

</details>


### [264] [AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](https://arxiv.org/abs/2506.08885)
*Danush Khanna,Krishna Kumar,Basab Ghosh,Vinija Jain,Vasu Sharma,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: 文章指出大语言模型面临对抗性威胁，提出ALKALI基准，揭示潜在伪装漏洞，引入GRACE框架和AVQI指标缓解问题并量化潜在对齐失败，代码公开。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对抗性威胁增长快，现有防御机制难以适应，需解决潜在伪装漏洞。

Method: 引入ALKALI基准评估21个大语言模型，提出GRACE框架重塑内部几何结构，引入AVQI指标量化潜在对齐失败。

Result: 评估显示大语言模型攻击成功率高，GRACE框架最多降低39%的攻击成功率，AVQI指标可揭示不安全完成模仿安全完成几何结构的情况。

Conclusion: 大语言模型存在潜在伪装漏洞，GRACE框架和AVQI指标有助于缓解和量化该问题。

Abstract: Adversarial threats against LLMs are escalating faster than current defenses
can adapt. We expose a critical geometric blind spot in alignment: adversarial
prompts exploit latent camouflage, embedding perilously close to the safe
representation manifold while encoding unsafe intent thereby evading surface
level defenses like Direct Preference Optimization (DPO), which remain blind to
the latent geometry. We introduce ALKALI, the first rigorously curated
adversarial benchmark and the most comprehensive to date spanning 9,000 prompts
across three macro categories, six subtypes, and fifteen attack families.
Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates
(ASRs) across both open and closed source models, exposing an underlying
vulnerability we term latent camouflage, a structural blind spot where
adversarial completions mimic the latent geometry of safe ones. To mitigate
this vulnerability, we introduce GRACE - Geometric Representation Aware
Contrastive Enhancement, an alignment framework coupling preference learning
with latent space regularization. GRACE enforces two constraints: latent
separation between safe and adversarial completions, and adversarial cohesion
among unsafe and jailbreak behaviors. These operate over layerwise pooled
embeddings guided by a learned attention profile, reshaping internal geometry
without modifying the base model, and achieve up to 39% ASR reduction.
Moreover, we introduce AVQI, a geometry aware metric that quantifies latent
alignment failure via cluster separation and compactness. AVQI reveals when
unsafe completions mimic the geometry of safe ones, offering a principled lens
into how models internally encode safety. We make the code publicly available
at https://anonymous.4open.science/r/alkali-B416/README.md.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [265] [syren-baryon: Analytic emulators for the impact of baryons on the matter power spectrum](https://arxiv.org/abs/2506.08783)
*Lukas Kammerer,Deaglan J. Bartlett,Gabriel Kronberger,Harry Desmond,Pedro G. Ferreira*

Main category: astro-ph.CO

TL;DR: 为重子物理对物质功率谱的影响构建符号参数化，提供近似函数及误差描述，代码公开。


<details>
  <summary>Details</summary>
Motivation: 重子物理对宇宙物质分布有重要影响，需简单符号参数化其对物质功率谱的影响。

Method: 使用符号回归构建解析近似，基于CAMELS水动力学模拟的四个子网格处方及重子化算法。

Result: 得到近似函数及误差描述，误差与先前数值模拟器相当，表达式符合大尺度和高红移物理行为。

Conclusion: 表达式可直接解释参数影响，能区分不同重子物理模型，代码公开可用。

Abstract: Baryonic physics has a considerable impact on the distribution of matter in
our Universe on scales probed by current and future cosmological surveys,
acting as a key systematic in such analyses. We seek simple symbolic
parametrisations for the impact of baryonic physics on the matter power
spectrum for a range of physically motivated models, as a function of
wavenumber, redshift, cosmology, and parameters controlling the baryonic
feedback. We use symbolic regression to construct analytic approximations for
the ratio of the matter power spectrum in the presence of baryons to that
without such effects. We obtain separate functions of each of four distinct
sub-grid prescriptions of baryonic physics from the CAMELS suite of
hydrodynamical simulations (Astrid, IllustrisTNG, SIMBA and Swift-EAGLE) as
well as for a baryonification algorithm. We also provide functions which
describe the uncertainty on these predictions, due to both the stochastic
nature of baryonic physics and the errors on our fits. The error on our
approximations to the hydrodynamical simulations is comparable to the sample
variance estimated through varying initial conditions, and our baryonification
expression has a root mean squared error of better than one percent, although
this increases on small scales. These errors are comparable to those of
previous numerical emulators for these models. Our expressions are enforced to
have the physically correct behaviour on large scales and at high redshift. Due
to their analytic form, we are able to directly interpret the impact of varying
cosmology and feedback parameters, and we can identify parameters which have
little to no effect. Each function is based on a different implementation of
baryonic physics, and can therefore be used to discriminate between these
models when applied to real data. We provide publicly available code for all
symbolic approximations found.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [266] [The World of AI: A Novel Approach to AI Literacy for First-year Engineering Students](https://arxiv.org/abs/2506.08041)
*Siddharth Siddharth,Brainerd Prince,Amol Harsh,Shreyas Ramachandran*

Main category: cs.CY

TL;DR: 本文介绍为大一工科生设计的跨学科AI课程，该课程分三模块，结果显示提升了学生对AI挑战的理解和认知。


<details>
  <summary>Details</summary>
Motivation: 解决工科生在学术初期缺乏AI基础知识及其社会影响认知的问题。

Method: 设计并开设跨学科课程，分行星、社会影响和职场三个模块，由工程和人文学院教师共同授课。

Result: 学生在多个指标上对AI挑战的理解有所提升，对AI对生活的变革性影响的认知也有变化。

Conclusion: 跨学科的课程设计和教学方法是有效的，能帮助学生理解AI及其影响。

Abstract: This work presents a novel course titled The World of AI designed for
first-year undergraduate engineering students with little to no prior exposure
to AI. The central problem addressed by this course is that engineering
students often lack foundational knowledge of AI and its broader societal
implications at the outset of their academic journeys. We believe the way to
address this gap is to design and deliver an interdisciplinary course that can
a) be accessed by first-year undergraduate engineering students across any
domain, b) enable them to understand the basic workings of AI systems sans
mathematics, and c) make them appreciate AI's far-reaching implications on our
lives. The course was divided into three modules co-delivered by faculty from
both engineering and humanities. The planetary module explored AI's dual role
as both a catalyst for sustainability and a contributor to environmental
challenges. The societal impact module focused on AI biases and concerns around
privacy and fairness. Lastly, the workplace module highlighted AI-driven job
displacement, emphasizing the importance of adaptation. The novelty of this
course lies in its interdisciplinary curriculum design and pedagogical
approach, which combines technical instruction with societal discourse. Results
revealed that students' comprehension of AI challenges improved across diverse
metrics like (a) increased awareness of AI's environmental impact, and (b)
efficient corrective solutions for AI fairness. Furthermore, it also indicated
the evolution in students' perception of AI's transformative impact on our
lives.

</details>


### [267] [Evaluation of Machine Learning Models in Student Academic Performance Prediction](https://arxiv.org/abs/2506.08047)
*A. G. R. Sandeepa,Sanka Mohottala*

Main category: cs.CY

TL;DR: 研究用机器学习方法预测学生学业成绩，MLPC表现佳，凸显神经网络潜力并采用特征选择等方法。


<details>
  <summary>Details</summary>
Motivation: 探究在学校环境中运用机器学习方法预测学生学业成绩。

Method: 使用含行为、学业和人口统计细节的学生数据，采用标准经典机器学习模型如MLPC，运用特征选择方法、多种评估方法和可解释机器学习方法。

Result: MLPC测试集最高准确率达86.46%，10折交叉验证下测试集平均准确率79.58%，训练集为99.65%，MLP比其他模型表现好。

Conclusion: 神经网络作为数据高效模型有潜在应用价值，特征选择方法对提升性能至关重要。

Abstract: This research investigates the use of machine learning methods to forecast
students' academic performance in a school setting. Students' data with
behavioral, academic, and demographic details were used in implementations with
standard classical machine learning models including multi-layer perceptron
classifier (MLPC). MLPC obtained 86.46% maximum accuracy for test set across
all implementations. Under 10-fold cross validation, MLPC obtained 79.58%
average accuracy for test set while for train set, it was 99.65%. MLP's better
performance over other machine learning models strongly suggest the potential
use of neural networks as data-efficient models. Feature selection approach
played a crucial role in improving the performance and multiple evaluation
approaches were used in order to compare with existing literature. Explainable
machine learning methods were utilized to demystify the black box models and to
validate the feature selection approach.

</details>


### [268] [WIP: Large Language Model-Enhanced Smart Tutor for Undergraduate Circuit Analysis](https://arxiv.org/abs/2506.08962)
*Liangliang Chen,Huiru Xie,Jacqueline Rohde,Ying Zhang*

Main category: cs.CY

TL;DR: 本文介绍了用于本科电路分析课程的智能辅导系统，部署后收集学生反馈评估效果，未来将做全量分析并探索更多工程学科应用。


<details>
  <summary>Details</summary>
Motivation: 为本科电路分析课程的学生提供作业评估和反馈，帮助教师了解学生困难以进行针对性教学。

Method: 详细设计智能辅导系统的核心组件，部署在微软Azure平台，收集学生反馈数据并分析。

Result: 90.9%的学生对辅导系统表示满意，分析初步数据可了解学生问题和系统使用频率。

Conclusion: 智能辅导系统有效，后续将等完整数据集发布进行全量分析，并探索其在更多工程学科的应用。

Abstract: This research-to-practice work-in-progress (WIP) paper presents an AI-enabled
smart tutor designed to provide homework assessment and feedback for students
in an undergraduate circuit analysis course. We detail the tutor's design
philosophy and core components, including open-ended question answering and
homework feedback generation. The prompts are carefully crafted to optimize
responses across different problems. The smart tutor was deployed on the
Microsoft Azure platform and is currently in use in an undergraduate circuit
analysis course at the School of Electrical and Computer Engineering in a
large, public, research-intensive institution in the Southeastern United
States. Beyond offering personalized instruction and feedback, the tutor
collects student interaction data, which is summarized and shared with the
course instructor. To evaluate its effectiveness, we collected student
feedback, with 90.9% of responses indicating satisfaction with the tutor.
Additionally, we analyze a subset of collected data on preliminary circuit
analysis topics to assess tutor usage frequency for each problem and identify
frequently asked questions. These insights help instructors gain real-time
awareness of student difficulties, enabling more targeted classroom
instruction. In future work, we will release a full analysis once the complete
dataset is available after the Spring 2025 semester. We also explore the
potential applications of this smart tutor across a broader range of
engineering disciplines by developing improved prompts, diagram-recognition
methods, and database management strategies, which remain ongoing areas of
research.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [269] [Domain Switching on the Pareto Front: Multi-Objective Deep Kernel Learning in Automated Piezoresponse Force Microscopy](https://arxiv.org/abs/2506.08073)
*Yu Liu,Utkarsh Pratiush,Kamyar Barakati,Hiroshi Funakubo,Ching-Che Lin,Jaegyu Kim,Lane W. Martin,Sergei V. Kalinin*

Main category: cond-mat.mtrl-sci

TL;DR: 提出多目标核学习工作流程，从高分辨率成像数据推断铁电极化切换的微观结构规则，适用于复杂设计空间。


<details>
  <summary>Details</summary>
Motivation: 铁电极化切换受复杂微观结构特征影响，手动或基于网格的光谱测量难以系统探索。

Method: 引入多目标核学习工作流程，应用于自动压电力显微镜实验，并进行实验后分析。

Result: 有效识别畴壁配置与局部开关动力学的关键关系，揭示特定壁几何形状和缺陷分布对极化反转的调制。

Conclusion: 该方法不仅实现高通量主动学习，还能深入理解切换现象的微观结构控制，是导航复杂、不可微设计空间的通用工具。

Abstract: Ferroelectric polarization switching underpins the functional performance of
a wide range of materials and devices, yet its dependence on complex local
microstructural features renders systematic exploration by manual or grid-based
spectroscopic measurements impractical. Here, we introduce a multi-objective
kernel-learning workflow that infers the microstructural rules governing
switching behavior directly from high-resolution imaging data. Applied to
automated piezoresponse force microscopy (PFM) experiments, our framework
efficiently identifies the key relationships between domain-wall configurations
and local switching kinetics, revealing how specific wall geometries and defect
distributions modulate polarization reversal. Post-experiment analysis projects
abstract reward functions, such as switching ease and domain symmetry, onto
physically interpretable descriptors including domain configuration and
proximity to boundaries. This enables not only high-throughput active learning,
but also mechanistic insight into the microstructural control of switching
phenomena. While demonstrated for ferroelectric domain switching, our approach
provides a powerful, generalizable tool for navigating complex,
non-differentiable design spaces, from structure-property correlations in
molecular discovery to combinatorial optimization across diverse imaging
modalities.

</details>


### [270] [Mic-hackathon 2024: Hackathon on Machine Learning for Electron and Scanning Probe Microscopy](https://arxiv.org/abs/2506.08423)
*Utkarsh Pratiush,Austin Houston,Kamyar Barakati,Aditya Raghavan,Dasol Yoon,Harikrishnan KP,Zhaslan Baraissov,Desheng Ma,Samuel S. Welborn,Mikolaj Jakowski,Shawn-Patrick Barhorst,Alexander J. Pattison,Panayotis Manganaris,Sita Sirisha Madugula,Sai Venkata Gayathri Ayyagari,Vishal Kennedy,Ralph Bulanadi,Michelle Wang,Kieran J. Pang,Ian Addison-Smith,Willy Menacho,Horacio V. Guzman,Alexander Kiefer,Nicholas Furth,Nikola L. Kolev,Mikhail Petrov,Viktoriia Liu,Sergey Ilyev,Srikar Rairao,Tommaso Rodani,Ivan Pinto-Huguet,Xuli Chen,Josep Cruañes,Marta Torrens,Jovan Pomar,Fanzhi Su,Pawan Vedanti,Zhiheng Lyu,Xingzhi Wang,Lehan Yao,Amir Taqieddin,Forrest Laskowski,Xiangyu Yin,Yu-Tsun Shao,Benjamin Fein-Ashley,Yi Jiang,Vineet Kumar,Himanshu Mishra,Yogesh Paul,Adib Bazgir,Rama chandra Praneeth Madugula,Yuwen Zhang,Pravan Omprakash,Jian Huang,Eric Montufar-Morales,Vivek Chawla,Harshit Sethi,Jie Huang,Lauri Kurki,Grace Guinan,Addison Salvador,Arman Ter-Petrosyan,Madeline Van Winkle,Steven R. Spurgeon,Ganesh Narasimha,Zijie Wu,Richard Liu,Yongtao Liu,Boris Slautin,Andrew R Lupini,Rama Vasudevan,Gerd Duscher,Sergei V. Kalinin*

Main category: cond-mat.mtrl-sci

TL;DR: 显微镜数据结构化但分析难，缺乏标准生态等，新 API 有潜力但 ML 与显微镜社区有差距，黑客松活动促进合作，产出基准数据集和数字孪生，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决显微镜数据利用低效、分析时间长的问题，弥合 ML 与显微镜社区的差距，推动物理学、材料发现和优化。

Method: 举办黑客松活动，促进 ML 研究人员和显微镜专家合作。

Result: 产生了基准数据集和显微镜的数字孪生，支持社区发展和标准化工作流程。

Conclusion: 黑客松活动有助于促进 ML 在显微镜领域的应用，为未来相关领域培养人才。

Abstract: Microscopy is a primary source of information on materials structure and
functionality at nanometer and atomic scales. The data generated is often
well-structured, enriched with metadata and sample histories, though not always
consistent in detail or format. The adoption of Data Management Plans (DMPs) by
major funding agencies promotes preservation and access. However, deriving
insights remains difficult due to the lack of standardized code ecosystems,
benchmarks, and integration strategies. As a result, data usage is inefficient
and analysis time is extensive. In addition to post-acquisition analysis, new
APIs from major microscope manufacturers enable real-time, ML-based analytics
for automated decision-making and ML-agent-controlled microscope operation.
Yet, a gap remains between the ML and microscopy communities, limiting the
impact of these methods on physics, materials discovery, and optimization.
Hackathons help bridge this divide by fostering collaboration between ML
researchers and microscopy experts. They encourage the development of novel
solutions that apply ML to microscopy, while preparing a future workforce for
instrumentation, materials science, and applied ML. This hackathon produced
benchmark datasets and digital twins of microscopes to support community growth
and standardized workflows. All related code is available at GitHub:
https://github.com/KalininGroup/Mic-hackathon-2024-codes-publication/tree/1.0.0.1

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [271] [Aligning Proteins and Language: A Foundation Model for Protein Retrieval](https://arxiv.org/abs/2506.08023)
*Qifeng Wu,Zhengzhe Liu,Han Zhu,Yizhou Zhao,Daisuke Kihara,Min Xu*

Main category: q-bio.BM

TL;DR: 本文提出CLIP风格框架，用对比学习对齐蛋白质结构与功能注释，在检索任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 从大规模蛋白质数据集中检索具有相似结构和语义的蛋白质，助力通过冷冻电镜等方法确定的蛋白质结构的功能解释。

Method: 提出CLIP风格框架，利用对比学习对齐3D蛋白质结构和功能注释；构建约200,000个蛋白质 - 描述对的大规模数据集用于模型训练。

Result: 在PDB和EMDB数据集的领域内和跨数据库检索中，模型均展现出良好的零样本检索性能。

Conclusion: 多模态基础模型在蛋白质生物学的结构 - 功能理解方面具有潜力。

Abstract: This paper aims to retrieve proteins with similar structures and semantics
from large-scale protein dataset, facilitating the functional interpretation of
protein structures derived by structural determination methods like
cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of
vision-language models (VLMs), we propose a CLIP-style framework for aligning
3D protein structures with functional annotations using contrastive learning.
For model training, we propose a large-scale dataset of approximately 200,000
protein-caption pairs with rich functional descriptors. We evaluate our model
in both in-domain and more challenging cross-database retrieval on Protein Data
Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In
both cases, our approach demonstrates promising zero-shot retrieval
performance, highlighting the potential of multimodal foundation models for
structure-function understanding in protein biology.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [272] [UAVs Meet Agentic AI: A Multidomain Survey of Autonomous Aerial Intelligence and Agentic UAVs](https://arxiv.org/abs/2506.08045)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.RO

TL;DR: 本文介绍智能无人机特点、对比传统无人机优势，探讨应用领域，指出挑战与解决方案并提出未来路线图，为其发展、部署和治理奠定基础。


<details>
  <summary>Details</summary>
Motivation: 随着智能人工智能发展，研究智能无人机以理解其与传统无人机区别，挖掘其在各领域价值。

Method: 对智能无人机架构组件和使能技术进行综合分析，对比传统无人机，探讨应用领域，识别挑战并提出解决方案。

Result: 明确智能无人机优势，探索七个高影响应用领域，识别技术、监管和数据模型可靠性方面挑战并给出解决方案。

Conclusion: 为智能无人机在不同社会和工业领域的未来发展、部署和治理建立了基础框架。

Abstract: Agentic UAVs represent a new frontier in autonomous aerial intelligence,
integrating perception, decision-making, memory, and collaborative planning to
operate adaptively in complex, real-world environments. Driven by recent
advances in Agentic AI, these systems surpass traditional UAVs by exhibiting
goal-driven behavior, contextual reasoning, and interactive autonomy. We
provide a comprehensive foundation for understanding the architectural
components and enabling technologies that distinguish Agentic UAVs from
traditional autonomous UAVs. Furthermore, a detailed comparative analysis
highlights advancements in autonomy with AI agents, learning, and mission
flexibility. This study explores seven high-impact application domains
precision agriculture, construction & mining, disaster response, environmental
monitoring, infrastructure inspection, logistics, security, and wildlife
conservation, illustrating the broad societal value of agentic aerial
intelligence. Furthermore, we identify key challenges in technical constraints,
regulatory limitations, and data-model reliability, and we present emerging
solutions across hardware innovation, learning architectures, and human-AI
interaction. Finally, a future roadmap is proposed, outlining pathways toward
self-evolving aerial ecosystems, system-level collaboration, and sustainable,
equitable deployments. This survey establishes a foundational framework for the
future development, deployment, and governance of agentic aerial systems
(Agentic UAVs) across diverse societal and industrial domains.

</details>


### [273] [Ego-centric Learning of Communicative World Models for Autonomous Driving](https://arxiv.org/abs/2506.08149)
*Hang Wang,Dechen Gao,Junshan Zhang*

Main category: cs.RO

TL;DR: 本文提出CALL方法用于复杂高维环境多智能体强化学习，利用生成式AI和世界模型的潜在表示，实验证明其在CARLA平台局部轨迹规划任务中有性能提升。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习存在部分可观测性和非平稳性问题，信息共享面临通信开销大与可扩展性问题。

Method: 开发CALL方法，每个智能体学习世界模型将状态和意图编码为低维潜在表示进行轻量级通信，同时进行以自我为中心的学习并利用信息共享丰富世界模型。

Result: 在CARLA平台的局部轨迹规划任务上进行大量实验，证明CALL方法有性能提升。

Conclusion: CALL方法能有效利用轻量级信息共享，提升多智能体强化学习在复杂高维环境中的性能。

Abstract: We study multi-agent reinforcement learning (MARL) for tasks in complex
high-dimensional environments, such as autonomous driving. MARL is known to
suffer from the \textit{partial observability} and \textit{non-stationarity}
issues. To tackle these challenges, information sharing is often employed,
which however faces major hurdles in practice, including overwhelming
communication overhead and scalability concerns. By making use of generative AI
embodied in world model together with its latent representation, we develop
{\it CALL}, \underline{C}ommunic\underline{a}tive Wor\underline{l}d
Mode\underline{l}, for MARL, where 1) each agent first learns its world model
that encodes its state and intention into low-dimensional latent representation
with smaller memory footprint, which can be shared with other agents of
interest via lightweight communication; and 2) each agent carries out
ego-centric learning while exploiting lightweight information sharing to enrich
her world model, and then exploits its generalization capacity to improve
prediction for better planning. We characterize the gain on the prediction
accuracy from the information sharing and its impact on performance gap.
Extensive experiments are carried out on the challenging local trajectory
planning tasks in the CARLA platform to demonstrate the performance gains of
using \textit{CALL}.

</details>


### [274] [Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning](https://arxiv.org/abs/2506.08344)
*Neşet Ünver Akmandor,Sarvesh Prajapati,Mark Zolotas,Taşkın Padır*

Main category: cs.RO

TL;DR: 提出新的多模型运动规划管道Re4MPC，用NMPC计算轨迹，通过DRL学习决策策略，实验显示其比NMPC基线更高效且成功率更高。


<details>
  <summary>Details</summary>
Motivation: 传统多自由度机器人运动规划方法在现实场景计算成本高，需要更高效的方法。

Method: 提出Re4MPC，根据任务复杂度和机器人状态选择NMPC问题的模型、成本和约束，通过DRL框架学习决策策略，引入数学公式将NMPC集成到DRL框架。

Result: 在基于物理的模拟实验中，Re4MPC比无学习机制的NMPC基线计算更高效，达到末端执行器目标的成功率更高。

Conclusion: Re4MPC是一种有效的多自由度机器人运动规划方法，在计算效率和任务成功率上表现更优。

Abstract: Traditional motion planning methods for robots with many degrees-of-freedom,
such as mobile manipulators, are often computationally prohibitive for
real-world settings. In this paper, we propose a novel multi-model motion
planning pipeline, termed Re4MPC, which computes trajectories using Nonlinear
Model Predictive Control (NMPC). Re4MPC generates trajectories in a
computationally efficient manner by reactively selecting the model, cost, and
constraints of the NMPC problem depending on the complexity of the task and
robot state. The policy for this reactive decision-making is learned via a Deep
Reinforcement Learning (DRL) framework. We introduce a mathematical formulation
to integrate NMPC into this DRL framework. To validate our methodology and
design choices, we evaluate DRL training and test outcomes in a physics-based
simulation involving a mobile manipulator. Experimental results demonstrate
that Re4MPC is more computationally efficient and achieves higher success rates
in reaching end-effector goals than the NMPC baseline, which computes
whole-body trajectories without our learning mechanism.

</details>


### [275] [Diffusion Models for Safety Validation of Autonomous Driving Systems](https://arxiv.org/abs/2506.08459)
*Juanran Wang,Marc R. Schlichting,Harrison Delecki,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: 为解决自动驾驶系统安全验证难题，训练去噪扩散模型生成潜在故障案例，实验表明该模型可生成真实故障样本，适用于交通路口安全验证。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统安全验证因真实测试风险高、成本大以及潜在故障罕见多样而极具挑战。

Method: 训练去噪扩散模型，根据初始交通状态生成自动驾驶车辆潜在故障案例。

Result: 在四路交叉口问题实验中，扩散模型能在多种场景下生成真实故障样本，捕获多种潜在故障。模型无需外部训练数据集，用适度计算资源即可训练和推理，且不依赖被测系统先验知识。

Conclusion: 模型适用于交通路口的安全验证。

Abstract: Safety validation of autonomous driving systems is extremely challenging due
to the high risks and costs of real-world testing as well as the rarity and
diversity of potential failures. To address these challenges, we train a
denoising diffusion model to generate potential failure cases of an autonomous
vehicle given any initial traffic state. Experiments on a four-way intersection
problem show that in a variety of scenarios, the diffusion model can generate
realistic failure samples while capturing a wide variety of potential failures.
Our model does not require any external training dataset, can perform training
and inference with modest computing resources, and does not assume any prior
knowledge of the system under test, with applicability to safety validation for
traffic intersections.

</details>


### [276] [Bayesian Inverse Physics for Neuro-Symbolic Robot Learning](https://arxiv.org/abs/2506.08756)
*Octavio Arriaga,Rebecca Adam,Melvin Laux,Lisa Gutzeit,Marco Ragni,Jan Peters,Frank Kirchner*

Main category: cs.RO

TL;DR: 本文指出深度学习在机器人应用中的局限，提出结合数据驱动学习与结构化推理的概念框架，并给出研究路线图。


<details>
  <summary>Details</summary>
Motivation: 现实机器人应用需要自适应、可解释和数据高效的学习范式，而深度学习在未知和动态环境中存在局限。

Method: 提出利用可微物理进行世界建模、贝叶斯推理进行不确定性决策、元学习快速适应新任务，将物理符号推理嵌入神经模型。

Result: 未提及具体结果。

Conclusion: 混合神经符号架构对下一代自主系统至关重要，并提供研究路线图以推动其发展。

Abstract: Real-world robotic applications, from autonomous exploration to assistive
technologies, require adaptive, interpretable, and data-efficient learning
paradigms. While deep learning architectures and foundation models have driven
significant advances in diverse robotic applications, they remain limited in
their ability to operate efficiently and reliably in unknown and dynamic
environments. In this position paper, we critically assess these limitations
and introduce a conceptual framework for combining data-driven learning with
deliberate, structured reasoning. Specifically, we propose leveraging
differentiable physics for efficient world modeling, Bayesian inference for
uncertainty-aware decision-making, and meta-learning for rapid adaptation to
new tasks. By embedding physical symbolic reasoning within neural models,
robots could generalize beyond their training data, reason about novel
situations, and continuously expand their knowledge. We argue that such hybrid
neuro-symbolic architectures are essential for the next generation of
autonomous systems, and to this end, we provide a research roadmap to guide and
accelerate their development.

</details>


### [277] [Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning](https://arxiv.org/abs/2506.08795)
*Kaijie Shi,Wanglong Lu,Hanli Zhao,Vinicius Prado da Fonseca,Ting Zou,Xianta Jiang*

Main category: cs.RO

TL;DR: 研究开发仅用腕部相机的全自主假肢手控制系统，用模仿学习训练模型，少量数据训练后算法成功率高。


<details>
  <summary>Details</summary>
Motivation: 传统sEMG和半自主方法对截肢者身心负担大，需开发易用、省力的假肢控制系统。

Method: 开发遥操作系统收集人类演示数据，用模仿学习训练假肢手控制模型。

Result: 仅用一名参与者少量物体数据训练模型，模仿学习算法成功率高，可推广到更多人和未见物体。

Conclusion: 所开发的全自主假肢手控制系统能为截肢者提供易用接口，大幅降低使用时的心理负担。

Abstract: Limb loss affects millions globally, impairing physical function and reducing
quality of life. Most traditional surface electromyographic (sEMG) and
semi-autonomous methods require users to generate myoelectric signals for each
control, imposing physically and mentally taxing demands. This study aims to
develop a fully autonomous control system that enables a prosthetic hand to
automatically grasp and release objects of various shapes using only a camera
attached to the wrist. By placing the hand near an object, the system will
automatically execute grasping actions with a proper grip force in response to
the hand's movements and the environment. To release the object being grasped,
just naturally place the object close to the table and the system will
automatically open the hand. Such a system would provide individuals with limb
loss with a very easy-to-use prosthetic control interface and greatly reduce
mental effort while using. To achieve this goal, we developed a teleoperation
system to collect human demonstration data for training the prosthetic hand
control model using imitation learning, which mimics the prosthetic hand
actions from human. Through training the model using only a few objects' data
from one single participant, we have shown that the imitation learning
algorithm can achieve high success rates, generalizing to more individuals and
unseen objects with a variation of weights. The demonstrations are available at
\href{https://sites.google.com/view/autonomous-prosthetic-hand}{https://sites.google.com/view/autonomous-prosthetic-hand}

</details>


### [278] [FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency](https://arxiv.org/abs/2506.08822)
*Yifei Su,Ning Liu,Dong Chen,Zhen Zhao,Kun Wu,Meng Li,Zhiyuan Xu,Zhengping Che,Jian Tang*

Main category: cs.RO

TL;DR: 提出FreqPolicy方法解决生成式视觉运动策略推理成本高问题，在模拟和真实场景验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式视觉运动策略推理成本高，且现有加速方法未有效利用机器人操作中的时间信息。

Method: 提出FreqPolicy，对基于流的视觉运动策略施加频率一致性约束，设计自适应一致性损失。

Result: 在3个模拟基准的53个任务中证明优于现有一步动作生成器，集成到VLA模型无性能损失，在真实场景推理频率达93.5Hz。

Conclusion: FreqPolicy能有效利用时间信息，实现高效、高质量的一步动作生成。

Abstract: Generative modeling-based visuomotor policies have been widely adopted in
robotic manipulation attributed to their ability to model multimodal action
distributions. However, the high inference cost of multi-step sampling limits
their applicability in real-time robotic systems. To address this issue,
existing approaches accelerate the sampling process in generative
modeling-based visuomotor policies by adapting acceleration techniques
originally developed for image generation. Despite this progress, a major
distinction remains: image generation typically involves producing independent
samples without temporal dependencies, whereas robotic manipulation involves
generating time-series action trajectories that require continuity and temporal
coherence. To effectively exploit temporal information in robotic manipulation,
we propose FreqPolicy, a novel approach that first imposes frequency
consistency constraints on flow-based visuomotor policies. Our work enables the
action model to capture temporal structure effectively while supporting
efficient, high-quality one-step action generation. We introduce a frequency
consistency constraint that enforces alignment of frequency-domain action
features across different timesteps along the flow, thereby promoting
convergence of one-step action generation toward the target distribution. In
addition, we design an adaptive consistency loss to capture structural temporal
variations inherent in robotic manipulation tasks. We assess FreqPolicy on 53
tasks across 3 simulation benchmarks, proving its superiority over existing
one-step action generators. We further integrate FreqPolicy into the
vision-language-action (VLA) model and achieve acceleration without performance
degradation on the 40 tasks of Libero. Besides, we show efficiency and
effectiveness in real-world robotic scenarios with an inference frequency
93.5Hz. The code will be publicly available.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [279] [Mean-Field-Type Game Theory with Rosenblatt Noise](https://arxiv.org/abs/2506.08025)
*Hamidou Tembine,Tyrone E. Duncan,Bozenna Pasik-Duncan*

Main category: math.OC

TL;DR: 研究将Rosenblatt噪声集成到随机系统、控制理论和平均场型博弈论中，开发新的随机微积分公式，分析控制问题和博弈论，强调纳入非高斯噪声的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统高斯和马尔可夫模型有局限性，现实数据存在非高斯特征，需要更好的模型。

Method: 为Rosenblatt过程开发新的随机微积分公式，应用于动力系统，分析最优控制问题，扩展博弈论分析。

Result: 揭示传统噪声近似方法的次优性，确定零和博弈的鞍点均衡条件和非零和博弈的状态反馈纳什均衡。

Conclusion: 纳入非高斯噪声对预测分析和控制策略很重要，提升了模型在现实应用中的准确性和鲁棒性，是平均场型博弈论的重要进展。

Abstract: We study the integration of Rosenblatt noise into stochastic systems, control
theory, and mean-field-type game theory, addressing the limitations of
traditional Gaussian and Markovian models. Empirical evidence from various
domains, including water demand, e-commerce, power grid operations, wireless
channels, and agricultural supply chains, demonstrates the prevalence of
non-Gaussian characteristics such as skews, heavy tails and strong long-range
dependencies. The Rosenblatt process, a non-Gaussian non-Markovian,
self-similar process, offers a baseline framework for capturing some the
behaviors observed in real data. We develop novel stochastic calculus formulas
for Rosenblatt processes, apply these to dynamical systems, and analyze optimal
control problems, revealing the suboptimality of traditional noise
approximation methods. We extend game-theoretic analysis to environments driven
by Rosenblatt noise, establishing conditions for saddle-point equilibria in
zero-sum games and identifying state-feedback Nash equilibria in non-zero-sum
games. Our findings underscore the importance of incorporating non-Gaussian
noise into predictive analytics and control strategies, enhancing the accuracy
and robustness of models in real-world applications. These findings represent a
significant advancement in mean-field-type game theory with variance-awareness,
offering new insights and tools for managing interactive systems influenced by
Rosenblatt noise.

</details>


### [280] [MOSS: Multi-Objective Optimization for Stable Rule Sets](https://arxiv.org/abs/2506.08030)
*Brian Liu,Rahul Mazumder*

Main category: math.OC

TL;DR: 提出MOSS多目标优化框架构建稳定决策规则集，开发算法计算Pareto前沿，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 将可解释性的三个重要标准（稀疏性、准确性和稳定性）整合到单一多目标优化框架，让从业者评估稀疏规则集准确性和稳定性的权衡。

Method: 开发专门的切割平面算法计算两个目标间的Pareto前沿。

Result: MOSS在预测性能和稳定性方面优于现有规则集成方法。

Conclusion: MOSS是构建稳定决策规则集的有效多目标优化框架。

Abstract: We present MOSS, a multi-objective optimization framework for constructing
stable sets of decision rules. MOSS incorporates three important criteria for
interpretability: sparsity, accuracy, and stability, into a single
multi-objective optimization framework. Importantly, MOSS allows a practitioner
to rapidly evaluate the trade-off between accuracy and stability in sparse rule
sets in order to select an appropriate model. We develop a specialized cutting
plane algorithm in our framework to rapidly compute the Pareto frontier between
these two objectives, and our algorithm scales to problem instances beyond the
capabilities of commercial optimization solvers. Our experiments show that MOSS
outperforms state-of-the-art rule ensembles in terms of both predictive
performance and stability.

</details>


### [281] [Continuous Policy and Value Iteration for Stochastic Control Problems and Its Convergence](https://arxiv.org/abs/2506.08121)
*Qi Feng,Gu Wang*

Main category: math.OC

TL;DR: 提出连续策略价值迭代算法，通过朗之万型动力学同时更新随机控制问题价值函数和最优控制，适用于两类问题，在哈密顿单调性条件下收敛。


<details>
  <summary>Details</summary>
Motivation: 解决随机控制问题中价值函数和最优控制的优化问题，将机器学习技术用于此类问题。

Method: 引入连续策略价值迭代算法，利用朗之万型随机微分方程沿策略迭代方向连续更新。

Result: 建立策略改进，在哈密顿单调性条件下证明收敛到最优控制。

Conclusion: 该算法可同时优化价值函数和确定最优控制，能利用机器学习中的分布采样和非凸学习技术。

Abstract: We introduce a continuous policy-value iteration algorithm where the
approximations of the value function of a stochastic control problem and the
optimal control are simultaneously updated through Langevin-type dynamics. This
framework applies to both the entropy-regularized relaxed control problems and
the classical control problems, with infinite horizon. We establish policy
improvement and demonstrate convergence to the optimal control under the
monotonicity condition of the Hamiltonian. By utilizing Langevin-type
stochastic differential equations for continuous updates along the policy
iteration direction, our approach enables the use of distribution sampling and
non-convex learning techniques in machine learning to optimize the value
function and identify the optimal control simultaneously.

</details>


### [282] [Solving Convex-Concave Problems with $\tilde{\mathcal{O}}(ε^{-4/7})$ Second-Order Oracle Complexity](https://arxiv.org/abs/2506.08362)
*Lesi Chen,Chengchang Liu,Luo Luo,Jingzhao Zhang*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Previous algorithms can solve convex-concave minimax problems $\min_{x \in
\mathcal{X}} \max_{y \in \mathcal{Y}} f(x,y)$ with
$\mathcal{O}(\epsilon^{-2/3})$ second-order oracle calls using Newton-type
methods. This result has been speculated to be optimal because the upper bound
is achieved by a natural generalization of the optimal first-order method. In
this work, we show an improved upper bound of
$\tilde{\mathcal{O}}(\epsilon^{-4/7})$ by generalizing the optimal second-order
method for convex optimization to solve the convex-concave minimax problem. We
further apply a similar technique to lazy Hessian algorithms and show that our
proposed algorithm can also be seen as a second-order ``Catalyst'' framework
(Lin et al., JMLR 2018) that could accelerate any globally convergent
algorithms for solving minimax problems.

</details>


### [283] [Sharper Convergence Rates for Nonconvex Optimisation via Reduction Mappings](https://arxiv.org/abs/2506.08428)
*Evan Markou,Thalaiyasingam Ajanthan,Stephen Gould*

Main category: math.OC

TL;DR: 提出通用框架理解降维映射对优化景观的影响，证明设计良好的映射可改善目标曲率，加速收敛。


<details>
  <summary>Details</summary>
Motivation: 许多高维优化问题的极小值集有丰富几何结构，利用这种结构的降维映射可产生低维目标，需理解其对优化景观的影响。

Method: 引入通用框架分析降维映射对优化景观的影响。

Result: 设计良好的降维映射能改善目标曲率特性，使问题条件更好，基于梯度的方法理论收敛更快。

Conclusion: 分析统一了利用最优结构信息加速收敛的场景，为优化算法的经验增益提供理论解释。

Abstract: Many high-dimensional optimisation problems exhibit rich geometric structures
in their set of minimisers, often forming smooth manifolds due to
over-parametrisation or symmetries. When this structure is known, at least
locally, it can be exploited through reduction mappings that reparametrise part
of the parameter space to lie on the solution manifold. These reductions
naturally arise from inner optimisation problems and effectively remove
redundant directions, yielding a lower-dimensional objective. In this work, we
introduce a general framework to understand how such reductions influence the
optimisation landscape. We show that well-designed reduction mappings improve
curvature properties of the objective, leading to better-conditioned problems
and theoretically faster convergence for gradient-based methods. Our analysis
unifies a range of scenarios where structural information at optimality is
leveraged to accelerate convergence, offering a principled explanation for the
empirical gains observed in such optimisation algorithms.

</details>


### [284] [Optimization over Sparse Support-Preserving Sets: Two-Step Projection with Global Optimality Guarantees](https://arxiv.org/abs/2506.08558)
*William de Vazelhes,Xiao-Tong Yuan,Bin Gu*

Main category: math.OC

TL;DR: 本文研究带额外支持保留约束的稀疏优化问题，提出新的迭代硬阈值算法，给出全局收敛保证并改进了零阶情况下的结果。


<details>
  <summary>Details</summary>
Motivation: 现有算法处理带额外约束的稀疏优化问题存在需闭形式投影及只有局部收敛保证的问题，本文旨在填补该空白。

Method: 提出配备两步连续投影算子的迭代硬阈值算法，引入稀疏松弛和次优性的权衡，开发经典三点引理的新扩展。

Result: 在确定性、随机和零阶设置下，算法输出在目标值上有全局保证，零阶情况改进了现有结果。

Conclusion: 新算法为带额外约束的稀疏优化问题提供了有效解决方案，证明技术有创新且能改进现有结果。

Abstract: In sparse optimization, enforcing hard constraints using the $\ell_0$
pseudo-norm offers advantages like controlled sparsity compared to convex
relaxations. However, many real-world applications demand not only sparsity
constraints but also some extra constraints. While prior algorithms have been
developed to address this complex scenario with mixed combinatorial and convex
constraints, they typically require the closed form projection onto the mixed
constraints which might not exist, and/or only provide local guarantees of
convergence which is different from the global guarantees commonly sought in
sparse optimization. To fill this gap, in this paper, we study the problem of
sparse optimization with extra \qw{\textit{support-preserving}} constraints
commonly encountered in the literature. We present a new variant of iterative
hard-thresholding algorithm equipped with a two-step consecutive projection
operator customized for these mixed constraints, serving as a simple
alternative to the Euclidean projection onto the mixed constraint. By
introducing a novel trade-off between sparsity relaxation and sub-optimality,
we provide global guarantees in objective value for the output of our
algorithm, in the deterministic, stochastic, and zeroth-order settings, under
the conventional restricted strong-convexity/smoothness assumptions. As a
fundamental contribution in proof techniques, we develop a novel extension of
the classic three-point lemma to the considered two-step non-convex projection
operator, which allows us to analyze the convergence in objective value in an
elegant way that has not been possible with existing techniques. In the
zeroth-order case, such technique also improves upon the state-of-the-art
result from de Vazelhes et. al. (2022), even in the case without additional
constraints, by allowing us to remove a non-vanishing system error present in
their work.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [285] [MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback](https://arxiv.org/abs/2506.08634)
*Alvaro Becerra,Daniel Andres,Pablo Villegas,Roberto Daza,Ruth Cobos*

Main category: cs.HC

TL;DR: 提出MOSAIC - F多模态反馈框架用于学生学习活动反馈，含四步骤，结合人机评估，在提升口语技能中测试。


<details>
  <summary>Details</summary>
Motivation: 为学生学习活动提供更准确、个性化和可操作的反馈。

Method: 框架包含四步：通过标准量规进行评估；收集多模态数据；用AI结合评估和数据生成反馈；学生通过视频回顾、自评和可视化反馈。

Result: 文中未明确提及具体测试结果。

Conclusion: 结合人机评估技术的MOSAIC - F框架能实现更准确、个性化和可操作的反馈。

Abstract: In this article, we present a novel multimodal feedback framework called
MOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal
Learning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),
and Collaborative assessments for generating personalized feedback on student
learning activities. This framework consists of four key steps. First, peers
and professors' assessments are conducted through standardized rubrics (that
include both quantitative and qualitative evaluations). Second, multimodal data
are collected during learning activities, including video recordings, audio
capture, gaze tracking, physiological signals (heart rate, motion data), and
behavioral interactions. Third, personalized feedback is generated using AI,
synthesizing human-based evaluations and data-based multimodal insights such as
posture, speech patterns, stress levels, and cognitive load, among others.
Finally, students review their own performance through video recordings and
engage in self-assessment and feedback visualization, comparing their own
evaluations with peers and professors' assessments, class averages, and
AI-generated recommendations. By combining human-based and data-based
evaluation techniques, this framework enables more accurate, personalized and
actionable feedback. We tested MOSAIC-F in the context of improving oral
presentation skills.

</details>


### [286] [Stop Misusing t-SNE and UMAP for Visual Analytics](https://arxiv.org/abs/2506.08725)
*Hyeon Jeon,Jeongin Park,Sungbok Shin,Jinwook Seo*

Main category: cs.HC

TL;DR: 本文指出t - SNE和UMAP在可视化分析中的滥用问题，通过文献回顾和访谈研究分析原因，最后提出促进合理使用降维技术的方向和措施。


<details>
  <summary>Details</summary>
Motivation: 解决t - SNE和UMAP在可视化分析中被频繁滥用的问题，探究滥用原因并找到预防方法。

Method: 对114篇论文进行文献回顾以验证滥用的普遍性并分析原因，开展访谈研究挖掘从业者使用这些技术的隐性动机。

Result: t - SNE和UMAP的滥用主要源于可视化分析中对其正确使用的讨论有限。

Conclusion: 提出促进降维技术更合理使用的未来方向和具体行动项目。

Abstract: Misuses of t-SNE and UMAP in visual analytics have become increasingly
common. For example, although t-SNE and UMAP projections often do not
faithfully reflect true distances between clusters, practitioners frequently
use them to investigate inter-cluster relationships. In this paper, we bring
this issue to the surface and comprehensively investigate why such misuse
occurs and how to prevent it. We conduct a literature review of 114 papers to
verify the prevalence of the misuse and analyze the reasonings behind it. We
then execute an interview study to uncover practitioners' implicit motivations
for using these techniques -- rationales often undisclosed in the literature.
Our findings indicate that misuse of t-SNE and UMAP primarily stems from
limited discourse on their appropriate use in visual analytics. We conclude by
proposing future directions and concrete action items to promote more
reasonable use of DR.

</details>


### [287] [Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU](https://arxiv.org/abs/2506.08911)
*Petar Jakuš,Hrvoje Džapo*

Main category: cs.HC

TL;DR: 本文在NXP MCXN947微控制器上实现关键词识别（KWS）系统，利用NPU实现实时语音交互，优化后效果良好。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上实现实时语音交互。

Method: 结合MFCC特征提取与CNN分类器，使用量化感知训练优化以减小模型大小。

Result: 利用NPU推理时间加速59倍，模型大小30.58 KB，准确率达97.06%。

Conclusion: 在嵌入式平台上实现高效、低功耗语音接口是可行的。

Abstract: This paper presents a keyword spotting (KWS) system implemented on the NXP
MCXN947 microcontroller with an integrated Neural Processing Unit (NPU),
enabling real-time voice interaction on resource-constrained devices. The
system combines MFCC feature extraction with a CNN classifier, optimized using
Quantization Aware Training to reduce model size with minimal accuracy drop.
Experimental results demonstrate a 59x speedup in inference time when
leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy
with a model size of 30.58 KB, demonstrating the feasibility of efficient,
low-power voice interfaces on embedded platforms.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [288] [TS-PIELM: Time-Stepping Physics-Informed Extreme Learning Machine Facilitates Soil Consolidation Analyses](https://arxiv.org/abs/2506.08381)
*He Yang,Fei Ren,Hai-Sui Yu,Xueyu Geng,Pei-Zhi Zhuang*

Main category: physics.geo-ph

TL;DR: 提出TS - PIELM方法解决传统PINN在土壤固结分析中准确性和效率问题，经测试性能优越，证明PIML可用于计算岩土工程。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络（PINN）在土壤固结分析中准确性和效率需提升，本文旨在克服这些局限。

Method: 提出时间步长物理信息极限学习机（TS - PIELM）方法，将固结过程划分为多个时间间隔，用单层前馈极限学习机（ELM）近似求解，随机生成并固定输入层权重，直接求解线性方程组计算输出层权重。

Result: 通过求解三个典型Terzaghi固结问题，一维情况下，与PINN相比，TS - PIELM计算效率提高超1000倍，准确性提高超100倍。

Conclusion: 物理信息机器学习（PIML）可成为计算岩土工程的有力工具。

Abstract: Accuracy and efficiency of the conventional physics-informed neural network
(PINN) need to be improved before it can be a competitive alternative for soil
consolidation analyses. This paper aims to overcome these limitations by
proposing a highly accurate and efficient physics-informed machine learning
(PIML) approach, termed time-stepping physics-informed extreme learning machine
(TS-PIELM). In the TS-PIELM framework the consolidation process is divided into
numerous time intervals, which helps overcome the limitation of PIELM in
solving differential equations with sharp gradients. To accelerate network
training, the solution is approximated by a single-layer feedforward extreme
learning machine (ELM), rather than using a fully connected neural network in
PINN. The input layer weights of the ELM network are generated randomly and
fixed during the training process. Subsequently, the output layer weights are
directly computed by solving a system of linear equations, which significantly
enhances the training efficiency compared to the time-consuming gradient
descent method in PINN. Finally, the superior performance of TS-PIELM is
demonstrated by solving three typical Terzaghi consolidation problems. Compared
to PINN, results show that the computational efficiency and accuracy of the
novel TS-PIELM framework are improved by more than 1000 times and 100 times for
one-dimensional cases, respectively. This paper provides compelling evidence
that PIML can be a powerful tool for computational geotechnics.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [289] [midr: Learning from Black-Box Models by Maximum Interpretation Decomposition](https://arxiv.org/abs/2506.08338)
*Ryoichi Asashiba,Reiji Kozuma,Hirokazu Iwasawa*

Main category: stat.ME

TL;DR: 介绍R包midr用于解释黑盒模型，实现最大解释分解（MID），阐述其使用和特点。


<details>
  <summary>Details</summary>
Motivation: 在需要模型和预测可解释性的领域采用黑盒预测模型，需合适的可解释机器学习和可解释人工智能方法。

Method: 引入R包midr实现MID，通过最小化模型预测函数与加性表示之间的平方误差，从黑盒模型构建具有高级分析能力的全局替代模型。

Result: 展示了midr包的使用并讨论其关键特征。

Conclusion: midr作为解释黑盒模型的新工具，有一定应用价值。

Abstract: The use of appropriate methods of Interpretable Machine Learning (IML) and
eXplainable Artificial Intelligence (XAI) is essential for adopting black-box
predictive models in fields where model and prediction explainability is
required. As a novel tool for interpreting black-box models, we introduce the R
package midr, which implements Maximum Interpretation Decomposition (MID). MID
is a functional decomposition approach that derives a low-order additive
representation of a black-box model by minimizing the squared error between the
model's prediction function and this additive representation. midr enables
learning from black-box models by constructing a global surrogate model with
advanced analytical capabilities. After reviewing related work and the
theoretical foundation of MID, we demonstrate the package's usage and discuss
some of its key features.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [290] [Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain](https://arxiv.org/abs/2506.08277)
*Subba Reddy Oota,Khushbu Pahwa,Prachi Jindal,Satya Sai Srinath Namburi,Maneesh Singh,Tanmoy Chakraborty,Bapi S. Raju,Manish Gupta*

Main category: q-bio.NC

TL;DR: 研究用指令微调的多模态大语言模型（MLLMs）对自然主义电影的脑对齐情况，发现其优于未微调模型，各层与大脑分层对齐，证明任务特定指令可改善对齐。


<details>
  <summary>Details</summary>
Motivation: 以往评估MLLMs脑对齐的工作主要聚焦单模态设置或使用非指令微调的多模态模型，本文旨在填补这一空白。

Method: 利用六个视频和两个音频指令微调的MLLMs的特定指令嵌入，进行13个视频任务特定指令的实验。

Result: 指令微调的视频MLLMs显著优于非指令微调的多模态（高15%）和单模态模型（高20%）；MLLM层与大脑分层对齐。

Conclusion: 任务特定指令有助于改善大脑活动与MLLMs的对齐，为映射两个系统的联合信息处理开辟新途径。

Abstract: Recent voxel-wise multimodal brain encoding studies have shown that
multimodal large language models (MLLMs) exhibit a higher degree of brain
alignment compared to unimodal models in both unimodal and multimodal stimulus
settings. More recently, instruction-tuned multimodal models have shown to
generate task-specific representations that align strongly with brain activity.
However, prior work evaluating the brain alignment of MLLMs has primarily
focused on unimodal settings or relied on non-instruction-tuned multimodal
models for multimodal stimuli. To address this gap, we investigated brain
alignment, that is, measuring the degree of predictivity of neural activity
recorded while participants were watching naturalistic movies (video along with
audio) with representations derived from MLLMs. We utilized
instruction-specific embeddings from six video and two audio instruction-tuned
MLLMs. Experiments with 13 video task-specific instructions show that
instruction-tuned video MLLMs significantly outperform non-instruction-tuned
multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for
both video and audio tasks using language-guided instructions shows clear
disentanglement in task-specific representations from MLLMs, leading to precise
differentiation of multimodal functional processing in the brain. We also find
that MLLM layers align hierarchically with the brain, with early sensory areas
showing strong alignment with early layers, while higher-level visual and
language regions align more with middle to late layers. These findings provide
clear evidence for the role of task-specific instructions in improving the
alignment between brain activity and MLLMs, and open new avenues for mapping
joint information processing in both the systems. We make the code publicly
available [https://github.com/subbareddy248/mllm_videos].

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [291] [Generalizing while preserving monotonicity in comparison-based preference learning models](https://arxiv.org/abs/2506.08616)
*Julien Fageot,Peva Blanchard,Gilles Bareilles,Lê-Nguyên Hoang*

Main category: math.ST

TL;DR: 本文探讨比较型偏好学习模型单调性问题，提出带扩散先验的线性广义Bradley - Terry模型，实验表明新模型提升了准确性。


<details>
  <summary>Details</summary>
Motivation: 许多广泛使用的比较型偏好学习模型无法保证单调性，现有证明单调的广义Bradley - Terry模型不能泛化到未比较数据。

Method: 提出带扩散先验的线性广义Bradley - Terry模型，并确定保证单调性的替代嵌入的充分条件。

Result: 单调性并非普遍保证，新的泛化模型提高了准确性，在数据集有限时效果更明显。

Conclusion: 新的线性广义Bradley - Terry模型在保证单调性的同时能提升模型准确性。

Abstract: If you tell a learning model that you prefer an alternative $a$ over another
alternative $b$, then you probably expect the model to be monotone, that is,
the valuation of $a$ increases, and that of $b$ decreases. Yet, perhaps
surprisingly, many widely deployed comparison-based preference learning models,
including large language models, fail to have this guarantee. Until now, the
only comparison-based preference learning algorithms that were proved to be
monotone are the Generalized Bradley-Terry models. Yet, these models are unable
to generalize to uncompared data. In this paper, we advance the understanding
of the set of models with generalization ability that are monotone. Namely, we
propose a new class of Linear Generalized Bradley-Terry models with Diffusion
Priors, and identify sufficient conditions on alternatives' embeddings that
guarantee monotonicity. Our experiments show that this monotonicity is far from
being a general guarantee, and that our new class of generalizing models
improves accuracy, especially when the dataset is limited.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [292] [MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning](https://arxiv.org/abs/2506.08507)
*Kuo Yang,Xingjie Yang,Linhui Yu,Qing Xu,Yan Fang,Xu Wang,Zhengyang Zhou,Yang Wang*

Main category: cs.MA

TL;DR: 提出基于强化学习的MasHost框架用于自主和查询自适应多智能体系统设计，实验表明其性能优于多数基线。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统构建方法依赖手工交互机制或启发式规则，存在人为偏差且限制自主能力，多为半自主模式。

Method: 将多智能体系统构建表述为图搜索问题，通过统一概率采样机制联合采样智能体角色和交互；引入组件合理性作为新设计原则；提出分层相对策略优化（HRPO）的强化学习策略。

Result: 在六个基准测试中，MasHost始终优于大多数竞争基线。

Conclusion: MasHost是首个基于强化学习的自主多智能体系统图构建框架，有效、高效且结构合理。

Abstract: Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently
emerged as a powerful paradigm for tackling complex real-world tasks. However,
existing Mas construction methods typically rely on manually crafted
interaction mechanisms or heuristic rules, introducing human biases and
constraining the autonomous ability. Even with recent advances in adaptive Mas
construction, existing systems largely remain within the paradigm of
semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement
Learning (RL)-based framework for autonomous and query-adaptive Mas design. By
formulating Mas construction as a graph search problem, our proposed MasHost
jointly samples agent roles and their interactions through a unified
probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives
pursued in prior works, we introduce component rationality as an additional and
novel design principle in Mas. To achieve this multi-objective optimization, we
propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy
that collaboratively integrates group-relative advantages and action-wise
rewards. To our knowledge, our proposed MasHost is the first RL-driven
framework for autonomous Mas graph construction. Extensive experiments on six
benchmarks demonstrate that MasHost consistently outperforms most competitive
baselines, validating its effectiveness, efficiency, and structure rationality.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [293] [Flow-Lenia: Emergent evolutionary dynamics in mass conservative continuous cellular automata](https://arxiv.org/abs/2506.08569)
*Erwan Plantec,Gautier Hamon,Mayalen Etcheverry,Bert Wang-Chak Chan,Pierre-Yves Oudeyer,Clément Moulin-Frier*

Main category: nlin.CG

TL;DR: 提出Flow - Lenia，它是Lenia的质量守恒扩展，实验证明其能生成复杂行为模式，可优化参数，还能进行多物种模拟并研究进化动力学。


<details>
  <summary>Details</summary>
Motivation: 人工生命致力于创造具有生命世界特性的人工系统，细胞自动机在该领域很重要，Lenia能产生类生命模式，本文旨在提出其扩展模型Flow - Lenia。

Method: 提出Flow - Lenia模型，进行实验，优化更新规则参数，使用进化活动框架和其他指标。

Result: Flow - Lenia能有效生成具有复杂行为的空间局部模式，可优化参数生成感兴趣行为的复杂生物，能进行多物种模拟。

Conclusion: Flow - Lenia是有潜力的模型，可用于研究新兴模式的属性和进化动力学。

Abstract: Central to the artificial life endeavour is the creation of artificial
systems spontaneously generating properties found in the living world such as
autopoiesis, self-replication, evolution and open-endedness. While numerous
models and paradigms have been proposed, cellular automata (CA) have taken a
very important place in the field notably as they enable the study of
phenomenons like self-reproduction and autopoiesis. Continuous CA like Lenia
have been showed to produce life-like patterns reminiscent, on an aesthetic and
ontological point of view, of biological organisms we call creatures. We
propose in this paper Flow-Lenia, a mass conservative extension of Lenia. We
present experiments demonstrating its effectiveness in generating
spatially-localized patters (SLPs) with complex behaviors and show that the
update rule parameters can be optimized to generate complex creatures showing
behaviors of interest. Furthermore, we show that Flow-Lenia allows us to embed
the parameters of the model, defining the properties of the emerging patterns,
within its own dynamics thus allowing for multispecies simulations. By using
the evolutionary activity framework as well as other metrics, we shed light on
the emergent evolutionary dynamics taking place in this system.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [294] [POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration](https://arxiv.org/abs/2506.08785)
*Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: 本文提出PARV - CE多精度MAC引擎，结合软硬件设计优化性能与能耗，结果显示有显著提升。


<details>
  <summary>Details</summary>
Motivation: AI模型复杂度增加，需要灵活硬件支持多样精度格式，尤其是能源受限的边缘平台。

Method: 提出PARV - CE引擎，采用统一数据路径执行多精度乘积累加运算，结合层自适应精度策略和量化感知执行，通过软硬件协同设计。

Result: 相比SoTA设计，PDP提升2倍，资源使用减少3倍，精度保持在FP32基线的1.8%以内，支持多种工作负载的设备端训练和推理。

Conclusion: PARV - CE结合POLARON是边缘端精度自适应AI加速的可扩展且节能的解决方案。

Abstract: The increasing complexity of AI models requires flexible hardware capable of
supporting diverse precision formats, particularly for energy-constrained edge
platforms. This work presents PARV-CE, a SIMD-enabled, multi-precision MAC
engine that performs efficient multiply-accumulate operations using a unified
data-path for 4/8/16-bit fixed-point, floating point, and posit formats. The
architecture incorporates a layer adaptive precision strategy to align
computational accuracy with workload sensitivity, optimizing both performance
and energy usage. PARV-CE integrates quantization-aware execution with a
reconfigurable SIMD pipeline, enabling high-throughput processing with minimal
overhead through hardware-software co-design. The results demonstrate up to 2x
improvement in PDP and 3x reduction in resource usage compared to SoTA designs,
while retaining accuracy within 1.8% FP32 baseline. The architecture supports
both on-device training and inference across a range of workloads, including
DNNs, RNNs, RL, and Transformer models. The empirical analysis establish PARVCE
incorporated POLARON as a scalable and energy-efficient solution for
precision-adaptive AI acceleration at edge.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [295] [ZTaint-Havoc: From Havoc Mode to Zero-Execution Fuzzing-Driven Taint Inference](https://arxiv.org/abs/2506.08838)
*Yuchong Xie,Wenhui Zhang,Dongdong She*

Main category: cs.CR

TL;DR: 提出ZTaint - Havoc进行轻量级模糊驱动污点推理，开销小且提升模糊测试覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统污点分析有可扩展性问题，现有模糊驱动污点推理运行开销大，需轻量级方法。

Method: 对模糊测试中常用的havoc突变方案进行调整，构建计算模型，设计利用热点字节的突变算法。

Result: ZTaint - Havoc开销小，在UniBench为3.84%、FuzzBench为12.58%；提升了覆盖率，FuzzBench最高33.71%、UniBench最高51.12%。

Conclusion: ZTaint - Havoc有效且高效，能提升模糊测试性能。

Abstract: Fuzzing is a widely used technique for discovering software vulnerabilities,
but identifying hot bytes that influence program behavior remains challenging.
Traditional taint analysis can track such bytes white-box, but suffers from
scalability issue. Fuzzing-Driven Taint Inference (FTI) offers a black-box
alternative, yet typically incurs significant runtime overhead due to extra
program executions. We observe that the commonly used havoc mutation scheme in
fuzzing can be adapted for lightweight FTI with zero extra executions. We
present a computational model of havoc mode, demonstrating that it can perform
FTI while generating new test cases. Building on this, we propose ZTaint-Havoc,
a novel, efficient FTI with minimal overhead (3.84% on UniBench, 12.58% on
FuzzBench). We further design an effective mutation algorithm utilizing the
identified hot bytes. Our comprehensive evaluation shows that ZTaint-Havoc,
implemented in AFL++, improves edge coverage by up to 33.71% on FuzzBench and
51.12% on UniBench over vanilla AFL++, with average gains of 2.97% and 6.12% in
24-hour fuzzing campaigns.

</details>


### [296] [How Good LLM-Generated Password Policies Are?](https://arxiv.org/abs/2506.08320)
*Vivek Vaidya,Aditya Patwardhan,Ashish Kundu*

Main category: cs.CR

TL;DR: 研究大语言模型（LLMs）在网络安全访问控制系统中生成密码策略的一致性和准确性，采用两种方法实验，发现当前LLMs存在挑战。


<details>
  <summary>Details</summary>
Motivation: LLMs输出的不一致和不可预测性在安全关键领域（如访问控制）带来挑战，需研究其在访问控制系统中生成响应的一致性。

Method: 采用两种方法：一是用预训练LLMs从自然语言提示直接生成配置文件；二是为模型提供官方文档作为参考，然后评估AI生成配置的合理性、准确性和一致性。

Result: 发现当前LLMs在生成密码策略方面存在显著挑战。

Conclusion: 研究结果为改进LLMs在访问控制系统中的部署提供了有价值的见解。

Abstract: Generative AI technologies, particularly Large Language Models (LLMs), are
rapidly being adopted across industry, academia, and government sectors, owing
to their remarkable capabilities in natural language processing. However,
despite their strengths, the inconsistency and unpredictability of LLM outputs
present substantial challenges, especially in security-critical domains such as
access control. One critical issue that emerges prominently is the consistency
of LLM-generated responses, which is paramount for ensuring secure and reliable
operations.
  In this paper, we study the application of LLMs within the context of
Cybersecurity Access Control Systems. Specifically, we investigate the
consistency and accuracy of LLM-generated password policies, translating
natural language prompts into executable pwquality.conf configuration files.
Our experimental methodology adopts two distinct approaches: firstly, we
utilize pre-trained LLMs to generate configuration files purely from natural
language prompts without additional guidance. Secondly, we provide these models
with official pwquality.conf documentation to serve as an informative baseline.
We systematically assess the soundness, accuracy, and consistency of these
AI-generated configurations. Our findings underscore significant challenges in
the current generation of LLMs and contribute valuable insights into refining
the deployment of LLMs in Access Control Systems.

</details>


### [297] [Your Agent Can Defend Itself against Backdoor Attacks](https://arxiv.org/abs/2506.08336)
*Li Changjiang,Liang Jiacheng,Cao Bochuan,Chen Jinghui,Wang Ting*

Main category: cs.CR

TL;DR: 介绍针对大语言模型驱动的智能体后门攻击的防御方法ReAgent，评估显示其效果良好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的智能体在训练和微调中面临后门攻击的安全风险，需解决该问题。

Method: ReAgent采用两级方法检测潜在后门，执行层面验证智能体思想和行动的一致性，规划层面检查重构指令和用户指令的一致性。

Result: ReAgent在各类任务中对多种后门攻击有效，如在数据库操作任务中使攻击成功率降低达90%，远超现有防御方法。

Conclusion: 利用受影响的智能体自身来减轻后门风险是可行的。

Abstract: Despite their growing adoption across domains, large language model
(LLM)-powered agents face significant security risks from backdoor attacks
during training and fine-tuning. These compromised agents can subsequently be
manipulated to execute malicious operations when presented with specific
triggers in their inputs or environments. To address this pressing risk, we
present ReAgent, a novel defense against a range of backdoor attacks on
LLM-based agents. Intuitively, backdoor attacks often result in inconsistencies
among the user's instruction, the agent's planning, and its execution. Drawing
on this insight, ReAgent employs a two-level approach to detect potential
backdoors. At the execution level, ReAgent verifies consistency between the
agent's thoughts and actions; at the planning level, ReAgent leverages the
agent's capability to reconstruct the instruction based on its thought
trajectory, checking for consistency between the reconstructed instruction and
the user's instruction. Extensive evaluation demonstrates ReAgent's
effectiveness against various backdoor attacks across tasks. For instance,
ReAgent reduces the attack success rate by up to 90\% in database operation
tasks, outperforming existing defenses by large margins. This work reveals the
potential of utilizing compromised agents themselves to mitigate backdoor
risks.

</details>


### [298] [WGLE:Backdoor-free and Multi-bit Black-box Watermarking for Graph Neural Networks](https://arxiv.org/abs/2506.08602)
*Tingzhi Li,Xuefeng Liu*

Main category: cs.CR

TL;DR: 论文提出针对GNN的无后门黑盒水印范式WGLE，经评估有高验证准确率、低性能下降等优点。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络所有权验证的指纹和黑盒水印方法存在计算昂贵、易受攻击、信息单一等问题，需新方法解决。

Method: 提出基于LDDE的WGLE，通过预定义选定边的LDDE值嵌入水印，不引入影响主要任务的错误映射。

Result: 在六个公共数据集和六种主流GNN架构上评估，WGLE实现100%所有权验证准确率，平均保真度下降0.85%，有可比的抗攻击鲁棒性和低嵌入开销。

Conclusion: WGLE是一种有效且实用的图神经网络所有权验证方法。

Abstract: Graph Neural Networks (GNNs) are increasingly deployed in graph-related
applications, making ownership verification critical to protect their
intellectual property against model theft. Fingerprinting and black-box
watermarking are two main methods. However, the former relies on determining
model similarity, which is computationally expensive and prone to ownership
collisions after model post-processing such as model pruning or fine-tuning.
The latter embeds backdoors, exposing watermarked models to the risk of
backdoor attacks. Moreover, both methods enable ownership verification but do
not convey additional information. As a result, each distributed model requires
a unique trigger graph, and all trigger graphs must be used to query the
suspect model during verification. Multiple queries increase the financial cost
and the risk of detection.
  To address these challenges, this paper proposes WGLE, a novel black-box
watermarking paradigm for GNNs that enables embedding the multi-bit string as
the ownership information without using backdoors. WGLE builds on a key insight
we term Layer-wise Distance Difference on an Edge (LDDE), which quantifies the
difference between the feature distance and the prediction distance of two
connected nodes. By predefining positive or negative LDDE values for multiple
selected edges, WGLE embeds the watermark encoding the intended information
without introducing incorrect mappings that compromise the primary task. WGLE
is evaluated on six public datasets and six mainstream GNN architectures along
with state-of-the-art methods. The results show that WGLE achieves 100%
ownership verification accuracy, an average fidelity degradation of 0.85%,
comparable robustness against potential attacks, and low embedding overhead.
The code is available in the repository.

</details>


### [299] [Interpreting Agent Behaviors in Reinforcement-Learning-Based Cyber-Battle Simulation Platforms](https://arxiv.org/abs/2506.08192)
*Jared Claypoole,Steven Cheung,Ashish Gehani,Vinod Yegneswaran,Ahmad Ridley*

Main category: cs.CR

TL;DR: 分析CAGE Challenge 2网络防御挑战中两个开源深度强化学习代理，简化状态和动作空间以理解其行为，分析事件模式、动作有效性、诱饵服务影响并讨论挑战的现实性。


<details>
  <summary>Details</summary>
Motivation: 理解CAGE Challenge 2中网络防御和攻击代理的细粒度行为，分析其成功和失败原因。

Method: 简化复杂状态和动作空间、跟踪重要事件、分析评估回合内重要事件、检查环境状态转换。

Result: 发现防御者能在一两个时间步内清除渗透；某些重要动作40% - 99%无效；诱饵服务可阻止高达94%的直接提权攻击。

Conclusion: 对CAGE Challenge 2进行了全面分析，指出问题并提及CAGE Challenge 4的改进。

Abstract: We analyze two open source deep reinforcement learning agents submitted to
the CAGE Challenge 2 cyber defense challenge, where each competitor submitted
an agent to defend a simulated network against each of several provided
rules-based attack agents. We demonstrate that one can gain interpretability of
agent successes and failures by simplifying the complex state and action spaces
and by tracking important events, shedding light on the fine-grained behavior
of both the defense and attack agents in each experimental scenario. By
analyzing important events within an evaluation episode, we identify patterns
in infiltration and clearing events that tell us how well the attacker and
defender played their respective roles; for example, defenders were generally
able to clear infiltrations within one or two timesteps of a host being
exploited. By examining transitions in the environment's state caused by the
various possible actions, we determine which actions tended to be effective and
which did not, showing that certain important actions are between 40% and 99%
ineffective. We examine how decoy services affect exploit success, concluding
for instance that decoys block up to 94% of exploits that would directly grant
privileged access to a host. Finally, we discuss the realism of the challenge
and ways that the CAGE Challenge 4 has addressed some of our concerns.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [300] [A Privacy-Preserving Federated Learning Framework for Generalizable CBCT to Synthetic CT Translation in Head and Neck](https://arxiv.org/abs/2506.08654)
*Ciro Benito Raggio,Paolo Zaffino,Maria Francesca Spadea*

Main category: physics.med-ph

TL;DR: 提出跨机构水平联邦学习方法进行头颈部CBCT到sCT合成，模型泛化效果好，证明了其技术可行性与数据隐私保护优势。


<details>
  <summary>Details</summary>
Motivation: 现有CBCT转sCT方法受机构异质性、扫描仪差异和数据隐私法规限制，无法进行多中心数据共享。

Method: 提出跨机构水平联邦学习方法，在公开SynthRAD2025挑战数据集上对条件生成对抗网络进行协作训练。

Result: 联邦模型在各中心间有效泛化，在外部验证数据集上无需额外训练也有可比表现。

Conclusion: 联邦学习用于CBCT到sCT合成技术可行，能保护数据隐私，无需集中数据共享或特定站点微调就能开发通用模型。

Abstract: Shortened Abstract
  Cone-beam computed tomography (CBCT) has become a widely adopted modality for
image-guided radiotherapy (IGRT). However, CBCT suffers from increased noise,
limited soft-tissue contrast, and artifacts, resulting in unreliable Hounsfield
unit values and hindering direct dose calculation. Synthetic CT (sCT)
generation from CBCT addresses these issues, especially using deep learning
(DL) methods. Existing approaches are limited by institutional heterogeneity,
scanner-dependent variations, and data privacy regulations that prevent
multi-center data sharing.
  To overcome these challenges, we propose a cross-silo horizontal federated
learning (FL) approach for CBCT-to-sCT synthesis in the head and neck region,
extending our FedSynthCT framework. A conditional generative adversarial
network was collaboratively trained on data from three European medical centers
in the public SynthRAD2025 challenge dataset.
  The federated model demonstrated effective generalization across centers,
with mean absolute error (MAE) ranging from $64.38\pm13.63$ to $85.90\pm7.10$
HU, structural similarity index (SSIM) from $0.882\pm0.022$ to $0.922\pm0.039$,
and peak signal-to-noise ratio (PSNR) from $32.86\pm0.94$ to $34.91\pm1.04$ dB.
Notably, on an external validation dataset of 60 patients, comparable
performance was achieved (MAE: $75.22\pm11.81$ HU, SSIM: $0.904\pm0.034$, PSNR:
$33.52\pm2.06$ dB) without additional training, confirming robust
generalization despite protocol, scanner differences and registration errors.
  These findings demonstrate the technical feasibility of FL for CBCT-to-sCT
synthesis while preserving data privacy and offer a collaborative solution for
developing generalizable models across institutions without centralized data
sharing or site-specific fine-tuning.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [301] [Enabling stratified sampling in high dimensions via nonlinear dimensionality reduction](https://arxiv.org/abs/2506.08921)
*Gianluca Geraci,Daniele E. Schiavazzi,Andrea Zanoni*

Main category: math.NA

TL;DR: 本文提出通过非线性降维实现高维分层抽样，以解决高维随机输入模型的不确定性传播问题，并降低多保真度蒙特卡罗估计器的方差。


<details>
  <summary>Details</summary>
Motivation: 现有分层抽样策略在高维输入模型应用中因难以创建均匀分区而受限，需解决高维随机输入模型的不确定性传播问题。

Method: 对单位区间上的均匀分布进行分层，再利用非线性降维在原始空间导出相应分层。

Result: 所提方法在高维情况下有效。

Conclusion: 该方法可用于进一步降低多保真度蒙特卡罗估计器的方差。

Abstract: We consider the problem of propagating the uncertainty from a possibly large
number of random inputs through a computationally expensive model. Stratified
sampling is a well-known variance reduction strategy, but its application, thus
far, has focused on models with a limited number of inputs due to the
challenges of creating uniform partitions in high dimensions. To overcome these
challenges, we perform stratification with respect to the uniform distribution
defined over the unit interval, and then derive the corresponding strata in the
original space using nonlinear dimensionality reduction. We show that our
approach is effective in high dimensions and can be used to further reduce the
variance of multifidelity Monte Carlo estimators.

</details>


### [302] [Structured Variational $D$-Decomposition for Accurate and Stable Low-Rank Approximation](https://arxiv.org/abs/2506.08535)
*Ronald Katende*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce the $D$-decomposition, a non-orthogonal matrix factorization of
the form $A \approx P D Q$, where $P \in \mathbb{R}^{n \times k}$, $D \in
\mathbb{R}^{k \times k}$, and $Q \in \mathbb{R}^{k \times n}$. The
decomposition is defined variationally by minimizing a regularized Frobenius
loss, allowing control over rank, sparsity, and conditioning. Unlike algebraic
factorizations such as LU or SVD, it is computed by alternating minimization.
We establish existence and perturbation stability of the solution and show that
each update has complexity $\mathcal{O}(n^2k)$. Benchmarks against truncated
SVD, CUR, and nonnegative matrix factorization show improved reconstruction
accuracy on MovieLens, MNIST, Olivetti Faces, and gene expression matrices,
particularly under sparsity and noise.

</details>


### [303] [sparseGeoHOPCA: A Geometric Solution to Sparse Higher-Order PCA Without Covariance Estimation](https://arxiv.org/abs/2506.08670)
*Renjie Xu,Chong Wu,Maolin Che,Zhuoheng Ran,Yimin Wei,Hong Yan*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose sparseGeoHOPCA, a novel framework for sparse higher-order
principal component analysis (SHOPCA) that introduces a geometric perspective
to high-dimensional tensor decomposition. By unfolding the input tensor along
each mode and reformulating the resulting subproblems as structured binary
linear optimization problems, our method transforms the original nonconvex
sparse objective into a tractable geometric form. This eliminates the need for
explicit covariance estimation and iterative deflation, enabling significant
gains in both computational efficiency and interpretability, particularly in
high-dimensional and unbalanced data scenarios. We theoretically establish the
equivalence between the geometric subproblems and the original SHOPCA
formulation, and derive worst-case approximation error bounds based on
classical PCA residuals, providing data-dependent performance guarantees. The
proposed algorithm achieves a total computational complexity of
$O\left(\sum_{n=1}^{N} (k_n^3 + J_n k_n^2)\right)$, which scales linearly with
tensor size. Extensive experiments demonstrate that sparseGeoHOPCA accurately
recovers sparse supports in synthetic settings, preserves classification
performance under 10$\times$ compression, and achieves high-quality image
reconstruction on ImageNet, highlighting its robustness and versatility.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [304] [Dynamic Diffusion Schrödinger Bridge in Astrophysical Observational Inversions](https://arxiv.org/abs/2506.08065)
*Ye Zhu,Duo Xu,Zhiwei Deng,Jonathon C. Tan,Olga Russakovsky*

Main category: astro-ph.IM

TL;DR: 研究Diffusion Schrödinger Bridge (DSB)模型在天体物理系统的应用，提出Astro - DSB模型，在模拟和真实数据中测试，显示出多方面优势，拓展扩散模型研究。


<details>
  <summary>Details</summary>
Motivation: 在动态天体物理系统中，解决巨分子云（GMCs）恒星形成的观测逆预测任务。

Method: 引入适用于天体物理动力学的Astro - DSB模型，在物理模拟数据和真实观测数据中研究其学习过程和预测性能。

Result: 从天体物理角度，配对DSB方法在可解释性、学习效率和预测性能上优于传统方法；从生成建模角度，概率生成建模在OOD测试中优于判别式像素到像素建模。

Conclusion: 研究拓展了扩散模型的应用，为未来物理感知生成模型发展提供方向。

Abstract: We study Diffusion Schr\"odinger Bridge (DSB) models in the context of
dynamical astrophysical systems, specifically tackling observational inverse
prediction tasks within Giant Molecular Clouds (GMCs) for star formation. We
introduce the Astro-DSB model, a variant of DSB with the pairwise domain
assumption tailored for astrophysical dynamics. By investigating its learning
process and prediction performance in both physically simulated data and in
real observations (the Taurus B213 data), we present two main takeaways. First,
from the astrophysical perspective, our proposed paired DSB method improves
interpretability, learning efficiency, and prediction performance over
conventional astrostatistical and other machine learning methods. Second, from
the generative modeling perspective, probabilistic generative modeling reveals
improvements over discriminative pixel-to-pixel modeling in Out-Of-Distribution
(OOD) testing cases of physical simulations with unseen initial conditions and
different dominant physical processes. Our study expands research into
diffusion models beyond the traditional visual synthesis application and
provides evidence of the models' learning abilities beyond pure data
statistics, paving a path for future physics-aware generative models which can
align dynamics between machine learning and real (astro)physical systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [305] [SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models](https://arxiv.org/abs/2506.08346)
*Wenhan Yao,Fen Xiao,Xiarun Chen,Jia Liu,YongQiang He,Weiping Wen*

Main category: cs.SD

TL;DR: 本文指出深度语音分类技术易受后门攻击，提出基于语音元素和SLLM的语音提示后门攻击（SPBA），并用MGDA缓解问题，实验显示SPBA效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有语音后门攻击因触发函数限制只能创建有限数量的后门，且增加触发器数量会带来成本上升等问题。

Method: 提出基于语音元素（如音色和情感），利用SLLM生成多样化触发器的SPBA攻击方法，并引入MGDA作为缓解策略。

Result: 在两个语音分类任务上进行攻击实验，SPBA显示出显著的触发效果，在攻击指标上表现出色。

Conclusion: 所提出的SPBA攻击方法在语音分类任务的后门攻击中具有有效性和优越性。

Abstract: Deep speech classification tasks, including keyword spotting and speaker
verification, are vital in speech-based human-computer interaction. Recently,
the security of these technologies has been revealed to be susceptible to
backdoor attacks. Specifically, attackers use noisy disruption triggers and
speech element triggers to produce poisoned speech samples that train models to
become vulnerable. However, these methods typically create only a limited
number of backdoors due to the inherent constraints of the trigger function. In
this paper, we propose that speech backdoor attacks can strategically focus on
speech elements such as timbre and emotion, leveraging the Speech Large
Language Model (SLLM) to generate diverse triggers. Increasing the number of
triggers may disproportionately elevate the poisoning rate, resulting in higher
attack costs and a lower success rate per trigger. We introduce the Multiple
Gradient Descent Algorithm (MGDA) as a mitigation strategy to address this
challenge. The proposed attack is called the Speech Prompt Backdoor Attack
(SPBA). Building on this foundation, we conducted attack experiments on two
speech classification tasks, demonstrating that SPBA shows significant trigger
effectiveness and achieves exceptional performance in attack metrics.

</details>


### [306] [MD-ViSCo: A Unified Model for Multi-Directional Vital Sign Waveform Conversion](https://arxiv.org/abs/2506.08357)
*Franck Meyer,Kyunghoon Hur,Edward Choi*

Main category: cs.SD

TL;DR: 提出多方向生命体征转换器MD - ViSCo统一框架，能以单一模型生成多种生命体征波形，效果超现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法多针对特定源 - 目标对设计模型，导致多个模型阻碍临床应用，需统一框架。

Method: 提出MD - ViSCo，采用浅1维U - Net集成Swin Transformer，并利用自适应实例归一化（AdaIN）捕捉不同波形风格。

Result: 在两个公开数据集上进行多方向波形生成，平均表现超现有基线，降低MAE 8.8%，提高PC 4.9%，生成的ABP波形满足相关标准。

Conclusion: 该工作提供统一框架，可在医疗监测中用单一模型处理任何生命体征波形。

Abstract: Despite the remarkable progress of deep-learning methods generating a target
vital sign waveform from a source vital sign waveform, most existing models are
designed exclusively for a specific source-to-target pair. This requires
distinct model architectures, optimization procedures, and pre-processing
pipelines, resulting in multiple models that hinder usability in clinical
settings. To address this limitation, we propose the Multi-Directional
Vital-Sign Converter (MD-ViSCo), a unified framework capable of generating any
target waveform such as electrocardiogram (ECG), photoplethysmogram (PPG), or
arterial blood pressure (ABP) from any single input waveform with a single
model. MD-ViSCo employs a shallow 1-Dimensional U-Net integrated with a Swin
Transformer that leverages Adaptive Instance Normalization (AdaIN) to capture
distinct waveform styles. To evaluate the efficacy of MD-ViSCo, we conduct
multi-directional waveform generation on two publicly available datasets. Our
framework surpasses state-of-the-art baselines (NabNet & PPG2ABP) on average
across all waveform types, lowering Mean absolute error (MAE) by 8.8% and
improving Pearson correlation (PC) by 4.9% over two datasets. In addition, the
generated ABP waveforms satisfy the Association for the Advancement of Medical
Instrumentation (AAMI) criterion and achieve Grade B on the British
Hypertension Society (BHS) standard, outperforming all baselines. By
eliminating the need for developing a distinct model for each task, we believe
that this work offers a unified framework that can deal with any kind of vital
sign waveforms with a single model in healthcare monitoring.

</details>


### [307] [Teaching Physical Awareness to LLMs through Sounds](https://arxiv.org/abs/2506.08524)
*Weiguo Wang,Andy Nie,Wenrui Zhou,Yi Kai,Chengchen Hu*

Main category: cs.SD

TL;DR: 提出ACORN框架让大语言模型通过声音获得物理感知，构建数据集并提出音频编码器，取得合理结果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型缺乏对现实世界物理现象的物理感知，需要赋予其物理意识。

Method: 引入基于物理的模拟器生成训练数据，构建AQA - PHY数据集，提出处理幅度和相位信息的音频编码器并连接到先进大语言模型。

Result: 在模拟和现实世界任务中取得合理结果，如视线检测、多普勒效应估计和到达方向估计。

Conclusion: 为大语言模型理解物理世界奠定了基础。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in text and
multimodal processing, yet they fundamentally lack physical
awareness--understanding of real-world physical phenomena. In this work, we
present ACORN, a framework that teaches LLMs physical awareness through sound,
focusing on fundamental physical phenomena like the Doppler effect, multipath
effect, and spatial relationships. To overcome data scarcity, ACORN introduce a
physics-based simulator combining real-world sound sources with controlled
physical channels to generate diverse training data. Using this simulator, we
build AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an
audio encoder that processes both magnitude and phase information. By
connecting our audio encoder to state-of-the-art LLMs, we demonstrate
reasonable results in both simulated and real-world tasks, such as
line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival
estimation, paving the way for enabling LLMs to understand physical world.

</details>


### [308] [Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation](https://arxiv.org/abs/2506.08570)
*Or Tal,Felix Kreuk,Yossi Adi*

Main category: cs.SD

TL;DR: 文章对文本到音乐生成的两种常见建模范式进行对比研究，以指导未来系统设计。


<details>
  <summary>Details</summary>
Motivation: 现有文本到音乐生成模型在多维度差异大，难以公平评估和确定影响性能的设计选择，本文聚焦建模范式展开研究。

Method: 对自回归解码和条件流匹配两种建模范式进行系统实证分析，使用相同数据集、训练配置和相似骨干架构从头训练模型，并从多方面评估性能。

Result: 揭示了两种范式各自的优缺点。

Conclusion: 研究为文本到音乐生成领域的架构和训练决策提供了可行见解。

Abstract: Recent progress in text-to-music generation has enabled models to synthesize
high-quality musical segments, full compositions, and even respond to
fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)
systems differ significantly across many dimensions, such as training datasets,
modeling paradigms, and architectural choices. This diversity complicates
efforts to evaluate models fairly and pinpoint which design choices most
influence performance. While factors like data and architecture are important,
in this study we focus exclusively on the modeling paradigm. We conduct a
systematic empirical analysis to isolate its effects, offering insights into
associated trade-offs and emergent behaviors that can guide future
text-to-music generation systems. Specifically, we compare the two arguably
most common modeling paradigms: Auto-Regressive decoding and Conditional
Flow-Matching. We conduct a controlled comparison by training all models from
scratch using identical datasets, training configurations, and similar backbone
architectures. Performance is evaluated across multiple axes, including
generation quality, robustness to inference configurations, scalability,
adherence to both textual and temporally aligned conditioning, and editing
capabilities in the form of audio inpainting. This comparative study sheds
light on distinct strengths and limitations of each paradigm, providing
actionable insights that can inform future architectural and training decisions
in the evolving landscape of text-to-music generation. Audio sampled examples
are available at: https://huggingface.co/spaces/ortal1602/ARvsFM

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [309] [Superposed Parameterised Quantum Circuits](https://arxiv.org/abs/2506.08749)
*Viktoria Patapovich,Mo Kordzanganeh,Alexey Melnikov*

Main category: quant-ph

TL;DR: 本文引入叠加参数化量子电路克服现有量子机器学习方法局限性，经实验验证其优势，是构建更高效复杂量子电路的途径。


<details>
  <summary>Details</summary>
Motivation: 现有量子机器学习方法依赖线性幺正操作和共享可训练参数，限制了表达能力和可扩展性，需新方法解决。

Method: 结合触发器量子随机存取存储器和重复直到成功协议，在单个电路中嵌入指数数量的参数化子模型，通过幅度变换和后选择诱导多项式激活函数。

Result: 在1D阶跃函数回归中，两量子比特叠加参数化量子电路使均方误差降低三个数量级；在2D星形分类任务中，引入二次激活将准确率提升至81.4%，并使运行间方差降低三倍。

Conclusion: 叠加参数化量子电路是实现更深、更多功能参数化量子电路的硬件高效途径，能学习复杂决策边界。

Abstract: Quantum machine learning has shown promise for high-dimensional data
analysis, yet many existing approaches rely on linear unitary operations and
shared trainable parameters across outputs. These constraints limit
expressivity and scalability relative to the multi-layered, non-linear
architectures of classical deep networks. We introduce superposed parameterised
quantum circuits to overcome these limitations. By combining flip-flop quantum
random-access memory with repeat-until-success protocols, a superposed
parameterised quantum circuit embeds an exponential number of parameterised
sub-models in a single circuit and induces polynomial activation functions
through amplitude transformations and post-selection. We provide an analytic
description of the architecture, showing how multiple parameter sets are
trained in parallel while non-linear amplitude transformations broaden
representational power beyond conventional quantum kernels. Numerical
experiments underscore these advantages: on a 1D step-function regression a
two-qubit superposed parameterised quantum circuit cuts the mean-squared error
by three orders of magnitude versus a parameter-matched variational baseline;
on a 2D star-shaped two-dimensional classification task, introducing a
quadratic activation lifts accuracy to 81.4% and reduces run-to-run variance
three-fold. These results position superposed parameterised quantum circuits as
a hardware-efficient route toward deeper, more versatile parameterised quantum
circuits capable of learning complex decision boundaries.

</details>


### [310] [Qymera: Simulating Quantum Circuits using RDBMS](https://arxiv.org/abs/2506.08759)
*Tim Littau,Rihan Hai*

Main category: quant-ph

TL;DR: 介绍Qymera系统，将量子电路转换为SQL查询以利用关系数据库管理系统进行模拟，支持多种电路，有基准测试框架，适用于量子计算开发等。


<details>
  <summary>Details</summary>
Motivation: 量子电路模拟对量子计算很重要，需要有效模拟方法。

Method: 将量子电路转换为SQL查询，利用关系数据库管理系统进行模拟，提供图形和代码接口，有基准测试框架。

Result: 展示了Qymera基于SQL的端到端执行、与经典工作流的无缝集成。

Conclusion: Qymera对量子计算和数据管理的开发、基准测试和教育有实用价值。

Abstract: Quantum circuit simulation is crucial for quantum computing such as
validating quantum algorithms. We present Qymera, a system that repurposes
relational database management systems (RDBMSs) for simulation by translating
circuits into SQL queries, allowing quantum operations to run natively within
an RDBMS. Qymera supports a wide range of quantum circuits, offering a
graphical circuit builder and code-based interfaces to input circuits. With a
benchmarking framework, Qymera facilitates comparison of RDBMS-based simulation
against state-of-the-art simulation methods. Our demonstration showcases
Qymera's end-to-end SQL-based execution, seamless integration with classical
workflows, and its utility for development, benchmarking, and education in
quantum computing and data management.

</details>


### [311] [Optimizing Sparse SYK](https://arxiv.org/abs/2506.09037)
*Matthew Ding,Robbie King,Bobak T. Kiani,Eric R. Anschuetz*

Main category: quant-ph

TL;DR: 研究稀疏SYK模型中量子和经典复杂度对稀疏化的鲁棒性，证明特定范围内量子算法有常数因子近似，高斯态近似有局限，拓展了前人结果。


<details>
  <summary>Details</summary>
Motivation: 探讨SYK模型量子和经典复杂度对稀疏化的鲁棒性，此前已知模型稀疏到一定程度量子 - 经典分离不持续。

Method: 对p在特定范围的稀疏SYK模型进行研究，证明Hastings - O'Donnell量子算法在p≥Ω(log n/n)时对基态能量有常数因子近似，通过一般经典电路复杂度下界证明高斯态近似局限。

Result: 证明Hastings - O'Donnell量子算法在p≥Ω(log n/n)时对基态能量有常数因子近似；高斯态对稀疏SYK基态能量近似不超过O(√(log n/pn))因子。

Conclusion: 当p≥Ω(log n/n)时，输出高斯态的经典算法和高效量子算法在寻找近似稀疏SYK基态上有可证明的分离，拓展了p = 1时的结果。

Abstract: Finding the ground state of strongly-interacting fermionic systems is often
the prerequisite for fully understanding both quantum chemistry and condensed
matter systems. The Sachdev--Ye--Kitaev (SYK) model is a representative example
of such a system; it is particularly interesting not only due to the existence
of efficient quantum algorithms preparing approximations to the ground state
such as Hastings--O'Donnell (STOC 2022), but also known no-go results for many
classical ansatzes in preparing low-energy states. However, this
quantum-classical separation is known to \emph{not} persist when the SYK model
is sufficiently sparsified, i.e., when terms in the model are discarded with
probability $1-p$, where $p=\Theta(1/n^3)$ and $n$ is the system size. This
raises the question of how robust the quantum and classical complexities of the
SYK model are to sparsification.
  In this work we initiate the study of the sparse SYK model where $p \in
[\Theta(1/n^3),1]$. We show there indeed exists a certain robustness of
sparsification. First, we prove that the quantum algorithm of
Hastings--O'Donnell for $p=1$ still achieves a constant-factor approximation to
the ground energy when $p\geq\Omega(\log n/n)$. Additionally, we prove that
with high probability, Gaussian states cannot achieve better than a
$O(\sqrt{\log n/pn})$-factor approximation to the true ground state energy of
sparse SYK. This is done through a general classical circuit complexity
lower-bound of $\Omega(pn^3)$ for any quantum state achieving a constant-factor
approximation. Combined, these show a provable separation between classical
algorithms outputting Gaussian states and efficient quantum algorithms for the
goal of finding approximate sparse SYK ground states when $p \geq \Omega(\log
n/n)$, extending the analogous $p=1$ result of Hastings--O'Donnell.

</details>


### [312] [Solving excited states for long-range interacting trapped ions with neural networks](https://arxiv.org/abs/2506.08594)
*Yixuan Ma,Chang Liu,Weikang Li,Shun-Yao Zhang,L. -M. Duan,Yukai Wu,Dong-Ling Deng*

Main category: quant-ph

TL;DR: 提出基于神经网络的NQES算法，能准确高效计算量子多体自旋系统多个低激发态，通过实例验证其有效性，有广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 强相互作用量子多体系统激发态计算重要但因希尔伯特空间维度指数增长而极具挑战。

Method: 引入神经网络算法NQES，无需态的显式正交化，适用于高维。

Result: 通过具体模型验证算法能高效计算多激发态及相关可观测量期望；应用于两类长程相互作用囚禁离子系统，得出相应结果。

Conclusion: 建立了可扩展且高效的算法，有从量子设备基准测试到光异构化等潜在应用。

Abstract: The computation of excited states in strongly interacting quantum many-body
systems is of fundamental importance. Yet, it is notoriously challenging due to
the exponential scaling of the Hilbert space dimension with the system size.
Here, we introduce a neural network-based algorithm that can simultaneously
output multiple low-lying excited states of a quantum many-body spin system in
an accurate and efficient fashion. This algorithm, dubbed the neural quantum
excited-state (NQES) algorithm, requires no explicit orthogonalization of the
states and is generally applicable to higher dimensions. We demonstrate,
through concrete examples including the Haldane-Shastry model with all-to-all
interactions, that the NQES algorithm is capable of efficiently computing
multiple excited states and their related observable expectations. In addition,
we apply the NQES algorithm to two classes of long-range interacting
trapped-ion systems in a two-dimensional Wigner crystal. For non-decaying
all-to-all interactions with alternating signs, our computed low-lying excited
states bear spatial correlation patterns similar to those of the ground states,
which closely match recent experimental observations that the
quasi-adiabatically prepared state accurately reproduces analytical
ground-state correlations. For a system of up to 300 ions with power-law
decaying antiferromagnetic interactions, we successfully uncover its gap
scaling and correlation features. Our results establish a scalable and
efficient algorithm for computing excited states of interacting quantum
many-body systems, which holds potential applications ranging from benchmarking
quantum devices to photoisomerization.

</details>


### [313] [Systematic and Efficient Construction of Quadratic Unconstrained Binary Optimization Forms for High-order and Dense Interactions](https://arxiv.org/abs/2506.08448)
*Hyakka Nakada,Shu Tanaka*

Main category: quant-ph

TL;DR: 研究量子退火（QA）解决机器学习高阶问题的二次化方法，验证概念并设计新优化方案。


<details>
  <summary>Details</summary>
Motivation: 现有量子退火二次化方法在解决涉及机器学习的复杂问题时存在不足，传统方法因强非线性和密集交互难以应用。

Method: 用修正线性单元基之和对目标函数建模，结合QA与提出的二次化方法设计新的黑箱优化方案。

Result: 概念验证在数值和分析上均得到验证。

Conclusion: 提出的方法可用于解决涉及机器学习的复杂高阶问题，结合QA设计的新优化方案有应用潜力。

Abstract: Quantum Annealing (QA) can efficiently solve combinatorial optimization
problems whose objective functions are represented by Quadratic Unconstrained
Binary Optimization (QUBO) formulations. For broader applicability of QA,
quadratization methods are used to transform higher-order problems into QUBOs.
However, quadratization methods for complex problems involving Machine Learning
(ML) remain largely unknown. In these problems, strong nonlinearity and dense
interactions prevent conventional methods from being applied. Therefore, we
model target functions by the sum of rectified linear unit bases, which not
only have the ability of universal approximation, but also have an equivalent
quadratic-polynomial representation. In this study, the proof of concept is
verified both numerically and analytically. In addition, by combining QA with
the proposed quadratization, we design a new black-box optimization scheme, in
which ML surrogate regressors are inputted to QA after the quadratization
process.

</details>


### [314] [The interplay of robustness and generalization in quantum machine learning](https://arxiv.org/abs/2506.08455)
*Julian Berberich,Tobias Fellner,Christian Holm*

Main category: quant-ph

TL;DR: 探讨变分量子模型中对抗鲁棒性与泛化能力的相互作用，提出基于正则化的训练方法并应用于时间序列分析。


<details>
  <summary>Details</summary>
Motivation: 现有量子机器学习文献中对抗鲁棒性与泛化能力的相互作用研究较少，本文针对变分量子模型开展该研究。

Method: 通过依赖模型参数的Lipschitz界量化鲁棒性和泛化能力，提出基于正则化的训练方法。

Result: 得到可用于训练鲁棒且具有泛化能力量子模型的方法，并通过时间序列分析展示理论结果的实际应用。

Conclusion: 强调了可训练数据编码策略的重要性，验证了理论结果在实际中的应用。

Abstract: While adversarial robustness and generalization have individually received
substantial attention in the recent literature on quantum machine learning,
their interplay is much less explored. In this chapter, we address this
interplay for variational quantum models, which were recently proposed as
function approximators in supervised learning. We discuss recent results
quantifying both robustness and generalization via Lipschitz bounds, which
explicitly depend on model parameters. Thus, they give rise to a
regularization-based training approach for robust and generalizable quantum
models, highlighting the importance of trainable data encoding strategies. The
practical implications of the theoretical results are demonstrated with an
application to time series analysis.

</details>


### [315] [Quantum Adiabatic Generation of Human-Like Passwords](https://arxiv.org/abs/2506.08917)
*Sascha Mücke,Raoul Heese,Thore Gerlach,David Biesner,Loong Kuan Lee,Nico Piatkowski*

Main category: quant-ph

TL;DR: 研究绝热量子计算机用于生成类似用户行为密码的效用，提出基于QUBO和UD - MIS问题的方法，在QuEra Aquila量子计算机上生成含类人密码的样本。


<details>
  <summary>Details</summary>
Motivation: 探讨量子计算能否降低生成式人工智能模型训练和运行的资源需求，利用量子计算机生成类似用户行为的密码用于测试认证系统。

Method: 研究不同的令牌字符串编码，提出基于二次无约束二进制优化（QUBO）和单位圆盘最大独立集（UD - MIS）问题的新方法，从数据估计令牌分布并绝热制备量子态，通过测量采样生成密码。

Result: 在QuEra Aquila 256 - 比特中性原子量子计算机上生成的128个密码的相对小样本中，包含如“Tunas200992”或“teedem28iglove”等类人密码。

Conclusion: 绝热量子计算机可用于生成类似用户行为的密码，在该任务中有一定效用。

Abstract: Generative Artificial Intelligence (GenAI) for Natural Language Processing
(NLP) is the predominant AI technology to date. An important perspective for
Quantum Computing (QC) is the question whether QC has the potential to reduce
the vast resource requirements for training and operating GenAI models. While
large-scale generative NLP tasks are currently out of reach for practical
quantum computers, the generation of short semantic structures such as
passwords is not. Generating passwords that mimic real user behavior has many
applications, for example to test an authentication system against realistic
threat models. Classical password generation via deep learning have recently
been investigated with significant progress in their ability to generate novel,
realistic password candidates. In the present work we investigate the utility
of adiabatic quantum computers for this task. More precisely, we study
different encodings of token strings and propose novel approaches based on the
Quadratic Unconstrained Binary Optimization (QUBO) and the Unit-Disk Maximum
Independent Set (UD-MIS) problems. Our approach allows us to estimate the token
distribution from data and adiabatically prepare a quantum state from which we
eventually sample the generated passwords via measurements. Our results show
that relatively small samples of 128 passwords, generated on the QuEra Aquila
256-qubit neutral atom quantum computer, contain human-like passwords such as
"Tunas200992" or "teedem28iglove".

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [316] [ChemGraph: An Agentic Framework for Computational Chemistry Workflows](https://arxiv.org/abs/2506.06363)
*Thang D. Pham,Aditya Tanikanti,Murat Keçeli*

Main category: physics.chem-ph

TL;DR: 提出AI驱动的ChemGraph框架简化计算化学和材料科学工作流，评估不同LLM在13个基准任务表现，发现小模型处理简单任务佳，复杂任务大模型更优，多智能体框架可提升小模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有原子模拟运行因计算方法多样、软件生态复杂、需专业知识和手动操作而具挑战，需简化自动化工作流。

Method: 构建ChemGraph框架，利用图神经网络基础模型计算、大语言模型实现自然语言理解等，提供交互界面，支持多种计算任务和方法。

Result: 评估显示小LLM在简单工作流表现好，复杂任务大模型更优，多智能体框架下小模型在特定场景可超越大模型。

Conclusion: ChemGraph框架有效，通过多智能体框架分解任务可提升小模型性能以处理复杂任务。

Abstract: Atomistic simulations are essential tools in chemistry and materials science,
accelerating the discovery of novel catalysts, energy storage materials, and
pharmaceuticals. However, running these simulations remains challenging due to
the wide range of computational methods, diverse software ecosystems, and the
need for expert knowledge and manual effort for the setup, execution, and
validation stages. In this work, we present ChemGraph, an agentic framework
powered by artificial intelligence and state-of-the-art simulation tools to
streamline and automate computational chemistry and materials science
workflows. ChemGraph leverages graph neural network-based foundation models for
accurate yet computationally efficient calculations and large language models
(LLMs) for natural language understanding, task planning, and scientific
reasoning to provide an intuitive and interactive interface. Users can perform
tasks such as molecular structure generation, single-point energy, geometry
optimization, vibrational analysis, and thermochemistry calculations with
methods ranging from tight-binding and machine learning interatomic potentials
to density functional theory or wave function theory-based methods. We evaluate
ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs
(GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows,
while more complex tasks benefit from using larger models like GPT-4o.
Importantly, we show that decomposing complex tasks into smaller subtasks
through a multi-agent framework enables smaller LLM models to match or exceed
GPT-4o's performance in specific scenarios.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [317] [Video-ColBERT: Contextualized Late Interaction for Text-to-Video Retrieval](https://arxiv.org/abs/2503.19009)
*Arun Reddy,Alexander Martin,Eugene Yang,Andrew Yates,Kate Sanders,Kenton Murray,Reno Kriz,Celso M. de Melo,Benjamin Van Durme,Rama Chellappa*

Main category: cs.CV

TL;DR: 提出Video - ColBERT方法解决文本到视频检索问题，性能优于其他双编码器方法。


<details>
  <summary>Details</summary>
Motivation: 受后期交互技术在文本 - 文档、文本 - 图像和文本 - 视频检索中成功的启发，解决文本到视频检索问题。

Method: Video - ColBERT由细粒度时空逐令牌交互、查询和视觉扩展以及训练时的双Sigmoid损失三个主要组件构成。

Result: 该方法在常见文本到视频检索基准上性能优于其他双编码器方法。

Conclusion: 该交互和训练范式能为视频内容编码提供强大且兼容的表示。

Abstract: In this work, we tackle the problem of text-to-video retrieval (T2VR).
Inspired by the success of late interaction techniques in text-document,
text-image, and text-video retrieval, our approach, Video-ColBERT, introduces a
simple and efficient mechanism for fine-grained similarity assessment between
queries and videos. Video-ColBERT is built upon 3 main components: a
fine-grained spatial and temporal token-wise interaction, query and visual
expansions, and a dual sigmoid loss during training. We find that this
interaction and training paradigm leads to strong individual, yet compatible,
representations for encoding video content. These representations lead to
increases in performance on common text-to-video retrieval benchmarks compared
to other bi-encoder methods.

</details>


### [318] [Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts](https://arxiv.org/abs/2506.08048)
*Zheng Han,Jun Zhou,Jialun Pei,Jing Qin,Yingfang Fan,Qi Dou*

Main category: cs.CV

TL;DR: 提出数据驱动生物力学算法及人在环机制用于AR手术导航，实验证明高效准确且提升协作。


<details>
  <summary>Details</summary>
Motivation: 现有AR手术导航中FEM计算成本高、算法难处理大解剖变化，影响AR引导可靠性。

Method: 提出数据驱动生物力学算法，引入人在环机制让外科医生交互校正解剖错位。

Result: 算法平均目标配准误差3.42mm，加入医生提示后降至2.78mm，超越现有方法。

Conclusion: 框架能实现高效准确变形建模，提升医生与算法协作，推动更安全可靠的计算机辅助手术。

Abstract: In augmented reality (AR)-guided surgical navigation, preoperative organ
models are superimposed onto the patient's intraoperative anatomy to visualize
critical structures such as vessels and tumors. Accurate deformation modeling
is essential to maintain the reliability of AR overlays by ensuring alignment
between preoperative models and the dynamically changing anatomy. Although the
finite element method (FEM) offers physically plausible modeling, its high
computational cost limits intraoperative applicability. Moreover, existing
algorithms often fail to handle large anatomical changes, such as those induced
by pneumoperitoneum or ligament dissection, leading to inaccurate anatomical
correspondences and compromised AR guidance. To address these challenges, we
propose a data-driven biomechanics algorithm that preserves FEM-level accuracy
while improving computational efficiency. In addition, we introduce a novel
human-in-the-loop mechanism into the deformation modeling process. This enables
surgeons to interactively provide prompts to correct anatomical misalignments,
thereby incorporating clinical expertise and allowing the model to adapt
dynamically to complex surgical scenarios. Experiments on a publicly available
dataset demonstrate that our algorithm achieves a mean target registration
error of 3.42 mm. Incorporating surgeon prompts through the interactive
framework further reduces the error to 2.78 mm, surpassing state-of-the-art
methods in volumetric accuracy. These results highlight the ability of our
framework to deliver efficient and accurate deformation modeling while
enhancing surgeon-algorithm collaboration, paving the way for safer and more
reliable computer-assisted surgeries.

</details>


### [319] [IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation](https://arxiv.org/abs/2506.08137)
*Oishee Bintey Hoque,Abhijin Adiga,Aniruddha Adiga,Siddharth Chaudhary,Madhav V. Marathe,S. S. Ravi,Kirti Rajagopalan,Amanda Wilson,Samarth Swarup*

Main category: cs.CV

TL;DR: 本文提出IGraSS框架结合语义分割与图基真值细化模块，可优化真值并映射运河网络，实验表明效果良好，且在道路网络上也有通用性。


<details>
  <summary>Details</summary>
Motivation: 现有基础设施映射的语义分割模型依赖大量标注数据，不完整或不充分的真值会阻碍学习，而基础设施网络的图级属性可用于改进现有真值。

Method: 开发IGraSS迭代框架，结合语义分割模块（融合RGB及其他模态）和图基真值细化模块，分别处理卫星图像块和将基础设施网络视为图处理全量数据。

Result: IGraSS将不可达运河段从约18%降至3%，使用细化真值训练显著提高运河识别效果，在道路网络应用也证明其有效性和通用性。

Conclusion: IGraSS是一个强大的框架，可用于细化有噪声的真值和从遥感图像中映射运河网络。

Abstract: Accurate canal network mapping is essential for water management, including
irrigation planning and infrastructure maintenance. State-of-the-art semantic
segmentation models for infrastructure mapping, such as roads, rely on large,
well-annotated remote sensing datasets. However, incomplete or inadequate
ground truth can hinder these learning approaches. Many infrastructure networks
have graph-level properties such as reachability to a source (like canals) or
connectivity (roads) that can be leveraged to improve these existing ground
truth. This paper develops a novel iterative framework IGraSS, combining a
semantic segmentation module-incorporating RGB and additional modalities (NDWI,
DEM)-with a graph-based ground-truth refinement module. The segmentation module
processes satellite imagery patches, while the refinement module operates on
the entire data viewing the infrastructure network as a graph. Experiments show
that IGraSS reduces unreachable canal segments from around 18% to 3%, and
training with refined ground truth significantly improves canal identification.
IGraSS serves as a robust framework for both refining noisy ground truth and
mapping canal networks from remote sensing imagery. We also demonstrate the
effectiveness and generalizability of IGraSS using road networks as an example,
applying a different graph-theoretic constraint to complete road networks.

</details>


### [320] [Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework](https://arxiv.org/abs/2506.08185)
*Huixin Zhan,Jason H. Moore*

Main category: cs.CV

TL;DR: 提出用离散扩散框架与VLA管道结合的方法进行机器人手术中外科医生细粒度指纹建模，在JIGSAWS数据集评估，发现个性化嵌入虽提升性能但增加身份泄露风险。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统常忽略外科医生因训练、经验和运动行为不同而产生的个性化信号。

Method: 用离散扩散框架与VLA管道结合，将手势预测作为结构化序列去噪任务，通过第三方语言模型的自然语言提示编码个性化指纹。

Result: 在JIGSAWS数据集准确重建手势序列，学习到每个外科医生独特的运动指纹；进行成员推理攻击发现更具表现力的嵌入在提升性能同时增加身份泄露易感性。

Conclusion: 个性化嵌入提升性能的同时增加身份泄露风险，外科手术建模中需平衡个性化与隐私风险。

Abstract: Surgeons exhibit distinct operating styles due to differences in training,
experience, and motor behavior - yet current AI systems often ignore this
personalization signal. We propose a novel approach to model fine-grained,
surgeon-specific fingerprinting in robotic surgery using a discrete diffusion
framework integrated with a vision-language-action (VLA) pipeline. Our method
formulates gesture prediction as a structured sequence denoising task,
conditioned on multimodal inputs including endoscopic video, surgical intent
language, and a privacy-aware embedding of surgeon identity and skill.
Personalized surgeon fingerprinting is encoded through natural language prompts
using third-party language models, allowing the model to retain individual
behavioral style without exposing explicit identity. We evaluate our method on
the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture
sequences while learning meaningful motion fingerprints unique to each surgeon.
To quantify the privacy implications of personalization, we perform membership
inference attacks and find that more expressive embeddings improve task
performance but simultaneously increase susceptibility to identity leakage.
These findings demonstrate that while personalized embeddings improve
performance, they also increase vulnerability to identity leakage, revealing
the importance of balancing personalization with privacy risk in surgical
modeling. Code is available at:
https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.

</details>


### [321] [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
*Andrew Z. Wang,Songwei Ge,Tero Karras,Ming-Yu Liu,Yogesh Balaji*

Main category: cs.CV

TL;DR: 研究用现代仅解码器大语言模型作为文生图扩散模型文本编码器的有效性，训练多模型分析影响因素，发现用各层归一化平均嵌入效果更好。


<details>
  <summary>Details</summary>
Motivation: 许多文生图模型使用较旧的T5和CLIP作为文本编码器，需研究现代仅解码器大语言模型作为文本编码器的有效性。

Method: 构建标准化训练和评估管道，用12种不同文本编码器训练27个文生图模型，分析影响文生图的关键方面。

Result: 使用最后一层嵌入作为条件性能较差，使用各层归一化平均嵌入能显著改善与复杂提示的对齐，多数使用此条件的大语言模型优于基线T5模型。

Conclusion: 使用各层归一化平均嵌入作为条件可提升文生图的高级视觉语言推理能力。

Abstract: Both text-to-image generation and large language models (LLMs) have made
significant advancements. However, many text-to-image models still employ the
somewhat outdated T5 and CLIP as their text encoders. In this work, we
investigate the effectiveness of using modern decoder-only LLMs as text
encoders for text-to-image diffusion models. We build a standardized training
and evaluation pipeline that allows us to isolate and evaluate the effect of
different text embeddings. We train a total of 27 text-to-image models with 12
different text encoders to analyze the critical aspects of LLMs that could
impact text-to-image generation, including the approaches to extract
embeddings, different LLMs variants, and model sizes. Our experiments reveal
that the de facto way of using last-layer embeddings as conditioning leads to
inferior performance. Instead, we explore embeddings from various layers and
find that using layer-normalized averaging across all layers significantly
improves alignment with complex prompts. Most LLMs with this conditioning
outperform the baseline T5 model, showing enhanced performance in advanced
visio-linguistic reasoning skills.

</details>


### [322] [Highly Compressed Tokenizer Can Generate Without Training](https://arxiv.org/abs/2506.08257)
*L. Lao Beyer,T. Li,X. Chen,S. Karaman,K. He*

Main category: cs.CV

TL;DR: 论文探讨1D图像分词器，其高压缩特性可实现图像编辑与生成，构建图像生成管道用于图像修复和文本引导编辑，无需训练生成模型。


<details>
  <summary>Details</summary>
Motivation: 1D图像分词器潜在空间具有表现力，希望利用其特性实现图像编辑和生成。

Method: 通过对1D分词器的向量量化实现高压缩，利用基于梯度的测试时间优化和即插即用的损失函数构建图像生成管道。

Result: 方法可用于图像修复和文本引导图像编辑，能生成多样且逼真的样本，无需训练生成模型。

Conclusion: 1D图像分词器的高压缩特性和潜在空间表现力可用于图像编辑和生成，且无需训练生成模型。

Abstract: Commonly used image tokenizers produce a 2D grid of spatially arranged
tokens. In contrast, so-called 1D image tokenizers represent images as highly
compressed one-dimensional sequences of as few as 32 discrete tokens. We find
that the high degree of compression achieved by a 1D tokenizer with vector
quantization enables image editing and generative capabilities through
heuristic manipulation of tokens, demonstrating that even very crude
manipulations -- such as copying and replacing tokens between latent
representations of images -- enable fine-grained image editing by transferring
appearance and semantic attributes. Motivated by the expressivity of the 1D
tokenizer's latent space, we construct an image generation pipeline leveraging
gradient-based test-time optimization of tokens with plug-and-play loss
functions such as reconstruction or CLIP similarity. Our approach is
demonstrated for inpainting and text-guided image editing use cases, and can
generate diverse and realistic samples without requiring training of any
generative model.

</details>


### [323] [Seeing Voices: Generating A-Roll Video from Audio with Mirage](https://arxiv.org/abs/2506.08279)
*Aditi Sundararaman,Amogh Adishesha,Andrew Jaegle,Dan Bigioi,Hyoung-Kyu Song,Jon Kyl,Justin Mao,Kevin Lan,Mojtaba Komeili,ShahRukh Athar,Sheila Babayan,Stanislau Beliasau,William Buchwalter*

Main category: cs.CV

TL;DR: 介绍音频到视频基础模型Mirage，可根据音频输入生成视频，与TTS结合产生多模态视频，有统一训练方法，输出质量优。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法要么忽略声音，要么应用领域受限，需新方法解决。

Method: 提出统一方法训练基于自注意力的音频到视频生成模型，可从头训练或基于现有权重训练。

Result: Mirage能生成逼真、富有表现力的图像，与TTS结合产生引人入胜的多模态视频，输出主观质量优于其他方法。

Conclusion: Mirage是一种通用的音频到视频生成方法，输出质量高。

Abstract: From professional filmmaking to user-generated content, creators and
consumers have long recognized that the power of video depends on the
harmonious integration of what we hear (the video's audio track) with what we
see (the video's image sequence). Current approaches to video generation either
ignore sound to focus on general-purpose but silent image sequence generation
or address both visual and audio elements but focus on restricted application
domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation
model that excels at generating realistic, expressive output imagery from
scratch given an audio input. When integrated with existing methods for speech
synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal
video. When trained on audio-video footage of people talking (A-roll) and
conditioned on audio containing speech, Mirage generates video of people
delivering a believable interpretation of the performance implicit in input
audio. Our central technical contribution is a unified method for training
self-attention-based audio-to-video generation models, either from scratch or
given existing weights. This methodology allows Mirage to retain generality as
an approach to audio-to-video generation while producing outputs of superior
subjective quality to methods that incorporate audio-specific architectures or
loss components specific to people, speech, or details of how images or audio
are captured. We encourage readers to watch and listen to the results of Mirage
for themselves (see paper and comments for links).

</details>


### [324] [SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging](https://arxiv.org/abs/2506.08297)
*Nhat Thanh Tran,Fanghui Xue,Shuai Zhang,Jiancheng Lyu,Yunling Zheng,Yingyong Qi,Jack Xin*

Main category: cs.CV

TL;DR: 本文提出广义注意力的数学定义，设计SEMA注意力机制，在Imagenet - 1k上证明其优于线性注意力和近期视觉Mamba模型。


<details>
  <summary>Details</summary>
Motivation: 传统全注意力计算复杂度高，线性注意力无法聚焦，以此为挑战设计新的注意力机制。

Method: 给出广义注意力数学定义，基于其分散特性和Mamba形式注意力设计SEMA，利用token定位避免分散并保持聚焦，结合算术平均捕捉全局信息。

Result: 在Imagenet - 1k分类任务中，SEMA是线性注意力的有效替代方案，在相似参数规模下，在更大尺寸图像上优于近期视觉Mamba模型。

Conclusion: SEMA是一种可扩展且有效的注意力机制，超越线性注意力。

Abstract: Attention is the critical component of a transformer. Yet the quadratic
computational complexity of vanilla full attention in the input size and the
inability of its linear attention variant to focus have been challenges for
computer vision tasks. We provide a mathematical definition of generalized
attention and formulate both vanilla softmax attention and linear attention
within the general framework. We prove that generalized attention disperses,
that is, as the number of keys tends to infinity, the query assigns equal
weights to all keys. Motivated by the dispersion property and recent
development of Mamba form of attention, we design Scalable and Efficient Mamba
like Attention (SEMA) which utilizes token localization to avoid dispersion and
maintain focusing, complemented by theoretically consistent arithmetic
averaging to capture global aspect of attention. We support our approach on
Imagenet-1k where classification results show that SEMA is a scalable and
effective alternative beyond linear attention, outperforming recent vision
Mamba models on increasingly larger scales of images at similar model parameter
sizes.

</details>


### [325] [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/abs/2506.08351)
*Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: 本文提出Step AG自适应引导策略，限制无分类器引导至前几个去噪步骤，能生成高质量图像，提速20%-30%。


<details>
  <summary>Details</summary>
Motivation: 现有无分类器引导方法成本高，之前的自适应引导方法缺乏分析和实证，无法应用于通用扩散模型。

Method: 提出Step AG这一简单且通用的自适应引导策略。

Result: 评估表明限制无分类器引导到前几个去噪步骤足以生成高质量、条件良好的图像，平均提速20% - 30%，在不同设置和多种模型中表现一致。

Conclusion: Step AG方法具有优越性。

Abstract: With the rapid development of text-to-vision generation diffusion models,
classifier-free guidance has emerged as the most prevalent method for
conditioning. However, this approach inherently requires twice as many steps
for model forwarding compared to unconditional generation, resulting in
significantly higher costs. While previous study has introduced the concept of
adaptive guidance, it lacks solid analysis and empirical results, making
previous method unable to be applied to general diffusion models. In this work,
we present another perspective of applying adaptive guidance and propose Step
AG, which is a simple, universally applicable adaptive guidance strategy. Our
evaluations focus on both image quality and image-text alignment. whose results
indicate that restricting classifier-free guidance to the first several
denoising steps is sufficient for generating high-quality, well-conditioned
images, achieving an average speedup of 20% to 30%. Such improvement is
consistent across different settings such as inference steps, and various
models including video generation models, highlighting the superiority of our
method.

</details>


### [326] [MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding](https://arxiv.org/abs/2506.08512)
*Zhiyi Zhu,Xiaoyu Wu,Zihao Liu,Linlin Yang*

Main category: cs.CV

TL;DR: 提出MLVTG框架解决现有基于Transformer的视频时间定位方法的问题，实验表明其达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的视频时间定位方法存在冗余注意力和多模态对齐不佳的问题。

Method: 提出MLVTG框架，包含MambaAligner和LLMRefiner模块，前者用Vision Mamba块建模时间依赖，后者利用预训练大语言模型的特定冻结层隐式转移语义先验。

Result: 在QVHighlights、Charades - STA和TVSum上的实验表明MLVTG达到了SOTA性能，显著优于现有基线。

Conclusion: MLVTG的双重对齐策略能实现更精确的视频时间定位。

Abstract: Video Temporal Grounding (VTG), which aims to localize video clips
corresponding to natural language queries, is a fundamental yet challenging
task in video understanding. Existing Transformer-based methods often suffer
from redundant attention and suboptimal multi-modal alignment. To address these
limitations, we propose MLVTG, a novel framework that integrates two key
modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba
blocks as a backbone instead of Transformers to model temporal dependencies and
extract robust video representations for multi-modal alignment. LLMRefiner
leverages the specific frozen layer of a pre-trained Large Language Model (LLM)
to implicitly transfer semantic priors, enhancing multi-modal alignment without
fine-tuning. This dual alignment strategy, temporal modeling via structured
state-space dynamics and semantic purification via textual priors, enables more
precise localization. Extensive experiments on QVHighlights, Charades-STA, and
TVSum demonstrate that MLVTG achieves state-of-the-art performance and
significantly outperforms existing baselines.

</details>


### [327] [TrajFlow: Multi-modal Motion Prediction via Flow Matching](https://arxiv.org/abs/2506.08541)
*Qi Yan,Brian Zhang,Yutong Zhang,Daniel Yang,Joshua White,Di Chen,Jiachao Liu,Langechuan Liu,Binnan Zhuang,Shaoshuai Shi,Renjie Liao*

Main category: cs.CV

TL;DR: 提出TrajFlow运动预测框架，单步预测多轨迹，结合排名损失和自条件训练技术，在WOMD数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 高效准确的运动预测对自动驾驶安全决策至关重要，现有生成式轨迹预测方法存在可扩展性和效率问题。

Method: 提出TrajFlow框架单步预测多轨迹；提出基于Plackett - Luce分布的排名损失；设计自条件训练技术。

Result: 在WOMD数据集上，TrajFlow在多个关键指标上达到了当前最优性能。

Conclusion: TrajFlow对安全关键的自动驾驶应用有效。

Abstract: Efficient and accurate motion prediction is crucial for ensuring safety and
informed decision-making in autonomous driving, particularly under dynamic
real-world conditions that necessitate multi-modal forecasts. We introduce
TrajFlow, a novel flow matching-based motion prediction framework that
addresses the scalability and efficiency challenges of existing generative
trajectory prediction methods. Unlike conventional generative approaches that
employ i.i.d. sampling and require multiple inference passes to capture diverse
outcomes, TrajFlow predicts multiple plausible future trajectories in a single
pass, significantly reducing computational overhead while maintaining coherence
across predictions. Moreover, we propose a ranking loss based on the
Plackett-Luce distribution to improve uncertainty estimation of predicted
trajectories. Additionally, we design a self-conditioning training technique
that reuses the model's own predictions to construct noisy inputs during a
second forward pass, thereby improving generalization and accelerating
inference. Extensive experiments on the large-scale Waymo Open Motion Dataset
(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across
various key metrics, underscoring its effectiveness for safety-critical
autonomous driving applications. The code and other details are available on
the project website https://traj-flow.github.io/.

</details>


### [328] [Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems](https://arxiv.org/abs/2506.08596)
*Guyang Zhang,Waleed Abdulla*

Main category: cs.CV

TL;DR: 本文对基于Transformer的高光谱图像（HSI）分类进行端到端综述，分析各阶段设计选择，指出面临的障碍并给出研究议程，以指导后续应用。


<details>
  <summary>Details</summary>
Motivation: Transformer在HSI领域应用尚在兴起，需要对其进行系统综述以指导后续研究。

Method: 回顾2025年前超300篇论文，对典型流程各阶段进行分类，并对比不同设计选择。

Result: 明确了领域进展及面临的障碍，如数据稀缺、维度高、计算开销大、可解释性有限等。

Conclusion: 提出优先发展有价值的公共数据集、轻量级边缘模型等研究议程，指导研究者选择合适的Transformer组件。

Abstract: Transformers have become the architecture of choice for learning long-range
dependencies, yet their adoption in hyperspectral imaging (HSI) is still
emerging. We reviewed more than 300 papers published up to 2025 and present the
first end-to-end survey dedicated to Transformer-based HSI classification. The
study categorizes every stage of a typical pipeline-pre-processing, patch or
pixel tokenization, positional encoding, spatial-spectral feature extraction,
multi-head self-attention variants, skip connections, and loss design-and
contrasts alternative design choices with the unique spatial-spectral
properties of HSI. We map the field's progress against persistent obstacles:
scarce labeled data, extreme spectral dimensionality, computational overhead,
and limited model explainability. Finally, we outline a research agenda
prioritizing valuable public data sets, lightweight on-edge models,
illumination and sensor shifts robustness, and intrinsically interpretable
attention mechanisms. Our goal is to guide researchers in selecting, combining,
or extending Transformer components that are truly fit for purpose for
next-generation HSI applications.

</details>


### [329] [ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network](https://arxiv.org/abs/2506.08629)
*Feixiang Du,Shengkun Wu*

Main category: cs.CV

TL;DR: 本文提出轻量级ECMNet用于语义分割，结合CNN和Mamba，设计了EDAB、MSAU和FFM模块，实验表明该模型在精度和效率上达到平衡。


<details>
  <summary>Details</summary>
Motivation: 现有CNN与Transformer模型在语义分割任务中全局上下文建模不足，而Mamba在视觉任务中展现出建模长距离依赖的优势，因此结合两者设计新模型。

Method: 提出ECMNet，在基于胶囊的框架中结合CNN和Mamba；设计EDAB用于轻量级瓶颈；设计MSAU整合多尺度特征聚合；使用FFM融合不同层次特征。

Result: 在Cityscapes测试集上达到70.6% mIoU，在CamVid测试集上达到73.6% mIoU，在单RTX 3090 GPU平台上有0.87M参数和8.27G FLOPs。

Conclusion: 提出的ECMNet模型在语义分割任务中能很好地平衡精度和效率。

Abstract: In the past decade, Convolutional Neural Networks (CNNs) and Transformers
have achieved wide applicaiton in semantic segmentation tasks. Although CNNs
with Transformer models greatly improve performance, the global context
modeling remains inadequate. Recently, Mamba achieved great potential in vision
tasks, showing its advantages in modeling long-range dependency. In this paper,
we propose a lightweight Efficient CNN-Mamba Network for semantic segmentation,
dubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based
framework to address their complementary weaknesses. Specifically, We design a
Enhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to
improve the representations ability of feature, We devise a Multi-Scale
Attention Unit (MSAU) to integrate multi-scale feature aggregation, spatial
aggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion
Module (FFM) merges diverse level feature, significantly enhancing segmented
accuracy. Extensive experiments on two representative datasets demonstrate that
the proposed model excels in accuracy and efficiency balance, achieving 70.6%
mIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M
parameters and 8.27G FLOPs on a single RTX 3090 GPU platform.

</details>


### [330] [Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces](https://arxiv.org/abs/2506.08729)
*Dieuwertje Alblas,Patryk Rygiel,Julian Suk,Kaj O. Kappe,Marieke Hofman,Christoph Brune,Kak Khee Yeung,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 提出用SE(3)对称变压器模型预测AAA生长，在数据集上验证可行，或有助于个性化监测策略


<details>
  <summary>Details</summary>
Motivation: 当前基于最大直径的AAA监测标准未考虑3D形状与生长的复杂关系，需个性化生长预测以改进监测策略

Method: 用SE(3)对称变压器模型，在富含局部多物理特征的血管模型表面预测AAA生长，用113个CTA扫描的纵向数据集训练模型

Result: 模型预测AAA生长的中位直径误差1.18mm，识别患者两年内是否符合手术标准准确率0.93，在外部验证集有一定泛化性

Conclusion: 从血管表面进行局部定向AAA生长预测可行，可助力个性化监测策略

Abstract: Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the
abdominal aorta. AAAs may rupture, with a survival rate of only 20\%. Current
clinical guidelines recommend elective surgical repair when the maximum AAA
diameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet
these criteria are periodically monitored, with surveillance intervals based on
the maximum AAA diameter. However, this diameter does not take into account the
complex relation between the 3D AAA shape and its growth, making standardized
intervals potentially unfit. Personalized AAA growth predictions could improve
monitoring strategies. We propose to use an SE(3)-symmetric transformer model
to predict AAA growth directly on the vascular model surface enriched with
local, multi-physical features. In contrast to other works which have
parameterized the AAA shape, this representation preserves the vascular
surface's anatomical structure and geometric fidelity. We train our model using
a longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24
AAA patients at irregularly sampled intervals. After training, our model
predicts AAA growth to the next scan moment with a median diameter error of
1.18 mm. We further demonstrate our model's utility to identify whether a
patient will become eligible for elective repair within two years (acc = 0.93).
Finally, we evaluate our model's generalization on an external validation set
consisting of 25 CTAs from 7 AAA patients from a different hospital. Our
results show that local directional AAA growth prediction from the vascular
surface is feasible and may contribute to personalized surveillance strategies.

</details>


### [331] [CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics](https://arxiv.org/abs/2506.08835)
*Shravan Nayak,Mehar Bhatia,Xiaofeng Zhang,Verena Rieser,Lisa Anne Hendricks,Sjoerd van Steenkiste,Yash Goyal,Karolina Stańczak,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: 本文首次系统量化文本到图像（T2I）模型及评估指标与文化期望的一致性，引入CulturalFrames基准，发现T2I模型常不满足文化期望，现有评估指标与人类判断相关性差。


<details>
  <summary>Details</summary>
Motivation: 随着T2I模型用于视觉内容生成的普及，需研究其准确呈现多元文化背景的能力。

Method: 引入CulturalFrames基准，包含10个国家、5个社会文化领域，有983个提示、3637张图像及超10k条人类注释。

Result: T2I模型平均44%的情况不满足文化期望，其中未满足明确期望平均达68%，隐含期望达49%；现有评估指标与人类判断文化一致性的相关性差。

Conclusion: 研究揭示关键差距，为开发更具文化意识的T2I模型和评估方法提供方向。

Abstract: The increasing ubiquity of text-to-image (T2I) models as tools for visual
content generation raises concerns about their ability to accurately represent
diverse cultural contexts. In this work, we present the first study to
systematically quantify the alignment of T2I models and evaluation metrics with
respect to both explicit as well as implicit cultural expectations. To this
end, we introduce CulturalFrames, a novel benchmark designed for rigorous human
evaluation of cultural representation in visual generations. Spanning 10
countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,
3637 corresponding images generated by 4 state-of-the-art T2I models, and over
10k detailed human annotations. We find that T2I models not only fail to meet
the more challenging implicit expectations but also the less challenging
explicit expectations. Across models and countries, cultural expectations are
missed an average of 44% of the time. Among these failures, explicit
expectations are missed at a surprisingly high average rate of 68%, while
implicit expectation failures are also significant, averaging 49%. Furthermore,
we demonstrate that existing T2I evaluation metrics correlate poorly with human
judgments of cultural alignment, irrespective of their internal reasoning.
Collectively, our findings expose critical gaps, providing actionable
directions for developing more culturally informed T2I models and evaluation
methodologies.

</details>


### [332] [Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning](https://arxiv.org/abs/2506.08854)
*Junzhuo Liu,Markus Eckstein,Zhixiang Wang,Friedrich Feuerhake,Dorit Merhof*

Main category: cs.CV

TL;DR: 开发基于对比学习的深度学习方法，从全切片图像预测空间分辨基因表达，在多疾病数据集上表现优于现有研究，还能保留基因相关性等。


<details>
  <summary>Details</summary>
Motivation: 空间转录组数据获取成本高，大规模数据获取有挑战，需要方法从其他数据预测空间分辨基因表达。

Method: 开发基于对比学习的深度学习方法，从全切片图像预测空间分辨基因表达。

Result: 在六个不同疾病数据集评估中，预测高表达基因、高可变基因和标记基因的皮尔逊相关系数分别比现有研究提高6.27%、6.11%和11.26%，保留基因相关性，适用于有限样本数据集，在基于生物标志物表达的癌组织定位有潜力。

Conclusion: 所提出的方法在预测空间分辨基因表达上有效，能保留基因相关性，适用于有限样本，在癌组织定位有应用潜力。

Abstract: Spatial transcriptomics is a technology that captures gene expression levels
at different spatial locations, widely used in tumor microenvironment analysis
and molecular profiling of histopathology, providing valuable insights into
resolving gene expression and clinical diagnosis of cancer. Due to the high
cost of data acquisition, large-scale spatial transcriptomics data remain
challenging to obtain. In this study, we develop a contrastive learning-based
deep learning method to predict spatially resolved gene expression from
whole-slide images. Evaluation across six different disease datasets
demonstrates that, compared to existing studies, our method improves Pearson
Correlation Coefficient (PCC) in the prediction of highly expressed genes,
highly variable genes, and marker genes by 6.27%, 6.11%, and 11.26%
respectively. Further analysis indicates that our method preserves gene-gene
correlations and applies to datasets with limited samples. Additionally, our
method exhibits potential in cancer tissue localization based on biomarker
expression.

</details>


### [333] [Product of Experts for Visual Generation](https://arxiv.org/abs/2506.08894)
*Yunzhi Zhang,Carson Murtuza-Lanier,Zizhang Li,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出无训练的PoE框架，实现异构模型推理时知识合成，在图像和视频合成任务有实际优势。


<details>
  <summary>Details</summary>
Motivation: 整合多源异构知识（视觉生成模型、视觉语言模型等）在当前研究不足。

Method: 提出PoE框架，通过退火重要性采样（AIS）从专家的乘积分布中采样。

Result: 在图像和视频合成任务中表现出实际益处，比单一方法有更好的可控性，还提供灵活用户界面。

Conclusion: PoE框架在异构模型知识合成方面具有可行性和优势。

Abstract: Modern neural models capture rich priors and have complementary knowledge
over shared data domains, e.g., images and videos. Integrating diverse
knowledge from multiple sources -- including visual generative models, visual
language models, and sources with human-crafted knowledge such as graphics
engines and physics simulators -- remains under-explored. We propose a Product
of Experts (PoE) framework that performs inference-time knowledge composition
from heterogeneous models. This training-free approach samples from the product
distribution across experts via Annealed Importance Sampling (AIS). Our
framework shows practical benefits in image and video synthesis tasks, yielding
better controllability than monolithic methods and additionally providing
flexible user interfaces for specifying visual generation goals.

</details>


### [334] [Inherently Faithful Attention Maps for Vision Transformers](https://arxiv.org/abs/2506.08915)
*Ananthu Aniraj,Cassio F. Dantas,Dino Ienco,Diego Marcos*

Main category: cs.CV

TL;DR: 提出基于注意力的方法，采用两阶段框架，提升对虚假关联和分布外背景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 上下文会影响物体感知，导致有偏表征，而图像级以物体为中心的任务需识别相关区域，要解决这一矛盾。

Method: 提出两阶段框架，第一阶段处理全图像发现物体部分和相关区域，第二阶段用输入注意力掩码限制感受野，两阶段联合训练。

Result: 在不同基准测试上的大量实验表明，该方法显著提升了对虚假关联和分布外背景的鲁棒性。

Conclusion: 所提出的基于注意力的两阶段方法有效，能提升模型鲁棒性。

Abstract: We introduce an attention-based method that uses learned binary attention
masks to ensure that only attended image regions influence the prediction.
Context can strongly affect object perception, sometimes leading to biased
representations, particularly when objects appear in out-of-distribution
backgrounds. At the same time, many image-level object-centric tasks require
identifying relevant regions, often requiring context. To address this
conundrum, we propose a two-stage framework: stage 1 processes the full image
to discover object parts and identify task-relevant regions, while stage 2
leverages input attention masking to restrict its receptive field to these
regions, enabling a focused analysis while filtering out potentially spurious
information. Both stages are trained jointly, allowing stage 2 to refine stage
1. Extensive experiments across diverse benchmarks demonstrate that our
approach significantly improves robustness against spurious correlations and
out-of-distribution backgrounds.

</details>


### [335] [Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions](https://arxiv.org/abs/2506.08927)
*David Acuna,Ximing Lu,Jaehun Jung,Hyunwoo Kim,Amlan Kar,Sanja Fidler,Yejin Choi*

Main category: cs.CV

TL;DR: 本文探索用受蒙特卡罗树搜索启发的算法，在无额外训练和监督下，让非推理视觉 - 语言模型产生长推理轨迹，经三个基准测试有一致改进。


<details>
  <summary>Details</summary>
Motivation: 研究已训练和部署在互联网上的非推理模型能否在无额外训练和监督下，通过搜索机制引出隐藏知识并产生长推理轨迹。

Method: 使用受蒙特卡罗树搜索（MCTS）启发的算法，将子问题 - 子答案对注入模型的输出流。

Result: 在三个基准测试中观察到一致改进，在MMMU - PRO上总体提升2%，文科方面显著提升9%。

Conclusion: 将推理构建为搜索过程，有助于非推理模型连接碎片化知识并产生扩展推理轨迹。

Abstract: Recent research in vision-language models (VLMs) has centered around the
possibility of equipping them with implicit long-form chain-of-thought
reasoning -- akin to the success observed in language models -- via
distillation and reinforcement learning. But what about the non-reasoning
models already trained and deployed across the internet? Should we simply
abandon them, or is there hope for a search mechanism that can elicit hidden
knowledge and induce long reasoning traces -- without any additional training
or supervision? In this paper, we explore this possibility using a Monte Carlo
Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer
pairs into the model's output stream. We show that framing reasoning as a
search process -- where subquestions act as latent decisions within a broader
inference trajectory -- helps the model "connect the dots" between fragmented
knowledge and produce extended reasoning traces in non-reasoning models. We
evaluate our method across three benchmarks and observe consistent
improvements. Notably, our approach yields a 2% overall improvement on
MMMU-PRO, including a significant 9% gain in Liberal Arts.

</details>


### [336] [Segment Concealed Objects with Incomplete Supervision](https://arxiv.org/abs/2506.08955)
*Chunming He,Kai Li,Yachao Zhang,Ziyun Yang,Youwei Pang,Longxiang Tang,Chengyu Fang,Yulun Zhang,Linghe Kong,Xiu Li,Sina Farsiu*

Main category: cs.CV

TL;DR: 本文提出首个用于不完全监督隐藏目标分割（ISCOS）的统一方法SEE，在多任务验证中取得SOTA性能，还能作为即插即用方案提升现有模型性能。


<details>
  <summary>Details</summary>
Motivation: ISCOS任务因训练数据监督有限和隐藏目标与背景难以区分而极具挑战，需统一方法解决。

Method: 提出统一的均值教师框架SEE，利用SAM生成伪标签，引入系列伪标签生成、存储和监督策略；设计混合粒度特征分组模块。

Result: 在多个ISCOS任务验证中取得了当前最优性能，SEE可作为即插即用方案提升现有模型性能。

Conclusion: 所提方法有效，在ISCOS任务中有良好表现且能辅助现有模型。

Abstract: Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves
segmenting objects that seamlessly blend into their surrounding environments,
utilizing incompletely annotated data, such as weak and semi-annotations, for
model training. This task remains highly challenging due to (1) the limited
supervision provided by the incompletely annotated training data, and (2) the
difficulty of distinguishing concealed objects from the background, which
arises from the intrinsic similarities in concealed scenarios. In this paper,
we introduce the first unified method for ISCOS to address these challenges. To
tackle the issue of incomplete supervision, we propose a unified mean-teacher
framework, SEE, that leverages the vision foundation model, ``\emph{Segment
Anything Model (SAM)}'', to generate pseudo-labels using coarse masks produced
by the teacher model as prompts. To mitigate the effect of low-quality
segmentation masks, we introduce a series of strategies for pseudo-label
generation, storage, and supervision. These strategies aim to produce
informative pseudo-labels, store the best pseudo-labels generated, and select
the most reliable components to guide the student model, thereby ensuring
robust network training. Additionally, to tackle the issue of intrinsic
similarity, we design a hybrid-granularity feature grouping module that groups
features at different granularities and aggregates these results. By clustering
similar features, this module promotes segmentation coherence, facilitating
more complete segmentation for both single-object and multiple-object images.
We validate the effectiveness of our approach across multiple ISCOS tasks, and
experimental results demonstrate that our method achieves state-of-the-art
performance. Furthermore, SEE can serve as a plug-and-play solution, enhancing
the performance of existing models.

</details>


### [337] [Diversity-Guided MLP Reduction for Efficient Large Vision Transformers](https://arxiv.org/abs/2506.08591)
*Chengchao Shen,Hourun Zhu,Gongfan Fang,Jianxin Wang,Xinchao Wang*

Main category: cs.CV

TL;DR: 提出DGMR方法减少大视觉Transformer模型参数，仅轻微性能下降，实验表明可近无损减少超57%参数和FLOPs。


<details>
  <summary>Details</summary>
Motivation: Transformer模型参数规模大导致计算和内存成本高，MLP模块占多数参数，需减少参数。

Method: 提出Diversity - Guided MLP Reduction (DGMR)方法，采用Gram - Schmidt权重剪枝策略消除MLP隐藏层冗余神经元，保留权重多样性以在蒸馏时恢复性能。

Result: 与从头训练模型相比，剪枝模型仅需0.06%无标签LAION - 2B数据恢复性能；在多个模型上近无损减少超57%参数和FLOPs，如EVA - CLIP - E (4.4B)减少71.5%。

Conclusion: DGMR方法能有效减少大视觉Transformer模型参数和FLOPs，且性能损失可忽略不计。

Abstract: Transformer models achieve excellent scaling property, where the performance
is improved with the increment of model capacity. However, large-scale model
parameters lead to an unaffordable cost of computing and memory. We analyze
popular transformer architectures and find that multilayer perceptron (MLP)
modules take up the majority of model parameters. To this end, we focus on the
recoverability of the compressed models and propose a Diversity-Guided MLP
Reduction (DGMR) method to significantly reduce the parameters of large vision
transformers with only negligible performance degradation. Specifically, we
conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons
of MLP hidden layer, while preserving weight diversity for better performance
recover during distillation. Compared to the model trained from scratch, our
pruned model only requires 0.06\% data of LAION-2B (for the training of large
vision transformers) without labels (ImageNet-1K) to recover the original
performance. Experimental results on several state-of-the-art large vision
transformers demonstrate that our method achieves a more than 57.0\% parameter
and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),
our method accomplishes a 71.5\% parameter and FLOPs reduction without
performance degradation. The source code and trained weights are available at
https://github.com/visresearch/DGMR.

</details>


### [338] [Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models](https://arxiv.org/abs/2506.08990)
*Chenyu Lian,Hong-Yu Zhou,Dongyun Liang,Jing Qin,Liansheng Wang*

Main category: cs.CV

TL;DR: 提出高效医学视觉 - 语言对齐方法ALTA，利用少量参数和计算量，结合多视图X光片输入提升对齐效果，实验表现优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于CLIP的跨模态对比学习方法视觉表征能力欠佳，基于多模态掩码建模的预训练模型不擅长直接跨模态匹配，需解决此矛盾。

Method: 提出ALTA方法，通过适配基于掩码记录建模的预训练视觉模型，集成多视图X光片输入增强信息一致性。

Result: ALTA在文本到图像准确率上比最佳对比方法高超4个绝对点，图像到文本检索准确率高约6个绝对点。

Conclusion: ALTA在视觉 - 语言匹配任务中表现出色，高效对齐过程促进了视觉和语言理解，代码开源。

Abstract: Medical vision-language alignment through cross-modal contrastive learning
shows promising performance in image-text matching tasks, such as retrieval and
zero-shot classification. However, conventional cross-modal contrastive
learning (CLIP-based) methods suffer from suboptimal visual representation
capabilities, which also limits their effectiveness in vision-language
alignment. In contrast, although the models pretrained via multimodal masked
modeling struggle with direct cross-modal matching, they excel in visual
representation. To address this contradiction, we propose ALTA (ALign Through
Adapting), an efficient medical vision-language alignment method that utilizes
only about 8% of the trainable parameters and less than 1/5 of the
computational consumption required for masked record modeling. ALTA achieves
superior performance in vision-language matching tasks like retrieval and
zero-shot classification by adapting the pretrained vision model from masked
record modeling. Additionally, we integrate temporal-multiview radiograph
inputs to enhance the information consistency between radiographs and their
corresponding descriptions in reports, further improving the vision-language
alignment. Experimental evaluations show that ALTA outperforms the
best-performing counterpart by over 4% absolute points in text-to-image
accuracy and approximately 6% absolute points in image-to-text retrieval
accuracy. The adaptation of vision-language models during efficient alignment
also promotes better vision and language understanding. Code is publicly
available at https://github.com/DopamineLcy/ALTA.

</details>


### [339] [Diffuse and Disperse: Image Generation with Representation Regularization](https://arxiv.org/abs/2506.09027)
*Runqian Wang,Kaiming He*

Main category: cs.CV

TL;DR: 提出Dispersive Loss正则化器改进基于扩散的生成模型，在ImageNet数据集上有提升。


<details>
  <summary>Details</summary>
Motivation: 过去十年基于扩散的生成模型发展与表征学习进展独立，且缺乏显式正则化。

Method: 提出Dispersive Loss正则化器，鼓励内部表征在隐空间分散，无需正样本对。

Result: 在ImageNet数据集上对一系列模型评估，相比广泛使用的强基线有持续改进。

Conclusion: 该工作有助于弥合生成建模和表征学习之间的差距。

Abstract: The development of diffusion-based generative models over the past decade has
largely proceeded independently of progress in representation learning. These
diffusion models typically rely on regression-based objectives and generally
lack explicit regularization. In this work, we propose \textit{Dispersive
Loss}, a simple plug-and-play regularizer that effectively improves
diffusion-based generative models. Our loss function encourages internal
representations to disperse in the hidden space, analogous to contrastive
self-supervised learning, with the key distinction that it requires no positive
sample pairs and therefore does not interfere with the sampling process used
for regression. Compared to the recent method of representation alignment
(REPA), our approach is self-contained and minimalist, requiring no
pre-training, no additional parameters, and no external data. We evaluate
Dispersive Loss on the ImageNet dataset across a range of models and report
consistent improvements over widely used and strong baselines. We hope our work
will help bridge the gap between generative modeling and representation
learning.

</details>


### [340] [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/abs/2506.09040)
*Dianyi Wang,Wei Song,Yikun Wang,Siyuan Wang,Kaicheng Yu,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出ASVR方法，在统一自回归框架下联合学习视觉和文本模态，发现自回归重建图像语义表示可提升多模态理解，在多基准测试中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 典型大视觉语言模型仅对文本序列进行自回归监督，未充分结合视觉模态，存在无法利用无字幕图像、字幕遗漏关键视觉细节、部分视觉内容难以用文本表达等局限，且有效利用自回归视觉监督提升图像理解仍是挑战。

Method: 引入Autoregressive Semantic Visual Reconstruction (ASVR)，在统一自回归框架下联合学习视觉和文本模态。

Result: 自回归重建图像原始视觉外观不能提升甚至可能损害多模态理解，而重建语义表示可稳定一致提升理解；模型用连续图像特征输入可有效重建离散语义标记；在不同数据规模和大语言模型骨干上有显著性能提升，如使LLaVA - 1.5在14个多模态基准测试中平均得分提高5%。

Conclusion: ASVR方法能有效提升多模态理解性能，代码开源。

Abstract: Typical large vision-language models (LVLMs) apply autoregressive supervision
solely to textual sequences, without fully incorporating the visual modality
into the learning process. This results in three key limitations: (1) an
inability to utilize images without accompanying captions, (2) the risk that
captions omit critical visual details, and (3) the challenge that certain
vision-centric content cannot be adequately conveyed through text. As a result,
current LVLMs often prioritize vision-to-language alignment while potentially
overlooking fine-grained visual information. While some prior works have
explored autoregressive image generation, effectively leveraging autoregressive
visual supervision to enhance image understanding remains an open challenge. In
this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),
which enables joint learning of visual and textual modalities within a unified
autoregressive framework. We show that autoregressively reconstructing the raw
visual appearance of images does not enhance and may even impair multimodal
understanding. In contrast, autoregressively reconstructing the semantic
representation of images consistently improves comprehension. Notably, we find
that even when models are given continuous image features as input, they can
effectively reconstruct discrete semantic tokens, resulting in stable and
consistent improvements across a wide range of multimodal understanding
benchmarks. Our approach delivers significant performance gains across varying
data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves
LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is
available at https://github.com/AlenjandroWang/ASVR.

</details>


### [341] [Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models](https://arxiv.org/abs/2506.08780)
*Isaac Corley,Lakshay Sharma,Ruth Crasto*

Main category: cs.CV

TL;DR: 本文介绍Landsat - Bench基准套件，为Landsat数据建立评估方法，显示SSL4EO - L预训练的GFMs表现更好。


<details>
  <summary>Details</summary>
Motivation: Landsat计划虽有超50年全球一致的地球图像数据，但缺乏相关基准限制了基于Landsat的地理空间基础模型（GFM）的发展。

Method: 引入Landsat - Bench基准套件，包含EuroSAT - L、BigEarthNet - L和LC100 - L，在常见架构和基于SSL4EO - L数据集预训练的Landsat基础模型上建立基线和标准化评估方法。

Result: SSL4EO - L预训练的GFMs在下游任务中提取特征表现更好，在EuroSAT - L和BigEarthNet - L上OA提升4%、mAP提升5.1%。

Conclusion: Landsat - Bench基准套件和评估方法有助于基于Landsat的地理空间基础模型的发展，SSL4EO - L预训练的GFMs有更好性能。

Abstract: The Landsat program offers over 50 years of globally consistent Earth
imagery. However, the lack of benchmarks for this data constrains progress
towards Landsat-based Geospatial Foundation Models (GFM). In this paper, we
introduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that
adapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and
LC100-L. We establish baseline and standardized evaluation methods across both
common architectures and Landsat foundation models pretrained on the SSL4EO-L
dataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract
better representations for downstream tasks in comparison to ImageNet,
including performance gains of +4% OA and +5.1% mAP on EuroSAT-L and
BigEarthNet-L.

</details>


### [342] [StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams](https://arxiv.org/abs/2506.08862)
*Zike Wu,Qi Yan,Xuanyu Yi,Lele Wang,Renjie Liao*

Main category: cs.CV

TL;DR: 介绍StreamSplat框架，可将未校准视频流在线转换为动态3D高斯表示，实验表明其在重建质量和动态场景建模上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时解决实时处理未校准输入、准确建模动态场景演变、保持长期稳定性和计算效率这三个关键挑战。

Method: 提出StreamSplat框架，包含静态编码器中的概率采样机制用于3DGS位置预测，动态解码器中的双向变形场用于动态建模。

Result: 在静态和动态基准测试中，StreamSplat在重建质量和动态场景建模方面始终优于先前工作，且支持任意长视频流的在线重建。

Conclusion: StreamSplat是有效的实时重建动态3D场景的框架，代码和模型已开源。

Abstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams
is crucial for numerous real-world applications. However, existing methods
struggle to jointly address three key challenges: 1) processing uncalibrated
inputs in real time, 2) accurately modeling dynamic scene evolution, and 3)
maintaining long-term stability and computational efficiency. To this end, we
introduce StreamSplat, the first fully feed-forward framework that transforms
uncalibrated video streams of arbitrary length into dynamic 3D Gaussian
Splatting (3DGS) representations in an online manner, capable of recovering
scene dynamics from temporally local observations. We propose two key technical
innovations: a probabilistic sampling mechanism in the static encoder for 3DGS
position prediction, and a bidirectional deformation field in the dynamic
decoder that enables robust and efficient dynamic modeling. Extensive
experiments on static and dynamic benchmarks demonstrate that StreamSplat
consistently outperforms prior works in both reconstruction quality and dynamic
scene modeling, while uniquely supporting online reconstruction of arbitrarily
long video streams. Code and models are available at
https://github.com/nickwzk/StreamSplat.

</details>


### [343] [Data Augmentation For Small Object using Fast AutoAugment](https://arxiv.org/abs/2506.08956)
*DaeEun Yoon,Semin Kim,SangWook Yoo,Jongha Lee*

Main category: cs.CV

TL;DR: 提出用Fast AutoAugment的最优数据增强方法提升小目标检测性能，在DOTA数据集上性能提升20%。


<details>
  <summary>Details</summary>
Motivation: 当前小目标检测性能远不如大目标，小目标检测是计算机视觉中重要且具挑战性的问题，需要提升小目标检测性能。

Method: 提出使用Fast AutoAugment的最优数据增强方法。

Result: 能快速找到克服小目标检测性能下降的最优增强策略，在DOTA数据集上性能提升20%。

Conclusion: 所提方法有效提升了小目标检测性能。

Abstract: In recent years, there has been tremendous progress in object detection
performance. However, despite these advances, the detection performance for
small objects is significantly inferior to that of large objects. Detecting
small objects is one of the most challenging and important problems in computer
vision. To improve the detection performance for small objects, we propose an
optimal data augmentation method using Fast AutoAugment. Through our proposed
method, we can quickly find optimal augmentation policies that can overcome
degradation when detecting small objects, and we achieve a 20% performance
improvement on the DOTA dataset.

</details>


### [344] [DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging](https://arxiv.org/abs/2506.09024)
*Felix Wagner,Pramit Saha,Harry Anthony,J. Alison Noble,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: 介绍隔离网络和去中心化隔离网络（DIsoN）用于解决机器学习模型部署时的OOD检测问题，在医学影像数据集上表现良好并尊重数据隐私。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域安全部署机器学习模型需要进行OOD检测，但现有方法在部署后难以有效利用训练数据。

Method: 引入隔离网络量化测试样本与训练数据分离难度，提出DIsoN在数据无法共享时通过交换模型参数比较训练和测试数据，并扩展DIsoN进行类条件比较。

Result: DIsoN在四个医学影像数据集的12个OOD检测任务中表现优于现有方法，且尊重数据隐私。

Conclusion: 该去中心化OOD检测框架为机器学习开发者提供新服务模式。

Abstract: Safe deployment of machine learning (ML) models in safety-critical domains
such as medical imaging requires detecting inputs with characteristics not seen
during training, known as out-of-distribution (OOD) detection, to prevent
unreliable predictions. Effective OOD detection after deployment could benefit
from access to the training data, enabling direct comparison between test
samples and the training data distribution to identify differences.
State-of-the-art OOD detection methods, however, either discard training data
after deployment or assume that test samples and training data are centrally
stored together, an assumption that rarely holds in real-world settings. This
is because shipping training data with the deployed model is usually impossible
due to the size of training databases, as well as proprietary or privacy
constraints. We introduce the Isolation Network, an OOD detection framework
that quantifies the difficulty of separating a target test sample from the
training data by solving a binary classification task. We then propose
Decentralized Isolation Networks (DIsoN), which enables the comparison of
training and test data when data-sharing is impossible, by exchanging only
model parameters between the remote computational nodes of training and
deployment. We further extend DIsoN with class-conditioning, comparing a target
sample solely with training data of its predicted class. We evaluate DIsoN on
four medical imaging datasets (dermatology, chest X-ray, breast ultrasound,
histopathology) across 12 OOD detection tasks. DIsoN performs favorably against
existing methods while respecting data-privacy. This decentralized OOD
detection framework opens the way for a new type of service that ML developers
could provide along with their models: providing remote, secure utilization of
their training data for OOD detection services. Code will be available upon
acceptance at: *****

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [345] [Balanced Area Deprivation Index (bADI): Enhancing social determinants of health indices to strengthen their association with healthcare clinical outcomes, utilization and costs](https://arxiv.org/abs/2506.08131)
*Mohammad Amin Morid,Robert E Tillman,Eran Halperin*

Main category: econ.GN

TL;DR: 为减少健康差距，开发新的社会健康决定因素指数bADI，评估显示其比ADI更具预测力，对医保组织有价值。


<details>
  <summary>Details</summary>
Motivation: 美国基于价值的医疗服务扩张，减少健康差距成优先事项，但常用的ADI依赖住房相关变量，在高成本地区有效性降低，需开发新指数。

Method: 开发通过标准化构建降低对住房指标依赖的bADI，用数百万医保受益人的数据评估，进行相关分析并与ADI对比。

Result: bADI与临床结果和预期寿命的相关性比ADI强，受高成本地区住房成本影响小，能更准确预测医疗使用和成本，揭示更细微的成本差异模式。

Conclusion: bADI对医疗结果和支出有更强预测力，是医保组织的有价值工具，可用于促进公平和成本效益医疗。

Abstract: Background: As value-based care expands across the U.S. healthcare system,
reducing health disparities has become a priority. Social determinants of
health (SDoH) indices, like the widely used Area Deprivation Index (ADI), guide
efforts to manage patient health and costs. However, the ADI's reliance on
housing-related variables (e.g., median home value) may reduce its
effectiveness, especially in high-cost regions, by masking inequalities and
poor health outcomes.
  Methods: To overcome these limitations, we developed the balanced ADI (bADI),
a new SDoH index that reduces dependence on housing metrics through
standardized construction. We evaluated the bADI using data from millions of
Medicare Fee-for-Service and Medicare Advantage beneficiaries. Correlation
analyses measured its association with clinical outcomes, life expectancy,
healthcare use, and cost, and compared results to the ADI.
  Results: The bADI showed stronger correlations with clinical outcomes and
life expectancy than the ADI. It was less influenced by housing costs in
expensive regions and more accurately predicted healthcare use and costs. While
ADI-based research suggested both the most and least disadvantaged groups had
higher healthcare costs, the bADI revealed a more nuanced pattern, showing more
accurate cost differences across groups.
  Conclusions: The bADI offers stronger predictive power for healthcare
outcomes and spending, making it a valuable tool for accountable care
organizations. By reallocating resources from less to more disadvantaged areas,
ACOs could use the bADI to promote equity and cost-effective care within
population health initiatives.

</details>


### [346] [Unmasking inequility: socio-economic determinants and gender disparities in Maharashtra and India's health outcomes -- Insights from NFHS-5](https://arxiv.org/abs/2506.08206)
*Sharmishtha Raghuvanshi,Supriya Sanjay Nikam,Manisha Karne,Satyanarayan Kishan Kothe*

Main category: econ.GN

TL;DR: 研究聚焦印度健康不平等问题，用NFHS - 5数据和健康经济学框架分析，发现男女发病率差异及影响因素，强调针对性政策干预重要性。


<details>
  <summary>Details</summary>
Motivation: 印度整体健康指标进步但分布不均，新冠疫情凸显问题，研究社会经济因素对健康差距的影响。

Method: 使用NFHS - 5数据，采用健康经济学框架，进行回归分析和Fairlie分解，按性别分层。

Result: 印度约九分之一、马哈拉施特拉邦约八分之一的人有自报发病率，女性发病率近男性两倍，分解分析找出不同地区影响性别差距的因素。

Conclusion: 应采取针对性政策干预，解决导致印度健康不平等的社会经济因素间的复杂相互作用。

Abstract: This research examines the persistent challenge of health inequalities in
India, departing from the conventional focus on aggregate improvements in
mortality rates. While India has achieved progress in overall health indicators
since independence, the distribution of health outcomes remains uneven, a fact
starkly highlighted by the COVID-19 pandemic. This study investigates the
socio-economic determinants of health disparities using the National Family and
Health Survey (NFHS)-5 data from 2019-20, focusing on both national and
state-level analyses, specifically for Maharashtra. Employing a health
economics framework, the analysis delves into individual-level data, population
shares, self-reported morbidity prevalence, and treatment patterns across
diverse socio-economic groups. Regression analyses, stratified by gender, are
conducted to quantify the impact of socio-economic factors on reported
morbidity. Furthermore, a Fairlie decomposition, an extension of the Oaxaca
decomposition, is utilised to dissect the gender gap in morbidity, assessing
the extent to which observed differences are attributable to explanatory
variables. The findings reveal a significant burden of self-reported morbidity,
with approximately one in nine individuals in India and one in eight in
Maharashtra reporting morbidity. Notably, women exhibit nearly double the
morbidity rate compared to men. The decomposition analysis identifies key
drivers of gender disparities. In India, marital status exacerbates these
differences, while insurance coverage, caste, urban residence, and wealth
mitigate them. In Maharashtra, urban residence and marital status widen the
gap, whereas religion, caste, and insurance coverage narrow it. This research
underscores the importance of targeted policy interventions to address the
complex interplay of socio-economic factors driving health inequalities in
India.

</details>


### [347] [Valuing Diffuse Global Public Goods from Satellite Constellations: Evidence from GPS and Airline Delays](https://arxiv.org/abs/2506.08209)
*Lev Ricanati*

Main category: econ.GN

TL;DR: 本文研究GPS精度提升的福利影响，发现美国因提高GPS精度带来至少2.68亿美元额外福利，还估算了卫星全球公共品供应受威胁的经济损失。


<details>
  <summary>Details</summary>
Motivation: 研究全球公共品离散改进（GPS精度提升）的福利影响。

Method: 对1999年和2000年运输统计局的航空公司准点绩效数据应用双重差分模型，估算每次航班因GPS改进节省的时间，并乘以次年乘客数量和他们的时间价值。

Result: 美国在2000年5月取消选择可用性、提高GPS精度，带来至少2.68亿美元（2000年美元）的额外福利收益。

Conclusion: 估算了当前卫星全球公共品供应面临威胁造成的经济损失。

Abstract: This paper studies the welfare impact of discrete improvements to global
public goods in the context of the Global Positioning System (GPS).
Specifically, I find that by disabling Selective Availability in May, 2000, and
thus significantly increasing the accuracy of GPS, the United States generated
at least \$268 million (2000 dollars) of additional welfare gains. To quantify
this welfare impact, I apply a difference-in-differences model to the Bureau of
Transportation Statistics's Airline On-Time Performance Data in the years 1999
and 2000. I use this model to estimate the time saved per flight attributable
to the improved GPS and multiply these time savings by the number of passengers
in the ensuing year and their values of time. I conclude by estimating the
economic loss from current threats to the provision of satellite-based global
public goods.

</details>


### [348] [Industrial Flexibility Investment Under Uncertainty: A Multi-Stage Stochastic Framework Considering Energy and Reserve Market Participation](https://arxiv.org/abs/2506.08638)
*Amund Norland,Lasse Skare,Ole Jakob Viken,Stian Backe*

Main category: econ.GN

TL;DR: 本文提出多阶段优化框架支持灵活资产投资决策和储备市场参与，以应对能源转型下复杂市场条件，并通过挪威工业场地案例研究不同情景下投资决策。


<details>
  <summary>Details</summary>
Motivation: 全球能源向净零排放转型，工业面临复杂市场条件，需抓住现货价格套利和储备市场参与机会并满足监管要求。

Method: 提出多阶段优化框架，纳入投资决策、现货和储备市场投标及实时操作，用节点公式处理市场价格和运营条件的不确定性。

Result: 对挪威一个大型工业场地进行案例研究，比较不同市场条件下未来技术和碳定价情景的投资决策。

Conclusion: 未明确提及结论，但框架或有助于工业在能源转型下做出投资决策和参与储备市场。

Abstract: The global energy transition toward net-zero emissions by 2050 is expected to
increase the share of variable renewable energy sources (VRES) in the energy
mix. As a result, industrial actors will encounter more complex market
conditions, characterized by volatile electricity prices, rising carbon costs,
and stricter regulations. This situation calls for the industry to capitalize
on opportunities in both spot-price arbitrage and reserve market participation,
while also meeting future regulatory demands. This paper presents a multi-stage
optimization framework that supports investment decisions in flexible assets
and enables reserve market participation by delivering ancillary services. The
framework incorporates investment decisions, spot- and reserve-market bidding,
and real-time operation. Uncertainty in market prices and operational
conditions is handled through a nodal formulation. A case study of a large
industrial site in Norway is performed, comparing the investment decisions with
future technology- and carbon pricing scenarios under varying market
conditions.

</details>


### [349] [Can knowledge reclassification accelerate technological innovation?](https://arxiv.org/abs/2506.08656)
*Peter Persoon*

Main category: econ.GN

TL;DR: 本文研究重新分类的发明如何加速技术进步，通过专利数据分析发现规律并建模，模型预测获支持。


<details>
  <summary>Details</summary>
Motivation: 探究重新分类的发明如何成为创新的新来源以加速技术进步。

Method: 以专利数据为技术知识代理，分析两个经验模式，即近期专利更易被重新分类、较大技术类别获得更多重新分类专利，然后构建模型。

Result: 模型预测在所有主要技术领域都得到支持，表明重新分类与技术进步速度有强关联。

Conclusion: 模型连接了看似无关的知识量，为增长模式的知识内在解释提供了基础。

Abstract: Technological knowledge evolves not only through the generation of new ideas,
but also through the reinterpretation of existing ones. Reinterpretations lead
to changes in the classification of knowledge, that is, reclassification. This
study investigates how reclassified inventions can serve as renewed sources of
innovation, thereby accelerating technological progress. Drawing on patent data
as a proxy for technological knowledge, I discuss two empirical patterns: (i)
more recent patents are more likely to get reclassified and (ii) larger
technological classes acquire proportionally more reclassified patents. Using
these patterns, I develop a model that explains how reclassified inventions
contribute to faster innovation. The predictions of the model are supported
across all major technology domains, suggesting a strong link between
reclassification and the pace of technological advancement. More generally, the
model connects various, seemingly unrelated knowledge quantities, providing a
basis for knowledge intrinsic explanations of growth patterns.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [350] [DCD: A Semantic Segmentation Model for Fetal Ultrasound Four-Chamber View](https://arxiv.org/abs/2506.08534)
*Donglian Li,Hui Guo,Minglang Chen,Huizhen Chen,Jialing Chen,Bocheng Liang,Pengchen Liang,Ying Tan*

Main category: eess.IV

TL;DR: 提出DCD模型用于胎儿A4C视图关键解剖结构自动分割，提高准确性。


<details>
  <summary>Details</summary>
Motivation: 胎儿超声心动图A4C视图解剖结构精确分割有挑战，为减轻超声医生工作量、提高分割准确性。

Method: 提出DCD模型，结合Dense ASPP模块进行多尺度特征提取，用CBAM模块增强自适应特征表示。

Result: DCD模型能有效捕捉局部和全局上下文信息，实现精确且稳健的分割。

Conclusion: DCD模型有助于改善产前心脏评估。

Abstract: Accurate segmentation of anatomical structures in the apical four-chamber
(A4C) view of fetal echocardiography is essential for early diagnosis and
prenatal evaluation of congenital heart disease (CHD). However, precise
segmentation remains challenging due to ultrasound artifacts, speckle noise,
anatomical variability, and boundary ambiguity across different gestational
stages. To reduce the workload of sonographers and enhance segmentation
accuracy, we propose DCD, an advanced deep learning-based model for automatic
segmentation of key anatomical structures in the fetal A4C view. Our model
incorporates a Dense Atrous Spatial Pyramid Pooling (Dense ASPP) module,
enabling superior multi-scale feature extraction, and a Convolutional Block
Attention Module (CBAM) to enhance adaptive feature representation. By
effectively capturing both local and global contextual information, DCD
achieves precise and robust segmentation, contributing to improved prenatal
cardiac assessment.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [351] [Real-Time Cascade Mitigation in Power Systems Using Influence Graph Improved by Reinforcement Learning](https://arxiv.org/abs/2506.08893)
*Kai Zhou,Youbiao He,Chong Zhong,Yifu Wu*

Main category: physics.soc-ph

TL;DR: 本文将影响图扩展为马尔可夫决策过程模型（MDP），用强化学习解决实时缓解输电系统连锁故障问题，所提学习方法收敛快，验证表明主动断开线路可降低连锁故障风险。


<details>
  <summary>Details</summary>
Motivation: 现代含大量可再生能源的电力系统连锁故障风险增加，实时缓解需在不确定下快速决策。

Method: 将影响图扩展为MDP模型，考虑发电、负荷和初始故障的不确定性，用强化学习求解，提出策略梯度学习算法，通过奖励设计学习保守策略。

Result: 所提学习方法比传统算法收敛快，在IEEE 14 - 母线和IEEE 118 - 母线系统验证，主动断开线路可有效降低连锁故障风险，部分线路对缓解连锁故障传播至关重要。

Conclusion: 提出的MDP模型和学习算法能有效缓解输电系统连锁故障，主动断开线路是降低风险的有效措施。

Abstract: Despite high reliability, modern power systems with growing renewable
penetration face an increasing risk of cascading outages. Real-time cascade
mitigation requires fast, complex operational decisions under uncertainty. In
this work, we extend the influence graph into a Markov decision process model
(MDP) for real-time mitigation of cascading outages in power transmission
systems, accounting for uncertainties in generation, load, and initial
contingencies. The MDP includes a do-nothing action to allow for conservative
decision-making and is solved using reinforcement learning. We present a policy
gradient learning algorithm initialized with a policy corresponding to the
unmitigated case and designed to handle invalid actions. The proposed learning
method converges faster than the conventional algorithm. Through careful reward
design, we learn a policy that takes conservative actions without deteriorating
system conditions. The model is validated on the IEEE 14-bus and IEEE 118-bus
systems. The results show that proactive line disconnections can effectively
reduce cascading risk, and certain lines consistently emerge as critical in
mitigating cascade propagation.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [352] [CaliciBoost: Performance-Driven Evaluation of Molecular Representations for Caco-2 Permeability Prediction](https://arxiv.org/abs/2506.08059)
*Huong Van Le,Weibin Ren,Junhong Kim,Yukyung Yun,Young Bin Park,Young Jun Kim,Bok Kyung Han,Inho Choi,Jong IL Park,Hwi-Yeol Yun,Jae-Mun Choi*

Main category: q-bio.QM

TL;DR: 研究多种分子特征表示结合自动机器学习技术预测Caco - 2渗透性，发现特定描述符有效，AutoML模型CaliciBoost表现最佳。


<details>
  <summary>Details</summary>
Motivation: 提高药物早期发现中计算预测Caco - 2渗透性的准确性和效率。

Method: 系统研究八种分子特征表示类型结合自动机器学习技术，用两个不同规模和多样性的数据集评估模型。

Result: PaDEL、Mordred和RDKit描述符对Caco - 2预测有效，AutoML模型CaliciBoost MAE表现最佳，3D描述符比2D特征降低15.73%的MAE。

Conclusion: AutoML方法在ADMET建模中有效，为数据有限的预测任务特征选择提供指导。

Abstract: Caco-2 permeability serves as a critical in vitro indicator for predicting
the oral absorption of drug candidates during early-stage drug discovery. To
enhance the accuracy and efficiency of computational predictions, we
systematically investigated the impact of eight molecular feature
representation types including 2D/3D descriptors, structural fingerprints, and
deep learning-based embeddings combined with automated machine learning
techniques to predict Caco-2 permeability. Using two datasets of differing
scale and diversity (TDC benchmark and curated OCHEM data), we assessed model
performance across representations and identified PaDEL, Mordred, and RDKit
descriptors as particularly effective for Caco-2 prediction. Notably, the
AutoML-based model CaliciBoost achieved the best MAE performance. Furthermore,
for both PaDEL and Mordred representations, the incorporation of 3D descriptors
resulted in a 15.73% reduction in MAE compared to using 2D features alone, as
confirmed by feature importance analysis. These findings highlight the
effectiveness of AutoML approaches in ADMET modeling and offer practical
guidance for feature selection in data-limited prediction tasks.

</details>


### [353] [Protriever: End-to-End Differentiable Protein Homology Search for Fitness Prediction](https://arxiv.org/abs/2506.08954)
*Ruben Weitzman,Peter Mørch Groth,Lood Van Niekerk,Aoi Otani,Yarin Gal,Debora Marks,Pascal Notin*

Main category: q-bio.QM

TL;DR: 提出端到端可微框架Protriever用于检索同源蛋白序列，在蛋白适应性预测任务中表现优异且速度快，可灵活适应不同策略和数据库。


<details>
  <summary>Details</summary>
Motivation: 传统基于多序列比对（MSA）的同源蛋白序列检索计算成本高、处理高差异序列有困难且与下游建模目标独立，需要更好的方法。

Method: 引入端到端可微框架Protriever，在训练目标任务的同时学习检索相关同源序列。

Result: 在蛋白适应性预测中，与基于MSA同源检索的序列模型相比达到了最先进性能，且通过高效向量搜索快两个数量级。

Conclusion: Protriever与架构和任务无关，能灵活适应不同检索策略和蛋白数据库，是基于比对方法的可扩展替代方案。

Abstract: Retrieving homologous protein sequences is essential for a broad range of
protein modeling tasks such as fitness prediction, protein design, structure
modeling, and protein-protein interactions. Traditional workflows have relied
on a two-step process: first retrieving homologs via Multiple Sequence
Alignments (MSA), then training models on one or more of these alignments.
However, MSA-based retrieval is computationally expensive, struggles with
highly divergent sequences or complex insertions & deletions patterns, and
operates independently of the downstream modeling objective. We introduce
Protriever, an end-to-end differentiable framework that learns to retrieve
relevant homologs while simultaneously training for the target task. When
applied to protein fitness prediction, Protriever achieves state-of-the-art
performance compared to sequence-based models that rely on MSA-based homolog
retrieval, while being two orders of magnitude faster through efficient vector
search. Protriever is both architecture- and task-agnostic, and can flexibly
adapt to different retrieval strategies and protein databases at inference time
-- offering a scalable alternative to alignment-centric approaches.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [354] [Excluding an induced wheel minor in graphs without large induced stars](https://arxiv.org/abs/2506.08829)
*Mujin Choi,Claire Hilaire,Martin Milanič,Sebastian Wiederrecht*

Main category: math.CO

TL;DR: 研究Dallard等人的猜想，证明当H为k - 轮图时猜想成立，给出相关算法及问题易处理性结果。


<details>
  <summary>Details</summary>
Motivation: 验证Dallard等人提出的关于特定图类树独立数有界的猜想。

Method: 使用将荆棘概念推广到树独立数的方法进行证明。

Result: 证明当H为k - 轮图时猜想成立；得出一些NP - 难问题在特定图类上易处理；给出在K_{1,d}- 自由图中寻找k - 轮图诱导子式模型的多项式时间算法。

Conclusion: Dallard等人的猜想在H为k - 轮图时成立，且相关算法和问题易处理性结果具有重要意义。

Abstract: We study a conjecture due to Dallard, Krnc, Kwon, Milani\v{c}, Munaro,
\v{S}torgel, and Wiederrecht stating that for any positive integer $d$ and any
planar graph $H$, the class of all $K_{1,d}$-free graphs without $H$ as an
induced minor has bounded tree-independence number. A $k$-wheel is the graph
obtained from a cycle of length $k$ by adding a vertex adjacent to all vertices
of the cycle. We show that the conjecture of Dallard et al. is true when $H$ is
a $k$-wheel for any $k\geq 3$. Our proof uses a generalization of the concept
of brambles to tree-independence number. As a consequence of our main result,
several important $\mathsf{NP}$-hard problems such as Maximum Independent Set
are tractable on $K_{1,d}$-free graphs without large induced wheel minors.
Moreover, for fixed $d$ and $k$, we provide a polynomial-time algorithm that,
given a $K_{1,d}$-free graph $G$ as input, finds an induced minor model of a
$k$-wheel in $G$ if one exists.

</details>
