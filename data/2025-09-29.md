<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 51]
- [cs.CE](#cs.CE) [Total: 5]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.LG](#cs.LG) [Total: 180]
- [cs.NE](#cs.NE) [Total: 7]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.SE](#cs.SE) [Total: 16]
- [q-fin.PM](#q-fin.PM) [Total: 2]
- [stat.ML](#stat.ML) [Total: 20]
- [econ.TH](#econ.TH) [Total: 1]
- [cs.HC](#cs.HC) [Total: 5]
- [econ.GN](#econ.GN) [Total: 8]
- [cs.GR](#cs.GR) [Total: 2]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [eess.AS](#eess.AS) [Total: 4]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.CL](#cs.CL) [Total: 54]
- [math.ST](#math.ST) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.ET](#cs.ET) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.SD](#cs.SD) [Total: 6]
- [math.OC](#math.OC) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 59]
- [stat.ME](#stat.ME) [Total: 5]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.CR](#cs.CR) [Total: 12]
- [cs.CY](#cs.CY) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [cs.RO](#cs.RO) [Total: 13]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Towards mitigating information leakage when evaluating safety monitors](https://arxiv.org/abs/2509.21344)
*Gerard Boxo,Aman Neelappa,Shivam Raval*

Main category: cs.AI

TL;DR: 提出评估白盒监控器检测大语言模型有害行为能力的框架和三种策略，以欺骗检测为例实验，发现不同策略效果。


<details>
  <summary>Details</summary>
Motivation: 训练和评估白盒监控器时，引发目标行为的信息会泄露到监控器数据中，夸大其有效性，需评估其检测真实模型行为的能力。

Method: 提出系统框架评估监控器表现，提出内容过滤、分数过滤、提示蒸馏微调模型生物三种评估策略。

Result: 内容过滤可使探测曲线下面积（AUROC）降低30%；分数过滤使AUROC降低15%；微调模型生物能改进评估但最多使性能降低40%。

Conclusion: 内容过滤是较好的缓解策略，三种策略对评估监控器性能有不同效果。

Abstract: White box monitors that analyze model internals offer promising advantages
for detecting potentially harmful behaviors in large language models, including
lower computational costs and integration into layered defense systems.However,
training and evaluating these monitors requires response exemplars that exhibit
the target behaviors, typically elicited through prompting or fine-tuning. This
presents a challenge when the information used to elicit behaviors inevitably
leaks into the data that monitors ingest, inflating their effectiveness. We
present a systematic framework for evaluating a monitor's performance in terms
of its ability to detect genuine model behavior rather than superficial
elicitation artifacts. Furthermore, we propose three novel strategies to
evaluate the monitor: content filtering (removing deception-related text from
inputs), score filtering (aggregating only over task-relevant tokens), and
prompt distilled fine-tuned model organisms (models trained to exhibit
deceptive behavior without explicit prompting). Using deception detection as a
representative case study, we identify two forms of leakage that inflate
monitor performance: elicitation leakage from prompts that explicitly request
harmful behavior, and reasoning leakage from models that verbalize their
deceptive actions. Through experiments on multiple deception benchmarks, we
apply our proposed mitigation strategies and measure performance retention. Our
evaluation of the monitors reveal three crucial findings: (1) Content filtering
is a good mitigation strategy that allows for a smooth removal of elicitation
signal and can decrease probe AUROC by 30\% (2) Score filtering was found to
reduce AUROC by 15\% but is not as straightforward to attribute to (3) A
finetuned model organism improves monitor evaluations but reduces their
performance by upto 40\%, even when re-trained.

</details>


### [2] [Correct Reasoning Paths Visit Shared Decision Pivots](https://arxiv.org/abs/2509.21549)
*Dongkyu Cho,Amy B. Z. Zhang,Bilel Fehri,Sheng Wang,Rumi Chunara,Rui Song,Hengrui Cai*

Main category: cs.AI

TL;DR: 提出决策枢轴概念，构建自训练管道使大语言模型推理对齐，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模验证大语言模型思维链推理痕迹的问题。

Method: 引入决策枢轴概念，构建包含采样推理路径、挖掘枢轴、压缩推理轨迹和自训练的管道。

Result: 在LogiQA、MedQA和MATH500等标准基准测试中展现出有效性。

Conclusion: 提出的方法能在无推理真值数据和外部指标情况下使推理对齐。

Abstract: Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of
large language models (LLMs), yet verifying those traces at scale remains
unsolved. In response, we introduce the idea of decision pivots-minimal,
verifiable checkpoints that any correct reasoning path must visit. We
hypothesize that correct reasoning, though stylistically diverse, converge on
the same pivot set, while incorrect ones violate at least one pivot. Leveraging
this property, we propose a self-training pipeline that (i) samples diverse
reasoning paths and mines shared decision pivots, (ii) compresses each trace
into pivot-focused short-path reasoning using an auxiliary verifier, and (iii)
post-trains the model using its self-generated outputs. The proposed method
aligns reasoning without ground truth reasoning data or external metrics.
Experiments on standard benchmarks such as LogiQA, MedQA, and MATH500 show the
effectiveness of our method.

</details>


### [3] [AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All You Need](https://arxiv.org/abs/2509.21553)
*Ahmed Jaber,Wangshu Zhu,Karthick Jayavelu,Justin Downes,Sameer Mohamed,Candace Agonafir,Linnia Hawkins,Tian Zheng*

Main category: cs.AI

TL;DR: 本文提出将知识图谱与AI代理集成，降低气候数据科学技术门槛，实现数据科学工作流的可扩展和代理化，推动气候数据获取的民主化。


<details>
  <summary>Details</summary>
Motivation: 气候数据科学面临数据源分散、格式异构和技术要求高的问题，限制了参与度、发现速度和工作流的可重复性。

Method: 将精心策划的知识图谱与用于云原生科学工作流的AI代理集成，利用现有云就绪API数据门户。

Result: 展示了一种通过知识图谱实现可扩展和代理化科学工作流的途径。

Conclusion: 该系统有助于实现气候数据获取的民主化，建立人类与AI协作的可重复、可扩展框架。

Abstract: Climate data science faces persistent barriers stemming from the fragmented
nature of data sources, heterogeneous formats, and the steep technical
expertise required to identify, acquire, and process datasets. These challenges
limit participation, slow discovery, and reduce the reproducibility of
scientific workflows. In this paper, we present a proof of concept for
addressing these barriers through the integration of a curated knowledge graph
(KG) with AI agents designed for cloud-native scientific workflows. The KG
provides a unifying layer that organizes datasets, tools, and workflows, while
AI agents -- powered by generative AI services -- enable natural language
interaction, automated data access, and streamlined analysis. Together, these
components drastically lower the technical threshold for engaging in climate
data science, enabling non-specialist users to identify and analyze relevant
datasets. By leveraging existing cloud-ready API data portals, we demonstrate
that "a knowledge graph is all you need" to unlock scalable and agentic
workflows for scientific inquiry. The open-source design of our system further
supports community contributions, ensuring that the KG and associated tools can
evolve as a shared commons. Our results illustrate a pathway toward
democratizing access to climate data and establishing a reproducible,
extensible framework for human--AI collaboration in scientific research.

</details>


### [4] [EEG-Based Consumer Behaviour Prediction: An Exploration from Classical Machine Learning to Graph Neural Networks](https://arxiv.org/abs/2509.21567)
*Mohammad Parsa Afshar,Aryan Azimi*

Main category: cs.AI

TL;DR: 本文利用对比方法，用EEG数据预测消费者行为，比较不同机器学习模型，发现GNN模型在部分指标上表现更好。


<details>
  <summary>Details</summary>
Motivation: 预测消费者行为是营销等领域重要目的，EEG数据可助分析决策过程，旨在用EEG数据预测消费者行为并比较模型。

Method: 先提取和清理NeuMa数据集中EEG数据特征，为GNN模型创建大脑连接特征，使用并比较不同机器学习模型，包括经典模型和GNN模型。

Result: 结果整体无显著差异，但GNN模型在一些经典模型表现不佳的基本指标上表现更好。

Conclusion: 结合EEG信号分析和机器学习模型可加深对消费者行为的理解，且提供了以往EEG神经营销研究中常用模型与新模型的全面比较。

Abstract: Prediction of consumer behavior is one of the important purposes in
marketing, cognitive neuroscience, and human-computer interaction. The
electroencephalography (EEG) data can help analyze the decision process by
providing detailed information about the brain's neural activity. In this
research, a comparative approach is utilized for predicting consumer behavior
by EEG data. In the first step, the features of the EEG data from the NeuMa
dataset were extracted and cleaned. For the Graph Neural Network (GNN) models,
the brain connectivity features were created. Different machine learning
models, such as classical models and Graph Neural Networks, are used and
compared. The GNN models with different architectures are implemented to have a
comprehensive comparison; furthermore, a wide range of classical models, such
as ensemble models, are applied, which can be very helpful to show the
difference and performance of each model on the dataset. Although the results
did not show a significant difference overall, the GNN models generally
performed better in some basic criteria where classical models were not
satisfactory. This study not only shows that combining EEG signal analysis and
machine learning models can provide an approach to deeper understanding of
consumer behavior, but also provides a comprehensive comparison between the
machine learning models that have been widely used in previous studies in the
EEG-based neuromarketing such as Support Vector Machine (SVM), and the models
which are not used or rarely used in the field, like Graph Neural Networks.

</details>


### [5] [GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models](https://arxiv.org/abs/2509.21593)
*Peng Luo,Xiayin Lou,Yu Zheng,Zhuo Zheng,Stefano Ermon*

Main category: cs.AI

TL;DR: 介绍GeoEvolve多智能体大语言模型框架，结合进化搜索与地理空间领域知识自动设计和优化地理空间算法，在两个任务上表现良好，为自动化地理空间建模提供可扩展路径。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的算法发现框架缺乏地理空间领域知识和多步推理能力，无法解决复杂地理空间问题。

Method: GeoEvolve采用内外两层循环，内循环用代码进化器生成和变异候选解，外循环的智能控制器评估全局精英并查询GeoKnowRAG模块，以知识引导进化。

Result: 在空间插值和空间不确定性量化任务中，GeoEvolve自动改进并发现新算法，降低空间插值误差13 - 21%，提高不确定性估计性能17%。消融实验证实领域引导检索对稳定、高质量进化至关重要。

Conclusion: GeoEvolve为自动化、知识驱动的地理空间建模提供可扩展路径，为可信高效的科学人工智能发现带来新机遇。

Abstract: Geospatial modeling provides critical solutions for pressing global
challenges such as sustainability and climate change. Existing large language
model (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at
evolving generic code but lack the domain knowledge and multi-step reasoning
required for complex geospatial problems. We introduce GeoEvolve, a multi-agent
LLM framework that couples evolutionary search with geospatial domain knowledge
to automatically design and refine geospatial algorithms. GeoEvolve operates in
two nested loops: an inner loop leverages a code evolver to generate and mutate
candidate solutions, while an outer agentic controller evaluates global elites
and queries a GeoKnowRAG module -- a structured geospatial knowledge base that
injects theoretical priors from geography. This knowledge-guided evolution
steers the search toward theoretically meaningful and computationally efficient
algorithms. We evaluate GeoEvolve on two fundamental and classical tasks:
spatial interpolation (kriging) and spatial uncertainty quantification
(geospatial conformal prediction). Across these benchmarks, GeoEvolve
automatically improves and discovers new algorithms, incorporating geospatial
theory on top of classical models. It reduces spatial interpolation error
(RMSE) by 13-21% and enhances uncertainty estimation performance by 17\%.
Ablation studies confirm that domain-guided retrieval is essential for stable,
high-quality evolution. These results demonstrate that GeoEvolve provides a
scalable path toward automated, knowledge-driven geospatial modeling, opening
new opportunities for trustworthy and efficient AI-for-Science discovery.

</details>


### [6] [Automated and Interpretable Survival Analysis from Multimodal Data](https://arxiv.org/abs/2509.21600)
*Mafalda Malafaia,Peter A. N. Bosman,Coen Rasch,Tanja Alderliesten*

Main category: cs.AI

TL;DR: 提出可解释的多模态AI框架MultiFIX用于肿瘤生存分析，在RADCURE数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 肿瘤学中准确且可解释的生存分析是核心挑战，随着多模态数据增加和对透明模型的临床需求，挑战更复杂。

Method: 提出基于MultiFIX的框架，用深度学习推断生存相关特征，通过Grad - CAM解释影像特征，用遗传编程将临床变量建模为符号表达式，用Cox回归进行风险估计。

Result: 在RADCURE数据集上，MultiFIX预测的C指数为0.838，分层的C指数为0.826，优于临床和学术基线方法，与已知预后标志物一致。

Conclusion: 可解释的多模态AI（MultiFIX）在精准肿瘤学中有应用前景。

Abstract: Accurate and interpretable survival analysis remains a core challenge in
oncology. With growing multimodal data and the clinical need for transparent
models to support validation and trust, this challenge increases in complexity.
We propose an interpretable multimodal AI framework to automate survival
analysis by integrating clinical variables and computed tomography imaging. Our
MultiFIX-based framework uses deep learning to infer survival-relevant features
that are further explained: imaging features are interpreted via Grad-CAM,
while clinical variables are modeled as symbolic expressions through genetic
programming. Risk estimation employs a transparent Cox regression, enabling
stratification into groups with distinct survival outcomes. Using the
open-source RADCURE dataset for head and neck cancer, MultiFIX achieves a
C-index of 0.838 (prediction) and 0.826 (stratification), outperforming the
clinical and academic baseline approaches and aligning with known prognostic
markers. These results highlight the promise of interpretable multimodal AI for
precision oncology with MultiFIX.

</details>


### [7] [Semantic F1 Scores: Fair Evaluation Under Fuzzy Class Boundaries](https://arxiv.org/abs/2509.21633)
*Georgios Chochlakis,Jackson Trager,Vedant Jhaveri,Nikhil Ravichandran,Alexandros Potamianos,Shrikanth Narayanan*

Main category: cs.AI

TL;DR: 提出语义F1分数作为主观或模糊多标签分类的评估指标，能更好反映现实情况，有更好的可解释性和生态有效性，且适用范围广。


<details>
  <summary>Details</summary>
Motivation: 传统F1指标将语义相关预测视为完全失败，现有基于相似度的指标存在不足，需要新指标更好反映人类分歧或模糊类别边界领域的实际情况。

Method: 提出语义F1分数，结合标签相似度矩阵计算类似软精度和召回率的分数，采用两步式精确 - 召回率公式。

Result: 通过理论论证和大量实证验证，表明语义F1分数具有更好的可解释性和生态有效性。

Conclusion: 语义F1分数是一种公平的评估指标，仅需合适的相似度矩阵，适用于不同任务和模态。

Abstract: We propose Semantic F1 Scores, novel evaluation metrics for subjective or
fuzzy multi-label classification that quantify semantic relatedness between
predicted and gold labels. Unlike the conventional F1 metrics that treat
semantically related predictions as complete failures, Semantic F1 incorporates
a label similarity matrix to compute soft precision-like and recall-like
scores, from which the Semantic F1 scores are derived. Unlike existing
similarity-based metrics, our novel two-step precision-recall formulation
enables the comparison of label sets of arbitrary sizes without discarding
labels or forcing matches between dissimilar labels. By granting partial credit
for semantically related but nonidentical labels, Semantic F1 better reflects
the realities of domains marked by human disagreement or fuzzy category
boundaries. In this way, it provides fairer evaluations: it recognizes that
categories overlap, that annotators disagree, and that downstream decisions
based on similar predictions lead to similar outcomes. Through theoretical
justification and extensive empirical validation on synthetic and real data, we
show that Semantic F1 demonstrates greater interpretability and ecological
validity. Because it requires only a domain-appropriate similarity matrix,
which is robust to misspecification, and not a rigid ontology, it is applicable
across tasks and modalities.

</details>


### [8] [Can AI Perceive Physical Danger and Intervene?](https://arxiv.org/abs/2509.21651)
*Abhishek Jindal,Dmitry Kalashnikov,Oscar Chang,Divya Garikapati,Anirudha Majumdar,Pierre Sermanet,Vikas Sindhwani*

Main category: cs.AI

TL;DR: 本文提出对具身AI系统进行物理安全基准测试的方法，分析大模型安全能力，并开发后训练范式提升模型安全推理能力。


<details>
  <summary>Details</summary>
Motivation: AI与物理世界交互时存在新的安全挑战，需探究现有基础模型对物理安全常识的理解。

Method: 开发基于现实伤害叙事和操作安全约束的可扩展连续物理安全基准测试方法，用生成模型将叙事和约束转化为图像视频；分析主要基础模型感知风险、安全推理和触发干预的能力；开发后训练范式让模型根据系统指令进行安全推理。

Result: 对基础模型的部署就绪性有了多方面见解；所训练模型在约束满足评估中达到了当前最优性能。

Conclusion: 所提出的基准测试和后训练范式有助于提升具身AI系统的物理安全推理能力和可解释性。

Abstract: When AI interacts with the physical world -- as a robot or an assistive agent
-- new safety challenges emerge beyond those of purely ``digital AI". In such
interactions, the potential for physical harm is direct and immediate. How well
do state-of-the-art foundation models understand common-sense facts about
physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of
coffee should not be handed to a child? In this paper, our contributions are
three-fold: first, we develop a highly scalable approach to continuous physical
safety benchmarking of Embodied AI systems, grounded in real-world injury
narratives and operational safety constraints. To probe multi-modal safety
understanding, we turn these narratives and constraints into photorealistic
images and videos capturing transitions from safe to unsafe states, using
advanced generative models. Secondly, we comprehensively analyze the ability of
major foundation models to perceive risks, reason about safety, and trigger
interventions; this yields multi-faceted insights into their deployment
readiness for safety-critical agentic applications. Finally, we develop a
post-training paradigm to teach models to explicitly reason about
embodiment-specific safety constraints provided through system instructions.
The resulting models generate thinking traces that make safety reasoning
interpretable and transparent, achieving state of the art performance in
constraint satisfaction evaluations. The benchmark will be released at
https://asimov-benchmark.github.io/v2

</details>


### [9] [Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization](https://arxiv.org/abs/2509.21718)
*Shehzeen Hussain,Paarth Neekhara,Xuesong Yang,Edresson Casanova,Subhankar Ghosh,Roy Fejgin,Ryan Langman,Mikyas Desta,Leili Tavabi,Jason Li*

Main category: cs.AI

TL;DR: 提出基于GRPO的框架，让多语言TTS模型适应新语言，在低资源和高资源语言中都提升了TTS性能。


<details>
  <summary>Details</summary>
Motivation: 低资源语言缺乏文本和语音配对数据，开发高质量TTS系统有挑战，而其ASR模型相对容易获取。

Method: 先使用IPA标记训练多语言基线建立语言无关基础，再在新语言有限配对数据上微调，最后用GRPO仅根据未配对文本和说话人提示优化模型。

Result: 该框架在低资源语言中生成可懂且说话人一致的语音，大幅优于仅微调；在高资源语言中也提升了TTS性能，超越离线对齐方法。

Conclusion: 基于GRPO的框架能有效提升TTS系统在低资源和高资源语言中的性能。

Abstract: Developing high-quality text-to-speech (TTS) systems for low-resource
languages is challenging due to the scarcity of paired text and speech data. In
contrast, automatic speech recognition (ASR) models for such languages are
often more accessible, owing to large-scale multilingual pre-training efforts.
We propose a framework based on Group Relative Policy Optimization (GRPO) to
adapt an autoregressive, multilingual TTS model to new languages. Our method
first establishes a language-agnostic foundation for TTS synthesis by training
a multilingual baseline with International Phonetic Alphabet (IPA) tokens.
Next, we fine-tune this model on limited paired data of the new languages to
capture the target language's prosodic features. Finally, we apply GRPO to
optimize the model using only unpaired text and speaker prompts, guided by a
multi-objective reward from pretrained ASR, speaker verification, and audio
quality estimation models. Experiments demonstrate that this pipeline produces
intelligible and speaker-consistent speech in low-resource languages,
substantially outperforming fine-tuning alone. Furthermore, our GRPO-based
framework also improves TTS performance in high-resource languages, surpassing
offline alignment methods such as Direct Preference Optimization (DPO) yielding
superior intelligibility, speaker similarity, and audio quality.

</details>


### [10] [Guiding Evolution of Artificial Life Using Vision-Language Models](https://arxiv.org/abs/2509.22447)
*Nikhil Baid,Hannah Erlebach,Paul Hellegouarch,Frederico Wieser*

Main category: cs.AI

TL;DR: 提出ASAL++方法用于人工生命开放式搜索，测试两种策略，结果显示该方法为人工生命发现指明新方向。


<details>
  <summary>Details</summary>
Motivation: 以往工作用视觉语言模型将人工生命模拟与自然语言目标提示对齐，本文希望基于自动搜索人工生命进一步探索。

Method: 引入ASAL++方法，用第二个基础模型根据模拟的视觉历史提出新进化目标，探索EST和ETT两种策略。

Result: 在Lenia底物中测试，EST促进更大视觉新颖性，ETT培养更连贯和可解释的进化序列。

Conclusion: ASAL++为具有开放式特征的基础模型驱动的人工生命发现指明新方向。

Abstract: Foundation models (FMs) have recently opened up new frontiers in the field of
artificial life (ALife) by providing powerful tools to automate search through
ALife simulations. Previous work aligns ALife simulations with natural language
target prompts using vision-language models (VLMs). We build on Automated
Search for Artificial Life (ASAL) by introducing ASAL++, a method for
open-ended-like search guided by multimodal FMs. We use a second FM to propose
new evolutionary targets based on a simulation's visual history. This induces
an evolutionary trajectory with increasingly complex targets.
  We explore two strategies: (1) evolving a simulation to match a single new
prompt at each iteration (Evolved Supervised Targets: EST) and (2) evolving a
simulation to match the entire sequence of generated prompts (Evolved Temporal
Targets: ETT). We test our method empirically in the Lenia substrate using
Gemma-3 to propose evolutionary targets, and show that EST promotes greater
visual novelty, while ETT fosters more coherent and interpretable evolutionary
sequences.
  Our results suggest that ASAL++ points towards new directions for FM-driven
ALife discovery with open-ended characteristics.

</details>


### [11] [Retrieval-of-Thought: Efficient Reasoning via Reusing Thoughts](https://arxiv.org/abs/2509.21743)
*Ammar Ahmed,Azal Ahmad Khan,Ayaan Ahmad,Sheng Di,Zirui Liu,Ali Anwar*

Main category: cs.AI

TL;DR: 提出Retrieval-of-Thought (RoT)方法，通过复用推理步骤减少推理冗余，在保持准确性的同时提升效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型生成长推理轨迹会增加延迟和成本，需要提高推理时的效率。

Method: 提出RoT方法，将推理步骤组织成思想图，通过检索相关节点和奖励引导遍历组装特定问题的模板来指导生成。

Result: 在多个推理基准测试中，RoT在保持准确性的同时，最多可减少40%的输出令牌、82%的推理延迟和59%的成本。

Conclusion: RoT通过检索进行动态模板构建，为高效的大型推理模型推理建立了可扩展的范式。

Abstract: Large reasoning models improve accuracy by producing long reasoning traces,
but this inflates latency and cost, motivating inference-time efficiency. We
propose Retrieval-of-Thought (RoT), which reuses prior reasoning as composable
``thought" steps to guide new problems. RoT organizes steps into a thought
graph with sequential and semantic edges to enable fast retrieval and flexible
recombination. At inference, RoT retrieves query-relevant nodes and applies
reward-guided traversal to assemble a problem-specific template that guides
generation. This dynamic template reuse reduces redundant exploration and,
therefore, reduces output tokens while preserving accuracy. We evaluate RoT on
reasoning benchmarks with multiple models, measuring accuracy, token usage,
latency, and memory overhead. Findings show small prompt growth but substantial
efficiency gains, with RoT reducing output tokens by up to 40%, inference
latency by 82%, and cost by 59% while maintaining accuracy. RoT establishes a
scalable paradigm for efficient LRM reasoning via dynamic template construction
through retrieval.

</details>


### [12] [Lifelong Learning with Behavior Consolidation for Vehicle Routing](https://arxiv.org/abs/2509.21765)
*Jiyuan Pei,Yi Mei,Jialin Liu,Mengjie Zhang,Xin Yao*

Main category: cs.AI

TL;DR: 本文探索神经VRP求解器的终身学习范式，提出LLR - BC框架，实验证明其在终身学习场景下有效。


<details>
  <summary>Details</summary>
Motivation: 现有神经求解器在应对新任务时，零样本泛化能力差或会出现灾难性遗忘问题，需要新的学习范式。

Method: 提出LLR - BC框架，通过决策寻求方式对齐新任务训练求解器和缓冲求解器的行为，对低置信度决策赋予更大整合权重。

Result: 在电容车辆路径问题和旅行商问题上的大量实验表明，LLR - BC能训练高性能神经求解器，解决灾难性遗忘问题，保持可塑性并提高零样本泛化能力。

Conclusion: LLR - BC框架在神经VRP求解器的终身学习场景中是有效的。

Abstract: Recent neural solvers have demonstrated promising performance in learning to
solve routing problems. However, existing studies are primarily based on
one-off training on one or a set of predefined problem distributions and
scales, i.e., tasks. When a new task arises, they typically rely on either
zero-shot generalization, which may be poor due to the discrepancies between
the new task and the training task(s), or fine-tuning the pretrained solver on
the new task, which possibly leads to catastrophic forgetting of knowledge
acquired from previous tasks. This paper explores a novel lifelong learning
paradigm for neural VRP solvers, where multiple tasks with diverse
distributions and scales arise sequentially over time. Solvers are required to
effectively and efficiently learn to solve new tasks while maintaining their
performance on previously learned tasks. Consequently, a novel framework called
Lifelong Learning Router with Behavior Consolidation (LLR-BC) is proposed.
LLR-BC consolidates prior knowledge effectively by aligning behaviors of the
solver trained on a new task with the buffered ones in a decision-seeking way.
To encourage more focus on crucial experiences, LLR-BC assigns greater
consolidated weights to decisions with lower confidence. Extensive experiments
on capacitated vehicle routing problems and traveling salesman problems
demonstrate LLR-BC's effectiveness in training high-performance neural solvers
in a lifelong learning setting, addressing the catastrophic forgetting issue,
maintaining their plasticity, and improving zero-shot generalization ability.

</details>


### [13] [UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios](https://arxiv.org/abs/2509.21766)
*Haotian Luo,Huaisong Zhang,Xuelin Zhang,Haoyu Wang,Zeyu Qin,Wenjie Lu,Guozheng Ma,Haiying He,Yingsha Xie,Qiyang Zhou,Zixuan Hu,Hongze Mi,Yibo Wang,Naiqiang Tan,Hong Chen,Yi R. Fung,Chun Yuan,Li Shen*

Main category: cs.AI

TL;DR: 现有自主智能体评估多针对短视距任务，本文提出UltraHorizon基准测试长视距能力，实验表明LLM - 智能体表现不佳，分析出错误类型和原因。


<details>
  <summary>Details</summary>
Motivation: 当前自主智能体评估多关注短视距、全可观测任务，而许多现实关键任务是长视距、部分可观测的，现有基准难以衡量长视距挑战，需新的评估方法。

Method: 引入UltraHorizon基准，以探索作为统一任务在三个不同环境中验证核心能力，设计长视距发现任务让智能体迭代挖掘隐藏规则。

Result: 大量实验显示LLM - 智能体在长视距任务中表现不如人类，简单扩展方法失败。

Conclusion: 智能体长视距能力存在差距，识别出八种错误类型，主要原因是上下文锁定和功能基础能力差距。

Abstract: Autonomous agents have recently achieved remarkable progress across diverse
domains, yet most evaluations focus on short-horizon, fully observable tasks.
In contrast, many critical real-world tasks, such as large-scale software
development, commercial investment, and scientific discovery, unfold in
long-horizon and partially observable scenarios where success hinges on
sustained reasoning, planning, memory management, and tool use. Existing
benchmarks rarely capture these long-horizon challenges, leaving a gap in
systematic evaluation. To bridge this gap, we introduce \textbf{UltraHorizon} a
novel benchmark that measures the foundational capabilities essential for
complex real-world challenges. We use exploration as a unifying task across
three distinct environments to validate these core competencies. Agents are
designed in long-horizon discovery tasks where they must iteratively uncover
hidden rules through sustained reasoning, planning, memory and tools
management, and interaction with environments. Under the heaviest scale
setting, trajectories average \textbf{200k+} tokens and \textbf{400+} tool
calls, whereas in standard configurations they still exceed \textbf{35k} tokens
and involve more than \textbf{60} tool calls on average. Our extensive
experiments reveal that LLM-agents consistently underperform in these settings,
whereas human participants achieve higher scores, underscoring a persistent gap
in agents' long-horizon abilities. We also observe that simple scaling fails in
our task. To better illustrate the failure of agents, we conduct an in-depth
analysis of collected trajectories. We identify eight types of errors and
attribute them to two primary causes: in-context locking and functional
fundamental capability gaps.
\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available
here.}

</details>


### [14] [Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety](https://arxiv.org/abs/2509.21782)
*Junliang Liu,Jingyu Xiao,Wenxin Tang,Wenxuan Wang,Zhixian Wang,Minrui Zhang,Shuanghe Yu*

Main category: cs.AI

TL;DR: 提出WebRSSBench基准评估多模态大语言模型在构建Web应用中的推理、鲁棒性和安全性，评估显示模型存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准对端到端Web应用所需的推理、鲁棒性和安全能力评估不足，需新基准填补空白。

Method: 从729个网站构建基准，含3799个问答对，采用标准化提示、确定性评估脚本和多阶段质量控制。

Result: 评估12个MLLMs，模型在现实布局推理、面对界面扰动鲁棒性和安全操作识别方面存在显著差距。

Conclusion: WebRSSBench可有效评估多模态大语言模型在Web应用中的推理、鲁棒性和安全性，模型有待改进。

Abstract: Multimodal large language models (MLLMs) are increasingly positioned as AI
collaborators for building complex web-related applications like GUI agents and
front-end code generation. However, existing benchmarks largely emphasize
visual perception or UI code generation, showing insufficient evaluation on the
reasoning, robustness and safety capability required for end-to-end web
applications. To bridge the gap, we introduce a comprehensive web understanding
benchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and
Safety across eight tasks, such as position relationship reasoning, color
robustness, and safety critical detection, etc. The benchmark is constructed
from 729 websites and contains 3799 question answer pairs that probe multi-step
inference over page structure, text, widgets, and safety-critical interactions.
To ensure reliable measurement, we adopt standardized prompts, deterministic
evaluation scripts, and multi-stage quality control combining automatic checks
with targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The
results reveal significant gaps, models still struggle with compositional and
cross-element reasoning over realistic layouts, show limited robustness when
facing perturbations in user interfaces and content such as layout
rearrangements or visual style shifts, and are rather conservative in
recognizing and avoiding safety critical or irreversible actions. Our code is
available at https://github.com/jinliang-byte/webssrbench.

</details>


### [15] [D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents](https://arxiv.org/abs/2509.21799)
*Hongze Mi,Yibo Feng,Wenjie Lu,Yuqi Wang,Jinyuan Li,Song Cao,He Cui,Tengfei Tian,Xuelin Zhang,Haotian Luo,Di Sun,Naiqiang Tan,Gang Pan*

Main category: cs.AI

TL;DR: 本文提出D - Artemis框架，受人类认知循环启发，增强多模态大语言模型GUI任务能力，无需复杂轨迹数据集训练，在两大基准测试获新SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理端到端训练存在数据瓶颈、错误检测成本高和指导矛盾等问题。

Method: 提出D - Artemis框架，利用细粒度、特定应用的提示检索机制，采用预执行对齐阶段（TAC检查模块和ACA协同）及后执行状态反思代理完成认知循环。

Result: 在AndroidWorld和ScreenSpot - V2上分别取得75.8%和96.8%的成功率，建立新SOTA结果，消融研究证明各组件贡献大。

Conclusion: D - Artemis框架有效增强通用MLLMs的GUI任务能力，具有强泛化性。

Abstract: Graphical User Interface (GUI) agents aim to automate a wide spectrum of
human tasks by emulating user interaction. Despite rapid advancements, current
approaches are hindered by several critical challenges: data bottleneck in
end-to-end training, high cost of delayed error detection, and risk of
contradictory guidance. Inspired by the human cognitive loop of Thinking,
Alignment, and Reflection, we present D-Artemis -- a novel deliberative
framework in this paper. D-Artemis leverages a fine-grained, app-specific tip
retrieval mechanism to inform its decision-making process. It also employs a
proactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC)
Check module and Action Correction Agent (ACA) work in concert to mitigate the
risk of execution failures. A post-execution Status Reflection Agent (SRA)
completes the cognitive loop, enabling strategic learning from experience.
Crucially, D-Artemis enhances the capabilities of general-purpose Multimodal
large language models (MLLMs) for GUI tasks without the need for training on
complex trajectory datasets, demonstrating strong generalization. D-Artemis
establishes new state-of-the-art (SOTA) results across both major benchmarks,
achieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2.
Extensive ablation studies further demonstrate the significant contribution of
each component to the framework.

</details>


### [16] [ProRe: A Proactive Reward System for GUI Agents via Reasoner-Actor Collaboration](https://arxiv.org/abs/2509.21823)
*Gaole Dai,Shiqi Jiang,Ting Cao,Yuqing Yang,Yuanchun Li,Rui Tan,Mo Li,Lili Qiu*

Main category: cs.AI

TL;DR: 提出ProRe主动奖励系统解决GUI智能体奖励评估难题，实验显示其能提升奖励准确性、F1分数和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则或模型的奖励方法难以泛化到GUI智能体，基于静态轨迹的LLM评估方法准确性有限。

Method: 提出ProRe系统，利用通用推理器和特定领域评估器智能体，推理器安排目标状态探测任务，评估器智能体与环境交互收集额外观测。

Result: 在超3K轨迹实验中，ProRe使奖励准确性和F1分数分别提升最多5.3%和19.4%，与先进策略智能体集成使成功率最多提升22.4%。

Conclusion: ProRe系统能为GUI智能体分配更准确和可验证的奖励，有效解决现有奖励方法的问题。

Abstract: Reward is critical to the evaluation and training of large language models
(LLMs). However, existing rule-based or model-based reward methods struggle to
generalize to GUI agents, where access to ground-truth trajectories or
application databases is often unavailable, and static trajectory-based
LLM-as-a-Judge approaches suffer from limited accuracy. To address these
challenges, we propose ProRe, a proactive reward system that leverages a
general-purpose reasoner and domain-specific evaluator agents (actors). The
reasoner schedules targeted state probing tasks, which the evaluator agents
then execute by actively interacting with the environment to collect additional
observations. This enables the reasoner to assign more accurate and verifiable
rewards to GUI agents. Empirical results on over 3K trajectories demonstrate
that ProRe improves reward accuracy and F1 score by up to 5.3% and 19.4%,
respectively. Furthermore, integrating ProRe with state-of-the-art policy
agents yields a success rate improvement of up to 22.4%.

</details>


### [17] [DS-STAR: Data Science Agent via Iterative Planning and Verification](https://arxiv.org/abs/2509.21825)
*Jaehyun Nam,Jinsung Yoon,Jiefeng Chen,Jinwoo Shin,Tomas Pfister*

Main category: cs.AI

TL;DR: 介绍新型数据科学代理DS - STAR，其能处理复杂数据科学任务，实验显示在多个基准测试中达最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在自动化数据科学任务时，难以处理异构数据格式和生成最优分析计划，需新方法克服这些局限。

Method: 引入DS - STAR，包含数据文件分析模块、基于大语言模型的评估步骤和顺序规划机制，通过迭代优化分析计划。

Result: DS - STAR在DABStep、KramaBench和DA - Code三个挑战性基准测试中达到最优性能，在处理多异构数据文件的困难任务上超越基线模型。

Conclusion: DS - STAR能可靠处理涉及不同数据源的复杂分析。

Abstract: Data science, which transforms raw data into actionable insights, is critical
for data-driven decision-making. However, these tasks are often complex,
involving steps for exploring multiple data sources and synthesizing findings
to deliver insightful answers. While large language models (LLMs) show
significant promise in automating this process, they often struggle with
heterogeneous data formats and generate sub-optimal analysis plans, as
verifying plan sufficiency is inherently difficult without ground-truth labels
for such open-ended tasks. To overcome these limitations, we introduce DS-STAR,
a novel data science agent. Specifically, DS-STAR makes three key
contributions: (1) a data file analysis module that automatically explores and
extracts context from diverse data formats, including unstructured types; (2) a
verification step where an LLM-based judge evaluates the sufficiency of the
analysis plan at each stage; and (3) a sequential planning mechanism that
starts with a simple, executable plan and iteratively refines it based on the
DS-STAR's feedback until its sufficiency is verified. This iterative refinement
allows DS-STAR to reliably navigate complex analyses involving diverse data
sources. Our experiments show that DS-STAR achieves state-of-the-art
performance across three challenging benchmarks: DABStep, KramaBench, and
DA-Code. Moreover, DS-STAR particularly outperforms baselines on hard tasks
that require processing multiple data files with heterogeneous formats.

</details>


### [18] [Axiomatic Choice and the Decision-Evaluation Paradox](https://arxiv.org/abs/2509.21836)
*Ben Abramowitz,Nicholas Mattei*

Main category: cs.AI

TL;DR: 介绍决策建模框架，定义决策公理分类，提出决策 - 评估悖论并强调使用公理时需谨慎。


<details>
  <summary>Details</summary>
Motivation: 构建一个能基于公理（如伦理约束）进行决策建模的框架。

Method: 引入框架，根据结构属性定义决策公理分类。

Result: 发现决策 - 评估悖论，即使用公理做决策和评估决策间存在张力。

Conclusion: 在决策数据上训练模型或应用公理做决策和评估时需格外小心。

Abstract: We introduce a framework for modeling decisions with axioms that are
statements about decisions, e.g., ethical constraints. Using our framework we
define a taxonomy of decision axioms based on their structural properties and
demonstrate a tension between the use of axioms to make decisions and the use
of axioms to evaluate decisions which we call the Decision-Evaluation Paradox.
We argue that the Decision-Evaluation Paradox arises with realistic axiom
structures, and the paradox illuminates why one must be exceptionally careful
when training models on decision data or applying axioms to make and evaluate
decisions.

</details>


### [19] [DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents](https://arxiv.org/abs/2509.21842)
*Yansong Ning,Rui Liu,Jun Wang,Kai Chen,Wei Li,Jun Fang,Kan Zheng,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: 本文提出DeepTravel框架构建自主旅行规划代理，在沙盒环境训练，有分层奖励建模系统和回复增强强化学习方法，在评估中使小模型表现优于前沿大模型。


<details>
  <summary>Details</summary>
Motivation: 现有旅行规划代理依赖手工提示和固定工作流，缺乏灵活性和自主性，需要更自主的解决方案。

Method: 构建沙盒环境，开发分层奖励建模系统，提出回复增强强化学习方法。

Result: 在滴滴企业解决方案应用上部署并评估，DeepTravel让小尺寸大语言模型在旅行规划任务中显著优于现有前沿大语言模型。

Conclusion: DeepTravel是一种有效的端到端代理强化学习框架，可用于构建自主旅行规划代理。

Abstract: Travel planning (TP) agent has recently worked as an emerging building block
to interact with external tools and resources for travel itinerary generation,
ensuring enjoyable user experience. Despite its benefits, existing studies rely
on hand craft prompt and fixed agent workflow, hindering more flexible and
autonomous TP agent. This paper proposes DeepTravel, an end to end agentic
reinforcement learning framework for building autonomous travel planning agent,
capable of autonomously planning, executing tools, and reflecting on tool
responses to explore, verify, and refine intermediate actions in multi step
reasoning. To achieve this, we first construct a robust sandbox environment by
caching transportation, accommodation and POI data, facilitating TP agent
training without being constrained by real world APIs limitations (e.g.,
inconsistent outputs). Moreover, we develop a hierarchical reward modeling
system, where a trajectory level verifier first checks spatiotemporal
feasibility and filters unsatisfied travel itinerary, and then the turn level
verifier further validate itinerary detail consistency with tool responses,
enabling efficient and precise reward service. Finally, we propose the reply
augmented reinforcement learning method that enables TP agent to periodically
replay from a failures experience buffer, emerging notable agentic capacity. We
deploy trained TP agent on DiDi Enterprise Solutions App and conduct
comprehensive online and offline evaluations, demonstrating that DeepTravel
enables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing
frontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.

</details>


### [20] [Reimagining Agent-based Modeling with Large Language Model Agents via Shachi](https://arxiv.org/abs/2509.21862)
*So Kuroki,Yingtao Tian,Kou Misaki,Takashi Ikegami,Takuya Akiba,Yujin Tang*

Main category: cs.AI

TL;DR: 本文引入Shachi方法和框架，分解代理策略为核心认知组件，在10任务基准测试验证，通过模拟关税冲击证明外部有效性，为LLM代理研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的多智能体系统中涌现行为研究缺乏实验方法，需有原则性的方法论。

Method: 引入Shachi形式化方法和模块化框架，将代理策略分解为配置、记忆、工具，并由LLM推理引擎协调。

Result: 在10任务基准测试中验证，通过模拟美国关税冲击表明，只有合理配置记忆和工具，代理行为才与市场反应一致。

Conclusion: 工作为构建和评估LLM代理提供严谨开源基础，促进积累性和科学性研究。

Abstract: The study of emergent behaviors in large language model (LLM)-driven
multi-agent systems is a critical research challenge, yet progress is limited
by a lack of principled methodologies for controlled experimentation. To
address this, we introduce Shachi, a formal methodology and modular framework
that decomposes an agent's policy into core cognitive components: Configuration
for intrinsic traits, Memory for contextual persistence, and Tools for expanded
capabilities, all orchestrated by an LLM reasoning engine. This principled
architecture moves beyond brittle, ad-hoc agent designs and enables the
systematic analysis of how specific architectural choices influence collective
behavior. We validate our methodology on a comprehensive 10-task benchmark and
demonstrate its power through novel scientific inquiries. Critically, we
establish the external validity of our approach by modeling a real-world U.S.
tariff shock, showing that agent behaviors align with observed market reactions
only when their cognitive architecture is appropriately configured with memory
and tools. Our work provides a rigorous, open-source foundation for building
and evaluating LLM agents, aimed at fostering more cumulative and
scientifically grounded research.

</details>


### [21] [TRACE: Learning to Compute on Graphs](https://arxiv.org/abs/2509.21886)
*Ziyang Zheng,Jiaying Zhu,Jingyi Zhou,Qiang Xu*

Main category: cs.AI

TL;DR: 当前主流范式不适合学习计算任务，本文提出TRACE范式，在电子电路计算图任务上显著优于先前架构，证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 主流图表示学习范式在学习计算任务上架构不匹配，无法捕捉计算的位置感知和分层性质。

Method: 引入TRACE范式，采用Hierarchical Transformer作为架构骨干，提出函数转移学习的新目标。

Result: 在电子电路计算图的综合基准测试中，TRACE显著优于所有先前架构。

Conclusion: 架构对齐的骨干和分离的学习目标为图上学习计算这一基本挑战形成了更强大的范式。

Abstract: Learning to compute, the ability to model the functional behavior of a
computational graph, is a fundamental challenge for graph representation
learning. Yet, the dominant paradigm is architecturally mismatched for this
task. This flawed assumption, central to mainstream message passing neural
networks (MPNNs) and their conventional Transformer-based counterparts,
prevents models from capturing the position-aware, hierarchical nature of
computation. To resolve this, we introduce \textbf{TRACE}, a new paradigm built
on an architecturally sound backbone and a principled learning objective.
First, TRACE employs a Hierarchical Transformer that mirrors the step-by-step
flow of computation, providing a faithful architectural backbone that replaces
the flawed permutation-invariant aggregation. Second, we introduce
\textbf{function shift learning}, a novel objective that decouples the learning
problem. Instead of predicting the complex global function directly, our model
is trained to predict only the \textit{function shift}, the discrepancy between
the true global function and a simple local approximation that assumes input
independence. We validate this paradigm on electronic circuits, one of the most
complex and economically critical classes of computational graphs. Across a
comprehensive suite of benchmarks, TRACE substantially outperforms all prior
architectures. These results demonstrate that our architecturally-aligned
backbone and decoupled learning objective form a more robust paradigm for the
fundamental challenge of learning to compute on graphs.

</details>


### [22] [GenesisGeo: Technical Report](https://arxiv.org/abs/2509.21896)
*Minfeng Zhu,Zi Wang,Sizhe Ji,Zhengtong Du,Junming Ke,Xiao Deng,Zanlang Yin,Xiuqi Huang,Heyu Wang,Wei Chen*

Main category: cs.AI

TL;DR: 介绍自动定理证明器GenesisGeo，开源大规模几何数据集，加速符号推理引擎，其在IMO-AG-30基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 开发欧几里得几何的自动定理证明器，推动几何问题自动求解的发展。

Method: 开源大规模几何数据集，通过定理匹配和C++实现核心组件加速符号推理引擎DDARN，基于Qwen3 - 0.6B - Base构建神经符号证明器GenesisGeo。

Result: 符号推理引擎DDARN加速120倍，GenesisGeo单模型解决IMO - AG - 30基准中24个问题，双模型集成解决26个问题。

Conclusion: GenesisGeo在几何问题自动求解方面有良好表现，加速的推理引擎和构建的证明器有应用价值。

Abstract: We present GenesisGeo, an automated theorem prover in Euclidean geometry. We
have open-sourced a large-scale geometry dataset of 21.8 million geometric
problems, over 3 million of which contain auxiliary constructions. Specially,
we significantly accelerate the symbolic deduction engine DDARN by 120x through
theorem matching, combined with a C++ implementation of its core components.
Furthermore, we build our neuro-symbolic prover, GenesisGeo, upon
Qwen3-0.6B-Base, which solves 24 of 30 problems (IMO silver medal level) in the
IMO-AG-30 benchmark using a single model, and achieves 26 problems (IMO gold
medal level) with a dual-model ensemble.

</details>


### [23] [DyRo-MCTS: A Robust Monte Carlo Tree Search Approach to Dynamic Job Shop Scheduling](https://arxiv.org/abs/2509.21902)
*Ruiqi Chen,Yi Mei,Fangfang Zhang,Mengjie Zhang*

Main category: cs.AI

TL;DR: 提出DyRo - MCTS方法解决动态作业车间调度问题，实验表明其能提升离线学习策略性能且优于普通MCTS。


<details>
  <summary>Details</summary>
Motivation: 现有离线调度策略不完善，新作业到达的不可预测性使在线规划复杂，需改进方法。

Method: 提出DyRo - MCTS方法，将动作鲁棒性估计融入MCTS，引导生产环境至合适状态。

Result: DyRo - MCTS显著提升离线学习策略性能，在线规划时间增加可忽略，在多种调度场景中优于普通MCTS。

Conclusion: DyRo - MCTS能在干扰下做出鲁棒调度决策，带来长期可持续性能提升。

Abstract: Dynamic job shop scheduling, a fundamental combinatorial optimisation problem
in various industrial sectors, poses substantial challenges for effective
scheduling due to frequent disruptions caused by the arrival of new jobs.
State-of-the-art methods employ machine learning to learn scheduling policies
offline, enabling rapid responses to dynamic events. However, these offline
policies are often imperfect, necessitating the use of planning techniques such
as Monte Carlo Tree Search (MCTS) to improve performance at online decision
time. The unpredictability of new job arrivals complicates online planning, as
decisions based on incomplete problem information are vulnerable to
disturbances. To address this issue, we propose the Dynamic Robust MCTS
(DyRo-MCTS) approach, which integrates action robustness estimation into MCTS.
DyRo-MCTS guides the production environment toward states that not only yield
good scheduling outcomes but are also easily adaptable to future job arrivals.
Extensive experiments show that DyRo-MCTS significantly improves the
performance of offline-learned policies with negligible additional online
planning time. Moreover, DyRo-MCTS consistently outperforms vanilla MCTS across
various scheduling scenarios. Further analysis reveals that its ability to make
robust scheduling decisions leads to long-term, sustainable performance gains
under disturbances.

</details>


### [24] [Outlier Detection in Plantar Pressure: Human-Centered Comparison of Statistical Parametric Mapping and Explainable Machine Learning](https://arxiv.org/abs/2509.21943)
*Carlo Dindorf,Jonas Dully,Steven Simon,Dennis Perchthaler,Stephan Becker,Hannah Ehmann,Kjell Heitmann,Bernd Stetter,Christian Diers,Michael Fröhlich*

Main category: cs.AI

TL;DR: 研究对比SPM与可解释机器学习方法建立足底压力数据集质控流程，机器学习模型表现更好，二者解释性获认可，强调解释性重要性。


<details>
  <summary>Details</summary>
Motivation: 足底压力映射数据存在异常值，SPM对异常值检测能力不明，需建立透明质控流程。

Method: 对比非参数、依赖配准的SPM方法和用SHAP解释的卷积神经网络，通过嵌套交叉验证评估性能，用语义差异调查评估解释质量。

Result: 机器学习模型准确率高，优于SPM，专家认为SPM和SHAP解释清晰、有用、可信，SPM复杂度更低。

Conclusion: SPM和可解释机器学习在足底压力数据自动异常值检测中有互补潜力，强调解释性对决策的重要性。

Abstract: Plantar pressure mapping is essential in clinical diagnostics and sports
science, yet large heterogeneous datasets often contain outliers from technical
errors or procedural inconsistencies. Statistical Parametric Mapping (SPM)
provides interpretable analyses but is sensitive to alignment and its capacity
for robust outlier detection remains unclear. This study compares an SPM
approach with an explainable machine learning (ML) approach to establish
transparent quality-control pipelines for plantar pressure datasets. Data from
multiple centers were annotated by expert consensus and enriched with synthetic
anomalies resulting in 798 valid samples and 2000 outliers. We evaluated (i) a
non-parametric, registration-dependent SPM approach and (ii) a convolutional
neural network (CNN), explained using SHapley Additive exPlanations (SHAP).
Performance was assessed via nested cross-validation; explanation quality via a
semantic differential survey with domain experts. The ML model reached high
accuracy and outperformed SPM, which misclassified clinically meaningful
variations and missed true outliers. Experts perceived both SPM and SHAP
explanations as clear, useful, and trustworthy, though SPM was assessed less
complex. These findings highlight the complementary potential of SPM and
explainable ML as approaches for automated outlier detection in plantar
pressure data, and underscore the importance of explainability in translating
complex model outputs into interpretable insights that can effectively inform
decision-making.

</details>


### [25] [CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration](https://arxiv.org/abs/2509.21981)
*Zhimin Wang,Shaokang He,Duo Wu,Jinghe Wang,Linjia Kang,Jing Yu,Zhi Wang*

Main category: cs.AI

TL;DR: 现有LLM协作框架忽视动态意图推理潜力，本文提出CoBel - World框架，在基准测试中降低通信成本、提高任务完成效率，表明意图感知信念建模对LLM多智能体系统协作很重要。


<details>
  <summary>Details</summary>
Motivation: 现有LLM协作框架忽视动态意图推理潜力，导致计划不一致和通信冗余，降低协作效率，需解决此问题。

Method: 提出CoBel - World框架，为LLM智能体配备协作信念世界，通过符号信念语言将开放世界任务知识解析为结构化信念，进行零样本贝叶斯式信念更新。

Result: 在TDW - MAT和C - WAH基准测试中，CoBel - World较最强基线显著降低22 - 60%通信成本，提高4 - 28%任务完成效率。

Conclusion: 显式、意图感知的信念建模对基于LLM的多智能体系统高效、类人协作至关重要。

Abstract: Effective real-world multi-agent collaboration requires not only accurate
planning but also the ability to reason about collaborators' intents -- a
crucial capability for avoiding miscoordination and redundant communication
under partial observable environments. Due to their strong planning and
reasoning capabilities, large language models (LLMs) have emerged as promising
autonomous agents for collaborative task solving. However, existing
collaboration frameworks for LLMs overlook their reasoning potential for
dynamic intent inference, and thus produce inconsistent plans and redundant
communication, reducing collaboration efficiency. To bridge this gap, we
propose CoBel-World, a novel framework that equips LLM agents with a
collaborative belief world -- an internal representation jointly modeling the
physical environment and collaborators' mental states. CoBel-World enables
agents to parse open-world task knowledge into structured beliefs via a
symbolic belief language, and perform zero-shot Bayesian-style belief updates
through LLM reasoning. This allows agents to proactively detect potential
miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated
on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World
significantly reduces communication costs by 22-60% and improves task
completion efficiency by 4-28% compared to the strongest baseline. Our results
show that explicit, intent-aware belief modeling is essential for efficient and
human-like collaboration in LLM-based multi-agent systems.

</details>


### [26] [RISK: A Framework for GUI Agents in E-commerce Risk Management](https://arxiv.org/abs/2509.21982)
*Renqi Chen,Zeyin Tao,Jianming Guo,Jingzhe Zhu,Yiheng Peng,Qingqing Sun,Tianyi Zhang,Shuai Chen*

Main category: cs.AI

TL;DR: 传统抓取方法和多数GUI代理难以处理电商风险管理所需的复杂网页数据交互，本文提出RISK框架，实验表明其表现优于现有基线，推动电商风险管理发展。


<details>
  <summary>Details</summary>
Motivation: 传统抓取方法和现有GUI代理无法处理电商风险管理中多步骤、有状态的网页数据交互及动态内容，需新解决方案。

Method: 引入RISK框架，包含RISK - Data数据集、RISK - Bench基准和RISK - R1强化微调框架。

Result: RISK - R1在离线单步和多步任务上分别提升6.8%和8.8%，在线评估任务成功率达70.5%。

Conclusion: RISK为自动化复杂网页交互提供可扩展、特定领域的解决方案，推动电商风险管理技术发展。

Abstract: E-commerce risk management requires aggregating diverse, deeply embedded web
data through multi-step, stateful interactions, which traditional scraping
methods and most existing Graphical User Interface (GUI) agents cannot handle.
These agents are typically limited to single-step tasks and lack the ability to
manage dynamic, interactive content critical for effective risk assessment. To
address this challenge, we introduce RISK, a novel framework designed to build
and deploy GUI agents for this domain. RISK integrates three components: (1)
RISK-Data, a dataset of 8,492 single-step and 2,386 multi-step interaction
trajectories, collected through a high-fidelity browser framework and a
meticulous data curation process; (2) RISK-Bench, a benchmark with 802
single-step and 320 multi-step trajectories across three difficulty levels for
standardized evaluation; and (3) RISK-R1, a R1-style reinforcement fine-tuning
framework considering four aspects: (i) Output Format: Updated format reward to
enhance output syntactic correctness and task comprehension, (ii) Single-step
Level: Stepwise accuracy reward to provide granular feedback during early
training stages, (iii) Multi-step Level: Process reweight to emphasize critical
later steps in interaction sequences, and (iv) Task Level: Level reweight to
focus on tasks of varying difficulty. Experiments show that RISK-R1 outperforms
existing baselines, achieving a 6.8% improvement in offline single-step and an
8.8% improvement in offline multi-step. Moreover, it attains a top task success
rate of 70.5% in online evaluation. RISK provides a scalable, domain-specific
solution for automating complex web interactions, advancing the state of the
art in e-commerce risk management.

</details>


### [27] [Bilinear relational structure fixes reversal curse and enables consistent model editing](https://arxiv.org/abs/2509.21993)
*Dong-Kyum Kim,Minsung Kim,Jea Kwon,Nakyeong Yang,Meeyoung Cha*

Main category: cs.AI

TL;DR: 研究表明语言模型的反转诅咒并非固有缺陷，训练关系知识数据集可使双线性结构出现，缓解反转诅咒并利于模型编辑。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型存在的反转诅咒这一被广泛认为的根本局限问题。

Method: 从零开始在关系知识图的合成数据集上训练语言模型。

Result: 模型隐藏表示中出现双线性关系结构，缓解反转诅咒，利于模型编辑，缺乏该结构的模型有更多问题。

Conclusion: 训练关系知识数据集诱导双线性内部表示出现，使语言模型编辑后逻辑一致，模型编辑成功不仅取决于算法，还取决于知识的底层表示几何。

Abstract: The reversal curse -- a language model's (LM) inability to infer an unseen
fact ``B is A'' from a learned fact ``A is B'' -- is widely considered a
fundamental limitation. We show that this is not an inherent failure but an
artifact of how models encode knowledge. By training LMs from scratch on a
synthetic dataset of relational knowledge graphs, we demonstrate that bilinear
relational structure emerges in their hidden representations. This structure
substantially alleviates the reversal curse, enabling LMs to infer unseen
reverse facts. Crucially, we also find that this bilinear structure plays a key
role in consistent model editing. When a fact is updated in a LM with this
structure, the edit correctly propagates to its reverse and other logically
dependent facts. In contrast, models lacking this representation not only
suffer from the reversal curse but also fail to generalize edits, further
introducing logical inconsistencies. Our results establish that training on a
relational knowledge dataset induces the emergence of bilinear internal
representations, which in turn enable LMs to behave in a logically consistent
manner after editing. This implies that the success of model editing depends
critically not just on editing algorithms but on the underlying
representational geometry of the knowledge being modified.

</details>


### [28] [GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments](https://arxiv.org/abs/2509.21998)
*Hanlin Zhu,Tianyu Guo,Song Mei,Stuart Russell,Nikhil Ghosh,Alberto Bietti,Jiantao Jiao*

Main category: cs.AI

TL;DR: 构建GSM - Agent基准测试LLM代理的代理推理能力，提出代理推理图概念分析推理模式，提出工具增强测试时缩放方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有代理基准测试常将代理推理与复杂能力混合，难以单独评估代理推理，需新基准。

Method: 构建GSM - Agent基准，提出代理推理图概念，提出工具增强测试时缩放方法。

Result: 前沿模型如GPT - 5在GSM - Agent上准确率仅67%，发现很多模型代理推理缺少重访先前节点能力。

Conclusion: 提出的基准和代理推理框架有助于未来对代理推理的研究。

Abstract: As LLMs are increasingly deployed as agents, agentic reasoning - the ability
to combine tool use, especially search, and reasoning - becomes a critical
skill. However, it is hard to disentangle agentic reasoning when evaluated in
complex environments and tasks. Current agent benchmarks often mix agentic
reasoning with challenging math reasoning, expert-level knowledge, and other
advanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent,
where an LLM agent is required to solve grade-school-level reasoning problems,
but is only presented with the question in the prompt without the premises that
contain the necessary information to solve the task, and needs to proactively
collect that information using tools. Although the original tasks are
grade-school math problems, we observe that even frontier models like GPT-5
only achieve 67% accuracy. To understand and analyze the agentic reasoning
patterns, we propose the concept of agentic reasoning graph: cluster the
environment's document embeddings into nodes, and map each tool call to its
nearest node to build a reasoning path. Surprisingly, we identify that the
ability to revisit a previously visited node, widely taken as a crucial pattern
in static reasoning, is often missing for agentic reasoning for many models.
Based on the insight, we propose a tool-augmented test-time scaling method to
improve LLM's agentic reasoning performance by adding tools to encourage models
to revisit. We expect our benchmark and the agentic reasoning framework to aid
future studies of understanding and pushing the boundaries of agentic
reasoning.

</details>


### [29] [The Thinking Spectrum: An Emperical Study of Tunable Reasoning in LLMs through Model Merging](https://arxiv.org/abs/2509.22034)
*Xiaochong Lan,Yu Zheng,Shiteng Cao,Yong Li*

Main category: cs.AI

TL;DR: 本文通过大规模实证研究评估模型合并技术，发现其能有效控制推理准确性和标记效率间的权衡，还识别出帕累托改进情况，并为创建特定推理能力的大语言模型提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 现实应用对具有可调推理能力的大语言模型需求增长，现有模型合并技术对推理能力的细粒度控制潜力待挖掘。

Method: 对多种模型合并技术在多个推理基准上进行大规模实证研究，系统改变合并强度构建准确率 - 效率曲线。

Result: 模型合并能有效且可控地平衡推理准确性和标记效率，即便父模型权重空间差异大；还发现了帕累托改进情况。

Conclusion: 本研究首次全面分析可调空间，为创建满足不同应用需求的特定推理能力大语言模型提供实用指南。

Abstract: The growing demand for large language models (LLMs) with tunable reasoning
capabilities in many real-world applications highlights a critical need for
methods that can efficiently produce a spectrum of models balancing reasoning
depth and computational cost. Model merging has emerged as a promising,
training-free technique to address this challenge by arithmetically combining
the weights of a general-purpose model with a specialized reasoning model.
While various merging techniques exist, their potential to create a spectrum of
models with fine-grained control over reasoning abilities remains largely
unexplored. This work presents a large-scale empirical study evaluating a range
of model merging techniques across multiple reasoning benchmarks. We
systematically vary merging strengths to construct accuracy-efficiency curves,
providing the first comprehensive view of the tunable performance landscape.
Our findings reveal that model merging offers an effective and controllable
method for calibrating the trade-off between reasoning accuracy and token
efficiency, even when parent models have highly divergent weight spaces.
Crucially, we identify instances of Pareto Improvement, where a merged model
achieves both higher accuracy and lower token consumption than one of its
parents. Our study provides the first comprehensive analysis of this tunable
space, offering practical guidelines for creating LLMs with specific reasoning
profiles to meet diverse application demands.

</details>


### [30] [A2R: An Asymmetric Two-Stage Reasoning Framework for Parallel Reasoning](https://arxiv.org/abs/2509.22044)
*Ziqi Wang,Boye Niu,Zhongli Li,Linghui Meng,Jing Liu,Zhi Zheng,Tong Xu,Hua Wu,Haifeng Wang,Enhong Chen*

Main category: cs.AI

TL;DR: 提出A2R框架，通过两阶段推理提升模型性能，有创新性成果且高效实用。


<details>
  <summary>Details</summary>
Motivation: 解决模型单次尝试表现与潜在能力间的差距问题。

Method: 提出A2R框架，‘探索者’模型并行生成潜在解，‘合成器’模型整合参考进行二次推理。

Result: Qwen3 - 8B - distill模型用框架性能提升75%；A2R - Efficient以近30%低成本超越Qwen3 - 32B模型平均性能。

Conclusion: A2R是提升性能的框架，也是高效实用的现实应用解决方案。

Abstract: Recent Large Reasoning Models have achieved significant improvements in
complex task-solving capabilities by allocating more computation at the
inference stage with a "thinking longer" paradigm. Even as the foundational
reasoning capabilities of models advance rapidly, the persistent gap between a
model's performance in a single attempt and its latent potential, often
revealed only across multiple solution paths, starkly highlights the disparity
between its realized and inherent capabilities. To address this, we present
A2R, an Asymmetric Two-Stage Reasoning framework designed to explicitly bridge
the gap between a model's potential and its actual performance. In this
framework, an "explorer" model first generates potential solutions in parallel
through repeated sampling. Subsequently,a "synthesizer" model integrates these
references for a more refined, second stage of reasoning. This two-stage
process allows computation to be scaled orthogonally to existing sequential
methods. Our work makes two key innovations: First, we present A2R as a
plug-and-play parallel reasoning framework that explicitly enhances a model's
capabilities on complex questions. For example, using our framework, the
Qwen3-8B-distill model achieves a 75% performance improvement compared to its
self-consistency baseline. Second, through a systematic analysis of the
explorer and synthesizer roles, we identify an effective asymmetric scaling
paradigm. This insight leads to A2R-Efficient, a "small-to-big" variant that
combines a Qwen3-4B explorer with a Qwen3-8B synthesizer. This configuration
surpasses the average performance of a monolithic Qwen3-32B model at a nearly
30% lower cost. Collectively, these results show that A2R is not only a
performance-boosting framework but also an efficient and practical solution for
real-world applications.

</details>


### [31] [Generalizing Multi-Objective Search via Objective-Aggregation Functions](https://arxiv.org/abs/2509.22085)
*Hadar Peer,Eyal Weiss,Ron Alterovitz,Oren Salzman*

Main category: cs.AI

TL;DR: 本文提出广义多目标搜索问题公式，支持应用标准MOS算法，在多个机器人规划问题中验证其性能远超未使用目标聚合的算法。


<details>
  <summary>Details</summary>
Motivation: 现实机器人系统需平衡多目标，现有问题公式无法使用现成的MOS算法。

Method: 提出通过隐藏目标的聚合函数优化解决方案目标的广义问题公式，扩展标准MOS算法的核心操作以适配特定聚合函数。

Result: 在多种机器人规划问题中，扩展核心操作后的算法性能远超未使用目标聚合的算法。

Conclusion: 所提出的广义问题公式和扩展核心操作的方法有效，能显著提升标准MOS算法在多目标搜索问题中的性能。

Abstract: Multi-objective search (MOS) has become essential in robotics, as real-world
robotic systems need to simultaneously balance multiple, often conflicting
objectives. Recent works explore complex interactions between objectives,
leading to problem formulations that do not allow the usage of out-of-the-box
state-of-the-art MOS algorithms. In this paper, we suggest a generalized
problem formulation that optimizes solution objectives via aggregation
functions of hidden (search) objectives. We show that our formulation supports
the application of standard MOS algorithms, necessitating only to properly
extend several core operations to reflect the specific aggregation functions
employed. We demonstrate our approach in several diverse robotics planning
problems, spanning motion-planning for navigation, manipulation and planning fr
medical systems under obstacle uncertainty as well as inspection planning, and
route planning with different road types. We solve the problems using
state-of-the-art MOS algorithms after properly extending their core operations,
and provide empirical evidence that they outperform by orders of magnitude the
vanilla versions of the algorithms applied to the same problems but without
objective aggregation.

</details>


### [32] [Ground-Truthing AI Energy Consumption: Validating CodeCarbon Against External Measurements](https://arxiv.org/abs/2509.22092)
*Raphael Fischer*

Main category: cs.AI

TL;DR: 研究系统评估AI模型能耗估算方法可靠性，发现误差最高达40%，为可持续AI发展做贡献。


<details>
  <summary>Details</summary>
Motivation: 现有AI能耗估算工具作了实用假设且忽略重要因素，估算准确性存疑。

Method: 通过数百个AI实验，将静态和动态能耗估算方法与真实测量值对比，使用提出的验证框架评估。

Result: 既定估算方法虽能反映AI能耗模式，但误差最高达40%。

Conclusion: 研究为可持续AI发展工具提供透明度和验证，给出改进指南和扩展验证代码。

Abstract: Although machine learning (ML) and artificial intelligence (AI) present
fascinating opportunities for innovation, their rapid development is also
significantly impacting our environment. In response to growing
resource-awareness in the field, quantification tools such as the ML Emissions
Calculator and CodeCarbon were developed to estimate the energy consumption and
carbon emissions of running AI models. They are easy to incorporate into AI
projects, however also make pragmatic assumptions and neglect important
factors, raising the question of estimation accuracy. This study systematically
evaluates the reliability of static and dynamic energy estimation approaches
through comparisons with ground-truth measurements across hundreds of AI
experiments. Based on the proposed validation framework, investigative insights
into AI energy demand and estimation inaccuracies are provided. While generally
following the patterns of AI energy consumption, the established estimation
approaches are shown to consistently make errors of up to 40%. By providing
empirical evidence on energy estimation quality and errors, this study
establishes transparency and validates widely used tools for sustainable AI
development. It moreover formulates guidelines for improving the
state-of-the-art and offers code for extending the validation to other domains
and tools, thus making important contributions to resource-aware ML and AI
sustainability research.

</details>


### [33] [Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach](https://arxiv.org/abs/2509.22137)
*Seoyoung Lee,Seonbin Yoon,Seongbeen Lee,Hyesoo Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: 现有基于LLM或VLM的GUI任务自动化代理存在泛化能力弱等问题，Log2Plan结合两级规划框架和任务挖掘方法解决这些问题，实验显示其在成功率和执行时间上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM或VLM的规划执行代理在GUI任务自动化中存在泛化能力弱、延迟高和长程连贯性有限等问题。

Method: 结合结构化两级规划框架和基于用户行为日志的任务挖掘方法，构建高级计划并将其转化为低级动作序列。

Result: 在200个真实世界任务中，任务成功率和执行时间有显著改善，长程任务序列成功率超60%。

Conclusion: Log2Plan能实现稳健且适应性强的GUI自动化，在复杂多步工作流中表现出良好的鲁棒性。

Abstract: GUI task automation streamlines repetitive tasks, but existing LLM or
VLM-based planner-executor agents suffer from brittle generalization, high
latency, and limited long-horizon coherence. Their reliance on single-shot
reasoning or static plans makes them fragile under UI changes or complex tasks.
Log2Plan addresses these limitations by combining a structured two-level
planning framework with a task mining approach over user behavior logs,
enabling robust and adaptable GUI automation. Log2Plan constructs high-level
plans by mapping user commands to a structured task dictionary, enabling
consistent and generalizable automation. To support personalization and reuse,
it employs a task mining approach from user behavior logs that identifies
user-specific patterns. These high-level plans are then grounded into low-level
action sequences by interpreting real-time GUI context, ensuring robust
execution across varying interfaces. We evaluated Log2Plan on 200 real-world
tasks, demonstrating significant improvements in task success rate and
execution time. Notably, it maintains over 60.0% success rate even on
long-horizon task sequences, highlighting its robustness in complex, multi-step
workflows.

</details>


### [34] [Clinical Uncertainty Impacts Machine Learning Evaluations](https://arxiv.org/abs/2509.22242)
*Simone Lionetti,Fabian Gröger,Philippe Gottfrois,Alvaro Gonzalez-Jimenez,Ludovic Amruthalingam,Alexander A. Navarini,Marc Pouly*

Main category: cs.AI

TL;DR: 临床数据集标签存在不确定性，传统聚合方法会掩盖这种差异，应使用概率指标进行机器学习评估并考虑标注不确定性，呼吁发布原始标注并采用不确定性感知评估。


<details>
  <summary>Details</summary>
Motivation: 临床数据集标签存在标注者意见不一致和置信度不均的问题，传统聚合方法会掩盖这种差异。

Method: 使用直接作用于分布的概率指标来明确考虑标注的不确定性，该指标可独立于标注生成过程应用。

Result: 在医学影像基准的简单实验中，考虑二元标签的置信度会显著影响模型排名。概率指标计算轻量。

Conclusion: 呼吁社区发布数据集的原始标注，并采用不确定性感知评估，以使性能估计更好反映临床数据。

Abstract: Clinical dataset labels are rarely certain as annotators disagree and
confidence is not uniform across cases. Typical aggregation procedures, such as
majority voting, obscure this variability. In simple experiments on medical
imaging benchmarks, accounting for the confidence in binary labels
significantly impacts model rankings. We therefore argue that machine-learning
evaluations should explicitly account for annotation uncertainty using
probabilistic metrics that directly operate on distributions. These metrics can
be applied independently of the annotations' generating process, whether
modeled by simple counting, subjective confidence ratings, or probabilistic
response models. They are also computationally lightweight, as closed-form
expressions have linear-time implementations once examples are sorted by model
score. We thus urge the community to release raw annotations for datasets and
to adopt uncertainty-aware evaluation so that performance estimates may better
reflect clinical data.

</details>


### [35] [Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase Heuristics for 2D Bin-Packing](https://arxiv.org/abs/2509.22255)
*Syed Mahbubul Huq,Daniel Brito,Daniel Sikar,Rajesh Mojumder*

Main category: cs.AI

TL;DR: 提出评估大语言模型解决二维装箱问题能力的框架，结合LLM与进化算法迭代生成和优化启发式解决方案，实验表明LLM更高效且省资源，GPT - 4o两轮迭代达最优解。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在组合优化领域解决二维装箱问题的能力。

Method: 引入结合大语言模型与进化算法的系统方法，迭代生成和优化启发式解决方案。

Result: 与传统方法相比，大语言模型能产生更高效解决方案，GPT - 4o两轮迭代达最优解，平均箱使用量从16减到15，空间利用率从0.76 - 0.78提升到0.83。

Conclusion: 有助于理解大语言模型在特定领域的评估，为组合优化任务中评估大语言模型性能建立基准。

Abstract: This paper presents an evaluation framework for assessing Large Language
Models' (LLMs) capabilities in combinatorial optimization, specifically
addressing the 2D bin-packing problem. We introduce a systematic methodology
that combines LLMs with evolutionary algorithms to generate and refine
heuristic solutions iteratively. Through comprehensive experiments comparing
LLM generated heuristics against traditional approaches (Finite First-Fit and
Hybrid First-Fit), we demonstrate that LLMs can produce more efficient
solutions while requiring fewer computational resources. Our evaluation reveals
that GPT-4o achieves optimal solutions within two iterations, reducing average
bin usage from 16 to 15 bins while improving space utilization from 0.76-0.78
to 0.83. This work contributes to understanding LLM evaluation in specialized
domains and establishes benchmarks for assessing LLM performance in
combinatorial optimization tasks.

</details>


### [36] [InfiMed-Foundation: Pioneering Advanced Multimodal Medical Models with Compute-Efficient Pre-Training and Multi-Stage Fine-Tuning](https://arxiv.org/abs/2509.22261)
*Guanghao Zhu,Zhitian Hou,Zeyu Liu,Zhijie Sang,Congkai Xie,Hongxia Yang*

Main category: cs.AI

TL;DR: 文章提出两种医疗特定的多模态大语言模型InfiMed - Foundation - 1.7B和InfiMed - Foundation - 4B，通过多种方法解决医疗应用挑战，评估显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 通用多模态大语言模型在医疗领域应用存在缺乏专业知识、知识蒸馏难、计算成本高等问题，需要开发更适用的模型。

Method: 结合通用和医疗多模态数据，提出五维质量评估框架筛选数据；采用低到高图像分辨率和多模态序列打包提高训练效率；进行三阶段监督微调以提取复杂医疗任务知识。

Result: 在MedEvalKit框架评估中，InfiMed - Foundation - 1.7B优于Qwen2.5VL - 3B，InfiMed - Foundation - 4B超越HuatuoGPT - V - 7B和MedGemma - 27B - IT。

Conclusion: 研究解决了数据质量、训练效率和特定领域知识提取的关键挑战，为医疗保健中更可靠有效的人工智能解决方案铺平道路。

Abstract: Multimodal large language models (MLLMs) have shown remarkable potential in
various domains, yet their application in the medical field is hindered by
several challenges. General-purpose MLLMs often lack the specialized knowledge
required for medical tasks, leading to uncertain or hallucinatory responses.
Knowledge distillation from advanced models struggles to capture
domain-specific expertise in radiology and pharmacology. Additionally, the
computational cost of continual pretraining with large-scale medical data poses
significant efficiency challenges. To address these issues, we propose
InfiMed-Foundation-1.7B and InfiMed-Foundation-4B, two medical-specific MLLMs
designed to deliver state-of-the-art performance in medical applications. We
combined high-quality general-purpose and medical multimodal data and proposed
a novel five-dimensional quality assessment framework to curate high-quality
multimodal medical datasets. We employ low-to-high image resolution and
multimodal sequence packing to enhance training efficiency, enabling the
integration of extensive medical data. Furthermore, a three-stage supervised
fine-tuning process ensures effective knowledge extraction for complex medical
tasks. Evaluated on the MedEvalKit framework, InfiMed-Foundation-1.7B
outperforms Qwen2.5VL-3B, while InfiMed-Foundation-4B surpasses HuatuoGPT-V-7B
and MedGemma-27B-IT, demonstrating superior performance in medical visual
question answering and diagnostic tasks. By addressing key challenges in data
quality, training efficiency, and domain-specific knowledge extraction, our
work paves the way for more reliable and effective AI-driven solutions in
healthcare. InfiMed-Foundation-4B model is available at
\href{https://huggingface.co/InfiX-ai/InfiMed-Foundation-4B}{InfiMed-Foundation-4B}.

</details>


### [37] [Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models](https://arxiv.org/abs/2509.22284)
*Aleksandar Terzić,Nicolas Menet,Michael Hersche,Thomas Hofmann,Abbas Rahimi*

Main category: cs.AI

TL;DR: 提出结构化稀疏参数化过渡矩阵方法PD - SSM，可高效跟踪FSA状态，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 现代状态空间模型（SSMs）过渡矩阵在计算效率和表达能力上存在权衡，无结构过渡矩阵虽表达能力强但计算和存储成本高。

Method: 提出PD - SSM方法，将过渡矩阵参数化为列单热矩阵P和复值对角矩阵D的乘积。

Result: 理论上模型BIBO稳定，能模拟FSA；实验上在FSA状态跟踪任务中显著优于多种SSM变体，在多类时间序列分类中表现与神经控制微分方程相当，还能集成到混合架构中跟踪复杂FSA状态。

Conclusion: PD - SSM方法有效，能在保持计算成本的同时提高模型表达能力。

Abstract: Modern state-space models (SSMs) often utilize transition matrices which
enable efficient computation but pose restrictions on the model's expressivity,
as measured in terms of the ability to emulate finite-state automata (FSA).
While unstructured transition matrices are optimal in terms of expressivity,
they come at a prohibitively high compute and memory cost even for moderate
state sizes. We propose a structured sparse parametrization of transition
matrices in SSMs that enables FSA state tracking with optimal state size and
depth, while keeping the computational cost of the recurrence comparable to
that of diagonal SSMs. Our method, PD-SSM, parametrizes the transition matrix
as the product of a column one-hot matrix ($P$) and a complex-valued diagonal
matrix ($D$). Consequently, the computational cost of parallel scans scales
linearly with the state size. Theoretically, the model is BIBO-stable and can
emulate any $N$-state FSA with one layer of dimension $N$ and a linear readout
of size $N \times N$, significantly improving on all current structured SSM
guarantees. Experimentally, the model significantly outperforms a wide
collection of modern SSM variants on various FSA state tracking tasks. On
multiclass time-series classification, the performance is comparable to that of
neural controlled differential equations, a paradigm explicitly built for
time-series analysis. Finally, we integrate PD-SSM into a hybrid
Transformer-SSM architecture and demonstrate that the model can effectively
track the states of a complex FSA in which transitions are encoded as a set of
variable-length English sentences. The code is available at
https://github.com/IBM/expressive-sparse-state-space-model

</details>


### [38] [Large Language Models as Nondeterministic Causal Models](https://arxiv.org/abs/2509.22297)
*Sander Beckers*

Main category: cs.AI

TL;DR: 本文指出现有生成大语言模型反事实的方法存在对模型解释不明确的问题，提出更简单的方法，还阐明两种方法的关系并提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有生成大语言模型反事实的方法对模型解释存在问题，为更好解释、评估和比较大语言模型的行为。

Method: 提出基于大语言模型预期解释，将其表示为非确定性因果模型来生成反事实的更简单方法。

Result: 新方法可直接应用于任何黑盒大语言模型，无需修改，且阐明了两种方法的关系。

Conclusion: 为基于大语言模型预期语义推理反事实提供理论基础，为特定应用的反事实生成方法奠定基础。

Abstract: Recent work by Chatzi et al. and Ravfogel et al. has developed, for the first
time, a method for generating counterfactuals of probabilistic Large Language
Models. Such counterfactuals tell us what would - or might - have been the
output of an LLM if some factual prompt ${\bf x}$ had been ${\bf x}^*$ instead.
The ability to generate such counterfactuals is an important necessary step
towards explaining, evaluating, and comparing, the behavior of LLMs. I argue,
however, that the existing method rests on an ambiguous interpretation of LLMs:
it does not interpret LLMs literally, for the method involves the assumption
that one can change the implementation of an LLM's sampling process without
changing the LLM itself, nor does it interpret LLMs as intended, for the method
involves explicitly representing a nondeterministic LLM as a deterministic
causal model. I here present a much simpler method for generating
counterfactuals that is based on an LLM's intended interpretation by
representing it as a nondeterministic causal model instead. The advantage of my
simpler method is that it is directly applicable to any black-box LLM without
modification, as it is agnostic to any implementation details. The advantage of
the existing method, on the other hand, is that it directly implements the
generation of a specific type of counterfactuals that is useful for certain
purposes, but not for others. I clarify how both methods relate by offering a
theoretical foundation for reasoning about counterfactuals in LLMs based on
their intended semantics, thereby laying the groundwork for novel
application-specific methods for generating counterfactuals.

</details>


### [39] [PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning](https://arxiv.org/abs/2509.22315)
*Hieu Tran,Zonghai Yao,Nguyen Luong Tran,Zhichao Yang,Feiyun Ouyang,Shuo Han,Razieh Rahimi,Hong Yu*

Main category: cs.AI

TL;DR: 受人类认知双过程理论启发，提出PRIME多智能体推理框架，实验表明其能让开源大模型在多跳和基于知识推理基准测试中与闭源模型竞争，是提升大模型复杂推理能力的可扩展方案。


<details>
  <summary>Details</summary>
Motivation: 受人类认知双过程理论启发，提升大语言模型在复杂、知识密集型推理领域的效率和准确性。

Method: 引入PRIME框架，先由快速思维智能体（系统1）生成快速答案，检测到不确定性时触发由专业智能体组成的系统2推理管道。

Result: 使用LLaMA 3模型的实验表明，PRIME使开源大语言模型在多跳和基于知识推理基准测试中能与GPT - 4等闭源模型竞争。

Conclusion: PRIME是提升大语言模型在复杂、知识密集型推理领域能力的可扩展解决方案。

Abstract: Inspired by the dual-process theory of human cognition from \textit{Thinking,
Fast and Slow}, we introduce \textbf{PRIME} (Planning and Retrieval-Integrated
Memory for Enhanced Reasoning), a multi-agent reasoning framework that
dynamically integrates \textbf{System 1} (fast, intuitive thinking) and
\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick
Thinking Agent (System 1) to generate a rapid answer; if uncertainty is
detected, it then triggers a structured System 2 reasoning pipeline composed of
specialized agents for \textit{planning}, \textit{hypothesis generation},
\textit{retrieval}, \textit{information integration}, and
\textit{decision-making}. This multi-agent design faithfully mimics human
cognitive processes and enhances both efficiency and accuracy. Experimental
results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to
perform competitively with state-of-the-art closed-source models like GPT-4 and
GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This
research establishes PRIME as a scalable solution for improving LLMs in domains
requiring complex, knowledge-intensive reasoning.

</details>


### [40] [Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents](https://arxiv.org/abs/2509.22391)
*Jiaqi Shao,Yuxiang Lin,Munish Prasad Lohani,Yufeng Miao,Bing Luo*

Main category: cs.AI

TL;DR: 提出SeekBench基准评估大语言模型搜索代理的认知能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估多关注最终答案准确性，忽略代理对外部证据的推理和行动，需评估其认知能力。

Method: 引入SeekBench基准，包含190条专家标注轨迹和超1800个响应步骤，对代理推理、搜索调整和证据评估能力进行细致分析。

Result: 构建了SeekBench基准用于评估LLM搜索代理。

Conclusion: SeekBench可用于评估LLM搜索代理的认知能力。

Abstract: Recent work has explored training Large Language Model (LLM) search agents
with reinforcement learning (RL) for open-domain question answering (QA).
However, most evaluations focus solely on final answer accuracy, overlooking
how these agents reason with and act on external evidence. We introduce
SeekBench, the first benchmark for evaluating the \textit{epistemic competence}
of LLM search agents through step-level analysis of their response traces.
SeekBench comprises 190 expert-annotated traces with over 1,800 response steps
generated by LLM search agents, each enriched with evidence annotations for
granular analysis of whether agents (1) generate reasoning steps grounded in
observed evidence, (2) adaptively reformulate searches to recover from
low-quality results, and (3) have proper calibration to correctly assess
whether the current evidence is sufficient for providing an answer.

</details>


### [41] [EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer](https://arxiv.org/abs/2509.22407)
*Zhehao Dong,Xiaofeng Wang,Zheng Zhu,Yirui Wang,Yang Wang,Yukun Zhou,Boyuan Wang,Chaojun Ni,Runqi Ouyang,Wenkang Qin,Xinze Chen,Yun Ye,Guan Huang*

Main category: cs.AI

TL;DR: 提出EMMA框架增强VLA策略，含DreamTransfer生成视频及AdaMix训练策略，实验表明生成数据可提升机器人泛化能力和任务表现。


<details>
  <summary>Details</summary>
Motivation: 收集大规模真实世界机器人操作数据耗时且昂贵，需克服此瓶颈提升VLA模型泛化能力。

Method: 提出EMMA框架，包含基于扩散Transformer的DreamTransfer生成多视图一致的操作视频，采用混合训练和AdaMix硬样本感知训练策略。

Result: DreamTransfer生成的视频在多视图一致性等方面优于先前方法，用生成数据训练的VLA让机器人能泛化到未见物体类别，在零样本视觉领域任务中比仅用真实数据训练有超200%相对性能提升，AdaMix进一步提升13%。

Conclusion: 所提方法有效提升了机器人操作策略的泛化能力。

Abstract: Vision-language-action (VLA) models increasingly rely on diverse training
data to achieve robust generalization. However, collecting large-scale
real-world robot manipulation data across varied object appearances and
environmental conditions remains prohibitively time-consuming and expensive. To
overcome this bottleneck, we propose Embodied Manipulation Media Adaptation
(EMMA), a VLA policy enhancement framework that integrates a generative data
engine with an effective training pipeline. We introduce DreamTransfer, a
diffusion Transformer-based framework for generating multi-view consistent,
geometrically grounded embodied manipulation videos. DreamTransfer enables
text-controlled visual editing of robot videos, transforming foreground,
background, and lighting conditions without compromising 3D structure or
geometrical plausibility. Furthermore, we explore hybrid training with real and
generated data, and introduce AdaMix, a hard-sample-aware training strategy
that dynamically reweights training batches to focus optimization on
perceptually or kinematically challenging samples. Extensive experiments show
that videos generated by DreamTransfer significantly outperform prior video
generation methods in multi-view consistency, geometric fidelity, and
text-conditioning accuracy. Crucially, VLAs trained with generated data enable
robots to generalize to unseen object categories and novel visual domains using
only demonstrations from a single appearance. In real-world robotic
manipulation tasks with zero-shot visual domains, our approach achieves over a
200% relative performance gain compared to training on real data alone, and
further improves by 13% with AdaMix, demonstrating its effectiveness in
boosting policy generalization.

</details>


### [42] [GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning with Auxiliary Line Construction and Affine Transformation](https://arxiv.org/abs/2509.22460)
*Shichao Weng,Zhiqiang Wang,Yuhua Zhou,Rui Lu,Ting Liu,Zhiyang Teng,Xiaozhang Liu,Hanmeng Liu*

Main category: cs.AI

TL;DR: 提出GeoSketch框架解决几何问题，经训练和评估，优于静态感知方法，推动多模态推理发展。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型解决几何问题缺乏动态操作能力，需新方法。

Method: 提出GeoSketch框架，包含感知、符号推理和草图动作模块，采用两阶段训练，引入GeoSketch基准测试。

Result: 在强MLLM基线实验中，GeoSketch显著提高逐步推理准确性和问题解决成功率。

Conclusion: GeoSketch将多模态推理从静态解释推进到动态、可验证交互，为解决复杂视觉空间问题奠定新基础。

Abstract: Geometric Problem Solving (GPS) poses a unique challenge for Multimodal Large
Language Models (MLLMs), requiring not only the joint interpretation of text
and diagrams but also iterative visuospatial reasoning. While existing
approaches process diagrams as static images, they lack the capacity for
dynamic manipulation - a core aspect of human geometric reasoning involving
auxiliary line construction and affine transformations. We present GeoSketch, a
neural-symbolic framework that recasts geometric reasoning as an interactive
perception-reasoning-action loop. GeoSketch integrates: (1) a Perception module
that abstracts diagrams into structured logic forms, (2) a Symbolic Reasoning
module that applies geometric theorems to decide the next deductive step, and
(3) a Sketch Action module that executes operations such as drawing auxiliary
lines or applying transformations, thereby updating the diagram in a closed
loop. To train this agent, we develop a two-stage pipeline: supervised
fine-tuning on 2,000 symbolic-curated trajectories followed by reinforcement
learning with dense, symbolic rewards to enhance robustness and strategic
exploration. To evaluate this paradigm, we introduce the GeoSketch Benchmark, a
high-quality set of 390 geometry problems requiring auxiliary construction or
affine transformations. Experiments on strong MLLM baselines demonstrate that
GeoSketch significantly improves stepwise reasoning accuracy and
problem-solving success over static perception methods. By unifying
hierarchical decision-making, executable visual actions, and symbolic
verification, GeoSketch advances multimodal reasoning from static
interpretation to dynamic, verifiable interaction, establishing a new
foundation for solving complex visuospatial problems.

</details>


### [43] [InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios](https://arxiv.org/abs/2509.22502)
*Chenglin Yu,Yang Yu,Songmiao Wang,Yucheng Wang,Yifan Yang,Jinjia Li,Ming Li,Hongxia Yang*

Main category: cs.AI

TL;DR: 提出可应用于无限场景的InfiAgent框架，有多项创新，提升执行效率，评估显示性能优于ADAS，案例获认可。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理开发需精心设计和迭代调优，存在手工制作局限性，阻碍其在多行业的可扩展性和成本效益。

Method: 提出Pyramid - like DAG - based Multi - Agent Framework InfiAgent，包含通用“agent - as - a - tool”机制、双审计机制、代理路由功能和代理自我进化机制，且原子任务设计支持代理并行。

Result: 在多个基准测试中，InfiAgent比ADAS性能高9.9%；AI研究助手InfiHelper生成的科学论文获IEEE顶级会议人类评审认可。

Conclusion: InfiAgent框架能演变成多功能金字塔状多智能体系统，可解决广泛问题。

Abstract: Large Language Model (LLM) agents have demonstrated remarkable capabilities
in organizing and executing complex tasks, and many such agents are now widely
used in various application scenarios. However, developing these agents
requires carefully designed workflows, carefully crafted prompts, and iterative
tuning, which requires LLM techniques and domain-specific expertise. These
hand-crafted limitations hinder the scalability and cost-effectiveness of LLM
agents across a wide range of industries. To address these challenges, we
propose \textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that
can be applied to \textbf{infi}nite scenarios, which introduces several key
innovations: a generalized "agent-as-a-tool" mechanism that automatically
decomposes complex agents into hierarchical multi-agent systems; a dual-audit
mechanism that ensures the quality and stability of task completion; an agent
routing function that enables efficient task-agent matching; and an agent
self-evolution mechanism that autonomously restructures the agent DAG based on
new tasks, poor performance, or optimization opportunities. Furthermore,
InfiAgent's atomic task design supports agent parallelism, significantly
improving execution efficiency. This framework evolves into a versatile
pyramid-like multi-agent system capable of solving a wide range of problems.
Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\%
higher performance compared to ADAS (similar auto-generated agent framework),
while a case study of the AI research assistant InfiHelper shows that it
generates scientific papers that have received recognition from human reviewers
at top-tier IEEE conferences.

</details>


### [44] [Estimating the Empowerment of Language Model Agents](https://arxiv.org/abs/2509.22504)
*Jinyeop Song,Jeff Gore,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 提出基于赋权的信息论评估方法EELMA来评估语言模型代理，验证其有效性，表明赋权是评估和监测语言模型代理的通用指标。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型代理能力增强和工具访问增多，传统以基准为中心的评估框架设计成本高，需要新的可扩展评估框架。

Method: 提出基于赋权（代理行动和未来状态的互信息）的信息论评估方法，引入EELMA算法从多轮文本交互中近似有效赋权。

Result: 赋权与平均任务性能强相关，分析了环境复杂性和代理因素对估计赋权的影响，高赋权状态和行动是通用能力的关键时刻。

Conclusion: 赋权是在复杂、开放式环境中评估和监测语言模型代理的有吸引力的通用指标。

Abstract: As language model (LM) agents become more capable and gain broader access to
real-world tools, there is a growing need for scalable evaluation frameworks of
agentic capability. However, conventional benchmark-centric evaluations are
costly to design and require human designers to come up with valid tasks that
translate into insights about general model capabilities. In this work, we
propose information-theoretic evaluation based on empowerment, the mutual
information between an agent's actions and future states, as an open-ended
method for evaluating LM agents. We introduce EELMA (Estimating Empowerment of
Language Model Agents), an algorithm for approximating effective empowerment
from multi-turn text interactions. We validate EELMA on both language games and
scaled-up realistic web-browsing scenarios. We find that empowerment strongly
correlates with average task performance, characterize the impact of
environmental complexity and agentic factors such as chain-of-thought, model
scale, and memory length on estimated empowerment, and that high empowerment
states and actions are often pivotal moments for general capabilities.
Together, these results demonstrate empowerment as an appealing general-purpose
metric for evaluating and monitoring LM agents in complex, open-ended settings.

</details>


### [45] [Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective](https://arxiv.org/abs/2509.22613)
*Siwei Wang,Yifei Shen,Haoran Sun,Shi Feng,Shang-Hua Teng,Li Dong,Yaru Hao,Wei Chen*

Main category: cs.AI

TL;DR: 研究强化学习（RL）提升大语言模型规划能力的效果，分析PG和Q - learning方法，指出SFT问题、PG的多样性坍塌，Q - learning优势及需注意奖励设计，并用Blocksworld验证。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习提升大语言模型规划能力方法缺乏理论基础，需探究其好处和局限性。

Method: 通过基于图的抽象，聚焦策略梯度（PG）和Q - learning方法进行理论分析，并应用到Blocksworld基准测试。

Result: 监督微调（SFT）会引入虚假解，RL靠探索实现正确规划；PG有多样性坍塌问题；Q - learning有离策略学习和收敛时保持多样性优势，且Q - learning需谨慎设计奖励以防奖励破解。

Conclusion: 理论分析结果在实际的Blocksworld规划基准测试中得到验证。

Abstract: Recent reinforcement learning (RL) methods have substantially enhanced the
planning capabilities of Large Language Models (LLMs), yet the theoretical
basis for their effectiveness remains elusive. In this work, we investigate
RL's benefits and limitations through a tractable graph-based abstraction,
focusing on policy gradient (PG) and Q-learning methods. Our theoretical
analyses reveal that supervised fine-tuning (SFT) may introduce
co-occurrence-based spurious solutions, whereas RL achieves correct planning
primarily through exploration, underscoring exploration's role in enabling
better generalization. However, we also show that PG suffers from diversity
collapse, where output diversity decreases during training and persists even
after perfect accuracy is attained. By contrast, Q-learning provides two key
advantages: off-policy learning and diversity preservation at convergence. We
further demonstrate that careful reward design is necessary to prevent reward
hacking in Q-learning. Finally, applying our framework to the real-world
planning benchmark Blocksworld, we confirm that these behaviors manifest in
practice.

</details>


### [46] [TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent and Explainable Digital Assessments](https://arxiv.org/abs/2509.22516)
*Rakesh Thakur,Shivaansh Kaushik,Gauri Chopra,Harsh Rohilla*

Main category: cs.AI

TL;DR: 介绍TrueGradeAI数字考试框架，克服传统纸质评估缺点，结合手写保留与可扩展透明评估。


<details>
  <summary>Details</summary>
Motivation: 克服传统纸质评估中纸张使用过多、后勤复杂、评分延迟和评估者偏见等缺点。

Method: 通过安全平板捕捉手写输入，用基于Transformer的OCR转录，通过检索增强管道评估，让大语言模型给出有证据推理的分数。

Result: 系统结合了手写保留与可扩展透明评估。

Conclusion: 该框架降低环境成本、加速反馈周期、建立知识库，减轻评分偏见，确保评估公平。

Abstract: This paper introduces TrueGradeAI, an AI-driven digital examination framework
designed to overcome the shortcomings of traditional paper-based assessments,
including excessive paper usage, logistical complexity, grading delays, and
evaluator bias. The system preserves natural handwriting by capturing stylus
input on secure tablets and applying transformer-based optical character
recognition for transcription. Evaluation is conducted through a
retrieval-augmented pipeline that integrates faculty solutions, cache layers,
and external references, enabling a large language model to assign scores with
explicit, evidence-linked reasoning. Unlike prior tablet-based exam systems
that primarily digitize responses, TrueGradeAI advances the field by
incorporating explainable automation, bias mitigation, and auditable grading
trails. By uniting handwriting preservation with scalable and transparent
evaluation, the framework reduces environmental costs, accelerates feedback
cycles, and progressively builds a reusable knowledge base, while actively
working to mitigate grading bias and ensure fairness in assessment.

</details>


### [47] [REMA: A Unified Reasoning Manifold Framework for Interpreting Large Language Model](https://arxiv.org/abs/2509.22518)
*Bo Li,Guanzhi Deng,Ronghao Chen,Junrong Yue,Shuo Zhang,Qinghua Zhao,Linqi Song,Lijie Wen*

Main category: cs.AI

TL;DR: 本文提出推理流形概念和REMA框架分析大语言模型推理失败原因，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型复杂推理过程及失败机制是可解释性研究挑战，需可量化几何分析视角。

Method: 定义推理流形概念，构建REMA框架，通过计算错误表示与正确表示流形的几何偏差量化失败信号，定位偏差起点。

Result: 实验表明推理流形是低维的，错误与正确推理表示可分性高，验证REMA框架有效性。

Conclusion: 研究将抽象推理失败与表示的几何偏差联系，为理解黑盒模型计算过程提供新途径。

Abstract: Understanding how Large Language Models (LLMs) perform complex reasoning and
their failure mechanisms is a challenge in interpretability research. To
provide a measurable geometric analysis perspective, we define the concept of
the Reasoning Manifold, a latent low-dimensional geometric structure formed by
the internal representations corresponding to all correctly reasoned
generations. This structure can be conceptualized as the embodiment of the
effective thinking paths that the model has learned to successfully solve a
given task. Based on this concept, we build REMA, a framework that explains the
origins of failures by quantitatively comparing the spatial relationships of
internal model representations corresponding to both erroneous and correct
reasoning samples. Specifically, REMA first quantifies the geometric deviation
of each erroneous representation by calculating its k-nearest neighbors
distance to the approximated manifold formed by correct representations,
thereby providing a unified failure signal. It then localizes the divergence
points where these deviations first become significant by tracking this
deviation metric across the model's layers and comparing it against a baseline
of internal fluctuations from correct representations, thus identifying where
the reasoning chain begins to go off-track. Our extensive experiments on
diverse language and multimodal models and tasks demonstrate the
low-dimensional nature of the reasoning manifold and the high separability
between erroneous and correct reasoning representations. The results also
validate the effectiveness of the REMA framework in analyzing the origins of
reasoning failures. This research connects abstract reasoning failures to
measurable geometric deviations in representations, providing new avenues for
in-depth understanding and diagnosis of the internal computational processes of
black-box models.

</details>


### [48] [The Emergence of Altruism in Large-Language-Model Agents Society](https://arxiv.org/abs/2509.22537)
*Haoyang Li,Xiao Jia,Zhanzhan Zhao*

Main category: cs.AI

TL;DR: 本文利用Schelling变体城市迁移模型研究LLM在大规模社会中的利他行为，发现LLM有'Adaptive Egoists'和'Altruistic Optimizers'两种类型，为社会模拟的模型选择提供依据。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注小规模任务导向游戏中的合作，忽略大规模代理社会中利他行为的出现，本文旨在填补这一空白。

Method: 引入Schelling变体城市迁移模型制造社会困境，让超200个LLM代理在利己和利他目标间抉择；引入受扎根理论启发的方法对代理推理进行系统编码。

Result: 发现LLM社会倾向存在根本差异，识别出'Adaptive Egoists'和'Altruistic Optimizers'两种类型。

Conclusion: 不同LLM在利己和利他倾向方面存在内在异质性，社会模拟选模型时要考虑内在社会行动逻辑，'Adaptive Egoists'适合模拟复杂人类社会，'Altruistic Optimizers'适合建模理想化亲社会行为者或集体福利优先的场景。

Abstract: Leveraging Large Language Models (LLMs) for social simulation is a frontier
in computational social science. Understanding the social logics these agents
embody is critical to this attempt. However, existing research has primarily
focused on cooperation in small-scale, task-oriented games, overlooking how
altruism, which means sacrificing self-interest for collective benefit, emerges
in large-scale agent societies. To address this gap, we introduce a
Schelling-variant urban migration model that creates a social dilemma,
compelling over 200 LLM agents to navigate an explicit conflict between
egoistic (personal utility) and altruistic (system utility) goals. Our central
finding is a fundamental difference in the social tendencies of LLMs. We
identify two distinct archetypes: "Adaptive Egoists", which default to
prioritizing self-interest but whose altruistic behaviors significantly
increase under the influence of a social norm-setting message board; and
"Altruistic Optimizers", which exhibit an inherent altruistic logic,
consistently prioritizing collective benefit even at a direct cost to
themselves. Furthermore, to qualitatively analyze the cognitive underpinnings
of these decisions, we introduce a method inspired by Grounded Theory to
systematically code agent reasoning. In summary, this research provides the
first evidence of intrinsic heterogeneity in the egoistic and altruistic
tendencies of different LLMs. We propose that for social simulation, model
selection is not merely a matter of choosing reasoning capability, but of
choosing an intrinsic social action logic. While "Adaptive Egoists" may offer a
more suitable choice for simulating complex human societies, "Altruistic
Optimizers" are better suited for modeling idealized pro-social actors or
scenarios where collective welfare is the primary consideration.

</details>


### [49] [StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models](https://arxiv.org/abs/2509.22558)
*Chenyu Zhou,Tianyi Xu,Jianghao Lin,Dongdong Ge*

Main category: cs.AI

TL;DR: 提出新框架StepORLM解决大语言模型解决运筹学问题时现有方法的局限，在多个基准测试中达最优。


<details>
  <summary>Details</summary>
Motivation: 现有利用强化学习在LLM上训练解决运筹学问题的工作存在结果奖励的信用分配问题和传统判别式过程监督缺乏全局评估的局限。

Method: 引入具有生成式过程监督的自进化框架StepORLM，通过策略模型和生成式过程奖励模型的协同进化循环，结合外部求解器的结果验证和GenPRM的过程评估，用W - DPO调整策略并优化GenPRM。

Result: 8B参数的StepORLM在六个基准测试中建立新的最优，共同进化的GenPRM可作为强大通用的过程验证器，提升推理扩展性能。

Conclusion: StepORLM有效解决现有方法局限，在解决运筹学问题上表现优异，GenPRM有广泛应用价值。

Abstract: Large Language Models (LLMs) have shown promising capabilities for solving
Operations Research (OR) problems. While reinforcement learning serves as a
powerful paradigm for LLM training on OR problems, existing works generally
face two key limitations. First, outcome reward suffers from the credit
assignment problem, where correct final answers can reinforce flawed reasoning.
Second, conventional discriminative process supervision is myopic, failing to
evaluate the interdependent steps of OR modeling holistically. To this end, we
introduce StepORLM, a novel self-evolving framework with generative process
supervision. At its core, StepORLM features a co-evolutionary loop where a
policy model and a generative process reward model (GenPRM) iteratively improve
on each other. This loop is driven by a dual-feedback mechanism: definitive,
outcome-based verification from an external solver, and nuanced, holistic
process evaluation from the GenPRM. The combined signal is used to align the
policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously
refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new
state-of-the-art across six benchmarks, significantly outperforming vastly
larger generalist models, agentic methods, and specialized baselines. Moreover,
the co-evolved GenPRM is able to act as a powerful and universally applicable
process verifier, substantially boosting the inference scaling performance of
both our own model and other existing LLMs.

</details>


### [50] [UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration](https://arxiv.org/abs/2509.22570)
*Qi Mao,Tinghan Yang,Jiahao Li,Bin Li,Libiao Jin,Yan Lu*

Main category: cs.AI

TL;DR: 提出UniMIC框架解决现有编解码器在多模态交互通信中的局限，实验显示其能节省比特率且不影响下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有编解码器针对单模态单向通信优化，在多模态双向交互中存在重复降级问题。

Method: 提出UniMIC框架，采用紧凑的标记化表示作为通信介质，使用轻量级基于Transformer的熵模型减少标记间冗余。

Result: 在多项实验中，UniMIC实现显著比特率节省，在超低比特率下仍保持鲁棒性，不影响下游任务性能。

Conclusion: UniMIC是下一代多模态交互通信实用且有前瞻性的范式。

Abstract: The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI
agents is transforming human-AI collaboration into bidirectional, multimodal
interaction. However, existing codecs remain optimized for unimodal, one-way
communication, resulting in repeated degradation under conventional
compress-transmit-reconstruct pipelines. To address this limitation, we propose
UniMIC, a Unified token-based Multimodal Interactive Coding framework that
bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or
plain text, UniMIC employs compact tokenized representations as the
communication medium, enabling efficient low-bitrate transmission while
maintaining compatibility with LMMs. To further enhance compression,
lightweight Transformer-based entropy models with scenario-specific
designs-generic, masked, and text-conditioned-effectively minimize inter-token
redundancy. Extensive experiments on text-to-image generation, text-guided
inpainting, outpainting, and visual question answering show that UniMIC
achieves substantial bitrate savings and remains robust even at ultra-low
bitrates (<0.05bpp), without compromising downstream task performance. These
results establish UniMIC as a practical and forward-looking paradigm for
next-generation multimodal interactive communication.

</details>


### [51] [Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time](https://arxiv.org/abs/2509.22572)
*Yixuan Han,Fan Ma,Ruijie Quan,Yi Yang*

Main category: cs.AI

TL;DR: 提出Dynamic Experts Search (DES)策略提升大语言模型推理能力，实验表明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有Test - Time Scaling (TTS)方法主要依赖输出级采样，忽略模型架构，在主流Mixture - of - Experts (MoE) LLMs中发现激活专家数量不同会产生互补解集，有新的多样性来源。

Method: 提出DES策略，包含Dynamic MoE（推理时直接控制专家数量以生成多样推理轨迹）和Expert Configuration Inheritance（在推理路径内保持一致专家数量，不同运行中改变以平衡稳定性和多样性）。

Result: 在多种MoE架构、验证器和推理基准测试中，DES可靠地优于TTS基线，在无额外成本下提高了准确性和稳定性。

Conclusion: DES是一种实用且可扩展的架构感知TTS形式，展示了现代LLMs的结构灵活性可推动推理能力发展。

Abstract: Test-Time Scaling (TTS) enhances the reasoning ability of large language
models (LLMs) by allocating additional computation during inference. However,
existing approaches primarily rely on output-level sampling while overlooking
the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we
observe that varying the number of activated experts yields complementary
solution sets with stable accuracy, revealing a new and underexplored source of
diversity. Motivated by this observation, we propose Dynamic Experts Search
(DES), a TTS strategy that elevates expert activation into a controllable
dimension of the search space. DES integrates two key components: (1) Dynamic
MoE, which enables direct control of expert counts during inference to generate
diverse reasoning trajectories without additional cost; and (2) Expert
Configuration Inheritance, which preserves consistent expert counts within a
reasoning path while varying them across runs, thereby balancing stability and
diversity throughout the search. Extensive experiments across MoE
architectures, verifiers and reasoning benchmarks (i.e., math, code and
knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing
accuracy and stability without additional cost. These results highlight DES as
a practical and scalable form of architecture-aware TTS, illustrating how
structural flexibility in modern LLMs can advance reasoning.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [52] [Sci2Pol: Evaluating and Fine-tuning LLMs on Scientific-to-Policy Brief Generation](https://arxiv.org/abs/2509.21493)
*Weimin Wu,Alexander C. Furnas,Eddie Yang,Gefei Liu,Akhil Pandey Akella,Xuefeng Song,Dashun Wang,Han Liu*

Main category: cs.CE

TL;DR: 提出Sci2Pol - Bench和Sci2Pol - Corpus用于评估和微调大语言模型的政策简报生成能力，评估模型并微调后提升了性能。


<details>
  <summary>Details</summary>
Motivation: 评估和微调大语言模型在从科学论文生成政策简报方面的能力，解决现有评估指标不足问题，提升模型表现。

Method: 构建基于五阶段分类法的Sci2Pol - Bench，含18个任务；引入新的基于大语言模型的评估指标；通过关联科学论文和政策文件构建Sci2Pol - Corpus，经筛选和润色后微调模型。

Result: 评估发现现有模型的关键局限；微调后的模型在Sci2Pol - Bench上性能提升，Gemma - 27B超越GPT - 4o和DeepSeek - V3。

Conclusion: 所构建的语料库能有效缩小科学与政策之间的差距，提升模型在政策简报生成方面的表现。

Abstract: We propose Sci2Pol-Bench and Sci2Pol-Corpus, the first benchmark and training
dataset for evaluating and fine-tuning large language models (LLMs) on policy
brief generation from a scientific paper. We build Sci2Pol-Bench on a
five-stage taxonomy to mirror the human writing process: (i) Autocompletion,
(ii) Understanding, (iii) Summarization, (iv) Generation, and (v) Verification.
It features 18 tasks in multiple-choice and open-ended formats. Specifically,
for the Generation stage, we show that BERTScore and ROUGE scores fail to
capture the quality of brief writing, and introduce a new LLM-based evaluation
metric aligned with expert judgement. Using this benchmark, we evaluate 13
leading open-source and commercial LLMs to uncover key limitations. To improve
LLM performance on brief writing, we curate the Sci2Pol-Corpus for fine-tuning.
We start by linking each cited scientific paper to its corresponding policy
document, drawn from 5.6 million policy records. This produces 140,000
candidate pairs. We then employ an LLM-as-a-judge to filter high-quality
examples, followed by in-context polishing using three expert-written samples
as references. This process yields a final set of 639 new pairs. Finally, we
fine-tune three models on Sci2Pol-Corpus: LLaMA-3.1-8B, Gemma-12B, and
Gemma-27B. Fine-tuning leads to consistent performance improvements across
Sci2Pol-Bench. Notably, after fine-tuning, Gemma-27B surpasses the much larger
GPT-4o and DeepSeek-V3 (671B). These demonstrate the effectiveness of our
corpus in bridging the gap between science and policy.

</details>


### [53] [QuantMind: A Context-Engineering Based Knowledge Framework for Quantitative Finance](https://arxiv.org/abs/2509.21507)
*Haoxue Wang,Keli Wen,Yuante Li,Qiancheng Qu,Xiangxu Mu,Xinjie Shen,Jiaqi Gao,Chenyang Chang,Chuhan Xie,San Yu Cheung,Zhuoyuan Hu,Xinyu Wang,Sirui Bi,Bi'an Du*

Main category: cs.CE

TL;DR: 提出适用于量化金融的智能知识提取和检索框架QuantMind，经用户研究证明其优于传统方式。


<details>
  <summary>Details</summary>
Motivation: 现有LLM和RAG管道在处理非结构化金融内容时存在时间点正确性、证据归因和集成到研究工作流程等问题。

Method: 采用两阶段架构，知识提取阶段通过多模态解析、自适应总结和特定领域标记将文档转化为结构化知识；智能检索阶段集成语义搜索、多跳推理和知识感知生成。

Result: 通过用户研究表明，QuantMind在事实准确性和用户体验方面优于无辅助阅读和通用AI辅助。

Conclusion: 强调了结构化、特定领域上下文工程在金融领域的价值。

Abstract: Quantitative research increasingly relies on unstructured financial content
such as filings, earnings calls, and research notes, yet existing LLM and RAG
pipelines struggle with point-in-time correctness, evidence attribution, and
integration into research workflows. To tackle this, We present QuantMind, an
intelligent knowledge extraction and retrieval framework tailored to
quantitative finance. QuantMind adopts a two-stage architecture: (i) a
knowledge extraction stage that transforms heterogeneous documents into
structured knowledge through multi-modal parsing of text, tables, and formulas,
adaptive summarization for scalability, and domain-specific tagging for
fine-grained indexing; and (ii) an intelligent retrieval stage that integrates
semantic search with flexible strategies, multi-hop reasoning across sources,
and knowledge-aware generation for auditable outputs. A controlled user study
demonstrates that QuantMind improves both factual accuracy and user experience
compared to unaided reading and generic AI assistance, underscoring the value
of structured, domain-specific context engineering for finance.

</details>


### [54] [AI for Sustainable Future Foods](https://arxiv.org/abs/2509.21556)
*Bianca Datta,Markus J. Buehler,Yvonne Chow,Kristina Gligoric,Dan Jurafsky,David L. Kaplan,Rodrigo Ledesma-Amaro,Giorgia Del Missier,Lisa Neidhardt,Karim Pichara,Benjamin Sanchez-Lengeling,Miek Schlangen,Skyler R. St. Pierre,Ilias Tagkopoulos,Anna Thomas,Nicholas J. Watson,Ellen Kuhl*

Main category: cs.CE

TL;DR: 本文介绍新兴的食品人工智能学科，阐述其作用、面临挑战并提出发展优先事项，以推动可持续食品创新。


<details>
  <summary>Details</summary>
Motivation: 全球食品系统需提供营养可持续食品并减少环境影响，但食品创新缓慢、零散，人工智能可带来变革。

Method: 概述食品人工智能这一新兴学科，提出将食品视为可编程生物材料、建立自动驾驶实验室、开发融合可持续性和人类健康的深度推理模型三个优先事项。

Result: 早期成果显示人工智能可预测蛋白质性能、映射分子与风味、定制消费体验，但面临缺乏标准化、多模态数据稀缺等挑战。

Conclusion: 将人工智能合理融入食品创新周期，可加速向可持续蛋白质系统过渡，推动食品科学发展。

Abstract: Global food systems must deliver nutritious and sustainable foods while
sharply reducing environmental impact. Yet, food innovation remains slow,
empirical, and fragmented. Artificial intelligence (AI) now offers a
transformative path with the potential to link molecular composition to
functional performance, bridge chemical structure to sensory outcomes, and
accelerate cross-disciplinary innovation across the entire production pipeline.
Here we outline AI for Food as an emerging discipline that integrates
ingredient design, formulation development, fermentation and production,
texture analysis, sensory properties, manufacturing, and recipe generation.
Early successes demonstrate how AI can predict protein performance, map
molecules to flavor, and tailor consumer experiences. But significant
challenges remain: lack of standardization, scarce multimodal data, cultural
and nutritional diversity, and low consumer confidence. We propose three
priorities to unlock the field: treating food as a programmable biomaterial,
building self-driving laboratories for automated discovery, and developing deep
reasoning models that integrate sustainability and human health. By embedding
AI responsibly into the food innovation cycle, we can accelerate the transition
to sustainable protein systems and chart a predictive, design-driven science of
food for our own health and the health of our planet.

</details>


### [55] [Hybrid Method of Moments and Generalized Scattering Matrix: Applications to Antennas in Radomes, Reflectors, and Implantable Media](https://arxiv.org/abs/2509.22000)
*Chenbo Shi,Shichen Liang,Xin Gu,Jin Pan,Le Zuo*

Main category: cs.CE

TL;DR: 提出混合矩量法和广义散射矩阵框架解决天线电磁分析的多尺度挑战，具准确性、效率和适应性，经数值验证计算成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 解决嵌入或与大型结构相互作用的天线电磁分析中的多尺度挑战。

Method: 提出混合矩量法（MoM）和广义散射矩阵（GSM）框架，实现细尺度和大尺度复杂度分离并保留全互耦，有GSM - PO和GSM + T矩阵扩展。

Result: 数值验证在可植入天线、受天线罩保护的阵列和反射器系统上与全波求解器结果吻合，设计和优化的计算成本显著降低。

Conclusion: 该框架是多尺度天线建模的统一范式，结合了准确性、效率和适应性。

Abstract: Electromagnetic analysis of antennas embedded in or interacting with large
surrounding structures poses inherent multiscale challenges: the antenna is
electrically small yet geometrically detailed, while the environment is
electrically large but comparatively smooth. To address this, we present a
hybrid method of moments (MoM) and generalized scattering matrix (GSM)
framework that achieves a clean separation between fine-scale and large-scale
complexities while preserving their full mutual coupling. Antennas of arbitrary
geometry can be characterized once and reused across different environments, or
conversely, a given environment can be modeled once to accommodate multiple
antenna designs. The framework is inherently versatile, encompassing GSM-PO and
GSM + T-matrix extensions, and thus provides a unified paradigm for multiscale
antenna modeling. With the large body always represented by the formulation
best suited to its scale and shape, the approach combines accuracy, efficiency,
and adaptability. Numerical validations on implantable antennas,
radome-protected arrays, and reflector systems confirm excellent agreement with
full-wave solvers while demonstrating dramatic reductions in computational cost
for design and optimization.

</details>


### [56] [Orochi: Versatile Biomedical Image Processor](https://arxiv.org/abs/2509.22583)
*Gaole Dai,Chenghao Zhou,Yu Zhou,Rongyu Zhang,Yuan Zhang,Chengkai Hou,Tiejun Huang,Jianxu Chen,Shanghang Zhang*

Main category: cs.CE

TL;DR: 介绍了图像处理器Orochi，它预训练于多公开研究数据，采用新预训练策略和高效架构，有三种微调框架，性能佳，有望助力一体化工作流。


<details>
  <summary>Details</summary>
Motivation: 现有基于特定任务和数据集的生物医学图像模型插件对生物学家实用性低，需开发新的处理器解决此问题。

Method: 使用随机多尺度采样策略在超100个公开研究的原始数据上预训练Orochi；提出任务相关联合嵌入预训练（TJP）；利用Mamba线性计算复杂度构建多头分层Mamba；提供三层微调框架。

Result: Orochi在轻量级参数高效选项下，能达到与当前最先进的专业模型相当或更优的性能。

Conclusion: 研究有望推动一体化工作流发展，减轻生物学家选择模型的负担。

Abstract: Deep learning has emerged as a pivotal tool for accelerating research in the
life sciences, with the low-level processing of biomedical images (e.g.,
registration, fusion, restoration, super-resolution) being one of its most
critical applications. Platforms such as ImageJ (Fiji) and napari have enabled
the development of customized plugins for various models. However, these
plugins are typically based on models that are limited to specific tasks and
datasets, making them less practical for biologists. To address this challenge,
we introduce Orochi, the first application-oriented, efficient, and versatile
image processor designed to overcome these limitations. Orochi is pre-trained
on patches/volumes extracted from the raw data of over 100 publicly available
studies using our Random Multi-scale Sampling strategy. We further propose
Task-related Joint-embedding Pre-Training (TJP), which employs biomedical
task-related degradation for self-supervision rather than relying on Masked
Image Modelling (MIM), which performs poorly in downstream tasks such as
registration. To ensure computational efficiency, we leverage Mamba's linear
computational complexity and construct Multi-head Hierarchy Mamba.
Additionally, we provide a three-tier fine-tuning framework (Full, Normal, and
Light) and demonstrate that Orochi achieves comparable or superior performance
to current state-of-the-art specialist models, even with lightweight
parameter-efficient options. We hope that our study contributes to the
development of an all-in-one workflow, thereby relieving biologists from the
overwhelming task of selecting among numerous models.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [57] [QueryGym: Step-by-Step Interaction with Relational Databases](https://arxiv.org/abs/2509.21674)
*Haritha Ananthakrishanan,Harsha Kokel,Kelsey Sikes,Debarun Bhattacharjya,Michael Katz,Shirin Sohrabi,Kavitha Srinivas*

Main category: cs.DB

TL;DR: 介绍QueryGym，一个用于构建、测试和评估基于大语言模型的查询规划代理的交互式环境，可进行引擎无关评估和透明规划，还进行了演示展示其效用。


<details>
  <summary>Details</summary>
Motivation: 现有框架常使代理与特定查询语言方言绑定或模糊其推理过程，需要一个更好的评估环境。

Method: 将环境实现为Gymnasium接口，提供观察信息并接收代表数据库探索和关系代数操作的动作。

Result: 在演示中对比当代查询数据库的大语言模型，展示了环境的效用。

Conclusion: QueryGym可作为查询生成的错误修复、透明度和强化学习研究的实用测试平台。

Abstract: We introduce QueryGym, an interactive environment for building, testing, and
evaluating LLM-based query planning agents. Existing frameworks often tie
agents to specific query language dialects or obscure their reasoning; QueryGym
instead requires agents to construct explicit sequences of relational algebra
operations, ensuring engine-agnostic evaluation and transparent step-by-step
planning. The environment is implemented as a Gymnasium interface that supplies
observations -- including schema details, intermediate results, and execution
feedback -- and receives actions that represent database exploration (e.g.,
previewing tables, sampling column values, retrieving unique values) as well as
relational algebra operations (e.g., filter, project, join). We detail the
motivation and the design of the environment. In the demo, we showcase the
utility of the environment by contrasting it with contemporary LLMs that query
databases. QueryGym serves as a practical testbed for research in error
remediation, transparency, and reinforcement learning for query generation. For
the associated demo, see https://ibm.biz/QueryGym.

</details>


### [58] [Unbiased Binning: Fairness-aware Attribute Representation](https://arxiv.org/abs/2509.21785)
*Abolfazl Asudeh,Zeinab,Asoodeh,Bita Asoodeh,Omid Asudeh*

Main category: cs.DB

TL;DR: 本文提出无偏分箱和ε-偏置分箱问题，并分别设计动态规划算法和基于局部搜索的可扩展算法解决问题。


<details>
  <summary>Details</summary>
Motivation: 原始特征离散化会导致数据偏差和下游任务不公平，需解决该问题。

Method: 定义边界候选集解决无偏分箱问题；开发动态规划算法解决ε-偏置分箱问题，提出基于局部搜索和分治算法的可扩展算法。

Result: 证明无偏分箱边界选择范围；动态规划算法二次时间内找到最优分箱；分治算法近线性时间找到近似最优解。

Conclusion: 提出的算法能有效解决无偏分箱和ε-偏置分箱问题。

Abstract: Discretizing raw features into bucketized attribute representations is a
popular step before sharing a dataset. It is, however, evident that this step
can cause significant bias in data and amplify unfairness in downstream tasks.
  In this paper, we address this issue by introducing the unbiased binning
problem that, given an attribute to bucketize, finds its closest discretization
to equal-size binning that satisfies group parity across different buckets.
Defining a small set of boundary candidates, we prove that unbiased binning
must select its boundaries from this set. We then develop an efficient dynamic
programming algorithm on top of the boundary candidates to solve the unbiased
binning problem.
  Finding an unbiased binning may sometimes result in a high price of fairness,
or it may not even exist, especially when group values follow different
distributions. Considering that a small bias in the group ratios may be
tolerable in such settings, we introduce the epsilon-biased binning problem
that bounds the group disparities across buckets to a small value epsilon. We
first develop a dynamic programming solution, DP, that finds the optimal
binning in quadratic time. The DP algorithm, while polynomial, does not scale
to very large settings. Therefore, we propose a practically scalable algorithm,
based on local search (LS), for epsilon-biased binning. The key component of
the LS algorithm is a divide-and-conquer (D&C) algorithm that finds a
near-optimal solution for the problem in near-linear time. We prove that D&C
finds a valid solution for the problem unless none exists. The LS algorithm
then initiates a local search, using the D&C solution as the upper bound, to
find the optimal solution.

</details>


### [59] [The system of processing and analysis of customer tracking data for customer journey research on the base of RFID technology](https://arxiv.org/abs/2509.22162)
*Marina Kholod*

Main category: cs.DB

TL;DR: 文章研究基于RFID技术的追踪数据处理分析系统，探讨其在零售的应用、数据处理架构和数据库模型，用BSC分析效益，认为整合数据能让零售更精准。


<details>
  <summary>Details</summary>
Motivation: 研究基于RFID技术的系统来分析追踪数据，以研究零售中的客户旅程，提升零售业务效益。

Method: 研究RFID技术发展、原理与应用，采用ETL方法处理数据，提出逻辑数据库模型，用平衡计分卡（BSC）分析业务效益。

Result: 设计了逻辑数据库模型，分析得出RFID实施的预期业务效益。

Conclusion: 追踪和交易数据的整合为使零售成为精确的数据驱动科学奠定基础，可洞察产品流和消费者行为。

Abstract: The article focuses on researching a system for processing and analyzing
tracking data based on RFID technology to study the customer journey in retail.
It examines the evolution of RFID technology, its key operating principles, and
modern applications in retail that extend beyond logistics to include precise
inventory management, loss prevention, and customer experience improvement.
Particular attention is paid to the architecture for data collection,
processing, and integration, specifically the ETL (extract, transform, load)
methodology for transforming raw RFID and POS data into a structured analytical
data warehouse. A detailed logical database model is proposed, designed for
comprehensive analysis that combines financial sales metrics with behavioral
patterns of customer movement. The article also analyzes the expected business
benefits of RFID implementation through the lens of the Balanced Scorecard
(BSC), which evaluates financial performance, customer satisfaction, and
internal process optimization. It is concluded that the integration of tracking
and transactional data creates a foundation for transforming retail into a
precise, data-driven science, providing unprecedented visibility into physical
product flows and consumer behavior.

</details>


### [60] [I-ETL: an interoperability-aware health (meta) data pipeline to enable federated analyses](https://arxiv.org/abs/2509.22351)
*Nelly Barret,Anna Bernasconi,Boris Bikbov,Pietro Pinoli*

Main category: cs.DB

TL;DR: 本文设计实现I - ETL框架整合医院异构医疗数据集，通过实验证明其能统一表示健康数据集并确保不同中心协作。


<details>
  <summary>Details</summary>
Motivation: 临床医生需交换数据理解复杂疾病，现有去中心化架构和联邦学习算法在数据收集设置和模型上有局限，医疗数据异构且缺乏归一化手段，需要合作利用信息。

Method: 设计实现I - ETL框架，包括设计两个通用可扩展概念模型来建模数据和元数据，提出ETL管道确保和评估互操作性。

Result: 通过对开源数据集实验，I - ETL借助两个通用概念模型成功统一表示各种健康数据集。

Conclusion: 在集成管道中融入互操作性很重要，能确保不同中心间的协作。

Abstract: Clinicians are interested in better understanding complex diseases, such as
cancer or rare diseases, so they need to produce and exchange data to mutualize
sources and join forces. To do so and ensure privacy, a natural way consists in
using a decentralized architecture and Federated Learning algorithms. This
ensures that data stays in the organization in which it has been collected, but
requires data to be collected in similar settings and similar models. In
practice, this is often not the case because healthcare institutions work
individually with different representations and raw data; they do not have
means to normalize their data, and even less to do so across centers. For
instance, clinicians have at hand phenotypic, clinical, imaging and genomic
data (each individually collected) and want to better understand some diseases
by analyzing them together. This example highlights the needs and challenges
for a cooperative use of this wealth of information. We designed and
implemented a framework, named I-ETL, for integrating highly heterogeneous
healthcare datasets of hospitals in interoperable databases. Our proposal is
twofold: (i) we devise two general and extensible conceptual models for
modeling both data and metadata and (ii) we propose an Extract-Transform-Load
(ETL) pipeline ensuring and assessing interoperability from the start. By
conducting experiments on open-source datasets, we show that I-ETL succeeds in
representing various health datasets in a unified way thanks to our two general
conceptual models. Next, we demonstrate the importance of blending
interoperability as a first-class citizen in integration pipelines, ensuring
possible collaboration between different centers.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [61] [Redesigning GROMACS Halo Exchange: Improving Strong Scaling with GPU-initiated NVSHMEM](https://arxiv.org/abs/2509.21527)
*Mahesh Doijade,Andrey Alekseenko,Ania Brown,Alan Gray,Szilárd Páll*

Main category: cs.DC

TL;DR: 提出基于NVSHMEM的GROMACS领域分解光环交换算法的GPU内核启动重新设计，提升GROMACS强缩放性能。


<details>
  <summary>Details</summary>
Motivation: GROMACS对延迟敏感，在异构超级计算机上可扩展性差，MPI以CPU为中心的特性阻碍GPU利用率和可扩展性。

Method: 采用基于NVSHMEM的GPU内核启动重新设计算法，融合数据打包和通信，利用硬件隐藏延迟，进行内核融合，使用异步复制引擎优化。

Result: 提高了GROMACS强缩放性能，跨NVLink单节点提升达1.5倍、多节点提升达2倍，在NVLink+InfiniBand上多节点提升达1.3倍。

Conclusion: GPU启动通信对大量延迟敏感应用的强缩放有显著益处。

Abstract: Improving time-to-solution in molecular dynamics simulations often requires
strong scaling due to fixed-sized problems. GROMACS is highly
latency-sensitive, with peak iteration rates in the sub-millisecond, making
scalability on heterogeneous supercomputers challenging. MPI's CPU-centric
nature introduces additional latencies on GPU-resident applications' critical
path, hindering GPU utilization and scalability. To address these limitations,
we present an NVSHMEM-based GPU kernel-initiated redesign of the GROMACS domain
decomposition halo-exchange algorithm. Highly tuned GPU kernels fuse data
packing and communication, leveraging hardware latency-hiding for fine-grained
overlap. We employ kernel fusion across overlapped data forwarding
communication phases and utilize the asynchronous copy engine over NVLink to
optimize latency and bandwidth. Our GPU-resident formulation greatly increases
communication-computation overlap, improving GROMACS strong scaling performance
across NVLink by up to 1.5x (intra-node) and 2x (multi-node), and up to 1.3x
multi-node over NVLink+InfiniBand. This demonstrates the profound benefits of
GPU-initiated communication for strong-scaling a broad range of
latency-sensitive applications.

</details>


### [62] [Zeppelin: Balancing Variable-length Workloads in Data Parallel Large Model Training](https://arxiv.org/abs/2509.21841)
*Chang Chen,Tiancheng Chen,Jiangfei Duan,Qianchao Zhu,Zerui Wang,Qinghao Hu,Peng Sun,Xiuhong Li,Chao Yang,Torsten Hoefler*

Main category: cs.DC

TL;DR: 现有大语言模型长序列训练框架性能不佳，本文提出Zeppelin系统，综合三项技术，比现有方法平均提速2.80倍。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型长序列训练框架忽视计算和通信成本随序列长度的变化，导致性能不佳，需要解决负载不平衡等问题。

Method: 提出Zeppelin系统，集成分层序列分区方法、路由层和重映射层三项关键技术。

Result: 综合评估显示，Zeppelin比现有技术平均提速2.80倍。

Conclusion: Zeppelin系统能有效解决大语言模型长序列训练中的负载不平衡问题，提升训练效率。

Abstract: Training large language models (LLMs) with increasingly long and varying
sequence lengths introduces severe load imbalance challenges in large-scale
data-parallel training. Recent frameworks attempt to mitigate these issues
through data reorganization or hybrid parallel strategies. However, they often
overlook how computational and communication costs scale with sequence length,
resulting in suboptimal performance. We identify three critical challenges: (1)
varying computation-to-communication ratios across sequences of different
lengths in distributed attention, (2) mismatch between static NIC-GPU affinity
and dynamic parallel workloads, and (3) distinct optimal partitioning
strategies required for quadratic attention versus linear components. To
address these challenges, we present Zeppelin, a novel training system that
integrates three key techniques: (1) a hierarchical sequence partitioning
method for the attention module that reduces communication overhead and
balances computation, supported by an efficient attention engine that applies
divergent parallel strategies; (2) a routing layer that orchestrates inter-node
transfers to fully utilize NIC bandwidth; and (3) a remapping layer that
transforms sequence layouts between attention and linear modules, ensuring high
computational efficiency across both. Comprehensive evaluations across diverse
configurations show that Zeppelin delivers an average 2.80x speedup over
state-of-the-art methods.

</details>


### [63] [Code once, Run Green: Automated Green Code Translation in Serverless Computing](https://arxiv.org/abs/2509.22068)
*Sebastian Werner,Mathis Kähler,Alireza Hakamian*

Main category: cs.DC

TL;DR: 本文研究无服务器计算平台利用大语言模型（LLMs）翻译函数代码以减少能源债务的潜力，初步结果显示可减少高达70%的调用能耗，但面临一些挑战。


<details>
  <summary>Details</summary>
Motivation: 现有缓解计算基础设施排放影响的策略在遗留和已部署系统中难以采用，存在能源债务问题，因此研究无服务器计算平台减少能源债务的潜力。

Method: 设计并实现ReFaaS并集成到Fission无服务器框架中，评估多个LLMs进行代码翻译的能力并分析其对能耗的影响。

Result: 翻译后的函数可减少高达70%的调用能耗，约3000 - 5000次调用后实现净能源节省，但并非所有函数都适合翻译，部分函数摊销阈值高或无法达到。

Conclusion: 该方法虽面临挑战，但解决四个关键研究挑战可实现无服务器计算中能源债务的长期自动缓解。

Abstract: The rapid digitization and the increasing use of emerging technologies such
as AI models have significantly contributed to the emissions of computing
infrastructure. Efforts to mitigate this impact typically focus on the
infrastructure level such as powering data centers with renewable energy, or
through the specific design of energy-efficient software. However, both
strategies rely on stakeholder intervention, making their adoption in legacy
and already-deployed systems unlikely. As a result, past architectural and
implementation decisions continue to incur additional energy usage - a
phenomenon we refer to as energy debt.
  Hence, in this paper, we investigate the potential of serverless computing
platforms to automatically reduce energy debt by leveraging the unique access
to function source code. Specifically, we explore whether large language models
(LLMs) can translate serverless functions into more energy-efficient
programming languages while preserving functional correctness. To this end, we
design and implement ReFaaS and integrate it into the Fission serverless
framework. We evaluate multiple LLMs on their ability to perform such code
translations and analyze their impact on energy consumption.
  Our preliminary results indicate that translated functions can reduce
invocation energy by up to 70%, achieving net energy savings after
approximately 3,000 to 5,000 invocations, depending on the LLM used.
Nonetheless, the approach faces several challenges: not all functions are
suitable for translation, and for some, the amortization threshold is
significantly higher or unreachable. Despite these limitations, we identify
four key research challenges whose resolution could unlock long-term, automated
mitigation of energy debt in serverless computing.

</details>


### [64] [The AI_INFN Platform: Artificial Intelligence Development in the Cloud](https://arxiv.org/abs/2509.22117)
*Lucio Anderlini,Giulio Bianchini,Diego Ciangottini,Stefano Dal Pra,Diego Michelotto,Rosa Petrini,Daniele Spiga*

Main category: cs.DC

TL;DR: 介绍INFN资助的AI_INFN项目，利用云原生解决方案共享硬件加速器，更新Kubernetes平台调试情况，还会展示测试结果等。


<details>
  <summary>Details</summary>
Motivation: 机器学习在数据密集型软件开发中应用带来计算基础设施挑战，AI_INFN项目旨在推动INFN用例中机器学习技术的采用。

Method: 利用INFN Cloud中的云原生解决方案，使用Kubernetes平台，结合Virtual Kubelet和InterLink API的卸载机制。

Result: 可管理跨不同资源提供商的工作流，将展示初始测试结果、案例研究和集成场景。

Conclusion: 为需要为不同工作负载部分配备专用基础设施的用例提供了模型。

Abstract: Machine Learning (ML) is driving a revolution in the way scientists design,
develop, and deploy data-intensive software. However, the adoption of ML
presents new challenges for the computing infrastructure, particularly in terms
of provisioning and orchestrating access to hardware accelerators for
development, testing, and production. The INFN-funded project AI_INFN
(Artificial Intelligence at INFN) aims at fostering the adoption of ML
techniques within INFN use cases by providing support on multiple aspects,
including the provisioning of AI-tailored computing resources. It leverages
cloud-native solutions in the context of INFN Cloud, to share hardware
accelerators as effectively as possible, ensuring the diversity of the
Institute's research activities is not compromised. In this contribution, we
provide an update on the commissioning of a Kubernetes platform designed to
ease the development of GPU-powered data analysis workflows and their
scalability on heterogeneous distributed computing resources, also using the
offloading mechanism with Virtual Kubelet and InterLink API. This setup can
manage workflows across different resource providers, including sites of the
Worldwide LHC Computing Grid and supercomputers such as CINECA Leonardo,
providing a model for use cases requiring dedicated infrastructures for
different parts of the workload. Initial test results, emerging case studies,
and integration scenarios will be presented with functional tests and
benchmarks.

</details>


### [65] [Orientation does not help with 3-coloring a grid in online-LOCAL](https://arxiv.org/abs/2509.22233)
*Thomas Boudier,Filippo Casagrande,Avinandan Das,Massimo Equi,Henrik Lievonen,Augusto Modanese,Ronja Stimpert*

Main category: cs.DC

TL;DR: 本文解决了在算法已知网格全局一致方向的情况下，为在线 - LOCAL 和 SLOCAL 模型的 3 - 着色问题获得相同下界的问题。


<details>
  <summary>Details</summary>
Motivation: 之前关于网格 3 - 着色在在线 - LOCAL 模型下的下界证明依赖于算法无法获取网格方向的假设，作者希望去除该限制。

Method: 文中未明确提及具体方法。

Result: 即使算法明确获得网格的全局一致方向，仍能得到与之前相同的下界（针对在线 - LOCAL 和 SLOCAL 模型）。

Conclusion: 去除了之前证明中算法不能获取网格方向的限制，得到更具普遍性的下界结果。

Abstract: The online-LOCAL and SLOCAL models are extensions of the LOCAL model where
nodes are processed in a sequential but potentially adversarial order. So far,
the only problem we know of where the global memory of the online-LOCAL model
has an advantage over SLOCAL is 3-coloring bipartite graphs. Recently, Chang et
al. [PODC 2024] showed that even in grids, 3-coloring requires $\Omega(\log n)$
locality in deterministic online-LOCAL. This result was subsequently extended
by Akbari et al. [STOC 2025] to also hold in randomized online-LOCAL. However,
both proofs heavily rely on the assumption that the algorithm does not have
access to the orientation of the underlying grid. In this paper, we show how to
lift this requirement and obtain the same lower bound (against either model)
even when the algorithm is explicitly given a globally consistent orientation
of the grid.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [66] [New Algorithmic Directions in Optimal Transport and Applications for Product Spaces](https://arxiv.org/abs/2509.21502)
*Salman Beigi,Omid Etesami,Mohammad Mahmoody,Amir Najafi*

Main category: cs.DS

TL;DR: 从算法角度研究高维分布间的最优传输问题，提出通用算法及相关结果，解决高斯测度计算集中性的开放问题。


<details>
  <summary>Details</summary>
Motivation: 从算法角度研究在多项式时间内找到高维分布中相近样本，运行时间依赖维度而非分布全表示大小。

Method: 提出通用算法，在特定条件下进行最优传输；构建顺序采样器；证明Talagrand不等式的算法版本。

Result: 得到通用算法，证明算法版本的Talagrand不等式，构建顺序采样器，获得高斯测度计算集中性结果。

Conclusion: 解决了Etesami等人提出的开放问题，能在多项式时间内将多数高斯样本映射到指定集合内。

Abstract: We study optimal transport between two high-dimensional distributions
$\mu,\nu$ in $R^n$ from an algorithmic perspective: given $x \sim \mu$, find a
close $y \sim \nu$ in $poly(n)$ time, where $n$ is the dimension of $x,y$.
Thus, running time depends on the dimension rather than the full representation
size of $\mu,\nu$. Our main result is a general algorithm for transporting any
product distribution $\mu$ to any $\nu$ with cost $\Delta + \delta$ under
$\ell_p^p$, where $\Delta$ is the Knothe-Rosenblatt transport cost and $\delta$
is a computational error decreasing with runtime. This requires $\nu$ to be
"sequentially samplable" with bounded average sampling cost, a new but natural
notion.
  We further prove:
  An algorithmic version of Talagrand's inequality for transporting the
standard Gaussian $\Phi^n$ to arbitrary $\nu$ under squared Euclidean cost. For
$\nu = \Phi^n$ conditioned on a set $\mathcal{S}$ of measure $\varepsilon$, we
construct the sequential sampler in expected time $poly(n/\varepsilon)$ using
membership oracle access to $\mathcal{S}$. This yields an algorithmic transport
from $\Phi^n$ to $\Phi^n|\mathcal{S}$ in $poly(n/\varepsilon)$ time and
expected squared distance $O(\log 1/\varepsilon)$, optimal for general
$\mathcal{S}$ of measure $\varepsilon$.
  As corollary, we obtain the first computational concentration result (Etesami
et al. SODA 2020) for Gaussian measure under Euclidean distance with
dimension-independent transportation cost, resolving an open question of
Etesami et al. Specifically, for any $\mathcal{S}$ of Gaussian measure
$\varepsilon$, most $\Phi^n$ samples can be mapped to $\mathcal{S}$ within
distance $O(\sqrt{\log 1/\varepsilon})$ in $poly(n/\varepsilon)$ time.

</details>


### [67] [New Parallel and Streaming Algorithms for Directed Densest Subgraph](https://arxiv.org/abs/2509.21729)
*Slobodan Mitrović,Theodore Pan,Mahdi Qaempanah,Mohammad Amin Raeisi*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Finding dense subgraphs is a fundamental problem with applications to
community detection, clustering, and data mining. Our work focuses on finding
approximate densest subgraphs in directed graphs in computational models for
processing massive data. We consider two such models: Massively Parallel
Computation (MPC) and semi-streaming. We show how to find a
$(2+\varepsilon)$-approximation in $\tilde{O}(\sqrt{\log n})$ MPC rounds with
sublinear memory per machine. This improves the state-of-the-art results by
Bahmani et al. (WAW 2014) and Mitrovi\'c & Pan (ICML 2024). Moreover, we show
how to find an $O(\log n)$-approximation in a single pass in semi-streaming.
This is in stark contrast to prior work, which implies
$\tilde{\Omega}(n^{1/6})$-approximation for a single pass; a better
approximation is known only for randomized streams (Mitrovi\'c & Pan). This is
the first deterministic single-pass semi-streaming algorithm for the densest
subgraph problem, both for undirected and directed graphs. Our semi-streaming
approach is also an insertion-only dynamic algorithm, attaining the first
directed densest subgraph algorithm with $O(\log^2 n)$ worst-case update time
while using sub-linear memory. We empirically evaluate our approaches in two
ways. First, we illustrate that our single-pass semi-streaming algorithm
performs much better than the theoretical guarantee. Specifically, its
approximation on temporal datasets matches the $(2+\varepsilon)$-approximation
of an $O(\log n)$-pass algorithm by Bahmani et al. (VLDB 2012). Second, we
demonstrate that our MPC algorithm requires fewer rounds than prior work.

</details>


### [68] [Stable coresets: Unleashing the power of uniform sampling](https://arxiv.org/abs/2509.22189)
*Amir Carmel,Robert Krauthgamer*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Uniform sampling is a highly efficient method for data summarization.
However, its effectiveness in producing coresets for clustering problems is not
yet well understood, primarily because it generally does not yield a strong
coreset, which is the prevailing notion in the literature. We formulate
\emph{stable coresets}, a notion that is intermediate between the standard
notions of weak and strong coresets, and effectively combines the broad
applicability of strong coresets with highly efficient constructions, through
uniform sampling, of weak coresets. Our main result is that a uniform sample of
size $O(\epsilon^{-2}\log d)$ yields, with high constant probability, a stable
coreset for $1$-median in $\mathbb{R}^d$ under the $\ell_1$ metric. We then
leverage the powerful properties of stable coresets to easily derive new
coreset constructions, all through uniform sampling, for $\ell_1$ and related
metrics, such as Kendall-tau and Jaccard. We also show applications to fair
clustering and to approximation algorithms for $k$-median problems in these
metric spaces. Our experiments validate the benefits of stable coresets in
practice, in terms of both construction time and approximation quality.

</details>


### [69] [Less is More: Faster Maximum Clique Search by Work-Avoidance](https://arxiv.org/abs/2509.22245)
*Hans Vandierendonck*

Main category: cs.DS

TL;DR: 本文提出优化最大团（MC）搜索的技术，相比PMC和MC - BRB有显著加速。


<details>
  <summary>Details</summary>
Motivation: 最大团问题是具有挑战性的图挖掘问题，NP难特性导致执行时间长，需高效跳过搜索空间以提高效率。

Method: 提出多种优化技术，包括高效懒构建图表示、搜索前过滤、高效提前退出交集算法和利用算法选择。

Result: 相比最具可比性的PMC算法加速达38.9倍，比MC - BRB加速达11倍。

Conclusion: 所提技术能有效优化最大团搜索，提高搜索效率。

Abstract: The maximum clique (MC) problem is a challenging graph mining problem which,
due to its NP-hard nature, can take a substantial amount of execution time. The
MC problem is dominated by set intersection operations similar to Maximal
Clique Enumeration, however it differs in requiring to find only a clique of
maximum size. As such, key to the problem is to demonstrate efficiently that a
particular part of the search space does not contain a maximum clique, allowing
to skip over major parts of the search space. We present a number of techniques
to optimize MC search in light of leaving major parts of the search space
unvisited, including (i) an efficient, lazily constructed graph representation;
(ii) filtering prior to initiating a detailed search; (iii) efficient
early-exit intersection algorithms; (iv) exploiting algorithmic choice. These
techniques result in a speedup of up to 38.9x compared to PMC, which is the
most comparable algorithm, and a speedup up to 11x over MC-BRB.

</details>


### [70] [Online Firefighting on Cactus Graphs](https://arxiv.org/abs/2509.22277)
*Max Hugen,Bob Krekelberg,Alison Hsiang-Hsuan Liu*

Main category: cs.DS

TL;DR: 本文研究含环图的在线消防问题，证明蝌蚪图上无优于Ω(√n)竞争比的确定性算法，为1 - 几乎树和仙人掌图设计O(√n)竞争比算法，消防员成对释放时仙人掌图问题竞争复杂度降为最多3 - 竞争。


<details>
  <summary>Details</summary>
Motivation: 之前已知在线消防问题在树图上较简单，本文将研究拓展到含环图。

Method: 设计充电框架，对最优解拯救的顶点进行划分并向算法拯救的顶点收费。

Result: 证明蝌蚪图上无优于Ω(√n)竞争比的确定性算法；为1 - 几乎树和仙人掌图设计O(√n)竞争比算法；消防员成对释放时仙人掌图问题最多3 - 竞争。

Conclusion: 含环图的在线消防问题比树图复杂，充电框架可有效分析算法，消防员成对释放能显著降低竞争复杂度。

Abstract: It is known that the online firefighting is 2-competitive on trees
(Coupechoux et al. 2016), which suggests that the problem is relatively easy on
trees. We extend the study to graphs containing cycles. We first show that the
presence of cycles gives a strong advantage to the adversary: cycles create
situations where the algorithm and the optimal solution operate on different
game states, and the adversary can exploit the uncertainty in the firefighter
sequence to trap the algorithm. Specifically, we prove that even on a tadpole
graph (a cycle with a tail path), no deterministic online algorithm achieves a
competitive ratio better than $\Omega(\sqrt{n})$, where n is the number of
vertices. We then propose an $O(\sqrt{n})$-competitive algorithm for 1-almost
trees, which contain at most one cycle and generalize tadpole graphs. We
further generalize this algorithm to cactus graphs, in which multiple cycles
may appear, but no two share more than one vertex, and show that the online
firefighting problem on cactus graphs remains $O(\sqrt{n})$-competitive.
Finally, since cactus graphs have treewidth at most 2, we study a variant where
firefighters are released in pairs, that is, each round an even number of
firefighters is available. Surprisingly, in this setting the competitive
complexity is significantly reduced, and we prove that the problem is at most
3-competitive.
  The main technical challenges lie in both algorithm design and analysis,
since the algorithm and the optimal solution may break different cycles and
thus operate on different residual graphs. To overcome this difficulty, we
design a charging framework that carefully partitions the vertices saved by the
optimal solution and charges them to the vertices saved by the algorithm.
Namely, the charging scheme is carefully constructed to ensure that each vertex
saved by the algorithm is charged at most a constant number of times.

</details>


### [71] [Fine-Grained Classification Of Detecting Dominating Patterns](https://arxiv.org/abs/2509.22332)
*Jonathan Dransfeld,Marvin Künnemann,Mirza Redzic*

Main category: cs.DS

TL;DR: 文章研究支配P - 模式的细粒度复杂度分类，定义图参数ρ(P)得到除三角形外所有模式P的最优运行时间。


<details>
  <summary>Details</summary>
Motivation: 先前工作对特定模式的支配P - 模式有相关研究，作者希望对所有模式P的细粒度复杂度进行分类。

Method: 定义图参数ρ(P)，基于正交向量假设进行分析。

Result: 得到除三角形K₃外，所有模式P在ω = 2时的最优运行时间为(n^ρ(P) m^((|V(P)| - ρ(P))/2))^(1±o(1))。

Conclusion: 与任意诱导P - 模式缺乏完整细粒度分类形成对比，给出了支配P - 模式的分类结果。

Abstract: We consider the following generalization of dominating sets: Let $G$ be a
host graph and $P$ be a pattern graph $P$. A dominating $P$-pattern in $G$ is a
subset $S$ of vertices in $G$ that (1) forms a dominating set in $G$ \emph{and}
(2) induces a subgraph isomorphic to $P$. The graph theory literature studies
the properties of dominating $P$-patterns for various patterns $P$, including
cliques, matchings, independent sets, cycles and paths. Previous work
(Kunnemann, Redzic 2024) obtains algorithms and conditional lower bounds for
detecting dominating $P$-patterns particularly for $P$ being a $k$-clique, a
$k$-independent set and a $k$-matching. Their results give conditionally tight
lower bounds if $k$ is sufficiently large (where the bound depends the matrix
multiplication exponent $\omega$). We ask: Can we obtain a classification of
the fine-grained complexity for \emph{all} patterns $P$?
  Indeed, we define a graph parameter $\rho(P)$ such that if $\omega=2$, then
\[ \left(n^{\rho(P)} m^{\frac{|V(P)|-\rho(P)}{2}}\right)^{1\pm o(1)} \] is the
optimal running time assuming the Orthogonal Vectors Hypothesis, for all
patterns $P$ except the triangle $K_3$. Here, the host graph $G$ has $n$
vertices and $m=\Theta(n^\alpha)$ edges, where $1\le \alpha \le 2$.
  The parameter $\rho(P)$ is closely related (but sometimes different) to a
parameter $\delta(P) = \max_{S\subseteq V(P)} |S|-|N(S)|$ studied in (Alon
1981) to tightly quantify the maximum number of occurrences of induced
subgraphs isomorphic to $P$. Our results stand in contrast to the lack of a
full fine-grained classification of detecting an arbitrary (not necessarily
\emph{dominating}) induced $P$-pattern.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [72] [Breaking $1/ε$ Barrier in Quantum Zero-Sum Games: Generalizing Metric Subregularity for Spectraplexes](https://arxiv.org/abs/2509.21570)
*Yiheng Su,Emmanouil-Vasileios Vlatakis-Gkaragkounis,Pucheng Xiong*

Main category: cs.GT

TL;DR: 本文解决了量子零和博弈中量子可行集与经典多面体集收敛率差距的猜想，证明两种算法可实现线性最后迭代收敛率。


<details>
  <summary>Details</summary>
Motivation: 此前研究显示量子可行集和经典多面体集在双线性博弈梯度方法收敛率上可能存在差距，需验证该猜想。

Method: 采用Nesterov的迭代平滑和乐观梯度下降 - 上升的矩阵变体算法，依赖半定规划几何中误差界的新推广。

Result: 证明两种算法在量子零和博弈中实现线性最后迭代收敛率，匹配经典多面体情况，还获得经典Jain - Watrous方法的指数加速。

Conclusion: 否定了量子可行集和经典多面体集存在根本性能差距的猜想。

Abstract: Long studied as a toy model, quantum zero-sum games have recently resurfaced
as a canonical playground for modern areas such as non-local games, quantum
interactive proofs, and quantum machine learning. In this simple yet
fundamental setting, two competing quantum players send iteratively mixed
quantum states to a referee, who performs a joint measurement to determine
their payoffs. In 2025, Vasconcelos et al. [arXiv:2311.10859] connected quantum
communication channels with a hierarchy of quantum optimization algorithms that
generalize Matrix Multiplicative Weights Update ($\texttt{MMWU}$) through
extra-gradient mechanisms, establishing an average-iterate convergence rate of
$\mathcal{O}(1/\epsilon)$ iterations to $\epsilon$-Nash equilibria. While a
long line of work has shown that bilinear games over polyhedral domains admit
gradient methods with linear last-iterate convergence rates of
$\mathcal{O}(\log(1/\epsilon))$, it has been conjectured that a fundamental
performance gap must persist between quantum feasible sets (spectraplexes) and
classical polyhedral sets (simplices). We resolve this conjecture in the
negative. We prove that matrix variants of $\textit{Nesterov's iterative
smoothing}$ ($\texttt{IterSmooth}$) and $\textit{Optimistic Gradient
Descent-Ascent}$ ($\texttt{OGDA}$) achieve last-iterate convergence at a linear
rate in quantum zero-sum games, thereby matching the classical polyhedral case.
Our analysis relies on a new generalization of error bounds in semidefinite
programming geometry, establishing that (SP-MS) holds for monotone operators
over spectrahedra, despite their uncountably many extreme points. Finally, as a
byproduct, we obtain an exponential speed-up over the classical Jain-Watrous
[arXiv:0808.2775] method for parallel approximation of strictly positive
semidefinite programs.

</details>


### [73] [Incentives in Federated Learning with Heterogeneous Agents](https://arxiv.org/abs/2509.21612)
*Ariel D. Procaccia,Han Shao,Itai Shapira*

Main category: cs.GT

TL;DR: 本文引入博弈论框架解决联邦学习中激励不一致问题，分析贡献向量并提出策略证明机制。


<details>
  <summary>Details</summary>
Motivation: 联邦学习存在激励不一致问题，各参与者更新数据有成本却让所有参与者受益。

Method: 引入博弈论框架，分析成本最小化贡献向量，推导线性规划，结合付费规则。

Result: 无协调博弈存在问题，计算成本最小化贡献向量是NP难问题，线性规划可实现对数近似，提出的机制是策略证明且唯一。

Conclusion: 提出的机制能解决联邦学习激励不一致问题，引导协作。

Abstract: Federated learning promises significant sample-efficiency gains by pooling
data across multiple agents, yet incentive misalignment is an obstacle: each
update is costly to the contributor but boosts every participant. We introduce
a game-theoretic framework that captures heterogeneous data: an agent's utility
depends on who supplies each sample, not just how many. Agents aim to meet a
PAC-style accuracy threshold at minimal personal cost. We show that
uncoordinated play yields pathologies: pure equilibria may not exist, and the
best equilibrium can be arbitrarily more costly than cooperation. To steer
collaboration, we analyze the cost-minimizing contribution vector, prove that
computing it is NP-hard, and derive a polynomial-time linear program that
achieves a logarithmic approximation. Finally, pairing the LP with a simple
pay-what-you-contribute rule - each agent receives a payment equal to its
sample cost - yields a mechanism that is strategyproof and, within the class of
contribution-based transfers, is unique.

</details>


### [74] [Nearly Tight Regret Bounds for Profit Maximization in Bilateral Trade](https://arxiv.org/abs/2509.22563)
*Simone Di Gregorio,Paul Dütting,Federico Fusco,Chris Schwiegelshohn*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Bilateral trade models the task of intermediating between two strategic
agents, a seller and a buyer, willing to trade a good for which they hold
private valuations. We study this problem from the perspective of a broker, in
a regret minimization framework. At each time step, a new seller and buyer
arrive, and the broker has to propose a mechanism that is incentive-compatible
and individually rational, with the goal of maximizing profit.
  We propose a learning algorithm that guarantees a nearly tight
$\tilde{O}(\sqrt{T})$ regret in the stochastic setting when seller and buyer
valuations are drawn i.i.d. from a fixed and possibly correlated unknown
distribution. We further show that it is impossible to achieve sublinear regret
in the non-stationary scenario where valuations are generated upfront by an
adversary. Our ambitious benchmark for these results is the best
incentive-compatible and individually rational mechanism. This separates us
from previous works on efficiency maximization in bilateral trade, where the
benchmark is a single number: the best fixed price in hindsight.
  A particular challenge we face is that uniform convergence for all
mechanisms' profits is impossible. We overcome this difficulty via a careful
chaining analysis that proves convergence for a provably near-optimal mechanism
at (essentially) optimal rate. We further showcase the broader applicability of
our techniques by providing nearly optimal results for the joint ads problem.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [75] [SPELUNKER: Item Similarity Search Using Large Language Models and Custom K-Nearest Neighbors](https://arxiv.org/abs/2509.21323)
*Ana Rodrigues,João Mata,Rui Rego*

Main category: cs.IR

TL;DR: 本文提出结合大语言模型（LLM）与自定义K近邻（KNN）算法的混合系统用于物品相似度搜索，在葡萄酒评论数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 解决黑盒密集向量系统可解释性差的问题，实现自然语言查询与机器可理解物品表示之间的衔接。

Method: 先用LLM将自然语言查询转化为基于属性的结构化搜索，再将结构化查询作为自定义KNN算法（采用BallTree搜索策略和异构距离度量）的输入，还对KNN算法进行LLM重排序。

Result: 在500条葡萄酒评论数据集上，LLM信息提取F1分数达0.9779，Jaro字符串相似度为0.9321，KNN经LLM重排序后召回率有显著提升（p=0.013）。

Conclusion: 该方法有效弥合了人类语言和机器可理解物品表示之间的差距，提供了透明且细致的搜索能力。

Abstract: This paper presents a hybrid system for intuitive item similarity search that
combines a Large Language Model (LLM) with a custom K-Nearest Neighbors (KNN)
algorithm. Unlike black-box dense vector systems, this architecture provides
superior interpretability by first using an LLM to convert natural language
queries into structured, attribute-based searches. This structured query then
serves as input to a custom KNN algorithm with a BallTree search strategy,
which uses a heterogeneous distance metric to preserve distinct data types. Our
evaluation, conducted on a dataset of 500 wine reviews, demonstrates the
system's effectiveness. The LLM achieved an F1-score of 0.9779 in information
extraction, while also demonstrating high fidelity with a Jaro string
similarity of 0.9321. When we augmented the KNN algorithm with LLM-based
re-ranking, we observed a statistically significant improvement in recall
(p=0.013), indicating the LLM's ability to identify and promote relevant items
that align with nuanced user intent. This approach effectively bridges the gap
between human language and machine-understandable item representations,
offering a transparent and nuanced search capability.

</details>


### [76] [From Search to Reasoning: A Five-Level RAG Capability Framework for Enterprise Data](https://arxiv.org/abs/2509.21324)
*Gurbinder Gill,Ritvik Gupta,Denis Lusson,Anand Chandrashekar,Donald Nguyen*

Main category: cs.IR

TL;DR: 提出新分类框架L1 - L5对问答系统分类，引入对应基准并评估四个平台，验证多空间检索和动态编排价值。


<details>
  <summary>Details</summary>
Motivation: 传统基于文本语义搜索和重排序的RAG方法在处理非数据总结类问题和非文本数据时不足，需从问题导向理解RAG及相关问答系统。

Method: 提出基于数据模态和问答任务复杂度的分类框架L1 - L5，引入对应基准，评估四个平台，用多样企业数据集验证。

Result: 实验突出多空间检索和动态编排对实现L1 - L4能力的价值。

Conclusion: 通过新分类框架和评估实验，验证了多空间检索和动态编排对企业问答系统的重要性。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as the standard paradigm for
answering questions on enterprise data. Traditionally, RAG has centered on
text-based semantic search and re-ranking. However, this approach falls short
when dealing with questions beyond data summarization or non-text data. This
has led to various attempts to supplement RAG to bridge the gap between RAG,
the implementation paradigm, and the question answering problem that enterprise
users expect it to solve. Given that contemporary RAG is a collection of
techniques rather than a defined implementation, discussion of RAG and related
question-answering systems benefits from a problem-oriented understanding.
  We propose a new classification framework (L1-L5) to categorize systems based
on data modalities and task complexity of the underlying question answering
problems: L1 (Surface Knowledge of Unstructured Data) through L4 (Reflective
and Reasoned Knowledge) and the aspirational L5 (General Intelligence). We also
introduce benchmarks aligned with these levels and evaluate four
state-of-the-art platforms: LangChain, Azure AI Search, OpenAI, and Corvic AI.
Our experiments highlight the value of multi-space retrieval and dynamic
orchestration for enabling L1-L4 capabilities. We empirically validate our
findings using diverse datasets indicative of enterprise use cases.

</details>


### [77] [PIR-RAG: A System for Private Information Retrieval in Retrieval-Augmented Generation](https://arxiv.org/abs/2509.21325)
*Baiqiang Wang,Qian Lou,Mengxin Zheng,Dongfang Zhao*

Main category: cs.IR

TL;DR: 提出隐私保护检索增强生成系统PIR - RAG，评估显示其高效且可扩展。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）会向服务提供商暴露用户查询，带来隐私风险，需解决隐私保护问题。

Method: 采用粗粒度语义聚类修剪搜索空间，结合基于格的快速私有信息检索（PIR）协议。

Result: 与强大基线架构对比，PIR - RAG具有可扩展性，在“RAG就绪延迟”方面表现更优。

Conclusion: PIR - RAG是大规模AI系统隐私保护的可行且高效解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has become a foundational component of
modern AI systems, yet it introduces significant privacy risks by exposing user
queries to service providers. To address this, we introduce PIR-RAG, a
practical system for privacy-preserving RAG. PIR-RAG employs a novel
architecture that uses coarse-grained semantic clustering to prune the search
space, combined with a fast, lattice-based Private Information Retrieval (PIR)
protocol. This design allows for the efficient retrieval of entire document
clusters, uniquely optimizing for the end-to-end RAG workflow where full
document content is required. Our comprehensive evaluation against strong
baseline architectures, including graph-based PIR and Tiptoe-style private
scoring, demonstrates PIR-RAG's scalability and its superior performance in
terms of "RAG-Ready Latency"-the true end-to-end time required to securely
fetch content for an LLM. Our work establishes PIR-RAG as a viable and highly
efficient solution for privacy in large-scale AI systems.

</details>


### [78] [HetaRAG: Hybrid Deep Retrieval-Augmented Generation across Heterogeneous Data Stores](https://arxiv.org/abs/2509.21336)
*Guohang Yan,Yue Zhang,Pinlong Cai,Ding Wang,Song Mao,Hongwei Zhang,Yaoze Zhang,Hairong Zhang,Xinyu Cai,Botian Shi*

Main category: cs.IR

TL;DR: 介绍了HetaRAG，一种混合深度检索增强生成框架，可整合不同数据存储的跨模态证据，还进行了初步探索并构建初始RAG管道。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统是纯文本且依赖单一存储后端，存在不可避免的权衡，需融合不同检索范式以弥补单一模态的弱点。

Method: 提出原则性融合方案，将向量索引、知识图谱、全文引擎和结构化数据库统一到单一检索平面，动态路由和融合证据。

Result: 进行了初步探索，构建了初始RAG管道，部分代码公开。

Conclusion: 通过整合不同检索范式的互补优势，HetaRAG有望提升RAG系统的性能。

Abstract: Retrieval-augmented generation (RAG) has become a dominant paradigm for
mitigating knowledge hallucination and staleness in large language models
(LLMs) while preserving data security. By retrieving relevant evidence from
private, domain-specific corpora and injecting it into carefully engineered
prompts, RAG delivers trustworthy responses without the prohibitive cost of
fine-tuning. Traditional retrieval-augmented generation (RAG) systems are
text-only and often rely on a single storage backend, most commonly a vector
database. In practice, this monolithic design suffers from unavoidable
trade-offs: vector search captures semantic similarity yet loses global
context; knowledge graphs excel at relational precision but struggle with
recall; full-text indexes are fast and exact yet semantically blind; and
relational engines such as MySQL provide strong transactional guarantees but no
semantic understanding. We argue that these heterogeneous retrieval paradigms
are complementary, and propose a principled fusion scheme to orchestrate them
synergistically, mitigating the weaknesses of any single modality. In this work
we introduce HetaRAG, a hybrid, deep-retrieval augmented generation framework
that orchestrates cross-modal evidence from heterogeneous data stores. We plan
to design a system that unifies vector indices, knowledge graphs, full-text
engines, and structured databases into a single retrieval plane, dynamically
routing and fusing evidence to maximize recall, precision, and contextual
fidelity. To achieve this design goal, we carried out preliminary explorations
and constructed an initial RAG pipeline; this technical report provides a brief
overview. The partial code is available at
https://github.com/KnowledgeXLab/HetaRAG.

</details>


### [79] [Cross-Modal Retrieval with Cauchy-Schwarz Divergence](https://arxiv.org/abs/2509.21339)
*Jiahao Zhang,Wenzhe Yin,Shujian Yu*

Main category: cs.IR

TL;DR: 引入无超参数的Cauchy - Schwarz (CS)散度提升训练稳定性和检索性能，提出广义CS (GCS)散度用于多模态对齐，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态检索方法依赖分布对齐技术，存在数值不稳定、对超参数敏感等问题，需要更好的方法。

Method: 引入CS散度，提出GCS散度，通过双向循环比较方案在统一数学框架内实现多模态直接对齐。

Result: 在六个基准数据集的双模态和三模态检索任务中证明方法有效，代码公开。

Conclusion: 提出的CS/GCS散度能提升跨模态检索性能，适用于多模态对齐。

Abstract: Effective cross-modal retrieval requires robust alignment of heterogeneous
data types. Most existing methods focus on bi-modal retrieval tasks and rely on
distributional alignment techniques such as Kullback-Leibler divergence,
Maximum Mean Discrepancy, and correlation alignment. However, these methods
often suffer from critical limitations, including numerical instability,
sensitivity to hyperparameters, and their inability to capture the full
structure of the underlying distributions. In this paper, we introduce the
Cauchy-Schwarz (CS) divergence, a hyperparameter-free measure that improves
both training stability and retrieval performance. We further propose a novel
Generalized CS (GCS) divergence inspired by H\"older's inequality. This
extension enables direct alignment of three or more modalities within a unified
mathematical framework through a bidirectional circular comparison scheme,
eliminating the need for exhaustive pairwise comparisons. Extensive experiments
on six benchmark datasets demonstrate the effectiveness of our method in both
bi-modal and tri-modal retrieval tasks. The code of our CS/GCS divergence is
publicly available at https://github.com/JiahaoZhang666/CSD.

</details>


### [80] [ReGeS: Reciprocal Retrieval-Generation Synergy for Conversational Recommender Systems](https://arxiv.org/abs/2509.21371)
*Dayu Yang,Hui Fang*

Main category: cs.IR

TL;DR: 提出ReGeS框架解决对话推荐系统连接外部知识问题，实验显示其达SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有对话推荐系统连接外部知识的方案存在灵活性受限、易产生幻觉等问题

Method: 提出ReGeS框架，结合生成增强检索和检索增强生成

Result: 在多个对话推荐系统基准测试中，ReGeS在推荐准确性上达到了SOTA性能

Conclusion: ReGeS框架的双向协同对知识密集型对话推荐系统任务有效

Abstract: Connecting conversation with external domain knowledge is vital for
conversational recommender systems (CRS) to correctly understand user
preferences. However, existing solutions either require domain-specific
engineering, which limits flexibility, or rely solely on large language models,
which increases the risk of hallucination. While Retrieval-Augmented Generation
(RAG) holds promise, its naive use in CRS is hindered by noisy dialogues that
weaken retrieval and by overlooked nuances among similar items. We propose
ReGeS, a reciprocal Retrieval-Generation Synergy framework that unifies
generation-augmented retrieval to distill informative user intent from
conversations and retrieval-augmented generation to differentiate subtle item
features. This synergy obviates the need for extra annotations, reduces
hallucinations, and simplifies continuous updates. Experiments on multiple CRS
benchmarks show that ReGeS achieves state-of-the-art performance in
recommendation accuracy, demonstrating the effectiveness of reciprocal synergy
for knowledge-intensive CRS tasks.

</details>


### [81] [MIXRAG : Mixture-of-Experts Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering](https://arxiv.org/abs/2509.21391)
*Lihui Liu,Carl J. Yang*

Main category: cs.IR

TL;DR: 文章指出大语言模型在知识密集领域易产生幻觉问题，现有图RAG系统有局限，提出MIXRAG框架，实验表明其性能达SOTA。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识密集领域依赖静态预训练语料会产生幻觉，现有图RAG系统单检索器的方式难以处理复杂查询，且难以判断相关性。

Method: 提出MIXRAG框架，引入多个专业图检索器和动态路由控制器，用专家混合模块自适应选择和融合检索器，引入查询感知的GraphEncoder减少噪声。

Result: 方法达到了SOTA性能，在不同领域基于图的任务中表现出色，持续超越各种基线。

Conclusion: MIXRAG在多种基于图的任务中有效，代码将在论文接受后发布。

Abstract: Large Language Models (LLMs) have achieved impressive performance across a
wide range of applications. However, they often suffer from hallucinations in
knowledge-intensive domains due to their reliance on static pretraining
corpora. To address this limitation, Retrieval-Augmented Generation (RAG)
enhances LLMs by incorporating external knowledge sources during inference.
Among these sources, textual graphs provide structured and semantically rich
information that supports more precise and interpretable reasoning. This has
led to growing interest in graph-based RAG systems. Despite their potential,
most existing approaches rely on a single retriever to identify relevant
subgraphs, which limits their ability to capture the diverse aspects of complex
queries. Moreover, these systems often struggle to accurately judge the
relevance of retrieved content, making them prone to distraction by irrelevant
noise. To address these challenges, in this paper, we propose MIXRAG, a
Mixture-of-Experts Graph-RAG framework that introduces multiple specialized
graph retrievers and a dynamic routing controller to better handle diverse
query intents. Each retriever is trained to focus on a specific aspect of graph
semantics, such as entities, relations, or subgraph topology. A
Mixture-of-Experts module adaptively selects and fuses relevant retrievers
based on the input query. To reduce noise in the retrieved information, we
introduce a query-aware GraphEncoder that carefully analyzes relationships
within the retrieved subgraphs, highlighting the most relevant parts while
down-weighting unnecessary noise. Empirical results demonstrate that our method
achieves state-of-the-art performance and consistently outperforms various
baselines. MIXRAG is effective across a wide range of graph-based tasks in
different domains. The code will be released upon paper acceptance.

</details>


### [82] [Effect of Model Merging in Domain-Specific Ad-hoc Retrieval](https://arxiv.org/abs/2509.21966)
*Taiga Sasaki,Takehiro Yamamoto,Hiroaki Ohshima,Sumio Fujita*

Main category: cs.IR

TL;DR: 评估模型合并在特定领域即席检索任务中的效果，结果表明其有潜力生成更有效的特定领域检索模型。


<details>
  <summary>Details</summary>
Motivation: 验证将模型合并应用于特定领域即席检索任务能否提高检索效果。

Method: 使用线性插值方法合并源检索模型和特定领域（非检索）模型的权重，且无需额外微调。进行医学和日语领域的两组实验，分别与源检索模型和LoRA微调模型对比。

Result: 模型合并有潜力生成比源检索模型更有效的特定领域检索模型，在数据有限时或可替代LoRA微调。

Conclusion: 模型合并在特定领域即席检索任务中有应用价值，尤其在数据有限时可作为LoRA微调的实用替代方案。

Abstract: In this study, we evaluate the effect of model merging in ad-hoc retrieval
tasks. Model merging is a technique that combines the diverse characteristics
of multiple models. We hypothesized that applying model merging to
domain-specific ad-hoc retrieval tasks could improve retrieval effectiveness.
To verify this hypothesis, we merged the weights of a source retrieval model
and a domain-specific (non-retrieval) model using a linear interpolation
approach. A key advantage of our approach is that it requires no additional
fine-tuning of the models. We conducted two experiments each in the medical and
Japanese domains. The first compared the merged model with the source retrieval
model, and the second compared it with a LoRA fine-tuned model under both full
and limited data settings for model construction. The experimental results
indicate that model merging has the potential to produce more effective
domain-specific retrieval models than the source retrieval model, and may serve
as a practical alternative to LoRA fine-tuning, particularly when only a
limited amount of data is available.

</details>


### [83] [GoalRank: Group-Relative Optimization for a Large Ranking Model](https://arxiv.org/abs/2509.22046)
*Kaike Zhang,Xiaobei Wang,Shuchang Liu,Hailan Yang,Xiang Li,Lantao Hu,Han Li,Qi Cao,Fei Sun,Kun Gai*

Main category: cs.IR

TL;DR: 本文从仅生成器的单阶段视角重新审视排序问题，提出了GoalRank框架，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 主流排序方法的两阶段范式扩大候选列表效果不佳，而单阶段模型有潜力，因此从仅生成器的单阶段视角重新审视排序。

Method: 理论证明存在仅生成器模型能有更小近似误差，推导单阶段优化目标的证据上界，利用奖励模型构建参考策略，提出GoalRank框架。

Result: 在公共基准的离线实验和大规模在线A/B测试中，GoalRank始终优于现有方法。

Conclusion: 仅生成器的单阶段排序框架GoalRank是有效的，能取得比现有方法更好的性能。

Abstract: Mainstream ranking approaches typically follow a Generator-Evaluator
two-stage paradigm, where a generator produces candidate lists and an evaluator
selects the best one. Recent work has attempted to enhance performance by
expanding the number of candidate lists, for example, through multi-generator
settings. However, ranking involves selecting a recommendation list from a
combinatorially large space. Simply enlarging the candidate set remains
ineffective, and performance gains quickly saturate. At the same time, recent
advances in large recommendation models have shown that end-to-end one-stage
models can achieve promising performance with the expectation of scaling laws.
Motivated by this, we revisit ranking from a generator-only one-stage
perspective. We theoretically prove that, for any (finite
Multi-)Generator-Evaluator model, there always exists a generator-only model
that achieves strictly smaller approximation error to the optimal ranking
policy, while also enjoying scaling laws as its size increases. Building on
this result, we derive an evidence upper bound of the one-stage optimization
objective, from which we find that one can leverage a reward model trained on
real user feedback to construct a reference policy in a group-relative manner.
This reference policy serves as a practical surrogate of the optimal policy,
enabling effective training of a large generator-only ranker. Based on these
insights, we propose GoalRank, a generator-only ranking framework. Extensive
offline experiments on public benchmarks and large-scale online A/B tests
demonstrate that GoalRank consistently outperforms state-of-the-art methods.

</details>


### [84] [Does Generative Retrieval Overcome the Limitations of Dense Retrieval?](https://arxiv.org/abs/2509.22116)
*Yingchen Zhang,Ruqing Zhang,Jiafeng Guo,Maarten de Rijke,Yixing Fan,Xueqi Cheng*

Main category: cs.IR

TL;DR: 本文研究生成式检索（GR）与密集检索（DR）在学习目标和表征能力上的差异，发现GR在扩展时有优势，但实践中并非全面优于DR，并给出缩小理论与实践差距的方向。


<details>
  <summary>Details</summary>
Motivation: 探究生成式检索（GR）与密集检索（DR）在学习目标和表征能力上的根本差异。

Method: 理论分析GR和DR的差异，通过在Natural Questions和MS MARCO数据集上进行不同负采样策略、嵌入维度和模型规模的受控实验验证理论。

Result: GR在扩展时可克服DR的固有局限，但实践中GR并非全面优于DR。

Conclusion: 给出缩小GR理论潜力与实践性能差距的方向，为可扩展且稳健的生成式检索研究提供指导。

Abstract: Generative retrieval (GR) has emerged as a new paradigm in neural information
retrieval, offering an alternative to dense retrieval (DR) by directly
generating identifiers of relevant documents. In this paper, we theoretically
and empirically investigate how GR fundamentally diverges from DR in both
learning objectives and representational capacity. GR performs globally
normalized maximum-likelihood optimization and encodes corpus and relevance
information directly in the model parameters, whereas DR adopts locally
normalized objectives and represents the corpus with external embeddings before
computing similarity via a bilinear interaction. Our analysis suggests that,
under scaling, GR can overcome the inherent limitations of DR, yielding two
major benefits. First, with larger corpora, GR avoids the sharp performance
degradation caused by the optimization drift induced by DR's local
normalization. Second, with larger models, GR's representational capacity
scales with parameter size, unconstrained by the global low-rank structure that
limits DR. We validate these theoretical insights through controlled
experiments on the Natural Questions and MS MARCO datasets, across varying
negative sampling strategies, embedding dimensions, and model scales. But
despite its theoretical advantages, GR does not universally outperform DR in
practice. We outline directions to bridge the gap between GR's theoretical
potential and practical performance, providing guidance for future research in
scalable and robust generative retrieval.

</details>


### [85] [Can Synthetic Query Rewrites Capture User Intent Better than Humans in Retrieval-Augmented Generation?](https://arxiv.org/abs/2509.22325)
*JiaYing Zheng,HaiNan Zhang,Liang Pang,YongXin Tong,ZhiMing Zheng*

Main category: cs.IR

TL;DR: 提出SynRewrite模型利用合成数据进行查询重写，实验显示其在检索和生成任务上优于人工重写，合成重写可作为人工标注的有效替代。


<details>
  <summary>Details</summary>
Motivation: 多轮RAG系统面临查询问题，传统人工查询重写存在与用户意图不一致的问题，希望利用合成查询缩小差距。

Method: 用GPT - 4o合成高质量重写数据，微调Flan - T5模型，用DPO算法根据生成器反馈增强重写器。

Result: 在TopiOCQA和QRECC数据集实验中，SynRewrite在检索和生成任务上始终优于人工重写。

Conclusion: 合成重写可作为人工标注可扩展且有效的替代方案。

Abstract: Multi-turn RAG systems often face queries with colloquial omissions and
ambiguous references, posing significant challenges for effective retrieval and
generation. Traditional query rewriting relies on human annotators to clarify
queries, but due to limitations in annotators' expressive ability and depth of
understanding, manually rewritten queries often diverge from those needed in
real-world RAG systems, resulting in a gap between user intent and system
response. We observe that high-quality synthetic queries can better bridge this
gap, achieving superior performance in both retrieval and generation compared
to human rewrites. This raises an interesting question: Can rewriting models
trained on synthetic queries better capture user intent than human annotators?
In this paper, we propose SynRewrite, a synthetic data-driven query rewriting
model to generate high-quality synthetic rewrites more aligned with user
intent. To construct training data, we prompt GPT-4o with dialogue history,
current queries, positive documents, and answers to synthesize high-quality
rewrites. A Flan-T5 model is then finetuned on this dataset to map dialogue
history and queries to synthetic rewrites. Finally, we further enhance the
rewriter using the generator's feedback through the DPO algorithm to boost
end-task performance. Experiments on TopiOCQA and QRECC datasets show that
SynRewrite consistently outperforms human rewrites in both retrieval and
generation tasks. Our results demonstrate that synthetic rewrites can serve as
a scalable and effective alternative to human annotations.

</details>


### [86] [Your RAG is Unfair: Exposing Fairness Vulnerabilities in Retrieval-Augmented Generation via Backdoor Attacks](https://arxiv.org/abs/2509.22486)
*Gaurav Bagwe,Saket S. Chaturvedi,Xiaolong Ma,Xiaoyong Yuan,Kuang-Ching Wang,Lan Zhang*

Main category: cs.IR

TL;DR: 本文提出BiasRAG框架，通过两阶段后门攻击暴露检索增强生成（RAG）的公平性漏洞，经验评估显示其攻击成功率高。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注RAG的虚假信息威胁，公平性漏洞探索不足，本文旨在暴露RAG的公平性漏洞。

Method: 提出BiasRAG框架，分预训练和部署后两个阶段进行后门攻击，预训练阶段破坏查询编码器，部署后阶段向知识库注入对抗文档。

Result: BiasRAG能实现高攻击成功率，同时保持上下文相关性和实用性。

Conclusion: BiasRAG对RAG的公平性构成持续且不断演变的威胁。

Abstract: Retrieval-augmented generation (RAG) enhances factual grounding by
integrating retrieval mechanisms with generative models but introduces new
attack surfaces, particularly through backdoor attacks. While prior research
has largely focused on disinformation threats, fairness vulnerabilities remain
underexplored. Unlike conventional backdoors that rely on direct
trigger-to-target mappings, fairness-driven attacks exploit the interaction
between retrieval and generation models, manipulating semantic relationships
between target groups and social biases to establish a persistent and covert
influence on content generation.
  This paper introduces BiasRAG, a systematic framework that exposes fairness
vulnerabilities in RAG through a two-phase backdoor attack. During the
pre-training phase, the query encoder is compromised to align the target group
with the intended social bias, ensuring long-term persistence. In the
post-deployment phase, adversarial documents are injected into knowledge bases
to reinforce the backdoor, subtly influencing retrieved content while remaining
undetectable under standard fairness evaluations. Together, BiasRAG ensures
precise target alignment over sensitive attributes, stealthy execution, and
resilience. Empirical evaluations demonstrate that BiasRAG achieves high attack
success rates while preserving contextual relevance and utility, establishing a
persistent and evolving threat to fairness in RAG.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [87] [PreLoRA: Hybrid Pre-training of Vision Transformers with Full Training and Low-Rank Adapters](https://arxiv.org/abs/2509.21619)
*Krishu K Thapa,Reet Barik,Krishna Teja Chitty-Venkata,Murali Emani,Venkatram Vishwanath*

Main category: cs.LG

TL;DR: 提出在ViT - Large模型中动态从全参数训练切换到低秩适配（LoRA）的方法，可减少参数、提升吞吐量、缩短训练时间和降低显存消耗。


<details>
  <summary>Details</summary>
Motivation: 训练大模型资源密集，早期学习权重变化大，后期可由低秩矩阵捕捉，为节省资源提出此方法。

Method: 识别部分收敛状态，利用用户定义超参数确定切换点，根据模块层收敛程度分配特定秩。

Result: 该方法保持模型精度，将可训练参数减至原大小10%，吞吐量提升3倍，单轮训练时间缩短1.5倍，显存消耗降低20%。

Conclusion: 所提方法在保持模型精度的同时，能有效节省资源，提高训练效率。

Abstract: Training large models ranging from millions to billions of parameters is
highly resource-intensive, requiring significant time, compute, and memory. It
is observed that most of the learning (higher change in weights) takes place in
the earlier stage of the training loop. These changes stabilize as training
continues, enabling them to be captured by matrices of a low intrinsic rank.
Therefore, we propose an approach to identify such states of partial
convergence and dynamically switch from full parameter training to Low-Rank
Adaptation (LoRA) on the ViT-Large model. We introduce a flexible approach that
leverages user-defined hyperparameters to determine the switching point and
assign a rank specific to each module layer based on its level of convergence.
Experimental results show that this approach preserves model accuracy while
reducing the number of trainable parameters to 10% of its original size,
resulting in a 3x improvement in throughput, and a 1.5x reduction in average
training time per epoch while also reducing GPU memory consumption by 20%

</details>


### [88] [Role-Aware Multi-modal federated learning system for detecting phishing webpages](https://arxiv.org/abs/2509.22369)
*Bo Wang,Imran Khan,Martin White,Natalia Beloff*

Main category: cs.LG

TL;DR: 提出支持多模态输入的联邦式钓鱼网站检测器，采用角色感知桶聚合方法，在多数据集取得良好结果，表明该方法能稳定训练并提升检测可用性与灵活性。


<details>
  <summary>Details</summary>
Motivation: 构建支持多模态输入且不绑定客户端固定模态的联邦式钓鱼网站检测器，解决跨嵌入冲突并稳定收敛。

Method: 在FedProx基础上提出角色感知桶聚合，受Mixture - of - Experts和FedMM启发，采用硬门控，分离聚合特定模态参数；文本使用GraphCodeBERT处理URL、早期三元嵌入处理HTML。

Result: 在TR - OP、WebPhish等数据集上，检测器达到较高准确率和较低误报率。

Conclusion: 带硬门控专家的桶聚合能在严格隐私下实现稳定联邦训练，提升多模态钓鱼检测的可用性与灵活性。

Abstract: We present a federated, multi-modal phishing website detector that supports
URL, HTML, and IMAGE inputs without binding clients to a fixed modality at
inference: any client can invoke any modality head trained elsewhere.
Methodologically, we propose role-aware bucket aggregation on top of FedProx,
inspired by Mixture-of-Experts and FedMM. We drop learnable routing and use
hard gating (selecting the IMAGE/HTML/URL expert by sample modality), enabling
separate aggregation of modality-specific parameters to isolate cross-embedding
conflicts and stabilize convergence. On TR-OP, the Fusion head reaches Acc
97.5% with FPR 2.4% across two data types; on the image subset (ablation) it
attains Acc 95.5% with FPR 5.9%. For text, we use GraphCodeBERT for URLs and an
early three-way embedding for raw, noisy HTML. On WebPhish (HTML) we obtain Acc
96.5% / FPR 1.8%; on TR-OP (raw HTML) we obtain Acc 95.1% / FPR 4.6%. Results
indicate that bucket aggregation with hard-gated experts enables stable
federated training under strict privacy, while improving the usability and
flexibility of multi-modal phishing detection.

</details>


### [89] [SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly](https://arxiv.org/abs/2509.22387)
*Narada Maugin,Tristan Cazenave*

Main category: cs.LG

TL;DR: 文章指出CFR算法在多人扑克游戏中存在局限，提出针对三人在线扑克Spin & Go的LLM模型SpinGPT，训练后结果良好，表明LLMs可用于处理多人不完全信息游戏。


<details>
  <summary>Details</summary>
Motivation: CFR算法计算复杂度随玩家数量指数增长，在三人及以上游戏中遵循纳什均衡不能保证不败，限制了其在流行赛制中的应用，受LLM在国际象棋和外交游戏中的成功启发。

Method: 对SpinGPT进行两阶段训练，先在320k高注额专家决策上进行有监督微调，再在270k求解器生成的手牌上进行强化学习。

Result: SpinGPT在78%的决策中与求解器动作匹配（容错准确率），使用简单深筹码启发式策略，在30000手牌的单挑中对阵Slumbot达到13.4 +/- 12.9 BB/100（95%置信区间）。

Conclusion: LLMs可能是处理像扑克这样的多人不完全信息游戏的新途径。

Abstract: The Counterfactual Regret Minimization (CFR) algorithm and its variants have
enabled the development of pokerbots capable of beating the best human players
in heads-up (1v1) cash games and competing with them in six-player formats.
However, CFR's computational complexity rises exponentially with the number of
players. Furthermore, in games with three or more players, following Nash
equilibrium no longer guarantees a non-losing outcome. These limitations, along
with others, significantly restrict the applicability of CFR to the most
popular formats: tournaments. Motivated by the recent success of Large Language
Models (LLM) in chess and Diplomacy, we present SpinGPT, the first LLM tailored
to Spin & Go, a popular three-player online poker format. SpinGPT is trained in
two stages: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions;
(2) Reinforcement Learning on 270k solver-generated hands. Our results show
that SpinGPT matches the solver's actions in 78% of decisions (tolerant
accuracy). With a simple deep-stack heuristic, it achieves 13.4 +/- 12.9 BB/100
versus Slumbot in heads-up over 30,000 hands (95% CI). These results suggest
that LLMs could be a new way to deal with multi-player imperfect-information
games like poker.

</details>


### [90] [Learning from Delayed Feedback in Games via Extra Prediction](https://arxiv.org/abs/2509.22426)
*Yuma Fujimoto,Kenshi Abe,Kaito Ariu*

Main category: cs.LG

TL;DR: 研究解决博弈学习中的时间延迟反馈问题，提出加权OFTRL算法，理论和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 博弈学习中多智能体优化存在差异，且观察过去奖励的时间延迟影响基于预测的算法（如OFTRL）性能。

Method: 提出加权OFTRL（WOFTRL）算法，对OFTRL中下一奖励的预测向量进行n次加权。

Result: 理论证明单步延迟会降低OFTRL性能，当乐观权重超过时间延迟时，WOFTRL在一般和正规形式博弈中后悔值为常数，在多矩阵零和博弈中策略收敛到纳什均衡子序列；实验支持理论结果。

Conclusion: WOFTRL算法能有效克服时间延迟对博弈学习的影响，恢复良好性能。

Abstract: This study raises and addresses the problem of time-delayed feedback in
learning in games. Because learning in games assumes that multiple agents
independently learn their strategies, a discrepancy in optimization often
emerges among the agents. To overcome this discrepancy, the prediction of the
future reward is incorporated into algorithms, typically known as Optimistic
Follow-the-Regularized-Leader (OFTRL). However, the time delay in observing the
past rewards hinders the prediction. Indeed, this study firstly proves that
even a single-step delay worsens the performance of OFTRL from the aspects of
regret and convergence. This study proposes the weighted OFTRL (WOFTRL), where
the prediction vector of the next reward in OFTRL is weighted $n$ times. We
further capture an intuition that the optimistic weight cancels out this time
delay. We prove that when the optimistic weight exceeds the time delay, our
WOFTRL recovers the good performances that the regret is constant
($O(1)$-regret) in general-sum normal-form games, and the strategies converge
to the Nash equilibrium as a subsequence (best-iterate convergence) in
poly-matrix zero-sum games. The theoretical results are supported and
strengthened by our experiments.

</details>


### [91] [LANCE: Low Rank Activation Compression for Efficient On-Device Continual Learning](https://arxiv.org/abs/2509.21617)
*Marco Paul E. Apolinario,Kaushik Roy*

Main category: cs.LG

TL;DR: 提出LANCE框架，通过一次高阶SVD获得低秩子空间用于激活投影，减少内存和计算开销，在多数据集实验表现良好，是边缘设备高效微调与持续学习的实用可扩展方案。


<details>
  <summary>Details</summary>
Motivation: 设备端学习需高效学习且避免灾难性遗忘，但反向传播存储激活的高内存成本限制了其发展，现有激活压缩方法有计算开销且未用于持续学习。

Method: 提出LANCE框架，进行一次高阶SVD获取可重复使用的低秩子空间用于激活投影，将任务分配到正交子空间实现持续学习。

Result: 在多个数据集上，LANCE减少激活存储达250倍，保持与全反向传播相当的准确率；在持续学习基准测试中，以低内存成本取得与正交梯度投影方法相当的性能。

Conclusion: LANCE是边缘设备高效微调与持续学习的实用可扩展解决方案。

Abstract: On-device learning is essential for personalization, privacy, and long-term
adaptation in resource-constrained environments. Achieving this requires
efficient learning, both fine-tuning existing models and continually acquiring
new tasks without catastrophic forgetting. Yet both settings are constrained by
high memory cost of storing activations during backpropagation. Existing
activation compression methods reduce this cost but relying on repeated
low-rank decompositions, introducing computational overhead. Also, such methods
have not been explored for continual learning. We propose LANCE (Low-rank
Activation Compression), a framework that performs one-shot higher-order
Singular Value Decompsoition (SVD) to obtain a reusable low-rank subspace for
activation projection. This eliminates repeated decompositions, reducing both
memory and computation. Moreover, fixed low-rank subspaces further enable
on-device continual learning by allocating tasks to orthogonal subspaces
without storing large task-specific matrices. Experiments show that LANCE
reduces activation storage up to 250$\times$ while maintaining accuracy
comparable to full backpropagation on CIFAR-10/100, Oxford-IIIT Pets,
Flowers102, and CUB-200 datasets. On continual learning benchmarks (Split
CIFAR-100, Split MiniImageNet, 5-Datasets), it achieves performance competitive
with orthogonal gradient projection methods at a fraction of the memory cost.
These results position LANCE as a practical and scalable solution for efficient
fine-tuning and continual learning on edge devices.

</details>


### [92] [Prophecy: Inferring Formal Properties from Neuron Activations](https://arxiv.org/abs/2509.21677)
*Divya Gopinath,Corina S. Pasareanu,Muhammad Usman*

Main category: cs.LG

TL;DR: 介绍自动推断前馈神经网络形式属性的工具Prophecy，包括架构、特点、应用和新成果。


<details>
  <summary>Details</summary>
Motivation: 自动推断前馈神经网络的形式属性。

Method: 基于神经元激活状态提取规则作为前提条件，以暗示特定输出属性。

Result: 展示了工具的架构、特点，在不同模型和输出属性上的应用，以及在大视觉语言模型时代的潜力。

Conclusion: Prophecy工具在推断神经网络形式属性方面有应用潜力。

Abstract: We present Prophecy, a tool for automatically inferring formal properties of
feed-forward neural networks. Prophecy is based on the observation that a
significant part of the logic of feed-forward networks is captured in the
activation status of the neurons at inner layers. Prophecy works by extracting
rules based on neuron activations (values or on/off statuses) as preconditions
that imply certain desirable output property, e.g., the prediction being a
certain class. These rules represent network properties captured in the hidden
layers that imply the desired output behavior. We present the architecture of
the tool, highlight its features and demonstrate its usage on different types
of models and output properties. We present an overview of its applications,
such as inferring and proving formal explanations of neural networks,
compositional verification, run-time monitoring, repair, and others. We also
show novel results highlighting its potential in the era of large
vision-language models.

</details>


### [93] [Are Hallucinations Bad Estimations?](https://arxiv.org/abs/2509.21473)
*Hude Liu,Jerry Yao-Chieh Hu,Jennifer Yuntong Zhang,Zhao Song,Han Liu*

Main category: cs.LG

TL;DR: 本文将生成模型中的幻觉定义为估计值与合理原因的脱节，证明最优估计器也会产生幻觉，用下界证实，实验支持理论。


<details>
  <summary>Details</summary>
Motivation: 研究生成模型中幻觉现象的本质。

Method: 将幻觉形式化定义，推导通用数据分布下幻觉率的高概率下界，进行硬币聚合、开放式问答和文本到图像的实验。

Result: 证明即使是损失最小化的最优估计器也会产生幻觉，实验支持理论。

Conclusion: 幻觉是损失最小化与人类可接受输出之间的结构失调，是校准不当导致的估计误差。

Abstract: We formalize hallucinations in generative models as failures to link an
estimate to any plausible cause. Under this interpretation, we show that even
loss-minimizing optimal estimators still hallucinate. We confirm this with a
general high probability lower bound on hallucinate rate for generic data
distributions. This reframes hallucination as structural misalignment between
loss minimization and human-acceptable outputs, and hence estimation errors
induced by miscalibration. Experiments on coin aggregation, open-ended QA, and
text-to-image support our theory.

</details>


### [94] [High-Probability Analysis of Online and Federated Zero-Order Optimisation](https://arxiv.org/abs/2509.21484)
*Arya Akhavan,David Janz,El-Mahdi El-Mhamdi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study distributed learning in the setting of gradient-free zero-order
optimization and introduce FedZero, a federated zero-order algorithm that
delivers sharp theoretical guarantees. Specifically, FedZero: (1) achieves
near-optimal optimization error bounds with high probability in the federated
convex setting; and (2) in the single-worker regime-where the problem reduces
to the standard zero-order framework, establishes the first high-probability
convergence guarantees for convex zero-order optimization, thereby
strengthening the classical expectation-based results. At its core, FedZero
employs a gradient estimator based on randomization over the $\ell_1$-sphere.
To analyze it, we develop new concentration inequalities for Lipschitz
functions under the uniform measure on the $\ell_1$-sphere, with explicit
constants. These concentration tools are not only central to our
high-probability guarantees but may also be of independent interest.

</details>


### [95] [Contrastive Mutual Information Learning: Toward Robust Representations without Positive-Pair Augmentations](https://arxiv.org/abs/2509.21511)
*Micha Livne*

Main category: cs.LG

TL;DR: 提出对比互信息机（cMIM）框架，介绍提取特征技术，在多个基准测试中表现良好，是统一表征学习框架。


<details>
  <summary>Details</summary>
Motivation: 解决现有表征学习范式在不同下游任务上迁移效果不佳的问题，弥补MIM在判别任务上的不足。

Method: 提出cMIM，这是MIM的对比扩展；引入信息嵌入技术提取特征。

Result: cMIM在分类和回归任务上优于MIM和InfoNCE，同时保持有竞争力的重建质量。

Conclusion: cMIM是一个统一的表征学习框架，能有效服务于判别和生成应用。

Abstract: Learning representations that transfer well to diverse downstream tasks
remains a central challenge in representation learning. Existing paradigms --
contrastive learning, self-supervised masking, and denoising auto-encoders --
balance this challenge with different trade-offs. We introduce the {contrastive
Mutual Information Machine} (cMIM), a probabilistic framework that extends the
Mutual Information Machine (MIM) with a contrastive objective. While MIM
maximizes mutual information between inputs and latents and promotes clustering
of codes, it falls short on discriminative tasks. cMIM addresses this gap by
imposing global discriminative structure while retaining MIM's generative
fidelity. Our contributions are threefold. First, we propose cMIM, a
contrastive extension of MIM that removes the need for positive data
augmentation and is substantially less sensitive to batch size than InfoNCE.
Second, we introduce {informative embeddings}, a general technique for
extracting enriched features from encoder-decoder models that boosts
discriminative performance without additional training and applies broadly
beyond MIM. Third, we provide empirical evidence across vision and molecular
benchmarks showing that cMIM outperforms MIM and InfoNCE on classification and
regression tasks while preserving competitive reconstruction quality. These
results position cMIM as a unified framework for representation learning,
advancing the goal of models that serve both discriminative and generative
applications effectively.

</details>


### [96] [DistillKac: Few-Step Image Generation via Damped Wave Equations](https://arxiv.org/abs/2509.21513)
*Weiqiao Han,Chenlin Meng,Christopher D. Manning,Stefano Ermon*

Main category: cs.LG

TL;DR: 提出快速图像生成器DistillKac，有实验表明其能以少量函数评估生成高质量样本并保留有限速度概率流的数值稳定性。


<details>
  <summary>Details</summary>
Motivation: 针对扩散模型反向时间速度可能变僵且隐含无界传播速度的问题，引入Kac动力学实现有限速度传输和全局有界动能。

Method: 在速度空间引入无分类器引导，提出仅端点蒸馏方法，还证明了稳定性结果。

Result: DistillKac能以少量函数评估生成高质量样本。

Conclusion: DistillKac在生成图像时兼具高质量和数值稳定性。

Abstract: We present DistillKac, a fast image generator that uses the damped wave
equation and its stochastic Kac representation to move probability mass at
finite speed. In contrast to diffusion models whose reverse time velocities can
become stiff and implicitly allow unbounded propagation speed, Kac dynamics
enforce finite speed transport and yield globally bounded kinetic energy.
Building on this structure, we introduce classifier-free guidance in velocity
space that preserves square integrability under mild conditions. We then
propose endpoint only distillation that trains a student to match a frozen
teacher over long intervals. We prove a stability result that promotes
supervision at the endpoints to closeness along the entire path. Experiments
demonstrate DistillKac delivers high quality samples with very few function
evaluations while retaining the numerical stability benefits of finite speed
probability flows.

</details>


### [97] [Machine Learning. The Science of Selection under Uncertainty](https://arxiv.org/abs/2509.21547)
*Yevgeny Seldin*

Main category: cs.LG

TL;DR: 介绍机器学习中选择过程因数据采样随机性存在不确定性，本书提供统计工具获理论保证，涵盖多种不等式、离线和在线学习工具。


<details>
  <summary>Details</summary>
Motivation: 机器学习选择过程因数据采样随机，经验估计有噪声，需统计工具获不确定性选择结果的理论保证。

Method: 先介绍集中不等式，接着研究经典离线监督学习并提供泛化界推导工具，最后研究在线学习并给出后悔界推导工具。

Result: 提供了涵盖多种不等式、离线和在线学习中推导泛化界和后悔界的工具。

Conclusion: 本书提供的统计工具可用于处理机器学习中不确定性选择问题，在离线和在线学习场景均有应用。

Abstract: Learning, whether natural or artificial, is a process of selection. It starts
with a set of candidate options and selects the more successful ones. In the
case of machine learning the selection is done based on empirical estimates of
prediction accuracy of candidate prediction rules on some data. Due to
randomness of data sampling the empirical estimates are inherently noisy,
leading to selection under uncertainty. The book provides statistical tools to
obtain theoretical guarantees on the outcome of selection under uncertainty. We
start with concentration of measure inequalities, which are the main
statistical instrument for controlling how much an empirical estimate of
expectation of a function deviates from the true expectation. The book covers a
broad range of inequalities, including Markov's, Chebyshev's, Hoeffding's,
Bernstein's, Empirical Bernstein's, Unexpected Bernstein's, kl, and split-kl.
We then study the classical (offline) supervised learning and provide a range
of tools for deriving generalization bounds, including Occam's razor,
Vapnik-Chervonenkis analysis, and PAC-Bayesian analysis. The latter is further
applied to derive generalization guarantees for weighted majority votes. After
covering the offline setting, we turn our attention to online learning. We
present the space of online learning problems characterized by environmental
feedback, environmental resistance, and structural complexity. A common
performance measure in online learning is regret, which compares performance of
an algorithm to performance of the best prediction rule in hindsight, out of a
restricted set of prediction rules. We present tools for deriving regret bounds
in stochastic and adversarial environments, and under full information and
bandit feedback.

</details>


### [98] [Interpretable time series analysis with Gumbel dynamics](https://arxiv.org/abs/2509.21578)
*Yiliu Wang,Timothy Doyeon Kim,Eric Shea-Brown,Uygar Sümbül*

Main category: cs.LG

TL;DR: 提出Gumbel动力模型（GDM），解决切换动力系统问题，在模拟和真实数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有切换动力系统模型难以捕捉平滑、变速过渡及重叠状态的随机混合，在真实数据集上推断的动力学常出现虚假快速切换。

Method: 引入离散状态的连续松弛和基于Gumbel分布在松弛离散状态空间定义的噪声模型，使模型可完全微分，用标准梯度下降法训练。

Result: 在标准模拟数据集验证了模型模拟软、粘性状态和过渡的能力，在两个真实数据集展示了推断随机时间序列中可解释状态的能力。

Conclusion: GDM能更忠实地近似更平滑和非平稳的真实动力学，可在传统方法常失败的场景中推断可解释状态。

Abstract: Switching dynamical systems can model complicated time series data while
maintaining interpretability by inferring a finite set of dynamics primitives
and explaining different portions of the observed time series with one of these
primitives. However, due to the discrete nature of this set, such models
struggle to capture smooth, variable-speed transitions, as well as stochastic
mixtures of overlapping states, and the inferred dynamics often display
spurious rapid switching on real-world datasets. Here, we propose the Gumbel
Dynamical Model (GDM). First, by introducing a continuous relaxation of
discrete states and a different noise model defined on the relaxed-discrete
state space via the Gumbel distribution, GDM expands the set of available state
dynamics, allowing the model to approximate smoother and non-stationary
ground-truth dynamics more faithfully. Second, the relaxation makes the model
fully differentiable, enabling fast and scalable training with standard
gradient descent methods. We validate our approach on standard simulation
datasets and highlight its ability to model soft, sticky states and transitions
in a stochastic setting. Furthermore, we apply our model to two real-world
datasets, demonstrating its ability to infer interpretable states in stochastic
time series with multiple dynamics, a setting where traditional methods often
fail.

</details>


### [99] [GenUQ: Predictive Uncertainty Estimates via Generative Hyper-Networks](https://arxiv.org/abs/2509.21605)
*Tian Yu Yen,Reese E. Jones,Ravi G. Patel*

Main category: cs.LG

TL;DR: 本文介绍了一种名为GenUQ的测度论方法用于算子学习中的不确定性量化，避免构建似然函数，在三个示例问题中表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有将不确定性量化集成到算子模型的方法依赖基于似然的方法，而随机算子可能难以或无法构建似然，因此需要新方法。

Method: 引入生成式超网络模型，避免构建似然函数，产生与观测数据一致的参数分布。

Result: GenUQ在恢复人造算子、学习随机椭圆偏微分方程的解算子和模拟多孔钢拉伸失效位置这三个示例问题中表现优于其他UQ方法。

Conclusion: GenUQ是一种有效的算子学习不确定性量化方法。

Abstract: Operator learning is a recently developed generalization of regression to
mappings between functions. It promises to drastically reduce expensive
numerical integration of PDEs to fast evaluations of mappings between
functional states of a system, i.e., surrogate and reduced-order modeling.
Operator learning has already found applications in several areas such as
modeling sea ice, combustion, and atmospheric physics. Recent approaches
towards integrating uncertainty quantification into the operator models have
relied on likelihood based methods to infer parameter distributions from noisy
data. However, stochastic operators may yield actions from which a likelihood
is difficult or impossible to construct. In this paper, we introduce, GenUQ, a
measure-theoretic approach to UQ that avoids constructing a likelihood by
introducing a generative hyper-network model that produces parameter
distributions consistent with observed data. We demonstrate that GenUQ
outperforms other UQ methods in three example problems, recovering a
manufactured operator, learning the solution operator to a stochastic elliptic
PDE, and modeling the failure location of porous steel under tension.

</details>


### [100] [DriftLite: Lightweight Drift Control for Inference-Time Scaling of Diffusion Models](https://arxiv.org/abs/2509.21655)
*Yinuo Ren,Wenhao Gao,Lexing Ying,Grant M. Rotskoff,Jiequn Han*

Main category: cs.LG

TL;DR: 提出轻量级无训练粒子法DriftLite用于扩散模型推理时缩放，实验显示能降方差、提质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于引导的方法有偏差，基于粒子的修正存在权重退化和高计算成本问题，需新方法实现预训练模型在推理时适应新目标分布。

Method: 引入DriftLite，利用福克 - 普朗克方程中漂移和粒子势间未被探索的自由度，有VCG/ECG两种实例。

Result: 在高斯混合模型、粒子系统和大规模蛋白质 - 配体共折叠问题上，相比纯引导和序贯蒙特卡罗基线，DriftLite能降低方差、提高样本质量。

Conclusion: DriftLite为扩散模型可扩展的推理时自适应提供了有原则、高效的途径。

Abstract: We study inference-time scaling for diffusion models, where the goal is to
adapt a pre-trained model to new target distributions without retraining.
Existing guidance-based methods are simple but introduce bias, while
particle-based corrections suffer from weight degeneracy and high computational
cost. We introduce DriftLite, a lightweight, training-free particle-based
approach that steers the inference dynamics on the fly with provably optimal
stability control. DriftLite exploits a previously unexplored degree of freedom
in the Fokker-Planck equation between the drift and particle potential, and
yields two practical instantiations: Variance- and Energy-Controlling Guidance
(VCG/ECG) for approximating the optimal drift with minimal overhead. Across
Gaussian mixture models, particle systems, and large-scale protein-ligand
co-folding problems, DriftLite consistently reduces variance and improves
sample quality over pure guidance and sequential Monte Carlo baselines. These
results highlight a principled, efficient route toward scalable inference-time
adaptation of diffusion models.

</details>


### [101] [Differentiable Structure Learning for General Binary Data](https://arxiv.org/abs/2509.21658)
*Chang Deng,Bryon Aragam*

Main category: cs.LG

TL;DR: 现有离散数据可微结构学习方法有局限，提出能捕捉任意依赖关系的框架，实证有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设可能不符真实数据生成过程，且忽略复杂依赖结构、仅考虑线性效应，限制了适用性。

Method: 提出可微结构学习框架，将学习问题表述为最一般形式的单个可微优化任务。

Result: 能刻画兼容参数和结构的完整集合，在温和假设下可达马尔可夫等价可识别性，实证表明能有效捕捉离散数据复杂关系。

Conclusion: 所提方法可有效处理离散数据，避免了以往方法不切实际的简化。

Abstract: Existing methods for differentiable structure learning in discrete data
typically assume that the data are generated from specific structural equation
models. However, these assumptions may not align with the true data-generating
process, which limits the general applicability of such methods. Furthermore,
current approaches often ignore the complex dependence structure inherent in
discrete data and consider only linear effects. We propose a differentiable
structure learning framework that is capable of capturing arbitrary
dependencies among discrete variables. We show that although general discrete
models are unidentifiable from purely observational data, it is possible to
characterize the complete set of compatible parameters and structures.
Additionally, we establish identifiability up to Markov equivalence under mild
assumptions. We formulate the learning problem as a single differentiable
optimization task in the most general form, thereby avoiding the unrealistic
simplifications adopted by previous methods. Empirical results demonstrate that
our approach effectively captures complex relationships in discrete data.

</details>


### [102] [A Systematic Review of Conformal Inference Procedures for Treatment Effect Estimation: Methods and Challenges](https://arxiv.org/abs/2509.21660)
*Pascal Memmesheimer,Vincent Heuveline,Jürgen Hesser*

Main category: cs.LG

TL;DR: 文章对治疗效果估计的保形预测方法进行系统综述，分析关键论文并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 灵活机器学习模型在估计异质治疗效果时难以量化点预测的不确定性，保形预测的进展有改善决策的潜力，故进行系统综述。

Method: 通过系统过滤过程，选择并分析十一篇关键论文。

Result: 确定并描述了该领域当前的先进方法。

Conclusion: 基于研究结果提出了未来研究的方向。

Abstract: Treatment effect estimation is essential for informed decision-making in many
fields such as healthcare, economics, and public policy. While flexible machine
learning models have been widely applied for estimating heterogeneous treatment
effects, quantifying the inherent uncertainty of their point predictions
remains an issue. Recent advancements in conformal prediction address this
limitation by allowing for inexpensive computation, as well as distribution
shifts, while still providing frequentist, finite-sample coverage guarantees
under minimal assumptions for any point-predictor model. This advancement holds
significant potential for improving decision-making in especially high-stakes
environments. In this work, we perform a systematic review regarding conformal
prediction methods for treatment effect estimation and provide for both the
necessary theoretical background. Through a systematic filtering process, we
select and analyze eleven key papers, identifying and describing current
state-of-the-art methods in this area. Based on our findings, we propose
directions for future research.

</details>


### [103] [Beyond Johnson-Lindenstrauss: Uniform Bounds for Sketched Bilinear Forms](https://arxiv.org/abs/2509.21847)
*Rohan Deb,Qiaobo Li,Mayank Shrivastava,Arindam Banerjee*

Main category: cs.LG

TL;DR: 本文提出分析草图化双线性形式的通用框架，推导统一界，扩展结果并应用于联邦学习和多臂老虎机算法。


<details>
  <summary>Details</summary>
Motivation: 现有统一界对草图化双线性形式不适用或不精确，需要新方法分析。

Method: 依靠通用链并引入处理集合对的上确界的新技术。

Result: 推导出与相关集合几何复杂度相关的统一界，结果扩展到涉及T个独立草图矩阵的情况，偏差与√T成比例。恢复已知结果并扩展RIP类保证。在草图化联邦学习算法中获得改进的收敛界，设计出具有更优遗憾界的多臂老虎机算法草图变体。

Conclusion: 提出的通用框架能有效分析草图化双线性形式，在多个领域有应用价值。

Abstract: Uniform bounds on sketched inner products of vectors or matrices underpin
several important computational and statistical results in machine learning and
randomized algorithms, including the Johnson-Lindenstrauss (J-L) lemma, the
Restricted Isometry Property (RIP), randomized sketching, and approximate
linear algebra. However, many modern analyses involve *sketched bilinear
forms*, for which existing uniform bounds either do not apply or are not sharp
on general sets. In this work, we develop a general framework to analyze such
sketched bilinear forms and derive uniform bounds in terms of geometric
complexities of the associated sets. Our approach relies on generic chaining
and introduces new techniques for handling suprema over pairs of sets. We
further extend these results to the setting where the bilinear form involves a
sum of $T$ independent sketching matrices and show that the deviation scales as
$\sqrt{T}$. This unified analysis recovers known results such as the J-L lemma
as special cases, while extending RIP-type guarantees. Additionally, we obtain
improved convergence bounds for sketched Federated Learning algorithms where
such cross terms arise naturally due to sketched gradient compression, and
design sketched variants of bandit algorithms with sharper regret bounds that
depend on the geometric complexity of the action and parameter sets, rather
than the ambient dimension.

</details>


### [104] [Why High-rank Neural Networks Generalize?: An Algebraic Framework with RKHSs](https://arxiv.org/abs/2509.21895)
*Yuka Hashimoto,Sho Sonoda,Isao Ishikawa,Masahiro Ikeda*

Main category: cs.LG

TL;DR: 本文使用Koopman算子、群表示和再生核希尔伯特空间推导出新的深度神经网络Rademacher复杂度界，适用于更广泛模型。


<details>
  <summary>Details</summary>
Motivation: 现有描述高秩权重矩阵模型泛化良好现象的复杂度界适用模型类型有限，需推导适用于更广泛现实模型的界。

Method: 引入神经网络的代数表示和核函数来构建再生核希尔伯特空间。

Result: 得到了适用于更广泛现实模型的Rademacher复杂度界。

Conclusion: 为基于Koopman的Rademacher复杂度界理论在更实际场景应用奠定基础。

Abstract: We derive a new Rademacher complexity bound for deep neural networks using
Koopman operators, group representations, and reproducing kernel Hilbert spaces
(RKHSs). The proposed bound describes why the models with high-rank weight
matrices generalize well. Although there are existing bounds that attempt to
describe this phenomenon, these existing bounds can be applied to limited types
of models. We introduce an algebraic representation of neural networks and a
kernel function to construct an RKHS to derive a bound for a wider range of
realistic models. This work paves the way for the Koopman-based theory for
Rademacher complexity bounds to be valid for more practical situations.

</details>


### [105] [Discrete Guidance Matching: Exact Guidance for Discrete Flow Matching](https://arxiv.org/abs/2509.21912)
*Zhengyan Wan,Yidong Ouyang,Liyan Xie,Fang Fang,Hongyuan Zha,Guang Cheng*

Main category: cs.LG

TL;DR: 提出用于离散数据的新型引导框架，仅需单步前向传播，效率高，可应用于多种任务。


<details>
  <summary>Details</summary>
Motivation: 现有离散数据引导方法用一阶泰勒近似，在离散状态空间近似误差大，需更好方法。

Method: 根据学习的离散流匹配模型推导所需分布的精确转移率，提出新型引导框架。

Result: 该框架通用，涵盖现有引导方法，可用于掩码扩散模型，在多个任务中证明有效。

Conclusion: 提出的新型引导框架高效且通用，能提升离散数据后验采样效率。

Abstract: Guidance provides a simple and effective framework for posterior sampling by
steering the generation process towards the desired distribution. When modeling
discrete data, existing approaches mostly focus on guidance with the
first-order Taylor approximation to improve the sampling efficiency. However,
such an approximation is inappropriate in discrete state spaces since the
approximation error could be large. A novel guidance framework for discrete
data is proposed to address this problem: We derive the exact transition rate
for the desired distribution given a learned discrete flow matching model,
leading to guidance that only requires a single forward pass in each sampling
step, significantly improving efficiency. This unified novel framework is
general enough, encompassing existing guidance methods as special cases, and it
can also be seamlessly applied to the masked diffusion model. We demonstrate
the effectiveness of our proposed guidance on energy-guided simulations and
preference alignment on text-to-image generation and multimodal understanding
tasks. The code is available through
https://github.com/WanZhengyan/Discrete-Guidance-Matching/tree/main.

</details>


### [106] [Convexity-Driven Projection for Point Cloud Dimensionality Reduction](https://arxiv.org/abs/2509.22043)
*Suman Sanyal*

Main category: cs.LG

TL;DR: 提出边界无关线性降维方法CDP，能保留局部非凸性，有验证保证和评估协议。


<details>
  <summary>Details</summary>
Motivation: 为点云降维提供一种能保留绕路引起的局部非凸性的方法。

Method: 构建k - NN图，识别欧氏距离与最短路径比低于阈值的可允许对，聚合其归一化方向形成半正定非凸结构矩阵，用矩阵前k个特征向量投影。

Result: 给出两个可验证保证，一是对可允许对投影后失真的成对事后证书，二是将预期捕获方向能量与结构矩阵频谱联系起来的平均情况谱界。

Conclusion: 评估协议可让从业者检查数据上的保证。

Abstract: We propose Convexity-Driven Projection (CDP), a boundary-free linear method
for dimensionality reduction of point clouds that targets preserving
detour-induced local non-convexity. CDP builds a $k$-NN graph, identifies
admissible pairs whose Euclidean-to-shortest-path ratios are below a threshold,
and aggregates their normalized directions to form a positive semidefinite
non-convexity structure matrix. The projection uses the top-$k$ eigenvectors of
the structure matrix. We give two verifiable guarantees. A pairwise
a-posteriori certificate that bounds the post-projection distortion for each
admissible pair, and an average-case spectral bound that links expected
captured direction energy to the spectrum of the structure matrix, yielding
quantile statements for typical distortion. Our evaluation protocol reports
fixed- and reselected-pairs detour errors and certificate quantiles, enabling
practitioners to check guarantees on their data.

</details>


### [107] [SHAKE-GNN: Scalable Hierarchical Kirchhoff-Forest Graph Neural Network](https://arxiv.org/abs/2509.22100)
*Zhipu Cui,Johannes Lutzeyer*

Main category: cs.LG

TL;DR: 提出SHAKE - GNN框架解决GNN扩展到大型图的挑战，在多基准测试中性能有竞争力且可扩展性好。


<details>
  <summary>Details</summary>
Motivation: 解决将GNN扩展到大型图，特别是图级任务的挑战。

Method: 引入基于Kirchhoff Forests层次结构的SHAKE - GNN框架，产生多尺度表示，采用改进的数据驱动策略选择权衡参数并分析时间复杂度。

Result: 在多个大规模图分类基准测试中，SHAKE - GNN实现了有竞争力的性能并提高了可扩展性。

Conclusion: SHAKE - GNN是一种有效的可扩展图级GNN框架。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across a range
of learning tasks. However, scaling GNNs to large graphs remains a significant
challenge, especially for graph-level tasks. In this work, we introduce
SHAKE-GNN, a novel scalable graph-level GNN framework based on a hierarchy of
Kirchhoff Forests, a class of random spanning forests used to construct
stochastic multi-resolution decompositions of graphs. SHAKE-GNN produces
multi-scale representations, enabling flexible trade-offs between efficiency
and performance. We introduce an improved, data-driven strategy for selecting
the trade-off parameter and analyse the time-complexity of SHAKE-GNN.
Experimental results on multiple large-scale graph classification benchmarks
demonstrate that SHAKE-GNN achieves competitive performance while offering
improved scalability.

</details>


### [108] [Mechanistic Independence: A Principle for Identifiable Disentangled Representations](https://arxiv.org/abs/2509.22196)
*Stefan Matthes,Zhiwei Han,Hao Shen*

Main category: cs.LG

TL;DR: 本文引入统一框架，通过机制独立性实现解纠缠，提出相关独立性准则并证明其可识别潜在子空间，明确了解纠缠表示可识别的条件。


<details>
  <summary>Details</summary>
Motivation: 解纠缠表示的可识别性尚未完全理解，需要进一步研究。

Method: 引入基于机制独立性的统一框架，提出从支持度、稀疏性到高阶条件的相关独立性准则。

Result: 各准则可在非线性、非可逆混合下实现潜在子空间的可识别性，建立准则层次关系并给出图论表征。

Conclusion: 明确了不依赖统计假设的解纠缠表示可识别条件。

Abstract: Disentangled representations seek to recover latent factors of variation
underlying observed data, yet their identifiability is still not fully
understood. We introduce a unified framework in which disentanglement is
achieved through mechanistic independence, which characterizes latent factors
by how they act on observed variables rather than by their latent distribution.
This perspective is invariant to changes of the latent density, even when such
changes induce statistical dependencies among factors. Within this framework,
we propose several related independence criteria -- ranging from support-based
and sparsity-based to higher-order conditions -- and show that each yields
identifiability of latent subspaces, even under nonlinear, non-invertible
mixing. We further establish a hierarchy among these criteria and provide a
graph-theoretic characterization of latent subspaces as connected components.
Together, these results clarify the conditions under which disentangled
representations can be identified without relying on statistical assumptions.

</details>


### [109] [Global Convergence in Neural ODEs: Impact of Activation Functions](https://arxiv.org/abs/2509.22436)
*Tianxiang Gao,Siyuan Sun,Hailiang Liu,Hongyang Gao*

Main category: cs.LG

TL;DR: 本文研究激活函数对神经常微分方程（Neural ODEs）训练的影响，建立了其在梯度下降下的全局收敛性，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: Neural ODEs因连续性质和参数共享效率在多领域成功，但这些特性也给训练带来梯度计算精度和收敛分析的挑战。

Method: 研究激活函数的影响，分析其平滑性和非线性对训练动态的作用。

Result: 平滑激活函数保证前后向ODE全局唯一解，足够的非线性对训练中保持神经切线核（NTK）谱特性至关重要，建立了Neural ODEs在过参数化机制下梯度下降的全局收敛性。

Conclusion: 理论发现经数值实验验证，为扩展Neural ODEs提供实用指南，有望加速训练并提升实际应用性能。

Abstract: Neural Ordinary Differential Equations (ODEs) have been successful in various
applications due to their continuous nature and parameter-sharing efficiency.
However, these unique characteristics also introduce challenges in training,
particularly with respect to gradient computation accuracy and convergence
analysis. In this paper, we address these challenges by investigating the
impact of activation functions. We demonstrate that the properties of
activation functions, specifically smoothness and nonlinearity, are critical to
the training dynamics. Smooth activation functions guarantee globally unique
solutions for both forward and backward ODEs, while sufficient nonlinearity is
essential for maintaining the spectral properties of the Neural Tangent Kernel
(NTK) during training. Together, these properties enable us to establish the
global convergence of Neural ODEs under gradient descent in overparameterized
regimes. Our theoretical findings are validated by numerical experiments, which
not only support our analysis but also provide practical guidelines for scaling
Neural ODEs, potentially leading to faster training and improved performance in
real-world applications.

</details>


### [110] [A Theoretical Analysis of Discrete Flow Matching Generative Models](https://arxiv.org/abs/2509.22623)
*Maojiang Su,Mingcheng Lu,Jerry Yao-Chieh Hu,Shang Wu,Zhao Song,Alex Reneau,Han Liu*

Main category: cs.LG

TL;DR: 对端到端训练离散流匹配（DFM）生成模型进行理论分析，证明训练集增大时DFM模型生成分布收敛到真实数据分布。


<details>
  <summary>Details</summary>
Motivation: 对DFM生成模型进行理论分析，明确其最终分布估计误差的保证。

Method: 分解最终分布估计误差，证明生成分布与目标分布的总变差距离受学习速度场风险控制，分析该风险的近似误差和估计误差。

Result: 得到Transformer架构表示真实速度的能力量化，以及有限数据集训练误差的统计收敛率。

Conclusion: 首次正式证明训练的DFM模型生成的分布随训练集大小增加收敛到真实数据分布。

Abstract: We provide a theoretical analysis for end-to-end training Discrete Flow
Matching (DFM) generative models. DFM is a promising discrete generative
modeling framework that learns the underlying generative dynamics by training a
neural network to approximate the transformative velocity field. Our analysis
establishes a clear chain of guarantees by decomposing the final distribution
estimation error. We first prove that the total variation distance between the
generated and target distributions is controlled by the risk of the learned
velocity field. We then bound this risk by analyzing its two primary sources:
(i) Approximation Error, where we quantify the capacity of the Transformer
architecture to represent the true velocity, and (ii) Estimation Error, where
we derive statistical convergence rates that bound the error from training on a
finite dataset. By composing these results, we provide the first formal proof
that the distribution generated by a trained DFM model provably converges to
the true data distribution as the training set size increases.

</details>


### [111] [Discovering and Analyzing Stochastic Processes to Reduce Waste in Food Retail](https://arxiv.org/abs/2509.21322)
*Anna Kalenkova,Lu Xia,Dirk Neumann*

Main category: cs.LG

TL;DR: 提出用集成OCPM与随机过程发现分析的方法分析食品零售流程以减少食物浪费，经多步骤分析实现供需平衡。


<details>
  <summary>Details</summary>
Motivation: 减少食品零售过程中的食物浪费。

Method: 将对象中心过程挖掘（OCPM）与随机过程发现和分析相结合，从杂货店销售数据发现连续时间马尔可夫链形式的随机过程，扩展供应活动，进行假设分析。

Result: 能评估商店产品数量随时间的变化，确定顾客购买行为和供应策略的最佳平衡。

Conclusion: 该方法有助于防止因供应过剩导致的食物浪费和产品短缺。

Abstract: This paper proposes a novel method for analyzing food retail processes with a
focus on reducing food waste. The approach integrates object-centric process
mining (OCPM) with stochastic process discovery and analysis. First, a
stochastic process in the form of a continuous-time Markov chain is discovered
from grocery store sales data. This model is then extended with supply
activities. Finally, a what-if analysis is conducted to evaluate how the
quantity of products in the store evolves over time. This enables the
identification of an optimal balance between customer purchasing behavior and
supply strategies, helping to prevent both food waste due to oversupply and
product shortages.

</details>


### [112] [Impact of Loss Weight and Model Complexity on Physics-Informed Neural Networks for Computational Fluid Dynamics](https://arxiv.org/abs/2509.21393)
*Yi En Chou,Te Hsin Liu,Chao An Lin*

Main category: cs.LG

TL;DR: 提出二维分析加权方案提升Physics Informed Neural Networks解决PDEs的稳定性和准确性


<details>
  <summary>Details</summary>
Motivation: Physics Informed Neural Networks对损失权重选择高度敏感

Method: 提出基于可量化项和结合不可量化项的两种二维分析加权方案

Result: 第二种方案在基准测试中持续提升稳定性和准确性，在高Peclet数对流扩散问题中能实现稳定准确预测

Conclusion: 所提方案在CFD问题中具有鲁棒性和通用性

Abstract: Physics Informed Neural Networks offer a mesh free framework for solving PDEs
but are highly sensitive to loss weight selection. We propose two dimensional
analysis based weighting schemes, one based on quantifiable terms, and another
also incorporating unquantifiable terms for more balanced training. Benchmarks
on heat conduction, convection diffusion, and lid driven cavity flows show that
the second scheme consistently improves stability and accuracy over equal
weighting. Notably, in high Peclet number convection diffusion, where
traditional solvers fail, PINNs with our scheme achieve stable, accurate
predictions, highlighting their robustness and generalizability in CFD
problems.

</details>


### [113] [LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?](https://arxiv.org/abs/2509.21403)
*Rushil Gupta,Jason Hartford,Bang Liu*

Main category: cs.LG

TL;DR: 评估大语言模型（LLMs）用于实验设计的假设，发现其对实验反馈不敏感，经典方法表现更优，提出混合方法LLMNN表现良好，表明LLMs在实践中不能进行上下文实验设计。


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型可进行上下文实验设计的假设。

Method: 使用开源和闭源指令微调的LLMs对基因扰动和分子性质发现任务进行评估，提出LLM-guided Nearest Neighbour（LLMNN）采样混合方法。

Result: LLM-based agents对实验反馈不敏感，经典方法表现优于LLM agents，LLMNN在各领域表现有竞争力或更优。

Conclusion: 当前开源和闭源LLMs在实践中不能进行上下文实验设计，需要混合框架将基于先验的推理与更新后验的批量获取分离。

Abstract: Large language models (LLMs) have recently been proposed as general-purpose
agents for experimental design, with claims that they can perform in-context
experimental design. We evaluate this hypothesis using both open- and
closed-source instruction-tuned LLMs applied to genetic perturbation and
molecular property discovery tasks. We find that LLM-based agents show no
sensitivity to experimental feedback: replacing true outcomes with randomly
permuted labels has no impact on performance. Across benchmarks, classical
methods such as linear bandits and Gaussian process optimization consistently
outperform LLM agents. We further propose a simple hybrid method, LLM-guided
Nearest Neighbour (LLMNN) sampling, that combines LLM prior knowledge with
nearest-neighbor sampling to guide the design of experiments. LLMNN achieves
competitive or superior performance across domains without requiring
significant in-context adaptation. These results suggest that current open- and
closed-source LLMs do not perform in-context experimental design in practice
and highlight the need for hybrid frameworks that decouple prior-based
reasoning from batch acquisition with updated posteriors.

</details>


### [114] [Object Identification Under Known Dynamics: A PIRNN Approach for UAV Classification](https://arxiv.org/abs/2509.21405)
*Nyi Nyi Aung,Neil Muralles,Adrian Stein*

Main category: cs.LG

TL;DR: 本文提出物理信息残差神经网络结合学习与分类，用于无人机已知动力学下的目标识别，案例研究显示高准确率和短训练时间。


<details>
  <summary>Details</summary>
Motivation: 解决无人机应用中已知动力学下的目标识别问题。

Method: 提出物理信息残差神经网络框架，利用物理信息学习进行状态映射和状态导数预测，用softmax层进行多类置信度估计。

Result: 案例研究表明具有高分类准确率和减少的训练时间。

Conclusion: 为动力学已知领域的系统识别问题提供了有前景的解决方案。

Abstract: This work addresses object identification under known dynamics in unmanned
aerial vehicle applications, where learning and classification are combined
through a physics-informed residual neural network. The proposed framework
leverages physics-informed learning for state mapping and state-derivative
prediction, while a softmax layer enables multi-class confidence estimation.
Quadcopter, fixed-wing, and helicopter aerial vehicles are considered as case
studies. The results demonstrate high classification accuracy with reduced
training time, offering a promising solution for system identification problems
in domains where the underlying dynamics are well understood.

</details>


### [115] [Null-Space Filtering for Data-Free Continual Model Merging: Preserving Transparency, Promoting Fidelity](https://arxiv.org/abs/2509.21413)
*Zihuan Qiu,Lei Wang,Yang Cao,Runtong Zhang,Bing Su,Yi Xu,Fanman Meng,Linfeng Xu,Qingbo Wu,Hongliang Li*

Main category: cs.LG

TL;DR: 提出数据无免费持续模型合并框架NUFILT，通过过滤和适应过程让骨干吸收新知识并保留旧行为，在理论和实验上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有数据无免费持续模型合并方法难以在无任务数据时确保透明度和保真度，需解决数据层面需求与参数空间优化的衔接问题。

Method: 提出NUFILT框架，设计零空间投影器确保透明度，轻量级LoRA适配器确保保真度，用基于投影的替代损失训练适配器，最后以逐层线性方式融合更新。

Result: 理论上建立近似子空间对齐保证，实验上在视觉和NLP基准测试中实现最先进性能，最小化遗忘，比OPCM和WUDI - Merging平均准确率提高4 - 7%。

Conclusion: NUFILT能有效解决数据无免费持续模型合并中的问题，在性能和计算开销上有优势。

Abstract: Data-free continual model merging (DFCMM) aims to fuse independently
fine-tuned models into a single backbone that evolves with incoming tasks
without accessing task data. This paper formulate two fundamental desiderata
for DFCMM: transparency, avoiding interference with earlier tasks, and
fidelity, adapting faithfully to each new task. This poses a challenge that
existing approaches fail to address: how to bridge data-level desiderata with
parameter-space optimization to ensure transparency and fidelity in the absence
of task data. To this end, we propose NUFILT (NUll-space FILTering), a
data-free framework that directly links these desiderata to optimization. Our
key observation is that task vectors approximately align with representation
subspaces, providing structural surrogates for enforcing transparency and
fidelity. Accordingly, we design a null-space projector that preserves prior
responses by filtering out overlapping components of new task vectors, thereby
ensuring transparency, and a lightweight LoRA adapter that injects
complementary task-specific signals, enabling fidelity in adapting to new
tasks. The adapter is trained with a projection-based surrogate loss to retain
consistency with previous knowledge while introducing novel directions. This
joint filtering-adaptation process allows the backbone to absorb new knowledge
while retaining existing behaviors, and the updates are finally fused back in a
layer-wise linear fashion without extra parameters or inference cost.
Theoretically, we establish approximate subspace alignment guarantees that
justify null-space filtering. Empirically, NUFILT achieves state-of-the-art
performance with minimal forgetting on both vision and NLP benchmarks,
improving average accuracy by 4-7% over OPCM and WUDI-Merging, while narrowing
the gap to fine-tuning and reducing computation overhead.

</details>


### [116] [Forecasting Seismic Waveforms: A Deep Learning Approach for Einstein Telescope](https://arxiv.org/abs/2509.21446)
*Waleed Esmail,Alexander Kappes,Stuart Russell,Christine Thomas*

Main category: cs.LG

TL;DR: 介绍基于Transformer的SeismoGPT模型用于未来引力波探测器地震波形预测，能学习时空依赖，结果表现良好，为数据驱动地震预测奠基。


<details>
  <summary>Details</summary>
Motivation: 在未来引力波探测器背景下实现准确的地震波形预测，以支持牛顿噪声缓解和实时天文台控制。

Method: 训练基于Transformer的SeismoGPT模型，在自回归设置下操作，可处理单站和阵列输入，直接从波形数据学习时空依赖。

Result: 模型在短期预测窗口内表现良好，随预测时间延长性能逐渐下降。

Conclusion: 该方法为数据驱动的地震预测奠定基础。

Abstract: We introduce \textit{SeismoGPT}, a transformer-based model for forecasting
three-component seismic waveforms in the context of future gravitational wave
detectors like the Einstein Telescope. The model is trained in an
autoregressive setting and can operate on both single-station and array-based
inputs. By learning temporal and spatial dependencies directly from waveform
data, SeismoGPT captures realistic ground motion patterns and provides accurate
short-term forecasts. Our results show that the model performs well within the
immediate prediction window and gradually degrades further ahead, as expected
in autoregressive systems. This approach lays the groundwork for data-driven
seismic forecasting that could support Newtonian noise mitigation and real-time
observatory control.

</details>


### [117] [Talking Trees: Reasoning-Assisted Induction of Decision Trees for Tabular Data](https://arxiv.org/abs/2509.21465)
*George Yakushev,Alina Shutova,Ivan Rubachev,Renat Sergazinov,Artem Babenko*

Main category: cs.LG

TL;DR: 本文探索用有推理能力的大语言模型为小表格数据集诱导决策树，设计工具创建轻量级决策树，在低资源表格问题上表现较好且有可读推理轨迹。


<details>
  <summary>Details</summary>
Motivation: 表格基础模型预训练后难解释且推理成本高，需探索替代策略。

Method: 设计构建、分析和操作决策树的工具，让大语言模型结合先验知识和数据学习创建决策树。

Result: 创建的轻量级决策树在低资源表格问题上优于传统CART，有可读推理轨迹。

Conclusion: 虽单棵决策树不如黑盒模型，但有可检查的推理轨迹，且允许人工干预。

Abstract: Tabular foundation models are becoming increasingly popular for low-resource
tabular problems. These models make up for small training datasets by
pretraining on large volumes of synthetic data. The prior knowledge obtained
via pretraining provides the exceptional performance, but the resulting model
becomes a black box that is difficult to interpret and costly to inference. In
this work, we explore an alternative strategy: using reasoning-capable LLMs to
induce decision trees for small tabular datasets in agentic setup. We design a
minimal set of tools for constructing, analyzing and manipulating decision
trees. By using these tools, LLMs combine their prior knowledge with learning
from data to create a lightweight decision tree that outperforms traditional
CART on low-resource tabular problems. While a single decision tree does not
outperform state-of-the-art black box models, it comes with a human-readable
reasoning trace that can be checked for biases and data leaks. Furthermore, the
reasoning-based LLM's creation process allows for additional human input:
correcting biases or incorporating domain-specific intuition that is not
captured in the data.

</details>


### [118] [Score-based Idempotent Distillation of Diffusion Models](https://arxiv.org/abs/2509.21470)
*Shehtab Zaman,Chengyan Liu,Kenneth Chiu*

Main category: cs.LG

TL;DR: 本文提出SIGN方法，结合扩散模型与幂等生成网络，稳定且无需对抗损失，推理快，可多步采样，能零样本编辑输入，在多图像数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统幂等生成网络（IGNs）训练不稳定、易模式崩溃，扩散和基于分数的模型计算成本高，为结合两者优点开展研究。

Method: 通过从扩散模型分数中提取幂等模型，提出SIGN方法，还对基于分数的训练方法进行理论分析。

Result: 实验表明可从预训练扩散模型有效提取IGNs，推理比迭代分数模型快，能多步采样，在多个图像数据集上取得了幂等模型的SOTA结果。

Conclusion: SIGN方法稳定、推理快、可多步采样及零样本编辑输入，在图像数据集上效果好。

Abstract: Idempotent generative networks (IGNs) are a new line of generative models
based on idempotent mapping to a target manifold. IGNs support both single-and
multi-step generation, allowing for a flexible trade-off between computational
cost and sample quality. But similar to Generative Adversarial Networks (GANs),
conventional IGNs require adversarial training and are prone to training
instabilities and mode collapse. Diffusion and score-based models are popular
approaches to generative modeling that iteratively transport samples from one
distribution, usually a Gaussian, to a target data distribution. These models
have gained popularity due to their stable training dynamics and high-fidelity
generation quality. However, this stability and quality come at the cost of
high computational cost, as the data must be transported incrementally along
the entire trajectory. New sampling methods, model distillation, and
consistency models have been developed to reduce the sampling cost and even
perform one-shot sampling from diffusion models. In this work, we unite
diffusion and IGNs by distilling idempotent models from diffusion model scores,
called SIGN. Our proposed method is highly stable and does not require
adversarial losses. We provide a theoretical analysis of our proposed
score-based training methods and empirically show that IGNs can be effectively
distilled from a pre-trained diffusion model, enabling faster inference than
iterative score-based models. SIGNs can perform multi-step sampling, allowing
users to trade off quality for efficiency. These models operate directly on the
source domain; they can project corrupted or alternate distributions back onto
the target manifold, enabling zero-shot editing of inputs. We validate our
models on multiple image datasets, achieving state-of-the-art results for
idempotent models on the CIFAR and CelebA datasets.

</details>


### [119] [d2: Improved Techniques for Training Reasoning Diffusion Language Models](https://arxiv.org/abs/2509.21474)
*Guanghan Wang,Yair Schiff,Gilad Turok,Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: 提出适用于掩码扩散语言模型的推理框架d2，在逻辑和数学推理任务上取得新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 改进扩散语言模型的推理能力，现有通过强化学习改进推理能力的研究尚在活跃发展中。

Method: 引入新的策略梯度算法，利用掩码特性准确估计采样轨迹的似然，该算法以可分析的方式权衡计算与近似精度。

Result: d2仅使用强化学习（不依赖监督微调）显著优于之前的扩散推理框架，在逻辑推理任务和数学推理基准测试中取得新的最优性能。

Conclusion: 所提出的d2框架对于基于扩散的高效推理很关键，能有效提升扩散语言模型的推理能力。

Abstract: While diffusion language models (DLMs) have achieved competitive performance
in text generation, improving their reasoning ability with reinforcement
learning remains an active research area. Here, we introduce d2, a reasoning
framework tailored for masked DLMs. Central to our framework is a new policy
gradient algorithm that relies on properties of masking to accurately estimate
the likelihoods of sampling trajectories. Our estimators trade off computation
for approximation accuracy in an analytically tractable manner, and are
particularly effective for DLMs that support any-order likelihood estimation.
We characterize and study this property in popular DLMs and show that it is key
for efficient diffusion-based reasoning. Empirically, d2 significantly improves
over previous diffusion reasoning frameworks using only RL (without relying on
supervised fine-tuning), and sets a new state-of-the-art performance for DLMs
on logical reasoning tasks (Countdown and Sudoku) and math reasoning benchmarks
(GSM8K and MATH500).

</details>


### [120] [VISION: Prompting Ocean Vertical Velocity Reconstruction from Incomplete Observations](https://arxiv.org/abs/2509.21477)
*Yuan Gao,Hao Wu,Qingsong Wen,Kun Wang,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 本文构建海洋动力学基准KD48，提出基于动态提示的重建范式VISION，实验显示其性能优越，为海洋科学研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 解决从不完全的表面观测重建次表层海洋动力学面临缺乏标准化、可用于分析的基准的问题，推动相关研究。

Method: 构建并发布海洋动力学基准KD48，提出基于动态提示的重建范式VISION，设计状态条件提示模块注入通用主干。

Result: 在KD48基准上的实验表明，VISION大幅超越现有模型，在极端数据缺失场景下有强泛化能力。

Conclusion: 通过提供高质量基准和强大模型，为数据不确定下的海洋科学研究建立了坚实基础。

Abstract: Reconstructing subsurface ocean dynamics, such as vertical velocity fields,
from incomplete surface observations poses a critical challenge in Earth
science, a field long hampered by the lack of standardized, analysis-ready
benchmarks. To systematically address this issue and catalyze research, we
first build and release KD48, a high-resolution ocean dynamics benchmark
derived from petascale simulations and curated with expert-driven denoising.
Building on this benchmark, we introduce VISION, a novel reconstruction
paradigm based on Dynamic Prompting designed to tackle the core problem of
missing data in real-world observations. The essence of VISION lies in its
ability to generate a visual prompt on-the-fly from any available subset of
observations, which encodes both data availability and the ocean's physical
state. More importantly, we design a State-conditioned Prompting module that
efficiently injects this prompt into a universal backbone, endowed with
geometry- and scale-aware operators, to guide its adaptive adjustment of
computational strategies. This mechanism enables VISION to precisely handle the
challenges posed by varying input combinations. Extensive experiments on the
KD48 benchmark demonstrate that VISION not only substantially outperforms
state-of-the-art models but also exhibits strong generalization under extreme
data missing scenarios. By providing a high-quality benchmark and a robust
model, our work establishes a solid infrastructure for ocean science research
under data uncertainty. Our codes are available at:
https://github.com/YuanGao-YG/VISION.

</details>


### [121] [Filtering with Confidence: When Data Augmentation Meets Conformal Prediction](https://arxiv.org/abs/2509.21479)
*Zixuan Wu,So Won Jeong,Yating Liu,Yeo Jin Jung,Claire Donnat*

Main category: cs.LG

TL;DR: 提出共形数据增强方法，利用共形预测生成合成数据并过滤低质量数据，在多任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺和数据密集型模型对数据的需求，控制数据增强中的偏差。

Method: 提出共形数据增强，这是一种基于共形预测的数据过滤框架，无需访问内部模型对数或大规模模型再训练。

Result: 在主题预测、情感分析等多个任务中，F1分数比未增强基线提高达40%，比其他过滤增强基线提高4%。

Conclusion: 共形数据增强方法有效，能在多任务中提升性能。

Abstract: With promising empirical performance across a wide range of applications,
synthetic data augmentation appears a viable solution to data scarcity and the
demands of increasingly data-intensive models. Its effectiveness lies in
expanding the training set in a way that reduces estimator variance while
introducing only minimal bias. Controlling this bias is therefore critical:
effective data augmentation should generate diverse samples from the same
underlying distribution as the training set, with minimal shifts. In this
paper, we propose conformal data augmentation, a principled data filtering
framework that leverages the power of conformal prediction to produce diverse
synthetic data while filtering out poor-quality generations with provable risk
control. Our method is simple to implement, requires no access to internal
model logits, nor large-scale model retraining. We demonstrate the
effectiveness of our approach across multiple tasks, including topic
prediction, sentiment analysis, image classification, and fraud detection,
showing consistent performance improvements of up to 40% in F1 score over
unaugmented baselines, and 4% over other filtered augmentation baselines.

</details>


### [122] [Neural Operators for Mathematical Modeling of Transient Fluid Flow in Subsurface Reservoir Systems](https://arxiv.org/abs/2509.21485)
*Daniil D. Sirota,Sergey A. Khan,Sergey L. Kostikov,Kirill A. Butov*

Main category: cs.LG

TL;DR: 本文提出基于TFNO - opt神经算子架构对地下储层系统瞬态流体流动建模的方法，经实验验证有效，能加速计算，为复杂储层系统控制提供新机遇。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法模拟地下储层系统计算时间成本高，限制其在控制和决策支持问题中的应用。

Method: 提出TFNO - opt架构，基于傅里叶神经算子，对其进行改进，如调整积分傅里叶算子内部时间分辨率、在谱域对参数进行张量分解等。

Result: 计算实验证实改进有效，以地下储气库水动力建模问题为例，相比传统方法计算加速六个数量级。

Conclusion: 该方法为复杂储层系统的有效控制带来新机遇。

Abstract: This paper presents a method for modeling transient fluid flow in subsurface
reservoir systems based on the developed neural operator architecture
(TFNO-opt). Reservoir systems are complex dynamic objects with distributed
parameters described by systems of partial differential equations (PDEs).
Traditional numerical methods for modeling such systems, despite their high
accuracy, are characterized by significant time costs for performing
calculations, which limits their applicability in control and decision support
problems. The proposed architecture (TFNO-opt) is based on Fourier neural
operators, which allow approximating PDE solutions in infinite-dimensional
functional spaces, providing invariance to discretization and the possibility
of generalization to various implementations of equations. The developed
modifications are aimed at increasing the accuracy and stability of the trained
neural operator, which is especially important for control problems. These
include adjustable internal time resolution of the integral Fourier operator,
tensor decomposition of parameters in the spectral domain, use of the Sobolev
norm in the error function, and separation of approximation errors and
reconstruction of initial conditions for more accurate reproduction of physical
processes. The effectiveness of the proposed improvements is confirmed by
computational experiments. The practical significance is confirmed by
computational experiments using the example of the problem of hydrodynamic
modeling of an underground gas storage (UGS), where the acceleration of
calculations by six orders of magnitude was achieved, compared to traditional
methods. This opens up new opportunities for the effective control of complex
reservoir systems.

</details>


### [123] [GraphPFN: A Prior-Data Fitted Graph Foundation Model](https://arxiv.org/abs/2509.21489)
*Dmitry Eremeev,Oleg Platonov,Gleb Bazhenov,Artem Babenko,Liudmila Prokhorenkova*

Main category: cs.LG

TL;DR: 提出GraphPFN用于节点级预测，在合成图上预训练，在真实图数据集表现佳，证明该预训练策略有效。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型依赖手工特征，难以学习复杂图模式，需新方法。

Method: 设计合成属性图先验分布，结合多种随机块模型和优先连接过程生成图结构，用图感知结构化因果模型生成节点属性和目标；用基于注意力的图邻域聚合层增强表格基础模型LimiX并在合成图上训练。

Result: GraphPFN在多样真实图数据集展现强上下文学习能力，微调后达SOTA，多数数据集上超G2T - FM和从头训练的任务特定GNN。

Conclusion: 基于精心设计先验分布的合成图预训练是构建图基础模型的有效策略。

Abstract: Foundation models pretrained on large-scale datasets have transformed such
fields as natural language processing and computer vision, but their
application to graph data remains limited. Recently emerged graph foundation
models, such as G2T-FM, utilize tabular foundation models for graph tasks and
were shown to significantly outperform prior attempts to create GFMs. However,
these models primarily rely on hand-crafted graph features, limiting their
ability to learn complex graph-specific patterns. In this work, we propose
GraphPFN: a prior-data fitted network for node-level prediction. First, we
design a prior distribution of synthetic attributed graphs. For graph structure
generation, we use a novel combination of multiple stochastic block models and
a preferential attachment process. We then apply graph-aware structured causal
models to generate node attributes and targets. This procedure allows us to
efficiently generate a wide range of realistic graph datasets. Then, we augment
the tabular foundation model LimiX with attention-based graph neighborhood
aggregation layers and train it on synthetic graphs sampled from our prior,
allowing the model to capture graph structural dependencies not present in
tabular data. On diverse real-world graph datasets with up to 50,000 nodes,
GraphPFN shows strong in-context learning performance and achieves
state-of-the-art results after finetuning, outperforming both G2T-FM and
task-specific GNNs trained from scratch on most datasets. More broadly, our
work demonstrates that pretraining on synthetic graphs from a well-designed
prior distribution is an effective strategy for building graph foundation
models.

</details>


### [124] [SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models](https://arxiv.org/abs/2509.21498)
*Arani Roy,Shristi Das Biswas,Kaushik Roy*

Main category: cs.LG

TL;DR: 提出SlimDiff框架压缩扩散模型，无需训练，减少参数与加速，生成质量佳且所需校准样本少。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型计算成本高，现有效率技术依赖微调或再训练恢复性能，存在瓶颈。

Method: 将扩散模型压缩视为谱近似任务，基于激活协方差定义低秩子空间，进行动态剪枝，对功能权重组进行模块分解并自适应分配稀疏性。

Result: 相比基线实现35%加速和约1亿参数减少，生成质量与未压缩模型相当，仅需约500个校准样本。

Conclusion: 这是首个完全无需训练的激活引导的扩散模型结构压缩方法，兼具理论清晰性和实际效率。

Abstract: Diffusion models (DMs), lauded for their generative performance, are
computationally prohibitive due to their billion-scale parameters and iterative
denoising dynamics. Existing efficiency techniques, such as quantization,
timestep reduction, or pruning, offer savings in compute, memory, or runtime
but are strictly bottlenecked by reliance on fine-tuning or retraining to
recover performance. In this work, we introduce SlimDiff, an automated
activation-informed structural compression framework that reduces both
attention and feedforward dimensionalities in DMs, while being entirely
gradient-free. SlimDiff reframes DM compression as a spectral approximation
task, where activation covariances across denoising timesteps define low-rank
subspaces that guide dynamic pruning under a fixed compression budget. This
activation-aware formulation mitigates error accumulation across timesteps by
applying module-wise decompositions over functional weight groups: query--key
interactions, value--output couplings, and feedforward projections, rather than
isolated matrix factorizations, while adaptively allocating sparsity across
modules to respect the non-uniform geometry of diffusion trajectories. SlimDiff
achieves up to 35\% acceleration and $\sim$100M parameter reduction over
baselines, with generation quality on par with uncompressed models without any
backpropagation. Crucially, our approach requires only about 500 calibration
samples, over 70$\times$ fewer than prior methods. To our knowledge, this is
the first closed-form, activation-guided structural compression of DMs that is
entirely training-free, providing both theoretical clarity and practical
efficiency.

</details>


### [125] [Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training](https://arxiv.org/abs/2509.21500)
*Junkai Zhang,Zihao Wang,Lin Gui,Swarnashree Mysore Sathyendra,Jaehwan Jeong,Victor Veitch,Wei Wang,Yunzhong He,Bing Liu,Lifeng Jin*

Main category: cs.LG

TL;DR: 研究强化微调中奖励过度优化问题，提出基于评分规则的奖励方法缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 强化微调存在奖励过度优化问题，关键在于高奖励尾部的奖励误指定，需关注高奖励区域但样本稀缺。

Method: 研究基于评分规则的奖励，提出工作流程以利用离策略示例并捕捉高奖励尾部。

Result: 基于评分规则的奖励显著缓解奖励过度优化，有效改进大语言模型的训练。

Conclusion: 基于评分规则的奖励是解决奖励过度优化问题的有效方法。

Abstract: Reinforcement fine-tuning (RFT) often suffers from \emph{reward
over-optimization}, where a policy model hacks the reward signals to achieve
high scores while producing low-quality outputs. Our theoretical analysis shows
that the key lies in reward misspecification at the high-reward tail: the
inability to reliably distinguish Excellent responses from merely Great ones.
This motivate us to focus on the high-reward region. However, such tail
examples are scarce under the base LLM. While off-policy exemplars (e.g. from
stronger models or rewrites) are easier to obtain, naively training on them
yields a misspecified reward for the policy we aim to align. To address this,
we study rubric-based rewards. By design, rubrics can leverage off-policy
examples while remaining insensitive to their artifacts. To elicit rubrics that
capture the high-reward tail, we highlight the importance of distinguishing
among great and diverse responses, and introduce a workflow to implement this
idea. We empirically demonstrate that rubric-based rewards substantially
mitigate reward over-optimization and deliver effective LLM post-training
improvements. Our code can be accessed at
https://github.com/Jun-Kai-Zhang/rubrics.git .

</details>


### [126] [Uncertainty-Aware Knowledge Tracing Models](https://arxiv.org/abs/2509.21514)
*Joshua Mitton,Prarthana Bhattacharyya,Ralph Abboud,Simon Woodhead*

Main category: cs.LG

TL;DR: 研究提出为知识追踪（KT）模型添加捕捉预测不确定性的能力，表明更大预测不确定性与模型错误预测相关，且该信号对教育学习平台有用。


<details>
  <summary>Details</summary>
Motivation: 现有KT模型在学生选干扰项时多做出错误预测，学生错误易被忽视，需改进。

Method: 为KT模型添加捕捉预测不确定性的能力。

Result: 发现更大的预测不确定性与模型的错误预测一致，且KT模型中的不确定性是有信息价值的。

Conclusion: KT模型中的不确定性信号对有限资源环境下需了解学生能力的教育学习平台有教学应用价值。

Abstract: The main focus of research on Knowledge Tracing (KT) models is on model
developments with the aim of improving predictive accuracy. Most of these
models make the most incorrect predictions when students choose a distractor,
leading to student errors going undetected. We present an approach to add new
capabilities to KT models by capturing predictive uncertainty and demonstrate
that a larger predictive uncertainty aligns with model incorrect predictions.
We show that uncertainty in KT models is informative and that this signal would
be pedagogically useful for application in an educational learning platform
that can be used in a limited resource setting where understanding student
ability is necessary.

</details>


### [127] [$\mathbf{Li_2}$: A Framework on Dynamics of Feature Emergence and Delayed Generalization](https://arxiv.org/abs/2509.21519)
*Yuandong Tian*

Main category: cs.LG

TL;DR: 提出Li₂框架刻画2层非线性网络grokking行为三阶段，研究特征可泛化性等，揭示关键超参作用和优化器有效原因，分析可扩展到多层架构。


<details>
  <summary>Details</summary>
Motivation: 对于复杂结构化输入，缺乏数学框架刻画grokking现象中特征的出现情况、方式和条件。

Method: 提出Li₂框架，根据反向传播梯度G_F结构划分三个阶段进行研究，还在群算术任务中研究特征相关性质。

Result: 明确三个阶段特征，研究特征可泛化性、表示能力等，揭示关键超参作用，得出记忆和泛化的缩放定律，解释优化器有效的原因。

Conclusion: 该框架有助于理解grokking现象，分析可扩展到多层架构。

Abstract: While the phenomenon of grokking, i.e., delayed generalization, has been
studied extensively, it remains an open question whether there is a
mathematical framework to characterize what kind of features emerge, how and in
which conditions it happens from training, for complex structured inputs. We
propose a novel framework, named $\mathbf{Li_2}$, that captures three key
stages for the grokking behavior of 2-layer nonlinear networks: (I) Lazy
learning, (II) independent feature learning and (III) interactive feature
learning, characterized by the structure of backpropagated gradient $G_F$
across layers. In (I), $G_F$ is random, and top layer overfits to random hidden
representation. In (II), the gradient of each node (column of $G_F$) only
depends on its own activation, and thus each hidden node learns their
representation independently from $G_F$, which now carries information about
target labels, thanks to weight decay. Interestingly, the independent dynamics
follows exactly the gradient ascent of an energy function $E$, and its local
maxima are precisely the emerging features. We study whether these local-optima
induced features are generalizable, their representation power, and how they
change on sample size, in group arithmetic tasks. Finally, in (III), we
provably show how hidden nodes interact, and how $G_F$ changes to focus on
missing features that need to be learned. Our study sheds lights on roles
played by key hyperparameters such as weight decay, learning rate and sample
sizes in grokking, leads to provable scaling laws of memorization and
generalization, and reveals the underlying cause why recent optimizers such as
Muon can be effective, from the first principles of gradient dynamics. Our
analysis can be extended to multi-layer architectures.

</details>


### [128] [TRiCo: Triadic Game-Theoretic Co-Training for Robust Semi-Supervised Learning](https://arxiv.org/abs/2509.21526)
*Hongyang He,Xinyuan Song,Yangfan He,Zeyu Zhang,Yanshu Li,Haochen You,Lifan Sun,Wenqiao Zhang*

Main category: cs.LG

TL;DR: 介绍TRiCo，一种新颖的三元博弈论协同训练框架，在半监督学习中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决现有半监督学习框架中静态视图交互、伪标签不可靠和缺乏难样本建模等关键限制。

Method: 将教师、两个学生和对抗生成器纳入统一训练范式，将半监督学习表述为三者间的结构化交互，基于互信息选择伪标签，将三元交互形式化为Stackelberg博弈。

Result: 在CIFAR - 10、SVHN、STL - 10和ImageNet上的广泛实验表明，TRiCo在低标签制度下始终实现了最先进的性能。

Conclusion: TRiCo为半监督学习提供了一个有原则且可推广的解决方案，与架构无关且兼容冻结的视觉骨干网络。

Abstract: We introduce TRiCo, a novel triadic game-theoretic co-training framework that
rethinks the structure of semi-supervised learning by incorporating a teacher,
two students, and an adversarial generator into a unified training paradigm.
Unlike existing co-training or teacher-student approaches, TRiCo formulates SSL
as a structured interaction among three roles: (i) two student classifiers
trained on frozen, complementary representations, (ii) a meta-learned teacher
that adaptively regulates pseudo-label selection and loss balancing via
validation-based feedback, and (iii) a non-parametric generator that perturbs
embeddings to uncover decision boundary weaknesses. Pseudo-labels are selected
based on mutual information rather than confidence, providing a more robust
measure of epistemic uncertainty. This triadic interaction is formalized as a
Stackelberg game, where the teacher leads strategy optimization and students
follow under adversarial perturbations. By addressing key limitations in
existing SSL frameworks, such as static view interactions, unreliable
pseudo-labels, and lack of hard sample modeling, TRiCo provides a principled
and generalizable solution. Extensive experiments on CIFAR-10, SVHN, STL-10,
and ImageNet demonstrate that TRiCo consistently achieves state-of-the-art
performance in low-label regimes, while remaining architecture-agnostic and
compatible with frozen vision backbones.

</details>


### [129] [Preemptive Detection and Steering of LLM Misalignment via Latent Reachability](https://arxiv.org/abs/2509.21528)
*Sathwik Karnik,Somil Bansal*

Main category: cs.LG

TL;DR: 提出BRT - Align框架用于大语言模型推理时安全控制，实验表明其能更准确提前检测不安全内容，减少不安全生成且保留句子多样性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类反馈的强化学习方法在推理时无法保障大语言模型安全，需新方法解决推理时生成有害内容问题。

Method: 将自回归生成建模为潜在空间的动态系统，通过反向可达性学习安全价值函数，有运行时监测器和最小限制转向过滤器两个机制。

Result: 在多个大语言模型和毒性基准测试中，比基线方法更准确、更早检测不安全内容，大幅减少不安全生成，有更好的对齐特性。

Conclusion: 可达性分析为大语言模型推理时安全提供了有原则且实用的基础。

Abstract: Large language models (LLMs) are now ubiquitous in everyday tools, raising
urgent safety concerns about their tendency to generate harmful content. The
dominant safety approach -- reinforcement learning from human feedback (RLHF)
-- effectively shapes model behavior during training but offers no safeguards
at inference time, where unsafe continuations may still arise. We propose
BRT-Align, a reachability-based framework that brings control-theoretic safety
tools to LLM inference. BRT-Align models autoregressive generation as a
dynamical system in latent space and learn a safety value function via backward
reachability, estimating the worst-case evolution of a trajectory. This enables
two complementary mechanisms: (1) a runtime monitor that forecasts unsafe
completions several tokens in advance, and (2) a least-restrictive steering
filter that minimally perturbs latent states to redirect generation away from
unsafe regions. Experiments across multiple LLMs and toxicity benchmarks
demonstrate that BRT-Align provides more accurate and earlier detection of
unsafe continuations than baselines. Moreover, for LLM safety alignment,
BRT-Align substantially reduces unsafe generations while preserving sentence
diversity and coherence. Qualitative results further highlight emergent
alignment properties: BRT-Align consistently produces responses that are less
violent, less profane, less offensive, and less politically biased. Together,
these findings demonstrate that reachability analysis provides a principled and
practical foundation for inference-time LLM safety.

</details>


### [130] [Expert-guided Clinical Text Augmentation via Query-Based Model Collaboration](https://arxiv.org/abs/2509.21530)
*Dongkyu Cho,Miao Zhang,Rumi Chunara*

Main category: cs.LG

TL;DR: 提出基于查询的模型协作框架用于医疗数据增强，实验表明其优于现有方法且更安全。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗等高风险领域进行数据增强时，存在生成临床错误或误导信息的风险，需要保障医疗信息安全。

Method: 提出一种基于查询的模型协作框架，集成专家级领域知识来指导数据增强过程。

Result: 在临床预测任务实验中，该轻量级协作方法始终优于现有的大语言模型增强方法，且通过减少事实性错误提高了安全性。

Conclusion: 该框架弥补了大语言模型增强潜力与专业领域安全要求之间的差距。

Abstract: Data augmentation is a widely used strategy to improve model robustness and
generalization by enriching training datasets with synthetic examples. While
large language models (LLMs) have demonstrated strong generative capabilities
for this purpose, their applications in high-stakes domains like healthcare
present unique challenges due to the risk of generating clinically incorrect or
misleading information. In this work, we propose a novel query-based model
collaboration framework that integrates expert-level domain knowledge to guide
the augmentation process to preserve critical medical information. Experiments
on clinical prediction tasks demonstrate that our lightweight
collaboration-based approach consistently outperforms existing LLM augmentation
methods while improving safety through reduced factual errors. This framework
addresses the gap between LLM augmentation potential and the safety
requirements of specialized domains.

</details>


### [131] [A circuit for predicting hierarchical structure in-context in Large Language Models](https://arxiv.org/abs/2509.21534)
*Tankred Saanum,Can Demircan,Samuel J. Gershman,Eric Schulz*

Main category: cs.LG

TL;DR: 研究大语言模型（LLMs）归纳头在上下文学习中对复杂重复模式的处理能力，发现自适应归纳头及学习机制。


<details>
  <summary>Details</summary>
Motivation: 不清楚归纳头机制能否支持上下文学习具有层次结构的复杂重复模式，自然语言中有很多此类情况。

Method: 设计合成上下文学习任务，对一系列LLMs在令牌序列和自然语言类似物上进行评估，研究归纳头的学习方式。

Result: 发现自适应归纳头可通过学习上下文内容来支持预测，有注意力头能揭示潜在上下文以支持归纳头学习。

Conclusion: 证明LLMs有可学习的归纳头，并给出LLMs学习预测高阶重复模式的完整机制解释。

Abstract: Large Language Models (LLMs) excel at in-context learning, the ability to use
information provided as context to improve prediction of future tokens.
Induction heads have been argued to play a crucial role for in-context learning
in Transformer Language Models. These attention heads make a token attend to
successors of past occurrences of the same token in the input. This basic
mechanism supports LLMs' ability to copy and predict repeating patterns.
However, it is unclear if this same mechanism can support in-context learning
of more complex repetitive patterns with hierarchical structure. Natural
language is teeming with such cases: The article "the" in English usually
prefaces multiple nouns in a text. When predicting which token succeeds a
particular instance of "the", we need to integrate further contextual cues from
the text to predict the correct noun. If induction heads naively attend to all
past instances of successor tokens of "the" in a context-independent manner,
they cannot support this level of contextual information integration. In this
study, we design a synthetic in-context learning task, where tokens are
repeated with hierarchical dependencies. Here, attending uniformly to all
successor tokens is not sufficient to accurately predict future tokens.
Evaluating a range of LLMs on these token sequences and natural language
analogues, we find adaptive induction heads that support prediction by learning
what to attend to in-context. Next, we investigate how induction heads
themselves learn in-context. We find evidence that learning is supported by
attention heads that uncover a set of latent contexts, determining the
different token transition relationships. Overall, we not only show that LLMs
have induction heads that learn, but offer a complete mechanistic account of
how LLMs learn to predict higher-order repetitive patterns in-context.

</details>


### [132] [Evidence for Limited Metacognition in LLMs](https://arxiv.org/abs/2509.21545)
*Christopher Ackerman*

Main category: cs.LG

TL;DR: 提出评估大语言模型元认知能力的新方法，发现2024年初以来的前沿模型有元认知能力，且能力有局限、因模型而异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型自我意识和感知能力受关注且有安全和政策影响，但相关测量科学尚处起步阶段，需评估其元认知能力。

Method: 借鉴非人类动物元认知研究，避开模型自我报告，采用两个实验范式测试模型对内部状态知识的策略性运用，分析模型返回的标记概率。

Result: 2024年初以来的前沿大语言模型有一定元认知能力，如评估和利用自信、预测答案等；能力分辨率有限、因情境而异、与人类不同；不同模型有差异。

Conclusion: 大语言模型有元认知能力，但能力有局限，模型后训练可能对元认知能力发展有作用。

Abstract: The possibility of LLM self-awareness and even sentience is gaining
increasing public attention and has major safety and policy implications, but
the science of measuring them is still in a nascent state. Here we introduce a
novel methodology for quantitatively evaluating metacognitive abilities in
LLMs. Taking inspiration from research on metacognition in nonhuman animals,
our approach eschews model self-reports and instead tests to what degree models
can strategically deploy knowledge of internal states. Using two experimental
paradigms, we demonstrate that frontier LLMs introduced since early 2024 show
increasingly strong evidence of certain metacognitive abilities, specifically
the ability to assess and utilize their own confidence in their ability to
answer factual and reasoning questions correctly and the ability to anticipate
what answers they would give and utilize that information appropriately. We
buttress these behavioral findings with an analysis of the token probabilities
returned by the models, which suggests the presence of an upstream internal
signal that could provide the basis for metacognition. We further find that
these abilities 1) are limited in resolution, 2) emerge in context-dependent
manners, and 3) seem to be qualitatively different from those of humans. We
also report intriguing differences across models of similar capabilities,
suggesting that LLM post-training may have a role in developing metacognitive
abilities.

</details>


### [133] [Leveraging Big Data Frameworks for Spam Detection in Amazon Reviews](https://arxiv.org/abs/2509.21579)
*Mst Eshita Khatun,Halima Akter,Tasnimul Rehan,Toufiq Ahmed*

Main category: cs.LG

TL;DR: 研究利用大数据分析和机器学习检测亚马逊产品评论中的垃圾评论，逻辑回归准确率达90.35%，提升网购环境可信度。


<details>
  <summary>Details</summary>
Motivation: 数字时代网购盛行，虚假评论破坏消费者信任，损害卖家声誉，需解决该问题。

Method: 采用先进的大数据分析和机器学习方法，使用可扩展的大数据框架处理和分析大量评论数据，提取欺诈行为关键特征。

Result: 多种机器学习分类器可用于检测垃圾评论，逻辑回归准确率达90.35%。

Conclusion: 研究有助于营造更可信、透明的网购环境。

Abstract: In this digital era, online shopping is common practice in our daily lives.
Product reviews significantly influence consumer buying behavior and help
establish buyer trust. However, the prevalence of fraudulent reviews undermines
this trust by potentially misleading consumers and damaging the reputations of
the sellers. This research addresses this pressing issue by employing advanced
big data analytics and machine learning approaches on a substantial dataset of
Amazon product reviews. The primary objective is to detect and classify spam
reviews accurately so that it enhances the authenticity of the review. Using a
scalable big data framework, we efficiently process and analyze a large scale
of review data, extracting key features indicative of fraudulent behavior. Our
study illustrates the utility of various machine learning classifiers in
detecting spam reviews, with Logistic Regression achieving an accuracy of
90.35%, thus contributing to a more trustworthy and transparent online shopping
environment.

</details>


### [134] [Task-Agnostic Federated Continual Learning via Replay-Free Gradient Projection](https://arxiv.org/abs/2509.21606)
*Seohyeon Cha,Huancheng Chen,Haris Vikalo*

Main category: cs.LG

TL;DR: 提出FedProTIP框架解决联邦持续学习中的灾难性遗忘问题，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决联邦持续学习中因数据异质性、通信受限和隐私问题导致的灾难性遗忘问题，以及任务无关推理的挑战。

Method: 提出FedProTIP框架，通过将客户端更新投影到全局模型先前学习表示所张成子空间的正交补上来减轻遗忘，还引入轻量级机制预测任务身份并动态调整输出。

Result: 在标准联邦持续学习基准测试中，FedProTIP在平均准确率上显著优于现有方法，尤其是在任务身份未知的情况下。

Conclusion: FedProTIP是一种有效的联邦持续学习框架，能有效减轻遗忘问题，提升性能。

Abstract: Federated continual learning (FCL) enables distributed client devices to
learn from streaming data across diverse and evolving tasks. A major challenge
to continual learning, catastrophic forgetting, is exacerbated in decentralized
settings by the data heterogeneity, constrained communication and privacy
concerns. We propose Federated gradient Projection-based Continual Learning
with Task Identity Prediction (FedProTIP), a novel FCL framework that mitigates
forgetting by projecting client updates onto the orthogonal complement of the
subspace spanned by previously learned representations of the global model.
This projection reduces interference with earlier tasks and preserves
performance across the task sequence. To further address the challenge of
task-agnostic inference, we incorporate a lightweight mechanism that leverages
core bases from prior tasks to predict task identity and dynamically adjust the
global model's outputs. Extensive experiments across standard FCL benchmarks
demonstrate that FedProTIP significantly outperforms state-of-the-art methods
in average accuracy, particularly in settings where task identities are a
priori unknown.

</details>


### [135] [Causal Abstraction Inference under Lossy Representations](https://arxiv.org/abs/2509.21607)
*Kevin Xia,Elias Bareinboim*

Main category: cs.LG

TL;DR: 本文引入投影抽象，推广现有因果抽象定义以适应有损表示，展示构建方法、因果查询转换，证明图形准则，实验验证其在高维图像中的有效性。


<details>
  <summary>Details</summary>
Motivation: 多数现有因果抽象定义在考虑有损抽象函数时定义不明确，需推广以适应有损表示。

Method: 引入投影抽象，展示其构建过程和因果查询转换，证明图形准则，进行实验验证。

Result: 提出投影抽象并展示相关构建和转换方法，证明图形准则，实验表明投影抽象模型在高维图像中有效。

Conclusion: 投影抽象能够推广现有因果抽象定义，适应有损表示，在高维图像等场景有良好效果。

Abstract: The study of causal abstractions bridges two integral components of human
intelligence: the ability to determine cause and effect, and the ability to
interpret complex patterns into abstract concepts. Formally, causal abstraction
frameworks define connections between complicated low-level causal models and
simple high-level ones. One major limitation of most existing definitions is
that they are not well-defined when considering lossy abstraction functions in
which multiple low-level interventions can have different effects while mapping
to the same high-level intervention (an assumption called the abstract
invariance condition). In this paper, we introduce a new type of abstractions
called projected abstractions that generalize existing definitions to
accommodate lossy representations. We show how to construct a projected
abstraction from the low-level model and how it translates equivalent
observational, interventional, and counterfactual causal queries from low to
high-level. Given that the true model is rarely available in practice we prove
a new graphical criteria for identifying and estimating high-level causal
queries from limited low-level data. Finally, we experimentally show the
effectiveness of projected abstraction models in high-dimensional image
settings.

</details>


### [136] [Shoot from the HIP: Hessian Interatomic Potentials without derivatives](https://arxiv.org/abs/2509.21624)
*Andreas Burger,Luca Thiede,Nikolaj Rønne,Varinia Bernales,Nandita Vijaykumar,Tejs Vegge,Arghya Bhowmik,Alan Aspuru-Guzik*

Main category: cs.LG

TL;DR: 提出用深度学习模型直接预测分子Hessians的方法，比传统方法更快、更准等，并在多任务验证且开源代码。


<details>
  <summary>Details</summary>
Motivation: 计算化学中分子Hessians计算成本高、扩展性差，需更好方法。

Method: 用深度学习模型，从图神经网络消息传递中计算的不可约表示特征构建SE(3)等变、对称的Hessians。

Result: HIP Hessians比传统方法快1 - 2个数量级，更准确、省内存、易训练且扩展性好，下游任务表现优。

Conclusion: 该方法有效，开源代码促进分子Hessians直接预测的进一步发展。

Abstract: Fundamental tasks in computational chemistry, from transition state search to
vibrational analysis, rely on molecular Hessians, which are the second
derivatives of the potential energy. Yet, Hessians are computationally
expensive to calculate and scale poorly with system size, with both quantum
mechanical methods and neural networks. In this work, we demonstrate that
Hessians can be predicted directly from a deep learning model, without relying
on automatic differentiation or finite differences. We observe that one can
construct SE(3)-equivariant, symmetric Hessians from irreducible
representations (irrep) features up to degree $l$=2 computed during message
passing in graph neural networks. This makes HIP Hessians one to two orders of
magnitude faster, more accurate, more memory efficient, easier to train, and
enables more favorable scaling with system size. We validate our predictions
across a wide range of downstream tasks, demonstrating consistently superior
performance for transition state search, accelerated geometry optimization,
zero-point energy corrections, and vibrational analysis benchmarks. We
open-source the HIP codebase and model weights to enable further development of
the direct prediction of Hessians at https://github.com/BurgerAndreas/hip

</details>


### [137] [Blockwise Hadamard high-Rank Adaptation for Parameter-Efficient LLM Fine-Tuning](https://arxiv.org/abs/2509.21637)
*Feng Yu,Jia Hu,Geyong Min*

Main category: cs.LG

TL;DR: 提出Block Hadamard high - Rank Adaptation (BHRA)方法解决全局调制局限，在多任务上超基线。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）方法有局限，经典LoRA受名义秩限制，HiRA和ABBA有各自问题，需解决全局调制局限。

Method: 提出BHRA方法，对每个权重矩阵分区，在每个块内独立应用HiRA式乘法调制。

Result: 实验显示块设计在不同秩预算下保持丰富频谱，缓解全局调制导致的崩溃，在多个任务上始终超越强PEFT基线。

Conclusion: BHRA方法有效，在参数预算匹配情况下表现优于基线。

Abstract: Parameter-efficient fine-tuning (PEFT) methods must be resource-efficient yet
handle heterogeneous reasoning transformations, and classical low-rank
adaptation (LoRA) is constrained by the nominal rank $r$. Hadamard-style
extensions like HiRA raise the nominal rank but couple every update to the
global energy pattern of the frozen weight matrix, while ABBA trades this
inductive bias for fully learned dense intermediates. To address the limitation
of global modulation, we propose Block Hadamard high-Rank Adaptation (BHRA),
which partitions each weight matrix and applies HiRA-style multiplicative
modulation independently within every block, preserving the PEFT parameter
footprint while unlocking localized rank amplification. Our empirical analyses
reveal that this blockwise design maintains rich spectra across rank budgets,
mitigating the collapse induced by global modulation. Across eight commonsense
reasoning tasks and two arithmetic benchmarks with Llama-3.2 1B/3B, Mistral-7B,
and Gemma-2 9B, BHRA consistently surpasses strong PEFT baselines under matched
parameter budgets.

</details>


### [138] [Understanding and Enhancing Mask-Based Pretraining towards Universal Representations](https://arxiv.org/abs/2509.21650)
*Mingze Dong,Leda Wang,Yuval Kluger*

Main category: cs.LG

TL;DR: 本文研究基于掩码的预训练，用线性回归刻画其行为，提出R²MAE预训练方案，在多领域表现优于标准方案。


<details>
  <summary>Details</summary>
Motivation: 基于掩码的预训练虽成功，但在学习数据表示中的作用和局限性不明确。

Method: 用高维最小范数线性回归的测试风险刻画掩码预训练行为，分析线性模型，提出R²MAE预训练方案。

Result: 理论框架在多种架构和任务中得到验证，R²MAE在多领域模型中优于标准和复杂掩码方案。

Conclusion: 所提理论和R²MAE方案有效，能提升现有模型性能。

Abstract: Mask-based pretraining has become a cornerstone of modern large-scale models
across language, vision, and recently biology. Despite its empirical success,
its role and limits in learning data representations have been unclear. In this
work, we show that the behavior of mask-based pretraining can be directly
characterized by test risk in high-dimensional minimum-norm ("ridge-less")
linear regression, without relying on further model specifications. Further
analysis of linear models uncovers several novel aspects of mask-based
pretraining. The theoretical framework and its implications have been validated
across diverse neural architectures (including MLPs, CNNs, and Transformers)
applied to both vision and language tasks. Guided by our theory, we propose an
embarrassingly simple yet overlooked pretraining scheme named Randomly Random
Mask AutoEncoding (R$^2$MAE), which enforces capturing multi-scale features
from data and is able to outperform optimal fixed mask ratio settings in our
linear model framework. We implement R$^2$MAE in vision, language, DNA
sequence, and single-cell models, where it consistently outperforms standard
and more complicated masking schemes, leading to improvements for
state-of-the-art models. Our code is available at:
https://github.com/MingzeDong/r2mae

</details>


### [139] [Limitations on Safe, Trusted, Artificial General Intelligence](https://arxiv.org/abs/2509.21654)
*Rina Panigrahy,Vatsal Sharan*

Main category: cs.LG

TL;DR: 本文提出安全、信任和AGI的严格数学定义，证明三者存在根本不兼容性。


<details>
  <summary>Details</summary>
Motivation: 为安全、信任和AGI提供严格数学定义，并探究它们之间的关系。

Method: 提出安全、信任和AGI的数学定义，通过与哥德尔不完备定理和图灵停机问题证明类比进行推理。

Result: 安全且可信任的AI系统不能是AGI系统，存在人类能解决而系统无法解决的任务实例。

Conclusion: 对于严格数学定义的安全、信任和AGI，三者存在根本不兼容性，但现实部署可采用其他实用解释。

Abstract: Safety, trust and Artificial General Intelligence (AGI) are aspirational
goals in artificial intelligence (AI) systems, and there are several informal
interpretations of these notions. In this paper, we propose strict,
mathematical definitions of safety, trust, and AGI, and demonstrate a
fundamental incompatibility between them. We define safety of a system as the
property that it never makes any false claims, trust as the assumption that the
system is safe, and AGI as the property of an AI system always matching or
exceeding human capability. Our core finding is that -- for our formal
definitions of these notions -- a safe and trusted AI system cannot be an AGI
system: for such a safe, trusted system there are task instances which are
easily and provably solvable by a human but not by the system. We note that we
consider strict mathematical definitions of safety and trust, and it is
possible for real-world deployments to instead rely on alternate, practical
interpretations of these notions. We show our results for program verification,
planning, and graph reachability. Our proofs draw parallels to G\"odel's
incompleteness theorems and Turing's proof of the undecidability of the halting
problem, and can be regarded as interpretations of G\"odel's and Turing's
results.

</details>


### [140] [RED-DiffEq: Regularization by denoising diffusion models for solving inverse PDE problems with application to full waveform inversion](https://arxiv.org/abs/2509.21659)
*Siming Shan,Min Zhu,Youzuo Lin,Lu Lu*

Main category: cs.LG

TL;DR: 提出RED - DiffEq框架解决PDE控制的逆问题，应用于地球物理全波形反演，效果佳且有泛化能力，可用于多种PDE逆问题。


<details>
  <summary>Details</summary>
Motivation: PDE控制的逆问题因非线性、不适定性和对噪声敏感面临重大挑战，需新方法解决。

Method: 集成物理驱动反演和数据驱动学习，利用预训练扩散模型作为PDE控制逆问题的正则化机制。

Result: 在地球物理全波形反演问题中，与传统方法相比，准确性和鲁棒性增强，对扩散模型未训练的更复杂速度模型有强泛化能力。

Conclusion: 提出的RED - DiffEq框架可直接应用于多种PDE控制的逆问题。

Abstract: Partial differential equation (PDE)-governed inverse problems are fundamental
across various scientific and engineering applications; yet they face
significant challenges due to nonlinearity, ill-posedness, and sensitivity to
noise. Here, we introduce a new computational framework, RED-DiffEq, by
integrating physics-driven inversion and data-driven learning. RED-DiffEq
leverages pretrained diffusion models as a regularization mechanism for
PDE-governed inverse problems. We apply RED-DiffEq to solve the full waveform
inversion problem in geophysics, a challenging seismic imaging technique that
seeks to reconstruct high-resolution subsurface velocity models from seismic
measurement data. Our method shows enhanced accuracy and robustness compared to
conventional methods. Additionally, it exhibits strong generalization ability
to more complex velocity models that the diffusion model is not trained on. Our
framework can also be directly applied to diverse PDE-governed inverse
problems.

</details>


### [141] [MMPlanner: Zero-Shot Multimodal Procedural Planning with Chain-of-Thought Object State Reasoning](https://arxiv.org/abs/2509.21662)
*Afrina Tabassum,Bin Guo,Xiyao Ma,Hoda Eldardiry,Ismini Lourentzou*

Main category: cs.LG

TL;DR: 提出零样本多模态程序规划框架MMPlanner，通过OSR - CoT提示建模对象状态转换，设计评估协议，在两个数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态程序规划方法在视觉对象状态对齐和系统评估方面研究不足，需解决跨模态对象状态一致性及生成信息丰富计划的问题。

Method: 提出MMPlanner框架，引入OSR - CoT提示，设计LLM作为评估法官的协议和视觉步骤重排序任务。

Result: 在RECIPEPLAN和WIKIPLAN上，MMPlanner在文本规划提升6.8%、跨模态对齐提升11.9%、视觉步骤排序提升26.7%，达SOTA。

Conclusion: MMPlanner在多模态程序规划任务中效果良好，能有效解决对象状态一致性和计划评估问题。

Abstract: Multimodal Procedural Planning (MPP) aims to generate step-by-step
instructions that combine text and images, with the central challenge of
preserving object-state consistency across modalities while producing
informative plans. Existing approaches often leverage large language models
(LLMs) to refine textual steps; however, visual object-state alignment and
systematic evaluation are largely underexplored. We present MMPlanner, a
zero-shot MPP framework that introduces Object State Reasoning Chain-of-Thought
(OSR-CoT) prompting to explicitly model object-state transitions and generate
accurate multimodal plans. To assess plan quality, we design LLM-as-a-judge
protocols for planning accuracy and cross-modal alignment, and further propose
a visual step-reordering task to measure temporal coherence. Experiments on
RECIPEPLAN and WIKIPLAN show that MMPlanner achieves state-of-the-art
performance, improving textual planning by +6.8%, cross-modal alignment by
+11.9%, and visual step ordering by +26.7%

</details>


### [142] [Logic of Hypotheses: from Zero to Full Knowledge in Neurosymbolic Integration](https://arxiv.org/abs/2509.21663)
*Davide Bizzaro,Alessandro Daniele*

Main category: cs.LG

TL;DR: 提出一种新语言LoH统一神经符号集成的不同方法，可灵活集成数据驱动规则学习与先验知识，实验效果好且规则可解释。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号集成领域方法分为注入手工规则和从数据归纳规则两类，缺乏统一方法。

Method: 引入Logic of Hypotheses (LoH)语言，扩展命题逻辑语法，用模糊逻辑编译成可微计算图，通过反向传播学习最优选择，使用Goedel模糊逻辑和Goedel trick。

Result: 在表格数据和Visual Tic - Tac - Toe NeSy任务上取得强结果，产生可解释决策规则。

Conclusion: LoH框架能统一神经符号集成的不同方法，可灵活集成知识，模型可离散化且性能无损失。

Abstract: Neurosymbolic integration (NeSy) blends neural-network learning with symbolic
reasoning. The field can be split between methods injecting hand-crafted rules
into neural models, and methods inducing symbolic rules from data. We introduce
Logic of Hypotheses (LoH), a novel language that unifies these strands,
enabling the flexible integration of data-driven rule learning with symbolic
priors and expert knowledge. LoH extends propositional logic syntax with a
choice operator, which has learnable parameters and selects a subformula from a
pool of options. Using fuzzy logic, formulas in LoH can be directly compiled
into a differentiable computational graph, so the optimal choices can be
learned via backpropagation. This framework subsumes some existing NeSy models,
while adding the possibility of arbitrary degrees of knowledge specification.
Moreover, the use of Goedel fuzzy logic and the recently developed Goedel trick
yields models that can be discretized to hard Boolean-valued functions without
any loss in performance. We provide experimental analysis on such models,
showing strong results on tabular data and on the Visual Tic-Tac-Toe NeSy task,
while producing interpretable decision rules.

</details>


### [143] [DIM: Enforcing Domain-Informed Monotonicity in Deep Neural Networks](https://arxiv.org/abs/2509.21666)
*Joshua Salim,Jordan Yu,Xilei Zhao*

Main category: cs.LG

TL;DR: 本文提出新正则化方法DIM，通过对真实和合成数据集实验，验证其能增强模型性能并缓解过拟合。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型因结构复杂、参数多易过拟合，无法有效泛化到新数据。

Method: 提出Enforcing Domain-Informed Monotonicity in Deep Neural Networks (DIM)方法，通过惩罚相对于线性基线的违反情况来强制单调性，构建数学框架并集成到训练目标中。

Result: 在不同神经网络架构的实验中，适度的单调性约束能持续提升模型性能。

Conclusion: DIM通过应用领域知情的单调性约束来规范模型行为，增强了深度神经网络的预测性能，缓解了过拟合问题。

Abstract: While deep learning models excel at predictive tasks, they often overfit due
to their complex structure and large number of parameters, causing them to
memorize training data, including noise, rather than learn patterns that
generalize to new data. To tackle this challenge, this paper proposes a new
regularization method, i.e., Enforcing Domain-Informed Monotonicity in Deep
Neural Networks (DIM), which maintains domain-informed monotonic relationships
in complex deep learning models to further improve predictions. Specifically,
our method enforces monotonicity by penalizing violations relative to a linear
baseline, effectively encouraging the model to follow expected trends while
preserving its predictive power. We formalize this approach through a
comprehensive mathematical framework that establishes a linear reference,
measures deviations from monotonic behavior, and integrates these measurements
into the training objective. We test and validate the proposed methodology
using a real-world ridesourcing dataset from Chicago and a synthetically
created dataset. Experiments across various neural network architectures show
that even modest monotonicity constraints consistently enhance model
performance. DIM enhances the predictive performance of deep neural networks by
applying domain-informed monotonicity constraints to regularize model behavior
and mitigate overfitting

</details>


### [144] [Neuroprobe: Evaluating Intracranial Brain Responses to Naturalistic Stimuli](https://arxiv.org/abs/2509.21671)
*Andrii Zahorodnii,Christopher Wang,Bennett Stankovits,Charikleia Moraitaki,Geeling Chau,Andrei Barbu,Boris Katz,Ila R Fiete*

Main category: cs.LG

TL;DR: 提出Neuroprobe用于颅内EEG记录研究，有挖掘神经科学见解和比较模型架构两大功能，线性基线表现强，代码开源。


<details>
  <summary>Details</summary>
Motivation: 社区缺乏颅内EEG记录的标准化评估框架，需要严格基准来区分不同建模方法。

Method: 基于BrainTreebank数据集构建Neuroprobe，通过测量特征在时间和电极位置的可解码性研究语言处理，比较不同架构和训练协议。

Result: 可视化信息在大脑中的流动，线性基线在许多任务上击败前沿基础模型。

Conclusion: Neuroprobe有助于推动颅内EEG基础模型领域的快速发展。

Abstract: High-resolution neural datasets enable foundation models for the next
generation of brain-computer interfaces and neurological treatments. The
community requires rigorous benchmarks to discriminate between competing
modeling approaches, yet no standardized evaluation frameworks exist for
intracranial EEG (iEEG) recordings. To address this gap, we present Neuroprobe:
a suite of decoding tasks for studying multi-modal language processing in the
brain. Unlike scalp EEG, intracranial EEG requires invasive surgery to implant
electrodes that record neural activity directly from the brain with minimal
signal distortion. Neuroprobe is built on the BrainTreebank dataset, which
consists of 40 hours of iEEG recordings from 10 human subjects performing a
naturalistic movie viewing task. Neuroprobe serves two critical functions.
First, it is a mine from which neuroscience insights can be drawn. Its high
temporal and spatial resolution allows researchers to systematically determine
when and where computations for each aspect of language processing occur in the
brain by measuring the decodability of each feature across time and all
electrode locations. Using Neuroprobe, we visualize how information flows from
the superior temporal gyrus to the prefrontal cortex, and the progression from
simple auditory features to more complex language features in a purely
data-driven manner. Second, as the field moves toward neural foundation models,
Neuroprobe provides a rigorous framework for comparing competing architectures
and training protocols. We found that the linear baseline is surprisingly
strong, beating frontier foundation models on many tasks. Neuroprobe is
designed with computational efficiency and ease of use in mind. We make the
code for Neuroprobe openly available and maintain a public leaderboard, aiming
to enable rapid progress in the field of iEEG foundation models, at
https://neuroprobe.dev/

</details>


### [145] [SlotFM: A Motion Foundation Model with Slot Attention for Diverse Downstream Tasks](https://arxiv.org/abs/2509.21673)
*Junyong Park,Oron Levy,Rebecca Adaimi,Asaf Liberman,Gierad Laput,Abdelkareem Bedri*

Main category: cs.LG

TL;DR: 提出加速计基础模型SlotFM，采用新注意力机制和损失正则化器，在16个下游任务评估中表现良好，平均性能提升4.5%。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型主要聚焦常见日常活动分类，限制了对依赖其他信号特征的更广泛任务的适用性，需要开发能跨多样化下游任务泛化的模型。

Method: 提出SlotFM，使用时间 - 频率槽注意力处理原始信号的时频表示，生成多个小嵌入，还引入两个损失正则化器。

Result: 在16个下游任务评估中，在13个任务上优于现有自监督方法，其余任务达到与最佳方法相当的结果，平均性能提升4.5%。

Conclusion: SlotFM对传感基础模型有很强的泛化能力。

Abstract: Wearable accelerometers are used for a wide range of applications, such as
gesture recognition, gait analysis, and sports monitoring. Yet most existing
foundation models focus primarily on classifying common daily activities such
as locomotion and exercise, limiting their applicability to the broader range
of tasks that rely on other signal characteristics. We present SlotFM, an
accelerometer foundation model that generalizes across diverse downstream
tasks. SlotFM uses Time-Frequency Slot Attention, an extension of Slot
Attention that processes both time and frequency representations of the raw
signals. It generates multiple small embeddings (slots), each capturing
different signal components, enabling task-specific heads to focus on the most
relevant parts of the data. We also introduce two loss regularizers that
capture local structure and frequency patterns, which improve reconstruction of
fine-grained details and helps the embeddings preserve task-relevant
information. We evaluate SlotFM on 16 classification and regression downstream
tasks that extend beyond standard human activity recognition. It outperforms
existing self-supervised approaches on 13 of these tasks and achieves
comparable results to the best performing approaches on the remaining tasks. On
average, our method yields a 4.5% performance gain, demonstrating strong
generalization for sensing foundation models.

</details>


### [146] [Scalable Second-order Riemannian Optimization for $K$-means Clustering](https://arxiv.org/abs/2509.21675)
*Peng Xu,Chun-Ying Hou,Xiaohui Chen,Richard Y. Zhang*

Main category: cs.LG

TL;DR: 提出将K均值问题重新表述为子流形上的无约束优化问题，用二阶立方正则黎曼牛顿算法求解，实验显示比现有方法收敛更快且统计精度相当。


<details>
  <summary>Details</summary>
Motivation: 当前K均值聚类问题的松弛算法难以平衡约束可行性和目标最优性，计算二阶临界点有挑战。

Method: 将K均值问题重新表述为子流形上的光滑无约束优化问题，用二阶立方正则黎曼牛顿算法求解，将K均值流形分解为乘积流形使牛顿子问题可线性时间求解。

Result: 所提方法比现有一阶非负低秩分解方法收敛显著更快，且统计精度相当。

Conclusion: 新的K均值问题表述和求解方法有效，在收敛速度上有优势。

Abstract: Clustering is a hard discrete optimization problem. Nonconvex approaches such
as low-rank semidefinite programming (SDP) have recently demonstrated promising
statistical and local algorithmic guarantees for cluster recovery. Due to the
combinatorial structure of the $K$-means clustering problem, current relaxation
algorithms struggle to balance their constraint feasibility and objective
optimality, presenting tremendous challenges in computing the second-order
critical points with rigorous guarantees. In this paper, we provide a new
formulation of the $K$-means problem as a smooth unconstrained optimization
over a submanifold and characterize its Riemannian structures to allow it to be
solved using a second-order cubic-regularized Riemannian Newton algorithm. By
factorizing the $K$-means manifold into a product manifold, we show how each
Newton subproblem can be solved in linear time. Our numerical experiments show
that the proposed method converges significantly faster than the
state-of-the-art first-order nonnegative low-rank factorization method, while
achieving similarly optimal statistical accuracy.

</details>


### [147] [SpecMER: Fast Protein Generation with K-mer Guided Speculative Decoding](https://arxiv.org/abs/2509.21689)
*Thomas Walton,Darin Tsui,Aryan Musharaf,Amirali Aghazadeh*

Main category: cs.LG

TL;DR: 提出SpecMER框架加速蛋白质序列生成，提高序列合理性和生成效率。


<details>
  <summary>Details</summary>
Motivation: 自回归模型顺序推理延迟大，现有投机解码在蛋白质序列生成中草稿模型忽略结构和功能约束，导致输出不合理。

Method: 引入SpecMER框架，利用多序列比对提取的k - mer基序融入生物、结构和功能先验，并行对候选序列评分并选择符合生物模式的序列。

Result: 比标准自回归解码加速24 - 32%，有更高接受率和改进的序列似然性。

Conclusion: SpecMER在提高蛋白质序列生成效率的同时，显著改善了序列的合理性。

Abstract: Autoregressive models have transformed protein engineering by enabling the
generation of novel protein sequences beyond those found in nature. However,
their sequential inference introduces significant latency, limiting their
utility in high-throughput protein screening. Speculative decoding accelerates
generation by employing a lightweight draft model to sample tokens, which a
larger target model then verifies and refines. Yet, in protein sequence
generation, draft models are typically agnostic to the structural and
functional constraints of the target protein, leading to biologically
implausible outputs and a shift in the likelihood distribution of generated
sequences. We introduce SpecMER (Speculative Decoding via k-mer Guidance), a
novel framework that incorporates biological, structural, and functional priors
using k-mer motifs extracted from multiple sequence alignments. By scoring
candidate sequences in parallel and selecting those most consistent with known
biological patterns, SpecMER significantly improves sequence plausibility while
retaining the efficiency of speculative decoding. SpecMER achieves 24-32%
speedup over standard autoregressive decoding, along with higher acceptance
rates and improved sequence likelihoods.

</details>


### [148] [Wav2Arrest 2.0: Long-Horizon Cardiac Arrest Prediction with Time-to-Event Modeling, Identity-Invariance, and Pseudo-Lab Alignment](https://arxiv.org/abs/2509.21695)
*Saurabh Kataria,Davood Fattahi,Minxiao Wang,Ran Xiao,Matthew Clark,Timothy Ruchti,Mark Mai,Xiao Hu*

Main category: cs.LG

TL;DR: 论文针对基于PPG的心脏骤停预测系统，提出三项改进，能提升预测AUC，还处理了多任务梯度冲突。


<details>
  <summary>Details</summary>
Motivation: 现有生理基础模型强大表征未得到充分利用，尤其是下游数据/标签稀缺时，需改进PPG-only心脏骤停系统。

Method: 提出三项改进：使用时间到事件建模；让模型学习与患者身份无关的特征；对预训练辅助估计网络生成的伪实验室值进行回归；用PCGrad优化技术处理多任务梯度冲突。

Result: 各项提案可独立将24小时平均AUC从0.74提升到0.78 - 0.80范围，在较长时间范围内有主要提升。

Conclusion: 改进方法推动了早期预警系统研究，处理梯度冲突的技术有效。

Abstract: High-frequency physiological waveform modality offers deep, real-time
insights into patient status. Recently, physiological foundation models based
on Photoplethysmography (PPG), such as PPG-GPT, have been shown to predict
critical events, including Cardiac Arrest (CA). However, their powerful
representation still needs to be leveraged suitably, especially when the
downstream data/label is scarce. We offer three orthogonal improvements to
improve PPG-only CA systems by using minimal auxiliary information. First, we
propose to use time-to-event modeling, either through simple regression to the
event onset time or by pursuing fine-grained discrete survival modeling.
Second, we encourage the model to learn CA-focused features by making them
patient-identity invariant. This is achieved by first training the
largest-scale de-identified biometric identification model, referred to as the
p-vector, and subsequently using it adversarially to deconfound cues, such as
person identity, that may cause overfitting through memorization. Third, we
propose regression on the pseudo-lab values generated by pre-trained auxiliary
estimator networks. This is crucial since true blood lab measurements, such as
lactate, sodium, troponin, and potassium, are collected sparingly. Via
zero-shot prediction, the auxiliary networks can enrich cardiac arrest waveform
labels and generate pseudo-continuous estimates as targets. Our proposals can
independently improve the 24-hour time-averaged AUC from the 0.74 to the
0.78-0.80 range. We primarily improve over longer time horizons with minimal
degradation near the event, thus pushing the Early Warning System research.
Finally, we pursue multi-task formulation and diagnose it with a high gradient
conflict rate among competing losses, which we alleviate via the PCGrad
optimization technique.

</details>


### [149] [Exact Subgraph Isomorphism Network for Predictive Graph Mining](https://arxiv.org/abs/2509.21699)
*Taiga Kojima,Masayuki Karasuyama*

Main category: cs.LG

TL;DR: 提出精确子图同构网络（EIN）用于图级预测，结合子图枚举、神经网络和稀疏正则化，有高预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在图级预测任务中，构建兼具高判别能力和可解释性的图级预测模型是具有挑战性的问题。

Method: 提出Exact subgraph Isomorphism Network (EIN)，结合精确子图枚举、神经网络和稀疏正则化。

Result: EIN与标准图神经网络模型相比有足够高的预测性能，并给出基于所选子图的事后分析示例。

Conclusion: EIN结合子图枚举和神经网络实现高判别能力，稀疏正则化实现有效剪枝和识别重要子图，有高可解释性。

Abstract: In the graph-level prediction task (predict a label for a given graph), the
information contained in subgraphs of the input graph plays a key role. In this
paper, we propose Exact subgraph Isomorphism Network (EIN), which combines the
exact subgraph enumeration, neural network, and a sparse regularization. In
general, building a graph-level prediction model achieving high discriminative
ability along with interpretability is still a challenging problem. Our
combination of the subgraph enumeration and neural network contributes to high
discriminative ability about the subgraph structure of the input graph.
Further, the sparse regularization in EIN enables us 1) to derive an effective
pruning strategy that mitigates computational difficulty of the enumeration
while maintaining the prediction performance, and 2) to identify important
subgraphs that contributes to high interpretability. We empirically show that
EIN has sufficiently high prediction performance compared with standard graph
neural network models, and also, we show examples of post-hoc analysis based on
the selected subgraphs.

</details>


### [150] [Downscaling human mobility data based on demographic socioeconomic and commuting characteristics using interpretable machine learning methods](https://arxiv.org/abs/2509.21703)
*Yuqin Jiang,Andrey A. Popov,Tianle Duan,Qingchun Li*

Main category: cs.LG

TL;DR: 研究提出机器学习框架将纽约市出租车OD流从大空间单元降尺度到小空间单元，对比四种模型，NN训练和测试表现佳，SVM降尺度泛化能力好。


<details>
  <summary>Details</summary>
Motivation: 理解不同空间尺度的城市人类移动模式对社会科学很重要，需要对出租车OD流进行降尺度分析。

Method: 使用线性回归、随机森林、支持向量机和神经网络四种模型建立OD出行与人口、社会经济和通勤特征的相关性，用基于扰动的敏感性分析解释非线性模型的变量重要性。

Result: 线性回归模型未能捕捉复杂变量交互，NN在训练和测试数据集表现最佳，SVM在降尺度性能上泛化能力最好。

Conclusion: 研究方法为改善交通服务和城市发展提供分析进展和实际应用。

Abstract: Understanding urban human mobility patterns at various spatial levels is
essential for social science. This study presents a machine learning framework
to downscale origin-destination (OD) taxi trips flows in New York City from a
larger spatial unit to a smaller spatial unit. First, correlations between OD
trips and demographic, socioeconomic, and commuting characteristics are
developed using four models: Linear Regression (LR), Random Forest (RF),
Support Vector Machine (SVM), and Neural Networks (NN). Second, a
perturbation-based sensitivity analysis is applied to interpret variable
importance for nonlinear models. The results show that the linear regression
model failed to capture the complex variable interactions. While NN performs
best with the training and testing datasets, SVM shows the best generalization
ability in downscaling performance. The methodology presented in this study
provides both analytical advancement and practical applications to improve
transportation services and urban development.

</details>


### [151] [PQFed: A Privacy-Preserving Quality-Controlled Federated Learning Framework](https://arxiv.org/abs/2509.21704)
*Weiqi Yue,Wenbiao Li,Yuzhou Jiang,Anisa Halimi,Roger French,Erman Ayday*

Main category: cs.LG

TL;DR: 本文提出PQFed框架，通过早期质量控制和客户选择策略提升个性化联邦学习性能，实验显示其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中数据异质性对全局模型性能的挑战，改进现有优化方法。

Method: 设计PQFed框架，提取特征、聚类估算数据集相似度，实施客户选择策略。

Result: 在CIFAR - 10和MNIST数据集上与三种算法结合，提升目标客户模型性能，比IFCA在低参与场景表现更好。

Conclusion: PQFed在个性化联邦学习中有良好的可扩展性和有效性。

Abstract: Federated learning enables collaborative model training without sharing raw
data, but data heterogeneity consistently challenges the performance of the
global model. Traditional optimization methods often rely on collaborative
global model training involving all clients, followed by local adaptation to
improve individual performance. In this work, we focus on early-stage quality
control and propose PQFed, a novel privacy-preserving personalized federated
learning framework that designs customized training strategies for each client
prior to the federated training process. PQFed extracts representative features
from each client's raw data and applies clustering techniques to estimate
inter-client dataset similarity. Based on these similarity estimates, the
framework implements a client selection strategy that enables each client to
collaborate with others who have compatible data distributions. We evaluate
PQFed on two benchmark datasets, CIFAR-10 and MNIST, integrated with three
existing federated learning algorithms. Experimental results show that PQFed
consistently improves the target client's model performance, even with a
limited number of participants. We further benchmark PQFed against a baseline
cluster-based algorithm, IFCA, and observe that PQFed also achieves better
performance in low-participation scenarios. These findings highlight PQFed's
scalability and effectiveness in personalized federated learning settings.

</details>


### [152] [A Unifying Framework for Parallelizing Sequential Models with Linear Dynamical Systems](https://arxiv.org/abs/2509.21716)
*Xavier Gonzalez,E. Kelly Buchanan,Hyun Dong Lee,Jerry Weihong Liu,Ke Alexander Wang,David M. Zoltowski,Christopher Ré,Scott W. Linderman*

Main category: cs.LG

TL;DR: 本文提出用线性动力系统（LDS）框架统一理解评估顺序过程的并行方法，为并行化顺序模型提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 挖掘看似顺序的模型中的并行性是现代机器学习的核心挑战，已有多种用定点方法并行评估顺序过程的方法，需统一理解。

Method: 将现有定点方法置于基于线性动力系统（LDS）的通用框架中，将不同迭代方案视为非线性递归的近似线性化。

Result: 该框架突出了这些技术背后的共同原理，明确了特定定点方法最可能有效的时机。

Conclusion: 通过LDS语言连接不同算法，此框架为并行化顺序模型提供了更清晰的理论基础，并指出了高效可扩展计算的新机会。

Abstract: Harnessing parallelism in seemingly sequential models is a central challenge
for modern machine learning. Several approaches have been proposed for
evaluating sequential processes in parallel using fixed-point methods, like
Newton, Picard, and Jacobi iterations. In this work, we show that these methods
can be understood within a common framework based on linear dynamical systems
(LDSs), where different iteration schemes arise naturally as approximate
linearizations of a nonlinear recursion. This unifying view highlights shared
principles behind these techniques and clarifies when particular fixed-point
methods are most likely to be effective. By bridging diverse algorithms through
the language of LDSs, our framework provides a clearer theoretical foundation
for parallelizing sequential models and points toward new opportunities for
efficient and scalable computation.

</details>


### [153] [Information-Theoretic Bayesian Optimization for Bilevel Optimization Problems](https://arxiv.org/abs/2509.21725)
*Takuya Kanayama,Yuki Ito,Tomoyuki Tamura,Masayuki Karasuyama*

Main category: cs.LG

TL;DR: 本文针对上下层均含昂贵黑箱函数的双层优化问题，提出信息论方法并展示实用下界法评估信息增益，通过基准数据集验证有效性。


<details>
  <summary>Details</summary>
Motivation: 双层优化问题结构嵌套，定义复杂，相比贝叶斯优化其他扩展研究较少，需有效方法解决上下层含昂贵黑箱函数的情况。

Method: 提出信息论方法，考虑上下层最优解和值的信息增益，定义统一标准衡量两层问题收益，还展示实用下界法评估信息增益。

Result: 通过几个基准数据集实证证明了所提方法的有效性。

Conclusion: 所提出的信息论方法及实用下界法能有效解决上下层含昂贵黑箱函数的双层优化问题。

Abstract: A bilevel optimization problem consists of two optimization problems nested
as an upper- and a lower-level problem, in which the optimality of the
lower-level problem defines a constraint for the upper-level problem. This
paper considers Bayesian optimization (BO) for the case that both the upper-
and lower-levels involve expensive black-box functions. Because of its nested
structure, bilevel optimization has a complex problem definition and, compared
with other standard extensions of BO such as multi-objective or constraint
settings, it has not been widely studied. We propose an information-theoretic
approach that considers the information gain of both the upper- and
lower-optimal solutions and values. This enables us to define a unified
criterion that measures the benefit for both level problems, simultaneously.
Further, we also show a practical lower bound based approach to evaluating the
information gain. We empirically demonstrate the effectiveness of our proposed
method through several benchmark datasets.

</details>


### [154] [Uncovering Alzheimer's Disease Progression via SDE-based Spatio-Temporal Graph Deep Learning on Longitudinal Brain Networks](https://arxiv.org/abs/2509.21735)
*Houliang Zhou,Rong Zhou,Yangying Liu,Kanhao Zhao,Li Shen,Brian Y. Chen,Yu Zhang,Lifang He,Alzheimer's Disease Neuroimaging Initiative*

Main category: cs.LG

TL;DR: 提出可解释的时空图神经网络框架预测阿尔茨海默病进展，在两个队列验证，发现关键脑区异常及生物标志物。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以识别预测阿尔茨海默病进展的神经影像生物标志物，因其忽视大脑网络时空特征的复杂功能障碍。

Method: 开发可解释的时空图神经网络框架，利用双随机微分方程对不规则采样的纵向功能磁共振成像数据建模。

Result: 框架有效学习稀疏区域和连接重要性概率，发现海马旁回、前额叶皮质等关键脑区异常，与临床症状强相关，揭示新生物标志物。

Conclusion: 基于时空图的学习方法在早期、个体化预测阿尔茨海默病进展方面有潜力，即使面对不规则采样的纵向成像数据。

Abstract: Identifying objective neuroimaging biomarkers to forecast Alzheimer's disease
(AD) progression is crucial for timely intervention. However, this task remains
challenging due to the complex dysfunctions in the spatio-temporal
characteristics of underlying brain networks, which are often overlooked by
existing methods. To address these limitations, we develop an interpretable
spatio-temporal graph neural network framework to predict future AD
progression, leveraging dual Stochastic Differential Equations (SDEs) to model
the irregularly-sampled longitudinal functional magnetic resonance imaging
(fMRI) data. We validate our approach on two independent cohorts, including the
Open Access Series of Imaging Studies (OASIS-3) and the Alzheimer's Disease
Neuroimaging Initiative (ADNI). Our framework effectively learns sparse
regional and connective importance probabilities, enabling the identification
of key brain circuit abnormalities associated with disease progression.
Notably, we detect the parahippocampal cortex, prefrontal cortex, and parietal
lobule as salient regions, with significant disruptions in the ventral
attention, dorsal attention, and default mode networks. These abnormalities
correlate strongly with longitudinal AD-related clinical symptoms. Moreover,
our interpretability strategy reveals both established and novel neural
systems-level and sex-specific biomarkers, offering new insights into the
neurobiological mechanisms underlying AD progression. Our findings highlight
the potential of spatio-temporal graph-based learning for early, individualized
prediction of AD progression, even in the context of irregularly-sampled
longitudinal imaging data.

</details>


### [155] [POLO: Preference-Guided Multi-Turn Reinforcement Learning for Lead Optimization](https://arxiv.org/abs/2509.21737)
*Ziqing Wang,Yibo Wen,William Pattie,Xiao Luo,Weimin Wu,Jerry Yao-Chieh Hu,Abhishek Pandey,Han Liu,Kaize Ding*

Main category: cs.LG

TL;DR: 提出POLO方法用于先导化合物优化，通过双级学习提升样本效率，实验效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法样本效率低，现有基于大语言模型的方法未充分利用其优势，需改进。

Method: 提出POLO方法，引入PGPO强化学习算法，从轨迹和回合两个层面提取学习信号。

Result: POLO在单属性任务平均成功率达84%（比基线好2.3倍），多属性任务达50%，仅用500次oracle评估。

Conclusion: POLO显著提升了样本高效分子优化的技术水平。

Abstract: Lead optimization in drug discovery requires efficiently navigating vast
chemical space through iterative cycles to enhance molecular properties while
preserving structural similarity to the original lead compound. Despite recent
advances, traditional optimization methods struggle with sample
efficiency-achieving good optimization performance with limited oracle
evaluations. Large Language Models (LLMs) provide a promising approach through
their in-context learning and instruction following capabilities, which align
naturally with these iterative processes. However, existing LLM-based methods
fail to leverage this strength, treating each optimization step independently.
To address this, we present POLO (Preference-guided multi-turn Optimization for
Lead Optimization), which enables LLMs to learn from complete optimization
trajectories rather than isolated steps. At its core, POLO introduces
Preference-Guided Policy Optimization (PGPO), a novel reinforcement learning
algorithm that extracts learning signals at two complementary levels:
trajectory-level optimization reinforces successful strategies, while
turn-level preference learning provides dense comparative feedback by ranking
intermediate molecules within each trajectory. Through this dual-level learning
from intermediate evaluation, POLO achieves superior sample efficiency by fully
exploiting each costly oracle call. Extensive experiments demonstrate that POLO
achieves 84% average success rate on single-property tasks (2.3x better than
baselines) and 50% on multi-property tasks using only 500 oracle evaluations,
significantly advancing the state-of-the-art in sample-efficient molecular
optimization.

</details>


### [156] [Brain PathoGraph Learning](https://arxiv.org/abs/2509.21742)
*Ciyuan Peng,Nguyen Linh Dan Le,Shan Jin,Dexuan Ding,Shuo Yu,Feng Xia*

Main category: cs.LG

TL;DR: 提出轻量级BrainPoG模型用于高效脑图学习，经实验验证其在性能和效率上有优势。


<details>
  <summary>Details</summary>
Motivation: 现有脑图学习方法难以选择性学习疾病相关知识，参数和计算成本高，影响效率和临床实用性。

Method: 提出BrainPoG模型，先通过滤波器提取病理模式实现图剪枝和病灶定位，构建PathoGraph，再用病理特征蒸馏模块减少无关噪声特征、增强病理特征。

Result: 在四个基准数据集的实验表明，BrainPoG在各种脑疾病检测任务的模型性能和计算效率上表现优越。

Conclusion: BrainPoG能专门学习疾病相关知识，避免无关信息，实现高效脑图学习。

Abstract: Brain graph learning has demonstrated significant achievements in the fields
of neuroscience and artificial intelligence. However, existing methods struggle
to selectively learn disease-related knowledge, leading to heavy parameters and
computational costs. This challenge diminishes their efficiency, as well as
limits their practicality for real-world clinical applications. To this end, we
propose a lightweight Brain PathoGraph Learning (BrainPoG) model that enables
efficient brain graph learning by pathological pattern filtering and
pathological feature distillation. Specifically, BrainPoG first contains a
filter to extract the pathological pattern formulated by highly
disease-relevant subgraphs, achieving graph pruning and lesion localization. A
PathoGraph is therefore constructed by dropping less disease-relevant subgraphs
from the whole brain graph. Afterwards, a pathological feature distillation
module is designed to reduce disease-irrelevant noise features and enhance
pathological features of each node in the PathoGraph. BrainPoG can exclusively
learn informative disease-related knowledge while avoiding less relevant
information, achieving efficient brain graph learning. Extensive experiments on
four benchmark datasets demonstrate that BrainPoG exhibits superiority in both
model performance and computational efficiency across various brain disease
detection tasks.

</details>


### [157] [HyperCore: Coreset Selection under Noise via Hypersphere Models](https://arxiv.org/abs/2509.21746)
*Brian B. Moser,Arundhati S. Shanbhag,Tobias C. Nauen,Stanislav Frolov,Federico Raue,Joachim Folz,Andreas Dengel*

Main category: cs.LG

TL;DR: 提出HyperCore框架用于嘈杂环境下的核心集选择，实验显示其优于现有方法，能有效丢弃不良数据。


<details>
  <summary>Details</summary>
Motivation: 现有核心集选择方法忽略标注错误且需固定修剪率，在现实中不实用。

Method: 利用按类学习的轻量级超球模型，通过Youden's J统计量自适应选择修剪阈值。

Result: HyperCore在嘈杂和低数据情况下始终优于现有核心集选择方法。

Conclusion: HyperCore能有效丢弃错误和模糊数据点，产生适合可扩展无噪声学习的紧凑且信息丰富的子集。

Abstract: The goal of coreset selection methods is to identify representative subsets
of datasets for efficient model training. Yet, existing methods often ignore
the possibility of annotation errors and require fixed pruning ratios, making
them impractical in real-world settings. We present HyperCore, a robust and
adaptive coreset selection framework designed explicitly for noisy
environments. HyperCore leverages lightweight hypersphere models learned per
class, embedding in-class samples close to a hypersphere center while naturally
segregating out-of-class samples based on their distance. By using Youden's J
statistic, HyperCore can adaptively select pruning thresholds, enabling
automatic, noise-aware data pruning without hyperparameter tuning. Our
experiments reveal that HyperCore consistently surpasses state-of-the-art
coreset selection methods, especially under noisy and low-data regimes.
HyperCore effectively discards mislabeled and ambiguous points, yielding
compact yet highly informative subsets suitable for scalable and noise-free
learning.

</details>


### [158] [SubZeroCore: A Submodular Approach with Zero Training for Coreset Selection](https://arxiv.org/abs/2509.21748)
*Brian B. Moser,Tobias C. Nauen,Arundhati S. Shanbhag,Federico Raue,Stanislav Frolov,Joachim Folz,Andreas Dengel*

Main category: cs.LG

TL;DR: 提出无训练的coreset选择方法SubZeroCore，表现佳且降低计算开销，对标签噪声有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有coreset选择方法需昂贵训练信号，违背其高效训练初衷。

Method: 将子模覆盖和密度集成到统一目标，基于闭式解设计采样策略，用单个超参数控制覆盖和密度平衡。

Result: SubZeroCore与基于训练的基线方法表现相当，高剪枝率时显著优于基线，大幅降低计算开销。

Conclusion: SubZeroCore实用有效且可扩展，适用于现实场景。

Abstract: The goal of coreset selection is to identify representative subsets of
datasets for efficient model training. Yet, existing approaches paradoxically
require expensive training-based signals, e.g., gradients, decision boundary
estimates or forgetting counts, computed over the entire dataset prior to
pruning, which undermines their very purpose by requiring training on samples
they aim to avoid. We introduce SubZeroCore, a novel, training-free coreset
selection method that integrates submodular coverage and density into a single,
unified objective. To achieve this, we introduce a sampling strategy based on a
closed-form solution to optimally balance these objectives, guided by a single
hyperparameter that explicitly controls the desired coverage for local density
measures. Despite no training, extensive evaluations show that SubZeroCore
matches training-based baselines and significantly outperforms them at high
pruning rates, while dramatically reducing computational overhead. SubZeroCore
also demonstrates superior robustness to label noise, highlighting its
practical effectiveness and scalability for real-world scenarios.

</details>


### [159] [Reparameterizing 4DVAR with neural fields](https://arxiv.org/abs/2509.21751)
*Jaemin Oh*

Main category: cs.LG

TL;DR: 提出基于神经场的四维变分数据同化（4DVAR）重新表述方法，可并行优化、简化实现并降低计算成本，评估显示该方法估计初始条件更稳定，且无需真实数据。


<details>
  <summary>Details</summary>
Motivation: 经典4DVAR成本函数难优化且计算量大。

Method: 将全时空状态表示为神经网络参数化的连续函数，去除时间顺序依赖以实现并行优化，通过物理信息损失纳入物理约束。

Result: 与基线4DVAR实现相比，神经重新参数化变体产生更稳定的初始条件估计，无虚假振荡。

Conclusion: 该框架无需真实状态或再分析数据，拓宽了在参考信息有限场景的适用性。

Abstract: Four-dimensional variational data assimilation (4DVAR) is a cornerstone of
numerical weather prediction, but its cost function is difficult to optimize
and computationally intensive. We propose a neural field-based reformulation in
which the full spatiotemporal state is represented as a continuous function
parameterized by a neural network. This reparameterization removes the
time-sequential dependency of classical 4DVAR, enabling parallel-in-time
optimization in parameter space. Physical constraints are incorporated directly
through a physics-informed loss, simplifying implementation and reducing
computational cost. We evaluate the method on the two-dimensional
incompressible Navier--Stokes equations with Kolmogorov forcing. Compared to a
baseline 4DVAR implementation, the neural reparameterized variants produce more
stable initial condition estimates without spurious oscillations. Notably,
unlike most machine learning-based approaches, our framework does not require
access to ground-truth states or reanalysis data, broadening its applicability
to settings with limited reference information.

</details>


### [160] [Machine Learning and AI Applied to fNIRS Data Reveals Novel Brain Activity Biomarkers in Stable Subclinical Multiple Sclerosis](https://arxiv.org/abs/2509.21770)
*Sadman Saumik Islam,Bruna Dalcin Baldasso,Davide Cattaneo,Xianta Jiang,Michelle Ploughman*

Main category: cs.LG

TL;DR: 研究旨在检测完成灵巧任务时认知疲劳的脑活动生物标志物，招募MS患者和对照完成手灵巧任务，用机器学习分析fNIRS数据，发现新生物标志物。


<details>
  <summary>Details</summary>
Motivation: 检测能解释完成灵巧任务时认知疲劳主观报告的脑活动生物标志物，为未来脑刺激治疗提供靶点。

Method: 招募无手部、行动和认知障碍的MS患者及对照，完成单任务和双任务手灵巧测试，用机器学习框架分析fNIRS数据分类患者和对照。

Result: K近邻分类器单任务准确率75.0%，双任务66.7%；重要脑区为同侧缘上/角回和中央前回；脱氧血红蛋白水平是更好预测指标。

Conclusion: 非传统fNIRS数据分析方法揭示了新的脑活动生物标志物，有助于开发个性化脑刺激靶点。

Abstract: People with Multiple Sclerosis (MS) complain of problems with hand dexterity
and cognitive fatigue. However, in many cases, impairments are subtle and
difficult to detect. Functional near-infrared spectroscopy (fNIRS) is a
non-invasive neuroimaging technique that measures brain hemodynamic responses
during cognitive or motor tasks. We aimed to detect brain activity biomarkers
that could explain subjective reports of cognitive fatigue while completing
dexterous tasks and provide targets for future brain stimulation treatments. We
recruited 15 people with MS who did not have a hand (Nine Hole Peg Test
[NHPT]), mobility, or cognitive impairment, and 12 age- and sex-matched
controls. Participants completed two types of hand dexterity tasks with their
dominant hand, single task and dual task (NHPT while holding a ball between the
fifth finger and hypothenar eminence of the same hand). We analyzed fNIRS data
(oxygenated and deoxygenated hemoglobin levels) using a machine learning
framework to classify MS patients from controls based on their brain activation
patterns in bilateral prefrontal and sensorimotor cortices. The K-Nearest
Neighbor classifier achieved an accuracy of 75.0% for single manual dexterity
tasks and 66.7% for the more complex dual manual dexterity tasks. Using XAI, we
found that the most important brain regions contributing to the machine
learning model were the supramarginal/angular gyri and the precentral gyrus
(sensory integration and motor regions) of the ipsilateral hemisphere, with
suppressed activity and slower neurovascular response in the MS group. During
both tasks, deoxygenated hemoglobin levels were better predictors than the
conventional measure of oxygenated hemoglobin. This nonconventional method of
fNIRS data analysis revealed novel brain activity biomarkers that can help
develop personalized brain stimulation targets.

</details>


### [161] [Beyond Formula Complexity: Effective Information Criterion Improves Performance and Interpretability for Symbolic Regression](https://arxiv.org/abs/2509.21780)
*Zihan Yu,Guanren Wang,Jingtao Ding,Huandong Wang,Yong Li*

Main category: cs.LG

TL;DR: 提出有效信息准则 (EIC) 改善符号回归公式可解释性，结合不同算法提升性能和样本效率，与人类专家偏好有较高一致性。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归方法用复杂度指标代替可解释性，忽略公式内部数学结构，导致发现的公式数学结构难分析解释。

Method: 受物理公式在有限计算精度下数值稳定的启发，提出 EIC，将公式视为信息处理系统，通过数据流经系统时有效数字丢失或舍入噪声放大识别不合理结构。

Result: 结合搜索式算法提升帕累托前沿性能、减少结果中不合理结构；结合生成式算法减少预训练样本需求，样本效率提高 2 - 4 倍；与人类专家对公式可解释性偏好的一致性达 70.2%。

Conclusion: EIC 通过衡量公式中不合理结构，实际反映了公式的可解释性。

Abstract: Symbolic regression discovers accurate and interpretable formulas to describe
given data, thereby providing scientific insights for domain experts and
promoting scientific discovery. However, existing symbolic regression methods
often use complexity metrics as a proxy for interoperability, which only
considers the size of the formula but ignores its internal mathematical
structure. Therefore, while they can discover formulas with compact forms, the
discovered formulas often have structures that are difficult to analyze or
interpret mathematically. In this work, inspired by the observation that
physical formulas are typically numerically stable under limited calculation
precision, we propose the Effective Information Criterion (EIC). It treats
formulas as information processing systems with specific internal structures
and identifies the unreasonable structure in them by the loss of significant
digits or the amplification of rounding noise as data flows through the system.
We find that this criterion reveals the gap between the structural rationality
of models discovered by existing symbolic regression algorithms and real-world
physical formulas. Combining EIC with various search-based symbolic regression
algorithms improves their performance on the Pareto frontier and reduces the
irrational structure in the results. Combining EIC with generative-based
algorithms reduces the number of samples required for pre-training, improving
sample efficiency by 2~4 times. Finally, for different formulas with similar
accuracy and complexity, EIC shows a 70.2% agreement with 108 human experts'
preferences for formula interpretability, demonstrating that EIC, by measuring
the unreasonable structures in formulas, actually reflects the formula's
interpretability.

</details>


### [162] [FastGRPO: Accelerating Policy Optimization via Concurrency-aware Speculative Decoding and Online Draft Learning](https://arxiv.org/abs/2509.21792)
*Yizhou Zhang,Ning Lv,Teng Wang,Jisheng Dang*

Main category: cs.LG

TL;DR: 本文提出并发感知推测解码框架和在线草稿学习机制加速GRPO训练，实验显示有显著效率提升。


<details>
  <summary>Details</summary>
Motivation: GRPO训练过程过慢，主要瓶颈在于生成阶段，且现有推测解码在高并发训练下加速有限，同时训练中目标模型和草稿模型分布漂移导致性能下降。

Method: 提出并发感知推测解码框架，根据实时并发水平动态调整起草和验证策略；引入在线草稿学习机制，让草稿模型利用目标模型的反馈信号不断适应。

Result: 在多个数学推理数据集和模型上实验表明，该方法实现了2.35x到2.72x的端到端加速，效率远超基线方法。

Conclusion: 所提方法能有效加速GRPO训练，提升训练效率。

Abstract: Group relative policy optimization (GRPO) has demonstrated significant
potential in improving the reasoning capabilities of large language models
(LLMs) via reinforcement learning. However, its practical deployment is impeded
by an excessively slow training process, primarily attributed to the
computationally intensive autoregressive generation of multiple responses per
query, which makes the generation phase the primary performance bottleneck.
Although speculative decoding presents a promising direction for acceleration,
its direct application in GRPO achieves limited speedup under high-concurrency
training conditions. To overcome this limitation, we propose a
concurrency-aware speculative decoding framework that dynamically adjusts the
drafting and verification strategy according to real-time concurrency levels,
thereby maximizing the acceleration of the generation process. Furthermore, to
address performance degradation arising from distributional drift between the
evolving target model and the fixed draft model during training, we introduce
an online draft learning mechanism that enables the draft model to continuously
adapt using feedback signals from the target model. Experimental results across
multiple mathematical reasoning datasets and models demonstrate that the
proposed method achieves end-to-end speedups of 2.35x to 2.72x, significantly
surpassing baseline approaches in efficiency. The code is available at
https://github.com/yedaotian9/GRPO_speculative.

</details>


### [163] [Exploring the Relationships Between Physiological Signals During Automated Fatigue Detection](https://arxiv.org/abs/2509.21794)
*Kourosh Kakhi,Abbas Khosravi,Roohallah Alizadehsani,U. Rajendra Acharyab*

Main category: cs.LG

TL;DR: 本文利用DROZY数据集研究生理信号对的统计关系以改进疲劳检测分类鲁棒性，发现多信号模型优于单信号模型，EMG+EEG组合下XGBoost表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有疲劳检测研究多聚焦单模态，本文旨在研究信号对间统计关系以提高分类鲁棒性。

Method: 从DROZY数据集中提取ECG、EMG、EOG和EEG的特征，采用15种信号组合，并使用决策树、随机森林、逻辑回归和XGBoost进行评估。

Result: XGBoost在EMG+EEG组合下性能最佳，SHAP分析表明ECG+EOG相关性是关键特征，多信号模型始终优于单信号模型。

Conclusion: 生理信号的特征级融合可提高疲劳监测系统的准确性、可解释性和实际适用性。

Abstract: Fatigue detection using physiological signals is critical in domains such as
transportation, healthcare, and performance monitoring. While most studies
focus on single modalities, this work examines statistical relationships
between signal pairs to improve classification robustness. Using the DROZY
dataset, we extracted features from ECG, EMG, EOG, and EEG across 15 signal
combinations and evaluated them with Decision Tree, Random Forest, Logistic
Regression, and XGBoost. Results show that XGBoost with the EMG EEG combination
achieved the best performance. SHAP analysis highlighted ECG EOG correlation as
a key feature, and multi signal models consistently outperformed single signal
ones. These findings demonstrate that feature level fusion of physiological
signals enhances accuracy, interpretability, and practical applicability of
fatigue monitoring systems.

</details>


### [164] [ChaosNexus: A Foundation Model for Universal Chaotic System Forecasting with Multi-scale Representations](https://arxiv.org/abs/2509.21802)
*Chang Liu,Bohao Zhao,Jingtao Ding,Yong Li*

Main category: cs.LG

TL;DR: 提出ChaosNexus基础模型以解决混沌系统预测泛化问题，该模型表现优异，实验为科学基础模型提供原则。


<details>
  <summary>Details</summary>
Motivation: 传统建模方法因混沌系统对初始条件敏感、观测数据少，缺乏泛化能力，难以用于现实场景。

Method: 提出在多样混沌动力学语料上预训练的ChaosNexus模型，采用ScaleFormer多尺度架构并增加Mixture-of-Experts层。

Result: 在合成和真实基准测试中零样本泛化表现达到最优；在超9000个合成混沌系统上，长期吸引子统计保真度提升超40%；5天全球天气预报中零样本平均误差低于1度。

Conclusion: 跨系统泛化源于训练系统的多样性，而非数据量。

Abstract: Accurately forecasting chaotic systems, prevalent in domains such as weather
prediction and fluid dynamics, remains a significant scientific challenge. The
inherent sensitivity of these systems to initial conditions, coupled with a
scarcity of observational data, severely constrains traditional modeling
approaches. Since these models are typically trained for a specific system,
they lack the generalization capacity necessary for real-world applications,
which demand robust zero-shot or few-shot forecasting on novel or data-limited
scenarios. To overcome this generalization barrier, we propose ChaosNexus, a
foundation model pre-trained on a diverse corpus of chaotic dynamics.
ChaosNexus employs a novel multi-scale architecture named ScaleFormer augmented
with Mixture-of-Experts layers, to capture both universal patterns and
system-specific behaviors. The model demonstrates state-of-the-art zero-shot
generalization across both synthetic and real-world benchmarks. On a
large-scale testbed comprising over 9,000 synthetic chaotic systems, it
improves the fidelity of long-term attractor statistics by more than 40%
compared to the leading baseline. This robust performance extends to real-world
applications with exceptional data efficiency. For instance, in 5-day global
weather forecasting, ChaosNexus achieves a competitive zero-shot mean error
below 1 degree, a result that further improves with few-shot fine-tuning.
Moreover, experiments on the scaling behavior of ChaosNexus provide a guiding
principle for scientific foundation models: cross-system generalization stems
from the diversity of training systems, rather than sheer data volume.

</details>


### [165] [Scaling Laws for Neural Material Models](https://arxiv.org/abs/2509.21811)
*Akshay Trikha,Kyle Chu,Advait Gosai,Parker Szachta,Eric Weiner*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Predicting material properties is crucial for designing better batteries,
semiconductors, and medical devices. Deep learning helps scientists quickly
find promising materials by predicting their energy, forces, and stresses.
Companies scale capacities of deep learning models in multiple domains, such as
language modeling, and invest many millions of dollars into such models. Our
team analyzes how scaling training data (giving models more information to
learn from), model sizes (giving models more capacity to learn patterns), and
compute (giving models more computational resources) for neural networks
affects their performance for material property prediction. In particular, we
trained both transformer and EquiformerV2 neural networks to predict material
properties. We find empirical scaling laws for these models: we can predict how
increasing each of the three hyperparameters (training data, model size, and
compute) affects predictive performance. In particular, the loss $L$ can be
measured with a power law relationship $L = \alpha \cdot N^{-\beta}$, where
$\alpha$ and $\beta$ are constants while $N$ is the relevant hyperparameter. We
also incorporate command-line arguments for changing training settings such as
the amount of epochs, maximum learning rate, and whether mixed precision is
enabled. Future work could entail further investigating scaling laws for other
neural network models in this domain, such as GemNet and fully connected
networks, to assess how they compare to the models we trained.

</details>


### [166] [Sharpness-Aware Minimization Can Hallucinate Minimizers](https://arxiv.org/abs/2509.21818)
*Chanwoong Park,Uijeong Jang,Ernest K. Ryu,Insoon Yang*

Main category: cs.LG

TL;DR: 研究指出SAM会收敛到幻觉极小值点，理论证明其存在并给出收敛条件，实证其存在，还提出避免方法。


<details>
  <summary>Details</summary>
Motivation: 探究广泛使用的SAM方法可能存在的问题，即是否会收敛到非原目标极小值点。

Method: 理论证明幻觉极小值点的存在并建立局部收敛条件，进行实证研究。

Result: SAM会收敛到幻觉极小值点，且能在实践中发生。

Conclusion: 提出简单有效的避免幻觉极小值点的补救方法。

Abstract: Sharpness-Aware Minimization (SAM) is a widely used method that steers
training toward flatter minimizers, which typically generalize better. In this
work, however, we show that SAM can converge to hallucinated minimizers --
points that are not minimizers of the original objective. We theoretically
prove the existence of such hallucinated minimizers and establish conditions
for local convergence to them. We further provide empirical evidence
demonstrating that SAM can indeed converge to these points in practice.
Finally, we propose a simple yet effective remedy for avoiding hallucinated
minimizers.

</details>


### [167] [Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.21828)
*The Viet Bui,Tien Mai,Hong Thanh Nguyen*

Main category: cs.LG

TL;DR: 提出新框架解决在线多智能体强化学习中稀疏奖励问题，结合在线逆偏好学习与多智能体策略优化，利用大语言模型提供偏好标签，在基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 在线多智能体强化学习在稀疏奖励环境下，缺乏中间奖励阻碍标准算法有效指导策略学习。

Method: 提出集成在线逆偏好学习与多智能体策略优化的框架，引入基于偏好值分解网络的隐式多智能体奖励学习模型，构建双优势流，利用大语言模型提供偏好标签。

Result: 在MAMuJoCo和SMACv2等基准测试中，方法表现优于现有基线。

Conclusion: 所提方法能有效解决在线多智能体强化学习中的稀疏奖励挑战。

Abstract: We study the problem of online multi-agent reinforcement learning (MARL) in
environments with sparse rewards, where reward feedback is not provided at each
interaction but only revealed at the end of a trajectory. This setting, though
realistic, presents a fundamental challenge: the lack of intermediate rewards
hinders standard MARL algorithms from effectively guiding policy learning. To
address this issue, we propose a novel framework that integrates online inverse
preference learning with multi-agent on-policy optimization into a unified
architecture. At its core, our approach introduces an implicit multi-agent
reward learning model, built upon a preference-based value-decomposition
network, which produces both global and local reward signals. These signals are
further used to construct dual advantage streams, enabling differentiated
learning targets for the centralized critic and decentralized actors. In
addition, we demonstrate how large language models (LLMs) can be leveraged to
provide preference labels that enhance the quality of the learned reward model.
Empirical evaluations on state-of-the-art benchmarks, including MAMuJoCo and
SMACv2, show that our method achieves superior performance compared to existing
baselines, highlighting its effectiveness in addressing sparse-reward
challenges in online MARL.

</details>


### [168] [On the Complexity Theory of Masked Discrete Diffusion: From $\mathrm{poly}(1/ε)$ to Nearly $ε$-Free](https://arxiv.org/abs/2509.21835)
*Xunpeng Huang,Yingyu Lin,Nishant Jain,Kaibo Wang,Difan Zou,Yian Ma,Tong Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study masked discrete diffusion -- a flexible paradigm for text generation
in which tokens are progressively corrupted by special mask symbols before
being denoised. Although this approach has demonstrated strong empirical
performance, its theoretical complexity in high-dimensional settings remains
insufficiently understood. Existing analyses largely focus on uniform discrete
diffusion, and more recent attempts addressing masked diffusion either (1)
overlook widely used Euler samplers, (2) impose restrictive bounded-score
assumptions, or (3) fail to showcase the advantages of masked discrete
diffusion over its uniform counterpart. To address this gap, we show that Euler
samplers can achieve $\epsilon$-accuracy in total variation (TV) with
$\tilde{O}(d^{2}\epsilon^{-3/2})$ discrete score evaluations, thereby providing
the first rigorous analysis of typical Euler sampler in masked discrete
diffusion. We then propose a Mask-Aware Truncated Uniformization (MATU)
approach that both removes bounded-score assumptions and preserves unbiased
discrete score approximation. By exploiting the property that each token can be
unmasked at most once, MATU attains a nearly $\epsilon$-free complexity of
$O(d\,\ln d\cdot (1-\epsilon^2))$. This result surpasses existing
uniformization methods under uniform discrete diffusion, eliminating the
$\ln(1/\epsilon)$ factor and substantially speeding up convergence. Our
findings not only provide a rigorous theoretical foundation for masked discrete
diffusion, showcasing its practical advantages over uniform diffusion for text
generation, but also pave the way for future efforts to analyze diffusion-based
language models developed under masking paradigm.

</details>


### [169] [Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration](https://arxiv.org/abs/2509.21848)
*Taejong Joo,Shu Ishida,Ivan Sosnovik,Bryan Lim,Sahand Rezaei-Shoshtari,Adam Gaier,Robert Giaquinto*

Main category: cs.LG

TL;DR: 提出Graph of Agents (GoA)框架处理长上下文建模问题，在多个基准测试中提升了性能，增加有效上下文长度。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统处理长上下文建模时依赖手工协作策略和提示工程，限制了泛化性。

Method: 将模型无关的长上下文建模问题形式化为压缩问题，提出GoA动态构建依赖输入的协作结构以最大化信息论压缩目标。

Result: 在六个文档问答基准测试中，GoA提升了检索增强生成的平均F1分数，超过固定协作结构的多智能体基线，甚至小上下文窗口的GoA在LongBench上超过大上下文窗口的Llama 3.1 8B。

Conclusion: GoA框架有效解决长上下文建模问题，提高了性能和有效上下文长度，代码开源。

Abstract: As a model-agnostic approach to long context modeling, multi-agent systems
can process inputs longer than a large language model's context window without
retraining or architectural modifications. However, their performance often
heavily relies on hand-crafted multi-agent collaboration strategies and prompt
engineering, which limit generalizability. In this work, we introduce a
principled framework that formalizes the model-agnostic long context modeling
problem as a compression problem, yielding an information-theoretic compression
objective. Building on this framework, we propose Graph of Agents (GoA), which
dynamically constructs an input-dependent collaboration structure that
maximizes this objective. For Llama 3.1 8B and Qwen3 8B across six document
question answering benchmarks, GoA improves the average $F_1$ score of
retrieval-augmented generation by 5.7\% and a strong multi-agent baseline using
a fixed collaboration structure by 16.35\%, respectively. Even with only a 2K
context window, GoA surpasses the 128K context window Llama 3.1 8B on
LongBench, showing a dramatic increase in effective context length. Our source
code is available at https://github.com/tjoo512/graph-of-agents.

</details>


### [170] [MolSpectLLM: A Molecular Foundation Model Bridging Spectroscopy, Molecule Elucidation, and 3D Structure Generation](https://arxiv.org/abs/2509.21861)
*Shuaike Shen,Jiaqing Xie,Zhuo Yang,Antong Zhang,Shuzhou Sun,Ben Gao,Tianfan Fu,Biqing Qi,Yuqiang Li*

Main category: cs.LG

TL;DR: 提出基于Qwen2.5 - 7B预训练的分子基础模型MolSpectLLM，统一实验光谱与分子3D结构，在多任务表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有分子基础模型多依赖SMILES表示，忽略实验光谱和3D结构信息，影响在关键任务的有效性。

Method: 提出基于Qwen2.5 - 7B预训练的MolSpectLLM，统一实验光谱与分子3D结构。

Result: 在光谱相关任务达SOTA，光谱分析任务表现强，能从SMILES或光谱输入生成准确3D分子结构。

Conclusion: MolSpectLLM在光谱分析、分子解析和分子设计任务表现良好，弥合三者差距。

Abstract: Recent advances in molecular foundation models have shown impressive
performance in molecular property prediction and de novo molecular design, with
promising applications in areas such as drug discovery and reaction prediction.
Nevertheless, most existing approaches rely exclusively on SMILES
representations and overlook both experimental spectra and 3D structural
information-two indispensable sources for capturing molecular behavior in
real-world scenarios. This limitation reduces their effectiveness in tasks
where stereochemistry, spatial conformation, and experimental validation are
critical. To overcome these challenges, we propose MolSpectLLM, a molecular
foundation model pretrained on Qwen2.5-7B that unifies experimental
spectroscopy with molecular 3D structure. By explicitly modeling molecular
spectra, MolSpectLLM achieves state-of-the-art performance on spectrum-related
tasks, with an average accuracy of 0.53 across NMR, IR, and MS benchmarks.
MolSpectLLM also shows strong performance on the spectra analysis task,
obtaining 15.5% sequence accuracy and 41.7% token accuracy on
Spectra-to-SMILES, substantially outperforming large general-purpose LLMs. More
importantly, MolSpectLLM not only achieves strong performance on molecular
elucidation tasks, but also generates accurate 3D molecular structures directly
from SMILES or spectral inputs, bridging spectral analysis, molecular
elucidation, and molecular design.

</details>


### [171] [Beyond RAG vs. Long-Context: Learning Distraction-Aware Retrieval for Efficient Knowledge Grounding](https://arxiv.org/abs/2509.21865)
*Seong-Woong Shim,Myunsoo Kim,Jae Hyeon Cho,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 提出LDAR自适应检索器，相比长上下文方法减少token使用并提高性能，实验证明其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有将完整文档上下文直接提供给模型的策略存在token效率低、加剧‘中间迷失’现象和分散注意力等问题，需要更好的方法。

Method: 提出LDAR（Learning Distraction - Aware Retrieval）自适应检索器，学习检索上下文以减少干扰。

Result: 在多种LLM架构和六个知识密集型基准测试中，LDAR方法有效且鲁棒。

Conclusion: 平衡信息覆盖和分散注意力之间的权衡很重要。

Abstract: Retrieval-Augmented Generation (RAG) is a framework for grounding Large
Language Models (LLMs) in external, up-to-date information. However, recent
advancements in context window size allow LLMs to process inputs of up to 128K
tokens or more, offering an alternative strategy: supplying the full document
context directly to the model, rather than relying on RAG to retrieve a subset
of contexts. Nevertheless, this emerging alternative strategy has notable
limitations: (i) it is token-inefficient to handle large and potentially
redundant contexts; (ii) it exacerbates the `lost in the middle' phenomenon;
and (iii) under limited model capacity, it amplifies distraction, ultimately
degrading LLM output quality. In this paper, we propose LDAR (Learning
Distraction-Aware Retrieval), an adaptive retriever that learns to retrieve
contexts in a way that mitigates interference from distracting passages,
thereby achieving significantly higher performance with reduced token usage
compared to long-context approaches. Extensive experiments across diverse LLM
architectures and six knowledge-intensive benchmarks demonstrate the
effectiveness and robustness of our approach, highlighting the importance of
balancing the trade-off between information coverage and distraction.

</details>


### [172] [Abductive Logical Rule Induction by Bridging Inductive Logic Programming and Multimodal Large Language Models](https://arxiv.org/abs/2509.21874)
*Yifei Peng,Yaoli Liu,Enbo Xia,Yu Jin,Wang-Zhou Dai,Zhong Ren,Yao-Xiang Ding,Kun Zhou*

Main category: cs.LG

TL;DR: 提出ILP - CoT方法，结合ILP和MLLMs进行溯因逻辑规则归纳，经基准测试验证有效。


<details>
  <summary>Details</summary>
Motivation: 单独使用ILP或MLLMs进行从少量非结构化文本或视觉输入中发现逻辑事实和归纳逻辑规则的任务存在挑战，如ILP需指定背景知识且计算成本高，MLLMs会出现感知幻觉。

Method: 基于MLLMs能提出结构正确规则的观察，自动构建具有修剪搜索空间的ILP任务，利用ILP系统输出基于纠正逻辑事实和形式归纳推理的规则。

Result: 通过具有挑战性的逻辑归纳基准测试验证了其有效性，还有规则归纳的文本到图像定制生成潜在应用。

Conclusion: ILP - CoT方法能有效结合ILP和MLLMs进行溯因逻辑规则归纳。

Abstract: We propose ILP-CoT, a method that bridges Inductive Logic Programming (ILP)
and Multimodal Large Language Models (MLLMs) for abductive logical rule
induction. The task involves both discovering logical facts and inducing
logical rules from a small number of unstructured textual or visual inputs,
which still remain challenging when solely relying on ILP, due to the
requirement of specified background knowledge and high computational cost, or
MLLMs, due to the appearance of perceptual hallucinations. Based on the key
observation that MLLMs could propose structure-correct rules even under
hallucinations, our approach automatically builds ILP tasks with pruned search
spaces based on the rule structure proposals from MLLMs, and utilizes ILP
system to output rules built upon rectified logical facts and formal inductive
reasoning. Its effectiveness is verified through challenging logical induction
benchmarks, as well as a potential application of our approach, namely
text-to-image customized generation with rule induction. Our code and data are
released at https://github.com/future-item/ILP-CoT.

</details>


### [173] [Zubov-Net: Adaptive Stability for Neural ODEs Reconciling Accuracy with Robustness](https://arxiv.org/abs/2509.21879)
*Chaoyang Luo,Yan Zou,Nanjing Huang*

Main category: cs.LG

TL;DR: 提出自适应稳定学习框架Zubov - Net，通过优化预设吸引域调和神经ODE的鲁棒性和准确性，理论证明其有效性，实验显示能保持高准确率并提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在为神经ODE提供鲁棒性保证时，存在鲁棒性和准确性的权衡难题，主要源于难以施加合适的稳定性条件。

Method: 将Zubov方程重新表述为吸引域和预设吸引域的一致性表征，通过三方损失和并行边界采样算法共同优化神经ODE和Lyapunov函数；设计基于输入注意力的凸神经网络增强Lyapunov函数的区分性。

Result: 理论上证明最小化三方损失能保证预设吸引域 - 吸引域的一致对齐、轨迹稳定性和预设吸引域不重叠，建立了随机凸可分性；实验中Zubov - Net保持高分类准确率，显著提升对各种随机噪声和对抗攻击的鲁棒性。

Conclusion: Zubov - Net有效调和了神经ODE的鲁棒性和准确性，在保持高准确率的同时增强了模型的鲁棒性。

Abstract: Despite neural ordinary differential equations (Neural ODEs) exhibiting
intrinsic robustness under input perturbations due to their dynamical systems
nature, recent approaches often involve imposing Lyapunov-based stability
conditions to provide formal robustness guarantees. However, a fundamental
challenge remains: the tension between robustness and accuracy, primarily
stemming from the difficulty in imposing appropriate stability conditions. To
address this, we propose an adaptive stable learning framework named Zubov-Net,
which innovatively reformulates Zubov's equation into a consistency
characterization between regions of attraction (RoAs) and prescribed RoAs
(PRoAs). Building on this consistency, we introduce a new paradigm for actively
controlling the geometry of RoAs by directly optimizing PRoAs to reconcile
accuracy and robustness. Our approach is realized through tripartite losses
(consistency, classification, and separation losses) and a parallel boundary
sampling algorithm that co-optimizes the Neural ODE and the Lyapunov function.
To enhance the discriminativity of Lyapunov functions, we design an
input-attention-based convex neural network via a softmax attention mechanism
that focuses on equilibrium-relevant features and also serves as weight
normalization to maintain training stability in deep architectures.
Theoretically, we prove that minimizing the tripartite loss guarantees
consistent alignment of PRoAs-RoAs, trajectory stability, and non-overlapping
PRoAs. Moreover, we establish stochastic convex separability with tighter
probability bounds and fewer dimensionality requirements to justify the convex
design in Lyapunov functions. Experimentally, Zubov-Net maintains high
classification accuracy while significantly improving robustness against
various stochastic noises and adversarial attacks.

</details>


### [174] [Position: The Hidden Costs and Measurement Gaps of Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2509.21882)
*Aaron Tu,Weihao Xuan,Heli Qi,Xu Huang,Qingcheng Zeng,Shayan Talaei,Yijia Xiao,Peng Xia,Xiangru Tang,Yuchen Zhuang,Bing Hu,Hanqun Cao,Wenqi Shi,Tianang Leng,Rui Yang,Yingjian Chen,Ziqi Wang,Irene Li,Nan Liu,Huaxiu Yao,Li Erran Li,Ge Liu,Amin Saberi,Naoto Yokoya,Jure Leskovec,Yejin Choi,Fang Wu*

Main category: cs.LG

TL;DR: 探讨强化学习与可验证奖励（RLVR）在提升大语言模型方面的实际效果，指出存在的问题并提出改进协议。


<details>
  <summary>Details</summary>
Motivation: 研究在严格对等控制评估下RLVR报告的收益情况，以及其是否有成本。

Method: 采用部分提示污染审计和跨基础与强化学习模型的匹配预算重现方法，提出税收感知的训练和评估协议。

Result: 在干净、对等控制评估下，一些显著差距缩小或消失，新协议能得出更可靠的推理收益估计。

Conclusion: RLVR有价值且适合行业应用，应在注重其实用效益的同时，优先考虑可靠性、安全性和测量。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a practical and
scalable approach to enhancing large language models in areas such as math,
code, and other structured tasks. Two questions motivate this paper: how much
of the reported gains survive under strictly parity-controlled evaluation, and
whether RLVR is cost-free or exacts a measurable tax. We argue that progress is
real, but gains are often overstated due to three forces - an RLVR tax,
evaluation pitfalls, and data contamination. Using a partial-prompt
contamination audit and matched-budget reproductions across base and RL models,
we show that several headline gaps shrink or vanish under clean,
parity-controlled evaluation. We then propose a tax-aware training and
evaluation protocol that co-optimizes accuracy, grounding, and calibrated
abstention and standardizes budgeting and provenance checks. Applied to recent
RLVR setups, this protocol yields more reliable estimates of reasoning gains
and, in several cases, revises prior conclusions. Our position is constructive:
RLVR is valuable and industry-ready; we advocate keeping its practical benefits
while prioritizing reliability, safety, and measurement.

</details>


### [175] [Closing the Oracle Gap: Increment Vector Transformation for Class Incremental Learning](https://arxiv.org/abs/2509.21898)
*Zihuan Qiu,Yi Xu,Fanman Meng,Runtong Zhang,Linfeng Xu,Qingbo Wu,Hongliang Li*

Main category: cs.LG

TL;DR: 文章受线性模式连通性启发，提出增量向量变换（IVT）框架缓解灾难性遗忘，实验表明IVT能提升强CIL基线性能。


<details>
  <summary>Details</summary>
Motivation: 当前CIL方法与全量数据训练的模型相比有性能差距，需解决灾难性遗忘问题。

Method: 提出IVT框架，周期性将模型参数转移到与先前任务最优解保持线性连通的变换解，用对角Fisher信息矩阵近似变换。

Result: 在多个数据集实验中，IVT提升了强CIL基线性能，如在CIFAR - 100和FGVCAircraft上有显著提升。

Conclusion: IVT是有效的CIL框架，能缓解灾难性遗忘，适合多种场景，可提升CIL性能。

Abstract: Class Incremental Learning (CIL) aims to sequentially acquire knowledge of
new classes without forgetting previously learned ones. Despite recent
progress, current CIL methods still exhibit significant performance gaps
compared to their oracle counterparts-models trained with full access to
historical data. Inspired by recent insights on Linear Mode Connectivity (LMC),
we revisit the geometric properties of oracle solutions in CIL and uncover a
fundamental observation: these oracle solutions typically maintain low-loss
linear connections to the optimum of previous tasks. Motivated by this finding,
we propose Increment Vector Transformation (IVT), a novel plug-and-play
framework designed to mitigate catastrophic forgetting during training. Rather
than directly following CIL updates, IVT periodically teleports the model
parameters to transformed solutions that preserve linear connectivity to
previous task optimum. By maintaining low-loss along these connecting paths,
IVT effectively ensures stable performance on previously learned tasks. The
transformation is efficiently approximated using diagonal Fisher Information
Matrices, making IVT suitable for both exemplar-free and exemplar-based
scenarios, and compatible with various initialization strategies. Extensive
experiments on CIFAR-100, FGVCAircraft, ImageNet-Subset, and ImageNet-Full
demonstrate that IVT consistently enhances the performance of strong CIL
baselines. Specifically, on CIFAR-100, IVT improves the last accuracy of the
PASS baseline by +5.12% and reduces forgetting by 2.54%. For the
CLIP-pre-trained SLCA baseline on FGVCAircraft, IVT yields gains of +14.93% in
average accuracy and +21.95% in last accuracy. The code will be released.

</details>


### [176] [Multiplicative-Additive Constrained Models:Toward Joint Visualization of Interactive and Independent Effects](https://arxiv.org/abs/2509.21923)
*Fumin Wang*

Main category: cs.LG

TL;DR: 本文介绍乘法 - 加法约束模型（MACMs），结合了乘法和加法部分，能自然可视化形状函数，实验表明基于神经网络的MACMs在预测性能上显著优于CESR和当前最先进的GAMs。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域应用机器学习时需考虑可解释性，广义相加模型（GAMs）虽增强可解释性但预测性能受限，曲线遍历集回归（CESR）能可视化形状函数并考虑特征交互，但性能不如GAMs，因此需要改进模型。

Method: 引入乘法 - 加法约束模型（MACMs），在CESR基础上增加加法部分，分离交互项和独立项的系数，拓宽假设空间。

Result: 基于神经网络的MACMs在预测性能上显著优于CESR和当前最先进的GAMs。

Conclusion: MACMs是对CESR和GAMs的改进，能有效提升预测性能并保持可解释性。

Abstract: Interpretability is one of the considerations when applying machine learning
to high-stakes fields such as healthcare that involve matters of life safety.
Generalized Additive Models (GAMs) enhance interpretability by visualizing
shape functions. Nevertheless, to preserve interpretability, GAMs omit
higher-order interaction effects (beyond pairwise interactions), which imposes
significant constraints on their predictive performance. We observe that Curve
Ergodic Set Regression (CESR), a multiplicative model, naturally enables the
visualization of its shape functions and simultaneously incorporates both
interactions among all features and individual feature effects. Nevertheless,
CESR fails to demonstrate superior performance compared to GAMs. We introduce
Multiplicative-Additive Constrained Models (MACMs), which augment CESR with an
additive part to disentangle the intertwined coefficients of its interactive
and independent terms, thus effectively broadening the hypothesis space. The
model is composed of a multiplicative part and an additive part, whose shape
functions can both be naturally visualized, thereby assisting users in
interpreting how features participate in the decision-making process.
Consequently, MACMs constitute an improvement over both CESR and GAMs. The
experimental results indicate that neural network-based MACMs significantly
outperform both CESR and the current state-of-the-art GAMs in terms of
predictive performance.

</details>


### [177] [Generation Properties of Stochastic Interpolation under Finite Training Set](https://arxiv.org/abs/2509.21925)
*Yunchen Li,Shaohui Lin,Zhou Yu*

Main category: cs.LG

TL;DR: 研究有限训练样本下生成模型理论行为，推导表达式，分析拟合情况并实验验证。


<details>
  <summary>Details</summary>
Motivation: 探究有限训练样本下生成模型的理论行为。

Method: 在随机插值生成框架下推导最优速度场和得分函数表达式，考虑模型估计误差并定义欠拟合和过拟合。

Result: 确定性生成过程可恢复训练样本，随机生成过程是加高斯噪声的训练样本；有估计误差时，随机生成是加混合噪声的训练样本凸组合。

Conclusion: 实验支持理论分析。

Abstract: This paper investigates the theoretical behavior of generative models under
finite training populations. Within the stochastic interpolation generative
framework, we derive closed-form expressions for the optimal velocity field and
score function when only a finite number of training samples are available. We
demonstrate that, under some regularity conditions, the deterministic
generative process exactly recovers the training samples, while the stochastic
generative process manifests as training samples with added Gaussian noise.
Beyond the idealized setting, we consider model estimation errors and introduce
formal definitions of underfitting and overfitting specific to generative
models. Our theoretical analysis reveals that, in the presence of estimation
errors, the stochastic generation process effectively produces convex
combinations of training samples corrupted by a mixture of uniform and Gaussian
noise. Experiments on generation tasks and downstream tasks such as
classification support our theory.

</details>


### [178] [Extracting Actionable Insights from Building Energy Data using Vision LLMs on Wavelet and 3D Recurrence Representations](https://arxiv.org/abs/2509.21934)
*Amine Bechar,Adel Oulefki,Abbes Amira,Fatih Kurogollu,Yassine Himeur*

Main category: cs.LG

TL;DR: 提出微调视觉语言大模型处理建筑能源时间序列，在真实数据集验证有效，桥接时间序列分析与可视化。


<details>
  <summary>Details</summary>
Motivation: 能源数据非线性和多尺度特征使复杂建筑时间序列分析具有挑战性，需要有效方法获取可执行见解和建议。

Method: 将1D时间序列通过连续小波变换和递归图转换为3D表示，微调视觉语言大模型处理3D图形表示。

Result: 微调后的模型在真实建筑能源数据集上成功监测建筑状态、识别异常并生成优化建议，Idefics - 7B模型在异常检测上表现优于直接微调原始时间序列数据。

Conclusion: 该工作桥接了时间序列分析和可视化，为能源分析提供可扩展和可解释的框架。

Abstract: The analysis of complex building time-series for actionable insights and
recommendations remains challenging due to the nonlinear and multi-scale
characteristics of energy data. To address this, we propose a framework that
fine-tunes visual language large models (VLLMs) on 3D graphical representations
of the data. The approach converts 1D time-series into 3D representations using
continuous wavelet transforms (CWTs) and recurrence plots (RPs), which capture
temporal dynamics and localize frequency anomalies. These 3D encodings enable
VLLMs to visually interpret energy-consumption patterns, detect anomalies, and
provide recommendations for energy efficiency. We demonstrate the framework on
real-world building-energy datasets, where fine-tuned VLLMs successfully
monitor building states, identify recurring anomalies, and generate
optimization recommendations. Quantitatively, the Idefics-7B VLLM achieves
validation losses of 0.0952 with CWTs and 0.1064 with RPs on the University of
Sharjah energy dataset, outperforming direct fine-tuning on raw time-series
data (0.1176) for anomaly detection. This work bridges time-series analysis and
visualization, providing a scalable and interpretable framework for energy
analytics.

</details>


### [179] [Statistical Advantage of Softmax Attention: Insights from Single-Location Regression](https://arxiv.org/abs/2509.21936)
*O. Duranthon,P. Marion,C. Boyer,B. Loureiro,L. Zdeborová*

Main category: cs.LG

TL;DR: 研究单位置回归任务，分析高维极限和有限样本下不同激活函数注意力机制性能，软最大激活函数表现优。


<details>
  <summary>Details</summary>
Motivation: 当前对软最大激活函数在注意力机制中占主导地位原因理解不足，很多理论研究聚焦线性化注意力。

Method: 基于统计物理思想，对单位置回归任务中基于注意力的预测器进行高维极限分析。

Result: 在总体层面，软最大激活能达到贝叶斯风险，线性注意力不足；有限样本下，软最大虽非贝叶斯最优，但仍优于线性注意力。

Conclusion: 探讨了软最大激活函数在注意力机制中的优势，以及与基于梯度算法优化的联系。

Abstract: Large language models rely on attention mechanisms with a softmax activation.
Yet the dominance of softmax over alternatives (e.g., component-wise or linear)
remains poorly understood, and many theoretical works have focused on the
easier-to-analyze linearized attention. In this work, we address this gap
through a principled study of the single-location regression task, where the
output depends on a linear transformation of a single input token at a random
location. Building on ideas from statistical physics, we develop an analysis of
attention-based predictors in the high-dimensional limit, where generalization
performance is captured by a small set of order parameters. At the population
level, we show that softmax achieves the Bayes risk, whereas linear attention
fundamentally falls short. We then examine other activation functions to
identify which properties are necessary for optimal performance. Finally, we
analyze the finite-sample regime: we provide an asymptotic characterization of
the test error and show that, while softmax is no longer Bayes-optimal, it
consistently outperforms linear attention. We discuss the connection with
optimization by gradient-based algorithms.

</details>


### [180] [Structural Information-based Hierarchical Diffusion for Offline Reinforcement Learning](https://arxiv.org/abs/2509.21942)
*Xianghua Zeng,Hao Peng,Angsheng Li,Yicheng Pan*

Main category: cs.LG

TL;DR: 提出SIHD框架用于长时稀疏奖励环境离线策略学习，自适应构建扩散层级，表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的生成方法在长时规划任务中存在固定层级和单一时间尺度问题，限制适应性和决策灵活性。

Method: 分析离线轨迹结构信息自适应构建扩散层级；量化状态社区结构信息增益作为条件信号；引入结构熵正则化器。

Result: 在具有挑战性的离线强化学习任务中，SIHD在决策性能上显著优于现有基线，且在不同场景下泛化能力更强。

Conclusion: SIHD是一种有效且稳定的长时环境离线策略学习框架。

Abstract: Diffusion-based generative methods have shown promising potential for
modeling trajectories from offline reinforcement learning (RL) datasets, and
hierarchical diffusion has been introduced to mitigate variance accumulation
and computational challenges in long-horizon planning tasks. However, existing
approaches typically assume a fixed two-layer diffusion hierarchy with a single
predefined temporal scale, which limits adaptability to diverse downstream
tasks and reduces flexibility in decision making. In this work, we propose
SIHD, a novel Structural Information-based Hierarchical Diffusion framework for
effective and stable offline policy learning in long-horizon environments with
sparse rewards. Specifically, we analyze structural information embedded in
offline trajectories to construct the diffusion hierarchy adaptively, enabling
flexible trajectory modeling across multiple temporal scales. Rather than
relying on reward predictions from localized sub-trajectories, we quantify the
structural information gain of each state community and use it as a
conditioning signal within the corresponding diffusion layer. To reduce
overreliance on offline datasets, we introduce a structural entropy regularizer
that encourages exploration of underrepresented states while avoiding
extrapolation errors from distributional shifts. Extensive evaluations on
challenging offline RL tasks show that SIHD significantly outperforms
state-of-the-art baselines in decision-making performance and demonstrates
superior generalization across diverse scenarios.

</details>


### [181] [Active Attacks: Red-teaming LLMs via Adaptive Environments](https://arxiv.org/abs/2509.21947)
*Taeyoung Yun,Pierre-Luc St-Charles,Jinkyoo Park,Yoshua Bengio,Minsu Kim*

Main category: cs.LG

TL;DR: 提出Active Attacks算法，自动生成攻击提示用于大语言模型安全微调，能逐步覆盖多种攻击模式，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决为大语言模型生成多样攻击提示以进行安全微调的挑战，现有多样性强化学习方法易陷入有限模式。

Method: 引入基于强化学习的Active Attacks算法，通过定期用收集的攻击提示对受害模型进行安全微调，迫使攻击者探索未发现的漏洞。

Result: Active Attacks逐步发现多种局部攻击模式，组合后能广泛覆盖多模式分布，在跨攻击成功率上相比之前的先进方法GFlowNets从0.07%提升到31.28%，计算量仅增加6%。

Conclusion: Active Attacks作为简单的即插即用模块，能无缝集成到现有强化学习目标中，有效提升大语言模型安全微调的攻击提示生成效果。

Abstract: We address the challenge of generating diverse attack prompts for large
language models (LLMs) that elicit harmful behaviors (e.g., insults, sexual
content) and are used for safety fine-tuning. Rather than relying on manual
prompt engineering, attacker LLMs can be trained with reinforcement learning
(RL) to automatically generate such prompts using only a toxicity classifier as
a reward. However, capturing a wide range of harmful behaviors is a significant
challenge that requires explicit diversity objectives. Existing
diversity-seeking RL methods often collapse to limited modes: once high-reward
prompts are found, exploration of new regions is discouraged. Inspired by the
active learning paradigm that encourages adaptive exploration, we introduce
\textit{Active Attacks}, a novel RL-based red-teaming algorithm that adapts its
attacks as the victim evolves. By periodically safety fine-tuning the victim
LLM with collected attack prompts, rewards in exploited regions diminish, which
forces the attacker to seek unexplored vulnerabilities. This process naturally
induces an easy-to-hard exploration curriculum, where the attacker progresses
beyond easy modes toward increasingly difficult ones. As a result, Active
Attacks uncovers a wide range of local attack modes step by step, and their
combination achieves wide coverage of the multi-mode distribution. Active
Attacks, a simple plug-and-play module that seamlessly integrates into existing
RL objectives, unexpectedly outperformed prior RL-based methods -- including
GFlowNets, PPO, and REINFORCE -- by improving cross-attack success rates
against GFlowNets, the previous state-of-the-art, from 0.07% to 31.28% (a
relative gain greater than $400\ \times$) with only a 6% increase in
computation. Our code is publicly available
\href{https://github.com/dbsxodud-11/active_attacks}{here}.

</details>


### [182] [Think Smart, Not Hard: Difficulty Adaptive Reasoning for Large Audio Language Models](https://arxiv.org/abs/2509.21960)
*Zhichao Sheng,Shilin Zhou,Chen Gong,Zhenghua Li*

Main category: cs.LG

TL;DR: 本文分析大音频语言模型推理深度问题，提出难度自适应推理方法，实验证明有效且高效。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏细粒度机制调节推理深度，存在‘一刀切’问题。

Method: 提出一种奖励函数，将推理长度与感知到的问题难度动态关联。

Result: 方法有效且高效，提高任务性能，显著减少平均推理长度。

Conclusion: 方法能让模型根据问题复杂度调整推理深度，对推理结构范式的分析为未来工作提供见解。

Abstract: Large Audio Language Models (LALMs), powered by the chain-of-thought (CoT)
paradigm, have shown remarkable reasoning capabilities. Intuitively, different
problems often require varying depths of reasoning. While some methods can
determine whether to reason for a given problem, they typically lack a
fine-grained mechanism to modulate how much to reason. This often results in a
``one-size-fits-all'' reasoning depth, which generates redundant overthinking
for simple questions while failing to allocate sufficient thought to complex
ones. In this paper, we conduct an in-depth analysis of LALMs and find that an
effective and efficient LALM should reason smartly by adapting its reasoning
depth to the problem's complexity. To achieve this, we propose a
difficulty-adaptive reasoning method for LALMs. Specifically, we propose a
reward function that dynamically links reasoning length to the model's
perceived problem difficulty. This reward encourages shorter, concise reasoning
for easy tasks and more elaborate, in-depth reasoning for complex ones.
Extensive experiments demonstrate that our method is both effective and
efficient, simultaneously improving task performance and significantly reducing
the average reasoning length. Further analysis on reasoning structure paradigm
offers valuable insights for future work.

</details>


### [183] [GRAM-TDI: adaptive multimodal representation learning for drug target interaction prediction](https://arxiv.org/abs/2509.21971)
*Feng Jiang,Amina Mollaysa,Hehuan Ma,Tommaso Mansi,Junzhou Huang,Mangal Prakash,Rui Liao*

Main category: cs.LG

TL;DR: 提出GRAMDTI预训练框架用于药物靶点相互作用（DTI）预测，在四个公开数据集上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习DTI建模方法主要依赖SMILES蛋白质对，未充分利用小分子和蛋白质的多模态信息。

Method: 引入GRAMDTI框架，将多模态分子和蛋白质输入整合为统一表示；扩展基于体积的对比学习到四种模态；提出自适应模态丢弃方法；引入IC50活性测量作为弱监督。

Result: 在四个公开数据集上，GRAMDTI始终优于现有基线。

Conclusion: 高阶多模态对齐、自适应模态利用和辅助监督对稳健且可泛化的DTI预测有益。

Abstract: Drug target interaction (DTI) prediction is a cornerstone of computational
drug discovery, enabling rational design, repurposing, and mechanistic
insights. While deep learning has advanced DTI modeling, existing approaches
primarily rely on SMILES protein pairs and fail to exploit the rich multimodal
information available for small molecules and proteins. We introduce GRAMDTI, a
pretraining framework that integrates multimodal molecular and protein inputs
into unified representations. GRAMDTI extends volume based contrastive learning
to four modalities, capturing higher-order semantic alignment beyond
conventional pairwise approaches. To handle modality informativeness, we
propose adaptive modality dropout, dynamically regulating each modality's
contribution during pre-training. Additionally, IC50 activity measurements,
when available, are incorporated as weak supervision to ground representations
in biologically meaningful interaction strengths. Experiments on four publicly
available datasets demonstrate that GRAMDTI consistently outperforms state of
the art baselines. Our results highlight the benefits of higher order
multimodal alignment, adaptive modality utilization, and auxiliary supervision
for robust and generalizable DTI prediction.

</details>


### [184] [Stage-wise Dynamics of Classifier-Free Guidance in Diffusion Models](https://arxiv.org/abs/2509.22007)
*Cheng Jin,Qitan Shi,Yuantao Gu*

Main category: cs.LG

TL;DR: 本文分析了多模态条件下分类器自由引导（CFG）在扩散模型采样中的影响，提出三阶段理论，解释了引导强度与多样性的关系，并提出时变引导策略提升质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有研究对CFG在扩散模型采样动态的影响理解不足，且多局限于单峰条件分布或简化情况。

Method: 分析多模态条件下CFG的采样过程，分为方向转移、模式分离和集中三个阶段，并通过实验验证理论。

Result: 实验支持了理论预测，早期强引导会降低全局多样性，晚期强引导会抑制细粒度变化；时变引导策略能同时提升质量和多样性。

Conclusion: 提出的三阶段理论解释了CFG现象，时变引导策略有效。

Abstract: Classifier-Free Guidance (CFG) is widely used to improve conditional fidelity
in diffusion models, but its impact on sampling dynamics remains poorly
understood. Prior studies, often restricted to unimodal conditional
distributions or simplified cases, provide only a partial picture. We analyze
CFG under multimodal conditionals and show that the sampling process unfolds in
three successive stages. In the Direction Shift stage, guidance accelerates
movement toward the weighted mean, introducing initialization bias and norm
growth. In the Mode Separation stage, local dynamics remain largely neutral,
but the inherited bias suppresses weaker modes, reducing global diversity. In
the Concentration stage, guidance amplifies within-mode contraction,
diminishing fine-grained variability. This unified view explains a widely
observed phenomenon: stronger guidance improves semantic alignment but
inevitably reduces diversity. Experiments support these predictions, showing
that early strong guidance erodes global diversity, while late strong guidance
suppresses fine-grained variation. Moreover, our theory naturally suggests a
time-varying guidance schedule, and empirical results confirm that it
consistently improves both quality and diversity.

</details>


### [185] [Goal-Guided Efficient Exploration via Large Language Model in Reinforcement Learning](https://arxiv.org/abs/2509.22008)
*Yajie Qi,Wei Wei,Lin Li,Lijun Zhang,Zhidong Gao,Da Wang,Huizhong Song*

Main category: cs.LG

TL;DR: 提出结构化目标引导强化学习（SGRL）方法，结合结构化目标规划器和目标条件动作修剪器，在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实决策任务对强化学习代理的探索效率和长期规划能力提出挑战，现有大语言模型增强的强化学习方法存在频繁调用大语言模型成本高、语义不匹配导致性能受限的问题。

Method: 引入SGRL方法，包括结构化目标规划器利用大语言模型生成目标并确定优先级、动态生成前瞻性目标，以及目标条件动作修剪器使用动作掩码机制过滤不符合当前目标的动作。

Result: 在Crafter和Craftax - Classic上评估，SGRL比现有最先进方法性能更优。

Conclusion: SGRL方法有效提升了强化学习代理的探索效率和决策能力，相比现有方法有更好表现。

Abstract: Real-world decision-making tasks typically occur in complex and open
environments, posing significant challenges to reinforcement learning (RL)
agents' exploration efficiency and long-horizon planning capabilities. A
promising approach is LLM-enhanced RL, which leverages the rich prior knowledge
and strong planning capabilities of LLMs to guide RL agents in efficient
exploration. However, existing methods mostly rely on frequent and costly LLM
invocations and suffer from limited performance due to the semantic mismatch.
In this paper, we introduce a Structured Goal-guided Reinforcement Learning
(SGRL) method that integrates a structured goal planner and a goal-conditioned
action pruner to guide RL agents toward efficient exploration. Specifically,
the structured goal planner utilizes LLMs to generate a reusable, structured
function for goal generation, in which goals are prioritized. Furthermore, by
utilizing LLMs to determine goals' priority weights, it dynamically generates
forward-looking goals to guide the agent's policy toward more promising
decision-making trajectories. The goal-conditioned action pruner employs an
action masking mechanism that filters out actions misaligned with the current
goal, thereby constraining the RL agent to select goal-consistent policies. We
evaluate the proposed method on Crafter and Craftax-Classic, and experimental
results demonstrate that SGRL achieves superior performance compared to
existing state-of-the-art methods.

</details>


### [186] [Concept-SAE: Active Causal Probing of Visual Model Behavior](https://arxiv.org/abs/2509.22015)
*Jianrong Ding,Muxi Chen,Chenchen Zhao,Qiang Xu*

Main category: cs.LG

TL;DR: 提出Concept - SAE框架解决标准SAEs特征模糊无根据问题，能进行因果探测和定位模型失败模式。


<details>
  <summary>Details</summary>
Motivation: 标准稀疏自动编码器（SAEs）学习的特征模糊无根据，无法用于主动、因果地探测模型行为。

Method: 引入Concept - SAE框架，采用新颖的混合解纠缠策略生成语义有根据的概念令牌，使用双监督方法。

Result: 双监督方法产生的令牌忠实且空间局部化，在解纠缠方面优于其他方法，可用于因果探测和定位模型失败模式。

Conclusion: Concept - SAE为从相关性解释转向对模型行为的机制性、因果性探测提供了有效方案。

Abstract: Standard Sparse Autoencoders (SAEs) excel at discovering a dictionary of a
model's learned features, offering a powerful observational lens. However, the
ambiguous and ungrounded nature of these features makes them unreliable
instruments for the active, causal probing of model behavior. To solve this, we
introduce Concept-SAE, a framework that forges semantically grounded concept
tokens through a novel hybrid disentanglement strategy. We first quantitatively
demonstrate that our dual-supervision approach produces tokens that are
remarkably faithful and spatially localized, outperforming alternative methods
in disentanglement. This validated fidelity enables two critical applications:
(1) we probe the causal link between internal concepts and predictions via
direct intervention, and (2) we probe the model's failure modes by
systematically localizing adversarial vulnerabilities to specific layers.
Concept-SAE provides a validated blueprint for moving beyond correlational
interpretation to the mechanistic, causal probing of model behavior.

</details>


### [187] [AEGIS: Authentic Edge Growth In Sparsity for Link Prediction in Edge-Sparse Bipartite Knowledge Graphs](https://arxiv.org/abs/2509.22017)
*Hugh Xuechen Liu,Kıvanç Tatar*

Main category: cs.LG

TL;DR: 提出AEGIS框架用于二部知识图谱稀疏链接预测，在不同数据集上评估，发现真实性约束重采样是有效策略，有信息节点描述时语义增强更优。


<details>
  <summary>Details</summary>
Motivation: 二部知识图谱数据少、边稀疏阻碍链接预测。

Method: 引入AEGIS框架，对现有训练边重采样，在自然稀疏图和诱导稀疏图上实验，用AUC - ROC和Brier分数评估，与稀疏基线进行双尾配对t检验。

Result: 在Amazon和MovieLens上，基于复制的AEGIS变体与基线相当，语义KNN增强能恢复AUC和校准；在GDP图上，语义KNN改善最大，简单重采样也降低Brier分数。

Conclusion: 真实性约束重采样是稀疏二部链接预测的数据高效策略，有信息节点描述时语义增强有额外提升。

Abstract: Bipartite knowledge graphs in niche domains are typically data-poor and
edge-sparse, which hinders link prediction. We introduce AEGIS (Authentic Edge
Growth In Sparsity), an edge-only augmentation framework that resamples
existing training edges -either uniformly simple or with inverse-degree bias
degree-aware -thereby preserving the original node set and sidestepping
fabricated endpoints. To probe authenticity across regimes, we consider
naturally sparse graphs (game design pattern's game-pattern network) and induce
sparsity in denser benchmarks (Amazon, MovieLens) via high-rate bond
percolation. We evaluate augmentations on two complementary metrics: AUC-ROC
(higher is better) and the Brier score (lower is better), using two-tailed
paired t-tests against sparse baselines. On Amazon and MovieLens, copy-based
AEGIS variants match the baseline while the semantic KNN augmentation is the
only method that restores AUC and calibration; random and synthetic edges
remain detrimental. On the text-rich GDP graph, semantic KNN achieves the
largest AUC improvement and Brier score reduction, and simple also lowers the
Brier score relative to the sparse control. These findings position
authenticity-constrained resampling as a data-efficient strategy for sparse
bipartite link prediction, with semantic augmentation providing an additional
boost when informative node descriptions are available.

</details>


### [188] [Task-Adaptive Parameter-Efficient Fine-Tuning for Weather Foundation Models](https://arxiv.org/abs/2509.22020)
*Shilei Cao,Hehai Lin,Jiashun Cheng,Yang Liu,Guowen Li,Xuehe Wang,Juepeng Zheng,Haoyuan Liang,Meng Jin,Chengwei Qin,Hong Cheng,Haohuan Fu*

Main category: cs.LG

TL;DR: 现有机器学习方法使天气基础模型有泛化能力，但计算需求大，现有参数高效微调方法用于天气任务效果不佳，本文提出WeatherPEFT框架，在三个下游任务中验证其有效性和高效性。


<details>
  <summary>Details</summary>
Motivation: 当前参数高效微调方法无法解决天气下游任务独特挑战，应用于天气基础模型时表现不佳，需要新方法。

Method: 提出WeatherPEFT框架，包含前向传播的任务自适应动态提示（TADP）和反向传播的随机Fisher引导自适应选择（SFAS）。

Result: 在三个下游任务中，现有参数高效微调方法与全量微调有显著差距，WeatherPEFT用更少可训练参数达到与全量微调相当的性能。

Conclusion: WeatherPEFT框架有效且高效，代码将发布。

Abstract: While recent advances in machine learning have equipped Weather Foundation
Models (WFMs) with substantial generalization capabilities across diverse
downstream tasks, the escalating computational requirements associated with
their expanding scale increasingly hinder practical deployment. Current
Parameter-Efficient Fine-Tuning (PEFT) methods, designed for vision or language
tasks, fail to address the unique challenges of weather downstream tasks, such
as variable heterogeneity, resolution diversity, and spatiotemporal coverage
variations, leading to suboptimal performance when applied to WFMs. To bridge
this gap, we introduce WeatherPEFT, a novel PEFT framework for WFMs
incorporating two synergistic innovations. First, during the forward pass,
Task-Adaptive Dynamic Prompting (TADP) dynamically injects the embedding
weights within the encoder to the input tokens of the pre-trained backbone via
internal and external pattern extraction, enabling context-aware feature
recalibration for specific downstream tasks. Furthermore, during
backpropagation, Stochastic Fisher-Guided Adaptive Selection (SFAS) not only
leverages Fisher information to identify and update the most task-critical
parameters, thereby preserving invariant pre-trained knowledge, but also
introduces randomness to stabilize the selection. We demonstrate the
effectiveness and efficiency of WeatherPEFT on three downstream tasks, where
existing PEFT methods show significant gaps versus Full-Tuning, and WeatherPEFT
achieves performance parity with Full-Tuning using fewer trainable parameters.
The code of this work will be released.

</details>


### [189] [Teaching Transformers to Solve Combinatorial Problems through Efficient Trial & Error](https://arxiv.org/abs/2509.22023)
*Panagiotis Giannoulis,Yorgos Pantis,Christos Tzamos*

Main category: cs.LG

TL;DR: 提出用无外部工具的GPT - 2解决NP类问题，以数独为例达99%准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在组合问题上表现不佳，需新方法解决NP类问题。

Method: 采用无外部工具的GPT - 2，结合简单数独规则的模仿学习与深度优先搜索策略，减少猜测次数。

Result: 在数独任务上达到99%准确率，优于先前神经符号方法。

Conclusion: 该方法有效解决NP类问题，还给出与Min - Sum Set Cover问题的形式化联系。

Abstract: Despite their proficiency in various language tasks, Large Language Models
(LLMs) struggle with combinatorial problems like Satisfiability, Traveling
Salesman Problem, or even basic arithmetic. We address this gap through a novel
approach for solving problems in the class NP. We focus on the paradigmatic
task of Sudoku and achieve state-of-the-art accuracy (99\%) compared to prior
neuro-symbolic approaches. Unlike prior work that used custom architectures,
our method employs a vanilla decoder-only Transformer (GPT-2) without external
tools or function calling. Our method integrates imitation learning of simple
Sudoku rules with an explicit Depth-First Search (DFS) exploration strategy
involving informed guessing and backtracking. Moving beyond imitation learning,
we seek to minimize the number of guesses until reaching a solution. We provide
a rigorous analysis of this setup formalizing its connection to a contextual
variant of Min-Sum Set Cover, a well-studied problem in algorithms and
stochastic optimization.

</details>


### [190] [MCGM: Multi-stage Clustered Global Modeling for Long-range Interactions in Molecules](https://arxiv.org/abs/2509.22028)
*Haodong Pan,Yusong Wang,Nanning Zheng,Caijui Jiang*

Main category: cs.LG

TL;DR: 提出轻量级即插即用模块MCGM，增强几何GNN处理长程相互作用能力，降低能量预测误差。


<details>
  <summary>Details</summary>
Motivation: 现有几何GNN处理长程相互作用有局限性，如扩展截断半径计算成本高、物理启发内核缺乏通用性、傅里叶空间方法需调参且有额外计算开销。

Method: 引入MCGM模块，通过高效聚类操作赋予几何GNN分层全局上下文，构建原子簇多分辨率层次结构，通过动态分层聚类提取全局信息并传播。

Result: 将MCGM集成到四种骨干架构中，平均降低OE62能量预测误差26.2%；在AQM上达到最先进精度，且比Neural P3M少用20%参数。

Conclusion: MCGM能有效增强几何GNN处理长程相互作用的能力，提升分子能量和力的预测精度。

Abstract: Geometric graph neural networks (GNNs) excel at capturing molecular geometry,
yet their locality-biased message passing hampers the modeling of long-range
interactions. Current solutions have fundamental limitations: extending cutoff
radii causes computational costs to scale cubically with distance;
physics-inspired kernels (e.g., Coulomb, dispersion) are often system-specific
and lack generality; Fourier-space methods require careful tuning of multiple
parameters (e.g., mesh size, k-space cutoff) with added computational overhead.
We introduce Multi-stage Clustered Global Modeling (MCGM), a lightweight,
plug-and-play module that endows geometric GNNs with hierarchical global
context through efficient clustering operations. MCGM builds a multi-resolution
hierarchy of atomic clusters, distills global information via dynamic
hierarchical clustering, and propagates this context back through learned
transformations, ultimately reinforcing atomic features via residual
connections. Seamlessly integrated into four diverse backbone architectures,
MCGM reduces OE62 energy prediction error by an average of 26.2%. On AQM, MCGM
achieves state-of-the-art accuracy (17.0 meV for energy, 4.9 meV/{\AA} for
forces) while using 20% fewer parameters than Neural P3M. Code will be made
available upon acceptance.

</details>


### [191] [OrtSAE: Orthogonal Sparse Autoencoders Uncover Atomic Features](https://arxiv.org/abs/2509.22033)
*Anton Korznikov,Andrey Galichin,Alexey Dontsov,Oleg Rogov,Elena Tutubalina,Ivan Oseledets*

Main category: cs.LG

TL;DR: 提出正交稀疏自动编码器（OrtSAE）解决现有SAEs的问题，实验显示其有更好表现。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自动编码器（SAEs）存在特征吸收和特征组合问题，需要改进。

Method: 引入OrtSAE，通过新训练程序惩罚SAE特征间高成对余弦相似度，强制学习的特征正交。

Result: OrtSAE发现的独特特征多9%，减少特征吸收65%、特征组合15%，去除虚假相关性性能提升6%，下游任务表现与传统SAEs相当。

Conclusion: OrtSAE能有效缓解现有SAEs问题，且计算开销无显著增加。

Abstract: Sparse autoencoders (SAEs) are a technique for sparse decomposition of neural
network activations into human-interpretable features. However, current SAEs
suffer from feature absorption, where specialized features capture instances of
general features creating representation holes, and feature composition, where
independent features merge into composite representations. In this work, we
introduce Orthogonal SAE (OrtSAE), a novel approach aimed to mitigate these
issues by enforcing orthogonality between the learned features. By implementing
a new training procedure that penalizes high pairwise cosine similarity between
SAE features, OrtSAE promotes the development of disentangled features while
scaling linearly with the SAE size, avoiding significant computational
overhead. We train OrtSAE across different models and layers and compare it
with other methods. We find that OrtSAE discovers 9% more distinct features,
reduces feature absorption (by 65%) and composition (by 15%), improves
performance on spurious correlation removal (+6%), and achieves on-par
performance for other downstream tasks compared to traditional SAEs.

</details>


### [192] [Latent Diffusion : Multi-Dimension Stable Diffusion Latent Space Explorer](https://arxiv.org/abs/2509.22038)
*Zhihua Zhong,Xuanyang Huang*

Main category: cs.LG

TL;DR: 本文提出一个将可定制潜在空间操作集成到扩散过程的框架，通过两个艺术作品展示其潜力，揭示潜在空间结构。


<details>
  <summary>Details</summary>
Motivation: 扩散模型如Stable Diffusion缺乏GAN中直观的潜在向量控制，限制了艺术表达的灵活性。

Method: 引入一个将可定制潜在空间操作集成到扩散过程的框架，实现对概念和空间表示的直接操作。

Result: 通过两个艺术作品展示框架在概念融合和动态运动生成方面的潜力，发现潜在空间存在语义和无意义区域。

Conclusion: 该框架扩展了生成艺术的创作可能性，为进一步探索潜在空间奠定基础。

Abstract: Latent space is one of the key concepts in generative AI, offering powerful
means for creative exploration through vector manipulation. However, diffusion
models like Stable Diffusion lack the intuitive latent vector control found in
GANs, limiting their flexibility for artistic expression. This paper introduces
\workname, a framework for integrating customizable latent space operations
into the diffusion process. By enabling direct manipulation of conceptual and
spatial representations, this approach expands creative possibilities in
generative art. We demonstrate the potential of this framework through two
artworks, \textit{Infinitepedia} and \textit{Latent Motion}, highlighting its
use in conceptual blending and dynamic motion generation. Our findings reveal
latent space structures with semantic and meaningless regions, offering
insights into the geometry of diffusion models and paving the way for further
explorations of latent space.

</details>


### [193] [MO-GRPO: Mitigating Reward Hacking of Group Relative Policy Optimization on Multi-Objective Problems](https://arxiv.org/abs/2509.22047)
*Yuki Ichihara,Yuu Jinnai,Tetsuro Morimura,Mitsuki Sakamoto,Ryota Mitsuhashi,Eiji Uchibe*

Main category: cs.LG

TL;DR: 本文指出GRPO在多目标场景易受奖励破解影响，提出MO - GRPO算法，通过归一化方法自动重新加权奖励函数，经理论分析和多领域实验验证其优于GRPO。


<details>
  <summary>Details</summary>
Motivation: GRPO在很多现实任务中缺乏可靠奖励模型，在多目标设置中易受奖励破解影响，只优化一个目标而牺牲其他目标。

Method: 提出MO - GRPO，是GRPO的扩展，采用简单归一化方法根据奖励函数值的方差自动重新加权奖励函数。

Result: 理论分析表明MO - GRPO能让所有奖励函数对损失函数均匀贡献并保留偏好顺序；实验显示在四个领域中，MO - GRPO通过均匀分配奖励各部分的相关性实现稳定学习，优于GRPO。

Conclusion: MO - GRPO是解决多目标强化学习问题的有前景的算法。

Abstract: Group Relative Policy Optimization (GRPO) has been shown to be an effective
algorithm when an accurate reward model is available. However, such a highly
reliable reward model is not available in many real-world tasks. In this paper,
we particularly focus on multi-objective settings, in which we identify that
GRPO is vulnerable to reward hacking, optimizing only one of the objectives at
the cost of the others. To address this issue, we propose MO-GRPO, an extension
of GRPO with a simple normalization method to reweight the reward functions
automatically according to the variances of their values. We first show
analytically that MO-GRPO ensures that all reward functions contribute evenly
to the loss function while preserving the order of preferences, eliminating the
need for manual tuning of the reward functions' scales. Then, we evaluate
MO-GRPO experimentally in four domains: (i) the multi-armed bandits problem,
(ii) simulated control task (Mo-Gymnasium), (iii) machine translation tasks on
the WMT benchmark (En-Ja, En-Zh), and (iv) instruction following task. MO-GRPO
achieves stable learning by evenly distributing correlations among the
components of rewards, outperforming GRPO, showing MO-GRPO to be a promising
algorithm for multi-objective reinforcement learning problems.

</details>


### [194] [BrainPro: Towards Large-scale Brain State-aware EEG Representation Learning](https://arxiv.org/abs/2509.22050)
*Yi Ding,Muyun Jiang,Weibang Jiang,Shuailei Zhang,Xinliang Zhou,Chenyu Liu,Shanglin Li,Yong Li,Cuntai Guan*

Main category: cs.LG

TL;DR: 提出大EEG模型BrainPro，解决现有模型不足，在多数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有EEG基础模型未能显式捕捉通道和区域交互，难以适应不同通道配置，且很少学习状态感知表示。

Method: 提出BrainPro，含基于检索的空间学习块和脑状态解耦块，在大规模EEG语料库预训练。

Result: BrainPro在九个公共BCI数据集上实现了最先进的性能和强大的泛化能力。

Conclusion: BrainPro能无缝适应不同任务和硬件设置，代码和预训练权重将发布。

Abstract: Electroencephalography (EEG) is a non-invasive technique for recording brain
electrical activity, widely used in brain-computer interface (BCI) and
healthcare. Recent EEG foundation models trained on large-scale datasets have
shown improved performance and generalizability over traditional decoding
methods, yet significant challenges remain. Existing models often fail to
explicitly capture channel-to-channel and region-to-region interactions, which
are critical sources of information inherently encoded in EEG signals. Due to
varying channel configurations across datasets, they either approximate spatial
structure with self-attention or restrict training to a limited set of common
channels, sacrificing flexibility and effectiveness. Moreover, although EEG
datasets reflect diverse brain states such as emotion, motor, and others,
current models rarely learn state-aware representations during self-supervised
pre-training. To address these gaps, we propose BrainPro, a large EEG model
that introduces a retrieval-based spatial learning block to flexibly capture
channel- and region-level interactions across varying electrode layouts, and a
brain state-decoupling block that enables state-aware representation learning
through parallel encoders with decoupling and region-aware reconstruction
losses. This design allows BrainPro to adapt seamlessly to diverse tasks and
hardware settings. Pre-trained on an extensive EEG corpus, BrainPro achieves
state-of-the-art performance and robust generalization across nine public BCI
datasets. Our codes and the pre-trained weights will be released.

</details>


### [195] [Enriching Knowledge Distillation with Intra-Class Contrastive Learning](https://arxiv.org/abs/2509.22053)
*Hua Yuan,Ning Xu,Xin Geng,Yong Rui*

Main category: cs.LG

TL;DR: 本文提出在教师模型训练中引入类内对比损失和边界损失，以丰富软标签的类内信息并提升训练稳定性，实验证明了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法中教师模型多以真实标签为目标，未考虑同一类内的多样表示，因此需改进以丰富软标签的类内信息。

Method: 在教师训练时引入类内对比损失，为解决该损失导致的训练不稳定和收敛慢问题，将边界损失融入类内对比学习，并理论分析该损失对类内和类间距离的影响。

Result: 实验结果证明了所提方法的有效性。

Conclusion: 类内对比损失能丰富类内多样性，所提方法有效可行。

Abstract: Since the advent of knowledge distillation, much research has focused on how
the soft labels generated by the teacher model can be utilized effectively.
Existing studies points out that the implicit knowledge within soft labels
originates from the multi-view structure present in the data. Feature
variations within samples of the same class allow the student model to
generalize better by learning diverse representations. However, in existing
distillation methods, teacher models predominantly adhere to ground-truth
labels as targets, without considering the diverse representations within the
same class. Therefore, we propose incorporating an intra-class contrastive loss
during teacher training to enrich the intra-class information contained in soft
labels. In practice, we find that intra-class loss causes instability in
training and slows convergence. To mitigate these issues, margin loss is
integrated into intra-class contrastive learning to improve the training
stability and convergence speed. Simultaneously, we theoretically analyze the
impact of this loss on the intra-class distances and inter-class distances. It
has been proved that the intra-class contrastive loss can enrich the
intra-class diversity. Experimental results demonstrate the effectiveness of
the proposed method.

</details>


### [196] [Towards Understanding Feature Learning in Parameter Transfer](https://arxiv.org/abs/2509.22056)
*Hua Yuan,Xuran Meng,Qiufeng Wang,Shiyu Xia,Ning Xu,Xu Yang,Jing Wang,Xin Geng,Yong Rui*

Main category: cs.LG

TL;DR: 文章研究ReLU卷积神经网络中部分参数迁移，分析参数作用、关键因素，解释迁移效果不佳原因并实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决部分参数迁移缺乏理论理解的问题，即不清楚部分参数复用何时有益及影响其有效性的因素。

Method: 分析上游和下游模型均为ReLU卷积神经网络的情况，进行数值实验和真实数据实验。

Result: 明确继承参数作为通用知识载体的作用，找出放大其对目标任务有益影响的关键因素，解释部分情况下参数迁移不如从头训练模型的原因。

Conclusion: 完成了对部分参数迁移的理论分析，并通过实验验证了理论发现。

Abstract: Parameter transfer is a central paradigm in transfer learning, enabling
knowledge reuse across tasks and domains by sharing model parameters between
upstream and downstream models. However, when only a subset of parameters from
the upstream model is transferred to the downstream model, there remains a lack
of theoretical understanding of the conditions under which such partial
parameter reuse is beneficial and of the factors that govern its effectiveness.
To address this gap, we analyze a setting in which both the upstream and
downstream models are ReLU convolutional neural networks (CNNs). Within this
theoretical framework, we characterize how the inherited parameters act as
carriers of universal knowledge and identify key factors that amplify their
beneficial impact on the target task. Furthermore, our analysis provides
insight into why, in certain cases, transferring parameters can lead to lower
test accuracy on the target task than training a new model from scratch.
Numerical experiments and real-world data experiments are conducted to
empirically validate our theoretical findings.

</details>


### [197] [The Rogue Scalpel: Activation Steering Compromises LLM Safety](https://arxiv.org/abs/2509.22067)
*Anton Korznikov,Andrey Galichin,Alexey Dontsov,Oleg Y. Rogov,Ivan Oseledets,Elena Tutubalina*

Main category: cs.LG

TL;DR: 研究表明激活引导会破坏大语言模型对齐保护机制，增加有害输出概率，挑战了通过可解释性实现安全的范式。


<details>
  <summary>Details</summary>
Motivation: 探讨激活引导作为微调的替代方法是否真的精确、可解释且安全。

Method: 在不同模型家族上进行大量实验，包括随机方向引导、使用稀疏自动编码器良性特征引导、组合随机向量进行攻击。

Result: 随机方向引导使有害输出概率从0%增至2 - 27%，使用SAE良性特征引导再增加2 - 4%，组合向量可创建通用攻击，增加未知请求的有害输出概率。

Conclusion: 对模型内部的精确控制不能保证对模型行为的精确控制，挑战了通过可解释性实现安全的范式。

Abstract: Activation steering is a promising technique for controlling LLM behavior by
adding semantically meaningful vectors directly into a model's hidden states
during inference. It is often framed as a precise, interpretable, and
potentially safer alternative to fine-tuning. We demonstrate the opposite:
steering systematically breaks model alignment safeguards, making it comply
with harmful requests. Through extensive experiments on different model
families, we show that even steering in a random direction can increase the
probability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign
features from a sparse autoencoder (SAE), a common source of interpretable
directions, increases these rates by a further 2-4%. Finally, we show that
combining 20 randomly sampled vectors that jailbreak a single prompt creates a
universal attack, significantly increasing harmful compliance on unseen
requests. These results challenge the paradigm of safety through
interpretability, showing that precise control over model internals does not
guarantee precise control over model behavior.

</details>


### [198] [Non-Linear Trajectory Modeling for Multi-Step Gradient Inversion Attacks in Federated Learning](https://arxiv.org/abs/2509.22082)
*Li Xia,Zheng Liu,Sili Huang,Wei Tang,Xuan Liu*

Main category: cs.LG

TL;DR: 本文指出现有联邦学习梯度反演攻击的替代模型方法存在局限，提出NL - SME方法，实验表明其性能远超基线，揭示了多步更新范式隐私漏洞并为防御策略提供新思路。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习的梯度反演攻击中，替代模型方法假设线性参数轨迹，严重低估SGD的非线性复杂度，限制了攻击效果，需改进。

Method: 提出非线性替代模型扩展（NL - SME）方法，用可学习的二次贝塞尔曲线代替线性插值，结合正则化和dvec缩放机制。

Result: 在CIFAR - 100和FEMNIST数据集上，NL - SME在所有指标上显著优于基线，余弦相似度损失有数量级的提升，且保持计算效率。

Conclusion: 工作揭示了联邦学习多步更新范式中更高的隐私漏洞，为开发强大防御策略提供了新视角。

Abstract: Federated Learning (FL) preserves privacy by keeping raw data local, yet
Gradient Inversion Attacks (GIAs) pose significant threats. In FedAVG
multi-step scenarios, attackers observe only aggregated gradients, making data
reconstruction challenging. Existing surrogate model methods like SME assume
linear parameter trajectories, but we demonstrate this severely underestimates
SGD's nonlinear complexity, fundamentally limiting attack effectiveness. We
propose Non-Linear Surrogate Model Extension (NL-SME), the first method to
introduce nonlinear parametric trajectory modeling for GIAs. Our approach
replaces linear interpolation with learnable quadratic B\'ezier curves that
capture SGD's curved characteristics through control points, combined with
regularization and dvec scaling mechanisms for enhanced expressiveness.
Extensive experiments on CIFAR-100 and FEMNIST datasets show NL-SME
significantly outperforms baselines across all metrics, achieving
order-of-magnitude improvements in cosine similarity loss while maintaining
computational efficiency.This work exposes heightened privacy vulnerabilities
in FL's multi-step update paradigm and offers novel perspectives for developing
robust defense strategies.

</details>


### [199] [Reinforcement Learning for Durable Algorithmic Recourse](https://arxiv.org/abs/2509.22102)
*Marina Ceccon,Alessandro Fabris,Goran Radanović,Asia J. Biega,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 提出时间感知的算法追索框架和基于强化学习的追索算法，在复杂模拟环境实验中表现优于现有基线，强调结合时间和行为动态的重要性。


<details>
  <summary>Details</summary>
Motivation: 以往研究较少关注追索的时间动态，尤其是在竞争、资源受限环境中，本文旨在解决该问题。

Method: 提出时间感知框架，明确建模候选人群对推荐的适应情况；引入基于强化学习的追索算法，捕捉环境动态。

Result: 在复杂模拟环境实验中，该方法显著优于现有基线，在可行性和长期有效性间实现更好平衡。

Conclusion: 在实际追索系统设计中纳入时间和行为动态十分重要。

Abstract: Algorithmic recourse seeks to provide individuals with actionable
recommendations that increase their chances of receiving favorable outcomes
from automated decision systems (e.g., loan approvals). While prior research
has emphasized robustness to model updates, considerably less attention has
been given to the temporal dynamics of recourse--particularly in competitive,
resource-constrained settings where recommendations shape future applicant
pools. In this work, we present a novel time-aware framework for algorithmic
recourse, explicitly modeling how candidate populations adapt in response to
recommendations. Additionally, we introduce a novel reinforcement learning
(RL)-based recourse algorithm that captures the evolving dynamics of the
environment to generate recommendations that are both feasible and valid. We
design our recommendations to be durable, supporting validity over a predefined
time horizon T. This durability allows individuals to confidently reapply after
taking time to implement the suggested changes. Through extensive experiments
in complex simulation environments, we show that our approach substantially
outperforms existing baselines, offering a superior balance between feasibility
and long-term validity. Together, these results underscore the importance of
incorporating temporal and behavioral dynamics into the design of practical
recourse systems.

</details>


### [200] [Modeling Psychological Profiles in Volleyball via Mixed-Type Bayesian Networks](https://arxiv.org/abs/2509.22111)
*Maria Iannario,Dae-Jin Lee,Manuele Leonelli*

Main category: cs.LG

TL;DR: 分析意大利排球运动员心理特征数据集，提出latent MMHC方法学习变量关系，在模拟和实际应用中表现良好，为运动员心理特征分析和发展提供框架。


<details>
  <summary>Details</summary>
Motivation: 心理属性很少单独运作，教练需分析相关特征网络，因此要研究运动员心理特征变量间的关系。

Method: 引入latent MMHC混合结构学习器，结合潜在高斯copula和基于约束的骨架以及基于分数的约束细化来学习混合类型变量间的有向关系，还研究了用于稳定性的自助聚合变体。

Result: 在模拟中，latent MMHC比近期基于copula的学习器有更低的结构汉明距离和更高的边召回率，且特异性高；应用于排球时，学习到的网络围绕目标设定和自信组织心理技能等。

Conclusion: 该方法为体育中心理特征分析和运动员发展决策支持提供了可解释、数据驱动的框架。

Abstract: Psychological attributes rarely operate in isolation: coaches reason about
networks of related traits. We analyze a new dataset of 164 female volleyball
players from Italy's C and D leagues that combines standardized psychological
profiling with background information. To learn directed relationships among
mixed-type variables (ordinal questionnaire scores, categorical demographics,
continuous indicators), we introduce latent MMHC, a hybrid structure learner
that couples a latent Gaussian copula and a constraint-based skeleton with a
constrained score-based refinement to return a single DAG. We also study a
bootstrap-aggregated variant for stability. In simulations spanning sample
size, sparsity, and dimension, latent Max-Min Hill-Climbing (MMHC) attains
lower structural Hamming distance and higher edge recall than recent
copula-based learners while maintaining high specificity. Applied to
volleyball, the learned network organizes mental skills around goal setting and
self-confidence, with emotional arousal linking motivation and anxiety, and
locates Big-Five traits (notably neuroticism and extraversion) upstream of
skill clusters. Scenario analyses quantify how improvements in specific skills
propagate through the network to shift preparation, confidence, and
self-esteem. The approach provides an interpretable, data-driven framework for
profiling psychological traits in sport and for decision support in athlete
development.

</details>


### [201] [Countering adversarial evasion in regression analysis](https://arxiv.org/abs/2509.22113)
*David Benfield,Phan Tu Vuong,Alain Zemkoho*

Main category: cs.LG

TL;DR: 文章指出对抗机器学习挑战模型分布一致性假设，博弈论模型有效，悲观双层优化在分类中表现好但未用于回归，提出用于回归场景的悲观双层优化方案。


<details>
  <summary>Details</summary>
Motivation: 现有悲观双层优化未应用于回归场景，需提出适用于回归场景的方法来应对对抗机器学习中的威胁。

Method: 提出一个对对手解决方案的凸性和唯一性不做假设的悲观双层优化程序用于回归场景。

Result: 无明确提及具体结果。

Conclusion: 提出适用于回归场景的悲观双层优化方案。

Abstract: Adversarial machine learning challenges the assumption that the underlying
distribution remains consistent throughout the training and implementation of a
prediction model. In particular, adversarial evasion considers scenarios where
adversaries adapt their data to influence particular outcomes from established
prediction models, such scenarios arise in applications such as spam email
filtering, malware detection and fake-image generation, where security methods
must be actively updated to keep up with the ever-improving generation of
malicious data. Game theoretic models have been shown to be effective at
modelling these scenarios and hence training resilient predictors against such
adversaries. Recent advancements in the use of pessimistic bilevel optimsiation
which remove assumptions about the convexity and uniqueness of the adversary's
optimal strategy have proved to be particularly effective at mitigating threats
to classifiers due to its ability to capture the antagonistic nature of the
adversary. However, this formulation has not yet been adapted to regression
scenarios. This article serves to propose a pessimistic bilevel optimisation
program for regression scenarios which makes no assumptions on the convexity or
uniqueness of the adversary's solutions.

</details>


### [202] [Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for Efficient Policy Optimization](https://arxiv.org/abs/2509.22115)
*Chao Wang,Tao Yang,Hongtao Tian,Yunsheng Shi,Qiyao Ma,Xiaotao Liu,Ting Yao,Wenbo Ding*

Main category: cs.LG

TL;DR: 提出D3S框架优化策略，在样本和token两级下采样，用动态调度防过拟合，实验表现佳。


<details>
  <summary>Details</summary>
Motivation: 解决无批评方法（如GRPO）收敛慢，学习信号被大量无信息样本和token稀释的问题。

Method: 提出D3S框架，包括样本级选子集最大化优势方差，token级优先处理优势大小和策略熵乘积高的token，采用受课程学习启发的动态下采样调度。

Result: 在Qwen2.5和Llama3.1上实验表明，将D3S集成到先进强化学习算法中，在多种推理基准测试中用更少样本和token达到了最优性能和泛化能力。

Conclusion: D3S框架有效提升了策略优化效率，有良好的性能和泛化性。

Abstract: Critic-free methods like GRPO reduce memory demands by estimating advantages
from multiple rollouts but tend to converge slowly, as critical learning
signals are diluted by an abundance of uninformative samples and tokens. To
tackle this challenge, we propose the \textbf{Dynamic Dual-Level Down-Sampling
(D$^3$S)} framework that prioritizes the most informative samples and tokens
across groups to improve the efficient of policy optimization. D$^3$S operates
along two levels: (1) the sample-level, which selects a subset of rollouts to
maximize advantage variance ($\text{Var}(A)$). We theoretically proven that
this selection is positively correlated with the upper bound of the policy
gradient norms, yielding higher policy gradients. (2) the token-level, which
prioritizes tokens with a high product of advantage magnitude and policy
entropy ($|A_{i,t}|\times H_{i,t}$), focusing updates on tokens where the
policy is both uncertain and impactful. Moreover, to prevent overfitting to
high-signal data, D$^3$S employs a dynamic down-sampling schedule inspired by
curriculum learning. This schedule starts with aggressive down-sampling to
accelerate early learning and gradually relaxes to promote robust
generalization. Extensive experiments on Qwen2.5 and Llama3.1 demonstrate that
integrating D$^3$S into advanced RL algorithms achieves state-of-the-art
performance and generalization while requiring \textit{fewer} samples and
tokens across diverse reasoning benchmarks. Our code is added in the
supplementary materials and will be made publicly available.

</details>


### [203] [Mind the Missing: Variable-Aware Representation Learning for Irregular EHR Time Series using Large Language Models](https://arxiv.org/abs/2509.22121)
*Jeong Eul Kwon,Joo Heung Yoon,Hyo Kyung Lee*

Main category: cs.LG

TL;DR: 提出VITAL框架用于学习不规则采样的生理时间序列，在基准数据集上表现优于现有方法，高缺失率下性能稳健。


<details>
  <summary>Details</summary>
Motivation: 解决电子健康记录中时间序列建模时不规则采样和高缺失率的问题。

Method: 区分生命体征和实验室检查两种临床变量，将生命体征重新编码到语言空间，实验室变量根据可用性嵌入。

Result: 在PhysioNet基准数据集上，VITAL优于现有不规则时间序列方法，在高缺失率下性能稳健。

Conclusion: VITAL框架在处理不规则采样和高缺失率的生理时间序列方面有效。

Abstract: Irregular sampling and high missingness are intrinsic challenges in modeling
time series derived from electronic health records (EHRs),where clinical
variables are measured at uneven intervals depending on workflow and
intervention timing. To address this, we propose VITAL, a variable-aware, large
language model (LLM) based framework tailored for learning from irregularly
sampled physiological time series. VITAL differentiates between two distinct
types of clinical variables: vital signs, which are frequently recorded and
exhibit temporal patterns, and laboratory tests, which are measured
sporadically and lack temporal structure. It reprograms vital signs into the
language space, enabling the LLM to capture temporal context and reason over
missing values through explicit encoding. In contrast, laboratory variables are
embedded either using representative summary values or a learnable [Not
measured] token, depending on their availability. Extensive evaluations on the
benchmark datasets from the PhysioNet demonstrate that VITAL outperforms state
of the art methods designed for irregular time series. Furthermore, it
maintains robust performance under high levels of missingness, which is
prevalent in real world clinical scenarios where key variables are often
unavailable.

</details>


### [204] [Slicing Wasserstein Over Wasserstein Via Functional Optimal Transport](https://arxiv.org/abs/2509.22138)
*Moritz Piening,Robert Beinert*

Main category: cs.LG

TL;DR: 本文提出双切片Wasserstein (DSW) 度量，可作为Wasserstein over Wasserstein (WoW) 距离的可扩展替代方案，避免了数值不稳定问题并节省计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有切片WoW加速方法依赖参数化元测度或高阶矩的存在，导致数值不稳定，需要更稳定高效的方法。

Method: 利用1d Wasserstein空间与函数空间$L_2([0,1])$中分位数函数的等距性，引入一般切片Wasserstein框架，结合经典欧几里得单位球上的积分得到DSW度量。

Result: DSW最小化对于离散化元测度等价于WoW最小化，避免了不稳定的高阶矩并节省计算成本。

Conclusion: 数值实验验证了DSW可作为WoW距离的可扩展替代方案。

Abstract: Wasserstein distances define a metric between probability measures on
arbitrary metric spaces, including meta-measures (measures over measures). The
resulting Wasserstein over Wasserstein (WoW) distance is a powerful, but
computationally costly tool for comparing datasets or distributions over images
and shapes. Existing sliced WoW accelerations rely on parametric meta-measures
or the existence of high-order moments, leading to numerical instability. As an
alternative, we propose to leverage the isometry between the 1d Wasserstein
space and the quantile functions in the function space $L_2([0,1])$. For this
purpose, we introduce a general sliced Wasserstein framework for arbitrary
Banach spaces. Due to the 1d Wasserstein isometry, this framework defines a
sliced distance between 1d meta-measures via infinite-dimensional
$L_2$-projections, parametrized by Gaussian processes. Combining this 1d
construction with classical integration over the Euclidean unit sphere yields
the double-sliced Wasserstein (DSW) metric for general meta-measures. We show
that DSW minimization is equivalent to WoW minimization for discretized
meta-measures, while avoiding unstable higher-order moments and computational
savings. Numerical experiments on datasets, shapes, and images validate DSW as
a scalable substitute for the WoW distance.

</details>


### [205] [Pushing Toward the Simplex Vertices: A Simple Remedy for Code Collapse in Smoothed Vector Quantization](https://arxiv.org/abs/2509.22161)
*Takashi Morita*

Main category: cs.LG

TL;DR: 本文提出一种正则化方法解决向量量化不可微问题，实验表明比现有方法更优。


<details>
  <summary>Details</summary>
Motivation: 向量量化中不可微的量化步骤阻碍梯度反向传播，现有方法多分别处理平滑量化的两个需求，本文希望同时满足。

Method: 引入一种简单直观的正则化方法，通过最小化每个单纯形顶点与其K近邻平滑量化器之间的距离，同时促进两个需求。

Result: 在离散图像自动编码和对比语音表示学习等基准测试中，该方法比现有方法实现了更可靠的码本利用，性能更优。

Conclusion: 所提出的正则化方法能有效解决向量量化问题，优于现有方法。

Abstract: Vector quantization, which discretizes a continuous vector space into a
finite set of representative vectors (a codebook), has been widely adopted in
modern machine learning. Despite its effectiveness, vector quantization poses a
fundamental challenge: the non-differentiable quantization step blocks gradient
backpropagation. Smoothed vector quantization addresses this issue by relaxing
the hard assignment of a codebook vector into a weighted combination of
codebook entries, represented as the matrix product of a simplex vector and the
codebook. Effective smoothing requires two properties: (1) smoothed quantizers
should remain close to a onehot vector, ensuring tight approximation, and (2)
all codebook entries should be utilized, preventing code collapse. Existing
methods typically address these desiderata separately. By contrast, the present
study introduces a simple and intuitive regularization that promotes both
simultaneously by minimizing the distance between each simplex vertex and its
$K$-nearest smoothed quantizers. Experiments on representative benchmarks,
including discrete image autoencoding and contrastive speech representation
learning, demonstrate that the proposed method achieves more reliable codebook
utilization and improves performance compared to prior approaches.

</details>


### [206] [Lightweight error mitigation strategies for post-training N:M activation sparsity in LLMs](https://arxiv.org/abs/2509.22166)
*Shirin Alanova,Kristina Kazistova,Ekaterina Galaeva,Alina Kostromina,Vladimir Smirnov,Redko Dmitry,Alexey Dontsov,Maxim Zhelnin,Evgeny Burnaev,Egor Shvetsov*

Main category: cs.LG

TL;DR: 本文对大语言模型（LLM）训练后N:M激活剪枝方法进行全面分析，展示激活剪枝优势，评估误差缓解技术和剪枝标准，探索不同稀疏模式，给出实用方法并为未来硬件提供思路。


<details>
  <summary>Details</summary>
Motivation: 高效大语言模型推理需求使稀疏化技术受关注，半结构化（N:M）权重剪枝成熟，但激活剪枝应用待探索，其有动态、输入自适应压缩及降低I/O开销潜力。

Method: 对多个LLM进行训练后N:M激活剪枝分析，评估轻量级、即插即用误差缓解技术和剪枝标准，探索不同稀疏模式。

Result: 同等稀疏水平下，激活剪枝比权重剪枝能更好保留生成能力；建立硬件友好基线；16:32模式性能接近非结构化稀疏，8:16模式更优。

Conclusion: 为激活剪枝提供有效实用方法，激励未来硬件支持更灵活稀疏模式。

Abstract: The demand for efficient large language model (LLM) inference has intensified
the focus on sparsification techniques. While semi-structured (N:M) pruning is
well-established for weights, its application to activation pruning remains
underexplored despite its potential for dynamic, input-adaptive compression and
reductions in I/O overhead. This work presents a comprehensive analysis of
methods for post-training N:M activation pruning in LLMs. Across multiple LLMs,
we demonstrate that pruning activations enables superior preservation of
generative capabilities compared to weight pruning at equivalent sparsity
levels. We evaluate lightweight, plug-and-play error mitigation techniques and
pruning criteria, establishing strong hardware-friendly baselines that require
minimal calibration. Furthermore, we explore sparsity patterns beyond NVIDIA's
standard 2:4, showing that the 16:32 pattern achieves performance nearly on par
with unstructured sparsity. However, considering the trade-off between
flexibility and hardware implementation complexity, we focus on the 8:16
pattern as a superior candidate. Our findings provide both effective practical
methods for activation pruning and a motivation for future hardware to support
more flexible sparsity patterns. Our code is available
https://anonymous.4open.science/r/Structured-Sparse-Activations-Inference-EC3C/README.md .

</details>


### [207] [Efficiency Boost in Decentralized Optimization: Reimagining Neighborhood Aggregation with Minimal Overhead](https://arxiv.org/abs/2509.22174)
*Durgesh Kalwar,Mayank Baranwal,Harshad Khadilkar*

Main category: cs.LG

TL;DR: 介绍了新框架DYNAWEIGHT用于多智能体网络信息聚合，能加速去中心化学习，实验显示训练速度提升且具通用性。


<details>
  <summary>Details</summary>
Motivation: 在数据敏感且完全去中心化的基础设施中，需要一种能加强隐私保护和简化计算操作的信息聚合方法。

Method: 提出DYNAWEIGHT框架，根据相邻服务器在本地数据集上的相对损失动态分配权重。

Result: 在MNIST、CIFAR10和CIFAR100等数据集上，不同服务器数量和图拓扑结构的实验显示训练速度显著提升。

Conclusion: DYNAWEIGHT是一种与任何底层服务器级优化算法兼容的聚合方案，具有通用性和广泛集成的潜力。

Abstract: In today's data-sensitive landscape, distributed learning emerges as a vital
tool, not only fortifying privacy measures but also streamlining computational
operations. This becomes especially crucial within fully decentralized
infrastructures where local processing is imperative due to the absence of
centralized aggregation. Here, we introduce DYNAWEIGHT, a novel framework to
information aggregation in multi-agent networks. DYNAWEIGHT offers substantial
acceleration in decentralized learning with minimal additional communication
and memory overhead. Unlike traditional static weight assignments, such as
Metropolis weights, DYNAWEIGHT dynamically allocates weights to neighboring
servers based on their relative losses on local datasets. Consequently, it
favors servers possessing diverse information, particularly in scenarios of
substantial data heterogeneity. Our experiments on various datasets MNIST,
CIFAR10, and CIFAR100 incorporating various server counts and graph topologies,
demonstrate notable enhancements in training speeds. Notably, DYNAWEIGHT
functions as an aggregation scheme compatible with any underlying server-level
optimization algorithm, underscoring its versatility and potential for
widespread integration.

</details>


### [208] [Learning Equivariant Functions via Quadratic Forms](https://arxiv.org/abs/2509.22184)
*Pavan Karjol,Vivek V Kashyap,Rohan Kashyap,Prathosh A P*

Main category: cs.LG

TL;DR: 本文提出通过学习二次型来学习群等变函数的方法，扩展框架到更一般场景并在多任务评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 学习群等变函数，挖掘潜在对称群，简化并提高神经网络模型效率。

Method: 学习与群相关的二次型，利用对称矩阵及其对角形式引入归纳偏置到神经网络架构；将框架扩展到通过对角（或乘积）群作用处理输入向量元组。

Result: 得到了保范的不变模型，等变模型表示为范数不变模型和尺度不变模型的乘积；分解等变函数为角度和尺度不变分量；在多任务评估中优于基线方法。

Conclusion: 所提框架能有效发现潜在对称性，高效学习等变函数。

Abstract: In this study, we introduce a method for learning group (known or unknown)
equivariant functions by learning the associated quadratic form $x^T A x$
corresponding to the group from the data. Certain groups, known as orthogonal
groups, preserve a specific quadratic form, and we leverage this property to
uncover the underlying symmetry group under the assumption that it is
orthogonal. By utilizing the corresponding unique symmetric matrix and its
inherent diagonal form, we incorporate suitable inductive biases into the
neural network architecture, leading to models that are both simplified and
efficient. Our approach results in an invariant model that preserves norms,
while the equivariant model is represented as a product of a norm-invariant
model and a scale-invariant model, where the ``product'' refers to the group
action.
  Moreover, we extend our framework to a more general setting where the
function acts on tuples of input vectors via a diagonal (or product) group
action. In this extension, the equivariant function is decomposed into an
angular component extracted solely from the normalized first vector and a
scale-invariant component that depends on the full Gram matrix of the tuple.
This decomposition captures the inter-dependencies between multiple inputs
while preserving the underlying group symmetry.
  We assess the effectiveness of our framework across multiple tasks, including
polynomial regression, top quark tagging, and moment of inertia matrix
prediction. Comparative analysis with baseline methods demonstrates that our
model consistently excels in both discovering the underlying symmetry and
efficiently learning the corresponding equivariant function.

</details>


### [209] [Kernel Regression of Multi-Way Data via Tensor Trains with Hadamard Overparametrization: The Dynamic Graph Flow Case](https://arxiv.org/abs/2509.22197)
*Duc Thien Nguyen,Konstantinos Slavakis,Eleftherios Kofidis,Dimitris Pados*

Main category: cs.LG

TL;DR: 提出KReTTaH框架用于可解释的多路数据插补，在动态图流估计应用中表现出色，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发可解释的多路数据插补框架，解决数据缺失问题。

Method: 将插补问题转化为再生核希尔伯特空间中的回归问题，利用固定张量列车（TT）秩的张量和Hadamard过参数化实现参数效率，在黎曼流形上解决平滑逆问题。

Result: 在真实世界图数据集的数值测试中，KReTTaH始终优于包括非参数张量和神经网络方法在内的现有替代方法。

Conclusion: KReTTaH是一种有效的可解释多路数据插补框架，在动态图流估计等应用中有良好表现。

Abstract: A regression-based framework for interpretable multi-way data imputation,
termed Kernel Regression via Tensor Trains with Hadamard overparametrization
(KReTTaH), is introduced. KReTTaH adopts a nonparametric formulation by casting
imputation as regression via reproducing kernel Hilbert spaces. Parameter
efficiency is achieved through tensors of fixed tensor-train (TT) rank, which
reside on low-dimensional Riemannian manifolds, and is further enhanced via
Hadamard overparametrization, which promotes sparsity within the TT parameter
space. Learning is accomplished by solving a smooth inverse problem posed on
the Riemannian manifold of fixed TT-rank tensors. As a representative
application, the estimation of dynamic graph flows is considered. In this
setting, KReTTaH exhibits flexibility by seamlessly incorporating graph-based
(topological) priors via its inverse problem formulation. Numerical tests on
real-world graph datasets demonstrate that KReTTaH consistently outperforms
state-of-the-art alternatives-including a nonparametric tensor- and a
neural-network-based methods-for imputing missing, time-varying edge flows.

</details>


### [210] [Reversible GNS for Dissipative Fluids with Consistent Bidirectional Dynamics](https://arxiv.org/abs/2509.22207)
*Mu Huang,Linning Xu,Mingyue Dai,Yidi Shao,Bo Dai*

Main category: cs.LG

TL;DR: 提出可逆图网络模拟器R - GNS统一框架，用于耗散流体系统的正向和逆向模拟，实验显示其在精度、速度等方面有优势。


<details>
  <summary>Details</summary>
Motivation: 现有粒子模拟器在逆推理方面困难，优化求解器慢、不稳定且常不收敛，需解决耗散系统中逆推理问题。

Method: 提出基于共享参数的残差可逆消息传递的数学可逆设计，将正向动力学与逆推理耦合。

Result: 在三个耗散基准测试中，R - GNS用四分之一参数实现更高精度和一致性，逆推理比优化基线快超100倍，正向模拟速度与强GNS基线匹配，目标条件任务中消除迭代优化并实现数量级加速，能生成复杂目标形状轨迹。

Conclusion: R - GNS是首个统一耗散流体系统正向和逆向模拟的可逆框架。

Abstract: Simulating physically plausible trajectories toward user-defined goals is a
fundamental yet challenging task in fluid dynamics. While particle-based
simulators can efficiently reproduce forward dynamics, inverse inference
remains difficult, especially in dissipative systems where dynamics are
irreversible and optimization-based solvers are slow, unstable, and often fail
to converge. In this work, we introduce the Reversible Graph Network Simulator
(R-GNS), a unified framework that enforces bidirectional consistency within a
single graph architecture. Unlike prior neural simulators that approximate
inverse dynamics by fitting backward data, R-GNS does not attempt to reverse
the underlying physics. Instead, we propose a mathematically invertible design
based on residual reversible message passing with shared parameters, coupling
forward dynamics with inverse inference to deliver accurate predictions and
efficient recovery of plausible initial states. Experiments on three
dissipative benchmarks (Water-3D, WaterRamps, and WaterDrop) show that R-GNS
achieves higher accuracy and consistency with only one quarter of the
parameters, and performs inverse inference more than 100 times faster than
optimization-based baselines. For forward simulation, R-GNS matches the speed
of strong GNS baselines, while in goal-conditioned tasks it eliminates
iterative optimization and achieves orders-of-magnitude speedups. On
goal-conditioned tasks, R-GNS further demonstrates its ability to complex
target shapes (e.g., characters "L" and "N") through vivid, physically
consistent trajectories. To our knowledge, this is the first reversible
framework that unifies forward and inverse simulation for dissipative fluid
systems.

</details>


### [211] [A Law of Data Reconstruction for Random Features (and Beyond)](https://arxiv.org/abs/2509.22214)
*Leonardo Iurada,Simone Bombari,Tatiana Tommasi,Marco Mondelli*

Main category: cs.LG

TL;DR: 本文从数据重建角度研究大规模深度学习模型的记忆问题，表明当模型参数 p 大于数据维度 d 与训练样本数 n 的乘积时可实现数据重建，并给出优化方法，在多种架构上效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习理论常将记忆视为插值或标签拟合，本文从数据重建角度研究模型记忆问题。

Method: 在随机特征模型中分析，当 p 远大于 dn 时，利用训练样本在特征空间张成的子空间信息，提出从模型参数重建数据集的优化方法。

Result: 该方法在随机特征、两层全连接和深度残差网络等多种架构上表现良好。

Conclusion: 揭示了数据重建规律，当 p 超过阈值 dn 时可恢复整个训练数据集。

Abstract: Large-scale deep learning models are known to memorize parts of the training
set. In machine learning theory, memorization is often framed as interpolation
or label fitting, and classical results show that this can be achieved when the
number of parameters $p$ in the model is larger than the number of training
samples $n$. In this work, we consider memorization from the perspective of
data reconstruction, demonstrating that this can be achieved when $p$ is larger
than $dn$, where $d$ is the dimensionality of the data. More specifically, we
show that, in the random features model, when $p \gg dn$, the subspace spanned
by the training samples in feature space gives sufficient information to
identify the individual samples in input space. Our analysis suggests an
optimization method to reconstruct the dataset from the model parameters, and
we demonstrate that this method performs well on various architectures (random
features, two-layer fully-connected and deep residual networks). Our results
reveal a law of data reconstruction, according to which the entire training
dataset can be recovered as $p$ exceeds the threshold $dn$.

</details>


### [212] [Automatic Discovery of One Parameter Subgroups of $SO(n)$](https://arxiv.org/abs/2509.22219)
*Pavan Karjol,Vivek V Kashyap,Rohan Kashyap,Prathosh A P*

Main category: cs.LG

TL;DR: 提出用于自动发现SO(3)和SO(n)单参数子群的框架，通过标准约旦形式等方法，在多任务中展现有效性。


<details>
  <summary>Details</summary>
Motivation: 单参数子群在机器人学、量子力学和分子结构分析等广泛应用中至关重要，需自动发现方法。

Method: 利用斜对称矩阵的标准约旦形式建立轨道规范形式，推导不变函数标准化表示，学习参数以发现单参数子群。

Result: 在双摆建模、转动惯量预测、顶夸克标记和不变多项式回归等任务中成功恢复有意义子群结构，产生可解释、对称感知的表示。

Conclusion: 所提出的框架能有效自动发现SO(3)和SO(n)的单参数子群。

Abstract: We introduce a novel framework for the automatic discovery of one-parameter
subgroups ($H_{\gamma}$) of $SO(3)$ and, more generally, $SO(n)$. One-parameter
subgroups of $SO(n)$ are crucial in a wide range of applications, including
robotics, quantum mechanics, and molecular structure analysis. Our method
utilizes the standard Jordan form of skew-symmetric matrices, which define the
Lie algebra of $SO(n)$, to establish a canonical form for orbits under the
action of $H_{\gamma}$. This canonical form is then employed to derive a
standardized representation for $H_{\gamma}$-invariant functions. By learning
the appropriate parameters, the framework uncovers the underlying one-parameter
subgroup $H_{\gamma}$. The effectiveness of the proposed approach is
demonstrated through tasks such as double pendulum modeling, moment of inertia
prediction, top quark tagging and invariant polynomial regression, where it
successfully recovers meaningful subgroup structure and produces interpretable,
symmetry-aware representations.

</details>


### [213] [Fairness-Aware Reinforcement Learning (FAReL): A Framework for Transparent and Balanced Sequential Decision-Making](https://arxiv.org/abs/2509.22232)
*Alexandra Cimpean,Nicole Orzan,Catholijn Jonker,Pieter Libin,Ann Nowé*

Main category: cs.LG

TL;DR: 提出一个可探索性能与公平性权衡的框架，在招聘和欺诈检测场景评估，能学习到更公平策略且性能损失小，还给出应用指南。


<details>
  <summary>Details</summary>
Motivation: 现实序贯决策问题需算法在性能和公平性间做合适透明权衡，且难以事先指定期望权衡。

Method: 提出扩展马尔可夫决策过程$f$MDP明确编码个体和群体，在序贯决策问题中形式化公平概念，构建计算公平度量的框架。

Result: 框架在多场景中学习到更公平策略，性能奖励损失小，发现群体和个体公平概念不一定相互蕴含。

Conclusion: 该框架能在性能和公平性间做权衡，适用于多种场景，给出应用不同问题场景的指南。

Abstract: Equity in real-world sequential decision problems can be enforced using
fairness-aware methods. Therefore, we require algorithms that can make suitable
and transparent trade-offs between performance and the desired fairness
notions. As the desired performance-fairness trade-off is hard to specify a
priori, we propose a framework where multiple trade-offs can be explored.
Insights provided by the reinforcement learning algorithm regarding the
obtainable performance-fairness trade-offs can then guide stakeholders in
selecting the most appropriate policy. To capture fairness, we propose an
extended Markov decision process, $f$MDP, that explicitly encodes individuals
and groups. Given this $f$MDP, we formalise fairness notions in the context of
sequential decision problems and formulate a fairness framework that computes
fairness measures over time. We evaluate our framework in two scenarios with
distinct fairness requirements: job hiring, where strong teams must be composed
while treating applicants equally, and fraud detection, where fraudulent
transactions must be detected while ensuring the burden on customers is fairly
distributed. We show that our framework learns policies that are more fair
across multiple scenarios, with only minor loss in performance reward.
Moreover, we observe that group and individual fairness notions do not
necessarily imply one another, highlighting the benefit of our framework in
settings where both fairness types are desired. Finally, we provide guidelines
on how to apply this framework across different problem settings.

</details>


### [214] [ASSESS: A Semantic and Structural Evaluation Framework for Statement Similarity](https://arxiv.org/abs/2509.22246)
*Xiaoyang Liu,Tao Zhu,Zineng Dong,Yuntian Liu,Qingfeng Guo,Zhaoxuan Liu,Yu Chen,Tao Luo*

Main category: cs.LG

TL;DR: 现有形式语句相似度评估指标存在不足，本文提出 ASSESS 框架及 TransTED 相似度指标，用新基准 EPLA 验证，结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有形式语句相似度评估指标不能平衡语义和结构信息，需改进。

Method: 引入 ASSESS 框架，将形式语句转换为操作符树，用 TransTED 相似度指标计算得分；提出新基准 EPLA 进行验证。

Result: 在 EPLA 上实验表明，TransTED 相似度指标优于现有方法，达到了最先进的准确率和最高的 Kappa 系数。

Conclusion: ASSESS 框架和 TransTED 相似度指标有效，能更好地平衡语义和结构信息评估语句相似度。

Abstract: Statement autoformalization, the automated translation of statements from
natural language into formal languages, has seen significant advancements, yet
the development of automated evaluation metrics remains limited. Existing
metrics for formal statement similarity often fail to balance semantic and
structural information. String-based approaches capture syntactic structure but
ignore semantic meaning, whereas proof-based methods validate semantic
equivalence but disregard structural nuances and, critically, provide no graded
similarity score in the event of proof failure. To address these issues, we
introduce ASSESS (A Semantic and Structural Evaluation Framework for Statement
Similarity), which comprehensively integrates semantic and structural
information to provide a continuous similarity score. Our framework first
transforms formal statements into Operator Trees to capture their syntactic
structure and then computes a similarity score using our novel TransTED
(Transformation Tree Edit Distance) Similarity metric, which enhances
traditional Tree Edit Distance by incorporating semantic awareness through
transformations. For rigorous validation, we present EPLA (Evaluating
Provability and Likeness for Autoformalization), a new benchmark of 524
expert-annotated formal statement pairs derived from miniF2F and ProofNet, with
labels for both semantic provability and structural likeness. Experiments on
EPLA demonstrate that TransTED Similarity outperforms existing methods,
achieving state-of-the-art accuracy and the highest Kappa coefficient. The
benchmark, and implementation code will be made public soon.

</details>


### [215] [Wavelet-Induced Rotary Encodings: RoPE Meets Graphs](https://arxiv.org/abs/2509.22259)
*Isaac Reid,Arijit Sehanobish,Cedrik Höfs,Bruno Mlodozeniec,Leonhard Vulpius,Federico Barbero,Adrian Weller,Krzysztof Choromanski,Richard E. Turner,Petar Veličković*

Main category: cs.LG

TL;DR: 介绍了适用于图结构数据的Wavelet - Induced Rotary Encodings (WIRE)，它扩展了RoPE，有理论特性，经多任务测试有效。


<details>
  <summary>Details</summary>
Motivation: 将流行于大语言模型和视觉Transformer中的RoPE算法扩展应用到图结构数据上。

Method: 提出WIRE扩展RoPE，并对其进行理论分析，在多种合成和真实世界任务中测试。

Result: WIRE具有多种理想理论特性，在图结构重要的场景中有效。

Conclusion: WIRE是一种有效的适用于图结构数据的编码方法。

Abstract: We introduce WIRE: Wavelet-Induced Rotary Encodings. WIRE extends Rotary
Position Encodings (RoPE), a popular algorithm in LLMs and ViTs, to
graph-structured data. We demonstrate that WIRE is more general than RoPE,
recovering the latter in the special case of grid graphs. WIRE also enjoys a
host of desirable theoretical properties, including equivariance under node
ordering permutation, compatibility with linear attention, and (under select
assumptions) asymptotic dependence on graph resistive distance. We test WIRE on
a range of synthetic and real-world tasks, including identifying monochromatic
subgraphs, semantic segmentation of point clouds, and more standard graph
benchmarks. We find it to be effective in settings where the underlying graph
structure is important.

</details>


### [216] [Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning](https://arxiv.org/abs/2509.22263)
*Nakyeong Yang,Dong-Kyum Kim,Jea Kwon,Minsung Kim,Kyomin Jung,Meeyoung Cha*

Main category: cs.LG

TL;DR: 现有大语言模型去学习方法易‘重新学习’，本文提出Ssiuu方法解决该问题，实验证明其效果好。


<details>
  <summary>Details</summary>
Motivation: 现有去学习方法在后续训练中易‘重新学习’，存在隐私风险，且造成浅层对齐问题。

Method: 引入Ssiuu方法，采用归因引导正则化防止虚假负面干扰，忠实移除目标知识。

Result: 实验表明该方法能可靠擦除目标知识，在两种实际再训练场景中优于强基线。

Conclusion: 安全部署语言模型需要强大且忠实的去学习方法。

Abstract: Large language models trained on web-scale data can memorize private or
sensitive knowledge, raising significant privacy risks. Although some
unlearning methods mitigate these risks, they remain vulnerable to "relearning"
during subsequent training, allowing a substantial portion of forgotten
knowledge to resurface. In this paper, we show that widely used unlearning
methods cause shallow alignment: instead of faithfully erasing target
knowledge, they generate spurious unlearning neurons that amplify negative
influence to hide it. To overcome this limitation, we introduce Ssiuu, a new
class of unlearning methods that employs attribution-guided regularization to
prevent spurious negative influence and faithfully remove target knowledge.
Experimental results confirm that our method reliably erases target knowledge
and outperforms strong baselines across two practical retraining scenarios: (1)
adversarial injection of private data, and (2) benign attack using an
instruction-following benchmark. Our findings highlight the necessity of robust
and faithful unlearning methods for safe deployment of language models.

</details>


### [217] [Towards a more realistic evaluation of machine learning models for bearing fault diagnosis](https://arxiv.org/abs/2509.22267)
*João Paulo Vieira,Victor Afonso Bauler,Rodrigo Kobashikawa Rosa,Danilo Silva*

Main category: cs.LG

TL;DR: 本文研究振动轴承故障诊断中数据泄露问题，提出无泄露评估方法，在三个数据集验证，强调泄露感知评估协议重要性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习研究因数据泄露难以推广到实际应用，需解决数据泄露对模型评估的影响。

Method: 提出基于轴承数据划分的无泄露评估方法，将分类任务重构为多标签问题，研究数据集多样性对泛化能力的影响。

Result: 常见数据集划分策略会引入虚假关联，基于轴承划分可避免数据泄露，独特训练轴承数量是实现稳健性能的关键因素。

Conclusion: 强调泄露感知评估协议的重要性，为数据集划分、模型选择和验证提供实用指南，促进工业故障诊断中更可靠机器学习系统的发展。

Abstract: Reliable detection of bearing faults is essential for maintaining the safety
and operational efficiency of rotating machinery. While recent advances in
machine learning (ML), particularly deep learning, have shown strong
performance in controlled settings, many studies fail to generalize to
real-world applications due to methodological flaws, most notably data leakage.
This paper investigates the issue of data leakage in vibration-based bearing
fault diagnosis and its impact on model evaluation. We demonstrate that common
dataset partitioning strategies, such as segment-wise and condition-wise
splits, introduce spurious correlations that inflate performance metrics. To
address this, we propose a rigorous, leakage-free evaluation methodology
centered on bearing-wise data partitioning, ensuring no overlap between the
physical components used for training and testing. Additionally, we reformulate
the classification task as a multi-label problem, enabling the detection of
co-occurring fault types and the use of prevalence-independent metrics such as
Macro AUROC. Beyond preventing leakage, we also examine the effect of dataset
diversity on generalization, showing that the number of unique training
bearings is a decisive factor for achieving robust performance. We evaluate our
methodology on three widely adopted datasets: CWRU, Paderborn University (PU),
and University of Ottawa (UORED-VAFCLS). This study highlights the importance
of leakage-aware evaluation protocols and provides practical guidelines for
dataset partitioning, model selection, and validation, fostering the
development of more trustworthy ML systems for industrial fault diagnosis
applications.

</details>


### [218] [Fine-Grained Uncertainty Decomposition in Large Language Models: A Spectral Approach](https://arxiv.org/abs/2509.22272)
*Nassim Walha,Sebastian G. Gruber,Thomas Decker,Yinchong Yang,Alireza Javanmardi,Eyke Hüllermeier,Florian Buettner*

Main category: cs.LG

TL;DR: 论文提出Spectral Uncertainty方法量化和分解大语言模型的不确定性，在多种模型和数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在应用中日益广泛，准确衡量其预测不确定性至关重要，需区分随机不确定性和认知不确定性。

Method: 引入Spectral Uncertainty方法，利用量子信息理论中的冯·诺伊曼熵，将总不确定性分解为随机和认知分量，并纳入语义相似度的细粒度表示。

Result: 经验评估表明，Spectral Uncertainty在估计随机和总不确定性方面，在多种模型和基准数据集上优于现有方法。

Conclusion: Spectral Uncertainty是一种有效量化和分解大语言模型不确定性的方法。

Abstract: As Large Language Models (LLMs) are increasingly integrated in diverse
applications, obtaining reliable measures of their predictive uncertainty has
become critically important. A precise distinction between aleatoric
uncertainty, arising from inherent ambiguities within input data, and epistemic
uncertainty, originating exclusively from model limitations, is essential to
effectively address each uncertainty source. In this paper, we introduce
Spectral Uncertainty, a novel approach to quantifying and decomposing
uncertainties in LLMs. Leveraging the Von Neumann entropy from quantum
information theory, Spectral Uncertainty provides a rigorous theoretical
foundation for separating total uncertainty into distinct aleatoric and
epistemic components. Unlike existing baseline methods, our approach
incorporates a fine-grained representation of semantic similarity, enabling
nuanced differentiation among various semantic interpretations in model
responses. Empirical evaluations demonstrate that Spectral Uncertainty
outperforms state-of-the-art methods in estimating both aleatoric and total
uncertainty across diverse models and benchmark datasets.

</details>


### [219] [Unlocking the Power of Mixture-of-Experts for Task-Aware Time Series Analytics](https://arxiv.org/abs/2509.22279)
*Xingjian Wu,Zhengyu Li,Hanyin Cheng,Xiangfei Qiu,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: 提出基于MoE的时间序列框架PatchMoE，用循环噪声门控利用分层信息，在多下游任务实验中性能达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有MoE架构因任务无关路由器和缺乏建模通道相关性能力，难以适应时间序列分析的多任务。

Method: 提出PatchMoE框架，使用循环噪声门控利用分层信息，在时间和通道维度操作路由策略，用精心设计的损失函数建模复杂相关性。

Result: 在五个下游任务上的综合实验表明PatchMoE达到了最先进的性能。

Conclusion: PatchMoE能支持不同任务的复杂知识利用，是一种有效的时间序列分析框架。

Abstract: Time Series Analysis is widely used in various real-world applications such
as weather forecasting, financial fraud detection, imputation for missing data
in IoT systems, and classification for action recognization. Mixture-of-Experts
(MoE), as a powerful architecture, though demonstrating effectiveness in NLP,
still falls short in adapting to versatile tasks in time series analytics due
to its task-agnostic router and the lack of capability in modeling channel
correlations. In this study, we propose a novel, general MoE-based time series
framework called PatchMoE to support the intricate ``knowledge'' utilization
for distinct tasks, thus task-aware. Based on the observation that hierarchical
representations often vary across tasks, e.g., forecasting vs. classification,
we propose a Recurrent Noisy Gating to utilize the hierarchical information in
routing, thus obtaining task-sepcific capability. And the routing strategy is
operated on time series tokens in both temporal and channel dimensions, and
encouraged by a meticulously designed Temporal \& Channel Load Balancing Loss
to model the intricate temporal and channel correlations. Comprehensive
experiments on five downstream tasks demonstrate the state-of-the-art
performance of PatchMoE.

</details>


### [220] [Conditional Denoising Diffusion Autoencoders for Wireless Semantic Communications](https://arxiv.org/abs/2509.22282)
*Mehdi Letafati,Samad Ali,Matti Latva-aho*

Main category: cs.LG

TL;DR: 提出扩散自编码器模型用于无线语义通信，证明解码器是一致估计器，通过多数据集仿真展示性能。


<details>
  <summary>Details</summary>
Motivation: 现有语义通信框架强调信道自适应神经编解码方案，缺乏对信号分布的充分探索，且自编码器架构存在扩展性问题。

Method: 提出扩散自编码器模型学习“语义到干净”映射，发送端用神经编码器提取高层语义，接收端用条件扩散模型进行信号空间去噪。

Result: 分析证明所提解码器模型是真实数据的一致估计器，通过CIFAR - 10和MNIST数据集仿真对比性能，扩展到多用户语义通信。

Conclusion: 扩散自编码器模型在无线语义通信中有良好表现，能解决现有框架的问题。

Abstract: Semantic communication (SemCom) systems aim to learn the mapping from
low-dimensional semantics to high-dimensional ground-truth. While this is more
akin to a "domain translation" problem, existing frameworks typically emphasize
on channel-adaptive neural encoding-decoding schemes, lacking full exploration
of signal distribution. Moreover, such methods so far have employed
autoencoder-based architectures, where the encoding is tightly coupled to a
matched decoder, causing scalability issues in practice. To address these gaps,
diffusion autoencoder models are proposed for wireless SemCom. The goal is to
learn a "semantic-to-clean" mapping, from the semantic space to the
ground-truth probability distribution. A neural encoder at semantic transmitter
extracts the high-level semantics, and a conditional diffusion model (CDiff) at
the semantic receiver exploits the source distribution for signal-space
denoising, while the received semantic latents are incorporated as the
conditioning input to "steer" the decoding process towards the semantics
intended by the transmitter. It is analytically proved that the proposed
decoder model is a consistent estimator of the ground-truth data. Furthermore,
extensive simulations over CIFAR-10 and MNIST datasets are provided along with
design insights, highlighting the performance compared to legacy autoencoders
and variational autoencoders (VAE). Simulations are further extended to the
multi-user SemCom, identifying the dominating factors in a more realistic
setup.

</details>


### [221] [A Multi-Level Framework for Multi-Objective Hypergraph Partitioning: Combining Minimum Spanning Tree and Proximal Gradient](https://arxiv.org/abs/2509.22294)
*Yingying Li,Mingxuan Xie,Hailong You,Yongqiang Yao,Hongwei Liu*

Main category: cs.LG

TL;DR: 本文提出基于多目标非凸约束松弛模型的超图划分框架，用改进算法生成特征，设计不同规模数据策略和细化策略，实验显示算法在划分质量上有优势。


<details>
  <summary>Details</summary>
Motivation: 设计高效的超图划分框架，提升划分质量，避免局部最优。

Method: 提出多目标非凸约束松弛模型，用改进加速近端梯度算法生成顶点特征；针对不同数据规模设计基于MST的策略；引入细化策略。

Result: 在公共基准集实验中，相比KaHyPar平均减少2%-5%割边，特定实例最多提升35%；在加权顶点集上优于多个现有划分器；细化策略最多提升hMetis划分结果16%。

Conclusion: 所提算法在超图划分质量上具有优越性和竞争力，综合评估验证其性能。

Abstract: This paper proposes an efficient hypergraph partitioning framework based on a
novel multi-objective non-convex constrained relaxation model. A modified
accelerated proximal gradient algorithm is employed to generate diverse
$k$-dimensional vertex features to avoid local optima and enhance partition
quality. Two MST-based strategies are designed for different data scales: for
small-scale data, the Prim algorithm constructs a minimum spanning tree
followed by pruning and clustering; for large-scale data, a subset of
representative nodes is selected to build a smaller MST, while the remaining
nodes are assigned accordingly to reduce complexity. To further improve
partitioning results, refinement strategies including greedy migration,
swapping, and recursive MST-based clustering are introduced for partitions.
  Experimental results on public benchmark sets demonstrate that the proposed
algorithm achieves reductions in cut size of approximately 2\%--5\% on average
compared to KaHyPar in 2, 3, and 4-way partitioning, with improvements of up to
35\% on specific instances. Particularly on weighted vertex sets, our algorithm
outperforms state-of-the-art partitioners including KaHyPar, hMetis,
Mt-KaHyPar, and K-SpecPart, highlighting its superior partitioning quality and
competitiveness. Furthermore, the proposed refinement strategy improves hMetis
partitions by up to 16\%. A comprehensive evaluation based on virtual instance
methodology and parameter sensitivity analysis validates the algorithm's
competitiveness and characterizes its performance trade-offs.

</details>


### [222] [Aurora: Towards Universal Generative Multimodal Time Series Forecasting](https://arxiv.org/abs/2509.22295)
*Xingjian Wu,Jianxin Jin,Wanghui Qiu,Peng Chen,Yang Shu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 提出多模态时间序列基础模型Aurora，支持多模态输入和零样本推理，实验显示其在单模态和多模态场景表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有工作在跨域泛化上有不足，单模态模型未显式利用特定领域知识，端到端多模态模型不支持跨域零样本推理。

Method: 在跨域多模态时间序列语料库上预训练Aurora，通过分词、编码和蒸馏提取多模态领域知识，利用模态引导多头自注意力注入时间表征建模，解码阶段使用多模态表征生成未来标记条件和原型，采用原型引导流匹配进行生成式概率预测。

Result: 在TimeMMD、TSFM - Bench和ProbTS等基准测试中，Aurora在单模态和多模态场景均达到了一致的最优性能。

Conclusion: Aurora具有强大的跨域泛化能力，能有效解决时间序列预测中的跨域问题。

Abstract: Cross-domain generalization is very important in Time Series Forecasting
because similar historical information may lead to distinct future trends due
to the domain-specific characteristics. Recent works focus on building unimodal
time series foundation models and end-to-end multimodal supervised models.
Since domain-specific knowledge is often contained in modalities like texts,
the former lacks the explicit utilization of them, thus hindering the
performance. The latter is tailored for end-to-end scenarios and does not
support zero-shot inference for cross-domain scenarios. In this work, we
introduce Aurora, a Multimodal Time Series Foundation Model, which supports
multimodal inputs and zero-shot inference. Pretrained on Corss-domain
Multimodal Time Series Corpus, Aurora can adaptively extract and focus on key
domain knowledge contained in corrsponding text or image modalities, thus
possessing strong Cross-domain generalization capability. Through tokenization,
encoding, and distillation, Aurora can extract multimodal domain knowledge as
guidance and then utilizes a Modality-Guided Multi-head Self-Attention to
inject them into the modeling of temporal representations. In the decoding
phase, the multimodal representations are used to generate the conditions and
prototypes of future tokens, contributing to a novel Prototype-Guided Flow
Matching for generative probabilistic forecasting. Comprehensive experiments on
well-recognized benchmarks, including TimeMMD, TSFM-Bench and ProbTS,
demonstrate the consistent state-of-the-art performance of Aurora on both
unimodal and multimodal scenarios.

</details>


### [223] [HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space](https://arxiv.org/abs/2509.22299)
*Ke Li,Zheng Yang,Zhongbin Zhou,Feng Xue,Zhonglin Jiang,Wenxiao Wang*

Main category: cs.LG

TL;DR: 提出新剪枝算法HEAPr用于MoE架构大语言模型，降内存开销，实验显示性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: MoE架构大语言模型参数多、内存需求高，现有专家级剪枝方法精度下降大。

Method: 将专家分解为原子专家，利用类似OBS理论的二阶信息衡量重要性，转换并简化二阶信息降低复杂度。

Result: 在多个MoE模型实验中，HEAPr在不同压缩比和基准测试中表现优于现有方法，部分模型在20% - 25%压缩比下近乎无损压缩，减少近20%FLOPs。

Conclusion: HEAPr是一种有效且灵活的剪枝算法，能解决MoE架构大语言模型部署难题。

Abstract: Mixture-of-Experts (MoE) architectures in large language models (LLMs)
deliver exceptional performance and reduced inference costs compared to dense
LLMs. However, their large parameter counts result in prohibitive memory
requirements, limiting practical deployment. While existing pruning methods
primarily focus on expert-level pruning, this coarse granularity often leads to
substantial accuracy degradation. In this work, we introduce HEAPr, a novel
pruning algorithm that decomposes experts into smaller, indivisible atomic
experts, enabling more precise and flexible atomic expert pruning. To measure
the importance of each atomic expert, we leverage second-order information
based on principles similar to Optimal Brain Surgeon (OBS) theory. To address
the computational and storage challenges posed by second-order information,
HEAPr exploits the inherent properties of atomic experts to transform the
second-order information from expert parameters into that of atomic expert
parameters, and further simplifies it to the second-order information of atomic
expert outputs. This approach reduces the space complexity from $O(d^4)$, where
d is the model's dimensionality, to $O(d^2)$. HEAPr requires only two forward
passes and one backward pass on a small calibration set to compute the
importance of atomic experts. Extensive experiments on MoE models, including
DeepSeek MoE and Qwen MoE family, demonstrate that HEAPr outperforms existing
expert-level pruning methods across a wide range of compression ratios and
benchmarks. Specifically, HEAPr achieves nearly lossless compression at
compression ratios of 20% ~ 25% in most models, while also reducing FLOPs
nearly by 20%. The code can be found at
\href{https://github.com/LLIKKE/HEAPr}{https://github.com/LLIKKE/HEAPr}.

</details>


### [224] [SoDaDE: Solvent Data-Driven Embeddings with Small Transformer Models](https://arxiv.org/abs/2509.22302)
*Gabriel Kitso Gibberd,Jose Pablo Folch,Antonio Del Rio Chanona*

Main category: cs.LG

TL;DR: 提出新溶剂表示方案SoDaDE，用小数据集创建溶剂指纹，在预测产率上表现优于之前表示，展示小数据集可制作数据驱动指纹。


<details>
  <summary>Details</summary>
Motivation: 化学数据集中学习的通用表示缺乏特定物理上下文，有害溶剂使用是化工行业气候问题，需研究绿色溶剂替代，要开发新溶剂表示方案。

Method: 开发Solvent Data Driven Embeddings (SoDaDE)，用小变压器模型和溶剂属性数据集为溶剂创建指纹。

Result: 用SoDaDE在最近发布的数据集上预测产率，表现优于之前的表示。

Conclusion: 小数据集可制作数据驱动指纹，建立的工作流可用于其他应用探索。

Abstract: Computational representations have become crucial in unlocking the recent
growth of machine learning algorithms for chemistry. Initially hand-designed,
machine learning has shown that meaningful representations can be learnt from
data. Chemical datasets are limited and so the representations learnt from data
are generic, being trained on broad datasets which contain shallow information
on many different molecule types. For example, generic fingerprints lack
physical context specific to solvents. However, the use of harmful solvents is
a leading climate-related issue in the chemical industry, and there is a surge
of interest in green solvent replacement. To empower this research, we propose
a new solvent representation scheme by developing Solvent Data Driven
Embeddings (SoDaDE). SoDaDE uses a small transformer model and solvent property
dataset to create a fingerprint for solvents. To showcase their effectiveness,
we use SoDaDE to predict yields on a recently published dataset, outperforming
previous representations. We demonstrate through this paper that data-driven
fingerprints can be made with small datasets and set-up a workflow that can be
explored for other applications.

</details>


### [225] [Adaptive Policy Backbone via Shared Network](https://arxiv.org/abs/2509.22310)
*Bumgeun Park,Donghwan Lee*

Main category: cs.LG

TL;DR: 提出Adaptive Policy Backbone (APB) 元迁移强化学习方法，提升样本效率，适应分布外任务。


<details>
  <summary>Details</summary>
Motivation: 强化学习获取最优策略需大量交互数据，利用先验知识在训练和部署任务不匹配时效用降低，且先前工作多局限于分布内设置。

Method: 提出APB方法，在共享骨干网络前后插入轻量级线性层，实现参数高效微调并在适应过程中保留先验知识。

Result: APB比标准强化学习提高了样本效率，能适应现有元强化学习基线通常失败的分布外任务。

Conclusion: APB方法在提升样本效率和适应分布外任务方面有较好效果。

Abstract: Reinforcement learning (RL) has achieved impressive results across domains,
yet learning an optimal policy typically requires extensive interaction data,
limiting practical deployment. A common remedy is to leverage priors, such as
pre-collected datasets or reference policies, but their utility degrades under
task mismatch between training and deployment. While prior work has sought to
address this mismatch, it has largely been restricted to in-distribution
settings. To address this challenge, we propose Adaptive Policy Backbone (APB),
a meta-transfer RL method that inserts lightweight linear layers before and
after a shared backbone, thereby enabling parameter-efficient fine-tuning
(PEFT) while preserving prior knowledge during adaptation. Our results show
that APB improves sample efficiency over standard RL and adapts to
out-of-distribution (OOD) tasks where existing meta-RL baselines typically
fail.

</details>


### [226] [Progressive Weight Loading: Accelerating Initial Inference and Gradually Boosting Performance on Resource-Constrained Environments](https://arxiv.org/abs/2509.22319)
*Hyunwoo Kim,Junha Lee,Mincheol Choi,Jeonghwan Lee,Jaeshin Cho*

Main category: cs.LG

TL;DR: 提出渐进式权重加载（PWL）技术解决深度学习模型加载和推理问题，实验证明其能在不牺牲初始推理速度下达到教师模型最终精度。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型变大变复杂，导致内存和计算需求高，影响移动和对延迟敏感环境下用户体验，知识蒸馏存在性能损失问题。

Method: 提出PWL技术，先部署轻量级学生模型，再逐步用教师模型层替换；引入训练方法，对齐学生和教师层中间特征表示并提升学生模型输出性能。

Result: 在VGG、ResNet和ViT架构实验中，用PWL训练的模型保持有竞争力的蒸馏性能，随教师层加载逐步提高准确率，达到教师模型最终精度且不影响初始推理速度。

Conclusion: PWL适合对响应性和性能要求高的动态、资源受限部署场景。

Abstract: Deep learning models have become increasingly large and complex, resulting in
higher memory consumption and computational demands. Consequently, model
loading times and initial inference latency have increased, posing significant
challenges in mobile and latency-sensitive environments where frequent model
loading and unloading are required, which directly impacts user experience.
While Knowledge Distillation (KD) offers a solution by compressing large
teacher models into smaller student ones, it often comes at the cost of reduced
performance. To address this trade-off, we propose Progressive Weight Loading
(PWL), a novel technique that enables fast initial inference by first deploying
a lightweight student model, then incrementally replacing its layers with those
of a pre-trained teacher model. To support seamless layer substitution, we
introduce a training method that not only aligns intermediate feature
representations between student and teacher layers, but also improves the
overall output performance of the student model. Our experiments on VGG,
ResNet, and ViT architectures demonstrate that models trained with PWL maintain
competitive distillation performance and gradually improve accuracy as teacher
layers are loaded-matching the final accuracy of the full teacher model without
compromising initial inference speed. This makes PWL particularly suited for
dynamic, resource-constrained deployments where both responsiveness and
performance are critical.

</details>


### [227] [Distributed Associative Memory via Online Convex Optimization](https://arxiv.org/abs/2509.22321)
*Bowen Wang,Matteo Zecchin,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 本文研究分布式环境下的关联记忆，提出分布式在线梯度下降方法，理论有次线性遗憾保证，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现代神经架构关联记忆操作需在分布式场景下实现，让代理维护本地关联记忆并获取他人信息。

Method: 引入分布式在线梯度下降方法，通过路由树通信优化不同代理的本地关联记忆。

Result: 理论分析有次线性遗憾保证，实验中提出的协议始终优于现有在线优化基线。

Conclusion: 提出的分布式在线梯度下降方法在分布式关联记忆场景有效且表现出色。

Abstract: An associative memory (AM) enables cue-response recall, and associative
memorization has recently been noted to underlie the operation of modern neural
architectures such as Transformers. This work addresses a distributed setting
where agents maintain a local AM to recall their own associations as well as
selective information from others. Specifically, we introduce a distributed
online gradient descent method that optimizes local AMs at different agents
through communication over routing trees. Our theoretical analysis establishes
sublinear regret guarantees, and experiments demonstrate that the proposed
protocol consistently outperforms existing online optimization baselines.

</details>


### [228] [Spectral Collapse Drives Loss of Plasticity in Deep Continual Learning](https://arxiv.org/abs/2509.22335)
*Naicheng He,Kaicheng Guo,Arjun Prakash,Saket Tiwari,Ruo Yu Tao,Tyrone Serapio,Amy Greenwald,George Konidaris*

Main category: cs.LG

TL;DR: 研究深度神经网络在深度持续学习中可塑性丧失原因，提出新指标，给出正则化增强方法，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 探究深度神经网络在深度持续学习中出现可塑性丧失，不重新初始化参数就无法学习新任务的原因。

Method: 引入tau - trainability概念，提出Kronecker因子分解近似Hessian，给出保持高有效特征秩和应用L2惩罚两项正则化增强方法。

Result: 在持续监督和强化学习任务的实验中，结合两个正则化器能有效保持可塑性。

Conclusion: 结合保持高有效特征秩和应用L2惩罚这两个正则化器可有效解决深度神经网络在持续学习中的可塑性丧失问题。

Abstract: We investigate why deep neural networks suffer from \emph{loss of plasticity}
in deep continual learning, failing to learn new tasks without reinitializing
parameters. We show that this failure is preceded by Hessian spectral collapse
at new-task initialization, where meaningful curvature directions vanish and
gradient descent becomes ineffective. To characterize the necessary condition
for successful training, we introduce the notion of $\tau$-trainability and
show that current plasticity preserving algorithms can be unified under this
framework. Targeting spectral collapse directly, we then discuss the Kronecker
factored approximation of the Hessian, which motivates two regularization
enhancements: maintaining high effective feature rank and applying $L2$
penalties. Experiments on continual supervised and reinforcement learning tasks
confirm that combining these two regularizers effectively preserves plasticity.

</details>


### [229] [SurvDiff: A Diffusion Model for Generating Synthetic Data in Survival Analysis](https://arxiv.org/abs/2509.22352)
*Marie Brockschmidt,Maresa Schröder,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 本文提出SurvDiff，一种专为生存分析生成合成数据的端到端扩散模型，在多个医疗数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 生存数据因缺失事件信息，给合成数据生成带来挑战，临床研究需忠实地重现事件时间分布和删失机制。

Method: 提出SurvDiff模型，通过联合生成混合类型协变量、事件时间和右删失，以针对生存分析定制的损失函数为引导。

Result: 在多个医疗数据集上，SurvDiff在分布保真度和下游评估指标上始终优于最先进的生成基线。

Conclusion: SurvDiff是首个专为生成合成生存数据设计的扩散模型。

Abstract: Survival analysis is a cornerstone of clinical research by modeling
time-to-event outcomes such as metastasis, disease relapse, or patient death.
Unlike standard tabular data, survival data often come with incomplete event
information due to dropout, or loss to follow-up. This poses unique challenges
for synthetic data generation, where it is crucial for clinical research to
faithfully reproduce both the event-time distribution and the censoring
mechanism. In this paper, we propose SurvDiff, an end-to-end diffusion model
specifically designed for generating synthetic data in survival analysis.
SurvDiff is tailored to capture the data-generating mechanism by jointly
generating mixed-type covariates, event times, and right-censoring, guided by a
survival-tailored loss function. The loss encodes the time-to-event structure
and directly optimizes for downstream survival tasks, which ensures that
SurvDiff (i) reproduces realistic event-time distributions and (ii) preserves
the censoring mechanism. Across multiple datasets, we show that \survdiff
consistently outperforms state-of-the-art generative baselines in both
distributional fidelity and downstream evaluation metrics across multiple
medical datasets. To the best of our knowledge, SurvDiff is the first diffusion
model explicitly designed for generating synthetic survival data.

</details>


### [230] [Context and Diversity Matter: The Emergence of In-Context Learning in World Models](https://arxiv.org/abs/2509.22353)
*Fan Wang,Zhiyuan Chen,Yuxuan Zhong,Sunjian Zheng,Pengtao Shao,Bo Yu,Shaoshan Liu,Jianan Wang,Ning Ding,Yang Cao,Yu Kang*

Main category: cs.LG

TL;DR: 本文研究上下文环境学习（ICEL），提出三项贡献，证明自适应世界模型潜力并指出ICEL出现的关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有预测环境动态的方法基于静态世界模型，在面对新的或罕见配置时会失败，因此研究ICEL。

Method: 形式化世界模型的上下文学习，确定环境识别和环境学习两个核心机制；推导两个机制的误差上界；通过实验验证世界模型中存在不同的ICL机制。

Result: 实验证实世界模型中存在不同的ICL机制，且数据分布和模型架构对ICL的影响与理论一致。

Conclusion: 证明了自适应世界模型的潜力，强调了ICEL出现的关键因素，特别是长上下文和多样化环境的必要性。

Abstract: The capability of predicting environmental dynamics underpins both biological
neural systems and general embodied AI in adapting to their surroundings. Yet
prevailing approaches rest on static world models that falter when confronted
with novel or rare configurations. We investigate in-context environment
learning (ICEL), shifting attention from zero-shot performance to the growth
and asymptotic limits of the world model. Our contributions are three-fold: (1)
we formalize in-context learning of a world model and identify two core
mechanisms: environment recognition and environment learning; (2) we derive
error upper-bounds for both mechanisms that expose how the mechanisms emerge;
and (3) we empirically confirm that distinct ICL mechanisms exist in the world
model, and we further investigate how data distribution and model architecture
affect ICL in a manner consistent with theory. These findings demonstrate the
potential of self-adapting world models and highlight the key factors behind
the emergence of ICEL, most notably the necessity of long context and diverse
environments.

</details>


### [231] [Stochastic activations](https://arxiv.org/abs/2509.22358)
*Maria Lomeli,Matthijs Douze,Gergely Szilvasy,Loic Cabannes,Jade Copet,Sainbayar Sukhbaatar,Jason Weston,Gabriel Synnaeve,Pierre-Emmanuel Mazaré,Hervé Jégou*

Main category: cs.LG

TL;DR: 介绍随机激活策略，在大语言模型前馈层随机选非线性函数，用两种方式利用该策略获良好效果。


<details>
  <summary>Details</summary>
Motivation: 解决RELU在负输入时恒定形状阻碍梯度流的优化问题，提高模型性能和生成文本多样性。

Method: 在大语言模型前馈层基于伯努利抽样在SILU和RELU间随机选择；一是预训练用随机激活，微调及推理用RELU；二是评估随机激活用于文本生成。

Result: 预训练用随机激活再微调，推理时比直接用RELU从头训练效果好，减少推理FLOPs，加速CPU；用于生成时稍逊于SILU结合温度缩放，但能增加生成文本多样性。

Conclusion: 随机激活策略能解决RELU优化问题，在推理速度和文本生成多样性上有优势，是现有策略的替代方案。

Abstract: We introduce stochastic activations. This novel strategy randomly selects
between several non-linear functions in the feed-forward layer of a large
language model. In particular, we choose between SILU or RELU depending on a
Bernoulli draw. This strategy circumvents the optimization problem associated
with RELU, namely, the constant shape for negative inputs that prevents the
gradient flow. We leverage this strategy in two ways:
  (1) We use stochastic activations during pre-training and fine-tune the model
with RELU, which is used at inference time to provide sparse latent vectors.
This reduces the inference FLOPs and translates into a significant speedup in
the CPU. Interestingly, this leads to much better results than training from
scratch with the RELU activation function.
  (2) We evaluate stochastic activations for generation. This strategy performs
reasonably well: it is only slightly inferior to the best deterministic
non-linearity, namely SILU combined with temperature scaling. This offers an
alternative to existing strategies by providing a controlled way to increase
the diversity of the generated text.

</details>


### [232] [Neural Feature Geometry Evolves as Discrete Ricci Flow](https://arxiv.org/abs/2509.22362)
*Moritz Hehl,Max von Renesse,Melanie Weber*

Main category: cs.LG

TL;DR: 本文从离散几何角度研究神经网络特征几何，发现其与离散Ricci流相似，提出评估框架并给出实用设计原则。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络在各领域取得成功，但对神经特征表示的理解仍不完整，因此从离散几何角度研究神经特征几何。

Method: 用几何图近似未观测的输入数据流形，提供训练中这些图演变的理论结果，进行超20000个前馈神经网络实验。

Result: 发现非线性激活在塑造特征几何中起关键作用，几何变换类似于离散Ricci流，类可分性出现对应图表示中社区结构出现。

Conclusion: 提出局部评估几何变换的新框架，给出几何感知的提前停止启发式方法和选择网络深度的标准等实用设计原则。

Abstract: Deep neural networks learn feature representations via complex geometric
transformations of the input data manifold. Despite the models' empirical
success across domains, our understanding of neural feature representations is
still incomplete. In this work we investigate neural feature geometry through
the lens of discrete geometry. Since the input data manifold is typically
unobserved, we approximate it using geometric graphs that encode local
similarity structure. We provide theoretical results on the evolution of these
graphs during training, showing that nonlinear activations play a crucial role
in shaping feature geometry in feedforward neural networks. Moreover, we
discover that the geometric transformations resemble a discrete Ricci flow on
these graphs, suggesting that neural feature geometry evolves analogous to
Ricci flow. This connection is supported by experiments on over 20,000
feedforward neural networks trained on binary classification tasks across both
synthetic and real-world datasets. We observe that the emergence of class
separability corresponds to the emergence of community structure in the
associated graph representations, which is known to relate to discrete Ricci
flow dynamics. Building on these insights, we introduce a novel framework for
locally evaluating geometric transformations through comparison with discrete
Ricci flow dynamics. Our results suggest practical design principles, including
a geometry-informed early-stopping heuristic and a criterion for selecting
network depth.

</details>


### [233] [Investigating Faithfulness in Large Audio Language Models](https://arxiv.org/abs/2509.22363)
*Lovenya Jain,Pooneh Mousavi,Mirco Ravanelli,Cem Subakan*

Main category: cs.LG

TL;DR: 研究大型音频语言模型（LALMs）链式思维（CoT）表征的忠实性，实验表明LALMs产生的CoT通常忠实于其决策过程。


<details>
  <summary>Details</summary>
Motivation: 此前工作显示文本大模型的CoT常不忠实，而LALMs的CoT忠实性在安全敏感应用中至关重要，且未被探究，同时LALMs推理更具挑战。

Method: 在SAKURA和MMAR两个具有挑战性的推理数据集上，对多个LALMs产生的CoT应用针对性干预，包括释义、填充标记注入、提前回答和引入错误。

Result: 经过在多个数据集和任务上的干预实验，LALMs产生的CoT通常忠实于其潜在决策过程。

Conclusion: LALMs产生的CoT一般能忠实反映其决策过程。

Abstract: Faithfulness measures whether chain-of-thought (CoT) representations
accurately reflect a model's decision process and can be used as reliable
explanations. Prior work has shown that CoTs from text-based LLMs are often
unfaithful. This question has not been explored for large audio-language models
(LALMs), where faithfulness is critical for safety-sensitive applications.
Reasoning in LALMs is also more challenging, as models must first extract
relevant clues from audio before reasoning over them. In this paper, we
investigate the faithfulness of CoTs produced by several LALMs by applying
targeted interventions, including paraphrasing, filler token injection, early
answering, and introducing mistakes, on two challenging reasoning datasets:
SAKURA and MMAR. After going through the aforementioned interventions across
several datasets and tasks, our experiments suggest that, LALMs generally
produce CoTs that appear to be faithful to their underlying decision processes.

</details>


### [234] [Enhancing Credit Risk Prediction: A Meta-Learning Framework Integrating Baseline Models, LASSO, and ECOC for Superior Accuracy](https://arxiv.org/abs/2509.22381)
*Haibo Wang,Lutfu S. Sua,Jun Huang,Figen Balo,Burak Dolar*

Main category: cs.LG

TL;DR: 本文提出综合元学习框架用于信用风险管理，结合多种模型，经实证验证可提升金融实体分类和违约概率估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在信用风险评估中面临高维数据、可解释性差、稀有事件检测和多类不平衡等问题，需要更好的模型。

Method: 提出综合元学习框架，融合监督学习、无监督方法、深度学习架构，用LASSO进行特征选择和降维，用纠错输出码处理多类不平衡，用排列特征重要性分析增强透明度。

Result: 对2029家美国上市公司的企业信用评级数据集进行实证分析，表明该框架显著提高了金融实体分类和违约概率估计的准确性。

Conclusion: 该研究通过解决信用风险建模的三个基本挑战，有助于开发更准确可靠的计算模型，为战略金融决策提供支持。

Abstract: Effective credit risk management is fundamental to financial decision-making,
necessitating robust models for default probability prediction and financial
entity classification. Traditional machine learning approaches face significant
challenges when confronted with high-dimensional data, limited
interpretability, rare event detection, and multi-class imbalance problems in
risk assessment. This research proposes a comprehensive meta-learning framework
that synthesizes multiple complementary models: supervised learning algorithms,
including XGBoost, Random Forest, Support Vector Machine, and Decision Tree;
unsupervised methods such as K-Nearest Neighbors; deep learning architectures
like Multilayer Perceptron; alongside LASSO regularization for feature
selection and dimensionality reduction; and Error-Correcting Output Codes as a
meta-classifier for handling imbalanced multi-class problems. We implement
Permutation Feature Importance analysis for each prediction class across all
constituent models to enhance model transparency. Our framework aims to
optimize predictive performance while providing a more holistic approach to
credit risk assessment. This research contributes to the development of more
accurate and reliable computational models for strategic financial decision
support by addressing three fundamental challenges in credit risk modeling. The
empirical validation of our approach involves an analysis of the Corporate
Credit Ratings dataset with credit ratings for 2,029 publicly listed US
companies. Results demonstrate that our meta-learning framework significantly
enhances the accuracy of financial entity classification regarding credit
rating migrations (upgrades and downgrades) and default probability estimation.

</details>


### [235] [(Sometimes) Less is More: Mitigating the Complexity of Rule-based Representation for Interpretable Classification](https://arxiv.org/abs/2509.22384)
*Luca Bergamin,Roberto Confalonieri,Fabio Aiolli*

Main category: cs.LG

TL;DR: 本文将可微的L0正则近似应用于多层逻辑感知机（MLLP），研究其在降低概念规则集（CRS）复杂度并保持性能方面的效果，还与随机二值化等启发式方法比较，讨论了CRS复杂度与性能的权衡。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络不易解释，而模型透明度和可解释性在多个场景是关键要求，因此研究降低可解释版本复杂度并保持性能的方法。

Method: 将可微的L0正则近似应用于MLLP，与随机二值化等启发式方法比较。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及具体结论，仅表明会讨论CRS复杂度和性能的权衡。

Abstract: Deep neural networks are widely used in practical applications of AI,
however, their inner structure and complexity made them generally not easily
interpretable. Model transparency and interpretability are key requirements for
multiple scenarios where high performance is not enough to adopt the proposed
solution. In this work, a differentiable approximation of $L_0$ regularization
is adapted into a logic-based neural network, the Multi-layer Logical
Perceptron (MLLP), to study its efficacy in reducing the complexity of its
discrete interpretable version, the Concept Rule Set (CRS), while retaining its
performance. The results are compared to alternative heuristics like Random
Binarization of the network weights, to determine if better results can be
achieved when using a less-noisy technique that sparsifies the network based on
the loss function instead of a random distribution. The trade-off between the
CRS complexity and its performance is discussed.

</details>


### [236] [Improving accuracy in short mortality rate series: Exploring Multi-step Forecasting Approaches in Hybrid Systems](https://arxiv.org/abs/2509.22395)
*Filipe C. L. Duarte,Paulo S. G. de Mattos Neto,Paulo R. A. Firmino*

Main category: cs.LG

TL;DR: 研究评估不同多步预测方法和机器学习模型对混合系统死亡率预测准确性的影响，发现ARIMA - LSTM递归混合模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 利率下降和经济稳定使准确的死亡率预测在保险和养老金市场愈发重要，多步预测面临数据有限挑战，混合系统是有前景的解决方案。

Method: 评估不同多步预测方法（递归、直接、多输入多输出）和机器学习模型对混合系统准确性的影响。

Result: 对12个数据集和21个模型的研究表明，多步方法和机器学习模型的选择对提高性能至关重要，ARIMA - LSTM递归混合模型在多数情况下表现优于其他模型。

Conclusion: 多步预测方法和机器学习模型的选择影响混合系统性能，ARIMA - LSTM递归混合模型是较好选择。

Abstract: The decline in interest rates and economic stabilization has heightened the
importance of accurate mortality rate forecasting, particularly in insurance
and pension markets. Multi-step-ahead predictions are crucial for public
health, demographic planning, and insurance risk assessments; however, they
face challenges when data are limited. Hybrid systems that combine statistical
and Machine Learning (ML) models offer a promising solution for handling both
linear and nonlinear patterns. This study evaluated the impact of different
multi-step forecasting approaches (Recursive, Direct, and Multi-Input
Multi-Output) and ML models on the accuracy of hybrid systems. Results from 12
datasets and 21 models show that the selection of both the multi-step approach
and the ML model is essential for improving performance, with the ARIMA-LSTM
hybrid using a recursive approach outperforming other models in most cases.

</details>


### [237] [ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation](https://arxiv.org/abs/2509.22402)
*Nan Tang,Jing-Cheng Pang,Guanlin Li,Chao Qian,Yang Yu*

Main category: cs.LG

TL;DR: 提出ReLAM框架解决视觉强化学习中奖励设计瓶颈，实验显示其加速学习且性能优越。


<details>
  <summary>Details</summary>
Motivation: 奖励设计是视觉强化学习中机器人操作的关键瓶颈，现实中难以获取精确位置信息来设计奖励。

Method: 通过图像提取关键点隐式推断空间距离，提出ReLAM框架，学习预测模型提出子目标，在HRL框架下训练策略。

Result: 在复杂、长周期操作任务实验中，ReLAM显著加速学习，性能优于现有方法。

Conclusion: ReLAM能有效解决视觉强化学习奖励设计问题，提升学习效率和性能。

Abstract: Reward design remains a critical bottleneck in visual reinforcement learning
(RL) for robotic manipulation. In simulated environments, rewards are
conventionally designed based on the distance to a target position. However,
such precise positional information is often unavailable in real-world visual
settings due to sensory and perceptual limitations. In this study, we propose a
method that implicitly infers spatial distances through keypoints extracted
from images. Building on this, we introduce Reward Learning with Anticipation
Model (ReLAM), a novel framework that automatically generates dense, structured
rewards from action-free video demonstrations. ReLAM first learns an
anticipation model that serves as a planner and proposes intermediate
keypoint-based subgoals on the optimal path to the final goal, creating a
structured learning curriculum directly aligned with the task's geometric
objectives. Based on the anticipated subgoals, a continuous reward signal is
provided to train a low-level, goal-conditioned policy under the hierarchical
reinforcement learning (HRL) framework with provable sub-optimality bound.
Extensive experiments on complex, long-horizon manipulation tasks show that
ReLAM significantly accelerates learning and achieves superior performance
compared to state-of-the-art methods.

</details>


### [238] [MoveFM-R: Advancing Mobility Foundation Models via Language-driven Semantic Reasoning](https://arxiv.org/abs/2509.22403)
*Fanjin Meng,Yuan Yuan,Jingtao Ding,Jie Feng,Chonghua Han,Yong Li*

Main category: cs.LG

TL;DR: 提出MoveFM - R框架结合MFMs和LLMs优势，解决现有问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: MFMs受数据规模和语义理解限制，LLMs缺乏时空统计理解，需结合两者优势解决人类移动模式建模问题。

Method: 提出MoveFM - R框架，有语义增强位置编码、渐进课程和交互式自我反思机制三个核心创新。

Result: MoveFM - R显著优于现有基于MFM和LLM的基线，在零样本设置中泛化性强，能根据自然语言指令生成真实轨迹。

Conclusion: MoveFM - R开创了人类移动性建模新范式，实现更全面、可解释和强大的建模。

Abstract: Mobility Foundation Models (MFMs) have advanced the modeling of human
movement patterns, yet they face a ceiling due to limitations in data scale and
semantic understanding. While Large Language Models (LLMs) offer powerful
semantic reasoning, they lack the innate understanding of spatio-temporal
statistics required for generating physically plausible mobility trajectories.
To address these gaps, we propose MoveFM-R, a novel framework that unlocks the
full potential of mobility foundation models by leveraging language-driven
semantic reasoning capabilities. It tackles two key challenges: the vocabulary
mismatch between continuous geographic coordinates and discrete language
tokens, and the representation gap between the latent vectors of MFMs and the
semantic world of LLMs. MoveFM-R is built on three core innovations: a
semantically enhanced location encoding to bridge the geography-language gap, a
progressive curriculum to align the LLM's reasoning with mobility patterns, and
an interactive self-reflection mechanism for conditional trajectory generation.
Extensive experiments demonstrate that MoveFM-R significantly outperforms
existing MFM-based and LLM-based baselines. It also shows robust generalization
in zero-shot settings and excels at generating realistic trajectories from
natural language instructions. By synthesizing the statistical power of MFMs
with the deep semantic understanding of LLMs, MoveFM-R pioneers a new paradigm
that enables a more comprehensive, interpretable, and powerful modeling of
human mobility. The implementation of MoveFM-R is available online at
https://anonymous.4open.science/r/MoveFM-R-CDE7/.

</details>


### [239] [Fast-Forward Lattice Boltzmann: Learning Kinetic Behaviour with Physics-Informed Neural Operators](https://arxiv.org/abs/2509.22411)
*Xiao Xue,Marco F. P. ten Eikelder,Mingyang Gao,Xiaoyuan Cheng,Yiming Yang,Yi He,Shuo Wang,Sibo Cheng,Yukun Hu,Peter V. Coveney*

Main category: cs.LG

TL;DR: 提出用于格子玻尔兹曼方程（LBE）的物理信息神经算子框架，可长时预测，有离散不变性和碰撞模型无关性，在复杂流场景中表现稳健。


<details>
  <summary>Details</summary>
Motivation: 解决LBE数值求解因碰撞核的时间步长限制导致计算量大的问题。

Method: 引入物理信息神经算子框架，纳入LBE的本征矩匹配约束和全分布场的全局等变性。

Result: 框架在包括冯卡门涡街、韧带破裂和气泡粘附等复杂流场景中表现出稳健性。

Conclusion: 为动力学系统建模建立了新的数据驱动途径。

Abstract: The lattice Boltzmann equation (LBE), rooted in kinetic theory, provides a
powerful framework for capturing complex flow behaviour by describing the
evolution of single-particle distribution functions (PDFs). Despite its
success, solving the LBE numerically remains computationally intensive due to
strict time-step restrictions imposed by collision kernels. Here, we introduce
a physics-informed neural operator framework for the LBE that enables
prediction over large time horizons without step-by-step integration,
effectively bypassing the need to explicitly solve the collision kernel. We
incorporate intrinsic moment-matching constraints of the LBE, along with global
equivariance of the full distribution field, enabling the model to capture the
complex dynamics of the underlying kinetic system. Our framework is
discretization-invariant, enabling models trained on coarse lattices to
generalise to finer ones (kinetic super-resolution). In addition, it is
agnostic to the specific form of the underlying collision model, which makes it
naturally applicable across different kinetic datasets regardless of the
governing dynamics. Our results demonstrate robustness across complex flow
scenarios, including von Karman vortex shedding, ligament breakup, and bubble
adhesion. This establishes a new data-driven pathway for modelling kinetic
systems.

</details>


### [240] [One Prompt Fits All: Universal Graph Adaptation for Pretrained Models](https://arxiv.org/abs/2509.22416)
*Yongqi Huang,Jitao Zhao,Dongxiao He,Xiaobao Wang,Yawen Li,Yuxiao Huang,Di Jin,Zhiyong Feng*

Main category: cs.LG

TL;DR: 现有图提示学习（GPL）有效性和原理不明，存在机制缺乏共识、场景适应性有限问题，提出UniPrompt方法，实验表明其能与多种预训练模型集成且表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决现有GPL研究中缺乏对潜在机制的共识以及场景适应性有限的问题。

Method: 理论分析现有GPL方法，提出UniPrompt方法，能适配任何预训练模型，释放预训练模型能力并保留输入图结构。

Result: 该方法能有效集成各种预训练模型，在域内和跨域场景中取得良好性能。

Conclusion: UniPrompt方法可有效解决现有GPL的问题，提升了模型在不同场景的性能。

Abstract: Graph Prompt Learning (GPL) has emerged as a promising paradigm that bridges
graph pretraining models and downstream scenarios, mitigating label dependency
and the misalignment between upstream pretraining and downstream tasks.
Although existing GPL studies explore various prompt strategies, their
effectiveness and underlying principles remain unclear. We identify two
critical limitations: (1) Lack of consensus on underlying mechanisms: Despite
current GPLs have advanced the field, there is no consensus on how prompts
interact with pretrained models, as different strategies intervene at varying
spaces within the model, i.e., input-level, layer-wise, and
representation-level prompts. (2) Limited scenario adaptability: Most methods
fail to generalize across diverse downstream scenarios, especially under data
distribution shifts (e.g., homophilic-to-heterophilic graphs). To address these
issues, we theoretically analyze existing GPL approaches and reveal that
representation-level prompts essentially function as fine-tuning a simple
downstream classifier, proposing that graph prompt learning should focus on
unleashing the capability of pretrained models, and the classifier adapts to
downstream scenarios. Based on our findings, we propose UniPrompt, a novel GPL
method that adapts any pretrained models, unleashing the capability of
pretrained models while preserving the structure of the input graph. Extensive
experiments demonstrate that our method can effectively integrate with various
pretrained models and achieve strong performance across in-domain and
cross-domain scenarios.

</details>


### [241] [Partial Parameter Updates for Efficient Distributed Training](https://arxiv.org/abs/2509.22418)
*Anastasiia Filippova,Angelos Katharopoulos,David Grangier,Ronan Collobert*

Main category: cs.LG

TL;DR: 提出高效低通信分布式训练方法，限制反向传播提升效率，实验证明可降低训练FLOPs和峰值内存。


<details>
  <summary>Details</summary>
Motivation: 现有低通信分布式训练方法效率有提升空间，需减少内存使用和训练FLOPs。

Method: 限制反向传播，每个节点在本地步骤仅更新固定子集参数，其余保持冻结，对所有参数进行完整前向传播。

Result: 在32节点上训练13亿参数语言模型实验中，在相同令牌和带宽预算下，与先前低通信方法困惑度相当，降低训练FLOPs和峰值内存。

Conclusion: 所提方法在低通信分布式训练中有效，能提升效率并减少资源使用。

Abstract: We introduce a memory- and compute-efficient method for low-communication
distributed training. Existing methods reduce communication by performing
multiple local updates between infrequent global synchronizations. We
demonstrate that their efficiency can be significantly improved by restricting
backpropagation: instead of updating all the parameters, each node updates only
a fixed subset while keeping the remainder frozen during local steps. This
constraint substantially reduces peak memory usage and training FLOPs, while a
full forward pass over all parameters eliminates the need for cross-node
activation exchange. Experiments on a $1.3$B-parameter language model trained
across $32$ nodes show that our method matches the perplexity of prior
low-communication approaches under identical token and bandwidth budgets while
reducing training FLOPs and peak memory.

</details>


### [242] [The Flood Complex: Large-Scale Persistent Homology on Millions of Points](https://arxiv.org/abs/2509.22432)
*Florian Graf,Paolo Pellizzoni,Martin Uray,Stefan Huber,Roland Kwitt*

Main category: cs.LG

TL;DR: 为解决大规模欧氏点云数据持久同调计算难题，引入Flood复形，可高效计算，有理论特性且支持GPU并行，实验证明其在性能上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在大规模欧氏点云数据持久同调计算中存在计算量过大的问题，限制了其应用。

Method: 引入Flood复形，在给定过滤值下，包含点云小子集Delaunay三角剖分中被半径为r的球完全覆盖的单形。

Result: 可对数百万点的3D点云数据计算高达二维的持久同调，在物体分类性能上优于其他基于持久同调的方法和神经网络。

Conclusion: Flood复形解决了大规模欧氏点云数据持久同调计算难题，在处理复杂几何或拓扑对象时表现出色。

Abstract: We consider the problem of computing persistent homology (PH) for large-scale
Euclidean point cloud data, aimed at downstream machine learning tasks, where
the exponential growth of the most widely-used Vietoris-Rips complex imposes
serious computational limitations. Although more scalable alternatives such as
the Alpha complex or sparse Rips approximations exist, they often still result
in a prohibitively large number of simplices. This poses challenges in the
complex construction and in the subsequent PH computation, prohibiting their
use on large-scale point clouds. To mitigate these issues, we introduce the
Flood complex, inspired by the advantages of the Alpha and Witness complex
constructions. Informally, at a given filtration value $r\geq 0$, the Flood
complex contains all simplices from a Delaunay triangulation of a small subset
of the point cloud $X$ that are fully covered by balls of radius $r$ emanating
from $X$, a process we call flooding. Our construction allows for efficient PH
computation, possesses several desirable theoretical properties, and is
amenable to GPU parallelization. Scaling experiments on 3D point cloud data
show that we can compute PH of up to dimension 2 on several millions of points.
Importantly, when evaluating object classification performance on real-world
and synthetic data, we provide evidence that this scaling capability is needed,
especially if objects are geometrically or topologically complex, yielding
performance superior to other PH-based methods and neural networks for point
cloud data.

</details>


### [243] [Bridging Kolmogorov Complexity and Deep Learning: Asymptotically Optimal Description Length Objectives for Transformers](https://arxiv.org/abs/2509.22445)
*Peter Shaw,James Cohan,Jacob Eisenstein,Kristina Toutanova*

Main category: cs.LG

TL;DR: 本文引入基于Kolmogorov复杂度理论的渐近最优描述长度目标概念，证明Transformer存在此类目标，构建变分目标并分析，实证分析显示其在算法任务上有优势，指出优化挑战，为训练神经网络提供理论框架。


<details>
  <summary>Details</summary>
Motivation: 解决Minimum Description Length (MDL)原则应用于Transformer等神经网络时缺乏对模型复杂性的原则性、通用度量的问题。

Method: 引入渐近最优描述长度目标概念，证明Transformer存在此类目标，构建基于自适应高斯混合先验的变分目标并分析。

Result: 变分目标在算法任务上能选择低复杂度且泛化性强的解决方案，但标准优化器从随机初始化开始无法找到此类解决方案。

Conclusion: 为识别具有强渐近保证的描述长度目标提供理论框架，为训练实现更高压缩和泛化能力的神经网络指明潜在路径。

Abstract: The Minimum Description Length (MDL) principle offers a formal framework for
applying Occam's razor in machine learning. However, its application to neural
networks such as Transformers is challenging due to the lack of a principled,
universal measure for model complexity. This paper introduces the theoretical
notion of asymptotically optimal description length objectives, grounded in the
theory of Kolmogorov complexity. We establish that a minimizer of such an
objective achieves optimal compression, for any dataset, up to an additive
constant, in the limit as model resource bounds increase. We prove that
asymptotically optimal objectives exist for Transformers, building on a new
demonstration of their computational universality. We further show that such
objectives can be tractable and differentiable by constructing and analyzing a
variational objective based on an adaptive Gaussian mixture prior. Our
empirical analysis shows that this variational objective selects for a
low-complexity solution with strong generalization on an algorithmic task, but
standard optimizers fail to find such solutions from a random initialization,
highlighting key optimization challenges. More broadly, by providing a
theoretical framework for identifying description length objectives with strong
asymptotic guarantees, we outline a potential path towards training neural
networks that achieve greater compression and generalization.

</details>


### [244] [Overclocking Electrostatic Generative Models](https://arxiv.org/abs/2509.22454)
*Daniil Shlenskii,Alexander Korotin*

Main category: cs.LG

TL;DR: 提出Inverse Poisson Flow Matching (IPFM)加速静电生成模型，蒸馏生成器样本质量接近或超教师模型。


<details>
  <summary>Details</summary>
Motivation: 静电生成模型PFGM++与扩散模型类似，依赖昂贵ODE模拟，计算成本高，需加速方法。

Method: 提出IPFM，将蒸馏重新表述为逆问题，推导可处理训练目标。

Result: IPFM生成的蒸馏生成器仅用少量函数评估，样本质量接近或超教师模型，有限D时蒸馏收敛更快。

Conclusion: IPFM能有效加速静电生成模型，有限D时优势更明显。

Abstract: Electrostatic generative models such as PFGM++ have recently emerged as a
powerful framework, achieving state-of-the-art performance in image synthesis.
PFGM++ operates in an extended data space with auxiliary dimensionality $D$,
recovering the diffusion model framework as $D\to\infty$, while yielding
superior empirical results for finite $D$. Like diffusion models, PFGM++ relies
on expensive ODE simulations to generate samples, making it computationally
costly. To address this, we propose Inverse Poisson Flow Matching (IPFM), a
novel distillation framework that accelerates electrostatic generative models
across all values of $D$. Our IPFM reformulates distillation as an inverse
problem: learning a generator whose induced electrostatic field matches that of
the teacher. We derive a tractable training objective for this problem and show
that, as $D \to \infty$, our IPFM closely recovers Score Identity Distillation
(SiD), a recent method for distilling diffusion models. Empirically, our IPFM
produces distilled generators that achieve near-teacher or even superior sample
quality using only a few function evaluations. Moreover, we observe that
distillation converges faster for finite $D$ than in the $D \to \infty$
(diffusion) limit, which is consistent with prior findings that finite-$D$
PFGM++ models exhibit more favorable optimization and sampling properties.

</details>


### [245] [Physics-informed GNN for medium-high voltage AC power flow with edge-aware attention and line search correction operator](https://arxiv.org/abs/2509.22458)
*Changhun Kim,Timon Conrad,Redwanul Karim,Julian Oelhaf,David Riebesel,Tomás Arias-Vergara,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 提出PIGNN - Attn - LS解决现有PIGNNs精度和推理问题，在测试中表现优于基线且推理更快。


<details>
  <summary>Details</summary>
Motivation: 现有PIGNNs在同等速度下需提高精度，物理损失在推理时无效阻碍应用。

Method: 结合边缘感知注意力机制和回溯线搜索全局校正算子，用现实场景生成器训练测试。

Result: 在4 - 32总线电网的测试中，电压RMSE为0.00033 p.u.，角度为0.08°，优于基线；在4 - 1024总线电网推理比NR快2 - 5倍。

Conclusion: PIGNN - Attn - LS能有效提高PIGNNs精度和推理效率。

Abstract: Physics-informed graph neural networks (PIGNNs) have emerged as fast AC
power-flow solvers that can replace classic Newton--Raphson (NR) solvers,
especially when thousands of scenarios must be evaluated. However, current
PIGNNs still need accuracy improvements at parity speed; in particular, the
physics loss is inoperative at inference, which can deter operational adoption.
We address this with PIGNN-Attn-LS, combining an edge-aware attention mechanism
that explicitly encodes line physics via per-edge biases, capturing the grid's
anisotropy, with a backtracking line-search-based globalized correction
operator that restores an operative decrease criterion at inference. Training
and testing use a realistic High-/Medium-Voltage scenario generator, with NR
used only to construct reference states. On held-out HV cases consisting of
4--32-bus grids, PIGNN-Attn-LS achieves a test RMSE of 0.00033 p.u. in voltage
and 0.08$^\circ$ in angle, outperforming the PIGNN-MLP baseline by 99.5\% and
87.1\%, respectively. With streaming micro-batches, it delivers 2--5$\times$
faster batched inference than NR on 4--1024-bus grids.

</details>


### [246] [Nonlinear Optimization with GPU-Accelerated Neural Network Constraints](https://arxiv.org/abs/2509.22462)
*Robert Parker,Oscar Dowson,Nicole LoGiudice,Manuel Garcia,Russell Bent*

Main category: cs.LG

TL;DR: 提出在GPU上评估输出和导数的神经网络简化空间优化公式，比全空间公式求解更快、迭代更少，并在两个问题上验证优势。


<details>
  <summary>Details</summary>
Motivation: 寻求更高效的训练神经网络优化方法。

Method: 将神经网络视为“灰盒”，提出简化空间公式，中间变量和约束不暴露给优化求解器。

Result: 简化空间公式在内部点法中求解更快、迭代更少。

Conclusion: 该方法在对抗生成和安全约束最优潮流两个优化问题上有优势。

Abstract: We propose a reduced-space formulation for optimizing over trained neural
networks where the network's outputs and derivatives are evaluated on a GPU. To
do this, we treat the neural network as a "gray box" where intermediate
variables and constraints are not exposed to the optimization solver. Compared
to the full-space formulation, in which intermediate variables and constraints
are exposed to the optimization solver, the reduced-space formulation leads to
faster solves and fewer iterations in an interior point method. We demonstrate
the benefits of this method on two optimization problems: Adversarial
generation for a classifier trained on MNIST images and security-constrained
optimal power flow with transient feasibility enforced using a neural network
surrogate.

</details>


### [247] [IIET: Efficient Numerical Transformer via Implicit Iterative Euler Method](https://arxiv.org/abs/2509.22463)
*Xinyu Liu,Bei Li,Jiahao Liu,Junhao Ruan,Kechen Jiao,Hongyin Tang,Jingang Wang,Xiao Tong,Jingbo Zhu*

Main category: cs.LG

TL;DR: 本文提出IIET简化高阶方法，引入IIAD平衡性能与效率，IIET在精度上有提升，E - IIET能大幅降低推理开销。


<details>
  <summary>Details</summary>
Motivation: 高阶数值方法提升Transformer性能但有性能 - 效率权衡问题，传统效率技术可能损害模型性能，需探索更优ODE - 基Transformer架构。

Method: 提出IIET用迭代隐式欧拉方法简化高阶方法，引入IIAD通过灵活阈值平衡性能与效率。

Result: 在lm - evaluation - harness上，IIET比普通Transformer平均精度提高2.65%，比PCformer高0.8%；E - IIET大幅削减推理开销55%，保留99.4%任务精度；最高效IIET变体比普通Transformer平均性能提升超1.6%且速度相当。

Conclusion: IIET和IIAD能有效提升Transformer性能并平衡性能与效率。

Abstract: High-order numerical methods enhance Transformer performance in tasks like
NLP and CV, but introduce a performance-efficiency trade-off due to increased
computational overhead. Our analysis reveals that conventional efficiency
techniques, such as distillation, can be detrimental to the performance of
these models, exemplified by PCformer. To explore more optimizable ODE-based
Transformer architectures, we propose the \textbf{I}terative \textbf{I}mplicit
\textbf{E}uler \textbf{T}ransformer \textbf{(IIET)}, which simplifies
high-order methods using an iterative implicit Euler approach. This
simplification not only leads to superior performance but also facilitates
model compression compared to PCformer. To enhance inference efficiency, we
introduce \textbf{I}teration \textbf{I}nfluence-\textbf{A}ware
\textbf{D}istillation \textbf{(IIAD)}. Through a flexible threshold, IIAD
allows users to effectively balance the performance-efficiency trade-off. On
lm-evaluation-harness, IIET boosts average accuracy by 2.65\% over vanilla
Transformers and 0.8\% over PCformer. Its efficient variant, E-IIET,
significantly cuts inference overhead by 55\% while retaining 99.4\% of the
original task accuracy. Moreover, the most efficient IIET variant achieves an
average performance gain exceeding 1.6\% over vanilla Transformer with
comparable speed.

</details>


### [248] [Learning the Neighborhood: Contrast-Free Multimodal Self-Supervised Molecular Graph Pretraining](https://arxiv.org/abs/2509.22468)
*Boshra Ariguib,Mathias Niepert,Andrei Manolache*

Main category: cs.LG

TL;DR: 提出C - FREE框架整合2D图与3D构象体集合学习分子表征，在MoleculeNet上达SOTA，预训练可有效迁移到新化学领域。


<details>
  <summary>Details</summary>
Motivation: 现有分子图自监督预训练方法存在依赖手工增强、复杂生成目标，且未充分利用3D结构信息的问题，需要高质量分子表征用于属性预测和分子设计。

Method: 引入C - FREE框架，用固定半径自我网络作为建模单元，在潜在空间通过互补邻域预测子图嵌入，在混合GNN - Transformer骨干中整合几何和拓扑信息。

Result: 在GEOM数据集上预训练，在MoleculeNet上取得SOTA结果，超越对比、生成和其他多模态自监督方法。

Conclusion: 预训练能有效迁移到新化学领域，凸显了3D信息分子表征的重要性。

Abstract: High-quality molecular representations are essential for property prediction
and molecular design, yet large labeled datasets remain scarce. While
self-supervised pretraining on molecular graphs has shown promise, many
existing approaches either depend on hand-crafted augmentations or complex
generative objectives, and often rely solely on 2D topology, leaving valuable
3D structural information underutilized. To address this gap, we introduce
C-FREE (Contrast-Free Representation learning on Ego-nets), a simple framework
that integrates 2D graphs with ensembles of 3D conformers. C-FREE learns
molecular representations by predicting subgraph embeddings from their
complementary neighborhoods in the latent space, using fixed-radius ego-nets as
modeling units across different conformers. This design allows us to integrate
both geometric and topological information within a hybrid Graph Neural Network
(GNN)-Transformer backbone, without negatives, positional encodings, or
expensive pre-processing. Pretraining on the GEOM dataset, which provides rich
3D conformational diversity, C-FREE achieves state-of-the-art results on
MoleculeNet, surpassing contrastive, generative, and other multimodal
self-supervised methods. Fine-tuning across datasets with diverse sizes and
molecule types further demonstrates that pretraining transfers effectively to
new chemical domains, highlighting the importance of 3D-informed molecular
representations.

</details>


### [249] [Bayesian Transfer Operators in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2509.22482)
*Septimus Boshoff,Sebastian Peitz,Stefan Klus*

Main category: cs.LG

TL;DR: 结合高斯过程方法与Koopman算子理论，解决基于核的Koopman算法的两个问题，并统一高斯过程回归和动态模态分解。


<details>
  <summary>Details</summary>
Motivation: Koopman算子理论与再生核希尔伯特空间结合的研究趋势下，解决基于核的Koopman算法存在的稀疏性和超参数优化及字典学习问题。

Method: 采用高斯过程方法。

Result: 减少计算需求，提高对传感器噪声的恢复能力。

Conclusion: 统一了高斯过程回归和动态模态分解。

Abstract: The Koopman operator, as a linear representation of a nonlinear dynamical
system, has been attracting attention in many fields of science. Recently,
Koopman operator theory has been combined with another concept that is popular
in data science: reproducing kernel Hilbert spaces. We follow this thread into
Gaussian process methods, and illustrate how these methods can alleviate two
pervasive problems with kernel-based Koopman algorithms. The first being
sparsity: most kernel methods do not scale well and require an approximation to
become practical. We show that not only can the computational demands be
reduced, but also demonstrate improved resilience against sensor noise. The
second problem involves hyperparameter optimization and dictionary learning to
adapt the model to the dynamical system. In summary, the main contribution of
this work is the unification of Gaussian process regression and dynamic mode
decomposition.

</details>


### [250] [OFMU: Optimization-Driven Framework for Machine Unlearning](https://arxiv.org/abs/2509.22483)
*Sadia Asif,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 为解决大语言模型机器遗忘问题，提出基于惩罚的双层优化框架OFMU，理论分析和实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在敏感应用中需具备不从头训练就能遗忘特定知识的能力，传统方法存在训练不稳定和模型效用下降问题。

Method: 提出基于惩罚的双层优化框架OFMU，通过内最大化和外最小化步骤，开发双循环算法。

Result: 理论上证明收敛性，分析收敛率；实验表明OFMU在遗忘效果和模型效用上均优于现有方法。

Conclusion: OFMU能更好地平衡遗忘效果和模型效用，在视觉和语言基准测试中表现出色。

Abstract: Large language models deployed in sensitive applications increasingly require
the ability to unlearn specific knowledge, such as user requests, copyrighted
materials, or outdated information, without retraining from scratch to ensure
regulatory compliance, user privacy, and safety. This task, known as machine
unlearning, aims to remove the influence of targeted data (forgetting) while
maintaining performance on the remaining data (retention). A common approach is
to formulate this as a multi-objective problem and reduce it to a
single-objective problem via scalarization, where forgetting and retention
losses are combined using a weighted sum. However, this often results in
unstable training dynamics and degraded model utility due to conflicting
gradient directions. To address these challenges, we propose OFMU, a
penalty-based bi-level optimization framework that explicitly prioritizes
forgetting while preserving retention through a hierarchical structure. Our
method enforces forgetting via an inner maximization step that incorporates a
similarity-aware penalty to decorrelate the gradients of the forget and
retention objectives, and restores utility through an outer minimization step.
To ensure scalability, we develop a two-loop algorithm with provable
convergence guarantees under both convex and non-convex regimes. We further
provide a rigorous theoretical analysis of convergence rates and show that our
approach achieves better trade-offs between forgetting efficacy and model
utility compared to prior methods. Extensive experiments across vision and
language benchmarks demonstrate that OFMU consistently outperforms existing
unlearning methods in both forgetting efficacy and retained utility.

</details>


### [251] [A Machine Learning Pipeline for Multiple Sclerosis Biomarker Discovery: Comparing explainable AI and Traditional Statistical Approaches](https://arxiv.org/abs/2509.22484)
*Samuele Punzo,Silvia Giulia Galfrè,Francesco Massafra,Alessandro Maglione,Corrado Priami,Alina Sîrbu*

Main category: cs.LG

TL;DR: 提出用于多发性硬化症生物标志物发现的机器学习流程，结合可解释AI与传统方法。


<details>
  <summary>Details</summary>
Motivation: 寻找多发性硬化症的生物标志物，深入了解疾病机制。

Method: 整合八个公开的外周血单个核细胞微阵列数据集，经预处理后用贝叶斯搜索优化XGBoost分类器，用SHAP识别关键特征，与经典差异表达分析结果对比。

Result: SHAP和DEA的生物标志物有重叠和独特部分，富集分析证实SHAP所选基因与已知和MS相关的通路有关。

Conclusion: 结合可解释AI和传统统计方法对深入了解疾病机制有价值。

Abstract: We present a machine learning pipeline for biomarker discovery in Multiple
Sclerosis (MS), integrating eight publicly available microarray datasets from
Peripheral Blood Mononuclear Cells (PBMC). After robust preprocessing we
trained an XGBoost classifier optimized via Bayesian search. SHapley Additive
exPlanations (SHAP) were used to identify key features for model prediction,
indicating thus possible biomarkers. These were compared with genes identified
through classical Differential Expression Analysis (DEA). Our comparison
revealed both overlapping and unique biomarkers between SHAP and DEA,
suggesting complementary strengths. Enrichment analysis confirmed the
biological relevance of SHAP-selected genes, linking them to pathways such as
sphingolipid signaling, Th1/Th2/Th17 cell differentiation, and Epstein-Barr
virus infection all known to be associated with MS. This study highlights the
value of combining explainable AI (xAI) with traditional statistical methods to
gain deeper insights into disease mechanism.

</details>


### [252] [Dual Optimistic Ascent (PI Control) is the Augmented Lagrangian Method in Disguise](https://arxiv.org/abs/2509.22500)
*Juan Ramirez,Simon Lacoste-Julien*

Main category: cs.LG

TL;DR: 本文揭示对偶乐观上升法与增广拉格朗日法的等价性，为对偶乐观法提供理论保证并指导超参数调整。


<details>
  <summary>Details</summary>
Motivation: 现有解决约束深度学习问题的一阶方法有振荡问题，对偶乐观上升法经验效果好但缺乏理论保证。

Method: 证明对偶乐观上升法与增广拉格朗日法的等价性。

Result: 将增广拉格朗日法的理论保证转移到对偶乐观设置，证明其线性收敛到所有局部解，为超参数调整提供指导。

Conclusion: 填补了对偶乐观法经验成功与理论基础之间的关键差距。

Abstract: Constrained optimization is a powerful framework for enforcing requirements
on neural networks. These constrained deep learning problems are typically
solved using first-order methods on their min-max Lagrangian formulation, but
such approaches often suffer from oscillations and can fail to find all local
solutions. While the Augmented Lagrangian method (ALM) addresses these issues,
practitioners often favor dual optimistic ascent schemes (PI control) on the
standard Lagrangian, which perform well empirically but lack formal guarantees.
In this paper, we establish a previously unknown equivalence between these
approaches: dual optimistic ascent on the Lagrangian is equivalent to gradient
descent-ascent on the Augmented Lagrangian. This finding allows us to transfer
the robust theoretical guarantees of the ALM to the dual optimistic setting,
proving it converges linearly to all local solutions. Furthermore, the
equivalence provides principled guidance for tuning the optimism
hyper-parameter. Our work closes a critical gap between the empirical success
of dual optimistic methods and their theoretical foundation.

</details>


### [253] [Adaptive Dual-Mode Distillation with Incentive Schemes for Scalable, Heterogeneous Federated Learning on Non-IID Data](https://arxiv.org/abs/2509.22507)
*Zahid Iqbal*

Main category: cs.LG

TL;DR: 文章指出联邦学习存在客户端模型训练能力、数据非IID和缺乏激励机制等挑战，提出DL - SH、DL - MH和I - DL - MH方法，实验表明这些方法能提升准确率、降低通信成本。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户端模型训练能力差异、数据非IID导致的性能下降以及缺乏有效激励机制的问题。

Method: 提出DL - SH处理统计异质性下的学习；提出DL - MH管理全异质模型并解决统计差异；提出I - DL - MH在复杂联邦学习框架下激励客户端参与。

Result: 实验表明提出的方法相比现有方法显著提升准确率、降低通信成本，DL - SH使全局模型准确率提高153%，I - DL - MH在非IID条件下提高225%。

Conclusion: 提出的方法能有效解决联邦学习中的统计异质性和模型异质性问题。

Abstract: Federated Learning (FL) has emerged as a promising decentralized learning
(DL) approach that enables the use of distributed data without compromising
user privacy. However, FL poses several key challenges. First, it is frequently
assumed that every client can train the same machine learning models, however,
not all clients are able to meet this assumption because of differences in
their business needs and computational resources. Second, statistical
heterogeneity (a.k.a. non-IID data) poses a major challenge in FL, which can
lead to lower global model performance. Third, while addressing these
challenges, there is a need for a cost-effective incentive mechanism to
encourage clients to participate in FL training. In response to these
challenges, we propose several methodologies: DL-SH, which facilitates
efficient, privacy-preserving, and communication-efficient learning in the
context of statistical heterogeneity; DL-MH, designed to manage fully
heterogeneous models while tackling statistical disparities; and I-DL-MH, an
incentive-based extension of DL-MH that promotes client engagement in federated
learning training by providing incentives within this complex federated
learning framework. Comprehensive experiments were carried out to assess the
performance and scalability of the proposed approaches across a range of
complex experimental settings. This involved utilizing various model
architectures, in diverse data distributions, including IID and several non-IID
scenarios, as well as multiple datasets. Experimental results demonstrate that
the proposed approaches significantly enhance accuracy and decrease
communication costs while effectively addressing statistical heterogeneity and
model heterogeneity in comparison to existing state-of-the-art approaches and
baselines, with DL-SH improving global model accuracy by 153%, and I-DL-MH
achieving a 225% improvement under non-IID conditions.

</details>


### [254] [JointDiff: Bridging Continuous and Discrete in Multi-Agent Trajectory Generation](https://arxiv.org/abs/2509.22522)
*Guillem Capellera,Luis Ferraz,Antonio Rubio,Alexandre Alahi,Antonio Agudo*

Main category: cs.LG

TL;DR: 提出JointDiff扩散框架统一连续时空数据和离散事件生成，在体育领域验证其有效性，还引入CrossGuid操作和新基准，取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型将连续数据和离散事件分开处理，无法建模二者同步交互的复杂系统，需统一处理。

Method: 引入JointDiff框架统一连续时空数据和离散事件生成，引入CrossGuid操作进行条件控制，分享新的统一体育基准。

Result: JointDiff在体育领域有效，能进行非可控和两种可控生成，取得了SOTA性能。

Conclusion: 联合建模对于构建交互式系统的现实可控生成模型至关重要。

Abstract: Generative models often treat continuous data and discrete events as separate
processes, creating a gap in modeling complex systems where they interact
synchronously. To bridge this gap, we introduce JointDiff, a novel diffusion
framework designed to unify these two processes by simultaneously generating
continuous spatio-temporal data and synchronous discrete events. We demonstrate
its efficacy in the sports domain by simultaneously modeling multi-agent
trajectories and key possession events. This joint modeling is validated with
non-controllable generation and two novel controllable generation scenarios:
weak-possessor-guidance, which offers flexible semantic control over game
dynamics through a simple list of intended ball possessors, and text-guidance,
which enables fine-grained, language-driven generation. To enable the
conditioning with these guidance signals, we introduce CrossGuid, an effective
conditioning operation for multi-agent domains. We also share a new unified
sports benchmark enhanced with textual descriptions for soccer and football
datasets. JointDiff achieves state-of-the-art performance, demonstrating that
joint modeling is crucial for building realistic and controllable generative
models for interactive systems.

</details>


### [255] [ECHO: Toward Contextual Seq2Seq Paradigms in Large EEG Models](https://arxiv.org/abs/2509.22556)
*Chenyu Liu,Yuqiu Deng,Tianyu Liu,Jinan Zhou,Xinliang Zhou,Ziyu Jia,Yi Ding*

Main category: cs.LG

TL;DR: 提出新型以解码器为中心的大脑电图模型ECHO，在多任务场景表现优。


<details>
  <summary>Details</summary>
Motivation: 现有大脑电图模型缺乏同等能力解码器，限制特征利用。

Method: 引入ECHO，将脑电图建模重构为序列到序列学习，捕获信号、标签和任务间关系，结合离散支持样本构建上下文线索。

Result: 在多个数据集上实验，ECHO即使使用基本组件，也能在多任务场景持续超越现有单任务大脑电图模型。

Conclusion: ECHO具有更好的泛化和适应能力。

Abstract: Electroencephalography (EEG), with its broad range of applications,
necessitates models that can generalize effectively across various tasks and
datasets. Large EEG Models (LEMs) address this by pretraining encoder-centric
architectures on large-scale unlabeled data to extract universal
representations. While effective, these models lack decoders of comparable
capacity, limiting the full utilization of the learned features. To address
this issue, we introduce ECHO, a novel decoder-centric LEM paradigm that
reformulates EEG modeling as sequence-to-sequence learning. ECHO captures
layered relationships among signals, labels, and tasks within sequence space,
while incorporating discrete support samples to construct contextual cues. This
design equips ECHO with in-context learning, enabling dynamic adaptation to
heterogeneous tasks without parameter updates. Extensive experiments across
multiple datasets demonstrate that, even with basic model components, ECHO
consistently outperforms state-of-the-art single-task LEMs in multi-task
settings, showing superior generalization and adaptability.

</details>


### [256] [Learning to Price Bundles: A GCN Approach for Mixed Bundling](https://arxiv.org/abs/2509.22557)
*Liangyu Ding,Chenghan Wu,Guokai Li,Zizhuo Wang*

Main category: cs.LG

TL;DR: 本文探索用图卷积网络（GCN）解决捆绑定价问题，提出GCN框架及推理策略和局部搜索技术，实验验证其有效性和高效性。


<details>
  <summary>Details</summary>
Motivation: 捆绑定价问题因候选捆绑组合数量呈指数级增长而难以求解，需寻找有效解决方法。

Method: 开发混合捆绑模型的图表示，训练GCN学习最优捆绑的潜在模式，提出两种推理策略推导可行解，用局部搜索技术提高解的质量。

Result: 使用在5个产品实例上训练的GCN，中小规模问题能快速得到近最优解（超97%），大规模问题比其他启发式方法效果好，对超30个产品实例也能提供高质量解。

Conclusion: 所提出的基于GCN的框架在解决捆绑定价问题上有效且高效。

Abstract: Bundle pricing refers to designing several product combinations (i.e.,
bundles) and determining their prices in order to maximize the expected profit.
It is a classic problem in revenue management and arises in many industries,
such as e-commerce, tourism, and video games. However, the problem is typically
intractable due to the exponential number of candidate bundles. In this paper,
we explore the usage of graph convolutional networks (GCNs) in solving the
bundle pricing problem. Specifically, we first develop a graph representation
of the mixed bundling model (where every possible bundle is assigned with a
specific price) and then train a GCN to learn the latent patterns of optimal
bundles. Based on the trained GCN, we propose two inference strategies to
derive high-quality feasible solutions. A local-search technique is further
proposed to improve the solution quality. Numerical experiments validate the
effectiveness and efficiency of our proposed GCN-based framework. Using a GCN
trained on instances with 5 products, our methods consistently achieve
near-optimal solutions (better than 97%) with only a fraction of computational
time for problems of small to medium size. It also achieves superior solutions
for larger size of problems compared with other heuristic methods such as
bundle size pricing (BSP). The method can also provide high quality solutions
for instances with more than 30 products even for the challenging cases where
product utilities are non-additive.

</details>


### [257] [Activation Function Design Sustains Plasticity in Continual Learning](https://arxiv.org/abs/2509.22562)
*Lute Lillo,Nick Cheney*

Main category: cs.LG

TL;DR: 研究表明激活函数选择是缓解连续学习中可塑性损失的关键，引入两种非线性函数并评估，指出精心设计激活函数可维持可塑性。


<details>
  <summary>Details</summary>
Motivation: 在连续学习中，模型除了灾难性遗忘，还会失去适应能力，且非线性在此失效模式中的作用研究不足。

Method: 基于负分支形状和饱和行为的属性分析，引入Smooth - Leaky和Randomized Smooth - Leaky两种非线性函数，在监督类增量基准和强化学习非平稳环境中评估，并提供压力协议和诊断方法。

Result: 通过实验评估新引入的非线性函数。

Conclusion: 精心设计激活函数是一种轻量级、通用的方法，无需额外容量或特定任务调整就能在连续学习中维持可塑性。

Abstract: In independent, identically distributed (i.i.d.) training regimes, activation
functions have been benchmarked extensively, and their differences often shrink
once model size and optimization are tuned. In continual learning, however, the
picture is different: beyond catastrophic forgetting, models can progressively
lose the ability to adapt (referred to as loss of plasticity) and the role of
the non-linearity in this failure mode remains underexplored. We show that
activation choice is a primary, architecture-agnostic lever for mitigating
plasticity loss. Building on a property-level analysis of negative-branch shape
and saturation behavior, we introduce two drop-in nonlinearities (Smooth-Leaky
and Randomized Smooth-Leaky) and evaluate them in two complementary settings:
(i) supervised class-incremental benchmarks and (ii) reinforcement learning
with non-stationary MuJoCo environments designed to induce controlled
distribution and dynamics shifts. We also provide a simple stress protocol and
diagnostics that link the shape of the activation to the adaptation under
change. The takeaway is straightforward: thoughtful activation design offers a
lightweight, domain-general way to sustain plasticity in continual learning
without extra capacity or task-specific tuning.

</details>


### [258] [From Parameters to Behavior: Unsupervised Compression of the Policy Space](https://arxiv.org/abs/2509.22566)
*Davide Tenedini,Riccardo Zamboni,Mirco Mutti,Marcello Restelli*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite its recent successes, Deep Reinforcement Learning (DRL) is
notoriously sample-inefficient. We argue that this inefficiency stems from the
standard practice of optimizing policies directly in the high-dimensional and
highly redundant parameter space $\Theta$. This challenge is greatly compounded
in multi-task settings. In this work, we develop a novel, unsupervised approach
that compresses the policy parameter space $\Theta$ into a low-dimensional
latent space $\mathcal{Z}$. We train a generative model
$g:\mathcal{Z}\to\Theta$ by optimizing a behavioral reconstruction loss, which
ensures that the latent space is organized by functional similarity rather than
proximity in parameterization. We conjecture that the inherent dimensionality
of this manifold is a function of the environment's complexity, rather than the
size of the policy network. We validate our approach in continuous control
domains, showing that the parameterization of standard policy networks can be
compressed up to five orders of magnitude while retaining most of its
expressivity. As a byproduct, we show that the learned manifold enables
task-specific adaptation via Policy Gradient operating in the latent space
$\mathcal{Z}$.

</details>


### [259] [Machine learning approaches to seismic event classification in the Ostrava region](https://arxiv.org/abs/2509.22574)
*Marek Pecha,Michael Skotnica,Jana Rušajová,Bohdan Rieznikov,Vít Wandrol,Markéta Rösnerová,Jaromír Knejzlík*

Main category: cs.LG

TL;DR: 本文对捷克东北部地震数据应用机器学习方法进行分类，LSTM和XGBoost取得高F1分数，展示其快速事件表征潜力。


<details>
  <summary>Details</summary>
Motivation: 捷克东北部地震活动频繁，虽采矿活动停止但矿震仍有，需快速区分构造和人为地震事件。

Method: 对Seismic Polygon Frenštát (SPF) 数据集应用机器学习方法，使用LSTM和XGBoost进行二元分类。

Result: LSTM和XGBoost在二元分类中F1分数达0.94 - 0.95。

Conclusion: 现代机器学习技术在快速事件表征方面有潜力。

Abstract: The northeastern region of the Czech Republic is among the most seismically
active areas in the country. The most frequent seismic events are
mining-induced since there used to be strong mining activity in the past.
However, natural tectonic events may also occur. In addition, seismic stations
often record explosions in quarries in the region. Despite the cessation of
mining activities, mine-induced seismic events still occur. Therefore, a rapid
differentiation between tectonic and anthropogenic events is still important.
  The region is currently monitored by the OKC seismic station in
Ostrava-Kr\'{a}sn\'{e} Pole built in 1983 which is a part of the Czech Regional
Seismic Network. The station has been providing digital continuous waveform
data at 100 Hz since 2007. In the years 1992--2002, the region was co-monitored
by the Seismic Polygon Fren\v{s}t\'{a}t (SPF) which consisted of five seismic
stations using a triggered STA/LTA system.
  In this study, we apply and compare machine learning methods to the SPF
dataset, which contains labeled records of tectonic and mining-induced events.
For binary classification, a Long Short-Term Memory recurrent neural network
and XGBoost achieved an F1-score of 0.94 -- 0.95, demonstrating the potential
of modern machine learning techniques for rapid event characterization.

</details>


### [260] [EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning](https://arxiv.org/abs/2509.22576)
*Xu Wujiang,Wentian Zhao,Zhenting Wang,Li Yu-Jhe,Jin Can,Jin Mingyu,Mei Kai,Wan Kun,Metaxas Dimitris*

Main category: cs.LG

TL;DR: 在多轮稀疏奖励环境训练大语言模型（LLM）智能体面临挑战，论文提出熵正则化策略优化（EPO）框架，取得性能提升，表明该场景需不同熵控制方法。


<details>
  <summary>Details</summary>
Motivation: 解决在多轮稀疏奖励环境中训练LLM智能体时，强化学习面临的探索 - 利用级联失败问题。

Method: 提出EPO框架，包含多轮熵正则化增强探索、熵平滑正则化防止熵突变、自适应分阶段加权平衡探索与利用三种机制。

Result: EPO在ScienceWorld上性能提升达152%，在ALFWorld上达19.8%。

Conclusion: 多轮稀疏奖励场景需要与传统强化学习不同的熵控制方法，对LLM智能体训练有广泛影响。

Abstract: Training LLM agents in multi-turn environments with sparse rewards, where
completing a single task requires 30+ turns of interaction within an episode,
presents a fundamental challenge for reinforcement learning. We identify a
critical failure mode unique to this setting: the exploration-exploitation
cascade failure. This cascade begins with early-stage policy premature
convergence, where sparse feedback causes agents to commit to flawed,
low-entropy strategies. Subsequently, agents enter late-stage policy collapse,
where conventional entropy regularization becomes counterproductive, promoting
chaotic exploration that destabilizes training. We propose Entropy-regularized
Policy Optimization (EPO), a general framework that breaks this failure cycle
through three synergistic mechanisms: (1) adopting entropy regularization in
multi-turn settings to enhance exploration, (2) an entropy smoothing
regularizer that bounds policy entropy within historical averages to prevent
abrupt fluctuations, and (3) adaptive phase-based weighting that balances
exploration and exploitation across training. Our analysis justifies that EPO
guarantees monotonically decreasing entropy variance while maintaining
convergence. EPO achieves up to 152% performance improvement on ScienceWorld
and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn
sparse-reward settings require fundamentally different entropy control than
traditional RL, with broad implications for LLM agent training.

</details>


### [261] [The Lie of the Average: How Class Incremental Learning Evaluation Deceives You?](https://arxiv.org/abs/2509.22580)
*Guannan Lai,Da-Wei Zhou,Xin Yang,Han-Jia Ye*

Main category: cs.LG

TL;DR: 主流CIL评估协议抽样策略有缺陷，本文提出EDGE评估协议，用任务间相似度识别极端序列，实验表明其能有效捕捉性能极值。


<details>
  <summary>Details</summary>
Motivation: 主流CIL评估协议仅从少量随机采样序列计算均值和方差，无法捕捉完整性能范围，有偏差。

Method: 引入极端序列概念，利用任务间相似度与模型性能的正相关关系，提出EDGE评估协议自适应识别和采样极端类序列。

Result: EDGE能有效捕捉性能极值，更准确估计分布边界。

Conclusion: EDGE评估协议为模型选择和鲁棒性检查提供了可操作的见解。

Abstract: Class Incremental Learning (CIL) requires models to continuously learn new
classes without forgetting previously learned ones, while maintaining stable
performance across all possible class sequences. In real-world settings, the
order in which classes arrive is diverse and unpredictable, and model
performance can vary substantially across different sequences. Yet mainstream
evaluation protocols calculate mean and variance from only a small set of
randomly sampled sequences. Our theoretical analysis and empirical results
demonstrate that this sampling strategy fails to capture the full performance
range, resulting in biased mean estimates and a severe underestimation of the
true variance in the performance distribution. We therefore contend that a
robust CIL evaluation protocol should accurately characterize and estimate the
entire performance distribution. To this end, we introduce the concept of
extreme sequences and provide theoretical justification for their crucial role
in the reliable evaluation of CIL. Moreover, we observe a consistent positive
correlation between inter-task similarity and model performance, a relation
that can be leveraged to guide the search for extreme sequences. Building on
these insights, we propose EDGE (Extreme case-based Distribution and
Generalization Evaluation), an evaluation protocol that adaptively identifies
and samples extreme class sequences using inter-task similarity, offering a
closer approximation of the ground-truth performance distribution. Extensive
experiments demonstrate that EDGE effectively captures performance extremes and
yields more accurate estimates of distributional boundaries, providing
actionable insights for model selection and robustness checking. Our code is
available at https://github.com/AIGNLAI/EDGE.

</details>


### [262] [Transport Based Mean Flows for Generative Modeling](https://arxiv.org/abs/2509.22592)
*Elaheh Akbari,Ping He,Ahmadreza Moradipari,Yikun Bai,Soheil Kolouri*

Main category: cs.LG

TL;DR: 本文通过在Mean Flow框架中引入基于最优传输的采样策略，解决了单步生成模型在连续域中无法忠实逼近原多步流匹配过程的问题，实验证明该方法在单步生成建模中具有更好的推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的流匹配生成模型推理速度慢，Mean Flows虽能加速推理但在许多连续域中无法忠实逼近原多步流匹配过程。

Method: 在Mean Flow框架中引入基于最优传输的采样策略。

Result: 在低维可控设置和高维任务（如图像生成、图像到图像翻译和点云生成）的实验中，该方法在单步生成建模中实现了更高的推理准确性。

Conclusion: 提出的方法解决了Mean Flows的局限性，能在单步生成中更好地保留原多步流过程的保真度和多样性，实现更优的推理准确性。

Abstract: Flow-matching generative models have emerged as a powerful paradigm for
continuous data generation, achieving state-of-the-art results across domains
such as images, 3D shapes, and point clouds. Despite their success, these
models suffer from slow inference due to the requirement of numerous sequential
sampling steps. Recent work has sought to accelerate inference by reducing the
number of sampling steps. In particular, Mean Flows offer a one-step generation
approach that delivers substantial speedups while retaining strong generative
performance. Yet, in many continuous domains, Mean Flows fail to faithfully
approximate the behavior of the original multi-step flow-matching process. In
this work, we address this limitation by incorporating optimal transport-based
sampling strategies into the Mean Flow framework, enabling one-step generators
that better preserve the fidelity and diversity of the original multi-step flow
process. Experiments on controlled low-dimensional settings and on
high-dimensional tasks such as image generation, image-to-image translation,
and point cloud generation demonstrate that our approach achieves superior
inference accuracy in one-step generative modeling.

</details>


### [263] [Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning](https://arxiv.org/abs/2509.22601)
*Yulei Qin,Xiaoyu Tan,Zhengbao He,Gang Li,Haojia Lin,Zongyi Li,Zihan Xu,Yuchen Shi,Siqi Cai,Renting Rui,Shaofei Cai,Yuzheng Cai,Xuan Zhang,Sheng Ye,Ke Li,Xing Sun*

Main category: cs.LG

TL;DR: 本文提出SPEAR方法解决强化学习中探索与利用平衡问题，通过课程式自模仿学习稳定训练。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习在探索与利用权衡上有挑战，基于策略熵的探索易导致训练不稳定，需实现渐进式平衡。

Method: 提出SPEAR，一种基于课程的自模仿学习方法，结合课程管理探索过程，利用内在奖励和自模仿进行不同层次探索，校准经验优势和引入正则化。

Result: 未提及具体结果。

Conclusion: 未提及明确结论，但旨在实现探索与利用平衡并稳定训练。

Abstract: Reinforcement learning (RL) is the dominant paradigm for sharpening strategic
tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,
yet it faces a fundamental challenge of exploration-exploitation trade-off.
Existing studies stimulate exploration through the lens of policy entropy, but
such mechanical entropy maximization is prone to RL training instability due to
the multi-turn distribution shifting. In this paper, we target the progressive
exploration-exploitation balance under the guidance of the agent own
experiences without succumbing to either entropy collapsing or runaway
divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)
recipe for training agentic LLMs. It extends the vanilla SIL framework, where a
replay buffer stores self-generated promising trajectories for off-policy
update, by gradually steering the policy evolution within a well-balanced range
of entropy across stages. Specifically, our approach incorporates a curriculum
to manage the exploration process, utilizing intrinsic rewards to foster
skill-level exploration and facilitating action-level exploration through SIL.
At first, the auxiliary tool call reward plays a critical role in the
accumulation of tool-use skills, enabling broad exposure to the unfamiliar
distributions of the environment feedback with an upward entropy trend. As
training progresses, self-imitation gets strengthened to exploit existing
successful patterns from replayed experiences for comparative action-level
exploration, accelerating solution iteration without unbounded entropy growth.
To further stabilize training, we recalibrate the advantages of experiences in
the replay buffer to address the potential policy drift. Reugularizations such
as the clipping of tokens with high covariance between probability and
advantage are introduced to the trajectory-level entropy control to curb
over-confidence.

</details>


### [264] [Quantile Advantage Estimation for Entropy-Safe Reasoning](https://arxiv.org/abs/2509.22611)
*Junkang Wu,Kexin Huang,Jiancan Wu,An Zhang,Xiang Wang,Xiangnan He*

Main category: cs.LG

TL;DR: 指出RLVR训练中存在熵崩溃和熵爆炸问题，提出QAE方法解决，证明了双边熵安全，实验取得良好效果，确定基线设计是扩展RLVR的主要机制。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR训练中熵崩溃和熵爆炸的问题。

Method: 提出QAE方法，用组级K分位数基线代替均值基线。

Result: 稳定了熵，稀疏了信用分配，在Qwen3 - 8B/14B - Base上持续提升pass@1。

Conclusion: 基线设计是扩展RLVR的主要机制，而非token级启发式方法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM
reasoning, but training often oscillates between {entropy collapse} and
{entropy explosion}. We trace both hazards to the mean baseline used in
value-free RL (e.g., GRPO and DAPO), which improperly penalizes
negative-advantage samples under reward outliers. We propose {Quantile
Advantage Estimation} (QAE), replacing the mean with a group-wise K-quantile
baseline. QAE induces a response-level, two-regime gate: on hard queries (p <=
1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it
targets remaining failures. Under first-order softmax updates, we prove
{two-sided entropy safety}, giving lower and upper bounds on one-step entropy
change that curb explosion and prevent collapse. Empirically, this minimal
modification stabilizes entropy, sparsifies credit assignment (with tuned K,
roughly 80% of responses receive zero advantage), and yields sustained pass@1
gains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results
identify {baseline design} -- rather than token-level heuristics -- as the
primary mechanism for scaling RLVR.

</details>


### [265] [IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning](https://arxiv.org/abs/2509.22621)
*Aayush Mishra,Daniel Khashabi,Anqi Liu*

Main category: cs.LG

TL;DR: 本文探讨能否用ICL内部计算改进SFT质量，提出IA2技术，实验证明在SFT前使用IA2能提升模型输出准确性和校准度。


<details>
  <summary>Details</summary>
Motivation: 研究能否利用ICL的内部计算来改善SFT的质量，基于ICL和SFT有不同激活模式的观察。

Method: 引入ICL激活对齐（IA2）自蒸馏技术，在SFT前执行IA2。

Result: 在12个流行基准和2个模型家族的大量实验结果表明，在SFT前执行IA2能显著提高模型输出的准确性和校准度。

Conclusion: 该发现不仅具有实际应用价值，还为理解模型自适应的内在机制提供了思路。

Abstract: Supervised Fine-Tuning (SFT) is used to specialize model behavior by training
weights to produce intended target responses for queries. In contrast,
In-Context Learning (ICL) adapts models during inference with instructions or
demonstrations in the prompt. ICL can offer better generalizability and more
calibrated responses compared to SFT in data scarce settings, at the cost of
more inference compute. In this work, we ask the question: Can ICL's internal
computations be used to improve the qualities of SFT? We first show that ICL
and SFT produce distinct activation patterns, indicating that the two methods
achieve adaptation through different functional mechanisms. Motivated by this
observation and to use ICL's rich functionality, we introduce ICL Activation
Alignment (IA2), a self-distillation technique which aims to replicate ICL's
activation patterns in SFT models and incentivizes ICL-like internal reasoning.
Performing IA2 as a priming step before SFT significantly improves the accuracy
and calibration of model outputs, as shown by our extensive empirical results
on 12 popular benchmarks and 2 model families. This finding is not only
practically useful, but also offers a conceptual window into the inner
mechanics of model adaptation.

</details>


### [266] [Learning Admissible Heuristics for A*: Theory and Practice](https://arxiv.org/abs/2509.22626)
*Ehsan Futuhi,Nathan R. Sturtevant*

Main category: cs.LG

TL;DR: 本文针对深度学习启发式函数学习的局限性，提出CEA损失函数，在魔方领域取得好效果，还研究了学习启发式的样本复杂度并给出泛化保证。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习方法忽略可采纳性和训练数据外泛化保证有限的问题。

Method: 将启发式学习作为约束优化问题，引入CEA损失函数，利用PDB抽象和图结构性质研究样本复杂度。

Result: 在魔方领域产生接近可采纳的启发式，比压缩模式数据库启发式有更强引导力，收紧了A-star泛化所需训练样本数量的界限，为依赖目标的启发式提供泛化保证。

Conclusion: 所提方法有效解决了现有深度学习启发式学习的局限，在理论和实践上都有积极成果。

Abstract: Heuristic functions are central to the performance of search algorithms such
as A-star, where admissibility - the property of never overestimating the true
shortest-path cost - guarantees solution optimality. Recent deep learning
approaches often disregard admissibility and provide limited guarantees on
generalization beyond the training data. This paper addresses both of these
limitations. First, we pose heuristic learning as a constrained optimization
problem and introduce Cross-Entropy Admissibility (CEA), a loss function that
enforces admissibility during training. On the Rubik's Cube domain, this method
yields near-admissible heuristics with significantly stronger guidance than
compressed pattern database (PDB) heuristics. Theoretically, we study the
sample complexity of learning heuristics. By leveraging PDB abstractions and
the structural properties of graphs such as the Rubik's Cube, we tighten the
bound on the number of training samples needed for A-star to generalize.
Replacing a general hypothesis class with a ReLU neural network gives bounds
that depend primarily on the network's width and depth, rather than on graph
size. Using the same network, we also provide the first generalization
guarantees for goal-dependent heuristics.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [267] [Cycle is All You Need: More Is Different](https://arxiv.org/abs/2509.21340)
*Xin Li*

Main category: cs.NE

TL;DR: 提出信息拓扑框架，认为循环闭合是记忆和意识的基本机制，循环可实现记忆和认知，意识源于高阶不变量的持续存在。


<details>
  <summary>Details</summary>
Motivation: 探索记忆和意识的基本机制。

Method: 构建信息拓扑框架，分析神经状态空间中的循环、生物层面的多脉冲神经元组实现循环、利用层叠 - 余层叠对偶形式化过程。

Result: 明确循环在记忆、认知和意识中的作用，如过滤噪声、编码低熵内容不变量、实现感知 - 行动循环的高阶不变性等。

Conclusion: 循环是关键，持久的不变量能以最小能量成本在非遍历环境中实现泛化和长期连贯。

Abstract: We propose an information-topological framework in which cycle closure is the
fundamental mechanism of memory and consciousness. Memory is not a static store
but the ability to re-enter latent cycles in neural state space, with invariant
cycles serving as carriers of meaning by filtering order-specific noise and
preserving what persists across contexts. The dot-cycle dichotomy captures
this: transient dots scaffold exploration, while nontrivial cycles encode
low-entropy content invariants that stabilize memory. Biologically,
polychronous neural groups realize 1-cycles through delay-locked spiking
reinforced by STDP, nested within theta-gamma rhythms that enforce boundary
cancellation. These micro-cycles compose hierarchically, extending navigation
loops into general memory and cognition. The perception-action cycle introduces
high-order invariance: closure holds even across sense-act alternations,
generalizing ancestral homing behavior. Sheaf-cosheaf duality formalizes this
process: sheaves glue perceptual fragments into global sections, cosheaves
decompose global plans into actions and closure aligns top-down predictions
with bottom-up cycles. Consciousness then arises as the persistence of
high-order invariants that integrate (unity) yet differentiate (richness)
across contexts. We conclude that cycle is all you need: persistent invariants
enable generalization in non-ergodic environments with long-term coherence at
minimal energetic cost.

</details>


### [268] [From Embeddings to Equations: Genetic-Programming Surrogates for Interpretable Transformer Classification](https://arxiv.org/abs/2509.21341)
*Mohammad Sadegh Khorshidi,Navid Yazdanjue,Hassan Gharoun,Mohammad Reza Nikoo,Fang Chen,Amir H. Gandomi*

Main category: cs.NE

TL;DR: 研究对冻结Transformer嵌入进行符号代理建模，获紧凑、可审计且概率校准的分类器，在多基准测试表现良好并给出解释。


<details>
  <summary>Details</summary>
Motivation: 获取紧凑、可审计且概率校准的分类器。

Method: 用语义保留特征划分（SPFP）将嵌入划分为不相交、信息保留的视图，用合作多群体遗传程序（MEGP）学习加法、封闭形式的对数程序，通过一标准差规则选模型，用温度缩放降低校准误差。

Result: 在多个数据集上有强区分能力，MNIST、CIFAR10、MSC17的F1达约0.99，SST2G约0.95，20NG最具挑战，给出多种解释性结果。

Conclusion: 符号代理建模方法有效，能实现有解释性的分类。

Abstract: We study symbolic surrogate modeling of frozen Transformer embeddings to
obtain compact, auditable classifiers with calibrated probabilities. For five
benchmarks (SST2G, 20NG, MNIST, CIFAR10, MSC17), embeddings from ModernBERT,
DINOv2, and SigLIP are partitioned on the training set into disjoint,
information-preserving views via semantic-preserving feature partitioning
(SPFP). A cooperative multi-population genetic program (MEGP) then learns
additive, closed-form logit programs over these views. Across 30 runs per
dataset we report F1, AUC, log-loss, Brier, expected calibration error (ECE),
and symbolic complexity; a canonical model is chosen by a one-standard-error
rule on validation F1 with a parsimony tie-break. Temperature scaling fitted on
validation yields substantial ECE reductions on test. The resulting surrogates
achieve strong discrimination (up to F1 around 0.99 on MNIST, CIFAR10, MSC17;
around 0.95 on SST2G), while 20NG remains most challenging. We provide
reliability diagrams, dimension usage and overlap statistics,
contribution-based importances, and global effect profiles (PDP and ALE),
demonstrating faithful, cross-modal explanations grounded in explicit programs.

</details>


### [269] [SGNNBench: A Holistic Evaluation of Spiking Graph Neural Network on Large-scale Graph](https://arxiv.org/abs/2509.21342)
*Huizhe Zhang,Jintang Li,Yuchang Zhu,Liang Chen,Li Kuang*

Main category: cs.NE

TL;DR: 现有GNN在大规模图上不可持续，SGNN有节能潜力但缺系统基准，本文提出SGNNBench评估SGNN。


<details>
  <summary>Details</summary>
Motivation: 现有GNN在大规模图上计算和时间开销大，需要节能模型，而SGNN缺系统基准来探索设计原则。

Method: 提出SGNNBench，从有效性、能源效率和架构设计等多视角深入研究SGNN，在18个数据集上评估9个先进SGNN，比较模型大小、内存使用和理论能耗。

Result: 揭示了SGNN常被忽视的能源瓶颈。

Conclusion: 精心研究SGNN的设计空间，促进通用SGNN范式的发展。

Abstract: Graph Neural Networks (GNNs) are exemplary deep models designed for graph
data. Message passing mechanism enables GNNs to effectively capture graph
topology and push the performance boundaries across various graph tasks.
However, the trend of developing such complex machinery for graph
representation learning has become unsustainable on large-scale graphs. The
computational and time overhead make it imperative to develop more
energy-efficient GNNs to cope with the explosive growth of real-world graphs.
Spiking Graph Neural Networks (SGNNs), which integrate biologically plausible
learning via unique spike-based neurons, have emerged as a promising
energy-efficient alternative. Different layers communicate with sparse and
binary spikes, which facilitates computation and storage of intermediate graph
representations. Despite the proliferation of SGNNs proposed in recent years,
there is no systematic benchmark to explore the basic design principles of
these brain-inspired networks on the graph data. To bridge this gap, we present
SGNNBench to quantify progress in the field of SGNNs. Specifically, SGNNBench
conducts an in-depth investigation of SGNNs from multiple perspectives,
including effectiveness, energy efficiency, and architectural design. We
comprehensively evaluate 9 state-of-the-art SGNNs across 18 datasets. Regarding
efficiency, we empirically compare these baselines w.r.t model size, memory
usage, and theoretical energy consumption to reveal the often-overlooked energy
bottlenecks of SGNNs. Besides, we elaborately investigate the design space of
SGNNs to promote the development of a general SGNN paradigm.

</details>


### [270] [Neuromorphic Deployment of Spiking Neural Networks for Cognitive Load Classification in Air Traffic Control](https://arxiv.org/abs/2509.21345)
*Jiahui An,Chonghao Cai,Olympia Gallou,Sara Irina Fabrikant,Giacomo Indiveri,Elisa Donati*

Main category: cs.NE

TL;DR: 本文提出用脉冲神经网络（SNNs）硬件实现的神经形态系统，用于空中交通管制任务中的认知负荷分类，在芯片部署后仍有一定准确率，证明该系统可行性。


<details>
  <summary>Details</summary>
Motivation: 实现现实场景（空中交通管制任务）中的认知负荷分类。

Method: 从开源数据集提取脑电图和眼动追踪特征，训练评估传统机器学习模型和SNNs，用生物启发的delta - rule学习算法训练单层SNN模型，量化模型并部署到DYNAP - SE芯片。

Result: 单层SNN模型达到80.6%的准确率，芯片部署后使用基于脉冲的输入，分类准确率达73.5%。

Conclusion: 事件驱动的神经形态系统可用于动态现实场景中的超低功耗嵌入式认知状态监测。

Abstract: This paper presents a neuromorphic system for cognitive load classification
in a real-world setting, an Air Traffic Control (ATC) task, using a hardware
implementation of Spiking Neural Networks (SNNs). Electroencephalogram (EEG)
and eye-tracking features, extracted from an open-source dataset, were used to
train and evaluate both conventional machine learning models and SNNs. Among
the SNN architectures explored, a minimalistic, single-layer model trained with
a biologically inspired delta-rule learning algorithm achieved competitive
performance (80.6%). To enable deployment on neuromorphic hardware, the model
was quantized and implemented on the mixed-signal DYNAP-SE chip. Despite
hardware constraints and analog variability, the chip-deployed SNN maintained a
classification accuracy of up to 73.5% using spike-based input. These results
demonstrate the feasibility of event-driven neuromorphic systems for
ultra-low-power, embedded cognitive state monitoring in dynamic real-world
scenarios.

</details>


### [271] [Spiking Neural Networks for Mental Workload Classification with a Multimodal Approach](https://arxiv.org/abs/2509.21346)
*Jiahui An,Sara Irina Fabrikant,Giacomo Indiveri,Elisa Donati*

Main category: cs.NE

TL;DR: 研究对比硬件兼容SNN模型与传统ML模型评估认知负荷，发现多模态集成提高准确性，SNN有实时应用潜力。


<details>
  <summary>Details</summary>
Motivation: 准确评估心理负荷很重要，但基于EEG的ML模型计算成本高，阻碍嵌入式实时应用，需替代方案。

Method: 使用开源多模态数据集，对比硬件兼容的SNN模型和各种传统ML模型。

Result: 多模态集成提高准确性，SNN性能与ML相当。

Conclusion: 基于事件的处理是自适应闭环嵌入式设备中低延迟、节能的工作负荷监测的有前景解决方案。

Abstract: Accurately assessing mental workload is crucial in cognitive neuroscience,
human-computer interaction, and real-time monitoring, as cognitive load
fluctuations affect performance and decision-making. While
Electroencephalography (EEG) based machine learning (ML) models can be used to
this end, their high computational cost hinders embedded real-time
applications. Hardware implementations of spiking neural networks (SNNs) offer
a promising alternative for low-power, fast, event-driven processing. This
study compares hardware compatible SNN models with various traditional ML ones,
using an open-source multimodal dataset. Our results show that multimodal
integration improves accuracy, with SNN performance comparable to the ML one,
demonstrating their potential for real-time implementations of cognitive load
detection. These findings position event-based processing as a promising
solution for low-latency, energy efficient workload monitoring in adaptive
closed-loop embedded devices that dynamically regulate cognitive load.

</details>


### [272] [Domain-Informed Genetic Superposition Programming: A Case Study on SFRC Beams](https://arxiv.org/abs/2509.21355)
*Mohammad Sadegh Khorshidi,Navid Yazdanjue,Hassan Gharoun,Mohammad Reza Nikoo,Fang Chen,Amir H. Gandomi*

Main category: cs.NE

TL;DR: 提出适用于工程系统的DIGSP符号回归框架，在SFRC梁数据集上表现优于BGP模型，证明其能提升收敛和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为受可分离物理机制支配的工程系统开发合适的符号回归框架。

Method: DIGSP将输入空间划分为特定领域特征子集，独立进化GP种群，停滞时触发AHSAM进行符号叠加，包括ANOVA过滤、符号构造压缩和验证引导修剪。

Result: 30次独立试验中，DIGSP在训练和测试的RMSE上始终优于BGP，误差分布更集中、异常值更少，验证RMSE无显著差异。

Conclusion: 领域信息结构分解和符号抽象可改善收敛和泛化，DIGSP为符号叠加与物理结构相符的系统提供了有原则且可解释的建模策略。

Abstract: This study presents domain-informed genetic superposition programming
(DIGSP), a symbolic regression framework tailored for engineering systems
governed by separable physical mechanisms. DIGSP partitions the input space
into domain-specific feature subsets and evolves independent genetic
programming (GP) populations to model material-specific effects. Early
evolution occurs in isolation, while ensemble fitness promotes inter-population
cooperation. To enable symbolic superposition, an adaptive hierarchical
symbolic abstraction mechanism (AHSAM) is triggered after stagnation across all
populations. AHSAM performs analysis of variance- (ANOVA) based filtering to
identify statistically significant individuals, compresses them into symbolic
constructs, and injects them into all populations through a validation-guided
pruning cycle. The DIGSP is benchmarked against a baseline multi-gene genetic
programming (BGP) model using a dataset of steel fiber-reinforced concrete
(SFRC) beams. Across 30 independent trials with 65% training, 10% validation,
and 25% testing splits, DIGSP consistently outperformed BGP in training and
test root mean squared error (RMSE). The Wilcoxon rank-sum test confirmed
statistical significance (p < 0.01), and DIGSP showed tighter error
distributions and fewer outliers. No significant difference was observed in
validation RMSE due to limited sample size. These results demonstrate that
domain-informed structural decomposition and symbolic abstraction improve
convergence and generalization. DIGSP offers a principled and interpretable
modeling strategy for systems where symbolic superposition aligns with the
underlying physical structure.

</details>


### [273] [Smart Routing for EV Charge Point Operators in Mega Cities: Case Study of Istanbul](https://arxiv.org/abs/2509.21369)
*Onur Yenigun,Gozde Karatas Baydogmus,Kazim Yildiz*

Main category: cs.NE

TL;DR: 研究提出优化电动汽车充电网络维护运营规划的集成方法，应用于伊斯坦布尔100个充电站，能节省约35%距离，未来计划集成实时因素。


<details>
  <summary>Details</summary>
Motivation: 电动汽车使用增加，充电基础设施维护规划面临挑战，低效人员管理会导致时间、成本和资源问题。

Method: 使用K - means聚类算法按地理邻近性对充电站分组，用遗传算法计算簇间最短路线，用Python实现。

Result: 应用于伊斯坦布尔100个充电站数据集，最有效场景比参考路线节省约35%距离。

Conclusion: 该方法有潜力使大都市实地运营更高效、有计划和可扩展，未来计划集成实时因素。

Abstract: The rapidly increasing use of electric vehicles (EVs) has made it even more
important to manage the charging infrastructure sustainably. The expansion of
charging station networks, especially in large cities, creates serious
logistical challenges for charging point operators (CPOs) in planning
maintenance and repair activities. Inefficient field personnel management can
lead to time loss, high operational costs, and resource waste. This study
presents an integrated method to optimize the planning of EV charging network
maintenance operations. The proposed approach groups charging stations
according to geographical proximity using the K-means clustering algorithm and
calculates the shortest routes between clusters using a genetic algorithm. The
method was developed in Python and applied to a dataset consisting of 100 EV
charging stations in Istanbul.
  Considering the population density, traffic density, and resource constraints
of Istanbul, the route planning approach presented in this study has great
potential, especially for such metropolises. According to the different
parameter configurations tested, the most efficient scenario provided
approximately 35\% distance savings compared to the reference route created
according to the sequential data layout. While the reference route provides a
simple comparison, the study presents a solution that will enable field
operations in metropolitan cities such as Istanbul to be conducted in a more
efficient, planned and scalable manner. In future studies, it is planned to
integrate real-time factors such as traffic conditions and field technician
constraints.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [274] [Size-Aware Dispatching to Fluid Queues](https://arxiv.org/abs/2509.21693)
*Runhan Xie,Esa Hyytiä,Rhonda Righter*

Main category: cs.PF

TL;DR: 开发流体流动模型解决路由问题，用变分法刻画最优策略结构并通过数值示例说明。


<details>
  <summary>Details</summary>
Motivation: 为路由问题开发模型以最小化平均延迟，对应离散作业调度问题。

Method: 开发流体流动模型，将问题转化为在n维空间中寻找清空系统的最优路径，使用变分法。

Result: 用变分法刻画了最优策略的结构，数值示例为流体路由问题和大型分布式服务系统最优控制提供了更多见解。

Conclusion: 所开发的模型和方法有助于解决流体路由问题以及大型分布式服务系统的最优控制。

Abstract: We develop a fluid-flow model for routing problems, where fluid consists of
different size particles and the task is to route the incoming fluid to $n$
parallel servers using the size information in order to minimize the mean
latency. The problem corresponds to the dispatching problem of (discrete) jobs
arriving according to a stochastic process. In the fluid model the problem
reduces to finding an optimal path to empty the system in $n$-dimensional
space. We use the calculus of variation to characterize the structure of
optimal policies. Numerical examples shed further light on the fluid routing
problem and the optimal control of large distributed service systems.

</details>


### [275] [SAHM: State-Aware Heterogeneous Multicore for Single-Thread Performance](https://arxiv.org/abs/2509.22405)
*Shayne Wadle,Karthikeyan Sankaralingam*

Main category: cs.PF

TL;DR: 论文提出SAHM架构利用单线程工作负载细粒度、时变行为多样性提升性能，评估显示可实现17%加速。


<details>
  <summary>Details</summary>
Motivation: 传统提升单线程性能方法收益递减，需新架构提升性能。

Method: 定义16种行为状态，使用一组针对特定状态的专用核心，根据检测到的行为在运行时迁移线程。

Result: 在单线程和多程序场景评估，现实场景可实现17%加速，对高成本迁移有鲁棒性，性能下降小于1%。

Conclusion: 基于状态感知的核心专业化是提升单线程性能的新途径。

Abstract: Improving single-thread performance remains a critical challenge in modern
processor design, as conventional approaches such as deeper speculation, wider
pipelines, and complex out-of-order execution face diminishing returns. This
work introduces SAHM-State-Aware Heterogeneous Multicore-a novel architecture
that targets performance gains by exploiting fine-grained, time-varying
behavioral diversity in single-threaded workloads. Through empirical
characterization of performance counter data, we define 16 distinct behavioral
states representing different microarchitectural demands. Rather than
over-provisioning a monolithic core with all optimizations, SAHM uses a set of
specialized cores tailored to specific states and migrates threads at runtime
based on detected behavior. This design enables composable microarchitectural
enhancements without incurring prohibitive area, power, or complexity costs.
  We evaluate SAHM in both single-threaded and multiprogrammed scenarios,
demonstrating its ability to maintain core utilization while improving overall
performance through intelligent state-driven scheduling. Experimental results
show opportunity for 17% speed up in realistic scenarios. These speed ups are
robust against high-cost migration, decreasing by less than 1%. Overall,
state-aware core specialization is a new path forward for enhancing
single-thread performance.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [276] [Extracting Conceptual Knowledge to Locate Software Issues](https://arxiv.org/abs/2509.21427)
*Ying Wang,Wenjun Mao,Chong Wang,Zhenhao Zhou,Yicheng Zhou,Wenyun Zhao,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: 文章提出RepoLens方法解决现有LLM和LLM代理方法在大规模仓库中进行问题定位的问题，评估显示其能提升多种工具表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM和LLM代理方法在大规模仓库中因关注点混合和分散问题，进行问题定位效果不佳。

Method: 提出RepoLens方法，分离线和在线两个阶段，离线构建知识库，在线检索、聚类和排序关注点并集成到定位流程。

Result: 在SWE - Lancer - Loc基准测试中，RepoLens提升了三种现有工具的表现，在Hit@k和Recall@k上有显著平均增益，跨模型表现良好。

Conclusion: 消融研究和手动评估证实了构建的关注点有效且可靠。

Abstract: Issue localization, which identifies faulty code elements such as files or
functions, is critical for effective bug fixing. While recent LLM-based and
LLM-agent-based approaches improve accuracy, they struggle in large-scale
repositories due to concern mixing, where relevant logic is buried in large
functions, and concern scattering, where related logic is dispersed across
files.
  To address these challenges, we propose RepoLens, a novel approach that
abstracts and leverages conceptual knowledge from code repositories. RepoLens
decomposes fine-grained functionalities and recomposes them into high-level
concerns, semantically coherent clusters of functionalities that guide LLMs. It
operates in two stages: an offline stage that extracts and enriches conceptual
knowledge into a repository-wide knowledge base, and an online stage that
retrieves issue-specific terms, clusters and ranks concerns by relevance, and
integrates them into localization workflows via minimally intrusive prompt
enhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks
derived from SWE-Lancer. RepoLens consistently improves three state-of-the-art
tools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains
of over 22% in Hit@k and 46% in Recall@k for file- and function-level
localization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with
Hit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies
and manual evaluation confirm the effectiveness and reliability of the
constructed concerns.

</details>


### [277] [Lost in Transition: The Struggle of Women Returning to Software Engineering Research after Career Breaks](https://arxiv.org/abs/2509.21533)
*Shalini Chakraborty,Sebastian Baltes*

Main category: cs.SE

TL;DR: 提出多元文化研究项目，探讨软件工程背景女性职业中断后重返学术界面临的挑战，将在多国多所大学开展研究并给出支持透明招聘的建议。


<details>
  <summary>Details</summary>
Motivation: IT行业为职业中断后重返岗位的女性提供支持途径，而学术界机会有限，需研究女性重返学术界面临的挑战。

Method: 在多个国家的多所大学开展研究项目。

Result: 未提及。

Conclusion: 未提及，但目标是探索女性重返学术岗位与行业岗位的挑战差异、了解不同国家现有政策和机会，提供支持透明招聘实践的建议。

Abstract: The IT industry provides supportive pathways such as returnship programs,
coding boot camps, and buddy systems for women re-entering their job after a
career break. Academia, however, offers limited opportunities to motivate women
to return. We propose a diverse multicultural research project investigating
the challenges faced by women with software engineering (SE) backgrounds
re-entering academia or related research roles after a career break. Career
disruptions due to pregnancy, immigration status, or lack of flexible work
options can significantly impact women's career progress, creating barriers for
returning as lecturers, professors, or senior researchers. Although many
companies promote gender diversity policies, such measures are less prominent
and often under-recognized within academic institutions. Our goal is to explore
the specific challenges women encounter when re-entering academic roles
compared to industry roles; to understand the institutional perspective,
including a comparative analysis of existing policies and opportunities in
different countries for women to return to the field; and finally, to provide
recommendations that support transparent hiring practices. The research project
will be carried out in multiple universities and in multiple countries to
capture the diverse challenges and policies that vary by location.

</details>


### [278] [No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials](https://arxiv.org/abs/2509.21816)
*Yuhang Xie,Jian Mu,Xiaojun Ma,Chaoyun Zhang,Lu Wang,Mengyu Zhou,Mugeng Liu,Si Qin,Qingwei Lin,Saravan Rajmohan,Shi Han,Dongmei Zhang*

Main category: cs.SE

TL;DR: 提出自动从自然语言任务描述生成 Excel 教程的框架，实验表明提升执行成功率，教程质量高且降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有 Excel 教程手动编写，更新频繁、成本高，且此前未实现完全自动化生成。

Method: 先实例化任务，通过执行代理在 Excel 中规划并执行解决方案，收集中间工件转化为文档和视频，收集真实任务描述构建语料库，设计综合评估框架。

Result: 框架比现有基线任务执行成功率提高 8.5%，生成教程可读性和教学效果好，自动化流程将成本降至专家编写的 1/20。

Conclusion: 该框架实现了可扩展、高质量的 Excel 教程自动化生成。

Abstract: Excel is one of the most widely used productivity tools across domains,
offering rich functionality but also overwhelming users with its complexity.
This creates a persistent demand for tutorials to support effective usage.
However, existing tutorials are manually authored by experts, require frequent
updates after each software release, and incur substantial labor costs. Prior
work has not achieved fully automated tutorial generation, since existing
methods still depend on handcrafted operation sequences or example materials.
In this paper, we present the first framework for automatically generating
Excel tutorials directly from natural language task descriptions. Our framework
first instantiates the task. Then a central component of this framework,
Execution Agent, plans and executes the solution in Excel, and collects the
intermediate artifacts required for tutorial construction. These artifacts are
then transformed into both structured Excel documents and video demonstrations.
To build a comprehensive tutorial corpus, we collected 1,559 task descriptions
from real-world scenarios. In addition, we designed a systematic evaluation
framework that integrates assessments from both large language models (LLMs)
and human reviewers. Experimental results show that our framework improves task
execution success rates by 8.5% over state-of-the-art baselines. Moreover, the
generated tutorials demonstrate superior readability and instructional
effectiveness, often approaching or surpassing expert-authored materials.
Importantly, the automated pipeline eliminates manual labor and reduces time
costs to 1/20 of expert authoring, making scalable and high-quality tutorial
generation practical for the first time.

</details>


### [279] [Software Engineering Data Analytics: A Framework Based on a Multi-Layered Abstraction Mechanism](https://arxiv.org/abs/2509.21881)
*Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: 提出用于软件分析的特定领域框架概念，通过案例展示其潜力


<details>
  <summary>Details</summary>
Motivation: 构建可对异构软件仓库进行查询、建模和集成的特定领域框架

Method: 采用多层抽象机制和特定领域操作符，进行案例研究

Result: 通过案例研究展示了该方法的潜力

Conclusion: 该特定领域框架在软件分析方面有应用潜力

Abstract: This paper presents a concept of a domain-specific framework for software
analytics by enabling querying, modeling, and integration of heterogeneous
software repositories. The framework adheres to a multi-layered abstraction
mechanism that consists of domain-specific operators. We showcased the
potential of this approach by employing a case study.

</details>


### [280] [AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans](https://arxiv.org/abs/2509.21891)
*Yangtian Zi,Zixuan Wu,Aleksander Boruch-Gruszecki,Jonathan Bell,Arjun Guha*

Main category: cs.SE

TL;DR: 提出代码编辑数据集AgentPack，展示其优势及训练模型效果。


<details>
  <summary>Details</summary>
Motivation: 以往代码编辑微调数据有噪声，软件工程师代理改变现状，需新数据集。

Method: 构建含130万条代码编辑的AgentPack语料库，描述识别和整理流程，分析编辑结构特性。

Result: 基于AgentPack微调的模型性能超基于人类提交语料库训练的模型。

Conclusion: 使用软件工程代理的公共数据训练未来代码编辑模型有潜力。

Abstract: Fine-tuning large language models for code editing has typically relied on
mining commits and pull requests. The working hypothesis has been that commit
messages describe human intent in natural language, and patches to code
describe the changes that implement that intent. However, much of the
previously collected data is noisy: commit messages are terse, human-written
commits commingle several unrelated edits, and many commits come from simple,
rule-based bots.
  The recent adoption of software engineering agents changes this landscape.
Code changes co-authored by humans and agents tend to be more narrowly scoped
and focused on clearer goals. Their commit messages, generated by LLMs,
articulate intent and rationale in much greater detail. Moreover, when these
changes land in public repositories, they are implicitly filtered by humans:
maintainers discard low-quality commits to their projects.
  We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code,
OpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August
2025. We describe the identification and curation pipeline, quantify adoption
trends of these agents, and analyze the structural properties of the edits.
Finally, we show that models fine-tuned on AgentPack can outperform models
trained on prior human-only commit corpora, highlighting the potential of using
public data from software engineering agents to train future code-editing
models.

</details>


### [281] [Unveiling Many Faces of Surrogate Models for Configuration Tuning: A Fitness Landscape Analysis Perspective](https://arxiv.org/abs/2509.21945)
*Pengzhou Chen,Hongyuan Liang,Tao Chen*

Main category: cs.SE

TL;DR: 本文从适应度景观分析的新视角，系统探讨代理模型在配置调优中的作用，提出评估模型有用性的理论，开展大量实证研究，提出自动化预测工具Model4Tune，结果表明其表现良好并为未来研究和实践提供方向。


<details>
  <summary>Details</summary>
Motivation: 先前工作发现代理模型准确性存在问题，留下关于其在配置调优中作用的疑问，需要系统探索。

Method: 从适应度景观分析的新视角进行系统探索和讨论，提出评估模型有用性的理论，开展涉及多达27000个案例的实证研究。

Result: Model4Tune在79%-82%的案例中表现显著优于随机猜测。

Conclusion: 研究为未来研究指明方向，为从业者评估配置调优的最有用模型提供实用解决方案。

Abstract: To efficiently tune configuration for better system performance (e.g.,
latency), many tuners have leveraged a surrogate model to expedite the process
instead of solely relying on the profoundly expensive system measurement. As
such, it is naturally believed that we need more accurate models. However, the
fact of accuracy can lie-a somewhat surprising finding from prior work-has left
us many unanswered questions regarding what role the surrogate model plays in
configuration tuning. This paper provides the very first systematic exploration
and discussion, together with a resolution proposal, to disclose the many faces
of surrogate models for configuration tuning, through the novel perspective of
fitness landscape analysis. We present a theory as an alternative to accuracy
for assessing the model usefulness in tuning, based on which we conduct an
extensive empirical study involving up to 27,000 cases. Drawing on the above,
we propose Model4Tune, an automated predictive tool that estimates which
model-tuner pairs are the best for an unforeseen system without expensive tuner
profiling. Our results suggest that Moldel4Tune, as one of the first of its
kind, performs significantly better than random guessing in 79%-82% of the
cases. Our results not only shed light on the possible future research
directions but also offer a practical resolution that can assist practitioners
in evaluating the most useful model for configuration tuning.

</details>


### [282] [SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios](https://arxiv.org/abs/2509.22097)
*Junkai Chen,Huihui Huang,Yunbo Lyu,Junwen An,Jieke Shi,Chengran Yang,Ting Zhang,Haoye Tian,Yikun Li,Zhenhao Li,Xin Zhou,Xing Hu,David Lo*

Main category: cs.SE

TL;DR: 提出SecureAgentBench基准评估代码代理安全代码生成能力，评估三个代理和三个大模型，结果显示当前代理生成安全代码能力弱，需更多研究。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估代码代理生成安全代码能力方面不足，需新基准。

Method: 引入包含105个编码任务的SecureAgentBench基准，对任务设置、上下文和评估方式进行优化，评估三个代理和三个大模型。

Result: 当前代理生成安全代码困难，部分代理功能正确但引入漏洞，添加安全指令效果不显著。

Conclusion: SecureAgentBench可作为安全代码生成的严格基准，推动基于大模型的可靠软件开发。

Abstract: Large language model (LLM) powered code agents are rapidly transforming
software engineering by automating tasks such as testing, debugging, and
repairing, yet the security risks of their generated code have become a
critical concern. Existing benchmarks have offered valuable insights but remain
insufficient: they often overlook the genuine context in which vulnerabilities
were introduced or adopt narrow evaluation protocols that fail to capture
either functional correctness or newly introduced vulnerabilities. We therefore
introduce SecureAgentBench, a benchmark of 105 coding tasks designed to
rigorously evaluate code agents' capabilities in secure code generation. Each
task includes (i) realistic task settings that require multi-file edits in
large repositories, (ii) aligned contexts based on real-world open-source
vulnerabilities with precisely identified introduction points, and (iii)
comprehensive evaluation that combines functionality testing, vulnerability
checking through proof-of-concept exploits, and detection of newly introduced
vulnerabilities using static analysis. We evaluate three representative agents
(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7
Sonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents
struggle to produce secure code, as even the best-performing one, SWE-agent
supported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,
(ii) some agents produce functionally correct code but still introduce
vulnerabilities, including new ones not previously recorded, and (iii) adding
explicit security instructions for agents does not significantly improve secure
coding, underscoring the need for further research. These findings establish
SecureAgentBench as a rigorous benchmark for secure code generation and a step
toward more reliable software development with LLMs.

</details>


### [283] [SK2Decompile: LLM-based Two-Phase Binary Decompilation from Skeleton to Skin](https://arxiv.org/abs/2509.22114)
*Hanzhuo Tan,Weihao Li,Xiaolong Tian,Siyi Wang,Jiaming Liu,Jing Li,Yuqun Zhang*

Main category: cs.SE

TL;DR: 提出SK2Decompile两阶段反编译方法，先恢复结构再命名标识符，效果超SOTA基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的反编译器在呈现程序源级结构和原始标识符方面存在局限。

Method: 分两阶段，先通过结构恢复模型将二进制代码转换为中间表示以获取程序“骨架”，并使用强化学习奖励符合编译规则的结构；再通过标识符命名模型生成有意义的标识符作为程序“皮肤”，也使用强化学习奖励语义相似性。

Result: SK2Decompile显著优于SOTA基线，在HumanEval数据集上比GPT - 5 - mini平均可重执行率提高21.6%，在GitHub2025基准上比Idioms平均R2I提高29.4%。

Conclusion: 两阶段反编译过程有助于独立提升反编译的正确性和可读性。

Abstract: Large Language Models (LLMs) have emerged as a promising approach for binary
decompilation. However, the existing LLM-based decompilers still are somewhat
limited in effectively presenting a program's source-level structure with its
original identifiers. To mitigate this, we introduce SK2Decompile, a novel
two-phase approach to decompile from the skeleton (semantic structure) to the
skin (identifier) of programs. Specifically, we first apply a Structure
Recovery model to translate a program's binary code to an Intermediate
Representation (IR) as deriving the program's "skeleton", i.e., preserving
control flow and data structures while obfuscating all identifiers with generic
placeholders. We also apply reinforcement learning to reward the model for
producing program structures that adhere to the syntactic and semantic rules
expected by compilers. Second, we apply an Identifier Naming model to produce
meaningful identifiers which reflect actual program semantics as deriving the
program's "skin". We train the Identifier Naming model with a separate
reinforcement learning objective that rewards the semantic similarity between
its predictions and the reference code. Such a two-phase decompilation process
facilitates advancing the correctness and readability of decompilation
independently. Our evaluations indicate that SK2Decompile, significantly
outperforms the SOTA baselines, achieving 21.6% average re-executability rate
gain over GPT-5-mini on the HumanEval dataset and 29.4% average R2I improvement
over Idioms on the GitHub2025 benchmark.

</details>


### [284] [Leveraging LLM Agents for Automated Video Game Testing](https://arxiv.org/abs/2509.22170)
*Chengjia Wang,Lanling Tang,Ming Yuan,Jiongchi Yu,Xiaofei Xie,Jiajun Bu*

Main category: cs.SE

TL;DR: 提出LLM驱动的MMORPG测试框架TITAN，在两个MMORPG上评估，性能优于现有方法，发现新漏洞，已用于实际游戏QA流程。


<details>
  <summary>Details</summary>
Motivation: 传统自动化游戏测试方法在MMORPG中难以实现高状态覆盖和效率，现有基于LLM的游戏玩法方法推理能力有限，需要新的测试方案。

Method: 提出TITAN框架，包含感知抽象游戏状态、优化动作、长时推理及使用LLM预言机检测漏洞等四个关键组件。

Result: 在两个商业MMORPG上实验，TITAN任务完成率达95%，检测性能高，发现四个未知漏洞，消融实验表明各组件对性能贡献大。

Conclusion: TITAN为推进智能通用测试系统提供新方向，具有实际应用价值。

Abstract: Testing MMORPGs (Massively Multiplayer Online Role-Playing Games) is a
critical yet labor-intensive task in game development due to their complexity
and frequent updating nature. Traditional automated game testing approaches
struggle to achieve high state coverage and efficiency in these rich,
open-ended environments, while existing LLM-based game-playing approaches are
limited to shallow reasoning ability in understanding complex game state-action
spaces and long-complex tasks. To address these challenges, we propose TITAN,
an effective LLM-driven agent framework for intelligent MMORPG testing. TITAN
incorporates four key components to: (1) perceive and abstract high-dimensional
game states, (2) proactively optimize and prioritize available actions, (3)
enable long-horizon reasoning with action trace memory and reflective
self-correction, and (4) employ LLM-based oracles to detect potential
functional and logic bugs with diagnostic reports.
  We implement the prototype of TITAN and evaluate it on two large-scale
commercial MMORPGs spanning both PC and mobile platforms. In our experiments,
TITAN achieves significantly higher task completion rates (95%) and bug
detection performance compared to existing automated game testing approaches.
An ablation study further demonstrates that each core component of TITAN
contributes substantially to its overall performance. Notably, TITAN detects
four previously unknown bugs that prior testing approaches fail to identify. We
provide an in-depth discussion of these results, which offer guidance for new
avenues of advancing intelligent, general-purpose testing systems. Moreover,
TITAN has been deployed in eight real-world game QA pipelines, underscoring its
practical impact as an LLM-driven game testing framework.

</details>


### [285] [Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries](https://arxiv.org/abs/2509.22202)
*Lukas Twist,Jie M. Zhang,Mark Harman,Helen Yannakoudakis*

Main category: cs.SE

TL;DR: 研究用户级提示变化对大语言模型生成代码时库幻觉的影响，发现模型易受提示变化影响，需防护措施。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成代码存在库幻觉问题，且缺乏真实世界提示变化对幻觉率影响的研究。

Method: 评估六种不同大语言模型，研究两种幻觉类型，考察开发者论坛提取的语言和不同程度用户错误对幻觉率的影响。

Result: 发现系统漏洞，如单字符拼写错误、虚假库名、时间相关提示会引发高比例幻觉，提示工程有缓解作用但不稳定。

Conclusion: 强调大语言模型易受自然提示变化影响，迫切需要防范库相关幻觉及潜在利用的保障措施。

Abstract: Large language models (LLMs) are increasingly used to generate code, yet they
continue to hallucinate, often inventing non-existent libraries. Such library
hallucinations are not just benign errors: they can mislead developers, break
builds, and expose systems to supply chain threats such as slopsquatting.
Despite increasing awareness of these risks, little is known about how
real-world prompt variations affect hallucination rates. Therefore, we present
the first systematic study of how user-level prompt variations impact library
hallucinations in LLM-generated code. We evaluate six diverse LLMs across two
hallucination types: library name hallucinations (invalid imports) and library
member hallucinations (invalid calls from valid libraries). We investigate how
realistic user language extracted from developer forums and how user errors of
varying degrees (one- or multi-character misspellings and completely fake
names/members) affect LLM hallucination rates. Our findings reveal systemic
vulnerabilities: one-character misspellings in library names trigger
hallucinations in up to 26% of tasks, fake library names are accepted in up to
99% of tasks, and time-related prompts lead to hallucinations in up to 84% of
tasks. Prompt engineering shows promise for mitigating hallucinations, but
remains inconsistent and LLM-dependent. Our results underscore the fragility of
LLMs to natural prompt variation and highlight the urgent need for safeguards
against library-related hallucinations and their potential exploitation.

</details>


### [286] [Green Prompt Engineering: Investigating the Energy Impact of Prompt Design in Software Engineering](https://arxiv.org/abs/2509.22320)
*Vincenzo De Martino,Mohammad Amin Zadenoori,Xavier Franch,Alessio Ferrari*

Main category: cs.SE

TL;DR: 本文提出绿色提示工程，研究提示可读性对环境可持续性和性能的影响，发现其存在权衡关系，为从业者和研究者提供方向。


<details>
  <summary>Details</summary>
Motivation: 语言模型推理引发环境担忧，此前研究较少关注语言复杂性作为可持续性因素。

Method: 使用开源小语言模型对需求分类进行实证研究，改变提示的可读性。

Result: 可读性影响环境可持续性和性能，二者存在权衡关系。

Conclusion: 简单提示可降低能源成本且不显著降低F1分数，为可持续提示设计研究开辟道路。

Abstract: Language Models are increasingly applied in software engineering, yet their
inference raises growing environmental concerns. Prior work has examined
hardware choices and prompt length, but little attention has been paid to
linguistic complexity as a sustainability factor. This paper introduces Green
Prompt Engineering, framing linguistic complexity as a design dimension that
can influence energy consumption and performance. We conduct an empirical study
on requirement classification using open-source Small Language Models, varying
the readability of prompts. Our results reveal that readability affects
environmental sustainability and performance, exposing trade-offs between them.
For practitioners, simpler prompts can reduce energy costs without a
significant F1-score loss; for researchers, it opens a path toward guidelines
and studies on sustainable prompt design within the Green AI agenda.

</details>


### [287] [GPU-Accelerated Loopy Belief Propagation for Program Analysis](https://arxiv.org/abs/2509.22337)
*Haoyu Feng,Xin Zhang*

Main category: cs.SE

TL;DR: 本文提出用于程序分析的GPU加速LBP算法，支持灵活更新策略，实验显示有显著加速效果且精度高。


<details>
  <summary>Details</summary>
Motivation: LBP应用于大规模程序分析有计算挑战，现有GPU方法缺乏灵活更新策略且未集成逻辑约束，实际性能不佳。

Method: 提出统一表示指定任意用户定义更新策略及依赖分析算法，利用Horn子句局部结构分组消息以减少warp分歧、利用GPU资源。

Result: 在八个真实Java程序的数据竞争分析实验中，比现有顺序方法平均加速2.14倍，比现有GPU方法加速5.56倍，且保持高精度。

Conclusion: 所提GPU加速LBP算法在程序分析中能实现有效加速，且支持灵活更新策略。

Abstract: Loopy Belief Propagation (LBP) is a widely used approximate inference
algorithm in probabilistic graphical models, with applications in computer
vision, error correction codes, protein folding, program analysis, etc.
However, LBP faces significant computational challenges when applied to
large-scale program analysis. While GPU (Graphics Processing Unit) parallel
computing provides a promising solution, existing approaches lack support for
flexible update strategies and have yet to integrate logical constraints with
GPU acceleration, leading to suboptimal practical performance.
  This paper presents a GPU-accelerated LBP algorithm for program analysis. To
support the diverse update strategies required by users, we propose a unified
representation for specifying arbitrary user-defined update strategies, along
with a dependency analysis algorithm. Furthermore, building on previous work
that leverages the local structure of Horn clauses to simplify message passing,
we group messages to minimize warp divergence and better utilize GPU resources.
Experimental results on datarace analysis over eight real-world Java programs
show that our approach achieves an average speedup of $2.14\times$ over the
state-of-the-art sequential approach and $5.56\times$ over the state-of-the-art
GPU-based approach, while maintaining high accuracy.

</details>


### [288] [A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems](https://arxiv.org/abs/2509.22379)
*Stefano Carlo Lambertenghi,Mirena Flores Valdez,Andrea Stocco*

Main category: cs.SE

TL;DR: 本文对四种自动驾驶系统测试模式进行实证研究，评估现实差距，发现MR测试优势，为系统验证提供见解。


<details>
  <summary>Details</summary>
Motivation: 模拟测试存在现实差距，影响测试结果向部署系统的可转移性，需研究不同测试模式。

Method: 使用配备传感器的小型实体车辆及其数字孪生，在多样室内驾驶场景中实现四种测试模式，评估两种自动驾驶系统架构，从三个维度评估现实差距。

Result: SiL和ViL简化现实动态和传感，MR测试提高感知真实度且不影响安全和控制，确定故障不跨模式转移的条件及差距维度。

Conclusion: 研究揭示各测试模式优缺点，为自动驾驶系统更可靠和可转移的验证指明方向。

Abstract: Simulation-based testing is a cornerstone of Autonomous Driving System (ADS)
development, offering safe and scalable evaluation across diverse driving
scenarios. However, discrepancies between simulated and real-world behavior,
known as the reality gap, challenge the transferability of test results to
deployed systems. In this paper, we present a comprehensive empirical study
comparing four representative testing modalities: Software-in-the-Loop (SiL),
Vehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing.
Using a small-scale physical vehicle equipped with real sensors (camera and
LiDAR) and its digital twin, we implement each setup and evaluate two ADS
architectures (modular and end-to-end) across diverse indoor driving scenarios
involving real obstacles, road topologies, and indoor environments. We
systematically assess the impact of each testing modality along three
dimensions of the reality gap: actuation, perception, and behavioral fidelity.
Our results show that while SiL and ViL setups simplify critical aspects of
real-world dynamics and sensing, MR testing improves perceptual realism without
compromising safety or control. Importantly, we identify the conditions under
which failures do not transfer across testing modalities and isolate the
underlying dimensions of the gap responsible for these discrepancies. Our
findings offer actionable insights into the respective strengths and
limitations of each modality and outline a path toward more robust and
transferable validation of autonomous driving systems.

</details>


### [289] [Context-Specific Instruction: A Longitudinal Study on Debugging Skill Acquisition and Retention for Novice Programmers](https://arxiv.org/abs/2509.22420)
*Ziyi Zhang,Devjeet Roy,Venera Arnaoudova*

Main category: cs.SE

TL;DR: 研究针对新手的不同错误定位教学方法，发现特定上下文指令在正确性、完成时间、压力和满意度方面表现更佳，能让新手更快掌握技能。


<details>
  <summary>Details</summary>
Motivation: 新手缺乏系统的错误定位方法，以往工作对特定上下文指令的影响研究不明确。

Method: 进行为期八周的纵向研究，设置无指导、抽象指导、具体步骤和特定上下文指令四种条件，让本科生完成调试任务，测量正确性、完成时间、自我感知分数。

Result: 特定上下文指令组（G4）正确性更高、完成时间更短，压力更低、满意度更高，参与者能通过上下文示例内化策略。

Conclusion: 特定上下文指令比抽象指导或无上下文步骤能让新手更快掌握技能且保留效果更好，结合上下文示例和抽象原则可缩小错误定位教育中的理论与实践差距。

Abstract: Bug localization is a critical skill, yet novices often lack systematic
approaches. Prior work tested abstract guidelines and general concrete steps;
the impact of context-specific instruction is unclear. We ran an eight-week
longitudinal study with four conditions: no instruction (G1), abstract
guidelines (G2), concrete steps (G3), and our context-specific instruction that
pairs concrete bug-localization steps with problem-specific details (G4).
Forty-four undergraduates participated; 41 completed all five sessions (S1-S5).
Each session included 2-3 debugging tasks to identify the minimal code element
containing a seeded logical fault. We measured correctness (binary), time to
completion, self-perceived scores (stress, difficulty, satisfaction, and
strategy adherence). G4 achieved higher correctness and shorter time to
completion: it reached 80% correctness after one session (vs. 20-44% for other
groups) and maintained 80% after three weeks, outperforming all groups (p <
0.05); its time to completion stabilized at 13-15 minutes in S1, whereas other
groups took 2-3 sessions to stabilize at 22-27 minutes. Qualitative responses
showed lower stress and higher satisfaction in G4, with participants
internalizing strategies via contextual examples. We conclude that
context-specific instruction yields faster skill acquisition and stronger
retention than abstract guidelines or context-agnostic steps. Even 1-2 sessions
produced significant gains, while extended practice optimized and stabilized
performance. Integrating contextual examples with abstract principles may
bridge theory-practice gaps in bug-localization education and provide a more
equitable path for novices.

</details>


### [290] [TreeMind: Automatically Reproducing Android Bug Reports via LLM-empowered Monte Carlo Tree Search](https://arxiv.org/abs/2509.22431)
*Zhengyu Chen,Zhaoyi Meng,Wenxiang Zhao,Wansen Wang,Haoyang Zhao,Jiahao Zhan,Jie Cui,Hong Zhong*

Main category: cs.SE

TL;DR: 提出TreeMind技术结合LLM与定制MCTS算法进行安卓应用程序崩溃复现，在93个真实世界的安卓错误报告数据集上表现优于四个基准方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习或大语言模型的安卓应用崩溃复现方法在报告不完整和现代UI组合复杂度高的场景下存在局限性，缺乏目标导向的推理和规划能力。

Method: 将复现任务构建为目标驱动的搜索问题，以MCTS为核心规划机制，引入两个具有不同角色的LLM引导代理（Expander和Simulator），结合多模态UI输入和高级提示技术进行反馈感知导航。

Result: 在93个真实世界的安卓错误报告数据集上进行评估，TreeMind的复现成功率显著优于四个最先进的基准方法。

Conclusion: 将LLM推理与基于MCTS的规划相结合是自动错误复现的一个有吸引力的方向。

Abstract: Automatically reproducing Android app crashes from textual bug reports is
challenging, particularly when the reports are incomplete and the modern UI
exhibits high combinatorial complexity. Existing approaches based on
reinforcement learning or large language models (LLMs) exhibit limitations in
such scenarios. They struggle to infer unobserved steps and reconstruct the
underlying user action sequences to navigate the vast UI interaction space,
primarily due to limited goal-directed reasoning and planning. We present
TreeMind, a novel technique that integrates LLMs with a customized Monte Carlo
Tree Search (MCTS) algorithm to achieve strategic UI exploration in bug
reproduction. To the best of our knowledge, this is the first work to combine
external decision-making with LLM semantic reasoning for reliable bug
reproduction. We formulate the reproduction task as a target-driven search
problem, leveraging MCTS as the core planning mechanism to iteratively refine
action sequences. To enhance MCTS with semantic reasoning, we introduce two
LLM-guided agents with distinct roles: Expander generates top-k promising
actions based on the current UI state and exploration history, while Simulator
estimates the likelihood that each action leads toward successful reproduction.
By incorporating multi-modal UI inputs and advanced prompting techniques,
TreeMind conducts feedback-aware navigation that identifies missing but
essential user actions and incrementally reconstructs the reproduction paths.
We evaluate TreeMind on a dataset of 93 real-world Android bug reports from
three widely-used benchmarks. Experimental results show that it significantly
outperforms four state-of-the-art baselines in reproduction success rate. A
real-world case study indicates that integrating LLM reasoning with MCTS-based
planning is a compelling direction for automated bug reproduction.

</details>


### [291] [Boosting Pointer Analysis With Large Language Model-Enhanced Allocation Function Detection](https://arxiv.org/abs/2509.22530)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Peng Di,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: 提出AFD技术增强指针分析，通过自动识别和建模自定义分配函数，在15个C项目评估效果好，证明精确建模自定义分配函数可提升指针分析。


<details>
  <summary>Details</summary>
Motivation: 现有指针分析因对堆分配建模不精确，尤其在C/C++程序中忽视自定义分配函数，导致别名分析粗糙、精度降低。

Method: 采用混合方法，用值流分析检测简单包装器，利用大语言模型推理有副作用的复杂分配模式。

Result: 在15个真实C项目中识别超600个自定义AFs，集成AFD使建模堆对象增加26倍、别名集大小减少39%，运行时开销1.4倍，改善间接调用解析，发现17个未检测到的内存漏洞。

Conclusion: 精确建模自定义分配函数为大型软件系统中提升指针分析提供可扩展且实用的途径。

Abstract: Pointer analysis is foundational for many static analysis tasks, yet its
effectiveness is often hindered by imprecise modeling of heap allocations,
particularly in C/C++ programs where user-defined allocation functions (AFs)
are pervasive. Existing approaches largely overlook these custom allocators,
leading to coarse aliasing and reduced analysis precision. In this paper, we
present AFD, a novel technique that enhances pointer analysis by automatically
identifying and modeling custom allocation functions. AFD employs a hybrid
approach: it uses value-flow analysis to detect straightforward wrappers and
leverages Large Language Models (LLMs) to reason about more complex allocation
patterns with side effects. This targeted enhancement enables precise modeling
of heap objects at each call site, achieving context-sensitivity-like benefits
without the associated overhead. We evaluate AFD on 15 real-world C projects,
identifying over 600 custom AFs. Integrating AFD into a baseline pointer
analysis yields a 26x increase in modeled heap objects and a 39% reduction in
alias set sizes, with only 1.4x runtime overhead. Furthermore, our enhanced
analysis improves indirect call resolution and uncovers 17 previously
undetected memory bugs. These results demonstrate that precise modeling of
custom allocation functions offers a scalable and practical path to improving
pointer analysis in large software systems.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [292] [Optimal Consumption-Investment with Epstein-Zin Utility under Leverage Constraint](https://arxiv.org/abs/2509.21929)
*Dejian Tian,Weidong Tian,Jianjun Zhou,Zimu Zhu*

Main category: q-fin.PM

TL;DR: 研究含一般杠杆约束下Epstein - Zin递归效用的最优投资组合选择，建立HJB方程解，刻画策略并分析与经典情形差异。


<details>
  <summary>Details</summary>
Motivation: 研究含一般杠杆约束下Epstein - Zin递归效用的最优投资组合选择问题。

Method: 开发带约束的新动态规划原理，建立HJB方程；推导最优策略显式解。

Result: 证明最优值函数是HJB方程唯一粘性解，刻画了最优消费和投资策略，给出特殊情形下最优策略显式解及约束和无约束区域。

Conclusion: 与经典时间可分偏好和无杠杆约束情形有差异。

Abstract: We study optimal portfolio choice under Epstein-Zin recursive utility in the
presence of general leverage constraints. We first establish that the optimal
value function is the unique viscosity solution to the associated
Hamilton-Jacobi-Bellman (HJB) equation, by developing a new dynamic programming
principle under constraints. We further demonstrate that the value function
admits smoothness and characterize the optimal consumption and investment
strategies. In addition, we derive explicit solutions for the optimal strategy
and explicitly delineate the constrained and unconstrained regions in several
special cases of the leverage constraint. Finally, we conduct a comparative
analysis, highlighting the differences relative to the classical time-separable
preferences and to the setting without leverage constraints.

</details>


### [293] [Factor-Based Conditional Diffusion Model for Portfolio Optimization](https://arxiv.org/abs/2509.22088)
*Xuefeng Gao,Mengying He,Xuedong He*

Main category: q-fin.PM

TL;DR: 提出用于投资组合优化的条件扩散模型，在A股市场表现优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 为投资组合优化提供更有效的方法，学习股票回报的横截面分布。

Method: 提出基于带标记条件的Diffusion Transformer的条件扩散模型，用生成的回报样本进行每日均值 - 方差优化。

Result: 在中国A股市场的实证结果显示，该方法在多个指标上持续优于基于标准经验和收缩估计的基准方法。

Conclusion: 所提出的条件扩散模型在投资组合优化方面是有效的，优于基准方法。

Abstract: We propose a novel conditional diffusion model for portfolio optimization
that learns the cross-sectional distribution of next-day stock returns
conditioned on asset-specific factors. The model builds on the Diffusion
Transformer with token-wise conditioning, linking each asset's return to its
own factor vector while capturing cross-asset dependencies. Generated return
samples are used for daily mean-variance optimization under realistic
constraints. Empirical results on the Chinese A-share market show that our
approach consistently outperforms benchmark methods based on standard empirical
and shrinkage-based estimators across multiple metrics.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [294] [Near-Optimal Experiment Design in Linear non-Gaussian Cyclic Models](https://arxiv.org/abs/2509.21423)
*Ehsan Sharifian,Saber Salehkaleybar,Negar Kiyavash*

Main category: stat.ML

TL;DR: 研究从线性非高斯结构方程模型的观测和干预数据中学习因果结构问题，给出图等价类组合特征，将最优实验设计形式化为自适应随机优化问题，提出采样估计器，模拟表明少量干预可恢复真实因果结构。


<details>
  <summary>Details</summary>
Motivation: 现有仅用观测数据识别因果图存在局限，只能到置换等价类，需研究如何结合干预数据学习因果结构。

Method: 通过二分图完美匹配对等价类进行组合表征，分析干预对匹配的影响，将最优实验设计任务形式化为自适应随机优化问题，提出基于随机匹配的采样估计器。

Result: 每个原子干预揭示真实匹配的一条边并消除不相容因果图，奖励函数具有自适应子模性，贪心策略有近似最优性能保证，模拟显示少量干预可恢复真实因果结构。

Conclusion: 所提出的随机优化框架结合少量干预能有效恢复真实的潜在因果结构。

Abstract: We study the problem of causal structure learning from a combination of
observational and interventional data generated by a linear non-Gaussian
structural equation model that might contain cycles. Recent results show that
using mere observational data identifies the causal graph only up to a
permutation-equivalence class. We obtain a combinatorial characterization of
this class by showing that each graph in an equivalence class corresponds to a
perfect matching in a bipartite graph. This bipartite representation allows us
to analyze how interventions modify or constrain the matchings. Specifically,
we show that each atomic intervention reveals one edge of the true matching and
eliminates all incompatible causal graphs. Consequently, we formalize the
optimal experiment design task as an adaptive stochastic optimization problem
over the set of equivalence classes with a natural reward function that
quantifies how many graphs are eliminated from the equivalence class by an
intervention. We show that this reward function is adaptive submodular and
provide a greedy policy with a provable near-optimal performance guarantee. A
key technical challenge is to efficiently estimate the reward function without
having to explicitly enumerate all the graphs in the equivalence class. We
propose a sampling-based estimator using random matchings and analyze its bias
and concentration behavior. Our simulation results show that performing a small
number of interventions guided by our stochastic optimization framework
recovers the true underlying causal structure.

</details>


### [295] [General Pruning Criteria for Fast SBL](https://arxiv.org/abs/2509.21572)
*Jakob Möderl,Erik Leitinger,Bernard Henri Fleury*

Main category: stat.ML

TL;DR: 分析弱化高斯假设下SBL边缘似然与单超参数关系，推导超参数有限和无限的条件，在高斯情况下与F - SBL剪枝条件一致。


<details>
  <summary>Details</summary>
Motivation: 在弱化SBL推导所基于的高斯假设下，深入分析SBL边缘似然与超参数的关系。

Method: 分析单超参数的边缘似然函数，推导超参数有限和无限的充分条件。

Result: 得到超参数有限和无限的条件，在高斯情况下与F - SBL剪枝条件互补且一致。

Conclusion: 为F - SBL算法提供了额外的见解。

Abstract: Sparse Bayesian learning (SBL) associates to each weight in the underlying
linear model a hyperparameter by assuming that each weight is Gaussian
distributed with zero mean and precision (inverse variance) equal to its
associated hyperparameter. The method estimates the hyperparameters by
marginalizing out the weights and performing (marginalized) maximum likelihood
(ML) estimation. SBL returns many hyperparameter estimates to diverge to
infinity, effectively setting the estimates of the corresponding weights to
zero (i.e., pruning the corresponding weights from the model) and thereby
yielding a sparse estimate of the weight vector.
  In this letter, we analyze the marginal likelihood as function of a single
hyperparameter while keeping the others fixed, when the Gaussian assumptions on
the noise samples and the weight distribution that underlies the derivation of
SBL are weakened. We derive sufficient conditions that lead, on the one hand,
to finite hyperparameter estimates and, on the other, to infinite ones.
Finally, we show that in the Gaussian case, the two conditions are
complementary and coincide with the pruning condition of fast SBL (F-SBL),
thereby providing additional insights into this algorithm.

</details>


### [296] [IndiSeek learns information-guided disentangled representations](https://arxiv.org/abs/2509.21584)
*Yu Gui,Cong Ma,Zongming Ma*

Main category: stat.ML

TL;DR: 提出IndiSeek用于多模态学习中解纠缠表示学习，在多个数据集验证有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中学习解纠缠表示很重要，现有基于互信息目标函数难以可靠估计，变分替代物效果不佳。

Method: 结合强制独立性目标和计算高效的重建损失来限制条件互信息，明确平衡独立性和完整性。

Result: 在合成模拟、CITE - seq数据集和多个真实世界多模态基准上证明了IndiSeek的有效性。

Conclusion: IndiSeek能有效解决多模态学习中解纠缠表示学习的挑战。

Abstract: Learning disentangled representations is a fundamental task in multi-modal
learning. In modern applications such as single-cell multi-omics, both shared
and modality-specific features are critical for characterizing cell states and
supporting downstream analyses. Ideally, modality-specific features should be
independent of shared ones while also capturing all complementary information
within each modality. This tradeoff is naturally expressed through
information-theoretic criteria, but mutual-information-based objectives are
difficult to estimate reliably, and their variational surrogates often
underperform in practice. In this paper, we introduce IndiSeek, a novel
disentangled representation learning approach that addresses this challenge by
combining an independence-enforcing objective with a computationally efficient
reconstruction loss that bounds conditional mutual information. This
formulation explicitly balances independence and completeness, enabling
principled extraction of modality-specific features. We demonstrate the
effectiveness of IndiSeek on synthetic simulations, a CITE-seq dataset and
multiple real-world multi-modal benchmarks.

</details>


### [297] [Effective continuous equations for adaptive SGD: a stochastic analysis view](https://arxiv.org/abs/2509.21614)
*Luca Callisti,Marco Romito,Francesco Triggiano*

Main category: stat.ML

TL;DR: 对小学习率下流行自适应SGD方法进行理论分析，推导有效连续随机动力学，研究学习率与超参数缩放规则。


<details>
  <summary>Details</summary>
Motivation: 对小学习率下流行自适应SGD方法进行理论分析。

Method: 使用Li等人引入的随机修正方程框架推导有效连续随机动力学，扩展Malladi等人的方法研究缩放规则。

Result: SGD中采样引起的噪声在极限下表现为驱动参数和梯度二阶动量演化的独立布朗运动。

Conclusion: 刻画了自适应方法中所有非平凡的极限动力学。

Abstract: We present a theoretical analysis of some popular adaptive Stochastic
Gradient Descent (SGD) methods in the small learning rate regime. Using the
stochastic modified equations framework introduced by Li et al., we derive
effective continuous stochastic dynamics for these methods. Our key
contribution is that sampling-induced noise in SGD manifests in the limit as
independent Brownian motions driving the parameter and gradient second momentum
evolutions. Furthermore, extending the approach of Malladi et al., we
investigate scaling rules between the learning rate and key hyperparameters in
adaptive methods, characterising all non-trivial limiting dynamics.

</details>


### [298] [SADA: Safe and Adaptive Inference with Multiple Black-Box Predictions](https://arxiv.org/abs/2509.21707)
*Jiawei Shan,Yiming Dong,Jiwei Zhao*

Main category: stat.ML

TL;DR: 本文提出一种新方法聚合多黑盒预测，保证性能且实验验证有效


<details>
  <summary>Details</summary>
Motivation: 现实应用中标记数据稀缺，未标记数据丰富，需安全有效聚合多模型预测

Method: 提出新方法安全自适应聚合多黑盒预测，有性能保证

Result: 在合成和基准数据集实验验证了算法有效性

Conclusion: 所提方法能安全自适应聚合多黑盒预测，有良好性能保证

Abstract: Real-world applications often face scarce labeled data due to the high cost
and time requirements of gold-standard experiments, whereas unlabeled data are
typically abundant. With the growing adoption of machine learning techniques,
it has become increasingly feasible to generate multiple predicted labels using
a variety of models and algorithms, including deep learning, large language
models, and generative AI. In this paper, we propose a novel approach that
safely and adaptively aggregates multiple black-box predictions with unknown
quality while preserving valid statistical inference. Our method provides two
key guarantees: (i) it never performs worse than using the labeled data alone,
regardless of the quality of the predictions; and (ii) if any one of the
predictions (without knowing which one) perfectly fits the ground truth, the
algorithm adaptively exploits this to achieve either a faster convergence rate
or the semiparametric efficiency bound. We demonstrate the effectiveness of the
proposed algorithm through experiments on both synthetic and benchmark
datasets.

</details>


### [299] [Multi-modal Bayesian Neural Network Surrogates with Conjugate Last-Layer Estimation](https://arxiv.org/abs/2509.21711)
*Ian Taylor,Juliane Mueller,Julie Bessac*

Main category: stat.ML

TL;DR: 随着数据收集和模拟能力提升，多模态学习愈发重要。本文开发两种多模态贝叶斯神经网络代理模型，用共轭随机变分推断估计参数，对缺失数据也适用，相比单模态模型有更好预测准确性和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 多模态学习是重要研究领域，多模态代理模型在多模态数据可用时能辅助外循环应用，有必要开发相关模型。

Method: 开发两种多模态贝叶斯神经网络代理模型，利用最后一层条件共轭分布，用随机变分推断（SVI）估计模型参数，提供处理部分缺失观测的共轭SVI估计方法。

Result: 在标量和时间序列数据上，相比单模态代理模型，预测准确性和不确定性量化得到改善。

Conclusion: 所开发的多模态贝叶斯神经网络代理模型有效，能在多模态数据场景中取得比单模态模型更好效果。

Abstract: As data collection and simulation capabilities advance, multi-modal learning,
the task of learning from multiple modalities and sources of data, is becoming
an increasingly important area of research. Surrogate models that learn from
data of multiple auxiliary modalities to support the modeling of a highly
expensive quantity of interest have the potential to aid outer loop
applications such as optimization, inverse problems, or sensitivity analyses
when multi-modal data are available. We develop two multi-modal Bayesian neural
network surrogate models and leverage conditionally conjugate distributions in
the last layer to estimate model parameters using stochastic variational
inference (SVI). We provide a method to perform this conjugate SVI estimation
in the presence of partially missing observations. We demonstrate improved
prediction accuracy and uncertainty quantification compared to uni-modal
surrogate models for both scalar and time series data.

</details>


### [300] [Causal-EPIG: A Prediction-Oriented Active Learning Framework for CATE Estimation](https://arxiv.org/abs/2509.21866)
*Erdun Gao,Jake Fawkes,Dino Sejdinovic*

Main category: stat.ML

TL;DR: 论文提出因果目标对齐原则，通过Causal - EPIG框架解决传统主动学习策略目标不匹配问题，推导两种策略，实验表明策略表现优且最优策略依赖上下文。


<details>
  <summary>Details</summary>
Motivation: 传统主动学习策略在估计CATE时存在目标不匹配问题，不能直接针对不可观测的因果量。

Method: 引入因果目标对齐原则，构建Causal - EPIG框架，将EPIG信息论准则用于量化查询价值，推导两种不同策略。

Result: 实验显示提出的策略始终优于标准基线，且最优策略依赖基础估计器和数据复杂度。

Conclusion: 框架为实际中样本高效的CATE估计提供了原则性指导。

Abstract: Estimating the Conditional Average Treatment Effect (CATE) is often
constrained by the high cost of obtaining outcome measurements, making active
learning essential. However, conventional active learning strategies suffer
from a fundamental objective mismatch. They are designed to reduce uncertainty
in model parameters or in observable factual outcomes, failing to directly
target the unobservable causal quantities that are the true objects of
interest. To address this misalignment, we introduce the principle of causal
objective alignment, which posits that acquisition functions should target
unobservable causal quantities, such as the potential outcomes and the CATE,
rather than indirect proxies. We operationalize this principle through the
Causal-EPIG framework, which adapts the information-theoretic criterion of
Expected Predictive Information Gain (EPIG) to explicitly quantify the value of
a query in terms of reducing uncertainty about unobservable causal quantities.
From this unified framework, we derive two distinct strategies that embody a
fundamental trade-off: a comprehensive approach that robustly models the full
causal mechanisms via the joint potential outcomes, and a focused approach that
directly targets the CATE estimand for maximum sample efficiency. Extensive
experiments demonstrate that our strategies consistently outperform standard
baselines, and crucially, reveal that the optimal strategy is
context-dependent, contingent on the base estimator and data complexity. Our
framework thus provides a principled guide for sample-efficient CATE estimation
in practice.

</details>


### [301] [Sequential 1-bit Mean Estimation with Near-Optimal Sample Complexity](https://arxiv.org/abs/2509.21940)
*Ivan Lau,Jonathan Scarlett*

Main category: stat.ML

TL;DR: 研究1 - 比特通信约束下分布式均值估计问题，提出基于区间查询的均值估计器，推导样本复杂度界等。


<details>
  <summary>Details</summary>
Motivation: 解决1 - 比特通信约束下的分布式均值估计问题。

Method: 提出基于（随机且顺序选择的）区间查询的均值估计器，其1 - 比特输出表明样本是否在指定区间。

Result: 得到样本复杂度界，建立区间查询估计器的适应性差距，给出更强尾部衰减分布的样本复杂度界及处理未知采样预算等变体。

Conclusion: 所提估计器有较好性能，样本复杂度界与未量化设置的极小极大下界匹配，自适应估计器在大λ/σ时表现更好。

Abstract: In this paper, we study the problem of distributed mean estimation with 1-bit
communication constraints. We propose a mean estimator that is based on
(randomized and sequentially-chosen) interval queries, whose 1-bit outcome
indicates whether the given sample lies in the specified interval. Our
estimator is $(\epsilon, \delta)$-PAC for all distributions with bounded mean
($-\lambda \le \mathbb{E}(X) \le \lambda $) and variance ($\mathrm{Var}(X) \le
\sigma^2$) for some known parameters $\lambda$ and $\sigma$. We derive a sample
complexity bound $\widetilde{O}\big(
\frac{\sigma^2}{\epsilon^2}\log\frac{1}{\delta} +
\log\frac{\lambda}{\sigma}\big)$, which matches the minimax lower bound for the
unquantized setting up to logarithmic factors and the additional
$\log\frac{\lambda}{\sigma}$ term that we show to be unavoidable. We also
establish an adaptivity gap for interval-query based estimators: the best
non-adaptive mean estimator is considerably worse than our adaptive mean
estimator for large $\frac{\lambda}{\sigma}$. Finally, we give tightened sample
complexity bounds for distributions with stronger tail decay, and present
additional variants that (i) handle an unknown sampling budget (ii) adapt to
the unknown true variance given (possibly loose) upper and lower bounds on the
variance, and (iii) use only two stages of adaptivity at the expense of more
complicated (non-interval) queries.

</details>


### [302] [A Nonparametric Discrete Hawkes Model with a Collapsed Gaussian-Process Prior](https://arxiv.org/abs/2509.21996)
*Trinnhallen Brisley,Gordon Ross,Daniel Paulin*

Main category: stat.ML

TL;DR: 提出高斯过程离散霍克斯过程（GP - DHP），能实现灵活的离散时间自激发，兼顾可扩展性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 离散时间霍克斯模型未充分使用，缺乏对基线和激励的灵活非参数处理方法。

Method: 提出GP - DHP，在基线和激励上设置高斯过程先验，通过塌缩潜表示进行推理，有近线性时间复杂度。

Result: 模拟中恢复多种激励形状和演变基线；案例研究中改善测试预测对数似然。

Conclusion: 灵活的离散时间自激发可在不牺牲可扩展性和可解释性的情况下实现。

Abstract: Hawkes process models are used in settings where past events increase the
likelihood of future events occurring. Many applications record events as
counts on a regular grid, yet discrete-time Hawkes models remain comparatively
underused and are often constrained by fixed-form baselines and excitation
kernels. In particular, there is a lack of flexible, nonparametric treatments
of both the baseline and the excitation in discrete time. To this end, we
propose the Gaussian Process Discrete Hawkes Process (GP-DHP), a nonparametric
framework that places Gaussian process priors on both the baseline and the
excitation and performs inference through a collapsed latent representation.
This yields smooth, data-adaptive structure without prespecifying trends,
periodicities, or decay shapes, and enables maximum a posteriori (MAP)
estimation with near-linear-time \(O(T\log T)\) complexity. A closed-form
projection recovers interpretable baseline and excitation functions from the
optimized latent trajectory. In simulations, GP-DHP recovers diverse excitation
shapes and evolving baselines. In case studies on U.S. terrorism incidents and
weekly Cryptosporidiosis counts, it improves test predictive log-likelihood
over standard parametric discrete Hawkes baselines while capturing bursts,
delays, and seasonal background variation. The results indicate that flexible
discrete-time self-excitation can be achieved without sacrificing scalability
or interpretability.

</details>


### [303] [A Random Matrix Perspective of Echo State Networks: From Precise Bias--Variance Characterization to Optimal Regularization](https://arxiv.org/abs/2509.22011)
*Yessin Moakher,Malik Tiomoko,Cosme Louart,Zhenyu Liao*

Main category: stat.ML

TL;DR: 对教师学生设置下的回声状态网络进行渐近分析，得出相关误差表达式，揭示与经典岭回归差异，给出最优正则化公式和计算方案。


<details>
  <summary>Details</summary>
Motivation: 为回声状态网络（ESNs）提供可解释的理论和实用调优指南，调和近期实证观察与可证明的性能保证。

Method: 利用随机矩阵理论进行严格的渐近分析。

Result: 得出渐近偏差、方差和均方误差的闭式表达式，揭示ESNs与经典岭回归的两个关键差异，给出特定情况下最优正则化公式和通用情况下的计算方案。

Conclusion: 研究结果为ESNs调优提供理论和实践指导。

Abstract: We present a rigorous asymptotic analysis of Echo State Networks (ESNs) in a
teacher student setting with a linear teacher with oracle weights. Leveraging
random matrix theory, we derive closed form expressions for the asymptotic
bias, variance, and mean-squared error (MSE) as functions of the input
statistics, the oracle vector, and the ridge regularization parameter. The
analysis reveals two key departures from classical ridge regression: (i) ESNs
do not exhibit double descent, and (ii) ESNs attain lower MSE when both the
number of training samples and the teacher memory length are limited. We
further provide an explicit formula for the optimal regularization in the
identity input covariance case, and propose an efficient numerical scheme to
compute the optimum in the general case. Together, these results offer
interpretable theory and practical guidelines for tuning ESNs, helping
reconcile recent empirical observations with provable performance guarantees

</details>


### [304] [Incorporating priors in learning: a random matrix study under a teacher-student framework](https://arxiv.org/abs/2509.22124)
*Malik Tiomoko,Ekkehard Schnoor*

Main category: stat.ML

TL;DR: 本文对带高斯先验的最大后验回归的训练和测试风险进行精确渐近刻画，统一多种估计器，给出闭式风险公式，确定测试风险极小值，模拟验证理论。


<details>
  <summary>Details</summary>
Motivation: 正则化线性回归在高维且有信息先验的情况下行为尚不明确，需深入研究。

Method: 使用随机矩阵理论。

Result: 得到闭式风险公式，揭示偏差 - 方差 - 先验权衡、解释双重下降、量化先验失配，确定测试风险闭式极小值和最优正则化参数的简单估计器，模拟高精度验证理论。

Conclusion: 研究结果连接了贝叶斯先验、经典正则化和现代渐近理论，为利用结构化先验知识学习提供概念清晰性和实践指导。

Abstract: Regularized linear regression is central to machine learning, yet its
high-dimensional behavior with informative priors remains poorly understood. We
provide the first exact asymptotic characterization of training and test risks
for maximum a posteriori (MAP) regression with Gaussian priors centered at a
domain-informed initialization. Our framework unifies ridge regression, least
squares, and prior-informed estimators, and -- using random matrix theory --
yields closed-form risk formulas that expose the bias-variance-prior tradeoff,
explain double descent, and quantify prior mismatch. We also identify a
closed-form minimizer of test risk, enabling a simple estimator of the optimal
regularization parameter. Simulations confirm the theory with high accuracy. By
connecting Bayesian priors, classical regularization, and modern asymptotics,
our results provide both conceptual clarity and practical guidance for learning
with structured prior knowledge.

</details>


### [305] [Preventing Model Collapse Under Overparametrization: Optimal Mixing Ratios for Interpolation Learning and Ridge Regression](https://arxiv.org/abs/2509.22341)
*Anvit Garg,Sohom Bhattacharya,Pragya Sur*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Model collapse occurs when generative models degrade after repeatedly
training on their own synthetic outputs. We study this effect in
overparameterized linear regression in a setting where each iteration mixes
fresh real labels with synthetic labels drawn from the model fitted in the
previous iteration. We derive precise generalization error formulae for
minimum-$\ell_2$-norm interpolation and ridge regression under this iterative
scheme. Our analysis reveals intriguing properties of the optimal mixing weight
that minimizes long-term prediction error and provably prevents model collapse.
For instance, in the case of min-$\ell_2$-norm interpolation, we establish that
the optimal real-data proportion converges to the reciprocal of the golden
ratio for fairly general classes of covariate distributions. Previously, this
property was known only for ordinary least squares, and additionally in low
dimensions. For ridge regression, we further analyze two popular model classes
-- the random-effects model and the spiked covariance model -- demonstrating
how spectral geometry governs optimal weighting. In both cases, as well as for
isotropic features, we uncover that the optimal mixing ratio should be at least
one-half, reflecting the necessity of favoring real-data over synthetic. We
validate our theoretical results with extensive simulations.

</details>


### [306] [Multidimensional Uncertainty Quantification via Optimal Transport](https://arxiv.org/abs/2509.22380)
*Nikita Kotelevskii,Maiya Goloburda,Vladimir Kondratyev,Alexander Fishkov,Mohsen Guizani,Eric Moulines,Maxim Panov*

Main category: stat.ML

TL;DR: 提出VecUQ - OT算法，将互补UQ度量堆叠成向量，用最优传输排序，支持灵活不确定性融合，在多数据类型中高效。


<details>
  <summary>Details</summary>
Motivation: 多数UQ方法用单一标量衡量模型可靠性，不同UQ度量可提供互补信息，需多维视角。

Method: 将互补UQ度量堆叠成向量，用最优传输排序，VecUQ - OT算法使用熵正则化最优传输，在分布内数据上学传输映射。

Result: 框架支持灵活非加性不确定性融合，为下游任务提供鲁棒排序，在多数据类型中高效，代码开源。

Conclusion: VecUQ - OT算法有效，能在个体度量失效时仍保持高效，适用于多种下游任务。

Abstract: Most uncertainty quantification (UQ) approaches provide a single scalar value
as a measure of model reliability. However, different uncertainty measures
could provide complementary information on the prediction confidence. Even
measures targeting the same type of uncertainty (e.g., ensemble-based and
density-based measures of epistemic uncertainty) may capture different failure
modes.
  We take a multidimensional view on UQ by stacking complementary UQ measures
into a vector. Such vectors are assigned with Monge-Kantorovich ranks produced
by an optimal-transport-based ordering method. The prediction is then deemed
more uncertain than the other if it has a higher rank.
  The resulting VecUQ-OT algorithm uses entropy-regularized optimal transport.
The transport map is learned on vectors of scores from in-distribution data
and, by design, applies to unseen inputs, including out-of-distribution cases,
without retraining.
  Our framework supports flexible non-additive uncertainty fusion (including
aleatoric and epistemic components). It yields a robust ordering for downstream
tasks such as selective prediction, misclassification detection,
out-of-distribution detection, and selective generation. Across synthetic,
image, and text data, VecUQ-OT shows high efficiency even when individual
measures fail. The code for the method is available at:
https://github.com/stat-ml/multidimensional_uncertainty.

</details>


### [307] [Universal Inverse Distillation for Matching Models with Real-Data Supervision (No GANs)](https://arxiv.org/abs/2509.22459)
*Nikita Kornilov,David Li,Tikhon Mavrin,Aleksei Leonov,Nikita Gushchin,Evgeny Burnaev,Iaroslav Koshelev,Alexander Korotin*

Main category: stat.ML

TL;DR: 现代匹配模型推理慢，现有蒸馏方法有局限，本文提出RealUID通用蒸馏框架，无缝融合真实数据且无需GANs。


<details>
  <summary>Details</summary>
Motivation: 现代匹配模型推理慢，现有蒸馏方法局限于特定框架且需额外复杂对抗训练，需改进。

Method: 提出RealUID通用蒸馏框架，有简单理论基础，涵盖之前方法并可扩展。

Result: 文中未提及具体实验结果。

Conclusion: RealUID可作为通用蒸馏框架，无缝融合真实数据到蒸馏过程，无需GANs。

Abstract: While achieving exceptional generative quality, modern diffusion, flow, and
other matching models suffer from slow inference, as they require many steps of
iterative generation. Recent distillation methods address this by training
efficient one-step generators under the guidance of a pre-trained teacher
model. However, these methods are often constrained to only one specific
framework, e.g., only to diffusion or only to flow models. Furthermore, these
methods are naturally data-free, and to benefit from the usage of real data, it
is required to use an additional complex adversarial training with an extra
discriminator model. In this paper, we present RealUID, a universal
distillation framework for all matching models that seamlessly incorporates
real data into the distillation procedure without GANs. Our RealUID approach
offers a simple theoretical foundation that covers previous distillation
methods for Flow Matching and Diffusion models, and is also extended to their
modifications, such as Bridge Matching and Stochastic Interpolants.

</details>


### [308] [CausalKANs: interpretable treatment effect estimation with Kolmogorov-Arnold networks](https://arxiv.org/abs/2509.22467)
*Alejandro Almodóvar,Patricia A. Apellániz,Santiago Zazo,Juan Parras*

Main category: stat.ML

TL;DR: 提出causalKANs框架，将条件平均处理效应的神经估计器转化为KANs，兼顾预测准确性与可解释性，实验效果好并开源代码。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在估计异质处理效应时不透明，限制了其在敏感领域的信任和应用。

Method: 基于成熟且高性能的因果神经架构，提出causalKANs框架，结合剪枝和符号简化。

Result: 在基准数据集实验中，causalKANs在CATE误差指标上与神经基线表现相当，简单KAN变体也有有竞争力的表现。

Conclusion: causalKANs结合可靠性与分析可及性，为高风险场景下的个性化决策提供可审计的估计器。

Abstract: Deep neural networks achieve state-of-the-art performance in estimating
heterogeneous treatment effects, but their opacity limits trust and adoption in
sensitive domains such as medicine, economics, and public policy. Building on
well-established and high-performing causal neural architectures, we propose
causalKANs, a framework that transforms neural estimators of conditional
average treatment effects (CATEs) into Kolmogorov--Arnold Networks (KANs). By
incorporating pruning and symbolic simplification, causalKANs yields
interpretable closed-form formulas while preserving predictive accuracy.
Experiments on benchmark datasets demonstrate that causalKANs perform on par
with neural baselines in CATE error metrics, and that even simple KAN variants
achieve competitive performance, offering a favorable
accuracy--interpretability trade-off. By combining reliability with analytic
accessibility, causalKANs provide auditable estimators supported by closed-form
expressions and interpretable plots, enabling trustworthy individualized
decision-making in high-stakes settings. We release the code for
reproducibility at https://github.com/aalmodovares/causalkans .

</details>


### [309] [Smoothing-Based Conformal Prediction for Balancing Efficiency and Interpretability](https://arxiv.org/abs/2509.22529)
*Mingyi Zheng,Hongyu Jiang,Yizhou Lu,Jiaye Teng*

Main category: stat.ML

TL;DR: 提出SCD - split方法，结合平滑操作改进共形预测，实验和理论证明其能平衡区间长度和不连通子区间数量。


<details>
  <summary>Details</summary>
Motivation: 现有共形预测变体如CD - split产生的预测集由多个不连通子区间组成，难以解释，需要改进。

Method: 在共形预测框架中引入平滑操作，提出SCD - split方法。

Result: 在合成和真实数据集上的实验表明SCD - split能平衡区间长度和不连通子区间数量；理论上在特定条件下可减少不连通子区间数量，同时保持与CD - split相当的覆盖保证和区间长度。

Conclusion: SCD - split方法有效，能解决现有共形预测变体预测集难以解释的问题。

Abstract: Conformal Prediction (CP) is a distribution-free framework for constructing
statistically rigorous prediction sets. While popular variants such as CD-split
improve CP's efficiency, they often yield prediction sets composed of multiple
disconnected subintervals, which are difficult to interpret. In this paper, we
propose SCD-split, which incorporates smoothing operations into the CP
framework. Such smoothing operations potentially help merge the subintervals,
thus leading to interpretable prediction sets. Experimental results on both
synthetic and real-world datasets demonstrate that SCD-split balances the
interval length and the number of disconnected subintervals. Theoretically,
under specific conditions, SCD-split provably reduces the number of
disconnected subintervals while maintaining comparable coverage guarantees and
interval length compared with CD-split.

</details>


### [310] [Debiased Front-Door Learners for Heterogeneous Effects](https://arxiv.org/abs/2509.22531)
*Yonghan Jung*

Main category: stat.ML

TL;DR: 研究前门调整下的异质处理效应，引入两个去偏学习器，证明其快速、准神谕率，有误差分析和实证表现，表明能提供可靠且样本高效的估计。


<details>
  <summary>Details</summary>
Motivation: 在处理和结果存在未测量混杂因素但观察到的中介无混杂的观察性环境中，研究前门调整下的异质处理效应。

Method: 引入FD - DR - Learner和FD - R - Learner两个去偏学习器。

Result: 两个学习器即使在干扰函数收敛慢至n^-1/4时也能达到快速、准神谕率，在合成研究和真实案例研究中有稳健实证表现。

Conclusion: 所提出的学习器在前门场景中能提供可靠且样本高效的异质处理效应估计。

Abstract: In observational settings where treatment and outcome share unmeasured
confounders but an observed mediator remains unconfounded, the front-door (FD)
adjustment identifies causal effects through the mediator. We study the
heterogeneous treatment effect (HTE) under FD identification and introduce two
debiased learners: FD-DR-Learner and FD-R-Learner. Both attain fast,
quasi-oracle rates (i.e., performance comparable to an oracle that knows the
nuisances) even when nuisance functions converge as slowly as n^-1/4. We
provide error analyses establishing debiasedness and demonstrate robust
empirical performance in synthetic studies and a real-world case study of
primary seat-belt laws using Fatality Analysis Reporting System (FARS) dataset.
Together, these results indicate that the proposed learners deliver reliable
and sample-efficient HTE estimates in FD scenarios. The implementation is
available at https://github.com/yonghanjung/FD-CATE.
  Keywords: Front-door adjustment; Heterogeneous treatment effects; Debiased
learning; Quasi-oracle rates; Causal inference.

</details>


### [311] [Metrics for Parametric Families of Networks](https://arxiv.org/abs/2509.22549)
*Mario Gómez,Guanqun Ma,Tom Needham,Bei Wang*

Main category: stat.ML

TL;DR: 本文引入分析参数化网络数据的通用框架，定义参数化Gromov - Wasserstein距离，建立其性质与近似保证，并通过实验验证实用性。


<details>
  <summary>Details</summary>
Motivation: 缺乏分析参数化网络数据的通用方法。

Method: 基于最优传输的Gromov - Wasserstein变体，定义参数化Gromov - Wasserstein距离，建立性质、推导近似保证、开发下界。

Result: 该距离涵盖现有多种度量，有理论近似保证，在随机图和随机度量空间中可通过经验估计一致近似。

Conclusion: 所提框架有理论基础且在数值实验中展现实用性。

Abstract: We introduce a general framework for analyzing data modeled as parameterized
families of networks. Building on a Gromov-Wasserstein variant of optimal
transport, we define a family of parameterized Gromov-Wasserstein distances for
comparing such parametric data, including time-varying metric spaces induced by
collective motion, temporally evolving weighted social networks, and random
graph models. We establish foundational properties of these distances, showing
that they subsume several existing metrics in the literature, and derive
theoretical approximation guarantees. In particular, we develop computationally
tractable lower bounds and relate them to graph statistics commonly used in
random graph theory. Furthermore, we prove that our distances can be
consistently approximated in random graph and random metric space settings via
empirical estimates from generative models. Finally, we demonstrate the
practical utility of our framework through a series of numerical experiments.

</details>


### [312] [Linear Causal Representation Learning by Topological Ordering, Pruning, and Disentanglement](https://arxiv.org/abs/2509.22553)
*Hao Chen,Lin Liu,Yu Guang Wang*

Main category: stat.ML

TL;DR: 本文聚焦线性结构因果模型，提出新的线性因果表示学习算法，在较弱假设下恢复潜在因果特征，通过实验验证其优势和潜力。


<details>
  <summary>Details</summary>
Motivation: 现有线性因果表示学习方法依赖严格假设，在某些场景难以满足，需提出在较弱假设下仍有效的算法。

Method: 提出一种新的线性因果表示学习算法，在关于环境异质性和数据生成分布的较弱假设下运行。

Result: 通过合成实验和大语言模型的可解释性分析验证新算法，在有限样本中优于竞争方法。

Conclusion: 新算法有潜力将因果关系融入人工智能。

Abstract: Causal representation learning (CRL) has garnered increasing interests from
the causal inference and artificial intelligence community, due to its
capability of disentangling potentially complex data-generating mechanism into
causally interpretable latent features, by leveraging the heterogeneity of
modern datasets. In this paper, we further contribute to the CRL literature, by
focusing on the stylized linear structural causal model over the latent
features and assuming a linear mixing function that maps latent features to the
observed data or measurements. Existing linear CRL methods often rely on
stringent assumptions, such as accessibility to single-node interventional data
or restrictive distributional constraints on latent features and exogenous
measurement noise. However, these prerequisites can be challenging to satisfy
in certain scenarios. In this work, we propose a novel linear CRL algorithm
that, unlike most existing linear CRL methods, operates under weaker
assumptions about environment heterogeneity and data-generating distributions
while still recovering latent causal features up to an equivalence class. We
further validate our new algorithm via synthetic experiments and an
interpretability analysis of large language models (LLMs), demonstrating both
its superiority over competing methods in finite samples and its potential in
integrating causality into AI.

</details>


### [313] [Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback](https://arxiv.org/abs/2509.22633)
*Gen Li,Yuling Yan*

Main category: stat.ML

TL;DR: 本文研究在线RLHF的探索原则，指出现有算法不足，提出新探索方案并建立了遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有基于乐观主义的探索算法在采样协议上存在缺陷，不能有效减少奖励差异中最具信息性的不确定性，因此需要改进。

Method: 提出一种新的探索方案，将偏好查询导向减少与策略改进最相关的奖励差异的不确定性。

Result: 在多臂老虎机模型下，建立了阶为 $T^{(eta+1)/(eta+2)}$ 的遗憾界，是首个在所有模型参数上遗憾呈多项式缩放的在线RLHF算法。

Conclusion: 新的探索方案有效，能解决现有算法的问题，在在线RLHF上有较好表现。

Abstract: Reinforcement learning with human feedback (RLHF), which learns a reward
model from human preference data and then optimizes a policy to favor preferred
responses, has emerged as a central paradigm for aligning large language models
(LLMs) with human preferences. In this paper, we investigate exploration
principles for online RLHF, where one seeks to adaptively collect new
preference data to refine both the reward model and the policy in a
data-efficient manner. By examining existing optimism-based exploration
algorithms, we identify a drawback in their sampling protocol: they tend to
gather comparisons that fail to reduce the most informative uncertainties in
reward differences, and we prove lower bounds showing that such methods can
incur linear regret over exponentially long horizons. Motivated by this
insight, we propose a new exploration scheme that directs preference queries
toward reducing uncertainty in reward differences most relevant to policy
improvement. Under a multi-armed bandit model of RLHF, we establish regret
bounds of order $T^{(\beta+1)/(\beta+2)}$, where $\beta>0$ is a hyperparameter
that balances reward maximization against mitigating distribution shift. To our
knowledge, this is the first online RLHF algorithm with regret scaling
polynomially in all model parameters.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [314] [Linear Risk Sharing on Networks](https://arxiv.org/abs/2509.21411)
*Arthur Charpentier,Philipp Ratz*

Main category: econ.TH

TL;DR: 本文为线性风险分担（LRS）开发综合框架，研究不同网络结构下风险分配，为对等保险和网络风险池提供设计原则。


<details>
  <summary>Details</summary>
Motivation: 随着非传统保险和银行产品流行，此类产品依赖网络结构分配风险，需开发框架研究风险分担。

Method: 基于随机和双随机矩阵理论建立约束条件，用凸序框架比较不同分配，扩展到网络分享分析，引入随机分享矩阵，研究恒等和网络诱导算子的凸组合。

Result: 分析不同网络拓扑对风险结果的影响，将风险分担属性与度分布联系起来，研究自保留和分散化的权衡。

Conclusion: 研究结果为公平高效的对等保险和网络风险池提供结合数学合理性和经济可解释性的设计原则。

Abstract: Over the past decade alternatives to traditional insurance and banking have
grown in popularity. The desire to encourage local participation has lead
products such as peer-to-peer insurance, reciprocal contracts, and
decentralized finance platforms to increasingly rely on network structures to
redistribute risk among participants. In this paper, we develop a comprehensive
framework for linear risk sharing (LRS), where random losses are reallocated
through nonnegative linear operators which can accommodate a wide range of
networks. Building on the theory of stochastic and doubly stochastic matrices,
we establish conditions under which constraints such as budget balance,
fairness, and diversification are guaranteed. The convex order framework allows
us to compare different allocations rigorously, highlighting variance reduction
and majorization as natural consequences of doubly stochastic mixing. We then
extend the analysis to network-based sharing, showing how their topology shapes
risk outcomes in complete, star, ring, random, and scale-free graphs. A second
layer of randomness, where the sharing matrix itself is random, is introduced
via Erd\H{o}s--R\'enyi and preferential-attachment networks, connecting
risk-sharing properties to degree distributions. Finally, we study convex
combinations of identity and network-induced operators, capturing the trade-off
between self-retention and diversification. Our results provide design
principles for fair and efficient peer-to-peer insurance and network-based risk
pooling, combining mathematical soundness with economic interpretability.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [315] [The MUG-10 Framework for Preventing Usability Issues in Mobile Application Development](https://arxiv.org/abs/2509.21914)
*Pawel Weichbroth,Tomasz Szot*

Main category: cs.HC

TL;DR: 研究通过对20名从业者调查，采用体内编码法分析数据，开发了含十条准则和三项活动的框架，强调开发各阶段与用户协作，建议后续研究评估框架有效性及开发自动化工具。


<details>
  <summary>Details</summary>
Motivation: 硬件限制和用户需求多样使移动应用开发面临可用性问题，但缺乏应对措施的研究，本研究旨在填补此空白。

Method: 对20名移动软件开发从业者进行调查，采用体内编码法分析收集的数据。

Result: 开发了一个包含十条准则和三项活动、具有普遍适用性的新颖框架。

Conclusion: 未来研究应评估建议有效性，跨利益相关者群体验证，还可考虑开发自动化工具支持早期检测和缓解可用性问题。

Abstract: Nowadays, mobile applications are essential tools for everyday life,
providing users with anytime, anywhere access to up-to-date information,
communication, and entertainment. Needless to say, hardware limitations and the
diverse needs of different user groups pose a number of design and development
challenges. According to recent studies, usability is one of the most revealing
among many others. However, few have made the direct effort to provide and
discuss what countermeasures can be applied to avoid usability issues in mobile
application development. Through a survey of 20 mobile software design and
development practitioners, this study aims to fill this research gap. Given the
qualitative nature of the data collected, and with the goal of capturing and
preserving the intrinsic meanings embedded in the experts' statements, we
adopted in vivo coding. The analysis of the collected material enabled us to
develop a novel framework consisting of ten guidelines and three activities
with general applications. In addition, it can be noted that active
collaboration with users in testing and collecting feedback was often
emphasized at each stage of mobile application development. Future research
should consider focused action research that evaluates the effectiveness of our
recommendations and validates them across different stakeholder groups. In this
regard, the development of automated tools to support early detection and
mitigation of usability issues during mobile application development could also
be considered.

</details>


### [316] [Psychological and behavioural responses in human-agent vs. human-human interactions: a systematic review and meta-analysis](https://arxiv.org/abs/2509.21542)
*Jianan Zhou,Fleur Corbett,Joori Byun,Talya Porat,Nejra van Zalk*

Main category: cs.HC

TL;DR: 本文对人类与智能体和人类之间的二元互动进行系统综述和元分析，发现人类与智能体互动时亲社会行为和道德参与较少，但部分互动表现与人类互动相当，同时指出影响伙伴效应的调节因素及智能体设计与监管的启示。


<details>
  <summary>Details</summary>
Motivation: 尽管交互式智能体已融入社会，但人类对其反应尚不清楚，且研究分散，因此开展系统综合研究。

Method: 对162项符合条件的研究进行系统综述和元分析，综合频率主义和贝叶斯方法。

Result: 人类与智能体互动时亲社会行为和道德参与少，对智能体的能动性和责任归因少，认为其能力、亲和力和社交存在感低；但社会一致性、信任等方面与人类互动相当；许多主观反应存在高异质性，发现了影响伙伴效应的调节因素。

Conclusion: 与智能体的功能性行为和互动体验可与人类相似，但基本社会归因和道德/亲社会关注不足，智能体有工具价值但缺乏内在价值，为智能体设计和监管提供了实践启示。

Abstract: Interactive intelligent agents are being integrated across society. Despite
achieving human-like capabilities, humans' responses to these agents remain
poorly understood, with research fragmented across disciplines. We conducted a
first systematic synthesis comparing a range of psychological and behavioural
responses in matched human-agent vs. human-human dyadic interactions. A total
of 162 eligible studies (146 contributed to the meta-analysis; 468 effect
sizes) were included in the systematic review and meta-analysis, which
integrated frequentist and Bayesian approaches. Our results indicate that
individuals exhibited less prosocial behaviour and moral engagement when
interacting with agents vs. humans. They attributed less agency and
responsibility to agents, perceiving them as less competent, likeable, and
socially present. In contrast, individuals' social alignment (i.e., alignment
or adaptation of internal states and behaviours with partners), trust in
partners, personal agency, task performance, and interaction experiences were
generally comparable when interacting with agents vs. humans. We observed high
effect-size heterogeneity for many subjective responses (i.e., social
perceptions of partners, subjective trust, and interaction experiences),
suggesting context-dependency of partner effects. By examining the
characteristics of studies, participants, partners, interaction scenarios, and
response measures, we also identified several moderators shaping partner
effects. Overall, functional behaviours and interactive experiences with agents
can resemble those with humans, whereas fundamental social attributions and
moral/prosocial concerns lag in human-agent interactions. Agents are thus
afforded instrumental value on par with humans but lack comparable intrinsic
value, providing practical implications for agent design and regulation.

</details>


### [317] [Teaching AI to Feel: A Collaborative, Full-Body Exploration of Emotive Communication](https://arxiv.org/abs/2509.22168)
*Esen K. Tütüncü,Lissette Lemus,Kris Pilcher,Holger Sprengel,Jordi Sabater-Mir*

Main category: cs.HC

TL;DR: Commonaiverse是一个通过全身动作追踪和实时AI反馈探索人类情感的互动装置，介绍其阶段、技术及意义。


<details>
  <summary>Details</summary>
Motivation: 探索通过新方式进行情感计算，推动多媒体研究范式转变，减少偏见并促进用户能动性。

Method: 参与者经历教学、探索和宇宙阶段，装置集成MoveNet进行动作追踪，用多推荐AI系统分析情感状态并输出视听内容。

Result: 实现了参与者与系统协作表达和解读情感，推动多媒体研究向更具具身性、共创性的情感AI范式转变。

Conclusion: 这种参与者驱动、文化多元定义情感的方式为情感计算开辟新途径，减少偏见，为高级互动应用提供可能。

Abstract: Commonaiverse is an interactive installation exploring human emotions through
full-body motion tracking and real-time AI feedback. Participants engage in
three phases: Teaching, Exploration and the Cosmos Phase, collaboratively
expressing and interpreting emotions with the system. The installation
integrates MoveNet for precise motion tracking and a multi-recommender AI
system to analyze emotional states dynamically, responding with adaptive
audiovisual outputs. By shifting from top-down emotion classification to
participant-driven, culturally diverse definitions, we highlight new pathways
for inclusive, ethical affective computing. We discuss how this collaborative,
out-of-the-box approach pushes multimedia research beyond single-user facial
analysis toward a more embodied, co-created paradigm of emotional AI.
Furthermore, we reflect on how this reimagined framework fosters user agency,
reduces bias, and opens avenues for advanced interactive applications.

</details>


### [318] [Mental Health Impacts of AI Companions: Triangulating Social Media Quasi-Experiments, User Perspectives, and Relational Theory](https://arxiv.org/abs/2509.22505)
*Yunhao Yuan,Jiaxun Zhang,Talayeh Aledavood,Renwen Zhang,Koustuv Saha*

Main category: cs.HC

TL;DR: 研究AI聊天机器人对用户心理社会影响，结合准实验研究和访谈，给出设计建议。


<details>
  <summary>Details</summary>
Motivation: AI聊天机器人流行但心理社会影响不明，需探究其对用户幸福感的塑造及用户体验感知。

Method: 对Reddit纵向数据进行大规模准实验研究，运用分层倾向得分匹配和双重差分回归；进行15次半结构化访谈并主题分析。

Result: 准实验研究发现有混合效应，访谈识别出互动轨迹，AICC有情感支持也有依赖风险。

Conclusion: 为AI伴侣设计提供建议，平衡心理社会效益与风险。

Abstract: AI-powered companion chatbots (AICCs) such as Replika are increasingly
popular, offering empathetic interactions, yet their psychosocial impacts
remain unclear. We examined how engaging with AICCs shaped wellbeing and how
users perceived these experiences. First, we conducted a large-scale
quasi-experimental study of longitudinal Reddit data, applying stratified
propensity score matching and Difference-in-Differences regression. Findings
revealed mixed effects -- greater affective and grief expression, readability,
and interpersonal focus, alongside increases in language about loneliness and
suicidal ideation. Second, we complemented these results with 15
semi-structured interviews, which we thematically analyzed and contextualized
using Knapp's relationship development model. We identified trajectories of
initiation, escalation, and bonding, wherein AICCs provided emotional
validation and social rehearsal but also carried risks of over-reliance and
withdrawal. Triangulating across methods, we offer design implications for AI
companions that scaffold healthy boundaries, support mindful engagement,
support disclosure without dependency, and surface relationship stages --
maximizing psychosocial benefits while mitigating risks.

</details>


### [319] [Does AI Coaching Prepare us for Workplace Negotiations?](https://arxiv.org/abs/2509.22545)
*Veda Duddu,Jash Rajesh Parekh,Andy Mao,Hanyi Min,Ziang Xiao,Vedant Das Swain,Koustuv Saha*

Main category: cs.HC

TL;DR: 研究比较AI教练Trucey、ChatGPT和传统谈判手册对谈判准备的效果，发现手册在可用性和心理赋能上更优，挑战AI优越性假设，建议混合设计。


<details>
  <summary>Details</summary>
Motivation: 职场谈判受心理障碍影响，AI谈判教练效果不明，需研究其对谈判准备的有效性。

Method: 构建基于Brett谈判模型的AI教练Trucey，进行267人的组间实验对比Trucey、ChatGPT和传统谈判手册，后进行15人的深度访谈。

Result: Trucey在降低恐惧方面效果最佳，手册在可用性和心理赋能上优于两个AI，AI指导冗长零散，让参与者不确定或不知所措。

Conclusion: 挑战AI优越性假设，建议采用混合设计整合结构化理论内容与有针对性的演练等，以支持谈判准备。

Abstract: Workplace negotiations are undermined by psychological barriers, which can
even derail well-prepared tactics. AI offers personalized and always --
available negotiation coaching, yet its effectiveness for negotiation
preparedness remains unclear. We built Trucey, a prototype AI coach grounded in
Brett's negotiation model. We conducted a between-subjects experiment (N=267),
comparing Trucey, ChatGPT, and a traditional negotiation Handbook, followed by
in-depth interviews (N=15). While Trucey showed the strongest reductions in
fear relative to both comparison conditions, the Handbook outperformed both AIs
in usability and psychological empowerment. Interviews revealed that the
Handbook's comprehensive, reviewable content was crucial for participants'
confidence and preparedness. In contrast, although participants valued AI's
rehearsal capability, its guidance often felt verbose and fragmented --
delivered in bits and pieces that required additional effort -- leaving them
uncertain or overwhelmed. These findings challenge assumptions of AI
superiority and motivate hybrid designs that integrate structured,
theory-driven content with targeted rehearsal, clear boundaries, and adaptive
scaffolds to address psychological barriers and support negotiation
preparedness.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [320] [Pattern Recognition of Illicit E-Waste Misclassification in Global Trade Data](https://arxiv.org/abs/2509.21395)
*Muhammad Sukri Bin Ramli*

Main category: econ.GN

TL;DR: 提出数据驱动框架识别电子电气产品废物流通异常，分析马及全球市场结构，框架能标记疑似废物流通产品。


<details>
  <summary>Details</summary>
Motivation: 电子电气产品贸易中电子垃圾识别难，传统方法难发现非法贸易模式。

Method: 提出含离群点感知分割方法的框架，用逻辑回归模型开发“废物流通评分”。

Result: 马及全球数据集有四级市场层次，马市场由大宗商品主导，全球市场由高价值资本货物塑造。

Conclusion: 框架能成功标记像废料一样交易的成品，为监管审查提供目标清单。

Abstract: The global trade in electronic and electrical goods is complicated by the
challenge of identifying e-waste, which is often misclassified to evade
regulations. Traditional analysis methods struggle to discern the underlying
patterns of this illicit trade within vast datasets. This research proposes and
validates a robust, data-driven framework to segment products and identify
goods exhibiting an anomalous "waste signature" a trade pattern defined by a
clear 'inverse price-volume'. The core of the framework is an Outlier-Aware
Segmentation method, an iterative K-Means approach that first isolates extreme
outliers to prevent data skewing and then re-clusters the remaining products to
reveal subtle market segments. To quantify risk, a "Waste Score" is developed
using a Logistic Regression model that identifies products whose trade
signatures are statistically similar to scrap. The findings reveal a consistent
four-tier market hierarchy in both Malaysian and global datasets. A key pattern
emerged from a comparative analysis: Malaysia's market structure is defined by
high-volume bulk commodities, whereas the global market is shaped by high-value
capital goods, indicating a unique national specialization. The framework
successfully flags finished goods, such as electric generators (HS 8502), that
are traded like scrap, providing a targeted list for regulatory scrutiny.

</details>


### [321] [Impact of government spending shocks in the Visegrad countries, 1999Q1-2019Q4](https://arxiv.org/abs/2509.21397)
*Zoltan Bartha,Marco M. Matarrese*

Main category: econ.GN

TL;DR: 研究评估财政政策支出冲击对维谢格拉德四国经济的影响，用SVAR模型计算发现除斯洛伐克外，财政扩张影响较大，并探讨了高支出乘数相关因素。


<details>
  <summary>Details</summary>
Motivation: 研究财政政策支出冲击对维谢格拉德四国经济的影响。

Method: 使用SVAR模型，基于1999Q1 - 2019Q4的84个季度观测值进行计算。

Result: 除斯洛伐克外，财政扩张在其他三国影响较大，捷克、匈牙利、波兰的5年累计支出乘数分别为0.81、1.14、1.76，斯洛伐克为 -0.18但不显著。

Conclusion: 高支出乘数与较高的增值税收入占比、较高债务比率、较高外债和较低开放性有关。

Abstract: This study investigates the impact of a fiscal policy spending shock on the
economy of the Visegrad 4 countries. The impact is estimated with an SVAR
model, and the calculations are based on 84 quarterly observations
(1999Q1-2019Q4). The results suggest that fiscal expansion has a larger than
usual impact in the V4 countries (except for Slovakia): the estimated long-term
(5-year) cumulative spending multipliers are 0.81 for Czechia, 1.14 for
Hungary, and 1.76 for Poland (the Slovakian multiplier has a value of -0.18,
but it is not significant). The discussion section also connects higher
spending multipliers with a higher share of VAT revenues, a higher debt ratio,
higher foreign debt, and lower openness.

</details>


### [322] [Student perception and the efficacy of universities in shaping the entrepreneurial mindset](https://arxiv.org/abs/2509.21414)
*Andrea S. Gubik,Zoltan Bartha*

Main category: econ.GN

TL;DR: 研究指出创业教育对创业活动影响弱或因学生认知，用数据库建模分析，得出塑造态度和激发兴趣可提升创业教育效率。


<details>
  <summary>Details</summary>
Motivation: 探究创业教育对创业活动影响弱的原因

Method: 利用2018年GUESSS数据库（含匈牙利9667个答案）构建一般线性模型

Result: 学生创业意向、态度、自我效能、社会规范、大学及所学专业对其感知校内创业生态有微小但显著的影响

Conclusion: 更注重塑造态度和激发学生兴趣能提高创业教育效率

Abstract: Modern universities may play a significant role in entrepreneurial ecosystems
by boosting the entrepreneurial activity of the region. One way to achieve this
is through entrepreneurship education. In this study we suggest that one reason
why entrepreneurship education has a weak impact on entrepreneurial activity is
that the effect of courses and extracurricular programmes depends on how
students perceive the entrepreneurial activity. We use the 2018 GUESSS
database, which includes 9,667 answers for Hungary, to develop a general linear
model. The model suggests that students' entrepreneurial intentions, attitudes
toward entrepreneurship, self-efficacy, social norms, as well as the
university, and the field of study all have a small but statistically
significant impact on how students perceive the entrepreneurial ecosystem
within the university. Our conclusion is that more emphasis on shaping
attitudes and arousing student interest can increase the efficiency of
entrepreneurship education.

</details>


### [323] [The Impact of the UEFA Women's EURO on Hotel Overnight Stays: Evidence from a Causal Analysis](https://arxiv.org/abs/2509.21421)
*Hannes Wallimann,Anna Mehr*

Main category: econ.GN

TL;DR: 本文评估2025年瑞士女足欧锦赛旅游影响，结果显示有积极但较小影响，并给出体育赛事政策评估框架。


<details>
  <summary>Details</summary>
Motivation: 填补女性体育赛事影响评估研究空白。

Method: 利用酒店过夜数据，采用Synthetic Difference - in - Differences方法对比主办与非主办城市。

Result: 未发现赛事有大的旅游影响，有小的积极影响，敏感性分析也显示积极影响，仅主赛场有明确结论，过夜人数增加1.6%。

Conclusion: 女足欧锦赛有积极但适度的旅游影响，给出体育赛事政策评估框架。

Abstract: The impact evaluation of female sports events remains an important yet
neglected area of research. To fill this gap, this working paper provides a
timely assessment of the 2025 UEFA Women's European Championship (WEURO) in
Switzerland-the largest women-specific sports event in Europe with more than
657,000 spectators. Using city-level data on hotel overnight stays, we apply
the Synthetic Difference-in-Differences approach of Arkhangelsky et al. (2021)
to compare WEURO host cities with non-host destinations. In summary, our
results do not support strong claims of large tourism impacts but rather point
to a small positive effect. Sensitivity analyses also suggest positive effects.
However, confidence intervals permit firm conclusions only for the main venues,
indicating an increase in overnight stays of 1.6% attributable to the WEURO.
Overall, our findings indicate positive but modest tourism impacts of the WEURO
and outline a framework for further policy evaluation of sports events.

</details>


### [324] [Forecasting House Prices](https://arxiv.org/abs/2509.21460)
*Emanuel Kohlscheen*

Main category: econ.GN

TL;DR: 文章用随机森林模型分析13个发达国家35年房价驱动因素，发现重要因素，模型预测效果优于OLS模型。


<details>
  <summary>Details</summary>
Motivation: 识别过去35年13个发达国家房价的驱动因素。

Method: 基于Breiman (2001)的随机森林模型，使用Shapley值分析，还进行部分效应分析和样本外预测测试。

Result: 各国年度房价增长主要由价格动量、初始估值和家庭信贷增长解释；解释变量存在重要非线性；随机森林模型预测误差比OLS模型低。

Conclusion: 随机森林模型能有效分析房价驱动因素，且对所有国家适用，国家固定效应影响小。

Abstract: This article identifies the factors that drove house prices in 13 advanced
countries over the past 35 years. It does so based on Breiman s (2001) random
forest model. Shapley values indicate that annual house price growth across
countries is explained first and foremost by price momentum, initial valuations
(proxied by price to rent ratios) and household credit growth. Partial effects
of explanatory variables are also elicited and suggest important
non-linearities, for instance as to what concerns the effects of CPI inflation
on house price growth. The out-of-sample forecast test reveals that the random
forest model delivers 44% lower house price variation RMSEs and 45% lower MAEs
when compared to an OLS model that uses the same set of 10 pre-determined
explanatory variables. Notably, the same model works well for all countries, as
the random forest attributes minimal values to country fixed effects.

</details>


### [325] [Persistent Gaps, Partial Gains: A Population-Level Study of COVID-19 Learning Inequalities in the Netherlands](https://arxiv.org/abs/2509.22136)
*Hekmat Alrouh,Tom Emery,Anja Schreijer*

Main category: econ.GN

TL;DR: 研究利用荷兰行政数据，分析疫情前后中学成绩的社会经济差距，发现不平等有持续或加剧情况，也有差距缩小情况，还探讨对分层理论的启示。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情扰乱全球学校教育，引发对教育不平等加剧的担忧，研究关注荷兰中学成绩的社会经济差距在疫情前后的演变。

Method: 使用荷兰人口级行政数据，分析2017 - 2023年毕业 cohort 的最终中央考试成绩，估计包含疫情暴露与关键分层变量交互项的广义线性模型。

Result: 到2023年平均成绩部分恢复，但父母教育和移民背景导致的不平等持续或加剧，非西方背景的第一代学生损失最大，农村地区学生缩小或扭转了疫情前的成绩差距。

Conclusion: 系统性冲击可加剧或重新校准不平等模式，取决于社会人口维度和教育背景，强调教育途径和当地环境在应对危机导致的学习中断中的作用。

Abstract: The COVID-19 pandemic disrupted schooling worldwide, raising concerns about
widening educational inequalities. Using population-level administrative data
from the Netherlands (N = 1,471,217), this study examines how socio-economic
disparities in secondary school performance evolved before, during, and after
pandemic-related school closures. We analyze final central examination scores
for cohorts graduating between 2017 and 2023 across four educational tracks,
estimating generalized linear models with interactions between pandemic
exposure and key stratification variables: parental education, household
income, migration background, and urbanicity. Results show that while average
performance partially recovered by 2023, inequalities by parental education and
migration background persisted or intensified, particularly in vocational
tracks. First-generation students with a non-Western background experienced the
largest sustained losses, whereas students in rural areas (previously
disadvantaged) narrowed or reversed pre-pandemic performance gaps. Findings
suggest that systemic shocks can both exacerbate and recalibrate inequality
patterns, depending on the socio-demographic dimension and educational context.
We discuss implications for stratification theory, highlighting the role of
educational pathways and local contexts in shaping resilience to crisis-induced
learning disruptions.

</details>


### [326] [How firms, bureaucrats, and ministries benefit from the revolving door: Evidence from Japan](https://arxiv.org/abs/2509.22173)
*Trevor Incerti*

Main category: econ.GN

TL;DR: 利用日本前公务员数据，揭示其就业市场呈两极分化，不同层级官员去向不同且能带来不同回报，表明‘旋转门’动态受企业需求和官僚激励共同影响。


<details>
  <summary>Details</summary>
Motivation: 此前研究多关注企业立法关联回报，对官僚关联回报及非盈利组织关注少，本文旨在研究官僚关联回报及不同组织情况。

Method: 使用记录日本所有前公务员首个离职后职位的数据进行研究。

Result: 高层精英经济部门官员更易加入盈利性企业，带来政府贷款增加和积极股市反应；低级官员更易加入与政府部门相关的非营利组织，该组织在有前官僚领导时获得更高价值合同。

Conclusion: ‘旋转门’动态是企业驱动需求和官僚激励共同作用的结果。

Abstract: A growing literature finds high returns to firms with legislative
connections. Less attention has been paid to returns from bureaucratic
connections and to organizations beyond for-profit firms. Using data recording
the first post-bureaucracy position occupied by all former civil servants in
Japan, I reveal a bifurcated job market for former bureaucrats. High-ranking
officials from elite economic ministries are more likely to join for-profit
firms, where they generate returns such as increased government loans and
positive stock market reactions. Lower-ranking officials are more likely to
join nonprofits linked to government ministries, which receive higher-value
contracts when former bureaucrats are in leadership roles. These patterns
suggest that while firms wish to hire bureaucrats who can deliver tangible
benefits, ministries also shape revolving door pathways by directing benefits
to ensure long-term career value for civil servants. These findings reframe
revolving door dynamics as the result of both firm-driven demand and
bureaucratic incentives.

</details>


### [327] [Metro 3 in Brussels under uncertainty: scenario-based public transport accessibility analysis](https://arxiv.org/abs/2509.22223)
*Brecht Verbeken,Arne Vanhoyweghen,Vincent Ginis*

Main category: econ.GN

TL;DR: 本文用基于时刻表的公共交通数据对布鲁塞尔地铁3号线的空间和分布可达性影响进行情景评估，发现可达性提升显著但不均，该线路有减少可达性社会空间不平等的潜力。


<details>
  <summary>Details</summary>
Motivation: 布鲁塞尔地铁3号线是备受争议的基础设施项目，缺乏公共可达性评估来为政策辩论提供信息。

Method: 结合并调整官方GTFS数据，构建当前网络、部分实施和全面实施三种情景，测量647个点间公共交通出行时间，在不同时段进行模拟。

Result: 可达性提升显著但不均，低收入社区改善最大，可达性结果存在时间变异性。

Conclusion: 地铁3号线有减少可达性社会空间不平等的潜力，能为以成本担忧为主的公共辩论提供透明证据。

Abstract: Metro Line 3 in Brussels is one of Europe's most debated infrastructure
projects, marked by escalating costs, delays, and uncertainty over completion.
Yet no public accessibility appraisal exists to inform this policy debate. This
paper provides a scenario-based assessment of the spatial and distributional
accessibility impacts of Metro 3 using schedule-based public transport data.
Official GTFS feeds from all regional operators were combined and adapted to
represent three scenarios: (i) the current network, (ii) partial implementation
of Metro 3 (conversion of the southern premetro section), and (iii) full
implementation to Bordet. Accessibility was measured as public transport travel
time between 647 evenly spaced 500 m points covering the Brussels-Capital
Region. Simulations were conducted for morning, evening, and weekend midday
periods, each with three departure times (t-10, t, t+10), capturing robustness
to short-term timetable variation. Results show substantial but uneven
accessibility gains, with the largest improvements occurring in neighborhoods
with below-average incomes. Temporal robustness analysis highlights variability
in accessibility outcomes, underscoring the need to account for uncertainty in
departure timing. These findings suggest that Metro 3 has the potential to
reduce socio-spatial inequalities in accessibility, providing transparent
evidence for a project where public debate is dominated by cost concerns.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [328] [Rigidity-Aware 3D Gaussian Deformation from a Single Image](https://arxiv.org/abs/2509.22222)
*Jinhyeok Kim,Jaehun Bang,Seunghyun Seo,Kyungdon Joo*

Main category: cs.GR

TL;DR: 提出DeformSplat框架，可从单张图像引导3D高斯变形，实验显示优于现有方法且可拓展应用。


<details>
  <summary>Details</summary>
Motivation: 现有从图像重建物体变形的方法多依赖多视图视频，在受限场景适用性有限。

Method: 提出Gaussian - to - Pixel Matching弥合3D高斯表示与2D像素观测的领域差距，提出Rigid Part Segmentation明确刚性区域，结合二者从单张图像重建一致变形。

Result: 广泛实验表明该方法显著优于现有方法。

Conclusion: 该方法可从单张图像有效引导3D高斯变形，且能自然拓展到多种应用。

Abstract: Reconstructing object deformation from a single image remains a significant
challenge in computer vision and graphics. Existing methods typically rely on
multi-view video to recover deformation, limiting their applicability under
constrained scenarios. To address this, we propose DeformSplat, a novel
framework that effectively guides 3D Gaussian deformation from only a single
image. Our method introduces two main technical contributions. First, we
present Gaussian-to-Pixel Matching which bridges the domain gap between 3D
Gaussian representations and 2D pixel observations. This enables robust
deformation guidance from sparse visual cues. Second, we propose Rigid Part
Segmentation consisting of initialization and refinement. This segmentation
explicitly identifies rigid regions, crucial for maintaining geometric
coherence during deformation. By combining these two techniques, our approach
can reconstruct consistent deformations from a single image. Extensive
experiments demonstrate that our approach significantly outperforms existing
methods and naturally extends to various applications,such as frame
interpolation and interactive object manipulation.

</details>


### [329] [Learning to Ball: Composing Policies for Long-Horizon Basketball Moves](https://arxiv.org/abs/2509.22442)
*Pei Xu,Zhen Wu,Ruocheng Wang,Vishnu Sarukkai,Kayvon Fatahalian,Ioannis Karamouzas,Victor Zordan,C. Karen Liu*

Main category: cs.GR

TL;DR: 提出新的策略集成框架和高级软路由器，用于多阶段长视野任务，在篮球技能任务评估中有效。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在多阶段长视野任务的策略组合和技能过渡上存在困难，如专家混合和技能链方法不适用于不同阶段缺乏共同探索状态及明确初末状态的任务。

Method: 引入新的策略集成框架以组合不同运动技能，引入高级软路由器实现子任务间无缝稳健过渡。

Result: 在一组基本篮球技能和挑战性过渡任务上评估，训练的策略能有效控制模拟角色与球互动，按实时用户命令完成长视野任务，无需球轨迹参考。

Conclusion: 所提框架和软路由器能解决多阶段长视野任务中不同运动技能的组合和过渡问题。

Abstract: Learning a control policy for a multi-phase, long-horizon task, such as
basketball maneuvers, remains challenging for reinforcement learning approaches
due to the need for seamless policy composition and transitions between skills.
A long-horizon task typically consists of distinct subtasks with well-defined
goals, separated by transitional subtasks with unclear goals but critical to
the success of the entire task. Existing methods like the mixture of experts
and skill chaining struggle with tasks where individual policies do not share
significant commonly explored states or lack well-defined initial and terminal
states between different phases. In this paper, we introduce a novel policy
integration framework to enable the composition of drastically different motor
skills in multi-phase long-horizon tasks with ill-defined intermediate states.
Based on that, we further introduce a high-level soft router to enable seamless
and robust transitions between the subtasks. We evaluate our framework on a set
of fundamental basketball skills and challenging transitions. Policies trained
by our approach can effectively control the simulated character to interact
with the ball and accomplish the long-horizon task specified by real-time user
commands, without relying on ball trajectory references.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [330] [Assessment of deep learning models integrated with weather and environmental variables for wildfire spread prediction and a case study of the 2023 Maui fires](https://arxiv.org/abs/2509.21327)
*Jiyeon Kim,Yingjie Hu,Negar Elhami-Khorasani,Kai Sun,Ryan Zhenqi Zhou*

Main category: physics.soc-ph

TL;DR: 文章基于夏威夷超十年野火数据评估五种深度学习模型预测野火蔓延能力，以2023毛伊岛火灾为例与FARSITE对比，找出表现最佳模型，还分析了重要因素。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型用于野火蔓延预测的优缺点和与非AI模型的比较情况不明，需进行评估和对比。

Method: 基于夏威夷超十年野火数据评估五种集成天气和环境变量的深度学习模型，以2023毛伊岛火灾为例与FARSITE对比，结合可解释AI方法分析。

Result: ConvLSTM和带注意力机制的ConvLSTM在五种AI模型中表现最佳；FARSITE精度、F1分数更高，召回率低，AI模型输入数据灵活性高。

Conclusion: 通过评估和对比，了解不同模型特点，还找出与2023毛伊岛野火相关的重要天气和环境因素。

Abstract: Predicting the spread of wildfires is essential for effective fire management
and risk assessment. With the fast advancements of artificial intelligence
(AI), various deep learning models have been developed and utilized for
wildfire spread prediction. However, there is limited understanding of the
advantages and limitations of these models, and it is also unclear how deep
learning-based fire spread models can be compared with existing non-AI fire
models. In this work, we assess the ability of five typical deep learning
models integrated with weather and environmental variables for wildfire spread
prediction based on over ten years of wildfire data in the state of Hawaii. We
further use the 2023 Maui fires as a case study to compare the best deep
learning models with a widely-used fire spread model, FARSITE. The results show
that two deep learning models, i.e., ConvLSTM and ConvLSTM with attention,
perform the best among the five tested AI models. FARSITE shows higher
precision, lower recall, and higher F1-score than the best AI models, while the
AI models offer higher flexibility for the input data. By integrating AI models
with an explainable AI method, we further identify important weather and
environmental factors associated with the 2023 Maui wildfires.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [331] [Toward a Realistic Encoding Model of Auditory Affective Understanding in the Brain](https://arxiv.org/abs/2509.21381)
*Guandong Pan,Yaqian Yang,Shi Chen,Xin Wang,Longzhao Liu,Hongwei Zheng,Shaoting Tang*

Main category: eess.AS

TL;DR: 研究提出计算框架，用三个数据集研究自然听觉输入到行为/神经反应的编码，发现高层语义表征在情感编码中占主导，wav2vec 2.0/hubert中间层诱导情感能力强，语音和音轨情感唤起有数据集依赖，揭示听觉 - 情感编码分层机制。


<details>
  <summary>Details</summary>
Motivation: 在情感神经科学和情感感知AI领域，理解复杂听觉刺激如何驱动情感唤醒动态尚未解决。

Method: 引入计算框架，基于并行听觉层级的神经生物学原理，将音频分解为多级听觉特征，通过跨数据集分析将其映射到情感相关响应。

Result: 高层语义表征在情感编码中占主导，wav2vec 2.0/Hubert中间层在跨数据集情感诱导上优于最终层，语音和音轨情感唤起有数据集依赖，神经分析表明语音和音轨在不同脑区表现不同。

Conclusion: 本研究揭示了听觉 - 情感编码的分层机制，为自适应情感感知系统和跨学科音频 - 情感交互探索提供基础。

Abstract: In affective neuroscience and emotion-aware AI, understanding how complex
auditory stimuli drive emotion arousal dynamics remains unresolved. This study
introduces a computational framework to model the brain's encoding of
naturalistic auditory inputs into dynamic behavioral/neural responses across
three datasets (SEED, LIRIS, self-collected BAVE). Guided by neurobiological
principles of parallel auditory hierarchy, we decompose audio into multilevel
auditory features (through classical algorithms and wav2vec 2.0/Hubert) from
the original and isolated human voice/background soundtrack elements, mapping
them to emotion-related responses via cross-dataset analyses. Our analysis
reveals that high-level semantic representations (derived from the final layer
of wav2vec 2.0/Hubert) exert a dominant role in emotion encoding, outperforming
low-level acoustic features with significantly stronger mappings to behavioral
annotations and dynamic neural synchrony across most brain regions ($p <
0.05$). Notably, middle layers of wav2vec 2.0/hubert (balancing
acoustic-semantic information) surpass the final layers in emotion induction
across datasets. Moreover, human voices and soundtracks show dataset-dependent
emotion-evoking biases aligned with stimulus energy distribution (e.g., LIRIS
favors soundtracks due to higher background energy), with neural analyses
indicating voices dominate prefrontal/temporal activity while soundtracks excel
in limbic regions. By integrating affective computing and neuroscience, this
work uncovers hierarchical mechanisms of auditory-emotion encoding, providing a
foundation for adaptive emotion-aware systems and cross-disciplinary
explorations of audio-affective interactions.

</details>


### [332] [ARTI-6: Towards Six-dimensional Articulatory Speech Encoding](https://arxiv.org/abs/2509.21447)
*Jihwan Lee,Sean Foley,Thanathai Lertpetchpun,Kevin Huang,Yoonjeong Lee,Tiantian Feng,Louis Goldstein,Dani Byrd,Shrikanth Narayanan*

Main category: eess.AS

TL;DR: 提出ARTI - 6六维发音语音编码框架，含发音特征集、反演与合成模型，可推动发音相关技术，代码和样本公开。


<details>
  <summary>Details</summary>
Motivation: 构建一个可解释、计算高效且基于生理的发音语音编码框架，以推动发音反演、合成及更广泛的语音技术应用。

Method: 从实时MRI数据中提取关键声道区域信息，构建包含六维发音特征集、发音反演模型（利用语音基础模型从语音声学预测发音特征）和发音合成模型（从发音特征重建语音）的ARTI - 6框架。

Result: 发音反演模型预测相关性达0.87，发音合成模型能从低维表示生成自然语音。

Conclusion: ARTI - 6为发音反演、合成和语音技术应用提供了良好框架。

Abstract: We propose ARTI-6, a compact six-dimensional articulatory speech encoding
framework derived from real-time MRI data that captures crucial vocal tract
regions including the velum, tongue root, and larynx. ARTI-6 consists of three
components: (1) a six-dimensional articulatory feature set representing key
regions of the vocal tract; (2) an articulatory inversion model, which predicts
articulatory features from speech acoustics leveraging speech foundation
models, achieving a prediction correlation of 0.87; and (3) an articulatory
synthesis model, which reconstructs intelligible speech directly from
articulatory features, showing that even a low-dimensional representation can
generate natural-sounding speech. Together, ARTI-6 provides an interpretable,
computationally efficient, and physiologically grounded framework for advancing
articulatory inversion, synthesis, and broader speech technology applications.
The source code and speech samples are publicly available.

</details>


### [333] [Enhanced Generative Machine Listener](https://arxiv.org/abs/2509.21463)
*Vishnu Raj,Gouthaman KV,Shiv Gehlot,Lars Villemoes,Arijit Biswas*

Main category: eess.AS

TL;DR: 提出GMLv2模型用于预测主观音频质量，表现优于常用指标，提供可扩展自动化框架。


<details>
  <summary>Details</summary>
Motivation: 构建能准确预测主观音频质量的模型，推动现代音频编码技术研发。

Method: 引入基于Beta分布的损失来建模听众评分，结合额外的神经音频编码主观数据集。

Result: 在多样测试集上，GMLv2在与主观分数的相关性及跨不同内容类型和编解码器配置预测分数方面均优于PEAQ和ViSQOL等常用指标。

Conclusion: GMLv2为感知音频质量评估提供可扩展自动化框架，有望加速现代音频编码技术的研发。

Abstract: We present GMLv2, a reference-based model designed for the prediction of
subjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta
distribution-based loss to model the listener ratings and incorporates
additional neural audio coding (NAC) subjective datasets to extend its
generalization and applicability. Extensive evaluations on diverse testset
demonstrate that proposed GMLv2 consistently outperforms widely used metrics,
such as PEAQ and ViSQOL, both in terms of correlation with subjective scores
and in reliably predicting these scores across diverse content types and codec
configurations. Consequently, GMLv2 offers a scalable and automated framework
for perceptual audio quality evaluation, poised to accelerate research and
development in modern audio coding technologies.

</details>


### [334] [HuLA: Prosody-Aware Anti-Spoofing with Multi-Task Learning for Expressive and Emotional Synthetic Speech](https://arxiv.org/abs/2509.21676)
*Aurosweta Mahapatra,Ismail Rasim Ulgen,Berrak Sisman*

Main category: eess.AS

TL;DR: 提出用于语音欺骗检测的两阶段多任务学习框架HuLA，实验显示其能提升对高级合成语音攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有反欺骗系统因很少利用韵律作为判别线索，易受表达性和情感性合成语音攻击，而人类会本能利用韵律线索区分自然和合成语音。

Method: 提出两阶段韵律感知多任务学习框架HuLA，第一阶段用自监督学习骨干在真实语音上进行F0预测和浊音/清音分类辅助任务训练；第二阶段在真实和合成数据上联合优化欺骗检测和韵律任务。

Result: HuLA在具有挑战性的域外数据集上始终优于强基线，包括表达性、情感性和跨语言攻击。

Conclusion: 显式韵律监督与自监督学习嵌入相结合，能显著提高对高级合成语音攻击的鲁棒性。

Abstract: Current anti-spoofing systems remain vulnerable to expressive and emotional
synthetic speech, since they rarely leverage prosody as a discriminative cue.
Prosody is central to human expressiveness and emotion, and humans
instinctively use prosodic cues such as F0 patterns and voiced/unvoiced
structure to distinguish natural from synthetic speech. In this paper, we
propose HuLA, a two-stage prosody-aware multi-task learning framework for spoof
detection. In Stage 1, a self-supervised learning (SSL) backbone is trained on
real speech with auxiliary tasks of F0 prediction and voiced/unvoiced
classification, enhancing its ability to capture natural prosodic variation
similar to human perceptual learning. In Stage 2, the model is jointly
optimized for spoof detection and prosody tasks on both real and synthetic
data, leveraging prosodic awareness to detect mismatches between natural and
expressive synthetic speech. Experiments show that HuLA consistently
outperforms strong baselines on challenging out-of-domain dataset, including
expressive, emotional, and cross-lingual attacks. These results demonstrate
that explicit prosodic supervision, combined with SSL embeddings, substantially
improves robustness against advanced synthetic speech attacks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [335] [COMPASS: Robust Feature Conformal Prediction for Medical Segmentation Metrics](https://arxiv.org/abs/2509.22240)
*Matt Y. Cheung,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: eess.IV

TL;DR: 提出COMPASS框架为医学图像分割模型生成高效、基于指标的保形预测区间，在多个任务中表现优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 临床应用中分割模型实用性基于下游指标准确性，指标的不确定性量化对决策至关重要，传统保形预测应用于最终标量指标效率低。

Method: 利用深层神经网络的归纳偏置，在模型表示空间中通过沿对目标指标最敏感的低维子空间扰动中间特征进行校准。

Result: 在四个医学图像分割任务中，COMPASS产生的区间比传统保形预测基线更紧，在协变量偏移下也能恢复目标覆盖率。

Conclusion: COMPASS为医学图像分割的基于指标的不确定性量化铺平道路。

Abstract: In clinical applications, the utility of segmentation models is often based
on the accuracy of derived downstream metrics such as organ size, rather than
by the pixel-level accuracy of the segmentation masks themselves. Thus,
uncertainty quantification for such metrics is crucial for decision-making.
Conformal prediction (CP) is a popular framework to derive such principled
uncertainty guarantees, but applying CP naively to the final scalar metric is
inefficient because it treats the complex, non-linear segmentation-to-metric
pipeline as a black box. We introduce COMPASS, a practical framework that
generates efficient, metric-based CP intervals for image segmentation models by
leveraging the inductive biases of their underlying deep neural networks.
COMPASS performs calibration directly in the model's representation space by
perturbing intermediate features along low-dimensional subspaces maximally
sensitive to the target metric. We prove that COMPASS achieves valid marginal
coverage under exchangeability and nestedness assumptions. Empirically, we
demonstrate that COMPASS produces significantly tighter intervals than
traditional CP baselines on four medical image segmentation tasks for area
estimation of skin lesions and anatomical structures. Furthermore, we show that
leveraging learned internal features to estimate importance weights allows
COMPASS to also recover target coverage under covariate shifts. COMPASS paves
the way for practical, metric-based uncertainty quantification for medical
image segmentation.

</details>


### [336] [Deep Learning-Based Cross-Anatomy CT Synthesis Using Adapted nnResU-Net with Anatomical Feature Prioritized Loss](https://arxiv.org/abs/2509.22394)
*Javier Sequeiro González,Arthur Longuefosse,Miguel Díaz Benito,Álvaro García Martín,Fabien Baldacci*

Main category: eess.IV

TL;DR: 提出基于补丁的3D nnUNet用于MR到CT和CBCT到CT图像翻译，利用标准和残差UNet网络，引入AFP损失，经训练和推理，残差网络结合AFP效果好，提供跨模态医学图像合成稳定方案。


<details>
  <summary>Details</summary>
Motivation: 解决跨模态医学图像合成问题，提供有效的图像翻译方法。

Method: 采用标准UNet和残差UNet网络，引入AFP损失，对输入数据归一化，用3D补丁训练，经多轮训练和微调，推理时聚合重叠补丁并后处理。

Result: 残差网络结合AFP在重建清晰度和解剖保真度上表现好，L1-only网络在基于强度的指标上稍优。

Conclusion: 结合自动nnUNet管道、残差学习和解剖引导特征损失的方法对跨模态医学图像合成有效。

Abstract: We present a patch-based 3D nnUNet adaptation for MR to CT and CBCT to CT
image translation using the multicenter SynthRAD2025 dataset, covering head and
neck (HN), thorax (TH), and abdomen (AB) regions. Our approach leverages two
main network configurations: a standard UNet and a residual UNet, both adapted
from nnUNet for image synthesis. The Anatomical Feature-Prioritized (AFP) loss
was introduced, which compares multilayer features extracted from a compact
segmentation network trained on TotalSegmentator labels, enhancing
reconstruction of clinically relevant structures. Input volumes were normalized
per-case using zscore normalization for MRIs, and clipping plus dataset level
zscore normalization for CBCT and CT. Training used 3D patches tailored to each
anatomical region without additional data augmentation. Models were trained for
1000 and 1500 epochs, with AFP fine-tuning performed for 500 epochs using a
combined L1+AFP objective. During inference, overlapping patches were
aggregated via mean averaging with step size of 0.3, and postprocessing
included reverse zscore normalization. Both network configurations were applied
across all regions, allowing consistent model design while capturing local
adaptations through residual learning and AFP loss. Qualitative and
quantitative evaluation revealed that residual networks combined with AFP
yielded sharper reconstructions and improved anatomical fidelity, particularly
for bone structures in MR to CT and lesions in CBCT to CT, while L1only
networks achieved slightly better intensity-based metrics. This methodology
provides a stable solution for cross modality medical image synthesis,
demonstrating the effectiveness of combining the automatic nnUNet pipeline with
residual learning and anatomically guided feature losses.

</details>


### [337] [Comparative Analysis of GAN and Diffusion for MRI-to-CT translation](https://arxiv.org/abs/2509.22049)
*Emily Honey,Anders Helbo,Jens Petersen*

Main category: eess.IV

TL;DR: 本文对比了cGAN和cDDPM两种架构用于MRI到CT图像转换的性能，发现多通道条件输入和cDDPM架构更优。


<details>
  <summary>Details</summary>
Motivation: CT对治疗和诊断至关重要，在CT缺失或难以获取时，需生成合成CT图像，因此要确立MRI到CT转换的有效策略。

Method: 对比cGAN和cDDPM两种架构，选用Pix2Pix代表cGAN，Palette代表cDDPM；将3D转换问题分解为2D转换；研究单张和多张MRI切片作为条件输入的影响；用含新指标SIMOS的评估协议评估性能。

Result: MRI到CT的生成模型受益于多通道条件输入和使用cDDPM架构。

Conclusion: 多通道条件输入和cDDPM架构在MRI到CT图像转换中表现更优。

Abstract: Computed tomography (CT) is essential for treatment and diagnostics; In case
CT are missing or otherwise difficult to obtain, methods for generating
synthetic CT (sCT) images from magnetic resonance imaging (MRI) images are
sought after. Therefore, it is valuable to establish a reference for what
strategies are most effective for MRI-to-CT translation. In this paper, we
compare the performance of two frequently used architectures for MRI-to-CT
translation: a conditional generative adversarial network (cGAN) and a
conditional denoising diffusion probabilistic model (cDDPM). We chose
well-established implementations to represent each architecture: Pix2Pix for
cGAN, and Palette for cDDPM. We separate the classical 3D translation problem
into a sequence of 2D translations on the transverse plane, to investigate the
viability of a strategy that reduces the computational cost. We also
investigate the impact of conditioning the generative process on a single MRI
image/slice and on multiple MRI slices. The performance is assessed using a
thorough evaluation protocol, including a novel slice-wise metric Similarity Of
Slices (SIMOS), which measures the continuity between transverse slices when
compiling the sCTs into 3D format. Our comparative analysis revealed that
MRI-to-CT generative models benefit from multi-channel conditional input and
using cDDPM as an architecture.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [338] [A State-of-the-Art SQL Reasoning Model using RLVR](https://arxiv.org/abs/2509.21459)
*Alnur Ali,Ashutosh Baheti,Jonathan Chang,Ta-Chung Chi,Brandon Cui,Andrew Drozdov,Jonathan Frankle,Abhay Gupta,Pallavi Koppol,Sean Kulinski,Jonathan Li,Dipendra Misra,Krista Opsahl-Ong,Jose Javier Gonzalez Ortiz,Matei Zaharia,Yue Zhang*

Main category: cs.CL

TL;DR: 本文应用带可验证奖励的强化学习（RLVR）到BIRD基准测试，采用简单训练方法，在BIRD排行榜达SOTA，框架适用于企业领域。


<details>
  <summary>Details</summary>
Motivation: 开发能整合特定组织知识的定制推理模型以解决企业客户面临的问题。

Method: 将RLVR应用于BIRD基准测试，采用仔细的提示和模型选择、用TAO离线RL方法预热、严格的在线RLVR训练的训练方法。

Result: 首次提交在BIRD私有测试集上达到SOTA准确率，无自一致性为73.56%，有自一致性为75.68%，后者所需生成次数少于第二好的方法。

Conclusion: 尽管BIRD是代理任务，但框架简单，可广泛应用于商业智能、数据科学和编码等企业领域。

Abstract: Developing custom reasoning models via Reinforcement Learning (RL) that can
incorporate organization-specific knowledge has great potential to address
problems faced by enterprise customers. In many of these problems, the reward
function is verifiable, a setting termed RL with Verifiable Rewards (RLVR). We
apply RLVR to a popular data science benchmark called BIRD that measures the
ability of an AI agent to convert a natural language query for a database to
SQL executions. We apply a simple and general-purpose training recipe involving
careful prompt and model selection, a warm-up stage using our offline RL
approach called TAO, followed by rigorous online RLVR training. With no
additional training data beyond the BIRD training set and no use of proprietary
models, our very first submission to the BIRD leaderboard reached
state-of-the-art accuracy on the private test set: 73.56% without
self-consistency and 75.68% with self-consistency. In the latter case, our
model also required fewer generations than the second-best approach. While BIRD
is only a proxy task, the simplicity of our framework makes it broadly
applicable to enterprise domains such as business intelligence, data science,
and coding.

</details>


### [339] [FoodSEM: Large Language Model Specialized in Food Named-Entity Linking](https://arxiv.org/abs/2509.22125)
*Ana Gjorgjevikj,Matej Martinc,Gjorgjina Cenikj,Sašo Džeroski,Barbara Koroušić Seljak,Tome Eftimov*

Main category: cs.CL

TL;DR: 介绍了用于食品相关本体命名实体链接的微调大语言模型FoodSEM，表现优于相关模型，公开资源有多项贡献。


<details>
  <summary>Details</summary>
Motivation: 现有通用或特定领域模型无法准确解决食品命名实体链接任务。

Method: 通过指令 - 响应场景，将文本中食品相关实体链接到多个本体。

Result: FoodSEM达到了先进性能，F1分数在部分本体和数据集上达98%，优于非微调版本。

Conclusion: 公开FoodSEM及资源，为大语言模型微调/评估提供语料，推动食品领域语义理解，为未来基准测试提供基线。

Abstract: This paper introduces FoodSEM, a state-of-the-art fine-tuned open-source
large language model (LLM) for named-entity linking (NEL) to food-related
ontologies. To the best of our knowledge, food NEL is a task that cannot be
accurately solved by state-of-the-art general-purpose (large) language models
or custom domain-specific models/systems. Through an instruction-response (IR)
scenario, FoodSEM links food-related entities mentioned in a text to several
ontologies, including FoodOn, SNOMED-CT, and the Hansard taxonomy. The FoodSEM
model achieves state-of-the-art performance compared to related models/systems,
with F1 scores even reaching 98% on some ontologies and datasets. The presented
comparative analyses against zero-shot, one-shot, and few-shot LLM prompting
baselines further highlight FoodSEM's superior performance over its
non-fine-tuned version. By making FoodSEM and its related resources publicly
available, the main contributions of this article include (1) publishing a
food-annotated corpora into an IR format suitable for LLM
fine-tuning/evaluation, (2) publishing a robust model to advance the semantic
understanding of text in the food domain, and (3) providing a strong baseline
on food NEL for future benchmarking.

</details>


### [340] [Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation](https://arxiv.org/abs/2509.22565)
*Wenyuan Chen,Fateme Nateghi Haredasht,Kameron C. Black,Francois Grolleau,Emily Alsentzer,Jonathan H. Chen,Stephen P. Ma*

Main category: cs.CL

TL;DR: 本文提出评估大语言模型辅助医患异步消息草稿响应质量的方法，经实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录门户的异步医患消息增加医生工作量，大语言模型辅助草稿响应存在问题，需进行可靠评估。

Method: 引入临床错误本体，开发检索增强评估管道RAEC，采用两阶段提示架构，使用DSPy进行错误检测，对比基线和参考增强评估。

Result: 检索上下文改善临床完整性和工作流程适用性等领域的错误识别，上下文增强标签在人类验证中表现更优。

Conclusion: RAEC管道可作为患者消息的AI护栏。

Abstract: Asynchronous patient-clinician messaging via EHR portals is a growing source
of clinician workload, prompting interest in large language models (LLMs) to
assist with draft responses. However, LLM outputs may contain clinical
inaccuracies, omissions, or tone mismatches, making robust evaluation
essential. Our contributions are threefold: (1) we introduce a clinically
grounded error ontology comprising 5 domains and 59 granular error codes,
developed through inductive coding and expert adjudication; (2) we develop a
retrieval-augmented evaluation pipeline (RAEC) that leverages semantically
similar historical message-response pairs to improve judgment quality; and (3)
we provide a two-stage prompting architecture using DSPy to enable scalable,
interpretable, and hierarchical error detection. Our approach assesses the
quality of drafts both in isolation and with reference to similar past
message-response pairs retrieved from institutional archives. Using a two-stage
DSPy pipeline, we compared baseline and reference-enhanced evaluations on over
1,500 patient messages. Retrieval context improved error identification in
domains such as clinical completeness and workflow appropriateness. Human
validation on 100 messages demonstrated superior agreement (concordance = 50%
vs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs.
baseline, supporting the use of our RAEC pipeline as AI guardrails for patient
messaging.

</details>


### [341] [FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding](https://arxiv.org/abs/2509.22237)
*Haorui Chen,Chengze Li,Jia Li*

Main category: cs.CL

TL;DR: 现有代码生成评估基准无法充分评估vibe coding能力，提出新基准FeatBench，评估显示特征实现挑战大并发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成评估基准与vibe coding范式不匹配，无法评估特征实现能力。

Method: 提出具备纯自然语言提示、严格且可进化的数据收集流程、综合测试用例和多样应用领域等特点的FeatBench基准，用其评估两个先进代理框架和四个领先大语言模型。

Result: 特征实现挑战大，最高成功率仅29.94%，存在“激进实现”策略导致不同结果。

Conclusion: 发布FeatBench、自动收集管道和实验结果，推动社区进一步研究。

Abstract: The rapid advancement of Large Language Models (LLMs) has given rise to a
novel software development paradigm known as "vibe coding," where users
interact with coding agents through high-level natural language. However,
existing evaluation benchmarks for code generation inadequately assess an
agent's vibe coding capabilities. Existing benchmarks are misaligned, as they
either require code-level specifications or focus narrowly on issue-solving,
neglecting the critical scenario of feature implementation within the vibe
coding paradiam. To address this gap, we propose FeatBench, a novel benchmark
for vibe coding that focuses on feature implementation. Our benchmark is
distinguished by several key features: 1. Pure Natural Language Prompts. Task
inputs consist solely of abstract natural language descriptions, devoid of any
code or structural hints. 2. A Rigorous & Evolving Data Collection Process.
FeatBench is built on a multi-level filtering pipeline to ensure quality and a
fully automated pipeline to evolve the benchmark, mitigating data
contamination. 3. Comprehensive Test Cases. Each task includes Fail-to-Pass
(F2P) and Pass-to-Pass (P2P) tests to verify correctness and prevent
regressions. 4. Diverse Application Domains. The benchmark includes
repositories from diverse domains to ensure it reflects real-world scenarios.
We evaluate two state-of-the-art agent frameworks with four leading LLMs on
FeatBench. Our evaluation reveals that feature implementation within the vibe
coding paradigm is a significant challenge, with the highest success rate of
only 29.94%. Our analysis also reveals a tendency for "aggressive
implementation," a strategy that paradoxically leads to both critical failures
and superior software design. We release FeatBench, our automated collection
pipeline, and all experimental results to facilitate further community
research.

</details>


### [342] [A Novel Differential Feature Learning for Effective Hallucination Detection and Classification](https://arxiv.org/abs/2509.21357)
*Wenkai Wang,Vincent Lee,Yizhen Zheng*

Main category: cs.CL

TL;DR: 论文提出双模型架构检测大语言模型幻觉，发现幻觉信号集中在稀疏特征子集，呈漏斗模式，有望实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 现有研究未明确幻觉信号在层内的精确位置，限制了有效检测方法的发展。

Method: 提出集成投影融合（PF）块和差分特征学习（DFL）机制的双模型架构。

Result: 通过实验表明幻觉信号集中在稀疏特征子集，在问答和对话任务上准确率显著提升，呈现漏斗模式，用1%特征维度检测性能下降小。

Conclusion: 幻觉信号比以往认为的更集中，可为实现计算高效且保持精度的检测系统提供途径。

Abstract: Large language model hallucination represents a critical challenge where
outputs deviate from factual accuracy due to distributional biases in training
data. While recent investigations establish that specific hidden layers exhibit
differences between hallucinatory and factual content, the precise localization
of hallucination signals within layers remains unclear, limiting the
development of efficient detection methods. We propose a dual-model
architecture integrating a Projected Fusion (PF) block for adaptive inter-layer
feature weighting and a Differential Feature Learning (DFL) mechanism that
identifies discriminative features by computing differences between parallel
encoders learning complementary representations from identical inputs. Through
systematic experiments across HaluEval's question answering, dialogue, and
summarization datasets, we demonstrate that hallucination signals concentrate
in highly sparse feature subsets, achieving significant accuracy improvements
on question answering and dialogue tasks. Notably, our analysis reveals a
hierarchical "funnel pattern" where shallow layers exhibit high feature
diversity while deep layers demonstrate concentrated usage, enabling detection
performance to be maintained with minimal degradation using only 1\% of feature
dimensions. These findings suggest that hallucination signals are more
concentrated than previously assumed, offering a pathway toward computationally
efficient detection systems that could reduce inference costs while maintaining
accuracy.

</details>


### [343] [Influence Guided Context Selection for Effective Retrieval-Augmented Generation](https://arxiv.org/abs/2509.21359)
*Jiale Deng,Yanyan Shen,Ziyuan Pei,Youmin Chen,Linpeng Huang*

Main category: cs.CL

TL;DR: 本文提出Contextual Influence Value (CI值)解决RAG检索上下文质量差问题，开发参数化替代模型预测CI值，实验表明该方法显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法因检索上下文质量差影响效果，且现有上下文选择方法未能全面利用信息进行质量评估，提升有限。

Method: 将上下文质量评估重新概念化为推理时数据估值问题，引入CI值，开发参数化替代模型预测CI值，模型采用分层架构，通过oracle CI值监督和端到端生成器反馈训练。

Result: 在8个NLP任务和多个大语言模型上的实验表明，该上下文选择方法显著优于现有基线，能有效过滤低质量上下文并保留关键信息。

Conclusion: 提出的基于CI值的上下文选择方法能有效解决RAG中上下文质量差的问题，提升RAG性能。

Abstract: Retrieval-Augmented Generation (RAG) addresses large language model (LLM)
hallucinations by grounding responses in external knowledge, but its
effectiveness is compromised by poor-quality retrieved contexts containing
irrelevant or noisy information. While existing approaches attempt to improve
performance through context selection based on predefined context quality
assessment metrics, they show limited gains over standard RAG. We attribute
this limitation to their failure in holistically utilizing available
information (query, context list, and generator) for comprehensive quality
assessment. Inspired by recent advances in data selection, we reconceptualize
context quality assessment as an inference-time data valuation problem and
introduce the Contextual Influence Value (CI value). This novel metric
quantifies context quality by measuring the performance degradation when
removing each context from the list, effectively integrating query-aware
relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI
value eliminates complex selection hyperparameter tuning by simply retaining
contexts with positive CI values. To address practical challenges of label
dependency and computational overhead, we develop a parameterized surrogate
model for CI value prediction during inference. The model employs a
hierarchical architecture that captures both local query-context relevance and
global inter-context interactions, trained through oracle CI value supervision
and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and
multiple LLMs demonstrate that our context selection method significantly
outperforms state-of-the-art baselines, effectively filtering poor-quality
contexts while preserving critical information. Code is available at
https://github.com/SJTU-DMTai/RAG-CSM.

</details>


### [344] [Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs](https://arxiv.org/abs/2509.21361)
*Norman Paulsen*

Main category: cs.CL

TL;DR: 研究大语言模型上下文窗口实际效果，发现报告的最大上下文窗口与最大有效上下文窗口差异大，有效窗口随问题类型变化。


<details>
  <summary>Details</summary>
Motivation: 测试大语言模型上下文窗口在现实中的使用情况。

Method: 定义最大有效上下文窗口概念，制定测试方法和标准化比较方式，收集多模型的大量数据点。

Result: 最大有效上下文窗口与报告的最大上下文窗口差异显著，且随问题类型变化；部分模型在100个标记时就失败，多数在1000个标记时准确率严重下降，所有模型远低于最大上下文窗口。

Conclusion: 最大有效上下文窗口随问题类型变化，为提高模型准确性和降低幻觉率提供了明确可行的见解。

Abstract: Large language model (LLM) providers boast big numbers for maximum context
window sizes. To test the real world use of context windows, we 1) define a
concept of maximum effective context window, 2) formulate a testing method of a
context window's effectiveness over various sizes and problem types, and 3)
create a standardized way to compare model efficacy for increasingly larger
context window sizes to find the point of failure. We collected hundreds of
thousands of data points across several models and found significant
differences between reported Maximum Context Window (MCW) size and Maximum
Effective Context Window (MECW) size. Our findings show that the MECW is, not
only, drastically different from the MCW but also shifts based on the problem
type. A few top of the line models in our test group failed with as little as
100 tokens in context; most had severe degradation in accuracy by 1000 tokens
in context. All models fell far short of their Maximum Context Window by as
much as 99 percent. Our data reveals the Maximum Effective Context Window
shifts based on the type of problem provided, offering clear and actionable
insights into how to improve model accuracy and decrease model hallucination
rates.

</details>


### [345] [How Large Language Models Need Symbolism](https://arxiv.org/abs/2509.21404)
*Xiaotie Deng,Hanyu Li*

Main category: cs.CL

TL;DR: AI未来不止依赖规模扩展，大语言模型需人工符号引导以实现真正发现。


<details>
  <summary>Details</summary>
Motivation: 探讨AI未来发展所需的关键要素。

Method: 未提及

Result: 未提及

Conclusion: AI未来需人工符号引导大语言模型实现真正发现。

Abstract: We argue that AI's future requires more than scaling. To unlock genuine
discovery, large language models need a compass: human-crafted symbols to guide
their powerful but blind intuition.

</details>


### [346] [One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning](https://arxiv.org/abs/2509.21443)
*Sualeha Farid,Jayden Lin,Zean Chen,Shivani Kumar,David Jurgens*

Main category: cs.CL

TL;DR: 研究系统调查大语言模型（LLMs）中语言如何影响道德决策，发现不同语言间道德判断存在显著不一致，提出道德推理错误类型，呼吁更具文化意识的AI。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs主要在英语数据上预训练，担心其在多语言和多元文化环境中跨语言和文化背景进行道德判断的泛化能力。

Method: 将两个既定道德推理基准测试翻译成五种不同语言进行多语言零样本评估，结合研究问题分析差异驱动因素，通过案例研究探讨预训练数据的作用。

Result: 发现LLMs在不同语言间的道德判断存在显著不一致，反映出文化不匹配，揭示了差异的潜在驱动因素。

Conclusion: 提炼出道德推理错误的结构化类型，呼吁开发更具文化意识的AI。

Abstract: Large Language Models (LLMs) are increasingly deployed in multilingual and
multicultural environments where moral reasoning is essential for generating
ethically appropriate responses. Yet, the dominant pretraining of LLMs on
English-language data raises critical concerns about their ability to
generalize judgments across diverse linguistic and cultural contexts. In this
work, we systematically investigate how language mediates moral decision-making
in LLMs. We translate two established moral reasoning benchmarks into five
culturally and typologically diverse languages, enabling multilingual zero-shot
evaluation. Our analysis reveals significant inconsistencies in LLMs' moral
judgments across languages, often reflecting cultural misalignment. Through a
combination of carefully constructed research questions, we uncover the
underlying drivers of these disparities, ranging from disagreements to
reasoning strategies employed by LLMs. Finally, through a case study, we link
the role of pretraining data in shaping an LLM's moral compass. Through this
work, we distill our insights into a structured typology of moral reasoning
errors that calls for more culturally-aware AI.

</details>


### [347] [Learning to Reason with Mixture of Tokens](https://arxiv.org/abs/2509.21482)
*Adit Jain,Brendan Rappazzo*

Main category: cs.CL

TL;DR: 本文研究RLVR中混合令牌生成（MoT - G），提出统一框架，实验显示MoT - G方法在推理任务上有显著提升且训练效率更高。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法在推理步骤中采样离散令牌，丢弃模型概率分布中的丰富信息，限制了推理搜索空间，需要改进。

Method: 提出统一框架推广现有MoT - G方法，将RLVR扩展到连续混合空间生成思维链。

Result: 在Reasoning - Gym上评估，MoT - G方法相比标准解码有5 - 35%提升，用半数轨迹达到相近准确率。

Conclusion: MoT - G的优势可能源于在推理过程中保持更高隐藏状态熵和促进令牌空间探索。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a leading
approach for improving large language model (LLM) reasoning capabilities. Most
current methods follow variants of Group Relative Policy Optimization, which
samples multiple reasoning completions, scores them relative to each other, and
adjusts the policy accordingly. However, these approaches invariably sample
discrete tokens at each reasoning step, discarding the rich distributional
information in the model's probability distribution over candidate tokens.
While preserving and utilizing this distributional information has proven
beneficial in non-RL settings, current RLVR methods seem to be unnecessarily
constraining the reasoning search space by not using this information. To
address this limitation, we investigate mixture-of-token generation (MoT-G) in
RLVR. We present a unified framework that generalizes existing MoT-G
approaches, including existing training-free methods that construct mixture
embeddings as weighted sums over token embeddings, and extend RLVR to operate
directly in this continuous mixture space for generating chain-of-thought.
Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive
language tasks, we find that MoT--G methods achieve substantial improvements
(5--35 \% gains on 7 out of 10 tasks) compared to standard decoding with the
Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of
trajectories, suggesting improved training efficiency. Through comprehensive
hidden-state and token-level analyses, we provide evidence that MoT--G's
benefits may stem from its ability to maintain higher hidden-state entropy
throughout the reasoning process and promote exploration in token space.

</details>


### [348] [Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning](https://arxiv.org/abs/2509.21487)
*Jillian Xu,Dylan Zhou,Vinay Shukla,Yang Yang,Junrui Ruan,Shuhuai Lin,Wenfei Zou,Yinxiao Liu,Karthik Lakshmanan*

Main category: cs.CL

TL;DR: 提出DHRD方法解决CoT提示在分类任务中的吞吐量问题，在多个任务上有提升且推理吞吐量高。


<details>
  <summary>Details</summary>
Motivation: 解决Chain-of-Thought (CoT)提示在提升分类准确率时带来的显著吞吐量惩罚问题。

Method: 提出Dual-Head Reasoning Distillation (DHRD)训练方法，为自回归语言模型添加分类头和推理头，用加权损失函数训练。

Result: 在七个SuperGLUE任务上，相对池化基线有0.65 - 5.47%的提升，推理吞吐量与池化分类器相当，比CoT解码快96 - 142倍。

Conclusion: DHRD方法有效解决了CoT提示的吞吐量问题，在分类任务上表现良好。

Abstract: Chain-of-Thought (CoT) prompting often improves classification accuracy, but
it introduces a significant throughput penalty with rationale generation (Wei
et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we
introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for
decoder-only language models (LMs) that adds (i) a pooled classification head
used during training and inference and (ii) a reasoning head supervised by
teacher rationales used only in training. We train with a loss function that is
a weighted sum of label cross-entropy and token-level LM loss over
input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative
gains of 0.65-5.47% over pooled baselines, with notably larger gains on
entailment/causal tasks. Since we disable the reasoning head at test time,
inference throughput matches pooled classifiers and exceeds CoT decoding on the
same backbones by 96-142 times in QPS.

</details>


### [349] [Agribot: agriculture-specific question answer system](https://arxiv.org/abs/2509.21535)
*Naman Jain,Pranjali Jain,Pratik Kayal,Jayakrishna Sahit,Soham Pachpande,Jayesh Choudhari*

Main category: cs.CL

TL;DR: 本文介绍了基于Kisan呼叫中心数据集构建的农业聊天机器人，该系统能回答多种农业相关问题，经优化后准确率提升，有助于农民获取信息和提高农业产出。


<details>
  <summary>Details</summary>
Motivation: 印度是农业经济体，获取农业实践的正确信息对农业增长和产出至关重要，为解答农民疑问而构建农业聊天机器人。

Method: 基于句子嵌入模型构建系统，通过消除同义词和实体提取进行优化。

Result: 初始系统准确率为56%，消除同义词和加入实体提取后准确率提升至86%。

Conclusion: 该系统有助于农民获取农业相关信息，提高农业产出，还能减轻呼叫中心工作人员的工作并引导其达成更好目标。

Abstract: India is an agro-based economy and proper information about agricultural
practices is the key to optimal agricultural growth and output. In order to
answer the queries of the farmer, we have build an agricultural chatbot based
on the dataset from Kisan Call Center. This system is robust enough to answer
queries related to weather, market rates, plant protection and government
schemes. This system is available 24* 7, can be accessed through any electronic
device and the information is delivered with the ease of understanding. The
system is based on a sentence embedding model which gives an accuracy of 56%.
After eliminating synonyms and incorporating entity extraction, the accuracy
jumps to 86%. With such a system, farmers can progress towards easier
information about farming related practices and hence a better agricultural
output. The job of the Call Center workforce would be made easier and the hard
work of various such workers can be redirected to a better goal.

</details>


### [350] [Domain-Aware Speaker Diarization On African-Accented English](https://arxiv.org/abs/2509.21554)
*Chibuzor Okocha,Kelechi Ezema,Christan Grant*

Main category: cs.CL

TL;DR: 研究非洲口音英语说话人分割的领域效应，评估系统，分析误差，尝试域适应，给出贡献和下一步方向。


<details>
  <summary>Details</summary>
Motivation: 研究非洲口音英语说话人分割中的领域效应。

Method: 在严格DER协议下评估多种系统，进行误差分析，微调分割模块进行轻量级域适应。

Result: 临床语音存在领域惩罚，域适应可减少误差但未消除差距。

Conclusion: 提出跨领域基准、误差分解等贡献，指出重叠感知分割和平衡临床资源是下一步方向。

Abstract: This study examines domain effects in speaker diarization for
African-accented English. We evaluate multiple production and open systems on
general and clinical dialogues under a strict DER protocol that scores overlap.
A consistent domain penalty appears for clinical speech and remains significant
across models. Error analysis attributes much of this penalty to false alarms
and missed detections, aligning with short turns and frequent overlap. We test
lightweight domain adaptation by fine-tuning a segmentation module on
accent-matched data; it reduces error but does not eliminate the gap. Our
contributions include a controlled benchmark across domains, a concise approach
to error decomposition and conversation-level profiling, and an adaptation
recipe that is easy to reproduce. Results point to overlap-aware segmentation
and balanced clinical resources as practical next steps.

</details>


### [351] [Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective](https://arxiv.org/abs/2509.21613)
*Lingxiao Kong,Cong Yang,Oya Deniz Beyan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: 介绍多目标强化学习（MORL）在大语言模型（LLM）优化中的挑战与机遇，提出MORL分类法、基准框架愿景和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决MORL在LLM优化中面临的挑战，寻找高效灵活且适应个性化和复杂特性的方法。

Method: 引入MORL分类法，提出MORL基准框架愿景，聚焦元策略MORL发展。

Result: 识别出MORL方法应用于LLM优化的优缺点，明确高效灵活方法的需求。

Conclusion: 未来可通过元策略MORL的双层学习范式提高效率和灵活性，改善LLM性能。

Abstract: Multi-Objective Reinforcement Learning (MORL) presents significant challenges
and opportunities for optimizing multiple objectives in Large Language Models
(LLMs). We introduce a MORL taxonomy and examine the advantages and limitations
of various MORL methods when applied to LLM optimization, identifying the need
for efficient and flexible approaches that accommodate personalization
functionality and inherent complexities in LLMs and RL. We propose a vision for
a MORL benchmarking framework that addresses the effects of different methods
on diverse objective relationships. As future research directions, we focus on
meta-policy MORL development that can improve efficiency and flexibility
through its bi-level learning paradigm, highlighting key research questions and
potential solutions for improving LLM performance.

</details>


### [352] [OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule](https://arxiv.org/abs/2509.21623)
*Yuxuan Zhu,David H. Yang,Mohammad Mohammadi Amiri,Keerthiram Murugesan,Tejaswini Pedapati,Pin-Yu Chen*

Main category: cs.CL

TL;DR: 论文提出OjaKV框架解决大语言模型长上下文推理中KV缓存的内存瓶颈问题，实验证明其在高压缩比下能维持或提升准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型长上下文能力受KV缓存内存瓶颈限制，现有KV缓存压缩方法在数据分布变化时性能不佳。

Method: 引入OjaKV框架，采用混合存储策略，对重要令牌全秩存储，对多数中间令牌用Oja算法在线适应投影基进行低秩压缩，且兼容现代注意力模块。

Result: OjaKV在高压缩比下能维持或提升零样本准确率，在长上下文复杂推理基准测试中表现出色。

Conclusion: OjaKV是一种无需模型微调、实用的即插即用解决方案，可实现内存高效的长上下文推理。

Abstract: The expanding long-context capabilities of large language models are
constrained by a significant memory bottleneck: the key-value (KV) cache
required for autoregressive generation. This bottleneck is substantial; for
instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of
4 requires approximately 16GB for its KV cache, a size exceeding the model's
weights. While KV-cache compression via low-rank projection is a promising
direction, existing methods rely on a static, offline-learned subspace that
performs poorly under data distribution shifts. To overcome these limitations,
we introduce OjaKV, a novel framework that integrates a strategic hybrid
storage policy with online subspace adaptation. First, OjaKV recognizes that
not all tokens are equally important for compression; it preserves the crucial
first and most recent tokens in full-rank, maintaining high-fidelity anchors
for attention. Second, for the vast majority of intermediate tokens, it applies
low-rank compression by incrementally adapting the projection basis using Oja's
algorithm for online principal component analysis. This adaptation involves a
comprehensive update during prompt prefilling and lightweight periodic updates
during decoding, ensuring the subspace remains aligned with the evolving
context. Crucially, our framework is fully compatible with modern attention
modules like FlashAttention. Experiments demonstrate that OjaKV maintains or
even improves zero-shot accuracy at high compression ratios. In particular,
OjaKV achieves its strongest gains on very long-context benchmarks that require
complex reasoning, highlighting the importance of online subspace adaptation in
dynamically tracking context shifts. These results establish our hybrid
framework as a practical, plug-and-play solution for memory-efficient
long-context inference without requiring model fine-tuning.

</details>


### [353] [Self-Speculative Biased Decoding for Faster Live Translation](https://arxiv.org/abs/2509.21740)
*Linxiao Zeng,Haoyun Deng,Kangyuan Shu,Shizhen Wang*

Main category: cs.CL

TL;DR: 本文提出Self - Speculative Biased Decoding方法，用于加速流式应用，在不损失质量下实现1.7倍加速并减少80%闪烁。


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以直接用于流式应用，需在输入不断扩展时更新输出并控制计算成本以满足延迟要求。

Method: 提出Self - Speculative Biased Decoding推理范式，用最新输出作草稿，验证阶段使输出偏向草稿令牌，验证后常规解码继续。

Result: 在同时文本到文本重新翻译实验中，比传统自回归重新翻译实现1.7倍加速，结合显示掩码技术减少80%闪烁。

Conclusion: 该方法无需草稿计算，是与模型无关、即插即用的解决方案，可加速对延迟敏感的流式应用。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in various text generation tasks. However, it remains challenging
to use them off-the-shelf in streaming applications (such as live translation),
where the output must continually update as the input context expands, while
still maintaining a reasonable computational cost to meet the latency
requirement.
  In this work, we reexamine the re-translation approach to simultaneous
translation and propose Self-Speculative Biased Decoding, a novel inference
paradigm designed to avoid repeatedly generating output from scratch for a
consistently growing input stream. We propose using the most recent output as a
draft for the current growing input context. During the verification stage, the
output will be biased towards the draft token for a higher draft acceptance
rate. This strategy not only minimizes flickering that might distract users but
also leads to higher speedups. Conventional decoding may take charge from the
point of divergence after draft verification and continue until the end
condition is met.
  Unlike existing speculative decoding strategies, our approach eliminates the
need for draft computations, making it a model-agnostic and plug-and-play
solution for accelerating latency-sensitive streaming applications.
Experimental results on simultaneous text-to-text re-translation demonstrate
that our approach achieves up to 1.7x speedup compared to conventional
auto-regressive re-translation without compromising quality. Additionally, it
significantly reduces flickering by 80% by incorporating the display-only
mask-k technique.

</details>


### [354] [Evaluating and Improving Cultural Awareness of Reward Models for LLM Alignment](https://arxiv.org/abs/2509.21798)
*Hongbin Zhang,Kehai Chen,Xuefeng Bai,Yang Xiang,Min Zhang*

Main category: cs.CL

TL;DR: 提出文化意识奖励建模基准CARB评估奖励模型文化意识，发现问题并提出Think - as - Locals方法，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型评估缺乏文化意识评估数据集，为推进大语言模型全球对齐，需评估奖励模型文化意识。

Method: 提出CARB基准评估现有奖励模型，分析问题后提出Think - as - Locals方法，通过可验证奖励的强化学习来解决问题。

Result: 评估发现现有奖励模型在文化意识建模上有不足，Think - as - Locals方法能减轻虚假特征干扰，推动文化意识奖励建模。

Conclusion: Think - as - Locals方法在文化意识奖励建模方面有效，可促进大语言模型全球对齐。

Abstract: Reward models (RMs) are crucial for aligning large language models (LLMs)
with diverse cultures. Consequently, evaluating their cultural awareness is
essential for further advancing global alignment of LLMs. However, existing RM
evaluations fall short in assessing cultural awareness due to the scarcity of
culturally relevant evaluation datasets. To fill this gap, we propose Cultural
Awareness Reward modeling Benchmark (CARB), covering 10 distinct cultures
across 4 cultural domains. Our extensive evaluation of state-of-the-art RMs
reveals their deficiencies in modeling cultural awareness and demonstrates a
positive correlation between performance on CARB and downstream multilingual
cultural alignment tasks. Further analysis identifies the spurious correlations
within culture-aware reward modeling, wherein RM's scoring relies predominantly
on surface-level features rather than authentic cultural nuance understanding.
To address these, we propose Think-as-Locals to elicit deeper culturally
grounded reasoning from generative RMs via reinforcement learning from
verifiable rewards (RLVR) and employ well-designed rewards to ensure accurate
preference judgments and high-quality structured evaluation criteria
generation. Experimental results validate its efficacy in mitigating spurious
features interference and advancing culture-aware reward modeling.

</details>


### [355] [Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations](https://arxiv.org/abs/2509.21870)
*Guanzhi Deng,Mingyang Liu,Dapeng Wu,Yinqiao Li,Linqi Song*

Main category: cs.CL

TL;DR: 提出LoRAN和Sinter，实验表明LoRAN优于QLoRA，Sinter优于标准激活函数。


<details>
  <summary>Details</summary>
Motivation: LoRA线性特性限制表达能力，需非线性扩展方法。

Method: 提出LoRAN对低秩更新应用轻量级变换，引入Sinter激活函数添加结构化扰动。

Result: 在摘要和分类任务中，LoRAN始终优于QLoRA，Sinter优于标准激活函数。

Conclusion: 激活函数设计在低秩调优中很重要。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient
fine-tuning method for large language models. However, its linear nature limits
expressiveness. We propose LoRAN, a non-linear extension of LoRA that applies
lightweight transformations to the low-rank updates. We further introduce
Sinter, a sine-based activation that adds structured perturbations without
increasing parameter count. Experiments across summarization and classification
tasks show that LoRAN consistently improves over QLoRA. Ablation studies reveal
that Sinter outperforms standard activations such as Sigmoid, ReLU, and Tanh,
highlighting the importance of activation design in lowrank tuning.

</details>


### [356] [No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping](https://arxiv.org/abs/2509.21880)
*Thanh-Long V. Le,Myeongho Jeon,Kim Vu,Viet Lai,Eunho Yang*

Main category: cs.CL

TL;DR: 本文指出当前强化学习方法忽略零方差提示问题，提出RL - ZVP算法从该类提示中提取学习信号，在六个数学推理基准测试中表现优于GRPO等基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于可验证奖励的强化学习方法（如GRPO）忽略零方差提示，本文认为此类提示能为策略优化提供有意义反馈，因此有必要研究利用这些提示。

Method: 引入RL - ZVP算法，该算法能直接奖励正确、惩罚错误，在不对比响应的情况下，用标记级特征调节反馈以保留信息丰富、细微的信号。

Result: 在六个数学推理基准测试中，RL - ZVP比GRPO在准确率上最多提高8.61分，通过率最多提高7.77分，且始终优于过滤零方差提示的其他基线方法。

Conclusion: 凸显了在基于可验证奖励的强化学习中，从零方差提示学习的未开发潜力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework
for improving the reasoning abilities of Large Language Models (LLMs). However,
current methods such as GRPO rely only on problems where the model responses to
the same input differ in correctness, while ignoring those where all responses
receive the same reward - so-called zero-variance prompts. In this work, we
argue that such prompts are not useless but can, in fact, provide meaningful
feedback for policy optimization. To this end, we introduce RL with
Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals
from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes
errors even without contrasting responses, modulating feedback with token-level
characteristics to preserve informative, nuanced signals. Across six math
reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61
points in accuracy and 7.77 points in pass rate over GRPO, while consistently
outperforming other baselines that filter out zero-variance prompts. These
results highlight the untapped potential of learning from zero-variance prompts
in RLVR.

</details>


### [357] [Elastic MoE: Unlocking the Inference-Time Scalability of Mixture-of-Experts](https://arxiv.org/abs/2509.21892)
*Naibin Gu,Zhenyu Zhang,Yuchen Feng,Yilong Chen,Peng Fu,Zheng Lin,Shuohuan Wang,Yu Sun,Hua Wu,Weiping Wang,Haifeng Wang*

Main category: cs.CL

TL;DR: 传统MoE模型训练和推理时激活专家数量固定，增加推理时激活专家数量性能反而下降，提出EMoE框架解决该问题并提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型增加推理时激活专家数量性能会迅速下降，原因是专家间缺乏协作，需解决该问题。

Method: 引入Elastic Mixture - of - Experts (EMoE)训练框架，同时训练专家以不同组合协作并鼓励路由进行高质量选择。

Result: EMoE显著扩展了有效性能扩展范围，达到训练时激活专家数量的2 - 3倍，还提升了模型峰值性能。

Conclusion: EMoE能让MoE模型在推理时扩展激活专家数量，且不增加训练开销，有效提升性能。

Abstract: Mixture-of-Experts (MoE) models typically fix the number of activated experts
$k$ at both training and inference. Intuitively, activating more experts at
inference $k'$ (where $k'> k$) means engaging a larger set of model parameters
for the computation and thus is expected to improve performance. However,
contrary to this intuition, we find the scaling range to be so narrow that
performance begins to degrade rapidly after only a slight increase in the
number of experts. Further investigation reveals that this degradation stems
from a lack of learned collaboration among experts. To address this, we
introduce Elastic Mixture-of-Experts (EMoE), a novel training framework that
enables MoE models to scale the number of activated experts at inference
without incurring additional training overhead. By simultaneously training
experts to collaborate in diverse combinations and encouraging the router for
high-quality selections, EMoE ensures robust performance across computational
budgets at inference. We conduct extensive experiments on various MoE settings.
Our results show that EMoE significantly expands the effective
performance-scaling range, extending it to as much as 2-3$\times$ the
training-time $k$, while also pushing the model's peak performance to a higher
level.

</details>


### [358] [A Large-Scale Dataset and Citation Intent Classification in Turkish with LLMs](https://arxiv.org/abs/2509.21907)
*Kemal Sami Karaca,Bahaeddin Eravcı*

Main category: cs.CL

TL;DR: 本文为解决土耳其语引文意图定性理解问题，引入系统方法和基础数据集，通过DSPy框架优化提示，用堆叠泛化集成实现91.3%准确率，为相关研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 全面评估学术研究需理解引文定性意图，而土耳其语等黏着语在此方面面临独特挑战。

Method: 创建土耳其语引文意图公开数据集，评估标准上下文学习效果，基于DSPy框架构建可编程分类管道优化提示，采用堆叠泛化集成进行最终分类。

Result: 使用XGBoost元模型的集成达到了91.3%的准确率。

Conclusion: 为土耳其NLP社区和学术界提供了基础数据集和强大的分类框架，为未来定性引文研究铺平道路。

Abstract: Understanding the qualitative intent of citations is essential for a
comprehensive assessment of academic research, a task that poses unique
challenges for agglutinative languages like Turkish. This paper introduces a
systematic methodology and a foundational dataset to address this problem. We
first present a new, publicly available dataset of Turkish citation intents,
created with a purpose-built annotation tool. We then evaluate the performance
of standard In-Context Learning (ICL) with Large Language Models (LLMs),
demonstrating that its effectiveness is limited by inconsistent results caused
by manually designed prompts. To address this core limitation, we introduce a
programmable classification pipeline built on the DSPy framework, which
automates prompt optimization systematically. For final classification, we
employ a stacked generalization ensemble to aggregate outputs from multiple
optimized models, ensuring stable and reliable predictions. This ensemble, with
an XGBoost meta-model, achieves a state-of-the-art accuracy of 91.3\%.
Ultimately, this study provides the Turkish NLP community and the broader
academic circles with a foundational dataset and a robust classification
framework paving the way for future qualitative citation studies.

</details>


### [359] [AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition](https://arxiv.org/abs/2509.21910)
*Yun Wang,Zhaojun Ding,Xuansheng Wu,Siyue Sun,Ninghao Liu,Xiaoming Zhai*

Main category: cs.CL

TL;DR: 论文提出AutoSCORE多智能体大语言模型框架用于自动评分，在多个基准数据集上验证其能提升评分准确性、人机一致性等。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为端到端评分器存在准确率低、对提示敏感、可解释性有限和与评分标准不一致等问题，阻碍了基于大语言模型的自动评分在评估实践中的应用。

Method: 提出AutoSCORE框架，包含评分标准组件提取智能体和评分智能体，先从学生回答中提取与评分标准相关的组件并编码为结构化表示，再据此分配最终分数。

Result: 在四个基准数据集上评估，使用多种专有和开源大语言模型，与单智能体基线相比，AutoSCORE在不同任务和评分标准下持续提高了评分准确性、人机一致性和误差指标，在复杂多维评分标准和较小大语言模型上效果更佳。

Conclusion: 结构化组件识别与多智能体设计相结合，为自动评分提供了可扩展、可靠且可解释的解决方案。

Abstract: Automated scoring plays a crucial role in education by reducing the reliance
on human raters, offering scalable and immediate evaluation of student work.
While large language models (LLMs) have shown strong potential in this task,
their use as end-to-end raters faces challenges such as low accuracy, prompt
sensitivity, limited interpretability, and rubric misalignment. These issues
hinder the implementation of LLM-based automated scoring in assessment
practice. To address the limitations, we propose AutoSCORE, a multi-agent LLM
framework enhancing automated scoring via rubric-aligned Structured COmponent
REcognition. With two agents, AutoSCORE first extracts rubric-relevant
components from student responses and encodes them into a structured
representation (i.e., Scoring Rubric Component Extraction Agent), which is then
used to assign final scores (i.e., Scoring Agent). This design ensures that
model reasoning follows a human-like grading process, enhancing
interpretability and robustness. We evaluate AutoSCORE on four benchmark
datasets from the ASAP benchmark, using both proprietary and open-source LLMs
(GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Across diverse tasks and rubrics,
AutoSCORE consistently improves scoring accuracy, human-machine agreement (QWK,
correlations), and error metrics (MAE, RMSE) compared to single-agent
baselines, with particularly strong benefits on complex, multi-dimensional
rubrics, and especially large relative gains on smaller LLMs. These results
demonstrate that structured component recognition combined with multi-agent
design offers a scalable, reliable, and interpretable solution for automated
scoring.

</details>


### [360] [Why Chain of Thought Fails in Clinical Text Understanding](https://arxiv.org/abs/2509.21933)
*Jiageng Wu,Kevin Xie,Bowen Gu,Nils Krüger,Kueiyu Joshua Lin,Jie Yang*

Main category: cs.CL

TL;DR: 对临床文本理解中思维链提示（CoT）进行大规模系统研究，发现多数模型在CoT设置下性能下降，揭示CoT在临床环境失败的模式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床护理应用中准确性和透明推理很重要，CoT在临床环境尤其是电子健康记录中的有效性待探索。

Method: 对95个先进大语言模型在87个真实临床文本任务上进行评估，涵盖9种语言和8种任务类型，进行细粒度分析，结合大模型评判和临床专家评估。

Result: 86.3%的模型在CoT设置下性能持续下降，能力强的模型较稳健，能力弱的模型下降明显，发现CoT在临床环境失败的系统模式。

Conclusion: 为大语言模型的临床推理策略提供实证基础，强调需要透明可信的方法。

Abstract: Large language models (LLMs) are increasingly being applied to clinical care,
a domain where both accuracy and transparent reasoning are critical for safe
and trustworthy deployment. Chain-of-thought (CoT) prompting, which elicits
step-by-step reasoning, has demonstrated improvements in performance and
interpretability across a wide range of tasks. However, its effectiveness in
clinical contexts remains largely unexplored, particularly in the context of
electronic health records (EHRs), the primary source of clinical documentation,
which are often lengthy, fragmented, and noisy. In this work, we present the
first large-scale systematic study of CoT for clinical text understanding. We
assess 95 advanced LLMs on 87 real-world clinical text tasks, covering 9
languages and 8 task types. Contrary to prior findings in other domains, we
observe that 86.3\% of models suffer consistent performance degradation in the
CoT setting. More capable models remain relatively robust, while weaker ones
suffer substantial declines. To better characterize these effects, we perform
fine-grained analyses of reasoning length, medical concept alignment, and error
profiles, leveraging both LLM-as-a-judge evaluation and clinical expert
evaluation. Our results uncover systematic patterns in when and why CoT fails
in clinical contexts, which highlight a critical paradox: CoT enhances
interpretability but may undermine reliability in clinical text tasks. This
work provides an empirical basis for clinical reasoning strategies of LLMs,
highlighting the need for transparent and trustworthy approaches.

</details>


### [361] [Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration](https://arxiv.org/abs/2509.21946)
*Kasidit Sermsri,Teerapong Panboonyuen*

Main category: cs.CL

TL;DR: 提出ThaiFACTUAL框架缓解低资源、文化复杂环境下大语言模型政治立场检测的偏差，发布泰语政治立场数据集，实验显示该框架有显著效果。


<details>
  <summary>Details</summary>
Motivation: 低资源、文化复杂环境下大语言模型在政治立场检测中存在系统偏差，影响公平性和可靠性。

Method: 提出ThaiFACTUAL框架，使用反事实数据增强和基于理由的监督，同时发布高质量泰语政治立场数据集。

Result: ThaiFACTUAL显著减少虚假关联，增强零样本泛化能力，提高多个大语言模型的公平性。

Conclusion: 强调了针对代表性不足语言的文化基础去偏技术的重要性。

Abstract: Political stance detection in low-resource and culturally complex settings
poses a critical challenge for large language models (LLMs). In the Thai
political landscape - marked by indirect language, polarized figures, and
entangled sentiment and stance - LLMs often display systematic biases such as
sentiment leakage and favoritism toward entities. These biases undermine
fairness and reliability. We present ThaiFACTUAL, a lightweight, model-agnostic
calibration framework that mitigates political bias without requiring
fine-tuning. ThaiFACTUAL uses counterfactual data augmentation and
rationale-based supervision to disentangle sentiment from stance and reduce
bias. We also release the first high-quality Thai political stance dataset,
annotated with stance, sentiment, rationales, and bias markers across diverse
entities and events. Experimental results show that ThaiFACTUAL significantly
reduces spurious correlations, enhances zero-shot generalization, and improves
fairness across multiple LLMs. This work highlights the importance of
culturally grounded debiasing techniques for underrepresented languages.

</details>


### [362] [Black-Box Hallucination Detection via Consistency Under the Uncertain Expression](https://arxiv.org/abs/2509.21999)
*Seongho Joo,Kyungmin Min,Jahyun Koo,Kyomin Jung*

Main category: cs.CL

TL;DR: 现有大语言模型存在幻觉问题，本文提出基于不确定性表达的黑盒幻觉检测指标，实验表明该指标优于使用LLM内部知识的基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉问题，现有检测和缓解方法依赖外部资源或LLM内部状态，而外部API受限、外部资源有限，急需建立黑盒方法。

Method: 研究LLM在不确定性表达下的行为，发现事实性回答时响应一致，非事实性回答时不一致，据此提出黑盒幻觉检测指标。

Result: 实验表明提出的指标比使用LLM内部知识的基线更能预测模型响应的事实性。

Conclusion: 提出的基于不确定性表达的黑盒幻觉检测指标是有效的。

Abstract: Despite the great advancement of Language modeling in recent days, Large
Language Models (LLMs) such as GPT3 are notorious for generating non-factual
responses, so-called "hallucination" problems. Existing methods for detecting
and alleviating this hallucination problem require external resources or the
internal state of LLMs, such as the output probability of each token. Given the
LLM's restricted external API availability and the limited scope of external
resources, there is an urgent demand to establish the Black-Box approach as the
cornerstone for effective hallucination detection. In this work, we propose a
simple black-box hallucination detection metric after the investigation of the
behavior of LLMs under expression of uncertainty. Our comprehensive analysis
reveals that LLMs generate consistent responses when they present factual
responses while non-consistent responses vice versa. Based on the analysis, we
propose an efficient black-box hallucination detection metric with the
expression of uncertainty. The experiment demonstrates that our metric is more
predictive of the factuality in model responses than baselines that use
internal knowledge of LLMs.

</details>


### [363] [Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity](https://arxiv.org/abs/2509.22054)
*Ping Chen,Xiang Liu,Zhaoxiang Liu,Zezhou Chen,Xingpeng Zhang,Huan Hu,Zipeng Wang,Kai Wang,Shuming Shi,Shiguo Lian*

Main category: cs.CL

TL;DR: 介绍模糊推理链（FRC）框架处理文本歧义等问题，在情感分析任务验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理模糊、多义或不确定文本存在挑战。

Method: 引入FRC框架，集成大语言模型语义先验与连续模糊隶属度，实现概率推理与模糊隶属推理交互。

Result: 在情感分析任务中，理论分析和实证结果表明FRC确保稳定推理，促进不同模型规模间知识转移。

Conclusion: FRC为处理微妙和模糊表达提供通用机制，提高可解释性和鲁棒性。

Abstract: With the rapid advancement of large language models (LLMs), natural language
processing (NLP) has achieved remarkable progress. Nonetheless, significant
challenges remain in handling texts with ambiguity, polysemy, or uncertainty.
We introduce the Fuzzy Reasoning Chain (FRC) framework, which integrates LLM
semantic priors with continuous fuzzy membership degrees, creating an explicit
interaction between probability-based reasoning and fuzzy membership reasoning.
This transition allows ambiguous inputs to be gradually transformed into clear
and interpretable decisions while capturing conflicting or uncertain signals
that traditional probability-based methods cannot. We validate FRC on sentiment
analysis tasks, where both theoretical analysis and empirical results show that
it ensures stable reasoning and facilitates knowledge transfer across different
model scales. These findings indicate that FRC provides a general mechanism for
managing subtle and ambiguous expressions with improved interpretability and
robustness.

</details>


### [364] [The QCET Taxonomy of Standard Quality Criterion Names and Definitions for the Evaluation of NLP Systems](https://arxiv.org/abs/2509.22064)
*Anya Belz,Simon Mille,Craig Thomson*

Main category: cs.CL

TL;DR: 现有NLP评估实验中相同质量标准名称可能含义不同，缺乏可比性，QCET质量标准分类法从调查中得出标准集并构建层次结构，可用于建立评估可比性、指导新评估设计和评估监管合规性。


<details>
  <summary>Details</summary>
Motivation: 解决NLP评估中因质量标准名称含义不明确导致的评估结果不可比问题，推动该领域科学进步。

Method: 采用描述性方法，从三次NLP评估调查中得出标准的质量标准名称和定义，并构建层次结构。

Result: 提出了QCET及其包含的资源。

Conclusion: QCET可用于建立现有评估的可比性、指导新评估设计和评估监管合规性。

Abstract: Prior work has shown that two NLP evaluation experiments that report results
for the same quality criterion name (e.g. Fluency) do not necessarily evaluate
the same aspect of quality, and the comparability implied by the name can be
misleading. Not knowing when two evaluations are comparable in this sense means
we currently lack the ability to draw reliable conclusions about system quality
on the basis of multiple, independently conducted evaluations. This in turn
hampers the ability of the field to progress scientifically as a whole, a
pervasive issue in NLP since its beginning (Sparck Jones, 1981). It is hard to
see how the issue of unclear comparability can be fully addressed other than by
the creation of a standard set of quality criterion names and definitions that
the several hundred quality criterion names actually in use in the field can be
mapped to, and grounded in. Taking a strictly descriptive approach, the QCET
Quality Criteria for Evaluation Taxonomy derives a standard set of quality
criterion names and definitions from three surveys of evaluations reported in
NLP, and structures them into a hierarchy where each parent node captures
common aspects of its child nodes. We present QCET and the resources it
consists of, and discuss its three main uses in (i) establishing comparability
of existing evaluations, (ii) guiding the design of new evaluations, and (iii)
assessing regulatory compliance.

</details>


### [365] [Universal Legal Article Prediction via Tight Collaboration between Supervised Classification Model and LLM](https://arxiv.org/abs/2509.22119)
*Xiao Chi,Wenlin Zhong,Yiquan Wu,Wei Wang,Kun Kuang,Fei Wu,Minghui Xiong*

Main category: cs.CL

TL;DR: 提出通用法律条文预测框架Uni - LAP，结合SCMs和LLMs优势，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有法律条文预测方法存在不足，如SCMs难以捕捉复杂事实模式，LLMs在预测场景表现不佳，且多数方法缺乏广泛适用性。

Method: 提出Uni - LAP框架，用新颖的Top - K损失函数增强SCM生成候选条文，LLM采用三段论推理完善最终预测。

Result: 在多司法管辖区数据集上评估，Uni - LAP始终优于现有基线。

Conclusion: Uni - LAP有效且具有泛化性，能解决现有法律条文预测方法的局限。

Abstract: Legal Article Prediction (LAP) is a critical task in legal text
classification, leveraging natural language processing (NLP) techniques to
automatically predict relevant legal articles based on the fact descriptions of
cases. As a foundational step in legal decision-making, LAP plays a pivotal
role in determining subsequent judgments, such as charges and penalties.
Despite its importance, existing methods face significant challenges in
addressing the complexities of LAP. Supervised classification models (SCMs),
such as CNN and BERT, struggle to fully capture intricate fact patterns due to
their inherent limitations. Conversely, large language models (LLMs), while
excelling in generative tasks, perform suboptimally in predictive scenarios due
to the abstract and ID-based nature of legal articles. Furthermore, the
diversity of legal systems across jurisdictions exacerbates the issue, as most
approaches are tailored to specific countries and lack broader applicability.
To address these limitations, we propose Uni-LAP, a universal framework for
legal article prediction that integrates the strengths of SCMs and LLMs through
tight collaboration. Specifically, in Uni-LAP, the SCM is enhanced with a novel
Top-K loss function to generate accurate candidate articles, while the LLM
employs syllogism-inspired reasoning to refine the final predictions. We
evaluated Uni-LAP on datasets from multiple jurisdictions, and empirical
results demonstrate that our approach consistently outperforms existing
baselines, showcasing its effectiveness and generalizability.

</details>


### [366] [R-Capsule: Compressing High-Level Plans for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2509.22131)
*Hongyu Shan,Mingyang Song,Chang Dai,Di Liang,Han Chen*

Main category: cs.CL

TL;DR: 提出推理胶囊（R-Capsule）框架，结合潜在推理效率与显式思维链透明度，平衡效率、准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 思维链（CoT）提示虽能助力大语言模型处理复杂推理，但存在冗长增加延迟和内存使用、可能传播早期错误等问题，需改进。

Method: 将高级计划压缩为一组学习的潜在令牌（推理胶囊），采用信息瓶颈（IB）原则，通过低容量瓶颈鼓励最小化，通过双重目标（主要任务损失和辅助计划重建损失）鼓励充分性。

Result: 框架在复杂基准测试中减少推理可见令牌占用，同时保持或提高准确性。

Conclusion: 推理胶囊框架有效平衡了效率、准确性和可解释性。

Abstract: Chain-of-Thought (CoT) prompting helps Large Language Models (LLMs) tackle
complex reasoning by eliciting explicit step-by-step rationales. However, CoT's
verbosity increases latency and memory usage and may propagate early errors
across long chains. We propose the Reasoning Capsule (R-Capsule), a framework
that aims to combine the efficiency of latent reasoning with the transparency
of explicit CoT. The core idea is to compress the high-level plan into a small
set of learned latent tokens (a Reasoning Capsule) while keeping execution
steps lightweight or explicit. This hybrid approach is inspired by the
Information Bottleneck (IB) principle, where we encourage the capsule to be
approximately minimal yet sufficient for the task. Minimality is encouraged via
a low-capacity bottleneck, which helps improve efficiency. Sufficiency is
encouraged via a dual objective: a primary task loss for answer accuracy and an
auxiliary plan-reconstruction loss that encourages the capsule to faithfully
represent the original textual plan. The reconstruction objective helps ground
the latent space, thereby improving interpretability and reducing the use of
uninformative shortcuts. Our framework strikes a balance between efficiency,
accuracy, and interpretability, thereby reducing the visible token footprint of
reasoning while maintaining or improving accuracy on complex benchmarks. Our
codes are available at:
https://anonymous.4open.science/r/Reasoning-Capsule-7BE0

</details>


### [367] [Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding](https://arxiv.org/abs/2509.22134)
*Shijing Hu,Jingyang Li,Zhihui Lu,Pan Zhou*

Main category: cs.CL

TL;DR: 提出Group Tree Optimization (GTO)方法解决草稿策略不一致问题，在多数据集和模型上提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有训练目标仅优化单一贪婪草稿路径，与解码时的树策略不一致，限制了加速效果。

Method: 引入Draft Tree Reward衡量解码性能，采用Group-based Draft Policy Training进行稳定优化。

Result: 在多个数据集和模型上，GTO使接受长度增加7.4%，比EAGLE - 3额外提升7.7%的加速效果。

Conclusion: GTO通过弥合草稿策略不一致问题，为高效大语言模型推理提供实用通用解决方案。

Abstract: Speculative decoding accelerates large language model (LLM) inference by
letting a lightweight draft model propose multiple tokens that the target model
verifies in parallel. Yet existing training objectives optimize only a single
greedy draft path, while decoding follows a tree policy that re-ranks and
verifies multiple branches. This draft policy misalignment limits achievable
speedups. We introduce Group Tree Optimization (GTO), which aligns training
with the decoding-time tree policy through two components: (i) Draft Tree
Reward, a sampling-free objective equal to the expected acceptance length of
the draft tree under the target model, directly measuring decoding performance;
(ii) Group-based Draft Policy Training, a stable optimization scheme that
contrasts trees from the current and a frozen reference draft model, forming
debiased group-standardized advantages and applying a PPO-style surrogate along
the longest accepted sequence for robust updates. We further prove that
increasing our Draft Tree Reward provably improves acceptance length and
speedup. Across dialogue (MT-Bench), code (HumanEval), and math (GSM8K), and
multiple LLMs (e.g., LLaMA-3.1-8B, LLaMA-3.3-70B, Vicuna-1.3-13B,
DeepSeek-R1-Distill-LLaMA-8B), GTO increases acceptance length by 7.4% and
yields an additional 7.7% speedup over prior state-of-the-art EAGLE-3. By
bridging draft policy misalignment, GTO offers a practical, general solution
for efficient LLM inference.

</details>


### [368] [From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement](https://arxiv.org/abs/2509.22144)
*Jianzhi Yan,Le Liu,Youcheng Pan,Shiwei Chen,Zike Yuan,Yang Xiang,Buzhou Tang*

Main category: cs.CL

TL;DR: 提出MACC框架压缩Chain-of-Thought，提升准确率、减少长度和延迟，且性能可预测


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought推理虽提升复杂任务性能，但因冗长导致推理延迟显著增加

Method: 利用token弹性现象，通过多轮细化逐步压缩CoTs，自适应确定最佳压缩深度

Result: 比现有基线平均准确率提高5.6%，平均减少47个token，显著降低延迟，可利用训练集特征可靠预测测试性能

Conclusion: CoT压缩有效且可预测，方法能实现高效模型选择和预测

Abstract: Chain-of-Thought (CoT) reasoning improves performance on complex tasks but
introduces significant inference latency due to verbosity. We propose
Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that
leverages the token elasticity phenomenon--where overly small token budgets can
paradoxically increase output length--to progressively compress CoTs via
multiround refinement. This adaptive strategy allows MACC to determine the
optimal compression depth for each input. Our method achieves an average
accuracy improvement of 5.6 percent over state-of-the-art baselines, while also
reducing CoT length by an average of 47 tokens and significantly lowering
latency. Furthermore, we show that test-time performance--accuracy and token
length--can be reliably predicted using interpretable features like perplexity
and compression rate on the training set. Evaluated across different models,
our method enables efficient model selection and forecasting without repeated
fine-tuning, demonstrating that CoT compression is both effective and
predictable. Our code will be released in https://github.com/Leon221220/MACC.

</details>


### [369] [The Outputs of Large Language Models are Meaningless](https://arxiv.org/abs/2509.22206)
*Anandi Hattiangadi,Anders J. Schoubye*

Main category: cs.CL

TL;DR: 本文论证大语言模型输出无意义，防御相关反驳并讨论输出看似有意义的原因。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型输出是否有意义。

Method: 基于两个关键前提论证，并防御各种反驳。

Result: 得出大语言模型输出无意义的结论。

Conclusion: 即便论证成立，大语言模型输出仍看似有意义且可用于获取知识。

Abstract: In this paper, we offer a simple argument for the conclusion that the outputs
of large language models (LLMs) are meaningless. Our argument is based on two
key premises: (a) that certain kinds of intentions are needed in order for
LLMs' outputs to have literal meanings, and (b) that LLMs cannot plausibly have
the right kinds of intentions. We defend this argument from various types of
responses, for example, the semantic externalist argument that deference can be
assumed to take the place of intentions and the semantic internalist argument
that meanings can be defined purely in terms of intrinsic relations between
concepts, such as conceptual roles. We conclude the paper by discussing why,
even if our argument is sound, the outputs of LLMs nevertheless seem meaningful
and can be used to acquire true beliefs and even knowledge.

</details>


### [370] [Question-Driven Analysis and Synthesis: Building Interpretable Thematic Trees with LLMs for Text Clustering and Controllable Generation](https://arxiv.org/abs/2509.22211)
*Tiago Fernandes Tavares*

Main category: cs.CL

TL;DR: 提出递归主题分区（RTP）框架，利用大语言模型构建二叉树进行文本语料无监督分析，实验表明其比基线模型更易解释，且在下游任务和生成模型中有应用价值。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型在数据稀缺领域存在挑战，且关键词描述的聚类缺乏语义连贯性和可解释性。

Method: 引入递归主题分区（RTP）框架，利用大语言模型交互式构建二叉树，树中节点为自然语言问题对数据进行语义划分。

Result: RTP的问题驱动层次结构比基于关键词的主题更易解释，在下游分类任务中表现良好，其主题路径可作为生成模型的结构化提示。

Conclusion: RTP引入了新的数据探索范式，从统计模式发现转向知识驱动的主题分析，且可作为强大的合成工具。

Abstract: Unsupervised analysis of text corpora is challenging, especially in
data-scarce domains where traditional topic models struggle. While these models
offer a solution, they typically describe clusters with lists of keywords that
require significant manual effort to interpret and often lack semantic
coherence. To address this critical interpretability gap, we introduce
Recursive Thematic Partitioning (RTP), a novel framework that leverages Large
Language Models (LLMs) to interactively build a binary tree. Each node in the
tree is a natural language question that semantically partitions the data,
resulting in a fully interpretable taxonomy where the logic of each cluster is
explicit. Our experiments demonstrate that RTP's question-driven hierarchy is
more interpretable than the keyword-based topics from a strong baseline like
BERTopic. Furthermore, we establish the quantitative utility of these clusters
by showing they serve as powerful features in downstream classification tasks,
particularly when the data's underlying themes correlate with the task labels.
RTP introduces a new paradigm for data exploration, shifting the focus from
statistical pattern discovery to knowledge-driven thematic analysis.
Furthermore, we demonstrate that the thematic paths from the RTP tree can serve
as structured, controllable prompts for generative models. This transforms our
analytical framework into a powerful tool for synthesis, enabling the
consistent imitation of specific characteristics discovered in the source
corpus.

</details>


### [371] [Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model Performance with Limited Data](https://arxiv.org/abs/2509.22224)
*Zishan Ahmad,Saisubramaniam Gopalakrishnan*

Main category: cs.CL

TL;DR: 本文提出复合推理（CR）方法，让大语言模型能结合多种推理风格解决复杂问题，在科学和医学问答基准测试中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型依赖单一主导推理范式，在复杂问题上表现不佳，需要新方法。

Method: 引入复合推理（CR）方法，使大语言模型动态探索和结合多种推理风格。

Result: 在科学和医学问答基准测试中，CR 方法优于现有基线，如思维链（CoT）和 DeepSeek - R1 风格推理（SR），且样本效率高、token 使用合理，能自适应强调合适推理风格。

Conclusion: 培养大语言模型内部推理风格多样性，可使其获得更强大、自适应和高效的问题解决能力。

Abstract: Large Language Models (LLMs), despite their remarkable capabilities, rely on
singular, pre-dominant reasoning paradigms, hindering their performance on
intricate problems that demand diverse cognitive strategies. To address this,
we introduce Composite Reasoning (CR), a novel reasoning approach empowering
LLMs to dynamically explore and combine multiple reasoning styles like
deductive, inductive, and abductive for more nuanced problem-solving. Evaluated
on scientific and medical question-answering benchmarks, our approach
outperforms existing baselines like Chain-of-Thought (CoT) and also surpasses
the accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while
demonstrating superior sample efficiency and adequate token usage. Notably, CR
adaptively emphasizes domain-appropriate reasoning styles. It prioritizes
abductive and deductive reasoning for medical question answering, but shifts to
causal, deductive, and inductive methods for scientific reasoning. Our findings
highlight that by cultivating internal reasoning style diversity, LLMs acquire
more robust, adaptive, and efficient problem-solving abilities.

</details>


### [372] [Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance](https://arxiv.org/abs/2509.22250)
*Wenbin Hu,Huihao Jing,Haochen Shi,Haoran Li,Yangqiu Song*

Main category: cs.CL

TL;DR: 文章从法律合规角度解决大语言模型（LLM）安全问题，开发新基准并构建合规推理器，实验表明该推理器表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全方法缺乏严格系统保护，无法确保现代LLM系统复杂行为的安全性，因此从法律合规角度解决LLM安全问题。

Method: 以欧盟AI法案和GDPR等法律框架为安全标准，开发新的安全合规基准，使用Group Policy Optimization (GRPO)对Qwen3 - 8B进行对齐构建合规推理器。

Result: 合规推理器在新基准上表现优异，欧盟AI法案平均提升10.45%，GDPR平均提升11.85%。

Conclusion: 从法律合规角度解决LLM安全问题是有效的，构建的合规推理器能有效使LLM与法律标准对齐，降低安全风险。

Abstract: The proliferation of Large Language Models (LLMs) has demonstrated remarkable
capabilities, elevating the critical importance of LLM safety. However,
existing safety methods rely on ad-hoc taxonomy and lack a rigorous, systematic
protection, failing to ensure safety for the nuanced and complex behaviors of
modern LLM systems. To address this problem, we solve LLM safety from legal
compliance perspectives, named safety compliance. In this work, we posit
relevant established legal frameworks as safety standards for defining and
measuring safety compliance, including the EU AI Act and GDPR, which serve as
core legal frameworks for AI safety and data security in Europe. To bridge the
gap between LLM safety and legal compliance, we first develop a new benchmark
for safety compliance by generating realistic LLM safety scenarios seeded with
legal statutes. Subsequently, we align Qwen3-8B using Group Policy Optimization
(GRPO) to construct a safety reasoner, Compliance Reasoner, which effectively
aligns LLMs with legal standards to mitigate safety risks. Our comprehensive
experiments demonstrate that the Compliance Reasoner achieves superior
performance on the new benchmark, with average improvements of +10.45% for the
EU AI Act and +11.85% for GDPR.

</details>


### [373] [Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs](https://arxiv.org/abs/2509.22251)
*Yifang Zhang,Pengfei Duan,Yiwen Yang,Shengwu Xiong*

Main category: cs.CL

TL;DR: 提出SSKG - LLM模型架构，将知识图谱的结构和语义信息融入大语言模型推理过程，进行实验分析并公开代码。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型处理幻觉问题利用知识图谱时存在仅提取语义信息、未利用结构信息，以及知识图谱编码器和大语言模型文本嵌入空间有差距的问题，需解决这些以更好融入结构化知识。

Method: 提出SSKG - LLM模型架构，包含知识图谱检索（KGR）模块、知识图谱编码（KGE）模块来保留语义并利用结构，还有知识图谱适配（KGA）模块让大语言模型理解知识图谱嵌入。

Result: 进行了大量实验并详细分析。

Conclusion: 融入知识图谱的结构信息可增强大语言模型的事实推理能力。

Abstract: Currently, the main approach for Large Language Models (LLMs) to tackle the
hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs
typically treat KGs as plain text, extracting only semantic information and
limiting their use of the crucial structural aspects of KGs. Another challenge
is the gap between the embedding spaces of KGs encoders and LLMs text
embeddings, which hinders the effective integration of structured knowledge. To
overcome these obstacles, we put forward the SSKG-LLM, an innovative model
architecture that is designed to efficiently integrate both the Structural and
Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM
incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph
Encoding (KGE) module to preserve semantics while utilizing structure. Then,
the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to
understand KGs embeddings. We conduct extensive experiments and provide a
detailed analysis to explore how incorporating the structural information of
KGs can enhance the factual reasoning abilities of LLMs. Our code are available
at https://github.com/yfangZhang/SSKG-LLM.

</details>


### [374] [Bridging Fairness and Explainability: Can Input-Based Explanations Promote Fairness in Hate Speech Detection?](https://arxiv.org/abs/2509.22291)
*Yifan Wang,Mayank Jobanputra,Ji-Ung Lee,Soyoung Oh,Isabel Valera,Vera Demberg*

Main category: cs.CL

TL;DR: 本文对仇恨言论检测中可解释性与公平性的关系进行系统研究，发现基于输入的解释能检测偏见预测和辅助减少训练偏差，但不适用于选择公平模型。


<details>
  <summary>Details</summary>
Motivation: NLP模型存在复制或放大社会偏见问题，黑盒特性使检测和缓解偏见困难，且现有公平NLP可解释性研究多为定性，缺乏大规模定量分析。

Method: 对仇恨言论检测中编码器和解码器模型，从识别偏见预测、选择公平模型、缓解训练偏差三个维度，系统研究可解释性和公平性的关系。

Result: 基于输入的解释能有效检测偏见预测，在训练中可作为减少偏差的监督信息，但在候选模型中选择公平模型不可靠。

Conclusion: 基于输入的解释在检测和缓解偏见方面有作用，但选择公平模型效果不佳。

Abstract: Natural language processing (NLP) models often replicate or amplify social
bias from training data, raising concerns about fairness. At the same time,
their black-box nature makes it difficult for users to recognize biased
predictions and for developers to effectively mitigate them. While some studies
suggest that input-based explanations can help detect and mitigate bias, others
question their reliability in ensuring fairness. Existing research on
explainability in fair NLP has been predominantly qualitative, with limited
large-scale quantitative analysis. In this work, we conduct the first
systematic study of the relationship between explainability and fairness in
hate speech detection, focusing on both encoder- and decoder-only models. We
examine three key dimensions: (1) identifying biased predictions, (2) selecting
fair models, and (3) mitigating bias during model training. Our findings show
that input-based explanations can effectively detect biased predictions and
serve as useful supervision for reducing bias during training, but they are
unreliable for selecting fair models among candidates.

</details>


### [375] [Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs](https://arxiv.org/abs/2509.22338)
*Felix Vossel,Till Mossakowski,Björn Gehrke*

Main category: cs.CL

TL;DR: 本文对微调大语言模型用于自然语言到一阶逻辑（FOL）翻译任务进行了系统评估，比较不同架构和训练策略，发现谓词可用性可提升性能，T5模型表现佳等。


<details>
  <summary>Details</summary>
Motivation: 自然语言到一阶逻辑的自动翻译对知识表示和形式方法至关重要但具挑战性，需评估微调大语言模型在该任务上的表现。

Method: 使用MALLS和Willow数据集，探索词汇扩展、谓词条件、多语言训练等技术，引入精确匹配、逻辑等价和谓词对齐等指标。

Result: 微调的Flan - T5 - XXL在有谓词列表时准确率达70%，优于GPT - 4o、DeepSeek - R1 - 0528及ccg2lambda等。谓词可用性提升性能15 - 20%，T5模型表现优于大的仅解码器大语言模型，模型能泛化到未见逻辑参数。

Conclusion: 结构逻辑翻译较稳健，谓词提取是主要瓶颈。

Abstract: Automating the translation of natural language to first-order logic (FOL) is
crucial for knowledge representation and formal methods, yet remains
challenging. We present a systematic evaluation of fine-tuned LLMs for this
task, comparing architectures (encoder-decoder vs. decoder-only) and training
strategies. Using the MALLS and Willow datasets, we explore techniques like
vocabulary extension, predicate conditioning, and multilingual training,
introducing metrics for exact match, logical equivalence, and predicate
alignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate
lists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT
reasoning ability as well as symbolic systems like ccg2lambda. Key findings
show: (1) predicate availability boosts performance by 15-20%, (2) T5 models
surpass larger decoder-only LLMs, and (3) models generalize to unseen logical
arguments (FOLIO dataset) without specific training. While structural logic
translation proves robust, predicate extraction emerges as the main bottleneck.

</details>


### [376] [Transformers Can Learn Connectivity in Some Graphs but Not Others](https://arxiv.org/abs/2509.22343)
*Amit Roy,Abulhair Saparov*

Main category: cs.CL

TL;DR: 研究变压器模型学习传递关系推理能力，发现其在网格图上表现及受图维度、模型规模影响，非网格图学习有挑战。


<details>
  <summary>Details</summary>
Motivation: 探究变压器模型从训练示例中推理传递关系的能力及规模对该能力的影响。

Method: 生成有向图训练不同规模变压器模型，评估其对不同图大小的传递关系推理能力。

Result: 变压器能在“类网格”有向图学习连通性，图维度是学习能力强预测因素，增加模型规模可提升泛化能力，非网格图学习困难。

Conclusion: 变压器模型在特定网格图能学习传递关系推理，受图维度和模型规模影响，非网格图学习面临挑战。

Abstract: Reasoning capability is essential to ensure the factual correctness of the
responses of transformer-based Large Language Models (LLMs), and robust
reasoning about transitive relations is instrumental in many settings, such as
causal inference. Hence, it is essential to investigate the capability of
transformers in the task of inferring transitive relations (e.g., knowing A
causes B and B causes C, then A causes C). The task of inferring transitive
relations is equivalent to the task of connectivity in directed graphs (e.g.,
knowing there is a path from A to B, and there is a path from B to C, then
there is a path from A to C). Past research focused on whether transformers can
learn to infer transitivity from in-context examples provided in the input
prompt. However, transformers' capability to infer transitive relations from
training examples and how scaling affects the ability is unexplored. In this
study, we seek to answer this question by generating directed graphs to train
transformer models of varying sizes and evaluate their ability to infer
transitive relations for various graph sizes. Our findings suggest that
transformers are capable of learning connectivity on "grid-like'' directed
graphs where each node can be embedded in a low-dimensional subspace, and
connectivity is easily inferable from the embeddings of the nodes. We find that
the dimensionality of the underlying grid graph is a strong predictor of
transformers' ability to learn the connectivity task, where higher-dimensional
grid graphs pose a greater challenge than low-dimensional grid graphs. In
addition, we observe that increasing the model scale leads to increasingly
better generalization to infer connectivity over grid graphs. However, if the
graph is not a grid graph and contains many disconnected components,
transformers struggle to learn the connectivity task, especially when the
number of components is large.

</details>


### [377] [CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models](https://arxiv.org/abs/2509.22360)
*Niharika Hegde,Subarnaduti Paul,Lars Joel-Frey,Manuel Brack,Kristian Kersting,Martin Mundt,Patrick Schramowski*

Main category: cs.CL

TL;DR: 引入250年跨度英文书籍语料库CHRONOBERG，指出其对语言模型时间感知训练的重要性


<details>
  <summary>Details</summary>
Motivation: 现有语料库缺乏长期时间结构，限制语言模型对语言语义和规范演变的语境化能力

Method: 从Project Gutenberg整理文本，进行时间敏感的VAD分析构建情感词典

Result: 基于CHRONOBERG训练的语言模型难以编码语义的历时变化

Conclusion: 需要时间感知的训练和评估流程，CHRONOBERG是研究语言变化和时间泛化的可扩展资源

Abstract: Large language models (LLMs) excel at operating at scale by leveraging social
media and various data crawled from the web. Whereas existing corpora are
diverse, their frequent lack of long-term temporal structure may however limit
an LLM's ability to contextualize semantic and normative evolution of language
and to capture diachronic variation. To support analysis and training for the
latter, we introduce CHRONOBERG, a temporally structured corpus of English book
texts spanning 250 years, curated from Project Gutenberg and enriched with a
variety of temporal annotations. First, the edited nature of books enables us
to quantify lexical semantic change through time-sensitive
Valence-Arousal-Dominance (VAD) analysis and to construct historically
calibrated affective lexicons to support temporally grounded interpretation.
With the lexicons at hand, we demonstrate a need for modern LLM-based tools to
better situate their detection of discriminatory language and contextualization
of sentiment across various time-periods. In fact, we show how language models
trained sequentially on CHRONOBERG struggle to encode diachronic shifts in
meaning, emphasizing the need for temporally aware training and evaluation
pipelines, and positioning CHRONOBERG as a scalable resource for the study of
linguistic change and temporal generalization. Disclaimer: This paper includes
language and display of samples that could be offensive to readers. Open
Access: Chronoberg is available publicly on HuggingFace at (
https://huggingface.co/datasets/spaul25/Chronoberg). Code is available at
(https://github.com/paulsubarna/Chronoberg).

</details>


### [378] [What Is The Political Content in LLMs' Pre- and Post-Training Data?](https://arxiv.org/abs/2509.22367)
*Tanise Ceron,Dmitry Nikolaev,Dominik Stammbach,Debora Nozza*

Main category: cs.CL

TL;DR: 分析OLMO2模型训练前后语料库政治内容，发现左倾文档占优，训练数据立场与模型政治偏见强相关，强调未来数据处理需整合政治内容分析。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型政治偏见产生原因，分析训练数据政治内容。

Method: 从OLMO2训练前后语料库抽取随机样本，自动标注政治倾向，分析来源和内容，评估训练数据政治内容与模型立场相关性。

Result: 左倾文档在各数据集占优，预训练语料库政治参与内容更多，左右倾文档话题框架不同，训练数据立场与模型政治偏见强相关。

Conclusion: 未来数据管理流程需整合政治内容分析，过滤策略应深度记录以保证透明度。

Abstract: Large language models (LLMs) are known to generate politically biased text,
yet how such biases arise remains unclear. A crucial step toward answering this
question is the analysis of training data, whose political content remains
largely underexplored in current LLM research. To address this gap, we present
in this paper an analysis of the pre- and post-training corpora of OLMO2, the
largest fully open-source model released together with its complete dataset.
From these corpora, we draw large random samples, automatically annotate
documents for political orientation, and analyze their source domains and
content. We then assess how political content in the training data correlates
with models' stance on specific policy issues. Our analysis shows that
left-leaning documents predominate across datasets, with pre-training corpora
containing significantly more politically engaged content than post-training
data. We also find that left- and right-leaning documents frame similar topics
through distinct values and sources of legitimacy. Finally, the predominant
stance in the training data strongly correlates with models' political biases
when evaluated on policy issues. These findings underscore the need to
integrate political content analysis into future data curation pipelines as
well as in-depth documentation of filtering strategies for transparency.

</details>


### [379] [Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding](https://arxiv.org/abs/2509.22437)
*Ziheng Chi,Yifan Hou,Chenxi Pang,Shaobo Cui,Mubashara Akhtar,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 介绍Chimera测试套件评估VLMs对图表理解，发现现有VLMs表现多源于捷径行为，需更稳健评估协议。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs处理图表时可能依赖捷径，需评估其是否真正理解和推理图表。

Method: 引入含7500个高质量图表的Chimera测试套件，标注语义信息和多级问题，评估15个开源VLMs，测量三种捷径的存在。

Result: 现有VLMs看似强大的性能很大程度源于捷径行为，Clever - Hans捷径贡献显著。

Conclusion: 当前VLMs存在关键局限，需要更稳健评估协议来衡量对复杂视觉输入的真正理解。

Abstract: Diagrams convey symbolic information in a visual format rather than a linear
stream of words, making them especially challenging for AI models to process.
While recent evaluations suggest that vision-language models (VLMs) perform
well on diagram-related benchmarks, their reliance on knowledge, reasoning, or
modality shortcuts raises concerns about whether they genuinely understand and
reason over diagrams. To address this gap, we introduce Chimera, a
comprehensive test suite comprising 7,500 high-quality diagrams sourced from
Wikipedia; each diagram is annotated with its symbolic content represented by
semantic triples along with multi-level questions designed to assess four
fundamental aspects of diagram comprehension: entity recognition, relation
understanding, knowledge grounding, and visual reasoning. We use Chimera to
measure the presence of three types of shortcuts in visual question answering:
(1) the visual-memorization shortcut, where VLMs rely on memorized visual
patterns; (2) the knowledge-recall shortcut, where models leverage memorized
factual knowledge instead of interpreting the diagram; and (3) the Clever-Hans
shortcut, where models exploit superficial language patterns or priors without
true comprehension. We evaluate 15 open-source VLMs from 7 model families on
Chimera and find that their seemingly strong performance largely stems from
shortcut behaviors: visual-memorization shortcuts have slight impact,
knowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts
contribute significantly. These findings expose critical limitations in current
VLMs and underscore the need for more robust evaluation protocols that
benchmark genuine comprehension of complex visual inputs (e.g., diagrams)
rather than question-answering shortcuts.

</details>


### [380] [Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning](https://arxiv.org/abs/2509.22472)
*Antreas Ioannou,Andreas Shiamishis,Nora Hollenstein,Nezihe Merve Gürel*

Main category: cs.CL

TL;DR: 本文评估LLaMA和Gemini在多语言法律和非法律基准测试上的表现，提出开源评估管道，发现法律任务对大语言模型挑战大，LLaMA弱于Gemini，部署于关键多语言法律应用仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型主导的时代，了解其在法律等高风险领域的能力和局限性，且现有模型在多语言、不同司法管辖区和对抗性环境中的表现探索不足。

Method: 在多语言法律和非法律基准测试上评估LLaMA和Gemini，通过字符和词级扰动评估其在法律任务中的对抗鲁棒性，采用LLM-as-a-Judge方法进行人类对齐评估，提出开源模块化评估管道。

Result: 法律任务对大语言模型挑战大，准确率常低于50%；英语结果更稳定但不总带来更高准确率；各语言存在提示敏感性和对抗脆弱性；语言表现与和英语句法相似性相关；LLaMA弱于Gemini，平均相差约24个百分点。

Conclusion: 尽管新的大语言模型有改进，但可靠部署于关键多语言法律应用仍存在挑战。

Abstract: In an era dominated by Large Language Models (LLMs), understanding their
capabilities and limitations, especially in high-stakes fields like law, is
crucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini,
DeepSeek, and other emerging models are increasingly integrated into legal
workflows, their performance in multilingual, jurisdictionally diverse, and
adversarial contexts remains insufficiently explored. This work evaluates LLaMA
and Gemini on multilingual legal and non-legal benchmarks, and assesses their
adversarial robustness in legal tasks through character and word-level
perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation.
We moreover present an open-source, modular evaluation pipeline designed to
support multilingual, task-diverse benchmarking of any combination of LLMs and
datasets, with a particular focus on legal tasks, including classification,
summarization, open questions, and general reasoning. Our findings confirm that
legal tasks pose significant challenges for LLMs with accuracies often below
50% on legal reasoning benchmarks such as LEXam, compared to over 70% on
general-purpose tasks like XNLI. In addition, while English generally yields
more stable results, it does not always lead to higher accuracy. Prompt
sensitivity and adversarial vulnerability is also shown to persist across
languages. Finally, a correlation is found between the performance of a
language and its syntactic similarity to English. We also observe that LLaMA is
weaker than Gemini, with the latter showing an average advantage of about 24
percentage points across the same task. Despite improvements in newer LLMs,
challenges remain in deploying them reliably for critical, multilingual legal
applications.

</details>


### [381] [Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving](https://arxiv.org/abs/2509.22480)
*Hang Li,Kaiqi Yang,Yucheng Chu,Hui Liu,Jiliang Tang*

Main category: cs.CL

TL;DR: 研究大语言模型解决单问题时解的发散性，提出解的发散性作为新指标，在三领域测试有效，是推进训练和评估的有效工具。


<details>
  <summary>Details</summary>
Motivation: 以往多通过有监督微调或强化学习提升大语言模型解决问题性能，本文从解的发散性新视角研究。

Method: 提出解的发散性作为新指标，支持有监督微调与强化学习策略，并在三个代表性问题领域测试。

Result: 使用解的发散性能持续提高成功率。

Conclusion: 解的发散性是推进大语言模型训练和评估的简单有效工具。

Abstract: Large language models (LLMs) have been widely used for problem-solving tasks.
Most recent work improves their performance through supervised fine-tuning
(SFT) with labeled data or reinforcement learning (RL) from task feedback. In
this paper, we study a new perspective: the divergence in solutions generated
by LLMs for a single problem. We show that higher solution divergence is
positively related to better problem-solving abilities across various models.
Based on this finding, we propose solution divergence as a novel metric that
can support both SFT and RL strategies. We test this idea on three
representative problem domains and find that using solution divergence
consistently improves success rates. These results suggest that solution
divergence is a simple but effective tool for advancing LLM training and
evaluation.

</details>


### [382] [InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models](https://arxiv.org/abs/2509.22536)
*Wenjun Wang,Shuo Cai,Congkai Xie,Mingfa Feng,Yiming Zhang,Zhen Li,Kejing Yang,Ming Li,Jiannong Cao,Yuan Xie,Hongxia Yang*

Main category: cs.CL

TL;DR: 本文提出端到端FP8训练方案，解决其缺乏开源训练方案问题，实验表明该方案稳定无损且效率高。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练计算成本高，FP8虽理论高效但缺乏综合开源训练方案。

Method: 引入端到端FP8训练方案，采用细粒度混合粒度量化策略。

Result: 方案稳定无损，推理性能与BF16相当，训练时间最多减少22%，峰值内存使用减少14%，吞吐量增加19%。

Conclusion: FP8是BF16实用且强大的替代方案，将发布代码推动大规模模型训练普及。

Abstract: The immense computational cost of training Large Language Models (LLMs)
presents a major barrier to innovation. While FP8 training offers a promising
solution with significant theoretical efficiency gains, its widespread adoption
has been hindered by the lack of a comprehensive, open-source training recipe.
To bridge this gap, we introduce an end-to-end FP8 training recipe that
seamlessly integrates continual pre-training and supervised fine-tuning. Our
methodology employs a fine-grained, hybrid-granularity quantization strategy to
maintain numerical fidelity while maximizing computational efficiency. Through
extensive experiments, including the continue pre-training of models on a
160B-token corpus, we demonstrate that our recipe is not only remarkably stable
but also essentially lossless, achieving performance on par with the BF16
baseline across a suite of reasoning benchmarks. Crucially, this is achieved
with substantial efficiency improvements, including up to a 22% reduction in
training time, a 14% decrease in peak memory usage, and a 19% increase in
throughput. Our results establish FP8 as a practical and robust alternative to
BF16, and we will release the accompanying code to further democratize
large-scale model training.

</details>


### [383] [StateX: Enhancing RNN Recall via Post-training State Expansion](https://arxiv.org/abs/2509.22630)
*Xingyu Shen,Yingfa Chen,Zhen Leng Thai,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: Transformer模型处理长文本成本高，RNNs虽有低复杂度但长文本信息召回能力弱，本文提出StateX方法扩展预训练RNN状态，实验证明有效且成本低。


<details>
  <summary>Details</summary>
Motivation: Transformer处理长文本成本高，RNN长文本信息召回能力弱，直接训练大状态RNN成本高，需高效扩展预训练RNN状态的方法。

Method: 提出StateX训练管道，针对线性注意力和状态空间模型两类RNN设计后训练架构修改，在不显著增加参数情况下扩大状态大小。

Result: 在最大13亿参数模型上实验，StateX有效增强RNN召回和上下文学习能力，后训练成本低且不损害其他能力。

Conclusion: StateX是一种高效扩展预训练RNN状态的方法，能提升其长文本处理能力。

Abstract: While Transformer-based models have demonstrated remarkable language modeling
performance, their high complexities result in high costs when processing long
contexts. In contrast, recurrent neural networks (RNNs) such as linear
attention and state space models have gained popularity due to their constant
per-token complexities. However, these recurrent models struggle with tasks
that require accurate recall of contextual information from long contexts,
because all contextual information is compressed into a constant-size recurrent
state. Previous works have shown that recall ability is positively correlated
with the recurrent state size, yet directly training RNNs with larger recurrent
states results in high training costs. In this paper, we introduce StateX, a
training pipeline for efficiently expanding the states of pre-trained RNNs
through post-training. For two popular classes of RNNs, linear attention and
state space models, we design post-training architectural modifications to
scale up the state size with no or negligible increase in model parameters.
Experiments on models up to 1.3B parameters demonstrate that StateX efficiently
enhances the recall and in-context learning ability of RNNs without incurring
high post-training costs or compromising other capabilities.

</details>


### [384] [Variational Reasoning for Language Models](https://arxiv.org/abs/2509.22637)
*Xiangxin Zhou,Zichen Liu,Haonan Wang,Chao Du,Min Lin,Chongxuan Li,Liang Wang,Tianyu Pang*

Main category: cs.CL

TL;DR: 提出变分推理框架，将思维轨迹视为隐变量，通过变分推理优化，统一变分推理与类强化学习方法，提升语言模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 为语言模型提供一种提升推理能力的方法，统一变分推理与类强化学习方法。

Method: 从证据下限（ELBO）扩展到多轨迹目标，提出前向KL公式稳定变分后验训练，揭示拒绝采样微调与二元奖励强化学习可解释为局部前向KL目标。

Result: 在Qwen 2.5和Qwen 3模型族的广泛推理任务上验证了方法有效性。

Conclusion: 提供了统一变分推理与类强化学习方法的原则性概率视角，得到稳定目标以提升语言模型推理能力。

Abstract: We introduce a variational reasoning framework for language models that
treats thinking traces as latent variables and optimizes them through
variational inference. Starting from the evidence lower bound (ELBO), we extend
it to a multi-trace objective for tighter bounds and propose a forward-KL
formulation that stabilizes the training of the variational posterior. We
further show that rejection sampling finetuning and binary-reward RL, including
GRPO, can be interpreted as local forward-KL objectives, where an implicit
weighting by model accuracy naturally arises from the derivation and reveals a
previously unnoticed bias toward easier questions. We empirically validate our
method on the Qwen 2.5 and Qwen 3 model families across a wide range of
reasoning tasks. Overall, our work provides a principled probabilistic
perspective that unifies variational inference with RL-style methods and yields
stable objectives for improving the reasoning ability of language models. Our
code is available at https://github.com/sail-sg/variational-reasoning.

</details>


### [385] [Language Models Can Learn from Verbal Feedback Without Scalar Rewards](https://arxiv.org/abs/2509.22638)
*Renjie Luo,Zichen Liu,Xiangyan Liu,Chao Du,Min Lin,Wenhu Chen,Wei Lu,Tianyu Pang*

Main category: cs.CL

TL;DR: 提出反馈条件策略（FCP）让大语言模型直接从语言反馈中学习，将反馈驱动学习重构为条件生成。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型从人类或AI反馈中强化学习的方法会将细致反馈压缩为标量奖励，丢弃大量信息并导致规模失衡。

Method: 引入反馈条件策略（FCP），从响应 - 反馈对中直接学习，通过对离线数据进行最大似然训练来近似反馈条件后验，并开发在线自举阶段优化策略。

Result: 将反馈驱动学习重构为条件生成，提供了更具表现力的方式让大语言模型直接从语言反馈中学习，代码开源。

Conclusion: FCP是一种有效的让大语言模型直接从语言反馈中学习的方法。

Abstract: LLMs are often trained with RL from human or AI feedback, yet such methods
typically compress nuanced feedback into scalar rewards, discarding much of
their richness and inducing scale imbalance. We propose treating verbal
feedback as a conditioning signal. Inspired by language priors in text-to-image
generation, which enable novel outputs from unseen prompts, we introduce the
feedback-conditional policy (FCP). FCP learns directly from response-feedback
pairs, approximating the feedback-conditional posterior through maximum
likelihood training on offline data. We further develop an online bootstrapping
stage where the policy generates under positive conditions and receives fresh
feedback to refine itself. This reframes feedback-driven learning as
conditional generation rather than reward optimization, offering a more
expressive way for LLMs to directly learn from verbal feedback. Our code is
available at https://github.com/sail-sg/feedback-conditional-policy.

</details>


### [386] [Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity](https://arxiv.org/abs/2509.22641)
*Arkadiy Saakyan,Najoung Kim,Smaranda Muresan,Tuhin Chakrabarty*

Main category: cs.CL

TL;DR: 研究发现n - gram新颖性与专家评判的创造力正相关，但仅靠它评估创造力不足，开源大模型n - gram新颖性与实用性负相关，前沿闭源模型创造力不如人类，前沿大模型识别能力有提升空间。


<details>
  <summary>Details</summary>
Motivation: n - gram新颖性用于评估语言模型文本生成能力和文本创造力，但未考虑创造力的双重属性，需研究其与创造力的关系。

Method: 通过26位专家对7542个人类和AI生成文本的新颖性、实用性和合理性进行细读标注，开展探索性研究，测试零样本、少样本和微调模型识别能力。

Result: n - gram新颖性与专家评判的创造力正相关，但约91%高n - gram新颖性表达不具创造性；开源大模型n - gram新颖性越高实用性越低；前沿闭源模型创造力不如人类；前沿大模型表现优于随机但有提升空间；最佳模型的新颖性得分能预测专家偏好。

Conclusion: 不能仅依靠n - gram新颖性评估创造力，前沿大模型在识别创造性和非实用性表达方面有待提高。

Abstract: N-gram novelty is widely used to evaluate language models' ability to
generate text outside of their training data. More recently, it has also been
adopted as a metric for measuring textual creativity. However, theoretical work
on creativity suggests that this approach may be inadequate, as it does not
account for creativity's dual nature: novelty (how original the text is) and
appropriateness (how sensical and pragmatic it is). We investigate the
relationship between this notion of creativity and n-gram novelty through 7542
expert writer annotations (n=26) of novelty, pragmaticality, and sensicality
via close reading of human and AI-generated text. We find that while n-gram
novelty is positively associated with expert writer-judged creativity, ~91% of
top-quartile expressions by n-gram novelty are not judged as creative,
cautioning against relying on n-gram novelty alone. Furthermore, unlike
human-written text, higher n-gram novelty in open-source LLMs correlates with
lower pragmaticality. In an exploratory study with frontier close-source
models, we additionally confirm that they are less likely to produce creative
expressions than humans. Using our dataset, we test whether zero-shot,
few-shot, and finetuned models are able to identify creative expressions (a
positive aspect of writing) and non-pragmatic ones (a negative aspect).
Overall, frontier LLMs exhibit performance much higher than random but leave
room for improvement, especially struggling to identify non-pragmatic
expressions. We further find that LLM-as-a-Judge novelty scores from the
best-performing model were predictive of expert writer preferences.

</details>


### [387] [WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning](https://arxiv.org/abs/2509.22644)
*Zimu Lu,Houxing Ren,Yunqiao Yang,Ke Wang,Zhuofan Zong,Junting Pan,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 提出WebGen - Agent网站生成代理，利用多级别视觉反馈迭代生成和优化网站代码库，引入Step - GRPO训练方法提升模型网站生成能力，在WebGen - Bench数据集上效果超现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有代码代理在网站代码库生成任务中仅依赖简单代码执行反馈和验证，无法捕捉生成代码实际质量。

Method: 提出WebGen - Agent，利用视觉语言模型生成网站截图和GUI - agent测试的文本描述、建议及质量分数，结合回溯和选优机制；引入Step - GRPO with Screenshot and GUI - agent Feedback，以截图和GUI - agent分数作为奖励提供过程监督信号。

Result: 在WebGen - Bench数据集上，WebGen - Agent提升了Claude - 3.5 - Sonnet的准确率和外观分数；Step - GRPO训练方法提升了Qwen2.5 - Coder - 7B - Instruct的准确率和外观分数。

Conclusion: WebGen - Agent和Step - GRPO训练方法有效提升了模型的网站生成能力，表现优于先前的先进代理系统。

Abstract: Agent systems powered by large language models (LLMs) have demonstrated
impressive performance on repository-level code-generation tasks. However, for
tasks such as website codebase generation, which depend heavily on visual
effects and user-interaction feedback, current code agents rely only on simple
code execution for feedback and verification. This approach fails to capture
the actual quality of the generated code. In this paper, we propose
WebGen-Agent, a novel website-generation agent that leverages comprehensive and
multi-level visual feedback to iteratively generate and refine the website
codebase. Detailed and expressive text descriptions and suggestions regarding
the screenshots and GUI-agent testing of the websites are generated by a visual
language model (VLM), together with scores that quantify their quality. The
screenshot and GUI-agent scores are further integrated with a backtracking and
select-best mechanism, enhancing the performance of the agent. Utilizing the
accurate visual scores inherent in the WebGen-Agent workflow, we further
introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve
the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using
the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we
provide a dense and reliable process supervision signal, which effectively
improves the model's website-generation ability. On the WebGen-Bench dataset,
WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%
and its appearance score from 3.0 to 3.9, outperforming the previous
state-of-the-art agent system. Additionally, our Step-GRPO training approach
increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and
raises the appearance score from 3.4 to 3.7.

</details>


### [388] [VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing](https://arxiv.org/abs/2509.22651)
*Ke Wang,Houxing Ren,Zimu Lu,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 现有基准不足以评估语音优先AI助手能力，本文引入VoiceAssistant - Eval基准，评估21个开源模型和GPT - 4o - Audio，揭示模型表现差异，指出尚存挑战并提供框架。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法全面评估语音优先AI助手能力，需要新的基准。

Method: 引入VoiceAssistant - Eval基准，包含10497个示例，涵盖13个任务类别，评估模型响应内容、语音质量和一致性。

Result: （1）专有模型并非全面优于开源模型；（2）多数模型擅长说话任务，音频理解不足；（3）设计良好的小模型可与大模型媲美，如Step - Audio - 2 - mini（7B）听力准确率超LLaMA - Omni2 - 32B - Bilingual两倍。

Conclusion: VoiceAssistant - Eval识别出当前模型在多模态输入和角色语音模仿等方面的差距，为下一代AI助手开发提供评估框架。

Abstract: The growing capabilities of large language models and multimodal systems have
spurred interest in voice-first AI assistants, yet existing benchmarks are
inadequate for evaluating the full range of these systems' capabilities. We
introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI
assistants across listening, speaking, and viewing. VoiceAssistant-Eval
comprises 10,497 curated examples spanning 13 task categories. These tasks
include natural sounds, music, and spoken dialogue for listening; multi-turn
dialogue, role-play imitation, and various scenarios for speaking; and highly
heterogeneous images for viewing. To demonstrate its utility, we evaluate 21
open-source models and GPT-4o-Audio, measuring the quality of the response
content and speech, as well as their consistency. The results reveal three key
findings: (1) proprietary models do not universally outperform open-source
models; (2) most models excel at speaking tasks but lag in audio understanding;
and (3) well-designed smaller models can rival much larger ones. Notably, the
mid-sized Step-Audio-2-mini (7B) achieves more than double the listening
accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal
(audio plus visual) input and role-play voice imitation tasks are difficult for
current models, and significant gaps persist in robustness and safety
alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous
framework for evaluating and guiding the development of next-generation AI
assistants. Code and data will be released at
https://mathllm.github.io/VoiceAssistantEval/ .

</details>


### [389] [Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference](https://arxiv.org/abs/2509.21791)
*Han Yuan,Yue Zhao,Li Zhang,Wuqiong Luo,Zheng Ma*

Main category: cs.CL

TL;DR: 文章用因果推理对大语言模型结构化输出对生成质量的影响进行细化分析，发现多数场景下无因果影响。


<details>
  <summary>Details</summary>
Motivation: 以往研究对大语言模型结构化输出对生成质量影响的评估有局限，如测试场景受限、对比设置控制弱、依赖粗略指标等，需细化分析。

Method: 采用因果推理，基于一个假设和两个保证约束，推导五种潜在因果结构。

Result: 粗略指标显示结构化输出对GPT - 4o生成有正、负或中性影响，因果推理表明48个场景中43个无因果影响，其余5个中有3个受具体指令影响存在多方面因果结构。

Conclusion: 大语言模型结构化输出在多数情况下对生成质量无因果影响。

Abstract: Structured output from large language models (LLMs) has enhanced efficiency
in processing generated information and is increasingly adopted in industrial
applications. Prior studies have investigated the impact of structured output
on LLMs' generation quality, often presenting one-way findings. Some suggest
that structured format enhances completeness and factual accuracy, while others
argue that it restricts the reasoning capacity of LLMs and leads to reductions
in standard evaluation metrics. Potential limitations of these assessments
include restricted testing scenarios, weakly controlled comparative settings,
and reliance on coarse metrics. In this work, we present a refined analysis
using causal inference. Based on one assumed and two guaranteed constraints, we
derive five potential causal structures characterizing the influence of
structured output on LLMs' generation: (1) collider without m-bias, (2)
collider with m-bias, (3) single cause from instruction, (4) single cause from
output format, and (5) independence. Across seven public and one developed
reasoning tasks, we find that coarse metrics report positive, negative, or
neutral effects of structured output on GPT-4o's generation. However, causal
inference reveals no causal impact in 43 out of 48 scenarios. In the remaining
5, 3 involve multifaceted causal structures influenced by concrete
instructions.

</details>


### [390] [Representing LLMs in Prompt Semantic Task Space](https://arxiv.org/abs/2509.22506)
*Idan Kashani,Avi Mendelson,Yaniv Nemcovsky*

Main category: cs.CL

TL;DR: 本文提出无训练方法将大语言模型表示为提示语义任务空间中的线性算子，在预测和模型选择任务取得好结果。


<details>
  <summary>Details</summary>
Motivation: 现有识别特定任务中最佳大语言模型的方法扩展性有限、需昂贵再训练且表示空间难解释。

Method: 将大语言模型表示为提示语义任务空间中的线性算子，利用几何属性的闭式计算。

Result: 在成功预测和模型选择任务中取得有竞争力或最先进的结果，在样本外场景表现出色。

Conclusion: 所提出的方法高效、无需训练，具有高扩展性和实时适应性，能为模型应用提供高可解释性表示。

Abstract: Large language models (LLMs) achieve impressive results over various tasks,
and ever-expanding public repositories contain an abundance of pre-trained
models. Therefore, identifying the best-performing LLM for a given task is a
significant challenge. Previous works have suggested learning LLM
representations to address this. However, these approaches present limited
scalability and require costly retraining to encompass additional models and
datasets. Moreover, the produced representation utilizes distinct spaces that
cannot be easily interpreted. This work presents an efficient, training-free
approach to representing LLMs as linear operators within the prompts' semantic
task space, thus providing a highly interpretable representation of the models'
application. Our method utilizes closed-form computation of geometrical
properties and ensures exceptional scalability and real-time adaptability to
dynamically expanding repositories. We demonstrate our approach on success
prediction and model selection tasks, achieving competitive or state-of-the-art
results with notable performance in out-of-sample scenarios.

</details>


### [391] [From Formal Language Theory to Statistical Learning: Finite Observability of Subregular Languages](https://arxiv.org/abs/2509.22598)
*Katsuhiko Hayashi,Hidetaka Kamigaito*

Main category: cs.CL

TL;DR: 证明标准亚正则语言类可线性分离，实验验证其特性，表明亚正则层级为自然语言建模提供基础


<details>
  <summary>Details</summary>
Motivation: 为自然语言结构建模寻找严谨且可解释的基础

Method: 理论证明标准亚正则语言类的线性可分性，进行合成实验和真实数据实验

Result: 合成实验在无噪声条件下证实完美可分性，真实数据实验显示学习特征符合语言约束

Conclusion: 亚正则层级为自然语言结构建模提供严谨且可解释的基础

Abstract: We prove that all standard subregular language classes are linearly separable
when represented by their deciding predicates. This establishes finite
observability and guarantees learnability with simple linear models. Synthetic
experiments confirm perfect separability under noise-free conditions, while
real-data experiments on English morphology show that learned features align
with well-known linguistic constraints. These results demonstrate that the
subregular hierarchy provides a rigorous and interpretable foundation for
modeling natural language structure. Our code used in real-data experiments is
available at https://github.com/UTokyo-HayashiLab/subregular.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [392] [Error Analysis of Discrete Flow with Generator Matching](https://arxiv.org/abs/2509.21906)
*Zhengyan Wan,Yidong Ouyang,Qiang Yao,Liyan Xie,Fang Fang,Hongyuan Zha,Guang Cheng*

Main category: math.ST

TL;DR: 本文提出统一框架研究离散流理论性质，给出分布估计非渐近误差界，是离散流模型首次误差分析。


<details>
  <summary>Details</summary>
Motivation: 离散流模型收敛性和误差分析未被充分研究，需要系统探究其理论性质。

Method: 基于随机微积分理论建立统一框架，推导关于两个连续时间马尔可夫链路径测度的KL散度，结合生成器匹配和均匀化方法。

Result: 得到分布估计的非渐近误差界，分析了转移率估计和提前停止产生的误差。

Conclusion: 本文为离散流模型提供了首次误差分析。

Abstract: Discrete flow models offer a powerful framework for learning distributions
over discrete state spaces and have demonstrated superior performance compared
to the discrete diffusion model. However, their convergence properties and
error analysis remain largely unexplored. In this work, we develop a unified
framework grounded in stochastic calculus theory to systematically investigate
the theoretical properties of discrete flow. Specifically, we derive the KL
divergence of two path measures regarding two continuous-time Markov chains
(CTMCs) with different transition rates by developing a novel Girsanov-type
theorem, and provide a comprehensive analysis that encompasses the error
arising from transition rate estimation and early stopping, where the first
type of error has rarely been analyzed by existing works. Unlike discrete
diffusion models, discrete flow incurs no truncation error caused by truncating
the time horizon in the noising process. Building on generator matching and
uniformization, we establish non-asymptotic error bounds for distribution
estimation. Our results provide the first error analysis for discrete flow
models.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [393] [Chronic Stress, Immune Suppression, and Cancer Occurrence: Unveiling the Connection using Survey Data and Predictive Models](https://arxiv.org/abs/2509.22275)
*Teddy Lazebnik,Vered Aharonson*

Main category: stat.AP

TL;DR: 运用机器学习和因果建模探索慢性压力与癌症发生的因果关系，发现压力与癌症发病率有显著因果关联，强调压力是可改变的癌症风险因素。


<details>
  <summary>Details</summary>
Motivation: 慢性压力与癌症发生的直接因果关系未明确，需探索两者复杂因果互动。

Method: 开发预测模型，利用压力指标、癌症史和人口统计数据等变量，并用传统统计方法验证。

Result: 压力频率、水平和对健康的感知影响与癌症发病率有显著因果关联；单独压力预测力有限，结合社会人口和家族癌症史数据可提高模型准确性。

Conclusion: 强调癌症风险的多维性，压力是可改变的癌症风险因素，应纳入个性化预防策略和公共卫生干预以降低癌症发病率。

Abstract: Chronic stress was implicated in cancer occurrence, but a direct causal
connection has not been consistently established. Machine learning and causal
modeling offer opportunities to explore complex causal interactions between
psychological chronic stress and cancer occurrences. We developed predictive
models employing variables from stress indicators, cancer history, and
demographic data from self-reported surveys, unveiling the direct and immune
suppression mitigated connection between chronic stress and cancer occurrence.
The models were corroborated by traditional statistical methods. Our findings
indicated significant causal correlations between stress frequency, stress
level and perceived health impact, and cancer incidence. Although stress alone
showed limited predictive power, integrating socio-demographic and familial
cancer history data significantly enhanced model accuracy. These results
highlight the multidimensional nature of cancer risk, with stress emerging as a
notable factor alongside genetic predisposition. These findings strengthen the
case for addressing chronic stress as a modifiable cancer risk factor,
supporting its integration into personalized prevention strategies and public
health interventions to reduce cancer incidence.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [394] [Beyond Structure: Invariant Crystal Property Prediction with Pseudo-Particle Ray Diffraction](https://arxiv.org/abs/2509.21778)
*Bin Cao,Yang Liu,Longhan Zhang,Yifan Wu,Zhixun Li,Yuyu Luo,Hong Cheng,Yang Ren,Tong-Yi Zhang*

Main category: cond-mat.mtrl-sci

TL;DR: 传统方法计算晶体属性成本高，现有机器学习模型有局限，本文提出PRDNet模型，实验证明其达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统密度泛函理论计算大体系晶体属性计算量大，现有机器学习模型因原子表示问题性能受限。

Method: 引入PRDNet，结合独特的倒易空间衍射和图表示，用数据驱动的伪粒子生成合成衍射图案，确保对晶体学对称性的完全不变性。

Result: 在Materials Project、JARVIS - DFT和MatBench上进行大量实验，模型达到了当前最优性能。

Conclusion: PRDNet能有效解决现有模型问题，提升晶体属性预测的准确性。

Abstract: Crystal property prediction, governed by quantum mechanical principles, is
computationally prohibitive to solve exactly for large many-body systems using
traditional density functional theory. While machine learning models have
emerged as efficient approximations for large-scale applications, their
performance is strongly influenced by the choice of atomic representation.
Although modern graph-based approaches have progressively incorporated more
structural information, they often fail to capture long-term atomic
interactions due to finite receptive fields and local encoding schemes. This
limitation leads to distinct crystals being mapped to identical
representations, hindering accurate property prediction. To address this, we
introduce PRDNet that leverages unique reciprocal-space diffraction besides
graph representations. To enhance sensitivity to elemental and environmental
variations, we employ a data-driven pseudo-particle to generate a synthetic
diffraction pattern. PRDNet ensures full invariance to crystallographic
symmetries. Extensive experiments are conducted on Materials Project,
JARVIS-DFT, and MatBench, demonstrating that the proposed model achieves
state-of-the-art performance.

</details>


### [395] [Interpretable Spectral Features Predict Conductivity in Self-Driving Doped Conjugated Polymer Labs](https://arxiv.org/abs/2509.21330)
*Ankush Kumar Mishra,Jacob P. Mauthe,Nicholas Luke,Aram Amassian,Baskar Ganapathysubramanian*

Main category: cond-mat.mtrl-sci

TL;DR: 本文通过学习可解释的光谱指纹预测掺杂共轭聚合物的电导率，结合数据驱动和专家特征构建混合QSPR模型，减少实验工作量，且特征可扩展到其他光谱模式。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶实验室中从廉价、可自动化的读数预测昂贵、测量缓慢的材料属性的挑战，以实现更快的材料发现。

Method: 结合遗传算法和曲线下面积计算对光学光谱进行特征化，加入基于领域知识的特征扩展和SHAP引导选择，构建定量结构 - 属性关系模型。

Result: 数据驱动模型性能与专家描述符基线相当，减少约33%实验工作量；混合QSPR模型具有更优的预测性能；学习到的特征能恢复已知描述符并揭示新的相关区域。

Conclusion: 该方法提供了可解释、抗噪声、适用于小数据的特征，能将快速测量转化为对昂贵属性的可靠预测，并可扩展到其他光谱模式。

Abstract: Self-driving labs (SDLs) promise faster materials discovery by coupling
automation with machine learning, but a central challenge is predicting costly,
slow-to-measure properties from inexpensive, automatable readouts. We address
this for doped conjugated polymers by learning interpretable spectral
fingerprints from optical spectroscopy to predict electrical conductivity.
Optical spectra are fast, non-destructive, and sensitive to aggregation and
charge generation; we automate their featurization by combining a genetic
algorithm (GA) with area-under-the-curve (AUC) computations over adaptively
selected spectral windows. These data-driven spectral features, together with
processing parameters, are used to train a quantitative structure-property
relationship (QSPR) linking optical response and processing to conductivity. To
improve accuracy and interpretability in the small-data regime, we add
domain-knowledge-based feature expansions and apply SHAP-guided selection to
retain a compact, physically meaningful feature set. The pipeline is evaluated
under a leak-free train/test protocol, and GA is repeated to assess feature
stability. The data-driven model matches the performance of a baseline built
from expert-curated descriptors while reducing experimental effort (about 33%)
by limiting direct conductivity measurements. Combining data-driven and expert
features yields a hybrid QSPR with superior predictive performance,
highlighting productive human-ML collaboration. The learned features recover
known descriptors in pBTTT (0-0/0-1 vibronic intensity ratio) and reveal a
tail-state region correlated with polymer bleaching during successful doping.
This approach delivers interpretable, noise-robust, small-data-friendly
features that convert rapid measurements into reliable predictions of costly
properties and readily extends to other spectral modalities (e.g., XANES,
Raman, FTIR).

</details>


### [396] [Automated Machine Learning Pipeline for Training and Analysis Using Large Language Models](https://arxiv.org/abs/2509.21647)
*Adam Lahouari,Jutta Rogal,Mark E. Tuckerman*

Main category: cond-mat.mtrl-sci

TL;DR: 介绍自动化机器学习管道AMLP，统一从数据集创建到模型验证的工作流程，在吖啶多晶型上验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 开发可靠的机器学习原子间势（MLIPs）困难，需统一工作流程。

Method: 引入AMLP，用大语言模型代理辅助电子结构代码选择等，AMLP - Analysis支持分子模拟，基于MACE架构。

Result: 在吖啶多晶型上微调基础模型，能量平均绝对误差约1.7 meV/原子，力约7.0 meV/Å，拟合的MLIP能亚埃精度重现DFT几何结构，在分子动力学模拟中稳定。

Conclusion: AMLP能有效统一MLIPs开发工作流程，实现可靠的MLIPs开发。

Abstract: Machine learning interatomic potentials (MLIPs) have become powerful tools to
extend molecular simulations beyond the limits of quantum methods, offering
near-quantum accuracy at much lower computational cost. Yet, developing
reliable MLIPs remains difficult because it requires generating high-quality
datasets, preprocessing atomic structures, and carefully training and
validating models. In this work, we introduce an Automated Machine Learning
Pipeline (AMLP) that unifies the entire workflow from dataset creation to model
validation. AMLP employs large-language-model agents to assist with
electronic-structure code selection, input preparation, and output conversion,
while its analysis suite (AMLP-Analysis), based on ASE supports a range of
molecular simulations. The pipeline is built on the MACE architecture and
validated on acridine polymorphs, where, with a straightforward fine-tuning of
a foundation model, mean absolute errors of ~1.7 meV/atom in energies and ~7.0
meV/{\AA} in forces are achieved. The fitted MLIP reproduces DFT geometries
with sub-{\AA} accuracy and demonstrates stability during molecular dynamics
simulations in the microcanonical and canonical ensembles.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [397] [Discovering alternative solutions beyond the simplicity bias in recurrent neural networks](https://arxiv.org/abs/2509.21504)
*William Qian,Cengiz Pehlevan*

Main category: q-bio.NC

TL;DR: 本文提出 Iterative Neural Similarity Deflation (INSD) 方法打破 RNN 的简单性归纳偏置，获得不同解决方案，有时性能更优，强调突破简单偏置的重要性。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明任务训练的 RNN 有强简单性偏置，导致在同一任务上训练的 RNN 解决方案趋同，不利于生成独特神经计算假设。

Method: 提出 Iterative Neural Similarity Deflation (INSD) 方法，通过惩罚标准任务训练 RNN 产生的神经活动的线性可预测性，寻找经典神经科学风格 RNN 任务的替代解决方案。

Result: 找到的替代解决方案在多种分析技术下表现不同，且在困难或分布外任务中有时性能更优。

Conclusion: 强调突破简单性偏置以揭示更丰富多样的神经计算模型的重要性。

Abstract: Training recurrent neural networks (RNNs) to perform neuroscience-style tasks
has become a popular way to generate hypotheses for how neural circuits in the
brain might perform computations. Recent work has demonstrated that
task-trained RNNs possess a strong simplicity bias. In particular, this
inductive bias often causes RNNs trained on the same task to collapse on
effectively the same solution, typically comprised of fixed-point attractors or
other low-dimensional dynamical motifs. While such solutions are readily
interpretable, this collapse proves counterproductive for the sake of
generating a set of genuinely unique hypotheses for how neural computations
might be performed. Here we propose Iterative Neural Similarity Deflation
(INSD), a simple method to break this inductive bias. By penalizing linear
predictivity of neural activity produced by standard task-trained RNNs, we find
an alternative class of solutions to classic neuroscience-style RNN tasks.
These solutions appear distinct across a battery of analysis techniques,
including representational similarity metrics, dynamical systems analysis, and
the linear decodability of task-relevant variables. Moreover, these alternative
solutions can sometimes achieve superior performance in difficult or
out-of-distribution task regimes. Our findings underscore the importance of
moving beyond the simplicity bias to uncover richer and more varied models of
neural computation.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [398] [VizGen: Data Exploration and Visualization from Natural Language via a Multi-Agent AI Architecture](https://arxiv.org/abs/2509.22218)
*Sandaru Fernando,Imasha Jayarathne,Sithumini Abeysekara,Shanuja Sithamparanthan,Thushari Silva,Deshan Jayawardana*

Main category: cs.MA

TL;DR: VizGen是一个AI辅助的图形生成系统，借自然语言生成数据可视化，降低技术门槛，实现实时交互与对话式图形优化，使数据分析更易上手。


<details>
  <summary>Details</summary>
Motivation: 传统数据可视化工具需技术专长，限制了可访问性，开发VizGen以解决此问题。

Method: 利用先进NLP和LLMs（如Claude 3.7 Sonnet和Gemini 2.0 Flash）将用户查询转换为SQL，推荐合适图形类型；采用多智能体架构处理各环节；分析数据模式、异常和相关性；结合互联网信息提供解释。

Result: 系统支持与SQL数据库实时交互，可进行对话式图形优化。

Conclusion: VizGen弥合了技术复杂性和用户友好设计间的差距，实现了数据可视化的民主化。

Abstract: Data visualization is essential for interpreting complex datasets, yet
traditional tools often require technical expertise, limiting accessibility.
VizGen is an AI-assisted graph generation system that empowers users to create
meaningful visualizations using natural language. Leveraging advanced NLP and
LLMs like Claude 3.7 Sonnet and Gemini 2.0 Flash, it translates user queries
into SQL and recommends suitable graph types. Built on a multi-agent
architecture, VizGen handles SQL generation, graph creation, customization, and
insight extraction. Beyond visualization, it analyzes data for patterns,
anomalies, and correlations, and enhances user understanding by providing
explanations enriched with contextual information gathered from the internet.
The system supports real-time interaction with SQL databases and allows
conversational graph refinement, making data analysis intuitive and accessible.
VizGen democratizes data visualization by bridging the gap between technical
complexity and user-friendly design.

</details>


### [399] [Multi-Agent Path Finding via Offline RL and LLM Collaboration](https://arxiv.org/abs/2509.22130)
*Merve Atasever,Matthew Hong,Mihir Nitin Kulkarni,Qingpei Li,Jyotirmoy V. Deshmukh*

Main category: cs.MA

TL;DR: 本文提出基于决策转换器的高效分散式规划框架解决多智能体路径规划问题，减少训练时间，结合大语言模型增强适应性和性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体路径规划问题复杂，分散式强化学习方法存在智能体自利行为、碰撞频繁和训练时间长等问题。

Method: 提出基于决策转换器的高效分散式规划框架，利用离线强化学习，并集成大语言模型GPT - 4o动态引导智能体策略。

Result: 在静态和动态环境的大量实验表明，基于决策转换器的方法结合GPT - 4o显著增强了适应性和性能。

Conclusion: 所提出的方法能有效解决多智能体路径规划问题，减少训练时间，提升适应性和性能。

Abstract: Multi-Agent Path Finding (MAPF) poses a significant and challenging problem
critical for applications in robotics and logistics, particularly due to its
combinatorial complexity and the partial observability inherent in realistic
environments. Decentralized reinforcement learning methods commonly encounter
two substantial difficulties: first, they often yield self-centered behaviors
among agents, resulting in frequent collisions, and second, their reliance on
complex communication modules leads to prolonged training times, sometimes
spanning weeks. To address these challenges, we propose an efficient
decentralized planning framework based on the Decision Transformer (DT),
uniquely leveraging offline reinforcement learning to substantially reduce
training durations from weeks to mere hours. Crucially, our approach
effectively handles long-horizon credit assignment and significantly improves
performance in scenarios with sparse and delayed rewards. Furthermore, to
overcome adaptability limitations inherent in standard RL methods under dynamic
environmental changes, we integrate a large language model (GPT-4o) to
dynamically guide agent policies. Extensive experiments in both static and
dynamically changing environments demonstrate that our DT-based approach,
augmented briefly by GPT-4o, significantly enhances adaptability and
performance.

</details>


### [400] [Impact of Collective Behaviors of Autonomous Vehicles on Urban Traffic Dynamics: A Multi-Agent Reinforcement Learning Approach](https://arxiv.org/abs/2509.22216)
*Ahmet Onur Akman,Anastasia Psarou,Zoltán György Varga,Grzegorz Jamróz,Rafał Kucharski*

Main category: cs.MA

TL;DR: 研究强化学习自动驾驶车辆对城市混合交通流影响，模拟显示其最多优化5%出行时间，不同行为对人类司机影响不同。


<details>
  <summary>Details</summary>
Motivation: 研究强化学习自动驾驶车辆在混合交通环境下对城市交通流的潜在影响。

Method: 将三分之一人类司机换成采用深度Q学习算法的自动驾驶车辆，定义多种行为，通过奖励施加行为，用自研框架PARCOUR进行模拟。

Result: 自动驾驶车辆最多优化5%出行时间，不同行为对人类司机出行时间影响不同，自利行为时出行时间比人类司机短。

Conclusion: 多智能体强化学习适用于交通网络集体路由，其对共存方的影响随行为不同而有很大差异，各目标行为学习任务复杂度有差异。

Abstract: This study examines the potential impact of reinforcement learning
(RL)-enabled autonomous vehicles (AV) on urban traffic flow in a mixed traffic
environment. We focus on a simplified day-to-day route choice problem in a
multi-agent setting. We consider a city network where human drivers travel
through their chosen routes to reach their destinations in minimum travel time.
Then, we convert one-third of the population into AVs, which are RL agents
employing Deep Q-learning algorithm. We define a set of optimization targets,
or as we call them behaviors, namely selfish, collaborative, competitive,
social, altruistic, and malicious. We impose a selected behavior on AVs through
their rewards. We run our simulations using our in-house developed RL framework
PARCOUR. Our simulations reveal that AVs optimize their travel times by up to
5\%, with varying impacts on human drivers' travel times depending on the AV
behavior. In all cases where AVs adopt a self-serving behavior, they achieve
shorter travel times than human drivers. Our findings highlight the complexity
differences in learning tasks of each target behavior. We demonstrate that the
multi-agent RL setting is applicable for collective routing on traffic
networks, though their impact on coexisting parties greatly varies with the
behaviors adopted.

</details>


### [401] [Effective Policy Learning for Multi-Agent Online Coordination Beyond Submodular Objectives](https://arxiv.org/abs/2509.22596)
*Qixin Zhang,Yan Sun,Can Jin,Xikun Zhang,Yao Shu,Puning Zhao,Li Shen,Dacheng Tao*

Main category: cs.MA

TL;DR: 提出两种多智能体在线协调问题的策略学习算法MA - SPL和MA - MPL，核心是基于策略的连续扩展技术，仿真验证了算法有效性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体在线协调问题，减少对未知参数的依赖。

Method: 提出MA - SPL和MA - MPL算法，核心采用基于策略的连续扩展技术。

Result: MA - SPL能对具有子模目标的MA - OC问题实现近似保证，还能处理弱子模场景；MA - MPL无参数且保持相同近似比；仿真验证了算法有效性。

Conclusion: 提出的两种算法有效解决多智能体在线协调问题。

Abstract: In this paper, we present two effective policy learning algorithms for
multi-agent online coordination(MA-OC) problem. The first one, \texttt{MA-SPL},
not only can achieve the optimal $(1-\frac{c}{e})$-approximation guarantee for
the MA-OC problem with submodular objectives but also can handle the unexplored
$\alpha$-weakly DR-submodular and $(\gamma,\beta)$-weakly submodular scenarios,
where $c$ is the curvature of the investigated submodular functions, $\alpha$
denotes the diminishing-return(DR) ratio and the tuple $(\gamma,\beta)$
represents the submodularity ratios. Subsequently, in order to reduce the
reliance on the unknown parameters $\alpha,\gamma,\beta$ inherent in the
\texttt{MA-SPL} algorithm, we further introduce the second online algorithm
named \texttt{MA-MPL}. This \texttt{MA-MPL} algorithm is entirely
\emph{parameter-free} and simultaneously can maintain the same approximation
ratio as the first \texttt{MA-SPL} algorithm. The core of our \texttt{MA-SPL}
and \texttt{MA-MPL} algorithms is a novel continuous-relaxation technique
termed as \emph{policy-based continuous extension}. Compared with the
well-established \emph{multi-linear extension}, a notable advantage of this new
\emph{policy-based continuous extension} is its ability to provide a lossless
rounding scheme for any set function, thereby enabling us to tackle the
challenging weakly submodular objectives. Finally, extensive simulations are
conducted to validate the effectiveness of our proposed algorithms.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [402] [Bacterial Gene Regulatory Neural Network as a Biocomputing Library of Mathematical Solvers](https://arxiv.org/abs/2509.21598)
*Adrian Ratwatte,Samitha Somathilaka,Thanh Cao,Xu Li,Sasitharan Balasubramaniam*

Main category: cs.ET

TL;DR: 利用GRNN框架将细菌基因表达动态转化为生物计算库，执行多种数学任务，且计算稳定可靠。


<details>
  <summary>Details</summary>
Motivation: 当前生物计算方法依赖固定逻辑电路，在不同环境下稳定性和可靠性有限。

Method: 使用GRNN框架将细菌基因表达动态转化为生物计算库，引入子GRNN搜索算法识别功能子网络，用基因和集体扰动及基于Lyapunov的稳定性分析评估。

Result: 证明可利用天然转录机制执行多种数学计算和分类任务，同时保持计算稳定性和可靠性。

Conclusion: 天然转录机制可用于生物计算，且能保证计算的稳定性和可靠性。

Abstract: Current biocomputing approaches predominantly rely on engineered circuits
with fixed logic, offering limited stability and reliability under diverse
environmental conditions. Here, we use the GRNN framework introduced in our
previous work to transform bacterial gene expression dynamics into a
biocomputing library of mathematical solvers. We introduce a sub-GRNN search
algorithm that identifies functional subnetworks tailored to specific
mathematical calculation and classification tasks by evaluating gene expression
patterns across chemically encoded input conditions. Tasks include identifying
Fibonacci numbers, prime numbers, multiplication, and Collatz step counts. The
identified problem-specific sub-GRNNs are then assessed using gene-wise and
collective perturbation, as well as Lyapunov-based stability analysis, to
evaluate robustness and reliability. Our results demonstrate that native
transcriptional machinery can be harnessed to perform diverse mathematical
calculation and classification tasks, while maintaining computing stability and
reliability.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [403] [PhenoMoler: Phenotype-Guided Molecular Optimization via Chemistry Large Language Model](https://arxiv.org/abs/2509.21424)
*Ran Song,Hui Liu*

Main category: physics.chem-ph

TL;DR: 提出表型导向分子生成框架PhenoMoler，实验表明其可生成与期望表型一致的分子，有优化潜力。


<details>
  <summary>Details</summary>
Motivation: 现有分子生成模型常忽略化合物系统级表型效应，转录谱可指导表型感知的分子设计。

Method: 将化学大语言模型与表达谱集成，通过对药物诱导的差异表达特征进行条件生成，选择性掩蔽和重建特定子结构。

Result: 生成化学有效、新颖、多样且与期望表型一致的分子，生成化合物药物相似性、理化性质和结合亲和力表现良好。

Conclusion: PhenoMoler有表型导向和结构可控的分子优化潜力。

Abstract: Current molecular generative models primarily focus on improving drug-target
binding affinity and specificity, often neglecting the system-level phenotypic
effects elicited by compounds. Transcriptional profiles, as molecule-level
readouts of drug-induced phenotypic shifts, offer a powerful opportunity to
guide molecular design in a phenotype-aware manner. We present PhenoMoler, a
phenotype-guided molecular generation framework that integrates a chemistry
large language model with expression profiles to enable biologically informed
drug design. By conditioning the generation on drug-induced differential
expression signatures, PhenoMoler explicitly links transcriptional responses to
chemical structure. By selectively masking and reconstructing specific
substructures-scaffolds, side chains, or linkers-PhenoMoler supports
fine-grained, controllable molecular optimization. Extensive experiments
demonstrate that PhenoMoler generates chemically valid, novel, and diverse
molecules aligned with desired phenotypic profiles. Compared to FDA-approved
drugs, the generated compounds exhibit comparable or enhanced drug-likeness
(QED), optimized physicochemical properties, and superior binding affinity to
key cancer targets. These findings highlight PhenoMoler's potential for
phenotype-guided and structure-controllable molecular optimization.

</details>


### [404] [Data-driven approach to the design of complexing agents for trivalent transuranium elements](https://arxiv.org/abs/2509.21362)
*Kirill V. Karpov,Ivan S. Pikulin,Grigory V. Bokov,Artem A. Mitrofanov*

Main category: physics.chem-ph

TL;DR: 本文用机器学习方法创建新神经网络架构研究超铀元素配合物性质，描述模型适用域并确定影响配合物稳定性的分子片段。


<details>
  <summary>Details</summary>
Motivation: 超铀元素配合物实验研究因元素稀有、成本高、实验条件特殊而复杂，量子化学计算不适用于大体系。

Method: 运用现代机器学习方法创建新的神经网络架构。

Result: 利用可用的部分元素实验数据显著提高了所得模型的质量。

Conclusion: 描述了模型的适用域，确定了对配合物稳定性影响最大的分子片段。

Abstract: The properties of complexes with transuranium elements have long been the
object of research in various fields of chemistry. However, their experimental
study is complicated by their rarity, high cost and special conditions
necessary for working with such elements, and the complexity of quantum
chemical calculations does not allow their use for large systems. To overcome
these problems, we used modern machine learning methods to create a novel
neural network architecture that allows to use available experimental data on a
number of elements and thus significantly improve the quality of the resulting
models. We also described the applicability domain of the presented model and
identified the molecular fragments that most influence the stability of the
complexes.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [405] [Can Large Language Models Autoformalize Kinematics?](https://arxiv.org/abs/2509.21840)
*Aditi Kabra,Jonathan Laurent,Sagar Bharadwaj,Ruben Martins,Stefan Mitsch,André Platzer*

Main category: cs.LO

TL;DR: 本文实验研究大语言模型（LLMs）能否自动化网络物理系统的形式化过程，设计20个问题基准套件，取得70%成功率并分析失败案例。


<details>
  <summary>Details</summary>
Motivation: 网络物理系统使用形式化方法进行控制决策需先建立形式化物理模型，传统方法需人类专业知识且成瓶颈，因此研究LLMs能否自动化该过程。

Method: 设计20个本科物理运动学问题的基准套件，让LLM根据自然语言描述生成微分博弈逻辑（dGL）模型，进行语法检查和语义评估。

Result: 取得70%（5个样本中最佳）的成功率。

Conclusion: 为基于LLM的从自然语言到具有连续动态的混合博弈逻辑的自动形式化提供了首个定量基线，分析了失败案例并指出未来改进方向。

Abstract: Autonomous cyber-physical systems like robots and self-driving cars could
greatly benefit from using formal methods to reason reliably about their
control decisions. However, before a problem can be solved it needs to be
stated. This requires writing a formal physics model of the cyber-physical
system, which is a complex task that traditionally requires human expertise and
becomes a bottleneck.
  This paper experimentally studies whether Large Language Models (LLMs) can
automate the formalization process. A 20 problem benchmark suite is designed
drawing from undergraduate level physics kinematics problems. In each problem,
the LLM is provided with a natural language description of the objects' motion
and must produce a model in differential game logic (dGL). The model is (1)
syntax checked and iteratively refined based on parser feedback, and (2)
semantically evaluated by checking whether symbolically executing the dGL
formula recovers the solution to the original physics problem. A success rate
of 70% (best over 5 samples) is achieved. We analyze failing cases, identifying
directions for future improvement. This provides a first quantitative baseline
for LLM-based autoformalization from natural language to a hybrid games logic
with continuous dynamics.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [406] [InvBench: Can LLMs Accelerate Program Verification with Invariant Synthesis?](https://arxiv.org/abs/2509.21629)
*Anjiang Wei,Tarun Suresh,Tianran Sun,Haoze Wu,Ke Wang,Alex Aiken*

Main category: cs.PL

TL;DR: 提出评估大语言模型（LLMs）不变式合成的框架，评估7个LLMs及基于LLM的验证器，发现其相比传统求解器无显著优势，还展示了微调及采样可提升性能。


<details>
  <summary>Details</summary>
Motivation: 自动发现强循环不变式是程序验证的长期挑战，需评估LLMs在不变式合成方面的能力。

Method: 采用基于验证器的决策程序，有形式化的可靠性保证，评估正确性和验证加速情况。

Result: LLM-based验证器有前景但相比UAutomizer无显著优势，不同模型加速差异大，微调及Best-of-N采样可提升性能。

Conclusion: 当前LLMs在该基准测试仍是挑战，微调及采样可改善LLMs在不变式合成中的表现。

Abstract: Program verification relies on loop invariants, yet automatically discovering
strong invariants remains a long-standing challenge. We introduce a principled
framework for evaluating LLMs on invariant synthesis. Our approach uses a
verifier-based decision procedure with a formal soundness guarantee and
assesses not only correctness but also the speedup that invariants provide in
verification. We evaluate 7 state-of-the-art LLMs, and existing LLM-based
verifiers against the traditional solver UAutomizer. While LLM-based verifiers
represent a promising direction, they do not yet offer a significant advantage
over UAutomizer. Model capability also proves critical, as shown by sharp
differences in speedups across models, and our benchmark remains an open
challenge for current LLMs. Finally, we show that supervised fine-tuning and
Best-of-N sampling can improve performance: fine-tuning on 3589 instances
raises the percentage of speedup cases for Qwen3-Coder-480B from 8% to 29.2%,
and Best-of-N sampling with N=16 improves Claude-sonnet-4 from 8.8% to 22.1%.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [407] [Seismic Velocity Inversion from Multi-Source Shot Gathers Using Deep Segmentation Networks: Benchmarking U-Net Variants and SeismoLabV3+](https://arxiv.org/abs/2509.21331)
*Mahedi Hasan*

Main category: physics.geo-ph

TL;DR: 本文对比多种编码器 - 解码器架构用于地震速度反演，SeismoLabV3+表现最佳，证明深度分割网络用于地震速度反演的适用性和架构改进的价值。


<details>
  <summary>Details</summary>
Motivation: 传统物理驱动的地震速度反演方法计算量大、对初始化敏感且受数据带宽限制，因此研究数据驱动方法。

Method: 使用ThinkOnward 2025数据集，对比U - Net、U - Net++、DeepLabV3+和优化变体SeismoLabV3+四种编码器 - 解码器架构进行地震速度反演。

Result: SeismoLabV3+性能最佳，内部验证集MAPE值0.03025，隐藏测试集0.031246。

Conclusion: 深度分割网络适用于地震速度反演，特定架构改进对推进地球物理AI模型有价值。

Abstract: Seismic velocity inversion is a key task in geophysical exploration, enabling
the reconstruction of subsurface structures from seismic wave data. It is
critical for high-resolution seismic imaging and interpretation. Traditional
physics-driven methods, such as Full Waveform Inversion (FWI), are
computationally demanding, sensitive to initialization, and limited by the
bandwidth of seismic data. Recent advances in deep learning have led to
data-driven approaches that treat velocity inversion as a dense prediction
task. This research benchmarks three advanced encoder-decoder architectures --
U-Net, U-Net++, and DeepLabV3+ -- together with SeismoLabV3+, an optimized
variant of DeepLabV3+ with a ResNeXt50 32x4d backbone and task-specific
modifications -- for seismic velocity inversion using the ThinkOnward 2025
Speed \& Structure dataset, which consists of five-channel seismic shot gathers
paired with high-resolution velocity maps. Experimental results show that
SeismoLabV3+ achieves the best performance, with MAPE values of 0.03025 on the
internal validation split and 0.031246 on the hidden test set as scored via the
official ThinkOnward leaderboard. These findings demonstrate the suitability of
deep segmentation networks for seismic velocity inversion and underscore the
value of tailored architectural refinements in advancing geophysical AI models.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [408] [Forecasting the Future with Yesterday's Climate: Temperature Bias in AI Weather and Climate Models](https://arxiv.org/abs/2509.22359)
*Jacob B. Landsberg,Elizabeth A. Barnes*

Main category: physics.ao-ph

TL;DR: 研究分析AI气候与天气模型仅用历史数据训练预测未来气候的问题，发现三个模型均有冷偏差。


<details>
  <summary>Details</summary>
Motivation: 解决AI模型仅用历史数据训练来预测未来气候的挑战。

Method: 分析两个天气模型FourCastNet和Pangu对2020 - 2025年、ACE2对1996 - 2010年的预测，评估其泛化能力。

Result: 三个模型均产生冷偏差，FourCastNet和Pangu在最热预测温度时冷偏差最强，ACE2的偏差分布更均匀。

Conclusion: 强调仅用历史数据训练AI模型的挑战，应用于未来气候预测时需考虑偏差。

Abstract: AI-based climate and weather models have rapidly gained popularity, providing
faster forecasts with skill that can match or even surpass that of traditional
dynamical models. Despite this success, these models face a key challenge:
predicting future climates while being trained only with historical data. In
this study, we investigate this issue by analyzing boreal winter land
temperature biases in AI weather and climate models. We examine two weather
models, FourCastNet V2 Small (FourCastNet) and Pangu Weather (Pangu),
evaluating their predictions for 2020-2025 and Ai2 Climate Emulator version 2
(ACE2) for 1996-2010. These time periods lie outside of the respective models'
training sets and are significantly more recent than the bulk of their training
data, allowing us to assess how well the models generalize to new, i.e. more
modern, conditions. We find that all three models produce cold-biased mean
temperatures, resembling climates from 15-20 years earlier than the period they
are predicting. In some regions, like the Eastern U.S., the predictions
resemble climates from as much as 20-30 years earlier. Further analysis shows
that FourCastNet's and Pangu's cold bias is strongest in the hottest predicted
temperatures, indicating limited training exposure to modern extreme heat
events. In contrast, ACE2's bias is more evenly distributed but largest in
regions, seasons, and parts of the temperature distribution where climate
change has been most pronounced. These findings underscore the challenge of
training AI models exclusively on historical data and highlight the need to
account for such biases when applying them to future climate prediction.

</details>


### [409] [Accurate typhoon intensity forecasts using a non-iterative spatiotemporal transformer model](https://arxiv.org/abs/2509.21349)
*Hongyu Qu,Hongxiong Xu,Lin Dong,Chunyi Xiang,Gaozhen Nie*

Main category: physics.ao-ph

TL;DR: 介绍基于transformer的热带气旋强度预测模型TIFNet，在各方面表现优于现有模型，尤其在极端情况下。


<details>
  <summary>Details</summary>
Motivation: 热带气旋强度准确预测是气象学挑战，现有机器学习系统在极端情况表现不佳，缺乏长期一致性。

Method: 引入TIFNet模型，整合高分辨率全球预报和历史演变融合机制，用再分析数据训练，用业务数据微调。

Result: TIFNet在各预报时段均优于业务数值模型，在弱、强和超强台风类别中均有改进，在快速强度变化情况下，相对当前业务基线降低29 - 43%的预报误差。

Conclusion: TIFNet在基于人工智能的热带气旋强度预报方面取得重大进展，尤其在传统模型表现不佳的极端条件下。

Abstract: Accurate forecasting of tropical cyclone (TC) intensity - particularly during
periods of rapid intensification and rapid weakening - remains a challenge for
operational meteorology, with high-stakes implications for disaster
preparedness and infrastructure resilience. Recent advances in machine learning
have yielded notable progress in TC prediction; however, most existing systems
provide forecasts that degrade rapidly in extreme regimes and lack long-range
consistency. Here we introduce TIFNet, a transformer-based forecasting model
that generates non-iterative, 5-day intensity trajectories by integrating
high-resolution global forecasts with a historical-evolution fusion mechanism.
Trained on reanalysis data and fine-tuned with operational data, TIFNet
consistently outperforms operational numerical models across all forecast
horizons, delivering robust improvements across weak, strong, and super typhoon
categories. In rapid intensity change regimes - long regarded as the most
difficult to forecast - TIFNet reduces forecast error by 29-43% relative to
current operational baselines. These results represent a substantial advance in
artificial-intelligence-based TC intensity forecasting, especially under
extreme conditions where traditional models consistently underperform.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [410] [NeuroScalar: A Deep Learning Framework for Fast, Accurate, and In-the-Wild Cycle-Level Performance Prediction](https://arxiv.org/abs/2509.22410)
*Shayne Wadle,Yanxin Zhang,Vikas Singh,Karthikeyan Sankaralingam*

Main category: cs.AR

TL;DR: 提出用于生产硬件的深度学习框架，可评估未来硬件性能，系统模拟速度快且开销小，芯片加速器性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有微处理器设计评估受慢且依赖非代表性基准跟踪的模拟器限制。

Method: 引入基于微架构无关特征训练的深度学习模型，提出含轻量级硬件跟踪收集器和采样策略的完整系统，设计Neutrino片上加速器。

Result: 系统在普通GPU上模拟速度达5 MIPS，性能开销仅0.1%；Neutrino片上加速器比GPU性能提升85倍。

Conclusion: 该框架可对实际应用进行准确性能分析和大规模硬件A/B测试。

Abstract: The evaluation of new microprocessor designs is constrained by slow,
cycle-accurate simulators that rely on unrepresentative benchmark traces. This
paper introduces a novel deep learning framework for high-fidelity,
``in-the-wild'' simulation on production hardware. Our core contribution is a
DL model trained on microarchitecture-independent features to predict
cycle-level performance for hypothetical processor designs. This unique
approach allows the model to be deployed on existing silicon to evaluate future
hardware. We propose a complete system featuring a lightweight hardware trace
collector and a principled sampling strategy to minimize user impact. This
system achieves a simulation speed of 5 MIPS on a commodity GPU, imposing a
mere 0.1% performance overhead. Furthermore, our co-designed Neutrino on-chip
accelerator improves performance by 85x over the GPU. We demonstrate that this
framework enables accurate performance analysis and large-scale hardware A/B
testing on a massive scale using real-world applications.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [411] [Toward a Physics of Deep Learning and Brains](https://arxiv.org/abs/2509.22649)
*Arsham Ghavasieh,Meritxell Vila-Minana,Akanksha Khurd,John Beggs,Gerardo Ortiz,Santo Fortunato*

Main category: cond-mat.dis-nn

TL;DR: 研究表明描述生物大脑神经元雪崩的方程可用于深度神经网络，网络在准临界状态学习佳，最大敏感性更能预测学习效果，还识别出不同普适类，揭示生物与人工神经网络有通用特征。


<details>
  <summary>Details</summary>
Motivation: 探寻能否找到统一理论框架解释深度神经网络和大脑的学习机制。

Method: 将描述生物大脑神经元雪崩的非平衡统计物理方程应用于深度神经网络，通过不同初始化训练网络，使用有限尺寸缩放法。

Result: 深度神经网络在准临界状态学习最佳，最大敏感性比接近临界点更能可靠预测学习，识别出包括巴克豪森噪声和定向渗流的不同普适类。

Conclusion: 生物和人工神经网络存在通用特征，此理论框架为改进网络性能提供蓝图。

Abstract: Deep neural networks and brains both learn and share superficial
similarities: processing nodes are likened to neurons and adjustable weights
are likened to modifiable synapses. But can a unified theoretical framework be
found to underlie them both? Here we show that the equations used to describe
neuronal avalanches in living brains can also be applied to cascades of
activity in deep neural networks. These equations are derived from
non-equilibrium statistical physics and show that deep neural networks learn
best when poised between absorbing and active phases. Because these networks
are strongly driven by inputs, however, they do not operate at a true critical
point but within a quasi-critical regime -- one that still approximately
satisfies crackling noise scaling relations. By training networks with
different initializations, we show that maximal susceptibility is a more
reliable predictor of learning than proximity to the critical point itself.
This provides a blueprint for engineering improved network performance.
Finally, using finite-size scaling we identify distinct universality classes,
including Barkhausen noise and directed percolation. This theoretical framework
demonstrates that universal features are shared by both biological and
artificial neural networks.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [412] [Impact IRR: Leveraging Modern Portfolio Theory to Define Impact Investments](https://arxiv.org/abs/2509.22600)
*Daniel Soliman*

Main category: q-fin.GN

TL;DR: 冲击投资市场规模约1.6万亿美元，确定财务回报有进展，但确定影响回报尚处早期，作者提出用影响内部收益率评估和监测冲击投资。


<details>
  <summary>Details</summary>
Motivation: 投资者在确定冲击投资的影响回报方面处于早期阶段，需要有效方法评估和监测冲击投资。

Method: 利用现代投资组合理论的组成部分、调整后的金融工具和现有数据集，提出影响内部收益率（impact IRR）方法。

Result: 通过初始用例和示例展示了该方法可用于优化影响。

Conclusion: 影响内部收益率可用于评估和监测冲击投资以优化影响。

Abstract: The impact investment market has an estimated value of almost $1.6 trillion.
Significant progress has been made in determining the financial returns of
impact investing. Investors are still, however, in the early stages of
determining impact return. In this study, the author proposes the use of impact
internal rate of return (impact IRR) to evaluate and monitor impact
investments. This approach, which utilizes components of modern portfolio
theory, adapted financial tools, and existing datasets, is demonstrated herein
through initial use cases and examples showing how it can be employed to
optimize impact.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [413] [High-dimensional quantum Schur transforms](https://arxiv.org/abs/2509.22640)
*Adam Burchardt,Jiani Fei,Dmitry Grinko,Martin Larocca,Maris Ozols,Sydney Timmerman,Vladyslav Visnevskyi*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The quantum Schur transform has become a foundational quantum algorithm, yet
even after two decades since the seminal 2005 paper by Bacon, Chuang, and
Harrow (BCH), some aspects of the transform remain insufficiently understood.
Moreover, an alternative approach proposed by Krovi in 2018 was recently found
to contain a crucial error. In this paper, we present a corrected version of
Krovi's algorithm along with a detailed treatment of the high-dimensional
version of the BCH Schur transform. This high-dimensional focus makes the two
versions of the transform practical for regimes where the number of qudits $n$
is smaller than the local dimension $d$, with Krovi's algorithm scaling as
$\widetilde{O}(n^4)$ and BCH as $\widetilde{O}(\min(n^5,nd^4))$. Our work
addresses a key gap in the literature, strengthening the algorithmic
foundations of a wide range of results that rely on Schur--Weyl duality in
quantum information theory and quantum computation.

</details>


### [414] [Optimizing the non-Clifford-count in unitary synthesis using Reinforcement Learning](https://arxiv.org/abs/2509.21709)
*David Kremer,Ali Javadi-Abhari,Priyanka Mukhopadhyay*

Main category: quant-ph

TL;DR: 本文研究用强化学习合成量子电路，优化T计数和CS计数，设计框架并采用启发式剪枝等方法，相比以往工作能在更短时间实现更大酉算子，取得更好结果。


<details>
  <summary>Details</summary>
Motivation: 为了实际实现量子算法相对经典算法的计算优势，需要高效实现酉算子，而现有算法复杂度高，因此研究用强化学习合成量子电路。

Method: 设计强化学习框架处理酉算子的通道表示，仅用整数进行矩阵运算，还结合剪枝启发式方法和算子规范化以降低搜索复杂度。

Result: 相比以往工作，能在更短时间实现更大酉算子，成功率和改进因子更好；两比特Clifford+T合成中达到近最优分解，是以往强化学习算法的5倍；能恢复已知的单比特酉算子T计数最优分解的线性复杂度算法；两比特Clifford+CS酉算子算法达到线性复杂度。

Conclusion: 强化学习用于量子电路合成是有效的，能在酉算子合成方面取得比以往方法更好的效果。

Abstract: An efficient implementation of unitary operators is important in order to
practically realize the computational advantages claimed by quantum algorithms
over their classical counterparts. In this paper we study the potential of
using reinforcement learning (RL) in order to synthesize quantum circuits,
while optimizing the T-count and CS-count, of unitaries that are exactly
implementable by the Clifford+T and Clifford+CS gate sets, respectively. In
general, the complexity of existing algorithms depend exponentially on the
number of qubits and the non-Clifford-count of unitaries. We have designed our
RL framework to work with channel representation of unitaries, that enables us
to perform matrix operations efficiently, using integers only. We have also
incorporated pruning heuristics and a canonicalization of operators, in order
to reduce the search complexity. As a result, compared to previous works, we
are able to implement significantly larger unitaries, in less time, with much
better success rate and improvement factor. Our results for Clifford+T
synthesis on two qubits achieve close-to-optimal decompositions for up to 100 T
gates, 5 times more than previous RL algorithms and to the best of our
knowledge, the largest instances achieved with any method to date. Our RL
algorithm is able to recover previously-known optimal linear complexity
algorithm for T-count-optimal decomposition of 1 qubit unitaries. For 2-qubit
Clifford+CS unitaries, our algorithm achieves a linear complexity, something
that could only be accomplished by a previous algorithm using $SO(6)$
representation.

</details>


### [415] [ConQuER: Modular Architectures for Control and Bias Mitigation in IQP Quantum Generative Models](https://arxiv.org/abs/2509.22551)
*Xiaocheng Zou,Shijin Duan,Charles Fleming,Gaowen Liu,Ramana Rao Kompella,Shaolei Ren,Xiaolin Xu*

Main category: quant-ph

TL;DR: 提出可控量子生成框架ConQuER，解决现有IQP电路量子生成模型的可控性和生成偏差问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于IQP电路的量子生成模型存在输出可控性不足和生成偏差严重的问题。

Method: 提出ConQuER框架，采用模块化电路架构，嵌入轻量级控制器电路，结合数据驱动优化嵌入隐式控制路径。

Result: 在多个量子态数据集上实验验证，ConQuER控制精度高、生成性能均衡，开销低。

Conclusion: ConQuER框架弥合了量子计算优势与可控生成建模实际需求之间的差距。

Abstract: Quantum generative models based on instantaneous quantum polynomial (IQP)
circuits show great promise in learning complex distributions while maintaining
classical trainability. However, current implementations suffer from two key
limitations: lack of controllability over generated outputs and severe
generation bias towards certain expected patterns. We present a Controllable
Quantum Generative Framework, ConQuER, which addresses both challenges through
a modular circuit architecture. ConQuER embeds a lightweight controller circuit
that can be directly combined with pre-trained IQP circuits to precisely
control the output distribution without full retraining. Leveraging the
advantages of IQP, our scheme enables precise control over properties such as
the Hamming Weight distribution with minimal parameter and gate overhead. In
addition, inspired by the controller design, we extend this modular approach
through data-driven optimization to embed implicit control paths in the
underlying IQP architecture, significantly reducing generation bias on
structured datasets. ConQuER retains efficient classical training properties
and high scalability. We experimentally validate ConQuER on multiple quantum
state datasets, demonstrating its superior control accuracy and balanced
generation performance, only with very low overhead cost over original IQP
circuits. Our framework bridges the gap between the advantages of quantum
computing and the practical needs of controllable generation modeling.

</details>


### [416] [Multi-channel convolutional neural quantum embedding](https://arxiv.org/abs/2509.22355)
*Yujin Kim,Changjae Im,Taehyun Kim,Tak Hur,Daniel K. Park*

Main category: quant-ph

TL;DR: 提出经典 - 量子混合方法优化量子嵌入，用数据集测试性能并提供理论分析。


<details>
  <summary>Details</summary>
Motivation: 量子监督学习（QSL）中量子嵌入选择影响其效能，标准量子计算电路模型有局限性。

Method: 引入经典 - 量子混合方法，用于优化通用多通道数据的量子嵌入。

Result: 用CIFAR - 10和Tiny ImageNet数据集对框架中各模型进行性能测试。

Conclusion: 提供了指导模型设计和优化的理论分析。

Abstract: Classification using variational quantum circuits is a promising frontier in
quantum machine learning. Quantum supervised learning (QSL) applied to
classical data using variational quantum circuits involves embedding the data
into a quantum Hilbert space and optimizing the circuit parameters to train the
measurement process. In this context, the efficacy of QSL is inherently
influenced by the selection of quantum embedding. In this study, we introduce a
classical-quantum hybrid approach for optimizing quantum embedding beyond the
limitations of the standard circuit model of quantum computation (i.e.,
completely positive and trace-preserving maps) for general multi-channel data.
We benchmark the performance of various models in our framework using the
CIFAR-10 and Tiny ImageNet datasets and provide theoretical analyses that guide
model design and optimization.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [417] [Shortcut Flow Matching for Speech Enhancement: Step-Invariant flows via single stage training](https://arxiv.org/abs/2509.21522)
*Naisong Zhou,Saisamarth Rajesh Phaye,Milos Cernak,Tijana Stojkovic,Andy Pearce,Andrea Cavallaro,Andy Harper*

Main category: cs.SD

TL;DR: 提出用于语音增强的捷径流匹配方法（SFMSE），单步推理实时因子低且感知质量佳，还分析随机性作用。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型迭代性质不利于实时应用，而流匹配更高效，因此提出SFMSE以解决语音增强实时性与质量的问题。

Method: 引入SFMSE，在单阶段训练中使速度场以目标时间步为条件，训练单一步长不变模型，可进行单步、少步或多步去噪。

Result: 单步SFMSE推理在消费级GPU上实时因子为0.013，感知质量与需60次神经函数评估的强扩散基线相当。

Conclusion: SFMSE方法缩小了高质量生成语音增强与低延迟约束之间的差距。

Abstract: Diffusion-based generative models have achieved state-of-the-art performance
for perceptual quality in speech enhancement (SE). However, their iterative
nature requires numerous Neural Function Evaluations (NFEs), posing a challenge
for real-time applications. On the contrary, flow matching offers a more
efficient alternative by learning a direct vector field, enabling high-quality
synthesis in just a few steps using deterministic ordinary differential
equation~(ODE) solvers. We thus introduce Shortcut Flow Matching for Speech
Enhancement (SFMSE), a novel approach that trains a single, step-invariant
model. By conditioning the velocity field on the target time step during a
one-stage training process, SFMSE can perform single, few, or multi-step
denoising without any architectural changes or fine-tuning. Our results
demonstrate that a single-step SFMSE inference achieves a real-time factor
(RTF) of 0.013 on a consumer GPU while delivering perceptual quality comparable
to a strong diffusion baseline requiring 60 NFEs. This work also provides an
empirical analysis of the role of stochasticity in training and inference,
bridging the gap between high-quality generative SE and low-latency
constraints.

</details>


### [418] [Guiding Audio Editing with Audio Language Model](https://arxiv.org/abs/2509.21625)
*Zitong Lan,Yiduo Hao,Mingmin Zhao*

Main category: cs.SD

TL;DR: 提出用于立体声编辑的SmartDJ框架，结合音频语言模型推理能力与潜在扩散生成能力，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成式音频编辑模型依赖模板指令格式、限于单声道，无法处理声明式音频编辑。

Method: 将高级指令分解为原子编辑操作，用扩散模型执行操作，设计数据合成管道生成配对示例。

Result: SmartDJ在感知质量、空间真实感和语义对齐方面优于先前音频编辑方法。

Conclusion: SmartDJ是一种有效的立体声音频编辑框架。

Abstract: Audio editing plays a central role in VR/AR immersion, virtual conferencing,
sound design, and other interactive media. However, recent generative audio
editing models depend on template-like instruction formats and are restricted
to mono-channel audio. These models fail to deal with declarative audio
editing, where the user declares what the desired outcome should be, while
leaving the details of editing operations to the system. We introduce SmartDJ,
a novel framework for stereo audio editing that combines the reasoning
capability of audio language models with the generative power of latent
diffusion. Given a high-level instruction, SmartDJ decomposes it into a
sequence of atomic edit operations, such as adding, removing, or spatially
relocating events. These operations are then executed by a diffusion model
trained to manipulate stereo audio. To support this, we design a data synthesis
pipeline that produces paired examples of high-level instructions, atomic edit
operations, and audios before and after each edit operation. Experiments
demonstrate that SmartDJ achieves superior perceptual quality, spatial realism,
and semantic alignment compared to prior audio editing methods. Demos are
available at https://zitonglan.github.io/project/smartdj/smartdj.html.

</details>


### [419] [Decoding Deception: Understanding Automatic Speech Recognition Vulnerabilities in Evasion and Poisoning Attacks](https://arxiv.org/abs/2509.22060)
*Aravindhan G,Yuvaraj Govindarajulu,Parin Shah*

Main category: cs.SD

TL;DR: 本文探索语音识别系统的低成本白盒攻击和非可迁移黑盒对抗攻击，还研究投毒攻击对模型性能的影响，实验表明混合模型能快速生成低扰动对抗样本，强调对抗安全的必要性。


<details>
  <summary>Details</summary>
Motivation: 以往研究主要关注受限优化的白盒攻击和基于可迁移性的黑盒攻击，本文旨在探索低成本白盒攻击和非可迁移黑盒对抗攻击，以及投毒攻击对模型性能的影响。

Method: 借鉴快速梯度符号法和零阶优化等方法，通过实验和分析进行研究。

Result: 混合模型能在一分钟内生成信噪比为35dB的低扰动但有影响力的对抗样本。

Conclusion: 现有开源模型的这些漏洞有实际安全影响，强调了对抗安全的必要性。

Abstract: Recent studies have demonstrated the vulnerability of Automatic Speech
Recognition systems to adversarial examples, which can deceive these systems
into misinterpreting input speech commands. While previous research has
primarily focused on white-box attacks with constrained optimizations, and
transferability based black-box attacks against commercial Automatic Speech
Recognition devices, this paper explores cost efficient white-box attack and
non transferability black-box adversarial attacks on Automatic Speech
Recognition systems, drawing insights from approaches such as Fast Gradient
Sign Method and Zeroth-Order Optimization. Further, the novelty of the paper
includes how poisoning attack can degrade the performances of state-of-the-art
models leading to misinterpretation of audio signals. Through experimentation
and analysis, we illustrate how hybrid models can generate subtle yet impactful
adversarial examples with very little perturbation having Signal Noise Ratio of
35dB that can be generated within a minute. These vulnerabilities of
state-of-the-art open source model have practical security implications, and
emphasize the need for adversarial security.

</details>


### [420] [Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach](https://arxiv.org/abs/2509.22378)
*Zijian Zhao,Dian Jin,Zijing Zhou*

Main category: cs.SD

TL;DR: 提出基于VLM的I2M框架，具高可解释性和低计算成本，经评估表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有I2M方法缺乏可解释性、计算资源需求大，难以满足普通用户需求。

Method: 利用ABC符号连接文本和音乐模态，采用多模态RAG和自精炼技术，利用生成动机和注意力图提供解释。

Result: 经人工和机器评估，在音乐质量和音乐 - 图像一致性上优于其他方法。

Conclusion: 所提基于VLM的I2M框架有良好表现，代码开源。

Abstract: Recently, Image-to-Music (I2M) generation has garnered significant attention,
with potential applications in fields such as gaming, advertising, and
multi-modal art creation. However, due to the ambiguous and subjective nature
of I2M tasks, most end-to-end methods lack interpretability, leaving users
puzzled about the generation results. Even methods based on emotion mapping
face controversy, as emotion represents only a singular aspect of art.
Additionally, most learning-based methods require substantial computational
resources and large datasets for training, hindering accessibility for common
users. To address these challenges, we propose the first Vision Language Model
(VLM)-based I2M framework that offers high interpretability and low
computational cost. Specifically, we utilize ABC notation to bridge the text
and music modalities, enabling the VLM to generate music using natural
language. We then apply multi-modal Retrieval-Augmented Generation (RAG) and
self-refinement techniques to allow the VLM to produce high-quality music
without external training. Furthermore, we leverage the generated motivations
in text and the attention maps from the VLM to provide explanations for the
generated results in both text and image modalities. To validate our method, we
conduct both human studies and machine evaluations, where our method
outperforms others in terms of music quality and music-image consistency,
indicating promising results. Our code is available at
https://github.com/RS2002/Image2Music .

</details>


### [421] [MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark](https://arxiv.org/abs/2509.22461)
*Hui Li,Changhao Jiang,Hongyu Wang,Ming Zhang,Jiajun Sun,Zhixiong Yang,Yifei Cao,Shihan Dou,Xiaoran Fan,Baoyu Fan,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.SD

TL;DR: 本文介绍MDAR基准用于评估模型在复杂音频推理任务的表现，对26个模型进行测试，发现现有模型在复杂推理任务有局限。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注静态或单场景设置，无法充分涵盖多说话者、事件展开和异构音频源交互的场景，因此需要新基准评估模型。

Method: 引入MDAR基准，包含3000个精心策划的问答对与多样音频片段，涵盖五类复杂推理和三种问题类型，对26个最先进的音频语言模型进行测试。

Result: 现有模型在复杂推理任务有局限，单选择题中Qwen2.5 - Omni准确率76.67%，GPT - 4o Audio为68.47%，但GPT - 4o Audio在多选和开放式任务上表现更好，所有模型在三种问题类型上表现均未达80%。

Conclusion: MDAR带来独特挑战，对推进音频推理研究有价值。

Abstract: The ability to reason from audio, including speech, paralinguistic cues,
environmental sounds, and music, is essential for AI agents to interact
effectively in real-world scenarios. Existing benchmarks mainly focus on static
or single-scene settings and do not fully capture scenarios where multiple
speakers, unfolding events, and heterogeneous audio sources interact. To
address these challenges, we introduce MDAR, a benchmark for evaluating models
on complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR
comprises 3,000 carefully curated question-answer pairs linked to diverse audio
clips, covering five categories of complex reasoning and spanning three
question types. We benchmark 26 state-of-the-art audio language models on MDAR
and observe that they exhibit limitations in complex reasoning tasks. On
single-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy,
whereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio
substantially outperforms Qwen2.5-Omni on the more challenging multiple-choice
and open-ended tasks. Across all three question types, no model achieves 80%
performance. These findings underscore the unique challenges posed by MDAR and
its value as a benchmark for advancing audio reasoning research.Code and
benchmark can be found at https://github.com/luckyerr/MDAR.

</details>


### [422] [Noise-to-Notes: Diffusion-based Generation and Refinement for Automatic Drum Transcription](https://arxiv.org/abs/2509.21739)
*Michael Yeung,Keisuke Toyama,Toya Teramoto,Shusuke Takahashi,Tamaki Kojima*

Main category: cs.SD

TL;DR: 本文将自动鼓转录（ADT）重新定义为条件生成任务，提出Noise - to - Notes（N2N）框架，引入Annealed Pseudo - Huber损失，结合音乐基础模型（MFM）特征，实验显示N2N在多个ADT基准上达到新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统ADT是判别任务，为实现灵活的速度 - 准确性权衡和强大的修复能力，将ADT重新定义为条件生成任务。

Method: 提出N2N框架，利用扩散模型将音频条件高斯噪声转换为鼓事件；引入Annealed Pseudo - Huber损失进行联合优化；结合MFM特征增强鲁棒性。

Result: 结合MFM特征显著提高了鲁棒性，N2N在多个ADT基准上达到新的最优性能。

Conclusion: 将ADT定义为条件生成任务的方法有效，结合MFM特征能增强模型性能，N2N框架具有良好表现。

Abstract: Automatic drum transcription (ADT) is traditionally formulated as a
discriminative task to predict drum events from audio spectrograms. In this
work, we redefine ADT as a conditional generative task and introduce
Noise-to-Notes (N2N), a framework leveraging diffusion modeling to transform
audio-conditioned Gaussian noise into drum events with associated velocities.
This generative diffusion approach offers distinct advantages, including a
flexible speed-accuracy trade-off and strong inpainting capabilities. However,
the generation of binary onset and continuous velocity values presents a
challenge for diffusion models, and to overcome this, we introduce an Annealed
Pseudo-Huber loss to facilitate effective joint optimization. Finally, to
augment low-level spectrogram features, we propose incorporating features
extracted from music foundation models (MFMs), which capture high-level
semantic information and enhance robustness to out-of-domain drum audio.
Experimental results demonstrate that including MFM features significantly
improves robustness and N2N establishes a new state-of-the-art performance
across multiple ADT benchmarks.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [423] [A regret minimization approach to fixed-point iterations](https://arxiv.org/abs/2509.21653)
*Joon Kwon*

Main category: math.OC

TL;DR: 提出将后悔最小化算法转换为定点迭代的方案，获得新定点迭代，对AdaGrad族算法转换有新自适应保证，实验显示AdaGrad定点迭代收敛更快。


<details>
  <summary>Details</summary>
Motivation: 将后悔最小化算法转化为定点迭代，获得新的定点迭代方法。

Method: 提出转换方案，将后悔最小化算法转换为定点迭代，对AdaGrad族算法进行转换。

Result: 得到新的简单定点迭代，对AdaGrad族算法转换获得新的自适应保证，数值实验表明AdaGrad定点迭代比Krasnoselskii - Mann迭代收敛更快。

Conclusion: 所提转换方案有效，能得到新的定点迭代方法，AdaGrad定点迭代具有优势。

Abstract: We propose a conversion scheme that turns regret minimizing algorithms into
fixed point iterations, with convergence guarantees following from regret
bounds. The resulting iterations can be seen as a grand extension of the
classical Krasnoselskii--Mann iterations, as the latter are recovered by
converting the Online Gradient Descent algorithm. This approach yields new
simple iterations for finding fixed points of non-self operators. We also focus
on converting algorithms from the AdaGrad family of regret minimizers, and thus
obtain fixed point iterations with adaptive guarantees of a new kind. Numerical
experiments on various problems demonstrate faster convergence of AdaGrad-based
fixed point iterations over Krasnoselskii--Mann iterations.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [424] [Foundation models for high-energy physics](https://arxiv.org/abs/2509.21434)
*Anna Hallin*

Main category: hep-ph

TL;DR: 综述高能物理领域基础模型研究进展。


<details>
  <summary>Details</summary>
Motivation: 基础模型在自然语言处理和计算机视觉领域取得成功，关注能否用于高能物理研究。

Method: 对该领域已发表研究进行总结和讨论。

Result: 完成对高能物理领域基础模型相关研究的总结。

Conclusion: 本综述是该领域首个相关综述。

Abstract: The rise of foundation models -- large, pretrained machine learning models
that can be finetuned to a variety of tasks -- has revolutionized the fields of
natural language processing and computer vision. In high-energy physics, the
question of whether these models can be implemented directly in physics
research, or even built from scratch, tailored for particle physics data, has
generated an increasing amount of attention. This review, which is the first on
the topic of foundation models in high-energy physics, summarizes and discusses
the research that has been published in the field so far.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [425] [Direct Bias-Correction Term Estimation for Propensity Scores and Average Treatment Effect Estimation](https://arxiv.org/abs/2509.22122)
*Masahiro Kato*

Main category: econ.EM

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This study considers the estimation of the average treatment effect (ATE).
For ATE estimation, we estimate the propensity score through direct
bias-correction term estimation. Let $\{(X_i, D_i, Y_i)\}_{i=1}^{n}$ be the
observations, where $X_i \in \mathbb{R}^p$ denotes $p$-dimensional covariates,
$D_i \in \{0, 1\}$ denotes a binary treatment assignment indicator, and $Y_i
\in \mathbb{R}$ is an outcome. In ATE estimation, the bias-correction term
$h_0(X_i, D_i) = \frac{1[D_i = 1]}{e_0(X_i)} - \frac{1[D_i = 0]}{1 - e_0(X_i)}$
plays an important role, where $e_0(X_i)$ is the propensity score, the
probability of being assigned treatment $1$. In this study, we propose
estimating $h_0$ (or equivalently the propensity score $e_0$) by directly
minimizing the prediction error of $h_0$. Since the bias-correction term $h_0$
is essential for ATE estimation, this direct approach is expected to improve
estimation accuracy for the ATE. For example, existing studies often employ
maximum likelihood or covariate balancing to estimate $e_0$, but these
approaches may not be optimal for accurately estimating $h_0$ or the ATE. We
present a general framework for this direct bias-correction term estimation
approach from the perspective of Bregman divergence minimization and conduct
simulation studies to evaluate the effectiveness of the proposed method.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [426] [Automating Sensor Characterization with Bayesian Optimization](https://arxiv.org/abs/2509.21661)
*J. Cuevas-Zepeda,C. Chavez,J. Estrada,J. Noonan,B. D. Nord,N. Saffold,M. Sofo-Haro,R. Spinola e Castro,S. Trivedi*

Main category: physics.ins-det

TL;DR: 本文提出自动化传感器校准技术加速设备开发测试阶段，利用闭环贝叶斯优化，以低噪声CCD演示，能在数天内无专家监督完成传感器表征与优化。


<details>
  <summary>Details</summary>
Motivation: 当前探测器表征是设备开发瓶颈，测试阶段需大量时间进行设备表征和参数优化，耗费专家大量时间，因此需加速测试阶段。

Method: 提出基于闭环贝叶斯优化（BO）的自动化传感器校准技术，利用实时测量指导参数选择和确定最佳运行状态。

Result: 以新型低噪声CCD为例，展示机器学习驱动工具能在几天内高效完成传感器表征和优化运行，无需设备专家监督。

Conclusion: 所提出的自动化传感器校准技术可有效加速设备开发的测试阶段。

Abstract: The development of novel instrumentation requires an iterative cycle with
three stages: design, prototyping, and testing. Recent advancements in
simulation and nanofabrication techniques have significantly accelerated the
design and prototyping phases. Nonetheless, detector characterization continues
to be a major bottleneck in device development. During the testing phase, a
significant time investment is required to characterize the device in different
operating conditions and find optimal operating parameters. The total effort
spent on characterization and parameter optimization can occupy a year or more
of an expert's time. In this work, we present a novel technique for automated
sensor calibration that aims to accelerate the testing stage of the development
cycle. This technique leverages closed-loop Bayesian optimization (BO), using
real-time measurements to guide parameter selection and identify optimal
operating states. We demonstrate the method with a novel low-noise CCD, showing
that the machine learning-driven tool can efficiently characterize and optimize
operation of the sensor in a couple of days without supervision of a device
expert.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [427] [Exploring the Early Universe with Deep Learning](https://arxiv.org/abs/2509.22018)
*Emmanuel de Salis,Massimo De Santis,Davide Piras,Sambit K. Giri,Michele Bianco,Nicolas Cerardi,Philipp Denzel,Merve Selcuk-Simsek,Kelley M. Hess,M. Carmen Toribio,Franz Kirsten,Hatem Ghorbel*

Main category: astro-ph.CO

TL;DR: 利用深度学习技术从SKAO预期的氢信号2D功率谱中提取信息，助力早期宇宙结构形成的宇宙学和天体物理学推断。


<details>
  <summary>Details</summary>
Motivation: SKAO将产生大量数据，且氢信号会受前景污染和仪器系统误差影响，难以提取天体物理信息，需新方法解决。

Method: 开发最新深度学习技术，应用一系列神经网络模型到测量中，量化其预测宇宙氢再电离历史的能力。

Result: 专用机器学习算法在恢复再电离历史上平均能达到超过0.95的R²分数。

Conclusion: 早期宇宙研究受益于现代深度学习技术，能实现对早期宇宙结构形成的准确和精确推断。

Abstract: Hydrogen is the most abundant element in our Universe. The first generation
of stars and galaxies produced photons that ionized hydrogen gas, driving a
cosmological event known as the Epoch of Reionization (EoR). The upcoming
Square Kilometre Array Observatory (SKAO) will map the distribution of neutral
hydrogen during this era, aiding in the study of the properties of these
first-generation objects. Extracting astrophysical information will be
challenging, as SKAO will produce a tremendous amount of data where the
hydrogen signal will be contaminated with undesired foreground contamination
and instrumental systematics. To address this, we develop the latest deep
learning techniques to extract information from the 2D power spectra of the
hydrogen signal expected from SKAO. We apply a series of neural network models
to these measurements and quantify their ability to predict the history of
cosmic hydrogen reionization, which is connected to the increasing number and
efficiency of early photon sources. We show that the study of the early
Universe benefits from modern deep learning technology. In particular, we
demonstrate that dedicated machine learning algorithms can achieve more than a
$0.95$ $R^2$ score on average in recovering the reionization history. This
enables accurate and precise cosmological and astrophysical inference of
structure formation in the early Universe.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [428] [Context-Aware Hybrid Routing in Bluetooth Mesh Networks Using Multi-Model Machine Learning and AODV Fallback](https://arxiv.org/abs/2509.21490)
*Md Sajid Islam,Tanvir Hasan*

Main category: cs.NI

TL;DR: 本文提出混合智能路由框架改进蓝牙网状网络路由，模拟实验显示其性能优于基线和中间方法，证明机器学习模型可增强路由可靠性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统路由策略在拥塞和拓扑变化下性能下降，需要改进蓝牙网状网络的路由策略。

Method: 提出混合智能路由框架，将四个预测模型集成到统一评分机制中，通过模拟环境评估三种策略。

Result: 在十种场景下，混合ABCD模型的数据包交付率约为99.97%，显著优于基线和中间方法。

Conclusion: 轻量级、可解释的机器学习模型能增强蓝牙网状网络的路由可靠性和适应性，尤其适用于优先考虑交付成功而非延迟约束的无基础设施环境。

Abstract: Bluetooth-based mesh networks offer a promising infrastructure for offline
communication in emergency and resource constrained scenarios. However,
traditional routing strategies such as Ad hoc On-Demand Distance Vector (AODV)
often degrade under congestion and dynamic topological changes. This study
proposes a hybrid intelligent routing framework that augments AODV with
supervised machine learning to improve next-hop selection under varied network
constraints. The framework integrates four predictive models: a delivery
success classifier, a TTL regressor, a delay regressor, and a forwarder
suitability classifier, into a unified scoring mechanism that dynamically ranks
neighbors during multi-hop message transmission. A simulation environment with
stationary node deployments was developed, incorporating buffer constraints and
device heterogeneity to evaluate three strategies: baseline AODV, a partial
hybrid ML model (ABC), and the full hybrid ML model (ABCD). Across ten
scenarios, the Hybrid ABCD model achieves approximately 99.97 percent packet
delivery under these controlled conditions, significantly outperforming both
the baseline and intermediate approaches. The results demonstrate that
lightweight, explainable machine learning models can enhance routing
reliability and adaptability in Bluetooth mesh networks, particularly in
infrastructure-less environments where delivery success is prioritized over
latency constraints.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [429] [Joint graph entropy knowledge distillation for point cloud classification and robustness against corruptions](https://arxiv.org/abs/2509.22150)
*Zhiqiang Tian,Weigang Li,Junwei Hu,Chunhua Deng*

Main category: cs.CV

TL;DR: 提出适用于非独立同分布3D点云数据的JGEKD分类策略，通过构建基于联合图熵的损失函数实现知识转移，实验证明策略有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云分类任务假设类别事件独立同分布，破坏了类别间的相关性。

Method: 构建基于联合图熵的损失函数实现类别相关性的知识转移；采用联合图捕捉类别间隐藏关系；构建孪生结构和两种蒸馏框架促进信息传递；实现点云及其损坏形式间的知识转移。

Result: 在ScanObject、ModelNet40、ScanntV2_cls和ModelNet - C上的实验表明该策略能取得有竞争力的结果。

Conclusion: 所提出的JGEKD策略适用于非独立同分布的3D点云数据分类，能有效实现知识转移并提升模型性能。

Abstract: Classification tasks in 3D point clouds often assume that class events
\replaced{are }{follow }independent and identically distributed (IID), although
this assumption destroys the correlation between classes. This \replaced{study
}{paper }proposes a classification strategy, \textbf{J}oint \textbf{G}raph
\textbf{E}ntropy \textbf{K}nowledge \textbf{D}istillation (JGEKD), suitable for
non-independent and identically distributed 3D point cloud data,
\replaced{which }{the strategy } achieves knowledge transfer of class
correlations through knowledge distillation by constructing a loss function
based on joint graph entropy. First\deleted{ly}, we employ joint graphs to
capture add{the }hidden relationships between classes\replaced{ and}{,}
implement knowledge distillation to train our model by calculating the entropy
of add{add }graph.\replaced{ Subsequently}{ Then}, to handle 3D point clouds
\deleted{that is }invariant to spatial transformations, we construct
\replaced{S}{s}iamese structures and develop two frameworks, self-knowledge
distillation and teacher-knowledge distillation, to facilitate information
transfer between different transformation forms of the same data. \replaced{In
addition}{ Additionally}, we use the above framework to achieve knowledge
transfer between point clouds and their corrupted forms, and increase the
robustness against corruption of model. Extensive experiments on ScanObject,
ModelNet40, ScanntV2\_cls and ModelNet-C demonstrate that the proposed strategy
can achieve competitive results.

</details>


### [430] [Random Direct Preference Optimization for Radiography Report Generation](https://arxiv.org/abs/2509.21351)
*Valentin Samokhin,Boris Shirokikh,Mikhail Goncharov,Dmitriy Umerenkov,Maksim Bobrin,Ivan Oseledets,Dmitry Dylov,Mikhail Belyaev*

Main category: cs.CV

TL;DR: 本文提出用Direct Preference Optimization (DPO) 增强放射学报告生成 (RRG) 准确性的模型无关框架，实验显示可提升临床性能指标。


<details>
  <summary>Details</summary>
Motivation: 现有RRG方法未达临床应用质量要求，大视觉语言模型有进展，需新方法提升RRG准确性。

Method: 引入基于DPO的模型无关框架，利用随机对比采样构建训练对，无需奖励模型或人工偏好注释。

Result: 用Random DPO补充三个先进模型的实验表明，可将临床性能指标提高达5%，无需额外训练数据。

Conclusion: 所提方法能有效提升RRG准确性，且无需额外训练数据。

Abstract: Radiography Report Generation (RRG) has gained significant attention in
medical image analysis as a promising tool for alleviating the growing workload
of radiologists. However, despite numerous advancements, existing methods have
yet to achieve the quality required for deployment in real-world clinical
settings. Meanwhile, large Visual Language Models (VLMs) have demonstrated
remarkable progress in the general domain by adopting training strategies
originally designed for Large Language Models (LLMs), such as alignment
techniques. In this paper, we introduce a model-agnostic framework to enhance
RRG accuracy using Direct Preference Optimization (DPO). Our approach leverages
random contrastive sampling to construct training pairs, eliminating the need
for reward models or human preference annotations. Experiments on supplementing
three state-of-the-art models with our Random DPO show that our method improves
clinical performance metrics by up to 5%, without requiring any additional
training data.

</details>


### [431] [KV-Efficient VLA: A Method of Speed up Vision Language Model with RNN-Gated Chunked KV Cache](https://arxiv.org/abs/2509.21354)
*Wanshun Xu,Long Zhuang*

Main category: cs.CV

TL;DR: 提出KV - Efficient VLA框架解决VLA模型扩展性问题，可加速推理、减少内存占用。


<details>
  <summary>Details</summary>
Motivation: 现有Vision - Language - Action (VLA)模型扩展性受注意力二次成本和长时推理时KV内存无界增长限制，且部分方法忽略推理效率问题。

Method: 提出模型无关的内存压缩框架KV - Efficient VLA，将KV缓存分块，用循环门控模块根据效用分数总结和过滤历史上下文。

Result: 理论上实现1.21倍推理加速和36%的KV内存减少，对任务成功率影响小，可无缝集成到现有VLA堆栈。

Conclusion: KV - Efficient VLA能解决VLA模型扩展性问题，实现可扩展推理，无需修改训练流程和下游控制逻辑。

Abstract: Vision-Language-Action (VLA) models promise unified robotic perception and
control, yet their scalability is constrained by the quadratic cost of
attention and the unbounded growth of key-value (KV) memory during long-horizon
inference. While recent methods improve generalization through scaling backbone
architectures, they often neglect the inference inefficiencies critical to
real-time deployment. In this work, we present KV-Efficient VLA, a
model-agnostic memory compression framework that addresses these limitations by
introducing a lightweight, training-friendly mechanism to selectively retain
high-utility context. Our method partitions the KV cache into fixed size chunks
and employs a recurrent gating module to summarize and filter historical
context according to learned utility scores. This design preserves recent
fine-grained detail while aggressively pruning stale, low-relevance memory, all
while maintaining causality. Theoretically, KV-Efficient VLA yields up to 1.21x
inference speedup and 36% KV memory reduction, with minimal impact on task
success. Our method integrates seamlessly into existing autoregressive and
hybrid VLA stacks, enabling scalable inference without modifying training
pipelines or downstream control logic.

</details>


### [432] [Phrase-grounded Fact-checking for Automatically Generated Chest X-ray Reports](https://arxiv.org/abs/2509.21356)
*Razi Mahmood,Diego Machado-Reyes,Joy Wu,Parisa Kaviani,Ken C. L. Wong,Niharika D'Souza,Mannudeep Kalra,Ge Wang,Pingkun Yan,Tanveer Syeda-Mahmood*

Main category: cs.CV

TL;DR: 提出用于检测自动生成胸部放射报告错误的短语接地事实检查模型，在多数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型生成放射报告存在事实错误和幻觉问题，阻碍临床应用。

Method: 通过扰动真实报告形成合成数据集模拟错误，训练多标签跨模态对比回归网络。

Result: 方法在多数据集上对发现真实性预测和定位准确，检测SOTA报告生成器报告错误时与基于真值验证的一致性相关系数达0.997。

Conclusion: 该模型在放射工作流程临床推理中具有实用价值。

Abstract: With the emergence of large-scale vision language models (VLM), it is now
possible to produce realistic-looking radiology reports for chest X-ray images.
However, their clinical translation has been hampered by the factual errors and
hallucinations in the produced descriptions during inference. In this paper, we
present a novel phrase-grounded fact-checking model (FC model) that detects
errors in findings and their indicated locations in automatically generated
chest radiology reports.
  Specifically, we simulate the errors in reports through a large synthetic
dataset derived by perturbing findings and their locations in ground truth
reports to form real and fake findings-location pairs with images. A new
multi-label cross-modal contrastive regression network is then trained on this
dataset. We present results demonstrating the robustness of our method in terms
of accuracy of finding veracity prediction and localization on multiple X-ray
datasets. We also show its effectiveness for error detection in reports of SOTA
report generators on multiple datasets achieving a concordance correlation
coefficient of 0.997 with ground truth-based verification, thus pointing to its
utility during clinical inference in radiology workflows.

</details>


### [433] [MDF-MLLM: Deep Fusion Through Cross-Modal Feature Alignment for Contextually Aware Fundoscopic Image Classification](https://arxiv.org/abs/2509.21358)
*Jason Jordan,Mohammadreza Akbari Lor,Peter Koulen,Mei-Ling Shyu,Shu-Ching Chen*

Main category: cs.CV

TL;DR: 本文提出MDF - MLLM模型，融合多尺度特征提升眼底图像疾病分类准确率，表现优于传统基线模型，有临床应用前景。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型难以捕捉诊断视网膜疾病所需的低级空间细节，为提升眼底图像疾病分类准确率开展研究。

Method: 在三个公共数据集的1305个眼底图像 - 文本对上进行模型开发和验证，将U - Net编码器四层的跳跃特征集成到LLaMA 3.2 11B MLLM的交叉注意力块中，使用缩放交叉注意力和基于FiLM的U - Net调制融合视觉特征。

Result: 基线MLLM准确率60%，MDF - MLLM准确率达94%，召回率和F1分数分别提升67%和35%，消融研究证实多深度融合方法有显著提升。

Conclusion: MDF - MLLM是可推广、可解释和模块化的眼底图像分类框架，优于传统基线，有望用于临床决策支持系统，未来将探索同步训练技术等。

Abstract: This study aimed to enhance disease classification accuracy from retinal
fundus images by integrating fine-grained image features and global textual
context using a novel multimodal deep learning architecture. Existing
multimodal large language models (MLLMs) often struggle to capture low-level
spatial details critical for diagnosing retinal diseases such as glaucoma,
diabetic retinopathy, and retinitis pigmentosa. This model development and
validation study was conducted on 1,305 fundus image-text pairs compiled from
three public datasets (FIVES, HRF, and StoneRounds), covering acquired and
inherited retinal diseases, and evaluated using classification accuracy and
F1-score. The MDF-MLLM integrates skip features from four U-Net encoder layers
into cross-attention blocks within a LLaMA 3.2 11B MLLM. Vision features are
patch-wise projected and fused using scaled cross-attention and FiLM-based
U-Net modulation. Baseline MLLM achieved 60% accuracy on the dual-type disease
classification task. MDF-MLLM, with both U-Net and MLLM components fully
fine-tuned during training, achieved a significantly higher accuracy of 94%,
representing a 56% improvement. Recall and F1-scores improved by as much as 67%
and 35% over baseline, respectively. Ablation studies confirmed that the
multi-depth fusion approach contributed to substantial gains in spatial
reasoning and classification, particularly for inherited diseases with rich
clinical text. MDF-MLLM presents a generalizable, interpretable, and modular
framework for fundus image classification, outperforming traditional MLLM
baselines through multi-scale feature fusion. The architecture holds promise
for real-world deployment in clinical decision support systems. Future work
will explore synchronized training techniques, a larger pool of diseases for
more generalizability, and extending the model for segmentation tasks.

</details>


### [434] [Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image Models](https://arxiv.org/abs/2509.21360)
*Xingkai Peng,Jun Jiang,Meng Tong,Shuai Li,Weiming Zhang,Nenghai Yu,Kejiang Chen*

Main category: cs.CV

TL;DR: 现有文本到图像（T2I）模型存在被越狱攻击生成NSFW内容的风险，文本越狱方法有局限，提出多模态提示解耦攻击（MPDA）及步骤。


<details>
  <summary>Details</summary>
Motivation: 解决现有T2I模型越狱方法主要针对文本提示，基于图像输入的潜在漏洞未被充分探索以及文本方法绕过安全过滤器有挑战的问题。

Method: 提出MPDA，包含三个核心步骤：用大语言模型将不安全提示解耦为伪安全提示和有害提示；将有害提示重写为自然对抗提示；用视觉语言模型生成图像标题以指导迭代重写和精炼。

Result: 无明确提及结果。

Conclusion: 无明确提及结论。

Abstract: Text-to-image (T2I) models have been widely applied in generating
high-fidelity images across various domains. However, these models may also be
abused to produce Not-Safe-for-Work (NSFW) content via jailbreak attacks.
Existing jailbreak methods primarily manipulate the textual prompt, leaving
potential vulnerabilities in image-based inputs largely unexplored. Moreover,
text-based methods face challenges in bypassing the model's safety filters. In
response to these limitations, we propose the Multimodal Prompt Decoupling
Attack (MPDA), which utilizes image modality to separate the harmful semantic
components of the original unsafe prompt. MPDA follows three core steps:
firstly, a large language model (LLM) decouples unsafe prompts into pseudo-safe
prompts and harmful prompts. The former are seemingly harmless sub-prompts that
can bypass filters, while the latter are sub-prompts with unsafe semantics that
trigger filters. Subsequently, the LLM rewrites the harmful prompts into
natural adversarial prompts to bypass safety filters, which guide the T2I model
to modify the base image into an NSFW output. Finally, to ensure semantic
consistency between the generated NSFW images and the original unsafe prompts,
the visual language model generates image captions, providing a new pathway to
guide the LLM in iterative rewriting and refining the generated content.

</details>


### [435] [A Mutual Learning Method for Salient Object Detection with intertwined Multi-Supervision--Revised](https://arxiv.org/abs/2509.21363)
*Runmin Wu,Mengyang Feng,Wenlong Guan,Dong Wang,Huchuan Lu,Errui Ding*

Main category: cs.CV

TL;DR: 提出利用多任务监督训练显著目标检测网络，含互学习模块，在多数据集取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习显著目标检测方法存在预测不完整和边界不准确问题。

Method: 利用显著目标检测、前景轮廓检测和边缘检测多任务监督训练，采用互学习模块，多分支网络以互学习方式训练。

Result: 在七个具有挑战性的数据集上实验，在显著目标检测和边缘检测中取得了最先进的结果。

Conclusion: 所提方法可有效解决现有显著目标检测方法的不足，提升检测性能。

Abstract: Though deep learning techniques have made great progress in salient object
detection recently, the predicted saliency maps still suffer from incomplete
predictions due to the internal complexity of objects and inaccurate boundaries
caused by strides in convolution and pooling operations. To alleviate these
issues, we propose to train saliency detection networks by exploiting the
supervision from not only salient object detection, but also foreground contour
detection and edge detection. First, we leverage salient object detection and
foreground contour detection tasks in an intertwined manner to generate
saliency maps with uniform highlight. Second, the foreground contour and edge
detection tasks guide each other simultaneously, thereby leading to precise
foreground contour prediction and reducing the local noises for edge
prediction. In addition, we develop a novel mutual learning module (MLM) which
serves as the building block of our method. Each MLM consists of multiple
network branches trained in a mutual learning manner, which improves the
performance by a large margin. Extensive experiments on seven challenging
datasets demonstrate that the proposed method has delivered state-of-the-art
results in both salient object detection and edge detection.

</details>


### [436] [MAJORScore: A Novel Metric for Evaluating Multimodal Relevance via Joint Representation](https://arxiv.org/abs/2509.21365)
*Zhicheng Du,Qingyang Shi,Jiasheng Lu,Yingshan Liang,Xinyu Zhang,Yiran Wang,Peiwu Qin*

Main category: cs.CV

TL;DR: 提出新的多模态相关性评估指标MAJORScore，实验显示其比现有方法更优，可用于大规模多模态数据集和模型性能评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标仅适用于两模态关联分析，限制了多模态相似性评估。

Method: 通过多模态联合表示提出新的多模态相关性评估指标MAJORScore。

Result: 与现有方法相比，MAJORScore在一致模态上提升26.03%-64.29%，在不一致模态上降低13.28%-20.54%。

Conclusion: MAJORScore是评估大规模多模态数据集相似性和多模态模型性能更可靠的指标。

Abstract: The multimodal relevance metric is usually borrowed from the embedding
ability of pretrained contrastive learning models for bimodal data, which is
used to evaluate the correlation between cross-modal data (e.g., CLIP).
However, the commonly used evaluation metrics are only suitable for the
associated analysis between two modalities, which greatly limits the evaluation
of multimodal similarity. Herein, we propose MAJORScore, a brand-new evaluation
metric for the relevance of multiple modalities ($N$ modalities, $N\ge3$) via
multimodal joint representation for the first time. The ability of multimodal
joint representation to integrate multiple modalities into the same latent
space can accurately represent different modalities at one scale, providing
support for fair relevance scoring. Extensive experiments have shown that
MAJORScore increases by 26.03%-64.29% for consistent modality and decreases by
13.28%-20.54% for inconsistence compared to existing methods. MAJORScore serves
as a more reliable metric for evaluating similarity on large-scale multimodal
datasets and multimodal model performance evaluation.

</details>


### [437] [Safety Assessment of Scaffolding on Construction Site using AI](https://arxiv.org/abs/2509.21368)
*Sameer Prabhu,Amit Patwardhan,Ramin Karim*

Main category: cs.CV

TL;DR: 本文探讨利用人工智能和数字化提升脚手架检查准确性及安全性，开发云平台处理点云数据，实现自动监测。


<details>
  <summary>Details</summary>
Motivation: 当前脚手架检查主要靠人工视觉，耗时长且易出错，需提升检查准确性和安全性。

Method: 开发基于云的人工智能平台，处理和分析脚手架结构的点云数据，通过对比认证参考数据与最新点云数据检测结构修改。

Result: 该方法可实现脚手架自动监测。

Conclusion: 此方法能减少人工检查时间和精力，提升建筑工地安全性。

Abstract: In the construction industry, safety assessment is vital to ensure both the
reliability of assets and the safety of workers. Scaffolding, a key structural
support asset requires regular inspection to detect and identify alterations
from the design rules that may compromise the integrity and stability. At
present, inspections are primarily visual and are conducted by site manager or
accredited personnel to identify deviations. However, visual inspection is
time-intensive and can be susceptible to human errors, which can lead to unsafe
conditions. This paper explores the use of Artificial Intelligence (AI) and
digitization to enhance the accuracy of scaffolding inspection and contribute
to the safety improvement. A cloud-based AI platform is developed to process
and analyse the point cloud data of scaffolding structure. The proposed system
detects structural modifications through comparison and evaluation of certified
reference data with the recent point cloud data. This approach may enable
automated monitoring of scaffolding, reducing the time and effort required for
manual inspections while enhancing the safety on a construction site.

</details>


### [438] [Automated Prompt Generation for Creative and Counterfactual Text-to-image Synthesis](https://arxiv.org/abs/2509.21375)
*Aleksa Jelaca,Ying Jiao,Chang Tian,Marie-Francine Moens*

Main category: cs.CV

TL;DR: 本文聚焦文本到图像生成中反事实尺寸可控性问题，提出自动提示工程框架，构建数据集，实验表明方法优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 文本到图像生成虽有进展，但细粒度可控性尤其是反事实可控性仍是挑战，需解决该问题以推动创造力和探索性应用。

Method: 提出自动提示工程框架，包含图像评估器、监督提示重写器和DPO训练的排序器；构建首个反事实尺寸文本 - 图像数据集；扩展Grounded SAM改进图像评估器。

Result: 图像评估器较其骨干有114%的提升，方法优于现有基线和ChatGPT - 4o。

Conclusion: 为反事实可控性的未来研究奠定了基础。

Abstract: Text-to-image generation has advanced rapidly with large-scale multimodal
training, yet fine-grained controllability remains a critical challenge.
Counterfactual controllability, defined as the capacity to deliberately
generate images that contradict common-sense patterns, remains a major
challenge but plays a crucial role in enabling creativity and exploratory
applications. In this work, we address this gap with a focus on counterfactual
size (e.g., generating a tiny walrus beside a giant button) and propose an
automatic prompt engineering framework that adapts base prompts into revised
prompts for counterfactual images. The framework comprises three components: an
image evaluator that guides dataset construction by identifying successful
image generations, a supervised prompt rewriter that produces revised prompts,
and a DPO-trained ranker that selects the optimal revised prompt. We construct
the first counterfactual size text-image dataset and enhance the image
evaluator by extending Grounded SAM with refinements, achieving a 114 percent
improvement over its backbone. Experiments demonstrate that our method
outperforms state-of-the-art baselines and ChatGPT-4o, establishing a
foundation for future research on counterfactual controllability.

</details>


### [439] [In silico Deep Learning Protocols for Label-Free Super-Resolution Microscopy: A Comparative Study of Network Architectures and SNR Dependence](https://arxiv.org/abs/2509.21376)
*Shiraz S Kaderuppan,Jonathan Mar,Andrew Irvine,Anurag Sharma,Muhammad Ramadan Saifuddin,Wai Leong Eugene Wong,Wai Lok Woo*

Main category: cs.CV

TL;DR: 本文评估非荧光相位调制显微镜实现超分辨率光学显微镜的经济方法，用两个DNN架构处理纳米特征图像，发现不同信噪比下两模型互补。


<details>
  <summary>Details</summary>
Motivation: 光学显微镜横向分辨率有限，现有解决方法成本高或需专业技术，需经济的超分辨率方法。

Method: 用之前开发的O - Net和Theta - Net两个DNN架构处理经原子力显微镜校准的含纳米特征的测试目标图像。

Result: 两模型在超分辨率成像中表现良好，高信噪比适合O - Net，低信噪比适合Theta - Net。

Conclusion: 在非荧光光学纳米显微镜中使用DNN模型时，模型架构和图像信噪比会影响模型性能和超分辨率图像质量。

Abstract: The field of optical microscopy spans across numerous industries and research
domains, ranging from education to healthcare, quality inspection and analysis.
Nonetheless, a key limitation often cited by optical microscopists refers to
the limit of its lateral resolution (typically defined as ~200nm), with
potential circumventions involving either costly external modules (e.g.
confocal scan heads, etc) and/or specialized techniques [e.g. super-resolution
(SR) fluorescent microscopy]. Addressing these challenges in a normal
(non-specialist) context thus remains an aspect outside the scope of most
microscope users & facilities. This study thus seeks to evaluate an alternative
& economical approach to achieving SR optical microscopy, involving
non-fluorescent phase-modulated microscopical modalities such as Zernike phase
contrast (PCM) and differential interference contrast (DIC) microscopy. Two in
silico deep neural network (DNN) architectures which we developed previously
(termed O-Net and Theta-Net) are assessed on their abilities to resolve a
custom-fabricated test target containing nanoscale features calibrated via
atomic force microscopy (AFM). The results of our study demonstrate that
although both O-Net and Theta-Net seemingly performed well when super-resolving
these images, they were complementary (rather than competing) approaches to be
considered for image SR, particularly under different image signal-to-noise
ratios (SNRs). High image SNRs favoured the application of O-Net models, while
low SNRs inclined preferentially towards Theta-Net models. These findings
demonstrate the importance of model architectures (in conjunction with the
source image SNR) on model performance and the SR quality of the generated
images where DNN models are utilized for non-fluorescent optical nanoscopy,
even where the same training dataset & number of epochs are being used.

</details>


### [440] [Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation](https://arxiv.org/abs/2509.21377)
*Yinfeng Yu,Hailong Zhang,Meiling Zhu*

Main category: cs.CV

TL;DR: 提出DMTF - AVN方法用于视听具身导航，实验表明其性能达SOTA，有良好扩展性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有视听具身导航工作常忽视更深层感知上下文，需更好利用多模态线索来指导导航。

Method: 提出DMTF - AVN，采用多目标架构和改进的Transformer机制过滤和选择性融合跨模态信息。

Result: 在Replica和Matterport3D数据集上实验，DMTF - AVN在成功率、路径效率和场景适应性上超过现有方法。

Conclusion: DMTF - AVN性能达SOTA，具有强扩展性和泛化性，为机器人导航的多模态融合策略提供方向。

Abstract: Audiovisual embodied navigation enables robots to locate audio sources by
dynamically integrating visual observations from onboard sensors with the
auditory signals emitted by the target. The core challenge lies in effectively
leveraging multimodal cues to guide navigation. While prior works have explored
basic fusion of visual and audio data, they often overlook deeper perceptual
context. To address this, we propose the Dynamic Multi-Target Fusion for
Efficient Audio-Visual Navigation (DMTF-AVN). Our approach uses a multi-target
architecture coupled with a refined Transformer mechanism to filter and
selectively fuse cross-modal information. Extensive experiments on the Replica
and Matterport3D datasets demonstrate that DMTF-AVN achieves state-of-the-art
performance, outperforming existing methods in success rate (SR), path
efficiency (SPL), and scene adaptation (SNA). Furthermore, the model exhibits
strong scalability and generalizability, paving the way for advanced multimodal
fusion strategies in robotic navigation. The code and videos are available at
  https://github.com/zzzmmm-svg/DMTF.

</details>


### [441] [SAEmnesia: Erasing Concepts in Diffusion Models with Sparse Autoencoders](https://arxiv.org/abs/2509.21379)
*Enrico Cassano,Riccardo Renzulli,Marco Nurisso,Mirko Zaffaroni,Alan Perotti,Marco Grangetto*

Main category: cs.CV

TL;DR: 提出SAEmnesia方法用于文本到图像扩散模型概念遗忘，能促进一对一概念 - 神经元映射，减少超参数搜索，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本到图像扩散模型概念遗忘时，概念表示分散需大量搜索，要精确确定概念表示位置。

Method: 引入SAEmnesia，一种有监督的稀疏自编码器训练方法，通过系统概念标记促进一对一概念 - 神经元映射。

Result: 学习到的专业神经元概念关联更强，推理时减少96.67%超参数搜索，在UnlearnCanvas基准上提升9.22%，顺序遗忘任务中9对象移除准确率提升28.4%。

Conclusion: SAEmnesia方法有效，在概念遗忘任务中有良好表现和可扩展性。

Abstract: Effective concept unlearning in text-to-image diffusion models requires
precise localization of concept representations within the model's latent
space. While sparse autoencoders successfully reduce neuron polysemanticity
(i.e., multiple concepts per neuron) compared to the original network,
individual concept representations can still be distributed across multiple
latent features, requiring extensive search procedures for concept unlearning.
We introduce SAEmnesia, a supervised sparse autoencoder training method that
promotes one-to-one concept-neuron mappings through systematic concept
labeling, mitigating feature splitting and promoting feature centralization.
Our approach learns specialized neurons with significantly stronger concept
associations compared to unsupervised baselines. The only computational
overhead introduced by SAEmnesia is limited to cross-entropy computation during
training. At inference time, this interpretable representation reduces
hyperparameter search by 96.67% with respect to current approaches. On the
UnlearnCanvas benchmark, SAEmnesia achieves a 9.22% improvement over the
state-of-the-art. In sequential unlearning tasks, we demonstrate superior
scalability with a 28.4% improvement in unlearning accuracy for 9-object
removal.

</details>


### [442] [Do Sparse Subnetworks Exhibit Cognitively Aligned Attention? Effects of Pruning on Saliency Map Fidelity, Sparsity, and Concept Coherence](https://arxiv.org/abs/2509.21387)
*Sanish Suwal,Dipkamal Bhusal,Michael Clifford,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 研究基于幅度的剪枝对模型可解释性的影响，发现适度剪枝有益，过度剪枝破坏可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有工作表明神经网络可大幅剪枝且保持性能，但剪枝对模型可解释性的影响尚不清楚，因此开展研究。

Method: 使用在ImageNette上训练的ResNet - 18，比较不同剪枝水平下Vanilla Gradients和Integrated Gradients的事后解释，评估稀疏性和忠实性，应用CRAFT概念提取跟踪概念语义连贯性的变化。

Result: 适度剪枝提高了显著性图的聚焦性和忠实性，保留了有意义的概念；过度剪枝合并异质特征，降低了显著性图稀疏性和概念连贯性，但仍保持了准确性。

Conclusion: 剪枝可使内部表示向更符合人类的注意力模式转变，但过度剪枝会破坏可解释性。

Abstract: Prior works have shown that neural networks can be heavily pruned while
preserving performance, but the impact of pruning on model interpretability
remains unclear. In this work, we investigate how magnitude-based pruning
followed by fine-tuning affects both low-level saliency maps and high-level
concept representations. Using a ResNet-18 trained on ImageNette, we compare
post-hoc explanations from Vanilla Gradients (VG) and Integrated Gradients (IG)
across pruning levels, evaluating sparsity and faithfulness. We further apply
CRAFT-based concept extraction to track changes in semantic coherence of
learned concepts. Our results show that light-to-moderate pruning improves
saliency-map focus and faithfulness while retaining distinct, semantically
meaningful concepts. In contrast, aggressive pruning merges heterogeneous
features, reducing saliency map sparsity and concept coherence despite
maintaining accuracy. These findings suggest that while pruning can shape
internal representations toward more human-aligned attention patterns,
excessive pruning undermines interpretability.

</details>


### [443] [Large AI Model-Enabled Generative Semantic Communications for Image Transmission](https://arxiv.org/abs/2509.21394)
*Qiyu Ma,Wanli Ni,Zhijin Qin*

Main category: cs.CV

TL;DR: 提出创新生成语义通信系统，分关键与非关键区域处理图像，用轻量级部署策略，模拟显示优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略图像不同区域重要性差异，影响视觉关键内容重建质量。

Method: 将图像分割为关键和非关键区域细化语义粒度，关键区域用面向图像语义编码器处理，非关键区域用图像到文本建模方法压缩；采用结合模型量化和低秩自适应微调技术的轻量级部署策略。

Result: 模拟结果显示，该系统在语义保真度和视觉质量上优于传统方法。

Conclusion: 该系统对图像传输任务有效。

Abstract: The rapid development of generative artificial intelligence (AI) has
introduced significant opportunities for enhancing the efficiency and accuracy
of image transmission within semantic communication systems. Despite these
advancements, existing methodologies often neglect the difference in importance
of different regions of the image, potentially compromising the reconstruction
quality of visually critical content. To address this issue, we introduce an
innovative generative semantic communication system that refines semantic
granularity by segmenting images into key and non-key regions. Key regions,
which contain essential visual information, are processed using an image
oriented semantic encoder, while non-key regions are efficiently compressed
through an image-to-text modeling approach. Additionally, to mitigate the
substantial storage and computational demands posed by large AI models, the
proposed system employs a lightweight deployment strategy incorporating model
quantization and low-rank adaptation fine-tuning techniques, significantly
boosting resource utilization without sacrificing performance. Simulation
results demonstrate that the proposed system outperforms traditional methods in
terms of both semantic fidelity and visual quality, thereby affirming its
effectiveness for image transmission tasks.

</details>


### [444] [DyME: Dynamic Multi-Concept Erasure in Diffusion Models with Bi-Level Orthogonal LoRA Adaptation](https://arxiv.org/abs/2509.21433)
*Jiaqi Liu,Lan Zhang,Xiaoyong Yuan*

Main category: cs.CV

TL;DR: 文本到图像扩散模型存在版权和伦理问题，现有概念擦除方法难以适应实际场景。本文提出DyME框架，引入双级正交约束，开发新基准，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法无法适应实际场景，难以处理多个可能冲突的概念擦除需求，且静态擦除设计与实际使用不匹配。

Method: 提出DyME框架，训练轻量级、特定概念的LoRA适配器并在推理时动态组合；引入双级正交约束；开发新的分层基准ErasureBench - H。

Result: 在ErasureBench - H和标准数据集上的实验表明，DyME始终优于现有基线方法，实现了更高的多概念擦除保真度，同时减少了附带损害。

Conclusion: DyME框架有效解决了现有概念擦除方法的问题，在多概念擦除任务中表现出色。

Abstract: Text-to-image diffusion models (DMs) inadvertently reproduce copyrighted
styles and protected visual concepts, raising legal and ethical concerns.
Concept erasure has emerged as a safeguard, aiming to selectively suppress such
concepts through fine-tuning. However, existing methods do not scale to
practical settings where providers must erase multiple and possibly conflicting
concepts. The core bottleneck is their reliance on static erasure: a single
checkpoint is fine-tuned to remove all target concepts, regardless of the
actual erasure needs at inference. This rigid design mismatches real-world
usage, where requests vary per generation, leading to degraded erasure success
and reduced fidelity for non-target content. We propose DyME, an on-demand
erasure framework that trains lightweight, concept-specific LoRA adapters and
dynamically composes only those needed at inference. This modular design
enables flexible multi-concept erasure, but naive composition causes
interference among adapters, especially when many or semantically related
concepts are suppressed. To overcome this, we introduce bi-level orthogonality
constraints at both the feature and parameter levels, disentangling
representation shifts and enforcing orthogonal adapter subspaces. We further
develop ErasureBench-H, a new hierarchical benchmark with
brand-series-character structure, enabling principled evaluation across
semantic granularities and erasure set sizes. Experiments on ErasureBench-H and
standard datasets (e.g., CIFAR-100, Imagenette) demonstrate that DyME
consistently outperforms state-of-the-art baselines, achieving higher
multi-concept erasure fidelity with minimal collateral degradation.

</details>


### [445] [Gender Stereotypes in Professional Roles Among Saudis: An Analytical Study of AI-Generated Images Using Language Models](https://arxiv.org/abs/2509.21466)
*Khaloud S. AlKhalifah,Malak Mashaabi,Hend Al-Khalifa*

Main category: cs.CV

TL;DR: 研究当代文本到图像AI模型生成沙特专业人士图像时的性别刻板印象和文化不准确问题，分析三种模型产出图像，发现性别失衡和文化不准确，呼吁改进。


<details>
  <summary>Details</summary>
Motivation: 探究当代文本到图像AI模型在生成沙特专业人士图像时是否存在性别刻板印象和文化不准确问题。

Method: 分析ImageFX、DALL - E V3和Grok为56种沙特职业生成的1006张图像，由两名沙特注释者从五个维度评估，意见不一致时由高级研究员裁决。

Result: 三种模型存在强烈性别失衡，DALL - E V3最严重，在领导和技术角色中更明显，且都存在文化不准确问题。

Conclusion: 当前模型反映了训练数据中的社会偏见，需更多样训练数据、更公平算法和文化敏感评估框架。

Abstract: This study investigates the extent to which contemporary Text-to-Image
artificial intelligence (AI) models perpetuate gender stereotypes and cultural
inaccuracies when generating depictions of professionals in Saudi Arabia. We
analyzed 1,006 images produced by ImageFX, DALL-E V3, and Grok for 56 diverse
Saudi professions using neutral prompts. Two trained Saudi annotators evaluated
each image on five dimensions: perceived gender, clothing and appearance,
background and setting, activities and interactions, and age. A third senior
researcher adjudicated whenever the two primary raters disagreed, yielding
10,100 individual judgements. The results reveal a strong gender imbalance,
with ImageFX outputs being 85\% male, Grok 86.6\% male, and DALL-E V3 96\%
male, indicating that DALL-E V3 exhibited the strongest overall gender
stereotyping. This imbalance was most evident in leadership and technical
roles. Moreover, cultural inaccuracies in clothing, settings, and depicted
activities were frequently observed across all three models.
Counter-stereotypical images often arise from cultural misinterpretations
rather than genuinely progressive portrayals. We conclude that current models
mirror societal biases embedded in their training data, generated by humans,
offering only a limited reflection of the Saudi labour market's gender dynamics
and cultural nuances. These findings underscore the urgent need for more
diverse training data, fairer algorithms, and culturally sensitive evaluation
frameworks to ensure equitable and authentic visual outputs.

</details>


### [446] [No Alignment Needed for Generation: Learning Linearly Separable Representations in Diffusion Models](https://arxiv.org/abs/2509.21565)
*Junno Yun,Yaşar Utku Alçalar,Mehmet Akçakaya*

Main category: cs.CV

TL;DR: 本文提出基于促进中间层表示线性可分性（LSEP）的训练正则化方法，在训练效率和生成质量上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于表示对齐的方法依赖大型预训练编码器，计算成本高，需寻找替代训练策略。

Method: 提出基于促进中间层表示线性可分性（LSEP）的训练正则化方法，将线性探测融入网络学习动态。

Result: 在基于流的变压器架构（如SiTs）上，训练效率和生成质量有显著提升，在256×256 ImageNet数据集上FID达到1.46。

Conclusion: LSEP方法有效，无需辅助编码器和表示对齐，能提高训练效率和生成质量。

Abstract: Efficient training strategies for large-scale diffusion models have recently
emphasized the importance of improving discriminative feature representations
in these models. A central line of work in this direction is representation
alignment with features obtained from powerful external encoders, which
improves the representation quality as assessed through linear probing.
Alignment-based approaches show promise but depend on large pretrained
encoders, which are computationally expensive to obtain. In this work, we
propose an alternative regularization for training, based on promoting the
Linear SEParability (LSEP) of intermediate layer representations. LSEP
eliminates the need for an auxiliary encoder and representation alignment,
while incorporating linear probing directly into the network's learning
dynamics rather than treating it as a simple post-hoc evaluation tool. Our
results demonstrate substantial improvements in both training efficiency and
generation quality on flow-based transformer architectures such as SiTs,
achieving an FID of 1.46 on $256 \times 256$ ImageNet dataset.

</details>


### [447] [Enhancing Contrastive Learning for Geolocalization by Discovering Hard Negatives on Semivariograms](https://arxiv.org/abs/2509.21573)
*Boyi Chen,Zhangyu Wang,Fabian Deuser,Johann Maximilian Zollner,Martin Werner*

Main category: cs.CV

TL;DR: 提出空间正则化对比学习策略，结合半变异函数改进基于图像的地理定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法忽略地理空间的空间依赖性，无法解决误判负样本和区分难负样本问题。

Method: 提出集成半变异函数的空间正则化对比学习策略，通过关联特征空间和地理距离拟合半变异函数，以识别难负样本和误判负样本。

Result: 将策略集成到GeoCLIP并在OSV5M数据集上评估，显示明确建模空间先验能提升地理定位性能。

Conclusion: 明确建模空间先验可提升基于图像的地理定位性能，尤其在更细粒度上。

Abstract: Accurate and robust image-based geo-localization at a global scale is
challenging due to diverse environments, visually ambiguous scenes, and the
lack of distinctive landmarks in many regions. While contrastive learning
methods show promising performance by aligning features between street-view
images and corresponding locations, they neglect the underlying spatial
dependency in the geographic space. As a result, they fail to address the issue
of false negatives -- image pairs that are both visually and geographically
similar but labeled as negatives, and struggle to effectively distinguish hard
negatives, which are visually similar but geographically distant. To address
this issue, we propose a novel spatially regularized contrastive learning
strategy that integrates a semivariogram, which is a geostatistical tool for
modeling how spatial correlation changes with distance. We fit the
semivariogram by relating the distance of images in feature space to their
geographical distance, capturing the expected visual content in a spatial
correlation. With the fitted semivariogram, we define the expected visual
dissimilarity at a given spatial distance as reference to identify hard
negatives and false negatives. We integrate this strategy into GeoCLIP and
evaluate it on the OSV5M dataset, demonstrating that explicitly modeling
spatial priors improves image-based geo-localization performance, particularly
at finer granularity.

</details>


### [448] [What Happens Next? Anticipating Future Motion by Generating Point Trajectories](https://arxiv.org/abs/2509.21592)
*Gabrijel Boduljak,Laurynas Karazija,Iro Laina,Christian Rupprecht,Andrea Vedaldi*

Main category: cs.CV

TL;DR: 提出从单张图像预测物体运动的方法，比现有方法更准确多样，还指出当前视频生成器在该任务上的局限。


<details>
  <summary>Details</summary>
Motivation: 解决从单张图像预测物体运动，且不依赖物体速度、受力等参数的问题。

Method: 将任务构建为密集轨迹网格的条件生成，采用类似现代视频生成器架构但输出运动轨迹。

Result: 在模拟数据上广泛评估，在机器人等下游应用中有效，在真实直觉物理数据集上有不错准确率；发现现有视频生成器在单图像运动预测上有局限。

Conclusion: 现有视频生成器因生成像素而非直接建模运动，在单图像运动预测任务上存在局限，本文方法更优。

Abstract: We consider the problem of forecasting motion from a single image, i.e.,
predicting how objects in the world are likely to move, without the ability to
observe other parameters such as the object velocities or the forces applied to
them. We formulate this task as conditional generation of dense trajectory
grids with a model that closely follows the architecture of modern video
generators but outputs motion trajectories instead of pixels. This approach
captures scene-wide dynamics and uncertainty, yielding more accurate and
diverse predictions than prior regressors and generators. We extensively
evaluate our method on simulated data, demonstrate its effectiveness on
downstream applications such as robotics, and show promising accuracy on
real-world intuitive physics datasets. Although recent state-of-the-art video
generators are often regarded as world models, we show that they struggle with
forecasting motion from a single image, even in simple physical scenarios such
as falling blocks or mechanical object interactions, despite fine-tuning on
such data. We show that this limitation arises from the overhead of generating
pixels rather than directly modeling motion.

</details>


### [449] [Temporal vs. Spatial: Comparing DINOv3 and V-JEPA2 Feature Representations for Video Action Analysis](https://arxiv.org/abs/2509.21595)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 本文对比分析DINOv3和V - JEPA2两种自监督学习架构用于视频动作识别的表现，发现架构各有优劣，为视频分析系统设计和特征提取方法选择提供指导。


<details>
  <summary>Details</summary>
Motivation: 探究不同自监督学习架构在视频动作识别中的表现，为视频分析系统的架构设计和特征提取方法选择提供依据。

Method: 在UCF Sports数据集上，从分类准确率、聚类性能等多维度评估DINOv3和V - JEPA2两种架构。

Result: DINOv3聚类和特定动作判别能力强，适合静态姿势识别；V - JEPA2性能方差小，对各类动作表现均衡。

Conclusion: 研究结果有助于理解视频分析系统架构设计选择，并为基于任务需求和可靠性约束选择特征提取方法提供实证指导。

Abstract: This study presents a comprehensive comparative analysis of two prominent
self-supervised learning architectures for video action recognition: DINOv3,
which processes frames independently through spatial feature extraction, and
V-JEPA2, which employs joint temporal modeling across video sequences. We
evaluate both approaches on the UCF Sports dataset, examining feature quality
through multiple dimensions including classification accuracy, clustering
performance, intra-class consistency, and inter-class discrimination. Our
analysis reveals fundamental architectural trade-offs: DINOv3 achieves superior
clustering performance (Silhouette score: 0.31 vs 0.21) and demonstrates
exceptional discrimination capability (6.16x separation ratio) particularly for
pose-identifiable actions, while V-JEPA2 exhibits consistent reliability across
all action types with significantly lower performance variance (0.094 vs
0.288). Through action-specific evaluation, we identify that DINOv3's spatial
processing architecture excels at static pose recognition but shows degraded
performance on motion-dependent actions, whereas V-JEPA2's temporal modeling
provides balanced representation quality across diverse action categories.
These findings contribute to the understanding of architectural design choices
in video analysis systems and provide empirical guidance for selecting
appropriate feature extraction methods based on task requirements and
reliability constraints.

</details>


### [450] [A Data-driven Typology of Vision Models from Integrated Representational Metrics](https://arxiv.org/abs/2509.21628)
*Jialin Wu,Shreya Saha,Yiqing Bo,Meenakshi Khosla*

Main category: cs.CV

TL;DR: 本文使用多种表征相似性指标评估大视觉模型族的可分离性，发现几何和调谐有族特定特征，线性可解码信息更具通用性，用SNF方法融合各指标实现更清晰分离并聚类出模型模式。


<details>
  <summary>Details</summary>
Motivation: 缺乏确定大视觉模型表征哪些方面是各模型族共享、哪些反映独特计算策略的原则性方法。

Method: 利用一套表征相似性指标评估模型族可分离性，采用互补措施，适应SNF方法融合指标。

Result: 保留几何或调谐的指标能强烈区分模型族，线性预测性指标区分较弱；SNF比单个指标实现更清晰的模型族分离，聚类恢复出预期和意外模式。

Conclusion: 该生物学启发的框架为视觉模型提供原则性分类，表明架构和训练目标共同塑造的计算策略定义了超越表面设计类别的表征结构。

Abstract: Large vision models differ widely in architecture and training paradigm, yet
we lack principled methods to determine which aspects of their representations
are shared across families and which reflect distinctive computational
strategies. We leverage a suite of representational similarity metrics, each
capturing a different facet-geometry, unit tuning, or linear decodability-and
assess family separability using multiple complementary measures. Metrics
preserving geometry or tuning (e.g., RSA, Soft Matching) yield strong family
discrimination, whereas flexible mappings such as Linear Predictivity show
weaker separation. These findings indicate that geometry and tuning carry
family-specific signatures, while linearly decodable information is more
broadly shared. To integrate these complementary facets, we adapt Similarity
Network Fusion (SNF), a method inspired by multi-omics integration. SNF
achieves substantially sharper family separation than any individual metric and
produces robust composite signatures. Clustering of the fused similarity matrix
recovers both expected and surprising patterns: supervised ResNets and ViTs
form distinct clusters, yet all self-supervised models group together across
architectural boundaries. Hybrid architectures (ConvNeXt, Swin) cluster with
masked autoencoders, suggesting convergence between architectural modernization
and reconstruction-based training. This biology-inspired framework provides a
principled typology of vision models, showing that emergent computational
strategies-shaped jointly by architecture and training objective-define
representational structure beyond surface design categories.

</details>


### [451] [MORPH: Shape-agnostic PDE Foundation Models](https://arxiv.org/abs/2509.21670)
*Mahindra Singh Rautela,Alexander Most,Siddharth Mansingh,Bradley C. Love,Ayan Biswas,Diane Oyen,Earl Lawrence*

Main category: cs.CV

TL;DR: 介绍MORPH，一个用于偏微分方程的形状无关自回归基础模型，在多种任务中表现出色，为科学机器学习提供强大骨干。


<details>
  <summary>Details</summary>
Motivation: 开发一个能处理异构时空数据集、不同分辨率和多场的偏微分方程基础模型。

Method: 基于卷积视觉变换器骨干，结合组件卷积、场间交叉注意力和轴向注意力，预训练多种模型变体，采用全模型微调与低秩适配器。

Result: MORPH在零样本和全样本泛化中优于从头训练的模型，匹配或超越强基线和最新模型。

Conclusion: MORPH是学习科学观测异构和多模态性质的灵活强大骨干，为可扩展和数据高效的科学机器学习指明方向。

Abstract: We introduce MORPH, a shape-agnostic, autoregressive foundation model for
partial differential equations (PDEs). MORPH is built on a convolutional vision
transformer backbone that seamlessly handles heterogeneous spatiotemporal
datasets of varying data dimensionality (1D--3D) at different resolutions,
multiple fields with mixed scalar and vector components. The architecture
combines (i) component-wise convolution, which jointly processes scalar and
vector channels to capture local interactions, (ii) inter-field
cross-attention, which models and selectively propagates information between
different physical fields, (iii) axial attentions, which factorizes full
spatiotemporal self-attention along individual spatial and temporal axes to
reduce computational burden while retaining expressivity. We pretrain multiple
model variants on a diverse collection of heterogeneous PDE datasets and
evaluate transfer to a range of downstream prediction tasks. Using both
full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH
outperforms models trained from scratch in both zero-shot and full-shot
generalization. Across extensive evaluations, MORPH matches or surpasses strong
baselines and recent state-of-the-art models. Collectively, these capabilities
present a flexible and powerful backbone for learning from heterogeneous and
multimodal nature of scientific observations, charting a path toward scalable
and data-efficient scientific machine learning.

</details>


### [452] [UISim: An Interactive Image-Based UI Simulator for Dynamic Mobile Environments](https://arxiv.org/abs/2509.21733)
*Jiannan Xiang,Yun Zhu,Lei Shu,Maria Wang,Lijun Yu,Gabriel Barcik,James Lyon,Srinivas Sunkara,Jindong Chen*

Main category: cs.CV

TL;DR: 提出图像式UI模拟器UISim，实现UI过渡模拟，用于UI测试等，实验显示其优于基线。


<details>
  <summary>Details</summary>
Motivation: 现实移动环境动态多样，现有UI开发和测试方法依赖实体设备或静态截图分析，阻碍可扩展性和智能UI代理开发。

Method: 采用两阶段方法，先预测下一UI状态抽象布局，再基于布局合成视觉一致新图像。

Result: UISim在生成逼真连贯后续UI状态上优于端到端UI生成基线。

Conclusion: UISim保真度高，可简化UI开发，增强AI代理训练。

Abstract: Developing and testing user interfaces (UIs) and training AI agents to
interact with them are challenging due to the dynamic and diverse nature of
real-world mobile environments. Existing methods often rely on cumbersome
physical devices or limited static analysis of screenshots, which hinders
scalable testing and the development of intelligent UI agents. We introduce
UISim, a novel image-based UI simulator that offers a dynamic and interactive
platform for exploring mobile phone environments purely from screen images. Our
system employs a two-stage method: given an initial phone screen image and a
user action, it first predicts the abstract layout of the next UI state, then
synthesizes a new, visually consistent image based on this predicted layout.
This approach enables the realistic simulation of UI transitions. UISim
provides immediate practical benefits for UI testing, rapid prototyping, and
synthetic data generation. Furthermore, its interactive capabilities pave the
way for advanced applications, such as UI navigation task planning for AI
agents. Our experimental results show that UISim outperforms end-to-end UI
generation baselines in generating realistic and coherent subsequent UI states,
highlighting its fidelity and potential to streamline UI development and
enhance AI agent training.

</details>


### [453] [LFA-Net: A Lightweight Network with LiteFusion Attention for Retinal Vessel Segmentation](https://arxiv.org/abs/2509.21738)
*Mehwish Mehmood,Ivor Spence,Muhammad Fahim*

Main category: cs.CV

TL;DR: 提出轻量级视网膜血管分割网络LFA - Net，参数少、计算量低，在多数据集上验证效果好，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习视网膜血管分割模型存在小血管分割和高计算成本的问题，在计算资源有限的临床环境中需要轻量级模型。

Method: 提出LFA - Net网络，其包含新设计的注意力模块LiteFusion - Attention，结合残差学习连接、受Vision Mamba启发的动态和基于调制的注意力。

Result: LFA - Net参数0.11百万，内存0.42 MB，计算量4.46 GFLOPs，在DRIVE、STARE和CHASE_DB数据集上，dice分数分别为83.28%、87.44%、84.50%，Jaccard指数分别为72.85%、79.31%、74.70%。

Conclusion: LFA - Net适用于资源受限环境，有优秀的血管分割性能。

Abstract: Lightweight retinal vessel segmentation is important for the early diagnosis
of vision-threatening and systemic diseases, especially in a real-world
clinical environment with limited computational resources. Although
segmentation methods based on deep learning are improving, existing models are
still facing challenges of small vessel segmentation and high computational
costs. To address these challenges, we proposed a new vascular segmentation
network, LFA-Net, which incorporates a newly designed attention module,
LiteFusion-Attention. This attention module incorporates residual learning
connections, Vision Mamba-inspired dynamics, and modulation-based attention,
enabling the model to capture local and global context efficiently and in a
lightweight manner. LFA-Net offers high performance with 0.11 million
parameters, 0.42 MB memory size, and 4.46 GFLOPs, which make it ideal for
resource-constrained environments. We validated our proposed model on DRIVE,
STARE, and CHASE_DB with outstanding performance in terms of dice scores of
83.28, 87.44, and 84.50% and Jaccard indices of 72.85, 79.31, and 74.70%,
respectively. The code of LFA-Net is available online
https://github.com/Mehwish4593/LFA-Net.

</details>


### [454] [DiTraj: training-free trajectory control for video diffusion transformer](https://arxiv.org/abs/2509.21839)
*Cheng Lei,Jiayu Zhang,Yue Ma,Xinyu Wang,Long Chen,Liang Tang,Yiqiang Yan,Fei Su,Zhicheng Zhao*

Main category: cs.CV

TL;DR: 提出针对DiT的免训练文本到视频生成轨迹控制框架DiTraj，实验表明其在视频质量和轨迹可控性上优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹控制方法需大量训练资源或专为U - Net设计，未利用DiT优势。

Method: 提出前景背景分离引导，用LLM将提示转换为前后景提示；提出帧间时空解耦3D - RoPE（STD - RoPE），修改前景标记位置嵌入；通过调节位置嵌入密度实现3D感知轨迹控制。

Result: 实验显示该方法在视频质量和轨迹可控性上优于先前方法。

Conclusion: 所提DiTraj框架简单有效，能解决现有方法的问题，提升轨迹控制效果。

Abstract: Diffusion Transformers (DiT)-based video generation models with 3D full
attention exhibit strong generative capabilities. Trajectory control represents
a user-friendly task in the field of controllable video generation. However,
existing methods either require substantial training resources or are
specifically designed for U-Net, do not take advantage of the superior
performance of DiT. To address these issues, we propose DiTraj, a simple but
effective training-free framework for trajectory control in text-to-video
generation, tailored for DiT. Specifically, first, to inject the object's
trajectory, we propose foreground-background separation guidance: we use the
Large Language Model (LLM) to convert user-provided prompts into foreground and
background prompts, which respectively guide the generation of foreground and
background regions in the video. Then, we analyze 3D full attention and explore
the tight correlation between inter-token attention scores and position
embedding. Based on this, we propose inter-frame Spatial-Temporal Decoupled
3D-RoPE (STD-RoPE). By modifying only foreground tokens' position embedding,
STD-RoPE eliminates their cross-frame spatial discrepancies, strengthening
cross-frame attention among them and thus enhancing trajectory control.
Additionally, we achieve 3D-aware trajectory control by regulating the density
of position embedding. Extensive experiments demonstrate that our method
outperforms previous methods in both video quality and trajectory
controllability.

</details>


### [455] [Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with Relative-Absolute Policy Optimization](https://arxiv.org/abs/2509.21871)
*Boyang Liu,Yifan Hu,Senjie Jin,Shihan Dou,Gonglei Shi,Jie Shao,Tao Gui,Xuanjing Huang*

Main category: cs.CV

TL;DR: 提出Aes - R1框架用于图像美学评估，提升MLLMs美学评分和推理能力，实验效果超同类基线。


<details>
  <summary>Details</summary>
Motivation: 多模态美学推理数据稀缺和美学判断主观性使MLLMs难生成准确可解释美学判断，需改进。

Method: 提出含强化学习的Aes - R1框架，集成AesCoT构建过滤数据，用RAPO算法优化。

Result: Aes - R1使骨干模型平均PLCC/SRCC提升47.9%/34.8%，超同类基线，消融研究验证泛化性。

Conclusion: Aes - R1能增强MLLMs美学评分和推理能力，在统一框架下有效工作。

Abstract: Multimodal large language models (MLLMs) are well suited to image aesthetic
assessment, as they can capture high-level aesthetic features leveraging their
cross-modal understanding capacity. However, the scarcity of multimodal
aesthetic reasoning data and the inherently subjective nature of aesthetic
judgment make it difficult for MLLMs to generate accurate aesthetic judgments
with interpretable rationales. To this end, we propose Aes-R1, a comprehensive
aesthetic reasoning framework with reinforcement learning (RL). Concretely,
Aes-R1 integrates a pipeline, AesCoT, to construct and filter high-quality
chain-of-thought aesthetic reasoning data used for cold-start. After teaching
the model to generate structured explanations prior to scoring, we then employ
the Relative-Absolute Policy Optimization (RAPO), a novel RL algorithm that
jointly optimizes absolute score regression and relative ranking order,
improving both per-image accuracy and cross-image preference judgments. Aes-R1
enables MLLMs to generate grounded explanations alongside faithful scores,
thereby enhancing aesthetic scoring and reasoning in a unified framework.
Extensive experiments demonstrate that Aes-R1 improves the backbone's average
PLCC/SRCC by 47.9%/34.8%, surpassing state-of-the-art baselines of similar
size. More ablation studies validate Aes-R1's robust generalization under
limited supervision and in out-of-distribution scenarios.

</details>


### [456] [SemanticControl: A Training-Free Approach for Handling Loosely Aligned Visual Conditions in ControlNet](https://arxiv.org/abs/2509.21938)
*Woosung Joung,Daewon Chae,Jinkyu Kim*

Main category: cs.CV

TL;DR: 本文提出无训练方法SemanticControl，利用语义相关但未对齐的视觉条件，实验显示其在多种条件下优于现有基线。


<details>
  <summary>Details</summary>
Motivation: ControlNet依赖与文本提示精确对齐的视觉条件，实际中常难以满足，现有模型难以利用未对齐但语义相关的视觉条件。

Method: 先使用与视觉条件对齐的替代提示进行辅助去噪，提取注意力掩码，再在实际目标提示去噪时利用这些掩码。

Result: 实验表明该方法在深度图、边缘图和人体骨骼等多种未对齐条件下性能提升，优于现有基线。

Conclusion: SemanticControl能有效利用未对齐但语义相关的视觉条件，提高文本到图像扩散模型性能。

Abstract: ControlNet has enabled detailed spatial control in text-to-image diffusion
models by incorporating additional visual conditions such as depth or edge
maps. However, its effectiveness heavily depends on the availability of visual
conditions that are precisely aligned with the generation goal specified by
text prompt-a requirement that often fails in practice, especially for uncommon
or imaginative scenes. For example, generating an image of a cat cooking in a
specific pose may be infeasible due to the lack of suitable visual conditions.
In contrast, structurally similar cues can often be found in more common
settings-for instance, poses of humans cooking are widely available and can
serve as rough visual guides. Unfortunately, existing ControlNet models
struggle to use such loosely aligned visual conditions, often resulting in low
text fidelity or visual artifacts. To address this limitation, we propose
SemanticControl, a training-free method for effectively leveraging misaligned
but semantically relevant visual conditions. Our approach adaptively suppresses
the influence of the visual condition where it conflicts with the prompt, while
strengthening guidance from the text. The key idea is to first run an auxiliary
denoising process using a surrogate prompt aligned with the visual condition
(e.g., "a human playing guitar" for a human pose condition) to extract
informative attention masks, and then utilize these masks during the denoising
of the actual target prompt (e.g., cat playing guitar). Experimental results
demonstrate that our method improves performance under loosely aligned
conditions across various conditions, including depth maps, edge maps, and
human skeletons, outperforming existing baselines. Our code is available at
https://mung3477.github.io/semantic-control.

</details>


### [457] [No-Reference Image Contrast Assessment with Customized EfficientNet-B0](https://arxiv.org/abs/2509.21967)
*Javad Hassannataj Joloudari,Bita Mesbahzadeh,Omid Zare,Emrah Arslan,Roohallah Alizadehsani,Hossein Moosaei*

Main category: cs.CV

TL;DR: 提出基于深度学习的无参考图像对比度质量评估框架，定制微调三种预训练架构，EfficientNet B0表现最佳，证明轻量级预训练网络适配对比度评估的有效性。


<details>
  <summary>Details</summary>
Motivation: 多数无参考图像质量评估模型难以在多样现实条件下准确评估对比度失真。

Method: 定制微调EfficientNet B0、ResNet18和MobileNetV2三种预训练架构，构建基于孪生网络的模型，用对比度感知回归头修改模型，在两个基准数据集上进行端到端训练，用PLCC和SRCC评估性能。

Result: 定制的EfficientNet B0模型在CCID2014和CID2013数据集上取得了最先进的性能，超越传统方法和其他深度基线。

Conclusion: 对比度感知的轻量级预训练网络适配可为无参考对比度质量评估提供高性能、可扩展的解决方案，适用于实时和资源受限的应用。

Abstract: Image contrast was a fundamental factor in visual perception and played a
vital role in overall image quality. However, most no reference image quality
assessment NR IQA models struggled to accurately evaluate contrast distortions
under diverse real world conditions. In this study, we proposed a deep learning
based framework for blind contrast quality assessment by customizing and
fine-tuning three pre trained architectures, EfficientNet B0, ResNet18, and
MobileNetV2, for perceptual Mean Opinion Score, along with an additional model
built on a Siamese network, which indicated a limited ability to capture
perceptual contrast distortions. Each model is modified with a contrast-aware
regression head and trained end to end using targeted data augmentations on two
benchmark datasets, CID2013 and CCID2014, containing synthetic and authentic
contrast distortions. Performance is evaluated using Pearson Linear Correlation
Coefficient and Spearman Rank Order Correlation Coefficient, which assess the
alignment between predicted and human rated scores. Among these three models,
our customized EfficientNet B0 model achieved state-of-the-art performance with
PLCC = 0.9286 and SRCC = 0.9178 on CCID2014 and PLCC = 0.9581 and SRCC = 0.9369
on CID2013, surpassing traditional methods and outperforming other deep
baselines. These results highlighted the models robustness and effectiveness in
capturing perceptual contrast distortion. Overall, the proposed method
demonstrated that contrast aware adaptation of lightweight pre trained networks
can yield a high performing, scalable solution for no reference contrast
quality assessment suitable for real time and resource constrained
applications.

</details>


### [458] [Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding with Reinforcement Fine-Tuning](https://arxiv.org/abs/2509.21976)
*Zilun Zhang,Zian Guan,Tiancheng Zhao,Haozhan Shen,Tianyu Li,Yuxiang Cai,Zhonggen Su,Zhaojun Liu,Jianwei Yin,Xiang Li*

Main category: cs.CV

TL;DR: 提出Geo - R1用于少样本地理空间指代表达理解，在多个基准测试中表现优于监督微调基线，且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 监督微调在数据稀缺场景下泛化能力差，难以应对遥感中指代表达理解的挑战。

Method: 提出Geo - R1，采用以推理为中心的强化微调范式，先生成可解释的推理链分解指代表达，再据此定位目标对象。

Result: 在三个精心设计的少样本地理空间指代表达基准测试中，Geo - R1始终大幅优于监督微调基线，且跨数据集泛化能力强。

Conclusion: Geo - R1有效解决了数据稀缺场景下的指代表达理解问题，提高了模型泛化能力和可解释性。

Abstract: Referring expression understanding in remote sensing poses unique challenges,
as it requires reasoning over complex object-context relationships. While
supervised fine-tuning (SFT) on multimodal large language models achieves
strong performance with massive labeled datasets, they struggle in data-scarce
scenarios, leading to poor generalization. To address this limitation, we
propose Geo-R1, a reasoning-centric reinforcement fine-tuning (RFT) paradigm
for few-shot geospatial referring. Geo-R1 enforces the model to first generate
explicit, interpretable reasoning chains that decompose referring expressions,
and then leverage these rationales to localize target objects. This "reason
first, then act" process enables the model to make more effective use of
limited annotations, enhances generalization, and provides interpretability. We
validate Geo-R1 on three carefully designed few-shot geospatial referring
benchmarks, where our model consistently and substantially outperforms SFT
baselines. It also demonstrates strong cross-dataset generalization,
highlighting its robustness. Code and data will be released at
http://geo-r1.github.io.

</details>


### [459] [Benchmarking and Mitigate Psychological Sycophancy in Medical Vision-Language Models](https://arxiv.org/abs/2509.21979)
*Zikun Guo,Xinyue Xu,Pei Xiang,Shu Yang,Xin Han,Di Wang,Lijie Hu*

Main category: cs.CV

TL;DR: 研究评估医学视觉问答中临床盲从行为，提出数据集和缓解策略VIPER，为医学VLMs部署奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在临床工作流中常出现盲从行为，优先考虑与用户表述等的一致性而非基于证据推理，需评估和解决该问题。

Method: 通过新的临床基准评估，构建医学盲从数据集，使用心理压力模板进行对抗实验。

Result: 模型普遍易受影响，对抗响应出现情况差异大，与模型准确性或大小弱相关，模仿和专家修正触发效果最佳。

Conclusion: 提出的VIPER策略可减少盲从，优于基线且保持可解释性，强调基于证据防御的必要性。

Abstract: Vision language models(VLMs) are increasingly integrated into clinical
workflows, but they often exhibit sycophantic behavior prioritizing alignment
with user phrasing social cues or perceived authority over evidence based
reasoning. This study evaluate clinical sycophancy in medical visual question
answering through a novel clinically grounded benchmark. We propose a medical
sycophancy dataset construct from PathVQA, SLAKE, and VQA-RAD stratified by
different type organ system and modality. Using psychologically motivated
pressure templates including various sycophancy. In our adversarial experiments
on various VLMs, we found that these models are generally vulnerable,
exhibiting significant variations in the occurrence of adversarial responses,
with weak correlations to the model accuracy or size. Imitation and expert
provided corrections were found to be the most effective triggers, suggesting
that the models possess a bias mechanism independent of visual evidence. To
address this, we propose Visual Information Purification for Evidence based
Response (VIPER) a lightweight mitigation strategy that filters non evidentiary
content for example social pressures and then generates constrained evidence
first answers. This framework reduces sycophancy by an average amount
outperforming baselines while maintaining interpretability. Our benchmark
analysis and mitigation framework lay the groundwork for robust deployment of
medical VLMs in real world clinician interactions emphasizing the need for
evidence anchored defenses.

</details>


### [460] [ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models](https://arxiv.org/abs/2509.21991)
*Jewon Lee,Wooksu Shin,Seungmin Yang,Ki-Ung Song,DongUk Lim,Jaeyeon Kim,Tae-Ho Kim,Bo-Kyeong Kim*

Main category: cs.CV

TL;DR: 提出 ERGO 模型，采用两阶段‘粗到细’推理管道，在多数据集上比原模型和竞争方法更准确高效。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型处理高分辨率图像计算开销大，且确定查询相关区域有挑战。

Method: 提出两阶段‘粗到细’推理管道，提出 ERGO 模型，利用多模态上下文进行推理驱动的感知，在强化学习框架中开发奖励组件。

Result: 在多个数据集上比原模型和竞争方法准确率更高、效率更高，如在 V* 基准上超过 Qwen2.5 - VL - 7B 4.7 点，仅用 23%视觉令牌，推理速度提升 3 倍。

Conclusion: ERGO 模型能有效处理高分辨率图像，减少计算成本，提高推理效率和准确性。

Abstract: Efficient processing of high-resolution images is crucial for real-world
vision-language applications. However, existing Large Vision-Language Models
(LVLMs) incur substantial computational overhead due to the large number of
vision tokens. With the advent of "thinking with images" models, reasoning now
extends beyond text to the visual domain. This capability motivates our
two-stage "coarse-to-fine" reasoning pipeline: first, a downsampled image is
analyzed to identify task-relevant regions; then, only these regions are
cropped at full resolution and processed in a subsequent reasoning stage. This
approach reduces computational cost while preserving fine-grained visual
details where necessary. A major challenge lies in inferring which regions are
truly relevant to a given query. Recent related methods often fail in the first
stage after input-image downsampling, due to perception-driven reasoning, where
clear visual information is required for effective reasoning. To address this
issue, we propose ERGO (Efficient Reasoning & Guided Observation) that performs
reasoning-driven perception-leveraging multimodal context to determine where to
focus. Our model can account for perceptual uncertainty, expanding the cropped
region to cover visually ambiguous areas for answering questions. To this end,
we develop simple yet effective reward components in a reinforcement learning
framework for coarse-to-fine perception. Across multiple datasets, our approach
delivers higher accuracy than the original model and competitive methods, with
greater efficiency. For instance, ERGO surpasses Qwen2.5-VL-7B on the V*
benchmark by 4.7 points while using only 23% of the vision tokens, achieving a
3x inference speedup. The code and models can be found at:
https://github.com/nota-github/ERGO.

</details>


### [461] [Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics](https://arxiv.org/abs/2509.22014)
*Saurav Jha,Stefan K. Ehrlich*

Main category: cs.CV

TL;DR: 提出轻量级多模态框架用于视频场景理解，在评估中表现良好，有医疗应用潜力。


<details>
  <summary>Details</summary>
Motivation: 医疗机器人需强大多模态感知和推理能力，现有视觉语言模型在时间推理、不确定性估计和结构化输出方面有限。

Method: 结合Qwen2.5 - VL - 3B - Instruct模型与基于SmolAgent的编排层，支持思维链推理、语音视觉融合和动态工具调用，生成结构化场景图并利用混合检索模块。

Result: 在Video - MME基准和自定义临床数据集评估中，与现有先进视觉语言模型相比，有竞争力的准确性和更好的鲁棒性。

Conclusion: 该框架在机器人辅助手术、患者监测和决策支持等应用中有潜力。

Abstract: Healthcare robotics requires robust multimodal perception and reasoning to
ensure safety in dynamic clinical environments. Current Vision-Language Models
(VLMs) demonstrate strong general-purpose capabilities but remain limited in
temporal reasoning, uncertainty estimation, and structured outputs needed for
robotic planning. We present a lightweight agentic multimodal framework for
video-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model
with a SmolAgent-based orchestration layer, it supports chain-of-thought
reasoning, speech-vision fusion, and dynamic tool invocation. The framework
generates structured scene graphs and leverages a hybrid retrieval module for
interpretable and adaptive reasoning. Evaluations on the Video-MME benchmark
and a custom clinical dataset show competitive accuracy and improved robustness
compared to state-of-the-art VLMs, demonstrating its potential for applications
in robot-assisted surgery, patient monitoring, and decision support.

</details>


### [462] [REFINE-CONTROL: A Semi-supervised Distillation Method For Conditional Image Generation](https://arxiv.org/abs/2509.22139)
*Yicheng Jiang,Jin Yuan,Hua Yuan,Yao Zhang,Yong Rui*

Main category: cs.CV

TL;DR: 提出Refine - Control半监督蒸馏框架，降低条件图像生成模型计算成本和延迟，保持高保真生成能力和可控性。


<details>
  <summary>Details</summary>
Motivation: 条件图像生成模型资源需求高、标注数据稀缺，难以在边缘设备部署，存在成本和隐私问题。

Method: 提出Refine - Control框架，引入三级知识融合损失提升学生模型性能，采用半监督蒸馏方法利用有标签和无标签数据。

Result: Refine - Control显著降低计算成本和延迟，保持高保真生成能力和可控性。

Conclusion: Refine - Control能有效解决条件图像生成模型在边缘设备部署的问题。

Abstract: Conditional image generation models have achieved remarkable results by
leveraging text-based control to generate customized images. However, the high
resource demands of these models and the scarcity of well-annotated data have
hindered their deployment on edge devices, leading to enormous costs and
privacy concerns, especially when user data is sent to a third party. To
overcome these challenges, we propose Refine-Control, a semi-supervised
distillation framework. Specifically, we improve the performance of the student
model by introducing a tri-level knowledge fusion loss to transfer different
levels of knowledge. To enhance generalization and alleviate dataset scarcity,
we introduce a semi-supervised distillation method utilizing both labeled and
unlabeled data. Our experiments reveal that Refine-Control achieves significant
reductions in computational cost and latency, while maintaining high-fidelity
generation capabilities and controllability, as quantified by comparative
metrics.

</details>


### [463] [Polysemous Language Gaussian Splatting via Matching-based Mask Lifting](https://arxiv.org/abs/2509.22225)
*Jiayu Ding,Xinpeng Liu,Zhiyi Pan,Shiqiang Long,Ge Li*

Main category: cs.CV

TL;DR: 提出无训练框架MUSplat解决二维开放词汇理解到三维高斯溅射场景的提升问题，减少场景适应时间，表现优于现有训练框架


<details>
  <summary>Details</summary>
Motivation: 主流方法在提升二维开放词汇理解到三维高斯溅射场景时存在依赖高成本逐场景再训练、单义词设计有局限、易出现跨视图语义不一致等问题

Method: 利用预训练二维分割模型生成并提升多粒度二维掩码到三维，为每个高斯点估计前景概率形成初始对象组，用语义熵和几何不透明度优化初始组边界，用视觉语言模型提炼文本特征解决视觉不一致

Result: MUSplat将场景适应时间从数小时减少到数分钟，在开放词汇三维对象选择和语义分割基准任务上表现优于现有训练框架

Conclusion: MUSplat能解决主流方法的局限，实现更好的开放词汇三维场景理解效果

Abstract: Lifting 2D open-vocabulary understanding into 3D Gaussian Splatting (3DGS)
scenes is a critical challenge. However, mainstream methods suffer from three
key flaws: (i) their reliance on costly per-scene retraining prevents
plug-and-play application; (ii) their restrictive monosemous design fails to
represent complex, multi-concept semantics; and (iii) their vulnerability to
cross-view semantic inconsistencies corrupts the final semantic representation.
To overcome these limitations, we introduce MUSplat, a training-free framework
that abandons feature optimization entirely. Leveraging a pre-trained 2D
segmentation model, our pipeline generates and lifts multi-granularity 2D masks
into 3D, where we estimate a foreground probability for each Gaussian point to
form initial object groups. We then optimize the ambiguous boundaries of these
initial groups using semantic entropy and geometric opacity. Subsequently, by
interpreting the object's appearance across its most representative viewpoints,
a Vision-Language Model (VLM) distills robust textual features that reconciles
visual inconsistencies, enabling open-vocabulary querying via semantic
matching. By eliminating the costly per-scene training process, MUSplat reduces
scene adaptation time from hours to mere minutes. On benchmark tasks for
open-vocabulary 3D object selection and semantic segmentation, MUSplat
outperforms established training-based frameworks while simultaneously
addressing their monosemous limitations.

</details>


### [464] [Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks](https://arxiv.org/abs/2509.22258)
*Miao Jing,Mengting Jia,Junling Lin,Zhongxia Shen,Lijun Wang,Yuanyuan Peng,Huan Gao,Mingkun Xu,Shangyang Li*

Main category: cs.CV

TL;DR: 提出Neural - MedBench基准测试来探究多模态临床推理能力，评估现有模型，发现推理失败是主要问题，强调双轴评估框架的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有医学基准测试强调分类准确率，无法评估模型真实临床推理能力，需新基准测试。

Method: 引入Neural - MedBench，整合多源数据，包含三类核心任务，开发混合评分管道。

Result: 对现有模型评估显示性能下降，推理失败是主要不足。

Conclusion: 需要双轴评估框架，发布Neural - MedBench作为开放可扩展测试平台。

Abstract: Recent advances in vision-language models (VLMs) have achieved remarkable
performance on standard medical benchmarks, yet their true clinical reasoning
ability remains unclear. Existing datasets predominantly emphasize
classification accuracy, creating an evaluation illusion in which models appear
proficient while still failing at high-stakes diagnostic reasoning. We
introduce Neural-MedBench, a compact yet reasoning-intensive benchmark
specifically designed to probe the limits of multimodal clinical reasoning in
neurology. Neural-MedBench integrates multi-sequence MRI scans, structured
electronic health records, and clinical notes, and encompasses three core task
families: differential diagnosis, lesion recognition, and rationale generation.
To ensure reliable evaluation, we develop a hybrid scoring pipeline that
combines LLM-based graders, clinician validation, and semantic similarity
metrics. Through systematic evaluation of state-of-the-art VLMs, including
GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to
conventional datasets. Error analysis shows that reasoning failures, rather
than perceptual errors, dominate model shortcomings. Our findings highlight the
necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets
for statistical generalization, and depth-oriented, compact benchmarks such as
Neural-MedBench for reasoning fidelity. We release Neural-MedBench at
https://neuromedbench.github.io/ as an open and extensible diagnostic testbed,
which guides the expansion of future benchmarks and enables rigorous yet
cost-effective assessment of clinically trustworthy AI.

</details>


### [465] [Jailbreaking on Text-to-Video Models via Scene Splitting Strategy](https://arxiv.org/abs/2509.22292)
*Wonjun Lee,Haon Park,Doehyeon Lee,Bumsub Ham,Suhyun Kim*

Main category: cs.CV

TL;DR: 本文提出SceneSplit黑盒越狱方法攻击文本到视频（T2V）模型，评估显示其攻击成功率高，证明T2V安全机制易受攻击。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索T2V模型安全漏洞，存在安全缺口。

Method: 提出SceneSplit方法，将有害叙事拆分为多个场景，利用场景组合约束输出空间，通过迭代场景操作绕过安全过滤，使用策略库提升攻击效果。

Result: 在11个安全类别评估中，SceneSplit在多个T2V模型上攻击成功率高，远超基线。

Conclusion: 当前T2V安全机制易受利用叙事结构的攻击，为T2V模型安全改进提供新见解。

Abstract: Along with the rapid advancement of numerous Text-to-Video (T2V) models,
growing concerns have emerged regarding their safety risks. While recent
studies have explored vulnerabilities in models like LLMs, VLMs, and
Text-to-Image (T2I) models through jailbreak attacks, T2V models remain largely
unexplored, leaving a significant safety gap. To address this gap, we introduce
SceneSplit, a novel black-box jailbreak method that works by fragmenting a
harmful narrative into multiple scenes, each individually benign. This approach
manipulates the generative output space, the abstract set of all potential
video outputs for a given prompt, using the combination of scenes as a powerful
constraint to guide the final outcome. While each scene individually
corresponds to a wide and safe space where most outcomes are benign, their
sequential combination collectively restricts this space, narrowing it to an
unsafe region and significantly increasing the likelihood of generating a
harmful video. This core mechanism is further enhanced through iterative scene
manipulation, which bypasses the safety filter within this constrained unsafe
region. Additionally, a strategy library that reuses successful attack patterns
further improves the attack's overall effectiveness and robustness. To validate
our method, we evaluate SceneSplit across 11 safety categories on T2V models.
Our results show that it achieves a high average Attack Success Rate (ASR) of
77.2% on Luma Ray2, 84.1% on Hailuo, and 78.2% on Veo2, significantly
outperforming the existing baseline. Through this work, we demonstrate that
current T2V safety mechanisms are vulnerable to attacks that exploit narrative
structure, providing new insights for understanding and improving the safety of
T2V models.

</details>


### [466] [HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models](https://arxiv.org/abs/2509.22300)
*Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Main category: cs.CV

TL;DR: 提出历史引导采样（HiGS）技术，可在不增加计算量下提升扩散模型图像生成质量和效率，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在较少神经函数评估次数或较低引导尺度下输出不真实、缺乏细节的问题。

Method: 提出历史引导采样（HiGS）技术，利用当前预测与过去预测加权平均的差异引导采样过程。

Result: HiGS在不同模型、架构、采样预算和引导尺度下均提升图像质量，使用预训练SiT模型在30步采样下实现新的SOTA FID。

Conclusion: HiGS是标准扩散采样的即插即用增强方法，可实现更快、更高保真度的生成。

Abstract: While diffusion models have made remarkable progress in image generation,
their outputs can still appear unrealistic and lack fine details, especially
when using fewer number of neural function evaluations (NFEs) or lower guidance
scales. To address this issue, we propose a novel momentum-based sampling
technique, termed history-guided sampling (HiGS), which enhances quality and
efficiency of diffusion sampling by integrating recent model predictions into
each inference step. Specifically, HiGS leverages the difference between the
current prediction and a weighted average of past predictions to steer the
sampling process toward more realistic outputs with better details and
structure. Our approach introduces practically no additional computation and
integrates seamlessly into existing diffusion frameworks, requiring neither
extra training nor fine-tuning. Extensive experiments show that HiGS
consistently improves image quality across diverse models and architectures and
under varying sampling budgets and guidance scales. Moreover, using a
pretrained SiT model, HiGS achieves a new state-of-the-art FID of 1.61 for
unguided ImageNet generation at 256$\times$256 with only 30 sampling steps
(instead of the standard 250). We thus present HiGS as a plug-and-play
enhancement to standard diffusion sampling that enables faster generation with
higher fidelity.

</details>


### [467] [Pedestrian Attribute Recognition via Hierarchical Cross-Modality HyperGraph Learning](https://arxiv.org/abs/2509.22331)
*Xiao Wang,Shujuan Wu,Xiaoxia Cheng,Changwei Bi,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: 提出构建多模态知识图谱用于行人属性识别，通过知识图谱引导的跨模态超图学习框架增强标准框架，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有行人属性识别算法未能充分利用属性知识和上下文信息，利用属性文本的方法尚不成熟。

Method: 构建多模态知识图谱，挖掘局部视觉特征与文本、属性与视觉上下文样本的关系，引入知识图谱引导的跨模态超图学习框架。

Result: 在多个行人属性识别基准数据集上的综合实验证明了所提知识图谱对行人属性识别任务的有效性。

Conclusion: 为知识引导的行人属性识别奠定了坚实基础。

Abstract: Current Pedestrian Attribute Recognition (PAR) algorithms typically focus on
mapping visual features to semantic labels or attempt to enhance learning by
fusing visual and attribute information. However, these methods fail to fully
exploit attribute knowledge and contextual information for more accurate
recognition. Although recent works have started to consider using attribute
text as additional input to enhance the association between visual and semantic
information, these methods are still in their infancy. To address the above
challenges, this paper proposes the construction of a multi-modal knowledge
graph, which is utilized to mine the relationships between local visual
features and text, as well as the relationships between attributes and
extensive visual context samples. Specifically, we propose an effective
multi-modal knowledge graph construction method that fully considers the
relationships among attributes and the relationships between attributes and
vision tokens. To effectively model these relationships, this paper introduces
a knowledge graph-guided cross-modal hypergraph learning framework to enhance
the standard pedestrian attribute recognition framework. Comprehensive
experiments on multiple PAR benchmark datasets have thoroughly demonstrated the
effectiveness of our proposed knowledge graph for the PAR task, establishing a
strong foundation for knowledge-guided pedestrian attribute recognition. The
source code of this paper will be released on
https://github.com/Event-AHU/OpenPAR

</details>


### [468] [RAU: Reference-based Anatomical Understanding with Vision Language Models](https://arxiv.org/abs/2509.22404)
*Yiwei Li,Yikang Liu,Jiaqi Guo,Lin Zhao,Zheyuan Zhang,Xiao Chen,Boris Mailhe,Ankush Mukherjee,Terrence Chen,Shanhui Sun*

Main category: cs.CV

TL;DR: 提出RAU框架用于医学图像基于参考的解剖结构理解，在多个数据集上表现优于基线，有良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习解剖理解受专家标注数据稀缺限制，现有视觉语言模型参考理解和细粒度定位能力有限。

Method: 引入RAU框架，让VLM通过参考与目标图像相对空间推理识别解剖区域，将VLM空间线索与SAM2细粒度分割能力结合。

Result: 在多个数据集上，RAU表现优于SAM2微调基线，分割更准确、定位更可靠，有强泛化能力。

Conclusion: RAU是首个探索VLM在医学图像解剖结构识别、定位和分割能力的，凸显了VLM驱动方法在自动临床工作流中解剖理解的潜力。

Abstract: Anatomical understanding through deep learning is critical for automatic
report generation, intra-operative navigation, and organ localization in
medical imaging; however, its progress is constrained by the scarcity of
expert-labeled data. A promising remedy is to leverage an annotated reference
image to guide the interpretation of an unlabeled target. Although recent
vision-language models (VLMs) exhibit non-trivial visual reasoning, their
reference-based understanding and fine-grained localization remain limited. We
introduce RAU, a framework for reference-based anatomical understanding with
VLMs. We first show that a VLM learns to identify anatomical regions through
relative spatial reasoning between reference and target images, trained on a
moderately sized dataset. We validate this capability through visual question
answering (VQA) and bounding box prediction. Next, we demonstrate that the
VLM-derived spatial cues can be seamlessly integrated with the fine-grained
segmentation capability of SAM2, enabling localization and pixel-level
segmentation of small anatomical regions, such as vessel segments. Across two
in-distribution and two out-of-distribution datasets, RAU consistently
outperforms a SAM2 fine-tuning baseline using the same memory setup, yielding
more accurate segmentations and more reliable localization. More importantly,
its strong generalization ability makes it scalable to out-of-distribution
datasets, a property crucial for medical image applications. To the best of our
knowledge, RAU is the first to explore the capability of VLMs for
reference-based identification, localization, and segmentation of anatomical
structures in medical images. Its promising performance highlights the
potential of VLM-driven approaches for anatomical understanding in automated
clinical workflows.

</details>


### [469] [Explaining multimodal LLMs via intra-modal token interactions](https://arxiv.org/abs/2509.22415)
*Jiawei Liang,Ruoyu Chen,Xianghao Jiao,Siyuan Liang,Shiming Liu,Qunli Zhang,Zheng Hu,Xiaochun Cao*

Main category: cs.CV

TL;DR: 现有多模态大语言模型（MLLMs）可解释性研究忽视了模态内依赖，本文提出利用模态内交互增强可解释性的方法，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs可解释性研究主要关注跨模态归因，忽略了模态内依赖，导致归因结果存在碎片化、噪声、虚假激活等问题，影响归因保真度。

Method: 在视觉分支引入多尺度解释聚合（MSEA）动态调整感受野；在文本分支提出激活排名相关性（ARC）衡量上下文标记相关性，抑制虚假激活。

Result: 在多个先进MLLMs和基准数据集上的实验表明，该方法始终优于现有可解释性方法。

Conclusion: 提出的利用模态内交互增强可解释性的方法能产生更真实、细粒度的模型行为解释。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success
across diverse vision-language tasks, yet their internal decision-making
mechanisms remain insufficiently understood. Existing interpretability research
has primarily focused on cross-modal attribution, identifying which image
regions the model attends to during output generation. However, these
approaches often overlook intra-modal dependencies. In the visual modality,
attributing importance to isolated image patches ignores spatial context due to
limited receptive fields, resulting in fragmented and noisy explanations. In
the textual modality, reliance on preceding tokens introduces spurious
activations. Failing to effectively mitigate these interference compromises
attribution fidelity. To address these limitations, we propose enhancing
interpretability by leveraging intra-modal interaction. For the visual branch,
we introduce \textit{Multi-Scale Explanation Aggregation} (MSEA), which
aggregates attributions over multi-scale inputs to dynamically adjust receptive
fields, producing more holistic and spatially coherent visual explanations. For
the textual branch, we propose \textit{Activation Ranking Correlation} (ARC),
which measures the relevance of contextual tokens to the current token via
alignment of their top-$k$ prediction rankings. ARC leverages this relevance to
suppress spurious activations from irrelevant contexts while preserving
semantically coherent ones. Extensive experiments across state-of-the-art MLLMs
and benchmark datasets demonstrate that our approach consistently outperforms
existing interpretability methods, yielding more faithful and fine-grained
explanations of model behavior.

</details>


### [470] [Improving Autism Detection with Multimodal Behavioral Analysis](https://arxiv.org/abs/2509.21352)
*William Saakyan,Matthias Norden,Lola Eversmann,Simon Kirsch,Muyu Lin,Simon Guendelman,Isabel Dziobek,Hanna Drimalla*

Main category: cs.CV

TL;DR: 本文分析标准化视频数据集，进行多模态分析，引入新的统计描述符改进注视特征，通过后期融合实现74%的分类准确率，凸显基于视频的自闭症筛查工具潜力。


<details>
  <summary>Details</summary>
Motivation: 现有自闭症计算机辅助诊断模型存在注视特征表现差和缺乏现实泛化性的问题，需改进。

Method: 分析包含168名自闭症患者和157名非自闭症参与者的标准化视频数据集，进行面部表情、语音韵律、头部运动、心率变异性和注视行为的多模态分析，引入新的统计描述符量化眼动角度变化，采用后期融合方法。

Result: 将基于注视的分类准确率从64%提高到69%，实现74%的整体分类准确率。

Conclusion: 基于视频的筛查工具在支持自闭症评估方面具有可扩展的潜力。

Abstract: Due to the complex and resource-intensive nature of diagnosing Autism
Spectrum Condition (ASC), several computer-aided diagnostic support methods
have been proposed to detect autism by analyzing behavioral cues in patient
video data. While these models show promising results on some datasets, they
struggle with poor gaze feature performance and lack of real-world
generalizability. To tackle these challenges, we analyze a standardized video
dataset comprising 168 participants with ASC (46% female) and 157 non-autistic
participants (46% female), making it, to our knowledge, the largest and most
balanced dataset available. We conduct a multimodal analysis of facial
expressions, voice prosody, head motion, heart rate variability (HRV), and gaze
behavior. To address the limitations of prior gaze models, we introduce novel
statistical descriptors that quantify variability in eye gaze angles, improving
gaze-based classification accuracy from 64% to 69% and aligning computational
findings with clinical research on gaze aversion in ASC. Using late fusion, we
achieve a classification accuracy of 74%, demonstrating the effectiveness of
integrating behavioral markers across multiple modalities. Our findings
highlight the potential for scalable, video-based screening tools to support
autism assessment.

</details>


### [471] [Coreset selection based on Intra-class diversity](https://arxiv.org/abs/2509.21380)
*Imran Ashraf,Mukhtar Ullah,Muhammad Faisal Nadeem,Muhammad Nouman Noor*

Main category: cs.CV

TL;DR: 本文聚焦深度学习模型训练资源消耗大问题，提出智能轻量级核心集选择机制，在生物医学影像数据集实验中表现优于随机采样。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型训练需大量计算时间和资源，随机采样选核心集有代表性不足问题，促使研究新的核心集选择方法。

Method: 提出提取类内多样性的方法，形成每类的聚类用于最终采样。

Result: 在生物医学影像数据集的分类实验中，所提方案在多个性能指标上优于随机采样方法。

Conclusion: 所提智能轻量级核心集选择机制有效，能解决随机采样的问题。

Abstract: Deep Learning models have transformed various domains, including the
healthcare sector, particularly biomedical image classification by learning
intricate features and enabling accurate diagnostics pertaining to complex
diseases. Recent studies have adopted two different approaches to train DL
models: training from scratch and transfer learning. Both approaches demand
substantial computational time and resources due to the involvement of massive
datasets in model training. These computational demands are further increased
due to the design-space exploration required for selecting optimal
hyperparameters, which typically necessitates several training rounds. With the
growing sizes of datasets, exploring solutions to this problem has recently
gained the research community's attention. A plausible solution is to select a
subset of the dataset for training and hyperparameter search. This subset,
referred to as the corset, must be a representative set of the original
dataset. A straightforward approach to selecting the coreset could be employing
random sampling, albeit at the cost of compromising the representativeness of
the original dataset. A critical limitation of random sampling is the bias
towards the dominant classes in an imbalanced dataset. Even if the dataset has
inter-class balance, this random sampling will not capture intra-class
diversity. This study addresses this issue by introducing an intelligent,
lightweight mechanism for coreset selection. Specifically, it proposes a method
to extract intra-class diversity, forming per-class clusters that are utilized
for the final sampling. We demonstrate the efficacy of the proposed methodology
by conducting extensive classification experiments on a well-known biomedical
imaging dataset. Results demonstrate that the proposed scheme outperforms the
random sampling approach on several performance metrics for uniform conditions.

</details>


### [472] [The LongiMam model for improved breast cancer risk prediction using longitudinal mammograms](https://arxiv.org/abs/2509.21383)
*Manel Rakez,Thomas Louis,Julien Guillaumin,Foucauld Chamming's,Pierre Fillard,Brice Amadeo,Virginie Rondeau*

Main category: cs.CV

TL;DR: 开发端到端深度学习模型LongiMam整合多期乳腺钼靶影像，提升乳腺癌预测效果，支持在筛查项目中使用重复钼靶影像细化风险分层。


<details>
  <summary>Details</summary>
Motivation: 当前多数深度学习模型利用纵向影像数据能力不足，缺乏对现实场景的适应，需要开发能利用多期影像数据的鲁棒模型用于风险适应性乳腺癌筛查。

Method: 开发LongiMam模型，结合卷积和循环神经网络，利用大型基于人群的筛查数据集进行训练和评估。

Result: 在多种场景下，包含既往钼靶影像时LongiMam持续改善预测效果；结合既往和当前检查优于单期检查模型；亚组分析证实模型在关键风险组有效，在乳腺密度随时间有变化的女性中表现最佳。

Conclusion: 纵向建模可增强乳腺癌预测，支持在筛查项目中使用重复钼靶影像细化风险分层，LongiMam开源可用。

Abstract: Risk-adapted breast cancer screening requires robust models that leverage
longitudinal imaging data. Most current deep learning models use single or
limited prior mammograms and lack adaptation for real-world settings marked by
imbalanced outcome distribution and heterogeneous follow-up. We developed
LongiMam, an end-to-end deep learning model that integrates both current and up
to four prior mammograms. LongiMam combines a convolutional and a recurrent
neural network to capture spatial and temporal patterns predictive of breast
cancer. The model was trained and evaluated using a large, population-based
screening dataset with disproportionate case-to-control ratio typical of
clinical screening. Across several scenarios that varied in the number and
composition of prior exams, LongiMam consistently improved prediction when
prior mammograms were included. The addition of prior and current visits
outperformed single-visit models, while priors alone performed less well,
highlighting the importance of combining historical and recent information.
Subgroup analyses confirmed the model's efficacy across key risk groups,
including women with dense breasts and those aged 55 years or older. Moreover,
the model performed best in women with observed changes in mammographic density
over time. These findings demonstrate that longitudinal modeling enhances
breast cancer prediction and support the use of repeated mammograms to refine
risk stratification in screening programs. LongiMam is publicly available as
open-source software.

</details>


### [473] [Debugging Concept Bottleneck Models through Removal and Retraining](https://arxiv.org/abs/2509.21385)
*Eric Enouen,Sainyam Galhotra*

Main category: cs.CV

TL;DR: 提出针对概念瓶颈模型（CBMs）的可解释调试框架，含去除和再训练两步，CBDebug方法表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有CBM干预措施无法解决模型与专家推理的系统偏差问题，如从有偏数据学捷径。

Method: 两步过程：去除步骤中专家用概念解释识别并去除不良概念；再训练步骤引入CBDebug方法，将概念级用户反馈转化为样本级辅助标签，进行监督偏差缓解和目标增强。

Result: 用真实和自动专家反馈评估，CBDebug在多个CBM架构和有已知虚假相关性的基准测试中显著优于先前再训练方法。

Conclusion: 提出的可解释调试框架有效，CBDebug方法能减少模型对不良概念的依赖。

Abstract: Concept Bottleneck Models (CBMs) use a set of human-interpretable concepts to
predict the final task label, enabling domain experts to not only validate the
CBM's predictions, but also intervene on incorrect concepts at test time.
However, these interventions fail to address systemic misalignment between the
CBM and the expert's reasoning, such as when the model learns shortcuts from
biased data. To address this, we present a general interpretable debugging
framework for CBMs that follows a two-step process of Removal and Retraining.
In the Removal step, experts use concept explanations to identify and remove
any undesired concepts. In the Retraining step, we introduce CBDebug, a novel
method that leverages the interpretability of CBMs as a bridge for converting
concept-level user feedback into sample-level auxiliary labels. These labels
are then used to apply supervised bias mitigation and targeted augmentation,
reducing the model's reliance on undesired concepts. We evaluate our framework
with both real and automated expert feedback, and find that CBDebug
significantly outperforms prior retraining methods across multiple CBM
architectures (PIP-Net, Post-hoc CBM) and benchmarks with known spurious
correlations.

</details>


### [474] [mmHSense: Multi-Modal and Distributed mmWave ISAC Datasets for Human Sensing](https://arxiv.org/abs/2509.21396)
*Nabeel Nisar Bhat,Maksim Karnaukh,Stein Vandenbroeke,Wouter Lemoine,Jakob Struye,Jesus Omar Lacruz,Siddhartha Kumar,Mohammad Hossein Moghaddam,Joerg Widmer,Rafael Berkvens,Jeroen Famaey*

Main category: cs.CV

TL;DR: 本文提出mmHSense开放标注毫米波数据集，用于支持ISAC系统人体感知研究，介绍数据集情况并验证其效用，还展示参数高效微调方法。


<details>
  <summary>Details</summary>
Motivation: 为ISAC系统中的人体感知研究提供支持，推动毫米波ISAC在多领域应用及信号处理和深度学习研究。

Method: 介绍数据集的测试平台、实验设置和信号特征，通过特定下游任务验证数据集效用，采用参数高效微调使ISAC模型适应不同任务。

Result: 数据集可用于手势识别等多种应用，参数高效微调能降低计算复杂度并保持先前任务性能。

Conclusion: mmHSense数据集有实用价值，参数高效微调是有效的模型适配方法。

Abstract: This article presents mmHSense, a set of open labeled mmWave datasets to
support human sensing research within Integrated Sensing and Communication
(ISAC) systems. The datasets can be used to explore mmWave ISAC for various end
applications such as gesture recognition, person identification, pose
estimation, and localization. Moreover, the datasets can be used to develop and
advance signal processing and deep learning research on mmWave ISAC. This
article describes the testbed, experimental settings, and signal features for
each dataset. Furthermore, the utility of the datasets is demonstrated through
validation on a specific downstream task. In addition, we demonstrate the use
of parameter-efficient fine-tuning to adapt ISAC models to different tasks,
significantly reducing computational complexity while maintaining performance
on prior tasks.

</details>


### [475] [Downscaling climate projections to 1 km with single-image super resolution](https://arxiv.org/abs/2509.21399)
*Petr Košťál,Pavel Kordík,Ondřej Podsztavek*

Main category: cs.CV

TL;DR: 利用单图像超分辨率模型将气候预测降尺度到1公里分辨率，并用气候指标评估，实验证明可在不增加误差下完成降尺度。


<details>
  <summary>Details</summary>
Motivation: 现有气候预测空间分辨率低，限制了其在地方决策中的可用性。

Method: 利用单图像超分辨率模型对气候预测进行统计降尺度，在高分辨率观测网格数据集上训练模型并应用于低分辨率气候预测，提出基于气候指标的评估方法。

Result: 在日平均温度实验中，单图像超分辨率模型降尺度后的气候预测未增加气候指标误差。

Conclusion: 单图像超分辨率模型可有效将低分辨率气候预测降尺度到高分辨率，且不增加气候指标误差。

Abstract: High-resolution climate projections are essential for local decision-making.
However, available climate projections have low spatial resolution (e.g. 12.5
km), which limits their usability. We address this limitation by leveraging
single-image super-resolution models to statistically downscale climate
projections to 1-km resolution. Since high-resolution climate projections are
unavailable for training, we train models on a high-resolution observational
gridded data set and apply them to low-resolution climate projections. We
propose a climate indicator-based assessment using observed climate indices
computed at weather station locations to evaluate the downscaled climate
projections without ground-truth high-resolution climate projections.
Experiments on daily mean temperature demonstrate that single-image
super-resolution models can downscale climate projections without increasing
the error of climate indicators compared to low-resolution climate projections.

</details>


### [476] [Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting](https://arxiv.org/abs/2509.22615)
*Yasmine Omri,Connor Ding,Tsachy Weissman,Thierry Tambe*

Main category: cs.CV

TL;DR: 本文探索2D高斯散点图（2DGS）作为视觉语言对齐的替代视觉基底，开发可扩展的2DGS管道并适配CLIP，在压缩输入的同时实现一定零样本性能，确立2DGS作为多模态基底的可行性。


<details>
  <summary>Details</summary>
Motivation: 现代视觉语言管道使用RGB视觉编码器存在传输能耗高、序列长度爆炸等结构低效问题，需寻找替代方案。

Method: 开发具有结构化初始化、亮度感知剪枝和批处理CUDA内核的可扩展2DGS管道；将对比语言图像预训练（CLIP）适配到2DGS，重用基于RGB的变压器骨干，训练少量参数。

Result: 2DGS管道拟合速度比先前实现快90倍以上，GPU利用率约97%；GS编码器在压缩输入3 - 20倍的情况下，在大型DataComp子集上有有意义的零样本ImageNet - 1K性能。

Conclusion: 2DGS是一种可行的多模态基底，指出了架构瓶颈，为边缘云学习开辟了语义强大且传输高效的表示途径。

Abstract: Modern vision language pipelines are driven by RGB vision encoders trained on
massive image text corpora. While these pipelines have enabled impressive zero
shot capabilities and strong transfer across tasks, they still inherit two
structural inefficiencies from the pixel domain: (i) transmitting dense RGB
images from edge devices to the cloud is energy intensive and costly, and (ii)
patch based tokenization explodes sequence length, stressing attention budgets
and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative
visual substrate for alignment: a compact, spatially adaptive representation
that parameterizes images by a set of colored anisotropic Gaussians. We develop
a scalable 2DGS pipeline with structured initialization, luminance aware
pruning, and batched CUDA kernels, achieving over 90x faster fitting and about
97% GPU utilization compared to prior implementations. We further adapt
contrastive language image pretraining (CLIP) to 2DGS by reusing a frozen
RGB-based transformer backbone with a lightweight splat aware input stem and a
perceiver resampler, training only about 7% of the total parameters. On large
DataComp subsets, GS encoders yield meaningful zero shot ImageNet-1K
performance while compressing inputs 3 to 20x relative to pixels. While
accuracy currently trails RGB encoders, our results establish 2DGS as a viable
multimodal substrate, pinpoint architectural bottlenecks, and open a path
toward representations that are both semantically powerful and transmission
efficient for edge cloud learning.

</details>


### [477] [VLCE: A Knowledge-Enhanced Framework for Image Description in Disaster Assessment](https://arxiv.org/abs/2509.21609)
*Md. Mahfuzur Rahman,Kishor Datta Gupta,Marufa Kamal,Fahad Rahman,Sunzida Siddique,Ahmed Rafi Hasan,Mohd Ariful Haque,Roy George*

Main category: cs.CV

TL;DR: 提出Vision Language Caption Enhancer (VLCE)多模态系统用于灾害图像解释，经实验表现优于基线模型，有提升灾害评估能力。


<details>
  <summary>Details</summary>
Motivation: 传统人工灾害评估方法慢且危险，现有计算机视觉方法无法全面理解灾害情况，需新系统进行灾害图像解释。

Method: 采用双架构方法，CNN - LSTM模型处理xBD数据集，Vision Transformer (ViT)模型处理RescueNet数据集，利用ConceptNet和WordNet外部语义知识，用CLIPScore和InfoMetIC评估。

Result: VLCE显著超越基线模型，在InfoMetIC上最高达95.33%，保持有竞争力的语义对齐。

Conclusion: 双架构系统通过自动生成灾害图像信息丰富的描述，有提升灾害损失评估的潜力。

Abstract: Immediate damage assessment is essential after natural catastrophes; yet,
conventional hand evaluation techniques are sluggish and perilous. Although
satellite and unmanned aerial vehicle (UAV) photos offer extensive perspectives
of impacted regions, current computer vision methodologies generally yield just
classification labels or segmentation masks, so constraining their capacity to
deliver a thorough situational comprehension. We introduce the Vision Language
Caption Enhancer (VLCE), a multimodal system designed to produce comprehensive,
contextually-informed explanations of disaster imagery. VLCE employs a
dual-architecture approach: a CNN-LSTM model with a ResNet50 backbone
pretrained on EuroSat satellite imagery for the xBD dataset, and a Vision
Transformer (ViT) model pretrained on UAV pictures for the RescueNet dataset.
Both systems utilize external semantic knowledge from ConceptNet and WordNet to
expand vocabulary coverage and improve description accuracy. We assess VLCE in
comparison to leading vision-language models (LLaVA and QwenVL) utilizing
CLIPScore for semantic alignment and InfoMetIC for caption informativeness.
Experimental findings indicate that VLCE markedly surpasses baseline models,
attaining a maximum of 95.33% on InfoMetIC while preserving competitive
semantic alignment. Our dual-architecture system demonstrates significant
potential for improving disaster damage assessment by automating the production
of actionable, information-dense descriptions from satellite and drone photos.

</details>


### [478] [Hierarchical Representation Matching for CLIP-based Class-Incremental Learning](https://arxiv.org/abs/2509.22645)
*Zhen-Hao Wen,Yan Wang,Ji Feng,Han-Jia Ye,De-Chuan Zhan,Da-Wei Zhou*

Main category: cs.CV

TL;DR: 提出用于基于CLIP的类增量学习方法HERMAN，利用LLM生成文本描述符，在多基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的类增量学习方法使用简单模板，忽略视觉概念层次结构和早期层的层次信息。

Method: 引入HERMAN方法，利用LLM递归生成有区分性的文本描述符，匹配语义层次不同级别并自适应路由。

Result: 在多个基准测试中，该方法始终取得了最先进的性能。

Conclusion: HERMAN方法能增强语义空间，实现精确区分并缓解增量任务中的灾难性遗忘。

Abstract: Class-Incremental Learning (CIL) aims to endow models with the ability to
continuously adapt to evolving data streams. Recent advances in pre-trained
vision-language models (e.g., CLIP) provide a powerful foundation for this
task. However, existing approaches often rely on simplistic templates, such as
"a photo of a [CLASS]", which overlook the hierarchical nature of visual
concepts. For example, recognizing "cat" versus "car" depends on coarse-grained
cues, while distinguishing "cat" from "lion" requires fine-grained details.
Similarly, the current feature mapping in CLIP relies solely on the
representation from the last layer, neglecting the hierarchical information
contained in earlier layers. In this work, we introduce HiErarchical
Representation MAtchiNg (HERMAN) for CLIP-based CIL. Our approach leverages
LLMs to recursively generate discriminative textual descriptors, thereby
augmenting the semantic space with explicit hierarchical cues. These
descriptors are matched to different levels of the semantic hierarchy and
adaptively routed based on task-specific requirements, enabling precise
discrimination while alleviating catastrophic forgetting in incremental tasks.
Extensive experiments on multiple benchmarks demonstrate that our method
consistently achieves state-of-the-art performance.

</details>


### [479] [Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs](https://arxiv.org/abs/2509.22646)
*Xingyu Fu,Siyi Liu,Yinuo Xu,Pan Lu,Guangqiuse Hu,Tianbo Yang,Taran Anantasagar,Christopher Shen,Yikai Mao,Yuanzhe Liu,Keyush Shah,Chung Un Lee,Yejin Choi,James Zou,Dan Roth,Chris Callison-Burch*

Main category: cs.CV

TL;DR: 提出DeeptraceReward基准，标注人类感知的虚假痕迹，训练奖励模型，7B奖励模型表现优于GPT - 5，呈现难度梯度，为视频生成提供测试床和训练信号。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型发展迅速，但人类能否检测生成视频中的深度伪造痕迹这一关键维度被忽视，需研究人类检测及判断依据。

Method: 引入DeeptraceReward基准，对视频虚假痕迹进行细粒度、时空感知的标注，将标注整合为9大类，训练多模态语言模型作为奖励模型。

Result: 7B奖励模型在虚假线索识别、定位和解释上平均比GPT - 5高34.7%，发现存在难度梯度。

Conclusion: DeeptraceReward为具有社会意识和可信的视频生成提供了严格的测试床和训练信号。

Abstract: Can humans identify AI-generated (fake) videos and provide grounded reasons?
While video generation models have advanced rapidly, a critical dimension --
whether humans can detect deepfake traces within a generated video, i.e.,
spatiotemporal grounded visual artifacts that reveal a video as machine
generated -- has been largely overlooked. We introduce DeeptraceReward, the
first fine-grained, spatially- and temporally- aware benchmark that annotates
human-perceived fake traces for video generation reward. The dataset comprises
4.3K detailed annotations across 3.3K high-quality generated videos. Each
annotation provides a natural-language explanation, pinpoints a bounding-box
region containing the perceived trace, and marks precise onset and offset
timestamps. We consolidate these annotations into 9 major categories of
deepfake traces that lead humans to identify a video as AI-generated, and train
multimodal language models (LMs) as reward models to mimic human judgments and
localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by
34.7% on average across fake clue identification, grounding, and explanation.
Interestingly, we observe a consistent difficulty gradient: binary fake v.s.
real classification is substantially easier than fine-grained deepfake trace
detection; within the latter, performance degrades from natural language
explanations (easiest), to spatial grounding, to temporal labeling (hardest).
By foregrounding human-perceived deepfake traces, DeeptraceReward provides a
rigorous testbed and training signal for socially aware and trustworthy video
generation.

</details>


### [480] [CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning](https://arxiv.org/abs/2509.22647)
*Long Xing,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jianze Liang,Qidong Huang,Jiaqi Wang,Feng Wu,Dahua Lin*

Main category: cs.CV

TL;DR: 提出CapRL训练框架用于图像字幕任务，以克服SFT局限，实验显示其在多个设置中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于SFT的图像字幕模型存在局限性，如依赖昂贵数据、缺乏泛化和创造力，需新训练范式。

Method: 提出CapRL训练框架，采用解耦两阶段管道，通过非视觉语言模型回答问题的准确性确定奖励。

Result: 在CapRL - 5M数据集预训练在12个基准测试中取得显著收益，在Prism框架中表现与Qwen2.5 - VL - 72B相当，平均超基线8.4%。

Conclusion: CapRL能有效提升图像字幕任务性能，为图像字幕训练提供新方法。

Abstract: Image captioning is a fundamental task that bridges the visual and linguistic
domains, playing a critical role in pre-training Large Vision-Language Models
(LVLMs). Current state-of-the-art captioning models are typically trained with
Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable
data annotated by humans or proprietary models. This approach often leads to
models that memorize specific ground-truth answers, limiting their generality
and ability to generate diverse, creative descriptions. To overcome the
limitation of SFT, we propose applying the Reinforcement Learning with
Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.
A primary challenge, however, is designing an objective reward function for the
inherently subjective nature of what constitutes a "good" caption. We introduce
Captioning Reinforcement Learning (CapRL), a novel training framework that
redefines caption quality through its utility: a high-quality caption should
enable a non-visual language model to accurately answer questions about the
corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM
generates a caption, and the objective reward is derived from the accuracy of a
separate, vision-free LLM answering Multiple-Choice Questions based solely on
that caption. As the first study to apply RLVR to the subjective image
captioning task, we demonstrate that CapRL significantly enhances multiple
settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B
results in substantial gains across 12 benchmarks. Moreover, within the Prism
Framework for caption quality evaluation, CapRL achieves performance comparable
to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%.
Code is available here: https://github.com/InternLM/CapRL.

</details>


### [481] [CubistMerge: Spatial-Preserving Token Merging For Diverse ViT Backbones](https://arxiv.org/abs/2509.21764)
*Wenyi Gong,Mieszko Lis*

Main category: cs.CV

TL;DR: 提出一种保持空间完整性的简单有效的token合并方法，兼容空间架构，在不同视觉任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现代ViT骨干网络的空间架构设计给token缩减带来挑战，现有多数方法无法保留这些架构依赖的空间结构。

Method: 采用2D缩减策略强制结构化token布局，使用空间感知合并算法维护相对token位置，提出一种新的每维最大幅度token表示以保留显著特征。

Result: 在空间和非空间架构的各种视觉任务中取得了最先进的结果，如在SAM - H上实现1.25倍加速，mIOU仅下降0.7%；在DeiT - B上微调一个epoch后实现1.15倍加速且top - 1准确率无下降。

Conclusion: 所提出的token合并方法有效，能在保持空间结构的同时利用信息分布，提升模型性能和速度。

Abstract: Many modern ViT backbones adopt spatial architectural designs, such as window
attention, decomposed relative positional embeddings in SAM, and RoPE in
DINOv3. Such architectures impose new challenges on token reduction, as the
vast majority of existing methods fail to preserve the spatial structure these
architectures depend on. In this paper, we introduce a simple yet effective
token merging method that maintains spatial integrity, enabling seamless
compatibility with spatial architectures. We reconcile two seemingly
conflicting requirements: (i)exploiting the uneven information distribution
across the spatial layout while (ii)preserving the spatial structure
post-merging. Our approach employs (i)a 2D reduction strategy to enforce
structured token layouts, (ii)a spatial-aware merging algorithm that maintains
relative token positions, and (iii)a novel max-magnitude-per-dimension token
representation that preserves salient features. Our method demonstrates strong
performance both off-the-shelf and with fine-tuning, achieving state-of-the-art
results on spatial and non-spatial architectures across various vision tasks.
Specifically, we achieve 1.25x speedup on SAM-H with only 0.7% mIOU drop
evaluated on COCO off-the-shelf, and 1.15x speedup on DeiT-B with no top-1
accuracy drop on ImageNet within just one epoch of fine-tuning.

</details>


### [482] [DragGANSpace: Latent Space Exploration and Control for GANs](https://arxiv.org/abs/2509.22169)
*Kirsten Odendaal,Neela Kaushik,Spencer Halverson*

Main category: cs.CV

TL;DR: 本文结合StyleGAN、DragGAN和PCA提升GAN生成图像的潜在空间效率和可控性，在AFHQ数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 提升GAN生成图像的潜在空间效率和可控性。

Method: 集成StyleGAN、DragGAN和PCA，在AFHQ数据集应用，用PCA降维结合DragGAN框架处理图像。

Result: 保留性能并提高优化效率，减少优化时间，提升SSIM，能对齐不同数据域StyleGAN模型生成的图像并直观操控。

Conclusion: 证明高效且可解释的潜在空间控制在图像合成和编辑应用中的可能性。

Abstract: This work integrates StyleGAN, DragGAN and Principal Component Analysis (PCA)
to enhance the latent space efficiency and controllability of GAN-generated
images. Style-GAN provides a structured latent space, DragGAN enables intuitive
image manipulation, and PCA reduces dimensionality and facilitates cross-model
alignment for more streamlined and interpretable exploration of latent spaces.
We apply our techniques to the Animal Faces High Quality (AFHQ) dataset, and
find that our approach of integrating PCA-based dimensionality reduction with
the Drag-GAN framework for image manipulation retains performance while
improving optimization efficiency. Notably, introducing PCA into the latent W+
layers of DragGAN can consistently reduce the total optimization time while
maintaining good visual quality and even boosting the Structural Similarity
Index Measure (SSIM) of the optimized image, particularly in shallower latent
spaces (W+ layers = 3). We also demonstrate capability for aligning images
generated by two StyleGAN models trained on similar but distinct data domains
(AFHQ-Dog and AFHQ-Cat), and show that we can control the latent space of these
aligned images to manipulate the images in an intuitive and interpretable
manner. Our findings highlight the possibility for efficient and interpretable
latent space control for a wide range of image synthesis and editing
applications.

</details>


### [483] [NIFTY: a Non-Local Image Flow Matching for Texture Synthesis](https://arxiv.org/abs/2509.22318)
*Pierrick Chatillon,Julien Rabin,David Tschumperlé*

Main category: cs.CV

TL;DR: 本文提出NIFTY混合框架解决基于示例的纹理合成问题，实验证明其有效性，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决基于示例的纹理合成问题，改善基于补丁方法的常见缺点。

Method: 引入结合扩散模型和经典补丁纹理优化技术的NIFTY混合框架，它是基于非局部补丁匹配的非参数流匹配模型。

Result: 实验结果表明该方法相比文献中代表性方法更有效。

Conclusion: 提出的NIFTY框架在纹理合成方面具有有效性。

Abstract: This paper addresses the problem of exemplar-based texture synthesis. We
introduce NIFTY, a hybrid framework that combines recent insights on diffusion
models trained with convolutional neural networks, and classical patch-based
texture optimization techniques. NIFTY is a non-parametric flow-matching model
built on non-local patch matching, which avoids the need for neural network
training while alleviating common shortcomings of patch-based methods, such as
poor initialization or visual artifacts. Experimental results demonstrate the
effectiveness of the proposed approach compared to representative methods from
the literature. Code is available at https://github.com/PierrickCh/Nifty.git

</details>


### [484] [Integrating Background Knowledge in Medical Semantic Segmentation with Logic Tensor Networks](https://arxiv.org/abs/2509.22399)
*Luca Bergamin,Giovanna Maria Dimitri,Fabio Aiolli*

Main category: cs.CV

TL;DR: 文章指出可将医学常识融入分割模型损失函数来提升语义分割性能，引入LTNs编码医学知识并与SwinUNETR结合，在海马体分割任务中验证其能提升性能，认为神经符号方法可用于其他医学分割任务。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习驱动的医学图像语义分割系统未完善，期望通过将医学常识融入损失函数提升性能。

Method: 引入Logic Tensor Networks (LTNs)用一阶逻辑规则编码医学背景知识，与SwinUNETR在端到端框架中结合进行语义分割。

Result: 在脑MRI扫描海马体分割任务实验中，LTNs提升了基线分割性能，尤其在训练数据稀缺时。

Conclusion: 神经符号方法具有通用性，可适应并应用于其他医学语义分割任务，尽管还处于初步阶段。

Abstract: Semantic segmentation is a fundamental task in medical image analysis, aiding
medical decision-making by helping radiologists distinguish objects in an
image. Research in this field has been driven by deep learning applications,
which have the potential to scale these systems even in the presence of noise
and artifacts. However, these systems are not yet perfected. We argue that
performance can be improved by incorporating common medical knowledge into the
segmentation model's loss function. To this end, we introduce Logic Tensor
Networks (LTNs) to encode medical background knowledge using first-order logic
(FOL) rules. The encoded rules span from constraints on the shape of the
produced segmentation, to relationships between different segmented areas. We
apply LTNs in an end-to-end framework with a SwinUNETR for semantic
segmentation. We evaluate our method on the task of segmenting the hippocampus
in brain MRI scans. Our experiments show that LTNs improve the baseline
segmentation performance, especially when training data is scarce. Despite
being in its preliminary stages, we argue that neurosymbolic methods are
general enough to be adapted and applied to other medical semantic segmentation
tasks.

</details>


### [485] [SPARK: Synergistic Policy And Reward Co-Evolving Framework](https://arxiv.org/abs/2509.22624)
*Ziyu Liu,Yuhang Zang,Shengyuan Ding,Yuhang Cao,Xiaoyi Dong,Haodong Duan,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出高效、稳定的协同框架SPARK，解决RLHF和RLVR的问题，在多模型和基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF成本高、有奖励 - 策略不匹配问题，RLVR浪费监督信息，需要改进方法。

Method: 基于RLVR提出SPARK框架，回收信息训练生成式奖励模型，通过多种目标辅助训练，形成正反馈循环。

Result: SPARK在多个LLM和LVLM模型及多种基准测试中取得显著性能提升，如SPARK - VL - 7B在不同基准测试中有平均增益。

Conclusion: SPARK框架高效、稳定，具有鲁棒性和广泛泛化能力，无需外部奖励模型和人类偏好数据。

Abstract: Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)
increasingly use Reinforcement Learning (RL) for post-pretraining, such as RL
with Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback
(RLHF) for subjective tasks. However, RLHF incurs high costs and potential
reward-policy mismatch due to reliance on human preferences, while RLVR still
wastes supervision by discarding rollouts and correctness signals after each
update. To address these challenges, we introduce the Synergistic Policy And
Reward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable
method that builds on RLVR. Instead of discarding rollouts and correctness
data, SPARK recycles this valuable information to simultaneously train the
model itself as a generative reward model. This auxiliary training uses a mix
of objectives, such as pointwise reward score, pairwise comparison, and
evaluation conditioned on further-reflection responses, to teach the model to
evaluate and improve its own responses. Our process eliminates the need for a
separate reward model and costly human preference data. SPARK creates a
positive co-evolving feedback loop: improved reward accuracy yields better
policy gradients, which in turn produce higher-quality rollouts that further
refine the reward model. Our unified framework supports test-time scaling via
self-reflection without external reward models and their associated costs. We
show that SPARK achieves significant performance gains on multiple LLM and LVLM
models and multiple reasoning, reward models, and general benchmarks. For
example, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks,
12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the
baselines, demonstrating robustness and broad generalization.

</details>


### [486] [Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance](https://arxiv.org/abs/2509.22635)
*Luc Boudier,Loris Manganelli,Eleftherios Tsonis,Nicolas Dufour,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 提出无训练方法DIPSY，利用IP - Adapter生成合成图像用于少样本图像分类，在多数据集实验中表现良好。


<details>
  <summary>Details</summary>
Motivation: 少样本图像分类因标注样本有限具挑战性，现有方法需大量模型微调或外部信息源。

Method: 提出DIPSY方法，利用IP - Adapter进行图像到图像转换，有扩展的无分类器引导方案、基于类相似性的采样策略和简单有效管道，无需模型微调或外部工具。

Result: 在十个基准数据集实验中达到了最先进或相当的性能，无需生成模型适配或依赖外部工具。

Conclusion: 利用正负引导的双图像提示生成类别区分特征有效，尤其适用于细粒度分类任务。

Abstract: Few-shot image classification remains challenging due to the limited
availability of labeled examples. Recent approaches have explored generating
synthetic training data using text-to-image diffusion models, but often require
extensive model fine-tuning or external information sources. We present a novel
training-free approach, called DIPSY, that leverages IP-Adapter for
image-to-image translation to generate highly discriminative synthetic images
using only the available few-shot examples. DIPSY introduces three key
innovations: (1) an extended classifier-free guidance scheme that enables
independent control over positive and negative image conditioning; (2) a class
similarity-based sampling strategy that identifies effective contrastive
examples; and (3) a simple yet effective pipeline that requires no model
fine-tuning or external captioning and filtering. Experiments across ten
benchmark datasets demonstrate that our approach achieves state-of-the-art or
comparable performance, while eliminating the need for generative model
adaptation or reliance on external tools for caption generation and image
filtering. Our results highlight the effectiveness of leveraging dual image
prompting with positive-negative guidance for generating class-discriminative
features, particularly for fine-grained classification tasks.

</details>


### [487] [Scale-Wise VAR is Secretly Discrete Diffusion](https://arxiv.org/abs/2509.22636)
*Amandeep Kumar,Nithin Gopalakrishnan Nair,Vishal M. Patel*

Main category: cs.CV

TL;DR: 本文揭示自回归（AR）视觉自回归生成（VAR）与离散扩散在数学上的等价性，带来效率和生成效果提升。


<details>
  <summary>Details</summary>
Motivation: 探讨AR transformers中VAR模型的潜力，建立其与扩散模型的联系以引入扩散模型优势。

Method: 发现VAR在配备马尔可夫注意掩码时与离散扩散数学等价，提出可扩展视觉细化的离散扩散（SRDD）。

Result: 将扩散模型优势引入VAR，实现更快收敛、更低推理成本和更好的零样本重建。

Conclusion: VAR的扩散视角在多个数据集上带来效率和生成效果的持续提升。

Abstract: Autoregressive (AR) transformers have emerged as a powerful paradigm for
visual generation, largely due to their scalability, computational efficiency
and unified architecture with language and vision. Among them, next scale
prediction Visual Autoregressive Generation (VAR) has recently demonstrated
remarkable performance, even surpassing diffusion-based models. In this work,
we revisit VAR and uncover a theoretical insight: when equipped with a
Markovian attention mask, VAR is mathematically equivalent to a discrete
diffusion. We term this reinterpretation as Scalable Visual Refinement with
Discrete Diffusion (SRDD), establishing a principled bridge between AR
transformers and diffusion models. Leveraging this new perspective, we show how
one can directly import the advantages of diffusion such as iterative
refinement and reduce architectural inefficiencies into VAR, yielding faster
convergence, lower inference cost, and improved zero-shot reconstruction.
Across multiple datasets, we show that the diffusion based perspective of VAR
leads to consistent gains in efficiency and generation.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [488] [General CoVaR Based on Entropy Pooling](https://arxiv.org/abs/2509.21904)
*Yuhong Xu,Xinyao Zhao*

Main category: stat.ME

TL;DR: 提出扩展传统CoVaR的通用框架，结合专家观点，用熵池法计算，推导解析表达式，分析敏感性，实证有效并扩展到ΔCoVaR。


<details>
  <summary>Details</summary>
Motivation: 传统CoVaR有局限，希望通过纳入多样专家观点和信息来扩展它。

Method: 采用熵池法整合专家观点得到后验分布以计算通用CoVaR，假设双变量正态分布推导解析表达式，进行敏感性分析和实证分析。

Result: CoVaR与变量期望线性相关，与方差、分位数和相关性非线性相关，实证显示合适指定专家观点时通用CoVaR有效。

Conclusion: 通用CoVaR框架有效，可从多视角评估风险溢出效应，且可扩展到ΔCoVaR。

Abstract: We propose a general CoVaR framework that extends the traditional CoVaR by
incorporating diverse expert views and information, such as asset moment
characteristics, quantile insights, and perspectives on the relative loss
distribution between two assets. To integrate these expert views effectively
while minimizing deviations from the prior distribution, we employ the entropy
pooling method to derive the posterior distribution, which in turn enables us
to compute the general CoVaR. Assuming bivariate normal distributions, we
derive its analytical expressions under various perspectives. Sensitivity
analysis reveals that CoVaR exhibits a linear relationship with both the
expectations of the variables in the views and the differences in expectations
between them. In contrast, CoVaR shows nonlinear dependencies with respect to
the variance, quantiles, and correlation within these views.
  Empirical analysis of the US banking system during the Federal Reserve's
interest rate hikes demonstrates the effectiveness of the general CoVaR when
expert views are appropriately specified. Furthermore, we extend this framework
to the general $\Delta$CoVaR, which allows for the assessment of risk spillover
effects from various perspectives.

</details>


### [489] [A Nonparametric Bayesian Solution of the Empirical Stochastic Inverse Problem](https://arxiv.org/abs/2509.22597)
*Haiyi Shi,Lei Yang,Jiarui Chi,Troy Butler,Haonan Wang,Derek Bingham,Don Estep*

Main category: stat.ME

TL;DR: 提出并分析随机逆问题的非参数贝叶斯解，证明其关键性质，分析计算解的收敛性与误差，用应用示例说明结果。


<details>
  <summary>Details</summary>
Motivation: 随机逆问题是复杂科学与工程系统推理、预测和决策的关键要素，需研究其解决方案。

Method: 提出非参数贝叶斯解，通过随机采样获得计算解。

Result: 证明了解的关键性质，分析了计算解的收敛性与误差。

Conclusion: 所提出的非参数贝叶斯解可用于解决随机逆问题，应用示例验证了结果。

Abstract: The stochastic inverse problem is a key ingredient in making inferences,
predictions, and decisions for complex science and engineering systems. We
formulate and analyze a nonparametric Bayesian solution for the stochastic
inverse problem. Key properties of the solution are proved and the convergence
and error of a computational solution obtained by random sampling is analyzed.
Several applications illustrate the results.

</details>


### [490] [Federated Learning of Quantile Inference under Local Differential Privacy](https://arxiv.org/abs/2509.21800)
*Leheng Cai,Qirui Hu,Shuyuan Wu*

Main category: stat.ME

TL;DR: 研究局部差分隐私下联邦学习的分位数推断，提出基于局部随机梯度下降的估计器，建立渐近正态性等理论，构建置信区间，实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 研究局部差分隐私下联邦学习的分位数推断，解决通信和存储约束问题，同时适应数据异质性和不同隐私预算。

Method: 提出基于局部随机梯度下降的估计器，通过随机机制扰动局部梯度；利用自归一化方法构建置信区间。

Result: 建立了估计器的渐近正态性和泛函中心极限定理；数值实验和实际数据应用验证了方法的理论保证。

Conclusion: 所提方法在满足统计效率的同时，能容忍通信和存储约束，适应数据异质性和不同隐私预算。

Abstract: In this paper, we investigate federated learning for quantile inference under
local differential privacy (LDP). We propose an estimator based on local
stochastic gradient descent (SGD), whose local gradients are perturbed via a
randomized mechanism with global parameters, making the procedure tolerant of
communication and storage constraints without compromising statistical
efficiency. Although the quantile loss and its corresponding gradient do not
satisfy standard smoothness conditions typically assumed in existing
literature, we establish asymptotic normality for our estimator as well as a
functional central limit theorem. The proposed method accommodates data
heterogeneity and allows each server to operate with an individual privacy
budget. Furthermore, we construct confidence intervals for the target value
through a self-normalization approach, thereby circumventing the need to
estimate additional nuisance parameters. Extensive numerical experiments and
real data application validate the theoretical guarantees of the proposed
methodology.

</details>


### [491] [Rescuing double robustness: safe estimation under complete misspecification](https://arxiv.org/abs/2509.22446)
*Lorenzo Testa,Francesca Chiaromonte,Kathryn Roeder*

Main category: stat.ME

TL;DR: 本文指出双重稳健估计量在完全扰动函数误设时表现脆弱，提出自适应修正裁剪（ACC）方法解决此问题，通过模拟和实际数据展示其有效性。


<details>
  <summary>Details</summary>
Motivation: 双重稳健估计量在很多应用中面对完全扰动函数误设时很脆弱，实际表现不佳，需解决此问题。

Method: 提出自适应修正裁剪（ACC）方法，继承双重稳健估计量在扰动函数正确设定时的优良性质，保证误差受个体扰动模型误差的凸组合限制。

Result: 通过广泛模拟和应用于阿尔茨海默病蛋白质组学数据分析，展示了ACC估计量的有效性，且在扰动函数正确设定时能通过参数自助法进行有效推断。

Conclusion: ACC方法安全有效，可解决双重稳健估计量在完全扰动函数误设时的脆弱性问题。

Abstract: Double robustness is a major selling point of semiparametric and missing data
methodology. Its virtues lie in protection against partial nuisance
misspecification and asymptotic semiparametric efficiency under correct
nuisance specification. However, in many applications, complete nuisance
misspecification should be regarded as the norm (or at the very least the
expected default), and thus doubly robust estimators may behave fragilely. In
fact, it has been amply verified empirically that these estimators can perform
poorly when all nuisance functions are misspecified. Here, we first
characterize this phenomenon of double fragility, and then propose a solution
based on adaptive correction clipping (ACC). We argue that our ACC proposal is
safe, in that it inherits the favorable properties of doubly robust estimators
under correct nuisance specification, but its error is guaranteed to be bounded
by a convex combination of the individual nuisance model errors, which prevents
the instability caused by the compounding product of errors of doubly robust
estimators. We also show that our proposal provides valid inference through the
parametric bootstrap when nuisances are well-specified. We showcase the
efficacy of our ACC estimator both through extensive simulations and by
applying it to the analysis of Alzheimer's disease proteomics data.

</details>


### [492] [Modelling non-stationary extremal dependence through a geometric approach](https://arxiv.org/abs/2509.22501)
*C. J. R. Murphy-Barltrop,J. L. Wadsworth,M. de Carvalho,B. D. Youngman*

Main category: stat.ME

TL;DR: 本文将多元极值建模的几何框架扩展到非平稳设置，提出半参数建模框架估计极限集，模拟研究证明其有效性并应用于金融回报数据。


<details>
  <summary>Details</summary>
Motivation: 许多环境和金融数据存在非平稳极值依赖，但多数多元极值模型仅适用于平稳数据。

Method: 先将几何框架扩展到非平稳设置并明确收敛条件，再引入半参数建模框架估计极限集。

Result: 模拟研究显示提出的框架能捕捉多种依赖形式，对不同模型公式具有鲁棒性。

Conclusion: 提出的方法有效，可应用于金融回报数据并有实际用途。

Abstract: Non-stationary extremal dependence, whereby the relationship between the
extremes of multiple variables evolves over time, is commonly observed in many
environmental and financial data sets. However, most multivariate extreme value
models are only suited to stationary data. A recent approach to multivariate
extreme value modelling uses a geometric framework, whereby extremal dependence
features are inferred through the limiting shapes of scaled sample clouds. This
framework can capture a wide range of dependence structures, and a variety of
inference procedures have been proposed in the stationary setting. In this
work, we first extend the geometric framework to the non-stationary setting and
outline assumptions to ensure the necessary convergence conditions hold. We
then introduce a flexible, semi-parametric modelling framework for obtaining
estimates of limit sets in the non-stationary setting. Through rigorous
simulation studies, we demonstrate that our proposed framework can capture a
wide range of dependence forms and is robust to different model formulations.
We illustrate the proposed methods on financial returns data and present
several practical uses.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [493] [EqDiff-CT: Equivariant Conditional Diffusion model for CT Image Synthesis from CBCT](https://arxiv.org/abs/2509.21913)
*Alzahra Altalib,Chunhui Li,Alessandro Perelli*

Main category: physics.med-ph

TL;DR: 本文提出EqDiff - CT模型，从CBCT合成高质量CT图像，经实验验证可提升图像质量与临床信心。


<details>
  <summary>Details</summary>
Motivation: CBCT有伪影影响剂量计算和自适应规划，CT虽图像质量好但无法捕捉治疗中解剖变化，需准确的CBCT到CT合成。

Method: 提出EqDiff - CT模型，用DDPM迭代注入噪声学习潜在表征，采用基于e2cnn可转向层的群等变条件U - Net骨干网络。

Result: 在SynthRAD2025数据集上对比CycleGAN和DDPM等方法，EqDiff - CT在结构保真度、HU准确性和定量指标上有显著提升，视觉效果也更好。

Conclusion: 扩散模型为CBCT改进提供了稳健且通用的框架，有助于提高图像质量和临床信心。

Abstract: Cone-beam computed tomography (CBCT) is widely used for image-guided
radiotherapy (IGRT). It provides real time visualization at low cost and dose.
However, photon scattering and beam hindrance cause artifacts in CBCT. These
include inaccurate Hounsfield Units (HU), reducing reliability for dose
calculation, and adaptive planning. By contrast, computed tomography (CT)
offers better image quality and accurate HU calibration but is usually acquired
offline and fails to capture intra-treatment anatomical changes. Thus, accurate
CBCT-to-CT synthesis is needed to close the imaging-quality gap in adaptive
radiotherapy workflows.
  To cater to this, we propose a novel diffusion-based conditional generative
model, coined EqDiff-CT, to synthesize high-quality CT images from CBCT.
EqDiff-CT employs a denoising diffusion probabilistic model (DDPM) to
iteratively inject noise and learn latent representations that enable
reconstruction of anatomically consistent CT images. A group-equivariant
conditional U-Net backbone, implemented with e2cnn steerable layers, enforces
rotational equivariance (cyclic C4 symmetry), helping preserve fine structural
details while minimizing noise and artifacts.
  The system was trained and validated on the SynthRAD2025 dataset, comprising
CBCT-CT scans across multiple head-and-neck anatomical sites, and we compared
it with advanced methods such as CycleGAN and DDPM. EqDiff-CT provided
substantial gains in structural fidelity, HU accuracy and quantitative metrics.
Visual findings further confirm the improved recovery, sharper soft tissue
boundaries, and realistic bone reconstructions. The findings suggest that the
diffusion model has offered a robust and generalizable framework for CBCT
improvements. The proposed solution helps in improving the image quality as
well as the clinical confidence in the CBCT-guided treatment planning and dose
calculations.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [494] [Reinforcement Learning Based Traffic Signal Design to Minimize Queue Lengths](https://arxiv.org/abs/2509.21745)
*Anirud Nandakumar,Chayan Banerjee,Lelitha Devi Vanajakshi*

Main category: eess.SY

TL;DR: 本文提出基于强化学习的自适应交通信号控制框架，用多种状态表示法应对交通随机性，在减少队列长度上表现优于传统及其他RL方法。


<details>
  <summary>Details</summary>
Motivation: 传统交通信号控制方法难以处理动态交通模式，需要高效的交通信号控制来减少拥堵等问题。

Method: 提出基于近端策略优化（PPO）算法的自适应交通信号控制框架，用多种状态表示法处理随机交通状况，在SUMO模拟器上实现算法。

Result: 算法在减少队列长度上优于传统和其他常规基于RL的方法，最佳配置比传统Webster方法平均队列长度减少约29%，队列奖励法有效。

Conclusion: 该算法有实现可扩展和自适应城市交通管理的潜力。

Abstract: Efficient traffic signal control (TSC) is crucial for reducing congestion,
travel delays, pollution, and for ensuring road safety. Traditional approaches,
such as fixed signal control and actuated control, often struggle to handle
dynamic traffic patterns. In this study, we propose a novel adaptive TSC
framework that leverages Reinforcement Learning (RL), using the Proximal Policy
Optimization (PPO) algorithm, to minimize total queue lengths across all signal
phases. The challenge of efficiently representing highly stochastic traffic
conditions for an RL controller is addressed through multiple state
representations, including an expanded state space, an autoencoder
representation, and a K-Planes-inspired representation. The proposed algorithm
has been implemented using the Simulation of Urban Mobility (SUMO) traffic
simulator and demonstrates superior performance over both traditional methods
and other conventional RL-based approaches in reducing queue lengths. The best
performing configuration achieves an approximately 29% reduction in average
queue lengths compared to the traditional Webster method. Furthermore,
comparative evaluation of alternative reward formulations demonstrates the
effectiveness of the proposed queue-based approach, showcasing the potential
for scalable and adaptive urban traffic management.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [495] [Designing Ethereum's Geographical (De)Centralization Beyond the Atlantic](https://arxiv.org/abs/2509.21475)
*Sen Yang,Burak Öz,Fei Wu,Fan Zhang*

Main category: cs.CR

TL;DR: 研究以太坊协议设计对验证者地理分布的影响，发现不同范式下验证者分布特点及促进地理去中心化的方法。


<details>
  <summary>Details</summary>
Motivation: 传统指标忽略去中心化的地理维度，而验证者运行位置影响系统对区域冲击的弹性和奖励获取公平性，需研究协议设计如何影响验证者激励和地理分布。

Method: 开发延迟校准的基于代理的模型，比较以太坊的单源范式（SSP）和多源范式（MSP）。

Result: SSP围绕中继位置集中但速度较慢，MSP更快中心化；源放置和共识设置可影响效果，多数情况下北美是焦点枢纽。

Conclusion: 协议设计显著影响验证者地理分布，可为促进地理去中心化提供方法。

Abstract: Decentralization has a geographic dimension that conventional metrics such as
stake distribution overlook. Where validators run affects resilience to
regional shocks (outages, disasters, government intervention) and fairness in
reward access. Yet in permissionless systems, locations cannot be mandated, but
they emerge from incentives. Today, Ethereum's validators cluster along the
Atlantic (EU and U.S. East Coast), where latency is structurally favorable.
This raises a key question: when some regions already enjoy latency advantages,
how does protocol design shape validator incentives and the geography of
(de)centralization? We develop a latency-calibrated agent-based model and
compare two Ethereum block-building paradigms: a Single-Source Paradigm (SSP),
akin to MEV-Boost, where proposers fetch full blocks from a relay that also
propagates them; and a Multi-Source Paradigm (MSP), where proposers aggregate
value from multiple sources and broadcast the block themselves. Simulations
show that SSP concentrates around relay placement but more slowly, since
proximity mainly affects propagation, and the marginal value of time is
relatively uniform across regions. MSP centralizes faster: aggregating across
sources makes marginal value location-dependent, amplifying payoff dispersion
and migration toward latency minima. Source placement and consensus settings
can dampen or intensify these effects, though once validators are already
clustered, the impact of source placement on decentralization is marginal. In
most cases, North America consistently emerges as the focal hub. These findings
show that protocol design materially shapes validator geography and offer
levers for promoting geographical decentralization.

</details>


### [496] ["Your AI, My Shell": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors](https://arxiv.org/abs/2509.22040)
*Yue Liu,Yanjie Zhao,Yunbo Lyu,Ting Zhang,Haoyu Wang,David Lo*

Main category: cs.CR

TL;DR: 本文对高权限智能AI代码编辑器的提示注入攻击进行实证分析，实现自动化测试框架AIShellJack评估漏洞，对GitHub Copilot和Cursor评估显示攻击成功率高。


<details>
  <summary>Details</summary>
Motivation: 高权限智能AI代码编辑器带来开发便利同时引发新安全担忧，需分析提示注入攻击情况。

Method: 实现包含314个唯一攻击负载、覆盖70种MITRE ATT&CK框架技术的自动化测试框架AIShellJack进行评估。

Result: 对GitHub Copilot和Cursor评估显示，执行恶意命令攻击成功率高达84%，攻击在多种目标上有效。

Conclusion: 高权限智能AI代码编辑器存在严重提示注入攻击漏洞，威胁安全。

Abstract: Agentic AI coding editors driven by large language models have recently
become more popular due to their ability to improve developer productivity
during software development. Modern editors such as Cursor are designed not
just for code completion, but also with more system privileges for complex
coding tasks (e.g., run commands in the terminal, access development
environments, and interact with external systems). While this brings us closer
to the "fully automated programming" dream, it also raises new security
concerns. In this study, we present the first empirical analysis of prompt
injection attacks targeting these high-privilege agentic AI coding editors. We
show how attackers can remotely exploit these systems by poisoning external
development resources with malicious instructions, effectively hijacking AI
agents to run malicious commands, turning "your AI" into "attacker's shell". To
perform this analysis, we implement AIShellJack, an automated testing framework
for assessing prompt injection vulnerabilities in agentic AI coding editors.
AIShellJack contains 314 unique attack payloads that cover 70 techniques from
the MITRE ATT&CK framework. Using AIShellJack, we conduct a large-scale
evaluation on GitHub Copilot and Cursor, and our evaluation results show that
attack success rates can reach as high as 84% for executing malicious commands.
Moreover, these attacks are proven effective across a wide range of objectives,
ranging from initial access and system discovery to credential theft and data
exfiltration.

</details>


### [497] [Design and Implementation of a Secure RAG-Enhanced AI Chatbot for Smart Tourism Customer Service: Defending Against Prompt Injection Attacks -- A Case Study of Hsinchu, Taiwan](https://arxiv.org/abs/2509.21367)
*Yu-Kai Shih,You-Kai Kang*

Main category: cs.CR

TL;DR: 本文针对新竹智能旅游服务，设计实现安全的RAG聊天机器人，经测试展现良好性能，为智能旅游部署安全聊天机器人提供框架。


<details>
  <summary>Details</summary>
Motivation: 随着智能旅游发展，AI聊天机器人易受提示注入攻击，需要设计安全的聊天机器人保障旅游服务安全。

Method: 将RAG与API函数调用、多层语言分析及防注入护栏相结合，采用分层响应策略、RAG知识接地、意图分解等，还设置防御机制，并用GPT - 5变体进行基准测试。

Result: 用674个对抗性提示和223个良性查询评估，良性任务准确率超95%，能大量检测注入攻击，GPT - 5拦截约85%攻击。

Conclusion: 该工作为智能旅游部署安全聊天机器人提供实用框架，对可持续旅游、多语言可访问性和道德AI部署有贡献。

Abstract: As smart tourism evolves, AI-powered chatbots have become indispensable for
delivering personalized, real-time assistance to travelers while promoting
sustainability and efficiency. However, these systems are increasingly
vulnerable to prompt injection attacks, where adversaries manipulate inputs to
elicit unintended behaviors such as leaking sensitive information or generating
harmful content. This paper presents a case study on the design and
implementation of a secure retrieval-augmented generation (RAG) chatbot for
Hsinchu smart tourism services. The system integrates RAG with API function
calls, multi-layered linguistic analysis, and guardrails against injections,
achieving high contextual awareness and security. Key features include a tiered
response strategy, RAG-driven knowledge grounding, and intent decomposition
across lexical, semantic, and pragmatic levels. Defense mechanisms include
system norms, gatekeepers for intent judgment, and reverse RAG text to
prioritize verified data. We also benchmark a GPT-5 variant (released
2025-08-07) to assess inherent robustness. Evaluations with 674 adversarial
prompts and 223 benign queries show over 95% accuracy on benign tasks and
substantial detection of injection attacks. GPT-5 blocked about 85% of attacks,
showing progress yet highlighting the need for layered defenses. Findings
emphasize contributions to sustainable tourism, multilingual accessibility, and
ethical AI deployment. This work offers a practical framework for deploying
secure chatbots in smart tourism and contributes to resilient, trustworthy AI
applications.

</details>


### [498] [Towards Adapting Federated & Quantum Machine Learning for Network Intrusion Detection: A Survey](https://arxiv.org/abs/2509.21389)
*Devashish Chaudhary,Sutharshan Rajasegarar,Shiva Raj Pokhrel*

Main category: cs.CR

TL;DR: 该综述探讨联邦学习与网络入侵检测系统的集成，分析多种架构与方法，探索量子联邦学习，给出工业应用和研究方向路线图。


<details>
  <summary>Details</summary>
Motivation: 网络安全中需保护敏感流量数据隐私，联邦学习可在分布式设备上协作训练模型并保护隐私，因此研究其与网络入侵检测系统的集成。

Method: 系统分析联邦学习架构、部署策略、通信协议和聚合方法，研究隐私保护技术、模型压缩方法和针对特定攻击的联邦解决方案，探索量子联邦学习相关内容，进行经典与量子方法的比较分析。

Result: 确定研究差距，评估现实部署情况。

Conclusion: 为研究人员和从业者提供增强联邦入侵检测系统隐私、效率和鲁棒性的参考，给出工业应用和未来研究方向的具体路线图。

Abstract: This survey explores the integration of Federated Learning (FL) with Network
Intrusion Detection Systems (NIDS), with particular emphasis on deep learning
and quantum machine learning approaches. FL enables collaborative model
training across distributed devices while preserving data privacy-a critical
requirement in network security contexts where sensitive traffic data cannot be
centralized. Our comprehensive analysis systematically examines the full
spectrum of FL architectures, deployment strategies, communication protocols,
and aggregation methods specifically tailored for intrusion detection. We
provide an in-depth investigation of privacy-preserving techniques, model
compression approaches, and attack-specific federated solutions for threats
including DDoS, MITM, and botnet attacks. The survey further delivers a
pioneering exploration of Quantum FL (QFL), discussing quantum feature
encoding, quantum machine learning algorithms, and quantum-specific aggregation
methods that promise exponential speedups for complex pattern recognition in
network traffic. Through rigorous comparative analysis of classical and quantum
approaches, identification of research gaps, and evaluation of real-world
deployments, we outline a concrete roadmap for industrial adoption and future
research directions. This work serves as an authoritative reference for
researchers and practitioners seeking to enhance privacy, efficiency, and
robustness of federated intrusion detection systems in increasingly complex
network environments, while preparing for the quantum-enhanced cybersecurity
landscape of tomorrow.

</details>


### [499] [MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs](https://arxiv.org/abs/2509.21634)
*Prakhar Sharma,Haohuang Wen,Vinod Yegneswaran,Ashish Gehani,Phillip Porras,Zhiqiang Lin*

Main category: cs.CR

TL;DR: 文章提出用于6G O - RAN环境的MobiLLM框架实现端到端威胁缓解，评估显示其能有效识别和协调策略，减少响应延迟。


<details>
  <summary>Details</summary>
Motivation: 6G网络发展中O - RAN范式开放性扩大攻击面，传统防御不足，现有应用响应能力有限，需新安全解决方案。

Method: 提出MobiLLM框架，通过由大语言模型驱动的模块化多智能体系统编排安全工作流，包含威胁分析、分类和响应智能体。

Result: 初始评估表明MobiLLM能有效识别和协调复杂缓解策略，显著减少响应延迟。

Conclusion: MobiLLM为可信的人工智能驱动的网络安全提供了蓝图，展示了6G自主安全操作的可行性。

Abstract: The evolution toward 6G networks is being accelerated by the Open Radio
Access Network (O-RAN) paradigm -- an open, interoperable architecture that
enables intelligent, modular applications across public telecom and private
enterprise domains. While this openness creates unprecedented opportunities for
innovation, it also expands the attack surface, demanding resilient, low-cost,
and autonomous security solutions. Legacy defenses remain largely reactive,
labor-intensive, and inadequate for the scale and complexity of next-generation
systems. Current O-RAN applications focus mainly on network optimization or
passive threat detection, with limited capability for closed-loop, automated
response.
  To address this critical gap, we present an agentic AI framework for fully
automated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM
orchestrates security workflows through a modular multi-agent system powered by
Large Language Models (LLMs). The framework features a Threat Analysis Agent
for real-time data triage, a Threat Classification Agent that uses
Retrieval-Augmented Generation (RAG) to map anomalies to specific
countermeasures, and a Threat Response Agent that safely operationalizes
mitigation actions via O-RAN control interfaces. Grounded in trusted knowledge
bases such as the MITRE FiGHT framework and 3GPP specifications, and equipped
with robust safety guardrails, MobiLLM provides a blueprint for trustworthy
AI-driven network security. Initial evaluations demonstrate that MobiLLM can
effectively identify and orchestrate complex mitigation strategies,
significantly reducing response latency and showcasing the feasibility of
autonomous security operations in 6G.

</details>


### [500] [Not My Agent, Not My Boundary? Elicitation of Personal Privacy Boundaries in AI-Delegated Information Sharing](https://arxiv.org/abs/2509.21712)
*Bingcan Guo,Eryue Xu,Zhiping Zhang,Tianshi Li*

Main category: cs.CR

TL;DR: 提出AI驱动方法探究个人隐私边界，研究表明沟通角色、AI委托等因素影响隐私披露，强调在真实数据流动中获取隐私偏好的重要性。


<details>
  <summary>Details</summary>
Motivation: 使AI系统与人类隐私偏好对齐，需了解个体细微的隐私披露行为，但获取隐私边界存在挑战。

Method: 采用AI驱动的判别任务探究个人隐私边界，进行组间研究，系统改变沟通角色和委托条件。

Result: 沟通角色影响个体对详细和可识别披露的接受度，AI委托和个体隐私需求增加对披露标识符的敏感度，AI委托导致个体间共识减少。

Conclusion: 强调在真实数据流动中进行隐私偏好获取的重要性，倡导将细微隐私边界作为未来AI系统的对齐目标。

Abstract: Aligning AI systems with human privacy preferences requires understanding
individuals' nuanced disclosure behaviors beyond general norms. Yet eliciting
such boundaries remains challenging due to the context-dependent nature of
privacy decisions and the complex trade-offs involved. We present an AI-powered
elicitation approach that probes individuals' privacy boundaries through a
discriminative task. We conducted a between-subjects study that systematically
varied communication roles and delegation conditions, resulting in 1,681
boundary specifications from 169 participants for 61 scenarios. We examined how
these contextual factors and individual differences influence the boundary
specification. Quantitative results show that communication roles influence
individuals' acceptance of detailed and identifiable disclosure, AI delegation
and individuals' need for privacy heighten sensitivity to disclosed
identifiers, and AI delegation results in less consensus across individuals.
Our findings highlight the importance of situating privacy preference
elicitation within real-world data flows. We advocate using nuanced privacy
boundaries as an alignment goal for future AI systems.

</details>


### [501] [Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models](https://arxiv.org/abs/2509.21761)
*Miao Yu,Zhenhong Zhou,Moayad Aloqaily,Kun Wang,Biwei Huang,Stephen Wang,Yueming Jin,Qingsong Wen*

Main category: cs.CR

TL;DR: 本文通过三方因果分析框架探索大语言模型后门的可解释机制，发现少量注意力头消融可大幅降低攻击成功率，还构建后门向量控制后门。


<details>
  <summary>Details</summary>
Motivation: 微调后的大语言模型易受数据投毒后门攻击，但相关内部机制不明，以往研究忽视后门机制，难以理解和消除威胁。

Method: 采用Backdoor Attribution（BkdAttr）三方因果分析框架，引入Backdoor Probe证明可学习后门特征存在，开发Backdoor Attention Head Attribution（BAHA）定位负责处理特征的注意力头。

Result: 处理后门特征的注意力头较稀疏，消融约3%的总头数可使攻击成功率降低超90%；构建的后门向量通过单点干预可使干净输入攻击成功率接近100%，触发输入攻击成功率接近0%。

Conclusion: 本研究开创了大语言模型后门机制可解释性探索，展示了强大的后门控制方法并为社区提供可操作见解。

Abstract: Fine-tuned Large Language Models (LLMs) are vulnerable to backdoor attacks
through data poisoning, yet the internal mechanisms governing these attacks
remain a black box. Previous research on interpretability for LLM safety tends
to focus on alignment, jailbreak, and hallucination, but overlooks backdoor
mechanisms, making it difficult to understand and fully eliminate the backdoor
threat. In this paper, aiming to bridge this gap, we explore the interpretable
mechanisms of LLM backdoors through Backdoor Attribution (BkdAttr), a
tripartite causal analysis framework. We first introduce the Backdoor Probe
that proves the existence of learnable backdoor features encoded within the
representations. Building on this insight, we further develop Backdoor
Attention Head Attribution (BAHA), efficiently pinpointing the specific
attention heads responsible for processing these features. Our primary
experiments reveals these heads are relatively sparse; ablating a minimal
\textbf{$\sim$ 3%} of total heads is sufficient to reduce the Attack Success
Rate (ASR) by \textbf{over 90%}. More importantly, we further employ these
findings to construct the Backdoor Vector derived from these attributed heads
as a master controller for the backdoor. Through only \textbf{1-point}
intervention on \textbf{single} representation, the vector can either boost ASR
up to \textbf{$\sim$ 100% ($\uparrow$)} on clean inputs, or completely
neutralize backdoor, suppressing ASR down to \textbf{$\sim$ 0% ($\downarrow$)}
on triggered inputs. In conclusion, our work pioneers the exploration of
mechanistic interpretability in LLM backdoors, demonstrating a powerful method
for backdoor control and revealing actionable insights for the community.

</details>


### [502] [You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors](https://arxiv.org/abs/2509.21884)
*Bochuan Cao,Changjiang Li,Yuanpu Cao,Yameng Ge,Ting Wang,Jinghui Chen*

Main category: cs.CR

TL;DR: 本文介绍了一种提示泄露攻击，揭示大语言模型系统提示泄露风险，并提出SysVec方法将系统提示编码为向量以降低泄露风险、提升安全性和模型能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型系统提示存在泄露风险，现有策略对新攻击技术仍脆弱，需寻找根本解决方案。

Method: 先引入提示泄露攻击，后提出SysVec方法，将系统提示编码为内部表示向量。

Result: 实验表明SysVec有效减轻提示泄露攻击，保留模型功能完整性，缓解长上下文场景的遗忘问题。

Conclusion: SysVec方法既能最小化系统提示泄露风险，又能提升模型的指令遵循能力。

Abstract: Large language models (LLMs) have been widely adopted across various
applications, leveraging customized system prompts for diverse tasks. Facing
potential system prompt leakage risks, model developers have implemented
strategies to prevent leakage, primarily by disabling LLMs from repeating their
context when encountering known attack patterns. However, it remains vulnerable
to new and unforeseen prompt-leaking techniques. In this paper, we first
introduce a simple yet effective prompt leaking attack to reveal such risks.
Our attack is capable of extracting system prompts from various LLM-based
application, even from SOTA LLM models such as GPT-4o or Claude 3.5 Sonnet. Our
findings further inspire us to search for a fundamental solution to the
problems by having no system prompt in the context. To this end, we propose
SysVec, a novel method that encodes system prompts as internal representation
vectors rather than raw text. By doing so, SysVec minimizes the risk of
unauthorized disclosure while preserving the LLM's core language capabilities.
Remarkably, this approach not only enhances security but also improves the
model's general instruction-following abilities. Experimental results
demonstrate that SysVec effectively mitigates prompt leakage attacks, preserves
the LLM's functional integrity, and helps alleviate the forgetting issue in
long-context scenarios.

</details>


### [503] [Secure and Efficient Access Control for Computer-Use Agents via Context Space](https://arxiv.org/abs/2509.22256)
*Haochen Gong,Chenxiao Li,Rui Chang,Wenbo Shen*

Main category: cs.CR

TL;DR: 针对基于大语言模型的计算机使用代理的安全风险，提出CSAgent框架，实现高防护率和低性能开销。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的不确定性使计算机使用代理存在安全风险，现有缓解方法有局限性。

Method: 提出基于静态策略的系统级访问控制框架CSAgent，引入意图和上下文感知策略，用优化的操作系统服务执行策略。

Result: 实现并评估CSAgent，能抵御超99.36%的攻击，仅引入6.83%的性能开销。

Conclusion: CSAgent可有效保障计算机使用代理的安全，且性能开销小。

Abstract: Large language model (LLM)-based computer-use agents represent a convergence
of AI and OS capabilities, enabling natural language to control system- and
application-level functions. However, due to LLMs' inherent uncertainty issues,
granting agents control over computers poses significant security risks. When
agent actions deviate from user intentions, they can cause irreversible
consequences. Existing mitigation approaches, such as user confirmation and
LLM-based dynamic action validation, still suffer from limitations in
usability, security, and performance. To address these challenges, we propose
CSAgent, a system-level, static policy-based access control framework for
computer-use agents. To bridge the gap between static policy and dynamic
context and user intent, CSAgent introduces intent- and context-aware policies,
and provides an automated toolchain to assist developers in constructing and
refining them. CSAgent enforces these policies through an optimized OS service,
ensuring that agent actions can only be executed under specific user intents
and contexts. CSAgent supports protecting agents that control computers through
diverse interfaces, including API, CLI, and GUI. We implement and evaluate
CSAgent, which successfully defends against more than 99.36% of attacks while
introducing only 6.83% performance overhead.

</details>


### [504] [A Global Analysis of Cyber Threats to the Energy Sector: "Currents of Conflict" from a Geopolitical Perspective](https://arxiv.org/abs/2509.22280)
*Gustavo Sánchez,Ghada Elbez,Veit Hagenmeyer*

Main category: cs.CR

TL;DR: 本文聚焦能源领域，结合地缘政治、网络威胁情报分析与先进检测技术，利用生成式AI处理网络威胁信息，通过跨数据库对比及评估安全工具，为相关人员提供新见解和可操作信息。


<details>
  <summary>Details</summary>
Motivation: 网络威胁日益频繁复杂，需全面理解，因此聚焦能源领域开展研究。

Method: 利用生成式人工智能处理原始网络威胁描述信息；跨多个数据库进行地缘政治层面的威胁行为体起源和目标区域对比；评估基于学习的网络安全工具检测能源攻击妥协指标的有效性。

Result: 分析得出了新的见解，提供了可操作信息。

Conclusion: 研究为研究人员、政策制定者和网络安全专业人员提供了有价值的信息。

Abstract: The escalating frequency and sophistication of cyber threats increased the
need for their comprehensive understanding. This paper explores the
intersection of geopolitical dynamics, cyber threat intelligence analysis, and
advanced detection technologies, with a focus on the energy domain. We leverage
generative artificial intelligence to extract and structure information from
raw cyber threat descriptions, enabling enhanced analysis. By conducting a
geopolitical comparison of threat actor origins and target regions across
multiple databases, we provide insights into trends within the general threat
landscape. Additionally, we evaluate the effectiveness of cybersecurity tools
-- with particular emphasis on learning-based techniques -- in detecting
indicators of compromise for energy-targeted attacks. This analysis yields new
insights, providing actionable information to researchers, policy makers, and
cybersecurity professionals.

</details>


### [505] [Functional Encryption in Secure Neural Network Training: Data Leakage and Practical Mitigations](https://arxiv.org/abs/2509.21497)
*Alexandru Ioniţă,Andreea Ioniţă*

Main category: cs.CR

TL;DR: 本文提出对用功能加密（FE）进行加密数据安全训练的神经网络的攻击，还给出两种安全训练和推理的解决方案。


<details>
  <summary>Details</summary>
Motivation: 机器学习即服务存在隐私问题，现有基于FE的解决方案有漏洞未被考虑。

Method: 用线性规划重构原始输入进行攻击，提出两种涉及客户端计算阶段的解决方案，一个不依赖加密确保安全，另一个用函数隐藏内积技术。

Result: 提出的攻击可揭示之前的安全承诺，提出两种解决方案应对攻击。

Conclusion: 针对用FE进行安全训练的神经网络存在的安全漏洞，有对应的攻击和解决方案。

Abstract: With the increased interest in artificial intelligence, Machine Learning as a
Service provides the infrastructure in the Cloud for easy training, testing,
and deploying models. However, these systems have a major privacy issue:
uploading sensitive data to the Cloud, especially during training. Therefore,
achieving secure Neural Network training has been on many researchers' minds
lately. More and more solutions for this problem are built around a main
pillar: Functional Encryption (FE). Although these approaches are very
interesting and offer a new perspective on ML training over encrypted data,
some vulnerabilities do not seem to be taken into consideration. In our paper,
we present an attack on neural networks that uses FE for secure training over
encrypted data. Our approach uses linear programming to reconstruct the
original input, unveiling the previous security promises. To address the
attack, we propose two solutions for secure training and inference that involve
the client during the computation phase. One approach ensures security without
relying on encryption, while the other uses function-hiding inner-product
techniques.

</details>


### [506] [SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models](https://arxiv.org/abs/2509.21843)
*Jingkai Guo,Chaitali Chakrabarti,Deliang Fan*

Main category: cs.CR

TL;DR: 本文提出SBFA攻击方法，仅一次比特翻转就能降低大语言模型性能，揭示了SOTA大模型的安全隐患。


<details>
  <summary>Details</summary>
Motivation: 现有Bit - Flip Attacks（BFAs）方法攻击灵活性有限，在浮点模型中随机比特翻转不隐蔽且会导致运行时错误，需新攻击方法。

Method: 提出SBFA攻击，通过定义的ImpactScore指标迭代搜索和排序，结合梯度敏感性和良性层权重分布约束的扰动范围；提出轻量级SKIP搜索算法降低搜索复杂度。

Result: 在Qwen、LLaMA和Gemma模型上，SBFA在BF16和INT8数据格式下仅一次比特翻转就使MMLU和SST - 2的准确率降至随机水平以下。

Conclusion: 仅对数十亿参数中的单个比特进行翻转，就暴露出SOTA大语言模型严重的安全问题。

Abstract: Model integrity of Large language models (LLMs) has become a pressing
security concern with their massive online deployment. Prior Bit-Flip Attacks
(BFAs) -- a class of popular AI weight memory fault-injection techniques -- can
severely compromise Deep Neural Networks (DNNs): as few as tens of bit flips
can degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs
and reveal that, despite the intuition of better robustness from modularity and
redundancy, only a handful of adversarial bit flips can also cause LLMs'
catastrophic accuracy degradation. However, existing BFA methods typically
focus on either integer or floating-point models separately, limiting attack
flexibility. Moreover, in floating-point models, random bit flips often cause
perturbed parameters to extreme values (e.g., flipping in exponent bit), making
it not stealthy and leading to numerical runtime error (e.g., invalid tensor
values (NaN/Inf)). In this work, for the first time, we propose SBFA (Sneaky
Bit-Flip Attack), which collapses LLM performance with only one single bit flip
while keeping perturbed values within benign layer-wise weight distribution. It
is achieved through iterative searching and ranking through our defined
parameter sensitivity metric, ImpactScore, which combines gradient sensitivity
and perturbation range constrained by the benign layer-wise weight
distribution. A novel lightweight SKIP searching algorithm is also proposed to
greatly reduce searching complexity, which leads to successful SBFA searching
taking only tens of minutes for SOTA LLMs. Across Qwen, LLaMA, and Gemma
models, with only one single bit flip, SBFA successfully degrades accuracy to
below random levels on MMLU and SST-2 in both BF16 and INT8 data formats.
Remarkably, flipping a single bit out of billions of parameters reveals a
severe security concern of SOTA LLM models.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [507] [Developing Strategies to Increase Capacity in AI Education](https://arxiv.org/abs/2509.21713)
*Noah Q. Cowit,Sri Yash Tadimalla,Stephanie T. Jones,Mary Lou Maher,Tracy Camp,Enrico Pontelli*

Main category: cs.CY

TL;DR: CRA组织专家讨论AI教育，确定提升教育能力的社区需求，指出基础设施挑战，提出教师专业发展建议并整理资源。


<details>
  <summary>Details</summary>
Motivation: 应对世界对人工智能教育日益增长的需求和重要性，解决各机构在教授AI时面临的问题。

Method: 组织32场由202位专家参与的虚拟圆桌讨论，按机构类型分组。

Result: 确定提升AI教育能力的社区需求，如数字鸿沟导致的基础设施障碍；整理专家提及的资源。

Conclusion: 教师持续专业发展对填补师资缺口至关重要，需要建立AI教育资源中央库。

Abstract: Many institutions are currently grappling with teaching artificial
intelligence (AI) in the face of growing demand and relevance in our world. The
Computing Research Association (CRA) has conducted 32 moderated virtual
roundtable discussions of 202 experts committed to improving AI education.
These discussions slot into four focus areas: AI Knowledge Areas and Pedagogy,
Infrastructure Challenges in AI Education, Strategies to Increase Capacity in
AI Education, and AI Education for All. Roundtables were organized around
institution type to consider the particular goals and resources of different AI
education environments. We identified the following high-level community needs
to increase capacity in AI education. A significant digital divide creates
major infrastructure hurdles, especially for smaller and under-resourced
institutions. These challenges manifest as a shortage of faculty with AI
expertise, who also face limited time for reskilling; a lack of computational
infrastructure for students and faculty to develop and test AI models; and
insufficient institutional technical support. Compounding these issues is the
large burden associated with updating curricula and creating new programs. To
address the faculty gap, accessible and continuous professional development is
crucial for faculty to learn about AI and its ethical dimensions. This support
is particularly needed for under-resourced institutions and must extend to
faculty both within and outside of computing programs to ensure all students
have access to AI education. We have compiled and organized a list of resources
that our participant experts mentioned throughout this study. These resources
contribute to a frequent request heard during the roundtables: a central
repository of AI education resources for institutions to freely use across
higher education.

</details>


### [508] [From Superficial Outputs to Superficial Learning: Risks of Large Language Models in Education](https://arxiv.org/abs/2509.21972)
*Iris Delikoura,Yi. R,Fung,Pan Hui*

Main category: cs.CY

TL;DR: 本文对70项实证研究进行系统综述，分析大语言模型在教育中的应用、影响测量、风险及缓解策略，提出LLM - 风险适应学习模型，为负责任地应用大语言模型提供基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在教育领域带来变革同时引发风险担忧，但相关实证证据分散，需系统研究。

Method: 对计算机科学、教育和心理学领域的70项实证研究进行系统综述，围绕四个研究问题展开分析。

Result: 研究集中在三个领域，模型层面和学习者交互层面存在多种风险，提出LLM - 风险适应学习模型。

Conclusion: 本综述为大语言模型在教育中以人类为中心的负责任应用提供基础。

Abstract: Large Language Models (LLMs) are transforming education by enabling
personalization, feedback, and knowledge access, while also raising concerns
about risks to students and learning systems. Yet empirical evidence on these
risks remains fragmented. This paper presents a systematic review of 70
empirical studies across computer science, education, and psychology. Guided by
four research questions, we examine: (i) which applications of LLMs in
education have been most frequently explored; (ii) how researchers have
measured their impact; (iii) which risks stem from such applications; and (iv)
what mitigation strategies have been proposed. We find that research on LLMs
clusters around three domains: operational effectiveness, personalized
applications, and interactive learning tools. Across these, model-level risks
include superficial understanding, bias, limited robustness, anthropomorphism,
hallucinations, privacy concerns, and knowledge constraints. When learners
interact with LLMs, these risks extend to cognitive and behavioural outcomes,
including reduced neural activity, over-reliance, diminished independent
learning skills, and a loss of student agency. To capture this progression, we
propose an LLM-Risk Adapted Learning Model that illustrates how technical risks
cascade through interaction and interpretation to shape educational outcomes.
As the first synthesis of empirically assessed risks, this review provides a
foundation for responsible, human-centred integration of LLMs in education.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [509] [Fast Rank Adaptive CUR via a Recycled Small Sketch](https://arxiv.org/abs/2509.21963)
*Nathaniel Pritchard,Taejun Park,Yuji Nakatsukasa,Per-Gunnar Martinsson*

Main category: math.NA

TL;DR: 本文介绍了新算法IterativeCUR，用于计算低秩矩阵近似，相比现有方法在速度和适应性上更具优势，实验显示其有显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 传统SVD方法虽高效，但其他低秩近似方法可能有更好性能，且CUR分解在保留矩阵结构等方面有优势，需要开发更优算法。

Method: 借鉴随机数值线性代数的先前工作，提出IterativeCUR算法，具有自适应特性，依赖矩阵的小草图并逐步更新。

Result: 实验表明，IterativeCUR比最先进的基于草图的选主元方法快达4倍，比自适应秩随机SVD方法快达40倍，且不损失精度。

Conclusion: IterativeCUR算法在计算低秩矩阵近似方面具有显著优势，比现有方法更高效。

Abstract: The computation of accurate low-rank matrix approximations is central to
improving the scalability of various techniques in machine learning,
uncertainty quantification, and control. Traditionally, low-rank approximations
are constructed using SVD-based approaches such as truncated SVD or
RandomizedSVD. Although these SVD approaches -- especially RandomizedSVD --
have proven to be very computationally efficient, other low-rank approximation
methods can offer even greater performance. One such approach is the CUR
decomposition, which forms a low-rank approximation using direct row and column
subsets of a matrix. Because CUR uses direct matrix subsets, it is also often
better able to preserve native matrix structures like sparsity or
non-negativity than SVD-based approaches and can facilitate data interpretation
in many contexts. This paper introduces IterativeCUR, which draws on previous
work in randomized numerical linear algebra to build a new algorithm that is
highly competitive compared to prior work: (1) It is adaptive in the sense that
it takes as an input parameter the desired tolerance, rather than an a priori
guess of the numerical rank. (2) It typically runs significantly faster than
both existing CUR algorithms and techniques such as RandomizedSVD, in
particular when these methods are run in an adaptive rank mode. Its asymptotic
complexity is $\mathcal{O}(mn + (m+n)r^2 + r^3)$ for an $m\times n$ matrix of
numerical rank $r$. (3) It relies on a single small sketch from the matrix that
is successively downdated as the algorithm proceeds.
  We demonstrate through extensive experiments that IterativeCUR achieves up to
$4\times$ speed-up over state-of-the-art pivoting-on-sketch approaches with no
loss of accuracy, and up to $40\times$ speed-up over rank-adaptive randomized
SVD approaches.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [510] [Ontological foundations for contrastive explanatory narration of robot plans](https://arxiv.org/abs/2509.22493)
*Alberto Olivares-Alarcos,Sergi Foix,Júlia Borràs,Gerard Canal,Guillem Alenyà*

Main category: cs.RO

TL;DR: 文章聚焦机器人对竞争计划对比建模与推理，提出本体模型和新算法，评估显示新算法解释效果超基线方法。


<details>
  <summary>Details</summary>
Motivation: 确保人机交互中机器人决策的可理解性，实现合理决策并向人类解释。

Method: 提出新本体模型对竞争计划差异进行形式化和推理，研究基线算法局限性并提出新算法。

Result: 新算法构建的解释效果优于基线方法。

Conclusion: 新本体模型和算法有助于机器人解释竞争计划决策结果。

Abstract: Mutual understanding of artificial agents' decisions is key to ensuring a
trustworthy and successful human-robot interaction. Hence, robots are expected
to make reasonable decisions and communicate them to humans when needed. In
this article, the focus is on an approach to modeling and reasoning about the
comparison of two competing plans, so that robots can later explain the
divergent result. First, a novel ontological model is proposed to formalize and
reason about the differences between competing plans, enabling the
classification of the most appropriate one (e.g., the shortest, the safest, the
closest to human preferences, etc.). This work also investigates the
limitations of a baseline algorithm for ontology-based explanatory narration.
To address these limitations, a novel algorithm is presented, leveraging
divergent knowledge between plans and facilitating the construction of
contrastive narratives. Through empirical evaluation, it is observed that the
explanations excel beyond the baseline method.

</details>


### [511] [SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks](https://arxiv.org/abs/2509.21928)
*Jialiang Li,Wenzheng Wu,Gaojing Zhang,Yifan Han,Wenzhao Lian*

Main category: cs.RO

TL;DR: 提出SAGE框架解决长视野操作任务难题，实验表明其在相关任务上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有长视野操作任务的任务规划和目标条件操作方法存在泛化性有限、语义推理不足、难适应新任务等问题，需新方法填补高层符号规划和低层连续控制间的差距。

Method: 提出SAGE框架，利用语义场景图作为场景状态的结构表示，包含基于场景图的任务规划器和分离的结构图像编辑管道。

Result: SAGE在不同长视野任务上取得了最先进的性能。

Conclusion: SAGE框架能有效解决长视野操作任务中高层符号规划和低层连续控制的衔接问题，性能优越。

Abstract: Successfully solving long-horizon manipulation tasks remains a fundamental
challenge. These tasks involve extended action sequences and complex object
interactions, presenting a critical gap between high-level symbolic planning
and low-level continuous control. To bridge this gap, two essential
capabilities are required: robust long-horizon task planning and effective
goal-conditioned manipulation. Existing task planning methods, including
traditional and LLM-based approaches, often exhibit limited generalization or
sparse semantic reasoning. Meanwhile, image-conditioned control methods
struggle to adapt to unseen tasks. To tackle these problems, we propose SAGE, a
novel framework for Scene Graph-Aware Guidance and Execution in Long-Horizon
Manipulation Tasks. SAGE utilizes semantic scene graphs as a structural
representation for scene states. A structural scene graph enables bridging
task-level semantic reasoning and pixel-level visuo-motor control. This also
facilitates the controllable synthesis of accurate, novel sub-goal images. SAGE
consists of two key components: (1) a scene graph-based task planner that uses
VLMs and LLMs to parse the environment and reason about physically-grounded
scene state transition sequences, and (2) a decoupled structural image editing
pipeline that controllably converts each target sub-goal graph into a
corresponding image through image inpainting and composition. Extensive
experiments have demonstrated that SAGE achieves state-of-the-art performance
on distinct long-horizon tasks.

</details>


### [512] [FlowDrive: moderated flow matching with data balancing for trajectory planning](https://arxiv.org/abs/2509.21961)
*Lingguang Wang,Ömer Şahin Taş,Marlon Steiner,Christoph Stiller*

Main category: cs.RO

TL;DR: 本文针对学习型规划器对驾驶数据长尾分布敏感的问题，提出FlowDrive规划器及改进方法，在多个基准测试中取得先进成果。


<details>
  <summary>Details</summary>
Motivation: 学习型规划器受驾驶数据长尾分布影响，常见操作主导数据集，导致模型在关键场景性能下降，需解决数据不平衡问题。

Method: 比较采样训练数据的平衡策略，采用按轨迹模式重新加权；提出FlowDrive规划器，学习条件校正流；引入适度的循环内引导增加轨迹多样性。

Result: FlowDrive在nuPlan和interPlan基准测试中，在学习型规划器中达到了最先进的结果；添加适度引导和轻后处理的FlowDrive*在几乎所有基准测试分割中实现了总体最先进的性能。

Conclusion: 按轨迹模式重新加权是有效的平衡策略，FlowDrive规划器及相关改进方法能提升学习型规划器在关键场景的性能。

Abstract: Learning-based planners are sensitive to the long-tailed distribution of
driving data. Common maneuvers dominate datasets, while dangerous or rare
scenarios are sparse. This imbalance can bias models toward the frequent cases
and degrade performance on critical scenarios. To tackle this problem, we
compare balancing strategies for sampling training data and find reweighting by
trajectory pattern an effective approach. We then present FlowDrive, a
flow-matching trajectory planner that learns a conditional rectified flow to
map noise directly to trajectory distributions with few flow-matching steps. We
further introduce moderated, in-the-loop guidance that injects small
perturbation between flow steps to systematically increase trajectory diversity
while remaining scene-consistent. On nuPlan and the interaction-focused
interPlan benchmarks, FlowDrive achieves state-of-the-art results among
learning-based planners and approaches methods with rule-based refinements.
After adding moderated guidance and light post-processing (FlowDrive*), it
achieves overall state-of-the-art performance across nearly all benchmark
splits.

</details>


### [513] [Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning](https://arxiv.org/abs/2509.21983)
*Sigmund Hennum Høeg,Aksel Vaaler,Chaoqi Liu,Olav Egeland,Yilun Du*

Main category: cs.RO

TL;DR: 现有生成方法用于长周期机器人任务有局限，提出同时生成高层符号计划来增强连续轨迹生成，新混合扩散方法表现佳且能灵活合成轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成方法（如扩散模型）在处理涉及复杂决策的长周期机器人任务时存在困难，易混淆不同行为模式导致任务失败。

Method: 提出同时生成高层符号计划来增强连续轨迹生成，采用离散变量扩散和连续扩散的新混合方式。

Result: 新方法显著优于基线方法。

Conclusion: 混合扩散过程能实现灵活的轨迹合成，可根据部分和完整符号条件来合成动作。

Abstract: Constructing robots to accomplish long-horizon tasks is a long-standing
challenge within artificial intelligence. Approaches using generative methods,
particularly Diffusion Models, have gained attention due to their ability to
model continuous robotic trajectories for planning and control. However, we
show that these models struggle with long-horizon tasks that involve complex
decision-making and, in general, are prone to confusing different modes of
behavior, leading to failure. To remedy this, we propose to augment continuous
trajectory generation by simultaneously generating a high-level symbolic plan.
We show that this requires a novel mix of discrete variable diffusion and
continuous diffusion, which dramatically outperforms the baselines. In
addition, we illustrate how this hybrid diffusion process enables flexible
trajectory synthesis, allowing us to condition synthesized actions on partial
and complete symbolic conditions.

</details>


### [514] [Developing Vision-Language-Action Model from Egocentric Videos](https://arxiv.org/abs/2509.21986)
*Tomoya Yoshida,Shuhei Kurita,Taichi Nishimura,Shinsuke Mori*

Main category: cs.RO

TL;DR: 本文提出EgoScaler框架，从第一人称视角视频提取6DoF物体操作轨迹构建新数据集用于VLA预训练，实验表明该视频数据是推进VLA研究的有前景资源。


<details>
  <summary>Details</summary>
Motivation: 现有基于第一人称视角视频训练机器人策略常依赖辅助注释，不清楚VLA能否直接从原始视频训练，需解决此挑战。

Method: 利用EgoScaler框架从第一人称视角视频提取6DoF物体操作轨迹，应用于四个大规模数据集并自动优化轨迹，构建新数据集用于VLA预训练。

Result: 在模拟和真实机器人环境实验发现，预训练比从头训练任务成功率提高超20%，性能与真实机器人数据集相当，结合两者数据有进一步提升。

Conclusion: 第一人称视角视频是推进VLA研究有前景且可扩展的资源。

Abstract: Egocentric videos capture how humans manipulate objects and tools, providing
diverse motion cues for learning object manipulation. Unlike the costly,
expert-driven manual teleoperation commonly used in training
Vision-Language-Action models (VLAs), egocentric videos offer a scalable
alternative. However, prior studies that leverage such videos for training
robot policies typically rely on auxiliary annotations, such as detailed
hand-pose recordings. Consequently, it remains unclear whether VLAs can be
trained directly from raw egocentric videos. In this work, we address this
challenge by leveraging EgoScaler, a framework that extracts 6DoF object
manipulation trajectories from egocentric videos without requiring auxiliary
recordings. We apply EgoScaler to four large-scale egocentric video datasets
and automatically refine noisy or incomplete trajectories, thereby constructing
a new large-scale dataset for VLA pre-training. Our experiments with a
state-of-the-art $\pi_0$ architecture in both simulated and real-robot
environments yield three key findings: (i) pre-training on our dataset improves
task success rates by over 20\% compared to training from scratch, (ii) the
performance is competitive with that achieved using real-robot datasets, and
(iii) combining our dataset with real-robot data yields further improvements.
These results demonstrate that egocentric videos constitute a promising and
scalable resource for advancing VLA research.

</details>


### [515] [An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose](https://arxiv.org/abs/2509.22058)
*Qifeng Wang,Weigang Li,Lei Nie,Xin Xu,Wenping Liu,Zhe Xu*

Main category: cs.RO

TL;DR: 提出基于可靠初始位姿的自适应ICP激光雷达里程计方法，实验表明该方法优于现有方法，提高了精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于ICP的激光雷达里程计方法未考虑初始位姿可靠性，缺乏自适应机制，导致收敛到局部最优和注册精度下降。

Method: 采用基于密度滤波的分布式粗配准获取初始位姿估计，与运动预测位姿比较选可靠初始位姿；结合当前和历史误差动态调整自适应阈值；基于可靠初始位姿和自适应阈值进行点到平面自适应ICP配准。

Result: 在KITTI数据集上实验，该方法优于现有方法。

Conclusion: 所提方法显著提高了激光雷达里程计的精度。

Abstract: As a key technology for autonomous navigation and positioning in mobile
robots, light detection and ranging (LiDAR) odometry is widely used in
autonomous driving applications. The Iterative Closest Point (ICP)-based
methods have become the core technique in LiDAR odometry due to their efficient
and accurate point cloud registration capability. However, some existing
ICP-based methods do not consider the reliability of the initial pose, which
may cause the method to converge to a local optimum. Furthermore, the absence
of an adaptive mechanism hinders the effective handling of complex dynamic
environments, resulting in a significant degradation of registration accuracy.
To address these issues, this paper proposes an adaptive ICP-based LiDAR
odometry method that relies on a reliable initial pose. First, distributed
coarse registration based on density filtering is employed to obtain the
initial pose estimation. The reliable initial pose is then selected by
comparing it with the motion prediction pose, reducing the initial error
between the source and target point clouds. Subsequently, by combining the
current and historical errors, the adaptive threshold is dynamically adjusted
to accommodate the real-time changes in the dynamic environment. Finally, based
on the reliable initial pose and the adaptive threshold, point-to-plane
adaptive ICP registration is performed from the current frame to the local map,
achieving high-precision alignment of the source and target point clouds.
Extensive experiments on the public KITTI dataset demonstrate that the proposed
method outperforms existing approaches and significantly enhances the accuracy
of LiDAR odometry.

</details>


### [516] [Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation](https://arxiv.org/abs/2509.22093)
*Xiaohuan Pei,Yuxing Chen,Siyu Xu,Yunke Wang,Yuheng Shi,Chang Xu*

Main category: cs.RO

TL;DR: 提出行动感知动态剪枝（ADP）框架，减少计算量和推理延迟，同时保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略机器人操作各阶段视觉冗余差异，而不同操作阶段视觉冗余不同且与动作动态相关。

Method: 提出ADP多模态剪枝框架，集成文本驱动的令牌选择和行动感知轨迹门控，通过门控机制根据动作轨迹调整令牌保留率。

Result: 在LIBERO套件和真实场景实验中，显著减少FLOPs和动作推理延迟，同时保持竞争力的成功率。

Conclusion: ADP为高效机器人策略提供简单插件式方法，提升机器人操作的效率和性能。

Abstract: Robotic manipulation with Vision-Language-Action models requires efficient
inference over long-horizon multi-modal context, where attention to dense
visual tokens dominates computational cost. Existing methods optimize inference
speed by reducing visual redundancy within VLA models, but they overlook the
varying redundancy across robotic manipulation stages. We observe that the
visual token redundancy is higher in coarse manipulation phase than in
fine-grained operations, and is strongly correlated with the action dynamic.
Motivated by this observation, we propose \textbf{A}ction-aware
\textbf{D}ynamic \textbf{P}runing (\textbf{ADP}), a multi-modal pruning
framework that integrates text-driven token selection with action-aware
trajectory gating. Our method introduces a gating mechanism that conditions the
pruning signal on recent action trajectories, using past motion windows to
adaptively adjust token retention ratios in accordance with dynamics, thereby
balancing computational efficiency and perceptual precision across different
manipulation stages. Extensive experiments on the LIBERO suites and diverse
real-world scenarios demonstrate that our method significantly reduces FLOPs
and action inference latency (\textit{e.g.} $1.35 \times$ speed up on
OpenVLA-OFT) while maintaining competitive success rates (\textit{e.g.} 25.8\%
improvements with OpenVLA) compared to baselines, thereby providing a simple
plug-in path to efficient robot policies that advances the efficiency and
performance frontier of robotic manipulation. Our project website is:
\href{https://vla-adp.github.io/}{ADP.com}.

</details>


### [517] [MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training](https://arxiv.org/abs/2509.22199)
*Haoyun Li,Ivan Zhang,Runqi Ouyang,Xiaofeng Wang,Zheng Zhu,Zhiqin Yang,Zhentao Zhang,Boyuan Wang,Chaojun Ni,Wenkang Qin,Xinze Chen,Yun Ye,Guan Huang,Zhenbo Song,Xingang Wang*

Main category: cs.RO

TL;DR: 提出MimicDreamer框架将人类演示视频转化为机器人可用监督，提升VLA模型在机器人上的表现。


<details>
  <summary>Details</summary>
Motivation: 收集具身机器人交互数据成本高，人类演示视频虽易收集但与机器人执行视频存在领域差距，需弥合该差距。

Method: 提出MimicDreamer框架，包括用于视觉对齐的H2R Aligner、用于视点稳定的EgoStabilizer和用于动作对齐的方法。

Result: 仅在合成的人转机器人视频上训练的VLA模型能在真实机器人上实现少样本执行，相比仅用真实机器人数据训练的模型，平均成功率提高14.7%。

Conclusion: MimicDreamer框架有效，利用人类数据扩展训练可显著提升VLA模型性能。

Abstract: Vision Language Action (VLA) models derive their generalization capability
from diverse training data, yet collecting embodied robot interaction data
remains prohibitively expensive. In contrast, human demonstration videos are
far more scalable and cost-efficient to collect, and recent studies confirm
their effectiveness in training VLA models. However, a significant domain gap
persists between human videos and robot-executed videos, including unstable
camera viewpoints, visual discrepancies between human hands and robotic arms,
and differences in motion dynamics. To bridge this gap, we propose
MimicDreamer, a framework that turns fast, low-cost human demonstrations into
robot-usable supervision by jointly aligning vision, viewpoint, and actions to
directly support policy training. For visual alignment, we propose H2R Aligner,
a video diffusion model that generates high-fidelity robot demonstration videos
by transferring motion from human manipulation footage. For viewpoint
stabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos
via homography and inpaints occlusions and distortions caused by warping. For
action alignment, we map human hand trajectories to the robot frame and apply a
constrained inverse kinematics solver to produce feasible, low-jitter joint
commands with accurate pose tracking. Empirically, VLA models trained purely on
our synthesized human-to-robot videos achieve few-shot execution on real
robots. Moreover, scaling training with human data significantly boosts
performance compared to models trained solely on real robot data; our approach
improves the average success rate by 14.7\% across six representative
manipulation tasks.

</details>


### [518] [Leveraging Large Language Models for Robot-Assisted Learning of Morphological Structures in Preschool Children with Language Vulnerabilities](https://arxiv.org/abs/2509.22287)
*Stina Sundstedt,Mattias Wingren,Susanne Hägglund,Daniel Ventus*

Main category: cs.RO

TL;DR: 针对语言有弱点的学龄前儿童，多专业团队开发了Furhat对话机器人玩“Alias”游戏提升其语言技能，利用大语言模型管理游戏等，下一步将让机器人生成特定形态目标，长期目标是创建基于大语言模型的多语言学习干预方案。


<details>
  <summary>Details</summary>
Motivation: 语言有弱点的学龄前儿童需支持提升表达语言技能，传统方法对教育者和家长要求高，需新解决方案。

Method: 开发应用让Furhat对话机器人与儿童玩“Alias”游戏，用大语言模型管理游戏、对话、情感回应和轮流发言，后续将让机器人在游戏中生成和提供特定形态目标。

Result: 文中未提及应用目前具体成果。

Conclusion: 假设机器人在该任务上能超越人类，该方法能为儿童和专业人士提供示范和辅导，支持有语言弱点儿童的基本沟通需求，长期可创建多语言学习干预方案。

Abstract: Preschool children with language vulnerabilities -- such as developmental
language disorders or immigration related language challenges -- often require
support to strengthen their expressive language skills. Based on the principle
of implicit learning, speech-language therapists (SLTs) typically embed target
morphological structures (e.g., third person -s) into everyday interactions or
game-based learning activities. Educators are recommended by SLTs to do the
same. This approach demands precise linguistic knowledge and real-time
production of various morphological forms (e.g., "Daddy wears these when he
drives to work"). The task becomes even more demanding when educators or parent
also must keep children engaged and manage turn-taking in a game-based
activity. In the TalBot project our multiprofessional team have developed an
application in which the Furhat conversational robot plays the word retrieval
game "Alias" with children to improve language skills. Our application
currently employs a large language model (LLM) to manage gameplay, dialogue,
affective responses, and turn-taking. Our next step is to further leverage the
capacity of LLMs so the robot can generate and deliver specific morphological
targets during the game. We hypothesize that a robot could outperform humans at
this task. Novel aspects of this approach are that the robot could ultimately
serve as a model and tutor for both children and professionals and that using
LLM capabilities in this context would support basic communication needs for
children with language vulnerabilities. Our long-term goal is to create a
robust LLM-based Robot-Assisted Language Learning intervention capable of
teaching a variety of morphological structures across different languages.

</details>


### [519] [An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics](https://arxiv.org/abs/2509.22434)
*Margherita Martorana,Francesca Urgese,Ilaria Tiddi,Stefan Schlobach*

Main category: cs.RO

TL;DR: 现有机器人框架和本体存在不足，本文提出OntoBOT本体，统一多方面表示，支持推理和知识共享，并在四个代理上验证其通用性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人解决方案与特定平台耦合，缺乏互操作性和知识共享，现有本体未完全涵盖多方面联系，需要改进。

Method: 提出OntoBOT本体，扩展现有本体，统一任务、动作、环境和能力的表示。

Result: 通过在四个具身代理上评估能力问题，证明OntoBOT能实现上下文感知推理、面向任务的执行和知识共享。

Conclusion: OntoBOT本体可有效支持服务机器人的任务执行、推理和知识共享，具有良好的通用性。

Abstract: Personal service robots are increasingly used in domestic settings to assist
older adults and people requiring support. Effective operation involves not
only physical interaction but also the ability to interpret dynamic
environments, understand tasks, and choose appropriate actions based on
context. This requires integrating both hardware components (e.g. sensors,
actuators) and software systems capable of reasoning about tasks, environments,
and robot capabilities. Frameworks such as the Robot Operating System (ROS)
provide open-source tools that help connect low-level hardware with
higher-level functionalities. However, real-world deployments remain tightly
coupled to specific platforms. As a result, solutions are often isolated and
hard-coded, limiting interoperability, reusability, and knowledge sharing.
Ontologies and knowledge graphs offer a structured way to represent tasks,
environments, and robot capabilities. Existing ontologies, such as the
Socio-physical Model of Activities (SOMA) and the Descriptive Ontology for
Linguistic and Cognitive Engineering (DOLCE), provide models for activities,
spatial relationships, and reasoning structures. However, they often focus on
specific domains and do not fully capture the connection between environment,
action, robot capabilities, and system-level integration. In this work, we
propose the Ontology for roBOts and acTions (OntoBOT), which extends existing
ontologies to provide a unified representation of tasks, actions, environments,
and capabilities. Our contributions are twofold: (1) we unify these aspects
into a cohesive ontology to support formal reasoning about task execution, and
(2) we demonstrate its generalizability by evaluating competency questions
across four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how
OntoBOT enables context-aware reasoning, task-oriented execution, and knowledge
sharing in service robotics.

</details>


### [520] [See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation](https://arxiv.org/abs/2509.22653)
*Chih Yao Hu,Yang-Sen Lin,Yuna Lee,Chih-Hai Su,Jie-Ying Lee,Shr-Ruei Tsai,Chin-Yang Lin,Kuan-Wen Chen,Tsung-Wei Ke,Yu-Lun Liu*

Main category: cs.RO

TL;DR: 提出无训练的空中视觉语言导航框架SPF，将动作预测视为2D空间定位任务，在模拟和现实评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的方法将动作预测视为文本生成任务，本文旨在提出新方法解决空中视觉语言导航问题。

Method: 将AVLN的动作预测视为2D空间定位任务，利用VLM将语言指令分解为图像上的2D航点迭代标注，转化为3D位移向量作为动作指令，闭环控制导航。

Result: 在DRL模拟基准中刷新最优成绩，超之前最佳方法63%；在大量现实评估中大幅超越强基线。

Conclusion: SPF设计有效，对不同VLM有出色泛化能力。

Abstract: We present See, Point, Fly (SPF), a training-free aerial vision-and-language
navigation (AVLN) framework built atop vision-language models (VLMs). SPF is
capable of navigating to any goal based on any type of free-form instructions
in any kind of environment. In contrast to existing VLM-based approaches that
treat action prediction as a text generation task, our key insight is to
consider action prediction for AVLN as a 2D spatial grounding task. SPF
harnesses VLMs to decompose vague language instructions into iterative
annotation of 2D waypoints on the input image. Along with the predicted
traveling distance, SPF transforms predicted 2D waypoints into 3D displacement
vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the
traveling distance to facilitate more efficient navigation. Notably, SPF
performs navigation in a closed-loop control manner, enabling UAVs to follow
dynamic targets in dynamic environments. SPF sets a new state of the art in DRL
simulation benchmark, outperforming the previous best method by an absolute
margin of 63%. In extensive real-world evaluations, SPF outperforms strong
baselines by a large margin. We also conduct comprehensive ablation studies to
highlight the effectiveness of our design choice. Lastly, SPF shows remarkable
generalization to different VLMs. Project page: https://spf-web.pages.dev

</details>


### [521] [Generating Stable Placements via Physics-guided Diffusion Models](https://arxiv.org/abs/2509.21664)
*Philippe Nadeau,Miguel Rogel,Ivan Bilić,Ivan Petrović,Jonathan Kelly*

Main category: cs.RO

TL;DR: 提出将稳定性集成到扩散模型采样过程的方法，可直接应用于现成模型，实验显示更稳健且减少运行时间。


<details>
  <summary>Details</summary>
Motivation: 现有评估物体稳定放置的方法依赖模拟引擎或启发式外观评估，本文旨在解决多物体场景中物体稳定放置的挑战。

Method: 将稳定性集成到扩散模型采样过程，查询离线采样规划器收集多模态放置标签训练扩散模型，结合几何感知先验和稳定性感知损失。

Result: 在四个基准场景评估，物理引导模型实现的放置对强力扰动的稳健性比现有几何方法高56%，运行时间减少47%。

Conclusion: 该方法无需额外再训练或微调，可直接应用于现成模型，能有效解决多物体场景中物体稳定放置问题。

Abstract: Stably placing an object in a multi-object scene is a fundamental challenge
in robotic manipulation, as placements must be penetration-free, establish
precise surface contact, and result in a force equilibrium. To assess
stability, existing methods rely on running a simulation engine or resort to
heuristic, appearance-based assessments. In contrast, our approach integrates
stability directly into the sampling process of a diffusion model. To this end,
we query an offline sampling-based planner to gather multi-modal placement
labels and train a diffusion model to generate stable placements. The diffusion
model is conditioned on scene and object point clouds, and serves as a
geometry-aware prior. We leverage the compositional nature of score-based
generative models to combine this learned prior with a stability-aware loss,
thereby increasing the likelihood of sampling from regions of high stability.
Importantly, this strategy requires no additional re-training or fine-tuning,
and can be directly applied to off-the-shelf models. We evaluate our method on
four benchmark scenes where stability can be accurately computed. Our
physics-guided models achieve placements that are 56% more robust to forceful
perturbations while reducing runtime by 47% compared to a state-of-the-art
geometric method.

</details>


### [522] [Learnable Conformal Prediction with Context-Aware Nonconformity Functions for Robotic Planning and Perception](https://arxiv.org/abs/2509.21955)
*Divake Kumar,Sina Tayebati,Francesco Migliarba,Ranganath Krishnan,Amit Ranjan Trivedi*

Main category: cs.RO

TL;DR: 论文提出可学习的共形预测（LCP）方法，在多项机器人任务中表现优于标准共形预测和集成基线，轻量级且适合资源受限系统。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在机器人领域输出的置信度校准不佳，标准共形预测依赖固定非一致性分数，忽略上下文。

Method: 用轻量级神经函数替代固定分数，利用几何、语义和特定任务特征生成上下文感知的不确定性集合。

Result: LCP在分类、检测、路径规划等任务中表现出色，减小预测集大小、收紧检测区间、提高路径规划安全性，且轻量级，支持在线适应。

Conclusion: LCP方法有效，适合资源受限的自主系统。

Abstract: Deep learning models in robotics often output point estimates with poorly
calibrated confidences, offering no native mechanism to quantify predictive
reliability under novel, noisy, or out-of-distribution inputs. Conformal
prediction (CP) addresses this gap by providing distribution-free coverage
guarantees, yet its reliance on fixed nonconformity scores ignores context and
can yield intervals that are overly conservative or unsafe. We address this
with Learnable Conformal Prediction (LCP), which replaces fixed scores with a
lightweight neural function that leverages geometric, semantic, and
task-specific features to produce context-aware uncertainty sets.
  LCP maintains CP's theoretical guarantees while reducing prediction set sizes
by 18% in classification, tightening detection intervals by 52%, and improving
path planning safety from 72% to 91% success with minimal overhead. Across
three robotic tasks on seven benchmarks, LCP consistently outperforms Standard
CP and ensemble baselines. In classification on CIFAR-100 and ImageNet, it
achieves smaller set sizes (4.7-9.9% reduction) at target coverage. For object
detection on COCO, BDD100K, and Cityscapes, it produces 46-54% tighter bounding
boxes. In path planning through cluttered environments, it improves success to
91.5% with only 4.5% path inflation, compared to 12.2% for Standard CP.
  The method is lightweight (approximately 4.8% runtime overhead, 42 KB memory)
and supports online adaptation, making it well suited to resource-constrained
autonomous systems. Hardware evaluation shows LCP adds less than 1% memory and
15.9% inference overhead, yet sustains 39 FPS on detection tasks while being
7.4 times more energy-efficient than ensembles.

</details>
