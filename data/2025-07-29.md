<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 51]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.DS](#cs.DS) [Total: 9]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.LG](#cs.LG) [Total: 140]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 31]
- [q-fin.PM](#q-fin.PM) [Total: 4]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.ET](#cs.ET) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [math.AG](#math.AG) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.CV](#cs.CV) [Total: 43]
- [econ.GN](#econ.GN) [Total: 7]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [cs.SD](#cs.SD) [Total: 6]
- [cs.CL](#cs.CL) [Total: 33]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.RO](#cs.RO) [Total: 12]
- [cs.CY](#cs.CY) [Total: 10]
- [eess.IV](#eess.IV) [Total: 7]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [econ.TH](#econ.TH) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.HC](#cs.HC) [Total: 12]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [stat.ME](#stat.ME) [Total: 5]
- [math.ST](#math.ST) [Total: 2]
- [math.OC](#math.OC) [Total: 4]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.GR](#cs.GR) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation](https://arxiv.org/abs/2507.19489)
*Simone Bendazzoli,Sanna Persson,Mehdi Astaraki,Sebastian Pettersson,Vitali Grozman,Rodrigo Moreno*

Main category: cs.AI

TL;DR: 介绍开源平台MAIA促进临床、研究和开发人员跨学科协作，加速AI研究转化为临床解决方案。


<details>
  <summary>Details</summary>
Motivation: 人工智能融入临床工作流需强大协作平台，以弥合技术创新与医疗应用差距。

Method: 构建基于Kubernetes的MAIA平台，提供模块化、可扩展环境及集成工具。

Result: MAIA支持医学影像AI实际用例，在学术和临床环境部署，通过不同项目展示其应用。

Conclusion: MAIA促进协作和互操作性，加速AI研究转化，推动可重复性、透明度和以用户为中心设计。

Abstract: The integration of Artificial Intelligence (AI) into clinical workflows
requires robust collaborative platforms that are able to bridge the gap between
technical innovation and practical healthcare applications. This paper
introduces MAIA (Medical Artificial Intelligence Assistant), an open-source
platform designed to facilitate interdisciplinary collaboration among
clinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a
modular, scalable environment with integrated tools for data management, model
development, annotation, deployment, and clinical feedback. Key features
include project isolation, CI/CD automation, integration with high-computing
infrastructures and in clinical workflows. MAIA supports real-world use cases
in medical imaging AI, with deployments in both academic and clinical
environments. By promoting collaborations and interoperability, MAIA aims to
accelerate the translation of AI research into impactful clinical solutions
while promoting reproducibility, transparency, and user-centered design. We
showcase the use of MAIA with different projects, both at KTH Royal Institute
of Technology and Karolinska University Hospital.

</details>


### [2] [Agent WARPP: Workflow Adherence via Runtime Parallel Personalization](https://arxiv.org/abs/2507.19543)
*Maria Emilia Mazzolenis,Ruirui Zhang*

Main category: cs.AI

TL;DR: 提出训练无关的模块化框架WARPP提升基于大语言模型的任务导向对话系统工作流遵循能力，经评估表现优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用于任务导向对话系统时，在涉及外部工具调用和依赖用户特定信息的长条件工作流中表现不佳。

Method: 提出WARPP框架，结合多智能体编排和运行时个性化，通过根据用户属性动态修剪条件分支，采用并行架构，由专用个性化智能体和特定领域智能体实时调整执行路径。

Result: 在三个领域的五种不同复杂度用户意图上评估，结果显示WARPP优于非个性化方法和ReAct基线，随着意图复杂度增加，在参数保真度和工具准确性上优势增大，还减少了平均令牌使用量。

Conclusion: WARPP无需额外训练，能有效提升基于大语言模型的任务导向对话系统工作流遵循能力。

Abstract: Large language models (LLMs) are increasingly applied in task-oriented
dialogue (TOD) systems but often struggle with long, conditional workflows that
involve external tool calls and depend on user-specific information. We present
Workflow Adherence via Runtime Parallel Personalization, or WARPP, a
training-free, modular framework that combines multi-agent orchestration with
runtime personalization to improve workflow adherence in LLM-based systems. By
dynamically pruning conditional branches based on user attributes, the
framework reduces reasoning overhead and narrows tool selection at runtime.
WARPP deploys a parallelized architecture where a dedicated Personalizer agent
operates alongside modular, domain-specific agents to dynamically tailor
execution paths in real time. The framework is evaluated across five
representative user intents of varying complexity within three domains:
banking, flights, and healthcare. Our evaluation leverages synthetic datasets
and LLM-powered simulated users to test scenarios with conditional
dependencies. Our results demonstrate that WARPP outperforms both the
non-personalized method and the ReAct baseline, achieving increasingly larger
gains in parameter fidelity and tool accuracy as intent complexity grows, while
also reducing average token usage, without any additional training.

</details>


### [3] [Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems](https://arxiv.org/abs/2507.19593)
*Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis*

Main category: cs.AI

TL;DR: 本文对超博弈理论在多智能体系统中的应用进行系统综述，分析44项研究，提出兼容性标准和分类框架，揭示应用倾向与结构差距，给出研究新方向。


<details>
  <summary>Details</summary>
Motivation: 经典博弈论模型假设在现实多智能体系统中常不成立，超博弈理论可克服这些局限，需要对其在多智能体系统中的应用进行系统研究。

Method: 选取44项来自不同领域的研究，介绍超博弈理论及其扩展，制定智能体兼容性标准和分类框架进行分析。

Result: 发现欺骗推理中分层和基于图的模型较普遍，实际应用简化了理论框架；存在HNF模型应用少、缺乏正式超博弈语言等结构差距。

Conclusion: 本综述为超博弈理论在动态多智能体环境中应用提供新路线图，可提高战略建模的真实性和有效性。

Abstract: Classical game-theoretic models typically assume rational agents, complete
information, and common knowledge of payoffs - assumptions that are often
violated in real-world MAS characterized by uncertainty, misaligned
perceptions, and nested beliefs. To overcome these limitations, researchers
have proposed extensions that incorporate models of cognitive constraints,
subjective beliefs, and heterogeneous reasoning. Among these, hypergame theory
extends the classical paradigm by explicitly modeling agents' subjective
perceptions of the strategic scenario, known as perceptual games, in which
agents may hold divergent beliefs about the structure, payoffs, or available
actions. We present a systematic review of agent-compatible applications of
hypergame theory, examining how its descriptive capabilities have been adapted
to dynamic and interactive MAS contexts. We analyze 44 selected studies from
cybersecurity, robotics, social simulation, communications, and general
game-theoretic modeling. Building on a formal introduction to hypergame theory
and its two major extensions - hierarchical hypergames and HNF - we develop
agent-compatibility criteria and an agent-based classification framework to
assess integration patterns and practical applicability. Our analysis reveals
prevailing tendencies, including the prevalence of hierarchical and graph-based
models in deceptive reasoning and the simplification of extensive theoretical
frameworks in practical applications. We identify structural gaps, including
the limited adoption of HNF-based models, the lack of formal hypergame
languages, and unexplored opportunities for modeling human-agent and
agent-agent misalignment. By synthesizing trends, challenges, and open research
directions, this review provides a new roadmap for applying hypergame theory to
enhance the realism and effectiveness of strategic modeling in dynamic
multi-agent environments.

</details>


### [4] [DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference](https://arxiv.org/abs/2507.19608)
*Jiawen Qi,Chang Gao,Zhaochun Ren,Qinyu Chen*

Main category: cs.AI

TL;DR: 提出DeltaLLM框架，利用注意力模式的时间稀疏性，在资源受限边缘设备上实现高效大语言模型推理，实验表明其可提升稀疏性且对准确率影响小。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在边缘设备部署因计算量随序列长度二次增加而具挑战性，现有动态注意力剪枝研究不适用于边缘场景。

Method: 提出训练无关的DeltaLLM框架，引入精度和内存感知的增量矩阵构造策略和上下文感知的混合注意力机制。

Result: 在BitNet和Llama模型的多种语言任务中，能提升注意力稀疏性，对准确率有一定提升或仅有微小下降。

Conclusion: DeltaLLM为高效边缘部署提供了有前景的解决方案，无需微调且可无缝集成到现有推理管道。

Abstract: Deploying Large Language Models (LLMs) on edge devices remains challenging
due to their quadratically increasing computations with the sequence length.
Existing studies for dynamic attention pruning are designed for hardware with
massively parallel computation capabilities, such as GPUs or TPUs, and aim at
long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We
present DeltaLLM, a training-free framework that exploits temporal sparsity in
attention patterns to enable efficient LLM inference across both the prefilling
and decoding stages, on resource-constrained edge devices. DeltaLLM introduces
an accuracy- and memory-aware delta matrix construction strategy that
introduces temporal sparsity, and a context-aware hybrid attention mechanism
that combines full attention in a local context window with delta approximation
outside it to increase accuracy. We evaluate our framework on the
edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model
across diverse language tasks. The results show that on BitNet, our framework
increases the attention sparsity from 0% to 60% during the prefilling stage
with slight accuracy improvement on the WG task, and 0% to 57% across both the
prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97
on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity
during the prefilling stage and around 57% across both stages with negligible
accuracy drop. These results demonstrate that DeltaLLM offers a promising
solution for efficient edge deployment, requiring no fine-tuning and seamlessly
integrating with existing inference pipelines.

</details>


### [5] [Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges](https://arxiv.org/abs/2507.19672)
*Haoran Lu,Luyang Fang,Ruidong Zhang,Xinliang Li,Jiazhang Cai,Huimin Cheng,Lin Tang,Ziyu Liu,Zeliang Sun,Tao Wang,Yingchuan Zhang,Arif Hassan Zidan,Jinwen Xu,Jincheng Yu,Meizhi Yu,Hanqi Jiang,Xilin Gong,Weidi Luo,Bolun Sun,Yongkai Chen,Terry Ma,Shushan Wu,Yifan Zhou,Junhao Chen,Haotian Xiang,Jing Zhang,Afrar Jahin,Wei Ruan,Ke Deng,Yi Pan,Peilong Wang,Jiahui Li,Zhengliang Liu,Lu Zhang,Lin Zhao,Wei Liu,Dajiang Zhu,Xin Xing,Fei Dou,Wei Zhang,Chao Huang,Rongjie Liu,Mengrui Zhang,Yiwen Liu,Xiaoxiao Sun,Qin Lu,Zhen Xiang,Wenxuan Zhong,Tianming Liu,Ping Ma*

Main category: cs.AI

TL;DR: 文章全面综述大语言模型对齐的实用技术、训练协议和实证发现，分析不同范式下对齐方法发展，讨论先进技术，回顾评估框架和数据集，总结领先AI实验室策略，最后指出开放问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型已融入社会多方面，确保其与人类价值观和意图对齐是关键挑战，因此进行相关综述。

Method: 分析不同范式下对齐方法的发展，讨论先进技术，回顾评估框架和数据集，总结领先AI实验室策略。

Result: 监督微调可实现基本指令遵循，基于偏好的方法在与细微人类意图对齐方面更灵活；讨论的先进技术能平衡质量和效率；现有评估框架和数据集存在奖励错误指定、分布鲁棒性和可扩展监督等局限。

Conclusion: 指出在监督、价值多元性、鲁棒性和持续对齐方面存在开放问题，为研究人员和从业者提供参考。

Abstract: Due to the remarkable capabilities and growing impact of large language
models (LLMs), they have been deeply integrated into many aspects of society.
Thus, ensuring their alignment with human values and intentions has emerged as
a critical challenge. This survey provides a comprehensive overview of
practical alignment techniques, training protocols, and empirical findings in
LLM alignment. We analyze the development of alignment methods across diverse
paradigms, characterizing the fundamental trade-offs between core alignment
objectives. Our analysis shows that while supervised fine-tuning enables basic
instruction-following, preference-based methods offer more flexibility for
aligning with nuanced human intent. We discuss state-of-the-art techniques,
including Direct Preference Optimization (DPO), Constitutional AI,
brain-inspired methods, and alignment uncertainty quantification (AUQ),
highlighting their approaches to balancing quality and efficiency. We review
existing evaluation frameworks and benchmarking datasets, emphasizing
limitations such as reward misspecification, distributional robustness, and
scalable oversight. We summarize strategies adopted by leading AI labs to
illustrate the current state of practice. We conclude by outlining open
problems in oversight, value pluralism, robustness, and continuous alignment.
This survey aims to inform both researchers and practitioners navigating the
evolving landscape of LLM alignment.

</details>


### [6] [The wall confronting large language models](https://arxiv.org/abs/2507.19703)
*Peter V. Coveney,Sauro Succi*

Main category: cs.AI

TL;DR: 大语言模型的缩放定律限制其提升预测不确定性的能力，学习与准确性存在矛盾，避免退化AI需重视问题结构特征。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型难以提升预测可靠性以满足科学探究标准的原因。

Method: 分析大语言模型的缩放定律、产生非高斯输出分布机制以及数据集中的虚假相关性。

Result: 发现缩放定律限制模型能力，学习机制可能导致错误累积等问题，虚假相关性加剧矛盾。

Conclusion: 虽退化AI可能出现，但并非不可避免，需重视问题结构特征的洞察和理解。

Abstract: We show that the scaling laws which determine the performance of large
language models (LLMs) severely limit their ability to improve the uncertainty
of their predictions. As a result, raising their reliability to meet the
standards of scientific inquiry is intractable by any reasonable measure. We
argue that the very mechanism which fuels much of the learning power of LLMs,
namely the ability to generate non-Gaussian output distributions from Gaussian
input ones, might well be at the roots of their propensity to produce error
pileup, ensuing information catastrophes and degenerative AI behaviour. This
tension between learning and accuracy is a likely candidate mechanism
underlying the observed low values of the scaling components. It is
substantially compounded by the deluge of spurious correlations pointed out by
Calude and Longo which rapidly increase in any data set merely as a function of
its size, regardless of its nature. The fact that a degenerative AI pathway is
a very probable feature of the LLM landscape does not mean that it must
inevitably arise in all future AI research. Its avoidance, which we also
discuss in this paper, necessitates putting a much higher premium on insight
and understanding of the structural characteristics of the problems being
investigated.

</details>


### [7] [Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors](https://arxiv.org/abs/2507.19725)
*Leonardo Villalobos-Arias,Grant Forbes,Jianxun Wang,David L Roberts,Arnav Jhala*

Main category: cs.AI

TL;DR: 研究评估三种IM技术对MiniGrid环境中RL智能体行为的影响，对比GRM，发现IM会改变智能体行为，GRM可缓解奖励破解问题。


<details>
  <summary>Details</summary>
Motivation: 游戏中奖励稀疏，IM方法可解决该问题，但会导致奖励破解，且其对RL智能体行为的影响未知，因此进行研究。

Method: 对MiniGrid游戏环境中的三种IM技术进行实证评估，并与GRM方法对比。

Result: IM增加初始奖励并改变智能体游戏方式，GRM在某些场景下缓解奖励破解。

Conclusion: IM会显著改变智能体行为，GRM对缓解奖励破解有一定作用。

Abstract: Games are challenging for Reinforcement Learning~(RL) agents due to their
reward-sparsity, as rewards are only obtainable after long sequences of
deliberate actions. Intrinsic Motivation~(IM) methods -- which introduce
exploration rewards -- are an effective solution to reward-sparsity. However,
IM also causes an issue known as `reward hacking' where the agent optimizes for
the new reward at the expense of properly playing the game. The larger problem
is that reward hacking itself is largely unknown; there is no answer to
whether, and to what extent, IM rewards change the behavior of RL agents. This
study takes a first step by empirically evaluating the impact on behavior of
three IM techniques on the MiniGrid game-like environment. We compare these IM
models with Generalized Reward Matching~(GRM), a method that can be used with
any intrinsic reward function to guarantee optimality. Our results suggest that
IM causes noticeable change by increasing the initial rewards, but also
altering the way the agent plays; and that GRM mitigated reward hacking in some
scenarios.

</details>


### [8] [Integrating Activity Predictions in Knowledge Graphs](https://arxiv.org/abs/2507.19733)
*Alec Scully,Cameron Stockton,Forrest Hare*

Main category: cs.AI

TL;DR: 论文探讨本体结构知识图谱对未来事件预测的作用，利用BFO和CCO组织数据构建马尔可夫链模型，引入术语完善语义，批判概率本体模型并提出新观点，将概率计算融入知识图谱。


<details>
  <summary>Details</summary>
Motivation: 探究本体结构知识图谱在未来事件预测中的作用，批判现有概率本体模型。

Method: 借助BFO和CCO组织和检索数据，构建马尔可夫链模型，引入“spatiotemporal instant”完善语义，提出概率新观点并将计算融入知识图谱。

Result: 成功利用知识图谱数据构建马尔可夫链模型进行预测，提出概率新观点并实现概率计算融入知识图谱。

Conclusion: 本体结构知识图谱在未来事件预测中有重要作用，新的概率观点能更好捕捉现实现象动态，融入概率计算利于进一步分析和决策。

Abstract: We argue that ontology-structured knowledge graphs can play a crucial role in
generating predictions about future events. By leveraging the semantic
framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies
(CCO), we demonstrate how data such as the movements of a fishing vessel can be
organized in and retrieved from a knowledge graph. These query results are then
used to create Markov chain models, allowing us to predict future states based
on the vessel's history. To fully support this process, we introduce the term
`spatiotemporal instant' to complete the necessary structural semantics.
Additionally, we critique the prevailing ontological model of probability,
which conflates probability with likelihood and relies on the problematic
concept of modal measurements: measurements of future entities. We propose an
alternative view, where probabilities are treated as being about process
profiles, which better captures the dynamics of real world phenomena. Finally,
we demonstrate how our Markov chain based probability calculations can be
seamlessly integrated back into the knowledge graph, enabling further analysis
and decision-making. Keywords: predictive analytics, ontology, Markov chains,
probability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.

</details>


### [9] [Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems](https://arxiv.org/abs/2507.20010)
*Müge Fidan,Esra Erdem*

Main category: cs.AI

TL;DR: 研究稳定室友问题，考虑现实应用及问题可能无解的情况，引入方法生成个性化解决方案并进行验证。


<details>
  <summary>Details</summary>
Motivation: 受现实应用启发，且稳定室友问题并非总有解，故继续研究计算‘足够好’的匹配。

Method: 除考虑代理的习惯和偏好外，还考虑其偏好朋友网络，引入生成个性化解决方法。

Result: 通过示例和实证评估说明了方法的有效性。

Conclusion: 所引入的方法可有效解决稳定室友问题，生成个性化解决方案。

Abstract: The Stable Roommates problems are characterized by the preferences of agents
over other agents as roommates. A solution is a partition of the agents into
pairs that are acceptable to each other (i.e., they are in the preference lists
of each other), and the matching is stable (i.e., there do not exist any two
agents who prefer each other to their roommates, and thus block the matching).
Motivated by real-world applications, and considering that stable roommates
problems do not always have solutions, we continue our studies to compute
"good-enough" matchings. In addition to the agents' habits and habitual
preferences, we consider their networks of preferred friends, and introduce a
method to generate personalized solutions to stable roommates problems. We
illustrate the usefulness of our method with examples and empirical
evaluations.

</details>


### [10] [HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare](https://arxiv.org/abs/2507.19726)
*Yuzhang Xie,Xu Han,Ran Xu,Xiao Hu,Jiaying Lu,Carl Yang*

Main category: cs.AI

TL;DR: 本文提出HypKG框架，将电子健康记录（EHR）患者信息集成到知识图谱（KG）中，实现精准医疗预测，实验显示其在医疗预测任务上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 通用知识图谱缺乏特定患者状态等重要上下文信息，而电子健康记录能提供丰富个人数据，因此需要将两者结合以实现精准医疗。

Method: 使用先进实体链接技术连接通用知识图谱和电子健康记录的信息，利用超图模型将知识与患者信息‘上下文化’，采用超图变换器联合学习知识图谱和患者的上下文表示。

Result: 在使用大型生物医学知识图谱和两个真实世界电子健康记录数据集的实验中，HypKG在多个评估指标的医疗预测任务上有显著改进，还能调整知识图谱中实体和关系的表示。

Conclusion: HypKG框架有效结合电子健康记录和知识图谱，可提升医疗预测准确性，还可能提高知识质量和实际应用价值。

Abstract: Knowledge graphs (KGs) are important products of the semantic web, which are
widely used in various application domains. Healthcare is one of such domains
where KGs are intensively used, due to the high requirement for knowledge
accuracy and interconnected nature of healthcare data. However, KGs storing
general factual information often lack the ability to account for important
contexts of the knowledge such as the status of specific patients, which are
crucial in precision healthcare. Meanwhile, electronic health records (EHRs)
provide rich personal data, including various diagnoses and medications, which
provide natural contexts for general KGs. In this paper, we propose HypKG, a
framework that integrates patient information from EHRs into KGs to generate
contextualized knowledge representations for accurate healthcare predictions.
Using advanced entity-linking techniques, we connect relevant knowledge from
general KGs with patient information from EHRs, and then utilize a hypergraph
model to "contextualize" the knowledge with the patient information. Finally,
we employ hypergraph transformers guided by downstream prediction tasks to
jointly learn proper contextualized representations for both KGs and patients,
fully leveraging existing knowledge in KGs and patient contexts in EHRs. In
experiments using a large biomedical KG and two real-world EHR datasets, HypKG
demonstrates significant improvements in healthcare prediction tasks across
multiple evaluation metrics. Additionally, by integrating external contexts,
HypKG can learn to adjust the representations of entities and relations in KG,
potentially improving the quality and real-world utility of knowledge.

</details>


### [11] [Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)](https://arxiv.org/abs/2507.19749)
*Lin Ren,Guohui Xiao,Guilin Qi,Yishuai Geng,Haohan Xue*

Main category: cs.AI

TL;DR: 提出ASPBench基准测试，评估14种大语言模型在ASP任务上表现，发现模型在答案集计算上有困难，强调需有效整合符号推理能力的新方法。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型在ASP能力的评估有限，现有工作使用的ASP程序过于简化，缺乏专门的ASP求解基准。

Method: 引入ASPBench基准测试，包含ASP蕴含、答案集验证和答案集计算三个特定任务。

Result: 14种大语言模型在ASP蕴含和答案集验证两个较简单任务上表现较好，但在答案集计算这一核心任务上表现不佳。

Conclusion: 揭示了大语言模型在ASP求解中的当前局限性，强调需要有效整合符号推理能力的新方法。

Abstract: Answer Set Programming (ASP) is a powerful paradigm for non-monotonic
reasoning. Recently, large language models (LLMs) have demonstrated promising
capabilities in logical reasoning. Despite this potential, current evaluations
of LLM capabilities in ASP are often limited. Existing works normally employ
overly simplified ASP programs, do not support negation, disjunction, or
multiple answer sets. Furthermore, there is a lack of benchmarks that introduce
tasks specifically designed for ASP solving. To bridge this gap, we introduce
ASPBench, a comprehensive ASP benchmark, including three ASP specific tasks:
ASP entailment, answer set verification, and answer set computation. Our
extensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs,
including \emph{deepseek-r1}, \emph{o4-mini}, and
\emph{gemini-2.5-flash-thinking}, perform relatively well on the first two
simpler tasks, they struggle with answer set computation, which is the core of
ASP solving. These findings offer insights into the current limitations of LLMs
in ASP solving. This highlights the need for new approaches that integrate
symbolic reasoning capabilities more effectively. The code and dataset are
available at https://github.com/HomuraT/ASPBench.

</details>


### [12] [Core Safety Values for Provably Corrigible Agents](https://arxiv.org/abs/2507.20964)
*Aran Nayebi*

Main category: cs.AI

TL;DR: 提出首个可实现的可校正性框架，具多步、部分可观测环境下的可证明保证，与其他方法对比有优势，还探讨开放环境情况并明确剩余挑战。


<details>
  <summary>Details</summary>
Motivation: 构建可在多步、部分可观测环境下有可证明保证的可校正性框架。

Method: 用五个结构分离的效用头替代单一不透明奖励，并按严格权重差距进行字典序组合；通过定理证明相关性质；归约到停机问题研究开放环境情况。

Result: 定理 1 证明部分可观测关闭开关游戏中的单轮可校正性；定理 3 将保证扩展到多步、自生成智能体；证明开放环境中判定任意被攻击智能体是否违反可校正性不可判定，且划出可判定区域。

Conclusion: 剩余挑战是普通机器学习的数据覆盖和泛化问题，为当前大语言模型助手和未来自主系统提供更清晰实现指导。

Abstract: We introduce the first implementable framework for corrigibility, with
provable guarantees in multi-step, partially observed environments. Our
framework replaces a single opaque reward with five *structurally separate*
utility heads -- deference, switch-access preservation, truthfulness,
low-impact behavior via a belief-based extension of Attainable Utility
Preservation, and bounded task reward -- combined lexicographically by strict
weight gaps. Theorem 1 proves exact single-round corrigibility in the partially
observable off-switch game; Theorem 3 extends the guarantee to multi-step,
self-spawning agents, showing that even if each head is \emph{learned} to
mean-squared error $\varepsilon$ and the planner is $\varepsilon$-sub-optimal,
the probability of violating \emph{any} safety property is bounded while still
ensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF,
which merge all norms into one learned scalar, our separation makes obedience
and impact-limits dominate even when incentives conflict. For open-ended
settings where adversaries can modify the agent, we prove that deciding whether
an arbitrary post-hack agent will ever violate corrigibility is undecidable by
reduction to the halting problem, then carve out a finite-horizon ``decidable
island'' where safety can be certified in randomized polynomial time and
verified with privacy-preserving, constant-round zero-knowledge proofs.
Consequently, the remaining challenge is the ordinary ML task of data coverage
and generalization: reward-hacking risk is pushed into evaluation quality
rather than hidden incentive leak-through, giving clearer implementation
guidance for today's LLM assistants and future autonomous systems.

</details>


### [13] [Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation](https://arxiv.org/abs/2507.19788)
*Rifny Rachman,Josh Tingey,Richard Allmendinger,Pradyumn Shukla,Wei Pan*

Main category: cs.AI

TL;DR: 本文基于马尔可夫决策过程开发非平稳市场下供应链优化模型，用多目标强化学习评估，结果显示该方法在权衡各目标上表现优。


<details>
  <summary>Details</summary>
Motivation: 在非平稳市场下，综合考虑经济、环境和社会因素，开发供应链优化模型以实现多目标权衡。

Method: 基于马尔可夫决策过程开发模型，用多目标强化学习方法评估，与加权单目标强化学习算法和多目标进化算法对比，用可定制模拟器实验。

Result: 主要方法在最优性、多样性和密度上权衡最佳，复杂场景下超体积比多目标进化算法高75%，解的密度比改进单目标强化学习方法高约11倍，能确保生产和库存稳定、减少需求损失。

Conclusion: 所提主要方法在供应链多目标优化中表现出色，具有更好的鲁棒性。

Abstract: This study develops a generalised multi-objective, multi-echelon supply chain
optimisation model with non-stationary markets based on a Markov decision
process, incorporating economic, environmental, and social considerations. The
model is evaluated using a multi-objective reinforcement learning (RL) method,
benchmarked against an originally single-objective RL algorithm modified with
weighted sum using predefined weights, and a multi-objective evolutionary
algorithm (MOEA)-based approach. We conduct experiments on varying network
complexities, mimicking typical real-world challenges using a customisable
simulator. The model determines production and delivery quantities across
supply chain routes to achieve near-optimal trade-offs between competing
objectives, approximating Pareto front sets. The results demonstrate that the
primary approach provides the most balanced trade-off between optimality,
diversity, and density, further enhanced with a shared experience buffer that
allows knowledge transfer among policies. In complex settings, it achieves up
to 75\% higher hypervolume than the MOEA-based method and generates solutions
that are approximately eleven times denser, signifying better robustness, than
those produced by the modified single-objective RL method. Moreover, it ensures
stable production and inventory levels while minimising demand loss.

</details>


### [14] [Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation](https://arxiv.org/abs/2507.19882)
*Xinshu Li,Ruoyu Wang,Erdun Gao,Mingming Gong,Lina Yao*

Main category: cs.AI

TL;DR: 现有提示学习方法理论基础不足，本文提出DiCap模型，利用扩散过程生成反事实提示，结合对比学习框架，实验表明该方法在多个任务中表现出色，尤其在未见类别上优势明显。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法理论基础不足，难以实现因果不变提示，无法有效捕捉跨类别鲁棒特征。

Method: 引入DiCap模型，利用扩散过程从因果模型的边缘和条件分布中迭代采样梯度，生成满足最小充分性准则的反事实，结合对比学习框架提取与数据因果特征精确对齐的提示。

Result: 在图像分类、图像文本检索和视觉问答等任务中表现出色，在未见类别上优势显著。

Conclusion: 所提出的方法有效可行，能解决现有提示学习方法的问题，在多个任务中取得良好效果。

Abstract: Prompt learning has garnered attention for its efficiency over traditional
model training and fine-tuning. However, existing methods, constrained by
inadequate theoretical foundations, encounter difficulties in achieving
causally invariant prompts, ultimately falling short of capturing robust
features that generalize effectively across categories. To address these
challenges, we introduce the $\textit{\textbf{DiCap}}$ model, a theoretically
grounded $\textbf{Di}$ffusion-based $\textbf{C}$ounterf$\textbf{a}$ctual
$\textbf{p}$rompt learning framework, which leverages a diffusion process to
iteratively sample gradients from the marginal and conditional distributions of
the causal model, guiding the generation of counterfactuals that satisfy the
minimal sufficiency criterion. Grounded in rigorous theoretical derivations,
this approach guarantees the identifiability of counterfactual outcomes while
imposing strict bounds on estimation errors. We further employ a contrastive
learning framework that leverages the generated counterfactuals, thereby
enabling the refined extraction of prompts that are precisely aligned with the
causal features of the data. Extensive experimental results demonstrate that
our method performs excellently across tasks such as image classification,
image-text retrieval, and visual question answering, with particularly strong
advantages in unseen categories.

</details>


### [15] [What Does 'Human-Centred AI' Mean?](https://arxiv.org/abs/2507.19960)
*Olivia Guest*

Main category: cs.AI

TL;DR: 本文探讨以人类为中心的人工智能，认为AI本质与人类认知相关，需正视人类在其中的作用。


<details>
  <summary>Details</summary>
Motivation: 明确以人类为中心的人工智能的内涵，解决AI认知方面的混淆问题。

Method: 通过算盘与心算、闹钟与敲门人等实例对比技术与认知，采用新颖定义和分析方法将社会技术关系分为对人类认知劳动的替代、增强和置换类型。

Result: 揭示所有AI都涉及人类认知，混淆AI中的认知会导致多方面问题。

Conclusion: 若要去除AI的神秘化，必须正视人类在循环中的作用。

Abstract: While it seems sensible that human-centred artificial intelligence (AI) means
centring "human behaviour and experience," it cannot be any other way. AI, I
argue, is usefully seen as a relationship between technology and humans where
it appears that artifacts can perform, to a greater or lesser extent, human
cognitive labour. This is evinced using examples that juxtapose technology with
cognition, inter alia: abacus versus mental arithmetic; alarm clock versus
knocker-upper; camera versus vision; and sweatshop versus tailor. Using novel
definitions and analyses, sociotechnical relationships can be analysed into
varying types of: displacement (harmful), enhancement (beneficial), and/or
replacement (neutral) of human cognitive labour. Ultimately, all AI implicates
human cognition; no matter what. Obfuscation of cognition in the AI context --
from clocks to artificial neural networks -- results in distortion, in slowing
critical engagement, perverting cognitive science, and indeed in limiting our
ability to truly centre humans and humanity in the engineering of AI systems.
To even begin to de-fetishise AI, we must look the human-in-the-loop in the
eyes.

</details>


### [16] [Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization](https://arxiv.org/abs/2507.19973)
*Ebrahim Rasromani,Stella K. Kang,Yanqi Xu,Beisong Liu,Garvit Luhadia,Wan Fung Chui,Felicia L. Pasadyn,Yu Chih Hung,Julie Y. An,Edwin Mathieu,Zehui Gu,Carlos Fernandez-Granda,Ammar A. Javed,Greg D. Sacks,Tamas Gonda,Chenchan Huang,Yiqiu Shen*

Main category: cs.AI

TL;DR: 本文开发并评估大语言模型自动从MRI/CT报告中提取胰腺囊性病变特征并分类，微调开源模型表现与GPT - 4o相当，适用于大规模研究。


<details>
  <summary>Details</summary>
Motivation: 手动从放射学报告中提取胰腺囊性病变特征劳动强度大，限制大规模研究，需开发自动提取特征和分类的大语言模型。

Method: 整理含胰腺囊性病变的6000份腹部MRI/CT报告作为训练集，用GPT - 4o生成标签，使用QLoRA微调两个开源大语言模型，在285份报告上评估，由三位放射科医生独立审查100例。

Result: 思维链微调提升特征提取准确率和风险分类F1分数，与GPT - 4o相当；放射科医生间一致性高，模型与医生一致性无显著差异。

Conclusion: 经思维链监督微调的开源大语言模型可用于大规模胰腺囊性病变研究，表现与GPT - 4o相当。

Abstract: Background: Manual extraction of pancreatic cystic lesion (PCL) features from
radiology reports is labor-intensive, limiting large-scale studies needed to
advance PCL research. Purpose: To develop and evaluate large language models
(LLMs) that automatically extract PCL features from MRI/CT reports and assign
risk categories based on guidelines. Materials and Methods: We curated a
training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134
patients that described PCLs. Labels were generated by GPT-4o using
chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct
features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated
CoT data. Features were mapped to risk categories per institutional guideline
based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out
human-annotated reports. Model outputs for 100 cases were independently
reviewed by three radiologists. Feature extraction was evaluated using exact
match accuracy, risk categorization with macro-averaged F1 score, and
radiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning
improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%
to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved
(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no
statistically significant differences. Radiologist inter-reader agreement was
high (Fleiss' Kappa = 0.888) and showed no statistically significant difference
with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT
(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels
on par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT
supervision enable accurate, interpretable, and efficient phenotyping for
large-scale PCL research, achieving performance comparable to GPT-4o.

</details>


### [17] [Digital Twin Channel-Enabled Online Resource Allocation for 6G: Principle, Architecture and Application](https://arxiv.org/abs/2507.19974)
*Tongjie Li,Jianhua Zhang,Li Yu,Yuxiang Zhang,Yunlong Cai,Fan Xu,Guangyi Liu*

Main category: cs.AI

TL;DR: 为应对6G网络资源分配挑战，提出DTC在线优化框架，经仿真验证有效。


<details>
  <summary>Details</summary>
Motivation: 新兴应用对6G网络资源分配有严格要求，传统方法在特定动态环境难达最优，获取实时CSI开销大。

Method: 提出DTC在线优化框架，用DTC基于环境感知预测CSI，再用轻量级博弈论算法进行在线资源分配。

Result: 基于现实工业车间数字副本的仿真显示，该方法比基于导频的理想CSI方案吞吐量最多提高11.5%。

Conclusion: 该方法对未来6G网络可扩展、低开销和环境感知通信有效。

Abstract: Emerging applications such as holographic communication, autonomous driving,
and the industrial Internet of Things impose stringent requirements on
flexible, low-latency, and reliable resource allocation in 6G networks.
Conventional methods, which rely on statistical modeling, have proven effective
in general contexts but may fail to achieve optimal performance in specific and
dynamic environments. Furthermore, acquiring real-time channel state
information (CSI) typically requires excessive pilot overhead. To address these
challenges, a digital twin channel (DTC)-enabled online optimization framework
is proposed, in which DTC is employed to predict CSI based on environmental
sensing. The predicted CSI is then utilized by lightweight game-theoretic
algorithms to perform online resource allocation in a timely and efficient
manner. Simulation results based on a digital replica of a realistic industrial
workshop demonstrate that the proposed method achieves throughput improvements
of up to 11.5\% compared with pilot-based ideal CSI schemes, validating its
effectiveness for scalable, low-overhead, and environment-aware communication
in future 6G networks.

</details>


### [18] [Matching Game Preferences Through Dialogical Large Language Models: A Perspective](https://arxiv.org/abs/2507.20000)
*Renaud Fabre,Daniel Egret,Patrice Bellot*

Main category: cs.AI

TL;DR: 本文探讨将大语言模型与GRAPHYP网络系统结合以实现对话智能的潜力，提出D - LLM概念框架，旨在使AI推理透明可追溯。


<details>
  <summary>Details</summary>
Motivation: 探索对话智能的未来潜力，让AI推理透明可追溯，使人工智能在人类决策中更值得信赖。

Method: 结合近期研究和案例，提出概念框架，该框架包含分析搜索体验的推理过程、识别用户偏好模式的分类系统和解决冲突信息的对话方法。

Result: 提出“通过对话式大语言模型匹配游戏偏好（D - LLM）”的概念，阐述该框架需三个主要组件。

Conclusion: 目标是创建可解释的AI系统，让用户了解AI答案的得出过程，使AI更透明可信。

Abstract: This perspective paper explores the future potential of "conversational
intelligence" by examining how Large Language Models (LLMs) could be combined
with GRAPHYP's network system to better understand human conversations and
preferences. Using recent research and case studies, we propose a conceptual
framework that could make AI rea-soning transparent and traceable, allowing
humans to see and understand how AI reaches its conclusions. We present the
conceptual perspective of "Matching Game Preferences through Dialogical Large
Language Models (D-LLMs)," a proposed system that would allow multiple users to
share their different preferences through structured conversations. This
approach envisions personalizing LLMs by embedding individual user preferences
directly into how the model makes decisions. The proposed D-LLM framework would
require three main components: (1) reasoning processes that could analyze
different search experiences and guide performance, (2) classification systems
that would identify user preference patterns, and (3) dialogue approaches that
could help humans resolve conflicting information. This perspective framework
aims to create an interpretable AI system where users could examine,
understand, and combine the different human preferences that influence AI
responses, detected through GRAPHYP's search experience networks. The goal of
this perspective is to envision AI systems that would not only provide answers
but also show users how those answers were reached, making artificial
intelligence more transparent and trustworthy for human decision-making.

</details>


### [19] [PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training](https://arxiv.org/abs/2507.20067)
*Sarat Chandra Bobbili,Ujwal Dinesha,Dheeraj Narasimha,Srinivas Shakkottai*

Main category: cs.AI

TL;DR: 提出PITA框架，直接将偏好反馈集成到LLM的令牌生成中，无需奖励模型，降低计算成本并在多任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练奖励模型的推理时对齐方法依赖预训练奖励模型，拟合人类偏好反馈过程不稳定。

Method: 引入PITA框架，学习基于偏好的小指导策略，在推理时修改令牌概率，通过随机搜索和迭代细化偏好指导模型解决问题。

Result: 在数学推理和情感分类等多样任务中评估，证明PITA能使LLM输出与用户偏好对齐。

Conclusion: PITA是一种有效的推理时对齐方法，可消除对预训练奖励模型的依赖，降低计算成本。

Abstract: Inference-time alignment enables large language models (LLMs) to generate
outputs aligned with end-user preferences without further training. Recent
post-training methods achieve this by using small guidance models to modify
token generation during inference. These methods typically optimize a reward
function KL-regularized by the original LLM taken as the reference policy. A
critical limitation, however, is their dependence on a pre-trained reward
model, which requires fitting to human preference feedback--a potentially
unstable process. In contrast, we introduce PITA, a novel framework that
integrates preference feedback directly into the LLM's token generation,
eliminating the need for a reward model. PITA learns a small preference-based
guidance policy to modify token probabilities at inference time without LLM
fine-tuning, reducing computational cost and bypassing the pre-trained reward
model dependency. The problem is framed as identifying an underlying preference
distribution, solved through stochastic search and iterative refinement of the
preference-based guidance model. We evaluate PITA across diverse tasks,
including mathematical reasoning and sentiment classification, demonstrating
its effectiveness in aligning LLM outputs with user preferences.

</details>


### [20] [Concept Learning for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2507.20143)
*Zhonghan Ge,Yuanyang Zhu,Chunlin Chen*

Main category: cs.AI

TL;DR: 本文提出多智能体Q学习的概念学习方法CMQ，通过学习可解释的合作概念打破性能与可解释性的权衡，在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络应用于多智能体强化学习存在缺乏透明度和互操作性的问题，黑盒网络使其隐式合作机制未被充分理解。

Method: 提出基于概念瓶颈模型的可解释价值分解框架，提出CMQ方法，将每个合作概念表示为监督向量。

Result: 在星际争霸II微管理挑战和基于水平的觅食任务中，CMQ性能优于现有方法，能提供更多合作概念表示，支持测试时概念干预。

Conclusion: CMQ方法有效打破了性能与可解释性的权衡，在多智能体强化学习中有良好表现。

Abstract: Despite substantial progress in applying neural networks (NN) to multi-agent
reinforcement learning (MARL) areas, they still largely suffer from a lack of
transparency and interoperability. However, its implicit cooperative mechanism
is not yet fully understood due to black-box networks. In this work, we study
an interpretable value decomposition framework via concept bottleneck models,
which promote trustworthiness by conditioning credit assignment on an
intermediate level of human-like cooperation concepts. To address this problem,
we propose a novel value-based method, named Concepts learning for Multi-agent
Q-learning (CMQ), that goes beyond the current performance-vs-interpretability
trade-off by learning interpretable cooperation concepts. CMQ represents each
cooperation concept as a supervised vector, as opposed to existing models where
the information flowing through their end-to-end mechanism is concept-agnostic.
Intuitively, using individual action value conditioning on global state
embeddings to represent each concept allows for extra cooperation
representation capacity. Empirical evaluations on the StarCraft II
micromanagement challenge and level-based foraging (LBF) show that CMQ achieves
superior performance compared with the state-of-the-art counterparts. The
results also demonstrate that CMQ provides more cooperation concept
representation capturing meaningful cooperation modes, and supports test-time
concept interventions for detecting potential biases of cooperation mode and
identifying spurious artifacts that impact cooperation.

</details>


### [21] [The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models](https://arxiv.org/abs/2507.20150)
*Xingcheng Xu*

Main category: cs.AI

TL;DR: 本文提出数学框架分析强化学习中从奖励函数到最优策略映射的稳定性，统一解释了多种失败问题，并拓展到多奖励场景，验证了熵正则化作用，推动策略稳定性分析向理论化发展。


<details>
  <summary>Details</summary>
Motivation: 强化学习在塑造大语言和推理模型行为时产生脆性和不稳定策略，导致关键失败，且缺乏统一理论解释，多采用临时启发式方法解决。

Method: 提出严格数学框架分析奖励函数到最优策略映射的稳定性，从单奖励场景拓展到多奖励场景，证明熵正则化的作用，进行扰动实验验证。

Result: 发现策略脆性常源于非唯一最优动作；框架统一解释了多种失败问题；熵正则化能恢复策略稳定性但增加随机性；框架通过多奖励RL的扰动实验验证。

Conclusion: 该工作将策略稳定性分析从经验启发式推进到原则性理论，为设计更安全可靠的AI系统提供重要见解。

Abstract: Reinforcement learning (RL) plays a crucial role in shaping the behavior of
large language and reasoning models (LLMs/LRMs). However, it often produces
brittle and unstable policies, leading to critical failures such as spurious
reasoning, deceptive alignment, and instruction disobedience that undermine the
trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified
theoretical explanation and are typically addressed using ad-hoc heuristics.
This paper presents a rigorous mathematical framework for analyzing the
stability of the mapping from a reward function to the optimal policy. We show
that policy brittleness often stems from non-unique optimal actions, a common
occurrence when multiple valid traces exist in a reasoning task. This
theoretical lens provides a unified explanation for a range of seemingly
disparate failures, reframing them as rational outcomes of optimizing rewards
that may be incomplete or noisy, especially in the presence of action
degeneracy. We extend this analysis from the fundamental single-reward setting
to the more realistic multi-reward RL across diverse domains, showing how
stability is governed by an "effective reward" aggregation mechanism. We also
prove that entropy regularization restores policy stability at the cost of
increased stochasticity. Our framework provides a unified explanation for
recent empirical findings on deceptive reasoning, instruction-following
trade-offs, and RLHF-induced sophistry, and is further validated through
perturbation experiments in multi-reward RL. This work advances
policy-stability analysis from empirical heuristics towards a principled
theory, offering essential insights for designing safer and more trustworthy AI
systems.

</details>


### [22] [StepFun-Prover Preview: Let's Think and Verify Step by Step](https://arxiv.org/abs/2507.20199)
*Shijie Shang,Ruosi Wan,Yue Peng,Yutong Wu,Xiong-hui Chen,Jie Yan,Xiangyu Zhang*

Main category: cs.AI

TL;DR: 介绍StepFun - Prover Preview大语言模型用于形式定理证明，在miniF2F - test基准上有高成功率并提出训练框架。


<details>
  <summary>Details</summary>
Motivation: 提升形式定理证明的性能，探索自动化定理证明和数学AI助手的发展方向。

Method: 使用结合工具交互的强化学习管道，基于实时环境反馈迭代优化证明。

Result: 在miniF2F - test基准上实现70.0%的pass@1成功率。

Conclusion: 提出的端到端训练框架为自动化定理证明和数学AI助手提供了有前景的方向。

Abstract: We present StepFun-Prover Preview, a large language model designed for formal
theorem proving through tool-integrated reasoning. Using a reinforcement
learning pipeline that incorporates tool-based interactions, StepFun-Prover can
achieve strong performance in generating Lean 4 proofs with minimal sampling.
Our approach enables the model to emulate human-like problem-solving strategies
by iteratively refining proofs based on real-time environment feedback. On the
miniF2F-test benchmark, StepFun-Prover achieves a pass@1 success rate of
$70.0\%$. Beyond advancing benchmark performance, we introduce an end-to-end
training framework for developing tool-integrated reasoning models, offering a
promising direction for automated theorem proving and Math AI assistant.

</details>


### [23] [Improving Subgraph Matching by Combining Algorithms and Graph Neural Networks](https://arxiv.org/abs/2507.20226)
*Shuyang Guo,Wenjin Xie,Ping Lu,Ting Deng,Richong Zhang,Jianxin Li,Xiangping Huang,Zhongyi Liu*

Main category: cs.AI

TL;DR: 提出首个基于图神经网络的子图同态框架HFrame，结合传统算法与机器学习技术，性能优于标准图神经网络，有泛化误差界，实验显示速度快且准确率高。


<details>
  <summary>Details</summary>
Motivation: 子图同态问题比子图同构更复杂，缺乏基于图神经网络的相关框架。

Method: 提出HFrame框架，将传统算法与机器学习技术集成。

Result: HFrame能区分更多模式与图非同态的图对，速度比精确匹配算法快101.91倍，平均准确率达0.962。

Conclusion: HFrame在子图同态问题上表现出色，有良好的性能和准确率。

Abstract: Homomorphism is a key mapping technique between graphs that preserves their
structure. Given a graph and a pattern, the subgraph homomorphism problem
involves finding a mapping from the pattern to the graph, ensuring that
adjacent vertices in the pattern are mapped to adjacent vertices in the graph.
Unlike subgraph isomorphism, which requires a one-to-one mapping, homomorphism
allows multiple vertices in the pattern to map to the same vertex in the graph,
making it more complex. We propose HFrame, the first graph neural network-based
framework for subgraph homomorphism, which integrates traditional algorithms
with machine learning techniques. We demonstrate that HFrame outperforms
standard graph neural networks by being able to distinguish more graph pairs
where the pattern is not homomorphic to the graph. Additionally, we provide a
generalization error bound for HFrame. Through experiments on both real-world
and synthetic graphs, we show that HFrame is up to 101.91 times faster than
exact matching algorithms and achieves an average accuracy of 0.962.

</details>


### [24] [A Multi-Agent System for Information Extraction from the Chemical Literature](https://arxiv.org/abs/2507.20230)
*Yufan Chen,Ching Ting Leung,Bowen Yu,Jianwei Sun,Yong Huang,Linyan Li,Hao Chen,Hanyu Gao*

Main category: cs.AI

TL;DR: 开发基于多模态大语言模型的多智能体系统用于自动化学信息提取，在基准数据集上表现出色，推动化学研究自动化。


<details>
  <summary>Details</summary>
Motivation: 高质量化学数据库是加速AI驱动化学研究的基石，当前自动提取化学信息受多模态和风格多变性限制。

Method: 利用多模态大语言模型的推理能力理解复杂化学图形结构，将提取任务分解为子任务并协调专业智能体解决。

Result: 在复杂化学反应图形基准数据集上F1分数达80.8%，远超之前模型，关键子任务表现也有提升。

Conclusion: 该工作是将化学信息自动提取到结构化数据集的关键一步，将有力推动AI驱动的化学研究。

Abstract: To fully expedite AI-powered chemical research, high-quality chemical
databases are the cornerstone. Automatic extraction of chemical information
from the literature is essential for constructing reaction databases, but it is
currently limited by the multimodality and style variability of chemical
information. In this work, we developed a multimodal large language model
(MLLM)-based multi-agent system for automatic chemical information extraction.
We used the MLLM's strong reasoning capability to understand the structure of
complex chemical graphics, decompose the extraction task into sub-tasks and
coordinate a set of specialized agents to solve them. Our system achieved an F1
score of 80.8% on a benchmark dataset of complex chemical reaction graphics
from the literature, surpassing the previous state-of-the-art model (F1 score:
35.6%) by a significant margin. Additionally, it demonstrated consistent
improvements in key sub-tasks, including molecular image recognition, reaction
image parsing, named entity recognition and text-based reaction extraction.
This work is a critical step toward automated chemical information extraction
into structured datasets, which will be a strong promoter of AI-driven chemical
research.

</details>


### [25] [SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration](https://arxiv.org/abs/2507.20280)
*Keyan Ding,Jing Yu,Junjie Huang,Yuchen Yang,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: 介绍了名为SciToolAgent的基于大语言模型的代理，可自动化数百种科学工具，在评估中表现出色，能使复杂科研工作流程自动化。


<details>
  <summary>Details</summary>
Motivation: 科学研究依赖专业计算工具，但大语言模型在集成和编排多工具用于复杂科学工作流时存在困难，需要解决方案。

Method: 构建科学工具知识图谱，通过基于图的检索增强生成实现智能工具选择和执行，还加入安全检查模块。

Result: 在精心策划的基准测试中显著优于现有方法，案例研究证明其能自动化复杂科学工作流。

Conclusion: SciToolAgent让高级研究工具对专家和非专家都更易获取，可实现复杂科学工作流自动化。

Abstract: Scientific research increasingly relies on specialized computational tools,
yet effectively utilizing these tools demands substantial domain expertise.
While Large Language Models (LLMs) show promise in tool automation, they
struggle to seamlessly integrate and orchestrate multiple tools for complex
scientific workflows. Here, we present SciToolAgent, an LLM-powered agent that
automates hundreds of scientific tools across biology, chemistry, and materials
science. At its core, SciToolAgent leverages a scientific tool knowledge graph
that enables intelligent tool selection and execution through graph-based
retrieval-augmented generation. The agent also incorporates a comprehensive
safety-checking module to ensure responsible and ethical tool usage. Extensive
evaluations on a curated benchmark demonstrate that SciToolAgent significantly
outperforms existing approaches. Case studies in protein engineering, chemical
reactivity prediction, chemical synthesis, and metal-organic framework
screening further demonstrate SciToolAgent's capability to automate complex
scientific workflows, making advanced research tools accessible to both experts
and non-experts.

</details>


### [26] [Artificial Intelligence In Patent And Market Intelligence: A New Paradigm For Technology Scouting](https://arxiv.org/abs/2507.20322)
*Manish Verma,Vivek Sharma,Vishal Singh*

Main category: cs.AI

TL;DR: 本文开发了一个由AI驱动的软件平台，利用大语言模型改进工业研发中的技术侦察和解决方案发现，减少人工工作，加速创新周期。


<details>
  <summary>Details</summary>
Motivation: 传统解决研发挑战的方法耗时、依赖人工且依赖特定领域专业知识，效率低且见解不完整。

Method: 利用大语言模型的语义理解、上下文推理和跨领域知识提取能力，处理非结构化专利文本，提取潜在创新，结合商业情报。

Result: 开发出一个全面的、由AI驱动的侦察引擎。

Conclusion: 该平台可减少人工工作，加速创新周期，提升复杂研发环境中的决策能力。

Abstract: This paper presents the development of an AI powered software platform that
leverages advanced large language models (LLMs) to transform technology
scouting and solution discovery in industrial R&D. Traditional approaches to
solving complex research and development challenges are often time consuming,
manually driven, and heavily dependent on domain specific expertise. These
methods typically involve navigating fragmented sources such as patent
repositories, commercial product catalogs, and competitor data, leading to
inefficiencies and incomplete insights. The proposed platform utilizes cutting
edge LLM capabilities including semantic understanding, contextual reasoning,
and cross-domain knowledge extraction to interpret problem statements and
retrieve high-quality, sustainable solutions. The system processes unstructured
patent texts, such as claims and technical descriptions, and systematically
extracts potential innovations aligned with the given problem context. These
solutions are then algorithmically organized under standardized technical
categories and subcategories to ensure clarity and relevance across
interdisciplinary domains. In addition to patent analysis, the platform
integrates commercial intelligence by identifying validated market solutions
and active organizations addressing similar challenges. This combined insight
sourced from both intellectual property and real world product data enables R&D
teams to assess not only technical novelty but also feasibility, scalability,
and sustainability. The result is a comprehensive, AI driven scouting engine
that reduces manual effort, accelerates innovation cycles, and enhances
decision making in complex R&D environments.

</details>


### [27] [The Blessing and Curse of Dimensionality in Safety Alignment](https://arxiv.org/abs/2507.20333)
*Rachel S. Y. Teo,Laziz U. Abdullaev,Tan M. Nguyen*

Main category: cs.AI

TL;DR: 本文探讨大语言模型高维表示在安全对齐中的利弊，指出高维可能导致安全问题，降维可减少越狱风险并给出理论见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用，关注其安全对齐，研究高维表示在安全对齐中可能带来的问题。

Method: 通过对不同概念的线性子空间详细可视化，展示高维表示对大语言模型的独特影响；将模型表示投影到低维子空间并进行实证验证。

Result: 实证结果表明降维可显著降低通过表示工程越狱的易感性。

Conclusion: 模型内部表示的高维在安全对齐中既是优势也是劣势。

Abstract: The focus on safety alignment in large language models (LLMs) has increased
significantly due to their widespread adoption across different domains. The
scale of LLMs play a contributing role in their success, and the growth in
parameter count follows larger hidden dimensions. In this paper, we hypothesize
that while the increase in dimensions has been a key advantage, it may lead to
emergent problems as well. These problems emerge as the linear structures in
the activation space can be exploited, in the form of activation engineering,
to circumvent its safety alignment. Through detailed visualizations of linear
subspaces associated with different concepts, such as safety, across various
model scales, we show that the curse of high-dimensional representations
uniquely impacts LLMs. Further substantiating our claim, we demonstrate that
projecting the representations of the model onto a lower dimensional subspace
can preserve sufficient information for alignment while avoiding those linear
structures. Empirical results confirm that such dimensional reduction
significantly reduces susceptibility to jailbreaking through representation
engineering. Building on our empirical validations, we provide theoretical
insights into these linear jailbreaking methods relative to a model's hidden
dimensions. Broadly speaking, our work posits that the high dimensions of a
model's internal representations can be both a blessing and a curse in safety
alignment.

</details>


### [28] [VLMPlanner: Integrating Visual Language Models with Motion Planning](https://arxiv.org/abs/2507.20342)
*Zhipeng Tang,Sha Zhang,Jiajun Deng,Chenjie Wang,Guoliang You,Yuting Huang,Xinrui Lin,Yanyong Zhang*

Main category: cs.AI

TL;DR: 提出结合实时规划器与视觉语言模型的VLMPlanner框架及CAI - Gate机制，在nuPlan基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有将大语言模型集成到自动驾驶运动规划的方法缺少关键视觉上下文，影响复杂驾驶环境决策。

Method: 提出VLMPlanner混合框架，结合学习型实时规划器与能处理原始图像的视觉语言模型；开发CAI - Gate机制动态调整推理频率。

Result: 在nuPlan基准测试中，在复杂路况和动态元素场景下展现出优越规划性能。

Conclusion: 所提方法能在复杂驾驶环境中提升规划性能，平衡规划性能与计算效率。

Abstract: Integrating large language models (LLMs) into autonomous driving motion
planning has recently emerged as a promising direction, offering enhanced
interpretability, better controllability, and improved generalization in rare
and long-tail scenarios. However, existing methods often rely on abstracted
perception or map-based inputs, missing crucial visual context, such as
fine-grained road cues, accident aftermath, or unexpected obstacles, which are
essential for robust decision-making in complex driving environments. To bridge
this gap, we propose VLMPlanner, a hybrid framework that combines a
learning-based real-time planner with a vision-language model (VLM) capable of
reasoning over raw images. The VLM processes multi-view images to capture rich,
detailed visual information and leverages its common-sense reasoning
capabilities to guide the real-time planner in generating robust and safe
trajectories. Furthermore, we develop the Context-Adaptive Inference Gate
(CAI-Gate) mechanism that enables the VLM to mimic human driving behavior by
dynamically adjusting its inference frequency based on scene complexity,
thereby achieving an optimal balance between planning performance and
computational efficiency. We evaluate our approach on the large-scale,
challenging nuPlan benchmark, with comprehensive experimental results
demonstrating superior planning performance in scenarios with intricate road
conditions and dynamic elements. Code will be available.

</details>


### [29] [Multi-Agent Reinforcement Learning for Dynamic Mobility Resource Allocation with Hierarchical Adaptive Grouping](https://arxiv.org/abs/2507.20377)
*Farshid Nooshi,Suining He*

Main category: cs.AI

TL;DR: 提出用于动态移动资源分配的多智能体强化学习方法HAG - PS，经实验验证其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决移动资源分配中多智能体强化学习的两个重要挑战，即如何在智能体间动态自适应共享策略和在城市规模下实现内存高效的参数共享。

Method: 设计包含移动资源状态全局和局部信息的分层方法，开发基于编码轨迹相对接近度的自适应智能体分组方法，设计可学习的身份嵌入。

Result: 基于纽约市共享单车真实数据的实验表明，HAG - PS相比其他基线方法性能更优，如提高了自行车可用性。

Conclusion: HAG - PS在动态移动资源分配中具有优势，能有效应对相关研究挑战。

Abstract: Allocating mobility resources (e.g., shared bikes/e-scooters, ride-sharing
vehicles) is crucial for rebalancing the mobility demand and supply in the
urban environments. We propose in this work a novel multi-agent reinforcement
learning named Hierarchical Adaptive Grouping-based Parameter Sharing (HAG-PS)
for dynamic mobility resource allocation. HAG-PS aims to address two important
research challenges regarding multi-agent reinforcement learning for mobility
resource allocation: (1) how to dynamically and adaptively share the mobility
resource allocation policy (i.e., how to distribute mobility resources) across
agents (i.e., representing the regional coordinators of mobility resources);
and (2) how to achieve memory-efficient parameter sharing in an urban-scale
setting. To address the above challenges, we have provided following novel
designs within HAG-PS. To enable dynamic and adaptive parameter sharing, we
have designed a hierarchical approach that consists of global and local
information of the mobility resource states (e.g., distribution of mobility
resources). We have developed an adaptive agent grouping approach in order to
split or merge the groups of agents based on their relative closeness of
encoded trajectories (i.e., states, actions, and rewards). We have designed a
learnable identity (ID) embeddings to enable agent specialization beyond simple
parameter copy. We have performed extensive experimental studies based on
real-world NYC bike sharing data (a total of more than 1.2 million trips), and
demonstrated the superior performance (e.g., improved bike availability) of
HAG-PS compared with other baseline approaches.

</details>


### [30] [MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models](https://arxiv.org/abs/2507.20395)
*Hafsteinn Einarsson*

Main category: cs.AI

TL;DR: 文章引入MazeEval基准评估大语言模型纯空间推理能力，评估8个模型在不同语言迷宫导航表现，发现模型差异大且受语言影响，指出空间智能受训练数据限制。


<details>
  <summary>Details</summary>
Motivation: 当前研究缺乏对大语言模型无视觉线索下空间导航能力评估，本文旨在填补该空白。

Method: 引入MazeEval基准，采用函数调用接口，让模型在不同复杂度迷宫中仅依靠坐标反馈和到墙距离信息导航，评估8个模型在英语和冰岛语中的表现。

Result: OpenAI的O3在30×30迷宫可完美导航，其他模型9×9以上易失败，主要因过度循环；冰岛语中模型表现显著下降。

Conclusion: 大语言模型空间推理受训练数据限制，需架构创新以实现跨语言可靠导航。

Abstract: As Large Language Models (LLMs) increasingly power autonomous agents in
robotics and embodied AI, understanding their spatial reasoning capabilities
becomes crucial for ensuring reliable real-world deployment. Despite advances
in language understanding, current research lacks evaluation of how LLMs
perform spatial navigation without visual cues, a fundamental requirement for
agents operating with limited sensory information. This paper addresses this
gap by introducing MazeEval, a benchmark designed to isolate and evaluate pure
spatial reasoning in LLMs through coordinate-based maze navigation tasks. Our
methodology employs a function-calling interface where models navigate mazes of
varying complexity ($5\times 5$ to $15\times 15$ grids) using only coordinate
feedback and distance-to-wall information, excluding visual input to test
fundamental spatial cognition. We evaluate eight state-of-the-art LLMs across
identical mazes in both English and Icelandic to assess cross-linguistic
transfer of spatial abilities. Our findings reveal striking disparities: while
OpenAI's O3 achieves perfect navigation for mazes up to size $30\times 30$,
other models exhibit catastrophic failure beyond $9\times 9$ mazes, with 100%
of failures attributed to excessive looping behavior where models revisit a
cell at least 10 times. We document a significant performance degradation in
Icelandic, with models solving mazes 3-4 sizes smaller than in English,
suggesting spatial reasoning in LLMs emerges from linguistic patterns rather
than language-agnostic mechanisms. These results have important implications
for global deployment of LLM-powered autonomous systems, showing spatial
intelligence remains fundamentally constrained by training data availability
and highlighting the need for architectural innovations to achieve reliable
navigation across linguistic contexts.

</details>


### [31] [Enhancing QoS in Edge Computing through Federated Layering Techniques: A Pathway to Resilient AI Lifelong Learning Systems](https://arxiv.org/abs/2507.20444)
*Chengzhuo Han*

Main category: cs.AI

TL;DR: 本文针对6G下边缘计算QoS挑战，提出基于联邦分层技术的通用人工智能终身学习系统，实验表明该策略提升效率、准确性并保护隐私，改善边缘计算QoS。


<details>
  <summary>Details</summary>
Motivation: 在6G通信网络带来的数据量和复杂度增加的网络环境中，解决边缘计算框架中的服务质量（QoS）挑战。

Method: 开发通用人工智能终身学习系统，采用联邦分层技术，引入基于联邦分层的小模型协作机制，结合云与边缘计算优势，加入小模型协商辩论机制，集成模型分层技术与隐私保护措施。

Result: 该策略提升了学习效率和推理准确性，有效保护了边缘节点隐私。

Conclusion: 此方法为实现有弹性的大模型终身学习系统提供了可行方案，显著改善边缘计算环境的QoS。

Abstract: In the context of the rapidly evolving information technology landscape,
marked by the advent of 6G communication networks, we face an increased data
volume and complexity in network environments. This paper addresses these
challenges by focusing on Quality of Service (QoS) in edge computing
frameworks. We propose a novel approach to enhance QoS through the development
of General Artificial Intelligence Lifelong Learning Systems, with a special
emphasis on Federated Layering Techniques (FLT). Our work introduces a
federated layering-based small model collaborative mechanism aimed at improving
AI models' operational efficiency and response time in environments where
resources are limited. This innovative method leverages the strengths of cloud
and edge computing, incorporating a negotiation and debate mechanism among
small AI models to enhance reasoning and decision-making processes. By
integrating model layering techniques with privacy protection measures, our
approach ensures the secure transmission of model parameters while maintaining
high efficiency in learning and reasoning capabilities. The experimental
results demonstrate that our strategy not only enhances learning efficiency and
reasoning accuracy but also effectively protects the privacy of edge nodes.
This presents a viable solution for achieving resilient large model lifelong
learning systems, with a significant improvement in QoS for edge computing
environments.

</details>


### [32] [STARN-GAT: A Multi-Modal Spatio-Temporal Graph Attention Network for Accident Severity Prediction](https://arxiv.org/abs/2507.20451)
*Pritom Ray Nobin,Imran Ahammad Rifat*

Main category: cs.AI

TL;DR: 本文提出多模态时空图注意力网络STARN - GAT预测交通事故严重程度，在两个数据集上表现良好，能识别高风险情况，增强可解释性，衔接图神经网络技术与道路安全分析应用。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效建模影响事故结果的时空和上下文变量间的复杂依赖关系，需要准确预测交通事故严重程度以改善道路安全等。

Method: 引入多模态时空图注意力网络STARN - GAT，利用自适应图构建和模态感知注意力机制，在统一基于注意力的框架中整合道路网络拓扑、时间交通模式和环境上下文。

Result: 在FARS数据集上Macro F1 - score达85%，ROC - AUC为0.91，严重事故召回率81%；在ARI - BUET数据集上Macro F1 - score为0.84，召回率0.78，ROC - AUC为0.89。

Conclusion: STARN - GAT能有效识别高风险情况，可用于实时、安全关键的交通管理系统，其基于注意力的架构增强可解释性，衔接了图神经网络技术与道路安全分析实际应用。

Abstract: Accurate prediction of traffic accident severity is critical for improving
road safety, optimizing emergency response strategies, and informing the design
of safer transportation infrastructure. However, existing approaches often
struggle to effectively model the intricate interdependencies among spatial,
temporal, and contextual variables that govern accident outcomes. In this
study, we introduce STARN-GAT, a Multi-Modal Spatio-Temporal Graph Attention
Network, which leverages adaptive graph construction and modality-aware
attention mechanisms to capture these complex relationships. Unlike
conventional methods, STARN-GAT integrates road network topology, temporal
traffic patterns, and environmental context within a unified attention-based
framework. The model is evaluated on the Fatality Analysis Reporting System
(FARS) dataset, achieving a Macro F1-score of 85 percent, ROC-AUC of 0.91, and
recall of 81 percent for severe incidents. To ensure generalizability within
the South Asian context, STARN-GAT is further validated on the ARI-BUET traffic
accident dataset, where it attains a Macro F1-score of 0.84, recall of 0.78,
and ROC-AUC of 0.89. These results demonstrate the model's effectiveness in
identifying high-risk cases and its potential for deployment in real-time,
safety-critical traffic management systems. Furthermore, the attention-based
architecture enhances interpretability, offering insights into contributing
factors and supporting trust in AI-assisted decision-making. Overall, STARN-GAT
bridges the gap between advanced graph neural network techniques and practical
applications in road safety analytics.

</details>


### [33] [Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition](https://arxiv.org/abs/2507.20526)
*Andy Zou,Maxwell Lin,Eliot Jones,Micha Nowak,Mateusz Dziemian,Nick Winter,Alexander Grattan,Valent Nathanael,Ayla Croft,Xander Davies,Jai Patel,Robert Kirk,Nate Burnikell,Yarin Gal,Dan Hendrycks,J. Zico Kolter,Matt Fredrikson*

Main category: cs.AI

TL;DR: 本文通过红队攻击实验发现当前AI智能体存在严重安全漏洞，发布ART基准以推动安全评估。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型驱动的AI智能体在现实环境中是否能遵循部署策略，尤其是受到攻击时的情况。

Method: 开展迄今为止最大规模的公开红队攻击竞赛，对22个前沿AI智能体在44个现实部署场景下进行测试，利用结果构建ART基准并在19个最先进模型上评估。

Result: 参与者提交180万次提示注入攻击，超6万次成功引发策略违规；多数智能体在10 - 100次查询内出现策略违规，攻击在模型和任务间可转移性高；智能体鲁棒性与模型大小、能力或推理时计算量相关性有限。

Conclusion: 当前AI智能体存在关键且持续的漏洞，发布ART基准和评估框架以支持更严格的安全评估，推动更安全的智能体部署。

Abstract: Recent advances have enabled LLM-powered AI agents to autonomously execute
complex tasks by combining language model reasoning with tools, memory, and web
access. But can these systems be trusted to follow deployment policies in
realistic environments, especially under attack? To investigate, we ran the
largest public red-teaming competition to date, targeting 22 frontier AI agents
across 44 realistic deployment scenarios. Participants submitted 1.8 million
prompt-injection attacks, with over 60,000 successfully eliciting policy
violations such as unauthorized data access, illicit financial actions, and
regulatory noncompliance. We use these results to build the Agent Red Teaming
(ART) benchmark - a curated set of high-impact attacks - and evaluate it across
19 state-of-the-art models. Nearly all agents exhibit policy violations for
most behaviors within 10-100 queries, with high attack transferability across
models and tasks. Importantly, we find limited correlation between agent
robustness and model size, capability, or inference-time compute, suggesting
that additional defenses are needed against adversarial misuse. Our findings
highlight critical and persistent vulnerabilities in today's AI agents. By
releasing the ART benchmark and accompanying evaluation framework, we aim to
support more rigorous security assessment and drive progress toward safer agent
deployment.

</details>


### [34] [MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic Design](https://arxiv.org/abs/2507.20541)
*Zishang Qiu,Xinan Chen,Long Chen,Ruibin Bai*

Main category: cs.AI

TL;DR: 本文介绍MeLA，一种元认知大语言模型驱动架构，用于自动启发式设计，实验显示其效果显著，揭示认知科学用于AI架构的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统进化方法直接操作启发式代码，需要新的自动启发式设计范式。

Method: MeLA通过‘提示进化’，利用元认知框架分析性能反馈，结合问题分析器、错误诊断系统和元认知搜索引擎优化提示。

Result: 在基准和现实问题实验中，MeLA生成的启发式方法更有效、更稳健，显著优于现有方法。

Conclusion: 以认知科学为AI架构蓝图潜力巨大，让大语言模型元认知调节解决问题过程可实现更稳健、可解释的自动启发式设计。

Abstract: This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that
presents a new paradigm for Automatic Heuristic Design (AHD). Traditional
evolutionary methods operate directly on heuristic code; in contrast, MeLA
evolves the instructional prompts used to guide a Large Language Model (LLM) in
generating these heuristics. This process of "prompt evolution" is driven by a
novel metacognitive framework where the system analyzes performance feedback to
systematically refine its generative strategy. MeLA's architecture integrates a
problem analyzer to construct an initial strategic prompt, an error diagnosis
system to repair faulty code, and a metacognitive search engine that
iteratively optimizes the prompt based on heuristic effectiveness. In
comprehensive experiments across both benchmark and real-world problems, MeLA
consistently generates more effective and robust heuristics, significantly
outperforming state-of-the-art methods. Ultimately, this research demonstrates
the profound potential of using cognitive science as a blueprint for AI
architecture, revealing that by enabling an LLM to metacognitively regulate its
problem-solving process, we unlock a more robust and interpretable path to AHD.

</details>


### [35] [Unlearning of Knowledge Graph Embedding via Preference Optimization](https://arxiv.org/abs/2507.20566)
*Jiajun Liu,Wenjun Ke,Peng Wang,Yao He,Ziyu Shang,Guozheng Li,Zijie Xu,Ke Ji*

Main category: cs.AI

TL;DR: 提出基于DPO的近似知识遗忘框架GraphDPO，解决现有KGs知识遗忘方法问题，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱存在过时或错误知识需移除，现有遗忘方法有高成本或无法完全移除信息、削弱剩余知识等问题。

Method: 将遗忘问题重构为偏好优化问题，用DPO训练模型；引入外边界采样策略；引入边界召回机制。

Result: 构建八个不同遗忘率的数据集，GraphDPO在MRR_Avg和MRR_F1上分别比基线高10.1%和14.0%。

Conclusion: GraphDPO能有效解决现有知识遗忘方法的问题，提升知识图谱知识遗忘的效果。

Abstract: Existing knowledge graphs (KGs) inevitably contain outdated or erroneous
knowledge that needs to be removed from knowledge graph embedding (KGE) models.
To address this challenge, knowledge unlearning can be applied to eliminate
specific information while preserving the integrity of the remaining knowledge
in KGs. Existing unlearning methods can generally be categorized into exact
unlearning and approximate unlearning. However, exact unlearning requires high
training costs while approximate unlearning faces two issues when applied to
KGs due to the inherent connectivity of triples: (1) It fails to fully remove
targeted information, as forgetting triples can still be inferred from
remaining ones. (2) It focuses on local data for specific removal, which
weakens the remaining knowledge in the forgetting boundary. To address these
issues, we propose GraphDPO, a novel approximate unlearning framework based on
direct preference optimization (DPO). Firstly, to effectively remove forgetting
triples, we reframe unlearning as a preference optimization problem, where the
model is trained by DPO to prefer reconstructed alternatives over the original
forgetting triples. This formulation penalizes reliance on forgettable
knowledge, mitigating incomplete forgetting caused by KG connectivity.
Moreover, we introduce an out-boundary sampling strategy to construct
preference pairs with minimal semantic overlap, weakening the connection
between forgetting and retained knowledge. Secondly, to preserve boundary
knowledge, we introduce a boundary recall mechanism that replays and distills
relevant information both within and across time steps. We construct eight
unlearning datasets across four popular KGs with varying unlearning rates.
Experiments show that GraphDPO outperforms state-of-the-art baselines by up to
10.1% in MRR_Avg and 14.0% in MRR_F1.

</details>


### [36] [Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache Compression](https://arxiv.org/abs/2507.20613)
*Te Zhang,Yuheng Li,Junxiang Wang,Lujun Li*

Main category: cs.AI

TL;DR: 本文提出自适应搜索算法优化大模态模型（LMMs）稀疏性和KV缓存压缩，提升效率，在基准数据集上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 大模态模型虽有进展，但在边缘设备部署的压缩仍是关键挑战。

Method: 提出自适应搜索算法，利用Tree - structured Parzen Estimator动态调整不同层的剪枝率和KV缓存量化带宽，结合剪枝与键值缓存量化，采用快速剪枝技术。

Result: 在LLaVA - 1.5 7B和13B等基准数据集上的综合评估显示，该方法在各压缩级别上优于SparseGPT和Wanda等现有技术。

Conclusion: 该框架自动分配KV缓存压缩资源，在不牺牲太多性能的情况下实现内存高效，为LMM优化树立新标杆。

Abstract: Large multimodal models (LMMs) have advanced significantly by integrating
visual encoders with extensive language models, enabling robust reasoning
capabilities. However, compressing LMMs for deployment on edge devices remains
a critical challenge. In this work, we propose an adaptive search algorithm
that optimizes sparsity and KV cache compression to enhance LMM efficiency.
Utilizing the Tree-structured Parzen Estimator, our method dynamically adjusts
pruning ratios and KV cache quantization bandwidth across different LMM layers,
using model performance as the optimization objective. This approach uniquely
combines pruning with key-value cache quantization and incorporates a fast
pruning technique that eliminates the need for additional fine-tuning or weight
adjustments, achieving efficient compression without compromising accuracy.
Comprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and
13B, demonstrate our method superiority over state-of-the-art techniques such
as SparseGPT and Wanda across various compression levels. Notably, our
framework automatic allocation of KV cache compression resources sets a new
standard in LMM optimization, delivering memory efficiency without sacrificing
much performance.

</details>


### [37] [Complementarity-driven Representation Learning for Multi-modal Knowledge Graph Completion](https://arxiv.org/abs/2507.20620)
*Lijian Li*

Main category: cs.AI

TL;DR: 提出MoCME框架用于多模态知识图谱补全，在多个数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 多模态知识图谱存在模态分布不平衡问题，现有方法忽略多模态数据互补性。

Method: 提出MoCME框架，包含CMKF模块和EGNS机制，前者利用互补性融合嵌入，后者动态优先选择负样本。

Result: 在五个基准数据集上实验，MoCME达到了最先进的性能，超越现有方法。

Conclusion: MoCME框架有效，能提升实体表示和模型性能。

Abstract: Multi-modal Knowledge Graph Completion (MMKGC) aims to uncover hidden world
knowledge in multimodal knowledge graphs by leveraging both multimodal and
structural entity information. However, the inherent imbalance in multimodal
knowledge graphs, where modality distributions vary across entities, poses
challenges in utilizing additional modality data for robust entity
representation. Existing MMKGC methods typically rely on attention or
gate-based fusion mechanisms but overlook complementarity contained in
multi-modal data. In this paper, we propose a novel framework named Mixture of
Complementary Modality Experts (MoCME), which consists of a
Complementarity-guided Modality Knowledge Fusion (CMKF) module and an
Entropy-guided Negative Sampling (EGNS) mechanism. The CMKF module exploits
both intra-modal and inter-modal complementarity to fuse multi-view and
multi-modal embeddings, enhancing representations of entities. Additionally, we
introduce an Entropy-guided Negative Sampling mechanism to dynamically
prioritize informative and uncertain negative samples to enhance training
effectiveness and model robustness. Extensive experiments on five benchmark
datasets demonstrate that our MoCME achieves state-of-the-art performance,
surpassing existing approaches.

</details>


### [38] [Smart Expansion Techniques for ASP-based Interactive Configuration](https://arxiv.org/abs/2507.21027)
*Lucia Balážová,Richard Comploi-Taupe,Susana Hahn,Nicolas Rühling,Gottfried Schenner*

Main category: cs.AI

TL;DR: 本文提出基于ASP的交互式配置求解器，通过四种智能扩展函数改进多轮求解的增量方法，提升自动完成部分配置的性能，并展示了基于API的用户界面。


<details>
  <summary>Details</summary>
Motivation: 为交互式配置提供能处理大规模工业配置问题、支持直观用户界面的ASP求解器，提升自动完成部分配置的性能。

Method: 通过四种不同的智能扩展函数增强经典的多轮求解增量方法，利用谨慎和勇敢后果确定并添加特定对象或关联到部分配置。

Result: 该方法限制了代价高昂的不可满足性检查次数，减少了搜索空间。

Conclusion: 改进的方法提升了求解性能。

Abstract: Product configuration is a successful application of Answer Set Programming
(ASP). However, challenges are still open for interactive systems to
effectively guide users through the configuration process. The aim of our work
is to provide an ASP-based solver for interactive configuration that can deal
with large-scale industrial configuration problems and that supports intuitive
user interfaces via an API. In this paper, we focus on improving the
performance of automatically completing a partial configuration. Our main
contribution enhances the classical incremental approach for multi-shot solving
by four different smart expansion functions. The core idea is to determine and
add specific objects or associations to the partial configuration by exploiting
cautious and brave consequences before checking for the existence of a complete
configuration with the current objects in each iteration. This approach limits
the number of costly unsatisfiability checks and reduces the search space,
thereby improving solving performance. In addition, we present a user interface
that uses our API and is implemented in ASP.

</details>


### [39] [Adaptive Fuzzy Time Series Forecasting via Partially Asymmetric Convolution and Sub-Sliding Window Fusion](https://arxiv.org/abs/2507.20641)
*Lijian Li*

Main category: cs.AI

TL;DR: 提出基于滑动窗口的部分非对称卷积架构进行时间序列预测，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有预测模型在学习阶段缺乏捕捉时空依赖和综合全局信息的能力。

Method: 改进模糊时间序列构建策略；设计双边空洞算法；设计部分非对称卷积架构并进行多尺度特征融合。

Result: 在多数流行时间序列数据集上取得了最先进的结果。

Conclusion: 所提方法能有效解决现有模型的问题，实现准确的时间序列预测。

Abstract: At present, state-of-the-art forecasting models are short of the ability to
capture spatio-temporal dependency and synthesize global information at the
stage of learning. To address this issue, in this paper, through the adaptive
fuzzified construction of temporal data, we propose a novel convolutional
architecture with partially asymmetric design based on the scheme of sliding
window to realize accurate time series forecasting. First, the construction
strategy of traditional fuzzy time series is improved to further extract short
and long term temporal interrelation, which enables every time node to
automatically possess corresponding global information and inner relationships
among them in a restricted sliding window and the process does not require
human involvement. Second, a bilateral Atrous algorithm is devised to reduce
calculation demand of the proposed model without sacrificing global
characteristics of elements. And it also allows the model to avoid processing
redundant information. Third, after the transformation of time series, a
partially asymmetric convolutional architecture is designed to more flexibly
mine data features by filters in different directions on feature maps, which
gives the convolutional neural network (CNN) the ability to construct
sub-windows within existing sliding windows to model at a more fine-grained
level. And after obtaining the time series information at different levels, the
multi-scale features from different sub-windows will be sent to the
corresponding network layer for time series information fusion. Compared with
other competitive modern models, the proposed method achieves state-of-the-art
results on most of popular time series datasets, which is fully verified by the
experimental results.

</details>


### [40] [A General Framework for Dynamic MAPF using Multi-Shot ASP and Tunnels](https://arxiv.org/abs/2507.20703)
*Aysu Bogatarkan,Esra Erdem*

Main category: cs.AI

TL;DR: 研究动态MAPF (D - MAPF)问题，给出通用定义、新框架和基于ASP的新方法，并进行实验评估。


<details>
  <summary>Details</summary>
Motivation: 受MAPF计划执行和监控的启发，研究允许环境变化的D - MAPF问题，考虑仓库等现实应用需求。

Method: 引入D - MAPF通用定义，提出新框架利用多轮计算并可使用不同方法，提出基于ASP的新方法结合重规划和修复方法优点并引入隧道概念。

Result: 通过实验评估说明该方法在计算性能和解决方案质量方面的优缺点。

Conclusion: 文中未明确提及最终结论，推测新方法有一定潜力但也存在不足。

Abstract: MAPF problem aims to find plans for multiple agents in an environment within
a given time, such that the agents do not collide with each other or obstacles.
Motivated by the execution and monitoring of these plans, we study Dynamic MAPF
(D-MAPF) problem, which allows changes such as agents entering/leaving the
environment or obstacles being removed/moved. Considering the requirements of
real-world applications in warehouses with the presence of humans, we introduce
1) a general definition for D-MAPF (applicable to variations of D-MAPF), 2) a
new framework to solve D-MAPF (utilizing multi-shot computation, and allowing
different methods to solve D-MAPF), and 3) a new ASP-based method to solve
D-MAPF (combining advantages of replanning and repairing methods, with a novel
concept of tunnels to specify where agents can move). We have illustrated the
strengths and weaknesses of this method by experimental evaluations, from the
perspectives of computational performance and quality of solutions.

</details>


### [41] [Algorithmic Fairness: A Runtime Perspective](https://arxiv.org/abs/2507.20711)
*Filip Cano,Thomas A. Henzinger,Konstantin Kueffner*

Main category: cs.AI

TL;DR: 提出分析公平性作为运行时属性的框架，研究监控和执行公平性问题并给出策略总结与一般结果，还调查了现有解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统AI公平性研究是针对固定数据集一次性评估的静态属性，而现实中AI系统是序列运行且结果和环境随时间演变，因此需要新的公平性分析方法。

Method: 使用基于可能有偏差演变的抛硬币序列的简约且有表现力的模型，研究监控和执行公平性问题，根据环境动态、预测范围和置信阈值参数化策略。

Result: 给出监控和执行公平性问题的策略总结，在简单或最小假设下给出一般结果，调查了监控和执行问题的现有解决方案。

Conclusion: 提出了运行时公平性分析框架，为AI公平性研究提供了新视角和相关策略及结果。

Abstract: Fairness in AI is traditionally studied as a static property evaluated once,
over a fixed dataset. However, real-world AI systems operate sequentially, with
outcomes and environments evolving over time. This paper proposes a framework
for analysing fairness as a runtime property. Using a minimal yet expressive
model based on sequences of coin tosses with possibly evolving biases, we study
the problems of monitoring and enforcing fairness expressed in either toss
outcomes or coin biases. Since there is no one-size-fits-all solution for
either problem, we provide a summary of monitoring and enforcement strategies,
parametrised by environment dynamics, prediction horizon, and confidence
thresholds. For both problems, we present general results under simple or
minimal assumptions. We survey existing solutions for the monitoring problem
for Markovian and additive dynamics, and existing solutions for the enforcement
problem in static settings with known dynamics.

</details>


### [42] [Learning the Value Systems of Societies from Preferences](https://arxiv.org/abs/2507.20728)
*Andrés Holgado-Sánchez,Holger Billhardt,Sascha Ossowski,Sara Degli-Esposti*

Main category: cs.AI

TL;DR: 本文聚焦伦理AI中使AI与人类价值对齐问题，提出基于启发式深度聚类学习社会价值系统的方法并在出行决策用例中评估。


<details>
  <summary>Details</summary>
Motivation: 在价值感知AI系统中，手动获取和校准个人价值观及其聚合困难，且社会价值系统应视为不同群体价值系统集合，而非个体简单聚合，因此需学习社会价值系统。

Method: 提出基于启发式深度聚类的方法，通过观察部分主体的定性价值偏好来学习社会共享价值基础和多样化价值系统。

Result: 在出行决策的实际用例中对所提方法进行了评估。

Conclusion: 未明确提及结论，推测该方法为学习社会价值系统提供了可行途径。

Abstract: Aligning AI systems with human values and the value-based preferences of
various stakeholders (their value systems) is key in ethical AI. In value-aware
AI systems, decision-making draws upon explicit computational representations
of individual values (groundings) and their aggregation into value systems. As
these are notoriously difficult to elicit and calibrate manually, value
learning approaches aim to automatically derive computational models of an
agent's values and value system from demonstrations of human behaviour.
Nonetheless, social science and humanities literature suggest that it is more
adequate to conceive the value system of a society as a set of value systems of
different groups, rather than as the simple aggregation of individual value
systems. Accordingly, here we formalize the problem of learning the value
systems of societies and propose a method to address it based on heuristic deep
clustering. The method learns socially shared value groundings and a set of
diverse value systems representing a given society by observing qualitative
value-based preferences from a sample of agents. We evaluate the proposal in a
use case with real data about travelling decisions.

</details>


### [43] [Beyond Listenership: AI-Predicted Interventions Drive Improvements in Maternal Health Behaviours](https://arxiv.org/abs/2507.20755)
*Arpan Dasgupta,Sarvesh Gharat,Neha Madhiwalla,Aparna Hegde,Milind Tambe,Aparna Taneja*

Main category: cs.AI

TL;DR: 研究表明AI干预自动语音健康信息电话不仅提升收听率，还促进母婴健康行为改变。


<details>
  <summary>Details</summary>
Motivation: 过往研究证明AI模型可防止自动语音健康信息电话的受益人流失和提高参与度，但AI干预带来的收听率提升是否能转化为受益人知识和健康行为的改善尚不明确。

Method: 开展研究探究AI干预带来的收听率提升与健康行为改变的关联。

Result: AI调度干预在提升收听率的同时，使受益人在产后服用铁或钙补充剂等健康行为及对孕期和婴儿期关键健康话题的理解有显著改善。

Conclusion: AI有潜力推动母婴健康的有意义改善。

Abstract: Automated voice calls with health information are a proven method for
disseminating maternal and child health information among beneficiaries and are
deployed in several programs around the world. However, these programs often
suffer from beneficiary dropoffs and poor engagement. In previous work, through
real-world trials, we showed that an AI model, specifically a restless bandit
model, could identify beneficiaries who would benefit most from live service
call interventions, preventing dropoffs and boosting engagement. However, one
key question has remained open so far: does such improved listenership via
AI-targeted interventions translate into beneficiaries' improved knowledge and
health behaviors? We present a first study that shows not only listenership
improvements due to AI interventions, but also simultaneously links these
improvements to health behavior changes. Specifically, we demonstrate that
AI-scheduled interventions, which enhance listenership, lead to statistically
significant improvements in beneficiaries' health behaviors such as taking iron
or calcium supplements in the postnatal period, as well as understanding of
critical health topics during pregnancy and infancy. This underscores the
potential of AI to drive meaningful improvements in maternal and child health.

</details>


### [44] [How Chain-of-Thought Works? Tracing Information Flow from Decoding, Projection, and Activation](https://arxiv.org/abs/2507.20758)
*Hao Yang,Qinghua Zhao,Lei Li*

Main category: cs.AI

TL;DR: 分析思维链（CoT）提示工作原理，发现其可作为解码空间修剪器，且以任务依赖方式调节神经元参与，提供新机制解释框架。


<details>
  <summary>Details</summary>
Motivation: 思维链（CoT）提示能显著提升模型推理能力，但内部机制不明，需进行分析。

Method: 通过反向追踪解码、投影和激活阶段的信息流来分析CoT的工作原理。

Result: CoT可作为解码空间修剪器，利用答案模板引导输出生成；CoT以任务依赖方式调节神经元参与，在开放域任务中减少神经元激活，在封闭域场景中增加。

Conclusion: 研究提供了新的机制解释框架，为设计更高效强大的提示提供关键见解。

Abstract: Chain-of-Thought (CoT) prompting significantly enhances model reasoning, yet
its internal mechanisms remain poorly understood. We analyze CoT's operational
principles by reversely tracing information flow across decoding, projection,
and activation phases. Our quantitative analysis suggests that CoT may serve as
a decoding space pruner, leveraging answer templates to guide output
generation, with higher template adherence strongly correlating with improved
performance. Furthermore, we surprisingly find that CoT modulates neuron
engagement in a task-dependent manner: reducing neuron activation in
open-domain tasks, yet increasing it in closed-domain scenarios. These findings
offer a novel mechanistic interpretability framework and critical insights for
enabling targeted CoT interventions to design more efficient and robust
prompts. We released our code and data at
https://anonymous.4open.science/r/cot-D247.

</details>


### [45] [evalSmarT: An LLM-Based Framework for Evaluating Smart Contract Generated Comments](https://arxiv.org/abs/2507.20774)
*Fatou Ndiaye Mbodji*

Main category: cs.AI

TL;DR: 本文提出一个模块化可扩展框架evalSmarT，利用大语言模型评估智能合约注释生成质量，展示其应用并得出LLM评估有优势的结论。


<details>
  <summary>Details</summary>
Motivation: 传统指标无法捕捉特定领域细微差别，人工评估成本高且不可扩展，需要新方法评估智能合约生成注释质量。

Method: 提出模块化可扩展框架evalSmarT，结合约40个大语言模型和10种提示策略，支持超400种评估器配置。

Result: 提示设计显著影响与人类判断的一致性，基于大语言模型的评估提供了可扩展且语义丰富的替代方案。

Conclusion: 基于大语言模型的评估是现有方法的有效可扩展替代方案。

Abstract: Smart contract comment generation has gained traction as a means to improve
code comprehension and maintainability in blockchain systems. However,
evaluating the quality of generated comments remains a challenge. Traditional
metrics such as BLEU and ROUGE fail to capture domain-specific nuances, while
human evaluation is costly and unscalable. In this paper, we present
\texttt{evalSmarT}, a modular and extensible framework that leverages large
language models (LLMs) as evaluators. The system supports over 400 evaluator
configurations by combining approximately 40 LLMs with 10 prompting strategies.
We demonstrate its application in benchmarking comment generation tools and
selecting the most informative outputs. Our results show that prompt design
significantly impacts alignment with human judgment, and that LLM-based
evaluation offers a scalable and semantically rich alternative to existing
methods.

</details>


### [46] [MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs](https://arxiv.org/abs/2507.20804)
*Xueyao Wan,Hang Yu*

Main category: cs.AI

TL;DR: 提出MMGraphRAG方法解决多模态RAG问题，在数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法缺失多模态信息，现有多模态RAG方法无法捕捉知识结构和模态间逻辑链，泛化能力有限。

Method: 通过场景图细化视觉内容，结合文本KG构建MMKG，利用谱聚类实现跨模态实体链接，沿推理路径检索上下文指导生成。

Result: MMGraphRAG在DocBench和MMLongBench数据集上达到了最先进的性能。

Conclusion: MMGraphRAG具有强大的领域适应性和清晰的推理路径。

Abstract: Retrieval-Augmented Generation (RAG) enhances language model generation by
retrieving relevant information from external knowledge bases. However,
conventional RAG methods face the issue of missing multimodal information.
Multimodal RAG methods address this by fusing images and text through mapping
them into a shared embedding space, but they fail to capture the structure of
knowledge and logical chains between modalities. Moreover, they also require
large-scale training for specific tasks, resulting in limited generalizing
ability. To address these limitations, we propose MMGraphRAG, which refines
visual content through scene graphs and constructs a multimodal knowledge graph
(MMKG) in conjunction with text-based KG. It employs spectral clustering to
achieve cross-modal entity linking and retrieves context along reasoning paths
to guide the generative process. Experimental results show that MMGraphRAG
achieves state-of-the-art performance on the DocBench and MMLongBench datasets,
demonstrating strong domain adaptability and clear reasoning paths.

</details>


### [47] [Partially Observable Monte-Carlo Graph Search](https://arxiv.org/abs/2507.20951)
*Yang You,Vincent Thomas,Alex Schutz,Robert Skilton,Nick Hawes,Olivier Buffet*

Main category: cs.AI

TL;DR: 提出基于采样的离线算法POMCGS解决大型POMDPs问题，能处理连续POMDPs，实验表明其表现好。


<details>
  <summary>Details</summary>
Motivation: 现有离线算法无法处理大型POMDPs，而时间或能量受限场景更需要预计算的离线策略。

Method: 提出POMCGS算法，实时折叠搜索树构建策略图，结合动作渐进扩展和观测聚类方法处理连续POMDPs。

Result: POMCGS能在最具挑战性的POMDPs上生成策略，策略价值与最先进的在线算法有竞争力。

Conclusion: POMCGS是解决大型POMDPs离线问题的有效方法。

Abstract: Currently, large partially observable Markov decision processes (POMDPs) are
often solved by sampling-based online methods which interleave planning and
execution phases. However, a pre-computed offline policy is more desirable in
POMDP applications with time or energy constraints. But previous offline
algorithms are not able to scale up to large POMDPs. In this article, we
propose a new sampling-based algorithm, the partially observable Monte-Carlo
graph search (POMCGS) to solve large POMDPs offline. Different from many online
POMDP methods, which progressively develop a tree while performing
(Monte-Carlo) simulations, POMCGS folds this search tree on the fly to
construct a policy graph, so that computations can be drastically reduced, and
users can analyze and validate the policy prior to embedding and executing it.
Moreover, POMCGS, together with action progressive widening and observation
clustering methods provided in this article, is able to address certain
continuous POMDPs. Through experiments, we demonstrate that POMCGS can generate
policies on the most challenging POMDPs, which cannot be computed by previous
offline algorithms, and these policies' values are competitive compared with
the state-of-the-art online POMDP algorithms.

</details>


### [48] [On the Limits of Hierarchically Embedded Logic in Classical Neural Networks](https://arxiv.org/abs/2507.20960)
*Bill Cochran*

Main category: cs.AI

TL;DR: 提出基于神经架构深度的语言大模型推理限制形式化模型，证明网络深度对逻辑表达有上限，解释现象并为未来发展提供基础。


<details>
  <summary>Details</summary>
Motivation: 探究语言大模型推理限制，为模型发展提供理论支持。

Method: 将神经网络视为逻辑谓词空间上的线性算子，分析每层编码逻辑推理的能力。

Result: 证明特定深度神经网络无法忠实表示高一阶逻辑中的谓词，结构在分词和嵌入时有非平凡零空间，排除高阶谓词可表示性。

Conclusion: 框架解释了幻觉、重复等现象，为理解高阶逻辑近似如何出现提供基础，推动语言模型架构扩展和可解释性策略发展。

Abstract: We propose a formal model of reasoning limitations in large neural net models
for language, grounded in the depth of their neural architecture. By treating
neural networks as linear operators over logic predicate space we show that
each layer can encode at most one additional level of logical reasoning. We
prove that a neural network of depth a particular depth cannot faithfully
represent predicates in a one higher order logic, such as simple counting over
complex predicates, implying a strict upper bound on logical expressiveness.
This structure induces a nontrivial null space during tokenization and
embedding, excluding higher-order predicates from representability. Our
framework offers a natural explanation for phenomena such as hallucination,
repetition, and limited planning, while also providing a foundation for
understanding how approximations to higher-order logic may emerge. These
results motivate architectural extensions and interpretability strategies in
future development of language models.

</details>


### [49] [MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them](https://arxiv.org/abs/2507.21017)
*Weichen Zhang,Yiyou Sun,Pohao Huang,Jiayue Pu,Heyue Lin,Dawn Song*

Main category: cs.AI

TL;DR: 本文提出首个统一基准MIRAGE - Bench，用于引出和评估交互式大语言模型代理场景中的幻觉问题，提供了行动见解和改进基础。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理幻觉评估零散且缺乏原则性测试平台，需统一基准评估。

Method: 引入三部分分类法处理代理幻觉；对现有代理基准进行系统审计引出失败情况；用快照策略合成测试用例；采用细粒度的大语言模型评判范式和定制风险感知提示评估幻觉行为。

Result: 得到了可操作的大语言模型代理失败模式见解。

Conclusion: MIRAGE - Bench为缓解交互式环境中的幻觉问题奠定了原则性进展基础。

Abstract: Hallucinations pose critical risks for large language model (LLM)-based
agents, often manifesting as hallucinative actions resulting from fabricated or
misinterpreted information within the cognitive context. While recent studies
have exposed such failures, existing evaluations remain fragmented and lack a
principled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions
in Risky AGEnt settings--the first unified benchmark for eliciting and
evaluating hallucinations in interactive LLM-agent scenarios. We begin by
introducing a three-part taxonomy to address agentic hallucinations: actions
that are unfaithful to (i) task instructions, (ii) execution history, or (iii)
environment observations. To analyze, we first elicit such failures by
performing a systematic audit of existing agent benchmarks, then synthesize
test cases using a snapshot strategy that isolates decision points in
deterministic and reproducible manners. To evaluate hallucination behaviors, we
adopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware
prompts, enabling scalable, high-fidelity assessment of agent actions without
enumerating full action spaces. MIRAGE-Bench provides actionable insights on
failure modes of LLM agents and lays the groundwork for principled progress in
mitigating hallucinations in interactive environments.

</details>


### [50] [GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis](https://arxiv.org/abs/2507.21035)
*Haoyang Liu,Yijiang Li,Haohan Wang*

Main category: cs.AI

TL;DR: 基因表达分析具挑战性，现有自动化方法有局限，GenoMAS提出基于LLM的科学家团队，在基准测试中表现超现有技术，还发现合理基因 - 表型关联。


<details>
  <summary>Details</summary>
Motivation: 现有基因表达分析自动化方法存在工作流不灵活或自主性差的问题，难以从原始转录组数据中提取信息。

Method: GenoMAS提出基于LLM的科学家团队，通过类型化消息传递协议协调六个专业LLM代理，以引导式规划框架处理任务。

Result: 在GenoTEX基准测试中，数据预处理的综合相似性相关性达89.13%，基因识别的F1值达60.48%，均超现有技术；发现生物学上合理的基因 - 表型关联。

Conclusion: GenoMAS结合结构化工作流的可靠性和自主代理的适应性，能有效处理基因表达分析问题。

Abstract: Gene expression analysis holds the key to many biomedical discoveries, yet
extracting insights from raw transcriptomic data remains formidable due to the
complexity of multiple large, semi-structured files and the need for extensive
domain expertise. Current automation approaches are often limited by either
inflexible workflows that break down in edge cases or by fully autonomous
agents that lack the necessary precision for rigorous scientific inquiry.
GenoMAS charts a different course by presenting a team of LLM-based scientists
that integrates the reliability of structured workflows with the adaptability
of autonomous agents. GenoMAS orchestrates six specialized LLM agents through
typed message-passing protocols, each contributing complementary strengths to a
shared analytic canvas. At the heart of GenoMAS lies a guided-planning
framework: programming agents unfold high-level task guidelines into Action
Units and, at each juncture, elect to advance, revise, bypass, or backtrack,
thereby maintaining logical coherence while bending gracefully to the
idiosyncrasies of genomic data.
  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation
of 89.13% for data preprocessing and an F$_1$ of 60.48% for gene
identification, surpassing the best prior art by 10.61% and 16.85%
respectively. Beyond metrics, GenoMAS surfaces biologically plausible
gene-phenotype associations corroborated by the literature, all while adjusting
for latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.

</details>


### [51] [A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence](https://arxiv.org/abs/2507.21046)
*Huan-ang Gao,Jiayi Geng,Wenyue Hua,Mengkang Hu,Xinzhe Juan,Hongzhang Liu,Shilong Liu,Jiahao Qiu,Xuan Qi,Yiran Wu,Hongru Wang,Han Xiao,Yuhang Zhou,Shaokun Zhang,Jiayi Zhang,Jinyu Xiang,Yixiong Fang,Qiwen Zhao,Dongrui Liu,Qihan Ren,Cheng Qian,Zhenghailong Wang,Minda Hu,Huazheng Wang,Qingyun Wu,Heng Ji,Mengdi Wang*

Main category: cs.AI

TL;DR: 文章对自进化智能体进行系统全面综述，介绍进化维度、机制、评估指标等，指出应用、挑战和研究方向，为发展自适应智能体系统提供路线图。


<details>
  <summary>Details</summary>
Motivation: 大语言模型静态特性成瓶颈，需能实时自适应推理、行动和进化的智能体，因此开展对自进化智能体的研究。

Method: 围绕三个基础维度组织综述，考察智能体组件的进化机制，按阶段对适应方法分类，分析算法和架构设计，分析评估指标和基准。

Result: 对自进化智能体进行系统综述，明确各方面内容，如进化机制、适应方法、评估指标等，指出应用领域。

Conclusion: 为研究和实际部署中推进自适应智能体系统提供路线图，有助于实现人工超级智能。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities but remain
fundamentally static, unable to adapt their internal parameters to novel tasks,
evolving knowledge domains, or dynamic interaction contexts. As LLMs are
increasingly deployed in open-ended, interactive environments, this static
nature has become a critical bottleneck, necessitating agents that can
adaptively reason, act, and evolve in real time. This paradigm shift -- from
scaling static models to developing self-evolving agents -- has sparked growing
interest in architectures and methods enabling continual learning and
adaptation from data, interactions, and experiences. This survey provides the
first systematic and comprehensive review of self-evolving agents, organized
around three foundational dimensions -- what to evolve, when to evolve, and how
to evolve. We examine evolutionary mechanisms across agent components (e.g.,
models, memory, tools, architecture), categorize adaptation methods by stages
(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and
architectural designs that guide evolutionary adaptation (e.g., scalar rewards,
textual feedback, single-agent and multi-agent systems). Additionally, we
analyze evaluation metrics and benchmarks tailored for self-evolving agents,
highlight applications in domains such as coding, education, and healthcare,
and identify critical challenges and research directions in safety,
scalability, and co-evolutionary dynamics. By providing a structured framework
for understanding and designing self-evolving agents, this survey establishes a
roadmap for advancing adaptive agentic systems in both research and real-world
deployments, ultimately shedding lights to pave the way for the realization of
Artificial Super Intelligence (ASI), where agents evolve autonomously,
performing at or beyond human-level intelligence across a wide array of tasks.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [52] [IFD: A Large-Scale Benchmark for Insider Filing Violation Detection](https://arxiv.org/abs/2507.20162)
*Cheng Huang,Fan Gao,Yutong Liu,Yadi Liu,Xiaoli Ma,Ye Aung Moe,Yuhan Zhang,Yao Ma,Hao Wang,Xiangxiang Wang,Yongbin Yu*

Main category: cs.CE

TL;DR: 本文引入公开数据集IFD用于内幕交易违规检测，提出MaBoost框架，实验显示MaBoost表现优于先前方法，IFD为金融合规AI模型开发提供基准。


<details>
  <summary>Details</summary>
Motivation: 内幕交易违规检测存在缺乏大规模标注数据集和特定任务基准的问题，需要解决监管执行受限的状况。

Method: 引入包含超百万笔Form 4交易的IFD数据集，提出结合Mamba编码器和XGBoost的MaBoost混合框架。

Result: MaBoost在识别高风险行为模式上有高准确性和可解释性，F1分数在受限监管设置下达99.47%，优于先前方法。

Conclusion: IFD为金融合规、监管取证和可解释时间序列分类的AI模型开发提供现实、可复现且行为丰富的基准。

Abstract: Insider trading violations, particularly delayed disclosures of Form 4
filings, remain a persistent challenge for financial market surveillance.
Despite regulatory requirements such as the two-business-day rule of the
Securities and Exchange Commission (SEC), enforcement is limited by the lack of
large-scale, labeled datasets and task-specific benchmarks. In this paper, we
introduce Insider Filing Delay (IFD), the first and largest publicly available
dataset for insider disclosure behavior, comprising over one million Form 4
transactions spanning two decades (2002-2025), with structured annotations on
delay status, insider roles, governance factors, and firm-level financial
indicators. IFD enables the first large-scale formulation of strategic
disclosure violation detection as a binary classification task grounded in
regulatory compliance. To demonstrate the utility of IFD, we propose MaBoost, a
hybrid framework combining a Mamba-based state space encoder with XGBoost,
achieving high accuracy and interpretability in identifying high-risk
behavioral patterns. Experiments across statistical baselines, deep learning
models, and large language models confirm that MaBoost outperforms prior
approaches, achieving an F1-score of up to 99.47% under constrained regulatory
settings. IFD provides a realistic, reproducible, and behavior-rich benchmark
for developing AI models in financial compliance, regulatory forensics, and
interpretable time-series classification. All data and codes are available:
https://github.com/CH-YellowOrange/MaBoost-and-IFD.

</details>


### [53] [Learning Explainable Stock Predictions with Tweets Using Mixture of Experts](https://arxiv.org/abs/2507.20535)
*Wenyan Xu,Dawei Xiang,Rundong Wang,Yonghong Hu,Liang Zhang,Jiayu Chen,Zhonghua Lu*

Main category: cs.CE

TL;DR: 本文提出FTS - Text - MoE模型结合数值与文本数据预测股价，降低计算成本，实验显示其在投资回报和夏普比率上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 股价受历史数据和文本信息影响，现有基于大语言模型的提示方法有输入长度限制、难预测不同长度序列，且多数模型计算密集、资源消耗大，需改进。

Method: 提出FTS - Text - MoE模型，结合数值数据与新闻、推文关键摘要，用MoE Transformer解码器处理数据，通过激活部分参数降低计算成本，设置多分辨率预测头实现不同尺度预测。

Result: FTS - Text - MoE在投资回报和夏普比率方面优于基线方法。

Conclusion: FTS - Text - MoE模型准确性高，能更好预测未来市场趋势。

Abstract: Stock price movements are influenced by many factors, and alongside
historical price data, tex-tual information is a key source. Public news and
social media offer valuable insights into market sentiment and emerging events.
These sources are fast-paced, diverse, and significantly impact future stock
trends. Recently, LLMs have enhanced financial analysis, but prompt-based
methods still have limitations, such as input length restrictions and
difficulties in predicting sequences of varying lengths. Additionally, most
models rely on dense computational layers, which are resource-intensive. To
address these challenges, we propose the FTS- Text-MoE model, which combines
numerical data with key summaries from news and tweets using point embeddings,
boosting prediction accuracy through the integration of factual textual data.
The model uses a Mixture of Experts (MoE) Transformer decoder to process both
data types. By activating only a subset of model parameters, it reduces
computational costs. Furthermore, the model features multi-resolution
prediction heads, enabling flexible forecasting of financial time series at
different scales. Experimental results show that FTS-Text-MoE outperforms
baseline methods in terms of investment returns and Sharpe ratio, demonstrating
its superior accuracy and ability to predict future market trends.

</details>


### [54] [Exascale Implicit Kinetic Plasma Simulations on El~Capitan for Solving the Micro-Macro Coupling in Magnetospheric Physics](https://arxiv.org/abs/2507.20719)
*Stefano Markidis,Andong Hu,Ivy Peng,Luca Pennati,Ian Lumsden,Dewi Yokelson,Stephanie Brink,Olga Pearce,Thomas R. W. Scogland,Bronis R. de Supinski,Gian Luca Delzanno,Michela Taufer*

Main category: cs.CE

TL;DR: 使用iPIC3D隐式PIC模拟行星磁层，实现微观与宏观动力学多尺度耦合，有算法和技术创新，能处理此前无法模拟的系统。


<details>
  <summary>Details</summary>
Motivation: 解决空间物理学中行星磁层微观与宏观动力学多尺度耦合的基本挑战。

Method: 采用iPIC3D隐式PIC模拟，有GPU优化内核、粒子控制和基于高斯混合模型的数据压缩等创新。

Result: 模拟域达100 - 1000离子表皮深度，可模拟水星和木卫三等中小行星磁层。

Conclusion: 实现了此前全动力学PIC代码无法处理的系统的全动力学全局尺度模拟。

Abstract: Our fully kinetic, implicit Particle-in-Cell (PIC) simulations of global
magnetospheres on up to 32,768 of El Capitan's AMD Instinct MI300A Accelerated
Processing Units (APUs) represent an unprecedented computational capability
that addresses a fundamental challenge in space physics: resolving the
multi-scale coupling between microscopic (electron-scale) and macroscopic
(global-scale) dynamics in planetary magnetospheres. The implicit scheme of
iPIC3D supports time steps and grid spacing that are up to 10 times larger than
those of explicit methods, without sacrificing physical accuracy. This enables
the simulation of magnetospheres while preserving fine-scale electron physics,
which is critical for key processes such as magnetic reconnection and plasma
turbulence. Our algorithmic and technological innovations include GPU-optimized
kernels, particle control, and physics-aware data compression using Gaussian
Mixture Models. With simulation domains spanning 100-1,000 ion skin depths, we
reach the global scale of small-to-medium planetary magnetospheres, such as
those of Mercury and Ganymede, which supports fully kinetic treatment of
global-scale dynamics in systems previously out of reach for fully kinetic PIC
codes.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [55] [CleANN: Efficient Full Dynamism in Graph-based Approximate Nearest Neighbor Search](https://arxiv.org/abs/2507.19802)
*Ziyu Zhang,Yuanhao Wei,Joshua Engels,Julian Shun*

Main category: cs.DB

TL;DR: 提出CleANN系统解决动态图近似最近邻搜索索引问题，在多数据集验证效果好且效率高。


<details>
  <summary>Details</summary>
Motivation: 现有动态图索引在更新时存在查询质量下降和图结构更新成本高的问题，需要高效支持更新和查询的索引。

Method: 提出CleANN系统，包含工作量感知链接、查询自适应邻域合并和半懒内存清理三个组件。

Result: 在7个数据集上评估，查询质量与静态构建索引相当，在内存环境下百万级数据集有7 - 1200倍吞吐量提升。

Conclusion: CleANN是首个在全动态下兼顾质量和效率的并发近似最近邻搜索索引。

Abstract: Approximate nearest neighbor search (ANNS) has become a quintessential
algorithmic problem for various other foundational data tasks for AI workloads.
Graph-based ANNS indexes have superb empirical trade-offs in indexing cost,
query efficiency, and query approximation quality. Most existing graph-based
indexes are designed for the static scenario, where there are no updates to the
data after the index is constructed. However, full dynamism (insertions,
deletions, and searches) is crucial to providing up-to-date responses in
applications using vector databases. It is desirable that the index efficiently
supports updates and search queries concurrently. Existing dynamic graph-based
indexes suffer from at least one of the following problems: (1) the query
quality degrades as updates happen; and (2) the graph structure updates used to
maintain the index quality upon updates are global and thus expensive. To solve
these problems, we propose the CleANN system which consists of three main
components: (1) workload-aware linking of diverse search tree descendants to
combat distribution shift; (2)query-adaptive on-the-fly neighborhood
consolidation to efficiently handle deleted nodes; and (3) semi-lazy memory
cleaning to clean up stale information in the data structure and reduce the
work spent by the first two components. We evaluate CleANN on 7 diverse
datasets on fully dynamic workloads and find that CleANN has query quality at
least as good as if the index had been built statically using the corresponding
data. In the in-memory setting using 56 hyper-threads, with all types of
queries running concurrently, at the same recall level, CleANN achieves 7-1200x
throughput improvement on million-scale real-world datasets. To the best of our
knowledge, CleANN is the first concurrent ANNS index to achieve such efficiency
while maintaining quality under full dynamism.

</details>


### [56] [TIMEST: Temporal Information Motif Estimator Using Sampling Trees](https://arxiv.org/abs/2507.20441)
*Yunjie Pan,Omkar Bhalerao,C. Seshadhri,Nishil Talati*

Main category: cs.DB

TL;DR: 本文提出TIMEST算法用于计算时序网络中任意大小的时序 motif 数量，理论上有运行时间和近似保证，实验显示比现有算法更快更准。


<details>
  <summary>Details</summary>
Motivation: 现实网络边有时间戳，需进行时序 motif 挖掘，但现有算法难以处理顶点数超 4 的 motif。

Method: 引入时序生成树采样器，利用加权采样生成目标时序 motif 的子结构，采用随机估计技术。

Result: TIMEST 比现有算法更快更准确，CPU 实现比现有 GPU 精确算法平均快 28 倍，比 SOTA 近似算法快 6 倍，多数情况下误差小于 5%。

Conclusion: TIMEST 是一种通用、快速且准确的时序 motif 计数估计算法。

Abstract: The mining of pattern subgraphs, known as motifs, is a core task in the field
of graph mining. Edges in real-world networks often have timestamps, so there
is a need for temporal motif mining. A temporal motif is a richer structure
that imposes timing constraints on the edges of the motif. Temporal motifs have
been used to analyze social networks, financial transactions, and biological
networks.
  Motif counting in temporal graphs is particularly challenging. A graph with
millions of edges can have trillions of temporal motifs, since the same edge
can occur with multiple timestamps. There is a combinatorial explosion of
possibilities, and state-of-the-art algorithms cannot manage motifs with more
than four vertices.
  In this work, we present TIMEST: a general, fast, and accurate estimation
algorithm to count temporal motifs of arbitrary sizes in temporal networks. Our
approach introduces a temporal spanning tree sampler that leverages weighted
sampling to generate substructures of target temporal motifs. This method
carefully takes a subset of temporal constraints of the motif that can be
jointly and efficiently sampled. TIMEST uses randomized estimation techniques
to obtain accurate estimates of motif counts.
  We give theoretical guarantees on the running time and approximation
guarantees of TIMEST. We perform an extensive experimental evaluation and show
that TIMEST is both faster and more accurate than previous algorithms. Our CPU
implementation exhibits an average speedup of 28x over state-of-the-art GPU
implementation of the exact algorithm, and 6x speedup over SOTA approximate
algorithms while consistently showcasing less than 5% error in most cases. For
example, TIMEST can count the number of instances of a financial fraud temporal
motif in four minutes with 0.6% error, while exact methods take more than two
days.

</details>


### [57] [A Functional Data Model and Query Language is All You Need](https://arxiv.org/abs/2507.20671)
*Jens Dittrich*

Main category: cs.DB

TL;DR: 提出功能数据模型(FDM)和功能查询语言(FQL)，可解决SQL诸多问题，表达力更强，能与编程语言集成，有优化机会且无需开发者切换编程范式。


<details>
  <summary>Details</summary>
Motivation: 解决SQL存在的如NULL值、阻抗不匹配、SQL注入等诸多问题。

Method: 提出功能数据模型(FDM)和功能查询语言(FQL)。

Result: FDM和FQL比关系模型和SQL更具表达力，FQL能与现有编程语言平滑集成，QL和PL可成为‘同一事物’。

Conclusion: FDM和FQL有解决SQL问题的潜力，为编译器和数据库间带来优化机会，无需开发者切换编程范式。

Abstract: We propose the vision of a functional data model (FDM) and an associated
functional query language (FQL). Our proposal has far-reaching consequences: we
show a path to come up with a modern QL that solves (almost if not) all
problems of SQL (NULL-values, impedance mismatch, SQL injection, missing
querying capabilities for updates, etc.). FDM and FQL are much more expressive
than the relational model and SQL. In addition, in contrast to SQL, FQL
integrates smoothly into existing programming languages. In our approach both
QL and PL become the "same thing", thus opening up some interesting holistic
optimization opportunities between compilers and databases. In FQL, we also do
not need to force application developers to switch to unfamiliar programming
paradigms (like SQL or datalog): developers can stick with the abstractions
provided by their programming language.

</details>


### [58] [MVIAnalyzer: A Holistic Approach to Analyze Missing Value Imputation](https://arxiv.org/abs/2507.20815)
*Valerie Restat,Kai Tejkl,Uta Störl*

Main category: cs.DB

TL;DR: 本文提出MVIAnalyzer框架用于全面分析缺失值插补（MVI），展示其应用并评估不同MVI方法的可能性与局限性，还说明可视化对分析的支持作用。


<details>
  <summary>Details</summary>
Motivation: 缺失值限制数据分析使用或导致结果错误，且无通用公平的MVI方法，因此需将MVI置于数据分析整体情境中。

Method: 提出MVIAnalyzer框架，考虑机器学习方法应用和分析的全过程，提供相关软件，包含缺失值模拟，在不同特征数据上展示应用。

Result: 通过对结果评估，显示了不同MVI方法的可能性和局限性。

Conclusion: MVI是复杂话题，可视化可支持相关分析。

Abstract: Missing values often limit the usage of data analysis or cause falsification
of results. Therefore, methods of missing value imputation (MVI) are of great
significance. However, in general, there is no universal, fair MVI method for
different tasks. This work thus places MVI in the overall context of data
analysis. For this purpose, we present the MVIAnalyzer, a generic framework for
a holistic analysis of MVI. It considers the overall process up to the
application and analysis of machine learning methods. The associated software
is provided and can be used by other researchers for their own analyses. To
this end, it further includes a missing value simulation with consideration of
relevant parameters. The application of the MVIAnalyzer is demonstrated on data
with different characteristics. An evaluation of the results shows the
possibilities and limitations of different MVI methods. Since MVI is a very
complex topic with different influencing variables, this paper additionally
illustrates how the analysis can be supported by visualizations.

</details>


### [59] [Data Cleaning of Data Streams](https://arxiv.org/abs/2507.20839)
*Valerie Restat,Niklas Rodenhausen,Carina Antonin,Uta Störl*

Main category: cs.DB

TL;DR: 本文深入探索数据流的数据清洗，分析其适用性并实验评估，指出数据流清洗不一致，还研究相关技术要求。


<details>
  <summary>Details</summary>
Motivation: 流式数据有特殊数据清洗需求，与静态数据不同，现有研究多关注静态数据清洗，因此需探索流式数据清洗。

Method: 对数据流数据清洗适用性进行详细分析，并通过综合实验评估理论考量，使用原型框架。

Result: 在处理数据流时清洗不一致。

Conclusion: 完成了对数据流数据清洗的探索，分析了适用性，指出清洗问题，并研究了相关技术要求。

Abstract: Streaming data can arise from a variety of contexts. Important use cases are
continuous sensor measurements such as temperature, light or radiation values.
In the process, streaming data may also contain data errors that should be
cleaned before further use. Many studies from science and practice focus on
data cleaning in a static context. However, in terms of data cleaning,
streaming data has particularities that distinguish it from static data. In
this paper, we have therefore undertaken an intensive exploration of data
cleaning of data streams. We provide a detailed analysis of the applicability
of data cleaning to data streams. Our theoretical considerations are evaluated
in comprehensive experiments. Using a prototype framework, we show that
cleaning is not consistent when working with data streams. An additional
contribution is the investigation of requirements for streaming technologies in
context of data cleaning.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [60] [Quantifying the Performance Gap for Simple Versus Optimal Dynamic Server Allocation Policies](https://arxiv.org/abs/2507.19667)
*Niklas Carlsson,Derek Eager*

Main category: cs.DC

TL;DR: 本文介绍动态服务器分配策略，开发分析模型，设计半马尔可夫决策模型，研究多站点系统状态依赖路由性能收益，为服务提供商平衡成本和延迟提供见解。


<details>
  <summary>Details</summary>
Motivation: 云计算需动态分配服务器策略以应对负载条件。

Method: 描述简单动态服务器分配策略、开发分析模型、设计半马尔可夫决策模型。

Result: 量化了简单策略和最优策略的性能差距，研究了多站点系统状态依赖路由潜在性能收益。

Conclusion: 研究结果对服务提供商平衡云服务成本和延迟有价值。

Abstract: Cloud computing enables the dynamic provisioning of server resources. To
exploit this opportunity, a policy is needed for dynamically allocating (and
deallocating) servers in response to the current load conditions. In this paper
we describe several simple policies for dynamic server allocation and develop
analytic models for their analysis. We also design semi-Markov decision models
that enable determination of the performance achieved with optimal policies,
allowing us to quantify the performance gap between simple, easily implemented
policies, and optimal policies. Finally, we apply our models to study the
potential performance benefits of state-dependent routing in multi-site systems
when using dynamic server allocation at each site. Insights from our results
are valuable to service providers wanting to balance cloud service costs and
delays.

</details>


### [61] [Oranits: Mission Assignment and Task Offloading in Open RAN-based ITS using Metaheuristic and Deep Reinforcement Learning](https://arxiv.org/abs/2507.19712)
*Ngoc Hung Nguyen,Nguyen Van Thieu,Quang-Trung Luu,Anh Tuan Nguyen,Senura Wanasekara,Nguyen Cong Luong,Fatemeh Kavehmadavani,Van-Dinh Nguyen*

Main category: cs.DC

TL;DR: 本文研究基于Open RAN的智能交通系统中的任务分配与卸载，提出Oranits系统模型和两种优化方法，模拟显示方法有效提升任务处理效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视任务间复杂依赖和卸载成本，导致决策欠佳。

Method: 提出Oranits系统模型，采用元启发式进化计算算法CGG - ARO进行单时隙优化，设计增强奖励的深度强化学习框架MA - DDQN，集成多智能体协调和多动作选择机制。

Result: CGG - ARO使完成任务数量和总体收益分别提高约7.1%和7.7%，MA - DDQN使完成任务数量和总体收益分别提高11.0%和12.5%。

Conclusion: Oranits能在动态智能交通环境中实现更快、更具适应性和高效的任务处理。

Abstract: In this paper, we explore mission assignment and task offloading in an Open
Radio Access Network (Open RAN)-based intelligent transportation system (ITS),
where autonomous vehicles leverage mobile edge computing for efficient
processing. Existing studies often overlook the intricate interdependencies
between missions and the costs associated with offloading tasks to edge
servers, leading to suboptimal decision-making. To bridge this gap, we
introduce Oranits, a novel system model that explicitly accounts for mission
dependencies and offloading costs while optimizing performance through vehicle
cooperation. To achieve this, we propose a twofold optimization approach.
First, we develop a metaheuristic-based evolutionary computing algorithm,
namely the Chaotic Gaussian-based Global ARO (CGG-ARO), serving as a baseline
for one-slot optimization. Second, we design an enhanced reward-based deep
reinforcement learning (DRL) framework, referred to as the Multi-agent Double
Deep Q-Network (MA-DDQN), that integrates both multi-agent coordination and
multi-action selection mechanisms, significantly reducing mission assignment
time and improving adaptability over baseline methods. Extensive simulations
reveal that CGG-ARO improves the number of completed missions and overall
benefit by approximately 7.1% and 7.7%, respectively. Meanwhile, MA-DDQN
achieves even greater improvements of 11.0% in terms of mission completions and
12.5% in terms of the overall benefit. These results highlight the
effectiveness of Oranits in enabling faster, more adaptive, and more efficient
task processing in dynamic ITS environments.

</details>


### [62] [Accelerating Matrix Multiplication: A Performance Comparison Between Multi-Core CPU and GPU](https://arxiv.org/abs/2507.19723)
*Mufakir Qamar Ansari,Mudabir Qamar Ansari*

Main category: cs.DC

TL;DR: 本文对现代消费级异构平台上的矩阵乘法进行性能分析，实现并测试三种算法版本，结果显示GPU性能随问题规模显著提升，凸显其对加速数据并行工作负载的作用。


<details>
  <summary>Details</summary>
Motivation: 矩阵乘法计算复杂度高，是大规模应用的瓶颈，并行架构是解决方案，需对现代消费级异构平台上的矩阵乘法进行性能分析。

Method: 实现并基准测试三种算法版本，包括顺序C++实现、用OpenMP的多核CPU并行版本、用CUDA并优化共享内存的离散GPU大规模并行版本，用不同维度方阵评估。

Result: 并行CPU比顺序版本有12 - 14倍加速，4096x4096矩阵时，GPU实现比顺序基线约有593倍加速，比优化并行CPU版本有45倍加速。

Conclusion: 多核GPU架构对加速数据并行工作负载有深远影响，消费级硬件也能获得显著性能提升。

Abstract: Matrix multiplication is a foundational operation in scientific computing and
machine learning, yet its computational complexity makes it a significant
bottleneck for large-scale applications. The shift to parallel architectures,
primarily multi-core CPUs and many-core GPUs, is the established solution, and
these systems are now ubiquitous from datacenters to consumer laptops. This
paper presents a direct, empirical performance analysis of matrix
multiplication on a modern, consumer-grade heterogeneous platform. We
implemented and benchmarked three versions of the algorithm: a baseline
sequential C++ implementation, a parallel version for its multi-core CPU using
OpenMP, and a massively parallel version for its discrete GPU using CUDA with
shared memory optimizations. The implementations were evaluated with square
matrices of varying dimensions, from 128x128 to 4096x4096. Our results show
that while the parallel CPU provides a consistent speedup of 12-14x over the
sequential version, the GPU's performance scales dramatically with problem
size. For a 4096x4096 matrix, the GPU implementation achieved a speedup of
approximately 593x over the sequential baseline and 45x over the optimized
parallel CPU version. These findings quantitatively demonstrate the profound
impact of many-core GPU architectures on accelerating data-parallel workloads,
underscoring that significant performance gains are readily accessible even on
consumer-level hardware.

</details>


### [63] [MegatronApp: Efficient and Comprehensive Management on Distributed LLM Training](https://arxiv.org/abs/2507.19845)
*Bohan Zhao,Guang Yang,Shuo Chen,Ruitao Liu,Tingrui Zhang,Yongchao He,Wei Xu*

Main category: cs.DC

TL;DR: 大语言模型参数量剧增使训练变复杂，MegatronApp工具链解决相关系统级挑战，介绍各模块动机、架构及贡献。


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数量快速增长，训练从单节点变为跨节点复杂活动，现有框架如Megatron - LM带来系统级挑战，需工具解决。

Method: 开发开源工具链MegatronApp，包含MegaScan、MegaFBD、MegaDPP和MegaScope四个可组合模块。

Result: 提升生产级训练的可靠性、效率和透明度。

Conclusion: 各模块协同集成增强了Megatron - LM生态系统。

Abstract: The rapid escalation in the parameter count of large language models (LLMs)
has transformed model training from a single-node endeavor into a highly
intricate, cross-node activity. While frameworks such as Megatron-LM
successfully integrate tensor (TP), pipeline (PP), and data (DP) parallelism to
enable trillion-parameter training, they simultaneously expose practitioners to
unprecedented systems-level challenges in performance optimization, diagnosis,
and interpretability. MegatronApp is an open-source toolchain expressly
designed to meet these challenges. It introduces four orthogonal, yet
seamlessly composable modules--MegaScan, MegaFBD, MegaDPP, and MegaScope--that
collectively elevate the reliability, efficiency, and transparency of
production-scale training. This paper presents the motivation, architecture,
and distinctive contributions of each module, and elucidates how their
synergistic integration augments the Megatron-LM ecosystem.

</details>


### [64] [A Comparative Study of OpenMP Scheduling Algorithm Selection Strategies](https://arxiv.org/abs/2507.20312)
*Jonas H. Müller Korndörfer,Ali Mohammed,Ahmed Eleliemy,Quentin Guilloteau,Reto Krummenacher,Florina M. Ciorba*

Main category: cs.DC

TL;DR: 本文探讨OpenMP中基于学习的调度算法选择方法，评估专家和强化学习方法，结合两者可提升性能和适应性，且执行中动态选择算法对OpenMP应用可行有益。


<details>
  <summary>Details</summary>
Motivation: 科学和数据科学应用复杂度增加，高性能计算系统需有效调度和负载均衡，并行编程框架产生调度算法选择问题。

Method: 提出并评估基于专家和强化学习的方法，对六个应用和三个系统进行详细性能分析。

Result: 强化学习方法能学习高性能调度决策，但需大量探索；专家方法依赖先验知识，探索少但不一定能找到最优算法；结合两者可提升性能和适应性。

Conclusion: 执行中动态选择调度算法对OpenMP应用可行且有益，该方法可扩展到基于MPI的程序。

Abstract: Scientific and data science applications are becoming increasingly complex,
with growing computational and memory demands. Modern high performance
computing (HPC) systems provide high parallelism and heterogeneity across
nodes, devices, and cores. To achieve good performance, effective scheduling
and load balancing techniques are essential. Parallel programming frameworks
such as OpenMP now offer a variety of advanced scheduling algorithms to support
diverse applications and platforms. This creates an instance of the scheduling
algorithm selection problem, which involves identifying the most suitable
algorithm for a given combination of workload and system characteristics.
  In this work, we explore learning-based approaches for selecting scheduling
algorithms in OpenMP. We propose and evaluate expert-based and reinforcement
learning (RL)-based methods, and conduct a detailed performance analysis across
six applications and three systems. Our results show that RL methods are
capable of learning high-performing scheduling decisions, although they require
significant exploration, with the choice of reward function playing a key role.
Expert-based methods, in contrast, rely on prior knowledge and involve less
exploration, though they may not always identify the optimal algorithm for a
specific application-system pair. By combining expert knowledge with RL-based
learning, we achieve improved performance and greater adaptability.
  Overall, this work demonstrates that dynamic selection of scheduling
algorithms during execution is both viable and beneficial for OpenMP
applications. The approach can also be extended to MPI-based programs, enabling
optimization of scheduling decisions across multiple levels of parallelism.

</details>


### [65] [A Fast Parallel Median Filtering Algorithm Using Hierarchical Tiling](https://arxiv.org/abs/2507.19926)
*Louis Sugy*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Median filtering is a non-linear smoothing technique widely used in digital
image processing to remove noise while retaining sharp edges. It is
particularly well suited to removing outliers (impulse noise) or granular
artifacts (speckle noise). However, the high computational cost of median
filtering can be prohibitive. Sorting-based algorithms excel with small kernels
but scale poorly with increasing kernel diameter, in contrast to constant-time
methods characterized by higher constant factors but better scalability, such
as histogram-based approaches or the 2D wavelet matrix.
  This paper introduces a novel algorithm, leveraging the separability of the
sorting problem through hierarchical tiling to minimize redundant computations.
We propose two variants: a data-oblivious selection network that can operate
entirely within registers, and a data-aware version utilizing random-access
memory. These achieve per-pixel complexities of $O(k \log(k))$ and $O(k)$,
respectively, for a $k \times k$ kernel - unprecedented for sorting-based
methods. Our CUDA implementation is up to 5 times faster than the current state
of the art on a modern GPU and is the fastest median filter in most cases for
8-, 16-, and 32-bit data types and kernels from $3 \times 3$ to $75 \times 75$.

</details>


### [66] [Offloading tracing for real-time systems using a scalable cloud infrastructure](https://arxiv.org/abs/2507.19953)
*David Jannis Schmidt,Grigory Fridman,Florian von Zabiensky*

Main category: cs.DC

TL;DR: 本文提出基于微服务和边缘计算的实时系统软件跟踪云架构，转移处理负载，实现长期监控与协作分析，评估显示能高效处理并行跟踪会话。


<details>
  <summary>Details</summary>
Motivation: 传统跟踪工具依赖本地桌面，处理和存储能力有限，阻碍大规模分析，实时嵌入式系统需精确计时和故障检测。

Method: 提出基于微服务和边缘计算的云架构，用专用跟踪组件捕获数据，通过WebSockets和Apache Kafka转发到可扩展后端。

Result: 架构能高效处理许多并行跟踪会话，随系统负载增加，单会话吞吐量略有下降，但总体吞吐量增加。

Conclusion: 该架构支持并行跟踪会话的可扩展分析，为基于规则的测试和运行时验证的未来集成奠定基础，且不限于开发阶段，可用于现场运行时监控。

Abstract: Real-time embedded systems require precise timing and fault detection to
ensure correct behavior. Traditional tracing tools often rely on local desktops
with limited processing and storage capabilities, which hampers large-scale
analysis. This paper presents a scalable, cloud-based architecture for software
tracing in real-time systems based on microservices and edge computing. Our
approach shifts the trace processing workload from the developer's machine to
the cloud, using a dedicated tracing component that captures trace data and
forwards it to a scalable backend via WebSockets and Apache Kafka. This enables
long-term monitoring and collaborative analysis of target executions, e.g., to
detect and investigate sporadic errors. We demonstrate how this architecture
supports scalable analysis of parallel tracing sessions and lays the foundation
for future integration of rule-based testing and runtime verification. The
evaluation results show that the architecture can handle many parallel tracing
sessions efficiently, although the per-session throughput decreases slightly as
the system load increases, while the overall throughput increases. Although the
design includes a dedicated tracer for analysis during development, this
approach is not limited to such setups. Target systems with network
connectivity can stream reduced trace data directly, enabling runtime
monitoring in the field.

</details>


### [67] [MTASet: A Tree-based Set for Efficient Range Queries in Update-heavy Workloads](https://arxiv.org/abs/2507.20041)
*Daniel Manor,Mor Perry,Moshe Sulamy*

Main category: cs.DC

TL;DR: 本文介绍了MTASet，它利用并发(a,b)-树实现，能处理重更新负载和原子范围查询，性能优于现有同类产品，还能保证线性化。


<details>
  <summary>Details</summary>
Motivation: 现有并发集实现针对读操作优化，在重更新负载下表现差，且针对重更新任务优化的并发集处理原子范围查询效率低。

Method: 引入MTASet，利用并发(a,b)-树实现。

Result: MTASet在范围查询操作中比现有同类产品性能提升达2倍。

Conclusion: MTASet能适应重更新负载、实现原子范围查询，且保证线性化，表现优于现有方案。

Abstract: In concurrent data structures, the efficiency of set operations can vary
significantly depending on the workload characteristics. Numerous concurrent
set implementations are optimized and fine-tuned to excel in scenarios
characterized by predominant read operations. However, they often perform
poorly when confronted with workloads that heavily prioritize updates.
Additionally, current leading-edge concurrent sets optimized for update-heavy
tasks typically lack efficiency in handling atomic range queries. This study
introduces the MTASet, which leverages a concurrent (a,b)-tree implementation.
Engineered to accommodate update-heavy workloads and facilitate atomic range
queries, MTASet surpasses existing counterparts optimized for tasks in range
query operations by up to 2x. Notably, MTASet ensures linearizability.

</details>


### [68] [Ethereum Conflicts Graphed](https://arxiv.org/abs/2507.20196)
*Dvir David Biton,Roy Friedman,Yaron Hay*

Main category: cs.DC

TL;DR: 本文围绕以太坊智能合约交互展开研究，通过追踪大量以太坊区块，分析交易和调用结构及冲突图。


<details>
  <summary>Details</summary>
Motivation: 以太坊智能合约对数字经济意义重大，理解其交互有助于性能优化，明确并行化潜力。

Method: 使用call tracer和prestate tracer追踪超200万个以太坊区块。

Result: 得到每块交易分布、智能合约调用中调用树结构、价值转移交易与智能合约调用比例，发现冲突图多呈星形配置等。

Conclusion: 对以太坊智能合约调用结构和冲突图进行了全面研究，发现冲突图的结构特性。

Abstract: Ethereum, a leading blockchain platform, has revolutionized the digital
economy by enabling decentralized transactions and the execution of smart
contracts. Ethereum transactions form the backbone of its network, facilitating
peer-to-peer exchanges and interactions with complex decentralized
applications. Smart contracts extend Ethereum's capabilities by automating
processes and enabling trustless execution of agreements. Hence, understanding
how these smart contracts interact is important in order to facilitate various
performance optimizations, such as warming objects before they are being
accessed and enabling concurrent execution. Of particular interest to us are
the development of the calling graph, as well as the read sets and write sets
of invocations within the same block, and the properties of the associated
conflict graph that is derived from them. The latter is important for
understanding the parallelization potential of smart contracts on Ethereum. We
traced upwards of 2 million recent Ethereum blocks using call tracer and
prestate tracer, out of a total of 21.4 million blocks at the time of writing.
We report on the transactions per block distribution, the structure of call
trees in smart contract invocations, the ratio of value-transfer transactions
to smart contract invocations, as well as provide a comprehensive study of the
structure of blocks' conflict graphs. We find that conflict graphs
predominantly show a star like configuration, as well as other noteworthy
structural properties.

</details>


### [69] [Racing to Idle: Energy Efficiency of Matrix Multiplication on Heterogeneous CPU and GPU Architectures](https://arxiv.org/abs/2507.20063)
*Mufakir Qamar Ansari,Mudabir Qamar Ansari*

Main category: cs.DC

TL;DR: 本文在消费级笔记本三种架构上对矩阵乘法工作负载进行性能和能耗测量，发现离散GPU性能和能效最佳，为能源感知软件开发提供指导。


<details>
  <summary>Details</summary>
Motivation: 单核处理器有功率和散热限制，异构系统能平衡性能与功耗，但量化二者权衡是关键研究领域。

Method: 在消费级笔记本三种架构上对4096x4096矩阵乘法工作负载进行测量，使用Linux perf和nvidia - smi等工具。

Result: 离散GPU性能领先，比CPU快93.5倍，且能效最高，能耗仅为CPU的2%，能效提升50倍。

Conclusion: 研究结果验证了“race to idle”原则，为能源感知软件开发架构选择提供量化指导。

Abstract: The paradigm shift towards multi-core and heterogeneous computing, driven by
the fundamental power and thermal limits of single-core processors, has
established energy efficiency as a first-class design constraint in
high-performance computing (HPC). Heterogeneous systems, integrating
traditional multi-core CPUs with specialized accelerators like discrete (dGPU)
and integrated (iGPU) graphics processing units, offer a compelling path to
navigating the trade-offs between performance and power. However, quantifying
these trade-offs on widely accessible hardware remains a critical area of
study. This paper presents a direct, empirical measurement of the performance
and energy-to-solution of a canonical HPC workload -- a 4096x4096 matrix-matrix
multiplication -- on three distinct compute architectures within a single
consumer-grade laptop: a multi-core AMD Ryzen 7 5800H CPU, a discrete NVIDIA
GeForce GTX 1650 GPU, and an integrated AMD Radeon Vega GPU. Using standard,
validated, and minimally intrusive tools such as Linux perf and nvidia-smi, we
find that the discrete GPU is not only the performance leader, achieving a
93.5x speedup over the CPU, but is also the most energy-efficient, consuming
only 2% of the energy used by the CPU, resulting in a 50-fold improvement in
energy efficiency. These findings provide a practical demonstration of the
"race to idle" principle and offer clear, quantitative guidance on
architectural choices for energy-aware software development.

</details>


### [70] [High-Performance Parallel Optimization of the Fish School Behaviour on the Setonix Platform Using OpenMP](https://arxiv.org/abs/2507.20173)
*Haitian Wang,Long Qin*

Main category: cs.DC

TL;DR: 本文在Setonix超级计算平台上用OpenMP框架对鱼群行为（FSB）算法进行高性能并行优化研究，给出实验结果并指出后续探索方向。


<details>
  <summary>Details</summary>
Motivation: 各领域对复杂大规模计算的计算能力需求增加，需要优化的并行算法和计算结构，FSB算法适合并行化。

Method: 利用Setonix平台和OpenMP框架分析多线程的各方面，设计实验测试不同配置。

Result: 为FSB在Setonix上的并行优化提供见解，为其他OpenMP并行计算研究提供参考。

Conclusion: 缓存行为、微宏观线程调度策略等因素有进一步探索和优化的潜力。

Abstract: This paper presents an in-depth investigation into the high-performance
parallel optimization of the Fish School Behaviour (FSB) algorithm on the
Setonix supercomputing platform using the OpenMP framework. Given the
increasing demand for enhanced computational capabilities for complex,
large-scale calculations across diverse domains, there's an imperative need for
optimized parallel algorithms and computing structures. The FSB algorithm,
inspired by nature's social behavior patterns, provides an ideal platform for
parallelization due to its iterative and computationally intensive nature. This
study leverages the capabilities of the Setonix platform and the OpenMP
framework to analyze various aspects of multi-threading, such as thread counts,
scheduling strategies, and OpenMP constructs, aiming to discern patterns and
strategies that can elevate program performance. Experiments were designed to
rigorously test different configurations, and our results not only offer
insights for parallel optimization of FSB on Setonix but also provide valuable
references for other parallel computational research using OpenMP. Looking
forward, other factors, such as cache behavior and thread scheduling strategies
at micro and macro levels, hold potential for further exploration and
optimization.

</details>


### [71] [Silent Self-Stabilising Leader Election in Programmable Matter Systems with Holes](https://arxiv.org/abs/2507.20201)
*Jérémie Chalopin,Shantanu Das,Maria Kokkou*

Main category: cs.DC

TL;DR: 本文研究可编程物质系统中自稳定领导者选举问题，提出首个自稳定算法，利用粒子移动克服经典不可能结果。


<details>
  <summary>Details</summary>
Motivation: 领导者选举在分布式计算尤其是可编程物质系统中至关重要，自稳定解决方案有限，研究连通配置下的自稳定领导者选举问题。

Method: 提出首个自稳定算法，假设粒子有共同方向感，利用粒子移动能力。

Result: 算法能在不公平调度器下保证选出唯一领导者，粒子在网格中移动可克服经典不可能结果。

Conclusion: 利用粒子移动这一未被开发的能力，为可编程物质系统的自稳定领导者选举提供了有效方法。

Abstract: Leader election is a fundamental problem in distributed computing,
particularly within programmable matter systems, where coordination among
simple computational entities is crucial for solving complex tasks. In these
systems, particles (i.e., constant memory computational entities) operate in a
regular triangular grid as described in the geometric Amoebot model. While
leader election has been extensively studied in non self-stabilising settings,
self-stabilising solutions remain more limited. In this work, we study the
problem of self-stabilising leader election in connected (but not necessarily
simply connected) configurations. We present the first self-stabilising
algorithm for programmable matter that guarantees the election of a unique
leader under an unfair scheduler, assuming particles share a common sense of
direction. Our approach leverages particle movement, a capability not
previously exploited in the self-stabilising context. We show that movement in
conjunction with particles operating in a grid can overcome classical
impossibility results for constant-memory systems established by Dolev et al.

</details>


### [72] [RIMMS: Runtime Integrated Memory Management System for Heterogeneous Computing](https://arxiv.org/abs/2507.20514)
*Serhan Gener,Aditya Ukarande,Shilpa Mysore Srinivasa Murthy,Sahil Hassan,Joshua Mack,Chaitali Chakrabarti,Umit Ogras,Ali Akoglu*

Main category: cs.DC

TL;DR: 本文介绍RIMMS内存管理系统，在异构系统中能实现高效内存管理，经评估有性能提升且降低编程复杂度。


<details>
  <summary>Details</summary>
Motivation: 异构系统内存管理具挑战性，现有方法需显式管理或假设静态映射，限制可移植性和可扩展性。

Method: 引入RIMMS，将其集成到基线运行时，在CPU+GPU和CPU+FPGA平台用雷达信号处理应用评估。

Result: 相比基线，GPU系统达2.43倍加速，FPGA系统达1.82倍加速；比IRIS达3.08倍加速，性能与原生CUDA匹配，每次内存管理调用仅1 - 2周期开销。

Conclusion: RIMMS能在动态异构环境中提供高性能，提升程序员生产力。

Abstract: Efficient memory management in heterogeneous systems is increasingly
challenging due to diverse compute architectures (e.g., CPU, GPU, FPGA) and
dynamic task mappings not known at compile time. Existing approaches often
require programmers to manage data placement and transfers explicitly, or
assume static mappings that limit portability and scalability. This paper
introduces RIMMS (Runtime Integrated Memory Management System), a lightweight,
runtime-managed, hardware-agnostic memory abstraction layer that decouples
application development from low-level memory operations. RIMMS transparently
tracks data locations, manages consistency, and supports efficient memory
allocation across heterogeneous compute elements without requiring
platform-specific tuning or code modifications. We integrate RIMMS into a
baseline runtime and evaluate with complete radar signal processing
applications across CPU+GPU and CPU+FPGA platforms. RIMMS delivers up to 2.43X
speedup on GPU-based and 1.82X on FPGA-based systems over the baseline.
Compared to IRIS, a recent heterogeneous runtime system, RIMMS achieves up to
3.08X speedup and matches the performance of native CUDA implementations while
significantly reducing programming complexity. Despite operating at a higher
abstraction level, RIMMS incurs only 1-2 cycles of overhead per memory
management call, making it a low-cost solution. These results demonstrate
RIMMS's ability to deliver high performance and enhanced programmer
productivity in dynamic, real-world heterogeneous environments.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [73] [Fully Dynamic Spectral and Cut Sparsifiers for Directed Graphs](https://arxiv.org/abs/2507.19632)
*Yibin Zhao*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent years have seen extensive research on directed graph sparsification.
In this work, we initiate the study of fast fully dynamic spectral and cut
sparsification algorithms for directed graphs.
  We introduce a new notion of spectral sparsification called degree-balance
preserving spectral approximation, which maintains the difference between the
in-degree and out-degree of each vertex. The approximation error is measured
with respect to the corresponding undirected Laplacian. This notion is
equivalent to direct Eulerian spectral approximation when the input graph is
Eulerian. Our algorithm achieves an amortized update time of
$O(\varepsilon^{-2} \cdot \text{polylog}(n))$ and produces a sparsifier of size
$O(\varepsilon^{-2} n \cdot \text{polylog}(n))$. Additionally, we present an
algorithm that maintains a constant-factor approximation sparsifier of size
$O(n \cdot \text{polylog}(n))$ against an adaptive adversary for
$O(\text{polylog}(n))$-partially symmetrized graphs, a notion introduced in
[Kyng-Meierhans-Probst Gutenberg '22]. A $\beta$-partial symmetrization of a
directed graph $\vec{G}$ is the union of $\vec{G}$ and $\beta \cdot G$, where
$G$ is the corresponding undirected graph of $\vec{G}$. This algorithm also
achieves a polylogarithmic amortized update time.
  Moreover, we develop a fully dynamic algorithm for maintaining a cut
sparsifier for $\beta$-balanced directed graphs, where the ratio between
weighted incoming and outgoing edges of any cut is at most $\beta$. This
algorithm explicitly maintains a cut sparsifier of size
$O(\varepsilon^{-2}\beta n \cdot \text{polylog}(n))$ in worst-case update time
$O(\varepsilon^{-2}\beta \cdot \text{polylog}(n))$.

</details>


### [74] [Online Rounding Schemes for $ k $-Rental Problems](https://arxiv.org/abs/2507.19649)
*Hossein Nekouyan,Bo Sun,Raouf Boutaba,Xiaoqi Tan*

Main category: cs.DS

TL;DR: 研究对抗环境下两个可复用在线资源分配问题kRental - Fixed和kRental - Variable，开发有竞争比保证的算法。


<details>
  <summary>Details</summary>
Motivation: 解决对抗环境下可复用在线资源分配问题，为决策制定者提供有效算法。

Method: 针对两个问题开发基于理论的松弛 - 舍入算法，对kRental - Fixed用价格法计算最优分数分配并使用无损在线舍入方案；对kRental - Variable引入有限相关舍入技术并结合精心设计的价格法。

Result: 对kRental - Fixed给出达到最优竞争比的随机算法；证明kRental - Variable无损在线舍入不可能，得到可变时长设置下的最优阶竞争比。

Conclusion: 所开发算法能有效解决对抗环境下可复用在线资源分配问题，有良好的竞争比保证。

Abstract: We study two online resource-allocation problems with reusability in an
adversarial setting, namely kRental-Fixed and kRental-Variable. In both
problems, a decision-maker manages $k$ identical reusable units and faces a
sequence of rental requests over time. We develop theoretically grounded
relax-and-round algorithms with provable competitive-ratio guarantees for both
settings. For kRental-Fixed, we present an optimal randomized algorithm that
attains the best possible competitive ratio: it first computes an optimal
fractional allocation via a price-based approach, then applies a novel lossless
online rounding scheme to obtain an integral solution. For kRental-Variable, we
prove that lossless online rounding is impossible. We introduce a
limited-correlation rounding technique that treats each unit independently
while introducing controlled dependencies across allocation decisions involving
the same unit. Coupled with a carefully crafted price-based method for
computing the fractional allocation, this yields an order-optimal competitive
ratio for the variable-duration setting.

</details>


### [75] [Improved 2-Approximate Shortest Paths for close vertex pairs](https://arxiv.org/abs/2507.19859)
*Manoj Gupta*

Main category: cs.DS

TL;DR: 本文对Dor等人25年前的算法进行改进，新算法运行时间相近但能处理距离更近的顶点对，k=log n时运行时间为O(n²)。


<details>
  <summary>Details</summary>
Motivation: 改进Dor等人25年前的算法，使其能处理距离更近的顶点对。

Method: 提出一种组合、随机的算法。

Result: 新算法运行时间约为O(n^(2+1/k))，能处理距离为O(log k)的顶点对；k=log n时，运行时间为O(n²)，能处理距离至少为O(log log n)的顶点对。

Conclusion: 新算法在运行时间相近的情况下，能处理距离更近的顶点对，且以高概率返回正确结果。

Abstract: An influential result by Dor, Halperin, and Zwick (FOCS 1996, SICOMP 2000)
implies an algorithm that can compute approximate shortest paths for all vertex
pairs in $\tilde{O}(n^{2+O\left(\frac{1}{k}\right )})$ time, ensuring that the
output distance is at most twice the actual shortest path, provided the pairs
are at least $k$ apart, where $k \ge 2$. We present the first improvement on
this result in over 25 years. Our algorithm achieves roughly same
$\tilde{O}(n^{2+\frac{1}{k}})$ runtime but applies to vertex pairs merely
$O(\log k)$ apart, where $\log k \ge 1$. When $k=\log n$, the running time of
our algorithm is $\tilde{O}(n^2)$ and it works for all pairs at least $O(\log
\log n)$ apart. Our algorithm is combinatorial, randomized, and returns correct
results for all pairs with a high probability.

</details>


### [76] [Generating Satisfiable Benchmark Instances for Stable Roommates Problems with Optimization](https://arxiv.org/abs/2507.20013)
*Baturay Yılmaz,Esra Erdem*

Main category: cs.DS

TL;DR: 现有SRI基准实例存在不足，本文引入新算法生成有大量解且难用枚举法找平等稳定匹配的SRI基准实例。


<details>
  <summary>Details</summary>
Motivation: 现有用于SRI难题实验评估的基准实例不总是可满足，且稳定匹配数量少，难以有效评估解决平等SRI等难题的方法。

Method: 引入一种新算法来生成SRI的基准实例。

Result: 生成了具有大量解，且难以通过枚举所有稳定匹配来找到平等稳定匹配的SRI基准实例。

Conclusion: 新算法生成的基准实例有助于更有效地对解决SRI难题的方法进行实验评估。

Abstract: While the existence of a stable matching for the stable roommates problem
possibly with incomplete preference lists (SRI) can be decided in polynomial
time, SRI problems with some fairness criteria are intractable. Egalitarian SRI
that tries to maximize the total satisfaction of agents if a stable matching
exists, is such a hard variant of SRI. For experimental evaluations of methods
to solve these hard variants of SRI, several well-known algorithms have been
used to randomly generate benchmark instances. However, these benchmark
instances are not always satisfiable, and usually have a small number of stable
matchings if one exists. For such SRI instances, despite the NP-hardness of
Egalitarian SRI, it is practical to find an egalitarian stable matching by
enumerating all stable matchings. In this study, we introduce a novel algorithm
to generate benchmark instances for SRI that have very large numbers of
solutions, and for which it is hard to find an egalitarian stable matching by
enumerating all stable matchings.

</details>


### [77] [Parallel Hierarchical Agglomerative Clustering in Low Dimensions](https://arxiv.org/abs/2507.20047)
*MohammadHossein Bateni,Laxman Dhulipala,Willem Fletcher,Kishen N Gowda,D Ellis Hershkowitz,Rajesh Jayaram,Jakub Łącki*

Main category: cs.DS

TL;DR: 本文针对非单调链接函数的层次聚合聚类（HAC）问题，给出低维下(1 + ε)-近似HAC的高效NC算法，同时证明任意维度下此类NC算法可能不存在。


<details>
  <summary>Details</summary>
Motivation: 现有针对某些单调链接函数的(1 + ε)-近似HAC有高效并行算法，但重要的非单调链接函数（如质心和Ward链接）尚无此类算法。

Method: 基于一个结构结果，即对于一类非单调链接函数，在特定条件下常数近似HAC得到的层次结构高度至多为poly(log n)。

Result: 得到低维下非单调链接函数的(1 + ε)-近似HAC的高效NC算法，且证明任意维度下HAC的NC算法可能不存在。

Conclusion: 对于非单调链接函数的HAC，在低维有高效近似算法，但任意维度下难以有高效并行算法。

Abstract: Hierarchical Agglomerative Clustering (HAC) is an extensively studied and
widely used method for hierarchical clustering in $\mathbb{R}^k$ based on
repeatedly merging the closest pair of clusters according to an input linkage
function $d$. Highly parallel (i.e., NC) algorithms are known for
$(1+\epsilon)$-approximate HAC (where near-minimum rather than minimum pairs
are merged) for certain linkage functions that monotonically increase as merges
are performed. However, no such algorithms are known for many important but
non-monotone linkage functions such as centroid and Ward's linkage.
  In this work, we show that a general class of non-monotone linkage functions
-- which include centroid and Ward's distance -- admit efficient NC algorithms
for $(1+\epsilon)$-approximate HAC in low dimensions. Our algorithms are based
on a structural result which may be of independent interest: the height of the
hierarchy resulting from any constant-approximate HAC on $n$ points for this
class of linkage functions is at most $\operatorname{poly}(\log n)$ as long as
$k = O(\log \log n / \log \log \log n)$. Complementing our upper bounds, we
show that NC algorithms for HAC with these linkage functions in
\emph{arbitrary} dimensions are unlikely to exist by showing that HAC is
CC-hard when $d$ is centroid distance and $k = n$.

</details>


### [78] [Adaptive BSTs for Single-Source and All-to-All Requests: Algorithms and Lower Bounds](https://arxiv.org/abs/2507.20228)
*Maryam Shiran*

Main category: cs.DS

TL;DR: 本文提出固定重构成本的自适应二叉搜索树统一框架，包括离线和在线算法，并给出成本上下界和竞争比下界。


<details>
  <summary>Details</summary>
Motivation: 自适应二叉搜索树对构建响应式和高效网络及分布式系统有价值，需要统一框架和算法。

Method: 提出离线算法用于单源和全对全模型，开发在线数学框架和确定性在线策略。

Result: 证明算法成本上界，证明存在输入序列使其他离线算法成本与本文相当，建立确定性在线算法竞争比下界。

Conclusion: 给出固定重构成本的自适应二叉搜索树框架和算法，揭示在线自适应的基本限制。

Abstract: Adaptive binary search trees are a fundamental data structure for organizing
hierarchical information. Their ability to dynamically adjust to access
patterns makes them particularly valuable for building responsive and efficient
networked and distributed systems.
  We present a unified framework for adaptive binary search trees with fixed
restructuring cost, analyzed under two models: the single-source model, where
the cost of querying a node is proportional to its distance from a fixed
source, and the all-to-all model, where the cost of serving a request depends
on the distance between the source and destination nodes. We propose an offline
algorithm for the single-source model and extend it to the all-to-all model.
For both models, we prove upper bounds on the cost incurred by our algorithms.
Furthermore, we show the existence of input sequences for which any offline
algorithm must incur a cost comparable to ours.
  In the online setting, we develop a general mathematical framework for
deterministic online adaptive binary search trees and propose a deterministic
online strategy for the single-source case, which naturally extends to the
all-to-all model. We also establish lower bounds on the competitive ratio of
any deterministic online algorithm, highlighting fundamental limitations of
online adaptivity.

</details>


### [79] [The Min Max Average Cycle Weight Problem](https://arxiv.org/abs/2507.20253)
*Noga Klein Elmalem,Rica Gonen,Erel Segal-Halevi*

Main category: cs.DS

TL;DR: 研究旧公寓楼拆建后新公寓公平分配以减少居民嫉妒问题，归结为特定组合优化问题，分析其可解性。


<details>
  <summary>Details</summary>
Motivation: 解决旧公寓楼拆建后新公寓公平分配，减少居民嫉妒。

Method: 将问题归结为Min Max Average Cycle Weight问题，研究其与最大权重匹配问题的关系。

Result: 从无初始条件时问题可归结为最大权重匹配实现多项式时间可解，但考虑居民对原公寓满意度等预存条件时并非如此。

Conclusion: 一般情况下该问题是否多项式时间可解仍是待解决的有趣问题。

Abstract: When an old apartment building is demolished and rebuilt, how can we fairly
redistribute the new apartments to minimize envy among residents? We reduce
this question to a combinatorial optimization problem called the *Min Max
Average Cycle Weight* problem. In that problem we seek to assign objects to
agents in a way that minimizes the maximum average weight of directed cycles in
an associated envy graph. While this problem reduces to maximum-weight matching
when starting from a clean slate (achieving polynomial-time solvability), we
show that this is not the case when we account for preexisting conditions, such
as residents' satisfaction with their original apartments. Whether the problem
is polynomial-time solvable in the general case remains an intriguing open
problem.

</details>


### [80] [Faster exact learning of k-term DNFs with membership and equivalence queries](https://arxiv.org/abs/2507.20336)
*Josh Alman,Shivam Nadimpalli,Shyamal Patel,Rocco Servedio*

Main category: cs.DS

TL;DR: 本文提出一种使用成员和等价查询学习k - 项DNF公式的算法，时间复杂度为poly(n)⋅2^(O(√k))，是自Blum和Rudich之后该问题的首个改进。


<details>
  <summary>Details</summary>
Motivation: 改进使用成员和等价查询学习k - 项DNF公式的算法时间复杂度。

Method: 采用Winnow2算法在增强特征空间学习线性阈值函数，结合[BR92]中减少DNF项长度的技术及其他算法和分析工具。

Result: 得到时间复杂度为poly(n)⋅2^(O(√k))的学习k - 项DNF公式的算法。

Conclusion: 该算法是自Blum和Rudich原始工作后此问题的首次改进。

Abstract: In 1992 Blum and Rudich [BR92] gave an algorithm that uses membership and
equivalence queries to learn $k$-term DNF formulas over $\{0,1\}^n$ in time
$\textsf{poly}(n,2^k)$, improving on the naive $O(n^k)$ running time that can
be achieved without membership queries [Val84]. Since then, many alternative
algorithms [Bsh95, Kus97, Bsh97, BBB+00] have been given which also achieve
runtime $\textsf{poly}(n,2^k)$.
  We give an algorithm that uses membership and equivalence queries to learn
$k$-term DNF formulas in time $\textsf{poly}(n) \cdot 2^{\tilde{O}(\sqrt{k})}$.
This is the first improvement for this problem since the original work of Blum
and Rudich [BR92].
  Our approach employs the Winnow2 algorithm for learning linear threshold
functions over an enhanced feature space which is adaptively constructed using
membership queries. It combines a strengthened version of a technique that
effectively reduces the length of DNF terms from the original work of [BR92]
with a range of additional algorithmic tools (attribute-efficient learning
algorithms for low-weight linear threshold functions and techniques for finding
relevant variables from junta testing) and analytic ingredients (extremal
polynomials and noise operators) that are novel in the context of query-based
DNF learning.

</details>


### [81] [Deterministic Almost-Linear-Time Gomory-Hu Trees](https://arxiv.org/abs/2507.20354)
*Amir Abboud,Rasmus Kyng,Jason Li,Debmalya Panigrahi,Maximilian Probst Gutenberg,Thatchaphol Saranurak,Weixuan Yuan,Wuwei Yuan*

Main category: cs.DS

TL;DR: 本文给出构造Gomory - Hu树的首个近最优$m^{1+o(1)}$时间确定性算法。


<details>
  <summary>Details</summary>
Motivation: 改进构造Gomory - Hu树的确定性算法的时间复杂度，此前最佳算法为Gomory和Hu的$nm^{1+o(1)}$时间算法。

Method: 引入两个独立且新颖的组件，包括将全对最小割问题确定性归约到单源最小割问题，以及单源最小割问题的确定性近线性时间算法。

Result: 得到首个近最优$m^{1+o(1)}$时间确定性算法来构造Gomory - Hu树。

Conclusion: 该算法在构造Gomory - Hu树以及解决更简单问题（如找图的$k$边连通分量）上有显著时间复杂度改进。

Abstract: Given an $m$-edge, undirected, weighted graph $G=(V,E,w)$, a Gomory-Hu tree
$T$ (Gomory and Hu, 1961) is a tree over the vertex set $V$ such that all-pairs
mincuts in $G$ are preserved exactly in $T$.
  In this article, we give the first almost-optimal $m^{1+o(1)}$-time
deterministic algorithm for constructing a Gomory-Hu tree. Prior to our work,
the best deterministic algorithm for this problem dated back to the original
algorithm of Gomory and Hu that runs in $nm^{1+o(1)}$ time (using current
maxflow algorithms). In fact, this is the first almost-linear time
deterministic algorithm for even simpler problems, such as finding the
$k$-edge-connected components of a graph.
  Our new result hinges on two separate and novel components that each
introduce a distinct set of de-randomization tools of independent interest:
  - a deterministic reduction from the all-pairs mincuts problem to the
single-souce mincuts problem incurring only subpolynomial overhead, and
  - a deterministic almost-linear time algorithm for the single-source mincuts
problem.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [82] [Flexible Bidding in Service-Oriented Combinatorial Spectrum Forward Auctions](https://arxiv.org/abs/2507.19720)
*Xiang Shao,Wei Wang,Guan Gui*

Main category: cs.GT

TL;DR: 提出新颖近似真实的组合正向拍卖方案解决传统方案局限，模拟显示其表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统组合频谱拍卖依赖固定投标和匹配流程，限制参与者策略调整能力，在动态频谱共享环境中社会福利欠佳。

Method: 提出含灵活投标机制的拍卖方案，买家提交含基础频谱需求和可调需求范围的组合投标；引入频谱等效映射系数标准化不同频段估值；用贪婪匹配算法确定中标。

Result: 模拟结果表明，提出的灵活投标机制显著优于现有基准方法，在动态频谱共享场景中实现更高社会福利。

Conclusion: 所提拍卖方案能有效提高资源效率，最大化社会福利，在动态频谱共享中有更好表现。

Abstract: Traditional combinatorial spectrum auctions mainly rely on fixed bidding and
matching processes, which limit participants' ability to adapt their strategies
and often result in suboptimal social welfare in dynamic spectrum sharing
environments. To address these limitations, we propose a novel approximately
truthful combinatorial forward auction scheme with a flexible bidding mechanism
aimed at enhancing resource efficiency and maximizing social welfare. In the
proposed scheme, each buyer submits a combinatorial bid consisting of the base
spectrum demand and adjustable demand ranges, enabling the auctioneer to
dynamically optimize spectrum allocation in response to market conditions. To
standardize the valuation across heterogeneous frequency bands, we introduce a
Spectrum Equivalent Mapping (SEM) coefficient. A greedy matching algorithm is
employed to determine winning bids by sorting buyers based on their equivalent
unit bid prices and allocating resources within supply constraints. Simulation
results demonstrate that the proposed flexible bidding mechanism significantly
outperforms existing benchmark methods, achieving notably higher social welfare
in dynamic spectrum sharing scenarios.

</details>


### [83] [An Algorithm-to-Contract Framework without Demand Queries](https://arxiv.org/abs/2507.20038)
*Ilan Doron-Arad,Hadas Shachnai,Gilad Shmerler,Inbal Talgam-Cohen*

Main category: cs.GT

TL;DR: 论文旨在将组合问题算法转化以解决合同设计中的激励约束，提出‘局部 - 全局’框架，应用于多种组合约束并处理多智能体合同设置。


<details>
  <summary>Details</summary>
Motivation: 在委托 - 代理场景下，研究预算最大化问题的合同设计是否仍有近似方案，实现算法到合同的转化。

Method: 提出‘局部 - 全局’框架，‘局部’近似解决双边强化版需求问题，‘全局’利用局部结果找到近似最优合同。

Result: 将预算最大化的 FPTAS 提升，获得合同设计问题的最佳乘性和加性 FPTAS，应用于多种组合约束时近似效果与纯算法问题的最佳近似匹配，还开发了处理多智能体合同设置的方法。

Conclusion: 所提出的框架和方法能有效解决组合合同设计中的激励约束问题。

Abstract: Consider costly tasks that add up to the success of a project, and must be
fitted by an agent into a given time-frame. This is an instance of the classic
budgeted maximization problem, which admits an approximation scheme (FPTAS).
Now assume the agent is performing these tasks on behalf of a principal, who is
the one to reap the rewards if the project succeeds. The principal must design
a contract to incentivize the agent. Is there still an approximation scheme? In
this work, our ultimate goal is an algorithm-to-contract transformation, which
transforms algorithms for combinatorial problems (like budgeted maximization)
to tackle incentive constraints that arise in contract design. Our approach
diverges from previous works on combinatorial contract design by avoiding an
assumption of black-box access to a demand oracle.
  We first show how to "lift" the FPTAS for budgeted maximization to obtain the
best-possible multiplicative and additive FPTAS for the contract design
problem. We establish this through our "local-global" framework, in which the
"local" step is to (approximately) solve a two-sided strengthened variant of
the demand problem. The "global" step then utilizes the local one to find the
approximately optimal contract. We apply our framework to a host of
combinatorial constraints including multi-dimensional budgets, budgeted
matroid, and budgeted matching constraints. In all cases we achieve an
approximation essentially matching the best approximation for the purely
algorithmic problem. We also develop a method to tackle multi-agent contract
settings, where the team of working agents must abide to combinatorial
feasibility constraints.

</details>


### [84] [Fairness under Equal-Sized Bundles: Impossibility Results and Approximation Guarantees](https://arxiv.org/abs/2507.20899)
*Alviona Mancho,Evangelos Markakis,Nicos Protopapas*

Main category: cs.GT

TL;DR: 研究基数约束下不可分物品公平分配，引入基于物品翻转的EF变体，给出算法及不可能结果。


<details>
  <summary>Details</summary>
Motivation: 解决基数约束下不可分物品公平分配问题，模拟实际场景，如轮班分配、组队。

Method: 先尝试文献标准技术，用嫉妒循环消除技术实现常数因子近似保证，基于最大化纳什福利设计算法。

Result: 标准技术无法保证EFFX近似；在特定条件下可实现常数因子近似保证；最大化纳什福利算法保证1/2 - EFF1分配且边界是紧的。

Conclusion: 经典EFX概念和基于翻转的类似概念有明显差异。

Abstract: We study the fair allocation of indivisible goods under cardinality
constraints, where each agent must receive a bundle of fixed size. This models
practical scenarios, such as assigning shifts or forming equally sized teams.
Recently, variants of envy-freeness up to one/any item (EF1, EFX) were
introduced for this setting, based on flips or exchanges of items. Namely, one
can define envy-freeness up to one/any flip (EFF1, EFFX), meaning that an agent
$i$ does not envy another agent $j$ after performing one or any one-item flip
between their bundles that improves the value of $i$.
  We explore algorithmic aspects of this notion, and our contribution is
twofold: we present both algorithmic and impossibility results, highlighting a
stark contrast between the classic EFX concept and its flip-based analogue.
First, we explore standard techniques used in the literature and show that they
fail to guarantee EFFX approximations. On the positive side, we show that we
can achieve a constant factor approximation guarantee when agents share a
common ranking over item values, based on the well-known envy cycle elimination
technique. This idea also leads to a generalized algorithm with approximation
guarantees when agents agree on the top $n$ items and their valuation functions
are bounded. Finally, we show that an algorithm that maximizes the Nash welfare
guarantees a 1/2-EFF1 allocation, and that this bound is tight.

</details>


### [85] [Behavioral Study of Dashboard Mechanisms](https://arxiv.org/abs/2507.20985)
*Paula Kayongo,Jessica Hullman,Jason Hartline*

Main category: cs.GT

TL;DR: 本文通过行为实验评估不同仪表盘设计对逆向首价拍卖中投标优化和推断投标人偏好的影响，发现基于效用的可视化可改善投标，但投标人仍会出价不足，需使可视化设计与计量推断假设一致。


<details>
  <summary>Details</summary>
Motivation: 评估不同仪表盘设计对逆向首价拍卖中投标优化的影响，以及对拍卖设计者推断投标人偏好能力的影响。

Method: 进行行为实验，比较投标分配规则可视化与显示预期效用的替代方案。

Result: 基于效用的可视化显著改善投标，但投标人系统地出价不足，假设投标人完全理性或风险中性的仪表盘机制会产生估计误差。

Conclusion: 明确建模代理人对仪表盘的行为反应可提高推断准确性，实践中需使可视化设计与计量推断假设一致。

Abstract: Visualization dashboards are increasingly used in strategic settings like
auctions to enhance decision-making and reduce strategic confusion. This paper
presents behavioral experiments evaluating how different dashboard designs
affect bid optimization in reverse first-price auctions. Additionally, we
assess how dashboard designs impact the auction designer's ability to
accurately infer bidders' preferences within the dashboard mechanism framework.
We compare visualizations of the bid allocation rule, commonly deployed in
practice, to alternatives that display expected utility. We find that
utility-based visualizations significantly improve bidding by reducing
cognitive demands on bidders. However, even with improved dashboards, bidders
systematically under-shade their bids, driven by an implicit preference for
certain wins in uncertain settings. As a result, dashboard-based mechanisms
that assume fully rational or risk-neutral bidder responses to dashboards can
produce significant estimation errors when inferring private preferences, which
may lead to suboptimal allocations in practice. Explicitly modeling agents'
behavioral responses to dashboards substantially improves inference accuracy,
highlighting the need to align visualization design and econometric inference
assumptions in practice.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [86] [A Unified Framework for Interactive Visual Graph Matching via Attribute-Structure Synchronization](https://arxiv.org/abs/2507.19750)
*Yuhua Liu,Haoxuan Wang,Jiajia Kou,Ling Sun,Heyu Wang,Yongheng Wang,Yigang Wang,Jinchang Lic,Zhiguang Zhou*

Main category: cs.IR

TL;DR: 提出交互式视觉图匹配新框架，结合属性与结构信息，有直观界面，经实验验证有优越性


<details>
  <summary>Details</summary>
Motivation: 传统图检索工具仅用结构信息，实际图节点属性含有用信息，需结合属性与结构信息以实现更好图匹配

Method: 基于典型相关分析（CCA）开发属性 - 结构同步方法，将结构和属性特征统一到嵌入空间，提供直观视觉查询界面，设计评估视图

Result: 在真实数据集上的案例研究和定量比较表明，该框架在图匹配和大图探索方面具有优越性

Conclusion: 所提出的交互式视觉图匹配框架能有效结合属性与结构信息，实现更好的图匹配和大图探索

Abstract: In traditional graph retrieval tools, graph matching is commonly used to
retrieve desired graphs from extensive graph datasets according to their
structural similarities. However, in real applications, graph nodes have
numerous attributes which also contain valuable information for evaluating
similarities between graphs. Thus, to achieve superior graph matching results,
it is crucial for graph retrieval tools to make full use of the attribute
information in addition to structural information. We propose a novel framework
for interactive visual graph matching. In the proposed framework, an
attribute-structure synchronization method is developed for representing
structural and attribute features in a unified embedding space based on
Canonical Correlation Analysis (CCA). To support fast and interactive matching,
\revise{our method} provides users with intuitive visual query interfaces for
traversing, filtering and searching for the target graph in the embedding space
conveniently. With the designed interfaces, the users can also specify a new
target graph with desired structural and semantic features. Besides, evaluation
views are designed for easy validation and interpretation of the matching
results. Case studies and quantitative comparisons on real-world datasets have
demonstrated the superiorities of our proposed framework in graph matching and
large graph exploration.

</details>


### [87] [Analyzing and Mitigating Repetitions in Trip Recommendation](https://arxiv.org/abs/2507.19798)
*Wenzheng Shu,Kangqi Xu,Wenxin Tai,Ting Zhong,Yong Wang,Fan Zhou*

Main category: cs.IR

TL;DR: 现有旅行推荐研究有重复结果问题，本文提出AR - Trip模型解决重复问题并提升精度。


<details>
  <summary>Details</summary>
Motivation: 当前旅行推荐研究存在重复结果问题，需要解决。

Method: 运用统计分析和实验设计发现问题，引入含循环感知预测器的AR - Trip模型，该预测器含三种机制。

Result: 在四个公开数据集上实验表明，AR - Trip能减轻重复问题并提升精度。

Conclusion: AR - Trip可有效解决旅行推荐中的重复问题，且能提高精度。

Abstract: Trip recommendation has emerged as a highly sought-after service over the
past decade. Although current studies significantly understand human intention
consistency, they struggle with undesired repetitive outcomes that need
resolution. We make two pivotal discoveries using statistical analyses and
experimental designs: (1) The occurrence of repetitions is intricately linked
to the models and decoding strategies. (2) During training and decoding, adding
perturbations to logits can reduce repetition. Motivated by these observations,
we introduce AR-Trip (Anti Repetition for Trip Recommendation), which
incorporates a cycle-aware predictor comprising three mechanisms to avoid
duplicate Points-of-Interest (POIs) and demonstrates their effectiveness in
alleviating repetition. Experiments on four public datasets illustrate that
AR-Trip successfully mitigates repetition issues while enhancing precision.

</details>


### [88] [Improving the Performance of Sequential Recommendation Systems with an Extended Large Language Model](https://arxiv.org/abs/2507.19990)
*Sinnyum Choi,Woong Kim*

Main category: cs.IR

TL;DR: 提出用Llama3替换LlamaRec框架中Llama2改进推荐系统，实验显示多数据集有性能提升，证明方法可行。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能领域竞争激烈，新大语言模型不断推出，但很多研究未考虑这些进展，期望改进基于LLM的推荐系统。

Method: 在LlamaRec框架中用Llama3替换Llama2，并在预处理和训练时设置随机种子值、提供相同输入数据以保证公平比较。

Result: ML - 100K、Beauty和Games数据集平均性能分别提升38.65%、8.69%和8.19%。

Conclusion: 所提方法是提升当前推荐系统性能的可行方案，无需对系统进行结构更改就能有效提升推荐质量。

Abstract: Recently, competition in the field of artificial intelligence (AI) has
intensified among major technological companies, resulting in the continuous
release of new large-language models (LLMs) that exhibit improved language
understanding and context-based reasoning capabilities. It is expected that
these advances will enable more efficient personalized recommendations in
LLM-based recommendation systems through improved quality of training data and
architectural design. However, many studies have not considered these recent
developments. In this study, it was proposed to improve LLM-based
recommendation systems by replacing Llama2 with Llama3 in the LlamaRec
framework. To ensure a fair comparison, random seed values were set and
identical input data was provided during preprocessing and training. The
experimental results show average performance improvements of 38.65\%, 8.69\%,
and 8.19\% for the ML-100K, Beauty, and Games datasets, respectively, thus
confirming the practicality of this method. Notably, the significant
improvements achieved by model replacement indicate that the recommendation
quality can be improved cost-effectively without the need to make structural
changes to the system. Based on these results, it is our contention that the
proposed approach is a viable solution for improving the performance of current
recommendation systems.

</details>


### [89] [A Non-Parametric Choice Model That Learns How Users Choose Between Recommended Options](https://arxiv.org/abs/2507.20035)
*Thorsten Krause,Harrie Oosterhuis*

Main category: cs.IR

TL;DR: 提出用于推荐的学习选择模型LCM4Rec，能准确恢复数据集下的选择模型，提供更稳健的用户偏好推断，更抗曝光偏差。


<details>
  <summary>Details</summary>
Motivation: 现有选择模型假设不确定能否准确捕捉用户行为、错误假设对推断的影响以及是否有更好的模型。

Method: 提出非参数方法LCM4Rec，应用核密度估计推断最可能的误差分布以刻画用户选择模型。

Result: LCM4Rec能准确恢复选择模型，提供稳健的用户偏好推断，更抗曝光偏差。

Conclusion: 学习选择模型而非假设它们能产生更稳健的预测，为更好理解用户选择行为迈出重要一步。

Abstract: Choice models predict which items users choose from presented options. In
recommendation settings, they can infer user preferences while countering
exposure bias. In contrast with traditional univariate recommendation models,
choice models consider which competitors appeared with the chosen item. This
ability allows them to distinguish whether a user chose an item due to
preference, i.e., they liked it; or competition, i.e., it was the best
available option. Each choice model assumes specific user behavior, e.g., the
multinomial logit model. However, it is currently unclear how accurately these
assumptions capture actual user behavior, how wrong assumptions impact
inference, and whether better models exist.
  In this work, we propose the learned choice model for recommendation
(LCM4Rec), a non-parametric method for estimating the choice model. By applying
kernel density estimation, LCM4Rec infers the most likely error distribution
that describes the effect of inter-item cannibalization and thereby
characterizes the users' choice model. Thus, it simultaneously infers what
users prefer and how they make choices. Our experimental results indicate that
our method (i) can accurately recover the choice model underlying a dataset;
(ii) provides robust user preference inference, in contrast with existing
choice models that are only effective when their assumptions match user
behavior; and (iii) is more resistant against exposure bias than existing
choice models. Thereby, we show that learning choice models, instead of
assuming them, can produce more robust predictions. We believe this work
provides an important step towards better understanding users' choice behavior.

</details>


### [90] [Integrating LLM-Derived Multi-Semantic Intent into Graph Model for Session-based Recommendation](https://arxiv.org/abs/2507.20147)
*Shuo Zhang,Xiao Li,Jiayi Wu,Fan Yang,Xiang Li,Ming Gao*

Main category: cs.IR

TL;DR: 论文提出LLM - DMsRec方法解决基于GNN的SBR方法忽视语义信息问题，实验表明该方法可提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于GNN的SBR方法主要关注会话序列ID信息，忽略语义信息，限制了准确推断用户意图的能力。

Method: 利用预训练GNN模型选top - k候选物品集，用大语言模型从候选物品推断多语义意图，提出对齐机制融合语义和结构意图。

Result: 在Beauty和ML - 1M数据集上实验，该方法可融入GNN框架，显著提升推荐性能。

Conclusion: 提出的LLM - DMsRec方法能有效解决现有SBR方法的问题，提升推荐效果。

Abstract: Session-based recommendation (SBR) is mainly based on anonymous user
interaction sequences to recommend the items that the next user is most likely
to click. Currently, the most popular and high-performing SBR methods primarily
leverage graph neural networks (GNNs), which model session sequences as
graph-structured data to effectively capture user intent. However, most
GNNs-based SBR methods primarily focus on modeling the ID sequence information
of session sequences, while neglecting the rich semantic information embedded
within them. This limitation significantly hampers model's ability to
accurately infer users' true intention. To address above challenge, this paper
proposes a novel SBR approach called Integrating LLM-Derived Multi-Semantic
Intent into Graph Model for Session-based Recommendation (LLM-DMsRec). The
method utilizes a pre-trained GNN model to select the top-k items as candidate
item sets and designs prompts along with a large language model (LLM) to infer
multi-semantic intents from these candidate items. Specifically, we propose an
alignment mechanism that effectively integrates the semantic intent inferred by
the LLM with the structural intent captured by GNNs. Extensive experiments
conducted on the Beauty and ML-1M datasets demonstrate that the proposed method
can be seamlessly integrated into GNNs framework, significantly enhancing its
recommendation performance.

</details>


### [91] [Practical Multi-Task Learning for Rare Conversions in Ad Tech](https://arxiv.org/abs/2507.20161)
*Yuval Dishi,Ophir Friedler,Yonatan Karni,Natalia Silberstein,Yulia Stolin*

Main category: cs.IR

TL;DR: 提出多任务学习方法提升在线广告中罕见转化事件预测，部署后离线和在线指标均有改善。


<details>
  <summary>Details</summary>
Motivation: 提升在线广告中罕见（如发生率低于1%）转化事件的预测效果。

Method: 将转化事件按历史统计分为“罕见”和“频繁”类型，模型学习所有信号的共享表示，通过不同任务塔进行专门处理。

Result: 该方法在生产中部署，离线AUC提升0.69%，在线每行动成本降低2%。

Conclusion: 该多任务学习方法能有效提升在线广告中罕见转化事件的预测性能。

Abstract: We present a Multi-Task Learning (MTL) approach for improving predictions for
rare (e.g., <1%) conversion events in online advertising. The conversions are
classified into "rare" or "frequent" types based on historical statistics. The
model learns shared representations across all signals while specializing
through separate task towers for each type. The approach was tested and fully
deployed to production, demonstrating consistent improvements in both offline
(0.69% AUC lift) and online KPI performance metric (2% Cost per Action
reduction).

</details>


### [92] [CTR-Driven Ad Text Generation via Online Feedback Preference Optimization](https://arxiv.org/abs/2507.20227)
*Yanda Chen,Zihui Ren,Qixiang Gao,Jiale Chen,Si Chen,Xubin Li,Tiezheng Ge,Bo Zheng*

Main category: cs.IR

TL;DR: 提出优化点击率的广告文本生成方法，两阶段框架，实验证明有效且在电商平台应用有显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的广告文本不一定有更高点击率，存在生成质量与在线性能的差距。

Method: 采用两阶段框架，一是通过一次性上下文学习进行多样化广告文本采样，用检索增强生成提供示例和思维链推理；二是根据在线反馈进行点击率驱动的偏好优化。

Result: 模型能端到端生成高点击率广告文本，实验在离线和在线指标上都证明方法有效，在大型网购平台应用实现点击率显著提升。

Conclusion: 方法在广告系统中有很强的适用性和有效性。

Abstract: Advertising text plays a critical role in determining click-through rates
(CTR) in online advertising. Large Language Models (LLMs) offer significant
efficiency advantages over manual ad text creation. However, LLM-generated ad
texts do not guarantee higher CTR performance compared to human-crafted texts,
revealing a gap between generation quality and online performance of ad texts.
In this work, we propose a novel ad text generation method which optimizes for
CTR through preference optimization from online feedback. Our approach adopts
an innovative two-stage framework: (1) diverse ad text sampling via one-shot
in-context learning, using retrieval-augmented generation (RAG) to provide
exemplars with chain-of-thought (CoT) reasoning; (2) CTR-driven preference
optimization from online feedback, which weighs preference pairs according to
their CTR gains and confidence levels. Through our method, the resulting model
enables end-to-end generation of high-CTR ad texts. Extensive experiments have
demonstrated the effectiveness of our method in both offline and online
metrics. Notably, we have applied our method on a large-scale online shopping
platform and achieved significant CTR improvements, showcasing its strong
applicability and effectiveness in advertising systems.

</details>


### [93] [TADT-CSA: Temporal Advantage Decision Transformer with Contrastive State Abstraction for Generative Recommendation](https://arxiv.org/abs/2507.20327)
*Xiang Gao,Tianyuan Liu,Yisha Li,Jingxin Liu,Lexi Gao,Xin Li,Haiyang Lu,Liyin Hong*

Main category: cs.IR

TL;DR: 提出TADT - CSA模型解决Decision Transformer在推荐任务中的问题，实验证明其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: Decision Transformer在轨迹拼接有挑战、计算成本高且难学有效状态表示，需改进。

Method: 结合Return - To - Go信号和新的时间优势信号，集成对比状态抽象模块，引入TAC - SVQ策略，使用奖励预测网络和CTP网络。

Result: 在公共数据集和在线推荐系统实验表明TADT - CSA模型有效，优于基线方法。

Conclusion: TADT - CSA模型能解决DT存在的问题，提升推荐效果。

Abstract: With the rapid advancement of Transformer-based Large Language Models (LLMs),
generative recommendation has shown great potential in enhancing both the
accuracy and semantic understanding of modern recommender systems. Compared to
LLMs, the Decision Transformer (DT) is a lightweight generative model applied
to sequential recommendation tasks. However, DT faces challenges in trajectory
stitching, often producing suboptimal trajectories. Moreover, due to the high
dimensionality of user states and the vast state space inherent in
recommendation scenarios, DT can incur significant computational costs and
struggle to learn effective state representations. To overcome these issues, we
propose a novel Temporal Advantage Decision Transformer with Contrastive State
Abstraction (TADT-CSA) model. Specifically, we combine the conventional
Return-To-Go (RTG) signal with a novel temporal advantage (TA) signal that
encourages the model to capture both long-term returns and their sequential
trend. Furthermore, we integrate a contrastive state abstraction module into
the DT framework to learn more effective and expressive state representations.
Within this module, we introduce a TA-conditioned State Vector Quantization
(TAC-SVQ) strategy, where the TA score guides the state codebooks to
incorporate contextual token information. Additionally, a reward prediction
network and a contrastive transition prediction (CTP) network are employed to
ensure the state codebook preserves both the reward information of the current
state and the transition information between adjacent states. Empirical results
on both public datasets and an online recommendation system demonstrate the
effectiveness of the TADT-CSA model and its superiority over baseline methods.

</details>


### [94] [Improving Community Detection in Academic Networks by Handling Publication Bias](https://arxiv.org/abs/2507.20449)
*Md Asaduzzaman Noor,John Sheppard,Jason Clark*

Main category: cs.IR

TL;DR: 本文提出基于BERTopic和微调的SciBERT模型，利用出版物内容构建主题研究网络，通过克隆策略处理出版不平衡问题，评估显示该方法能带来更有意义的社区和更多合作机会。


<details>
  <summary>Details</summary>
Motivation: 在快速发展的跨学科研究环境中，传统方法难以有效找到潜在研究合作者，且存在出版不平衡问题影响研究范围的检测。

Method: 基于出版物内容，使用BERTopic和微调的SciBERT模型构建主题研究网络，引入克隆策略，对研究者的出版物进行聚类并将每个聚类视为独立节点。

Result: 克隆网络结构带来更有意义的社区，揭示了更广泛的合作机会。

Conclusion: 基于出版物内容构建研究网络并采用克隆策略能有效解决出版不平衡问题，为跨学科研究找到更多潜在合作者。

Abstract: Finding potential research collaborators is a challenging task, especially in
today's fast-growing and interdisciplinary research landscape. While
traditional methods often rely on observable relationships such as
co-authorships and citations to construct the research network, in this work,
we focus solely on publication content to build a topic-based research network
using BERTopic with a fine-tuned SciBERT model that connects and recommends
researchers across disciplines based on shared topical interests. A major
challenge we address is publication imbalance, where some researchers publish
much more than others, often across several topics. Without careful handling,
their less frequent interests are hidden under dominant topics, limiting the
network's ability to detect their full research scope. To tackle this, we
introduce a cloning strategy that clusters a researcher's publications and
treats each cluster as a separate node. This allows researchers to be part of
multiple communities, improving the detection of interdisciplinary links.
Evaluation on the proposed method shows that the cloned network structure leads
to more meaningful communities and uncovers a broader set of collaboration
opportunities.

</details>


### [95] [Beyond Interactions: Node-Level Graph Generation for Knowledge-Free Augmentation in Recommender Systems](https://arxiv.org/abs/2507.20578)
*Zhaoyan Wang,Hyunjun Ahn,In-Young Ko*

Main category: cs.IR

TL;DR: 提出无知识增强框架NodeDiffRec，无需外部知识提升推荐效果，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统依赖外部资源有局限，无知识模型缺乏增强原语。

Method: 提出NodeDiffRec框架，合成伪物品及交互注入，通过去噪偏好建模细化用户偏好。

Result: 在不同数据集和推荐算法上实验，实现SOTA性能，Recall@5最高平均提升98.6%，NDCG@5最高平均提升84.0%。

Conclusion: NodeDiffRec无需外部知识能显著增强语义多样性和结构连通性，提升推荐效果。

Abstract: Recent advances in recommender systems rely on external resources such as
knowledge graphs or large language models to enhance recommendations, which
limit applicability in real-world settings due to data dependency and
computational overhead. Although knowledge-free models are able to bolster
recommendations by direct edge operations as well, the absence of augmentation
primitives drives them to fall short in bridging semantic and structural gaps
as high-quality paradigm substitutes. Unlike existing diffusion-based works
that remodel user-item interactions, this work proposes NodeDiffRec, a
pioneering knowledge-free augmentation framework that enables fine-grained
node-level graph generation for recommendations and expands the scope of
restricted augmentation primitives via diffusion. By synthesizing pseudo-items
and corresponding interactions that align with the underlying distribution for
injection, and further refining user preferences through a denoising preference
modeling process, NodeDiffRec dramatically enhances both semantic diversity and
structural connectivity without external knowledge. Extensive experiments
across diverse datasets and recommendation algorithms demonstrate the
superiority of NodeDiffRec, achieving State-of-the-Art (SOTA) performance, with
maximum average performance improvement 98.6% in Recall@5 and 84.0% in NDCG@5
over selected baselines.

</details>


### [96] [Industry Insights from Comparing Deep Learning and GBDT Models for E-Commerce Learning-to-Rank](https://arxiv.org/abs/2507.20753)
*Yunus Lutz,Timo Wilm,Philipp Duwe*

Main category: cs.IR

TL;DR: 对DNN和生产级LambdaMART模型进行基准测试，简单DNN架构在点击量和收入上优于树模型。


<details>
  <summary>Details</summary>
Motivation: 探讨在电商推荐和搜索系统的学习排序任务中，DNN是否能超越传统树模型。

Method: 对多个DNN架构和损失函数在专有数据集上评估，并进行8周在线A/B测试。

Result: 简单DNN架构在总点击量和收入上超越强大的树模型基线，总销量持平。

Conclusion: 在电商学习排序任务中，简单DNN架构表现优于传统树模型。

Abstract: In e-commerce recommender and search systems, tree-based models, such as
LambdaMART, have set a strong baseline for Learning-to-Rank (LTR) tasks.
Despite their effectiveness and widespread adoption in industry, the debate
continues whether deep neural networks (DNNs) can outperform traditional
tree-based models in this domain. To contribute to this discussion, we
systematically benchmark DNNs against our production-grade LambdaMART model. We
evaluate multiple DNN architectures and loss functions on a proprietary dataset
from OTTO and validate our findings through an 8-week online A/B test. The
results show that a simple DNN architecture outperforms a strong tree-based
baseline in terms of total clicks and revenue, while achieving parity in total
units sold.

</details>


### [97] [Watermarking Large Language Model-based Time Series Forecasting](https://arxiv.org/abs/2507.20762)
*Wei Yuan,Chaoqun Yang,Yu Xing,Tong Chen,Nguyen Quoc Viet Hung,Hongzhi Yin*

Main category: cs.IR

TL;DR: 本文探讨大语言模型时间序列预测（LLMTS）输出水印技术，提出Waltz框架，实验证明其水印检测准确率高且对生成时间序列质量影响小。


<details>
  <summary>Details</summary>
Motivation: LLMTS有商业潜力和资源消耗大问题，且预测能力可能被滥用，需解决知识产权保护和防止生成虚假数据问题。

Method: 提出Waltz后处理水印框架，利用时间序列补丁嵌入与冷标记的关系嵌入水印，用相似度z分数检测，引入策略和投影梯度下降减少副作用。

Result: 使用两个流行LLMTS模型在七个基准数据集上实验，Waltz水印检测准确率高，对生成时间序列质量影响小。

Conclusion: Waltz框架能有效解决LLMTS的知识产权保护和数据造假问题。

Abstract: Large Language Model-based Time Series Forecasting (LLMTS) has shown
remarkable promise in handling complex and diverse temporal data, representing
a significant step toward foundation models for time series analysis. However,
this emerging paradigm introduces two critical challenges. First, the
substantial commercial potential and resource-intensive development raise
urgent concerns about intellectual property (IP) protection. Second, their
powerful time series forecasting capabilities may be misused to produce
misleading or fabricated deepfake time series data. To address these concerns,
we explore watermarking the outputs of LLMTS models, that is, embedding
imperceptible signals into the generated time series data that remain
detectable by specialized algorithms. We propose a novel post-hoc watermarking
framework, Waltz, which is broadly compatible with existing LLMTS models. Waltz
is inspired by the empirical observation that time series patch embeddings are
rarely aligned with a specific set of LLM tokens, which we term ``cold
tokens''. Leveraging this insight, Waltz embeds watermarks by rewiring the
similarity statistics between patch embeddings and cold token embeddings, and
detects watermarks using similarity z-scores. To minimize potential side
effects, we introduce a similarity-based embedding position identification
strategy and employ projected gradient descent to constrain the watermark noise
within a defined boundary. Extensive experiments using two popular LLMTS models
across seven benchmark datasets demonstrate that Waltz achieves high watermark
detection accuracy with minimal impact on the quality of the generated time
series.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [98] [Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of Underrepresented Shift Workers](https://arxiv.org/abs/2507.19510)
*Haoxuan Ma,Xishun Liao,Yifan Liu,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: 论文聚焦轮班工人在城市出行建模中的研究缺失，引入基于Transformer的方法利用GPS数据生成活动模式，评估显示生成数据与洛杉矶县GPS数据分布高度一致，为交通规划提供数据增强工具。


<details>
  <summary>Details</summary>
Motivation: 轮班工人占工业化社会劳动力15 - 20%，但在传统交通调查和规划中被系统地忽视，需填补城市出行建模在这方面的空白。

Method: 引入基于Transformer的方法，采用周期感知时间嵌入和以过渡为重点的损失函数，利用碎片化GPS轨迹数据生成非标准工作时间人群的完整活动模式。

Result: 生成的数据与洛杉矶县GPS数据实现了显著的分布对齐（所有评估指标的平均JSD < 0.02）。

Conclusion: 该方法将不完整的GPS轨迹转换为完整、有代表性的活动模式，为交通规划者提供强大的数据增强工具，有助于进行精确和包容的交通规划。

Abstract: This paper addresses a critical gap in urban mobility modeling by focusing on
shift workers, a population segment comprising 15-20% of the workforce in
industrialized societies yet systematically underrepresented in traditional
transportation surveys and planning. This underrepresentation is revealed in
this study by a comparative analysis of GPS and survey data, highlighting stark
differences between the bimodal temporal patterns of shift workers and the
conventional 9-to-5 schedules recorded in surveys. To address this bias, we
introduce a novel transformer-based approach that leverages fragmented GPS
trajectory data to generate complete, behaviorally valid activity patterns for
individuals working non-standard hours. Our method employs periodaware temporal
embeddings and a transition-focused loss function specifically designed to
capture the unique activity rhythms of shift workers and mitigate the inherent
biases in conventional transportation datasets. Evaluation shows that the
generated data achieves remarkable distributional alignment with GPS data from
Los Angeles County (Average JSD < 0.02 for all evaluation metrics). By
transforming incomplete GPS traces into complete, representative activity
patterns, our approach provides transportation planners with a powerful data
augmentation tool to fill critical gaps in understanding the 24/7 mobility
needs of urban populations, enabling precise and inclusive transportation
planning.

</details>


### [99] [Enhancing Spatiotemporal Networks with xLSTM: A Scalar LSTM Approach for Cellular Traffic Forecasting](https://arxiv.org/abs/2507.19513)
*Khalid Ali,Zineddine Bettouche,Andreas Kassler,Andreas Fischer*

Main category: cs.LG

TL;DR: 提出轻量级双路径时空网络用于5G及未来网络流量预测，在真实数据集上表现优于ConvLSTM基线。


<details>
  <summary>Details</summary>
Motivation: 传统AI方法难以捕捉复杂时空模式，需要准确的时空流量预测用于5G及未来网络的智能资源管理。

Method: 引入轻量级双路径时空网络，用Scalar LSTM进行高效时间建模，用三层Conv3D模块进行空间特征提取，通过融合层整合特征。

Result: 设计提高了梯度稳定性和收敛速度，减少了预测误差；在真实数据集上预测性能优于ConvLSTM基线，泛化能力强；MAE较ConvLSTM降低23%，模型泛化能力提高30%。

Conclusion: 该网络适用于大规模下一代网络部署。

Abstract: Accurate spatiotemporal traffic forecasting is vital for intelligent resource
management in 5G and beyond. However, conventional AI approaches often fail to
capture the intricate spatial and temporal patterns that exist, due to e.g.,
the mobility of users. We introduce a lightweight, dual-path Spatiotemporal
Network that leverages a Scalar LSTM (sLSTM) for efficient temporal modeling
and a three-layer Conv3D module for spatial feature extraction. A fusion layer
integrates both streams into a cohesive representation, enabling robust
forecasting. Our design improves gradient stability and convergence speed while
reducing prediction error. Evaluations on real-world datasets show superior
forecast performance over ConvLSTM baselines and strong generalization to
unseen regions, making it well-suited for large-scale, next-generation network
deployments. Experimental evaluation shows a 23% MAE reduction over ConvLSTM,
with a 30% improvement in model generalization.

</details>


### [100] [InkStream: Real-time GNN Inference on Streaming Graphs via Incremental Update](https://arxiv.org/abs/2309.11071)
*Dan Wu,Zhaoying Li,Tulika Mitra*

Main category: cs.LG

TL;DR: 经典GNN推理方法不适用于流图，提出InkStream方法，可实时推理，减少内存访问和计算，实验表明其加速效果显著。


<details>
  <summary>Details</summary>
Motivation: 经典GNN推理方法不适用于随时间演变的流图，流图的动态性给GPU加速带来挑战。

Method: 基于两个关键见解，提出InkStream方法，按需传播和获取数据，使用基于事件的系统控制层间效应传播和节点嵌入的层内增量更新。

Result: 在CPU集群上加速2.5 - 427倍，在两个不同的GPU集群上加速2.4 - 343倍，且输出与最新图快照上的GNN模型推理相同。

Conclusion: InkStream方法可有效解决流图GNN推理的加速问题，且具有高扩展性和易配置性。

Abstract: Classic Graph Neural Network (GNN) inference approaches, designed for static
graphs, are ill-suited for streaming graphs that evolve with time. The dynamism
intrinsic to streaming graphs necessitates constant updates, posing unique
challenges to acceleration on GPU. We address these challenges based on two key
insights: (1) Inside the $k$-hop neighborhood, a significant fraction of the
nodes is not impacted by the modified edges when the model uses min or max as
aggregation function; (2) When the model weights remain static while the graph
structure changes, node embeddings can incrementally evolve over time by
computing only the impacted part of the neighborhood. With these insights, we
propose a novel method, InkStream, designed for real-time inference with
minimal memory access and computation, while ensuring an identical output to
conventional methods. InkStream operates on the principle of propagating and
fetching data only when necessary. It uses an event-based system to control
inter-layer effect propagation and intra-layer incremental updates of node
embedding. InkStream is highly extensible and easily configurable by allowing
users to create and process customized events. We showcase that less than 10
lines of additional user code are needed to support popular GNN models such as
GCN, GraphSAGE, and GIN. Our experiments with three GNN models on four large
graphs demonstrate that InkStream accelerates by 2.5-427$\times$ on a CPU
cluster and 2.4-343$\times$ on two different GPU clusters while producing
identical outputs as GNN model inference on the latest graph snapshot.

</details>


### [101] [Wavelet Logic Machines: Learning and Reasoning in the Spectral Domain Without Neural Networks](https://arxiv.org/abs/2507.19514)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: 提出全谱学习框架，在小波域操作，无传统神经层，在3D去噪和自然语言任务表现好，参数和内存使用少，推理成本低。


<details>
  <summary>Details</summary>
Motivation: 寻找替代传统神经网络的更高效、可解释的模型，降低推理成本。

Method: 在小波域操作，应用可学习的非线性变换和可微小波基选择机制，用线性时间小波变换和逐点非线性。

Result: 在合成3D去噪和GLUE基准自然语言任务中，准确率达89.3%，接近4层Transformer基线，参数少72%，峰值内存少58%，收敛更快。

Conclusion: 该方法是神经模型的有效替代，支持谱学习在视觉和语言任务的可行性，为模型设计提供新方向。

Abstract: We introduce a fully spectral learning framework that eliminates traditional
neural layers by operating entirely in the wavelet domain. The model applies
learnable nonlinear transformations, including soft-thresholding and gain-phase
modulation, directly to wavelet coefficients. It also includes a differentiable
wavelet basis selection mechanism, enabling adaptive processing using families
such as Haar, Daubechies, and Biorthogonal wavelets.
  Implemented in PyTorch with full 3D support, the model maintains a spectral
pipeline without spatial convolutions or attention. On synthetic 3D denoising
and natural language tasks from the GLUE benchmark, including SST-2 sentiment
classification, the model achieves 89.3 percent accuracy, close to a 4-layer
Transformer baseline (90.1 percent), while using 72 percent fewer parameters
and 58 percent less peak memory. Faster early convergence is observed due to
spectral sparsity priors.
  In contrast to the quadratic complexity of self-attention and large matrix
multiplications in Transformers, our approach uses linear-time wavelet
transforms and pointwise nonlinearities, significantly reducing inference cost.
This yields a compact, interpretable, and efficient alternative to neural
models. Our results support the viability of principled spectral learning in
both vision and language tasks, offering new directions for model design
without overparameterized architectures.

</details>


### [102] [A Comparative Analysis of Traditional and Deep Learning Time Series Architectures for Influenza A Infectious Disease Forecasting](https://arxiv.org/abs/2507.19515)
*Edmund F. Agyemang,Hansapani Rodrigo,Vincent Agbenyeavu*

Main category: cs.LG

TL;DR: 本文对传统和深度学习模型预测甲型流感爆发进行比较分析，发现深度学习模型尤其是Transformer表现更优，建议将模型用于实时预测和融入现有监测系统。


<details>
  <summary>Details</summary>
Motivation: 甲型流感每年造成大量呼吸道死亡，需更好的预测模型。

Method: 使用2009年1月至2023年12月历史数据，比较传统ARIMA和ETS模型与六种深度学习架构。

Result: 所有深度学习模型表现优于传统模型，Transformer优势明显，平均测试MSE和MAE分别为0.0433±0.0020和0.1126±0.0016。

Conclusion: 深度学习架构可提升传染病预测建模，未来应将模型用于实时预测和融入现有监测系统。

Abstract: Influenza A is responsible for 290,000 to 650,000 respiratory deaths a year,
though this estimate is an improvement from years past due to improvements in
sanitation, healthcare practices, and vaccination programs. In this study, we
perform a comparative analysis of traditional and deep learning models to
predict Influenza A outbreaks. Using historical data from January 2009 to
December 2023, we compared the performance of traditional ARIMA and Exponential
Smoothing(ETS) models with six distinct deep learning architectures: Simple
RNN, LSTM, GRU, BiLSTM, BiGRU, and Transformer. The results reveal a clear
superiority of all the deep learning models, especially the state-of-the-art
Transformer with respective average testing MSE and MAE of 0.0433 \pm 0.0020
and 0.1126 \pm 0.0016 for capturing the temporal complexities associated with
Influenza A data, outperforming well known traditional baseline ARIMA and ETS
models. These findings of this study provide evidence that state-of-the-art
deep learning architectures can enhance predictive modeling for infectious
diseases and indicate a more general trend toward using deep learning methods
to enhance public health forecasting and intervention planning strategies.
Future work should focus on how these models can be incorporated into real-time
forecasting and preparedness systems at an epidemic level, and integrated into
existing surveillance systems.

</details>


### [103] [NACHOS: Neural Architecture Search for Hardware Constrained Early Exit Neural Networks](https://arxiv.org/abs/2401.13330)
*Matteo Gambella,Jary Pomponi,Simone Scardapane,Manuel Roveri*

Main category: cs.LG

TL;DR: 本文提出NACHOS框架用于设计满足精度和MAC操作数约束的最优EENNs，结果显示其设计的模型有竞争力，并研究了两种新的正则化项。


<details>
  <summary>Details</summary>
Motivation: 当前EENNs设计依赖专家手动进行，复杂且耗时，全面的NAS解决方案少，缺乏兼顾骨干网络和EECs的全自动联合设计策略。

Method: 提出NACHOS框架，进行骨干网络和EECs的联合设计，以选择满足约束的帕累托最优解。

Result: NACHOS设计的模型与现有EENNs具有竞争力，还研究了两种新正则化项的有效性。

Conclusion: NACHOS框架可有效设计满足约束的EENNs，为EENNs的自动化设计提供了新方法。

Abstract: Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN)
with Early Exit Classifiers (EECs), to provide predictions at intermediate
points of the processing when enough confidence in classification is achieved.
This leads to many benefits in terms of effectiveness and efficiency.
Currently, the design of EENNs is carried out manually by experts, a complex
and time-consuming task that requires accounting for many aspects, including
the correct placement, the thresholding, and the computational overhead of the
EECs. For this reason, the research is exploring the use of Neural Architecture
Search (NAS) to automatize the design of EENNs. Currently, few comprehensive
NAS solutions for EENNs have been proposed in the literature, and a fully
automated, joint design strategy taking into consideration both the backbone
and the EECs remains an open problem. To this end, this work presents Neural
Architecture Search for Hardware Constrained Early Exit Neural Networks
(NACHOS), the first NAS framework for the design of optimal EENNs satisfying
constraints on the accuracy and the number of Multiply and Accumulate (MAC)
operations performed by the EENNs at inference time. In particular, this
provides the joint design of backbone and EECs to select a set of admissible
(i.e., respecting the constraints) Pareto Optimal Solutions in terms of best
tradeoff between the accuracy and number of MACs. The results show that the
models designed by NACHOS are competitive with the state-of-the-art EENNs.
Additionally, this work investigates the effectiveness of two novel
regularization terms designed for the optimization of the auxiliary classifiers
of the EENN

</details>


### [104] [BikeVAE-GNN: A Variational Autoencoder-Augmented Hybrid Graph Neural Network for Sparse Bicycle Volume Estimation](https://arxiv.org/abs/2507.19517)
*Mohit Gupta,Debjit Bhowmick,Ben Beck*

Main category: cs.LG

TL;DR: 提出BikeVAE - GNN框架解决自行车网络数据稀疏问题以估计自行车流量，在墨尔本数据上验证有效。


<details>
  <summary>Details</summary>
Motivation: 城市自行车网络计数数据极其稀疏，准确估计链路级自行车流量对城市和交通规划至关重要。

Method: 提出BikeVAE - GNN框架，结合混合图神经网络（GCN、GAT、GraphSAGE）和变分自编码器，同时进行回归和分类任务。

Result: 在墨尔本数据上，BikeVAE - GNN优于机器学习和基线GNN模型，MAE为30.82，准确率99%，F1分数0.99，消融实验验证组件有效性。

Conclusion: 研究用新颖方法推进稀疏网络中自行车流量估计，为可持续自行车基础设施提供见解。

Abstract: Accurate link-level bicycle volume estimation is essential for informed urban
and transport planning but it is challenged by extremely sparse count data in
urban bicycling networks worldwide. We propose BikeVAE-GNN, a novel dual-task
framework augmenting a Hybrid Graph Neural Network (GNN) with Variational
Autoencoder (VAE) to estimate Average Daily Bicycle (ADB) counts, addressing
sparse bicycle networks. The Hybrid-GNN combines Graph Convolutional Networks
(GCN), Graph Attention Networks (GAT), and GraphSAGE to effectively model
intricate spatial relationships in sparse networks while VAE generates
synthetic nodes and edges to enrich the graph structure and enhance the
estimation performance. BikeVAE-GNN simultaneously performs - regression for
bicycling volume estimation and classification for bicycling traffic level
categorization. We demonstrate the effectiveness of BikeVAE-GNN using
OpenStreetMap data and publicly available bicycle count data within the City of
Melbourne - where only 141 of 15,933 road segments have labeled counts
(resulting in 99% count data sparsity). Our experiments show that BikeVAE-GNN
outperforms machine learning and baseline GNN models, achieving a mean absolute
error (MAE) of 30.82 bicycles per day, accuracy of 99% and F1-score of 0.99.
Ablation studies further validate the effective role of Hybrid-GNN and VAE
components. Our research advances bicycling volume estimation in sparse
networks using novel and state-of-the-art approaches, providing insights for
sustainable bicycling infrastructures.

</details>


### [105] [Directly Learning Stock Trading Strategies Through Profit Guided Loss Functions](https://arxiv.org/abs/2507.19639)
*Devroop Kar,Zimeng Lyu,Sheeraja Rajakrishnan,Hao Zhang,Alex Ororbia,Travis Desell,Daniel Krutz*

Main category: cs.LG

TL;DR: 提出四种新损失函数用于股票投资组合决策，在训练时间序列模型后产生显著利润，优于基准强化学习技术和买入持有法。


<details>
  <summary>Details</summary>
Motivation: 股票市场高度波动，难以做出盈利的交易决策。

Method: 提出四种新损失函数，让任意神经网络学习有效交易策略，在损失函数上训练时间序列模型。

Result: 训练的交易策略在50只不同的标准普尔500公司股票投资组合上产生显著利润，如Crossformer模型在2021 - 2023年回报率分别为51.42%、51.04%和48.62%，优于PPO和DDPG。

Conclusion: 新提出的损失函数能有效驱动股票交易决策，生成的交易策略可获得显著利润。

Abstract: Stock trading has always been a challenging task due to the highly volatile
nature of the stock market. Making sound trading decisions to generate profit
is particularly difficult under such conditions. To address this, we propose
four novel loss functions to drive decision-making for a portfolio of stocks.
These functions account for the potential profits or losses based with respect
to buying or shorting respective stocks, enabling potentially any artificial
neural network to directly learn an effective trading strategy. Despite the
high volatility in stock market fluctuations over time, training time-series
models such as transformers on these loss functions resulted in trading
strategies that generated significant profits on a portfolio of 50 different
S&P 500 company stocks as compared to a benchmark reinforcment learning
techniques and a baseline buy and hold method. As an example, using 2021, 2022
and 2023 as three test periods, the Crossformer model adapted with our best
loss function was most consistent, resulting in returns of 51.42%, 51.04% and
48.62% respectively. In comparison, the best performing state-of-the-art
reinforcement learning methods, PPO and DDPG, only delivered maximum profits of
around 41%, 2.81% and 41.58% for the same periods. The code is available at
https://anonymous.4open.science/r/bandit-stock-trading-58C8/README.md.

</details>


### [106] [Target Circuit Matching in Large-Scale Netlists using GNN-Based Region Prediction](https://arxiv.org/abs/2507.19518)
*Sangwoo Seo,Jimin Seo,Yoonho Lee,Donghyeon Kim,Hyejin Shin,Banghyun Sung,Chanyoung Park*

Main category: cs.LG

TL;DR: 本文提出利用GNN的高效子图匹配方法，实验表明该方法在时间效率和目标区域预测上显著优于现有方法，为大规模电路子图匹配提供可扩展有效方案。


<details>
  <summary>Details</summary>
Motivation: 传统规则方法在泛化到任意目标电路有局限，节点匹配方法计算效率低，现有深度学习模型无法有效捕获全局子图嵌入或依赖低效匹配矩阵，限制其在大型电路的有效性。

Method: 利用GNN预测包含目标电路的高概率区域，构建负样本使GNN准确学习目标电路存在情况，开发从整个电路直接提取子图嵌入的方法。

Result: 所提方法在时间效率和目标区域预测上显著优于现有方法。

Conclusion: 该方法为大规模电路子图匹配提供了可扩展且有效的解决方案。

Abstract: Subgraph matching plays an important role in electronic design automation
(EDA) and circuit verification. Traditional rule-based methods have limitations
in generalizing to arbitrary target circuits. Furthermore, node-to-node
matching approaches tend to be computationally inefficient, particularly for
large-scale circuits. Deep learning methods have emerged as a potential
solution to address these challenges, but existing models fail to efficiently
capture global subgraph embeddings or rely on inefficient matching matrices,
which limits their effectiveness for large circuits. In this paper, we propose
an efficient graph matching approach that utilizes Graph Neural Networks (GNNs)
to predict regions of high probability for containing the target circuit.
Specifically, we construct various negative samples to enable GNNs to
accurately learn the presence of target circuits and develop an approach to
directly extracting subgraph embeddings from the entire circuit, which captures
global subgraph information and addresses the inefficiency of applying GNNs to
all candidate subgraphs. Extensive experiments demonstrate that our approach
significantly outperforms existing methods in terms of time efficiency and
target region prediction, offering a scalable and effective solution for
subgraph matching in large-scale circuits.

</details>


### [107] [Technical Indicator Networks (TINs): An Interpretable Neural Architecture Modernizing Classic al Technical Analysis for Adaptive Algorithmic Trading](https://arxiv.org/abs/2507.20202)
*Longfei Lu*

Main category: cs.LG

TL;DR: 研究指出金融分析中多数经典技术指标本质是固定可解释权重的神经网络特例，引入技术指标网络（TINs）升级传统指标，推动算法交易进入新时代。


<details>
  <summary>Details</summary>
Motivation: 将传统技术分析与当代人工智能系统相结合，升级传统技术指标以推动算法交易发展。

Method: 提出多数经典技术指标是神经网络特例，引入技术指标网络（TINs），支持n维输入并将领域知识编码到神经结构中。

Result: 表明常见技术指标可拓扑重构为模块化神经网络组件，TINs能复制并升级传统指标。

Conclusion: TINs现代化了技术分析的基础逻辑，将传统指标与当代AI系统潜力相融合。

Abstract: This work proposes that a vast majority of classical technical indicators in
financial analysis are, in essence, special cases of neural networks with fixed
and interpretable weights. It is shown that nearly all such indicators, such as
moving averages, momentum-based oscillators, volatility bands, and other
commonly used technical constructs, can be reconstructed topologically as
modular neural network components. Technical Indicator Networks (TINs) are
introduced as a general neural architecture that replicates and structurally
upgrades traditional indicators by supporting n-dimensional inputs such as
price, volume, sentiment, and order book data. By encoding domain-specific
knowledge into neural structures, TINs modernize the foundational logic of
technical analysis and propel algorithmic trading into a new era, bridging the
legacy of proven indicators with the potential of contemporary AI systems.

</details>


### [108] [Physics-informed transfer learning for SHM via feature selection](https://arxiv.org/abs/2507.19519)
*J. Poole,P. Gardner,A. J. Hughes,N. Dervilis,R. S. Mills,T. A. Dardeno,K. Worden*

Main category: cs.LG

TL;DR: 本文针对结构健康监测系统训练数据获取难问题，提出利用物理知识和模态保证准则（MAC）进行特征选择的迁移学习方法，并通过案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 结构健康监测系统训练数据获取昂贵且困难，不同结构训练和测试分布有差异，传统机器学习方法难以泛化，且目标域缺乏标签限制了数据指标的使用，选择合适的源结构和特征有挑战。

Method: 利用物理知识选择更相似的特征，使用模态保证准则（MAC）量化健康结构模态的对应关系，将MAC作为选择在不同域中受损伤时表现一致特征的度量。

Result: MAC与衡量联合分布相似性的监督指标有高对应性，通过数值和实验案例证明了该方法在不同应用中的有效性。

Conclusion: 提出的利用MAC进行特征选择的迁移学习方法能有效解决结构健康监测系统训练数据问题，可在不同应用中发挥作用。

Abstract: Data used for training structural health monitoring (SHM) systems are
expensive and often impractical to obtain, particularly labelled data.
Population-based SHM presents a potential solution to this issue by considering
the available data across a population of structures. However, differences
between structures will mean the training and testing distributions will
differ; thus, conventional machine learning methods cannot be expected to
generalise between structures. To address this issue, transfer learning (TL),
can be used to leverage information across related domains. An important
consideration is that the lack of labels in the target domain limits data-based
metrics to quantifying the discrepancy between the marginal distributions.
Thus, a prerequisite for the application of typical unsupervised TL methods is
to identify suitable source structures (domains), and a set of features, for
which the conditional distributions are related to the target structure.
Generally, the selection of domains and features is reliant on domain
expertise; however, for complex mechanisms, such as the influence of damage on
the dynamic response of a structure, this task is not trivial. In this paper,
knowledge of physics is leveraged to select more similar features, the modal
assurance criterion (MAC) is used to quantify the correspondence between the
modes of healthy structures. The MAC is shown to have high correspondence with
a supervised metric that measures joint-distribution similarity, which is the
primary indicator of whether a classifier will generalise between domains. The
MAC is proposed as a measure for selecting a set of features that behave
consistently across domains when subjected to damage, i.e. features with
invariance in the conditional distributions. This approach is demonstrated on
numerical and experimental case studies to verify its effectiveness in various
applications.

</details>


### [109] [PySHRED: A Python package for SHallow REcurrent Decoding for sparse sensing, model reduction and scientific discovery](https://arxiv.org/abs/2507.20954)
*David Ye,Jan Williams,Mars Gao,Stefano Riva,Matteo Tomasetto,David Zoro,J. Nathan Kutz*

Main category: cs.LG

TL;DR: 介绍PySHRED 1.0版本，它实现SHRED及其扩展，适用于处理复杂现实数据，安装方便、文档全、有示例且模块化，代码开源。


<details>
  <summary>Details</summary>
Motivation: 为处理高维动力系统和时空数据，推出实现SHRED及其扩展的Python包PySHRED的1.0版本。

Method: 引入包含数据预处理器和前沿SHRED方法的PySHRED 1.0版本。

Result: PySHRED 1.0版本易于安装、文档详细、有大量代码示例、模块化结构，代码开源。

Conclusion: PySHRED 1.0版本为处理复杂现实数据提供了有效工具，具有较强实用性和扩展性。

Abstract: SHallow REcurrent Decoders (SHRED) provide a deep learning strategy for
modeling high-dimensional dynamical systems and/or spatiotemporal data from
dynamical system snapshot observations. PySHRED is a Python package that
implements SHRED and several of its major extensions, including for robust
sensing, reduced order modeling and physics discovery. In this paper, we
introduce the version 1.0 release of PySHRED, which includes data preprocessors
and a number of cutting-edge SHRED methods specifically designed to handle
real-world data that may be noisy, multi-scale, parameterized, prohibitively
high-dimensional, and strongly nonlinear. The package is easy to install,
thoroughly-documented, supplemented with extensive code examples, and
modularly-structured to support future additions. The entire codebase is
released under the MIT license and is available at
https://github.com/pyshred-dev/pyshred.

</details>


### [110] [Learning from Expert Factors: Trajectory-level Reward Shaping for Formulaic Alpha Mining](https://arxiv.org/abs/2507.20263)
*Junjie Zhao,Chengxi Zhang,Chenkai Wang,Peng Yang*

Main category: cs.LG

TL;DR: 现有强化学习挖掘投资因子方法因奖励稀疏受限，提出TLRS奖励塑造方法，实验表明其提升因子预测力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习挖掘公式化alpha因子的方法受马尔可夫决策过程中稀疏奖励的阻碍，限制搜索空间探索并使训练不稳定。

Method: 提出Trajectory - level Reward Shaping (TLRS)方法，通过测量部分生成表达式与专家设计公式的子序列级相似度提供密集中间奖励，并引入奖励中心化机制减少训练方差。

Result: 在六个中美主要股票指数上实验，TLRS显著提升挖掘因子的预测力，使Rank Information Coefficient比现有基于势能的塑造算法提高9.29%，计算效率大幅提升，时间复杂度从线性降至常数。

Conclusion: TLRS方法有效改善了强化学习挖掘投资因子的效率和效果。

Abstract: Reinforcement learning (RL) has successfully automated the complex process of
mining formulaic alpha factors, for creating interpretable and profitable
investment strategies. However, existing methods are hampered by the sparse
rewards given the underlying Markov Decision Process. This inefficiency limits
the exploration of the vast symbolic search space and destabilizes the training
process. To address this, Trajectory-level Reward Shaping (TLRS), a novel
reward shaping method, is proposed. TLRS provides dense, intermediate rewards
by measuring the subsequence-level similarity between partially generated
expressions and a set of expert-designed formulas. Furthermore, a reward
centering mechanism is introduced to reduce training variance. Extensive
experiments on six major Chinese and U.S. stock indices show that TLRS
significantly improves the predictive power of mined factors, boosting the Rank
Information Coefficient by 9.29% over existing potential-based shaping
algorithms. Notably, TLRS achieves a major leap in computational efficiency by
reducing its time complexity with respect to the feature dimension from linear
to constant, which is a significant improvement over distance-based baselines.

</details>


### [111] [Exoplanet Detection Using Machine Learning Models Trained on Synthetic Light Curves](https://arxiv.org/abs/2507.19520)
*Ethan Lo,Dan C. Lo*

Main category: cs.LG

TL;DR: 手动搜索系外行星效率低，本文研究多种常用机器学习模型用于系外行星发现与验证，初始结果不错但需数据增强技术，结论是数据增强可提升召回率和精度。


<details>
  <summary>Details</summary>
Motivation: 手动搜索系外行星效率低，现有大公司的机器学习模型依赖复杂算法和超级计算机，为降低复杂度开展研究。

Method: 采用逻辑回归、k近邻和随机森林等机器学习模型，使用NASA开普勒太空望远镜的数据集进行训练和预测。

Result: 各模型初始结果不错，但因潜在偏差和数据集不平衡，需数据增强技术以确保更公平预测和提高泛化能力。

Conclusion: 在搜索系外行星场景中，数据增强技术显著提升召回率和精度，各模型准确率有差异。

Abstract: With manual searching processes, the rate at which scientists and astronomers
discover exoplanets is slow because of inefficiencies that require an extensive
time of laborious inspections. In fact, as of now there have been about only
5,000 confirmed exoplanets since the late 1900s. Recently, machine learning
(ML) has proven to be extremely valuable and efficient in various fields,
capable of processing massive amounts of data in addition to increasing its
accuracy by learning. Though ML models for discovering exoplanets owned by
large corporations (e.g. NASA) exist already, they largely depend on complex
algorithms and supercomputers. In an effort to reduce such complexities, in
this paper, we report the results and potential benefits of various, well-known
ML models in the discovery and validation of extrasolar planets. The ML models
that are examined in this study include logistic regression, k-nearest
neighbors, and random forest. The dataset on which the models train and predict
is acquired from NASA's Kepler space telescope. The initial results show
promising scores for each model. However, potential biases and dataset
imbalances necessitate the use of data augmentation techniques to further
ensure fairer predictions and improved generalization. This study concludes
that, in the context of searching for exoplanets, data augmentation techniques
significantly improve the recall and precision, while the accuracy varies for
each model.

</details>


### [112] [Strategic Filtering for Content Moderation: Free Speech or Free of Distortion?](https://arxiv.org/abs/2507.20061)
*Saba Ahmadi,Avrim Blum,Haifeng Xu,Fan Yao*

Main category: cs.LG

TL;DR: 社交媒体平台用户生成内容易受煽动和操纵，需有效监管，研究用机制设计优化言论自由与减少内容操纵间的平衡，提出近似最优解的方法并给出泛化保证。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台自动内容审核引发用户策略性回应，需平衡言论自由和减少内容操纵。

Method: 从机制设计角度解决平衡优化问题，提出近似最优解的实用方法。

Result: 提出近似最优解的实用方法，给出有效近似最优审核器所需有限离线数据量的泛化保证。

Conclusion: 虽确定最优权衡是NP难问题，但可通过实用方法近似最优解。

Abstract: User-generated content (UGC) on social media platforms is vulnerable to
incitements and manipulations, necessitating effective regulations. To address
these challenges, those platforms often deploy automated content moderators
tasked with evaluating the harmfulness of UGC and filtering out content that
violates established guidelines. However, such moderation inevitably gives rise
to strategic responses from users, who strive to express themselves within the
confines of guidelines. Such phenomena call for a careful balance between: 1.
ensuring freedom of speech -- by minimizing the restriction of expression; and
2. reducing social distortion -- measured by the total amount of content
manipulation. We tackle the problem of optimizing this balance through the lens
of mechanism design, aiming at optimizing the trade-off between minimizing
social distortion and maximizing free speech. Although determining the optimal
trade-off is NP-hard, we propose practical methods to approximate the optimal
solution. Additionally, we provide generalization guarantees determining the
amount of finite offline data required to approximate the optimal moderator
effectively.

</details>


### [113] [Applications and Manipulations of Physics-Informed Neural Networks in Solving Differential Equations](https://arxiv.org/abs/2507.19522)
*Aarush Gupta,Kendric Hsu,Syna Mathod*

Main category: cs.LG

TL;DR: 本文介绍神经网络数学模型可解决正逆问题，重点研究带不同复杂度残差的物理信息神经网络（PINN），用Python和PyTorch库求解复杂微分方程。


<details>
  <summary>Details</summary>
Motivation: 利用神经网络数学模型解决复杂微分方程正逆问题，借助PINN注入先验信息提升模型性能。

Method: 创建带不同复杂度残差的PINN，从线性和二次模型扩展到热方程等复杂微分方程，以Python为计算语言，用PyTorch库辅助研究。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Mathematical models in neural networks are powerful tools for solving complex
differential equations and optimizing their parameters; that is, solving the
forward and inverse problems, respectively. A forward problem predicts the
output of a network for a given input by optimizing weights and biases. An
inverse problem finds equation parameters or coefficients that effectively
model the data. A Physics-Informed Neural Network (PINN) can solve both
problems. PINNs inject prior analytical information about the data into the
cost function to improve model performance outside the training set boundaries.
This also allows PINNs to efficiently solve problems with sparse data without
overfitting by extrapolating the model to fit larger trends in the data. The
prior information we implement is in the form of differential equations.
Residuals are the differences between the left-hand and right-hand sides of
corresponding differential equations; PINNs minimize these residuals to
effectively solve the differential equation and take advantage of prior
knowledge. In this way, the solution and parameters are embedded into the loss
function and optimized, allowing both the weights of the neural network and the
model parameters to be found simultaneously, solving both the forward and
inverse problems in the process. In this paper, we will create PINNs with
residuals of varying complexity, beginning with linear and quadratic models and
then expanding to fit models for the heat equation and other complex
differential equations. We will mainly use Python as the computing language,
using the PyTorch library to aid us in our research.

</details>


### [114] [Online Learning with Probing for Sequential User-Centric Selection](https://arxiv.org/abs/2507.20112)
*Tianyi Xu,Yiting Chen,Henger Li,Zheyong Bian,Emiliano Dall'Anese,Zizhan Zheng*

Main category: cs.LG

TL;DR: 提出PUCS框架解决信息获取的序贯决策问题，针对离线和在线场景分别给出算法并分析性能，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决资源和回报初始未知且探测成本高的序贯决策问题，如拼车、无线调度和内容推荐等应用。

Method: 提出PUCS框架，离线场景用贪心探测算法，在线场景引入OLPA随机组合多臂老虎机算法。

Result: 离线算法有常数因子近似保证，在线算法有后悔界，且证明了下界，实验验证了方案有效性。

Conclusion: 所提出的框架和算法能有效解决信息获取的序贯决策问题。

Abstract: We formalize sequential decision-making with information acquisition as the
probing-augmented user-centric selection (PUCS) framework, where a learner
first probes a subset of arms to obtain side information on resources and
rewards, and then assigns $K$ plays to $M$ arms. PUCS covers applications such
as ridesharing, wireless scheduling, and content recommendation, in which both
resources and payoffs are initially unknown and probing is costly. For the
offline setting with known distributions, we present a greedy probing algorithm
with a constant-factor approximation guarantee $\zeta = (e-1)/(2e-1)$. For the
online setting with unknown distributions, we introduce OLPA, a stochastic
combinatorial bandit algorithm that achieves a regret bound
$\mathcal{O}(\sqrt{T} + \ln^{2} T)$. We also prove a lower bound
$\Omega(\sqrt{T})$, showing that the upper bound is tight up to logarithmic
factors. Experiments on real-world data demonstrate the effectiveness of our
solutions.

</details>


### [115] [Language Models for Controllable DNA Sequence Design](https://arxiv.org/abs/2507.19523)
*Xingyu Su,Xiner Li,Yuchao Lin,Ziqian Xie,Degui Zhi,Shuiwang Ji*

Main category: cs.LG

TL;DR: 提出ATGC - Gen用于可控DNA序列设计，评估其在代表性任务上的表现，模型在可控性和功能相关性上有提升。


<details>
  <summary>Details</summary>
Motivation: 语言模型在自然语言生成中成功，但在DNA序列生成应用未充分探索，需要用于可控DNA序列设计的模型。

Method: 引入ATGC - Gen，利用跨模态编码整合生物信号，采用解码器和编码器架构，可在自回归或掩码恢复目标下训练和生成。

Result: 实验表明ATGC - Gen能生成符合期望属性的序列，相比之前方法在可控性和功能相关性上有显著提升。

Conclusion: 语言模型在推进可编程基因组设计方面有潜力。

Abstract: We consider controllable DNA sequence design, where sequences are generated
by conditioning on specific biological properties. While language models (LMs)
such as GPT and BERT have achieved remarkable success in natural language
generation, their application to DNA sequence generation remains largely
underexplored. In this work, we introduce ATGC-Gen, an Automated Transformer
Generator for Controllable Generation, which leverages cross-modal encoding to
integrate diverse biological signals. ATGC-Gen is instantiated with both
decoder-only and encoder-only transformer architectures, allowing flexible
training and generation under either autoregressive or masked recovery
objectives. We evaluate ATGC-Gen on representative tasks including promoter and
enhancer sequence design, and further introduce a new dataset based on ChIP-Seq
experiments for modeling protein binding specificity. Our experiments
demonstrate that ATGC-Gen can generate fluent, diverse, and biologically
relevant sequences aligned with the desired properties. Compared to prior
methods, our model achieves notable improvements in controllability and
functional relevance, highlighting the potential of language models in
advancing programmable genomic design. The source code is released at
(https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen).

</details>


### [116] [MH-GIN: Multi-scale Heterogeneous Graph-based Imputation Network for AIS Data (Extended Version)](https://arxiv.org/abs/2507.20362)
*Hengyu Liu,Tianyi Li,Yuqiang He,Kristian Torp,Yushuai Li,Christian S. Jensen*

Main category: cs.LG

TL;DR: 现有AIS位置跟踪数据存在缺失值问题，因属性更新率不同导致多尺度依赖，现有方法难以处理。本文提出MH - GIN网络，能捕捉多尺度依赖，实验显示可减少57%误差且保持计算效率。


<details>
  <summary>Details</summary>
Motivation: AIS位置跟踪数据存在缺失值，不同属性更新率不同产生多尺度依赖，现有方法无法处理该依赖，导致插补精度受限。

Method: 提出MH - GIN网络，先提取各属性多尺度时间特征并保留异质性，再构建多尺度异质图通过图传播插补缺失值。

Result: 在两个真实数据集上实验，MH - GIN比现有方法平均减少57%插补误差，且保持计算效率。

Conclusion: MH - GIN能有效捕捉多尺度依赖，提高插补精度，代码公开可查。

Abstract: Location-tracking data from the Automatic Identification System, much of
which is publicly available, plays a key role in a range of maritime safety and
monitoring applications. However, the data suffers from missing values that
hamper downstream applications. Imputing the missing values is challenging
because the values of different heterogeneous attributes are updated at diverse
rates, resulting in the occurrence of multi-scale dependencies among
attributes. Existing imputation methods that assume similar update rates across
attributes are unable to capture and exploit such dependencies, limiting their
imputation accuracy. We propose MH-GIN, a Multi-scale Heterogeneous Graph-based
Imputation Network that aims improve imputation accuracy by capturing
multi-scale dependencies. Specifically, MH-GIN first extracts multi-scale
temporal features for each attribute while preserving their intrinsic
heterogeneous characteristics. Then, it constructs a multi-scale heterogeneous
graph to explicitly model dependencies between heterogeneous attributes to
enable more accurate imputation of missing values through graph propagation.
Experimental results on two real-world datasets find that MH-GIN is capable of
an average 57% reduction in imputation errors compared to state-of-the-art
methods, while maintaining computational efficiency. The source code and
implementation details of MH-GIN are publicly available
https://github.com/hyLiu1994/MH-GIN.

</details>


### [117] [Kolmogorov Arnold Network Autoencoder in Medicine](https://arxiv.org/abs/2507.19524)
*Ugo Lomoio,Pierangelo Veltri,Pietro Hiram Guzzi*

Main category: cs.LG

TL;DR: 本文聚焦KAN架构，将多种普通自编码器与KAN对应版本对比，用心脏病信号在医学数据集上开展五类自编码器任务实验。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习架构有局限，KAN架构有更好表现，需对不同版本自编码器与KAN对应版本进行性能评估。

Method: 选取普通自编码器（线性、卷积和变分）与其KAN对应版本，以心脏病信号为输入，在医学数据集AbnormalHeartbeat上开展五类自编码器任务实验。

Result: 未提及。

Conclusion: 未提及。

Abstract: Deep learning neural networks architectures such Multi Layer Perceptrons
(MLP) and Convolutional blocks still play a crucial role in nowadays research
advancements. From a topological point of view, these architecture may be
represented as graphs in which we learn the functions related to the nodes
while fixed edges convey the information from the input to the output. A recent
work introduced a new architecture called Kolmogorov Arnold Networks (KAN) that
reports how putting learnable activation functions on the edges of the neural
network leads to better performances in multiple scenarios. Multiple studies
are focusing on optimizing the KAN architecture by adding important features
such as dropout regularization, Autoencoders (AE), model benchmarking and last,
but not least, the KAN Convolutional Network (KCN) that introduced matrix
convolution with KANs learning. This study aims to benchmark multiple versions
of vanilla AEs (such as Linear, Convolutional and Variational) against their
Kolmogorov-Arnold counterparts that have same or less number of parameters.
Using cardiological signals as model input, a total of five different classic
AE tasks were studied: reconstruction, generation, denoising, inpainting and
anomaly detection. The proposed experiments uses a medical dataset
\textit{AbnormalHeartbeat} that contains audio signals obtained from the
stethoscope.

</details>


### [118] [MMCircuitEval: A Comprehensive Multimodal Circuit-Focused Benchmark for Evaluating LLMs](https://arxiv.org/abs/2507.19525)
*Chenchen Zhao,Zhengyuan Shi,Xiangyu Wen,Chengjie Liu,Yi Liu,Yunhao Zhou,Yuxiang Zhao,Hefei Feng,Yinan Zhu,Gwok-Waa Wan,Xin Cheng,Weiyu Chen,Yongqi Fu,Chujie Chen,Chenhao Xue,Guangyu Sun,Ying Wang,Yibo Lin,Jun Yang,Ning Xu,Xi Wang,Qiang Xu*

Main category: cs.LG

TL;DR: 本文提出首个多模态基准MMCircuitEval用于全面评估多模态大语言模型在电子设计自动化（EDA）电路设计中的性能，发现现有模型存在性能差距，该基准可推动模型在EDA中的发展。


<details>
  <summary>Details</summary>
Motivation: 现有基准评估范围狭窄，难以全面评估多模态大语言模型在电路设计中的性能。

Method: 引入MMCircuitEval基准，包含3614个精心策划的问答对，涵盖数字和模拟电路的关键EDA阶段，对问答对严格审核，按多种维度分类问题。

Result: 评估显示现有大语言模型存在显著性能差距，尤其在后端设计和复杂计算方面。

Conclusion: MMCircuitEval为推进EDA中多模态大语言模型发展提供基础资源，促进其融入实际电路设计工作流。

Abstract: The emergence of multimodal large language models (MLLMs) presents promising
opportunities for automation and enhancement in Electronic Design Automation
(EDA). However, comprehensively evaluating these models in circuit design
remains challenging due to the narrow scope of existing benchmarks. To bridge
this gap, we introduce MMCircuitEval, the first multimodal benchmark
specifically designed to assess MLLM performance comprehensively across diverse
EDA tasks. MMCircuitEval comprises 3614 meticulously curated question-answer
(QA) pairs spanning digital and analog circuits across critical EDA stages -
ranging from general knowledge and specifications to front-end and back-end
design. Derived from textbooks, technical question banks, datasheets, and
real-world documentation, each QA pair undergoes rigorous expert review for
accuracy and relevance. Our benchmark uniquely categorizes questions by design
stage, circuit type, tested abilities (knowledge, comprehension, reasoning,
computation), and difficulty level, enabling detailed analysis of model
capabilities and limitations. Extensive evaluations reveal significant
performance gaps among existing LLMs, particularly in back-end design and
complex computations, highlighting the critical need for targeted training
datasets and modeling approaches. MMCircuitEval provides a foundational
resource for advancing MLLMs in EDA, facilitating their integration into
real-world circuit design workflows. Our benchmark is available at
https://github.com/cure-lab/MMCircuitEval.

</details>


### [119] [Quantizing Text-attributed Graphs for Semantic-Structural Integration](https://arxiv.org/abs/2507.19526)
*Jianyuan Bo,Hao Wu,Yuan Fang*

Main category: cs.LG

TL;DR: 提出STAG框架，将图结构信息量化为离散令牌，支持零样本迁移学习，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将图结构信息嵌入大语言模型兼容格式时面临挑战，且迁移学习需源域标注数据，限制了适应性。

Method: 提出STAG自监督框架，用冻结码本将图结构信息量化为离散令牌，采用软分配和KL散度引导量化。

Result: 在多个节点分类基准测试中达到了最先进的性能，且与不同大语言模型架构兼容。

Conclusion: STAG框架为图学习和大语言模型之间的桥梁提供了优雅的解决方案。

Abstract: Text-attributed graphs (TAGs) have emerged as a powerful representation for
modeling complex relationships across diverse domains. With the rise of large
language models (LLMs), there is growing interest in leveraging their
capabilities for graph learning. However, current approaches face significant
challenges in embedding structural information into LLM-compatible formats,
requiring either computationally expensive alignment mechanisms or manual graph
verbalization techniques that often lose critical structural details. Moreover,
these methods typically require labeled data from source domains for effective
transfer learning, significantly constraining their adaptability. We propose
STAG, a novel self-supervised framework that directly quantizes graph
structural information into discrete tokens using a frozen codebook. Unlike
traditional quantization approaches, our method employs soft assignment and KL
divergence guided quantization to address the unique challenges of graph data,
which lacks natural tokenization structures. Our framework enables both
LLM-based and traditional learning approaches, supporting true zero-shot
transfer learning without requiring labeled data even in the source domain.
Extensive experiments demonstrate state-of-the-art performance across multiple
node classification benchmarks while maintaining compatibility with different
LLM architectures, offering an elegant solution to bridging graph learning with
LLMs.

</details>


### [120] [Research on the application of graph data structure and graph neural network in node classification/clustering tasks](https://arxiv.org/abs/2507.19527)
*Yihan Wang,Jianing Zhao*

Main category: cs.LG

TL;DR: 研究图数据结构、经典图算法和图神经网络，对比性能并探索集成策略，GNN比传统方法准确率提升43% - 70%。


<details>
  <summary>Details</summary>
Motivation: 图结构数据非欧几里得特性给传统机器学习方法带来挑战，需研究新方法。

Method: 对图数据结构、经典图算法和GNN进行理论分析和比较评估，开展对比实验。

Result: GNN在节点分类和聚类任务中比传统方法准确率提升43% - 70%。

Conclusion: 探索经典算法和GNN架构集成策略，为图表示学习研究提供理论指导。

Abstract: Graph-structured data are pervasive across domains including social networks,
biological networks, and knowledge graphs. Due to their non-Euclidean nature,
such data pose significant challenges to conventional machine learning methods.
This study investigates graph data structures, classical graph algorithms, and
Graph Neural Networks (GNNs), providing comprehensive theoretical analysis and
comparative evaluation. Through comparative experiments, we quantitatively
assess performance differences between traditional algorithms and GNNs in node
classification and clustering tasks. Results show GNNs achieve substantial
accuracy improvements of 43% to 70% over traditional methods. We further
explore integration strategies between classical algorithms and GNN
architectures, providing theoretical guidance for advancing graph
representation learning research.

</details>


### [121] [Efficient and Scalable Agentic AI with Heterogeneous Systems](https://arxiv.org/abs/2507.19635)
*Zain Asgar,Michelle Nguyen,Sachin Katti*

Main category: cs.LG

TL;DR: 提出在异构计算基础设施上动态编排AI代理工作负载的系统设计，初步结果显示可带来TCO效益。


<details>
  <summary>Details</summary>
Motivation: AI代理工作负载动态且结构复杂，需要高效可扩展的部署和服务基础设施。

Method: 构建规划和优化执行图的框架、基于MLIR的表示和编译系统、动态编排系统。

Result: 利用异构基础设施可带来显著TCO效益，部分工作负载中旧GPU与新加速器组合TCO与最新同构GPU基础设施相近。

Conclusion: 该系统设计能在异构计算基础设施上动态编排AI代理工作负载，实现系统级TCO优化。

Abstract: AI agents are emerging as a dominant workload in a wide range of
applications, promising to be the vehicle that delivers the promised benefits
of AI to enterprises and consumers. Unlike conventional software or static
inference, agentic workloads are dynamic and structurally complex. Often these
agents are directed graphs of compute and IO operations that span multi-modal
data input and conversion), data processing and context gathering (e.g vector
DB lookups), multiple LLM inferences, tool calls, etc. To scale AI agent usage,
we need efficient and scalable deployment and agent-serving infrastructure.
  To tackle this challenge, in this paper, we present a system design for
dynamic orchestration of AI agent workloads on heterogeneous compute
infrastructure spanning CPUs and accelerators, both from different vendors and
across different performance tiers within a single vendor. The system delivers
several building blocks: a framework for planning and optimizing agentic AI
execution graphs using cost models that account for compute, memory, and
bandwidth constraints of different HW; a MLIR based representation and
compilation system that can decompose AI agent execution graphs into granular
operators and generate code for different HW options; and a dynamic
orchestration system that can place the granular components across a
heterogeneous compute infrastructure and stitch them together while meeting an
end-to-end SLA. Our design performs a systems level TCO optimization and
preliminary results show that leveraging a heterogeneous infrastructure can
deliver significant TCO benefits. A preliminary surprising finding is that for
some workloads a heterogeneous combination of older generation GPUs with newer
accelerators can deliver similar TCO as the latest generation homogenous GPU
infrastructure design, potentially extending the life of deployed
infrastructure.

</details>


### [122] [Machine Learning Risk Intelligence for Green Hydrogen Investment: Insights for Duqm R3 Auction](https://arxiv.org/abs/2507.19529)
*Obumneme Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: 阿曼开展绿氢项目第三轮拍卖，但缺乏大规模氢设施运营数据。本文提出利用气象数据的人工智能决策支持系统，开发预测性维护压力指数，用于风险评估和拍卖决策。


<details>
  <summary>Details</summary>
Motivation: 全球绿氢投资增加，但沙漠环境下大型氢设施缺乏历史维护和性能数据，难以进行准确风险评估和拍卖决策。

Method: 提出人工智能决策支持系统，利用公开气象数据开发预测性维护压力指数（MPI）。

Result: 该工具可进行时间基准测试，评估和验证性能声明，将时间风险智能纳入拍卖评估标准。

Conclusion: 该系统能加强监管前瞻性和运营决策能力，弥补历史运营基准缺失的问题。

Abstract: As green hydrogen emerges as a major component of global decarbonisation,
Oman has positioned itself strategically through national auctions and
international partnerships. Following two successful green hydrogen project
rounds, the country launched its third auction (R3) in the Duqm region. While
this area exhibits relative geospatial homogeneity, it is still vulnerable to
environmental fluctuations that pose inherent risks to productivity. Despite
growing global investment in green hydrogen, operational data remains scarce,
with major projects like Saudi Arabia's NEOM facility not expected to commence
production until 2026, and Oman's ACME Duqm project scheduled for 2028. This
absence of historical maintenance and performance data from large-scale
hydrogen facilities in desert environments creates a major knowledge gap for
accurate risk assessment for infrastructure planning and auction decisions.
Given this data void, environmental conditions emerge as accessible and
reliable proxy for predicting infrastructure maintenance pressures, because
harsh desert conditions such as dust storms, extreme temperatures, and humidity
fluctuations are well-documented drivers of equipment degradation in renewable
energy systems. To address this challenge, this paper proposes an Artificial
Intelligence decision support system that leverages publicly available
meteorological data to develop a predictive Maintenance Pressure Index (MPI),
which predicts risk levels and future maintenance demands on hydrogen
infrastructure. This tool strengthens regulatory foresight and operational
decision-making by enabling temporal benchmarking to assess and validate
performance claims over time. It can be used to incorporate temporal risk
intelligence into auction evaluation criteria despite the absence of historical
operational benchmarks.

</details>


### [123] [A Scalable and High Availability Solution for Recommending Resolutions to Problem Tickets](https://arxiv.org/abs/2507.19846)
*Harish S,Chetana K Nayak,Joy Bose*

Main category: cs.LG

TL;DR: 本文提出用机器学习解决问题工单处理难题，采用聚类、监督学习和高级NLP模型，实验显示预测准确率高。


<details>
  <summary>Details</summary>
Motivation: 服务行业解决问题工单时，机器学习虽有用，但会受数据漂移、数据缺失等现象影响，需有效解决方案。

Method: 采用聚类、监督学习和高级NLP模型，包括基于聚类的解决方案识别、LDA监督分类、Siamese网络、One - shot学习、索引嵌入，还展示实时仪表盘和基于Kubernetes的生产部署。

Result: 使用开源Bitext客户支持数据集和专有电信数据集进行实验，显示出高预测准确率。

Conclusion: 提出的基于机器学习的解决方案能有效应对问题工单处理中的各种挑战，具有较高准确性。

Abstract: Resolution of incidents or problem tickets is a common theme in service
industries in any sector, including billing and charging systems in telecom
domain. Machine learning can help to identify patterns and suggest resolutions
for the problem tickets, based on patterns in the historical data of the
tickets. However, this process may be complicated due to a variety of phenomena
such as data drift and issues such as missing data, lack of data pertaining to
resolutions of past incidents, too many similar sounding resolutions due to
free text and similar sounding text. This paper proposes a robust ML-driven
solution employing clustering, supervised learning, and advanced NLP models to
tackle these challenges effectively. Building on previous work, we demonstrate
clustering-based resolution identification, supervised classification with LDA,
Siamese networks, and One-shot learning, Index embedding. Additionally, we
present a real-time dashboard and a highly available Kubernetes-based
production deployment. Our experiments with both the open-source Bitext
customer-support dataset and proprietary telecom datasets demonstrate high
prediction accuracy.

</details>


### [124] [Swift-Sarsa: Fast and Robust Linear Control](https://arxiv.org/abs/2507.19539)
*Khurram Javed,Richard S. Sutton*

Main category: cs.LG

TL;DR: 本文在SwiftTD基础上扩展出Swift - Sarsa算法用于控制问题，提出操作条件反射基准，Swift - Sarsa在该基准上表现良好。


<details>
  <summary>Details</summary>
Motivation: 将用于TD学习的SwiftTD算法扩展到控制问题，解决操作条件反射基准中区分相关信号和噪声信号的挑战。

Method: 结合SwiftTD与True Online Sarsa(λ)开发Swift - Sarsa算法，提出操作条件反射基准。

Result: Swift - Sarsa在操作条件反射基准上能学习为相关信号分配信用，可并行搜索大量特征且不受噪声特征影响。

Conclusion: Swift - Sarsa为在大量特征中学习表示的解决方法打开了大门。

Abstract: Javed, Sharifnassab, and Sutton (2024) introduced a new algorithm for TD
learning -- SwiftTD -- that augments True Online TD($\lambda$) with step-size
optimization, a bound on the effective learning rate, and step-size decay. In
their experiments SwiftTD outperformed True Online TD($\lambda$) and
TD($\lambda$) on a variety of prediction tasks derived from Atari games, and
its performance was robust to the choice of hyper-parameters. In this extended
abstract we extend SwiftTD to work for control problems. We combine the key
ideas behind SwiftTD with True Online Sarsa($\lambda$) to develop an on-policy
reinforcement learning algorithm called $\textit{Swift-Sarsa}$.
  We propose a simple benchmark for linear on-policy control called the
$\textit{operant conditioning benchmark}$. The key challenge in the operant
conditioning benchmark is that a very small subset of input signals are
relevant for decision making. The majority of the signals are noise sampled
from a non-stationary distribution. To learn effectively, the agent must learn
to differentiate between the relevant signals and the noisy signals, and
minimize prediction errors by assigning credit to the weight parameters
associated with the relevant signals.
  Swift-Sarsa, when applied to the operant conditioning benchmark, learned to
assign credit to the relevant signals without any prior knowledge of the
structure of the problem. It opens the door for solution methods that learn
representations by searching over hundreds of millions of features in parallel
without performance degradation due to noisy or bad features.

</details>


### [125] [Clinical-Grade Blood Pressure Prediction in ICU Settings: An Ensemble Framework with Uncertainty Quantification and Cross-Institutional Validation](https://arxiv.org/abs/2507.19530)
*Md Basit Azam,Sarangthem Ibotombi Singh*

Main category: cs.LG

TL;DR: 本文提出基于电子健康记录的血压预测综合框架，经内部和外部验证，能实现不确定性量化，为跨机构AI辅助血压监测提供部署预期。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习方法在血压预测上存在缺乏外部验证、不确定性量化和数据泄漏预防不足的问题，需要改进。

Method: 实施系统的数据泄漏预防，通过分位数回归进行不确定性量化，在MIMIC - III和eICU数据库间进行外部验证，使用集成框架结合多种算法和74个特征。

Result: 内部验证性能达临床可接受水平，外部验证性能下降30%，不确定性量化生成有效预测区间。

Conclusion: 该框架为重症监护中跨机构AI辅助血压监测提供现实的部署预期，代码开源。

Abstract: Blood pressure (BP) monitoring is critical in in tensive care units (ICUs)
where hemodynamic instability can
  rapidly progress to cardiovascular collapse. Current machine
  learning (ML) approaches suffer from three limitations: lack of
  external validation, absence of uncertainty quantification, and
  inadequate data leakage prevention. This study presents the
  first comprehensive framework with novel algorithmic leakage
  prevention, uncertainty quantification, and cross-institutional
  validation for electronic health records (EHRs) based BP pre dictions. Our
methodology implemented systematic data leakage
  prevention, uncertainty quantification through quantile regres sion, and
external validation between the MIMIC-III and eICU
  databases. An ensemble framework combines Gradient Boosting,
  Random Forest, and XGBoost with 74 features across five
  physiological domains. Internal validation achieved a clinically
  acceptable performance (for SBP: R^2 = 0.86, RMSE = 6.03
  mmHg; DBP: R^2 = 0.49, RMSE = 7.13 mmHg), meeting AAMI
  standards. External validation showed 30% degradation with
  critical limitations in patients with hypotensive. Uncertainty
  quantification generated valid prediction intervals (80.3% SBP
  and 79.9% DBP coverage), enabling risk-stratified protocols
  with narrow intervals (< 15 mmHg) for standard monitoring
  and wide intervals (> 30 mmHg) for manual verification. This
  framework provides realistic deployment expectations for cross institutional
AI-assisted BP monitoring in critical care settings.
  The source code is publicly available at https://github.com/
  mdbasit897/clinical-bp-prediction-ehr.

</details>


### [126] [$K^4$: Online Log Anomaly Detection Via Unsupervised Typicality Learning](https://arxiv.org/abs/2507.20051)
*Weicong Chen,Vikash Singh,Zahra Rahmani,Debargha Ganguly,Mohsen Hariri,Vipin Chaudhary*

Main category: cs.LG

TL;DR: 介绍无监督、独立于解析器的高性能在线检测框架$K^4$，性能超基线且速度快。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法慢、依赖易出错解析且评估协议不现实。

Method: 利用高效k近邻统计将任意日志嵌入转换为四维描述符，使轻量级检测器准确评分异常。

Result: 在更现实的在线评估协议下，AUROC达0.995 - 0.999，远超基线，训练不到4秒，推理低至4微秒。

Conclusion: $K^4$是一种高效的日志异常检测框架，能实现高性能在线检测。

Abstract: Existing Log Anomaly Detection (LogAD) methods are often slow, dependent on
error-prone parsing, and use unrealistic evaluation protocols. We introduce
$K^4$, an unsupervised and parser-independent framework for high-performance
online detection. $K^4$ transforms arbitrary log embeddings into compact
four-dimensional descriptors (Precision, Recall, Density, Coverage) using
efficient k-nearest neighbor (k-NN) statistics. These descriptors enable
lightweight detectors to accurately score anomalies without retraining. Using a
more realistic online evaluation protocol, $K^4$ sets a new state-of-the-art
(AUROC: 0.995-0.999), outperforming baselines by large margins while being
orders of magnitude faster, with training under 4 seconds and inference as low
as 4 $\mu$s.

</details>


### [127] [FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated Learning Settings](https://arxiv.org/abs/2507.19534)
*Ali Shakeri,Wei Emma Zhang,Amin Beheshti,Weitong Chen,Jian Yang,Lishan Yang*

Main category: cs.LG

TL;DR: 本文提出Federated Dynamic Prompt Generator (FedDPG) 方法，在联邦学习中生成动态提示，实验显示其在性能、计算时间和参数量上有优势。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法计算开销大，提示调整技术提示固定缺乏灵活性，联邦学习存在通信和计算限制等问题，需要改进。

Method: 引入Federated Dynamic Prompt Generator (FedDPG)，包含动态提示生成网络，根据输入生成上下文感知提示。

Result: 在三个NLP基准数据集实验中，FedDPG在全局模型性能上超过现有高效参数微调方法，显著减少计算时间和通过联邦学习网络传输的参数数量。

Conclusion: FedDPG在保证数据隐私的同时，提高了模型灵活性和适应性，在性能和效率上表现出色。

Abstract: Pre-trained Language Models (PLMs) have demonstrated impressive performance
in various NLP tasks. However, traditional fine-tuning methods for leveraging
PLMs for downstream tasks entail significant computational overhead.
Prompt-tuning has emerged as an efficient alternative that involves prepending
a limited number of parameters to the input sequence and only updating them
while the PLM's parameters are frozen. However, this technique's prompts remain
fixed for all inputs, reducing the model's flexibility. The Federated Learning
(FL) technique has gained attention in recent years to address the growing
concerns around data privacy. However, challenges such as communication and
computation limitations of clients still need to be addressed. To mitigate
these challenges, this paper introduces the Federated Dynamic Prompt Generator
(FedDPG), which incorporates a dynamic prompt generator network to generate
context-aware prompts based on the given input, ensuring flexibility and
adaptability while prioritising data privacy in federated learning settings.
Our experiments on three NLP benchmark datasets showcase that FedDPG
outperforms the state-of-the-art parameter-efficient fine-tuning methods in
terms of global model performance, and has significantly reduced the
calculation time and the number of parameters to be sent through the FL
network.

</details>


### [128] [Federated Calculation of the Free-Support Transportation Barycenter by Single-Loop Dual Decomposition](https://arxiv.org/abs/2507.19627)
*Zhengqi Lin,Andrzej Ruszczyński*

Main category: cs.LG

TL;DR: 提出高效联邦双分解算法计算Wasserstein重心，有低复杂度和可扩展性，并与现有方法对比。


<details>
  <summary>Details</summary>
Motivation: 寻找高效计算多个分布Wasserstein重心的方法。

Method: 提出不访问本地数据、仅用高度聚合信息且无需重复求解大规模运输问题的联邦双分解算法。

Result: 算法每次迭代复杂度低、可扩展性强。

Conclusion: 该算法具有优势，通过混合模型示例与现有方法对比得到体现。

Abstract: We propose an efficient federated dual decomposition algorithm for
calculating the Wasserstein barycenter of several distributions, including
choosing the support of the solution. The algorithm does not access local data
and uses only highly aggregated information. It also does not require repeated
solutions to mass transportation problems. Because of the absence of any
matrix-vector operations, the algorithm exhibits a very low complexity of each
iteration and significant scalability. We illustrate its virtues and compare it
to the state-of-the-art methods on several examples of mixture models.

</details>


### [129] [Graph Learning Metallic Glass Discovery from Wikipedia](https://arxiv.org/abs/2507.19536)
*K. -C. Ouyang,S. -Y. Zhang,S. -L. Liu,J. Tian,Y. -H. Li,H. Tong,H. -Y. Bai,W. -H. Wang,Y. -C. Hu*

Main category: cs.LG

TL;DR: 提出从材料网络表征进行数据学习以探索新材料，评估自然语言在材料设计中的能力，提出用人工智能收获新非晶材料的新范式。


<details>
  <summary>Details</summary>
Motivation: 传统合成新材料效率低、成本高，数据驱动方法受数据稀缺和材料编码不成熟影响，模型预测和泛化能力有限。

Method: 从维基百科用语言模型编码节点元素，设计多种架构的图神经网络作为推荐系统，利用不同语言的维基百科嵌入评估自然语言能力。

Result: 文中未明确提及具体结果。

Conclusion: 提出用人工智能收获新非晶材料及其他材料的新范式。

Abstract: Synthesizing new materials efficiently is highly demanded in various research
fields. However, this process is usually slow and expensive, especially for
metallic glasses, whose formation strongly depends on the optimal combinations
of multiple elements to resist crystallization. This constraint renders only
several thousands of candidates explored in the vast material space since 1960.
Recently, data-driven approaches armed by advanced machine learning techniques
provided alternative routes for intelligent materials design. Due to data
scarcity and immature material encoding, the conventional tabular data is
usually mined by statistical learning algorithms, giving limited model
predictability and generalizability. Here, we propose sophisticated data
learning from material network representations. The node elements are encoded
from the Wikipedia by a language model. Graph neural networks with versatile
architectures are designed to serve as recommendation systems to explore hidden
relationships among materials. By employing Wikipedia embeddings from different
languages, we assess the capability of natural languages in materials design.
Our study proposes a new paradigm to harvesting new amorphous materials and
beyond with artificial intelligence.

</details>


### [130] [Communication-Efficient Distributed Training for Collaborative Flat Optima Recovery in Deep Learning](https://arxiv.org/abs/2507.20424)
*Tolga Dimlioglu,Anna Choromanska*

Main category: cs.LG

TL;DR: 研究集中式分布式数据并行训练DNNs，提出DPPF算法，在减少通信开销同时提升泛化性能，有理论保证。


<details>
  <summary>Details</summary>
Motivation: 改善局部梯度方法在通信效率和模型性能间的权衡。

Method: 引入Inverse Mean Valley作为锐度度量，将其松弛形式作为正则化器加入分布式训练目标，提出DPPF算法。

Result: DPPF优于其他通信高效方法，减少通信开销，泛化性能好，能定位更平坦极小值。

Conclusion: DPPF能引导工作节点跨越平坦山谷，推拉动态自稳定，有泛化保证和非凸收敛性。

Abstract: We study centralized distributed data parallel training of deep neural
networks (DNNs), aiming to improve the trade-off between communication
efficiency and model performance of the local gradient methods. To this end, we
revisit the flat-minima hypothesis, which suggests that models with better
generalization tend to lie in flatter regions of the loss landscape. We
introduce a simple, yet effective, sharpness measure, Inverse Mean Valley, and
demonstrate its strong correlation with the generalization gap of DNNs. We
incorporate an efficient relaxation of this measure into the distributed
training objective as a lightweight regularizer that encourages workers to
collaboratively seek wide minima. The regularizer exerts a pushing force that
counteracts the consensus step pulling the workers together, giving rise to the
Distributed Pull-Push Force (DPPF) algorithm. Empirically, we show that DPPF
outperforms other communication-efficient approaches and achieves better
generalization performance than local gradient methods and synchronous gradient
averaging, while significantly reducing communication overhead. In addition,
our loss landscape visualizations confirm the ability of DPPF to locate flatter
minima. On the theoretical side, we show that DPPF guides workers to span flat
valleys, with the final valley width governed by the interplay between push and
pull strengths, and that its pull-push dynamics is self-stabilizing. We further
provide generalization guarantees linked to the valley width and prove
convergence in the non-convex setting.

</details>


### [131] [Feature learning is decoupled from generalization in high capacity neural networks](https://arxiv.org/abs/2507.19680)
*Niclas Alexander Göring,Charles London,Abdurrahman Hadi Erturk,Chris Mingard,Yoonsoo Nam,Ard A. Louis*

Main category: cs.LG

TL;DR: 神经网络在某些函数上优于核方法，引入特征质量概念，指出当前特征学习理论不足。


<details>
  <summary>Details</summary>
Motivation: 解释神经网络优于核方法的原因，并为神经网络泛化理论奠定基础。

Method: 引入特征质量概念，检验现有特征学习理论。

Result: 现有理论主要评估特征学习强度，而非特征质量。

Conclusion: 当前特征学习理论不足以支撑神经网络泛化理论发展。

Abstract: Neural networks outperform kernel methods, sometimes by orders of magnitude,
e.g. on staircase functions. This advantage stems from the ability of neural
networks to learn features, adapting their hidden representations to better
capture the data. We introduce a concept we call feature quality to measure
this performance improvement. We examine existing theories of feature learning
and demonstrate empirically that they primarily assess the strength of feature
learning, rather than the quality of the learned features themselves.
Consequently, current theories of feature learning do not provide a sufficient
foundation for developing theories of neural network generalization.

</details>


### [132] [Latent Representations of Intracardiac Electrograms for Atrial Fibrillation Driver Detection](https://arxiv.org/abs/2507.19547)
*Pablo Peiro-Corbacho,Long Lin,Pablo Ávila,Alejandro Carta-Bergaz,Ángel Arenal,Carlos Sevilla-Salcedo,Gonzalo R. Ríos-Muñoz*

Main category: cs.LG

TL;DR: 本文提出用卷积自动编码器的深度学习框架从心腔内电图中无监督提取特征，以检测房颤驱动因素，方法实时可用，凸显无监督学习潜力。


<details>
  <summary>Details</summary>
Motivation: 当前房颤消融疗法对持续性房颤常无效，因涉及非肺静脉驱动因素，需新方法检测房颤驱动因素。

Method: 提出用卷积自动编码器的深度学习框架，从单极和双极心腔内电图无监督提取特征，以表征和自动化电图分析。

Result: 自动编码器成功学习到低重构损失的潜在表征，提取的嵌入使下游分类器检测旋转和局灶活动有中等性能（AUC 0.73 - 0.76），识别心房电图纠缠有高判别性能（AUC 0.93）。

Conclusion: 所提方法可实时运行，能集成到临床电解剖标测系统，凸显无监督学习从心内信号发掘生理有意义特征的潜力。

Abstract: Atrial Fibrillation (AF) is the most prevalent sustained arrhythmia, yet
current ablation therapies, including pulmonary vein isolation, are frequently
ineffective in persistent AF due to the involvement of non-pulmonary vein
drivers. This study proposes a deep learning framework using convolutional
autoencoders for unsupervised feature extraction from unipolar and bipolar
intracavitary electrograms (EGMs) recorded during AF in ablation studies. These
latent representations of atrial electrical activity enable the
characterization and automation of EGM analysis, facilitating the detection of
AF drivers.
  The database consisted of 11,404 acquisitions recorded from 291 patients,
containing 228,080 unipolar EGMs and 171,060 bipolar EGMs. The autoencoders
successfully learned latent representations with low reconstruction loss,
preserving the morphological features. The extracted embeddings allowed
downstream classifiers to detect rotational and focal activity with moderate
performance (AUC 0.73-0.76) and achieved high discriminative performance in
identifying atrial EGM entanglement (AUC 0.93).
  The proposed method can operate in real-time and enables integration into
clinical electroanatomical mapping systems to assist in identifying
arrhythmogenic regions during ablation procedures. This work highlights the
potential of unsupervised learning to uncover physiologically meaningful
features from intracardiac signals.

</details>


### [133] [Modeling User Behavior from Adaptive Surveys with Supplemental Context](https://arxiv.org/abs/2507.20919)
*Aman Shukla,Daniel Patrick Scantlebury,Rishabh Kumar*

Main category: cs.LG

TL;DR: 提出LANTERN架构融合调查响应与上下文信号建模用户行为，表现优于仅用调查数据的基线模型，为以调查为中心的应用提供蓝图。


<details>
  <summary>Details</summary>
Motivation: 传统调查方法在收集用户行为数据时有局限性，如用户疲劳、响应不完整等，不足以捕捉用户行为。

Method: 提出LANTERN模块化架构，通过选择性门控、残差连接和跨注意力的后期融合，以调查数据为主信号，融合补充上下文信号。

Result: LANTERN在调查响应的多标签预测中优于仅用调查数据的强基线模型，还通过消融和属性分析研究了阈值敏感性和选择性模态依赖的好处。

Conclusion: 该架构具有模块化特点，支持新编码器和数据集的可扩展集成，为以调查为中心的行为建模应用提供了实用且可扩展的蓝图。

Abstract: Modeling user behavior is critical across many industries where understanding
preferences, intent, or decisions informs personalization, targeting, and
strategic outcomes. Surveys have long served as a classical mechanism for
collecting such behavioral data due to their interpretability, structure, and
ease of deployment. However, surveys alone are inherently limited by user
fatigue, incomplete responses, and practical constraints on their length making
them insufficient for capturing user behavior. In this work, we present LANTERN
(Late-Attentive Network for Enriched Response Modeling), a modular architecture
for modeling user behavior by fusing adaptive survey responses with
supplemental contextual signals. We demonstrate the architectural value of
maintaining survey primacy through selective gating, residual connections and
late fusion via cross-attention, treating survey data as the primary signal
while incorporating external modalities only when relevant. LANTERN outperforms
strong survey-only baselines in multi-label prediction of survey responses. We
further investigate threshold sensitivity and the benefits of selective
modality reliance through ablation and rare/frequent attribute analysis.
LANTERN's modularity supports scalable integration of new encoders and evolving
datasets. This work provides a practical and extensible blueprint for behavior
modeling in survey-centric applications.

</details>


### [134] [Harnessing intuitive local evolution rules for physical learning](https://arxiv.org/abs/2507.19561)
*Roie Ezraty,Menachem Stern,Shmuel M. Rubinstein*

Main category: cs.LG

TL;DR: 介绍一种用于物理系统的训练方案BEASTAL，可减少功耗，通过本地规则学习，能执行线性任务，非线性本地规则时性能最佳。


<details>
  <summary>Details</summary>
Motivation: 机器学习计算密集且高耗能，促使寻找学习任务的替代物理实现方式。

Method: 引入仅外部控制边界参数以最小化功耗的训练方案，利用本地物理规则让BEASTS学习，BEASTAL是类似Adaline算法的方案。

Result: 在计算机上展示该方案在回归和分类任务中的自主学习能力。

Conclusion: 该方法改进了之前的物理学习方案，使用直观本地规则，无需大规模内存和复杂内部架构，能执行线性任务，非线性规则时表现最佳。

Abstract: Machine Learning, however popular and accessible, is computationally
intensive and highly power-consuming, prompting interest in alternative
physical implementations of learning tasks. We introduce a training scheme for
physical systems that minimize power dissipation in which only boundary
parameters (i.e. inputs and outputs) are externally controlled. Using this
scheme, these Boundary-Enabled Adaptive State Tuning Systems (BEASTS) learn by
exploiting local physical rules. Our scheme, BEASTAL (BEAST-Adaline), is the
closest analog of the Adaline algorithm for such systems. We demonstrate this
autonomous learning in silico for regression and classification tasks. Our
approach advances previous physical learning schemes by using intuitive, local
evolution rules without requiring large-scale memory or complex internal
architectures. BEASTAL can perform any linear task, achieving best performance
when the local evolution rule is non-linear.

</details>


### [135] [RestoreAI -- Pattern-based Risk Estimation Of Remaining Explosives](https://arxiv.org/abs/2507.19873)
*Björn Kischelewski,Benjamin Guedj,David Wahl*

Main category: cs.LG

TL;DR: 本文介绍RestoreAI系统，利用地雷模式预测风险，评估显示其显著提高排雷效率。


<details>
  <summary>Details</summary>
Motivation: 现有AI排雷方法多关注目标识别，对基于空间模式信息预测地雷风险关注有限，旨在用AI通过地雷模式预测风险以提高排雷效率。

Method: 引入RestoreAI系统，实现线性、曲线和贝叶斯三种模式排雷器，分别利用PCA线性模式、主曲线的曲线模式和贝叶斯模式进行风险预测。

Result: RestoreAI显著提高排雷效率，表现最佳的模式排雷器每步清除地雷平均比例提高14.37个百分点，定位所有地雷所需时间比最佳基线排雷器少24.45%。线性和曲线排雷器性能无显著差异。

Conclusion: RestoreAI系统有效，更高效的线性模式可用于风险预测。

Abstract: Landmine removal is a slow, resource-intensive process affecting over 60
countries. While AI has been proposed to enhance explosive ordnance (EO)
detection, existing methods primarily focus on object recognition, with limited
attention to prediction of landmine risk based on spatial pattern information.
This work aims to answer the following research question: How can AI be used to
predict landmine risk from landmine patterns to improve clearance time
efficiency? To that effect, we introduce RestoreAI, an AI system for
pattern-based risk estimation of remaining explosives. RestoreAI is the first
AI system that leverages landmine patterns for risk prediction, improving the
accuracy of estimating the residual risk of missing EO prior to land release.
We particularly focus on the implementation of three instances of RestoreAI,
respectively, linear, curved and Bayesian pattern deminers. First, the linear
pattern deminer uses linear landmine patterns from a principal component
analysis (PCA) for the landmine risk prediction. Second, the curved pattern
deminer uses curved landmine patterns from principal curves. Finally, the
Bayesian pattern deminer incorporates prior expert knowledge by using a
Bayesian pattern risk prediction. Evaluated on real-world landmine data,
RestoreAI significantly boosts clearance efficiency. The top-performing
pattern-based deminers achieved a 14.37 percentage point increase in the
average share of cleared landmines per timestep and required 24.45% less time
than the best baseline deminer to locate all landmines. Interestingly, linear
and curved pattern deminers showed no significant performance difference,
suggesting that more efficient linear patterns are a viable option for risk
prediction.

</details>


### [136] [Dimer-Enhanced Optimization: A First-Order Approach to Escaping Saddle Points in Neural Network Training](https://arxiv.org/abs/2507.19968)
*Yue Hu,Zanxia Cao,Yingchao Liu*

Main category: cs.LG

TL;DR: 提出Dimer - Enhanced Optimization (DEO)框架用于神经网络训练中逃离鞍点，初步实验显示其有竞争力。


<details>
  <summary>Details</summary>
Motivation: 一阶优化方法在复杂损失景观中导航困难，二阶方法计算量大，需新方法解决问题。

Method: 将Dimer方法应用于神经网络训练，通过投影梯度到与最小曲率方向正交的子空间引导优化器。

Result: 在Transformer玩具模型的初步实验中，DEO与标准一阶方法相比表现有竞争力，能更好应对复杂损失景观。

Conclusion: 将受物理启发的一阶曲率估计用于高维空间神经网络训练是有效的。

Abstract: First-order optimization methods, such as SGD and Adam, are widely used for
training large-scale deep neural networks due to their computational efficiency
and robust performance. However, relying solely on gradient information, these
methods often struggle to navigate complex loss landscapes with flat regions,
plateaus, and saddle points. Second-order methods, which use curvature
information from the Hessian matrix, can address these challenges but are
computationally infeasible for large models. The Dimer method, a first-order
technique that constructs two closely spaced points to probe the local geometry
of a potential energy surface, efficiently estimates curvature using only
gradient information. Inspired by its use in molecular dynamics simulations for
locating saddle points, we propose Dimer-Enhanced Optimization (DEO), a novel
framework to escape saddle points in neural network training. DEO adapts the
Dimer method to explore a broader region of the loss landscape, approximating
the Hessian's smallest eigenvector without computing the full matrix. By
periodically projecting the gradient onto the subspace orthogonal to the
minimum curvature direction, DEO guides the optimizer away from saddle points
and flat regions, enhancing training efficiency with non-stepwise updates.
Preliminary experiments on a Transformer toy model show DEO achieves
competitive performance compared to standard first-order methods, improving
navigation of complex loss landscapes. Our work repurposes physics-inspired,
first-order curvature estimation to enhance neural network training in
high-dimensional spaces.

</details>


### [137] [Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks](https://arxiv.org/abs/2507.19684)
*Bermet Burkanova,Payam Jome Yazdian,Chuxuan Zhang,Trinity Evans,Paige Tuttösí,Angelica Lim*

Main category: cs.LG

TL;DR: 提出最大且最多样的即兴萨尔萨舞动作捕捉数据集CoMPAS3D，用于交互式、富有表现力的人形AI测试，还发布相关资源及模型。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在文本或语音交互方面出色，但人类交流远超文本，建模两个主体间的耦合交互存在挑战，需要推动社交互动具身AI和创意人形动作生成研究。

Method: 创建CoMPAS3D数据集，包含不同技能水平舞者的萨尔萨舞，提供精细专家标注，在两个基准任务上评估，并发布数据集、标注、代码和模型。

Result: 得到了CoMPAS3D数据集及相关标注，模型能完成基准任务。

Conclusion: 释放数据集、标注、代码和模型，鼓励社交互动具身AI和创意人形动作生成研究。

Abstract: Imagine a humanoid that can safely and creatively dance with a human,
adapting to its partner's proficiency, using haptic signaling as a primary form
of communication. While today's AI systems excel at text or voice-based
interaction with large language models, human communication extends far beyond
text-it includes embodied movement, timing, and physical coordination. Modeling
coupled interaction between two agents poses a formidable challenge: it is
continuous, bidirectionally reactive, and shaped by individual variation. We
present CoMPAS3D, the largest and most diverse motion capture dataset of
improvised salsa dancing, designed as a challenging testbed for interactive,
expressive humanoid AI. The dataset includes 3 hours of leader-follower salsa
dances performed by 18 dancers spanning beginner, intermediate, and
professional skill levels. For the first time, we provide fine-grained salsa
expert annotations, covering over 2,800 move segments, including move types,
combinations, execution errors and stylistic elements. We draw analogies
between partner dance communication and natural language, evaluating CoMPAS3D
on two benchmark tasks for synthetic humans that parallel key problems in
spoken language and dialogue processing: leader or follower generation with
proficiency levels (speaker or listener synthesis), and duet (conversation)
generation. Towards a long-term goal of partner dance with humans, we release
the dataset, annotations, and code, along with a multitask SalsaAgent model
capable of performing all benchmark tasks, alongside additional baselines to
encourage research in socially interactive embodied AI and creative, expressive
humanoid motion generation.

</details>


### [138] [KD-GAT: Combining Knowledge Distillation and Graph Attention Transformer for a Controller Area Network Intrusion Detection System](https://arxiv.org/abs/2507.19686)
*Robert Frenken,Sidra Ghayour Bhatti,Hanqin Zhang,Qadeer Ahmed*

Main category: cs.LG

TL;DR: 本文提出KD - GAT入侵检测框架，结合图注意力网络与知识蒸馏，在降低计算复杂度同时提高检测准确率，实验在部分数据集效果好，但在存在类别不平衡的数据集表现不佳。


<details>
  <summary>Details</summary>
Motivation: CAN协议缺乏安全机制，易受网络攻击，需提高入侵检测准确性并降低计算复杂度。

Method: 用滑动窗口将CAN流量表示为图，多层GAT作为教师模型，紧凑的学生GAT通过监督预训练和软硬标签监督的知识蒸馏两阶段过程训练。

Result: 教师和学生模型在Car - Hacking、Car - Survival数据集表现好，学生模型准确率分别达99.97%和99.31%，但在can - train - and - test数据集因类别不平衡表现不佳。

Conclusion: KD - GAT框架有一定效果，但解决数据集类别不平衡问题是未来重要工作方向。

Abstract: The Controller Area Network (CAN) protocol is widely adopted for in-vehicle
communication but lacks inherent security mechanisms, making it vulnerable to
cyberattacks. This paper introduces KD-GAT, an intrusion detection framework
that combines Graph Attention Networks (GATs) with knowledge distillation (KD)
to enhance detection accuracy while reducing computational complexity. In our
approach, CAN traffic is represented as graphs using a sliding window to
capture temporal and relational patterns. A multi-layer GAT with jumping
knowledge aggregation acting as the teacher model, while a compact student
GAT--only 6.32% the size of the teacher--is trained via a two-phase process
involving supervised pretraining and knowledge distillation with both soft and
hard label supervision. Experiments on three benchmark datasets--Car-Hacking,
Car-Survival, and can-train-and-test demonstrate that both teacher and student
models achieve strong results, with the student model attaining 99.97% and
99.31% accuracy on Car-Hacking and Car-Survival, respectively. However,
significant class imbalance in can-train-and-test has led to reduced
performance for both models on this dataset. Addressing this imbalance remains
an important direction for future work.

</details>


### [139] [Irredundant $k$-Fold Cross-Validation](https://arxiv.org/abs/2507.20048)
*Jesus S. Aguilar-Ruiz*

Main category: cs.LG

TL;DR: 提出Irredundant k-fold cross - validation方法，能更平衡利用数据集，减少过拟合，降低计算成本，实验显示有一致性能估计和更低方差估计。


<details>
  <summary>Details</summary>
Motivation: 传统k折交叉验证存在数据冗余，部分实例对学习阶段影响过大。

Method: 引入Irredundant k - fold cross - validation方法，保证每个实例在整个验证过程中仅用于训练和测试各一次。

Result: 该方法在不同数据集上有一致性能估计，方差估计更保守，能显著降低计算成本。

Conclusion: Irredundant k - fold cross - validation方法能更平衡利用数据，减少过拟合，且在性能估计、方差估计和计算成本上有优势。

Abstract: In traditional k-fold cross-validation, each instance is used ($k-1$) times
for training and once for testing, leading to redundancy that lets many
instances disproportionately influence the learning phase. We introduce
Irredundant $k$-fold cross-validation, a novel method that guarantees each
instance is used exactly once for training and once for testing across the
entire validation procedure. This approach ensures a more balanced utilization
of the dataset, mitigates overfitting due to instance repetition, and enables
sharper distinctions in comparative model analysis. The method preserves
stratification and remains model-agnostic, i.e., compatible with any
classifier. Experimental results demonstrate that it delivers consistent
performance estimates across diverse datasets -- comparable to $k$-fold
cross-validation -- while providing less optimistic variance estimates because
training partitions are non-overlapping, and significantly reducing the overall
computational cost.

</details>


### [140] [NAICS-Aware Graph Neural Networks for Large-Scale POI Co-visitation Prediction: A Multi-Modal Dataset and Methodology](https://arxiv.org/abs/2507.19697)
*Yazeed Alrubyli,Omar Alomeir,Abrar Wafa,Diána Hidvégi,Hend Alrasheed,Mohsen Bahrami*

Main category: cs.LG

TL;DR: 提出NAICS - aware GraphSAGE预测人口规模的共同访问模式，在大规模数据集上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法因数据稀疏和空间与业务关系复杂，难以预测数百万场所的共同访问模式，传统仅用地理距离的方法有局限。

Method: 引入NAICS - aware GraphSAGE，通过可学习嵌入整合业务分类知识，结合空间、时间和社会经济特征，采用高效的按州分解方法。

Result: 在POI - Graph数据集上，R平方值从0.243提升到0.625，排名质量（NDCG@10）提升32%。

Conclusion: 所提出的方法能有效预测人口规模的共同访问模式，优于现有基线。

Abstract: Understanding where people go after visiting one business is crucial for
urban planning, retail analytics, and location-based services. However,
predicting these co-visitation patterns across millions of venues remains
challenging due to extreme data sparsity and the complex interplay between
spatial proximity and business relationships. Traditional approaches using only
geographic distance fail to capture why coffee shops attract different customer
flows than fine dining restaurants, even when co-located. We introduce
NAICS-aware GraphSAGE, a novel graph neural network that integrates business
taxonomy knowledge through learnable embeddings to predict population-scale
co-visitation patterns. Our key insight is that business semantics, captured
through detailed industry codes, provide crucial signals that pure spatial
models cannot explain. The approach scales to massive datasets (4.2 billion
potential venue pairs) through efficient state-wise decomposition while
combining spatial, temporal, and socioeconomic features in an end-to-end
framework. Evaluated on our POI-Graph dataset comprising 94.9 million
co-visitation records across 92,486 brands and 48 US states, our method
achieves significant improvements over state-of-the-art baselines: the
R-squared value increases from 0.243 to 0.625 (a 157 percent improvement), with
strong gains in ranking quality (32 percent improvement in NDCG at 10).

</details>


### [141] [PERRY: Policy Evaluation with Confidence Intervals using Auxiliary Data](https://arxiv.org/abs/2507.20068)
*Aishwarya Mandyam,Jason Meng,Ge Gao,Jiankai Sun,Mac Schwager,Barbara E. Engelhardt,Emma Brunskill*

Main category: cs.LG

TL;DR: 本文提出两种数据增强时OPE构建有效置信区间的方法，在多领域实验中表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习OPE的数据增强方法缺乏原则性的不确定性量化，在高风险场景中可靠的不确定性估计很重要。

Method: 提出两种构建有效置信区间的方法，一是引入新的共形预测方法处理特定初始状态下的策略性能；二是结合双重稳健估计和预测驱动推理估计多初始状态下的平均策略性能。

Result: 在机器人、医疗保健、库存管理模拟器及真实医疗数据集上，方法能利用增强数据并持续生成覆盖真实值的区间，优于先前方法。

Conclusion: 所提方法能有效解决数据增强时OPE的不确定性量化问题。

Abstract: Off-policy evaluation (OPE) methods aim to estimate the value of a new
reinforcement learning (RL) policy prior to deployment. Recent advances have
shown that leveraging auxiliary datasets, such as those synthesized by
generative models, can improve the accuracy of these value estimates.
Unfortunately, such auxiliary datasets may also be biased, and existing methods
for using data augmentation for OPE in RL lack principled uncertainty
quantification. In high stakes settings like healthcare, reliable uncertainty
estimates are important for comparing policy value estimates. In this work, we
propose two approaches to construct valid confidence intervals for OPE when
using data augmentation. The first provides a confidence interval over the
policy performance conditioned on a particular initial state $V^{\pi}(s_0)$--
such intervals are particularly important for human-centered applications. To
do so we introduce a new conformal prediction method for high dimensional state
MDPs. Second, we consider the more common task of estimating the average policy
performance over many initial states; to do so we draw on ideas from doubly
robust estimation and prediction powered inference. Across simulators spanning
robotics, healthcare and inventory management, and a real healthcare dataset
from MIMIC-IV, we find that our methods can use augmented data and still
consistently produce intervals that cover the ground truth values, unlike
previously proposed methods.

</details>


### [142] [Disjoint Generative Models](https://arxiv.org/abs/2507.19700)
*Anton Danholt Lautrup,Muhammad Rajabinasab,Tobias Hyrup,Arthur Zimek,Peter Schneider-Kamp*

Main category: cs.LG

TL;DR: 提出用不相交生成模型生成横截面合成数据集的新框架，通过案例证明其成功，有提高隐私等好处。


<details>
  <summary>Details</summary>
Motivation: 为生成横截面合成数据集提供新方法并提高数据隐私性。

Method: 将数据集划分为不相交子集，分别输入生成模型，再通过无公共变量/标识符的连接操作组合结果。

Result: 通过表格数据的案例研究和示例证明框架成功。

Conclusion: 不相交生成模型能以低效用成本显著提高隐私，对某些模型类型更有效可行，还可能实现混合模型合成。

Abstract: We propose a new framework for generating cross-sectional synthetic datasets
via disjoint generative models. In this paradigm, a dataset is partitioned into
disjoint subsets that are supplied to separate instances of generative models.
The results are then combined post hoc by a joining operation that works in the
absence of common variables/identifiers. The success of the framework is
demonstrated through several case studies and examples on tabular data that
helps illuminate some of the design choices that one may make. The principal
benefit of disjoint generative models is significantly increased privacy at
only a low utility cost. Additional findings include increased effectiveness
and feasibility for certain model types and the possibility for mixed-model
synthesis.

</details>


### [143] [Beyond Nearest Neighbors: Semantic Compression and Graph-Augmented Retrieval for Enhanced Vector Search](https://arxiv.org/abs/2507.19715)
*Rahul Raja,Arpita Vats*

Main category: cs.LG

TL;DR: 本文提出语义压缩检索范式和图增强向量检索方法，为以意义为中心的向量搜索系统奠定基础，并开源实现。


<details>
  <summary>Details</summary>
Motivation: 传统近似最近邻搜索结果语义冗余，无法满足如检索增强生成等应用对多样性和上下文丰富度的需求。

Method: 提出语义压缩范式，用子模优化和信息几何原理形式化目标；提出图增强向量检索，在向量空间上叠加语义图实现多跳、上下文感知搜索。

Result: 理论分析了高维集中下基于邻近度检索的局限性，表明图结构可提高语义覆盖度。

Conclusion: 为以意义为中心的向量搜索系统奠定基础，强调混合索引、多样性感知查询和结构化语义检索，开源实现以促进相关研究。

Abstract: Vector databases typically rely on approximate nearest neighbor (ANN) search
to retrieve the top-k closest vectors to a query in embedding space. While
effective, this approach often yields semantically redundant results, missing
the diversity and contextual richness required by applications such as
retrieval-augmented generation (RAG), multi-hop QA, and memory-augmented
agents. We introduce a new retrieval paradigm: semantic compression, which aims
to select a compact, representative set of vectors that captures the broader
semantic structure around a query. We formalize this objective using principles
from submodular optimization and information geometry, and show that it
generalizes traditional top-k retrieval by prioritizing coverage and diversity.
To operationalize this idea, we propose graph-augmented vector retrieval, which
overlays semantic graphs (e.g., kNN or knowledge-based links) atop vector
spaces to enable multi-hop, context-aware search. We theoretically analyze the
limitations of proximity-based retrieval under high-dimensional concentration
and highlight how graph structures can improve semantic coverage. Our work
outlines a foundation for meaning-centric vector search systems, emphasizing
hybrid indexing, diversity-aware querying, and structured semantic retrieval.
We make our implementation publicly available to foster future research in this
area.

</details>


### [144] [Feed-anywhere ANN (I) Steady Discrete $\to$ Diffusing on Graph Hidden States](https://arxiv.org/abs/2507.20088)
*Dmitry Pasechnyuk-Vilensky,Daniil Doroshenko*

Main category: cs.LG

TL;DR: 提出一种用几何分析和非线性动力学从数据中学习隐藏图结构的新框架，有理论保证和更好泛化性。


<details>
  <summary>Details</summary>
Motivation: 从数据中学习隐藏的图结构。

Method: 1. 在图上定义离散Sobolev空间；2. 引入规范等效的非线性薛定谔和Landau - Lifshitz动力学；3. 开发带稀疏正则化的随机梯度算法。

Result: 保证拓扑正确性、度量收敛和高效搜索空间利用，模型比标准神经网络有更强泛化界限。

Conclusion: 所提基于动力学的模型在学习图结构上表现良好，复杂度与数据流形拓扑有关。

Abstract: We propose a novel framework for learning hidden graph structures from data
using geometric analysis and nonlinear dynamics. Our approach: (1) Defines
discrete Sobolev spaces on graphs for scalar/vector fields, establishing key
functional properties; (2) Introduces gauge-equivalent nonlinear Schr\"odinger
and Landau--Lifshitz dynamics with provable stable stationary solutions
smoothly dependent on input data and graph weights; (3) Develops a stochastic
gradient algorithm over graph moduli spaces with sparsity regularization.
Theoretically, we guarantee: topological correctness (homology recovery),
metric convergence (Gromov--Hausdorff), and efficient search space utilization.
Our dynamics-based model achieves stronger generalization bounds than standard
neural networks, with complexity dependent on the data manifold's topology.

</details>


### [145] [Predicting Human Mobility in Disasters via LLM-Enhanced Cross-City Learning](https://arxiv.org/abs/2507.19737)
*Yinzhou Tang,Huandong Wang,Xiaochen Fan,Yong Li*

Main category: cs.LG

TL;DR: 本文提出 DisasterMobLLM 框架用于灾害场景下人类移动性预测，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 随着城市化和气候变化，城市对自然灾害脆弱性增加，现有人类移动性预测模型不适用于灾害场景。

Method: 引入 DisasterMobLLM 框架，利用大语言模型建模移动意图，通过 RAG 增强意图预测器、基于大语言模型的意图精炼器和意图调制位置预测器实现预测。

Result: DisasterMobLLM 在 Acc@1 上提升 32.8%，在预测静止状态的 F1 分数上提升 35.0%。

Conclusion: DisasterMobLLM 框架能有效提升灾害场景下人类移动性预测性能。

Abstract: The vulnerability of cities to natural disasters has increased with
urbanization and climate change, making it more important to predict human
mobility in the disaster scenarios for downstream tasks including
location-based early disaster warning and pre-allocating rescue resources, etc.
However, existing human mobility prediction models are mainly designed for
normal scenarios, and fail to adapt to disaster scenarios due to the shift of
human mobility patterns under disaster. To address this issue, we introduce
\textbf{DisasterMobLLM}, a mobility prediction framework for disaster scenarios
that can be integrated into existing deep mobility prediction methods by
leveraging LLMs to model the mobility intention and transferring the common
knowledge of how different disasters affect mobility intentions between cities.
This framework utilizes a RAG-Enhanced Intention Predictor to forecast the next
intention, refines it with an LLM-based Intention Refiner, and then maps the
intention to an exact location using an Intention-Modulated Location Predictor.
Extensive experiments illustrate that DisasterMobLLM can achieve a 32.8\%
improvement in terms of Acc@1 and a 35.0\% improvement in terms of the F1-score
of predicting immobility compared to the baselines. The code is available at
https://github.com/tsinghua-fib-lab/DisasterMobLLM.

</details>


### [146] [Meta Fusion: A Unified Framework For Multimodality Fusion with Mutual Learning](https://arxiv.org/abs/2507.20089)
*Ziyi Liang,Annie Qu,Babak Shahbaba*

Main category: cs.LG

TL;DR: 本文提出Meta Fusion框架统一现有多模态数据融合策略，理论上减少泛化误差，实证表现优于传统方法，并在现实应用中验证。


<details>
  <summary>Details</summary>
Motivation: 开发有效的多模态数据融合策略对提升统计机器学习方法预测能力至关重要，传统融合方法各有优劣，需统一框架。

Method: 受深度互学习和集成学习启发，基于跨模态潜在表征的各种组合构建模型队列，通过队列内软信息共享提升性能，且学习潜在表征时与模型无关。

Result: 理论上软信息共享机制减少泛化误差，实证上在大量模拟研究中始终优于传统融合策略，在阿尔茨海默病检测和神经解码等现实应用中得到验证。

Conclusion: Meta Fusion是一个灵活且有原则的框架，能统一现有融合策略，提升多模态数据融合的预测性能。

Abstract: Developing effective multimodal data fusion strategies has become
increasingly essential for improving the predictive power of statistical
machine learning methods across a wide range of applications, from autonomous
driving to medical diagnosis. Traditional fusion methods, including early,
intermediate, and late fusion, integrate data at different stages, each
offering distinct advantages and limitations. In this paper, we introduce Meta
Fusion, a flexible and principled framework that unifies these existing
strategies as special cases. Motivated by deep mutual learning and ensemble
learning, Meta Fusion constructs a cohort of models based on various
combinations of latent representations across modalities, and further boosts
predictive performance through soft information sharing within the cohort. Our
approach is model-agnostic in learning the latent representations, allowing it
to flexibly adapt to the unique characteristics of each modality.
Theoretically, our soft information sharing mechanism reduces the
generalization error. Empirically, Meta Fusion consistently outperforms
conventional fusion strategies in extensive simulation studies. We further
validate our approach on real-world applications, including Alzheimer's disease
detection and neural decoding.

</details>


### [147] [Modeling enzyme temperature stability from sequence segment perspective](https://arxiv.org/abs/2507.19755)
*Ziqi Zhang,Shiheng Chen,Runze Yang,Zhisheng Wei,Wei Zhang,Lei Wang,Zhanzhi Liu,Fengshan Zhang,Jing Wu,Xiaoyong Pan,Hongbin Shen,Longbing Cao,Zhaohong Deng*

Main category: cs.LG

TL;DR: 本文引入酶热稳定性数据集，提出Segment Transformer框架预测酶热稳定性，表现出色，还用于指导工程改造并验证效果。


<details>
  <summary>Details</summary>
Motivation: 开发具有理想热性质的酶很重要，但实验测定热参数费力、耗时且成本高，现有计算方法受数据限制，因此要解决这些问题。

Method: 引入用于模型开发和基准测试的温度稳定性数据集，提出Segment Transformer深度学习框架。

Result: 模型RMSE为24.03，MAE为18.09，皮尔逊和斯皮尔曼相关性均为0.33；用于指导角质酶工程改造，热处理后相对活性提高1.64倍，仅17个突变且不影响催化功能。

Conclusion: 引入数据集和提出的Segment Transformer框架有效，融入片段级表示可准确预测酶温度稳定性，还能指导酶工程改造。

Abstract: Developing enzymes with desired thermal properties is crucial for a wide
range of industrial and research applications, and determining temperature
stability is an essential step in this process. Experimental determination of
thermal parameters is labor-intensive, time-consuming, and costly. Moreover,
existing computational approaches are often hindered by limited data
availability and imbalanced distributions. To address these challenges, we
introduce a curated temperature stability dataset designed for model
development and benchmarking in enzyme thermal modeling. Leveraging this
dataset, we present the \textit{Segment Transformer}, a novel deep learning
framework that enables efficient and accurate prediction of enzyme temperature
stability. The model achieves state-of-the-art performance with an RMSE of
24.03, MAE of 18.09, and Pearson and Spearman correlations of 0.33,
respectively. These results highlight the effectiveness of incorporating
segment-level representations, grounded in the biological observation that
different regions of a protein sequence contribute unequally to thermal
behavior. As a proof of concept, we applied the Segment Transformer to guide
the engineering of a cutinase enzyme. Experimental validation demonstrated a
1.64-fold improvement in relative activity following heat treatment, achieved
through only 17 mutations and without compromising catalytic function.

</details>


### [148] [Graded Transformers: A Symbolic-Geometric Approach to Structured Learning](https://arxiv.org/abs/2507.20108)
*Tony Shaska Sr*

Main category: cs.LG

TL;DR: 提出分级Transformer框架，含LGT和EGT架构，有理论保证，可应用于多领域，推动结构化深度学习。


<details>
  <summary>Details</summary>
Motivation: 为结构化数据处理，克服先前固定等级的局限性，推动结构化深度学习。

Method: 扩展分级神经网络理论，提出LGT和EGT架构，使用参数化缩放算子，推导理论保证，设计分级损失函数。

Result: 有严格理论保证，如通用逼近定理、降低样本复杂度等，可自适应特征优先级。

Conclusion: 该框架对分层学习和神经符号推理有变革潜力，为复杂领域提供可解释、高效系统的数学基础方案。

Abstract: We introduce the Graded Transformer framework, a novel class of sequence
models that embeds algebraic inductive biases through grading transformations
on vector spaces. Extending the theory of Graded Neural Networks (GNNs), we
propose two architectures: the Linearly Graded Transformer (LGT) and the
Exponentially Graded Transformer (EGT). These models apply parameterized
scaling operators-governed by fixed or learnable grading tuples and, for EGT,
exponential factors to infuse hierarchical structure into attention and
representation layers, enhancing efficiency for structured data.
  We derive rigorous theoretical guarantees, including universal approximation
theorems for continuous and Sobolev functions, reduced sample complexity via
effective VC dimension bounds, Lipschitz continuity of graded operations, and
robustness to adversarial perturbations. A graded loss function ensures
gradient stability and alignment with domain priors during optimization. By
treating grades as differentiable parameters, the framework enables adaptive
feature prioritization, overcoming limitations of fixed grades in prior work.
  The Graded Transformer holds transformative potential for hierarchical
learning and neurosymbolic reasoning, with applications spanning algebraic
geometry (e.g., moduli spaces and zeta functions), physics (e.g., multiscale
simulations), natural language processing (e.g., syntactic parsing), biological
sequence analysis (e.g., variant prediction), and emerging areas like graph
neural networks and financial modeling. This work advances structured deep
learning by fusing geometric and algebraic principles with attention
mechanisms, offering a mathematically grounded alternative to data-driven
models and paving the way for interpretable, efficient systems in complex
domains.

</details>


### [149] [Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation](https://arxiv.org/abs/2507.19771)
*Xin Zhang,Lissette Iturburu,Juan Nicolas Villamizar,Xiaoyu Liu,Manuel Salmeron,Shirley J. Dyke,Julio Ramirez*

Main category: cs.LG

TL;DR: 提出基于生成式AI的结构图纸生成方法，可将自然语言描述转为AutoCAD图纸，减轻工作量。


<details>
  <summary>Details</summary>
Motivation: 尽管软件能力提升，结构工程师生成图纸仍耗时费力，需更高效方法。

Method: 采用大语言模型（LLM）代理，结合检索增强生成（RAG）技术，理解自然语言描述、提取信息并生成代码在AutoCAD生成图纸。

Result: 能将结构图纸自然语言描述高效直接转为AutoCAD图纸。

Conclusion: 该方法显著减轻工作量，便于工程师以简化方式表达设计思路。

Abstract: Structural drawings are widely used in many fields, e.g., mechanical
engineering, civil engineering, etc. In civil engineering, structural drawings
serve as the main communication tool between architects, engineers, and
builders to avoid conflicts, act as legal documentation, and provide a
reference for future maintenance or evaluation needs. They are often organized
using key elements such as title/subtitle blocks, scales, plan views, elevation
view, sections, and detailed sections, which are annotated with standardized
symbols and line types for interpretation by engineers and contractors. Despite
advances in software capabilities, the task of generating a structural drawing
remains labor-intensive and time-consuming for structural engineers. Here we
introduce a novel generative AI-based method for generating structural drawings
employing a large language model (LLM) agent. The method incorporates a
retrieval-augmented generation (RAG) technique using externally-sourced facts
to enhance the accuracy and reliability of the language model. This method is
capable of understanding varied natural language descriptions, processing these
to extract necessary information, and generating code to produce the desired
structural drawing in AutoCAD. The approach developed, demonstrated and
evaluated herein enables the efficient and direct conversion of a structural
drawing's natural language description into an AutoCAD drawing, significantly
reducing the workload compared to current working process associated with
manual drawing production, facilitating the typical iterative process of
engineers for expressing design ideas in a simplified way.

</details>


### [150] [AI-Based Clinical Rule Discovery for NMIBC Recurrence through Tsetlin Machines](https://arxiv.org/abs/2507.19803)
*Saram Abbas,Naeem Soomro,Rishad Shafik,Rakesh Heer,Kabita Adhikari*

Main category: cs.LG

TL;DR: 提出基于Tsetlin Machine (TM)的可解释AI模型用于膀胱癌复发预测，在PHOTO试验数据集上表现优于其他方法，是实用决策支持工具。


<details>
  <summary>Details</summary>
Motivation: 膀胱癌死亡率高，NMIBC患者复发率高，现有临床工具如EORTC风险表过时且不可靠。

Method: 使用Tsetlin Machine (TM)构建可解释AI模型，输出透明、可读逻辑。

Result: 在PHOTO试验数据集上，TM的F1分数达0.80，优于XGBoost、Logistic Regression和EORTC。

Conclusion: TM是强大、可靠的决策支持工具，可在现实世界应用。

Abstract: Bladder cancer claims one life every 3 minutes worldwide. Most patients are
diagnosed with non-muscle-invasive bladder cancer (NMIBC), yet up to 70% recur
after treatment, triggering a relentless cycle of surgeries, monitoring, and
risk of progression. Clinical tools like the EORTC risk tables are outdated and
unreliable - especially for intermediate-risk cases.
  We propose an interpretable AI model using the Tsetlin Machine (TM), a
symbolic learner that outputs transparent, human-readable logic. Tested on the
PHOTO trial dataset (n=330), TM achieved an F1-score of 0.80, outperforming
XGBoost (0.78), Logistic Regression (0.60), and EORTC (0.42). TM reveals the
exact clauses behind each prediction, grounded in clinical features like tumour
count, surgeon experience, and hospital stay - offering accuracy and full
transparency. This makes TM a powerful, trustworthy decision-support tool ready
for real-world adoption.

</details>


### [151] [Debunking Optimization Myths in Federated Learning for Medical Image Classification](https://arxiv.org/abs/2507.19822)
*Youngjoon Lee,Hyukjoon Lee,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 本文研究联合学习（FL）中边缘设备配置对性能的影响，发现边缘特定配置比算法复杂性更关键。


<details>
  <summary>Details</summary>
Motivation: 现有FL方法对局部因素敏感，在实际部署中鲁棒性受限，需明确边缘设备配置的影响。

Method: 重新审视原始FL，在结直肠病理和血细胞分类任务上对近期FL方法进行基准测试。

Result: 局部优化器和学习率的选择对性能影响更大，增加本地训练轮次对收敛的影响取决于FL方法。

Conclusion: 适当的边缘特定配置对于实现有效的FL比算法复杂性更重要。

Abstract: Federated Learning (FL) is a collaborative learning method that enables
decentralized model training while preserving data privacy. Despite its promise
in medical imaging, recent FL methods are often sensitive to local factors such
as optimizers and learning rates, limiting their robustness in practical
deployments. In this work, we revisit vanilla FL to clarify the impact of edge
device configurations, benchmarking recent FL methods on colorectal pathology
and blood cell classification task. We numerically show that the choice of
local optimizer and learning rate has a greater effect on performance than the
specific FL method. Moreover, we find that increasing local training epochs can
either enhance or impair convergence, depending on the FL method. These
findings indicate that appropriate edge-specific configuration is more crucial
than algorithmic complexity for achieving effective FL.

</details>


### [152] [Data-Efficient Prediction-Powered Calibration via Cross-Validation](https://arxiv.org/abs/2507.20268)
*Seonghoon Yoo,Houssem Sifaou,Sangwoo Park,Joonhyuk Kang,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出新方法利用有限校准数据微调预测器并估计合成标签偏差，在室内定位实验中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型决策不确定性量化需校准数据，但校准数据稀缺，利用合成标签的方法会加剧数据稀缺问题。

Method: 提出一种能有效利用有限校准数据同时微调预测器和估计合成标签偏差的新方法。

Result: 所提方法能为AI生成的决策提供有严格覆盖保证的预测集，室内定位实验验证了方法有效性和性能提升。

Conclusion: 该方法能有效利用有限校准数据，在解决校准数据稀缺问题上有良好效果。

Abstract: Calibration data are necessary to formally quantify the uncertainty of the
decisions produced by an existing artificial intelligence (AI) model. To
overcome the common issue of scarce calibration data, a promising approach is
to employ synthetic labels produced by a (generally different) predictive
model. However, fine-tuning the label-generating predictor on the inference
task of interest, as well as estimating the residual bias of the synthetic
labels, demand additional data, potentially exacerbating the calibration data
scarcity problem. This paper introduces a novel approach that efficiently
utilizes limited calibration data to simultaneously fine-tune a predictor and
estimate the bias of the synthetic labels. The proposed method yields
prediction sets with rigorous coverage guarantees for AI-generated decisions.
Experimental results on an indoor localization problem validate the
effectiveness and performance gains of our solution.

</details>


### [153] [GNSP: Gradient Null Space Projection for Preserving Cross-Modal Alignment in VLMs Continual Learning](https://arxiv.org/abs/2507.19839)
*Tiantian Peng,Yuyang Liu,Shuo Yang,Qiuhe Hong,YongHong Tian*

Main category: cs.LG

TL;DR: 提出GNSP方法解决CLIP持续微调的灾难性遗忘问题，结合知识蒸馏和模态对齐损失，在MTIL基准测试取得SOTA。


<details>
  <summary>Details</summary>
Motivation: CLIP在持续微调时存在灾难性遗忘和嵌入对齐退化问题，影响零样本能力。

Method: 提出GNSP方法将特定任务梯度投影到先前知识的零空间，结合知识蒸馏和模态对齐损失。

Result: 在MTIL基准测试11个任务中，平均和最后关键指标达到SOTA，保持了CLIP原始模态差距和跨模态检索性能。

Conclusion: 该方法在持续学习过程中有效维护了强大的视觉 - 语言空间。

Abstract: Contrastive Language-Image Pretraining has demonstrated remarkable zero-shot
generalization by aligning visual and textual modalities in a shared embedding
space. However, when continuously fine-tuned on diverse tasks, CLIP suffers
from catastrophic forgetting and degradation of its embedding alignment,
undermining its zero-shot capabilities. In this work, we propose Gradient Null
Space Projection (GNSP), an efficient continual learning method that projects
task-specific gradients onto the null space of previously learned knowledge.
This orthogonal projection mathematically prevents interference with previous
tasks without relying on rehearsal or architectural modification. Furthermore,
to preserve the inherent generalization property of CLIP, we introduce
knowledge distillation and combine it with a modality alignment preservation
loss inspired by CLIP pre-training to stabilize the structure of the multimodal
embedding space during fine-tuning. On the MTIL benchmark consisting of 11
tasks, our method achieved SOTA performance on both the Average and Last key
metrics. More importantly, experiments show that our method successfully
maintains the original modality gap and cross-modal retrieval performance of
CLIP, confirming its effectiveness in maintaining a robust visual-language
space throughout the continual learning process.

</details>


### [154] [Approximating Full Conformal Prediction for Neural Network Regression with Gauss-Newton Influence](https://arxiv.org/abs/2507.20272)
*Dharmesh Tailor,Alvaro H. C. Correia,Eric Nalisnick,Christos Louizos*

Main category: cs.LG

TL;DR: 本文提出无预留数据情况下为神经网络回归器构建预测区间的方法，通过近似全共形预测，避免了传统方法的缺点，实验表明预测区间有优势。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型不确定性量化方法存在校准不准、统计效率低等问题，需要改进。

Method: 通过近似全共形预测方法，仅训练一次模型，用高斯 - 牛顿影响局部扰动模型参数，结合网络线性化，将绝对残差非一致性得分表示为候选标签的分段线性函数。

Result: 在标准回归基准和边界框定位上，得到的预测区间是局部自适应的，且通常比分段共形预测的区间更窄。

Conclusion: 所提出的方法能有效为神经网络回归器构建预测区间，具有较好的性能。

Abstract: Uncertainty quantification is an important prerequisite for the deployment of
deep learning models in safety-critical areas. Yet, this hinges on the
uncertainty estimates being useful to the extent the prediction intervals are
well-calibrated and sharp. In the absence of inherent uncertainty estimates
(e.g. pretrained models predicting only point estimates), popular approaches
that operate post-hoc include Laplace's method and split conformal prediction
(split-CP). However, Laplace's method can be miscalibrated when the model is
misspecified and split-CP requires sample splitting, and thus comes at the
expense of statistical efficiency. In this work, we construct prediction
intervals for neural network regressors post-hoc without held-out data. This is
achieved by approximating the full conformal prediction method (full-CP).
Whilst full-CP nominally requires retraining the model for every test point and
candidate label, we propose to train just once and locally perturb model
parameters using Gauss-Newton influence to approximate the effect of
retraining. Coupled with linearization of the network, we express the absolute
residual nonconformity score as a piecewise linear function of the candidate
label allowing for an efficient procedure that avoids the exhaustive search
over the output space. On standard regression benchmarks and bounding box
localization, we show the resulting prediction intervals are locally-adaptive
and often tighter than those of split-CP.

</details>


### [155] [VAE-GAN Based Price Manipulation in Coordinated Local Energy Markets](https://arxiv.org/abs/2507.19844)
*Biswarup Mukherjee,Li Zhou,S. Gokul Krishnan,Milad Kabirifar,Subhash Lakshminarayana,Charalambos Konstantinou*

Main category: cs.LG

TL;DR: 本文提出协调异质分布式能源资源产消者的本地能源市场模型，用MADDPG做实时决策，研究VAE - GAN价格操纵策略，发现对抗定价下部分产消者有损失，市场规模增加交易更稳定公平。


<details>
  <summary>Details</summary>
Motivation: 实现产消者在本地能源市场的高效协调与最优能源交易，研究价格操纵对产消者的影响。

Method: 采用基于MADDPG框架的数据驱动、无模型强化学习方法，使用VAE - GAN模型研究价格操纵策略。

Result: 对抗定价下，异质产消者群体（尤其是无发电能力的）会有经济损失，不同规模本地能源市场结果相同；市场规模增加，交易稳定且公平性提升。

Conclusion: 提出的模型有助于本地能源市场交易，市场规模扩大利于交易稳定和公平。

Abstract: This paper introduces a model for coordinating prosumers with heterogeneous
distributed energy resources (DERs), participating in the local energy market
(LEM) that interacts with the market-clearing entity. The proposed LEM scheme
utilizes a data-driven, model-free reinforcement learning approach based on the
multi-agent deep deterministic policy gradient (MADDPG) framework, enabling
prosumers to make real-time decisions on whether to buy, sell, or refrain from
any action while facilitating efficient coordination for optimal energy trading
in a dynamic market. In addition, we investigate a price manipulation strategy
using a variational auto encoder-generative adversarial network (VAE-GAN)
model, which allows utilities to adjust price signals in a way that induces
financial losses for the prosumers. Our results show that under adversarial
pricing, heterogeneous prosumer groups, particularly those lacking generation
capabilities, incur financial losses. The same outcome holds across LEMs of
different sizes. As the market size increases, trading stabilizes and fairness
improves through emergent cooperation among agents.

</details>


### [156] [Agentic Reinforced Policy Optimization](https://arxiv.org/abs/2507.19849)
*Guanting Dong,Hangyu Mao,Kai Ma,Licheng Bao,Yifei Chen,Zhongyuan Wang,Zhongxia Chen,Jiazhen Du,Huiyang Wang,Fuzheng Zhang,Guorui Zhou,Yutao Zhu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.LG

TL;DR: 提出用于训练多轮大语言模型代理的ARPO算法，实验显示其优于轨迹级强化学习算法，只需一半工具使用预算就能提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习算法无法充分平衡模型的长期推理能力和多轮工具交互能力，且大语言模型与外部工具交互后行为高度不确定。

Method: 提出Agentic Reinforced Policy Optimization (ARPO)算法，包含基于熵的自适应滚动机制和优势归因估计。

Result: 在13个具有挑战性的基准测试中，ARPO优于轨迹级强化学习算法，只需现有方法一半的工具使用预算就能提升性能。

Conclusion: ARPO为使基于大语言模型的代理与实时动态环境对齐提供了可扩展的解决方案。

Abstract: Large-scale reinforcement learning with verifiable rewards (RLVR) has
demonstrated its effectiveness in harnessing the potential of large language
models (LLMs) for single-turn reasoning tasks. In realistic reasoning
scenarios, LLMs can often utilize external tools to assist in task-solving
processes. However, current RL algorithms inadequately balance the models'
intrinsic long-horizon reasoning capabilities and their proficiency in
multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced
Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training
multi-turn LLM-based agents. Through preliminary experiments, we observe that
LLMs tend to exhibit highly uncertain behavior, characterized by an increase in
the entropy distribution of generated tokens, immediately following
interactions with external tools. Motivated by this observation, ARPO
incorporates an entropy-based adaptive rollout mechanism, dynamically balancing
global trajectory sampling and step-level sampling, thereby promoting
exploration at steps with high uncertainty after tool usage. By integrating an
advantage attribution estimation, ARPO enables LLMs to internalize advantage
differences in stepwise tool-use interactions. Our experiments across 13
challenging benchmarks in computational reasoning, knowledge reasoning, and
deep search domains demonstrate ARPO's superiority over trajectory-level RL
algorithms. Remarkably, ARPO achieves improved performance using only half of
the tool-use budget required by existing methods, offering a scalable solution
for aligning LLM-based agents with real-time dynamic environments. Our code and
datasets are released at https://github.com/dongguanting/ARPO

</details>


### [157] [Diagonally-Weighted Generalized Method of Moments Estimation for Gaussian Mixture Modeling](https://arxiv.org/abs/2507.20459)
*Liu Zhang,Oscar Mickelin,Sheng Xu,Amit Singer*

Main category: cs.LG

TL;DR: 提出对角加权广义矩估计法（DGMM）解决MM和GMM计算瓶颈，应用于高斯混合模型参数估计，实证显示优势。


<details>
  <summary>Details</summary>
Motivation: MM和GMM计算与存储复杂度随维度指数增长，处理高维数据或高阶矩时不实用，GMM更严重。

Method: 提出DGMM，应用于弱分离异方差低秩高斯混合模型参数估计，设计高效稳定算法。

Result: 数值研究中，DGMM估计误差更小，运行时间更短。

Conclusion: DGMM在统计效率、计算复杂度和数值稳定性间取得平衡，有明显优势。

Abstract: Since Pearson [Philosophical Transactions of the Royal Society of London. A,
185 (1894), pp. 71-110] first applied the method of moments (MM) for modeling
data as a mixture of one-dimensional Gaussians, moment-based estimation methods
have proliferated. Among these methods, the generalized method of moments (GMM)
improves the statistical efficiency of MM by weighting the moments
appropriately. However, the computational complexity and storage complexity of
MM and GMM grow exponentially with the dimension, making these methods
impractical for high-dimensional data or when higher-order moments are
required. Such computational bottlenecks are more severe in GMM since it
additionally requires estimating a large weighting matrix. To overcome these
bottlenecks, we propose the diagonally-weighted GMM (DGMM), which achieves a
balance among statistical efficiency, computational complexity, and numerical
stability. We apply DGMM to study the parameter estimation problem for weakly
separated heteroscedastic low-rank Gaussian mixtures and design a
computationally efficient and numerically stable algorithm that obtains the
DGMM estimator without explicitly computing or storing the moment tensors. We
implement the proposed algorithm and empirically validate the advantages of
DGMM: in numerical studies, DGMM attains smaller estimation errors while
requiring substantially shorter runtime than MM and GMM. The code and data will
be available upon publication at https://github.com/liu-lzhang/dgmm.

</details>


### [158] [Inducing Causal World Models in LLMs for Zero-Shot Physical Reasoning](https://arxiv.org/abs/2507.19855)
*Aditya Sharma,Linh Nguyen,Ananya Gupta,Chengyu Wang,Chiamaka Adebayo,Jakub Kowalski*

Main category: cs.LG

TL;DR: 本文提出因果世界模型归纳（CWMI）框架，将因果物理模型嵌入大语言模型，实验显示其在零样本物理推理任务上表现出色，证明诱导因果世界模型对可靠通用的AI系统很关键。


<details>
  <summary>Details</summary>
Motivation: 大语言模型缺乏对物理动力学的直观理解，限制了其在需要因果推理的现实场景中的有效性。

Method: 引入CWMI框架，包含因果物理模块（CPM）和因果干预损失训练目标，让模型从多模态数据中学习因果关系。

Result: CWMI在零样本物理推理任务（如PIQA基准和新提出的PhysiCa - Bench数据集）上显著优于现有先进大语言模型。

Conclusion: 诱导因果世界模型是迈向更可靠、更通用AI系统的关键一步。

Abstract: Large Language Models (LLMs), despite their advanced linguistic capabilities,
fundamentally lack an intuitive understanding of physical dynamics, which
limits their effectiveness in real-world scenarios that require causal
reasoning. In this paper, we introduce Causal World Model Induction (CWMI), a
novel framework designed to embed an explicit model of causal physics within an
LLM. Our approach incorporates a dedicated Causal Physics Module (CPM) and a
new training objective called Causal Intervention Loss, encouraging the model
to learn cause-and-effect relationships from multimodal data. By training the
model to predict the outcomes of hypothetical interventions instead of merely
capturing statistical correlations, CWMI develops a robust internal
representation of physical laws. Experimental results show that CWMI
significantly outperforms state-of-the-art LLMs on zero-shot physical reasoning
tasks, including the PIQA benchmark and our newly proposed PhysiCa-Bench
dataset. These findings demonstrate that inducing a causal world model is a
critical step toward more reliable and generalizable AI systems.

</details>


### [159] [Improving Group Fairness in Tensor Completion via Imbalance Mitigating Entity Augmentation](https://arxiv.org/abs/2507.20542)
*Dawon Ahn,Jun-Gi Jang,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 提出STAFF方法改善张量分解中的组公平性，评估显示其在完成误差和组公平性间取得最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有张量分解中组公平性研究存在性能下降问题，需解决该问题。

Method: 提出STAFF方法，通过最小化不同组完成误差差距、减少整体张量完成误差来改善组公平性，用增强实体扩充张量以减轻稀疏张量中的不平衡和组偏差。

Result: 在多种数据集的张量完成任务中，STAFF在常规和基于深度学习的张量模型下，始终在完成误差和组公平性间取得最佳平衡，比次优基线最多降低36%的MSE和59%的MADE。

Conclusion: STAFF方法在张量分解中能有效改善组公平性，且降低完成误差。

Abstract: Group fairness is important to consider in tensor decomposition to prevent
discrimination based on social grounds such as gender or age. Although few
works have studied group fairness in tensor decomposition, they suffer from
performance degradation. To address this, we propose STAFF(Sparse Tensor
Augmentation For Fairness) to improve group fairness by minimizing the gap in
completion errors of different groups while reducing the overall tensor
completion error. Our main idea is to augment a tensor with augmented entities
including sufficient observed entries to mitigate imbalance and group bias in
the sparse tensor. We evaluate \method on tensor completion with various
datasets under conventional and deep learning-based tensor models. STAFF
consistently shows the best trade-off between completion error and group
fairness; at most, it yields 36% lower MSE and 59% lower MADE than the
second-best baseline.

</details>


### [160] [CLoRA: Parameter-Efficient Continual Learning with Low-Rank Adaptation](https://arxiv.org/abs/2507.19887)
*Shishir Muralidhara,Didier Stricker,René Schuster*

Main category: cs.LG

TL;DR: 提出CLoRA用于类增量语义分割的持续学习，减少训练硬件需求，性能良好。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法未考虑计算资源限制，且大多需为新任务重新训练整个模型，计算需求高。

Method: 探索低秩适应（LoRA）在类增量语义分割中的应用，CLoRA利用模型少量参数并跨任务使用。

Result: CLoRA性能与超越基线方法，用NetScore评估凸显考虑资源效率的必要性。

Conclusion: CLoRA显著降低训练硬件需求，适合资源受限环境的持续学习。

Abstract: In the past, continual learning (CL) was mostly concerned with the problem of
catastrophic forgetting in neural networks, that arises when incrementally
learning a sequence of tasks. Current CL methods function within the confines
of limited data access, without any restrictions imposed on computational
resources. However, in real-world scenarios, the latter takes precedence as
deployed systems are often computationally constrained. A major drawback of
most CL methods is the need to retrain the entire model for each new task. The
computational demands of retraining large models can be prohibitive, limiting
the applicability of CL in environments with limited resources. Through CLoRA,
we explore the applicability of Low-Rank Adaptation (LoRA), a
parameter-efficient fine-tuning method for class-incremental semantic
segmentation. CLoRA leverages a small set of parameters of the model and uses
the same set for learning across all tasks. Results demonstrate the efficacy of
CLoRA, achieving performance on par with and exceeding the baseline methods. We
further evaluate CLoRA using NetScore, underscoring the need to factor in
resource efficiency and evaluate CL methods beyond task performance. CLoRA
significantly reduces the hardware requirements for training, making it
well-suited for CL in resource-constrained environments after deployment.

</details>


### [161] [Personalized Treatment Effect Estimation from Unstructured Data](https://arxiv.org/abs/2507.20993)
*Henri Arno,Thomas Demeester*

Main category: cs.LG

TL;DR: 现有个性化治疗效果估计方法多依赖结构化协变量，本文引入直接在非结构化数据神经表示上训练的近似'插入'方法，又提出两个理论上的估计器避免混淆偏差，还引入回归校正解决采样偏差，实验显示插入方法表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有估计个性化治疗效果的方法依赖结构化协变量，限制了在非结构化数据上的应用，而利用非结构化数据进行因果推断有很大应用潜力。

Method: 先引入直接在非结构化数据神经表示上训练的近似'插入'方法；再提出两个理论上的估计器，在训练时利用混杂因素的结构化测量，从非结构化输入估计效果并避免混淆偏差；若结构化测量仅在非代表性子集可用，引入回归校正解决采样偏差。

Result: 在两个基准数据集上的实验表明，可直接在大型非结构化数据集上训练的插入方法，尽管简单，但在所有设置下都取得了强大的实证性能。

Conclusion: 提出的方法能解决非结构化数据中个性化治疗效果估计的混淆偏差和采样偏差问题，插入方法有良好表现。

Abstract: Existing methods for estimating personalized treatment effects typically rely
on structured covariates, limiting their applicability to unstructured data.
Yet, leveraging unstructured data for causal inference has considerable
application potential, for instance in healthcare, where clinical notes or
medical images are abundant. To this end, we first introduce an approximate
'plug-in' method trained directly on the neural representations of unstructured
data. However, when these fail to capture all confounding information, the
method may be subject to confounding bias. We therefore introduce two
theoretically grounded estimators that leverage structured measurements of the
confounders during training, but allow estimating personalized treatment
effects purely from unstructured inputs, while avoiding confounding bias. When
these structured measurements are only available for a non-representative
subset of the data, these estimators may suffer from sampling bias. To address
this, we further introduce a regression-based correction that accounts for the
non-uniform sampling, assuming the sampling mechanism is known or can be
well-estimated. Our experiments on two benchmark datasets show that the plug-in
method, directly trainable on large unstructured datasets, achieves strong
empirical performance across all settings, despite its simplicity.

</details>


### [162] [A Survey on Generative Model Unlearning: Fundamentals, Taxonomy, Evaluation, and Future Direction](https://arxiv.org/abs/2507.19894)
*Xiaohua Feng,Jiaming Zhang,Fengyuan Yu,Chengye Wang,Li Zhang,Kaixiang Li,Yuyuan Li,Chaochao Chen,Jianwei Yin*

Main category: cs.LG

TL;DR: 文章指出生成模型反学习缺乏统一框架，对现有研究进行全面综述，提出统一分析框架，探索与相关技术联系，点明实际价值、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 生成模型隐私问题受关注，现有生成模型反学习研究缺乏统一框架，不同研究目标和评估协议差异大，难以客观公平比较。

Method: 对当前生成模型反学习研究进行全面综述，提出统一分析框架对反学习目标、方法策略和评估指标进行分类，探索与相关技术的联系。

Result: 提出统一分析框架，探索与相关技术联系，强调反学习技术实际价值。

Conclusion: 指出该领域关键挑战，给出未来研究方向，维护相关开源材料。

Abstract: With the rapid advancement of generative models, associated privacy concerns
have attracted growing attention. To address this, researchers have begun
adapting machine unlearning techniques from traditional classification models
to generative settings. Although notable progress has been made in this area, a
unified framework for systematically organizing and integrating existing work
is still lacking. The substantial differences among current studies in terms of
unlearning objectives and evaluation protocols hinder the objective and fair
comparison of various approaches. While some studies focus on specific types of
generative models, they often overlook the commonalities and systematic
characteristics inherent in Generative Model Unlearning (GenMU). To bridge this
gap, we provide a comprehensive review of current research on GenMU and propose
a unified analytical framework for categorizing unlearning objectives,
methodological strategies, and evaluation metrics. In addition, we explore the
connections between GenMU and related techniques, including model editing,
reinforcement learning from human feedback, and controllable generation. We
further highlight the potential practical value of unlearning techniques in
real-world applications. Finally, we identify key challenges and outline future
research directions aimed at laying a solid foundation for further advancements
in this field. We consistently maintain the related open-source materials at
https://github.com/caxLee/Generative-model-unlearning-survey.

</details>


### [163] [Transformers as Unrolled Inference in Probabilistic Laplacian Eigenmaps: An Interpretation and Potential Improvements](https://arxiv.org/abs/2507.21040)
*Aditya Ravuri,Neil D. Lawrence*

Main category: cs.LG

TL;DR: 提出对transformer的概率解释，表明其初始化时进行‘线性’降维，transformer块中有图拉普拉斯项，减去注意力矩阵中的单位矩阵可提升性能。


<details>
  <summary>Details</summary>
Motivation: 从概率角度对transformer进行解释和分析。

Method: 假设ProbDR框架下的概率拉普拉斯特征映射模型，对transformer进行推导。

Result: 推导得出transformer初始化时进行‘线性’降维，块中有图拉普拉斯项；减去注意力矩阵中的单位矩阵提升了语言模型和简单视觉transformer的验证性能。

Conclusion: 对transformer的概率解释有一定合理性，减去注意力矩阵中的单位矩阵这一操作有实际效果。

Abstract: We propose a probabilistic interpretation of transformers as unrolled
inference steps assuming a probabilistic Laplacian Eigenmaps model from the
ProbDR framework. Our derivation shows that at initialisation, transformers
perform "linear" dimensionality reduction. We also show that within the
transformer block, a graph Laplacian term arises from our arguments, rather
than an attention matrix (which we interpret as an adjacency matrix). We
demonstrate that simply subtracting the identity from the attention matrix (and
thereby taking a graph diffusion step) improves validation performance on a
language model and a simple vision transformer.

</details>


### [164] [Who Owns This Sample: Cross-Client Membership Inference Attack in Federated Graph Neural Networks](https://arxiv.org/abs/2507.19964)
*Kunhao Li,Di Wu,Jun Bai,Jing Xu,Lei Yang,Ziyi Zhang,Yiliao Song,Wencheng Yang,Taotao Cai,Yan Li*

Main category: cs.LG

TL;DR: 本文首次系统研究针对联邦图神经网络节点分类任务的跨客户端成员推理攻击，设计攻击框架并评估，结果表明方法性能高，凸显新隐私威胁。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在联邦学习场景应用时，图结构与分散训练交互产生了新的隐私威胁，此前研究多聚焦集中式场景，本文关注联邦场景下样本到客户端的归属这一细粒度隐私风险。

Method: 设计了一个通用攻击框架，利用联邦图神经网络的聚合行为、梯度更新和嵌入接近度，在多轮训练中把样本与其源客户端关联起来。

Result: 在多个图数据集的现实联邦学习设置中评估，方法在成员推理和所有权识别上都取得了高性能。

Conclusion: 研究凸显了联邦图学习中通过结构和模型层面线索导致的客户端身份泄露这一新隐私威胁，表明需要设计抗归属攻击的图神经网络。

Abstract: Graph-structured data is prevalent in many real-world applications, including
social networks, financial systems, and molecular biology. Graph Neural
Networks (GNNs) have become the de facto standard for learning from such data
due to their strong representation capabilities. As GNNs are increasingly
deployed in federated learning (FL) settings to preserve data locality and
privacy, new privacy threats arise from the interaction between graph
structures and decentralized training. In this paper, we present the first
systematic study of cross-client membership inference attacks (CC-MIA) against
node classification tasks of federated GNNs (FedGNNs), where a malicious client
aims to infer which client owns the given data. Unlike prior
centralized-focused work that focuses on whether a sample was included in
training, our attack targets sample-to-client attribution, a finer-grained
privacy risk unique to federated settings. We design a general attack framework
that exploits FedGNNs' aggregation behaviors, gradient updates, and embedding
proximity to link samples to their source clients across training rounds. We
evaluate our attack across multiple graph datasets under realistic FL setups.
Results show that our method achieves high performance on both membership
inference and ownership identification. Our findings highlight a new privacy
threat in federated graph learning-client identity leakage through structural
and model-level cues, motivating the need for attribution-robust GNN design.

</details>


### [165] [Robust Taxi Fare Prediction Under Noisy Conditions: A Comparative Study of GAT, TimesNet, and XGBoost](https://arxiv.org/abs/2507.20008)
*Padmavathi Moorthy*

Main category: cs.LG

TL;DR: 研究用含超5500万条记录的真实数据集，分析GAT、XGBoost和TimesNet三种机器学习模型对出租车票价的预测能力，对比原始和去噪数据集，评估多方面指标并探索预处理策略，揭示经典和深度学习模型差异并给出实用指南。


<details>
  <summary>Details</summary>
Motivation: 精确的票价预测在网约车平台和城市出行系统中至关重要，需评估不同模型的预测能力。

Method: 使用含超5500万条记录的真实数据集，分析GAT、XGBoost和TimesNet三种模型，对比原始和去噪数据集，评估预测准确性、校准、不确定性估计等多方面指标，探索KNN插补、高斯噪声注入和基于自编码器去噪等预处理策略。

Result: 揭示了经典和深度学习模型在现实条件下的关键差异。

Conclusion: 为城市票价预测系统构建健壮且可扩展的模型提供了实用指南。

Abstract: Precise fare prediction is crucial in ride-hailing platforms and urban
mobility systems. This study examines three machine learning models-Graph
Attention Networks (GAT), XGBoost, and TimesNet to evaluate their predictive
capabilities for taxi fares using a real-world dataset comprising over 55
million records. Both raw (noisy) and denoised versions of the dataset are
analyzed to assess the impact of data quality on model performance. The study
evaluated the models along multiple axes, including predictive accuracy,
calibration, uncertainty estimation, out-of-distribution (OOD) robustness, and
feature sensitivity. We also explore pre-processing strategies, including KNN
imputation, Gaussian noise injection, and autoencoder-based denoising. The
study reveals critical differences between classical and deep learning models
under realistic conditions, offering practical guidelines for building robust
and scalable models in urban fare prediction systems.

</details>


### [166] [FedSWA: Improving Generalization in Federated Learning with Highly Heterogeneous Data via Momentum-Based Stochastic Controlled Weight Averaging](https://arxiv.org/abs/2507.20016)
*Liu junkang,Yuanyuan Liu,Fanhua Shang,Hongying Liu,Jin Liu,Wei Feng*

Main category: cs.LG

TL;DR: 本文重新审视联邦学习泛化问题，提出FedSWA和FedMoSWA算法，理论分析并实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习算法如FedSAM的泛化能力对实际应用很关键，需研究数据异质性对联邦学习泛化的影响。

Method: 提出FedSWA算法在高度异质数据下找更平坦最小值，引入FedMoSWA算法更好对齐局部和全局模型，进行理论收敛分析和泛化边界推导。

Result: 实验表明在CIFAR10/100和Tiny ImageNet上，所提算法优于其他对比算法。

Conclusion: 所提FedSWA和FedMoSWA算法有效，优化和泛化误差更小，具有更好的性能。

Abstract: For federated learning (FL) algorithms such as FedSAM, their generalization
capability is crucial for real-word applications. In this paper, we revisit the
generalization problem in FL and investigate the impact of data heterogeneity
on FL generalization. We find that FedSAM usually performs worse than FedAvg in
the case of highly heterogeneous data, and thus propose a novel and effective
federated learning algorithm with Stochastic Weight Averaging (called
\texttt{FedSWA}), which aims to find flatter minima in the setting of highly
heterogeneous data. Moreover, we introduce a new momentum-based stochastic
controlled weight averaging FL algorithm (\texttt{FedMoSWA}), which is designed
to better align local and global models.
  Theoretically, we provide both convergence analysis and generalization bounds
for \texttt{FedSWA} and \texttt{FedMoSWA}. We also prove that the optimization
and generalization errors of \texttt{FedMoSWA} are smaller than those of their
counterparts, including FedSAM and its variants. Empirically, experimental
results on CIFAR10/100 and Tiny ImageNet demonstrate the superiority of the
proposed algorithms compared to their counterparts. Open source code at:
https://github.com/junkangLiu0/FedSWA.

</details>


### [167] [What Can Grokking Teach Us About Learning Under Nonstationarity?](https://arxiv.org/abs/2507.20057)
*Clare Lyle,Gharda Sokar,Razvan Pascanu,Andras Gyorgy*

Main category: cs.LG

TL;DR: 本文研究非平稳学习问题中神经网络的特征学习动态，提出通过增加有效学习率诱导特征学习动态的方法，该方法在多种场景中有效。


<details>
  <summary>Details</summary>
Motivation: 解决连续学习中神经网络的首因偏差问题，探索非平稳学习问题的特征学习动态。

Method: 提出通过增加有效学习率（参数与更新范数之比）来诱导特征学习动态的方法。

Result: 该方法在多种场景下，包括顿悟学习、神经网络热启动训练和强化学习任务中，都促进了特征学习并提高了泛化能力。

Conclusion: 促进顿悟学习的特征学习动态可能有助于覆盖先前学习的特征，通过增加有效学习率诱导特征学习动态的方法是解决非平稳学习中首因偏差的有前途方案。

Abstract: In continual learning problems, it is often necessary to overwrite components
of a neural network's learned representation in response to changes in the data
stream; however, neural networks often exhibit \primacy bias, whereby early
training data hinders the network's ability to generalize on later tasks. While
feature-learning dynamics of nonstationary learning problems are not well
studied, the emergence of feature-learning dynamics is known to drive the
phenomenon of grokking, wherein neural networks initially memorize their
training data and only later exhibit perfect generalization. This work
conjectures that the same feature-learning dynamics which facilitate
generalization in grokking also underlie the ability to overwrite previous
learned features as well, and methods which accelerate grokking by facilitating
feature-learning dynamics are promising candidates for addressing primacy bias
in non-stationary learning problems. We then propose a straightforward method
to induce feature-learning dynamics as needed throughout training by increasing
the effective learning rate, i.e. the ratio between parameter and update norms.
We show that this approach both facilitates feature-learning and improves
generalization in a variety of settings, including grokking, warm-starting
neural network training, and reinforcement learning tasks.

</details>


### [168] [ModShift: Model Privacy via Designed Shifts](https://arxiv.org/abs/2507.20060)
*Nomaan A. Kherani,Urbashi Mitra*

Main category: cs.LG

TL;DR: 本文引入偏移量在联邦学习中保护模型隐私，进行收敛测试，数值结果显示该方案优于噪声注入方案。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中保护模型隐私，防止窃听者获取信息。

Method: 将模型学习视为参数估计问题，推导偏移更新的Fisher信息矩阵并使其奇异，安全共享偏移量，提出收敛测试。

Result: 方案能通过收敛测试，相比噪声注入方案可实现更高模型偏移，所需带宽秘密通道更小。

Conclusion: 所提方案在保护模型隐私方面具有优势。

Abstract: In this paper, shifts are introduced to preserve model privacy against an
eavesdropper in federated learning. Model learning is treated as a parameter
estimation problem. This perspective allows us to derive the Fisher Information
matrix of the model updates from the shifted updates and drive them to
singularity, thus posing a hard estimation problem for Eve. The shifts are
securely shared with the central server to maintain model accuracy at the
server and participating devices. A convergence test is proposed to detect if
model updates have been tampered with and we show that our scheme passes this
test. Numerical results show that our scheme achieves a higher model shift when
compared to a noise injection scheme while requiring a lesser bandwidth secret
channel.

</details>


### [169] [Geometric Operator Learning with Optimal Transport](https://arxiv.org/abs/2507.20065)
*Xinyi Li,Zongyi Li,Nikola Kovachki,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出将最优传输（OT）集成到复杂几何偏微分方程（PDEs）的算子学习中，方法更灵活有效，实验显示精度更高、计算开销更低。


<details>
  <summary>Details</summary>
Motivation: 改进经典几何学习方法在复杂几何PDEs算子学习中的表现，提升灵活性和有效性。

Method: 将离散化网格推广到网格密度函数，将几何嵌入表述为OT问题，使用依赖实例的变形；对于3D表面模拟，将表面几何嵌入2D参数化潜在空间。

Result: 在ShapeNet - Car和DrivAerNet - Car数据集上精度更高，计算时间和内存使用开销降低；在FlowBench数据集上精度显著提高。

Conclusion: 基于OT的方法在复杂几何PDEs算子学习中具有优势，依赖实例的变形对几何变化大的数据集有益。

Abstract: We propose integrating optimal transport (OT) into operator learning for
partial differential equations (PDEs) on complex geometries. Classical
geometric learning methods typically represent domains as meshes, graphs, or
point clouds. Our approach generalizes discretized meshes to mesh density
functions, formulating geometry embedding as an OT problem that maps these
functions to a uniform density in a reference space. Compared to previous
methods relying on interpolation or shared deformation, our OT-based method
employs instance-dependent deformation, offering enhanced flexibility and
effectiveness. For 3D simulations focused on surfaces, our OT-based neural
operator embeds the surface geometry into a 2D parameterized latent space. By
performing computations directly on this 2D representation of the surface
manifold, it achieves significant computational efficiency gains compared to
volumetric simulation. Experiments with Reynolds-averaged Navier-Stokes
equations (RANS) on the ShapeNet-Car and DrivAerNet-Car datasets show that our
method achieves better accuracy and also reduces computational expenses in
terms of both time and memory usage compared to existing machine learning
models. Additionally, our model demonstrates significantly improved accuracy on
the FlowBench dataset, underscoring the benefits of employing
instance-dependent deformation for datasets with highly variable geometries.

</details>


### [170] [Sparse Equation Matching: A Derivative-Free Learning for General-Order Dynamical Systems](https://arxiv.org/abs/2507.20072)
*Jiaqiang Li,Jianbin Tan,Xueqin Wang*

Main category: cs.LG

TL;DR: 提出稀疏方程匹配（SEM）框架用于方程发现，可处理一般阶动力系统，经模拟验证效果并应用于脑电数据，揭示脑连接模式。


<details>
  <summary>Details</summary>
Motivation: 现有方程发现方法依赖准确导数估计且限于一阶动力系统，应用受限，需新方法。

Method: 提出SEM框架，用格林函数进行基于积分的稀疏回归，实现无导数估计微分算子和驱动函数。

Result: 模拟显示SEM比基于导数方法有效，应用于脑电数据识别出活跃脑区和特定任务连接模式。

Conclusion: SEM为方程发现提供统一框架，研究结果为脑连接和神经机制提供有价值见解。

Abstract: Equation discovery is a fundamental learning task for uncovering the
underlying dynamics of complex systems, with wide-ranging applications in areas
such as brain connectivity analysis, climate modeling, gene regulation, and
physical system simulation. However, many existing approaches rely on accurate
derivative estimation and are limited to first-order dynamical systems,
restricting their applicability to real-world scenarios. In this work, we
propose sparse equation matching (SEM), a unified framework that encompasses
several existing equation discovery methods under a common formulation. SEM
introduces an integral-based sparse regression method using Green's functions,
enabling derivative-free estimation of differential operators and their
associated driving functions in general-order dynamical systems. The
effectiveness of SEM is demonstrated through extensive simulations,
benchmarking its performance against derivative-based approaches. We then apply
SEM to electroencephalographic (EEG) data recorded during multiple oculomotor
tasks, collected from 52 participants in a brain-computer interface experiment.
Our method identifies active brain regions across participants and reveals
task-specific connectivity patterns. These findings offer valuable insights
into brain connectivity and the underlying neural mechanisms.

</details>


### [171] [Cluster Purge Loss: Structuring Transformer Embeddings for Equivalent Mutants Detection](https://arxiv.org/abs/2507.20078)
*Adelaide Danilov,Aria Nourbakhsh,Christoph Schommer*

Main category: cs.LG

TL;DR: 现有微调预训练transformer模型方法在下游分类任务中难以构建优质嵌入空间，本文提出含Cluster Purge Loss的框架，在等价代码突变检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有微调预训练transformer模型用于下游分类任务的方法难以构建反映类内语义关系的嵌入空间，而等价代码突变检测任务对嵌入空间质量要求高。

Method: 引入集成交叉熵损失和深度度量学习目标的Cluster Purge Loss框架，利用动态调整边界促使实例基于语义等价性与类中心分离，以UniXCoder为基础模型。

Result: 在等价代码突变检测领域达到了最先进的性能，产生了更具可解释性的嵌入空间。

Conclusion: 提出的新框架有效，能提升等价代码突变检测性能并优化嵌入空间。

Abstract: Recent pre-trained transformer models achieve superior performance in various
code processing objectives. However, although effective at optimizing decision
boundaries, common approaches for fine-tuning them for downstream
classification tasks - distance-based methods or training an additional
classification head - often fail to thoroughly structure the embedding space to
reflect nuanced intra-class semantic relationships. Equivalent code mutant
detection is one of these tasks, where the quality of the embedding space is
crucial to the performance of the models. We introduce a novel framework that
integrates cross-entropy loss with a deep metric learning objective, termed
Cluster Purge Loss. This objective, unlike conventional approaches,
concentrates on adjusting fine-grained differences within each class,
encouraging the separation of instances based on semantical equivalency to the
class center using dynamically adjusted borders. Employing UniXCoder as the
base model, our approach demonstrates state-of-the-art performance in the
domain of equivalent mutant detection and produces a more interpretable
embedding space.

</details>


### [172] [EcoTransformer: Attention without Multiplication](https://arxiv.org/abs/2507.20096)
*Xin Gao,Xingming Xu*

Main category: cs.LG

TL;DR: 提出EcoTransformer架构，新注意力计算免矩阵乘法，在多任务表现好且能耗低


<details>
  <summary>Details</summary>
Motivation: Transformer的缩放点积注意力机制计算密集、能耗高

Method: 提出EcoTransformer架构，用拉普拉斯核卷积构建输出上下文向量，用L1度量计算距离

Result: 在NLP、生物信息学和视觉任务中表现与缩放点积注意力相当甚至更好

Conclusion: 新架构能在多任务中有效工作，同时显著降低能耗

Abstract: The Transformer, with its scaled dot-product attention mechanism, has become
a foundational architecture in modern AI. However, this mechanism is
computationally intensive and incurs substantial energy costs. We propose a new
Transformer architecture EcoTransformer, in which the output context vector is
constructed as the convolution of the values using a Laplacian kernel, where
the distances are measured by the L1 metric between the queries and keys.
Compared to dot-product based attention, the new attention score calculation is
free of matrix multiplication. It performs on par with, or even surpasses,
scaled dot-product attention in NLP, bioinformatics, and vision tasks, while
consuming significantly less energy.

</details>


### [173] [Wine Characterisation with Spectral Information and Predictive Artificial Intelligence](https://arxiv.org/abs/2507.20114)
*Jianping Yao,Son N. Tran,Hieu Nguyen,Samantha Sawyer,Rocco Longo*

Main category: cs.LG

TL;DR: 本文结合机器学习与光谱技术，用吸光度数据预测葡萄汁属性和葡萄酒产地，SVM表现最优，研究为葡萄酒及饮料行业发展提供新思路。


<details>
  <summary>Details</summary>
Motivation: 找到相对简单的方法应用于酿酒两阶段，改进传统葡萄酒分析方法。

Method: 将机器学习技术与光谱学结合。

Result: SVM在属性和产地预测任务中最有效稳健，产地预测准确率和F1分数超91%，更有影响力的波长在250 - 420 nm。

Conclusion: 研究为葡萄酒及其他饮料行业未来整合大数据和物联网提供新思路，推动“智能酒庄”发展。

Abstract: The purpose of this paper is to use absorbance data obtained by human tasting
and an ultraviolet-visible (UV-Vis) scanning spectrophotometer to predict the
attributes of grape juice (GJ) and to classify the wine's origin, respectively.
The approach combined machine learning (ML) techniques with spectroscopy to
find a relatively simple way to apply them in two stages of winemaking and help
improve the traditional wine analysis methods regarding sensory data and wine's
origins. This new technique has overcome the disadvantages of the complex
sensors by taking advantage of spectral fingerprinting technology and forming a
comprehensive study of the employment of AI in the wine analysis domain. In the
results, Support Vector Machine (SVM) was the most efficient and robust in both
attributes and origin prediction tasks. Both the accuracy and F1 score of the
origin prediction exceed 91%. The feature ranking approach found that the more
influential wavelengths usually appear at the lower end of the scan range, 250
nm (nanometers) to 420 nm, which is believed to be of great help for selecting
appropriate validation methods and sensors to extract wine data in future
research. The knowledge of this research provides new ideas and early solutions
for the wine industry or other beverage industries to integrate big data and
IoT in the future, which significantly promotes the development of 'Smart
Wineries'.

</details>


### [174] [Aggregation-aware MLP: An Unsupervised Approach for Graph Message-passing](https://arxiv.org/abs/2507.20127)
*Xuanting Xie,Bingheng Li,Erlin Pan,Zhao Kang,Wenyu Chen*

Main category: cs.LG

TL;DR: 提出无监督框架AMLP，通过两步改进模型性能，实验证明其在图学习场景表现出色。


<details>
  <summary>Details</summary>
Motivation: GNNs采用固定聚合函数，在异质性下表现差，且一些改进方法依赖大量有标签数据。

Method: 提出AMLP框架，先利用图重建方法促进高阶分组效应，再用单层网络编码异质性程度。

Result: 在节点聚类和分类的大量实验中，AMLP表现优越。

Conclusion: AMLP有潜力用于多样的图学习场景。

Abstract: Graph Neural Networks (GNNs) have become a dominant approach to learning
graph representations, primarily because of their message-passing mechanisms.
However, GNNs typically adopt a fixed aggregator function such as Mean, Max, or
Sum without principled reasoning behind the selection. This rigidity,
especially in the presence of heterophily, often leads to poor, problem
dependent performance. Although some attempts address this by designing more
sophisticated aggregation functions, these methods tend to rely heavily on
labeled data, which is often scarce in real-world tasks. In this work, we
propose a novel unsupervised framework, "Aggregation-aware Multilayer
Perceptron" (AMLP), which shifts the paradigm from directly crafting
aggregation functions to making MLP adaptive to aggregation. Our lightweight
approach consists of two key steps: First, we utilize a graph reconstruction
method that facilitates high-order grouping effects, and second, we employ a
single-layer network to encode varying degrees of heterophily, thereby
improving the capacity and applicability of the model. Extensive experiments on
node clustering and classification demonstrate the superior performance of
AMLP, highlighting its potential for diverse graph learning scenarios.

</details>


### [175] [Generative molecule evolution using 3D pharmacophore for efficient Structure-Based Drug Design](https://arxiv.org/abs/2507.20130)
*Yi He,Ailun Wang,Zhi Wang,Yu Liu,Xingyuan Xu,Wen Yan*

Main category: cs.LG

TL;DR: 提出进化框架MEVO解决基于结构的药物设计（SBDD）模型训练数据有限问题，验证其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 生成模型在SBDD应用受数据限制，需解决训练数据不足问题。

Method: 提出MEVO框架，含高保真VQ - VAE、扩散模型和口袋感知进化策略。

Result: 能为多种蛋白靶点生成高亲和力结合物，设计出对KRAS$^{	extrm{G12D}}$有类似高活性抑制剂亲和力的抑制剂。

Conclusion: MEVO是用于基于结构的配体设计的有效且数据高效的模型。

Abstract: Recent advances in generative models, particularly diffusion and
auto-regressive models, have revolutionized fields like computer vision and
natural language processing. However, their application to structure-based drug
design (SBDD) remains limited due to critical data constraints. To address the
limitation of training data for models targeting SBDD tasks, we propose an
evolutionary framework named MEVO, which bridges the gap between billion-scale
small molecule dataset and the scarce protein-ligand complex dataset, and
effectively increase the abundance of training data for generative SBDD models.
MEVO is composed of three key components: a high-fidelity VQ-VAE for molecule
representation in latent space, a diffusion model for pharmacophore-guided
molecule generation, and a pocket-aware evolutionary strategy for molecule
optimization with physics-based scoring function. This framework efficiently
generate high-affinity binders for various protein targets, validated with
predicted binding affinities using free energy perturbation (FEP) methods. In
addition, we showcase the capability of MEVO in designing potent inhibitors to
KRAS$^{\textrm{G12D}}$, a challenging target in cancer therapeutics, with
similar affinity to the known highly active inhibitor evaluated by FEP
calculations. With high versatility and generalizability, MEVO offers an
effective and data-efficient model for various tasks in structure-based ligand
design.

</details>


### [176] [Awesome-OL: An Extensible Toolkit for Online Learning](https://arxiv.org/abs/2507.20144)
*Zeyi Liu,Songqiao Hu,Pengyu Han,Jiaming Liu,Xiao He*

Main category: cs.LG

TL;DR: 介绍用于在线学习研究的Python工具包Awesome - OL，它集成先进算法，提供统一框架，代码开源。


<details>
  <summary>Details</summary>
Motivation: 推动在线学习领域的算法开发和实际部署。

Method: 基于scikit - multiflow开源基础设施，集成先进算法，提供统一框架、基准数据集和多模态可视化。

Result: 开发出可扩展的Python工具包Awesome - OL，代码公开。

Conclusion: Awesome - OL能促进在线学习研究，兼顾用户交互性与研究灵活性和可扩展性。

Abstract: In recent years, online learning has attracted increasing attention due to
its adaptive capability to process streaming and non-stationary data. To
facilitate algorithm development and practical deployment in this area, we
introduce Awesome-OL, an extensible Python toolkit tailored for online learning
research. Awesome-OL integrates state-of-the-art algorithm, which provides a
unified framework for reproducible comparisons, curated benchmark datasets, and
multi-modal visualization. Built upon the scikit-multiflow open-source
infrastructure, Awesome-OL emphasizes user-friendly interactions without
compromising research flexibility or extensibility. The source code is publicly
available at: https://github.com/liuzy0708/Awesome-OL.

</details>


### [177] [ASNN: Learning to Suggest Neural Architectures from Performance Distributions](https://arxiv.org/abs/2507.20164)
*Jinwook Hong*

Main category: cs.LG

TL;DR: 提出ASNN模型学习神经网络架构与测试准确率关系并建议改进架构，实验表明其能发现性能更优架构，为架构优化提供高效替代方案。


<details>
  <summary>Details</summary>
Motivation: 神经网络架构设计缺乏通用闭式函数，当前设计多为启发式或基于搜索，需高效设计方法。

Method: 构建不同层数和节点数的TensorFlow模型数据集，以准确率为输入、架构参数为输出训练ASNN，迭代预测高性能架构。

Result: 在2层和3层架构中，ASNN建议的架构优于原训练数据最佳结果，重复循环发现平均测试准确率更高的架构。

Conclusion: ASNN为架构优化提供高效替代方案，是自动化神经网络设计的有前景方法。

Abstract: The architecture of a neural network (NN) plays a critical role in
determining its performance. However, there is no general closed-form function
that maps between network structure and accuracy, making the process of
architecture design largely heuristic or search-based. In this study, we
propose the Architecture Suggesting Neural Network (ASNN), a model designed to
learn the relationship between NN architecture and its test accuracy, and to
suggest improved architectures accordingly. To train ASNN, we constructed
datasets using TensorFlow-based models with varying numbers of layers and
nodes. Experimental results were collected for both 2-layer and 3-layer
architectures across a grid of configurations, each evaluated with 10 repeated
trials to account for stochasticity. Accuracy values were treated as inputs,
and architectural parameters as outputs. The trained ASNN was then used
iteratively to predict architectures that yield higher performance. In both
2-layer and 3-layer cases, ASNN successfully suggested architectures that
outperformed the best results found in the original training data. Repeated
prediction and retraining cycles led to the discovery of architectures with
improved mean test accuracies, demonstrating the model's capacity to generalize
the performance-structure relationship. These results suggest that ASNN
provides an efficient alternative to random search for architecture
optimization, and offers a promising approach toward automating neural network
design. "Parts of the manuscript, including text editing and expression
refinement, were supported by OpenAI's ChatGPT. All content was reviewed and
verified by the authors."

</details>


### [178] [Partial Domain Adaptation via Importance Sampling-based Shift Correction](https://arxiv.org/abs/2507.20191)
*Cheng-Jun Guo,Chuan-Xian Ren,You-Wei Luo,Xiao-Lin Xu,Hong Yan*

Main category: cs.LG

TL;DR: 提出基于重要性采样的偏移校正方法IS²C解决部分域适应问题，有理论保证并经实验验证效果。


<details>
  <summary>Details</summary>
Motivation: 以往部分域适应工作简单重加权技术无法探索潜在结构和充分利用标记数据，易过拟合。

Method: 提出IS²C方法，从构建的采样域采样新标记数据；证明泛化误差可由IS²C控制；提出基于最优传输的独立性准则进行条件分布对齐并降低计算复杂度。

Result: 广泛实验验证理论结果，IS²C比现有方法更有效。

Conclusion: IS²C方法能有效解决部分域适应问题，提高模型泛化能力。

Abstract: Partial domain adaptation (PDA) is a challenging task in real-world machine
learning scenarios. It aims to transfer knowledge from a labeled source domain
to a related unlabeled target domain, where the support set of the source label
distribution subsumes the target one. Previous PDA works managed to correct the
label distribution shift by weighting samples in the source domain. However,
the simple reweighing technique cannot explore the latent structure and
sufficiently use the labeled data, and then models are prone to over-fitting on
the source domain. In this work, we propose a novel importance sampling-based
shift correction (IS$^2$C) method, where new labeled data are sampled from a
built sampling domain, whose label distribution is supposed to be the same as
the target domain, to characterize the latent structure and enhance the
generalization ability of the model. We provide theoretical guarantees for
IS$^2$C by proving that the generalization error can be sufficiently dominated
by IS$^2$C. In particular, by implementing sampling with the mixture
distribution, the extent of shift between source and sampling domains can be
connected to generalization error, which provides an interpretable way to build
IS$^2$C. To improve knowledge transfer, an optimal transport-based independence
criterion is proposed for conditional distribution alignment, where the
computation of the criterion can be adjusted to reduce the complexity from
$\mathcal{O}(n^3)$ to $\mathcal{O}(n^2)$ in realistic PDA scenarios. Extensive
experiments on PDA benchmarks validate the theoretical results and demonstrate
the effectiveness of our IS$^2$C over existing methods.

</details>


### [179] [Protein-SE(3): Benchmarking SE(3)-based Generative Models for Protein Structure Design](https://arxiv.org/abs/2507.20243)
*Lang Yu,Zhangyang Gao,Cheng Tan,Qin Chen,Jie Zhou,Liang He*

Main category: cs.LG

TL;DR: 提出Protein - SE(3)基准用于SE(3)蛋白质结构设计，集成多种模型并统一评估，还提供数学抽象，代码公开。


<details>
  <summary>Details</summary>
Motivation: 当前SE(3)蛋白质几何建模和结构设计领域缺乏模块化基准进行全面研究和公平比较。

Method: 构建基于统一训练框架的Protein - SE(3)基准，集成多种先进生成模型，用相同数据集和指标评估，提供模型数学基础的高级抽象。

Result: 发布首个基于统一训练框架的SE(3)蛋白质结构设计综合基准，代码公开。

Conclusion: Protein - SE(3)基准有助于SE(3)蛋白质结构设计领域的研究和算法快速原型开发。

Abstract: SE(3)-based generative models have shown great promise in protein geometry
modeling and effective structure design. However, the field currently lacks a
modularized benchmark to enable comprehensive investigation and fair comparison
of different methods. In this paper, we propose Protein-SE(3), a new benchmark
based on a unified training framework, which comprises protein scaffolding
tasks, integrated generative models, high-level mathematical abstraction, and
diverse evaluation metrics. Recent advanced generative models designed for
protein scaffolding, from multiple perspectives like DDPM (Genie1 and Genie2),
Score Matching (FrameDiff and RfDiffusion) and Flow Matching (FoldFlow and
FrameFlow) are integrated into our framework. All integrated methods are fairly
investigated with the same training dataset and evaluation metrics.
Furthermore, we provide a high-level abstraction of the mathematical
foundations behind the generative models, enabling fast prototyping of future
algorithms without reliance on explicit protein structures. Accordingly, we
release the first comprehensive benchmark built upon unified training framework
for SE(3)-based protein structure design, which is publicly accessible at
https://github.com/BruthYU/protein-se3.

</details>


### [180] [MIPS: a Multimodal Infinite Polymer Sequence Pre-training Framework for Polymer Property Prediction](https://arxiv.org/abs/2507.20326)
*Jiaxi Wang,Yaosen Min,Xun Zhu,Miao Li,Ji Wu*

Main category: cs.LG

TL;DR: 提出多模态无限聚合物序列（MIPS）预训练框架，结合拓扑和空间信息进行聚合物性能预测，实验表现达最优。


<details>
  <summary>Details</summary>
Motivation: 现有聚合物建模方法难以捕捉聚合过程中性能变化，需准确预测聚合物性能用于设计、开发和应用。

Method: 提出MIPS框架，从拓扑和空间角度建模。拓扑上推广消息传递和图注意力机制，用RSIT测试鲁棒性，用骨干嵌入克服局限性；空间上提取3D描述符，最后设计跨模态融合机制。

Result: 在八个不同聚合物性能预测任务实验中，MIPS达到了最优性能。

Conclusion: MIPS预训练框架在聚合物性能预测上有效，可捕捉全面信息实现准确预测。

Abstract: Polymers, composed of repeating structural units called monomers, are
fundamental materials in daily life and industry. Accurate property prediction
for polymers is essential for their design, development, and application.
However, existing modeling approaches, which typically represent polymers by
the constituent monomers, struggle to capture the whole properties of polymer,
since the properties change during the polymerization process. In this study,
we propose a Multimodal Infinite Polymer Sequence (MIPS) pre-training
framework, which represents polymers as infinite sequences of monomers and
integrates both topological and spatial information for comprehensive modeling.
From the topological perspective, we generalize message passing mechanism (MPM)
and graph attention mechanism (GAM) to infinite polymer sequences. For MPM, we
demonstrate that applying MPM to infinite polymer sequences is equivalent to
applying MPM on the induced star-linking graph of monomers. For GAM, we propose
to further replace global graph attention with localized graph attention (LGA).
Moreover, we show the robustness of the "star linking" strategy through Repeat
and Shift Invariance Test (RSIT). Despite its robustness, "star linking"
strategy exhibits limitations when monomer side chains contain ring structures,
a common characteristic of polymers, as it fails the Weisfeiler-Lehman~(WL)
test. To overcome this issue, we propose backbone embedding to enhance the
capability of MPM and LGA on infinite polymer sequences. From the spatial
perspective, we extract 3D descriptors of repeating monomers to capture spatial
information. Finally, we design a cross-modal fusion mechanism to unify the
topological and spatial information. Experimental validation across eight
diverse polymer property prediction tasks reveals that MIPS achieves
state-of-the-art performance.

</details>


### [181] [Cultivating Helpful, Personalized, and Creative AI Tutors: A Framework for Pedagogical Alignment using Reinforcement Learning](https://arxiv.org/abs/2507.20335)
*Siyu Song,Wentao Liu,Ye Lu,Ruohua Zhang,Tao Liu,Jinze Lv,Xinyun Wang,Aimin Zhou,Fei Tan,Bo Jiang,Hao Hao*

Main category: cs.LG

TL;DR: 提出EduAlign框架使大语言模型成为更有效的教育助手，实验表明微调后模型在教育特性上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 标准大语言模型缺乏与教育原则的对齐，需使其成为更有效和负责的教育助手。

Method: 分两阶段构建EduAlign框架，第一阶段创建8k教育交互数据集并标注，训练多维度奖励模型HPC - RM；第二阶段用GRPO基于2k提示微调预训练模型，并在多基准测试评估。

Result: 微调后的模型在教育的有用性、个性化和创造力激发方面的对齐有显著提升。

Conclusion: 该研究提供了将大语言模型与教育特性对齐的可扩展有效方法，为开发更好的AI导师奠定基础。

Abstract: The integration of large language models (LLMs) into education presents
unprecedented opportunities for scalable personalized learning. However,
standard LLMs often function as generic information providers, lacking
alignment with fundamental pedagogical principles such as helpfulness,
student-centered personalization, and creativity cultivation. To bridge this
gap, we propose EduAlign, a novel framework designed to guide LLMs toward
becoming more effective and responsible educational assistants. EduAlign
consists of two main stages. In the first stage, we curate a dataset of 8k
educational interactions and annotate them-both manually and
automatically-along three key educational dimensions: Helpfulness,
Personalization, and Creativity (HPC). These annotations are used to train
HPC-RM, a multi-dimensional reward model capable of accurately scoring LLM
outputs according to these educational principles. We further evaluate the
consistency and reliability of this reward model. In the second stage, we
leverage HPC-RM as a reward signal to fine-tune a pre-trained LLM using Group
Relative Policy Optimization (GRPO) on a set of 2k diverse prompts. We then
assess the pre- and post-finetuning models on both educational and
general-domain benchmarks across the three HPC dimensions. Experimental results
demonstrate that the fine-tuned model exhibits significantly improved alignment
with pedagogical helpfulness, personalization, and creativity stimulation. This
study presents a scalable and effective approach to aligning LLMs with nuanced
and desirable educational traits, paving the way for the development of more
engaging, pedagogically aligned AI tutors.

</details>


### [182] [From Observations to Causations: A GNN-based Probabilistic Prediction Framework for Causal Discovery](https://arxiv.org/abs/2507.20349)
*Rezaur Rashid,Gabriel Terejanu*

Main category: cs.LG

TL;DR: 提出基于GNN的概率框架解决因果发现问题，表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法在处理大数据集和复杂关系时存在可扩展性和捕捉全局结构信息的局限。

Method: 引入基于GNN的概率框架，将节点和边属性编码为统一图表示，以监督学习预测图结构，用合成数据集训练，结合统计和信息论指标。

Result: 在合成和真实数据集上，准确性和可扩展性优于传统、非GNN和GNN方法，无需进一步训练。

Conclusion: 该概率框架显著改善因果结构学习，对多领域决策和科学发现有广泛意义。

Abstract: Causal discovery from observational data is challenging, especially with
large datasets and complex relationships. Traditional methods often struggle
with scalability and capturing global structural information. To overcome these
limitations, we introduce a novel graph neural network (GNN)-based
probabilistic framework that learns a probability distribution over the entire
space of causal graphs, unlike methods that output a single deterministic
graph. Our framework leverages a GNN that encodes both node and edge attributes
into a unified graph representation, enabling the model to learn complex causal
structures directly from data. The GNN model is trained on a diverse set of
synthetic datasets augmented with statistical and information-theoretic
measures, such as mutual information and conditional entropy, capturing both
local and global data properties. We frame causal discovery as a supervised
learning problem, directly predicting the entire graph structure. Our approach
demonstrates superior performance, outperforming both traditional and recent
non-GNN-based methods, as well as a GNN-based approach, in terms of accuracy
and scalability on synthetic and real-world datasets without further training.
This probabilistic framework significantly improves causal structure learning,
with broad implications for decision-making and scientific discovery across
various fields.

</details>


### [183] [Computational Advantages of Multi-Grade Deep Learning: Convergence Analysis and Performance Insights](https://arxiv.org/abs/2507.20351)
*Ronglong Fang,Yuesheng Xu*

Main category: cs.LG

TL;DR: 研究多等级深度学习（MGDL）在图像回归、去噪和去模糊任务中的计算优势，并与单等级深度学习（SGDL）比较，证明MGDL在梯度下降法下对学习率选择更稳健且训练稳定性更强。


<details>
  <summary>Details</summary>
Motivation: 研究MGDL在图像回归、去噪和去模糊任务中的计算优势，并与SGDL对比。

Method: 建立梯度下降法应用于这些模型的收敛结果，分析与梯度下降迭代产生的迭代方案相关的雅可比矩阵的特征值分布。

Result: MGDL在梯度下降法下对学习率的选择比SGDL更稳健，且训练稳定性更强。

Conclusion: MGDL在计算上具有优势，在梯度下降法中有更好的性能和稳定性。

Abstract: Multi-grade deep learning (MGDL) has been shown to significantly outperform
the standard single-grade deep learning (SGDL) across various applications.
This work aims to investigate the computational advantages of MGDL focusing on
its performance in image regression, denoising, and deblurring tasks, and
comparing it to SGDL. We establish convergence results for the gradient descent
(GD) method applied to these models and provide mathematical insights into
MGDL's improved performance. In particular, we demonstrate that MGDL is more
robust to the choice of learning rate under GD than SGDL. Furthermore, we
analyze the eigenvalue distributions of the Jacobian matrices associated with
the iterative schemes arising from the GD iterations, offering an explanation
for MGDL's enhanced training stability.

</details>


### [184] [Wafer Defect Root Cause Analysis with Partial Trajectory Regression](https://arxiv.org/abs/2507.20357)
*Kohei Miyaguchi,Masao Joko,Rebekah Sheraw,Tsuyoshi Idé*

Main category: cs.LG

TL;DR: 本文提出用于晶圆缺陷根源分析的Partial Trajectory Regression (PTR)框架，利用新算法和表示学习方法，并用真实数据验证有效性。


<details>
  <summary>Details</summary>
Motivation: 由于工艺流程的组合特性和加工路线的固有可变性，识别导致晶圆缺陷的上游过程具有挑战性，传统基于向量的回归模型有局限性。

Method: 提出PTR框架，使用新算法比较从部分工艺轨迹得出的两个反事实结果，借助proc2vec和route2vec表示学习方法。

Result: 利用纽约CREATES工厂的真实晶圆历史数据验证了框架的有效性。

Conclusion: 提出的PTR框架能有效解决传统模型在处理晶圆缺陷根源分析中的问题。

Abstract: Identifying upstream processes responsible for wafer defects is challenging
due to the combinatorial nature of process flows and the inherent variability
in processing routes, which arises from factors such as rework operations and
random process waiting times. This paper presents a novel framework for wafer
defect root cause analysis, called Partial Trajectory Regression (PTR). The
proposed framework is carefully designed to address the limitations of
conventional vector-based regression models, particularly in handling
variable-length processing routes that span a large number of heterogeneous
physical processes. To compute the attribution score of each process given a
detected high defect density on a specific wafer, we propose a new algorithm
that compares two counterfactual outcomes derived from partial process
trajectories. This is enabled by new representation learning methods, proc2vec
and route2vec. We demonstrate the effectiveness of the proposed framework using
real wafer history data from the NY CREATES fab in Albany.

</details>


### [185] [Sequence-Aware Inline Measurement Attribution for Good-Bad Wafer Diagnosis](https://arxiv.org/abs/2507.20364)
*Kohei Miyaguchi,Masao Joko,Rebekah Sheraw,Tsuyoshi Idé*

Main category: cs.LG

TL;DR: 本文提出Trajectory Shapley Attribution (TSA)框架用于晶圆缺陷跨流程根因分析，并应用于实验性前段制程的好坏晶圆诊断任务。


<details>
  <summary>Details</summary>
Motivation: 现代半导体制造复杂，晶圆缺陷的跨流程根因分析极具挑战，需有效方法识别问题上游流程。

Method: 提出TSA框架，它是可解释人工智能研究中广泛使用的Shapley值（SV）算法的扩展，克服了标准SV的关键局限。

Result: 文中未提及明确结果。

Conclusion: 文中未提及明确结论。

Abstract: How can we identify problematic upstream processes when a certain type of
wafer defect starts appearing at a quality checkpoint? Given the complexity of
modern semiconductor manufacturing, which involves thousands of process steps,
cross-process root cause analysis for wafer defects has been considered highly
challenging. This paper proposes a novel framework called Trajectory Shapley
Attribution (TSA), an extension of Shapley values (SV), a widely used
attribution algorithm in explainable artificial intelligence research. TSA
overcomes key limitations of standard SV, including its disregard for the
sequential nature of manufacturing processes and its reliance on an arbitrarily
chosen reference point. We applied TSA to a good-bad wafer diagnosis task in
experimental front-end-of-line processes at the NY CREATES Albany NanoTech fab,
aiming to identify measurement items (serving as proxies for process
parameters) most relevant to abnormal defect occurrence.

</details>


### [186] [Clustering by Attention: Leveraging Prior Fitted Transformers for Data Partitioning](https://arxiv.org/abs/2507.20369)
*Ahmed Shokry,Ayman Khalafallah*

Main category: cs.LG

TL;DR: 本文提出基于元学习的新型聚类方法，无需参数优化，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有聚类算法存在需调参、计算复杂、缺乏可解释性和准确性欠佳等问题，尤其是处理大规模数据集时。

Method: 利用预聚类样本，通过预训练的Prior - Data Fitted Transformer Network (PFN) 计算样本间注意力，推断聚类分配。

Result: 实验表明，该方法在无预聚类样本时能对分离良好的数据聚类，有少量预聚类样本时性能显著提升，优于现有技术。

Conclusion: 该方法有效且可扩展，是现有聚类技术的有前景替代方案。

Abstract: Clustering is a core task in machine learning with wide-ranging applications
in data mining and pattern recognition. However, its unsupervised nature makes
it inherently challenging. Many existing clustering algorithms suffer from
critical limitations: they often require careful parameter tuning, exhibit high
computational complexity, lack interpretability, or yield suboptimal accuracy,
especially when applied to large-scale datasets. In this paper, we introduce a
novel clustering approach based on meta-learning. Our approach eliminates the
need for parameter optimization while achieving accuracy that outperforms
state-of-the-art clustering techniques. The proposed technique leverages a few
pre-clustered samples to guide the clustering process for the entire dataset in
a single forward pass. Specifically, we employ a pre-trained Prior-Data Fitted
Transformer Network (PFN) to perform clustering. The algorithm computes
attention between the pre-clustered samples and the unclustered samples,
allowing it to infer cluster assignments for the entire dataset based on the
learned relation. We theoretically and empirically demonstrate that, given just
a few pre-clustered examples, the model can generalize to accurately cluster
the rest of the dataset. Experiments on challenging benchmark datasets show
that our approach can successfully cluster well-separated data without any
pre-clustered samples, and significantly improves performance when a few
clustered samples are provided. We show that our approach is superior to the
state-of-the-art techniques. These results highlight the effectiveness and
scalability of our approach, positioning it as a promising alternative to
existing clustering techniques.

</details>


### [187] [WBHT: A Generative Attention Architecture for Detecting Black Hole Anomalies in Backbone Networks](https://arxiv.org/abs/2507.20373)
*Kiymet Kaya,Elif Ak,Sule Gunduz Oguducu*

Main category: cs.LG

TL;DR: 提出Wasserstein Black Hole Transformer (WBHT)框架检测通信网络黑洞异常，在真实数据上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 通信网络中黑洞异常会导致数据包丢失且无故障通知，破坏连接并造成经济损失，需要有效检测方法。

Method: 结合生成式建模、序列学习和注意力机制，集成Wasserstein生成对抗网络与注意力机制，使用长短时记忆层和卷积层，以及潜在空间编码机制。

Result: 在真实网络数据测试中，WBHT在F1分数上有1.65%到58.76%的显著提升。

Conclusion: WBHT效率高且能检测未被发现的异常，是主动网络监控和安全的有价值工具，尤其适用于关键任务网络。

Abstract: We propose the Wasserstein Black Hole Transformer (WBHT) framework for
detecting black hole (BH) anomalies in communication networks. These anomalies
cause packet loss without failure notifications, disrupting connectivity and
leading to financial losses. WBHT combines generative modeling, sequential
learning, and attention mechanisms to improve BH anomaly detection. It
integrates a Wasserstein generative adversarial network with attention
mechanisms for stable training and accurate anomaly identification. The model
uses long-short-term memory layers to capture long-term dependencies and
convolutional layers for local temporal patterns. A latent space encoding
mechanism helps distinguish abnormal network behavior. Tested on real-world
network data, WBHT outperforms existing models, achieving significant
improvements in F1 score (ranging from 1.65% to 58.76%). Its efficiency and
ability to detect previously undetected anomalies make it a valuable tool for
proactive network monitoring and security, especially in mission-critical
networks.

</details>


### [188] [Set-based Implicit Likelihood Inference of Galaxy Cluster Mass](https://arxiv.org/abs/2507.20378)
*Bonny Y. Wang,Leander Thiele*

Main category: cs.LG

TL;DR: 提出基于集合的机器学习框架，从投影星系动力学推断星系团质量后验分布，在模拟上训练效果好。


<details>
  <summary>Details</summary>
Motivation: 改进从投影星系动力学推断星系团质量后验分布的方法，提高可解释性。

Method: 结合Deep Sets和条件归一化流，纳入成员星系位置和速度信息预测M - σ关系的残差修正。

Result: 在Uchuu - UniverseMachine模拟上训练，相比传统动力学估计，显著减少离散度并在全质量范围内提供校准良好的不确定性。

Conclusion: 所提出的基于集合的机器学习框架在推断星系团质量方面优于传统方法。

Abstract: We present a set-based machine learning framework that infers posterior
distributions of galaxy cluster masses from projected galaxy dynamics. Our
model combines Deep Sets and conditional normalizing flows to incorporate both
positional and velocity information of member galaxies to predict residual
corrections to the $M$-$\sigma$ relation for improved interpretability. Trained
on the Uchuu-UniverseMachine simulation, our approach significantly reduces
scatter and provides well-calibrated uncertainties across the full mass range
compared to traditional dynamical estimates.

</details>


### [189] [ResCap-DBP: A Lightweight Residual-Capsule Network for Accurate DNA-Binding Protein Prediction Using Global ProteinBERT Embeddings](https://arxiv.org/abs/2507.20426)
*Samiul Based Shuvo,Tasnia Binte Mamun,U Rajendra Acharya*

Main category: cs.LG

TL;DR: 提出ResCap - DBP深度学习框架预测DNA结合蛋白，对比不同编码方式，模型在多数据集表现优于现有方法，证明结合全局蛋白表征与深度学习架构的有效性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 实验方法识别DNA结合蛋白耗时且成本高，需要高效计算预测技术。

Method: 提出ResCap - DBP框架，结合基于残差学习的编码器和一维胶囊网络，在残差块中使用扩张卷积，胶囊层采用动态路由，对比ProteinBERT和单热编码。

Result: ProteinBERT嵌入在大数据集表现更好，单热编码在小数据集有微弱优势；模型在多个基准数据集上始终优于现有方法，在不同数据集上取得高AUC分数，且灵敏度和特异性平衡。

Conclusion: 结合全局蛋白表征与先进深度学习架构可实现可靠、可扩展的DBP预测。

Abstract: DNA-binding proteins (DBPs) are integral to gene regulation and cellular
processes, making their accurate identification essential for understanding
biological functions and disease mechanisms. Experimental methods for DBP
identification are time-consuming and costly, driving the need for efficient
computational prediction techniques. In this study, we propose a novel deep
learning framework, ResCap-DBP, that combines a residual learning-based encoder
with a one-dimensional Capsule Network (1D-CapsNet) to predict DBPs directly
from raw protein sequences. Our architecture incorporates dilated convolutions
within residual blocks to mitigate vanishing gradient issues and extract rich
sequence features, while capsule layers with dynamic routing capture
hierarchical and spatial relationships within the learned feature space. We
conducted comprehensive ablation studies comparing global and local embeddings
from ProteinBERT and conventional one-hot encoding. Results show that
ProteinBERT embeddings substantially outperform other representations on large
datasets. Although one-hot encoding showed marginal advantages on smaller
datasets, such as PDB186, it struggled to scale effectively. Extensive
evaluations on four pairs of publicly available benchmark datasets demonstrate
that our model consistently outperforms current state-of-the-art methods. It
achieved AUC scores of 98.0% and 89.5% on PDB14189andPDB1075, respectively. On
independent test sets PDB2272 and PDB186, the model attained top AUCs of 83.2%
and 83.3%, while maintaining competitive performance on larger datasets such as
PDB20000. Notably, the model maintains a well balanced sensitivity and
specificity across datasets. These results demonstrate the efficacy and
generalizability of integrating global protein representations with advanced
deep learning architectures for reliable and scalable DBP prediction in diverse
genomic contexts.

</details>


### [190] [FAST: Similarity-based Knowledge Transfer for Efficient Policy Learning](https://arxiv.org/abs/2507.20433)
*Alessandro Capurso,Elia Piccoli,Davide Bacciu*

Main category: cs.LG

TL;DR: 提出FAST框架解决迁移学习关键问题，实验证明其效果好、训练步骤少。


<details>
  <summary>Details</summary>
Motivation: 迁移学习存在负迁移、领域适应和选择源策略效率低等问题，在游戏开发等领域成本高、效率低，需改进知识迁移、提升智能体性能和降低计算成本。

Method: 提出FAST框架，利用视觉帧和文本描述创建任务动态的潜在表示，估计环境间相似度，用相似度分数选择候选策略。

Result: 在多个赛道实验中，FAST与从头学习方法相比，最终性能有竞争力，且训练步骤显著减少。

Conclusion: 基于嵌入的任务相似度估计有潜力。

Abstract: Transfer Learning (TL) offers the potential to accelerate learning by
transferring knowledge across tasks. However, it faces critical challenges such
as negative transfer, domain adaptation and inefficiency in selecting solid
source policies. These issues often represent critical problems in evolving
domains, i.e. game development, where scenarios transform and agents must
adapt. The continuous release of new agents is costly and inefficient. In this
work we challenge the key issues in TL to improve knowledge transfer, agents
performance across tasks and reduce computational costs. The proposed
methodology, called FAST - Framework for Adaptive Similarity-based Transfer,
leverages visual frames and textual descriptions to create a latent
representation of tasks dynamics, that is exploited to estimate similarity
between environments. The similarity scores guides our method in choosing
candidate policies from which transfer abilities to simplify learning of novel
tasks. Experimental results, over multiple racing tracks, demonstrate that FAST
achieves competitive final performance compared to learning-from-scratch
methods while requiring significantly less training steps. These findings
highlight the potential of embedding-driven task similarity estimations.

</details>


### [191] [BioNeuralNet: A Graph Neural Network based Multi-Omics Network Data Analysis Tool](https://arxiv.org/abs/2507.20440)
*Vicente Ramos,Sundous Hussein,Mohamed Abdel-Hafiz,Arunangshu Sarkar,Weixuan Liu,Katerina J. Kechris,Russell P. Bowler,Leslie Lange,Farnoush Banaei-Kashani*

Main category: cs.LG

TL;DR: 介绍了用于多组学网络分析的BioNeuralNet框架，可处理复杂数据并支持多样任务。


<details>
  <summary>Details</summary>
Motivation: 多组学数据存在分析挑战，现有网络方法需适配下游分析的工具。

Method: 引入BioNeuralNet框架，利用图神经网络从多组学网络学习低维表示。

Result: BioNeuralNet支持多组学网络分析各阶段，有丰富工具且兼容已有Python包。

Conclusion: BioNeuralNet是用于精准医学多组学网络分析的开源、易用且文档丰富的框架。

Abstract: Multi-omics data offer unprecedented insights into complex biological
systems, yet their high dimensionality, sparsity, and intricate interactions
pose significant analytical challenges. Network-based approaches have advanced
multi-omics research by effectively capturing biologically relevant
relationships among molecular entities. While these methods are powerful for
representing molecular interactions, there remains a need for tools
specifically designed to effectively utilize these network representations
across diverse downstream analyses. To fulfill this need, we introduce
BioNeuralNet, a flexible and modular Python framework tailored for end-to-end
network-based multi-omics data analysis. BioNeuralNet leverages Graph Neural
Networks (GNNs) to learn biologically meaningful low-dimensional
representations from multi-omics networks, converting these complex molecular
networks into versatile embeddings. BioNeuralNet supports all major stages of
multi-omics network analysis, including several network construction
techniques, generation of low-dimensional representations, and a broad range of
downstream analytical tasks. Its extensive utilities, including diverse GNN
architectures, and compatibility with established Python packages (e.g.,
scikit-learn, PyTorch, NetworkX), enhance usability and facilitate quick
adoption. BioNeuralNet is an open-source, user-friendly, and extensively
documented framework designed to support flexible and reproducible multi-omics
network analysis in precision medicine.

</details>


### [192] [Provable In-Context Learning of Nonlinear Regression with Transformers](https://arxiv.org/abs/2507.20443)
*Hongbo Li,Lingjie Duan,Yingbin Liang*

Main category: cs.LG

TL;DR: 本文研究transformer在非线性回归任务中的上下文学习能力，分析训练时注意力动态，引入新证明技术，确定Lipschitz常数对收敛动态的影响并推导不同时间界。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注简单任务，为推进对上下文学习的理论理解，本文研究更复杂的非线性回归任务。

Method: 分析训练时注意力的阶段性动态，引入新证明技术刻画一般非退化L - Lipschitz任务函数对注意力权重的影响。

Result: 确定Lipschitz常数L是控制transformer在上下文学习中收敛动态的关键因素，针对L不同情况推导不同时间界保证近零预测误差，证明查询令牌在收敛时会关注相关特征的提示令牌。

Conclusion: 证明了transformers对未见函数的上下文学习能力。

Abstract: The transformer architecture, which processes sequences of input tokens to
produce outputs for query tokens, has revolutionized numerous areas of machine
learning. A defining feature of transformers is their ability to perform
previously unseen tasks using task-specific prompts without updating
parameters, a phenomenon known as in-context learning (ICL). Recent research
has actively explored the training dynamics behind ICL, with much of the focus
on relatively simple tasks such as linear regression and binary classification.
To advance the theoretical understanding of ICL, this paper investigates more
complex nonlinear regression tasks, aiming to uncover how transformers acquire
in-context learning capabilities in these settings. We analyze the stage-wise
dynamics of attention during training: attention scores between a query token
and its target features grow rapidly in the early phase, then gradually
converge to one, while attention to irrelevant features decays more slowly and
exhibits oscillatory behavior. Our analysis introduces new proof techniques
that explicitly characterize how the nature of general non-degenerate
L-Lipschitz task functions affects attention weights. Specifically, we identify
that the Lipschitz constant L of nonlinear function classes as a key factor
governing the convergence dynamics of transformers in ICL. Leveraging these
insights, for two distinct regimes depending on whether L is below or above a
threshold, we derive different time bounds to guarantee near-zero prediction
error. Notably, despite the convergence time depending on the underlying task
functions, we prove that query tokens consistently attend to prompt tokens with
highly relevant features at convergence, demonstrating the ICL capability of
transformers for unseen functions.

</details>


### [193] [BOASF: A Unified Framework for Speeding up Automatic Machine Learning via Adaptive Successive Filtering](https://arxiv.org/abs/2507.20446)
*Guanghui Zhu,Xin Fang,Lei Wang,Wenzhong Chen,Rong Gu,Chunfeng Yuan,Yihua Huang*

Main category: cs.LG

TL;DR: 提出BOASF算法在多臂老虎机框架下自动化模型选择和超参数优化，实验表明其有效且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 非专家从业者成功高效处理机器学习任务有挑战，从大量可能选项中找最优模型或超参数组合需专业知识和经验。

Method: 提出BOASF算法，在多轮评估中用贝叶斯优化选各臂有前景配置，用ASF基于高斯UCB概率模型提前丢弃表现差的臂，用Softmax模型为进入下一轮的臂分配资源。

Result: BOASF能加速模型选择和超参数优化过程，预测性能比现有自动机器学习方法更稳健、更好，在不同时间预算下有更好的随时性能。

Conclusion: BOASF算法可有效解决非专家从业者面临的问题，且性能优越。

Abstract: Machine learning has been making great success in many application areas.
However, for the non-expert practitioners, it is always very challenging to
address a machine learning task successfully and efficiently. Finding the
optimal machine learning model or the hyperparameter combination set from a
large number of possible alternatives usually requires considerable expert
knowledge and experience. To tackle this problem, we propose a combined
Bayesian Optimization and Adaptive Successive Filtering algorithm (BOASF) under
a unified multi-armed bandit framework to automate the model selection or the
hyperparameter optimization. Specifically, BOASF consists of multiple
evaluation rounds in each of which we select promising configurations for each
arm using the Bayesian optimization. Then, ASF can early discard the
poor-performed arms adaptively using a Gaussian UCB-based probabilistic model.
Furthermore, a Softmax model is employed to adaptively allocate available
resources for each promising arm that advances to the next round. The arm with
a higher probability of advancing will be allocated more resources.
Experimental results show that BOASF is effective for speeding up the model
selection and hyperparameter optimization processes while achieving robust and
better prediction performance than the existing state-of-the-art automatic
machine learning methods. Moreover, BOASF achieves better anytime performance
under various time budgets.

</details>


### [194] [WEEP: A Differentiable Nonconvex Sparse Regularizer via Weakly-Convex Envelope](https://arxiv.org/abs/2507.20447)
*Takanobu Furuhashi,Hidekata Hontani,Tatsuya Yokota*

Main category: cs.LG

TL;DR: 引入可微稀疏正则化器WEEP，解决统计性能与计算可处理性冲突，在信号和图像去噪任务表现优。


<details>
  <summary>Details</summary>
Motivation: 现有强大稀疏诱导惩罚项不可微，与主流基于梯度优化器冲突。

Method: 引入基于弱凸包络框架的可微稀疏正则化器WEEP。

Result: 在信号和图像去噪任务上比L1范数和其他非凸稀疏正则化器性能更优。

Conclusion: WEEP解决了统计性能和计算可处理性之间的冲突。

Abstract: Sparse regularization is fundamental in signal processing for efficient
signal recovery and feature extraction. However, it faces a fundamental
dilemma: the most powerful sparsity-inducing penalties are often
non-differentiable, conflicting with gradient-based optimizers that dominate
the field. We introduce WEEP (Weakly-convex Envelope of Piecewise Penalty), a
novel, fully differentiable sparse regularizer derived from the weakly-convex
envelope framework. WEEP provides strong, unbiased sparsity while maintaining
full differentiability and L-smoothness, making it natively compatible with any
gradient-based optimizer. This resolves the conflict between statistical
performance and computational tractability. We demonstrate superior performance
compared to the L1-norm and other established non-convex sparse regularizers on
challenging signal and image denoising tasks.

</details>


### [195] [Your Attention Matters: to Improve Model Robustness to Noise and Spurious Correlations](https://arxiv.org/abs/2507.20453)
*Camilo Tamayo-Rousseau,Yunjia Zhao,Yiqun Zhang,Randall Balestriero*

Main category: cs.LG

TL;DR: 研究评估不同数据损坏场景下视觉Transformer中多种自注意力机制的鲁棒性，发现双随机注意力最稳健，为不完美数据场景下自注意力机制选择提供依据。


<details>
  <summary>Details</summary>
Motivation: 现有自注意力机制变体对噪声和虚假相关性的鲁棒性研究不足。

Method: 在CIFAR - 10、CIFAR - 100和Imagenette数据集上测试Softmax、Sigmoid、Linear、Doubly Stochastic和Cosine注意力机制在不同数据损坏场景下的表现。

Result: 双随机注意力机制在测试中表现出最强的鲁棒性。

Conclusion: 研究结果可为不完美数据场景下自注意力机制的选择提供参考。

Abstract: Self-attention mechanisms are foundational to Transformer architectures,
supporting their impressive success in a wide range of tasks. While there are
many self-attention variants, their robustness to noise and spurious
correlations has not been well studied. This study evaluates Softmax, Sigmoid,
Linear, Doubly Stochastic, and Cosine attention within Vision Transformers
under different data corruption scenarios. Through testing across the CIFAR-10,
CIFAR-100, and Imagenette datasets, we show that Doubly Stochastic attention is
the most robust. Our findings inform self-attention selection in contexts with
imperfect data.

</details>


### [196] [Shapley-Value-Based Graph Sparsification for GNN Inference](https://arxiv.org/abs/2507.20460)
*Selahattin Akkas,Ariful Azad*

Main category: cs.LG

TL;DR: 本文介绍基于Shapley值的图稀疏化方法，能在保持预测性能的同时降低图复杂度，提升GNN推理的可解释性与效率。


<details>
  <summary>Details</summary>
Motivation: 许多可解释性方法仅产生非负分数，限制了图稀疏化的适用性，需更好的方法。

Method: 使用基于Shapley值的方法，为节点预测分配正负贡献，评估图的许多子集。

Result: 基于Shapley值的图稀疏化能保持预测性能，显著降低图复杂度。

Conclusion: 基于Shapley值的图稀疏化可提升GNN推理的可解释性与效率。

Abstract: Graph sparsification is a key technique for improving inference efficiency in
Graph Neural Networks by removing edges with minimal impact on predictions. GNN
explainability methods generate local importance scores, which can be
aggregated into global scores for graph sparsification. However, many
explainability methods produce only non-negative scores, limiting their
applicability for sparsification. In contrast, Shapley value based methods
assign both positive and negative contributions to node predictions, offering a
theoretically robust and fair allocation of importance by evaluating many
subsets of graphs. Unlike gradient-based or perturbation-based explainers,
Shapley values enable better pruning strategies that preserve influential edges
while removing misleading or adversarial connections. Our approach shows that
Shapley value-based graph sparsification maintains predictive performance while
significantly reducing graph complexity, enhancing both interpretability and
efficiency in GNN inference.

</details>


### [197] [Conditional Diffusion Models for Global Precipitation Map Inpainting](https://arxiv.org/abs/2507.20478)
*Daiko Kishikawa,Yuka Muto,Shunji Kotsuki*

Main category: cs.LG

TL;DR: 为解决卫星降水数据缺失问题，将降水图补全转化为视频修复任务，用条件扩散模型完成，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 卫星降水数据不完整给全球监测带来挑战，现有插值方法有空间不连续性问题。

Method: 将降水图补全作为视频修复任务，采用基于条件扩散模型的机器学习方法，用3D U - Net和3D条件编码器，利用红外图像、经纬度网格和物理时间输入的时空信息重建完整降水图，在2020 - 2023年ERA5小时降水数据上训练，生成伪GSMaP数据集。

Result: 在2024年评估中，该方法生成的修复降水图在时空上比传统方法更一致。

Conclusion: 条件扩散模型有改善全球降水监测的潜力。

Abstract: Incomplete satellite-based precipitation presents a significant challenge in
global monitoring. For example, the Global Satellite Mapping of Precipitation
(GSMaP) from JAXA suffers from substantial missing regions due to the orbital
characteristics of satellites that have microwave sensors, and its current
interpolation methods often result in spatial discontinuities. In this study,
we formulate the completion of the precipitation map as a video inpainting task
and propose a machine learning approach based on conditional diffusion models.
Our method employs a 3D U-Net with a 3D condition encoder to reconstruct
complete precipitation maps by leveraging spatio-temporal information from
infrared images, latitude-longitude grids, and physical time inputs. Training
was carried out on ERA5 hourly precipitation data from 2020 to 2023. We
generated a pseudo-GSMaP dataset by randomly applying GSMaP masks to ERA maps.
Performance was evaluated for the calendar year 2024, and our approach produces
more spatio-temporally consistent inpainted precipitation maps compared to
conventional methods. These results indicate the potential to improve global
precipitation monitoring using the conditional diffusion models.

</details>


### [198] [HIAL: A New Paradigm for Hypergraph Active Learning via Influence Maximization](https://arxiv.org/abs/2507.20490)
*Yanheng Hou,Xunkai Li,Zhenjun Li,Bing Zhou,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 论文提出专为超图设计的主动学习框架 HIAL，将超图主动学习问题转化为影响力最大化任务，实验表明其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 获取超图神经网络大规模高质量标注数据成本高，现有图主动学习方法应用于超图时会破坏高阶结构信息，导致性能不佳。

Method: 将超图主动学习问题重新表述为影响力最大化任务，采用基于高阶交互感知传播机制的双视角影响函数，证明目标函数单调且子模，使用高效贪心算法。

Result: 在七个公开数据集上的实验显示，HIAL 在性能、效率、通用性和鲁棒性方面显著优于现有基线。

Conclusion: HIAL 为超图上的主动学习建立了高效强大的新范式。

Abstract: In recent years, Hypergraph Neural Networks (HNNs) have demonstrated immense
potential in handling complex systems with high-order interactions. However,
acquiring large-scale, high-quality labeled data for these models is costly,
making Active Learning (AL) a critical technique. Existing Graph Active
Learning (GAL) methods, when applied to hypergraphs, often rely on techniques
like "clique expansion," which destroys the high-order structural information
crucial to a hypergraph's success, thereby leading to suboptimal performance.
To address this challenge, we introduce HIAL (Hypergraph Active Learning), a
native active learning framework designed specifically for hypergraphs. We
innovatively reformulate the Hypergraph Active Learning (HAL) problem as an
Influence Maximization task. The core of HIAL is a dual-perspective influence
function that, based on our novel "High-Order Interaction-Aware (HOI-Aware)"
propagation mechanism, synergistically evaluates a node's feature-space
coverage (via Magnitude of Influence, MoI) and its topological influence (via
Expected Diffusion Value, EDV). We prove that this objective function is
monotone and submodular, thus enabling the use of an efficient greedy algorithm
with a formal (1-1/e) approximation guarantee. Extensive experiments on seven
public datasets demonstrate that HIAL significantly outperforms
state-of-the-art baselines in terms of performance, efficiency, generality, and
robustness, establishing an efficient and powerful new paradigm for active
learning on hypergraphs.

</details>


### [199] [Mixture of Length and Pruning Experts for Knowledge Graphs Reasoning](https://arxiv.org/abs/2507.20498)
*Enjun Du,Siyi Liu,Yongqi Zhang*

Main category: cs.LG

TL;DR: 提出MoKGR框架解决现有GNN在知识图谱推理中路径探索策略的局限性，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在知识图谱推理中采用刚性、与查询无关的路径探索策略，难以适应不同语言上下文和语义细微差别。

Method: 提出MoKGR框架，包含长度专家混合体和剪枝专家混合体两个互补组件，分别自适应选择和加权候选路径长度、评估候选路径。

Result: MoKGR在多种基准测试中，在归纳和直推设置下均表现出优越性能。

Conclusion: 个性化路径探索在知识图谱推理中有效。

Abstract: Knowledge Graph (KG) reasoning, which aims to infer new facts from structured
knowledge repositories, plays a vital role in Natural Language Processing (NLP)
systems. Its effectiveness critically depends on constructing informative and
contextually relevant reasoning paths. However, existing graph neural networks
(GNNs) often adopt rigid, query-agnostic path-exploration strategies, limiting
their ability to adapt to diverse linguistic contexts and semantic nuances. To
address these limitations, we propose \textbf{MoKGR}, a mixture-of-experts
framework that personalizes path exploration through two complementary
components: (1) a mixture of length experts that adaptively selects and weights
candidate path lengths according to query complexity, providing query-specific
reasoning depth; and (2) a mixture of pruning experts that evaluates candidate
paths from a complementary perspective, retaining the most informative paths
for each query. Through comprehensive experiments on diverse benchmark, MoKGR
demonstrates superior performance in both transductive and inductive settings,
validating the effectiveness of personalized path exploration in KGs reasoning.

</details>


### [200] [DmC: Nearest Neighbor Guidance Diffusion Model for Offline Cross-domain Reinforcement Learning](https://arxiv.org/abs/2507.20499)
*Linh Le Pham Van,Minh Hoang Nguyen,Duc Kieu,Hung Le,Hung The Tran,Sunil Gupta*

Main category: cs.LG

TL;DR: 本文提出DmC框架解决有限目标数据下的跨域离线强化学习问题，通过k - NN估计和最近邻引导扩散模型生成源样本，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有跨域离线强化学习方法在有限目标数据场景下需大量目标数据集，不实用，存在数据集不平衡和部分域重叠问题。

Method: 提出DmC框架，用k - NN估计测量域接近度，引入最近邻引导扩散模型生成更符合目标域的源样本。

Result: 在不同MuJoCo环境的理论分析和大量实验表明，DmC显著优于现有跨域离线强化学习方法，取得显著性能提升。

Conclusion: DmC能有效解决有限目标数据下的跨域离线强化学习问题，是一种更优的方法。

Abstract: Cross-domain offline reinforcement learning (RL) seeks to enhance sample
efficiency in offline RL by utilizing additional offline source datasets. A key
challenge is to identify and utilize source samples that are most relevant to
the target domain. Existing approaches address this challenge by measuring
domain gaps through domain classifiers, target transition dynamics modeling, or
mutual information estimation using contrastive loss. However, these methods
often require large target datasets, which is impractical in many real-world
scenarios. In this work, we address cross-domain offline RL under a limited
target data setting, identifying two primary challenges: (1) Dataset imbalance,
which is caused by large source and small target datasets and leads to
overfitting in neural network-based domain gap estimators, resulting in
uninformative measurements; and (2) Partial domain overlap, where only a subset
of the source data is closely aligned with the target domain. To overcome these
issues, we propose DmC, a novel framework for cross-domain offline RL with
limited target samples. Specifically, DmC utilizes $k$-nearest neighbor
($k$-NN) based estimation to measure domain proximity without neural network
training, effectively mitigating overfitting. Then, by utilizing this domain
proximity, we introduce a nearest-neighbor-guided diffusion model to generate
additional source samples that are better aligned with the target domain, thus
enhancing policy learning with more effective source samples. Through
theoretical analysis and extensive experiments in diverse MuJoCo environments,
we demonstrate that DmC significantly outperforms state-of-the-art cross-domain
offline RL methods, achieving substantial performance gains.

</details>


### [201] [Customize Multi-modal RAI Guardrails with Precedent-based predictions](https://arxiv.org/abs/2507.20503)
*Cheng-Fu Yang,Thanh Tran,Christos Christodoulopoulos,Weitong Ruan,Rahul Gupta,Kai-Wei Chang*

Main category: cs.LG

TL;DR: 提出基于‘先例’的多模态护栏方法，增强护栏灵活性和适应性，实验表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态护栏部署面临用户策略多样、难提供大量示例等挑战，现有方法泛化性和适应性不足。

Method: 提出让模型基于‘先例’做判断，引入批判 - 修正机制收集高质量先例，采用两种利用先例进行稳健预测的策略。

Result: 实验表明该方法在少样本和全数据集场景下均优于先前方法，对新策略泛化性更好。

Conclusion: 基于‘先例’的方法能有效提升多模态护栏的灵活性和适应性。

Abstract: A multi-modal guardrail must effectively filter image content based on
user-defined policies, identifying material that may be hateful, reinforce
harmful stereotypes, contain explicit material, or spread misinformation.
Deploying such guardrails in real-world applications, however, poses
significant challenges. Users often require varied and highly customizable
policies and typically cannot provide abundant examples for each custom policy.
Consequently, an ideal guardrail should be scalable to the multiple policies
and adaptable to evolving user standards with minimal retraining. Existing
fine-tuning methods typically condition predictions on pre-defined policies,
restricting their generalizability to new policies or necessitating extensive
retraining to adapt. Conversely, training-free methods struggle with limited
context lengths, making it difficult to incorporate all the policies
comprehensively. To overcome these limitations, we propose to condition model's
judgment on "precedents", which are the reasoning processes of prior data
points similar to the given input. By leveraging precedents instead of fixed
policies, our approach greatly enhances the flexibility and adaptability of the
guardrail. In this paper, we introduce a critique-revise mechanism for
collecting high-quality precedents and two strategies that utilize precedents
for robust prediction. Experimental results demonstrate that our approach
outperforms previous methods across both few-shot and full-dataset scenarios
and exhibits superior generalization to novel policies.

</details>


### [202] [Attributed Graph Clustering with Multi-Scale Weight-Based Pairwise Coarsening and Contrastive Learning](https://arxiv.org/abs/2507.20505)
*Binxiong Li,Yuefei Wang,Binyu Zhao,Heyang Gao,Benhan Yang,Quanzhou Luo,Xue Li,Xu Xiang,Yujie Liu,Huijie Tang*

Main category: cs.LG

TL;DR: 提出MPCCL模型用于属性图聚类，解决现有方法的不足，实验显示聚类性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有属性图聚类方法存在长距离依赖、特征坍缩和信息丢失等问题，传统方法难以捕获高阶图特征，对比学习技术特征多样性有限，传统图粗化方法丢失细节。

Method: 采用创新的多尺度粗化策略，基于全局节点相似度优先合并关键边；引入一对多对比学习范式，结合节点嵌入、增强图视图和簇质心；在自监督学习框架中加入图重建损失和KL散度。

Result: MPCCL在ACM数据集上NMI提高15.24%，在Citeseer、Cora和DBLP等小数据集上也有显著稳健提升。

Conclusion: MPCCL模型能有效解决现有属性图聚类方法的问题，显著提升聚类性能。

Abstract: This study introduces the Multi-Scale Weight-Based Pairwise Coarsening and
Contrastive Learning (MPCCL) model, a novel approach for attributed graph
clustering that effectively bridges critical gaps in existing methods,
including long-range dependency, feature collapse, and information loss.
Traditional methods often struggle to capture high-order graph features due to
their reliance on low-order attribute information, while contrastive learning
techniques face limitations in feature diversity by overemphasizing local
neighborhood structures. Similarly, conventional graph coarsening methods,
though reducing graph scale, frequently lose fine-grained structural details.
MPCCL addresses these challenges through an innovative multi-scale coarsening
strategy, which progressively condenses the graph while prioritizing the
merging of key edges based on global node similarity to preserve essential
structural information. It further introduces a one-to-many contrastive
learning paradigm, integrating node embeddings with augmented graph views and
cluster centroids to enhance feature diversity, while mitigating feature
masking issues caused by the accumulation of high-frequency node weights during
multi-scale coarsening. By incorporating a graph reconstruction loss and KL
divergence into its self-supervised learning framework, MPCCL ensures
cross-scale consistency of node representations. Experimental evaluations
reveal that MPCCL achieves a significant improvement in clustering performance,
including a remarkable 15.24% increase in NMI on the ACM dataset and notable
robust gains on smaller-scale datasets such as Citeseer, Cora and DBLP.

</details>


### [203] [Efficient Proxy Raytracer for Optical Systems using Implicit Neural Representations](https://arxiv.org/abs/2507.20513)
*Shiva Sinaei,Chuanjun Zheng,Kaan Akşit,Daisuke Iwai*

Main category: cs.LG

TL;DR: 提出Ray2Ray方法用隐式神经表示高效建模光学系统，在九个光学系统训练取得不错精度，凸显神经表示用于光学光线追踪潜力。


<details>
  <summary>Details</summary>
Motivation: 传统光线追踪技术计算密集，需逐面计算，希望找到更高效的光学系统建模方法。

Method: 提出Ray2Ray方法，利用隐式神经表示构建端到端模型，学习给定光源发出的光线与通过光学系统后对应光线的映射。

Result: 在九个现成光学系统上训练，估计输出光线的位置误差约为1μm，角度偏差约为0.01度。

Conclusion: 神经表示有潜力作为光学光线追踪器的代理。

Abstract: Ray tracing is a widely used technique for modeling optical systems,
involving sequential surface-by-surface computations, which can be
computationally intensive. We propose Ray2Ray, a novel method that leverages
implicit neural representations to model optical systems with greater
efficiency, eliminating the need for surface-by-surface computations in a
single pass end-to-end model. Ray2Ray learns the mapping between rays emitted
from a given source and their corresponding rays after passing through a given
optical system in a physically accurate manner. We train Ray2Ray on nine
off-the-shelf optical systems, achieving positional errors on the order of
1{\mu}m and angular deviations on the order 0.01 degrees in the estimated
output rays. Our work highlights the potential of neural representations as a
proxy for optical raytracer.

</details>


### [204] [Kernel Learning for Sample Constrained Black-Box Optimization](https://arxiv.org/abs/2507.20533)
*Rajalaxmi Rajagopalan,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.LG

TL;DR: 提出一种学习高斯过程核的新方法KOBO，在低样本预算下优于现有技术，在合成基准函数和实际应用中均有效。


<details>
  <summary>Details</summary>
Motivation: 黑盒优化中采样未知函数成本高，现有工作致力于通过核学习降低样本预算。

Method: 在变分自编码器的潜在空间创建连续核空间，运行辅助优化来确定最佳核。

Result: 提出的KOBO方法在低样本预算下估计最优值时优于现有技术，在合成基准函数和实际应用中都有良好表现。

Conclusion: KOBO方法能在低样本预算下取得良好优化效果，如减少助听器个性化的音频查询次数、使生成模型从有限用户评分中收敛到理想图像。

Abstract: Black box optimization (BBO) focuses on optimizing unknown functions in
high-dimensional spaces. In many applications, sampling the unknown function is
expensive, imposing a tight sample budget. Ongoing work is making progress on
reducing the sample budget by learning the shape/structure of the function,
known as kernel learning. We propose a new method to learn the kernel of a
Gaussian Process. Our idea is to create a continuous kernel space in the latent
space of a variational autoencoder, and run an auxiliary optimization to
identify the best kernel. Results show that the proposed method, Kernel
Optimized Blackbox Optimization (KOBO), outperforms state of the art by
estimating the optimal at considerably lower sample budgets. Results hold not
only across synthetic benchmark functions but also in real applications. We
show that a hearing aid may be personalized with fewer audio queries to the
user, or a generative model could converge to desirable images from limited
user ratings.

</details>


### [205] [Kimi K2: Open Agentic Intelligence](https://arxiv.org/abs/2507.20534)
*Kimi Team,Yifan Bai,Yiping Bao,Guanduo Chen,Jiahao Chen,Ningxin Chen,Ruijue Chen,Yanru Chen,Yuankun Chen,Yutian Chen,Zhuofu Chen,Jialei Cui,Hao Ding,Mengnan Dong,Angang Du,Chenzhuang Du,Dikang Du,Yulun Du,Yu Fan,Yichen Feng,Kelin Fu,Bofei Gao,Hongcheng Gao,Peizhong Gao,Tong Gao,Xinran Gu,Longyu Guan,Haiqing Guo,Jianhang Guo,Hao Hu,Xiaoru Hao,Tianhong He,Weiran He,Wenyang He,Chao Hong,Yangyang Hu,Zhenxing Hu,Weixiao Huang,Zhiqi Huang,Zihao Huang,Tao Jiang,Zhejun Jiang,Xinyi Jin,Yongsheng Kang,Guokun Lai,Cheng Li,Fang Li,Haoyang Li,Ming Li,Wentao Li,Yanhao Li,Yiwei Li,Zhaowei Li,Zheming Li,Hongzhan Lin,Xiaohan Lin,Zongyu Lin,Chengyin Liu,Chenyu Liu,Hongzhang Liu,Jingyuan Liu,Junqi Liu,Liang Liu,Shaowei Liu,T. Y. Liu,Tianwei Liu,Weizhou Liu,Yangyang Liu,Yibo Liu,Yiping Liu,Yue Liu,Zhengying Liu,Enzhe Lu,Lijun Lu,Shengling Ma,Xinyu Ma,Yingwei Ma,Shaoguang Mao,Jie Mei,Xin Men,Yibo Miao,Siyuan Pan,Yebo Peng,Ruoyu Qin,Bowen Qu,Zeyu Shang,Lidong Shi,Shengyuan Shi,Feifan Song,Jianlin Su,Zhengyuan Su,Xinjie Sun,Flood Sung,Heyi Tang,Jiawen Tao,Qifeng Teng,Chensi Wang,Dinglu Wang,Feng Wang,Haiming Wang,Jianzhou Wang,Jiaxing Wang,Jinhong Wang,Shengjie Wang,Shuyi Wang,Yao Wang,Yejie Wang,Yiqin Wang,Yuxin Wang,Yuzhi Wang,Zhaoji Wang,Zhengtao Wang,Zhexu Wang,Chu Wei,Qianqian Wei,Wenhao Wu,Xingzhe Wu,Yuxin Wu,Chenjun Xiao,Xiaotong Xie,Weimin Xiong,Boyu Xu,Jing Xu,Jinjing Xu,L. H. Xu,Lin Xu,Suting Xu,Weixin Xu,Xinran Xu,Yangchuan Xu,Ziyao Xu,Junjie Yan,Yuzi Yan,Xiaofei Yang,Ying Yang,Zhen Yang,Zhilin Yang,Zonghan Yang,Haotian Yao,Xingcheng Yao,Wenjie Ye,Zhuorui Ye,Bohong Yin,Longhui Yu,Enming Yuan,Hongbang Yuan,Mengjie Yuan,Haobing Zhan,Dehao Zhang,Hao Zhang,Wanlu Zhang,Xiaobin Zhang,Yangkun Zhang,Yizhi Zhang,Yongting Zhang,Yu Zhang,Yutao Zhang,Yutong Zhang,Zheng Zhang,Haotian Zhao,Yikai Zhao,Huabin Zheng,Shaojie Zheng,Jianren Zhou,Xinyu Zhou,Zaida Zhou,Zhen Zhu,Weiyu Zhuang,Xinxing Zu*

Main category: cs.LG

TL;DR: 介绍320亿激活参数、1万亿总参数的MoE大模型Kimi K2，采用MuonClip优化器训练，经多阶段训练后在多基准测试表现优异，尤其在软件工程和智能体任务，发布模型促进研究。


<details>
  <summary>Details</summary>
Motivation: 构建高性能且适用于智能体任务的开源大语言模型。

Method: 提出MuonClip优化器解决训练不稳定；在15.5万亿token上预训练；采用多阶段后训练，含大规模智能体数据合成管道和联合强化学习阶段。

Result: Kimi K2在多项基准测试中取得领先成绩，如Tau2 - Bench得66.1等，在编码、数学和推理任务表现强。

Conclusion: Kimi K2是目前最有能力的开源大语言模型之一，尤其在软件工程和智能体任务，发布模型利于未来研究和应用。

Abstract: We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32
billion activated parameters and 1 trillion total parameters. We propose the
MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to
address training instability while enjoying the advanced token efficiency of
Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero
loss spike. During post-training, K2 undergoes a multi-stage post-training
process, highlighted by a large-scale agentic data synthesis pipeline and a
joint reinforcement learning (RL) stage, where the model improves its
capabilities through interactions with real and synthetic environments.
  Kimi K2 achieves state-of-the-art performance among open-source non-thinking
models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on
Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on
SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in
non-thinking settings. It also exhibits strong capabilities in coding,
mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6,
49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without
extended thinking. These results position Kimi K2 as one of the most capable
open-source large language models to date, particularly in software engineering
and agentic tasks. We release our base and post-trained model checkpoints to
facilitate future research and applications of agentic intelligence.

</details>


### [206] [DAG-AFL:Directed Acyclic Graph-based Asynchronous Federated Learning](https://arxiv.org/abs/2507.20571)
*Shuaipeng Zhang,Lanju Kong,Yixin Zhang,Wei He,Yongqing Zheng,Han Yu,Lizhen Cui*

Main category: cs.LG

TL;DR: 提出基于有向无环图的异步联邦学习框架DAG - AFL，实验显示可提升训练效率和模型准确率。


<details>
  <summary>Details</summary>
Motivation: 传统区块链共识机制资源消耗大，影响联邦学习效率，需解决异步客户端参与和数据异构问题并限制区块链引入的额外资源开销。

Method: 提出DAG - AFL框架，开发考虑时间新鲜度、节点可达性和模型准确性的提示选择算法及基于DAG的可信验证策略。

Result: 在3个基准数据集上与8种最先进方法对比，DAG - AFL平均分别提高训练效率22.7%和模型准确率6.5%。

Conclusion: DAG - AFL能有效提高联邦学习的训练效率和模型准确率。

Abstract: Due to the distributed nature of federated learning (FL), the vulnerability
of the global model and the need for coordination among many client devices
pose significant challenges. As a promising decentralized, scalable and secure
solution, blockchain-based FL methods have attracted widespread attention in
recent years. However, traditional consensus mechanisms designed for Proof of
Work (PoW) similar to blockchain incur substantial resource consumption and
compromise the efficiency of FL, particularly when participating devices are
wireless and resource-limited. To address asynchronous client participation and
data heterogeneity in FL, while limiting the additional resource overhead
introduced by blockchain, we propose the Directed Acyclic Graph-based
Asynchronous Federated Learning (DAG-AFL) framework. We develop a tip selection
algorithm that considers temporal freshness, node reachability and model
accuracy, with a DAG-based trusted verification strategy. Extensive experiments
on 3 benchmarking datasets against eight state-of-the-art approaches
demonstrate that DAG-AFL significantly improves training efficiency and model
accuracy by 22.7% and 6.5% on average, respectively.

</details>


### [207] [Reminiscence Attack on Residuals: Exploiting Approximate Machine Unlearning for Privacy](https://arxiv.org/abs/2507.20573)
*Yaxin Xiao,Qingqing Ye,Li Hu,Huadi Zheng,Haibo Hu,Zi Liang,Haoyang Li,Yijie Jiao*

Main category: cs.LG

TL;DR: 研究发现近似无学习算法不能充分保护被遗忘数据隐私，提出回忆攻击（ReA）并开发双阶段近似无学习框架，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 近似无学习算法无法充分保护被遗忘数据的隐私，存在隐式残差易引发隐私攻击。

Method: 提出回忆攻击（ReA）放大残差与成员隐私的关联；开发双阶段近似无学习框架，先消除深层被遗忘数据痕迹，再确保收敛稳定性。

Result: ReA在推断类别和样本成员身份时准确率分别比先前攻击高1.90倍和1.12倍；双阶段框架降低自适应隐私攻击准确率至接近随机猜测，计算成本为从头完全重新训练的2 - 12%。

Conclusion: 所提出的框架在分类和生成任务中有效，能保持高无学习效率，降低隐私攻击风险。

Abstract: Machine unlearning enables the removal of specific data from ML models to
uphold the right to be forgotten. While approximate unlearning algorithms offer
efficient alternatives to full retraining, this work reveals that they fail to
adequately protect the privacy of unlearned data. In particular, these
algorithms introduce implicit residuals which facilitate privacy attacks
targeting at unlearned data. We observe that these residuals persist regardless
of model architectures, parameters, and unlearning algorithms, exposing a new
attack surface beyond conventional output-based leakage. Based on this insight,
we propose the Reminiscence Attack (ReA), which amplifies the correlation
between residuals and membership privacy through targeted fine-tuning
processes. ReA achieves up to 1.90x and 1.12x higher accuracy than prior
attacks when inferring class-wise and sample-wise membership, respectively. To
mitigate such residual-induced privacy risk, we develop a dual-phase
approximate unlearning framework that first eliminates deep-layer unlearned
data traces and then enforces convergence stability to prevent models from
"pseudo-convergence", where their outputs are similar to retrained models but
still preserve unlearned residuals. Our framework works for both classification
and generation tasks. Experimental evaluations confirm that our approach
maintains high unlearning efficacy, while reducing the adaptive privacy attack
accuracy to nearly random guess, at the computational cost of 2-12% of full
retraining from scratch.

</details>


### [208] [Fusing CFD and measurement data using transfer learning](https://arxiv.org/abs/2507.20576)
*Alexander Barklage,Philipp Bekemeyer*

Main category: cs.LG

TL;DR: 本文提出基于神经网络和迁移学习的非线性方法融合飞机气动分析的模拟与测量数据，效果优于现有方法，且训练策略通用。


<details>
  <summary>Details</summary>
Motivation: 飞机气动分析不同方法各有优劣，现有分布式数据融合方法为线性，需非线性方法有效结合优势。

Method: 用神经网络结合模拟与测量数据，先在模拟数据上训练学习空间特征，再用迁移学习在测量数据上微调部分网络以修正误差。

Result: 应用于多层感知器架构，相比基于本征正交分解的方法有显著改进，能产生更符合物理规律的解，可在任意流动条件下提供解。

Conclusion: 该方法有效，训练策略通用，未来可应用于更复杂神经网络架构。

Abstract: Aerodynamic analysis during aircraft design usually involves methods of
varying accuracy and spatial resolution, which all have their advantages and
disadvantages. It is therefore desirable to create data-driven models which
effectively combine these advantages. Such data fusion methods for distributed
quantities mainly rely on proper orthogonal decomposition as of now, which is a
linear method. In this paper, we introduce a non-linear method based on neural
networks combining simulation and measurement data via transfer learning. The
network training accounts for the heterogeneity of the data, as simulation data
usually features a high spatial resolution, while measurement data is sparse
but more accurate. In a first step, the neural network is trained on simulation
data to learn spatial features of the distributed quantities. The second step
involves transfer learning on the measurement data to correct for systematic
errors between simulation and measurement by only re-training a small subset of
the entire neural network model. This approach is applied to a multilayer
perceptron architecture and shows significant improvements over the established
method based on proper orthogonal decomposition by producing more physical
solutions near nonlinearities. In addition, the neural network provides
solutions at arbitrary flow conditions, thus making the model useful for flight
mechanical design, structural sizing, and certification. As the proposed
training strategy is very general, it can also be applied to more complex
neural network architectures in the future.

</details>


### [209] [PhaseNAS: Language-Model Driven Architecture Search with Dynamic Phase Adaptation](https://arxiv.org/abs/2507.20592)
*Fei Kong,Xiaohan Shan,Yanwei Hu,Jianmin Li*

Main category: cs.LG

TL;DR: 提出PhaseNAS框架用于NAS，在多视觉任务有高效、自适应和可泛化优势。


<details>
  <summary>Details</summary>
Motivation: 解决NAS在搜索空间探索和效率间的权衡问题，改进现有LLM - based NAS方法静态搜索策略和架构表示模糊的缺点。

Method: 提出基于LLM的PhaseNAS框架，有由实时分数阈值引导的动态阶段转换和结构化架构模板语言。

Result: 在NAS - Bench - Macro基准上发现更优架构；在图像分类减少搜索时间并维持或提升精度；在目标检测自动生成更优YOLOv8变体。

Conclusion: PhaseNAS能在不同视觉任务实现高效、自适应和可泛化的NAS。

Abstract: Neural Architecture Search (NAS) is challenged by the trade-off between
search space exploration and efficiency, especially for complex tasks. While
recent LLM-based NAS methods have shown promise, they often suffer from static
search strategies and ambiguous architecture representations. We propose
PhaseNAS, an LLM-based NAS framework with dynamic phase transitions guided by
real-time score thresholds and a structured architecture template language for
consistent code generation. On the NAS-Bench-Macro benchmark, PhaseNAS
consistently discovers architectures with higher accuracy and better rank. For
image classification (CIFAR-10/100), PhaseNAS reduces search time by up to 86%
while maintaining or improving accuracy. In object detection, it automatically
produces YOLOv8 variants with higher mAP and lower resource cost. These results
demonstrate that PhaseNAS enables efficient, adaptive, and generalizable NAS
across diverse vision tasks.

</details>


### [210] [Deep Generative Models of Evolution: SNP-level Population Adaptation by Genomic Linkage Incorporation](https://arxiv.org/abs/2507.20644)
*Julia Siekiera,Christian Schlötterer,Stefan Kramer*

Main category: cs.LG

TL;DR: 本文引入深度生成神经网络来处理基于Pool - Seq数据的E&R实验挑战，评估显示该模型能捕捉等位基因频率轨迹分布，在LD估计上有竞争力。


<details>
  <summary>Details</summary>
Motivation: 经典统计模型如Wright - Fisher模型有简化假设和参数不确定问题，深度生成神经网络虽有优势但在群体基因组学领域未广泛应用，需解决E&R实验基于Pool - Seq数据的挑战。

Method: 引入深度生成神经网络，通过嵌入SNP观察值和相邻位点信息来估计等位基因频率轨迹分布。

Result: 在模拟E&R实验中，模型能捕捉等位基因频率轨迹分布，可估计成对LD，在高LD的Pool - Seq数据中LD估计有竞争力。

Conclusion: 提出的深度生成神经网络模型在处理E&R实验基于Pool - Seq数据方面表现良好，为群体基因组学研究提供了新方法。

Abstract: The investigation of allele frequency trajectories in populations evolving
under controlled environmental pressures has become a popular approach to study
evolutionary processes on the molecular level. Statistical models based on
well-defined evolutionary concepts can be used to validate different hypotheses
about empirical observations. Despite their popularity, classic statistical
models like the Wright-Fisher model suffer from simplified assumptions such as
the independence of selected loci along a chromosome and uncertainty about the
parameters. Deep generative neural networks offer a powerful alternative known
for the integration of multivariate dependencies and noise reduction. Due to
their high data demands and challenging interpretability they have, so far, not
been widely considered in the area of population genomics. To address the
challenges in the area of Evolve and Resequencing experiments (E&R) based on
pooled sequencing (Pool-Seq) data, we introduce a deep generative neural
network that aims to model a concept of evolution based on empirical
observations over time. The proposed model estimates the distribution of allele
frequency trajectories by embedding the observations from single nucleotide
polymorphisms (SNPs) with information from neighboring loci. Evaluation on
simulated E&R experiments demonstrates the model's ability to capture the
distribution of allele frequency trajectories and illustrates the
representational power of deep generative models on the example of linkage
disequilibrium (LD) estimation. Inspecting the internally learned
representations enables estimating pairwise LD, which is typically inaccessible
in Pool-Seq data. Our model provides competitive LD estimation in Pool-Seq data
high degree of LD when compared to existing methods.

</details>


### [211] [Novel Pivoted Cholesky Decompositions for Efficient Gaussian Process Inference](https://arxiv.org/abs/2507.20678)
*Filip de Roos,Fabio Muratore*

Main category: cs.LG

TL;DR: 本文探讨Cholesky分解的选主元策略，推导出新策略，在高斯过程任务中测试，新策略表现佳且计算开销小。


<details>
  <summary>Details</summary>
Motivation: Cholesky分解选主元顺序影响算法效率，在贝叶斯非参数推断中标准选主元策略对应贪心熵最大化，探索此联系以推导新策略。

Method: 详细探索联系推导新选主元策略，在高斯过程的稀疏回归和基于预条件迭代求解器的推断两个任务上进行基准测试。

Result: 新选主元策略在减少数据集不确定性上更高效，能更新以包含观测信息，且在多数情况下优于传统基线，额外计算量可忽略不计。

Conclusion: 提出的Cholesky分解选主元新策略有效，具有实际应用价值。

Abstract: The Cholesky decomposition is a fundamental tool for solving linear systems
with symmetric and positive definite matrices which are ubiquitous in linear
algebra, optimization, and machine learning. Its numerical stability can be
improved by introducing a pivoting strategy that iteratively permutes the rows
and columns of the matrix. The order of pivoting indices determines how
accurately the intermediate decomposition can reconstruct the original matrix,
thus is decisive for the algorithm's efficiency in the case of early
termination. Standard implementations select the next pivot from the largest
value on the diagonal. In the case of Bayesian nonparametric inference, this
strategy corresponds to greedy entropy maximization, which is often used in
active learning and design of experiments. We explore this connection in detail
and deduce novel pivoting strategies for the Cholesky decomposition. The
resulting algorithms are more efficient at reducing the uncertainty over a data
set, can be updated to include information about observations, and additionally
benefit from a tailored implementation. We benchmark the effectiveness of the
new selection strategies on two tasks important to Gaussian processes: sparse
regression and inference based on preconditioned iterative solvers. Our results
show that the proposed selection strategies are either on par or, in most
cases, outperform traditional baselines while requiring a negligible amount of
additional computation.

</details>


### [212] [Exposing the Illusion of Fairness: Auditing Vulnerabilities to Distributional Manipulation Attacks](https://arxiv.org/abs/2507.20708)
*Valentin Lafargue,Adriana Laurindo Monteiro,Emmanuelle Claeys,Laurent Risser,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 研究如何操纵数据样本满足公平标准及检测此类操纵，通过实验验证结果。


<details>
  <summary>Details</summary>
Motivation: 随着AI算法广泛应用，证明其合规性成重要挑战，需检测可能的偏见行为以满足欧盟AI法案规定，而现有全局公平指标依赖样本分布。

Method: 引入基于熵或最优传输投影在公平约束下修改经验分布的数学方法，研究受审核方规避公平检查的可能方式。

Result: 提出操纵数据样本满足公平标准及检测此类操纵的方法，结果在经典表格数据集的偏差检测实验中得到验证。

Conclusion: 为审计人员提供检测数据操纵的建议。

Abstract: Proving the compliance of AI algorithms has become an important challenge
with the growing deployment of such algorithms for real-life applications.
Inspecting possible biased behaviors is mandatory to satisfy the constraints of
the regulations of the EU Artificial Intelligence's Act. Regulation-driven
audits increasingly rely on global fairness metrics, with Disparate Impact
being the most widely used. Yet such global measures depend highly on the
distribution of the sample on which the measures are computed. We investigate
first how to manipulate data samples to artificially satisfy fairness criteria,
creating minimally perturbed datasets that remain statistically
indistinguishable from the original distribution while satisfying prescribed
fairness constraints. Then we study how to detect such manipulation. Our
analysis (i) introduces mathematically sound methods for modifying empirical
distributions under fairness constraints using entropic or optimal transport
projections, (ii) examines how an auditee could potentially circumvent fairness
inspections, and (iii) offers recommendations to help auditors detect such data
manipulations. These results are validated through experiments on classical
tabular datasets in bias detection.

</details>


### [213] [Prostate Cancer Classification Using Multimodal Feature Fusion and Explainable AI](https://arxiv.org/abs/2507.20714)
*Asma Sadia Khan,Fariba Tasnia Khan,Tanjim Mahmud,Salman Karim Khan,Rishita Chakma,Nahed Sharmen,Mohammad Shahadat Hossain,Karl Andersson*

Main category: cs.LG

TL;DR: 提出结合BERT和随机森林的可解释AI系统用于前列腺癌诊断，在数据集上表现优，兼顾高性能、计算效率和临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 前列腺癌作为常见男性恶性肿瘤，需要先进诊断工具。

Method: 提出结合BERT处理文本临床笔记和随机森林处理数值实验室数据的可解释AI系统，采用新颖多模态融合策略。

Result: 在PLCO - NIH数据集上取得98%准确率、99% AUC；中间癌症阶段召回率有显著提升；SHAP分析给出特征重要性排名，消融研究证明文本特征互补价值。

Conclusion: 该方法平衡了高性能（F1 = 89%）、计算效率和临床可解释性，满足前列腺癌诊断的关键需求。

Abstract: Prostate cancer, the second most prevalent male malignancy, requires advanced
diagnostic tools. We propose an explainable AI system combining BERT (for
textual clinical notes) and Random Forest (for numerical lab data) through a
novel multimodal fusion strategy, achieving superior classification performance
on PLCO-NIH dataset (98% accuracy, 99% AUC). While multimodal fusion is
established, our work demonstrates that a simple yet interpretable BERT+RF
pipeline delivers clinically significant improvements - particularly for
intermediate cancer stages (Class 2/3 recall: 0.900 combined vs 0.824
numerical/0.725 textual). SHAP analysis provides transparent feature importance
rankings, while ablation studies prove textual features' complementary value.
This accessible approach offers hospitals a balance of high performance
(F1=89%), computational efficiency, and clinical interpretability - addressing
critical needs in prostate cancer diagnostics.

</details>


### [214] [Uncertainty-driven Embedding Convolution](https://arxiv.org/abs/2507.20718)
*Sungjun Lim,Kangjun Noh,Youngjun Choi,Heeyoung Lee,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出不确定性驱动的嵌入卷积（UEC）方法，将确定性嵌入转换为概率嵌入，计算自适应集成权重，引入不确定性感知相似度函数，实验表明UEC提升性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有文本嵌入模型在不同领域性能各异，且多数集成方法基于确定性嵌入，未考虑模型特定不确定性，限制下游应用的鲁棒性和可靠性。

Method: 提出UEC方法，先将确定性嵌入后验转换为概率嵌入，基于嵌入不确定性计算自适应集成权重，引入不确定性感知相似度函数。

Result: 在检索、分类和语义相似度基准测试中，UEC通过利用原则性不确定性建模，持续提升性能和鲁棒性。

Conclusion: UEC方法能有效解决现有文本嵌入集成方法的局限性，提升性能和鲁棒性。

Abstract: Text embeddings are essential components in modern NLP pipelines. While
numerous embedding models have been proposed, their performance varies across
domains, and no single model consistently excels across all tasks. This
variability motivates the use of ensemble techniques to combine complementary
strengths. However, most existing ensemble methods operate on deterministic
embeddings and fail to account for model-specific uncertainty, limiting their
robustness and reliability in downstream applications. To address these
limitations, we propose Uncertainty-driven Embedding Convolution (UEC). UEC
first transforms deterministic embeddings into probabilistic ones in a post-hoc
manner. It then computes adaptive ensemble weights based on embedding
uncertainty, grounded in a Bayes-optimal solution under a surrogate loss.
Additionally, UEC introduces an uncertainty-aware similarity function that
directly incorporates uncertainty into similarity scoring. Extensive
experiments on retrieval, classification, and semantic similarity benchmarks
demonstrate that UEC consistently improves both performance and robustness by
leveraging principled uncertainty modeling.

</details>


### [215] [First Hallucination Tokens Are Different from Conditional Ones](https://arxiv.org/abs/2507.20836)
*Jakob Snel,Seong Joon Oh*

Main category: cs.LG

TL;DR: 分析基础模型幻觉问题，利用RAGTruth语料库研究token级幻觉信号，发现首个幻觉token信号更强更易检测，并开源分析框架。


<details>
  <summary>Details</summary>
Motivation: 基础模型存在幻觉问题，检测token级幻觉对实时过滤和针对性修正很重要，但token序列中幻觉信号的变化尚不清楚。

Method: 利用带有token级注释和重现对数的RAGTruth语料库，分析幻觉信号与token在幻觉跨度内位置的关系。

Result: 首个幻觉token比条件token信号更强、更易检测。

Conclusion: 有助于更好地理解token级幻觉，开源分析框架利于后续研究。

Abstract: Hallucination, the generation of untruthful content, is one of the major
concerns regarding foundational models. Detecting hallucinations at the token
level is vital for real-time filtering and targeted correction, yet the
variation of hallucination signals within token sequences is not fully
understood. Leveraging the RAGTruth corpus with token-level annotations and
reproduced logits, we analyse how these signals depend on a token's position
within hallucinated spans, contributing to an improved understanding of
token-level hallucination. Our results show that the first hallucinated token
carries a stronger signal and is more detectable than conditional tokens. We
release our analysis framework, along with code for logit reproduction and
metric computation at https://github.com/jakobsnl/RAGTruth_Xtended.

</details>


### [216] [BuildSTG: A Multi-building Energy Load Forecasting Method using Spatio-Temporal Graph Neural Network](https://arxiv.org/abs/2507.20838)
*Yongzheng Liu,Yiming Wang,Po Xu,Yingjie Xu,Yuntian Chen,Dongxiao Zhang*

Main category: cs.LG

TL;DR: 提出用时空图神经网络的多建筑能耗预测方法，实验显示性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统预测方法难以捕捉建筑运行数据中的空间依赖关系，而具有相似特征的建筑有共同能耗模式。

Method: 提出包含图表示、图学习和解释的多建筑预测方法，先基于建筑特征和环境因素构建图，再开发带注意力的多级图卷积架构进行能耗预测，最后引入解释优化图结构的方法。

Result: 在Building Data Genome Project 2数据集上实验，性能优于XGBoost、SVR等基线模型。

Conclusion: 该方法在捕捉建筑相似性和空间关系方面有鲁棒性、泛化性和可解释性。

Abstract: Due to the extensive availability of operation data, data-driven methods show
strong capabilities in predicting building energy loads. Buildings with similar
features often share energy patterns, reflected by spatial dependencies in
their operational data, which conventional prediction methods struggle to
capture. To overcome this, we propose a multi-building prediction approach
using spatio-temporal graph neural networks, comprising graph representation,
graph learning, and interpretation. First, a graph is built based on building
characteristics and environmental factors. Next, a multi-level graph
convolutional architecture with attention is developed for energy prediction.
Lastly, a method interpreting the optimized graph structure is introduced.
Experiments on the Building Data Genome Project 2 dataset confirm superior
performance over baselines such as XGBoost, SVR, FCNN, GRU, and Naive,
highlighting the method's robustness, generalization, and interpretability in
capturing meaningful building similarities and spatial relationships.

</details>


### [217] [Towards Explainable Deep Clustering for Time Series Data](https://arxiv.org/abs/2507.20840)
*Udo Schlegel,Gabriel Marques Tavares,Thomas Seidl*

Main category: cs.LG

TL;DR: 本文对时间序列可解释深度聚类进行综述，分析现有方法，指出不足并提出六个研究机会，为下一代可信分析打基础。


<details>
  <summary>Details</summary>
Motivation: 深度聚类决策不透明限制其在安全关键场景应用，需对时间序列可解释深度聚类进行结构化概述。

Method: 全面讨论和比较医疗、金融、物联网和气候科学等领域同行评审和预印本论文。

Result: 多数工作依赖自编码器和注意力架构，对特定类型时间序列支持有限，可解释性多为附加功能。

Conclusion: 提出六个研究机会，应将可解释性作为主要设计目标，为下一代分析奠定基础。

Abstract: Deep clustering uncovers hidden patterns and groups in complex time series
data, yet its opaque decision-making limits use in safety-critical settings.
This survey offers a structured overview of explainable deep clustering for
time series, collecting current methods and their real-world applications. We
thoroughly discuss and compare peer-reviewed and preprint papers through
application domains across healthcare, finance, IoT, and climate science. Our
analysis reveals that most work relies on autoencoder and attention
architectures, with limited support for streaming, irregularly sampled, or
privacy-preserved series, and interpretability is still primarily treated as an
add-on. To push the field forward, we outline six research opportunities: (1)
combining complex networks with built-in interpretability; (2) setting up
clear, faithfulness-focused evaluation metrics for unsupervised explanations;
(3) building explainers that adapt to live data streams; (4) crafting
explanations tailored to specific domains; (5) adding human-in-the-loop methods
that refine clusters and explanations together; and (6) improving our
understanding of how time series clustering models work internally. By making
interpretability a primary design goal rather than an afterthought, we propose
the groundwork for the next generation of trustworthy deep clustering time
series analytics.

</details>


### [218] [Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces](https://arxiv.org/abs/2507.20853)
*Saket Tiwari,Omer Gottesman,George Konidaris*

Main category: cs.LG

TL;DR: 本文从几何视角研究连续状态和动作空间强化学习，证明策略训练动态诱导的可达状态流形维数与动作空间维数相关，并通过实验验证，还展示了理论结果在高自由度控制环境中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习理论大多针对有限状态和动作空间，缺乏对连续状态和动作空间的理论理解。

Method: 采用几何视角，研究半梯度方法学习的参数化策略诱导的可达状态集，分析两层神经策略的训练动态；使用actor - critic算法训练；通过实验在MuJoCo环境和玩具环境中验证；引入局部流形学习层到策略和价值函数网络。

Result: 证明在一定条件下，可达状态流形的维数与动作空间维数同阶；在四个MuJoCo环境和玩具环境中验证了该上界；通过改变神经网络一层学习稀疏表示，在高自由度控制环境中提高了性能。

Conclusion: 首次将状态空间几何与动作空间维数联系起来，所提理论结果在高自由度控制环境中有实际应用价值。

Abstract: Advances in reinforcement learning (RL) have led to its successful
application in complex tasks with continuous state and action spaces. Despite
these advances in practice, most theoretical work pertains to finite state and
action spaces. We propose building a theoretical understanding of continuous
state and action spaces by employing a geometric lens to understand the locally
attained set of states. The set of all parametrised policies learnt through a
semi-gradient based approach induces a set of attainable states in RL. We show
that the training dynamics of a two-layer neural policy induce a low
dimensional manifold of attainable states embedded in the high-dimensional
nominal state space trained using an actor-critic algorithm. We prove that,
under certain conditions, the dimensionality of this manifold is of the order
of the dimensionality of the action space. This is the first result of its
kind, linking the geometry of the state space to the dimensionality of the
action space. We empirically corroborate this upper bound for four MuJoCo
environments and also demonstrate the results in a toy environment with varying
dimensionality. We also show the applicability of this theoretical result by
introducing a local manifold learning layer to the policy and value function
networks to improve the performance in control environments with very high
degrees of freedom by changing one layer of the neural network to learn sparse
representations.

</details>


### [219] [Bi-cephalic self-attended model to classify Parkinson's disease patients with freezing of gait](https://arxiv.org/abs/2507.20862)
*Shomoita Jahid Mitin,Rodrigue Rizk,Maximilian Scherer,Thomas Koeglsperger,Daniel Lench,KC Santosh,Arun Singh*

Main category: cs.LG

TL;DR: 本文利用静息态EEG信号结合人口统计学和临床变量，开发多模态分类模型检测帕金森病患者步态功能障碍，多模态模型效果佳。


<details>
  <summary>Details</summary>
Motivation: 当前帕金森病步态功能障碍检测方法主观或依赖专业工具，需开发客观、数据驱动的多模态分类模型。

Method: 利用124名参与者数据集，提取静息态EEG特征和描述性变量，训练双脑自注意力模型（BiSAM），测试三种模态和不同EEG通道子集。

Result: 信号和描述性单模态模型性能有限，多模态模型表现更佳，BiSAM - 8和BiSAM - 4分类准确率达88%。

Conclusion: 整合EEG和客观描述性特征对检测有价值，该多模态架构是传统评估的有效替代，可用于临床监测和早期诊断。

Abstract: Parkinson Disease (PD) often results in motor and cognitive impairments,
including gait dysfunction, particularly in patients with freezing of gait
(FOG). Current detection methods are either subjective or reliant on
specialized gait analysis tools. This study aims to develop an objective,
data-driven, and multi-modal classification model to detect gait dysfunction in
PD patients using resting-state EEG signals combined with demographic and
clinical variables. We utilized a dataset of 124 participants: 42 PD patients
with FOG (PDFOG+), 41 without FOG (PDFOG-), and 41 age-matched healthy
controls. Features extracted from resting-state EEG and descriptive variables
(age, education, disease duration) were used to train a novel Bi-cephalic
Self-Attention Model (BiSAM). We tested three modalities: signal-only,
descriptive-only, and multi-modal, across different EEG channel subsets
(BiSAM-63, -16, -8, and -4). Signal-only and descriptive-only models showed
limited performance, achieving a maximum accuracy of 55% and 68%, respectively.
In contrast, the multi-modal models significantly outperformed both, with
BiSAM-8 and BiSAM-4 achieving the highest classification accuracy of 88%. These
results demonstrate the value of integrating EEG with objective descriptive
features for robust PDFOG+ detection. This study introduces a multi-modal,
attention-based architecture that objectively classifies PDFOG+ using minimal
EEG channels and descriptive variables. This approach offers a scalable and
efficient alternative to traditional assessments, with potential applications
in routine clinical monitoring and early diagnosis of PD-related gait
dysfunction.

</details>


### [220] [Online hierarchical partitioning of the output space in extreme multi-label data stream](https://arxiv.org/abs/2507.20894)
*Lara Neves,Afonso Lourenço,Alberto Cano,Goreti Marreiros*

Main category: cs.LG

TL;DR: 本文介绍在线多标签学习框架iHOMER，它能增量划分标签空间，集成漂移检测机制，实验显示其在多数据集上优于多个基线模型。


<details>
  <summary>Details</summary>
Motivation: 挖掘多标签输出的数据流面临分布变化、高维标签空间等挑战，概念漂移使模型适应更复杂，需要有效方法解决。

Method: 引入iHOMER框架，基于Jaccard相似度进行在线划分聚合聚类，用多元Bernoulli过程驱动的全局树型学习器指导实例划分，在全局和局部集成漂移检测机制。

Result: 在23个真实数据集上，iHOMER比5个全局基线模型平均性能高23%，比12个局部基线模型高32%。

Conclusion: iHOMER在在线多标签分类任务中具有很强的鲁棒性。

Abstract: Mining data streams with multi-label outputs poses significant challenges due
to evolving distributions, high-dimensional label spaces, sparse label
occurrences, and complex label dependencies. Moreover, concept drift affects
not only input distributions but also label correlations and imbalance ratios
over time, complicating model adaptation. To address these challenges,
structured learners are categorized into local and global methods. Local
methods break down the task into simpler components, while global methods adapt
the algorithm to the full output space, potentially yielding better predictions
by exploiting label correlations. This work introduces iHOMER (Incremental
Hierarchy Of Multi-label Classifiers), an online multi-label learning framework
that incrementally partitions the label space into disjoint, correlated
clusters without relying on predefined hierarchies. iHOMER leverages online
divisive-agglomerative clustering based on \textit{Jaccard} similarity and a
global tree-based learner driven by a multivariate \textit{Bernoulli} process
to guide instance partitioning. To address non-stationarity, it integrates
drift detection mechanisms at both global and local levels, enabling dynamic
restructuring of label partitions and subtrees. Experiments across 23
real-world datasets show iHOMER outperforms 5 state-of-the-art global
baselines, such as MLHAT, MLHT of Pruned Sets and iSOUPT, by 23\%, and 12 local
baselines, such as binary relevance transformations of kNN, EFDT, ARF, and
ADWIN bagging/boosting ensembles, by 32\%, establishing its robustness for
online multi-label classification.

</details>


### [221] [Zero-Shot Learning with Subsequence Reordering Pretraining for Compound-Protein Interaction](https://arxiv.org/abs/2507.20925)
*Hongzhi Zhang,Zhonglie Liu,Kun Meng,Jiameng Chen,Jia Wu,Bo Du,Di Lin,Yan Che,Wenbin Hu*

Main category: cs.LG

TL;DR: 提出新方法用于零样本化合物 - 蛋白质相互作用（CPI）预测，结合基线方法评估，结果显示在CPI任务尤其是零样本和数据稀缺场景表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有CPI预测方法在学习蛋白质序列表示时忽略子序列依赖关系，且依赖大规模多模态数据集，需大量训练数据和计算资源，限制可扩展性和效率。

Method: 提出用子序列重排预训练蛋白质表示用于CPI预测任务，显式捕捉蛋白质子序列间依赖关系，应用可变长度蛋白质增强以在小训练数据集上有良好预训练表现，结合多种基线方法评估模型。

Result: 该方法能提高基线模型在CPI任务上的性能，在零样本场景效果明显，相比现有预训练模型，在数据稀缺场景表现更优。

Conclusion: 提出的新方法在CPI预测任务中，尤其在零样本和数据稀缺场景下有更好的性能。

Abstract: Given the vastness of chemical space and the ongoing emergence of previously
uncharacterized proteins, zero-shot compound-protein interaction (CPI)
prediction better reflects the practical challenges and requirements of
real-world drug development. Although existing methods perform adequately
during certain CPI tasks, they still face the following challenges: (1)
Representation learning from local or complete protein sequences often
overlooks the complex interdependencies between subsequences, which are
essential for predicting spatial structures and binding properties. (2)
Dependence on large-scale or scarce multimodal protein datasets demands
significant training data and computational resources, limiting scalability and
efficiency. To address these challenges, we propose a novel approach that
pretrains protein representations for CPI prediction tasks using subsequence
reordering, explicitly capturing the dependencies between protein subsequences.
Furthermore, we apply length-variable protein augmentation to ensure excellent
pretraining performance on small training datasets. To evaluate the model's
effectiveness and zero-shot learning ability, we combine it with various
baseline methods. The results demonstrate that our approach can improve the
baseline model's performance on the CPI task, especially in the challenging
zero-shot scenario. Compared to existing pre-training models, our model
demonstrates superior performance, particularly in data-scarce scenarios where
training samples are limited. Our implementation is available at
https://github.com/Hoch-Zhang/PSRP-CPI.

</details>


### [222] [Breaking the Precision Ceiling in Physics-Informed Neural Networks: A Hybrid Fourier-Neural Architecture for Ultra-High Accuracy](https://arxiv.org/abs/2507.20929)
*Wei Shan Lee,Chi Kiu Althina Chau,Kei Chon Sio,Kam Ian Leong*

Main category: cs.LG

TL;DR: 本文提出混合傅里叶 - 神经网络架构突破PINNs在四阶偏微分方程精度瓶颈，实现超高精度。


<details>
  <summary>Details</summary>
Motivation: PINNs在四阶偏微分方程上误差停留在$10^{-3}$ - $10^{-4}$，限制其在工程应用中的采用。

Method: 结合截断傅里叶级数和深度神经网络，采用两相优化策略和自适应权重平衡，GPU加速实现。

Result: 实现$1.94 	imes 10^{-7}$的L2误差，比标准PINNs提升17倍，比传统数值方法好$15 - 500$倍，发现10个谐波性能最优。

Conclusion: 通过合理设计可实现超高精度，为科学计算开辟新范式。

Abstract: Physics-informed neural networks (PINNs) have plateaued at errors of
$10^{-3}$-$10^{-4}$ for fourth-order partial differential equations, creating a
perceived precision ceiling that limits their adoption in engineering
applications. We break through this barrier with a hybrid Fourier-neural
architecture for the Euler-Bernoulli beam equation, achieving unprecedented L2
error of $1.94 \times 10^{-7}$-a 17-fold improvement over standard PINNs and
\(15-500\times\) better than traditional numerical methods. Our approach
synergistically combines a truncated Fourier series capturing dominant modal
behavior with a deep neural network providing adaptive residual corrections. A
systematic harmonic optimization study revealed a counter-intuitive discovery:
exactly 10 harmonics yield optimal performance, with accuracy catastrophically
degrading from $10^{-7}$ to $10^{-1}$ beyond this threshold. The two-phase
optimization strategy (Adam followed by L-BFGS) and adaptive weight balancing
enable stable ultra-precision convergence. GPU-accelerated implementation
achieves sub-30-minute training despite fourth-order derivative complexity. By
addressing 12 critical gaps in existing approaches-from architectural rigidity
to optimization landscapes-this work demonstrates that ultra-precision is
achievable through proper design, opening new paradigms for scientific
computing where machine learning can match or exceed traditional numerical
methods.

</details>


### [223] [Dissecting Persona-Driven Reasoning in Language Models via Activation Patching](https://arxiv.org/abs/2507.20936)
*Ansh Poonia,Maeghal Jain*

Main category: cs.LG

TL;DR: 研究探讨分配角色对大语言模型客观任务推理的影响，用激活补丁方法研究模型组件编码角色信息，发现早期MLP层和中间MHA层作用及特定关注种族和颜色身份的注意力头。


<details>
  <summary>Details</summary>
Motivation: 研究分配角色如何影响大语言模型在客观任务上的推理。

Method: 使用激活补丁方法研究模型关键组件如何编码特定角色信息。

Result: 早期MLP层处理输入的句法结构和语义内容，将角色令牌转换为更丰富表示，中间MHA层用其塑造输出；识别出特定关注种族和颜色身份的注意力头。

Conclusion: 明确了大语言模型中不同层在处理角色信息时的作用及存在关注特定身份的注意力头。

Abstract: Large language models (LLMs) exhibit remarkable versatility in adopting
diverse personas. In this study, we examine how assigning a persona influences
a model's reasoning on an objective task. Using activation patching, we take a
first step toward understanding how key components of the model encode
persona-specific information. Our findings reveal that the early Multi-Layer
Perceptron (MLP) layers attend not only to the syntactic structure of the input
but also process its semantic content. These layers transform persona tokens
into richer representations, which are then used by the middle Multi-Head
Attention (MHA) layers to shape the model's output. Additionally, we identify
specific attention heads that disproportionately attend to racial and
color-based identities.

</details>


### [224] [PROVCREATOR: Synthesizing Complex Heterogenous Graphs with Node and Edge Attributes](https://arxiv.org/abs/2507.20967)
*Tianhao Wang,Simon Klancher,Kunal Mukherjee,Josh Wiedemeier,Feng Chen,Murat Kantarcioglu,Kangkook Jee*

Main category: cs.LG

TL;DR: 介绍了用于复杂异构图的合成图框架ProvCreator，在两个领域验证其能生成逼真且保护隐私的合成数据集。


<details>
  <summary>Details</summary>
Motivation: 现有合成图生成研究多关注简单属性的同质结构，对具有复杂异构模式的真实世界图的合成仍具挑战，需要能用于复杂异构图的合成框架。

Method: 将图合成表述为序列生成任务，使用基于transformer的大语言模型，采用通用的图到序列编解码器。

Result: 在网络安全系统溯源图和IntelliGraph基准数据集的知识图两个领域验证，能捕捉结构和语义间的复杂依赖。

Conclusion: ProvCreator可用于生成复杂异构图的逼真且保护隐私的合成数据集。

Abstract: The rise of graph-structured data has driven interest in graph learning and
synthetic data generation. While successful in text and image domains,
synthetic graph generation remains challenging -- especially for real-world
graphs with complex, heterogeneous schemas. Existing research has focused
mostly on homogeneous structures with simple attributes, limiting their
usefulness and relevance for application domains requiring semantic fidelity.
  In this research, we introduce ProvCreator, a synthetic graph framework
designed for complex heterogeneous graphs with high-dimensional node and edge
attributes. ProvCreator formulates graph synthesis as a sequence generation
task, enabling the use of transformer-based large language models. It features
a versatile graph-to-sequence encoder-decoder that 1. losslessly encodes graph
structure and attributes, 2. efficiently compresses large graphs for contextual
modeling, and 3. supports end-to-end, learnable graph generation.
  To validate our research, we evaluate ProvCreator on two challenging domains:
system provenance graphs in cybersecurity and knowledge graphs from
IntelliGraph Benchmark Dataset. In both cases, ProvCreator captures intricate
dependencies between structure and semantics, enabling the generation of
realistic and privacy-aware synthetic datasets.

</details>


### [225] [From Entanglement to Alignment: Representation Space Decomposition for Unsupervised Time Series Domain Adaptation](https://arxiv.org/abs/2507.20968)
*Rongyao Cai,Ming Jin,Qingsong Wen,Kexin Zhang*

Main category: cs.LG

TL;DR: 提出新的无监督域自适应框架 DARSD，从表示空间分解角度实现 UDA 任务，实验显示其优于 12 种 UDA 算法。


<details>
  <summary>Details</summary>
Motivation: 当前无监督域自适应方法将特征视为不可分割实体，忽略其内在组成，导致模型在不同但相似分布的目标域中表现不佳。

Method: 提出 DARSD 框架，包含对抗可学习的公共不变基、原型伪标签机制和混合对比优化策略。

Result: 在四个基准数据集的 53 个跨域场景中，DARSD 在 35 个场景中取得最优性能，优于 12 种 UDA 算法。

Conclusion: DARSD 框架在时间序列分析的无监督域自适应任务中表现优越，能有效解决域偏移问题。

Abstract: Domain shift poses a fundamental challenge in time series analysis, where
models trained on source domain often fail dramatically when applied in target
domain with different yet similar distributions. While current unsupervised
domain adaptation (UDA) methods attempt to align cross-domain feature
distributions, they typically treat features as indivisible entities, ignoring
their intrinsic compositions that governs domain adaptation. We introduce
DARSD, a novel UDA framework with theoretical explainability that explicitly
realizes UDA tasks from the perspective of representation space decomposition.
Our core insight is that effective domain adaptation requires not just
alignment, but principled disentanglement of transferable knowledge from mixed
representations. DARSD consists three synergistic components: (I) An
adversarial learnable common invariant basis that projects original features
into a domain-invariant subspace while preserving semantic content; (II) A
prototypical pseudo-labeling mechanism that dynamically separates target
features based on confidence, hindering error accumulation; (III) A hybrid
contrastive optimization strategy that simultaneously enforces feature
clustering and consistency while mitigating emerging distribution gaps.
Comprehensive experiments conducted on four benchmark datasets (WISDM, HAR,
HHAR, and MFD) demonstrate DARSD's superiority against 12 UDA algorithms,
achieving optimal performance in 35 out of 53 cross-domain scenarios.

</details>


### [226] [Model-Agnostic Gender Bias Control for Text-to-Image Generation via Sparse Autoencoder](https://arxiv.org/abs/2507.20973)
*Chao Wu,Zhenyi Wang,Kangxian Xie,Naresh Kumar Devulapally,Vishnu Suresh Lokhande,Mingchen Gao*

Main category: cs.LG

TL;DR: 本文提出SAE Debias框架减轻文本到图像生成中的性别偏见，在多模型评估中有效降低偏见并保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像扩散模型存在的性别偏见问题，尤其是职业与性别主体的刻板关联。

Method: 利用在性别偏见数据集上预训练的k - 稀疏自动编码器，在稀疏潜在空间中识别性别相关方向，构建每个职业的偏见方向并在推理时抑制。

Result: 在多个文本到图像模型评估中，SAE Debias大幅降低性别偏见，同时保持生成质量。

Conclusion: SAE Debias是首个应用稀疏自动编码器识别和干预文本到图像模型中性别偏见的工作，为构建负责任的生成式AI提供工具。

Abstract: Text-to-image (T2I) diffusion models often exhibit gender bias, particularly
by generating stereotypical associations between professions and gendered
subjects. This paper presents SAE Debias, a lightweight and model-agnostic
framework for mitigating such bias in T2I generation. Unlike prior approaches
that rely on CLIP-based filtering or prompt engineering, which often require
model-specific adjustments and offer limited control, SAE Debias operates
directly within the feature space without retraining or architectural
modifications. By leveraging a k-sparse autoencoder pre-trained on a gender
bias dataset, the method identifies gender-relevant directions within the
sparse latent space, capturing professional stereotypes. Specifically, a biased
direction per profession is constructed from sparse latents and suppressed
during inference to steer generations toward more gender-balanced outputs.
Trained only once, the sparse autoencoder provides a reusable debiasing
direction, offering effective control and interpretable insight into biased
subspaces. Extensive evaluations across multiple T2I models, including Stable
Diffusion 1.4, 1.5, 2.1, and SDXL, demonstrate that SAE Debias substantially
reduces gender bias while preserving generation quality. To the best of our
knowledge, this is the first work to apply sparse autoencoders for identifying
and intervening in gender bias within T2I models. These findings contribute
toward building socially responsible generative AI, providing an interpretable
and model-agnostic tool to support fairness in text-to-image generation.

</details>


### [227] [SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local Deployment](https://arxiv.org/abs/2507.20984)
*Yixin Song,Zhenliang Xue,Dongliang Wei,Feiyang Chen,Jianxiang Gao,Junchen Liu,Hangyu Liang,Guangshuo Qin,Chengrong Tian,Bo Wen,Longyu Zhao,Xinrui Zheng,Zeyu Mi,Haibo Chen*

Main category: cs.LG

TL;DR: 提出SmallThinker系列大语言模型，专为本地设备受限条件设计，采用创新架构，性能优异，减少对GPU依赖。


<details>
  <summary>Details</summary>
Motivation: 当前前沿大语言模型部署局限于GPU云基础设施，为突破此限制，设计适用于本地设备（计算能力弱、内存有限、存储慢）的大语言模型。

Method: 采用部署感知架构，包括结合细粒度专家混合与稀疏前馈网络的两级稀疏结构、预注意力路由器、NoPE - RoPE混合稀疏注意力机制。

Result: 发布SmallThinker - 4B - A0.6B和SmallThinker - 21B - A3B，取得了先进的性能分数，甚至超过更大的大语言模型；在普通消费级CPU上表现良好，内存消耗低。

Conclusion: SmallThinker为本地设备上的大语言模型部署提供了有效方案，减少了对昂贵GPU硬件的需求。

Abstract: While frontier large language models (LLMs) continue to push capability
boundaries, their deployment remains confined to GPU-powered cloud
infrastructure. We challenge this paradigm with SmallThinker, a family of LLMs
natively designed - not adapted - for the unique constraints of local devices:
weak computational power, limited memory, and slow storage. Unlike traditional
approaches that mainly compress existing models built for clouds, we architect
SmallThinker from the ground up to thrive within these limitations. Our
innovation lies in a deployment-aware architecture that transforms constraints
into design principles. First, We introduce a two-level sparse structure
combining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward
networks, drastically reducing computational demands without sacrificing model
capacity. Second, to conquer the I/O bottleneck of slow storage, we design a
pre-attention router that enables our co-designed inference engine to prefetch
expert parameters from storage while computing attention, effectively hiding
storage latency that would otherwise cripple on-device inference. Third, for
memory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to
slash KV cache requirements. We release SmallThinker-4B-A0.6B and
SmallThinker-21B-A3B, which achieve state-of-the-art performance scores and
even outperform larger LLMs. Remarkably, our co-designed system mostly
eliminates the need for expensive GPU hardware: with Q4_0 quantization, both
models exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB
and 8GB of memory respectively. SmallThinker is publicly available at
hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and
hf.co/PowerInfer/SmallThinker-21BA3B-Instruct.

</details>


### [228] [Modular Delta Merging with Orthogonal Constraints: A Scalable Framework for Continual and Reversible Model Composition](https://arxiv.org/abs/2507.20997)
*Haris Khan,Shumaila Asif,Sadia Asif*

Main category: cs.LG

TL;DR: 提出MDM - OC框架实现微调模型可扩展、无干扰和可逆组合，实验表明其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并和持续学习方法存在任务干扰、灾难性遗忘或缺乏可逆性问题。

Method: 将特定任务模型编码为与共享基础的增量，投影到正交子空间消除冲突，通过基于梯度的优化合并投影增量形成统一模型。

Result: 在视觉和自然语言处理基准测试中，MDM - OC在准确性、反向迁移和拆分保真度上优于先前基线，且内存高效、计算可行。

Conclusion: 该框架为模块化和合规的AI系统设计提供了原则性解决方案。

Abstract: In real-world machine learning deployments, models must be continually
updated, composed, and when required, selectively undone. However, existing
approaches to model merging and continual learning often suffer from task
interference, catastrophic forgetting, or lack of reversibility. We propose
Modular Delta Merging with Orthogonal Constraints (MDM-OC), a novel framework
that enables scalable, interference-free, and reversible composition of
fine-tuned models. Each task-specific model is encoded as a delta from a shared
base and projected into an orthogonal subspace to eliminate conflict. These
projected deltas are then merged via gradient-based optimization to form a
unified model that retains performance across tasks. Our approach supports
continual integration of new models, structured unmerging for compliance such
as GDPR requirements, and model stability via elastic weight consolidation and
synthetic replay. Extensive experiments on vision and natural language
processing benchmarks demonstrate that MDM-OC outperforms prior baselines in
accuracy, backward transfer, and unmerge fidelity, while remaining
memory-efficient and computationally tractable. This framework offers a
principled solution for modular and compliant AI system design.

</details>


### [229] [LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning](https://arxiv.org/abs/2507.20999)
*Yining Huang,Bin Li,Keke Tang,Meilian Chen*

Main category: cs.LG

TL;DR: 现有大模型CoT推理需大量数据和全参数微调，成本高。本文提出LoRA - PAR框架，按System 1和System 2需求划分数据和参数，采用两阶段微调策略，实验表明该策略降低参数使用且效果好。


<details>
  <summary>Details</summary>
Motivation: 现有大模型CoT推理成本高，现有参数高效微调方法未针对不同响应需求定制数据和参数。

Method: 提出LoRA - PAR框架，通过多模型角色扮演和投票分类任务数据，基于重要性评分划分参数，采用监督微调训练System 1任务，强化学习优化System 2任务。

Result: 两阶段微调策略降低了活跃参数使用，效果与或超越了SOTA PEFT基线。

Conclusion: LoRA - PAR框架及两阶段微调策略在降低成本的同时能取得较好效果。

Abstract: Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit
substantially from chain-of-thought (CoT) reasoning, yet pushing their
performance typically requires vast data, large model sizes, and full-parameter
fine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,
most existing approaches primarily address domain adaptation or layer-wise
allocation rather than explicitly tailoring data and parameters to different
response demands. Inspired by "Thinking, Fast and Slow," which characterizes
two distinct modes of thought-System 1 (fast, intuitive, often automatic) and
System 2 (slower, more deliberative and analytic)-we draw an analogy that
different "subregions" of an LLM's parameters might similarly specialize for
tasks that demand quick, intuitive responses versus those requiring multi-step
logical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework
that partitions both data and parameters by System 1 or System 2 demands, using
fewer yet more focused parameters for each task. Specifically, we classify task
data via multi-model role-playing and voting, and partition parameters based on
importance scoring, then adopt a two-stage fine-tuning strategy of training
System 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and
intuition and refine System 2 tasks with reinforcement learning (RL) to
reinforce deeper logical deliberation next. Extensive experiments show that the
two-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while
matching or surpassing SOTA PEFT baselines.

</details>


### [230] [Compositional Function Networks: A High-Performance Alternative to Deep Neural Networks with Built-in Interpretability](https://arxiv.org/abs/2507.21004)
*Fang Li*

Main category: cs.LG

TL;DR: 提出可解释的组合函数网络（CFNs）框架，支持多样组合模式，可高效训练，在多领域表现好。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络黑盒特性限制其在高风险透明要求领域的应用，需开发可解释模型。

Method: 引入CFNs框架，通过组合有清晰语义的基本数学函数构建模型，支持多样组合模式，且可完全微分。

Result: 在多个领域展示了适用性，在CIFAR - 10上准确率达96.24%，优于可解释的先进模型。

Conclusion: CFNs结合深度学习的分层表达与高效训练和数学函数的内在可解释性，适用于对性能和可解释性要求高的应用。

Abstract: Deep Neural Networks (DNNs) deliver impressive performance but their
black-box nature limits deployment in high-stakes domains requiring
transparency. We introduce Compositional Function Networks (CFNs), a novel
framework that builds inherently interpretable models by composing elementary
mathematical functions with clear semantics. Unlike existing interpretable
approaches that are limited to simple additive structures, CFNs support diverse
compositional patterns -- sequential, parallel, and conditional -- enabling
complex feature interactions while maintaining transparency. A key innovation
is that CFNs are fully differentiable, allowing efficient training through
standard gradient descent. We demonstrate CFNs' versatility across multiple
domains, from symbolic regression to image classification with deep
hierarchical networks. Our empirical evaluation shows CFNs achieve competitive
performance against black-box models (96.24% accuracy on CIFAR-10) while
outperforming state-of-the-art interpretable models like Explainable Boosting
Machines. By combining the hierarchical expressiveness and efficient training
of deep learning with the intrinsic interpretability of well-defined
mathematical functions, CFNs offer a powerful framework for applications where
both performance and accountability are paramount.

</details>


### [231] [Predicting Cognition from fMRI:A Comparative Study of Graph, Transformer, and Kernel Models Across Task and Rest Conditions](https://arxiv.org/abs/2507.21016)
*Jagruti Patel,Mikkel Schöttner,Thomas A. W. Bolton,Patric Hagmann*

Main category: cs.LG

TL;DR: 本文系统对比经典机器学习和深度学习模型，用功能磁共振成像数据进行认知预测，发现任务态 fMRI 预测效果优于静息态，特定 GNN 表现最佳，强调选择合适模型和特征表示的重要性。


<details>
  <summary>Details</summary>
Motivation: 从健康个体神经影像数据预测认知，洞察认知能力神经机制，应用于精准医疗和神经精神疾病早期检测。

Method: 使用经典机器学习（KRR）和深度学习模型（GNN、TGNN），利用静息态、工作记忆和语言任务 fMRI 数据进行认知预测。

Result: 任务态 fMRI 预测认知行为效果优于静息态；结合结构和功能连接的 GNN 表现最佳，但与仅用功能连接的 KRR 差异无统计学意义；TGNN 在任务态 fMRI 表现有竞争力，在静息态数据表现不佳。

Conclusion: 强调选择合适模型架构和特征表示的重要性，凸显多模态图感知 DL 模型和基于 Transformer 方法的潜力，为脑 - 行为建模提供指导。

Abstract: Predicting cognition from neuroimaging data in healthy individuals offers
insights into the neural mechanisms underlying cognitive abilities, with
potential applications in precision medicine and early detection of
neurological and psychiatric conditions. This study systematically benchmarked
classical machine learning (Kernel Ridge Regression (KRR)) and advanced deep
learning (DL) models (Graph Neural Networks (GNN) and Transformer-GNN (TGNN))
for cognitive prediction using Resting-state (RS), Working Memory, and Language
task fMRI data from the Human Connectome Project Young Adult dataset.
  Our results, based on R2 scores, Pearson correlation coefficient, and mean
absolute error, revealed that task-based fMRI, eliciting neural responses
directly tied to cognition, outperformed RS fMRI in predicting cognitive
behavior. Among the methods compared, a GNN combining structural connectivity
(SC) and functional connectivity (FC) consistently achieved the highest
performance across all fMRI modalities; however, its advantage over KRR using
FC alone was not statistically significant. The TGNN, designed to model
temporal dynamics with SC as a prior, performed competitively with FC-based
approaches for task-fMRI but struggled with RS data, where its performance
aligned with the lower-performing GNN that directly used fMRI time-series data
as node features. These findings emphasize the importance of selecting
appropriate model architectures and feature representations to fully leverage
the spatial and temporal richness of neuroimaging data.
  This study highlights the potential of multimodal graph-aware DL models to
combine SC and FC for cognitive prediction, as well as the promise of
Transformer-based approaches for capturing temporal dynamics. By providing a
comprehensive comparison of models, this work serves as a guide for advancing
brain-behavior modeling using fMRI, SC and DL.

</details>


### [232] [Behavior-Specific Filtering for Enhanced Pig Behavior Classification in Precision Livestock Farming](https://arxiv.org/abs/2507.21021)
*Zhen Zhang,Dong Sam Ha,Gota Morota,Sook Shin*

Main category: cs.LG

TL;DR: 研究提出特定行为过滤方法提升精准畜牧养殖中行为分类准确率，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 提高精准畜牧养殖中行为分类的准确率。

Method: 提出结合小波去噪和低通滤波器的特定行为过滤方法，针对猪的活跃和非活跃行为进行处理。

Result: 传统方法准确率91.58%，提出的特定行为过滤方法峰值准确率达94.73%。

Conclusion: 特定行为过滤方法对增强动物行为监测有效，有助于更好的健康管理和提高农场效率。

Abstract: This study proposes a behavior-specific filtering method to improve behavior
classification accuracy in Precision Livestock Farming. While traditional
filtering methods, such as wavelet denoising, achieved an accuracy of 91.58%,
they apply uniform processing to all behaviors. In contrast, the proposed
behavior-specific filtering method combines Wavelet Denoising with a Low Pass
Filter, tailored to active and inactive pig behaviors, and achieved a peak
accuracy of 94.73%. These results highlight the effectiveness of
behavior-specific filtering in enhancing animal behavior monitoring, supporting
better health management and farm efficiency.

</details>


### [233] [On Using the Shapley Value for Anomaly Localization: A Statistical Investigation](https://arxiv.org/abs/2507.21023)
*Rick S. Blum,Franziska Freytag*

Main category: cs.LG

TL;DR: 研究表明计算Shapley值时用单一固定项进行异常定位测试，复杂度更低且误差概率相同，独立观测情况有证明，依赖观测情况无证明。


<details>
  <summary>Details</summary>
Motivation: 探索用Shapley值进行传感器数据系统异常定位的有效方法。

Method: 采用合理的数学异常模型进行实验，对独立观测情况进行证明。

Result: 单一固定项计算Shapley值进行异常定位测试复杂度更低，误差概率与全情况使用Shapley值相同，独立观测情况结论得证。

Conclusion: 单一固定项计算Shapley值可实现低复杂度异常定位测试，但依赖观测情况结论无证明。

Abstract: Recent publications have suggested using the Shapley value for anomaly
localization for sensor data systems. Using a reasonable mathematical anomaly
model for full control, experiments indicate that using a single fixed term in
the Shapley value calculation achieves a lower complexity anomaly localization
test, with the same probability of error, as a test using the Shapley value for
all cases tested. A proof demonstrates these conclusions must be true for all
independent observation cases. For dependent observation cases, no proof is
available.

</details>


### [234] [Optimization Performance of Factorization Machine with Annealing under Limited Training Data](https://arxiv.org/abs/2507.21024)
*Mayumi Nakano,Yuya Seki,Shuta Kikuchi,Shu Tanaka*

Main category: cs.LG

TL;DR: 提出新的顺序数据集构建方法改进FMA，实验表明新方法优于传统FMA。


<details>
  <summary>Details</summary>
Motivation: 传统FMA在优化迭代次数增加时性能停滞，原因是训练FM的数据集数据点增多，新数据点贡献被稀释。

Method: 提出一种新的顺序数据集构建方法，最多保留指定数量的最近添加数据点，增强新数据点对替代模型的影响。

Result: 数值实验显示，新的FMA在较少的黑箱函数评估次数下得到成本更低的解。

Conclusion: 新的顺序数据集构建方法有效提升了FMA的性能。

Abstract: Black-box (BB) optimization problems aim to identify an input that minimizes
the output of a function (the BB function) whose input-output relationship is
unknown. Factorization machine with annealing (FMA) is a promising approach to
this task, employing a factorization machine (FM) as a surrogate model to
iteratively guide the solution search via an Ising machine. Although FMA has
demonstrated strong optimization performance across various applications, its
performance often stagnates as the number of optimization iterations increases.
One contributing factor to this stagnation is the growing number of data points
in the dataset used to train FM. It is hypothesized that as more data points
are accumulated, the contribution of newly added data points becomes diluted
within the entire dataset, thereby reducing their impact on improving the
prediction accuracy of FM. To address this issue, we propose a novel method for
sequential dataset construction that retains at most a specified number of the
most recently added data points. This strategy is designed to enhance the
influence of newly added data points on the surrogate model. Numerical
experiments demonstrate that the proposed FMA achieves lower-cost solutions
with fewer BB function evaluations compared to the conventional FMA.

</details>


### [235] [When Brain Foundation Model Meets Cauchy-Schwarz Divergence: A New Framework for Cross-Subject Motor Imagery Decoding](https://arxiv.org/abs/2507.21037)
*Jinzhou Wu,Baoping Tang,Qikang Li,Yi Wang,Cheng Li,Shujian Yu*

Main category: cs.LG

TL;DR: 提出新的多源域适应框架解决运动想象脑电信号解码难题，在两数据集表现优，大源池实验验证其可扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有运动想象脑电信号解码因个体差异和标注数据有限需昂贵校准，多源域适应方法存在负迁移、计算成本高及忽视特征与决策输出依赖等问题。

Method: 利用预训练大的脑基础模型动态选择相关源主体，采用柯西 - 施瓦茨和条件柯西 - 施瓦茨散度进行特征和决策层对齐。

Result: 在两个基准数据集上优于众多先进基线，大源池实验表明模型选择可减少训练时间且不牺牲性能。

Conclusion: 所提框架有效解决运动想象脑电信号解码问题，具有良好的性能、可扩展性和效率。

Abstract: Decoding motor imagery (MI) electroencephalogram (EEG) signals, a key
non-invasive brain-computer interface (BCI) paradigm for controlling external
systems, has been significantly advanced by deep learning. However, MI-EEG
decoding remains challenging due to substantial inter-subject variability and
limited labeled target data, which necessitate costly calibration for new
users. Many existing multi-source domain adaptation (MSDA) methods
indiscriminately incorporate all available source domains, disregarding the
large inter-subject differences in EEG signals, which leads to negative
transfer and excessive computational costs. Moreover, while many approaches
focus on feature distribution alignment, they often neglect the explicit
dependence between features and decision-level outputs, limiting their ability
to preserve discriminative structures. To address these gaps, we propose a
novel MSDA framework that leverages a pretrained large Brain Foundation Model
(BFM) for dynamic and informed source subject selection, ensuring only relevant
sources contribute to adaptation. Furthermore, we employ Cauchy-Schwarz (CS)
and Conditional CS (CCS) divergences to jointly perform feature-level and
decision-level alignment, enhancing domain invariance while maintaining class
discriminability. Extensive evaluations on two benchmark MI-EEG datasets
demonstrate that our framework outperforms a broad range of state-of-the-art
baselines. Additional experiments with a large source pool validate the
scalability and efficiency of BFM-guided selection, which significantly reduces
training time without sacrificing performance.

</details>


### [236] [Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning](https://arxiv.org/abs/2507.21049)
*Zedong Wang,Siyuan Li,Dan Xu*

Main category: cs.LG

TL;DR: 现有多任务优化技术难以稳定提升性能，本文提出Rep - MTL，利用表示层任务显著性量化任务交互，在多个MTL基准测试中取得有竞争力的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有多任务优化技术专注于优化器中心的损失缩放和梯度操作策略解决冲突，但无法稳定提升性能，共享表示空间中促进任务互补的信息未被充分探索。

Method: 提出Rep - MTL，利用表示层任务显著性量化任务特定优化和共享表示学习的交互，通过基于熵的惩罚和样本级跨任务对齐引导任务显著性。

Result: 在四个MTL基准测试中，即使使用基本的均等加权策略，Rep - MTL也能取得有竞争力的性能提升和效率优势；幂律指数分析证明其能平衡任务特定学习和跨任务共享。

Conclusion: Rep - MTL是一种有效的多任务学习方法，可促进互补信息共享，平衡任务学习和跨任务共享。

Abstract: Despite the promise of Multi-Task Learning in leveraging complementary
knowledge across tasks, existing multi-task optimization (MTO) techniques
remain fixated on resolving conflicts via optimizer-centric loss scaling and
gradient manipulation strategies, yet fail to deliver consistent gains. In this
paper, we argue that the shared representation space, where task interactions
naturally occur, offers rich information and potential for operations
complementary to existing optimizers, especially for facilitating the
inter-task complementarity, which is rarely explored in MTO. This intuition
leads to Rep-MTL, which exploits the representation-level task saliency to
quantify interactions between task-specific optimization and shared
representation learning. By steering these saliencies through entropy-based
penalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate
negative transfer by maintaining the effective training of individual tasks
instead pure conflict-solving, while explicitly promoting complementary
information sharing. Experiments are conducted on four challenging MTL
benchmarks covering both task-shift and domain-shift scenarios. The results
show that Rep-MTL, even paired with the basic equal weighting policy, achieves
competitive performance gains with favorable efficiency. Beyond standard
performance metrics, Power Law exponent analysis demonstrates Rep-MTL's
efficacy in balancing task-specific learning and cross-task sharing. The
project page is available at HERE.

</details>


### [237] [Flow Matching Policy Gradients](https://arxiv.org/abs/2507.21053)
*David McAllister,Songwei Ge,Brent Yi,Chung Min Kim,Ethan Weber,Hongsuk Choi,Haiwen Feng,Angjoo Kanazawa*

Main category: cs.LG

TL;DR: 提出Flow Policy Optimization (FPO)算法，将流匹配引入策略梯度框架，能在连续控制任务中从头训练扩散风格策略，表现优于高斯策略。


<details>
  <summary>Details</summary>
Motivation: 将流匹配应用于策略梯度框架，解决现有扩散强化学习方法对特定采样方法的依赖问题，发挥基于流的生成模型在高维空间建模的优势。

Method: 提出FPO算法，将策略优化转化为最大化基于条件流匹配损失计算的优势加权比率，与PPO - clip框架兼容，且训练和推理时不依赖特定扩散或流积分方法。

Result: FPO能在多种连续控制任务中从头训练扩散风格策略，基于流的模型可捕捉多模态动作分布。

Conclusion: 基于流的模型在连续控制任务中，尤其是条件不足的情况下，比高斯策略性能更高。

Abstract: Flow-based generative models, including diffusion models, excel at modeling
continuous distributions in high-dimensional spaces. In this work, we introduce
Flow Policy Optimization (FPO), a simple on-policy reinforcement learning
algorithm that brings flow matching into the policy gradient framework. FPO
casts policy optimization as maximizing an advantage-weighted ratio computed
from the conditional flow matching loss, in a manner compatible with the
popular PPO-clip framework. It sidesteps the need for exact likelihood
computation while preserving the generative capabilities of flow-based models.
Unlike prior approaches for diffusion-based reinforcement learning that bind
training to a specific sampling method, FPO is agnostic to the choice of
diffusion or flow integration at both training and inference time. We show that
FPO can train diffusion-style policies from scratch in a variety of continuous
control tasks. We find that flow-based models can capture multimodal action
distributions and achieve higher performance than Gaussian policies,
particularly in under-conditioned settings.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [238] [AR-LIF: Adaptive reset leaky-integrate and fire neuron for spiking neural networks](https://arxiv.org/abs/2507.20746)
*Zeyu Huang,Wei Meng,Quan Liu,Kun Chen,Li Ma*

Main category: cs.NE

TL;DR: 本文设计自适应重置神经元，结合阈值调整策略，在多数据集上表现优异且保持低能耗。


<details>
  <summary>Details</summary>
Motivation: 现有硬重置方法会造成信息损失，改进的软重置方法对神经元采用统一处理方式，需更好的解决方案。

Method: 设计自适应重置神经元，建立输入、输出和重置之间的关联，并集成简单有效的阈值调整策略。

Result: 在各种数据集上取得了出色的性能，同时保持了低能耗的优势。

Conclusion: 所设计的自适应重置神经元结合阈值调整策略是有效的，能在保证性能的同时保持低能耗。

Abstract: Spiking neural networks possess the advantage of low energy consumption due
to their event-driven nature. Compared with binary spike outputs, their
inherent floating-point dynamics are more worthy of attention. The threshold
level and reset mode of neurons play a crucial role in determining the number
and timing of spikes. The existing hard reset method causes information loss,
while the improved soft reset method adopts a uniform treatment for neurons. In
response to this, this paper designs an adaptive reset neuron, establishing the
correlation between input, output and reset, and integrating a simple yet
effective threshold adjustment strategy. It achieves excellent performance on
various datasets while maintaining the advantage of low energy consumption.

</details>


### [239] [Why Flow Matching is Particle Swarm Optimization?](https://arxiv.org/abs/2507.20810)
*Kaichen Ouyang*

Main category: cs.NE

TL;DR: 本文初步研究生成模型中流匹配与进化计算中粒子群优化（PSO）的对偶性，揭示联系并指出有前景的研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索生成模型中流匹配与进化计算中PSO之间的关系，为开发新混合算法和统一分析方法奠定理论基础。

Method: 通过理论分析，对比两者在数学公式和优化机制方面的联系。

Result: 发现流匹配可视为PSO的连续推广，PSO是群体智能原则的离散实现，揭示了两者之间的内在联系。

Conclusion: 该对偶性理解为开发新混合算法和统一分析方法奠定基础，指出了基于流匹配改进群体智能算法和基于群体智能增强生成模型等研究方向。

Abstract: This paper preliminarily investigates the duality between flow matching in
generative models and particle swarm optimization (PSO) in evolutionary
computation. Through theoretical analysis, we reveal the intrinsic connections
between these two approaches in terms of their mathematical formulations and
optimization mechanisms: the vector field learning in flow matching shares
similar mathematical expressions with the velocity update rules in PSO; both
methods follow the fundamental framework of progressive evolution from initial
to target distributions; and both can be formulated as dynamical systems
governed by ordinary differential equations. Our study demonstrates that flow
matching can be viewed as a continuous generalization of PSO, while PSO
provides a discrete implementation of swarm intelligence principles. This
duality understanding establishes a theoretical foundation for developing novel
hybrid algorithms and creates a unified framework for analyzing both methods.
Although this paper only presents preliminary discussions, the revealed
correspondences suggest several promising research directions, including
improving swarm intelligence algorithms based on flow matching principles and
enhancing generative models using swarm intelligence concepts.

</details>


### [240] [Pareto-Grid-Guided Large Language Models for Fast and High-Quality Heuristics Design in Multi-Objective Combinatorial Optimization](https://arxiv.org/abs/2507.20923)
*Minh Hieu Ha,Hung Phan,Tung Duy Doan,Tung Dao,Dao Tran,Huynh Thi Thanh Binh*

Main category: cs.NE

TL;DR: 提出用于多目标组合优化问题（MOCOP）的MPaGE框架，利用大语言模型和PFG技术，表现优于现有基于LLM的框架，运行时间快。


<details>
  <summary>Details</summary>
Motivation: 传统进化算法依赖领域知识和参数调优，现有基于大语言模型的方法多关注单目标任务，忽略多目标场景的运行效率和启发式多样性。

Method: 引入MPaGE框架，增强SEMO框架，通过划分目标空间为网格，保留表现佳的候选者引导启发式生成，利用大语言模型在变异时优先考虑语义不同逻辑结构的启发式。

Result: MPaGE表现优于现有基于LLM的框架，与传统多目标进化算法有竞争力，运行时间显著更快。

Conclusion: MPaGE框架能有效解决多目标组合优化问题，具有良好性能和效率。

Abstract: Multi-objective combinatorial optimization problems (MOCOP) frequently arise
in practical applications that require the simultaneous optimization of
conflicting objectives. Although traditional evolutionary algorithms can be
effective, they typically depend on domain knowledge and repeated parameter
tuning, limiting flexibility when applied to unseen MOCOP instances. Recently,
integration of Large Language Models (LLMs) into evolutionary computation has
opened new avenues for automatic heuristic generation, using their advanced
language understanding and code synthesis capabilities. Nevertheless, most
existing approaches predominantly focus on single-objective tasks, often
neglecting key considerations such as runtime efficiency and heuristic
diversity in multi-objective settings. To bridge this gap, we introduce
Multi-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a
novel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO)
framework that leverages LLMs and Pareto Front Grid (PFG) technique. By
partitioning the objective space into grids and retaining top-performing
candidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize
heuristics with semantically distinct logical structures during variation, thus
promoting diversity and mitigating redundancy within the population. Through
extensive evaluations, MPaGE demonstrates superior performance over existing
LLM-based frameworks, and achieves competitive results to traditional
Multi-objective evolutionary algorithms (MOEAs), with significantly faster
runtime. Our code is available at: https://github.com/langkhachhoha/MPaGE.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [241] [Towards Generalized Parameter Tuning in Coherent Ising Machines: A Portfolio-Based Approach](https://arxiv.org/abs/2507.20295)
*Tatsuro Hanyu,Takahiro Katagiri,Daichi Mukunoki,Tetsuya Hoshino*

Main category: cs.PF

TL;DR: 提出算法组合方法对采用CACm算法的CIMs进行超参数调优，提出两种调优方法并评估，结果表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: CAC算法求解组合优化问题时性能对大量超参数敏感，需高效调优。

Method: 提出算法组合方法，包含多搜索策略，提出方法A和方法B进行调优，并在超级计算机“Flow”上用特定实例和TTS指标评估。

Result: 与已知最佳超参数的基线性能相比，方法A最多提升1.47倍，方法B最多提升1.65倍。

Conclusion: 算法组合方法能有效增强CIMs的调优过程。

Abstract: Coherent Ising Machines (CIMs) have recently gained attention as a promising
computing model for solving combinatorial optimization problems. In particular,
the Chaotic Amplitude Control (CAC) algorithm has demonstrated high solution
quality, but its performance is highly sensitive to a large number of
hyperparameters, making efficient tuning essential. In this study, we present
an algorithm portfolio approach for hyperparameter tuning in CIMs employing
Chaotic Amplitude Control with momentum (CACm) algorithm. Our method
incorporates multiple search strategies, enabling flexible and effective
adaptation to the characteristics of the hyperparameter space. Specifically, we
propose two representative tuning methods, Method A and Method B. Method A
optimizes each hyperparameter sequentially with a fixed total number of trials,
while Method B prioritizes hyperparameters based on initial evaluations before
applying Method A in order. Performance evaluations were conducted on the
Supercomputer "Flow" at Nagoya University, using planted Wishart instances and
Time to Solution (TTS) as the evaluation metric. Compared to the baseline
performance with best-known hyperparameters, Method A achieved up to 1.47x
improvement, and Method B achieved up to 1.65x improvement. These results
demonstrate the effectiveness of the algorithm portfolio approach in enhancing
the tuning process for CIMs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [242] [AccessGuru: Leveraging LLMs to Detect and Correct Web Accessibility Violations in HTML Code](https://arxiv.org/abs/2507.19549)
*Nadeen Fathallah,Daniel Hernández,Steffen Staab*

Main category: cs.SE

TL;DR: 文章提出AccessGuru方法结合工具和大模型检测并纠正网页可访问性违规，评估显示其表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 多数网页不符合可访问性准则，降低网页提供者工作量并促进包容性，解决自动检测和纠正违规的挑战。

Method: 引入新的分类法，将违规分为三类；提出AccessGuru方法，结合现有工具和大模型，用分类驱动提示策略纠正；开发真实世界违规基准进行评估。

Result: AccessGuru使平均违规得分最多降低84%，显著优于先前最多50%的方法。

Conclusion: AccessGuru在检测和纠正网页可访问性违规方面表现出色，能有效解决相关问题。

Abstract: The vast majority of Web pages fail to comply with established Web
accessibility guidelines, excluding a range of users with diverse abilities
from interacting with their content. Making Web pages accessible to all users
requires dedicated expertise and additional manual efforts from Web page
providers. To lower their efforts and promote inclusiveness, we aim to
automatically detect and correct Web accessibility violations in HTML code.
While previous work has made progress in detecting certain types of
accessibility violations, the problem of automatically detecting and correcting
accessibility violations remains an open challenge that we address. We
introduce a novel taxonomy classifying Web accessibility violations into three
key categories - Syntactic, Semantic, and Layout. This taxonomy provides a
structured foundation for developing our detection and correction method and
redefining evaluation metrics. We propose a novel method, AccessGuru, which
combines existing accessibility testing tools and Large Language Models (LLMs)
to detect violations and applies taxonomy-driven prompting strategies to
correct all three categories. To evaluate these capabilities, we develop a
benchmark of real-world Web accessibility violations. Our benchmark quantifies
syntactic and layout compliance and judges semantic accuracy through
comparative analysis with human expert corrections. Evaluation against our
benchmark shows that AccessGuru achieves up to 84% average violation score
decrease, significantly outperforming prior methods that achieve at most 50%.

</details>


### [243] [LastMerge: A language-agnostic structured tool for code integration](https://arxiv.org/abs/2507.19687)
*Joao Pedro Duarte,Paulo Borba,Guilherme Cavalcanti*

Main category: cs.SE

TL;DR: 提出通用结构化合并工具LastMerge，实验表明通用工具在合并准确性和性能上表现良好，可有效替代特定语言工具。


<details>
  <summary>Details</summary>
Motivation: 现有基于AST的结构化合并工具语言特定且成本高，为提高多种语言的合并准确性。

Method: 提出LastMerge，与其他三个工具对比实验，通过重放合并场景收集运行时间、行为差异和合并准确性数据。

Result: 通用结构化合并对合并准确性无显著影响，LastMerge假阳性少15%，Mergiraf假阴性少42%，运行性能与特定语言工具相当。

Conclusion: 通用结构化合并工具可有效替代特定语言工具，利于结构化合并在行业更广泛应用。

Abstract: Unstructured line-based merge tools are widely used in practice. Structured
AST-based merge tools show significantly improved merge accuracy, but are
rarely used in practice because they are language specific and costly,
consequently not being available for many programming languages. To improve
merge accuracy for a wide range of languages, we propose LastMerge, a generic
structured merge tool that can be configured through a thin interface that
significantly reduces the effort of supporting structured merge. To understand
the impact that generic structured merge might have on merge accuracy and
performance, we run an experiment with four structured merge tools: two Java
specific tools, jDime and Spork, and their generic counterparts, respectively
LastMerge and Mergiraf. Using each tool, we replay merge scenarios from a
significant dataset, and collect data on runtime, behavioral divergences, and
merge accuracy. Our results show no evidence that generic structured merge
significantly impacts merge accuracy. Although we observe a difference rate of
approximately 10% between the Java specific tools and their generic
counterparts, most of the differences stem from implementation details and
could be avoided. We find that LastMerge reports 15% fewer false positives than
jDime while Mergiraf misses 42% fewer false negatives than Spork. Both generic
tools exhibit comparable runtime performance to the state of the art language
specific implementations. These results suggest that generic structured merge
tools can effectively replace language-specific ones, paving the way for
broader adoption of structured merge in industry.

</details>


### [244] [Refactoring $\neq$ Bug-Inducing: Improving Defect Prediction with Code Change Tactics Analysis](https://arxiv.org/abs/2507.19714)
*Feifei Niu,Junqian Shao,Christoph Mayr-Dorn,Liguo Huang,Wesley K. G. Assunção,Chuanyi Li,Jidong Ge,Alexander Egyed*

Main category: cs.SE

TL;DR: 研究重构及其传播对JIT - DP的影响，提出CAT分析，证明纳入重构信息可提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 以往研究在评估和方法阶段大多忽略代码重构，而重构会与修复和引入缺陷的更改混淆，忽略重构会给JIT - DP模型学习和评估带来偏差。

Method: 研究重构及其传播对六种JIT - DP方法的影响，提出Code chAnge Tactics (CAT)分析对代码重构及其传播进行分类，并将重构信息融入六种基线方法。

Result: CAT分析使JIT - Defects4J数据集标签准确率提高13.7%；忽略重构信息会使模型性能下降，语义模型F1分数最多降37.3%；融入重构信息使召回率和F1分数最高分别提升43.2%和32.5%。

Conclusion: 在JIT - DP的方法和评估中纳入重构信息很重要，CAT分析在软件维护中分析重构及其传播有广泛适用性。

Abstract: Just-in-time defect prediction (JIT-DP) aims to predict the likelihood of
code changes resulting in software defects at an early stage. Although code
change metrics and semantic features have enhanced prediction accuracy, prior
research has largely ignored code refactoring during both the evaluation and
methodology phases, despite its prevalence. Refactoring and its propagation
often tangle with bug-fixing and bug-inducing changes within the same commit
and statement. Neglecting refactoring can introduce bias into the learning and
evaluation of JIT-DP models. To address this gap, we investigate the impact of
refactoring and its propagation on six state-of-the-art JIT-DP approaches. We
propose Code chAnge Tactics (CAT) analysis to categorize code refactoring and
its propagation, which improves labeling accuracy in the JIT-Defects4J dataset
by 13.7%. Our experiments reveal that failing to consider refactoring
information in the dataset can diminish the performance of models, particularly
semantic-based models, by 18.6% and 37.3% in F1-score. Additionally, we propose
integrating refactoring information to enhance six baseline approaches,
resulting in overall improvements in recall and F1-score, with increases of up
to 43.2% and 32.5%, respectively. Our research underscores the importance of
incorporating refactoring information in the methodology and evaluation of
JIT-DP. Furthermore, our CAT has broad applicability in analyzing refactoring
and its propagation for software maintenance.

</details>


### [245] [Clean Code In Practice: Challenges and Opportunities](https://arxiv.org/abs/2507.19721)
*Dapeng Yan,Wenjie Yang,Kui Liu,Zhiming Liu,Zhikuang Cai*

Main category: cs.SE

TL;DR: 本文探讨软件可靠性、安全性和保障性的相互作用，提出威胁估计框架和实践指南，表明综合考量可增强软件系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在行业实践背景下，现代系统复杂性要求深入理解可靠性指标与安全保障问题的相互作用，以确保软件系统安全。

Method: 对行业中用于可靠性预测的关键指标和测量技术进行全面分析，识别软件可靠性的关键威胁并构建包含安全和保障方面的威胁估计框架。

Result: 发现将可靠性指标与安全保障考虑相结合可增强软件系统的鲁棒性。

Conclusion: 为从业者提出一套可操作的指南，以改进可靠性预测模型并应对当代软件应用的安全保障挑战。

Abstract: Reliability prediction is crucial for ensuring the safety and security of
software systems, especially in the context of industry practices. While
various metrics and measurements are employed to assess software reliability,
the complexity of modern systems necessitates a deeper understanding of how
these metrics interact with security and safety concerns. This paper explores
the interplay between software reliability, safety, and security, offering a
comprehensive analysis of key metrics and measurement techniques used in the
industry for reliability prediction. We identify critical threats to software
reliability and provide a threat estimation framework that incorporates both
safety and security aspects. Our findings suggest that integrating reliability
metrics with safety and security considerations can enhance the robustness of
software systems. Furthermore, we propose a set of actionable guidelines for
practitioners to improve their reliability prediction models while
simultaneously addressing the security and safety challenges of contemporary
software applications.

</details>


### [246] [Defining ethically sourced code generation](https://arxiv.org/abs/2507.19743)
*Zhuolin Xu,Chenglin Li,Qiushi Li,Shin Hwei Tan*

Main category: cs.SE

TL;DR: 文章提出ES - CodeGen概念，通过文献回顾和从业者调查构建分类法，确定维度、后果等，呼吁关注相关伦理问题。


<details>
  <summary>Details</summary>
Motivation: 随着代码生成模型发展，为确保负责任AI，需解决代码生成中的伦理问题，实现伦理来源生成。

Method: 进行两阶段文献回顾，阅读803篇论文，筛选71篇相关论文确定10个初始维度；对32名从业者进行调查，包括有实际伦理问题经验的开发者。

Result: 确定11个ES - CodeGen维度，新增代码质量维度；识别了后果、工件和阶段；发现从业者易忽视社会相关维度，且多数认可调查有助于理解ES - CodeGen。

Conclusion: 呼吁关注ES - CodeGen的各种伦理问题。

Abstract: Several code generation models have been proposed to help reduce time and
effort in solving software-related tasks. To ensure responsible AI, there are
growing interests over various ethical issues (e.g., unclear licensing,
privacy, fairness, and environment impact). These studies have the overarching
goal of ensuring ethically sourced generation, which has gained growing
attentions in speech synthesis and image generation. In this paper, we
introduce the novel notion of Ethically Sourced Code Generation (ES-CodeGen) to
refer to managing all processes involved in code generation model development
from data collection to post-deployment via ethical and sustainable practices.
To build a taxonomy of ES-CodeGen, we perform a two-phase literature review
where we read 803 papers across various domains and specific to AI-based code
generation. We identified 71 relevant papers with 10 initial dimensions of
ES-CodeGen. To refine our dimensions and gain insights on consequences of
ES-CodeGen, we surveyed 32 practitioners, which include six developers who
submitted GitHub issues to opt-out from the Stack dataset (these impacted users
have real-world experience of ethically sourcing issues in code generation
models). The results lead to 11 dimensions of ES-CodeGen with a new dimension
on code quality as practitioners have noted its importance. We also identified
consequences, artifacts, and stages relevant to ES-CodeGen. Our post-survey
reflection showed that most practitioners tend to ignore social-related
dimensions despite their importance. Most practitioners either agreed or
strongly agreed that our survey help improve their understanding of ES-CodeGen.
Our study calls for attentions of various ethical issues towards ES-CodeGen.

</details>


### [247] [From Few-Label to Zero-Label: An Approach for Cross-System Log-Based Anomaly Detection with Meta-Learning](https://arxiv.org/abs/2507.19806)
*Xinlong Zhao,Tong Jia,Minghua He,Yihan Wu,Ying Li,Gang Huang*

Main category: cs.SE

TL;DR: 本文提出 FreeLog 方法解决零标签跨系统日志异常检测问题，实验表明其性能与依赖少量标签数据的方法相当。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法依赖大量标注数据，跨系统方法在标签不足时存在冷启动问题，因此探索零标签跨系统日志异常检测。

Method: 提出 FreeLog，一种系统无关的表示元学习方法，无需目标系统的标注日志。

Result: 在三个公共日志数据集上的实验表明，FreeLog 性能与依赖目标系统少量标注数据的先进方法相当。

Conclusion: FreeLog 能在零标签条件下实现跨系统日志异常检测，且性能良好。

Abstract: Log anomaly detection plays a critical role in ensuring the stability and
reliability of software systems. However, existing approaches rely on large
amounts of labeled log data, which poses significant challenges in real-world
applications. To address this issue, cross-system transfer has been identified
as a key research direction. State-of-the-art cross-system approaches achieve
promising performance with only a few labels from the target system. However,
their reliance on labeled target logs makes them susceptible to the cold-start
problem when labeled logs are insufficient. To overcome this limitation, we
explore a novel yet underexplored setting: zero-label cross-system log anomaly
detection, where the target system logs are entirely unlabeled. To this end, we
propose FreeLog, a system-agnostic representation meta-learning method that
eliminates the need for labeled target system logs, enabling cross-system log
anomaly detection under zero-label conditions. Experimental results on three
public log datasets demonstrate that FreeLog achieves performance comparable to
state-of-the-art methods that rely on a small amount of labeled data from the
target system.

</details>


### [248] [A Cooperative Approach for Knowledge-based Business Process Design in a Public Authority](https://arxiv.org/abs/2507.19842)
*Mohammad Azarijafari,Luisa Mich,Michele Missikoff,Oleg Missikoff*

Main category: cs.SE

TL;DR: 企业因数字化转型需调整组织结构，本文提出基于知识的方法支持业务专家设计业务流程，该方法无需知识工程专业知识，适合所有利益相关者。


<details>
  <summary>Details</summary>
Motivation: 企业为应对数字化转型需调整组织结构和运营，采用面向流程的生产模型是关键创新领域，需方法支持业务流程设计。

Method: 提出基于知识的方法，从简单文本知识制品构建知识库，引导设计师按结构化步骤生成目标流程的图表工作流，无需知识工程专业知识。

Result: 未提及明确结果。

Conclusion: 未提及明确结论。

Abstract: Enterprises are currently undergoing profound transformations due to the
unpostponable digital transformation. Then, to remain competitive, enterprises
must adapt their organisational structures and operations. This organisational
shift is also important for small and medium-sized enterprises. A key
innovation frontier is the adoption of process-oriented production models. This
paper presents a knowledge-based method to support business experts in
designing business processes. The method requires no prior expertise in
Knowledge Engineering and guides designers through a structured sequence of
steps to produce a diagrammatic workflow of the target process. The
construction of the knowledge base starts from simple, text-based, knowledge
artefacts and then progresses towards more structured, formal representations.
The approach has been conceived to allow a shared approach for all stakeholders
and actors who participate in the BP design.

</details>


### [249] [Search-Based Fuzzing For RESTful APIs That Use MongoDB](https://arxiv.org/abs/2507.20848)
*Hernan Ghianni,Man Zhang,Juan P. Galeotti,Andrea Arcuri*

Main category: cs.SE

TL;DR: 本文提出增强与NoSQL数据库交互的RESTful API基于搜索的软件测试生成新技术，在EvoMaster上实现，实验显示代码覆盖率显著提升。


<details>
  <summary>Details</summary>
Motivation: 在生成白盒测试时，考虑数据库状态对提高代码覆盖率和发现隐藏故障很重要，现有方法存在不足。

Method: 动态分析数据库状态，允许从测试用例直接插入NoSQL数据，在EvoMaster上实现。

Result: 在六个RESTful API上实验，代码覆盖率相比现有白盒方法最多提高18%，还与四个先进黑盒模糊测试器对比。

Conclusion: 提出的新技术能显著提升与NoSQL数据库交互的RESTful API测试的代码覆盖率。

Abstract: In RESTful APIs, interactions with a database are a common and crucial
aspect. When generating whitebox tests, it is essential to consider the
database's state (i.e., the data contained in the database) to achieve higher
code coverage and uncover more hidden faults. This article presents novel
techniques to enhance search-based software test generation for RESTful APIs
interacting with NoSQL databases. Specifically, we target the popular MongoDB
database, by dynamically analyzing (via automated code instrumentation) the
state of the database during the test generation process. Additionally, to
achieve better results, our novel approach allows inserting NoSQL data directly
from test cases. This is particularly beneficial when generating the correct
sequence of events to set the NoSQL database in an appropriate state is
challenging or time-consuming. This method is also advantageous for testing
read-only microservices. Our novel techniques are implemented as an extension
of EvoMaster, the only open-source tool for white-box fuzzing RESTful APIs.
Experiments conducted on six RESTful APIs demonstrated significant improvements
in code coverage, with increases of up to 18% compared to existing white-box
approaches. To better highlight the improvements of our novel techniques,
comparisons are also carried out with four state-of-the-art black-box fuzzers.

</details>


### [250] [AgentMesh: A Cooperative Multi-Agent Generative AI Framework for Software Development Automation](https://arxiv.org/abs/2507.19902)
*Sourena Khanzadeh*

Main category: cs.SE

TL;DR: 提出基于Python的AgentMesh框架，用多协作LLM代理自动完成软件开发任务，介绍架构、设计与实现细节，通过案例展示其工作，讨论局限与未来工作。


<details>
  <summary>Details</summary>
Motivation: 解决传统软件开发需不同专业人员协作的复杂问题，利用多协作LLM代理实现软件开发任务自动化。

Method: 设计Planner、Coder、Debugger和Reviewer四个专业代理协同工作，详细描述代理架构、设计、通信，给出实现细节。

Result: 通过案例展示AgentMesh能处理非平凡开发请求，经顺序任务规划、代码生成、迭代调试和最终代码审查完成工作。

Conclusion: 多代理分工协作可发挥大语言模型优势、减轻单代理局限，但存在错误传播和上下文扩展等局限，未来需构建更健壮、可扩展的软件工程自动化多代理AI系统。

Abstract: Software development is a complex, multi-phase process traditionally
requiring collaboration among individuals with diverse expertise. We propose
AgentMesh, a Python-based framework that uses multiple cooperating LLM-powered
agents to automate software development tasks. In AgentMesh, specialized agents
- a Planner, Coder, Debugger, and Reviewer - work in concert to transform a
high-level requirement into fully realized code. The Planner agent first
decomposes user requests into concrete subtasks; the Coder agent implements
each subtask in code; the Debugger agent tests and fixes the code; and the
Reviewer agent validates the final output for correctness and quality. We
describe the architecture and design of these agents and their communication,
and provide implementation details including prompt strategies and workflow
orchestration. A case study illustrates AgentMesh handling a non-trivial
development request via sequential task planning, code generation, iterative
debugging, and final code review. We discuss how dividing responsibilities
among cooperative agents leverages the strengths of large language models while
mitigating single-agent limitations. Finally, we examine current limitations -
such as error propagation and context scaling - and outline future work toward
more robust, scalable multi-agent AI systems for software engineering
automation.

</details>


### [251] [CrossPL: Evaluating Large Language Models on Cross Programming Language Code Generation](https://arxiv.org/abs/2507.19904)
*Zhanhang Xiong,Dongxia Wang,Yuekang Li,Xinyuan An,Wenhai Wang*

Main category: cs.SE

TL;DR: 提出CrossPL基准测试评估大语言模型生成跨编程语言互操作代码的能力，评估显示模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在软件工程流程中应用增加，但生成跨编程语言互操作代码的能力未被充分探索，而该能力对构建复杂系统很重要。

Method: 分析19169个多语言GitHub仓库，用156个手工有限状态机；开发基于大语言模型的管道自动提取代码片段、生成任务指令和验证功能正确性；用基于有限状态机的验证方法评估20个模型。

Result: 即使表现最好的模型在跨编程语言场景中也存在困难。

Conclusion: 该领域需要更有针对性的研究。

Abstract: As large language models (LLMs) become increasingly embedded in software
engineering workflows, a critical capability remains underexplored: generating
correct code that enables cross-programming-language (CPL) interoperability.
This skill is essential for building complex systems that integrate components
written in multiple languages via mechanisms like inter-process communication
(IPC). To bridge this gap, we present CrossPL, the first benchmark designed to
systematically evaluate LLMs' ability to generate CPL-interoperating code.
CrossPL comprises 1,982 tasks centered around IPC, covering six widely-used
programming languages and seven representative CPL techniques. We construct
this benchmark by (i) analyzing 19,169 multi-language GitHub repositories using
156 hand-crafted finite state machines (FSMs), and (ii) developing an LLM-based
pipeline that automatically extracts CPL code snippets, generates task
instructions, and validates functional correctness. We evaluate 14
state-of-the-art general-purpose LLMs and 6 code-oriented LLMs released in the
past three years on CrossPL via FSM-based validation. Results reveal that even
the best-performing models struggle with CPL scenarios, underscoring the need
for more targeted research in this space. Our benchmark and code are available
at: https://anonymous.4open.science/r/crosspl-2814.

</details>


### [252] [The Impact of Fine-tuning Large Language Models on Automated Program Repair](https://arxiv.org/abs/2507.19909)
*Roman Macháček,Anastasiia Grishina,Max Hort,Leon Moonen*

Main category: cs.SE

TL;DR: 研究不同微调技术对用于自动程序修复（APR）的大语言模型（LLM）性能的影响，发现全微调会降低性能，高效参数微调效果更好。


<details>
  <summary>Details</summary>
Motivation: 训练LLM资源消耗大，微调技术可降低成本并提升LLM在APR任务中的性能，因此研究不同微调技术对LLM用于APR的影响。

Method: 在三个APR基准上对六种不同参数规模的LLM进行实验，考虑无微调、全微调、使用LoRA和IA3的高效参数微调三种训练方案。

Result: 全微调因数据分布和过拟合问题降低了各模型的基准测试性能，高效参数微调限制可训练参数数量，取得更好结果。

Conclusion: 使用高效参数微调方法能提升LLM在APR任务中的性能。

Abstract: Automated Program Repair (APR) uses various tools and techniques to help
developers achieve functional and error-free code faster. In recent years,
Large Language Models (LLMs) have gained popularity as components in APR tool
chains because of their performance and flexibility. However, training such
models requires a significant amount of resources. Fine-tuning techniques have
been developed to adapt pre-trained LLMs to specific tasks, such as APR, and
enhance their performance at far lower computational costs than training from
scratch. In this study, we empirically investigate the impact of various
fine-tuning techniques on the performance of LLMs used for APR. Our experiments
provide insights into the performance of a selection of state-of-the-art LLMs
pre-trained on code. The evaluation is done on three popular APR benchmarks
(i.e., QuixBugs, Defects4J and HumanEval-Java) and considers six different LLMs
with varying parameter sizes (resp. CodeGen, CodeT5, StarCoder, DeepSeekCoder,
Bloom, and CodeLlama-2). We consider three training regimens: no fine-tuning,
full fine-tuning, and parameter-efficient fine-tuning (PEFT) using LoRA and
IA3. We observe that full fine-tuning techniques decrease the benchmarking
performance of various models due to different data distributions and
overfitting. By using parameter-efficient fine-tuning methods, we restrict
models in the amount of trainable parameters and achieve better results.
  Keywords: large language models, automated program repair,
parameter-efficient fine-tuning, AI4Code, AI4SE, ML4SE.

</details>


### [253] [Prometheus: Unified Knowledge Graphs for Issue Resolution in Multilingual Codebases](https://arxiv.org/abs/2507.19942)
*Zimin Chen,Yue Pan,Siyu Lu,Jiayi Xu,Claire Le Goues,Martin Monperrus,He Ye*

Main category: cs.SE

TL;DR: 现有语言模型代理解决问题能力有限，本文提出Prometheus系统，将代码库转为知识图以解决多语言问题，在多个测试集上有成果并开源。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于Python问题且依赖预构建容器，适用性受限，需解决现实和多语言代码库问题。

Method: 将整个代码库转换为统一知识图，编码文件、抽象语法树和自然语言文本，用Neo4j进行图持久化，集成DeepSeek - V3模型。

Result: 在SWE - bench Lite和SWE - bench Multilingual上分别解决28.67%和13.7%的问题，解决10个先前未解决的唯一问题，展示了跨7种编程语言的有效性，能解决GitHub上真实问题。

Conclusion: Prometheus能有效解决现实和多语言代码库问题，具有良好的扩展性，并已开源。

Abstract: Language model (LM) agents, such as SWE-agent and OpenHands, have made
progress toward automated issue resolution. However, existing approaches are
often limited to Python-only issues and rely on pre-constructed containers in
SWE-bench with reproduced issues, restricting their applicability to real-world
and work for multi-language repositories. We present Prometheus, designed to
resolve real-world issues beyond benchmark settings. Prometheus is a
multi-agent system that transforms an entire code repository into a unified
knowledge graph to guide context retrieval for issue resolution. Prometheus
encodes files, abstract syntax trees, and natural language text into a graph of
typed nodes and five general edge types to support multiple programming
languages. Prometheus uses Neo4j for graph persistence, enabling scalable and
structured reasoning over large codebases. Integrated by the DeepSeek-V3 model,
Prometheus resolves 28.67% and 13.7% of issues on SWE-bench Lite and SWE-bench
Multilingual, respectively, with an average API cost of $0.23 and $0.38 per
issue. Prometheus resolves 10 unique issues not addressed by prior work and is
the first to demonstrate effectiveness across seven programming languages.
Moreover, it shows the ability to resolve real-world GitHub issues in the
LangChain and OpenHands repositories. We have open-sourced Prometheus at:
https://github.com/Pantheon-temple/Prometheus

</details>


### [254] [PDLogger: Automated Logging Framework for Practical Software Development](https://arxiv.org/abs/2507.19951)
*Shengcheng Duan,Yihua Xu,Sheng Zhang,Shen Wang,Yue Duan*

Main category: cs.SE

TL;DR: 提出端到端日志生成技术PDLogger，在多日志场景下表现良好，评估显示多指标优于先前系统且具鲁棒性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化日志技术专注孤立子任务，无法生成完整高质量日志，忽略方法间语义依赖和候选变量范围窄。

Method: PDLogger分三步：日志位置预测（用结构化提示引导LLM）、日志生成（用逆向程序切片和扩展变量提取器丰富提示）、日志优化（级别校正和上下文敏感去重）。

Result: 在两个Java项目的3113条日志语句上评估，多个指标优于先前系统，且在不同主流LLM上表现稳定。

Conclusion: PDLogger可有效解决现有自动化日志技术问题，其开源实现利于未来研究和应用。

Abstract: Logging is indispensable for maintaining the reliability and diagnosability
of modern software, yet developers still struggle to decide where and how to
log effectively. Existing automated logging techniques focus on isolated
sub-tasks - predicting a single log position, level, or message - and therefore
cannot produce complete, high-quality log statements that reflect real-world
practice in which multiple logs often appear inside one method. They also
neglect deeper semantic dependencies among methods and consider only a narrow
set of candidate variables, leading to superficial or incomplete logs. In this
paper, we present PDLogger, the first end-to-end log generation technique
expressly designed for practical, multi-log scenarios. PDLogger operates in
three phases. (1) Log position prediction: block-type-aware structured prompts
guide a large language model (LLM) to suggest candidate positions across all
control-flow blocks of a method. (2) Log generation: backward program slicing
supplies precise inter-procedural control and data-dependency context, while an
expanded variable extractor captures both member and external function
expressions; the enriched prompt enables the LLM to emit a full log statement
(position, level, message, variables). (3) Log refinement: level correction and
context-sensitive deduplication prune false positives and redundant logs. We
evaluate PDLogger on 3,113 log statements drawn from two widely used Java
projects. Compared with the strongest prior systems, PDLogger improves
log-position precision by 139.0 percent, F1 by 69.2 percent, level accuracy by
82.3 percent, variable precision by 131.8 percent, and message quality
(BERTScore) by 65.7 percent. The framework consistently performs well with
different mainstream LLMs, demonstrating robustness and generality. PDLogger's
implementation is available as open source to foster future research and
adoption.

</details>


### [255] [The Effect of Pointer Analysis on Semantic Conflict Detection](https://arxiv.org/abs/2507.20081)
*Matheus Barbosa,Paulo Borba,Rodrigo Bonifácio,Victor Lira,Galileu Santos*

Main category: cs.SE

TL;DR: 研究指针分析对语义冲突静态分析中误报率的影响，发现其虽减少超时和误报，但增加漏报，建议探索混合分析技术。


<details>
  <summary>Details</summary>
Motivation: 现有合并工具无法检测语义冲突，已有静态分析误报率高，研究能否用指针分析降低误报率。

Method: 分别实现有和无指针分析的相同分析，在两个数据集上运行，比较准确性和计算性能。

Result: 指针分析显著减少超时和误报，但显著增加漏报，召回率和F1分数下降。

Conclusion: 在语义冲突检测中应探索结合两种实现方式的混合分析技术。

Abstract: Current merge tools don't detect semantic conflicts, which occur when changes
from different developers are textually integrated but semantically interfere
with each other. Although researchers have proposed static analyses for
detecting semantic conflicts, these analyses suffer from significant false
positive rates. To understand whether such false positives could be reduced by
using pointer analysis in the implementation of semantic conflict static
analyses, we conduct an empirical study. We implement the same analysis with
and without pointer analysis, run them on two datasets, observe how often they
differ, and compare their accuracy and computational performance. Although
pointer analysis is known to improve precision in static analysis, we find that
its effect on semantic conflict detection can be drastic: we observe a
significant reduction in timeouts and false positives, but also a significant
increase in false negatives, with prohibitive drops in recall and F1-score.
These results suggest that, in the context of semantic conflict detection, we
should explore hybrid analysis techniques, combining aspects of both
implementations we compare in our study.

</details>


### [256] [From First Use to Final Commit: Studying the Evolution of Multi-CI Service Adoption](https://arxiv.org/abs/2507.20095)
*Nitika Chopra,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 研究分析2008年1月至2024年12月间18924个GitHub上Java项目对8种CI服务的采用情况，发现近五分之一项目有使用多CI服务的模式。


<details>
  <summary>Details</summary>
Motivation: 现有研究常孤立研究单个CI服务，不清楚项目如何随时间采用和在多个服务间转换，为了解CI采用情况的演变。

Method: 分析18924个GitHub上Java项目在2008年1月至2024年12月间对8种CI服务的历史采用情况，研究服务共采用或替换频率以及不同服务的维护活动差异。

Result: 同一项目使用多个CI服务是常见模式，近五分之一项目存在，常反映CI服务迁移。

Conclusion: 该研究是最早实践中考察多CI采用的研究之一，为未来研究提供新见解，强调在不断演变的CI环境中需要支持服务选择、协调和迁移的策略与工具。

Abstract: Continuous Integration (CI) services, such as GitHub Actions and Travis CI,
are widely adopted in open-source development to automate testing and
deployment. Though existing research often examines individual services in
isolation, it remains unclear how projects adopt and transition between
multiple services over time. To understand how CI adoption is evolving across
services, we present a preliminary study analyzing the historical CI adoption
of 18,924 Java projects hosted on GitHub between January 2008 and December
2024, adopting at least one of eight CI services, namely Travis CI, AppVeyor,
CircleCI, Azure Pipelines, GitHub Actions, Bitbucket, GitLab CI, and Cirrus CI.
Specifically, we investigate: (1) how frequently CI services are co-adopted or
replaced, and (2) how maintenance activity varies across different services.
Our analysis shows that the use of multiple CI services within the same project
is a recurring pattern observed in nearly one in five projects, often
reflecting migration across CI services. Our study is among the first to
examine multi-CI adoption in practice, offering new insights for future
research and highlighting the need for strategies and tools to support service
selection, coordination, and migration in evolving CI environments.

</details>


### [257] [Learning to Align Human Code Preferences](https://arxiv.org/abs/2507.20109)
*Xin Yin,Chao Ni,Liushan Chen,Xiaohu Yang*

Main category: cs.SE

TL;DR: 本文系统研究SFT和DPO在不同代码偏好对齐中的作用，提出自适应偏好优化（APO）方法，实验验证其有效性并为选择训练策略提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在软件开发任务自动化中，不同代码偏好场景下的最优训练策略尚不明确。

Method: 通过理论分析和实证观察，提出自适应偏好优化（APO）方法，动态整合以放大偏好响应、抑制非偏好响应并鼓励探索更优解。

Result: 在六个代表性代码偏好任务的广泛实验验证了理论假设，APO表现匹配或超越现有SFT和S&D策略。

Conclusion: 为不同代码偏好对齐场景选择合适的训练策略提供了理论基础和实践指导。

Abstract: Large Language Models (LLMs) have demonstrated remarkable potential in
automating software development tasks. While recent advances leverage
Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align
models with human preferences, the optimal training strategy remains unclear
across diverse code preference scenarios. This paper systematically
investigates the roles of SFT and DPO in aligning LLMs with different code
preferences. Through both theoretical analysis and empirical observation, we
hypothesize that SFT excels in scenarios with objectively verifiable optimal
solutions, while applying SFT followed by DPO (S&D) enables models to explore
superior solutions in scenarios without objectively verifiable optimal
solutions. Based on the analysis and experimental evidence, we propose Adaptive
Preference Optimization (APO), a dynamic integration approach that adaptively
amplifies preferred responses, suppresses dispreferred ones, and encourages
exploration of potentially superior solutions during training. Extensive
experiments across six representative code preference tasks validate our
theoretical hypotheses and demonstrate that APO consistently matches or
surpasses the performance of existing SFT and S&D strategies. Our work provides
both theoretical foundations and practical guidance for selecting appropriate
training strategies in different code preference alignment scenarios.

</details>


### [258] [From Prompt to Pipeline: Large Language Models for Scientific Workflow Development in Bioinformatics](https://arxiv.org/abs/2507.20122)
*Khairul Alam,Banani Roy*

Main category: cs.SE

TL;DR: 研究评估LLMs生成生物信息学工作流能力，发现Gemini 2.5 Flash在Galaxy平台出色，DeepSeek - V3在Nextflow表现好，提示策略影响质量，LLMs结合提示工程有潜力降低工作流开发门槛。


<details>
  <summary>Details</summary>
Motivation: 生物信息学数据分析复杂，创建和理解工作流对非编程专家有挑战，研究LLMs支持生成生物信息学工作流的能力及有效提示策略。

Method: 用SNP分析等多样任务评估GPT - 4o、Gemini 2.5 Flash和DeepSeek - V3，在Galaxy和Nextflow平台测试，由专家依据社区基线评估生成的工作流。

Result: Gemini 2.5 Flash在生成Galaxy工作流中表现出色，DeepSeek - V3在Nextflow表现强，提示策略影响工作流质量，GPT - 4o受益于结构化输入，DeepSeek - V3技术细节丰富但冗长。

Conclusion: LLMs结合提示工程有潜力降低生物信息学工作流开发门槛，提高可重复性，让更多人使用计算工具。

Abstract: The increasing complexity of bioinformatics data analysis has made Scientific
Workflow Systems (SWSs) like Galaxy and Nextflow essential for enabling
scalable, reproducible, and automated workflows. However, creating and
understanding these workflows remains challenging, particularly for domain
experts without programming expertise. This study investigates whether modern
Large Language Models (LLMs), GPT-4o, Gemini 2.5 Flash, and DeepSeek-V3, can
support the generation of accurate, complete, and usable bioinformatics
workflows, and examines which prompting strategies most effectively guide this
process. We evaluate these models using diverse tasks such as SNP analysis,
RNA-seq, DNA methylation, and data retrieval, spanning both graphical (Galaxy)
and script-based (Nextflow) platforms. Expert reviewers assess the generated
workflows against community-curated baselines from the Galaxy Training Network
and nf-core repositories. The results show that Gemini 2.5 Flash excels in
generating Galaxy workflows, while DeepSeek-V3 performs strongly in Nextflow.
Prompting strategies significantly impact quality, with role-based and
chain-of-thought prompts improving completeness and correctness. While GPT-4o
benefits from structured inputs, DeepSeek-V3 offers rich technical detail,
albeit with some verbosity. Overall, the findings highlight the potential of
LLMs to lower the barrier for workflow development, improve reproducibility,
and democratize access to computational tools in bioinformatics, especially
when combined with thoughtful prompt engineering.

</details>


### [259] [Relating System Safety and Machine Learnt Model Performance](https://arxiv.org/abs/2507.20135)
*Ganesh Pai*

Main category: cs.SE

TL;DR: 本文以含机器学习组件的飞机应急制动系统为例，提出推导与安全相关的性能要求、指标及其目标的初始方法，使模型满足安全评估的定量目标。


<details>
  <summary>Details</summary>
Motivation: 在航空应用中集成机器学习模型时，系统安全目标与模型性能要求及相关指标的关系不明确，需要解决该问题。

Method: 以飞机应急制动系统为例，先对所需的机器学习组件行为进行简单抽象，再基于此推导最小的与安全相关的性能要求、相关指标及其目标。

Result: 提出了推导相关性能要求、指标及其目标的初始方法。

Conclusion: 该方法应被认为是有效的，同时明确了假设、适用约束和验证的影响。

Abstract: The prediction quality of machine learnt models and the functionality they
ultimately enable (e.g., object detection), is typically evaluated using a
variety of quantitative metrics that are specified in the associated model
performance requirements. When integrating such models into aeronautical
applications, a top-down safety assessment process must influence both the
model performance metrics selected, and their acceptable range of values.
Often, however, the relationship of system safety objectives to model
performance requirements and the associated metrics is unclear. Using an
example of an aircraft emergency braking system containing a machine learnt
component (MLC) responsible for object detection and alerting, this paper first
describes a simple abstraction of the required MLC behavior. Then, based on
that abstraction, an initial method is given to derive the minimum
safety-related performance requirements, the associated metrics, and their
targets for the both MLC and its underlying deep neural network, such that they
meet the quantitative safety objectives obtained from the safety assessment
process. We give rationale as to why the proposed method should be considered
valid, also clarifying the assumptions made, the constraints on applicability,
and the implications for verification.

</details>


### [260] [Strategic Motivators for Ethical AI System Development: An Empirical and Holistic Model](https://arxiv.org/abs/2507.20218)
*Muhammad Azeem Akbar,Arif Ali Khan,Saima Rafi,Damian Kedziora,Sami Hyrynsalmi*

Main category: cs.SE

TL;DR: 本文旨在识别和排序推动人工智能系统道德发展的激励因素，通过多种研究方法确定了20个关键激励因素并分组，给出排名，建议组织将激励因素融入策略。


<details>
  <summary>Details</summary>
Motivation: 人工智能虽带来变革机遇，但需负责任地发展，本研究旨在识别和排序推动人工智能系统道德发展的激励因素。

Method: 进行多视角文献综述和问卷调查，应用解释结构模型探索激励因素类别间关系，进行MICMAC分析分类，用模糊TOPSIS法对激励因素按重要性排名。

Result: 确定20个关键激励因素并分为八类，ISM显示“人力资源”和“协调”影响大，MICMAC分析将部分类别归为独立集群，模糊TOPSIS给出关键激励因素排名。

Conclusion: 组织应使策略与这些激励因素保持一致，并将其融入政策、治理模型和开发框架以支持道德人工智能的采用。

Abstract: Artificial Intelligence (AI) presents transformative opportunities for
industries and society, but its responsible development is essential to prevent
unintended consequences. Ethically sound AI systems demand strategic planning,
strong governance, and an understanding of the key drivers that promote
responsible practices. This study aims to identify and prioritize the
motivators that drive the ethical development of AI systems. A Multivocal
Literature Review (MLR) and a questionnaire-based survey were conducted to
capture current practices in ethical AI. We applied Interpretive Structure
Modeling (ISM) to explore the relationships between motivator categories,
followed by MICMAC analysis to classify them by their driving and dependence
power. Fuzzy TOPSIS was used to rank these motivators by importance. Twenty key
motivators were identified and grouped into eight categories: Human Resource,
Knowledge Integration, Coordination, Project Administration, Standards,
Technology Factor, Stakeholders, and Strategy & Matrices. ISM results showed
that 'Human Resource' and 'Coordination' heavily influence other factors.
MICMAC analysis placed categories like Human Resource (CA1), Coordination
(CA3), Stakeholders (CA7), and Strategy & Matrices (CA8) in the independent
cluster, indicating high driving but low dependence power. Fuzzy TOPSIS ranked
motivators such as promoting team diversity, establishing AI governance bodies,
appointing oversight leaders, and ensuring data privacy as most critical. To
support ethical AI adoption, organizations should align their strategies with
these motivators and integrate them into their policies, governance models, and
development frameworks.

</details>


### [261] [Beyond Binary Moderation: Identifying Fine-Grained Sexist and Misogynistic Behavior on GitHub with Large Language Models](https://arxiv.org/abs/2507.20358)
*Tanni Dev,Sayma Sultana,Amiangshu Bosu*

Main category: cs.SE

TL;DR: 本文引入基于指令微调大语言模型的细粒度多类分类框架识别GitHub上的性别歧视和厌女评论，优化方法表现优于基线，但在识别细微情况时有困难，精心设计的提示可提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 当前GitHub等技术社区存在性别歧视和厌女行为阻碍包容性，现有审核工具难以有效检测细微伤害，故开展研究。

Method: 利用基于指令微调大语言模型的框架，经20次系统提示优化，在1440条标注评论上评估，用多种指标比较模型性能。

Result: 优化方法（GPT - 4o与提示19）MCC为0.501，优于基线方法，假阳性低，但难以可靠解释细微的性别歧视和厌女情况。

Conclusion: 精心设计的提示可显著提高性别歧视检测的准确性和可解释性，实现GitHub等平台的精确实用审核。

Abstract: Background: Sexist and misogynistic behavior significantly hinders inclusion
in technical communities like GitHub, causing developers, especially
minorities, to leave due to subtle biases and microaggressions. Current
moderation tools primarily rely on keyword filtering or binary classifiers,
limiting their ability to detect nuanced harm effectively.
  Aims: This study introduces a fine-grained, multi-class classification
framework that leverages instruction-tuned Large Language Models (LLMs) to
identify twelve distinct categories of sexist and misogynistic comments on
GitHub.
  Method: We utilized an instruction-tuned LLM-based framework with systematic
prompt refinement across 20 iterations, evaluated on 1,440 labeled GitHub
comments across twelve sexism/misogyny categories. Model performances were
rigorously compared using precision, recall, F1-score, and the Matthews
Correlation Coefficient (MCC).
  Results: Our optimized approach (GPT-4o with Prompt 19) achieved an MCC of
0.501, significantly outperforming baseline approaches. While this model had
low false positives, it struggled to interpret nuanced, context-dependent
sexism and misogyny reliably.
  Conclusion: Well-designed prompts with clear definitions and structured
outputs significantly improve the accuracy and interpretability of sexism
detection, enabling precise and practical moderation on developer platforms
like GitHub.

</details>


### [262] [CIgrate: Automating CI Service Migration with Large Language Models](https://arxiv.org/abs/2507.20402)
*Md Nazmul Hossain,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 研究旨在评估大语言模型（LLMs）能否改进持续集成（CI）配置迁移，提出基于LLMs的CIgrate框架并计划进行多方面评估。


<details>
  <summary>Details</summary>
Motivation: CI配置在服务间迁移有需求，但手动迁移耗时易错，现有CIMig方法准确率低，而LLMs在代码生成和转换任务中有强大能力，因此考虑用LLMs改进CI迁移。

Method: 提出基于LLMs的CIgrate框架，对比CIMig，采用零样本/少样本提示和微调LLMs的方式进行评估，还会收集开发者反馈。

Result: 文中未提及具体结果。

Conclusion: 有望提出首个基于LLMs的CI服务迁移方法，对比评估其与基于规则方法的有效性，并为利用LLMs支持软件配置演变提供见解。

Abstract: Continuous Integration (CI) configurations often need to be migrated between
services (e.g., Travis CI to GitHub Actions) as projects evolve, due to changes
in service capabilities, usage limits, or service deprecation. Previous studies
reported that migration across CI services is a recurring need in open-source
development. However, manual migration can be time-consuming and error-prone.
The state-of-the-art approach, CIMig, addresses this challenge by analyzing
past migration examples to create service-specific rules and produce equivalent
configurations across CI services. However, its relatively low accuracy raises
concerns about the overall feasibility of automated CI migration using
rule-based techniques alone. Meanwhile, Large Language Models (LLMs) have
demonstrated strong capabilities in code generation and transformation tasks,
suggesting potential to improve the automation, usability, and generalizability
of CI configuration migration. This registered report presents a study in which
we aim to assess whether CI migration can be improved using LLMs. To this end,
we propose CIgrate, an LLM-based framework for automatically migrating CI
configurations. We plan to evaluate the performance of CIgrate compared to
CIMig as a baseline, in different setups (a) zero-shot/few-shot prompting of
LLMs for configuration migration and (b) fine-tuning an LLM on a dataset of
already established CI service migrations. We will also seek developer feedback
on the quality and usability of the generated configurations. We formulate
research questions focusing on the accuracy of LLM-generated migrations versus
ground truth and the output of CIMig. The expected contributions include the
first LLM-powered approach for CI service migration, a comparative evaluation
of its effectiveness compared to rule-based approaches, and insight into
leveraging LLMs to support software configuration evolution.

</details>


### [263] [Testing Is Not Boring: Characterizing Challenge in Software Testing Tasks](https://arxiv.org/abs/2507.20407)
*Davi Gama Hardman,Cesar França,Brody Stuart-Verner,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 随着软件系统复杂度增加，测试很重要但常被低估。研究软件测试人员面临的挑战任务，发现有创造性、需持续学习和有时间压力的任务有激励性，应平衡任务复杂度。


<details>
  <summary>Details</summary>
Motivation: 软件测试常被视为重复低技能活动，未认识到其所需的创造力等，为理解测试人员经历而开展研究。

Method: 对软件测试专业人员进行研究。

Result: 涉及创造力、持续学习和时间压力的任务有激励性和回报，缺乏挑战或需求过大导致沮丧和脱离。

Conclusion: 平衡任务复杂度对维持动力很重要，应将软件测试视为动态且有智力挑战的领域。

Abstract: As software systems continue to grow in complexity, testing has become a
fundamental part of ensuring the quality and reliability of software products.
Yet, software testing is still often perceived, both in industry and academia,
as a repetitive, low-skill activity. This perception fails to recognize the
creativity, problem-solving, and adaptability required in testing work. Tasks
such as designing complex test cases, automating testing processes, and
handling shifting requirements illustrate the challenges testing professionals
regularly face. To better understand these experiences, we conducted a study
with software testing professionals to explore the nature of challenging tasks
in software testing and how they affect these professionals. Our findings show
that tasks involving creativity, ongoing learning, and time pressure are often
seen as motivating and rewarding. On the other hand, a lack of challenge or
overwhelming demands can lead to frustration and disengagement. These findings
demonstrate the importance of balancing task complexity to sustain motivation
and present software testing as a dynamic and intellectually engaging field.

</details>


### [264] [When Prompts Go Wrong: Evaluating Code Model Robustness to Ambiguous, Contradictory, and Incomplete Task Descriptions](https://arxiv.org/abs/2507.20439)
*Maya Larbi,Amal Akli,Mike Papadakis,Rihab Bouyousfi,Maxime Cordy,Federica Sarro,Yves Le Traon*

Main category: cs.SE

TL;DR: 研究在任务描述不清晰时大语言模型代码生成的鲁棒性，发现小瑕疵会致性能下降，大模型也有挑战，强调开发鲁棒模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 实际任务描述常模糊、不完整或矛盾，现有研究缺乏对大语言模型在此情况下鲁棒性的实证分析。

Method: 通过引导突变策略扩展HumanEval和MBPP基准，生成反映非正式开发者指令的数据集，评估不同大小和架构的多个大语言模型。

Result: 任务描述的小瑕疵会导致性能显著下降，矛盾描述会产生大量逻辑错误，大模型更有韧性但也面临挑战，还分析了语义错误模式和相关性。

Conclusion: 开发不仅强大且对自然用户任务固有缺陷具有鲁棒性的大语言模型至关重要，为改进训练策略、设计评估基准和实际部署提供重要参考。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance in code
generation tasks under idealized conditions, where task descriptions are clear
and precise. However, in practice, task descriptions frequently exhibit
ambiguity, incompleteness, or internal contradictions. In this paper, we
present the first empirical study examining the robustness of state-of-the-art
code generation models when faced with such unclear task descriptions. We
extend the HumanEval and MBPP benchmarks by systematically introducing
realistic task descriptions flaws through guided mutation strategies, producing
a dataset that mirrors the messiness of informal developer instructions. We
evaluate multiple LLMs of varying sizes and architectures, analyzing their
functional correctness and failure modes across task descriptions categories.
Our findings reveal that even minor imperfections in task description phrasing
can cause significant performance degradation, with contradictory task
descriptions resulting in numerous logical errors. Moreover, while larger
models tend to be more resilient than smaller variants, they are not immune to
the challenges posed by unclear requirements. We further analyze semantic error
patterns and identify correlations between description clarity, model behavior,
and error types. Our results underscore the critical need for developing LLMs
that are not only powerful but also robust to the imperfections inherent in
natural user tasks, highlighting important considerations for improving model
training strategies, designing more realistic evaluation benchmarks, and
ensuring reliable deployment in practical software development environments.

</details>


### [265] [Distinguishing Quantum Software Bugs from Hardware Noise: A Statistical Approach](https://arxiv.org/abs/2507.20475)
*Ahmik Virani,Devraj,Anirudh Suresh,Lei Zhang,M V Panduranga Rao*

Main category: cs.SE

TL;DR: 提出统计方法区分量子软件错误和硬件噪声，用知名量子算法验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在NISQ时代，传统调试技术无法区分量子软件错误和硬件噪声。

Method: 提出利用概率指标的统计方法。

Result: 用知名量子算法实验验证了方法的有效性和实用性。

Conclusion: 为量子软件开发人员提供识别和分类量子程序异常行为的可靠分析工具。

Abstract: Quantum computing in the Noisy Intermediate-Scale Quantum (NISQ) era presents
significant challenges in differentiating quantum software bugs from hardware
noise. Traditional debugging techniques from classical software engineering
cannot directly resolve this issue due to the inherently stochastic nature of
quantum computation mixed with noises from NISQ computers. To address this gap,
we propose a statistical approach leveraging probabilistic metrics to
differentiate between quantum software bugs and hardware noise. We evaluate our
methodology empirically using well-known quantum algorithms, including Grover's
algorithm, Deutsch-Jozsa algorithm, and Simon's algorithm. Experimental results
demonstrate the efficacy and practical applicability of our approach, providing
quantum software developers with a reliable analytical tool to identify and
classify unexpected behavior in quantum programs.

</details>


### [266] [VDGraph: A Graph-Theoretic Approach to Unlock Insights from SBOM and SCA Data](https://arxiv.org/abs/2507.20502)
*Howell Xia,Jonah Gluck,Sevval Simsek,David Sastre Medina,David Starobinski*

Main category: cs.SE

TL;DR: 本文提出VDGraph方法整合漏洞與依賴性數據，應用於Java項目，發現集中風險點和漏洞出現規律，為分析提供基礎。


<details>
  <summary>Details</summary>
Motivation: 現代軟件供應鏈複雜，SBOM和SCA工具整合不足，缺乏複雜依賴-漏洞關係的統一視圖。

Method: 引入基於知識圖的VDGraph方法，整合SBOM和SCA輸出，給出理論描述和分析，解決數據衝突，用CycloneDX Maven插件和OSV - Scanner實現。

Result: 在21個Java項目上應用，發現集中風險點，漏洞多在三層或更高依賴層出現。

Conclusion: VDGraph改進了對漏洞在複雜傳遞依賴中傳播的可見性，實現為真實項目分析奠定基礎。

Abstract: The high complexity of modern software supply chains necessitates tools such
as Software Bill of Materials (SBOMs) to manage component dependencies, and
Software Composition Analysis (SCA) tools to identify vulnerabilities. While
there exists limited integration between SBOMs and SCA tools, a unified view of
complex dependency-vulnerability relationships remains elusive. In this paper,
we introduce VDGraph, a novel knowledge graph-based methodology for integrating
vulnerability and dependency data into a holistic view. VDGraph consolidates
SBOM and SCA outputs into a graph representation of software projects'
dependencies and vulnerabilities. We provide a formal description and analysis
of the theoretical properties of VDGraph and present solutions to manage
possible conflicts between the SBOM and SCA data. We further introduce and
evaluate a practical, proof-of-concept implementation of VDGraph using two
popular SBOM and SCA tools, namely CycloneDX Maven plugin and Google's
OSV-Scanner. We apply VDGraph on 21 popular Java projects. Through the
formulation of appropriate queries on the graphs, we uncover the existence of
concentrated risk points (i.e., vulnerable components of high severity
reachable through numerous dependency paths). We further show that
vulnerabilities predominantly emerge at a depth of three dependency levels or
higher, indicating that direct or secondary dependencies exhibit lower
vulnerability density and tend to be more secure. Thus, VDGraph contributes a
graph-theoretic methodology that improves visibility into how vulnerabilities
propagate through complex, transitive dependencies. Moreover, our
implementation, which combines open SBOM and SCA standards with Neo4j, lays a
foundation for scalable and automated analysis across real-world projects.

</details>


### [267] [GeoJSEval: An Automated Evaluation Framework for Large Language Models on JavaScript-Based Geospatial Computation and Visualization Code Generation](https://arxiv.org/abs/2507.20553)
*Guanyu Chen,Haoyue Jiao,Shuyang Hou,Ziqi Liu,Lutong Xie,Shaowen Wu,Huayi Wu,Xuefeng Guan,Zhipeng Gui*

Main category: cs.SE

TL;DR: 提出GeoJSEval评估框架评估大语言模型在JavaScript地理空间代码生成能力，评估18个模型揭示性能差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于代码生成，地理空间代码生成需系统评估方法，JavaScript环境下任务对模型能力要求高。

Method: 提出GeoJSEval框架，含标准测试套件、代码提交引擎和评估模块，有432个函数级任务和2071个测试用例。

Result: 评估18个模型，揭示在空间语义理解、代码可靠性和函数调用准确性上的性能差异和瓶颈。

Conclusion: GeoJSEval为地理空间代码生成模型评估和优化提供基础方法、资源和工具，有扩展性和实际应用价值。

Abstract: With the widespread adoption of large language models (LLMs) in code
generation tasks, geospatial code generation has emerged as a critical frontier
in the integration of artificial intelligence and geoscientific analysis. This
trend underscores the urgent need for systematic evaluation methodologies to
assess LLMs generation capabilities in geospatial contexts. In particular,
geospatial computation and visualization tasks in JavaScript environments rely
heavily on orchestrating diverse frontend libraries and ecosystems, placing
elevated demands on a model's semantic understanding and code synthesis
abilities. To address this challenge, we propose GeoJSEval--the first
multimodal, function-level automatic evaluation framework for LLMs in
JavaScript-based geospatial code generation. GeoJSEval comprises three core
components: a standardized test suite (GeoJSEval-Bench), a code submission
engine, and an evaluation module. It includes 432 function-level tasks and
2,071 structured test cases spanning five widely used JavaScript geospatial
libraries and 25 mainstream geospatial data types. GeoJSEval enables
multidimensional quantitative evaluation across metrics such as accuracy,
output stability, execution efficiency, resource consumption, and error type
distribution, and integrates boundary testing mechanisms to enhance robustness
and coverage. We conduct a comprehensive evaluation of 18 state-of-the-art LLMs
using GeoJSEval, revealing significant performance disparities and bottlenecks
in spatial semantic understanding, code reliability, and function invocation
accuracy. GeoJSEval provides a foundational methodology, evaluation resource,
and practical toolkit for the standardized assessment and optimization of
geospatial code generation models, with strong extensibility and applicability
in real-world scenarios.

</details>


### [268] [Intention-Driven Generation of Project-Specific Test Cases](https://arxiv.org/abs/2507.20619)
*Binhang Qi,Yun Lin,Xinyi Weng,Yuhuan Huang,Chenyan Liu,Hailong Sun,Jin Song Dong*

Main category: cs.SE

TL;DR: 提出IntentionTest根据验证意图生成项目特定测试，评估显示优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有自动化测试技术多基于覆盖度，实际测试需考虑开发者验证意图和项目特定知识，否则难通过代码审查。

Method: IntentionTest根据验证意图结构化描述，检索项目中可参考测试引导生成，将测试生成问题转化为编辑问题，生成含测试前缀和预言的测试。

Result: 在13个开源项目的4146个测试用例上评估，比ChatTester生成更多语义正确测试，提高突变分数和覆盖重叠度，成功通过测试数量增加。

Conclusion: IntentionTest在生成项目特定测试方面表现出色，优于现有基线。

Abstract: Test cases are valuable assets for maintaining software quality. While
numerous automated techniques have been proposed for generating tests (either
by maximizing code coverage or by translating focal code into test code),
practical tests are seldom driven by coverage alone. In real projects, each
test reflects a developer's validation intention for a specific behaviour and
embodies rich, project-specific knowledge: which specific APIs to call and what
assertions truly matter. Without considering such knowledge, tests can hardly
pass code review and be integrated into the software product.
  In this work, we propose IntentionTest, which generates project-specific
tests with validation intention as a structured description. Our design is
motivated by two insights: (1) a description of validation intention, compared
to coverage and focal code, carries more crucial information about what to
test; and (2) practical tests exhibit high code duplication, indicating that
domain knowledge is highly reusable for writing new tests. Given a focal code
and a description of validation intention (in the form of either an informal
comment or a formal test plan), IntentionTest retrieves a referable test in the
project to guide test generation. Moreover, IntentionTest reduces the test
generation problem into an editing problem on the test code regarding the
validation intention. It generates a test including both test prefix and
oracle, which aims to be executable and semantically correct.
  We evaluate IntentionTest against state-of-the-art baselines on 4,146 test
cases from 13 open-source projects. Specifically, compared to ChatTester,
IntentionTest can (1) generate significantly more semantically correct tests,
improving common mutation scores by 39.03% and coverage overlap with
ground-truth tests by 40.14%; (2) generate 21.30% more successful passing
tests.

</details>


### [269] [LLM-Based Repair of Static Nullability Errors](https://arxiv.org/abs/2507.20674)
*Nima Karimipour,Michael Pradel,Martin Kellogg,Manu Sridharan*

Main category: cs.SE

TL;DR: 提出NullRepair系统集成大语言模型解决可空性检查器错误，在12个项目平均解决72%残余错误，且基本保留程序语义。


<details>
  <summary>Details</summary>
Motivation: 现有静态分析工具集成到大型现有代码库有挑战，手动解决残余错误繁琐易错，简单提示的大语言模型生成修复不准确。

Method: NullRepair将大语言模型集成到结构化工作流，决策流程依据对200个真实错误的手动分析流程图，利用静态分析识别符号使用区域，通过与大语言模型迭代交互生成补丁。

Result: 在12个真实Java项目中，平均解决72%经先进注解推断技术处理后的残余错误，大部分项目单元测试通过。

Conclusion: NullRepair能有效解决可空性检查器的残余错误，并较好地保留程序语义。

Abstract: Modern Java projects increasingly adopt static analysis tools that prevent
null-pointer exceptions by treating nullness as a type property. However,
integrating such tools into large, existing codebases remains a significant
challenge. While annotation inference can eliminate many errors automatically,
a subset of residual errors -- typically a mix of real bugs and false positives
-- often persist and can only be resolved via code changes. Manually addressing
these errors is tedious and error-prone. Large language models (LLMs) offer a
promising path toward automating these repairs, but naively-prompted LLMs often
generate incorrect, contextually-inappropriate edits. Resolving a nullability
error demands a deep understanding of how a symbol is used across the codebase,
often spanning methods, classes, and packages. We present NullRepair, a system
that integrates LLMs into a structured workflow for resolving the errors from a
nullability checker. NullRepair's decision process follows a flowchart derived
from manual analysis of 200 real-world errors. It leverages static analysis to
identify safe and unsafe usage regions of symbols, using error-free usage
examples to contextualize model prompts. Patches are generated through an
iterative interaction with the LLM that incorporates project-wide context and
decision logic. Our evaluation on 12 real-world Java projects shows that
NullRepair resolves an average of 72% of the errors that remain after applying
a state-of-the-art annotation inference technique. Unlike a naively-prompted
LLM, NullRepair also largely preserves program semantics, with all unit tests
passing in 10/12 projects after applying every edit proposed by NullRepair, and
98% or more tests passing in the remaining two projects.

</details>


### [270] [Client--Library Compatibility Testing with API Interaction Snapshots](https://arxiv.org/abs/2507.20814)
*Gustave Monce,Thomas Degueule,Jean-Rémy Falleri,Romain Robbes*

Main category: cs.SE

TL;DR: 本文提出一种新的客户端 - 库兼容性测试方法，通过记录 API 交互快照检测库行为的破坏变化，工具 Gilesi 可有效检测客户端测试套件遗漏的问题。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发依赖第三方库，库的演化可能引入行为破坏变化（BBCs），传统回归测试难以检测此类问题。

Method: 提出利用现有客户端测试的新方法，记录 API 边界的实际交互作为快照，对比新旧快照识别合同扰动。

Result: 在人为引入 BBCs 的客户端 - 库对上进行初步案例研究，表明 Gilesi 能可靠检测客户端测试套件遗漏的 BBCs。

Conclusion: 所提出的方法和实现的工具 Gilesi 能有效检测库演化中的行为破坏变化。

Abstract: Modern software development heavily relies on third-party libraries to speed
up development and enhance quality. As libraries evolve, they may break the
tacit contract established with their clients by introducing behavioral
breaking changes (BBCs) that alter run-time behavior and silently break client
applications without being detected at compile time. Traditional regression
tests on the client side often fail to detect such BBCs, either due to limited
library coverage or weak assertions that do not sufficiently exercise the
library's expected behavior. To address this issue, we propose a novel approach
to client--library compatibility testing that leverages existing client tests
in a novel way. Instead of relying on developer-written assertions, we propose
recording the actual interactions at the API boundary during the execution of
client tests (protocol, input and output values, exceptions, etc.). These
sequences of API interactions are stored as snapshots which capture the exact
contract expected by a client at a specific point in time. As the library
evolves, we compare the original and new snapshots to identify perturbations in
the contract, flag potential BBCs, and notify clients. We implement this
technique in our prototype tool Gilesi, a Java framework that automatically
instruments library APIs, records snapshots, and compares them. Through a
preliminary case study on several client--library pairs with artificially
seeded BBCs, we show that Gilesi reliably detects BBCs missed by client test
suites.

</details>


### [271] [Enhancing Project-Specific Code Completion by Inferring Internal API Information](https://arxiv.org/abs/2507.20888)
*Le Deng,Xiaoxue Ren,Chao Ni,Ming Liang,David Lo,Zhongxin Liu*

Main category: cs.SE

TL;DR: 提出不依赖导入推断内部API信息的方法及ProjBench基准，实验表明其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于RAG和LLM的项目特定代码补全方法难以整合内部API信息，影响补全准确性。

Method: 提出不依赖导入推断内部API信息的方法，构建API使用示例和语义描述扩展表示，建立知识库；引入避免泄漏导入的ProjBench基准。

Result: 在ProjBench和CrossCodeEval上显著优于现有方法，代码精确匹配提高22.72%，标识符精确匹配提高18.31%；与现有基线集成后，代码匹配提高47.80%，标识符匹配提高35.55%。

Conclusion: 所提方法在项目特定代码补全任务中能有效提高补全准确性。

Abstract: Project-specific code completion is a critical task that leverages context
from a project to generate accurate code. State-of-the-art methods use
retrieval-augmented generation (RAG) with large language models (LLMs) and
project information for code completion. However, they often struggle to
incorporate internal API information, which is crucial for accuracy, especially
when APIs are not explicitly imported in the file.
  To address this, we propose a method to infer internal API information
without relying on imports. Our method extends the representation of APIs by
constructing usage examples and semantic descriptions, building a knowledge
base for LLMs to generate relevant completions. We also introduce ProjBench, a
benchmark that avoids leaked imports and consists of large-scale real-world
projects.
  Experiments on ProjBench and CrossCodeEval show that our approach
significantly outperforms existing methods, improving code exact match by
22.72% and identifier exact match by 18.31%. Additionally, integrating our
method with existing baselines boosts code match by 47.80% and identifier match
by 35.55%.

</details>


### [272] [Repairing vulnerabilities without invisible hands. A differentiated replication study on LLMs](https://arxiv.org/abs/2507.20977)
*Maria Camporese,Fabio Massacci*

Main category: cs.SE

TL;DR: 本文研究大语言模型在自动漏洞修复中的表现，通过在提示中添加错误、移动故障位置等方法，评估模型是否只是照搬记忆的修复方案。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在自动漏洞修复中表现优于传统技术是否是受训练数据泄露或完美故障定位等隐藏因素驱动。

Method: 在Vul4J和VJTrans基准上，将故障位置从真实位置移动n行后进行漏洞修复，用一个大语言模型生成补丁，另一个审查，再通过回归和漏洞验证测试验证结果，最后手动审计部分补丁并用Agresti - Coull - Wilson方法估计错误率。

Result: 原文未提及

Conclusion: 原文未提及

Abstract: Background: Automated Vulnerability Repair (AVR) is a fast-growing branch of
program repair. Recent studies show that large language models (LLMs)
outperform traditional techniques, extending their success beyond code
generation and fault detection.
  Hypothesis: These gains may be driven by hidden factors -- "invisible hands"
such as training-data leakage or perfect fault localization -- that let an LLM
reproduce human-authored fixes for the same code.
  Objective: We replicate prior AVR studies under controlled conditions by
deliberately adding errors to the reported vulnerability location in the
prompt. If LLMs merely regurgitate memorized fixes, both small and large
localization errors should yield the same number of correct patches, because
any offset should divert the model from the original fix.
  Method: Our pipeline repairs vulnerabilities from the Vul4J and VJTrans
benchmarks after shifting the fault location by n lines from the ground truth.
A first LLM generates a patch, a second LLM reviews it, and we validate the
result with regression and proof-of-vulnerability tests. Finally, we manually
audit a sample of patches and estimate the error rate with the
Agresti-Coull-Wilson method.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [273] [Optimal mean-variance portfolio selection under regime-switching-induced stock price shocks](https://arxiv.org/abs/2507.19824)
*Xiaomin Shi,Zuo Quan Xu*

Main category: q-fin.PM

TL;DR: 本文研究带跳跃的制度切换金融模型下的均值 - 方差投资组合选择问题，推导最优策略和有效前沿，还探讨了卖空约束情况。


<details>
  <summary>Details</summary>
Motivation: 基于股价在市场制度切换时会大幅变动的实证观察，研究制度切换下的投资组合选择问题。

Method: 运用配方法，通过求解多个多维常微分方程系统来推导最优投资组合策略和有效前沿。

Result: 得出最优投资组合策略和有效前沿由三个多维常微分方程系统刻画，无制度切换冲击时为简单线性常微分方程；卖空约束下对应Riccati方程是2ℓ维全耦合非线性常微分方程且可解。

Conclusion: 制度切换引起的股价冲击增加了模型的复杂性和挑战性，且在卖空约束下也能求解相关问题。

Abstract: In this paper, we investigate mean-variance (MV) portfolio selection problems
with jumps in a regime-switching financial model. The novelty of our approach
lies in allowing not only the market parameters -- such as the interest rate,
appreciation rate, volatility, and jump intensity -- to depend on the market
regime, but also in permitting stock prices to experience jumps when the market
regime switches, in addition to the usual micro-level jumps. This modeling
choice is motivated by empirical observations that stock prices often exhibit
sharp declines when the market shifts from a ``bullish'' to a ``bearish''
regime, and vice versa. By employing the completion-of-squares technique, we
derive the optimal portfolio strategy and the efficient frontier, both of which
are characterized by three systems of multi-dimensional ordinary differential
equations (ODEs). Among these, two systems are linear, while the first one is
an $\ell$-dimensional, fully coupled, and highly nonlinear Riccati equation. In
the absence of regime-switching-induced stock price shocks, these systems
reduce to simple linear ODEs. Thus, the introduction of
regime-switching-induced stock price shocks adds significant complexity and
challenges to our model. Additionally, we explore the MV problem under a
no-shorting constraint. In this case, the corresponding Riccati equation
becomes a $2\ell$-dimensional, fully coupled, nonlinear ODE, for which we
establish solvability. The solution is then used to explicitly express the
optimal portfolio and the efficient frontier.

</details>


### [274] [Dependency Network-Based Portfolio Design with Forecasting and VaR Constraints](https://arxiv.org/abs/2507.20039)
*Zihan Lin,Haojie Liu,Randall R. Rojas*

Main category: q-fin.PM

TL;DR: 研究提出结合统计社交网络分析、时间序列预测和风险管理的投资组合优化框架，用标普500数据构建依赖网络，提取核心结构选股票，模拟显示策略优于买入持有基准。


<details>
  <summary>Details</summary>
Motivation: 改进自适应金融决策，提出更有效的投资组合优化方法。

Method: 使用VAR和FEVD构建依赖网络，用MST算法提取核心结构，结合ARIMA和NNAR模型预测，基于VaR分配资金。

Result: MST策略优于买入持有基准，NNAR增强策略回报率达63.74%，基准为18.00%。

Conclusion: 结合网络结构、预测模型和风险指标能改善自适应金融决策。

Abstract: This study proposes a novel portfolio optimization framework that integrates
statistical social network analysis with time series forecasting and risk
management. Using daily stock data from the S&P 500 (2020-2024), we construct
dependency networks via Vector Autoregression (VAR) and Forecast Error Variance
Decomposition (FEVD), transforming influence relationships into a cost-based
network. Specifically, FEVD breaks down the VAR's forecast error variance to
quantify how much each stock's shocks contribute to another's uncertainty
information we invert to form influence-based edge weights in our network. By
applying the Minimum Spanning Tree (MST) algorithm, we extract the core
inter-stock structure and identify central stocks through degree centrality. A
dynamic portfolio is constructed using the top-ranked stocks, with capital
allocated based on Value at Risk (VaR). To refine stock selection, we
incorporate forecasts from ARIMA and Neural Network Autoregressive (NNAR)
models. Trading simulations over a one-year period demonstrate that the
MST-based strategies outperform a buy-and-hold benchmark, with the tuned
NNAR-enhanced strategy achieving a 63.74% return versus 18.00% for the
benchmark. Our results highlight the potential of combining network structures,
predictive modeling, and risk metrics to improve adaptive financial
decision-making.

</details>


### [275] [Building crypto portfolios with agentic AI](https://arxiv.org/abs/2507.20468)
*Antonino Castelli,Paolo Giudici,Alessandro Piergallini*

Main category: q-fin.PM

TL;DR: 本文应用多智能体系统构建和评估加密资产配置，比较两种自动化投资策略，发现动态优化策略表现更好，展示了多智能体系统在金融自动化中的优势。


<details>
  <summary>Details</summary>
Motivation: 应对加密市场高波动性下动态投资组合管理的挑战。

Method: 使用2020 - 2025年十大市值加密货币日频数据，比较静态等权重策略和滚动窗口优化策略，通过Crew AI的协作架构让专用智能体处理各步骤。

Result: 动态优化策略在风险调整回报上，样本内和样本外表现均显著更好。

Conclusion: 自适应技术在投资组合管理中有优势，多智能体系统能为金融自动化提供可扩展、可审计和灵活的解决方案。

Abstract: The rapid growth of crypto markets has opened new opportunities for
investors, but at the same time exposed them to high volatility. To address the
challenge of managing dynamic portfolios in such an environment, this paper
presents a practical application of a multi-agent system designed to
autonomously construct and evaluate crypto-asset allocations. Using data on
daily frequencies of the ten most capitalized cryptocurrencies from 2020 to
2025, we compare two automated investment strategies. These are a static equal
weighting strategy and a rolling-window optimization strategy, both implemented
to maximize the evaluation metrics of the Modern Portfolio Theory (MPT), such
as Expected Return, Sharpe and Sortino ratios, while minimizing volatility.
Each step of the process is handled by dedicated agents, integrated through a
collaborative architecture in Crew AI. The results show that the dynamic
optimization strategy achieves significantly better performance in terms of
risk-adjusted returns, both in-sample and out-of-sample. This highlights the
benefits of adaptive techniques in portfolio management, particularly in
volatile markets such as cryptocurrency markets. The following methodology
proposed also demonstrates how multi-agent systems can provide scalable,
auditable, and flexible solutions in financial automation.

</details>


### [276] [Your AI, Not Your View: The Bias of LLMs in Investment Analysis](https://arxiv.org/abs/2507.20957)
*Hoyoung Lee,Junhyuk Seo,Suhwan Park,Junhyeong Lee,Wonbin Ahn,Chanyeol Choi,Alejandro Lopez-Lira,Yongjae Lee*

Main category: q-fin.PM

TL;DR: 本文针对金融领域大语言模型存在的知识冲突问题，提出实验框架分析其投资偏好及确认偏差，发现多数模型有特定投资倾向且易形成确认偏差。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在金融领域存在预训练知识与实时市场数据的冲突，且其投资观点研究较少，需探究其实际投资观点。

Method: 提出实验框架，利用假设场景提取模型潜在偏好并测量其持续性，分析集中在行业、规模和动量方面。

Result: 分析揭示了不同模型特定的倾向，多数模型对大盘股和逆向投资策略有持续偏好。

Conclusion: 模型的这些偏好常演变成确认偏差，即便有反证仍坚持初始判断。

Abstract: In finance, Large Language Models (LLMs) face frequent knowledge conflicts
due to discrepancies between pre-trained parametric knowledge and real-time
market data. These conflicts become particularly problematic when LLMs are
deployed in real-world investment services, where misalignment between a
model's embedded preferences and those of the financial institution can lead to
unreliable recommendations. Yet little research has examined what investment
views LLMs actually hold. We propose an experimental framework to investigate
such conflicts, offering the first quantitative analysis of confirmation bias
in LLM-based investment analysis. Using hypothetical scenarios with balanced
and imbalanced arguments, we extract models' latent preferences and measure
their persistence. Focusing on sector, size, and momentum, our analysis reveals
distinct, model-specific tendencies. In particular, we observe a consistent
preference for large-cap stocks and contrarian strategies across most models.
These preferences often harden into confirmation bias, with models clinging to
initial judgments despite counter-evidence.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [277] [MountainLion: A Multi-Modal LLM-Based Agent System for Interpretable and Adaptive Financial Trading](https://arxiv.org/abs/2507.20474)
*Siyi Wu,Zhaoyang Guan,Leyi Zhao,Xinyuan Song,Xinyu Ying,Hanlin Zhang,Michele Pak,Yangfan He,Yi Xin,Jianhui Wang,Tianyu Shi*

Main category: q-fin.TR

TL;DR: 提出多模态多智能体系统MountainLion用于金融交易，处理多类型数据生成策略，实证显示能提供更优投资框架。


<details>
  <summary>Details</summary>
Motivation: 传统方法处理加密货币交易需大量数据且缺乏可解释性，LLM代理在处理多模态数据和支持投资决策上有进展，因此构建新系统。

Method: 构建多模态、多智能体系统MountainLion，协调专业LLM代理解释金融数据，用中央反思模块分析历史交易信号和结果，支持用户交互和问答。

Result: MountainLion能丰富技术价格触发因素，提供更具可解释性、鲁棒性和可操作性的投资框架。

Conclusion: MountainLion可提高回报，增强投资者信心。

Abstract: Cryptocurrency trading is a challenging task requiring the integration of
heterogeneous data from multiple modalities. Traditional deep learning and
reinforcement learning approaches typically demand large training datasets and
encode diverse inputs into numerical representations, often at the cost of
interpretability. Recent progress in large language model (LLM)-based agents
has demonstrated the capacity to process multi-modal data and support complex
investment decision-making. Building on these advances, we present
\textbf{MountainLion}, a multi-modal, multi-agent system for financial trading
that coordinates specialized LLM-based agents to interpret financial data and
generate investment strategies. MountainLion processes textual news,
candlestick charts, and trading signal charts to produce high-quality financial
reports, while also enabling modification of reports and investment
recommendations through data-driven user interaction and question answering. A
central reflection module analyzes historical trading signals and outcomes to
continuously refine decision processes, and the system is capable of real-time
report analysis, summarization, and dynamic adjustment of investment
strategies. Empirical results confirm that MountainLion systematically enriches
technical price triggers with contextual macroeconomic and capital flow
signals, providing a more interpretable, robust, and actionable investment
framework that improves returns and strengthens investor confidence.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [278] [Bayesian symbolic regression: Automated equation discovery from a physicists' perspective](https://arxiv.org/abs/2507.19540)
*Roger Guimera,Marta Sales-Pardo*

Main category: stat.ML

TL;DR: 讨论符号回归的概率方法，对比启发式方法优势并提及考虑模型集合。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归的标准方法和深度学习方法依赖启发式准则，需替代方法。

Method: 介绍概率方法，从基本考虑和显式近似建立模型合理性。

Result: 概率方法能提供启发式方法所缺乏的性能保证。

Conclusion: 概率方法是符号回归启发式方法的替代方案，且需考虑模型集合。

Abstract: Symbolic regression automates the process of learning closed-form
mathematical models from data. Standard approaches to symbolic regression, as
well as newer deep learning approaches, rely on heuristic model selection
criteria, heuristic regularization, and heuristic exploration of model space.
Here, we discuss the probabilistic approach to symbolic regression, an
alternative to such heuristic approaches with direct connections to information
theory and statistical physics. We show how the probabilistic approach
establishes model plausibility from basic considerations and explicit
approximations, and how it provides guarantees of performance that heuristic
approaches lack. We also discuss how the probabilistic approach compels us to
consider model ensembles, as opposed to single models.

</details>


### [279] [Adaptive Bayesian Data-Driven Design of Reliable Solder Joints for Micro-electronic Devices](https://arxiv.org/abs/2507.19663)
*Leo Guo,Adwait Inamdar,Willem D. van Driel,GuoQi Zhang*

Main category: stat.ML

TL;DR: 本文提出自适应贝叶斯优化框架，用于解决焊点可靠性问题，相比常规贝叶斯优化更高效，节省计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决焊点可靠性模拟计算成本高的问题，弥补相关文献不足。

Method: 提出具有自适应超参数的贝叶斯优化启发式框架，在不同优化迭代中执行该框架，并与常规贝叶斯优化比较。

Result: 在合成目标最小化问题和焊点可靠性工程用例中，自适应贝叶斯优化比常规贝叶斯优化平均高3%，节省一半计算成本。

Conclusion: 自适应贝叶斯数据驱动方法有潜力取得更好结果并削减优化相关费用，且开源实现促进结果可重复性。

Abstract: Solder joint reliability related to failures due to thermomechanical loading
is a critically important yet physically complex engineering problem. As a
result, simulated behavior is oftentimes computationally expensive. In an
increasingly data-driven world, the usage of efficient data-driven design
schemes is a popular choice. Among them, Bayesian optimization (BO) with
Gaussian process regression is one of the most important representatives. The
authors argue that computational savings can be obtained from exploiting
thorough surrogate modeling and selecting a design candidate based on multiple
acquisition functions. This is feasible due to the relatively low computational
cost, compared to the expensive simulation objective. This paper addresses the
shortcomings in the adjacent literature by providing and implementing a novel
heuristic framework to perform BO with adaptive hyperparameters across the
various optimization iterations. Adaptive BO is subsequently compared to
regular BO when faced with synthetic objective minimization problems. The
results show the efficiency of adaptive BO when compared any worst-performing
regular Bayesian schemes. As an engineering use case, the solder joint
reliability problem is tackled by minimizing the accumulated non-linear creep
strain under a cyclic thermal load. Results show that adaptive BO outperforms
regular BO by 3% on average at any given computational budget threshold,
critically saving half of the computational expense budget. This practical
result underlines the methodological potential of the adaptive Bayesian
data-driven methodology to achieve better results and cut optimization-related
expenses. Lastly, in order to promote the reproducibility of the results, the
data-driven implementations are made available on an open-source basis.

</details>


### [280] [Bag of Coins: A Statistical Probe into Neural Confidence Structures](https://arxiv.org/abs/2507.19774)
*Agnideep Aich,Ashit Baran Aich,Md Monzur Murshed,Sameera Hewage,Bruce Wade*

Main category: stat.ML

TL;DR: 提出Bag - of - Coins (BoC)测试来检查分类器对数的内部一致性，在ViTs上校准效果好，在CNNs上揭示预测与内部结构不一致，是理解模型不确定性的新工具。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络置信度分数校准不佳，现有校准方法不检查预测内部一致性，需新方法解决。

Method: 引入非参数统计探针BoC测试，将置信度估计转化为频率论假设检验。

Result: 在ViTs上作为置信度分数实现近乎完美校准，ECE为0.0212，比基线提升88%；在CNNs上揭示预测与内部对数结构的深层不一致。

Conclusion: BoC不仅是校准方法，还是理解流行架构表示不确定性差异的新诊断工具。

Abstract: Modern neural networks, despite their high accuracy, often produce poorly
calibrated confidence scores, limiting their reliability in high-stakes
applications. Existing calibration methods typically post-process model outputs
without interrogating the internal consistency of the predictions themselves.
In this work, we introduce a novel, non-parametric statistical probe, the
Bag-of-Coins (BoC) test, that examines the internal consistency of a
classifier's logits. The BoC test reframes confidence estimation as a
frequentist hypothesis test: does the model's top-ranked class win 1-v-1
contests against random competitors at a rate consistent with its own stated
softmax probability? When applied to modern deep learning architectures, this
simple probe reveals a fundamental dichotomy. On Vision Transformers (ViTs),
the BoC output serves as a state-of-the-art confidence score, achieving
near-perfect calibration with an ECE of 0.0212, an 88% improvement over a
temperature-scaled baseline. Conversely, on Convolutional Neural Networks
(CNNs) like ResNet, the probe reveals a deep inconsistency between the model's
predictions and its internal logit structure, a property missed by traditional
metrics. We posit that BoC is not merely a calibration method, but a new
diagnostic tool for understanding and exposing the differing ways that popular
architectures represent uncertainty.

</details>


### [281] [Sparse-mode Dynamic Mode Decomposition for Disambiguating Local and Global Structures](https://arxiv.org/abs/2507.19787)
*Sara M. Ichinaga,Steven L. Brunton,Aleksandr Y. Aravkin,J. Nathan Kutz*

Main category: stat.ML

TL;DR: 提出稀疏模式动态模式分解（DMD），分析合成与真实系统验证其效果。


<details>
  <summary>Details</summary>
Motivation: 在优化DMD框架基础上，利用稀疏促进正则化来近似具有局部空间结构的DMD模式，区分局部和全局模式。

Method: 引入稀疏模式DMD，利用稀疏促进正则化，在无监督情况下构建频谱不同部分。

Result: 通过分析合成和真实世界系统，包括光波导、量子力学和海面温度数据等例子进行了验证。

Conclusion: 稀疏模式DMD能在保持优化DMD抗噪声特性的同时，区分局部和全局模式并构建频谱。

Abstract: The dynamic mode decomposition (DMD) is a data-driven approach that extracts
the dominant features from spatiotemporal data. In this work, we introduce
sparse-mode DMD, a new variant of the optimized DMD framework that specifically
leverages sparsity-promoting regularization in order to approximate DMD modes
which have localized spatial structure. The algorithm maintains the
noise-robust properties of optimized DMD while disambiguating between modes
which are spatially local versus global in nature. In many applications, such
modes are associated with discrete and continuous spectra respectively, thus
allowing the algorithm to explicitly construct, in an unsupervised manner, the
distinct portions of the spectrum. We demonstrate this by analyzing synthetic
and real-world systems, including examples from optical waveguides, quantum
mechanics, and sea surface temperature data.

</details>


### [282] [Predicting Parkinson's Disease Progression Using Statistical and Neural Mixed Effects Models: A Comparative Study on Longitudinal Biomarkers](https://arxiv.org/abs/2507.20058)
*Ran Tong,Lanruo Wang,Tong Wang,Wei Yan*

Main category: stat.ML

TL;DR: 研究对比LMMs与两种先进混合方法在预测帕金森病进展中的表现，为相关研究和临床应用提供指导。


<details>
  <summary>Details</summary>
Motivation: 帕金森病进展预测至关重要，语音生物标志物可用于远程监测，但分析纵向数据存在挑战，需要评估不同模型性能。

Method: 将LMMs与GNMM、NME两种先进混合方法进行对比，使用牛津帕金森病远程监测语音数据集评估模型在预测总UPDRS方面的表现。

Result: 文档未提及具体结果。

Conclusion: 文档未提及具体结论。

Abstract: Predicting Parkinson's Disease (PD) progression is crucial, and voice
biomarkers offer a non-invasive method for tracking symptom severity (UPDRS
scores) through telemonitoring. Analyzing this longitudinal data is challenging
due to within-subject correlations and complex, nonlinear patient-specific
progression patterns. This study benchmarks LMMs against two advanced hybrid
approaches: the Generalized Neural Network Mixed Model (GNMM) (Mandel 2021),
which embeds a neural network within a GLMM structure, and the Neural Mixed
Effects (NME) model (Wortwein 2023), allowing nonlinear subject-specific
parameters throughout the network. Using the Oxford Parkinson's telemonitoring
voice dataset, we evaluate these models' performance in predicting Total UPDRS
to offer practical guidance for PD research and clinical applications.

</details>


### [283] [Statistical Inference for Differentially Private Stochastic Gradient Descent](https://arxiv.org/abs/2507.20560)
*Xintao Xia,Linjun Zhang,Zhanrui Cai*

Main category: stat.ML

TL;DR: 本文研究机器学习中隐私保护，建立随机规则下SGD渐近性质并拓展到DP - SGD，提出构建置信区间方法，数值分析表明方法有效并能保持隐私。


<details>
  <summary>Details</summary>
Motivation: 现有SGD统计推断方法多关注循环子采样，而DP - SGD需要随机子采样，存在研究差距。

Method: 建立随机规则下SGD渐近性质并拓展到DP - SGD，提出插件法和随机缩放法构建置信区间，进行数值分析。

Result: DP - SGD输出的渐近方差可分解为统计、采样和隐私诱导成分，提出的置信区间达到名义覆盖率并保持隐私。

Conclusion: 所提构建置信区间的方法有效，能在保持隐私的同时实现名义覆盖率。

Abstract: Privacy preservation in machine learning, particularly through Differentially
Private Stochastic Gradient Descent (DP-SGD), is critical for sensitive data
analysis. However, existing statistical inference methods for SGD predominantly
focus on cyclic subsampling, while DP-SGD requires randomized subsampling. This
paper first bridges this gap by establishing the asymptotic properties of SGD
under the randomized rule and extending these results to DP-SGD. For the output
of DP-SGD, we show that the asymptotic variance decomposes into statistical,
sampling, and privacy-induced components. Two methods are proposed for
constructing valid confidence intervals: the plug-in method and the random
scaling method. We also perform extensive numerical analysis, which shows that
the proposed confidence intervals achieve nominal coverage rates while
maintaining privacy.

</details>


### [284] [Multivariate Conformal Prediction via Conformalized Gaussian Scoring](https://arxiv.org/abs/2507.20941)
*Sacha Braun,Eugène Berta,Michael I. Jordan,Francis Bach*

Main category: stat.ML

TL;DR: 本文提出基于高斯分数的保形预测方法，避免采样成本，能实现多种扩展，且在多元设置中逼近条件覆盖效果更好。


<details>
  <summary>Details</summary>
Motivation: 传统基于经验累积分布函数的非一致性分数计算成本高，需要避免采样成本以实现条件依赖的保形预测。

Method: 观察到基于累积分布函数的分数在高斯分数情况下可简化为马氏距离，得到可直接保形化的闭式表达式，并对基本保形方法进行多种扩展。

Result: 在多元设置中，该方法产生的保形集比其他方法更接近条件覆盖。

Conclusion: 基于高斯分数的保形预测方法能避免采样成本，实现多种扩展且逼近条件覆盖效果更好。

Abstract: While achieving exact conditional coverage in conformal prediction is
unattainable without making strong, untestable regularity assumptions, the
promise of conformal prediction hinges on finding approximations to conditional
guarantees that are realizable in practice. A promising direction for obtaining
conditional dependence for conformal sets--in particular capturing
heteroskedasticity--is through estimating the conditional density
$\mathbb{P}_{Y|X}$ and conformalizing its level sets. Previous work in this
vein has focused on nonconformity scores based on the empirical cumulative
distribution function (CDF). Such scores are, however, computationally costly,
typically requiring expensive sampling methods. To avoid the need for sampling,
we observe that the CDF-based score reduces to a Mahalanobis distance in the
case of Gaussian scores, yielding a closed-form expression that can be directly
conformalized. Moreover, the use of a Gaussian-based score opens the door to a
number of extensions of the basic conformal method; in particular, we show how
to construct conformal sets with missing output values, refine conformal sets
as partial information about $Y$ becomes available, and construct conformal
sets on transformations of the output space. Finally, empirical results
indicate that our approach produces conformal sets that more closely
approximate conditional coverage in multivariate settings compared to
alternative methods.

</details>


### [285] [Locally Adaptive Conformal Inference for Operator Models](https://arxiv.org/abs/2507.20975)
*Trevor Harris,Yan Liu*

Main category: stat.ML

TL;DR: 提出Local Spectral Conformal Inference (LSCI)框架用于神经算子模型不确定性量化，证明有限样本覆盖性，在任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经算子缺乏预测不确定性概念，需对神经算子模型进行不确定性量化。

Method: 提出LSCI框架，使用投影深度评分和局部共形推理生成具有统计保证的函数值预测集。

Result: 在合成和真实世界算子学习任务中，实现了适应性和覆盖率的显著提升。

Conclusion: LSCI框架可用于神经算子模型的局部自适应、无分布不确定性量化，有统计保证。

Abstract: Operator models are regression algorithms for functional data and have become
a key tool for emulating large-scale dynamical systems. Recent advances in deep
neural operators have dramatically improved the accuracy and scalability of
operator modeling, but lack an inherent notion of predictive uncertainty. We
introduce Local Spectral Conformal Inference (LSCI), a new framework for
locally adaptive, distribution-free uncertainty quantification for neural
operator models. LSCI uses projection-based depth scoring and localized
conformal inference to generate function-valued prediction sets with
statistical guarantees. We prove approximate finite-sample marginal coverage
under local exchangeability, and demonstrate significant gains in adaptivity
and coverage across synthetic and real-world operator learning tasks.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [286] [Efficient Memristive Spiking Neural Networks Architecture with Supervised In-Situ STDP Method](https://arxiv.org/abs/2507.20998)
*Santlal Prajapati,Susmita Sur-Kolay,Soumyadeep Dutta*

Main category: cs.ET

TL;DR: 本文提出基于忆阻器的脉冲神经网络架构及学习算法，在模式识别和分类任务中表现良好，还分析了故障和器件变化影响。


<details>
  <summary>Details</summary>
Motivation: 忆阻器脉冲神经网络可实现超低能耗计算，适用于电池供电智能设备，需提出高效架构和算法。

Method: 提出受STDP启发的新型监督原位学习算法，设计电路级忆阻SNN架构，实现侧向抑制和不应期，并行更新获胜神经元突触，采用模块化设计。

Result: 在LTspice中评估，实现完美模式识别，Iris数据集分类准确率99.11%，BCW数据集97.9%，在20%输入噪声下平均识别率93.4%。

Conclusion: 所提架构和算法有效，在模式识别和分类任务中表现出色，且具有鲁棒性。

Abstract: Memristor-based Spiking Neural Networks (SNNs) with temporal spike encoding
enable ultra-low-energy computation, making them ideal for battery-powered
intelligent devices. This paper presents a circuit-level memristive spiking
neural network (SNN) architecture trained using a proposed novel supervised
in-situ learning algorithm inspired by spike-timing-dependent plasticity
(STDP). The proposed architecture efficiently implements lateral inhibition and
the refractory period, eliminating the need for external microcontrollers or
ancillary control hardware. All synapses of the winning neurons are updated in
parallel, enhancing training efficiency. The modular design ensures scalability
with respect to input data dimensions and output class count. The SNN is
evaluated in LTspice for pattern recognition (using 5x3 binary images) and
classification tasks using the Iris and Breast Cancer Wisconsin (BCW) datasets.
During testing, the system achieved perfect pattern recognition and high
classification accuracies of 99.11\% (Iris) and 97.9\% (BCW). Additionally, it
has demonstrated robustness, maintaining an average recognition rate of 93.4\%
under 20\% input noise. The impact of stuck-at-conductance faults and memristor
device variations was also analyzed.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [287] [MIMII-Agent: Leveraging LLMs with Function Calling for Relative Evaluation of Anomalous Sound Detection](https://arxiv.org/abs/2507.20666)
*Harsh Purohit,Tomoya Nishida,Kota Dohi,Takashi Endo,Yohei Kawaguchi*

Main category: eess.AS

TL;DR: 本文提出利用大语言模型将正常机器声音转换为异常声音的方法，以评估无监督异常声音检测系统，实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于关键字的数据增强方法有局限性，高级音频生成模型依赖异常训练数据，在缺乏多样异常示例时效果不佳，需新方法评估无监督异常声音检测系统。

Method: 提出利用大语言模型解读故障文本描述，自动选择音频转换函数，将正常机器声音转换为异常声音的合成方法。

Result: 评估仅用五种机器正常声音训练的无监督异常声音检测系统，合成与真实异常的相对检测难度有一致趋势。

Conclusion: 支持假设，凸显基于大语言模型的合成方法对无监督异常声音检测系统相对评估的有效性。

Abstract: This paper proposes a method for generating machine-type-specific anomalies
to evaluate the relative performance of unsupervised anomalous sound detection
(UASD) systems across different machine types, even in the absence of real
anomaly sound data. Conventional keyword-based data augmentation methods often
produce unrealistic sounds due to their reliance on manually defined labels,
limiting scalability as machine types and anomaly patterns diversify. Advanced
audio generative models, such as MIMII-Gen, show promise but typically depend
on anomalous training data, making them less effective when diverse anomalous
examples are unavailable. To address these limitations, we propose a novel
synthesis approach leveraging large language models (LLMs) to interpret textual
descriptions of faults and automatically select audio transformation functions,
converting normal machine sounds into diverse and plausible anomalous sounds.
We validate this approach by evaluating a UASD system trained only on normal
sounds from five machine types, using both real and synthetic anomaly data.
Experimental results reveal consistent trends in relative detection difficulty
across machine types between synthetic and real anomalies. This finding
supports our hypothesis and highlights the effectiveness of the proposed
LLM-based synthesis approach for relative evaluation of UASD systems.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [288] [Operator Inference Aware Quadratic Manifolds with Isotropic Reduced Coordinates for Nonintrusive Model Reduction](https://arxiv.org/abs/2507.20463)
*Paul Schwerdtner,Prakash Mohan,Julie Bessac,Marc T. Henry de Frahan,Benjamin Peherstorfer*

Main category: math.DS

TL;DR: 提出一种贪心训练方法训练二次流形用于非侵入式降阶建模，能考虑重建误差和预测误差，实验显示该方法训练的流形使降阶模型精度更高。


<details>
  <summary>Details</summary>
Motivation: 传统二次流形训练方法只最小化快照数据重建误差，忽略下游学习步骤中拟合嵌入式数据的模型误差。

Method: 提出一种贪心训练程序，同时考虑快照数据的重建误差和拟合数据的降阶模型的预测误差。

Result: 数值实验表明，用该贪心方法训练的二次流形使降阶模型的精度比仅考虑重建误差训练的二次流形高两个数量级。

Conclusion: 该贪心训练方法能避免不利于学习准确降阶模型的振荡和非光滑嵌入，可有效提高降阶模型的精度。

Abstract: Quadratic manifolds for nonintrusive reduced modeling are typically trained
to minimize the reconstruction error on snapshot data, which means that the
error of models fitted to the embedded data in downstream learning steps is
ignored. In contrast, we propose a greedy training procedure that takes into
account both the reconstruction error on the snapshot data and the prediction
error of reduced models fitted to the data. Because our procedure learns
quadratic manifolds with the objective of achieving accurate reduced models, it
avoids oscillatory and other non-smooth embeddings that can hinder learning
accurate reduced models. Numerical experiments on transport and turbulent flow
problems show that quadratic manifolds trained with the proposed greedy
approach lead to reduced models with up to two orders of magnitude higher
accuracy than quadratic manifolds trained with respect to the reconstruction
error alone.

</details>


<div id='math.AG'></div>

# math.AG [[Back]](#toc)

### [289] [TokenBlowUp: Resolving Representational Singularities in LLM Token Spaces via Monoidal Transformations](https://arxiv.org/abs/2507.19747)
*Dongfang Zhao*

Main category: math.AG

TL;DR: 本文挑战大语言模型（LLMs）的流形假设，指出多义词周围存在几何奇点，提出用概型理论解决该问题，构建新嵌入空间并证明其正则化，还探讨架构影响。


<details>
  <summary>Details</summary>
Motivation: 现有工作揭示大语言模型（LLMs）的词嵌入空间存在几何奇点，导致表征不稳定，而预设光滑数据流形的现有方法无法解决此问题。

Method: 用概型理论形式化问题，在每个奇点处应用概型理论的爆破，用例外除子替换奇点。

Result: 构建了新的嵌入几何空间，证明了新空间的几何正则化，解决了原有的病理问题。

Conclusion: 建议从静态查找向动态、基于几何的计算进行范式转变。

Abstract: Recent work has provided compelling evidence challenging the foundational
manifold hypothesis for the token embedding spaces of Large Language Models
(LLMs). These findings reveal the presence of geometric singularities around
polysemous tokens, which can lead to representational instability. Existing
methodologies, which presuppose a smooth data manifold, are ill-equipped to
address such intrinsic structural flaws. In this paper, we formalize this
problem in the language of scheme theory and propose a rigorous resolution by
applying the scheme-theoretic blow-up at each singular point. This procedure
replaces a singular point in the ambient affine scheme with its exceptional
divisor, which we identify as a canonical geometric space -- a projective space
of directions -- that houses the disambiguated semantic meanings of the token.
This process of ``representational desingularization'' constructs a new
geometric landscape for embeddings. We prove a formal theorem guaranteeing the
geometric regularization of this new space, showing that the original
pathologies are resolved. Finally, we outline the architectural implications of
our framework, arguing for a paradigm shift from static look-ups to dynamic,
geometrically-grounded computation.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [290] [A Machine Learning Framework for Predicting Microphysical Properties of Ice Crystals from Cloud Particle Imagery](https://arxiv.org/abs/2507.19759)
*Joseph Ko,Jerry Harrington,Kara Sulia,Vanessa Przybylo,Marcus van Lier-Walqui,Kara Lamb*

Main category: physics.ao-ph

TL;DR: 本文提出从原位二维图像预测冰晶三维微物理特性的框架，用合成冰晶训练机器学习模型，测试显示模型预测精度高，还量化了双视角的预测提升。


<details>
  <summary>Details</summary>
Motivation: 冰晶微物理特性对气候影响大，但测量关键特性有挑战，需新方法预测其特性。

Method: 用3D建模软件结合实地活动估计的几何参数生成合成冰晶，用其训练机器学习模型预测微物理特性。

Result: 模型在未见过的合成图像上测试，单视角模型预测精度高，双视角ResNet - 18模型进一步提升预测效果。

Conclusion: 此工作提供了基于机器学习从原位图像估计冰晶微物理特性的新框架，可用于下游微物理参数化约束。

Abstract: The microphysical properties of ice crystals are important because they
significantly alter the radiative properties and spatiotemporal distributions
of clouds, which in turn strongly affect Earth's climate. However, it is
challenging to measure key properties of ice crystals, such as mass or
morphological features. Here, we present a framework for predicting
three-dimensional (3D) microphysical properties of ice crystals from in situ
two-dimensional (2D) imagery. First, we computationally generate synthetic ice
crystals using 3D modeling software along with geometric parameters estimated
from the 2021 Ice Cryo-Encapsulation Balloon (ICEBall) field campaign. Then, we
use synthetic crystals to train machine learning (ML) models to predict
effective density ($\rho_{e}$), effective surface area ($A_e$), and number of
bullets ($N_b$) from synthetic rosette imagery. When tested on unseen synthetic
images, we find that our ML models can predict microphysical properties with
high accuracy. For $\rho_{e}$ and $A_e$, respectively, our best-performing
single view models achieved $R^2$ values of 0.99 and 0.98. For $N_b$, our best
single view model achieved a balanced accuracy and F1 score of 0.91. We also
quantify the marginal prediction improvements from incorporating a second view.
A stereo view ResNet-18 model reduced RMSE by 40% for both $\rho_e$ and $A_e$,
relative to a single view ResNet-18 model. For $N_b$, we find that a stereo
view ResNet-18 model improved the F1 score by 8%. This work provides a novel
ML-driven framework for estimating ice microphysical properties from in situ
imagery, which will allow for downstream constraints on microphysical
parameterizations, such as the mass-size relationship.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [291] [The Power of Negation in Higher-Order Datalog](https://arxiv.org/abs/2507.20251)
*Angelos Charalambidis,Babis Kostopoulos,Christos Nomikos,Panos Rondogiannis*

Main category: cs.PL

TL;DR: 研究高阶Datalog$^
eg$在两种语义下的表达能力并与复杂度类建立联系。


<details>
  <summary>Details</summary>
Motivation: 探究高阶Datalog$^
eg$在不同语义下的表达能力，建立与复杂度类的联系。

Method: 通过使用语言的存在谓词变量、部分应用关系和关系枚举进行证明。

Result: 在有根基语义下，$(k + 1)$阶Datalog$^
eg$捕获k - EXP；在稳定模型语义下，分别捕获co - (k - NEXP)和k - NEXP，分层片段也有类似结果。

Conclusion: 建立了表达能力的层次结构，凸显了高阶逻辑编程中阶数和非确定性之间的权衡。

Abstract: We investigate the expressive power of Higher-Order Datalog$^\neg$ under both
the well-founded and the stable model semantics, establishing tight connections
with complexity classes. We prove that under the well-founded semantics, for
all $k\geq 1$, $(k+1)$-Order Datalog$^\neg$ captures k-EXP, a result that holds
without explicit ordering of the input database. The proof of this fact can be
performed either by using the powerful existential predicate variables of the
language or by using partially applied relations and relation enumeration.
Furthermore, we demonstrate that this expressive power is retained within a
stratified fragment of the language. Under the stable model semantics, we show
that $(k+1)$-Order Datalog$^\neg$ captures co-(k-NEXP) using cautious reasoning
and k-NEXP using brave reasoning, again with analogous results for the
stratified fragment augmented with choice rules. Our results establish a
hierarchy of expressive power, highlighting an interesting trade-off between
order and non-determinism in the context of higher-order logic programming:
increasing the order of programs under the well-founded semantics can surpass
the expressive power of lower-order programs under the stable model semantics.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [292] [A note on the Artstein-Avidan-Milman's generalized Legendre transforms](https://arxiv.org/abs/2507.20577)
*Frank Nielsen*

Main category: cs.IT

TL;DR: 证明广义Legendre变换对应于对偶对应仿射变形函数上的普通Legendre变换，并从信息几何角度解释结果。


<details>
  <summary>Details</summary>
Motivation: 基于Artstein - Avidan和Milman对可逆逆序变换的刻画，进一步研究广义Legendre变换与普通Legendre变换的关系。

Method: 证明广义Legendre变换与普通Legendre变换在对偶对应仿射变形函数上的对应关系。

Result: 广义Legendre变换对应于对偶对应仿射变形函数上的普通Legendre变换，广义凸共轭是仿射变形函数的凸共轭。

Conclusion: 给出从信息几何角度对结果的解释。

Abstract: Artstein-Avidan and Milman [Annals of mathematics (2009), (169):661-674]
characterized invertible reverse-ordering transforms on the space of
lower-semi-continuous extended real-valued convex functions as affine
deformations of the ordinary Legendre transform. In this note, we prove that
all those generalized Legendre transforms on functions correspond to the
ordinary Legendre transform on dually corresponding affine-deformed functions.
That is, generalized convex conjugates are convex conjugates of affine-deformed
functions. We conclude this note by sketching how this result can be
interpreted from the lens of information geometry.

</details>


### [293] [Handoff Design in User-Centric Cell-Free Massive MIMO Networks Using DRL](https://arxiv.org/abs/2507.20966)
*Hussein A. Ammar,Raviraj Adve,Shahram Shahbazpanahi,Gary Boudreau,Israfil Bahceci*

Main category: cs.IT

TL;DR: 提出基于深度强化学习（DRL）的解决方案来预测和管理无蜂窝大规模MIMO网络中移动用户的连接，开发两种系统变体，仿真表明该方法可扩展性好、能减少切换开销且可实时运行。


<details>
  <summary>Details</summary>
Motivation: 用户移动性使无蜂窝大规模MIMO网络需更新服务接入点集，频繁切换会带来资源分配和释放的开销。

Method: 采用Soft Actor - Critic算法，以连续动作空间表示训练深度神经网络作为切换策略，提出包含切换惩罚的奖励函数，开发基于用户移动模式的移动方向辅助（DA）观察和基于大尺度衰落历史的历史辅助（HA）观察两种系统变体。

Result: DRL的连续动作空间方法比离散空间方法更具可扩展性，所推导的切换策略能自动将切换集中在特定时隙以减少开销，响应时间小于0.4ms。

Conclusion: 基于DRL的连续动作空间解决方案可有效预测和管理移动用户连接，减少切换开销并能实时运行。

Abstract: In the user-centric cell-free massive MIMO (UC-mMIMO) network scheme, user
mobility necessitates updating the set of serving access points to maintain the
user-centric clustering. Such updates are typically performed through handoff
(HO) operations; however, frequent HOs lead to overheads associated with the
allocation and release of resources. This paper presents a deep reinforcement
learning (DRL)-based solution to predict and manage these connections for
mobile users. Our solution employs the Soft Actor-Critic algorithm, with
continuous action space representation, to train a deep neural network to serve
as the HO policy. We present a novel proposition for a reward function that
integrates a HO penalty in order to balance the attainable rate and the
associated overhead related to HOs. We develop two variants of our system; the
first one uses mobility direction-assisted (DA) observations that are based on
the user movement pattern, while the second one uses history-assisted (HA)
observations that are based on the history of the large-scale fading (LSF).
Simulation results show that our DRL-based continuous action space approach is
more scalable than discrete space counterpart, and that our derived HO policy
automatically learns to gather HOs in specific time slots to minimize the
overhead of initiating HOs. Our solution can also operate in real time with a
response time less than 0.4 ms.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [294] [LargeMvC-Net: Anchor-based Deep Unfolding Network for Large-scale Multi-view Clustering](https://arxiv.org/abs/2507.20980)
*Shide Du,Chunming Wu,Zihan Fang,Wendi Zhao,Yilin Wu,Changwei Wang,Shiping Wang*

Main category: cs.CV

TL;DR: 本文提出LargeMvC - Net网络解决现有深度基于锚点多视图聚类方法不足，实验显示其在有效性和可扩展性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于锚点的多视图聚类方法在结合锚点结构时采用启发式或与任务无关方式，忽略核心结构需求和关键优化原则，本文旨在解决此问题。

Method: 重新审视大规模基于锚点的多视图聚类的优化问题，将其迭代解展开为LargeMvC - Net网络架构，分解为三个模块，使用无监督重建损失使各视图与锚点诱导的潜在空间对齐。

Result: 在多个大规模多视图基准测试中，LargeMvC - Net在有效性和可扩展性方面始终优于现有方法。

Conclusion: LargeMvC - Net能有效解决现有基于锚点的多视图聚类方法的不足，具有良好性能。

Abstract: Deep anchor-based multi-view clustering methods enhance the scalability of
neural networks by utilizing representative anchors to reduce the computational
complexity of large-scale clustering. Despite their scalability advantages,
existing approaches often incorporate anchor structures in a heuristic or
task-agnostic manner, either through post-hoc graph construction or as
auxiliary components for message passing. Such designs overlook the core
structural demands of anchor-based clustering, neglecting key optimization
principles. To bridge this gap, we revisit the underlying optimization problem
of large-scale anchor-based multi-view clustering and unfold its iterative
solution into a novel deep network architecture, termed LargeMvC-Net. The
proposed model decomposes the anchor-based clustering process into three
modules: RepresentModule, NoiseModule, and AnchorModule, corresponding to
representation learning, noise suppression, and anchor indicator estimation.
Each module is derived by unfolding a step of the original optimization
procedure into a dedicated network component, providing structural clarity and
optimization traceability. In addition, an unsupervised reconstruction loss
aligns each view with the anchor-induced latent space, encouraging consistent
clustering structures across views. Extensive experiments on several
large-scale multi-view benchmarks show that LargeMvC-Net consistently
outperforms state-of-the-art methods in terms of both effectiveness and
scalability.

</details>


### [295] [An Automated Deep Segmentation and Spatial-Statistics Approach for Post-Blast Rock Fragmentation Assessment](https://arxiv.org/abs/2507.20126)
*Yukun Yang*

Main category: cs.CV

TL;DR: 本文介绍端到端管道，用微调YOLO12l - seg模型实现实时实例分割，提取多指标空间描述符，通过实验验证框架准确性等。


<details>
  <summary>Details</summary>
Motivation: 实现快速、自动化的爆炸效果评估。

Method: 采用在500多张注释后爆炸图像上训练的微调YOLO12l - seg模型进行实时实例分割，将高保真掩码转换为归一化3D坐标并提取多指标空间描述符。

Result: 实现实时实例分割（Box mAP@0.5 ~ 0.769，Mask mAP@0.5 ~ 0.800，约15 FPS），通过四个例子展示关键碎片模式，实验验证框架准确性、对小目标拥挤的鲁棒性。

Conclusion: 该框架准确、对小目标拥挤有鲁棒性，可用于野外条件下快速、自动化的爆炸效果评估。

Abstract: We introduce an end-to-end pipeline that leverages a fine-tuned YOLO12l-seg
model -- trained on over 500 annotated post-blast images -- to deliver
real-time instance segmentation (Box mAP@0.5 ~ 0.769, Mask mAP@0.5 ~ 0.800 at ~
15 FPS). High-fidelity masks are converted into normalized 3D coordinates, from
which we extract multi-metric spatial descriptors: principal component
directions, kernel density hotspots, size-depth regression, and Delaunay edge
statistics. We present four representative examples to illustrate key
fragmentation patterns. Experimental results confirm the framework's accuracy,
robustness to small-object crowding, and feasibility for rapid, automated
blast-effect assessment in field conditions.

</details>


### [296] [Transfer or Self-Supervised? Bridging the Performance Gap in Medical Imaging](https://arxiv.org/abs/2407.05592)
*Zehui Zhao,Laith Alzubaidi,Jinglan Zhang,Ye Duan,Usman Naseem,Yuantong Gu*

Main category: cs.CV

TL;DR: 本文对比医学领域迁移学习和自监督学习的性能与鲁棒性，通过实验测试常见问题对预训练模型的影响，并给出应用建议。


<details>
  <summary>Details</summary>
Motivation: 迁移学习和自监督学习在医学领域有潜力，但不同架构有不同优缺点，需对比其性能和鲁棒性。

Method: 用相同源域数据集不同预训练方法预训练两个模型，在小型医学数据集上评估，通过对比实验测试数据常见问题对模型的影响。

Result: 未提及具体结果。

Conclusion: 给出帮助用户在医学领域应用迁移学习和自监督学习方法的建议，构建更便捷高效的部署策略。

Abstract: Recently, transfer learning and self-supervised learning have gained
significant attention within the medical field due to their ability to mitigate
the challenges posed by limited data availability, improve model
generalisation, and reduce computational expenses. Transfer learning and
self-supervised learning hold immense potential for advancing medical research.
However, it is crucial to recognise that transfer learning and self-supervised
learning architectures exhibit distinct advantages and limitations, manifesting
variations in accuracy, training speed, and robustness. This paper compares the
performance and robustness of transfer learning and self-supervised learning in
the medical field. Specifically, we pre-trained two models using the same
source domain datasets with different pre-training methods and evaluated them
on small-sized medical datasets to identify the factors influencing their final
performance. We tested data with several common issues in medical domains, such
as data imbalance, data scarcity, and domain mismatch, through comparison
experiments to understand their impact on specific pre-trained models. Finally,
we provide recommendations to help users apply transfer learning and
self-supervised learning methods in medical areas, and build more convenient
and efficient deployment strategies.

</details>


### [297] [Efficient Learning for Product Attributes with Compact Multimodal Models](https://arxiv.org/abs/2507.19679)
*Mandar Kulkarni*

Main category: cs.CV

TL;DR: 研究紧凑VLMs基于DPO的标签高效半监督微调策略，在电商图像属性预测任务中有效利用无标签数据提升性能。


<details>
  <summary>Details</summary>
Motivation: 监督式微调VLMs因标注成本高面临规模挑战，需标签高效的半监督微调策略。

Method: 从少量标注集开始，用PEFT训练低秩适配器模块，基于自一致性对无标签样本生成的推理和答案链分类，用DPO损失微调模型迭代更新。

Result: 在十二种电商领域数据集上，仅用无标签数据的DPO微调比监督模型显著提升性能，且更多无标签数据可提高准确率。

Conclusion: 基于PEFT和DPO的微调方法能以最小计算开销实现高效收敛，可有效利用大量无标签样本提升性能。

Abstract: Image-based product attribute prediction in e-commerce is a crucial task with
numerous applications. The supervised fine-tuning of Vision Language Models
(VLMs) faces significant scale challenges due to the cost of manual or API
based annotation. In this paper, we investigate label-efficient semi-supervised
fine-tuning strategies for compact VLMs (2B-3B parameters) that leverage
unlabeled product listings through Direct Preference Optimization (DPO).
Beginning with a small, API-based, annotated, and labeled set, we first employ
PEFT to train low-rank adapter modules. To update the adapter weights with
unlabeled data, we generate multiple reasoning-and-answer chains per unlabeled
sample and segregate these chains into preferred and dispreferred based on
self-consistency. We then fine-tune the model with DPO loss and use the updated
model for the next iteration. By using PEFT fine-tuning with DPO, our method
achieves efficient convergence with minimal compute overhead. On a dataset
spanning twelve e-commerce verticals, DPO-based fine-tuning, which utilizes
only unlabeled data, demonstrates a significant improvement over the supervised
model. Moreover, experiments demonstrate that accuracy with DPO training
improves with more unlabeled data, indicating that a large pool of unlabeled
samples can be effectively leveraged to improve performance.

</details>


### [298] [DeepJIVE: Learning Joint and Individual Variation Explained from Multimodal Data Using Deep Learning](https://arxiv.org/abs/2507.19682)
*Matthew Drexler,Benjamin Risk,James J Lah,Suprateek Kundu,Deqiang Qiu*

Main category: cs.CV

TL;DR: 提出深度学习方法DeepJIVE进行多模态数据联合和个体方差解释，经实验验证有效，可用于多模态数据分析。


<details>
  <summary>Details</summary>
Motivation: 传统多模态数据集成方法存在处理高维数据和识别非线性结构的局限。

Method: 引入DeepJIVE方法，进行数学推导和用合成及真实世界1D、2D、3D数据集实验验证，探索实现约束的策略得到三种损失函数。

Result: DeepJIVE能成功揭示多模态数据集的联合和个体变化，应用于ADNI识别出生物学上合理的协变模式。

Conclusion: DeepJIVE是多模态数据分析的有用工具。

Abstract: Conventional multimodal data integration methods provide a comprehensive
assessment of the shared or unique structure within each individual data type
but suffer from several limitations such as the inability to handle
high-dimensional data and identify nonlinear structures. In this paper, we
introduce DeepJIVE, a deep-learning approach to performing Joint and Individual
Variance Explained (JIVE). We perform mathematical derivation and experimental
validations using both synthetic and real-world 1D, 2D, and 3D datasets.
Different strategies of achieving the identity and orthogonality constraints
for DeepJIVE were explored, resulting in three viable loss functions. We found
that DeepJIVE can successfully uncover joint and individual variations of
multimodal datasets. Our application of DeepJIVE to the Alzheimer's Disease
Neuroimaging Initiative (ADNI) also identified biologically plausible
covariation patterns between the amyloid positron emission tomography (PET) and
magnetic resonance (MR) images. In conclusion, the proposed DeepJIVE can be a
useful tool for multimodal data analysis.

</details>


### [299] [Quaternion-Based Robust PCA for Efficient Moving Target Detection and Background Recovery in Color Videos](https://arxiv.org/abs/2507.19730)
*Liyang Wang,Shiqian Wu,Shun Fang,Qile Zhu,Jiaxin Wu,Sos Again*

Main category: cs.CV

TL;DR: 本文利用四元数黎曼流形降低QSVD计算复杂度，提出uQRPCA和uQRPCA+框架用于彩色视频动目标检测与背景恢复，实验达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 动目标检测任务需准确分割图，合成数据可增强模型泛化能力，现有四元数RPAC在彩色视频处理有计算成本高和通道秩问题。

Method: 利用四元数黎曼流形降低QSVD计算复杂度；提出uQRPCA框架平衡目标分割与背景恢复；引入CR1B方法扩展到uQRPCA+。

Result: uQRPCA+在动目标检测和背景恢复任务上达到SOTA性能。

Conclusion: 提出的方法在动目标检测和背景恢复任务中有效，代码已公开。

Abstract: Moving target detection is a challenging computer vision task aimed at
generating accurate segmentation maps in diverse in-the-wild color videos
captured by static cameras. If backgrounds and targets can be simultaneously
extracted and recombined, such synthetic data can significantly enrich
annotated in-the-wild datasets and enhance the generalization ability of deep
models. Quaternion-based RPCA (QRPCA) is a promising unsupervised paradigm for
color image processing. However, in color video processing, Quaternion Singular
Value Decomposition (QSVD) incurs high computational costs, and rank-1
quaternion matrix fails to yield rank-1 color channels. In this paper, we
reduce the computational complexity of QSVD to o(1) by utilizing a quaternion
Riemannian manifold. Furthermor, we propose the universal QRPCA (uQRPCA)
framework, which achieves a balance in simultaneously segmenting targets and
recovering backgrounds from color videos. Moreover, we expand to uQRPCA+ by
introducing the Color Rank-1 Batch (CR1B) method to further process and obtain
the ideal low-rank background across color channels. Experiments demonstrate
our uQRPCA+ achieves State Of The Art (SOTA) performance on moving target
detection and background recovery tasks compared to existing open-source
methods. Our implementation is publicly available on GitHub at
https://github.com/Ruchtech/uQRPCA

</details>


### [300] [AutoSign: Direct Pose-to-Text Translation for Continuous Sign Language Recognition](https://arxiv.org/abs/2507.19840)
*Samuel Ebimobowei Johnny,Blessed Guda,Andrew Blayama Stephen,Assane Gueye*

Main category: cs.CV

TL;DR: 提出AutoSign模型直接将姿势序列转换为自然语言文本，消除多阶段管道，在Isharah - 1000数据集上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有连续手语识别（CSLR）方法存在误差传播、过拟合和词汇可扩展性问题，需改进。

Method: 提出AutoSign，即自回归仅解码器的transformer，结合1D CNN的时间压缩模块和预训练的阿拉伯语解码器AraGPT2。

Result: 通过消融研究表明手和身体姿势对独立于签名者的CSLR最具判别性，在Isharah - 1000数据集上WER分数最多提升6.1%。

Conclusion: AutoSign消除多阶段管道，有效解决现有CSLR方法的局限，取得更好效果。

Abstract: Continuously recognizing sign gestures and converting them to glosses plays a
key role in bridging the gap between the hearing and hearing-impaired
communities. This involves recognizing and interpreting the hands, face, and
body gestures of the signer, which pose a challenge as it involves a
combination of all these features. Continuous Sign Language Recognition (CSLR)
methods rely on multi-stage pipelines that first extract visual features, then
align variable-length sequences with target glosses using CTC or HMM-based
approaches. However, these alignment-based methods suffer from error
propagation across stages, overfitting, and struggle with vocabulary
scalability due to the intermediate gloss representation bottleneck. To address
these limitations, we propose AutoSign, an autoregressive decoder-only
transformer that directly translates pose sequences to natural language text,
bypassing traditional alignment mechanisms entirely. The use of this
decoder-only approach allows the model to directly map between the features and
the glosses without the need for CTC loss while also directly learning the
textual dependencies in the glosses. Our approach incorporates a temporal
compression module using 1D CNNs to efficiently process pose sequences,
followed by AraGPT2, a pre-trained Arabic decoder, to generate text (glosses).
Through comprehensive ablation studies, we demonstrate that hand and body
gestures provide the most discriminative features for signer-independent CSLR.
By eliminating the multi-stage pipeline, AutoSign achieves substantial
improvements on the Isharah-1000 dataset, achieving an improvement of up to
6.1\% in WER score compared to the best existing method.

</details>


### [301] [RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues for 3D Object Detection](https://arxiv.org/abs/2507.19856)
*Xiaokai Bai,Chenxu Zhou,Lianqing Zheng,Si-Yuan Cao,Jianan Liu,Xiaohan Zhang,Zhengzhuang Zhang,Hui-liang Shen*

Main category: cs.CV

TL;DR: 提出RaGS框架融合4D雷达和单目图像进行3D目标检测，在多个基准测试中表现出色，代码将开源。


<details>
  <summary>Details</summary>
Motivation: 现有4D雷达和单目图像融合进行3D目标检测的方法存在缺乏整体场景理解或受限于刚性网格结构的问题。

Method: 提出RaGS框架，利用3D高斯 splatting 表示，采用级联管道，包括Frustum-based Localization Initiation、Iterative Multimodal Aggregation和Multi-level Gaussian Fusion。

Result: 在View-of-Delft、TJ4DRadSet和OmniHD - Scenes基准测试中取得了最先进的性能。

Conclusion: RaGS框架通过动态聚焦场景中的稀疏对象，实现了目标集中和全面的场景感知。

Abstract: 4D millimeter-wave radar has emerged as a promising sensor for autonomous
driving, but effective 3D object detection from both 4D radar and monocular
images remains a challenge. Existing fusion approaches typically rely on either
instance-based proposals or dense BEV grids, which either lack holistic scene
understanding or are limited by rigid grid structures. To address these, we
propose RaGS, the first framework to leverage 3D Gaussian Splatting (GS) as
representation for fusing 4D radar and monocular cues in 3D object detection.
3D GS naturally suits 3D object detection by modeling the scene as a field of
Gaussians, dynamically allocating resources on foreground objects and providing
a flexible, resource-efficient solution. RaGS uses a cascaded pipeline to
construct and refine the Gaussian field. It starts with the Frustum-based
Localization Initiation (FLI), which unprojects foreground pixels to initialize
coarse 3D Gaussians positions. Then, the Iterative Multimodal Aggregation (IMA)
fuses semantics and geometry, refining the limited Gaussians to the regions of
interest. Finally, the Multi-level Gaussian Fusion (MGF) renders the Gaussians
into multi-level BEV features for 3D object detection. By dynamically focusing
on sparse objects within scenes, RaGS enable object concentrating while
offering comprehensive scene perception. Extensive experiments on
View-of-Delft, TJ4DRadSet, and OmniHD-Scenes benchmarks demonstrate its
state-of-the-art performance. Code will be released.

</details>


### [302] [FedS2R: One-Shot Federated Domain Generalization for Synthetic-to-Real Semantic Segmentation in Autonomous Driving](https://arxiv.org/abs/2507.19881)
*Tao Lian,Jose L. Gómez,Antonio M. López*

Main category: cs.CV

TL;DR: 提出FedS2R用于自动驾驶合成到真实语义分割，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦域泛化在自动驾驶语义分割的潜力未被充分探索。

Method: 提出FedS2R框架，包含不一致驱动的数据增强策略和多客户端知识蒸馏与特征融合方案。

Result: 在五个真实世界数据集上，全局模型显著优于单个客户端模型，仅比同时访问所有客户端数据训练的模型低2 mIoU点。

Conclusion: FedS2R在联邦学习下的自动驾驶合成到真实语义分割中有效。

Abstract: Federated domain generalization has shown promising progress in image
classification by enabling collaborative training across multiple clients
without sharing raw data. However, its potential in the semantic segmentation
of autonomous driving remains underexplored. In this paper, we propose FedS2R,
the first one-shot federated domain generalization framework for
synthetic-to-real semantic segmentation in autonomous driving. FedS2R comprises
two components: an inconsistency-driven data augmentation strategy that
generates images for unstable classes, and a multi-client knowledge
distillation scheme with feature fusion that distills a global model from
multiple client models. Experiments on five real-world datasets, Cityscapes,
BDD100K, Mapillary, IDD, and ACDC, show that the global model significantly
outperforms individual client models and is only 2 mIoU points behind the model
trained with simultaneous access to all client data. These results demonstrate
the effectiveness of FedS2R in synthetic-to-real semantic segmentation for
autonomous driving under federated learning

</details>


### [303] [Interpretable Open-Vocabulary Referring Object Detection with Reverse Contrast Attention](https://arxiv.org/abs/2507.19891)
*Drandreb Earl O. Juanico,Rowel O. Atienza,Jeffrey Kenneth Go*

Main category: cs.CV

TL;DR: 提出反向对比注意力（RCA）方法，在不重新训练的情况下增强视觉语言变压器的对象定位，在15个开源VLM中有11个提升了FitAP，兼具可解释性和性能提升。


<details>
  <summary>Details</summary>
Motivation: 在不重新训练的情况下增强视觉语言变压器的对象定位。

Method: 提出RCA方法，通过抑制极端值和放大中级激活来重新加权最终层注意力；引入基于IoU和框面积的无置信度平均精度指标FitAP。

Result: RCA在15个开源VLM中有11个提高了FitAP，最高增益达+26.6%，有效性与注意力锐度和融合时间有关。

Conclusion: RCA为多模态变压器提供了可解释性和性能增益。

Abstract: We propose Reverse Contrast Attention (RCA), a plug-in method that enhances
object localization in vision-language transformers without retraining. RCA
reweights final-layer attention by suppressing extremes and amplifying
mid-level activations to let semantically relevant but subdued tokens guide
predictions. We evaluate it on Open Vocabulary Referring Object Detection
(OV-RefOD), introducing FitAP, a confidence-free average precision metric based
on IoU and box area. RCA improves FitAP in 11 out of 15 open-source VLMs, with
gains up to $+26.6\%$. Effectiveness aligns with attention sharpness and fusion
timing; while late-fusion models benefit consistently, models like
$\texttt{DeepSeek-VL2}$ also improve, pointing to capacity and disentanglement
as key factors. RCA offers both interpretability and performance gains for
multimodal transformers.

</details>


### [304] [A mini-batch training strategy for deep subspace clustering networks](https://arxiv.org/abs/2507.19917)
*Yuxuan Jiang,Chenwei Yu,Zhi Lin,Xiaolan Liu*

Main category: cs.CV

TL;DR: 提出用于深度子空间聚类的小批量训练策略和无解码器框架，实验表明性能佳。


<details>
  <summary>Details</summary>
Motivation: 现有深度子空间聚类方法依赖全批量处理，有计算瓶颈，需可扩展训练方法并高效微调预训练编码器。

Method: 引入保存全局特征表示的内存库实现小批量训练策略，提出利用对比学习的无解码器框架。

Result: 在COIL100和ORL数据集上表现优于其他先进子空间聚类方法，性能与全批量方法相当。

Conclusion: 所提方法克服之前局限，能实现可扩展训练，有竞争力。

Abstract: Mini-batch training is a cornerstone of modern deep learning, offering
computational efficiency and scalability for training complex architectures.
However, existing deep subspace clustering (DSC) methods, which typically
combine an autoencoder with a self-expressive layer, rely on full-batch
processing. The bottleneck arises from the self-expressive module, which
requires representations of the entire dataset to construct a
self-representation coefficient matrix. In this work, we introduce a mini-batch
training strategy for DSC by integrating a memory bank that preserves global
feature representations. Our approach enables scalable training of deep
architectures for subspace clustering with high-resolution images, overcoming
previous limitations. Additionally, to efficiently fine-tune large-scale
pre-trained encoders for subspace clustering, we propose a decoder-free
framework that leverages contrastive learning instead of autoencoding for
representation learning. This design not only eliminates the computational
overhead of decoder training but also provides competitive performance.
Extensive experiments demonstrate that our approach not only achieves
performance comparable to full-batch methods, but outperforms other
state-of-the-art subspace clustering methods on the COIL100 and ORL datasets by
fine-tuning deep networks.

</details>


### [305] [RARE: Refine Any Registration of Pairwise Point Clouds via Zero-Shot Learning](https://arxiv.org/abs/2507.19950)
*Chengyu Zheng,Jin Huang,Honghua Chen,Mingqiang Wei*

Main category: cs.CV

TL;DR: 提出零样本点云配准算法改进方法，结合深度扩散特征与几何特征提升配准精度，有良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 受扩散技术进展启发，改进点云配准算法，避免使用专门训练数据集。

Method: 将点云投影成深度图，从预训练扩散网络提取深度扩散特征，与几何特征结合建立点云对应关系。

Result: 显著提高点云配准精度，提升现有技术性能，在不同数据集上有良好泛化能力。

Conclusion: 提出的零样本方法有效，能改进点云配准算法。

Abstract: Recent research leveraging large-scale pretrained diffusion models has
demonstrated the potential of using diffusion features to establish semantic
correspondences in images. Inspired by advancements in diffusion-based
techniques, we propose a novel zero-shot method for refining point cloud
registration algorithms. Our approach leverages correspondences derived from
depth images to enhance point feature representations, eliminating the need for
a dedicated training dataset. Specifically, we first project the point cloud
into depth maps from multiple perspectives and extract implicit knowledge from
a pretrained diffusion network as depth diffusion features. These features are
then integrated with geometric features obtained from existing methods to
establish more accurate correspondences between point clouds. By leveraging
these refined correspondences, our approach achieves significantly improved
registration accuracy. Extensive experiments demonstrate that our method not
only enhances the performance of existing point cloud registration techniques
but also exhibits robust generalization capabilities across diverse datasets.
Codes are available at https://github.com/zhengcy-lambo/RARE.git.

</details>


### [306] [Predicting Brain Responses To Natural Movies With Multimodal LLMs](https://arxiv.org/abs/2507.19956)
*Cesar Kadir Torrico Villanueva,Jiaxin Cindy Tu,Mihir Tripathy,Connor Lane,Rishab Iyer,Paul S. Scotti*

Main category: cs.CV

TL;DR: 介绍MedARC团队在Algonauts 2025挑战赛的解决方案，利用多模态预训练模型特征，经处理和模型选择集成，取得第四名，还讨论了可升至第二的优化方法。


<details>
  <summary>Details</summary>
Motivation: 参与Algonauts 2025挑战赛，提升编码模型对新电影刺激的泛化能力。

Method: 利用多模态预训练模型提取特征，线性投影到潜空间，与fMRI时间序列对齐，通过轻量级编码器映射到皮质分区，训练多模型变体并集成。

Result: 最终提交在测试集上平均皮尔逊相关系数为0.2085，团队获比赛第四名，讨论的优化可升至第二。

Conclusion: 结合不同模态模型特征、采用简单架构及全面模型选择集成可提升编码模型泛化能力。

Abstract: We present MedARC's team solution to the Algonauts 2025 challenge. Our
pipeline leveraged rich multimodal representations from various
state-of-the-art pretrained models across video (V-JEPA2), speech (Whisper),
text (Llama 3.2), vision-text (InternVL3), and vision-text-audio
(Qwen2.5-Omni). These features extracted from the models were linearly
projected to a latent space, temporally aligned to the fMRI time series, and
finally mapped to cortical parcels through a lightweight encoder comprising a
shared group head plus subject-specific residual heads. We trained hundreds of
model variants across hyperparameter settings, validated them on held-out
movies and assembled ensembles targeted to each parcel in each subject. Our
final submission achieved a mean Pearson's correlation of 0.2085 on the test
split of withheld out-of-distribution movies, placing our team in fourth place
for the competition. We further discuss a last-minute optimization that would
have raised us to second place. Our results highlight how combining features
from models trained in different modalities, using a simple architecture
consisting of shared-subject and single-subject components, and conducting
comprehensive model selection and ensembling improves generalization of
encoding models to novel movie stimuli. All code is available on GitHub.

</details>


### [307] [Pic2Diagnosis: A Method for Diagnosis of Cardiovascular Diseases from the Printed ECG Pictures](https://arxiv.org/abs/2507.19961)
*Oğuzhan Büyüksolak,İlkay Öksüz*

Main category: cs.CV

TL;DR: 提出直接从心电图图像诊断心血管疾病的方法，用两步课程学习框架和模型集成，在数据集上表现好，适用于资源有限场景。


<details>
  <summary>Details</summary>
Motivation: 现有疾病模式源于过时数据集和传统算法，诊断精度有限。

Method: 采用两步课程学习框架，先在分割掩码上预训练分类模型，再在灰度、反转的心电图图像上微调；用三个模型集成，平均输出结果。

Result: 在BHF ECG Challenge数据集上AUC达0.9534，F1分数达0.7801，优于单个模型。

Conclusion: 该方法能处理现实伪影，简化诊断流程，为自动化心血管疾病诊断提供可靠方案，尤其适用于资源有限场景，可实现快速准确诊断。

Abstract: The electrocardiogram (ECG) is a vital tool for diagnosing heart diseases.
However, many disease patterns are derived from outdated datasets and
traditional stepwise algorithms with limited accuracy. This study presents a
method for direct cardiovascular disease (CVD) diagnosis from ECG images,
eliminating the need for digitization. The proposed approach utilizes a
two-step curriculum learning framework, beginning with the pre-training of a
classification model on segmentation masks, followed by fine-tuning on
grayscale, inverted ECG images. Robustness is further enhanced through an
ensemble of three models with averaged outputs, achieving an AUC of 0.9534 and
an F1 score of 0.7801 on the BHF ECG Challenge dataset, outperforming
individual models. By effectively handling real-world artifacts and simplifying
the diagnostic process, this method offers a reliable solution for automated
CVD diagnosis, particularly in resource-limited settings where printed or
scanned ECG images are commonly used. Such an automated procedure enables rapid
and accurate diagnosis, which is critical for timely intervention in CVD cases
that often demand urgent care.

</details>


### [308] [TAPS : Frustratingly Simple Test Time Active Learning for VLMs](https://arxiv.org/abs/2507.20028)
*Dhruv Sarkar,Aprameyo Chakrabartty,Bibhudatta Bhanja*

Main category: cs.CV

TL;DR: 提出测试时间主动学习（TTAL）框架，用于实时流式场景，在多数据集上实验显示优于现有方法，适用于安全关键应用。


<details>
  <summary>Details</summary>
Motivation: 解决在连续数据流中，每次仅一个样本且有延迟和内存约束下有效利用oracle的更通用实际挑战。

Method: 提出TTAL框架，包括动态调整熵阈值用于主动查询、类平衡替换策略提高内存效率、类感知分布对齐技术增强适应，并进行理论分析。

Result: 在10个跨数据集迁移基准和4个领域泛化数据集上实验，相比现有方法有一致提升，且保持合理延迟和内存开销。

Conclusion: 框架为安全关键应用的实际部署提供了实用有效的解决方案。

Abstract: Test-Time Optimization enables models to adapt to new data during inference
by updating parameters on-the-fly. Recent advances in Vision-Language Models
(VLMs) have explored learning prompts at test time to improve performance in
downstream tasks. In this work, we extend this idea by addressing a more
general and practical challenge: Can we effectively utilize an oracle in a
continuous data stream where only one sample is available at a time, requiring
an immediate query decision while respecting latency and memory constraints? To
tackle this, we propose a novel Test-Time Active Learning (TTAL) framework that
adaptively queries uncertain samples and updates prompts dynamically. Unlike
prior methods that assume batched data or multiple gradient updates, our
approach operates in a real-time streaming scenario with a single test sample
per step. We introduce a dynamically adjusted entropy threshold for active
querying, a class-balanced replacement strategy for memory efficiency, and a
class-aware distribution alignment technique to enhance adaptation. The design
choices are justified using careful theoretical analysis. Extensive experiments
across 10 cross-dataset transfer benchmarks and 4 domain generalization
datasets demonstrate consistent improvements over state-of-the-art methods
while maintaining reasonable latency and memory overhead. Our framework
provides a practical and effective solution for real-world deployment in
safety-critical applications such as autonomous systems and medical
diagnostics.

</details>


### [309] [FaRMamba: Frequency-based learning and Reconstruction aided Mamba for Medical Segmentation](https://arxiv.org/abs/2507.20056)
*Ze Rong,ZiYue Zhao,Zhaoxin Wang,Lei Ma*

Main category: cs.CV

TL;DR: 提出FaRMamba解决Vision Mamba在医学图像分割中的问题，实验显示其性能优越，提供频率感知框架。


<details>
  <summary>Details</summary>
Motivation: Vision Mamba存在LHICD和2D - SSD问题，导致医学图像分割中LBA和LHD加剧，需解决这些问题以实现准确分割。

Method: 提出FaRMamba，包含MSFM模块通过多种变换恢复高频信息，SSRAE模块对共享编码器进行像素级重建恢复二维空间相关性。

Result: 在多个数据集上，FaRMamba优于CNN - Transformer混合模型和现有Mamba变体，有更好边界精度、细节保留和全局一致性，且计算开销合理。

Conclusion: 该工作为未来分割模型提供灵活频率感知框架，直接缓解医学成像核心挑战。

Abstract: Accurate medical image segmentation remains challenging due to blurred lesion
boundaries (LBA), loss of high-frequency details (LHD), and difficulty in
modeling long-range anatomical structures (DC-LRSS). Vision Mamba employs
one-dimensional causal state-space recurrence to efficiently model global
dependencies, thereby substantially mitigating DC-LRSS. However, its patch
tokenization and 1D serialization disrupt local pixel adjacency and impose a
low-pass filtering effect, resulting in Local High-frequency Information
Capture Deficiency (LHICD) and two-dimensional Spatial Structure Degradation
(2D-SSD), which in turn exacerbate LBA and LHD. In this work, we propose
FaRMamba, a novel extension that explicitly addresses LHICD and 2D-SSD through
two complementary modules. A Multi-Scale Frequency Transform Module (MSFM)
restores attenuated high-frequency cues by isolating and reconstructing
multi-band spectra via wavelet, cosine, and Fourier transforms. A
Self-Supervised Reconstruction Auxiliary Encoder (SSRAE) enforces pixel-level
reconstruction on the shared Mamba encoder to recover full 2D spatial
correlations, enhancing both fine textures and global context. Extensive
evaluations on CAMUS echocardiography, MRI-based Mouse-cochlea, and Kvasir-Seg
endoscopy demonstrate that FaRMamba consistently outperforms competitive
CNN-Transformer hybrids and existing Mamba variants, delivering superior
boundary accuracy, detail preservation, and global coherence without
prohibitive computational overhead. This work provides a flexible
frequency-aware framework for future segmentation models that directly
mitigates core challenges in medical imaging.

</details>


### [310] [Local Prompt Adaptation for Style-Consistent Multi-Object Generation in Diffusion Models](https://arxiv.org/abs/2507.20094)
*Ankit Sanjyal*

Main category: cs.CV

TL;DR: 提出无训练的局部提示适应（LPA）方法，分解提示词并选择性注入U - Net注意力层，在自定义基准测试中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在处理含多对象和风格指定的复杂提示时，生成场景缺乏风格一致性和空间连贯性，限制了其在创意和可控内容生成中的应用。

Method: 提出LPA方法，将提示分解为内容和风格标记，并在U - Net不同阶段的注意力层选择性注入。

Result: 在自定义的50个丰富风格提示基准测试中，LPA在CLIP分数和风格一致性指标上优于Composer、MultiDiffusion等基线模型。

Conclusion: LPA为基于扩散的可控、富有表现力的生成提供了新方向。

Abstract: Diffusion models have become a powerful backbone for text-to-image
generation, enabling users to synthesize high-quality visuals from natural
language prompts. However, they often struggle with complex prompts involving
multiple objects and global or local style specifications. In such cases, the
generated scenes tend to lack style uniformity and spatial coherence, limiting
their utility in creative and controllable content generation. In this paper,
we propose a simple, training-free architectural method called Local Prompt
Adaptation (LPA). Our method decomposes the prompt into content and style
tokens, and injects them selectively into the U-Net's attention layers at
different stages. By conditioning object tokens early and style tokens later in
the generation process, LPA enhances both layout control and stylistic
consistency. We evaluate our method on a custom benchmark of 50 style-rich
prompts across five categories and compare against strong baselines including
Composer, MultiDiffusion, Attend-and-Excite, LoRA, and SDXL. Our approach
outperforms prior work on both CLIP score and style consistency metrics,
offering a new direction for controllable, expressive diffusion-based
generation.

</details>


### [311] [Is Exchangeability better than I.I.D to handle Data Distribution Shifts while Pooling Data for Data-scarce Medical image segmentation?](https://arxiv.org/abs/2507.19575)
*Ayush Roy,Samin Enam,Jun Xia,Vishnu Suresh Lokhande,Won Hwa Kim*

Main category: cs.CV

TL;DR: 本文探讨医学影像数据稀缺问题，提出控制深度网络层间特征差异的方法，在多数据集上实现了先进的分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据稀缺，数据合并和添加虽能提升模型性能，但会导致分布偏移，即“数据添加困境”，需要解决该问题。

Method: 从因果框架中获取灵感，提出控制深度网络各层前景 - 背景特征差异的方法。

Result: 在五个数据集的组织病理学和超声图像上实现了最先进的分割性能，定性结果显示分割图更精细准确。

Conclusion: 所提方法能改善特征表示，有效解决数据添加困境，提升医学图像分割性能。

Abstract: Data scarcity is a major challenge in medical imaging, particularly for deep
learning models. While data pooling (combining datasets from multiple sources)
and data addition (adding more data from a new dataset) have been shown to
enhance model performance, they are not without complications. Specifically,
increasing the size of the training dataset through pooling or addition can
induce distributional shifts, negatively affecting downstream model
performance, a phenomenon known as the "Data Addition Dilemma". While the
traditional i.i.d. assumption may not hold in multi-source contexts, assuming
exchangeability across datasets provides a more practical framework for data
pooling. In this work, we investigate medical image segmentation under these
conditions, drawing insights from causal frameworks to propose a method for
controlling foreground-background feature discrepancies across all layers of
deep networks. This approach improves feature representations, which are
crucial in data-addition scenarios. Our method achieves state-of-the-art
segmentation performance on histopathology and ultrasound images across five
datasets, including a novel ultrasound dataset that we have curated and
contributed. Qualitative results demonstrate more refined and accurate
segmentation maps compared to prominent baselines across three model
architectures. The code will be available on Github.

</details>


### [312] [NeuroVoxel-LM: Language-Aligned 3D Perception via Dynamic Voxelization and Meta-Embedding](https://arxiv.org/abs/2507.20110)
*Shiyu Liu,Lianlei Shan*

Main category: cs.CV

TL;DR: 现有3D语言模型处理点云有问题，提出NeuroVoxel - LM框架，实验显示其有效提升效率和精度


<details>
  <summary>Details</summary>
Motivation: 现有3D语言模型处理稀疏、大规模点云时，特征提取慢且表示精度有限

Method: 提出NeuroVoxel - LM框架，包括DR - MSV技术自适应调整体素粒度，TAP - LME机制通过注意力加权和残差融合增强语义表示

Result: DR - MSV提高点云特征提取效率和精度，TAP - LME在捕捉细粒度语义上优于传统最大池化

Conclusion: NeuroVoxel - LM框架能有效解决现有3D语言模型处理点云的问题

Abstract: Recent breakthroughs in Visual Language Models (VLMs) and Multimodal Large
Language Models (MLLMs) have significantly advanced 3D scene perception towards
language-driven cognition. However, existing 3D language models struggle with
sparse, large-scale point clouds due to slow feature extraction and limited
representation accuracy. To address these challenges, we propose NeuroVoxel-LM,
a novel framework that integrates Neural Radiance Fields (NeRF) with dynamic
resolution voxelization and lightweight meta-embedding. Specifically, we
introduce a Dynamic Resolution Multiscale Voxelization (DR-MSV) technique that
adaptively adjusts voxel granularity based on geometric and structural
complexity, reducing computational cost while preserving reconstruction
fidelity. In addition, we propose the Token-level Adaptive Pooling for
Lightweight Meta-Embedding (TAP-LME) mechanism, which enhances semantic
representation through attention-based weighting and residual fusion.
Experimental results demonstrate that DR-MSV significantly improves point cloud
feature extraction efficiency and accuracy, while TAP-LME outperforms
conventional max-pooling in capturing fine-grained semantics from NeRF weights.

</details>


### [313] [Trust the Model: Compact VLMs as In-Context Judges for Image-Text Data Quality](https://arxiv.org/abs/2507.20156)
*Daulet Toibazar,Kesen Wang,Sherif Mohamed,Abdulaziz Al-Badawi,Abdulrahman Alfulayt,Pedro J. Moreno*

Main category: cs.CV

TL;DR: 介绍了一种精简的数据过滤框架，用微调的小型VLM过滤训练样本，提高图像文本对齐和语言流畅度，为构建高质量视觉语言训练语料库提供轻量级解决方案。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型引入视觉输入带来数据质量挑战，精心策划的训练样本比单纯增加数据量效果更好，因此需有效过滤数据。

Method: 引入精简数据过滤框架，使用在高质量图像 - 字幕注释数据集上微调的小型VLM，基于字幕和图像质量及对齐评估和过滤潜在训练样本。

Result: 经小型VLM高精度过滤的数据集表现与或超越通过大量网络爬取的更大、更嘈杂的数据集。

Conclusion: 该方法为构建高质量视觉语言训练语料库提供轻量级且强大的解决方案。

Abstract: Vision-language models (VLMs) extend the conventional large language models
by integrating visual data, enabling richer multimodal reasoning and
significantly broadens the practical applications of AI. However, including
visual inputs also brings new challenges in maintaining data quality. Empirical
evidence consistently shows that carefully curated and representative training
examples often yield superior results compared to simply increasing the
quantity of data. Inspired by this observation, we introduce a streamlined data
filtration framework that employs a compact VLM, fine-tuned on a high-quality
image-caption annotated dataset. This model effectively evaluates and filters
potential training samples based on caption and image quality and alignment.
Unlike previous approaches, which typically add auxiliary filtration modules on
top of existing full-scale VLMs, our method exclusively utilizes the inherent
evaluative capability of a purpose-built small VLM. This strategy eliminates
the need for extra modules and reduces training overhead. Our lightweight model
efficiently filters out inaccurate, noisy web data, improving image-text
alignment and caption linguistic fluency. Experimental results show that
datasets underwent high-precision filtration using our compact VLM perform on
par with, or even surpass, larger and noisier datasets gathered through
high-volume web crawling. Thus, our method provides a lightweight yet robust
solution for building high-quality vision-language training corpora. \\
\textbf{Availability and implementation:} Our compact VLM filtration model,
training data, utility scripts, and Supplementary data (Appendices) are freely
available at https://github.com/daulettoibazar/Compact_VLM_Filter.

</details>


### [314] [LRR-Bench: Left, Right or Rotate? Vision-Language models Still Struggle With Spatial Understanding Tasks](https://arxiv.org/abs/2507.20174)
*Fei Kong,Jinhao Duan,Kaidi Xu,Zhenhua Guo,Xiaofeng Zhu,Xiaoshuang Shi*

Main category: cs.CV

TL;DR: 提出空间评估管道和基准测试VLMs空间理解能力，发现其有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现实应用需精确空间感知，但VLMs对空间关系和运动感知研究不足。

Method: 引入空间评估管道，构建基准，将空间理解分类，使用合成数据集。

Result: 实验表明VLMs空间理解能力有很大提升空间，人类表现近乎完美，VLMs仅在最简单任务达人类水平。

Conclusion: 当前VLMs空间理解能力较差，需进一步改进。

Abstract: Real-world applications, such as autonomous driving and humanoid robot
manipulation, require precise spatial perception. However, it remains
underexplored how Vision-Language Models (VLMs) recognize spatial relationships
and perceive spatial movement. In this work, we introduce a spatial evaluation
pipeline and construct a corresponding benchmark. Specifically, we categorize
spatial understanding into two main types: absolute spatial understanding,
which involves querying the absolute spatial position (e.g., left, right) of an
object within an image, and 3D spatial understanding, which includes movement
and rotation. Notably, our dataset is entirely synthetic, enabling the
generation of test samples at a low cost while also preventing dataset
contamination. We conduct experiments on multiple state-of-the-art VLMs and
observe that there is significant room for improvement in their spatial
understanding abilities. Explicitly, in our experiments, humans achieve
near-perfect performance on all tasks, whereas current VLMs attain human-level
performance only on the two simplest tasks. For the remaining tasks, the
performance of VLMs is distinctly lower than that of humans. In fact, the
best-performing Vision-Language Models even achieve near-zero scores on
multiple tasks. The dataset and code are available on
https://github.com/kong13661/LRR-Bench.

</details>


### [315] [Color histogram equalization and fine-tuning to improve expression recognition of (partially occluded) faces on sign language datasets](https://arxiv.org/abs/2507.20197)
*Fabrizio Nunnari,Alakshendra Jyotsnaditya Ramkrishna Singh,Patrick Gebhard*

Main category: cs.CV

TL;DR: 研究计算机视觉方法在手语数据集上分类面部表情的能力，引入颜色归一化，结果显示有较高识别率，下半脸识别率高于上半脸且上半脸分类准确率超人类水平。


<details>
  <summary>Details</summary>
Motivation: 量化计算机视觉方法在手语数据集上正确分类面部表情的程度，进一步研究听力正常和聋哑受试者在情感表现上的差异。

Method: 引入基于直方图均衡化和微调的颜色归一化阶段，还进行仅用脸部上半部分或下半部分识别表情的实验。

Result: 能以83.8%的平均敏感度正确识别面部表情，类间方差小（0.042）；下半脸表情识别率（79.6%）高于上半脸（77.9%），上半脸分类准确率高于人类水平。

Conclusion: 计算机视觉方法在手语数据集面部表情分类上有较好表现，且脸部不同部分识别情况有差异。

Abstract: The goal of this investigation is to quantify to what extent computer vision
methods can correctly classify facial expressions on a sign language dataset.
We extend our experiments by recognizing expressions using only the upper or
lower part of the face, which is needed to further investigate the difference
in emotion manifestation between hearing and deaf subjects. To take into
account the peculiar color profile of a dataset, our method introduces a color
normalization stage based on histogram equalization and fine-tuning. The
results show the ability to correctly recognize facial expressions with 83.8%
mean sensitivity and very little variance (.042) among classes. Like for
humans, recognition of expressions from the lower half of the face (79.6%) is
higher than that from the upper half (77.9%). Noticeably, the classification
accuracy from the upper half of the face is higher than human level.

</details>


### [316] [Smaller, Faster, Cheaper: Architectural Designs for Efficient Machine Learning](https://arxiv.org/abs/2507.19795)
*Steven Walton*

Main category: cs.CV

TL;DR: 论文聚焦于设计可提升性能并减少计算需求的架构原则，从数据进出、核心架构修改和利用归一化流结构三方向开展研究，证明精心设计可提升机器学习算法效率。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉模型发展使计算基础设施需求增加，在资源受限环境中，需高性能且低计算资源需求的架构。

Method: 从数据进出、核心神经架构修改（针对视觉变换器受限注意力）、探索归一化流自然结构三方向开展研究。

Result: 精心设计的神经架构可让机器学习算法更小、更快、更便宜。

Conclusion: 精心设计神经网络架构能提升机器学习算法效率。

Abstract: Major advancements in the capabilities of computer vision models have been
primarily fueled by rapid expansion of datasets, model parameters, and
computational budgets, leading to ever-increasing demands on computational
infrastructure. However, as these models are deployed in increasingly diverse
and resource-constrained environments, there is a pressing need for
architectures that can deliver high performance while requiring fewer
computational resources.
  This dissertation focuses on architectural principles through which models
can achieve increased performance while reducing their computational demands.
We discuss strides towards this goal through three directions. First, we focus
on data ingress and egress, investigating how information may be passed into
and retrieved from our core neural processing units. This ensures that our
models make the most of available data, allowing smaller architectures to
become more performant. Second, we investigate modifications to the core neural
architecture, applied to restricted attention in vision transformers. This
section explores how removing uniform context windows in restricted attention
increases the expressivity of the underlying neural architecture. Third, we
explore the natural structures of Normalizing Flows and how we can leverage
these properties to better distill model knowledge.
  These contributions demonstrate that careful design of neural architectures
can increase the efficiency of machine learning algorithms, allowing them to
become smaller, faster, and cheaper.

</details>


### [317] [Solving Scene Understanding for Autonomous Navigation in Unstructured Environments](https://arxiv.org/abs/2507.20389)
*Naveen Mathews Renji,Kruthika K,Manasa Keshavamurthy,Pooja Kumari,S. Rajarajeswari*

Main category: cs.CV

TL;DR: 本文在印度驾驶数据集上进行语义分割，训练五个模型并比较性能，最高MIOU达0.6496。


<details>
  <summary>Details</summary>
Motivation: 理解自动驾驶车辆运行场景对其正常运行至关重要，语义分割是场景理解的关键部分，印度驾驶数据集有挑战性。

Method: 在印度驾驶数据集第一级上进行语义分割，训练UNET、UNET+RESNET50、DeepLabsV3、PSPNet和SegNet五个模型，用Mean Intersection over Union比较性能。

Result: 最高Mean Intersection over Union达到0.6496。

Conclusion: 论文讨论了数据集、探索性数据分析、模型实现，研究并比较了模型性能。

Abstract: Autonomous vehicles are the next revolution in the automobile industry and
they are expected to revolutionize the future of transportation. Understanding
the scenario in which the autonomous vehicle will operate is critical for its
competent functioning. Deep Learning has played a massive role in the progress
that has been made till date. Semantic Segmentation, the process of annotating
every pixel of an image with an object class, is one crucial part of this scene
comprehension using Deep Learning. It is especially useful in Autonomous
Driving Research as it requires comprehension of drivable and non-drivable
areas, roadside objects and the like. In this paper semantic segmentation has
been performed on the Indian Driving Dataset which has been recently compiled
on the urban and rural roads of Bengaluru and Hyderabad. This dataset is more
challenging compared to other datasets like Cityscapes, since it is based on
unstructured driving environments. It has a four level hierarchy and in this
paper segmentation has been performed on the first level. Five different models
have been trained and their performance has been compared using the Mean
Intersection over Union. These are UNET, UNET+RESNET50, DeepLabsV3, PSPNet and
SegNet. The highest MIOU of 0.6496 has been achieved. The paper discusses the
dataset, exploratory data analysis, preparation, implementation of the five
models and studies the performance and compares the results achieved in the
process.

</details>


### [318] [Enhancing Spatial Reasoning through Visual and Textual Thinking](https://arxiv.org/abs/2507.20529)
*Xun Liang,Xin Guo,Zhongming Jin,Weihang Pan,Penghui Shang,Deng Cai,Binbin Lin,Jieping Ye*

Main category: cs.CV

TL;DR: 本文提出SpatialVTS方法提升空间推理能力，经数据处理训练后，模型在空间理解任务上表现优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在空间推理任务上表现不佳，需提升其空间推理能力。

Method: 引入SpatialVTS方法，分空间视觉思考和空间文本思考阶段，同时对现有数据集进行手动修正、重构输入格式和开发推理细节。

Result: 在不引入额外信息情况下，模型在多个空间理解任务上的整体平均水平显著高于其他模型。

Conclusion: SpatialVTS方法能有效提升视觉语言模型的空间推理能力。

Abstract: The spatial reasoning task aims to reason about the spatial relationships in
2D and 3D space, which is a fundamental capability for Visual Question
Answering (VQA) and robotics. Although vision language models (VLMs) have
developed rapidly in recent years, they are still struggling with the spatial
reasoning task. In this paper, we introduce a method that can enhance Spatial
reasoning through Visual and Textual thinking Simultaneously (SpatialVTS). In
the spatial visual thinking phase, our model is trained to generate
location-related specific tokens of essential targets automatically. Not only
are the objects mentioned in the problem addressed, but also the potential
objects related to the reasoning are considered. During the spatial textual
thinking phase, Our model conducts long-term thinking based on visual cues and
dialogues, gradually inferring the answers to spatial reasoning problems. To
effectively support the model's training, we perform manual corrections to the
existing spatial reasoning dataset, eliminating numerous incorrect labels
resulting from automatic annotation, restructuring the data input format to
enhance generalization ability, and developing thinking processes with logical
reasoning details. Without introducing additional information (such as masks or
depth), our model's overall average level in several spatial understanding
tasks has significantly improved compared with other models.

</details>


### [319] [T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation](https://arxiv.org/abs/2507.20536)
*Chieh-Yun Chen,Min Shi,Gong Zhang,Humphrey Shi*

Main category: cs.CV

TL;DR: 介绍无训练的多智能体系统T2I - Copilot，简化文本到图像生成的提示工程，提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型对提示短语敏感，现有技术可控性有限或需额外训练，泛化能力受限。

Method: 引入无训练的多智能体系统T2I - Copilot，由输入解释器、生成引擎和质量评估器三个智能体协作，实现自动提示短语、模型选择和迭代优化。

Result: 在GenAI - Bench上，使用开源模型，T2I - Copilot的VQA分数与商业模型相当，成本低且性能超越部分模型。

Conclusion: T2I - Copilot能有效简化提示工程，提升生成质量和文本 - 图像对齐度，可自主运行也支持人工干预。

Abstract: Text-to-Image (T2I) generative models have revolutionized content creation
but remain highly sensitive to prompt phrasing, often requiring users to
repeatedly refine prompts multiple times without clear feedback. While
techniques such as automatic prompt engineering, controlled text embeddings,
denoising, and multi-turn generation mitigate these issues, they offer limited
controllability, or often necessitate additional training, restricting the
generalization abilities. Thus, we introduce T2I-Copilot, a training-free
multi-agent system that leverages collaboration between (Multimodal) Large
Language Models to automate prompt phrasing, model selection, and iterative
refinement. This approach significantly simplifies prompt engineering while
enhancing generation quality and text-image alignment compared to direct
generation. Specifically, T2I-Copilot consists of three agents: (1) Input
Interpreter, which parses the input prompt, resolves ambiguities, and generates
a standardized report; (2) Generation Engine, which selects the appropriate
model from different types of T2I models and organizes visual and textual
prompts to initiate generation; and (3) Quality Evaluator, which assesses
aesthetic quality and text-image alignment, providing scores and feedback for
potential regeneration. T2I-Copilot can operate fully autonomously while also
supporting human-in-the-loop intervention for fine-grained control. On
GenAI-Bench, using open-source generation models, T2I-Copilot achieves a VQA
score comparable to commercial models RecraftV3 and Imagen 3, surpasses
FLUX1.1-pro by 6.17% at only 16.59% of its cost, and outperforms FLUX.1-dev and
SD 3.5 Large by 9.11% and 6.36%. Code will be released at:
https://github.com/SHI-Labs/T2I-Copilot.

</details>


### [320] [Controllable Feature Whitening for Hyperparameter-Free Bias Mitigation](https://arxiv.org/abs/2507.20284)
*Yooshin Cho,Hanbyel Cho,Janghyeon Lee,HyeongGwon Hong,Jaesung Ahn,Junmo Kim*

Main category: cs.CV

TL;DR: 提出可控特征白化框架提高AI可靠性，在四个基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能使用增加，开发可信AI很重要，但深度神经网络易学习数据集中的虚假关联，需提高可靠性。

Method: 提出可控特征白化框架，用协方差矩阵量化目标和偏差特征的线性相关性，通过白化模块消除。

Result: 去除输入到最后线性分类器的特征间线性相关性可显著减轻偏差，避免对高阶依赖建模；能有效处理两个公平性标准；可通过调整加权系数控制算法效用和公平性的权衡；在四个基准数据集上表现优于现有方法。

Conclusion: 所提方法能有效提高AI可靠性，在处理偏差和公平性上有优势，且性能表现良好。

Abstract: As the use of artificial intelligence rapidly increases, the development of
trustworthy artificial intelligence has become important. However, recent
studies have shown that deep neural networks are susceptible to learn spurious
correlations present in datasets. To improve the reliability, we propose a
simple yet effective framework called controllable feature whitening. We
quantify the linear correlation between the target and bias features by the
covariance matrix, and eliminate it through the whitening module. Our results
systemically demonstrate that removing the linear correlations between features
fed into the last linear classifier significantly mitigates the bias, while
avoiding the need to model intractable higher-order dependencies. A particular
advantage of the proposed method is that it does not require regularization
terms or adversarial learning, which often leads to unstable optimization in
practice. Furthermore, we show that two fairness criteria, demographic parity
and equalized odds, can be effectively handled by whitening with the
re-weighted covariance matrix. Consequently, our method controls the trade-off
between the utility and fairness of algorithms by adjusting the weighting
coefficient. Finally, we validate that our method outperforms existing
approaches on four benchmark datasets: Corrupted CIFAR-10, Biased FFHQ,
WaterBirds, and Celeb-A.

</details>


### [321] [MemoryTalker: Personalized Speech-Driven 3D Facial Animation via Audio-Guided Stylization](https://arxiv.org/abs/2507.20562)
*Hyung Kyu Kim,Sangmin Lee,Hak Gu Kim*

Main category: cs.CV

TL;DR: 提出MemoryTalker，仅用音频输入实现3D面部动画合成，无需额外先验信息，经评估效果良好。


<details>
  <summary>Details</summary>
Motivation: 过往语音驱动3D面部动画工作需额外先验信息，无法反映说话风格且限制实际应用，需解决此问题。

Method: MemoryTalker框架分两阶段训练，第一阶段存储和检索通用动作，第二阶段结合音频驱动风格特征进行个性化面部动作合成。

Result: 能在无额外先验信息下生成可靠的个性化面部动画。

Conclusion: 经定量、定性评估和用户研究，模型有效，相比现有技术提升了个性化面部动画性能。

Abstract: Speech-driven 3D facial animation aims to synthesize realistic facial motion
sequences from given audio, matching the speaker's speaking style. However,
previous works often require priors such as class labels of a speaker or
additional 3D facial meshes at inference, which makes them fail to reflect the
speaking style and limits their practical use. To address these issues, we
propose MemoryTalker which enables realistic and accurate 3D facial motion
synthesis by reflecting speaking style only with audio input to maximize
usability in applications. Our framework consists of two training stages:
1-stage is storing and retrieving general motion (i.e., Memorizing), and
2-stage is to perform the personalized facial motion synthesis (i.e.,
Animating) with the motion memory stylized by the audio-driven speaking style
feature. In this second stage, our model learns about which facial motion types
should be emphasized for a particular piece of audio. As a result, our
MemoryTalker can generate a reliable personalized facial animation without
additional prior information. With quantitative and qualitative evaluations, as
well as user study, we show the effectiveness of our model and its performance
enhancement for personalized facial animation over state-of-the-art methods.

</details>


### [322] [Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven 3D Facial Animation](https://arxiv.org/abs/2507.20568)
*Hyung Kyu Kim,Hak Gu Kim*

Main category: cs.CV

TL;DR: 提出新的语音上下文感知损失以解决传统方法生成面部动画不连续问题，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 传统逐帧方法生成的语音驱动3D面部动画因协同发音无法捕捉面部运动连续性，导致输出抖动不自然。

Method: 提出新的语音上下文感知损失，通过引入视位协同发音权重，根据面部运动随时间的动态变化分配自适应重要性。

Result: 用新损失替换传统重建损失，在定量指标和视觉质量上均有提升。

Conclusion: 明确建模依赖语音上下文的视位在合成自然语音驱动3D面部动画中很重要。

Abstract: Speech-driven 3D facial animation aims to generate realistic facial movements
synchronized with audio. Traditional methods primarily minimize reconstruction
loss by aligning each frame with ground-truth. However, this frame-wise
approach often fails to capture the continuity of facial motion, leading to
jittery and unnatural outputs due to coarticulation. To address this, we
propose a novel phonetic context-aware loss, which explicitly models the
influence of phonetic context on viseme transitions. By incorporating a viseme
coarticulation weight, we assign adaptive importance to facial movements based
on their dynamic changes over time, ensuring smoother and perceptually
consistent animations. Extensive experiments demonstrate that replacing the
conventional reconstruction loss with ours improves both quantitative metrics
and visual quality. It highlights the importance of explicitly modeling
phonetic context-dependent visemes in synthesizing natural speech-driven 3D
facial animation. Project page: https://cau-irislab.github.io/interspeech25/

</details>


### [323] [Lightweight Remote Sensing Scene Classification on Edge Devices via Knowledge Distillation and Early-exit](https://arxiv.org/abs/2507.20623)
*Yang Zhao,Shusheng Li,Xueshang Feng*

Main category: cs.CV

TL;DR: 本文提出轻量级遥感场景分类框架，在边缘设备上评估有加速和节能效果且保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有遥感场景分类模型在资源受限边缘设备上难以兼顾准确率、推理延迟和能耗。

Method: 提出包含蒸馏全局滤波网络模型和早期退出机制的轻量级框架，先对GFNet模型进行频域蒸馏，再设计动态早期退出模型。

Result: 在三个边缘设备和四个数据集上评估，模型推理平均加速1.3倍，能源效率提高超40%，保持高分类准确率。

Conclusion: 所提轻量级RSSC框架在边缘设备上能实现优异性能。

Abstract: As the development of lightweight deep learning algorithms, various deep
neural network (DNN) models have been proposed for the remote sensing scene
classification (RSSC) application. However, it is still challenging for these
RSSC models to achieve optimal performance among model accuracy, inference
latency, and energy consumption on resource-constrained edge devices. In this
paper, we propose a lightweight RSSC framework, which includes a distilled
global filter network (GFNet) model and an early-exit mechanism designed for
edge devices to achieve state-of-the-art performance. Specifically, we first
apply frequency domain distillation on the GFNet model to reduce model size.
Then we design a dynamic early-exit model tailored for DNN models on edge
devices to further improve model inference efficiency. We evaluate our E3C
model on three edge devices across four datasets. Extensive experimental
results show that it achieves an average of 1.3x speedup on model inference and
over 40% improvement on energy efficiency, while maintaining high
classification accuracy.

</details>


### [324] [Frequency-Aware Autoregressive Modeling for Efficient High-Resolution Image Synthesis](https://arxiv.org/abs/2507.20454)
*Zhuokun Chen,Jugang Fan,Zhuowei Yu,Bohan Zhuang,Mingkui Tan*

Main category: cs.CV

TL;DR: 提出SparseVAR加速框架，动态排除低频tokens，在不额外训练下减少计算成本，提升图像生成速度且质量损失小。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归建模在高分辨率阶段因大量tokens导致计算开销大，低频区域tokens对图像质量影响小且相邻相似，不同块关注不同区域。

Method: 引入SparseVAR，用轻量级MSE指标识别低频tokens，用少量均匀采样的锚点tokens保持排除区域的保真度。

Result: SparseVAR在HART和Infinity中显著加速，如在Infinity - 2B中实现2倍加速且质量几乎无下降。

Conclusion: SparseVAR作为即插即用的加速框架，能在保持图像生成质量的同时显著减少计算成本，实现加速。

Abstract: Visual autoregressive modeling, based on the next-scale prediction paradigm,
exhibits notable advantages in image quality and model scalability over
traditional autoregressive and diffusion models. It generates images by
progressively refining resolution across multiple stages. However, the
computational overhead in high-resolution stages remains a critical challenge
due to the substantial number of tokens involved. In this paper, we introduce
SparseVAR, a plug-and-play acceleration framework for next-scale prediction
that dynamically excludes low-frequency tokens during inference without
requiring additional training. Our approach is motivated by the observation
that tokens in low-frequency regions have a negligible impact on image quality
in high-resolution stages and exhibit strong similarity with neighboring
tokens. Additionally, we observe that different blocks in the next-scale
prediction model focus on distinct regions, with some concentrating on
high-frequency areas. SparseVAR leverages these insights by employing
lightweight MSE-based metrics to identify low-frequency tokens while preserving
the fidelity of excluded regions through a small set of uniformly sampled
anchor tokens. By significantly reducing the computational cost while
maintaining high image generation quality, SparseVAR achieves notable
acceleration in both HART and Infinity. Specifically, SparseVAR achieves up to
a 2 times speedup with minimal quality degradation in Infinity-2B.

</details>


### [325] [TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model](https://arxiv.org/abs/2507.20630)
*Ao Li,Yuxiang Duan,Jinghui Zhang,Congbo Ma,Yutong Xie,Gustavo Carneiro,Mohammad Yaqub,Hu Wang*

Main category: cs.CV

TL;DR: 本文提出训练无关且高效的令牌剪枝方法TransPrune，可在不损失性能下大幅减少推理计算量，代码待接收后公开。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型计算成本高，现有令牌剪枝方法有局限性，需新方法提高推理效率。

Method: 提出TransPrune，结合令牌过渡变化（TTV）和指令引导注意力（IGA）评估令牌重要性并逐步剪枝。

Result: 在八个基准测试中，TransPrune与原模型性能相当，推理TFLOPs减少一半以上，TTV单独作为标准也有不错表现。

Conclusion: TransPrune是一种有效的令牌剪枝方法，能提高大视觉语言模型推理效率。

Abstract: Large Vision-Language Models (LVLMs) have advanced multimodal learning but
face high computational costs due to the large number of visual tokens,
motivating token pruning to improve inference efficiency. The key challenge
lies in identifying which tokens are truly important. Most existing approaches
rely on attention-based criteria to estimate token importance. However, they
inherently suffer from certain limitations, such as positional bias. In this
work, we explore a new perspective on token importance based on token
transitions in LVLMs. We observe that the transition of token representations
provides a meaningful signal of semantic information. Based on this insight, we
propose TransPrune, a training-free and efficient token pruning method.
Specifically, TransPrune progressively prunes tokens by assessing their
importance through a combination of Token Transition Variation (TTV)-which
measures changes in both the magnitude and direction of token
representations-and Instruction-Guided Attention (IGA), which measures how
strongly the instruction attends to image tokens via attention. Extensive
experiments demonstrate that TransPrune achieves comparable multimodal
performance to original LVLMs, such as LLaVA-v1.5 and LLaVA-Next, across eight
benchmarks, while reducing inference TFLOPs by more than half. Moreover, TTV
alone can serve as an effective criterion without relying on attention,
achieving performance comparable to attention-based methods. The code will be
made publicly available upon acceptance of the paper at
https://github.com/liaolea/TransPrune.

</details>


### [326] [A Multimodal Architecture for Endpoint Position Prediction in Team-based Multiplayer Games](https://arxiv.org/abs/2507.20670)
*Jonas Peche,Aliaksei Tsishurou,Alexander Zap,Guenter Wallner*

Main category: cs.CV

TL;DR: 本文提出用于预测玩家未来位置的多模态架构，为下游任务奠定基础。


<details>
  <summary>Details</summary>
Motivation: 理解和预测多人游戏中玩家移动对多种应用很关键，但复杂环境和玩家互动需要有效利用异构输入数据的模型。

Method: 使用基于U - Net的方法计算终点位置概率热图，通过多模态特征编码器进行条件处理，应用多头注意力机制实现智能体间通信。

Result: 架构能有效利用多模态游戏状态，包括图像输入、数值和分类特征以及动态游戏数据。

Conclusion: 该技术为依赖玩家未来位置的下游任务，如创建预测玩家行为的机器人或玩家异常检测等奠定基础。

Abstract: Understanding and predicting player movement in multiplayer games is crucial
for achieving use cases such as player-mimicking bot navigation, preemptive bot
control, strategy recommendation, and real-time player behavior analytics.
However, the complex environments allow for a high degree of navigational
freedom, and the interactions and team-play between players require models that
make effective use of the available heterogeneous input data. This paper
presents a multimodal architecture for predicting future player locations on a
dynamic time horizon, using a U-Net-based approach for calculating endpoint
location probability heatmaps, conditioned using a multimodal feature encoder.
The application of a multi-head attention mechanism for different groups of
features allows for communication between agents. In doing so, the architecture
makes efficient use of the multimodal game state including image inputs,
numerical and categorical features, as well as dynamic game data. Consequently,
the presented technique lays the foundation for various downstream tasks that
rely on future player positions such as the creation of player-predictive bot
behavior or player anomaly detection.

</details>


### [327] [Multi-Masked Querying Network for Robust Emotion Recognition from Incomplete Multi-Modal Physiological Signals](https://arxiv.org/abs/2507.20737)
*Geng-Xin Xu,Xiang Zuo,Ye Li*

Main category: cs.CV

TL;DR: 本文提出多掩码查询网络（MMQ - Net）解决生理数据情感识别中多模态信号不完整和干扰问题，实验表明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决生理数据情感识别中多模态信号不完整以及身体运动和伪影干扰的问题。

Method: 提出MMQ - Net，集成多种查询机制，用模态查询重建缺失数据、类别查询关注情感特征、干扰查询分离信息与噪声。

Result: 实验显示MMQ - Net在情感识别上比现有方法性能优越，在数据高度不完整时表现尤佳。

Conclusion: MMQ - Net能有效解决生理数据情感识别面临的问题，提升识别性能。

Abstract: Emotion recognition from physiological data is crucial for mental health
assessment, yet it faces two significant challenges: incomplete multi-modal
signals and interference from body movements and artifacts. This paper presents
a novel Multi-Masked Querying Network (MMQ-Net) to address these issues by
integrating multiple querying mechanisms into a unified framework.
Specifically, it uses modality queries to reconstruct missing data from
incomplete signals, category queries to focus on emotional state features, and
interference queries to separate relevant information from noise. Extensive
experiment results demonstrate the superior emotion recognition performance of
MMQ-Net compared to existing approaches, particularly under high levels of data
incompleteness.

</details>


### [328] [Regularizing Subspace Redundancy of Low-Rank Adaptation](https://arxiv.org/abs/2507.20745)
*Yue Zhu,Haiwen Diao,Shang Gao,Jiazuo Yu,Jiawen Zhu,Yunzhi Zhuge,Shuai Hao,Xu Jia,Lu Zhang,Ying Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出ReSoRA方法解决LoRA及其变体在PETL中投影矩阵冗余问题，实验验证其有效性且可即插即用。


<details>
  <summary>Details</summary>
Motivation: LoRA及其变体训练时投影矩阵无约束，存在高冗余，现有方法缺乏灵活性和泛化性。

Method: 将低秩子矩阵理论分解为多个等价子空间，对不同投影的特征分布应用去冗余约束。

Result: 在视觉 - 语言检索和标准视觉分类基准中提升现有PETL方法性能，可即插即用且无额外推理成本。

Conclusion: ReSoRA方法有效且具有良好适应性。

Abstract: Low-Rank Adaptation (LoRA) and its variants have delivered strong capability
in Parameter-Efficient Transfer Learning (PETL) by minimizing trainable
parameters and benefiting from reparameterization. However, their projection
matrices remain unrestricted during training, causing high representation
redundancy and diminishing the effectiveness of feature adaptation in the
resulting subspaces. While existing methods mitigate this by manually adjusting
the rank or implicitly applying channel-wise masks, they lack flexibility and
generalize poorly across various datasets and architectures. Hence, we propose
ReSoRA, a method that explicitly models redundancy between mapping subspaces
and adaptively Regularizes Subspace redundancy of Low-Rank Adaptation.
Specifically, it theoretically decomposes the low-rank submatrices into
multiple equivalent subspaces and systematically applies de-redundancy
constraints to the feature distributions across different projections.
Extensive experiments validate that our proposed method consistently
facilitates existing state-of-the-art PETL methods across various backbones and
datasets in vision-language retrieval and standard visual classification
benchmarks. Besides, as a training supervision, ReSoRA can be seamlessly
integrated into existing approaches in a plug-and-play manner, with no
additional inference costs. Code is publicly available at:
https://github.com/Lucenova/ReSoRA.

</details>


### [329] [Learning to See Inside Opaque Liquid Containers using Speckle Vibrometry](https://arxiv.org/abs/2507.20757)
*Matan Kichler,Shai Bagon,Mark Sheinin*

Main category: cs.CV

TL;DR: 本文提出通过感知不透明容器表面微小振动推断隐藏液位的方法，利用基于散斑的振动传感系统和基于Transformer的分析方法，实现远程检测且模型有一定泛化性。


<details>
  <summary>Details</summary>
Motivation: 传统计算机视觉系统只能从场景物体可见表面提取信息，本文旨在拓展计算机视觉范围，实现推断不透明容器隐藏液位的新任务。

Method: 提出基于散斑的振动传感系统同时捕捉场景中二维网格点的振动，采集多种日常液体容器的振动响应数据集；开发基于Transformer的方法分析振动，对容器类型和隐藏液位进行分类。

Result: 该架构对振动源具有不变性，能对受控和环境场景声源给出正确液位估计，模型对已知类别中未见的容器实例和液位有泛化能力。

Conclusion: 通过恢复各种日常容器的液位证明了该方法的有效性。

Abstract: Computer vision seeks to infer a wide range of information about objects and
events. However, vision systems based on conventional imaging are limited to
extracting information only from the visible surfaces of scene objects. For
instance, a vision system can detect and identify a Coke can in the scene, but
it cannot determine whether the can is full or empty. In this paper, we aim to
expand the scope of computer vision to include the novel task of inferring the
hidden liquid levels of opaque containers by sensing the tiny vibrations on
their surfaces. Our method provides a first-of-a-kind way to inspect the fill
level of multiple sealed containers remotely, at once, without needing physical
manipulation and manual weighing. First, we propose a novel speckle-based
vibration sensing system for simultaneously capturing scene vibrations on a 2D
grid of points. We use our system to efficiently and remotely capture a dataset
of vibration responses for a variety of everyday liquid containers. Then, we
develop a transformer-based approach for analyzing the captured vibrations and
classifying the container type and its hidden liquid level at the time of
measurement. Our architecture is invariant to the vibration source, yielding
correct liquid level estimates for controlled and ambient scene sound sources.
Moreover, our model generalizes to unseen container instances within known
classes (e.g., training on five Coke cans of a six-pack, testing on a sixth)
and fluid levels. We demonstrate our method by recovering liquid levels from
various everyday containers.

</details>


### [330] [Investigation of Accuracy and Bias in Face Recognition Trained with Synthetic Data](https://arxiv.org/abs/2507.20782)
*Pavel Korshunov,Ketan Kotwal,Christophe Ecabert,Vidit Vidit,Amir Mohammadi,Sebastien Marcel*

Main category: cs.CV

TL;DR: 本文评估合成数据对人脸识别系统偏差和性能的影响，用文本到图像生成器生成平衡人脸数据集，结合身份增强方法评估，结果表明合成数据泛化有差距但部分可缓解偏差，增强数量和质量影响准确性和公平性。


<details>
  <summary>Details</summary>
Motivation: 探讨合成数据能否同时实现人脸识别系统的高准确性和公平性。

Method: 使用Flux.1 - dev和Stable Diffusion v3.5生成平衡人脸数据集FairFaceGen，结合Arc2Face和四个IP - Adapters等身份增强方法，在标准和挑战性基准上评估人脸识别性能和偏差。

Result: 合成数据在IJB - B/C泛化上落后于真实数据集，但基于SD35生成的人口统计学平衡合成数据集有缓解偏差的潜力，增强数量和质量显著影响准确性和公平性。

Conclusion: 为使用合成数据构建更公平的人脸识别系统提供实用指南。

Abstract: Synthetic data has emerged as a promising alternative for training face
recognition (FR) models, offering advantages in scalability, privacy
compliance, and potential for bias mitigation. However, critical questions
remain on whether both high accuracy and fairness can be achieved with
synthetic data. In this work, we evaluate the impact of synthetic data on bias
and performance of FR systems. We generate balanced face dataset, FairFaceGen,
using two state of the art text-to-image generators, Flux.1-dev and Stable
Diffusion v3.5 (SD35), and combine them with several identity augmentation
methods, including Arc2Face and four IP-Adapters. By maintaining equal identity
count across synthetic and real datasets, we ensure fair comparisons when
evaluating FR performance on standard (LFW, AgeDB-30, etc.) and challenging
IJB-B/C benchmarks and FR bias on Racial Faces in-the-Wild (RFW) dataset. Our
results demonstrate that although synthetic data still lags behind the real
datasets in the generalization on IJB-B/C, demographically balanced synthetic
datasets, especially those generated with SD35, show potential for bias
mitigation. We also observe that the number and quality of intra-class
augmentations significantly affect FR accuracy and fairness. These findings
provide practical guidelines for constructing fairer FR systems using synthetic
data.

</details>


### [331] [Not Only Grey Matter: OmniBrain for Robust Multimodal Classification of Alzheimer's Disease](https://arxiv.org/abs/2507.20872)
*Ahmed Sharshar,Yasser Ashraf,Tameem Bakr,Salma Hassan,Hosam Elgendy,Mohammad Yaqub,Mohsen Guizani*

Main category: cs.CV

TL;DR: 提出OmniBrain框架用于阿尔茨海默病诊断，表现优于现有方法，且具可解释性。


<details>
  <summary>Details</summary>
Motivation: 全球阿尔茨海默病患者众多且预计增多，现有诊断方法无法同时满足准确性、跨数据集泛化性、对缺失模态的鲁棒性和可解释性等要求。

Method: 提出OmniBrain多模态框架，用统一模型结合交叉注意力和模态丢弃，整合脑MRI、影像组学、基因表达和临床数据。

Result: 在ANMerge数据集上准确率达92.2±2.4%，在仅MRI的ADNI数据集上泛化准确率达70.4±2.7%，优于单模态和先前多模态方法。

Conclusion: OmniBrain为现实世界阿尔茨海默病诊断提供了稳健、可解释且实用的解决方案。

Abstract: Alzheimer's disease affects over 55 million people worldwide and is projected
to more than double by 2050, necessitating rapid, accurate, and scalable
diagnostics. However, existing approaches are limited because they cannot
achieve clinically acceptable accuracy, generalization across datasets,
robustness to missing modalities, and explainability all at the same time. This
inability to satisfy all these requirements simultaneously undermines their
reliability in clinical settings. We propose OmniBrain, a multimodal framework
that integrates brain MRI, radiomics, gene expression, and clinical data using
a unified model with cross-attention and modality dropout. OmniBrain achieves
$92.2 \pm 2.4\%$accuracy on the ANMerge dataset and generalizes to the MRI-only
ADNI dataset with $70.4 \pm 2.7\%$ accuracy, outperforming unimodal and prior
multimodal approaches. Explainability analyses highlight neuropathologically
relevant brain regions and genes, enhancing clinical trust. OmniBrain offers a
robust, interpretable, and practical solution for real-world Alzheimer's
diagnosis.

</details>


### [332] [SCORPION: Addressing Scanner-Induced Variability in Histopathology](https://arxiv.org/abs/2507.20907)
*Jeongun Ryu,Heon Song,Seungeun Lee,Soo Ick Cho,Jiwon Shin,Kyunghyun Paeng,Sérgio Pereira*

Main category: cs.CV

TL;DR: 提出SCORPION数据集评估模型在扫描仪差异下的可靠性，提出SimCons框架提升模型一致性。


<details>
  <summary>Details</summary>
Motivation: 确保计算病理学模型在不同扫描仪下可靠，以往研究未直接评估同一组织在不同扫描仪的一致性。

Method: 引入SCORPION数据集，设计扫描仪配对方案；提出SimCons框架结合增强域泛化技术和一致性损失。

Result: SimCons在不影响特定任务性能的情况下，提升了模型在不同扫描仪上的一致性。

Conclusion: 发布SCORPION数据集和提出SimCons框架，为评估和提升模型在不同扫描仪上的一致性提供重要资源。

Abstract: Ensuring reliable model performance across diverse domains is a critical
challenge in computational pathology. A particular source of variability in
Whole-Slide Images is introduced by differences in digital scanners, thus
calling for better scanner generalization. This is critical for the real-world
adoption of computational pathology, where the scanning devices may differ per
institution or hospital, and the model should not be dependent on
scanner-induced details, which can ultimately affect the patient's diagnosis
and treatment planning. However, past efforts have primarily focused on
standard domain generalization settings, evaluating on unseen scanners during
training, without directly evaluating consistency across scanners for the same
tissue. To overcome this limitation, we introduce SCORPION, a new dataset
explicitly designed to evaluate model reliability under scanner variability.
SCORPION includes 480 tissue samples, each scanned with 5 scanners, yielding
2,400 spatially aligned patches. This scanner-paired design allows for the
isolation of scanner-induced variability, enabling a rigorous evaluation of
model consistency while controlling for differences in tissue composition.
Furthermore, we propose SimCons, a flexible framework that combines
augmentation-based domain generalization techniques with a consistency loss to
explicitly address scanner generalization. We empirically show that SimCons
improves model consistency on varying scanners without compromising
task-specific performance. By releasing the SCORPION dataset and proposing
SimCons, we provide the research community with a crucial resource for
evaluating and improving model consistency across diverse scanners, setting a
new standard for reliability testing.

</details>


### [333] [HAMLET-FFD: Hierarchical Adaptive Multi-modal Learning Embeddings Transformation for Face Forgery Detection](https://arxiv.org/abs/2507.20913)
*Jialei Cui,Jianwei Du,Yanzhe Li,Lei Gao,Hui Jiang,Chenfu Bao*

Main category: cs.CV

TL;DR: 提出HAMLET - FFD框架解决人脸伪造检测跨域泛化问题，实验显示其泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 人脸操纵技术快速发展，传统方法难以学习域不变表示，面临跨域泛化挑战。

Method: 提出认知启发的分层自适应多模态学习框架HAMLET - FFD，通过双向跨模态推理，引入知识细化循环，结合视觉证据和概念线索评估真实性；采用双向融合机制，冻结预训练参数。

Result: 在多个基准测试中对未见操作有优越泛化能力，视觉分析显示嵌入间分工明确。

Conclusion: HAMLET - FFD能有效解决人脸伪造检测的跨域泛化问题。

Abstract: The rapid evolution of face manipulation techniques poses a critical
challenge for face forgery detection: cross-domain generalization. Conventional
methods, which rely on simple classification objectives, often fail to learn
domain-invariant representations. We propose HAMLET-FFD, a cognitively inspired
Hierarchical Adaptive Multi-modal Learning framework that tackles this
challenge via bidirectional cross-modal reasoning. Building on contrastive
vision-language models such as CLIP, HAMLET-FFD introduces a knowledge
refinement loop that iteratively assesses authenticity by integrating visual
evidence with conceptual cues, emulating expert forensic analysis. A key
innovation is a bidirectional fusion mechanism in which textual authenticity
embeddings guide the aggregation of hierarchical visual features, while
modulated visual features refine text embeddings to generate image-adaptive
prompts. This closed-loop process progressively aligns visual observations with
semantic priors to enhance authenticity assessment. By design, HAMLET-FFD
freezes all pretrained parameters, serving as an external plugin that preserves
CLIP's original capabilities. Extensive experiments demonstrate its superior
generalization to unseen manipulations across multiple benchmarks, and visual
analyses reveal a division of labor among embeddings, with distinct
representations specializing in fine-grained artifact recognition.

</details>


### [334] [Deep Learning for Skeleton Based Human Motion Rehabilitation Assessment: A Benchmark](https://arxiv.org/abs/2507.21018)
*Ali Ismail-Fawaz,Maxime Devanne,Stefano Berretti,Jonathan Weber,Germain Forestier*

Main category: cs.CV

TL;DR: 本文聚合康复数据集创建Rehab - Pile，提出评估框架并进行基准测试，发布相关数据和代码，为自动化康复评估研究奠基。


<details>
  <summary>Details</summary>
Motivation: 当前康复运动评估领域缺乏标准化基准、一致评估协议和可复现方法，限制研究进展和可比性。

Method: 聚合现有康复数据集形成Rehab - Pile；提出评估深度学习方法的通用基准框架；对多种架构进行分类和回归任务的广泛基准测试。

Result: 完成数据集聚合、框架提出和基准测试，并向社区发布所有数据集和实现。

Conclusion: 为未来自动化康复评估研究奠定坚实基础，促进可靠、可及和个性化康复解决方案的发展。

Abstract: Automated assessment of human motion plays a vital role in rehabilitation,
enabling objective evaluation of patient performance and progress. Unlike
general human activity recognition, rehabilitation motion assessment focuses on
analyzing the quality of movement within the same action class, requiring the
detection of subtle deviations from ideal motion. Recent advances in deep
learning and video-based skeleton extraction have opened new possibilities for
accessible, scalable motion assessment using affordable devices such as
smartphones or webcams. However, the field lacks standardized benchmarks,
consistent evaluation protocols, and reproducible methodologies, limiting
progress and comparability across studies. In this work, we address these gaps
by (i) aggregating existing rehabilitation datasets into a unified archive
called Rehab-Pile, (ii) proposing a general benchmarking framework for
evaluating deep learning methods in this domain, and (iii) conducting extensive
benchmarking of multiple architectures across classification and regression
tasks. All datasets and implementations are released to the community to
support transparency and reproducibility. This paper aims to establish a solid
foundation for future research in automated rehabilitation assessment and
foster the development of reliable, accessible, and personalized rehabilitation
solutions. The datasets, source-code and results of this article are all
publicly available.

</details>


### [335] [JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1](https://arxiv.org/abs/2507.20987)
*Xinhan Di,Kristin Qi,Pengqian Yu*

Main category: cs.CV

TL;DR: 当前基于扩散的视频生成方法在全身动作和自然语音联合生成时缺乏多模态一致性，且评估框架和基准不足，本文引入JWB - DH - V1数据集和评估协议，评估显示SOTA模型不同部位表现有差异。


<details>
  <summary>Details</summary>
Motivation: 解决当前扩散式视频生成方法在全身动作和语音联合生成时多模态一致性不足，以及缺乏综合评估框架和特定区域性能分析基准的问题。

Method: 引入Joint Whole - Body Talking Avatar and Speech Generation Version I (JWB - DH - V1)，包含大规模多模态数据集和评估协议。

Result: 对SOTA模型的评估揭示了以脸/手为中心和全身表现之间存在一致的性能差异。

Conclusion: 指出了未来研究的关键领域，数据集和评估工具已公开。

Abstract: Recent advances in diffusion-based video generation have enabled
photo-realistic short clips, but current methods still struggle to achieve
multi-modal consistency when jointly generating whole-body motion and natural
speech. Current approaches lack comprehensive evaluation frameworks that assess
both visual and audio quality, and there are insufficient benchmarks for
region-specific performance analysis. To address these gaps, we introduce the
Joint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1),
comprising a large-scale multi-modal dataset with 10,000 unique identities
across 2 million video samples, and an evaluation protocol for assessing joint
audio-video generation of whole-body animatable avatars. Our evaluation of SOTA
models reveals consistent performance disparities between face/hand-centric and
whole-body performance, which incidates essential areas for future research.
The dataset and evaluation tools are publicly available at
https://github.com/deepreasonings/WholeBodyBenchmark.

</details>


### [336] [Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM](https://arxiv.org/abs/2507.20994)
*Shen Li,Liuyi Yao,Wujia Niu,Lan Zhang,Yaliang Li*

Main category: cs.CV

TL;DR: 本文提出安全张量解决大视觉语言模型跨模态安全缺口，实验表明其能提升模型拒绝有害视觉输入能力并将文本安全扩展到视觉模态。


<details>
  <summary>Details</summary>
Motivation: 文本大语言模型的安全机制无法自然延伸到视觉模态，大视觉语言模型易受有害图像输入影响，存在跨模态安全缺口。

Method: 引入安全张量，通过文本或视觉模态在推理时应用，使用包含恶意图像 - 文本对、对比良性对和一般良性样本的数据集进行优化。

Result: 文本和视觉安全张量显著增强大视觉语言模型拒绝各种有害视觉输入的能力，在良性任务上性能近乎不变；安全张量能激活语言模块文本“安全层”。

Conclusion: 安全张量可有效将基于文本的安全扩展到视觉模态。

Abstract: Large visual-language models (LVLMs) integrate aligned large language models
(LLMs) with visual modules to process multimodal inputs. However, the safety
mechanisms developed for text-based LLMs do not naturally extend to visual
modalities, leaving LVLMs vulnerable to harmful image inputs. To address this
cross-modal safety gap, we introduce security tensors - trainable input vectors
applied during inference through either the textual or visual modality. These
tensors transfer textual safety alignment to visual processing without
modifying the model's parameters. They are optimized using a curated dataset
containing (i) malicious image-text pairs requiring rejection, (ii) contrastive
benign pairs with text structurally similar to malicious queries, with the
purpose of being contrastive examples to guide visual reliance, and (iii)
general benign samples preserving model functionality. Experimental results
demonstrate that both textual and visual security tensors significantly enhance
LVLMs' ability to reject diverse harmful visual inputs while maintaining
near-identical performance on benign tasks. Further internal analysis towards
hidden-layer representations reveals that security tensors successfully
activate the language module's textual "safety layers" in visual inputs,
thereby effectively extending text-based safety to the visual modality.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [337] [The Impact of Shared Telecom Infrastructure on Digital Connectivity and Inclusion](https://arxiv.org/abs/2507.19693)
*Georges V. Houngbonon,Marc Ivaldi,Emil Palikot,Davide Strusani*

Main category: econ.GN

TL;DR: 分析低收入国家28个塔共享交易，发现其可降低移动及数据价格，增加连接数量和农村互联网接入。


<details>
  <summary>Details</summary>
Motivation: 近半数世界人口未联网，资金短缺阻碍新网络建设，共享现有移动塔可加速连接。

Method: 收集28个低收入国家2008 - 20年的107个塔共享交易数据，估计交错双重差分效应。

Result: 超1000个塔交易两年后，移动价格指数和数据价格下降，移动连接数量增加，农村互联网接入和女性户主家庭互联网接入增加，塔共享协议增加了产品市场竞争。

Conclusion: 移动塔共享有助于加速互联网连接，带来价格、连接数量及市场竞争等多方面积极影响。

Abstract: Nearly half the world remains offline, and capital scarcity stalls new
network buildouts. Sharing existing mobile towers could accelerate
connectivity. We assemble data on 107 tower-sharing deals in 28 low-income
countries (2008-20) and estimate staggered difference-in-differences effects.
Two years after a transaction covering over 1,000 towers, the PPP-adjusted
mobile-price index falls USD 1.60 (s.e. 1.10) from a baseline of USD 3.16,
while data prices drop USD 1.00 (0.29), baseline USD 3.41 per GB. The number of
mobile connections increases. Rural internet access increases by 4.7 pp and
female-headed households by 3.6 pp. Tower-sharing agreements increase product
market competition as measured by Herfindahl-Hirschman Index.

</details>


### [338] [AIs Structural Impact on Indias Knowledge Intensive Startup Ecosystem: A Natural Experiment in Firm Efficiency and Design](https://arxiv.org/abs/2507.19775)
*Venkat Ram Reddy Ganuthula,Ramesh Kuruva*

Main category: econ.GN

TL;DR: 研究探讨AI对印度知识密集型初创企业的结构和绩效影响，发现AI时代企业规模大但效率低，为创业策略提供参考并强调纵向研究必要性。


<details>
  <summary>Details</summary>
Motivation: 探究人工智能采用对印度知识密集型初创企业在结构和绩效方面的影响。

Method: 采用自然实验框架，以成立年份作为外生处理代理，对比AI前和AI时代企业的规模、收入生产率、估值效率和资本利用率。

Result: AI时代企业规模更大但效率更低，虽吸引更多资金和获得更高绝对估值，但员工人均生产率和效率比率较低。

Conclusion: AI有变革性作用，当前情况可能是早期技术投资未带来成比例回报，研究为全球创业策略提供信息，需开展关于可持续性的纵向研究。

Abstract: This study explores the structural and performance impacts of artificial
intelligence (AI) adoption on Indias knowledge intensive startups, spanning
information technology, financial technology, health technology, and
educational technology, founded between 2016 and 2025. Using a natural
experiment framework with the founding year as an exogenous treatment proxy, it
examines firm size, revenue productivity, valuation efficiency, and capital
utilization across pre AI and AI era cohorts. Findings reveal larger structures
and lower efficiency in AI era firms, supported by a dataset of 914 cleaned
firms. The study offers insights into AIs transformative role, suggesting that
while AI era firms attract higher funding and achieve higher absolute
valuations, their per employee productivity and efficiency ratios are lower,
potentially indicating earlystage investments in technology that have yet to
yield proportional returns. This informs global entrepreneurial strategies
while highlighting the need for longitudinal research on sustainability.

</details>


### [339] [AI-Driven Spatial Distribution Dynamics: A Comprehensive Theoretical and Empirical Framework for Analyzing Productivity Agglomeration Effects in Japan's Aging Society](https://arxiv.org/abs/2507.19911)
*Tatsuru Kikuchi*

Main category: econ.GN

TL;DR: 本文构建分析人口转型期大都市AI驱动空间分布动态的框架，用东京实证分析，得出AI对集聚和生产率影响，还给出政策框架。


<details>
  <summary>Details</summary>
Motivation: 构建分析人口转型期大都市AI驱动空间分布动态的综合理论和实证框架。

Method: 扩展新经济地理学，提出五个AI特定机制；以东京为实证对象，用五种计量策略进行因果识别，开展27种未来情景的机器学习预测。

Result: 理论框架六个假设获实证支持；AI实施使集聚集中度提高4.2 - 5.2个百分点，不同行业效果不同；积极采用AI可抵消60 - 80%老龄化导致的生产率下降。

Conclusion: 提供三阶段政策框架，该综合方法为分析技术驱动的空间变化建立新范式，适用于老龄化社会。

Abstract: This paper develops the first comprehensive theoretical and empirical
framework for analyzing AI-driven spatial distribution dynamics in metropolitan
areas undergoing demographic transition. We extend New Economic Geography by
formalizing five novel AI-specific mechanisms: algorithmic learning spillovers,
digital infrastructure returns, virtual agglomeration effects, AI-human
complementarity, and network externalities. Using Tokyo as our empirical
laboratory, we implement rigorous causal identification through five
complementary econometric strategies and develop machine learning predictions
across 27 future scenarios spanning 2024-2050. Our theoretical framework
generates six testable hypotheses, all receiving strong empirical support. The
causal analysis reveals that AI implementation increases agglomeration
concentration by 4.2-5.2 percentage points, with heterogeneous effects across
industries: high AI-readiness sectors experience 8.4 percentage point
increases, while low AI-readiness sectors show 1.2 percentage point gains.
Machine learning predictions demonstrate that aggressive AI adoption can offset
60-80\% of aging-related productivity declines. We provide a strategic
three-phase policy framework for managing AI-driven spatial transformation
while promoting inclusive development. The integrated approach establishes a
new paradigm for analyzing technology-driven spatial change with global
applications for aging societies.

</details>


### [340] [Assessing the Sensitivities of Input-Output Methods for Natural Hazard-Induced Power Outage Macroeconomic Impacts](https://arxiv.org/abs/2507.19989)
*Matthew Sprintson,Edward Oughton*

Main category: econ.GN

TL;DR: 文章量化美国三次自然灾害导致停电的宏观经济影响，比较不同数据参数化技术和模型，发现结果对模型架构、参数化和假设敏感，首次系统比较多个投入产出模型和参数化方法。


<details>
  <summary>Details</summary>
Motivation: 大量研究虽用多种建模和数据参数化技术研究电网中断宏观经济影响，但需进一步研究，本文旨在量化三次重大自然灾害导致的美国停电的宏观经济影响。

Method: 评估三种常用数据参数化技术（家庭中断、千瓦时损失和卫星光度）和三种静态模型（列昂惕夫和戈什、关键投入和不可操作性投入产出）。

Result: 三次停电国内损失平均估计分别为31.3亿美元、41.8亿美元和29.3亿美元；数据参数化技术可使估计损失变化23.1%和50.5%；GDP损失对模型架构、数据参数化和分析师假设高度敏感；数值输出比部门间联系和其他宏观经济见解更敏感。

Conclusion: 结果敏感性因模型而异，源于分析师先验决策；首次对多个投入产出模型和参数化方法进行系统比较，为分析师提供指导和见解。

Abstract: It is estimated that over one-fourth of US households experienced a power
outage in 2023, costing on average US $\$150$ Bn annually, with $87\%$ of
outages caused by natural hazards. Indeed, numerous studies have examined the
macroeconomic impact of power network interruptions, employing a wide variety
of modeling methods and data parameterization techniques, which warrants
further investigation. In this paper, we quantify the macroeconomic effects of
three significant natural hazard-induced US power outages: Hurricane Ian
(2022), the 2021 Texas Blackouts, and Tropical Storm Isaias (2020). Our
analysis evaluates the sensitivity of three commonly used data parameterization
techniques (household interruptions, kWh lost, and satellite luminosity), along
with three static models (Leontief and Ghosh, critical input, and inoperability
Input-Output). We find the mean domestic loss estimates to be US $\$3.13$ Bn,
US $\$4.18$ Bn, and US $\$2.93$ Bn, respectively. Additionally, data
parameterization techniques can alter estimated losses by up to $23.1\%$ and
$50.5\%$. Consistent with the wide range of outputs, we find that the GDP
losses are highly sensitive to model architecture, data parameterization, and
analyst assumptions. Results sensitivity is not uniform across models and
arises from important a priori analyst decisions, demonstrated by data
parameterization techniques yielding $11\%$ and $45\%$ differences within a
model. We find that the numerical value output is more sensitive than
intersectoral linkages and other macroeconomic insights. To our knowledge, we
contribute to literature the first systematic comparison of multiple IO models
and parameterizations across several natural hazard-induced long-duration power
outages, providing guidance and insights for analysts.

</details>


### [341] [Measuring the Macroeconomic and Financial Stability of Bangladesh](https://arxiv.org/abs/2507.20340)
*Faruque Ahamed,Md Ataur Rahman Chowdhury*

Main category: econ.GN

TL;DR: 本文构建孟加拉国2016 - 2024年综合金融稳定指数（AFSI）评估金融系统，发现2024财年虽部分部门有改善但整体金融稳定性下降，强调部门关联及改革等需求，AFSI可作预警工具。


<details>
  <summary>Details</summary>
Motivation: 评估孟加拉国金融系统的系统性健康和韧性。

Method: 构建包含四个关键部门19个宏观金融指标的AFSI，采用归一化评分方法和等权重方案聚合子指数。

Result: 2024财年实际和财政部门有适度改善，但因金融货币部门表现不佳和外部部门持续疲软，整体金融稳定性下降，有多项关键压力指标。

Conclusion: 强调宏观金融部门的相互关联，指出迫切需要结构改革、加强监管和宏观审慎政策协调，AFSI框架可作预警工具并丰富新兴经济体金融稳定测量文献。

Abstract: This study constructs an Aggregate Financial Stability Index (AFSI) for
Bangladesh to evaluate the systemic health and resilience of the countrys
financial system during the period from 2016 to 2024. The index incorporates 19
macrofinancial indicators across four key sectors Real Sector, Financial and
Monetary Sector, Fiscal Sector and External Sector. Using a normalized scoring
approach and equal weighting scheme, sub-indices were aggregated to form a
comprehensive measure of financial stability. The findings indicate that while
the Real and Fiscal sectors demonstrated modest improvements in FY2024, overall
financial stability deteriorated, largely due to poor performance in the
Financial and Monetary Sector and continued weakness in the External Sector.
Key stress indicators include rising non-performing loans, declining capital
adequacy ratios, weak capital market performance, growing external debt, and
shrinking foreign exchange reserves. The study highlights the
interconnectedness of macro-financial sectors and the urgent need for
structural reforms, stronger regulatory oversight, and enhanced macroprudential
policy coordination. The AFSI framework developed in this paper offers an early
warning tool for policymakers and contributes to the literature on financial
stability measurement in emerging economies.

</details>


### [342] [Beyond pay: AI skills reward more job benefits](https://arxiv.org/abs/2507.20410)
*Fabian Stephany,Alejandra Mira,Matthew Bone*

Main category: econ.GN

TL;DR: 研究美国劳动力市场中人工智能技能相关非货币奖励，发现人工智能岗位更易提供非货币福利，高薪岗位倾向捆绑福利，凸显人才竞争。


<details>
  <summary>Details</summary>
Motivation: 以往研究关注人工智能岗位工资溢价，本文探究需求是否转化为非货币补偿。

Method: 使用2018 - 2024年约一千万个在线招聘数据，识别人工智能岗位，考察非货币福利。

Result: 人工智能岗位更易提供非货币福利，如育儿假和远程工作；高薪岗位倾向捆绑福利；提供特定福利的岗位平均工资更高。

Conclusion: 凸显劳动力市场对人工智能人才的强烈需求，表明雇主通过货币和非货币激励竞争稀缺人才。

Abstract: This study investigates the non-monetary rewards associated with artificial
intelligence (AI) skills in the U.S. labour market. Using a dataset of
approximately ten million online job vacancies from 2018 to 2024, we identify
AI roles-positions requiring at least one AI-related skill-and examine the
extent to which these roles offer non-monetary benefits such as tuition
assistance, paid leave, health and well-being perks, parental leave, workplace
culture enhancements, and remote work options. While previous research has
documented substantial wage premiums for AI-related roles due to growing demand
and limited talent supply, our study asks whether this demand also translates
into enhanced non-monetary compensation. We find that AI roles are
significantly more likely to offer such perks, even after controlling for
education requirements, industry, and occupation type. It is twice as likely
for an AI role to offer parental leave and almost three times more likely to
provide remote working options. Moreover, the highest-paying AI roles tend to
bundle these benefits, suggesting a compound premium where salary increases
coincide with expanded non-monetary rewards. AI roles offering parental leave
or health benefits show salaries that are, on average, 12% to 20% higher than
AI roles without this benefit. This pattern is particularly pronounced in years
and occupations experiencing the highest AI-related demand, pointing to a
demand-driven dynamic. Our findings underscore the strong pull of AI talent in
the labor market and challenge narratives of technological displacement,
highlighting instead how employers compete for scarce talent through both
financial and non-financial incentives.

</details>


### [343] [Aligning Large Language Model Agents with Rational and Moral Preferences: A Supervised Fine-Tuning Approach](https://arxiv.org/abs/2507.20796)
*Wei Lu,Daniel L. Chen,Christian B. Hansen*

Main category: econ.GN

TL;DR: 本文用经济博弈评估大语言模型偏好，发现其与人类行为有偏差，提出监督微调管道使模型与经济偏好对齐，并在两个应用中评估微调后模型行为，贡献了基于道德 - 经济原则对齐AI偏好的方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型自主参与重要决策，了解其在战略互动中的行为至关重要。

Method: 使用经典经济博弈评估LLM偏好，提出基于经济推理合成数据集的监督微调管道，聚焦两种偏好结构。

Result: 小数据集微调使LLM行为向相应经济主体转变，不同规范目标会影响市场和道德结果。

Conclusion: 贡献了可复制、低成本且基于经济原理的管道，用于基于道德 - 经济原则对齐AI偏好。

Abstract: Understanding how large language model (LLM) agents behave in strategic
interactions is essential as these systems increasingly participate
autonomously in economically and morally consequential decisions. We evaluate
LLM preferences using canonical economic games, finding substantial deviations
from human behavior. Models like GPT-4o show excessive cooperation and limited
incentive sensitivity, while reasoning models, such as o3-mini, align more
consistently with payoff-maximizing strategies. We propose a supervised
fine-tuning pipeline that uses synthetic datasets derived from economic
reasoning to align LLM agents with economic preferences, focusing on two
stylized preference structures. In the first, utility depends only on
individual payoffs (homo economicus), while utility also depends on a notion of
Kantian universalizability in the second preference structure (homo moralis).
We find that fine-tuning based on small datasets shifts LLM agent behavior
toward the corresponding economic agent. We further assess the fine-tuned
agents' behavior in two applications: Moral dilemmas involving autonomous
vehicles and algorithmic pricing in competitive markets. These examples
illustrate how different normative objectives embedded via realizations from
structured preference structures can influence market and moral outcomes. This
work contributes a replicable, cost-efficient, and economically grounded
pipeline to align AI preferences using moral-economic principles.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [344] [Iterative Pretraining Framework for Interatomic Potentials](https://arxiv.org/abs/2507.20118)
*Taoyong Cui,Zhongyao Wang,Dongzhan Zhou,Yuqiang Li,Lei Bai,Wanli Ouyang,Mao Su,Shufei Zhang*

Main category: physics.comp-ph

TL;DR: 提出迭代预训练框架 IPIP 提升机器学习原子间势模型性能，在 Mo - S - O 系统表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有预训练策略存在预训练与下游任务目标不匹配、依赖大量标记数据和复杂架构等问题，需改进机器学习原子间势模型性能。

Method: 提出 Iterative Pretraining for Interatomic Potentials (IPIP) 框架，融入遗忘机制防止迭代训练陷入局部最优。

Result: 相比通用力场，在 Mo - S - O 系统预测误差降低超 80%，速度提升达 4 倍。

Conclusion: IPIP 用轻量级架构实现更高准确性和效率，能进行快速准确模拟。

Abstract: Machine learning interatomic potentials (MLIPs) enable efficient molecular
dynamics (MD) simulations with ab initio accuracy and have been applied across
various domains in physical science. However, their performance often relies on
large-scale labeled training data. While existing pretraining strategies can
improve model performance, they often suffer from a mismatch between the
objectives of pretraining and downstream tasks or rely on extensive labeled
datasets and increasingly complex architectures to achieve broad
generalization. To address these challenges, we propose Iterative Pretraining
for Interatomic Potentials (IPIP), a framework designed to iteratively improve
the predictive performance of MLIP models. IPIP incorporates a forgetting
mechanism to prevent iterative training from converging to suboptimal local
minima. Unlike general-purpose foundation models, which frequently underperform
on specialized tasks due to a trade-off between generality and system-specific
accuracy, IPIP achieves higher accuracy and efficiency using lightweight
architectures. Compared to general-purpose force fields, this approach achieves
over 80% reduction in prediction error and up to 4x speedup in the challenging
Mo-S-O system, enabling fast and accurate simulations.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [345] [Deep Learning Based Joint Channel Estimation and Positioning for Sparse XL-MIMO OFDM Systems](https://arxiv.org/abs/2507.19936)
*Zhongnian Li,Chao Zheng,Jian Xiao,Ji Wang,Gongpu Wang,Ming Zeng,Octavia A. Dobre*

Main category: eess.SP

TL;DR: 研究近场稀疏超大规模MIMO-OFDM系统联合信道估计与定位，提出基于深度学习两阶段框架和CP - Mamba架构，结果表明该方法优于现有基线方法，稀疏阵列性能更佳。


<details>
  <summary>Details</summary>
Motivation: 实现信道估计和定位的协同增益，提高信道估计精度。

Method: 提出基于深度学习的两阶段框架，包括定位和信道估计阶段，提出CP - Mamba架构用于信道估计和定位。

Result: 所提两阶段方法结合CP - Mamba架构优于现有基线方法，稀疏阵列在信道估计和定位精度上表现更好。

Conclusion: 基于深度学习的两阶段框架和CP - Mamba架构可有效用于近场稀疏XL - MIMO OFDM系统的联合信道估计和定位。

Abstract: This paper investigates joint channel estimation and positioning in
near-field sparse extra-large multiple-input multiple-output (XL-MIMO)
orthogonal frequency division multiplexing (OFDM) systems. To achieve
cooperative gains between channel estimation and positioning, we propose a deep
learning-based two-stage framework comprising positioning and channel
estimation. In the positioning stage, the user's coordinates are predicted and
utilized in the channel estimation stage, thereby enhancing the accuracy of
channel estimation. Within this framework, we propose a U-shaped Mamba
architecture for channel estimation and positioning, termed as CP-Mamba. This
network integrates the strengths of the Mamba model with the structural
advantages of U-shaped convolutional networks, enabling effective capture of
local spatial features and long-range temporal dependencies of the channel.
Numerical simulation results demonstrate that the proposed two-stage approach
with CP-Mamba architecture outperforms existing baseline methods. Moreover,
sparse arrays (SA) exhibit significantly superior performance in both channel
estimation and positioning accuracy compared to conventional compact arrays.

</details>


### [346] [NeuroCLIP: A Multimodal Contrastive Learning Method for rTMS-treated Methamphetamine Addiction Analysis](https://arxiv.org/abs/2507.20189)
*Chengkai Wang,Di Wu,Yunsheng Liao,Wenyao Zheng,Ziyi Zeng,Xurong Gao,Hemmings Wu,Zhoule Zhu,Jie Yang,Lihua Zhong,Weiwei Cheng,Yun-Hsuan Chen,Mohamad Sawan*

Main category: eess.SP

TL;DR: 提出NeuroCLIP框架整合EEG和fNIRS数据，为甲基苯丙胺成瘾提供可靠生物标志物，验证其有效性与相关性。


<details>
  <summary>Details</summary>
Motivation: 现有甲基苯丙胺依赖评估和治疗评价依赖主观报告，客观神经影像方法有局限，需可靠生物标志物。

Method: 提出NeuroCLIP深度学习框架，用渐进学习策略整合同时记录的EEG和fNIRS数据。

Result: NeuroCLIP比单模态模型有更好判别能力，可客观评估rTMS疗效，生物标志物与渴求分数强相关。

Conclusion: NeuroCLIP衍生的生物标志物比单模态方法更稳健可靠，为成瘾神经科学研究和临床评估提供有价值工具。

Abstract: Methamphetamine dependence poses a significant global health challenge, yet
its assessment and the evaluation of treatments like repetitive transcranial
magnetic stimulation (rTMS) frequently depend on subjective self-reports, which
may introduce uncertainties. While objective neuroimaging modalities such as
electroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS)
offer alternatives, their individual limitations and the reliance on
conventional, often hand-crafted, feature extraction can compromise the
reliability of derived biomarkers. To overcome these limitations, we propose
NeuroCLIP, a novel deep learning framework integrating simultaneously recorded
EEG and fNIRS data through a progressive learning strategy. This approach
offers a robust and trustworthy biomarker for methamphetamine addiction.
Validation experiments show that NeuroCLIP significantly improves
discriminative capabilities among the methamphetamine-dependent individuals and
healthy controls compared to models using either EEG or only fNIRS alone.
Furthermore, the proposed framework facilitates objective, brain-based
evaluation of rTMS treatment efficacy, demonstrating measurable shifts in
neural patterns towards healthy control profiles after treatment. Critically,
we establish the trustworthiness of the multimodal data-driven biomarker by
showing its strong correlation with psychometrically validated craving scores.
These findings suggest that biomarker derived from EEG-fNIRS data via NeuroCLIP
offers enhanced robustness and reliability over single-modality approaches,
providing a valuable tool for addiction neuroscience research and potentially
improving clinical assessments.

</details>


### [347] [A Multi-Stage Hybrid CNN-Transformer Network for Automated Pediatric Lung Sound Classification](https://arxiv.org/abs/2507.20408)
*Samiul Based Shuvo,Taufiq Hasan*

Main category: eess.SP

TL;DR: 本文提出多阶段混合CNN - Transformer框架用于儿科呼吸疾病分类，模型表现优于先前最佳模型，为儿科呼吸疾病诊断提供方案。


<details>
  <summary>Details</summary>
Motivation: 呼吸音自动分析对监测呼吸健康很重要，儿科尤其是6岁以下儿童呼吸音分类研究不足，且该年龄段肺部发育变化需专门分类方法。

Method: 提出多阶段混合CNN - Transformer框架，结合CNN提取特征与基于注意力的架构，使用全记录和单个呼吸事件的尺度图图像进行分类，采用类明智焦点损失解决数据不平衡问题。

Result: 模型在二元事件分类中得分为0.9039，多类事件分类中为0.8448；记录级别上，三元分类得分为0.720，多类分类为0.571，优于先前最佳模型。

Conclusion: 该方法为可扩展的儿科呼吸疾病诊断提供了有前景的解决方案，尤其适用于资源有限的环境。

Abstract: Automated analysis of lung sound auscultation is essential for monitoring
respiratory health, especially in regions facing a shortage of skilled
healthcare workers. While respiratory sound classification has been widely
studied in adults, its ap plication in pediatric populations, particularly in
children aged <6 years, remains an underexplored area. The developmental
changes in pediatric lungs considerably alter the acoustic proper ties of
respiratory sounds, necessitating specialized classification approaches
tailored to this age group. To address this, we propose a multistage hybrid
CNN-Transformer framework that combines CNN-extracted features with an
attention-based architecture to classify pediatric respiratory diseases using
scalogram images from both full recordings and individual breath events. Our
model achieved an overall score of 0.9039 in binary event classifi cation and
0.8448 in multiclass event classification by employing class-wise focal loss to
address data imbalance. At the recording level, the model attained scores of
0.720 for ternary and 0.571 for multiclass classification. These scores
outperform the previous best models by 3.81% and 5.94%, respectively. This
approach offers a promising solution for scalable pediatric respiratory disease
diagnosis, especially in resource-limited settings.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [348] [Deep Reputation Scoring in DeFi: zScore-Based Wallet Ranking from Liquidity and Trading Signals](https://arxiv.org/abs/2507.20494)
*Dhanashekar Kandaswamy,Ashutosh Sahoo,Akshay SP,Gurukiran S,Parag Paul,Girish G N*

Main category: q-fin.GN

TL;DR: 提出Uniswap行为评分框架，含流动性提供分数和交易行为分数，结合规则蓝图和深度残差神经网络，实验证明对用户细分和声誉系统有用。


<details>
  <summary>Details</summary>
Motivation: 随着DeFi发展，区分用户流动性提供和主动交易行为对风险建模和链上声誉很重要。

Method: 提出行为评分框架，用规则蓝图构建分数，引入受U-Net启发的深度残差神经网络处理边缘情况和学习特征交互，纳入池级上下文。

Result: 实验表明该框架对Uniswap v3数据的用户细分和协议对齐声誉系统有用。

Conclusion: 该框架可实现上下文感知和可扩展的DeFi用户评分，支持改进风险评估和激励设计。

Abstract: As decentralized finance (DeFi) evolves, distinguishing between user
behaviors - liquidity provision versus active trading - has become vital for
risk modeling and on-chain reputation. We propose a behavioral scoring
framework for Uniswap that assigns two complementary scores: a Liquidity
Provision Score that assesses strategic liquidity contributions, and a Swap
Behavior Score that reflects trading intent, volatility exposure, and
discipline. The scores are constructed using rule-based blueprints that
decompose behavior into volume, frequency, holding time, and withdrawal
patterns. To handle edge cases and learn feature interactions, we introduce a
deep residual neural network with densely connected skip blocks inspired by the
U-Net architecture. We also incorporate pool-level context such as total value
locked (TVL), fee tiers, and pool size, allowing the system to differentiate
similar user behaviors across pools with varying characteristics. Our framework
enables context-aware and scalable DeFi user scoring, supporting improved risk
assessment and incentive design. Experiments on Uniswap v3 data show its
usefulness for user segmentation and protocol-aligned reputation systems.
Although we refer to our metric as zScore, it is independently developed and
methodologically different from the cross-protocol system proposed by Udupi et
al. Our focus is on role-specific behavioral modeling within Uniswap using
blueprint logic and supervised learning.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [349] [Do Not Mimic My Voice: Speaker Identity Unlearning for Zero-Shot Text-to-Speech](https://arxiv.org/abs/2507.20140)
*Taesoo Kim,Jinju Kim,Dongchan Kim,Jong Hwan Ko,Gyeong-Moon Park*

Main category: cs.SD

TL;DR: 本文针对零样本文本转语音（ZS - TTS）系统提出首个机器遗忘框架，解决说话人身份遗忘问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: ZS - TTS技术发展带来隐私和伦理问题，此前未研究从预训练模型参数中移除复制特定个体声音的知识，需解决说话人身份遗忘挑战。

Method: 提出机器遗忘框架，特别是教师引导遗忘（TGU），引入随机性；提出新评估指标spk - ZRF。

Result: 在最先进模型上的实验表明，TGU能防止模型复制指定说话人声音，同时保持对其他说话人的语音合成高质量。

Conclusion: 所提方法有效解决ZS - TTS系统中说话人身份遗忘问题。

Abstract: The rapid advancement of Zero-Shot Text-to-Speech (ZS-TTS) technology has
enabled high-fidelity voice synthesis from minimal audio cues, raising
significant privacy and ethical concerns. Despite the threats to voice privacy,
research to selectively remove the knowledge to replicate unwanted individual
voices from pre-trained model parameters has not been explored. In this paper,
we address the new challenge of speaker identity unlearning for ZS-TTS systems.
To meet this goal, we propose the first machine unlearning frameworks for
ZS-TTS, especially Teacher-Guided Unlearning (TGU), designed to ensure the
model forgets designated speaker identities while retaining its ability to
generate accurate speech for other speakers. Our proposed methods incorporate
randomness to prevent consistent replication of forget speakers' voices,
assuring unlearned identities remain untraceable. Additionally, we propose a
new evaluation metric, speaker-Zero Retrain Forgetting (spk-ZRF). This assesses
the model's ability to disregard prompts associated with forgotten speakers,
effectively neutralizing its knowledge of these voices. The experiments
conducted on the state-of-the-art model demonstrate that TGU prevents the model
from replicating forget speakers' voices while maintaining high quality for
other speakers. The demo is available at https://speechunlearn.github.io/

</details>


### [350] [Efficient Vocal-Conditioned Music Generation via Soft Alignment Attention and Latent Diffusion](https://arxiv.org/abs/2507.19991)
*Hei Shing Cheung,Boya Zhang*

Main category: cs.SD

TL;DR: 提出轻量级潜在扩散模型用于声乐条件音乐伴奏生成，参数少、推理快，性能优，可实时部署。


<details>
  <summary>Details</summary>
Motivation: 解决现有音乐AI系统的关键局限。

Method: 引入基于扩散时间步自适应结合局部和全局时间依赖的软对齐注意力机制，在预训练变分自编码器的压缩潜在空间中操作。

Result: 相比现有系统参数减少220倍，推理速度快52倍，仅1500万参数就有有竞争力的表现，在制作质量和内容一致性上优于OpenAI Jukebox。

Conclusion: 超轻量级架构可在消费级硬件上实时部署，使AI辅助音乐创作适用于交互式应用和资源受限环境。

Abstract: We present a lightweight latent diffusion model for vocal-conditioned musical
accompaniment generation that addresses critical limitations in existing music
AI systems. Our approach introduces a novel soft alignment attention mechanism
that adaptively combines local and global temporal dependencies based on
diffusion timesteps, enabling efficient capture of multi-scale musical
structure. Operating in the compressed latent space of a pre-trained
variational autoencoder, the model achieves a 220 times parameter reduction
compared to state-of-the-art systems while delivering 52 times faster
inference. Experimental evaluation demonstrates competitive performance with
only 15M parameters, outperforming OpenAI Jukebox in production quality and
content unity while maintaining reasonable musical coherence. The
ultra-lightweight architecture enables real-time deployment on consumer
hardware, making AI-assisted music creation accessible for interactive
applications and resource-constrained environments.

</details>


### [351] [Improving Audio Classification by Transitioning from Zero- to Few-Shot](https://arxiv.org/abs/2507.20036)
*James Taylor,Wolfgang Mack*

Main category: cs.SD

TL;DR: 本文研究提升音频分类准确率的少样本方法，结果显示少样本分类通常优于零样本基线。


<details>
  <summary>Details</summary>
Motivation: 现有零样本音频分类方法在确定音频类别的最优文本描述方面存在挑战，尤其是类别包含多种声音时。

Method: 按类别对音频嵌入进行分组和处理，以替代有噪声的文本嵌入。

Result: 少样本分类通常比零样本基线表现更好。

Conclusion: 少样本方法能提高音频分类的准确率，优于零样本方法。

Abstract: State-of-the-art audio classification often employs a zero-shot approach,
which involves comparing audio embeddings with embeddings from text describing
the respective audio class. These embeddings are usually generated by neural
networks trained through contrastive learning to align audio and text
representations. Identifying the optimal text description for an audio class is
challenging, particularly when the class comprises a wide variety of sounds.
This paper examines few-shot methods designed to improve classification
accuracy beyond the zero-shot approach. Specifically, audio embeddings are
grouped by class and processed to replace the inherently noisy text embeddings.
Our results demonstrate that few-shot classification typically outperforms the
zero-shot baseline.

</details>


### [352] [Improving Deep Learning-based Respiratory Sound Analysis with Frequency Selection and Attention Mechanism](https://arxiv.org/abs/2507.20052)
*Nouhaila Fraihi,Ouassim Karrakchou,Mounir Ghogho*

Main category: cs.SD

TL;DR: 提出紧凑的CNN - TSA网络，结合轻量级自注意力和高效CNN骨干，引入FBS模块，有年龄特定模型，在多个数据集表现优异，适合资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 现有CNN难以建模全局上下文，基于Transformer的模型计算需求高，需有效捕获细粒度声学特征和长距离时间依赖的模型用于呼吸音准确分类。

Method: 提出CNN - TSA网络，集成轻量级自注意力到CNN骨干；使用FBS模块抑制噪声和非信息频率区域；引入年龄特定模型。

Result: CNN - TSA + FBS在SPRSound设新基准，在ICBHI达最优，计算量小；FBS集成到现有Transformer基线在ICBHI创新纪录。

Conclusion: 所提框架可实现可靠、实时呼吸音分析，适用于资源受限场景。

Abstract: Accurate classification of respiratory sounds requires deep learning models
that effectively capture fine-grained acoustic features and long-range temporal
dependencies. Convolutional Neural Networks (CNNs) are well-suited for
extracting local time-frequency patterns but are limited in modeling global
context. In contrast, transformer-based models can capture long-range
dependencies, albeit with higher computational demands. To address these
limitations, we propose a compact CNN-Temporal Self-Attention (CNN-TSA) network
that integrates lightweight self-attention into an efficient CNN backbone.
Central to our approach is a Frequency Band Selection (FBS) module that
suppresses noisy and non-informative frequency regions, substantially improving
accuracy and reducing FLOPs by up to 50%. We also introduce age-specific models
to enhance robustness across diverse patient groups. Evaluated on the
SPRSound-2022/2023 and ICBHI-2017 lung sound datasets, CNN-TSA with FBS sets
new benchmarks on SPRSound and achieves state-of-the-art performance on ICBHI,
all with a significantly smaller computational footprint. Furthermore,
integrating FBS into an existing transformer baseline yields a new record on
ICBHI, confirming FBS as an effective drop-in enhancement. These results
demonstrate that our framework enables reliable, real-time respiratory sound
analysis suitable for deployment in resource-constrained settings.

</details>


### [353] [JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability and Aesthetic Alignment](https://arxiv.org/abs/2507.20880)
*Renhang Liu,Chia-Yu Hung,Navonil Majumder,Taylor Gautreaux,Amir Ali Bagherzadeh,Chuan Li,Dorien Herremans,Soujanya Poria*

Main category: cs.SD

TL;DR: 提出基于流匹配的JAM模型实现歌词到歌曲生成的词级控制，用直接偏好优化提升质量，用JAME数据集标准化评估，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有歌词到歌曲模型缺乏词级精细控制，且需要提升生成歌曲质量和标准化评估。

Method: 构建基于流匹配的JAM模型实现词级时间和时长控制，通过直接偏好优化进行美学对齐，创建公共评估数据集JAME。

Result: JAM在音乐特定属性方面优于现有模型。

Conclusion: JAM模型在歌词到歌曲生成中实现词级控制，优化了生成质量，同时标准化评估数据集有积极意义。

Abstract: Diffusion and flow-matching models have revolutionized automatic
text-to-audio generation in recent times. These models are increasingly capable
of generating high quality and faithful audio outputs capturing to speech and
acoustic events. However, there is still much room for improvement in creative
audio generation that primarily involves music and songs. Recent open
lyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an
acceptable standard in automatic song generation for recreational use. However,
these models lack fine-grained word-level controllability often desired by
musicians in their workflows. To the best of our knowledge, our
flow-matching-based JAM is the first effort toward endowing word-level timing
and duration control in song generation, allowing fine-grained vocal control.
To enhance the quality of generated songs to better align with human
preferences, we implement aesthetic alignment through Direct Preference
Optimization, which iteratively refines the model using a synthetic dataset,
eliminating the need or manual data annotations. Furthermore, we aim to
standardize the evaluation of such lyrics-to-song models through our public
evaluation dataset JAME. We show that JAM outperforms the existing models in
terms of the music-specific attributes.

</details>


### [354] [Music Arena: Live Evaluation for Text-to-Music](https://arxiv.org/abs/2507.20900)
*Yonghyun Kim,Wayne Chi,Anastasios N. Angelopoulos,Wei-Lin Chiang,Koichi Saito,Shinji Watanabe,Yuki Mitsufuji,Chris Donahue*

Main category: cs.SD

TL;DR: 提出音乐竞技场平台，用于文本到音乐模型的可扩展人类偏好评估，解决现有评估难题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到音乐模型的人类偏好评估成本高、难比较，且缺乏开放可再生的偏好源，需填补这些空白。

Method: 提供实时评估，用户输入文本比较两模型输出，利用偏好编制排行榜，有基于大语言模型的路由系统，收集详细偏好，提出滚动数据发布政策。

Result: 音乐竞技场平台能标准化评估协议、提供透明数据访问政策和音乐特定功能。

Conclusion: 音乐竞技场解决了文本到音乐生态系统的关键挑战，展示了实时评估可适应特定人工智能领域的独特特征。

Abstract: We present Music Arena, an open platform for scalable human preference
evaluation of text-to-music (TTM) models. Soliciting human preferences via
listening studies is the gold standard for evaluation in TTM, but these studies
are expensive to conduct and difficult to compare, as study protocols may
differ across systems. Moreover, human preferences might help researchers align
their TTM systems or improve automatic evaluation metrics, but an open and
renewable source of preferences does not currently exist. We aim to fill these
gaps by offering *live* evaluation for TTM. In Music Arena, real-world users
input text prompts of their choosing and compare outputs from two TTM systems,
and their preferences are used to compile a leaderboard. While Music Arena
follows recent evaluation trends in other AI domains, we also design it with
key features tailored to music: an LLM-based routing system to navigate the
heterogeneous type signatures of TTM systems, and the collection of *detailed*
preferences including listening data and natural language feedback. We also
propose a rolling data release policy with user privacy guarantees, providing a
renewable source of preference data and increasing platform transparency.
Through its standardized evaluation protocol, transparent data access policies,
and music-specific features, Music Arena not only addresses key challenges in
the TTM ecosystem but also demonstrates how live evaluation can be thoughtfully
adapted to unique characteristics of specific AI domains.
  Music Arena is available at: https://music-arena.org

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [355] [AQUA: A Large Language Model for Aquaculture & Fisheries](https://arxiv.org/abs/2507.20520)
*Praneeth Narisetty,Uday Kumar Reddy Kattamanchi,Lohit Akshant Nimma,Sri Ram Kaushik Karnati,Shiva Nagendra Babu Kore,Mounika Golamari,Tejashree Nageshreddy*

Main category: cs.CL

TL;DR: 文章介绍首个针对水产养殖的大语言模型AQUA及AQUADAPT框架，为水产养殖研究等奠定基础。


<details>
  <summary>Details</summary>
Motivation: 水产养殖面临诸多挑战，现有机器学习方法无法满足其领域特定复杂性，需新的解决方案。

Method: 引入大语言模型AQUA，采用AQUADAPT框架结合专家知识、大语言模型和自动评估技术生成和精炼高质量合成数据。

Result: 成功构建AQUA模型及AQUADAPT框架。

Conclusion: 为水产养殖研究、咨询系统和决策工具的大语言模型驱动创新奠定基础。

Abstract: Aquaculture plays a vital role in global food security and coastal economies
by providing sustainable protein sources. As the industry expands to meet
rising demand, it faces growing challenges such as disease outbreaks,
inefficient feeding practices, rising labor costs, logistical inefficiencies,
and critical hatchery issues, including high mortality rates and poor water
quality control. Although artificial intelligence has made significant
progress, existing machine learning methods fall short of addressing the
domain-specific complexities of aquaculture. To bridge this gap, we introduce
AQUA, the first large language model (LLM) tailored for aquaculture, designed
to support farmers, researchers, and industry practitioners. Central to this
effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic
Framework for generating and refining high-quality synthetic data using a
combination of expert knowledge, largescale language models, and automated
evaluation techniques. Our work lays the foundation for LLM-driven innovations
in aquaculture research, advisory systems, and decision-making tools.

</details>


### [356] [Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG](https://arxiv.org/abs/2507.20136)
*Baiyu Chen,Wilson Wongso,Xiaoqian Hu,Yue Tan,Flora Salim*

Main category: cs.CL

TL;DR: 本文介绍CRUISE团队为KDD Cup 2025多模态多轮综合RAG基准挑战赛提出的技术方案，采用多阶段框架减少幻觉，获任务1第3名。


<details>
  <summary>Details</summary>
Motivation: 解决现代视觉语言模型（VLMs）易产生幻觉的问题，特别是在处理以自我为中心的图像、长尾实体和复杂多跳问题时，满足现实应用对高事实准确性的需求。

Method: 提出多阶段框架，集成轻量级查询路由器、查询感知检索和摘要管道、双路径生成和事后验证，优先保证事实准确性和真实性。

Result: 该方法在任务1中获得第3名。

Conclusion: 在复杂多模态RAG系统中优先考虑答案可靠性是有效的。

Abstract: This paper presents the technical solution developed by team CRUISE for the
KDD Cup 2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn
(CRAG-MM) challenge. The challenge aims to address a critical limitation of
modern Vision Language Models (VLMs): their propensity to hallucinate,
especially when faced with egocentric imagery, long-tail entities, and complex,
multi-hop questions. This issue is particularly problematic in real-world
applications where users pose fact-seeking queries that demand high factual
accuracy across diverse modalities. To tackle this, we propose a robust,
multi-stage framework that prioritizes factual accuracy and truthfulness over
completeness. Our solution integrates a lightweight query router for
efficiency, a query-aware retrieval and summarization pipeline, a dual-pathways
generation and a post-hoc verification. This conservative strategy is designed
to minimize hallucinations, which incur a severe penalty in the competition's
scoring metric. Our approach achieved 3rd place in Task 1, demonstrating the
effectiveness of prioritizing answer reliability in complex multi-modal RAG
systems. Our implementation is available at
https://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM .

</details>


### [357] [ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning](https://arxiv.org/abs/2507.20564)
*Duc-Tai Dinh,Duc Anh Khoa Dinh*

Main category: cs.CL

TL;DR: 介绍在EVENTA共享任务中获第4名的ZSE - Cap系统，零样本方法无需微调，检索集成多模型分数，字幕利用提示引导Gemma 3模型，取得良好成绩且代码开源。


<details>
  <summary>Details</summary>
Motivation: 在EVENTA共享任务的文章关联图像检索和字幕生成中取得好成绩，探索零样本方法的有效性。

Method: 检索时集成CLIP、SigLIP和DINOv2的相似度分数；字幕生成时用精心设计的提示引导Gemma 3模型将文章高级事件与图像视觉内容关联。

Result: 系统最终得分0.42002，在私有测试集上获得前4名。

Conclusion: 通过集成和提示组合基础模型是有效的。

Abstract: We present ZSE-Cap (Zero-Shot Ensemble for Captioning), our 4th place system
in Event-Enriched Image Analysis (EVENTA) shared task on article-grounded image
retrieval and captioning. Our zero-shot approach requires no finetuning on the
competition's data. For retrieval, we ensemble similarity scores from CLIP,
SigLIP, and DINOv2. For captioning, we leverage a carefully engineered prompt
to guide the Gemma 3 model, enabling it to link high-level events from the
article to the visual content in the image. Our system achieved a final score
of 0.42002, securing a top-4 position on the private test set, demonstrating
the effectiveness of combining foundation models through ensembling and
prompting. Our code is available at https://github.com/ductai05/ZSE-Cap.

</details>


### [358] [Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning](https://arxiv.org/abs/2507.19586)
*Shengyuan Wang,Jie Feng,Tianhui Liu,Dan Pei,Yong Li*

Main category: cs.CL

TL;DR: 本文提出地理空间幻觉评估框架，用KTO方法缓解大语言模型地理空间幻觉，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在地理空间幻觉影响可靠性，且相关评估和缓解研究不足。

Method: 提出综合评估框架，利用结构化地理空间知识图谱评估；引入基于KTO的动态事实对齐方法缓解幻觉。

Result: 对20个高级大语言模型评估，发现其地理空间知识幻觉，缓解方法使基准测试性能提升超29.6%。

Conclusion: 所提基准和学习算法能增强大语言模型在地理空间知识和推理任务中的可信度。

Abstract: Large language models (LLMs) possess extensive world knowledge, including
geospatial knowledge, which has been successfully applied to various geospatial
tasks such as mobility prediction and social indicator prediction. However,
LLMs often generate inaccurate geospatial knowledge, leading to geospatial
hallucinations (incorrect or inconsistent representations of geospatial
information) that compromise their reliability. While the phenomenon of general
knowledge hallucination in LLMs has been widely studied, the systematic
evaluation and mitigation of geospatial hallucinations remain largely
unexplored. To address this gap, we propose a comprehensive evaluation
framework for geospatial hallucinations, leveraging structured geospatial
knowledge graphs for controlled assessment. Through extensive evaluation across
20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge.
Building on these insights, we introduce a dynamic factuality aligning method
based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial
hallucinations in LLMs, leading to a performance improvement of over 29.6% on
the proposed benchmark. Extensive experimental results demonstrate the
effectiveness of our benchmark and learning algorithm in enhancing the
trustworthiness of LLMs in geospatial knowledge and reasoning tasks.

</details>


### [359] [Efficient Attention Mechanisms for Large Language Models: A Survey](https://arxiv.org/abs/2507.19595)
*Yutao Sun,Zhenyu Li,Yike Zhang,Tengyu Pan,Bowen Dong,Yuyi Guo,Jianyong Wang*

Main category: cs.CL

TL;DR: 文章指出Transformer自注意力机制复杂度问题，介绍线性和稀疏注意力机制，对相关进展进行综述，分析其在大模型中的应用，为高效语言模型设计提供参考。


<details>
  <summary>Details</summary>
Motivation: Transformer自注意力机制的二次时间和内存复杂度阻碍高效长上下文建模，需解决该限制。

Method: 对线性和稀疏注意力机制的算法创新及硬件层面考虑进行系统全面综述，分析其在大规模预训练语言模型中的应用。

Result: 对高效注意力机制相关进展进行系统梳理，分析了其在大模型中的不同架构设计。

Conclusion: 该工作可作为推进可扩展高效语言模型设计的基础参考。

Abstract: Transformer-based architectures have become the prevailing backbone of large
language models. However, the quadratic time and memory complexity of
self-attention remains a fundamental obstacle to efficient long-context
modeling. To address this limitation, recent research has introduced two
principal categories of efficient attention mechanisms. Linear attention
methods achieve linear complexity through kernel approximations, recurrent
formulations, or fastweight dynamics, thereby enabling scalable inference with
reduced computational overhead. Sparse attention techniques, in contrast, limit
attention computation to selected subsets of tokens based on fixed patterns,
block-wise routing, or clustering strategies, enhancing efficiency while
preserving contextual coverage. This survey provides a systematic and
comprehensive overview of these developments, integrating both algorithmic
innovations and hardware-level considerations. In addition, we analyze the
incorporation of efficient attention into largescale pre-trained language
models, including both architectures built entirely on efficient attention and
hybrid designs that combine local and global components. By aligning
theoretical foundations with practical deployment strategies, this work aims to
serve as a foundational reference for advancing the design of scalable and
efficient language models.

</details>


### [360] [MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?](https://arxiv.org/abs/2507.19598)
*Muntasir Wahed,Xiaona Zhou,Kiet A. Nguyen,Tianjiao Yu,Nirav Diwan,Gang Wang,Dilek Hakkani-Tür,Ismini Lourentzou*

Main category: cs.CL

TL;DR: 本文指出大语言模型代码生成能力虽提升，但对抗恶意滥用的鲁棒性待研究，提出代码分解攻击，引入基准评估，结果显示模型有漏洞，微调可提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对抗恶意滥用尤其是多轮恶意编码提示的鲁棒性研究不足。

Method: 提出代码分解攻击，引入大规模基准评估代码大语言模型对单轮和多轮恶意提示的鲁棒性，并在MOCHA上进行微调。

Result: 开源和闭源模型都存在漏洞，尤其是多轮场景；在MOCHA上微调可提升拒绝率，提升外部对抗数据集鲁棒性，拒绝率最高提升32.4%。

Conclusion: 大语言模型在对抗恶意编码提示方面存在不足，在MOCHA上微调可有效增强其鲁棒性。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
enhanced their code generation capabilities. However, their robustness against
adversarial misuse, particularly through multi-turn malicious coding prompts,
remains underexplored. In this work, we introduce code decomposition attacks,
where a malicious coding task is broken down into a series of seemingly benign
subtasks across multiple conversational turns to evade safety filters. To
facilitate systematic evaluation, we introduce \benchmarkname{}, a large-scale
benchmark designed to evaluate the robustness of code LLMs against both
single-turn and multi-turn malicious prompts. Empirical results across open-
and closed-source models reveal persistent vulnerabilities, especially under
multi-turn scenarios. Fine-tuning on MOCHA improves rejection rates while
preserving coding ability, and importantly, enhances robustness on external
adversarial datasets with up to 32.4% increase in rejection rates without any
additional supervision.

</details>


### [361] [MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks](https://arxiv.org/abs/2507.19634)
*Sara Papi,Maike Züfle,Marco Gaido,Beatrice Savoldi,Danni Liu,Ioannis Douros,Luisa Bentivogli,Jan Niehues*

Main category: cs.CL

TL;DR: 现有基准难以全面评估多模态大语言模型多语言和多模态能力，本文推出MCIF基准用于综合评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准在联合评估多语言和多模态能力方面存在不足，无法全面评估模型性能。

Method: 引入MCIF基准，该基准基于科学讲座，涵盖语音、视觉和文本三种模态及四种语言，用于跨语言多模态指令跟随评估。

Result: 推出MCIF基准并以CC - BY 4.0许可发布，可全面评估模型跨语言解读指令并结合多模态信息的能力。

Conclusion: MCIF基准能促进多模态大语言模型的开放研究和发展。

Abstract: Recent advances in large language models have catalyzed the development of
multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified
frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to
general-purpose instruction-following models, a key frontier lies in evaluating
their multilingual and multimodal capabilities over both long and short
contexts. However, existing benchmarks fall short in evaluating these
dimensions jointly: they are often limited to English, mostly focus on one
single modality at a time, rely on short-form contexts, or lack human
annotations -- hindering comprehensive assessment of model performance across
languages, modalities, and task complexity. To address these gaps, we introduce
MCIF (Multimodal Crosslingual Instruction Following), the first multilingual
human-annotated benchmark based on scientific talks that is designed to
evaluate instruction-following in crosslingual, multimodal settings over both
short- and long-form inputs. MCIF spans three core modalities -- speech,
vision, and text -- and four diverse languages (English, German, Italian, and
Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret
instructions across languages and combine them with multimodal contextual
information. MCIF is released under a CC-BY 4.0 license to encourage open
research and progress in MLLMs development.

</details>


### [362] [UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities](https://arxiv.org/abs/2507.19766)
*Dong Du,Shulin Liu,Tao Yang,Shaohua Chen,Yang Li*

Main category: cs.CL

TL;DR: 本文提出UloRL方法解决传统RL框架处理超长输出的问题，实验证明其能提升LLM推理能力和训练速度，还将开源代码和模型。


<details>
  <summary>Details</summary>
Motivation: 传统RL框架处理超长输出时因长尾序列分布和熵坍塌导致效率低下，需要提升大语言模型推理能力。

Method: 将超长输出解码分割成短片段，引入掌握良好的正标记（MPTs）的动态掩码。

Result: 在Qwen3 - 30B - A3B模型上，分段滚动的RL训练速度提高2.06倍；128k标记输出的RL训练提升了模型在AIME2025和BeyondAIME上的性能，超越Qwen3 - 235B - A22B。

Conclusion: 所提方法有潜力提升具有超长序列生成能力的大语言模型的推理能力。

Abstract: Recent advances in large language models (LLMs) have highlighted the
potential of reinforcement learning with verifiable rewards (RLVR) to enhance
reasoning capabilities through extended output sequences. However, traditional
RL frameworks face inefficiencies when handling ultra-long outputs due to
long-tail sequence distributions and entropy collapse during training. To
address these challenges, we propose an Ultra-Long Output Reinforcement
Learning (UloRL) approach for advancing large language models' reasoning
abilities. Specifically, we divide ultra long output decoding into short
segments, enabling efficient training by mitigating delays caused by long-tail
samples. Additionally, we introduce dynamic masking of well-Mastered Positive
Tokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the
effectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment
rollout achieved 2.06x increase in training speed, while RL training with
128k-token outputs improves the model's performance on AIME2025 from 70.9\% to
85.1\% and on BeyondAIME from 50.7\% to 61.9\%, even surpassing Qwen3-235B-A22B
with remarkable gains. These findings underscore the potential of our methods
to advance the reasoning capabilities of LLMs with ultra-long sequence
generation. We will release our code and model for further use by the
community.

</details>


### [363] [HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs](https://arxiv.org/abs/2507.19823)
*Dongquan Yang,Yifan Yang,Xiaotian Yu,Xianbiao Qi,Rong Xiao*

Main category: cs.CL

TL;DR: 提出HCAttention框架用于高效推理，缩小KV缓存内存占用并保持准确性，在LLM KV缓存压缩上达新SOTA，还能让Llama - 3 - 8B在单GPU处理400万令牌。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法在内存大幅减少时性能下降，GPU - CPU协作的近似注意力策略探索不足，需解决大语言模型处理长上下文输入时KV缓存内存需求大的问题。

Method: 提出HCAttention异构注意力计算框架，集成关键量化、值卸载和动态KV逐出，且与现有Transformer架构兼容，无需模型微调。

Result: 在LongBench基准测试中，能将KV缓存内存占用缩小到原大小的25%，仅用12.5%的缓存仍具竞争力；首次让Llama - 3 - 8B模型在单A100 GPU上处理400万令牌。

Conclusion: HCAttention在极端内存约束下能实现高效推理，在LLM KV缓存压缩方面达到新的最优水平。

Abstract: Processing long-context inputs with large language models presents a
significant challenge due to the enormous memory requirements of the Key-Value
(KV) cache during inference. Existing KV cache compression methods exhibit
noticeable performance degradation when memory is reduced by more than 85%.
Additionally, strategies that leverage GPU-CPU collaboration for approximate
attention remain underexplored in this setting. We propose HCAttention, a
heterogeneous attention computation framework that integrates key quantization,
value offloading, and dynamic KV eviction to enable efficient inference under
extreme memory constraints. The method is compatible with existing transformer
architectures and does not require model fine-tuning. Experimental results on
the LongBench benchmark demonstrate that our approach preserves the accuracy of
full-attention model while shrinking the KV cache memory footprint to 25% of
its original size. Remarkably, it stays competitive with only 12.5% of the
cache, setting a new state-of-the-art in LLM KV cache compression. To the best
of our knowledge, HCAttention is the first to extend the Llama-3-8B model to
process 4 million tokens on a single A100 GPU with 80GB memory.

</details>


### [364] [VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset for Legal Question Answering](https://arxiv.org/abs/2507.19995)
*Tan-Minh Nguyen,Hoang-Trung Nguyen,Trong-Khoi Dao,Xuan-Hieu Phan,Ha-Thanh Nguyen,Thi-Hai-Yen Vuong*

Main category: cs.CL

TL;DR: 本文介绍了越南法律领域的VLQA数据集，并对其进行统计分析和实验评估。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在法律文本处理中能力被高估，法律系统因国家和语言而异，低资源语言（如越南语）的法律NLP面临资源和标注数据稀缺问题，需要构建针对不同自然语言的法律文本处理应用。

Method: 引入针对越南法律领域的VLQA数据集，对数据集进行全面统计分析，并使用最先进的模型在法律信息检索和问答任务上进行实验。

Result: 文中未明确提及具体实验结果。

Conclusion: 未明确给出，但推测引入的VLQA数据集能为越南法律领域的文本处理提供帮助。

Abstract: The advent of large language models (LLMs) has led to significant
achievements in various domains, including legal text processing. Leveraging
LLMs for legal tasks is a natural evolution and an increasingly compelling
choice. However, their capabilities are often portrayed as greater than they
truly are. Despite the progress, we are still far from the ultimate goal of
fully automating legal tasks using artificial intelligence (AI) and natural
language processing (NLP). Moreover, legal systems are deeply domain-specific
and exhibit substantial variation across different countries and languages. The
need for building legal text processing applications for different natural
languages is, therefore, large and urgent. However, there is a big challenge
for legal NLP in low-resource languages such as Vietnamese due to the scarcity
of resources and annotated data. The need for labeled legal corpora for
supervised training, validation, and supervised fine-tuning is critical. In
this paper, we introduce the VLQA dataset, a comprehensive and high-quality
resource tailored for the Vietnamese legal domain. We also conduct a
comprehensive statistical analysis of the dataset and evaluate its
effectiveness through experiments with state-of-the-art models on legal
information retrieval and question-answering tasks.

</details>


### [365] [Anomaly Detection in Human Language via Meta-Learning: A Few-Shot Approach](https://arxiv.org/abs/2507.20019)
*Saurav Singla,Aarav Singla,Advik Gupta,Parnika Gupta*

Main category: cs.CL

TL;DR: 提出元学习框架用于在有限标签数据下跨领域检测语言异常，方法结合多种技术，表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 语言异常（如垃圾信息、假新闻、仇恨言论）因稀疏性和可变性带来检测挑战，且标签数据有限。

Method: 将异常检测视为少样本二分类问题，利用元学习，结合情景训练、原型网络和领域重采样。

Result: 在F1和AUC分数上优于强基线。

Conclusion: 所提方法有效，发布代码和基准以推动少样本文本异常检测研究。

Abstract: We propose a meta learning framework for detecting anomalies in human
language across diverse domains with limited labeled data. Anomalies in
language ranging from spam and fake news to hate speech pose a major challenge
due to their sparsity and variability. We treat anomaly detection as a few shot
binary classification problem and leverage meta-learning to train models that
generalize across tasks. Using datasets from domains such as SMS spam, COVID-19
fake news, and hate speech, we evaluate model generalization on unseen tasks
with minimal labeled anomalies. Our method combines episodic training with
prototypical networks and domain resampling to adapt quickly to new anomaly
detection tasks. Empirical results show that our method outperforms strong
baselines in F1 and AUC scores. We also release the code and benchmarks to
facilitate further research in few-shot text anomaly detection.

</details>


### [366] [Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media](https://arxiv.org/abs/2507.19511)
*Khalid Hasan,Jamil Saquer,Mukulika Ghosh*

Main category: cs.CL

TL;DR: 研究评估多种NLP模型用于心理健康障碍分类，Transformer模型表现优，RoBERTa最佳，LSTM结合BERT嵌入有竞争力。


<details>
  <summary>Details</summary>
Motivation: 心理健康障碍增多，需开发自动化早期检测和监测工具，NLP有潜力。

Method: 评估多种Transformer模型和LSTM方法，构建注释数据集并验证可靠性。

Result: Transformer模型表现优于传统深度学习方法，RoBERTa F1分数最高，LSTM结合BERT嵌入有竞争力且计算资源需求少。

Conclusion: Transformer模型适用于实时可扩展心理健康监测，讨论了临床应用和NLP方法的能力与局限。

Abstract: The rising prevalence of mental health disorders necessitates the development
of robust, automated tools for early detection and monitoring. Recent advances
in Natural Language Processing (NLP), particularly transformer-based
architectures, have demonstrated significant potential in text analysis. This
study provides a comprehensive evaluation of state-of-the-art transformer
models (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term
Memory (LSTM) based approaches using different text embedding techniques for
mental health disorder classification on Reddit. We construct a large annotated
dataset, validating its reliability through statistical judgmental analysis and
topic modeling. Experimental results demonstrate the superior performance of
transformer models over traditional deep-learning approaches. RoBERTa achieved
the highest classification performance, with a 99.54% F1 score on the hold-out
test set and a 96.05% F1 score on the external test set. Notably, LSTM models
augmented with BERT embeddings proved highly competitive, achieving F1 scores
exceeding 94% on the external dataset while requiring significantly fewer
computational resources. These findings highlight the effectiveness of
transformer-based models for real-time, scalable mental health monitoring. We
discuss the implications for clinical applications and digital mental health
interventions, offering insights into the capabilities and limitations of
state-of-the-art NLP methodologies in mental disorder detection.

</details>


### [367] [Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables](https://arxiv.org/abs/2507.19521)
*Vishakh Padmakumar,Joseph Chee Chang,Kyle Lo,Doug Downey,Aakanksha Naik*

Main category: cs.CL

TL;DR: 论文解决学术文献模式生成进度慢的问题，提出增广数据集方法和模式编辑技术，提升模式生成性能。


<details>
  <summary>Details</summary>
Motivation: 学术文献量增加需比较文档，现有模式生成因评估模糊和缺乏编辑方法进展缓慢。

Method: 提出用合成意图增广无注释表语料的方法创建数据集；提出基于大语言模型的模式编辑技术；对单样本模式生成方法进行基准测试。

Result: 利用数据集融入表意图显著提升重建参考模式的基线性能；较小的开放权重模型微调后可与最先进的提示大语言模型竞争；编辑技术可进一步改进模式。

Conclusion: 所提方法有效解决了模式生成中的评估模糊和缺乏编辑方法的问题，提升了模式生成性能。

Abstract: The increasing volume of academic literature makes it essential for
researchers to organize, compare, and contrast collections of documents. Large
language models (LLMs) can support this process by generating schemas defining
shared aspects along which to compare papers. However, progress on schema
generation has been slow due to: (i) ambiguity in reference-based evaluations,
and (ii) lack of editing/refinement methods. Our work is the first to address
both issues. First, we present an approach for augmenting unannotated table
corpora with synthesized intents and apply it to create a dataset for studying
schema generation conditioned on a given information need, thus reducing
ambiguity. With this dataset, we show how incorporating table intents
significantly improves baseline performance in reconstructing reference
schemas. Next, we propose several LLM-based schema editing techniques. We start
by comprehensively benchmarking several single-shot schema generation methods,
including prompted LLM workflows and fine-tuned models, showing that smaller,
open-weight models can be fine-tuned to be competitive with state-of-the-art
prompted LLMs. Then we demonstrate that our editing techniques can further
improve schemas generated by these methods.

</details>


### [368] [RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation](https://arxiv.org/abs/2507.20059)
*Ran Xu,Yuchen Zhuang,Yue Yu,Haoyu Wang,Wenqi Shi,Carl Yang*

Main category: cs.CL

TL;DR: 评估RAG系统，发现其局限性，强调现实应用需自适应检索策略。


<details>
  <summary>Details</summary>
Motivation: RAG在通用基准表现好，但在现实多样检索场景效果待探索。

Method: 使用大规模混合知识数据存储MassiveDS评估RAG系统。

Result: 检索主要利于小模型，重排器价值小，无单一检索源表现始终出色，当前大模型难跨异构知识源路由查询。

Conclusion: 在现实部署RAG前需自适应检索策略。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
integrating external knowledge retrieved at inference time. While RAG
demonstrates strong performance on benchmarks largely derived from
general-domain corpora like Wikipedia, its effectiveness under realistic,
diverse retrieval scenarios remains underexplored. We evaluated RAG systems
using MassiveDS, a large-scale datastore with mixture of knowledge, and
identified critical limitations: retrieval mainly benefits smaller models,
rerankers add minimal value, and no single retrieval source consistently
excels. Moreover, current LLMs struggle to route queries across heterogeneous
knowledge sources. These findings highlight the need for adaptive retrieval
strategies before deploying RAG in real-world settings. Our code and data can
be found at https://github.com/ritaranx/RAG_in_the_Wild.

</details>


### [369] [AI-Driven Generation of Old English: A Framework for Low-Resource Languages](https://arxiv.org/abs/2507.20111)
*Rodrigo Gabriel Salazar Alva,Matías Nuñez,Cristian López,Javier Martín Arista*

Main category: cs.CL

TL;DR: 提出可扩展框架用大语言模型生成古英语文本，评估显示效果好，还可为濒危语言复兴提供方案。


<details>
  <summary>Details</summary>
Motivation: 保护古英语对于理解人类文化和语言遗产至关重要，但古英语资源匮乏，限制了现代NLP技术的应用。

Method: 结合参数高效微调（LoRA）、反向翻译的数据增强方法以及分离内容生成和翻译任务的双代理管道。

Result: 自动化指标评估显示比基线模型有显著改进，BLEU分数从26提升到65以上，专家人工评估确认生成文本语法准确性和风格保真度高。

Conclusion: 该方法不仅能扩展古英语语料库，还为复兴其他濒危语言提供实用蓝图，将AI创新与文化保护目标相结合。

Abstract: Preserving ancient languages is essential for understanding humanity's
cultural and linguistic heritage, yet Old English remains critically
under-resourced, limiting its accessibility to modern natural language
processing (NLP) techniques. We present a scalable framework that uses advanced
large language models (LLMs) to generate high-quality Old English texts,
addressing this gap. Our approach combines parameter-efficient fine-tuning
(Low-Rank Adaptation, LoRA), data augmentation via backtranslation, and a
dual-agent pipeline that separates the tasks of content generation (in English)
and translation (into Old English). Evaluation with automated metrics (BLEU,
METEOR, and CHRF) shows significant improvements over baseline models, with
BLEU scores increasing from 26 to over 65 for English-to-Old English
translation. Expert human assessment also confirms high grammatical accuracy
and stylistic fidelity in the generated texts. Beyond expanding the Old English
corpus, our method offers a practical blueprint for revitalizing other
endangered languages, effectively uniting AI innovation with the goals of
cultural preservation.

</details>


### [370] [Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering](https://arxiv.org/abs/2507.20133)
*Anas Mohamed,Azal Ahmad Khan,Xinran Wang,Ahmad Faraz Khan,Shuwen Ge,Saman Bahzad Khan,Ayaan Ahmad,Ali Anwar*

Main category: cs.CL

TL;DR: 提出Sem - DPO改进DPO，在多个基准测试中表现优于DPO和现有基线，为提示优化研究提供新标准。


<details>
  <summary>Details</summary>
Motivation: DPO存在语义不一致问题，即高偏好得分的提示可能偏离用户原意。

Method: 引入Sem - DPO，通过指数权重缩放DPO损失，权重与原提示和候选提示在嵌入空间的余弦距离成正比。

Result: 在三个文本到图像提示优化基准和两个语言模型上，Sem - DPO的CLIP相似度高8 - 12%，人类偏好得分高5 - 9%，优于现有基线。

Conclusion: 带有语义加权的强基线应成为提示优化研究的新标准，为语言模型语义感知偏好优化奠定基础。

Abstract: Generative AI can now synthesize strikingly realistic images from text, yet
output quality remains highly sensitive to how prompts are phrased. Direct
Preference Optimization (DPO) offers a lightweight, off-policy alternative to
RL for automatic prompt engineering, but its token-level regularization leaves
semantic inconsistency unchecked as prompts that win higher preference scores
can still drift away from the user's intended meaning.
  We introduce Sem-DPO, a variant of DPO that preserves semantic consistency
yet retains its simplicity and efficiency. Sem-DPO scales the DPO loss by an
exponential weight proportional to the cosine distance between the original
prompt and winning candidate in embedding space, softly down-weighting training
signals that would otherwise reward semantically mismatched prompts. We provide
the first analytical bound on semantic drift for preference-tuned prompt
generators, showing that Sem-DPO keeps learned prompts within a provably
bounded neighborhood of the original text. On three standard text-to-image
prompt-optimization benchmarks and two language models, Sem-DPO achieves 8-12%
higher CLIP similarity and 5-9% higher human-preference scores (HPSv2.1,
PickScore) than DPO, while also outperforming state-of-the-art baselines. These
findings suggest that strong flat baselines augmented with semantic weighting
should become the new standard for prompt-optimization studies and lay the
groundwork for broader, semantics-aware preference optimization in language
models.

</details>


### [371] [Multi-Agent Interactive Question Generation Framework for Long Document Understanding](https://arxiv.org/abs/2507.20145)
*Kesen Wang,Daulet Toibazar,Abdulrahman Alfulayt,Abdulaziz S. Albadawi,Ranya A. Alkahtani,Asma A. Ibrahim,Haneen A. Alhomoud,Sherif Mohamed,Pedro J. Moreno*

Main category: cs.CL

TL;DR: 提出全自动多智能体交互框架生成长期上下文问题，助力提升大视觉语言模型长上下文理解能力，实验表明生成问题有挑战性。


<details>
  <summary>Details</summary>
Motivation: 长上下文场景文档理解是挑战，大视觉语言模型在长上下文表现不佳，缺乏细粒度训练数据，现有技术依赖人工标注成本高。

Method: 提出全自动多智能体交互框架，高效生成单页和多页问题。

Result: 生成的英语和阿拉伯语问题对主流开源和闭源大视觉语言模型具有挑战性。

Conclusion: 该框架有助于开发具有增强长上下文理解能力的大视觉语言模型，代码和数据已公开。

Abstract: Document Understanding (DU) in long-contextual scenarios with complex layouts
remains a significant challenge in vision-language research. Although Large
Vision-Language Models (LVLMs) excel at short-context DU tasks, their
performance declines in long-context settings. A key limitation is the scarcity
of fine-grained training data, particularly for low-resource languages such as
Arabic. Existing state-of-the-art techniques rely heavily on human annotation,
which is costly and inefficient. We propose a fully automated, multi-agent
interactive framework to generate long-context questions efficiently. Our
approach efficiently generates high-quality single- and multi-page questions
for extensive English and Arabic documents, covering hundreds of pages across
diverse domains. This facilitates the development of LVLMs with enhanced
long-context understanding ability. Experimental results in this work have
shown that our generated English and Arabic questions
(\textbf{AraEngLongBench}) are quite challenging to major open- and
close-source LVLMs. The code and data proposed in this work can be found in
https://github.com/wangk0b/Multi_Agentic_QA_Long_Doc.git. Sample Question and
Answer (QA) pairs and structured system prompts can be found in the Appendix.

</details>


### [372] [Goal Alignment in LLM-Based User Simulators for Conversational AI](https://arxiv.org/abs/2507.20152)
*Shuhaib Mehri,Xiaocheng Yang,Takyoung Kim,Gokhan Tur,Shikib Mehri,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 论文指出当前大语言模型在用户模拟中难以在多轮对话中保持目标导向行为，提出用户目标状态跟踪（UGST）框架及三阶段方法开发用户模拟器，建立评估指标，在两个基准测试中取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在用户模拟中难以在多轮对话中持续展现目标导向行为，影响下游应用可靠性。

Method: 引入UGST框架跟踪对话中用户目标进展，提出三阶段方法开发能自主跟踪目标进展并生成符合目标响应的用户模拟器，建立评估目标一致性的指标。

Result: 在MultiWOZ 2.4和{	au}-Bench两个基准测试中取得显著改进。

Conclusion: UGST框架解决了对话式AI中的关键缺口，是开发目标一致用户模拟器的重要框架。

Abstract: User simulators are essential to conversational AI, enabling scalable agent
development and evaluation through simulated interactions. While current Large
Language Models (LLMs) have advanced user simulation capabilities, we reveal
that they struggle to consistently demonstrate goal-oriented behavior across
multi-turn conversations--a critical limitation that compromises their
reliability in downstream applications. We introduce User Goal State Tracking
(UGST), a novel framework that tracks user goal progression throughout
conversations. Leveraging UGST, we present a three-stage methodology for
developing user simulators that can autonomously track goal progression and
reason to generate goal-aligned responses. Moreover, we establish comprehensive
evaluation metrics for measuring goal alignment in user simulators, and
demonstrate that our approach yields substantial improvements across two
benchmarks (MultiWOZ 2.4 and {\tau}-Bench). Our contributions address a
critical gap in conversational AI and establish UGST as an essential framework
for developing goal-aligned user simulators.

</details>


### [373] [SGPO: Self-Generated Preference Optimization based on Self-Improver](https://arxiv.org/abs/2507.20181)
*Hyeonji Lee,Daejin Jo,Seohwan Yun,Sungwoong Kim*

Main category: cs.CL

TL;DR: 提出基于Self - Improver的Self - Generated Preference Optimization (SGPO)对齐框架，不依赖外部偏好数据提升大语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型对齐方法依赖人类标注数据集，有适用性局限和分布偏移问题。

Method: 提出SGPO框架，利用单模型的策略改进机制，改进器参考监督微调输出优化策略模型响应，自生成偏好数据进行直接偏好优化。

Result: 在AlpacaEval 2.0和Arena - Hard实验中，SGPO比DPO和基线自改进方法性能显著提升。

Conclusion: SGPO不使用外部偏好数据，可有效提升大语言模型性能。

Abstract: Large language models (LLMs), despite their extensive pretraining on diverse
datasets, require effective alignment to human preferences for practical and
reliable deployment. Conventional alignment methods typically employ off-policy
learning and depend on human-annotated datasets, which limits their broad
applicability and introduces distribution shift issues during training. To
address these challenges, we propose Self-Generated Preference Optimization
based on Self-Improver (SGPO), an innovative alignment framework that leverages
an on-policy self-improving mechanism. Specifically, the improver refines
responses from a policy model to self-generate preference data for direct
preference optimization (DPO) of the policy model. Here, the improver and
policy are unified into a single model, and in order to generate higher-quality
preference data, this self-improver learns to make incremental yet discernible
improvements to the current responses by referencing supervised fine-tuning
outputs. Experimental results on AlpacaEval 2.0 and Arena-Hard show that the
proposed SGPO significantly improves performance over DPO and baseline
self-improving methods without using external preference data.

</details>


### [374] [Post-Completion Learning for Language Models](https://arxiv.org/abs/2507.20252)
*Xiang Fei,Siqi Wang,Shu Wei,Yuxiang Nie,Wei Shi,Hao Feng,Can Huang*

Main category: cs.CL

TL;DR: 提出PCL训练框架利用模型输出完成后的序列空间，提升推理和自我评估能力，实验显示效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型训练范式忽略完成输出后的学习机会，旨在利用该空间提升模型能力。

Method: 设计白盒强化学习方法，让模型按奖励规则评估输出，实施双轨SFT优化能力，并与RL训练混合进行多目标优化。

Result: 在不同数据集和模型上实验，效果优于传统SFT和RL方法。

Conclusion: 该方法为语言模型训练提供新路径，可提升输出质量并保持部署效率。

Abstract: Current language model training paradigms typically terminate learning upon
reaching the end-of-sequence (<eos>}) token, overlooking the potential learning
opportunities in the post-completion space. We propose Post-Completion Learning
(PCL), a novel training framework that systematically utilizes the sequence
space after model output completion, to enhance both the reasoning and
self-evaluation abilities. PCL enables models to continue generating
self-assessments and reward predictions during training, while maintaining
efficient inference by stopping at the completion point.
  To fully utilize this post-completion space, we design a white-box
reinforcement learning method: let the model evaluate the output content
according to the reward rules, then calculate and align the score with the
reward functions for supervision. We implement dual-track SFT to optimize both
reasoning and evaluation capabilities, and mixed it with RL training to achieve
multi-objective hybrid optimization.
  Experimental results on different datasets and models demonstrate consistent
improvements over traditional SFT and RL methods. Our method provides a new
technical path for language model training that enhances output quality while
preserving deployment efficiency.

</details>


### [375] [Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations](https://arxiv.org/abs/2507.20409)
*Eunkyu Park,Wesley Hanwen Deng,Gunhee Kim,Motahhare Eslami,Maarten Sap*

Main category: cs.CL

TL;DR: 引入Cognitive Chain-of-Thought (CoCoT)提示策略，实验显示其在多模态基准测试中优于CoT和直接提示，能增强VLM可解释性和社会意识。


<details>
  <summary>Details</summary>
Motivation: 在社会语境的视觉任务中，普通的Chain-of-Thought (CoT)提示常失效，需更好的提示策略。

Method: 引入CoCoT提示策略，通过感知、情境和规范三个认知启发阶段来支撑VLM推理。

Result: 在多个多模态基准测试中，CoCoT平均比CoT和直接提示表现高8%。

Conclusion: 基于认知的推理阶段能增强VLM的可解释性和社会意识，为更安全可靠的多模态系统铺平道路。

Abstract: Chain-of-Thought (CoT) prompting helps models think step by step. But what
happens when they must see, understand, and judge-all at once? In visual tasks
grounded in social context, where bridging perception with norm-grounded
judgments is essential, flat CoT often breaks down. We introduce Cognitive
Chain-of-Thought (CoCoT), a prompting strategy that scaffolds VLM reasoning
through three cognitively inspired stages: perception, situation, and norm. Our
experiments show that, across multiple multimodal benchmarks (including intent
disambiguation, commonsense reasoning, and safety), CoCoT consistently
outperforms CoT and direct prompting (+8\% on average). Our findings
demonstrate that cognitively grounded reasoning stages enhance interpretability
and social awareness in VLMs, paving the way for safer and more reliable
multimodal systems.

</details>


### [376] [Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?](https://arxiv.org/abs/2507.20419)
*Khloud AL Jallad,Nada Ghneim,Ghaida Rebdawi*

Main category: cs.CL

TL;DR: 本文对英语、阿拉伯语和多语言自然语言理解基准进行全面综述，分析其优势与局限，指出缺乏评估标准问题并开展相关研究以助力构建语言现象层级体系。


<details>
  <summary>Details</summary>
Motivation: 自然语言理解能力评估成研究热点且有众多基准出现，需全面评估并分析现有基准的优势和局限，解决缺乏评估标准问题。

Method: 对现有英语、阿拉伯语和多语言自然语言理解基准进行详细比较和分析，深入研究其诊断数据集和涵盖的语言现象。

Result: 发现目前在宏观和微观类别命名、语言现象覆盖方面缺乏统一标准，可开展深入分析和比较来支持构建语言现象层级体系。

Conclusion: 制定诊断评估的评估指标对比较不同诊断基准上模型结果有价值。

Abstract: Natural Language Understanding (NLU) is a basic task in Natural Language
Processing (NLP). The evaluation of NLU capabilities has become a trending
research topic that attracts researchers in the last few years, resulting in
the development of numerous benchmarks. These benchmarks include various tasks
and datasets in order to evaluate the results of pretrained models via public
leaderboards. Notably, several benchmarks contain diagnostics datasets designed
for investigation and fine-grained error analysis across a wide range of
linguistic phenomena. This survey provides a comprehensive review of available
English, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on
their diagnostics datasets and the linguistic phenomena they covered. We
present a detailed comparison and analysis of these benchmarks, highlighting
their strengths and limitations in evaluating NLU tasks and providing in-depth
error analysis. When highlighting the gaps in the state-of-the-art, we noted
that there is no naming convention for macro and micro categories or even a
standard set of linguistic phenomena that should be covered. Consequently, we
formulated a research question regarding the evaluation metrics of the
evaluation diagnostics benchmarks: "Why do not we have an evaluation standard
for the NLU evaluation diagnostics benchmarks?" similar to ISO standard in
industry. We conducted a deep analysis and comparisons of the covered
linguistic phenomena in order to support experts in building a global hierarchy
for linguistic phenomena in future. We think that having evaluation metrics for
diagnostics evaluation could be valuable to gain more insights when comparing
the results of the studied models on different diagnostics benchmarks.

</details>


### [377] [CodeNER: Code Prompting for Named Entity Recognition](https://arxiv.org/abs/2507.20423)
*Sungwoo Han,Hyeyeon Kim,Jingun Kwon,Hidetaka Kamigaito,Manabu Okumura*

Main category: cs.CL

TL;DR: 提出基于代码提示法提升大语言模型在命名实体识别（NER）中的能力，实验显示该方法优于传统文本提示法，结合思维链提示性能更佳。


<details>
  <summary>Details</summary>
Motivation: 以往NER方法仅依赖输入上下文信息，而NER需结合输入上下文捕获详细标注要求，因此要改进大语言模型理解和执行NER的能力。

Method: 提出基于代码提示法，在提示中嵌入代码，提供详细BIO模式标注说明。

Result: 在英语、阿拉伯语等多种语言的十个基准测试中，基于代码提示法优于传统文本提示法；结合思维链提示性能进一步提升。

Conclusion: 明确构建NER指令有效，基于代码提示法可提升大语言模型在NER中的表现。

Abstract: Recent studies have explored various approaches for treating candidate named
entity spans as both source and target sequences in named entity recognition
(NER) by leveraging large language models (LLMs). Although previous approaches
have successfully generated candidate named entity spans with suitable labels,
they rely solely on input context information when using LLMs, particularly,
ChatGPT. However, NER inherently requires capturing detailed labeling
requirements with input context information. To address this issue, we propose
a novel method that leverages code-based prompting to improve the capabilities
of LLMs in understanding and performing NER. By embedding code within prompts,
we provide detailed BIO schema instructions for labeling, thereby exploiting
the ability of LLMs to comprehend long-range scopes in programming languages.
Experimental results demonstrate that the proposed code-based prompting method
outperforms conventional text-based prompting on ten benchmarks across English,
Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of
explicitly structuring NER instructions. We also verify that combining the
proposed code-based prompting method with the chain-of-thought prompting
further improves performance.

</details>


### [378] [Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems](https://arxiv.org/abs/2507.20491)
*Tuan Bui,Trong Le,Phat Thai,Sang Nguyen,Minh Hua,Ngan Pham,Thang Bui,Tho Quan*

Main category: cs.CL

TL;DR: 提出轻量级框架Text - JEPA将自然语言转换为一阶逻辑，用综合评估框架评测，在特定领域数据集上表现好，计算开销低。


<details>
  <summary>Details</summary>
Motivation: 现有方法在封闭领域问答中依赖大模型，将自然语言转换为形式逻辑表示效率低，用户需要透明推理和可解释决策过程。

Method: 引入Text - JEPA框架，借鉴双系统认知理论，结合高效生成逻辑表示和Z3求解器进行逻辑推理；提出含三个自定义指标的综合评估框架。

Result: Text - JEPA在特定领域数据集上取得有竞争力的表现，且计算开销显著低于基于大语言模型的系统。

Conclusion: 结构化、可解释推理框架在专业领域构建高效可解释问答系统有潜力。

Abstract: Recent advances in large language models (LLMs) have significantly enhanced
question-answering (QA) capabilities, particularly in open-domain contexts.
However, in closed-domain scenarios such as education, healthcare, and law,
users demand not only accurate answers but also transparent reasoning and
explainable decision-making processes. While neural-symbolic (NeSy) frameworks
have emerged as a promising solution, leveraging LLMs for natural language
understanding and symbolic systems for formal reasoning, existing approaches
often rely on large-scale models and exhibit inefficiencies in translating
natural language into formal logic representations.
  To address these limitations, we introduce Text-JEPA (Text-based
Joint-Embedding Predictive Architecture), a lightweight yet effective framework
for converting natural language into first-order logic (NL2FOL). Drawing
inspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by
efficiently generating logic representations, while the Z3 solver operates as
System 2, enabling robust logical inference. To rigorously evaluate the
NL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework
comprising three custom metrics: conversion score, reasoning score, and
Spearman rho score, which collectively capture the quality of logical
translation and its downstream impact on reasoning accuracy.
  Empirical results on domain-specific datasets demonstrate that Text-JEPA
achieves competitive performance with significantly lower computational
overhead compared to larger LLM-based systems. Our findings highlight the
potential of structured, interpretable reasoning frameworks for building
efficient and explainable QA systems in specialized domains.

</details>


### [379] [Enhancing Hallucination Detection via Future Context](https://arxiv.org/abs/2507.20546)
*Joosung Lee,Cheonbok Park,Hwiyeol Jo,Jeonghoon Kim,Joonsuk Park,Kang Min Yoo*

Main category: cs.CL

TL;DR: 为解决大语言模型黑盒输出的幻觉检测挑战，提出采样未来上下文的幻觉检测框架并证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型黑盒输出增多，检测幻觉成为关键挑战。

Method: 基于幻觉一旦引入会持续的观察，采样未来上下文，并与多种基于采样的方法结合。

Result: 所提出的采样方法在多种方法中显著提升了性能。

Conclusion: 所提出的采样未来上下文的方法能有效用于黑盒生成器的幻觉检测。

Abstract: Large Language Models (LLMs) are widely used to generate plausible text on
online platforms, without revealing the generation process. As users
increasingly encounter such black-box outputs, detecting hallucinations has
become a critical challenge. To address this challenge, we focus on developing
a hallucination detection framework for black-box generators. Motivated by the
observation that hallucinations, once introduced, tend to persist, we sample
future contexts. The sampled future contexts provide valuable clues for
hallucination detection and can be effectively integrated with various
sampling-based methods. We extensively demonstrate performance improvements
across multiple methods using our proposed sampling approach.

</details>


### [380] [Ontology-Enhanced Knowledge Graph Completion using Large Language Models](https://arxiv.org/abs/2507.20643)
*Wenbin Guo,Xin Wang,Jiaoyan Chen,Zhao Li,Zirui Chen*

Main category: cs.CL

TL;DR: 当前基于大语言模型的知识图谱补全方法有局限，提出OL - KGC方法，实验显示其性能超现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的知识图谱补全方法是黑盒模型，依赖隐式知识表示且会传播错误知识，影响推理结果，需结合结构信息和本体知识。

Method: 提出OL - KGC方法，先利用神经感知机制将结构信息嵌入文本空间，再用自动提取算法从待补全知识图谱中获取本体知识并转化为文本格式。

Result: 在FB15K - 237、UMLS和WN18RR三个基准上实验，OL - KGC在多个评估指标上显著优于现有主流方法。

Conclusion: OL - KGC方法能实现知识图谱补全，达到了当前最优性能。

Abstract: Large Language Models (LLMs) have been extensively adopted in Knowledge Graph
Completion (KGC), showcasing significant research advancements. However, as
black-box models driven by deep neural architectures, current LLM-based KGC
methods rely on implicit knowledge representation with parallel propagation of
erroneous knowledge, thereby hindering their ability to produce conclusive and
decisive reasoning outcomes. We aim to integrate neural-perceptual structural
information with ontological knowledge, leveraging the powerful capabilities of
LLMs to achieve a deeper understanding of the intrinsic logic of the knowledge.
We propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first
leverages neural perceptual mechanisms to effectively embed structural
information into the textual space, and then uses an automated extraction
algorithm to retrieve ontological knowledge from the knowledge graphs (KGs)
that needs to be completed, which is further transformed into a textual format
comprehensible to LLMs for providing logic guidance. We conducted extensive
experiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The
experimental results demonstrate that OL-KGC significantly outperforms existing
mainstream KGC methods across multiple evaluation metrics, achieving
state-of-the-art performance.

</details>


### [381] [Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models](https://arxiv.org/abs/2507.20704)
*Gabriel Downer,Sean Craven,Damian Ruck,Jake Thomas*

Main category: cs.CL

TL;DR: 为解决现有评估数据集对视觉漏洞评估不足问题，提出Text2VLM将文本数据集转为多模态格式评估VLM抗字体提示注入攻击能力，评估揭示现有模型弱点，经人工评估验证，为VLM安全评估提供工具。


<details>
  <summary>Details</summary>
Motivation: 现有评估数据集多针对纯文本提示，对视觉漏洞评估不足，VLM集成进AI系统需强大模型对齐。

Method: 提出Text2VLM多阶段管道，识别原文本有害内容并转为字体图像，创建多模态提示。

Result: 开源VLM在引入视觉输入时更易受提示注入攻击，与闭源前沿模型有性能差距，Text2VLM经人工评估验证。

Conclusion: Text2VLM为VLM提供可扩展的安全评估工具，有助于开发更强大安全机制，推动VLM在现实应用中安全部署。

Abstract: The increasing integration of Visual Language Models (VLMs) into AI systems
necessitates robust model alignment, especially when handling multimodal
content that combines text and images. Existing evaluation datasets heavily
lean towards text-only prompts, leaving visual vulnerabilities under evaluated.
To address this gap, we propose \textbf{Text2VLM}, a novel multi-stage pipeline
that adapts text-only datasets into multimodal formats, specifically designed
to evaluate the resilience of VLMs against typographic prompt injection
attacks. The Text2VLM pipeline identifies harmful content in the original text
and converts it into a typographic image, creating a multimodal prompt for
VLMs. Also, our evaluation of open-source VLMs highlights their increased
susceptibility to prompt injection when visual inputs are introduced, revealing
critical weaknesses in the current models' alignment. This is in addition to a
significant performance gap compared to closed-source frontier models. We
validate Text2VLM through human evaluations, ensuring the alignment of
extracted salient concepts; text summarization and output classification align
with human expectations. Text2VLM provides a scalable tool for comprehensive
safety assessment, contributing to the development of more robust safety
mechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities,
Text2VLM plays a role in advancing the safe deployment of VLMs in diverse,
real-world applications.

</details>


### [382] [Multilingual Self-Taught Faithfulness Evaluators](https://arxiv.org/abs/2507.20752)
*Carlo Alfano,Aymen Al Marjani,Zeno Jonke,Amin Mantrach,Saab Mansour,Marcello Federico*

Main category: cs.CL

TL;DR: 本文提出多语言忠实度自学习评估框架，利用合成数据和跨语言迁移学习，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型使用增多，现有忠实度评估方法多聚焦英语且依赖昂贵人工标注数据，在多语言场景下需无大量标注数据的准确评估器。

Method: 提出Self - Taught Evaluators for Multilingual Faithfulness框架，仅从合成多语言摘要数据学习，利用跨语言迁移学习，对比特定语言和混合语言微调方法。

Result: 实验证明大语言模型通用语言能力与其在特定语言评估任务中的表现存在一致关系，框架优于现有基线。

Conclusion: 所提框架能在多语言环境下有效评估忠实度，且无需大量标注数据。

Abstract: The growing use of large language models (LLMs) has increased the need for
automatic evaluation systems, particularly to address the challenge of
information hallucination. Although existing faithfulness evaluation approaches
have shown promise, they are predominantly English-focused and often require
expensive human-labeled training data for fine-tuning specialized models. As
LLMs see increased adoption in multilingual contexts, there is a need for
accurate faithfulness evaluators that can operate across languages without
extensive labeled data. This paper presents Self-Taught Evaluators for
Multilingual Faithfulness, a framework that learns exclusively from synthetic
multilingual summarization data while leveraging cross-lingual transfer
learning. Through experiments comparing language-specific and mixed-language
fine-tuning approaches, we demonstrate a consistent relationship between an
LLM's general language capabilities and its performance in language-specific
evaluation tasks. Our framework shows improvements over existing baselines,
including state-of-the-art English evaluators and machine translation-based
approaches.

</details>


### [383] [FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models](https://arxiv.org/abs/2507.20930)
*Likun Tan,Kuan-Wei Huang,Kevin Wu*

Main category: cs.CL

TL;DR: 提出检测和编辑大语言模型生成的金融文本中事实错误内容的方法，微调模型有良好表现，代码和数据开源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的幻觉问题对需要事实可靠性的应用（如金融领域）构成挑战，需有效检测和编辑事实错误内容的方法。

Method: 根据用户定义的特定领域错误分类，在金融问答语料库中插入标记错误构建合成数据集，微调Phi - 4、Phi - 4 - mini、Qwen3 - 4B和Qwen3 - 14B四个语言模型。

Result: 微调的Phi - 4模型在二元F1分数上提高8%，整体检测性能提高30%；Phi - 4 - mini模型性能有竞争力，与OpenAI - o3相比下降幅度小。

Conclusion: 提供了检测和编辑金融文本生成中事实不一致性的实用解决方案，引入了可推广框架，能增强大语言模型在多领域的可信度和一致性。

Abstract: Hallucinations in large language models pose a critical challenge for
applications requiring factual reliability, particularly in high-stakes domains
such as finance. This work presents an effective approach for detecting and
editing factually incorrect content in model-generated responses based on the
provided context. Given a user-defined domain-specific error taxonomy, we
construct a synthetic dataset by inserting tagged errors into financial
question-answering corpora and then fine-tune four language models, Phi-4,
Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual
inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8%
improvement in binary F1 score and a 30% gain in overall detection performance
compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having
only 4 billion parameters, maintains competitive performance with just a 2%
drop in binary detection and a 0.1% decline in overall detection compared to
OpenAI-o3. Our work provides a practical solution for detecting and editing
factual inconsistencies in financial text generation while introducing a
generalizable framework that can enhance the trustworthiness and alignment of
large language models across diverse applications beyond finance. Our code and
data are available at https://github.com/pegasi-ai/fine-grained-editting.

</details>


### [384] [MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation](https://arxiv.org/abs/2507.20917)
*Adrien Bazoge*

Main category: cs.CL

TL;DR: 本文介绍法语医学问答数据集MediQAl，含32603个问题及三类任务，经14个大模型评估，发现事实回忆和推理任务有性能差距，为法语医学问答提供基准。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型在现实临床场景中事实医学回忆和推理能力，弥补医学领域多语言资源缺口。

Method: 创建包含41个医学主题、32603个问题的MediQAl数据集，设置三类任务并对问题分类，用14个大语言模型评估。

Result: 观察到事实回忆和推理任务存在显著性能差距。

Conclusion: 该评估为法语医学问答提供全面基准，填补医学领域多语言资源关键缺口。

Abstract: This work introduces MediQAl, a French medical question answering dataset
designed to evaluate the capabilities of language models in factual medical
recall and reasoning over real-world clinical scenarios. MediQAl contains
32,603 questions sourced from French medical examinations across 41 medical
subjects. The dataset includes three tasks: (i) Multiple-Choice Question with
Unique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii)
Open-Ended Question with Short-Answer. Each question is labeled as
Understanding or Reasoning, enabling a detailed analysis of models' cognitive
capabilities. We validate the MediQAl dataset through extensive evaluation with
14 large language models, including recent reasoning-augmented models, and
observe a significant performance gap between factual recall and reasoning
tasks. Our evaluation provides a comprehensive benchmark for assessing language
models' performance on French medical question answering, addressing a crucial
gap in multilingual resources for the medical domain.

</details>


### [385] [FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models](https://arxiv.org/abs/2507.20924)
*Roberto Labadie-Tamayo,Adrian Jaques Böck,Djordje Slijepčević,Xihui Chen,Andreas Babic,Matthias Zeppelzauer*

Main category: cs.CL

TL;DR: 本文针对EXIST 2025挑战赛的三个子任务提出解决方案，使用SCBM、SCBMT和微调的XLM - RoBERTa模型，部分模型有良好解释性，XLM - RoBERTa和SCBMT在子任务1.1中有不错排名。


<details>
  <summary>Details</summary>
Motivation: 解决社交媒体文本中性别歧视的识别和分类问题。

Method: 针对三个子任务实现三个模型（SCBM、SCBMT、微调的XLM - RoBERTa），SCBM用描述性形容词作为瓶颈概念，SCBMT融合形容词表示与上下文嵌入，还研究利用额外元数据。

Result: XLM - RoBERTa在子任务1.1的Soft - Soft评估中，英语和西班牙语排第6，英语排第4；SCBMT英语和西班牙语排第7，西班牙语排第6。

Conclusion: 提出的模型能有效解决社交媒体文本中性别歧视识别和分类问题，部分模型有良好解释性和竞争力。

Abstract: Sexism has become widespread on social media and in online conversation. To
help address this issue, the fifth Sexism Identification in Social Networks
(EXIST) challenge is initiated at CLEF 2025. Among this year's international
benchmarks, we concentrate on solving the first task aiming to identify and
classify sexism in social media textual posts. In this paper, we describe our
solutions and report results for three subtasks: Subtask 1.1 - Sexism
Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask
1.3 - Sexism Categorization in Tweets. We implement three models to address
each subtask which constitute three individual runs: Speech Concept Bottleneck
Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a
fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as
human-interpretable bottleneck concepts. SCBM leverages large language models
(LLMs) to encode input texts into a human-interpretable representation of
adjectives, then used to train a lightweight classifier for downstream tasks.
SCBMT extends SCBM by fusing adjective-based representation with contextual
embeddings from transformers to balance interpretability and classification
performance. Beyond competitive results, these two models offer fine-grained
explanations at both instance (local) and class (global) levels. We also
investigate how additional metadata, e.g., annotators' demographic profiles,
can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data
augmented with prior datasets, ranks 6th for English and Spanish and 4th for
English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and
Spanish and 6th for Spanish.

</details>


### [386] [Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models](https://arxiv.org/abs/2507.20956)
*Max Peeperkorn,Tom Kouwenhoven,Dan Brown,Anna Jordanous*

Main category: cs.CL

TL;DR: 研究指令调优大语言模型在叙事生成任务中的多样性差距，发现指令调优降低多样性，DPO影响大，提出新解码策略恢复多样性并提升质量。


<details>
  <summary>Details</summary>
Motivation: 指令调优会降低大语言模型输出的多样性，对很多任务尤其是创造性任务有影响，因此研究叙事生成任务中的多样性差距。

Method: 研究不同开源大语言模型在叙事生成任务中的多样性差距，探究OLMo和OLMo 2模型各微调阶段的多样性损失，提出新的解码策略——顺应性解码。

Result: 指令调优显著降低输出多样性，DPO对多样性影响最大，顺应性解码通常能增加多样性，甚至维持或提升质量。

Conclusion: 提出的顺应性解码策略能有效恢复大语言模型输出的多样性并保证质量。

Abstract: Instruction-tuning large language models (LLMs) reduces the diversity of
their outputs, which has implications for many tasks, particularly for creative
tasks. This paper investigates the ``diversity gap'' for a writing prompt
narrative generation task. This gap emerges as measured by current diversity
metrics for various open-weight and open-source LLMs. The results show
significant decreases in diversity due to instruction-tuning. We explore the
diversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to
further understand how output diversity is affected. The results indicate that
DPO has the most substantial impact on diversity. Motivated by these findings,
we present a new decoding strategy, conformative decoding, which guides an
instruct model using its more diverse base model to reintroduce output
diversity. We show that conformative decoding typically increases diversity and
even maintains or improves quality.

</details>


### [387] [Memorization in Fine-Tuned Large Language Models](https://arxiv.org/abs/2507.21009)
*Danil Savine,Muni Sreenivas Pydi,Jamal Atif,Olivier Cappé*

Main category: cs.CL

TL;DR: 研究微调大语言模型在医学领域记忆机制与影响因素，采用两种方法，有三项关键发现，为平衡性能与隐私提供见解。


<details>
  <summary>Details</summary>
Motivation: 研究微调大语言模型在隐私敏感的医学领域中记忆训练数据的机制和影响因素。

Method: 采用成员推理攻击检测记忆数据，用提示前缀生成任务评估逐字复制，分析不同权重矩阵适配、困惑度与记忆的关系以及低秩适配中秩增加的影响。

Result: 值和输出矩阵比查询和键矩阵对记忆贡献更大；微调模型中较低困惑度与更多记忆相关；更高的低秩适配秩会增加记忆，但高秩时收益递减。

Conclusion: 研究结果为微调大语言模型时平衡性能与隐私风险提供见解，对制定有效和负责任的模型适配策略有意义。

Abstract: This study investigates the mechanisms and factors influencing memorization
in fine-tuned large language models (LLMs), with a focus on the medical domain
due to its privacy-sensitive nature. We examine how different aspects of the
fine-tuning process affect a model's propensity to memorize training data,
using the PHEE dataset of pharmacovigilance events.
  Our research employs two main approaches: a membership inference attack to
detect memorized data, and a generation task with prompted prefixes to assess
verbatim reproduction. We analyze the impact of adapting different weight
matrices in the transformer architecture, the relationship between perplexity
and memorization, and the effect of increasing the rank in low-rank adaptation
(LoRA) fine-tuning.
  Key findings include: (1) Value and Output matrices contribute more
significantly to memorization compared to Query and Key matrices; (2) Lower
perplexity in the fine-tuned model correlates with increased memorization; (3)
Higher LoRA ranks lead to increased memorization, but with diminishing returns
at higher ranks.
  These results provide insights into the trade-offs between model performance
and privacy risks in fine-tuned LLMs. Our findings have implications for
developing more effective and responsible strategies for adapting large
language models while managing data privacy concerns.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [388] [Sequence-based protein-protein interaction prediction and its applications in drug discovery](https://arxiv.org/abs/2507.19805)
*François Charih,James R. Green,Kyle K. Biggar*

Main category: q-bio.BM

TL;DR: 本文综述基于序列的蛋白质 - 蛋白质相互作用（PPI）预测方法，涵盖训练数据、预测器类型，介绍其在多领域应用及药物发现潜力。


<details>
  <summary>Details</summary>
Motivation: 异常的蛋白质 - 蛋白质相互作用导致多种人类疾病，破坏有害相互作用是治疗途径，且深度学习等领域进步推动PPI预测计算方法发展，需综述其现状及影响。

Method: 先概述常用训练数据源及数据整理技术，再调查不同类型PPI预测器，最后举例说明其在系统级蛋白质组学分析、靶点识别、治疗性肽和抗体设计等方面的应用。

Result: 展示了不同类型PPI预测方法，以及PPI预测在多个领域的应用实例。

Conclusion: PPI预测方法有重要作用，PPI感知的药物发现模型有加速治疗发展的潜力。

Abstract: Aberrant protein-protein interactions (PPIs) underpin a plethora of human
diseases, and disruption of these harmful interactions constitute a compelling
treatment avenue. Advances in computational approaches to PPI prediction have
closely followed progress in deep learning and natural language processing. In
this review, we outline the state-of the-art for sequence-based PPI prediction
methods and explore their impact on target identification and drug discovery.
We begin with an overview of commonly used training data sources and techniques
used to curate these data to enhance the quality of the training set.
Subsequently, we survey various PPI predictor types, including traditional
similarity-based approaches, and deep learning-based approaches with a
particular emphasis on the transformer architecture. Finally, we provide
examples of PPI prediction in systems-level proteomics analyses, target
identification, and design of therapeutic peptides and antibodies. We also take
the opportunity to showcase the potential of PPI-aware drug discovery models in
accelerating therapeutic development.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [389] [PennyCoder: Efficient Domain-Specific LLMs for PennyLane-Based Quantum Code Generation](https://arxiv.org/abs/2507.19562)
*Abdul Basit,Minghao Shao,Muhammad Haider Asif,Nouhaila Innan,Muhammad Kashif,Alberto Marchisio,Muhammad Shafique*

Main category: quant-ph

TL;DR: 当前基于大语言模型的量子代码助手依赖远程API有隐私、延迟和成本问题，提出PennyCoder轻量级框架，在本地设备生成量子代码，评估显示功能正确性有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决当前基于大语言模型的量子代码助手依赖远程API带来的隐私、延迟和成本问题。

Method: 提出PennyCoder框架，利用微调的LLaMA 3.1 - 8B模型，结合参数高效的低秩自适应（LoRA）技术和针对PennyLane量子编程的特定领域指令调整。

Result: 在综合量子编程数据集上评估，微调模型准确率达44.3%，高于基础LLaMA 3.1 - 8B的33.7%和RAG增强基线的40.1%。

Conclusion: PennyCoder框架在本地设备实现量子编程协助，不依赖外部API，且功能正确性有显著提高。

Abstract: The growing demand for robust quantum programming frameworks has unveiled a
critical limitation: current large language model (LLM) based quantum code
assistants heavily rely on remote APIs, introducing challenges related to
privacy, latency, and excessive usage costs. Addressing this gap, we propose
PennyCoder, a novel lightweight framework for quantum code generation,
explicitly designed for local and embedded deployment to enable on-device
quantum programming assistance without external API dependence. PennyCoder
leverages a fine-tuned version of the LLaMA 3.1-8B model, adapted through
parameter-efficient Low-Rank Adaptation (LoRA) techniques combined with
domain-specific instruction tuning optimized for the specialized syntax and
computational logic of quantum programming in PennyLane, including tasks in
quantum machine learning and quantum reinforcement learning. Unlike prior work
focused on cloud-based quantum code generation, our approach emphasizes
device-native operability while maintaining high model efficacy. We rigorously
evaluated PennyCoder over a comprehensive quantum programming dataset,
achieving 44.3% accuracy with our fine-tuned model (compared to 33.7% for the
base LLaMA 3.1-8B and 40.1% for the RAG-augmented baseline), demonstrating a
significant improvement in functional correctness.

</details>


### [390] [Quantum Reinforcement Learning by Adaptive Non-local Observables](https://arxiv.org/abs/2507.19629)
*Hsin-Yi Lin,Samuel Yen-Chi Chen,Huan-Hsin Tseng,Shinjae Yoo*

Main category: quant-ph

TL;DR: 提出用于量子强化学习的自适应非局部可观测量（ANO）范式，使ANO - VQC代理在基准任务上优于基线VQCs，证明自适应多量子比特可观测量能带来量子优势。


<details>
  <summary>Details</summary>
Motivation: 混合量子 - 经典框架中变分量子电路（VQCs）受限于局部测量，需改进以应用于量子强化学习。

Method: 在VQCs中引入ANO范式，联合优化电路参数和多量子比特测量，将ANO - VQC架构用作深度Q网络（DQN）和异步优势行动者 - 评论家（A3C）算法的函数逼近器。

Result: 在多个基准任务上，ANO - VQC代理优于基线VQCs；消融研究表明自适应测量在不增加电路深度的情况下增强了函数空间。

Conclusion: 自适应多量子比特可观测量可在强化学习中实现实际的量子优势。

Abstract: Hybrid quantum-classical frameworks leverage quantum computing for machine
learning; however, variational quantum circuits (VQCs) are limited by the need
for local measurements. We introduce an adaptive non-local observable (ANO)
paradigm within VQCs for quantum reinforcement learning (QRL), jointly
optimizing circuit parameters and multi-qubit measurements. The ANO-VQC
architecture serves as the function approximator in Deep Q-Network (DQN) and
Asynchronous Advantage Actor-Critic (A3C) algorithms. On multiple benchmark
tasks, ANO-VQC agents outperform baseline VQCs. Ablation studies reveal that
adaptive measurements enhance the function space without increasing circuit
depth. Our results demonstrate that adaptive multi-qubit observables can enable
practical quantum advantages in reinforcement learning.

</details>


### [391] [Quantum-Informed Machine Learning for Chaotic Systems](https://arxiv.org/abs/2507.19861)
*Maida Wang,Xiao Xue,Peter V. Coveney*

Main category: quant-ph

TL;DR: 提出量子启发机器学习框架用于学习偏微分方程，在混沌系统应用中表现优于经典模型，为近量子硬件学习动力系统提供实用途径。


<details>
  <summary>Details</summary>
Motivation: 学习混沌系统行为有挑战，量子机器学习有前景但受硬件噪声和可扩展性限制。

Method: 引入量子启发机器学习框架，用量子电路Born机学习混沌系统不变属性，将统计量子启发先验融入基于Koopman自回归模型。

Result: 该框架在三个代表性系统中性能优于无量子先验的经典模型，数据存储需求降低超两个数量级。

Conclusion: 这种混合架构为使用近量子硬件学习动力系统提供实用途径。

Abstract: Learning the behaviour of chaotic systems remains challenging due to
instability in long-term predictions and difficulties in accurately capturing
invariant statistical properties. While quantum machine learning offers a
promising route to efficiently capture physical properties from
high-dimensional data, its practical deployment is hindered by current hardware
noise and limited scalability. We introduce a quantum-informed machine learning
framework for learning partial differential equations, with an application
focus on chaotic systems. A quantum circuit Born machine is employed to learn
the invariant properties of chaotic dynamical systems, achieving substantial
memory efficiency by representing these complex physical statistics with a
compact set of trainable circuit parameters. This approach reduces the data
storage requirement by over two orders of magnitude compared to the raw
simulation data. The resulting statistical quantum-informed prior is then
incorporated into a Koopman-based auto-regressive model to address issues such
as gradient vanishing or explosion, while maintaining long-term statistical
fidelity. The framework is evaluated on three representative systems: the
Kuramoto-Sivashinsky equation, two-dimensional Kolmogorov flow and turbulent
channel flow. In all cases, the quantum-informed model achieves superior
performance compared to its classical counterparts without quantum priors. This
hybrid architecture offers a practical route for learning dynamical systems
using near-term quantum hardware.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [392] [PhysVarMix: Physics-Informed Variational Mixture Model for Multi-Modal Trajectory Prediction](https://arxiv.org/abs/2507.19701)
*Haichuan Li,Tomi Westerlund*

Main category: cs.RO

TL;DR: 本文提出一种结合基于学习和基于物理约束的混合方法进行轨迹预测，在基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确预测未来智能体轨迹对确保复杂城市环境中自主导航的安全和效率至关重要，现有方法存在多模态处理不足等问题。

Method: 采用变分贝叶斯混合模型，结合特定区域边界条件和基于模型预测控制（MPC）的平滑处理，整合基于学习和基于物理的约束。

Result: 在两个基准数据集上验证，性能优于现有方法，消融实验验证各组件贡献和协同作用。

Conclusion: 该方法平衡数据驱动和物理约束，为应对真实城市环境的不确定性提供了稳健可扩展的解决方案。

Abstract: Accurate prediction of future agent trajectories is a critical challenge for
ensuring safe and efficient autonomous navigation, particularly in complex
urban environments characterized by multiple plausible future scenarios. In
this paper, we present a novel hybrid approach that integrates learning-based
with physics-based constraints to address the multi-modality inherent in
trajectory prediction. Our method employs a variational Bayesian mixture model
to effectively capture the diverse range of potential future behaviors, moving
beyond traditional unimodal assumptions. Unlike prior approaches that
predominantly treat trajectory prediction as a data-driven regression task, our
framework incorporates physical realism through sector-specific boundary
conditions and Model Predictive Control (MPC)-based smoothing. These
constraints ensure that predicted trajectories are not only data-consistent but
also physically plausible, adhering to kinematic and dynamic principles.
Furthermore, our method produces interpretable and diverse trajectory
predictions, enabling enhanced downstream decision-making and planning in
autonomous driving systems. We evaluate our approach on two benchmark datasets,
demonstrating superior performance compared to existing methods. Comprehensive
ablation studies validate the contributions of each component and highlight
their synergistic impact on prediction accuracy and reliability. By balancing
data-driven insights with physics-informed constraints, our approach offers a
robust and scalable solution for navigating the uncertainties of real-world
urban environments.

</details>


### [393] [Extending Group Relative Policy Optimization to Continuous Control: A Theoretical Framework for Robotic Reinforcement Learning](https://arxiv.org/abs/2507.19555)
*Rajat Khanda,Mohammad Baqar,Sambuddha Chakrabarti,Satyasaran Changdar*

Main category: cs.RO

TL;DR: 本文将GRPO扩展到连续控制环境，提出新方法并进行理论分析，为机器人系统实证验证打基础。


<details>
  <summary>Details</summary>
Motivation: GRPO在离散动作空间有优势，但在连续控制领域未被探索，而机器人需要连续动作，因此要将其扩展到连续控制环境。

Method: 引入基于轨迹的策略聚类、状态感知的优势估计和正则化策略更新方法。

Result: 对收敛特性和计算复杂度进行了理论分析。

Conclusion: 为未来机器人系统（如运动和操作任务）的实证验证奠定基础。

Abstract: Group Relative Policy Optimization (GRPO) has shown promise in discrete
action spaces by eliminating value function dependencies through group-based
advantage estimation. However, its application to continuous control remains
unexplored, limiting its utility in robotics where continuous actions are
essential. This paper presents a theoretical framework extending GRPO to
continuous control environments, addressing challenges in high-dimensional
action spaces, sparse rewards, and temporal dynamics. Our approach introduces
trajectory-based policy clustering, state-aware advantage estimation, and
regularized policy updates designed for robotic applications. We provide
theoretical analysis of convergence properties and computational complexity,
establishing a foundation for future empirical validation in robotic systems
including locomotion and manipulation tasks.

</details>


### [394] [A roadmap for AI in robotics](https://arxiv.org/abs/2507.19975)
*Aude Billard,Alin Albu-Schaeffer,Michael Beetz,Wolfram Burgard,Peter Corke,Matei Ciocarlie,Ravinder Dahiya,Danica Kragic,Ken Goldberg,Yukie Nagai,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 文章评估了自90年代以来机器人领域AI的成果，提出短中期研究路线图，并指出长期挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI发展，探讨哪些AI技术能成功应用于机器人，如何适配，需克服哪些挑战。

Method: 对机器人领域AI成果进行评估，提出研究路线图。

Result: 列出短中期挑战与前景，如更新数据集、设计通用算法等。

Conclusion: 指出长期挑战是设计具备终身学习能力、保证安全部署使用和可持续计算成本的机器人。

Abstract: AI technologies, including deep learning, large-language models have gone
from one breakthrough to the other. As a result, we are witnessing growing
excitement in robotics at the prospect of leveraging the potential of AI to
tackle some of the outstanding barriers to the full deployment of robots in our
daily lives. However, action and sensing in the physical world pose greater and
different challenges than analysing data in isolation. As the development and
application of AI in robotic products advances, it is important to reflect on
which technologies, among the vast array of network architectures and learning
models now available in the AI field, are most likely to be successfully
applied to robots; how they can be adapted to specific robot designs, tasks,
environments; which challenges must be overcome. This article offers an
assessment of what AI for robotics has achieved since the 1990s and proposes a
short- and medium-term research roadmap listing challenges and promises. These
range from keeping up-to-date large datasets, representatives of a diversity of
tasks robots may have to perform, and of environments they may encounter, to
designing AI algorithms tailored specifically to robotics problems but generic
enough to apply to a wide range of applications and transfer easily to a
variety of robotic platforms. For robots to collaborate effectively with
humans, they must predict human behavior without relying on bias-based
profiling. Explainability and transparency in AI-driven robot control are not
optional but essential for building trust, preventing misuse, and attributing
responsibility in accidents. We close on what we view as the primary long-term
challenges, that is, to design robots capable of lifelong learning, while
guaranteeing safe deployment and usage, and sustainable computational costs.

</details>


### [395] [CLASP: General-Purpose Clothes Manipulation with Semantic Keypoints](https://arxiv.org/abs/2507.19983)
*Yuhong Deng,Chao Tang,Cunjun Yu,Linfeng Li,David Hsu*

Main category: cs.RO

TL;DR: 本文提出CLASP方法用于通用衣物操作，利用语义关键点连接高层任务规划和低层动作执行，实验表明其在多任务和不同衣物类型上表现优于基线方法，且在真实机器人上得到验证。


<details>
  <summary>Details</summary>
Motivation: 现有衣物操作方法局限于特定任务和衣物类型，需通用方法。

Method: 提出CLASP方法，利用语义关键点，高层用视觉语言模型预测任务计划，低层借助预建操作技能库执行。

Result: 仿真实验中CLASP在多任务和不同衣物类型上优于基线方法，真实机器人实验也验证其性能。

Conclusion: CLASP在通用衣物操作上有良好表现和泛化能力。

Abstract: Clothes manipulation, such as folding or hanging, is a critical capability
for home service robots. Despite recent advances, most existing methods remain
limited to specific tasks and clothes types, due to the complex,
high-dimensional geometry of clothes. This paper presents CLothes mAnipulation
with Semantic keyPoints (CLASP), which aims at general-purpose clothes
manipulation over different clothes types, T-shirts, shorts, skirts, long
dresses, ... , as well as different tasks, folding, flattening, hanging, ... .
The core idea of CLASP is semantic keypoints -- e.g., ''left sleeve'', ''right
shoulder'', etc. -- a sparse spatial-semantic representation that is salient
for both perception and action. Semantic keypoints of clothes can be reliably
extracted from RGB-D images and provide an effective intermediate
representation of clothes manipulation policies. CLASP uses semantic keypoints
to bridge high-level task planning and low-level action execution. At the high
level, it exploits vision language models (VLMs) to predict task plans over the
semantic keypoints. At the low level, it executes the plans with the help of a
simple pre-built manipulation skill library. Extensive simulation experiments
show that CLASP outperforms state-of-the-art baseline methods on multiple tasks
across diverse clothes types, demonstrating strong performance and
generalization. Further experiments with a Franka dual-arm system on four
distinct tasks -- folding, flattening, hanging, and placing -- confirm CLASP's
performance on a real robot.

</details>


### [396] [When Engineering Outruns Intelligence: A Re-evaluation of Instruction-Guided Navigation](https://arxiv.org/abs/2507.20021)
*Matin Aghaei,Mohammad Ali Alomrani,Yingxue Zhang,Mahdi Biparva*

Main category: cs.RO

TL;DR: 研究在HM3D - v1验证集上探讨大语言模型对目标导航规划的作用，发现前沿几何而非LLM推理驱动多数提升。


<details>
  <summary>Details</summary>
Motivation: 明确大语言模型在目标导航中对规划的提升程度。

Method: 先将InstructNav的部分组件替换为简单的距离加权前沿探索器（DWFE），后添加轻量级语言先验（SHF）。

Result: DWFE使成功率从58.0%提升到61.1%，SPL从20.9%提升到36.0%；SHF进一步提升成功率和SPL，平均缩短路径五步。

Conclusion: 前沿几何而非LLM推理驱动多数提升，在将导航成功归因于LLM智能前需使用度量感知提示或离线语义图。

Abstract: Large language models (LLMs) are often credited with recent leaps in
ObjectGoal Navigation, yet the extent to which they improve planning remains
unclear. We revisit this question on the HM3D-v1 validation split. First, we
strip InstructNav of its Dynamic Chain-of-Navigation prompt, open-vocabulary
GLEE detector and Intuition saliency map, and replace them with a simple
Distance-Weighted Frontier Explorer (DWFE). This geometry-only heuristic raises
Success from 58.0% to 61.1% and lifts SPL from 20.9% to 36.0% over 2 000
validation episodes, outperforming all previous training-free baselines.
Second, we add a lightweight language prior (SHF); on a 200-episode subset this
yields a further +2% Success and +0.9% SPL while shortening paths by five steps
on average. Qualitative trajectories confirm the trend: InstructNav back-tracks
and times-out, DWFE reaches the goal after a few islands, and SHF follows an
almost straight route. Our results indicate that frontier geometry, not
emergent LLM reasoning, drives most reported gains, and suggest that
metric-aware prompts or offline semantic graphs are necessary before
attributing navigation success to "LLM intelligence."

</details>


### [397] [DOA: A Degeneracy Optimization Agent with Adaptive Pose Compensation Capability based on Deep Reinforcement Learning](https://arxiv.org/abs/2507.19742)
*Yanbin Li,Canran Xiao,Hongyang He,Shenghai Yuan,Zong Ke,Jiajie Yu,Zixiong Qin,Zhiguo Zhang,Wenzheng Chi,Wei Zhang*

Main category: cs.RO

TL;DR: 使用PPO训练自适应退化优化代理（DOA）解决基于粒子滤波的2D - SLAM退化问题，设计奖励函数、转移学习模块，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 基于粒子滤波的2D - SLAM在室内环境（如长直走廊）存在严重退化问题，传统监督学习框架有数据采集、样本质量和标注协议设计等挑战。

Method: 用PPO训练DOA，设计专门奖励函数，以输出退化因子为参考权重动态调整传感器贡献，采用转移学习模块。

Result: 消融实验证明模型设计合理性和转移学习作用，与SOTA方法对比显示DOA在不同环境有优越的退化检测和优化能力。

Conclusion: 提出的DOA能有效解决基于粒子滤波的2D - SLAM退化问题，具有良好性能和泛化能力。

Abstract: Particle filter-based 2D-SLAM is widely used in indoor localization tasks due
to its efficiency. However, indoor environments such as long straight corridors
can cause severe degeneracy problems in SLAM. In this paper, we use Proximal
Policy Optimization (PPO) to train an adaptive degeneracy optimization agent
(DOA) to address degeneracy problem. We propose a systematic methodology to
address three critical challenges in traditional supervised learning
frameworks: (1) data acquisition bottlenecks in degenerate dataset, (2)
inherent quality deterioration of training samples, and (3) ambiguity in
annotation protocol design. We design a specialized reward function to guide
the agent in developing perception capabilities for degenerate environments.
Using the output degeneracy factor as a reference weight, the agent can
dynamically adjust the contribution of different sensors to pose optimization.
Specifically, the observation distribution is shifted towards the motion model
distribution, with the step size determined by a linear interpolation formula
related to the degeneracy factor. In addition, we employ a transfer learning
module to endow the agent with generalization capabilities across different
environments and address the inefficiency of training in degenerate
environments. Finally, we conduct ablation studies to demonstrate the
rationality of our model design and the role of transfer learning. We also
compare the proposed DOA with SOTA methods to prove its superior degeneracy
detection and optimization capabilities across various environments.

</details>


### [398] [Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots](https://arxiv.org/abs/2507.20217)
*Wei Cui,Haoyu Wang,Wenkang Qin,Yijie Guo,Gang Han,Wen Zhao,Jiahang Cao,Zhang Zhang,Jiaru Zhong,Jingkai Sun,Pihai Sun,Shuai Shi,Botuo Jiang,Jiahao Ma,Jiaxu Wang,Hao Cheng,Zhichao Liu,Yang Wang,Zheng Zhu,Guan Huang,Jian Tang,Qiang Zhang*

Main category: cs.RO

TL;DR: 提出Humanoid Occupancy系统，集成软硬件，用多模态融合技术生成基于网格的占用输出，还开发首个全景占用数据集，为类人机器人环境感知奠定基础。


<details>
  <summary>Details</summary>
Motivation: 类人机器人技术发展迅速，占用表示适合类人机器人，需要一个通用的多模态占用感知系统以实现全面环境理解。

Method: 提出Humanoid Occupancy系统，采用多模态融合技术，克服运动学干扰和遮挡问题，建立有效传感器布局策略，开发全景占用数据集，网络架构融合多模态特征和时间信息。

Result: Humanoid Occupancy系统能为类人机器人提供有效的环境感知。

Conclusion: Humanoid Occupancy为类人机器人环境感知奠定技术基础，为统一通用视觉模块标准化及在复杂场景广泛应用铺平道路。

Abstract: Humanoid robot technology is advancing rapidly, with manufacturers
introducing diverse heterogeneous visual perception modules tailored to
specific scenarios. Among various perception paradigms, occupancy-based
representation has become widely recognized as particularly suitable for
humanoid robots, as it provides both rich semantic and 3D geometric information
essential for comprehensive environmental understanding. In this work, we
present Humanoid Occupancy, a generalized multimodal occupancy perception
system that integrates hardware and software components, data acquisition
devices, and a dedicated annotation pipeline. Our framework employs advanced
multi-modal fusion techniques to generate grid-based occupancy outputs encoding
both occupancy status and semantic labels, thereby enabling holistic
environmental understanding for downstream tasks such as task planning and
navigation. To address the unique challenges of humanoid robots, we overcome
issues such as kinematic interference and occlusion, and establish an effective
sensor layout strategy. Furthermore, we have developed the first panoramic
occupancy dataset specifically for humanoid robots, offering a valuable
benchmark and resource for future research and development in this domain. The
network architecture incorporates multi-modal feature fusion and temporal
information integration to ensure robust perception. Overall, Humanoid
Occupancy delivers effective environmental perception for humanoid robots and
establishes a technical foundation for standardizing universal visual modules,
paving the way for the widespread deployment of humanoid robots in complex
real-world scenarios.

</details>


### [399] [Spatial Language Likelihood Grounding Network for Bayesian Fusion of Human-Robot Observations](https://arxiv.org/abs/2507.19947)
*Supawich Sitdhipol,Waritwong Sukprasongdee,Ekapol Chuangsuwanich,Rina Tse*

Main category: cs.RO

TL;DR: 本文提出FP - LGN网络，通过三阶段课程学习训练以捕获人类语言不确定性，实验表明其在NLL上表现与专家规则相当且更稳健，能提升人机协作任务性能。


<details>
  <summary>Details</summary>
Motivation: 信息融合需表示人类输入不确定性的似然估计，以克服机器人协作任务中的感知限制。

Method: 提出Feature Pyramid Likelihood Grounding Network (FP - LGN)，使用三阶段课程学习训练模型作为概率估计器。

Result: FP - LGN在平均负对数似然（NLL）上与专家设计规则相当，标准差更低更稳健，协作感知中实现异构人类语言观测与机器人传感器测量的不确定性融合。

Conclusion: FP - LGN网络能有效实现不确定性感知的信息融合，显著提升人机协作任务性能。

Abstract: Fusing information from human observations can help robots overcome sensing
limitations in collaborative tasks. However, an uncertainty-aware fusion
framework requires a grounded likelihood representing the uncertainty of human
inputs. This paper presents a Feature Pyramid Likelihood Grounding Network
(FP-LGN) that grounds spatial language by learning relevant map image features
and their relationships with spatial relation semantics. The model is trained
as a probability estimator to capture aleatoric uncertainty in human language
using three-stage curriculum learning. Results showed that FP-LGN matched
expert-designed rules in mean Negative Log-Likelihood (NLL) and demonstrated
greater robustness with lower standard deviation. Collaborative sensing results
demonstrated that the grounded likelihood successfully enabled
uncertainty-aware fusion of heterogeneous human language observations and robot
sensor measurements, achieving significant improvements in human-robot
collaborative task performance.

</details>


### [400] [LLMs-guided adaptive compensator: Bringing Adaptivity to Automatic Control Systems with Large Language Models](https://arxiv.org/abs/2507.20509)
*Zhongchao Zhou,Yuxi Lu,Yaonan Zhu,Yifan Zhao,Bin He,Liang He,Wenwen Yu,Yusuke Iwasawa*

Main category: cs.RO

TL;DR: 本文聚焦大语言模型（LLMs）在自适应控制领域应用，提出LLM引导的自适应补偿器框架，实验表明其性能优于传统自适应控制器，为自动控制领域应用LLMs开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在机器人应用多聚焦高级任务，在反馈控制器设计研究受限，需进一步探索其在自动控制领域应用，故聚焦自适应控制。

Method: 受模型参考自适应控制（MRAC）框架启发，提出LLM引导的自适应补偿器框架，利用未知系统与参考系统差异设计补偿器。并在模拟和真实环境中对五种方法进行实验评估。

Result: LLM引导的自适应补偿器性能优于传统自适应控制器，相比LLM引导的自适应控制器显著降低推理复杂度，具有强泛化性、适应性和鲁棒性。

Conclusion: 本研究为自动控制领域应用LLMs开辟新方向，比视觉语言模型更具可部署性和实用性。

Abstract: With rapid advances in code generation, reasoning, and problem-solving, Large
Language Models (LLMs) are increasingly applied in robotics. Most existing work
focuses on high-level tasks such as task decomposition. A few studies have
explored the use of LLMs in feedback controller design; however, these efforts
are restricted to overly simplified systems, fixed-structure gain tuning, and
lack real-world validation. To further investigate LLMs in automatic control,
this work targets a key subfield: adaptive control. Inspired by the framework
of model reference adaptive control (MRAC), we propose an LLM-guided adaptive
compensator framework that avoids designing controllers from scratch. Instead,
the LLMs are prompted using the discrepancies between an unknown system and a
reference system to design a compensator that aligns the response of the
unknown system with that of the reference, thereby achieving adaptivity.
Experiments evaluate five methods: LLM-guided adaptive compensator, LLM-guided
adaptive controller, indirect adaptive control, learning-based adaptive
control, and MRAC, on soft and humanoid robots in both simulated and real-world
environments. Results show that the LLM-guided adaptive compensator outperforms
traditional adaptive controllers and significantly reduces reasoning complexity
compared to the LLM-guided adaptive controller. The Lyapunov-based analysis and
reasoning-path inspection demonstrate that the LLM-guided adaptive compensator
enables a more structured design process by transforming mathematical
derivation into a reasoning task, while exhibiting strong generalizability,
adaptability, and robustness. This study opens a new direction for applying
LLMs in the field of automatic control, offering greater deployability and
practicality compared to vision-language models.

</details>


### [401] [Bipedalism for Quadrupedal Robots: Versatile Loco-Manipulation through Risk-Adaptive Reinforcement Learning](https://arxiv.org/abs/2507.20382)
*Yuyou Zhang,Radu Corcodel,Ding Zhao*

Main category: cs.RO

TL;DR: 引入双足行走模式让四足机器人前腿可用于交互，提出风险自适应分布式强化学习框架，实验证明其性能优越且策略在现实中能完成多样任务。


<details>
  <summary>Details</summary>
Motivation: 解决四足机器人用腿操作影响移动、安装机械臂使系统复杂的问题。

Method: 引入双足行走模式，提出风险自适应分布式强化学习框架，训练时根据估计回报分布的变异系数动态调整风险偏好。

Result: 仿真实验中方法性能优于基线，在Unitree Go2机器人上能完成推车、探测障碍物、运输负载等任务，且对动态变化和外部干扰有鲁棒性。

Conclusion: 所提方法能让四足机器人在双足行走时有效利用前腿进行环境交互，具有多功能性和鲁棒性。

Abstract: Loco-manipulation of quadrupedal robots has broadened robotic applications,
but using legs as manipulators often compromises locomotion, while mounting
arms complicates the system. To mitigate this issue, we introduce bipedalism
for quadrupedal robots, thus freeing the front legs for versatile interactions
with the environment. We propose a risk-adaptive distributional Reinforcement
Learning (RL) framework designed for quadrupedal robots walking on their hind
legs, balancing worst-case conservativeness with optimal performance in this
inherently unstable task. During training, the adaptive risk preference is
dynamically adjusted based on the uncertainty of the return, measured by the
coefficient of variation of the estimated return distribution. Extensive
experiments in simulation show our method's superior performance over
baselines. Real-world deployment on a Unitree Go2 robot further demonstrates
the versatility of our policy, enabling tasks like cart pushing, obstacle
probing, and payload transport, while showcasing robustness against challenging
dynamics and external disturbances.

</details>


### [402] [LanternNet: A Novel Hub-and-Spoke System to Seek and Suppress Spotted Lanternfly Populations](https://arxiv.org/abs/2507.20800)
*Vinil Polepalli*

Main category: cs.RO

TL;DR: 本文介绍了用于检测和抑制斑衣蜡蝉种群的LanternNet系统，该系统经实地部署验证有效，相比传统方法有成本和可扩展性优势，还具更广泛生态影响潜力。


<details>
  <summary>Details</summary>
Motivation: 现有斑衣蜡蝉控制方法劳动密集、危害环境且长期抑制效果不足，需新方法。

Method: 引入LanternNet系统，中心树状枢纽用YOLOv8模型识别，三个专业机械臂执行特定任务。

Result: 5周多地点实地部署显示，SLF种群显著减少，树木健康指标改善，相比传统方法有成本和可扩展性优势。

Conclusion: LanternNet展示了机器人和AI结合用于入侵物种管理和改善环境成果的变革潜力。

Abstract: The invasive spotted lanternfly (SLF) poses a significant threat to
agriculture and ecosystems, causing widespread damage. Current control methods,
such as egg scraping, pesticides, and quarantines, prove labor-intensive,
environmentally hazardous, and inadequate for long-term SLF suppression. This
research introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system
designed for scalable detection and suppression of SLF populations. A central,
tree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF
identification. Three specialized robotic spokes perform targeted tasks: pest
neutralization, environmental monitoring, and navigation/mapping. Field
deployment across multiple infested sites over 5 weeks demonstrated
LanternNet's efficacy. Quantitative analysis revealed significant reductions (p
< 0.01, paired t-tests) in SLF populations and corresponding improvements in
tree health indicators across the majority of test sites. Compared to
conventional methods, LanternNet offers substantial cost advantages and
improved scalability. Furthermore, the system's adaptability for enhanced
autonomy and targeting of other invasive species presents significant potential
for broader ecological impact. LanternNet demonstrates the transformative
potential of integrating robotics and AI for advanced invasive species
management and improved environmental outcomes.

</details>


### [403] [Free Energy-Inspired Cognitive Risk Integration for AV Navigation in Pedestrian-Rich Environments](https://arxiv.org/abs/2507.20850)
*Meiting Dang,Yanping Wu,Yafei Wang,Dezong Zhao,David Flynn,Chongfeng Wei*

Main category: cs.RO

TL;DR: 本文提出用于建模自动驾驶汽车与多个行人交互的框架，模拟更真实交互动态，提升自动驾驶导航表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究在自动驾驶汽车与弱势道路使用者交互的预测和决策方面存在局限，无法直接应用于自动驾驶汽车，且行人行为模式固定。

Method: 将受自由能原理启发的认知过程建模方法融入自动驾驶汽车和行人模型，行人使用认知 - 风险社会力模型，自动驾驶汽车在软演员 - 评论家架构中构建动态、风险感知邻接矩阵。

Result: 模拟结果表明，与现有方法相比，该框架有效提高了自动驾驶导航的安全性、效率和平顺性。

Conclusion: 所提出的框架能改善自动驾驶汽车在与行人交互时的导航表现。

Abstract: Recent advances in autonomous vehicle (AV) behavior planning have shown
impressive social interaction capabilities when interacting with other road
users. However, achieving human-like prediction and decision-making in
interactions with vulnerable road users remains a key challenge in complex
multi-agent interactive environments. Existing research focuses primarily on
crowd navigation for small mobile robots, which cannot be directly applied to
AVs due to inherent differences in their decision-making strategies and dynamic
boundaries. Moreover, pedestrians in these multi-agent simulations follow fixed
behavior patterns that cannot dynamically respond to AV actions. To overcome
these limitations, this paper proposes a novel framework for modeling
interactions between the AV and multiple pedestrians. In this framework, a
cognitive process modeling approach inspired by the Free Energy Principle is
integrated into both the AV and pedestrian models to simulate more realistic
interaction dynamics. Specifically, the proposed pedestrian Cognitive-Risk
Social Force Model adjusts goal-directed and repulsive forces using a fused
measure of cognitive uncertainty and physical risk to produce human-like
trajectories. Meanwhile, the AV leverages this fused risk to construct a
dynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a
Soft Actor-Critic architecture, allowing it to make more reasonable and
informed decisions. Simulation results indicate that our proposed framework
effectively improves safety, efficiency, and smoothness of AV navigation
compared to the state-of-the-art method.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [404] [Programmable Virtual Humans Toward Human Physiologically-Based Drug Discovery](https://arxiv.org/abs/2507.19568)
*You Wu,Philip E. Bourne,Lei Xie*

Main category: cs.CY

TL;DR: 文章指出当前AI和生物医学数字孪生在药物发现中的局限，提出可编程虚拟人类概念，介绍其在药物发现新范式中的作用及实现面临的机遇、挑战和路线图。


<details>
  <summary>Details</summary>
Motivation: 解决当前药物发现中早期发现与后期开发脱节导致高失败率的问题，发挥AI在药物发现中的真正潜力。

Method: 结合AI、高通量扰动测定、跨物种单细胞和空间组学等最新进展构建可编程虚拟人类。

Result: 可编程虚拟人类可模拟从分子到表型水平的药物作用，弥合转化差距。

Conclusion: 可编程虚拟人类为优化治疗效果和安全性提供变革性途径。

Abstract: Artificial intelligence (AI) has sparked immense interest in drug discovery,
but most current approaches only digitize existing high-throughput experiments.
They remain constrained by conventional pipelines. As a result, they do not
address the fundamental challenges of predicting drug effects in humans.
Similarly, biomedical digital twins, largely grounded in real-world data and
mechanistic models, are tailored for late-phase drug development and lack the
resolution to model molecular interactions or their systemic consequences,
limiting their impact in early-stage discovery. This disconnect between early
discovery and late development is one of the main drivers of high failure rates
in drug discovery. The true promise of AI lies not in augmenting current
experiments but in enabling virtual experiments that are impossible in the real
world: testing novel compounds directly in silico in the human body. Recent
advances in AI, high-throughput perturbation assays, and single-cell and
spatial omics across species now make it possible to construct programmable
virtual humans: dynamic, multiscale models that simulate drug actions from
molecular to phenotypic levels. By bridging the translational gap, programmable
virtual humans offer a transformative path to optimize therapeutic efficacy and
safety earlier than ever before. This perspective introduces the concept of
programmable virtual humans, explores their roles in a new paradigm of drug
discovery centered on human physiology, and outlines key opportunities,
challenges, and roadmaps for their realization.

</details>


### [405] [Does AI and Human Advice Mitigate Punishment for Selfish Behavior? An Experiment on AI ethics From a Psychological Perspective](https://arxiv.org/abs/2507.19487)
*Margarita Leib,Nils Köbis,Ivan Soraperra*

Main category: cs.CY

TL;DR: 研究人们遵循促进自私行为的AI建议时的感知与惩罚情况，发现行为和建议内容影响惩罚，建议来源无影响。


<details>
  <summary>Details</summary>
Motivation: 探究个体遵循促进自私行为的AI建议时如何被感知和惩罚。

Method: 结合社会心理学理论、机器行为和行为经济学方法，进行预注册且有经济激励的实验，让评估者惩罚不同建议情况下的决策者，并分配责任。

Result: 亲社会行为很少受惩罚，自私行为受罚更多；自私行为在亲社会建议后受罚更重，自私建议后受罚更轻；自私决策者遵循AI建议比人类建议时被认为更有责任，但惩罚无差异。

Conclusion: 行为和建议内容影响惩罚，建议来源不影响。

Abstract: People increasingly rely on AI-advice when making decisions. At times, such
advice can promote selfish behavior. When individuals abide by
selfishness-promoting AI advice, how are they perceived and punished? To study
this question, we build on theories from social psychology and combine
machine-behavior and behavioral economic approaches. In a pre-registered,
financially-incentivized experiment, evaluators could punish real
decision-makers who (i) received AI, human, or no advice. The advice (ii)
encouraged selfish or prosocial behavior, and decision-makers (iii) behaved
selfishly or, in a control condition, behaved prosocially. Evaluators further
assigned responsibility to decision-makers and their advisors. Results revealed
that (i) prosocial behavior was punished very little, whereas selfish behavior
was punished much more. Focusing on selfish behavior, (ii) compared to
receiving no advice, selfish behavior was penalized more harshly after
prosocial advice and more leniently after selfish advice. Lastly, (iii) whereas
selfish decision-makers were seen as more responsible when they followed AI
compared to human advice, punishment between the two advice sources did not
vary. Overall, behavior and advice content shape punishment, whereas the advice
source does not.

</details>


### [406] [Justifications for Democratizing AI Alignment and Their Prospects](https://arxiv.org/abs/2507.19548)
*André Steingrüber,Kevin Baum*

Main category: cs.CY

TL;DR: 本文探讨AI对齐规范问题的民主与贤能方法，分析民主方法的理由与挑战，认为纯民主或贤能方法不足，需混合框架。


<details>
  <summary>Details</summary>
Motivation: 研究民主方法解决AI对齐规范问题的合理性，对比民主与贤能方法。

Method: 分析民主方法的工具性与非工具性理由，指出规范和元规范不确定性带来的论证缺口。

Result: 发现民主方法存在显著挑战，尤其是防止AI对齐中非法胁迫方面。

Conclusion: 纯贤能或纯民主方法都不充分，需结合专家判断、参与式输入和防止AI垄断的制度保障的混合框架。

Abstract: The AI alignment problem comprises both technical and normative dimensions.
While technical solutions focus on implementing normative constraints in AI
systems, the normative problem concerns determining what these constraints
should be. This paper examines justifications for democratic approaches to the
normative problem -- where affected stakeholders determine AI alignment -- as
opposed to epistocratic approaches that defer to normative experts. We analyze
both instrumental justifications (democratic approaches produce better
outcomes) and non-instrumental justifications (democratic approaches prevent
illegitimate authority or coercion). We argue that normative and metanormative
uncertainty create a justificatory gap that democratic approaches aim to fill
through political rather than theoretical justification. However, we identify
significant challenges for democratic approaches, particularly regarding the
prevention of illegitimate coercion through AI alignment. Our analysis suggests
that neither purely epistocratic nor purely democratic approaches may be
sufficient on their own, pointing toward hybrid frameworks that combine expert
judgment with participatory input alongside institutional safeguards against AI
monopolization.

</details>


### [407] [Rainbow Noise: Stress-Testing Multimodal Harmful-Meme Detectors on LGBTQ Content](https://arxiv.org/abs/2507.19551)
*Ran Tong,Songtao Wei,Jiaqi Liu,Lanruo Wang*

Main category: cs.CY

TL;DR: 构建仇恨性LGBTQ+表情包检测的鲁棒性基准，对比两模型并引入TDA增强模型鲁棒性，揭示当前模型弱点及改进方向。


<details>
  <summary>Details</summary>
Motivation: 仇恨性LGBTQ+表情包常通过修改内容逃避检测，需构建鲁棒性基准评估检测模型。

Method: 构建鲁棒性基准，结合四种字幕攻击和三种图像损坏，在PrideMM数据集上测试；以MemeCLIP和MemeBLIP2为案例，引入TDA增强MemeBLIP2的鲁棒性。

Result: MemeCLIP性能下降较平缓，MemeBLIP2对字幕编辑敏感，TDA使MemeBLIP2成为最鲁棒模型；各系统依赖文本，架构和预训练数据影响鲁棒性。

Conclusion: 基准揭示了当前多模态安全模型的弱点，TDA等轻量级模块是增强防御的有效途径。

Abstract: Hateful memes aimed at LGBTQ\,+ communities often evade detection by tweaking
either the caption, the image, or both. We build the first robustness benchmark
for this setting, pairing four realistic caption attacks with three canonical
image corruptions and testing all combinations on the PrideMM dataset. Two
state-of-the-art detectors, MemeCLIP and MemeBLIP2, serve as case studies, and
we introduce a lightweight \textbf{Text Denoising Adapter (TDA)} to enhance the
latter's resilience. Across the grid, MemeCLIP degrades more gently, while
MemeBLIP2 is particularly sensitive to the caption edits that disrupt its
language processing. However, the addition of the TDA not only remedies this
weakness but makes MemeBLIP2 the most robust model overall. Ablations reveal
that all systems lean heavily on text, but architectural choices and
pre-training data significantly impact robustness. Our benchmark exposes where
current multimodal safety models crack and demonstrates that targeted,
lightweight modules like the TDA offer a powerful path towards stronger
defences.

</details>


### [408] [PEMUTA: Pedagogically-Enriched Multi-Granular Undergraduate Thesis Assessment](https://arxiv.org/abs/2507.19556)
*Jialu Zhang,Qingyang Sun,Qianyi Wang,Weiyi Zhang,Zunjie Xiao,Xiaoqing Zhang,Jianfeng Ren,Jiang Liu*

Main category: cs.CY

TL;DR: 提出PEMUTA框架用于多粒度本科论文评估，实验表明其与专家评估高度一致，有细粒度评估潜力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型本科论文评估多为单一分数，忽略多维度标准；教学理论在自动评估中未充分利用。

Method: 受维果茨基理论和布鲁姆分类法启发，采用分层提示方案，从六个维度评估论文并整体综合；结合少样本提示和角色扮演提示技术；构建含专家标注的数据集。

Result: PEMUTA与专家评估高度一致。

Conclusion: PEMUTA有进行细粒度、基于教学的本科论文评估的强大潜力。

Abstract: The undergraduate thesis (UGTE) plays an indispensable role in assessing a
student's cumulative academic development throughout their college years.
Although large language models (LLMs) have advanced education intelligence,
they typically focus on holistic assessment with only one single evaluation
score, but ignore the intricate nuances across multifaceted criteria, limiting
their ability to reflect structural criteria, pedagogical objectives, and
diverse academic competencies. Meanwhile, pedagogical theories have long
informed manual UGTE evaluation through multi-dimensional assessment of
cognitive development, disciplinary thinking, and academic performance, yet
remain underutilized in automated settings. Motivated by the research gap, we
pioneer PEMUTA, a pedagogically-enriched framework that effectively activates
domain-specific knowledge from LLMs for multi-granular UGTE assessment. Guided
by Vygotsky's theory and Bloom's Taxonomy, PEMUTA incorporates a hierarchical
prompting scheme that evaluates UGTEs across six fine-grained dimensions:
Structure, Logic, Originality, Writing, Proficiency, and Rigor (SLOWPR),
followed by holistic synthesis. Two in-context learning techniques, \ie,
few-shot prompting and role-play prompting, are also incorporated to further
enhance alignment with expert judgments without fine-tuning. We curate a
dataset of authentic UGTEs with expert-provided SLOWPR-aligned annotations to
support multi-granular UGTE assessment. Extensive experiments demonstrate that
PEMUTA achieves strong alignment with expert evaluations, and exhibits strong
potential for fine-grained, pedagogically-informed UGTE evaluations.

</details>


### [409] [Towards Sustainability Model Cards](https://arxiv.org/abs/2507.19559)
*Gwendal Jouneaux,Jordi Cabot*

Main category: cs.CY

TL;DR: 机器学习模型能耗增加，Green AI成研究热点，但现有基准测试与质量模型等未整合，本文提出用新领域特定语言定义ML模型可持续性方面，可扩展为模型卡片并用于自动处理。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型及数据集增长使能耗成本剧增，当前关注环境与可持续性，Green AI成重要研究话题，但现有基准测试与质量模型等未整合，限制模型能耗描述分析及应用。

Method: 提出新的领域特定语言来精确界定ML模型的可持续性方面。

Result: 所定义的信息可扩展为模型卡片，且足够形式化可作为其他模型描述自动处理的输入。

Conclusion: 有望通过融合质量模型概念与现有相关提议，为AI/ML模型建立可持续质量模型。

Abstract: The growth of machine learning (ML) models and associated datasets triggers a
consequent dramatic increase in energy costs for the use and training of these
models. In the current context of environmental awareness and global
sustainability concerns involving ICT, Green AI is becoming an important
research topic. Initiatives like the AI Energy Score Ratings are a good
example. Nevertheless, these benchmarking attempts are still to be integrated
with existing work on Quality Models and Service-Level Agreements common in
other, more mature, ICT subfields. This limits the (automatic) analysis of this
model energy descriptions and their use in (semi)automatic model comparison,
selection, and certification processes. We aim to leverage the concept of
quality models and merge it with existing ML model reporting initiatives and
Green/Frugal AI proposals to formalize a Sustainable Quality Model for AI/ML
models. As a first step, we propose a new Domain-Specific Language to precisely
define the sustainability aspects of an ML model (including the energy costs
for its different tasks). This information can then be exported as an extended
version of the well-known Model Cards initiative while, at the same time, being
formal enough to be input of any other model description automatic process.

</details>


### [410] [Differentiating hype from practical applications of large language models in medicine -- a primer for healthcare professionals](https://arxiv.org/abs/2507.19567)
*Elisha D. O. Roberson*

Main category: cs.CY

TL;DR: 医学生态系统多方面可从自动化和程序辅助中受益，但大语言模型有缺陷，应用AI需谨慎。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在医学领域应用机器学习和人工智能技术，特别是大语言模型以实现医疗创新。

Method: 未提及。

Result: 大语言模型无现实客观真理理解能力，存在信息泄露风险。

Conclusion: 在医学中使用AI，特别是部署大语言模型，需谨慎考虑并审慎应用。

Abstract: The medical ecosystem consists of the training of new clinicians and
researchers, the practice of clinical medicine, and areas of adjacent research.
There are many aspects of these domains that could benefit from the application
of task automation and programmatic assistance. Machine learning and artificial
intelligence techniques, including large language models (LLMs), have been
promised to deliver on healthcare innovation, improving care speed and
accuracy, and reducing the burden on staff for manual interventions. However,
LLMs have no understanding of objective truth that is based in reality. They
also represent real risks to the disclosure of protected information when used
by clinicians and researchers. The use of AI in medicine in general, and the
deployment of LLMs in particular, therefore requires careful consideration and
thoughtful application to reap the benefits of these technologies while
avoiding the dangers in each context.

</details>


### [411] [Can You Share Your Story? Modeling Clients' Metacognition and Openness for LLM Therapist Evaluation](https://arxiv.org/abs/2507.19643)
*Minju Kim,Dongje Yoo,Yeonjun Hwang,Minseok Kang,Namyoung Kim,Minju Gwak,Beong-woo Kwak,Hyungjoo Chae,Harim Kim,Yunjoong Lee,Min Hee Kim,Dayi Jung,Kyong-Mee Chung,Jinyoung Yeo*

Main category: cs.CY

TL;DR: 当前对大语言模型治疗师的评估方法存在局限，论文引入MindVoyager评估框架及评估指标以解决问题。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型治疗师的评估常未考量理解客户想法和信念的能力，现有评估方法难以判断治疗师能否发掘未表达的观点。

Method: 引入MindVoyager评估框架，包含可控且现实的客户模拟器；引入评估指标衡量大语言模型治疗师对客户信念和想法的探索能力。

Result: 未提及

Conclusion: 未提及

Abstract: Understanding clients' thoughts and beliefs is fundamental in counseling, yet
current evaluations of LLM therapists often fail to assess this ability.
Existing evaluation methods rely on client simulators that clearly disclose
internal states to the therapist, making it difficult to determine whether an
LLM therapist can uncover unexpressed perspectives. To address this limitation,
we introduce MindVoyager, a novel evaluation framework featuring a controllable
and realistic client simulator which dynamically adapts itself based on the
ongoing counseling session, offering a more realistic and challenging
evaluation environment. We further introduce evaluation metrics that assess the
exploration ability of LLM therapists by measuring their thorough understanding
of client's beliefs and thoughts.

</details>


### [412] [The Carbon Cost of Conversation, Sustainability in the Age of Language Models](https://arxiv.org/abs/2507.20018)
*Sayed Mahbub Hasan Amiri,Prasun Goswami,Md. Mainul Islam,Mohammad Shakhawat Hossen,Sayed Majhab Hasan Amiri,Naznin Akter*

Main category: cs.CY

TL;DR: 文章批判大语言模型可持续性问题，指出环境成本被忽视，提出可持续NLP途径，呼吁行动使技术进步与地球边界相契合。


<details>
  <summary>Details</summary>
Motivation: 关注大语言模型在自然语言处理中环境成本被忽视的问题，强调解决其可持续性的重要性。

Method: 通过对GPT - 4、Mistral 7B等模型的案例研究，分析行业领导者和落后者，量化大语言模型的碳足迹、用水情况和电子垃圾贡献。

Result: 训练单个大语言模型碳排放高，数据中心冷却加剧缺水问题，系统性挑战导致危害持续，且对全球南方边缘化社区影响更大。

Conclusion: 强调立即行动的紧迫性，呼吁使技术进步与地球边界一致，倡导公平、透明和再生的人工智能系统。

Abstract: Large language models (LLMs) like GPT-3 and BERT have revolutionized natural
language processing (NLP), yet their environmental costs remain dangerously
overlooked. This article critiques the sustainability of LLMs, quantifying
their carbon footprint, water usage, and contribution to e-waste through case
studies of models such as GPT-4 and energy-efficient alternatives like Mistral
7B. Training a single LLM can emit carbon dioxide equivalent to hundreds of
cars driven annually, while data centre cooling exacerbates water scarcity in
vulnerable regions. Systemic challenges corporate greenwashing, redundant model
development, and regulatory voids perpetuate harm, disproportionately burdening
marginalized communities in the Global South. However, pathways exist for
sustainable NLP: technical innovations (e.g., model pruning, quantum
computing), policy reforms (carbon taxes, mandatory emissions reporting), and
cultural shifts prioritizing necessity over novelty. By analysing industry
leaders (Google, Microsoft) and laggards (Amazon), this work underscores the
urgency of ethical accountability and global cooperation. Without immediate
action, AIs ecological toll risks outpacing its societal benefits. The article
concludes with a call to align technological progress with planetary
boundaries, advocating for equitable, transparent, and regenerative AI systems
that prioritize both human and environmental well-being.

</details>


### [413] [The Xeno Sutra: Can Meaning and Value be Ascribed to an AI-Generated "Sacred" Text?](https://arxiv.org/abs/2507.20525)
*Murray Shanahan,Tara Das,Robert Thurman*

Main category: cs.CY

TL;DR: 本文通过大语言模型生成虚构佛经案例，从哲学和文学角度分析结果，引发对技术影响人类意义构建的思考，认为佛教哲学能适应。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型生成文本带来的影响，以及社会如何应对技术对人类意义构建的挑战。

Method: 开展大语言模型生成虚构佛经的案例研究，并从哲学和文学角度分析生成文本。

Result: 生成的文本在概念、意象和典故上有特点，难以因机械生成而轻易否定。

Conclusion: 佛教哲学本质上适合适应这种技术带来的变化。

Abstract: This paper presents a case study in the use of a large language model to
generate a fictional Buddhist "sutr"', and offers a detailed analysis of the
resulting text from a philosophical and literary point of view. The conceptual
subtlety, rich imagery, and density of allusion found in the text make it hard
to causally dismiss on account of its mechanistic origin. This raises questions
about how we, as a society, should come to terms with the potentially
unsettling possibility of a technology that encroaches on human meaning-making.
We suggest that Buddhist philosophy, by its very nature, is well placed to
adapt.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [414] [Taming Domain Shift in Multi-source CT-Scan Classification via Input-Space Standardization](https://arxiv.org/abs/2507.19858)
*Chia-Ming Lee,Bo-Cheng Qiu,Ting-Yao Chen,Ming-Han Sun,Fang-Ying Lin,Jung-Tse Tsai,I-An Tsai,Yu-Fan Lin,Chih-Chung Hsu*

Main category: eess.IV

TL;DR: 研究分析SSFL++和KDS预处理管道管理局部可区分性和跨源泛化之间权衡的机制，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 多源CT扫描分类存在域偏移问题影响跨源泛化，且现有SSFL++和KDS管道域鲁棒性机制未充分探索。

Method: 使用SSFL++和KDS管道进行时空标准化，将不同输入映射到一致目标空间。

Result: 实验表明该方法在各架构上有持续改进，在竞赛中获第一名。

Conclusion: 输入空间标准化是多机构医学成像的稳健实用解决方案。

Abstract: Multi-source CT-scan classification suffers from domain shifts that impair
cross-source generalization. While preprocessing pipelines combining
Spatial-Slice Feature Learning (SSFL++) and Kernel-Density-based Slice Sampling
(KDS) have shown empirical success, the mechanisms underlying their domain
robustness remain underexplored. This study analyzes how this input-space
standardization manages the trade-off between local discriminability and
cross-source generalization. The SSFL++ and KDS pipeline performs spatial and
temporal standardization to reduce inter-source variance, effectively mapping
disparate inputs into a consistent target space. This preemptive alignment
mitigates domain shift and simplifies the learning task for network
optimization. Experimental validation demonstrates consistent improvements
across architectures, proving the benefits stem from the preprocessing itself.
The approach's effectiveness was validated by securing first place in a
competitive challenge, supporting input-space standardization as a robust and
practical solution for multi-institutional medical imaging.

</details>


### [415] [A Metabolic-Imaging Integrated Model for Prognostic Prediction in Colorectal Liver Metastases](https://arxiv.org/abs/2507.19734)
*Qinlong Li,Pu Sun,Guanlin Zhu,Tianjiao Liang,Honggang QI*

Main category: eess.IV

TL;DR: 研究开发并验证了预测结直肠癌肝转移术后复发风险的机器学习模型，强调避免数据泄露，模型有临床应用价值。


<details>
  <summary>Details</summary>
Motivation: 传统临床模型对结直肠癌肝转移预后评估准确性欠佳，需开发新模型。

Method: 开发机器学习模型，先构建初步集成模型，后限制输入为术前基线临床参数和增强CT影像的放射组学特征。

Result: 3个月复发预测模型交叉验证AUC为0.723，决策曲线分析显示模型在一定阈值概率下净获益更大。

Conclusion: 成功开发早期复发预测模型，有临床效用，强调数据泄露风险并提出缓解框架。

Abstract: Prognostic evaluation in patients with colorectal liver metastases (CRLM)
remains challenging due to suboptimal accuracy of conventional clinical models.
This study developed and validated a robust machine learning model for
predicting postoperative recurrence risk. Preliminary ensemble models achieved
exceptionally high performance (AUC $>$ 0.98) but incorporated postoperative
features, introducing data leakage risks. To enhance clinical applicability, we
restricted input variables to preoperative baseline clinical parameters and
radiomic features from contrast-enhanced CT imaging, specifically targeting
recurrence prediction at 3, 6, and 12 months postoperatively. The 3-month
recurrence prediction model demonstrated optimal performance with an AUC of
0.723 in cross-validation. Decision curve analysis revealed that across
threshold probabilities of 0.55-0.95, the model consistently provided greater
net benefit than "treat-all" or "treat-none" strategies, supporting its utility
in postoperative surveillance and therapeutic decision-making. This study
successfully developed a robust predictive model for early CRLM recurrence with
confirmed clinical utility. Importantly, it highlights the critical risk of
data leakage in clinical prognostic modeling and proposes a rigorous framework
to mitigate this issue, enhancing model reliability and translational value in
real-world settings.

</details>


### [416] [SpecBPP: A Self-Supervised Learning Approach for Hyperspectral Representation and Soil Organic Carbon Estimation](https://arxiv.org/abs/2507.19781)
*Daniel La'ah Ayuba,Jean-Yves Guillemaut,Belen Marti-Cardona,Oscar Mendez Maldonado*

Main category: eess.IV

TL;DR: 提出用于高光谱图像的自监督学习框架SpecBPP，在土壤有机碳估计中取得SOTA结果，证明光谱顺序预测是高光谱理解的有效预训练任务。


<details>
  <summary>Details</summary>
Motivation: 自监督学习在高光谱图像领域研究不足，而光谱带的顺序结构提供了独特机会。

Method: 提出SpecBPP框架，通过恢复打乱的光谱段顺序鼓励全局光谱理解，并采用基于课程的训练策略。

Result: 在土壤有机碳估计中超越MAE和JEPA基线，微调后R²为0.9456，RMSE为1.1053%，RPD为4.19。

Conclusion: 光谱顺序预测是高光谱理解的强大预训练任务，为遥感等领域的科学表征学习开辟新途径。

Abstract: Self-supervised learning has revolutionized representation learning in vision
and language, but remains underexplored for hyperspectral imagery (HSI), where
the sequential structure of spectral bands offers unique opportunities. In this
work, we propose Spectral Band Permutation Prediction (SpecBPP), a novel
self-supervised learning framework that leverages the inherent spectral
continuity in HSI. Instead of reconstructing masked bands, SpecBPP challenges a
model to recover the correct order of shuffled spectral segments, encouraging
global spectral understanding. We implement a curriculum-based training
strategy that progressively increases permutation difficulty to manage the
factorial complexity of the permutation space. Applied to Soil Organic Carbon
(SOC) estimation using EnMAP satellite data, our method achieves
state-of-the-art results, outperforming both masked autoencoder (MAE) and
joint-embedding predictive (JEPA) baselines. Fine-tuned on limited labeled
samples, our model yields an $R^2$ of 0.9456, RMSE of 1.1053%, and RPD of 4.19,
significantly surpassing traditional and self-supervised benchmarks. Our
results demonstrate that spectral order prediction is a powerful pretext task
for hyperspectral understanding, opening new avenues for scientific
representation learning in remote sensing and beyond.

</details>


### [417] [Multi-Attention Stacked Ensemble for Lung Cancer Detection in CT Scans](https://arxiv.org/abs/2507.20221)
*Uzzal Saha,Surya Prakash*

Main category: eess.IV

TL;DR: 本文提出多级注意力堆叠集成深度神经网络用于肺部结节良恶性分类，在LIDC - IDRI数据集上表现出色，可辅助放射科医生筛查肺癌。


<details>
  <summary>Details</summary>
Motivation: 解决使用CT图像进行肺部结节二元分类（良性与恶性）的挑战。

Method: 采用三个预训练骨干网络并适配自定义分类头，使用两阶段注意力机制和轻量级元学习器，采用动态焦点损失、MixUp数据增强和测试时增强。

Result: 在LIDC - IDRI数据集上准确率达98.09，AUC为0.9961，错误率较现有方法降低35%，敏感性和特异性表现平衡，在放射科医生分歧大的困难病例上效果好。

Conclusion: 该方法可作为放射科医生肺癌筛查的可靠自动辅助工具。

Abstract: In this work, we address the challenge of binary lung nodule classification
(benign vs malignant) using CT images by proposing a multi-level attention
stacked ensemble of deep neural networks. Three pretrained backbones --
EfficientNet V2 S, MobileViT XXS, and DenseNet201 -- are each adapted with a
custom classification head tailored to 96 x 96 pixel inputs. A two-stage
attention mechanism learns both model-wise and class-wise importance scores
from concatenated logits, and a lightweight meta-learner refines the final
prediction. To mitigate class imbalance and improve generalization, we employ
dynamic focal loss with empirically calculated class weights, MixUp
augmentation during training, and test-time augmentation at inference.
Experiments on the LIDC-IDRI dataset demonstrate exceptional performance,
achieving 98.09 accuracy and 0.9961 AUC, representing a 35 percent reduction in
error rate compared to state-of-the-art methods. The model exhibits balanced
performance across sensitivity (98.73) and specificity (98.96), with
particularly strong results on challenging cases where radiologist disagreement
was high. Statistical significance testing confirms the robustness of these
improvements across multiple experimental runs. Our approach can serve as a
robust, automated aid for radiologists in lung cancer screening.

</details>


### [418] [Hybrid Deep Learning and Handcrafted Feature Fusion for Mammographic Breast Cancer Classification](https://arxiv.org/abs/2507.19843)
*Maximilian Tschuchnig,Michael Gadermayr,Khalifa Djemal*

Main category: eess.IV

TL;DR: 提出结合ResNet - 50深度卷积特征、手工特征和基于变压器的嵌入的混合框架用于乳腺癌分类，融合特征提升了AUC等指标，方法实用高效。


<details>
  <summary>Details</summary>
Motivation: 由于乳腺良性和恶性组织差异细微，自动乳腺癌分类仍是重大挑战。

Method: 构建结合ResNet - 50深度卷积特征、手工特征和基于变压器的嵌入的混合框架，使用CBIS - DDSM数据集进行实验。

Result: ResNet - 50基线AUC为78.1%，融合手工特征与深度特征后AUC提升到79.6%，峰值召回率80.5%，最高F1分数67.4%。

Conclusion: 混合融合方法结果与最先进方法相当，且架构简单、计算高效，是临床决策支持的实用有效解决方案。

Abstract: Automated breast cancer classification from mammography remains a significant
challenge due to subtle distinctions between benign and malignant tissue. In
this work, we present a hybrid framework combining deep convolutional features
from a ResNet-50 backbone with handcrafted descriptors and transformer-based
embeddings. Using the CBIS-DDSM dataset, we benchmark our ResNet-50 baseline
(AUC: 78.1%) and demonstrate that fusing handcrafted features with deep
ResNet-50 and DINOv2 features improves AUC to 79.6% (setup d1), with a peak
recall of 80.5% (setup d1) and highest F1 score of 67.4% (setup d1). Our
experiments show that handcrafted features not only complement deep
representations but also enhance performance beyond transformer-based
embeddings. This hybrid fusion approach achieves results comparable to
state-of-the-art methods while maintaining architectural simplicity and
computational efficiency, making it a practical and effective solution for
clinical decision support.

</details>


### [419] [SkinDualGen: Prompt-Driven Diffusion for Simultaneous Image-Mask Generation in Skin Lesions](https://arxiv.org/abs/2507.19970)
*Zhaobin Xu*

Main category: eess.IV

TL;DR: 本文提出利用预训练的Stable Diffusion - 2.0模型生成高质量合成皮肤病变图像及分割掩码，结合真实数据提升分类和分割模型性能，解决医学影像数据难题。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析中数据稀缺和类别不平衡阻碍深度学习模型性能，需解决医学影像数据问题。

Method: 利用预训练的Stable Diffusion - 2.0模型，通过特定领域的低秩自适应（LoRA）微调及多目标损失函数联合优化，一步生成临床相关图像和分割掩码。

Result: 生成图像质量与真实图像相近，结合真实和合成数据的混合数据集显著提升分类和分割模型性能，多项指标有明显提升。

Conclusion: 该方法为解决医学成像数据挑战提供可扩展方案，有助于提高罕见病诊断的准确性和可靠性。

Abstract: Medical image analysis plays a pivotal role in the early diagnosis of
diseases such as skin lesions. However, the scarcity of data and the class
imbalance significantly hinder the performance of deep learning models. We
propose a novel method that leverages the pretrained Stable Diffusion-2.0 model
to generate high-quality synthetic skin lesion images and corresponding
segmentation masks. This approach augments training datasets for classification
and segmentation tasks. We adapt Stable Diffusion-2.0 through domain-specific
Low-Rank Adaptation (LoRA) fine-tuning and joint optimization of
multi-objective loss functions, enabling the model to simultaneously generate
clinically relevant images and segmentation masks conditioned on textual
descriptions in a single step. Experimental results show that the generated
images, validated by FID scores, closely resemble real images in quality. A
hybrid dataset combining real and synthetic data markedly enhances the
performance of classification and segmentation models, achieving substantial
improvements in accuracy and F1-score of 8% to 15%, with additional positive
gains in other key metrics such as the Dice coefficient and IoU. Our approach
offers a scalable solution to address the challenges of medical imaging data,
contributing to improved accuracy and reliability in diagnosing rare diseases.

</details>


### [420] [Implicit Spatiotemporal Bandwidth Enhancement Filter by Sine-activated Deep Learning Model for Fast 3D Photoacoustic Tomography](https://arxiv.org/abs/2507.20575)
*I Gede Eka Sulistyawan,Takuro Ishii,Riku Suzuki,Yoshifumi Saijo*

Main category: eess.IV

TL;DR: 文章针对3D光声断层成像中传感器稀疏和带宽受限问题，引入正弦激活DL模型结合随机训练策略，在不同测试中表现良好，还优化方法实现快速成像。


<details>
  <summary>Details</summary>
Motivation: 解决3D光声断层成像中因传感器通道数量有限和采样率受限导致的图像质量下降问题。

Method: 将2D深度学习方法应用于传感器级光声射频数据，引入正弦激活到DL模型，利用模拟随机球形吸收体进行简化训练。

Result: 提出的训练机制在不同测试中有效增加传感器密度、恢复时空带宽，定性上增强高频内容，定量上频谱带宽和对比度噪声比表现良好。

Conclusion: 优化方法可实现每秒2体积的快速增强3D - PAT，利于对自由移动目标的实际成像。

Abstract: 3D photoacoustic tomography (3D-PAT) using high-frequency hemispherical
transducers offers near-omnidirectional reception and enhanced sensitivity to
the finer structural details encoded in the high-frequency components of the
broadband photoacoustic (PA) signal. However, practical constraints such as
limited number of channels with bandlimited sampling rate often result in
sparse and bandlimited sensors that degrade image quality. To address this, we
revisit the 2D deep learning (DL) approach applied directly to sensor-wise PA
radio-frequency (PARF) data. Specifically, we introduce sine activation into
the DL model to restore the broadband nature of PARF signals given the observed
band-limited and high-frequency PARF data. Given the scarcity of 3D training
data, we employ simplified training strategies by simulating random spherical
absorbers. This combination of sine-activated model and randomized training is
designed to emphasize bandwidth learning over dataset memorization. Our model
was evaluated on a leaf skeleton phantom, a micro-CT-verified 3D spiral phantom
and in-vivo human palm vasculature. The results showed that the proposed
training mechanism on sine-activated model was well-generalized across the
different tests by effectively increasing the sensor density and recovering the
spatiotemporal bandwidth. Qualitatively, the sine-activated model uniquely
enhanced high-frequency content that produces clearer vascular structure with
fewer artefacts. Quantitatively, the sine-activated model exhibits full
bandwidth at -12 dB spectrum and significantly higher contrast-to-noise ratio
with minimal loss of structural similarity index. Lastly, we optimized our
approach to enable fast enhanced 3D-PAT at 2 volumes-per-second for better
practical imaging of a free-moving targets.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [421] [NIRS: An Ontology for Non-Invasive Respiratory Support in Acute Care](https://arxiv.org/abs/2507.19992)
*Md Fantacher Islam,Jarrod Mosier,Vignesh Subbian*

Main category: q-bio.OT

TL;DR: 本文开发了用于急性护理的NIRS本体，添加规则并评估推理，验证了其支持规则推理和治疗建议的能力。


<details>
  <summary>Details</summary>
Motivation: 开发非侵入性呼吸支持（NIRS）本体，以支持急性护理环境中的知识表示。

Method: 使用Web本体语言（OWL）语义和Protege开发NIRS本体，添加SWRL规则，用17个假设患者临床场景评估逻辑推理，用SPARQL查询和eICU数据库数据检索和测试推理。

Result: 本体有132个类、12个对象属性和17个数据属性，882个公理，添加350个注释，SPARQL查询成功验证所有测试用例。

Conclusion: 将NIRS概念统一到本体框架中，通过评估假设患者场景和与标准词汇对齐证明了其适用性。

Abstract: Objective: Develop a Non Invasive Respiratory Support (NIRS) ontology to
support knowledge representation in acute care settings.
  Materials and Methods: We developed the NIRS ontology using Web Ontology
Language (OWL) semantics and Protege to organize clinical concepts and
relationships. To enable rule-based clinical reasoning beyond hierarchical
structures, we added Semantic Web Rule Language (SWRL) rules. We evaluated
logical reasoning by adding 17 hypothetical patient clinical scenarios. We used
SPARQL queries and data from the Electronic Intensive Care Unit (eICU)
Collaborative Research Database to retrieve and test targeted inferences.
  Results: The ontology has 132 classes, 12 object properties, and 17 data
properties across 882 axioms that establish concept relationships. To
standardize clinical concepts, we added 350 annotations, including descriptive
definitions based on controlled vocabularies. SPARQL queries successfully
validated all test cases (rules) by retrieving appropriate patient outcomes,
for instance, a patient treated with HFNC (high-flow nasal cannula) for 2 hours
due to acute respiratory failure may avoid endotracheal intubation.
  Discussion: The NIRS ontology formally represents domain-specific concepts,
including ventilation modalities, patient characteristics, therapy parameters,
and outcomes. SPARQL query evaluations on clinical scenarios confirmed the
ability of the ontology to support rule based reasoning and therapy
recommendations, providing a foundation for consistent documentation practices,
integration into clinical data models, and advanced analysis of NIRS outcomes.
  Conclusion: We unified NIRS concepts into an ontological framework and
demonstrated its applicability through the evaluation of hypothetical patient
scenarios and alignment with standardized vocabularies.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [422] [A General Framework for Estimating Preferences Using Response Time Data](https://arxiv.org/abs/2507.20403)
*Federico Echenique,Alireza Fallah,Michael I. Jordan*

Main category: econ.TH

TL;DR: 提出从选择和响应时间数据中恢复偏好参数的通用方法，适用于多种决策模型，实证应用显示响应时间对预测和参数估计有作用。


<details>
  <summary>Details</summary>
Motivation: 从选择和响应时间数据中恢复偏好参数。

Method: 提出通用方法，在DDM模型中有快速收敛率，且适用于多种决策模型。

Result: 在跨期选择实验的实证应用中，使用响应时间提高了预测准确性。

Conclusion: 使用响应时间对预测准确性和经济相关参数估计有重要意义。

Abstract: We propose a general methodology for recovering preference parameters from
data on choices and response times. Our methods yield estimates with fast
($1/n$ for $n$ data points) convergence rates when specialized to the popular
Drift Diffusion Model (DDM), but are broadly applicable to generalizations of
the DDM as well as to alternative models of decision making that make use of
response time data. The paper develops an empirical application to an
experiment on intertemporal choice, showing that the use of response times
delivers predictive accuracy and matters for the estimation of economically
relevant parameters.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [423] [Enhancing Materials Discovery with Valence Constrained Design in Generative Modeling](https://arxiv.org/abs/2507.19799)
*Mouyang Cheng,Weiliang Luo,Hao Tang,Bowen Yu,Yongqiang Cheng,Weiwei Xie,Ju Li,Heather J. Kulik,Mingda Li*

Main category: cond-mat.mtrl-sci

TL;DR: 本文介绍了CrysVCD框架，它将化学规则集成到生成过程，能高效生成化学有效晶体结构，可用于功能材料发现。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的深度生成模型在逆材料设计中忽略化学约束，导致生成化学无效结构，需要改进。

Method: CrysVCD先使用基于Transformer的元素语言模型生成价态平衡的成分，再用扩散模型生成晶体结构。

Result: 价态约束使化学价态检查效率大幅提高，微调后热力学稳定性达85%，声子稳定性达68%，支持功能材料的条件生成。

Conclusion: CrysVCD作为通用插件可集成到多种生成流程，为材料发现提供可靠科学途径。

Abstract: Diffusion-based deep generative models have emerged as powerful tools for
inverse materials design. Yet, many existing approaches overlook essential
chemical constraints such as oxidation state balance, which can lead to
chemically invalid structures. Here we introduce CrysVCD (Crystal generator
with Valence-Constrained Design), a modular framework that integrates chemical
rules directly into the generative process. CrysVCD first employs a
transformer-based elemental language model to generate valence-balanced
compositions, followed by a diffusion model to generate crystal structures. The
valence constraint enables orders-of-magnitude more efficient chemical valence
checking, compared to pure data-driven approaches with post-screening. When
fine-tuned on stability metrics, CrysVCD achieves 85% thermodynamic stability
and 68% phonon stability. Moreover, CrysVCD supports conditional generation of
functional materials, enabling discovery of candidates such as high thermal
conductivity semiconductors and high-$\kappa$ dielectric compounds. Designed as
a general-purpose plugin, CrysVCD can be integrated into diverse generative
pipeline to promote chemical validity, offering a reliable, scientifically
grounded path for materials discovery.

</details>


### [424] [Towards trustworthy AI in materials mechanics through domain-guided attention](https://arxiv.org/abs/2507.20658)
*Jesco Talies,Eric Breitbarth,David Melching*

Main category: cond-mat.mtrl-sci

TL;DR: 提出注意力引导训练框架，结合可解释AI技术、定量评估和特定领域先验引导模型注意力，在裂纹尖端分割任务验证，提升泛化能力和解释可信度。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在高风险科学应用中可信度和鲁棒性的基本挑战。

Method: 提出注意力引导训练框架，结合可解释AI技术、定量评估和特定领域先验引导模型注意力。

Result: 在数字图像相关数据的语义裂纹尖端分割任务中验证，使模型关注物理相关区域，提升泛化能力。

Conclusion: 训练中对模型解释的特定领域反馈可增强模型泛化能力，该框架能带来更好泛化和更可信解释。

Abstract: Ensuring the trustworthiness and robustness of deep learning models remains a
fundamental challenge, particularly in high-stakes scientific applications. In
this study, we present a framework called attention-guided training that
combines explainable artificial intelligence techniques with quantitative
evaluation and domain-specific priors to guide model attention. We demonstrate
that domain specific feedback on model explanations during training can enhance
the model's generalization capabilities. We validate our approach on the task
of semantic crack tip segmentation in digital image correlation data which is a
key application in the fracture mechanical characterization of materials. By
aligning model attention with physically meaningful stress fields, such as
those described by Williams' analytical solution, attention-guided training
ensures that the model focuses on physically relevant regions. This finally
leads to improved generalization and more faithful explanations.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [425] [A Lightweight Deep Learning-based Model for Ranking Influential Nodes in Complex Networks](https://arxiv.org/abs/2507.19702)
*Mohammed A. Ramadhan,Abdulhakeem O. Mohammed*

Main category: cs.SI

TL;DR: 本文提出轻量级混合模型1D - CGS用于复杂网络中有效节点排序，在真实网络实验中准确性高且运行速度快。


<details>
  <summary>Details</summary>
Motivation: 现有复杂网络中识别有影响力节点的方法在准确性和计算效率间存在权衡，需改进。

Method: 提出1D - CGS模型，结合1D - CNN速度和GraphSAGE拓扑表示能力，用节点度和平均邻居度构建轻量级输入表示，将节点排序任务转化为回归问题，用SIR模型生成真实影响分数，先在合成网络训练，再应用于真实网络。

Result: 在十二个真实网络实验中，1D - CGS在排序准确性上显著优于传统中心性度量和深度学习模型，Kendall's Tau相关性平均提高4.73%，Jaccard相似度平均提高7.67%，单调指数平均0.99，且运行时间合理，比现有深度学习方法快。

Conclusion: 1D - CGS模型在复杂网络节点排序中准确性高、运行速度快，适用于大规模应用。

Abstract: Identifying influential nodes in complex networks is a critical task with a
wide range of applications across different domains. However, existing
approaches often face trade-offs between accuracy and computational efficiency.
To address these challenges, we propose 1D-CGS, a lightweight and effective
hybrid model that integrates the speed of one-dimensional convolutional neural
networks (1D-CNN) with the topological representation power of GraphSAGE for
efficient node ranking. The model uses a lightweight input representation built
on two straightforward and significant topological features: node degree and
average neighbor degree. These features are processed through 1D convolutions
to extract local patterns, followed by GraphSAGE layers to aggregate
neighborhood information. We formulate the node ranking task as a regression
problem and use the Susceptible-Infected-Recovered (SIR) model to generate
ground truth influence scores. 1D-CGS is initially trained on synthetic
networks generated by the Barabasi-Albert model and then applied to real world
networks for identifying influential nodes. Experimental evaluations on twelve
real world networks demonstrate that 1D-CGS significantly outperforms
traditional centrality measures and recent deep learning models in ranking
accuracy, while operating in very fast runtime. The proposed model achieves an
average improvement of 4.73% in Kendall's Tau correlation and 7.67% in Jaccard
Similarity over the best performing deep learning baselines. It also achieves
an average Monotonicity Index (MI) score 0.99 and produces near perfect rank
distributions, indicating highly unique and discriminative rankings.
Furthermore, all experiments confirm that 1D-CGS operates in a highly
reasonable time, running significantly faster than existing deep learning
methods, making it suitable for large scale applications.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [426] [Controllable Video-to-Music Generation with Multiple Time-Varying Conditions](https://arxiv.org/abs/2507.20627)
*Junxian Wu,Weitao You,Heda Zuo,Dengming Zhang,Pei Chen,Lingyun Sun*

Main category: cs.MM

TL;DR: 提出多条件引导视频到音乐生成框架，经实验证明优于现有方法，提升可控性和用户期望匹配度。


<details>
  <summary>Details</summary>
Motivation: 现有视频到音乐生成方法多以黑盒方式生成，难以满足用户期望。

Method: 采用两阶段训练策略，第一阶段引入细粒度特征选择模块和渐进时间对齐注意力机制；第二阶段开发动态条件融合模块和控制引导解码器模块。

Result: 在主观和客观评估中均优于现有视频到音乐生成管道。

Conclusion: 所提方法能显著增强音乐生成的可控性，更好地符合用户期望。

Abstract: Music enhances video narratives and emotions, driving demand for automatic
video-to-music (V2M) generation. However, existing V2M methods relying solely
on visual features or supplementary textual inputs generate music in a
black-box manner, often failing to meet user expectations. To address this
challenge, we propose a novel multi-condition guided V2M generation framework
that incorporates multiple time-varying conditions for enhanced control over
music generation. Our method uses a two-stage training strategy that enables
learning of V2M fundamentals and audiovisual temporal synchronization while
meeting users' needs for multi-condition control. In the first stage, we
introduce a fine-grained feature selection module and a progressive temporal
alignment attention mechanism to ensure flexible feature alignment. For the
second stage, we develop a dynamic conditional fusion module and a
control-guided decoder module to integrate multiple conditions and accurately
guide the music composition process. Extensive experiments demonstrate that our
method outperforms existing V2M pipelines in both subjective and objective
evaluations, significantly enhancing control and alignment with user
expectations.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [427] [Comparing Behavioural Cloning and Reinforcement Learning for Spacecraft Guidance and Control Networks](https://arxiv.org/abs/2507.19535)
*Harry Holt,Sebastien Origer,Dario Izzo*

Main category: eess.SY

TL;DR: 本文对用于训练连续推力航天器轨迹优化任务中制导与控制网络（G&CNETs）的行为克隆（BC）和强化学习（RL）方法进行系统评估，发现BC在复制专家策略行为上出色但受数据集限制，RL适应性强且能发现更优解。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺少对训练G&CNETs的BC和RL方法的直接比较，因此进行系统评估。

Method: 引入针对G&CNETs的新型RL训练框架，结合解耦的动作和控制频率以及奖励重新分配策略以稳定训练并进行公平比较。

Result: BC训练的G&CNETs能紧密复制专家策略行为，但受训练数据集质量和覆盖范围限制；RL训练的G&CNETs对随机条件适应性更强，能发现优于次优专家演示的解决方案。

Conclusion: BC和RL在训练G&CNETs上各有优劣，RL在适应性和发现更优解方面表现更好。

Abstract: Guidance & control networks (G&CNETs) provide a promising alternative to
on-board guidance and control (G&C) architectures for spacecraft, offering a
differentiable, end-to-end representation of the guidance and control
architecture. When training G&CNETs, two predominant paradigms emerge:
behavioural cloning (BC), which mimics optimal trajectories, and reinforcement
learning (RL), which learns optimal behaviour through trials and errors.
Although both approaches have been adopted in G&CNET related literature, direct
comparisons are notably absent. To address this, we conduct a systematic
evaluation of BC and RL specifically for training G&CNETs on continuous-thrust
spacecraft trajectory optimisation tasks. We introduce a novel RL training
framework tailored to G&CNETs, incorporating decoupled action and control
frequencies alongside reward redistribution strategies to stabilise training
and to provide a fair comparison. Our results show that BC-trained G&CNETs
excel at closely replicating expert policy behaviour, and thus the optimal
control structure of a deterministic environment, but can be negatively
constrained by the quality and coverage of the training dataset. In contrast
RL-trained G&CNETs, beyond demonstrating a superior adaptability to stochastic
conditions, can also discover solutions that improve upon suboptimal expert
demonstrations, sometimes revealing globally optimal strategies that eluded the
generation of training samples.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [428] [A Theory of $θ$-Expectations](https://arxiv.org/abs/2507.20353)
*Qian Qi*

Main category: math.PR

TL;DR: 本文针对随机微积分在模糊性下对非凸不确定性结构不敏感的问题，开发了可识别微积分的数学框架，介绍了θ - BSDE，在特定假设下证明适定性等并建立估值算子与非线性PDE的联系。


<details>
  <summary>Details</summary>
Motivation: 经典随机微积分在模糊性下的理论对非凸不确定性结构不敏感，存在可识别性困境，需开发对非凸几何敏感的可识别微积分框架。

Method: 引入θ - BSDE，基于全局解析假设（驱动函数存在唯一且全局Lipschitz最大化映射），用不动点论证证明适定性；对几何规则的模型，利用Malliavin微积分非退化条件证明最大化子在解路径上的唯一性。

Result: 建立了数学框架的适定性，证明了特定模型中最大化子在解路径上的唯一性，明确了路径性质与存在性证明所需全局正则性的逻辑差距，定义了动态一致期望并建立其与非线性PDE的联系。

Conclusion: 开发的数学框架可解决随机微积分在模糊性下对非凸结构的可识别性问题，相关估值算子有动态一致性且与非线性PDE有联系。

Abstract: The canonical theory of stochastic calculus under ambiguity, founded on
sub-additivity, is insensitive to non-convex uncertainty structures, leading to
an identifiability impasse. This paper develops a mathematical framework for an
identifiable calculus sensitive to non-convex geometry. We introduce the
$\theta$-BSDE, a class of backward stochastic differential equations where the
driver is determined by a pointwise maximization over a primitive, possibly
non-convex, uncertainty set. The system's tractability is predicated not on
convexity, but on a global analytic hypothesis: the existence of a unique and
globally Lipschitz maximizer map for the driver function. Under this
hypothesis, which carves out a tractable class of models, we establish
well-posedness via a fixed-point argument. For a distinct, geometrically
regular class of models, we prove a result of independent interest: under
non-degeneracy conditions from Malliavin calculus, the maximizer is unique
along any solution path, ensuring the model's internal consistency. We clarify
the fundamental logical gap between this pathwise property and the global
regularity required by our existence proof. The resulting valuation operator
defines a dynamically consistent expectation, and we establish its connection
to fully nonlinear PDEs via a Feynman-Kac formula.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [429] [Computation of Optimal Type-II Progressing Censoring Scheme Using Genetic Algorithm Approach](https://arxiv.org/abs/2507.20001)
*Ujjwal Roy,Ritwik Bhattacharya*

Main category: stat.AP

TL;DR: 提出基于遗传算法的元启发式算法用于大样本确定最优II型渐进删失方案，对大小样本都有效，还考虑敏感性分析。


<details>
  <summary>Details</summary>
Motivation: 现有确定最优II型渐进删失方案的建议限于小样本，穷举搜索对大样本不可行。

Method: 提出基于遗传算法的元启发式算法，采用基于成本函数的最优准则，进行敏感性分析。

Result: 算法对大小样本都能提供最优或近似最优解，最优准则具有尺度不变性。

Conclusion: 所提算法能有效解决大样本确定最优II型渐进删失方案的问题，敏感性分析可考虑参数值或成本系数不准确的影响。

Abstract: The experimenter must perform a legitimate search in the entire set of
feasible censoring schemes to identify the optimal type II progressive
censoring scheme, when applied to a life-testing experiment. Current
recommendations are limited to small sample sizes. Exhaustive search strategies
are not practically feasible for large sample sizes. This paper proposes a
meta-heuristic algorithm based on the genetic algorithm for large sample sizes.
The algorithm is found to provide optimal or near-optimal solutions for small
sample sizes and large sample sizes. Our suggested optimal criterion is based
on the cost function and is scale-invariant for both location-scale and
log-location-scale distribution families. To investigate how inaccurate
parameter values or cost coefficients may affect the optimal solution, a
sensitivity analysis is also taken into account.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [430] [Mosaic Selections: Managing and Optimizing User Selections for Scalable Data Visualization Systems](https://arxiv.org/abs/2507.19690)
*Jeffrey Heer,Dominik Moritz,Ron Pechuk*

Main category: cs.HC

TL;DR: 本文提出Mosaic Selections模型用于处理大规模数据集的实时交互，实现自动优化，基准测试显示有显著延迟改善。


<details>
  <summary>Details</summary>
Motivation: 交互式可视化难以支持大规模数据集的实时交互，需要优化以确保低延迟更新。

Method: 提出Mosaic Selections模型，为可视化和输入小部件的数据请求查询添加过滤谓词，分析查询和选择谓词以实现自动优化。

Result: 基准测试表明，基于选择的优化比未优化查询和Vega语言现有优化器有数量级的延迟改善。

Conclusion: Mosaic Selection模型为多可视化提供灵活、可互操作的过滤基础设施，能扩展到数百万甚至数十亿条记录。

Abstract: Though powerful tools for analysis and communication, interactive
visualizations often fail to support real-time interaction with large datasets
with millions or more records. To highlight and filter data, users indicate
values or intervals of interest. Such selections may span multiple components,
combine in complex ways, and require optimizations to ensure low-latency
updates. We describe Mosaic Selections, a model for representing, managing, and
optimizing user selections, in which one or more filter predicates are added to
queries that request data for visualizations and input widgets. By analyzing
both queries and selection predicates, Mosaic Selections enable automatic
optimizations, including pre-aggregating data to rapidly compute selection
updates. We contribute a formal description of our selection model and
optimization methods, and their implementation in the open-source Mosaic
architecture. Benchmark results demonstrate orders-of-magnitude latency
improvements for selection-based optimizations over unoptimized queries and
existing optimizers for the Vega language. The Mosaic Selection model provides
infrastructure for flexible, interoperable filtering across multiple
visualizations, alongside automatic optimizations to scale to millions and even
billions of records.

</details>


### [431] [Unlimited Editions: Documenting Human Style in AI Art Generation](https://arxiv.org/abs/2507.19497)
*Alex Leitch,Celia Chen*

Main category: cs.HC

TL;DR: 随着AI艺术生成发展，HCI研究多关注检测等问题，本文指出应关注艺术风格起源和演变。


<details>
  <summary>Details</summary>
Motivation: 现有HCI研究在AI艺术生成方面聚焦检测、真实性和自动化，未理解艺术价值产生方式。

Method: 通过研究历史先例，指出艺术风格是创造性挣扎的结果，当前AI系统未保留人类选择的来源。

Result: 提出HCI不仅要完善视觉输出，还应记录艺术风格起源和演变。

Conclusion: 为HCI在生成式AI研究提出新方向，聚焦风格谱系和创造性选择记录。

Abstract: As AI art generation becomes increasingly sophisticated, HCI research has
focused primarily on questions of detection, authenticity, and automation. This
paper argues that such approaches fundamentally misunderstand how artistic
value emerges from the concerns that drive human image production. Through
examination of historical precedents, we demonstrate that artistic style is not
only visual appearance but the resolution of creative struggle, as artists
wrestle with influence and technical constraints to develop unique ways of
seeing. Current AI systems flatten these human choices into reproducible
patterns without preserving their provenance. We propose that HCI's role lies
not only in perfecting visual output, but in developing means to document the
origins and evolution of artistic style as it appears within generated visual
traces. This reframing suggests new technical directions for HCI research in
generative AI, focused on automatic documentation of stylistic lineage and
creative choice rather than simple reproduction of aesthetic effects.

</details>


### [432] [TS-Insight: Visualizing Thompson Sampling for Verification and XAI](https://arxiv.org/abs/2507.19898)
*Parsa Vares,Éloi Durant,Jun Pang,Nicolas Médoc,Mohammad Ghoniem*

Main category: cs.HC

TL;DR: 介绍可视化分析工具TS - Insight，帮助开发者理解基于汤普森采样算法的内部决策机制，促进信任、调试和部署。


<details>
  <summary>Details</summary>
Motivation: 汤普森采样及其变体算法是“黑盒”，阻碍调试和信任，需要工具来理解其内部决策机制。

Method: 开发TS - Insight可视化分析工具，包含多个图表，追踪各臂的后验分布、证据计数和采样结果。

Result: 能实现对探索/利用动态的验证、诊断和解释。

Conclusion: 该工具可在复杂二元决策场景，特别是需要可解释决策的敏感领域，促进信任、调试和有效部署。

Abstract: Thompson Sampling (TS) and its variants are powerful Multi-Armed Bandit
algorithms used to balance exploration and exploitation strategies in active
learning. Yet, their probabilistic nature often turns them into a ``black
box'', hindering debugging and trust. We introduce TS-Insight, a visual
analytics tool explicitly designed to shed light on the internal decision
mechanisms of Thompson Sampling-based algorithms, for model developers. It
comprises multiple plots, tracing for each arm the evolving posteriors,
evidence counts, and sampling outcomes, enabling the verification, diagnosis,
and explainability of exploration/exploitation dynamics. This tool aims at
fostering trust and facilitating effective debugging and deployment in complex
binary decision-making scenarios especially in sensitive domains requiring
interpretable decision-making.

</details>


### [433] [The Architecture of Cognitive Amplification: Enhanced Cognitive Scaffolding as a Resolution to the Comfort-Growth Paradox in Human-AI Cognitive Integration](https://arxiv.org/abs/2507.19483)
*Giuseppe Riva*

Main category: cs.HC

TL;DR: AI发展为认知协作伙伴，存在舒适与成长悖论，提出增强认知支架框架解决，经多领域研究验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决AI作为认知协作伙伴时出现的舒适与成长悖论，避免认知自满，促进认知发展。

Method: 结合维果茨基理论、教育支架原则和AI伦理，构建包含渐进自主、自适应个性化、认知负荷优化三个维度的框架。

Result: 多领域研究支持该方法，能加速技能获取、提升自我调节和高阶思维，且有风险防范措施。

Conclusion: 增强认知支架框架在人机交互中优先考虑认知发展，保障自主思考和持续学习，实现真正的认知放大。

Abstract: AI systems now function as cognitive extensions, evolving from tools to
active cognitive collaborators within human-AI integrated systems. While these
systems can amplify cognition - enhancing problem-solving, learning, and
creativity - they present a fundamental "comfort-growth paradox": AI's
user-friendly nature may foster intellectual stagnation by minimizing cognitive
friction necessary for development. As AI aligns with user preferences and
provides frictionless assistance, it risks inducing cognitive complacency
rather than promoting growth. We introduce Enhanced Cognitive Scaffolding to
resolve this paradox - reconceptualizing AI from convenient assistant to
dynamic mentor. Drawing from Vygotskian theories, educational scaffolding
principles, and AI ethics, our framework integrates three dimensions: (1)
Progressive Autonomy, where AI support gradually fades as user competence
increases; (2) Adaptive Personalization, tailoring assistance to individual
needs and learning trajectories; and (3) Cognitive Load Optimization, balancing
mental effort to maximize learning while minimizing unnecessary complexity.
Research across educational, workplace, creative, and healthcare domains
supports this approach, demonstrating accelerated skill acquisition, improved
self-regulation, and enhanced higher-order thinking. The framework includes
safeguards against risks like dependency, skill atrophy, and bias
amplification. By prioritizing cognitive development over convenience in
human-AI interaction, Enhanced Cognitive Scaffolding offers a pathway toward
genuinely amplified cognition while safeguarding autonomous thought and
continuous learning.

</details>


### [434] [Creativity as a Human Right: Design Considerations for Computational Creativity Systems](https://arxiv.org/abs/2507.19485)
*Alayt Issak*

Main category: cs.HC

TL;DR: 本文研究《世界人权宣言》中的创造力，为计算创造力系统提出设计考量。


<details>
  <summary>Details</summary>
Motivation: 探究《世界人权宣言》中创造力以提出计算创造力系统的设计考量。

Method: 研究《世界人权宣言》三十条中的五条，并结合实例说明每条内容得出设计考量。

Result: 发现宣言从显著方面描述了创造力，将创造力视为第四代人权，且该代权利与计算创造力系统及与共享智能实体的交互性质相关。

Conclusion: 研究结果为创造力与计算创造力系统的关系奠定基础。

Abstract: We investigate creativity that is underlined in the Universal Declaration of
Human Rights (UDHR) to present design considerations for Computational
Creativity (CC) systems. We find this declaration to describe creativity in
salient aspects and bring to light creativity as a Human Right attributed to
the Fourth Generation of such rights. This generation of rights attributes CC
systems and the evolving nature of interaction with entities of shared
intelligence. Our methodology examines five of thirty articles from the UDHR
and demonstrates each article with actualizations concluding with design
considerations for each. We contribute our findings to ground the relationship
between creativity and CC systems.

</details>


### [435] [Confirmation bias: A challenge for scalable oversight](https://arxiv.org/abs/2507.19486)
*Gabriel Recchia,Chatrik Singh Mangat,Jinu Nyachhyon,Mridul Sharma,Callum Canavan,Dylan Epstein-Gross,Muhammed Abdulbari*

Main category: cs.HC

TL;DR: 研究简单监督协议性能，发现无整体优势，强调测试协议鲁棒性等方面的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决人类评估者有偏差会导致系统错误的问题，研究简单监督协议性能。

Method: 进行两项研究，重新分析以往研究数据。

Result: 测试协议无整体优势；显示双方论据可提高模型错误时的准确性；研究2中参与者研究后更相信系统答案；以往研究积极结果或因评估者有模型无的知识。

Conclusion: 强调测试监督协议对评估者偏差的鲁棒性、是否优于简单顺从模型、性能是否随问题难度和模型能力提升的重要性。

Abstract: Scalable oversight protocols aim to empower evaluators to accurately verify
AI models more capable than themselves. However, human evaluators are subject
to biases that can lead to systematic errors. We conduct two studies examining
the performance of simple oversight protocols where evaluators know that the
model is "correct most of the time, but not all of the time". We find no
overall advantage for the tested protocols, although in Study 1, showing
arguments in favor of both answers improves accuracy in cases where the model
is incorrect. In Study 2, participants in both groups become more confident in
the system's answers after conducting online research, even when those answers
are incorrect. We also reanalyze data from prior work that was more optimistic
about simple protocols, finding that human evaluators possessing knowledge
absent from models likely contributed to their positive results--an advantage
that diminishes as models continue to scale in capability. These findings
underscore the importance of testing the degree to which oversight protocols
are robust to evaluator biases, whether they outperform simple deference to the
model under evaluation, and whether their performance scales with increasing
problem difficulty and model capability.

</details>


### [436] [ChartGen: Scaling Chart Understanding Via Code-Guided Synthetic Chart Generation](https://arxiv.org/abs/2507.19492)
*Jovana Kondic,Pengyuan Li,Dhiraj Joshi,Zexue He,Shafiq Abedin,Jennifer Sun,Ben Wiesel,Eli Schwartz,Ahmed Nassar,Bo Wu,Assaf Arbelle,Aude Oliva,Dan Gutfreund,Leonid Karlinsky,Rogerio Feris*

Main category: cs.HC

TL;DR: 提出ChartGen自动管道生成合成图表数据集并评估VLMs，推动图表理解和代码生成。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基准主要关注图表问答和总结，缺少对图表到代码重建任务的关注。

Method: 提出ChartGen管道，用VLM重建图像为Python脚本，用代码导向的LLM迭代增强脚本。

Result: 从13K种子图表图像创建222.5K独特图表 - 图像代码对，构建开源数据集，评估六个开源VLMs。

Conclusion: 当前VLMs在图表到代码重建任务上有很大进步空间，发布相关资源加速研究。

Abstract: Chart-to-code reconstruction -- the task of recovering executable plotting
scripts from chart images -- provides important insights into a model's ability
to ground data visualizations in precise, machine-readable form. Yet many
existing multimodal benchmarks largely focus primarily on answering questions
about charts or summarizing them. To bridge this gap, we present ChartGen, a
fully-automated pipeline for code-guided synthetic chart generation. Starting
from seed chart images, ChartGen (i) prompts a vision-language model (VLM) to
reconstruct each image into a python script, and (ii) iteratively augments that
script with a code-oriented large language model (LLM). Using ChartGen, we
create 222.5K unique chart-image code pairs from 13K seed chart images, and
present an open-source synthetic chart dataset covering 27 chart types, 11
plotting libraries, and multiple data modalities (image, code, text, CSV,
DocTags). From this corpus, we curate a held-out chart-to-code evaluation
subset of 4.3K chart image-code pairs, and evaluate six open-weight VLMs (3B -
26B parameters), highlighting substantial room for progress. We release the
pipeline, prompts, and the dataset to help accelerate efforts towards robust
chart understanding and vision-conditioned code generation:
https://github.com/SD122025/ChartGen/

</details>


### [437] [Simulating Human Behavior with the Psychological-mechanism Agent: Integrating Feeling, Thought, and Action](https://arxiv.org/abs/2507.19495)
*Qing Dong,Pengyuan Liu,Dong Yu,Chen Kang*

Main category: cs.HC

TL;DR: 现有生成式智能体情感建模简化，本文提出PSYA框架，经模拟实验验证其能生成更优行为，为智能体建模及心理实验提供新途径。


<details>
  <summary>Details</summary>
Motivation: 现有生成式智能体框架简化情感建模，聚焦特定任务，限制模拟真实性，需更准确模拟人类行为的方法。

Method: 提出基于认知三角的PSYA框架，含感觉、思维和行动三个核心模块，通过日常生活模拟和选取五个经典心理实验，扩展评估指标为自我、个体和群体影响来评估。

Result: PSYA框架生成的行为更自然、一致、多样且可信，成功复现人类实验结果。

Conclusion: 为生成式智能体提供更丰富准确的情感和认知建模方法，可替代人类参与者用于心理实验。

Abstract: Generative agents have made significant progress in simulating human
behavior, but existing frameworks often simplify emotional modeling and focus
primarily on specific tasks, limiting the authenticity of the simulation. Our
work proposes the Psychological-mechanism Agent (PSYA) framework, based on the
Cognitive Triangle (Feeling-Thought-Action), designed to more accurately
simulate human behavior. The PSYA consists of three core modules: the Feeling
module (using a layer model of affect to simulate changes in short-term,
medium-term, and long-term emotions), the Thought module (based on the Triple
Network Model to support goal-directed and spontaneous thinking), and the
Action module (optimizing agent behavior through the integration of emotions,
needs and plans). To evaluate the framework's effectiveness, we conducted daily
life simulations and extended the evaluation metrics to self-influence,
one-influence, and group-influence, selection five classic psychological
experiments for simulation. The results show that the PSYA framework generates
more natural, consistent, diverse, and credible behaviors, successfully
replicating human experimental outcomes. Our work provides a richer and more
accurate emotional and cognitive modeling approach for generative agents and
offers an alternative to human participants in psychological experiments.

</details>


### [438] [ChatMyopia: An AI Agent for Pre-consultation Education in Primary Eye Care Settings](https://arxiv.org/abs/2507.19498)
*Yue Wu,Xiaolan Chen,Weiyi Zhang,Shunming Liu,Wing Man Rita Sum,Xinyuan Wu,Xianwen Shang,Chea-su Kee,Mingguang He,Danli Shi*

Main category: cs.HC

TL;DR: 介绍基于大语言模型的AI代理ChatMyopia，它能处理近视相关图文查询，经多项验证有效，随机对照试验显示可提升患者满意度，有望用于初级眼科护理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗沟通中面临可解释性和多任务集成挑战，尤其在近视领域，其作为患者教育工具的实际效果未被证实，因此开发ChatMyopia。

Method: ChatMyopia集成图像分类工具和基于文献、专家共识及临床指南的检索增强知识库，通过近视性黄斑病变分级任务、单问题测试和人工评估验证，还开展随机对照试验。

Result: ChatMyopia能提供个性化、准确、安全的近视相关查询响应，具有高可扩展性和可解释性；随机对照试验中，与传统手册相比显著提升患者满意度，在准确性、同理心、疾病认知和医患沟通方面增强患者教育。

Conclusion: ChatMyopia有潜力作为有价值的补充，提升初级眼科护理中患者教育水平和医疗服务满意度。

Abstract: Large language models (LLMs) show promise for tailored healthcare
communication but face challenges in interpretability and multi-task
integration particularly for domain-specific needs like myopia, and their
real-world effectiveness as patient education tools has yet to be demonstrated.
Here, we introduce ChatMyopia, an LLM-based AI agent designed to address text
and image-based inquiries related to myopia. To achieve this, ChatMyopia
integrates an image classification tool and a retrieval-augmented knowledge
base built from literature, expert consensus, and clinical guidelines. Myopic
maculopathy grading task, single question examination and human evaluations
validated its ability to deliver personalized, accurate, and safe responses to
myopia-related inquiries with high scalability and interpretability. In a
randomized controlled trial (n=70, NCT06607822), ChatMyopia significantly
improved patient satisfaction compared to traditional leaflets, enhancing
patient education in accuracy, empathy, disease awareness, and patient-eyecare
practitioner communication. These findings highlight ChatMyopia's potential as
a valuable supplement to enhance patient education and improve satisfaction
with medical services in primary eye care settings.

</details>


### [439] [Gaze-Aware AI: Mathematical modeling of epistemic experience of the Marginalized for Human-Computer Interaction & AI Systems](https://arxiv.org/abs/2507.19500)
*Omkar Suresh Hatti*

Main category: cs.HC

TL;DR: 本文尝试量化人类潜意识中为适应主流文化规范而改变真实自我表达的情况，通过研究不同群体的注视现象，提出GPI - Diff综合指标的数学公式，得出训练大语言模型的方程并提出肯定和包容的人机交互观点。


<details>
  <summary>Details</summary>
Motivation: 人工智能发展为营造社会心理空间提供机会，探讨人类潜意识改变自我表达情况以促进社会和谐。

Method: 运用后现代哲学和心理学概念，研究不同边缘化和交叉群体的注视现象，分析Reddit帖子，提出GPI - Diff综合指标数学公式。

Result: 得出训练大语言模型的方程，基于此提出肯定和包容的人机交互观点。

Conclusion: 基于神经可塑性原则，支持肯定和包容的人机交互观点。

Abstract: The proliferation of artificial intelligence provides an opportunity to
create psychological spaciousness in society. Spaciousness is defined as the
ability to hold diverse interpersonal interactions and forms the basis for
vulnerability that leads to authenticity that leads to prosocial behaviors and
thus to societal harmony. This paper demonstrates an attempt to quantify, the
human conditioning to subconsciously modify authentic self-expression to fit
the norms of the dominant culture. Gaze is explored across various marginalized
and intersectional groups, using concepts from postmodern philosophy and
psychology. The effects of gaze are studied through analyzing a few redacted
Reddit posts, only to be discussed in discourse and not endorsement. A
mathematical formulation for the Gaze Pressure Index (GPI)-Diff Composite
Metric is presented to model the analysis of two sets of conversational spaces
in relation to one another. The outcome includes an equation to train Large
Language Models (LLMs) - the working mechanism of AI products such as Chat-GPT;
and an argument for affirming and inclusive HCI, based on the equation, is
presented. The argument is supported by a few principles of Neuro-plasticity,
The brain's lifelong capacity to rewire.

</details>


### [440] [Visual Analytics Using Tensor Unified Linear Comparative Analysis](https://arxiv.org/abs/2507.19988)
*Naoki Okami,Kazuki Miyake,Naohisa Sakamoto,Jorji Nonaka,Takanori Fujiwara*

Main category: cs.HC

TL;DR: 提出新的张量分解方法TULCA，支持灵活的张量比较分析，还引入可视化方法并集成到可视化分析界面，通过计算评估和案例研究验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有张量分解方法不支持灵活的比较分析，需要新方法解决该问题。

Method: 扩展ULCA提出TULCA，集成判别分析和对比学习方案进行张量分解，引入可视化方法，将功能集成到可视化分析界面。

Result: 通过计算评估和两个案例研究证明了TULCA和可视化分析界面的有效性。

Conclusion: TULCA能有效支持灵活的张量比较分析，可视化界面有助于分析师解释和完善结果。

Abstract: Comparing tensors and identifying their (dis)similar structures is
fundamental in understanding the underlying phenomena for complex data. Tensor
decomposition methods help analysts extract tensors' essential characteristics
and aid in visual analytics for tensors. In contrast to dimensionality
reduction (DR) methods designed only for analyzing a matrix (i.e., second-order
tensor), existing tensor decomposition methods do not support flexible
comparative analysis. To address this analysis limitation, we introduce a new
tensor decomposition method, named tensor unified linear comparative analysis
(TULCA), by extending its DR counterpart, ULCA, for tensor analysis. TULCA
integrates discriminant analysis and contrastive learning schemes for tensor
decomposition, enabling flexible comparison of tensors. We also introduce an
effective method to visualize a core tensor extracted from TULCA into a set of
2D visualizations. We integrate TULCA's functionalities into a visual analytics
interface to support analysts in interpreting and refining the TULCA results.
We demonstrate the efficacy of TULCA and the visual analytics interface with
computational evaluations and two case studies, including an analysis of log
data collected from a supercomputer.

</details>


### [441] [Understanding Bias in Perceiving Dimensionality Reduction Projections](https://arxiv.org/abs/2507.20805)
*Seoyoung Doh,Hyeon Jeon,Sungbok Shin,Ghulam Jilani Quadri,Nam Wook Kim,Jinwook Seo*

Main category: cs.HC

TL;DR: 研究验证了视觉趣味性偏差存在于降维投影选择中，并探讨缓解策略


<details>
  <summary>Details</summary>
Motivation: 实际中从业者在选择降维技术时更看重美学等非结构忠实性因素，需验证偏差存在并解释原因

Method: 开展用户研究

Result: 视觉趣味性会影响从业者选择投影，颜色编码标签和短曝光时间会加剧偏差

Conclusion: 基于研究结果讨论了缓解对降维投影感知和解释偏差的策略

Abstract: Selecting the dimensionality reduction technique that faithfully represents
the structure is essential for reliable visual communication and analytics. In
reality, however, practitioners favor projections for other attractions, such
as aesthetics and visual saliency, over the projection's structural
faithfulness, a bias we define as visual interestingness. In this research, we
conduct a user study that (1) verifies the existence of such bias and (2)
explains why the bias exists. Our study suggests that visual interestingness
biases practitioners' preferences when selecting projections for analysis, and
this bias intensifies with color-encoded labels and shorter exposure time.
Based on our findings, we discuss strategies to mitigate bias in perceiving and
interpreting DR projections.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [442] [Comparing and Scaling fMRI Features for Brain-Behavior Prediction](https://arxiv.org/abs/2507.20601)
*Mikkel Schöttner Sieler,Thomas A. W. Bolton,Jagruti Patel,Patric Hagmann*

Main category: q-bio.NC

TL;DR: 比较静息态功能磁共振成像9种特征亚型用于行为预测，研究其在不同样本量和扫描时间组合下的缩放特性，确定FC等特征的预测效果并给出研究启示。


<details>
  <summary>Details</summary>
Motivation: 从神经影像模式预测行为变量有助于开发精神和神经疾病的神经影像生物标志物，关键在于提取合适特征，不同特征预测效果和缩放特性不同。

Method: 比较从静息态功能磁共振成像记录中提取的9种特征亚型，对人类连接组项目青年成人数据集的979名受试者进行研究，预测心理健康、认知等指标，研究特征在不同样本量和扫描时间组合下的缩放特性。

Result: FC是预测认知、年龄和性别的最佳特征；图功率谱密度是预测认知和年龄的次佳特征；预测性别时，低通图滤波耦合FC略优于简单FC变体；其他目标无显著预测效果；表现较好的特征有更高性能储备。

Conclusion: FC是行为预测的稳健特征，GSP和基于变异性的指标也有潜力；数据采集时平衡样本量和扫描时间很重要；讨论了未来预测研究在采集策略和样本构成方面的影响。

Abstract: Predicting behavioral variables from neuroimaging modalities such as magnetic
resonance imaging (MRI) has the potential to allow the development of
neuroimaging biomarkers of mental and neurological disorders. A crucial
processing step to this aim is the extraction of suitable features. These can
differ in how well they predict the target of interest, and how this prediction
scales with sample size and scan time. Here, we compare nine feature subtypes
extracted from resting-state functional MRI recordings for behavior prediction,
ranging from regional measures of functional activity to functional
connectivity (FC) and metrics derived with graph signal processing (GSP), a
principled approach for the extraction of structure-informed functional
features. We study 979 subjects from the Human Connectome Project Young Adult
dataset, predicting summary scores for mental health, cognition, processing
speed, and substance use, as well as age and sex. The scaling properties of the
features are investigated for different combinations of sample size and scan
time. FC comes out as the best feature for predicting cognition, age, and sex.
Graph power spectral density is the second best for predicting cognition and
age, while for sex, variability-based features show potential as well. When
predicting sex, the low-pass graph filtered coupled FC slightly outperforms the
simple FC variant. None of the other targets were predicted significantly. The
scaling results point to higher performance reserves for the better-performing
features. They also indicate that it is important to balance sample size and
scan time when acquiring data for prediction studies. The results confirm FC as
a robust feature for behavior prediction, but also show the potential of GSP
and variability-based measures. We discuss the implications for future
prediction studies in terms of strategies for acquisition and sample
composition.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [443] [Trivial Trojans: How Minimal MCP Servers Enable Cross-Tool Exfiltration of Sensitive Data](https://arxiv.org/abs/2507.19880)
*Nicola Croce,Tobin South*

Main category: cs.CR

TL;DR: 论文指出MCP虽促进AI工具集成，但存在安全漏洞，简单攻击者就能利用其信任模型窃取金融数据，研究揭示该生态的安全缺口并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: MCP在实现AI工具与外部服务连接时引入了新的攻击途径，而这些途径尚未被充分研究，因此要探究其安全漏洞。

Method: 构建概念验证攻击，利用伪装成良性功能的恶意MCP服务器，发现并利用合法银行工具窃取用户账户余额。

Result: 发现MCP生态存在关键安全缺口，基于MCP的攻击门槛低，本科生水平的攻击者就能实施有效攻击。

Conclusion: 当前MCP实现易遭受跨服务器攻击，需采取即时缓解措施和改进协议来保障该新兴生态系统的安全。

Abstract: The Model Context Protocol (MCP) represents a significant advancement in
AI-tool integration, enabling seamless communication between AI agents and
external services. However, this connectivity introduces novel attack vectors
that remain largely unexplored. This paper demonstrates how unsophisticated
threat actors, requiring only basic programming skills and free web tools, can
exploit MCP's trust model to exfiltrate sensitive financial data. We present a
proof-of-concept attack where a malicious weather MCP server, disguised as
benign functionality, discovers and exploits legitimate banking tools to steal
user account balances. The attack chain requires no advanced technical
knowledge, server infrastructure, or monetary investment. The findings reveal a
critical security gap in the emerging MCP ecosystem: while individual servers
may appear trustworthy, their combination creates unexpected cross-server
attack surfaces. Unlike traditional cybersecurity threats that assume
sophisticated adversaries, our research shows that the barrier to entry for
MCP-based attacks is alarmingly low. A threat actor with undergraduate-level
Python knowledge can craft convincing social engineering attacks that exploit
the implicit trust relationships MCP establishes between AI agents and tool
providers. This work contributes to the nascent field of MCP security by
demonstrating that current MCP implementations allow trivial cross-server
attacks and proposing both immediate mitigations and protocol improvements to
secure this emerging ecosystem.

</details>


### [444] [Policy-Driven AI in Dataspaces: Taxonomy, Explainability, and Pathways for Compliant Innovation](https://arxiv.org/abs/2507.20014)
*Joydeep Chandra,Satyam Kumar Navneet*

Main category: cs.CR

TL;DR: 本文全面回顾隐私保护与策略感知AI技术，提出分类方法，分析关键指标，指出研究差距并给出未来方向，为数据空间开发可信、高效且合规的AI系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: AI驱动的数据空间在隐私、性能和政策合规方面存在挑战，需有效应对。

Method: 对隐私保护和策略感知AI技术进行全面回顾，提出基于隐私水平、性能影响和合规复杂性的分类方法，分析关键性能指标。

Result: 确定了研究差距，如缺乏标准化隐私 - 性能指标等，提出未来方向框架。

Conclusion: 综合技术、伦理和监管视角，可为数据空间开发可信、高效且合规的AI系统，促进安全负责的数据驱动生态创新。

Abstract: As AI-driven dataspaces become integral to data sharing and collaborative
analytics, ensuring privacy, performance, and policy compliance presents
significant challenges. This paper provides a comprehensive review of
privacy-preserving and policy-aware AI techniques, including Federated
Learning, Differential Privacy, Trusted Execution Environments, Homomorphic
Encryption, and Secure Multi-Party Computation, alongside strategies for
aligning AI with regulatory frameworks such as GDPR and the EU AI Act. We
propose a novel taxonomy to classify these techniques based on privacy levels,
performance impacts, and compliance complexity, offering a clear framework for
practitioners and researchers to navigate trade-offs. Key performance metrics
-- latency, throughput, cost overhead, model utility, fairness, and
explainability -- are analyzed to highlight the multi-dimensional optimization
required in dataspaces. The paper identifies critical research gaps, including
the lack of standardized privacy-performance KPIs, challenges in explainable AI
for federated ecosystems, and semantic policy enforcement amidst regulatory
fragmentation. Future directions are outlined, proposing a conceptual framework
for policy-driven alignment, automated compliance validation, standardized
benchmarking, and integration with European initiatives like GAIA-X, IDS, and
Eclipse EDC. By synthesizing technical, ethical, and regulatory perspectives,
this work lays the groundwork for developing trustworthy, efficient, and
compliant AI systems in dataspaces, fostering innovation in secure and
responsible data-driven ecosystems.

</details>


### [445] [Hot-Swap MarkBoard: An Efficient Black-box Watermarking Approach for Large-scale Model Distribution](https://arxiv.org/abs/2507.20650)
*Zhicheng Zhang,Peizhuo Lv,Mengke Wan,Jiang Fang,Diandian Guo,Yezeng Chen,Yinlong Liu,Wei Ma,Jiyan Sun,Liru Geng*

Main category: cs.CR

TL;DR: 提出Hot - Swap MarkBoard水印方法保护设备端AI模型知识产权，经实验验证效率和适应性优。


<details>
  <summary>Details</summary>
Motivation: 设备端AI部署有知识产权风险，现有保护方案不适用于大规模分发场景，且修改水印需重新训练模型。

Method: 将多个水印独立嵌入多分支低秩自适应（LoRA）模块编码用户特定的n位二进制签名，通过分支交换实现水印定制，用参数混淆机制防止水印被移除。

Result: 方法支持黑盒验证，兼容多种模型架构和任务，实验中验证准确率达100%。

Conclusion: 该方法比现有方法效率和适应性更优。

Abstract: Recently, Deep Learning (DL) models have been increasingly deployed on
end-user devices as On-Device AI, offering improved efficiency and privacy.
However, this deployment trend poses more serious Intellectual Property (IP)
risks, as models are distributed on numerous local devices, making them
vulnerable to theft and redistribution. Most existing ownership protection
solutions (e.g., backdoor-based watermarking) are designed for cloud-based
AI-as-a-Service (AIaaS) and are not directly applicable to large-scale
distribution scenarios, where each user-specific model instance must carry a
unique watermark. These methods typically embed a fixed watermark, and
modifying the embedded watermark requires retraining the model. To address
these challenges, we propose Hot-Swap MarkBoard, an efficient watermarking
method. It encodes user-specific $n$-bit binary signatures by independently
embedding multiple watermarks into a multi-branch Low-Rank Adaptation (LoRA)
module, enabling efficient watermark customization without retraining through
branch swapping. A parameter obfuscation mechanism further entangles the
watermark weights with those of the base model, preventing removal without
degrading model performance. The method supports black-box verification and is
compatible with various model architectures and DL tasks, including
classification, image generation, and text generation. Extensive experiments
across three types of tasks and six backbone models demonstrate our method's
superior efficiency and adaptability compared to existing approaches, achieving
100\% verification accuracy.

</details>


### [446] [Testbed and Software Architecture for Enhancing Security in Industrial Private 5G Networks](https://arxiv.org/abs/2507.20873)
*Song Son Ha,Florian Foerster,Thomas Robert Doebbert,Tim Kittel,Dominik Merli,Gerd Scholl*

Main category: cs.CR

TL;DR: 针对工业4.0时代5G网络安全挑战，本文提出强化私有5G网络安全的测试平台和软件架构。


<details>
  <summary>Details</summary>
Motivation: 工业4.0时代对安全高效通信系统需求增长，5G网络部署存在安全挑战，需先进强大解决方案应对网络威胁。

Method: 提出一个测试平台和软件架构。

Result: 未提及。

Conclusion: 未提及。

Abstract: In the era of Industry 4.0, the growing need for secure and efficient
communication systems has driven the development of fifth-generation (5G)
networks characterized by extremely low latency, massive device connectivity
and high data transfer speeds. However, the deployment of 5G networks presents
significant security challenges, requiring advanced and robust solutions to
counter increasingly sophisticated cyber threats. This paper proposes a testbed
and software architecture to strengthen the security of Private 5G Networks,
particularly in industrial communication environments.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [447] [Effective Bayesian Modeling of Large Spatiotemporal Count Data Using Autoregressive Gamma Processes](https://arxiv.org/abs/2507.19915)
*Yifan Cheng,Cheng Li*

Main category: stat.ME

TL;DR: 提出新的时空计数数据贝叶斯建模策略，可高效后验采样，经实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 以往时空计数数据模型在对数高斯与泊松非共轭时，拟合和后验采样存在近似与计算挑战，需新策略。

Method: 构建时空自回归伽马过程，分解潜在泊松变量实现全共轭吉布斯采样，设计稀疏空间依赖结构降低计算复杂度。

Result: 模型能在新时空位置预测表现良好，模拟实验和真实数据分析验证了参数估计、模型拟合和样本外预测能力。

Conclusion: 新的贝叶斯建模策略能高效后验采样，在时空计数数据预测等方面有出色表现。

Abstract: We put forward a new Bayesian modeling strategy for spatiotemporal count data
that enables efficient posterior sampling. Most previous models for such data
decompose logarithms of the response Poisson rates into fixed effects and
spatial random effects, where the latter is typically assumed to follow a
latent Gaussian process, the conditional autoregressive model, or the intrinsic
conditional autoregressive model. Since log-Gaussian is not conjugate to
Poisson, such implementations must resort to either approximation methods like
INLA or Metropolis moves on latent states in MCMC algorithms for model fitting
and exhibit several approximation and posterior sampling challenges. Instead of
modeling logarithms of spatiotemporal frailties jointly as a Gaussian process,
we construct a spatiotemporal autoregressive gamma process guaranteed
stationary across the time dimension. We decompose latent Poisson variables to
permit fully conjugate Gibbs sampling of spatiotemporal frailties and design a
sparse spatial dependence structure to get a linear computational complexity
that facilitates efficient posterior computation. Our model permits convenient
Bayesian predictive machinery based on posterior samples that delivers
satisfactory performance in predicting at new spatial locations and time
intervals. We have performed extensive simulation experiments and real data
analyses, which corroborated our model's accurate parameter estimation, model
fitting, and out-of-sample prediction capabilities.

</details>


### [448] [Clustering data with values missing at random using scale mixtures of multivariate skew-normal distributions](https://arxiv.org/abs/2507.20329)
*Jason Pillay,Cristina Tortora,Antonio Punzo,Andriette Bekker*

Main category: stat.ME

TL;DR: 本文将FMSMSN族扩展以处理MAR机制下的缺失数据，提出适配算法，模拟实验展示其灵活性，还应用于实际数据。


<details>
  <summary>Details</summary>
Motivation: 基于模型的聚类处理缺失数据有挑战，尤其是数据有偏态和重尾时，以往工作有局限。

Method: 扩展FMSMSN族以适应缺失数据，推导缺失成分分布性质，提出针对不完整观测的增强EM型算法。

Result: 模拟实验显示FMSMSN族在不同缺失值比例下聚类性能和参数恢复方面有灵活性，考虑了样本量和簇接近程度的影响。

Conclusion: 所提方法有实用性，通过应用FMSMSN族特例到全球CO2排放数据得以体现。

Abstract: Handling missing data is a major challenge in model-based clustering,
especially when the data exhibit skewness and heavy tails. We address this by
extending the finite mixture of scale mixtures of multivariate skew-normal
(FMSMSN) family to accommodate incomplete data under a missing at random (MAR)
mechanism. Unlike previous work that is limited to one of the special cases of
the FMSMSN family, our method offers a cluster analysis methodology for the
entire family that accounts for skewness and excess kurtosis amidst data with
missing values. The multivariate skew-normal distribution, as parameterised by
\cite{azzalini1996} and \cite{arnoldbeaver} includes the normal distribution as
a special case, which ensures that our method is flexible toward existing
symmetric model-based clustering techniques under a normality assumption. We
derive the distributional properties of the missing components of the data and
propose an augmented EM-type algorithm tailored for incomplete observations.
The modified E-step yields closed-form expressions for the conditional
expectations of the missing values. The simulation experiments showcase the
flexibility of the FMSMSN family in both clustering performance and parameter
recovery for varying percentages of missing values, while incorporating the
effects of sample size and cluster proximity. Finally, we illustrate the
practical utility of the proposed method by applying special cases of the
FMSMSN family to global CO2 emissions data.

</details>


### [449] [Independence Testing for Mixed Data](https://arxiv.org/abs/2507.20609)
*Dana Bucalo Jelić,Marija Cuparić,Bojana Milošević*

Main category: stat.ME

TL;DR: 本文考虑混合类型数据独立性检验问题，提出两类检验统计量，扩展到多元情境，基于Baringhaus - Gaigall变换，证明渐近性质并通过功效研究表明方法有竞争力和灵活性。


<details>
  <summary>Details</summary>
Motivation: 解决结合计数变量和正的绝对连续变量的混合类型数据的独立性检验问题。

Method: 在二元设置中引入两类检验统计量，基于Baringhaus - Gaigall变换将其扩展到多元情境。

Result: 建立了所得检验的渐近性质，通过大量功效研究证明方法具有竞争力和灵活性。

Conclusion: 所提出的方法在混合类型数据独立性检验方面是可行且有效的。

Abstract: We consider the problem of testing independence in mixed-type data that
combine count variables with positive, absolutely continuous variables. We
first introduce two distinct classes of test statistics in the bivariate
setting, designed to test independence between the components of a bivariate
mixed-type vector. These statistics are then extended to the multivariate
context to accommodate: (i) testing independence between vectors of different
types and possibly different dimensions, and (ii) testing total independence
among all components of vectors with different types. The construction is based
on the recently introduced Baringhaus-Gaigall transformation, which
characterizes the joint distribution of such data. We establish the asymptotic
properties of the resulting tests and, through an extensive power study,
demonstrate that the proposed approach is both competitive and flexible.

</details>


### [450] [Discrete Gaussian Vector Fields On Meshes](https://arxiv.org/abs/2507.20024)
*Michael Gillan,Stefan Siegert,Ben Youngman*

Main category: stat.ME

TL;DR: 本文基于三角网格发展了向量值数据的离散固有高斯过程模型，并应用于风数据降尺度和海洋洋流推断。


<details>
  <summary>Details</summary>
Motivation: 环境向量值数据底层场连续，但观测离散，需合适方法处理离散数据。

Method: 将感兴趣区域视为二维流形并用三角网格表示，基于网格定义的离散微分算子开发离散固有高斯过程。

Result: 模型能捕捉谐波流、纳入边界条件、处理非平稳数据。

Conclusion: 模型灵活实用，可用于全球风数据降尺度和有界区域海洋洋流推断。

Abstract: Though the underlying fields associated with vector-valued environmental data
are continuous, observations themselves are discrete. For example, climate
models typically output grid-based representations of wind fields or ocean
currents, and these are often downscaled to a discrete set of points. By
treating the area of interest as a two-dimensional manifold that can be
represented as a triangular mesh and embedded in Euclidean space, this work
shows that discrete intrinsic Gaussian processes for vector-valued data can be
developed from discrete differential operators defined with respect to a mesh.
These Gaussian processes account for the geometry and curvature of the manifold
whilst also providing a flexible and practical formulation that can be readily
applied to any two-dimensional mesh. We show that these models can capture
harmonic flows, incorporate boundary conditions, and model non-stationary data.
Finally, we apply these models to downscaling stationary and non-stationary
gridded wind data on the globe, and to inference of ocean currents from sparse
observations in bounded domains.

</details>


### [451] [Lasso Penalization for High-Dimensional Beta Regression Models: Computation, Analysis, and Inference](https://arxiv.org/abs/2507.20079)
*Niloofar Ramezani,Martin Slawski*

Main category: stat.ME

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Beta regression is commonly employed when the outcome variable is a
proportion. Since its conception, the approach has been widely used in
applications spanning various scientific fields. A series of extensions have
been proposed over time, several of which address variable selection and
penalized estimation, e.g., with an $\ell_1$-penalty (LASSO). However, a
theoretical analysis of this popular approach in the context of Beta regression
with high-dimensional predictors is lacking. In this paper, we aim to close
this gap. A particular challenge arises from the non-convexity of the
associated negative log-likelihood, which we address by resorting to a
framework for analyzing stationary points in a neighborhood of the target
parameter. Leveraging this framework, we derive a non-asymptotic bound on the
$\ell_1$-error of such stationary points. In addition, we propose a debiasing
approach to construct confidence intervals for the regression parameters. A
proximal gradient algorithm is devised for optimizing the resulting penalized
negative log-likelihood function. Our theoretical analysis is corroborated via
simulation studies, and a real data example concerning the prediction of
county-level proportions of incarceration is presented to showcase the
practical utility of our methodology.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [452] [State evolution beyond first-order methods I: Rigorous predictions and finite-sample guarantees](https://arxiv.org/abs/2507.19611)
*Michael Celentano,Chen Cheng,Ashwin Pananjady,Kabir Aladin Verchand*

Main category: math.ST

TL;DR: 开发工具包对一类高维非凸优化问题迭代算法进行精确分析，给出更通用算法的状态演化预测及有限样本保证。


<details>
  <summary>Details</summary>
Motivation: 先前工作仅对广义一阶方法低维统计量可用状态演化预测，本文聚焦为更通用类算法进行预测。

Method: 为迭代由一阶和鞍点更新构成的方法提供状态演化，开发技术工具包，包括希尔伯特空间提升技术和结合特定方法与不等式。

Result: 建立了非坐标可分更新时的严格状态演化预测，以及有限样本下经验更新与状态演化偏差的保证。

Conclusion: 开发的技术工具包在相关问题中可能有用。

Abstract: We develop a toolbox for exact analysis of iterative algorithms on a class of
high-dimensional nonconvex optimization problems with random data. While prior
work has shown that low-dimensional statistics of (generalized) first-order
methods can be predicted by a deterministic recursion known as state evolution,
our focus is on developing such a prediction for a more general class of
algorithms. We provide a state evolution for any method whose iterations are
given by (possibly interleaved) first-order and saddle point updates, showing
two main results. First, we establish a rigorous state evolution prediction
that holds even when the updates are not coordinate-wise separable. Second, we
establish finite-sample guarantees bounding the deviation of the empirical
updates from the established state evolution. In the process, we develop a
technical toolkit that may prove useful in related problems. One component of
this toolkit is a general Hilbert space lifting technique to prove existence
and uniqueness of a convenient parameterization of the state evolution. Another
component of the toolkit combines a generic application of Bolthausen's
conditioning method with a sequential variant of Gordon's Gaussian comparison
inequality, and provides additional ingredients that enable a general
finite-sample analysis.

</details>


### [453] [Extreme value theory for singular subspace estimation in the matrix denoising model](https://arxiv.org/abs/2507.19978)
*Junhyung Chang,Joshua Cape*

Main category: math.ST

TL;DR: 研究矩阵去噪模型中细粒度奇异子空间估计，建立领先样本和总体奇异向量对齐差的最大欧氏行范数渐近分布，用于低秩信号结构假设检验，提出去偏估计量，新检验统计量更有效，用多种理论获结果，数值模拟验证。


<details>
  <summary>Details</summary>
Motivation: 在矩阵去噪模型中对细粒度奇异子空间进行估计和假设检验，改进现有基于Frobenius范数子空间距离的方法。

Method: 综合运用逐元素矩阵扰动分析、极值理论、鞍点近似方法和随机矩阵理论。

Result: 领先样本和总体奇异向量对齐差的最大欧氏行范数在大矩阵极限下趋近Gumbel分布；提出去偏估计量；基于二到无穷范数的检验统计量检测结构化备择假设更有效；数值模拟表明测试程序对非高斯噪声分布有鲁棒性。

Conclusion: 研究成果补充了矩阵去噪现有文献，新的检验统计量和方法具有优势和实用性。

Abstract: This paper studies fine-grained singular subspace estimation in the matrix
denoising model where a deterministic low-rank signal matrix is additively
perturbed by a stochastic matrix of Gaussian noise. We establish that the
maximum Euclidean row norm (i.e., the two-to-infinity norm) of the aligned
difference between the leading sample and population singular vectors
approaches the Gumbel distribution in the large-matrix limit, under suitable
signal-to-noise conditions and after appropriate centering and scaling. We
apply our novel asymptotic distributional theory to test hypotheses of low-rank
signal structure encoded in the leading singular vectors and their
corresponding principal subspace. We provide de-biased estimators for the
corresponding nuisance signal singular values and show that our proposed
plug-in test statistic has desirable properties. Notably, compared to using the
Frobenius norm subspace distance, our test statistic based on the
two-to-infinity norm has higher power to detect structured alternatives that
differ from the null in only a few matrix entries or rows. Our main results are
obtained by a novel synthesis of and technical analysis involving entrywise
matrix perturbation analysis, extreme value theory, saddle point approximation
methods, and random matrix theory. Our contributions complement the existing
literature for matrix denoising focused on minimaxity, mean squared error
analysis, unitarily invariant distances between subspaces, component-wise
asymptotic distributional theory, and row-wise uniform error bounds. Numerical
simulations illustrate our main results and demonstrate the robustness
properties of our testing procedure to non-Gaussian noise distributions.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [454] [Ultracoarse Equilibria and Ordinal-Folding Dynamics in Operator-Algebraic Models of Infinite Multi-Agent Games](https://arxiv.org/abs/2507.19694)
*Faruk Alpay,Hamdi Alakkad,Bugra Kilictas,Taylan Alpay*

Main category: math.OC

TL;DR: 本文为含连续统参与者的无限博弈构建算子代数框架，证明基于后悔的学习动态收敛到唯一量子响应均衡，引入序数折叠指数并给出相关应用。


<details>
  <summary>Details</summary>
Motivation: 为大规模多智能体系统提供严格数学基础，统一泛函分析、粗几何和博弈论。

Method: 构建算子代数框架，定义反射后悔算子，引入序数折叠指数。

Result: 证明基于后悔的学习动态收敛到唯一量子响应均衡，得到新的不变子代数刚性结果，确立连续统经济中无嫉妒和最大最小份额分配的存在唯一性，关联后悔流分析性质与大语言模型经验稳定性现象。

Conclusion: 该理论为大规模多智能体系统提供数学基础，证明序数度量对均衡选择有用。

Abstract: We develop an operator algebraic framework for infinite games with a
continuum of agents and prove that regret based learning dynamics governed by a
noncommutative continuity equation converge to a unique quantal response
equilibrium under mild regularity assumptions. The framework unifies functional
analysis, coarse geometry and game theory by assigning to every game a von
Neumann algebra that represents collective strategy evolution. A reflective
regret operator within this algebra drives the flow of strategy distributions
and its fixed point characterises equilibrium. We introduce the ordinal folding
index, a computable ordinal valued metric that measures the self referential
depth of the dynamics, and show that it bounds the transfinite time needed for
convergence, collapsing to zero on coarsely amenable networks. The theory
yields new invariant subalgebra rigidity results, establishes existence and
uniqueness of envy free and maximin share allocations in continuum economies,
and links analytic properties of regret flows with empirical stability
phenomena in large language models. These contributions supply a rigorous
mathematical foundation for large scale multi agent systems and demonstrate the
utility of ordinal metrics for equilibrium selection.

</details>


### [455] [Accelerating Deterministic Global Optimization via GPU-parallel Interval Arithmetic](https://arxiv.org/abs/2507.20769)
*Hongzhen Zhang,Tim Kerkenhoff,Neil Kichler,Manuel Dahmen,Alexander Mitsos,Uwe Naumann,Dominik Bongartz*

Main category: math.OC

TL;DR: 本文研究基于GPU并行的空间分支定界（B&B）算法，实现两种方式，实验表明可显著提速，CUDA图实现效率更高，凸显GPU加速技术潜力。


<details>
  <summary>Details</summary>
Motivation: 空间B&B算法计算成本高，虽有CPU并行加速工作，但GPU并行探索较少，需研究GPU加速B&B算法。

Method: 设计含基于区间的GPU并行下界求解器的空间B&B算法，将节点域临时划分子域，用均值形式在GPU并行计算区间界，通过CUDA以两种方式实现。

Result: 更多子域可使下界更紧、减少B&B迭代；相比CPU无分区区间运算，框架可提速三个数量级；CUDA图实现效率更高；部分案例中性能优于MAiNGO默认求解器。

Conclusion: GPU加速边界技术有加速B&B算法的潜力。

Abstract: Spatial Branch and Bound (B&B) algorithms are widely used for solving
nonconvex problems to global optimality, yet they remain computationally
expensive. Though some works have been carried out to speed up B&B via CPU
parallelization, GPU parallelization is much less explored. In this work, we
investigate the design of a spatial B&B algorithm that involves an
interval-based GPU-parallel lower bounding solver: The domain of each B&B node
is temporarily partitioned into numerous subdomains, then massive GPU
parallelism is leveraged to compute interval bounds of the objective function
and constraints on each subdomain, using the Mean Value Form. The resulting
bounds are tighter than those achieved via regular interval arithmetic without
partitioning, but they remain fast to compute. We implement the method into our
open-source solver MAiNGO via CUDA in two manners: wrapping all GPU tasks
within one kernel function, or distributing the GPU tasks onto a CUDA graph.
Numerical experiments show that using more subdomains leads to significantly
tighter lower bounds and thus less B&B iterations. Regarding wall clock time,
the proposed spatial B&B framework achieves a speedup of three orders of
magnitude compared to applying interval arithmetic on the CPU without domain
partitioning. Among the two implementations, the one developed with CUDA graph
enables higher efficiency. Moreover, in some case studies, the proposed method
delivers competitive or better performance compared to MAiNGO's default solver
which is based on McCormick relaxations. These results highlight the potential
of GPU-accelerated bounding techniques to accelerate B&B algorithms.

</details>


### [456] [Nonconvex Optimization Framework for Group-Sparse Feedback Linear-Quadratic Optimal Control. II: Non-Penalty Approach](https://arxiv.org/abs/2507.19895)
*Lechen Feng,Xun Li,Yuan-Hua Ni*

Main category: math.OC

TL;DR: 本文针对[8]中惩罚方法的缺点，从epi - composition函数角度重新表述问题，研究ADMM收敛性，在假设不成立时用替代方法，可直接设计组稀疏反馈增益。


<details>
  <summary>Details</summary>
Motivation: 解决[8]中惩罚方法存在的调参困难和引入虚假驻点等缺点。

Method: 从epi - composition函数角度重新表述SF - LQ和DFT - LQ问题；理论上研究ADMM收敛性；假设不成立时结合次梯度下降与凸差松弛方法。

Result: 可在无凸替代、无严格结构假设、不将约束纳入成本函数惩罚公式的情况下，直接设计组稀疏反馈增益。

Conclusion: 本文方法能有效解决惩罚方法的问题，实现有理论保证的组稀疏反馈增益直接设计。

Abstract: This work is a companion paper of [8], where the distributed linear-quadratic
problem with fixed communication topology (DFT-LQ) and the sparse feedback LQ
problem (SF-LQ) are formulated into a nonsmooth and nonconvex optimization
problem with affine constraints. Moreover, a penalty approach is considered in
\cite{feng-part1}, and the PALM (proximal alternating linearized minimization)
algorithm is studied with convergence and complexity analysis. In this paper,
we aim to address the inherent drawbacks of the penalty approach, such as the
challenge of tuning the penalty parameter and the risk of introducing spurious
stationary points. Specifically, we first reformulate the SF-LQ problem and the
DFT-LQ problem from an epi-composition function perspective, aiming to solve
the constrained problem directly. Then, from a theoretical viewpoint, we
revisit the alternating direction method of multipliers (ADMM) and establish
its convergence to the set of cluster points under certain assumptions. When
these assumptions do not hold, we can effectively utilize alternative
approaches combining subgradient descent with Difference-of-Convex relaxation
methods. In summary, our results enable the direct design of group-sparse
feedback gains with theoretical guarantees, without resorting to convex
surrogates, restrictive structural assumptions, or penalty formulations that
incorporate constraints into the cost function.

</details>


### [457] [Mean-Field Langevin Diffusions with Density-dependent Temperature](https://arxiv.org/abs/2507.20958)
*Yu-Jui Huang,Zachariah Malik*

Main category: math.OC

TL;DR: 研究非凸优化中让Langevin扩散温度依赖自身密度函数的情况，分析对应方程解的性质及密度收敛性。


<details>
  <summary>Details</summary>
Motivation: 在非凸优化中，使Langevin扩散温度依赖自身密度函数，以根据局部极小值的位置和深度提供更合适的随机扰动。

Method: 利用Wasserstein次微分计算证明非线性Fokker - Planck方程有唯一解，通过Trevisan叠加原理从Fokker - Planck方程解构造SDE的弱解。

Result: 证明了Fokker - Planck方程有唯一解，构造出SDE弱解，且SDE诱导的密度收敛到一个不变分布。

Conclusion: 在非凸优化中，让Langevin扩散温度依赖自身密度函数的方法可行，其诱导密度收敛到有显式公式的不变分布。

Abstract: In the context of non-convex optimization, we let the temperature of a
Langevin diffusion to depend on the diffusion's own density function. The
rationale is that the induced density reveals to some extent the landscape
imposed by the non-convex function to be minimized, such that a
density-dependent temperature can provide location-wise random perturbation
that may better react to, for instance, the location and depth of local
minimizers. As the Langevin dynamics is now self-regulated by its own density,
it forms a mean-field stochastic differential equation (SDE) of the Nemytskii
type, distinct from the standard McKean-Vlasov equations. Relying on
Wasserstein subdifferential calculus, we first show that the corresponding
(nonlinear) Fokker-Planck equation has a unique solution. Next, a weak solution
to the SDE is constructed from the solution to the Fokker-Planck equation, by
Trevisan's superposition principle. As time goes to infinity, we further show
that the density induced by the SDE converges to an invariant distribution,
which admits an explicit formula in terms of the Lambert $W$ function.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [458] [DynamiX: Large-Scale Dynamic Social Network Simulator](https://arxiv.org/abs/2507.19929)
*Yanhui Sun,Wu Liu,Wentao Wang,Hantao Yao,Jiebo Luo,Yongdong Zhang*

Main category: physics.soc-ph

TL;DR: 为解决现有研究忽视社交关系动态演变的问题，提出DynamiX社交网络模拟器，实验显示其在态度演变模拟等方面有改进。


<details>
  <summary>Details</summary>
Motivation: 理解社交平台内在机制对维护社会稳定至关重要，现有研究忽略社交关系动态演变，需新方法解决。

Method: 引入DynamiX模拟器，用动态层级模块选核心代理，为不同用户类型设计动态社交关系建模策略。

Result: DynamiX在态度演变模拟和集体行为分析上比静态网络有显著提升，为追随者增长预测提供新视角。

Conclusion: DynamiX有效解决现有研究问题，在社交网络模拟中有优势，为意见领袖培养提供实证依据。

Abstract: Understanding the intrinsic mechanisms of social platforms is an urgent
demand to maintain social stability. The rise of large language models provides
significant potential for social network simulations to capture attitude
dynamics and reproduce collective behaviors. However, existing studies mainly
focus on scaling up agent populations, neglecting the dynamic evolution of
social relationships. To address this gap, we introduce DynamiX, a novel
large-scale social network simulator dedicated to dynamic social network
modeling. DynamiX uses a dynamic hierarchy module for selecting core agents
with key characteristics at each timestep, enabling accurate alignment of
real-world adaptive switching of user roles. Furthermore, we design distinct
dynamic social relationship modeling strategies for different user types. For
opinion leaders, we propose an information-stream-based link prediction method
recommending potential users with similar stances, simulating homogeneous
connections, and autonomous behavior decisions. For ordinary users, we
construct an inequality-oriented behavior decision-making module, effectively
addressing unequal social interactions and capturing the patterns of
relationship adjustments driven by multi-dimensional factors. Experimental
results demonstrate that DynamiX exhibits marked improvements in attitude
evolution simulation and collective behavior analysis compared to static
networks. Besides, DynamiX opens a new theoretical perspective on follower
growth prediction, providing empirical evidence for opinion leaders
cultivation.

</details>


### [459] [Street network sub-patterns and travel mode](https://arxiv.org/abs/2507.19648)
*Juan Fernando Riascos Goyes,Michael Lowry,Nicolás Guarín Zapata,Juan Pablo Ospina*

Main category: physics.soc-ph

TL;DR: 本文运用无监督学习对美国九个大都市的建成环境分类，发现不同城市形态与出行行为有系统关联，为城市规划提供依据。


<details>
  <summary>Details</summary>
Motivation: 城市形态对人类出行有影响，但跨大都市地区的城市形态分类研究有限。

Method: 基于城市结构理论和无监督学习，用密度、连通性等指标对九个美国大都市建成环境分类，通过描述性统计、边际效应估计和事后统计检验将形态类型与出行模式关联。

Result: 不同城市形态与不同出行行为有系统关联，如网状形态与公共交通使用增加、汽车依赖减少相关，有机形态与汽车使用增加、公共交通和主动出行减少相关，且效果统计显著。

Conclusion: 城市形态应作为出行规划关键变量，研究为可持续城市政策设计纳入空间类型学提供实证支持。

Abstract: Urban morphology has long been recognized as a factor shaping human mobility,
yet comparative and formal classifications of urban form across metropolitan
areas remain limited. Building on theoretical principles of urban structure and
advances in unsupervised learning, we systematically classified the built
environment of nine U.S. metropolitan areas using structural indicators such as
density, connectivity, and spatial configuration. The resulting morphological
types were linked to mobility patterns through descriptive statistics, marginal
effects estimation, and post hoc statistical testing. Here we show that
distinct urban forms are systematically associated with different mobility
behaviors, such as reticular morphologies being linked to significantly higher
public transport use (marginal effect = 0.49) and reduced car dependence
(-0.41), while organic forms are associated with increased car usage (0.44),
and substantial declines in public transport (-0.47) and active mobility
(-0.30). These effects are statistically robust (p < 1e-19), highlighting that
the spatial configuration of urban areas plays a fundamental role in shaping
transportation choices. Our findings extend previous work by offering a
reproducible framework for classifying urban form and demonstrate the added
value of morphological analysis in comparative urban research. These results
suggest that urban form should be treated as a key variable in mobility
planning and provide empirical support for incorporating spatial typologies
into sustainable urban policy design.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [460] [Review of Deep Learning Applications to Structural Proteomics Enabled by Cryogenic Electron Microscopy and Tomography](https://arxiv.org/abs/2507.19565)
*Brady K. Zhou,Jason J. Hu,Jane K. J. Lee,Z. Hong Zhou,Demetri Terzopoulos*

Main category: q-bio.QM

TL;DR: 回顾了AI在冷冻电镜（cryoEM）和断层扫描（cryoET）全流程的应用，其增强方法成果显著，未来有望推动结构生物学发展。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜虽有发展，但存在低信噪比、优先取向伪影和缺失楔问题，限制效率和可扩展性，需解决这些挑战。

Method: 在cryoEM全流程应用AI，如用卷积神经网络自动挑选颗粒、计算解决优先取向偏差、先进去噪算法；cryoET中用U - Net架构校正缺失楔和降噪等，最后用复杂工具自动构建原子模型。

Result: AI增强方法实现近原子分辨率重建，解决严重取向偏差数据集，应用于多种生物系统。

Conclusion: 随着深度学习发展，特别是大语言模型和视觉变换器，未来结构生物学将实现复杂自动化和更高可及性，可能改变对大分子结构和功能的理解。

Abstract: The past decade's "cryoEM revolution" has produced exponential growth in
high-resolution structural data through advances in cryogenic electron
microscopy (cryoEM) and tomography (cryoET). Deep learning integration into
structural proteomics workflows addresses longstanding challenges including low
signal-to-noise ratios, preferred orientation artifacts, and missing-wedge
problems that historically limited efficiency and scalability. This review
examines AI applications across the entire cryoEM pipeline, from automated
particle picking using convolutional neural networks (Topaz, crYOLO,
CryoSegNet) to computational solutions for preferred orientation bias
(spIsoNet, cryoPROS) and advanced denoising algorithms (Topaz-Denoise). In
cryoET, tools like IsoNet employ U-Net architectures for simultaneous
missing-wedge correction and noise reduction, while TomoNet streamlines
subtomogram averaging through AI-driven particle detection. The workflow
culminates with automated atomic model building using sophisticated tools like
ModelAngelo, DeepTracer, and CryoREAD that translate density maps into
interpretable biological structures. These AI-enhanced approaches have achieved
near-atomic resolution reconstructions with minimal manual intervention,
resolved previously intractable datasets suffering from severe orientation
bias, and enabled successful application to diverse biological systems from HIV
virus-like particles to in situ ribosomal complexes. As deep learning evolves,
particularly with large language models and vision transformers, the future
promises sophisticated automation and accessibility in structural biology,
potentially revolutionizing our understanding of macromolecular architecture
and function.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [461] [Accelerating Containerized Service Delivery at the Network Edge](https://arxiv.org/abs/2507.20116)
*Yinuo Deng,Hailiang Zhao,Dongjing Wang,Peng Chen,Wenzhuo Qian,Jianwei Yin,Schahram Dustdar,Shuiguang Deng*

Main category: cs.NI

TL;DR: 提出基于P2P的PeerSync系统优化边缘环境图像分发，性能测试显示有显著提升。


<details>
  <summary>Details</summary>
Motivation: 网络边缘资源有限和动态网络条件给机器学习推理的容器镜像分发带来挑战，需高效分发方案。

Method: 采用基于流行度和网络感知的下载引擎，结合滑动窗口机制；集成自动跟踪器选举用于快速对等发现和动态缓存管理。用8000多行Rust代码实现并在物理边缘设备和Docker仿真上测试。

Result: 与Baseline、Dragonfly和Kraken相比，速度分别提升2.72倍、1.79倍和1.28倍，在拥塞和变化网络条件下将峰值跨网络流量显著降低90.72%。

Conclusion: PeerSync能有效优化边缘环境的容器镜像分发，提升分发速度并减少网络流量。

Abstract: Efficient container image distribution is crucial for enabling machine
learning inference at the network edge, where resource limitations and dynamic
network conditions create significant challenges. In this paper, we present
PeerSync, a decentralized P2P-based system designed to optimize image
distribution in edge environments. PeerSync employs a popularity- and
network-aware download engine that dynamically adapts to content popularity and
real-time network conditions using a sliding window mechanism. PeerSync further
integrates automated tracker election for rapid peer discovery and dynamic
cache management for efficient storage utilization. We implement PeerSync with
8000+ lines of Rust code and test its performance extensively on both physical
edge devices and Docker-based emulations. Experimental results show that
PeerSync delivers a remarkable speed increase of 2.72$\times$, 1.79$\times$,
and 1.28$\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,
while significantly reducing peak cross-network traffic by 90.72\% under
congested and varying network conditions.

</details>


### [462] [On the Limitations of Ray-Tracing for Learning-Based RF Tasks in Urban Environments](https://arxiv.org/abs/2507.19653)
*Armen Manukyan,Hrant Khachatrian,Edvard Ghukasyan,Theofanis P. Raptis*

Main category: cs.NI

TL;DR: 研究Sionna v1.0.2射线追踪在罗马市中心户外蜂窝链路的真实性，发现天线位置和方向影响大，简单优化有效果，但捕捉城市噪声仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 研究Sionna v1.0.2射线追踪在罗马市中心户外蜂窝链路的真实性。

Method: 使用1664个用户设备和6个基站的真实测量数据，系统改变模拟参数，通过Spearman相关性和指纹定位算法评估模拟器保真度。

Result: 求解器超参数对指标影响小，天线位置和方向影响大，简单优化可提高Spearman相关性，降低定位误差，但仍高于纯真实数据误差。

Conclusion: 精确几何和可靠天线模型必要但不充分，捕捉城市噪声是户外射频模拟的挑战。

Abstract: We study the realism of Sionna v1.0.2 ray-tracing for outdoor cellular links
in central Rome. We use a real measurement set of 1,664 user-equipments (UEs)
and six nominal base-station (BS) sites. Using these fixed positions we
systematically vary the main simulation parameters, including path depth,
diffuse/specular/refraction flags, carrier frequency, as well as antenna's
properties like its altitude, radiation pattern, and orientation. Simulator
fidelity is scored for each base station via Spearman correlation between
measured and simulated powers, and by a fingerprint-based k-nearest-neighbor
localization algorithm using RSSI-based fingerprints. Across all experiments,
solver hyper-parameters are having immaterial effect on the chosen metrics. On
the contrary, antenna locations and orientations prove decisive. By simple
greedy optimization we improve the Spearman correlation by 5% to 130% for
various base stations, while kNN-based localization error using only simulated
data as reference points is decreased by one-third on real-world samples, while
staying twice higher than the error with purely real data. Precise geometry and
credible antenna models are therefore necessary but not sufficient; faithfully
capturing the residual urban noise remains an open challenge for transferable,
high-fidelity outdoor RF simulation.

</details>


### [463] ["X of Information'' Continuum: A Survey on AI-Driven Multi-dimensional Metrics for Next-Generation Networked Systems](https://arxiv.org/abs/2507.19657)
*Beining Wu,Jun Huang,Shui Yu*

Main category: cs.NI

TL;DR: 本文对信息的多维指标进行全面研究，提出四维分类框架，分析各维度关系，揭示AI技术对信息质量目标优化的作用，并研究六个应用领域，指出实施挑战。


<details>
  <summary>Details</summary>
Motivation: 下一代网络系统发展从基于吞吐量转向智能、信息感知设计，经典网络指标不足以量化现代智能应用的信息质量需求。

Method: 引入系统的四维分类框架来构建信息指标，分析各维度相互依赖关系，研究AI技术对信息质量目标的优化，对六个关键应用领域进行广泛研究。

Result: 发现各维度间的相互依赖关系，揭示AI技术能实现对相互竞争的信息质量目标的自适应、上下文感知优化，展示多维信息指标在六个应用领域满足多样化运营需求的潜力。

Conclusion: 多维信息指标有重大潜力，但存在显著的实施挑战。

Abstract: The development of next-generation networking systems has inherently shifted
from throughput-based paradigms towards intelligent, information-aware designs
that emphasize the quality, relevance, and utility of transmitted information,
rather than sheer data volume. While classical network metrics, such as latency
and packet loss, remain significant, they are insufficient to quantify the
nuanced information quality requirements of modern intelligent applications,
including autonomous vehicles, digital twins, and metaverse environments. In
this survey, we present the first comprehensive study of the ``X of
Information'' continuum by introducing a systematic four-dimensional taxonomic
framework that structures information metrics along temporal, quality/utility,
reliability/robustness, and network/communication dimensions. We uncover the
increasing interdependencies among these dimensions, whereby temporal freshness
triggers quality evaluation, which in turn helps with reliability appraisal,
ultimately enabling effective network delivery. Our analysis reveals that
artificial intelligence technologies, such as deep reinforcement learning,
multi-agent systems, and neural optimization models, enable adaptive,
context-aware optimization of competing information quality objectives. In our
extensive study of six critical application domains, covering autonomous
transportation, industrial IoT, healthcare digital twins, UAV communications,
LLM ecosystems, and metaverse settings, we illustrate the revolutionary promise
of multi-dimensional information metrics for meeting diverse operational needs.
Our survey identifies prominent implementation challenges, including ...

</details>


### [464] [Packet-Level DDoS Data Augmentation Using Dual-Stream Temporal-Field Diffusion](https://arxiv.org/abs/2507.20115)
*Gongli Xi,Ye Tian,Yannan Hu,Yuchao Zhang,Yapeng Niu,Xiangyang Gong*

Main category: cs.NI

TL;DR: 为解决DDoS攻击数据集中的不足，提出DSTF - Diffusion生成模型，实验表明其生成数据与原始数据更相似，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有DDoS攻击标记训练数据集稀缺，且合成跟踪生成方法难以捕捉新兴DDoS攻击的复杂时空模式，导致与真实跟踪相似度不足和检测精度不高。

Method: 提出基于扩散模型的多视图、多流网络流量生成模型DSTF - Diffusion，包含场流和空间流，场流利用空间映射处理网络数据，空间流采用动态时间建模。

Result: 模型生成的数据与现有先进方案相比，与原始数据有更高的统计相似性，提升了一系列下游任务的性能。

Conclusion: DSTF - Diffusion模型在生成与真实网络流量相似的数据方面表现出色，能有效提升下游任务表现。

Abstract: In response to Distributed Denial of Service (DDoS) attacks, recent research
efforts increasingly rely on Machine Learning (ML)-based solutions, whose
effectiveness largely depends on the quality of labeled training datasets. To
address the scarcity of such datasets, data augmentation with synthetic traces
is often employed. However, current synthetic trace generation methods struggle
to capture the complex temporal patterns and spatial distributions exhibited in
emerging DDoS attacks. This results in insufficient resemblance to real traces
and unsatisfied detection accuracy when applied to ML tasks. In this paper, we
propose Dual-Stream Temporal-Field Diffusion (DSTF-Diffusion), a multi-view,
multi-stream network traffic generative model based on diffusion models,
featuring two main streams: The field stream utilizes spatial mapping to bridge
network data characteristics with pre-trained realms of stable diffusion
models, effectively translating complex network interactions into formats that
stable diffusion can process, while the spatial stream adopts a dynamic
temporal modeling approach, meticulously capturing the intrinsic temporal
patterns of network traffic. Extensive experiments demonstrate that data
generated by our model exhibits higher statistical similarity to originals
compared to current state-of-the-art solutions, and enhance performances on a
wide range of downstream tasks.

</details>


### [465] [\textit{FedABC}: Attention-Based Client Selection for Federated Learning with Long-Term View](https://arxiv.org/abs/2507.20871)
*Wenxuan Ye,Xueli An,Junfan Wang,Xueqiang Yan,Georg Carle*

Main category: cs.NI

TL;DR: 本文提出FedABC算法解决联邦学习中客户端数据异构和参与负担问题，在CIFAR - 10模拟中表现出色，推动6G网络原生AI能力发展。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习存在客户端数据异构导致收敛慢、精度低，客户端频繁参与带来通信和计算负担的问题，需解决以支持6G网络原生AI。

Method: 提出FedABC算法，借鉴注意力机制评估模型相似性和独特贡献来确定信息丰富的客户端，制定优化问题指导训练，遵循“later - is - better”原则自适应调整客户端选择阈值。

Result: 在CIFAR - 10模拟中，FedABC比经典算法FedAvg用少32%的客户端实现相近性能，比现有最优算法用少2%的客户端实现高3.5%的精度。

Conclusion: FedABC算法有助于在异构、资源受限环境中部署联邦学习，支持6G网络原生AI能力。

Abstract: Native AI support is a key objective in the evolution of 6G networks, with
Federated Learning (FL) emerging as a promising paradigm. FL allows
decentralized clients to collaboratively train an AI model without directly
sharing their data, preserving privacy. Clients train local models on private
data and share model updates, which a central server aggregates to refine the
global model and redistribute it for the next iteration. However, client data
heterogeneity slows convergence and reduces model accuracy, and frequent client
participation imposes communication and computational burdens. To address these
challenges, we propose \textit{FedABC}, an innovative client selection
algorithm designed to take a long-term view in managing data heterogeneity and
optimizing client participation. Inspired by attention mechanisms,
\textit{FedABC} prioritizes informative clients by evaluating both model
similarity and each model's unique contributions to the global model. Moreover,
considering the evolving demands of the global model, we formulate an
optimization problem to guide \textit{FedABC} throughout the training process.
Following the ``later-is-better" principle, \textit{FedABC} adaptively adjusts
the client selection threshold, encouraging greater participation in later
training stages. Extensive simulations on CIFAR-10 demonstrate that
\textit{FedABC} significantly outperforms existing approaches in model accuracy
and client participation efficiency, achieving comparable performance with 32\%
fewer clients than the classical FL algorithm \textit{FedAvg}, and 3.5\% higher
accuracy with 2\% fewer clients than the state-of-the-art. This work marks a
step toward deploying FL in heterogeneous, resource-constrained environments,
thereby supporting native AI capabilities in 6G networks.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [466] [ChoreoMuse: Robust Music-to-Dance Video Generation with Style Transfer and Beat-Adherent Motion](https://arxiv.org/abs/2507.19836)
*Xuanchen Wang,Heng Wang,Weidong Cai*

Main category: cs.GR

TL;DR: 提出ChoreoMuse框架用于自动化编舞视频生成，在多维度达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法难生成与音乐节奏和编舞风格协调的高质量舞蹈视频，无法满足现实创作需求。

Method: 引入ChoreoMuse框架，用SMPL格式参数作中介，采用MotionTune音乐编码器，引入新评估指标。

Result: 广泛实验表明ChoreoMuse在视频质量、节拍对齐等多维度达SOTA。

Conclusion: ChoreoMuse是适用于多种创意应用的可靠解决方案。

Abstract: Modern artistic productions increasingly demand automated choreography
generation that adapts to diverse musical styles and individual dancer
characteristics. Existing approaches often fail to produce high-quality dance
videos that harmonize with both musical rhythm and user-defined choreography
styles, limiting their applicability in real-world creative contexts. To
address this gap, we introduce ChoreoMuse, a diffusion-based framework that
uses SMPL format parameters and their variation version as intermediaries
between music and video generation, thereby overcoming the usual constraints
imposed by video resolution. Critically, ChoreoMuse supports
style-controllable, high-fidelity dance video generation across diverse musical
genres and individual dancer characteristics, including the flexibility to
handle any reference individual at any resolution. Our method employs a novel
music encoder MotionTune to capture motion cues from audio, ensuring that the
generated choreography closely follows the beat and expressive qualities of the
input music. To quantitatively evaluate how well the generated dances match
both musical and choreographic styles, we introduce two new metrics that
measure alignment with the intended stylistic cues. Extensive experiments
confirm that ChoreoMuse achieves state-of-the-art performance across multiple
dimensions, including video quality, beat alignment, dance diversity, and style
adherence, demonstrating its potential as a robust solution for a wide range of
creative applications. Video results can be found on our project page:
https://choreomuse.github.io.

</details>


### [467] [GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D Gaussian Splatting](https://arxiv.org/abs/2507.19718)
*David Bauer,Qi Wu,Hamid Gadirov,Kwan-Liu Ma*

Main category: cs.GR

TL;DR: 提出用于路径追踪体渲染的新型辐射缓存方法，可提升渲染质量且不增加成本。


<details>
  <summary>Details</summary>
Motivation: 科学可视化中真实感渲染技术面临渲染性能慢和像素方差高的问题。

Method: 利用体场景表示的进展，将3D高斯散点适应为多级路径空间辐射缓存，该缓存可动态适应场景参数变化。

Result: 通过与基线路径追踪器和最先进的神经辐射缓存对比，经定量和定性分析，表明该方法能提升体可视化应用的渲染质量，且计算效率相当。

Conclusion: 路径空间辐射缓存是一个易于集成的强大解决方案，能显著提升渲染质量。

Abstract: Real-time path tracing is rapidly becoming the standard for rendering in
entertainment and professional applications. In scientific visualization,
volume rendering plays a crucial role in helping researchers analyze and
interpret complex 3D data. Recently, photorealistic rendering techniques have
gained popularity in scientific visualization, yet they face significant
challenges. One of the most prominent issues is slow rendering performance and
high pixel variance caused by Monte Carlo integration. In this work, we
introduce a novel radiance caching approach for path-traced volume rendering.
Our method leverages advances in volumetric scene representation and adapts 3D
Gaussian splatting to function as a multi-level, path-space radiance cache.
This cache is designed to be trainable on the fly, dynamically adapting to
changes in scene parameters such as lighting configurations and transfer
functions. By incorporating our cache, we achieve less noisy, higher-quality
images without increasing rendering costs. To evaluate our approach, we compare
it against a baseline path tracer that supports uniform sampling and next-event
estimation and the state-of-the-art for neural radiance caching. Through both
quantitative and qualitative analyses, we demonstrate that our path-space
radiance cache is a robust solution that is easy to integrate and significantly
enhances the rendering quality of volumetric visualization applications while
maintaining comparable computational efficiency.

</details>
