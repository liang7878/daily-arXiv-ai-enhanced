<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 8]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 13]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.SE](#cs.SE) [Total: 9]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [stat.ML](#stat.ML) [Total: 4]
- [stat.CO](#stat.CO) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [math.ST](#math.ST) [Total: 2]
- [econ.GN](#econ.GN) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.DL](#cs.DL) [Total: 3]
- [math.CO](#math.CO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [stat.ME](#stat.ME) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A hybrid solution approach for the Integrated Healthcare Timetabling Competition 2024](https://arxiv.org/abs/2511.04685)
*Daniela Guericke,Rolf van der Hulst,Asal Karimpour,Ieke Schrader,Matthias Walter*

Main category: cs.AI

TL;DR: 本文介绍特温特团队参加2024年综合医疗排课竞赛的算法、实现和结果，该团队获第三名，分享方法、见解和下界，还指出待解决问题。


<details>
  <summary>Details</summary>
Motivation: 参加综合医疗排课竞赛并取得好成绩，分享经验和见解。

Method: 采用分三个阶段的解决方案，结合混合整数规划、约束规划和模拟退火算法，将问题分解为子问题。

Result: 团队在竞赛中获得第三名，首次给出基准实例最优解值的下界。

Conclusion: 指出一些开放问题，解决这些问题可能进一步改进方法。

Abstract: We report about the algorithm, implementation and results submitted to the
Integrated Healthcare Timetabling Competition 2024 by Team Twente, which scored
third in the competition. Our approach combines mixed-integer programming,
constraint programming and simulated annealing in a 3-phase solution approach
based on decomposition into subproblems. Next to describing our approach and
describing our design decisions, we share our insights and, for the first time,
lower bounds on the optimal solution values for the benchmark instances. We
finally highlight open problems for which we think that addressing them could
improve our approach even further.

</details>


### [2] [Epistemic Reject Option Prediction](https://arxiv.org/abs/2511.04855)
*Vojtech Franc,Jakub Paplham*

Main category: cs.AI

TL;DR: 本文提出认知拒绝选项预测器，基于贝叶斯学习，以最小化预期遗憾为目标，在训练数据不足导致高认知不确定性区域选择弃权。


<details>
  <summary>Details</summary>
Motivation: 传统拒绝选项方法仅关注偶然不确定性，在实际数据有限场景下不适用，需考虑认知不确定性。

Method: 基于贝叶斯学习，将最优预测器重新定义为最小化预期遗憾的预测器，当输入的遗憾超过指定拒绝成本时弃权。

Result: 提出了认知拒绝选项预测器这一框架。

Conclusion: 这是首个能让预测器识别训练数据不足以做出可靠决策的输入的原则性框架。

Abstract: In high-stakes applications, predictive models must not only produce accurate
predictions but also quantify and communicate their uncertainty. Reject-option
prediction addresses this by allowing the model to abstain when prediction
uncertainty is high. Traditional reject-option approaches focus solely on
aleatoric uncertainty, an assumption valid only when large training data makes
the epistemic uncertainty negligible. However, in many practical scenarios,
limited data makes this assumption unrealistic. This paper introduces the
epistemic reject-option predictor, which abstains in regions of high epistemic
uncertainty caused by insufficient data. Building on Bayesian learning, we
redefine the optimal predictor as the one that minimizes expected regret -- the
performance gap between the learned model and the Bayes-optimal predictor with
full knowledge of the data distribution. The model abstains when the regret for
a given input exceeds a specified rejection cost. To our knowledge, this is the
first principled framework that enables learning predictors capable of
identifying inputs for which the training data is insufficient to make reliable
decisions.

</details>


### [3] [DMA: Online RAG Alignment with Human Feedback](https://arxiv.org/abs/2511.04880)
*Yu Bai,Yukai Miao,Dawei Wang,Li Chen,Fei Long,Rundi Zhai,Dan Li,Yanyu Ren,Tianfeng Liu,Hongtao Xie,Ce Yang,Xuhui Cai*

Main category: cs.AI

TL;DR: 提出Dynamic Memory Alignment (DMA)在线学习框架用于检索增强生成系统，通过多粒度人类反馈调整排序，经线上线下评估有显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成系统依赖静态检索，难以适应意图演变和内容漂移。

Method: 引入DMA框架，组织文档、列表和响应级信号到学习管道，包括监督训练、策略优化和知识蒸馏；采用双轨评估协议。

Result: 线上工业部署提升人类参与度，线下在知识密集型基准测试有显著提升。

Conclusion: DMA是一种在不牺牲基线能力的情况下，基于反馈驱动实时适应的有效方法。

Abstract: Retrieval-augmented generation (RAG) systems often rely on static retrieval,
limiting adaptation to evolving intent and content drift. We introduce Dynamic
Memory Alignment (DMA), an online learning framework that systematically
incorporates multi-granularity human feedback to align ranking in interactive
settings. DMA organizes document-, list-, and response-level signals into a
coherent learning pipeline: supervised training for pointwise and listwise
rankers, policy optimization driven by response-level preferences, and
knowledge distillation into a lightweight scorer for low-latency serving.
Throughout this paper, memory refers to the model's working memory, which is
the entire context visible to the LLM for In-Context Learning.
  We adopt a dual-track evaluation protocol mirroring deployment: (i)
large-scale online A/B ablations to isolate the utility of each feedback
source, and (ii) few-shot offline tests on knowledge-intensive benchmarks.
Online, a multi-month industrial deployment further shows substantial
improvements in human engagement. Offline, DMA preserves competitive
foundational retrieval while yielding notable gains on conversational QA
(TriviaQA, HotpotQA). Taken together, these results position DMA as a
principled approach to feedback-driven, real-time adaptation in RAG without
sacrificing baseline capability.

</details>


### [4] [Real-Time Reasoning Agents in Evolving Environments](https://arxiv.org/abs/2511.04898)
*Yule Wen,Yixin Ye,Yanzhe Zhang,Diyi Yang,Hao Zhu*

Main category: cs.AI

TL;DR: 提出实时推理问题，构建测试平台，研究两种推理范式，提出AgileThinker解决现有模型问题，为实时智能体研究奠基。


<details>
  <summary>Details</summary>
Motivation: 现实世界智能体需进行逻辑且及时的判断，现有语言模型推理方法未考虑环境动态性。

Method: 引入实时推理问题，构建测试平台，研究两种部署语言模型的范式，提出AgileThinker同时运用两种推理范式。

Result: 实验表明现有模型在两种范式中难以兼顾逻辑和及时性，AgileThinker在任务难度和时间压力增加时表现更优。

Conclusion: 实时推理是开发实用智能体的关键测试平台，为时间受限的AI系统研究提供基础。

Abstract: Agents in the real world must make not only logical but also timely
judgments. This requires continuous awareness of the dynamic environment:
hazards emerge, opportunities arise, and other agents act, while the agent's
reasoning is still unfolding. Despite advances in language model reasoning,
existing approaches fail to account for this dynamic nature. We introduce
real-time reasoning as a new problem formulation for agents in evolving
environments and build Real-Time Reasoning Gym to demonstrate it. We study two
paradigms for deploying language models in agents: (1) reactive agents, which
employ language models with bounded reasoning computation for rapid responses,
and (2) planning agents, which allow extended reasoning computation for complex
problems. Our experiments show that even state-of-the-art models struggle with
making logical and timely judgments in either paradigm. To address this
limitation, we propose AgileThinker, which simultaneously engages both
reasoning paradigms. AgileThinker consistently outperforms agents engaging only
one reasoning paradigm as the task difficulty and time pressure rise,
effectively balancing reasoning depth and response latency. Our work
establishes real-time reasoning as a critical testbed for developing practical
agents and provides a foundation for research in temporally constrained AI
systems, highlighting a path toward real-time capable agents.

</details>


### [5] [ORCHID: Orchestrated Retrieval-Augmented Classification with Human-in-the-Loop Intelligent Decision-Making for High-Risk Property](https://arxiv.org/abs/2511.04956)
*Maria Mahbub,Vanessa Lama,Sanjay Das,Brian Starks,Christopher Polchek,Saffell Silvers,Lauren Deck,Prasanna Balaprakash,Tirthankar Ghosal*

Main category: cs.AI

TL;DR: 介绍了用于高风险资产（HRP）分类的模块化智能系统ORCHID，它结合检索增强生成和人工监督，初步测试显示能提高准确性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 传统仅依赖专家的HRP分类工作流程耗时、易积压且难以跟上监管变化，需要新的解决方案。

Method: 构建模块化智能系统ORCHID，小协作代理通过代理间消息协调，利用模型上下文协议操作，界面遵循特定循环并提供逐步推理和审计包。

Result: 在实际HRP案例的初步测试中，ORCHID比非智能基线提高了准确性和可追溯性，将不确定项目交由专家处理。

Conclusion: 展示了ORCHID在敏感的美国能源部合规工作流程中提供可信大语言模型辅助的实用途径。

Abstract: High-Risk Property (HRP) classification is critical at U.S. Department of
Energy (DOE) sites, where inventories include sensitive and often dual-use
equipment. Compliance must track evolving rules designated by various export
control policies to make transparent and auditable decisions. Traditional
expert-only workflows are time-consuming, backlog-prone, and struggle to keep
pace with shifting regulatory boundaries. We demo ORCHID, a modular agentic
system for HRP classification that pairs retrieval-augmented generation (RAG)
with human oversight to produce policy-based outputs that can be audited. Small
cooperating agents, retrieval, description refiner, classifier, validator, and
feedback logger, coordinate via agent-to-agent messaging and invoke tools
through the Model Context Protocol (MCP) for model-agnostic on-premise
operation. The interface follows an Item to Evidence to Decision loop with
step-by-step reasoning, on-policy citations, and append-only audit bundles
(run-cards, prompts, evidence). In preliminary tests on real HRP cases, ORCHID
improves accuracy and traceability over a non-agentic baseline while deferring
uncertain items to Subject Matter Experts (SMEs). The demonstration shows
single item submission, grounded citations, SME feedback capture, and
exportable audit artifacts, illustrating a practical path to trustworthy LLM
assistance in sensitive DOE compliance workflows.

</details>


### [6] [Autonomous generation of different courses of action in mechanized combat operations](https://arxiv.org/abs/2511.05182)
*Johan Schubert,Patrik Hansen,Pontus Hörling,Ronnie Johansson*

Main category: cs.AI

TL;DR: 提出支持军事地面作战执行阶段决策的方法论，生成并评估行动方案。


<details>
  <summary>Details</summary>
Motivation: 为军事地面作战执行阶段的决策提供支持。

Method: 从初始评估集出发，生成数千个行动替代方案，结合对手情况评估，生成与评估并行。

Result: 产生多种替代行动方案，可根据先前评估行动管理新方案生成。

Conclusion: 能在作战情况变化时，在顺序决策框架内为决策者制定修订后的行动方案。

Abstract: In this paper, we propose a methodology designed to support decision-making
during the execution phase of military ground combat operations, with a focus
on one's actions. This methodology generates and evaluates recommendations for
various courses of action for a mechanized battalion, commencing with an
initial set assessed by their anticipated outcomes. It systematically produces
thousands of individual action alternatives, followed by evaluations aimed at
identifying alternative courses of action with superior outcomes. These
alternatives are appraised in light of the opponent's status and actions,
considering unit composition, force ratios, types of offense and defense, and
anticipated advance rates. Field manuals evaluate battle outcomes and
advancement rates. The processes of generation and evaluation work
concurrently, yielding a variety of alternative courses of action. This
approach facilitates the management of new course generation based on
previously evaluated actions. As the combat unfolds and conditions evolve,
revised courses of action are formulated for the decision-maker within a
sequential decision-making framework.

</details>


### [7] [Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance](https://arxiv.org/abs/2511.05311)
*Valeriu Dimidov,Faisal Hawlader,Sasan Jafarnejad,Raphaël Frank*

Main category: cs.AI

TL;DR: 本文探讨基于大语言模型（LLM）的代理支持汽车行业预测性维护（PdM）数据清洗管道的潜力，发现LLM能有效处理通用清洗任务，为工业应用提供基础。


<details>
  <summary>Details</summary>
Motivation: 经济限制、数据集可用性和专业知识短缺阻碍汽车行业PdM发展，LLM发展带来克服障碍的机会。

Method: 聚焦维护日志，评估LLM代理处理六种不同类型噪声的清洗任务。

Result: LLM能有效处理通用清洗任务。

Conclusion: LLM为未来工业应用提供有前景的基础，特定领域错误仍具挑战，可通过专业训练和增强代理能力改进。

Abstract: Economic constraints, limited availability of datasets for reproducibility
and shortages of specialized expertise have long been recognized as key
challenges to the adoption and advancement of predictive maintenance (PdM) in
the automotive sector. Recent progress in large language models (LLMs) presents
an opportunity to overcome these barriers and speed up the transition of PdM
from research to industrial practice. Under these conditions, we explore the
potential of LLM-based agents to support PdM cleaning pipelines. Specifically,
we focus on maintenance logs, a critical data source for training
well-performing machine learning (ML) models, but one often affected by errors
such as typos, missing fields, near-duplicate entries, and incorrect dates. We
evaluate LLM agents on cleaning tasks involving six distinct types of noise.
Our findings show that LLMs are effective at handling generic cleaning tasks
and offer a promising foundation for future industrial applications. While
domain-specific errors remain challenging, these results highlight the
potential for further improvements through specialized training and enhanced
agentic capabilities.

</details>


### [8] [Reasoning Is All You Need for Urban Planning AI](https://arxiv.org/abs/2511.05375)
*Sijie Yang,Jiatong Li,Filip Biljecki*

Main category: cs.AI

TL;DR: 本文提出用于城市规划的智能体框架，展示AI增强人类规划能力而非取代人类判断。


<details>
  <summary>Details</summary>
Motivation: 当前AI在城市规划分析成功，下一步是实现AI辅助决策，现有推理AI突破使愿景可实现。

Method: 提出集成三个认知层和六个逻辑组件的智能体城市规划AI框架，通过多智能体协作框架实现。

Result: 展示AI智能体可系统探索解决方案空间、验证合规性和透明权衡。

Conclusion: 该框架能以计算推理能力增强人类规划者判断，而非取代。

Abstract: AI has proven highly successful at urban planning analysis -- learning
patterns from data to predict future conditions. The next frontier is
AI-assisted decision-making: agents that recommend sites, allocate resources,
and evaluate trade-offs while reasoning transparently about constraints and
stakeholder values. Recent breakthroughs in reasoning AI -- CoT prompting,
ReAct, and multi-agent collaboration frameworks -- now make this vision
achievable.
  This position paper presents the Agentic Urban Planning AI Framework for
reasoning-capable planning agents that integrates three cognitive layers
(Perception, Foundation, Reasoning) with six logic components (Analysis,
Generation, Verification, Evaluation, Collaboration, Decision) through a
multi-agents collaboration framework. We demonstrate why planning decisions
require explicit reasoning capabilities that are value-based (applying
normative principles), rule-grounded (guaranteeing constraint satisfaction),
and explainable (generating transparent justifications) -- requirements that
statistical learning alone cannot fulfill. We compare reasoning agents with
statistical learning, present a comprehensive architecture with benchmark
evaluation metrics, and outline critical research challenges. This framework
shows how AI agents can augment human planners by systematically exploring
solution spaces, verifying regulatory compliance, and deliberating over
trade-offs transparently -- not replacing human judgment but amplifying it with
computational reasoning capabilities.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [9] [Connectomics Informed by Large Language Models](https://arxiv.org/abs/2511.05383)
*Elinor Thompson,Tiantian He,Anna Schroder,Ahmed Abdulaal,Alec Sargood,Sonja Soskic,Henry F. J. Tregidgo,Daniel C. Alexander*

Main category: cs.CE

TL;DR: 本文提出用大语言模型（LLMs）为脑连接组学生成定量先验知识的管道，可提高脑白质纤维束成像准确性及病理传播模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有脑白质纤维束成像算法在敏感性和特异性上存在权衡，限制准确性，整合先验知识虽有效但用于连接组学存在困难。

Method: 开发基于LLMs的管道生成定量先验知识，与金标准图谱对比进行基准测试，确定提示技术和整合外部知识源的策略。

Result: 识别出能从LLMs获取准确连接信息的提示技术，整合外部知识可提高准确性，LLM先验能增强现有过滤方法。

Conclusion: LLM生成的先验知识能提高脑连接组学模型的准确性，保留的连接是有效的。

Abstract: Tractography is a unique method for mapping white matter connections in the
brain, but tractography algorithms suffer from an inherent trade-off between
sensitivity and specificity that limits accuracy. Incorporating prior knowledge
of white matter anatomy is an effective strategy for improving accuracy and has
been successful for reducing false positives and false negatives in
bundle-mapping protocols. However, it is challenging to scale this approach for
connectomics due to the difficulty in synthesising information relating to many
thousands of possible connections. In this work, we develop and evaluate a
pipeline using large language models (LLMs) to generate quantitative priors for
connectomics, based on their knowledge of neuroanatomy. We benchmark our
approach against an evaluation set derived from a gold-standard tractography
atlas, identifying prompting techniques to elicit accurate connectivity
information from the LLMs. We further identify strategies for incorporating
external knowledge sources into the pipeline, which can provide grounding for
the LLM and improve accuracy. Finally, we demonstrate how the LLM-derived
priors can augment existing tractography filtering approaches by identifying
true-positive connections to retain during the filtering process. We show that
these additional connections can improve the accuracy of a connectome-based
model of pathology spread, which provides supporting evidence that the
connections preserved by the LLM are valid.

</details>


### [10] [Block-structured Operator Inference for coupled multiphysics model reduction](https://arxiv.org/abs/2511.05389)
*Benjamin G. Zastrow,Anirban Chaudhuri,Karen E. Willcox,Anthony Ashley,Michael Chamberlain Henson*

Main category: cs.CE

TL;DR: 提出块结构的算子推理公式来学习多物理系统的结构降阶模型，在气动弹性分析中展现优势。


<details>
  <summary>Details</summary>
Motivation: 为多物理系统学习结构化降阶模型，保留物理系统结构和特性。

Method: 指定各物理组件控制方程结构和耦合项结构，按非侵入式算子推理方法从快照数据学习降阶模型。

Result: 在AGARD 445.6机翼基准测试中，块结构算子推理比整体算子推理在线预测平均提速20%，且保持精度。

Conclusion: 块结构公式相比整体算子推理公式有优势，能降维、允许定制正则化。

Abstract: This paper presents a block-structured formulation of Operator Inference as a
way to learn structured reduced-order models for multiphysics systems. The
approach specifies the governing equation structure for each physics component
and the structure of the coupling terms. Once the multiphysics structure is
specified, the reduced-order model is learned from snapshot data following the
nonintrusive Operator Inference methodology. In addition to preserving physical
system structure, which in turn permits preservation of system properties such
as stability and second-order structure, the block-structured approach has the
advantages of reducing the overall dimensionality of the learning problem and
admitting tailored regularization for each physics component. The numerical
advantages of the block-structured formulation over a monolithic Operator
Inference formulation are demonstrated for aeroelastic analysis, which couples
aerodynamic and structural models. For the benchmark test case of the AGARD
445.6 wing, block-structured Operator Inference provides an average 20% online
prediction speedup over monolithic Operator Inference across subsonic and
supersonic flow conditions in both the stable and fluttering parameter regimes
while preserving the accuracy achieved with monolithic Operator Inference.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [11] [An Efficient Proximity Graph-based Approach to Table Union Search](https://arxiv.org/abs/2511.05082)
*Yiming Xie,Hua Dai,Mingfeng Jiang,Pengyue Li,zhengkai Zhang,Bohan Li*

Main category: cs.DB

TL;DR: 本文针对表格联合搜索问题效率挑战，提出PGTUS方法，在保持召回率的同时提升搜索速度。


<details>
  <summary>Details</summary>
Motivation: 表格联合搜索问题中多向量模型虽检索质量高，但因依赖二分图最大匹配计算联合性得分，面临比单向量问题更严峻的效率挑战。

Method: 提出基于邻近图的表格联合搜索（PGTUS）方法，采用多阶段流水线，结合新颖的细化策略、基于多对一二分匹配的过滤策略，还提出增强的剪枝策略。

Result: 在六个基准数据集上的实验表明，该方法比现有方法实现了3.6 - 6.0倍的加速，同时保持了相当的召回率。

Conclusion: PGTUS方法能有效提高表格联合搜索的效率。

Abstract: Neural embedding models are extensively employed in the table union search
problem, which aims to find semantically compatible tables that can be merged
with a given query table. In particular, multi-vector models, which represent a
table as a vector set (typically one vector per column), have been demonstrated
to achieve superior retrieval quality by capturing fine-grained semantic
alignments. However, this problem faces more severe efficiency challenges than
the single-vector problem due to the inherent dependency on bipartite graph
maximum matching to compute unionability scores. Therefore, this paper proposes
an efficient Proximity Graph-based Table Union Search (PGTUS) approach. PGTUS
employs a multi-stage pipeline that combines a novel refinement strategy, a
filtering strategy based on many-to-one bipartite matching. Besides, we propose
an enhanced pruning strategy to prune the candidate set, which further improve
the search efficiency. Extensive experiments on six benchmark datasets
demonstrate that our approach achieves 3.6-6.0X speedup over existing
approaches while maintaining comparable recall rates.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [12] [Marionette: Data Structure Description and Management for Heterogeneous Computing](https://arxiv.org/abs/2511.04853)
*Nuno dos Santos Fernandes,Pedro Tomás,Nuno Roma,Frank Winklmeier,Patricia Conde-Muíño*

Main category: cs.DC

TL;DR: Marionette是一个C++17库，可解决大型面向对象C++代码库硬件加速适配难题，本文介绍其设计、用法和性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型面向对象C++代码库在异构平台（如GPU）上进行硬件加速适配的挑战。

Method: 设计C++17库Marionette，解耦数据布局与接口描述，支持多内存管理策略，提供高效数据传输和转换。

Result: 通过基于CUDA的案例研究证明了Marionette的效率和灵活性。

Conclusion: Marionette能实现灵活、高效、可移植的数据结构定义，与现有代码兼容，支持多种用例。

Abstract: Adapting large, object-oriented C++ codebases for hardware acceleration might
be extremely challenging, particularly when targeting heterogeneous platforms
such as GPUs. Marionette is a C++17 library designed to address this by
enabling flexible, efficient, and portable data structure definitions. It
decouples data layout from the description of the interface, supports multiple
memory management strategies, and provides efficient data transfers and
conversions across devices, all of this with minimal runtime overhead due to
the compile-time nature of its abstractions. By allowing interfaces to be
augmented with arbitrary functions, Marionette maintains compatibility with
existing code and offers a streamlined interface that supports both
straightforward and advanced use cases. This paper outlines its design, usage,
and performance, including a CUDA-based case study demonstrating its efficiency
and flexibility.

</details>


### [13] [Accelerating HDC-CNN Hybrid Models Using Custom Instructions on RISC-V GPUs](https://arxiv.org/abs/2511.05053)
*Wakuto Matsumi,Riaz-Ul-Haque Mian*

Main category: cs.DC

TL;DR: 本文设计实现针对HDC操作优化的定制GPU指令，用于混合HDC - CNN工作负载，实验显示性能大幅提升。


<details>
  <summary>Details</summary>
Motivation: 神经网络机器学习能耗高，HDC精度低，混合加速器通用性和可编程性差，RISC - V架构为特定领域GPU设计带来新机遇。

Method: 设计并实现针对HDC操作优化的定制GPU指令，用于处理混合HDC - CNN工作负载。

Result: 使用四种定制HDC指令的实验在微基准测试中性能提升达56.2倍。

Conclusion: RISC - V GPU在节能高性能计算方面有潜力。

Abstract: Machine learning based on neural networks has advanced rapidly, but the high
energy consumption required for training and inference remains a major
challenge. Hyperdimensional Computing (HDC) offers a lightweight,
brain-inspired alternative that enables high parallelism but often suffers from
lower accuracy on complex visual tasks. To overcome this, hybrid accelerators
combining HDC and Convolutional Neural Networks (CNNs) have been proposed,
though their adoption is limited by poor generalizability and programmability.
The rise of open-source RISC-V architectures has created new opportunities for
domain-specific GPU design. Unlike traditional proprietary GPUs, emerging
RISC-V-based GPUs provide flexible, programmable platforms suitable for custom
computation models such as HDC. In this study, we design and implement custom
GPU instructions optimized for HDC operations, enabling efficient processing
for hybrid HDC-CNN workloads. Experimental results using four types of custom
HDC instructions show a performance improvement of up to 56.2 times in
microbenchmark tests, demonstrating the potential of RISC-V GPUs for
energy-efficient, high-performance computing.

</details>


### [14] [GPU Under Pressure: Estimating Application's Stress via Telemetry and Performance Counters](https://arxiv.org/abs/2511.05067)
*Giuseppe Esposito,Juan-David Guerrero-Balaguera,Josie Esteban Rodriguez Condia,Matteo Sonza Reorda,Marco Barbiero,Rossella Fortuna*

Main category: cs.DC

TL;DR: 结合在线遥测参数和硬件性能计数器评估GPU压力，实验表明可通过相关数据和计数器估计并行工作负载压力。


<details>
  <summary>Details</summary>
Motivation: 持续工作负载会给GPU组件带来压力，引发可靠性问题，需估计应用程序带来的压力以预测可靠性。

Method: 结合在线遥测参数和硬件性能计数器评估不同应用程序对GPU造成的压力。

Result: 通过结合遥测数据和能揭示目标工作负载资源使用效率的性能计数器，可以估计并行工作负载带来的压力，所选性能计数器关注吞吐量、发出指令数量和停顿事件。

Conclusion: 结合相关数据和性能计数器可有效估计GPU压力。

Abstract: Graphics Processing Units (GPUs) are specialized accelerators in data centers
and high-performance computing (HPC) systems, enabling the fast execution of
compute-intensive applications, such as Convolutional Neural Networks (CNNs).
However, sustained workloads can impose significant stress on GPU components,
raising reliability concerns due to potential faults that corrupt the
intermediate application computations, leading to incorrect results. Estimating
the stress induced by an application is thus crucial to predict reliability
(with\,special\,emphasis\,on\,aging\,effects). In this work, we combine online
telemetry parameters and hardware performance counters to assess GPU stress
induced by different applications. The experimental results indicate the stress
induced by a parallel workload can be estimated by combining telemetry data and
Performance Counters that reveal the efficiency in the resource usage of the
target workload. For this purpose the selected performance counters focus on
measuring the i) throughput, ii) amount of issued instructions and iii) stall
events.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [15] [Improved Additive Approximation Algorithms for APSP](https://arxiv.org/abs/2511.04775)
*Ce Jin,Yael Kirkpatrick,Michał Stawarz,Virginia Vassilevska Williams*

Main category: cs.DS

TL;DR: 本文改进无向无权图中All - Pairs Shortest Paths (APSP)问题近似算法的时间复杂度，不使用有界差(min, +)积算法，采用图分解技术。


<details>
  <summary>Details</summary>
Motivation: 前人用快速矩阵乘法代数工具改进APSP近似算法，作者希望进一步提升其时间复杂度。

Method: 将输入图分解为少量恒定直径的簇和低度数顶点的剩余部分，仅使用标准快速矩阵乘法。

Result: 对于+2近似APSP，算法时间复杂度从$O(n^{2.259})$提升到$O(n^{2.2255})$；对于+4和+6近似APSP，时间复杂度分别从$O(n^{2.155})$和$O(n^{2.103})$提升到$O(n^{2.1462})$和$O(n^{2.1026})$。

Conclusion: 提出的不依赖有界差(min, +)积算法的图分解技术有效提升了APSP近似算法的时间复杂度，该技术对最短路径问题研究有独立价值。

Abstract: The All-Pairs Shortest Paths (APSP) is a foundational problem in theoretical
computer science. Approximating APSP in undirected unweighted graphs has been
studied for many years, beginning with the work of Dor, Halperin and Zwick
[SICOMP'01]. Many recent works have attempted to improve these original
algorithms using the algebraic tools of fast matrix multiplication. We improve
on these results for the following problems.
  For $+2$-approximate APSP, the state-of-the-art algorithm runs in
$O(n^{2.259})$ time [D\"urr, IPL 2023; Deng, Kirkpatrick, Rong, Vassilevska
Williams, and Zhong, ICALP 2022]. We give an improved algorithm in
$O(n^{2.2255})$ time.
  For $+4$ and $+6$-approximate APSP, we achieve time complexities
$O(n^{2.1462})$ and $O(n^{2.1026})$ respectively, improving the previous
$O(n^{2.155})$ and $O(n^{2.103})$ achieved by [Saha and Ye, SODA 2024].
  In contrast to previous works, we do not use the big hammer of
bounded-difference $(\min,+)$-product algorithms. Instead, our algorithms are
based on a simple technique that decomposes the input graph into a small number
of clusters of constant diameter and a remainder of low degree vertices, which
could be of independent interest in the study of shortest paths problems. We
then use only standard fast matrix multiplication to obtain our improvements.

</details>


### [16] [Optimal Parallel Basis Finding in Graphic and Related Matroids](https://arxiv.org/abs/2511.04826)
*Sanjeev Khanna,Aaron Putterman,Junkai Song*

Main category: cs.DS

TL;DR: 本文研究图拟阵基的并行复杂度，给出确定性算法解决长期未决问题，并将框架拓展到满足特定性质的二元拟阵。


<details>
  <summary>Details</summary>
Motivation: Karp等人提出能否同时实现对数轮数和多项式查询数的问题，本文旨在解决此问题。

Method: 给出确定性算法，使用O(log m)自适应轮数和每轮poly(m)非自适应查询来返回生成森林，并给出匹配的下界。

Result: 得到确定性算法，同时有匹配的下界，解决长期问题，且框架可拓展到满足特定性质的二元拟阵。

Conclusion: 精确刻画图拟阵的自适应轮复杂度，框架可用于满足平滑回路计数性质的二元拟阵。

Abstract: We study the parallel complexity of finding a basis of a graphic matroid
under independence-oracle access. Karp, Upfal, and Wigderson (FOCS 1985, JCSS
1988) initiated the study of this problem and established two algorithms for
finding a spanning forest: one running in $O(\log m)$ rounds with
$m^{\Theta(\log m)}$ queries, and another, for any $d \in \mathbb{Z}^+$,
running in $O(m^{2/d})$ rounds with $\Theta(m^d)$ queries. A key open question
they posed was whether one could simultaneously achieve polylogarithmic rounds
and polynomially many queries. We give a deterministic algorithm that uses
$O(\log m)$ adaptive rounds and $\mathrm{poly}(m)$ non-adaptive queries per
round to return a spanning forest on $m$ edges, and complement this result with
a matching $\Omega(\log m)$ lower bound for any (even randomized) algorithm
with $\mathrm{poly}(m)$ queries per round. Thus, the adaptive round complexity
for graphic matroids is characterized exactly, settling this long-standing
problem. Beyond graphs, we show that our framework also yields an $O(\log
m)$-round, $\mathrm{poly}(m)$-query algorithm for any binary matroid satisfying
a smooth circuit counting property, implying, among others, an optimal $O(\log
m)$-round parallel algorithms for finding bases of cographic matroids.

</details>


### [17] [Tight Bounds for Sampling q-Colorings via Coupling from the Past](https://arxiv.org/abs/2511.04982)
*Tianxing Ding,Hongyang Liu,Yitong Yin,Can Zhou*

Main category: cs.DS

TL;DR: 本文确定了基于边界链的图着色CFTP算法的渐近紧阈值，证明下界并给出达到最优阈值的算法。


<details>
  <summary>Details</summary>
Motivation: 前人对图的均匀q着色的CFTP算法的适用范围（q与最大度Δ的关系）逐步改进，本文旨在确定基于边界链的CFTP算法的渐近紧阈值。

Method: 证明满足标准收缩性质的算法的下界，通过最优设计边界链给出达到渐近最优阈值的高效CFTP算法。

Result: 证明所有满足标准收缩性质的算法需要q ≥ 2.5Δ，并给出达到渐近最优阈值q ≥ (2.5 + o(1))Δ的高效CFTP算法。

Conclusion: 确定了基于边界链的图着色CFTP算法的渐近紧阈值。

Abstract: The Coupling from the Past (CFTP) paradigm is a canonical method for perfect
sampling. For uniform sampling of proper $q$-colorings in graphs with maximum
degree $\Delta$, the bounding chains of Huber (STOC 1998) provide a systematic
framework for efficiently implementing CFTP algorithms within the classical
regime $q \ge (1 + o(1))\Delta^2$. This was subsequently improved to $q >
3\Delta$ by Bhandari and Chakraborty (STOC 2020) and to $q \ge (8/3 +
o(1))\Delta$ by Jain, Sah, and Sawhney (STOC 2021).
  In this work, we establish the asymptotically tight threshold for
bounding-chain-based CFTP algorithms for graph colorings. We prove a lower
bound showing that all such algorithms satisfying the standard contraction
property require $q \ge 2.5\Delta$, and we present an efficient CFTP algorithm
that achieves this asymptotically optimal threshold $q \ge (2.5 + o(1))\Delta$
via an optimal design of bounding chains.

</details>


### [18] [Language Generation and Identification From Partial Enumeration: Tight Density Bounds and Topological Characterizations](https://arxiv.org/abs/2511.05295)
*Jon Kleinberg,Fan Wei*

Main category: cs.DS

TL;DR: 研究语言生成与学习理论，解决语言生成极限框架下最佳可实现低密度的紧界问题，拓展到部分枚举情况，还重新审视语言识别模型并给出新拓扑表述。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的成功促使对语言生成和学习进行形式化理论研究。

Method: 理论证明，解决语言生成极限框架下的开放问题，拓展模型到部分枚举情况，并对语言识别模型进行拓扑分析。

Result: 证明语言生成极限框架下最佳可实现低密度的紧界为1/2；在部分枚举情况下，算法输出密度至少为α/2；给出语言识别的新拓扑表述。

Conclusion: 解决了语言生成极限框架的主要开放问题，拓展模型到部分信息设置，为语言识别提供新拓扑视角。

Abstract: The success of large language models (LLMs) has motivated formal theories of
language generation and learning. We study the framework of \emph{language
generation in the limit}, where an adversary enumerates strings from an unknown
language $K$ drawn from a countable class, and an algorithm must generate
unseen strings from $K$. Prior work showed that generation is always possible,
and that some algorithms achieve positive lower density, revealing a
\emph{validity--breadth} trade-off between correctness and coverage. We resolve
a main open question in this line, proving a tight bound of $1/2$ on the best
achievable lower density. We then strengthen the model to allow \emph{partial
enumeration}, where the adversary reveals only an infinite subset $C \subseteq
K$. We show that generation in the limit remains achievable, and if $C$ has
lower density $\alpha$ in $K$, the algorithm's output achieves density at least
$\alpha/2$, matching the upper bound. This generalizes the $1/2$ bound to the
partial-information setting, where the generator must recover within a factor
$1/2$ of the revealed subset's density. We further revisit the classical
Gold--Angluin model of \emph{language identification} under partial
enumeration. We characterize when identification in the limit is possible --
when hypotheses $M_t$ eventually satisfy $C \subseteq M \subseteq K$ -- and in
the process give a new topological formulation of Angluin's characterization,
showing that her condition is precisely equivalent to an appropriate
topological space having the $T_D$ separation property.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [19] [Dynamic Allocation of Public Goods with Approximate Core Equilibria](https://arxiv.org/abs/2511.04817)
*Chido Onyeze,David X. Lin,Siddhartha Banerjee,Éva Tardos*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of repeatedly allocating multiple shareable public
goods that have limited availability in an online setting without the use of
money. In our setting, agents have additive values, and the value each agent
receives from getting access to the goods in each period is drawn i.i.d. from
some joint distribution $\mathcal{D}$ (that can be arbitrarily correlated
between agents). The principal also has global constraints on the set of goods
they can select over the horizon, which is represented via a submodular
allocation-cost function. Our goal is to select the periods to allocate the
good to ensure high value for each group of agents.
  We develop mechanisms for this problem using an artificial currency, where we
give each agent a budget proportional to their (exogenous) fair share. The
correlated value distribution makes this an especially challenging problem, as
agents may attempt to free-ride by declaring low valuations for the good when
they know other agents have high values-hoping those agents will bear a larger
share of the cost of the resource. We offer a black-box reduction from monetary
mechanisms for the allocation of a costly excludable public good. We focus on
pacing strategies, the natural strategies when using AI agents, where agents
report a scaled version of their value to the mechanism. Our main results show
that when using a truthful monetary mechanism as our building block, the
resulting online mechanism has a focal equilibrium in which each agent plays a
pacing strategy whose outcome results in an allocation that is a
$(\mathcal{H}_n-1)$-approximation of the core, where $\mathcal{H}_n$ is the
Harmonic number, and $n$ is the number of agents. Remarkably, we are able to
achieve an approximate core solution as a Nash outcome without explicit
collaboration or coordination between the agents.

</details>


### [20] [Persuading Stable Matching](https://arxiv.org/abs/2511.04846)
*Jonathan Shaki,Jiarui Gan,Sarit Kraus*

Main category: cs.GT

TL;DR: 研究在二分匹配问题中，主体如何通过贝叶斯说服塑造代理人信念以诱导稳定匹配，考虑两种特殊情况，得出完整复杂度图景。


<details>
  <summary>Details</summary>
Motivation: 在二分匹配问题不确定环境下，研究主体如何通过贝叶斯说服塑造代理人信念，以诱导出最大化期望效用的稳定匹配。

Method: 考虑两种重要特殊情况，即代理人可按价值函数分为少量类型和可能世界状态数量较少的情况，研究公开和私人信号设置。

Result: 绘制完整复杂度图景，私人说服在世界数量少的情况下仍难处理，其他情况有多项式时间算法，给出可处理情况的高效算法并证明难处理情况的NP难。

Conclusion: 阐明了信息设计下稳定匹配的算法边界，明确了最优说服在计算上可行的情况。

Abstract: In bipartite matching problems, agents on two sides of a graph want to be
paired according to their preferences. The stability of a matching depends on
these preferences, which in uncertain environments also reflect agents' beliefs
about the underlying state of the world. We investigate how a principal -- who
observes the true state of the world -- can strategically shape these beliefs
through Bayesian persuasion to induce stable matching that maximizes a desired
utility. Due to the general intractability of the underlying matching
optimization problem as well as the multi-receiver persuasion problem, our main
considerations are two important special cases: (1) when agents can be
categorized into a small number of types based on their value functions, and
(2) when the number of possible world states is small. For each case, we study
both public and private signaling settings. Our results draw a complete
complexity landscape: we show that private persuasion remains intractable even
when the number of worlds is small, while all other settings admit
polynomial-time algorithms. We present efficient algorithms for each tractable
case and prove NP-hardness for the intractable ones. These results illuminate
the algorithmic frontier of stable matching under information design and
clarify when optimal persuasion is computationally feasible.

</details>


### [21] [Optimal Selection Using Algorithmic Rankings with Side Information](https://arxiv.org/abs/2511.04867)
*Kate Donahue,Nicole Immorlica,Brendan Lucier*

Main category: cs.GT

TL;DR: 研究在线平台中代理选择候选人问题，发现排名工具准确性提高可能降低社会福利。


<details>
  <summary>Details</summary>
Motivation: 受在线平台（如就业市场）启发，研究代理在有排名噪声和自由 - 忙碌信号下的最优选择问题及排名工具准确性对结果的影响。

Method: 研究一个高价值候选人和任意数量低价值候选人的情况，并探讨结果在更一般情况下的适用性。

Result: 排名工具准确性提高可能因代理更倾向向忙碌候选人发出邀约和更可能选择低排名候选人而降低社会福利。

Conclusion: 讨论了结果在更一般设置下成立的条件。

Abstract: Motivated by online platforms such as job markets, we study an agent choosing
from a list of candidates, each with a hidden quality that determines match
value. The agent observes only a noisy ranking of the candidates plus a binary
signal that indicates whether each candidate is "free" or "busy." Being busy is
positively correlated with higher quality, but can also reduce value due to
decreased availability. We study the agent's optimal selection problem in the
presence of ranking noise and free-busy signals and ask how the accuracy of the
ranking tool impacts outcomes. In a setting with one high-valued candidate and
an arbitrary number of low-valued candidates, we show that increased accuracy
of the ranking tool can result in reduced social welfare. This can occur for
two reasons: agents may be more likely to make offers to busy candidates, and
(paradoxically) may be more likely to select lower-ranked candidates when
rankings are more indicative of quality. We further discuss conditions under
which these results extend to more general settings.

</details>


### [22] [Fair Division with Indivisible Goods, Chores, and Cake](https://arxiv.org/abs/2511.04891)
*Haris Aziz,Xinhang Lu,Simon Mackenzie,Mashbat Suzuki*

Main category: cs.GT

TL;DR: 研究具有加性效用的代理对不可分割物品和可分割物品（蛋糕）的公平分配问题，证明了EFM分配总是存在。


<details>
  <summary>Details</summary>
Motivation: 解决具有加性效用的代理对不可分割物品和可分割物品的公平分配问题。

Method: 研究几乎无嫉妒分配，使用无嫉妒混合资源（EFM）的公平概念。

Result: 证明了对于任何数量具有加性效用的代理，存在不可分割物品和蛋糕的EFM分配。

Conclusion: 在不可分割物品和蛋糕的分配中，EFM分配总是存在。

Abstract: We study the problem of fairly allocating indivisible items and a desirable
heterogeneous divisible good (i.e., cake) to agents with additive utilities. In
our paper, each indivisible item can be a good that yields non-negative
utilities to some agents and a chore that yields negative utilities to the
other agents. Given a fixed set of divisible and indivisible resources, we
investigate almost envy-free allocations, captured by the natural fairness
concept of envy-freeness for mixed resources (EFM). It requires that an agent
$i$ does not envy another agent $j$ if agent $j$'s bundle contains any piece of
cake yielding positive utility to agent $i$ (i.e., envy-freeness), and agent
$i$ is envy-free up to one item (EF1) towards agent $j$ otherwise. We prove
that with indivisible items and a cake, an EFM allocation always exists for any
number of agents with additive utilities.

</details>


### [23] [On the Coordination of Value-Maximizing Bidders](https://arxiv.org/abs/2511.04993)
*Yanru Guan,Jiahao Zhang,Zhe Feng,Tao Lin*

Main category: cs.GT

TL;DR: 本文研究在线广告平台中多个自动出价者的协调问题，证明协调机制优于独立出价。


<details>
  <summary>Details</summary>
Motivation: 现有自动出价文献多考虑独立出价，本文研究多个自动出价者的协调问题，以第三方代理管理的多个出价者协作出价和单个广告商管理的多个广告活动战略出价为场景。

Method: 将协调问题形式化为理论模型，在合成和真实数据集上进行模拟。

Result: 理论上，仅最高价值出价者与外部出价竞争的协调机制严格优于独立出价；模拟结果支持协调机制表现更好。

Conclusion: 在线拍卖自动出价中协调机制具有理论潜力和实际稳健性。

Abstract: While the auto-bidding literature predominantly considers independent
bidding, we investigate the coordination problem among multiple auto-bidders in
online advertising platforms. Two motivating scenarios are: collaborative
bidding among multiple distinct bidders managed by a third-party bidding agent,
and strategic bid selection for multiple ad campaigns managed by a single
advertiser. We formalize this coordination problem as a theoretical model and
demonstrate that a straightforward coordination mechanism, where only the
highest-value bidder competes with outside bids, strictly dominates independent
bidding, improving both Return-on-Spend (RoS) compliance and the total value
accrued for each participating auto-bidder or ad campaign. Additionally, our
simulations on synthetic and real-world datasets support the theoretical result
that coordinated mechanism outperforms independent bidding. These findings
highlight both the theoretical potential and the practical robustness of
coordination in auto-bidding in online auctions.

</details>


### [24] [Cooperation Under Network-Constrained Communication](https://arxiv.org/abs/2511.05290)
*Tommy Mordo,Omer Madmon,Moshe Tennenholtz*

Main category: cs.GT

TL;DR: 研究网络通信受限下分布式博弈中的合作，推导合作均衡充分条件并分析不同通信情况，阐明通信延迟和网络设计对分布式合作的影响。


<details>
  <summary>Details</summary>
Motivation: 研究网络通信受限下分布式博弈中的合作问题。

Method: 基于Monderer和Tennenholtz（1999）的框架，推导依赖网络直径和位置数量的合作均衡充分条件，分析不同通信情况。

Result: 得到了合作均衡的充分条件，分析了瞬时、延迟和按比例延迟通信的极端情况以及无标度通信网络的渐近情况。

Conclusion: 通信延迟和网络设计共同决定分布式合作的出现。

Abstract: In this paper, we study cooperation in distributed games under
network-constrained communication. Building on the framework of Monderer and
Tennenholtz (1999), we derive a sufficient condition for cooperative
equilibrium in settings where communication between agents is delayed by the
underlying network topology. Each player deploys an agent at every location,
and local interactions follow a Prisoner's Dilemma structure. We derive a
sufficient condition that depends on the network diameter and the number of
locations, and analyze extreme cases of instantaneous, delayed, and
proportionally delayed communication. We also discuss the asymptotic case of
scale-free communication networks, in which the network diameter grows
sub-linearly in the number of locations. These insights clarify how
communication latency and network design jointly determine the emergence of
distributed cooperation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [25] [Association via Entropy Reduction](https://arxiv.org/abs/2511.04901)
*Anthony Gamst,Lawrence Wilson*

Main category: cs.IR

TL;DR: 提出aver分数，对比发现其在找关联文档对方面优于tf - idf，并阐述二者优缺点。


<details>
  <summary>Details</summary>
Motivation: 在神经网络取得成功前，tf - idf被视为识别查询相关文档的最佳选择，作者希望提供不同的分数。

Method: 在有真实关联标记的数据集上对比aver和tf - idf。

Result: aver在找关联对方面比tf - idf好，且有诸多优势，但也存在计算复杂、分数难解释等问题。

Conclusion: aver在某些方面比tf - idf更优，但也有自身局限性。

Abstract: Prior to recent successes using neural networks, term frequency-inverse
document frequency (tf-idf) was clearly regarded as the best choice for
identifying documents related to a query. We provide a different score, aver,
and observe, on a dataset with ground truth marking for association, that aver
does do better at finding assciated pairs than tf-idf. This example involves
finding associated vertices in a large graph and that may be an area where
neural networks are not currently an obvious best choice. Beyond this one
anecdote, we observe that (1) aver has a natural threshold for declaring pairs
as unassociated while tf-idf does not, (2) aver can distinguish between pairs
of documents for which tf-idf gives a score of 1.0, (3) aver can be applied to
larger collections of documents than pairs while tf-idf cannot, and (4) that
aver is derived from entropy under a simple statistical model while tf-idf is a
construction designed to achieve a certain goal and hence aver may be more
"natural." To be fair, we also observe that (1) writing down and computing the
aver score for a pair is more complex than for tf-idf and (2) that the fact
that the aver score is naturally scale-free makes it more complicated to
interpret aver scores.

</details>


### [26] [Search Is Not Retrieval: Decoupling Semantic Matching from Contextual Assembly in RAG](https://arxiv.org/abs/2511.04939)
*Harshit Nainwani,Hediyeh Baban*

Main category: cs.IR

TL;DR: 介绍SINR框架，可增强检索系统性能，为下一代AI检索系统提供基础。


<details>
  <summary>Details</summary>
Motivation: 解决当前检索系统混淆搜索和推理上下文的问题。

Method: 引入SINR双层次架构，区分细粒度搜索表示和粗粒度检索上下文。

Result: 增强检索系统的可组合性、可扩展性和上下文保真度，使检索从被动变为主动。

Conclusion: 为下一代使用检索的AI系统提供了实用基础。

Abstract: Retrieval systems are essential to contemporary AI pipelines, although most
confuse two separate processes: finding relevant information and giving enough
context for reasoning. We introduce the Search-Is-Not-Retrieve (SINR)
framework, a dual-layer architecture that distinguishes between fine-grained
search representations and coarse-grained retrieval contexts. SINR enhances the
composability, scalability, and context fidelity of retrieval systems by
directly connecting small, semantically accurate search chunks to larger,
contextually complete retrieve chunks, all without incurring extra processing
costs. This design changes retrieval from a passive step to an active one,
making the system architecture more like how people process information. We
discuss the SINR framework's conceptual foundation, formal structure,
implementation issues, and qualitative outcomes. This provides a practical
foundation for the next generation of AI systems that use retrieval.

</details>


### [27] [Query Generation Pipeline with Enhanced Answerability Assessment for Financial Information Retrieval](https://arxiv.org/abs/2511.05000)
*Hyunkyu Kim,Yeeun Yoo,Youngjun Kwak*

Main category: cs.IR

TL;DR: 文章提出基于大语言模型的查询生成方法构建特定领域信息检索基准，构建了KoBankIR，实验显示现有检索模型在其复杂查询上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法满足现实银行场景需求，构建特定领域基准成本高且受法律限制。

Method: 提出基于大语言模型的查询生成构建特定领域信息检索基准的系统方法，结合单文档和多文档查询生成与增强的可回答性评估方法。

Result: 构建了包含815个查询的KoBankIR，现有检索模型在其复杂多文档查询上表现不佳。

Conclusion: 该系统方法对特定领域基准构建有价值，金融领域需改进检索技术。

Abstract: As financial applications of large language models (LLMs) gain attention,
accurate Information Retrieval (IR) remains crucial for reliable AI services.
However, existing benchmarks fail to capture the complex and domain-specific
information needs of real-world banking scenarios. Building domain-specific IR
benchmarks is costly and constrained by legal restrictions on using real
customer data. To address these challenges, we propose a systematic methodology
for constructing domain-specific IR benchmarks through LLM-based query
generation. As a concrete implementation of this methodology, our pipeline
combines single and multi-document query generation with an enhanced and
reasoning-augmented answerability assessment method, achieving stronger
alignment with human judgments than prior approaches. Using this methodology,
we construct KoBankIR, comprising 815 queries derived from 204 official banking
documents. Our experiments show that existing retrieval models struggle with
the complex multi-document queries in KoBankIR, demonstrating the value of our
systematic approach for domain-specific benchmark construction and underscoring
the need for improved retrieval techniques in financial domains.

</details>


### [28] [Wikipedia-based Datasets in Russian Information Retrieval Benchmark RusBEIR](https://arxiv.org/abs/2511.05079)
*Grigory Kovalev,Natalia Loukachevitch,Mikhail Tikhomirov,Olga Babina,Pavel Mamaev*

Main category: cs.IR

TL;DR: 本文构建俄语信息检索数据集，对比多种模型，分析文档长度影响，强调评估重要性，数据集和代码公开。


<details>
  <summary>Details</summary>
Motivation: 构建俄语信息检索数据集，扩展现有俄语信息检索资源，推动相关研究。

Method: 从俄语维基百科构建数据集，对比词法检索模型、针对俄语微调的神经架构和多语言模型，结合检索与神经重排。

Result: 词法方法在全文档检索表现更好，神经方法在短文本语义捕捉更佳，结合检索与神经重排可提升效果。

Conclusion: 扩展俄语信息检索研究资源，强调准确评估检索模型对实现最优性能的重要性。

Abstract: In this paper, we present a novel series of Russian information retrieval
datasets constructed from the "Did you know..." section of Russian Wikipedia.
Our datasets support a range of retrieval tasks, including fact-checking,
retrieval-augmented generation, and full-document retrieval, by leveraging
interesting facts and their referenced Wikipedia articles annotated at the
sentence level with graded relevance. We describe the methodology for dataset
creation that enables the expansion of existing Russian Information Retrieval
(IR) resources. Through extensive experiments, we extend the RusBEIR research
by comparing lexical retrieval models, such as BM25, with state-of-the-art
neural architectures fine-tuned for Russian, as well as multilingual models.
Results of our experiments show that lexical methods tend to outperform neural
models on full-document retrieval, while neural approaches better capture
lexical semantics in shorter texts, such as in fact-checking or fine-grained
retrieval. Using our newly created datasets, we also analyze the impact of
document length on retrieval performance and demonstrate that combining
retrieval with neural reranking consistently improves results. Our contribution
expands the resources available for Russian information retrieval research and
highlights the importance of accurate evaluation of retrieval models to achieve
optimal performance. All datasets are publicly available at HuggingFace. To
facilitate reproducibility and future research, we also release the full
implementation on GitHub.

</details>


### [29] [QUESTER: Query Specification for Generative Retrieval](https://arxiv.org/abs/2511.05301)
*Arthur Satouf,Yuxuan Zong,Habiboulaye Amadou-Boubacar,Pablo Piantanida,Benjamin Piwowarski*

Main category: cs.IR

TL;DR: 提出QUESTER方法，将生成式检索重构为查询规范生成，模型在评估中比BM25有效，与神经IR模型有竞争力且效率高。


<details>
  <summary>Details</summary>
Motivation: 生成式检索（GR）泛化困难且扩展成本高。

Method: 将GR重构为查询规范生成，使用小语言模型处理简单关键词查询，用强化学习技术（GRPO）训练策略。

Result: 模型在内外领域评估中比BM25更有效，与神经IR模型有竞争力。

Conclusion: QUESTER方法在保证效率的同时有较好的检索效果。

Abstract: Generative Retrieval (GR) differs from the traditional index-then-retrieve
pipeline by storing relevance in model parameters and directly generating
document identifiers. However, GR often struggles to generalize and is costly
to scale. We introduce QUESTER (QUEry SpecificaTion gEnerative Retrieval),
which reframes GR as query specification generation - in this work, a simple
keyword query handled by BM25 - using a (small) LLM. The policy is trained
using reinforcement learning techniques (GRPO). Across in- and out-of-domain
evaluations, we show that our model is more effective than BM25, and
competitive with neural IR models, while maintaining a good efficiency

</details>


### [30] [TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation Framework](https://arxiv.org/abs/2511.05385)
*Chao Zhang,Yuhao Wang,Derong Xu,Haoxin Zhang,Yuanjie Lyu,Yuhao Chen,Shuochen Liu,Tong Xu,Xiangyu Zhao,Yan Gao,Yao Hu,Enhong Chen*

Main category: cs.IR

TL;DR: 提出TeaRAG框架解决代理RAG的令牌开销问题，能压缩检索内容和推理步骤，在多个数据集上提升性能并减少输出令牌。


<details>
  <summary>Details</summary>
Motivation: 现有代理RAG虽经强化学习改进，但搜索和推理过程有大量令牌开销，存在准确性与效率的权衡，需解决此问题。

Method: 1. 用图检索增强基于块的语义检索，构建知识关联图并利用个性化PageRank减少每次检索的令牌数。2. 提出迭代过程感知直接偏好优化（IP - DPO），通过知识匹配机制评估知识充分性并惩罚过多推理步骤。

Result: 在六个数据集上，TeaRAG在Llama3 - 8B - Instruct和Qwen2.5 - 14B - Instruct上分别将平均精确匹配提高4%和2%，同时减少61%和59%的输出令牌。

Conclusion: TeaRAG框架能有效解决代理RAG的令牌开销问题，提升效率和性能。代码开源。

Abstract: Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment
Large Language Models' (LLMs) reliability. For flexibility, agentic RAG employs
autonomous, multi-round retrieval and reasoning to resolve queries. Although
recent agentic RAG has improved via reinforcement learning, they often incur
substantial token overhead from search and reasoning processes. This trade-off
prioritizes accuracy over efficiency. To address this issue, this work proposes
TeaRAG, a token-efficient agentic RAG framework capable of compressing both
retrieval content and reasoning steps. 1) First, the retrieved content is
compressed by augmenting chunk-based semantic retrieval with a graph retrieval
using concise triplets. A knowledge association graph is then built from
semantic similarity and co-occurrence. Finally, Personalized PageRank is
leveraged to highlight key knowledge within this graph, reducing the number of
tokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative
Process-aware Direct Preference Optimization (IP-DPO) is proposed.
Specifically, our reward function evaluates the knowledge sufficiency by a
knowledge matching mechanism, while penalizing excessive reasoning steps. This
design can produce high-quality preference-pair datasets, supporting iterative
DPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the
average Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on
Llama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at
https://github.com/Applied-Machine-Learning-Lab/TeaRAG.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity](https://arxiv.org/abs/2511.04686)
*Pratik Poudel*

Main category: cs.LG

TL;DR: 论文研究大语言模型KV缓存管理策略，指出缓存接近或超模型上下文窗口时生成质量下降，简单保留连续上下文块策略效果更好。


<details>
  <summary>Details</summary>
Motivation: 解决有状态多轮场景下KV缓存无界增长带来的挑战。

Method: 使用有状态基准测试框架进行实证分析。

Result: KV缓存接近或超模型训练上下文窗口时生成质量骤降；常见驱逐策略破坏位置连贯性会恶化性能；简单保留连续上下文块策略生成更连贯。

Conclusion: 提倡尊重架构限制、保留位置结构、全面看待“缓存健康”的驱逐技术。

Abstract: The Key-Value (KV) cache is integral to efficient autoregressive inference in
large language models (LLMs), yet its unbounded growth in stateful multi-turn
scenarios presents major challenges. This paper examines the interplay between
KV cache management strategies, the architectural context limits of models like
meta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of
positional encodings. Through empirical analysis using a stateful benchmarking
framework, we show that LLM generation quality degrades sharply when the
accumulated KV cache approaches or exceeds the model's trained context window
(e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory
exhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via
AttentionTop), can worsen performance if they disrupt positional coherence.
Because LLMs rely on consistent positional signals (e.g., RoPE), compacting a
cache by removing non-contiguous tokens can scramble these signals and lead to
degenerative outputs. We further show that simple strategies preserving
contiguous context blocks (e.g., keeping an initial "gist") can yield more
coherent generations than complex or positionally disruptive ones. We advocate
for eviction techniques that respect architectural limits, preserve positional
structure, and view "cache health" holistically beyond mere size.

</details>


### [32] [Ada-FCN: Adaptive Frequency-Coupled Network for fMRI-Based Brain Disorder Classification](https://arxiv.org/abs/2511.04718)
*Yue Xun,Jiaxing Xu,Wenbo Gao,Chen Yang,Shujun Wang*

Main category: cs.LG

TL;DR: 提出新框架Ada - FCN分析静息态fMRI，在ADNI和ABIDE数据集上表现优于现有方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有模型忽视神经元振荡多频特性，处理BOLD信号存在局限，已有频域方法使用预定义频段有不足。

Method: 提出自适应级联分解学习每个脑区相关频率子带，频率耦合连接学习构建统一功能网络，结合统一GCN中的消息传递机制进行诊断预测。

Result: 在ADNI和ABIDE数据集实验中，性能优于现有方法。

Conclusion: 所提新框架Ada - FCN有效且可行，在脑疾病诊断中有应用潜力。

Abstract: Resting-state fMRI has become a valuable tool for classifying brain disorders
and constructing brain functional connectivity networks
  by tracking BOLD signals across brain regions. However, existing mod els
largely neglect the multi-frequency nature of neuronal oscillations,
  treating BOLD signals as monolithic time series. This overlooks the cru cial
fact that neurological disorders often manifest as disruptions within
  specific frequency bands, limiting diagnostic sensitivity and specificity.
  While some methods have attempted to incorporate frequency informa tion, they
often rely on predefined frequency bands, which may not be
  optimal for capturing individual variability or disease-specific alterations.
  To address this, we propose a novel framework featuring Adaptive Cas cade
Decomposition to learn task-relevant frequency sub-bands for each
  brain region and Frequency-Coupled Connectivity Learning to capture
  both intra- and nuanced cross-band interactions in a unified functional
  network. This unified network informs a novel message-passing mecha nism
within our Unified-GCN, generating refined node representations
  for diagnostic prediction. Experimental results on the ADNI and ABIDE
  datasets demonstrate superior performance over existing methods. The
  code is available at https://github.com/XXYY20221234/Ada-FCN.

</details>


### [33] [DL101 Neural Network Outputs and Loss Functions](https://arxiv.org/abs/2511.05131)
*Fernando Berzal*

Main category: cs.LG

TL;DR: 本文分析神经网络输出层激活函数和损失函数，将损失函数与最大似然估计联系起来，并探讨实际场景。


<details>
  <summary>Details</summary>
Motivation: 从统计角度分析神经网络输出层激活函数与损失函数的联系，为选择合适损失函数提供依据。

Method: 分析常见激活函数数学性质及适用场景，将常见损失函数与最大似然估计原理相连接。

Result: 指出选择特定损失函数等同于假设模型输出的特定概率分布，强调其与广义线性模型的联系。

Conclusion: 考虑了如替代输出编码、受限输出和重尾分布等实际场景。

Abstract: The loss function used to train a neural network is strongly connected to its
output layer from a statistical point of view. This technical report analyzes
common activation functions for a neural network output layer, like linear,
sigmoid, ReLU, and softmax, detailing their mathematical properties and their
appropriate use cases. A strong statistical justification exists for the
selection of the suitable loss function for training a deep learning model.
This report connects common loss functions such as Mean Squared Error (MSE),
Mean Absolute Error (MAE), and various Cross-Entropy losses to the statistical
principle of Maximum Likelihood Estimation (MLE). Choosing a specific loss
function is equivalent to assuming a specific probability distribution for the
model output, highlighting the link between these functions and the Generalized
Linear Models (GLMs) that underlie network output layers. Additional scenarios
of practical interest are also considered, such as alternative output
encodings, constrained outputs, and distributions with heavy tails.

</details>


### [34] [AWEMixer: Adaptive Wavelet-Enhanced Mixer Network for Long-Term Time Series Forecasting](https://arxiv.org/abs/2511.04722)
*Qianyang Li,Xingjun Zhang,Peng Tao,Shaoxun Wang,Yancheng Pan,Jia Wei*

Main category: cs.LG

TL;DR: 提出AWEMixer用于物联网环境长序列时间序列预测，含创新组件，经7个公开基准测试验证比现有模型有效。


<details>
  <summary>Details</summary>
Motivation: 物联网环境长序列时间序列预测因传感器信号特性和误差积累有挑战，传统方法有局限。

Method: 提出Adaptive Wavelet - Enhanced Mixer Network（AWEMixer），含Frequency Router和Coherent Gated Fusion Block两个创新组件。

Result: 七个公开基准测试表明该模型比现有模型更有效，在长序列时间序列预测中比基于transformer和MLP的模型有性能提升。

Conclusion: AWEMixer能实现准确的时频定位且对噪声鲁棒，适用于物联网环境长序列时间序列预测。

Abstract: Forecasting long-term time series in IoT environments remains a significant
challenge due to the non-stationary and multi-scale characteristics of sensor
signals. Furthermore, error accumulation causes a decrease in forecast quality
when predicting further into the future. Traditional methods are restricted to
operate in time-domain, while the global frequency information achieved by
Fourier transform would be regarded as stationary signals leading to blur the
temporal patterns of transient events. We propose AWEMixer, an Adaptive
Wavelet-Enhanced Mixer Network including two innovative components: 1) a
Frequency Router designs to utilize the global periodicity pattern achieved by
Fast Fourier Transform to adaptively weight localized wavelet subband, and 2) a
Coherent Gated Fusion Block to achieve selective integration of prominent
frequency features with multi-scale temporal representation through
cross-attention and gating mechanism, which realizes accurate time-frequency
localization while remaining robust to noise. Seven public benchmarks validate
that our model is more effective than recent state-of-the-art models.
Specifically, our model consistently achieves performance improvement compared
with transformer-based and MLP-based state-of-the-art models in long-sequence
time series forecasting. Code is available at
https://github.com/hit636/AWEMixer

</details>


### [35] [Temporal convolutional and fusional transformer model with Bi-LSTM encoder-decoder for multi-time-window remaining useful life prediction](https://arxiv.org/abs/2511.04723)
*Mohamadreza Akbari Pour,Mohamad Sadeq Karimi,Amir Hossein Mazloumi*

Main category: cs.LG

TL;DR: 提出集成TCNs与改进TFT的框架用于RUL预测，提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以捕捉细粒度时间依赖和动态区分关键特征，需改进工业系统健康预测。

Method: 提出集成TCNs进行局部时间特征提取，结合由Bi - LSTM编码器 - 解码器改进的TFT的框架，采用多时间窗口方法。

Result: 在基准数据集上评估，模型平均RMSE最多降低5.5%，比现有方法预测更准确。

Conclusion: 该框架弥补现有方法的不足，提高工业预测系统有效性，凸显先进时间序列变压器用于RUL预测的潜力。

Abstract: Health prediction is crucial for ensuring reliability, minimizing downtime,
and optimizing maintenance in industrial systems. Remaining Useful Life (RUL)
prediction is a key component of this process; however, many existing models
struggle to capture fine-grained temporal dependencies while dynamically
prioritizing critical features across time for robust prognostics. To address
these challenges, we propose a novel framework that integrates Temporal
Convolutional Networks (TCNs) for localized temporal feature extraction with a
modified Temporal Fusion Transformer (TFT) enhanced by Bi-LSTM encoder-decoder.
This architecture effectively bridges short- and long-term dependencies while
emphasizing salient temporal patterns. Furthermore, the incorporation of a
multi-time-window methodology improves adaptability across diverse operating
conditions. Extensive evaluations on benchmark datasets demonstrate that the
proposed model reduces the average RMSE by up to 5.5%, underscoring its
improved predictive accuracy compared to state-of-the-art methods. By closing
critical gaps in current approaches, this framework advances the effectiveness
of industrial prognostic systems and highlights the potential of advanced
time-series transformers for RUL prediction.

</details>


### [36] [Regularized GLISp for sensor-guided human-in-the-loop optimization](https://arxiv.org/abs/2511.04751)
*Matteo Cercola,Michele Lomuscio,Dario Piga,Simone Formentin*

Main category: cs.LG

TL;DR: 提出GLISp的传感器引导正则化扩展，结合主观反馈与定量传感器信息，在数值评估中表现优于基线GLISp。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好优化的人在环校准方法将系统视为黑盒，忽略了信息丰富的传感器测量。

Method: 引入GLISp的传感器引导正则化扩展，通过物理信息假设函数和最小二乘正则化项将可测量描述符集成到偏好学习循环中。

Result: 在分析基准和人在环车辆悬架调整任务的数值评估中，比基线GLISp收敛更快，最终解决方案更优。

Conclusion: 所提出的方法结合了主观反馈和定量传感器信息，同时保留了基于偏好搜索的灵活性，具有更好的性能。

Abstract: Human-in-the-loop calibration is often addressed via preference-based
optimization, where algorithms learn from pairwise comparisons rather than
explicit cost evaluations. While effective, methods such as Preferential
Bayesian Optimization or Global optimization based on active preference
learning with radial basis functions (GLISp) treat the system as a black box
and ignore informative sensor measurements. In this work, we introduce a
sensor-guided regularized extension of GLISp that integrates measurable
descriptors into the preference-learning loop through a physics-informed
hypothesis function and a least-squares regularization term. This injects
grey-box structure, combining subjective feedback with quantitative sensor
information while preserving the flexibility of preference-based search.
Numerical evaluations on an analytical benchmark and on a human-in-the-loop
vehicle suspension tuning task show faster convergence and superior final
solutions compared to baseline GLISp.

</details>


### [37] [When Data Falls Short: Grokking Below the Critical Threshold](https://arxiv.org/abs/2511.04760)
*Vaibhav Singh,Eugene Belilovsky,Rahaf Aljundi*

Main category: cs.LG

TL;DR: 研究模型grokking现象，探讨知识蒸馏（KD）在数据稀缺和分布转移场景下诱导、加速grokking及实现泛化的作用。


<details>
  <summary>Details</summary>
Motivation: 研究数据稀缺（训练样本低于临界阈值）和分布转移场景下的grokking现象，探索实现模型泛化的方法。

Method: 1. 从已在分布p1上发生grokking的模型进行知识蒸馏，在分布p2上诱导和加速grokking；2. 研究在联合分布（p1, p2）上的训练；3. 考察持续预训练场景，让已grokking的模型从p1过渡到p2。

Result: 1. KD可在数据低于临界阈值时，在不同分布上诱导和加速grokking；2. 从单个分布上grokked的模型进行蒸馏能实现联合分布上的泛化；3. KD能加速泛化、减轻灾难性遗忘，仅用10%数据就能取得良好性能。

Conclusion: 研究为知识转移下的grokking机制提供新见解，强调KD在低数据和分布变化场景下实现泛化的核心作用。

Abstract: In this paper, we investigate the phenomenon of grokking, where models
exhibit delayed generalization following overfitting on training data. We focus
on data-scarce regimes where the number of training samples falls below the
critical threshold, making grokking unobservable, and on practical scenarios
involving distribution shift. We first show that Knowledge Distillation (KD)
from a model that has already grokked on a distribution (p1) can induce and
accelerate grokking on a different distribution (p2), even when the available
data lies below the critical threshold. This highlights the value of KD for
deployed models that must adapt to new distributions under limited data. We
then study training on the joint distribution (p1, p2) and demonstrate that
while standard supervised training fails when either distribution has
insufficient data, distilling from models grokked on the individual
distributions enables generalization. Finally, we examine a continual
pretraining setup, where a grokked model transitions from p1 to p2, and find
that KD both accelerates generalization and mitigates catastrophic forgetting,
achieving strong performance even with only 10% of the data. Together, our
results provide new insights into the mechanics of grokking under knowledge
transfer and underscore the central role of KD in enabling generalization in
low-data and evolving distribution settings.

</details>


### [38] [Scaling Up ROC-Optimizing Support Vector Machines](https://arxiv.org/abs/2511.04979)
*Gimun Bae,Seung Jun Shin*

Main category: cs.LG

TL;DR: 提出ROC - SVM可扩展变体，降低计算复杂度并扩展到非线性分类，理论分析证明近似合理，实验显示性能相当且训练时间大幅减少。


<details>
  <summary>Details</summary>
Motivation: 原始ROC - SVM计算成本高，实际应用受限。

Method: 开发利用不完全U统计量的可扩展变体，通过低秩核近似扩展到非线性分类。

Result: 在合成和真实数据集上，提出方法与原始ROC - SVM的AUC性能相当，但训练时间大幅减少。

Conclusion: 提出的方法有效降低了计算复杂度，能在保证性能的同时提高训练效率。

Abstract: The ROC-SVM, originally proposed by Rakotomamonjy, directly maximizes the
area under the ROC curve (AUC) and has become an attractive alternative of the
conventional binary classification under the presence of class imbalance.
However, its practical use is limited by high computational cost, as training
involves evaluating all $O(n^2)$. To overcome this limitation, we develop a
scalable variant of the ROC-SVM that leverages incomplete U-statistics, thereby
substantially reducing computational complexity. We further extend the
framework to nonlinear classification through a low-rank kernel approximation,
enabling efficient training in reproducing kernel Hilbert spaces. Theoretical
analysis establishes an error bound that justifies the proposed approximation,
and empirical results on both synthetic and real datasets demonstrate that the
proposed method achieves comparable AUC performance to the original ROC-SVM
with drastically reduced training time.

</details>


### [39] [FuseFlow: A Fusion-Centric Compilation Framework for Sparse Deep Learning on Streaming Dataflow](https://arxiv.org/abs/2511.04768)
*Rubens Lacouture,Nathan Zhang,Ritvik Sharma,Marco Siracusa,Fredrik Kjolstad,Kunle Olukotun,Olivia Hsu*

Main category: cs.LG

TL;DR: 提出编译器FuseFlow将稀疏机器学习模型转换为融合稀疏数据流图，支持多种优化，通过实验表明全融合并非对稀疏模型总是最优，还能实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型规模扩大，需解决效率问题，稀疏计算和专用数据流硬件是有效方案，提出FuseFlow编译器。

Method: 开发FuseFlow编译器，支持通用跨表达式稀疏操作融合，还支持并行化等优化，以周期精确数据流模拟器为目标进行微架构分析。

Result: 通过四个有稀疏性的机器学习应用进行设计空间探索，发现全融合并非对稀疏模型总是最优，FuseFlow能识别并剪枝次优配置，在GPT - 3上实现约2.7倍加速。

Conclusion: FuseFlow编译器能有效处理稀疏机器学习模型，提高性能，且融合粒度依赖于模型本身。

Abstract: As deep learning models scale, sparse computation and specialized dataflow
hardware have emerged as powerful solutions to address efficiency. We propose
FuseFlow, a compiler that converts sparse machine learning models written in
PyTorch to fused sparse dataflow graphs for reconfigurable dataflow
architectures (RDAs). FuseFlow is the first compiler to support general
cross-expression fusion of sparse operations. In addition to fusion across
kernels (expressions), FuseFlow also supports optimizations like
parallelization, dataflow ordering, and sparsity blocking. It targets a
cycle-accurate dataflow simulator for microarchitectural analysis of fusion
strategies. We use FuseFlow for design-space exploration across four real-world
machine learning applications with sparsity, showing that full fusion (entire
cross-expression fusion across all computation in an end-to-end model) is not
always optimal for sparse models-fusion granularity depends on the model
itself. FuseFlow also provides a heuristic to identify and prune suboptimal
configurations. Using Fuseflow, we achieve performance improvements, including
a ~2.7x speedup over an unfused baseline for GPT-3 with BigBird block-sparse
attention.

</details>


### [40] [Causal Structure and Representation Learning with Biomedical Applications](https://arxiv.org/abs/2511.04790)
*Caroline Uhler,Jiaqi Zhang*

Main category: cs.LG

TL;DR: 大数据收集有潜力助力复杂现象理解与决策，表征学习在预测任务成功但因果任务欠佳，需结合因果推断，本文提出因果结构和表征学习框架。


<details>
  <summary>Details</summary>
Motivation: 表征学习在因果任务表现不佳，且多模态数据可用性增加，受生物医学基础问题驱动。

Method: 文中未明确提及具体方法，仅提出统计和计算框架。

Result: 未提及具体结果。

Conclusion: 需将表征学习与因果推断结合，并提出因果结构和表征学习框架以解决相关生物医学问题。

Abstract: Massive data collection holds the promise of a better understanding of
complex phenomena and, ultimately, better decisions. Representation learning
has become a key driver of deep learning applications, as it allows learning
latent spaces that capture important properties of the data without requiring
any supervised annotations. Although representation learning has been hugely
successful in predictive tasks, it can fail miserably in causal tasks including
predicting the effect of a perturbation/intervention. This calls for a marriage
between representation learning and causal inference. An exciting opportunity
in this regard stems from the growing availability of multi-modal data
(observational and perturbational, imaging-based and sequencing-based, at the
single-cell level, tissue-level, and organism-level). We outline a statistical
and computational framework for causal structure and representation learning
motivated by fundamental biomedical questions: how to effectively use
observational and perturbational data to perform causal discovery on observed
causal variables; how to use multi-modal views of the system to learn causal
variables; and how to design optimal perturbations.

</details>


### [41] [SLOFetch: Compressed-Hierarchical Instruction Prefetching for Cloud Microservices](https://arxiv.org/abs/2511.04774)
*Liu Jiang,Zerui Bao,Shiqi Sheng,Di Zhu*

Main category: cs.LG

TL;DR: 针对云工作负载重新审视指令预取，提出新设计，在数据中心应用中提升网络服务效率。


<details>
  <summary>Details</summary>
Motivation: 大规模网络服务因软件栈和微服务编排导致指令足迹增加、前端停顿，使尾延迟和能耗上升。

Method: 基于EIP，引入压缩条目和分层元数据存储方案，添加轻量级在线机器学习控制器。

Result: 在数据中心应用中保持类似EIP的加速效果，减少片上状态。

Conclusion: 新方法提升了机器学习时代网络服务的效率。

Abstract: Large-scale networked services rely on deep soft-ware stacks and microservice
orchestration, which increase instruction footprints and create frontend stalls
that inflate tail latency and energy. We revisit instruction prefetching for
these cloud workloads and present a design that aligns with SLO driven and self
optimizing systems. Building on the Entangling Instruction Prefetcher (EIP), we
introduce a Compressed Entry that captures up to eight destinations around a
base using 36 bits by exploiting spatial clustering, and a Hierarchical
Metadata Storage scheme that keeps only L1 resident and frequently queried
entries on chip while virtualizing bulk metadata into lower levels. We further
add a lightweight Online ML Controller that scores prefetch profitability using
context features and a bandit adjusted threshold. On data center applications,
our approach preserves EIP like speedups with smaller on chip state and
improves efficiency for networked services in the ML era.

</details>


### [42] [Conditional Neural ODE for Longitudinal Parkinson's Disease Progression Forecasting](https://arxiv.org/abs/2511.04789)
*Xiaoda Wang,Yuji Zhao,Kaiqiao Han,Xiao Luo,Sanne van Rooij,Jennifer Stevens,Lifang He,Liang Zhan,Yizhou Sun,Wei Wang,Carl Yang*

Main category: cs.LG

TL;DR: 提出CNODE框架用于连续、个性化帕金森病进展预测，在PPMI数据集验证效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理帕金森病队列中不规则和稀疏的MRI数据，且难以捕捉个体异质性，需要新方法进行连续、个性化的疾病进展预测。

Method: 提出CNODE框架，用神经ODE模型将大脑形态变化建模为连续时间过程，联合学习患者特定的初始时间和进展速度以对齐个体轨迹。

Result: 在PPMI数据集上实验，该方法在预测帕金森病纵向进展方面优于现有基线方法。

Conclusion: CNODE框架在帕金森病进展预测上有良好表现，可用于相关研究和应用。

Abstract: Parkinson's disease (PD) shows heterogeneous, evolving brain-morphometry
patterns. Modeling these longitudinal trajectories enables mechanistic insight,
treatment development, and individualized 'digital-twin' forecasting. However,
existing methods usually adopt recurrent neural networks and transformer
architectures, which rely on discrete, regularly sampled data while struggling
to handle irregular and sparse magnetic resonance imaging (MRI) in PD cohorts.
Moreover, these methods have difficulty capturing individual heterogeneity
including variations in disease onset, progression rate, and symptom severity,
which is a hallmark of PD. To address these challenges, we propose CNODE
(Conditional Neural ODE), a novel framework for continuous, individualized PD
progression forecasting. The core of CNODE is to model morphological brain
changes as continuous temporal processes using a neural ODE model. In addition,
we jointly learn patient-specific initial time and progress speed to align
individual trajectories into a shared progression trajectory. We validate CNODE
on the Parkinson's Progression Markers Initiative (PPMI) dataset. Experimental
results show that our method outperforms state-of-the-art baselines in
forecasting longitudinal PD progression.

</details>


### [43] [Efficient Swap Multicalibration of Elicitable Properties](https://arxiv.org/abs/2511.04907)
*Lunjia Hu,Haipeng Luo,Spandan Senapati,Vatsal Sharan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Multicalibration [HJKRR18] is an algorithmic fairness perspective that
demands that the predictions of a predictor are correct conditional on
themselves and membership in a collection of potentially overlapping subgroups
of a population. The work of [NR23] established a surprising connection between
multicalibration for an arbitrary property $\Gamma$ (e.g., mean or median) and
property elicitation: a property $\Gamma$ can be multicalibrated if and only if
it is elicitable, where elicitability is the notion that the true property
value of a distribution can be obtained by solving a regression problem over
the distribution. In the online setting, [NR23] proposed an inefficient
algorithm that achieves $\sqrt T$ $\ell_2$-multicalibration error for a
hypothesis class of group membership functions and an elicitable property
$\Gamma$, after $T$ rounds of interaction between a forecaster and adversary.
  In this paper, we generalize multicalibration for an elicitable property
$\Gamma$ from group membership functions to arbitrary bounded hypothesis
classes and introduce a stronger notion -- swap multicalibration, following
[GKR23]. Subsequently, we propose an oracle-efficient algorithm which, when
given access to an online agnostic learner, achieves $T^{1/(r+1)}$
$\ell_r$-swap multicalibration error with high probability (for $r\ge2$) for a
hypothesis class with bounded sequential Rademacher complexity and an
elicitable property $\Gamma$. For the special case of $r=2$, this implies an
oracle-efficient algorithm that achieves $T^{1/3}$ $\ell_2$-swap
multicalibration error, which significantly improves on the previously
established bounds for the problem [NR23, GMS25, LSS25a], and completely
resolves an open question raised in [GJRR24] on the possibility of an
oracle-efficient algorithm that achieves $\sqrt{T}$ $\ell_2$-mean
multicalibration error by answering it in a strongly affirmative sense.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [44] [NeuroFlex: Column-Exact ANN-SNN Co-Execution Accelerator with Cost-Guided Scheduling](https://arxiv.org/abs/2511.05215)
*Varun Manjunath,Pranav Ramesh,Gopalakrishnan Srinivasan*

Main category: cs.NE

TL;DR: NeuroFlex是一种列级加速器，可共同执行人工和脉冲神经网络，在稀疏边缘工作负载上实现低能耗延迟积和高准确性。


<details>
  <summary>Details</summary>
Motivation: 在稀疏边缘工作负载上最小化能量延迟积并保证竞争精度。

Method: 将整数精确的QCFS ANN - SNN转换从层扩展到独立列，统一INT8存储与实时尖峰生成，使用离线成本模型分配列到ANN或SNN核心并跨处理元素打包工作。

Result: 成本导向调度算法比随机映射提高吞吐量16 - 19%，比仅ANN基线降低EDP 57 - 67%，比LoAS加速达2.5倍，比SparTen节能2.51倍。

Conclusion: 细粒度和整数精确的混合设计在不牺牲精度的情况下，在能耗和延迟方面优于单模式设计。

Abstract: NeuroFlex is a column-level accelerator that co-executes artificial and
spiking neural networks to minimize energy-delay product on sparse edge
workloads with competitive accuracy. The design extends integer-exact QCFS
ANN-SNN conversion from layers to independent columns. It unifies INT8 storage
with on-the-fly spike generation using an offline cost model to assign columns
to ANN or SNN cores and pack work across processing elements with deterministic
runtime. Our cost-guided scheduling algorithm improves throughput by 16-19%
over random mapping and lowers EDP by 57-67% versus a strong ANN-only baseline
across VGG-16, ResNet-34, GoogLeNet, and BERT models. NeuroFlex also delivers
up to 2.5x speedup over LoAS and 2.51x energy reduction over SparTen. These
results indicate that fine-grained and integer-exact hybridization outperforms
single-mode designs on energy and latency without sacrificing accuracy.

</details>


### [45] [FPGA-Based Real-Time Waveform Classification](https://arxiv.org/abs/2511.05479)
*Alperen Aksoy,Ilja Bekman,Chimezie Eguzo,Christian Grewing,Andre Zambanini*

Main category: cs.NE

TL;DR: 研究基于查找表的神经网络用于SiPM和信号自触发读出，可早期提取信息并减少数据传输量，且能实现低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 实现SiPM和信号自触发读出，早期可靠提取粒子击中信息并减少传输数据量。

Method: 考虑基于查找表的神经网络，用遗传算法训练。

Result: 这些神经网络结构能通过遗传算法训练，实现与无死区时间在线处理兼容的推理延迟。

Conclusion: 基于查找表的神经网络可用于SiPM和信号自触发读出，在数据处理方面有良好表现。

Abstract: For self-triggered readout of SiPM sum signals, a waveform classification can
aid a simple threshold trigger to reliably extract calorimetric particle hit
information online at an early stage and thus reduce the volume of transmitted
data. Typically, the ADC data acquisition is based on FPGAs for edge data
processing. In this study, we consider look-up-table-based neural-networks and
address challenges of binary multi-layer neural networks' layout, footprint,
performance and training. We show that these structures can be trained using a
genetic algorithm and achieve the inference latency compatible with dead-time
free processing online.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [46] [Agentic Refactoring: An Empirical Study of AI Coding Agents](https://arxiv.org/abs/2511.04824)
*Kosei Horikawa,Hao Li,Yutaro Kashiwa,Bram Adams,Hajimu Iida,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文对真实开源Java项目中AI代理生成的重构进行大规模研究，分析15451个重构实例，发现代理重构常见且有针对性，多为低层次一致性编辑，注重内部质量，能提升代码结构指标。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对代理重构在实践中如何使用、与人类重构对比及对代码质量影响的实证理解，为填补这一空白开展研究。

Method: 对真实开源Java项目中AI代理生成的重构进行大规模研究，分析来自AIDev数据集的15451个重构实例，涉及12256个拉取请求和14988个提交。

Result: 重构是常见且有意的活动，代理在26.1%的提交中明确针对重构；代理重构多为低层次一致性编辑；动机主要关注内部质量；能在结构指标上有小但显著的提升。

Conclusion: 代理重构在实际开发中较为常见，偏好低层次局部改进，注重内部质量，对代码结构指标有积极影响。

Abstract: Agentic coding tools, such as OpenAI Codex, Claude Code, and Cursor, are
transforming the software engineering landscape. These AI-powered systems
function as autonomous teammates capable of planning and executing complex
development tasks. Agents have become active participants in refactoring, a
cornerstone of sustainable software development aimed at improving internal
code quality without altering observable behavior. Despite their increasing
adoption, there is a critical lack of empirical understanding regarding how
agentic refactoring is utilized in practice, how it compares to human-driven
refactoring, and what impact it has on code quality. To address this empirical
gap, we present a large-scale study of AI agent-generated refactorings in
real-world open-source Java projects, analyzing 15,451 refactoring instances
across 12,256 pull requests and 14,988 commits derived from the AIDev dataset.
Our empirical analysis shows that refactoring is a common and intentional
activity in this development paradigm, with agents explicitly targeting
refactoring in 26.1% of commits. Analysis of refactoring types reveals that
agentic efforts are dominated by low-level, consistency-oriented edits, such as
Change Variable Type (11.8%), Rename Parameter (10.4%), and Rename Variable
(8.5%), reflecting a preference for localized improvements over the high-level
design changes common in human refactoring. Additionally, the motivations
behind agentic refactoring focus overwhelmingly on internal quality concerns,
with maintainability (52.5%) and readability (28.1%). Furthermore, quantitative
evaluation of code quality metrics shows that agentic refactoring yields small
but statistically significant improvements in structural metrics, particularly
for medium-level changes, reducing class size and complexity (e.g., Class LOC
median $\Delta$ = -15.25).

</details>


### [47] [Software Defined Vehicle Code Generation: A Few-Shot Prompting Approach](https://arxiv.org/abs/2511.04849)
*Quang-Dung Nguyen,Tri-Dung Tran,Thanh-Hieu Chu,Hoang-Loc Tran,Xiangwei Cheng,Dirk Slama*

Main category: cs.SE

TL;DR: 研究探讨用提示策略使大语言模型适应软件定义汽车（SDV）代码生成任务，经实验发现少样本提示策略表现更优。


<details>
  <summary>Details</summary>
Motivation: SDV 应用开发需先进工具提升效率，通用大语言模型因专有架构访问受限难用于 SDV 代码生成。

Method: 采用提示策略，利用高级提示工程技术设计合适高效的系统提示结构，对不同模型应用多种提示技术进行实验。

Result: 基于定量指标，少样本提示策略的模型在调整大语言模型答案以匹配预期结果方面表现优于其他模型。

Conclusion: 使用提示策略可使大语言模型适应 SDV 代码生成任务，少样本提示策略效果更好。

Abstract: The emergence of Software-Defined Vehicles (SDVs) marks a paradigm shift in
the automotive industry, where software now plays a pivotal role in defining
vehicle functionality, enabling rapid innovation of modern vehicles. Developing
SDV-specific applications demands advanced tools to streamline code generation
and improve development efficiency. In recent years, general-purpose large
language models (LLMs) have demonstrated transformative potential across
domains. Still, restricted access to proprietary model architectures hinders
their adaption to specific tasks like SDV code generation. In this study, we
propose using prompts, a common and basic strategy to interact with LLMs and
redirect their responses. Using only system prompts with an appropriate and
efficient prompt structure designed using advanced prompt engineering
techniques, LLMs can be crafted without requiring a training session or access
to their base design. This research investigates the extensive experiments on
different models by applying various prompting techniques, including bare
models, using a benchmark specifically created to evaluate LLMs' performance in
generating SDV code. The results reveal that the model with a few-shot
prompting strategy outperforms the others in adjusting the LLM answers to match
the expected outcomes based on quantitative metrics.

</details>


### [48] [What About Our Bug? A Study on the Responsiveness of NPM Package Maintainers](https://arxiv.org/abs/2511.04986)
*Mohammadreza Saeidi,Ethan Thoma,Raula Gaikovina Kula,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: 研究npm包维护者对bug报告的响应情况，发现维护者总体响应积极，还给出部分bug未解决的原因分类。


<details>
  <summary>Details</summary>
Motivation: 第三方库广泛使用使npm生态系统对软件开发至关重要，但依赖链会传播bug，推测维护者可能不修复某些bug，需验证此假设。

Method: 采用混合方法挖掘仓库问题数据，并进行定性开放编码分析未处理bug报告的原因。

Result: 维护者总体响应积极，项目层面响应中位数为70%（IQR：55%-89%）。

Conclusion: 给出部分bug未解决的原因分类，包括贡献实践、依赖约束和库特定标准等，了解维护者行为有助于促进更强大和响应迅速的开源生态系统。

Abstract: Background: Widespread use of third-party libraries makes ecosystems like
Node Package Manager (npm) critical to modern software development. However,
this interconnected chain of dependencies also creates challenges: bugs in one
library can propagate downstream, potentially impacting many other libraries
that rely on it. We hypothesize that maintainers may not always decide to fix a
bug, especially if the maintainer decides it falls out of their responsibility
within the chain of dependencies. Aims: To confirm this hypothesis, we
investigate the responsiveness of 30,340 bug reports across 500 of the most
depended-upon npm packages. Method: We adopt a mixed-method approach to mine
repository issue data and perform qualitative open coding to analyze reasons
behind unaddressed bug reports. Results: Our findings show that maintainers are
generally responsive, with a median project-level responsiveness of 70% (IQR:
55%-89%), reflecting their commitment to support downstream developers.
Conclusions: We present a taxonomy of the reasons some bugs remain unresolved.
The taxonomy includes contribution practices, dependency constraints, and
library-specific standards as reasons for not being responsive. Understanding
maintainer behavior can inform practices that promote a more robust and
responsive open-source ecosystem that benefits the entire community.

</details>


### [49] [Generating Software Architecture Description from Source Code using Reverse Engineering and Large Language Model](https://arxiv.org/abs/2511.05165)
*Ahmad Hatahet,Christoph Knieke,Andreas Rausch*

Main category: cs.SE

TL;DR: 提出从源代码半自动化生成软件架构描述（SADs），结合逆向工程和大语言模型，减少人工并增强系统可维护性。


<details>
  <summary>Details</summary>
Motivation: 实际中SADs常缺失、过时或与实现不符，开发者从代码获取架构信息耗时且增加认知负担。

Method: 结合逆向工程技术和大语言模型，提取组件图、过滤关键元素、生成状态机图。

Result: 生成的视图可替代传统手动架构文档，大语言模型能抽象组件图、准确表示复杂行为。

Conclusion: 该方法可显著减少人工，增强系统理解和长期可维护性。

Abstract: Software Architecture Descriptions (SADs) are essential for managing the
inherent complexity of modern software systems. They enable high-level
architectural reasoning, guide design decisions, and facilitate effective
communication among diverse stakeholders. However, in practice, SADs are often
missing, outdated, or poorly aligned with the system's actual implementation.
Consequently, developers are compelled to derive architectural insights
directly from source code-a time-intensive process that increases cognitive
load, slows new developer onboarding, and contributes to the gradual
degradation of clarity over the system's lifetime. To address these issues, we
propose a semi-automated generation of SADs from source code by integrating
reverse engineering (RE) techniques with a Large Language Model (LLM). Our
approach recovers both static and behavioral architectural views by extracting
a comprehensive component diagram, filtering architecturally significant
elements (core components) via prompt engineering, and generating state machine
diagrams to model component behavior based on underlying code logic with
few-shots prompting. This resulting views representation offer a scalable and
maintainable alternative to traditional manual architectural documentation.
This methodology, demonstrated using C++ examples, highlights the potent
capability of LLMs to: 1) abstract the component diagram, thereby reducing the
reliance on human expert involvement, and 2) accurately represent complex
software behaviors, especially when enriched with domain-specific knowledge
through few-shot prompting. These findings suggest a viable path toward
significantly reducing manual effort while enhancing system understanding and
long-term maintainability.

</details>


### [50] [CodeMapper: A Language-Agnostic Approach to Mapping Code Regions Across Commits](https://arxiv.org/abs/2511.05205)
*Huimin Hu,Michael Pradel*

Main category: cs.SE

TL;DR: 本文介绍了CodeMapper方法解决代码映射问题，该方法独立于特定程序元素和编程语言，评估显示其表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有技术如git diff不能有效解决代码映射问题，其他技术又局限于特定代码元素和编程语言，因此需要新方法。

Method: CodeMapper方法包含两个阶段：一是通过分析差异、检测代码移动和搜索特定代码片段计算候选区域；二是通过计算相似度选择最可能的目标区域。

Result: 将CodeMapper应用于四个数据集，在71.0% - 94.5%的情况下能正确识别目标区域，比最佳基线提高1.5 - 58.8个绝对百分点。

Conclusion: CodeMapper能有效解决代码映射问题，且表现优于现有方法。

Abstract: During software evolution, developers commonly face the problem of mapping a
specific code region from one commit to another. For example, they may want to
determine how the condition of an if-statement, a specific line in a
configuration file, or the definition of a function changes. We call this the
code mapping problem. Existing techniques, such as git diff, address this
problem only insufficiently because they show all changes made to a file
instead of focusing on a code region of the developer's choice. Other
techniques focus on specific code elements and programming languages (e.g.,
methods in Java), limiting their applicability. This paper introduces
CodeMapper, an approach to address the code mapping problem in a way that is
independent of specific program elements and programming languages. Given a
code region in one commit, CodeMapper finds the corresponding region in another
commit. The approach consists of two phases: (i) computing candidate regions by
analyzing diffs, detecting code movements, and searching for specific code
fragments, and (ii) selecting the most likely target region by calculating
similarities. Our evaluation applies CodeMapper to four datasets, including two
new hand-annotated datasets containing code region pairs in ten popular
programming languages. CodeMapper correctly identifies the expected target
region in 71.0%--94.5% of all cases, improving over the best available
baselines by 1.5--58.8 absolute percent points.

</details>


### [51] [Building Specialized Software-Assistant ChatBot with Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2511.05297)
*Mohammed Hilel,Yannis Karmim,Jean De Bodinat,Reda Sarehane,Antoine Gillon*

Main category: cs.SE

TL;DR: 介绍基于图的检索增强生成框架，自动将企业Web应用转换为状态 - 动作知识图，助力LLMs生成可靠辅助，并讨论相关工程流程及工业用例经验。


<details>
  <summary>Details</summary>
Motivation: 现有数字采用平台构建维护需大量人力，LLMs缺乏目标软件结构化理解会产生不可靠答案，且生产级LLMs难以微调。

Method: 引入基于图的检索增强生成框架，自动将企业Web应用转换为状态 - 动作知识图，详细阐述工程流程、图检索设计及集成到生产DAP工作流。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及具体结论，但讨论了可扩展性、鲁棒性和部署经验。

Abstract: Digital Adoption Platforms (DAPs) have become essential tools for helping
employees navigate complex enterprise software such as CRM, ERP, or HRMS
systems. Companies like LemonLearning have shown how digital guidance can
reduce training costs and accelerate onboarding. However, building and
maintaining these interactive guides still requires extensive manual effort.
Leveraging Large Language Models as virtual assistants is an appealing
alternative, yet without a structured understanding of the target software,
LLMs often hallucinate and produce unreliable answers. Moreover, most
production-grade LLMs are black-box APIs, making fine-tuning impractical due to
the lack of access to model weights. In this work, we introduce a Graph-based
Retrieval-Augmented Generation framework that automatically converts enterprise
web applications into state-action knowledge graphs, enabling LLMs to generate
grounded and context-aware assistance. The framework was co-developed with the
AI enterprise RAKAM, in collaboration with Lemon Learning. We detail the
engineering pipeline that extracts and structures software interfaces, the
design of the graph-based retrieval process, and the integration of our
approach into production DAP workflows. Finally, we discuss scalability,
robustness, and deployment lessons learned from industrial use cases.

</details>


### [52] [Code Review Automation using Retrieval Augmented Generation](https://arxiv.org/abs/2511.05302)
*Qianru Meng,Xiao Zhang,Zhaochen Ren,Joost Visser*

Main category: cs.SE

TL;DR: 提出Retrieval - Augmented Reviewer (RARe)结合检索和生成方法进行代码审查，在两个基准数据集上表现优于现有方法，经评估验证其实用性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有自动代码审查生成方法存在生成的审查偏离重点或过于笼统的问题，需要改进。

Method: 引入RARe，利用Retrieval - Augmented Generation (RAG)结合检索和生成方法，用密集检索器从代码库中选择相关审查，丰富神经生成器输入以生成最终审查。

Result: 在两个基准数据集上超越现有方法，BLEU - 4分数分别为12.32和12.96，通过人工评估和案例研究验证有效性。

Conclusion: RARe具有实用价值和可靠性，能有效解决现有自动代码审查生成方法的局限。

Abstract: Code review is essential for maintaining software quality but is
labor-intensive. Automated code review generation offers a promising solution
to this challenge. Both deep learning-based generative techniques and
retrieval-based methods have demonstrated strong performance in this task.
However, despite these advancements, there are still some limitations where
generated reviews can be either off-point or overly general. To address these
issues, we introduce Retrieval-Augmented Reviewer (RARe), which leverages
Retrieval-Augmented Generation (RAG) to combine retrieval-based and generative
methods, explicitly incorporating external domain knowledge into the code
review process. RARe uses a dense retriever to select the most relevant reviews
from the codebase, which then enrich the input for a neural generator,
utilizing the contextual learning capacity of large language models (LLMs), to
produce the final review. RARe outperforms state-of-the-art methods on two
benchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96, respectively.
Its effectiveness is further validated through a detailed human evaluation and
a case study using an interpretability tool, demonstrating its practical
utility and reliability.

</details>


### [53] [SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for Large Language Models](https://arxiv.org/abs/2511.05459)
*Jingxuan Xu,Ken Deng,Weihao Li,Songwei Yu,Huaixi Tang,Haoyang Huang,Zhiyi Lai,Zizheng Zhan,Yanan Wu,Chenchen Zhang,Kepeng Lei,Yifan Yao,Xinping Lei,Wenqiang Zhu,Zongxian Feng,Han Li,Junqi Xiong,Dailin Li,Zuchen Gao,Kun Wu,Wen Xiang,Ziqi Zhan,Yuanxing Zhang,Wuxuan Gong,Ziyuan Gao,Guanxiang Wang,Yirong Xue,Xiaojiang Zhang,Jinghui Wang,Huiming Wang,Wenhao Zhuang,Zhaoxiang Zhang,Yuqun Zhang,Haotian Zhang,Bin Chen,Jiaheng Liu*

Main category: cs.SE

TL;DR: 现有大语言模型软件工程评估有局限，本文提出SWE - Compass基准，对10个模型进行测试，为评估模型编码能力提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有评估大语言模型在软件工程方面的基准存在任务覆盖窄、语言有偏差和与实际开发者工作流程不匹配等问题。

Method: 引入SWE - Compass基准，涵盖8种任务类型、8种编程场景和10种编程语言，从真实GitHub拉取请求中选取2000个实例，在SWE - Agent和Claude Code两个框架下对10个模型进行测试。

Result: 揭示了不同任务类型、语言和场景的难度层次。

Conclusion: SWE - Compass为诊断和提升大语言模型的代理编码能力提供了严格且可重复的基础。

Abstract: Evaluating large language models (LLMs) for software engineering has been
limited by narrow task coverage, language bias, and insufficient alignment with
real-world developer workflows. Existing benchmarks often focus on algorithmic
problems or Python-centric bug fixing, leaving critical dimensions of software
engineering underexplored. To address these gaps, we introduce SWE-Compass1, a
comprehensive benchmark that unifies heterogeneous code-related evaluations
into a structured and production-aligned framework. SWE-Compass spans 8 task
types, 8 programming scenarios, and 10 programming languages, with 2000
high-quality instances curated from authentic GitHub pull requests and refined
through systematic filtering and validation. We benchmark ten state-of-the-art
LLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear
hierarchy of difficulty across task types, languages, and scenarios. Moreover,
by aligning evaluation with real-world developer practices, SWE-Compass
provides a rigorous and reproducible foundation for diagnosing and advancing
agentic coding capabilities in large language models.

</details>


### [54] [A Metamorphic Testing Perspective on Knowledge Distillation for Language Models of Code: Does the Student Deeply Mimic the Teacher?](https://arxiv.org/abs/2511.05476)
*Md. Abdul Awal,Mrigank Rochan,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 当前代码大模型部署受限，知识蒸馏可压缩模型，但学生模型对教师模型行为模仿深度缺乏研究。本文提出MetaCompress框架评估行为保真度，实验表明其能识别学生模型中高达62%的行为差异。


<details>
  <summary>Details</summary>
Motivation: 解决代码大模型部署时计算成本高、推理慢和环境影响大的问题，同时弥补现有基于准确率评估无法深入探究学生模型对教师模型行为模仿程度的不足。

Method: 提出MetaCompress框架，通过在一组保行为的变形关系下比较教师和学生模型的输出来系统评估行为保真度，并在两个广泛研究的任务上使用三种知识蒸馏技术得到的压缩模型进行评估。

Result: MetaCompress能识别学生模型中高达62%的行为差异，且发现学生模型在对抗攻击下性能下降幅度比传统评估显示的更大。

Conclusion: 知识蒸馏流程中需要进行行为保真度评估，MetaCompress是测试通过知识蒸馏得到的压缩代码语言模型的实用框架。

Abstract: Transformer-based language models of code have achieved state-of-the-art
performance across a wide range of software analytics tasks, but their
practical deployment remains limited due to high computational costs, slow
inference speeds, and significant environmental impact. To address these
challenges, recent research has increasingly explored knowledge distillation as
a method for compressing a large language model of code (the teacher) into a
smaller model (the student) while maintaining performance. However, the degree
to which a student model deeply mimics the predictive behavior and internal
representations of its teacher remains largely unexplored, as current
accuracy-based evaluation provides only a surface-level view of model quality
and often fails to capture more profound discrepancies in behavioral fidelity
between the teacher and student models. To address this gap, we empirically
show that the student model often fails to deeply mimic the teacher model,
resulting in up to 285% greater performance drop under adversarial attacks,
which is not captured by traditional accuracy-based evaluation. Therefore, we
propose MetaCompress, a metamorphic testing framework that systematically
evaluates behavioral fidelity by comparing the outputs of teacher and student
models under a set of behavior-preserving metamorphic relations. We evaluate
MetaCompress on two widely studied tasks, using compressed versions of popular
language models of code, obtained via three different knowledge distillation
techniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress
identifies up to 62% behavioral discrepancies in student models, underscoring
the need for behavioral fidelity evaluation within the knowledge distillation
pipeline and establishing MetaCompress as a practical framework for testing
compressed language models of code derived through knowledge distillation.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [55] [Economic uncertainty and exchange rates linkage revisited: modelling tail dependence with high frequency data](https://arxiv.org/abs/2511.05315)
*Nourhaine Nefzi,Abir Abid*

Main category: q-fin.CP

TL;DR: 本文利用每日推特不确定性指数和金砖国家汇率，在时变copula框架下研究汇率与不确定性的极端尾部依赖，得出部分货币依赖结构特点及巴西和中国货币的避风港作用。


<details>
  <summary>Details</summary>
Motivation: 深入理解汇率与不确定性之间的依赖关系。

Method: 使用Baker等人（2020）的每日推特不确定性指数和金砖国家汇率，在原创的时变copula框架下进行研究。

Result: 印度、俄罗斯和南非货币依赖结构无不对称特征和极端变动；巴西和中国货币尾部依赖呈上升趋势，在高全球经济不确定性时有避风港作用。

Conclusion: 在高全球经济不确定性时，巴西和中国市场可通过投资组合多样化带来显著收益机会。

Abstract: The aim of this paper is to dig deeper into understanding the exchange rates
and uncertainty dependence. Using the novel Baker et al. (2020)'s daily Twitter
Uncertainty Index and BRICS exchange rates, we investigate their extreme tail
dependence within an original time-varying copula framework. Our analysis makes
several noteworthy results. Evidence for Indian, Russian and South African
currencies indicates an elliptical copulas' dominance implying neither
asymmetric features nor extreme movements in their dependence structure with
the global economic uncertainty. Importantly, Brazilian and Chinese currencies
tail dependence is upward trending suggesting a safe-haven role in times of
high global economic uncertainty including the recent COVID-19 pandemic. In
such circumstances, these markets offer opportunities to significant gains
through portfolio diversification.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [56] [The Shape of Markets: Machine learning modeling and Prediction Using 2-Manifold Geometries](https://arxiv.org/abs/2511.05030)
*Panagiotis Papaioannou,Athanassios N. Yannacopoulos*

Main category: q-fin.ST

TL;DR: 引入几何信息模型用于金融预测，将高维市场数据嵌入常曲率二维流形，发现环面表现最佳，证明微分几何与数据驱动推理结合对金融建模有价值。


<details>
  <summary>Details</summary>
Motivation: 为金融预测引入新的模型，利用微分几何改进金融建模。

Method: 将高维市场数据嵌入球面S2、欧几里得平面R2、双曲平面H2和环面T等二维流形，用流形学习技术从金融数据中推断潜在曲率。

Result: 环面是表现最佳的几何形状。

Conclusion: 微分几何与数据驱动推理结合对金融建模有价值。

Abstract: We introduce a Geometry Informed Model for financial forecasting by embedding
high dimensional market data onto constant curvature 2manifolds. Guided by the
uniformization theorem, we model market dynamics as Brownian motion on
spherical S2, Euclidean R2, and hyperbolic H2 geometries. We further include
the torus T, a compact, flat manifold admissible as a quotient space of the
Euclidean plane anticipating its relevance for capturing cyclical dynamics.
Manifold learning techniques infer the latent curvature from financial data,
revealing the torus as the best performing geometry. We interpret this result
through a macroeconomic lens, the torus circular dimensions align with
endogenous cycles in output, interest rates, and inflation described by IS LM
theory. Our findings demonstrate the value of integrating differential geometry
with data-driven inference for financial modeling.

</details>


### [57] [From sectorial coarse graining to extreme coarse graining of S&P 500 correlation matrices](https://arxiv.org/abs/2511.05463)
*Manan Vyas,M. Mijaíl Martínez-Ramos,Parisa Majari,Thomas H. Seligman*

Main category: q-fin.ST

TL;DR: 从股票收益的皮尔逊相关矩阵出发，将部门矩阵理念简化为2x2对称矩阵，通过分块平均保留平均相关性，用随机和特定选择分块，结果显示一种非随机选择有不同特性待经济分析。


<details>
  <summary>Details</summary>
Motivation: 从股票收益的皮尔逊相关矩阵出发，希望减少与金融市场动态相关的参数数量。

Method: 将部门矩阵简化为2x2实对称矩阵，通过对选择的两组股票子集行和列构成的块进行平均，保留相关矩阵的平均值，采用随机选择相同块大小和两种特定非等块大小选择。

Result: 一种非随机选择有不同特性。

Conclusion: 该非随机选择特性的意义需从经济学角度分析。

Abstract: Starting from the Pearson Correlation Matrix of stock returns and from the
desire to obtain a reduced number of parameters relevant for the dynamics of a
financial market, we propose to take the idea of a sectorial matrix, which
would have a large number of parameters, to the reduced picture of a real
symmetric $2 \times 2$ matrix, extreme case, that still conserves the desirable
feature that the average correlation can be one of the parameters. This is
achieved by averaging the correlation matrix over blocks created by choosing
two subsets of stocks for rows and columns and averaging over each of the
resulting blocks. Averaging over these blocks, we retain the average of the
correlation matrix. We shall use a random selection for two equal block sizes
as well as two specific, hopefully relevant, ones that do not produce equal
block sizes. The results show that one of the non-random choices has somewhat
different properties, whose meaning will have to be analyzed from an economy
point of view.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [58] [Prototype Selection Using Topological Data Analysis](https://arxiv.org/abs/2511.04873)
*Jordan Eckert,Elvan Ceyhan,Henry Schenck*

Main category: stat.ML

TL;DR: 提出基于拓扑数据分析的框架TPS选择数据集原型，在模拟和真实数据中验证其有效性，推动原型学习发展。


<details>
  <summary>Details</summary>
Motivation: 利用拓扑原理表示数据以捕捉结构和关系，解决从大型数据集中选择代表性子集的问题。

Method: 提出拓扑原型选择器（TPS）框架。

Result: 在模拟和真实数据中，TPS显著保留或提高分类性能，同时大幅减小数据规模。

Conclusion: 这些贡献推动了原型学习的算法和几何方面发展，为分类提供实用工具。

Abstract: Recently, there has been an explosion in statistical learning literature to
represent data using topological principles to capture structure and
relationships. We propose a topological data analysis (TDA)-based framework,
named Topological Prototype Selector (TPS), for selecting representative
subsets (prototypes) from large datasets. We demonstrate the effectiveness of
TPS on simulated data under different data intrinsic characteristics, and
compare TPS against other currently used prototype selection methods in real
data settings. In all simulated and real data settings, TPS significantly
preserves or improves classification performance while substantially reducing
data size. These contributions advance both algorithmic and geometric aspects
of prototype learning and offer practical tools for parallelized,
interpretable, and efficient classification.

</details>


### [59] [Estimating Bidirectional Causal Effects with Large Scale Online Kernel Learning](https://arxiv.org/abs/2511.05050)
*Masahiro Tanaka*

Main category: stat.ML

TL;DR: 提出可扩展在线核学习框架估计双向因果效应，经模拟验证有效，适用于多领域。


<details>
  <summary>Details</summary>
Motivation: 传统因果推断常关注单向效应，忽略现实中常见的双向关系。

Method: 基于异方差识别，将联立方程模型的准极大似然估计与大规模在线核学习结合，用随机傅里叶特征近似建模，自适应在线梯度下降算法保证效率。

Result: 模拟显示该方法比单方程和多项式近似基线更准确稳定，偏差和均方根误差更低，能以近线性计算规模捕捉复杂双向因果效应。

Conclusion: 该框架结合计量识别与机器学习技术，为多领域大规模因果推断提供实用、可扩展且理论可靠的解决方案。

Abstract: In this study, a scalable online kernel learning framework is proposed for
estimating bidirectional causal effects in systems characterized by mutual
dependence and heteroskedasticity. Traditional causal inference often focuses
on unidirectional effects, overlooking the common bidirectional relationships
in real-world phenomena. Building on heteroskedasticity-based identification,
the proposed method integrates a quasi-maximum likelihood estimator for
simultaneous equation models with large scale online kernel learning. It
employs random Fourier feature approximations to flexibly model nonlinear
conditional means and variances, while an adaptive online gradient descent
algorithm ensures computational efficiency for streaming and high-dimensional
data. Results from extensive simulations demonstrate that the proposed method
achieves superior accuracy and stability than single equation and polynomial
approximation baselines, exhibiting lower bias and root mean squared error
across various data-generating processes. These results confirm that the
proposed approach effectively captures complex bidirectional causal effects
with near-linear computational scaling. By combining econometric identification
with modern machine learning techniques, the proposed framework offers a
practical, scalable, and theoretically grounded solution for large scale causal
inference in natural/social science, policy making, business, and industrial
applications.

</details>


### [60] [A New Framework for Convex Clustering in Kernel Spaces: Finite Sample Bounds, Consistency and Performance Insights](https://arxiv.org/abs/2511.05159)
*Shubhayan Pan,Saptarshi Chakraborty,Debolina Paul,Kushal Bose,Swagatam Das*

Main category: stat.ML

TL;DR: 提出凸聚类方法的核化扩展，处理非线性和非凸数据，理论证明收敛性与有限样本界，实验验证性能优。


<details>
  <summary>Details</summary>
Motivation: 传统凸聚类方法处理线性不可分或非凸结构数据时存在局限，需改进。

Method: 将数据点通过特征映射投影到再生核希尔伯特空间（RKHS），在变换空间进行凸聚类。

Result: 理论证明算法收敛性和有限样本界，实验表明在合成和真实数据集上性能优于现有聚类技术。

Conclusion: 该方法是该领域重大进展，为非线性和非凸数据聚类提供有效解决方案。

Abstract: Convex clustering is a well-regarded clustering method, resembling the
similar centroid-based approach of Lloyd's $k$-means, without requiring a
predefined cluster count. It starts with each data point as its centroid and
iteratively merges them. Despite its advantages, this method can fail when
dealing with data exhibiting linearly non-separable or non-convex structures.
To mitigate the limitations, we propose a kernelized extension of the convex
clustering method. This approach projects the data points into a Reproducing
Kernel Hilbert Space (RKHS) using a feature map, enabling convex clustering in
this transformed space. This kernelization not only allows for better handling
of complex data distributions but also produces an embedding in a
finite-dimensional vector space. We provide a comprehensive theoretical
underpinnings for our kernelized approach, proving algorithmic convergence and
establishing finite sample bounds for our estimates. The effectiveness of our
method is demonstrated through extensive experiments on both synthetic and
real-world datasets, showing superior performance compared to state-of-the-art
clustering techniques. This work marks a significant advancement in the field,
offering an effective solution for clustering in non-linear and non-convex data
scenarios.

</details>


### [61] [Self-adaptive weighting and sampling for physics-informed neural networks](https://arxiv.org/abs/2511.05452)
*Wenqian Chen,Amanda Howard,Panos Stinis*

Main category: stat.ML

TL;DR: 本文提出混合自适应采样和加权方法提升物理信息神经网络（PINNs）求解偏微分方程（PDEs）的性能，结合两种策略能提高预测精度和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有物理信息深度学习模型在复杂问题上训练存在精度和效率受限的问题。

Method: 引入混合自适应采样和加权方法，自适应采样识别解快速变化区域的训练点，自适应加权平衡训练点的收敛率。

Result: 仅使用自适应采样或加权在训练点不足时难以实现准确预测，两种方法效果因问题而异，结合二者能提升性能。

Conclusion: 所提框架为PINNs求解PDEs提供更稳健的方法，能持续提高预测精度和训练效率。

Abstract: Physics-informed deep learning has emerged as a promising framework for
solving partial differential equations (PDEs). Nevertheless, training these
models on complex problems remains challenging, often leading to limited
accuracy and efficiency. In this work, we introduce a hybrid adaptive sampling
and weighting method to enhance the performance of physics-informed neural
networks (PINNs). The adaptive sampling component identifies training points in
regions where the solution exhibits rapid variation, while the adaptive
weighting component balances the convergence rate across training points.
Numerical experiments show that applying only adaptive sampling or only
adaptive weighting is insufficient to consistently achieve accurate
predictions, particularly when training points are scarce. Since each method
emphasizes different aspects of the solution, their effectiveness is problem
dependent. By combining both strategies, the proposed framework consistently
improves prediction accuracy and training efficiency, offering a more robust
approach for solving PDEs with PINNs.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [62] [BayesChange: an R package for Bayesian Change Point Analysis](https://arxiv.org/abs/2511.04785)
*Luca Danese,Riccardo Corradin,Andrea Ongaro*

Main category: stat.CO

TL;DR: 介绍了基于C++构建的计算高效R包BayesChange，用于贝叶斯变点检测和聚类，展示理论、算法并通过示例说明用法。


<details>
  <summary>Details</summary>
Motivation: 现有R包中缺少BayesChange提供的方法，需要开发新的包。

Method: 核心函数用C++实现保证计算效率，通过R用户界面简化使用，包含两个集成C++后端函数的R包装器和S3方法。

Result: 成功开发BayesChange包，并可以通过合成示例展示其使用。

Conclusion: BayesChange包提供了独特的方法，且使用方便、计算高效。

Abstract: We introduce BayesChange, a computationally efficient R package, built on
C++, for Bayesian change point detection and clustering of observations sharing
common change points. While many R packages exist for change point analysis,
BayesChange offers methods not currently available elsewhere. The core
functions are implemented in C++ to ensures computational efficiency, while an
R user interface simplifies the package usage. The BayesChange package includes
two R wrappers that integrate the C++ backend functions, along with S3 methods
for summarizing the results. We present the theory beyond each method, the
algorithms for posterior simulation and we illustrate the package's usage
through synthetic examples.

</details>


### [63] [Do we Need Dozens of Methods for Real World Missing Value Imputation?](https://arxiv.org/abs/2511.04833)
*Krystyna Grzesiak,Christophe Muller,Julie Josse,Jeffrey Näf*

Main category: stat.CO

TL;DR: 本文针对数据缺失值问题，提出基于分布预测任务的系统基准测试方法，评估大量算法，考虑真实缺失场景和混合数据集，分析表明迭代插补算法更优。


<details>
  <summary>Details</summary>
Motivation: 现有缺失值插补方法比较研究存在关注算法有限、评估指标不适合衡量数据分布保留情况的问题，需要更好的基准测试方法。

Method: 将插补视为分布预测任务，采用插补分数的概念，评估大量算法，不仅考虑合成缺失机制，还考虑真实世界缺失场景，同时研究混合数据集。

Result: 分析结果压倒性地证实了迭代插补算法的优越性，尤其是R包mice中实现的方法。

Conclusion: 基于分布预测任务的系统基准测试方法有效，迭代插补算法在处理缺失值方面表现更优。

Abstract: Missing values pose a persistent challenge in modern data science.
Consequently, there is an ever-growing number of publications introducing new
imputation methods in various fields. While many studies compare imputation
approaches, they often focus on a limited subset of algorithms and evaluate
performance primarily through pointwise metrics such as RMSE, which are not
suitable to measure the preservation of the true data distribution. In this
work, we provide a systematic benchmarking method based on the idea of treating
imputation as a distributional prediction task. We consider a large number of
algorithms and, for the first time, evaluate them not only on synthetic missing
mechanisms, but also on real-world missingness scenarios, using the concept of
Imputation Scores. Finally, while the focus of previous benchmark has often
been on numerical data, we also consider mixed data sets in our study. The
analysis overwhelmingly confirms the superiority of iterative imputation
algorithms, especially the methods implemented in the mice R package.

</details>


### [64] [Sequential Markov chain Monte Carlo for Filtering of State-Space Models with Low or Degenerate Observation Noise](https://arxiv.org/abs/2511.04975)
*Abylay Zhumekenov,Alexandros Beskos,Dan Crisan,Matthew Graham,Ajay Jasra,Nikolas Kantas*

Main category: stat.CO

TL;DR: 本文研究观测噪声退化或较低场景下的离散时间滤波问题，推导滤波密度及递归公式，设计序贯马尔可夫链蒙特卡罗方法，证明特定线性模型下低噪声与退化噪声情况的收敛关系，并在多个随机模型上验证方法性能。


<details>
  <summary>Details</summary>
Motivation: 解决观测噪声退化或较低场景下的离散时间滤波问题。

Method: 在特定流形序列上推导滤波密度及递归公式，设计序贯马尔可夫链蒙特卡罗方法近似滤波器。

Result: 对于特定线性观测模型，低噪声下序贯马尔可夫链蒙特卡罗方法在噪声消失时收敛到退化噪声时的情况。

Conclusion: 所提方法在来自统计学和应用数学的多个具有挑战性的随机模型上表现良好。

Abstract: We consider the discrete-time filtering problem in scenarios where the
observation noise is degenerate or low. More precisely, one is given access to
a discrete time observation sequence which at any time $k$ depends only on the
state of an unobserved Markov chain. We specifically assume that the functional
relationship between observations and hidden Markov chain has either degenerate
or low noise. In this article, under suitable assumptions, we derive the
filtering density and its recursions for this class of problems on a specific
sequence of manifolds defined through the observation function. We then design
sequential Markov chain Monte Carlo methods to approximate the filter serially
in time. For a certain linear observation model, we show that using sequential
Markov chain Monte Carlo for low noise will converge as the noise disappears to
that of using sequential Markov chain Monte Carlo for degenerate noise. We
illustrate the performance of our methodology on several challenging stochastic
models deriving from Statistics and Applied Mathematics.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [65] [Efficient Deployment of CNN Models on Multiple In-Memory Computing Units](https://arxiv.org/abs/2511.04682)
*Eleni Bougioukou,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: 本文利用IMC模拟器研究CNN模型在多处理系统中的部署对性能的影响，提出LBLP算法并与其他调度策略对比，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 在基于IMC的硬件上高效部署CNN需要先进的任务分配策略以实现最大计算效率。

Method: 利用具有多个处理单元的IMC模拟器，引入LBLP算法动态分配CNN节点到可用处理单元。

Result: 对多个CNN模型将LBLP算法与其他调度策略进行基准测试，实验结果证明了该算法的有效性。

Conclusion: LBLP算法能通过高效利用资源，最大化处理速率并最小化延迟。

Abstract: In-Memory Computing (IMC) represents a paradigm shift in deep learning
acceleration by mitigating data movement bottlenecks and leveraging the
inherent parallelism of memory-based computations. The efficient deployment of
Convolutional Neural Networks (CNNs) on IMC-based hardware necessitates the use
of advanced task allocation strategies for achieving maximum computational
efficiency. In this work, we exploit an IMC Emulator (IMCE) with multiple
Processing Units (PUs) for investigating how the deployment of a CNN model in a
multi-processing system affects its performance, in terms of processing rate
and latency. For that purpose, we introduce the Load-Balance-Longest-Path
(LBLP) algorithm, that dynamically assigns all CNN nodes to the available IMCE
PUs, for maximizing the processing rate and minimizing latency due to efficient
resources utilization. We are benchmarking LBLP against other alternative
scheduling strategies for a number of CNN models and experimental results
demonstrate the effectiveness of the proposed algorithm.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [66] [The Future of Fully Homomorphic Encryption System: from a Storage I/O Perspective](https://arxiv.org/abs/2511.04946)
*Lei Chen,Erci Xu,Yiming Sun,Shengyu Fan,Xianglong Deng,Guiming Shi,Guang Fan,Liang Kong,Yilan Zhu,Shoumeng Yan,Mingzhe Zhang*

Main category: cs.CR

TL;DR: 分析存储I/O对FHE应用性能的影响，发现其会严重降低ASIC和GPU性能。


<details>
  <summary>Details</summary>
Motivation: FHE虽能增强用户隐私，但部署时的I/O挑战研究不足，需分析存储I/O对FHE应用性能的影响。

Method: 分析存储I/O对FHE应用性能的影响并总结现状经验。

Result: 存储I/O可使ASIC性能降低达357倍，使GPU性能降低达22倍。

Conclusion: 未明确提及，但暗示存储I/O对FHE应用性能影响大，需关注解决。

Abstract: Fully Homomorphic Encryption (FHE) allows computations to be performed on
encrypted data, significantly enhancing user privacy. However, the I/O
challenges associated with deploying FHE applications remains understudied. We
analyze the impact of storage I/O on the performance of FHE applications and
summarize key lessons from the status quo. Key results include that storage I/O
can degrade the performance of ASICs by as much as 357$\times$ and reduce GPUs
performance by up to 22$\times$.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [67] [Reconstructing Riemannian Metrics From Random Geometric Graphs](https://arxiv.org/abs/2511.05434)
*Han Huang,Pakawut Jiradilok,Elchanan Mossel*

Main category: math.PR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Random geometric graphs are random graph models defined on metric measure
spaces. A random geometric graph is generated by first sampling points from a
metric space and then connecting each pair of sampled points independently with
a probability that depends on their distance.
  In recent work of Huang, Jiradilok, and Mossel~\cite{HJM24}, the authors
study the problem of reconstructing an embedded manifold form a random
geometric graph sampled from the manifold, where edge probabilities depend
monotonically on the Euclidean distance between the embedded points. They show
that, under mild regularity assumptions on the manifold, the sampling measure,
and the connection probability function, it is possible to recover the pairwise
Euclidean distances of the embedded sampled points up to a vanishing error as
the number of vertices grows.
  In this work we consider a similar and arguably more natural problem where
the metric is the Riemannian metric on the manifold. Again points are sampled
from the manifold and a random graph is generated where the connection
probability is monotone in the Riemannian distance. Perhaps surprisingly we
obtain stronger results in this setup.
  Unlike the previous work that only considered dense graph we provide
reconstruction algorithms from sparse graphs with average degree $n^{1/2}{\rm
polylog}(n)$, where $n$ denotes the number of vertices. Our algorithm is also a
more efficient algorithm for distance reconstruction with improved error
bounds. The running times of the algorithm is
  $O(n^2\,{\rm polylog}(n))$ which up to polylog factor matches the size of the
input graph.
  Our distance error also nearly matches the volumetric lower bounds for
distance estimation.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [68] [Insights into Tail-Based and Order Statistics](https://arxiv.org/abs/2511.04784)
*Hamidreza Maleki Almani*

Main category: math.ST

TL;DR: 本文对重尾分布中分位数贡献统计量及其与顺序统计量的关系进行理论研究，推导相关分布函数，研究渐近行为并通过模拟验证结果。


<details>
  <summary>Details</summary>
Motivation: 重尾分布挑战经典统计模型，分位数贡献是关键描述指标，需深入研究其统计特性。

Method: 推导顺序统计量的联合累积分布函数，在此基础上得到分位数贡献的累积分布函数，研究其渐近行为，进行模拟研究。

Result: 得到适用于小样本的分位数贡献的显式累积分布函数，确定分子的渐近正态性和分位数贡献的极限分布，模拟研究验证了理论结果的收敛性和经验准确性。

Conclusion: 为分位数贡献在重尾数据分析中的应用提供了理论基础。

Abstract: Heavy-tailed phenomena appear across diverse domains --from wealth and firm
sizes in economics to network traffic, biological systems, and physical
processes-- characterized by the disproportionate influence of extreme values.
These distributions challenge classical statistical models, as their tails
decay too slowly for conventional approximations to hold. Among their key
descriptive measures are quantile contributions, which quantify the proportion
of a total quantity (such as income, energy, or risk) attributed to
observations above a given quantile threshold. This paper presents a
theoretical study of the quantile contribution statistic and its relationship
with order statistics. We derive a closed-form expression for the joint
cumulative distribution function (CDF) of order statistics and, based on it,
obtain an explicit CDF for quantile contributions applicable to small samples.
We then investigate the asymptotic behavior of these contributions as the
sample size increases, establishing the asymptotic normality of the numerator
and characterizing the limiting distribution of the quantile contribution.
Finally, simulation studies illustrate the convergence properties and empirical
accuracy of the theoretical results, providing a foundation for applying
quantile contributions in the analysis of heavy-tailed data.

</details>


### [69] [Generalization in Representation Models via Random Matrix Theory: Application to Recurrent Networks](https://arxiv.org/abs/2511.02401)
*Yessin Moakher,Malik Tiomoko,Cosme Louart,Zhenyu Liao*

Main category: math.ST

TL;DR: 研究固定特征表示加可训练读出层模型的泛化误差，用随机矩阵理论推导渐近泛化误差，分析循环表示，揭示线性ESN与岭回归关系，实验验证性能，提供分析过参数化模型框架。


<details>
  <summary>Details</summary>
Motivation: 研究使用固定特征表示和可训练读出层的模型的泛化误差，分析相关架构性能。

Method: 在高维情况下应用随机矩阵理论推导渐近泛化误差，对循环表示进行分析。

Result: 得出线性ESN等同于具有指数时间加权输入协方差的岭回归，实验表明ESN在低样本、短记忆场景获胜，岭回归在数据多或长程依赖场景占优。

Conclusion: 所提方法为分析过参数化模型提供通用框架，为深度学习网络行为提供见解。

Abstract: We first study the generalization error of models that use a fixed feature
representation (frozen intermediate layers) followed by a trainable readout
layer. This setting encompasses a range of architectures, from deep
random-feature models to echo-state networks (ESNs) with recurrent dynamics.
Working in the high-dimensional regime, we apply Random Matrix Theory to derive
a closed-form expression for the asymptotic generalization error. We then apply
this analysis to recurrent representations and obtain concise formula that
characterize their performance. Surprisingly, we show that a linear ESN is
equivalent to ridge regression with an exponentially time-weighted (''memory'')
input covariance, revealing a clear inductive bias toward recent inputs.
Experiments match predictions: ESNs win in low-sample, short-memory regimes,
while ridge prevails with more data or long-range dependencies. Our methodology
provides a general framework for analyzing overparameterized models and offers
insights into the behavior of deep learning networks.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [70] [Clearing Up the Effective Lower Bound Morass](https://arxiv.org/abs/2511.04782)
*Haochun Ma,Jordan Roulleau-Pasdeloup*

Main category: econ.GN

TL;DR: 使用截断马尔可夫链解决标准新凯恩斯模型在有效下限的结论困境，证明特定陷阱不会出现且均衡唯一，政府支出有积极影响但存在难题。


<details>
  <summary>Details</summary>
Motivation: 标准新凯恩斯模型在有效下限因潜在马尔可夫链冲击持久性不同得出不同结论，需要解决这一困境。

Method: 使用截断马尔可夫链。

Result: 证明Mertens & Ravn (2014)提出的预期驱动陷阱不会作为均衡结果出现，截断马尔可夫链下的均衡是唯一的，政府支出对消费有积极影响且不改变符号，但影响可能无界增长。

Conclusion: 使用截断马尔可夫链能解决模型困境，但存在政府支出影响无界增长的难题。

Abstract: Depending on the persistence of the underlying Markov chain shock, the
standard New Keynesian model predicts starkly different conclusions at the
Effective Lower Bound. We clear up this morass by using a truncated Markov
chain. We prove that the expectations-driven trap \`a la Mertens & Ravn (2014)
doesn't arise as an equilibrium outcome. In addition, the equilibrium under a
truncated Markov chain is guaranteed to be unique, the effect of government
spending is positive on consumption and does not switch signs but may grow
unbounded \textemdash a puzzle.

</details>


### [71] [Election and Subjective Well-Being:Evidence from the 2024 U.S. Presidential Election](https://arxiv.org/abs/2511.04912)
*Dongyoung Kim,Young-Il Albert Kim,Haedong Aiden Rho*

Main category: econ.GN

TL;DR: 本文用行为风险因素监测系统数据，通过断点回归设计评估2024年美国大选对心理健康和生活满意度的因果影响，发现大选后主观幸福感下降，揭示政治极化是心理压力源。


<details>
  <summary>Details</summary>
Motivation: 评估2024年美国大选这一竞争激烈且结果消除不确定性的事件，对民众心理健康和生活满意度的影响。

Method: 运用每日行为风险因素监测系统数据，采用断点回归设计。

Result: 大选结果确定后，主观幸福感出现急剧且持续的下降，主要集中在女性、非白人、城市和高学历受访者中。

Conclusion: 政治极化本身而非选举意外，可成为长期的心理压力源。

Abstract: This paper uses daily Behavioral Risk Factor Surveillance System data to
estimate the causal effect of the 2024 U.S. presidential election, a highly
competitive race whose outcome resolved lingering uncertainty on election day,
on mental-health and life-satisfaction outcomes through a regression
discontinuity design. Following the resolution of electoral uncertainty on
election day, we find a sharp and persistent post-election decline in
subjective well-being, concentrated among female, non-White, urban, and
more-educated respondents. These findings reveal an expected-outcome shock,
showing that political polarization itself, not electoral surprise, can act as
a chronic psychological stressor.

</details>


### [72] [Impacts of large-scale food fortification on the cost of nutrient-adequate diets: a modeling study in 89 countries](https://arxiv.org/abs/2511.05438)
*Leah Costlow,Yan Bai,Katherine P. Adams,Ty Beal,Kathryn G. Dewey,Christopher M. Free,Valerie M. Friesen,Mduduzi N. N. Mbuya,Stella Nordhagen,Florencia C. Vasta,William A. Masters*

Main category: econ.GN

TL;DR: 研究用89国数据估算大规模食物强化（LSFF）对营养充足饮食成本的降低程度，发现不同场景下成本有不同程度降低，强调因地制宜设计政策可提高饮食可负担性。


<details>
  <summary>Details</summary>
Motivation: LSFF政策实施常不完整且其对饮食成本的影响不明，需估算其降低营养充足饮食成本的程度。

Method: 利用89个国家的零售食品价格和强化政策数据，对22个性别 - 年龄组和3种营养充足情景下的5874种最低成本饮食进行建模。

Result: 假设现有LSFF标准实施率达90%，三种情景下饮食成本中位数分别降低1.7%、2.4%和4.5%，成本降低幅度因性别 - 年龄组、国家强化策略和食品价格结构而异。

Conclusion: LSFF政策因地制宜设计时可提高饮食可负担性，是改善营养饮食可及性其他努力的重要补充。

Abstract: Large-scale food fortification (LSFF) is a widely accepted intervention to
alleviate micronutrient deficiencies, yet policy implementation is often
incomplete and its effects on diet costs are not well established. We estimated
the extent to which LSFF reduces the cost of nutrient-adequate diets using
retail food prices and fortification policy data from 89 countries. In total,
we modeled 5,874 least-cost diets across 22 sex-age groups and 3
nutrient-adequacy scenarios: meeting nutrient requirements only; adding minimum
intakes for starchy staples and fruits and vegetables; and aligning food group
shares with national consumption patterns. Assuming 90% implementation of
existing LSFF standards, we found median cost reductions of 1.7%, 2.4%, and
4.5% across the three scenarios. Cost reductions varied widely by sex-age
groups, national fortification strategies and food price structures. These
findings highlight that LSFF may improve diet affordability when policies are
carefully designed for local contexts, making it a valuable complement to other
efforts that improve access to nutritious diets.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [73] [EMO100DB: An Open Dataset of Improvised Songs with Emotion Data](https://arxiv.org/abs/2511.04755)
*Daeun Hwang,Saebyul Park*

Main category: cs.SD

TL;DR: 本文介绍了Emo100DB数据集，该数据集基于Russell情绪模型，包含即兴歌曲及情绪数据，旨在全面探索音乐与情绪关系。


<details>
  <summary>Details</summary>
Motivation: 提供一个综合数据集，以支持对音乐和情绪关系的多样化探索。

Method: 收集20名年轻人演奏、演唱和录制的包含旋律、歌词和器乐伴奏的即兴歌曲，记录前让参与者按Russell模型报告情绪状态，将数据集按四个情绪象限组织。

Result: 开发出Emo100DB数据集，包含歌词文本、旋律MIDI文件和原始WAV音频。

Conclusion: 通过提供数据和分析的综合组成，该数据集可用于全面探索音乐与情绪的关系。

Abstract: In this study, we introduce Emo100DB: a dataset consisting of improvised
songs that were recorded and transcribed with emotion data based on Russell's
circumplex model of emotion. The dataset was developed by collecting improvised
songs that consist of melody, lyrics, and an instrumental accompaniment played,
sung, and recorded by 20 young adults. Before recording each song, the
participants were asked to report their emotional state, with the axes
representing arousal and valence based on Russell's circumplex model of
emotions. The dataset is organized into four emotion quadrants, and it includes
the lyrics text and MIDI file of the melody extracted from the participant
recordings, along with the original audio in WAV format. By providing an
integrated composition of data and analysis, this study aims to offer a
comprehensive dataset that allows for a diverse exploration of the relationship
between music and emotion.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [74] [The use of social media among library professionals and patrons: A review of literature](https://arxiv.org/abs/2511.05051)
*Abimbola Agboke,Felicia Nkatv Undie*

Main category: cs.DL

TL;DR: 本文探讨图书馆专业人员和用户对社交媒体的使用，分析流行平台、采用原因，揭示其贡献并建议鼓励使用。


<details>
  <summary>Details</summary>
Motivation: 了解图书馆专业人员和用户对社交媒体的使用情况。

Method: 文献综述。

Result: 社交媒体有助于图书馆专业人员和用户，能促进连接、传播信息和推广资源服务。

Conclusion: 建议图书馆管理委员会鼓励在图书馆使用社交媒体。

Abstract: This paper focused on the utilization of social media by library
professionals and library users. It provides an understanding of social media,
the most popular social media platforms utilized in the libraries. It also
mentions the reasons for the adoption of social media in libraries be it
academic, public, school libraries and other types of libraries. This is a
review paper on the use of social media among library professionals and
patrons. The findings reveal the contributions of social media to the
libraries. Social media makes things easy for library professionals and library
users. It enables them to connect, create awareness to new information,
disseminate information instantly, and helps to market the library resources
and services. Therefore, it is recommended amongst others that the library
management board should encourage the use of social media in libraries.

</details>


### [75] [AI-Powered Citation Auditing: A Zero-Assumption Protocol for Systematic Reference Verification in Academic Research](https://arxiv.org/abs/2511.04683)
*L. J. Janse van Rensburg*

Main category: cs.DL

TL;DR: 文章提出用有工具使用能力的AI进行学术引用审核，验证了方法有效性，提高效率且降低误报率。


<details>
  <summary>Details</summary>
Motivation: 学术引用完整性面临挑战，人工验证耗时久，需新方法。

Method: 开发零假设验证协议，用AI将每个引用与多个学术数据库独立验证。

Result: 在30份学术文档上验证，PLOS论文平均验证率91.7%，检测多种问题，大幅提高效率，误报率<0.5%。

Conclusion: 建立首个经验证的AI代理方法，有实际应用价值。

Abstract: Academic citation integrity faces persistent challenges, with research
indicating 20% of citations contain errors and manual verification requiring
months of expert time. This paper presents a novel AI-powered methodology for
systematic, comprehensive reference auditing using agentic AI with tool-use
capabilities. We develop a zero-assumption verification protocol that
independently validates every reference against multiple academic databases
(Semantic Scholar, Google Scholar, CrossRef) without assuming any citation is
correct. The methodology was validated across 30 academic documents (2,581
references) spanning undergraduate projects to doctoral theses and
peer-reviewed publications. Results demonstrate 91.7% average verification rate
on published PLOS papers, with successful detection of fabricated references,
retracted articles, orphan citations, and predatory journals. Time efficiency
improved dramatically: 90-minute audits for 916-reference doctoral theses
versus months of manual review. The system achieved <0.5% false positive rate
while identifying critical issues manual review might miss. This work
establishes the first validated AI-agent methodology for academic citation
integrity, demonstrating practical applicability for supervisors, students, and
institutional quality assurance.

</details>


### [76] [Mapping Research Productivity of BRICS Countries with Special Reference to Coronary Artery Disease (CAD): A Scientometric Study](https://arxiv.org/abs/2511.05211)
*Muneer Ahmad*

Main category: cs.DL

TL;DR: 对1990 - 2019年BRICS国家冠心病研究产出进行科学计量分析，发现产出和合作增长显著但成员国有差异，给出相关建议。


<details>
  <summary>Details</summary>
Motivation: 全面分析BRICS国家冠心病研究产出情况。

Method: 从Web of Science数据库获取数据，分析50,036条记录，评估发表增长趋势、作者模式、合作水平和引文影响。

Result: 冠心病相关出版物稳步增加，中国贡献最大；英语为主；合著比例高；验证部分定律，普莱斯平方根定律不适用；期刊文章为主；俄罗斯《Kardiologiya》最活跃；成员国有显著差异。

Conclusion: 建议提高作者生产力、扩大国际合作，通过战略举措支持研究，为相关方提供见解以加强发展中经济体心血管研究能力。

Abstract: This study presents a comprehensive scientometric analysis of research
productivity on Coronary Artery Disease (CAD) among the BRICS countries,
Brazil, Russia, India, China, and South Africa, using data retrieved from the
Web of Science database for the period 1990 to 2019. A total of 50,036 records
were analyzed to assess publication growth trends, authorship patterns,
collaboration levels, and citation impact. The findings reveal a steady
increase in CAD-related publications, with China emerging as the leading
contributor, followed by Brazil, Russia, India, and South Africa. English
dominated as the primary language of communication, accounting for over 93% of
publications. Authorship and collaboration analysis indicate a high degree of
joint research, with 97.91% of studies being co-authored and a degree of
collaboration of 0.98, underscoring the collective nature of scientific inquiry
in this domain. The study validates the applicability of Lotkas Law for author
productivity, Bradfords Law for journal distribution, and Zipfs Law for keyword
frequency, while the Price Square Root Law was found inapplicable. The
predominant publication format was journal articles (79.7%), and Kardiologiya
(Russia) emerged as the most prolific journal. The results demonstrate
significant growth in CAD research output and collaboration within BRICS,
though notable disparities persist among member nations. The study recommends
enhancing individual author productivity, expanding international
collaboration, and supporting CAD research through strategic institutional and
governmental initiatives. These findings provide valuable insights for
policymakers, funding agencies, and the academic community to strengthen
cardiovascular research capacity within developing economies.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [77] [Awesome graph parameters](https://arxiv.org/abs/2511.05285)
*Kenny Bešter Štorgel,Clément Dallard,Vadim Lozin,Martin Milanič,Viktor Zamaraev*

Main category: math.CO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: For a graph $G$, we denote by $\alpha(G)$ the size of a maximum independent
set and by $\omega(G)$ the size of a maximum clique in $G$. Our paper lies on
the edge of two lines of research, related to $\alpha$ and $\omega$,
respectively. One of them studies $\alpha$-variants of graph parameters, such
as $\alpha$-treewidth or $\alpha$-degeneracy. The second line deals with graph
classes where some parameters are bounded by a function of $\omega(G)$. A
famous example of this type is the family of $\chi$-bounded classes, where the
chromatic number $\chi(G)$ is bounded by a function of $\omega(G)$.
  A Ramsey-type argument implies that if the $\alpha$-variant of a graph
parameter $\rho$ is bounded by a constant in a class $\mathcal{G}$, then $\rho$
is bounded by a function of $\omega$ in $\mathcal{G}$. If the reverse
implication also holds, we say that $\rho$ is awesome. Otherwise, we say that
$\rho$ is awful. In the present paper, we identify a number of awesome and
awful graph parameters, derive some algorithmic applications of awesomeness,
and propose a number of open problems related to these notions.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [78] [EncouRAGe: Evaluating RAG Local, Fast, and Reliable](https://arxiv.org/abs/2511.04696)
*Jan Strich,Adeline Scharfenberg,Chris Biemann,Martin Semmann*

Main category: cs.CL

TL;DR: 介绍EncouRAGe框架用于简化RAG系统开发与评估，展示其组件、评估结果等。


<details>
  <summary>Details</summary>
Motivation: 简化使用大语言模型和嵌入模型的RAG系统的开发与评估。

Method: 构建包含五个模块化可扩展组件的EncouRAGe框架，对多个基准数据集进行评估。

Result: RAG表现不如Oracle Context，Hybrid BM25在四个数据集上效果最佳，重排性能提升有限且增加响应延迟。

Conclusion: EncouRAGe框架可助力RAG系统研究，且呈现了RAG系统在不同设置下的性能情况。

Abstract: We introduce EncouRAGe, a comprehensive Python framework designed to
streamline the development and evaluation of Retrieval-Augmented Generation
(RAG) systems using Large Language Models (LLMs) and Embedding Models.
EncouRAGe comprises five modular and extensible components: Type Manifest, RAG
Factory, Inference, Vector Store, and Metrics, facilitating flexible
experimentation and extensible development. The framework emphasizes scientific
reproducibility, diverse evaluation metrics, and local deployment, enabling
researchers to efficiently assess datasets within RAG workflows. This paper
presents implementation details and an extensive evaluation across multiple
benchmark datasets, including 25k QA pairs and over 51k documents. Our results
show that RAG still underperforms compared to the Oracle Context, while Hybrid
BM25 consistently achieves the best results across all four datasets. We
further examine the effects of reranking, observing only marginal performance
improvements accompanied by higher response latency.

</details>


### [79] [Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs](https://arxiv.org/abs/2511.04869)
*Preetum Nakkiran,Arwen Bradley,Adam Goliński,Eugene Ndiaye,Michael Kirchhof,Sinead Williamson*

Main category: cs.CL

TL;DR: 研究发现基础大语言模型在语义校准方面表现良好，提出理论解释并通过实验验证相关预测。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型能否评估其输出实际意义的置信度，解决基础大语言模型缺乏有意义置信估计的问题。

Method: 建立语义校准作为下一个标记预测副产品出现的机制，基于“B - 校准”定义，提出可测试预测并通过实验验证。

Result: 基础大语言模型在问答任务中语义校准，强化学习指令微调及思维链推理会破坏校准。

Conclusion: 首次对大语言模型中语义校准何时以及为何出现给出了有原则的解释。

Abstract: Large Language Models (LLMs) often lack meaningful confidence estimates for
their outputs. While base LLMs are known to exhibit next-token calibration, it
remains unclear whether they can assess confidence in the actual meaning of
their responses beyond the token level. We find that, when using a certain
sampling-based notion of semantic calibration, base LLMs are remarkably
well-calibrated: they can meaningfully assess confidence in open-domain
question-answering tasks, despite not being explicitly trained to do so. Our
main theoretical contribution establishes a mechanism for why semantic
calibration emerges as a byproduct of next-token prediction, leveraging a
recent connection between calibration and local loss optimality. The theory
relies on a general definition of "B-calibration," which is a notion of
calibration parameterized by a choice of equivalence classes (semantic or
otherwise). This theoretical mechanism leads to a testable prediction: base
LLMs will be semantically calibrated when they can easily predict their own
distribution over semantic answer classes before generating a response. We
state three implications of this prediction, which we validate through
experiments: (1) Base LLMs are semantically calibrated across
question-answering tasks, (2) RL instruction-tuning systematically breaks this
calibration, and (3) chain-of-thought reasoning breaks calibration. To our
knowledge, our work provides the first principled explanation of when and why
semantic calibration emerges in LLMs.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [80] [Local Technological Access, Income Disparities, and Job-Seeking in the United States Since 2010](https://arxiv.org/abs/2511.05294)
*Shaolong Wu*

Main category: cs.CY

TL;DR: 研究利用NLSY97纵向数据，分析地域技术因素等对收入和求职决策的影响，揭示教育程度等因素与工资和求职强度的关系，强调需本地化干预保障技术公平获取。


<details>
  <summary>Details</summary>
Motivation: 现代美国劳动力市场中，数字基础设施影响个人就业，区域数字差异对劳动力公平和可持续性有重要影响，需研究相关因素对收入和求职的作用。

Method: 利用NLSY97纵向数据进行回归分析。

Result: 教育程度、婚姻状况和互联网使用频率能强烈预测工资和求职强度，存在区域收入差距。

Conclusion: 需要更多本地化干预以确保技术公平获取，同时提出数字基础设施对服务不足社区系统性不平等影响的问题。

Abstract: In the modern U.S. labor market, digital infrastructures strongly influence
how individuals locate opportunities, build skills, and advance wages. Regional
differences in computing access, broadband coverage, and digital literacy have
significant labor implications for equity and sustainability. Drawing on
longitudinal data from the NLSY97 (National Longitudinal Surveys of Youth)
cohort, this study examines how place-based technological factors, personal
demographics, household characteristics, and education shape income levels and
decisions to seek new employment. The regression analyses reveal that
educational attainment, marital status, and frequency of Internet usage
strongly predict both wages and individuals' job-seeking intensity. Regional
disparities in income underscore the need for more localized interventions to
ensure equitable access to technology. This study raises key questions about
how digital infrastructures can reinforce or challenge systemic inequalities in
underserved communities.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [81] [An Overview of Some Extensions of Mean Field Games beyond Perfect Homogeneity and Anonymity](https://arxiv.org/abs/2511.04929)
*Mathieu Laurière*

Main category: math.OC

TL;DR: 本文旨在对放宽标准平均场博弈（MFG）框架假设的模型进行教学式介绍。


<details>
  <summary>Details</summary>
Motivation: 标准MFG框架的同质性和匿名性假设在很多应用中具有局限性，需要对放宽这些假设的模型进行介绍。

Method: 讨论多群体MFG、图论MFG、主次MFG、Stackelberg MFG以及涉及合作参与者的变体。

Result: 无明确提及具体结果。

Conclusion: 提供了对放宽标准MFG框架假设的模型的教学式介绍。

Abstract: The mean field games (MFG) paradigm was introduced to provide tractable
approximations of games involving very large populations. The theory typically
rests on two key assumptions: homogeneity, meaning that all players share the
same dynamics and cost functions, and anonymity, meaning that each player
interacts with others only through their empirical distribution. While these
assumptions simplify the analysis, they can be restrictive for many
applications. Fortunately, several extensions of the standard MFG framework
that relax these assumptions have been developed in the literature. The purpose
of these notes is to offer a pedagogical introduction to such models. In
particular, we discuss multi-population MFGs, graphon MFGs, major-minor MFGs,
and Stackelberg MFGs, as well as variants involving cooperative players.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [82] [A Gate-Based Quantum Genetic Algorithm for Real-Valued Global Optimization](https://arxiv.org/abs/2511.05254)
*Leandro C. Souza,Laurent E. Dardenne,Renato Portugal*

Main category: quant-ph

TL;DR: 提出基于门的量子遗传算法用于实值全局优化，对比不同门集，证明叠加和纠缠能增强算法搜索能力。


<details>
  <summary>Details</summary>
Motivation: 寻找量子增强的全局优化方法，探索量子资源对优化算法的影响。

Method: 用量子电路表示个体，进化算子作用于电路结构，通过量子采样评估适应度，对比不同门集和引入个体间纠缠。

Result: 叠加能提高收敛性和鲁棒性，个体间纠缠可加速早期收敛。

Conclusion: 叠加和纠缠可增强进化量子算法搜索动态，基于门的量子遗传算法是有前景的量子增强全局优化框架。

Abstract: We propose a gate-based Quantum Genetic Algorithm (QGA) for real-valued
global optimization. In this model, individuals are represented by quantum
circuits whose measurement outcomes are decoded into real-valued vectors
through binary discretization. Evolutionary operators act directly on circuit
structures, allowing mutation and crossover to explore the space of gate-based
encodings. Both fixed-depth and variable-depth variants are introduced,
enabling either uniform circuit complexity or adaptive structural evolution.
Fitness is evaluated through quantum sampling, using the mean decoded output of
measurement outcomes as the argument of the objective function. To isolate the
impact of quantum resources, we compare gate sets with and without the Hadamard
gate, showing that superposition consistently improves convergence and
robustness across benchmark functions such as the Rastrigin function.
Furthermore, we demonstrate that introducing pairwise inter-individual
entanglement in the population accelerates early convergence, revealing that
quantum correlations among individuals provide an additional optimization
advantage. Together, these results show that both superposition and
entanglement enhance the search dynamics of evolutionary quantum algorithms,
establishing gate-based QGAs as a promising framework for quantum-enhanced
global optimization.

</details>


### [83] [CUNQA: a Distributed Quantum Computing emulator for HPC](https://arxiv.org/abs/2511.05209)
*Jorge Vázquez-Pérez,Daniel Expósito-Patiño,Marta Losada,Álvaro Carballido,Andrés Gómez,Tomás F. Pena*

Main category: quant-ph

TL;DR: 本文介绍了开源DQC模拟器CUNQA，可在HPC环境中测试、评估和研究DQC，实现三种DQC模型，并用QPE算法演示分析，是首个在HPC环境中模拟三种DQC方案的工具。


<details>
  <summary>Details</summary>
Motivation: 应对量子计算机扩展挑战，将分布式量子计算与高性能计算环境结合，在DQC成为现实前进行测试研究。

Method: 实现无通信、经典通信和量子通信三种DQC模型，用QPE算法演示分析模型的仿真。

Result: 开发出CUNQA模拟器，可在HPC环境中对三种DQC模型进行仿真。

Conclusion: CUNQA是首个能在HPC环境中模拟三种DQC方案的工具。

Abstract: The challenge of scaling quantum computers to gain computational power is
expected to lead to architectures with multiple connected quantum processing
units (QPUs), commonly referred to as Distributed Quantum Computing (DQC). In
parallel, there is a growing momentum toward treating quantum computers as
accelerators, integrating them into the heterogeneous architectures of
high-performance computing (HPC) environments. This work combines these two
foreseeable futures in CUNQA, an open-source DQC emulator designed for HPC
environments that allows testing, evaluating and studying DQC in HPC before it
even becomes real. It implements the three DQC models of no-communication,
classical-communication and quantum-communication; which will be examined in
this work. Addressing programming considerations, explaining emulation and
simulation details, and delving into the specifics of the implementation will
be part of the effort. The well-known Quantum Phase Estimation (QPE) algorithm
is used to demonstrate and analyze the emulation of the models. To the best of
our knowledge, CUNQA is the first tool designed to emulate the three DQC
schemes in an HPC environment.

</details>


### [84] [QCircuitBench: A Large-Scale Dataset for Benchmarking Quantum Algorithm Design](https://arxiv.org/abs/2410.07961)
*Rui Yang,Ziruo Wang,Yuntian Gu,Tianyi Chen,Yitao Liang,Tongyang Li*

Main category: quant-ph

TL;DR: 本文介绍首个用于评估AI设计和实现量子算法能力的基准数据集QCircuitBench，阐述其贡献、实验现象，揭示大语言模型在此领域的局限性。


<details>
  <summary>Details</summary>
Motivation: 量子算法设计和实现有挑战，且缺乏针对性数据集，需要评估AI在该领域的能力。

Method: 构建通用框架，实现从基本原语到高级应用的量子算法，设置自动验证和验证功能，进行初步微调。

Result: 观察到LLMs有一致错误模式，微调不总是优于少样本学习。

Conclusion: QCircuitBench是全面的基准，揭示了LLMs在量子算法设计领域的局限性。

Abstract: Quantum computing is an emerging field recognized for the significant speedup
it offers over classical computing through quantum algorithms. However,
designing and implementing quantum algorithms pose challenges due to the
complex nature of quantum mechanics and the necessity for precise control over
quantum states. Despite the significant advancements in AI, there has been a
lack of datasets specifically tailored for this purpose. In this work, we
introduce QCircuitBench, the first benchmark dataset designed to evaluate AI's
capability in designing and implementing quantum algorithms using quantum
programming languages. Unlike using AI for writing traditional codes, this task
is fundamentally more complicated due to highly flexible design space. Our key
contributions include: 1. A general framework which formulates the key features
of quantum algorithm design for Large Language Models. 2. Implementations for
quantum algorithms from basic primitives to advanced applications, spanning 3
task suites, 25 algorithms, and 120,290 data points. 3. Automatic validation
and verification functions, allowing for iterative evaluation and interactive
reasoning without human inspection. 4. Promising potential as a training
dataset through preliminary fine-tuning results. We observed several
interesting experimental phenomena: LLMs tend to exhibit consistent error
patterns, and fine-tuning does not always outperform few-shot learning. In all,
QCircuitBench is a comprehensive benchmark for LLM-driven quantum algorithm
design, and it reveals limitations of LLMs in this domain.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [85] [Inference for the Extended Functional Cox Model: A UK Biobank Case Study](https://arxiv.org/abs/2511.04852)
*Erjia Cui,Angela Zhao,Ciprian M. Crainiceanu*

Main category: stat.ME

TL;DR: 研究表明身体活动（PA）的昼夜模式及其日常变异性可提供更多关于死亡率的信息，引入扩展功能Cox模型及推理工具并应用于UK Biobank研究，模拟研究显示方法表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明加速度计测量的PA标量摘要对死亡率预测能力强，本文旨在探究PA的昼夜模式及其日常变异性是否能提供更多关于死亡率的信息。

Method: 引入扩展功能Cox模型及相应推理工具，用于量化大规模高维数据中多个功能和标量预测因子与事件发生时间结果之间的关联，并应用于UK Biobank研究。

Result: 模拟研究显示方法在现实场景中表现良好，可扩展到比UK Biobank加速度计研究大一个数量级的研究。

Conclusion: 建立这些方法在处理复杂大数据集上的可行性和可扩展性是应用功能数据分析的一个重要里程碑。

Abstract: Multiple studies have shown that scalar summaries of objectively measured
physical activity (PA) using accelerometers are the strongest predictors of
mortality, outperforming all traditional risk factors, including age, sex, body
mass index (BMI), and smoking. Here we show that diurnal patterns of PA and
their day-to-day variability provide additional information about mortality. To
do that, we introduce a class of extended functional Cox models and
corresponding inferential tools designed to quantify the association between
multiple functional and scalar predictors with time-to-event outcomes in
large-scale (large $n$) high-dimensional (large $p$) datasets. Methods are
applied to the UK Biobank study, which collected PA at every minute of the day
for up to seven days, as well as time to mortality ($93{,}370$ participants
with good quality accelerometry data and $931$ events). Simulation studies show
that methods perform well in realistic scenarios and scale up to studies an
order of magnitude larger than the UK Biobank accelerometry study. Establishing
the feasibility and scalability of these methods for such complex and large
data sets is a major milestone in applied Functional Data Analysis (FDA).

</details>


### [86] [Clustering in Networks with Time-varying Nodal Attributes](https://arxiv.org/abs/2511.04859)
*Yik Lun Kei,Oscar Hernan Madrid Padilla,Rebecca Killick,James Wilson,Xi Chen,Robert Lund*

Main category: stat.ME

TL;DR: 研究带时间序列图的节点聚类，提出融合结构和时间模式的方法并验证有效性


<details>
  <summary>Details</summary>
Motivation: 解决图节点聚类中节点属性演化常被忽视的问题

Method: 构建含低维表示先验和解码器的框架，用最大近似似然学习参数，加图融合LASSO正则化，用交替方向乘子法求解优化问题，Langevin动力学进行后验推断

Result: 在块和网格图的模拟研究及加州县温度、书籍词共现网络应用中证明方法有效

Conclusion: 所提方法能有效解决带时间序列图的节点聚类问题

Abstract: This manuscript studies nodal clustering in graphs having a time series at
each node. The framework includes priors for low-dimensional representations
and a decoder that bridges the latent representations and time series. The
structural and temporal patterns are fused into representations that facilitate
clustering, addressing the limitation that the evolution of nodal attributes is
often overlooked. Parameters are learned via maximum approximate likelihood,
with a graph-fused LASSO regularization imposed on prior parameters. The
optimization problem is solved via alternating direction method of multipliers;
Langevin dynamics are employed for posterior inference. Simulation studies on
block and grid graphs with autoregressive dynamics, and applications to
California county temperatures and a book word co-occurrence network
demonstrate the effectiveness of the proposed method.

</details>
