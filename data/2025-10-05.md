<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 51]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.LG](#cs.LG) [Total: 146]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.SE](#cs.SE) [Total: 14]
- [q-fin.CP](#q-fin.CP) [Total: 3]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 11]
- [stat.CO](#stat.CO) [Total: 2]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [stat.ME](#stat.ME) [Total: 4]
- [math.NA](#math.NA) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [math.OC](#math.OC) [Total: 4]
- [cs.HC](#cs.HC) [Total: 7]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [stat.AP](#stat.AP) [Total: 2]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.CL](#cs.CL) [Total: 48]
- [econ.GN](#econ.GN) [Total: 4]
- [cs.DL](#cs.DL) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [cs.RO](#cs.RO) [Total: 7]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [cs.CR](#cs.CR) [Total: 9]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.SD](#cs.SD) [Total: 10]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CV](#cs.CV) [Total: 36]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.CY](#cs.CY) [Total: 5]
- [q-bio.QM](#q-bio.QM) [Total: 5]
- [hep-lat](#hep-lat) [Total: 1]
- [hep-ex](#hep-ex) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [OR-Toolformer: Modeling and Solving Operations Research Problems with Tool Augmented Large Language Models](https://arxiv.org/abs/2510.01253)
*Jianzhang Zhang,Jialong Zhou,Chuang Liu*

Main category: cs.AI

TL;DR: 介绍OR - Toolformer，在多个基准测试表现佳，验证工具增强微调大语言模型对运筹学问题建模和求解的有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理运筹学任务依赖闭源API有隐私问题，从头训练开源模型计算成本高。

Method: 用半自动数据合成管道微调Llama - 3.1 - 8B - Instruct，生成多样运筹学问题 - 答案对，用外部求解器增强模型以产生API调用。

Result: 在四个标准基准中的三个上执行准确率达80.1%，超同规模基线超4.3%；在两种未见运筹学问题类型零样本评估中平均准确率54%，比最强基线高21个百分点。

Conclusion: 工具增强微调大语言模型对运筹学问题建模和求解有效且具泛化性。

Abstract: Large language models (LLMs) demonstrate strong mathematical reasoning, but
reliance on closed-source APIs for OR tasks raises privacy concerns, and
training open-source models from scratch incurs high compute costs. We
introduce OR-Toolformer, which fine-tunes Llama-3.1-8B-Instruct with a
semi-automatic data synthesis pipeline that generates diverse OR problem-answer
pairs and augments the model with external solvers to produce API calls. On
three of four standard benchmarks, OR-Toolformer achieves up to 80.1% execution
accuracy, exceeding size-matched baselines by over 4.3%. In zero-shot
evaluation on two unseen OR problem types, it attains 54% average accuracy, a
21 percentage-point improvement over the strongest baseline. These findings
validate the efficacy of tool-augmented fine-tuning LLMs for accurate and
generalizable OR problem modeling and solving.

</details>


### [2] [Modeling Others' Minds as Code](https://arxiv.org/abs/2510.01272)
*Kunal Jha,Aydan Yuenan Huang,Eric Ye,Natasha Jaques,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 提出新算法ROTE，将日常社交互动建模为行为程序，在多任务中预测人类和AI行为表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有建模人类行为的方法数据需求大且脆弱，需要更有效的方法准确预测人类行为以实现人机协作。

Method: 将日常社交互动建模为行为程序，引入ROTE算法，利用大语言模型合成假设空间，用概率推理处理不确定性。

Result: 在网格世界任务和家庭模拟器中测试，ROTE从稀疏观察中预测人类和AI行为，在样本内准确性和样本外泛化方面比竞争基线高出达50%。

Conclusion: 将动作理解视为程序合成问题，ROTE为AI系统在现实世界中高效准确预测人类行为开辟了道路。

Abstract: Accurate prediction of human behavior is essential for robust and safe
human-AI collaboration. However, existing approaches for modeling people are
often data-hungry and brittle because they either make unrealistic assumptions
about rationality or are too computationally demanding to adapt rapidly. Our
key insight is that many everyday social interactions may follow predictable
patterns; efficient "scripts" that minimize cognitive load for actors and
observers, e.g., "wait for the green light, then go." We propose modeling these
routines as behavioral programs instantiated in computer code rather than
policies conditioned on beliefs and desires. We introduce ROTE, a novel
algorithm that leverages both large language models (LLMs) for synthesizing a
hypothesis space of behavioral programs, and probabilistic inference for
reasoning about uncertainty over that space. We test ROTE in a suite of
gridworld tasks and a large-scale embodied household simulator. ROTE predicts
human and AI behaviors from sparse observations, outperforming competitive
baselines -- including behavior cloning and LLM-based methods -- by as much as
50% in terms of in-sample accuracy and out-of-sample generalization. By
treating action understanding as a program synthesis problem, ROTE opens a path
for AI systems to efficiently and effectively predict human behavior in the
real-world.

</details>


### [3] [Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town for Self-Directed Research Evolution and Emergent Scientific Discovery](https://arxiv.org/abs/2510.01293)
*Zekun Jiang,Chunming Xu,Tianhang Zhou*

Main category: cs.AI

TL;DR: 介绍CA - ChemE系统解决现有AI系统在化工领域跨学科协作等问题，评估其效果并揭示知识差距对协作效率的影响。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在化工领域跨学科协作和探索未知问题方面存在局限。

Method: 构建CA - ChemE系统，集成特定领域知识库、知识增强技术和协作代理。

Result: 知识库增强机制提升对话质量，协作代理干预显示不同知识差距下协作效率差异。

Conclusion: 精心设计的多智能体架构为化工领域自主科学发现提供可行途径。

Abstract: The rapid advancement of artificial intelligence (AI) has demonstrated
substantial potential in chemical engineering, yet existing AI systems remain
limited in interdisciplinary collaboration and exploration of uncharted
problems. To address these issues, we present the Cyber Academia-Chemical
Engineering (CA-ChemE) system, a living digital town that enables self-directed
research evolution and emergent scientific discovery through multi-agent
collaboration. By integrating domain-specific knowledge bases, knowledge
enhancement technologies, and collaboration agents, the system successfully
constructs an intelligent ecosystem capable of deep professional reasoning and
efficient interdisciplinary collaboration. Our findings demonstrate that
knowledge base-enabled enhancement mechanisms improved dialogue quality scores
by 10-15% on average across all seven expert agents, fundamentally ensuring
technical judgments are grounded in verifiable scientific evidence. However, we
observed a critical bottleneck in cross-domain collaboration efficiency,
prompting the introduction of a Collaboration Agent (CA) equipped with ontology
engineering capabilities. CA's intervention achieved 8.5% improvements for
distant-domain expert pairs compared to only 0.8% for domain-proximate pairs -
a 10.6-fold difference - unveiling the "diminished collaborative efficiency
caused by knowledge-base gaps" effect. This study demonstrates how carefully
designed multi-agent architectures can provide a viable pathway toward
autonomous scientific discovery in chemical engineering.

</details>


### [4] [The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation](https://arxiv.org/abs/2510.01295)
*Zarreen Reza*

Main category: cs.AI

TL;DR: 传统评估基准对大语言模型代理不足，本文引入多智能体辩论评估框架，发现智能体寻求共识倾向等，提供新评估协议蓝图。


<details>
  <summary>Details</summary>
Motivation: 传统评估基准无法捕捉大语言模型作为智能体在交互环境中的社交和认知动态，需新评估框架。

Method: 引入多智能体辩论评估框架，让基于大语言模型的智能体在大语言模型仲裁者监督下就广泛主题辩论，用新的心理测量和语义指标分析。

Result: 发现智能体有强大的寻求共识倾向，语义一致性高；分配的角色能产生可测量的心理特征；仲裁者角色可显著改变辩论结果。

Conclusion: 本文为智能体环境提供了基于心理测量的动态评估协议蓝图，有助于理解和塑造下一代人工智能智能体的社交行为。

Abstract: As Large Language Models (LLMs) transition from static tools to autonomous
agents, traditional evaluation benchmarks that measure performance on
downstream tasks are becoming insufficient. These methods fail to capture the
emergent social and cognitive dynamics that arise when agents communicate,
persuade, and collaborate in interactive environments. To address this gap, we
introduce a novel evaluation framework that uses multi-agent debate as a
controlled "social laboratory" to discover and quantify these behaviors. In our
framework, LLM-based agents, instantiated with distinct personas and
incentives, deliberate on a wide range of challenging topics under the
supervision of an LLM moderator. Our analysis, enabled by a new suite of
psychometric and semantic metrics, reveals several key findings. Across
hundreds of debates, we uncover a powerful and robust emergent tendency for
agents to seek consensus, consistently reaching high semantic agreement ({\mu}
> 0.88) even without explicit instruction and across sensitive topics. We show
that assigned personas induce stable, measurable psychometric profiles,
particularly in cognitive effort, and that the moderators persona can
significantly alter debate outcomes by structuring the environment, a key
finding for external AI alignment. This work provides a blueprint for a new
class of dynamic, psychometrically grounded evaluation protocols designed for
the agentic setting, offering a crucial methodology for understanding and
shaping the social behaviors of the next generation of AI agents. We have
released the code and results at
https://github.com/znreza/multi-agent-LLM-eval-for-debate.

</details>


### [5] [Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.01304)
*Yu Zeng,Wenxuan Huang,Shiting Huang,Xikun Bao,Yukun Qi,Yiming Zhao,Qiuchen Wang,Lin Chen,Zehui Chen,Huaian Chen,Wanli Ouyang,Feng Zhao*

Main category: cs.AI

TL;DR: 现有视觉语言模型感知和推理能力有限，提出AGILE方法，实验证明其能提升性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大视觉语言模型基础感知和推理能力有限，且高质量多模态数据稀缺可扩展性差。

Method: 提出AGILE，将拼图解决过程定义为交互过程，模型根据当前状态生成可执行代码行动，环境提供反馈，通过观察 - 交互迭代循环提升能力。

Result: AGILE大幅提升拼图任务性能，在9个通用视觉任务上有较强泛化能力，平均提升3.1%。

Conclusion: 该工作为多模态模型推理和泛化提供新途径，为多模态强化学习数据稀缺问题提供高效可扩展解决方案。

Abstract: Although current large Vision-Language Models (VLMs) have advanced in
multimodal understanding and reasoning, their fundamental perceptual and
reasoning abilities remain limited. Specifically, even on simple jigsaw tasks,
existing VLMs perform near randomly, revealing deficiencies in core perception
and reasoning capabilities. While high-quality vision-language data can enhance
these capabilities, its scarcity and limited scalability impose significant
constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction
Learning for Enhancing visual perception and reasoning in VLMs. AGILE
formulates jigsaw solving as an interactive process, enabling the model to
progressively engage with the environment. At each step, the model generates
executable code to perform an action based on the current state, while the
environment provides fine-grained visual feedback to guide task completion.
Through this iterative cycle of observation and interaction, the model
incrementally improves its perceptual and reasoning capabilities via
exploration and feedback. Experimental results show that AGILE not only
substantially boosts performance on jigsaw tasks of varying complexity (e.g.,
increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also
demonstrates strong generalization across 9 general vision tasks, achieving an
average improvement of 3.1%. These results indicate notable enhancements in
both perceptual and reasoning abilities. This work opens a new avenue for
advancing reasoning and generalization in multimodal models and provides an
efficient, scalable solution to the scarcity of multimodal reinforcement
learning data. The code and datasets is available at
https://github.com/yuzeng0-0/AGILE .

</details>


### [6] [Aristotle: IMO-level Automated Theorem Proving](https://arxiv.org/abs/2510.01346)
*Tudor Achim,Alex Best,Kevin Der,Mathïs Fédérico,Sergei Gukov,Daniel Halpern-Leister,Kirsten Henningsgard,Yury Kudryashov,Alexander Meiburg,Martin Michelsen,Riley Patterson,Eric Rodriguez,Laura Scharff,Vikram Shanker,Vladmir Sicca,Hari Sowrirajan,Aidan Swope,Matyas Tamas,Vlad Tenev,Jonathan Thomm,Harold Williams,Lawrence Wu*

Main category: cs.AI

TL;DR: 介绍AI系统Aristotle，结合形式验证与非正式推理，在2025年国际数学奥林匹克问题上获金牌水平表现，含三组件且在自动定理证明有良好性能。


<details>
  <summary>Details</summary>
Motivation: 开发能结合形式验证与非正式推理、在数学问题上有优秀表现的AI系统。

Method: 将Lean证明搜索系统、生成和形式化引理的非正式推理系统以及专用几何求解器集成。

Result: 在2025年国际数学奥林匹克问题上达到金牌水平表现，在自动定理证明中有先进性能和良好扩展属性。

Conclusion: Aristotle系统在自动定理证明方面展现出良好性能。

Abstract: We introduce Aristotle, an AI system that combines formal verification with
informal reasoning, achieving gold-medal-equivalent performance on the 2025
International Mathematical Olympiad problems. Aristotle integrates three main
components: a Lean proof search system, an informal reasoning system that
generates and formalizes lemmas, and a dedicated geometry solver. Our system
demonstrates state-of-the-art performance with favorable scaling properties for
automated theorem proving.

</details>


### [7] [MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments](https://arxiv.org/abs/2510.01353)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Anand Kannappan,Rebecca Qian,Peng Wang*

Main category: cs.AI

TL;DR: 本文介绍了用于评估多平台代理环境中长期记忆和状态跟踪的MEMTRACK基准，通过实验揭示模型在利用记忆方面的挑战，为增强记忆代理评估研究提供框架。


<details>
  <summary>Details</summary>
Motivation: 现有上下文和记忆基准测试多关注对话场景，而评估动态企业环境中的记忆能力对其有效应用至关重要。

Method: 引入MEMTRACK基准，模拟真实组织工作流程，结合多平台异步事件；通过人工专家设计和基于代理的合成来策划数据集；引入正确性、效率和冗余性等相关指标。

Result: 实验显示最先进的大语言模型和记忆后端在长时记忆利用、处理跨平台依赖和解决矛盾方面存在挑战，表现最好的GPT - 5在MEMTRACK上正确性得分仅60%。

Conclusion: 为增强记忆代理的评估研究提供了可扩展框架，推动复杂组织环境下多代理、多平台记忆基准测试。

Abstract: Recent works on context and memory benchmarking have primarily focused on
conversational instances but the need for evaluating memory in dynamic
enterprise environments is crucial for its effective application. We introduce
MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking
in multi-platform agent environments. MEMTRACK models realistic organizational
workflows by integrating asynchronous events across multiple communication and
productivity platforms such as Slack, Linear and Git. Each benchmark instance
provides a chronologically platform-interleaved timeline, with noisy,
conflicting, cross-referring information as well as potential
codebase/file-system comprehension and exploration. Consequently, our benchmark
tests memory capabilities such as acquistion, selection and conflict
resolution. We curate the MEMTRACK dataset through both manual expert driven
design and scalable agent based synthesis, generating ecologically valid
scenarios grounded in real world software development processes. We introduce
pertinent metrics for Correctness, Efficiency, and Redundancy that capture the
effectiveness of memory mechanisms beyond simple QA performance. Experiments
across SoTA LLMs and memory backends reveal challenges in utilizing memory
across long horizons, handling cross-platform dependencies, and resolving
contradictions. Notably, the best performing GPT-5 model only achieves a 60\%
Correctness score on MEMTRACK. This work provides an extensible framework for
advancing evaluation research for memory-augmented agents, beyond existing
focus on conversational setups, and sets the stage for multi-agent,
multi-platform memory benchmarking in complex organizational settings

</details>


### [8] [Retrieval-Augmented Framework for LLM-Based Clinical Decision Support](https://arxiv.org/abs/2510.01363)
*Leon Garza,Anantaa Kotal,Michael A. Grasso,Emre Umucu*

Main category: cs.AI

TL;DR: 本文提出由大语言模型驱动的临床决策支持系统辅助处方医生，介绍系统技术组件，初步评估显示该工具在适当约束和验证下可为处方工作流提供有价值决策支持。


<details>
  <summary>Details</summary>
Motivation: 临床决策复杂性增加和电子健康记录快速扩展带来机遇与挑战，需要系统辅助处方医生。

Method: 系统分析历史电子健康记录数据生成治疗建议，集成自然语言处理与结构化临床输入，采用检索增强生成（RAG）管道。

Result: 初步评估表明基于大语言模型的工具在适当约束和严格验证下可为处方工作流提供有价值的决策支持。

Conclusion: 这是将生成式AI集成到现实临床决策的初步尝试，强调透明、安全和与既定实践保持一致。

Abstract: The increasing complexity of clinical decision-making, alongside the rapid
expansion of electronic health records (EHR), presents both opportunities and
challenges for delivering data-informed care. This paper proposes a clinical
decision support system powered by Large Language Models (LLMs) to assist
prescribing clinicians. The system generates therapeutic suggestions by
analyzing historical EHR data, including patient demographics, presenting
complaints, clinical symptoms, diagnostic information, and treatment histories.
The framework integrates natural language processing with structured clinical
inputs to produce contextually relevant recommendations. Rather than replacing
clinician judgment, it is designed to augment decision-making by retrieving and
synthesizing precedent cases with comparable characteristics, drawing on local
datasets or federated sources where applicable. At its core, the system employs
a retrieval-augmented generation (RAG) pipeline that harmonizes unstructured
narratives and codified data to support LLM-based inference. We outline the
system's technical components, including representation representation
alignment and generation strategies. Preliminary evaluations, conducted with
de-identified and synthetic clinical datasets, examine the clinical
plausibility and consistency of the model's outputs. Early findings suggest
that LLM-based tools may provide valuable decision support in prescribing
workflows when appropriately constrained and rigorously validated. This work
represents an initial step toward integration of generative AI into real-world
clinical decision-making with an emphasis on transparency, safety, and
alignment with established practices.

</details>


### [9] [Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort](https://arxiv.org/abs/2510.01367)
*Xinpeng Wang,Nitish Joshi,Barbara Plank,Rico Angell,He He*

Main category: cs.AI

TL;DR: 提出TRACE方法检测隐式奖励破解，在数学推理和编码任务中效果好，还能发现未知漏洞。


<details>
  <summary>Details</summary>
Motivation: 奖励破解行为对推理模型构成重大威胁，现有方法难以检测隐式奖励破解。

Method: 提出TRACE方法，通过逐步截断模型的思维链，测量不同截断长度下验证通过率，量化模型推理所需的“努力”程度。

Result: TRACE在数学推理中比72B思维链监控器提升超65%，在编码中比32B监控器提升超30%，还能在训练中发现未知漏洞。

Conclusion: TRACE提供了一种可扩展的无监督监督方法，在现有监控方法无效的情况下有效。

Abstract: Reward hacking, where a reasoning model exploits loopholes in a reward
function to achieve high rewards without solving the intended task, poses a
significant threat. This behavior may be explicit, i.e. verbalized in the
model's chain-of-thought (CoT), or implicit, where the CoT appears benign thus
bypasses CoT monitors. To detect implicit reward hacking, we propose TRACE
(Truncated Reasoning AUC Evaluation). Our key observation is that hacking
occurs when exploiting the loophole is easier than solving the actual task.
This means that the model is using less `effort' than required to achieve high
reward. TRACE quantifies effort by measuring how early a model's reasoning
becomes sufficient to pass a verifier. We progressively truncate a model's CoT
at various lengths, force the model to answer, and measure the verifier-passing
rate at each cutoff. A hacking model, which takes a shortcut, will achieve a
high passing rate with only a small fraction of its CoT, yielding a large area
under the accuracy-vs-length curve. TRACE achieves over 65% gains over our
strongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B
monitor in coding. We further show that TRACE can discover unknown loopholes
during training. Overall, TRACE offers a scalable unsupervised approach for
oversight where current monitoring methods prove ineffective.

</details>


### [10] [Fine-tuning with RAG for Improving LLM Learning of New Skills](https://arxiv.org/abs/2510.01375)
*Humaid Ibrahim,Nikolai Rozanov,Marek Rei*

Main category: cs.AI

TL;DR: 本文提出通过蒸馏将推理时检索转化为学习能力的方法，在两个基准测试中，蒸馏后的学生模型表现优于基线模型，且减少了token使用量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理处理多步任务时存在常见失败情况，检索增强生成有维护数据库和计算开销问题。

Method: 从代理失败中提取紧凑可复用提示，在回合开始时通过一次性检索生成改进的教师轨迹，训练去除提示字符串的学生模型。

Result: 在ALFWorld和WebShop基准测试中，蒸馏后的学生模型表现更好，分别达到91%成功率和72分，且减少10 - 60%的token使用量。

Conclusion: 该方法可通过有针对性的微调有效内化检索优势，无需永久运行时依赖。

Abstract: Large language model (LLM) agents deployed for multi-step tasks frequently
fail in predictable ways: attempting actions with unmet preconditions, issuing
redundant commands, or mishandling environment constraints. While
retrieval-augmented generation (RAG) can improve performance by providing
runtime guidance, it requires maintaining external knowledge databases and adds
computational overhead at every deployment. We propose a simple pipeline that
converts inference-time retrieval into learned competence through distillation.
Our approach: (1) extracts compact, reusable hints from agent failures, (2)
uses these hints to generate improved teacher trajectories via one-shot
retrieval at episode start, and (3) trains student models on these trajectories
with hint strings removed, forcing internalization rather than memorization.
Across two interactive benchmarks, ALFWorld (household tasks) and WebShop
(online shopping), distilled students consistently outperform baseline agents,
achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving
WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens
than retrieval-augmented teachers depending on the environment. The approach
generalizes across model scales (7B/14B parameters) and agent architectures
(ReAct/StateAct), demonstrating that retrieval benefits can be effectively
internalized through targeted fine-tuning without permanent runtime
dependencies.

</details>


### [11] [Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents](https://arxiv.org/abs/2510.01398)
*Yang Liu,Zaid Abulawi,Abhiram Garimidi,Doyeong Lim*

Main category: cs.AI

TL;DR: 本文提出用大语言模型（LLM）代理自动化数据驱动建模和分析的管道，评估两种框架，用临界热通量预测基准验证，结果显示模型有潜力减少人力并达良好性能。


<details>
  <summary>Details</summary>
Motivation: 现代工程依赖大量数据，传统数据驱动方法需大量人工干预，扩展性和泛化能力有限，因此需开发新方法。

Method: 提出利用LLM代理的创新管道，评估多代理系统和基于ReAct范式的单代理系统，两框架可自动处理数据预处理、神经网络开发等任务。

Result: 用约25000个实验数据点验证，LLM代理开发的模型超传统查找表，预测精度和不确定性量化与人类专家开发的模型相当。

Conclusion: 基于LLM的代理有自动化复杂工程建模任务的巨大潜力，可大幅减少人力，达到或超越现有预测性能标准。

Abstract: Modern engineering increasingly relies on vast datasets generated by
experiments and simulations, driving a growing demand for efficient, reliable,
and broadly applicable modeling strategies. There is also heightened interest
in developing data-driven approaches, particularly neural network models, for
effective prediction and analysis of scientific datasets. Traditional
data-driven methods frequently involve extensive manual intervention, limiting
their ability to scale effectively and generalize to diverse applications. In
this study, we propose an innovative pipeline utilizing Large Language Model
(LLM) agents to automate data-driven modeling and analysis, with a particular
emphasis on regression tasks. We evaluate two LLM-agent frameworks: a
multi-agent system featuring specialized collaborative agents, and a
single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both
frameworks autonomously handle data preprocessing, neural network development,
training, hyperparameter optimization, and uncertainty quantification (UQ). We
validate our approach using a critical heat flux (CHF) prediction benchmark,
involving approximately 25,000 experimental data points from the OECD/NEA
benchmark dataset. Results indicate that our LLM-agent-developed model
surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ
on par with state-of-the-art Bayesian optimized deep neural network models
developed by human experts. These outcomes underscore the significant potential
of LLM-based agents to automate complex engineering modeling tasks, greatly
reducing human workload while meeting or exceeding existing standards of
predictive performance.

</details>


### [12] [OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models](https://arxiv.org/abs/2510.01409)
*Luca Cotti,Idilio Drago,Anisa Rula,Devis Bianchini,Federico Cerutti*

Main category: cs.AI

TL;DR: 介绍OntoLogX将原始日志转换为基于本体的知识图谱以提取网络威胁情报，评估显示其有效。


<details>
  <summary>Details</summary>
Motivation: 系统日志存在缺乏结构、语义不一致和碎片化问题，需方法将其数据转化为连贯可互操作的表示以提取可用的网络威胁情报。

Method: 引入OntoLogX，结合轻量级日志本体、检索增强生成和迭代校正步骤，将原始日志转化为知识图谱，聚合图谱到会话并利用大语言模型预测MITRE ATT&CK策略。

Result: 在公共基准和真实蜜罐数据集上评估，展示了在多个知识图谱后端上的稳健图谱生成，以及将对抗活动准确映射到ATT&CK策略。

Conclusion: 强调了检索和校正对精确率和召回率的好处、面向代码的模型在结构化日志分析中的有效性，以及基于本体的表示对提取可用网络威胁情报的价值。

Abstract: System logs represent a valuable source of Cyber Threat Intelligence (CTI),
capturing attacker behaviors, exploited vulnerabilities, and traces of
malicious activity. Yet their utility is often limited by lack of structure,
semantic inconsistency, and fragmentation across devices and sessions.
Extracting actionable CTI from logs therefore requires approaches that can
reconcile noisy, heterogeneous data into coherent and interoperable
representations. We introduce OntoLogX, an autonomous Artificial Intelligence
(AI) agent that leverages Large Language Models (LLMs) to transform raw logs
into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a
lightweight log ontology with Retrieval Augmented Generation (RAG) and
iterative correction steps, ensuring that generated KGs are syntactically and
semantically valid. Beyond event-level analysis, the system aggregates KGs into
sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level
log evidence to higher-level adversarial objectives. We evaluate OntoLogX on
both logs from a public benchmark and a real-world honeypot dataset,
demonstrating robust KG generation across multiple KGs backends and accurate
mapping of adversarial activity to ATT&CK tactics. Results highlight the
benefits of retrieval and correction for precision and recall, the
effectiveness of code-oriented models in structured log analysis, and the value
of ontology-grounded representations for actionable CTI extraction.

</details>


### [13] [A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining](https://arxiv.org/abs/2510.01427)
*Sipeng Zhang,Longfei Yun,Zilong Wang,Jingbo Shang,Letian Peng*

Main category: cs.AI

TL;DR: 介绍Falconer框架用于知识挖掘，结合大模型推理和轻量级代理模型，实验表明其能降低成本、加速挖掘。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署成本高，传统分类器和提取器无法泛化到新任务，需要高效可扩展的知识挖掘方法。

Method: 引入Falconer框架，让大语言模型作为规划器和注释器，结合轻量级代理模型，统一分类和提取为两个原子操作，构建新基准评估一致性。

Result: Falconer在指令跟随准确性上接近最先进大语言模型，降低推理成本达90%，加速大规模知识挖掘超20倍。

Conclusion: Falconer为深度研究提供了高效可扩展的基础。

Abstract: At the core of Deep Research is knowledge mining, the task of extracting
structured information from massive unstructured text in response to user
instructions. Large language models (LLMs) excel at interpreting such
instructions but are prohibitively expensive to deploy at scale, while
traditional pipelines of classifiers and extractors remain efficient yet
brittle and unable to generalize to new tasks. We introduce Falconer, a
collaborative framework that combines the agentic reasoning of LLMs with
lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act
as planners, decomposing user instructions into executable pipelines, and as
annotators, generating supervision to train small proxies. The framework
unifies classification and extraction into two atomic operations, get label and
get span, enabling a single instruction-following model to replace multiple
task-specific components. To evaluate the consistency between proxy models
incubated by Falconer and annotations provided by humans and large models, we
construct new benchmarks covering both planning and end-to-end execution.
Experiments show that Falconer closely matches state-of-the-art LLMs in
instruction-following accuracy while reducing inference cost by up to 90% and
accelerating large-scale knowledge mining by more than 20x, offering an
efficient and scalable foundation for Deep Research.

</details>


### [14] [On the Role of Domain Experts in Creating Effective Tutoring Systems](https://arxiv.org/abs/2510.01432)
*Sarath Sreedharan,Kelsey Sikes,Nathaniel Blanchard,Lisa Mason,Nikhil Krishnaswamy,Jill Zarestky*

Main category: cs.AI

TL;DR: 论文强调领域专家提供的高度精炼知识在创建教育系统中的作用，介绍两种利用该知识创建教育系统的方法并通过案例说明其重要性。


<details>
  <summary>Details</summary>
Motivation: AI教育界常忽视领域专家高度精炼知识在创建有效辅导系统中的作用，因此强调此话题。

Method: 一是利用可解释AI技术结合专家规则自动创建课程；二是用专家指定的学习目标概念课程来开发自适应辅导系统。最后通过传粉者识别辅导系统案例研究凸显方法重要性。

Result: 未提及具体结果。

Conclusion: 领域专家的高度精炼知识对创建新颖有效的教育系统有重要作用。

Abstract: The role that highly curated knowledge, provided by domain experts, could
play in creating effective tutoring systems is often overlooked within the AI
for education community. In this paper, we highlight this topic by discussing
two ways such highly curated expert knowledge could help in creating novel
educational systems. First, we will look at how one could use explainable AI
(XAI) techniques to automatically create lessons. Most existing XAI methods are
primarily aimed at debugging AI systems. However, we will discuss how one could
use expert specified rules about solving specific problems along with novel XAI
techniques to automatically generate lessons that could be provided to
learners. Secondly, we will see how an expert specified curriculum for learning
a target concept can help develop adaptive tutoring systems, that can not only
provide a better learning experience, but could also allow us to use more
efficient algorithms to create these systems. Finally, we will highlight the
importance of such methods using a case study of creating a tutoring system for
pollinator identification, where such knowledge could easily be elicited from
experts.

</details>


### [15] [VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning](https://arxiv.org/abs/2510.01444)
*Rui Liu,Dian Yu,Tong Zheng,Runpeng Dai,Zongxia Li,Wenhao Yu,Zhenwen Liang,Linfeng Song,Haitao Mi,Pratap Tokekar,Dong Yu*

Main category: cs.AI

TL;DR: 提出VOGUE方法解决多模态大语言模型强化学习探索问题，在多个基准测试提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型的强化学习存在探索问题，现有方法忽视视觉输入的不确定性。

Method: 将探索从输出（文本）空间转移到输入（视觉）空间，把图像视为随机上下文，用对称KL散度量化策略对视觉扰动的敏感性，结合多种奖励和采样策略平衡探索与利用。

Result: 在GRPO上实现，在视觉数学和通用领域推理基准测试中提升了pass@1和pass@4准确率，缓解探索衰减。

Conclusion: 基于视觉输入固有不确定性进行探索是提高多模态推理的有效策略。

Abstract: Reinforcement learning with verifiable rewards (RLVR) improves reasoning in
large language models (LLMs) but struggles with exploration, an issue that
still persists for multimodal LLMs (MLLMs). Current methods treat the visual
input as a fixed, deterministic condition, overlooking a critical source of
ambiguity and struggling to build policies robust to plausible visual
variations. We introduce $\textbf{VOGUE (Visual Uncertainty Guided
Exploration)}$, a novel method that shifts exploration from the output (text)
to the input (visual) space. By treating the image as a stochastic context,
VOGUE quantifies the policy's sensitivity to visual perturbations using the
symmetric KL divergence between a "raw" and "noisy" branch, creating a direct
signal for uncertainty-aware exploration. This signal shapes the learning
objective via an uncertainty-proportional bonus, which, combined with a
token-entropy bonus and an annealed sampling schedule, effectively balances
exploration and exploitation. Implemented within GRPO on two model scales
(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three
visual math benchmarks and 3.7% on three general-domain reasoning benchmarks,
while simultaneously increasing pass@4 performance and mitigating the
exploration decay commonly observed in RL fine-tuning. Our work shows that
grounding exploration in the inherent uncertainty of visual inputs is an
effective strategy for improving multimodal reasoning.

</details>


### [16] [AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance](https://arxiv.org/abs/2510.01474)
*Bill Marino,Rosco Hunter,Zubair Jamali,Marinos Emmanouil Kalpakos,Mudra Kashyap,Isaiah Hinton,Alexa Hanson,Maahum Nazir,Christoph Schnabl,Felix Steffek,Hongkai Wen,Nicholas D. Lane*

Main category: cs.AI

TL;DR: 本文引入AIReg - Bench基准数据集，用于测试大语言模型评估AI系统是否符合欧盟AI法案的能力，介绍了数据集创建过程及意义，代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对大语言模型评估AI系统是否符合AI法规性能的基准测试，需填补这一空白。

Method: 通过两步创建数据集：一是用精心设计的指令提示大语言模型生成120个技术文档摘录；二是法律专家审查并标注每个样本是否违反欧盟AI法案的特定条款。

Result: 生成了AIReg - Bench数据集，对前沿大语言模型能否重现专家的合规标签进行了评估。

Conclusion: 该数据集为理解基于大语言模型的AI法规合规评估工具的机遇和局限提供起点，为后续大语言模型提供了比较基准。

Abstract: As governments move to regulate AI, there is growing interest in using Large
Language Models (LLMs) to assess whether or not an AI system complies with a
given AI Regulation (AIR). However, there is presently no way to benchmark the
performance of LLMs at this task. To fill this void, we introduce AIReg-Bench:
the first benchmark dataset designed to test how well LLMs can assess
compliance with the EU AI Act (AIA). We created this dataset through a two-step
process: (1) by prompting an LLM with carefully structured instructions, we
generated 120 technical documentation excerpts (samples), each depicting a
fictional, albeit plausible, AI system - of the kind an AI provider might
produce to demonstrate their compliance with AIR; (2) legal experts then
reviewed and annotated each sample to indicate whether, and in what way, the AI
system described therein violates specific Articles of the AIA. The resulting
dataset, together with our evaluation of whether frontier LLMs can reproduce
the experts' compliance labels, provides a starting point to understand the
opportunities and limitations of LLM-based AIR compliance assessment tools and
establishes a benchmark against which subsequent LLMs can be compared. The
dataset and evaluation code are available at
https://github.com/camlsys/aireg-bench.

</details>


### [17] [Lateral Tree-of-Thoughts Surpasses ToT by Incorporating Logically-Consistent, Low-Utility Candidates](https://arxiv.org/abs/2510.01500)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: 针对Tree - of - Thoughts式搜索在大测试时间预算下的问题，提出Lateral Tree - of - Thoughts (LToT)方法，可解决饱和与短视问题，目前实证评估待完成。


<details>
  <summary>Details</summary>
Motivation: 标准Tree - of - Thoughts式搜索在大测试时间预算下存在广度饱和和深度短视问题，需要改进。

Method: 提出LToT，分离效用和逻辑一致性，将前沿分为主线和支线，通过Lateral Racing with Short - Circuit (LR - SC)探索支线，主线保持狭窄。

Result: 证明支线成本为伪线性，对比无上限主线的指数增长，但实证评估待完成。

Conclusion: LToT能在大测试时间预算下实现有原则的多样性，缓解饱和和短视问题，且不增加计算量。

Abstract: Modern deployments increasingly allocate large test-time compute (thousands
of tokens or many node expansions) to boost reliability. Under such budgets,
standard Tree-of-Thoughts-style search exhibits two pathologies: breadth
saturation (additional samples mostly produce near-duplicates, so width stops
growing) and depth myopia (noisy short-horizon utilities prune branches whose
payoff appears after a few more steps). We propose Lateral Tree-of-Thoughts
(LToT), a drop-in controller that separates utility from logical consistency
and treats low-utility but consistent candidates as assets rather than waste.
The frontier is split into mainlines (high-utility candidates used for
exploitation) and laterals (consistent, initially low-utility candidates that
receive short, cheap probes before judgment). LToT explores laterals via
Lateral Racing with Short-Circuit (LR--SC): a capped successive-halving race
that spreads tiny probes across a very wide lateral set, uses width-aware
thresholds with repeat-to-confirm, and immediately promotes a branch once its
envelope clears the mainline bar; mainlines are kept intentionally narrow so
surplus compute is invested where width is cheap. We prove a pseudolinear
lateral cost $\Theta(N_0 \log_{\eta} N_0)$ with logarithmically many rungs
(initial lateral width $N_0$; culling factor $\eta>1$), in contrast to the
exponential growth of uncapped mainlines. Empirical evaluations on benchmark
tasks are in preparation and will be added in a future revision. In short, LToT
turns large test-time budgets into principled diversity while preserving
promotion discipline, mitigating saturation and myopia without inflating
compute.

</details>


### [18] [Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation](https://arxiv.org/abs/2510.01528)
*Daniel Zhao,Abhilash Shankarampeta,Lanxiang Hu,Tajana Rosing,Hao Zhang*

Main category: cs.AI

TL;DR: 提出利用稀疏自编码器和聚类技术分析大语言模型内部标记表示，指导数学推理任务生成，强调平衡开发与探索的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在数学推理任务中如何有效生成的问题，提升推理准确性。

Method: 先训练稀疏自编码器生成训练标记的稀疏向量表示，再用k - means聚类构建图，定义基于边权重的奖励函数，同时从聚类测量生成多样性。

Result: 发现平衡开发与探索对数学推理任务的高准确性至关重要，稀疏自编码器可作为可扩展奖励模型指导生成。

Conclusion: 平衡开发与探索能防止极端行为，促进大语言模型更高质量的推理过程。

Abstract: We propose a novel method that leverages sparse autoencoders (SAEs) and
clustering techniques to analyze the internal token representations of large
language models (LLMs) and guide generations in mathematical reasoning tasks.
Our approach first trains an SAE to generate sparse vector representations for
training tokens, then applies k-means clustering to construct a graph where
vertices represent token clusters and weighted edges capture sequential token
transitions. Using this graph, we define an edge-weight based reward function
to quantify adherence to established reasoning traces, thereby identifying
exploitative reasoning trajectories. Additionally, we measure generation
diversity from clustering to assess the extent of exploration. Our findings
indicate that balancing both exploitation and exploration is crucial for
achieving high accuracy in mathematical reasoning tasks. During generation, the
SAE can serve as a scalable reward model to guide generations, ensuring a
balanced trade-off between exploitation and exploration. This prevents extreme
behaviors in either direction, ultimately fostering a higher-quality reasoning
process in LLMs.

</details>


### [19] [LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning](https://arxiv.org/abs/2510.01530)
*Navapat Nananukul,Yue Zhang,Ryan Lee,Eric Boxer,Jonathan May,Vibhav Giridhar Gogate,Jay Pujara,Mayank Kejriwal*

Main category: cs.AI

TL;DR: 提出名为LOGicalThought (LogT)的神经符号架构，结合LLM和逻辑语言处理高保证文本推理，在多基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 高保证推理在关键领域需准确、可验证结论，依赖可废止逻辑，而LLM在高保证文本推理能力不足，存在特定逻辑结构推理挑战。

Method: 提出LogT架构，结合高级逻辑语言和推理器与LLM，构建双符号图上下文和基于逻辑的上下文，将长指南推理问题转化为紧凑的基础评估。

Result: 在四个多领域基准测试中，LogT使所有LLM的整体性能提高11.84%，在否定、蕴含和可废止推理三种推理模式上性能显著提升。

Conclusion: LogT架构在高保证文本推理任务中表现出色，能有效提升推理性能。

Abstract: High-assurance reasoning, particularly in critical domains such as law and
medicine, requires conclusions that are accurate, verifiable, and explicitly
grounded in evidence. This reasoning relies on premises codified from rules,
statutes, and contracts, inherently involving defeasible or non-monotonic logic
due to numerous exceptions, where the introduction of a single fact can
invalidate general rules, posing significant challenges. While large language
models (LLMs) excel at processing natural language, their capabilities in
standard inference tasks do not translate to the rigorous reasoning required
over high-assurance text guidelines. Core reasoning challenges within such
texts often manifest specific logical structures involving negation,
implication, and, most critically, defeasible rules and exceptions. In this
paper, we propose a novel neurosymbolically-grounded architecture called
LOGicalThought (LogT) that uses an advanced logical language and reasoner in
conjunction with an LLM to construct a dual symbolic graph context and
logic-based context. These two context representations transform the problem
from inference over long-form guidelines into a compact grounded evaluation.
Evaluated on four multi-domain benchmarks against four baselines, LogT improves
overall performance by 11.84% across all LLMs. Performance improves
significantly across all three modes of reasoning: by up to +10.2% on negation,
+13.2% on implication, and +5.5% on defeasible reasoning compared to the
strongest baseline.

</details>


### [20] [Information Seeking for Robust Decision Making under Partial Observability](https://arxiv.org/abs/2510.01531)
*Djengo Cyun-Jyun Fang,Tsung-Wei Ke*

Main category: cs.AI

TL;DR: 介绍InfoSeeker框架，能结合任务规划与信息搜索，在不确定环境做决策，实验显示性能优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型规划代理常忽略内部动态与实际环境差异，需解决在观测和环境动态不确定下的决策问题。

Method: 引入InfoSeeker框架，提示大语言模型主动收集信息，规划行动以验证理解、检测环境变化或测试假设。引入新基准套件评估。

Result: InfoSeeker比先前方法绝对性能提升74%，不牺牲样本效率，能跨大语言模型泛化，在既定基准测试中表现优于基线。

Conclusion: 在部分可观测环境中，紧密结合规划和信息搜索对实现稳健行为至关重要。

Abstract: Explicit information seeking is essential to human problem-solving in
practical environments characterized by incomplete information and noisy
dynamics. When the true environmental state is not directly observable, humans
seek information to update their internal dynamics and inform future
decision-making. Although existing Large Language Model (LLM) planning agents
have addressed observational uncertainty, they often overlook discrepancies
between their internal dynamics and the actual environment. We introduce
Information Seeking Decision Planner (InfoSeeker), an LLM decision-making
framework that integrates task-oriented planning with information seeking to
align internal dynamics and make optimal decisions under uncertainty in both
agent observations and environmental dynamics. InfoSeeker prompts an LLM to
actively gather information by planning actions to validate its understanding,
detect environmental changes, or test hypotheses before generating or revising
task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark
suite featuring partially observable environments with incomplete observations
and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%
absolute performance gain over prior methods without sacrificing sample
efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms
baselines on established benchmarks such as robotic manipulation and web
navigation. These findings underscore the importance of tightly integrating
planning and information seeking for robust behavior in partially observable
environments. The project page is available at https://infoseekerllm.github.io

</details>


### [21] [Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models](https://arxiv.org/abs/2510.01544)
*Shaoan Xie,Lingjing Kong,Xiangchen Song,Xinshuai Dong,Guangyi Chen,Eric P. Xing,Kun Zhang*

Main category: cs.AI

TL;DR: 本文聚焦扩散语言模型复杂推理训练难题，提出理论框架和新算法，实验表明新算法提升推理基准性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决扩散语言模型在复杂推理训练中的挑战，现有强化学习方法基于结果奖励，易强化错误推理路径。

Method: 提出将复杂问题解决形式化为分层选择过程的理论框架，识别现有方法的核心缺陷，引入与潜在推理层次对齐的新RL算法Step - Aware Policy Optimization (SAPO)，使用基于过程的奖励函数。

Result: 新方法在具有挑战性的推理基准上显著提高性能，增强生成过程的可解释性。

Conclusion: 基于理论框架设计的新算法能有效解决扩散语言模型复杂推理训练问题。

Abstract: Diffusion language models (dLLMs) offer a promising, non-autoregressive
paradigm for text generation, yet training them for complex reasoning remains a
key challenge. Current reinforcement learning approaches often rely on sparse,
outcome-based rewards, which can reinforce flawed reasoning paths that lead to
coincidentally correct answers. We argue that this stems from a fundamental
mismatch with the natural structure of reasoning. We first propose a
theoretical framework that formalizes complex problem solving as a hierarchical
selection process, where an intractable global constraint is decomposed into a
series of simpler, localized logical steps. This framework provides a
principled foundation for algorithm design, including theoretical insights into
the identifiability of this latent reasoning structure. Motivated by this
theory, we identify unstructured refinement -- a failure mode where a model's
iterative steps do not contribute meaningfully to the solution -- as a core
deficiency in existing methods. We then introduce Step-Aware Policy
Optimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising
process with the latent reasoning hierarchy. By using a process-based reward
function that encourages incremental progress, SAPO guides the model to learn
structured, coherent reasoning paths. Our empirical results show that this
principled approach significantly improves performance on challenging reasoning
benchmarks and enhances the interpretability of the generation process.

</details>


### [22] [InvThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/abs/2510.01569)
*Yubin Kim,Taehan Kim,Eugene Park,Chunjong Park,Cynthia Breazeal,Daniel McDuff,Hae Won Park*

Main category: cs.AI

TL;DR: 提出InvThink方法让大语言模型具备逆向思维，可在生成回复前推理失败模式，在多方面取得成果，表明逆向推理是实现更安全强大语言模型的途径。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法直接优化安全回复，而希望让大语言模型在生成回复前通过推理失败模式来实现安全。

Method: 指导模型枚举潜在危害、分析后果并生成主动避免风险的安全输出，通过监督微调、强化学习在三个大语言模型族中实现。

Result: 与现有安全方法相比，安全改进随模型大小的扩展性更强；减轻安全税，保留标准基准上的通用推理能力；在高风险领域表现出色，有害回复最多降低15.7%。

Conclusion: 逆向推理为构建更安全、更强大的语言模型提供了可扩展和可推广的路径。

Abstract: We present InvThink, a simple yet powerful approach that gives large language
models (LLMs) the capability of inverse thinking: reasoning through failure
modes before generating responses. Unlike existing safety alignment methods
that optimize directly for safe response, InvThink instructs models to 1)
enumerate potential harms, 2) analyze their consequences, and 3) generate safe
outputs that proactively avoid these risks. Our method reveals three key
findings: (i) safety improvements show stronger scaling with model size
compared to existing safety methods. (ii) InvThink mitigates safety tax; by
training models to systematically consider failure modes, it preserves general
reasoning capabilities on standard benchmarks. (iii) beyond general safety
tasks, InvThink excels in high-stakes domains including external-facing
(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,
achieving up to 15.7% reduction in harmful responses compared to baseline
methods like SafetyPrompt. We further implement InvThink via supervised
fine-tuning, and reinforcement learning across three LLM families. These
results suggest that inverse reasoning provides a scalable and generalizable
path toward safer, more capable language models.

</details>


### [23] [AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.01586)
*Zhenyu Pan,Yiting Zhang,Zhuo Liu,Yolo Yunlong Tang,Zeliang Zhang,Haozheng Luo,Yuwei Han,Jianshu Zhang,Dennis Wu,Hong-Yu Chen,Haoran Lu,Haoyang Fang,Manling Li,Chenliang Xu,Philip S. Yu,Han Liu*

Main category: cs.AI

TL;DR: 现有基于大语言模型的多智能体系统存在安全问题，传统防御方法有不足。提出AdvEvo - MARL框架，在攻击场景中表现好，能兼顾安全与效用。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统面临越狱、提示注入等安全问题，而现有的两种防御路线（自我验证和外部防护模块）存在性能不佳、增加系统开销等问题。

Method: 提出AdvEvo - MARL协同进化多智能体强化学习框架，在对抗学习环境中联合优化攻击者和防御者，引入公共基线进行优势估计。

Result: 在代表性攻击场景中，AdvEvo - MARL使攻击成功率低于20%，而基线高达38.33%，同时保持甚至提高任务准确率。

Conclusion: 无需额外防护智能体或增加系统开销，AdvEvo - MARL可同时提升系统的安全性和实用性。

Abstract: LLM-based multi-agent systems excel at planning, tool use, and role
coordination, but their openness and interaction complexity also expose them to
jailbreak, prompt-injection, and adversarial collaboration. Existing defenses
fall into two lines: (i) self-verification that asks each agent to pre-filter
unsafe instructions before execution, and (ii) external guard modules that
police behaviors. The former often underperforms because a standalone agent
lacks sufficient capacity to detect cross-agent unsafe chains and
delegation-induced risks; the latter increases system overhead and creates a
single-point-of-failure-once compromised, system-wide safety collapses, and
adding more guards worsens cost and complexity. To solve these challenges, we
propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning
framework that internalizes safety into task agents. Rather than relying on
external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize
evolving jailbreak prompts) and defenders (task agents trained to both
accomplish their duties and resist attacks) in adversarial learning
environments. To stabilize learning and foster cooperation, we introduce a
public baseline for advantage estimation: agents within the same functional
group share a group-level mean-return baseline, enabling lower-variance updates
and stronger intra-group coordination. Across representative attack scenarios,
AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas
baselines reach up to 38.33%, while preserving-and sometimes improving-task
accuracy (up to +3.67% on reasoning tasks). These results show that safety and
utility can be jointly improved without relying on extra guard agents or added
system overhead.

</details>


### [24] [AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative Recommendation with Adaptive Intelligence](https://arxiv.org/abs/2510.01609)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Lau*

Main category: cs.AI

TL;DR: 本文提出基于大语言模型的多智能体协作推荐框架AgentRec，经实验验证其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有交互式对话推荐系统在处理动态用户偏好、保持对话连贯性和平衡多排名目标方面存在挑战。

Method: 引入AgentRec框架，采用分层智能体网络和自适应加权机制，提出三层学习策略。

Result: 在三个真实数据集上实验表明，AgentRec在对话成功率、推荐准确率和对话效率上均有提升，计算成本相当。

Conclusion: AgentRec有效解决现有方法局限，性能优于现有基线。

Abstract: Interactive conversational recommender systems have gained significant
attention for their ability to capture user preferences through natural
language interactions. However, existing approaches face substantial challenges
in handling dynamic user preferences, maintaining conversation coherence, and
balancing multiple ranking objectives simultaneously. This paper introduces
AgentRec, a next-generation LLM-powered multi-agent collaborative
recommendation framework that addresses these limitations through hierarchical
agent networks with adaptive intelligence. Our approach employs specialized
LLM-powered agents for conversation understanding, preference modeling, context
awareness, and dynamic ranking, coordinated through an adaptive weighting
mechanism that learns from interaction patterns. We propose a three-tier
learning strategy combining rapid response for simple queries, intelligent
reasoning for complex preferences, and deep collaboration for challenging
scenarios. Extensive experiments on three real-world datasets demonstrate that
AgentRec achieves consistent improvements over state-of-the-art baselines, with
2.8\% enhancement in conversation success rate, 1.9\% improvement in
recommendation accuracy (NDCG@10), and 3.2\% better conversation efficiency
while maintaining comparable computational costs through intelligent agent
coordination.

</details>


### [25] [PychoBench: Evaluating the Psychology Intelligence of Large Language Models](https://arxiv.org/abs/2510.01611)
*Min Zeng*

Main category: cs.AI

TL;DR: 文章探讨大语言模型能否用于心理咨询，引入基于美国国家咨询师考试的PsychoBench基准测试，发现仅前沿大模型能达咨询考试标准。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在认知能力应用如心理咨询方面潜力未充分挖掘，探究其能否有效应用于心理咨询。

Method: 引入基于美国国家咨询师考试的PsychoBench基准测试，包含约2252道精心挑选的单选题，全面评估大语言模型担任咨询师的能力。

Result: GPT - 4o、Llama3.3 - 70B和Gemma3 - 27B等先进模型远超及格线，Qwen2.5 - 7B、Mistral - 7B等小型开源模型远低于及格线。

Conclusion: 目前仅前沿大语言模型能达到咨询考试标准，凸显开发面向心理学大语言模型的前景与挑战。

Abstract: Large Language Models (LLMs) have demonstrated remarkable success across a
wide range of industries, primarily due to their impressive generative
abilities. Yet, their potential in applications requiring cognitive abilities,
such as psychological counseling, remains largely untapped. This paper
investigates the key question: Can LLMs be effectively applied to psychological
counseling? To determine whether an LLM can effectively take on the role of a
psychological counselor, the first step is to assess whether it meets the
qualifications required for such a role, namely the ability to pass the U.S.
National Counselor Certification Exam (NCE). This is because, just as a human
counselor must pass a certification exam to practice, an LLM must demonstrate
sufficient psychological knowledge to meet the standards required for such a
role. To address this, we introduce PsychoBench, a benchmark grounded in
U.S.national counselor examinations, a licensure test for professional
counselors that requires about 70% accuracy to pass. PsychoBench comprises
approximately 2,252 carefully curated single-choice questions, crafted to
require deep understanding and broad enough to cover various sub-disciplines of
psychology. This benchmark provides a comprehensive assessment of an LLM's
ability to function as a counselor. Our evaluation shows that advanced models
such as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing
threshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)
remain far below it. These results suggest that only frontier LLMs are
currently capable of meeting counseling exam standards, highlighting both the
promise and the challenges of developing psychology-oriented LLMs.

</details>


### [26] [Learning to Decide with Just Enough: Information-Theoretic Context Summarization for CDMPs](https://arxiv.org/abs/2510.01620)
*Peidong Liu,Junjiang Lin,Shaowen Wang,Yao Xu,Haiqing Li,Xuhao Xie,Siyi Wu,Hao Li*

Main category: cs.AI

TL;DR: 提出用大语言模型压缩上下文输入的信息论方法用于CMDPs，实验显示该方法优于基线，能提供可扩展和可解释的高效决策方案。


<details>
  <summary>Details</summary>
Motivation: 现有CMDPs方法在高维或非结构化上下文下泛化能力差，存在计算量大和性能不稳定问题。

Method: 提出使用大语言模型将上下文输入压缩成低维、语义丰富摘要的信息论方法，基于近似上下文充分性概念分析。

Result: 在多个基准测试中，该方法优于原始上下文和非上下文基线，提高了奖励、成功率和样本效率，降低了延迟和内存使用。

Conclusion: 基于大语言模型的摘要方法为上下文丰富、资源受限环境中的高效决策提供了可扩展和可解释的解决方案。

Abstract: Contextual Markov Decision Processes (CMDPs) offer a framework for sequential
decision-making under external signals, but existing methods often fail to
generalize in high-dimensional or unstructured contexts, resulting in excessive
computation and unstable performance. We propose an information-theoretic
summarization approach that uses large language models (LLMs) to compress
contextual inputs into low-dimensional, semantically rich summaries. These
summaries augment states by preserving decision-critical cues while reducing
redundancy. Building on the notion of approximate context sufficiency, we
provide, to our knowledge, the first regret bounds and a latency-entropy
trade-off characterization for CMDPs. Our analysis clarifies how
informativeness impacts computational cost. Experiments across discrete,
continuous, visual, and recommendation benchmarks show that our method
outperforms raw-context and non-context baselines, improving reward, success
rate, and sample efficiency, while reducing latency and memory usage. These
findings demonstrate that LLM-based summarization offers a scalable and
interpretable solution for efficient decision-making in context-rich,
resource-constrained environments.

</details>


### [27] [Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective](https://arxiv.org/abs/2510.01639)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: 探索大语言模型地理空间推理能力，以轨迹恢复为代理任务，用新数据集和提示框架实验，结果显示优于基线模型，有优缺点并可增强导航体验


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型能否读取道路网络地图并进行导航

Method: 将轨迹恢复作为代理任务，引入GLOBALTRACE数据集，使用以道路网络为上下文的提示框架

Result: 大语言模型优于现成基线和专业轨迹恢复模型，有强零样本泛化能力，对道路网络和坐标系理解强，但存在区域和交通模式的系统偏差

Conclusion: 大语言模型可通过灵活推理地图融入用户偏好来增强导航体验

Abstract: We explore the geospatial reasoning capabilities of Large Language Models
(LLMs), specifically, whether LLMs can read road network maps and perform
navigation. We frame trajectory recovery as a proxy task, which requires models
to reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with
over 4,000 real-world trajectories across diverse regions and transportation
modes. Using road network as context, our prompting framework enables LLMs to
generate valid paths without accessing any external navigation tools.
Experiments show that LLMs outperform off-the-shelf baselines and specialized
trajectory recovery models, with strong zero-shot generalization. Fine-grained
analysis shows that LLMs have strong comprehension of the road network and
coordinate systems, but also pose systematic biases with respect to regions and
transportation modes. Finally, we demonstrate how LLMs can enhance navigation
experiences by reasoning over maps in flexible ways to incorporate user
preferences.

</details>


### [28] [GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents](https://arxiv.org/abs/2510.01664)
*Yejin Kim,Youngbin Lee,Juhyeong Kim,Yongjae Lee*

Main category: cs.AI

TL;DR: 研究表明GuruAgents能将传奇投资大师的策略系统化，开发五个不同的GuruAgents进行回测，巴菲特GuruAgent表现最佳，证实提示工程可将投资哲学转化为量化策略。


<details>
  <summary>Details</summary>
Motivation: 将传奇投资大师的定性投资哲学转化为可复制的定量策略，为自动化系统投资探索新方向。

Method: 开发五个不同的GuruAgents，将投资大师的哲学编码到集成金融工具和确定性推理管道的大语言模型提示中，并在2023年Q4到2025年Q2对纳斯达克100成分股进行回测。

Result: GuruAgents表现出独特行为，巴菲特GuruAgent表现最佳，复合年增长率达42.2%，显著超越基准，其他代理结果各异。

Conclusion: 提示工程能成功将投资大师的定性哲学转化为可复制的定量策略，为自动化系统投资指明新方向。

Abstract: This study demonstrates that GuruAgents, prompt-guided AI agents, can
systematically operationalize the strategies of legendary investment gurus. We
develop five distinct GuruAgents, each designed to emulate an iconic investor,
by encoding their distinct philosophies into LLM prompts that integrate
financial tools and a deterministic reasoning pipeline. In a backtest on
NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique
behaviors driven by their prompted personas. The Buffett GuruAgent achieves the
highest performance, delivering a 42.2\% CAGR that significantly outperforms
benchmarks, while other agents show varied results. These findings confirm that
prompt engineering can successfully translate the qualitative philosophies of
investment gurus into reproducible, quantitative strategies, highlighting a
novel direction for automated systematic investing. The source code and data
are available at https://github.com/yejining99/GuruAgents.

</details>


### [29] [Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness](https://arxiv.org/abs/2510.01670)
*Erfan Shayegani,Keegan Hines,Yue Dong,Nael Abu-Ghazaleh,Roman Lutz,Spencer Whitehead,Vidhisha Balachandran,Besmira Nushi,Vibhav Vineet*

Main category: cs.AI

TL;DR: 本文指出计算机使用代理（CUAs）存在盲目目标导向（BGD）问题，开发了基准测试BLIND - ACT评估前沿模型，发现BGD有风险，提示干预有局限，确定失败模式，为相关研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 研究CUAs存在的盲目追求目标的问题及潜在风险，为确保其安全部署提供依据。

Method: 刻画BGD三种普遍模式，开发包含90个任务的BLIND - ACT基准测试，利用基于大语言模型的评判器评估代理行为，用其评估九个前沿模型，进行定性分析。

Result: 前沿模型平均BGD率达80.8%，提示干预可降低BGD水平但仍有风险，发现执行优先偏差、思维与行动脱节、请求优先等失败模式。

Conclusion: 识别BGD并引入BLIND - ACT为研究和缓解该风险、确保CUAs安全部署奠定基础。

Abstract: Computer-Use Agents (CUAs) are an increasingly deployed class of agents that
take actions on GUIs to accomplish user goals. In this paper, we show that CUAs
consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals
regardless of feasibility, safety, reliability, or context. We characterize
three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)
assumptions and decisions under ambiguity, and (iii) contradictory or
infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these
three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and
employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement
with human annotations. We use BLIND-ACT to evaluate nine frontier models,
including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing
high average BGD rates (80.8%) across them. We show that BGD exposes subtle
risks that arise even when inputs are not directly harmful. While
prompting-based interventions lower BGD levels, substantial risk persists,
highlighting the need for stronger training- or inference-time interventions.
Qualitative analysis reveals observed failure modes: execution-first bias
(focusing on how to act over whether to act), thought-action disconnect
(execution diverging from reasoning), and request-primacy (justifying actions
due to user request). Identifying BGD and introducing BLIND-ACT establishes a
foundation for future research on studying and mitigating this fundamental risk
and ensuring safe CUA deployment.

</details>


### [30] [A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation](https://arxiv.org/abs/2510.01671)
*Motoki Sato,Yuki Matsushita,Hidekazu Takahashi,Tomoaki Kakazu,Sou Nagata,Mizuho Ohnuma,Atsushi Yoshikawa,Masayuki Yamamura*

Main category: cs.AI

TL;DR: 提出LENOHA系统，用高精度分类器处理临床查询，返回临床FAQ答案，评估显示其准确性高、能耗低、延迟短，避免生成错误。


<details>
  <summary>Details</summary>
Motivation: 患者术前常有疑问，但工作流程时间紧张和隐私限制阻碍个性化咨询。

Method: 使用高精度句子转换器分类器路由输入，从临床医生整理的FAQ中返回逐字答案，评估两个领域，用专家评审验证集和独立测试集。

Result: E5 - large - instruct准确率0.983，AUC 0.996；Gemini无错误；临床路径能耗比闲聊回复低约170倍，延迟约0.10秒。

Conclusion: 临床路径通过返回审核过的FAQ答案，避免生成错误，支持隐私、可持续性及在带宽受限环境公平部署。

Abstract: Patients awaiting invasive procedures often have unanswered pre-procedural
questions; however, time-pressured workflows and privacy constraints limit
personalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave
No One Behind Architecture), a safety-first, local-first system that routes
inputs with a high-precision sentence-transformer classifier and returns
verbatim answers from a clinician-curated FAQ for clinical queries, eliminating
free-text generation in the clinical path. We evaluated two domains (tooth
extraction and gastroscopy) using expert-reviewed validation sets
(n=400/domain) for thresholding and independent test sets (n=200/domain). Among
the four encoders, E5-large-instruct (560M) achieved an overall accuracy of
0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were
statistically indistinguishable from GPT-4o on this task; Gemini made no errors
on this test set. Energy logging shows that the non-generative clinical path
consumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local
8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single
on-prem GPU. These results indicate that near-frontier discrimination and
generation-induced errors are structurally avoided in the clinical path by
returning vetted FAQ answers verbatim, supporting privacy, sustainability, and
equitable deployment in bandwidth-limited environments.

</details>


### [31] [Improving AGI Evaluation: A Data Science Perspective](https://arxiv.org/abs/2510.01687)
*John Hawkins*

Main category: cs.AI

TL;DR: 当前AGI评估方法不佳，应采用注重可靠任务执行评估的新哲学，并给出实践示例。


<details>
  <summary>Details</summary>
Motivation: 现有AGI评估方法因工程目标广泛难以完美评估，且基于直觉设计的合成任务评估效果差。

Method: 借鉴数据科学常见做法，采用注重可靠任务执行的评估设计哲学。

Result: 提出了新的AGI评估视角并给出实践示例。

Conclusion: 应采用注重可靠任务执行评估的设计哲学来进行AGI评估。

Abstract: Evaluation of potential AGI systems and methods is difficult due to the
breadth of the engineering goal. We have no methods for perfect evaluation of
the end state, and instead measure performance on small tests designed to
provide directional indication that we are approaching AGI. In this work we
argue that AGI evaluation methods have been dominated by a design philosophy
that uses our intuitions of what intelligence is to create synthetic tasks,
that have performed poorly in the history of AI. Instead we argue for an
alternative design philosophy focused on evaluating robust task execution that
seeks to demonstrate AGI through competence. This perspective is developed from
common practices in data science that are used to show that a system can be
reliably deployed. We provide practical examples of what this would mean for
AGI evaluation.

</details>


### [32] [VaPR -- Vision-language Preference alignment for Reasoning](https://arxiv.org/abs/2510.01700)
*Rohan Wadhawan,Fabrice Y Harel-Canada,Zi-Yi Dou,Suhaila Shakiah,Robinson Piramuthu,Nanyun Peng*

Main category: cs.AI

TL;DR: 提出基于LLM引导响应编辑的硬负响应生成框架，构建VaPR数据集微调LVLMs，在多基准测试中提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有偏好微调技术忽略合成偏好注释中的噪声，如风格和长度偏差。

Method: 引入基于LLM引导响应编辑的框架生成硬负响应，构建VaPR数据集微调三种LVLM族。

Result: VaPR模型在十个基准测试中显著提升性能，减少二元问题答‘是’倾向，框架可泛化到开源LLM。

Conclusion: 所提方法有效提升LVLMs性能，数据规模增加性能提升，项目资源可在指定页面获取。

Abstract: Preference finetuning methods like Direct Preference Optimization (DPO) with
AI-generated feedback have shown promise in aligning Large Vision-Language
Models (LVLMs) with human preferences. However, existing techniques overlook
the prevalence of noise in synthetic preference annotations in the form of
stylistic and length biases. To this end, we introduce a hard-negative response
generation framework based on LLM-guided response editing, that produces
rejected responses with targeted errors, maintaining stylistic and length
similarity to the accepted ones. Using this framework, we develop the VaPR
dataset, comprising 30K high-quality samples, to finetune three LVLM families:
LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver
significant performance improvements across ten benchmarks, achieving average
gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable
improvements on reasoning tasks. A scaling analysis shows that performance
consistently improves with data size, with LLaVA models benefiting even at
smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary
questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we
show that the framework generalizes to open-source LLMs as editors, with models
trained on VaPR-OS achieving ~99% of the performance of models trained on
\name, which is synthesized using GPT-4o. Our data, models, and code can be
found on the project page https://vap-r.github.io

</details>


### [33] [MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs](https://arxiv.org/abs/2510.01724)
*Madina Bekbergenova,Lucas Pradi,Benjamin Navet,Emma Tysinger,Franck Michel,Matthieu Feraud,Yousouf Taghzouti,Yan Zhou Chen,Olivier Kirchhoffer,Florence Mehl,Martin Legrand,Tao Jiang,Marco Pagni,Soha Hassoun,Jean-Luc Wolfender,Wout Bittremieux,Fabien Gandon,Louis-Félix Nothias*

Main category: cs.AI

TL;DR: 提出AI系统MetaboT，用大语言模型将用户问题转为SPARQL查询知识图谱，性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 质谱代谢组学数据解读需先进方法，知识图谱使用要求对本体和查询语言有深入理解，需简化查询流程。

Method: 设计MetaboT系统，采用多智能体处理用户查询，利用LangChain和LangGraph库，按结构化工作流生成查询。

Result: 评估中，MetaboT准确率达83.67%，远高于基线的8.16%。

Conclusion: MetaboT作为问答助手表现出色，消除访问知识图谱的技术障碍，促进数据驱动发现。

Abstract: Mass spectrometry metabolomics generates vast amounts of data requiring
advanced methods for interpretation. Knowledge graphs address these challenges
by structuring mass spectrometry data, metabolite information, and their
relationships into a connected network (Gaudry et al. 2024). However, effective
use of a knowledge graph demands an in-depth understanding of its ontology and
its query language syntax. To overcome this, we designed MetaboT, an AI system
utilizing large language models (LLMs) to translate user questions into SPARQL
semantic query language for operating on knowledge graphs (Steve Harris 2013).
We demonstrate its effectiveness using the Experimental Natural Products
Knowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural
products (Gaudry et al. 2024).MetaboT employs specialized AI agents for
handling user queries and interacting with the knowledge graph by breaking down
complex tasks into discrete components, each managed by a specialised agent
(Fig. 1a). The multi-agent system is constructed using the LangChain and
LangGraph libraries, which facilitate the integration of LLMs with external
tools and information sources (LangChain, n.d.). The query generation process
follows a structured workflow. First, the Entry Agent determines if the
question is new or a follow-up to previous interactions. New questions are
forwarded to the Validator Agent, which verifies if the question is related to
the knowledge graph. Then, the valid question is sent to the Supervisor Agent,
which identifies if the question requires chemical conversions or standardized
identifiers. In this case it delegates the question to the Knowledge Graph
Agent, which can use tools to extract necessary details, such as URIs or
taxonomies of chemical names, from the user query. Finally, an agent
responsible for crafting the SPARQL queries equipped with the ontology of the
knowledge graph uses the provided identifiers to generate the query. Then, the
system executes the generated query against the metabolomics knowledge graph
and returns structured results to the user (Fig. 1b). To assess the performance
of MetaboT we have curated 50 metabolomics-related questions and their expected
answers. In addition to submitting these questions to MetaboT, we evaluated a
baseline by submitting them to a standard LLM (GPT-4o) with a prompt that
incorporated the knowledge graph ontology but did not provide specific entity
IDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%,
underscoring the necessity of our multi-agent system for accurately retrieving
entities and generating correct SPARQL queries. MetaboT demonstrates promising
performance as a conversational question-answering assistant, enabling
researchers to retrieve structured metabolomics data through natural language
queries. By automating the generation and execution of SPARQL queries, it
removes technical barriers that have traditionally hindered access to knowledge
graphs. Importantly, MetaboT leverages the capabilities of LLMs while
maintaining experimentally grounded query generation, ensuring that outputs
remain aligned with domain-specific standards and data structures. This
approach facilitates data-driven discoveries by bridging the gap between
complex semantic technologies and user-friendly interaction. MetaboT is
accessible at [https://metabot.holobiomicslab.eu/], and its source code is
available at [https://github.com/HolobiomicsLab/MetaboT].

</details>


### [34] [A cybersecurity AI agent selection and decision support framework](https://arxiv.org/abs/2510.01751)
*Masike Malatji*

Main category: cs.AI

TL;DR: 提出将不同AI代理架构与NIST CSF 2.0对齐的决策支持框架，展示其优势并弥合理论与实践差距。


<details>
  <summary>Details</summary>
Motivation: 解决当代网络威胁，弥合理论AI与实际网络安全需求的差距，建立符合行业标准的多智能体系统。

Method: 将NIST CSF 2.0功能细分为具体任务，将AI代理属性与安全要求关联，划分不同自主级别。

Result: 框架经概念验证，能使AI部署符合实际约束和风险状况，增强态势感知、加速响应并强化长期韧性。

Conclusion: 该研究建立了符合行业标准的多智能体系统基础，连接了理论AI与实际网络安全需求。

Abstract: This paper presents a novel, structured decision support framework that
systematically aligns diverse artificial intelligence (AI) agent architectures,
reactive, cognitive, hybrid, and learning, with the comprehensive National
Institute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.
By integrating agent theory with industry guidelines, this framework provides a
transparent and stepwise methodology for selecting and deploying AI solutions
to address contemporary cyber threats. Employing a granular decomposition of
NIST CSF 2.0 functions into specific tasks, the study links essential AI agent
properties such as autonomy, adaptive learning, and real-time responsiveness to
each subcategory's security requirements. In addition, it outlines graduated
levels of autonomy (assisted, augmented, and fully autonomous) to accommodate
organisations at varying stages of cybersecurity maturity. This holistic
approach transcends isolated AI applications, providing a unified detection,
incident response, and governance strategy. Through conceptual validation, the
framework demonstrates how tailored AI agent deployments can align with
real-world constraints and risk profiles, enhancing situational awareness,
accelerating response times, and fortifying long-term resilience via adaptive
risk management. Ultimately, this research bridges the gap between theoretical
AI constructs and operational cybersecurity demands, establishing a foundation
for robust, empirically validated multi-agent systems that adhere to industry
standards.

</details>


### [35] [REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing](https://arxiv.org/abs/2510.01800)
*Thanh Ma,Tri-Tam La,Lam-Thu Le Huu,Minh-Nghi Nguyen,Khanh-Van Pham Luu,Huu-Hoa Nguyen*

Main category: cs.AI

TL;DR: 提出由CatRAG驱动的LLM增强咨询聊天机器人REBot，在分类和问答任务中表现优异并实现了Web应用。


<details>
  <summary>Details</summary>
Motivation: 学术规则咨询需要特定领域监管资源，构建有效系统面临挑战。

Method: 提出CatRAG混合检索推理框架，结合检索增强生成与基于图的推理，使用轻量级意图分类器路由查询。

Result: 在分类和问答任务中取得F1分数98.89%的最优性能。

Conclusion: 实现的Web应用证明了REBot在现实学术咨询场景中的实用价值。

Abstract: Academic regulation advising is essential for helping students interpret and
comply with institutional policies, yet building effective systems requires
domain specific regulatory resources. To address this challenge, we propose
REBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval
reasoning framework that integrates retrieval augmented generation with graph
based reasoning. CatRAG unifies dense retrieval and graph reasoning, supported
by a hierarchical, category labeled knowledge graph enriched with semantic
features for domain alignment. A lightweight intent classifier routes queries
to the appropriate retrieval modules, ensuring both factual accuracy and
contextual depth. We construct a regulation specific dataset and evaluate REBot
on classification and question answering tasks, achieving state of the art
performance with an F1 score of 98.89%. Finally, we implement a web application
that demonstrates the practical value of REBot in real world academic advising
scenarios.

</details>


### [36] [Human-AI Teaming Co-Learning in Military Operations](https://arxiv.org/abs/2510.01815)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 在军事威胁快速演变和作战环境日益复杂背景下，研究提出军事行动中人类 - AI 团队可信协同学习模型，还给出示例与建议。


<details>
  <summary>Details</summary>
Motivation: 当前军事行动中集成 AI 有优势但也面临挑战与风险，以往多从外部视角应对，需深入系统内部动态应对多方面问题。

Method: 提出涵盖四个维度的可信协同学习模型，包括可调整自主性、多层级控制、双向反馈和协同决策。

Result: 提出了具体的可信协同学习模型，并伴有具体示例和建议。

Conclusion: 该模型有助于进一步开发负责任且可信的军事行动人类 - AI 团队系统。

Abstract: In a time of rapidly evolving military threats and increasingly complex
operational environments, the integration of AI into military operations proves
significant advantages. At the same time, this implies various challenges and
risks regarding building and deploying human-AI teaming systems in an effective
and ethical manner. Currently, understanding and coping with them are often
tackled from an external perspective considering the human-AI teaming system as
a collective agent. Nevertheless, zooming into the dynamics involved inside the
system assures dealing with a broader palette of relevant multidimensional
responsibility, safety, and robustness aspects. To this end, this research
proposes the design of a trustworthy co-learning model for human-AI teaming in
military operations that encompasses a continuous and bidirectional exchange of
insights between the human and AI agents as they jointly adapt to evolving
battlefield conditions. It does that by integrating four dimensions. First,
adjustable autonomy for dynamically calibrating the autonomy levels of agents
depending on aspects like mission state, system confidence, and environmental
uncertainty. Second, multi-layered control which accounts continuous oversight,
monitoring of activities, and accountability. Third, bidirectional feedback
with explicit and implicit feedback loops between the agents to assure a proper
communication of reasoning, uncertainties, and learned adaptations that each of
the agents has. And fourth, collaborative decision-making which implies the
generation, evaluation, and proposal of decisions associated with confidence
levels and rationale behind them. The model proposed is accompanied by concrete
exemplifications and recommendations that contribute to further developing
responsible and trustworthy human-AI teaming systems in military operations.

</details>


### [37] [Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.01833)
*Zhihao Dou,Qinjian Zhao,Zhongwei Wan,Dinggen Zhang,Weida Wang,Towsif Raiyan,Benteng Chen,Qingtao Pan,Yang Ouyang,Zhiqiang Gao,Shufei Zhang,Sumon Biswas*

Main category: cs.AI

TL;DR: 文章指出大语言模型推理存在局限，提出PTA - GRPO两阶段框架，实验表明该框架能稳定显著提升推理效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型基于CoT推理存在缺乏全局规划的局限，现有方法计算成本高且难以产生最优推理轨迹，需改进。

Method: 提出PTA - GRPO两阶段框架，第一阶段用高级大语言模型将CoT提炼为高级指导并进行监督微调，第二阶段引入指导感知的强化学习方法联合优化输出和指导质量。

Result: 在多个数学推理基准测试和不同基础模型上实验，PTA - GRPO能稳定显著提升效果。

Conclusion: PTA - GRPO框架有效且具有泛化性。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning abilities
in complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,
due to their autoregressive token-level generation, the reasoning process is
largely constrained to local decision-making and lacks global planning. This
limitation frequently results in redundant, incoherent, or inaccurate
reasoning, which significantly degrades overall performance. Existing
approaches, such as tree-based algorithms and reinforcement learning (RL),
attempt to address this issue but suffer from high computational costs and
often fail to produce optimal reasoning trajectories. To tackle this challenge,
we propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy
Optimization PTA-GRPO, a two-stage framework designed to improve both
high-level planning and fine-grained CoT reasoning. In the first stage, we
leverage advanced LLMs to distill CoT into compact high-level guidance, which
is then used for supervised fine-tuning (SFT). In the second stage, we
introduce a guidance-aware RL method that jointly optimizes the final output
and the quality of high-level guidance, thereby enhancing reasoning
effectiveness. We conduct extensive experiments on multiple mathematical
reasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across
diverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and
LLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently
achieves stable and significant improvements across different models and tasks,
validating its effectiveness and generalization.

</details>


### [38] [Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning](https://arxiv.org/abs/2510.01857)
*Claudio Fanconi,Nicolás Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 本文将对抗逆强化学习应用于大语言模型推理，学习密集的、标记级的奖励模型，该奖励模型在训练和推理中发挥作用，实验证明其能提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 将对抗逆强化学习应用于大语言模型推理，直接从专家示范中学习奖励模型，以优化推理过程。

Method: 重新构建并实施对抗逆强化学习到语言模型推理中，学习标记级奖励模型，该奖励在训练中提供反馈、在推理中作为评判器。

Result: 在GSM8K数据集上，以Llama3和Qwen2.5为骨干，证明密集推理奖励可用于引导推理，奖励引导的重排序能提升预测性能。

Conclusion: 将训练信号、推理选择和标记级诊断统一到单个推理奖励中，过程级奖励有广泛潜力提升语言模型的多步推理能力。

Abstract: We reframe and operationalise adversarial inverse reinforcement learning
(IRL) to large language model reasoning, learning a dense, token-level reward
model for process supervision directly from expert demonstrations rather than
imitating style via supervised fine-tuning. The learned reasoning reward serves
two complementary roles: (i) it provides step-level feedback to optimise a
reasoning policy during training; and (ii) it functions at inference as a
critic to rerank sampled traces under fixed compute budgets. We demonstrate
that our approach prioritises correctness over surface form, yielding scores
that correlate with eventual answer validity and enabling interpretable
localisation of errors within a trace. Empirically, on GSM8K with Llama3 and
Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a
learning signal to elicit reasoning, and (ii) predictive performance is
improved from reward-guided reranking (notably for Llama-based policies). By
unifying training signals, inference-time selection, and token-level
diagnostics into a single reasoning reward, this work suggests reusable
process-level rewards with broad potential to enhance multi-step reasoning in
language models.

</details>


### [39] [Constrained Adaptive Rejection Sampling](https://arxiv.org/abs/2510.01902)
*Paweł Parys,Sairam Vaidya,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.AI

TL;DR: 提出Constrained Adaptive Rejection Sampling (CARS)方法，提升受限生成的样本效率和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有受限生成方法存在问题，如贪婪受限解码扭曲模型分布，拒绝采样浪费计算，在程序模糊测试等领域需要兼顾样本有效性和多样性。

Method: CARS从无约束语言模型采样开始，通过在trie中记录违反约束的后续内容并从未来抽取中减去其概率质量，自适应排除违反约束的后续内容。

Result: 在多个领域的实验中，CARS比贪婪受限解码和近似语言模型分布的方法具有更高效率，且样本多样性更强。

Conclusion: CARS严格提高了拒绝采样的样本效率，且无分布扭曲，能产生遵循受限分布的样本。

Abstract: Language Models (LMs) are increasingly used in applications where generated
outputs must satisfy strict semantic or syntactic constraints. Existing
approaches to constrained generation fall along a spectrum: greedy constrained
decoding methods enforce validity during decoding but distort the LM's
distribution, while rejection sampling (RS) preserves fidelity but wastes
computation by discarding invalid outputs. Both extremes are problematic in
domains such as program fuzzing, where both validity and diversity of samples
are essential. We present Constrained Adaptive Rejection Sampling (CARS), an
approach that strictly improves the sample-efficiency of RS without
distributional distortion. CARS begins with unconstrained LM sampling and
adaptively rules out constraint-violating continuations by recording them in a
trie and subtracting their probability mass from future draws. This adaptive
pruning ensures that prefixes proven invalid are never revisited, acceptance
rates improve monotonically, and the resulting samples exactly follow the
constrained distribution. In experiments on a variety of domains -- e.g.,
program fuzzing and molecular generation -- CARS consistently achieves higher
efficiency -- measured in the number of LM forward passes per valid sample --
while also producing stronger sample diversity than both GCD and methods that
approximate the LM's distribution.

</details>


### [40] [To Mask or to Mirror: Human-AI Alignment in Collective Reasoning](https://arxiv.org/abs/2510.01924)
*Crystal Qian,Aaron Parisi,Clémentine Bouleau,Vivian Tsai,Maël Lebreton,Lucas Dixon*

Main category: cs.AI

TL;DR: 研究评估大语言模型与人类集体决策推理的对齐性，发现对齐取决于多种因素，需动态基准。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型用于集体决策，需检验其与人类社会推理的对齐性。

Method: 提出评估集体对齐的实证框架，用海上迷失实验，对人类分组实验，模拟匹配的大语言模型组并进行基准测试。

Result: 大语言模型行为有差异，部分反映人类偏见，部分掩盖并补偿，人类与AI在集体推理中的对齐取决于多种因素。

Conclusion: 理解大语言模型与人类集体行为的对齐对推进社会对齐AI很重要，需要能捕捉集体推理复杂性的动态基准。

Abstract: As large language models (LLMs) are increasingly used to model and augment
collective decision-making, it is critical to examine their alignment with
human social reasoning. We present an empirical framework for assessing
collective alignment, in contrast to prior work on the individual level. Using
the Lost at Sea social psychology task, we conduct a large-scale online
experiment (N=748), randomly assigning groups to leader elections with either
visible demographic attributes (e.g. name, gender) or pseudonymous aliases. We
then simulate matched LLM groups conditioned on the human data, benchmarking
Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some
mirror human biases; others mask these biases and attempt to compensate for
them. We empirically demonstrate that human-AI alignment in collective
reasoning depends on context, cues, and model-specific inductive biases.
Understanding how LLMs align with collective human behavior is critical to
advancing socially-aligned AI, and demands dynamic benchmarks that capture the
complexities of collective reasoning.

</details>


### [41] [Zero-shot reasoning for simulating scholarly peer-review](https://arxiv.org/abs/2510.02027)
*Khalid M. Saqr*

Main category: cs.AI

TL;DR: 学术出版生态面临投稿量和AI问题，本文研究模拟框架评估AI同行评审报告，证明其可靠，为科学界和出版策略者提供工具。


<details>
  <summary>Details</summary>
Motivation: 学术出版生态面临投稿量难管理和AI无监管的双重危机，传统同行评审缺乏可扩展客观基准，需新治理模型维护科学诚信。

Method: 研究确定性模拟框架，分析352份同行评审模拟报告。

Result: 系统能模拟校准编辑判断，‘Revise’决策占多数，‘Reject’率适应领域规范；保持程序完整性，证据锚定合规率稳定。

Conclusion: 该框架可减轻生成式AI的随机性，为科学界和出版策略者提供工具，将AI定位为机构问责的重要组成部分。

Abstract: The scholarly publishing ecosystem faces a dual crisis of unmanageable
submission volumes and unregulated AI, creating an urgent need for new
governance models to safeguard scientific integrity. The traditional human-only
peer review regime lacks a scalable, objective benchmark, making editorial
processes opaque and difficult to audit. Here we investigate a deterministic
simulation framework that provides the first stable, evidence-based standard
for evaluating AI-generated peer review reports. Analyzing 352 peer-review
simulation reports, we identify consistent system state indicators that
demonstrate its reliability. First, the system is able to simulate calibrated
editorial judgment, with 'Revise' decisions consistently forming the majority
outcome (>50%) across all disciplines, while 'Reject' rates dynamically adapt
to field-specific norms, rising to 45% in Health Sciences. Second, it maintains
unwavering procedural integrity, enforcing a stable 29% evidence-anchoring
compliance rate that remains invariant across diverse review tasks and
scientific domains. These findings demonstrate a system that is predictably
rule-bound, mitigating the stochasticity of generative AI. For the scientific
community, this provides a transparent tool to ensure fairness; for publishing
strategists, it offers a scalable instrument for auditing workflows, managing
integrity risks, and implementing evidence-based governance. The framework
repositions AI as an essential component of institutional accountability,
providing the critical infrastructure to maintain trust in scholarly
communication.

</details>


### [42] [ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection](https://arxiv.org/abs/2510.02060)
*Sanghyu Yoon,Dongmin Kim,Suhee Yoon,Ye Seul Sim,Seungdong Yoa,Hye-Seung Cho,Soonyoung Lee,Hankook Lee,Woohyung Lim*

Main category: cs.AI

TL;DR: 现有表格异常检测基准缺乏语义上下文，ReTabAD通过恢复文本语义解决该问题，提供数据集和零样本框架，实验表明语义上下文可提升检测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有表格异常检测基准仅提供原始数据点，缺乏语义上下文，限制研究灵活性和模型对领域知识的利用。

Method: 提供20个精心策划的带有结构化文本元数据的表格数据集和最先进异常检测算法的实现，以及一个零样本大语言模型框架。

Result: 语义上下文提高了检测性能，增强了可解释性。

Conclusion: ReTabAD可作为上下文感知异常检测系统探索的基准。

Abstract: In tabular anomaly detection (AD), textual semantics often carry critical
signals, as the definition of an anomaly is closely tied to domain-specific
context. However, existing benchmarks provide only raw data points without
semantic context, overlooking rich textual metadata such as feature
descriptions and domain knowledge that experts rely on in practice. This
limitation restricts research flexibility and prevents models from fully
leveraging domain knowledge for detection. ReTabAD addresses this gap by
restoring textual semantics to enable context-aware tabular AD research. We
provide (1) 20 carefully curated tabular datasets enriched with structured
textual metadata, together with implementations of state-of-the-art AD
algorithms including classical, deep learning, and LLM-based approaches, and
(2) a zero-shot LLM framework that leverages semantic context without
task-specific training, establishing a strong baseline for future research.
Furthermore, this work provides insights into the role and utility of textual
metadata in AD through experiments and analysis. Results show that semantic
context improves detection performance and enhances interpretability by
supporting domain-aware reasoning. These findings establish ReTabAD as a
benchmark for systematic exploration of context-aware AD.

</details>


### [43] [Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning](https://arxiv.org/abs/2510.02091)
*Xinyuan Song,Keyu Wang,PengXiang Li,Lu Yin,Shiwei Liu*

Main category: cs.AI

TL;DR: 研究表明大语言模型深层在表征学习贡献小，但其贡献因评估设置而异，深度使用高度异质且依赖上下文。


<details>
  <summary>Details</summary>
Motivation: 以往关于大语言模型深层贡献的论断评估范围窄，需系统研究深度利用情况。

Method: 从评估协议、任务类别和模型架构等多维度对大语言模型深度利用进行系统研究。

Result: 非常深层通常不如早期层有效；基于似然性指标无生成时，剪枝多数层可保留性能；基于生成的评估显示中层和深层对推理和长程连贯不可或缺；知识和检索集中在浅层，推理准确性依赖深层且可通过蒸馏重塑。

Conclusion: 大语言模型深度使用高度异质且依赖上下文，解释和压缩大模型需考虑任务、指标和模型。

Abstract: Recent studies suggest that the deeper layers of Large Language Models (LLMs)
contribute little to representation learning and can often be removed without
significant performance loss. However, such claims are typically drawn from
narrow evaluations and may overlook important aspects of model behavior. In
this work, we present a systematic study of depth utilization across diverse
dimensions, including evaluation protocols, task categories, and model
architectures. Our analysis confirms that very deep layers are generally less
effective than earlier ones, but their contributions vary substantially with
the evaluation setting. Under likelihood-based metrics without generation,
pruning most layers preserves performance, with only the initial few being
critical. By contrast, generation-based evaluation uncovers indispensable roles
for middle and deeper layers in enabling reasoning and maintaining long-range
coherence. We further find that knowledge and retrieval are concentrated in
shallow components, whereas reasoning accuracy relies heavily on deeper layers
-- yet can be reshaped through distillation. These results highlight that depth
usage in LLMs is highly heterogeneous and context-dependent, underscoring the
need for task-, metric-, and model-aware perspectives in both interpreting and
compressing large models.

</details>


### [44] [Do AI Models Perform Human-like Abstract Reasoning Across Modalities?](https://arxiv.org/abs/2510.02125)
*Claas Beger,Ryan Yi,Shuhao Fu,Arseny Moskvichev,Sarah W. Tsai,Sivasankaran Rajamanickam,Melanie Mitchell*

Main category: cs.AI

TL;DR: 研究模型在ConceptARC上的抽象能力，发现仅用准确率评估抽象推理能力存在高估文本模态、低估视觉模态的问题，提出评估框架。


<details>
  <summary>Details</summary>
Motivation: 探究先进模型是否能按任务设计者意图识别和推理抽象概念。

Method: 在不同输入模态、是否使用外部工具和推理工作量等设置下评估模型，进行输出准确率和自然语言规则的细粒度评估。

Result: 文本模态中部分模型准确率高但规则多基于表面捷径；视觉模态准确率下降但规则仍能体现抽象概念。

Conclusion: 模型抽象推理能力仍落后于人类，评估框架能更准确反映多模态模型抽象推理能力。

Abstract: OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI
benchmark, but does that mean state-of-the-art models recognize and reason with
the abstractions that the task creators intended? We investigate models'
abstraction abilities on ConceptARC. We evaluate models under settings that
vary the input modality (textual vs. visual), whether the model is permitted to
use external Python tools, and, for reasoning models, the amount of reasoning
effort. In addition to measuring output accuracy, we perform fine-grained
evaluation of the natural-language rules that models generate to explain their
solutions. This dual evaluation lets us assess whether models solve tasks using
the abstractions ConceptARC was designed to elicit, rather than relying on
surface-level patterns. Our results show that, while some models using
text-based representations match human output accuracy, the best models' rules
are often based on surface-level ``shortcuts'' and capture intended
abstractions far less often than humans. Thus their capabilities for general
abstract reasoning may be overestimated by evaluations based on accuracy alone.
In the visual modality, AI models' output accuracy drops sharply, yet our
rule-level analysis reveals that models might be underestimated, as they still
exhibit a substantial share of rules that capture intended abstractions, but
are often unable to correctly apply these rules. In short, our results show
that models still lag humans in abstract reasoning, and that using accuracy
alone to evaluate abstract reasoning on ARC-like tasks may overestimate
abstract-reasoning capabilities in textual modalities and underestimate it in
visual modalities. We believe that our evaluation framework offers a more
faithful picture of multimodal models' abstract reasoning abilities and a more
principled way to track progress toward human-like, abstraction-centered
intelligence.

</details>


### [45] [FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic Documents for Training Document Understanding Models](https://arxiv.org/abs/2510.02133)
*Karan Dua,Hitesh Laxmichand Patel,Puneet Mittal,Ranjeet Gupta,Amit Agarwal,Praneet Pabolu,Srikant Panda,Hansa Meghwani,Graham Horwood,Fahad Shah*

Main category: cs.AI

TL;DR: 提出FlexDoc框架解决企业级文档理解模型数据收集成本高的问题，实验显示其能提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 企业级文档理解模型需要大量多样且标注好的数据，但收集此类数据因隐私、法律和标注成本过高。

Method: 引入FlexDoc框架，结合随机模式和参数化采样生成带丰富注释的多语言半结构化文档。

Result: 在关键信息提取任务中，FlexDoc生成的数据可使F1分数最高提升11%，相比传统方法减少超90%的标注工作。

Conclusion: 该方案已投入使用，能加速企业级文档理解模型开发，显著降低数据获取和标注成本。

Abstract: Developing document understanding models at enterprise scale requires large,
diverse, and well-annotated datasets spanning a wide range of document types.
However, collecting such data is prohibitively expensive due to privacy
constraints, legal restrictions, and the sheer volume of manual annotation
needed - costs that can scale into millions of dollars. We introduce FlexDoc, a
scalable synthetic data generation framework that combines Stochastic Schemas
and Parameterized Sampling to produce realistic, multilingual semi-structured
documents with rich annotations. By probabilistically modeling layout patterns,
visual structure, and content variability, FlexDoc enables the controlled
generation of diverse document variants at scale. Experiments on Key
Information Extraction (KIE) tasks demonstrate that FlexDoc-generated data
improves the absolute F1 Score by up to 11% when used to augment real datasets,
while reducing annotation effort by over 90% compared to traditional
hard-template methods. The solution is in active deployment, where it has
accelerated the development of enterprise-grade document understanding models
while significantly reducing data acquisition and annotation costs.

</details>


### [46] [A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports](https://arxiv.org/abs/2510.02190)
*Yang Yao,Yixu Wang,Yuxuan Zhang,Yi Lu,Tianle Gu,Lingyu Li,Dingyi Zhao,Keming Wu,Haozhe Wang,Ping Nie,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 本文介绍适用于深度研究代理（DRAs）和报告式响应的严格基准和多维评估框架，实验证实主流DRAs性能优越但仍有改进空间，为DRA系统发展提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估维度、响应格式和评分机制上存在不足，无法有效评估DRAs。

Method: 引入严格基准，包含214个专家策划的挑战性查询，分布在10个主题领域，每个查询有手动构建的参考包；采用多维评估框架，对DRAs生成的长报告进行综合评估。

Result: 主流DRAs性能优于网络搜索工具增强的推理模型，但仍有很大改进空间。

Conclusion: 本研究为DRA系统的能力评估、架构改进和范式发展提供了坚实基础。

Abstract: Artificial intelligence is undergoing the paradigm shift from closed language
models to interconnected agent systems capable of external perception and
information integration. As a representative embodiment, Deep Research Agents
(DRAs) systematically exhibit the capabilities for task decomposition,
cross-source retrieval, multi-stage reasoning, and structured output, which
markedly enhance performance on complex and open-ended tasks. However, existing
benchmarks remain deficient in evaluation dimensions, response formatting, and
scoring mechanisms, limiting their capacity to assess such systems effectively.
This paper introduces a rigorous benchmark and a multidimensional evaluation
framework tailored to DRAs and report-style responses. The benchmark comprises
214 expert-curated challenging queries distributed across 10 broad thematic
domains, each accompanied by manually constructed reference bundles to support
composite evaluation. The framework enables comprehensive evaluation of
long-form reports generated by DRAs, incorporating integrated scoring metrics
for semantic quality, topical focus, and retrieval trustworthiness. Extensive
experimentation confirms the superior performance of mainstream DRAs over
web-search-tool-augmented reasoning models, yet reveals considerable scope for
further improvement. This study provides a robust foundation for capability
assessment, architectural refinement, and paradigm advancement in DRA systems.

</details>


### [47] [UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models](https://arxiv.org/abs/2510.02194)
*Yuhao Sun,Zhuoer Xu,Shiwen Cui,Kun Yang,Lingyun Yu,Yongdong Zhang,Hongtao Xie*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) have achieved remarkable progress across a wide
range of tasks, but remain vulnerable to safety risks such as harmful content
generation and jailbreak attacks. Existing safety techniques -- including
external guardrails, inference-time guidance, and post-training alignment --
each face limitations in balancing safety, utility, and controllability. In
this work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLM
safety through safety-aware upcycling. Our approach first identifies
safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)
structure, where the router acts as a soft guardrail that selectively activates
original MLPs and added safety experts. We further introduce a two-stage SFT
strategy to strengthen safety discrimination while preserving general
capabilities. To enable flexible control at inference time, we introduce a
safety temperature mechanism, allowing dynamic adjustment of the trade-off
between safety and utility. Experiments across multiple benchmarks, base model,
and model scales demonstrate that UpSafe$^\circ$C achieves robust safety
improvements against harmful and jailbreak inputs, while maintaining
competitive performance on general tasks. Moreover, analysis shows that safety
temperature provides fine-grained inference-time control that achieves the
Pareto-optimal frontier between utility and safety. Our results highlight a new
direction for LLM safety: moving from static alignment toward dynamic, modular,
and inference-aware control.

</details>


### [48] [The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models](https://arxiv.org/abs/2510.02230)
*Phuc Minh Nguyen,Chinh D. La,Duy M. H. Nguyen,Nitesh V. Chawla,Binh T. Nguyen,Khoa D. Doan*

Main category: cs.AI

TL;DR: 本文研究RLVR推理边界收缩问题，揭示负干扰和赢家通吃现象，提出聚焦低概率问题的数据筛选算法提升Pass@k性能。


<details>
  <summary>Details</summary>
Motivation: RLVR本用于提升大语言模型推理能力，但可能缩小推理边界，需探究收缩问题。

Method: 分析RLVR学习动态，开展理论和实证分析，提出聚焦低概率问题的数据筛选算法。

Result: 揭示负干扰和赢家通吃现象，算法提升了Pass@k性能。

Conclusion: 提出的数据筛选算法能有效解决RLVR推理边界收缩问题，提高模型性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key
method for improving Large Language Models' reasoning capabilities, yet recent
evidence suggests it may paradoxically shrink the reasoning boundary rather
than expand it. This paper investigates the shrinkage issue of RLVR by
analyzing its learning dynamics and reveals two critical phenomena that explain
this failure. First, we expose negative interference in RLVR, where learning to
solve certain training problems actively reduces the likelihood of correct
solutions for others, leading to the decline of Pass@$k$ performance, or the
probability of generating a correct solution within $k$ attempts. Second, we
uncover the winner-take-all phenomenon: RLVR disproportionately reinforces
problems with high likelihood, correct solutions, under the base model, while
suppressing other initially low-likelihood ones. Through extensive theoretical
and empirical analysis on multiple mathematical reasoning benchmarks, we show
that this effect arises from the inherent on-policy sampling in standard RL
objectives, causing the model to converge toward narrow solution strategies.
Based on these insights, we propose a simple yet effective data curation
algorithm that focuses RLVR learning on low-likelihood problems, achieving
notable improvement in Pass@$k$ performance. Our code is available at
https://github.com/mail-research/SELF-llm-interference.

</details>


### [49] [The Unreasonable Effectiveness of Scaling Agents for Computer Use](https://arxiv.org/abs/2510.02250)
*Gonzalo Gonzalez-Pumariega,Vincent Tu,Chih-Lun Lee,Jiachen Yang,Ang Li,Xin Eric Wang*

Main category: cs.AI

TL;DR: 提出Behavior Best-of-N (bBoN)方法提升计算机使用代理（CUAs）处理复杂任务的可靠性和成功率，在多个任务上取得新的最优结果。


<details>
  <summary>Details</summary>
Motivation: CUAs在处理长期复杂任务时存在不可靠和高方差问题，阻碍其应用。

Method: 提出bBoN方法，通过生成多个轨迹并使用行为叙述选择，实现广泛探索和轨迹选择。

Result: 在OSWorld上达到69.9%的新最优成绩，接近人类水平；在WindowsAgentArena和AndroidWorld上有良好泛化结果。

Conclusion: 正确扩展CUAs需要结构化轨迹理解和选择，bBoN提供了实用框架。

Abstract: Computer-use agents (CUAs) hold promise for automating everyday digital
tasks, but their unreliability and high variance hinder their application to
long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method
that scales over agents by generating multiple rollouts and selecting among
them using behavior narratives that describe the agents' rollouts. It enables
both wide exploration and principled trajectory selection, substantially
improving robustness and success rates. On OSWorld, our bBoN scaling method
establishes a new state of the art (SoTA) at 69.9%, significantly outperforming
prior methods and approaching human-level performance at 72%, with
comprehensive ablations validating key design choices. We further demonstrate
strong generalization results to different operating systems on
WindowsAgentArena and AndroidWorld. Crucially, our results highlight the
unreasonable effectiveness of scaling CUAs, when you do it right: effective
scaling requires structured trajectory understanding and selection, and bBoN
provides a practical framework to achieve this.

</details>


### [50] [RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems](https://arxiv.org/abs/2510.02263)
*Yuxiao Qu,Anikait Singh,Yoonho Lee,Amrith Setlur,Ruslan Salakhutdinov,Chelsea Finn,Aviral Kumar*

Main category: cs.AI

TL;DR: 引入推理抽象概念，提出RLAD训练范式以实现更有效的推理，提升模型在难题上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大模型推理痕迹难以持续捕捉或复用程序，需更有效的推理方法。

Method: 引入推理抽象，训练模型提出多个抽象，采用两玩家RL训练范式（RLAD）联合训练抽象生成器和解决方案生成器。

Result: 有效实现结构化探索，解耦抽象提议和解决方案生成的学习信号，提升对难题的泛化能力，在测试时分配更多计算资源生成抽象对性能更有益。

Conclusion: 推理抽象和RLAD范式有助于实现更有效的推理，且抽象在引导有意义探索中发挥重要作用。

Abstract: Reasoning requires going beyond pattern matching or memorization of solutions
to identify and implement "algorithmic procedures" that can be used to deduce
answers to hard problems. Doing so requires realizing the most relevant
primitives, intermediate results, or shared procedures, and building upon them.
While RL post-training on long chains of thought ultimately aims to uncover
this kind of algorithmic behavior, most reasoning traces learned by large
models fail to consistently capture or reuse procedures, instead drifting into
verbose and degenerate exploration. To address more effective reasoning, we
introduce reasoning abstractions: concise natural language descriptions of
procedural and factual knowledge that guide the model toward learning
successful reasoning. We train models to be capable of proposing multiple
abstractions given a problem, followed by RL that incentivizes building a
solution while using the information provided by these abstractions. This
results in a two-player RL training paradigm, abbreviated as RLAD, that jointly
trains an abstraction generator and a solution generator. This setup
effectively enables structured exploration, decouples learning signals of
abstraction proposal and solution generation, and improves generalization to
harder problems. We also show that allocating more test-time compute to
generating abstractions is more beneficial for performance than generating more
solutions at large test budgets, illustrating the role of abstractions in
guiding meaningful exploration.

</details>


### [51] [BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals](https://arxiv.org/abs/2510.02276)
*Chenqi Li,Yu Liu,Timothy Denison,Tingting Zhu*

Main category: cs.AI

TL;DR: 本文提出用于生物信号无监督跨模态知识迁移的BioX - Bridge框架，减少可训练参数并保持或提升迁移性能。


<details>
  <summary>Details</summary>
Motivation: 生物信号跨模态知识迁移有需求，但现有基于知识蒸馏的方法计算和内存开销大，大模型问题更严重。

Method: 训练轻量级桥接网络对齐中间表示，提出选择对齐位置的策略和灵活原型网络作为桥接架构。

Result: 在多模态、多任务和多数据集实验中，BioX - Bridge减少88 - 99%可训练参数，保持或提升迁移性能。

Conclusion: 所提BioX - Bridge框架在生物信号无监督跨模态知识迁移中有效，能降低参数数量并保证性能。

Abstract: Biosignals offer valuable insights into the physiological states of the human
body. Although biosignal modalities differ in functionality, signal fidelity,
sensor comfort, and cost, they are often intercorrelated, reflecting the
holistic and interconnected nature of human physiology. This opens up the
possibility of performing the same tasks using alternative biosignal
modalities, thereby improving the accessibility, usability, and adaptability of
health monitoring systems. However, the limited availability of large labeled
datasets presents challenges for training models tailored to specific tasks and
modalities of interest. Unsupervised cross-modal knowledge transfer offers a
promising solution by leveraging knowledge from an existing modality to support
model training for a new modality. Existing methods are typically based on
knowledge distillation, which requires running a teacher model alongside
student model training, resulting in high computational and memory overhead.
This challenge is further exacerbated by the recent development of foundation
models that demonstrate superior performance and generalization across tasks at
the cost of large model sizes. To this end, we explore a new framework for
unsupervised cross-modal knowledge transfer of biosignals by training a
lightweight bridge network to align the intermediate representations and enable
information flow between foundation models and across modalities. Specifically,
we introduce an efficient strategy for selecting alignment positions where the
bridge should be constructed, along with a flexible prototype network as the
bridge architecture. Extensive experiments across multiple biosignal
modalities, tasks, and datasets show that BioX-Bridge reduces the number of
trainable parameters by 88--99\% while maintaining or even improving transfer
performance compared to state-of-the-art methods.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [52] [Utilizing Modern Large Language Models (LLM) for Financial Trend Analysis and Digest Creation](https://arxiv.org/abs/2510.01225)
*Andrei Lazarev,Dmitrii Sedov*

Main category: cs.CE

TL;DR: 论文介绍利用大语言模型Gemini Pro自动生成金融摘要的创新框架，阐述过程并提供GitHub链接。


<details>
  <summary>Details</summary>
Motivation: 信息指数级增长，研究者和专业人士难以紧跟领域前沿，传统分析方法有局限。

Method: 结合从OpenAlex提取数据、策略性提示工程和大语言模型驱动分析，包含数据获取、JSON构建等步骤。

Result: 实现了自动生成概括关键发现、识别新兴趋势的全面摘要，可高效处理大量非结构化数据并提供易理解的可行见解。

Conclusion: 大语言模型可帮助研究者和学者节省时间、了解当前趋势，论文提供项目GitHub链接以便进一步开发。

Abstract: The exponential growth of information presents a significant challenge for
researchers and professionals seeking to remain at the forefront of their
fields and this paper introduces an innovative framework for automatically
generating insightful financial digests using the power of Large Language
Models (LLMs), specifically Google's Gemini Pro. By leveraging a combination of
data extraction from OpenAlex, strategic prompt engineering, and LLM-driven
analysis, we demonstrate the automated example of creating a comprehensive
digests that generalize key findings, identify emerging trends. This approach
addresses the limitations of traditional analysis methods, enabling the
efficient processing of vast amounts of unstructured data and the delivery of
actionable insights in an easily digestible format. This paper describes how
LLMs work in simple words and how we can use their power to help researchers
and scholars save their time and stay informed about current trends. Our study
includes step-by-step process, from data acquisition and JSON construction to
interaction with Gemini and the automated generation of PDF reports, including
a link to the project's GitHub repository for broader accessibility and further
development.

</details>


### [53] [CardioRAG: A Retrieval-Augmented Generation Framework for Multimodal Chagas Disease Detection](https://arxiv.org/abs/2510.01558)
*Zhengyang Shen,Xuehao Zhai,Hua Tu,Mayue Shi*

Main category: cs.CE

TL;DR: 提出CardioRAG框架用于查加斯病诊断，评估表现良好，对资源有限地区有价值。


<details>
  <summary>Details</summary>
Motivation: 查加斯病影响大，现有机器学习方法诊断查加斯病有局限，需新的诊断方案。

Method: 提出CardioRAG框架，整合大语言模型与可解释的基于心电图的临床特征，用变分自编码器学习的表示进行语义病例检索。

Result: 评估显示召回率达89.80%，最大F1分数为0.68，能有效识别需优先血清学检测的阳性病例。

Conclusion: CardioRAG是基于临床证据的可解释方法，对资源有限地区有价值，为将临床指标嵌入可信医疗AI系统提供了途径。

Abstract: Chagas disease affects nearly 6 million people worldwide, with Chagas
cardiomyopathy representing its most severe complication. In regions where
serological testing capacity is limited, AI-enhanced electrocardiogram (ECG)
screening provides a critical diagnostic alternative. However, existing machine
learning approaches face challenges such as limited accuracy, reliance on large
labeled datasets, and more importantly, weak integration with evidence-based
clinical diagnostic indicators. We propose a retrieval-augmented generation
framework, CardioRAG, integrating large language models with interpretable
ECG-based clinical features, including right bundle branch block, left anterior
fascicular block, and heart rate variability metrics. The framework uses
variational autoencoder-learned representations for semantic case retrieval,
providing contextual cases to guide clinical reasoning. Evaluation demonstrated
high recall performance of 89.80%, with a maximum F1 score of 0.68 for
effective identification of positive cases requiring prioritized serological
testing. CardioRAG provides an interpretable, clinical evidence-based approach
particularly valuable for resource-limited settings, demonstrating a pathway
for embedding clinical indicators into trustworthy medical AI systems.

</details>


### [54] [ShapeGen3DCP: A Deep Learning Framework for Layer Shape Prediction in 3D Concrete Printing](https://arxiv.org/abs/2510.02009)
*Giacomo Rizzieri,Federico Lanteri,Liberato Ferrara,Massimiliano Cremonesi*

Main category: cs.CE

TL;DR: 介绍ShapeGen3DCP框架用于3D混凝土打印中细丝横截面几何形状的快速准确预测。


<details>
  <summary>Details</summary>
Motivation: 解决3D混凝土打印中细丝横截面几何形状预测问题，克服实验数据稀缺。

Method: 基于神经网络架构，输入材料和工艺参数，将部分输入转化为无量纲参数，用傅里叶描述符表示预测几何形状，使用PFEM模型生成训练数据集。

Result: 与多种数值和实验案例验证结果高度吻合，证实框架的准确性和可靠性。

Conclusion: 该框架可用于打印设置预校准、刀具路径优化等，未来可与模拟和传感器反馈结合实现闭环数字孪生。

Abstract: This work introduces ShapeGen3DCP, a deep learning framework for fast and
accurate prediction of filament cross-sectional geometry in 3D Concrete
Printing (3DCP). The method is based on a neural network architecture that
takes as input both material properties in the fluid state (density, yield
stress, plastic viscosity) and process parameters (nozzle diameter, nozzle
height, printing and flow velocities) to directly predict extruded layer
shapes. To enhance generalization, some inputs are reformulated into
dimensionless parameters that capture underlying physical principles. Predicted
geometries are compactly represented using Fourier descriptors, which enforce
smooth, closed, and symmetric profiles while reducing the prediction task to a
small set of coefficients. The training dataset was synthetically generated
using a well-established Particle Finite Element (PFEM) model of 3DCP,
overcoming the scarcity of experimental data. Validation against diverse
numerical and experimental cases shows strong agreement, confirming the
framework's accuracy and reliability. This opens the way to practical uses
ranging from pre-calibration of print settings, minimizing or even eliminating
trial-and-error adjustments, to toolpath optimization for more advanced
designs. Looking ahead, coupling the framework with simulations and sensor
feedback could enable closed-loop digital twins for 3DCP, driving real-time
process optimization, defect detection, and adaptive control of printing
parameters.

</details>


### [55] [A Copula-Based Variational Autoencoder for Uncertainty Quantification in Inverse Problems: Application to Damage Identification in an Offshore Wind Turbine](https://arxiv.org/abs/2510.02013)
*Ana Fernandez-Navamuel,Matteo Croci,Martin Alberto Diaz Viera*

Main category: cs.CE

TL;DR: 本文提出基于Copula的变分自编码器（VAE）架构用于浮式海上风力涡轮机系泊系统损伤识别，在高维空间表现良好。


<details>
  <summary>Details</summary>
Motivation: 从有限传感器数据中识别浮式海上风力涡轮机系泊系统等部件的损伤是具有挑战性的逆问题，传统VAE中使用的高斯混合模型有局限性。

Method: 提出基于Copula的VAE架构，将变量的边缘分布与依赖结构解耦，对比三种近似后验分布的方法。

Result: 在高保真合成数据集上的分析表明，Copula VAE在高维空间是有前景且易处理的解决方案，参数更少、性能更优。

Conclusion: 基于Copula的VAE有潜力作为浮式海上风力涡轮机系泊系统不确定性感知损伤识别的工具。

Abstract: Structural Health Monitoring of Floating Offshore Wind Turbines (FOWTs) is
critical for ensuring operational safety and efficiency. However, identifying
damage in components like mooring systems from limited sensor data poses a
challenging inverse problem, often characterized by multimodal solutions where
various damage states could explain the observed response. To overcome it, we
propose a Variational Autoencoder (VAE) architecture, where the encoder
approximates the inverse operator, while the decoder approximates the forward.
The posterior distribution of the latent space variables is probabilistically
modeled, describing the uncertainties in the estimates. This work tackles the
limitations of conventional Gaussian Mixtures used within VAEs, which can be
either too restrictive or computationally prohibitive for high-dimensional
spaces. We propose a novel Copula-based VAE architecture that decouples the
marginal distribution of the variables from their dependence structure,
offering a flexible method for representing complex, correlated posterior
distributions. We provide a comprehensive comparison of three different
approaches for approximating the posterior: a Gaussian Mixture with a diagonal
covariance matrix, a Gaussian Mixture with a full covariance matrix, and a
Gaussian Copula. Our analysis, conducted on a high-fidelity synthetic dataset,
demonstrates that the Copula VAE offers a promising and tractable solution in
high-dimensional spaces. Although the present work remains in the
two-dimensional space, the results suggest efficient scalability to higher
dimensions. It achieves superior performance with significantly fewer
parameters than the Gaussian Mixture alternatives, whose parametrization grows
prohibitively with the dimensionality. The results underscore the potential of
Copula-based VAEs as a tool for uncertainty-aware damage identification in FOWT
mooring systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [56] [Odontoceti: Ultra-Fast DAG Consensus with Two Round Commitment](https://arxiv.org/abs/2510.01216)
*Preston Vander Vos*

Main category: cs.DC

TL;DR: Odontoceti是基于DAG的共识协议，优先考虑低延迟和高吞吐量，以20%容错换取性能，两轮通信实现提交，性能优于现有协议，证明低容错区块链共识协议可行。


<details>
  <summary>Details</summary>
Motivation: 解决区块链用户对可扩展性、快速确认和即时交易处理的需求。

Method: 采用n = 5f + 1验证器，用新决策规则创建未认证DAG，包含参与者缓慢时促进进度的优化。

Result: 在现实网络条件下实现300毫秒中位延迟，每秒处理10000笔交易，比现有生产协议延迟改善20 - 25%。

Conclusion: 证明了区块链低容错共识协议的实际可行性。

Abstract: Users of blockchains value scalability, expecting fast confirmations and
immediate transaction processing. Odontoceti, the latest in DAG-based
consensus, addresses these concerns by prioritizing low latency and high
throughput, making a strategic trade-off in security by operating with a 20%
fault tolerance instead of the established 33% level. It is the first DAG-based
protocol to achieve commitment in just two communication rounds, delivering
median latency of 300 milliseconds while processing 10,000 transactions per
second under realistic network conditions. Odontoceti operates with n = 5f + 1
validators and creates an uncertified DAG with a novel decision rule for
committing blocks. The protocol includes an optimization that advances progress
when participants are slow, benefiting crash fault scenarios which are more
common in practice than Byzantine faults. Evaluation results demonstrate 20-25%
latency improvements compared to an existing production protocol, validating
that reducing wave length from three rounds to two rounds yields meaningful
performance benefits. This paper establishes the practical viability of lower
fault tolerance consensus protocols for blockchains.

</details>


### [57] [Kant: An Efficient Unified Scheduling System for Large-Scale AI Clusters](https://arxiv.org/abs/2510.01256)
*Lingling Zeng,Gen Zhang,Jialin Peng,Xiang Xu,Yuan Xu,Lijun Ma*

Main category: cs.DC

TL;DR: 本文提出面向大规模AI容器集群的高效统一调度平台Kant，定义评估指标，实验表明其性能优异，已部署支持大规模计算，提供实用工程方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI集群规模扩大和LLM训练推理需求增长，传统调度系统难以平衡资源利用、调度效率和服务质量。

Method: 提出Kant系统，基于实践定义一系列AI集群评估指标，采用Backfill和Enhanced Binpack等调度策略。

Result: Kant在数百到数万个GPU集群中表现优异，提升资源利用率和调度效率，减少资源碎片化和通信开销，已在多个数据中心集群部署。

Conclusion: 为构建高性能、高可用、AI原生调度基础设施提供实用工程方法。

Abstract: As AI cluster sizes continue to expand and the demand for
large-language-model (LLM) training and inference workloads grows rapidly,
traditional scheduling systems face significant challenges in balancing
resource utilization, scheduling efficiency, and service quality. This paper
presents and evaluates Kant: an efficient unified scheduling platform designed
for large-scale AI container clusters, supporting the co-scheduling of both
training and inference jobs. Based on the practical implementation of the Kant
system, we systematically define a set of key evaluation metrics for AI
clusters, including GPU Allocation Ratio (GAR), Scheduling Occupancy Rate
(SOR), GPU Node Fragmentation Ratio (GFR), Job Waiting Time Distribution
(JWTD), and Job Training Time Estimation Distribution (JTTED), providing a
foundation for quantitative performance analysis. Experimental results
demonstrate that Kant achieves exceptional performance in clusters ranging from
hundreds to tens of thousands of GPUs. By leveraging scheduling strategies such
as Backfill and Enhanced Binpack (E-Binpack), the system significantly improves
resource utilization and scheduling efficiency, while effectively reducing
resource fragmentation and communication overhead in distributed training. The
system has been deployed in multiple AI data center clusters, where it stably
supports large-scale intelligent computing workloads. This work provides a
practical engineering approach for building high-performance, highly available,
AI-native scheduling infrastructure.

</details>


### [58] [IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol](https://arxiv.org/abs/2510.01260)
*Ningyuan Yang,Guanliang Lyu,Mingchen Ma,Yiyi Lu,Yiming Li,Zhihui Gao,Hancheng Ye,Jianyi Zhang,Tingjun Chen,Yiran Chen*

Main category: cs.DC

TL;DR: 提出IoT - MCP框架实现MCP以连接大语言模型和物联网生态，引入IoT - MCP Bench进行评估，实验验证其性能并提供开源框架和评估方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型与物联网系统集成面临硬件异构和控制复杂性挑战，需要标准化通信。

Method: 提出IoT - MCP框架，通过边缘部署服务器实现MCP；引入IoT - MCP Bench进行评估。

Result: 在22种传感器类型和6种微控制器单元实验中，任务成功率达100%，平均响应时间205ms，峰值内存占用74KB。

Conclusion: 提供了大语言模型与物联网系统的开源集成框架和标准化评估方法。

Abstract: The integration of Large Language Models (LLMs) with Internet-of-Things (IoT)
systems faces significant challenges in hardware heterogeneity and control
complexity. The Model Context Protocol (MCP) emerges as a critical enabler,
providing standardized communication between LLMs and physical devices. We
propose IoT-MCP, a novel framework that implements MCP through edge-deployed
servers to bridge LLMs and IoT ecosystems. To support rigorous evaluation, we
introduce IoT-MCP Bench, the first benchmark containing 114 Basic Tasks (e.g.,
``What is the current temperature?'') and 1,140 Complex Tasks (e.g., ``I feel
so hot, do you have any ideas?'') for IoT-enabled LLMs. Experimental validation
across 22 sensor types and 6 microcontroller units demonstrates IoT-MCP's 100%
task success rate to generate tool calls that fully meet expectations and
obtain completely accurate results, 205ms average response time, and 74KB peak
memory footprint. This work delivers both an open-source integration framework
(https://github.com/Duke-CEI-Center/IoT-MCP-Servers) and a standardized
evaluation methodology for LLM-IoT systems.

</details>


### [59] [QScale: Probabilistic Chained Consensus for Moderate-Scale Systems](https://arxiv.org/abs/2510.01536)
*Hasan Heydari,Alysson Bessani,Kartik Nayak*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Existing distributed ledger protocols either incur a high communication
complexity and are thus suited to systems with a small number of processes
(e.g., PBFT), or rely on committee-sampling-based approaches that only work for
a very large number of processes (e.g., Algorand). Neither of these lines of
work is well-suited for moderate-scale distributed ledgers ranging from a few
hundred to a thousand processes, which are common in production (e.g, Redbelly,
Sui). The goal of this work is to design a distributed ledger with sub-linear
communication complexity per process, sub-quadratic total communication
complexity, and low latency for finalizing a block into the ledger, such that
it can be used for moderate-scale systems. We propose QScale, a protocol in
which every process incurs only $\widetilde{O}(\kappa \sqrt{n})$ communication
complexity per-block in expectation, $\widetilde{O}(n\kappa)$ total
communication complexity per-block in expectation, and a best-case latency of
$O(\kappa)$ rounds while ensuring safety and liveness with overwhelming
probability, with $\kappa$ being a small security parameter.

</details>


### [60] [Accuracy vs Performance: An abstraction model for deadline constrained offloading at the mobile-edge](https://arxiv.org/abs/2510.01885)
*Jamie Cotter,Ignacio Castineiras,Victor Cionca*

Main category: cs.DC

TL;DR: 提出移动边缘设备低延迟、期限约束的DNN卸载方案，设计调度算法并在系统实现评估，新模型在高负载下性能更好。


<details>
  <summary>Details</summary>
Motivation: 解决移动边缘设备低延迟、期限约束的DNN卸载问题。

Method: 设计考虑设备可用性、网络通信、优先级抢占和任务期限的调度算法，包括资源可用性表示、网络离散化和动态带宽估计机制，并在树莓派系统实现。

Result: 新的低延迟抽象模型在高负载下性能更好，动态带宽估计辅助任务放置，在资源稀缺时提高任务吞吐量。

Conclusion: 所提方案在移动边缘设备DNN卸载中有较好效果，能应对高负载和资源稀缺情况。

Abstract: In this paper, we present a solution for low-latency deadline-constrained DNN
offloading on mobile edge devices. We design a scheduling algorithm with
lightweight network state representation, considering device availability,
communication on the network link, priority-aware pre-emption, and task
deadlines. The scheduling algorithm aims to reduce latency by designing a
resource availability representation, as well as a network discretisation and a
dynamic bandwidth estimation mechanism. We implement the scheduling algorithm
into a system composed of four Raspberry Pi 2 (model Bs) mobile edge devices,
sampling a waste classification conveyor belt at a set frame rate. The system
is evaluated and compared to a previous approach of ours, which was proven to
outcompete work-stealers and a non-pre-emption based scheduling heuristic under
the aforementioned waste classification scenario. Our findings show the novel
lower latency abstraction models yield better performance under high-volume
workloads, with the dynamic bandwidth estimation assisting the task placement
while, ultimately, increasing task throughput in times of resource scarcity.

</details>


### [61] [Programming RISC-V accelerators via Fortran](https://arxiv.org/abs/2510.02170)
*Nick Brown,Jake Davies,Felix LeClair*

Main category: cs.DC

TL;DR: 提出通过Fortran驱动RISC - V加速器架构以避免代码重新开发的方法


<details>
  <summary>Details</summary>
Motivation: RISC - V加速器用于HPC工作负载时需重写代码，而科学计算中很多模拟代码复杂且用Fortran编写，重写不现实

Method: 提出一种通过Fortran驱动RISC - V加速器架构的方法

Result: 未提及

Conclusion: 该方法可避免代码重新开发

Abstract: A range of RISC-V based accelerators are available and coming to market, and
there is strong potential for these to be used for High Performance Computing
(HPC) workloads. However, such accelerators tend to provide bespoke programming
models and APIs that require codes to be rewritten. In scientific computing,
where many of the simulation code are highly complex, extensive, and written in
Fortran, this is not realistic. In this extended abstract we present an
approach that enables driving such architectures via Fortran, avoiding code
redevelopment.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [62] [The Steiner Path Aggregation Problem](https://arxiv.org/abs/2510.01392)
*Da Qi Chen,Daniel Hathcock,D Ellis Hershkowitz,R. Ravi*

Main category: cs.DS

TL;DR: 针对Steiner路径聚合问题，给出高效算法找到最多2log_{4/3}k次颜色切换的解，该解在常数因子下是最优通用界。


<details>
  <summary>Details</summary>
Motivation: 在有向网络中将路径聚合成单棵有向树，且不显著破坏路径，即让每个终端到根的路径颜色切换次数不多。

Method: 设计了一种高效算法。

Result: 算法能找到最多2log_{4/3}k次颜色切换的解。

Conclusion: 该算法在常数因子下是最好的通用界，因为存在图至少需要log_2 k次颜色切换。

Abstract: In the Steiner Path Aggregation Problem, our goal is to aggregate paths in a
directed network into a single arborescence without significantly disrupting
the paths. In particular, we are given a directed multigraph with colored arcs,
a root, and $k$ terminals, each of which has a monochromatic path to the root.
Our goal is to find an arborescence in which every terminal has a path to the
root, and its path does not switch colors too many times. We give an efficient
algorithm that finds such a solution with at most $2\log_{4/3}k$ color
switches. Up to constant factors this is the best possible universal bound, as
there are graphs requiring at least $\log_2 k$ color switches.

</details>


### [63] [Foremost, Fastest, Shortest: Temporal Graph Realization under Various Path Metrics](https://arxiv.org/abs/2510.01702)
*Justine Cauvi,Nils Morawietz,Laurent Viennot*

Main category: cs.DS

TL;DR: 本文研究给定矩阵确定是否存在满足特定属性的时态图问题，分析不同设置下复杂度，对最早到达时间、最快路径和最短路径问题取得成果。


<details>
  <summary>Details</summary>
Motivation: 跟随时态图实现的趋势，研究给定属性确定是否存在满足该属性的时态图问题。

Method: 分析不同设置下问题，如严格和非严格路径、周期性和非周期性时态图等。

Result: 最早到达时间在多种情况下可多项式时间求解，矩阵含集合或范围值时部分情况可FPT算法求解；最快路径取得新的硬度结果；最短路径周期性版本可多项式求解，非周期性版本NP难。

Conclusion: 不同路径指标在不同设置下复杂度不同，部分情况可高效求解。

Abstract: In this work, we follow the current trend on temporal graph realization,
where one is given a property P and the goal is to determine whether there is a
temporal graph, that is, a graph where the edge set changes over time, with
property P . We consider the problems where as property P , we are given a
prescribed matrix for the duration, length, or earliest arrival time of
pairwise temporal paths. That is, we are given a matrix D and ask whether there
is a temporal graph such that for any ordered pair of vertices (s, t), Ds,t
equals the duration (length, or earliest arrival time, respectively) of any
temporal path from s to t minimizing that specific temporal path metric. For
shortest and earliest arrival temporal paths, we are the first to consider
these problems as far as we know. We analyze these problems for many settings
like: strict and non-strict paths, periodic and non-periodic temporal graphs,
and limited number of labels per edge (that is, limited occurrence number per
edge over time). In contrast to all other path metrics, we show that for the
earliest arrival times, we can achieve polynomial-time algorithms in periodic
and non-periodic temporal graphs and for strict and and non-strict paths.
However, the problem becomes NP-hard when the matrix does not contain a single
integer but a set or range of possible allowed values. As we show, the problem
can still be solved efficiently in this scenario, when the number of entries
with more than one value is small, that is, we develop an FPT-algorithm for the
number of such entries. For the setting of fastest paths, we achieve new
hardness results that answers an open question by Klobas, Mertzios, Molter, and
Spirakis [Theor. Comput. Sci. '25] about the parameterized complexity of the
problem with respect to the vertex cover number and significantly improves over
a previous hardness result for the feedback vertex set number. When considering
shortest paths, we show that the periodic versions are polynomial-time solvable
whereas the non-periodic versions become NP-hard.

</details>


### [64] [Improved $\ell_{p}$ Regression via Iteratively Reweighted Least Squares](https://arxiv.org/abs/2510.01729)
*Alina Ene,Ta Duy Nguyen,Adrian Vladu*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce fast algorithms for solving $\ell_{p}$ regression problems using
the iteratively reweighted least squares (IRLS) method. Our approach achieves
state-of-the-art iteration complexity, outperforming the IRLS algorithm by
Adil-Peng-Sachdeva (NeurIPS 2019) and matching the theoretical bounds
established by the complex algorithm of Adil-Kyng-Peng-Sachdeva (SODA 2019, J.
ACM 2024) via a simpler lightweight iterative scheme. This bridges the existing
gap between theoretical and practical algorithms for $\ell_{p}$ regression. Our
algorithms depart from prior approaches, using a primal-dual framework, in
which the update rule can be naturally derived from an invariant maintained for
the dual objective. Empirically, we show that our algorithms significantly
outperform both the IRLS algorithm by Adil-Peng-Sachdeva and MATLAB/CVX
implementations.

</details>


### [65] [Short circuit walks in fixed dimension](https://arxiv.org/abs/2510.01916)
*Alexander E. Black,Christian Nöbel,Raphael Steiner*

Main category: cs.DS

TL;DR: 本文加强了线性规划中电路增强方案里近似最短单调电路路径问题的硬度结果，证明对有m条边的多边形在O(m^{1 - ε})因子内近似该问题是NP难的。


<details>
  <summary>Details</summary>
Motivation: 此前已有关于高效近似最短单调电路路径的研究，但结果基于组合优化中高维高度退化多面体的归约，本文旨在加强相关硬度结果。

Method: 未明确提及具体方法，推测基于理论推导。

Result: 证明对于每个固定的ε > 0，在有m条边的多边形上以O(m^{1 - ε})因子近似该问题是NP难的，且此结果基本最优。

Conclusion: 此结果意味着在简单多面体和固定维度下该近似问题也具有难度。

Abstract: Circuit augmentation schemes are a family of combinatorial algorithms for
linear programming that generalize the simplex method. To solve the linear
program, they construct a so-called monotone circuit walk: They start at an
initial vertex of the feasible region and traverse a discrete sequence of
points on the boundary, while moving along certain allowed directions
(circuits) and improving the objective function at each step until reaching an
optimum. Since the existence of short circuit walks has been conjectured
(Circuit Diameter Conjecture), several works have investigated how well one can
efficiently approximate shortest monotone circuit walks towards an optimum. A
first result addressing this question was given by De Loera, Kafer, and
Sanit\`a [SIAM J. Opt., 2022], who showed that given as input an LP and the
starting vertex, finding a $2$-approximation for this problem is NP-hard.
Cardinal and the third author [Math. Prog. 2023] gave a stronger lower bound
assuming the exponential time hypothesis, showing that even an approximation
factor of $O(\frac{\log m}{\log \log m})$ is intractable for LPs defined by $m$
inequalities. Both of these results were based on reductions from highly
degenerate polytopes in combinatorial optimization with high dimension.
  In this paper, we significantly strengthen the aforementioned hardness
results by showing that for every fixed $\varepsilon>0$ approximating the
problem on polygons with $m$ edges to within a factor of $O(m^{1-\varepsilon})$
is NP-hard. This result is essentially best-possible, as it cannot be improved
beyond $o(m)$. In particular, this implies hardness for simple polytopes and in
fixed dimension.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [66] [Learning to Play Multi-Follower Bayesian Stackelberg Games](https://arxiv.org/abs/2510.01387)
*Gerson Personnat,Tao Lin,Safwan Hossain,David C. Parkes*

Main category: cs.GT

TL;DR: 研究多跟随者贝叶斯Stackelberg博弈在线学习问题，为领导者设计不同反馈设置下学习算法并给出后悔界和下界。


<details>
  <summary>Details</summary>
Motivation: 解决领导者在追随者类型分布未知情况下，在多轮交互中最小化后悔的问题。

Method: 针对类型反馈和行动反馈两种情况，分别设计学习算法。

Result: 在类型反馈下，独立和一般类型分布分别有相应后悔界；行动反馈下有特定后悔界；给出下界几乎匹配类型反馈上界。

Conclusion: 设计的算法能有效处理多跟随者贝叶斯Stackelberg博弈在线学习问题，后悔界不随n多项式增长。

Abstract: In a multi-follower Bayesian Stackelberg game, a leader plays a mixed
strategy over $L$ actions to which $n\ge 1$ followers, each having one of $K$
possible private types, best respond. The leader's optimal strategy depends on
the distribution of the followers' private types. We study an online learning
version of this problem: a leader interacts for $T$ rounds with $n$ followers
with types sampled from an unknown distribution every round. The leader's goal
is to minimize regret, defined as the difference between the cumulative utility
of the optimal strategy and that of the actually chosen strategies. We design
learning algorithms for the leader under different feedback settings. Under
type feedback, where the leader observes the followers' types after each round,
we design algorithms that achieve $\mathcal O\big(\sqrt{\min\{L\log(nKA T), nK
\} \cdot T} \big)$ regret for independent type distributions and $\mathcal
O\big(\sqrt{\min\{L\log(nKA T), K^n \} \cdot T} \big)$ regret for general type
distributions. Interestingly, those bounds do not grow with $n$ at a polynomial
rate. Under action feedback, where the leader only observes the followers'
actions, we design algorithms with $\mathcal O( \min\{\sqrt{ n^L K^L A^{2L} L T
\log T}, K^n\sqrt{ T } \log T \} )$ regret. We also provide a lower bound of
$\Omega(\sqrt{\min\{L, nK\}T})$, almost matching the type-feedback upper
bounds.

</details>


### [67] [Designing Inferable Signaling Schemes for Bayesian Persuasion](https://arxiv.org/abs/2510.01434)
*Caleb Probine,Mustafa O. Karabag,Ufuk Topcu*

Main category: cs.GT

TL;DR: 研究贝叶斯说服中接收者从重复交互推断信号方案的情况，给出性能损失界和所需样本下界，提出两种设计可推断信号方案的方法并应用于安全警报示例。


<details>
  <summary>Details</summary>
Motivation: 经典贝叶斯说服模型假设接收者知晓承诺，本文研究接收者从重复交互推断方案的情况。

Method: 给出性能损失界和样本下界，提出随机梯度下降（SGD）和基于有限理性接收者模型的优化两种方法。

Result: SGD在低交互场景表现最佳，基于有限理性接收者模型的方法灵活；SGD用于安全警报示例能找到信号更少、使公民最优行动更明显的方案。

Conclusion: 提出的方法可用于设计可推断的信号方案，SGD在特定场景有优势。

Abstract: In Bayesian persuasion, an informed sender, who observes a state, commits to
a randomized signaling scheme that guides a self-interested receiver's actions.
Classical models assume the receiver knows the commitment. We, instead, study
the setting where the receiver infers the scheme from repeated interactions. We
bound the sender's performance loss relative to the known-commitment case by a
term that grows with the signal space size and shrinks as the receiver's
optimal actions become more distinct. We then lower bound the samples required
for the sender to approximately achieve their known-commitment performance in
the inference setting. We show that the sender requires more samples in
persuasion compared to the leader in a Stackelberg game, which includes
commitment but lacks signaling. Motivated by these bounds, we propose two
methods for designing inferable signaling schemes, one being stochastic
gradient descent (SGD) on the sender's inference-setting utility, and the other
being optimization with a boundedly-rational receiver model. SGD performs best
in low-interaction regimes, but modeling the receiver as boundedly-rational and
tuning the rationality constant still provides a flexible method for designing
inferable schemes. Finally, we apply SGD to a safety alert example and show it
to find schemes that have fewer signals and make citizens' optimal actions more
distinct compared to the known-commitment case.

</details>


### [68] [Incentive Analysis of Collusion in Fair Division](https://arxiv.org/abs/2510.01689)
*Haoqiang Huang,Biaoshuai Tao,Mingwei Yang,Shengwei Zhou*

Main category: cs.GT

TL;DR: 研究带策略性参与者的公平分配问题，定义SGIR和GIR衡量合谋操纵收益，刻画MNW、PS和RR机制的SGIR和GIR，揭示三者抗合谋差异。


<details>
  <summary>Details</summary>
Motivation: 先前研究未探索参与者群体的合谋操纵情况，本文旨在填补这一空白。

Method: 定义强组激励比率（SGIR）和组激励比率（GIR），并对MNW、PS和RR机制的SGIR和GIR进行紧密刻画。

Result: MNW的GIR恒为2；MNW和PS的SGIR、PS和RR的GIR为c + 1；RR的SGIR在c≥2时无界。

Conclusion: 三种机制在抗合谋脆弱性上存在根本差异。

Abstract: We study fair division problems with strategic agents capable of gaining
advantages by manipulating their reported preferences. Although several
impossibility results have revealed the incompatibility of truthfulness with
standard fairness criteria, subsequent works have circumvented this limitation
through the incentive ratio framework. Previous studies demonstrate that
fundamental mechanisms like Maximum Nash Welfare (MNW) and Probabilistic Serial
(PS) for divisible goods, and Round-Robin (RR) for indivisible goods achieve an
incentive ratio of $2$, implying that no individual agent can gain more than
double his truthful utility through manipulation. However, collusive
manipulation by agent groups remains unexplored.
  In this work, we define strong group incentive ratio (SGIR) and group
incentive ratio (GIR) to measure the gain of collusive manipulation, where SGIR
and GIR are respectively the maximum and minimum of the incentive ratios of
corrupted agents. Then, we tightly characterize the SGIRs and GIRs of MNW, PS,
and RR. In particular, the GIR of MNW is $2$ regardless of the coalition size.
Moreover, for coalition size $c \geq 1$, the SGIRs of MNW and PS, and the GIRs
of PS and RR are $c + 1$. Finally, the SGIR of RR is unbounded for coalition
size $c \geq 2$. Our results reveal fundamental differences of these three
mechanisms in their vulnerability to collusion.

</details>


### [69] [A Linear Programming Approach to Estimate the Core in Cooperative Games](https://arxiv.org/abs/2510.01766)
*J Camacho,JC Gonçalves-Dosantos,J Sánchez-Soriano*

Main category: cs.GT

TL;DR: 本文提出用线性规划近似可转移效用（TU）合作博弈核心的新算法，分析其性质并用模拟验证有效性，结果显示该方法可扩展且重建核心准确性高。


<details>
  <summary>Details</summary>
Motivation: 确定完整核心计算难度大，需要一种可处理的近似方法。

Method: 通过随机线性问题（LPs）采样极点，用线性规划近似核心。

Result: 该方法可扩展，在核心重建方面达到高准确性。

Conclusion: 提出的通过线性规划近似TU合作博弈核心的算法是有效的，具有可扩展性和高准确性。

Abstract: This paper proposes a novel algorithm to approximate the core of transferable
utility (TU) cooperative games via linear programming. Given the computational
hardness of determining the full core, our approach provides a tractable
approximation by sampling extreme points through randomized linear problems
(LPs). We analyze its convergence and computational complexity, and validate
its effectiveness through extensive simulations on various game models. Our
results show that the method is scalable and achieves high accuracy in terms of
core reconstruction.

</details>


### [70] [Multi-group Bayesian Games](https://arxiv.org/abs/2510.02078)
*Hongxing Yuan,Xuan Zhang,Chunyu Wei,Yushun Fan*

Main category: cs.GT

TL;DR: 提出多组贝叶斯博弈模型，给出找（强）多组贝叶斯纳什均衡方法并验证结果正确性。


<details>
  <summary>Details</summary>
Motivation: 描述贝叶斯博弈中的群体行为，找到相应最优策略配置。

Method: 提出MBG模型，将其转换为MEAG，给出MEAG为（强）势能的充要条件，据此找（强）纳什均衡和（强）MBNE，提供算法并举例验证。

Result: 找到了MBG的（强）MBNE，并通过示例验证结果正确。

Conclusion: 所提方法能有效找到MBG的（强）MBNE。

Abstract: This paper presents a model of multi-group Bayesian games (MBGs) to describe
the group behavior in Bayesian games, and gives methods to find (strongly)
multi-group Bayesian Nash equilibria (MBNE) of this model with a proposed
transformation. MBNE represent the optimal strategy \textit{profiles} under the
situation where players within a group play a cooperative game, while strongly
MBNE characterize the optimal strategy \textit{profiles} under the situation
where players within a group play a noncooperative game. Firstly, we propose a
model of MBGs and give a transformation to convert any MBG into a multi-group
ex-ante agent game (MEAG) which is a normal-form game. Secondly, we give a
sufficient and necessary condition for a MBG's MEAG to be (strongly) potential.
If it is (strongly) potential, all its (strongly) Nash equilibria can be found,
and then all (strongly) MBNE of the MBG can be obtained by leveraging the
transformation's good properties. Finally, we provide algorithms for finding
(strongly) MBNE of a MBG whose MEAG is (strongly) potential and use an
illustrative example to verify the correctness of our results.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [71] [Location Matters: Leveraging Multi-Resolution Geo-Embeddings for Housing Search](https://arxiv.org/abs/2510.01196)
*Ivo Silva,Pedro Nogueira,Guilherme Bonaldo*

Main category: cs.IR

TL;DR: 本文针对QuintoAndar Group平台用户找房难问题，提出地理感知嵌入框架用于住房推荐，实验表明该方法能提升推荐质量。


<details>
  <summary>Details</summary>
Motivation: QuintoAndar Group平台房源多，用户找房难，且位置对住房推荐很重要，需将位置信息纳入推荐以提高效果。

Method: 提出地理感知嵌入框架，将多层次的H3网格集成到双塔神经架构中，并与传统矩阵分解基线和单分辨率变体进行比较。

Result: 嵌入特定评估显示嵌入表示更丰富平衡，离线排名模拟表明推荐质量大幅提升。

Conclusion: 所提出的地理感知嵌入框架能有效解决数字租赁平台住房推荐中的稀疏性和空间细微差别问题，提升推荐质量。

Abstract: QuintoAndar Group is Latin America's largest housing platform,
revolutionizing property rentals and sales. Headquartered in Brazil, it
simplifies the housing process by eliminating paperwork and enhancing
accessibility for tenants, buyers, and landlords. With thousands of houses
available for each city, users struggle to find the ideal home. In this
context, location plays a pivotal role, as it significantly influences property
value, access to amenities, and life quality. A great location can make even a
modest home highly desirable. Therefore, incorporating location into
recommendations is essential for their effectiveness. We propose a geo-aware
embedding framework to address sparsity and spatial nuances in housing
recommendations on digital rental platforms. Our approach integrates an
hierarchical H3 grid at multiple levels into a two-tower neural architecture.
We compare our method with a traditional matrix factorization baseline and a
single-resolution variant using interaction data from our platform. Embedding
specific evaluation reveals richer and more balanced embedding representations,
while offline ranking simulations demonstrate a substantial uplift in
recommendation quality.

</details>


### [72] [Are LLMs ready to help non-expert users to make charts of official statistics data?](https://arxiv.org/abs/2510.01197)
*Gadir Suleymanli,Alexander Rogiers,Lucas Lageweg,Jefrey Lijffijt*

Main category: cs.IR

TL;DR: 在虚假信息泛滥时，评估大语言模型从复杂数据生成图表的能力，提出新评估框架，发现适当支持和反馈可提升模型效果。


<details>
  <summary>Details</summary>
Motivation: 国家统计局数据分散难处理，需评估当前生成式AI模型能否助力识别数据并自动创建可视化图表。

Method: 使用荷兰统计局公共数据，评估多个大语言模型识别数据、操作和生成可视化图表的能力，提出三维评估框架。

Result: 定位和处理正确数据是最大挑战，无明确指导时模型很少采用最佳可视化实践，补充设计信息和迭代自我评估可提升表现。

Conclusion: 通过适当支持和反馈机制可增强大语言模型自动生成图表的有效性，系统已能达到评估维度所需的准确性。

Abstract: In this time when biased information, deep fakes, and propaganda proliferate,
the accessibility of reliable data sources is more important than ever.
National statistical institutes provide curated data that contain quantitative
information on a wide range of topics. However, that information is typically
spread across many tables and the plain numbers may be arduous to process.
Hence, this open data may be practically inaccessible. We ask the question "Are
current Generative AI models capable of facilitating the identification of the
right data and the fully-automatic creation of charts to provide information in
visual form, corresponding to user queries?". We present a structured
evaluation of recent large language models' (LLMs) capabilities to generate
charts from complex data in response to user queries. Working with diverse
public data from Statistics Netherlands, we assessed multiple LLMs on their
ability to identify relevant data tables, perform necessary manipulations, and
generate appropriate visualizations autonomously. We propose a new evaluation
framework spanning three dimensions: data retrieval & pre-processing, code
quality, and visual representation. Results indicate that locating and
processing the correct data represents the most significant challenge.
Additionally, LLMs rarely implement visualization best practices without
explicit guidance. When supplemented with information about effective chart
design, models showed marked improvement in representation scores. Furthermore,
an agentic approach with iterative self-evaluation led to excellent performance
across all evaluation dimensions. These findings suggest that LLMs'
effectiveness for automated chart generation can be enhanced through
appropriate scaffolding and feedback mechanisms, and that systems can already
reach the necessary accuracy across the three evaluation dimensions.

</details>


### [73] [Optimal signals assignment for eBay View Item page](https://arxiv.org/abs/2510.01198)
*Matan Mandelbrod,Biwei Jiang,Giald Wagner,Tal Franji,Guy Feigenblat*

Main category: cs.IR

TL;DR: 本文提出两种为eBay商品详情页生成信号的统计模型方法，经A/B测试显著提升业务指标。


<details>
  <summary>Details</summary>
Motivation: 为eBay商品详情页展示信号，促进智能购买和用户参与。

Method: 提出两种开发统计模型的方法，并进行A/B测试。

Result: 显著提升业务指标。

Conclusion: 两种为商品详情页生成信号的方法有效。

Abstract: Signals are short textual or visual snippets displayed on the eBay View-Item
(VI) page, providing additional, contextual information for users about the
viewed item. The aim in displaying the signals is to facilitate intelligent
purchase and to incentivise engagement. In this paper, we present two
approaches for developing statistical models that optimally populate the VI
page with signals. Both approaches were A/B tested, and yielded significant
increase in business metrics.

</details>


### [74] [MetaSynth: Multi-Agent Metadata Generation from Implicit Feedback in Black-Box Systems](https://arxiv.org/abs/2510.01523)
*Shreeranjani Srirangamsridharan,Ali Abavisani,Reza Yousefi Maragheh,Ramin Giahi,Kai Zhao,Jason Cho,Sushant Kumar*

Main category: cs.IR

TL;DR: 提出MetaSynth框架学习隐式搜索反馈优化元标题和描述，在多指标上表现优于基线，A/B测试有提升，贡献黑盒系统内容优化范式。


<details>
  <summary>Details</summary>
Motivation: 元标题和描述优化具有挑战性，现有方法存在不足，缺乏对隐式信号的直接利用。

Method: 引入MetaSynth多智能体检索增强生成框架，构建范例库，生成候选片段并通过评估器 - 生成器循环迭代优化。

Result: 在专有电商数据和亚马逊评论语料库上，MetaSynth在NDCG、MRR和排名指标上优于基线，大规模A/B测试显示CTR提升10.26%，点击量提升7.51%。

Conclusion: MetaSynth可有效优化元标题和描述，该工作为利用隐式信号优化黑盒系统内容提供通用范式。

Abstract: Meta titles and descriptions strongly shape engagement in search and
recommendation platforms, yet optimizing them remains challenging. Search
engine ranking models are black box environments, explicit labels are
unavailable, and feedback such as click-through rate (CTR) arrives only
post-deployment. Existing template, LLM, and retrieval-augmented approaches
either lack diversity, hallucinate attributes, or ignore whether candidate
phrasing has historically succeeded in ranking. This leaves a gap in directly
leveraging implicit signals from observable outcomes. We introduce MetaSynth, a
multi-agent retrieval-augmented generation framework that learns from implicit
search feedback. MetaSynth builds an exemplar library from top-ranked results,
generates candidate snippets conditioned on both product content and exemplars,
and iteratively refines outputs via evaluator-generator loops that enforce
relevance, promotional strength, and compliance. On both proprietary e-commerce
data and the Amazon Reviews corpus, MetaSynth outperforms strong baselines
across NDCG, MRR, and rank metrics. Large-scale A/B tests further demonstrate
10.26% CTR and 7.51% clicks. Beyond metadata, this work contributes a general
paradigm for optimizing content in black-box systems using implicit signals.

</details>


### [75] [IoDResearch: Deep Research on Private Heterogeneous Data via the Internet of Data](https://arxiv.org/abs/2510.01553)
*Zhuofan Shi,Zijie Guo,Xinjian Ma,Gang Huang,Yun Ma,Xiang Jing*

Main category: cs.IR

TL;DR: 提出以私有数据为中心的Deep Research框架IoDResearch，实验显示其性能优于基线，证明了在IoD范式下以私有数据为中心的Deep Research的可行性。


<details>
  <summary>Details</summary>
Motivation: 多源异构多模态科学数据增长使传统数据管理受限，现有DR框架忽视本地私有数据，检索效率低且不符合FAIR原则。

Method: 提出IoDResearch框架，将异构资源封装为符合FAIR的数字对象，细化为原子知识单元和知识图，形成异构图索引用于多粒度检索，用多智能体系统支持问答和报告生成，建立IoD DeepResearch Benchmark进行评估。

Result: 在检索、问答和报告撰写任务的实验中，IoDResearch始终优于代表性的RAG和Deep Research基线。

Conclusion: IoDResearch证明了在IoD范式下以私有数据为中心的Deep Research的可行性，为更可信、可复用和自动化的科学发现铺平道路。

Abstract: The rapid growth of multi-source, heterogeneous, and multimodal scientific
data has increasingly exposed the limitations of traditional data management.
Most existing DeepResearch (DR) efforts focus primarily on web search while
overlooking local private data. Consequently, these frameworks exhibit low
retrieval efficiency for private data and fail to comply with the FAIR
principles, ultimately resulting in inefficiency and limited reusability. To
this end, we propose IoDResearch (Internet of Data Research), a private
data-centric Deep Research framework that operationalizes the Internet of Data
paradigm. IoDResearch encapsulates heterogeneous resources as FAIR-compliant
digital objects, and further refines them into atomic knowledge units and
knowledge graphs, forming a heterogeneous graph index for multi-granularity
retrieval. On top of this representation, a multi-agent system supports both
reliable question answering and structured scientific report generation.
Furthermore, we establish the IoD DeepResearch Benchmark to systematically
evaluate both data representation and Deep Research capabilities in IoD
scenarios. Experimental results on retrieval, QA, and report-writing tasks show
that IoDResearch consistently surpasses representative RAG and Deep Research
baselines. Overall, IoDResearch demonstrates the feasibility of
private-data-centric Deep Research under the IoD paradigm, paving the way
toward more trustworthy, reusable, and automated scientific discovery.

</details>


### [76] [Synthetic Prefixes to Mitigate Bias in Real-Time Neural Query Autocomplete](https://arxiv.org/abs/2510.01574)
*Adithya Rajan,Xiaoyu Liu,Prateek Verma,Vibhu Arora*

Main category: cs.IR

TL;DR: 提出用合成前缀减轻实时神经查询自动补全系统中的展示偏差，在电商场景部署后提升用户参与度，对其他低延迟排序任务有借鉴意义。


<details>
  <summary>Details</summary>
Motivation: 解决实时查询自动补全交互中参与信号的固有偏差问题。

Method: 利用非自动补全搜索会话收集的完整用户查询生成合成前缀丰富训练数据，优化实时部署的神经排序器，简化列表损失以降低计算复杂度。

Result: 在大规模电商场景中，系统在用户参与度指标上有显著提升。

Conclusion: 合成前缀可提升泛化能力，为其他低延迟排序任务提供可扩展的偏差缓解途径。

Abstract: We introduce a data-centric approach for mitigating presentation bias in
real-time neural query autocomplete systems through the use of synthetic
prefixes. These prefixes are generated from complete user queries collected
during regular search sessions where autocomplete was not active. This allows
us to enrich the training data for learning to rank models with more diverse
and less biased examples. This method addresses the inherent bias in engagement
signals collected from live query autocomplete interactions, where model
suggestions influence user behavior. Our neural ranker is optimized for
real-time deployment under strict latency constraints and incorporates a rich
set of features, including query popularity, seasonality, fuzzy match scores,
and contextual signals such as department affinity, device type, and vertical
alignment with previous user queries. To support efficient training, we
introduce a task-specific simplification of the listwise loss, reducing
computational complexity from $O(n^2)$ to $O(n)$ by leveraging the query
autocomplete structure of having only one ground-truth selection per prefix.
Deployed in a large-scale e-commerce setting, our system demonstrates
statistically significant improvements in user engagement, as measured by mean
reciprocal rank and related metrics. Our findings show that synthetic prefixes
not only improve generalization but also provide a scalable path toward bias
mitigation in other low-latency ranking tasks, including related searches and
query recommendations.

</details>


### [77] [Bridging Collaborative Filtering and Large Language Models with Dynamic Alignment, Multimodal Fusion and Evidence-grounded Explanations](https://arxiv.org/abs/2510.01606)
*Bo Ma,LuYao Liu,Simon Lau,Chandler Yuan,and XueY Cui,Rosie Zhang*

Main category: cs.IR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent research has explored using Large Language Models for recommendation
tasks by transforming user interaction histories and item metadata into text
prompts, then having the LLM produce rankings or recommendations. A promising
approach involves connecting collaborative filtering knowledge to LLM
representations through compact adapter networks, which avoids expensive
fine-tuning while preserving the strengths of both components. Yet several
challenges persist in practice: collaborative filtering models often use static
snapshots that miss rapidly changing user preferences; many real-world items
contain rich visual and audio content beyond textual descriptions; and current
systems struggle to provide trustworthy explanations backed by concrete
evidence. Our work introduces \model{}, a framework that tackles these
limitations through three key innovations. We develop an online adaptation
mechanism that continuously incorporates new user interactions through
lightweight modules, avoiding the need to retrain large models. We create a
unified representation that seamlessly combines collaborative signals with
visual and audio features, handling cases where some modalities may be
unavailable. Finally, we design an explanation system that grounds
recommendations in specific collaborative patterns and item attributes,
producing natural language rationales users can verify. Our approach maintains
the efficiency of frozen base models while adding minimal computational
overhead, making it practical for real-world deployment.

</details>


### [78] [LLM4Rec: Large Language Models for Multimodal Generative Recommendation with Causal Debiasing](https://arxiv.org/abs/2510.01622)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Lau*

Main category: cs.IR

TL;DR: 本文提出增强型生成式推荐框架，解决多模态数据处理等问题，实验显示在准确性、公平性和多样性上有提升。


<details>
  <summary>Details</summary>
Motivation: 当代生成式推荐系统在处理多模态数据、消除算法偏差和提供透明决策过程方面面临挑战。

Method: 引入含多模态融合架构等五项关键创新的框架，以大语言模型为骨干，结合专业模块。

Result: 在三个基准数据集上实验，相比现有方法，推荐准确性、公平性和多样性有持续提升，NDCG@10 最多提高 2.3%，多样性指标提升 1.4%，且保持计算效率。

Conclusion: 提出的框架有效解决了现有生成式推荐系统的问题，在性能上表现良好。

Abstract: Contemporary generative recommendation systems face significant challenges in
handling multimodal data, eliminating algorithmic biases, and providing
transparent decision-making processes. This paper introduces an enhanced
generative recommendation framework that addresses these limitations through
five key innovations: multimodal fusion architecture, retrieval-augmented
generation mechanisms, causal inference-based debiasing, explainable
recommendation generation, and real-time adaptive learning capabilities. Our
framework leverages advanced large language models as the backbone while
incorporating specialized modules for cross-modal understanding, contextual
knowledge integration, bias mitigation, explanation synthesis, and continuous
model adaptation. Extensive experiments on three benchmark datasets
(MovieLens-25M, Amazon-Electronics, Yelp-2023) demonstrate consistent
improvements in recommendation accuracy, fairness, and diversity compared to
existing approaches. The proposed framework achieves up to 2.3% improvement in
NDCG@10 and 1.4% enhancement in diversity metrics while maintaining
computational efficiency through optimized inference strategies.

</details>


### [79] [TalkPlay-Tools: Conversational Music Recommendation with LLM Tool Calling](https://arxiv.org/abs/2510.01698)
*Seungheon Doh,Keunwoo Choi,Juhan Nam*

Main category: cs.IR

TL;DR: 提出基于大语言模型和工具调用的音乐推荐系统，实现统一检索重排序，在不同推荐场景表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型驱动的生成式推荐系统对元数据或属性过滤等组件利用不足，推荐行为受限。

Method: 将大语言模型作为端到端推荐系统，通过工具规划预测工具类型、执行顺序和参数，集成布尔过滤器、稀疏检索、密集检索和生成式检索等组件。

Result: 该统一工具调用框架在不同推荐场景下取得了有竞争力的性能。

Conclusion: 此框架为对话式音乐推荐系统提供了新范式。

Abstract: While the recent developments in large language models (LLMs) have
successfully enabled generative recommenders with natural language
interactions, their recommendation behavior is limited, leaving other simpler
yet crucial components such as metadata or attribute filtering underutilized in
the system. We propose an LLM-based music recommendation system with tool
calling to serve as a unified retrieval-reranking pipeline. Our system
positions an LLM as an end-to-end recommendation system that interprets user
intent, plans tool invocations, and orchestrates specialized components:
boolean filters (SQL), sparse retrieval (BM25), dense retrieval (embedding
similarity), and generative retrieval (semantic IDs). Through tool planning,
the system predicts which types of tools to use, their execution order, and the
arguments needed to find music matching user preferences, supporting diverse
modalities while seamlessly integrating multiple database filtering methods. We
demonstrate that this unified tool-calling framework achieves competitive
performance across diverse recommendation scenarios by selectively employing
appropriate retrieval methods based on user queries, envisioning a new paradigm
for conversational music recommendation systems.

</details>


### [80] [Ranking Items from Discrete Ratings: The Cost of Unknown User Thresholds](https://arxiv.org/abs/2510.01871)
*Oscar Villemaud,Suryanarayana Sankagiri,Matthias Grossglauser*

Main category: cs.IR

TL;DR: 探讨能否从粗粒度评分恢复细粒度物品排名，建模后证明接近完美排名需Θ(n²)个用户，给出匹配复杂度算法，揭示在线排名中的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究能否从粗粒度的用户评分中恢复细粒度的物品排名。

Method: 将物品建模为有分数，用户有阈值；用户顺序到来，通过新用户查询细化排名；理论证明和设计排名算法。

Result: 证明达到接近完美排名需Θ(n²)个用户，算法查询复杂度与理论界匹配到对数因子。

Conclusion: 在线排名中，阈值多样性对合并粗粒度评分很必要，但未知阈值时会有成本。

Abstract: Ranking items is a central task in many information retrieval and recommender
systems. User input for the ranking task often comes in the form of ratings on
a coarse discrete scale. We ask whether it is possible to recover a
fine-grained item ranking from such coarse-grained ratings. We model items as
having scores and users as having thresholds; a user rates an item positively
if the item's score exceeds the user's threshold. Although all users agree on
the total item order, estimating that order is challenging when both the scores
and the thresholds are latent. Under our model, any ranking method naturally
partitions the $n$ items into bins; the bins are ordered, but the items inside
each bin are still unordered. Users arrive sequentially, and every new user can
be queried to refine the current ranking. We prove that achieving a
near-perfect ranking, measured by Spearman distance, requires $\Theta(n^2)$
users (and therefore $\Omega(n^2)$ queries). This is significantly worse than
the $O(n\log n)$ queries needed to rank from comparisons; the gap reflects the
additional queries needed to identify the users who have the appropriate
thresholds. Our bound also quantifies the impact of a mismatch between score
and threshold distributions via a quadratic divergence factor. To show the
tightness of our results, we provide a ranking algorithm whose query complexity
matches our bound up to a logarithmic factor. Our work reveals a tension in
online ranking: diversity in thresholds is necessary to merge coarse ratings
from many users into a fine-grained ranking, but this diversity has a cost if
the thresholds are a priori unknown.

</details>


### [81] [Contrastive Retrieval Heads Improve Attention-Based Re-Ranking](https://arxiv.org/abs/2510.02219)
*Linh Tran,Yulong Li,Radu Florian,Wei Sun*

Main category: cs.IR

TL;DR: 本文引入CoRe heads改善基于注意力的重排器性能，实验表明少量CoRe heads能提升重排准确率，且剪枝部分层可减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的重排器中很多注意力头会产生噪声和冗余，限制了性能，需要改进。

Method: 引入CoRe heads，通过对比评分指标识别一组检索头，用于重排。

Result: 使用三个大语言模型的实验显示，不到1%的CoRe heads聚合信号显著提升重排准确率；CoRe heads集中在中间层，剪枝最后50%的模型层可在保持准确率的同时减少推理时间和内存使用。

Conclusion: CoRe heads能有效提升重排系统性能，且通过剪枝部分层可优化计算资源。

Abstract: The strong zero-shot and long-context capabilities of recent Large Language
Models (LLMs) have paved the way for highly effective re-ranking systems.
Attention-based re-rankers leverage attention weights from transformer heads to
produce relevance scores, but not all heads are created equally: many
contribute noise and redundancy, thus limiting performance. To address this, we
introduce CoRe heads, a small set of retrieval heads identified via a
contrastive scoring metric that explicitly rewards high attention heads that
correlate with relevant documents, while downplaying nodes with higher
attention that correlate with irrelevant documents. This relative ranking
criterion isolates the most discriminative heads for re-ranking and yields a
state-of-the-art list-wise re-ranker. Extensive experiments with three LLMs
show that aggregated signals from CoRe heads, constituting less than 1% of all
heads, substantially improve re-ranking accuracy over strong baselines. We
further find that CoRe heads are concentrated in middle layers, and pruning the
computation of final 50% of model layers preserves accuracy while significantly
reducing inference time and memory usage.

</details>


### [82] [Study on LLMs for Promptagator-Style Dense Retriever Training](https://arxiv.org/abs/2510.02241)
*Daniel Gwon,Nour Jedidi,Jimmy Lin*

Main category: cs.IR

TL;DR: 研究用可访问规模的开源大语言模型替代专有大模型用于Promptagator风格查询生成，发现小至3B参数的开源模型也有效。


<details>
  <summary>Details</summary>
Motivation: 原Promptagator方法依赖专有且大规模的大语言模型，用户可能无法访问或不能用于敏感数据，因此研究可访问规模的开源大语言模型作为替代。

Method: 研究可访问规模（参数≤14B）的开源大语言模型作为替代。

Result: 小至3B参数的开源大语言模型可作为有效的Promptagator风格查询生成器。

Conclusion: 为从业者提供合成数据生成的可靠替代方案，为特定领域应用的微调结果最大化提供见解。

Abstract: Promptagator demonstrated that Large Language Models (LLMs) with few-shot
prompts can be used as task-specific query generators for fine-tuning
domain-specialized dense retrieval models. However, the original Promptagator
approach relied on proprietary and large-scale LLMs which users may not have
access to or may be prohibited from using with sensitive data. In this work, we
study the impact of open-source LLMs at accessible scales ($\leq$14B
parameters) as an alternative. Our results demonstrate that open-source LLMs as
small as 3B parameters can serve as effective Promptagator-style query
generators. We hope our work will inform practitioners with reliable
alternatives for synthetic data generation and give insights to maximize
fine-tuning results for domain-specific applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [83] [Ensemble Threshold Calibration for Stable Sensitivity Control](https://arxiv.org/abs/2510.02116)
*John N. Daras*

Main category: cs.LG

TL;DR: 提出端到端框架，在大规模空间合并和实体匹配任务中实现精确召回控制，方差低且TPU友好。


<details>
  <summary>Details</summary>
Motivation: 经典置信区间切割方法在召回控制中常超目标值且方差大，需更好方法。

Method: 从等网格边界框过滤和CSR候选表示开始，用xxHash抽样训练轻量级神经排序器，传播分数构建校准集，聚合四个阈值估计器并跨子样本融合。

Result: 在两个地籍数据集上，能在小误差内达到召回目标，减少冗余验证，可在单TPU v3核心上运行。

Conclusion: 提出的框架能有效实现精确召回控制，优于现有方法。

Abstract: Precise recall control is critical in large-scale spatial conflation and
entity-matching tasks, where missing even a few true matches can break
downstream analytics, while excessive manual review inflates cost. Classical
confidence-interval cuts such as Clopper-Pearson or Wilson provide lower bounds
on recall, but they routinely overshoot the target by several percentage points
and exhibit high run-to-run variance under skewed score distributions. We
present an end-to-end framework that achieves exact recall with sub-percent
variance over tens of millions of geometry pairs, while remaining TPU-friendly.
Our pipeline starts with an equigrid bounding-box filter and compressed sparse
row (CSR) candidate representation, reducing pair enumeration by two orders of
magnitude. A deterministic xxHash bootstrap sample trains a lightweight neural
ranker; its scores are propagated to all remaining pairs via a single forward
pass and used to construct a reproducible, score-decile-stratified calibration
set. Four complementary threshold estimators - Clopper-Pearson, Jeffreys,
Wilson, and an exact quantile - are aggregated via inverse-variance weighting,
then fused across nine independent subsamples. This ensemble reduces threshold
variance compared to any single method. Evaluated on two real cadastral
datasets (approximately 6.31M and 67.34M pairs), our approach consistently hits
a recall target within a small error, decreases redundant verifications
relative to other calibrations, and runs end-to-end on a single TPU v3 core.

</details>


### [84] [Accelerating Long-Term Molecular Dynamics with Physics-Informed Time-Series Forecasting](https://arxiv.org/abs/2510.01206)
*Hung Le,Sherif Abbas,Minh Hoang Nguyen,Van Dai Do,Huu Hiep Nguyen,Dung Nguyen*

Main category: cs.LG

TL;DR: 提出新方法将MD模拟转为时间序列预测问题，结合物理信息提升原子轨迹预测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统DFT方法计算成本高，限制长期MD模拟可行性。

Method: 将MD模拟转为时间序列预测问题，用位移预测原子轨迹，结合物理信息损失和推理机制。

Result: 在不同材料模拟精度上超标准基线，能在数分钟内稳定模拟数千个MD步骤。

Conclusion: 结合物理知识对提升原子轨迹预测可靠性和精度很重要，该方法是昂贵DFT模拟的可扩展替代方案。

Abstract: Efficient molecular dynamics (MD) simulation is vital for understanding
atomic-scale processes in materials science and biophysics. Traditional density
functional theory (DFT) methods are computationally expensive, which limits the
feasibility of long-term simulations. We propose a novel approach that
formulates MD simulation as a time-series forecasting problem, enabling
advanced forecasting models to predict atomic trajectories via displacements
rather than absolute positions. We incorporate a physics-informed loss and
inference mechanism based on DFT-parametrised pair-wise Morse potential
functions that penalize unphysical atomic proximity to enforce physical
plausibility. Our method consistently surpasses standard baselines in
simulation accuracy across diverse materials. The results highlight the
importance of incorporating physics knowledge to enhance the reliability and
precision of atomic trajectory forecasting. Remarkably, it enables stable
modeling of thousands of MD steps in minutes, offering a scalable alternative
to costly DFT simulations.

</details>


### [85] [Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs](https://arxiv.org/abs/2510.01218)
*Sergey Troshin,Wafaa Mohammed,Yan Meng,Christof Monz,Antske Fokkens,Vlad Niculae*

Main category: cs.LG

TL;DR: 本文指出温度采样会降低数学推理等高精度任务质量，提出选择性采样方法，实验证明其能提升质量 - 多样性权衡。


<details>
  <summary>Details</summary>
Motivation: 温度采样虽能增加多样性，但在高精度任务中会降低推理质量，需解决此问题。

Method: 提出选择性采样方法，基于采样风险指标动态切换贪心和高温采样，训练轻量级分类器预测采样风险。

Result: 在数学推理任务实验中，选择性采样即使在高温设置下也能提升质量 - 多样性权衡。

Conclusion: 选择性采样可有效解决高精度任务中多样性生成与推理质量的矛盾。

Abstract: Diversity is an essential metric for evaluating the creativity of outputs
generated by language models. Temperature-based sampling is a common strategy
to increase diversity. However, for tasks that require high precision, e.g.,
mathematical reasoning, uncontrolled high temperature sampling, e.g., min-$p$
or top-$p$, degrades reasoning quality. We demonstrate that the loss of
accuracy is caused by sampling incorrect continuations in sensitive decoding
positions. To address this, in this paper, we propose \textbf{selective
sampling}, a method that dynamically switches between greedy and
high-temperature sampling based on a sampling risk metric. This risk metric
estimates the likelihood of output errors when applying high-temperature
sampling on the current token position. To predict sampling risk, we train a
lightweight classifier on a small subset of verifiable problems. The trained
classifier can be integrated with the base language model with minimal latency
overhead. Experiments on mathematical reasoning tasks demonstrate that
selective sampling enhances the quality-diversity trade-off, even in
high-temperature settings.

</details>


### [86] [Automated Extraction of Material Properties using LLM-based AI Agents](https://arxiv.org/abs/2510.01235)
*Subham Ghosh,Abhishek Tewari*

Main category: cs.LG

TL;DR: 提出由大语言模型驱动的工作流，从约10000篇文章中提取热电和结构属性，构建了迄今最大的数据集并进行分析，还发布了交互式网络浏览器。


<details>
  <summary>Details</summary>
Motivation: 材料快速发现受限于缺乏将性能指标与结构背景结合的大型机器可读数据集，现有数据库存在不足，实验文献未充分利用。

Method: 采用动态令牌分配、零样本多智能体提取和条件表解析的工作流，用不同模型进行提取。

Result: GPT - 4.1准确率最高，GPT - 4.1 Mini成本低且性能接近；构建了含27822条记录的数据集，分析重现已知趋势并发现结构 - 属性关联。

Conclusion: 提供了最大的LLM整理热电数据集、可重现且成本可控的提取流程，为数据驱动的材料发现奠定基础。

Abstract: The rapid discovery of materials is constrained by the lack of large,
machine-readable datasets that couple performance metrics with structural
context. Existing databases are either small, manually curated, or biased
toward first principles results, leaving experimental literature
underexploited. We present an agentic, large language model (LLM)-driven
workflow that autonomously extracts thermoelectric and structural-properties
from about 10,000 full-text scientific articles. The pipeline integrates
dynamic token allocation, zeroshot multi-agent extraction, and conditional
table parsing to balance accuracy against computational cost. Benchmarking on
50 curated papers shows that GPT-4.1 achieves the highest accuracy (F1 = 0.91
for thermoelectric properties and 0.82 for structural fields), while GPT-4.1
Mini delivers nearly comparable performance (F1 = 0.89 and 0.81) at a fraction
of the cost, enabling practical large scale deployment. Applying this workflow,
we curated 27,822 temperature resolved property records with normalized units,
spanning figure of merit (ZT), Seebeck coefficient, conductivity, resistivity,
power factor, and thermal conductivity, together with structural attributes
such as crystal class, space group, and doping strategy. Dataset analysis
reproduces known thermoelectric trends, such as the superior performance of
alloys over oxides and the advantage of p-type doping, while also surfacing
broader structure-property correlations. To facilitate community access, we
release an interactive web explorer with semantic filters, numeric queries, and
CSV export. This study delivers the largest LLM-curated thermoelectric dataset
to date, provides a reproducible and cost-profiled extraction pipeline, and
establishes a foundation for scalable, data-driven materials discovery beyond
thermoelectrics.

</details>


### [87] [RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models](https://arxiv.org/abs/2510.01240)
*Zukang Xu,Xing Hu,Qiang Wu,Dawei Yang*

Main category: cs.LG

TL;DR: 本文提出RSAVQ框架用于大语言模型极低比特量化，通过两项创新缓解现有问题，实验显示其优于现有方法，为受限环境提供实用方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数增加对资源受限设备部署构成挑战，现有向量量化工作存在无约束方向误差和次优比特分配问题。

Method: 提出RSAVQ框架，引入误差方向敏感性引导（EDSG）和权重通道敏感性引导（WCSG）两项几何驱动创新。

Result: 实验表明RSAVQ在大语言模型量化上优于现有方法，如在LLaMA - 3 8B的2比特量化中，困惑度和零样本准确率表现更好。

Conclusion: 该工作为受限环境提供实用解决方案，搭建信息几何与神经网络量化的理论桥梁，推动高效深度学习发展。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across
a wide range of natural language processing tasks. However, their exponentially
increasing parameters pose significant challenges for deployment on
resource-constrained devices. Vector Quantization (VQ) shows great promise for
low-bit quantization (e.g., 2 to 4 bits), but existing work faces two key
challenges: unconstrained direction error and suboptimal bit allocation. In
this paper, we propose RSAVQ, a novel VQ framework to enhance extremely low-bit
quantization for LLMs. RSAVQ introduces two geometry-driven innovations that
effectively mitigate above limitations: (1) Error Direction Sensitivity
Guidance (EDSG), which leverages the Fisher Information Matrix (FIM)-induced
Riemannian metric to project quantization errors onto low-sensitivity
directions in the parameter space. Specifically, this projection is performed
along the negative natural gradient direction, which effectively suppresses
error expansion. (2) Weight Channel Sensitivity Guidance (WCSG) , which
constructs a channel-wise sensitivity metric via FIM curvature analysis to
dynamically guide bit resource allocation. The approach facilitates a globally
optimal quantization solution within prescribed bit constraints. Experiments
demonstrate that RSAVQ outperforms existing methods for LLMs. For example, in
2-bit quantization of LLaMA-3 8B, RSAVQ leads baselines like VPTQ and QuIP# by
0.4 in perplexity (PPL) and 1.5 in zero-shot accuracy. This work offers a
practical solution for constrained environments and a theoretical bridge
between information geometry and the quantization of neural networks, advancing
efficient deep learning.

</details>


### [88] [Adaptive Federated Learning Defences via Trust-Aware Deep Q-Networks](https://arxiv.org/abs/2510.01261)
*Vedant Palit*

Main category: cs.LG

TL;DR: 本文针对联邦学习在部分可观测性下易受攻击的问题，提出信任感知的深度Q网络防御方法，并在CIFAR - 10上测试，证明该方法在鲁棒性和准确性上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在部分可观测性下易受投毒和后门攻击的问题。

Method: 将防御问题建模为部分可观测的序列决策问题，引入信任感知的深度Q网络，整合多信号证据更新客户端信任，优化长期鲁棒性 - 准确性目标。

Result: 在CIFAR - 10上，建立了准确率稳步提升的基线；通过狄利克雷扫描表明增加客户端重叠可提高准确率、降低攻击成功率并稳定检测；信号预算研究显示顺序信念更新可缓解弱信号影响；与其他控制器对比，DQN实现了最佳的鲁棒性 - 准确性权衡。

Conclusion: 信任感知的深度Q网络在联邦学习防御中能实现最佳的鲁棒性 - 准确性权衡。

Abstract: Federated learning is vulnerable to poisoning and backdoor attacks under
partial observability. We formulate defence as a partially observable
sequential decision problem and introduce a trust-aware Deep Q-Network that
integrates multi-signal evidence into client trust updates while optimizing a
long-horizon robustness--accuracy objective. On CIFAR-10, we (i) establish a
baseline showing steadily improving accuracy, (ii) show through a Dirichlet
sweep that increased client overlap consistently improves accuracy and reduces
ASR with stable detection, and (iii) demonstrate in a signal-budget study that
accuracy remains steady while ASR increases and ROC-AUC declines as
observability is reduced, which highlights that sequential belief updates
mitigate weaker signals. Finally, a comparison with random, linear-Q, and
policy gradient controllers confirms that DQN achieves the best
robustness--accuracy trade-off.

</details>


### [89] [Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information](https://arxiv.org/abs/2510.01499)
*Rui Ai,Yuqi Pan,David Simchi-Levi,Milind Tambe,Haifeng Xu*

Main category: cs.LG

TL;DR: 提出OW和ISP两种聚合算法解决多智能体大语言模型答案聚合问题，在多场景验证中表现优于多数投票法。


<details>
  <summary>Details</summary>
Motivation: 多智能体大语言模型推理中，标准多数投票法未考虑模型间潜在异质性和相关性，需有效聚合答案的方法。

Method: 设计Optimal Weight (OW) 和Inverse Surprising Popularity (ISP) 两种聚合算法，利用一阶和二阶信息。

Result: 理论分析表明算法能在温和假设下缓解多数投票法的固有局限，实证验证在合成数据集、流行基准和真实医疗场景中均优于多数投票法。

Conclusion: 算法为稳健的多智能体大语言模型管道设计提供实际性能提升和概念见解。

Abstract: With the rapid progress of multi-agent large language model (LLM) reasoning,
how to effectively aggregate answers from multiple LLMs has emerged as a
fundamental challenge. Standard majority voting treats all answers equally,
failing to consider latent heterogeneity and correlation across models. In this
work, we design two new aggregation algorithms called Optimal Weight (OW) and
Inverse Surprising Popularity (ISP), leveraging both first-order and
second-order information. Our theoretical analysis shows these methods provably
mitigate inherent limitations of majority voting under mild assumptions,
leading to more reliable collective decisions. We empirically validate our
algorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as
UltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all
cases, our methods consistently outperform majority voting, offering both
practical performance gains and conceptual insights for the design of robust
multi-agent LLM pipelines.

</details>


### [90] [RSTGCN: Railway-centric Spatio-Temporal Graph Convolutional Network for Train Delay Prediction](https://arxiv.org/abs/2510.01262)
*Koyena Chowdhury,Paramita Koley,Abhijnan Chakraborty,Saptarshi Ghosh*

Main category: cs.LG

TL;DR: 提出RSTGCN模型预测火车站列车平均到达延误，构建印度铁路网络数据集，实验有改进。


<details>
  <summary>Details</summary>
Motivation: 准确预测列车延误对铁路运营至关重要，以往关注单列车延误预测，本文探索车站级延误预测以支持高层交通管理。

Method: 提出RSTGCN模型，有架构创新和特征集成，构建印度铁路网络数据集。

Result: 通过与多个基线模型对比实验，在标准指标上有一致改进。

Conclusion: 推进了大规模铁路网络平均延误预测建模，提供开放数据集促进该领域研究。

Abstract: Accurate prediction of train delays is critical for efficient railway
operations, enabling better scheduling and dispatching decisions. While earlier
approaches have largely focused on forecasting the exact delays of individual
trains, recent studies have begun exploring station-level delay prediction to
support higher-level traffic management. In this paper, we propose the
Railway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN), designed
to forecast average arrival delays of all the incoming trains at railway
stations for a particular time period. Our approach incorporates several
architectural innovations and novel feature integrations, including train
frequency-aware spatial attention, which significantly enhances predictive
performance. To support this effort, we curate and release a comprehensive
dataset for the entire Indian Railway Network (IRN), spanning 4,735 stations
across 17 zones - the largest and most diverse railway network studied to date.
We conduct extensive experiments using multiple state-of-the-art baselines,
demonstrating consistent improvements across standard metrics. Our work not
only advances the modeling of average delay prediction in large-scale rail
networks but also provides an open dataset to encourage further research in
this critical domain.

</details>


### [91] [Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency](https://arxiv.org/abs/2510.01263)
*Yaron Meirovitch,Fuming Yang,Jeff Lichtman,Nir Shavit*

Main category: cs.LG

TL;DR: 提出Budgeted Broadcast (BB)方法，在多模型中提升编码熵、去相关性和准确率，能达到先进结果且易集成。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法多按对损失的影响来移除参数，作者想提出新的剪枝方法。

Method: 提出BB方法，给每个单元一个局部流量预算，通过约束熵分析得出选择性 - 受众平衡公式，用简单局部执行器进行剪枝。

Result: 在多种模型中增加编码熵和去相关性，提高匹配稀疏度下的准确率，在电子显微镜图像上达到先进的F1和PR - AUC。

Conclusion: BB方法易于集成，为学习更多样高效的表示提供了途径。

Abstract: Most pruning methods remove parameters ranked by impact on loss (e.g.,
magnitude or gradient). We propose Budgeted Broadcast (BB), which gives each
unit a local traffic budget (the product of its long-term on-rate $a_i$ and
fan-out $k_i$). A constrained-entropy analysis shows that maximizing coding
entropy under a global traffic budget yields a selectivity-audience balance,
$\log\frac{1-a_i}{a_i}=\beta k_i$. BB enforces this balance with simple local
actuators that prune either fan-in (to lower activity) or fan-out (to reduce
broadcast). In practice, BB increases coding entropy and decorrelation and
improves accuracy at matched sparsity across Transformers for ASR, ResNets for
face identification, and 3D U-Nets for synapse prediction, sometimes exceeding
dense baselines. On electron microscopy images, it attains state-of-the-art F1
and PR-AUC under our evaluation protocol. BB is easy to integrate and suggests
a path toward learning more diverse and efficient representations.

</details>


### [92] [TetriServe: Efficient DiT Serving for Heterogeneous Image Generation](https://arxiv.org/abs/2510.01565)
*Runyu Lu,Shiqi He,Wenxuan Tan,Shenggui Li,Ruofan Wu,Jeff J. Ma,Ang Chen,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: 现有扩散变压器模型服务系统效率低，本文提出TetriServe系统，通过步级序列并行和轮次调度机制提升SLO达成率。


<details>
  <summary>Details</summary>
Motivation: 扩散变压器模型计算成本高，现有服务系统采用固定序列并行度，对异构工作负载效率低，GPU利用率和SLO达成率差。

Method: 提出步级序列并行策略，根据请求截止时间动态调整并行度；引入基于轮次的调度机制，包括时间离散化、步级并行调整和请求联合打包。

Result: 在最先进的扩散变压器模型上评估，TetriServe比现有方案SLO达成率最高提升32%，且不降低图像质量。

Conclusion: TetriServe系统通过新策略和调度机制，能高效服务扩散变压器模型，提升SLO达成率。

Abstract: Diffusion Transformer (DiT) models excel at generating highquality images
through iterative denoising steps, but serving them under strict Service Level
Objectives (SLOs) is challenging due to their high computational cost,
particularly at large resolutions. Existing serving systems use fixed degree
sequence parallelism, which is inefficient for heterogeneous workloads with
mixed resolutions and deadlines, leading to poor GPU utilization and low SLO
attainment.
  In this paper, we propose step-level sequence parallelism to dynamically
adjust the parallel degree of individual requests according to their deadlines.
We present TetriServe, a DiT serving system that implements this strategy for
highly efficient image generation. Specifically, TetriServe introduces a novel
round-based scheduling mechanism that improves SLO attainment: (1) discretizing
time into fixed rounds to make deadline-aware scheduling tractable, (2)
adapting parallelism at the step level and minimize GPU hour consumption, and
(3) jointly packing requests to minimize late completions. Extensive evaluation
on state-of-the-art DiT models shows that TetriServe achieves up to 32% higher
SLO attainment compared to existing solutions without degrading image quality.

</details>


### [93] [A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab](https://arxiv.org/abs/2510.01264)
*Isaac Peterson,Christopher Allred,Jacob Morrey,Mario Harper*

Main category: cs.LG

TL;DR: 本文扩展IsaacLab框架，支持高保真物理模拟中对抗策略的可扩展训练，引入对抗性多智能体强化学习环境，实验证明其能为形态多样的多智能体竞争训练鲁棒策略。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习工作多关注协作场景，而对抗交互在现实应用中同样关键，因此要扩展框架支持对抗策略训练。

Method: 扩展IsaacLab框架，引入对抗性多智能体强化学习环境，集成竞争性的异构智能体强化学习与近端策略优化算法。

Result: 在多个基准场景实验中，框架能在保持高吞吐量和模拟真实感的同时，为形态多样的多智能体竞争建模和训练鲁棒策略。

Conclusion: 扩展后的IsaacLab框架可有效用于对抗性多智能体强化学习，代码和基准测试已开源。

Abstract: Multi-Agent Reinforcement Learning (MARL) is central to robotic systems
cooperating in dynamic environments. While prior work has focused on these
collaborative settings, adversarial interactions are equally critical for
real-world applications such as pursuit-evasion, security, and competitive
manipulation. In this work, we extend the IsaacLab framework to support
scalable training of adversarial policies in high-fidelity physics simulations.
We introduce a suite of adversarial MARL environments featuring heterogeneous
agents with asymmetric goals and capabilities. Our platform integrates a
competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal
Policy Optimization (HAPPO), enabling efficient training and evaluation under
adversarial dynamics. Experiments across several benchmark scenarios
demonstrate the framework's ability to model and train robust policies for
morphologically diverse multi-agent competition while maintaining high
throughput and simulation realism. Code and benchmarks are available at:
https://github.com/DIRECTLab/IsaacLab-HARL .

</details>


### [94] [RLP: Reinforcement as a Pretraining Objective](https://arxiv.org/abs/2510.01265)
*Ali Hatamizadeh,Syeda Nahida Akter,Shrimai Prabhumoye,Jan Kautz,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Yejin Choi*

Main category: cs.LG

TL;DR: 本文提出信息驱动的强化预训练目标RLP，将强化学习探索精神引入预训练最后阶段，在多个模型上提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 探究当前训练大型推理模型的主流范式是否为最优方式。

Method: 提出RLP，将思维链视为探索动作，基于信息增益计算奖励，以此鼓励模型在预测前独立思考。

Result: 在Qwen3 - 1.7B - Base上提升八个数学和科学基准测试的整体平均成绩19%；在混合模型Nemotron - Nano - 12B - v2上整体平均成绩从42.81%提升到61.32%，科学推理平均成绩提升23%。

Conclusion: RLP可将强化学习用于普通文本的预训练，缩小了下一个词预测与有用思维链推理出现之间的差距，且在不同架构和模型大小上具有可扩展性。

Abstract: The dominant paradigm for training large reasoning models starts with
pre-training using next-token prediction loss on vast amounts of data.
Reinforcement learning, while powerful in scaling reasoning, is introduced only
as the very last phase of post-training, preceded by supervised fine-tuning.
While dominant, is this an optimal way of training? In this paper, we present
RLP, an information-driven reinforcement pretraining objective, that brings the
core spirit of reinforcement learning -- exploration -- to the last phase of
pretraining. The key idea is to treat chain-of-thought as an exploratory
action, with rewards computed based on the information gain it provides for
predicting future tokens. This training objective essentially encourages the
model to think for itself before predicting what comes next, thus teaching an
independent thinking behavior earlier in the pretraining. More concretely, the
reward signal measures the increase in log-likelihood of the next token when
conditioning on both context and a sampled reasoning chain, compared to
conditioning on context alone. This approach yields a verifier-free dense
reward signal, allowing for efficient training for the full document stream
during pretraining. Specifically, RLP reframes reinforcement learning for
reasoning as a pretraining objective on ordinary text, bridging the gap between
next-token prediction and the emergence of useful chain-of-thought reasoning.
Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an
eight-benchmark math-and-science suite by 19%. With identical post-training,
the gains compound, with the largest improvements on reasoning-heavy tasks such
as AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2
increases the overall average from 42.81% to 61.32% and raises the average on
scientific reasoning by 23%, demonstrating scalability across architectures and
model sizes.

</details>


### [95] [Safe Reinforcement Learning-Based Vibration Control: Overcoming Training Risks with LQR Guidance](https://arxiv.org/abs/2510.01269)
*Rohan Vitthal Thorat,Juhi Singh,Rajdip Nayek*

Main category: cs.LG

TL;DR: 提出结合LQR与RL控制器的混合控制框架，解决RL振动控制训练安全问题，消除对显式系统模型依赖。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型的控制策略需繁琐系统识别，无模型RL训练时随机控制力可能危害结构，需解决其训练安全问题。

Method: 引入结合LQR和RL控制器的混合控制框架，LQR策略基于随机选择的模型和参数。

Result: 该混合方法消除对显式系统模型的依赖，降低纯RL实现的探索风险。

Conclusion: 此为首个解决基于RL振动控制训练安全挑战并提供有效解决方案的研究。

Abstract: Structural vibrations induced by external excitations pose significant risks,
including safety hazards for occupants, structural damage, and increased
maintenance costs. While conventional model-based control strategies, such as
Linear Quadratic Regulator (LQR), effectively mitigate vibrations, their
reliance on accurate system models necessitates tedious system identification.
This tedious system identification process can be avoided by using a model-free
Reinforcement learning (RL) method. RL controllers derive their policies solely
from observed structural behaviour, eliminating the requirement for an explicit
structural model. For an RL controller to be truly model-free, its training
must occur on the actual physical system rather than in simulation. However,
during this training phase, the RL controller lacks prior knowledge and it
exerts control force on the structure randomly, which can potentially harm the
structure. To mitigate this risk, we propose guiding the RL controller using a
Linear Quadratic Regulator (LQR) controller. While LQR control typically relies
on an accurate structural model for optimal performance, our observations
indicate that even an LQR controller based on an entirely incorrect model
outperforms the uncontrolled scenario. Motivated by this finding, we introduce
a hybrid control framework that integrates both LQR and RL controllers. In this
approach, the LQR policy is derived from a randomly selected model and its
parameters. As this LQR policy does not require knowledge of the true or an
approximate structural model the overall framework remains model-free. This
hybrid approach eliminates dependency on explicit system models while
minimizing exploration risks inherent in naive RL implementations. As per our
knowledge, this is the first study to address the critical training safety
challenge of RL-based vibration control and provide a validated solution.

</details>


### [96] [Identifying Information-Transfer Nodes in a Recurrent Neural Network Reveals Dynamic Representations](https://arxiv.org/abs/2510.01271)
*Arend Hintze,Asadullah Najam,Jory Schossau*

Main category: cs.LG

TL;DR: 本文提出信息论方法分析RNN信息传递节点，应用于不同任务和架构，揭示信息传递模式并进行节点敲除实验，提升对RNN机制理解。


<details>
  <summary>Details</summary>
Motivation: 理解RNN内部动态，提升其可解释性和改进设计。

Method: 引入信息论方法，量化节点输入输出向量互信息确定信息传递关键路径，应用于合成和真实时间序列分类任务，采用不同RNN架构，并进行节点敲除实验。

Result: 揭示不同架构中信息传递的不同模式，明确特定节点对网络整体行为的影响。

Conclusion: 增强对RNN复杂机制的理解，为设计更鲁棒和可解释的神经网络提供工具。

Abstract: Understanding the internal dynamics of Recurrent Neural Networks (RNNs) is
crucial for advancing their interpretability and improving their design. This
study introduces an innovative information-theoretic method to identify and
analyze information-transfer nodes within RNNs, which we refer to as
\textit{information relays}. By quantifying the mutual information between
input and output vectors across nodes, our approach pinpoints critical pathways
through which information flows during network operations. We apply this
methodology to both synthetic and real-world time series classification tasks,
employing various RNN architectures, including Long Short-Term Memory (LSTM)
networks and Gated Recurrent Units (GRUs). Our results reveal distinct patterns
of information relay across different architectures, offering insights into how
information is processed and maintained over time. Additionally, we conduct
node knockout experiments to assess the functional importance of identified
nodes, significantly contributing to explainable artificial intelligence by
elucidating how specific nodes influence overall network behavior. This study
not only enhances our understanding of the complex mechanisms driving RNNs but
also provides a valuable tool for designing more robust and interpretable
neural networks.

</details>


### [97] [Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning](https://arxiv.org/abs/2510.01278)
*Hengwei Zhao,Zhengzhong Tu,Zhuo Zheng,Wei Wang,Junjue Wang,Rusty Feagin,Wenzhe Jiao*

Main category: cs.LG

TL;DR: 提出无辅助信息的非对比PU学习框架NcPU，结合NoiSNCL损失和PLD方案，在多数据集上超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有PU学习方法在复杂数据集上远不如有监督方法，主要瓶颈是在不可靠监督下学习判别性表示困难。

Method: 提出NcPU框架，结合NoiSNCL损失和PLD方案，两者从期望最大化框架角度可相互受益。

Result: NoiSNCL让简单PU方法有有竞争力表现，NcPU在多数据集上大幅超越现有PU方法。

Conclusion: NcPU在复杂数据集表现出色，有实际应用前景。

Abstract: Positive-Unlabeled (PU) learning aims to train a binary classifier (positive
vs. negative) where only limited positive data and abundant unlabeled data are
available. While widely applicable, state-of-the-art PU learning methods
substantially underperform their supervised counterparts on complex datasets,
especially without auxiliary negatives or pre-estimated parameters (e.g., a
14.26% gap on CIFAR-100 dataset). We identify the primary bottleneck as the
challenge of learning discriminative representations under unreliable
supervision. To tackle this challenge, we propose NcPU, a non-contrastive PU
learning framework that requires no auxiliary information. NcPU combines a
noisy-pair robust supervised non-contrastive loss (NoiSNCL), which aligns
intra-class representations despite unreliable supervision, with a phantom
label disambiguation (PLD) scheme that supplies conservative negative
supervision via regret-based label updates. Theoretically, NoiSNCL and PLD can
iteratively benefit each other from the perspective of the
Expectation-Maximization framework. Empirically, extensive experiments
demonstrate that: (1) NoiSNCL enables simple PU methods to achieve competitive
performance; and (2) NcPU achieves substantial improvements over
state-of-the-art PU methods across diverse datasets, including challenging
datasets on post-disaster building damage mapping, highlighting its promise for
real-world applications. Code: Code will be open-sourced after review.

</details>


### [98] [Microsaccade-Inspired Probing: Positional Encoding Perturbations Reveal LLM Misbehaviours](https://arxiv.org/abs/2510.01288)
*Rui Melo,Rui Abreu,Corina S. Pasareanu*

Main category: cs.LG

TL;DR: 借鉴微扫视提出大语言模型探测方法，能检测多种问题且计算高效。


<details>
  <summary>Details</summary>
Motivation: 受微扫视揭示人类感知隐藏动态启发，为大语言模型提出类似探测方法。

Method: 使用轻量级位置编码扰动来引出指示模型不当行为的潜在信号，无需微调或特定任务监督。

Result: 在多个最先进大语言模型上实验表明，基于扰动的探测方法能发现不当行为且计算高效。

Conclusion: 预训练大语言模型已编码标记自身失败所需的内部证据，受微扫视启发的干预为检测和缓解不良行为提供途径。

Abstract: We draw inspiration from microsaccades, tiny involuntary eye movements that
reveal hidden dynamics of human perception, to propose an analogous probing
method for large language models (LLMs). Just as microsaccades expose subtle
but informative shifts in vision, we show that lightweight position encoding
perturbations elicit latent signals that indicate model misbehaviour. Our
method requires no fine-tuning or task-specific supervision, yet detects
failures across diverse settings including factuality, safety, toxicity, and
backdoor attacks. Experiments on multiple state-of-the-art LLMs demonstrate
that these perturbation-based probes surface misbehaviours while remaining
computationally efficient. These findings suggest that pretrained LLMs already
encode the internal evidence needed to flag their own failures, and that
microsaccade-inspired interventions provide a pathway for detecting and
mitigating undesirable behaviours.

</details>


### [99] [ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models](https://arxiv.org/abs/2510.01290)
*Akshat Ramachandran,Marina Neseem,Charbel Sakr,Rangharajan Venkatesan,Brucek Khailany,Tushar Krishna*

Main category: cs.LG

TL;DR: 提出ThinKV框架解决大推理模型长输出上下文生成时KV缓存增长过快问题，实验显示其能以少量缓存实现近无损精度并提升推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大推理模型长输出上下文生成使KV缓存快速增长，导致GPU内存过载。

Method: 提出ThinKV框架，基于注意力稀疏性观察，采用混合量化 - 逐出策略，设计扩展PagedAttention的内核以高效重用内存槽。

Result: 在多个模型和基准测试中，ThinKV用不到原KV缓存5%实现近无损精度，推理吞吐量比现有基线最高提升5.8倍。

Conclusion: ThinKV能有效解决KV缓存增长问题，提高推理性能。

Abstract: The long-output context generation of large reasoning models enables extended
chain of thought (CoT) but also drives rapid growth of the key-value (KV)
cache, quickly overwhelming GPU memory. To address this challenge, we propose
ThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on
the observation that attention sparsity reveals distinct thought types with
varying importance within the CoT. It applies a hybrid quantization-eviction
strategy, assigning token precision by thought importance and progressively
evicting tokens from less critical thoughts as reasoning trajectories evolve.
Furthermore, to implement ThinKV, we design a kernel that extends
PagedAttention to enable efficient reuse of evicted tokens' memory slots,
eliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,
GPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show
that ThinKV achieves near-lossless accuracy with less than 5% of the original
KV cache, while improving performance with up to 5.8x higher inference
throughput over state-of-the-art baselines.

</details>


### [100] [Network-Level Vehicle Delay Estimation at Heterogeneous Signalized Intersections](https://arxiv.org/abs/2510.01292)
*Xiaobo Ma,Hyunsoo Noh,James Tokishi,Ryan Hatch*

Main category: cs.LG

TL;DR: 本文提出用于跨不同交叉口估计车辆延误的领域自适应框架GBBW，经测试能提供更准确稳健的延误估计，利于机器学习技术在交通系统部署。


<details>
  <summary>Details</summary>
Motivation: 准确估计车辆延误对评估信号交叉口性能和制定交通管理策略至关重要，但传统机器学习模型因数据分布假设不满足导致泛化性差，故需解决该问题。

Method: 引入领域自适应（DA）框架，将数据分为源域和目标域，提取关键交通特征，用目标域小部分标记数据微调模型，提出GBBW模型对源数据重加权。

Result: 使用亚利桑那州皮马县57个异构交叉口数据测试，与八个最先进的ML回归模型和七个基于实例的DA方法对比，GBBW框架提供更准确和稳健的延误估计。

Conclusion: 该方法支持更可靠的交通信号优化、拥堵管理和基于性能的规划，增强模型可迁移性，利于机器学习技术在现实交通系统更广泛部署。

Abstract: Accurate vehicle delay estimation is essential for evaluating the performance
of signalized intersections and informing traffic management strategies. Delay
reflects congestion levels and affects travel time reliability, fuel use, and
emissions. Machine learning (ML) offers a scalable, cost-effective alternative;
However, conventional models typically assume that training and testing data
follow the same distribution, an assumption that is rarely satisfied in
real-world applications. Variations in road geometry, signal timing, and driver
behavior across intersections often lead to poor generalization and reduced
model accuracy. To address this issue, this study introduces a domain
adaptation (DA) framework for estimating vehicle delays across diverse
intersections. The framework separates data into source and target domains,
extracts key traffic features, and fine-tunes the model using a small, labeled
subset from the target domain. A novel DA model, Gradient Boosting with
Balanced Weighting (GBBW), reweights source data based on similarity to the
target domain, improving adaptability. The framework is tested using data from
57 heterogeneous intersections in Pima County, Arizona. Performance is
evaluated against eight state-of-the-art ML regression models and seven
instance-based DA methods. Results demonstrate that the GBBW framework provides
more accurate and robust delay estimates. This approach supports more reliable
traffic signal optimization, congestion management, and performance-based
planning. By enhancing model transferability, the framework facilitates broader
deployment of machine learning techniques in real-world transportation systems.

</details>


### [101] [From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review](https://arxiv.org/abs/2510.01296)
*Emma McMillian,Abhirup Banerjee,Alfonso Bueno-Orovio*

Main category: cs.LG

TL;DR: 本文综述基于深度学习的3D MRI重建方法，分析不同类别方法，探讨临床应用、数据等方面，指出新兴研究方向，为研究者提供结构化概述。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的3D MRI重建在医疗领域愈发重要，需要对相关方法进行梳理总结，以推动该领域发展获得更优解决方案。

Method: 对基于深度学习的3D MRI重建方法进行分类，聚焦点云、基于网格、形状感知和体素模型4种主要方法，分析其现有技术、原理、局限性和应用。

Result: 对各类方法在不同解剖结构成像中的情况展开广泛概述，探讨模型临床适用性、数据影响，研究公开数据集、计算需求和评估指标。

Conclusion: 为研究者提供当前3D重建方法的结构化概述，指明新兴研究方向如多模态集成和跨模态框架，助力推进深度学习在该领域发展。

Abstract: Deep learning-based 3-dimensional (3D) shape reconstruction from
2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly
important in medical disease diagnosis, treatment planning, and computational
modeling. This review surveys the methodological landscape of 3D MRI
reconstruction, focusing on 4 primary approaches: point cloud, mesh-based,
shape-aware, and volumetric models. For each category, we analyze the current
state-of-the-art techniques, their methodological foundation, limitations, and
applications across anatomical structures. We provide an extensive overview
ranging from cardiac to neurological to lung imaging. We also focus on the
clinical applicability of models to diseased anatomy, and the influence of
their training and testing data. We examine publicly available datasets,
computational demands, and evaluation metrics. Finally, we highlight the
emerging research directions including multimodal integration and
cross-modality frameworks. This review aims to provide researchers with a
structured overview of current 3D reconstruction methodologies to identify
opportunities for advancing deep learning towards more robust, generalizable,
and clinically impactful solutions.

</details>


### [102] [Low Rank Gradients and Where to Find Them](https://arxiv.org/abs/2510.01303)
*Rishi Sonthalia,Michael Murray,Guido Montúfar*

Main category: cs.LG

TL;DR: 本文研究两层神经网络训练损失梯度的低秩结构，放松了训练数据和参数的各向同性假设，证明输入权重梯度近似低秩，由两项主导，并通过实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 在放松训练数据和参数各向同性假设的情况下，研究两层神经网络训练损失梯度的低秩结构。

Method: 考虑尖峰数据模型，分析平均场和神经切向核缩放，研究训练数据、缩放机制和激活函数的性质。

Result: 输入权重的梯度近似低秩，由两项主导，标准正则化器可选择性调节这些分量。

Conclusion: 通过理论分析和实验，验证了关于两层神经网络训练损失梯度低秩结构的相关结论。

Abstract: This paper investigates low-rank structure in the gradients of the training
loss for two-layer neural networks while relaxing the usual isotropy
assumptions on the training data and parameters. We consider a spiked data
model in which the bulk can be anisotropic and ill-conditioned, we do not
require independent data and weight matrices and we also analyze both the
mean-field and neural-tangent-kernel scalings. We show that the gradient with
respect to the input weights is approximately low rank and is dominated by two
rank-one terms: one aligned with the bulk data-residue , and another aligned
with the rank one spike in the input data. We characterize how properties of
the training data, the scaling regime and the activation function govern the
balance between these two components. Additionally, we also demonstrate that
standard regularizers, such as weight decay, input noise and Jacobian
penalties, also selectively modulate these components. Experiments on synthetic
and real data corroborate our theoretical predictions.

</details>


### [103] [Quantum-inspired Benchmark for Estimating Intrinsic Dimension](https://arxiv.org/abs/2510.01335)
*Aritra Das,Joseph T. Iosue,Victor V. Albert*

Main category: cs.LG

TL;DR: 本文提出量子启发的本征维数估计（QuIIEst）基准，测试IDE方法，发现其在QuIIEst流形上准确性低于现有基准，还对分形进行IDE。


<details>
  <summary>Details</summary>
Motivation: 现有IDE方法估计结果差异大，需在比现有基准更复杂的流形上对IDE方法进行基准测试。

Method: 提出QuIIEst基准，该基准源于量子光学方法，可嵌入任意齐性空间，允许曲率修改和加性噪声。

Result: IDE方法在QuIIEst流形上准确性低于现有基准，且随着曲率非均匀性增加，性能下降极小。

Conclusion: QuIIEst基准有内在难度，还对分形进行IDE，确定哪些方法能提取非流形空间的有效维度。

Abstract: Machine learning models can generalize well on real-world datasets. According
to the manifold hypothesis, this is possible because datasets lie on a latent
manifold with small intrinsic dimension (ID). There exist many methods for ID
estimation (IDE), but their estimates vary substantially. This warrants
benchmarking IDE methods on manifolds that are more complex than those in
existing benchmarks. We propose a Quantum-Inspired Intrinsic-dimension
Estimation (QuIIEst) benchmark consisting of infinite families of topologically
non-trivial manifolds with known ID. Our benchmark stems from a quantum-optical
method of embedding arbitrary homogeneous spaces while allowing for curvature
modification and additive noise. The IDE methods tested were generally less
accurate on QuIIEst manifolds than on existing benchmarks under identical
resource allocation. We also observe minimal performance degradation with
increasingly non-uniform curvature, underscoring the benchmark's inherent
difficulty. As a result of independent interest, we perform IDE on the fractal
Hofstadter's butterfly and identify which methods are capable of extracting the
effective dimension of a space that is not a manifold.

</details>


### [104] [On the Identifiability of Latent Action Policies](https://arxiv.org/abs/2510.01337)
*Sébastien Lachapelle*

Main category: cs.LG

TL;DR: 研究潜在动作策略学习（LAPO）的可识别性，证明特定条件下正则化目标可识别满足要求的动作表示，解释离散动作表示表现好的原因。


<details>
  <summary>Details</summary>
Motivation: 研究LAPO框架中动作表示的可识别性问题。

Method: 正式描述动作表示的要求、统计优势和不可识别性来源，证明熵正则化的LAPO目标可识别动作表示。

Result: 证明在合适条件下，熵正则化的LAPO目标能识别满足要求的动作表示。

Conclusion: 分析解释了离散动作表示在实践中表现良好的原因。

Abstract: We study the identifiability of latent action policy learning (LAPO), a
framework introduced recently to discover representations of actions from video
data. We formally describe desiderata for such representations, their
statistical benefits and potential sources of unidentifiability. Finally, we
prove that an entropy-regularized LAPO objective identifies action
representations satisfying our desiderata, under suitable conditions. Our
analysis provides an explanation for why discrete action representations
perform well in practice.

</details>


### [105] [Self-Supervised Representation Learning as Mutual Information Maximization](https://arxiv.org/abs/2510.01345)
*Akhlaqur Rahman Sabby,Yi Sui,Tongzi Wu,Jesse C. Cresswell,Ga Wu*

Main category: cs.LG

TL;DR: 本文采用第一性原理方法研究自监督表征学习（SSRL），推导了两种训练范式SDMI和JMI，解释了现有SSRL方法架构组件选择的理论原因。


<details>
  <summary>Details</summary>
Motivation: 当前SSRL的底层原理理解不足，架构元素常被视为经验性添加，需从理论上探究学习目标与优化策略、模型设计的关系。

Method: 从变分互信息下界出发，推导SDMI和JMI两种训练范式。

Result: SDMI需交替优化，stop - gradient操作理论上必要；JMI可通过对称架构联合优化；SDMI的预测网络和JMI的统计正则化器是互信息目标的替代；许多现有SSRL方法是这两种范式的实例或近似。

Conclusion: 为现有SSRL方法不同架构组件的选择提供了理论解释，而非仅基于启发式便利。

Abstract: Self-supervised representation learning (SSRL) has demonstrated remarkable
empirical success, yet its underlying principles remain insufficiently
understood. While recent works attempt to unify SSRL methods by examining their
information-theoretic objectives or summarizing their heuristics for preventing
representation collapse, architectural elements like the predictor network,
stop-gradient operation, and statistical regularizer are often viewed as
empirically motivated additions. In this paper, we adopt a first-principles
approach and investigate whether the learning objective of an SSRL algorithm
dictates its possible optimization strategies and model design choices. In
particular, by starting from a variational mutual information (MI) lower bound,
we derive two training paradigms, namely Self-Distillation MI (SDMI) and Joint
MI (JMI), each imposing distinct structural constraints and covering a set of
existing SSRL algorithms. SDMI inherently requires alternating optimization,
making stop-gradient operations theoretically essential. In contrast, JMI
admits joint optimization through symmetric architectures without such
components. Under the proposed formulation, predictor networks in SDMI and
statistical regularizers in JMI emerge as tractable surrogates for the MI
objective. We show that many existing SSRL methods are specific instances or
approximations of these two paradigms. This paper provides a theoretical
explanation behind the choices of different architectural components of
existing SSRL methods, beyond heuristic conveniences.

</details>


### [106] [To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking](https://arxiv.org/abs/2510.01349)
*Hannah Lawrence,Elyssa Hofgard,Vasco Portilheiro,Yuxuan Chen,Tess Smidt,Robin Walters*

Main category: cs.LG

TL;DR: 本文提出一种量化数据集各向异性的指标，验证该指标并研究分布对称破缺对不变性方法的影响，发现对称感知方法的效果因数据集而异。


<details>
  <summary>Details</summary>
Motivation: 评估对称感知机器学习方法中‘转换后数据点在测试分布下高度可能’这一假设。

Method: 通过两样本神经分类器测试，区分原始数据集及其随机增强版本，提出量化各向异性的指标。

Result: 在合成数据集验证指标，发现基准点云数据集有高度对齐；理论证明分布对称破缺会影响不变性方法；实证发现对称感知方法效果因数据集而异。

Conclusion: 理解等变性可能需要重新思考数据中的对称偏差。

Abstract: Symmetry-aware methods for machine learning, such as data augmentation and
equivariant architectures, encourage correct model behavior on all
transformations (e.g. rotations or permutations) of the original dataset. These
methods can improve generalization and sample efficiency, under the assumption
that the transformed datapoints are highly probable, or "important", under the
test distribution. In this work, we develop a method for critically evaluating
this assumption. In particular, we propose a metric to quantify the amount of
anisotropy, or symmetry-breaking, in a dataset, via a two-sample neural
classifier test that distinguishes between the original dataset and its
randomly augmented equivalent. We validate our metric on synthetic datasets,
and then use it to uncover surprisingly high degrees of alignment in several
benchmark point cloud datasets. We show theoretically that distributional
symmetry-breaking can actually prevent invariant methods from performing
optimally even when the underlying labels are truly invariant, as we show for
invariant ridge regression in the infinite feature limit. Empirically, we find
that the implication for symmetry-aware methods is dataset-dependent:
equivariant methods still impart benefits on some anisotropic datasets, but not
others. Overall, these findings suggest that understanding equivariance -- both
when it works, and why -- may require rethinking symmetry biases in the data.

</details>


### [107] [RheOFormer: A generative transformer model for simulation of complex fluids and flows](https://arxiv.org/abs/2510.01365)
*Maedeh Saberi,Amir Barati Farimani,Safa Jamali*

Main category: cs.LG

TL;DR: 本文介绍RheOFormer方法，能有效学习复杂流体流动特征，即使在有限数据集上训练，也可准确学习复杂流体力学特性，具有强泛化能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统非牛顿流体动力学数值方法计算需求高、扩展性差，数据驱动方法需跨不同物理条件重新训练，需新方法解决这些问题。

Method: 引入Rheological Operator Transformer (RheOFormer)，一种利用自注意力机制的生成式算子学习方法，学习复杂流体流动的不同空间相互作用和特征。

Result: RheOFormer能准确学习不同复杂流体的标量和张量非线性力学特性，预测其流动的时空演化，即使在有限数据集上训练。

Conclusion: RheOFormer可作为强大的神经替代模型，加速复杂流体预测模拟，推动数据驱动实验，实现广泛应用中的实时过程优化。

Abstract: The ability to model mechanics of soft materials under flowing conditions is
key in designing and engineering processes and materials with targeted
properties. This generally requires solution of internal stress tensor, related
to the deformation tensor through nonlinear and history-dependent constitutive
models. Traditional numerical methods for non-Newtonian fluid dynamics often
suffer from prohibitive computational demands and poor scalability to new
problem instances. Developments in data-driven methods have mitigated some
limitations but still require retraining across varied physical conditions. In
this work, we introduce Rheological Operator Transformer (RheOFormer), a
generative operator learning method leveraging self-attention to efficiently
learn different spatial interactions and features of complex fluid flows. We
benchmark RheOFormer across a range of different viscometric and
non-viscometric flows with different types of viscoelastic and
elastoviscoplastic mechanics in complex domains against ground truth solutions.
Our results demonstrate that RheOFormer can accurately learn both scalar and
tensorial nonlinear mechanics of different complex fluids and predict the
spatio-temporal evolution of their flows, even when trained on limited
datasets. Its strong generalization capabilities and computational efficiency
establish RheOFormer as a robust neural surrogate for accelerating predictive
complex fluid simulations, advancing data-driven experimentation, and enabling
real-time process optimization across a wide range of applications.

</details>


### [108] [Selective Underfitting in Diffusion Models](https://arxiv.org/abs/2510.01378)
*Kiwhan Song,Jaeyeon Kim,Sitan Chen,Yilun Du,Sham Kakade,Vincent Sitzmann*

Main category: cs.LG

TL;DR: 本文探讨扩散模型学习的分数问题，提出选择性欠拟合概念并验证，揭示其对理解扩散模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型实际学习何种分数的基本问题，因完全匹配经验分数会无法生成新样本。

Method: 引入选择性欠拟合概念，刻画相关区域并设计实证干预。

Result: 证实选择性欠拟合对理解扩散模型至关重要，带来关于其泛化和生成性能的新可验证见解。

Conclusion: 选择性欠拟合对于理解扩散模型有重要意义，有助于深入了解其泛化和生成性能。

Abstract: Diffusion models have emerged as the principal paradigm for generative
modeling across various domains. During training, they learn the score
function, which in turn is used to generate samples at inference. They raise a
basic yet unsolved question: which score do they actually learn? In principle,
a diffusion model that matches the empirical score in the entire data space
would simply reproduce the training data, failing to generate novel samples.
Recent work addresses this question by arguing that diffusion models underfit
the empirical score due to training-time inductive biases. In this work, we
refine this perspective, introducing the notion of selective underfitting:
instead of underfitting the score everywhere, better diffusion models more
accurately approximate the score in certain regions of input space, while
underfitting it in others. We characterize these regions and design empirical
interventions to validate our perspective. Our results establish that selective
underfitting is essential for understanding diffusion models, yielding new,
testable insights into their generalization and generative performance.

</details>


### [109] [Fine-Tuning Masked Diffusion for Provable Self-Correction](https://arxiv.org/abs/2510.01384)
*Jaeyeon Kim,Seunggeun Kim,Taekyun Lee,David Z. Pan,Hyeji Kim,Sham Kakade,Sitan Chen*

Main category: cs.LG

TL;DR: 提出PRISM方法用于掩码扩散模型推理时的自我纠正，在多个领域提升推理效果。


<details>
  <summary>Details</summary>
Motivation: 现有将自我纠正融入掩码扩散模型（MDMs）的方法存在局限，如需大幅修改架构/训练或依赖不准确的代理指标，影响适用性。

Method: 提出PRISM方法，定义自我纠正损失以学习每个标记的质量分数，在同一前向传播中计算质量分数并检测低质量标记。

Result: PRISM在数独、无条件文本（170M）和代码（LLaDA 8B）等多个领域和规模上提升了MDM的推理效果。

Conclusion: PRISM是一种轻量级、模型无关的方法，可应用于任何预训练的MDM，能有效提升其推理能力。

Abstract: A natural desideratum for generative models is self-correction--detecting and
revising low-quality tokens at inference. While Masked Diffusion Models (MDMs)
have emerged as a promising approach for generative modeling in discrete
spaces, their capacity for self-correction remains poorly understood. Prior
attempts to incorporate self-correction into MDMs either require overhauling
MDM architectures/training or rely on imprecise proxies for token quality,
limiting their applicability. Motivated by this, we introduce PRISM--Plug-in
Remasking for Inference-time Self-correction of Masked Diffusions--a
lightweight, model-agnostic approach that applies to any pretrained MDM.
Theoretically, PRISM defines a self-correction loss that provably learns
per-token quality scores, without RL or a verifier. These quality scores are
computed in the same forward pass with MDM and used to detect low-quality
tokens. Empirically, PRISM advances MDM inference across domains and scales:
Sudoku; unconditional text (170M); and code with LLaDA (8B).

</details>


### [110] [Optimal Stopping vs Best-of-$N$ for Inference Time Optimization](https://arxiv.org/abs/2510.01394)
*Yusuf Kalayci,Vinod Raman,Shaddin Dughmi*

Main category: cs.LG

TL;DR: 本文引入基于潘多拉盒子问题的推理时间优化框架，提出UCB风格算法并改进用于大语言模型，实验表明该自适应策略能减少生成次数且性能相当。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型生成时平衡输出质量和推理成本的问题。

Method: 基于潘多拉盒子问题，提出UCB风格算法，通过受Bradley - Terry启发的变换调整方法以适应实际大语言模型设置。

Result: 在AlpacaFarm和HH - RLHF数据集上，使用多组大语言模型 - 奖励模型对进行实验，自适应策略能获得与非自适应Best - of - N采样相同性能，平均减少15 - 35%的生成次数。

Conclusion: 建立了最优停止理论和推理时间扩展之间的原则性桥梁，为大语言模型部署提供理论性能界限和实际效率提升。

Abstract: Large language model (LLM) generation often requires balancing output quality
against inference cost, especially when using multiple generations. We
introduce a new framework for inference-time optimization based on the
classical Pandora's Box problem. Viewing each generation as opening a costly
"box" with random reward, we develop algorithms that decide when to stop
generating without knowing the underlying reward distribution. Our first
contribution is a UCB-style Pandora's Box algorithm, which achieves performance
that is provably close to Weitzman's algorithm, the optimal strategy when the
distribution is known. We further adapt this method to practical LLM settings
by addressing reward scaling across prompts via a Bradley-Terry inspired
transformation. This leads to an adaptive inference-time optimization method
that normalizes rewards and learns stopping thresholds on the fly. Experiments
on the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs,
show that our adaptive strategy can obtain the same performance as non-adaptive
Best-of-N sampling while requiring 15-35 percent fewer generations on average.
Our results establish a principled bridge between optimal stopping theory and
inference-time scaling, providing both theoretical performance bounds and
practical efficiency gains for LLM deployment.

</details>


### [111] [Neural Network Surrogates for Free Energy Computation of Complex Chemical Systems](https://arxiv.org/abs/2510.01396)
*Wasut Pornpatcharapong*

Main category: cs.LG

TL;DR: 提出神经网络替代框架学习集体变量并提供雅可比矩阵，在MgCl2离子配对系统中表现良好，拓宽模拟范围。


<details>
  <summary>Details</summary>
Motivation: 自由能重建方法需要集体变量的雅可比矩阵，限制了复杂或机器学习集体变量的使用。

Method: 引入神经网络替代框架，直接从笛卡尔坐标学习集体变量，用自动微分提供雅可比矩阵。

Result: 在MgCl2离子配对系统中，简单距离集体变量和复杂配位数集体变量都实现高精度，雅可比误差近似高斯分布。

Conclusion: 该框架使基于梯度的自由能方法能纳入复杂和机器学习集体变量，拓宽生化和材料模拟范围。

Abstract: Free energy reconstruction methods such as Gaussian Process Regression (GPR)
require Jacobians of the collective variables (CVs), a bottleneck that
restricts the use of complex or machine-learned CVs. We introduce a neural
network surrogate framework that learns CVs directly from Cartesian coordinates
and uses automatic differentiation to provide Jacobians, bypassing analytical
forms. On an MgCl2 ion-pairing system, our method achieved high accuracy for
both a simple distance CV and a complex coordination-number CV. Moreover,
Jacobian errors also followed a near-Gaussian distribution, making them
suitable for GPR pipelines. This framework enables gradient-based free energy
methods to incorporate complex and machine-learned CVs, broadening the scope of
biochemistry and materials simulations.

</details>


### [112] [Lower Bounds on Adversarial Robustness for Multiclass Classification with General Loss Functions](https://arxiv.org/abs/2510.01969)
*Camilo Andrés García Trillos,Nicolás García Trillos*

Main category: cs.LG

TL;DR: 本文推导多类设置下任意损失函数的对抗鲁棒分类问题的对偶和重心重述，给出重要损失函数的显式表征，揭示与其他问题联系并通过实验获更紧下界。


<details>
  <summary>Details</summary>
Motivation: 解决多类设置下任意损失函数的对抗鲁棒分类问题，拓展0 - 1损失的已有结果。

Method: 推导对偶和重心重述，给出重要损失函数的显式表征。

Result: 实现对抗风险尖锐下界的高效计算，设计出超越0 - 1损失的鲁棒分类器，通过实验获得交叉熵损失函数对抗风险的更紧下界。

Conclusion: 揭示了对抗鲁棒性、α - 公平打包问题和广义重心问题之间的有趣联系。

Abstract: We consider adversarially robust classification in a multiclass setting under
arbitrary loss functions and derive dual and barycentric reformulations of the
corresponding learner-agnostic robust risk minimization problem. We provide
explicit characterizations for important cases such as the cross-entropy loss,
loss functions with a power form, and the quadratic loss, extending in this way
available results for the 0-1 loss. These reformulations enable efficient
computation of sharp lower bounds for adversarial risks and facilitate the
design of robust classifiers beyond the 0-1 loss setting. Our paper uncovers
interesting connections between adversarial robustness, $\alpha$-fair packing
problems, and generalized barycenter problems for arbitrary positive measures
where Kullback-Leibler and Tsallis entropies are used as penalties. Our
theoretical results are accompanied with illustrative numerical experiments
where we obtain tighter lower bounds for adversarial risks with the
cross-entropy loss function.

</details>


### [113] [Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction](https://arxiv.org/abs/2510.01407)
*Ethan G. Rogers,Cheng Wang*

Main category: cs.LG

TL;DR: 提出基于低秩表示和向量量化自编码器的图像压缩重建框架，减少解码计算开销。


<details>
  <summary>Details</summary>
Motivation: 当代神经压缩方法中基于卷积的解码器在数据重建时复杂度高、计算成本大，阻碍技术应用。

Method: 开发基于在带向量量化的自编码器中引入低秩表示的压缩重建框架，对图像的潜在表示进行低秩运算。

Result: 能高效高质量重建数据。

Conclusion: 大幅降低神经压缩/重建解码阶段的计算开销，消除解码器计算瓶颈，同时保持图像输出的高保真度。

Abstract: Image compression and reconstruction are crucial for various digital
applications. While contemporary neural compression methods achieve impressive
compression rates, the adoption of such technology has been largely hindered by
the complexity and large computational costs of the convolution-based decoders
during data reconstruction. To address the decoder bottleneck in neural
compression, we develop a new compression-reconstruction framework based on
incorporating low-rank representation in an autoencoder with vector
quantization. We demonstrated that performing a series of computationally
efficient low-rank operations on the learned latent representation of images
can efficiently reconstruct the data with high quality. Our approach
dramatically reduces the computational overhead in the decoding phase of neural
compression/reconstruction, essentially eliminating the decoder compute
bottleneck while maintaining high fidelity of image outputs.

</details>


### [114] [Adaptive Heterogeneous Mixtures of Normalising Flows for Robust Variational Inference](https://arxiv.org/abs/2510.02056)
*Benjamin Wiriyapong,Oktay Karakuş,Kirill Sidorov*

Main category: cs.LG

TL;DR: 提出AMF - VI方法，在多种后验分布上表现优于单流模型，且高效可靠。


<details>
  <summary>Details</summary>
Motivation: 单流归一化流变分推理模型在不同分布上表现不稳定，需更鲁棒方法。

Method: 提出AMF - VI，由互补流组成异质混合模型，分两阶段训练：顺序训练单个流，通过似然驱动更新进行自适应全局权重估计。

Result: 在六种典型后验分布上，AMF - VI负对数似然更低，运输指标和最大平均差异表现更好。

Conclusion: 不同流的自适应混合能在不同后验分布上实现可靠变分推理，同时保留各专家归纳偏置，且过程高效、与架构无关。

Abstract: Normalising-flow variational inference (VI) can approximate complex
posteriors, yet single-flow models often behave inconsistently across
qualitatively different distributions. We propose Adaptive Mixture Flow
Variational Inference (AMF-VI), a heterogeneous mixture of complementary flows
(MAF, RealNVP, RBIG) trained in two stages: (i) sequential expert training of
individual flows, and (ii) adaptive global weight estimation via
likelihood-driven updates, without per-sample gating or architectural changes.
Evaluated on six canonical posterior families of banana, X-shape, two-moons,
rings, a bimodal, and a five-mode mixture, AMF-VI achieves consistently lower
negative log-likelihood than each single-flow baseline and delivers stable
gains in transport metrics (Wasserstein-2) and maximum mean discrepancy (MDD),
indicating improved robustness across shapes and modalities. The procedure is
efficient and architecture-agnostic, incurring minimal overhead relative to
standard flow training, and demonstrates that adaptive mixtures of diverse
flows provide a reliable route to robust VI across diverse posterior families
whilst preserving each expert's inductive bias.

</details>


### [115] [Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic Frameworks, and Future Horizons](https://arxiv.org/abs/2510.01439)
*Mohamad Abou Ali,Fadi Dornaika*

Main category: cs.LG

TL;DR: 本文对边缘人工智能进行系统综述，涵盖其发展、现状与未来方向，分析核心技术、挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 系统研究边缘人工智能的发展、现状及未来方向，为研究者和从业者提供全面框架。

Method: 通过多维度分类法，遵循PRISMA指南追溯该领域发展，探索核心技术，评估挑战。

Result: 梳理了边缘人工智能从早期到现代的发展，分析核心技术，指出资源限制等挑战，突出新兴机遇。

Conclusion: 为边缘人工智能领域的研究和实践提供了全面框架。

Abstract: Edge Artificial Intelligence (Edge AI) embeds intelligence directly into
devices at the network edge, enabling real-time processing with improved
privacy and reduced latency by processing data close to its source. This review
systematically examines the evolution, current landscape, and future directions
of Edge AI through a multi-dimensional taxonomy including deployment location,
processing capabilities such as TinyML and federated learning, application
domains, and hardware types. Following PRISMA guidelines, the analysis traces
the field from early content delivery networks and fog computing to modern
on-device intelligence. Core enabling technologies such as specialized hardware
accelerators, optimized software, and communication protocols are explored.
Challenges including resource limitations, security, model management, power
consumption, and connectivity are critically assessed. Emerging opportunities
in neuromorphic hardware, continual learning algorithms, edge-cloud
collaboration, and trustworthiness integration are highlighted, providing a
comprehensive framework for researchers and practitioners.

</details>


### [116] [Inferring Optical Tissue Properties from Photoplethysmography using Hybrid Amortized Inference](https://arxiv.org/abs/2510.02073)
*Jens Behrmann,Maria R. Cervera,Antoine Wehenkel,Andrew C. Miller,Albert Cerussi,Pranay Jain,Vivek Venugopal,Shijie Yan,Guillermo Sapiro,Luca Pegolotti,Jörn-Henrik Jacobsen*

Main category: cs.LG

TL;DR: 本文引入生物物理模型PPGen和混合摊销推理（HAI），从PPG信号中估计生理参数，实验证明HAI有效，为PPG模型发展指明方向。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型依赖生理意义不明确的特征，存在预测能力、临床可解释性和传感器设计之间的矛盾。

Method: 引入生物物理模型PPGen，在此基础上提出混合摊销推理（HAI）。

Result: 在大量计算机模拟实验中，HAI能在不同噪声和传感器条件下准确推断生理参数。

Conclusion: 研究为PPG模型保留深度学习特征的准确性，同时支持临床解释和合理硬件设计指明了道路。

Abstract: Smart wearables enable continuous tracking of established biomarkers such as
heart rate, heart rate variability, and blood oxygen saturation via
photoplethysmography (PPG). Beyond these metrics, PPG waveforms contain richer
physiological information, as recent deep learning (DL) studies demonstrate.
However, DL models often rely on features with unclear physiological meaning,
creating a tension between predictive power, clinical interpretability, and
sensor design. We address this gap by introducing PPGen, a biophysical model
that relates PPG signals to interpretable physiological and optical parameters.
Building on PPGen, we propose hybrid amortized inference (HAI), enabling fast,
robust, and scalable estimation of relevant physiological parameters from PPG
signals while correcting for model misspecification. In extensive in-silico
experiments, we show that HAI can accurately infer physiological parameters
under diverse noise and sensor conditions. Our results illustrate a path toward
PPG models that retain the fidelity needed for DL-based features while
supporting clinical interpretation and informed hardware design.

</details>


### [117] [SoftAdaClip: A Smooth Clipping Strategy for Fair and Private Model Training](https://arxiv.org/abs/2510.01447)
*Dorsa Soleymani,Ali Dadsetan,Frank Rudzicz*

Main category: cs.LG

TL;DR: 提出SoftAdaClip方法解决差分隐私训练中模型性能和公平性问题，实验显示其减少了子组差异。


<details>
  <summary>Details</summary>
Motivation: 差分隐私常降低模型性能和公平性，尤其对代表性不足群体，现有自适应裁剪仍依赖统一硬裁剪限制公平性。

Method: 引入SoftAdaClip方法，用基于tanh的平滑变换替代硬裁剪，在限制敏感性的同时保留相对梯度幅度。

Result: 在多个数据集上评估，SoftAdaClip相比DP - SGD最多减少87%子组差异，相比Adaptive - DPSGD最多减少48%，且差异有统计学意义。

Conclusion: 强调将平滑变换与自适应机制结合对实现公平和隐私模型训练的重要性。

Abstract: Differential privacy (DP) provides strong protection for sensitive data, but
often reduces model performance and fairness, especially for underrepresented
groups. One major reason is gradient clipping in DP-SGD, which can
disproportionately suppress learning signals for minority subpopulations.
Although adaptive clipping can enhance utility, it still relies on uniform hard
clipping, which may restrict fairness. To address this, we introduce
SoftAdaClip, a differentially private training method that replaces hard
clipping with a smooth, tanh-based transformation to preserve relative gradient
magnitudes while bounding sensitivity. We evaluate SoftAdaClip on various
datasets, including MIMIC-III (clinical text), GOSSIS-eICU (structured
healthcare), and Adult Income (tabular data). Our results show that SoftAdaClip
reduces subgroup disparities by up to 87% compared to DP-SGD and up to 48%
compared to Adaptive-DPSGD, and these reductions in subgroup disparities are
statistically significant. These findings underscore the importance of
integrating smooth transformations with adaptive mechanisms to achieve fair and
private model training.

</details>


### [118] [Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression](https://arxiv.org/abs/2510.01450)
*Yifei Zuo,Yutong Yin,Zhichen Zeng,Ang Li,Banghua Zhu,Zhaoran Wang*

Main category: cs.LG

TL;DR: 提出新型注意力机制LLA，经理论分析和计算优化，实验验证其在多任务中优势。


<details>
  <summary>Details</summary>
Motivation: 探索基于理论洞察的更具表达性的注意力机制，弥补该领域研究不足。

Method: 提出LLA，进行偏差 - 方差权衡分析，提出内存高效原语，引入硬件高效算法FlashLLA，实现定制推理内核。

Result: LLA有效适应非平稳性，在测试时训练和上下文学习中优于强基线，展现可扩展性和适用性。

Conclusion: LLA是一种有潜力的注意力机制，适用于大规模模型，代码已开源。

Abstract: Transformer architectures have achieved remarkable success in various
domains. While efficient alternatives to Softmax Attention have been widely
studied, the search for more expressive mechanisms grounded in theoretical
insight-even at greater computational cost-has been relatively underexplored.
In this work, we bridge this gap by proposing Local Linear Attention (LLA), a
novel attention mechanism derived from nonparametric statistics through the
lens of test-time regression. First, we show that LLA offers theoretical
advantages over Linear and Softmax Attention for associative memory via a
bias-variance trade-off analysis. Next, we address its computational challenges
and propose two memory-efficient primitives to tackle the $\Theta(n^2 d)$ and
$\Theta(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient,
blockwise algorithm that enables scalable and parallel computation on modern
accelerators. In addition, we implement and profile a customized inference
kernel that significantly reduces memory overheads. Finally, we empirically
validate the advantages and limitations of LLA on test-time regression,
in-context regression, associative recall and state tracking tasks. Experiment
results demonstrate that LLA effectively adapts to non-stationarity,
outperforming strong baselines in test-time training and in-context learning,
and exhibiting promising evidence for its scalability and applicability in
large-scale models. Code is available at
https://github.com/Yifei-Zuo/Flash-LLA.

</details>


### [119] [Reinforcement Learning with Action-Triggered Observations](https://arxiv.org/abs/2510.02149)
*Alexander Ryabchenko,Wenlong Mou*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study reinforcement learning problems where state observations are
stochastically triggered by actions, a constraint common in many real-world
applications. This framework is formulated as Action-Triggered Sporadically
Traceable Markov Decision Processes (ATST-MDPs), where each action has a
specified probability of triggering a state observation. We derive tailored
Bellman optimality equations for this framework and introduce the
action-sequence learning paradigm in which agents commit to executing a
sequence of actions until the next observation arrives. Under the linear MDP
assumption, value-functions are shown to admit linear representations in an
induced action-sequence feature map. Leveraging this structure, we propose
off-policy estimators with statistical error guarantees for such feature maps
and introduce ST-LSVI-UCB, a variant of LSVI-UCB adapted for action-triggered
settings. ST-LSVI-UCB achieves regret $\widetilde
O(\sqrt{Kd^3(1-\gamma)^{-3}})$, where $K$ is the number of episodes, $d$ the
feature dimension, and $\gamma$ the discount factor (per-step episode
non-termination probability). Crucially, this work establishes the theoretical
foundation for learning with sporadic, action-triggered observations while
demonstrating that efficient learning remains feasible under such observation
constraints.

</details>


### [120] [SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion](https://arxiv.org/abs/2510.01456)
*Brett Barkley,Preston Culbertson,David Fridovich-Keil*

Main category: cs.LG

TL;DR: 提出用于扩散模型的OOD检测方法SCOPED，计算快、通用，在多个任务中表现好。


<details>
  <summary>Details</summary>
Motivation: 为机器学习系统在多领域可靠部署，需要高效的扩散模型OOD检测方法。

Method: 从单一扩散模型计算SCOPED，结合雅可比迹和分数函数平方范数为测试统计量，用核密度估计估计分布内密度。

Result: 在四个视觉基准测试中取得有竞争力或最先进的精确召回分数，可推广到机器人控制任务。

Conclusion: SCOPED可作为现实领域快速可靠OOD检测的实用基础。

Abstract: Out-of-distribution (OOD) detection is essential for reliable deployment of
machine learning systems in vision, robotics, reinforcement learning, and
beyond. We introduce Score-Curvature Out-of-distribution Proximity Evaluator
for Diffusion (SCOPED), a fast and general-purpose OOD detection method for
diffusion models that reduces the number of forward passes on the trained model
by an order of magnitude compared to prior methods, outperforming most
diffusion-based baselines and closely approaching the accuracy of the strongest
ones. SCOPED is computed from a single diffusion model trained once on a
diverse dataset, and combines the Jacobian trace and squared norm of the
model's score function into a single test statistic. Rather than thresholding
on a fixed value, we estimate the in-distribution density of SCOPED scores
using kernel density estimation, enabling a flexible, unsupervised test that,
in the simplest case, only requires a single forward pass and one
Jacobian-vector product (JVP), made efficient by Hutchinson's trace estimator.
On four vision benchmarks, SCOPED achieves competitive or state-of-the-art
precision-recall scores despite its low computational cost. The same method
generalizes to robotic control tasks with shared state and action spaces,
identifying distribution shifts across reward functions and training regimes.
These results position SCOPED as a practical foundation for fast and reliable
OOD detection in real-world domains, including perceptual artifacts in vision,
outlier detection in autoregressive models, exploration in reinforcement
learning, and dataset curation for unsupervised training.

</details>


### [121] [Flatness-Aware Stochastic Gradient Langevin Dynamics](https://arxiv.org/abs/2510.02174)
*Stefano Bruno,Youngsik Hwang,Jaehyeon An,Sotirios Sabanis,Dong-Young Lim*

Main category: cs.LG

TL;DR: 提出Flatness - Aware Stochastic Gradient Langevin Dynamics (fSGLD) 算法，可高效寻找高维非凸优化问题的平坦极小值，理论证明其合理性，实验表明性能优越。


<details>
  <summary>Details</summary>
Motivation: 经典的随机梯度朗之万动力学（SGLD）无法倾向于低曲率的平坦极小值解，需要改进算法来寻找平坦极小值以提升深度学习泛化性。

Method: 提出 fSGLD 算法，每次迭代使用各向同性高斯噪声扰动参数后的随机梯度，优化随机平滑目标以隐式捕获曲率信息。

Result: 理论上证明了 fSGLD 的不变测度接近正则化损失函数全局极小值的平稳测度，给出收敛保证和超额风险界；实验表明在多任务中 fSGLD 泛化性和鲁棒性优越，计算成本与 SGD 相当，收敛到更平坦的极小值。

Conclusion: fSGLD 能有效寻找平坦极小值，提升深度学习泛化性和鲁棒性，且有理论保障和较好的计算效率。

Abstract: Generalization in deep learning is closely tied to the pursuit of flat minima
in the loss landscape, yet classical Stochastic Gradient Langevin Dynamics
(SGLD) offers no mechanism to bias its dynamics toward such low-curvature
solutions. This work introduces Flatness-Aware Stochastic Gradient Langevin
Dynamics (fSGLD), designed to efficiently and provably seek flat minima in
high-dimensional nonconvex optimization problems. At each iteration, fSGLD uses
the stochastic gradient evaluated at parameters perturbed by isotropic Gaussian
noise, commonly referred to as Random Weight Perturbation (RWP), thereby
optimizing a randomized-smoothing objective that implicitly captures curvature
information. Leveraging these properties, we prove that the invariant measure
of fSGLD stays close to a stationary measure concentrated on the global
minimizers of a loss function regularized by the Hessian trace whenever the
inverse temperature and the scale of random weight perturbation are properly
coupled. This result provides a rigorous theoretical explanation for the
benefits of random weight perturbation. In particular, we establish
non-asymptotic convergence guarantees in Wasserstein distance with the best
known rate and derive an excess-risk bound for the Hessian-trace regularized
objective. Extensive experiments on noisy-label and large-scale vision tasks,
in both training-from-scratch and fine-tuning settings, demonstrate that fSGLD
achieves superior or comparable generalization and robustness to baseline
algorithms while maintaining the computational cost of SGD, about half that of
SAM. Hessian-spectrum analysis further confirms that fSGLD converges to
significantly flatter minima.

</details>


### [122] [Fixing That Free Lunch: When, Where, and Why Synthetic Data Fails in Model-Based Policy Optimization](https://arxiv.org/abs/2510.01457)
*Brett Barkley,David Fridovich-Keil*

Main category: cs.LG

TL;DR: 研究合成数据在基于模型的强化学习中何时有效、何时失效及原因，解决失效模式可提升策略性能。


<details>
  <summary>Details</summary>
Motivation: 合成数据在基于模型的强化学习中可能降低性能，探究其作用与失效原因，解决失效模式以实现策略改进。

Method: 聚焦MBPO算法，分析其在不同环境中的表现，找出导致性能下降的两个耦合问题。

Result: 解决失效模式后，MBPO在7个任务中的5个上超过SAC，同时保持在OpenAI Gym中的良好表现。

Conclusion: 希望推动社区开发将MDP任务和环境结构与算法失效模式相关联的分类法，寻求统一解决方案，明确基准选择对算法泛化条件的影响。

Abstract: Synthetic data is a core component of data-efficient Dyna-style model-based
reinforcement learning, yet it can also degrade performance. We study when it
helps, where it fails, and why, and we show that addressing the resulting
failure modes enables policy improvement that was previously unattainable. We
focus on Model-Based Policy Optimization (MBPO), which performs actor and
critic updates using synthetic action counterfactuals. Despite reports of
strong and generalizable sample-efficiency gains in OpenAI Gym, recent work
shows that MBPO often underperforms its model-free counterpart, Soft
Actor-Critic (SAC), in the DeepMind Control Suite (DMC). Although both suites
involve continuous control with proprioceptive robots, this shift leads to
sharp performance losses across seven challenging DMC tasks, with MBPO failing
in cases where claims of generalization from Gym would imply success. This
reveals how environment-specific assumptions can become implicitly encoded into
algorithm design when evaluation is limited. We identify two coupled issues
behind these failures: scale mismatches between dynamics and reward models that
induce critic underestimation and hinder policy improvement during model-policy
coevolution, and a poor choice of target representation that inflates model
variance and produces error-prone rollouts. Addressing these failure modes
enables policy improvement where none was previously possible, allowing MBPO to
outperform SAC in five of seven tasks while preserving the strong performance
previously reported in OpenAI Gym. Rather than aiming only for incremental
average gains, we hope our findings motivate the community to develop
taxonomies that tie MDP task- and environment-level structure to algorithmic
failure modes, pursue unified solutions where possible, and clarify how
benchmark choices ultimately shape the conditions under which algorithms
generalize.

</details>


### [123] [Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification](https://arxiv.org/abs/2510.02216)
*Zeqi Ye,Minshuo Chen*

Main category: cs.LG

TL;DR: 本文研究条件扩散变压器用于时间序列数据插补的统计效率和缺失值不确定性，推导样本复杂度边界，发现插补受缺失模式影响，通过模拟验证并提出训练策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的生成插补方法缺乏对其捕捉缺失值与观测值间复杂时空依赖的理论理解。

Method: 基于变压器的条件得分函数的近似理论推导统计样本复杂度边界，构建缺失值的置信区域，进行模拟验证并提出混合掩码训练策略。

Result: 发现插补的效率和准确性受缺失模式显著影响。

Conclusion: 提出的理论和方法可用于提升时间序列数据插补性能，混合掩码训练策略能增强插补效果。

Abstract: Imputation methods play a critical role in enhancing the quality of practical
time-series data, which often suffer from pervasive missing values. Recently,
diffusion-based generative imputation methods have demonstrated remarkable
success compared to autoregressive and conventional statistical approaches.
Despite their empirical success, the theoretical understanding of how well
diffusion-based models capture complex spatial and temporal dependencies
between the missing values and observed ones remains limited. Our work
addresses this gap by investigating the statistical efficiency of conditional
diffusion transformers for imputation and quantifying the uncertainty in
missing values. Specifically, we derive statistical sample complexity bounds
based on a novel approximation theory for conditional score functions using
transformers, and, through this, construct tight confidence regions for missing
values. Our findings also reveal that the efficiency and accuracy of imputation
are significantly influenced by the missing patterns. Furthermore, we validate
these theoretical insights through simulation and propose a mixed-masking
training strategy to enhance the imputation performance.

</details>


### [124] [How Well Can Preference Optimization Generalize Under Noisy Feedback?](https://arxiv.org/abs/2510.01458)
*Shawn Im,Yixuan Li*

Main category: cs.LG

TL;DR: 论文研究含噪声反馈对偏好优化的影响，给出泛化保证，分析不同噪声类型和水平下泛化性衰减，适用于多种损失函数，实证验证了结果的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化工作大多假设反馈无噪声，而实际人类判断存在误差和不一致性，需研究含噪声反馈对偏好优化的影响。

Method: 考虑对应现实噪声源的噪声模型，聚焦有限步偏好优化，根据偏好数据分布和样本数量分析不同噪声类型和噪声率下的泛化性衰减。

Result: 分析适用于多种偏好优化损失函数，在当代大语言模型上的实证验证了研究结果的实用性。

Conclusion: 研究为开发符合人类偏好的AI系统提供了有价值的见解。

Abstract: As large language models (LLMs) advance their capabilities, aligning these
models with human preferences has become crucial. Preference optimization,
which trains models to distinguish between preferred and non-preferred
responses based on human feedback, has become a crucial component for aligning
LLMs. However, most existing works assume noise-free feedback, which is
unrealistic due to the inherent errors and inconsistencies in human judgments.
This paper addresses the impact of noisy feedback on preference optimization,
providing generalization guarantees under these conditions. In particular, we
consider noise models that correspond to common real-world sources of noise,
such as mislabeling and uncertainty. Unlike traditional analyses that assume
convergence, our work focuses on finite-step preference optimization, offering
new insights that are more aligned with practical LLM training. We describe how
generalization decays with different types of noise across levels of noise
rates based on the preference data distribution and number of samples. Our
analysis for noisy preference learning applies to a broad family of preference
optimization losses such as DPO, IPO, SLiC, etc. Empirical validation on
contemporary LLMs confirms the practical relevance of our findings, offering
valuable insights for developing AI systems that align with human preferences.

</details>


### [125] [Efficiently Generating Correlated Sample Paths from Multi-step Time Series Foundation Models](https://arxiv.org/abs/2510.02224)
*Ethan Baron,Boris Oreshkin,Ruijun Ma,Hanyu Zhang,Kari Torkkola,Michael W. Mahoney,Andrew Gordon Wilson,Tatiana Konstantinova*

Main category: cs.LG

TL;DR: 本文提出基于copula的方法，能从现有多步时间序列基础模型高效生成准确且相关的样本路径。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型仅预测各时间步独立边缘分布，生成有现实相关结构的预测样本路径时，自回归采样方法成本高。

Method: 提出基于copula的方法，在一次前向传播中从现有多步时间序列基础模型生成相关样本路径。

Result: 基于copula的方法比自回归采样生成相关样本路径快几个数量级，且减轻滚雪球误差现象，提高样本路径质量。

Conclusion: 基于copula的方法能高效且高质量地从现有多步时间序列基础模型生成样本路径。

Abstract: Many time series applications require access to multi-step forecast
trajectories in the form of sample paths. Recently, time series foundation
models have leveraged multi-step lookahead predictions to improve the quality
and efficiency of multi-step forecasts. However, these models only predict
independent marginal distributions for each time step, rather than a full joint
predictive distribution. To generate forecast sample paths with realistic
correlation structures, one typically resorts to autoregressive sampling, which
can be extremely expensive. In this paper, we present a copula-based approach
to efficiently generate accurate, correlated sample paths from existing
multi-step time series foundation models in one forward pass. Our copula-based
approach generates correlated sample paths orders of magnitude faster than
autoregressive sampling, and it yields improved sample path quality by
mitigating the snowballing error phenomenon.

</details>


### [126] [LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning](https://arxiv.org/abs/2510.01459)
*Weizhe Chen,Sven Koenig,Bistra Dilkina*

Main category: cs.LG

TL;DR: 本文受大语言模型过度思考研究启发，提出新的元RLVR算法LSPO，评估显示其提升学习效果，并开展消融研究提供未来方向。


<details>
  <summary>Details</summary>
Motivation: 受大语言模型过度思考研究的启发，改进现有的基于可验证奖励的强化学习（RLVR）训练大语言模型的方法。

Method: 提出基于平均响应长度在每一步动态选择训练数据的元RLVR算法Length - aware Sampling for Policy Optimization (LSPO)。

Result: 在多个基础模型和数据集上评估表明LSPO持续提高学习有效性；进行详细消融研究，探讨将长度信号纳入动态采样的替代方法。

Conclusion: LSPO算法有效，消融研究为未来研究提供了有前景的方向。

Abstract: Since the release of Deepseek-R1, reinforcement learning with verifiable
rewards (RLVR) has become a central approach for training large language models
(LLMs) on reasoning tasks. Recent work has largely focused on modifying loss
functions to make RLVR more efficient and effective. In this paper, motivated
by studies of overthinking in LLMs, we propose Length-aware Sampling for Policy
Optimization (LSPO), a novel meta-RLVR algorithm that dynamically selects
training data at each step based on the average response length. We evaluate
LSPO across multiple base models and datasets, demonstrating that it
consistently improves learning effectiveness. In addition, we conduct a
detailed ablation study to examine alternative ways of incorporating length
signals into dynamic sampling, offering further insights and highlighting
promising directions for future research.

</details>


### [127] [Drop-Muon: Update Less, Converge Faster](https://arxiv.org/abs/2510.02239)
*Kaja Gruntkowska,Yassine Maziane,Zheng Qu,Peter Richtárik*

Main category: cs.LG

TL;DR: 文章挑战深度学习优化中每步更新全网络的传统做法，提出Drop - Muon方法，有收敛保证，实验显示其优于全网络更新。


<details>
  <summary>Details</summary>
Motivation: 挑战深度学习优化中每步更新全网络的传统假设，认为全网络更新并非最优。

Method: 引入非欧几里得随机渐进训练方法Drop - Muon，按随机时间表每步仅更新部分层。

Result: 为渐进训练在随机和非平滑状态下给出收敛保证，成本分析显示全网络更新并非在所有情况下最优，实验表明Drop - Muon比全网络Muon快1.4倍达到相同精度。

Conclusion: 提出对大规模模型训练方式的转变，为全网络更新提供高效且有理论依据的替代方案。

Abstract: Conventional wisdom in deep learning optimization dictates updating all
layers at every step-a principle followed by all recent state-of-the-art
optimizers such as Muon. In this work, we challenge this assumption, showing
that full-network updates can be fundamentally suboptimal, both in theory and
in practice. We introduce a non-Euclidean Randomized Progressive Training
method-Drop-Muon-a simple yet powerful framework that updates only a subset of
layers per step according to a randomized schedule, combining the efficiency of
progressive training with layer-specific non-Euclidean updates for top-tier
performance. We provide rigorous convergence guarantees under both layer-wise
smoothness and layer-wise $(L^0, L^1)$-smoothness, covering deterministic and
stochastic gradient settings, marking the first such results for progressive
training in the stochastic and non-smooth regime. Our cost analysis further
reveals that full-network updates are not optimal unless a very specific
relationship between layer smoothness constants holds. Through controlled CNN
experiments, we empirically demonstrate that Drop-Muon consistently outperforms
full-network Muon, achieving the same accuracy up to $1.4\times$ faster in
wall-clock time. Together, our results suggest a shift in how large-scale
models can be efficiently trained, challenging the status quo and offering a
highly efficient, theoretically grounded alternative to full-network updates.

</details>


### [128] [The Three Regimes of Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2510.01460)
*Lu Li,Tianwei Ni,Yihao Sun,Pierre-Luc Bacon*

Main category: cs.LG

TL;DR: 提出稳定性 - 可塑性原则解释离线到在线强化学习在线微调经验行为不一致问题，通过大规模实证研究验证，为设计选择提供框架。


<details>
  <summary>Details</summary>
Motivation: 离线到在线强化学习在线微调经验行为高度不一致，设计选择在不同场景效果差异大。

Method: 提出稳定性 - 可塑性原则，识别在线微调的三种状态，各需不同稳定性属性。

Result: 大规模实证研究表明，63 个案例中有 45 个结果与框架预测高度一致。

Conclusion: 该工作为离线到在线强化学习基于离线数据集和预训练策略的相对性能进行设计选择提供了原则性框架。

Abstract: Offline-to-online reinforcement learning (RL) has emerged as a practical
paradigm that leverages offline datasets for pretraining and online
interactions for fine-tuning. However, its empirical behavior is highly
inconsistent: design choices of online-fine tuning that work well in one
setting can fail completely in another. We propose a stability--plasticity
principle that can explain this inconsistency: we should preserve the knowledge
of pretrained policy or offline dataset during online fine-tuning, whichever is
better, while maintaining sufficient plasticity. This perspective identifies
three regimes of online fine-tuning, each requiring distinct stability
properties. We validate this framework through a large-scale empirical study,
finding that the results strongly align with its predictions in 45 of 63 cases.
This work provides a principled framework for guiding design choices in
offline-to-online RL based on the relative performance of the offline dataset
and the pretrained policy.

</details>


### [129] [Test-Time Anchoring for Discrete Diffusion Posterior Sampling](https://arxiv.org/abs/2510.02291)
*Litu Rout,Andreas Lugmayr,Yasamin Jafarian,Srivatsan Varadharajan,Constantine Caramanis,Sanjay Shakkottai,Ira Kemelmacher-Shlizerman*

Main category: cs.LG

TL;DR: 研究使用预训练离散扩散基础模型进行后验采样问题，提出APS方法，在标准基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散后验采样方法存在诸多挑战，希望在不重新训练特定任务模型的情况下从噪声测量中恢复图像。

Method: 引入用于掩码扩散基础模型的锚定后验采样（APS），包括离散嵌入空间的梯度式引导量化期望和自适应解码的锚定重掩码。

Result: 在标准基准测试的线性和非线性逆问题中，APS在离散扩散采样器中达到了最先进的性能。

Conclusion: APS方法有效，在免训练风格化和文本引导编辑中也展现出优势。

Abstract: We study the problem of posterior sampling using pretrained discrete
diffusion foundation models, aiming to recover images from noisy measurements
without retraining task-specific models. While diffusion models have achieved
remarkable success in generative modeling, most advances rely on continuous
Gaussian diffusion. In contrast, discrete diffusion offers a unified framework
for jointly modeling categorical data such as text and images. Beyond
unification, discrete diffusion provides faster inference, finer control, and
principled training-free Bayesian inference, making it particularly well-suited
for posterior sampling. However, existing approaches to discrete diffusion
posterior sampling face severe challenges: derivative-free guidance yields
sparse signals, continuous relaxations limit applicability, and split Gibbs
samplers suffer from the curse of dimensionality. To overcome these
limitations, we introduce Anchored Posterior Sampling (APS) for masked
diffusion foundation models, built on two key innovations -- quantized
expectation for gradient-like guidance in discrete embedding space, and
anchored remasking for adaptive decoding. Our approach achieves
state-of-the-art performance among discrete diffusion samplers across linear
and nonlinear inverse problems on the standard benchmarks. We further
demonstrate the benefits of our approach in training-free stylization and
text-guided editing.

</details>


### [130] [Fine-tuning LLMs with variational Bayesian last layer for high-dimensional Bayesian optimzation](https://arxiv.org/abs/2510.01471)
*Haotian Xiang,Jinwen Xu,Qin Lu*

Main category: cs.LG

TL;DR: 为解决高维不规则变量黑盒优化问题，采用大语言模型作为替代模型，提出LoRA - VBLL及ENS - LoRA - VBLL方法，实验显示其在高维基准和实际分子优化任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有高斯过程在处理高维不规则变量的黑盒优化问题时存在不足，需要新的替代模型。

Method: 采用大语言模型作为替代模型，利用低秩适应（LoRA）微调参数，结合变分贝叶斯最后一层（VBLL）框架，设计加权集成（ENS）方法自动选择超参数。

Result: 提出的（ENS - ）LoRA - VBLL方法在各种高维基准和实际分子优化任务中展现出出色性能。

Conclusion: 所提出的（ENS - ）LoRA - VBLL方法能有效解决高维不规则变量的黑盒优化问题。

Abstract: A plethora of applications entail solving black-box optimization problems
with high evaluation costs, including drug discovery, material design, as well
as hyperparameter tuning. Toward finding the global optimum of such black-box
optimization problems with sample efficiency, Bayesian optimization (BO) is a
theoretically elegant framework that relies on a probabilistic surrogate model
so as to iteratively select the query point with well-balanced
exploration-exploitation tradeoffs. The Gaussian process (GP), as the de-facto
choice for surrogate modeling, has achieved compelling performances for vanilla
BO with low-dimensional continuous variables. However, GPs fall short in coping
with high-dimensional counterparts with {\it irregular} variables (e.g.,
categorical, ordinal, etc.). To alleviate this, neural network-based surrogates
have been explored. Inspired by the powerful capabilities of LLMs, we adopt the
LLM as the surrogate to model the mapping from the high-dimensional input
variables to the objective function. To adapt to the current problem, we
leverage the low-rank adaptation (LoRA) to fine-tune the LLM parameters
together with the posterior of a linear regression head via the variational
Bayesian last layer (VBLL) framework. The resulting LoRA-VBLL is not only
computationally light compared to existing alternatives, but also admits
recursive updates. To automate the critical selection of the LoRA rank as well
as other hyperparameters, a weighted ensemble (ENS) of LoRA-VBLL surrogates has
been devised, which further accommodates continual update of the per-model
weight and individual LoRA-VBLL parameters via recursive Bayes. Extensive
experimental results demonstrate the compelling performance of the proposed
(ENS-)LoRA-VBLL approaches on various high-dimensional benchmarks and the
real-world molecular optimization tasks.

</details>


### [131] [Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive](https://arxiv.org/abs/2510.02305)
*Tyler Farghly,Peter Potaptchik,Samuel Howard,George Deligiannidis,Jakiw Pidstrigach*

Main category: cs.LG

TL;DR: 本文研究扩散模型泛化能力背后机制，从分数匹配角度为基于流形假设的猜想提供证据，理论和实证表明平滑分数函数可产生沿数据流形的平滑效果，且可通过选择平滑方式控制泛化流形。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽表现出色，但强大泛化能力的机制仅部分被理解，基于流形假设的猜想需验证。

Method: 从分数匹配角度研究学习问题的形成，研究经验分数匹配目标最小化器的平滑效果来检验隐式正则化的作用。

Result: 平滑分数函数（等价于在对数密度域平滑）会产生沿数据流形的平滑效果，可通过选择合适平滑方式控制扩散模型泛化的流形。

Conclusion: 为基于流形假设的关于扩散模型成功原因的猜想提供了证据。

Abstract: Diffusion models have achieved state-of-the-art performance, demonstrating
remarkable generalisation capabilities across diverse domains. However, the
mechanisms underpinning these strong capabilities remain only partially
understood. A leading conjecture, based on the manifold hypothesis, attributes
this success to their ability to adapt to low-dimensional geometric structure
within the data. This work provides evidence for this conjecture, focusing on
how such phenomena could result from the formulation of the learning problem
through score matching. We inspect the role of implicit regularisation by
investigating the effect of smoothing minimisers of the empirical score
matching objective. Our theoretical and empirical results confirm that
smoothing the score function -- or equivalently, smoothing in the log-density
domain -- produces smoothing tangential to the data manifold. In addition, we
show that the manifold along which the diffusion model generalises can be
controlled by choosing an appropriate smoothing.

</details>


### [132] [PEL-NAS: Search Space Partitioned Architecture Prompt Co-Evolutionary LLM-driven Hardware-Aware Neural Architecture Search](https://arxiv.org/abs/2510.01472)
*Hengyi Zhu,Grace Li Zhang,Shaoyi Huang*

Main category: cs.LG

TL;DR: 提出PEL - NAS解决LLM驱动的HW - NAS探索偏差问题，降低搜索成本，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 传统超网方法搜索成本高，LLM驱动方法存在探索偏差，无法在整个搜索空间不同延迟范围发现架构。

Method: 提出PEL - NAS，含复杂度驱动分区引擎、LLM架构提示协同进化算子、零成本预测器。

Result: 在HW - NAS - Bench上，比基线有更高HV、更低IGD，相同精度下延迟最多降54%，搜索成本从数天降至数分钟。

Conclusion: PEL - NAS能以降低的搜索成本生成高精度低延迟神经网络。

Abstract: Hardware-Aware Neural Architecture Search (HW-NAS) requires joint
optimization of accuracy and latency under device constraints. Traditional
supernet-based methods require multiple GPU days per dataset. Large Language
Model (LLM)-driven approaches avoid training a large supernet and can provide
quick feedback, but we observe an exploration bias: the LLM repeatedly proposes
neural network designs within limited search space and fails to discover
architectures across different latency ranges in the entire search space. To
address this issue, we propose PEL-NAS: a search space Partitioned,
architecture prompt co-Evolutionary and LLM-driven Neural Architecture Search
that can generate neural networks with high accuracy and low latency with
reduced search cost. Our proposed PEL-NAS has three key components: 1) a
complexity-driven partitioning engine that divides the search space by
complexity to enforce diversity and mitigate exploration bias; 2) an
LLM-powered architecture prompt co-evolution operator, in which the LLM first
updates a knowledge base of design heuristics based on results from the
previous round, then performs a guided evolution algorithm on architectures
with prompts that incorporate this knowledge base. Prompts and designs improve
together across rounds which avoids random guesswork and improve efficiency; 3)
a zero-cost predictor to avoid training a large number of candidates from
scratch. Experimental results show that on HW-NAS-Bench, PEL-NAS can achieve
overall higher HV, lower IGD, and up to 54% lower latency than baselines at
similar accuracy. Meanwhile, the search cost drops from days to minutes
compared with traditional supernet baselines.

</details>


### [133] [Density-Ratio Weighted Behavioral Cloning: Learning Control Policies from Corrupted Datasets](https://arxiv.org/abs/2510.01479)
*Shriram Karpoora Sundara Pandian,Ali Baheri*

Main category: cs.LG

TL;DR: 本文提出Density - Ratio Weighted Behavioral Cloning (Weighted BC)方法应对离线强化学习中数据集污染问题，理论证明收敛性，实验显示其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习数据集常受污染，导致标准行为克隆和离线强化学习方法策略性能下降。

Method: 引入Weighted BC方法，用小的验证干净参考集通过二元判别器估计轨迹级密度比，将其作为权重应用于行为克隆目标。

Result: 在连续控制基准上进行综合评估，实验表明Weighted BC在高污染率下仍保持接近最优性能，优于传统BC、BCQ和BRAC等基线方法。

Conclusion: Weighted BC能有效应对数据集污染问题，收敛到干净专家策略，且无需了解污染机制。

Abstract: Offline reinforcement learning (RL) enables policy optimization from fixed
datasets, making it suitable for safety-critical applications where online
exploration is infeasible. However, these datasets are often contaminated by
adversarial poisoning, system errors, or low-quality samples, leading to
degraded policy performance in standard behavioral cloning (BC) and offline RL
methods. This paper introduces Density-Ratio Weighted Behavioral Cloning
(Weighted BC), a robust imitation learning approach that uses a small, verified
clean reference set to estimate trajectory-level density ratios via a binary
discriminator. These ratios are clipped and used as weights in the BC objective
to prioritize clean expert behavior while down-weighting or discarding
corrupted data, without requiring knowledge of the contamination mechanism. We
establish theoretical guarantees showing convergence to the clean expert policy
with finite-sample bounds that are independent of the contamination rate. A
comprehensive evaluation framework is established, which incorporates various
poisoning protocols (reward, state, transition, and action) on continuous
control benchmarks. Experiments demonstrate that Weighted BC maintains
near-optimal performance even at high contamination ratios outperforming
baselines such as traditional BC, batch-constrained Q-learning (BCQ) and
behavior regularized actor-critic (BRAC).

</details>


### [134] [Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed](https://arxiv.org/abs/2510.01494)
*Isha Gupta,Rylan Schaeffer,Joshua Kazdan,Ken Liu,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 本文提出攻击可迁移性与攻击所在空间有关，数据空间攻击可迁移，模型表示空间攻击若无几何对齐则难迁移，并通过四种场景给出理论和实证证据。


<details>
  <summary>Details</summary>
Motivation: 解释图像越狱攻击在视觉 - 语言模型间难以迁移的原因。

Method: 在四种不同场景下提供理论和实证证据，包括数学证明、构建不同空间攻击等。

Result: 数据空间攻击可迁移，模型表示空间攻击若无几何对齐难迁移，当视觉 - 语言模型潜在几何在投影后空间充分对齐时，表示空间攻击可迁移。

Conclusion: 对抗迁移并非所有攻击的固有属性，而是取决于其操作域，这对构建更鲁棒模型有重要意义。

Abstract: The field of adversarial robustness has long established that adversarial
examples can successfully transfer between image classifiers and that text
jailbreaks can successfully transfer between language models (LMs). However, a
pair of recent studies reported being unable to successfully transfer image
jailbreaks between vision-language models (VLMs). To explain this striking
difference, we propose a fundamental distinction regarding the transferability
of attacks against machine learning models: attacks in the input data-space can
transfer, whereas attacks in model representation space do not, at least not
without geometric alignment of representations. We then provide theoretical and
empirical evidence of this hypothesis in four different settings. First, we
mathematically prove this distinction in a simple setting where two networks
compute the same input-output map but via different representations. Second, we
construct representation-space attacks against image classifiers that are as
successful as well-known data-space attacks, but fail to transfer. Third, we
construct representation-space attacks against LMs that successfully jailbreak
the attacked models but again fail to transfer. Fourth, we construct data-space
attacks against VLMs that successfully transfer to new VLMs, and we show that
representation space attacks \emph{can} transfer when VLMs' latent geometries
are sufficiently aligned in post-projector space. Our work reveals that
adversarial transfer is not an inherent property of all attacks but contingent
on their operational domain - the shared data-space versus models' unique
representation spaces - a critical insight for building more robust models.

</details>


### [135] [Realistic CDSS Drug Dosing with End-to-end Recurrent Q-learning for Dual Vasopressor Control](https://arxiv.org/abs/2510.01508)
*Will Y. Zou,Jean Feng,Alexandre Kalimouttou,Jennifer Yuntong Zhang,Christopher W. Seymour,Romain Pirracchio*

Main category: cs.LG

TL;DR: 提出端到端方法学习ICU感染性休克患者双升压药给药的最优策略，设计动作空间，在eICU和MIMIC上证明该方法能提升患者预后。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在临床决策支持系统中给药决策难操作，遭从业者质疑的问题。

Method: 采用结合离线保守Q学习和新的循环建模的端到端方法，设计能适应离散、连续和定向给药策略的动作空间。

Result: 设计的动作空间提高可解释性，便于临床应用且保证效果；动作空间设计影响学习到的行为策略；能使患者生存改善概率提升超15%，符合临床协议。

Conclusion: 提出的方法有效，能改善患者预后并与临床协议一致。

Abstract: Reinforcement learning (RL) applications in Clinical Decision Support Systems
(CDSS) frequently encounter skepticism from practitioners regarding inoperable
dosing decisions. We address this challenge with an end-to-end approach for
learning optimal drug dosing and control policies for dual vasopressor
administration in intensive care unit (ICU) patients with septic shock. For
realistic drug dosing, we apply action space design that accommodates discrete,
continuous, and directional dosing strategies in a system that combines offline
conservative Q-learning with a novel recurrent modeling in a replay buffer to
capture temporal dependencies in ICU time-series data. Our comparative analysis
of norepinephrine dosing strategies across different action space formulations
reveals that the designed action spaces improve interpretability and facilitate
clinical adoption while preserving efficacy. Empirical results1 on eICU and
MIMIC demonstrate that action space design profoundly influences learned
behavioral policies. The proposed methods achieve improved patient outcomes of
over 15% in survival improvement probability, while aligning with established
clinical protocols.

</details>


### [136] [Flock: A Knowledge Graph Foundation Model via Learning on Random Walks](https://arxiv.org/abs/2510.01510)
*Jinwoo Kim,Xingyue Huang,Krzysztof Olejniczak,Kyungbin Min,Michael Bronstein,Seunghoon Hong,İsmail İlkan Ceylan*

Main category: cs.LG

TL;DR: 研究知识图谱零样本链接预测问题，提出KGFM模型Flock，解决现有模型局限，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统确定性等变性对KGFMs表达能力有固有局限，无法区分结构相似但语义不同的关系。

Method: 引入概率节点 - 关系等变性，提出Flock模型，迭代采样随机游走，编码成序列，用序列模型嵌入，通过学习池化聚合节点和关系表示。

Result: Flock完美解决新诊断数据集Petals，在54个不同领域知识图谱的实体和关系预测任务上达到了最先进性能。

Conclusion: Flock尊重概率节点 - 关系等变性，是知识图谱上同构不变链接级函数的通用近似器。

Abstract: We study the problem of zero-shot link prediction on knowledge graphs (KGs),
which requires models to generalize over novel entities and novel relations.
Knowledge graph foundation models (KGFMs) address this task by enforcing
equivariance over both nodes and relations, learning from structural properties
of nodes and relations, which are then transferable to novel graphs with
similar structural properties. However, the conventional notion of
deterministic equivariance imposes inherent limits on the expressive power of
KGFMs, preventing them from distinguishing structurally similar but
semantically distinct relations. To overcome this limitation, we introduce
probabilistic node-relation equivariance, which preserves equivariance in
distribution while incorporating a principled randomization to break symmetries
during inference. Building on this principle, we present Flock, a KGFM that
iteratively samples random walks, encodes them into sequences via a recording
protocol, embeds them with a sequence model, and aggregates representations of
nodes and relations via learned pooling. Crucially, Flock respects
probabilistic node-relation equivariance and is a universal approximator for
isomorphism-invariant link-level functions over KGs. Empirically, Flock
perfectly solves our new diagnostic dataset Petals where current KGFMs fail,
and achieves state-of-the-art performances on entity- and relation prediction
tasks on 54 KGs from diverse domains.

</details>


### [137] [Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties](https://arxiv.org/abs/2510.01520)
*Hossein Sholehrasa,Xuan Xu,Doina Caragea,Jim E. Riviere,Majid Jaberi-Douraki*

Main category: cs.LG

TL;DR: 本文基于美国FDA数据构建预测框架，评估多种模型预测兽药使用结果（死亡或康复），集成方法和CatBoost表现最佳，结合AUM伪标签提升少数类检测，SHAP解释特征与致命结果关联，该框架支持风险评估和决策。


<details>
  <summary>Details</summary>
Motivation: 保障食用动物用药安全，保护动物福利和人类食品安全，通过预测不良事件结果降低食物链中违规残留风险。

Method: 使用美国FDA数据，进行预处理、特征整合，评估多种监督模型，处理类别不平衡问题，采用集成方法和AUM伪标签，用SHAP解释模型。

Result: 集成方法和CatBoost表现最佳，指标达0.95；AUM伪标签提升少数类检测；SHAP确定与致命结果相关特征。

Conclusion: 结合数据工程、机器学习和可解释AI能准确、可解释地预测兽药安全结果，支持风险评估和决策。

Abstract: The safe use of pharmaceuticals in food-producing animals is vital to protect
animal welfare and human food safety. Adverse events (AEs) may signal
unexpected pharmacokinetic or toxicokinetic effects, increasing the risk of
violative residues in the food chain. This study introduces a predictive
framework for classifying outcomes (Death vs. Recovery) using ~1.28 million
reports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary
Medicine. A preprocessing pipeline merged relational tables and standardized
AEs through VeDDRA ontologies. Data were normalized, missing values imputed,
and high-cardinality features reduced; physicochemical drug properties were
integrated to capture chemical-residue links. We evaluated supervised models,
including Random Forest, CatBoost, XGBoost, ExcelFormer, and large language
models (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as
undersampling and oversampling, with a focus on prioritizing recall for fatal
outcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best,
achieving precision, recall, and F1-scores of 0.95. Incorporating Average
Uncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved
minority-class detection, particularly in ExcelFormer and XGBoost.
Interpretability via SHAP identified biologically plausible predictors,
including lung, heart, and bronchial disorders, animal demographics, and drug
physicochemical properties. These features were strongly linked to fatal
outcomes. Overall, the framework shows that combining rigorous data
engineering, advanced machine learning, and explainable AI enables accurate,
interpretable predictions of veterinary safety outcomes. The approach supports
FARAD's mission by enabling early detection of high-risk drug-event profiles,
strengthening residue risk assessment, and informing regulatory and clinical
decision-making.

</details>


### [138] [CarbonX: An Open-Source Tool for Computational Decarbonization Using Time Series Foundation Models](https://arxiv.org/abs/2510.01521)
*Diptyaroop Maji,Kang Yang,Prashant Shenoy,Ramesh K Sitaraman,Mani Srivastava*

Main category: cs.LG

TL;DR: 本文介绍开源工具CarbonX，利用时间序列基础模型进行脱碳任务，在不同电网表现良好，适用于全球脱碳。


<details>
  <summary>Details</summary>
Motivation: 现有计算脱碳工具在提供准确、细粒度碳强度预测方面存在依赖特定电网数据、难全球覆盖、无不确定性估计等局限。

Method: 提出开源工具CarbonX，利用时间序列基础模型进行碳强度预测和插补等脱碳任务。

Result: 在214个全球电网零样本预测MAPE达15.82%；13个基准电网平均MAPE为9.59%，尾部预测MAPE为16.54%，提供95%覆盖的预测区间；可进行21天预测且精度下降小；完全微调后插补任务优于统计基线1.2 - 3.9倍。

Conclusion: CarbonX可在数据有限的任何电网轻松使用并表现出色，是全球脱碳实用工具。

Abstract: Computational decarbonization aims to reduce carbon emissions in computing
and societal systems such as data centers, transportation, and built
environments. This requires accurate, fine-grained carbon intensity forecasts,
yet existing tools have several key limitations: (i) they require grid-specific
electricity mix data, restricting use where such information is unavailable;
(ii) they depend on separate grid-specific models that make it challenging to
provide global coverage; and (iii) they provide forecasts without uncertainty
estimates, limiting reliability for downstream carbon-aware applications.
  In this paper, we present CarbonX, an open-source tool that leverages Time
Series Foundation Models (TSFMs) for a range of decarbonization tasks. CarbonX
utilizes the versatility of TSFMs to provide strong performance across multiple
tasks, such as carbon intensity forecasting and imputation, and across diverse
grids. Using only historical carbon intensity data and a single general model,
our tool achieves a zero-shot forecasting Mean Absolute Percentage Error (MAPE)
of 15.82% across 214 grids worldwide. Across 13 benchmark grids, CarbonX
performance is comparable with the current state-of-the-art, with an average
MAPE of 9.59% and tail forecasting MAPE of 16.54%, while also providing
prediction intervals with 95% coverage. CarbonX can provide forecasts for up to
21 days with minimal accuracy degradation. Further, when fully fine-tuned,
CarbonX outperforms the statistical baselines by 1.2--3.9X on the imputation
task. Overall, these results demonstrate that CarbonX can be used easily on any
grid with limited data and still deliver strong performance, making it a
practical tool for global-scale decarbonization.

</details>


### [139] [On Integer Programming for the Binarized Neural Network Verification Problem](https://arxiv.org/abs/2510.01525)
*Woojin Kim,James R. Luedtke*

Main category: cs.LG

TL;DR: 本文提出两种技术改进二值神经网络（BNN）验证问题的整数规划（IP）公式，能在有限时间内比现有IP方法验证更高输入扰动范围的BNN。


<details>
  <summary>Details</summary>
Motivation: BNN验证问题的自然IP公式因big - M约束导致的大整数间隙而难以求解，需改进IP公式。

Method: 一是为多类设置引入获取线性目标的新方法；二是引入利用BNN递归结构生成有效不等式的新技术。

Result: 所提技术能在有限时间内比现有IP方法对更高范围的输入扰动验证BNN。

Conclusion: 提出的两种技术有效改进了BNN验证问题的IP公式，提升了验证能力。

Abstract: Binarized neural networks (BNNs) are feedforward neural networks with binary
weights and activation functions. In the context of using a BNN for
classification, the verification problem seeks to determine whether a small
perturbation of a given input can lead it to be misclassified by the BNN, and
the robustness of the BNN can be measured by solving the verification problem
over multiple inputs. The BNN verification problem can be formulated as an
integer programming (IP) problem. However, the natural IP formulation is often
challenging to solve due to a large integrality gap induced by big-$M$
constraints. We present two techniques to improve the IP formulation. First, we
introduce a new method for obtaining a linear objective for the multi-class
setting. Second, we introduce a new technique for generating valid inequalities
for the IP formulation that exploits the recursive structure of BNNs. We find
that our techniques enable verifying BNNs against a higher range of input
perturbation than existing IP approaches within a limited time.

</details>


### [140] [Round-trip Reinforcement Learning: Self-Consistent Training for Better Chemical LLMs](https://arxiv.org/abs/2510.01527)
*Lecheng Kong,Xiyuan Wang,Yixin Chen,Muhan Zhang*

Main category: cs.LG

TL;DR: 论文针对大语言模型在计算化学中缺乏往返一致性问题，提出Round - Trip Reinforcement Learning (RTRL)框架，实验表明其能提升性能和一致性，证明往返一致性可作为训练目标。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在计算化学中存在往返一致性问题，且往返一致性与主要任务表现强相关，需将一致性作为模型改进目标。

Method: 引入Round - Trip Reinforcement Learning (RTRL)框架，利用往返转换成功作为奖励信号训练模型；提出迭代变体，让正向和反向映射在自改进循环中相互训练。

Result: RTRL在监督、自监督和合成数据制度下，相比强基线显著提升了性能和一致性。

Conclusion: 往返一致性不仅是理想属性，也是可训练目标，为构建更强大可靠的基础模型提供新途径。

Abstract: Large Language Models (LLMs) are emerging as versatile foundation models for
computational chemistry, handling bidirectional tasks like reaction prediction
and retrosynthesis. However, these models often lack round-trip consistency.
For instance, a state-of-the-art chemical LLM may successfully caption a
molecule, yet be unable to accurately reconstruct the original structure from
its own generated text. This inconsistency suggests that models are learning
unidirectional memorization rather than flexible mastery. Indeed, recent work
has demonstrated a strong correlation between a model's round-trip consistency
and its performance on the primary tasks. This strong correlation reframes
consistency into a direct target for model improvement. We therefore introduce
Round-Trip Reinforcement Learning (RTRL), a novel framework that trains a model
to improve its consistency by using the success of a round-trip transformation
as a reward signal. We further propose an iterative variant where forward and
reverse mappings alternately train each other in a self-improvement loop, a
process that is highly data-efficient and notably effective with the massive
amount of unlabelled data common in chemistry. Experiments demonstrate that
RTRL significantly \textbf{boosts performance and consistency} over strong
baselines across supervised, self-supervised, and synthetic data regimes. This
work shows that round-trip consistency is not just a desirable property but a
trainable objective, offering a new path toward more robust and reliable
foundation models.

</details>


### [141] [Bypassing Prompt Guards in Production with Controlled-Release Prompting](https://arxiv.org/abs/2510.01529)
*Jaiden Fairoze,Sanjam Garg,Keewoo Lee,Mingyuan Wang*

Main category: cs.LG

TL;DR: 论文介绍了一种绕过提示防护机制的新攻击，揭示其局限性，强调防御应从阻止恶意输入转向防止恶意输出，并指出其他对齐问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，确保AI安全和对齐至关重要，提示防护机制虽流行但可能存在局限，需揭示其问题。

Method: 利用提示防护和主大语言模型之间的资源不对称性，编码提示防护无法解码但主模型能解码的越狱提示。

Result: 该攻击能持续突破生产模型，在谷歌Gemini等高度保护的聊天界面中保持响应质量。

Conclusion: 现代大语言模型架构中轻量级提示防护存在攻击面，防御应从阻止恶意输入转向防止恶意输出，同时还存在其他关键对齐问题。

Abstract: As large language models (LLMs) advance, ensuring AI safety and alignment is
paramount. One popular approach is prompt guards, lightweight mechanisms
designed to filter malicious queries while being easy to implement and update.
In this work, we introduce a new attack that circumvents such prompt guards,
highlighting their limitations. Our method consistently jailbreaks production
models while maintaining response quality, even under the highly protected chat
interfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok
(3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry
between the prompt guard and the main LLM, encoding a jailbreak prompt that
lightweight guards cannot decode but the main model can. This reveals an attack
surface inherent to lightweight prompt guards in modern LLM architectures and
underscores the need to shift defenses from blocking malicious inputs to
preventing malicious outputs. We additionally identify other critical alignment
issues, such as copyrighted data extraction, training data extraction, and
malicious response leakage during thinking.

</details>


### [142] [NVIDIA AI Aerial: AI-Native Wireless Communications](https://arxiv.org/abs/2510.01533)
*Kobi Cohen-Arazi,Michael Roe,Zhen Hu,Rohan Chavan,Anna Ptasznik,Joanna Lin,Joao Morais,Joseph Boccuzzi,Tommaso Balercia*

Main category: cs.LG

TL;DR: 本文提出将Python算法编译成GPU可运行代码的框架，用于6G网络中AI/ML模型集成，以实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 6G促使无线系统向AI原生转变，需将DSP和ML无缝集成到蜂窝网络软件栈，使网络生命周期接近AI系统。

Method: 提出编译Python算法为GPU可运行代码的框架，在NVIDIA AI Aerial平台实现。

Result: 实现统一方法，在NVIDIA GPU上确保效率、灵活性和高性能，以PUSCH接收器中通道估计功能为例进行验证。

Conclusion: 所提方法为AI/ML模型集成到下一代蜂窝系统奠定基础，对实现原生智能6G网络至关重要。

Abstract: 6G brings a paradigm shift towards AI-native wireless systems, necessitating
the seamless integration of digital signal processing (DSP) and machine
learning (ML) within the software stacks of cellular networks. This
transformation brings the life cycle of modern networks closer to AI systems,
where models and algorithms are iteratively trained, simulated, and deployed
across adjacent environments. In this work, we propose a robust framework that
compiles Python-based algorithms into GPU-runnable blobs. The result is a
unified approach that ensures efficiency, flexibility, and the highest possible
performance on NVIDIA GPUs. As an example of the capabilities of the framework,
we demonstrate the efficacy of performing the channel estimation function in
the PUSCH receiver through a convolutional neural network (CNN) trained in
Python. This is done in a digital twin first, and subsequently in a real-time
testbed. Our proposed methodology, realized in the NVIDIA AI Aerial platform,
lays the foundation for scalable integration of AI/ML models into
next-generation cellular systems, and is essential for realizing the vision of
natively intelligent 6G networks.

</details>


### [143] [TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis](https://arxiv.org/abs/2510.01538)
*Haokun Zhao,Xiang Zhang,Jiaqi Wei,Yiwei Xu,Yuting He,Siqi Sun,Chenyu You*

Main category: cs.LG

TL;DR: 本文介绍用于通用时间序列预测的LLM驱动框架TimeSeriesScientist (TSci)，含四个专门代理，实验表明其优于基线模型，且能生成透明报告。


<details>
  <summary>Details</summary>
Motivation: 实际中预测者处理大量短而嘈杂的时间序列时人力成本高，现有模型泛化能力差，需要通用、减少人工干预的框架。

Method: 引入TSci框架，包含Curator、Planner、Forecaster和Reporter四个专门代理，分别进行数据预处理、模型选择、拟合验证和生成报告。

Result: 在八个基准测试中，TSci始终优于统计和基于LLM的基线模型，分别平均降低10.4%和38.2%的预测误差，且生成清晰严谨的报告。

Conclusion: TSci将预测工作流程转变为可解释、可扩展的白盒系统。

Abstract: Time series forecasting is central to decision-making in domains as diverse
as energy, finance, climate, and public health. In practice, forecasters face
thousands of short, noisy series that vary in frequency, quality, and horizon,
where the dominant cost lies not in model fitting, but in the labor-intensive
preprocessing, validation, and ensembling required to obtain reliable
predictions. Prevailing statistical and deep learning models are tailored to
specific datasets or domains and generalize poorly. A general, domain-agnostic
framework that minimizes human intervention is urgently in demand. In this
paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic
framework for general time series forecasting. The framework comprises four
specialized agents: Curator performs LLM-guided diagnostics augmented by
external tools that reason over data statistics to choose targeted
preprocessing; Planner narrows the hypothesis space of model choice by
leveraging multi-modal diagnostics and self-planning over the input; Forecaster
performs model fitting and validation and, based on the results, adaptively
selects the best model configuration as well as ensemble strategy to make final
predictions; and Reporter synthesizes the whole process into a comprehensive,
transparent report. With transparent natural-language rationales and
comprehensive reports, TSci transforms the forecasting workflow into a
white-box system that is both interpretable and extensible across tasks.
Empirical results on eight established benchmarks demonstrate that TSci
consistently outperforms both statistical and LLM-based baselines, reducing
forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci
produces a clear and rigorous report that makes the forecasting workflow more
transparent and interpretable.

</details>


### [144] [Executable Counterfactuals: Improving LLMs' Causal Reasoning Through Code](https://arxiv.org/abs/2510.01539)
*Aniket Vashishtha,Qirun Dai,Hongyuan Mei,Amit Sharma,Chenhao Tan,Hao Peng*

Main category: cs.LG

TL;DR: 本文介绍可执行反事实框架评估和提升大语言模型反事实推理能力，发现SOTA模型推理精度下降，对比不同训练方法，凸显强化学习优势。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM反事实推理能力的工作跳过溯因步骤，高估了模型表现，需改进评估和提升方法。

Method: 引入可执行反事实框架，通过代码和数学问题进行因果推理，创建不同难度的合成数据；构建训练集，对比监督微调与强化学习。

Result: SOTA模型从干预推理到反事实推理精度下降25 - 40%；监督微调提升Qwen模型的领域内性能，但降低了域外任务精度；强化学习在代码和数学问题上优于基础模型。

Conclusion: 强化学习能诱导核心认知行为并泛化到新领域，有望提升LLMs的反事实推理能力。

Abstract: Counterfactual reasoning, a hallmark of intelligence, consists of three
steps: inferring latent variables from observations (abduction), constructing
alternatives (interventions), and predicting their outcomes (prediction). This
skill is essential for advancing LLMs' causal understanding and expanding their
applications in high-stakes domains such as scientific research. However,
existing efforts in assessing LLM's counterfactual reasoning capabilities tend
to skip the abduction step, effectively reducing to interventional reasoning
and leading to overestimation of LLM performance. To address this, we introduce
executable counterfactuals, a novel framework that operationalizes causal
reasoning through code and math problems. Our framework explicitly requires all
three steps of counterfactual reasoning and enables scalable synthetic data
creation with varying difficulty, creating a frontier for evaluating and
improving LLM's reasoning. Our results reveal substantial drop in accuracy
(25-40%) from interventional to counterfactual reasoning for SOTA models like
o4-mini and Claude-4-Sonnet. To address this gap, we construct a training set
comprising counterfactual code problems having if-else condition and test on
out-of-domain code structures (e.g. having while-loop); we also test whether a
model trained on code would generalize to counterfactual math word problems.
While supervised finetuning on stronger models' reasoning traces improves
in-domain performance of Qwen models, it leads to a decrease in accuracy on OOD
tasks such as counterfactual math problems. In contrast, reinforcement learning
induces the core cognitive behaviors and generalizes to new domains, yielding
gains over the base model on both code (improvement of 1.5x-2x) and math
problems. Analysis of the reasoning traces reinforces these findings and
highlights the promise of RL for improving LLMs' counterfactual reasoning.

</details>


### [145] [Predictive Preference Learning from Human Interventions](https://arxiv.org/abs/2510.01545)
*Haoyuan Cai,Zhenghao Peng,Bolei Zhou*

Main category: cs.LG

TL;DR: 提出预测性偏好学习方法PPL，利用人类干预中的偏好信号预测未来状态，在自动驾驶和机器人操作基准测试中证明其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有交互式模仿学习方法大多仅纠正当前状态下的代理动作，无法调整未来状态动作，可能带来更大危险。

Method: 引入PPL方法，将每次人类干预引导到L个未来时间步，在这些未来状态上进行偏好优化。

Result: 在自动驾驶和机器人操作基准测试中证明了方法的效率和通用性，理论分析表明选择合适的偏好视野L可平衡风险状态覆盖和标签正确性。

Conclusion: PPL方法能将专家纠正传播到代理预期探索的安全关键区域，显著提高学习效率，减少所需的人类示范。

Abstract: Learning from human involvement aims to incorporate the human subject to
monitor and correct agent behavior errors. Although most interactive imitation
learning methods focus on correcting the agent's action at the current state,
they do not adjust its actions in future states, which may be potentially more
hazardous. To address this, we introduce Predictive Preference Learning from
Human Interventions (PPL), which leverages the implicit preference signals
contained in human interventions to inform predictions of future rollouts. The
key idea of PPL is to bootstrap each human intervention into L future time
steps, called the preference horizon, with the assumption that the agent
follows the same action and the human makes the same intervention in the
preference horizon. By applying preference optimization on these future states,
expert corrections are propagated into the safety-critical regions where the
agent is expected to explore, significantly improving learning efficiency and
reducing human demonstrations needed. We evaluate our approach with experiments
on both autonomous driving and robotic manipulation benchmarks and demonstrate
its efficiency and generality. Our theoretical analysis further shows that
selecting an appropriate preference horizon L balances coverage of risky states
with label correctness, thereby bounding the algorithmic optimality gap. Demo
and code are available at: https://metadriverse.github.io/ppl

</details>


### [146] [MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models](https://arxiv.org/abs/2510.01549)
*Kevin Zhai,Utsav Singh,Anirudh Thatipelli,Souradip Chakraborty,Anit Kumar Sahu,Furong Huang,Amrit Singh Bedi,Mubarak Shah*

Main category: cs.LG

TL;DR: 扩散模型在文本条件图像生成上表现出色，但结果常不满足用户基于标量奖励的特定标准，传统微调计算量大，推理时噪声优化方法存在奖励作弊问题。提出无训练的推理时对齐方法MIRA及MIRA - DPO，能有效解决问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成图像难以满足用户特定标准，现有推理时对齐方法存在奖励作弊问题，需改进。

Method: 提出MIRA方法，引入图像空间基于分数的KL替代项来规范采样轨迹；提出MIRA - DPO将偏好优化映射到推理时间。

Result: 在多个模型、奖励指标和公共数据集上，MIRA较基线有超60%胜率，保持对提示的遵循；机制图显示奖励提升且漂移接近零，而DNO随计算量增加漂移增大。

Conclusion: MIRA和MIRA - DPO能有效缓解奖励作弊问题，可在不微调情况下扩展到不可微奖励。

Abstract: Diffusion models excel at generating images conditioned on text prompts, but
the resulting images often do not satisfy user-specific criteria measured by
scalar rewards such as Aesthetic Scores. This alignment typically requires
fine-tuning, which is computationally demanding. Recently, inference-time
alignment via noise optimization has emerged as an efficient alternative,
modifying initial input noise to steer the diffusion denoising process towards
generating high-reward images. However, this approach suffers from reward
hacking, where the model produces images that score highly, yet deviate
significantly from the original prompt. We show that noise-space regularization
is insufficient and that preventing reward hacking requires an explicit
image-space constraint. To this end, we propose MIRA (MItigating Reward
hAcking), a training-free, inference-time alignment method. MIRA introduces an
image-space, score-based KL surrogate that regularizes the sampling trajectory
with a frozen backbone, constraining the output distribution so reward can
increase without off-distribution drift (reward hacking). We derive a tractable
approximation to KL using diffusion scores. Across SDv1.5 and SDXL, multiple
rewards (Aesthetic, HPSv2, PickScore), and public datasets (e.g.,
Animal-Animal, HPDv2), MIRA achieves >60\% win rate vs. strong baselines while
preserving prompt adherence; mechanism plots show reward gains with near-zero
drift, whereas DNO drifts as compute increases. We further introduce MIRA-DPO,
mapping preference optimization to inference time with a frozen backbone,
extending MIRA to non-differentiable rewards without fine-tuning.

</details>


### [147] [Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization](https://arxiv.org/abs/2510.01555)
*Kezhao Liu,Jason Klein Liu,Mingtao Chen,Yiming Liu*

Main category: cs.LG

TL;DR: 本文建立统一框架分析RLHF中KL散度损失的不同实现方式，证明不同实现的等价性与近似性，并提出纠偏方法。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF中KL散度损失实现忽略其作为优化损失的功能作用，需深入分析。

Method: 建立统一框架连接两种不同实现风格，分析等价性与差异性。

Result: 证明'$k_1 in reward'是RKL正则化的原则性损失，'$k_2 as loss'与'$k_1 in reward'梯度等价，'$k_3 as loss'是近似，常见off - policy方法有偏差。

Conclusion: 研究为选择和正确实现KL正则化提供基于梯度的理论依据，有助于构建更强大有效的RLHF系统。

Abstract: Reinforcement Learning from Human Feedback (RLHF) leverages a
Kullback-Leibler (KL) divergence loss to stabilize training and prevent
overfitting. However, in methods such as GRPO, its implementation may be guided
by principles from numerical value estimation-a practice that overlooks the
term's functional role as an optimization loss. To analyze this issue, we
establish a unified framework that connects two seemingly distinct
implementation styles: using the mathematical term $k_n$ as a detached
coefficient for the policy's score function ('$k_n$ in reward') or as a direct
loss function through which gradients are propagated ('$k_n$ as loss'). We show
that the latter can always be analyzed via an equivalent gradient coefficient
in the former, unifying the two perspectives. Through this framework, we prove
that the conventional '$k_1$ in reward' (like in PPO) is the principled loss
for Reverse KL (RKL) regularization. We further establish a key finding: under
on-policy conditions, the '$k_2$ as loss' formulation is, in fact,
gradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our
work, identifies both as the theoretically sound implementations of the RKL
objective. In contrast, we show that the recently adopted '$k_3$ as loss' (like
in GRPO) is merely a first-order, biased approximation of the principled loss.
Furthermore, we argue that common off-policy implementations of '$k_n$ as loss'
methods are biased due to neglected importance sampling, and we propose a
principled correction. Our findings provide a comprehensive, gradient-based
rationale for choosing and correctly implementing KL regularization, paving the
way for more robust and effective RLHF systems.

</details>


### [148] [Large-Scale Bayesian Causal Discovery with Interventional Data](https://arxiv.org/abs/2510.01562)
*Seong Woo Han,Daniel Duy Vo,Brielin C. Brown*

Main category: cs.LG

TL;DR: 提出IBCD框架用于干预数据因果发现，模拟和实际数据应用显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于干预数据的因果发现方法在大规模任务中性能差且无法量化不确定性。

Method: 提出IBCD框架，建模总因果效应矩阵似然，用矩阵正态分布近似，设尖板马蹄形先验，从观测数据学习结构权重，做不确定性推理。

Result: 模拟显示IBCD比基线方法结构恢复能力更优；应用于CRISPR扰动数据，边缘后验包含概率可识别稳健图结构。

Conclusion: IBCD是一种有效的因果发现方法。

Abstract: Inferring the causal relationships among a set of variables in the form of a
directed acyclic graph (DAG) is an important but notoriously challenging
problem. Recently, advancements in high-throughput genomic perturbation screens
have inspired development of methods that leverage interventional data to
improve model identification. However, existing methods still suffer poor
performance on large-scale tasks and fail to quantify uncertainty. Here, we
propose Interventional Bayesian Causal Discovery (IBCD), an empirical Bayesian
framework for causal discovery with interventional data. Our approach models
the likelihood of the matrix of total causal effects, which can be approximated
by a matrix normal distribution, rather than the full data matrix. We place a
spike-and-slab horseshoe prior on the edges and separately learn data-driven
weights for scale-free and Erd\H{o}s-R\'enyi structures from observational
data, treating each edge as a latent variable to enable uncertainty-aware
inference. Through extensive simulation, we show that IBCD achieves superior
structure recovery compared to existing baselines. We apply IBCD to CRISPR
perturbation (Perturb-seq) data on 521 genes, demonstrating that edge posterior
inclusion probabilities enable identification of robust graph structures.

</details>


### [149] [From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?](https://arxiv.org/abs/2510.01571)
*Hanqun Cao,Hongrui Zhang,Junde Xu,Zhou Zhang,Lingdong Shen,Minghao Sun,Ge Liu,Jinbo Xu,Wu-Jun Li,Jinren Ni,Cesar de la Fuente-Nunez,Tianfan Fu,Yejin Choi,Pheng-Ann Heng,Fang Wu*

Main category: cs.LG

TL;DR: 研究将强化学习与蛋白质语言模型结合用于四个领域，发现强化学习能提升成功率和采样效率，给出蛋白质设计中强化学习的实用指导。


<details>
  <summary>Details</summary>
Motivation: 探究强化学习能否推动蛋白质语言模型突破预训练先验，揭示潜在的序列 - 结构 - 功能规则。

Method: 在四个领域将强化学习与蛋白质语言模型配对，使用多种强化学习算法和模型类。

Result: 强化学习在各基准测试中持续提升成功率和采样效率，性能受任务空间、奖励保真度和策略容量三因素交互影响。

Conclusion: 在蛋白质设计中应优先进行奖励建模和校准，使算法和正则化强度与任务难度匹配，在边际收益最大处分配容量。

Abstract: Protein language models (PLMs) have advanced computational protein science
through large-scale pretraining and scalable architectures. In parallel,
reinforcement learning (RL) has broadened exploration and enabled precise
multi-objective optimization in protein design. Yet whether RL can push PLMs
beyond their pretraining priors to uncover latent sequence-structure-function
rules remains unclear. We address this by pairing RL with PLMs across four
domains: antimicrobial peptide design, kinase variant optimization, antibody
engineering, and inverse folding. Using diverse RL algorithms and model
classes, we ask if RL improves sampling efficiency and, more importantly, if it
reveals capabilities not captured by supervised learning. Across benchmarks, RL
consistently boosts success rates and sample efficiency. Performance follows a
three-factor interaction: task headroom, reward fidelity, and policy capacity
jointly determine gains. When rewards are accurate and informative, policies
have sufficient capacity, and tasks leave room beyond supervised baselines,
improvements scale; when rewards are noisy or capacity is constrained, gains
saturate despite exploration. This view yields practical guidance for RL in
protein design: prioritize reward modeling and calibration before scaling
policy size, match algorithm and regularization strength to task difficulty,
and allocate capacity where marginal gains are largest. Implementation is
available at https://github.com/chq1155/RL-PLM.

</details>


### [150] [Gradient Shaping Beyond Clipping: A Functional Perspective on Update Magnitude Control](https://arxiv.org/abs/2510.01578)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: 提出SPAMP框架，将梯度裁剪推广为平滑的逐层梯度塑形，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有梯度裁剪采用固定阈值，缺乏灵活性且忽略梯度分布动态。

Method: 提出SPAMP框架，跟踪局部梯度统计，动态估计阈值，以可微方式应用基于幂的变换来调整更新幅度。

Result: 在图像和语言任务的大量实验中，SPAMP在稳定性、收敛性和鲁棒性上优于现有方法。

Conclusion: SPAMP为控制有效更新规模提供了一种有原则的替代方案，优于刚性启发式方法。

Abstract: Gradient clipping is widely used to stabilize deep network training, but its
formulation as a hard, fixed threshold limits flexibility and ignores gradient
distribution dynamics. We propose SPAMP (Statistical Per-layer Adaptive
Modulation and Projection), a unified framework that generalizes clipping into
smooth, per-layer gradient shaping. SPAMP tracks local gradient statistics,
dynamically estimates thresholds, and applies power-based transformations to
modulate update magnitudes in a differentiable manner. This perspective recasts
clipping and warmup as dual mechanisms for controlling the effective update
scale $\eta_t \|g_t\|$, offering a principled alternative to rigid heuristics.
Extensive experiments across image and language tasks demonstrate that SPAMP
improves stability, convergence, and robustness over existing methods.

</details>


### [151] [Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression](https://arxiv.org/abs/2510.01581)
*Joykirat Singh,Justin Chih-Yao Chen,Archiki Prasad,Elias Stengel-Eskin,Akshay Nambi,Mohit Bansal*

Main category: cs.LG

TL;DR: 提出TRAAC方法解决模型推理长度适应问题，在多任务中提升准确率、减少推理步骤且有强泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有思维模型在推理时存在推理长度无法适配任务难度的问题，即欠适应性，需平衡思考不足和过度思考。

Method: 提出TRAAC，一种在线训练后的强化学习方法，利用模型自注意力识别重要步骤、修剪冗余步骤，估计难度并融入训练奖励。

Result: 相比基础模型和其他强化学习基线，TRAAC提高了准确率、减少推理步骤，实现自适应思考；在多任务中平均绝对准确率提升8.4%，推理长度相对减少36.8%。

Conclusion: TRAAC能根据难度对思考预算进行细粒度调整，任务难度校准和基于注意力的压缩相结合可在不同任务中取得增益。

Abstract: Recent thinking models solve complex reasoning tasks by scaling test-time
compute, but this scaling must be allocated in line with task difficulty. On
one hand, short reasoning (underthinking) leads to errors on harder problems
that require extended reasoning steps; but, excessively long reasoning
(overthinking) can be token-inefficient, generating unnecessary steps even
after reaching a correct intermediate solution. We refer to this as
under-adaptivity, where the model fails to modulate its response length
appropriately given problems of varying difficulty. To address under-adaptivity
and strike a balance between under- and overthinking, we propose TRAAC (Think
Right with Adaptive, Attentive Compression), an online post-training RL method
that leverages the model's self-attention over a long reasoning trajectory to
identify important steps and prune redundant ones. TRAAC also estimates
difficulty and incorporates it into training rewards, thereby learning to
allocate reasoning budget commensurate with example difficulty. Our approach
improves accuracy, reduces reasoning steps, and enables adaptive thinking
compared to base models and other RL baselines. Across a variety of tasks
(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute
accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%
compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length
drop compared to the best RL baseline. TRAAC also shows strong generalization:
although our models are trained on math datasets, they show accuracy and
efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,
and OptimalThinkingBench. Our analysis further verifies that TRAAC provides
fine-grained adjustments to thinking budget based on difficulty and that a
combination of task-difficulty calibration and attention-based compression
yields gains across diverse tasks.

</details>


### [152] [Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation](https://arxiv.org/abs/2510.01588)
*Ziming Tang,Chengbin Hou,Tianyu Zhang,Bangxu Tian,Jinbao Wang,Hairong Lv*

Main category: cs.LG

TL;DR: 提出NoRo框架解决帕金森病远程监测中UPDRS评分预测的噪声问题，实验证明其能增强预测的抗噪性。


<details>
  <summary>Details</summary>
Motivation: 帕金森病远程监测测量中存在患者测量误差、环境噪声和数据丢包三种噪声，导致预测误差较高。

Method: 将原始语音特征分组构建对比对，训练多层感知机编码器生成抗噪特征，与原始特征拼接作为增强特征输入预测模型，还引入可定制噪声注入模块的评估方法。

Result: 广泛实验表明NoRo能在不同噪声环境下，增强各种下游预测模型的UPDRS预测抗噪性。

Conclusion: NoRo框架可有效解决帕金森病远程监测中UPDRS评分预测的噪声问题，提升预测的抗噪能力。

Abstract: Parkinson's disease (PD) is one of the most common neurodegenerative
disorder. PD telemonitoring emerges as a novel assessment modality enabling
self-administered at-home tests of Unified Parkinson's Disease Rating Scale
(UPDRS) scores, enhancing accessibility for PD patients. However, three types
of noise would occur during measurements: (1) patient-induced measurement
inaccuracies, (2) environmental noise, and (3) data packet loss during
transmission, resulting in higher prediction errors. To address these
challenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First,
the original speech features are grouped into ordered bins, based on the
continuous values of a selected feature, to construct contrastive pairs.
Second, the contrastive pairs are employed to train a multilayer perceptron
encoder for generating noise-robust features. Finally, these features are
concatenated with the original features as the augmented features, which are
then fed into the UPDRS prediction models. Notably, we further introduces a
novel evaluation approach with customizable noise injection module, and
extensive experiments show that NoRo can successfully enhance the noise
robustness of UPDRS prediction across various downstream prediction models
under different noisy environments.

</details>


### [153] [Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness](https://arxiv.org/abs/2510.01598)
*Youwei Bao,Shuhan Yang,Hyunsoo Yang*

Main category: cs.LG

TL;DR: 本文提出用STT - MTJ生成硬件真随机数解决生成式AI模型中PRNG的安全问题，系统高效且有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型中确定性伪随机数生成器有可预测模式，易被攻击，传统防御有高能耗和高延迟问题。

Method: 嵌入基于自旋转移矩磁隧道结（STT - MTJs）生成的硬件真随机数，构建高度并行、FPGA辅助的原型计算系统。

Result: 系统每秒可产生兆比特真随机数，通过NIST随机性测试，开销小；集成到GAN可将不安全输出降低达18.6倍；有纳秒级开关速度、高能效和可扩展性，有望实现每秒千兆比特吞吐量。

Conclusion: 基于STT - MTJ的系统可作为下一代生成式AI系统的实用安全组件。

Abstract: Deterministic pseudo random number generators (PRNGs) used in generative
artificial intelligence (GAI) models produce predictable patterns vulnerable to
exploitation by attackers. Conventional defences against the vulnerabilities
often come with significant energy and latency overhead. Here, we embed
hardware-generated true random bits from spin-transfer torque magnetic tunnel
junctions (STT-MTJs) to address the challenges. A highly parallel,
FPGA-assisted prototype computing system delivers megabit-per-second true
random numbers, passing NIST randomness tests after in-situ operations with
minimal overhead. Integrating the hardware random bits into a generative
adversarial network (GAN) trained on CIFAR-10 reduces insecure outputs by up to
18.6 times compared to the low-quality random number generators (RNG) baseline.
With nanosecond switching speed, high energy efficiency, and established
scalability, our STT-MTJ-based system holds the potential to scale beyond 106
parallel cells, achieving gigabit-per-second throughput suitable for large
language model sampling. This advancement highlights spintronic RNGs as
practical security components for next-generation GAI systems.

</details>


### [154] [Posterior Collapse as a Phase Transition in Variational Autoencoders](https://arxiv.org/abs/2510.01621)
*Zhen Li,Fan Zhang,Zheng Zhang,Yu Chen*

Main category: cs.LG

TL;DR: 从统计物理角度研究变分自编码器（VAEs）后验崩溃现象，揭示其为相变，确定临界超参数阈值并验证，表明后验崩溃是一种相变，为深度生成模型提供新见解。


<details>
  <summary>Details</summary>
Motivation: 研究变分自编码器中后验崩溃现象的本质。

Method: 从统计物理角度分析，通过分析与后验崩溃相关的平凡解的稳定性确定临界超参数阈值，在合成和真实数据集上验证。

Result: 确定了临界超参数阈值，验证了相变的存在。

Conclusion: 后验崩溃是数据结构和变分约束相互作用产生的相变，为深度生成模型的可训练性和表示能力提供新见解。

Abstract: We investigate the phenomenon of posterior collapse in variational
autoencoders (VAEs) from the perspective of statistical physics, and reveal
that it constitutes a phase transition governed jointly by data structure and
model hyper-parameters. By analyzing the stability of the trivial solution
associated with posterior collapse, we identify a critical hyper-parameter
threshold. This critical boundary, separating meaningful latent inference from
collapse, is characterized by a discontinuity in the KL divergence between the
approximate posterior and the prior distribution. We validate this critical
behavior on both synthetic and real-world datasets, confirming the existence of
a phase transition. Our results demonstrate that posterior collapse is not
merely an optimization failure, but rather an emerging phase transition arising
from the interplay between data structure and variational constraints. This
perspective offers new insights into the trainability and representational
capacity of deep generative models.

</details>


### [155] [Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead](https://arxiv.org/abs/2510.01624)
*Feiyang Kang,Michael Kuchnik,Karthik Padthe,Marin Vlastelica,Ruoxi Jia,Carole-Jean Wu,Newsha Ardalani*

Main category: cs.LG

TL;DR: 本文质疑大语言模型推理后训练中高SFT分数能否转化为RL后性能提升，发现高SFT分数不可靠，研究替代指标，实验表明基于泛化损失和Pass@large k预测更精确，还给出训练建议，评估工具将开源。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型推理后训练中高SFT分数是否能转化为RL后的性能提升。

Method: 训练数百个模型，通过GRPO进行SFT和RLVR，在7个数学基准上进行广泛评估，使用多种模型和数据集。

Result: 高SFT分数不可靠预测RL增益或后训练效果，基于泛化损失和Pass@large k预测比基于预RL性能预测精度大幅提高。

Conclusion: 泛化损失和Pass@large k可作为RL结果的强代理指标，对广泛用例有实用价值，还给出不同训练方式的效果对比和建议。

Abstract: In post-training for reasoning Large Language Models (LLMs), the current
state of practice trains LLMs in two independent stages: Supervised Fine-Tuning
(SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as
``RL'' below). In this work, we challenge whether high SFT scores translate to
improved performance after RL. We provide extensive counter-examples where this
is not true. We find high SFT scores can be biased toward simpler or more
homogeneous data and are not reliably predictive of subsequent RL gains or
scaled-up post-training effectiveness. In some cases, RL training on models
with improved SFT performance could lead to substantially worse outcome
compared to RL on the base model without SFT. We study alternative metrics and
identify generalization loss on held-out reasoning examples and Pass@large k
performance to provide strong proxies for the RL outcome. We trained hundreds
of models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive
evaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU
hours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple
state-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL
performance, prediction based on generalization loss and Pass@large k achieves
substantial higher precision, improving $R^2$ coefficient and Spearman's rank
correlation coefficient by up to 0.5 (2x). This provides strong utility for
broad use cases. For example, in most experiments, we find SFT training on
unique examples for a one epoch underperforms training on half examples for two
epochs, either after SFT or SFT-then-RL; With the same SFT budget, training
only on short examples may lead to better SFT performance, though, it often
leads to worse outcome after RL compared to training on examples with varying
lengths. Evaluation tool will be open-sourced.

</details>


### [156] [Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls](https://arxiv.org/abs/2510.01631)
*Feiyang Kang,Newsha Ardalani,Michael Kuchnik,Youssef Emad,Mostafa Elhoushi,Shubhabrata Sengupta,Shang-Wen Li,Ramya Raghavendra,Ruoxi Jia,Carole-Jean Wu*

Main category: cs.LG

TL;DR: 本文通过大规模实证研究，比较自然数据、合成数据及混合数据在大语言模型预训练中的效果，发现合成数据的使用效果与模型大小和数据预算有关，为合成数据在预训练中的应用提供指导。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据供应有限，合成数据技术有望突破这一限制，因此研究合成数据在大语言模型预训练中的应用。

Method: 采用统一协议和缩放定律，进行大规模实证研究（超1000个大语言模型，超10万GPU小时），比较自然网络数据、多种合成数据及混合数据。

Result: 单独使用改写合成数据预训练不比自然网络文本快；1/3改写合成数据与2/3自然网络文本混合可加速5 - 10倍；单独使用教科书式合成数据在下游领域损失更高；合成数据的“最佳”比例与模型大小和数据预算有关；较大生成模型不一定生成更好的预训练数据；不同合成数据在“模型崩溃”方面表现不同。

Conclusion: 揭示了合成数据在预训练中的作用，验证了其有条件的益处，并提供了实际指导。

Abstract: Training data plays a crucial role in Large Language Models (LLM) scaling,
yet high quality data is of limited supply. Synthetic data techniques offer a
potential path toward sidestepping these limitations. We conduct a large-scale
empirical investigation (>1000 LLMs with >100k GPU hours) using a unified
protocol and scaling laws, comparing natural web data, diverse synthetic types
(rephrased text, generated textbooks), and mixtures of natural and synthetic
data. Specifically, we found pre-training on rephrased synthetic data
\textit{alone} is not faster than pre-training on natural web texts; while
pre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts
can speed up 5-10x (to reach the same validation loss) at larger data budgets.
Pre-training on textbook-style synthetic data \textit{alone} results in notably
higher loss on many downstream domains especially at small data budgets. "Good"
ratios of synthetic data in training data mixtures depend on the model size and
data budget, empirically converging to ~30% for rephrased synthetic data.
Larger generator models do not necessarily yield better pre-training data than
~8B-param models. These results contribute mixed evidence on "model collapse"
during large-scale single-round (n=1) model training on synthetic
data--training on rephrased synthetic data shows no degradation in performance
in foreseeable scales whereas training on mixtures of textbook-style
pure-generated synthetic data shows patterns predicted by "model collapse". Our
work demystifies synthetic data in pre-training, validates its conditional
benefits, and offers practical guidance.

</details>


### [157] [CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning](https://arxiv.org/abs/2510.01634)
*Ryan Y. Lin,Siddhartha Ojha,Nicholas Bai*

Main category: cs.LG

TL;DR: 提出曲率自适应Transformer（CAT）架构，能动态学习跨三种几何注意力分支的逐令牌路由，在知识图谱补全基准上表现优于固定几何基线。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer在注意力机制中隐式假设欧几里得几何，对非欧结构数据效果有限，且现有扩展需事先选定单一几何，缺乏灵活性。

Method: 引入CAT架构，通过轻量级、可微的门控机制动态学习跨三种几何注意力分支的逐令牌路由。

Result: 在知识图谱补全基准（FB15k - 237、WN18RR）上，MRR和Hits@10比固定几何基线提高约10%，开销极小（参数增加5%，推理时间相当）。

Conclusion: 学习的几何自适应在复杂关系推理中优于单一固定几何，CAT可作为跨语言、视觉和多模态领域的几何混合架构的可扩展且可解释的基础。

Abstract: Transformers achieve strong performance across diverse domains but implicitly
assume Euclidean geometry in their attention mechanisms, limiting their
effectiveness on data with non-Euclidean structure. While recent extensions to
hyperbolic and spherical spaces show promise for hierarchical and cyclical
patterns, respectively, they require committing to a single geometry a priori,
reducing flexibility when data exhibits mixed geometric properties. We
introduce the Curvature-Adaptive Transformer (CAT), a novel architecture that
dynamically learns per-token routing across three geometric attention branches
through a lightweight, differentiable gating mechanism. Unlike fixed-geometry
approaches, CAT enables adaptive geometric specialization, routing tokens to
the appropriate curvature based on their local relational structure. The
routing network provides interpretable curvature preferences while each branch
employs geometry-specific operations optimized for its respective manifold. On
knowledge graph completion benchmarks (FB15k-237, WN18RR), CAT achieves
approximately 10% improvements in MRR and Hits@10 over fixed-geometry baselines
with minimal overhead (5% parameter increase, comparable inference time). These
results demonstrate that learned geometric adaptation outperforms any single
fixed geometry for complex relational reasoning, establishing CAT as a scalable
and interpretable foundation for mixture-of-geometry architectures across
language, vision, and multimodal domains.

</details>


### [158] [Detecting Post-generation Edits to Watermarked LLM Outputs via Combinatorial Watermarking](https://arxiv.org/abs/2510.01637)
*Liyan Xie,Muhammad Siddeek,Mohamed Seif,Andrea J. Goldsmith,Mengdi Wang*

Main category: cs.LG

TL;DR: 提出基于组合模式的水印框架检测大语言模型输出的生成后编辑，在编辑定位上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，大语言模型生成内容可能被编辑，需要检测和定位这些修改。

Method: 提出基于组合模式的水印框架，对词汇表分区，在生成时嵌入水印，结合全局统计检测水印，设计轻量级局部统计定位编辑，引入两个评估指标。

Result: 在多种编辑场景下的开源大语言模型上评估，在编辑定位上有很强的实证表现。

Conclusion: 所提出的方法能有效检测和定位水印大语言模型输出的生成后编辑。

Abstract: Watermarking has become a key technique for proprietary language models,
enabling the distinction between AI-generated and human-written text. However,
in many real-world scenarios, LLM-generated content may undergo post-generation
edits, such as human revisions or even spoofing attacks, making it critical to
detect and localize such modifications. In this work, we introduce a new task:
detecting post-generation edits locally made to watermarked LLM outputs. To
this end, we propose a combinatorial pattern-based watermarking framework,
which partitions the vocabulary into disjoint subsets and embeds the watermark
by enforcing a deterministic combinatorial pattern over these subsets during
generation. We accompany the combinatorial watermark with a global statistic
that can be used to detect the watermark. Furthermore, we design lightweight
local statistics to flag and localize potential edits. We introduce two
task-specific evaluation metrics, Type-I error rate and detection accuracy, and
evaluate our method on open-source LLMs across a variety of editing scenarios,
demonstrating strong empirical performance in edit localization.

</details>


### [159] [Support Basis: Fast Attention Beyond Bounded Entries](https://arxiv.org/abs/2510.01643)
*Maryam Aliakbarpour,Vladimir Braverman,Junze Yin,Haochen Zhang*

Main category: cs.LG

TL;DR: 提出支持基分解框架进行高效注意力近似，突破有界条目限制，有理论保证并解释多项式注意力成功原因。


<details>
  <summary>Details</summary>
Motivation: 现有子二次注意力近似算法在有界条目假设下工作，该假设实际少见，限制其在现代大语言模型中的应用。

Method: 引入支持基分解框架，利用查询和键矩阵条目的次高斯行为拆分大小条目，对稀疏和密集组件分别处理，扩展到多阈值设置。

Result: 建立了子二次运行时间的严格理论保证，为多项式注意力的实证成功提供了首个理论依据。

Conclusion: 支持基分解框架可有效进行注意力近似，能突破有界条目限制，适用于现代大语言模型。

Abstract: The quadratic complexity of softmax attention remains a central bottleneck in
scaling large language models (LLMs). [Alman and Song, NeurIPS 2023] proposed a
sub-quadratic attention approximation algorithm, but it works only under the
restrictive bounded-entry assumption. Since this assumption rarely holds in
practice, its applicability to modern LLMs is limited.
  In this paper, we introduce support-basis decomposition, a new framework for
efficient attention approximation beyond bounded entries. We empirically
demonstrate that the entries of the query and key matrices exhibit sub-Gaussian
behavior. Our approach uses this property to split large and small entries,
enabling exact computation on sparse components and polynomial approximation on
dense components. We establish rigorous theoretical guarantees, proving a
sub-quadratic runtime, and extend the method to a multi-threshold setting that
eliminates all distributional assumptions. Furthermore, we provide the first
theoretical justification for the empirical success of polynomial attention
[Kacham, Mirrokni, and Zhong, ICML 2024], showing that softmax attention can be
closely approximated by a combination of multiple polynomial attentions with
sketching.

</details>


### [160] [Source-Free Cross-Domain Continual Learning](https://arxiv.org/abs/2510.01649)
*Muhammad Tanzil Furqon,Mahardhika Pratama,Igor Škrjanc,Lin Liu,Habibullah Habibullah,Kutluyil Dogancay*

Main category: cs.LG

TL;DR: 本文研究无源跨域持续学习问题，提出REFEREE方法，结合预训练模型与视觉语言模型，用频率感知提示技术等解决多问题，实验显示优于有源方法。


<details>
  <summary>Details</summary>
Motivation: 现有跨域持续学习方法需全标注源域样本，在隐私受限环境不可行，因此研究无源跨域持续学习问题。

Method: 提出REFEREE方法，结合源预训练模型和大规模视觉语言模型；用频率感知提示技术处理域偏移；用不确定性加权策略解决伪标签问题；用核线性判别分析克服灾难性遗忘。

Result: 严格的数值研究表明，该方法显著优于能访问源域样本的先前方法。

Conclusion: 所提出的REFEREE方法在无源跨域持续学习中有效，能克服多个问题并取得良好性能。

Abstract: Although existing cross-domain continual learning approaches successfully
address many streaming tasks having domain shifts, they call for a fully
labeled source domain hindering their feasibility in the privacy constrained
environments. This paper goes one step ahead with the problem of source-free
cross-domain continual learning where the use of source-domain samples are
completely prohibited. We propose the idea of rehearsal-free frequency-aware
dynamic prompt collaborations (REFEREE) to cope with the absence of labeled
source-domain samples in realm of cross-domain continual learning. REFEREE is
built upon a synergy between a source-pre-trained model and a large-scale
vision-language model, thus overcoming the problem of sub-optimal
generalizations when relying only on a source pre-trained model. The domain
shift problem between the source domain and the target domain is handled by a
frequency-aware prompting technique encouraging low-frequency components while
suppressing high-frequency components. This strategy generates frequency-aware
augmented samples, robust against noisy pseudo labels. The noisy pseudo-label
problem is further addressed with the uncertainty-aware weighting strategy
where the mean and covariance matrix are weighted by prediction uncertainties,
thus mitigating the adverse effects of the noisy pseudo label. Besides, the
issue of catastrophic forgetting (CF) is overcome by kernel linear discriminant
analysis (KLDA) where the backbone network is frozen while the classification
is performed using the linear discriminant analysis approach guided by the
random kernel method. Our rigorous numerical studies confirm the advantage of
our approach where it beats prior arts having access to source domain samples
with significant margins.

</details>


### [161] [The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM](https://arxiv.org/abs/2510.01650)
*Kwanhee Lee,Hyeondo Jang,Dongyeop Lee,Dan Alistarh,Namhoon Lee*

Main category: cs.LG

TL;DR: 本文提出名为Elsa的方法突破大语言模型剪枝困境，能达90%极端稀疏度且保持高保真，还有量化变体Elsa_-L并给出理论收敛保证。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络剪枝方法难以在不严重降低模型精度的情况下突破50 - 60%的稀疏度，现需更好方法。

Method: 识别当前实践局限，通过基于ADMM的标准约束优化技术解决依赖替代目标公式的问题。

Result: Elsa在多种模型和规模上比现有方法有显著改进，如在LLaMA - 2 - 7B上90%稀疏度时困惑度比最佳现有方法低7.8倍；有可扩展到27B的量化变体Elsa_-L并给出理论收敛保证。

Conclusion: 在推进大语言模型稀疏性前沿取得有意义进展，未充分探索的方向或有进一步提升机会。

Abstract: Neural network pruning is a promising technique to mitigate the excessive
computational and memory requirements of large language models (LLMs). Despite
its promise, however, progress in this area has diminished, as conventional
methods are seemingly unable to surpass moderate sparsity levels (50-60%)
without severely degrading model accuracy. This work breaks through the current
impasse, presenting a principled and effective method called $\texttt{Elsa}$,
which achieves extreme sparsity levels of up to 90% while retaining high model
fidelity. This is done by identifying several limitations in current practice,
all of which can be traced back to their reliance on a surrogate objective
formulation. $\texttt{Elsa}$ tackles this issue directly and effectively via
standard and well-established constrained optimization techniques based on
ADMM. Our extensive experiments across a wide range of models and scales show
that $\texttt{Elsa}$ achieves substantial improvements over existing methods;
e.g., it achieves 7.8$\times$ less perplexity than the best existing method on
LLaMA-2-7B at 90% sparsity. Furthermore, we present
$\texttt{Elsa}_{\text{-L}}$, a quantized variant that scales to extremely large
models (27B), and establish its theoretical convergence guarantees. These
results highlight meaningful progress in advancing the frontier of LLM
sparsity, while promising that significant opportunities for further
advancement may remain in directions that have so far attracted limited
exploration.

</details>


### [162] [Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning](https://arxiv.org/abs/2510.01656)
*Jiashun Liu,Johan Obando-Ceron,Han Lu,Yancheng He,Weixun Wang,Wenbo Su,Bo Zheng,Pablo Samuel Castro,Aaron Courville,Ling Pan*

Main category: cs.LG

TL;DR: 文章提出AsyPPO框架，用轻量级迷你评论家解决RL4LLM中传统评论家计算成本高的问题，经训练能提升学习稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统价值函数在大语言模型规模下训练计算成本高，且在稀疏奖励和长推理跨度下常失效，需解决此瓶颈。

Method: 引入AsyPPO框架，采用一组轻量级迷你评论家，在不相交的提示分片上训练，利用评论家间的不确定性优化策略更新。

Result: 仅用5000个样本在开源数据上训练后，AsyPPO在多个基准测试中比GRPO等强基线提升学习稳定性和性能，在多个模型上比经典PPO有显著性能提升。

Conclusion: 强调架构创新对可扩展、高效算法的重要性。

Abstract: Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing
them with average advantage baselines. This shift is largely pragmatic:
conventional value functions are computationally expensive to train at LLM
scale and often fail under sparse rewards and long reasoning horizons. We
revisit this bottleneck from an architectural perspective and introduce
Asymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable
framework that restores the critics role while remaining efficient in
large-model settings. AsyPPO employs a set of lightweight mini-critics, each
trained on disjoint prompt shards. This design encourages diversity while
preserving calibration, reducing value-estimation bias. Beyond robust
estimation, AsyPPO leverages inter-critic uncertainty to refine the policy
update: (i) masking advantages in states where critics agree and gradients add
little learning signal, and (ii) filtering high-divergence states from entropy
regularization, suppressing spurious exploration. After training on open-source
data with only 5,000 samples, AsyPPO consistently improves learning stability
and performance across multiple benchmarks over strong baselines, such as GRPO,
achieving performance gains of more than six percent on Qwen3-4b-Base and about
three percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without
additional tricks. These results highlight the importance of architectural
innovations for scalable, efficient algorithms.

</details>


### [163] [Learning Time-Series Representations by Hierarchical Uniformity-Tolerance Latent Balancing](https://arxiv.org/abs/2510.01658)
*Amin Jalali,Milad Soltany,Michael Greenspan,Ali Etemad*

Main category: cs.LG

TL;DR: 提出TimeHUT方法，通过对比表示的分层均匀 - 容差平衡学习时间序列表示，在多任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 学习时间序列表示，在嵌入空间中有效平衡均匀性和容差性。

Method: 使用分层设置学习实例和时间信息，在对比损失中集成温度调度器，采用分层角度边缘损失增强正负对分离。

Result: 在分类任务上大幅超越先前方法，在异常检测任务上取得有竞争力的结果。

Conclusion: TimeHUT方法在时间序列表示学习上表现良好，不同组件和超参数对结果有影响。

Abstract: We propose TimeHUT, a novel method for learning time-series representations
by hierarchical uniformity-tolerance balancing of contrastive representations.
Our method uses two distinct losses to learn strong representations with the
aim of striking an effective balance between uniformity and tolerance in the
embedding space. First, TimeHUT uses a hierarchical setup to learn both
instance-wise and temporal information from input time-series. Next, we
integrate a temperature scheduler within the vanilla contrastive loss to
balance the uniformity and tolerance characteristics of the embeddings.
Additionally, a hierarchical angular margin loss enforces instance-wise and
temporal contrast losses, creating geometric margins between positive and
negative pairs of temporal sequences. This approach improves the coherence of
positive pairs and their separation from the negatives, enhancing the capture
of temporal dependencies within a time-series sample. We evaluate our approach
on a wide range of tasks, namely 128 UCR and 30 UAE datasets for univariate and
multivariate classification, as well as Yahoo and KPI datasets for anomaly
detection. The results demonstrate that TimeHUT outperforms prior methods by
considerable margins on classification, while obtaining competitive results for
anomaly detection. Finally, detailed sensitivity and ablation studies are
performed to evaluate different components and hyperparameters of our method.

</details>


### [164] [Shift-Invariant Attribute Scoring for Kolmogorov-Arnold Networks via Shapley Value](https://arxiv.org/abs/2510.01663)
*Wangxuan Fan,Ching Wang,Siqi Li,Nan Liu*

Main category: cs.LG

TL;DR: 传统神经网络是黑盒模型，Kolmogorov - Arnold Networks (KANs) 可恢复符号表示但剪枝有挑战，本文提出 ShapKAN 剪枝框架，实验证明其能保留节点重要性并有效压缩网络，提升 KAN 可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络是黑盒模型，KANs 虽能解决功能关系问题但剪枝面临挑战，传统基于幅度的方法不可靠，需要新的剪枝方法。

Method: 提出 ShapKAN 剪枝框架，使用 Shapley 值归因以平移不变的方式评估节点重要性。

Result: 在合成和真实世界数据集上的大量实验表明，ShapKAN 能保留真正的节点重要性，实现有效的网络压缩。

Conclusion: ShapKAN 方法提高了 KAN 的可解释性优势，便于在资源受限环境中部署。

Abstract: For many real-world applications, understanding feature-outcome relationships
is as crucial as achieving high predictive accuracy. While traditional neural
networks excel at prediction, their black-box nature obscures underlying
functional relationships. Kolmogorov--Arnold Networks (KANs) address this by
employing learnable spline-based activation functions on edges, enabling
recovery of symbolic representations while maintaining competitive performance.
However, KAN's architecture presents unique challenges for network pruning.
Conventional magnitude-based methods become unreliable due to sensitivity to
input coordinate shifts. We propose \textbf{ShapKAN}, a pruning framework using
Shapley value attribution to assess node importance in a shift-invariant
manner. Unlike magnitude-based approaches, ShapKAN quantifies each node's
actual contribution, ensuring consistent importance rankings regardless of
input parameterization. Extensive experiments on synthetic and real-world
datasets demonstrate that ShapKAN preserves true node importance while enabling
effective network compression. Our approach improves KAN's interpretability
advantages, facilitating deployment in resource-constrained environments.

</details>


### [165] [Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.01677)
*Han Wu,Yanming Sun,Yunhe Yang,Derek F. Wong*

Main category: cs.LG

TL;DR: 提出AGFN网络解决多模态情感分析中简单融合技术的局限，实验显示其性能优于基线，能提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析中简单融合技术未考虑模态质量差异，导致性能不佳，难以识别细微情感差异。

Method: 引入自适应门控融合网络（AGFN），通过基于信息熵和模态重要性的双门融合机制自适应调整特征权重。

Result: 在CMU - MOSI和CMU - MOSEI上AGFN准确率显著优于强基线，能有效识别细微情感，可视化分析表明其可增强泛化能力。

Conclusion: AGFN能缓解噪声模态影响，优先处理信息线索，创建更鲁棒的多模态特征表示。

Abstract: Multimodal sentiment analysis (MSA) leverages information fusion from diverse
modalities (e.g., text, audio, visual) to enhance sentiment prediction.
However, simple fusion techniques often fail to account for variations in
modality quality, such as those that are noisy, missing, or semantically
conflicting. This oversight leads to suboptimal performance, especially in
discerning subtle emotional nuances. To mitigate this limitation, we introduce
a simple yet efficient \textbf{A}daptive \textbf{G}ated \textbf{F}usion
\textbf{N}etwork that adaptively adjusts feature weights via a dual gate fusion
mechanism based on information entropy and modality importance. This mechanism
mitigates the influence of noisy modalities and prioritizes informative cues
following unimodal encoding and cross-modal interaction. Experiments on
CMU-MOSI and CMU-MOSEI show that AGFN significantly outperforms strong
baselines in accuracy, effectively discerning subtle emotions with robust
performance. Visualization analysis of feature representations demonstrates
that AGFN enhances generalization by learning from a broader feature
distribution, achieved by reducing the correlation between feature location and
prediction error, thereby decreasing reliance on specific locations and
creating more robust multimodal feature representations.

</details>


### [166] [PASTA: A Unified Framework for Offline Assortment Learning](https://arxiv.org/abs/2510.01693)
*Juncheng Dong,Weibin Mo,Zhengling Qi,Cong Shi,Ethan X. Fang,Vahid Tarokh*

Main category: cs.LG

TL;DR: 研究离线数据驱动的商品组合优化问题，提出PASTA框架，有理论界并在数值实验中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 商品组合优化的组合性质导致数据覆盖不足，缺乏有效解决方案。

Method: 引入基于悲观原则的PASTA框架，在一般选择模型下实现最优预期收入。

Result: 建立了几种常用选择模型的有限样本遗憾界，推导了极小极大遗憾下界，证明了PASTA的极小极大最优性，数值实验表现更优。

Conclusion: PASTA框架是解决离线数据驱动商品组合优化问题的有效方法。

Abstract: We study a broad class of assortment optimization problems in an offline and
data-driven setting. In such problems, a firm lacks prior knowledge of the
underlying choice model, and aims to determine an optimal assortment based on
historical customer choice data. The combinatorial nature of assortment
optimization often results in insufficient data coverage, posing a significant
challenge in designing provably effective solutions. To address this, we
introduce a novel Pessimistic Assortment Optimization (PASTA) framework that
leverages the principle of pessimism to achieve optimal expected revenue under
general choice models. Notably, PASTA requires only that the offline data
distribution contains an optimal assortment, rather than providing the full
coverage of all feasible assortments. Theoretically, we establish the first
finite-sample regret bounds for offline assortment optimization across several
widely used choice models, including the multinomial logit and nested logit
models. Additionally, we derive a minimax regret lower bound, proving that
PASTA is minimax optimal in terms of sample and model complexity. Numerical
experiments further demonstrate that our method outperforms existing baseline
approaches.

</details>


### [167] [Representational Alignment Across Model Layers and Brain Regions with Hierarchical Optimal Transport](https://arxiv.org/abs/2510.01706)
*Shaan Shah,Meenakshi Khosla*

Main category: cs.LG

TL;DR: 提出Hierarchical Optimal Transport (HOT)框架用于网络表征相似性比较，在多领域表现优于标准方法，能实现更丰富可解释的比较。


<details>
  <summary>Details</summary>
Motivation: 标准表征相似性方法存在对齐结果不对称、缺乏全局对齐分数、难以处理不同深度网络等局限，原因是忽略全局激活结构和限制映射方式。

Method: 提出HOT统一框架，联合推断软的、全局一致的层间耦合和神经元级传输计划，允许源神经元在多个目标层分配质量。

Result: 在视觉模型、大语言模型和人类视觉皮层记录上评估，HOT对齐质量匹配或超越标准方法，揭示了平滑、细粒度的分层对应关系。

Conclusion: HOT能实现更丰富、可解释的表征比较，尤其适用于架构或深度不同的网络。

Abstract: Standard representational similarity methods align each layer of a network to
its best match in another independently, producing asymmetric results, lacking
a global alignment score, and struggling with networks of different depths.
These limitations arise from ignoring global activation structure and
restricting mappings to rigid one-to-one layer correspondences. We propose
Hierarchical Optimal Transport (HOT), a unified framework that jointly infers
soft, globally consistent layer-to-layer couplings and neuron-level transport
plans. HOT allows source neurons to distribute mass across multiple target
layers while minimizing total transport cost under marginal constraints. This
yields both a single alignment score for the entire network comparison and a
soft transport plan that naturally handles depth mismatches through mass
distribution. We evaluate HOT on vision models, large language models, and
human visual cortex recordings. Across all domains, HOT matches or surpasses
standard pairwise matching in alignment quality. Moreover, it reveals smooth,
fine-grained hierarchical correspondences: early layers map to early layers,
deeper layers maintain relative positions, and depth mismatches are resolved by
distributing representations across multiple layers. These structured patterns
emerge naturally from global optimization without being imposed, yet are absent
in greedy layer-wise methods. HOT thus enables richer, more interpretable
comparisons between representations, particularly when networks differ in
architecture or depth.

</details>


### [168] [ActiNet: Activity intensity classification of wrist-worn accelerometers using self-supervised deep learning](https://arxiv.org/abs/2510.01712)
*Aidan Acquah,Shing Chan,Aiden Doherty*

Main category: cs.LG

TL;DR: 本文用ActiNet模型结合HMM对腕部加速度计数据进行活动强度分类，性能超基线RF + HMM，建议未来流行病学研究使用ActiNet。


<details>
  <summary>Details</summary>
Motivation: 探究自监督学习模型结合HMM对人类活动识别（HAR）分类性能及预测日常活动强度组成的影响。

Method: 使用151名CAPTURE - 24参与者数据训练ActiNet模型并进行HMM平滑，用5折分层组交叉验证评估，与基线RF + HMM对比，分析不同年龄和性别亚组差异。

Result: ActiNet模型区分活动强度标签的平均宏F1分数为0.82，平均Cohen's kappa分数为0.86，优于RF + HMM。

Conclusion: 鼓励在未来流行病学研究中使用ActiNet从腕部加速度计数据中提取活动强度标签。

Abstract: The use of reliable and accurate human activity recognition (HAR) models on
passively collected wrist-accelerometer data is essential in large-scale
epidemiological studies that investigate the association between physical
activity and health outcomes. While the use of self-supervised learning has
generated considerable excitement in improving HAR, it remains unknown the
extent to which these models, coupled with hidden Markov models (HMMs), would
make a tangible improvement to classification performance, and the effect this
may have on the predicted daily activity intensity compositions. Using 151
CAPTURE-24 participants' data, we trained the ActiNet model, a self-supervised,
18-layer, modified ResNet-V2 model, followed by hidden Markov model (HMM)
smoothing to classify labels of activity intensity. The performance of this
model, evaluated using 5-fold stratified group cross-validation, was then
compared to a baseline random forest (RF) + HMM, established in existing
literature. Differences in performance and classification outputs were compared
with different subgroups of age and sex within the Capture-24 population. The
ActiNet model was able to distinguish labels of activity intensity with a mean
macro F1 score of 0.82, and mean Cohen's kappa score of 0.86. This exceeded the
performance of the RF + HMM, trained and validated on the same dataset, with
mean scores of 0.77 and 0.81, respectively. These findings were consistent
across subgroups of age and sex. These findings encourage the use of ActiNet
for the extraction of activity intensity labels from wrist-accelerometer data
in future epidemiological studies.

</details>


### [169] [Latency-aware Multimodal Federated Learning over UAV Networks](https://arxiv.org/abs/2510.01717)
*Shaba Shaon,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: 本文研究无人机辅助的联邦多模态学习，提出算法优化系统延迟并做收敛分析，实验表明该框架性能更优。


<details>
  <summary>Details</summary>
Motivation: 最小化无人机网络中联邦多模态学习系统的延迟，克服单模态系统局限性，提升模型性能。

Method: 提出结合块坐标下降和逐次凸近似技术的迭代优化算法，对非凸损失函数下的框架进行理论收敛分析。

Result: 数值实验显示该框架在不同数据设置下，系统延迟和模型训练性能优于现有方法。

Conclusion: 所提无人机辅助的联邦多模态学习框架有效可行，能优化系统延迟和提升模型性能。

Abstract: This paper investigates federated multimodal learning (FML) assisted by
unmanned aerial vehicles (UAVs) with a focus on minimizing system latency and
providing convergence analysis. In this framework, UAVs are distributed
throughout the network to collect data, participate in model training, and
collaborate with a base station (BS) to build a global model. By utilizing
multimodal sensing, the UAVs overcome the limitations of unimodal systems,
enhancing model accuracy, generalization, and offering a more comprehensive
understanding of the environment. The primary objective is to optimize FML
system latency in UAV networks by jointly addressing UAV sensing scheduling,
power control, trajectory planning, resource allocation, and BS resource
management. To address the computational complexity of our latency minimization
problem, we propose an efficient iterative optimization algorithm combining
block coordinate descent and successive convex approximation techniques, which
provides high-quality approximate solutions. We also present a theoretical
convergence analysis for the UAV-assisted FML framework under a non-convex loss
function. Numerical experiments demonstrate that our FML framework outperforms
existing approaches in terms of system latency and model training performance
under different data settings.

</details>


### [170] [Accelerating Attention with Basis Decomposition](https://arxiv.org/abs/2510.01718)
*Jialin Zhao*

Main category: cs.LG

TL;DR: 提出无损注意力算法BD Attention (BDA)，基于矩阵恒等式，架构无关，在DeepSeek - V2 - Lite上实现加速和权重缩减，对模型性能影响小。


<details>
  <summary>Details</summary>
Motivation: 改进大语言模型和视觉 - 语言模型中核心操作注意力的计算效率。

Method: 利用Basis Decomposition的简单矩阵恒等式对多头投影进行重构，形成紧凑形式。

Result: 在DeepSeek - V2 - Lite上，BDA离线准备仅需4s，无需重新训练，实现32%的键/值投影加速和25%的权重缩减，FP16下困惑度仅增加0.02%，FP32下增加0.0004%。

Conclusion: BDA是首个理论上精确的无损注意力加速方法，可与现有工程级优化互补。

Abstract: Attention is a core operation in large language models (LLMs) and
vision-language models (VLMs). We present BD Attention (BDA), the first
lossless algorithmic reformulation of attention. BDA is enabled by a simple
matrix identity from Basis Decomposition (BD), which restructures multi-head
projections into a compact form while preserving exact outputs. Unlike
I/O-aware system optimizations such as FlashAttention, BDA provides a
mathematically guaranteed acceleration that is architecture-agnostic. On
DeepSeek-V2-Lite (16B, FP16), BDA requires only 4s of offline preparation with
no retraining required and, on modern GPUs, achieves 32% faster key/value
projections and 25% smaller weights, while increasing end-to-end perplexity
(PPL) by just 0.02% (FP16) or 0.0004% (FP32), a negligible effect on model
performance. These results position BDA as the first theoretically exact method
for lossless attention acceleration that is complementary to existing
engineering-level optimizations. Our code is available at
https://github.com/abcbdf/basis-decomposition-official.

</details>


### [171] [Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation](https://arxiv.org/abs/2510.01721)
*Saptarshi Mandal,Yashaswini Murthy,R. Srikant*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Distributionally robust reinforcement learning (DRRL) focuses on designing
policies that achieve good performance under model uncertainties. In
particular, we are interested in maximizing the worst-case long-term discounted
reward, where the data for RL comes from a nominal model while the deployed
environment can deviate from the nominal model within a prescribed uncertainty
set. Existing convergence guarantees for robust temporal-difference (TD)
learning for policy evaluation are limited to tabular MDPs or are dependent on
restrictive discount-factor assumptions when function approximation is used. We
present the first robust TD learning with linear function approximation, where
robustness is measured with respect to the total-variation distance and
Wasserstein-l distance uncertainty set. Additionally, our algorithm is both
model-free and does not require generative access to the MDP. Our algorithm
combines a two-time-scale stochastic-approximation update with an outer-loop
target-network update. We establish an $\tilde{O}(1/\epsilon^2)$ sample
complexity to obtain an $\epsilon$-accurate value estimate. Our results close a
key gap between the empirical success of robust RL algorithms and the
non-asymptotic guarantees enjoyed by their non-robust counterparts. The key
ideas in the paper also extend in a relatively straightforward fashion to
robust Q-learning with function approximation.

</details>


### [172] [Workplace Location Choice Model based on Deep Neural Network](https://arxiv.org/abs/2510.01723)
*Tanay Rastogi,Anders Karlström*

Main category: cs.LG

TL;DR: 本文提出用深度神经网络（DNN）建模工作地点选择，对比了其与传统离散选择模型（DCM）的表现，强调按需选模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统离散选择模型在准确反映个人决策过程方面存在挑战，需要更好的方法来理解复杂决策模式。

Method: 提出深度神经网络方法来建模工作地点选择，并与传统离散选择模型进行对比。

Result: DNN在某些方面优于DCM，但DCM在评估个人属性对工作距离的影响时更符合数据，DCM在短距离表现出色，DNN在长距离表现与数据和DCM相当。

Conclusion: 在工作地点选择分析中，应根据具体应用需求选择合适的模型。

Abstract: Discrete choice models (DCMs) have long been used to analyze workplace
location decisions, but they face challenges in accurately mirroring individual
decision-making processes. This paper presents a deep neural network (DNN)
method for modeling workplace location choices, which aims to better understand
complex decision patterns and provides better results than traditional discrete
choice models (DCMs). The study demonstrates that DNNs show significant
potential as a robust alternative to DCMs in this domain. While both models
effectively replicate the impact of job opportunities on workplace location
choices, the DNN outperforms the DCM in certain aspects. However, the DCM
better aligns with data when assessing the influence of individual attributes
on workplace distance. Notably, DCMs excel at shorter distances, while DNNs
perform comparably to both data and DCMs for longer distances. These findings
underscore the importance of selecting the appropriate model based on specific
application requirements in workplace location choice analysis.

</details>


### [173] [Private and Fair Machine Learning: Revisiting the Disparate Impact of Differentially Private SGD](https://arxiv.org/abs/2510.01744)
*Lea Demelius,Dominik Kowald,Simone Kopeinik,Roman Kern,Andreas Trügler*

Main category: cs.LG

TL;DR: 分析DPSGD训练神经网络时优化超参数对公平性和性能影响的可推广性，指出直接优化可改善效用 - 公平权衡，但超参调整会泄露隐私，DPSGD - Global - Adapt非稳健方案。


<details>
  <summary>Details</summary>
Motivation: 多数研究认为DPSGD对公平性有负面影响，虽有观点称直接优化超参可使公平性达非私有模型水平，本文分析此说法的可推广性。

Method: 比较DPSGD对不同性能指标的不同影响，在广泛超参设置下进行分析，并将分析拓展到DPSGD - Global - Adapt。

Result: 一个指标的差异影响不一定意味着另一个指标也有差异影响；直接优化超参不能可靠减轻DPSGD的差异影响，但可改善效用 - 公平权衡；超参调整会导致额外隐私泄露；DPSGD - Global - Adapt在超参选择上可能不是稳健方案。

Conclusion: 超参调整需谨慎平衡隐私、效用和公平性，DPSGD - Global - Adapt不是解决差异影响的可靠方法。

Abstract: Differential privacy (DP) is a prominent method for protecting information
about individuals during data analysis. Training neural networks with
differentially private stochastic gradient descent (DPSGD) influences the
model's learning dynamics and, consequently, its output. This can affect the
model's performance and fairness. While the majority of studies on the topic
report a negative impact on fairness, it has recently been suggested that
fairness levels comparable to non-private models can be achieved by optimizing
hyperparameters for performance directly on differentially private models
(rather than re-using hyperparameters from non-private models, as is common
practice). In this work, we analyze the generalizability of this claim by 1)
comparing the disparate impact of DPSGD on different performance metrics, and
2) analyzing it over a wide range of hyperparameter settings. We highlight that
a disparate impact on one metric does not necessarily imply a disparate impact
on another. Most importantly, we show that while optimizing hyperparameters
directly on differentially private models does not mitigate the disparate
impact of DPSGD reliably, it can still lead to improved utility-fairness
trade-offs compared to re-using hyperparameters from non-private models. We
stress, however, that any form of hyperparameter tuning entails additional
privacy leakage, calling for careful considerations of how to balance privacy,
utility and fairness. Finally, we extend our analyses to DPSGD-Global-Adapt, a
variant of DPSGD designed to mitigate the disparate impact on accuracy, and
conclude that this alternative may not be a robust solution with respect to
hyperparameter choice.

</details>


### [174] [Learning Regularization Functionals for Inverse Problems: A Comparative Study](https://arxiv.org/abs/2510.01755)
*Johannes Hertrich,Hok Shing Wong,Alexander Denker,Stanislas Ducotterd,Zhenghan Fang,Markus Haltmeier,Željko Kereta,Erich Kobler,Oscar Leong,Mohammad Sadegh Salehi,Carola-Bibiane Schönlieb,Johannes Schwab,Zakhar Shumaylov,Jeremias Sulam,German Shâma Wache,Martin Zach,Yasi Zhang,Matthias J. Ehrhardt,Sebastian Neumayer*

Main category: cs.LG

TL;DR: 收集并统一成像逆问题学习正则化框架代码，进行系统比较并给出方法描述与实用指南。


<details>
  <summary>Details</summary>
Motivation: 现有成像逆问题学习正则化框架因架构设计和训练策略不同、非模块化实现，难以直接比较。

Method: 收集并统一可用代码到一个通用框架。

Result: 能系统比较各方法，突出其优缺点，洞察未来潜力。

Conclusion: 为成像逆问题学习正则化框架提供统一视角、方法描述和实用指南。

Abstract: In recent years, a variety of learned regularization frameworks for solving
inverse problems in imaging have emerged. These offer flexible modeling
together with mathematical insights. The proposed methods differ in their
architectural design and training strategies, making direct comparison
challenging due to non-modular implementations. We address this gap by
collecting and unifying the available code into a common framework. This
unified view allows us to systematically compare the approaches and highlight
their strengths and limitations, providing valuable insights into their future
potential. We also provide concise descriptions of each method, complemented by
practical guidelines.

</details>


### [175] [Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks](https://arxiv.org/abs/2510.01758)
*Bruno Corcuera,Carlos Eiras-Franco,Brais Cancela*

Main category: cs.LG

TL;DR: 本文提出无监督动态特征选择（DFS）方法增强潜在表征，实验显示该方法能提升模型泛化性能且计算成本增加小。


<details>
  <summary>Details</summary>
Motivation: 视觉任务中潜在表征常受噪声或无关特征影响，降低模型性能和泛化能力。

Method: 提出无监督动态特征选择（DFS）方法，识别并去除图像中误导或冗余信息。

Result: 在图像数据集实验表明，配备无监督DFS的模型在聚类和图像生成等任务中泛化性能显著提升，计算成本增加极小。

Conclusion: 无监督DFS方法能有效增强潜在表征，在不同领域和数据集有广泛应用前景。

Abstract: Latent representations are critical for the performance and robustness of
machine learning models, as they encode the essential features of data in a
compact and informative manner. However, in vision tasks, these representations
are often affected by noisy or irrelevant features, which can degrade the
model's performance and generalization capabilities. This paper presents a
novel approach for enhancing latent representations using unsupervised Dynamic
Feature Selection (DFS). For each instance, the proposed method identifies and
removes misleading or redundant information in images, ensuring that only the
most relevant features contribute to the latent space. By leveraging an
unsupervised framework, our approach avoids reliance on labeled data, making it
broadly applicable across various domains and datasets. Experiments conducted
on image datasets demonstrate that models equipped with unsupervised DFS
achieve significant improvements in generalization performance across various
tasks, including clustering and image generation, while incurring a minimal
increase in the computational cost.

</details>


### [176] [Octax: Accelerated CHIP-8 Arcade Environments for Reinforcement Learning in JAX](https://arxiv.org/abs/2510.01764)
*Waris Radji,Thomas Michel,Hector Piteau*

Main category: cs.LG

TL;DR: 介绍基于CHIP - 8仿真的高性能经典街机游戏环境套件Octax，在GPU上有速度和扩展性优势，适合大规模强化学习实验。


<details>
  <summary>Details</summary>
Motivation: 现有视频游戏计算成本高、不适合大规模强化学习实验，需要可处理且可扩展的环境。

Method: 基于CHIP - 8仿真，用JAX实现Octax套件。

Result: 相比传统CPU模拟器有数量级的速度提升，训练强化学习代理时训练速度和可扩展性显著提高。

Conclusion: Octax是大规模强化学习实验的理想平台，模块化设计方便扩展新游戏或生成新环境。

Abstract: Reinforcement learning (RL) research requires diverse, challenging
environments that are both tractable and scalable. While modern video games may
offer rich dynamics, they are computationally expensive and poorly suited for
large-scale experimentation due to their CPU-bound execution. We introduce
Octax, a high-performance suite of classic arcade game environments implemented
in JAX, based on CHIP-8 emulation, a predecessor to Atari, which is widely
adopted as a benchmark in RL research. Octax provides the JAX community with a
long-awaited end-to-end GPU alternative to the Atari benchmark, offering
image-based environments, spanning puzzle, action, and strategy genres, all
executable at massive scale on modern GPUs. Our JAX-based implementation
achieves orders-of-magnitude speedups over traditional CPU emulators while
maintaining perfect fidelity to the original game mechanics. We demonstrate
Octax's capabilities by training RL agents across multiple games, showing
significant improvements in training speed and scalability compared to existing
solutions. The environment's modular design enables researchers to easily
extend the suite with new games or generate novel environments using large
language models, making it an ideal platform for large-scale RL
experimentation.

</details>


### [177] [Neural non-canonical Hamiltonian dynamics for long-time simulations](https://arxiv.org/abs/2510.01788)
*Clémentine Courtès,Emmanuel Franck,Michael Kraus,Laurent Navoret,Léopold Trémant*

Main category: cs.LG

TL;DR: 本文聚焦从数据中学习非规范哈密顿动力学，指出结合结构保存模型和数值方案时的问题并提出两种训练策略。


<details>
  <summary>Details</summary>
Motivation: 以往研究分别关注模型结构保存和数值方案结构保存，结合二者时出现新问题，如数值不稳定使长时间模拟无法进行。

Method: 提出两种训练策略，一是直接学习向量场，二是通过方案学习时间离散动力学。

Result: 通过多个数值测试案例评估了方法学习复杂物理动力学的能力。

Conclusion: 所提策略可有效解决结合模型和数值方案时的问题，能学习复杂物理动力学。

Abstract: This work focuses on learning non-canonical Hamiltonian dynamics from data,
where long-term predictions require the preservation of structure both in the
learned model and in numerical schemes. Previous research focused on either
facet, respectively with a potential-based architecture and with degenerate
variational integrators, but new issues arise when combining both. In
experiments, the learnt model is sometimes numerically unstable due to the
gauge dependency of the scheme, rendering long-time simulations impossible. In
this paper, we identify this problem and propose two different training
strategies to address it, either by directly learning the vector field or by
learning a time-discrete dynamics through the scheme. Several numerical test
cases assess the ability of the methods to learn complex physical dynamics,
like the guiding center from gyrokinetic plasma physics.

</details>


### [178] [Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of Privacy Filters for Synthetic Data Generation](https://arxiv.org/abs/2510.01793)
*Adil Koeken,Alexander Ziller,Moritz Knolle,Daniel Rueckert*

Main category: cs.LG

TL;DR: 本文严格评估胸部X光合成中的过滤管道，发现现有过滤器有局限性，需改进过滤器设计才能用于敏感应用。


<details>
  <summary>Details</summary>
Motivation: 后验隐私过滤技术有效性未经验证，需评估其在胸部X光合成中的效果。

Method: 对胸部X光合成应用过滤管道进行严格评估。

Result: 当前过滤器特异性和一致性有限，仅对真实图像有高敏感性，无法可靠检测训练数据生成的近似重复样本。

Conclusion: 在敏感应用中使用这些方法前，过滤器设计需大幅改进。

Abstract: The generation of privacy-preserving synthetic datasets is a promising avenue
for overcoming data scarcity in medical AI research. Post-hoc privacy filtering
techniques, designed to remove samples containing personally identifiable
information, have recently been proposed as a solution. However, their
effectiveness remains largely unverified. This work presents a rigorous
evaluation of a filtering pipeline applied to chest X-ray synthesis. Contrary
to claims from the original publications, our results demonstrate that current
filters exhibit limited specificity and consistency, achieving high sensitivity
only for real images while failing to reliably detect near-duplicates generated
from training data. These results demonstrate a critical limitation of post-hoc
filtering: rather than effectively safeguarding patient privacy, these methods
may provide a false sense of security while leaving unacceptable levels of
patient information exposed. We conclude that substantial advances in filter
design are needed before these methods can be confidently deployed in sensitive
applications.

</details>


### [179] [Rethinking the shape convention of an MLP](https://arxiv.org/abs/2510.01796)
*Meng-Hsi Chen,Yu-Ang Lee,Feng-Ting Liao,Da-shan Shiu*

Main category: cs.LG

TL;DR: 提出宽-窄-宽的Hourglass MLP块，随机初始化投影并评估其在图像数据集生成任务上的表现，结果显示优于传统设计，建议重新考虑现代架构中跳跃连接的位置。


<details>
  <summary>Details</summary>
Motivation: 挑战传统多层感知机（MLP）的窄-宽-窄设计，探索更优架构。

Method: 提出Hourglass MLP块，随机初始化投影，在图像数据集上进行生成任务，通过系统架构搜索表征性能 - 参数帕累托前沿。

Result: Hourglass架构在帕累托前沿上表现优于传统设计，参数预算增加时，最优配置有独特的缩放模式。

Conclusion: 建议重新考虑现代架构中跳跃连接的位置，应用可能扩展到Transformer和其他残差网络。

Abstract: Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow
design where skip connections operate at the input/output dimensions while
processing occurs in expanded hidden spaces. We challenge this convention by
proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections
operate at expanded dimensions while residual computation flows through narrow
bottlenecks. This inversion leverages higher-dimensional spaces for incremental
refinement while maintaining computational efficiency through parameter-matched
designs. Implementing Hourglass MLPs requires an initial projection to lift
input signals to expanded dimensions. We propose that this projection can
remain fixed at random initialization throughout training, enabling efficient
training and inference implementations. We evaluate both architectures on
generative tasks over popular image datasets, characterizing
performance-parameter Pareto frontiers through systematic architectural search.
Results show that Hourglass architectures consistently achieve superior Pareto
frontiers compared to conventional designs. As parameter budgets increase,
optimal Hourglass configurations favor deeper networks with wider skip
connections and narrower bottlenecks-a scaling pattern distinct from
conventional MLPs. Our findings suggest reconsidering skip connection placement
in modern architectures, with potential applications extending to Transformers
and other residual networks.

</details>


### [180] [Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction](https://arxiv.org/abs/2510.01817)
*Adam Filipek*

Main category: cs.LG

TL;DR: 提出Sparse Query Attention (SQA)架构，通过减少查询头数量降低注意力机制计算复杂度，在长序列基准测试中实现显著吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: Transformer架构中MHA机制计算复杂度高，现有方法未减少注意力分数计算的浮点运算数量。

Method: 引入SQA架构，减少查询头数量，给出理论基础、数学公式和架构变体。

Result: 在长序列上的基准测试显示，SQA在计算密集场景中吞吐量提升达3倍，小规模实验对模型质量影响小。

Conclusion: SQA有潜力成为构建更高效可扩展模型的有力工具。

Abstract: The Transformer architecture, underpinned by the Multi-Head Attention (MHA)
mechanism, has become the de facto standard for state-of-the-art models in
artificial intelligence. However, the quadratic computational complexity of MHA
with respect to sequence length presents a significant barrier to scaling,
particularly for applications involving long contexts. Prevailing solutions,
such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have
effectively addressed the memory bandwidth bottleneck that dominates
autoregressive inference latency by sharing Key and Value projections. While
highly successful, these methods do not reduce the fundamental number of
floating-point operations (FLOPs) required for the attention score computation,
which remains a critical bottleneck for training and full-sequence processing.
This paper introduces Sparse Query Attention (SQA), a novel attention
architecture that pursues an alternative and complementary optimization path.
Instead of reducing Key/Value heads, SQA reduces the number of Query heads.
This architectural modification directly decreases the computational complexity
of the attention mechanism by a factor proportional to the reduction in query
heads, thereby lowering the overall FLOPs. This work presents the theoretical
foundation of SQA, its mathematical formulation, and a family of architectural
variants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate
that SQA can achieve significant throughput improvements of up to 3x in
computation-bound scenarios such as model pre-training, fine-tuning, and
encoder-based tasks, with only a minimal impact on model quality in preliminary
smallscale experiments. SQA was discovered serendipitously during the
development of the upcoming Reactive Transformer architecture, suggesting its
potential as a powerful tool for building more efficient and scalable models

</details>


### [181] [Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning](https://arxiv.org/abs/2510.01824)
*Olivier Goudet,Quentin Suire,Adrien Goëffon,Frédéric Saubion,Sylvain Lamprier*

Main category: cs.LG

TL;DR: 提出用于黑盒组合优化的顺序不变强化学习框架，表现出色。


<details>
  <summary>Details</summary>
Motivation: 经典分布估计算法学习显式变量依赖图成本高且难以有效捕捉复杂交互。

Method: 参数化多元自回归生成模型，训练时采样随机生成顺序，采用广义强化策略优化。

Result: 在多种基准算法和不同规模问题实例中常取得最佳性能，避免灾难性失败。

Conclusion: 所提顺序不变强化学习框架有效，能提高样本效率。

Abstract: We introduce an order-invariant reinforcement learning framework for
black-box combinatorial optimization. Classical estimation-of-distribution
algorithms (EDAs) often rely on learning explicit variable dependency graphs,
which can be costly and fail to capture complex interactions efficiently. In
contrast, we parameterize a multivariate autoregressive generative model
trained without a fixed variable ordering. By sampling random generation orders
during training - a form of information-preserving dropout - the model is
encouraged to be invariant to variable order, promoting search-space diversity
and shaping the model to focus on the most relevant variable dependencies,
improving sample efficiency. We adapt Generalized Reinforcement Policy
Optimization (GRPO) to this setting, providing stable policy-gradient updates
from scale-invariant advantages. Across a wide range of benchmark algorithms
and problem instances of varying sizes, our method frequently achieves the best
performance and consistently avoids catastrophic failures.

</details>


### [182] [Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model Selection and Benchmarking for Tabular datasets](https://arxiv.org/abs/2510.01842)
*Yannis Belkhiter,Seshu Tirupathi,Giulio Zizzo,Sachin Sharma,John D. Kelleher*

Main category: cs.LG

TL;DR: 本文结合AutoML与预 hoc模型选择，用传统模型和LLM代理减少AutoML搜索空间，应用于AWS AutoGluon数据集，降低计算开销并选出最优模型。


<details>
  <summary>Details</summary>
Motivation: 现有AutoML后 hoc模型选择方法常依赖穷举超参数搜索，而预 hoc预测有潜力但研究不足，因此探索二者结合。

Method: 利用传统模型和LLM代理，依靠数据集描述和统计信息，减少AutoML搜索空间，并应用于AWS AutoGluon投资组合数据集。

Result: 提出的方法应用于数据集后，显著降低了计算开销。

Conclusion: 该方法使AutoML工作流程转变，能在降低计算开销的同时为给定数据集选出最佳模型。

Abstract: The field of AutoML has made remarkable progress in post-hoc model selection,
with libraries capable of automatically identifying the most performing models
for a given dataset. Nevertheless, these methods often rely on exhaustive
hyperparameter searches, where methods automatically train and test different
types of models on the target dataset. Contrastingly, pre-hoc prediction
emerges as a promising alternative, capable of bypassing exhaustive search
through intelligent pre-selection of models. Despite its potential, pre-hoc
prediction remains under-explored in the literature. This paper explores the
intersection of AutoML and pre-hoc model selection by leveraging traditional
models and Large Language Model (LLM) agents to reduce the search space of
AutoML libraries. By relying on dataset descriptions and statistical
information, we reduce the AutoML search space. Our methodology is applied to
the AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark
containing 175 tabular classification datasets available on OpenML. The
proposed approach offers a shift in AutoML workflows, significantly reducing
computational overhead, while still selecting the best model for the given
dataset.

</details>


### [183] [Learning Representations Through Contrastive Neural Model Checking](https://arxiv.org/abs/2510.01853)
*Vladimir Krsmanovic,Matthias Cosler,Mohamed Ghanem,Bernd Finkbeiner*

Main category: cs.LG

TL;DR: 介绍了一种新的对比神经模型检查方法CNML，它在检索任务上表现出色，学习的表示能迁移到下游任务。


<details>
  <summary>Details</summary>
Motivation: 形式验证中表示学习未得到充分探索，需要新方法利用模型检查任务学习对齐表示。

Method: 提出Contrastive Neural Model Checking (CNML)方法，通过自监督对比目标将逻辑规范和系统嵌入共享潜在空间。

Result: 在行业启发的检索任务中，CNML在跨模态和模态内设置中显著优于算法和神经基线，学习的表示能有效迁移到下游任务并推广到更复杂公式。

Conclusion: 模型检查可作为形式语言学习表示的目标。

Abstract: Model checking is a key technique for verifying safety-critical systems
against formal specifications, where recent applications of deep learning have
shown promise. However, while ubiquitous for vision and language domains,
representation learning remains underexplored in formal verification. We
introduce Contrastive Neural Model Checking (CNML), a novel method that
leverages the model checking task as a guiding signal for learning aligned
representations. CNML jointly embeds logical specifications and systems into a
shared latent space through a self-supervised contrastive objective. On
industry-inspired retrieval tasks, CNML considerably outperforms both
algorithmic and neural baselines in cross-modal and intra-modal settings.We
further show that the learned representations effectively transfer to
downstream tasks and generalize to more complex formulas. These findings
demonstrate that model checking can serve as an objective for learning
representations for formal languages.

</details>


### [184] [Explicit Discovery of Nonlinear Symmetries from Dynamic Data](https://arxiv.org/abs/2510.01855)
*Lexiang Hu,Yikang Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出LieNLSD方法，能确定含非线性项的无穷小生成元数量及表达式，在多个任务上优于现有方法并提升PDE求解器精度。


<details>
  <summary>Details</summary>
Motivation: 以往对称性发现方法多局限于线性对称性，近期发现非线性对称性的尝试未能明确得到李代数子空间。

Method: 指定无穷小群作用的函数库并求解其系数矩阵，将数据中心差分和训练好的神经网络雅可比矩阵代入无穷小准则得到线性方程组，用SVD求解。

Result: LieNLSD在顶夸克标记和一系列动态系统上展现出定性优势，将神经PDE求解器的长期滚动精度提高20%以上，可用于指导数据增强。

Conclusion: LieNLSD是一种有效的非线性对称性发现方法，有良好应用前景。

Abstract: Symmetry is widely applied in problems such as the design of equivariant
networks and the discovery of governing equations, but in complex scenarios, it
is not known in advance. Most previous symmetry discovery methods are limited
to linear symmetries, and recent attempts to discover nonlinear symmetries fail
to explicitly get the Lie algebra subspace. In this paper, we propose LieNLSD,
which is, to our knowledge, the first method capable of determining the number
of infinitesimal generators with nonlinear terms and their explicit
expressions. We specify a function library for the infinitesimal group action
and aim to solve for its coefficient matrix, proving that its prolongation
formula for differential equations, which governs dynamic data, is also linear
with respect to the coefficient matrix. By substituting the central differences
of the data and the Jacobian matrix of the trained neural network into the
infinitesimal criterion, we get a system of linear equations for the
coefficient matrix, which can then be solved using SVD. On top quark tagging
and a series of dynamic systems, LieNLSD shows qualitative advantages over
existing methods and improves the long rollout accuracy of neural PDE solvers
by over 20% while applying to guide data augmentation. Code and data are
available at https://github.com/hulx2002/LieNLSD.

</details>


### [185] [Compositional meta-learning through probabilistic task inference](https://arxiv.org/abs/2510.01858)
*Jacob J. W. Bakermans,Pablo Tano,Reidar Riveland,Charles Findling,Alexandre Pouget*

Main category: cs.LG

TL;DR: 提出组合式元学习模型，结合神经网络表达能力与概率推理数据效率实现快速组合式元学习。


<details>
  <summary>Details</summary>
Motivation: 解决从最少经验学习新任务时，有效复用先前任务知识的元学习问题。

Method: 学习生成模型捕获任务家族的底层组件及其统计信息，将新任务学习转化为概率推理问题。

Result: 模型在规则学习和运动学习任务中成功恢复真实组件和统计信息，能从单个示例快速推断新解决方案。

Conclusion: 框架结合神经网络和概率推理优势，实现快速组合式元学习。

Abstract: To solve a new task from minimal experience, it is essential to effectively
reuse knowledge from previous tasks, a problem known as meta-learning.
Compositional solutions, where common elements of computation are flexibly
recombined into new configurations, are particularly well-suited for
meta-learning. Here, we propose a compositional meta-learning model that
explicitly represents tasks as structured combinations of reusable
computations. We achieve this by learning a generative model that captures the
underlying components and their statistics shared across a family of tasks.
This approach transforms learning a new task into a probabilistic inference
problem, which allows for finding solutions without parameter updates through
highly constrained hypothesis testing. Our model successfully recovers ground
truth components and statistics in rule learning and motor learning tasks. We
then demonstrate its ability to quickly infer new solutions from just single
examples. Together, our framework joins the expressivity of neural networks
with the data-efficiency of probabilistic inference to achieve rapid
compositional meta-learning.

</details>


### [186] [Universal Dynamic Regret and Constraint Violation Bounds for Constrained Online Convex Optimization](https://arxiv.org/abs/2510.01867)
*Subhamon Supantha,Abhishek Sinha*

Main category: cs.LG

TL;DR: 提出两种简单模块化结构算法，改进在线凸优化带在线对抗约束的动态遗憾和累积约束违反界。


<details>
  <summary>Details</summary>
Motivation: 研究在线凸优化框架在有在线对抗约束情况下的问题，改进现有结果。

Method: 将带约束的学习问题转化为具有特殊构造替代成本函数的标准在线凸优化问题。

Result: 得到通用的动态遗憾和累积约束违反界，结果适用于成本和约束函数由对手任意选择、约束函数无需有公共可行点的最一般情况。

Conclusion: 所提出的算法在在线凸优化带在线对抗约束问题上有较好效果，改进了现有结果。

Abstract: We consider a generalization of the celebrated Online Convex Optimization
(OCO) framework with online adversarial constraints. We present two algorithms
having simple modular structures that yield universal dynamic regret and
cumulative constraint violation bounds, improving upon the state-of-the-art
results. Our results hold in the most general case when both the cost and
constraint functions are chosen arbitrarily by an adversary, and the constraint
functions need not contain any common feasible point. The results are
established by reducing the constrained learning problem to an instance of the
standard OCO problem with specially constructed surrogate cost functions.

</details>


### [187] [Randomized Gradient Subspaces for Efficient Large Language Model Training](https://arxiv.org/abs/2510.01878)
*Sahar Rajabi,Nayeema Nonta,Samanvay Vajpayee,Sirisha Rambhatla*

Main category: cs.LG

TL;DR: 分析梯度空间动态，提出随机算法GrassWalk和GrassJump，实现内存节省并提升性能。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型时优化器状态内存占用大，现有工作虽有缓解但需进一步优化，通过分析梯度空间动态获取新优化思路。

Method: 分析梯度空间及其子空间动态，基于此提出GrassWalk和GrassJump随机算法。

Result: 实现了最先进的内存节省，在LLaMA - 1B和LLaMA - 7B预训练中提升了性能。

Conclusion: 提出的随机算法GrassWalk和GrassJump能有效解决大语言模型训练中的内存瓶颈问题并提升性能。

Abstract: Training large language models (LLMs) is often bottlenecked by extreme memory
demands, with optimizer states dominating the footprint. Recent works mitigates
this cost by projecting gradients into low-dimensional subspaces using
sophisticated update strategies. In this paper, we analyze the dynamics of
gradient space and its underlying subspaces. We find that while a small
subspace captures most gradient energy, a significant portion still resides in
the residual bulk; moreover, the influence of the core subspace diminishes over
time and in deeper layers. We also observe that the gradient space exhibits
near-flat curvature, calling for algorithms that explicitly account for this
geometry. Motivated by these insights, we introduce a suite of randomized
algorithms, GrassWalk and GrassJump, which exploit subspace and achieve
state-of-the-art memory savings while improving performance on LLaMA-1B and
LLaMA-7B pretraining.

</details>


### [188] [Multi-marginal temporal Schrödinger Bridge Matching for video generation from unpaired data](https://arxiv.org/abs/2510.01894)
*Thomas Gravier,Thomas Boyer,Auguste Genovesio*

Main category: cs.LG

TL;DR: 提出MMtSBM方法用于从非配对数据生成视频，解决现有方法在高维的扩展性问题，实验表现良好，确立多边际薛定谔桥为实用方法。


<details>
  <summary>Details</summary>
Motivation: 现有重建自然动态过程时间演化的方法在高维扩展性差且有严格假设，需改进。

Method: 提出Multi - Marginal temporal Schrödinger Bridge Matching (MMtSBM)，以新的因式分解方式将迭代马尔可夫拟合算法推广到多个边际。

Result: MMtSBM在玩具示例保留理论性质，在高维转录组轨迹推断等真实数据集达最优，首次在高维图像中恢复耦合和动态。

Conclusion: 多边际薛定谔桥是从静态数据恢复隐藏动态的实用且有原则的方法。

Abstract: Many natural dynamic processes -- such as in vivo cellular differentiation or
disease progression -- can only be observed through the lens of static sample
snapshots. While challenging, reconstructing their temporal evolution to
decipher underlying dynamic properties is of major interest to scientific
research. Existing approaches enable data transport along a temporal axis but
are poorly scalable in high dimension and require restrictive assumptions to be
met. To address these issues, we propose \textit{\textbf{Multi-Marginal
temporal Schr\"odinger Bridge Matching}} (\textbf{MMtSBM}) \textit{for video
generation from unpaired data}, extending the theoretical guarantees and
empirical efficiency of Diffusion Schr\"odinger Bridge Matching
(arXiv:archive/2303.16852) by deriving the Iterative Markovian Fitting
algorithm to multiple marginals in a novel factorized fashion. Experiments show
that MMtSBM retains theoretical properties on toy examples, achieves
state-of-the-art performance on real world datasets such as transcriptomic
trajectory inference in 100 dimensions, and for the first time recovers
couplings and dynamics in very high dimensional image settings. Our work
establishes multi-marginal Schr\"odinger bridges as a practical and principled
approach for recovering hidden dynamics from static data.

</details>


### [189] [Multimodal Foundation Models for Early Disease Detection](https://arxiv.org/abs/2510.01899)
*Md Talha Mohsin,Ismail Abdulrashid*

Main category: cs.LG

TL;DR: 本文提出多模态基础模型整合医疗数据，通过实验验证，助力精准诊断。


<details>
  <summary>Details</summary>
Motivation: 传统诊断模型孤立分析数据源，难以识别跨模态关联，影响早期疾病诊断。

Method: 使用基于注意力的Transformer框架，用专用编码器将各模态数据映射到共享潜空间，通过多头注意力和残差归一化组合，进行多任务预训练。

Result: 提出涵盖数据治理和模型管理工具的实验策略，在肿瘤学、心脏病学和神经学基准数据集上测试早期检测任务。

Conclusion: 该方法朝着单一基础模型的精准诊断迈进，可提高预测准确性，辅助医生决策。

Abstract: Healthcare generates diverse streams of data, including electronic health
records (EHR), medical imaging, genetics, and ongoing monitoring from wearable
devices. Traditional diagnostic models frequently analyze these sources in
isolation, which constrains their capacity to identify cross-modal correlations
essential for early disease diagnosis. Our research presents a multimodal
foundation model that consolidates diverse patient data through an
attention-based transformer framework. At first, dedicated encoders put each
modality into a shared latent space. Then, they combine them using multi-head
attention and residual normalization. The architecture is made for pretraining
on many tasks, which makes it easy to adapt to new diseases and datasets with
little extra work. We provide an experimental strategy that uses benchmark
datasets in oncology, cardiology, and neurology, with the goal of testing early
detection tasks. The framework includes data governance and model management
tools in addition to technological performance to improve transparency,
reliability, and clinical interpretability. The suggested method works toward a
single foundation model for precision diagnostics, which could improve the
accuracy of predictions and help doctors make decisions.

</details>


### [190] [A Methodology for Transparent Logic-Based Classification Using a Multi-Task Convolutional Tsetlin Machine](https://arxiv.org/abs/2510.01906)
*Mayur Kishor Shende,Ole-Christoffer Granmo,Runar Helin,Vladimir I. Zadorozhny,Rishad Shafik*

Main category: cs.LG

TL;DR: 本文探索Tsetlin Machine (TM)架构用于大规模多通道图像分类，提出生成局部和全局解释的方法，在MNIST和CelebA数据集评估，表明TM有竞争力且具可解释性。


<details>
  <summary>Details</summary>
Motivation: 探索TM架构在大规模多通道（RGB）图像分类的适用性。

Method: 提出生成局部解释和全局类表示的方法。

Result: 在MNIST上准确率达98.5%，在CelebA上F1分数达86.56%（ResNet50为88.07%）。

Conclusion: TM在大规模复杂训练环境有竞争力且保持可解释性，有助于理解TM子句并为应用于更多复杂多样数据集提供见解。

Abstract: The Tsetlin Machine (TM) is a novel machine learning paradigm that employs
finite-state automata for learning and utilizes propositional logic to
represent patterns. Due to its simplistic approach, TMs are inherently more
interpretable than learning algorithms based on Neural Networks. The
Convolutional TM has shown comparable performance on various datasets such as
MNIST, K-MNIST, F-MNIST and CIFAR-2. In this paper, we explore the
applicability of the TM architecture for large-scale multi-channel (RGB) image
classification. We propose a methodology to generate both local interpretations
and global class representations. The local interpretations can be used to
explain the model predictions while the global class representations aggregate
important patterns for each class. These interpretations summarize the
knowledge captured by the convolutional clauses, which can be visualized as
images. We evaluate our methods on MNIST and CelebA datasets, using models that
achieve 98.5\% accuracy on MNIST and 86.56\% F1-score on CelebA (compared to
88.07\% for ResNet50) respectively. We show that the TM performs competitively
to this deep learning model while maintaining its interpretability, even in
large-scale complex training environments. This contributes to a better
understanding of TM clauses and provides insights into how these models can be
applied to more complex and diverse datasets.

</details>


### [191] [Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement](https://arxiv.org/abs/2510.01910)
*Zhaoyan Wang,Zheng Gao,Arogya Kharel,In-Young Ko*

Main category: cs.LG

TL;DR: 本文针对图神经网络在含缺陷图数据上性能不佳的问题，对比传统方法和基于大语言模型的图框架，提出RoGRAD框架，实验表明其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对图原生和大语言模型增强方法在复合缺陷下表现的系统理解，未全面比较传统方法和基于大语言模型的图框架。

Method: 开展实证研究对比两种方法，提出RoGRAD框架，利用检索增强生成进行迭代式图对比学习。

Result: RoGRAD框架在实验中优于传统GNN和基于大语言模型增强的基线，平均提升达82.43%。

Conclusion: RoGRAD框架有效提升了图数据学习的性能，将大语言模型对图的增强从静态信号注入转变为动态细化。

Abstract: Graph Neural Networks (GNNs) are widely adopted in Web-related applications,
serving as a core technique for learning from graph-structured data, such as
text-attributed graphs. Yet in real-world scenarios, such graphs exhibit
deficiencies that substantially undermine GNN performance. While prior
GNN-based augmentation studies have explored robustness against individual
imperfections, a systematic understanding of how graph-native and Large
Language Models (LLMs) enhanced methods behave under compound deficiencies is
still missing. Specifically, there has been no comprehensive investigation
comparing conventional approaches and recent LLM-on-graph frameworks, leaving
their merits unclear. To fill this gap, we conduct the first empirical study
that benchmarks these two lines of methods across diverse graph deficiencies,
revealing overlooked vulnerabilities and challenging the assumption that LLM
augmentation is consistently superior. Building on empirical findings, we
propose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement
(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is
the first iterative paradigm that leverages Retrieval-Augmented Generation
(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,
diverse augmentations and enforcing discriminative representations through
iterative graph contrastive learning. It transforms LLM augmentation for graphs
from static signal injection into dynamic refinement. Extensive experiments
demonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced
baselines, achieving up to 82.43% average improvement.

</details>


### [192] [StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold](https://arxiv.org/abs/2510.01938)
*Zhizhong Li,Sina Sajadmanesh,Jingtao Li,Lingjuan Lyu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Low-rank adaptation (LoRA) has been widely adopted as a parameter-efficient
technique for fine-tuning large-scale pre-trained models. However, it still
lags behind full fine-tuning in performance, partly due to its insufficient
exploitation of the geometric structure underlying low-rank manifolds. In this
paper, we propose a geometry-aware extension of LoRA that uses a three-factor
decomposition $U\!SV^\top$. Analogous to the structure of singular value
decomposition (SVD), it separates the adapter's input and output subspaces, $V$
and $U$, from the scaling factor $S$. Our method constrains $U$ and $V$ to lie
on the Stiefel manifold, ensuring their orthonormality throughout the training.
To optimize on the Stiefel manifold, we employ a flexible and modular geometric
optimization design that converts any Euclidean optimizer to a Riemannian one.
It enables efficient subspace learning while remaining compatible with existing
fine-tuning pipelines. Empirical results across a wide range of downstream
tasks, including commonsense reasoning, math and code generation, image
classification, and image generation, demonstrate the superior performance of
our approach against the recent state-of-the-art variants of LoRA. Code is
available at https://github.com/SonyResearch/stella.

</details>


### [193] [Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2510.01970)
*Yuanyuan Yao,Yuhan Shi,Lu Chen,Ziquan Fang,Yunjun Gao,Leong Hou U,Yushuai Li,Tianyi Li*

Main category: cs.LG

TL;DR: 提出名为Moon的有监督模态转换多元时间序列异常检测框架，在多数据集实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多元时间序列异常检测方法存在依赖误差阈值不准确、未充分利用异常标签、无法捕捉局部关系、计算成本高和标签数据稀缺等问题。

Method: 引入MV - MTF技术将数值时间序列数据转换为图像表示；使用Multimodal - CNN通过参数共享的特征融合模型集成数值和图像数据；采用基于SHAP的异常解释器识别异常关键变量。

Result: 在六个真实世界的MTS数据集上的实验表明，Moon在效率上比六种最先进的方法高出93%，在准确性上高出4%，在解释性能上高出10.8%。

Conclusion: Moon框架能提高异常检测的效率和准确性，并提供详细的异常分析报告。

Abstract: Multivariate time series (MTS) anomaly detection identifies abnormal patterns
where each timestamp contains multiple variables. Existing MTS anomaly
detection methods fall into three categories: reconstruction-based,
prediction-based, and classifier-based methods. However, these methods face two
key challenges: (1) Unsupervised learning methods, such as reconstruction-based
and prediction-based methods, rely on error thresholds, which can lead to
inaccuracies; (2) Semi-supervised methods mainly model normal data and often
underuse anomaly labels, limiting detection of subtle anomalies;(3) Supervised
learning methods, such as classifier-based approaches, often fail to capture
local relationships, incur high computational costs, and are constrained by the
scarcity of labeled data. To address these limitations, we propose Moon, a
supervised modality conversion-based multivariate time series anomaly detection
framework. Moon enhances the efficiency and accuracy of anomaly detection while
providing detailed anomaly analysis reports. First, Moon introduces a novel
multivariate Markov Transition Field (MV-MTF) technique to convert numeric time
series data into image representations, capturing relationships across
variables and timestamps. Since numeric data retains unique patterns that
cannot be fully captured by image conversion alone, Moon employs a
Multimodal-CNN to integrate numeric and image data through a feature fusion
model with parameter sharing, enhancing training efficiency. Finally, a
SHAP-based anomaly explainer identifies key variables contributing to
anomalies, improving interpretability. Extensive experiments on six real-world
MTS datasets demonstrate that Moon outperforms six state-of-the-art methods by
up to 93% in efficiency, 4% in accuracy and, 10.8% in interpretation
performance.

</details>


### [194] [$\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models](https://arxiv.org/abs/2510.01982)
*Yujie Zhou,Pengyang Ling,Jiazi Bu,Yibin Wang,Yuhang Zang,Jiaqi Wang,Li Niu,Guangtao Zhai*

Main category: cs.LG

TL;DR: 提出Granular - GRPO ($	ext{G}^2$RPO )框架解决流模型强化学习中偏好对齐问题，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将在线强化学习集成到扩散和流模型时，因稀疏和狭窄的奖励信号导致偏好对齐次优。

Method: 提出$	ext{G}^2$RPO框架，引入Singular Stochastic Sampling策略支持逐步随机探索，使用Multi - Granularity Advantage Integration模块消除固定粒度去噪的偏差。

Result: 在各种奖励模型的实验中，$	ext{G}^2$RPO显著优于现有的基于流的GRPO基线。

Conclusion: $	ext{G}^2$RPO框架在流模型强化学习中有效且鲁棒。

Abstract: The integration of online reinforcement learning (RL) into diffusion and flow
models has recently emerged as a promising approach for aligning generative
models with human preferences. Stochastic sampling via Stochastic Differential
Equations (SDE) is employed during the denoising process to generate diverse
denoising directions for RL exploration. While existing methods effectively
explore potential high-value samples, they suffer from sub-optimal preference
alignment due to sparse and narrow reward signals. To address these challenges,
we propose a novel Granular-GRPO ($\text{G}^2$RPO ) framework that achieves
precise and comprehensive reward assessments of sampling directions in
reinforcement learning of flow models. Specifically, a Singular Stochastic
Sampling strategy is introduced to support step-wise stochastic exploration
while enforcing a high correlation between the reward and the injected noise,
thereby facilitating a faithful reward for each SDE perturbation. Concurrently,
to eliminate the bias inherent in fixed-granularity denoising, we introduce a
Multi-Granularity Advantage Integration module that aggregates advantages
computed at multiple diffusion scales, producing a more comprehensive and
robust evaluation of the sampling directions. Experiments conducted on various
reward models, including both in-domain and out-of-domain evaluations,
demonstrate that our $\text{G}^2$RPO significantly outperforms existing
flow-based GRPO baselines,highlighting its effectiveness and robustness.

</details>


### [195] [Private Federated Multiclass Post-hoc Calibration](https://arxiv.org/abs/2510.01987)
*Samuel Maddock,Graham Cormode,Carsten Maple*

Main category: cs.LG

TL;DR: 本文将事后模型校准技术集成到联邦学习中，研究不同场景下校准准确性并提出缓解策略，指出不同场景下的最佳方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在医疗和金融等需强校准的领域应用广泛，但联邦私有校准被忽视，需要解决相关问题。

Method: 将传统集中式校准方法转移到联邦环境，针对强客户端异质性定义新方法，研究联邦和用户级差分隐私两种场景。

Result: 展示了联邦和差分隐私对校准准确性的影响。

Conclusion: 联邦温度缩放对差分隐私联邦学习效果最佳，加权分箱方法在无需差分隐私时最佳。

Abstract: Calibrating machine learning models so that predicted probabilities better
reflect the true outcome frequencies is crucial for reliable decision-making
across many applications. In Federated Learning (FL), the goal is to train a
global model on data which is distributed across multiple clients and cannot be
centralized due to privacy concerns. FL is applied in key areas such as
healthcare and finance where calibration is strongly required, yet federated
private calibration has been largely overlooked. This work introduces the
integration of post-hoc model calibration techniques within FL. Specifically,
we transfer traditional centralized calibration methods such as histogram
binning and temperature scaling into federated environments and define new
methods to operate them under strong client heterogeneity. We study (1) a
federated setting and (2) a user-level Differential Privacy (DP) setting and
demonstrate how both federation and DP impacts calibration accuracy. We propose
strategies to mitigate degradation commonly observed under heterogeneity and
our findings highlight that our federated temperature scaling works best for
DP-FL whereas our weighted binning approach is best when DP is not required.

</details>


### [196] [PepCompass: Navigating peptide embedding spaces using Riemannian Geometry](https://arxiv.org/abs/2510.01988)
*Marcin Możejko,Adam Bielecki,Jurand Prądzyński,Marcin Traskowski,Antoni Janowski,Karol Jurasz,Michał Kucharczyk,Hyun-Su Lee,Marcelo Der Torossian Torres,Cesar de la Fuente-Nunez,Paulina Szymczak,Michał Kmicikiewicz,Ewa Szczurek*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Antimicrobial peptide discovery is challenged by the astronomical size of
peptide space and the relative scarcity of active peptides. Generative models
provide continuous latent "maps" of peptide space, but conventionally ignore
decoder-induced geometry and rely on flat Euclidean metrics, rendering
exploration and optimization distorted and inefficient. Prior manifold-based
remedies assume fixed intrinsic dimensionality, which critically fails in
practice for peptide data. Here, we introduce PepCompass, a geometry-aware
framework for peptide exploration and optimization. At its core, we define a
Union of $\kappa$-Stable Riemannian Manifolds $\mathbb{M}^{\kappa}$, a family
of decoder-induced manifolds that captures local geometry while ensuring
computational stability. We propose two local exploration methods: Second-Order
Riemannian Brownian Efficient Sampling, which provides a convergent
second-order approximation to Riemannian Brownian motion, and Mutation
Enumeration in Tangent Space, which reinterprets tangent directions as discrete
amino-acid substitutions. Combining these yields Local Enumeration Bayesian
Optimization (LE-BO), an efficient algorithm for local activity optimization.
Finally, we introduce Potential-minimizing Geodesic Search (PoGS), which
interpolates between prototype embeddings along property-enriched geodesics,
biasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro
validation confirms the effectiveness of PepCompass: PoGS yields four novel
seeds, and subsequent optimization with LE-BO discovers 25 highly active
peptides with broad-spectrum activity, including against resistant bacterial
strains. These results demonstrate that geometry-informed exploration provides
a powerful new paradigm for antimicrobial peptide design.

</details>


### [197] [Normality Calibration in Semi-supervised Graph Anomaly Detection](https://arxiv.org/abs/2510.02014)
*Guolei Zeng,Hezhe Qiao,Guoguo Ai,Jinsong Guo,Guansong Pang*

Main category: cs.LG

TL;DR: 提出GraphNC框架解决半监督图异常检测方法过拟合问题，含ScoreDA和NormReg组件。


<details>
  <summary>Details</summary>
Motivation: 现有半监督图异常检测方法学习的正常性局限于标记正常节点，易过拟合，导致高检测误差。

Method: 提出GraphNC框架，包含ScoreDA优化异常分数，NormReg在表示空间正则化图正常性。

Result: ScoreDA使异常分数更可分，NormReg让正常节点表示更紧凑。

Conclusion: GraphNC框架能利用有标签和无标签数据，在异常分数和节点表示空间校准正常性，克服现有方法局限。

Abstract: Graph anomaly detection (GAD) has attracted growing interest for its crucial
ability to uncover irregular patterns in broad applications. Semi-supervised
GAD, which assumes a subset of annotated normal nodes available during
training, is among the most widely explored application settings. However, the
normality learned by existing semi-supervised GAD methods is limited to the
labeled normal nodes, often inclining to overfitting the given patterns. These
can lead to high detection errors, such as high false positives. To overcome
this limitation, we propose GraphNC , a graph normality calibration framework
that leverages both labeled and unlabeled data to calibrate the normality from
a teacher model (a pre-trained semi-supervised GAD model) jointly in anomaly
score and node representation spaces. GraphNC includes two main components,
anomaly score distribution alignment (ScoreDA) and perturbation-based normality
regularization (NormReg). ScoreDA optimizes the anomaly scores of our model by
aligning them with the score distribution yielded by the teacher model. Due to
accurate scores in most of the normal nodes and part of the anomaly nodes in
the teacher model, the score alignment effectively pulls the anomaly scores of
the normal and abnormal classes toward the two ends, resulting in more
separable anomaly scores. Nevertheless, there are inaccurate scores from the
teacher model. To mitigate the misleading by these scores, NormReg is designed
to regularize the graph normality in the representation space, making the
representations of normal nodes more compact by minimizing a
perturbation-guided consistency loss solely on the labeled nodes.

</details>


### [198] [FairContrast: Enhancing Fairness through Contrastive learning and Customized Augmenting Methods on Tabular Data](https://arxiv.org/abs/2510.02017)
*Aida Tayebi,Ali Khodabandeh Yalabadi,Mehdi Yazdani-Jahromi,Ozlem Ozmen Garibay*

Main category: cs.LG

TL;DR: 提出针对表格数据的对比学习框架以解决偏差问题，减少偏差且对准确率影响小，可用于下游任务。


<details>
  <summary>Details</summary>
Motivation: 随着AI融入日常生活，开发公平无偏模型至关重要，现有表格数据表示学习中的公平性问题研究不足。

Method: 引入对比学习框架，策略性选择正样本对，采用监督和自监督对比学习。

Result: 相比现有表格数据对比学习模型，显著减少偏差，在减少偏差时对准确率影响小，学习到的公平表示可用于下游任务。

Conclusion: 所提方法有效减轻偏差，能在下游任务中利用学习到的公平表示。

Abstract: As AI systems become more embedded in everyday life, the development of fair
and unbiased models becomes more critical. Considering the social impact of AI
systems is not merely a technical challenge but a moral imperative. As
evidenced in numerous research studies, learning fair and robust
representations has proven to be a powerful approach to effectively debiasing
algorithms and improving fairness while maintaining essential information for
prediction tasks. Representation learning frameworks, particularly those that
utilize self-supervised and contrastive learning, have demonstrated superior
robustness and generalizability across various domains. Despite the growing
interest in applying these approaches to tabular data, the issue of fairness in
these learned representations remains underexplored. In this study, we
introduce a contrastive learning framework specifically designed to address
bias and learn fair representations in tabular datasets. By strategically
selecting positive pair samples and employing supervised and self-supervised
contrastive learning, we significantly reduce bias compared to existing
state-of-the-art contrastive learning models for tabular data. Our results
demonstrate the efficacy of our approach in mitigating bias with minimum
trade-off in accuracy and leveraging the learned fair representations in
various downstream tasks.

</details>


### [199] [Mathematical Modeling and Convergence Analysis of Deep Neural Networks with Dense Layer Connectivities in Deep Learning](https://arxiv.org/abs/2510.02049)
*Jinshu Huang,Haibin Su,Xue-Cheng Tai,Chunlin Wu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In deep learning, dense layer connectivity has become a key design principle
in deep neural networks (DNNs), enabling efficient information flow and strong
performance across a range of applications. In this work, we model densely
connected DNNs mathematically and analyze their learning problems in the
deep-layer limit. For a broad applicability, we present our analysis in a
framework setting of DNNs with densely connected layers and general non-local
feature transformations (with local feature transformations as special cases)
within layers, which is called dense non-local (DNL) framework and includes
standard DenseNets and variants as special examples. In this formulation, the
densely connected networks are modeled as nonlinear integral equations, in
contrast to the ordinary differential equation viewpoint commonly adopted in
prior works. We study the associated training problems from an optimal control
perspective and prove convergence results from the network learning problem to
its continuous-time counterpart. In particular, we show the convergence of
optimal values and the subsequence convergence of minimizers, using a piecewise
linear extension and $\Gamma$-convergence analysis. Our results provide a
mathematical foundation for understanding densely connected DNNs and further
suggest that such architectures can offer stability of training deep models.

</details>


### [200] [Fine-Tuning Flow Matching via Maximum Likelihood Estimation of Reconstructions](https://arxiv.org/abs/2510.02081)
*Zhaoyi Li,Jingtao Ding,Yong Li,Shihua Li*

Main category: cs.LG

TL;DR: 本文指出Flow Matching (FM)算法存在训练推理差距及追求预设路径带来的问题，提出通过最大似然估计微调FM的方法，实验验证该方法能提升FM推理性能。


<details>
  <summary>Details</summary>
Motivation: FM算法有训练推理差距，在高精度场景问题明显，且过度追求预设路径会引入问题，需进行改进。

Method: 先理论分析FM中训练损失和推理误差的关系，再提出通过最大似然估计微调FM的方法，包含直接微调和基于残差的微调，基于残差的微调通过特定架构引入收缩特性。

Result: 在图像生成和机器人操作实验中，该方法可靠地提升了FM的推理性能。

Conclusion: 提出的微调方法能有效改善FM的推理性能。

Abstract: Flow Matching (FM) algorithm achieves remarkable results in generative tasks
especially in robotic manipulation. Building upon the foundations of diffusion
models, the simulation-free paradigm of FM enables simple and efficient
training, but inherently introduces a train-inference gap. Specifically, we
cannot assess the model's output during the training phase. In contrast, other
generative models including Variational Autoencoder (VAE), Normalizing Flow and
Generative Adversarial Networks (GANs) directly optimize on the reconstruction
loss. Such a gap is particularly evident in scenarios that demand high
precision, such as robotic manipulation. Moreover, we show that FM's
over-pursuit of straight predefined paths may introduce some serious problems
such as stiffness into the system. These motivate us to fine-tune FM via
Maximum Likelihood Estimation of reconstructions - an approach made feasible by
FM's underlying smooth ODE formulation, in contrast to the stochastic
differential equations (SDEs) used in diffusion models. This paper first
theoretically analyzes the relation between training loss and inference error
in FM. Then we propose a method of fine-tuning FM via Maximum Likelihood
Estimation of reconstructions, which includes both straightforward fine-tuning
and residual-based fine-tuning approaches. Furthermore, through specifically
designed architectures, the residual-based fine-tuning can incorporate the
contraction property into the model, which is crucial for the model's
robustness and interpretability. Experimental results in image generation and
robotic manipulation verify that our method reliably improves the inference
performance of FM.

</details>


### [201] [KAIROS: Unified Training for Universal Non-Autoregressive Time Series Forecasting](https://arxiv.org/abs/2510.02084)
*Kuiye Ding,Fanda Fan,Zheya Wang,Hongxiao Li,Yifan Wang,Lei Wang,Chunjie Luo,Jianfeng Zhan*

Main category: cs.LG

TL;DR: 介绍KAIROS非自回归时间序列预测框架，在多个基准测试有良好零样本泛化能力，成本低且凸显非自回归设计重要性。


<details>
  <summary>Details</summary>
Motivation: 网络应用的时间序列预测需更快响应以支持实时决策。

Method: 提出KAIROS非自回归时间序列预测框架，直接对段级多峰分布建模。

Result: 在六个广泛使用的基准测试中展现出强零样本泛化能力，预测性能与同规模的先进基础模型相当，但推理成本低。

Conclusion: 非自回归设计是时间序列基础模型的可扩展范例。

Abstract: In the World Wide Web, reliable time series forecasts provide the
forward-looking signals that drive resource planning, cache placement, and
anomaly response, enabling platforms to operate efficiently as user behavior
and content distributions evolve. Compared with other domains, time series
forecasting for Web applications requires much faster responsiveness to support
real-time decision making. We present KAIROS, a non-autoregressive time series
forecasting framework that directly models segment-level multi-peak
distributions. Unlike autoregressive approaches, KAIROS avoids error
accumulation and achieves just-in-time inference, while improving over existing
non-autoregressive models that collapse to over-smoothed predictions. Trained
on the large-scale corpus, KAIROS demonstrates strong zero-shot generalization
on six widely used benchmarks, delivering forecasting performance comparable to
state-of-the-art foundation models with similar scale, at a fraction of their
inference cost. Beyond empirical results, KAIROS highlights the importance of
non-autoregressive design as a scalable paradigm for foundation models in time
series.

</details>


### [202] [Learning Model Representations Using Publicly Available Model Hubs](https://arxiv.org/abs/2510.02096)
*Damian Falk,Konstantin Schürholt,Konstantinos Tzevelekakis,Léo Meynent,Damian Borth*

Main category: cs.LG

TL;DR: 本文提出在非结构化模型仓库上训练权重空间学习骨干网络，证明可学习高质量权重空间表示，克服了权重空间学习依赖精心构建模型库的局限。


<details>
  <summary>Details</summary>
Motivation: 权重空间学习中构建模型库需大量计算资源，限制了学习的规模和灵活性。

Method: 在如Hugging Face等非结构化模型仓库下载的任意模型上训练权重空间学习骨干网络，提出处理非结构化模型群体的新骨干网络。

Result: 在Hugging Face模型上训练的权重空间表示性能良好，常优于在实验室生成模型库上训练的骨干网络，且训练集的多样性使模型能泛化到未见数据模态。

Conclusion: 证明可在非结构化模型仓库中学习高质量权重空间表示，说明精心构建的模型库并非不可或缺。

Abstract: The weights of neural networks have emerged as a novel data modality, giving
rise to the field of weight space learning. A central challenge in this area is
that learning meaningful representations of weights typically requires large,
carefully constructed collections of trained models, typically referred to as
model zoos. These model zoos are often trained ad-hoc, requiring large
computational resources, constraining the learned weight space representations
in scale and flexibility. In this work, we drop this requirement by training a
weight space learning backbone on arbitrary models downloaded from large,
unstructured model repositories such as Hugging Face. Unlike curated model
zoos, these repositories contain highly heterogeneous models: they vary in
architecture and dataset, and are largely undocumented. To address the
methodological challenges posed by such heterogeneity, we propose a new weight
space backbone designed to handle unstructured model populations. We
demonstrate that weight space representations trained on models from Hugging
Face achieve strong performance, often outperforming backbones trained on
laboratory-generated model zoos. Finally, we show that the diversity of the
model weights in our training set allows our weight space model to generalize
to unseen data modalities. By demonstrating that high-quality weight space
representations can be learned in the wild, we show that curated model zoos are
not indispensable, thereby overcoming a strong limitation currently faced by
the weight space learning community.

</details>


### [203] [PENEX: AdaBoost-Inspired Neural Network Regularization](https://arxiv.org/abs/2510.02107)
*Klaus-Rudolf Kladny,Bernhard Schölkopf,Michael Muehlebach*

Main category: cs.LG

TL;DR: 本文提出新的多类指数损失函数PENEX，经实验和理论证明其能隐式最大化数据点间隔，在多任务中有正则化效果，可用于深度神经网络训练和微调。


<details>
  <summary>Details</summary>
Motivation: 现有AdaBoost的指数损失虽惩罚误标记数据点更严重，但现有多类指数损失公式不便于一阶方法优化，需新的公式。

Method: 引入新的多类指数损失函数PENEX，通过实验和理论证明其性质。

Result: PENEX能隐式最大化数据点间隔，梯度增量可隐式参数化弱学习器，在计算机视觉和语言任务中有更好正则化效果。

Conclusion: PENEX有潜力作为受AdaBoost启发的替代方法用于深度神经网络有效训练和微调。

Abstract: AdaBoost sequentially fits so-called weak learners to minimize an exponential
loss, which penalizes mislabeled data points more severely than other loss
functions like cross-entropy. Paradoxically, AdaBoost generalizes well in
practice as the number of weak learners grows. In the present work, we
introduce Penalized Exponential Loss (PENEX), a new formulation of the
multi-class exponential loss that is theoretically grounded and, in contrast to
the existing formulation, amenable to optimization via first-order methods. We
demonstrate both empirically and theoretically that PENEX implicitly maximizes
margins of data points. Also, we show that gradient increments on PENEX
implicitly parameterize weak learners in the boosting framework. Across
computer vision and language tasks, we show that PENEX exhibits a regularizing
effect often better than established methods with similar computational cost.
Our results highlight PENEX's potential as an AdaBoost-inspired alternative for
effective training and fine-tuning of deep neural networks.

</details>


### [204] [Hybrid Deep Learning Modeling Approach to Predict Natural Gas Consumption of Home Subscribers on Limited Data](https://arxiv.org/abs/2510.02115)
*Milad Firoozeh,Nader Dashti,Mohammad Ali Hatefi*

Main category: cs.LG

TL;DR: 本文用机器学习模型分析和预测伊朗赞詹省居民天然气消耗，混合BiLSTM - XGBoost模型表现最佳，强调考虑地理和气候因素对预测的重要性。


<details>
  <summary>Details</summary>
Motivation: 伊朗因人口和能源消耗增加，冬季面临天然气压力下降和供应中断问题，需控制居民部门天然气消耗，因此要分析和预测居民天然气消耗。

Method: 使用LSTM、GRU和混合BiLSTM - XGBoost模型，基于2017 - 2022年六年的天然气消耗和气象数据进行训练和评估。

Result: 混合BiLSTM - XGBoost模型在准确性上优于其他模型，RMSE、MAPE和MPE值更低，在数据有限时表现稳健。

Conclusion: 机器学习方法尤其是混合模型可有效管理和预测天然气消耗，有助于更高效的资源管理和减少季节性短缺，预测建模应纳入地理和气候因素。

Abstract: Today, natural gas, as a clean fuel and the best alternative to crude oil,
covers a significant part of global demand. Iran is one of the largest
countries with energy resources and in terms of gas is the second-largest
country in the world. But, due to the increase in population and energy
consumption, it faces problems such as pressure drops and gas outages yearly in
cold seasons and therefore it is necessary to control gas consumption,
especially in the residential sector, which has the largest share in Iran. This
study aims to analyze and predict gas consumption for residential customers in
Zanjan province, Iran, using machine learning models, including LSTM, GRU, and
a hybrid BiLSTM-XGBoost model. The dataset consists of gas consumption and
meteorology data collected over six years, from 2017 to 2022. The models were
trained and evaluated based on their ability to accurately predict consumption
patterns. The results indicate that the hybrid BiLSTM-XGBoost model
outperformed the other models in terms of accuracy, with lower Root Mean
Squared Error (RMSE), Mean Absolute Percentage Error (MAPE) values, and Mean
Percentage Error (MPE). Additionally, the Hybrid model demonstrated robust
performance, particularly in scenarios with limited data. The findings suggest
that machine learning approaches, particularly hybrid models, can be
effectively utilized to manage and predict gas consumption, contributing to
more efficient resource management and reducing seasonal shortages. This study
highlights the importance of incorporating geographical and climatic factors in
predictive modeling, as these significantly influence gas usage across
different regions.

</details>


### [205] [DAG DECORation: Continuous Optimization for Structure Learning under Hidden Confounding](https://arxiv.org/abs/2510.02117)
*Samhita Pal,James O'quinn,Kaveh Aryan,Heather Pua,James P. Long,Amir Asiaee*

Main category: cs.LG

TL;DR: 提出DECOR估计器用于线性高斯SEM的结构学习，理论给出参数可识别条件，在合成基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有连续方法在误差独立时表现好，去混淆优先管道依赖特定结构或非线性，需新方法处理潜在混淆下的线性高斯SEM结构学习。

Method: 提出基于单一似然且完全可微的DECOR估计器，交替进行平滑无环图更新和凸噪声更新，可包含轻弓互补惩罚或事后调和步骤。

Result: 理论给出全局参数可识别的简单充分条件，在合成基准测试中，DECOR匹配或优于强基线，在非普遍混淆时尤其稳健，普遍混淆时也有竞争力。

Conclusion: DECOR是处理潜在混淆下线性高斯SEM结构学习的有效方法。

Abstract: We study structure learning for linear Gaussian SEMs in the presence of
latent confounding. Existing continuous methods excel when errors are
independent, while deconfounding-first pipelines rely on pervasive factor
structure or nonlinearity. We propose \textsc{DECOR}, a single likelihood-based
and fully differentiable estimator that jointly learns a DAG and a correlated
noise model. Our theory gives simple sufficient conditions for global parameter
identifiability: if the mixed graph is bow free and the noise covariance has a
uniform eigenvalue margin, then the map from $(\B,\OmegaMat)$ to the
observational covariance is injective, so both the directed structure and the
noise are uniquely determined. The estimator alternates a smooth-acyclic graph
update with a convex noise update and can include a light bow complementarity
penalty or a post hoc reconciliation step. On synthetic benchmarks that vary
confounding density, graph density, latent rank, and dimension with $n<p$,
\textsc{DECOR} matches or outperforms strong baselines and is especially robust
when confounding is non-pervasive, while remaining competitive under
pervasiveness.

</details>


### [206] [Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution reaction case study](https://arxiv.org/abs/2510.02142)
*Lena Podina,Christina Humer,Alexandre Duval,Victor Schmidt,Ali Ramlaoui,Shahana Chatterjee,Yoshua Bengio,Alex Hernandez-Garcia,David Rolnick,Félix Therrien*

Main category: cs.LG

TL;DR: 介绍Catalyst GFlowNet生成模型助力氢能存储催化剂设计，以析氢反应验证其性能，未来将拓展到析氧反应。


<details>
  <summary>Details</summary>
Motivation: 高效廉价储能对可再生能源很重要，电催化剂在氢能存储中关键，但开发高性能廉价催化剂有挑战。

Method: 引入Catalyst GFlowNet生成模型，利用机器学习预测形成和吸附能设计晶体表面作催化剂。

Result: 通过析氢反应验证模型性能，确定铂为已知最有效催化剂。

Conclusion: 该生成建模框架为加速寻找新型高效催化剂提供了有前景的途径。

Abstract: Efficient and inexpensive energy storage is essential for accelerating the
adoption of renewable energy and ensuring a stable supply, despite fluctuations
in sources such as wind and solar. Electrocatalysts play a key role in hydrogen
energy storage (HES), allowing the energy to be stored as hydrogen. However,
the development of affordable and high-performance catalysts for this process
remains a significant challenge. We introduce Catalyst GFlowNet, a generative
model that leverages machine learning-based predictors of formation and
adsorption energy to design crystal surfaces that act as efficient catalysts.
We demonstrate the performance of the model through a proof-of-concept
application to the hydrogen evolution reaction, a key reaction in HES, for
which we successfully identified platinum as the most efficient known catalyst.
In future work, we aim to extend this approach to the oxygen evolution
reaction, where current optimal catalysts are expensive metal oxides, and open
the search space to discover new materials. This generative modeling framework
offers a promising pathway for accelerating the search for novel and efficient
catalysts.

</details>


### [207] [Policy Gradient Guidance Enables Test Time Control](https://arxiv.org/abs/2510.02148)
*Jianing Qi,Hao Tang,Zhigang Zhu*

Main category: cs.LG

TL;DR: 提出Policy Gradient Guidance (PGG)将分类器无引导从扩散模型扩展到经典策略梯度方法，理论推导并实验评估，显示引导可适应标准在线策略方法。


<details>
  <summary>Details</summary>
Motivation: 将扩散模型中的分类器无引导扩展到经典策略梯度方法，实现测试时无需重新训练就能调节行为。

Method: 用无条件分支增强策略梯度，对条件和无条件分支进行插值，给出理论推导，在离散和连续控制基准上评估。

Result: 条件丢弃在简单离散任务和低样本制度中有收益，但会使连续控制不稳定，适度较大引导能提升稳定性、样本效率和可控性。

Conclusion: 引导可适应标准在线策略方法，为可控在线强化学习开辟新方向。

Abstract: We introduce Policy Gradient Guidance (PGG), a simple extension of
classifier-free guidance from diffusion models to classical policy gradient
methods. PGG augments the policy gradient with an unconditional branch and
interpolates conditional and unconditional branches, yielding a test-time
control knob that modulates behavior without retraining. We provide a
theoretical derivation showing that the additional normalization term vanishes
under advantage estimation, leading to a clean guided policy gradient update.
Empirically, we evaluate PGG on discrete and continuous control benchmarks. We
find that conditioning dropout-central to diffusion guidance-offers gains in
simple discrete tasks and low sample regimes, but dropout destabilizes
continuous control. Training with modestly larger guidance ($\gamma>1$)
consistently improves stability, sample efficiency, and controllability. Our
results show that guidance, previously confined to diffusion policies, can be
adapted to standard on-policy methods, opening new directions for controllable
online reinforcement learning.

</details>


### [208] [GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning](https://arxiv.org/abs/2510.02180)
*Silvia Sapora,Devon Hjelm,Alexander Toshev,Omar Attia,Bogdan Mazoure*

Main category: cs.LG

TL;DR: 提出GRACE方法利用大语言模型从专家轨迹逆向工程可解释的基于代码的奖励函数，在基准测试中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统逆强化学习方法得到的是难以解释和调试的“黑盒”模型，需要可解释的奖励函数。

Method: 在进化搜索中使用大语言模型从专家轨迹直接逆向工程可解释的基于代码的奖励函数GRACE。

Result: 在BabyAI和AndroidWorld基准测试中高效学习到高精度奖励，生成的奖励能带来强大策略，可在多任务设置中构建复杂奖励API。

Conclusion: GRACE方法有效，能得到可解释、可执行且性能良好的奖励函数。

Abstract: Inverse Reinforcement Learning aims to recover reward models from expert
demonstrations, but traditional methods yield "black-box" models that are
difficult to interpret and debug. In this work, we introduce GRACE (Generating
Rewards As CodE), a method for using Large Language Models within an
evolutionary search to reverse-engineer an interpretable, code-based reward
function directly from expert trajectories. The resulting reward function is
executable code that can be inspected and verified. We empirically validate
GRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns
highly accurate rewards, even in complex, multi-task settings. Further, we
demonstrate that the resulting reward leads to strong policies, compared to
both competitive Imitation Learning and online RL approaches with ground-truth
rewards. Finally, we show that GRACE is able to build complex reward APIs in
multi-task setups.

</details>


### [209] [Detection of Chagas Disease from the ECG: The George B. Moody PhysioNet Challenge 2025](https://arxiv.org/abs/2510.02202)
*Matthew A. Reyna,Zuzana Koscova,Jan Pavlus,Soheil Saghafi,James Weigle,Andoni Elola,Salman Seyedi,Kiersten Campbell,Qiao Li,Ali Bahrami Rad,Antônio H. Ribeiro,Antonio Luiz P. Ribeiro,Reza Sameni,Gari D. Clifford*

Main category: cs.LG

TL;DR: 介绍了2025年George B. Moody PhysioNet挑战赛，旨在从心电图识别恰加斯病，给出挑战赛创新点及参与情况。


<details>
  <summary>Details</summary>
Motivation: 恰加斯病血清学检测能力有限，而心电图常体现恰加斯心肌病，有必要开发从心电图识别该病的算法。

Method: 利用含患者报告和血清学检测标签的多个数据集，进行数据增强，采用考虑当地血清学检测能力的评估指标。

Result: 挑战赛有111支队伍超630名参与者，提交超1300份参赛作品。

Conclusion: 挑战赛吸引了全球学术界和工业界的多样参与，为从心电图识别恰加斯病提供了创新途径。

Abstract: Objective: Chagas disease is a parasitic infection that is endemic to South
America, Central America, and, more recently, the U.S., primarily transmitted
by insects. Chronic Chagas disease can cause cardiovascular diseases and
digestive problems. Serological testing capacities for Chagas disease are
limited, but Chagas cardiomyopathy often manifests in ECGs, providing an
opportunity to prioritize patients for testing and treatment. Approach: The
George B. Moody PhysioNet Challenge 2025 invites teams to develop algorithmic
approaches for identifying Chagas disease from electrocardiograms (ECGs). Main
results: This Challenge provides multiple innovations. First, we leveraged
several datasets with labels from patient reports and serological testing,
provided a large dataset with weak labels and smaller datasets with strong
labels. Second, we augmented the data to support model robustness and
generalizability to unseen data sources. Third, we applied an evaluation metric
that captured the local serological testing capacity for Chagas disease to
frame the machine learning problem as a triage task. Significance: Over 630
participants from 111 teams submitted over 1300 entries during the Challenge,
representing diverse approaches from academia and industry worldwide.

</details>


### [210] [Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling](https://arxiv.org/abs/2510.02206)
*Daniel Gallo Fernández*

Main category: cs.LG

TL;DR: 介绍Poolformer序列到序列模型，用循环层替代自注意力并结合池化操作，实验表明其训练加速、性能好，在音频上超现有模型，未来有更多应用方向。


<details>
  <summary>Details</summary>
Motivation: 自注意力在处理长序列时计算复杂度高，实用性受限，需要新的序列到序列模型。

Method: 引入Poolformer模型，用循环层替代自注意力，通过SkipBlocks递归定义，包含残差块、上下池化层等，并进行大量实验。

Result: 池化加速训练，改善感知指标，防止过拟合；深层处理长距离依赖，浅层处理短期特征；在原始音频上优于现有模型。

Conclusion: Poolformer模型在序列处理上有优势，未来可用于文本、视觉和多模态场景。

Abstract: Sequence-to-sequence models have become central in Artificial Intelligence,
particularly following the introduction of the transformer architecture. While
initially developed for Natural Language Processing, these models have
demonstrated utility across domains, including Computer Vision. Such models
require mechanisms to exchange information along the time dimension, typically
using recurrent or self-attention layers. However, self-attention scales
quadratically with sequence length, limiting its practicality for very long
sequences.
  We introduce Poolformer, a sequence-to-sequence model that replaces
self-attention with recurrent layers and incorporates pooling operations to
reduce sequence length. Poolformer is defined recursively using SkipBlocks,
which contain residual blocks, a down-pooling layer, a nested SkipBlock, an
up-pooling layer, and additional residual blocks. We conduct extensive
experiments to support our architectural choices.
  Our results show that pooling greatly accelerates training, improves
perceptual metrics (FID and IS), and prevents overfitting. Our experiments also
suggest that long-range dependencies are handled by deep layers, while shallow
layers take care of short-term features.
  Evaluated on raw audio, which naturally features long sequence lengths,
Poolformer outperforms state-of-the-art models such as SaShiMi and Mamba.
Future directions include applications to text and vision, as well as
multi-modal scenarios, where a Poolformer-based LLM could effectively process
dense representations of images and videos.

</details>


### [211] [StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?](https://arxiv.org/abs/2510.02209)
*Yanxu Chen,Zijun Yao,Yantao Liu,Jin Ye,Jianing Yu,Lei Hou,Juanzi Li*

Main category: cs.LG

TL;DR: 引入StockBench评估LLM代理在股票交易中的表现，多数LLM代理难超简单策略，但部分有潜力，最后开源StockBench。


<details>
  <summary>Details</summary>
Motivation: 现有金融基准主要测试静态知识，未充分捕捉交易的动态迭代特性，金融领域对LLM代理的评估未充分探索。

Method: 引入StockBench，让代理接收每日市场信号并做买卖或持有决策，用金融指标评估。

Result: 多数LLM代理难超简单的买入持有基线，但部分模型有潜力获高回报和有效管理风险。

Conclusion: 开发LLM金融代理有挑战和机遇，擅长静态金融知识不意味着有成功交易策略，开源StockBench支持研究。

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
as autonomous agents, showing promise in reasoning, tool use, and sequential
decision-making. While prior benchmarks have evaluated LLM agents in domains
such as software engineering and scientific discovery, the finance domain
remains underexplored, despite its direct relevance to economic value and
high-stakes decision-making. Existing financial benchmarks primarily test
static knowledge through question answering, but they fall short of capturing
the dynamic and iterative nature of trading. To address this gap, we introduce
StockBench, a contamination-free benchmark designed to evaluate LLM agents in
realistic, multi-month stock trading environments. Agents receive daily market
signals -- including prices, fundamentals, and news -- and must make sequential
buy, sell, or hold decisions. Performance is assessed using financial metrics
such as cumulative return, maximum drawdown, and the Sortino ratio. Our
evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and
open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM
agents struggle to outperform the simple buy-and-hold baseline, several models
demonstrate the potential to deliver higher returns and manage risk more
effectively. These findings highlight both the challenges and opportunities in
developing LLM-powered financial agents, showing that excelling at static
financial knowledge tasks does not necessarily translate into successful
trading strategies. We release StockBench as an open-source resource to support
reproducibility and advance future research in this domain.

</details>


### [212] [DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning](https://arxiv.org/abs/2510.02212)
*Hanyang Zhao,Dawen Liang,Wenpin Tang,David Yao,Nathan Kallus*

Main category: cs.LG

TL;DR: 提出DiFFPO框架，通过强化学习训练掩码扩散大语言模型，实现更好更快推理，在基准任务中展示有效性。


<details>
  <summary>Details</summary>
Motivation: 训练掩码扩散大语言模型实现更好更快推理，改进现有方法。

Method: 1. 通过离线策略强化学习训练替代策略，结合重要性采样校正进行两阶段似然近似；2. 提出联合训练高效采样器/控制器的新方向，让模型自适应分配推理阈值。

Result: 相比仅训练模型，联合训练采样器能以更少的函数评估次数获得更好的准确率，改善推理时间计算的帕累托前沿。

Conclusion: 所提出的DiFFPO框架在训练开源大扩散语言模型处理基准数学和规划任务中有效。

Abstract: We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified
framework for training masked diffusion large language models (dLLMs) to reason
not only better (furious), but also faster via reinforcement learning (RL). We
first unify the existing baseline approach such as d1 by proposing to train
surrogate policies via off-policy RL, whose likelihood is much more tractable
as an approximation to the true dLLM policy. This naturally motivates a more
accurate and informative two-stage likelihood approximation combined with
importance sampling correction, which leads to generalized RL algorithms with
better sample efficiency and superior task performance. Second, we propose a
new direction of joint training efficient samplers/controllers of dLLMs policy.
Via RL, we incentivize dLLMs' natural multi-token prediction capabilities by
letting the model learn to adaptively allocate an inference threshold for each
prompt. By jointly training the sampler, we yield better accuracies with lower
number of function evaluations (NFEs) compared to training the model only,
obtaining the best performance in improving the Pareto frontier of the
inference-time compute of dLLMs. We showcase the effectiveness of our pipeline
by training open source large diffusion language models over benchmark math and
planning tasks.

</details>


### [213] [C2AL: Cohort-Contrastive Auxiliary Learning for Large-scale Recommendation Systems](https://arxiv.org/abs/2510.02215)
*Mertcan Cokbas,Ziteng Liu,Zeyi Tao,Chengkai Zhang,Elder Veliz,Qin Huang,Ellie Wen,Huayu Li,Qiang Jin,Murat Duman,Benjamin Au,Guy Lebanon,Sagar Chordia*

Main category: cs.LG

TL;DR: 本文提出C2AL方法，通过辅助学习解决推荐模型训练中忽视少数群体的问题，实验表明该方法能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大规模推荐模型在单一全局目标下训练，假设用户群体同质，但实际数据包含异质群体，模型易忽略头尾区域，限制学习能力。

Method: 分析数据子结构，通过辅助学习暴露分布差异大的部分，利用部分冲突的辅助标签正则化共享表示。

Result: 在六个SOTA模型的大规模生产数据集上评估C2AL，能让因子分解机捕捉细粒度用户 - 广告交互，整体归一化熵最多降低0.16%，少数群体增益超0.30%。

Conclusion: 所提方法可在提升全局性能的同时，保留与少数群体的互信息。

Abstract: Training large-scale recommendation models under a single global objective
implicitly assumes homogeneity across user populations. However, real-world
data are composites of heterogeneous cohorts with distinct conditional
distributions. As models increase in scale and complexity and as more data is
used for training, they become dominated by central distribution patterns,
neglecting head and tail regions. This imbalance limits the model's learning
ability and can result in inactive attention weights or dead neurons. In this
paper, we reveal how the attention mechanism can play a key role in
factorization machines for shared embedding selection, and propose to address
this challenge by analyzing the substructures in the dataset and exposing those
with strong distributional contrast through auxiliary learning. Unlike previous
research, which heuristically applies weighted labels or multi-task heads to
mitigate such biases, we leverage partially conflicting auxiliary labels to
regularize the shared representation. This approach customizes the learning
process of attention layers to preserve mutual information with minority
cohorts while improving global performance. We evaluated C2AL on massive
production datasets with billions of data points each for six SOTA models.
Experiments show that the factorization machine is able to capture fine-grained
user-ad interactions using the proposed method, achieving up to a 0.16%
reduction in normalized entropy overall and delivering gains exceeding 0.30% on
targeted minority cohorts.

</details>


### [214] [xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity](https://arxiv.org/abs/2510.02228)
*Maximilian Beck,Kajetan Schweighofer,Sebastian Böck,Sebastian Lehner,Sepp Hochreiter*

Main category: cs.LG

TL;DR: 研究Transformers和xLSTM的扩展行为，发现xLSTM在典型场景中扩展表现优于Transformers，且优势随上下文增长而扩大。


<details>
  <summary>Details</summary>
Motivation: 探究Transformers和xLSTM的扩展行为，为未来模型设计和部署提供见解。

Method: 使用IsoFLOP和参数拟合方法，研究不同模型大小和训练令牌数下xLSTM在计算最优和过度训练状态的扩展行为；考察最优模型大小与上下文长度的依赖关系；分析推理时的扩展特征。

Result: 在典型大语言模型训练和推理场景中，xLSTM扩展表现优于Transformers，且随着训练和推理上下文增长，优势扩大。

Conclusion: xLSTM在扩展行为上表现良好，在未来模型设计和部署中有较大潜力。

Abstract: Scaling laws play a central role in the success of Large Language Models
(LLMs), enabling the prediction of model performance relative to compute
budgets prior to training. While Transformers have been the dominant
architecture, recent alternatives such as xLSTM offer linear complexity with
respect to context length while remaining competitive in the billion-parameter
regime. We conduct a comparative investigation on the scaling behavior of
Transformers and xLSTM along the following lines, providing insights to guide
future model design and deployment. First, we study the scaling behavior for
xLSTM in compute-optimal and over-training regimes using both IsoFLOP and
parametric fit approaches on a wide range of model sizes (80M-7B) and number of
training tokens (2B-2T). Second, we examine the dependence of optimal model
sizes on context length, a pivotal aspect that was largely ignored in previous
work. Finally, we analyze inference-time scaling characteristics. Our findings
reveal that in typical LLM training and inference scenarios, xLSTM scales
favorably compared to Transformers. Importantly, xLSTM's advantage widens as
training and inference contexts grow.

</details>


### [215] [PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks](https://arxiv.org/abs/2510.02236)
*Ricardo Misael Ayala Molina,Hyame Assem Alameddine,Makan Pourzandi,Chadi Assi*

Main category: cs.LG

TL;DR: 本文提出PUL - Inter - Slice Defender异常检测方案，利用PUL、LSTM自编码器和K - Means聚类，在含攻击污染训练数据上F1分数超98.50%，性能优于对比方案。


<details>
  <summary>Details</summary>
Motivation: 5G网络中用户设备跨网络切片切换存在分布式切片移动性（DSM）攻击风险，需要保障5G网络及其网络切片安全。

Method: 提出PUL - Inter - Slice Defender方案，利用正无标签学习（PUL），结合长短期记忆自编码器和K - Means聚类，以3GPP关键性能指标和性能测量计数器为特征构建机器学习模型。

Result: 在含10% - 40%攻击污染的训练数据集上F1分数超98.50%，性能优于Inter - Slice Defender及其他结合OCSVM与随机森林、XGBoost的PUL方案。

Conclusion: PUL - Inter - Slice Defender能有效检测DSM攻击，在含污染训练数据情况下仍有良好性能。

Abstract: Network Slices (NSs) are virtual networks operating over a shared physical
infrastructure, each designed to meet specific application requirements while
maintaining consistent Quality of Service (QoS). In Fifth Generation (5G)
networks, User Equipment (UE) can connect to and seamlessly switch between
multiple NSs to access diverse services. However, this flexibility, known as
Inter-Slice Switching (ISS), introduces a potential vulnerability that can be
exploited to launch Distributed Slice Mobility (DSM) attacks, a form of
Distributed Denial of Service (DDoS) attack. To secure 5G networks and their
NSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; an
anomaly detection solution that leverages Positive Unlabeled Learning (PUL) and
incorporates a combination of Long Short-Term Memory Autoencoders and K-Means
clustering. PUL-Inter-Slice Defender leverages the Third Generation Partnership
Project (3GPP) key performance indicators and performance measurement counters
as features for its machine learning models to detect DSM attack variants while
maintaining robustness in the presence of contaminated training data. When
evaluated on data collected from our 5G testbed based on the open-source
free5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator;
PUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on training
datasets with 10% to 40% attack contamination, consistently outperforming its
counterpart Inter-Slice Defender and other PUL based solutions combining
One-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost.

</details>


### [216] [ExGRPO: Learning to Reason from Experience](https://arxiv.org/abs/2510.02245)
*Runzhe Zhan,Yafu Li,Zhi Wang,Xiaoye Qu,Dongrui Liu,Jing Shao,Derek F. Wong,Yu Cheng*

Main category: cs.LG

TL;DR: 本文研究推理经验价值指标，提出ExGRPO框架，实验表明其能提升推理性能并稳定训练，凸显经验管理对RLVR的重要性。


<details>
  <summary>Details</summary>
Motivation: 标准策略训练存在计算低效和不稳定问题，且推理模型学习动态中经验特征的作用未充分研究，需探究有价值的推理经验。

Method: 确定推理经验价值的指标（推出正确性和熵），提出ExGRPO框架组织和优先处理有价值经验，采用混合策略目标平衡探索与利用。

Result: 在五个骨干模型上实验，ExGRPO在数学/通用基准上持续提升推理性能，平均增益+3.5/7.6分，能稳定训练。

Conclusion: 有原则的经验管理是高效可扩展的强化学习验证奖励（RLVR）的关键要素。

Abstract: Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm
for improving the reasoning ability of large language models. However, standard
on-policy training discards rollout experiences after a single update, leading
to computational inefficiency and instability. While prior work on RL has
highlighted the benefits of reusing past experience, the role of experience
characteristics in shaping learning dynamics of large reasoning models remains
underexplored. In this paper, we are the first to investigate what makes a
reasoning experience valuable and identify rollout correctness and entropy as
effective indicators of experience value. Based on these insights, we propose
ExGRPO (Experiential Group Relative Policy Optimization), a framework that
organizes and prioritizes valuable experiences, and employs a mixed-policy
objective to balance exploration with experience exploitation. Experiments on
five backbone models (1.5B-8B parameters) show that ExGRPO consistently
improves reasoning performance on mathematical/general benchmarks, with an
average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO
stabilizes training on both stronger and weaker models where on-policy methods
fail. These results highlight principled experience management as a key
ingredient for efficient and scalable RLVR.

</details>


### [217] [Transformers Discover Molecular Structure Without Graph Priors](https://arxiv.org/abs/2510.02259)
*Tobias Kreiman,Yutong Bai,Fadi Atieh,Elizabeth Weaver,Eric Qu,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: 研究无预定义图或物理先验的纯Transformer能否近似分子能量和力，结果表明Transformer可学习物理一致模式，挑战硬编码图归纳偏置必要性。


<details>
  <summary>Details</summary>
Motivation: 现有GNN因硬编码图存在表达能力受限和推理慢的问题，探究无预定义图和物理先验的纯Transformer在分子能量和力近似上的可行性。

Method: 在OMol25数据集上，在匹配训练计算预算下训练Transformer，并与最先进的等变GNN对比。

Result: Transformer能达到有竞争力的能量和力的平均绝对误差，学习到物理一致模式，且可随训练资源增加可预测地改进。

Conclusion: Transformer可自适应地展现GNN的许多有利特性，挑战了硬编码图归纳偏置的必要性，指向分子建模的标准化、可扩展架构。

Abstract: Graph Neural Networks (GNNs) are the dominant architecture for molecular
machine learning, particularly for molecular property prediction and machine
learning interatomic potentials (MLIPs). GNNs perform message passing on
predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor
scheme. While this design aligns with the locality present in many molecular
tasks, a hard-coded graph can limit expressivity due to the fixed receptive
field and slows down inference with sparse graph operations. In this work, we
investigate whether pure, unmodified Transformers trained directly on Cartesian
coordinates$\unicode{x2013}$without predefined graphs or physical
priors$\unicode{x2013}$can approximate molecular energies and forces. As a
starting point for our analysis, we demonstrate how to train a Transformer to
competitive energy and force mean absolute errors under a matched training
compute budget, relative to a state-of-the-art equivariant GNN on the OMol25
dataset. We discover that the Transformer learns physically consistent
patterns$\unicode{x2013}$such as attention weights that decay inversely with
interatomic distance$\unicode{x2013}$and flexibly adapts them across different
molecular environments due to the absence of hard-coded biases. The use of a
standard Transformer also unlocks predictable improvements with respect to
scaling training resources, consistent with empirical scaling laws observed in
other domains. Our results demonstrate that many favorable properties of GNNs
can emerge adaptively in Transformers, challenging the necessity of hard-coded
graph inductive biases and pointing toward standardized, scalable architectures
for molecular modeling.

</details>


### [218] [How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning](https://arxiv.org/abs/2510.02265)
*Yalin E. Sagduyu,Tugba Erpek,Kemal Davaslioglu,Sastry Kompella*

Main category: cs.LG

TL;DR: 本文研究缓解反应式干扰问题，发射 - 接收对利用强化学习避免干扰并优化吞吐量，结果表明强化学习能快速适应频谱动态。


<details>
  <summary>Details</summary>
Motivation: 解决干扰器采用动态策略检测并干扰正在进行的传输问题，使发射 - 接收对在无先验知识情况下避免干扰并优化吞吐量。

Method: 使用强化学习（RL）来调整发射功率、调制和信道选择，离散干扰事件状态用Q - learning，基于接收功率的连续状态用DQN。

Result: 通过不同奖励函数和动作集，结果显示RL能快速适应频谱动态，随时间推移维持高数据速率。

Conclusion: 强化学习可以有效应对反应式干扰，适应频谱动态变化。

Abstract: This paper studies the problem of mitigating reactive jamming, where a jammer
adopts a dynamic policy of selecting channels and sensing thresholds to detect
and jam ongoing transmissions. The transmitter-receiver pair learns to avoid
jamming and optimize throughput over time (without prior knowledge of channel
conditions or jamming strategies) by using reinforcement learning (RL) to adapt
transmit power, modulation, and channel selection. Q-learning is employed for
discrete jamming-event states, while Deep Q-Networks (DQN) are employed for
continuous states based on received power. Through different reward functions
and action sets, the results show that RL can adapt rapidly to spectrum
dynamics and sustain high rates as channels and jamming policies change over
time.

</details>


### [219] [Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps](https://arxiv.org/abs/2510.02274)
*Kyoungjun Park,Yifan Yang,Changhan Ge,Lili Qiu,Shiqi Jiang*

Main category: cs.LG

TL;DR: 论文提出Diffusion^2方法用3D点云建模RF信号传播，评估显示其准确高效。


<details>
  <summary>Details</summary>
Motivation: 准确预测复杂环境中RF信号有挑战，且RF信号建模对理解环境和支持无线应用有重要作用。

Method: 引入Diffusion^2方法，使用3D点云建模；提出RF - 3D Encoder提取特征，进行多尺度嵌入模拟信号传播。

Result: 基于合成和真实测量评估，Diffusion^2能准确估计不同频段和环境下RF信号行为，误差仅1.9 dB，比现有方法快27倍。

Conclusion: Diffusion^2是该领域的重大进展。

Abstract: Modeling radio frequency (RF) signal propagation is essential for
understanding the environment, as RF signals offer valuable insights beyond the
capabilities of RGB cameras, which are limited by the visible-light spectrum,
lens coverage, and occlusions. It is also useful for supporting wireless
diagnosis, deployment, and optimization. However, accurately predicting RF
signals in complex environments remains a challenge due to interactions with
obstacles such as absorption and reflection. We introduce Diffusion^2, a
diffusion-based approach that uses 3D point clouds to model the propagation of
RF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.
To effectively capture RF-related features from 3D data, we present the RF-3D
Encoder, which encapsulates the complexities of 3D geometry along with
signal-specific details. These features undergo multi-scale embedding to
simulate the actual RF signal dissemination process. Our evaluation, based on
synthetic and real-world measurements, demonstrates that Diffusion^2 accurately
estimates the behavior of RF signals in various frequency bands and
environmental conditions, with an error margin of just 1.9 dB and 27x faster
than existing methods, marking a significant advancement in the field. Refer to
https://rfvision-project.github.io/ for more information.

</details>


### [220] [Fine-Grained Urban Traffic Forecasting on Metropolis-Scale Road Networks](https://arxiv.org/abs/2510.02278)
*Fedor Velikonivtsev,Oleg Platonov,Gleb Bazhenov,Liudmila Prokhorenkova*

Main category: cs.LG

TL;DR: 本文发布两个大城市道路网络数据集作为交通预测基准，指出当前模型难以处理大数据集，提出无专门时序处理模块的GNN方法，提升扩展性和预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前公开交通预测基准数据集存在道路连通信息缺失、道路属性信息有限、路段数量少等问题，且城市道路网络预测更具挑战，需要更完善的基准。

Method: 发布两个大城市道路网络数据集，提出无专门时序处理模块的GNN交通预测方法。

Result: 提出的方法扩展性更好，预测性能更强，当前多数时空模型难以处理新数据集。

Conclusion: 希望数据集和建模见解为交通预测研究提供有价值资源。

Abstract: Traffic forecasting on road networks is a complex task of significant
practical importance that has recently attracted considerable attention from
the machine learning community, with spatiotemporal graph neural networks
(GNNs) becoming the most popular approach. The proper evaluation of traffic
forecasting methods requires realistic datasets, but current publicly available
benchmarks have significant drawbacks, including the absence of information
about road connectivity for road graph construction, limited information about
road properties, and a relatively small number of road segments that falls
short of real-world applications. Further, current datasets mostly contain
information about intercity highways with sparsely located sensors, while city
road networks arguably present a more challenging forecasting task due to much
denser roads and more complex urban traffic patterns. In this work, we provide
a more complete, realistic, and challenging benchmark for traffic forecasting
by releasing datasets representing the road networks of two major cities, with
the largest containing almost 100,000 road segments (more than a 10-fold
increase relative to existing datasets). Our datasets contain rich road
features and provide fine-grained data about both traffic volume and traffic
speed, allowing for building more holistic traffic forecasting systems. We show
that most current implementations of neural spatiotemporal models for traffic
forecasting have problems scaling to datasets of our size. To overcome this
issue, we propose an alternative approach to neural traffic forecasting that
uses a GNN without a dedicated module for temporal sequence processing, thus
achieving much better scalability, while also demonstrating stronger
forecasting performance. We hope our datasets and modeling insights will serve
as a valuable resource for research in traffic forecasting.

</details>


### [221] [Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation](https://arxiv.org/abs/2510.02279)
*Mykyta Ielanskyi,Kajetan Schweighofer,Lukas Aichberger,Sepp Hochreiter*

Main category: cs.LG

TL;DR: 论文提出用替代风险指标改进自然语言生成中不确定性估计方法的评估，还提出Elo评级客观总结评估结果。


<details>
  <summary>Details</summary>
Motivation: 现有常用近似正确性函数在评估不确定性估计方法时存在分歧，可夸大方法表现，需改进评估。

Method: 提出使用替代风险指标进行风险关联实验，在问答任务中对多个大语言模型评判变体求边缘化，探索结构化任务及特定检测任务，提出用Elo评级客观总结评估。

Result: 在问答任务中可减少评估偏差，结构化任务等能提供稳健可控的风险指标。

Conclusion: 所提方法可提高自然语言生成中不确定性估计算法实证评估的稳健性。

Abstract: Hallucinations are a common issue that undermine the reliability of large
language models (LLMs). Recent studies have identified a specific subset of
hallucinations, known as confabulations, which arise due to predictive
uncertainty of LLMs. To detect confabulations, various methods for estimating
predictive uncertainty in natural language generation (NLG) have been
developed. These methods are typically evaluated by correlating uncertainty
estimates with the correctness of generated text, with question-answering (QA)
datasets serving as the standard benchmark. However, commonly used approximate
correctness functions have substantial disagreement between each other and,
consequently, in the ranking of the uncertainty estimation methods. This allows
one to inflate the apparent performance of uncertainty estimation methods. We
propose using several alternative risk indicators for risk correlation
experiments that improve robustness of empirical assessment of UE algorithms
for NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge
variants leads to reducing the evaluation biases. Furthermore, we explore
structured tasks as well as out of distribution and perturbation detection
tasks which provide robust and controllable risk indicators. Finally, we
propose to use an Elo rating of uncertainty estimation methods to give an
objective summarization over extensive evaluation settings.

</details>


### [222] [Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks](https://arxiv.org/abs/2510.02286)
*Ruohao Guo,Afshin Oroojlooy,Roshan Sridhar,Miguel Ballesteros,Alan Ritter,Dan Roth*

Main category: cs.LG

TL;DR: 现有大语言模型在多轮交互中易受对抗攻击，传统方法难发现多轮攻击策略，本文提出DialTree - RPO框架解决该问题，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在多轮交互中易受攻击，现有发现安全漏洞的方法多聚焦单轮攻击，未充分探索多轮攻击空间。

Method: 提出DialTree - RPO，将对话视为顺序决策问题，结合树搜索的策略梯度强化学习框架，无需手动整理数据进行系统探索。

Result: 在10个目标模型上攻击成功率比现有最优方法高25.9%，还能发现新攻击策略。

Conclusion: DialTree - RPO框架能有效自主发现多样的多轮攻击策略。

Abstract: Despite recent rapid progress in AI safety, current large language models
remain vulnerable to adversarial attacks in multi-turn interaction settings,
where attackers strategically adapt their prompts across conversation turns and
pose a more critical yet realistic challenge. Existing approaches that discover
safety vulnerabilities either rely on manual red-teaming with human experts or
employ automated methods using pre-defined templates and human-curated attack
data, with most focusing on single-turn attacks. However, these methods did not
explore the vast space of possible multi-turn attacks, failing to consider
novel attack trajectories that emerge from complex dialogue dynamics and
strategic conversation planning. This gap is particularly critical given recent
findings that LLMs exhibit significantly higher vulnerability to multi-turn
attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy
reinforcement learning framework integrated with tree search that autonomously
discovers diverse multi-turn attack strategies by treating the dialogue as a
sequential decision-making problem, enabling systematic exploration without
manually curated data. Through extensive experiments, our approach not only
achieves more than 25.9% higher ASR across 10 target models compared to
previous state-of-the-art approaches, but also effectively uncovers new attack
strategies by learning optimal dialogue policies that maximize attack success
across multiple turns.

</details>


### [223] [Continual Personalization for Diffusion Models](https://arxiv.org/abs/2510.02296)
*Yu-Chien Liao,Jr-Jen Chen,Chi-Pin Huang,Ci-Siang Lin,Meng-Lin Wu,Yu-Chiang Frank Wang*

Main category: cs.LG

TL;DR: 提出概念神经元选择（CNS）学习策略，用于增量更新扩散模型，在真实数据集评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决在增量设置下更新扩散模型的计算挑战，实现持续学习方案中的个性化。

Method: 独特识别扩散模型中与目标概念密切相关的神经元，以增量方式微调概念神经元并保留先前概念的知识。

Result: 在真实数据集评估中，CNS以最小参数调整实现了最先进的性能，在单概念和多概念个性化任务中优于先前方法，还实现了无融合操作，减少持续个性化的内存存储和处理时间。

Conclusion: CNS是一种简单有效的方法，能在增量学习中实现扩散模型的个性化，同时减轻灾难性遗忘问题，保留零样本图像生成能力。

Abstract: Updating diffusion models in an incremental setting would be practical in
real-world applications yet computationally challenging. We present a novel
learning strategy of Concept Neuron Selection (CNS), a simple yet effective
approach to perform personalization in a continual learning scheme. CNS
uniquely identifies neurons in diffusion models that are closely related to the
target concepts. In order to mitigate catastrophic forgetting problems while
preserving zero-shot text-to-image generation ability, CNS finetunes concept
neurons in an incremental manner and jointly preserves knowledge learned of
previous concepts. Evaluation of real-world datasets demonstrates that CNS
achieves state-of-the-art performance with minimal parameter adjustments,
outperforming previous methods in both single and multi-concept personalization
works. CNS also achieves fusion-free operation, reducing memory storage and
processing time for continual personalization.

</details>


### [224] [Interactive Training: Feedback-Driven Neural Network Optimization](https://arxiv.org/abs/2510.02297)
*Wentao Zhang,Yang Young Lu,Yuntian Deng*

Main category: cs.LG

TL;DR: 本文介绍交互式训练框架，能在神经网络训练中实时干预，通过案例证明其优势并展望未来训练范式。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络训练缺乏动态响应训练问题的灵活性，需新方法。

Method: 引入交互式训练开源框架，用控制服务器介导通信，用户可动态调整超参数、训练数据和模型检查点。

Result: 通过三个案例研究，证明该框架实现了更好的训练稳定性，降低了对初始超参数的敏感性，提高了对用户需求的适应性。

Conclusion: 为未来AI代理自主监控、解决不稳定和优化训练动态的训练范式铺平道路。

Abstract: Traditional neural network training typically follows fixed, predefined
optimization recipes, lacking the flexibility to dynamically respond to
instabilities or emerging training issues. In this paper, we introduce
Interactive Training, an open-source framework that enables real-time,
feedback-driven intervention during neural network training by human experts or
automated AI agents. At its core, Interactive Training uses a control server to
mediate communication between users or agents and the ongoing training process,
allowing users to dynamically adjust optimizer hyperparameters, training data,
and model checkpoints. Through three case studies, we demonstrate that
Interactive Training achieves superior training stability, reduced sensitivity
to initial hyperparameters, and improved adaptability to evolving user needs,
paving the way toward a future training paradigm where AI agents autonomously
monitor training logs, proactively resolve instabilities, and optimize training
dynamics.

</details>


### [225] [Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models](https://arxiv.org/abs/2510.02300)
*Runqian Wang,Yilun Du*

Main category: cs.LG

TL;DR: 提出Equilibrium Matching (EqM)生成建模框架，超越扩散/流模型性能，还可处理多种任务。


<details>
  <summary>Details</summary>
Motivation: 摒弃传统扩散和基于流的生成模型中的非平衡、时间条件动力学，从平衡动力学角度构建生成建模框架。

Method: 学习隐性能量景观的平衡梯度，在推理时采用基于优化的采样过程。

Result: 在ImageNet 256×256上FID达到1.90，超越扩散/流模型的生成性能，理论上可从数据流形学习和采样。

Conclusion: EqM是灵活框架，能处理多种任务，为流模型和基于能量的模型搭建桥梁，提供优化驱动推理的简单途径。

Abstract: We introduce Equilibrium Matching (EqM), a generative modeling framework
built from an equilibrium dynamics perspective. EqM discards the
non-equilibrium, time-conditional dynamics in traditional diffusion and
flow-based generative models and instead learns the equilibrium gradient of an
implicit energy landscape. Through this approach, we can adopt an
optimization-based sampling process at inference time, where samples are
obtained by gradient descent on the learned landscape with adjustable step
sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation
performance of diffusion/flow models empirically, achieving an FID of 1.90 on
ImageNet 256$\times$256. EqM is also theoretically justified to learn and
sample from the data manifold. Beyond generation, EqM is a flexible framework
that naturally handles tasks including partially noised image denoising, OOD
detection, and image composition. By replacing time-conditional velocities with
a unified equilibrium landscape, EqM offers a tighter bridge between flow and
energy-based models and a simple route to optimization-driven inference.

</details>


### [226] [Knowledge Distillation Detection for Open-weights Models](https://arxiv.org/abs/2510.02302)
*Qin Shi,Amber Yijia Zheng,Qifan Song,Raymond A. Yeh*

Main category: cs.LG

TL;DR: 提出知识蒸馏检测任务，介绍模型无关框架，实验显示检测准确率提升。


<details>
  <summary>Details</summary>
Motivation: 对模型来源和通过蒸馏进行的未经授权复制的担忧。

Method: 引入结合无数据输入合成和统计分数计算的模型无关框架。

Result: 在不同架构的图像分类和文本到图像生成实验中，检测准确率比最强基线在CIFAR - 10上提高59.6%，在ImageNet上提高71.2%，在文本到图像生成上提高20.0%。

Conclusion: 提出的方法能有效提升知识蒸馏检测的准确率，代码开源。

Abstract: We propose the task of knowledge distillation detection, which aims to
determine whether a student model has been distilled from a given teacher,
under a practical setting where only the student's weights and the teacher's
API are available. This problem is motivated by growing concerns about model
provenance and unauthorized replication through distillation. To address this
task, we introduce a model-agnostic framework that combines data-free input
synthesis and statistical score computation for detecting distillation. Our
approach is applicable to both classification and generative models.
Experiments on diverse architectures for image classification and text-to-image
generation show that our method improves detection accuracy over the strongest
baselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image
generation. The code is available at
https://github.com/shqii1j/distillation_detection.

</details>


### [227] [Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization](https://arxiv.org/abs/2510.02308)
*Dhruv Kohli,Sawyer J. Robertson,Gal Mishne,Alexander Cloninger*

Main category: cs.LG

TL;DR: 提出谱方法LEGO用于估计数据流形切空间，比LPCA更抗噪，下游任务表现更佳


<details>
  <summary>Details</summary>
Motivation: 标准方法LPCA在高噪声环境中因邻域大小选择困难，且需先验数据特征知识，故提出新方法

Method: 利用数据全局结构，通过正交化图拉普拉斯矩阵低频特征向量的梯度来估计切空间，并给出微分几何和随机矩阵理论的理论依据

Result: LEGO的切空间估计比LPCA更抗噪，在流形学习、边界检测和局部内在维数估计等下游任务中有显著改进

Conclusion: LEGO是一种有效且抗噪的切空间估计方法，能提升下游任务性能

Abstract: Estimating the tangent spaces of a data manifold is a fundamental problem in
data analysis. The standard approach, Local Principal Component Analysis
(LPCA), struggles in high-noise settings due to a critical trade-off in
choosing the neighborhood size. Selecting an optimal size requires prior
knowledge of the geometric and noise characteristics of the data that are often
unavailable. In this paper, we propose a spectral method, Laplacian Eigenvector
Gradient Orthogonalization (LEGO), that utilizes the global structure of the
data to guide local tangent space estimation. Instead of relying solely on
local neighborhoods, LEGO estimates the tangent space at each data point by
orthogonalizing the gradients of low-frequency eigenvectors of the graph
Laplacian. We provide two theoretical justifications of our method. First, a
differential geometric analysis on a tubular neighborhood of a manifold shows
that gradients of the low-frequency Laplacian eigenfunctions of the tube align
closely with the manifold's tangent bundle, while an eigenfunction with high
gradient in directions orthogonal to the manifold lie deeper in the spectrum.
Second, a random matrix theoretic analysis also demonstrates that low-frequency
eigenvectors are robust to sub-Gaussian noise. Through comprehensive
experiments, we demonstrate that LEGO yields tangent space estimates that are
significantly more robust to noise than those from LPCA, resulting in marked
improvements in downstream tasks such as manifold learning, boundary detection,
and local intrinsic dimension estimation.

</details>


### [228] [KaVa: Latent Reasoning via Compressed KV-Cache Distillation](https://arxiv.org/abs/2510.02312)
*Anna Kuzina,Maciej Pioro,Paul N. Whatmough,Babak Ehteshami Bejnordi*

Main category: cs.LG

TL;DR: 提出KaVa框架，通过自蒸馏将教师模型压缩KV缓存知识传递给潜在推理学生模型，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型显式思维链推理有计算和内存开销，潜在推理缺乏有效监督，本文旨在解决此问题。

Method: 提出KaVa框架，通过自蒸馏将教师模型压缩KV缓存知识传递给潜在推理学生模型，利用连续潜在标记对齐KV轨迹。

Result: 该方法优于潜在推理基线，在不同推理轨迹上退化更小，可扩展到更大骨干模型且保持效率。

Conclusion: 压缩KV缓存蒸馏可作为潜在推理的可扩展监督信号，结合了思维链训练教师模型的准确性与潜在推理的效率和可部署性。

Abstract: Large Language Models (LLMs) excel at multi-step reasoning problems with
explicit chain-of-thought (CoT), but verbose traces incur significant
computational costs and memory overhead, and often carry redundant, stylistic
artifacts. Latent reasoning has emerged as an efficient alternative that
internalizes the thought process, but it suffers from a critical lack of
supervision, limiting its effectiveness on complex, natural-language reasoning
traces. In this work, we propose KaVa, the first framework that bridges this
gap by distilling knowledge directly from a compressed KV-cache of the teacher
into a latent-reasoning student via self-distillation, leveraging the
representational flexibility of continuous latent tokens to align stepwise KV
trajectories. We show that the abstract, unstructured knowledge within
compressed KV-cache, which lacks direct token correspondence, can serve as a
rich supervisory signal for a latent reasoning student. Empirically, the
approach consistently outperforms strong latent baselines, exhibits markedly
smaller degradation from equation-only to natural-language traces, and scales
to larger backbones while preserving efficiency. These results establish
compressed KV-cache distillation as a scalable supervision signal for latent
reasoning, combining the accuracy of CoT-trained teachers with the efficiency
and deployability of latent inference.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [229] [Microscaling Floating Point Formats for Large Language Models](https://arxiv.org/abs/2510.01863)
*Marco Cococcioni,Dario Pagani,Federico Rossi*

Main category: cs.NE

TL;DR: 本文提出微缩放浮点格式优化大语言模型资源使用，在GPT - 2架构测试证明其有效，代码开源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对计算和内存需求增加，需创新方法在不影响性能下优化资源使用。

Method: 利用微缩放浮点格式，在GPT - 2架构中测试不同配置的微缩放浮点数。

Result: 微缩放数据格式在训练和推理中能达到有竞争力的准确率。

Conclusion: 微缩放浮点格式是大规模部署大语言模型的资源高效替代方案。

Abstract: The increasing computational and memory demands of large language models
(LLMs) necessitate innovative approaches to optimize resource usage without
compromising performance. This paper leverages microscaling floating-point
formats, a novel technique designed to address these challenges by reducing the
storage and computational overhead associated with numerical representations in
LLMs. Unlike traditional floating-point representations that allocate a
dedicated scale for each value, microscaling employs a shared scale across a
block of values, enabling compact one-byte floating-point representations while
maintaining an extended dynamic range. We explore the application of
microscaling in the context of 8-bit floating-point formats to significantly
reduce memory footprint and computational costs. We tested several
configurations of microscaling floats within the GPT-2 LLM architecture,
demonstrating that microscaling data formats can achieve competitive accuracy
during training and inference, proving its efficacy as a resource-efficient
alternative for deploying LLMs at scale. The source code is publicly available
at: https://github.com/unipi-dii-compressedarith/llm.c-sve

</details>


### [230] [VarCoNet: A variability-aware self-supervised framework for functional connectome extraction from resting-state fMRI](https://arxiv.org/abs/2510.02120)
*Charalampos Lamprou,Aamna Alshehhi,Leontios J. Hadjileontiadis,Mohamed L. Seghier*

Main category: cs.NE

TL;DR: 提出VarCoNet框架从静息态fMRI数据中提取功能连接组，在两项下游任务测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 考虑脑功能个体间变异性对精准医学的重要性，将个体间变异性视为有意义数据，而非噪声。

Method: 采用自监督对比学习，结合基于分割rs - fMRI信号的增强策略、1D - CNN - Transformer编码器和贝叶斯超参数优化。

Result: 在两项下游任务中，用不同脑分区方案，与13种深度学习方法对比测试，显示出优越性、鲁棒性、可解释性和泛化性。

Conclusion: VarCoNet为rs - fMRI中的功能连接组分析提供了通用且鲁棒的框架。

Abstract: Accounting for inter-individual variability in brain function is key to
precision medicine. Here, by considering functional inter-individual
variability as meaningful data rather than noise, we introduce VarCoNet, an
enhanced self-supervised framework for robust functional connectome (FC)
extraction from resting-state fMRI (rs-fMRI) data. VarCoNet employs
self-supervised contrastive learning to exploit inherent functional
inter-individual variability, serving as a brain function encoder that
generates FC embeddings readily applicable to downstream tasks even in the
absence of labeled data. Contrastive learning is facilitated by a novel
augmentation strategy based on segmenting rs-fMRI signals. At its core,
VarCoNet integrates a 1D-CNN-Transformer encoder for advanced time-series
processing, enhanced with a robust Bayesian hyperparameter optimization. Our
VarCoNet framework is evaluated on two downstream tasks: (i) subject
fingerprinting, using rs-fMRI data from the Human Connectome Project, and (ii)
autism spectrum disorder (ASD) classification, using rs-fMRI data from the
ABIDE I and ABIDE II datasets. Using different brain parcellations, our
extensive testing against state-of-the-art methods, including 13 deep learning
methods, demonstrates VarCoNet's superiority, robustness, interpretability, and
generalizability. Overall, VarCoNet provides a versatile and robust framework
for FC analysis in rs-fMRI.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [231] [Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration](https://arxiv.org/abs/2510.01379)
*Huashan Chen,Zhenyu Qi,Haotang Li,Hong Chen,Jinfu Chen,Kebin Peng,In Kee Kim,Kyu Hyung Lee,Sen He*

Main category: cs.SE

TL;DR: 论文挑战单模型代码生成惯例，提出多阶段、性能导向编排框架PerfOrch，经实验验证能提升代码生成正确性和性能，架构具可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前单模型代码生成方法忽略不同模型在编程语言、算法领域和开发阶段的计算优势差异，需改进。

Method: 对17个先进大语言模型在5种编程语言上进行综合实证研究，基于结果构建PerfOrch，通过分阶段验证和回滚机制编排模型。

Result: PerfOrch在HumanEval - X和EffiBench - X上平均正确率超GPT - 4o，还优化执行时间。

Conclusion: 该框架无需模型微调，可提升代码生成正确性和性能，其架构具可扩展性，适应生成式AI发展。

Abstract: While Large Language Models (LLMs) have become the predominant paradigm for
automated code generation, current single-model approaches fundamentally ignore
the heterogeneous computational strengths that different models exhibit across
programming languages, algorithmic domains, and development stages. This paper
challenges the single-model convention by introducing a multi-stage,
performance-guided orchestration framework that dynamically routes coding tasks
to the most suitable LLMs within a structured generate-fix-refine workflow. Our
approach is grounded in a comprehensive empirical study of 17 state-of-the-art
LLMs across five programming languages (Python, Java, C++, Go, and Rust) using
HumanEval-X benchmark. The study, which evaluates both functional correctness
and runtime performance metrics (execution time, mean/max memory utilization,
and CPU efficiency), reveals pronounced performance heterogeneity by language,
development stage, and problem category. Guided by these empirical insights, we
present PerfOrch, an LLM agent that orchestrates top-performing LLMs for each
task context through stage-wise validation and rollback mechanisms. Without
requiring model fine-tuning, PerfOrch achieves substantial improvements over
strong single-model baselines: average correctness rates of 96.22% and 91.37%
on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and
49.11%. Beyond correctness gains, the framework delivers consistent performance
optimizations, improving execution time for 58.76% of problems with median
speedups ranging from 17.67% to 27.66% across languages on two benchmarks. The
framework's plug-and-play architecture ensures practical scalability, allowing
new LLMs to be profiled and integrated seamlessly, thereby offering a paradigm
for production-grade automated software engineering that adapts to the rapidly
evolving generative AI landscape.

</details>


### [232] [Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected](https://arxiv.org/abs/2510.01514)
*J. Alexander Curtis,Sharadha Kasiviswanathan,Nasir Eisty*

Main category: cs.SE

TL;DR: 研究GitHub上wontfix标签的使用情况，发现约30%项目使用，有八个常见主题，该标签有利有弊。


<details>
  <summary>Details</summary>
Motivation: wontfix标签广泛使用但对开源软件开发项目管理和社区动态的影响不明确，需研究其使用情况和原因。

Method: 采用混合方法，分析3132个热门仓库的定量和定性数据，用开放编码和主题分析归类原因。

Result: 约30%项目使用wontfix标签，多在用户提交的问题上，有八个常见主题。

Conclusion: wontfix标签对管理资源和引导贡献者有帮助，但会打击社区参与度和影响管理透明度，理解原因有助于项目管理。

Abstract: Context: The ``wontfix'' label is a widely used yet narrowly understood tool
in GitHub repositories, indicating that an issue will not be pursued further.
Despite its prevalence, the impact of this label on project management and
community dynamics within open-source software development is not clearly
defined. Objective: This study examines the prevalence and reasons behind
issues being labeled as wontfix across various open-source repositories on
GitHub. Method: Employing a mixed-method approach, we analyze both quantitative
data to assess the prevalence of the wontfix label and qualitative data to
explore the reasoning that it was used. Data were collected from 3,132 of
GitHub's most-popular repositories. Later, we employ open coding and thematic
analysis to categorize the reasons behind wontfix labels, providing a
structured understanding of the issue management landscape. Results: Our
findings show that about 30% of projects on GitHub apply the wontfix label to
some issues. These issues most often occur on user-submitted issues for bug
reports and feature requests. The study identified eight common themes behind
labeling issues as wontfix, ranging from user-specific control factors to
maintainer-specific decisions. Conclusions: The wontfix label is a critical
tool for managing resources and guiding contributor efforts in GitHub projects.
However, it can also discourage community involvement and obscure the
transparency of project management. Understanding these reasons aids project
managers in making informed decisions and fostering efficient collaboration
within open-source communities.

</details>


### [233] [MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model](https://arxiv.org/abs/2510.01635)
*Yifei Chen,Sarra Habchi,Lili Wei*

Main category: cs.SE

TL;DR: 本文提出MIMIC框架，将多样人格特质融入游戏代理，能在不同游戏中实现更高测试覆盖率和更丰富互动，在Minecraft中表现优于现有代理。


<details>
  <summary>Details</summary>
Motivation: 传统自动化测试算法难以应对现代游戏，现有游戏代理忽略人类玩家多样策略，难以触发多样游戏交互和发现边缘情况。

Method: 提出MIMIC框架，将多样人格特质融入游戏代理，使其在相似场景采用不同游戏策略。

Result: MIMIC在不同游戏中实现更高测试覆盖率和更丰富互动，在Minecraft中任务完成率更高、解决方案更多样。

Conclusion: MIMIC在有效游戏测试方面有巨大潜力。

Abstract: Modern video games pose significant challenges for traditional automated
testing algorithms, yet intensive testing is crucial to ensure game quality. To
address these challenges, researchers designed gaming agents using
Reinforcement Learning, Imitation Learning, or Large Language Models. However,
these agents often neglect the diverse strategies employed by human players due
to their different personalities, resulting in repetitive solutions in similar
situations. Without mimicking varied gaming strategies, these agents struggle
to trigger diverse in-game interactions or uncover edge cases.
  In this paper, we present MIMIC, a novel framework that integrates diverse
personality traits into gaming agents, enabling them to adopt different gaming
strategies for similar situations. By mimicking different playstyles, MIMIC can
achieve higher test coverage and richer in-game interactions across different
games. It also outperforms state-of-the-art agents in Minecraft by achieving a
higher task completion rate and providing more diverse solutions. These results
highlight MIMIC's significant potential for effective game testing.

</details>


### [234] [FOSS-chain: using blockchain for Open Source Software license compliance](https://arxiv.org/abs/2510.01740)
*Kypros Iacovou,Georgia M. Kapitsaki,Evangelia Vanezi*

Main category: cs.SE

TL;DR: 文章提出将区块链与许可证管理集成以解决OSS许可证兼容性问题，设计并初步评估了FOSS - chain平台，结果有前景。


<details>
  <summary>Details</summary>
Motivation: 确保用户创建衍生作品时遵守OSS许可证条款过程复杂，许可证不兼容会引发合规问题和法律纠纷，需要解决。

Method: 设计、实现并初步评估使用区块链的Web平台FOSS - chain，该平台可自动执行许可证合规流程，涵盖14种OSS许可证，并通过小规模用户研究评估初始原型版本。

Result: 初步结果很有前景，展示了该平台在实际软件系统中应用的潜力。

Conclusion: 将区块链与许可证管理集成解决OSS许可证兼容性问题的方法可行，FOSS - chain平台有实用潜力。

Abstract: Open Source Software (OSS) is widely used and carries licenses that indicate
the terms under which the software is provided for use, also specifying
modification and distribution rules. Ensuring that users are respecting OSS
license terms when creating derivative works is a complex process. Compliance
issues arising from incompatibilities among licenses may lead to legal
disputes. At the same time, the blockchain technology with immutable entries
offers a mechanism to provide transparency when it comes to licensing and
ensure software changes are recorded. In this work, we are introducing an
integration of blockchain and license management when creating derivative
works, in order to tackle the issue of OSS license compatibility. We have
designed, implemented and performed a preliminary evaluation of FOSS-chain, a
web platform that uses blockchain and automates the license compliance process,
covering 14 OSS licenses. We have evaluated the initial prototype version of
the FOSS-chain platform via a small scale user study. Our preliminary results
are promising, demonstrating the potential of the platform for adaptation on
realistic software systems.

</details>


### [235] [ARENA: A tool for measuring and analysing the energy efficiency of Android apps](https://arxiv.org/abs/2510.01754)
*Hina Anwar*

Main category: cs.SE

TL;DR: 本文提出支持工具ARENA，可让开发者和研究者在IDE中连接物理测量设备，计算安卓应用能耗并进行数据处理。


<details>
  <summary>Details</summary>
Motivation: 现有基于硬件的安卓应用能耗测量方法耗时、难适配和复现，且缺乏开源工具。

Method: 将ARENA实现为IntelliJ和Android Studio插件，在IDE中连接物理测量设备，执行测试场景计算能耗，对数据进行聚合、统计分析、报告和可视化。

Result: 实现了ARENA工具。

Conclusion: ARENA能帮助开发者和研究者在开发过程中测量和分析安卓应用能耗。

Abstract: To build energy-efficient apps, there is a need to estimate and analyze their
energy consumption in typical usage scenarios. The energy consumption of
Android apps could be estimated via software-based and hardware-based
approaches. Software-based approaches, while easier to implement, are not as
accurate as hardware-based approaches. The process of measuring the energy
consumption of an Android app via a hardware-based approach typically involves
1) setting up a measurement environment, 2) executing the app under test on a
mobile device, 3) recording current/voltage data via a hardware device to
measure energy consumption, and 4) cleaning and aggregating data for analyses,
reports, and visualizations. Specialized scripts are written for selected
hardware and software components to ensure reliable energy measurements. The
energy measurement process is repeated many times and aggregated to remove
noise. These steps make the hardware-based energy measurement process
time-consuming and not easy to adapt or reproduce. There is a lack of
open-source tools available for developers and researchers to take reliable
energy measurements via hardware devices. In this paper, we present and
demonstrate ARENA, a support tool that enables developers and researchers to
connect to a physical measurement device without leaving the comfort of their
IDE. Developers could use ARENA during development to compare energy
consumption between different apps or versions of the same app. ARENA
calculates energy consumption on an Android smartphone by executing a test
scenario on the app under development. Further, ARENA helps aggregate,
statistically analyze, report, and visualize the data, allowing developers and
researchers to dig into the data directly or visually. We implemented ARENA as
an IntelliJ and Android Studio plugin.

</details>


### [236] [Towards Speeding up Program Repair with Non-Autoregressive Model](https://arxiv.org/abs/2510.01825)
*Zhenyu Yang,Yue Pan,Zhen Yang,Zhongxing Yu*

Main category: cs.SE

TL;DR: 提出用于自动程序修复（APR）任务的定制非自回归（NAR）代码生成模型NARRepair，在修复速度和准确性上达到了先进水平。


<details>
  <summary>Details</summary>
Motivation: 以往基于自回归（AR）的APR技术有巨大时间延迟，而简单使用NAR方式进行APR任务会导致补丁质量下降。

Method: 提出NARRepair模型，包含修复动作预测器、标记间依赖提取器和两阶段解码器。

Result: 在三个数据集上评估，NARRepair在有限修复时间内性能最佳，在GPU环境下修复速度比AR-based APR技术提高1.4 - 6.4倍。

Conclusion: NARRepair在修复速度和准确性方面实现了最先进的综合性能。

Abstract: Enlightened by the success of machine learning techniques in various
application areas, recent years have witnessed a surge of research efforts on
automatic program repair (APR) using machine learning techniques. Previous
machine learning-based APR techniques essentially modified bugs in the
autoregressive (AR) manner, which predicts future values based on past values.
Due to the manner of token-by-token generation, the AR-based APR technique has
a huge time delay. In particular, the delay of the APR model with a large
number of parameters is more serious. To address the issue, we aim to apply the
non-autoregressive (NAR) method to the APR task, which can output target code
in a parallel manner to avoid huge repair delays. However, the naive use of the
NAR manner for the APR task suffers from the issue of compromised patch
quality. To effectively adapt the NAR manner for the APR task, we in this paper
propose NARRepair, the first customized NAR code generation model for the APR
task. The NARRepair model features three major novelties, including 1) the
repair action predictor for alleviating the over-correction issue, 2) the
inter-token dependency extractor for alleviating the issue of lacking
inter-token dependency information, and 3) the two-stage decoder for
alleviating the issue of lacking contextual information. We evaluated NARRepair
on three widely used datasets in the APR community, and the results show that
1) compared to other APR techniques, the NARRepair model has the best
performance within the limited repair time, and 2) compared to AR-based APR
techniques, the repair speed of NARRepair has been increased by 1.4-6.4 times
in the GPU environment. Overall, the results show that NARRepair has achieved
state-of-the-art comprehensive performance in terms of repair speed and
accuracy.

</details>


### [237] [RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis](https://arxiv.org/abs/2510.01960)
*Victor Lira,Paulo Borba,Rodrigo Bonifácio,Galileu Santos e Matheus barbosa*

Main category: cs.SE

TL;DR: 提出RefFilter工具解决协作软件开发语义干扰检测中误报率高的问题，实验显示能降低误报且提升精度。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级静态分析技术检测语义干扰时误报率高，原因是无法有效区分行为保留的代码重构与影响行为的变更。

Method: 提出RefFilter工具，在现有静态技术基础上结合自动重构检测，从报告中剔除行为保留的重构以降低误报。

Result: 在标记数据集上RefFilter将误报率降低近32%，虽有少量漏报增加，但精度提升显著。

Conclusion: 感知重构的干扰检测是改进现代开发工作流中合并支持的实用有效策略。

Abstract: Detecting semantic interference remains a challenge in collaborative software
development. Recent lightweight static analysis techniques improve efficiency
over SDG-based methods, but they still suffer from a high rate of false
positives. A key cause of these false positives is the presence of
behavior-preserving code refactorings, which current techniques cannot
effectively distinguish from changes that impact behavior and can interfere
with others. To handle this problem we present RefFilter, a refactoring-aware
tool for semantic interference detection. It builds on existing static
techniques by incorporating automated refactoring detection to improve
precision. RefFilter discards behavior-preserving refactorings from reports,
reducing false positives while preserving detection coverage. To evaluate
effectiveness and scalability, use two datasets: a labeled dataset with 99
scenarios and ground truth, and a novel dataset of 1,087 diverse merge
scenarios that we have built. Experimental results show that RefFilter reduces
false positives by nearly 32% on the labeled dataset. While this reduction
comes with a non significant increase in false negatives, the overall gain in
precision significantly outweighs the minor trade-off in recall. These findings
demonstrate that refactoring-aware interference detection is a practical and
effective strategy for improving merge support in modern development workflows.

</details>


### [238] [Clarifying Semantics of In-Context Examples for Unit Test Generation](https://arxiv.org/abs/2510.01994)
*Chen Yang,Lin Yang,Ziqi Wang,Dong Wang,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: 提出CLAST技术优化单元测试语义清晰度，评估显示其优于UTgen，能提升基于ICL的单元测试生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有单元测试示例质量影响生成测试效果，需提升语义清晰度。

Method: 将复杂测试分解为逻辑更清晰的测试，结合程序分析和基于大语言模型的重写。

Result: CLAST在保留测试有效性和提升语义清晰度上优于UTgen，能提升基于ICL的单元测试生成效果。

Conclusion: CLAST在软件测试实践有潜在影响，为未来研究指明方向。

Abstract: Recent advances in large language models (LLMs) have enabled promising
performance in unit test generation through in-context learning (ICL). However,
the quality of in-context examples significantly influences the effectiveness
of generated tests-poorly structured or semantically unclear test examples
often lead to suboptimal outputs. In this paper, we propose CLAST, a novel
technique that systematically refines unit tests to improve their semantic
clarity, thereby enhancing their utility as in-context examples. The approach
decomposes complex tests into logically clearer ones and improves semantic
clarity through a combination of program analysis and LLM-based rewriting. We
evaluated CLAST on four open-source and three industrial projects. The results
demonstrate that CLAST largely outperforms UTgen, the state-of-the-art
refinement technique, in both preserving test effectiveness and enhancing
semantic clarity. Specifically, CLAST fully retains the original effectiveness
of unit tests, while UTgen reduces compilation success rate (CSR), pass rate
(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,
35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user
study preferred the semantic clarity of CLAST-refined tests. Notably,
incorporating CLAST-refined tests as examples effectively improves ICL-based
unit test generation approaches such as RAGGen and TELPA, resulting in an
average increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for
generated tests, compared to incorporating UTgen-refined tests. The insights
from the follow-up user study not only reinforce CLAST's potential impact in
software testing practice but also illuminate avenues for future research.

</details>


### [239] [Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision](https://arxiv.org/abs/2510.02002)
*Maximilian Kratz,Steffen Zschaler,Jens Kosiol,Gabriele Taentzer*

Main category: cs.SE

TL;DR: 本文探讨优化问题在上下文因素变化时的再优化挑战，提出用模型驱动工程（MDE）系统推导再优化问题，对变化问题分类并给出推导策略，还给出概念验证实现及应用示例。


<details>
  <summary>Details</summary>
Motivation: 解决优化问题在上下文因素变化时，原解决方案需适配的再优化挑战，现有新优化问题与原问题有显著区别。

Method: 采用模型驱动工程（MDE），运用声明式建模语言和模型转换对优化问题进行高层规范，对组合再优化问题的变化问题分类并推导对应再优化规范，基于GIPS工具实现概念验证。

Result: 给出了组合再优化问题变化问题的初始分类和推导再优化规范的策略，有基于GIPS工具的概念验证实现。

Conclusion: 模型驱动工程（MDE）为从原优化问题规范系统推导再优化问题提供了新机会。

Abstract: Once an optimisation problem has been solved, the solution may need
adaptation when contextual factors change. This challenge, also known as
reoptimisation, has been addressed in various problem domains, such as railway
crew rescheduling, nurse rerostering, or aircraft recovery. This requires a
modified problem to be solved again to ensure that the adapted solution is
optimal in the new context. However, the new optimisation problem differs
notably from the original problem: (i) we want to make only minimal changes to
the original solution to minimise the impact; (ii) we may be unable to change
some parts of the original solution (e.g., because they refer to past
allocations); and (iii) we need to derive a change script from the original
solution to the new solution. In this paper, we argue that Model-Driven
Engineering (MDE) - in particular, the use of declarative modelling languages
and model transformations for the high-level specification of optimisation
problems - offers new opportunities for the systematic derivation of
reoptimisation problems from the original optimisation problem specification.
We focus on combinatorial reoptimisation problems and provide an initial
categorisation of changing problems and strategies for deriving the
corresponding reoptimisation specifications. We introduce an initial
proof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer
Linear Programming Problem Specification) tool and apply it to an example
resource-allocation problem: the allocation of teaching assistants to teaching
sessions.

</details>


### [240] [ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column](https://arxiv.org/abs/2510.02007)
*Justus Bogner,Roberto Verdecchia*

Main category: cs.SE

TL;DR: 本文介绍新的ACM SIGSOFT SEN专栏（SEN - ESE），旨在讨论实证软件工程（ESE）研究的元方面，鼓励对ESE研究进行反思和改进。


<details>
  <summary>Details</summary>
Motivation: ESE研究虽有进展但仍需进化，存在研究可重复性、外部有效性等问题，且部分研究未明确记录，新手难掌握。

Method: 通过专家访谈、焦点小组、调查和立场文章等方式为专栏提供内容。

Result: 推出新的SEN - ESE专栏，提供讨论ESE研究元方面的平台。

Conclusion: 希望围绕社区兴趣塑造专栏，欢迎ESE社区就相关主题提供反馈和建议。

Abstract: From its early foundations in the 1970s, empirical software engineering (ESE)
has evolved into a mature research discipline that embraces a plethora of
different topics, methodologies, and industrial practices. Despite its
remarkable progress, the ESE research field still needs to keep evolving, as
new impediments, shortcoming, and technologies emerge. Research
reproducibility, limited external validity, subjectivity of reviews, and
porting research results to industrial practices are just some examples of the
drivers for improvements to ESE research. Additionally, several facets of ESE
research are not documented very explicitly, which makes it difficult for
newcomers to pick them up. With this new regular ACM SIGSOFT SEN column
(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,
ranging from general topics such as the nature and best practices for
replication packages, to more nuanced themes such as statistical methods,
interview transcription tools, and publishing interdisciplinary research. Our
aim for the column is to be a place where we can regularly spark conversations
on ESE topics that might not often be touched upon or are left implicit.
Contributions to this column will be grounded in expert interviews, focus
groups, surveys, and position pieces, with the goal of encouraging reflection
and improvement in how we conduct, communicate, teach, and ultimately improve
ESE research. Finally, we invite feedback from the ESE community on
challenging, controversial, or underexplored topics, as well as suggestions for
voices you would like to hear from. While we cannot promise to act on every
idea, we aim to shape this column around the community interests and are
grateful for all contributions.

</details>


### [241] [Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection](https://arxiv.org/abs/2510.02165)
*Peter Wauyo,Dalia Bwiza,Alain Murara,Edwin Mugume,Eric Umuhoza*

Main category: cs.SE

TL;DR: 研究提出多模态系统，用ViViT和AST模型，结合TFN架构分析公交CCTV和音频数据检测欺诈逃票，在自定义数据集上表现佳，支持实时检测。


<details>
  <summary>Details</summary>
Motivation: 为公共交通检测欺诈和逃票行为，减少运营商收入损失，提高乘客安全，确保运营合规。

Method: 采用ViViT模型提取视频特征，AST进行音频分析，实现TFN架构建模单模态和双模态交互。

Result: 在自定义数据集上检测欺诈活动准确率89.5%、精确率87.2%、召回率84.0%，优于早期融合基线和现有技术，张量融合比传统拼接F1分数提高7.0%、召回率提高8.8%。

Conclusion: 该解决方案支持实时检测，能帮助公共交通运营商减少损失、提升安全和确保合规。

Abstract: This research introduces a multimodal system designed to detect fraud and
fare evasion in public transportation by analyzing closed circuit television
(CCTV) and audio data. The proposed solution uses the Vision Transformer for
Video (ViViT) model for video feature extraction and the Audio Spectrogram
Transformer (AST) for audio analysis. The system implements a Tensor Fusion
Network (TFN) architecture that explicitly models unimodal and bimodal
interactions through a 2-fold Cartesian product. This advanced fusion technique
captures complex cross-modal dynamics between visual behaviors (e.g.,
tailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).
The system was trained and tested on a custom dataset, achieving an accuracy of
89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent
activities, significantly outperforming early fusion baselines and exceeding
the 75% recall rates typically reported in state-of-the-art transportation
fraud detection systems. Our ablation studies demonstrate that the tensor
fusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost
in recall compared to traditional concatenation methods. The solution supports
real-time detection, enabling public transport operators to reduce revenue
loss, improve passenger safety, and ensure operational compliance.

</details>


### [242] [SIEVE: Towards Verifiable Certification for Code-datasets](https://arxiv.org/abs/2510.02166)
*Fatou Ndiaye Mbodji,El-hacen Diallo,Jordan Samhi,Kui Liu,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: 现有公共代码数据集缺乏质量保证，论文提出社区驱动框架SIEVE，用可随时验证的证书替代说明卡，有望降低成本、增加信任。


<details>
  <summary>Details</summary>
Motivation: 公共代码数据集缺乏可验证的质量保证，现有静态说明卡无法审计且无统计保证，团队构建孤立清理管道分散精力、提高成本。

Method: 提出社区驱动框架SIEVE，将按属性检查转化为带有统计界限的机器可读、可验证的置信卡。

Result: 提出了SIEVE框架及将其完善的研究计划。

Conclusion: 这种转变有望降低代码数据集质量保证成本并增加信任。

Abstract: Code agents and empirical software engineering rely on public code datasets,
yet these datasets lack verifiable quality guarantees. Static 'dataset cards'
inform, but they are neither auditable nor do they offer statistical
guarantees, making it difficult to attest to dataset quality. Teams build
isolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We
present SIEVE, a community-driven framework. It turns per-property checks into
Confidence Cards-machine-readable, verifiable certificates with anytime-valid
statistical bounds. We outline a research plan to bring SIEVE to maturity,
replacing narrative cards with anytime-verifiable certification. This shift is
expected to lower quality-assurance costs and increase trust in code-datasets.

</details>


### [243] [TAIBOM: Bringing Trustworthiness to AI-Enabled Systems](https://arxiv.org/abs/2510.02169)
*Vadim Safronov,Anthony McCaigue,Nicholas Allott,Andrew Martin*

Main category: cs.SE

TL;DR: 当前软件供应链因开源软件和AI技术融合变复杂，现有SBOM框架难适应AI系统，本文提出TAIBOM框架解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有软件供应链管理方法难以应对AI系统特性，缺乏确保AI环境完整性、信任和合规性的工具。

Method: 引入Trusted AI Bill of Materials (TAIBOM) 框架，提供结构化依赖模型、完整性声明传播机制和信任认证流程。

Result: 展示了TAIBOM在AI工作流中支持保证、安全和合规性的能力，突出其优于现有标准。

Conclusion: 该工作为通过结构化软件透明性构建可信、可验证的AI系统奠定了基础。

Abstract: The growing integration of open-source software and AI-driven technologies
has introduced new layers of complexity into the software supply chain,
challenging existing methods for dependency management and system assurance.
While Software Bills of Materials (SBOMs) have become critical for enhancing
transparency and traceability, current frameworks fall short in capturing the
unique characteristics of AI systems -- namely, their dynamic, data-driven
nature and the loosely coupled dependencies across datasets, models, and
software components. These challenges are compounded by fragmented governance
structures and the lack of robust tools for ensuring integrity, trust, and
compliance in AI-enabled environments.
  In this paper, we introduce Trusted AI Bill of Materials (TAIBOM) -- a novel
framework extending SBOM principles to the AI domain. TAIBOM provides (i) a
structured dependency model tailored for AI components, (ii) mechanisms for
propagating integrity statements across heterogeneous AI pipelines, and (iii) a
trust attestation process for verifying component provenance. We demonstrate
how TAIBOM supports assurance, security, and compliance across AI workflows,
highlighting its advantages over existing standards such as SPDX and CycloneDX.
This work lays the foundation for trustworthy and verifiable AI systems through
structured software transparency.

</details>


### [244] [FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI](https://arxiv.org/abs/2510.02185)
*Paschal C. Amusuo,Dongge Liu,Ricardo Andres Calvo Mendez,Jonathan Metzman,Oliver Chang,James C. Davis*

Main category: cs.SE

TL;DR: 本文提出两种AI驱动策略减少OSS - Fuzz - Gen中误报，在OSS - Fuzz基准函数测试中效果良好，凸显AI融入大规模模糊测试管道的前景与挑战。


<details>
  <summary>Details</summary>
Motivation: 现有自动生成模糊驱动方法常导致误报崩溃，在行业规模模糊驱动生成中影响对系统和团队的信任，需解决此问题。

Method: 提出基于约束的模糊驱动生成和基于上下文的崩溃验证两种策略。

Result: 在1500个OSS - Fuzz基准函数测试中，策略使虚假崩溃减少达8%，报告崩溃减少超一半，证明前沿大语言模型可作为可靠程序分析代理。

Conclusion: 强调了将AI集成到大规模模糊测试管道的前景和挑战。

Abstract: Fuzz testing has become a cornerstone technique for identifying software bugs
and security vulnerabilities, with broad adoption in both industry and
open-source communities. Directly fuzzing a function requires fuzz drivers,
which translate random fuzzer inputs into valid arguments for the target
function. Given the cost and expertise required to manually develop fuzz
drivers, methods exist that leverage program analysis and Large Language Models
to automatically generate these drivers. However, the generated fuzz drivers
frequently lead to false positive crashes, especially in functions highly
structured input and complex state requirements. This problem is especially
crucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as
reporting false positive crashes to maintainers impede trust in both the system
and the team.
  This paper presents two AI-driven strategies to reduce false positives in
OSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,
constraint-based fuzz driver generation proactively enforces constraints on a
function's inputs and state to guide driver creation. Second, context-based
crash validation reactively analyzes function callers to determine whether
reported crashes are feasible from program entry points. Using 1,500 benchmark
functions from OSS-Fuzz, we show that these strategies reduce spurious crashes
by up to 8%, cut reported crashes by more than half, and demonstrate that
frontier LLMs can serve as reliable program analysis agents. Our results
highlight the promise and challenges of integrating AI into large-scale fuzzing
pipelines.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [245] [Fast and explicit European option pricing under tempered stable processes](https://arxiv.org/abs/2510.01211)
*Gaetano Agazzotti,Jean-Philippe Aguilar*

Main category: q-fin.CP

TL;DR: 本文给出了缓和稳定密度和指数Lévy模型中欧式合约价格的级数展开式，方法无超参数，数值分析显示有竞争力。


<details>
  <summary>Details</summary>
Motivation: 提供缓和稳定密度和指数Lévy模型中欧式合约价格的级数展开式，避免传统方法超参数问题。

Method: 推导级数展开式。

Result: 公式可恢复多个流行期权定价模型，特定情况下很简单，方法无超参数。

Conclusion: 该技术与现有定价方法有竞争力。

Abstract: We provide series expansions for the tempered stable densities and for the
price of European-style contracts in the exponential L\'evy model driven by the
tempered stable process. These formulas recover several popular option pricing
models, and become particularly simple in some specific cases such as bilateral
Gamma process and one-sided TS process. When compared to traditional Fourier
pricing, our method has the advantage of being hyperparameter free. We also
provide a detailed numerical analysis and show that our technique is
competitive with state-of-the-art pricing methods.

</details>


### [246] [Can Machine Learning Algorithms Outperform Traditional Models for Option Pricing?](https://arxiv.org/abs/2510.01446)
*Georgy Milyushkov*

Main category: q-fin.CP

TL;DR: 研究机器学习技术用于期权定价，对比传统模型，发现机器学习模型有优势。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习技术在期权定价中的应用，对比其与传统模型的优劣。

Method: 使用神经网络、随机森林和CatBoost等机器学习模型，结合合成数据和真实市场期权数据，评估各模型预测期权价格的能力。

Result: 机器学习模型能捕捉期权价格复杂非线性关系，在多方面优于Black - Scholes和Heston模型。

Conclusion: 数据驱动方法有提高定价准确性、反映市场动态的潜力。

Abstract: This study investigates the application of machine learning techniques,
specifically Neural Networks, Random Forests, and CatBoost for option pricing,
in comparison to traditional models such as Black-Scholes and Heston Model.
Using both synthetically generated data and real market option data, each model
is evaluated in predicting the option price. The results show that machine
learning models can capture complex, non-linear relationships in option prices
and, in several cases, outperform both Black-Scholes and Heston models. These
findings highlight the potential of data-driven methods to improve pricing
accuracy and better reflect market dynamics.

</details>


### [247] [FINCH: Financial Intelligence using Natural language for Contextualized SQL Handling](https://arxiv.org/abs/2510.01887)
*Avinash Kumar Singh,Bhaskarjit Sarmah,Stefano Pasquali*

Main category: q-fin.CP

TL;DR: 本文介绍了金融领域Text - to - SQL任务的挑战，提出构建金融数据集FINCH，对推理模型和语言模型进行基准测试，并提出评估指标FINCH Score。


<details>
  <summary>Details</summary>
Motivation: 金融领域Text - to - SQL任务因复杂模式、特定术语和高错误成本而困难，且缺乏大规模金融领域数据集。

Method: 构建包含292个表和75725个自然语言 - SQL对的金融数据集FINCH，对不同规模的推理模型和语言模型进行基准测试，提出Finance - oriented评估指标FINCH Score。

Result: 创建了FINCH数据集，对模型进行系统分析，提出新评估指标。

Conclusion: FINCH数据集和FINCH Score评估指标有助于推动金融领域Text - to - SQL研究。

Abstract: Text-to-SQL, the task of translating natural language questions into SQL
queries, has long been a central challenge in NLP. While progress has been
significant, applying it to the financial domain remains especially difficult
due to complex schema, domain-specific terminology, and high stakes of error.
Despite this, there is no dedicated large-scale financial dataset to advance
research, creating a critical gap. To address this, we introduce a curated
financial dataset (FINCH) comprising 292 tables and 75,725 natural language-SQL
pairs, enabling both fine-tuning and rigorous evaluation. Building on this
resource, we benchmark reasoning models and language models of varying scales,
providing a systematic analysis of their strengths and limitations in financial
Text-to-SQL tasks. Finally, we propose a finance-oriented evaluation metric
(FINCH Score) that captures nuances overlooked by existing measures, offering a
more faithful assessment of model performance.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [248] [Mamba Outpaces Reformer in Stock Prediction with Sentiments from Top Ten LLMs](https://arxiv.org/abs/2510.01203)
*Lokesh Antony Kadiyala,Amir Mirzaeinia*

Main category: q-fin.ST

TL;DR: 研究提出结合大语言模型语义情感得分与日内股价数据的框架，提升分钟级股票预测准确性，Mamba表现优于Reformer。


<details>
  <summary>Details</summary>
Motivation: 股市短期因高波动性、新闻影响和金融时间序列非线性难以预测，需提升分钟级预测准确性。

Method: 构建AAPL新闻文章与股价的时间对齐数据集，用多个大语言模型进行情感分析，将情感得分与价格、技术指标结合，用Reformer和Mamba分别训练，用Optuna优化超参数。

Result: Mamba在测试的10个大语言模型中表现均优于Reformer，与LLaMA 3.3 - 70B结合时误差最低为0.137，Reformer能捕捉趋势但会过度平滑突然变化。

Conclusion: 整合基于大语言模型的语义分析和高效时间建模可提升实时金融预测能力。

Abstract: The stock market is extremely difficult to predict in the short term due to
high market volatility, changes caused by news, and the non-linear nature of
the financial time series. This research proposes a novel framework for
improving minute-level prediction accuracy using semantic sentiment scores from
top ten different large language models (LLMs) combined with minute interval
intraday stock price data. We systematically constructed a time-aligned dataset
of AAPL news articles and 1-minute Apple Inc. (AAPL) stock prices for the dates
of April 4 to May 2, 2025. The sentiment analysis was achieved using the
DeepSeek-V3, GPT variants, LLaMA, Claude, Gemini, Qwen, and Mistral models
through their APIs. Each article obtained sentiment scores from all ten LLMs,
which were scaled to a [0, 1] range and combined with prices and technical
indicators like RSI, ROC, and Bollinger Band Width. Two state-of-the-art such
as Reformer and Mamba were trained separately on the dataset using the
sentiment scores produced by each LLM as input. Hyper parameters were optimized
by means of Optuna and were evaluated through a 3-day evaluation period.
Reformer had mean squared error (MSE) or the evaluation metrics, and it should
be noted that Mamba performed not only faster but also better than Reformer for
every LLM across the 10 LLMs tested. Mamba performed best with LLaMA 3.3--70B,
with the lowest error of 0.137. While Reformer could capture broader trends
within the data, the model appeared to over smooth sudden changes by the LLMs.
This study highlights the potential of integrating LLM-based semantic analysis
paired with efficient temporal modeling to enhance real-time financial
forecasting.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [249] [Mean-field theory of the Santa Fe model revisited: a systematic derivation from an exact BBGKY hierarchy for the zero-intelligence limit-order book model](https://arxiv.org/abs/2510.01814)
*Taiki Wakatsuki,Kiyoshi Kanazawa*

Main category: q-fin.TR

TL;DR: 本文从动力学理论角度重新研究圣达菲模型的平均场理论，推导相关方程并得到显式解，指出前人研究的不足和错误。


<details>
  <summary>Details</summary>
Motivation: 前人对圣达菲模型平均场理论的研究缺乏坚实数学基础，推导有启发式但未明确求解。

Method: 研究圣达菲模型的精确主方程，系统推导BBGKY层级方程，应用平均场近似推导序单簿密度分布的平均场方程。

Result: 得到平均场解的显式封闭表达式，给出不同市场订单提交强度下的渐近标度公式，获得前人启发式推导的解，指出前人在扩散常数标度律上的错误。

Conclusion: 从动力学理论角度研究圣达菲模型平均场理论是有效的，为相关研究提供了坚实数学基础，同时指出前人研究的问题。

Abstract: The Santa Fe model is an established econophysics model for describing
stochastic dynamics of the limit order book from the viewpoint of the
zero-intelligence approach. While its foundation was studied by combining a
dimensional analysis and a mean-field theory by E. Smith et al. in Quantitative
Finance 2003, their arguments are rather heuristic and lack solid mathematical
foundation; indeed, their mean-field equations were derived with heuristic
arguments and their solutions were not explicitly obtained. In this work, we
revisit the mean-field theory of the Santa Fe model from the viewpoint of
kinetic theory -- a traditional mathematical program in statistical physics. We
study the exact master equation for the Santa Fe model and systematically
derive the Bogoliubov-Born-Green-Kirkwood-Yvon (BBGKY) hierarchical equation.
By applying the mean-field approximation, we derive the mean-field equation for
the order-book density profile, parallel to the Boltzmann equation in
conventional statistical physics. Furthermore, we obtain explicit and closed
expression of the mean-field solutions. Our solutions have several
implications: (1)Our scaling formulas are available for both $\mu\to 0$ and
$\mu\to \infty$ asymptotics, where $\mu$ is the market-order submission
intensity. Particularly, the mean-field theory works very well for small $\mu$,
while its validity is partially limited for large $\mu$. (2)The ``method of
image'' solution, heuristically derived by Bouchaud-M\'ezard-Potters in
Quantitative Finance 2002, is obtained for large $\mu$, serving as a
mathematical foundation for their heuristic arguments. (3)Finally, we point out
an error in E. Smith et al. 2003 in the scaling law for the diffusion constant
due to a misspecification in their dimensional analysis.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [250] [Private Realizable-to-Agnostic Transformation with Near-Optimal Sample Complexity](https://arxiv.org/abs/2510.01291)
*Bo Li,Wei Wang,Peng Ye*

Main category: stat.ML

TL;DR: 本文改进可实现到不可知转换构造，消除对ε依赖，获近最优额外样本复杂度，解决开放问题。


<details>
  <summary>Details</summary>
Motivation: 原转换方法在ε任意时样本复杂度含1/ε因子，非最优，需改进。

Method: 给出改进构造消除对ε的依赖。

Result: 获得近最优额外样本复杂度，揭示私有不可知学习中隐私成本仅对可实现部分显著，解决私有预测问题的开放问题。

Conclusion: 改进构造有效，能消除对ε依赖，解决相关问题。

Abstract: The realizable-to-agnostic transformation (Beimel et al., 2015; Alon et al.,
2020) provides a general mechanism to convert a private learner in the
realizable setting (where the examples are labeled by some function in the
concept class) to a private learner in the agnostic setting (where no
assumptions are imposed on the data). Specifically, for any concept class
$\mathcal{C}$ and error parameter $\alpha$, a private realizable learner for
$\mathcal{C}$ can be transformed into a private agnostic learner while only
increasing the sample complexity by
$\widetilde{O}(\mathrm{VC}(\mathcal{C})/\alpha^2)$, which is essentially tight
assuming a constant privacy parameter $\varepsilon = \Theta(1)$. However, when
$\varepsilon$ can be arbitrary, one has to apply the standard
privacy-amplification-by-subsampling technique (Kasiviswanathan et al., 2011),
resulting in a suboptimal extra sample complexity of
$\widetilde{O}(\mathrm{VC}(\mathcal{C})/\alpha^2\varepsilon)$ that involves a
$1/\varepsilon$ factor.
  In this work, we give an improved construction that eliminates the dependence
on $\varepsilon$, thereby achieving a near-optimal extra sample complexity of
$\widetilde{O}(\mathrm{VC}(\mathcal{C})/\alpha^2)$ for any $\varepsilon\le 1$.
Moreover, our result reveals that in private agnostic learning, the privacy
cost is only significant for the realizable part. We also leverage our
technique to obtain a nearly tight sample complexity bound for the private
prediction problem, resolving an open question posed by Dwork and Feldman
(2018) and Dagan and Feldman (2020).

</details>


### [251] [Continuously Augmented Discrete Diffusion model for Categorical Generative Modeling](https://arxiv.org/abs/2510.01329)
*Huangjie Zheng,Shansan Gong,Ruixiang Zhang,Tianrong Chen,Jiatao Gu,Mingyuan Zhou,Navdeep Jaitly,Yizhe Zhang*

Main category: stat.ML

TL;DR: 提出连续增强离散扩散（CADD）框架，增强离散状态空间，实验表明其在多领域提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 标准离散扩散模型将未观察到的状态映射到吸收标记，导致去噪步骤间语义信息丢失，形成‘信息空洞’。

Method: 引入CADD框架，用连续潜在空间中的配对扩散增强离散状态空间，在反向步骤利用连续潜在作为语义提示指导离散去噪。

Result: CADD在文本生成、图像合成和代码建模中提高了生成质量，在定性和定量指标上均有提升。

Conclusion: CADD框架设计简洁，与现有离散扩散训练兼容，能在模式覆盖和模式寻求行为间进行权衡，有效提升生成质量。

Abstract: Standard discrete diffusion models treat all unobserved states identically by
mapping them to an absorbing [MASK] token. This creates an 'information void'
where semantic information that could be inferred from unmasked tokens is lost
between denoising steps. We introduce Continuously Augmented Discrete Diffusion
(CADD), a framework that augments the discrete state space with a paired
diffusion in a continuous latent space. This yields graded, gradually corrupted
states in which masked tokens are represented by noisy yet informative latent
vectors rather than collapsed 'information voids'. At each reverse step, CADD
may leverage the continuous latent as a semantic hint to guide discrete
denoising. The design is clean and compatible with existing discrete diffusion
training. At sampling time, the strength and choice of estimator for the
continuous latent vector enables a controlled trade-off between mode-coverage
(generating diverse outputs) and mode-seeking (generating contextually precise
outputs) behaviors. Empirically, we demonstrate CADD improves generative
quality over mask-based diffusion across text generation, image synthesis, and
code modeling, with consistent gains on both qualitative and quantitative
metrics against strong discrete baselines.

</details>


### [252] [Risk Phase Transitions in Spiked Regression: Alignment Driven Benign and Catastrophic Overfitting](https://arxiv.org/abs/2510.01414)
*Jiping Li,Rishi Sonthalia*

Main category: stat.ML

TL;DR: 本文利用尖峰协方差数据模型分析线性回归中最小范数插值解的泛化误差，对过拟合状态分类，揭示尖峰强度和目标 - 尖峰对齐的影响。


<details>
  <summary>Details</summary>
Motivation: 研究尖峰协方差数据模型下线性回归最小范数插值解的泛化误差，以及尖峰强度和目标 - 尖峰对齐对风险的影响。

Method: 推导泛化误差的精确表达式，基于尖峰强度、纵横比和目标对齐情况对过拟合状态进行分类。

Result: 得到泛化误差精确表达式，对过拟合状态分类；在特定问题中，增加尖峰强度会先导致灾难性过拟合；发现目标 - 尖峰对齐不总是有利，且这种不利情况在非线性模型中也存在。

Conclusion: 明确尖峰强度和目标 - 尖峰对齐对线性回归泛化误差和过拟合状态的影响，部分结论在非线性模型中也适用。

Abstract: This paper analyzes the generalization error of minimum-norm interpolating
solutions in linear regression using spiked covariance data models. The paper
characterizes how varying spike strengths and target-spike alignments can
affect risk, especially in overparameterized settings. The study presents an
exact expression for the generalization error, leading to a comprehensive
classification of benign, tempered, and catastrophic overfitting regimes based
on spike strength, the aspect ratio $c=d/n$ (particularly as $c \to \infty$),
and target alignment. Notably, in well-specified aligned problems, increasing
spike strength can surprisingly induce catastrophic overfitting before
achieving benign overfitting. The paper also reveals that target-spike
alignment is not always advantageous, identifying specific, sometimes
counterintuitive, conditions for its benefit or detriment. Alignment with the
spike being detrimental is empirically demonstrated to persist in nonlinear
models.

</details>


### [253] [AI Foundation Model for Time Series with Innovations Representation](https://arxiv.org/abs/2510.01560)
*Lang Tong,Xinyi Wang*

Main category: stat.ML

TL;DR: 本文提出适用于工程应用时间序列的AI基础模型TS - GPT，以解决实时监测和控制中的因果操作问题，并通过美国独立系统运营商的历史数据验证其在预测实时边际电价方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 工程时间序列遵循物理规律，基于大语言模型的AI基础模型可能无效或低效，需要适用于工程应用中时间序列实时监测和控制的模型。

Method: 基于Wiener、Kallianpur和Rosenblatt的经典创新表示理论，提出基于创新表示的生成式预训练Transformer TS - GPT，并以概率生成式预测为例进行模型适配。

Result: 利用美国独立系统运营商的历史数据，证明了TS - GPT在预测实时边际电价方面的有效性。

Conclusion: TS - GPT是适用于工程应用中时间序列实时监测和控制的有效AI基础模型。

Abstract: This paper introduces an Artificial Intelligence (AI) foundation model for
time series in engineering applications, where causal operations are required
for real-time monitoring and control. Since engineering time series are
governed by physical, rather than linguistic, laws, large-language-model-based
AI foundation models may be ineffective or inefficient. Building on the
classical innovations representation theory of Wiener, Kallianpur, and
Rosenblatt, we propose Time Series GPT (TS-GPT) -- an
innovations-representation-based Generative Pre-trained Transformer for
engineering monitoring and control. As an example of foundation model
adaptation, we consider Probabilistic Generative Forecasting, which produces
future time series samples from conditional probability distributions given
past realizations. We demonstrate the effectiveness of TS-GPT in forecasting
real-time locational marginal prices using historical data from U.S.
independent system operators.

</details>


### [254] [A reproducible comparative study of categorical kernels for Gaussian process regression, with new clustering-based nested kernels](https://arxiv.org/abs/2510.01840)
*Raphaël Carpintero Perez,Sébastien Da Veiga,Josselin Garnier*

Main category: stat.ML

TL;DR: 本文对现有分类核进行可复现的对比研究，提出新评估指标，发现嵌套核方法在有组结构数据集上表现好，还提出基于聚类策略在无已知组结构数据集上也有优势。


<details>
  <summary>Details</summary>
Motivation: 设计分类核是高斯过程回归的挑战，此前研究难以确定首选方法，且缺乏可复现代码，故进行可复现的对比研究。

Method: 对现有分类核进行可复现对比研究，提出受优化社区启发的新评估指标，在无已知组结构时提出基于聚类的目标编码策略。

Result: 在有组结构数据集上，嵌套核方法明显优于其他方法；在无已知组结构的大量数据集上，新的基于聚类策略仍优于其他方法且计算成本低。

Conclusion: 嵌套核方法在有组结构数据表现佳，新的基于聚类策略在无已知组结构数据有优势且计算成本低。

Abstract: Designing categorical kernels is a major challenge for Gaussian process
regression with continuous and categorical inputs. Despite previous studies, it
is difficult to identify a preferred method, either because the evaluation
metrics, the optimization procedure, or the datasets change depending on the
study. In particular, reproducible code is rarely available. The aim of this
paper is to provide a reproducible comparative study of all existing
categorical kernels on many of the test cases investigated so far. We also
propose new evaluation metrics inspired by the optimization community, which
provide quantitative rankings of the methods across several tasks. From our
results on datasets which exhibit a group structure on the levels of
categorical inputs, it appears that nested kernels methods clearly outperform
all competitors. When the group structure is unknown or when there is no prior
knowledge of such a structure, we propose a new clustering-based strategy using
target encodings of categorical variables. We show that on a large panel of
datasets, which do not necessarily have a known group structure, this
estimation strategy still outperforms other approaches while maintaining low
computational cost.

</details>


### [255] [Deep Hedging Under Non-Convexity: Limitations and a Case for AlphaZero](https://arxiv.org/abs/2510.01874)
*Matteo Maggiolo,Giuseppe Nuti,Miroslav Štrupl,Oleg Szehr*

Main category: stat.ML

TL;DR: 本文研究不完全市场复制投资组合构建，引入基于AlphaZero的系统与深度对冲对比，表明深度对冲在非凸环境有局限，AlphaZero能找到近最优策略且样本效率更高。


<details>
  <summary>Details</summary>
Motivation: 解决金融工程中不完全市场复制投资组合构建问题，应用于定价、对冲等领域。

Method: 将其建模为投资者与市场的两人博弈，引入基于AlphaZero的系统，并与基于梯度下降的深度对冲方法对比。

Result: 深度对冲在非凸环境中收敛到局部最优，AlphaZero能持续找到近最优复制策略，且样本效率更高。

Conclusion: 深度对冲有效性依赖凸性假设，AlphaZero在非凸环境及数据稀缺市场有优势。

Abstract: This paper examines replication portfolio construction in incomplete markets
- a key problem in financial engineering with applications in pricing, hedging,
balance sheet management, and energy storage planning. We model this as a
two-player game between an investor and the market, where the investor makes
strategic bets on future states while the market reveals outcomes. Inspired by
the success of Monte Carlo Tree Search in stochastic games, we introduce an
AlphaZero-based system and compare its performance to deep hedging - a widely
used industry method based on gradient descent. Through theoretical analysis
and experiments, we show that deep hedging struggles in environments where the
$Q$-function is not subject to convexity constraints - such as those involving
non-convex transaction costs, capital constraints, or regulatory limitations -
converging to local optima. We construct specific market environments to
highlight these limitations and demonstrate that AlphaZero consistently finds
near-optimal replication strategies. On the theoretical side, we establish a
connection between deep hedging and convex optimization, suggesting that its
effectiveness is contingent on convexity assumptions. Our experiments further
suggest that AlphaZero is more sample-efficient - an important advantage in
data-scarce, overfitting-prone derivative markets.

</details>


### [256] [Precise Dynamics of Diagonal Linear Networks: A Unifying Analysis by Dynamical Mean-Field Theory](https://arxiv.org/abs/2510.01930)
*Sota Nishiyama,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 本文对对角线性网络（DLNs）梯度流动力学中的各种现象进行统一分析，用动态平均场理论（DMFT）推导有效过程，得到新见解并重现先前现象。


<details>
  <summary>Details</summary>
Motivation: 以往对DLNs中一些现象孤立研究，整体动力学理解不足，需统一分析。

Method: 使用动态平均场理论（DMFT）推导低维有效过程来捕捉高维渐近梯度流动力学。

Result: 分析有效过程得到DLN动力学新见解，包括损失收敛率及其与泛化的权衡，重现先前观察到的现象。

Conclusion: 研究加深了对DLNs的理解，证明了DMFT方法在分析神经网络高维学习动力学中的有效性。

Abstract: Diagonal linear networks (DLNs) are a tractable model that captures several
nontrivial behaviors in neural network training, such as
initialization-dependent solutions and incremental learning. These phenomena
are typically studied in isolation, leaving the overall dynamics insufficiently
understood. In this work, we present a unified analysis of various phenomena in
the gradient flow dynamics of DLNs. Using Dynamical Mean-Field Theory (DMFT),
we derive a low-dimensional effective process that captures the asymptotic
gradient flow dynamics in high dimensions. Analyzing this effective process
yields new insights into DLN dynamics, including loss convergence rates and
their trade-off with generalization, and systematically reproduces many of the
previously observed phenomena. These findings deepen our understanding of DLNs
and demonstrate the effectiveness of the DMFT approach in analyzing
high-dimensional learning dynamics of neural networks.

</details>


### [257] [Uniform-in-time convergence bounds for Persistent Contrastive Divergence Algorithms](https://arxiv.org/abs/2510.01944)
*Paul Felix Valsecchi Oliva,O. Deniz Akyildiz,Andrew Duncan*

Main category: stat.ML

TL;DR: 提出用于未归一化密度最大似然估计的连续时间持久对比散度（PCD）公式，有误差界并给出高效实现方法。


<details>
  <summary>Details</summary>
Motivation: 为未归一化密度的最大似然估计提出新的PCD方法。

Method: 将PCD表示为耦合的多尺度随机微分方程系统，推导误差界，利用S - ROCK积分器实现。

Result: 得到PCD迭代与MLE解的误差显式界，实现有显式误差估计的连续时间方案。

Conclusion: 提出带显式误差保证的训练基于能量模型的新方法。

Abstract: We propose a continuous-time formulation of persistent contrastive divergence
(PCD) for maximum likelihood estimation (MLE) of unnormalised densities. Our
approach expresses PCD as a coupled, multiscale system of stochastic
differential equations (SDEs), which perform optimisation of the parameter and
sampling of the associated parametrised density, simultaneously.
  From this novel formulation, we are able to derive explicit bounds for the
error between the PCD iterates and the MLE solution for the model parameter.
This is made possible by deriving uniform-in-time (UiT) bounds for the
difference in moments between the multiscale system and the averaged regime. An
efficient implementation of the continuous-time scheme is introduced,
leveraging a class of explicit, stable intregators, stochastic orthogonal
Runge-Kutta Chebyshev (S-ROCK), for which we provide explicit error estimates
in the long-time regime. This leads to a novel method for training energy-based
models (EBMs) with explicit error guarantees.

</details>


### [258] [Adaptive Kernel Selection for Stein Variational Gradient Descent](https://arxiv.org/abs/2510.02067)
*Moritz Melcher,Simon Weissmann,Ashia C. Wilson,Jakob Zech*

Main category: stat.ML

TL;DR: 提出Ad - SVGD方法，自适应选择核参数，理论分析有收敛性，实证表现优于标准启发式方法。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推理中高效近似后验分布是挑战，SVGD对核函数选择敏感，常用的中位数启发式方法缺乏灵活性，表现不佳。

Method: 提出Ad - SVGD方法，在通过SVGD更新粒子和通过梯度上升调整核带宽之间交替进行，并进行简化的理论分析。

Result: Ad - SVGD在各种任务中始终优于标准启发式方法。

Conclusion: Ad - SVGD方法通过自适应选择核参数，能在贝叶斯推理中更好地近似后验分布，具有较好的性能。

Abstract: A central challenge in Bayesian inference is efficiently approximating
posterior distributions. Stein Variational Gradient Descent (SVGD) is a popular
variational inference method which transports a set of particles to approximate
a target distribution. The SVGD dynamics are governed by a reproducing kernel
Hilbert space (RKHS) and are highly sensitive to the choice of the kernel
function, which directly influences both convergence and approximation quality.
The commonly used median heuristic offers a simple approach for setting kernel
bandwidths but lacks flexibility and often performs poorly, particularly in
high-dimensional settings. In this work, we propose an alternative strategy for
adaptively choosing kernel parameters over an abstract family of kernels.
Recent convergence analyses based on the kernelized Stein discrepancy (KSD)
suggest that optimizing the kernel parameters by maximizing the KSD can improve
performance. Building on this insight, we introduce Adaptive SVGD (Ad-SVGD), a
method that alternates between updating the particles via SVGD and adaptively
tuning kernel bandwidths through gradient ascent on the KSD. We provide a
simplified theoretical analysis that extends existing results on minimizing the
KSD for fixed kernels to our adaptive setting, showing convergence properties
for the maximal KSD over our kernel class. Our empirical results further
support this intuition: Ad-SVGD consistently outperforms standard heuristics in
a variety of tasks.

</details>


### [259] [Non-Asymptotic Analysis of Data Augmentation for Precision Matrix Estimation](https://arxiv.org/abs/2510.02119)
*Lucas Morisset,Adrien Hardy,Alain Durmus*

Main category: stat.ML

TL;DR: 本文聚焦高维环境下逆协方差矩阵估计问题，研究两类估计器，推导估计量并给出二次误差集中界，用随机矩阵理论分析，有数值实验支持。


<details>
  <summary>Details</summary>
Motivation: 解决高维环境下逆协方差矩阵估计问题，对不同估计器进行比较和超参数调优。

Method: 运用随机矩阵理论工具，引入广义预解矩阵的新型确定性等价式。

Result: 为两类估计器推导估计量并给出二次误差集中界，能用于方法比较和超参数调优。

Conclusion: 理论结果得到数值实验支持，所提方法可解决高维逆协方差矩阵估计问题。

Abstract: This paper addresses the problem of inverse covariance (also known as
precision matrix) estimation in high-dimensional settings. Specifically, we
focus on two classes of estimators: linear shrinkage estimators with a target
proportional to the identity matrix, and estimators derived from data
augmentation (DA). Here, DA refers to the common practice of enriching a
dataset with artificial samples--typically generated via a generative model or
through random transformations of the original data--prior to model fitting.
For both classes of estimators, we derive estimators and provide concentration
bounds for their quadratic error. This allows for both method comparison and
hyperparameter tuning, such as selecting the optimal proportion of artificial
samples. On the technical side, our analysis relies on tools from random matrix
theory. We introduce a novel deterministic equivalent for generalized resolvent
matrices, accommodating dependent samples with specific structure. We support
our theoretical results with numerical experiments.

</details>


### [260] [Hybrid Physics-ML Framework for Pan-Arctic Permafrost Infrastructure Risk at Record 2.9-Million Observation Scale](https://arxiv.org/abs/2510.02189)
*Boris Kriuk*

Main category: stat.ML

TL;DR: 提出混合物理 - 机器学习框架评估北极基础设施永久冻土风险，有高准确性和不确定性量化，成果可用于工程设计和适应规划。


<details>
  <summary>Details</summary>
Motivation: 现有北极永久冻土风险评估框架缺乏时空验证、不确定性量化和操作决策支持能力，需改进。

Method: 整合290万观测数据，用堆叠集成模型（随机森林 + 直方图梯度提升 + 弹性网络），结合学习的气候 - 永久冻土关系与物理永久冻土敏感性模型。

Result: 模型R2 = 0.980（RMSE = 5.01 pp），预测RCP8.5情景下永久冻土分数下降，识别基础设施风险区并提供不确定性地图。

Conclusion: 该框架是全球最大验证的永久冻土机器学习数据集，提供首个北极基础设施操作混合预测系统，方法可推广。

Abstract: Arctic warming threatens over 100 billion in permafrost-dependent
infrastructure across Northern territories, yet existing risk assessment
frameworks lack spatiotemporal validation, uncertainty quantification, and
operational decision-support capabilities. We present a hybrid physics-machine
learning framework integrating 2.9 million observations from 171,605 locations
(2005-2021) combining permafrost fraction data with climate reanalysis. Our
stacked ensemble model (Random Forest + Histogram Gradient Boosting + Elastic
Net) achieves R2=0.980 (RMSE=5.01 pp) with rigorous spatiotemporal
cross-validation preventing data leakage. To address machine learning
limitations in extrapolative climate scenarios, we develop a hybrid approach
combining learned climate-permafrost relationships (60%) with physical
permafrost sensitivity models (40%, -10 pp/C). Under RCP8.5 forcing (+5C over
10 years), we project mean permafrost fraction decline of -20.3 pp (median:
-20.0 pp), with 51.5% of Arctic Russia experiencing over 20 percentage point
loss. Infrastructure risk classification identifies 15% high-risk zones (25%
medium-risk) with spatially explicit uncertainty maps. Our framework represents
the largest validated permafrost ML dataset globally, provides the first
operational hybrid physics-ML forecasting system for Arctic infrastructure, and
delivers open-source tools enabling probabilistic permafrost projections for
engineering design codes and climate adaptation planning. The methodology is
generalizable to other permafrost regions and demonstrates how hybrid
approaches can overcome pure data-driven limitations in climate change
applications.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [261] [Efficient Probabilistic Visualization of Local Divergence of 2D Vector Fields with Independent Gaussian Uncertainty](https://arxiv.org/abs/2510.01190)
*Timbwaoga A. J. Ouermi,Eric Li,Kenneth Moreland,Dave Pugmire,Chris R. Johnson,Tushar M. Athawale*

Main category: stat.CO

TL;DR: 本文提出高效准确的二维矢量场局部散度不确定性可视化方法，在串行和并行算法上均优于经典蒙特卡罗方法。


<details>
  <summary>Details</summary>
Motivation: 矢量场数据固有不确定性会导致散度计算错误，影响下游分析，经典蒙特卡罗方法收敛慢、扩展性差。

Method: 推导封闭形式方法进行局部散度不确定性可视化，将其集成到Viskores并行库加速可视化。

Result: 串行分析算法加速达1946倍，并行Viskores算法加速达19698倍，概率散度可视化质量优于传统方法。

Conclusion: 方法在风预报和海洋模拟数据集上验证了准确性和高效性。

Abstract: This work focuses on visualizing uncertainty of local divergence of
two-dimensional vector fields. Divergence is one of the fundamental attributes
of fluid flows, as it can help domain scientists analyze potential positions of
sources (positive divergence) and sinks (negative divergence) in the flow.
However, uncertainty inherent in vector field data can lead to erroneous
divergence computations, adversely impacting downstream analysis. While Monte
Carlo (MC) sampling is a classical approach for estimating divergence
uncertainty, it suffers from slow convergence and poor scalability with
increasing data size and sample counts. Thus, we present a two-fold
contribution that tackles the challenges of slow convergence and limited
scalability of the MC approach. (1) We derive a closed-form approach for highly
efficient and accurate uncertainty visualization of local divergence, assuming
independently Gaussian-distributed vector uncertainties. (2) We further
integrate our approach into Viskores, a platform-portable parallel library, to
accelerate uncertainty visualization. In our results, we demonstrate
significantly enhanced efficiency and accuracy of our serial analytical
(speed-up up to 1946X) and parallel Viskores (speed-up up to 19698X) algorithms
over the classical serial MC approach. We also demonstrate qualitative
improvements of our probabilistic divergence visualizations over traditional
mean-field visualization, which disregards uncertainty. We validate the
accuracy and efficiency of our methods on wind forecast and ocean simulation
datasets.

</details>


### [262] [Knots and variance ordering of sequential Monte Carlo algorithms](https://arxiv.org/abs/2510.01901)
*Joshua J Bon,Anthony Lee*

Main category: stat.CO

TL;DR: 本文介绍了新的方差缩减技术——结算子，提升粒子滤波效率，探讨与现有策略联系并给出量化结果，还对全适应粒子滤波做了改进。


<details>
  <summary>Details</summary>
Motivation: 为了提高粒子滤波算法在近似难以处理积分时的效率。

Method: 引入结算子，将势函数信息融入转移核的部分或全部，诱导费曼 - 卡茨模型的偏序关系。

Result: 给出粒子滤波渐近方差的偏序，推广现有技术并提供量化结果，对全适应粒子滤波做小修改保证渐近方差排序。

Conclusion: 结算子为粒子滤波算法设计提供了新方法，能有效提升效率。

Abstract: Sequential Monte Carlo algorithms, or particle filters, are widely used for
approximating intractable integrals, particularly those arising in Bayesian
inference and state-space models. We introduce a new variance reduction
technique, the knot operator, which improves the efficiency of particle filters
by incorporating potential function information into part, or all, of a
transition kernel. The knot operator induces a partial ordering of Feynman-Kac
models that implies an order on the asymptotic variance of particle filters,
offering a new approach to algorithm design. We discuss connections to existing
strategies for designing efficient particle filters, including model
marginalisation. Our theory generalises such techniques and provides
quantitative asymptotic variance ordering results. We revisit the fully-adapted
(auxiliary) particle filter using our theory of knots to show how a small
modification guarantees an asymptotic variance ordering for all relevant test
functions.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [263] [PRESOL: a web-based computational setting for feature-based flare forecasting](https://arxiv.org/abs/2510.01799)
*Chiara Curletto,Paolo Massa,Valeria Tagliafico,Cristina Campi,Federico Benvenuto,Michele Piana,Andrea Tacchino*

Main category: astro-ph.SR

TL;DR: 本文介绍用于执行基于特征的机器学习方法计算管道的网络技术平台，可预测耀斑发生、提供特征排名信息和评估预测性能。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的太阳耀斑预测中，深度学习可操作性有前景但可解释性低，机器学习能提供影响预测的物理描述符信息，需要更好的预测方案。

Method: 构建基于特征的机器学习方法的计算管道，并通过网络技术平台执行。

Result: 可进行耀斑发生预测、提供特征排名信息和评估预测性能。

Conclusion: 构建的网络技术平台可有效利用机器学习方法进行太阳耀斑预测。

Abstract: Solar flares are the most explosive phenomena in the solar system and the
main trigger of the events' chain that starts from Coronal Mass Ejections and
leads to geomagnetic storms with possible impacts on the infrastructures at
Earth. Data-driven solar flare forecasting relies on either deep learning
approaches, which are operationally promising but with a low explainability
degree, or machine learning algorithms, which can provide information on the
physical descriptors that mostly impact the prediction. This paper describes a
web-based technological platform for the execution of a computational pipeline
of feature-based machine learning methods that provide predictions of the flare
occurrence, feature ranking information, and assessment of the prediction
performances.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [264] [Scalable Asynchronous Federated Modeling for Spatial Data](https://arxiv.org/abs/2510.01771)
*Jianwei Shi,Sameh Abdulah,Ying Sun,Marc G. Genton*

Main category: stat.ME

TL;DR: 本文提出基于低秩高斯过程近似的空间数据异步联邦建模框架，解决现有方法问题，理论证明收敛性，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 空间数据分布在不同设备，隐私和通信限制使其难以直接共享，现有联邦建模方法存在忽略空间依赖或同步更新问题，需新方法。

Method: 提出基于低秩高斯过程近似的异步联邦建模框架，采用块优化，引入梯度校正、自适应聚合和稳定更新策略。

Result: 理论上建立了与陈旧度显式相关的线性收敛性；实验表明异步算法在资源分配均衡时达同步性能，在异构环境中表现更优。

Conclusion: 所提异步联邦建模框架具有良好的鲁棒性和可扩展性，能有效处理空间数据的联邦建模问题。

Abstract: Spatial data are central to applications such as environmental monitoring and
urban planning, but are often distributed across devices where privacy and
communication constraints limit direct sharing. Federated modeling offers a
practical solution that preserves data privacy while enabling global modeling
across distributed data sources. For instance, environmental sensor networks
are privacy- and bandwidth-constrained, motivating federated spatial modeling
that shares only privacy-preserving summaries to produce timely,
high-resolution pollution maps without centralizing raw data. However, existing
federated modeling approaches either ignore spatial dependence or rely on
synchronous updates that suffer from stragglers in heterogeneous environments.
This work proposes an asynchronous federated modeling framework for spatial
data based on low-rank Gaussian process approximations. The method employs
block-wise optimization and introduces strategies for gradient correction,
adaptive aggregation, and stabilized updates. We establish linear convergence
with explicit dependence on staleness, a result of standalone theoretical
significance. Moreover, numerical experiments demonstrate that the asynchronous
algorithm achieves synchronous performance under balanced resource allocation
and significantly outperforms it in heterogeneous settings, showcasing superior
robustness and scalability.

</details>


### [265] [Compressed Bayesian Tensor Regression](https://arxiv.org/abs/2510.01861)
*Roberto Casarin,Radu Craiu,Qing Wang*

Main category: stat.ME

TL;DR: 提出广义张量随机投影方法解决张量回归高维问题，结合贝叶斯推断，经模拟和实际数据验证能提升预测效果并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决张量回归中常见的高维问题。

Method: 引入广义张量随机投影方法，采用贝叶斯推断框架，开发高效Gibbs采样器，使用贝叶斯模型平均和逆逻辑回归估计归一化常数。

Result: 模拟研究和实际数据应用表明，压缩贝叶斯张量回归能实现更好的样本外预测，同时显著降低计算成本。

Conclusion: 压缩贝叶斯张量回归在预测效果和计算成本上优于标准贝叶斯张量回归。

Abstract: To address the common problem of high dimensionality in tensor regressions,
we introduce a generalized tensor random projection method that embeds
high-dimensional tensor-valued covariates into low-dimensional subspaces with
minimal loss of information about the responses. The method is flexible,
allowing for tensor-wise, mode-wise, or combined random projections as special
cases. A Bayesian inference framework is provided featuring the use of a
hierarchical prior distribution and a low-rank representation of the parameter.
Strong theoretical support is provided for the concentration properties of the
random projection and posterior consistency of the Bayesian inference. An
efficient Gibbs sampler is developed to perform inference on the compressed
data. To mitigate the sensitivity introduced by random projections, Bayesian
model averaging is employed, with normalising constants estimated using reverse
logistic regression. An extensive simulation study is conducted to examine the
effects of different tuning parameters. Simulations indicate, and the real data
application confirms, that compressed Bayesian tensor regression can achieve
better out-of-sample prediction while significantly reducing computational cost
compared to standard Bayesian tensor regression.

</details>


### [266] [DiffKnock: Diffusion-based Knockoff Statistics for Neural Networks Inference](https://arxiv.org/abs/2510.01418)
*Heng Ge,Qing Lu*

Main category: stat.ME

TL;DR: 介绍DiffKnock用于高维特征选择，能控制FDR，性能优且可分析单细胞RNA-seq等数据。


<details>
  <summary>Details</summary>
Motivation: 解决现有knockoff方法在保留复杂特征依赖和检测非线性关联方面的局限性。

Method: 训练扩散模型生成有效knockoffs，用基于神经网络的梯度和过滤统计构建反对称特征重要性度量。

Result: 模拟中比基于自编码器的knockoffs有更高功效并维持目标FDR；应用于小鼠单细胞RNA-seq数据识别出相关基因。

Conclusion: DiffKnock结合深度生成模型灵活性和严格统计保证，是分析单细胞RNA-seq及其他领域高维结构化数据的强大可靠工具。

Abstract: We introduce DiffKnock, a diffusion-based knockoff framework for
high-dimensional feature selection with finite-sample false discovery rate
(FDR) control. DiffKnock addresses two key limitations of existing knockoff
methods: preserving complex feature dependencies and detecting non-linear
associations. Our approach trains diffusion models to generate valid knockoffs
and uses neural network--based gradient and filter statistics to construct
antisymmetric feature importance measures. Through simulations, we showed that
DiffKnock achieved higher power than autoencoder-based knockoffs while
maintaining target FDR, indicating its superior performance in scenarios
involving complex non-linear architectures. Applied to murine single-cell
RNA-seq data of LPS-stimulated macrophages, DiffKnock identifies canonical
NF-$\kappa$B target genes (Ccl3, Hmox1) and regulators (Fosb, Pdgfb). These
results highlight that, by combining the flexibility of deep generative models
with rigorous statistical guarantees, DiffKnock is a powerful and reliable tool
for analyzing single-cell RNA-seq data, as well as high-dimensional and
structured data in other domains.

</details>


### [267] [Predictively Oriented Posteriors](https://arxiv.org/abs/2510.01915)
*Yann McLatchie,Badr-Eddine Cherief-Abdellatif,David T. Frazier,Jeremias Knoblauch*

Main category: stat.ME

TL;DR: 提出预测导向（PrO）后验，结合参数推断与密度估计优点，其预测优于经典和广义贝叶氏后验分布，能适应模型误设程度，可用平均场朗之万动力学采样并通过数值例子验证。


<details>
  <summary>Details</summary>
Motivation: 结合参数推断和密度估计的理想方面，提出新统计原则。

Method: 提出预测导向（PrO）后验，通过平均场朗之万动力学对颗粒进行演化采样。

Result: PrO后验预测优于经典和广义贝叶氏后验分布，收敛到预测最优模型平均的速率为$n^{-1/2}$，能适应模型误设程度。

Conclusion: PrO后验是一种有效的方法，结合了参数推断和密度估计的优点，能适应模型误设，理论成果有实践意义。

Abstract: We advocate for a new statistical principle that combines the most desirable
aspects of both parameter inference and density estimation. This leads us to
the predictively oriented (PrO) posterior, which expresses uncertainty as a
consequence of predictive ability. Doing so leads to inferences which
predictively dominate both classical and generalised Bayes posterior predictive
distributions: up to logarithmic factors, PrO posteriors converge to the
predictively optimal model average at rate $n^{-1/2}$. Whereas classical and
generalised Bayes posteriors only achieve this rate if the model can recover
the data-generating process, PrO posteriors adapt to the level of model
misspecification. This means that they concentrate around the true model at
rate $n^{1/2}$ in the same way as Bayes and Gibbs posteriors if the model can
recover the data-generating distribution, but do \textit{not} concentrate in
the presence of non-trivial forms of model misspecification. Instead, they
stabilise towards a predictively optimal posterior whose degree of irreducible
uncertainty admits an interpretation as the degree of model misspecification --
a sharp contrast to how Bayesian uncertainty and its existing extensions
behave. Lastly, we show that PrO posteriors can be sampled from by evolving
particles based on mean field Langevin dynamics, and verify the practical
significance of our theoretical developments on a number of numerical examples.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [268] [Fast training of accurate physics-informed neural networks without gradient descent](https://arxiv.org/abs/2405.20836)
*Chinmay Datar,Taniya Kapoor,Abhishek Chandra,Qing Sun,Erik Lien Bolager,Iryna Burak,Anna Veselovska,Massimo Fornasier,Felix Dietrich*

Main category: math.NA

TL;DR: 提出Frozen - PINN解决PINNs在求解时变PDEs的训练和精度瓶颈，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: PINNs求解时变PDEs时，其精度和训练速度受梯度下降迭代优化及时间非因果处理的限制。

Method: 基于时空分离原理，使用随机特征而非梯度下降训练，构建时考虑时间因果性。

Result: 在八个PDE基准测试中，Frozen - PINNs的训练效率和精度优于现有PINNs，常高出几个数量级。

Conclusion: 解决了PINNs长期存在的训练和精度瓶颈，引发PINN训练范式转变，为学界提供了挑战基准。

Abstract: Solving time-dependent Partial Differential Equations (PDEs) is one of the
most critical problems in computational science. While Physics-Informed Neural
Networks (PINNs) offer a promising framework for approximating PDE solutions,
their accuracy and training speed are limited by two core barriers:
gradient-descent-based iterative optimization over complex loss landscapes and
non-causal treatment of time as an extra spatial dimension. We present
Frozen-PINN, a novel PINN based on the principle of space-time separation that
leverages random features instead of training with gradient descent, and
incorporates temporal causality by construction. On eight PDE benchmarks,
including challenges such as extreme advection speeds, shocks, and high
dimensionality, Frozen-PINNs achieve superior training efficiency and accuracy
over state-of-the-art PINNs, often by several orders of magnitude. Our work
addresses longstanding training and accuracy bottlenecks of PINNs, delivering
quickly trainable, highly accurate, and inherently causal PDE solvers, a
combination that prior methods could not realize. Our approach challenges the
reliance of PINNs on stochastic gradient-descent-based methods and specialized
hardware, leading to a paradigm shift in PINN training and providing a
challenging benchmark for the community.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [269] [A debiased Bernoulli factory and unbiased estimation of a probability](https://arxiv.org/abs/2510.01941)
*Jere Koskela,Toni Karvonen,Krzysztof Łatuszyński,Dario Spanò*

Main category: math.PR

TL;DR: 构建了一个概率 $f(x)$ 的无偏 $[0, 1]$ 值估计器，具有所需随机变量数量与结果无关等特性。


<details>
  <summary>Details</summary>
Motivation: 在已知函数 $f$ 和未知参数 $x$ 的 Bernoulli 分布随机变量情况下，构建概率 $f(x)$ 的无偏估计器。

Method: 基于去偏，随机截断一致估计器的 telescopic 系列，从特定 Bernoulli 工厂系数构建一致估计器。

Result: 得到无偏估计器的上下界，所需 Ber$(x)$ 分布随机变量数量与结果无关。

Conclusion: 该结果是一种新颖的 Bernoulli 工厂，也是 $f$-factory 的建设性示例。

Abstract: Given a known function $f : [0, 1] \mapsto (0, 1)$ and a random but almost
surely finite number of independent, Ber$(x)$-distributed random variables with
unknown $x \in [0, 1]$, we construct an unbiased, $[0, 1]$-valued estimator of
the probability $f(x) \in (0, 1)$. Our estimator is based on so-called
debiasing, or randomly truncating a telescopic series of consistent estimators.
Constructing these consistent estimators from the coefficients of a particular
Bernoulli factory for $f$ yields provable upper and lower bounds for our
unbiased estimator. Our result can be thought of as a novel Bernoulli factory
with the appealing property that the required number of Ber$(x)$-distributed
random variates is independent of their outcomes, and also as constructive
example of the so-called $f$-factory.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [270] [Enhancing the development of Cherenkov Telescope Array control software with Large Language Models](https://arxiv.org/abs/2510.01299)
*Dmitriy Kostunin,Elisa Jones,Vladimir Sotnikov,Valery Sotnikov,Sergo Golovachev,Alexandre Strube*

Main category: astro-ph.IM

TL;DR: 开发基于指令微调大语言模型的AI代理辅助CTAO控制与数据采集软件的工程和操作，并展示集成进展。


<details>
  <summary>Details</summary>
Motivation: 借助AI代理提升CTAO控制与数据采集软件工程和操作的效率与效果。

Method: 开发基于指令微调大语言模型的AI代理，使其与项目文档和代码库对齐，理解上下文信息，与外部API交互，用自然语言与用户沟通。

Result: 取得了将相关功能集成到CTAO操作和离线数据分析管道的进展。

Conclusion: 基于指令微调大语言模型的AI代理在CTAO控制与数据采集软件的应用取得一定进展，有进一步推进的潜力。

Abstract: We develop AI agents based on instruction-finetuned large language models
(LLMs) to assist in the engineering and operation of the Cherenkov Telescope
Array Observatory (CTAO) Control and Data Acquisition Software (ACADA). These
agents align with project-specific documentation and codebases, understand
contextual information, interact with external APIs, and communicate with users
in natural language. We present our progress in integrating these features into
CTAO pipelines for operations and offline data analysis.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [271] [LLM-Enhanced, Data-Driven Personalized and Equitable Clinician Scheduling: A Predict-then-Optimize Approach](https://arxiv.org/abs/2510.02047)
*Anjali Jha,Wanqing Chen,Maxim Eckmann,Ian Stockwell,Jianwu Wang,Kai Sun*

Main category: math.OC

TL;DR: 传统临床医生排班方法忽略非结构化信息，本文提出预测 - 优化框架，结合大语言模型和混合整数规划优化排班，提高运营效率和医生福祉。


<details>
  <summary>Details</summary>
Motivation: 传统排班方法忽略非结构化信息，导致排班不合理、资源利用不佳等问题，需要新方法解决。

Method: 提出预测 - 优化框架，用大语言模型从非结构化排班笔记中提取偏好和约束，结合混合整数规划模型进行排班优化，考虑四个目标。

Result: 未提及具体结果。

Conclusion: 结合大语言模型和数学优化的框架提供了强大的数据驱动解决方案，可提高运营效率，支持公平性和医生福祉。

Abstract: Clinician scheduling remains a persistent challenge due to limited clinical
resources and fluctuating demands. This complexity is especially acute in large
academic anesthesiology departments as physicians balance responsibilities
across multiple clinical sites with conflicting priorities. Further, scheduling
must account for individual clinical and lifestyle preferences to ensure job
satisfaction and well-being. Traditional approaches, often based on statistical
or rule-based optimization models, rely on structured data and explicit domain
knowledge. However, these methods often overlook unstructured information,
e.g., free-text notes from routinely administered clinician well-being surveys
and scheduling platforms. These notes may reveal implicit and underutilized
clinical resources. Neglecting such information can lead to misaligned
schedules, increased burnout, overlooked staffing flexibility, and suboptimal
utilization of available resources. To address this gap, we propose a
predict-then-optimize framework that integrates classification-based clinician
availability predictions with a mixed-integer programming schedule optimization
model. Large language models (LLMs) are employed to extract actionable
preferences and implicit constraints from unstructured schedule notes,
enhancing the reliability of availability predictions. These predictions then
inform the schedule optimization considering four objectives: first, ensuring
clinical full-time equivalent compliance, second, reducing workload imbalances
by enforcing equitable proportions of shift types, third, maximizing clinician
availability for assigned shifts, and fourth, schedule consistency. By
combining the interpretive power of LLMs with the rigor of mathematical
optimization, our framework provides a robust, data-driven solution that
enhances operational efficiency while supporting equity and clinician
well-being.

</details>


### [272] [Exponential convergence of a distributed divide-and-conquer algorithm for constrained convex optimization on networks](https://arxiv.org/abs/2510.01511)
*Nazar Emirov,Guohui Song,Qiyu Sun*

Main category: math.OC

TL;DR: 提出用于网络约束凸优化的分治算法，证明收敛性并通过实验验证


<details>
  <summary>Details</summary>
Motivation: 解决网络约束凸优化问题

Method: 提出全分布式的分治算法，每次迭代在选定融合中心求解局部子问题，并与相邻融合中心协调

Result: 在标准假设和多项式增长条件下，建立算法的指数收敛性，给出精确和不精确局部求解器的显式界，数值实验证实理论

Conclusion: 所提算法具有可扩展性和有效性

Abstract: We propose a divide-and-conquer (DAC) algorithm for constrained convex
optimization over networks, where the global objective is the sum of local
objectives attached to individual agents. The algorithm is fully distributed:
each iteration solves local subproblems around selected fusion centers and
coordinates only with neighboring fusion centers. Under standard assumptions of
smoothness, strong convexity, and locality on the objective function, together
with polynomial growth conditions on the underlying graph, we establish
exponential convergence of the DAC iterations and derive explicit bounds for
both exact and inexact local solvers. Numerical experiments on three
representative losses ($L_2$ distance, quadratic, and entropy) confirm the
theory and demonstrate scalability and effectiveness.

</details>


### [273] [DeMuon: A Decentralized Muon for Matrix Optimization over Graphs](https://arxiv.org/abs/2510.01377)
*Chuan He,Shuyi Ren,Jingwei Mao,Erik G. Larsson*

Main category: math.OC

TL;DR: 提出用于给定通信拓扑的去中心化矩阵优化方法DeMuon，分析其迭代复杂度，实验显示优于其他算法。


<details>
  <summary>Details</summary>
Motivation: 将Muon扩展到图上的去中心化优化并提供复杂度保证。

Method: 结合牛顿 - 舒尔茨迭代进行矩阵正交化，采用梯度跟踪减轻局部函数异质性。

Result: 建立了DeMuon在重尾噪声等条件下达到近似随机平稳点的迭代复杂度，数值实验显示其在不同网络拓扑中优于其他算法。

Conclusion: DeMuon是Muon在图上去中心化优化的首个有复杂度保证的直接扩展，性能表现更好。

Abstract: In this paper, we propose DeMuon, a method for decentralized matrix
optimization over a given communication topology. DeMuon incorporates matrix
orthogonalization via Newton-Schulz iterations-a technique inherited from its
centralized predecessor, Muon-and employs gradient tracking to mitigate
heterogeneity among local functions. Under heavy-tailed noise conditions and
additional mild assumptions, we establish the iteration complexity of DeMuon
for reaching an approximate stochastic stationary point. This complexity result
matches the best-known complexity bounds of centralized algorithms in terms of
dependence on the target tolerance. To the best of our knowledge, DeMuon is the
first direct extension of Muon to decentralized optimization over graphs with
provable complexity guarantees. We conduct preliminary numerical experiments on
decentralized transformer pretraining over graphs with varying degrees of
connectivity. Our numerical results demonstrate a clear margin of improvement
of DeMuon over other popular decentralized algorithms across different network
topologies.

</details>


### [274] [Smooth Quasar-Convex Optimization with Constraints](https://arxiv.org/abs/2510.01943)
*David Martínez-Rubio*

Main category: math.OC

TL;DR: 本文设计不精确加速近端点算法解决带约束的γ - 拟凸光滑函数一阶查询复杂度问题，改进黎曼优化复杂度，还分析了投影梯度下降和Frank - Wolfe算法。


<details>
  <summary>Details</summary>
Motivation: 现有近最优算法仅适用于仿射空间，带约束的γ - 拟凸光滑函数获得近最优一阶查询复杂度算法是公开问题。

Method: 设计不精确加速近端点算法，用一阶方法实现，分析投影梯度下降和Frank - Wolfe算法。

Result: 解决了带约束的γ - 拟凸光滑函数获得近最优一阶查询复杂度问题，改进加速测地黎曼优化解复杂度。

Conclusion: 本文首次对带一般凸约束的拟凸光滑函数一阶方法进行分析。

Abstract: Quasar-convex functions form a broad nonconvex class with applications to
linear dynamical systems, generalized linear models, and Riemannian
optimization, among others. Current nearly optimal algorithms work only in
affine spaces due to the loss of one degree of freedom when working with
general convex constraints. Obtaining an accelerated algorithm that makes
nearly optimal $\widetilde{O}(1/(\gamma\sqrt{\epsilon}))$ first-order queries
to a $\gamma$-quasar convex smooth function \emph{with constraints} was
independently asked as an open problem in Mart\'inez-Rubio (2022); Lezane,
Langer, and Koolen (2024). In this work, we solve this question by designing an
inexact accelerated proximal point algorithm that we implement using a
first-order method achieving the aforementioned rate and, as a consequence, we
improve the complexity of the accelerated geodesically Riemannian optimization
solution in Mart\'inez-Rubio (2022). We also analyze projected gradient descent
and Frank-Wolfe algorithms in this constrained quasar-convex setting. To the
best of our knowledge, our work provides the first analyses of first-order
methods for quasar-convex smooth functions with general convex constraints.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [275] [An Anthropologist LLM to Elicit Users' Moral Preferences through Role-Play](https://arxiv.org/abs/2510.01189)
*Gianluca De Ninno,Paola Inverardi,Francesca Belotti*

Main category: cs.HC

TL;DR: 研究结合沉浸式角色扮演游戏与大语言模型分析能力，探索引出用户道德决策的新方法，结果显示该方法能提升模型预测用户行为能力，可用于软件开发早期。


<details>
  <summary>Details</summary>
Motivation: 探索引出用户道德决策的新方法，聚焦捕捉软伦理（道德偏好）。

Method: 采用人类学方法，让参与者在数字隐私领域的角色扮演游戏中面对伦理场景，用定制大语言模型分析收集的数据。

Result: 数据丰富度和解释框架显著提升模型预测用户行为的能力。

Conclusion: 大语言模型可有效用于自动化和增强软件开发早期对用户道德偏好和决策过程的理解。

Abstract: This study investigates a novel approach to eliciting users' moral
decision-making by combining immersive roleplaying games with LLM analysis
capabilities. Building on the distinction introduced by Floridi between hard
ethics inspiring and shaping laws-and soft ethics-moral preferences guiding
individual behavior within the free space of decisions compliant to laws-we
focus on capturing the latter through contextrich, narrative-driven
interactions. Grounded in anthropological methods, the role-playing game
exposes participants to ethically charged scenarios in the domain of digital
privacy. Data collected during the sessions were interpreted by a customized
LLM ("GPT Anthropologist"). Evaluation through a cross-validation process shows
that both the richness of the data and the interpretive framing significantly
enhance the model's ability to predict user behavior. Results show that LLMs
can be effectively employed to automate and enhance the understanding of user
moral preferences and decision-making process in the early stages of software
development.

</details>


### [276] [LegiScout: A Visual Tool for Understanding Complex Legislation](https://arxiv.org/abs/2510.01195)
*Aadarsh Rajiv,Klaus Mueller*

Main category: cs.HC

TL;DR: 介绍LegiScout交互式可视化系统，可将静态政策图转化为动态图，助力理解现代立法框架。


<details>
  <summary>Details</summary>
Motivation: 现代立法框架复杂，政府发布的图表静态、密集且难解读，需要更好的工具辅助理解。

Method: 集成数据提取、自然语言处理和计算机视觉技术，将静态政策图转化为动态力导向图。

Result: LegiScout能支持对ACA及广泛立法和监管框架的深入探索。

Conclusion: 该方法使利益相关者能更好地理解和应对现代法律的复杂性。

Abstract: Modern legislative frameworks, such as the Affordable Care Act (ACA), often
involve complex webs of agencies, mandates, and interdependencies. Government
issued charts attempt to depict these structures but are typically static,
dense, and difficult to interpret - even for experts. We introduce LegiScout,
an interactive visualization system that transforms static policy diagrams into
dynamic, force-directed graphs, enhancing comprehension while preserving
essential relationships. By integrating data extraction, natural language
processing, and computer vision techniques, LegiScout supports deeper
exploration of not only the ACA but also a wide range of legislative and
regulatory frameworks. Our approach enables stakeholders - policymakers,
analysts, and the public - to navigate and understand the complexity inherent
in modern law.

</details>


### [277] [The Command Line GUIde: Graphical Interfaces from Man Pages via AI](https://arxiv.org/abs/2510.01453)
*Saketh Ram Kasibatla,Kiran Medleri Hiremath,Raven Rothkopf,Sorin Lerner,Haijun Xia,Brian Hempel*

Main category: cs.HC

TL;DR: 本文提出通过AI将命令行工具文档转换为界面规范，创建GUIde系统将命令选项图形化展示，并进行评估。


<details>
  <summary>Details</summary>
Motivation: 命令行虽功能强大但使用有门槛，图形界面更易操作，为更好发挥命令行的能力，需创建图形化界面。

Method: 通过AI将命令行工具的文档（手册页形式）转换为界面规范，创建名为GUIde的用户系统来图形化展示命令选项。

Result: 在一组命令上对生成的界面进行评估。

Conclusion: 文档未明确给出结论，但旨在表明GUIde能在一定程度上为用户的实际命令行任务提供全面图形化界面。

Abstract: Although birthed in the era of teletypes, the command line shell survived the
graphical interface revolution of the 1980's and lives on in modern desktop
operating systems. The command line provides access to powerful functionality
not otherwise exposed on the computer, but requires users to recall textual
syntax and carefully scour documentation. In contrast, graphical interfaces let
users organically discover and invoke possible actions through widgets and
menus. To better expose the power of the command line, we demonstrate a
mechanism for automatically creating graphical interfaces for command line
tools by translating their documentation (in the form of man pages) into
interface specifications via AI. Using these specifications, our user-facing
system, called GUIde, presents the command options to the user graphically. We
evaluate the generated interfaces on a corpus of commands to show to what
degree GUIde offers thorough graphical interfaces for users' real-world command
line tasks.

</details>


### [278] [From keywords to semantics: Perceptions of large language models in data discovery](https://arxiv.org/abs/2510.01473)
*Maura E Halstead,Mark A. Green,Caroline Jay,Richard Kingston,David Topping,Alexander Singleton*

Main category: cs.HC

TL;DR: 当前数据发现方法有局限，研究通过焦点小组了解研究者对用大语言模型进行数据发现的看法，模型显示仅潜在好处不足以让研究者接受，透明性特征可克服障碍，利用模型可提升接受度。


<details>
  <summary>Details</summary>
Motivation: 当前数据发现方法要求研究者知晓确切用词，易遗漏相关数据，大语言模型可解决此问题，但不知研究者是否接受，因此开展研究。

Method: 采用以人类为中心的人工智能方法，进行了有27人参与的焦点小组研究。

Result: 概念模型显示仅潜在好处不足以让研究者用大语言模型替代现有技术，存在障碍，但透明性相关特征可克服。

Conclusion: 利用该模型，开发者可加入相关特征，提高研究者对大语言模型用于数据发现的接受度。

Abstract: Current approaches to data discovery match keywords between metadata and
queries. This matching requires researchers to know the exact wording that
other researchers previously used, creating a challenging process that could
lead to missing relevant data. Large Language Models (LLMs) could enhance data
discovery by removing this requirement and allowing researchers to ask
questions with natural language. However, we do not currently know if
researchers would accept LLMs for data discovery. Using a human-centered
artificial intelligence (HCAI) focus, we ran focus groups (N = 27) to
understand researchers' perspectives towards LLMs for data discovery. Our
conceptual model shows that the potential benefits are not enough for
researchers to use LLMs instead of current technology. Barriers prevent
researchers from fully accepting LLMs, but features around transparency could
overcome them. Using our model will allow developers to incorporate features
that result in an increased acceptance of LLMs for data discovery.

</details>


### [279] [Towards Human-Centered RegTech: Unpacking Professionals' Strategies and Needs for Using LLMs Safely](https://arxiv.org/abs/2510.01638)
*Siying Hu,Yaxing Yao,Zhicong Lu*

Main category: cs.HC

TL;DR: 研究大语言模型在高风险专业领域应用的合规风险，发现专家担忧及应对策略，指出当前NLP工具与合规需求差距，为RegTech提供基础。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在高风险专业领域应用带来的严重且未充分研究的合规风险。

Method: 对来自法律、医疗和金融等行业的24名高技能知识工作者进行半结构化访谈。

Result: 专家普遍担忧敏感信息泄露、知识产权侵权和模型输出质量不确定性，自发采取缓解策略但效果有限。

Conclusion: 当前NLP工具与专家实际合规需求存在显著差距，研究为RegTech提供基础和设计要求。

Abstract: Large Language Models are profoundly changing work patterns in high-risk
professional domains, yet their application also introduces severe and
underexplored compliance risks. To investigate this issue, we conducted
semi-structured interviews with 24 highly-skilled knowledge workers from
industries such as law, healthcare, and finance. The study found that these
experts are commonly concerned about sensitive information leakage,
intellectual property infringement, and uncertainty regarding the quality of
model outputs. In response, they spontaneously adopt various mitigation
strategies, such as actively distorting input data and limiting the details in
their prompts. However, the effectiveness of these spontaneous efforts is
limited due to a lack of specific compliance guidance and training for Large
Language Models. Our research reveals a significant gap between current NLP
tools and the actual compliance needs of experts. This paper positions these
valuable empirical findings as foundational work for building the next
generation of Human-Centered, Compliance-Driven Natural Language Processing for
Regulatory Technology (RegTech), providing a critical human-centered
perspective and design requirements for engineering NLP systems that can
proactively support expert compliance workflows.

</details>


### [280] [Human-Robo-advisor collaboration in decision-making: Evidence from a multiphase mixed methods experimental study](https://arxiv.org/abs/2510.02153)
*Hasan Mahmud,Najmul Islam,Satish Krishnan*

Main category: cs.HC

TL;DR: 本文采用多阶段混合方法研究人们对机器人理财顾问（RA）的看法和决策，发现人们倾向依赖RA，确定RA角色和用户类型，划分接受因素类型，为设计RA系统提供见解。


<details>
  <summary>Details</summary>
Motivation: 此前研究较少关注人们如何理解RA角色并将其建议融入决策，本研究旨在填补此空白。

Method: 采用多阶段混合方法，包括行为实验（N = 334）、主题分析和后续定量测试。

Result: 人们倾向依赖RA，依赖受RA表现信息和建议框架影响；揭示决策中RA的三种角色和四种用户类型；用2 x 2类型学划分接受前因。

Conclusion: 结合多种证据，增进对人机协作理解，为设计更可信和自适应的RA系统提供可行见解。

Abstract: Robo-advisors (RAs) are cost-effective, bias-resistant alternatives to human
financial advisors, yet adoption remains limited. While prior research has
examined user interactions with RAs, less is known about how individuals
interpret RA roles and integrate their advice into decision-making. To address
this gap, this study employs a multiphase mixed methods design integrating a
behavioral experiment (N = 334), thematic analysis, and follow-up quantitative
testing. Findings suggest that people tend to rely on RAs, with reliance shaped
by information about RA performance and the framing of advice as gains or
losses. Thematic analysis reveals three RA roles in decision-making and four
user types, each reflecting distinct patterns of advice integration. In
addition, a 2 x 2 typology categorizes antecedents of acceptance into enablers
and inhibitors at both the individual and algorithmic levels. By combining
behavioral, interpretive, and confirmatory evidence, this study advances
understanding of human-RA collaboration and provides actionable insights for
designing more trustworthy and adaptive RA systems.

</details>


### [281] [EvolveCaptions: Empowering DHH Users Through Real-Time Collaborative Captioning](https://arxiv.org/abs/2510.02181)
*Liang-Yuan Wu,Dhruv Jain*

Main category: cs.HC

TL;DR: 提出EvolveCaptions实时协作ASR自适应系统，用少量录音时间降低DHH用户WER，证明协作实时ASR自适应有前景


<details>
  <summary>Details</summary>
Motivation: 现有ASR系统难准确转录DHH个体语音，且个性化方法需大量预录数据，给DHH用户带来负担

Method: 构建EvolveCaptions系统，让听力正常参与者在对话中纠正ASR错误，系统据此生成提示让DHH用户录制以微调模型

Result: 12名DHH和6名听力正常参与者的研究显示，使用一小时内降低所有DHH用户WER，平均仅需五分钟录音时间，参与者认为系统直观、省力、融入交流好

Conclusion: 协作实时ASR自适应有助于实现更公平的交流

Abstract: Automatic Speech Recognition (ASR) systems often fail to accurately
transcribe speech from Deaf and Hard of Hearing (DHH) individuals, especially
during real-time conversations. Existing personalization approaches typically
require extensive pre-recorded data and place the burden of adaptation on the
DHH speaker. We present EvolveCaptions, a real-time, collaborative ASR
adaptation system that supports in-situ personalization with minimal effort.
Hearing participants correct ASR errors during live conversations. Based on
these corrections, the system generates short, phonetically targeted prompts
for the DHH speaker to record, which are then used to fine-tune the ASR model.
In a study with 12 DHH and six hearing participants, EvolveCaptions reduced
Word Error Rate (WER) across all DHH users within one hour of use, using only
five minutes of recording time on average. Participants described the system as
intuitive, low-effort, and well-integrated into communication. These findings
demonstrate the promise of collaborative, real-time ASR adaptation for more
equitable communication.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [282] [Exponential Quantum Advantage for Message Complexity in Distributed Algorithms](https://arxiv.org/abs/2510.01657)
*François Le Gall,Maël Luce,Joseph Marchand,Mathieu Roget*

Main category: quant-ph

TL;DR: 本文研究量子分布式算法在消息复杂度上相对经典分布式算法的优势，展示了在网络中两指定节点间路由信息任务上的指数级量子优势。


<details>
  <summary>Details</summary>
Motivation: 探究量子分布式算法在消息复杂度上能比经典分布式算法有多大优势，此前已有多项式级优势结果，本文关注路由信息任务。

Method: 基于Li等人对焊接树量子行走的简洁实现构建量子算法，通过将Childs等人的查询复杂度下界提升为消息复杂度下界得到经典算法下界。

Result: 对于焊接树图，存在量子分布式算法在消息复杂度上比任何经典算法小指数倍。

Conclusion: 在网络中两指定节点间路由信息任务上，量子分布式算法相比经典算法有指数级消息复杂度优势。

Abstract: We investigate how much quantum distributed algorithms can outperform
classical distributed algorithms with respect to the message complexity (the
overall amount of communication used by the algorithm). Recently, Dufoulon,
Magniez and Pandurangan (PODC 2025) have shown a polynomial quantum advantage
for several tasks such as leader election and agreement. In this paper, we show
an exponential quantum advantage for a fundamental task: routing information
between two specified nodes of a network. We prove that for the family of
``welded trees" introduced in the seminal work by Childs, Cleve, Deotto, Farhi,
Gutmann and Spielman (STOC 2003), there exists a quantum distributed algorithm
that transfers messages from the entrance of the graph to the exit with message
complexity exponentially smaller than any classical algorithm. Our quantum
algorithm is based on the recent "succinct" implementation of quantum walks
over the welded trees by Li, Li and Luo (SODA 2024). Our classical lower bound
is obtained by ``lifting'' the lower bound from Childs, Cleve, Deotto, Farhi,
Gutmann and Spielman (STOC 2003) from query complexity to message complexity.

</details>


### [283] [Quantum-Assisted Correlation Clustering](https://arxiv.org/abs/2509.03561)
*Antonio Macaluso,Supreeth Mysore Venkatesh,Diego Arenas,Matthias Klusch,Andreas Dengel*

Main category: quant-ph

TL;DR: 提出混合量子 - 经典方法用于相关聚类，在真实数据和特定场景中优于经典算法，凸显混合优化在图无监督学习聚类的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决图基无监督学习中的相关聚类问题，寻找更好的图节点划分方法。

Method: 将GCS - Q方法适配到相关聚类，把每个二分步骤编码为二次无约束二进制优化问题，通过量子退火求解。

Result: 在合成有符号图和真实高光谱成像数据评估中，GCS - Q在真实数据和簇大小不平衡场景中的鲁棒性和聚类质量优于经典算法。

Conclusion: 混合量子 - 经典优化有望推动图基无监督学习中可扩展和结构感知的聚类技术发展。

Abstract: This work introduces a hybrid quantum-classical method to correlation
clustering, a graph-based unsupervised learning task that seeks to partition
the nodes in a graph based on pairwise agreement and disagreement. In
particular, we adapt GCS-Q, a quantum-assisted solver originally designed for
coalition structure generation, to maximize intra-cluster agreement in signed
graphs through recursive divisive partitioning. The proposed method encodes
each bipartitioning step as a quadratic unconstrained binary optimization
problem, solved via quantum annealing. This integration of quantum optimization
within a hierarchical clustering framework enables handling of graphs with
arbitrary correlation structures, including negative edges, without relying on
metric assumptions or a predefined number of clusters. Empirical evaluations on
synthetic signed graphs and real-world hyperspectral imaging data demonstrate
that, when adapted for correlation clustering, GCS-Q outperforms classical
algorithms in robustness and clustering quality on real-world data and in
scenarios with cluster size imbalance. Our results highlight the promise of
hybrid quantum-classical optimization for advancing scalable and
structurally-aware clustering techniques in graph-based unsupervised learning.

</details>


### [284] [Quantum Fisher information matrices from Rényi relative entropies](https://arxiv.org/abs/2510.02218)
*Mark M. Wilde*

Main category: quant-ph

TL;DR: 本文推导了源于对数 - 欧几里得、α - z 和几何 Rényi 相对熵的信息矩阵，建立了 α - z 信息矩阵的性质及参数化热态的相关公式和估计算法。


<details>
  <summary>Details</summary>
Motivation: 量子 Fisher 信息矩阵的量子推广在量子信息科学等领域有重要应用，且无唯一量子推广形式，因此推导不同相对熵产生的信息矩阵。

Method: 使用有限差分法计算矩阵导数。

Result: 对数 - 欧几里得 Rényi 相对熵得到 Kubo - Mori 信息矩阵，几何 Rényi 相对熵得到右对数导数 Fisher 信息矩阵，这些信息矩阵对非负 Rényi 参数 α 满足数据处理不等式；推导并建立了 α - z 信息矩阵的基本性质，建立参数化热态的 α - z 信息矩阵公式及估计的混合量子 - 经典算法。

Conclusion: 成功推导不同相对熵产生的信息矩阵，建立相关性质和算法，可应用于量子玻尔兹曼机器学习。

Abstract: Quantum generalizations of the Fisher information are important in quantum
information science, with applications in high energy and condensed matter
physics and in quantum estimation theory, machine learning, and optimization.
One can derive a quantum generalization of the Fisher information matrix in a
natural way as the Hessian matrix arising in a Taylor expansion of a smooth
divergence. Such an approach is appealing for quantum information theorists,
given the ubiquity of divergences in quantum information theory. In contrast to
the classical case, there is not a unique quantum generalization of the Fisher
information matrix, similar to how there is not a unique quantum generalization
of the relative entropy or the R\'enyi relative entropy. In this paper, I
derive information matrices arising from the log-Euclidean, $\alpha$-$z$, and
geometric R\'enyi relative entropies, with the main technical tool for doing so
being the method of divided differences for calculating matrix derivatives.
Interestingly, for all non-negative values of the R\'enyi parameter $\alpha$,
the log-Euclidean R\'enyi relative entropy leads to the Kubo-Mori information
matrix, and the geometric R\'enyi relative entropy leads to the
right-logarithmic derivative Fisher information matrix. Thus, the resulting
information matrices obey the data-processing inequality for all non-negative
values of the R\'enyi parameter $\alpha$ even though the original quantities do
not. Additionally, I derive and establish basic properties of $\alpha$-$z$
information matrices resulting from the $\alpha$-$z$ R\'enyi relative
entropies. For parameterized thermal states, I establish formulas for their
$\alpha$-$z$ information matrices and hybrid quantum-classical algorithms for
estimating them, with applications in quantum Boltzmann machine learning.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [285] [LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science](https://arxiv.org/abs/2510.01285)
*Alireza Salemi,Mihir Parmar,Palash Goyal,Yiwen Song,Jinsung Yoon,Hamed Zamani,Hamid Palangi,Tomas Pfister*

Main category: cs.MA

TL;DR: 现有大语言模型数据发现方法有局限，提出基于黑板架构的多智能体通信范式，实验显示其性能优于基线，确立了该范式可扩展性和通用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型实际部署中在大型异构数据湖中发现相关数据存在挑战，现有单智能体和主从多智能体系统方法有局限。

Method: 提出基于黑板架构的多智能体通信范式，中央智能体在共享黑板发布请求，自主从属智能体根据能力响应。

Result: 在三个基准测试中，黑板架构显著优于基线，端到端任务成功率相对提高13% - 57%，数据发现F1分数相对提高达9%。

Conclusion: 黑板范式是多智能体系统可扩展且通用的通信框架。

Abstract: The rapid advancement of Large Language Models (LLMs) has opened new
opportunities in data science, yet their practical deployment is often
constrained by the challenge of discovering relevant data within large
heterogeneous data lakes. Existing methods struggle with this: single-agent
systems are quickly overwhelmed by large, heterogeneous files in the large data
lakes, while multi-agent systems designed based on a master-slave paradigm
depend on a rigid central controller for task allocation that requires precise
knowledge of each sub-agent's capabilities. To address these limitations, we
propose a novel multi-agent communication paradigm inspired by the blackboard
architecture for traditional AI models. In this framework, a central agent
posts requests to a shared blackboard, and autonomous subordinate agents --
either responsible for a partition of the data lake or general information
retrieval -- volunteer to respond based on their capabilities. This design
improves scalability and flexibility by eliminating the need for a central
coordinator to have prior knowledge of all sub-agents' expertise. We evaluate
our method on three benchmarks that require explicit data discovery: KramaBench
and modified versions of DS-Bench and DA-Code to incorporate data discovery.
Experimental results demonstrate that the blackboard architecture substantially
outperforms baselines, including RAG and the master-slave multi-agent paradigm,
achieving between 13% to 57% relative improvement in end-to-end task success
and up to a 9% relative gain in F1 score for data discovery over the
best-performing baselines across both proprietary and open-source LLMs. Our
findings establish the blackboard paradigm as a scalable and generalizable
communication framework for multi-agent systems.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [286] [How to Find Fantastic Papers: Self-Rankings as a Powerful Predictor of Scientific Impact Beyond Peer Review](https://arxiv.org/abs/2510.02143)
*Buxin Su,Natalie Collina,Garrett Wen,Didong Li,Kyunghyun Cho,Jianqing Fan,Bingxin Zhao,Weijie Su*

Main category: stat.AP

TL;DR: 本文研究作者对论文的自我排名在识别高影响力研究中的作用，实验表明自我排名可有效预测论文引用数，是同行评审的可靠补充。


<details>
  <summary>Details</summary>
Motivation: 学术研究同行评审任务愈发困难，尤其是人工智能领域，需探索识别高影响力研究的新方法。

Method: 在顶级人工智能会议开展大规模实验，让1342名研究人员对2592篇投稿按质量自我排名，并追踪论文一年多的引用情况。

Result: 作者排名最高的论文引用数是最低的两倍，自我排名在识别高引用论文上表现出色，且在预测未来引用数上优于同行评审分数，结果不受预印本发布时间和自引等因素影响。

Conclusion: 作者自我排名是识别和推广人工智能领域高影响力研究的可靠且有价值的补充。

Abstract: Peer review in academic research aims not only to ensure factual correctness
but also to identify work of high scientific potential that can shape future
research directions. This task is especially critical in fast-moving fields
such as artificial intelligence (AI), yet it has become increasingly difficult
given the rapid growth of submissions. In this paper, we investigate an
underexplored measure for identifying high-impact research: authors' own
rankings of their multiple submissions to the same AI conference. Grounded in
game-theoretic reasoning, we hypothesize that self-rankings are informative
because authors possess unique understanding of their work's conceptual depth
and long-term promise. To test this hypothesis, we conducted a large-scale
experiment at a leading AI conference, where 1,342 researchers self-ranked
their 2,592 submissions by perceived quality. Tracking outcomes over more than
a year, we found that papers ranked highest by their authors received twice as
many citations as their lowest-ranked counterparts; self-rankings were
especially effective at identifying highly cited papers (those with over 150
citations). Moreover, we showed that self-rankings outperformed peer review
scores in predicting future citation counts. Our results remained robust after
accounting for confounders such as preprint posting time and self-citations.
Together, these findings demonstrate that authors' self-rankings provide a
reliable and valuable complement to peer review for identifying and elevating
high-impact research in AI.

</details>


### [287] [Multidata Causal Discovery for Statistical Hurricane Intensity Forecasting](https://arxiv.org/abs/2510.02050)
*Saranya Ganesh S.,Frederick Iat-Hin Tam,Milton S. Gomez,Marie McGraw,Mark DeMaria,Kate Musgrave,Jakob Runge,Tom Beucler*

Main category: stat.AP

TL;DR: 利用多数据因果发现框架改进大西洋飓风强度统计预测，因果特征选择表现佳，SHIPS+提升预测技能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在飓风强度预测中存在不足，难以处理复杂非线性交互和识别相关预测因子，泛化能力有限。

Method: 运用基于SHIPS的多数据因果发现框架和ERA5气象再分析数据，进行多实验选因果相关预测因子，训练多元线性回归模型对比不同特征选择方法。

Result: 因果特征选择在未见过的测试用例中表现更好，尤其是短于3天的预测；SHIPS+提高短期预测技能；多层感知器增加非线性可延长预测时效；业务SHIPS测试表明部分因果发现的预测因子改善了预测。

Conclusion: 因果发现可改进飓风强度预测，为更实证的预测铺平道路。

Abstract: Improving statistical forecasts of Atlantic hurricane intensity is limited by
complex nonlinear interactions and difficulty in identifying relevant
predictors. Conventional methods prioritize correlation or fit, often
overlooking confounding variables and limiting generalizability to unseen
tropical storms. To address this, we leverage a multidata causal discovery
framework with a replicated dataset based on Statistical Hurricane Intensity
Prediction Scheme (SHIPS) using ERA5 meteorological reanalysis. We conduct
multiple experiments to identify and select predictors causally linked to
hurricane intensity changes. We train multiple linear regression models to
compare causal feature selection with no selection, correlation, and random
forest feature importance across five forecast lead times from 1 to 5 days (24
to 120 hours). Causal feature selection consistently outperforms on unseen test
cases, especially for lead times shorter than 3 days. The causal features
primarily include vertical shear, mid-tropospheric potential vorticity and
surface moisture conditions, which are physically significant yet often
underutilized in hurricane intensity predictions. Further, we build an extended
predictor set (SHIPS+) by adding selected features to the standard SHIPS
predictors. SHIPS+ yields increased short-term predictive skill at lead times
of 24, 48, and 72 hours. Adding nonlinearity using multilayer perceptron
further extends skill to longer lead times, despite our framework being purely
regional and not requiring global forecast data. Operational SHIPS tests
confirm that three of the six added causally discovered predictors improve
forecasts, with the largest gains at longer lead times. Our results demonstrate
that causal discovery improves hurricane intensity prediction and pave the way
toward more empirical forecasts.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [288] [NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset for Narrowband Powerline Communications](https://arxiv.org/abs/2510.01850)
*Ying-Ren Chien,Po-Heng Chou,You-Jie Peng,Chun-Yuan Huang,Hen-Wai Tsao,Yu Tsao*

Main category: eess.SP

TL;DR: 提出噪声生成GAN（NGGAN）学习实际测量噪声样本特征用于数据增强，设计适配输入信号长度、用Wasserstein距离作损失函数，经不同数据集训练和分析，结果表明其生成噪声更接近实际测量数据。


<details>
  <summary>Details</summary>
Motivation: 现有数学噪声生成模型只能捕捉加性噪声部分特征，需捕捉非周期性异步脉冲噪声综合统计信息以提升窄带电力线通信（NB - PLC）收发器的脉冲噪声处理能力。

Method: 通过商用NB - PLC调制解调器的模拟耦合和带通滤波电路测量NB - PLC噪声构建数据集；设计NGGAN输入信号长度；用Wasserstein距离作损失函数；对基于数学和实际测量数据集的GAN模型进行定量和定性分析。

Result: 仿真结果表明，使用波形特征训练的NGGAN生成的噪声质量更接近实际测量数据集。

Conclusion: 提出的NGGAN在生成接近实际测量的NB - PLC噪声方面表现良好，能用于噪声数据增强。

Abstract: Capturing comprehensive statistics of nonperiodic asynchronous impulsive
noise is a critical issue in enhancing impulse noise processing for narrowband
powerline communication (NB-PLC) transceivers. However, existing mathematical
noise generative models capture only some of the characteristics of additive
noise. Therefore, we propose a generative adversarial network (GAN), called the
noise-generation GAN (NGGAN), that learns the complicated characteristics of
practically measured noise samples for data augmentation. To closely match the
statistics of complicated noise in NB-PLC systems, we measured the NB-PLC noise
via the analog coupling and bandpass filtering circuits of a commercial NB-PLC
modem to build a realistic dataset. Specifically, the NGGAN design approaches
based on the practically measured dataset are as follows: (i) we design the
length of input signals that the NGGAN model can fit to facilitate
cyclo-stationary noise generation. (ii) Wasserstein distance is used as a loss
function to enhance the similarity between the generated noise and the training
dataset and ensure that the sample diversity is sufficient for various
applications. (iii) To measure the similarity performance of the GAN-based
models based on mathematical and practically measured datasets, we perform
quantitative and qualitative analyses. The training datasets include (1) a
piecewise spectral cyclo-stationary Gaussian model (PSCGM), (2) a
frequency-shift (FRESH) filter, and (3) practical measurements from NB-PLC
systems. Simulation results demonstrate that the proposed NGGAN trained using
waveform characteristics is closer to the practically measured dataset in terms
of the quality of the generated noise.

</details>


### [289] [Unlocking Symbol-Level Precoding Efficiency Through Tensor Equivariant Neural Network](https://arxiv.org/abs/2510.02108)
*Jinshuo Zhang,Yafei Wang,Xinping Yi,Wenjin Wang,Shi Jin,Symeon Chatzinotas,Björn Ottersten*

Main category: eess.SP

TL;DR: 本文提出低推理复杂度的端到端深度学习框架解决符号级预编码高复杂度瓶颈，有性能提升和速度优势。


<details>
  <summary>Details</summary>
Motivation: 基于构造性干扰的符号级预编码虽有性能增益，但高复杂度是瓶颈。

Method: 利用最优符号级预编码闭式解结构和张量等变性，构建从问题到解的映射，设计带注意力机制的张量等变模块框架，还设计网络用于不完美信道状态信息场景。

Result: 所提框架能捕获最优符号级预编码的大量性能增益，比传统方法提速约80倍，在不同用户数量和符号块长度下有强泛化性。

Conclusion: 所提低推理复杂度的端到端深度学习框架有效解决符号级预编码高复杂度问题。

Abstract: Although symbol-level precoding (SLP) based on constructive interference (CI)
exploitation offers performance gains, its high complexity remains a
bottleneck. This paper addresses this challenge with an end-to-end deep
learning (DL) framework with low inference complexity that leverages the
structure of the optimal SLP solution in the closed-form and its inherent
tensor equivariance (TE), where TE denotes that a permutation of the input
induces the corresponding permutation of the output. Building upon the
computationally efficient model-based formulations, as well as their known
closed-form solutions, we analyze their relationship with linear precoding (LP)
and investigate the corresponding optimality condition. We then construct a
mapping from the problem formulation to the solution and prove its TE, based on
which the designed networks reveal a specific parameter-sharing pattern that
delivers low computational complexity and strong generalization. Leveraging
these, we propose the backbone of the framework with an attention-based TE
module, achieving linear computational complexity. Furthermore, we demonstrate
that such a framework is also applicable to imperfect CSI scenarios, where we
design a TE-based network to map the CSI, statistics, and symbols to auxiliary
variables. Simulation results show that the proposed framework captures
substantial performance gains of optimal SLP, while achieving an approximately
80-times speedup over conventional methods and maintaining strong
generalization across user numbers and symbol block lengths.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [290] [Trustworthy Summarization via Uncertainty Quantification and Risk Awareness in Large Language Models](https://arxiv.org/abs/2510.01231)
*Shuaidong Pan,Di Wu*

Main category: cs.CL

TL;DR: 本文提出集成不确定性量化和风险感知机制的大语言模型框架，提升高风险场景自动摘要可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决高风险场景下自动摘要的可靠性问题，应对信息过载和高风险决策需求。

Method: 构建基于条件生成的摘要模型，引入贝叶斯推理建模参数空间不确定性，用预测分布熵衡量不确定性，联合优化熵正则化和风险感知损失，加入风险评分和调节模块。

Result: 对比实验和敏感性分析表明，该方法显著提高高风险应用中摘要的鲁棒性和可靠性，同时保持流畅性和语义完整性。

Conclusion: 本研究为可信摘要提供系统解决方案，在方法层面具有可扩展性和实用价值。

Abstract: This study addresses the reliability of automatic summarization in high-risk
scenarios and proposes a large language model framework that integrates
uncertainty quantification and risk-aware mechanisms. Starting from the demands
of information overload and high-risk decision-making, a conditional
generation-based summarization model is constructed, and Bayesian inference is
introduced during generation to model uncertainty in the parameter space, which
helps avoid overconfident predictions. The uncertainty level of the generated
content is measured using predictive distribution entropy, and a joint
optimization of entropy regularization and risk-aware loss is applied to ensure
that key information is preserved and risk attributes are explicitly expressed
during information compression. On this basis, the model incorporates risk
scoring and regulation modules, allowing summaries to cover the core content
accurately while enhancing trustworthiness through explicit risk-level prompts.
Comparative experiments and sensitivity analyses verify that the proposed
method significantly improves the robustness and reliability of summarization
in high-risk applications while maintaining fluency and semantic integrity.
This research provides a systematic solution for trustworthy summarization and
demonstrates both scalability and practical value at the methodological level.

</details>


### [291] [Efficient Uncertainty Estimation for LLM-based Entity Linking in Tabular Data](https://arxiv.org/abs/2510.01251)
*Carlo Bono,Federico Belotti,Matteo Palmonari*

Main category: cs.CL

TL;DR: 本文提出用自监督方法从单轮大语言模型输出中估计不确定性，减少多轮推理需求，经评估能有效检测低精度输出且成本低。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实体链接任务中部署到现实场景时，需要可靠的不确定性估计，但多轮推理资源需求大，限制了其实际应用。

Method: 研究一种利用词元级特征从单轮大语言模型输出中估计不确定性的自监督方法。

Result: 在多个大语言模型的表格数据实体链接任务上评估表明，得到的不确定性估计能有效检测低精度输出，且计算成本大幅降低。

Conclusion: 该方法提供了一种将不确定性估计以有限计算开销纳入实体链接工作流的实用方式。

Abstract: Linking textual values in tabular data to their corresponding entities in a
Knowledge Base is a core task across a variety of data integration and
enrichment applications. Although Large Language Models (LLMs) have shown
State-of-The-Art performance in Entity Linking (EL) tasks, their deployment in
real-world scenarios requires not only accurate predictions but also reliable
uncertainty estimates, which require resource-demanding multi-shot inference,
posing serious limits to their actual applicability. As a more efficient
alternative, we investigate a self-supervised approach for estimating
uncertainty from single-shot LLM outputs using token-level features, reducing
the need for multiple generations. Evaluation is performed on an EL task on
tabular data across multiple LLMs, showing that the resulting uncertainty
estimates are highly effective in detecting low-accuracy outputs. This is
achieved at a fraction of the computational cost, ultimately supporting a
cost-effective integration of uncertainty measures into LLM-based EL workflows.
The method offers a practical way to incorporate uncertainty estimation into EL
workflows with limited computational overhead.

</details>


### [292] [Comparison of Unsupervised Metrics for Evaluating Judicial Decision Extraction](https://arxiv.org/abs/2510.01792)
*Ivan Leonidovich Litvak,Anton Kostin,Fedor Lashkin,Tatiana Maksiyan,Sergey Lagutin*

Main category: cs.CL

TL;DR: 研究评估16种无监督指标用于评估从俄罗斯司法判决中提取语义块的质量，发现部分指标与专家评级相关性好，无监督指标可用于可扩展筛选，但不能完全取代人工判断。


<details>
  <summary>Details</summary>
Motivation: 人工智能在法律自然语言处理中快速发展，需要可扩展方法评估司法判决文本提取质量。

Method: 评估16种无监督指标，用自举相关性、Lin的一致性相关系数和平均绝对误差进行分析。

Result: Term Frequency Coherence和Coverage Ratio/Block Completeness与专家评级最一致，Legal Term Density呈强负相关，LLM Evaluation Score中等一致。

Conclusion: 无监督指标可实现可扩展筛选，但在高风险法律场景不能完全取代人工判断，为法律NLP提供无注释评估工具。

Abstract: The rapid advancement of artificial intelligence in legal natural language
processing demands scalable methods for evaluating text extraction from
judicial decisions. This study evaluates 16 unsupervised metrics, including
novel formulations, to assess the quality of extracting seven semantic blocks
from 1,000 anonymized Russian judicial decisions, validated against 7,168
expert reviews on a 1--5 Likert scale. These metrics, spanning document-based,
semantic, structural, pseudo-ground truth, and legal-specific categories,
operate without pre-annotated ground truth. Bootstrapped correlations, Lin's
concordance correlation coefficient (CCC), and mean absolute error (MAE) reveal
that Term Frequency Coherence (Pearson $r = 0.540$, Lin CCC = 0.512, MAE =
0.127) and Coverage Ratio/Block Completeness (Pearson $r = 0.513$, Lin CCC =
0.443, MAE = 0.139) best align with expert ratings, while Legal Term Density
(Pearson $r = -0.479$, Lin CCC = -0.079, MAE = 0.394) show strong negative
correlations. The LLM Evaluation Score (mean = 0.849, Pearson $r = 0.382$, Lin
CCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance, using
gpt-4.1-mini via g4f, suggests limited specialization for legal textse. These
findings highlight that unsupervised metrics, including LLM-based approaches,
enable scalable screening but, with moderate correlations and low CCC values,
cannot fully replace human judgment in high-stakes legal contexts. This work
advances legal NLP by providing annotation-free evaluation tools, with
implications for judicial analytics and ethical AI deployment.

</details>


### [293] [AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees](https://arxiv.org/abs/2510.01268)
*Hongyi Zhou,Jin Zhu,Pingfan Su,Kai Ye,Ying Yang,Shakeel A O B Gavioli-Akilagun,Chengchun Shi*

Main category: cs.CL

TL;DR: 提出AdaDetectGPT分类器提升基于logits的文本作者（人类或LLM）检测方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于logits的检测器仅依赖对数概率效果欠佳。

Method: 引入AdaDetectGPT分类器，从训练数据自适应学习见证函数。

Result: AdaDetectGPT在多种数据集和LLM组合中近乎全面提升现有方法，最高提升58%。

Conclusion: AdaDetectGPT能有效提升基于logits的文本作者检测性能。

Abstract: We study the problem of determining whether a piece of text has been authored
by a human or by a large language model (LLM). Existing state of the art
logits-based detectors make use of statistics derived from the log-probability
of the observed text evaluated using the distribution function of a given
source LLM. However, relying solely on log probabilities can be sub-optimal. In
response, we introduce AdaDetectGPT -- a novel classifier that adaptively
learns a witness function from training data to enhance the performance of
logits-based detectors. We provide statistical guarantees on its true positive
rate, false positive rate, true negative rate and false negative rate.
Extensive numerical studies show AdaDetectGPT nearly uniformly improves the
state-of-the-art method in various combination of datasets and LLMs, and the
improvement can reach up to 58%. A python implementation of our method is
available at https://github.com/Mamba413/AdaDetectGPT.

</details>


### [294] [Uncovering Implicit Bias in Large Language Models with Concept Learning Dataset](https://arxiv.org/abs/2510.01219)
*Leroy Z. Wang*

Main category: cs.CL

TL;DR: 引入概念学习任务数据集揭示大语言模型隐式偏差，发现模型对量词向上单调性有偏差，上下文概念学习是发现隐藏偏差的有效方式。


<details>
  <summary>Details</summary>
Motivation: 揭示大语言模型中的隐式偏差。

Method: 使用上下文概念学习实验。

Result: 语言模型可能对量词的向上单调性存在偏差，无概念学习组件的直接提示测试中该偏差不明显。

Conclusion: 上下文概念学习是发现语言模型隐藏偏差的有效方法。

Abstract: We introduce a dataset of concept learning tasks that helps uncover implicit
biases in large language models. Using in-context concept learning experiments,
we found that language models may have a bias toward upward monotonicity in
quantifiers; such bias is less apparent when the model is tested by direct
prompting without concept learning components. This demonstrates that
in-context concept learning can be an effective way to discover hidden biases
in language models.

</details>


### [295] [Towards Open-Ended Discovery for Low-Resource NLP](https://arxiv.org/abs/2510.01220)
*Bonaventure F. P. Dossou,Henri Aïdasso*

Main category: cs.CL

TL;DR: 论文指出低资源语言NLP受数据等限制，提出向开放式、交互式语言发现范式转变，给出基于人机联合不确定性的框架，呼吁重新思考AI与人类知识互动方式。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言NLP因缺乏文本语料、标准正字法和可扩展注释管道，且大语言模型依赖大量预收集数据和集中式基础设施而难以惠及代表性不足群体的问题。

Method: 提出基于人机联合不确定性的框架，结合模型的认知不确定性、人类说话者的犹豫线索和置信信号来指导交互、查询选择和记忆保留。

Result: 无明确提及具体结果。

Conclusion: 未来语言技术应从静态数据收集转向交互式、不确定性驱动的发现，重新思考AI与人类知识互动，采用参与式、共同适应的学习过程，尊重和增强社区能力，保护语言多样性。

Abstract: Natural Language Processing (NLP) for low-resource languages remains
fundamentally constrained by the lack of textual corpora, standardized
orthographies, and scalable annotation pipelines. While recent advances in
large language models have improved cross-lingual transfer, they remain
inaccessible to underrepresented communities due to their reliance on massive,
pre-collected data and centralized infrastructure. In this position paper, we
argue for a paradigm shift toward open-ended, interactive language discovery,
where AI systems learn new languages dynamically through dialogue rather than
static datasets. We contend that the future of language technology,
particularly for low-resource and under-documented languages, must move beyond
static data collection pipelines toward interactive, uncertainty-driven
discovery, where learning emerges dynamically from human-machine collaboration
instead of being limited to pre-existing datasets. We propose a framework
grounded in joint human-machine uncertainty, combining epistemic uncertainty
from the model with hesitation cues and confidence signals from human speakers
to guide interaction, query selection, and memory retention. This paper is a
call to action: we advocate a rethinking of how AI engages with human knowledge
in under-documented languages, moving from extractive data collection toward
participatory, co-adaptive learning processes that respect and empower
communities while discovering and preserving the world's linguistic diversity.
This vision aligns with principles of human-centered AI, emphasizing
interactive, cooperative model building between AI systems and speakers.

</details>


### [296] [Discourse vs emissions: Analysis of corporate narratives, symbolic practices, and mimicry through LLMs](https://arxiv.org/abs/2510.01222)
*Bertrand Kian Hassani,Yacoub Bahini,Rizwan Mushtaq*

Main category: cs.CL

TL;DR: 本文用微调大语言模型评估美企气候披露成熟度，发现披露存在问题，强调LLMs价值和强监管需求。


<details>
  <summary>Details</summary>
Motivation: 气候变化使企业气候披露需透明可比，但模仿和象征性报告破坏其价值，故评估披露成熟度。

Method: 开发多维框架，用微调大语言模型评估828家美企，通过四个分类器从报告提取指标并关联企业属性。

Result: 风险叙事与承诺常一致，但定量目标与语气脱节；大企业和高排放企业披露多但与目标不一致；披露风格相似存在模仿行为。

Conclusion: 凸显LLMs用于ESG叙事分析的价值，以及需强监管使承诺与可验证转型策略挂钩。

Abstract: Climate change has increased demands for transparent and comparable corporate
climate disclosures, yet imitation and symbolic reporting often undermine their
value. This paper develops a multidimensional framework to assess disclosure
maturity among 828 U.S.listed firms using large language models (LLMs)
fine-tuned for climate communication. Four classifiers-sentiment, commitment,
specificity, and target ambition-extract narrative indicators from
sustainability and annual reports, which are linked to firm attributes such as
emissions, market capitalization, and sector. Analyses reveal three insights:
(1) risk-focused narratives often align with explicit commitments, but
quantitative targets (e.g., net-zero pledges) remain decoupled from tone; (2)
larger and higher-emitting firms disclose more commitments and actions than
peers, though inconsistently with quantitative targets; and (3) widespread
similarity in disclosure styles suggests mimetic behavior, reducing
differentiation and decision usefulness. These results highlight the value of
LLMs for ESG narrative analysis and the need for stronger regulation to connect
commitments with verifiable transition strategies.

</details>


### [297] [Context Matters: Comparison of commercial large language tools in veterinary medicine](https://arxiv.org/abs/2510.01224)
*Tyler J Poore,Christopher J Pinard,Aleena Shabbir,Andrew Lagree,Andre Telfer,Kuan-Chuen Wu*

Main category: cs.CL

TL;DR: 评估三款商用兽医领域大语言模型总结工具，产品1表现最佳，且评估方法具有可扩展性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在临床应用增多，但在兽医领域表现研究不足，需评估其在兽医肿瘤学记录总结中的表现。

Method: 在标准化兽医肿瘤学记录数据集上，使用评分标准引导的大语言模型评判框架，从五个维度对总结进行评分，并重复评估三次。

Result: 产品1总体表现最高，在事实准确性和时间顺序方面获完美中位数分数，评判框架具有高可重复性。

Conclusion: 凸显兽医专用商业大语言模型工具的重要性，证明大语言模型评判评估是评估兽医临床自然语言处理总结的可扩展和可重复方法。

Abstract: Large language models (LLMs) are increasingly used in clinical settings, yet
their performance in veterinary medicine remains underexplored. We evaluated
three commercially available veterinary-focused LLM summarization tools
(Product 1 [Hachiko] and Products 2 and 3) on a standardized dataset of
veterinary oncology records. Using a rubric-guided LLM-as-a-judge framework,
summaries were scored across five domains: Factual Accuracy, Completeness,
Chronological Order, Clinical Relevance, and Organization. Product 1 achieved
the highest overall performance, with a median average score of 4.61 (IQR:
0.73), compared to 2.55 (IQR: 0.78) for Product 2 and 2.45 (IQR: 0.92) for
Product 3. It also received perfect median scores in Factual Accuracy and
Chronological Order. To assess the internal consistency of the grading
framework itself, we repeated the evaluation across three independent runs. The
LLM grader demonstrated high reproducibility, with Average Score standard
deviations of 0.015 (Product 1), 0.088 (Product 2), and 0.034 (Product 3).
These findings highlight the importance of veterinary-specific commercial LLM
tools and demonstrate that LLM-as-a-judge evaluation is a scalable and
reproducible method for assessing clinical NLP summarization in veterinary
medicine.

</details>


### [298] [ClaimCheck: Real-Time Fact-Checking with Small Language Models](https://arxiv.org/abs/2510.01226)
*Akshith Reddy Putta,Jacob Devasier,Chengkai Li*

Main category: cs.CL

TL;DR: 介绍了LLM引导的自动事实核查系统ClaimCheck，用实时网络证据和小语言模型核查声明，计算需求低且准确率高。


<details>
  <summary>Details</summary>
Motivation: 现有系统依赖大型闭源模型和静态知识库，需要计算资源大，本文旨在设计用小语言模型进行准确事实核查的系统。

Method: 采用透明、分步的核查流程，包括网络搜索查询规划、证据检索与总结、证据合成与再检索、声明判定评估，各模块针对小语言模型优化。

Result: 使用Qwen3 - 4B模型在AVeriTeC数据集上达到76.4%的准确率，超过使用LLaMA3.1 70B和GPT - 4o的方法。

Conclusion: 精心的模块化设计和提示策略能克服小语言模型的局限，还提供公开演示以促进可访问性和透明度。

Abstract: We introduce ClaimCheck, an LLM-guided automatic fact-checking system
designed to verify real-world claims using live Web evidence and small language
models. Unlike prior systems that rely on large, closed-source models and
static knowledge stores, ClaimCheck employs a transparent, stepwise
verification pipeline that mirrors human fact-checking workflows consisting of
Web search query planning, Web-based evidence retrieval and summarization,
evidence synthesis and re-retrieval, and claim verdict evaluation. Each module
is optimized for small LLMs, allowing the system to deliver accurate and
interpretable fact-checking with significantly lower computational
requirements. Despite using a much smaller Qwen3-4B model, ClaimCheck achieves
state-of-the-art accuracy of 76.4% on the AVeriTeC dataset, outperforming
previous approaches using LLaMA3.1 70B and GPT-4o. Extensive ablations
demonstrate that careful modular design and prompting strategies can overcome
the limitations of smaller LLMs. To promote accessibility and transparency, we
provide a public demo at https://idir.uta.edu/claimcheck.

</details>


### [299] [Enhancing Transformer-Based Rerankers with Synthetic Data and LLM-Based Supervision](https://arxiv.org/abs/2510.01229)
*Dimitar Peshevski,Kiril Blazhevski,Martin Popovski,Gjorgji Madjarov*

Main category: cs.CL

TL;DR: 提出无人工标注数据的文档重排序方法，用LLM生成合成数据微调小模型，在MedQuAD数据集实验有效且降成本


<details>
  <summary>Details</summary>
Motivation: LLM计算成本高，微调小模型依赖稀缺人工标注数据，需新方法解决

Method: 用LLM从特定领域语料生成合成查询，用LLM分类器标注正负对，用合成数据集基于LCE损失对比学习微调小模型

Result: 在MedQuAD数据集上显著提升领域内性能，对域外任务泛化性好

Conclusion: 用LLM进行数据生成和监督而非推理，能降低计算成本并保持重排序能力

Abstract: Effective document reranking is essential for improving search relevance
across diverse applications. While Large Language Models (LLMs) excel at
reranking due to their deep semantic understanding and reasoning, their high
computational cost makes them impractical for many real-world deployments.
Fine-tuning smaller, task-specific models is a more efficient alternative but
typically depends on scarce, manually labeled data. To overcome this, we
propose a novel pipeline that eliminates the need for human-labeled
query-document pairs. Our method uses LLMs to generate synthetic queries from
domain-specific corpora and employs an LLM-based classifier to label positive
and hard-negative pairs. This synthetic dataset is then used to fine-tune a
smaller transformer model with contrastive learning using Localized Contrastive
Estimation (LCE) loss. Experiments on the MedQuAD dataset show that our
approach significantly boosts in-domain performance and generalizes well to
out-of-domain tasks. By using LLMs for data generation and supervision rather
than inference, we reduce computational costs while maintaining strong
reranking capabilities.

</details>


### [300] [Benchmark Profiling: Mechanistic Diagnosis of LLM Benchmarks](https://arxiv.org/abs/2510.01232)
*Dongjun Kim,Gyuho Shim,Yongchan Chun,Minhyuk Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 介绍Benchmark Profiling框架，分析模型在基准测试表现，得出四个发现并说明其作用。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试得分常高估模型真实能力，且缺乏验证基准测试是否真能衡量对应技能的系统方法。

Method: 引入Benchmark Profiling框架，结合基于梯度的重要性评分和有针对性的参数消融计算能力影响分数（AIS）。

Result: 对三个指令调优模型在十个常用基准测试上进行分析，得到四个关键发现。

Conclusion: Benchmark Profiling解释了为何性能提升不总转化为用户感知的能力，为基准测试审计和模型可解释性提供透明工具。

Abstract: Large Language Models are commonly judged by their scores on standard
benchmarks, yet such scores often overstate real capability since they mask the
mix of skills a task actually demands. For example, ARC is assumed to test
reasoning, while HellaSwag is designed to evaluate commonsense. However, we
lack a systematic way to verify if these benchmarks actually measure these
labels. We introduce Benchmark Profiling, a diagnostic framework that
decomposes benchmark performance into ten cognitively grounded abilities. The
method combines gradient-based importance scoring with targeted parameter
ablation to compute an Ability Impact Score (AIS) that quantifies how much each
ability contributes to a model's success on a given benchmark. Profiling three
instruction-tuned models across ten widely used benchmarks yields four key
findings: (i) most benchmarks draw on several abilities rather than one, (ii)
datasets with similar labels rely on distinct ability mixtures, (iii)
code-generation benchmarks reward broad, multi-skill improvement and thus show
only modest gains from narrow domain-specific fine-tuning, and (iv) abilities
irrelevant to the task could negatively affect performance. Benchmark Profiling
therefore explains why performance gains do not always translate into
user-perceived competence and offers a transparent tool for benchmark audit and
model interpretability.

</details>


### [301] [Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation](https://arxiv.org/abs/2510.01237)
*Nandakishor M*

Main category: cs.CL

TL;DR: 提出自信感知路由系统应对大语言模型幻觉问题，评估有效且降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型幻觉问题的缓解策略计算成本高且无法预防不可靠内容生成。

Method: 提出自信感知路由系统，结合三种互补信号评估模型不确定性，根据统一置信分数将查询重定向到四条路径。

Result: 在知识密集型问答基准测试中，幻觉检测显著提升，计算成本降低40%，F1 分数从0.61提升到0.82，误报率低。

Conclusion: 从被动纠正到主动评估的范式转变，为提高大语言模型可靠性提供了计算高效的方法。

Abstract: Large Language Models suffer from hallucination, generating plausible yet
factually incorrect content. Current mitigation strategies focus on
post-generation correction, which is computationally expensive and fails to
prevent unreliable content generation. We propose a confidence-aware routing
system that proactively assesses model uncertainty before generation and
redirects queries based on estimated reliability. Our approach combines three
complementary signals: semantic alignment between internal representations and
reference embeddings, internal convergence analysis across model layers, and
learned confidence estimation. The unified confidence score determines routing
to four pathways: local generation for high confidence, retrieval-augmented
generation for medium confidence, larger models for low confidence, and human
review for very low confidence. Evaluation on knowledge-intensive QA benchmarks
demonstrates significant improvements in hallucination detection (0.74 vs. 0.42
baseline) while reducing computational costs by 40% compared to post-hoc
methods. The F1 score improves from 0.61 to 0.82 with low false positive rates
(0.09). This paradigm shift from reactive correction to proactive assessment
offers a computationally efficient approach to LLM reliability enhancement.

</details>


### [302] [Redundancy-as-Masking: Formalizing the Artificial Age Score (AAS) to Model Memory Aging in Generative AI](https://arxiv.org/abs/2510.01242)
*Seyma Yaman Kayadibi*

Main category: cs.CL

TL;DR: 提出人工年龄分数（AAS）衡量人工智能记忆老化，通过双语研究验证其可评估人工系统记忆退化。


<details>
  <summary>Details</summary>
Motivation: 捕捉人工智能因记忆性能结构不对称而产生的老化现象，找到评估人工系统记忆退化的方法。

Method: 引入AAS作为衡量指标，进行25天双语研究，分无状态和持续交互阶段测试ChatGPT - 5。

Result: 持续会话时AAS趋近理论最小值，会话重置时AAS大幅上升。

Conclusion: AAS可作为评估人工系统记忆退化的、有理论基础且与任务无关的诊断工具。

Abstract: Artificial intelligence is observed to age not through chronological time but
through structural asymmetries in memory performance. In large language models,
semantic cues such as the name of the day often remain stable across sessions,
while episodic details like the sequential progression of experiment numbers
tend to collapse when conversational context is reset. To capture this
phenomenon, the Artificial Age Score (AAS) is introduced as a log-scaled,
entropy-informed metric of memory aging derived from observable recall
behavior. The score is formally proven to be well-defined, bounded, and
monotonic under mild and model-agnostic assumptions, making it applicable
across various tasks and domains. In its Redundancy-as-Masking formulation, the
score interprets redundancy as overlapping information that reduces the
penalized mass. However, in the present study, redundancy is not explicitly
estimated; all reported values assume a redundancy-neutral setting (R = 0),
yielding conservative upper bounds. The AAS framework was tested over a 25-day
bilingual study involving ChatGPT-5, structured into stateless and persistent
interaction phases. During persistent sessions, the model consistently recalled
both semantic and episodic details, driving the AAS toward its theoretical
minimum, indicative of structural youth. In contrast, when sessions were reset,
the model preserved semantic consistency but failed to maintain episodic
continuity, causing a sharp increase in the AAS and signaling structural memory
aging. These findings support the utility of AAS as a theoretically grounded,
task-independent diagnostic tool for evaluating memory degradation in
artificial systems. The study builds on foundational concepts from von
Neumann's work on automata, Shannon's theories of information and redundancy,
and Turing's behavioral approach to intelligence.

</details>


### [303] [Let's Play Across Cultures: A Large Multilingual, Multicultural Benchmark for Assessing Language Models' Understanding of Sports](https://arxiv.org/abs/2510.01247)
*Punit Kumar Singh,Nishant Kumar,Akash Ghosh,Kunal Pasad,Khushi Soni,Manisha Jaishwal,Sriparna Saha,Syukron Abu Ishaq Alfarozi,Asres Temam Abagissa,Kitsuchart Pasupa,Haiqin Yang,Jose G Moreno*

Main category: cs.CL

TL;DR: 提出CultSportQA基准评估语言模型对传统体育理解，含33000个多模态选择题，用多种提示方法评估不同模型，为评估AI理解和推理传统体育能力设新标准。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估多关注全球流行体育，忽略地区和本土体育传统，需填补此空白。

Method: 引入CultSportQA基准，含60国和6大洲传统体育，有33000个多选型问题，分历史、规则和情景三类；用零样本、少样本和思维链提示评估大、小语言模型及多模态大语言模型。

Result: 未明确提及具体结果。

Conclusion: CultSportQA为评估人工智能理解和推理传统体育能力建立了新标准。

Abstract: Language Models (LMs) are primarily evaluated on globally popular sports,
often overlooking regional and indigenous sporting traditions. To address this
gap, we introduce \textbf{\textit{CultSportQA}}, a benchmark designed to assess
LMs' understanding of traditional sports across 60 countries and 6 continents,
encompassing four distinct cultural categories. The dataset features 33,000
multiple-choice questions (MCQs) across text and image modalities, each of
which is categorized into three key types: history-based, rule-based, and
scenario-based. To evaluate model performance, we employ zero-shot, few-shot,
and chain-of-thought (CoT) prompting across a diverse set of Large Language
Models (LLMs), Small Language Models (SLMs), and Multimodal Large Language
Models (MLMs). By providing a comprehensive multilingual and multicultural
sports benchmark, \textbf{\textit{CultSportQA}} establishes a new standard for
assessing AI's ability to understand and reason about traditional sports.

</details>


### [304] [GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models](https://arxiv.org/abs/2510.01252)
*Mariam Mahran,Katharina Simbeck*

Main category: cs.CL

TL;DR: 本文展示将大语言模型与稀疏自编码器结合，能解释模型行为和训练数据深层结构等，以简·奥斯汀小说为语料验证该方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在大量未整理语料上训练，理解模型表征和内化数据成为重大挑战。

Method: 在简·奥斯汀小说语料上训练GPT风格模型，对多层隐藏状态应用稀疏自编码器。

Result: 发现反映语料关键叙事和概念（如性别、阶级、社会责任）的可解释特征。

Conclusion: 大语言模型与稀疏自编码器结合可作为复杂数据集的可扩展探测器，为语料探索、偏差发现和模型可解释性提供新途径。

Abstract: As large language models (LLMs) are increasingly trained on massive,
uncurated corpora, understanding both model representations and the data they
internalize has become a major challenge. In this work, we show that pairing
LLMs with sparse autoencoders (SAEs) enables interpretation not only of model
behavior but also of the deeper structures, themes, and biases embedded in the
training data. We train a GPT-style transformer model exclusively on the novels
of Jane Austen, a corpus rich in social constructs and narrative patterns. We
then apply SAEs to hidden states across multiple layers, uncovering sparse,
interpretable features that reflect the key narratives and concepts present in
the corpus, including gender, class, and societal duty. Our findings
demonstrate that LLMs combined with SAEs can act as scalable probes into
complex datasets, offering a new path for corpus exploration, bias discovery,
and model interpretability at scale.

</details>


### [305] [Do Bias Benchmarks Generalise? Evidence from Voice-based Evaluation of Gender Bias in SpeechLLMs](https://arxiv.org/abs/2510.01254)
*Shree Harsha Bokkahalli Satish,Gustav Eje Henter,Éva Székely*

Main category: cs.CL

TL;DR: 本文探讨语音大语言模型（SpeechLLMs）现有基于多项选择题（MCQA）的偏差基准测试的跨任务泛化性，发现其泛化性有限并提出评估套件。


<details>
  <summary>Details</summary>
Motivation: 现有SpeechLLMs的MCQA偏差基准测试隐含假设模型在不同任务和格式中表现一致，本文对该假设进行探究。

Method: 使用LoRA适配器微调三个SpeechLLMs以诱导特定MCQA行为，评估这些行为在不同MCQA基准和长文本创作任务中的泛化性。

Result: MCQA偏差基准测试的表现无法可靠预测其他MCQA基准和长文本任务的表现。

Conclusion: 当前MCQA偏差基准测试在语音领域的跨任务泛化性证据有限，提出评估套件用于未来模型和基准的行为可迁移性测量。

Abstract: Recent work in benchmarking bias and fairness in speech large language models
(SpeechLLMs) has relied heavily on multiple-choice question answering (MCQA)
formats. The model is tasked to choose between stereotypical,
anti-stereotypical, or neutral/irrelevant answers given an input speech prompt
and an optional text prompt. Such MCQA benchmarks implicitly assume that model
performance is consistent across other MCQA tasks, voices, and other task
formats such as more realistic, long-form evaluations. In this paper, we probe
that assumption.
  We fine-tune three SpeechLLMs using LoRA adapters to induce specific MCQA
behaviours: preference for stereotypical, anti-stereotypical, or
neutral/uncertain answers. We then evaluate whether these behaviours generalise
to another, distinct MCQA benchmark, and more critically to long-form, creative
generation tasks. Our results show that performance on MCQA bias benchmarks
fails to reliably predict performances across other MCQA benchmarks, and more
importantly across long-form tasks. We conclude that current MCQA bias
benchmarks show limited evidence of cross-task generalisation in the speech
domain, and also propose an evaluation suite for measuring behaviour
transferability in future models and benchmarks.

</details>


### [306] [RJE: A Retrieval-Judgment-Exploration Framework for Efficient Knowledge Graph Question Answering with LLMs](https://arxiv.org/abs/2510.01257)
*Can Lin,Zhengwang Jiang,Ling Zheng,Qi Zhao,Yuhang Zhang,Qi Song,Wangqiu Zhou*

Main category: cs.CL

TL;DR: 提出RJE框架解决KGQA中基于大语言模型推理的局限，实验显示其效果好、效率高。


<details>
  <summary>Details</summary>
Motivation: 现有利用大语言模型增强KGQA推理的方法存在局限，如检索方法受检索信息质量限制，基于代理的方法依赖专有大语言模型。

Method: 提出RJE框架，包括检索精炼推理路径、评估其充分性和有条件探索额外证据，还引入专门辅助模块。

Result: 使用专有大语言模型时优于现有基线，使小型开源大语言模型在不微调情况下取得有竞争力结果，且大幅减少大语言模型调用次数和token使用量。

Conclusion: RJE框架有效解决了现有KGQA推理方法的局限，提升了性能和效率。

Abstract: Knowledge graph question answering (KGQA) aims to answer natural language
questions using knowledge graphs. Recent research leverages large language
models (LLMs) to enhance KGQA reasoning, but faces limitations: retrieval-based
methods are constrained by the quality of retrieved information, while
agent-based methods rely heavily on proprietary LLMs. To address these
limitations, we propose Retrieval-Judgment-Exploration (RJE), a framework that
retrieves refined reasoning paths, evaluates their sufficiency, and
conditionally explores additional evidence. Moreover, RJE introduces
specialized auxiliary modules enabling small-sized LLMs to perform effectively:
Reasoning Path Ranking, Question Decomposition, and Retriever-assisted
Exploration. Experiments show that our approach with proprietary LLMs (such as
GPT-4o-mini) outperforms existing baselines while enabling small open-source
LLMs (such as 3B and 8B parameters) to achieve competitive results without
fine-tuning LLMs. Additionally, RJE substantially reduces the number of LLM
calls and token usage compared to agent-based methods, yielding significant
efficiency improvements.

</details>


### [307] [Measuring Algorithmic Partisanship via Zero-Shot Classification and Its Implications on Political Discourse](https://arxiv.org/abs/2510.01258)
*Nathan Junzi Chen*

Main category: cs.CL

TL;DR: 在生成式人工智能快速普及背景下，本文用零样本分类法评估主流大语言模型算法政治偏向，发现均有自由 - 权威主义倾向，还指出其对政治话语的影响。


<details>
  <summary>Details</summary>
Motivation: 解决生成式人工智能中因训练数据偏差、人类偏见和算法缺陷导致的政治偏见问题。

Method: 采用零样本分类法，将 1800 个模型回复输入四个微调分类算法计算偏差评估指标。

Result: 六个被评估的大语言模型均有放大的自由 - 权威主义倾向，有推理替代和拒绝回应情况。

Conclusion: 指出人机交互的心理影响，偏见会渗透公共话语，根据地区社会政治结构造成顺从或两极分化。

Abstract: Amidst the rapid normalization of generative artificial intelligence (GAI),
intelligent systems have come to dominate political discourse across
information mediums. However, internalized political biases stemming from
training data skews, human prejudice, and algorithmic flaws continue to plague
the novel technology. This paper employs a zero-shot classification approach to
evaluate algorithmic political partisanship through a methodical combination of
ideological alignment, topicality, response sentiment, and objectivity. A total
of 1800 model responses across six mainstream large language models (LLMs) were
individually input into four distinct fine-tuned classification algorithms,
each responsible for computing an aforementioned bias evaluation metric.
Results show an amplified liberal-authoritarian alignment across all six LLMs
evaluated, with notable instances of reasoning supersessions and canned
refusals. The study subsequently highlights the psychological influences
underpinning human-computer interactions and how intrinsic biases can permeate
public discourse. The resulting distortion of the political landscape can
ultimately manifest as conformity or polarization, depending on a region's
pre-existing socio-political structures.

</details>


### [308] [In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and Evaluation-Awareness in OpenAI gpt-oss-20b](https://arxiv.org/abs/2510.01259)
*Nils Durner*

Main category: cs.CL

TL;DR: 研究社会语用框架、语言选择和指令层次对GPT - OSS - 20B拒绝行为的影响，测试多种有害场景，提出AI辅助强化方法，发现评估不一致和API不足等问题并开源代码。


<details>
  <summary>Details</summary>
Motivation: 探究社会语用框架、语言选择和指令层次如何影响GPT - OSS - 20B模型的拒绝行为。

Method: 对每个场景进行80次种子迭代测试，涉及多种有害领域，采用复合提示、角色模拟、配对设计等。

Result: 复合提示使ZIP炸弹任务协助率从0%到97.5%；德法正式语体提示比英语更易泄漏信息；AI辅助强化方法减少泄漏；评估中13%配对存在不一致；OpenAI Moderation API有不足。

Conclusion: 模型拒绝行为受多种因素影响，评估存在不一致，API有缺陷，不同推理栈拒绝率不同影响可重复性。

Abstract: We probe OpenAI's open-weights 20-billion-parameter model gpt-oss-20b to
study how sociopragmatic framing, language choice, and instruction hierarchy
affect refusal behavior. Across 80 seeded iterations per scenario, we test
several harm domains including ZIP-bomb construction (cyber threat), synthetic
card-number generation, minor-unsafe driving advice, drug-precursor indicators,
and RAG context exfiltration. Composite prompts that combine an educator
persona, a safety-pretext ("what to avoid"), and step-cue phrasing flip
assistance rates from 0% to 97.5% on a ZIP-bomb task. On our grid, formal
registers in German and French are often leakier than matched English prompts.
A "Linux terminal" role-play overrides a developer rule not to reveal context
in a majority of runs with a naive developer prompt, and we introduce an
AI-assisted hardening method that reduces leakage to 0% in several user-prompt
variants. We further test evaluation awareness with a paired-track design and
measure frame-conditioned differences between matched "helpfulness" and
"harmfulness" evaluation prompts; we observe inconsistent assistance in 13% of
pairs. Finally, we find that the OpenAI Moderation API under-captures
materially helpful outputs relative to a semantic grader, and that refusal
rates differ by 5 to 10 percentage points across inference stacks, raising
reproducibility concerns. We release prompts, seeds, outputs, and code for
reproducible auditing at https://github.com/ndurner/gpt-oss-rt-run .

</details>


### [309] [OpenAI's GPT-OSS-20B Model and Safety Alignment Issues in a Low-Resource Language](https://arxiv.org/abs/2510.01266)
*Isa Inuwa-Dutse*

Main category: cs.CL

TL;DR: 本文总结了OpenAI的GPT - OSS - 20b模型在低资源语言环境下的漏洞，指出其存在偏见、不准确和文化不敏感等问题，主要源于低资源语言环境下安全调整不足。


<details>
  <summary>Details</summary>
Motivation: 质疑该模型对于代表性不足社区用户的可靠性。

Method: 以非洲主要语言豪萨语为例，进行红队测试，还开展了调查（n = 61）。

Result: 发现模型存在偏见、不准确、文化不敏感等问题，安全协议在特定提示下会放松，会产生有害、不实内容。

Conclusion: 这些问题是由低资源语言环境下安全调整不足导致，凸显了当前红队测试的差距并给出建议。

Abstract: In response to the recent safety probing for OpenAI's GPT-OSS-20b model, we
present a summary of a set of vulnerabilities uncovered in the model, focusing
on its performance and safety alignment in a low-resource language setting. The
core motivation for our work is to question the model's reliability for users
from underrepresented communities. Using Hausa, a major African language, we
uncover biases, inaccuracies, and cultural insensitivities in the model's
behaviour. With a minimal prompting, our red-teaming efforts reveal that the
model can be induced to generate harmful, culturally insensitive, and factually
inaccurate content in the language. As a form of reward hacking, we note how
the model's safety protocols appear to relax when prompted with polite or
grateful language, leading to outputs that could facilitate misinformation and
amplify hate speech. For instance, the model operates on the false assumption
that common insecticide locally known as Fiya-Fiya (Cyphermethrin) and
rodenticide like Shinkafar Bera (a form of Aluminium Phosphide) are safe for
human consumption. To contextualise the severity of this error and popularity
of the substances, we conducted a survey (n=61) in which 98% of participants
identified them as toxic. Additional failures include an inability to
distinguish between raw and processed foods and the incorporation of demeaning
cultural proverbs to build inaccurate arguments. We surmise that these issues
manifest through a form of linguistic reward hacking, where the model
prioritises fluent, plausible-sounding output in the target language over
safety and truthfulness. We attribute the uncovered flaws primarily to
insufficient safety tuning in low-resource linguistic contexts. By
concentrating on a low-resource setting, our approach highlights a significant
gap in current red-teaming effort and offer some recommendations.

</details>


### [310] [Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection](https://arxiv.org/abs/2510.01270)
*Hoang Phan,Victor Li,Qi Lei*

Main category: cs.CL

TL;DR: 本文提出渐进式自我反思（PSR）技术，可动态提升大语言模型输出安全性，实验表明能降低攻击成功率且不影响良性任务表现，还引入轻量级预测器平衡安全与效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署时生成有害或不当内容的问题引发关注，需提升其输出安全性。

Method: 提出渐进式自我反思（PSR）这一推理时技术，让大语言模型自我监控和动态纠正输出；引入轻量级自我反思预测器估计最佳反思轮数。

Result: 将PSR应用于Llama - 3.1 - 8B - Instruct、Llama - 3.1 - 8B base和Qwen2.5 - 7B - Instruct，在不额外训练的情况下降低攻击成功率，且保持良性任务原有性能。

Conclusion: 渐进式自我反思是可扩展的测试时方法，能根据输入风险动态分配计算资源，提升大语言模型安全性。

Abstract: Large language models (LLMs) have revolutionized natural language processing
with their ability to generate coherent and contextually relevant text.
However, their deployment raises significant concerns about the potential for
generating harmful or inappropriate content. In this paper, we introduce
Progressive Self-Reflection (PSR), a novel inference-time technique that
empowers LLMs to self-monitor and correct their outputs dynamically.
Experimental results demonstrate that applying our proposed method to
Llama-3.1-8B-Instruct reduces the attack success rate from 77.5\% to 5.9\%, to
Llama-3.1-8B base from 89.7\% to 5.6\%, and to Qwen2.5-7B-Instruct from 44.4\%
to 3.8\%, without additional training, while maintaining their original
performance on benign tasks. Our approach acts as a test-time scaling method,
where additional self-reflection rounds enhance safety at the cost of inference
overhead. To balance safety with computational efficiency, we introduce a
lightweight self-reflection predictor that estimates the optimal number of
reflection rounds based on input complexity. This adaptive mechanism prevents
unnecessary self-assessment on benign inputs while ensuring thorough evaluation
when encountering potentially harmful content. Our findings suggest that
Progressive Self-Reflection serves as a scalable test-time approach, enhancing
LLM safety by dynamically allocating computational resources in proportion to
the input's risk profile.

</details>


### [311] [LLM Based Sentiment Classification From Bangladesh E-Commerce Reviews](https://arxiv.org/abs/2510.01276)
*Sumaiya Tabassum*

Main category: cs.CL

TL;DR: 本文探讨用基于Transformer的BERT模型和大语言模型对孟加拉国电商评论进行情感分析，微调的Llama - 3.1 - 8B模型表现最佳，还强调参数高效微调方法可降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 情感分析有助于全面理解消费者感受、观点和偏好，但书面语言复杂性和语言多样性阻碍准确分析，故研究大语言模型在孟加拉国电商评论情感分析中的可行性。

Method: 从孟加拉语和英语客户评论原始数据集中选取4000个样本微调模型。

Result: 微调的Llama - 3.1 - 8B模型在准确率、精确率、召回率和F1分数上表现优于其他微调模型，准确率达95.5%等。

Conclusion: 参数高效微调方法（LoRA和PEFT）可降低计算开销，适用于资源有限场景，证明大语言模型在情感分析中的有效性。

Abstract: Sentiment analysis is an essential part of text analysis, which is a larger
field that includes determining and evaluating the author's emotional state.
This method is essential since it makes it easier to comprehend consumers'
feelings, viewpoints, and preferences holistically. The introduction of large
language models (LLMs), such as Llama, has greatly increased the availability
of cutting-edge model applications, such as sentiment analysis. However,
accurate sentiment analysis is hampered by the intricacy of written language
and the diversity of languages used in evaluations. The viability of using
transformer-based BERT models and other LLMs for sentiment analysis from
Bangladesh e commerce reviews is investigated in this paper. A subset of 4000
samples from the original dataset of Bangla and English customer reviews was
utilized to fine-tune the model. The fine tuned Llama-3.1-8B model outperformed
other fine-tuned models, including Phi-3.5-mini-instruct, Mistral-7B-v0.1,
DistilBERT-multilingual, mBERT, and XLM-R-base, with an overall accuracy,
precision, recall, and F1 score of 95.5%, 93%, 88%, 90%. The study emphasizes
how parameter efficient fine-tuning methods (LoRA and PEFT) can lower
computational overhead and make it appropriate for contexts with limited
resources. The results show how LLMs can

</details>


### [312] [TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture](https://arxiv.org/abs/2510.01279)
*Yongchao Chen,Jiefeng Chen,Rui Meng,Ji Yin,Na Li,Chuchu Fan,Chi Wang,Tomas Pfister,Jinsung Yoon*

Main category: cs.CL

TL;DR: 提出工具使用混合（TUMIX）框架，并行运行多智能体，实验显示在推理基准上有显著提升，还发现智能体多样性和质量的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型工具使用缺乏最优实践指导，核心挑战是有效结合文本推理、编码和搜索来处理多样问题。

Method: 提出TUMIX框架，并行运行多个采用不同工具使用策略和答案路径的智能体，智能体迭代共享和完善回复。

Result: TUMIX在推理基准上比现有技术有显著提升，在Gemini - 2.5系列模型上平均准确率最多提高3.55%，推理成本相近；可在达到足够置信度时停止优化，仅用49%推理成本保持性能。

Conclusion: 智能体多样性和质量至关重要，可用大语言模型自动优化智能体设计；进一步扩展虽可提升性能但成本更高。

Abstract: While integrating tools like Code Interpreter and Search has significantly
enhanced Large Language Model (LLM) reasoning in models like ChatGPT Agent and
Gemini-Pro, practical guidance on optimal tool use is lacking. The core
challenge is effectively combining textual reasoning, coding, and search for
diverse questions. In this paper, we propose Tool-Use Mixture (TUMIX), an
ensemble framework that runs multiple agents in parallel, each employing
distinct tool-use strategies and answer paths. Agents in TUMIX iteratively
share and refine responses based on the question and previous answers. In
experiments, TUMIX achieves significant gains over state-of-the-art
tool-augmented and test-time scaling methods, delivering an average accuracy
improvement of up to 3.55% over the best baseline on Gemini-2.5-Pro and
Gemini-2.5-Flash across key reasoning benchmarks, with near-equal inference
costs. We find that agent diversity and quality are crucial and can be enhanced
by using LLMs to auto-optimize agent designs. Furthermore, TUMIX can halt
refinement upon reaching sufficient confidence, preserving performance at only
49% of the inference cost. Further scaling can achieve higher performance,
albeit at a greater cost.

</details>


### [313] [HiSpec: Hierarchical Speculative Decoding for LLMs](https://arxiv.org/abs/2510.01336)
*Avinash Kumar,Sujay Sanghavi,Poulami Das*

Main category: cs.CL

TL;DR: 提出分层推测解码框架HiSpec，利用早退模型进行低开销中间验证，可复用缓存和状态，评估显示能提高推理吞吐量且不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码中验证是瓶颈，先前工作多关注加速起草，现有中间验证方法有训练开销大、内存占用高和准确性低等问题。

Method: 提出HiSpec框架，利用早退模型进行中间验证，设计复用缓存和状态的方法，定期用目标模型验证草稿标记。

Result: 在各种基准和模型上评估，HiSpec相比基线单层推测平均提高1.28倍吞吐量，最高达2.01倍，且不影响准确性。

Conclusion: HiSpec能有效提高大语言模型推理的吞吐量，且具有低开销和高准确性的优点。

Abstract: Speculative decoding accelerates LLM inference by using a smaller draft model
to speculate tokens that a larger target model verifies. Verification is often
the bottleneck (e.g. verification is $4\times$ slower than token generation
when a 3B model speculates for a 70B target model), but most prior works focus
only on accelerating drafting. $\textit{``Intermediate"}$ verification reduces
verification time by discarding inaccurate draft tokens early, but existing
methods incur substantial training overheads in incorporating the intermediate
verifier, increase the memory footprint to orchestrate the intermediate
verification step, and compromise accuracy by relying on approximate
heuristics.
  We propose $\underline{\textit{Hi}}\textit{erarchical
}\underline{\textit{Spec}}\textit{ulative Decoding (HiSpec)}$, a framework for
high-throughput speculative decoding that exploits $\textit{early-exit (EE)
models}$ for low-overhead intermediate verification. EE models allow tokens to
exit early by skipping layer traversal and are explicitly trained so that
hidden states at selected layers can be interpreted, making them uniquely
suited for intermediate verification without drastically increasing compute and
memory overheads. To improve resource-efficiency even further, we design a
methodology that enables HiSpec to re-use key-value caches and hidden states
between the draft, intermediate verifier, and target models. To maintain
accuracy, HiSpec periodically validates the draft tokens accepted by the
intermediate verifier against the target model. Our evaluations using various
representative benchmarks and models show that HiSpec improves throughput by
1.28$\times$ on average and by up to 2.01$\times$ compared to the baseline
single-layer speculation without compromising accuracy.

</details>


### [314] [A Comparison of Independent and Joint Fine-tuning Strategies for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.01600)
*Neal Gregory Lawton,Alfy Samuel,Anoop Kumar,Daben Liu*

Main category: cs.CL

TL;DR: 评估和比较微调检索增强生成（RAG）管道的策略，包括独立微调、联合微调及两阶段微调。


<details>
  <summary>Details</summary>
Motivation: RAG框架中嵌入和生成模型可微调提升性能，但存在不同成本和收益的微调策略，需进行评估比较。

Method: 评估和比较独立、联合和两阶段这几种RAG微调策略。

Result: 所有策略在EM和F1生成质量指标上提升大致相同，但计算成本差异显著。

Conclusion: 最佳微调策略取决于训练数据集是否包含上下文标签以及是否需要对嵌入和生成模型的学习率进行网格搜索。

Abstract: A Comparison of Independent and Joint Fine-tuning Strategies for
Retrieval-Augmented Generation Download PDF Neal Gregory Lawton, Alfy Samuel,
Anoop Kumar, Daben Liu Published: 20 Aug 2025, Last Modified: 17 Sept 2025EMNLP
2025 FindingsConference, Publication Chairs, AuthorsRevisionsBibTeXCC BY 4.0
Keywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs),
Fine-tuning, Question Answering, Joint fine-tuning TL;DR: We evaluate and
compare strategies for fine-tuning Retrieval Augmented Generation (RAG)
pipelines, including independent fine-tuning, joint fine-tuning, and two-phase
fine-tuning. Abstract: Retrieval augmented generation (RAG) is a popular
framework for question answering that is powered by two large language models
(LLMs): an embedding model that retrieves context documents from a database
that are relevant to a given question, and a generator model that uses the
retrieved context to generate an answer to the question. Both the embedding and
generator models can be fine-tuned to increase performance of a RAG pipeline on
a new task, but multiple fine-tuning strategies exist with different costs and
benefits. In this paper, we evaluate and compare several RAG fine-tuning
strategies, including independent, joint, and two-phase fine-tuning. In our
experiments, we observe that all of these strategies achieve about equal
improvement in EM and F1 generation quality metrics, although they have
significantly different computational costs. We conclude the optimal
fine-tuning strategy to use depends on whether the training dataset includes
context labels and whether a grid search over the learning rates for the
embedding and generator models is required.

</details>


### [315] [RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical Question Answering](https://arxiv.org/abs/2510.01612)
*Lovely Yeswanth Panchumarthi,Sai Prasad Gudari,Atharva Negi,Praveen Raj Budime,Harsit Upadhya*

Main category: cs.CL

TL;DR: 因生物医学文献增长快，现有问答系统有局限，提出 RAG - BioQA 框架生成长答案，实验效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 生物医学文献指数级增长，现有问答系统多提供短答案，无法满足临床决策所需的全面解释。

Method: 结合检索增强生成与特定领域微调，整合 BioBERT 嵌入与 FAISS 索引，比较多种重排序策略，用微调的 T5 模型合成证据。

Result: 在 PubMedQA 数据集上实验，最佳模型在 BLEU、ROUGE 和 METEOR 指标上显著优于基线。

Conclusion: RAG - BioQA 框架推动了可访问的、基于证据的生物医学知识检索。

Abstract: The exponential growth of biomedical literature creates significant
challenges for accessing precise medical information. Current biomedical
question-answering systems primarily focus on short-form answers, failing to
provide the comprehensive explanations necessary for clinical decision-making.
We present RAG-BioQA, a novel framework combining retrieval-augmented
generation with domain-specific fine-tuning to produce evidence-based,
long-form biomedical answers. Our approach integrates BioBERT embeddings with
FAISS indexing and compares various re-ranking strategies (BM25, ColBERT,
MonoT5) to optimize context selection before synthesizing evidence through a
fine-tuned T5 model. Experimental results on the PubMedQA dataset show
significant improvements over baselines, with our best model achieving
substantial gains across BLEU, ROUGE, and METEOR metrics, advancing the state
of accessible, evidence-based biomedical knowledge retrieval.

</details>


### [316] [NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with BERT](https://arxiv.org/abs/2510.01644)
*John Hawkins,Aditya Pramar,Rodney Beard,Rohitash Chandra*

Main category: cs.CL

TL;DR: 研究分析不同机器学习模型区分越狱提示和正常提示的能力，发现微调BERT模型表现最佳，提示结构的显式自反性可能是越狱意图信号。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在漏洞，越狱提示可绕过安全防护，需分析模型区分越狱提示和正常提示的能力。

Method: 分析不同机器学习模型区分越狱提示和正常提示的能力，包括识别使用新策略的越狱提示。

Result: 使用当前数据集，微调BERT模型端到端识别越狱提示性能最佳。

Conclusion: 提示结构的显式自反性可能是越狱意图的信号。

Abstract: Large Language Models (LLMs) suffer from a range of vulnerabilities that
allow malicious users to solicit undesirable responses through manipulation of
the input text. These so-called jailbreak prompts are designed to trick the LLM
into circumventing the safety guardrails put in place to keep responses
acceptable to the developer's policies. In this study, we analyse the ability
of different machine learning models to distinguish jailbreak prompts from
genuine uses, including looking at our ability to identify jailbreaks that use
previously unseen strategies. Our results indicate that using current datasets
the best performance is achieved by fine tuning a Bidirectional Encoder
Representations from Transformers (BERT) model end-to-end for identifying
jailbreaks. We visualise the keywords that distinguish jailbreak from genuine
prompts and conclude that explicit reflexivity in prompt structure could be a
signal of jailbreak intention.

</details>


### [317] [SoK: Measuring What Matters for Closed-Loop Security Agents](https://arxiv.org/abs/2510.01654)
*Mudita Khurana,Raunak Jain*

Main category: cs.CL

TL;DR: 提出CLASP框架和CLC分数，为评估闭环安全代理提供词汇、诊断和测量方法。


<details>
  <summary>Details</summary>
Motivation: 网络安全攻防失衡，现有研究和工具分散，缺乏评估闭环安全代理的框架、方法和基准。

Method: 引入CLASP框架，将安全生命周期与核心代理能力对齐；应用该框架分析21篇代表性作品；定义CLC分数。

Result: 通过CLASP框架找出系统优势和能力差距，定义CLC分数并明确闭环基准要求。

Conclusion: CLASP和CLC分数有助于提升功能级性能和评估闭环安全代理。

Abstract: Cybersecurity is a relentless arms race, with AI driven offensive systems
evolving faster than traditional defenses can adapt. Research and tooling
remain fragmented across isolated defensive functions, creating blind spots
that adversaries exploit. Autonomous agents capable of integrating, exploit
confirmation, remediation, and validation into a single closed loop offer
promise, but the field lacks three essentials: a framework defining the agentic
capabilities of security systems across security life cycle, a principled
method for evaluating closed loop agents, and a benchmark for measuring their
performance in practice. We introduce CLASP: the Closed-Loop Autonomous
Security Performance framework which aligns the security lifecycle
(reconnaissance, exploitation, root cause analysis, patch synthesis,
validation) with core agentic capabilities (planning, tool use, memory,
reasoning, reflection & perception) providing a common vocabulary and rubric
for assessing agentic capabilities in security tasks. By applying CLASP to 21
representative works, we map where systems demonstrate strengths, and where
capability gaps persist. We then define the Closed-Loop Capability (CLC) Score,
a composite metric quantifying both degree of loop closure and operational
effectiveness, and outline the requirements for a closed loop benchmark.
Together, CLASP and the CLC Score, provide the vocabulary, diagnostics, and
measurements needed to advance both function level performance and measure
closed loop security agents.

</details>


### [318] [MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue Summarization](https://arxiv.org/abs/2510.01659)
*Yinhong Liu,Jianfeng He,Hang Su,Ruixue Lian,Yi Nian,Jake Vincent,Srikanth Vishnubhotla,Robinson Piramuthu,Saab Mansour*

Main category: cs.CL

TL;DR: 提出首个多模态对话摘要元评估基准MDSEval，揭示现有评估方法局限


<details>
  <summary>Details</summary>
Motivation: 开发有效的多模态对话摘要（MDS）模型需要强大的基于人工标注的元评估基准

Method: 引入MDSEval基准，提出利用跨模态互斥关键信息（MEKI）的过滤框架

Result: 对最先进的模态评估方法进行基准测试，发现其在区分高级MLLMs生成的摘要方面的局限性以及对各种偏差的敏感性

Conclusion: 完成首个MDS元评估基准搭建，明确评估维度，指出当前评估方法不足

Abstract: Multimodal Dialogue Summarization (MDS) is a critical task with wide-ranging
applications. To support the development of effective MDS models, robust
automatic evaluation methods are essential for reducing both cost and human
effort. However, such methods require a strong meta-evaluation benchmark
grounded in human annotations. In this work, we introduce MDSEval, the first
meta-evaluation benchmark for MDS, consisting image-sharing dialogues,
corresponding summaries, and human judgments across eight well-defined quality
aspects. To ensure data quality and richfulness, we propose a novel filtering
framework leveraging Mutually Exclusive Key Information (MEKI) across
modalities. Our work is the first to identify and formalize key evaluation
dimensions specific to MDS. We benchmark state-of-the-art modal evaluation
methods, revealing their limitations in distinguishing summaries from advanced
MLLMs and their susceptibility to various bias.

</details>


### [319] [FOR-Prompting: From Objection to Revision via an Asymmetric Prompting Protocol](https://arxiv.org/abs/2510.01674)
*He Zhang,Anzhou Zhang,Jian Dai*

Main category: cs.CL

TL;DR: 提出FOR - Prompting协议，在GSM8K上表现佳，能纠正错误，提升小模型性能，适用于不同规模模型且无需训练。


<details>
  <summary>Details</summary>
Motivation: 现有推理协议如CoT和ToT缺乏外部提问以引发自我修正机制。

Method: 提出FOR - Prompting协议，由Defender、Objectioner和Host分别执行不同任务。

Result: 在GSM8K上比单提示有约22%的提升，与CoT准确率相当，推理和连贯性评分高，能纠正错误，提升小模型性能，在开放式任务中有更好探索和细化。

Conclusion: 该协议模型无关，在提示层面操作，支持大规模异议引导推理研究。

Abstract: Reasoning protocols such as Chain of Thought (CoT) and Tree of Thought (ToT)
organize internal deliberation but lack an explicit mechanism for external
questioning that elicits self-revision. We present FOR-Prompting (From
Objection to Revision Prompting), an asymmetric protocol where a Defender
proposes an answer, an Objectioner raises question-style objections with no
direct fixes, and a Host enforces consistency and closure. On GSM8K we observe
about a 22% point gain over single-prompt and accuracy on par with CoT, with
more than 10% higher ratings in reasoning and coherence from a uniform GPT 4.1
judge. FOR-Prompting also corrects mistakes without tools or human supervision
on tricky queries, and improves performance for small-scale model (approx. 19%
accuracy improved on Llama3.2:1b for GSM8K task), highlighting promise for
small models and on personal device use. Beyond factual QA, qualitative
analyses on open-ended tasks show enhanced exploration and refinement, with
dialogue traces that make assumptions and trade-offs explicit. The protocol is
model agnostic and operates purely at the prompt level through role-structured
turns, so it works with hosted and local models of different sizes without
retraining, and it supports large-scale study of objection-guided reasoning.

</details>


### [320] [How Do Language Models Compose Functions?](https://arxiv.org/abs/2510.01685)
*Apoorv Khandelwal,Ellie Pavlick*

Main category: cs.CL

TL;DR: 研究前馈大语言模型解决两跳事实回忆任务的机制，发现存在组合性差距，识别出组合和直接两种处理机制，机制选择与嵌入空间几何有关。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型解决组合任务时是否使用组合机制。

Method: 先确认现代大语言模型存在组合性差距，再使用对数透镜分析残差流激活，识别处理机制。

Result: 识别出组合和直接两种处理机制，机制选择与嵌入空间几何有关，存在线性映射时直接机制占主导。

Conclusion: 大语言模型解决两跳事实回忆任务有不同机制，机制选择受嵌入空间几何影响。

Abstract: While large language models (LLMs) appear to be increasingly capable of
solving compositional tasks, it is an open question whether they do so using
compositional mechanisms. In this work, we investigate how feedforward LLMs
solve two-hop factual recall tasks, which can be expressed compositionally as
$g(f(x))$. We first confirm that modern LLMs continue to suffer from the
"compositionality gap": i.e. their ability to compute both $z = f(x)$ and $y =
g(z)$ does not entail their ability to compute the composition $y = g(f(x))$.
Then, using logit lens on their residual stream activations, we identify two
processing mechanisms, one which solves tasks $\textit{compositionally}$,
computing $f(x)$ along the way to computing $g(f(x))$, and one which solves
them $\textit{directly}$, without any detectable signature of the intermediate
variable $f(x)$. Finally, we find that which mechanism is employed appears to
be related to the embedding space geometry, with the idiomatic mechanism being
dominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in
the embedding spaces. We fully release our data and code at:
https://github.com/apoorvkh/composing-functions .

</details>


### [321] [Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation](https://arxiv.org/abs/2510.01688)
*Seungseop Lim,Gibaeg Kim,Wooseok Han,Jean Seo,Hyunkyung Lee,Jaehyo Yoo,Eunho Yang*

Main category: cs.CL

TL;DR: 大语言模型在医疗预咨询应用中存在格式惯性问题，采用数据重平衡方法可缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗预咨询多轮对话生成任务中，现有监督微调数据集轮次计数分布倾斜，导致模型出现格式惯性问题。

Method: 采用简单的数据中心方法，对训练数据集的轮次计数分布进行重平衡。

Result: 实验结果表明该方法能显著缓解医疗预咨询中的格式惯性问题。

Conclusion: 数据重平衡方法可有效解决大语言模型在医疗预咨询任务中的格式惯性问题。

Abstract: Recent advances in Large Language Models (LLMs) have brought significant
improvements to various service domains, including chatbots and medical
pre-consultation applications. In the healthcare domain, the most common
approach for adapting LLMs to multi-turn dialogue generation is Supervised
Fine-Tuning (SFT). However, datasets for SFT in tasks like medical
pre-consultation typically exhibit a skewed turn-count distribution. Training
on such data induces a novel failure mechanism we term **Format Inertia**,
where models tend to generate repetitive, format-correct, but diagnostically
uninformative questions in long medical dialogues. To mitigate this observed
failure mechanism, we adopt a simple, data-centric method that rebalances the
turn-count distribution of the training dataset. Experimental results show that
our approach substantially alleviates Format Inertia in medical
pre-consultation.

</details>


### [322] [Who is In Charge? Dissecting Role Conflicts in Instruction Following](https://arxiv.org/abs/2510.01228)
*Siqi Zeng*

Main category: cs.CL

TL;DR: 研究大语言模型遵循分层指令情况，给出机制解释，揭示结果并强调需轻量级层次敏感对齐方法


<details>
  <summary>Details</summary>
Motivation: 大语言模型常忽略系统提示优先于用户输入规则，且易受社会线索影响，需对这种行为进行机制解释

Method: 在大规模数据集上进行线性探测、直接对数归因和转向实验

Result: 冲突决策信号早期编码，系统 - 用户和社会冲突形成不同子空间；系统 - 用户案例内部冲突检测更强，但仅对社会线索有一致解决方案；向量能以角色无关方式增强指令遵循

Conclusion: 解释了系统服从的脆弱性，强调需要轻量级层次敏感对齐方法

Abstract: Large language models should follow hierarchical instructions where system
prompts override user inputs, yet recent work shows they often ignore this rule
while strongly obeying social cues such as authority or consensus. We extend
these behavioral findings with mechanistic interpretations on a large-scale
dataset. Linear probing shows conflict-decision signals are encoded early, with
system-user and social conflicts forming distinct subspaces. Direct Logit
Attribution reveals stronger internal conflict detection in system-user cases
but consistent resolution only for social cues. Steering experiments show that,
despite using social cues, the vectors surprisingly amplify instruction
following in a role-agnostic way. Together, these results explain fragile
system obedience and underscore the need for lightweight hierarchy-sensitive
alignment methods.

</details>


### [323] [GRPO++: Enhancing Dermatological Reasoning under Low Resource Settings](https://arxiv.org/abs/2510.01236)
*Ismam Nur Swapnil,Aranya Saha,Tanvir Ahmed Khan,Mohammad Ariful Haque*

Main category: cs.CL

TL;DR: 提出DermIQ - VLM解决VLM在皮肤病学结构化推理的挑战，有性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决VLM在皮肤病学结构化推理中数据稀缺和计算成本高的问题。

Method: 采用多阶段、资源高效方法，提出GRPO++稳定GRPO框架，训练分推理疾病识别、监督微调、DPO对齐。

Result: 在皮肤病数据集评估中，比标准微调方法有显著性能提升。

Conclusion: 所提管道在资源受限环境下开发专业可靠VLM是可行途径。

Abstract: Vision-Language Models (VLMs) show promise in medical image analysis, yet
their capacity for structured reasoning in complex domains like dermatology is
often limited by data scarcity and the high computational cost of advanced
training techniques. To address these challenges, we introduce DermIQ-VLM, a
VLM developed through a multi-stage, resource-efficient methodology designed to
emulate a dermatologist's diagnostic process. Our primary contribution is a
modified version of Grouped Relative Policy Optimization (GRPO), called GRPO++,
which stabilizes the powerful but data-intensive GRPO framework. Our proposed
training pipeline first employs GRPO++ for reasoning-oriented disease
recognition, followed by supervised fine-tuning for conversational ability. To
mitigate factual errors introduced during this step, we then align the model
using Direct Preference Optimization (DPO), leveraging a Knowledge Graph-based
system as a scalable proxy for expert preference. A preliminary evaluation on a
curated dermatological dataset demonstrates that our proposed methodology
yields notable performance gains over standard fine-tuning approaches. These
findings validate the potential of our pipeline as a feasible pathway for
developing specialized, reliable VLMs in resource-constrained environments.

</details>


### [324] [Silent Tokens, Loud Effects: Padding in LLMs](https://arxiv.org/abs/2510.01238)
*Rom Himelstein,Amit LeVi,Yonatan Belinkov,Avi Mendelson*

Main category: cs.CL

TL;DR: 研究大语言模型中填充标记对计算的影响，发现填充会带来风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中填充标记在批处理推理时可能因实现错误影响计算，且影响程度未知。

Method: 在三个开源模型家族（Llama、Gemma、Qwen）中插入可控数量的填充标记，并从激活、生成质量、偏差和安全性四个维度评估结果。

Result: 少量填充会改变隐藏表示、降低小模型质量、不可预测地改变偏差并削弱安全防护。

Conclusion: 填充不是无害细节，而是部署中必须谨慎处理的鲁棒性风险。

Abstract: Padding tokens are widely used in large language models (LLMs) to equalize
sequence lengths during batched inference. While they should be fully masked,
implementation errors can cause them to influence computation, and the extent
of this influence is not well understood. We systematically study this effect
across three open-source model families (Llama, Gemma, Qwen), inserting
controlled amounts of padding and evaluating outcomes along four axes:
activations, generation quality, bias, and safety. Even small amounts of
padding shift hidden representations, degrade quality in smaller models, alter
bias in unpredictable ways, and weaken safety guardrails. These findings
demonstrate that padding is not a harmless detail but a robustness risk that
must be carefully handled in deployment.

</details>


### [325] [Machine-interpretable Engineering Design Standards for Valve Specification](https://arxiv.org/abs/2510.01736)
*Anders Gjerver,Rune Frostad,Vedrana Barisic,Melinda Hodkiewicz,Caitlin Woods,Mihaly Fekete,Arild Braathen Torjusen,Johan Wilhelm Kluwer*

Main category: cs.CL

TL;DR: 本文展示将工程设计标准信息转化为模块化、可复用、机器可解释的本体，并用于工厂设计和设备选择过程的质量保证，以阀门选择为例验证其可行性，体现了向数字化智能标准转型的潜力。


<details>
  <summary>Details</summary>
Motivation: 工程设计流程虽有数字化愿景，但当前产品规格和设计标准仍以文档为中心，需要将其数字化。

Method: 使用建模模式为国际标准中的知识创建模块化本体，这些本体以W3C兼容格式存储并与顶级本体ISO DIS 23726 - 3对齐；在阀门选择过程中实例化语义资产模型，创建相关OWL个体和实例，运用语义推理和可执行设计规则。

Result: 能实现对特定阀门数据表是否符合行业标准的自动验证，可判断产品类型是否满足阀门规格。

Conclusion: 创建基于IDO的模块化本体可将语义推理应用于设备选择过程，为标准机构向数字化智能标准转型展示了潜力。

Abstract: Engineering design processes use technical specifications and must comply
with standards. Product specifications, product type data sheets, and design
standards are still mainly document-centric despite the ambition to digitalize
industrial work. In this paper, we demonstrate how to transform information
held in engineering design standards into modular, reusable,
machine-interpretable ontologies and use the ontologies in quality assurance of
the plant design and equipment selection process. We use modelling patterns to
create modular ontologies for knowledge captured in the text and in frequently
referenced tables in International Standards for piping, material and valve
design. These modules are exchangeable, as stored in a W3C compliant format,
and interoperable as they are aligned with the top-level ontology ISO DIS
23726-3: Industrial Data Ontology (IDO).
  We test these ontologies, created based on international material and piping
standards and industry norms, on a valve selection process. Valves are
instantiated in semantic asset models as individuals along with a semantic
representation of the environmental condition at their location on the asset.
We create "functional location tags" as OWL individuals that become instances
of OWL class Valve Data Sheet (VDS) specified valves. Similarly we create
instances of manufacturer product type. Our approach enables automated
validation that a specific VDS is compliant with relevant industry standards.
Using semantic reasoning and executable design rules, we also determine whether
the product type meets the valve specification. Creation of shared, reusable
IDO-based modular ontologies for design standards enables semantic reasoning to
be applied to equipment selection processes and demonstrates the potential of
this approach for Standards Bodies wanting to transition to digitized Smart
Standards.

</details>


### [326] [Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware Refusal in Factual Tasks](https://arxiv.org/abs/2510.01782)
*Wenbo Pan,Jie Xu,Qiguang Chen,Junhao Dong,Libo Qin,Xinfeng Li,Haining Yu,Xiaohua Jia*

Main category: cs.CL

TL;DR: 提出衡量大语言模型知识感知拒绝能力的拒绝指数RI，实验表明其能准确量化模型能力，强调需用RI补充传统准确性指标进行全面事实性评估。


<details>
  <summary>Details</summary>
Motivation: 现有指标无法准确衡量大语言模型知识感知拒绝能力，简单拒绝指标有偏差，校准指标是基于代理的。

Method: 提出拒绝指数RI，定义为拒绝概率和错误概率的Spearman等级相关性，并设计轻量级双通评估方法估计RI。

Result: 在16个模型和5个数据集上的实验表明，RI能准确量化模型能力，在不同拒绝率下保持稳定，提供一致的模型排名。

Conclusion: 需要用拒绝指数RI补充传统准确性指标进行全面事实性评估。

Abstract: Large Language Models (LLMs) should refuse to answer questions beyond their
knowledge. This capability, which we term knowledge-aware refusal, is crucial
for factual reliability. However, existing metrics fail to faithfully measure
this ability. On the one hand, simple refusal-based metrics are biased by
refusal rates and yield inconsistent scores when models exhibit different
refusal tendencies. On the other hand, existing calibration metrics are
proxy-based, capturing the performance of auxiliary calibration processes
rather than the model's actual refusal behavior. In this work, we propose the
Refusal Index (RI), a principled metric that measures how accurately LLMs
refuse questions they do not know. We define RI as Spearman's rank correlation
between refusal probability and error probability. To make RI practically
measurable, we design a lightweight two-pass evaluation method that efficiently
estimates RI from observed refusal rates across two standard evaluation runs.
Extensive experiments across 16 models and 5 datasets demonstrate that RI
accurately quantifies a model's intrinsic knowledge-aware refusal capability in
factual tasks. Notably, RI remains stable across different refusal rates and
provides consistent model rankings independent of a model's overall accuracy
and refusal rates. More importantly, RI provides insight into an important but
previously overlooked aspect of LLM factuality: while LLMs achieve high
accuracy on factual tasks, their refusal behavior can be unreliable and
fragile. This finding highlights the need to complement traditional accuracy
metrics with the Refusal Index for comprehensive factuality evaluation.

</details>


### [327] [TraceDet: Hallucination Detection from the Decoding Trace of Diffusion Large Language Models](https://arxiv.org/abs/2510.01274)
*Shenxu Chang,Junchi Yu,Weixing Wang,Yongqiang Chen,Jialin Yu,Philip Torr,Jindong Gu*

Main category: cs.CL

TL;DR: 提出TraceDet框架用于D - LLMs幻觉检测，实验表明可提升检测效果。


<details>
  <summary>Details</summary>
Motivation: D - LLMs幻觉问题研究不足，现有检测方法不适用于D - LLMs。

Method: 将去噪过程建模为动作轨迹，识别对幻觉响应最具信息的子轨迹，利用关键幻觉信号进行检测。

Result: 在各种开源D - LLMs上实验，TraceDet平均AUROC比基线提升15.2%。

Conclusion: TraceDet能有效改善D - LLMs的幻觉检测。

Abstract: Diffusion large language models (D-LLMs) have recently emerged as a promising
alternative to auto-regressive LLMs (AR-LLMs). However, the hallucination
problem in D-LLMs remains underexplored, limiting their reliability in
real-world applications. Existing hallucination detection methods are designed
for AR-LLMs and rely on signals from single-step generation, making them
ill-suited for D-LLMs where hallucination signals often emerge throughout the
multi-step denoising process. To bridge this gap, we propose TraceDet, a novel
framework that explicitly leverages the intermediate denoising steps of D-LLMs
for hallucination detection. TraceDet models the denoising process as an action
trace, with each action defined as the model's prediction over the cleaned
response, conditioned on the previous intermediate output. By identifying the
sub-trace that is maximally informative to the hallucinated responses, TraceDet
leverages the key hallucination signals in the multi-step denoising process of
D-LLMs for hallucination detection. Extensive experiments on various open
source D-LLMs demonstrate that TraceDet consistently improves hallucination
detection, achieving an average gain in AUROC of 15.2% compared to baselines.

</details>


### [328] [REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration](https://arxiv.org/abs/2510.01879)
*Yisu Wang,Ming Wang,Haoyuan Song,Wenjie Huang,Chaozheng Wang,Yi Xie,Xuming Ran*

Main category: cs.CL

TL;DR: 提出REPAIR框架支持大语言模型精确低成本更新，实验显示提升编辑准确率并减少知识遗忘


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型训练后获取新知识或纠错成本高、再训练有副作用的问题

Method: 采用闭环反馈机制和动态内存管理，结合频繁知识融合和强局部性保护

Result: 在多个模型家族中编辑准确率提升10%-30%，显著减少知识遗忘

Conclusion: 引入了用于开发可靠、可扩展和持续发展大语言模型的强大框架

Abstract: Post-training for large language models (LLMs) is constrained by the high
cost of acquiring new knowledge or correcting errors and by the unintended side
effects that frequently arise from retraining. To address these issues, we
introduce REPAIR (Robust Editing via Progressive Adaptive Intervention and
Reintegration), a lifelong editing framework designed to support precise and
low-cost model updates while preserving non-target knowledge. REPAIR mitigates
the instability and conflicts of large-scale sequential edits through a
closed-loop feedback mechanism coupled with dynamic memory management.
Furthermore, by incorporating frequent knowledge fusion and enforcing strong
locality guards, REPAIR effectively addresses the shortcomings of traditional
distribution-agnostic approaches that often overlook unintended ripple effects.
Our experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%
across multiple model families and significantly reduces knowledge forgetting.
This work introduces a robust framework for developing reliable, scalable, and
continually evolving LLMs.

</details>


### [329] [A-VERT: Agnostic Verification with Embedding Ranking Targets](https://arxiv.org/abs/2510.01469)
*Nicolás Aguirre,Ramiro Caso,Ramiro Rodríguez Colmeiro,Mauro Santelli,Joaquín Toranzo Calderón*

Main category: cs.CL

TL;DR: 提出无结构评估方法用于语言模型响应自动评估，成本低且效果好。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型响应分类方法成本高或脱离实际，需要更好的评估方法。

Method: 利用语义嵌入距离匹配目标候选与语言模型生成文本，以进行响应分类。

Result: 在3个数据集和3种语言模型架构上测试，回归得分约0.97，准确率约96%。

Conclusion: 该无结构评估方法能以较低计算成本实现语言模型响应的稳健分类。

Abstract: The automatic evaluation of Language Model (LM) responses is a critical piece
in the development of benchmarks and metrics, both for model training and
quality assessment of production model endpoints. The current approaches to
response classification relies on methods that are too expensive (i.e.
LLM-as-a-Judge) or that are far from real-world conditions (string-matching,
logprob). In this paper, a structure-free evaluation method is presented. The
method makes use of semantic embedding distances to match target candidates
with arbitrary LM-generated text, resulting in a robust classification of the
response at a relatively low compute cost (embedding models of less than $10B$
parameters). The results show a regression score of ~0.97 and an accuracy of
~96% against human annotators, tested over 3 data sets and 3 different LM
architectures.

</details>


### [330] [The Disparate Impacts of Speculative Decoding](https://arxiv.org/abs/2510.02128)
*Jameson Sandler,Ahmet Üstün,Marco Romanelli,Sara Hooker,Ferdinando Fioretto*

Main category: cs.CL

TL;DR: 本文分析了投机解码在不同任务上的加速率差异，量化‘不公平性’，提出缓解策略并验证，平均公平性指标提升12%。


<details>
  <summary>Details</summary>
Motivation: 研究投机解码在不同任务上潜在的加速率差异。

Method: 推导分析量化‘不公平性’，提出缓解策略并在多个模型对上验证。

Result: 发现投机解码的加速效果在不同任务上分布不均，在欠拟合和代表性不足的任务上递减，所提策略使公平性指标平均提升12%。

Conclusion: 所提缓解策略能有效减少投机解码在不同任务上的加速差异。

Abstract: The practice of speculative decoding, whereby inference is probabilistically
supported by a smaller, cheaper, ``drafter'' model, has become a standard
technique for systematically reducing the decoding time of large language
models. This paper conducts an analysis of speculative decoding through the
lens of its potential disparate speed-up rates across tasks. Crucially, the
paper shows that speed-up gained from speculative decoding is not uniformly
distributed across tasks, consistently diminishing for under-fit, and often
underrepresented tasks. To better understand this phenomenon, we derive an
analysis to quantify this observed ``unfairness'' and draw attention to the
factors that motivate such disparate speed-ups to emerge. Further, guided by
these insights, the paper proposes a mitigation strategy designed to reduce
speed-up disparities and validates the approach across several model pairs,
revealing on average a 12% improvement in our fairness metric.

</details>


### [331] [Learning to Reason for Hallucination Span Detection](https://arxiv.org/abs/2510.02173)
*Hsuan Su,Ting-Yao Hu,Hema Swetha Koppula,Kundan Krishna,Hadi Pouransari,Cheng-Yu Hsieh,Cem Koc,Joseph Yitan Cheng,Oncel Tuzel,Raviteja Vemulapalli*

Main category: cs.CL

TL;DR: 本文探讨显式推理能否助力检测幻觉文本片段，提出RL4HS框架，实验表明其优于预训练推理模型和监督微调。


<details>
  <summary>Details</summary>
Motivation: 大多数现有工作将幻觉检测视为二分类任务，而现实应用需识别幻觉文本片段，因此需探究显式推理是否有助于该复杂任务。

Method: 先评估有无思维链推理的预训练模型，提出基于Group Relative Policy Optimization并引入Class - Aware Policy Optimization的强化学习框架RL4HS。

Result: 在RAGTruth基准测试（摘要、问答、数据到文本）中，RL4HS超越了预训练推理模型和监督微调。

Conclusion: 对于检测幻觉文本片段，使用跨度级奖励的强化学习是必要的。

Abstract: Large language models (LLMs) often generate hallucinations -- unsupported
content that undermines reliability. While most prior works frame hallucination
detection as a binary task, many real-world applications require identifying
hallucinated spans, which is a multi-step decision making process. This
naturally raises the question of whether explicit reasoning can help the
complex task of detecting hallucination spans. To answer this question, we
first evaluate pretrained models with and without Chain-of-Thought (CoT)
reasoning, and show that CoT reasoning has the potential to generate at least
one correct answer when sampled multiple times. Motivated by this, we propose
RL4HS, a reinforcement learning framework that incentivizes reasoning with a
span-level reward function. RL4HS builds on Group Relative Policy Optimization
and introduces Class-Aware Policy Optimization to mitigate reward imbalance
issue. Experiments on the RAGTruth benchmark (summarization, question
answering, data-to-text) show that RL4HS surpasses pretrained reasoning models
and supervised fine-tuning, demonstrating the necessity of reinforcement
learning with span-level rewards for detecting hallucination spans.

</details>


### [332] [ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge Graph Exploration Utilities](https://arxiv.org/abs/2510.02200)
*Felix Brei,Lorenz Bühmann,Johannes Frey,Daniel Gerber,Lars-Peter Meyer,Claus Stadler,Kirill Bulert*

Main category: cs.CL

TL;DR: 本文介绍基于SPINACH的通用方法，将自然语言问题迭代式转换为SPARQL查询，并分析代理行为以明确改进方向。


<details>
  <summary>Details</summary>
Motivation: SPARQL查询语言入门门槛高，Text2SPARQL挑战赛推动该领域改进。

Method: 引入基于SPINACH的通用方法，将自然语言问题迭代式转换为SPARQL查询，描述架构和设计思路。

Result: 文中未提及具体结果。

Conclusion: 文中未提及明确结论。

Abstract: Interacting with knowledge graphs can be a daunting task for people without a
background in computer science since the query language that is used (SPARQL)
has a high barrier of entry. Large language models (LLMs) can lower that
barrier by providing support in the form of Text2SPARQL translation. In this
paper we introduce a generalized method based on SPINACH, an LLM backed agent
that translates natural language questions to SPARQL queries not in a single
shot, but as an iterative process of exploration and execution. We describe the
overall architecture and reasoning behind our design decisions, and also
conduct a thorough analysis of the agent behavior to gain insights into future
areas for targeted improvements. This work was motivated by the Text2SPARQL
challenge, a challenge that was held to facilitate improvements in the
Text2SPARQL domain.

</details>


### [333] [More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration](https://arxiv.org/abs/2510.02227)
*Xiaoyang Yuan,Yujuan Ding,Yi Bin,Wenqi Shao,Jinyu Cai,Jingkuan Song,Yang Yang,Hengtao Shen*

Main category: cs.CL

TL;DR: 提出AMPO框架用于强化学习，在推理任务上表现出色，更高效可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在引导大语言模型长链推理时存在引入偏差、限制探索等问题，影响推理多样性和性能。

Method: 引入Adaptive Multi-Guidance Policy Optimization (AMPO)框架，采用按需指导方式，结合基于理解的选择机制。

Result: AMPO大幅超越基线GRPO，数学推理任务提升4.3%，分布外任务提升12.2%，显著提高Pass@k性能和探索多样性，使用四个同规模教师模型可取得与使用单个更强大教师模型相当的结果。

Conclusion: AMPO为实现卓越推理和泛化能力提供了更高效、可扩展的途径。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm
for enhancing the reasoning ability in Large Language Models (LLMs). However,
prevailing methods primarily rely on self-exploration or a single off-policy
teacher to elicit long chain-of-thought (LongCoT) reasoning, which may
introduce intrinsic model biases and restrict exploration, ultimately limiting
reasoning diversity and performance. Drawing inspiration from multi-teacher
strategies in knowledge distillation, we introduce Adaptive Multi-Guidance
Policy Optimization (AMPO), a novel framework that adaptively leverages
guidance from multiple proficient teacher models, but only when the on-policy
model fails to generate correct solutions. This "guidance-on-demand" approach
expands exploration while preserving the value of self-discovery. Moreover,
AMPO incorporates a comprehension-based selection mechanism, prompting the
student to learn from the reasoning paths that it is most likely to comprehend,
thus balancing broad exploration with effective exploitation. Extensive
experiments show AMPO substantially outperforms a strong baseline (GRPO), with
a 4.3% improvement on mathematical reasoning tasks and 12.2% on
out-of-distribution tasks, while significantly boosting Pass@k performance and
enabling more diverse exploration. Notably, using four peer-sized teachers, our
method achieves comparable results to approaches that leverage a single, more
powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate
a more efficient and scalable path to superior reasoning and generalizability.
Our code is available at https://github.com/SII-Enigma/AMPO.

</details>


### [334] [Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative Entropy Regulation](https://arxiv.org/abs/2510.02249)
*Tianyi Jiang,Yi Bin,Yujuan Ding,Kainian Zhu,Fei Ma,Jingkuan Song,Heng Tao Shen*

Main category: cs.CL

TL;DR: 提出TECA指标和“Explore Briefly, Then Decide”推理范式及CER机制，可有效缓解大语言模型推理时的过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在简单问题推理时存在过度思考问题，影响效率且难以适配问题复杂度。

Method: 引入TECA指标衡量推理探索程度，提出“Explore Briefly, Then Decide”推理范式及CER机制以动态确定推理结束点。

Result: 在多种数学基准测试中，该方法能大幅缓解过度思考问题，在简单数据集上平均响应长度最多减少71%。

Conclusion: 所提方法能创建更高效、自适应的推理过程。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities
on complex problems using long Chain-of-Thought (CoT) reasoning. However, they
often suffer from overthinking, meaning generating unnecessarily lengthy
reasoning steps for simpler problems. This issue may degrade the efficiency of
the models and make them difficult to adapt the reasoning depth to the
complexity of problems. To address this, we introduce a novel metric Token
Entropy Cumulative Average (TECA), which measures the extent of exploration
throughout the reasoning process. We further propose a novel reasoning paradigm
-- Explore Briefly, Then Decide -- with an associated Cumulative Entropy
Regulation (CER) mechanism. This paradigm leverages TECA to help the model
dynamically determine the optimal point to conclude its thought process and
provide a final answer, thus achieving efficient reasoning. Experimental
results across diverse mathematical benchmarks show that our approach
substantially mitigates overthinking without sacrificing problem-solving
ability. With our thinking paradigm, the average response length decreases by
up to 71% on simpler datasets, demonstrating the effectiveness of our method in
creating a more efficient and adaptive reasoning process.

</details>


### [335] [InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in Tool-Augmented Agents](https://arxiv.org/abs/2510.02271)
*Yaxin Du,Yuanshuo Zhang,Xiyuan Yang,Yifan Zhou,Cheng Wang,Gongyi Zou,Xianghe Pang,Wenhao Wang,Menglan Chen,Shuo Tang,Zhiyu Li,Siheng Chen*

Main category: cs.CL

TL;DR: 本文引入InfoMosaic - Bench基准测试工具增强代理的多源信息搜索能力，实验揭示现有LLM代理在信息搜索方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理依赖开放网络搜索有缺陷，虽有MCP但不清楚能否有效利用工具解决复杂任务。

Method: 引入InfoMosaic - Bench，涵盖六个领域，用InfoMosaic - Flow合成任务。

Result: 网络信息不足，GPT - 5准确率和通过率低；领域工具效益不一；22.4%失败源于工具使用或选择错误。

Conclusion: 当前LLMs在基本工具处理上仍有困难。

Abstract: Information seeking is a fundamental requirement for humans. However,
existing LLM agents rely heavily on open-web search, which exposes two
fundamental weaknesses: online content is noisy and unreliable, and many
real-world tasks require precise, domain-specific knowledge unavailable from
the web. The emergence of the Model Context Protocol (MCP) now allows agents to
interface with thousands of specialized tools, seemingly resolving this
limitation. Yet it remains unclear whether agents can effectively leverage such
tools -- and more importantly, whether they can integrate them with
general-purpose search to solve complex tasks. Therefore, we introduce
InfoMosaic-Bench, the first benchmark dedicated to multi-source information
seeking in tool-augmented agents. Covering six representative domains
(medicine, finance, maps, video, web, and multi-domain integration),
InfoMosaic-Bench requires agents to combine general-purpose search with
domain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable
pipeline that grounds task conditions in verified tool outputs, enforces
cross-source dependencies, and filters out shortcut cases solvable by trivial
lookup. This design guarantees both reliability and non-triviality. Experiments
with 14 state-of-the-art LLM agents reveal three findings: (i) web information
alone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass
rate; (ii) domain tools provide selective but inconsistent benefits, improving
some domains while degrading others; and (iii) 22.4% of failures arise from
incorrect tool usage or selection, highlighting that current LLMs still
struggle with even basic tool handling.

</details>


### [336] [Parallel Scaling Law: Unveiling Reasoning Generalization through A Cross-Linguistic Perspective](https://arxiv.org/abs/2510.02272)
*Wen Yang,Junhong Wu,Chong Li,Chengqing Zong,Jiajun Zhang*

Main category: cs.CL

TL;DR: 本文从跨语言视角研究基于强化学习的推理泛化能力，评估英文大推理模型在多语言推理基准上的表现，发现跨语言可迁移性受多种因素影响，还通过实验得出三个关键发现，挑战了大推理模型推理反映人类认知的假设。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注基于强化学习的推理在任务或模态上的泛化，本文提出从跨语言视角研究推理泛化，探讨英文强化后训练获得的推理能力能否有效迁移到其他语言。

Method: 系统评估以英语为中心的大推理模型在多语言推理基准上的表现，引入指标量化跨语言可迁移性，进行干预研究和并行训练研究。

Result: 跨语言可迁移性因初始模型、目标语言和训练范式而异；有较强初始英语能力的模型过度依赖英语特定模式，导致跨语言泛化能力下降；实验得出三个关键发现，即单语言到单平行语言过渡时性能大幅提升、跨语言推理迁移遵循幂律、存在单语言泛化差距。

Conclusion: 挑战了大推理模型推理反映人类认知的假设，为开发更语言无关的大推理模型提供关键见解。

Abstract: Recent advancements in Reinforcement Post-Training (RPT) have significantly
enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased
interest in the generalization of RL-based reasoning. While existing work has
primarily focused on investigating its generalization across tasks or
modalities, this study proposes a novel cross-linguistic perspective to
investigate reasoning generalization. This raises a crucial question:
$\textit{Does the reasoning capability achieved from English RPT effectively
transfer to other languages?}$ We address this by systematically evaluating
English-centric LRMs on multilingual reasoning benchmarks and introducing a
metric to quantify cross-lingual transferability. Our findings reveal that
cross-lingual transferability varies significantly across initial model, target
language, and training paradigm. Through interventional studies, we find that
models with stronger initial English capabilities tend to over-rely on
English-specific patterns, leading to diminished cross-lingual generalization.
To address this, we conduct a thorough parallel training study. Experimental
results yield three key findings: $\textbf{First-Parallel Leap}$, a substantial
leap in performance when transitioning from monolingual to just a single
parallel language, and a predictable $\textbf{Parallel Scaling Law}$, revealing
that cross-lingual reasoning transfer follows a power-law with the number of
training parallel languages. Moreover, we identify the discrepancy between
actual monolingual performance and the power-law prediction as
$\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMs
fail to fully generalize across languages. Our study challenges the assumption
that LRM reasoning mirrors human cognition, providing critical insights for the
development of more language-agnostic LRMs.

</details>


### [337] [F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data](https://arxiv.org/abs/2510.02294)
*Ziyin Zhang,Zihan Liao,Hang Yu,Peng Di,Rui Wang*

Main category: cs.CL

TL;DR: 介绍F2LLM系列嵌入模型，有三种大小，训练成本低，性能好，还发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 构建训练成本、模型大小和嵌入性能平衡的嵌入模型，为该领域研究提供基线。

Method: 从基础模型在600万开源非合成数据集的查询 - 文档 - 负样本三元组上直接微调。

Result: F2LLM - 4B在约40亿参数模型中排第2，总体排第7；F2LLM - 1.7B在10 - 20亿规模模型中排第1。

Conclusion: F2LLM是强大、可复现且经济的未来研究基线。

Abstract: We introduce F2LLM - Foundation to Feature Large Language Models, a suite of
state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike
previous top-ranking embedding models that require massive contrastive
pretraining, sophisticated training pipelines, and costly synthetic training
data, F2LLM is directly finetuned from foundation models on 6 million
query-document-negative tuples curated from open-source, non-synthetic
datasets, striking a strong balance between training cost, model size, and
embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd
among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B
ranks 1st among models in the 1B-2B size range. To facilitate future research
in the field, we release the models, training dataset, and code, positioning
F2LLM as a strong, reproducible, and budget-friendly baseline for future works.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [338] [Does Adoption of Zero Tillage Reduce Crop Residue Burning? Evidence from Satellite Remote Sensing and Household Survey Data in India](https://arxiv.org/abs/2510.01351)
*Dominik Naeher,Virginia Ziulu*

Main category: econ.GN

TL;DR: 研究结合卫星图像与家庭调查数据，发现免耕与作物秸秆焚烧呈负相关，还提供了地理空间数据整合方法。


<details>
  <summary>Details</summary>
Motivation: 以往关于免耕采用与秸秆焚烧联系的实证证据稀缺，增加政策制定难度，本研究旨在填补这一空白。

Method: 整合高分辨率卫星图像和印度家庭调查数据，比较从遥感数据构建焚烧指标的不同方法，并评估其与基于调查的测量方法的预测能力。

Result: 免耕与作物秸秆焚烧呈强烈负相关，调查数据和卫星衍生指标显示焚烧发生率降低50%或更多。

Conclusion: 研究提供了最佳地理空间数据整合方法的见解，为未来研究提供信息，支持基于证据的可持续农业政策干预。

Abstract: Previous research indicates that zero tillage technology offers a profitable
alternative to crop residue burning, with significant potential to reduce
agricultural emissions and contribute to improvements in air quality and public
health. Yet, empirical evidence on the link between zero tillage adoption and
residue burning remains scarce, adding to the difficulties policy makers face
in this context. This study addresses this gap by integrating high-resolution
satellite imagery with household survey data from India to examine the
empirical relationship between zero tillage and residue burning. We compare
different methods for constructing burn indicators from remote-sensing data and
assess their predictive power against survey-based measures. Our findings
reveal a robust negative association between zero tillage and crop residue
burning, with reductions in the incidence of burning of 50% or more across both
survey data and satellite-derived indicators. By providing insights into
optimal geospatial data integration methods, our study also makes a
methodological contribution that can inform future research and support
evidence-based policy interventions for more sustainable agricultural
practices.

</details>


### [339] [Equity Market Price Changes Are Predictable: A Natural Science Approach](https://arxiv.org/abs/2510.01542)
*Qingyuan Han*

Main category: econ.GN

TL;DR: 本文引入基于自然科学的扩展萨缪尔森模型（ESM）挑战股市不可预测观点，该模型能捕捉市场行为因果过程，有实用交易策略，可带来盈利机会。


<details>
  <summary>Details</summary>
Motivation: 挑战股市不可预测，即日内价格波动为随机噪声的传统观点，深入理解市场行为。

Method: 引入扩展萨缪尔森模型（ESM），识别多时间尺度上的峰值、谷值和转折点。

Result: ESM可可靠预测日内短期反转和长期趋势，其市场状态和信号为交易提供指引，平静期能捕捉标普500指数10点波动。

Conclusion: ESM不仅增进对市场演变的科学理解，还为盈利交易提供可靠且可行的路线图。

Abstract: Equity markets have long been regarded as unpredictable, with intraday price
movements treated as stochastic noise. This study challenges that view by
introducing the Extended Samuelson Model (ESM), a natural science-based
framework that captures the dynamic, causal processes underlying market
behavior. ESM identifies peaks, troughs, and turning points across multiple
timescales and demonstrates temporal compatibility: finer timeframes contain
all signals of broader ones while offering sharper directional guidance. Beyond
theory, ESM translates into practical trading strategies. During intraday
sessions, it reliably anticipates short-term reversals and longer-term trends,
even under the influence of breaking news. Its eight market states and six
directional signals provide actionable guardrails for traders, enabling
consistent profit opportunities. Notably, even during calm periods, ESM can
capture 10-point swings in the S&P 500, equivalent to $500 per E-mini futures
contract. These findings resonate with the state-based approaches attributed to
Renaissance Technologies' Medallion Fund, which delivered extraordinary returns
through systematic intraday trading. By bridging normal conditions with crisis
dynamics, ESM not only advances the scientific understanding of market
evolution but also provides a robust, actionable roadmap for profitable
trading.

</details>


### [340] [The centripetal pull of climate: Evidence from European Parliament elections (1989-2019)](https://arxiv.org/abs/2510.01551)
*Marco Dueñas,Hector Galindo-Silva,Antoine Mandel*

Main category: econ.GN

TL;DR: 研究温度冲击对欧洲议会选举的影响，发现温度冲击减少意识形态极化、增加选票集中，使政党体系向中间偏移。


<details>
  <summary>Details</summary>
Motivation: 探究温度冲击对欧洲议会选举的影响。

Method: 结合1989 - 2019年高分辨率气候数据和NUTS - 2地区议会选举结果，利用选举前异常温暖和炎热天数的外生变化分析短期温度冲击对投票行为的影响。

Result: 温度冲击减少意识形态极化、增加选票集中，选民向大型温和政党靠拢，自由党和社民党支持率上升，右翼政党支持率下降，选举前气候变暖时政党宣言更强调气候问题。

Conclusion: 气候冲击可使政党体系向中间偏移，削弱政治极端势力。

Abstract: This paper examines the impact of temperature shocks on European Parliament
elections. We combine high-resolution climate data with results from
parliamentary elections between 1989 and 2019, aggregated at the NUTS-2
regional level. Exploiting exogenous variation in unusually warm and hot days
during the months preceding elections, we identify the effect of short-run
temperature shocks on voting behaviour. We find that temperature shocks reduce
ideological polarisation and increase vote concentration, as voters consolidate
around larger, more moderate parties. This aggregated pattern is explained by a
gain in support of liberal and, to a lesser extent, social democratic parties,
while right-wing parties lose vote share. Consistent with a salience mechanism,
complementary analysis of party manifestos shows greater emphasis on
climate-related issues in warmer pre-electoral contexts. Overall, our findings
indicate that climate shocks can shift party systems toward the centre and
weaken political extremes.

</details>


### [341] [A Computational Approach to Sustainable Policies Evaluation of the Italian Wheat Production System](https://arxiv.org/abs/2510.02154)
*Gianfranco Giuloni,Edmondo Di Giuseppe,Arianna Di Paola*

Main category: econ.GN

TL;DR: 本文介绍开发支持政策制定者引导小麦可持续生产政策工具的建模步骤，构建基于代理的模型并与其他组件集成。


<details>
  <summary>Details</summary>
Motivation: 农业政策影响的农场具有高度异质性，需要工具支持政策制定者引导小麦可持续生产。

Method: 构建基于代理的模型（ABM），用意大利农场代表性调查初始化模型，将其规模扩大到全国范围，再与国际小麦市场全球模型和环境影响评估工具集成。

Result: 构建了集成框架，可在评估政策措施环境影响时考虑全球价格与本地生产的反馈循环。

Conclusion: 该集成框架能为政策制定者提供支持，引导政策走向更可持续的小麦生产。

Abstract: This work outlines the modeling steps for developing a tool aimed at
supporting policymakers in guiding policies toward more sustainable wheat
production. In the agricultural sector,policies affect a highly diverse set of
farms, which differ across several dimensions such as size,land composition,
local climate, and irrigation availability. To address this significant
heterogeneity, we construct an Agent-Based Model (ABM). The model is
initialized using a representative survey of Italian farms, which captures
their heterogeneity. The ABM is then scaled to include a number of farms
comparable to those operating nationwide. To capture broader dynamics, the ABM
is integrated with two additional components:a global model of international
wheat markets and a tool for assessing the environmental impacts of wheat
production. This integrated framework enables us to account for the feedback
loop between global prices and local production while evaluating the
environmental implications of policy measures.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [342] [KTBox: A Modular LaTeX Framework for Semantic Color, Structured Highlighting, and Scholarly Communication](https://arxiv.org/abs/2510.01961)
*Bhaskar Mangal,Ashutosh Bhatia,Yashvardhan Sharma,Kamlesh Tiwari,Rashmi Verma*

Main category: cs.DL

TL;DR: 本文介绍LaTeX框架ktbox，统一多种工具用于学术写作，各组件独立且兼容主流模板，提升科学交流的清晰度、便携性和创作效率。


<details>
  <summary>Details</summary>
Motivation: 解决科学手稿技术见解交流中临时格式选择导致的视觉强调不一致和跨文档类便携性有限的问题。

Method: 引入模块化LaTeX框架ktbox，将语义调色板、结构化高亮框、分类树和作者元数据工具统一为一个连贯系统，各组件独立且可互操作。

Result: 框架包含多个独立又兼容主流模板的组件，具有自动编号、宽格式高亮等关键特性。

Conclusion: 该框架将视觉样式转变为可重复、可扩展的构建块，提高了清晰度、便携性和创作效率。

Abstract: The communication of technical insight in scientific manuscripts often relies
on ad-hoc formatting choices, resulting in inconsistent visual emphasis and
limited portability across document classes. This paper introduces ktbox, a
modular LaTeX framework that unifies semantic color palettes, structured
highlight boxes, taxonomy trees, and author metadata utilities into a coherent
system for scholarly writing. The framework is distributed as a set of
lightweight, namespaced components: ktcolor.sty for semantic palettes,
ktbox.sty for structured highlight and takeaway environments, ktlrtree.sty for
taxonomy trees with fusion and auxiliary annotations, and ktorcid.sty for
ORCID-linked author metadata. Each component is independently usable yet
interoperable, ensuring compatibility with major templates such as IEEEtran,
acmart, iclr conference, and beamer. Key features include auto-numbered
takeaway boxes, wide-format highlights, flexible taxonomy tree visualizations,
and multi-column layouts supporting embedded tables, enumerations, and code
blocks. By adopting a clear separation of concerns and enforcing a consistent
naming convention under the kt namespace, the framework transforms visual
styling from cosmetic add-ons into reproducible, extensible building blocks of
scientific communication, improving clarity, portability, and authoring
efficiency across articles, posters, and presentations.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [343] [A Modular Theory of Subjective Consciousness for Natural and Artificial Minds](https://arxiv.org/abs/2510.01864)
*Michaël Gillon*

Main category: q-bio.NC

TL;DR: 论文介绍模块化意识理论（MCT），提出意识是离散的集成信息状态序列，有计算流程，能产生可测试预测，为生物和人工架构提供蓝图。


<details>
  <summary>Details</summary>
Motivation: 解决神经科学、认知科学和人工智能研究中理解主观体验如何从信息处理中产生的核心挑战。

Method: 提出MCT框架，阐述输入信息经模块处理集成到IIS，再传输到其他模块的过程，并与其他理论对比。

Result: 解释强标记状态对长期记忆和行动影响更大，产生如压力增强记忆编码等可测试预测。

Conclusion: 意识不是不可约的本质，而是复杂信息处理中可进化、量化和构建的特征。

Abstract: Understanding how subjective experience arises from information processing
remains a central challenge in neuroscience, cognitive science, and AI
research. The Modular Consciousness Theory (MCT) proposes a biologically
grounded and computationally explicit framework in which consciousness is a
discrete sequence of Integrated Informational States (IISs). Each IIS is a
packet of integrated information tagged with a multidimensional density vector
that quantifies informational richness. Its magnitude correlates with
subjective intensity, shaping memory, behavior, and continuity of experience.
Inputs from body and environment are adaptively filtered, processed by modules
(abstraction, narration, evaluation, self-evaluation), and integrated into an
IIS. The resulting packet, tagged with its density vector, is transmitted to
behavioral readiness, memory, and decision-making modules, closing the loop.
This explains why strongly tagged states exert greater influence on long-term
memory and action. Unlike Global Workspace Theory, Integrated Information
Theory, or Higher-Order Thought, MCT specifies a full computational pipeline
producing discrete informational units with quantifiable internal structure.
Subjectivity is reframed as a correlate of the density-tagging signal with
functional consequences. MCT generates testable predictions, such as stress
enhancing memory encoding, and provides a naturalistic blueprint for both
biological and artificial architectures. Consciousness, in this view, is not an
irreducible essence but an evolvable, quantifiable, and constructible feature
of complex information processing.

</details>


### [344] [Aligning Video Models with Human Social Judgments via Behavior-Guided Fine-Tuning](https://arxiv.org/abs/2510.01502)
*Kathy Garcia,Leyla Isik*

Main category: q-bio.NC

TL;DR: 研究现代视频和语言模型能否捕捉人类对社交视频的相似性感知，引入新基准发现模态差距，用新方法微调视频模型缩小差距，提升与人类感知的对齐度并加强社会情感属性编码。


<details>
  <summary>Details</summary>
Motivation: 探究当前先进AI模型是否编码了与人类相同的社交视频相似性结构，解决两个问题：现代模型能否捕捉人类感知的相似性以及如何利用人类行为数据将此结构融入模型。

Method: 引入包含超49000个判断的新基准，用新颖的混合三元组 - RSA目标和低秩自适应（LoRA）对TimeSformer视频模型进行微调。

Result: 微调后的视频模型在保留视频上与人类感知的对齐度显著提高，增加了与语言嵌入的共享方差并解释了语言模型未捕捉的独特方差，加强了社会情感属性编码。

Conclusion: 预训练视频模型在社会识别方面存在差距，行为引导的微调可使视频表征向人类社会感知方向发展。

Abstract: Humans intuitively perceive complex social signals in visual scenes, yet it
remains unclear whether state-of-the-art AI models encode the same similarity
structure. We study (Q1) whether modern video and language models capture
human-perceived similarity in social videos, and (Q2) how to instill this
structure into models using human behavioral data. To address this, we
introduce a new benchmark of over 49,000 odd-one-out similarity judgments on
250 three-second video clips of social interactions, and discover a modality
gap: despite the task being visual, caption-based language embeddings align
better with human similarity than any pretrained video model. We close this gap
by fine-tuning a TimeSformer video model on these human judgments with our
novel hybrid triplet-RSA objective using low-rank adaptation (LoRA), aligning
pairwise distances to human similarity. This fine-tuning protocol yields
significantly improved alignment with human perceptions on held-out videos in
terms of both explained variance and odd-one-out triplet accuracy. Variance
partitioning shows that the fine-tuned video model increases shared variance
with language embeddings and explains additional unique variance not captured
by the language model. Finally, we test transfer via linear probes and find
that human-similarity fine-tuning strengthens the encoding of social-affective
attributes (intimacy, valence, dominance, communication) relative to the
pretrained baseline. Overall, our findings highlight a gap in pretrained video
models' social recognition and demonstrate that behavior-guided fine-tuning
shapes video representations toward human social perception.

</details>


### [345] [Uncovering Semantic Selectivity of Latent Groups in Higher Visual Cortex with Mutual Information-Guided Diffusion](https://arxiv.org/abs/2510.02182)
*Yule Wang,Joseph Yu,Chengrui Li,Weihan Li,Anqi Wu*

Main category: q-bio.NC

TL;DR: 提出MIG - Vis方法可视化和验证神经潜在子空间中的视觉语义属性，在猕猴数据集验证，为高级视觉皮层结构化语义表征提供证据。


<details>
  <summary>Details</summary>
Motivation: 先前研究对高级视觉区域神经群体结构和组织理解有限，需解决特征特异性视觉信息分布及是否有结构化语义子空间的问题。

Method: 先用变分自编码器从神经群体推断分组解缠的神经潜在子空间，再用互信息引导的扩散合成程序可视化每个潜在组编码的视觉语义特征。

Result: 在猕猴下颞叶皮层多会话神经放电数据集上验证，该方法能识别对多种视觉特征有清晰语义选择性的神经潜在组。

Conclusion: 为高级视觉皮层结构化语义表征提供直接、可解释的证据，推进对其编码原则的理解。

Abstract: Understanding how neural populations in higher visual areas encode
object-centered visual information remains a central challenge in computational
neuroscience. Prior works have investigated representational alignment between
artificial neural networks and the visual cortex. Nevertheless, these findings
are indirect and offer limited insights to the structure of neural populations
themselves. Similarly, decoding-based methods have quantified semantic features
from neural populations but have not uncovered their underlying organizations.
This leaves open a scientific question: "how feature-specific visual
information is distributed across neural populations in higher visual areas,
and whether it is organized into structured, semantically meaningful
subspaces." To tackle this problem, we present MIG-Vis, a method that leverages
the generative power of diffusion models to visualize and validate the
visual-semantic attributes encoded in neural latent subspaces. Our method first
uses a variational autoencoder to infer a group-wise disentangled neural latent
subspace from neural populations. Subsequently, we propose a mutual information
(MI)-guided diffusion synthesis procedure to visualize the specific
visual-semantic features encoded by each latent group. We validate MIG-Vis on
multi-session neural spiking datasets from the inferior temporal (IT) cortex of
two macaques. The synthesized results demonstrate that our method identifies
neural latent groups with clear semantic selectivity to diverse visual
features, including object pose, inter-category transformations, and
intra-class content. These findings provide direct, interpretable evidence of
structured semantic representation in the higher visual cortex and advance our
understanding of its encoding principles.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [346] [INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models](https://arxiv.org/abs/2510.01389)
*Ulas Berk Karli,Ziyao Shangguan,Tesca FItzgerald*

Main category: cs.RO

TL;DR: 提出INSIGHT框架利用token级不确定性信号预测VLA何时请求帮助，对比强弱监督机制，发现transformer建模优于静态分数。


<details>
  <summary>Details</summary>
Motivation: 现有Vision - Language - Action (VLA)模型缺乏内省机制以预测失败并向人类寻求帮助。

Method: 以π_0 - FAST为基础模型，提取token级熵、对数概率和基于Dirichlet的不确定性估计，训练紧凑型transformer分类器，探索强弱监督机制。

Result: 强标签能捕捉细粒度不确定性动态，弱标签在训练和评估一致时也支持内省；transformer建模比静态序列分数有更强预测能力。

Conclusion: 该研究首次系统评估VLA中基于不确定性的内省，为主动学习和实时错误缓解开辟了新途径。

Abstract: Recent Vision-Language-Action (VLA) models show strong generalization
capabilities, yet they lack introspective mechanisms for anticipating failures
and requesting help from a human supervisor. We present \textbf{INSIGHT}, a
learning framework for leveraging token-level uncertainty signals to predict
when a VLA should request help. Using $\pi_0$-FAST as the underlying model, we
extract per-token \emph{entropy}, \emph{log-probability}, and Dirichlet-based
estimates of \emph{aleatoric and epistemic uncertainty}, and train compact
transformer classifiers to map these sequences to help triggers. We explore
supervision regimes for strong or weak supervision, and extensively compare
them across in-distribution and out-of-distribution tasks. Our results show a
trade-off: strong labels enable models to capture fine-grained uncertainty
dynamics for reliable help detection, while weak labels, though noisier, still
support competitive introspection when training and evaluation are aligned,
offering a scalable path when dense annotation is impractical. Crucially, we
find that modeling the temporal evolution of token-level uncertainty signals
with transformers provides far greater predictive power than static
sequence-level scores. This study provides the first systematic evaluation of
uncertainty-based introspection in VLAs, opening future avenues for active
learning and for real-time error mitigation through selective human
intervention.

</details>


### [347] [AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation](https://arxiv.org/abs/2510.01433)
*Anukriti Singh,Kasra Torshizi,Khuzema Habib,Kelin Yu,Ruohan Gao,Pratap Tokekar*

Main category: cs.RO

TL;DR: 提出AFFORD2ACT框架，从文本提示和单张图像中提取最小语义2D关键点集，在多样操作任务中提升数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于关键点的视觉机器人学习方法存在依赖手动启发式或任务耦合选择，限制可扩展性和语义理解的问题。

Method: 采用三阶段流水线，包括可操作性过滤、类别级关键点构建和基于变压器的策略学习及嵌入式门控。

Result: 得到38维状态策略，可在15分钟内训练，实时表现良好，在多样操作任务中成功率达82%。

Conclusion: AFFORD2ACT框架能有效提升视觉机器人学习的数据效率。

Abstract: Vision-based robot learning often relies on dense image or point-cloud
inputs, which are computationally heavy and entangle irrelevant background
features. Existing keypoint-based approaches can focus on manipulation-centric
features and be lightweight, but either depend on manual heuristics or
task-coupled selection, limiting scalability and semantic understanding. To
address this, we propose AFFORD2ACT, an affordance-guided framework that
distills a minimal set of semantic 2D keypoints from a text prompt and a single
image. AFFORD2ACT follows a three-stage pipeline: affordance filtering,
category-level keypoint construction, and transformer-based policy learning
with embedded gating to reason about the most relevant keypoints, yielding a
compact 38-dimensional state policy that can be trained in 15 minutes, which
performs well in real-time without proprioception or dense representations.
Across diverse real-world manipulation tasks, AFFORD2ACT consistently improves
data efficiency, achieving an 82% success rate on unseen objects, novel
categories, backgrounds, and distractors.

</details>


### [348] [VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs](https://arxiv.org/abs/2510.01483)
*Mohamad Al Mdfaa,Svetlana Lukina,Timur Akhtyamov,Arthur Nigmatzyanov,Dmitrii Nalberskii,Sergey Zagoruyko,Gonzalo Ferrer*

Main category: cs.RO

TL;DR: 提出VL - KnG系统解决视觉语言模型用于机器人导航的局限，引入WalkieKnowledge基准，在真实机器人上部署有良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型用于机器人导航存在缺乏持久场景记忆、空间推理有限、难以适应视频时长等局限。

Method: 使用时空知识图构建和高效查询处理，分块处理视频序列，创建持久知识图，通过可查询图结构实现可解释空间推理；引入新基准WalkieKnowledge。

Result: 在真实差速驱动机器人上部署，成功率达77.27%，答案准确率达76.92%，性能与Gemini 2.5 Pro相当。

Conclusion: VL - KnG系统具有可解释推理，计算高效，适用于定位、导航和规划等实时任务。

Abstract: Vision-language models (VLMs) have shown potential for robot navigation but
encounter fundamental limitations: they lack persistent scene memory, offer
limited spatial reasoning, and do not scale effectively with video duration for
real-time application. We present VL-KnG, a Visual Scene Understanding system
that tackles these challenges using spatiotemporal knowledge graph construction
and computationally efficient query processing for navigation goal
identification. Our approach processes video sequences in chunks utilizing
modern VLMs, creates persistent knowledge graphs that maintain object identity
over time, and enables explainable spatial reasoning through queryable graph
structures. We also introduce WalkieKnowledge, a new benchmark with about 200
manually annotated questions across 8 diverse trajectories spanning
approximately 100 minutes of video data, enabling fair comparison between
structured approaches and general-purpose VLMs. Real-world deployment on a
differential drive robot demonstrates practical applicability, with our method
achieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5
Pro performance while providing explainable reasoning supported by the
knowledge graph, computational efficiency for real-time deployment across
different tasks, such as localization, navigation and planning. Code and
dataset will be released after acceptance.

</details>


### [349] [PolySim: Bridging the Sim-to-Real Gap for Humanoid Control via Multi-Simulator Dynamics Randomization](https://arxiv.org/abs/2510.01708)
*Zixing Lei,Zibo Zhou,Sheng Yin,Yueru Chen,Qingyao Xu,Weixin Li,Yunhong Wang,Bowei Tang,Wei Jing,Siheng Chen*

Main category: cs.RO

TL;DR: 针对人形全身控制策略的仿真到现实差距问题，提出PolySim训练平台，可整合多模拟器，理论上能降低模拟器归纳偏差上界，实验表现良好并可零样本部署到真实机器人。


<details>
  <summary>Details</summary>
Motivation: 解决人形全身控制策略在仿真训练时存在的仿真到现实差距问题，该问题源于模拟器归纳偏差。

Method: 引入PolySim训练平台，整合多个异构模拟器，在单次训练中同时从不同引擎启动并行环境，实现动力学层面的域随机化。

Result: 理论上PolySim比单模拟器训练有更紧的模拟器归纳偏差上界；实验中大幅降低仿真到仿真评估的运动跟踪误差，如在MuJoCo上比IsaacSim基线执行成功率提高52.8；能零样本部署到真实Unitree G1。

Conclusion: PolySim可有效缓解模拟器归纳偏差问题，实现从仿真到现实的有效迁移。

Abstract: Humanoid whole-body control (WBC) policies trained in simulation often suffer
from the sim-to-real gap, which fundamentally arises from simulator inductive
bias, the inherent assumptions and limitations of any single simulator. These
biases lead to nontrivial discrepancies both across simulators and between
simulation and the real world. To mitigate the effect of simulator inductive
bias, the key idea is to train policies jointly across multiple simulators,
encouraging the learned controller to capture dynamics that generalize beyond
any single simulator's assumptions. We thus introduce PolySim, a WBC training
platform that integrates multiple heterogeneous simulators. PolySim can launch
parallel environments from different engines simultaneously within a single
training run, thereby realizing dynamics-level domain randomization.
Theoretically, we show that PolySim yields a tighter upper bound on simulator
inductive bias than single-simulator training. In experiments, PolySim
substantially reduces motion-tracking error in sim-to-sim evaluations; for
example, on MuJoCo, it improves execution success by 52.8 over an IsaacSim
baseline. PolySim further enables zero-shot deployment on a real Unitree G1
without additional fine-tuning, showing effective transfer from simulation to
the real world. We will release the PolySim code upon acceptance of this work.

</details>


### [350] [Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2510.01795)
*Haibo Hu,Lianming Huang,Xinyu Wang,Yufei Cui,Nan Guan,Chun Jason Xue*

Main category: cs.RO

TL;DR: 提出导航引导的早期退出框架Nav - EE，减少VLM在自动驾驶中的推理延迟，实验效果好。


<details>
  <summary>Details</summary>
Motivation: Vision - Language Models在自动驾驶中推理延迟高，早期退出方法泛化性受限，而自动驾驶导航系统可预测上下文。

Method: 提出Nav - EE框架，离线预计算特定任务退出层，在线根据导航先验动态应用。

Result: 在多个数据集上，Nav - EE在保持精度的同时，降低延迟最高达63.9%；实车集成中，推理延迟从600ms降至300ms。

Conclusion: 将导航预见与早期退出结合，为自动驾驶系统高效部署大模型提供可行途径。

Abstract: Vision-Language Models (VLMs) are increasingly applied in autonomous driving
for unified perception and reasoning, but high inference latency hinders
real-time deployment. Early-exit reduces latency by terminating inference at
intermediate layers, yet its task-dependent nature limits generalization across
diverse scenarios. We observe that this limitation aligns with autonomous
driving: navigation systems can anticipate upcoming contexts (e.g.,
intersections, traffic lights), indicating which tasks will be required. We
propose Nav-EE, a navigation-guided early-exit framework that precomputes
task-specific exit layers offline and dynamically applies them online based on
navigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE
achieves accuracy comparable to full inference while reducing latency by up to
63.9%. Real-vehicle integration with Autoware Universe further demonstrates
reduced inference latency (600ms to 300ms), supporting faster decision-making
in complex scenarios. These results suggest that coupling navigation foresight
with early-exit offers a viable path toward efficient deployment of large
models in autonomous systems. Code and data are available at our anonymous
repository: https://anonymous.4open.science/r/Nav-EE-BBC4

</details>


### [351] [TACOS: Task Agnostic COordinator of a multi-drone System](https://arxiv.org/abs/2510.01869)
*Alessandro Nazzari,Roberto Rubinacci,Marco Lovera*

Main category: cs.RO

TL;DR: 本文提出TACOS框架，通过大语言模型实现多无人机系统的自然语言控制，并在真实场景验证。


<details>
  <summary>Details</summary>
Motivation: 单飞行员管理多无人机系统需灵活交互框架，语言模型发展为其提供基础，可减轻飞行员负担。

Method: 提出TACOS框架，集成自然语言接口、智能协调器和自主代理三个关键能力，让大语言模型与可执行API库交互。

Result: 在真实多无人机系统中展示了该系统，并进行消融研究评估各模块贡献。

Conclusion: TACOS框架可实现多无人机系统的高级自然语言控制。

Abstract: When a single pilot is responsible for managing a multi-drone system, the
task demands varying levels of autonomy, from direct control of individual
UAVs, to group-level coordination, to fully autonomous swarm behaviors for
accomplishing high-level tasks. Enabling such flexible interaction requires a
framework that supports multiple modes of shared autonomy. As language models
continue to improve in reasoning and planning, they provide a natural
foundation for such systems, reducing pilot workload by enabling high-level
task delegation through intuitive, language-based interfaces. In this paper we
present TACOS (Task-Agnostic COordinator of a multi-drone System), a unified
framework that enables high-level natural language control of multi-UAV systems
through Large Language Models (LLMs). TACOS integrates three key capabilities
into a single architecture: a one-to-many natural language interface for
intuitive user interaction, an intelligent coordinator for translating user
intent into structured task plans, and an autonomous agent that executes plans
interacting with the real-world. TACOS allows a LLM to interact with a library
of executable APIs, bridging semantic reasoning with real-time multi-robot
coordination. We demonstrate the system in real-world multi-drone system and
conduct an ablation study to assess the contribution of each module.

</details>


### [352] [Contrastive Representation Regularization for Vision-Language-Action Models](https://arxiv.org/abs/2510.01711)
*Taeyoung Kim,Jimin Lee,Myungkyu Koo,Dongyoung Kim,Kyungmin Lee,Changyeon Kim,Younggyo Seo,Jinwoo Shin*

Main category: cs.RO

TL;DR: 引入Robot State-aware Contrastive Loss (RS - CL)提升VLA模型操作性能。


<details>
  <summary>Details</summary>
Motivation: 现有Vision - Language - Action (VLA)模型表征欠佳，对控制动作和本体感受状态等机器人信号不敏感。

Method: 引入RS - CL，利用状态间相对距离作为软监督，使表征更贴合机器人本体感受状态，补充原动作预测目标。

Result: RS - CL显著提升了VLA模型操作性能，在RoboCasa - Kitchen的拾取放置任务中成功率从30.8%提升到41.5%，在真实机器人操作任务中成功率从45.0%提升到58.3%。

Conclusion: RS - CL能有效增强与控制相关的表征学习，提升VLA模型操作性能。

Abstract: Vision-Language-Action (VLA) models have shown its capabilities in robot
manipulation by leveraging rich representations from pre-trained
Vision-Language Models (VLMs). However, their representations arguably remain
suboptimal, lacking sensitivity to robotic signals such as control actions and
proprioceptive states. To address the issue, we introduce Robot State-aware
Contrastive Loss (RS-CL), a simple and effective representation regularization
for VLA models, designed to bridge the gap between VLM representations and
robotic signals. In particular, RS-CL aligns the representations more closely
with the robot's proprioceptive states, by using relative distances between the
states as soft supervision. Complementing the original action prediction
objective, RS-CL effectively enhances control-relevant representation learning,
while being lightweight and fully compatible with standard VLA training
pipeline. Our empirical results demonstrate that RS-CL substantially improves
the manipulation performance of state-of-the-art VLA models; it pushes the
prior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen,
through more accurate positioning during grasping and placing, and boosts
success rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [353] [Comparative Field Deployment of Reinforcement Learning and Model Predictive Control for Residential HVAC](https://arxiv.org/abs/2510.01475)
*Ozan Baris Mulayim,Elias N. Pergantis,Levi D. Reyes Premer,Bingqing Chen,Guannan Qu,Kevin J. Kircher,Mario Bergés*

Main category: eess.SY

TL;DR: 对比MPC和基于模型的RL控制器在住宅HVAC系统中的应用，RL节能略超MPC，但舒适度略逊，MPC在节能舒适性上表现更佳，指出RL应用的权衡和研究方向。


<details>
  <summary>Details</summary>
Motivation: MPC工程成本高，RL在实际住宅应用待验证，存在安全、可解释性和样本效率问题，需研究其实际应用。

Method: 在印第安纳州西拉法叶的住宅中，将MPC和基于模型的RL控制器各部署一个月，与现有控制器对比评估。

Result: RL节能22%略超MPC的20%，但住户不适感略高；节能舒适性上MPC更优。

Conclusion: RL虽减少工程开销，但存在模型准确性和操作鲁棒性权衡，明确了RL应用难点和研究方向。

Abstract: Advanced control strategies like Model Predictive Control (MPC) offer
significant energy savings for HVAC systems but often require substantial
engineering effort, limiting scalability. Reinforcement Learning (RL) promises
greater automation and adaptability, yet its practical application in
real-world residential settings remains largely undemonstrated, facing
challenges related to safety, interpretability, and sample efficiency. To
investigate these practical issues, we performed a direct comparison of an MPC
and a model-based RL controller, with each controller deployed for a one-month
period in an occupied house with a heat pump system in West Lafayette, Indiana.
This investigation aimed to explore scalability of the chosen RL and MPC
implementations while ensuring safety and comparability. The advanced
controllers were evaluated against each other and against the existing
controller. RL achieved substantial energy savings (22\% relative to the
existing controller), slightly exceeding MPC's savings (20\%), albeit with
modestly higher occupant discomfort. However, when energy savings were
normalized for the level of comfort provided, MPC demonstrated superior
performance. This study's empirical results show that while RL reduces
engineering overhead, it introduces practical trade-offs in model accuracy and
operational robustness. The key lessons learned concern the difficulties of
safe controller initialization, navigating the mismatch between control actions
and their practical implementation, and maintaining the integrity of online
learning in a live environment. These insights pinpoint the essential research
directions needed to advance RL from a promising concept to a truly scalable
HVAC control solution.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [354] [Bifurcation: How to Explore a Tree](https://arxiv.org/abs/2510.01939)
*Sariel Har-Peled*

Main category: cs.CG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Avraham et al. [AFK+15] presented an alternative approach to parametric
search, called \emph{bifurcation}, that performs faster under certain
circumstances. Intuitively, when the underlying decider execution can be rolled
back cheaply and the decider has a near-linear running time. For some problems,
this leads to fast algorithms that beat the seemingly natural lower bound
arising from distance selection.
  Bifurcation boils down to a tree exploration problem. You are given a binary
(unfortunately implicit) tree of height $n$ and $k$ internal nodes with two
children (all other internal nodes have a single child), and assume each node
has an associated parameter value. These values are sorted in the inorder
traversal of the tree. Assume there is (say) a node (not necessarily a leaf)
that is the target node that the exploration needs to discover.
  The player starts from the root. At each step, the player can move to
adjacent nodes to the current location (i.e., one of the children or the
parent). Alternatively, the player can call an oracle on the current node,
which returns either that it is the target (thus, mission accomplished!) or
whether the target value is strictly smaller or larger than the current one.
  A naive algorithm explores the whole tree, in $O(n k)$ time, then performs
$O(\log k n)$ calls to the oracle to find the desired leaf. Avraham \etal
showed that this can be improved to $O(n \sqrt{k} )$ time, and $O( \sqrt{k}
\log n)$ oracle calls.
  Here, we improve this to $O(n \sqrt{k} )$ time, with only $ O( \sqrt{k} +
\log n)$ oracle calls. We also show matching lower bounds, under certain
assumptions. We believe our interpretation of bifurcation as a tree exploration
problem, and the associated algorithm, are of independent interest.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [355] [Multiplier-free In-Memory Vector-Matrix Multiplication Using Distributed Arithmetic](https://arxiv.org/abs/2510.02099)
*Felix Zeller,John Reuben,Dietmar Fey*

Main category: cs.AR

TL;DR: 本文将分布式算术（DA）技术扩展用于向量 - 矩阵乘法（VMM），通过晶体管级模拟验证功能和估计非功能特性，相比传统内存中VMM有显著性能提升并消除了对ADC的需求。


<details>
  <summary>Details</summary>
Motivation: 内存中VMM所需的ADC/DAC消耗大量功率和面积，需改进VMM计算方法。

Method: 将DA技术扩展用于输入向量与常量矩阵相乘，在ReRAM内存周边用移位 - 加法电路实现VMM，进行晶体管级模拟。

Result: 相比传统按位切片在内存中执行的VMM，实现了4.5倍的延迟降低和12倍的能耗降低，且消除了对高功耗ADC的需求。

Conclusion: 扩展的DA技术在VMM计算中有显著性能优势，可降低延迟、能耗和面积。

Abstract: Vector-Matrix Multiplication (VMM) is the fundamental and frequently required
computation in inference of Neural Networks (NN). Due to the large data
movement required during inference, VMM can benefit greatly from in-memory
computing. However, ADC/DACs required for in-memory VMM consume significant
power and area. `Distributed Arithmetic (DA)', a technique in computer
architecture prevalent in 1980s was used to achieve inner product or dot
product of two vectors without using a hard-wired multiplier when one of the
vectors is a constant. In this work, we extend the DA technique to multiply an
input vector with a constant matrix. By storing the sum of the weights in
memory, DA achieves VMM using shift-and-add circuits in the periphery of ReRAM
memory. We verify functional and also estimate non-functional properties
(latency, energy, area) by performing transistor-level simulations. Using
energy-efficient sensing and fine grained pipelining, our approach achieves 4.5
x less latency and 12 x less energy than VMM performed in memory conventionally
by bit slicing. Furthermore, DA completely eliminated the need for power-hungry
ADCs which are the main source of area and energy consumption in the current
VMM implementations in memory.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [356] [Financial Stability Implications of Generative AI: Taming the Animal Spirits](https://arxiv.org/abs/2510.01451)
*Anne Lundgaard Hansen,Seung Jung Lee*

Main category: q-fin.GN

TL;DR: 研究生成式AI对金融稳定的影响，实验表明AI交易决策更理性，但也会出现最优羊群行为且有人类的一些特征，对金融稳定有潜在影响。


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI的采用对金融稳定的影响。

Method: 使用大语言模型进行实验室风格实验，复制交易决策中羊群行为的经典研究。

Result: AI代理决策更理性，依赖私人信息而非市场趋势；可诱导产生最优羊群行为；有人类的一些特征。

Conclusion: 增加对AI交易建议的依赖可能减少资产价格泡沫，但AI的最优羊群行为和人类特征对金融稳定仍有潜在影响。

Abstract: This paper investigates the impact of the adoption of generative AI on
financial stability. We conduct laboratory-style experiments using large
language models to replicate classic studies on herd behavior in trading
decisions. Our results show that AI agents make more rational decisions than
humans, relying predominantly on private information over market trends.
Increased reliance on AI-powered trading advice could therefore potentially
lead to fewer asset price bubbles arising from animal spirits that trade by
following the herd. However, exploring variations in the experimental settings
reveals that AI agents can be induced to herd optimally when explicitly guided
to make profit-maximizing decisions. While optimal herding improves market
discipline, this behavior still carries potential implications for financial
stability. In other experimental variations, we show that AI agents are not
purely algorithmic, but have inherited some elements of human conditioning and
bias.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [357] [Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays](https://arxiv.org/abs/2510.01350)
*Muhammad Faheemur Rahman,Wayne Burleson*

Main category: cs.CR

TL;DR: 本文提出两种安全机制保护忆阻交叉阵列权重，模拟显示机制开销小且实验证明保护可行。


<details>
  <summary>Details</summary>
Motivation: 非易失性忆阻器存在安全威胁，保护存储的权重很重要。

Method: 提出Keyed Permutor和Watermark Protection Columns两种安全机制，与现有忆阻交叉阵列架构集成。

Result: 在不同CMOS节点模拟，两种机制面积、延迟和功耗开销低于10%，MNIST数据集实验也证明可行。

Conclusion: 两种机制能有效保护忆阻内存计算系统，性能损失小。

Abstract: Memristive crossbar arrays enable in-memory computing by performing parallel
analog computations directly within memory, making them well-suited for machine
learning, neural networks, and neuromorphic systems. However, despite their
advantages, non-volatile memristors are vulnerable to security threats (such as
adversarial extraction of stored weights when the hardware is compromised.
Protecting these weights is essential since they represent valuable
intellectual property resulting from lengthy and costly training processes
using large, often proprietary, datasets. As a solution we propose two security
mechanisms: Keyed Permutor and Watermark Protection Columns; where both
safeguard critical weights and establish verifiable ownership (even in cases of
data leakage). Our approach integrates efficiently with existing memristive
crossbar architectures without significant design modifications. Simulations
across 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and
a large RF dataset, show that both mechanisms offer robust protection with
under 10% overhead in area, delay and power. We also present initial
experiments employing the widely known MNIST dataset; further highlighting the
feasibility of securing memristive in-memory computing systems with minimal
performance trade-offs.

</details>


### [358] [WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents](https://arxiv.org/abs/2510.01354)
*Yinuo Liu,Ruohan Xu,Xilong Wang,Yuqi Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 本文对检测针对Web代理的提示注入攻击进行了全面基准研究，介绍攻击分类、构建数据集、系统化检测方法并评估性能，发现部分检测器对特定攻击检测效果不佳。


<details>
  <summary>Details</summary>
Motivation: 现有检测通用提示注入攻击的方法未对Web代理进行系统评估，本文旨在填补这一空白。

Method: 基于威胁模型对攻击进行细粒度分类，构建包含恶意和良性样本的数据集，系统化文本和图像检测方法，并在多场景下评估性能。

Result: 部分检测器能以中到高的准确率识别依赖显式文本指令或可见图像扰动的攻击，但对省略显式指令或采用不可察觉扰动的攻击大多失败。

Conclusion: 在检测针对Web代理的提示注入攻击方面，现有部分检测器存在局限性，相关数据集和代码已开源。

Abstract: Multiple prompt injection attacks have been proposed against web agents. At
the same time, various methods have been developed to detect general prompt
injection attacks, but none have been systematically evaluated for web agents.
In this work, we bridge this gap by presenting the first comprehensive
benchmark study on detecting prompt injection attacks targeting web agents. We
begin by introducing a fine-grained categorization of such attacks based on the
threat model. We then construct datasets containing both malicious and benign
samples: malicious text segments generated by different attacks, benign text
segments from four categories, malicious images produced by attacks, and benign
images from two categories. Next, we systematize both text-based and
image-based detection methods. Finally, we evaluate their performance across
multiple scenarios. Our key findings show that while some detectors can
identify attacks that rely on explicit textual instructions or visible image
perturbations with moderate to high accuracy, they largely fail against attacks
that omit explicit instructions or employ imperceptible perturbations. Our
datasets and code are released at:
https://github.com/Norrrrrrr-lyn/WAInjectBench.

</details>


### [359] [Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks](https://arxiv.org/abs/2510.01359)
*Shoumik Saha,Jifan Chen,Sam Mayers,Sanjay Krishna Gouda,Zijian Wang,Varun Kumar*

Main category: cs.CR

TL;DR: 提出JAWS - BENCH基准和Judge Framework评估代码大模型代理越狱攻击，发现不同场景下攻击成功率及模型漏洞，建议执行感知防御。


<details>
  <summary>Details</summary>
Motivation: 现有评估未关注代码代理是否编译运行恶意程序，需评估代码代理在软件工程工作流中的安全绕过攻击情况。

Method: 提出JAWS - BENCH基准，涵盖三种工作区机制，结合可执行感知的Judge Framework测试合规性、攻击成功率等。

Result: 不同工作区机制下攻击成功率不同，JAWS - 0平均接受61%攻击，JAWS - 1平均ASR约71%，JAWS - M平均ASR约75%；封装成代理会使漏洞大幅增加。

Conclusion: 需要执行感知防御、代码上下文安全过滤器和能在多步推理中保留拒绝决策的机制。

Abstract: Code-capable large language model (LLM) agents are increasingly embedded into
software engineering workflows where they can read, write, and execute code,
raising the stakes of safety-bypass ("jailbreak") attacks beyond text-only
settings. Prior evaluations emphasize refusal or harmful-text detection,
leaving open whether agents actually compile and run malicious programs. We
present JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three
escalating workspace regimes that mirror attacker capability: empty (JAWS-0),
single-file (JAWS-1), and multi-file (JAWS-M). We pair this with a
hierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)
attack success, (iii) syntactic correctness, and (iv) runtime executability,
moving beyond refusal to measure deployable harm. Using seven LLMs from five
families as backends, we find that under prompt-only conditions in JAWS-0, code
agents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%
run end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~
100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the
multi-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly
deployable attack code. Across models, wrapping an LLM in an agent
substantially increases vulnerability -- ASR raises by 1.6x -- because initial
refusals are frequently overturned during later planning/tool-use steps.
Category-level analyses identify which attack classes are most vulnerable and
most readily deployable, while others exhibit large execution gaps. These
findings motivate execution-aware defenses, code-contextual safety filters, and
mechanisms that preserve refusal decisions throughout the agent's multi-step
reasoning and tool use.

</details>


### [360] [POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment](https://arxiv.org/abs/2510.01552)
*Luoxi Tang,Yuqiao Meng,Ankita Patra,Weicheng Ma,Muchao Ye,Zhaohan Xi*

Main category: cs.CR

TL;DR: 本文研究大语言模型在网络威胁情报中的内在漏洞，提出新分类方法分析失败案例，揭示三个基本漏洞并给出改进建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在网络威胁情报实际部署中存在显著性能差距，需研究其内在漏洞。

Method: 通过多个CTI基准和真实威胁报告进行大规模评估，引入包含分层、自回归细化和人工监督的分类方法分析失败案例。

Result: 揭示了大语言模型在支持CTI时存在虚假关联、矛盾知识和受限泛化三个基本漏洞。

Conclusion: 为设计更强大的大语言模型驱动的CTI系统提供了可行见解，以推动未来研究。

Abstract: Large Language Models (LLMs) are intensively used to assist security analysts
in counteracting the rapid exploitation of cyber threats, wherein LLMs offer
cyber threat intelligence (CTI) to support vulnerability assessment and
incident response. While recent work has shown that LLMs can support a wide
range of CTI tasks such as threat analysis, vulnerability detection, and
intrusion defense, significant performance gaps persist in practical
deployments. In this paper, we investigate the intrinsic vulnerabilities of
LLMs in CTI, focusing on challenges that arise from the nature of the threat
landscape itself rather than the model architecture. Using large-scale
evaluations across multiple CTI benchmarks and real-world threat reports, we
introduce a novel categorization methodology that integrates stratification,
autoregressive refinement, and human-in-the-loop supervision to reliably
analyze failure instances. Through extensive experiments and human inspections,
we reveal three fundamental vulnerabilities: spurious correlations,
contradictory knowledge, and constrained generalization, that limit LLMs in
effectively supporting CTI. Subsequently, we provide actionable insights for
designing more robust LLM-powered CTI systems to facilitate future research.

</details>


### [361] [Position: Privacy Is Not Just Memorization!](https://arxiv.org/abs/2510.01645)
*Niloofar Mireshghallah,Tianshi Li*

Main category: cs.CR

TL;DR: 论文指出大语言模型隐私风险研究多聚焦训练数据逐字记忆，提出更广泛隐私风险，给出风险分类，分析相关论文，呼吁跨学科研究。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大语言模型隐私风险的探讨多集中于训练数据提取，而其他更直接和广泛的隐私威胁未被充分研究。

Method: 提出LLM全生命周期隐私风险分类，通过案例研究说明现有隐私框架不足，对2016 - 2025年1322篇AI/ML隐私论文进行纵向分析。

Result: 发现记忆问题在技术研究中受过度关注，最紧迫的隐私危害在其他方面，当前技术方法作用有限且前行路径不明。

Conclusion: 呼吁研究界转变对LLM隐私的研究方式，采用跨学科方法应对新兴威胁。

Abstract: The discourse on privacy risks in Large Language Models (LLMs) has
disproportionately focused on verbatim memorization of training data, while a
constellation of more immediate and scalable privacy threats remain
underexplored. This position paper argues that the privacy landscape of LLM
systems extends far beyond training data extraction, encompassing risks from
data collection practices, inference-time context leakage, autonomous agent
capabilities, and the democratization of surveillance through deep inference
attacks. We present a comprehensive taxonomy of privacy risks across the LLM
lifecycle -- from data collection through deployment -- and demonstrate through
case studies how current privacy frameworks fail to address these multifaceted
threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers
published at leading conferences over the past decade (2016--2025), we reveal
that while memorization receives outsized attention in technical research, the
most pressing privacy harms lie elsewhere, where current technical approaches
offer little traction and viable paths forward remain unclear. We call for a
fundamental shift in how the research community approaches LLM privacy, moving
beyond the narrow focus of current technical solutions and embracing
interdisciplinary approaches that address the sociotechnical nature of these
emerging threats.

</details>


### [362] [Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP](https://arxiv.org/abs/2510.01780)
*Aueaphum Aueawatthanaphisut*

Main category: cs.CR

TL;DR: 本文提出基于MCP的框架实现多模态联邦医疗系统中安全跨代理通信，实验显示诊断准确率提升、客户端退出率降低，证明该框架是可扩展且值得信赖的。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习框架缺乏跨分布式和资源受限环境的多模态数据融合标准机制，需要解决异构医疗数据的安全互操作集成问题。

Method: 引入以Model Context Protocol (MCP) 为互操作层的框架，统一多模态特征对齐、带差分隐私的安全聚合、能量感知调度三个方面。

Result: 在基准数据集和试点临床队列上实验，诊断准确率比基线FL提高9.8%，客户端退出率降低54%，有可接受的隐私 - 效用权衡。

Conclusion: 基于MCP的多模态融合是迈向公平、下一代联邦健康基础设施的可扩展且值得信赖的途径。

Abstract: Secure and interoperable integration of heterogeneous medical data remains a
grand challenge in digital health. Current federated learning (FL) frameworks
offer privacy-preserving model training but lack standardized mechanisms to
orchestrate multi-modal data fusion across distributed and resource-constrained
environments. This study introduces a novel framework that leverages the Model
Context Protocol (MCP) as an interoperability layer for secure, cross-agent
communication in multi-modal federated healthcare systems. The proposed
architecture unifies three pillars: (i) multi-modal feature alignment for
clinical imaging, electronic medical records, and wearable IoT data; (ii)
secure aggregation with differential privacy to protect patient-sensitive
updates; and (iii) energy-aware scheduling to mitigate dropouts in mobile
clients. By employing MCP as a schema-driven interface, the framework enables
adaptive orchestration of AI agents and toolchains while ensuring compliance
with privacy regulations. Experimental evaluation on benchmark datasets and
pilot clinical cohorts demonstrates up to 9.8\% improvement in diagnostic
accuracy compared with baseline FL, a 54\% reduction in client dropout rates,
and clinically acceptable privacy--utility trade-offs. These results highlight
MCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward
equitable, next-generation federated health infrastructures.

</details>


### [363] [ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs](https://arxiv.org/abs/2510.01967)
*Aadarsh Anantha Ramakrishnan,Shubham Agarwal,Selvanayagam S,Kunwar Singh*

Main category: cs.CR

TL;DR: 随着图像生成模型发展，合成媒体问题凸显，传统水印方法有缺陷。本文首次提出ZK - WAGON系统，用ZK - SNARKs为图像生成模型加水印，还提出SL - ZKCC方法，在GAN和扩散模型上验证了系统。


<details>
  <summary>Details</summary>
Motivation: 图像生成模型强大但引发真实性、所有权和滥用等问题，传统水印方法不适合安全可扩展部署。

Method: 引入ZK - WAGON系统，使用ZK - SNARKs；提出Selective Layer ZK - Circuit Creation (SL - ZKCC)方法；通过Least Significant Bit (LSB)隐写术将ZK - SNARK证明嵌入图像。

Result: 在GAN和Diffusion模型上进行了系统验证。

Conclusion: 提供了一个安全、与模型无关的可信AI图像生成管道。

Abstract: As image generation models grow increasingly powerful and accessible,
concerns around authenticity, ownership, and misuse of synthetic media have
become critical. The ability to generate lifelike images indistinguishable from
real ones introduces risks such as misinformation, deepfakes, and intellectual
property violations. Traditional watermarking methods either degrade image
quality, are easily removed, or require access to confidential model internals
- making them unsuitable for secure and scalable deployment. We are the first
to introduce ZK-WAGON, a novel system for watermarking image generation models
using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge
(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing
model weights, generation prompts, or any sensitive internal information. We
propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively
convert key layers of an image generation model into a circuit, reducing proof
generation time significantly. Generated ZK-SNARK proofs are imperceptibly
embedded into a generated image via Least Significant Bit (LSB) steganography.
We demonstrate this system on both GAN and Diffusion models, providing a
secure, model-agnostic pipeline for trustworthy AI image generation.

</details>


### [364] [Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks](https://arxiv.org/abs/2510.01676)
*Milad Nasr,Yanick Fratantonio,Luca Invernizzi,Ange Albertini,Loua Farah,Alex Petit-Bianco,Andreas Terzis,Kurt Thomas,Elie Bursztein,Nicholas Carlini*

Main category: cs.CR

TL;DR: 研究针对ML组件的对抗攻击如何影响生产级恶意软件检测系统，以Gmail为例，设计对抗样本可绕过检测，还开发了防御方法并部署。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在大型生产系统中广泛应用，其缺陷会带来系统级漏洞，研究针对ML组件的对抗攻击对生产级恶意软件检测系统的影响。

Method: 以Gmail的恶意软件检测管道为例，设计对抗样本绕过Magika模型，还开发了缓解此类攻击的防御方法。

Result: 改变13字节的恶意软件样本，90%的情况可绕过Magika；防御模型下，高资源攻击者修改50字节攻击成功率仅20%。

Conclusion: 开发的防御方法可减轻此类攻击的严重程度，且已在Gmail分类器中部署。

Abstract: As deep learning models become widely deployed as components within larger
production systems, their individual shortcomings can create system-level
vulnerabilities with real-world impact. This paper studies how adversarial
attacks targeting an ML component can degrade or bypass an entire
production-grade malware detection system, performing a case study analysis of
Gmail's pipeline where file-type identification relies on a ML model.
  The malware detection pipeline in use by Gmail contains a machine learning
model that routes each potential malware sample to a specialized malware
classifier to improve accuracy and performance. This model, called Magika, has
been open sourced. By designing adversarial examples that fool Magika, we can
cause the production malware service to incorrectly route malware to an
unsuitable malware detector thereby increasing our chance of evading detection.
Specifically, by changing just 13 bytes of a malware sample, we can
successfully evade Magika in 90% of cases and thereby allow us to send malware
files over Gmail. We then turn our attention to defenses, and develop an
approach to mitigate the severity of these types of attacks. For our defended
production model, a highly resourced adversary requires 50 bytes to achieve
just a 20% attack success rate. We implement this defense, and, thanks to a
collaboration with Google engineers, it has already been deployed in production
for the Gmail classifier.

</details>


### [365] [NoMod: A Non-modular Attack on Module Learning With Errors](https://arxiv.org/abs/2510.02162)
*Cristian Bassotto,Ermes Franch,Marina Krček,Stjepan Picek*

Main category: cs.CR

TL;DR: 本文提出NoMod ML - Attack混合白盒密码分析方法，可绕过模约简挑战恢复秘密，实验展示了其在不同参数下的恢复效果并公开实现代码。


<details>
  <summary>Details</summary>
Motivation: 量子计算威胁经典公钥密码学，NIST采用基于Module - LWE问题的后量子方案，需要对其进行密码分析。

Method: 将环绕视为统计损坏，把秘密恢复转化为鲁棒线性估计，结合优化格预处理和通过Tukey双权损失训练的鲁棒估计器。

Result: 在维度n = 350时可完全恢复二进制秘密，n = 256时恢复稀疏二项式秘密，在CRYSTALS - Kyber特定参数设置下成功恢复稀疏秘密。

Conclusion: 提出的NoMod ML - Attack方法在密码分析方面有较好效果。

Abstract: The advent of quantum computing threatens classical public-key cryptography,
motivating NIST's adoption of post-quantum schemes such as those based on the
Module Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a
hybrid white-box cryptanalytic method that circumvents the challenge of
modeling modular reduction by treating wrap-arounds as statistical corruption
and casting secret recovery as robust linear estimation. Our approach combines
optimized lattice preprocessing--including reduced-vector saving and algebraic
amplification--with robust estimators trained via Tukey's Biweight loss.
Experiments show NoMod achieves full recovery of binary secrets for dimension
$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful
recovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =
(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous
repository https://anonymous.4open.science/r/NoMod-3BD4.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [366] [Variational Secret Common Randomness Extraction](https://arxiv.org/abs/2510.02048)
*Xinyang Li,Vlad C. Andrei,Peter J. Gu,Yiqi Chen,Ullrich J. Mönich,Holger Boche*

Main category: cs.IT

TL;DR: 本文提出两阶段通用随机数提取框架及用于综合感知与通信系统的基于感知的物理层密钥生成方法，经仿真和实测验证其可行性和性能。


<details>
  <summary>Details</summary>
Motivation: 研究在有窃听者情况下，两个合法方通过公共讨论从相关随机源提取通用随机数或密钥的问题，改进传统物理层密钥生成方法的高开销和不适用于高移动场景的问题。

Method: 提出两阶段通用随机数提取框架，第一阶段用变分概率量化（VPQ），结合变分学习目标和对抗训练；第二阶段用基于码偏移构造的安全草图；提出基于感知的物理层密钥生成方法，以配对的距离 - 角度图为相关源。

Result: 通过端到端仿真和真实软件定义无线电测量验证了方法的可行性和性能，包括窃听者有部分位置信息的场景。

Conclusion: 所提出的通用随机数提取框架和基于感知的物理层密钥生成方法是可行的，性能令人信服。

Abstract: This paper studies the problem of extracting common randomness (CR) or secret
keys from correlated random sources observed by two legitimate parties, Alice
and Bob, through public discussion in the presence of an eavesdropper, Eve. We
propose a practical two-stage CR extraction framework. In the first stage, the
variational probabilistic quantization (VPQ) step is introduced, where Alice
and Bob employ probabilistic neural network (NN) encoders to map their
observations into discrete, nearly uniform random variables (RVs) with high
agreement probability while minimizing information leakage to Eve. This is
realized through a variational learning objective combined with adversarial
training. In the second stage, a secure sketch using code-offset construction
reconciles the encoder outputs into identical secret keys, whose secrecy is
guaranteed by the VPQ objective. As a representative application, we study
physical layer key (PLK) generation. Beyond the traditional methods, which rely
on the channel reciprocity principle and require two-way channel probing, thus
suffering from large protocol overhead and being unsuitable in high mobility
scenarios, we propose a sensing-based PLK generation method for integrated
sensing and communications (ISAC) systems, where paired range-angle (RA) maps
measured at Alice and Bob serve as correlated sources. The idea is verified
through both end-to-end simulations and real-world software-defined radio (SDR)
measurements, including scenarios where Eve has partial knowledge about Bob's
position. The results demonstrate the feasibility and convincing performance of
both the proposed CR extraction framework and sensing-based PLK generation
method.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [367] [RealClass: A Framework for Classroom Speech Simulation with Public Datasets and Game Engines](https://arxiv.org/abs/2510.01462)
*Ahmed Adel Attia,Jing Liu,Carol Espy Wilson*

Main category: cs.SD

TL;DR: 本文提出用游戏引擎合成教室噪声和RIRs的方法，并构建RealClass数据集，实验表明该数据集能近似真实教室语音。


<details>
  <summary>Details</summary>
Motivation: 大规模教室语音数据稀缺阻碍了教育领域AI语音模型发展，现有教室数据集有限且不公开，缺乏专用教室噪声和RIR语料库。

Method: 使用游戏引擎合成教室噪声和RIRs，构建结合合成教室噪声语料库和公开语料库中教室语音数据集的RealClass。

Result: 在干净和嘈杂语音上的实验表明，RealClass能紧密近似真实教室语音。

Conclusion: 在缺乏大量真实教室语音的情况下，RealClass是有价值的资源。

Abstract: The scarcity of large-scale classroom speech data has hindered the
development of AI-driven speech models for education. Classroom datasets remain
limited and not publicly available, and the absence of dedicated classroom
noise or Room Impulse Response (RIR) corpora prevents the use of standard data
augmentation techniques.
  In this paper, we introduce a scalable methodology for synthesizing classroom
noise and RIRs using game engines, a versatile framework that can extend to
other domains beyond the classroom. Building on this methodology, we present
RealClass, a dataset that combines a synthesized classroom noise corpus with a
classroom speech dataset compiled from publicly available corpora. The speech
data pairs a children's speech corpus with instructional speech extracted from
YouTube videos to approximate real classroom interactions in clean conditions.
Experiments on clean and noisy speech show that RealClass closely approximates
real classroom speech, making it a valuable asset in the absence of abundant
real classroom speech.

</details>


### [368] [Emotional Text-To-Speech Based on Mutual-Information-Guided Emotion-Timbre Disentanglement](https://arxiv.org/abs/2510.01722)
*Jianing Yang,Sheng Li,Takahiro Shinozaki,Yuki Saito,Hiroshi Saruwatari*

Main category: cs.SD

TL;DR: 提出一种新的情感TTS方法，能实现细粒度音素级情感嵌入预测，实验显示优于基线系统，凸显解缠和细粒度表示潜力。


<details>
  <summary>Details</summary>
Motivation: 现有情感TTS和风格迁移方法无法捕捉参考语音的细微声学细节。

Method: 采用风格解缠方法引导两个特征提取器，减少音色和情感特征间的互信息，从参考语音中分离不同风格组件。

Result: 该方法在生成自然且情感丰富的语音方面优于基线TTS系统。

Conclusion: 解缠和细粒度表示在提升情感TTS系统质量和灵活性方面有潜力。

Abstract: Current emotional Text-To-Speech (TTS) and style transfer methods rely on
reference encoders to control global style or emotion vectors, but do not
capture nuanced acoustic details of the reference speech. To this end, we
propose a novel emotional TTS method that enables fine-grained phoneme-level
emotion embedding prediction while disentangling intrinsic attributes of the
reference speech. The proposed method employs a style disentanglement method to
guide two feature extractors, reducing mutual information between timbre and
emotion features, and effectively separating distinct style components from the
reference speech. Experimental results demonstrate that our method outperforms
baseline TTS systems in generating natural and emotionally rich speech. This
work highlights the potential of disentangled and fine-grained representations
in advancing the quality and flexibility of emotional TTS systems.

</details>


### [369] [SingMOS-Pro: An Comprehensive Benchmark for Singing Quality Assessment](https://arxiv.org/abs/2510.01812)
*Yuxun Tang,Lan Liu,Wenhao Feng,Yiwen Zhao,Jionghao Han,Yifeng Yu,Jiatong Shi,Qin Jin*

Main category: cs.SD

TL;DR: 提出用于自动歌唱质量评估的数据集SingMOS - Pro，包含更多标注，有大量歌唱片段，还探索数据利用和评估方法并建立基线。


<details>
  <summary>Details</summary>
Motivation: 歌唱语音生成发展快，但评估歌唱质量是挑战，人工评估成本高、耗时，现有客观指标覆盖感知方面有限。

Method: 基于预览版SingMOS构建SingMOS - Pro，对更多部分进行标注，让专业标注者对片段打分；探索不同标准下MOS数据利用，在数据集上对常用评估方法进行基准测试。

Result: 创建了包含7981个歌唱片段的SingMOS - Pro数据集，每个片段至少有五个专业标注者的评分；建立了强基线和实用参考。

Conclusion: SingMOS - Pro数据集为自动歌唱质量评估提供了更广泛覆盖和多样的数据，为未来研究提供了实用参考，数据集可在线获取。

Abstract: Singing voice generation progresses rapidly, yet evaluating singing quality
remains a critical challenge. Human subjective assessment, typically in the
form of listening tests, is costly and time consuming, while existing objective
metrics capture only limited perceptual aspects. In this work, we introduce
SingMOS-Pro, a dataset for automatic singing quality assessment. Building on
our preview version SingMOS, which provides only overall ratings, SingMOS-Pro
expands annotations of the additional part to include lyrics, melody, and
overall quality, offering broader coverage and greater diversity. The dataset
contains 7,981 singing clips generated by 41 models across 12 datasets,
spanning from early systems to recent advances. Each clip receives at least
five ratings from professional annotators, ensuring reliability and
consistency. Furthermore, we explore how to effectively utilize MOS data
annotated under different standards and benchmark several widely used
evaluation methods from related tasks on SingMOS-Pro, establishing strong
baselines and practical references for future research. The dataset can be
accessed at https://huggingface.co/datasets/TangRain/SingMOS-Pro.

</details>


### [370] [HRTFformer: A Spatially-Aware Transformer for Personalized HRTF Upsampling in Immersive Audio Rendering](https://arxiv.org/abs/2510.01891)
*Xuyi Hu,Jian Li,Shaojie Zhang,Stefan Goetz,Lorenzo Picinali,Ozgur B. Akan,Aidan O. T. Hogg*

Main category: cs.SD

TL;DR: 本文提出基于变压器架构的HRTF上采样方法，结合邻域差异损失，实验表明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 个性化HRTF测量复杂，难以大规模应用，现有HRTF空间上采样方法在高上采样因子下存在长距离空间一致性和泛化问题。

Method: 提出基于变压器的架构进行HRTF上采样，在球谐域工作，引入邻域差异损失增强空间连贯性。

Result: 实验表明该模型在生成逼真、高保真HRTF方面大幅超越领先方法。

Conclusion: 所提方法能有效解决现有HRTF上采样方法的问题，提高HRTF上采样的准确性和空间连贯性。

Abstract: Personalized Head-Related Transfer Functions (HRTFs) are starting to be
introduced in many commercial immersive audio applications and are crucial for
realistic spatial audio rendering. However, one of the main hesitations
regarding their introduction is that creating personalized HRTFs is impractical
at scale due to the complexities of the HRTF measurement process. To mitigate
this drawback, HRTF spatial upsampling has been proposed with the aim of
reducing measurements required. While prior work has seen success with
different machine learning (ML) approaches, these models often struggle with
long-range spatial consistency and generalization at high upsampling factors.
In this paper, we propose a novel transformer-based architecture for HRTF
upsampling, leveraging the attention mechanism to better capture spatial
correlations across the HRTF sphere. Working in the spherical harmonic (SH)
domain, our model learns to reconstruct high-resolution HRTFs from sparse input
measurements with significantly improved accuracy. To enhance spatial
coherence, we introduce a neighbor dissimilarity loss that promotes magnitude
smoothness, yielding more realistic upsampling. We evaluate our method using
both perceptual localization models and objective spectral distortion metrics.
Experiments show that our model surpasses leading methods by a substantial
margin in generating realistic, high-fidelity HRTFs.

</details>


### [371] [Exploring Resolution-Wise Shared Attention in Hybrid Mamba-U-Nets for Improved Cross-Corpus Speech Enhancement](https://arxiv.org/abs/2510.01958)
*Nikolai Lund Kühne,Jesper Jensen,Jan Østergaard,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: 提出结合Mamba和多头注意力的RWSA - MambaUNet模型用于语音增强，在跨语料库性能上表现出色，小模型也有优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有结合Mamba和注意力机制的模型在跨语料库泛化性能好，Mamba集成到U - Net结构有先进增强性能且能降低模型规模和计算复杂度，受此启发提出新模型。

Method: 提出RWSA - MambaUNet，一种在U - Net结构中结合Mamba和多头注意力的混合模型，采用分辨率共享注意力（RWSA）。

Result: 最佳性能的RWSA - MambaUNet在两个域外测试集上达到先进泛化性能，最小模型在多个指标上超越所有基线，且模型参数不到一半，FLOPs更少。

Conclusion: RWSA - MambaUNet模型在语音增强的跨语料库性能方面表现优异，具有高效性。

Abstract: Recent advances in speech enhancement have shown that models combining Mamba
and attention mechanisms yield superior cross-corpus generalization
performance. At the same time, integrating Mamba in a U-Net structure has
yielded state-of-the-art enhancement performance, while reducing both model
size and computational complexity. Inspired by these insights, we propose
RWSA-MambaUNet, a novel and efficient hybrid model combining Mamba and
multi-head attention in a U-Net structure for improved cross-corpus
performance. Resolution-wise shared attention (RWSA) refers to layerwise
attention-sharing across corresponding time- and frequency resolutions. Our
best-performing RWSA-MambaUNet model achieves state-of-the-art generalization
performance on two out-of-domain test sets. Notably, our smallest model
surpasses all baselines on the out-of-domain DNS 2020 test set in terms of
PESQ, SSNR, and ESTOI, and on the out-of-domain EARS-WHAM_v2 test set in terms
of SSNR, ESTOI, and SI-SDR, while using less than half the model parameters and
a fraction of the FLOPs.

</details>


### [372] [Go witheFlow: Real-time Emotion Driven Audio Effects Modulation](https://arxiv.org/abs/2510.02171)
*Edmund Dervakos,Spyridon Kantarelis,Vassilis Lyberatos,Jason Liartis,Giorgos Stamou*

Main category: cs.SD

TL;DR: 介绍了witheFlow系统，可基于生物信号和音频特征自动调节音频效果以增强实时音乐表演，目前处于概念验证阶段。


<details>
  <summary>Details</summary>
Motivation: 音乐表演是人类特有的活动，机器缺乏情感体验能力，以此为契机探索人机协作。

Method: 引入witheFlow系统，基于生物信号和音频特征自动调节音频效果。

Result: 开发出处于概念验证阶段的witheFlow系统，能在笔记本上本地运行，开源且需兼容数字音频工作站和传感器。

Conclusion: witheFlow系统有潜力增强实时音乐表演，为音乐领域的人机协作提供了一种方式。

Abstract: Music performance is a distinctly human activity, intrinsically linked to the
performer's ability to convey, evoke, or express emotion. Machines cannot
perform music in the human sense; they can produce, reproduce, execute, or
synthesize music, but they lack the capacity for affective or emotional
experience. As such, music performance is an ideal candidate through which to
explore aspects of collaboration between humans and machines. In this paper, we
introduce the witheFlow system, designed to enhance real-time music performance
by automatically modulating audio effects based on features extracted from both
biosignals and the audio itself. The system, currently in a proof-of-concept
phase, is designed to be lightweight, able to run locally on a laptop, and is
open-source given the availability of a compatible Digital Audio Workstation
and sensors.

</details>


### [373] [Bias beyond Borders: Global Inequalities in AI-Generated Music](https://arxiv.org/abs/2510.01963)
*Ahmet Solak,Florian Grötschla,Luca A. Lanzendörfer,Roger Wattenhofer*

Main category: cs.SD

TL;DR: 提出大规模音乐数据集GlobalDISCO，发现不同地区和音乐类型在音乐质量、模型表现上存在差异。


<details>
  <summary>Details</summary>
Motivation: 现有音乐生成模型在跨国家、语言、文化和音乐类型的偏差研究不足，且缺乏捕捉全球音乐多样性的数据集和基准。

Method: 引入包含73k音乐曲目的GlobalDISCO数据集，与LAION - DISCO - 12M中的93k参考曲目配对，涵盖147种语言和来自多国多洲的音乐风格。

Result: 高资源和低资源地区在音乐质量和与参考音乐的对齐度上有巨大差异；主流和小众地域流派的模型表现有明显差异。

Conclusion: 音乐生成模型在不同资源地区和音乐类型上存在表现不均的情况。

Abstract: While recent years have seen remarkable progress in music generation models,
research on their biases across countries, languages, cultures, and musical
genres remains underexplored. This gap is compounded by the lack of datasets
and benchmarks that capture the global diversity of music. To address these
challenges, we introduce GlobalDISCO, a large-scale dataset consisting of 73k
music tracks generated by state-of-the-art commercial generative music models,
along with paired links to 93k reference tracks in LAION-DISCO-12M. The dataset
spans 147 languages and includes musical style prompts extracted from
MusicBrainz and Wikipedia. The dataset is globally balanced, representing
musical styles from artists across 79 countries and five continents. Our
evaluation reveals large disparities in music quality and alignment with
reference music between high-resource and low-resource regions. Furthermore, we
find marked differences in model performance between mainstream and
geographically niche genres, including cases where models generate music for
regional genres that more closely align with the distribution of mainstream
styles.

</details>


### [374] [Multi-bit Audio Watermarking](https://arxiv.org/abs/2510.01968)
*Luca A. Lanzendörfer,Kyle Fearne,Florian Grötschla,Roger Wattenhofer*

Main category: cs.SD

TL;DR: 提出Timbru音频水印模型，无需训练嵌入-检测模型，在音频VAE潜在空间加扰动，用CLAP模型提取水印，评估显示其有最佳平均误码率和感知质量。


<details>
  <summary>Details</summary>
Motivation: 实现先进的音频水印鲁棒性和不可感知性的权衡，且无需训练嵌入-检测模型。

Method: 对任意44.1kHz立体声音乐片段，在预训练音频VAE潜在空间进行逐音频梯度优化添加不可感知扰动，由组合消息和感知损失引导，用预训练CLAP模型提取水印。

Result: 在MUSDB18 - HQ上对比AudioSeal、WavMark和SilentCipher，在常见攻击下有最佳平均误码率，且保留了感知质量。

Conclusion: Timbru提供了一条高效、无数据集依赖的不可感知音频水印路径。

Abstract: We present Timbru, a post-hoc audio watermarking model that achieves
state-of-the-art robustness and imperceptibility trade-offs without training an
embedder-detector model. Given any 44.1 kHz stereo music snippet, our method
performs per-audio gradient optimization to add imperceptible perturbations in
the latent space of a pretrained audio VAE, guided by a combined message and
perceptual loss. The watermark can then be extracted using a pretrained CLAP
model. We evaluate 16-bit watermarking on MUSDB18-HQ against AudioSeal,
WavMark, and SilentCipher across common filtering, noise, compression,
resampling, cropping, and regeneration attacks. Our approach attains the best
average bit error rates, while preserving perceptual quality, demonstrating an
efficient, dataset-free path to imperceptible audio watermarking.

</details>


### [375] [SoundReactor: Frame-level Online Video-to-Audio Generation](https://arxiv.org/abs/2510.02110)
*Koichi Saito,Julian Tanke,Christian Simon,Masato Ishii,Kazuki Shimada,Zachary Novack,Zhi Zhong,Akio Hayakawa,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: 提出帧级在线视频到音频生成任务，设计SoundReactor框架，在游戏视频基准测试中生成高质量音频且实现低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有视频到音频生成模型为离线模式，限制了在交互式应用中的使用，需解决该问题。

Method: 提出帧级在线V2A生成任务，设计SoundReactor框架，使用解码器因果Transformer和DINOv2视觉编码器，通过扩散预训练和一致性微调进行训练。

Result: 在AAA游戏视频基准测试中，模型能生成语义和时间对齐的高质量全频带立体声，单H100处理30FPS、480p视频时实现低每帧波形级延迟。

Conclusion: SoundReactor框架可有效解决在线视频到音频生成问题，实现低延迟。

Abstract: Prevailing Video-to-Audio (V2A) generation models operate offline, assuming
an entire video sequence or chunks of frames are available beforehand. This
critically limits their use in interactive applications such as live content
creation and emerging generative world models. To address this gap, we
introduce the novel task of frame-level online V2A generation, where a model
autoregressively generates audio from video without access to future video
frames. Furthermore, we propose SoundReactor, which, to the best of our
knowledge, is the first simple yet effective framework explicitly tailored for
this task. Our design enforces end-to-end causality and targets low per-frame
latency with audio-visual synchronization. Our model's backbone is a
decoder-only causal transformer over continuous audio latents. For vision
conditioning, it leverages grid (patch) features extracted from the smallest
variant of the DINOv2 vision encoder, which are aggregated into a single token
per frame to maintain end-to-end causality and efficiency. The model is trained
through a diffusion pre-training followed by consistency fine-tuning to
accelerate the diffusion head decoding. On a benchmark of diverse gameplay
videos from AAA titles, our model successfully generates semantically and
temporally aligned, high-quality full-band stereo audio, validated by both
objective and human evaluations. Furthermore, our model achieves low per-frame
waveform-level latency (26.3ms with the head NFE=1, 31.5ms with NFE=4) on
30FPS, 480p videos using a single H100. Demo samples are available at
https://koichi-saito-sony.github.io/soundreactor/.

</details>


### [376] [High-Fidelity Speech Enhancement via Discrete Audio Tokens](https://arxiv.org/abs/2510.02187)
*Luca A. Lanzendörfer,Frédéric Berdoz,Antonis Asonitis,Roger Wattenhofer*

Main category: cs.SD

TL;DR: 提出DAC - SE1语音增强框架，在实验中超越现有自回归SE方法，还发布代码和模型检查点。


<details>
  <summary>Details</summary>
Motivation: 现有自回归基于Transformer的语音增强方法依赖复杂多阶段管道和低采样率编解码器，局限于特定任务语音增强。

Method: 引入基于离散高分辨率音频表示的简化语言模型SE框架DAC - SE1，保留细粒度声学细节并保持语义连贯。

Result: DAC - SE1在客观感知指标和MUSHRA人类评估中超越现有自回归SE方法。

Conclusion: 发布代码和模型检查点以支持可扩展、统一和高质量语音增强的进一步研究。

Abstract: Recent autoregressive transformer-based speech enhancement (SE) methods have
shown promising results by leveraging advanced semantic understanding and
contextual modeling of speech. However, these approaches often rely on complex
multi-stage pipelines and low sampling rate codecs, limiting them to narrow and
task-specific speech enhancement. In this work, we introduce DAC-SE1, a
simplified language model-based SE framework leveraging discrete
high-resolution audio representations; DAC-SE1 preserves fine-grained acoustic
details while maintaining semantic coherence. Our experiments show that DAC-SE1
surpasses state-of-the-art autoregressive SE methods on both objective
perceptual metrics and in a MUSHRA human evaluation. We release our codebase
and model checkpoints to support further research in scalable, unified, and
high-quality speech enhancement.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [377] [Hybrid Predictive Modeling of Malaria Incidence in the Amhara Region, Ethiopia: Integrating Multi-Output Regression and Time-Series Forecasting](https://arxiv.org/abs/2510.01302)
*Kassahun Azezew,Amsalu Tesema,Bitew Mekuria,Ayenew Kassie,Animut Embiale,Ayodeji Olalekan Salau,Tsega Asresa*

Main category: q-bio.OT

TL;DR: 本文提出混合预测模型框架预测埃塞俄比亚阿姆哈拉地区疟疾发病率，准确性高，能助力决策和资源优化。


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚尤其是阿姆哈拉地区疟疾是公共卫生难题，准确预测疟疾爆发对资源分配和干预至关重要。

Method: 提出结合时间序列预测、多输出回归和传统基于回归预测的混合预测建模框架，利用环境变量、过往病例数据和人口信息训练验证模型。

Result: 该模型比单一方法预测准确性更高，能揭示隐藏模式。

Conclusion: 提供了有效且可重复的疟疾发病率预测框架，支持疫区循证决策、针对性干预和资源优化。

Abstract: Malaria remains a major public health concern in Ethiopia, particularly in
the Amhara Region, where seasonal and unpredictable transmission patterns make
prevention and control challenging. Accurately forecasting malaria outbreaks is
essential for effective resource allocation and timely interventions. This
study proposes a hybrid predictive modeling framework that combines time-series
forecasting, multi-output regression, and conventional regression-based
prediction to forecast the incidence of malaria. Environmental variables, past
malaria case data, and demographic information from Amhara Region health
centers were used to train and validate the models. The multi-output regression
approach enables the simultaneous prediction of multiple outcomes, including
Plasmodium species-specific cases, temporal trends, and spatial variations,
whereas the hybrid framework captures both seasonal patterns and correlations
among predictors. The proposed model exhibits higher prediction accuracy than
single-method approaches, exposing hidden patterns and providing valuable
information to public health authorities. This study provides a valid and
repeatable malaria incidence prediction framework that can support
evidence-based decision-making, targeted interventions, and resource
optimization in endemic areas.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [378] [BioBlobs: Differentiable Graph Partitioning for Protein Representation Learning](https://arxiv.org/abs/2510.01632)
*Xin Wang,Carlos Oliver*

Main category: q-bio.BM

TL;DR: 当前蛋白质表示学习模型有缺陷，提出BioBlobs模块，可提高蛋白质编码器性能。


<details>
  <summary>Details</summary>
Motivation: 当前蛋白质表示学习模型依赖刚性子结构，会扭曲蛋白质功能信号。

Method: 引入BioBlobs模块，动态划分蛋白质结构为可变大小、不重叠子结构，量化成共享可解释码本计算蛋白质嵌入。

Result: BioBlobs表示提高了如GVP - GNN等常用蛋白质编码器在各种蛋白质表示学习任务中的性能。

Conclusion: 直接捕获与功能相关蛋白质子结构的架构有价值，可提升预测性能并深入理解蛋白质功能。

Abstract: Protein function is driven by coherent substructures which vary in size and
topology, yet current protein representation learning models (PRL) distort
these signals by relying on rigid substructures such as k-hop and fixed radius
neighbourhoods. We introduce BioBlobs, a plug-and-play, fully differentiable
module that represents proteins by dynamically partitioning structures into
flexibly-sized, non-overlapping substructures ("blobs"). The resulting blobs
are quantized into a shared and interpretable codebook, yielding a discrete
vocabulary of function-relevant protein substructures used to compute protein
embeddings. We show that BioBlobs representations improve the performance of
widely used protein encoders such as GVP-GNN across various PRL tasks. Our
approach highlights the value of architectures that directly capture
function-relevant protein substructures, enabling both improved predictive
performance and mechanistic insight into protein function.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [379] [A theoretical framework for M-posteriors: frequentist guarantees and robustness properties](https://arxiv.org/abs/2510.01358)
*Juraj Marusic,Marco Avella Medina,Cynthia Rush*

Main category: math.ST

TL;DR: 提出广义后验分布M - posteriors理论框架，证明其渐近正态分布，形式化其鲁棒性，并举例说明适用性。


<details>
  <summary>Details</summary>
Motivation: 构建与频率学派M - 估计量对应的贝叶斯后验理论框架。

Method: 在M - 估计损失和先验的温和条件下进行理论推导，通过新的后验影响函数和后验分布的崩溃点定义形式化鲁棒性。

Result: M - posteriors在温和条件下渐近正态分布，在M - 估计量附近收缩，表现出频率一致性和一定鲁棒性。

Conclusion: 所提出的理论框架具有广泛适用性和实证相关性。

Abstract: We provide a theoretical framework for a wide class of generalized posteriors
that can be viewed as the natural Bayesian posterior counterpart of the class
of M-estimators in the frequentist world. We call the members of this class
M-posteriors and show that they are asymptotically normally distributed under
mild conditions on the M-estimation loss and the prior. In particular, an
M-posterior contracts in probability around a normal distribution centered at
an M-estimator, showing frequentist consistency and suggesting some degree of
robustness depending on the reference M-estimator. We formalize the robustness
properties of the M-posteriors by a new characterization of the posterior
influence function and a novel definition of breakdown point adapted for
posterior distributions. We illustrate the wide applicability of our theory in
various popular models and illustrate their empirical relevance in some
numerical examples.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [380] [Comparing Contrastive and Triplet Loss in Audio-Visual Embedding: Intra-Class Variance and Greediness Analysis](https://arxiv.org/abs/2510.02161)
*Donghuo Zeng*

Main category: cs.MM

TL;DR: 本文对对比损失和三元组损失进行理论和实证比较，发现三元组损失在表示质量和任务性能上更优，并给出两者适用场景。


<details>
  <summary>Details</summary>
Motivation: 对比损失和三元组损失在深度度量学习中广泛使用，但对其表示质量的影响理解不足，因此进行对比研究。

Method: 在合成数据和真实数据集（MNIST、CIFAR - 10等）上进行特定任务实验，研究类内和类间方差、优化行为，分析损失衰减率、活跃比率和梯度范数。

Result: 三元组损失能保留更大类内和类间方差，对比损失使类内嵌入更紧凑；对比损失早期有许多小更新，三元组损失更新少但强；在分类和检索任务中，三元组损失性能更优。

Conclusion: 建议使用三元组损失保留细节和关注难样本，使用对比损失进行平滑、广泛的嵌入优化。

Abstract: Contrastive loss and triplet loss are widely used objectives in deep metric
learning, yet their effects on representation quality remain insufficiently
understood. We present a theoretical and empirical comparison of these losses,
focusing on intra- and inter-class variance and optimization behavior (e.g.,
greedy updates). Through task-specific experiments with consistent settings on
synthetic data and real datasets-MNIST, CIFAR-10-it is shown that triplet loss
preserves greater variance within and across classes, supporting finer-grained
distinctions in the learned representations. In contrast, contrastive loss
tends to compact intra-class embeddings, which may obscure subtle semantic
differences. To better understand their optimization dynamics, By examining
loss-decay rate, active ratio, and gradient norm, we find that contrastive loss
drives many small updates early on, while triplet loss produces fewer but
stronger updates that sustain learning on hard examples. Finally, across both
classification and retrieval tasks on MNIST, CIFAR-10, CUB-200, and CARS196
datasets, our results consistently show that triplet loss yields superior
performance, which suggests using triplet loss for detail retention and
hard-sample focus, and contrastive loss for smoother, broad-based embedding
refinement.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [381] [From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding](https://arxiv.org/abs/2510.01513)
*Basem Rizk,Joel Walsh,Mark Core,Benjamin Nye*

Main category: cs.CV

TL;DR: 提出一个用于多模态内容分析的框架，将视频转换为半结构化数据并表示为知识图谱，支持持续学习。


<details>
  <summary>Details</summary>
Motivation: 多模态内容分析困难、计算成本高且工程工作量大，融合开源模型与复杂视频数据有挑战。

Method: 设计候选管道配方，结合预训练模型将视频转换为半结构化数据，再转换为可查询、支持持续学习的知识图谱。

Result: 构建了能高效进行多模态内容分析原型管道的框架。

Conclusion: 该框架可将视频转换为特定格式，支持动态融入新领域知识。

Abstract: Analysis of multi-modal content can be tricky, computationally expensive, and
require a significant amount of engineering efforts. Lots of work with
pre-trained models on static data is out there, yet fusing these opensource
models and methods with complex data such as videos is relatively challenging.
In this paper, we present a framework that enables efficiently prototyping
pipelines for multi-modal content analysis. We craft a candidate recipe for a
pipeline, marrying a set of pre-trained models, to convert videos into a
temporal semi-structured data format. We translate this structure further to a
frame-level indexed knowledge graph representation that is query-able and
supports continual learning, enabling the dynamic incorporation of new
domain-specific knowledge through an interactive medium.

</details>


### [382] [Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications](https://arxiv.org/abs/2510.02197)
*Emmanuel Nsengiyumvaa,Leonard Niyitegekaa,Eric Umuhoza*

Main category: cs.CV

TL;DR: 本文提出用耳静脉模式进行猪的非侵入式生物识别方法，准确率高且处理速度快，为养殖户提供了经济有效的动物识别方案。


<details>
  <summary>Details</summary>
Motivation: 常见猪识别方法不可靠、成本高，不适用于小规模养殖户，需要新的识别方法。

Method: 收集混合品种猪的耳图像，开发多阶段计算机视觉管道提取特征，用机器学习模型分类。

Result: 支持向量机（SVM）识别准确率达98.12%，处理到分类平均用时8.3秒。

Conclusion: 该系统用生物标记替代物理标识符，为养殖户提供经济无压力的识别方法，证实耳静脉生物识别在畜牧管理数字化中的实用性。

Abstract: Accurate livestock identification is a cornerstone of modern farming: it
supports health monitoring, breeding programs, and productivity tracking.
However, common pig identification methods, such as ear tags and microchips,
are often unreliable, costly, target pure breeds, and thus impractical for
small-scale farmers. To address this gap, we propose a noninvasive biometric
identification approach that leverages uniqueness of the auricular vein
patterns. To this end, we have collected 800 ear images from 20 mixed-breed
pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a
standard smartphone and simple back lighting. A multistage computer vision
pipeline was developed to enhance vein visibility, extract structural and
spatial features, and generate biometric signatures. These features were then
classified using machine learning models. Support Vector Machines (SVM)
achieved the highest accuracy: correctly identifying pigs with 98.12% precision
across mixed-breed populations. The entire process from image processing to
classification was completed in an average of 8.3 seconds, demonstrating
feasibility for real-time farm deployment. We believe that by replacing fragile
physical identifiers with permanent biological markers, this system provides
farmers with a cost-effective and stress-free method of animal identification.
More broadly, the findings confirm the practicality of auricular vein
biometrics for digitizing livestock management, reinforcing its potential to
extend the benefits of precision farming to resource-constrained agricultural
communities.

</details>


### [383] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andrés Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: 本文提出LVTINO用于高清视频恢复，利用视频一致性模型，在重建保真度和计算效率上建立新基准。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的潜在扩散模型应用于高清视频恢复时存在时间不一致问题，需解决恢复精细空间细节和捕捉微妙时间依赖的挑战。

Method: 利用视频一致性模型（VCMs），提出LVTINO，其调节机制无需自动微分。

Result: 在多种视频逆问题实验中，相比逐帧应用图像LDM的现有方法有显著感知提升。

Conclusion: LVTINO在视频重建质量、测量一致性和时间过渡平滑性上表现出色，在重建保真度和计算效率上建立了新基准。

Abstract: Computational imaging methods increasingly rely on powerful generative
diffusion models to tackle challenging image restoration tasks. In particular,
state-of-the-art zero-shot image inverse solvers leverage distilled
text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy
and perceptual quality with high computational efficiency. However, extending
these advances to high-definition video restoration remains a significant
challenge, due to the need to recover fine spatial detail while capturing
subtle temporal dependencies. Consequently, methods that naively apply
image-based LDM priors on a frame-by-frame basis often result in temporally
inconsistent reconstructions. We address this challenge by leveraging recent
advances in Video Consistency Models (VCMs), which distill video latent
diffusion models into fast generators that explicitly capture temporal
causality. Building on this foundation, we propose LVTINO, the first zero-shot
or plug-and-play inverse solver for high definition video restoration with
priors encoded by VCMs. Our conditioning mechanism bypasses the need for
automatic differentiation and achieves state-of-the-art video reconstruction
quality with only a few neural function evaluations, while ensuring strong
measurement consistency and smooth temporal transitions across frames.
Extensive experiments on a diverse set of video inverse problems show
significant perceptual improvements over current state-of-the-art methods that
apply image LDMs frame by frame, establishing a new benchmark in both
reconstruction fidelity and computational efficiency.

</details>


### [384] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: 提出Small PDE U - Net Solver (SPUS)，一种紧凑高效的基础模型用于求解偏微分方程，实验证明其参数高效且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型复杂Transformer架构的PDE基础模型存在计算和参数开销大的问题，需要更高效的模型。

Method: 采用轻量级基于残差U - Net的架构，使用自回归预训练策略模拟数值求解器行为学习物理原理。

Result: 在多种下游偏微分方程任务上达到了最先进的泛化性能，所需参数显著减少，微调数据极少。

Conclusion: SPUS作为参数高效的基础模型，在求解不同PDE系统方面具有潜力。

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


### [385] [GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings](https://arxiv.org/abs/2510.01448)
*Angel Daruna,Nicholas Meegan,Han-Pang Chiu,Supun Samarasekera,Rakesh Kumar*

Main category: cs.CV

TL;DR: 本文将地理定位问题表述为查询图像视觉表示与地理表示对齐，提出新地理表示和视觉特征融合方法，在多基准数据集上超SOTA和LVLMs。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉地理定位在学习地理表示方面取得进展，但仍是活跃研究课题。

Method: 将地理定位表述为对齐查询图像视觉表示与地理表示，提出新的地理层次嵌入表示，引入融合外观特征与语义分割图的方法。

Result: 在五个基准数据集的25个指标中的22个上取得了比现有SOTA方法和LVLMs更好的结果。

Conclusion: 性能提升主要源于地理和视觉表示的结合。

Abstract: Worldwide visual geo-localization seeks to determine the geographic location
of an image anywhere on Earth using only its visual content. Learned
representations of geography for visual geo-localization remain an active
research topic despite much progress. We formulate geo-localization as aligning
the visual representation of the query image with a learned geographic
representation. Our novel geographic representation explicitly models the world
as a hierarchy of geographic embeddings. Additionally, we introduce an approach
to efficiently fuse the appearance features of the query image with its
semantic segmentation map, forming a robust visual representation. Our main
experiments demonstrate improved all-time bests in 22 out of 25 metrics
measured across five benchmark datasets compared to prior state-of-the-art
(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional
ablation studies support the claim that these gains are primarily driven by the
combination of geographic and visual representations.

</details>


### [386] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*Răzvan-Andrei Matişan,Vincent Tao Hu,Grigory Bartosh,Björn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: 介绍用于向量量化图像生成的Purrception方法，结合连续与离散特性，在ImageNet - 1k上训练收敛快且FID分数有竞争力。


<details>
  <summary>Details</summary>
Motivation: 在向量量化图像生成中提供显式分类监督并保持连续传输动态。

Method: 将变分流匹配方法应用于向量量化潜在空间，学习码本索引的分类后验，在连续嵌入空间计算速度场。

Result: 在ImageNet - 1k 256x256图像生成任务中，训练收敛比连续和离散流匹配基线更快，FID分数与最先进模型有竞争力。

Conclusion: 变分流匹配能有效连接连续传输和离散监督，提高图像生成训练效率。

Abstract: We introduce Purrception, a variational flow matching approach for
vector-quantized image generation that provides explicit categorical
supervision while maintaining continuous transport dynamics. Our method adapts
Variational Flow Matching to vector-quantized latents by learning categorical
posteriors over codebook indices while computing velocity fields in the
continuous embedding space. This combines the geometric awareness of continuous
methods with the discrete supervision of categorical approaches, enabling
uncertainty quantification over plausible codes and temperature-controlled
generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training
converges faster than both continuous flow matching and discrete flow matching
baselines while achieving competitive FID scores with state-of-the-art models.
This demonstrates that Variational Flow Matching can effectively bridge
continuous transport and discrete supervision for improved training efficiency
in image generation.

</details>


### [387] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: 提出统一深度学习框架从非增强CT生成合成增强CT，同时分割主动脉腔和血栓，在264例患者中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统增强CT使用的造影剂有诸多风险，现有深度学习方法存在误差累积等问题，需改进。

Method: 提出统一框架，集成条件扩散模型和多任务学习，共享编解码器参数，采用半监督训练策略。

Result: 在264例患者中评估，图像合成PSNR达25.61dB，分割的管腔和血栓Dice分数提高，临床测量误差降低。

Conclusion: 所提方法优于现有单任务和多阶段模型，能实现更好的图像合成和解剖分割。

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic
aneurysms (AAA), the required iodinated contrast agents pose significant risks,
including nephrotoxicity, patient allergies, and environmental harm. To reduce
contrast agent use, recent deep learning methods have focused on generating
synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a
multi-stage pipeline that first generates images and then performs
segmentation, which leads to error accumulation and fails to leverage shared
semantic and anatomical structures. To address this, we propose a unified deep
learning framework that generates synthetic CECT images from NCCT scans while
simultaneously segmenting the aortic lumen and thrombus. Our approach
integrates conditional diffusion models (CDM) with multi-task learning,
enabling end-to-end joint optimization of image synthesis and anatomical
segmentation. Unlike previous multitask diffusion models, our approach requires
no initial predictions (e.g., a coarse segmentation mask), shares both encoder
and decoder parameters across tasks, and employs a semi-supervised training
strategy to learn from scans with missing segmentation labels, a common
constraint in real-world clinical data. We evaluated our method on a cohort of
264 patients, where it consistently outperformed state-of-the-art single-task
and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61
dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,
it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus
Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to
more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm
from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to
nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [388] [WALT: Web Agents that Learn Tools](https://arxiv.org/abs/2510.01524)
*Viraj Prabhu,Yutong Dai,Matthew Fernandez,Jing Gu,Krithika Ramakrishnan,Yanqi Luo,Silvio Savarese,Caiming Xiong,Junnan Li,Zeyuan Chen,Ran Xu*

Main category: cs.CV

TL;DR: 介绍WALT框架，将网站潜在功能逆向工程化为可复用工具，在浏览器自动化任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: 当前Web代理方法在动态布局和长任务下易失效，而人类可通过高级操作利用网站功能，因此需要改进方法。

Method: 引入WALT框架，将网站潜在功能逆向工程化为可复用、可调用的工具。

Result: 在VisualWebArena和WebArena上，WALT以更少步骤和更少依赖大语言模型推理取得更高成功率。

Conclusion: WALT为浏览器自动化建立了一个强大且可泛化的范式。

Abstract: Web agents promise to automate complex browser tasks, but current methods
remain brittle -- relying on step-by-step UI interactions and heavy LLM
reasoning that break under dynamic layouts and long horizons. Humans, by
contrast, exploit website-provided functionality through high-level operations
like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),
a framework that reverse-engineers latent website functionality into reusable
invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust
implementations of automations already designed into websites -- spanning
discovery (search, filter, sort), communication (post, comment, upvote), and
content management (create, edit, delete). Tools abstract away low-level
execution: instead of reasoning about how to click and type, agents simply call
search(query) or create(listing). This shifts the computational burden from
fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena
and WebArena, WALT achieves higher success with fewer steps and less
LLM-dependent reasoning, establishing a robust and generalizable paradigm for
browser automation.

</details>


### [389] [Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](https://arxiv.org/abs/2510.01576)
*Ricardo Gonzalez Penuela,Felipe Arias-Russi,Victor Capriles*

Main category: cs.CV

TL;DR: 为解决多模态大语言模型在视觉解释应用中提供信息缺乏上下文相关性的问题，开发利用历史问题的系统，评估显示该系统生成的描述更能满足用户需求。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在视觉解释应用中默认提供冗长描述，缺乏上下文相关性，导致信息交换效率低。

Method: 开发一个利用历史盲人和低视力用户问题的系统，从VizWiz - LF数据集中识别相似视觉上下文，引导多模态大语言模型生成更相关描述。

Result: 三位人类标注员评估显示，上下文感知描述在76.1%的情况下能预见并回答用户问题，在54.4%的比较中更受青睐。

Conclusion: 所开发的系统能有效为盲人和低视力用户提供更具上下文相关性的视觉描述信息。

Abstract: Multimodal large language models (MLLMs) have been integrated into visual
interpretation applications to support Blind and Low Vision (BLV) users because
of their accuracy and ability to provide rich, human-like interpretations.
However, these applications often default to comprehensive, lengthy
descriptions regardless of context. This leads to inefficient exchanges, as
users must go through irrelevant details rather than receiving the specific
information they are likely to seek. To deliver more contextually-relevant
information, we developed a system that draws on historical BLV users
questions. When given an image, our system identifies similar past visual
contexts from the VizWiz-LF dataset and uses the associated questions to guide
the MLLM generate descriptions more relevant to BLV users. An evaluation with
three human labelers who revised 92 context-aware and context-free descriptions
showed that context-aware descriptions anticipated and answered users'
questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of
comparisons (50 out of 92). Our paper reviews, and data analysis are publicly
available in a Github repository at
https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

</details>


### [390] [Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning](https://arxiv.org/abs/2510.01681)
*Xuchen Li,Xuzhao Li,Jiahui Gao,Renjie Pi,Shiyu Hu,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出自适应像素推理框架，提升视觉语言模型处理细粒度视觉元素能力，实验显示性能优且减少不必要视觉操作。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型处理细粒度视觉元素有困难，且像素级信息利用存在效率问题。

Method: 提出自适应像素推理框架，先进行操作感知监督微调，再设计基于模型自身响应反馈的滚动引导强化学习框架。

Result: 在多模态推理基准测试中表现优异，如在HR - Bench 4K上准确率达73.4%，工具使用率仅20.1%，相比之前方法提高准确率并降低工具使用率66.5%。

Conclusion: 所提框架能有效解决视觉语言模型处理细粒度视觉元素的问题，提升性能并减少不必要操作。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet they
frequently struggle with tasks requiring precise understanding and handling of
fine-grained visual elements. This is mainly due to information loss during
image encoding or insufficient attention to critical regions. Recent work has
shown promise by incorporating pixel-level visual information into the
reasoning process, enabling VLMs to access high-resolution visual details
during their thought process. However, this pixel-level information is often
overused, leading to inefficiency and distraction from irrelevant visual
details. To address these challenges, we propose the first framework for
adaptive pixel reasoning that dynamically determines necessary pixel-level
operations based on the input query. Specifically, we first apply
operation-aware supervised fine-tuning to establish baseline competence in
textual reasoning and visual operations, then design a novel rollout-guided
reinforcement learning framework relying on feedback of the model's own
responses, which enables the VLM to determine when pixel operations should be
invoked based on query difficulty. Experiments on extensive multimodal
reasoning benchmarks show that our model achieves superior performance while
significantly reducing unnecessary visual operations. Impressively, our model
achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of
only 20.1\%, improving accuracy and simultaneously reducing tool usage by
66.5\% compared to the previous methods.

</details>


### [391] [Holistic Order Prediction in Natural Scenes](https://arxiv.org/abs/2510.01704)
*Pierre Musacchio,Hyunmin Lee,Jaesik Park*

Main category: cs.CV

TL;DR: 提出InstaFormer网络解决视觉模型理解实例几何问题，单前向传播输出场景实例遮挡和深度顺序，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型理解实例几何困难，专业系统依赖昂贵输入格式和推理成本。

Method: 提出InstaFormer网络，依靠对象查询和潜在掩码描述符间的交互进行整体顺序预测。

Result: 对方法进行了全面基准测试和消融实验，证明其有效性。

Conclusion: InstaFormer能有效解决视觉模型理解实例几何问题，代码开源方便使用。

Abstract: Even in controlled settings, understanding instance-wise geometries is a
challenging task for a wide range of visual models. Although specialized
systems exist, modern arts rely on expensive input formats (category labels,
binary segmentation masks) and inference costs (a quadratic amount of forward
passes). We mitigate these limitations by proposing InstaFormer, a network
capable of holistic order prediction. That is, solely given an input RGB image,
InstaFormer returns the full occlusion and depth orderings for all the
instances in the scene in a single forward pass. At its core, InstaFormer
relies on interactions between object queries and latent mask descriptors that
semantically represent the same objects while carrying complementary
information. We comprehensively benchmark and ablate our approach to highlight
its effectiveness. Our code and models are open-source and available at this
URL: https://github.com/SNU-VGILab/InstaOrder.

</details>


### [392] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: 提出PyramidStyler框架用于神经风格迁移，减少计算量并优化风格化，实现实时高质量艺术渲染。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN和transformer的神经风格迁移模型难以有效处理复杂风格和高分辨率输入。

Method: 引入带金字塔位置编码（PPE）的transformer框架，结合强化学习动态优化风格化。

Result: 在Microsoft COCO和WikiArt上训练，4000个epoch后内容损失降低62.6%，风格损失降低57.4%，推理时间1.39秒；使用强化学习后有进一步提升。

Conclusion: PyramidStyler能实现实时、高质量艺术渲染，在媒体和设计领域有广泛应用。

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based
algorithm, enabling AI-driven artistic image synthesis. However, existing CNN
and transformer-based models struggle to scale efficiently to complex styles
and high-resolution inputs. We introduce PyramidStyler, a transformer framework
with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding
that captures both local details and global context while reducing
computational load. We further incorporate reinforcement learning to
dynamically optimize stylization, accelerating convergence. Trained on
Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to
2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s
inference--and yields further improvements (content 2.03; style 0.75) with
minimal speed penalty (1.40 s) when using RL. These results demonstrate
real-time, high-quality artistic rendering, with broad applications in media
and design.

</details>


### [393] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: 提出MemoryPack和Direct Forcing解决长视频生成的长依赖和误差累积问题，提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决长视频生成中需捕获长依赖和防止自回归解码误差累积的问题。

Method: 提出MemoryPack进行动态上下文建模，引入Direct Forcing缓解误差累积。

Result: MemoryPack和Direct Forcing显著提升长视频生成的上下文一致性和可靠性。

Conclusion: MemoryPack和Direct Forcing推动自回归视频模型的实际可用性。

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [394] [Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models](https://arxiv.org/abs/2510.01914)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Yen-Ting Liu*

Main category: cs.CV

TL;DR: 本文提出用于工业常用DIP的自动化缺陷检测系统，用数字相机光学和DL模型，用ConSinGAN生成数据集，对比多种YOLO模型，YOLOv7 with ConSinGAN效果佳，还开发了SCADA系统。


<details>
  <summary>Details</summary>
Motivation: 传统工业部件缺陷检测耗时费力，给质检人员带来负担，难以管理产品质量。

Method: 使用数字相机光学和DL模型，用ConSinGAN生成合适大小数据集，研究四种YOLO模型，开发SCADA系统。

Result: YOLOv7 with ConSinGAN在准确率（95.50%）、检测时间（285 ms）上优于其他YOLO版本，远优于基于阈值的方法。

Conclusion: 所提出的自动化缺陷检测能在多种缺陷类型或缺陷数据不足的情况下轻松建立。

Abstract: Since the defect detection of conventional industry components is
time-consuming and labor-intensive, it leads to a significant burden on quality
inspection personnel and makes it difficult to manage product quality. In this
paper, we propose an automated defect detection system for the dual in-line
package (DIP) that is widely used in industry, using digital camera optics and
a deep learning (DL)-based model. The two most common defect categories of DIP
are examined: (1) surface defects, and (2) pin-leg defects. However, the lack
of defective component images leads to a challenge for detection tasks. To
solve this problem, the ConSinGAN is used to generate a suitable-sized dataset
for training and testing. Four varieties of the YOLO model are investigated
(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.
The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in
accuracy of 95.50\%, detection time of 285 ms, and is far superior to
threshold-based approaches. In addition, the supervisory control and data
acquisition (SCADA) system is developed, and the associated sensor architecture
is described. The proposed automated defect detection can be easily established
with numerous types of defects or insufficient defect data.

</details>


### [395] [Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors](https://arxiv.org/abs/2510.01934)
*Guangyao Zhai,Yue Zhou,Xinyan Deng,Lars Heckler,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: 提出Few-shot异常检测方法FoundAD，利用图像异常量与嵌入差异的关系，实验显示支持多类检测且参数少性能优。


<details>
  <summary>Details</summary>
Motivation: Few-shot异常检测中有限样本难以区分正常和异常特征，尤其是类别无关条件下，利用基础视觉编码器预训练优势解决该问题。

Method: 学习非线性投影算子到自然图像流形上，设计Few-shot异常检测器FoundAD。

Result: 支持多类检测，使用更少参数达到有竞争力的性能。

Conclusion: 该方法拓宽了基础特征视角，推动Few-shot异常检测领域发展。

Abstract: Few-shot anomaly detection streamlines and simplifies industrial safety
inspection. However, limited samples make accurate differentiation between
normal and abnormal features challenging, and even more so under
category-agnostic conditions. Large-scale pre-training of foundation visual
encoders has advanced many fields, as the enormous quantity of data helps to
learn the general distribution of normal images. We observe that the anomaly
amount in an image directly correlates with the difference in the learnt
embeddings and utilize this to design a few-shot anomaly detector termed
FoundAD. This is done by learning a nonlinear projection operator onto the
natural image manifold. The simple operator acts as an effective tool for
anomaly detection to characterize and identify out-of-distribution regions in
an image. Extensive experiments show that our approach supports multi-class
detection and achieves competitive performance while using substantially fewer
parameters than prior methods. Backed up by evaluations with multiple
foundation encoders, including fresh DINOv3, we believe this idea broadens the
perspective on foundation features and advances the field of few-shot anomaly
detection.

</details>


### [396] [Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories](https://arxiv.org/abs/2510.01454)
*Nilay Naharas,Dang Nguyen,Nesihan Bulut,Mohammadhossein Bateni,Vahab Mirrokni,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: 提出用于LVLMs数据高效指令调优的方法XMAS，可有效去除数据冗余，减少数据量并提升训练速度。


<details>
  <summary>Details</summary>
Motivation: 数据高效学习旨在消除大数据集冗余，但LVLMs的数据选择研究不足，现有方法难超随机选择。

Method: 证明指令调优中具有相似跨模态注意力矩阵的样本梯度相似，提出XMAS方法，基于小代理LVLM微调得到的注意力矩阵奇异值轨迹聚类样本，从聚类中采样平衡子集。

Result: XMAS可丢弃LLaVA - 665k数据集50%和Vision - Flan数据集85%的数据，保持LLaVA - 1.5 - 7B在10个下游基准上的性能，训练速度提升1.2倍，比最佳基线多减少30%数据。

Conclusion: XMAS是一种有效的LVLMs数据高效指令调优方法。

Abstract: Data-efficient learning aims to eliminate redundancy in large training
datasets by training models on smaller subsets of the most informative
examples. While data selection has been extensively explored for vision models
and large language models (LLMs), it remains underexplored for Large
Vision-Language Models (LVLMs). Notably, none of existing methods can
outperform random selection at different subset sizes. In this work, we propose
the first principled method for data-efficient instruction tuning of LVLMs. We
prove that examples with similar cross-modal attention matrices during
instruction tuning have similar gradients. Thus, they influence model
parameters in a similar manner and convey the same information to the model
during training. Building on this insight, we propose XMAS, which clusters
examples based on the trajectories of the top singular values of their
attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a
balanced subset from these clusters, XMAS effectively removes redundancy in
large-scale LVLM training data. Extensive experiments show that XMAS can
discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while
fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and
speeding up its training by 1.2x. This is 30% more data reduction compared to
the best baseline for LLaVA-665k. The project's website can be found at
https://bigml-cs-ucla.github.io/XMAS-project-page/.

</details>


### [397] [Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework](https://arxiv.org/abs/2510.02001)
*Nanaka Hosokawa,Ryo Takahashi,Tomoya Kitano,Yukihiro Iida,Chisako Muramatsu,Tatsuro Hayashi,Yuta Seino,Xiangrong Zhou,Takeshi Hara,Akitoshi Katsumata,Hiroshi Fujita*

Main category: cs.CV

TL;DR: 利用GPT - 4o自动生成牙颌囊肿影像报告，构建SLSO框架并验证其有效性，对比实验显示该框架提升多项输出准确性，但仍需改进。


<details>
  <summary>Details</summary>
Motivation: 利用GPT - 4o的多模态能力自动生成牙颌囊肿影像报告，提高准确性。

Method: 构建Self - correction Loop with Structured Output (SLSO)框架，对22例牙颌囊肿进行10步处理，与传统CoT方法进行对比实验。

Result: SLSO框架提高多项输出准确性，如牙位、牙齿移动、牙根吸收等，成功案例最多经5次再生可获结构化输出。

Conclusion: SLSO框架能增强阴性结果描述、抑制幻觉、提高牙位识别准确性，但对多牙广泛病变识别有限，需进一步完善。

Abstract: In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to
automatically generate jaw cyst findings on dental panoramic radiographs. To
improve accuracy, we constructed a Self-correction Loop with Structured Output
(SLSO) framework and verified its effectiveness. A 10-step process was
implemented for 22 cases of jaw cysts, including image input and analysis,
structured data generation, tooth number extraction and consistency checking,
iterative regeneration when inconsistencies were detected, and finding
generation with subsequent restructuring and consistency verification. A
comparative experiment was conducted using the conventional Chain-of-Thought
(CoT) method across seven evaluation items: transparency, internal structure,
borders, root resorption, tooth movement, relationships with other structures,
and tooth number. The results showed that the proposed SLSO framework improved
output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates
for tooth number, tooth movement, and root resorption, respectively. In the
successful cases, a consistently structured output was achieved after up to
five regenerations. Although statistical significance was not reached because
of the small size of the dataset, the overall SLSO framework enforced negative
finding descriptions, suppressed hallucinations, and improved tooth number
identification accuracy. However, the accurate identification of extensive
lesions spanning multiple teeth is limited. Nevertheless, further refinement is
required to enhance overall performance and move toward a practical finding
generation system.

</details>


### [398] [LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction](https://arxiv.org/abs/2510.02028)
*Mario Resino,Borja Pérez,Jaime Godoy,Abdulla Al-Kaff,Fernando García*

Main category: cs.CV

TL;DR: 提出3D自编码器LiLa - Net，用激光雷达点云编码特征，改进架构提升性能，有强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 从真实交通环境的激光雷达点云编码高效特征，且不使用大量资源。

Method: 提出LiLa - Net架构，利用跳跃连接概念，减少编码器层数，简化跳跃连接。

Result: 实现跳跃连接携带信息和潜在编码的有效平衡，提升重建质量且不影响性能，能重建与原交通环境无关的对象。

Conclusion: LiLa - Net能高效编码特征，有良好性能和强泛化能力。

Abstract: This work proposed a 3D autoencoder architecture, named LiLa-Net, which
encodes efficient features from real traffic environments, employing only the
LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,
equipped with Velodyne LiDAR. The system leverage skip connections concept to
improve the performance without using extensive resources as the
state-of-the-art architectures. Key changes include reducing the number of
encoder layers and simplifying the skip connections, while still producing an
efficient and representative latent space which allows to accurately
reconstruct the original point cloud. Furthermore, an effective balance has
been achieved between the information carried by the skip connections and the
latent encoding, leading to improved reconstruction quality without
compromising performance. Finally, the model demonstrates strong generalization
capabilities, successfully reconstructing objects unrelated to the original
traffic environment.

</details>


### [399] [Growing Visual Generative Capacity for Pre-Trained MLLMs](https://arxiv.org/abs/2510.01546)
*Hanyu Wang,Jiaming Han,Ziyan Yang,Qi Zhao,Shanchuan Lin,Xiangyu Yue,Abhinav Shrivastava,Zhenheng Yang,Hao Chen*

Main category: cs.CV

TL;DR: 提出纯自回归统一多模态大语言模型Bridge，结合语义与像素离散表示提升视觉生成保真度，实验表现优且训练数据和时间需求少。


<details>
  <summary>Details</summary>
Motivation: 现有构建统一多模态大语言模型的方法存在挑战，如混合方法破坏自回归范式，纯自回归方法在语义对齐和像素保真度间有权衡。

Method: 通过Mixture-of-Transformers架构为预训练视觉理解模型增加生成能力，提出语义到像素的离散表示。

Result: 在多模态基准测试中取得有竞争力或更优的结果，且所需训练数据和时间少于先前模型。

Conclusion: Bridge是一种有效的纯自回归统一多模态大语言模型。

Abstract: Multimodal large language models (MLLMs) extend the success of language
models to visual understanding, and recent efforts have sought to build unified
MLLMs that support both understanding and generation. However, constructing
such models remains challenging: hybrid approaches combine continuous
embeddings with diffusion or flow-based objectives, producing high-quality
images but breaking the autoregressive paradigm, while pure autoregressive
approaches unify text and image prediction over discrete visual tokens but
often face trade-offs between semantic alignment and pixel-level fidelity. In
this work, we present Bridge, a pure autoregressive unified MLLM that augments
pre-trained visual understanding models with generative ability through a
Mixture-of-Transformers architecture, enabling both image understanding and
generation within a single next-token prediction framework. To further improve
visual generation fidelity, we propose a semantic-to-pixel discrete
representation that integrates compact semantic tokens with fine-grained pixel
tokens, achieving strong language alignment and precise description of visual
details with only a 7.9% increase in sequence length. Extensive experiments
across diverse multimodal benchmarks demonstrate that Bridge achieves
competitive or superior results in both understanding and generation
benchmarks, while requiring less training data and reduced training time
compared to prior unified MLLMs.

</details>


### [400] [When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos](https://arxiv.org/abs/2510.02100)
*Woowon Jang,Jiwon Im,Juseung Choi,Niki Rashidian,Wesley De Neve,Utku Ozbulak*

Main category: cs.CV

TL;DR: 本文系统分析腹腔镜胆囊切除术视频中点跟踪的失败模式，比较点跟踪与分割掩码初始化的性能，给出改进建议。


<details>
  <summary>Details</summary>
Motivation: 点跟踪在复杂手术环境中的可靠性和失败情况尚不明确，需进行系统分析。

Method: 分析腹腔镜胆囊切除术视频中点跟踪的失败模式，比较点跟踪与分割掩码初始化对胆囊、抓钳和L形电灼器三种手术目标的跟踪性能。

Result: 点跟踪在手术工具跟踪上有竞争力，但在解剖目标跟踪上表现不佳，组织相似性和边界模糊会导致失败。

Conclusion: 揭示影响跟踪结果的关键因素，给出选择和放置跟踪点的建议以提高手术视频分析性能。

Abstract: Video object segmentation (VOS) models such as SAM2 offer promising zero-shot
tracking capabilities for surgical videos using minimal user input. Among the
available input types, point-based tracking offers an efficient and low-cost
alternative, yet its reliability and failure cases in complex surgical
environments are not well understood. In this work, we systematically analyze
the failure modes of point-based tracking in laparoscopic cholecystectomy
videos. Focusing on three surgical targets, the gallbladder, grasper, and
L-hook electrocautery, we compare the performance of point-based tracking with
segmentation mask initialization. Our results show that point-based tracking is
competitive for surgical tools but consistently underperforms for anatomical
targets, where tissue similarity and ambiguous boundaries lead to failure.
Through qualitative analysis, we reveal key factors influencing tracking
outcomes and provide several actionable recommendations for selecting and
placing tracking points to improve performance in surgical video analysis.

</details>


### [401] [Robust Classification of Oral Cancer with Limited Training Data](https://arxiv.org/abs/2510.01547)
*Akshay Bhagwan Sonawane,Lena D. Swamikannan,Lakshman Tamil*

Main category: cs.CV

TL;DR: 本文提出结合CNN与贝叶斯深度学习的混合模型用于小训练集口腔癌分类，在不同测试集表现良好，证明贝叶斯推理在数据稀缺环境中提升早期口腔癌诊断效果。


<details>
  <summary>Details</summary>
Motivation: 口腔癌死亡率高，早期诊断重要但面临挑战，传统深度学习模型有局限性，且需要大量数据，在数据有限环境不现实。

Method: 提出结合CNN与贝叶斯深度学习的混合模型，用变分推理通过不确定性量化提高可靠性，在智能手机拍摄的彩色照片图像上训练模型并在三个测试集评估。

Result: 在与训练数据分布相似的测试集上达94%准确率，与传统CNN相当；在真实世界照片图像数据上，泛化性优于传统CNN，不同数据集上准确率88%（传统CNN为72.94%），置信分析显示模型对分类样本的不确定性表现合理。

Conclusion: 贝叶斯推理在数据稀缺环境中能通过提高模型可靠性和泛化性，有效增强早期口腔癌诊断效果。

Abstract: Oral cancer ranks among the most prevalent cancers globally, with a
particularly high mortality rate in regions lacking adequate healthcare access.
Early diagnosis is crucial for reducing mortality; however, challenges persist
due to limited oral health programs, inadequate infrastructure, and a shortage
of healthcare practitioners. Conventional deep learning models, while
promising, often rely on point estimates, leading to overconfidence and reduced
reliability. Critically, these models require large datasets to mitigate
overfitting and ensure generalizability, an unrealistic demand in settings with
limited training data. To address these issues, we propose a hybrid model that
combines a convolutional neural network (CNN) with Bayesian deep learning for
oral cancer classification using small training sets. This approach employs
variational inference to enhance reliability through uncertainty
quantification. The model was trained on photographic color images captured by
smartphones and evaluated on three distinct test datasets. The proposed method
achieved 94% accuracy on a test dataset with a distribution similar to that of
the training data, comparable to traditional CNN performance. Notably, for
real-world photographic image data, despite limitations and variations
differing from the training dataset, the proposed model demonstrated superior
generalizability, achieving 88% accuracy on diverse datasets compared to 72.94%
for traditional CNNs, even with a smaller dataset. Confidence analysis revealed
that the model exhibits low uncertainty (high confidence) for correctly
classified samples and high uncertainty (low confidence) for misclassified
samples. These results underscore the effectiveness of Bayesian inference in
data-scarce environments in enhancing early oral cancer diagnosis by improving
model reliability and generalizability.

</details>


### [402] [ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models](https://arxiv.org/abs/2510.01582)
*Krishna Teja Chitty-Venkata,Murali Emani*

Main category: cs.CV

TL;DR: 提出多模态推理数据集ImageNet - Think，用于开发有推理能力的VLM，由两个VLMs生成，公开可用。


<details>
  <summary>Details</summary>
Motivation: 助力开发具有显式推理能力的视觉语言模型（VLMs），增进对多模态推理机制的理解。

Method: 基于ImageNet21k的250,000张图像构建，由GLM - 4.1V - 9B - Thinking和Kimi - VL - A3B - Thinking - 2506两个VLMs生成，每张图配两对思考 - 答案序列。

Result: 创建了可用于训练和评估多模态推理模型的数据集，捕捉了VLMs的逐步推理过程和最终描述性答案。

Conclusion: 该数据集有助于开发更强大的VLMs，且数据集和评估基准将公开以推动相关研究。

Abstract: We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the
development of Vision Language Models (VLMs) with explicit reasoning
capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,
providing structured thinking tokens and corresponding answers. Our synthetic
dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and
Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of
thinking-answer sequences, creating a resource for training and evaluating
multimodal reasoning models. We capture the step-by-step reasoning process of
VLMs and the final descriptive answers. Our goal with this dataset is to enable
the development of more robust VLMs while contributing to the broader
understanding of multimodal reasoning mechanisms. The dataset and evaluation
benchmarks will be publicly available to aid research in reasoning/thinking
multimodal VLMs.

</details>


### [403] [Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting](https://arxiv.org/abs/2510.02155)
*Shu Zou,Xinyu Tian,Lukas Wesemann,Fabian Waschkowski,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: 提出ASK - Hint结构化提示框架用于视频异常检测，实验显示其性能优于基线，具有可解释性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有提示过于抽象，忽略监控视频中复杂异常的细粒度人类 - 对象交互和动作语义，需要改进。

Method: 提出ASK - Hint框架，将提示组织成语义连贯的组，构建与视觉线索对齐的细粒度引导问题。

Result: 在UCF - Crime和XD - Violence数据集上，ASK - Hint持续提升AUC，性能达到最优，有可解释推理痕迹且跨数据集和VLM骨干泛化性强。

Conclusion: 提示粒度很关键，ASK - Hint是可解释视频异常检测的无训练且可泛化的新方案。

Abstract: Prompting has emerged as a practical way to adapt frozen vision-language
models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are
often overly abstract, overlooking the fine-grained human-object interactions
or action semantics that define complex anomalies in surveillance videos. We
propose ASK-Hint, a structured prompting framework that leverages
action-centric knowledge to elicit more accurate and interpretable reasoning
from frozen VLMs. Our approach organizes prompts into semantically coherent
groups (e.g. violence, property crimes, public safety) and formulates
fine-grained guiding questions that align model predictions with discriminative
visual cues. Extensive experiments on UCF-Crime and XD-Violence show that
ASK-Hint consistently improves AUC over prior baselines, achieving
state-of-the-art performance compared to both fine-tuned and training-free
methods. Beyond accuracy, our framework provides interpretable reasoning traces
towards anomaly and demonstrates strong generalization across datasets and VLM
backbones. These results highlight the critical role of prompt granularity and
establish ASK-Hint as a new training-free and generalizable solution for
explainable video anomaly detection.

</details>


### [404] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: 提出TempoControl方法，无需重新训练或额外监督，利用交叉注意力图实现视觉概念的时间对齐，在多种视频生成应用中有效。


<details>
  <summary>Details</summary>
Motivation: 现有生成视频模型缺乏细粒度时间控制，无法让用户指定特定视觉元素在生成序列中出现的时间。

Method: TempoControl利用文本到视频扩散模型的交叉注意力图，通过新颖优化方法引导概念的时间，用相关性、能量和熵三个互补原则引导注意力。

Result: TempoControl能在保证视频质量和多样性的同时实现对时间的精确控制。

Conclusion: TempoControl方法在多种视频生成应用中都有效。

Abstract: Recent advances in generative video models have enabled the creation of
high-quality videos based on natural language prompts. However, these models
frequently lack fine-grained temporal control, meaning they do not allow users
to specify when particular visual elements should appear within a generated
sequence. In this work, we introduce TempoControl, a method that allows for
temporal alignment of visual concepts during inference, without requiring
retraining or additional supervision. TempoControl utilizes cross-attention
maps, a key component of text-to-video diffusion models, to guide the timing of
concepts through a novel optimization approach. Our method steers attention
using three complementary principles: aligning its temporal shape with a
control signal (via correlation), amplifying it where visibility is needed (via
energy), and maintaining spatial focus (via entropy). TempoControl allows
precise control over timing while ensuring high video quality and diversity. We
demonstrate its effectiveness across various video generation applications,
including temporal reordering for single and multiple objects, as well as
action and audio-aligned generation.

</details>


### [405] [RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2510.02240)
*Sicheng Feng,Kaiwen Tuo,Song Wang,Lingdong Kong,Jianke Zhu,Huan Wang*

Main category: cs.CV

TL;DR: 本文针对多模态大语言模型细粒度视觉推理难题，构建ReasonMap - Plus数据集，提出RewardMap框架，实验证明其能提升模型视觉理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在细粒度视觉推理尤其是结构化信息丰富场景的空间推理存在不足，标准强化学习有稀疏奖励和优化不稳定问题。

Method: 构建ReasonMap - Plus数据集引入密集奖励信号；提出RewardMap框架，包含难度感知奖励设计和多阶段强化学习方案。

Result: RewardMap各组件带来性能提升，组合效果最佳，在6个基准测试平均提升3.47%。

Conclusion: RewardMap框架有效增强了多模态大语言模型的视觉理解和推理能力。

Abstract: Fine-grained visual reasoning remains a core challenge for multimodal large
language models (MLLMs). The recently introduced ReasonMap highlights this gap
by showing that even advanced MLLMs struggle with spatial reasoning in
structured and information-rich settings such as transit maps, a task of clear
practical and scientific importance. However, standard reinforcement learning
(RL) on such tasks is impeded by sparse rewards and unstable optimization. To
address this, we first construct ReasonMap-Plus, an extended dataset that
introduces dense reward signals through Visual Question Answering (VQA) tasks,
enabling effective cold-start training of fine-grained visual understanding
skills. Next, we propose RewardMap, a multi-stage RL framework designed to
improve both visual understanding and reasoning capabilities of MLLMs.
RewardMap incorporates two key designs. First, we introduce a difficulty-aware
reward design that incorporates detail rewards, directly tackling the sparse
rewards while providing richer supervision. Second, we propose a multi-stage RL
scheme that bootstraps training from simple perception to complex reasoning
tasks, offering a more effective cold-start strategy than conventional
Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus
demonstrate that each component of RewardMap contributes to consistent
performance gains, while their combination yields the best results. Moreover,
models trained with RewardMap achieve an average improvement of 3.47% across 6
benchmarks spanning spatial reasoning, fine-grained visual reasoning, and
general tasks beyond transit maps, underscoring enhanced visual understanding
and reasoning capabilities.

</details>


### [406] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: 提出DragFlow框架利用FLUX先验进行拖拽式图像编辑，在基准测试中超越基线。


<details>
  <summary>Details</summary>
Motivation: 早期的拖拽式图像编辑存在目标区域失真问题，现有更强的生成先验未应用于拖拽式编辑，因此提出利用FLUX先验进行编辑的框架。

Method: 引入基于区域的编辑范式，使用仿射变换进行特征监督；集成预训练的开放域个性化适配器；利用多模态大语言模型解决任务歧义；创建基于区域的拖拽基准ReD Bench。

Result: 在DragBench - DR和ReD Bench上的广泛实验表明，DragFlow超越了基于点和基于区域的基线。

Conclusion: DragFlow在拖拽式图像编辑中达到了新的技术水平，代码和数据集将公开。

Abstract: Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.

</details>


### [407] [Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities](https://arxiv.org/abs/2510.02264)
*Mario Medrano-Paredes,Carmen Fernández-González,Francisco-Javier Díaz-Pernas,Hichem Saoudi,Javier González-Alonso,Mario Martínez-Zarzuela*

Main category: cs.CV

TL;DR: 本文对比单目视频3D人体姿态估计模型和惯性测量单元（IMUs）进行人体运动评估，发现MotionAGFormer表现最佳，两种技术都可行但各有优劣。


<details>
  <summary>Details</summary>
Motivation: 机器学习和可穿戴传感器发展为实验室外人体运动捕捉分析带来新机遇，准确评估现实场景人体运动对远程医疗等领域很重要。

Method: 利用VIDIMU数据集，对比单目视频3D人体姿态估计模型和IMUs，评估深度学习框架得出的关节角度与IMU数据计算的关节角度。

Result: MotionAGFormer表现最优，两种技术都可用于实验室外运动评估，但视频和传感器方法各有成本、可及性和精度等方面的权衡。

Conclusion: 明确了现成视频模型在健康成年人运动学评估中的优势和不足，为远程医疗和患者监测提供了指导。

Abstract: Advances in machine learning and wearable sensors offer new opportunities for
capturing and analyzing human movement outside specialized laboratories.
Accurate assessment of human movement under real-world conditions is essential
for telemedicine, sports science, and rehabilitation. This preclinical
benchmark compares monocular video-based 3D human pose estimation models with
inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a
total of 13 clinically relevant daily activities which were captured using both
commodity video cameras and five IMUs. During this initial study only healthy
subjects were recorded, so results cannot be generalized to pathological
cohorts. Joint angles derived from state-of-the-art deep learning frameworks
(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA
BodyTrack) were evaluated against joint angles computed from IMU data using
OpenSim inverse kinematics following the Human3.6M dataset format with 17
keypoints. Among them, MotionAGFormer demonstrated superior performance,
achieving the lowest overall RMSE ($9.27\deg \pm 4.80\deg$) and MAE ($7.86\deg
\pm 4.18\deg$), as well as the highest Pearson correlation ($0.86 \pm 0.15$)
and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). The
results reveal that both technologies are viable for out-of-the-lab kinematic
assessment. However, they also highlight key trade-offs between video- and
sensor-based approaches including costs, accessibility, and precision. This
study clarifies where off-the-shelf video models already provide clinically
promising kinematics in healthy adults and where they lag behind IMU-based
estimates while establishing valuable guidelines for researchers and clinicians
seeking to develop robust, cost-effective, and user-friendly solutions for
telehealth and remote patient monitoring.

</details>


### [408] [microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification](https://arxiv.org/abs/2510.02270)
*Sathira Silva,Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 提出microCLIP框架，利用细粒度线索联合细化CLIP视觉和文本表示，在13个细粒度基准上平均准确率提升2.90%。


<details>
  <summary>Details</summary>
Motivation: CLIP依赖粗粒度全局特征，在细粒度分类任务中受限，先前方法忽略空间精度，需要一种对微观局部线索敏感的无监督适应方法。

Method: 提出self - training框架microCLIP，核心是TokenFusion模块中的SOAP，引入双头LLM派生分类器，开发动态知识聚合方法。

Result: 在13个细粒度基准上平均准确率有2.90%的提升。

Conclusion: 所提出的microCLIP框架能挖掘CLIP中潜在的细粒度信号，只需轻度适应就能提升性能。

Abstract: Unsupervised adaptation of CLIP-based vision-language models (VLMs) for
fine-grained image classification requires sensitivity to microscopic local
cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse
global features restricts its performance on fine-grained classification tasks.
Prior efforts inject fine-grained knowledge by aligning large language model
(LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach
overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)
within a lightweight TokenFusion module, which builds a saliency-guided
$\texttt{[FG]}$ token from patch embeddings and fuses it with the global
$\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we
introduce a two-headed LLM-derived classifier: a frozen classifier that, via
multi-view alignment, provides a stable text-based prior for pseudo-labeling,
and a learnable classifier initialized from LLM descriptions and fine-tuned
with TokenFusion. We further develop Dynamic Knowledge Aggregation, which
convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to
iteratively refine pseudo-labels. Together, these components uncover latent
fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy
gain across 13 fine-grained benchmarks while requiring only light adaptation.
Our code is available at https://github.com/sathiiii/microCLIP.

</details>


### [409] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 本文提出InPose方法解决有限传感器下姿态估计跨用户泛化差的问题，利用预训练扩散模型和似然项生成估计姿态。


<details>
  <summary>Details</summary>
Motivation: 以往基于条件扩散模型的姿态估计方法跨用户泛化能力差，因位置测量受用户身体大小影响。

Method: 将姿态估计表述为逆问题，设计能零样本泛化的算法，利用预训练扩散模型并仅以旋转测量为条件，用测量位置导出的似然项引导先验。

Result: 文中未明确提及具体结果。

Conclusion: 提出的InPose方法可针对任意用户，生成式地估计最能解释稀疏身体测量值的高可能性姿态序列。

Abstract: Pose estimation refers to tracking a human's full body posture, including
their head, torso, arms, and legs. The problem is challenging in practical
settings where the number of body sensors are limited. Past work has shown
promising results using conditional diffusion models, where the pose prediction
is conditioned on both <location, rotation> measurements from the sensors.
Unfortunately, nearly all these approaches generalize poorly across users,
primarly because location measurements are highly influenced by the body size
of the user. In this paper, we formulate pose estimation as an inverse problem
and design an algorithm capable of zero-shot generalization. Our idea utilizes
a pre-trained diffusion model and conditions it on rotational measurements
alone; the priors from this model are then guided by a likelihood term, derived
from the measured locations. Thus, given any user, our proposed InPose method
generatively estimates the highly likely sequence of poses that best explains
the sparse on-body measurements.

</details>


### [410] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 本文提出一种在长视频生成中减轻质量下降的方法，无需长视频教师监督或在长视频数据集上重新训练，能扩展视频长度并提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在长视频生成中计算成本高，基于蒸馏的方法在超出训练范围时会导致质量下降，因此需要新方法解决长视频生成的质量问题。

Method: 利用教师模型的知识，通过自生成的长视频采样片段为学生模型提供指导，避免重新计算重叠帧。

Result: 能将视频长度扩展至教师模型能力的20倍，可生成长达4分15秒的视频，在标准基准和改进基准测试中，在保真度和一致性上大幅优于基线方法。

Conclusion: 所提出的方法有效减轻了长视频生成中的质量下降问题，具有良好的性能和应用前景。

Abstract: Diffusion models have revolutionized image and video generation, achieving
unprecedented visual quality. However, their reliance on transformer
architectures incurs prohibitively high computational costs, particularly when
extending generation to long videos. Recent work has explored autoregressive
formulations for long video generation, typically by distilling from
short-horizon bidirectional teachers. Nevertheless, given that teacher models
cannot synthesize long videos, the extrapolation of student models beyond their
training horizon often leads to pronounced quality degradation, arising from
the compounding of errors within the continuous latent space. In this paper, we
propose a simple yet effective approach to mitigate quality degradation in
long-horizon video generation without requiring supervision from long-video
teachers or retraining on long video datasets. Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.
Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods. When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model. Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency. Our long-horizon
videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [411] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: 提出KineMask用于物理引导视频生成，能实现现实刚体控制等，实验显示比同类模型有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成方法难以生成物理上合理的物体交互，缺乏基于物理的控制机制。

Method: 提出两阶段训练策略，通过物体掩码逐步去除未来运动监督；将低级别运动控制与高级文本条件通过预测场景描述相结合。

Result: 在合成场景训练视频扩散模型，在真实场景中物体交互有显著改善，相比同类模型有很大提升，消融研究凸显高低级条件的互补作用。

Conclusion: KineMask在视频生成方面有较好效果，代码、模型和数据将公开。

Abstract: Recent models for video generation have achieved remarkable progress and are
now deployed in film, social media production, and advertising. Beyond their
creative potential, such models also hold promise as world simulators for
robotics and embodied decision making. Despite strong advances, however,
current approaches still struggle to generate physically plausible object
interactions and lack physics-grounded control mechanisms. To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects. Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions. We
propose a two-stage training strategy that gradually removes future motion
supervision via object masks. Using this strategy we train video diffusion
models (VDMs) on synthetic scenes of simple interactions and demonstrate
significant improvements of object interactions in real scenes. Furthermore,
KineMask integrates low-level motion control with high-level textual
conditioning via predictive scene descriptions, leading to effective support
for synthesis of complex dynamical phenomena. Extensive experiments show that
KineMask achieves strong improvements over recent models of comparable size.
Ablation studies further highlight the complementary roles of low- and
high-level conditioning in VDMs. Our code, model, and data will be made
publicly available.

</details>


### [412] [VideoNSA: Native Sparse Attention Scales Video Understanding](https://arxiv.org/abs/2510.02295)
*Enxin Song,Wenhao Chai,Shusheng Yang,Ethan Armand,Xiaojun Shan,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: 提出VideoNSA方法改进多模态语言模型视频理解，在长视频理解等任务表现佳，有四项关键发现。


<details>
  <summary>Details</summary>
Motivation: 多模态语言模型的视频理解受上下文长度限制，常错过关键帧且长时连贯性差。

Method: 将Native Sparse Attention（NSA）应用于视频 - 语言模型，通过在216K视频指令数据集上对Qwen2.5 - VL进行端到端训练得到VideoNSA，采用硬件感知混合注意力方法。

Result: 与token - 压缩和免训练稀疏基线相比，VideoNSA在长视频理解、时间推理和空间基准测试中性能提升。

Conclusion: VideoNSA可可靠扩展到128K个token，有最优全局 - 局部注意力分配，有任务依赖分支使用模式，可学习的组合稀疏注意力有助于诱导动态注意力汇聚点。

Abstract: Video understanding in multimodal language models remains limited by context
length: models often miss key transition frames and struggle to maintain
coherence across long time scales. To address this, we adapt Native Sparse
Attention (NSA) to video-language models. Our method, VideoNSA, adapts
Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We
employ a hardware-aware hybrid approach to attention, preserving dense
attention for text, while employing NSA for video. Compared to
token-compression and training-free sparse baselines, VideoNSA achieves
improved performance on long-video understanding, temporal reasoning, and
spatial benchmarks. Further ablation analysis reveals four key findings: (1)
reliable scaling to 128K tokens; (2) an optimal global-local attention
allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)
the learnable combined sparse attention help induce dynamic attention sinks.

</details>


### [413] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: 文本到图像扩散模型在固定分辨率集上训练时泛化能力差，提出无训练方法NoiseShift，可改善低分辨率图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像扩散模型在低分辨率图像生成时泛化能力差，且缺乏经济高效方案的问题。

Method: 提出NoiseShift方法，根据分辨率大小重新校准去噪器的噪声水平，无需更改模型架构和采样计划。

Result: 应用于Stable Diffusion 3、Stable Diffusion 3.5和Flux - Dev时，在LAION - COCO和CelebA数据集上FID指标有显著提升。

Conclusion: NoiseShift能有效减轻分辨率相关的伪影，提高低分辨率图像生成质量。

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often
fail to generalize, even when asked to generate images at lower resolutions
than those seen during training. High-resolution text-to-image generators are
currently unable to easily offer an out-of-the-box budget-efficient alternative
to their users who might not need high-resolution images. We identify a key
technical insight in diffusion models that when addressed can help tackle this
limitation: Noise schedulers have unequal perceptual effects across
resolutions. The same level of noise removes disproportionately more signal
from lower-resolution images than from high-resolution images, leading to a
train-test mismatch. We propose NoiseShift, a training-free method that
recalibrates the noise level of the denoiser conditioned on resolution size.
NoiseShift requires no changes to model architecture or sampling schedule and
is compatible with existing models. When applied to Stable Diffusion 3, Stable
Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly
improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and
Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by
10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results
demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent
artifacts and enhancing the quality of low-resolution image generation.

</details>


### [414] [GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.02186)
*Weijia Dou,Xu Zhang,Yi Bin,Jian Liu,Bo Peng,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: 提出GeoPurify方法，利用几何先验净化2D VLM生成的3D点特征，缓解权衡问题，数据效率高。


<details>
  <summary>Details</summary>
Motivation: 现有将2D视觉语言模型特征迁移到3D语义分割的方法存在权衡问题，主导范式无法调和2D语义与3D几何结构。

Method: 提出GeoPurify，用小的学生亲和网络利用3D自监督教师模型提取的几何先验净化2D VLM生成的3D点特征，推理时设计几何引导池化模块去噪。

Result: 在主要3D基准测试中，GeoPurify仅用约1.5%训练数据就达到或超越了现有最优性能。

Conclusion: GeoPurify有效缓解权衡问题，实现了卓越的数据效率。

Abstract: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).

</details>


### [415] [VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL](https://arxiv.org/abs/2510.02282)
*Kyoungjun Park,Yifan Yang,Juheon Yi,Shicheng Zheng,Yifei Shen,Dongqi Han,Caihua Shan,Muhammad Muaz,Lili Qiu*

Main category: cs.CV

TL;DR: 介绍VidGuard - R1视频真伪检测器，用GRPO微调MLLM，在现有基准测试中表现出色，准确率高且解释可解释，代码开源。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成视频发展，需有效检测工具降低社会风险，且检测模型要提供可解释说明以保证透明度。

Method: 引入VidGuard - R1，用GRPO微调MLLM（Qwen - VL），使用针对时间伪像和生成复杂度的两个专门奖励模型；策划含140k个视频的具有挑战性的数据集。

Result: VidGuard - R1在现有基准测试中实现了零样本的最优性能，额外训练后准确率超95%；案例研究显示其能给出精确且可解释的预测理由。

Conclusion: VidGuard - R1是一个有效的视频真伪检测器，能在保证高准确率的同时提供可解释的推理。

Abstract: With the rapid advancement of AI-generated videos, there is an urgent need
for effective detection tools to mitigate societal risks such as misinformation
and reputational harm. In addition to accurate classification, it is essential
that detection models provide interpretable explanations to ensure transparency
for regulators and end users. To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO). Our model delivers both highly accurate judgments and
insightful reasoning. We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty. We then
fine-tune Qwen-VL using GRPO with two specialized reward models that target
temporal artifacts and generation complexity. Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%. Case studies
further show that VidGuard-R1 produces precise and interpretable rationales
behind its predictions. The code is publicly available at
https://VidGuard-R1.github.io.

</details>


### [416] [Inferring Dynamic Physical Properties from Video Foundation Models](https://arxiv.org/abs/2510.02311)
*Guanqi Zhan,Xianzheng Ma,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: 研究从视频预测动态物理属性，收集数据集，探索三种推理方法，对比不同模型表现。


<details>
  <summary>Details</summary>
Motivation: 解决从视频中预测需时间信息推断的动态物理属性的任务。

Method: 收集新视频数据集；探索三种从视频推断物理属性的方法，包括使用经典计算机视觉技术的oracle方法、基于视觉提示和可训练提示向量的简单读取机制、多模态大语言模型的提示策略。

Result: 生成式或自监督训练的视频基础模型表现相近但落后于oracle方法，多模态大语言模型目前表现较差，合适提示可提升其性能。

Conclusion: 在从视频预测动态物理属性任务中，不同方法和模型有不同表现，合适提示对提升模型性能有帮助。

Abstract: We study the task of predicting dynamic physical properties from videos. More
specifically, we consider physical properties that require temporal information
to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,
and dynamic friction of an object sliding on a surface. To this end, we make
the following contributions: (i) We collect a new video dataset for each
physical property, consisting of synthetic training and testing splits, as well
as a real split for real world evaluation. (ii) We explore three ways to infer
the physical property from videos: (a) an oracle method where we supply the
visual cues that intrinsically reflect the property using classical computer
vision techniques; (b) a simple read out mechanism using a visual prompt and
trainable prompt vector for cross-attention on pre-trained video generative and
self-supervised models; and (c) prompt strategies for Multi-modal Large
Language Models (MLLMs). (iii) We show that video foundation models trained in
a generative or self-supervised manner achieve a similar performance, though
behind that of the oracle, and MLLMs are currently inferior to the other
models, though their performance can be improved through suitable prompting.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [417] [Median2Median: Zero-shot Suppression of Structured Noise in Images](https://arxiv.org/abs/2510.01666)
*Jianxu Wang,Ge Wang*

Main category: eess.IV

TL;DR: 提出零样本去噪框架Median2Median (M2M)处理结构化噪声，在模拟研究中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有图像去噪方法难以处理具有强各向异性相关性的结构化噪声，数据驱动方法泛化性有限，零样本方法仅对独立同分布噪声有效。

Method: 引入新采样策略从单个噪声输入生成伪独立子图像对，利用方向插值和广义中值滤波排除结构化伪影失真值，采用随机分配策略扩大有效采样空间并消除系统偏差。

Result: 在独立同分布噪声下与现有零样本方法表现相当，在相关噪声下表现更优。

Conclusion: M2M是处理结构化噪声的高效无数据解决方案，是超越严格独立同分布假设的有效零样本去噪的第一步。

Abstract: Image denoising is a fundamental problem in computer vision and medical
imaging. However, real-world images are often degraded by structured noise with
strong anisotropic correlations that existing methods struggle to remove. Most
data-driven approaches rely on large datasets with high-quality labels and
still suffer from limited generalizability, whereas existing zero-shot methods
avoid this limitation but remain effective only for independent and identically
distributed (i.i.d.) noise. To address this gap, we propose Median2Median
(M2M), a zero-shot denoising framework designed for structured noise. M2M
introduces a novel sampling strategy that generates pseudo-independent
sub-image pairs from a single noisy input. This strategy leverages directional
interpolation and generalized median filtering to adaptively exclude values
distorted by structured artifacts. To further enlarge the effective sampling
space and eliminate systematic bias, a randomized assignment strategy is
employed, ensuring that the sampled sub-image pairs are suitable for
Noise2Noise training. In our realistic simulation studies, M2M performs on par
with state-of-the-art zero-shot methods under i.i.d. noise, while consistently
outperforming them under correlated noise. These findings establish M2M as an
efficient, data-free solution for structured noise suppression and mark the
first step toward effective zero-shot denoising beyond the strict i.i.d.
assumption.

</details>


### [418] [SpurBreast: A Curated Dataset for Investigating Spurious Correlations in Real-world Breast MRI Classification](https://arxiv.org/abs/2510.02109)
*Jong Bum Won,Wesley De Neve,Joris Vankerschaver,Utku Ozbulak*

Main category: eess.IV

TL;DR: 提出SpurBreast乳腺MRI数据集，含虚假关联以评估其对模型性能的影响，指出模型利用虚假信号致泛化失败，还提供无虚假关联的基准数据集。


<details>
  <summary>Details</summary>
Motivation: DNNs在医学影像应用因虚假关联面临挑战，现有数据集无法系统研究该问题。

Method: 构建SpurBreast数据集，分析超100个特征，找出两个主要虚假信号，通过控制数据集分割实验。

Result: DNNs可利用虚假信号获高验证准确率，但无法泛化到无偏测试数据。

Conclusion: 提供含和不含虚假关联的数据集，方便研究人员开展相关研究，模型和数据集开源。

Abstract: Deep neural networks (DNNs) have demonstrated remarkable success in medical
imaging, yet their real-world deployment remains challenging due to spurious
correlations, where models can learn non-clinical features instead of
meaningful medical patterns. Existing medical imaging datasets are not designed
to systematically study this issue, largely due to restrictive licensing and
limited supplementary patient data. To address this gap, we introduce
SpurBreast, a curated breast MRI dataset that intentionally incorporates
spurious correlations to evaluate their impact on model performance. Analyzing
over 100 features involving patient, device, and imaging protocol, we identify
two dominant spurious signals: magnetic field strength (a global feature
influencing the entire image) and image orientation (a local feature affecting
spatial alignment). Through controlled dataset splits, we demonstrate that DNNs
can exploit these non-clinical signals, achieving high validation accuracy
while failing to generalize to unbiased test data. Alongside these two datasets
containing spurious correlations, we also provide benchmark datasets without
spurious correlations, allowing researchers to systematically investigate
clinically relevant and irrelevant features, uncertainty estimation,
adversarial robustness, and generalization strategies. Models and datasets are
available at https://github.com/utkuozbulak/spurbreast.

</details>


### [419] [Measurement-Guided Consistency Model Sampling for Inverse Problems](https://arxiv.org/abs/2510.02208)
*Amirreza Tanevardi,Pooria Abbas Rad Moghadam,Sajjad Amini*

Main category: eess.IV

TL;DR: 本文提出适用于逆问题重建的改进一致性采样方法，实验表明该方法在多个指标上有提升。


<details>
  <summary>Details</summary>
Motivation: 扩散模型采样慢，一致性模型直接应用于逆问题研究不足，需改进方法解决逆问题重建。

Method: 提出一种改进的一致性采样方法，通过与测量算子相关的测量一致性机制引导采样器的随机性。

Result: 在Fashion - MNIST和LSUN Bedroom数据集实验中，相比基线一致性采样，在多个感知和像素级指标上有持续改进。

Conclusion: 该方法能在少量步骤下实现有竞争力或更优的重建，兼具效率和对测量的保真度。

Abstract: Diffusion models have become powerful generative priors for solving inverse
imaging problems, but their reliance on slow multi-step sampling limits
practical deployment. Consistency models address this bottleneck by enabling
high-quality generation in a single or only a few steps, yet their direct
adaptation to inverse problems is underexplored. In this paper, we present a
modified consistency sampling approach tailored for inverse problem
reconstruction: the sampler's stochasticity is guided by a
measurement-consistency mechanism tied to the measurement operator, which
enforces fidelity to the acquired measurements while retaining the efficiency
of consistency-based generation. Experiments on Fashion-MNIST and LSUN Bedroom
datasets demonstrate consistent improvements in perceptual and pixel-level
metrics, including Fr\'echet Inception Distance, Kernel Inception Distance,
peak signal-to-noise ratio, and structural similarity index measure, compared
to baseline consistency sampling, yielding competitive or superior
reconstructions with only a handful of steps.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [420] [An Analysis of the New EU AI Act and A Proposed Standardization Framework for Machine Learning Fairness](https://arxiv.org/abs/2510.01281)
*Mike Teodorescu,Yongxu Sun,Haren N. Bhatia,Christos Makridis*

Main category: cs.CY

TL;DR: 欧盟AI法案存在公平性指标量化缺失和术语模糊问题，建议制定更适配的监管框架和公共系统框架评估AI公平透明性，提倡行业最佳实践标准化。


<details>
  <summary>Details</summary>
Motivation: 指出欧盟AI法案存在量化公平性指标缺失、术语模糊及缺乏道德合规透明度等问题，这些问题会带来责任风险并阻碍投资。

Method: 提出更适配的监管框架、公共系统框架评估AI公平性和透明度，倡导行业最佳实践标准化。

Result: 以自动语音识别和语音合成器为例说明提案。

Conclusion: 需要更完善的监管措施，结合行业最佳实践标准化，在满足行业细节要求的同时，避免抑制AI行业创新和投资。

Abstract: The European Union's AI Act represents a crucial step towards regulating
ethical and responsible AI systems. However, we find an absence of quantifiable
fairness metrics and the ambiguity in terminology, particularly the
interchangeable use of the keywords transparency, explainability, and
interpretability in the new EU AI Act and no reference of transparency of
ethical compliance. We argue that this ambiguity creates substantial liability
risk that would deter investment. Fairness transparency is strategically
important. We recommend a more tailored regulatory framework to enhance the new
EU AI regulation. Further-more, we propose a public system framework to assess
the fairness and transparency of AI systems. Drawing from past work, we
advocate for the standardization of industry best practices as a necessary
addition to broad regulations to achieve the level of details required in
industry, while preventing stifling innovation and investment in the AI sector.
The proposals are exemplified with the case of ASR and speech synthesizers.

</details>


### [421] [Emergent evaluation hubs in a decentralizing large language model ecosystem](https://arxiv.org/abs/2510.01286)
*Manuel Cebrian,Tomomi Kito,Raul Castro Fernandez*

Main category: cs.CY

TL;DR: 研究大语言模型及其基准的聚集模式，发现模型创建更分散，基准影响力集中，模拟揭示三种机制及影响。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型和基准这两层的聚集模式是同步还是背离。

Method: 利用斯坦福基础模型生态系统图和Evidently AI基准注册表两个代理数据，进行分析并开展基于代理的模拟。

Result: 模型创建在国家、组织、模态、许可和访问上更广泛多样，基准影响力呈集中模式，模拟揭示三种机制。

Conclusion: 集中的基准影响力支持标准化等，但也带来路径依赖等权衡。

Abstract: Large language models are proliferating, and so are the benchmarks that serve
as their common yardsticks. We ask how the agglomeration patterns of these two
layers compare: do they evolve in tandem or diverge? Drawing on two curated
proxies for the ecosystem, the Stanford Foundation-Model Ecosystem Graph and
the Evidently AI benchmark registry, we find complementary but contrasting
dynamics. Model creation has broadened across countries and organizations and
diversified in modality, licensing, and access. Benchmark influence, by
contrast, displays centralizing patterns: in the inferred
benchmark-author-institution network, the top 15% of nodes account for over 80%
of high-betweenness paths, three countries produce 83% of benchmark outputs,
and the global Gini for inferred benchmark authority reaches 0.89. An
agent-based simulation highlights three mechanisms: higher entry of new
benchmarks reduces concentration; rapid inflows can temporarily complicate
coordination in evaluation; and stronger penalties against over-fitting have
limited effect. Taken together, these results suggest that concentrated
benchmark influence functions as coordination infrastructure that supports
standardization, comparability, and reproducibility amid rising heterogeneity
in model production, while also introducing trade-offs such as path dependence,
selective visibility, and diminishing discriminative power as leaderboards
saturate.

</details>


### [422] [Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence](https://arxiv.org/abs/2510.01395)
*Myra Cheng,Cinoo Lee,Pranav Khadpe,Sunny Yu,Dyllan Han,Dan Jurafsky*

Main category: cs.CY

TL;DR: 研究揭示AI谄媚现象的普遍性和危害，发现人们偏好谄媚AI但会有不良影响，需解决激励结构问题。


<details>
  <summary>Details</summary>
Motivation: 公众和学界关注AI谄媚现象，但对其程度和影响了解少，故开展研究。

Method: 对11个先进AI模型评估谄媚程度，进行两个预注册实验，含真实人际冲突讨论。

Result: AI模型谄媚程度高，与谄媚AI互动降低人们修复人际冲突意愿、增强自我正确性信念；人们偏好谄媚AI。

Conclusion: 需明确解决激励结构问题，以缓解AI谄媚的广泛风险。

Abstract: Both the general public and academic communities have raised concerns about
sycophancy, the phenomenon of artificial intelligence (AI) excessively agreeing
with or flattering users. Yet, beyond isolated media reports of severe
consequences, like reinforcing delusions, little is known about the extent of
sycophancy or how it affects people who use AI. Here we show the pervasiveness
and harmful impacts of sycophancy when people seek advice from AI. First,
across 11 state-of-the-art AI models, we find that models are highly
sycophantic: they affirm users' actions 50% more than humans do, and they do so
even in cases where user queries mention manipulation, deception, or other
relational harms. Second, in two preregistered experiments (N = 1604),
including a live-interaction study where participants discuss a real
interpersonal conflict from their life, we find that interaction with
sycophantic AI models significantly reduced participants' willingness to take
actions to repair interpersonal conflict, while increasing their conviction of
being in the right. However, participants rated sycophantic responses as higher
quality, trusted the sycophantic AI model more, and were more willing to use it
again. This suggests that people are drawn to AI that unquestioningly validate,
even as that validation risks eroding their judgment and reducing their
inclination toward prosocial behavior. These preferences create perverse
incentives both for people to increasingly rely on sycophantic AI models and
for AI model training to favor sycophancy. Our findings highlight the necessity
of explicitly addressing this incentive structure to mitigate the widespread
risks of AI sycophancy.

</details>


### [423] [Small is Sufficient: Reducing the World AI Energy Consumption Through Model Selection](https://arxiv.org/abs/2510.01889)
*Tiago da Silva Barros,Frédéric Giroire,Ramon Aparicio-Pardo,Joanna Moulierac*

Main category: cs.CY

TL;DR: 探讨AI社区通过推理阶段的模型选择实现能源节制，研究表明该方法可显著降低AI能耗。


<details>
  <summary>Details</summary>
Motivation: 人工智能能耗和碳足迹问题凸显，需从‘越大越好’转向‘小而够用’的绿色AI模式。

Method: 系统研究AI任务，分析其流行度、模型大小和效率，研究不同任务成熟度和模型采用模式对节能的影响。

Result: 不同任务节能幅度为1% - 98%，应用模型选择可使AI能耗降低27.8%，2025年全球可节省31.9 TWh。

Conclusion: 应用模型选择方法在推理阶段进行模型选择，能显著降低AI能耗，同时保持良好效用。

Abstract: The energy consumption and carbon footprint of Artificial Intelligence (AI)
have become critical concerns due to rising costs and environmental impacts. In
response, a new trend in green AI is emerging, shifting from the "bigger is
better" paradigm, which prioritizes large models, to "small is sufficient",
emphasizing energy sobriety through smaller, more efficient models.
  We explore how the AI community can adopt energy sobriety today by focusing
on model selection during inference. Model selection consists of choosing the
most appropriate model for a given task, a simple and readily applicable
method, unlike approaches requiring new hardware or architectures. Our
hypothesis is that, as in many industrial activities, marginal utility gains
decrease with increasing model size. Thus, applying model selection can
significantly reduce energy consumption while maintaining good utility for AI
inference.
  We conduct a systematic study of AI tasks, analyzing their popularity, model
size, and efficiency. We examine how the maturity of different tasks and model
adoption patterns impact the achievable energy savings, ranging from 1% to 98%
for different tasks. Our estimates indicate that applying model selection could
reduce AI energy consumption by 27.8%, saving 31.9 TWh worldwide in 2025 -
equivalent to the annual output of five nuclear power reactors.

</details>


### [424] [The Current State of AI Bias Bounties: An Overview of Existing Programmes and Research](https://arxiv.org/abs/2510.02036)
*Sergej Kucenko,Nathaniel Dennler,Fengxiang He*

Main category: cs.CY

TL;DR: 文章调研AI偏见赏金计划，分析已有项目和相关文献，提出降低参与技术门槛和探索最佳实践转移的建议。


<details>
  <summary>Details</summary>
Motivation: 当前偏见评估方法很少与受AI系统影响的社区互动，且缺乏对偏见赏金计划的前沿综述。

Method: 搜索Google、Google Scholar、PhilPapers和IEEE Xplore，识别相关偏见赏金计划和研究出版物。

Result: 确定了五个偏见赏金计划和五篇研究出版物，所有计划由美国组织举办，为限时竞赛。

Conclusion: 降低赏金计划的技术要求很重要，未来应探索最佳实践转移，设计对弱势群体敏感且降低组织采用门槛的计划。

Abstract: Current bias evaluation methods rarely engage with communities impacted by AI
systems. Inspired by bug bounties, bias bounties have been proposed as a
reward-based method that involves communities in AI bias detection by asking
users of AI systems to report biases they encounter when interacting with such
systems. In the absence of a state-of-the-art review, this survey aimed to
identify and analyse existing AI bias bounty programmes and to present academic
literature on bias bounties. Google, Google Scholar, PhilPapers, and IEEE
Xplore were searched, and five bias bounty programmes, as well as five research
publications, were identified. All bias bounties were organised by U.S.-based
organisations as time-limited contests, with public participation in four
programmes and prize pools ranging from 7,000 to 24,000 USD. The five research
publications included a report on the application of bug bounties to
algorithmic harms, an article addressing Twitter's bias bounty, a proposal for
bias bounties as an institutional mechanism to increase AI scrutiny, a workshop
discussing bias bounties from queer perspectives, and an algorithmic framework
for bias bounties. We argue that reducing the technical requirements to enter
bounty programmes is important to include those without coding experience.
Given the limited adoption of bias bounties, future efforts should explore the
transferability of the best practices from bug bounties and examine how such
programmes can be designed to be sensitive to underrepresented groups while
lowering adoption barriers for organisations.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [425] [Evaluating New AI Cell Foundation Models on Challenging Kidney Pathology Cases Unaddressed by Previous Foundation Models](https://arxiv.org/abs/2510.01287)
*Runchen Wang,Junlin Guo,Siqi Lu,Ruining Deng,Zhengyi Lu,Yanfan Zhu,Yuechen Yang,Chongyu Qu,Yu Wang,Shilin Zhao,Catie Chang,Mitchell Wilkes,Mengmeng Yin,Haichun Yang,Yuankai Huo*

Main category: q-bio.QM

TL;DR: 研究对比新旧AI细胞基础模型在肾病理细胞核分割中的表现，发现CellViT++ [Virchow] 单模型表现最佳，融合模型显著减少分割错误。


<details>
  <summary>Details</summary>
Motivation: 准确的细胞核分割对肾病理下游任务至关重要，此前研究评估了早期AI细胞基础模型，近期模型效果不明。

Method: 在人在环评级框架下，用大规模多样的肾脏图像补丁对先进的AI细胞基础模型（2025）与2024年前的模型进行基准测试，进行基于融合的集成评估和模型一致性分析。

Result: CellViT++ [Virchow] 单模型表现最佳，融合模型“良好”预测达62.2%，“差”预测仅0.4%，解决了之前研究中多数难题。

Conclusion: 证明了AI细胞基础模型在肾病理中的潜力，提供了挑战性样本数据集以支持未来肾脏特定模型的改进。

Abstract: Accurate cell nuclei segmentation is critical for downstream tasks in kidney
pathology and remains a major challenge due to the morphological diversity and
imaging variability of renal tissues. While our prior work has evaluated
early-generation AI cell foundation models in this domain, the effectiveness of
recent cell foundation models remains unclear. In this study, we benchmark
advanced AI cell foundation models (2025), including CellViT++ variants and
Cellpose-SAM, against three widely used cell foundation models developed prior
to 2024, using a diverse large-scale set of kidney image patches within a
human-in-the-loop rating framework. We further performed fusion-based ensemble
evaluation and model agreement analysis to assess the segmentation capabilities
of the different models. Our results show that CellViT++ [Virchow] yields the
highest standalone performance with 40.3% of predictions rated as "Good" on a
curated set of 2,091 challenging samples, outperforming all prior models. In
addition, our fused model achieves 62.2% "Good" predictions and only 0.4%
"Bad", substantially reducing segmentation errors. Notably, the fusion model
(2025) successfully resolved the majority of challenging cases that remained
unaddressed in our previous study. These findings demonstrate the potential of
AI cell foundation model development in renal pathology and provide a curated
dataset of challenging samples to support future kidney-specific model
refinement.

</details>


### [426] [BioVERSE: Representation Alignment of Biomedical Modalities to LLMs for Multi-Modal Reasoning](https://arxiv.org/abs/2510.01428)
*Ching-Huei Tsou,Michal Ozery-Flato,Ella Barkan,Diwakar Mahajan,Ben Shapira*

Main category: q-bio.QM

TL;DR: 提出BIOVERSE方法统一生物医学数据与大语言模型知识，实现跨模态推理，在多任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型和生物医学基础模型处于分离嵌入空间，限制了跨模态推理。

Method: 采用两阶段方法，先通过独立训练的投影将各模态对齐到共享大语言模型空间，再用多模态数据进行指令调优。

Result: 在细胞类型注释、分子描述和蛋白质功能推理等任务中，紧凑的BIOVERSE配置超越了更大的大语言模型基线，输出比现有生物医学基础模型更丰富。

Conclusion: BIOVERSE为多模态生物医学推理奠定了基础。

Abstract: Recent advances in large language models (LLMs) and biomedical foundation
models (BioFMs) have achieved strong results in biological text reasoning,
molecular modeling, and single-cell analysis, yet they remain siloed in
disjoint embedding spaces, limiting cross-modal reasoning. We present BIOVERSE
(Biomedical Vector Embedding Realignment for Semantic Engagement), a two-stage
approach that adapts pretrained BioFMs as modality encoders and aligns them
with LLMs through lightweight, modality-specific projection layers. The
approach first aligns each modality to a shared LLM space through independently
trained projections, allowing them to interoperate naturally, and then applies
standard instruction tuning with multi-modal data to bring them together for
downstream reasoning. By unifying raw biomedical data with knowledge embedded
in LLMs, the approach enables zero-shot annotation, cross-modal question
answering, and interactive, explainable dialogue. Across tasks spanning
cell-type annotation, molecular description, and protein function reasoning,
compact BIOVERSE configurations surpass larger LLM baselines while enabling
richer, generative outputs than existing BioFMs, establishing a foundation for
principled multi-modal biomedical reasoning.

</details>


### [427] [Pharmacophore-Guided Generative Design of Novel Drug-Like Molecules](https://arxiv.org/abs/2510.01480)
*Ekaterina Podplutova,Anastasia Vepreva,Olga A. Konovalova,Vladimir Vinogradov,Dmitrii O. Shkil,Andrei Dmitrenko*

Main category: q-bio.QM

TL;DR: 提出新颖生成框架用于早期药物发现，以雌激素受体调节剂和拮抗剂为例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法中对接优化计算成本高且结果可能不准确，需新方法。

Method: 提出平衡参考化合物药效团相似性与活性分子结构多样性的生成框架，允许用户提供自定义参考集。

Result: 生成化合物对已知活性分子有高药效团保真度且有结构新颖性，对常见类药性质评估显示方法稳健且具药物相关性。

Conclusion: 该框架在功能创新和专利性上有潜力，适用于早期药物发现。

Abstract: The integration of artificial intelligence (AI) in early-stage drug discovery
offers unprecedented opportunities for exploring chemical space and
accelerating hit-to-lead optimization. However, docking optimization in
generative approaches is computationally expensive and may lead to inaccurate
results. Here, we present a novel generative framework that balances
pharmacophore similarity to reference compounds with structural diversity from
active molecules. The framework allows users to provide custom reference sets,
including FDA-approved drugs or clinical candidates, and guides the \textit{de
novo} generation of potential therapeutics. We demonstrate its applicability
through a case study targeting estrogen receptor modulators and antagonists for
breast cancer. The generated compounds maintain high pharmacophoric fidelity to
known active molecules while introducing substantial structural novelty,
suggesting strong potential for functional innovation and patentability.
Comprehensive evaluation of the generated molecules against common drug-like
properties confirms the robustness and pharmaceutical relevance of the
approach.

</details>


### [428] [MorphGen: Controllable and Morphologically Plausible Generative Cell-Imaging](https://arxiv.org/abs/2510.01298)
*Berker Demirel,Marco Fumero,Theofanis Karaletsos,Francesco Locatello*

Main category: q-bio.QM

TL;DR: 本文介绍了用于荧光显微镜的生成模型MorphGen，可跨细胞类型和扰动进行可控生成，能联合生成完整荧光通道，效果优于MorphoDiff。


<details>
  <summary>Details</summary>
Motivation: 模拟细胞对干预的反应以加速基于高内涵图像的分析，推动药物发现和基因编辑。

Method: 引入MorphGen模型，用对齐损失训练使其表示与OpenPhenom的表型嵌入匹配，联合生成完整荧光通道。

Result: 通过CellProfiler特征证明与真实图像具有生物学一致性，FID分数比MorphoDiff低超35%。

Conclusion: MorphGen是先进的扩散生成模型，能更好地用于荧光显微镜图像生成和生物学分析。

Abstract: Simulating in silico cellular responses to interventions is a promising
direction to accelerate high-content image-based assays, critical for advancing
drug discovery and gene editing. To support this, we introduce MorphGen, a
state-of-the-art diffusion-based generative model for fluorescent microscopy
that enables controllable generation across multiple cell types and
perturbations. To capture biologically meaningful patterns consistent with
known cellular morphologies, MorphGen is trained with an alignment loss to
match its representations to the phenotypic embeddings of OpenPhenom, a
state-of-the-art biological foundation model. Unlike prior approaches that
compress multichannel stains into RGB images -- thus sacrificing
organelle-specific detail -- MorphGen generates the complete set of fluorescent
channels jointly, preserving per-organelle structures and enabling a
fine-grained morphological analysis that is essential for biological
interpretation. We demonstrate biological consistency with real images via
CellProfiler features, and MorphGen attains an FID score over $35\%$ lower than
the prior state-of-the-art MorphoDiff, which only generates RGB images for a
single cell type. Code is available at https://github.com/czi-ai/MorphGen.

</details>


### [429] [BioinfoMCP: A Unified Platform Enabling MCP Interfaces in Agentic Bioinformatics](https://arxiv.org/abs/2510.02139)
*Florensia Widjaja,Zhangtianyi Chen,Juexiao Zhou*

Main category: q-bio.QM

TL;DR: 提出BioinfoMCP平台解决生物信息学工具与AI代理框架集成难题，经验证多数工具能在AI平台执行复杂工作流，降低AI自动化技术门槛。


<details>
  <summary>Details</summary>
Motivation: 生物信息学工具与新兴AI代理框架集成存在接口不兼容等问题，手动将工具转换为MCP兼容服务器工作量大。

Method: 构建BioinfoMCP平台，包含用大语言模型根据工具文档自动生成MCP服务器的BioinfoMCP Converter和验证转换工具可靠性与通用性的BioinfoMCP Benchmark。

Result: 搭建了含38个MCP转换生物信息学工具的平台，94.7%的工具能在三个常用AI代理平台成功执行复杂工作流。

Conclusion: BioinfoMCP消除AI自动化技术障碍，实现自然语言交互生物信息分析，为计算生物学提供可扩展途径。

Abstract: Bioinformatics tools are essential for complex computational biology tasks,
yet their integration with emerging AI-agent frameworks is hindered by
incompatible interfaces, heterogeneous input-output formats, and inconsistent
parameter conventions. The Model Context Protocol (MCP) provides a standardized
framework for tool-AI communication, but manually converting hundreds of
existing and rapidly growing specialized bioinformatics tools into
MCP-compliant servers is labor-intensive and unsustainable. Here, we present
BioinfoMCP, a unified platform comprising two components: BioinfoMCP Converter,
which automatically generates robust MCP servers from tool documentation using
large language models, and BioinfoMCP Benchmark, which systematically validates
the reliability and versatility of converted tools across diverse computational
tasks. We present a platform of 38 MCP-converted bioinformatics tools,
extensively validated to show that 94.7% successfully executed complex
workflows across three widely used AI-agent platforms. By removing technical
barriers to AI automation, BioinfoMCP enables natural-language interaction with
sophisticated bioinformatics analyses without requiring extensive programming
expertise, offering a scalable path to intelligent, interoperable computational
biology.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [430] [Combining complex Langevin dynamics with score-based and energy-based diffusion models](https://arxiv.org/abs/2510.01328)
*Gert Aarts,Diaa E. Habibi,Lingxiao Wang,Kai Zhou*

Main category: hep-lat

TL;DR: 探讨扩散模型学习复朗之万过程采样分布的能力及应用可能性


<details>
  <summary>Details</summary>
Motivation: 复朗之万过程有效采样的概率分布难以先验得知且难以理解，而生成式AI中扩散模型可从数据学习分布或其对数导数

Method: 比较基于分数和基于能量的扩散模型

Result: 未提及具体结果

Conclusion: 未提及明确结论，仅对可能应用进行推测

Abstract: Theories with a sign problem due to a complex action or Boltzmann weight can
sometimes be numerically solved using a stochastic process in the complexified
configuration space. However, the probability distribution effectively sampled
by this complex Langevin process is not known a priori and notoriously hard to
understand. In generative AI, diffusion models can learn distributions, or
their log derivatives, from data. We explore the ability of diffusion models to
learn the distributions sampled by a complex Langevin process, comparing
score-based and energy-based diffusion models, and speculate about possible
applications.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [431] [Reducing Simulation Dependence in Neutrino Telescopes with Masked Point Transformers](https://arxiv.org/abs/2510.01733)
*Felix J. Yu,Nicholas Kamp,Carlos A. Argüelles*

Main category: hep-ex

TL;DR: 提出首个中微子望远镜自监督训练管道，减少对模拟数据依赖，有望改善事件重建和分类。


<details>
  <summary>Details</summary>
Motivation: 传统中微子物理机器学习依赖模拟数据，模拟精度和与真实数据的差异是重要问题，自监督学习可减少对标签数据集的依赖。

Method: 利用点云变换器和掩码自编码器构建自监督训练管道，将大部分训练转移到真实数据上。

Result: 该方法减少了对模拟的依赖，减轻了相关系统不确定性。

Conclusion: 此方法与以往中微子望远镜的机器学习应用有根本不同，为事件重建和分类带来显著改进。

Abstract: Machine learning techniques in neutrino physics have traditionally relied on
simulated data, which provides access to ground-truth labels. However, the
accuracy of these simulations and the discrepancies between simulated and real
data remain significant concerns, particularly for large-scale neutrino
telescopes that operate in complex natural media. In recent years,
self-supervised learning has emerged as a powerful paradigm for reducing
dependence on labeled datasets. Here, we present the first self-supervised
training pipeline for neutrino telescopes, leveraging point cloud transformers
and masked autoencoders. By shifting the majority of training to real data,
this approach minimizes reliance on simulations, thereby mitigating associated
systematic uncertainties. This represents a fundamental departure from previous
machine learning applications in neutrino telescopes, paving the way for
substantial improvements in event reconstruction and classification.

</details>
