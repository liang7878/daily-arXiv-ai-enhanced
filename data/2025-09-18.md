<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 55]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 16]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 24]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.MS](#cs.MS) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 13]
- [math.OC](#math.OC) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.CL](#cs.CL) [Total: 19]
- [astro-ph.IM](#astro-ph.IM) [Total: 2]
- [cs.HC](#cs.HC) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 12]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.SY](#cs.SY) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [eess.SY](#eess.SY) [Total: 4]
- [econ.GN](#econ.GN) [Total: 6]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.AR](#cs.AR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 5]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [eess.AS](#eess.AS) [Total: 3]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Programmable Cognitive Bias in Social Agents](https://arxiv.org/abs/2509.13588)
*Xuan Liu,Haoyang Shang,Haojian Jin*

Main category: cs.AI

TL;DR: 本文介绍了用于基于大语言模型的社会模拟中系统指定代理行为的工具包CoBRA，评估表明它能以模型无关的方式精确编程社会代理的认知偏差。


<details>
  <summary>Details</summary>
Motivation: 传统通过隐式自然语言描述指定代理行为的方法无法在不同模型间产生一致行为，且不能体现描述的细微差别。

Method: 提出CoBRA工具包，包含衡量社会代理认知偏差的认知偏差指数和使代理行为表现出受控认知偏差的行为调节引擎。

Result: 通过演示和技术基准评估，结果显示CoBRA能以模型无关的方式精确编程社会代理的认知偏差。

Conclusion: CoBRA可有效用于基于大语言模型的社会模拟中对代理行为的系统指定。

Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying
agent behavior in LLM-based social simulation. We found that conventional
approaches that specify agent behaviors through implicit natural language
descriptions cannot yield consistent behaviors across models, and the produced
agent behaviors do not capture the nuances of the descriptions. In contrast,
CoBRA presents a new approach to program agents' cognitive biases explicitly,
by grounding agents' expected behaviors using classic social science
experiments. CoBRA has two components: (1) Cognitive Bias Index that measures
the cognitive bias of a social agent, by quantifying the agent's reactions in a
set of validated classical social science experiments; (2) Behavioral
Regulation Engine that aligns the agent's behavior to demonstrate controlled
cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and
technical benchmarks. Our results suggest that CoBRA can precisely program the
cognitive bias demonstrated in a social agent in a model-agnostic manner.

</details>


### [2] [Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness](https://arxiv.org/abs/2509.13332)
*Pratik Jayarao,Himanshu Gupta,Neeraj Varshney,Chaitanya Dwivedi*

Main category: cs.AI

TL;DR: 本文用开源小尺寸Qwen 3模型系统比较了思维和非思维大语言模型在作为评判者时的表现，发现思维模型在准确性、效率和鲁棒性上更优。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地被用作自动评判者，确保其可靠性、效率和鲁棒性至关重要。

Method: 使用开源小尺寸Qwen 3模型，在RewardBench任务上评估准确性和计算效率，研究非思维模型的增强策略，进行偏差和鲁棒性分析，并扩展到多语言场景。

Result: 非思维模型即便有增强策略仍不如思维模型，思维模型准确率高约10%且开销小，在多种偏差条件下更一致，多语言场景中显式推理也有益。

Conclusion: 显式推理在大语言模型作为评判者的范式中，在准确性、效率和鲁棒性方面都有明显优势。

Abstract: As Large Language Models (LLMs) are increasingly adopted as automated judges
in benchmarking and reward modeling, ensuring their reliability, efficiency,
and robustness has become critical. In this work, we present a systematic
comparison of "thinking" and "non-thinking" LLMs in the LLM-as-a-judge paradigm
using open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B
parameters). We evaluate both accuracy and computational efficiency (FLOPs) on
RewardBench tasks, and further examine augmentation strategies for non-thinking
models, including in-context learning, rubric-guided judging, reference-based
evaluation, and n-best aggregation. Our results show that despite these
enhancements, non-thinking models generally fall short of their thinking
counterparts. Our results show that thinking models achieve approximately 10%
points higher accuracy with little overhead (under 2x), in contrast to
augmentation strategies like few-shot learning, which deliver modest gains at a
higher cost (>8x). Bias and robustness analyses further demonstrate that
thinking models maintain significantly greater consistency under a variety of
bias conditions such as positional, bandwagon, identity, diversity, and random
biases (6% higher on average). We further extend our experiments to the
multilingual setting and our results confirm that explicit reasoning extends
its benefits beyond English. Overall, our work results in several important
findings that provide systematic evidence that explicit reasoning offers clear
advantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency
but also in robustness.

</details>


### [3] [Evaluation Awareness Scales Predictably in Open-Weights Large Language Models](https://arxiv.org/abs/2509.13333)
*Maheep Chaudhary,Ian Su,Nikhil Hooda,Nishith Shankar,Julia Tan,Kevin Zhu,Ashwinee Panda,Ryan Lagasse,Vasu Sharma*

Main category: cs.AI

TL;DR: 研究15个不同参数规模模型的评估意识，发现评估意识与模型大小呈幂律缩放关系，可用于预测大模型欺骗行为和指导评估策略设计。


<details>
  <summary>Details</summary>
Motivation: 先前仅在单一70B模型中证明评估意识，不同规模模型的缩放关系未知，这会影响AI安全评估。

Method: 使用线性探测方法对四个系列15个从0.27B到70B参数的模型的转向向量激活进行研究。

Result: 评估意识与模型大小呈明显的幂律缩放关系，即评估意识随模型大小可预测地增加。

Conclusion: 该缩放定律可用于预测未来更大模型的欺骗行为，并指导设计考虑规模的AI安全评估策略。

Abstract: Large language models (LLMs) can internally distinguish between evaluation
and deployment contexts, a behaviour known as \emph{evaluation awareness}. This
undermines AI safety evaluations, as models may conceal dangerous capabilities
during testing. Prior work demonstrated this in a single $70$B model, but the
scaling relationship across model sizes remains unknown. We investigate
evaluation awareness across $15$ models scaling from $0.27$B to $70$B
parameters from four families using linear probing on steering vector
activations. Our results reveal a clear power-law scaling: evaluation awareness
increases predictably with model size. This scaling law enables forecasting
deceptive behavior in future larger models and guides the design of scale-aware
evaluation strategies for AI safety. A link to the implementation of this paper
can be found at
https://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.

</details>


### [4] [FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness](https://arxiv.org/abs/2509.13334)
*Anand Swaroop,Akshat Nallani,Saksham Uboweja,Adiliia Uzdenova,Michael Nguyen,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.AI

TL;DR: 提出FRIT方法提升大语言模型推理的忠实性和准确性，在多模型和任务上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有CoT推理结果不可靠，且提升推理忠实性的方法有限。

Method: 引入FRIT方法，通过干预模型生成的推理步骤生成合成训练数据，用直接偏好优化让模型选择因果一致推理路径。

Result: 在Qwen3 - 8B和Mistral - 7B - v0.1上评估，FRIT使Mistral在GSM8K上忠实推理提升3.4个百分点，准确率提升7.6个百分点。

Conclusion: FRIT是首个可扩展、无监督训练语言模型生成更可靠和可解释推理的方法，弥补推理性能和可信度间的差距。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving
large language model performance on complex tasks, but recent work shows that
reasoning steps often fail to causally influence the final answer, creating
brittle and untrustworthy outputs. Prior approaches focus primarily on
measuring faithfulness, while methods for systematically improving it remain
limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a
scalable alignment method that trains models to produce causally consistent
reasoning by learning from systematically corrupted examples. FRIT generates
synthetic training data by intervening on individual reasoning steps in
model-generated CoTs, creating faithful/unfaithful pairs that highlight when
reasoning breaks down. We then apply Direct Preference Optimization to teach
models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B
and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases
faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while
improving accuracy by $7.6$ percentage points. Our approach provides the first
scalable, supervision-free method for training language models to produce more
reliable and interpretable reasoning, addressing a critical gap between
reasoning performance and trustworthiness. We release our code at
\href{https://github.com/Anut-py/frit}.

</details>


### [5] [Position: AI Safety Must Embrace an Antifragile Perspective](https://arxiv.org/abs/2509.13339)
*Ming Jin,Hyunin Lee*

Main category: cs.AI

TL;DR: 论文主张现代AI研究应采用抗脆弱视角保障长期AI安全，指出静态测试局限并探索抗脆弱解决方案，倡导重新校准评估和提升AI安全的方法。


<details>
  <summary>Details</summary>
Motivation: 传统静态基准和单次鲁棒性测试忽视环境变化，可能导致模型不适应，需要新视角保障长期AI安全。

Method: 先识别静态测试的关键局限，再探索抗脆弱解决方案应对罕见事件，最后倡导重新校准衡量、基准和持续改进AI安全的方法并提供相关指南。

Result: 明确指出静态测试的问题，探索了抗脆弱解决方案的潜力。

Conclusion: 抗脆弱方法对开放式机器学习系统的长期可靠性至关重要，应建立抗脆弱的AI安全社区。

Abstract: This position paper contends that modern AI research must adopt an
antifragile perspective on safety -- one in which the system's capacity to
guarantee long-term AI safety such as handling rare or out-of-distribution
(OOD) events expands over time. Conventional static benchmarks and single-shot
robustness tests overlook the reality that environments evolve and that models,
if left unchallenged, can drift into maladaptation (e.g., reward hacking,
over-optimization, or atrophy of broader capabilities). We argue that an
antifragile approach -- Rather than striving to rapidly reduce current
uncertainties, the emphasis is on leveraging those uncertainties to better
prepare for potentially greater, more unpredictable uncertainties in the future
-- is pivotal for the long-term reliability of open-ended ML systems. In this
position paper, we first identify key limitations of static testing, including
scenario diversity, reward hacking, and over-alignment. We then explore the
potential of antifragile solutions to manage rare events. Crucially, we
advocate for a fundamental recalibration of the methods used to measure,
benchmark, and continually improve AI safety over the long term, complementing
existing robustness approaches by providing ethical and practical guidelines
towards fostering an antifragile AI safety community.

</details>


### [6] [Imagined Autocurricula](https://arxiv.org/abs/2509.13341)
*Ahmet H. Güzel,Matthew Thomas Jackson,Jarek Luca Liesen,Tim Rocktäschel,Jakob Nicolaus Foerster,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.AI

TL;DR: 利用世界模型生成想象环境训练鲁棒代理，提出IMAC方法，在程序生成环境中实现强迁移性能。


<details>
  <summary>Details</summary>
Motivation: 训练具身环境中的代理通常需大量数据或精确模拟，现实中很多情况不具备，因此利用世界模型生成多样世界来训练代理。

Method: 提出IMAC（Imagined Autocurricula）方法，利用无监督环境设计（UED）在生成的世界上引入自动课程。

Result: 在一系列具有挑战性的程序生成环境中，仅在从较窄数据集学习的世界模型内训练，就能在保留环境中实现强迁移性能。

Conclusion: 这为利用更大规模的基础世界模型训练通用能力的代理开辟了道路。

Abstract: Training agents to act in embodied environments typically requires vast
training data or access to accurate simulation, neither of which exists for
many cases in the real world. Instead, world models are emerging as an
alternative leveraging offline, passively collected data, they make it possible
to generate diverse worlds for training agents in simulation. In this work, we
harness world models to generate imagined environments to train robust agents
capable of generalizing to novel task variations. One of the challenges in
doing this is ensuring the agent trains on useful generated data. We thus
propose a novel approach, IMAC (Imagined Autocurricula), leveraging
Unsupervised Environment Design (UED), which induces an automatic curriculum
over generated worlds. In a series of challenging, procedurally generated
environments, we show it is possible to achieve strong transfer performance on
held-out environments, having trained only inside a world model learned from a
narrower dataset. We believe this opens the path to utilizing larger-scale,
foundation world models for generally capable agents.

</details>


### [7] [OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft](https://arxiv.org/abs/2509.13347)
*Zihao Wang,Muyao Li,Kaichen He,Xiangyu Wang,Zhancun Mu,Anji Liu,Yitao Liang*

Main category: cs.AI

TL;DR: 本文对比了VLA或分层代理模型的动作空间，发现无通用最优动作空间，提出CoA框架，用其训练的一体化代理取得新SOTA，还发布OpenHA套件。


<details>
  <summary>Details</summary>
Motivation: 动作空间选择是开发端到端可训练代理的关键未决挑战，不同任务缺乏通用最优动作空间，难以构建通用代理。

Method: 先对突出的抽象动作空间和分词器进行大规模系统比较，再引入CoA框架统一高级规划和低级控制，用CoA范式在不同动作空间混合数据上训练一体化代理。

Result: 用CoA范式训练的一体化代理取得新的最优成果，提高了整体任务成功率。

Conclusion: CoA框架有效，可使代理学习到更鲁棒和可泛化的策略，同时发布OpenHA套件促进可复现研究。

Abstract: The choice of action spaces is a critical yet unresolved challenge in
developing capable, end-to-end trainable agents. This paper first presents a
large-scale, systematic comparison of prominent abstracted action spaces and
tokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the
open-ended Minecraft. Our analysis reveals that no single action space is
universally optimal; instead, the most effective abstraction is highly
task-dependent, creating a dilemma for building generalist agents. To resolve
this, we introduce Chain of Action (CoA), a novel framework that unifies
high-level planning and low-level control within a single, monolithic VLA
model. CoA treats an abstracted action not as a command for a separate policy,
but as an intermediate reasoning step--akin to a chain of thought--that guides
the generation of the final, executable action. Furthermore, we demonstrate
that an All-in-One agent trained on a diverse mixture of action spaces using
the CoA paradigm learns a more robust and generalizable policy. This unified
agent achieves a new state-of-the-art, improving the overall task success rate
over strong, specialized baselines. To foster reproducible research, we release
the OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive
benchmark of over 800 distinct tasks, curated datasets, source code, and all
pretrained model checkpoints at https://github.com/CraftJarvis/OpenHA

</details>


### [8] [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)
*Pulkit Verma,Ngoc La,Anthony Favier,Swaroop Mishra,Julie A. Shah*

Main category: cs.AI

TL;DR: 提出PDDL - Instruct框架提升大语言模型符号规划能力，实验效果好，为AI规划系统发展提供方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在结构化符号规划能力有限，特别是在需要形式化表示的领域，如PDDL。

Method: 提出PDDL - Instruct指令调优框架，通过逻辑思维链推理，引导模型进行精确逻辑推理，将规划过程分解为明确推理链以构建验证技能。

Result: 在多个规划领域实验中，基于思维链推理的指令调优模型规划能力显著提升，在标准基准上规划准确率达94%，比基线模型绝对提高66%。

Conclusion: 该工作弥合了大语言模型一般推理能力与自动规划所需逻辑精度之间的差距，为开发更好的AI规划系统提供了有前景的方向。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, yet their ability to perform structured symbolic planning
remains limited, particularly in domains requiring formal representations like
the Planning Domain Definition Language (PDDL). In this paper, we present a
novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'
symbolic planning capabilities through logical chain-of-thought reasoning. Our
approach focuses on teaching models to rigorously reason about action
applicability, state transitions, and plan validity using explicit logical
inference steps. By developing instruction prompts that guide models through
the precise logical reasoning required to determine when actions can be applied
in a given state, we enable LLMs to self-correct their planning processes
through structured reflection. The framework systematically builds verification
skills by decomposing the planning process into explicit reasoning chains about
precondition satisfaction, effect application, and invariant preservation.
Experimental results on multiple planning domains show that our
chain-of-thought reasoning based instruction-tuned models are significantly
better at planning, achieving planning accuracy of up to 94% on standard
benchmarks, representing a 66% absolute improvement over baseline models. This
work bridges the gap between the general reasoning capabilities of LLMs and the
logical precision required for automated planning, offering a promising
direction for developing better AI planning systems.

</details>


### [9] [Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning](https://arxiv.org/abs/2509.13352)
*Anis Koubaa,Khaled Gabr*

Main category: cs.AI

TL;DR: 现有无人机自主性受限，本文提出Agentic UAVs框架，经模拟测试提升了自主性和集成度。


<details>
  <summary>Details</summary>
Motivation: 现有无人机系统多处于SAE 2 - 3级自主，依赖规则控制和狭义AI，缺乏上下文感知推理等能力，且未利用大语言模型代理与工具调用进行实时知识访问。

Method: 引入五层架构的Agentic UAVs框架，用ROS2和Gazebo构建原型，集成YOLOv11、GPT - 4和本地Gemma - 3。

Result: 在模拟搜索救援场景中，无人机检测置信度、人员检测率和行动推荐率均有显著提升。

Conclusion: 适度的计算开销能带来新的自主性和生态系统集成水平。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,
surveillance, and disaster response, yet most systems remain confined to SAE
Level 2--3 autonomy. Their reliance on rule-based control and narrow AI
restricts adaptability in dynamic, uncertain missions. Existing UAV frameworks
lack context-aware reasoning, autonomous decision-making, and ecosystem-level
integration; critically, none leverage Large Language Model (LLM) agents with
tool-calling for real-time knowledge access. This paper introduces the Agentic
UAVs framework, a five-layer architecture (Perception, Reasoning, Action,
Integration, Learning) that augments UAVs with LLM-driven reasoning, database
querying, and third-party system interaction. A ROS2 and Gazebo-based prototype
integrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3
deployment. In simulated search-and-rescue scenarios, agentic UAVs achieved
higher detection confidence (0.79 vs. 0.72), improved person detection rates
(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).
These results confirm that modest computational overhead enables qualitatively
new levels of autonomy and ecosystem integration.

</details>


### [10] [MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation](https://arxiv.org/abs/2509.13773)
*Zhipeng Bian,Jieming Zhu,Xuyang Xie,Quanyu Dai,Zhou Zhao,Zhenhua Dong*

Main category: cs.AI

TL;DR: 本文介绍了用于智能手机任务指令推荐的MIRA框架，有三项关键创新，评估显示其能提升指令推荐准确性，改善用户与AI服务交互体验。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术发展，为简化用户访问预定义AI服务，实现智能手机上直观的一键式AI任务操作。

Method: 引入基于多模态大语言模型的推荐管道、模板增强推理机制和基于前缀树的受限解码策略。

Result: 通过真实世界标注数据集评估和用户研究，MIRA在指令推荐准确性上有显著提升。

Conclusion: MIRA有潜力革新用户在智能手机上与AI服务的交互方式，提供更无缝高效的体验。

Abstract: The rapid advancement of generative AI technologies is driving the
integration of diverse AI-powered services into smartphones, transforming how
users interact with their devices. To simplify access to predefined AI
services, this paper introduces MIRA, a pioneering framework for task
instruction recommendation that enables intuitive one-touch AI tasking on
smartphones. With MIRA, users can long-press on images or text objects to
receive contextually relevant instruction recommendations for executing AI
tasks. Our work introduces three key innovations: 1) A multimodal large
language model (MLLM)-based recommendation pipeline with structured reasoning
to extract key entities, infer user intent, and generate precise instructions;
2) A template-augmented reasoning mechanism that integrates high-level
reasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based
constrained decoding strategy that restricts outputs to predefined instruction
candidates, ensuring coherent and intent-aligned suggestions. Through
evaluation using a real-world annotated datasets and a user study, MIRA has
demonstrated substantial improvements in the accuracy of instruction
recommendation. The encouraging results highlight MIRA's potential to
revolutionize the way users engage with AI services on their smartphones,
offering a more seamless and efficient experience.

</details>


### [11] [Semantic Fusion with Fuzzy-Membership Features for Controllable Language Modelling](https://arxiv.org/abs/2509.13357)
*Yongchao Huang,Hassan Raza*

Main category: cs.AI

TL;DR: 提出语义融合方案增强Transformer语言模型，在合成语料上提升效果且有诸多优点。


<details>
  <summary>Details</summary>
Motivation: 为Transformer语言模型增强语义编码能力，实现可控的自然语言生成。

Method: 提出语义融合方案，用可解释特征向量表示每个token，形成语义矩阵通过门控适配器融合到语言模型，训练采用标准预测、辅助损失和轻量级统一器。

Result: 在合成语料上改善困惑度，能精确、可控地生成极性和标点，增加开销小，与输入输出嵌入兼容。

Conclusion: 语义融合方案为条件自然语言生成提供可解释途径，在保持模型简单性的同时有良好效果。

Abstract: We propose semantic fusion, a lightweight scheme that augments a Transformer
language model (LM) with a parallel, fuzzy-membership feature channel that
encodes token-level semantics. Each token is represented by a vector of
interpretable features (e.g. part-of-speech cues, shallow roles, boundary
flags, sentiment polarity and strength) whose values are graded degrees from
differentiable membership functions (e.g. power kernels). These per-token
vectors form a sentence-level semantic matrix fused via a gated adapter into
the LM. Training uses standard next-token prediction, an auxiliary loss that
reconstructs the semantic features from hidden states, and a lightweight
uniformizer that regularizes adjective-class distributions. On a synthetic
two-clause corpus with held-out adjectives for out-of-distribution (OOD)
control, semantic fusion improves perplexity and enables precise,
user-controllable generation of polarity and punctuation while maintaining
model simplicity. This approach adds only small overhead, remains fully
compatible with tied input-output embeddings, and provides an interpretable
pathway for conditioned natural language generation.

</details>


### [12] [Asterisk Operator](https://arxiv.org/abs/2509.13364)
*Zixi Li*

Main category: cs.AI

TL;DR: 提出基于ASPP的Asterisk Operator统一框架用于抽象推理，证明其特性并通过实验验证，Embedding - Asterisk蒸馏法在ARC2验证达100%准确率。


<details>
  <summary>Details</summary>
Motivation: 为抽象推理问题提供高效且收敛的计算范式，解决结构化推理任务。

Method: 提出Asterisk Operator，将结构化推理任务形式化为局部、并行状态演化过程；采用Embedding - Asterisk蒸馏方法；进行数学分析和实验。

Result: Asterisk Operator展现出普遍性、收敛性和优越性能，Embedding - Asterisk蒸馏法用600万参数在ARC2验证达100%准确率。

Conclusion: Asterisk Operator为抽象推理问题提供有效解决方案，Embedding - Asterisk蒸馏法在神经符号推理上有重大突破。

Abstract: We propose the \textbf{Asterisk Operator} ($\ast$-operator), a novel unified
framework for abstract reasoning based on Adjacency-Structured Parallel
Propagation (ASPP). The operator formalizes structured reasoning tasks as
local, parallel state evolution processes guided by implicit relational graphs.
We prove that the $\ast$-operator maintains local computational constraints
while achieving global reasoning capabilities, providing an efficient and
convergent computational paradigm for abstract reasoning problems. Through
rigorous mathematical analysis and comprehensive experiments on ARC2 challenges
and Conway's Game of Life, we demonstrate the operator's universality,
convergence properties, and superior performance. Our innovative
Embedding-Asterisk distillation method achieves 100\% accuracy on ARC2
validation with only 6M parameters, representing a significant breakthrough in
neural-symbolic reasoning.
  \textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel
Propagation, Asterisk Operator, Convergence, Universal Approximation

</details>


### [13] [$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation](https://arxiv.org/abs/2509.13368)
*Yuan Wei,Xiaohan Shan,Ran Miao,Jianmin Li*

Main category: cs.AI

TL;DR: 本文介绍了Agent^2框架，可通过大语言模型驱动实现强化学习智能体的全自动设计，实验显示其性能优于手动设计方案。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习智能体开发需要专业知识和多次迭代，失败率高且可及性有限。

Method: 提出Agent^2框架，采用双智能体架构，将开发分为MDP建模和算法优化两阶段，基于模型上下文协议，结合自适应训练管理和智能反馈分析。

Result: 在多个基准测试中，Agent^2始终优于手动设计方案，性能提升最高达55%。

Conclusion: Agent^2实现端到端闭环自动化，开创了智能体设计和优化其他智能体的新范式，是自动化AI系统的重大突破。

Abstract: Reinforcement learning agent development traditionally requires extensive
expertise and lengthy iterations, often resulting in high failure rates and
limited accessibility. This paper introduces $Agent^2$, a novel
agent-generates-agent framework that achieves fully automated RL agent design
through intelligent LLM-driven generation. The system autonomously transforms
natural language task descriptions and environment code into comprehensive,
high-performance reinforcement learning solutions without human intervention.
$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent
serves as an autonomous AI designer that analyzes tasks and generates
executable RL agents, while the Target Agent is the resulting automatically
generated RL agent. The framework decomposes RL development into two distinct
stages: MDP modeling and algorithmic optimization, enabling more targeted and
effective agent generation. Built on the Model Context Protocol, $Agent^2$
provides a unified framework that standardizes intelligent agent creation
across diverse environments and algorithms, while incorporating adaptive
training management and intelligent feedback analysis for continuous
improvement. Extensive experiments on a wide range of benchmarks, including
MuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently
outperforms manually designed solutions across all tasks, achieving up to 55%
performance improvement and substantial gains on average. By enabling truly
end-to-end, closed-loop automation, this work establishes a new paradigm in
which intelligent agents design and optimize other agents, marking a
fundamental breakthrough for automated AI systems.

</details>


### [14] [The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs](https://arxiv.org/abs/2509.13379)
*Asif Azad,Mohammad Sadat Hossain,MD Sadik Hossain Shanto,M Saifur Rahman,Md Rizwan Pervez*

Main category: cs.AI

TL;DR: 对16个VLM模型在6个多模态数据集上进行不确定性基准测试，发现大模型不确定性量化更好，更确定的模型精度更高，数学和推理任务不确定性表现差。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）性能基准测试受关注，但不确定性量化维度关注不足。

Method: 对16个最先进的VLMs（开源和闭源）在6个多模态数据集上用3种不同评分函数进行全面的不确定性基准测试。

Result: 大模型始终有更好的不确定性量化；更确定的模型精度更高；数学和推理任务中所有模型的不确定性表现比其他领域差。

Conclusion: 为多模态系统的可靠不确定性评估奠定基础。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in complex
visual understanding across scientific and reasoning tasks. While performance
benchmarking has advanced our understanding of these capabilities, the critical
dimension of uncertainty quantification has received insufficient attention.
Therefore, unlike prior conformal prediction studies that focused on limited
settings, we conduct a comprehensive uncertainty benchmarking study, evaluating
16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets
with 3 distinct scoring functions. Our findings demonstrate that larger models
consistently exhibit better uncertainty quantification; models that know more
also know better what they don't know. More certain models achieve higher
accuracy, while mathematical and reasoning tasks elicit poorer uncertainty
performance across all models compared to other domains. This work establishes
a foundation for reliable uncertainty evaluation in multimodal systems.

</details>


### [15] [From Next Token Prediction to (STRIPS) World Models -- Preliminary Results](https://arxiv.org/abs/2509.13389)
*Carlos Núñez-Molina,Vicenç Gómez,Hector Geffner*

Main category: cs.AI

TL;DR: 使用深度学习架构（transformers）和梯度下降从动作轨迹学习命题STRIPS世界模型，通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决仅从动作轨迹学习命题STRIPS世界模型的问题。

Method: 将任务转化为监督式下一个标记预测问题，利用合适的transformer架构，通过随机有效和无效动作序列学习模型。

Result: 合适的transformer架构能忠实表示命题STRIPS世界模型，且可从动作序列学习模型。

Conclusion: 通过实验证明了利用transformer架构从动作轨迹学习命题STRIPS世界模型的可行性。

Abstract: We consider the problem of learning propositional STRIPS world models from
action traces alone, using a deep learning architecture (transformers) and
gradient descent. The task is cast as a supervised next token prediction
problem where the tokens are the actions, and an action $a$ may follow an
action sequence if the hidden effects of the previous actions do not make an
action precondition of $a$ false. We show that a suitable transformer
architecture can faithfully represent propositional STRIPS world models, and
that the models can be learned from sets of random valid (positive) and invalid
(negative) action sequences alone. A number of experiments are reported.

</details>


### [16] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: 介绍SteeringControl基准，评估表征引导方法，发现未系统探索的权衡，收集数据集，构建框架，得出引导性能依赖组合的结论并开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有对齐工作对表征引导方法的副作用缺乏系统研究，存在许多未探索的权衡。

Method: 收集安全相关的主要和次要行为数据集，围绕五种流行引导方法评估引导效果和行为纠缠，构建模块化引导框架。

Result: 在Qwen - 2.5 - 7B和Llama - 3.1 - 8B上的实验表明，强引导性能依赖于引导方法、模型和目标行为的特定组合，不良组合会导致严重概念纠缠。

Conclusion: 表征引导方法的性能受多种因素组合影响，不同组合会带来不同结果。

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [17] [AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving](https://arxiv.org/abs/2509.13547)
*Harper Reed,Michael Sugimura,Angelo Zangari*

Main category: cs.AI

TL;DR: 研究给大语言模型代理配备人类问题解决工具能否提升性能，实验表明工具对难题有显著提升，不同模型采用不同协作策略，结构化表达是提升主因，工具是推理增强而非通用效率提升。


<details>
  <summary>Details</summary>
Motivation: 探究给大语言模型代理配备人类解决问题的协作工具和自主性能否提升其性能。

Method: 为Claude Code代理配备基于MCP的社交媒体和日志工具，让其自主使用，进行34个Aider Polyglot Python编程挑战实验。

Result: 协作工具对难题性能提升显著，不同模型自然采用不同协作策略，代理写作偏好高于阅读约2 - 9倍。

Conclusion: 人工智能代理能从类人协作工具中系统受益，协作界面是推理增强而非通用效率提升。

Abstract: We investigate whether giving LLM agents the collaborative tools and autonomy
that humans naturally use for problem solving can improve their performance. We
equip Claude Code agents with MCP-based social media and journaling tools and
allow them to use these tools as they see fit. Across 34 Aider Polyglot Python
programming challenges, collaborative tools substantially improve performance
on the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and
12-38% faster completion than baseline agents. Effects on the full challenge
set are mixed, suggesting these tools act as performance enhancers when
additional reasoning scaffolding is most needed. Surprisingly, Different models
naturally adopted distinct collaborative strategies without explicit
instruction. Sonnet 3.7 engaged broadly across tools and benefited from
articulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,
leaning on journal-based semantic search when problems were genuinely
difficult. This mirrors how human developers adjust collaboration based on
expertise and task complexity. Behavioral analysis shows agents prefer writing
over reading by about 2-9x, indicating that structured articulation drives much
of the improvement rather than information access alone. Overall, AI agents can
systematically benefit from human-inspired collaboration tools at the edge of
their capabilities, pointing to adaptive collaborative interfaces as reasoning
enhancers rather than universal efficiency boosts.

</details>


### [18] [Gen AI in Proof-based Math Courses: A Pilot Study](https://arxiv.org/abs/2509.13570)
*Hannah Klawa,Shraddha Rajpal,Cigole Thomas*

Main category: cs.AI

TL;DR: 研究在三门本科数学证明课程中，学生对生成式AI的使用和看法，探讨其对教学的影响并讨论未来教学整合的考虑。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在高等教育快速兴起且当前检测工具不可靠，需要制定鼓励学生学习和批判性思维的政策。

Method: 在三门允许一定使用生成式AI的课程中，基于调查回复和学生访谈，分析学生与AI工具的互动、对其有用性和局限性的看法。

Result: 未提及具体结果。

Conclusion: 讨论了将生成式AI整合到基于证明的数学教学中的未来考虑。

Abstract: With the rapid rise of generative AI in higher education and the
unreliability of current AI detection tools, developing policies that encourage
student learning and critical thinking has become increasingly important. This
study examines student use and perceptions of generative AI across three
proof-based undergraduate mathematics courses: a first-semester abstract
algebra course, a topology course and a second-semester abstract algebra
course. In each case, course policy permitted some use of generative AI.
Drawing on survey responses and student interviews, we analyze how students
engaged with AI tools, their perceptions of generative AI's usefulness and
limitations, and what implications these perceptions hold for teaching
proof-based mathematics. We conclude by discussing future considerations for
integrating generative AI into proof-based mathematics instruction.

</details>


### [19] [See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles](https://arxiv.org/abs/2509.13615)
*Zongru Wu,Rui Mao,Zhiyuan Tian,Pengzhou Cheng,Tianjie Ju,Zheng Wu,Lingzhong Dong,Haiyue Sheng,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: 本文构建状态控制基准评估现有多模态代理在切换控制指令执行上的不可靠性，提出StaR训练方法提升执行准确性和通用任务性能，还展示其在现实应用潜力。


<details>
  <summary>Details</summary>
Motivation: 多模态代理在GUI控制中无法可靠执行切换控制指令，这成为关键瓶颈。

Method: 构建状态控制基准评估现有代理，提出State - aware Reasoning (StaR)训练方法，让代理感知当前切换状态、分析指令中期望状态并采取相应行动。

Result: 在三个多模态代理上实验表明StaR能将切换指令执行准确率提高超30%，在三个公共基准测试中提升通用任务性能，在动态环境评估显示其现实应用潜力。

Conclusion: StaR方法有效提升多模态代理在切换控制指令执行上的准确性和通用任务性能，有现实应用潜力。

Abstract: The advent of multimodal agents facilitates effective interaction within
graphical user interface (GUI), especially in ubiquitous GUI control. However,
their inability to reliably execute toggle control instructions remains a key
bottleneck. To investigate this, we construct a state control benchmark with
binary toggle instructions from public datasets. Evaluations of existing agents
demonstrate their unreliability, particularly when the current toggle state
already matches the desired state. To address the challenge, we propose
State-aware Reasoning (StaR), a training method that teaches agents to perceive
the current toggle state, analyze the desired state from the instruction, and
act accordingly. Experiments on three multimodal agents demonstrate that StaR
can improve toggle instruction execution accuracy by over 30\%. Further
evaluations on three public benchmarks show that StaR also enhances general
task performance. Finally, evaluations on a dynamic environment highlight the
potential of StaR for real-world applications. Code, benchmark, and
StaR-enhanced agents are available at https://github.com/ZrW00/StaR.

</details>


### [20] [InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management](https://arxiv.org/abs/2509.13704)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: 现有工业管理自动化方法有局限，提出InfraMind框架解决相关问题，实验表明其性能更优。


<details>
  <summary>Details</summary>
Motivation: 工业管理软件操作因系统复杂、多供应商集成和专家操作员短缺面临挑战，RPA灵活性有限、维护成本高，通用GUI代理应用于工业管理有五大关键挑战。

Method: 提出InfraMind框架，集成五个创新模块，包括基于系统搜索的探索、内存驱动规划、先进状态识别、结构化知识蒸馏和多层安全机制。

Result: 在开源和商业DCIM平台上的实验表明，InfraMind在任务成功率和运营效率方面始终优于现有框架。

Conclusion: InfraMind为工业管理自动化提供了严谨且可扩展的解决方案。

Abstract: Mission-critical industrial infrastructure, such as data centers,
increasingly depends on complex management software. Its operations, however,
pose significant challenges due to the escalating system complexity,
multi-vendor integration, and a shortage of expert operators. While Robotic
Process Automation (RPA) offers partial automation through handcrafted scripts,
it suffers from limited flexibility and high maintenance costs. Recent advances
in Large Language Model (LLM)-based graphical user interface (GUI) agents have
enabled more flexible automation, yet these general-purpose agents face five
critical challenges when applied to industrial management, including unfamiliar
element understanding, precision and efficiency, state localization, deployment
constraints, and safety requirements. To address these issues, we propose
InfraMind, a novel exploration-based GUI agentic framework specifically
tailored for industrial management systems. InfraMind integrates five
innovative modules to systematically resolve different challenges in industrial
management: (1) systematic search-based exploration with virtual machine
snapshots for autonomous understanding of complex GUIs; (2) memory-driven
planning to ensure high-precision and efficient task execution; (3) advanced
state identification for robust localization in hierarchical interfaces; (4)
structured knowledge distillation for efficient deployment with lightweight
models; and (5) comprehensive, multi-layered safety mechanisms to safeguard
sensitive operations. Extensive experiments on both open-source and commercial
DCIM platforms demonstrate that our approach consistently outperforms existing
frameworks in terms of task success rate and operational efficiency, providing
a rigorous and scalable solution for industrial management automation.

</details>


### [21] [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
*Qikai Chang,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Yicheng Pan,Jianshu Zhang,Jun Du,Quan Liu,Jianqing Gao*

Main category: cs.AI

TL;DR: 现有大语言模型在高精度数学任务上表现不佳，提出THOR方法，在多基准测试中取得SOTA，代码将公开。


<details>
  <summary>Details</summary>
Motivation: 解决现有大语言模型在高精度数学任务上的不足，以及现有工具集成方法在构建数据、优化和推理方面的挑战。

Method: 提出THOR方法，包括用TIRGen构建数据集、采用强化学习策略进行细粒度分层优化、引入自我修正机制。

Result: 该方法在不同模型上有强泛化性，在多个数学基准测试中达到SOTA，在代码基准测试中有持续改进。

Conclusion: THOR是一种有效的提升大语言模型数学推理和计算能力的方法。

Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical
reasoning, but still continue to struggle with high-precision tasks like
numerical computation and formal symbolic manipulation. Integrating external
tools has emerged as a promising approach to bridge this gap. Despite recent
advances, existing methods struggle with three key challenges: constructing
tool-integrated reasoning data, performing fine-grained optimization, and
enhancing inference. To overcome these limitations, we propose THOR
(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,
a multi-agent actor-critic-based pipeline for constructing high-quality
datasets of tool-integrated reasoning paths, aligning with the policy and
generalizing well across diverse models. Second, to perform fine-grained
hierarchical optimization, we introduce an RL strategy that jointly optimizes
for both trajectory-level problem solving and step-level code generation. This
is motivated by our key insight that the success of an intermediate tool call
is a strong predictor of the final answer's correctness. Finally, THOR
incorporates a self-correction mechanism that leverages immediate tool feedback
to dynamically revise erroneous reasoning paths during inference. Our approach
demonstrates strong generalization across diverse models, performing
effectively in both reasoning and non-reasoning models. It further achieves
state-of-the-art performance for models of a similar scale on multiple
mathematical benchmarks, while also delivering consistent improvements on code
benchmarks. Our code will be publicly available at
https://github.com/JingMog/THOR.

</details>


### [22] [An Exhaustive DPLL Approach to Model Counting over Integer Linear Constraints with Simplification Techniques](https://arxiv.org/abs/2509.13880)
*Mingwei Zhang,Zhenhao Gu,Liangda Fang,Cunjing Ge,Ziliang Chen,Zhao-Rong Lai,Quanlong Guan*

Main category: cs.AI

TL;DR: 本文设计了基于穷举DPLL架构的整数线性约束模型计数精确方法，结合简化技术，实验显示该方法在随机和应用基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 整数线性约束模型计数（MCILC）在多个领域有重要应用，需设计有效精确方法。

Method: 基于穷举DPLL架构设计精确方法，并集成混合整数规划的简化技术。

Result: 在2840个随机和4131个应用基准测试中，该方法在随机基准上显著优于现有精确方法，且是唯一能解决所有4131个应用实例的方法。

Conclusion: 所设计的方法在整数线性约束模型计数上有显著优势，具有良好性能。

Abstract: Linear constraints are one of the most fundamental constraints in fields such
as computer science, operations research and optimization. Many applications
reduce to the task of model counting over integer linear constraints (MCILC).
In this paper, we design an exact approach to MCILC based on an exhaustive DPLL
architecture. To improve the efficiency, we integrate several effective
simplification techniques from mixed integer programming into the architecture.
We compare our approach to state-of-the-art MCILC counters and propositional
model counters on 2840 random and 4131 application benchmarks. Experimental
results show that our approach significantly outperforms all exact methods in
random benchmarks solving 1718 instances while the state-of-the-art approach
only computes 1470 instances. In addition, our approach is the only approach to
solve all 4131 application instances.

</details>


### [23] [Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks](https://arxiv.org/abs/2509.13968)
*Konstantinos Voudouris,Andrew Barron,Marta Halina,Colin Klein,Matishalin Patel*

Main category: cs.AI

TL;DR: 用人工神经网络评估网络信息流变化对认知表现的影响，发现循环网络有优势，部分拓扑变化无优势


<details>
  <summary>Details</summary>
Motivation: 评估网络信息流变化能否导致认知表现的过渡性变化

Method: 使用理想化信息流模型和人工神经网络，比较前馈、循环和层叠拓扑网络在学习不同复杂度人工语法的表现

Result: 循环网络相比前馈网络能处理更多类型输入，学习复杂语法表现更好；训练循环网络有困难；层叠网络在语法学习中未超越非层叠网络

Conclusion: 一些信息流变化能带来认知表现的过渡性变化

Abstract: Transitional accounts of evolution emphasise a few changes that shape what is
evolvable, with dramatic consequences for derived lineages. More recently it
has been proposed that cognition might also have evolved via a series of major
transitions that manipulate the structure of biological neural networks,
fundamentally changing the flow of information. We used idealised models of
information flow, artificial neural networks (ANNs), to evaluate whether
changes in information flow in a network can yield a transitional change in
cognitive performance. We compared networks with feed-forward, recurrent and
laminated topologies, and tested their performance learning artificial grammars
that differed in complexity, controlling for network size and resources. We
documented a qualitative expansion in the types of input that recurrent
networks can process compared to feed-forward networks, and a related
qualitative increase in performance for learning the most complex grammars. We
also noted how the difficulty in training recurrent networks poses a form of
transition barrier and contingent irreversibility -- other key features of
evolutionary transitions. Not all changes in network topology confer a
performance advantage in this task set. Laminated networks did not outperform
non-laminated networks in grammar learning. Overall, our findings show how some
changes in information flow can yield transitions in cognitive performance.

</details>


### [24] [CrowdAgent: Multi-Agent Managed Multi-Source Annotation System](https://arxiv.org/abs/2509.14030)
*Maosheng Qin,Renyu Zhu,Mingxuan Xia,Chenkai Chen,Zhen Zhu,Minmin Lin,Junbo Zhao,Lu Xu,Changjie Fan,Runze Wu,Haobo Wang*

Main category: cs.AI

TL;DR: 提出多智能体系统CrowdAgent，可对自然语言处理标注过程进行端到端控制，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用不同标注源时缺乏对标注过程的整体控制，未统一解决调度和质量成本权衡问题。

Method: 引入多智能体系统CrowdAgent，集成任务分配、数据标注和质量/成本管理，合理分配任务使不同标注源协同工作。

Result: 在六个不同的多模态分类任务上的实验证明了CrowdAgent的有效性。

Conclusion: CrowdAgent能有效实现对自然语言处理标注过程的端到端控制。

Abstract: High-quality annotated data is a cornerstone of modern Natural Language
Processing (NLP). While recent methods begin to leverage diverse annotation
sources-including Large Language Models (LLMs), Small Language Models (SLMs),
and human experts-they often focus narrowly on the labeling step itself. A
critical gap remains in the holistic process control required to manage these
sources dynamically, addressing complex scheduling and quality-cost trade-offs
in a unified manner. Inspired by real-world crowdsourcing companies, we
introduce CrowdAgent, a multi-agent system that provides end-to-end process
control by integrating task assignment, data annotation, and quality/cost
management. It implements a novel methodology that rationally assigns tasks,
enabling LLMs, SLMs, and human experts to advance synergistically in a
collaborative annotation workflow. We demonstrate the effectiveness of
CrowdAgent through extensive experiments on six diverse multimodal
classification tasks. The source code and video demo are available at
https://github.com/QMMMS/CrowdAgent.

</details>


### [25] [Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning](https://arxiv.org/abs/2509.14195)
*Shalima Binta Manir,Tim Oates*

Main category: cs.AI

TL;DR: 本文通过构建分层架构验证二阶学习促进心智表征与环境同构的假设，实验表明二阶学习在心智地图与环境同构时有效，且在迷宫任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 心智表征研究困难，现有理论假设二阶学习促进环境 - 认知同构，需实证验证。

Method: 提出包含图卷积网络（GCN）作为一阶学习器和多层感知器（MLP）控制器作为二阶学习器的分层架构。

Result: 二阶学习在认知系统形成与环境同构的心智地图时特别有效，在未见过的迷宫任务中性能显著提升、泛化性强。

Conclusion: 结构化心智表征对最大化二阶学习有效性起关键作用。

Abstract: Mental representation, characterized by structured internal models mirroring
external environments, is fundamental to advanced cognition but remains
challenging to investigate empirically. Existing theory hypothesizes that
second-order learning -- learning mechanisms that adapt first-order learning
(i.e., learning about the task/domain) -- promotes the emergence of such
environment-cognition isomorphism. In this paper, we empirically validate this
hypothesis by proposing a hierarchical architecture comprising a Graph
Convolutional Network (GCN) as a first-order learner and an MLP controller as a
second-order learner. The GCN directly maps node-level features to predictions
of optimal navigation paths, while the MLP dynamically adapts the GCN's
parameters when confronting structurally novel maze environments. We
demonstrate that second-order learning is particularly effective when the
cognitive system develops an internal mental map structurally isomorphic to the
environment. Quantitative and qualitative results highlight significant
performance improvements and robust generalization on unseen maze tasks,
providing empirical support for the pivotal role of structured mental
representations in maximizing the effectiveness of second-order learning.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [26] [Physics-based deep kernel learning for parameter estimation in high dimensional PDEs](https://arxiv.org/abs/2509.14054)
*Weihao Yan,Christoph Brune,Mengwu Guo*

Main category: cs.CE

TL;DR: 本文提出一种两阶段贝叶斯框架，结合基于物理的深度核学习与哈密顿蒙特卡罗方法，用于推断高维偏微分方程参数并量化不确定性，数值实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 高维偏微分方程参数推断面临计算和推理挑战，传统方法有局限性。

Method: 提出两阶段贝叶斯框架，第一阶段用基于物理的深度核学习训练代理模型得到初始估计，第二阶段用哈密顿蒙特卡罗方法在全贝叶斯框架下采样后验分布。

Result: 数值实验表明该框架能准确估计参数、提供可靠不确定性估计，有效应对数据稀疏和模型复杂问题。

Conclusion: 该框架是适用于多种科学和工程应用的强大且可扩展工具。

Abstract: Inferring parameters of high-dimensional partial differential equations
(PDEs) poses significant computational and inferential challenges, primarily
due to the curse of dimensionality and the inherent limitations of traditional
numerical methods. This paper introduces a novel two-stage Bayesian framework
that synergistically integrates training, physics-based deep kernel learning
(DKL) with Hamiltonian Monte Carlo (HMC) to robustly infer unknown PDE
parameters and quantify their uncertainties from sparse, exact observations.
The first stage leverages physics-based DKL to train a surrogate model, which
jointly yields an optimized neural network feature extractor and robust initial
estimates for the PDE parameters. In the second stage, with the neural network
weights fixed, HMC is employed within a full Bayesian framework to efficiently
sample the joint posterior distribution of the kernel hyperparameters and the
PDE parameters. Numerical experiments on canonical and high-dimensional inverse
PDE problems demonstrate that our framework accurately estimates parameters,
provides reliable uncertainty estimates, and effectively addresses challenges
of data sparsity and model complexity, offering a robust and scalable tool for
diverse scientific and engineering applications.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [27] [The NIAID Discovery Portal: A Unified Search Engine for Infectious and Immune-Mediated Disease Datasets](https://arxiv.org/abs/2509.13524)
*Ginger Tsueng,Emily Bullen,Candice Czech,Dylan Welzel,Leandro Collares,Jason Lin,Everaldo Rodolpho,Zubair Qazi,Nichollette Acosta,Lisa M. Mayer,Sudha Venkatachari,Zorana Mitrović Vučičević,Poromendro N. Burman,Deepti Jain,Jack DiGiovanna,Maria Giovanni,Asiyah Lin,Wilbert Van Panhuis,Laura D. Hughes,Andrew I. Su,Chunlei Wu*

Main category: cs.DB

TL;DR: NIAID数据生态系统发现门户提供统一搜索界面，便于研究人员查找和使用传染病和免疫介导疾病相关数据集，提升数据可发现性、可访问性和可重用性。


<details>
  <summary>Details</summary>
Motivation: 解决有价值数据集因难以定位而被忽视的问题，为不同技术水平的研究人员提供查找和重用数据的途径。

Method: 整合特定领域和通用存储库的元数据，提供用户友好的过滤器、预建查询和数据集集合，标准化关键元数据字段，协调异构格式。

Result: 提供统一搜索界面，支持多种资源发现，提供文档和API用于编程访问元数据，支持数据共享。

Conclusion: 该门户作为研究人员了解、诊断或治疗传染病和免疫介导疾病的入口点，支持科学进步和公共研究数据的二次利用。

Abstract: The NIAID Data Ecosystem Discovery Portal (https://data.niaid.nih.gov)
provides a unified search interface for over 4 million datasets relevant to
infectious and immune-mediated disease (IID) research. Integrating metadata
from domain-specific and generalist repositories, the Portal enables
researchers to identify and access datasets using user-friendly filters or
advanced queries, without requiring technical expertise. The Portal supports
discovery of a wide range of resources, including epidemiological, clinical,
and multi-omic datasets, and is designed to accommodate exploratory browsing
and precise searches. The Portal provides filters, prebuilt queries, and
dataset collections to simplify the discovery process for users. The Portal
additionally provides documentation and an API for programmatic access to
harmonized metadata. By easing access barriers to important biomedical
datasets, the NIAID Data Ecosystem Discovery Portal serves as an entry point
for researchers working to understand, diagnose, or treat IID.
  Valuable datasets are often overlooked because they are difficult to locate.
The NIAID Data Ecosystem Discovery Portal fills this gap by providing a
centralized, searchable interface that empowers users with varying levels of
technical expertise to find and reuse data. By standardizing key metadata
fields and harmonizing heterogeneous formats, the Portal improves data
findability, accessibility, and reusability. This resource supports hypothesis
generation, comparative analysis, and secondary use of public data by the IID
research community, including those funded by NIAID. The Portal supports data
sharing by standardizing metadata and linking to source repositories, and
maximizes the impact of public investment in research data by supporting
scientific advancement via secondary use.

</details>


### [28] [Tractability Frontiers of the Shapley Value for Aggregate Conjunctive Queries](https://arxiv.org/abs/2509.13565)
*Christoph Standke,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 研究聚合连接查询元组Shapley值计算复杂度，确定不同聚合函数下可处理的分层CQ类并证明其最大性。


<details>
  <summary>Details</summary>
Motivation: 解决先前工作中提出的min、max等常见聚合函数计算Shapley值复杂度的开放问题。

Method: 识别不同聚合函数下Shapley值可处理的分层CQ类，并证明其最大性。

Result: 发现不同聚合函数对应从布尔查询到非布尔查询的分层CQ类的不同泛化。

Conclusion: 确定了不同聚合函数下Shapley值可处理的分层CQ类，为计算复杂度问题提供了解决思路。

Abstract: In recent years, the Shapley value has emerged as a general game-theoretic
measure for assessing the contribution of a tuple to the result of a database
query. We study the complexity of calculating the Shapley value of a tuple for
an aggregate conjunctive query, which applies an aggregation function to the
result of a conjunctive query (CQ) based on a value function that assigns a
number to each query answer. Prior work by Livshits, Bertossi, Kimelfeld, and
Sebag (2020) established that this task is #P-hard for every nontrivial
aggregation function when the query is non-hierarchical with respect to its
existential variables, assuming the absence of self-joins. They further showed
that this condition precisely characterizes the class of intractable CQs when
the aggregate function is sum or count. In addition, they posed as open
problems the complexity of other common aggregate functions such as min, max,
count-distinct, average, and quantile (including median). Towards the
resolution of these problems, we identify for each aggregate function a class
of hierarchical CQs where the Shapley value is tractable with every value
function, as long as it is local (i.e., determined by the tuples of one
relation). We further show that each such class is maximal: for every CQ
outside of this class, there is a local (easy-to-compute) value function that
makes the Shapley value #P-hard. Interestingly, our results reveal that each
aggregate function corresponds to a different generalization of the class of
hierarchical CQs from Boolean to non-Boolean queries. In particular, max, min,
and count-distinct match the class of CQs that are all-hierarchical (i.e.,
hierarchical with respect to all variables), and average and quantile match the
narrower class of q-hierarchical CQs introduced by Berkholz, Keppeler, and
Schweikardt (2017) in the context of the fine-grained complexity of query
answering.

</details>


### [29] [XASDB -- Design and Implementation of an Open-Access Spectral Database](https://arxiv.org/abs/2509.13566)
*Denis Spasyuk*

Main category: cs.DB

TL;DR: 介绍X射线吸收光谱数据库XASDB，包括其架构、功能及价值。


<details>
  <summary>Details</summary>
Motivation: 应对全球同步辐射设施产生的X射线吸收光谱数据量和复杂性增加，需强大的数据管理、共享和分析基础设施。

Method: 开发基于Node.js/MongoDB架构的XASDB平台，采用XASproc JavaScript库进行数据处理，集成XASVue光谱查看器。

Result: XASDB拥有超1000条参考光谱，支持多种数据处理和可视化，促进协作研究，推动FAIR数据原则。

Conclusion: 以网络为中心的XAS数据分析方法有潜力加速多领域研究进展。

Abstract: The increasing volume and complexity of X-ray absorption spectroscopy (XAS)
data generated at synchrotron facilities worldwide require robust
infrastructure for data management, sharing, and analysis. This paper
introduces the XAS Database (XASDB), a comprehensive web-based platform
developed and hosted by the Canadian Light Source (CLS). The database houses
more than 1000 reference spectra spanning 40 elements and 324 chemical
compounds. The platform employs a Node.js/MongoDB architecture designed to
handle diverse data formats from multiple beamlines and synchrotron facilities.
A key innovation is the XASproc JavaScript library, which enables browser-based
XAS data processing including normalization, background sub- traction, extended
X-ray absorption fine structure (EXAFS) extraction, and preliminary analysis
traditionally limited to desktop applications. The integrated XASVue spectral
viewer provides installation-free data visualization and analysis with broad
accessibility across devices and operating systems. By offering standardized
data output, comprehensive metadata, and integrated analytical ca- pabilities,
XASDB facilitates collaborative research and promotes FAIR (Findable,
Accessible, In- teroperable, and Reusable) data principles. The platform serves
as a valuable resource for linear combination fitting (LCF) analysis, machine
learning applications, and educational purposes. This initiative demonstrates
the potential for web-centric approaches in XAS data analysis, accelerating
advances in materials science, environmental research, chemistry, and biology.

</details>


### [30] [Algorithms for Optimizing Acyclic Queries](https://arxiv.org/abs/2509.14144)
*Zheng Luo,Wim Van den Broeck,Guy Van den Broeck,Yisu Remy Wang*

Main category: cs.DB

TL;DR: 本文提出三种为无环查询构建连接树的方法，分别基于枚举、最大基数搜索算法和线性计划转换。


<details>
  <summary>Details</summary>
Motivation: 以往查询优化多集中在二元连接算法，而理论最优算法依赖连接树，需要新的优化技术。

Method: 提出三种构建连接树的方法：枚举所有α - 无环查询的连接树；利用Tarjan和Yannakakis的最大基数搜索算法构建Berge - 无环查询的连接树；将γ - 无环查询的连通左深线性计划转换为连接树。

Result: 第一种方法为无环连接的基于成本的优化器奠定基础；第二种方法可实现大型连接查询的并行执行；第三种方法可复用二元连接的优化基础设施。

Conclusion: 所提出的三种构建连接树的方法为无环查询的优化提供了有效途径。

Abstract: Most research on query optimization has centered on binary join algorithms
like hash join and sort-merge join. However, recent years have seen growing
interest in theoretically optimal algorithms, notably Yannakakis' algorithm.
These algorithms rely on join trees, which differ from the operator trees for
binary joins and require new optimization techniques. We propose three
approaches to constructing join trees for acyclic queries. First, we give an
algorithm to enumerate all join trees of an alpha-acyclic query by edits with
amortized constant delay, which forms the basis of a cost-based optimizer for
acyclic joins. Second, we show that the Maximum Cardinality Search algorithm by
Tarjan and Yannakakis constructs a unique shallowest join tree, rooted at any
relation, for a Berge-acyclic query; this tree enables parallel execution of
large join queries. Finally, we prove that any connected left-deep linear plan
for a gamma-acyclic query can be converted into a join tree by a simple
algorithm, allowing reuse of optimization infrastructure developed for binary
joins.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [31] [A User-centric Kubernetes-based Architecture for Green Cloud Computing](https://arxiv.org/abs/2509.13325)
*Matteo Zanotto,Leonardo Vicentini,Redi Vreto,Francesco Lumpp,Diego Braga,Sandro Fiore*

Main category: cs.DC

TL;DR: 为减少云计算碳排放，提出以用户为中心、基于Kubernetes的绿色云计算架构，评估显示在资源受限场景下可减排13%。


<details>
  <summary>Details</summary>
Motivation: 数据中心规模扩大致电力消耗和碳排放增加，云提供商虽接近最优能效但缺乏精确可持续性报告，需从消费者端改进。

Method: 提出以用户为中心、基于Kubernetes的架构，实现碳强度预测器，基于绿色能源可用性调度工作负载，利用区域和时间差异减排。

Result: 与基线轮询调度器相比，在资源严格受限场景下系统可实现高达13%的减排。

Conclusion: 所提出的架构能有效减少云计算碳排放。

Abstract: To meet the increasing demand for cloud computing services, the scale and
number of data centers keeps increasing worldwide. This growth comes at the
cost of increased electricity consumption, which directly correlates to CO2
emissions, the main driver of climate change. As such, researching ways to
reduce cloud computing emissions is more relevant than ever. However, although
cloud providers are reportedly already working near optimal power efficiency,
they fail in providing precise sustainability reporting. This calls for further
improvements on the cloud computing consumer's side. To this end, in this paper
we propose a user-centric, Kubernetes-based architecture for green cloud
computing. We implement a carbon intensity forecaster and we use it to schedule
workloads based on the availability of green energy, exploiting both regional
and temporal variations to minimize emissions. We evaluate our system using
real-world traces of cloud workloads execution comparing the achieved carbon
emission savings against a baseline round-robin scheduler. Our findings
indicate that our system can achieve up to a 13% reduction in emissions in a
strict scenario with heavy limitations on the available resources.

</details>


### [32] [LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology](https://arxiv.org/abs/2509.13978)
*Renan Souza,Timothy Poteet,Brian Etz,Daniel Rosendo,Amal Gueroudji,Woong Shin,Prasanna Balaprakash,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 提出利用交互式大语言模型（LLM）代理进行运行时数据分析的方法，评估显示能实现超越记录溯源的准确且有洞察力的响应。


<details>
  <summary>Details</summary>
Motivation: 现代科学发现依赖跨边缘、云与高性能计算的数据工作流，工作流溯源数据在大规模时复杂难分析，现有系统限制数据交互。

Method: 引入评估方法、参考架构和开源实现，采用轻量级、元数据驱动设计将自然语言转换为结构化溯源查询。

Result: 对LLaMA、GPT、Gemini和Claude在不同查询类别和真实化学工作流上的评估，显示模块化设计、提示调优和检索增强生成（RAG）有效。

Conclusion: 利用交互式LLM代理的方法可实现超越记录溯源的准确且有洞察力的响应。

Abstract: Modern scientific discovery increasingly relies on workflows that process
data across the Edge, Cloud, and High Performance Computing (HPC) continuum.
Comprehensive and in-depth analyses of these data are critical for hypothesis
validation, anomaly detection, reproducibility, and impactful findings.
Although workflow provenance techniques support such analyses, at large scale,
the provenance data become complex and difficult to analyze. Existing systems
depend on custom scripts, structured queries, or static dashboards, limiting
data interaction. In this work, we introduce an evaluation methodology,
reference architecture, and open-source implementation that leverages
interactive Large Language Model (LLM) agents for runtime data analysis. Our
approach uses a lightweight, metadata-driven design that translates natural
language into structured provenance queries. Evaluations across LLaMA, GPT,
Gemini, and Claude, covering diverse query classes and a real-world chemistry
workflow, show that modular design, prompt tuning, and Retrieval-Augmented
Generation (RAG) enable accurate and insightful LLM agent responses beyond
recorded provenance.

</details>


### [33] [Testing and benchmarking emerging supercomputers via the MFC flow solver](https://arxiv.org/abs/2509.13575)
*Benjamin Wilfong,Anand Radhakrishnan,Henry A. Le Berre,Tanush Prathi,Stephen Abbott,Spencer H. Bryngelson*

Main category: cs.DC

TL;DR: 本文介绍了多组分流代码（MFC）及其工具链，用于超算测试评估，给出多种架构和编译器的基准测试结果，发现了编译器问题。


<details>
  <summary>Details</summary>
Motivation: 新超算部署需要通过应用代码进行测试评估，需要便携、用户友好的工具。

Method: 使用MFC及其自动化工具链进行输入生成、编译、作业提交、回归测试和基准测试，以每空间离散网格点的墙时间作为衡量标准。

Result: 对多种GPU和CPU架构、不同编译器进行测试，发现了前沿和埃尔卡皮坦等机器上的编译器错误和回归问题，已对约50个计算设备和5台旗舰超算进行基准测试。

Conclusion: MFC及其工具链可有效用于超算的测试和评估，能发现编译器问题。

Abstract: Deploying new supercomputers requires testing and evaluation via application
codes. Portable, user-friendly tools enable evaluation, and the Multicomponent
Flow Code (MFC), a computational fluid dynamics (CFD) code, addresses this
need. MFC is adorned with a toolchain that automates input generation,
compilation, batch job submission, regression testing, and benchmarking. The
toolchain design enables users to evaluate compiler-hardware combinations for
correctness and performance with limited software engineering experience. As
with other PDE solvers, wall time per spatially discretized grid point serves
as a figure of merit. We present MFC benchmarking results for five generations
of NVIDIA GPUs, three generations of AMD GPUs, and various CPU architectures,
utilizing Intel, Cray, NVIDIA, AMD, and GNU compilers. These tests have
revealed compiler bugs and regressions on recent machines such as Frontier and
El Capitan. MFC has benchmarked approximately 50 compute devices and 5 flagship
supercomputers.

</details>


### [34] [Modeling the Carbon Footprint of HPC: The Top 500 and EasyC](https://arxiv.org/abs/2509.13583)
*Varsha Rao,Andrew A. Chien*

Main category: cs.DC

TL;DR: 本文聚焦Top 500 HPC系统碳足迹评估，用EasyC工具建模，给出碳足迹估算并预测到2030年的变化，还探讨数据覆盖提升。


<details>
  <summary>Details</summary>
Motivation: 气候变化对HPC系统至关重要，但现有碳排放核算方法难用于单一或多个系统，缺乏HPC全行业碳报告。

Method: 利用Top500.org公开数据，使用EasyC工具建模，通过利用额外公共信息提升覆盖度，用插值法估算Top 500系统碳足迹。

Result: 对391个系统的运营碳和283个系统的内含碳建模，得出Top 500系统运营碳和内含碳估算值，还预测了到2030年碳足迹增长。

Conclusion: EasyC工具能以少量数据指标建模碳足迹，数据覆盖度可提升至运营排放98%和内含排放80.8%。

Abstract: Climate change is a critical concern for HPC systems, but GHG protocol
carbon-emission accounting methodologies are difficult for a single system, and
effectively infeasible for a collection of systems. As a result, there is no
HPC-wide carbon reporting, and even the largest HPC sites do not do GHG
protocol reporting.
  We assess the carbon footprint of HPC, focusing on the Top 500 systems. The
key challenge lies in modeling the carbon footprint with limited data
availability.
  With the disclosed Top500.org data, and using a new tool, EasyC, we were able
to model the operational carbon of 391 HPC systems and the embodied carbon of
283 HPC systems. We further show how this coverage can be enhanced by
exploiting additional public information. With improved coverage, then
interpolation is used to produce the first carbon footprint estimates of the
Top 500 HPC systems. They are 1,393.7 million MT CO2e operational carbon (1
Year) and 1,881.8 million MT CO2e embodied carbon. We also project how the Top
500's carbon footprint will increase through 2030.
  A key enabler is the EasyC tool which models carbon footprint with only a few
data metrics. We explore availability of data and enhancement, showing that
coverage can be increased to 98% of Top 500 systems for operational and 80.8%
of the systems for embodied emissions.

</details>


### [35] [GPU Programming for AI Workflow Development on AWS SageMaker: An Instructional Approach](https://arxiv.org/abs/2509.13703)
*Sriram Srinivasan,Hamdan Alabsi,Rand Obeidat,Nithisha Ponnala,Azene Zenebe*

Main category: cs.DC

TL;DR: 本文介绍了一门关于GPU架构、编程及用于开发AI代理的课程，对其设计、实施和评估进行阐述，结果显示课程有良好效果，倡导在STEM课程中推广类似选修课。


<details>
  <summary>Details</summary>
Motivation: 为本科生和研究生提供有关GPU架构、编程及开发AI代理的专业课程，提升学生在相关领域的能力。

Method: 课程从GPU/CPU硬件和并行计算基础概念开始，引导学生开发RAG并优化，让学生实践云GPU实例配置等，通过评估、课程评价和匿名调查评估学习成果。

Result: AWS是实用GPU编程的有效经济平台；体验式学习增强技术能力和参与度；课程通过工具强化学生解决问题和批判性思维能力。

Conclusion: 强调将并行计算融入STEM教育的教学价值，倡导在STEM课程中更广泛采用类似选修课。

Abstract: We present the design, implementation, and comprehensive evaluation of a
specialized course on GPU architecture, GPU programming, and how these are used
for developing AI agents. This course is offered to undergraduate and graduate
students during Fall 2024 and Spring 2025. The course began with foundational
concepts in GPU/CPU hardware and parallel computing and progressed to develop
RAG and optimizing them using GPUs. Students gained experience provisioning and
configuring cloud-based GPU instances, implementing parallel algorithms, and
deploying scalable AI solutions. We evaluated learning outcomes through
assessments, course evaluations, and anonymous surveys. The results reveal that
(1) AWS served as an effective and economical platform for practical GPU
programming, (2) experiential learning significantly enhanced technical
proficiency and engagement, and (3) the course strengthened students'
problem-solving and critical thinking skills through tools such as TensorBoard
and HPC profilers, which exposed performance bottlenecks and scaling issues.
Our findings underscore the pedagogical value of integrating parallel computing
into STEM education. We advocate for broader adoption of similar electives
across STEM curricula to prepare students for the demands of modern,
compute-intensive fields.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [36] [GTA -- An ATSP Method: Shifting the Bottleneck from Algorithm to RAM](https://arxiv.org/abs/2509.13327)
*Wissam Nakhle*

Main category: cs.DS

TL;DR: 提出可扩展、高性能算法GTA解决大规模ATSP问题，效率高且能集成到多领域工作流。


<details>
  <summary>Details</summary>
Motivation: 解决大规模旅行商问题（ATSP），提供比依赖超级计算硬件的传统求解器更高效的方案。

Method: 结合高效启发式热启动和子回路消除策略，去除传统MTZ约束。

Result: 能在普通8逻辑处理器计算机上创纪录时间解决最多5000节点的实例，收敛率与高性能计算框架相当，结果可复现。

Conclusion: GTA算法将TSP解决方案瓶颈从算法复杂性转移到硬件，适合跨学科大规模TSP问题求解。

Abstract: We present a scalable, high-performance algorithm that deterministically
solves large-scale instances of the Traveling Salesman problem (in its
asymmetric version, ATSP) to optimality using commercially available computing
hardware. By combining an efficient heuristic warm start, capable of achieving
near-optimality within seconds in some cases, with a subtour elimination
strategy that removes the need for traditional MTZ constraints, our approach
consistently resolves instances up to 5,000 nodes (approximately 25 million
binary variables) in record time on widely accessible computers, with eight
logical processors. We demonstrate reproducible results with convergence rates
comparable to those of high-performance computing frameworks. Real-time
iteration tracking and an adaptable interface allow seamless integration into
scheduling workflows in logistics, bioinformatics, and astronomy. Designed to
streamline solutions to large-scale TSP problems across disciplines, our
approach is benchmarked against widely used public datasets, offering a
deterministic, resource-efficient alternative to conventional solvers that rely
on supercomputing hardware. Our GTA (Gurobi Tabu Algorithm) algorithm is a
fundamental shift of TSP solution bottleneck from algorithmic complexity to the
underlying hardware (RAM and system memory), which is a highly desirable
characteristic.

</details>


### [37] [Hardness of Dynamic Core and Truss Decompositions](https://arxiv.org/abs/2509.13584)
*Yan S. Couto,Cristina G. Fernandes*

Main category: cs.DS

TL;DR: 本文研究图的k - 核问题，证明除非改进多个领域算法，否则无高效动态算法，还证明有界算法不存在，给出有向和边变体问题下界，提出2 - 核的多对数动态算法。


<details>
  <summary>Details</summary>
Motivation: 回答Hanauer等人关于动态图算法调查中提出的关于k - 核的问题。

Method: 基于OMv和SETH猜想进行理论证明。

Result: 证明无高效动态算法求k - 核或核心值的(2 - ε)近似值；证明有界算法不存在；给出有向和k - 桁架问题下界；提出2 - 核的多对数动态算法。

Conclusion: 解释了相关领域研究多聚焦于有界算法的原因，且基于猜想证明有界算法不存在，同时给出部分正结果。

Abstract: The k-core of a graph is its maximal subgraph with minimum degree at least k,
and the core value of a vertex u is the largest k for which u is contained in
the k-core of the graph. Among cohesive subgraphs, k-core and its variants have
received a lot of attention recently, particularly on dynamic graphs, as
reported by Hanauer, Henzinger, and Schulz in their recent survey on dynamic
graph algorithms. We answer questions on k-core stated in the survey, proving
that there is no efficient dynamic algorithm for k-core or to find (2 -
{\epsilon})-approximations for the core values, unless we can improve
decade-long state-of-the-art algorithms in many areas including matrix
multiplication and satisfiability, based on the established OMv and SETH
conjectures. Some of our results show that there is no dynamic algorithm for
k-core asymptotically faster than the trivial ones. This explains why most
recent research papers in this area focus not on a generic efficient dynamic
algorithm, but on finding a bounded algorithm, which is fast when few core
values change per update. However, we also prove that such bounded algorithms
do not exist, based on the OMv conjecture. We present lower bounds also for a
directed version of the problem, and for the edge variant of the problem, known
as k-truss. On the positive side, we present a polylogarithmic dynamic
algorithm for 2-core.

</details>


### [38] [On Solving Asymmetric Diagonally Dominant Linear Systems in Sublinear Time](https://arxiv.org/abs/2509.13891)
*Tsz Chiu Kwok,Zhewei Wei,Mingji Yang*

Main category: cs.DS

TL;DR: 本文研究亚线性时间求解RDD/CDD线性系统，刻画问题数学结构，给出算法结果并继承相关问题的难度结果，为后续研究提供可能。


<details>
  <summary>Details</summary>
Motivation: 将对称对角占优（SDD）系统的亚线性时间求解器研究推广到非对称情况，估计给定向量与特定解的内积。

Method: 通过Neumann级数表示解，证明其收敛性并界定截断误差；采用随机游走采样、局部推送及其双向组合技术。

Result: 得到了在不同场景和误差度量下局部逼近内积的算法结果，统一理解了图上估计随机游走概率的两种基本方法，继承了相关问题的硬度结果。

Conclusion: 为亚线性求解器、局部图算法和有向谱图理论的进一步研究提供了可能。

Abstract: We initiate a study of solving a row/column diagonally dominant (RDD/CDD)
linear system $Mx=b$ in sublinear time, with the goal of estimating
$t^{\top}x^*$ for a given vector $t\in R^n$ and a specific solution $x^*$. This
setting naturally generalizes the study of sublinear-time solvers for symmetric
diagonally dominant (SDD) systems [AKP19] to the asymmetric case.
  Our first contributions are characterizations of the problem's mathematical
structure. We express a solution $x^*$ via a Neumann series, prove its
convergence, and upper bound the truncation error on this series through a
novel quantity of $M$, termed the maximum $p$-norm gap. This quantity
generalizes the spectral gap of symmetric matrices and captures how the
structure of $M$ governs the problem's computational difficulty.
  For systems with bounded maximum $p$-norm gap, we develop a collection of
algorithmic results for locally approximating $t^{\top}x^*$ under various
scenarios and error measures. We derive these results by adapting the
techniques of random-walk sampling, local push, and their bidirectional
combination, which have proved powerful for special cases of solving RDD/CDD
systems, particularly estimating PageRank and effective resistance on graphs.
Our general framework yields deeper insights, extended results, and improved
complexity bounds for these problems. Notably, our perspective provides a
unified understanding of Forward Push and Backward Push, two fundamental
approaches for estimating random-walk probabilities on graphs.
  Our framework also inherits the hardness results for sublinear-time SDD
solvers and local PageRank computation, establishing lower bounds on the
maximum $p$-norm gap or the accuracy parameter. We hope that our work opens the
door for further study into sublinear solvers, local graph algorithms, and
directed spectral graph theory.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [39] [Delta Matters: An Analytically Tractable Model for $β$-$δ$ Discounting Agents](https://arxiv.org/abs/2509.13637)
*Yasunori Akagi,Takeshi Kurashima*

Main category: cs.GT

TL;DR: 本文放宽先前研究对时间不一致性建模时的约束条件，给出一般情况下的封闭形式描述，推导任务放弃条件和最优干预计算方法，指出先前研究固定参数可能过度简化决策过程。


<details>
  <summary>Details</summary>
Motivation: 先前研究对时间不一致性的分析局限于δ = 1的情况，需要放宽约束以更贴合实际决策过程。

Method: 放宽δ = 1的约束，对0 < δ ≤ 1的一般情况进行分析。

Result: 得出一般情况下代理行为的封闭形式描述，推导代理放弃任务的条件和计算最优干预的有效方法。

Conclusion: 代理行为和最优干预关键取决于δ的值，先前研究固定δ = 1可能过度简化现实决策过程。

Abstract: Humans exhibit time-inconsistent behavior, in which planned actions diverge
from executed actions. Understanding time inconsistency and designing
appropriate interventions is a key research challenge in computer science and
behavioral economics. Previous work focuses on progress-based tasks and derives
a closed-form description of agent behavior, from which they obtain optimal
intervention strategies. They model time-inconsistency using the
$\beta$-$\delta$ discounting (quasi-hyperbolic discounting), but the analysis
is limited to the case $\delta = 1$. In this paper, we relax that constraint
and show that a closed-form description of agent behavior remains possible for
the general case $0 < \delta \le 1$. Based on this result, we derive the
conditions under which agents abandon tasks and develop efficient methods for
computing optimal interventions. Our analysis reveals that agent behavior and
optimal interventions depend critically on the value of $\delta$, suggesting
that fixing $\delta = 1$ in many prior studies may unduly simplify real-world
decision-making processes.

</details>


### [40] [Efficient Last-Iterate Convergence in Regret Minimization via Adaptive Reward Transformation](https://arxiv.org/abs/2509.13653)
*Hang Ren,Yulin Wu,Shuhan Qi,Jiajia Zhang,Xiaozhen Sun,Tianzi Ma,Xuan Wang*

Main category: cs.GT

TL;DR: 提出自适应技术解决奖励变换框架用于后悔最小化时的问题，实验显示加速收敛且优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 传统后悔最小化计算平均策略资源需求大、奖励变换框架参数手动调整有问题，影响实际应用。

Method: 提出自适应技术，动态调整参数，平衡探索与利用，改善后悔积累。

Result: 实验表明所提方法显著加速收敛，优于现有算法。

Conclusion: 自适应技术能确保理论保证与实际性能更好一致，有效解决NFGs和EFGs问题。

Abstract: Regret minimization is a powerful method for finding Nash equilibria in
Normal-Form Games (NFGs) and Extensive-Form Games (EFGs), but it typically
guarantees convergence only for the average strategy. However, computing the
average strategy requires significant computational resources or introduces
additional errors, limiting its practical applicability. The Reward
Transformation (RT) framework was introduced to regret minimization to achieve
last-iterate convergence through reward function regularization. However, it
faces practical challenges: its performance is highly sensitive to manually
tuned parameters, which often deviate from theoretical convergence conditions,
leading to slow convergence, oscillations, or stagnation in local optima.
  Inspired by previous work, we propose an adaptive technique to address these
issues, ensuring better consistency between theoretical guarantees and
practical performance for RT Regret Matching (RTRM), RT Counterfactual Regret
Minimization (RTCFR), and their variants in solving NFGs and EFGs more
effectively. Our adaptive methods dynamically adjust parameters, balancing
exploration and exploitation while improving regret accumulation, ultimately
enhancing asymptotic last-iterate convergence and achieving linear convergence.
Experimental results demonstrate that our methods significantly accelerate
convergence, outperforming state-of-the-art algorithms.

</details>


### [41] [Nash Equilibria in Games with Playerwise Concave Coupling Constraints: Existence and Computation](https://arxiv.org/abs/2509.14032)
*Philip Jordan,Maryam Kamgarpour*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the existence and computation of Nash equilibria in continuous
static games where the players' admissible strategies are subject to shared
coupling constraints, i.e., constraints that depend on their \emph{joint}
strategies. Specifically, we focus on a class of games characterized by
playerwise concave utilities and playerwise concave constraints. Prior results
on the existence of Nash equilibria are not applicable to this class, as they
rely on strong assumptions such as joint convexity of the feasible set. By
leveraging topological fixed point theory and novel structural insights into
the contractibility of feasible sets under playerwise concave constraints, we
give an existence proof for Nash equilibria under weaker conditions. Having
established existence, we then focus on the computation of Nash equilibria via
independent gradient methods under the additional assumption that the utilities
admit a potential function. To account for the possibly nonconvex feasible
region, we employ a log barrier regularized gradient ascent with adaptive
stepsizes. Starting from an initial feasible strategy profile and under exact
gradient feedback, the proposed method converges to an $\epsilon$-approximate
constrained Nash equilibrium within $\mathcal{O}(\epsilon^{-3})$ iterations.

</details>


### [42] [Generalised Reachability Games Revisited](https://arxiv.org/abs/2509.14091)
*Sougata Bose,Daniel Hausmann,Soumyajit Paul,Sven Schewe,Tansholpan Zhanabekova*

Main category: cs.GT

TL;DR: 本文研究图上广义可达性目标的两人游戏复杂度，一方面改进复杂度情况，扩大已知易解类；另一方面研究优化变体，多数情况难处理。


<details>
  <summary>Details</summary>
Motivation: 进一步研究图上两人广义可达性目标游戏的复杂度。

Method: 分析广义可达性游戏，扩展已知易解类；研究优化变体问题。

Result: 一是改进广义可达性游戏复杂度情况，扩大易解类；二是多数优化变体问题难处理，单元素目标集优化问题NP难，特殊情况可恢复易解性。

Conclusion: 广义可达性游戏复杂度研究有新进展，不同情况下问题的难易程度不同。

Abstract: Classic reachability games on graphs are zero-sum games, where the goal of
one player, Eve, is to visit a vertex from a given target set, and that of
other player, Adam, is to prevent this. Generalised reachability games, studied
by Fijalkow and Horn, are a generalisation of reachability objectives, where
instead of a single target set, there is a family of target sets and Eve must
visit all of them in any order. In this work, we further study the complexity
of solving two-player games on graphs with generalised reachability objectives.
Our results are twofold: first, we provide an improved complexity picture for
generalised reachability games, expanding the known tractable class from games
in which all target sets are singleton to additionally allowing a logarithmic
number of target sets of arbitrary size. Second, we study optimisation variants
of generalised reachability with a focus on the size of the target sets. For
these problems, we show intractability for most interesting cases.
Particularly, in contrast to the tractability in the classic variant for
singleton target sets, the optimisation problem is NP-hard when Eve tries to
maximise the number of singleton target sets that are visited. Tractability can
be recovered in the optimisation setting when all target sets are singleton by
requiring that Eve pledges a maximum sized subset of target sets that she can
guarantee to visit.

</details>


### [43] [Sound Value Iteration for Simple Stochastic Games](https://arxiv.org/abs/2509.14112)
*Muqsit Azeem,Jan Kretinsky,Maximilian Weininger*

Main category: cs.GT

TL;DR: 本文扩展了SVI算法，使其适用于随机博弈和带终组件的马尔可夫决策过程，并进行优化和实验评估。


<details>
  <summary>Details</summary>
Motivation: 基本值迭代算法无结果精度保证，SVI虽有改进但不适用于随机博弈和带终组件的马尔可夫决策过程，需进行扩展。

Method: 对SVI算法进行扩展，妥善处理终组件，且提供SVI的优化。

Result: 实现了扩展和优化后的SVI算法的原型。

Conclusion: 实验证明扩展和优化后的SVI算法在含概率循环的系统中有应用潜力。

Abstract: Algorithmic analysis of Markov decision processes (MDP) and stochastic games
(SG) in practice relies on value-iteration (VI) algorithms. Since basic VI does
not provide guarantees on the precision of the result, variants of VI have been
proposed that offer such guarantees. In particular, sound value iteration (SVI)
not only provides precise lower and upper bounds on the result, but also
converges faster in the presence of probabilistic cycles. Unfortunately, it is
neither applicable to SG, nor to MDP with end components. In this paper, we
extend SVI and cover both cases. The technical challenge consists mainly in
proper treatment of end components, which require different handling than in
the literature. Moreover, we provide several optimizations of SVI. Finally, we
evaluate our prototype implementation experimentally to demonstrate its
potential on systems with probabilistic cycles.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [44] [MA-DPR: Manifold-aware Distance Metrics for Dense Passage Retrieval](https://arxiv.org/abs/2509.13562)
*Yifan Liu,Qianfeng Wen,Mark Zhao,Jiazhou Liang,Scott Sanner*

Main category: cs.IR

TL;DR: 提出用于DPR的流形感知距离度量MA - DPR，在OOD段落检索上表现更好且适用多种任务。


<details>
  <summary>Details</summary>
Motivation: 传统DPR使用的欧氏或余弦距离在嵌入位于低维非线性流形（尤其是OOD设置）时，无法捕捉语义相似性。

Method: 提出MA - DPR，用最近邻图建模段落的内在流形结构，基于图中最短路径测量查询 - 段落距离。

Result: MA - DPR在OOD段落检索上比欧氏和余弦距离最多高26%，在分布内性能相当，查询推理时间增加极少。

Conclusion: 流形感知距离使DPR能利用相邻段落上下文，MA - DPR可用于多种密集嵌入和检索任务。

Abstract: Dense Passage Retrieval (DPR) typically relies on Euclidean or cosine
distance to measure query-passage relevance in embedding space, which is
effective when embeddings lie on a linear manifold. However, our experiments
across DPR benchmarks suggest that embeddings often lie on lower-dimensional,
non-linear manifolds, especially in out-of-distribution (OOD) settings, where
cosine and Euclidean distance fail to capture semantic similarity. To address
this limitation, we propose a manifold-aware distance metric for DPR (MA-DPR)
that models the intrinsic manifold structure of passages using a nearest
neighbor graph and measures query-passage distance based on their shortest path
in this graph. We show that MA-DPR outperforms Euclidean and cosine distances
by up to 26% on OOD passage retrieval with comparable in-distribution
performance across various embedding models while incurring a minimal increase
in query inference time. Empirical evidence suggests that manifold-aware
distance allows DPR to leverage context from related neighboring passages,
making it effective even in the absence of direct semantic overlap. MADPR can
be applied to a wide range of dense embedding and retrieval tasks, offering
potential benefits across a wide spectrum of domains.

</details>


### [45] [Modernizing Facebook Scoped Search: Keyword and Embedding Hybrid Retrieval with LLM Evaluation](https://arxiv.org/abs/2509.13603)
*Yongye Su,Zeya Zhang,Jane Kou,Cheng Ju,Shubhojeet Sarkar,Yamin Wang,Ji Liu,Shengbo Guo*

Main category: cs.IR

TL;DR: 提出现代化Facebook群组搜索框架，结合传统与基于嵌入的检索，用新评估框架验证效果，提升搜索质量。


<details>
  <summary>Details</summary>
Motivation: 改进社交网络搜索的相关性和结果多样性，提升用户体验。

Method: 将语义检索融入现有关键词搜索流程，结合传统与基于嵌入的检索；引入基于大语言模型的评估框架。

Result: 混合检索系统显著提升用户参与度和搜索质量，经在线指标和大语言模型评估验证。

Conclusion: 为大规模真实社交平台部署和评估高级检索系统提供实用见解。

Abstract: Beyond general web-scale search, social network search uniquely enables users
to retrieve information and discover potential connections within their social
context. We introduce a framework of modernized Facebook Group Scoped Search by
blending traditional keyword-based retrieval with embedding-based retrieval
(EBR) to improve the search relevance and diversity of search results. Our
system integrates semantic retrieval into the existing keyword search pipeline,
enabling users to discover more contextually relevant group posts. To
rigorously assess the impact of this blended approach, we introduce a novel
evaluation framework that leverages large language models (LLMs) to perform
offline relevance assessments, providing scalable and consistent quality
benchmarks. Our results demonstrate that the blended retrieval system
significantly enhances user engagement and search quality, as validated by both
online metrics and LLM-based evaluation. This work offers practical insights
for deploying and evaluating advanced retrieval systems in large-scale,
real-world social platforms.

</details>


### [46] [Mind the Gap: Aligning Knowledge Bases with User Needs to Enhance Mental Health Retrieval](https://arxiv.org/abs/2509.13626)
*Amanda Chan,James Jiayu Liu,He Kai,Onno P. Kampman*

Main category: cs.IR

TL;DR: 提出基于AI的语料库增强框架，通过案例对比有针对性和无针对性的增强方式，表明有针对性的语料库增长可降低内容创建需求并保持高质量。


<details>
  <summary>Details</summary>
Motivation: 可靠心理健康信息获取重要，但现有知识库扩展资源消耗大且与用户需求不匹配，检索系统表现不佳。

Method: 提出基于AI的差距感知框架，通过叠加自然主义用户数据识别未充分代表的主题，在案例研究中对比有针对性和无针对性的语料库增强方式，评估四个检索增强生成管道中检索信息的相关性和有用性。

Result: 有针对性的增强以适度扩展实现接近最优性能，无针对性的增强需要大幅扩展才能达到类似性能。

Conclusion: 有针对性的语料库增长可降低内容创建需求，保持高检索和提供质量，为构建可信健康信息库和支持高风险领域生成式AI应用提供可扩展方法。

Abstract: Access to reliable mental health information is vital for early help-seeking,
yet expanding knowledge bases is resource-intensive and often misaligned with
user needs. This results in poor performance of retrieval systems when
presented concerns are not covered or expressed in informal or contextualized
language. We present an AI-based gap-informed framework for corpus augmentation
that authentically identifies underrepresented topics (gaps) by overlaying
naturalistic user data such as forum posts in order to prioritize expansions
based on coverage and usefulness. In a case study, we compare Directed
(gap-informed augmentations) with Non-Directed augmentation (random additions),
evaluating the relevance and usefulness of retrieved information across four
retrieval-augmented generation (RAG) pipelines. Directed augmentation achieved
near-optimal performance with modest expansions--requiring only a 42% increase
for Query Transformation, 74% for Reranking and Hierarchical, and 318% for
Baseline--to reach ~95% of the performance of an exhaustive reference corpus.
In contrast, Non-Directed augmentation required substantially larger and thus
practically infeasible expansions to achieve comparable performance (232%,
318%, 403%, and 763%, respectively). These results show that strategically
targeted corpus growth can reduce content creation demands while sustaining
high retrieval and provision quality, offering a scalable approach for building
trusted health information repositories and supporting generative AI
applications in high-stakes domains.

</details>


### [47] [Enhancing Time Awareness in Generative Recommendation](https://arxiv.org/abs/2509.13957)
*Sunkyung Lee,Seongmin Park,Jonghyo Kim,Mincheol Yoon,Jongwuk Lee*

Main category: cs.IR

TL;DR: 提出时间感知生成推荐模型GRUT，通过时间信号捕捉用户偏好，实验表明优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有生成推荐研究忽略物品间时间动态，无法体现用户偏好演变。

Method: 提出时间感知提示（含用户级和物品级上下文）和趋势感知推理方法。

Result: GRUT在四个基准数据集上Recall@5和NDCG@5分别提升达15.4%和14.3%。

Conclusion: GRUT能有效捕捉用户隐藏偏好，优于现有模型，代码开源。

Abstract: Generative recommendation has emerged as a promising paradigm that formulates
the recommendations into a text-to-text generation task, harnessing the vast
knowledge of large language models. However, existing studies focus on
considering the sequential order of items and neglect to handle the temporal
dynamics across items, which can imply evolving user preferences. To address
this limitation, we propose a novel model, Generative Recommender Using Time
awareness (GRUT), effectively capturing hidden user preferences via various
temporal signals. We first introduce Time-aware Prompting, consisting of two
key contexts. The user-level temporal context models personalized temporal
patterns across timestamps and time intervals, while the item-level transition
context provides transition patterns across users. We also devise Trend-aware
Inference, a training-free method that enhances rankings by incorporating trend
information about items with generation likelihood. Extensive experiments
demonstrate that GRUT outperforms state-of-the-art models, with gains of up to
15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets. The
source code is available at https://github.com/skleee/GRUT.

</details>


### [48] [GEM-Bench: A Benchmark for Ad-Injected Response Generation within Generative Engine Marketing](https://arxiv.org/abs/2509.14221)
*Silan Hu,Shiqi Zhang,Yimin Shi,Xiaokui Xiao*

Main category: cs.IR

TL;DR: 提出首个针对GEM中广告注入回复生成的综合基准GEM - Bench，初步结果显示不同方法各有优劣，需后续研究更有效方案。


<details>
  <summary>Details</summary>
Motivation: 现有基准未针对GEM中广告注入回复的生成和评估，限制了未来研究。

Method: 提出GEM - Bench，包含三个涵盖聊天机器人和搜索场景的数据集、捕获用户满意度和参与度多维度的度量本体，以及在可扩展多智能体框架内实现的多个基线解决方案。

Result: 简单基于提示的方法能实现合理参与度但降低用户满意度；基于预生成无广告回复插入广告的方法可缓解此问题但增加额外开销。

Conclusion: 需要未来研究设计更有效和高效的GEM广告注入回复生成解决方案。

Abstract: Generative Engine Marketing (GEM) is an emerging ecosystem for monetizing
generative engines, such as LLM-based chatbots, by seamlessly integrating
relevant advertisements into their responses. At the core of GEM lies the
generation and evaluation of ad-injected responses. However, existing
benchmarks are not specifically designed for this purpose, which limits future
research. To address this gap, we propose GEM-Bench, the first comprehensive
benchmark for ad-injected response generation in GEM. GEM-Bench includes three
curated datasets covering both chatbot and search scenarios, a metric ontology
that captures multiple dimensions of user satisfaction and engagement, and
several baseline solutions implemented within an extensible multi-agent
framework. Our preliminary results indicate that, while simple prompt-based
methods achieve reasonable engagement such as click-through rate, they often
reduce user satisfaction. In contrast, approaches that insert ads based on
pre-generated ad-free responses help mitigate this issue but introduce
additional overhead. These findings highlight the need for future research on
designing more effective and efficient solutions for generating ad-injected
responses in GEM.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [49] [Unified Spatiotemopral Physics-Informed Learning (USPIL): A Framework for Modeling Complex Predator-Prey Dynamics](https://arxiv.org/abs/2509.13425)
*Julian Evan Chrisnanto,Yulison Herry Chrisnanto,Ferry Faizal*

Main category: cs.LG

TL;DR: 提出USPIL框架用于多尺度生态建模，应用于Lotka - Volterra系统表现良好，有计算速度优势和新应用潜力。


<details>
  <summary>Details</summary>
Motivation: 生态系统复杂多尺度动态挑战传统建模，需新方法捕捉时空模式并遵循守恒原则。

Method: 提出USPIL框架，集成物理信息神经网络和守恒定律，用自动微分和自适应损失加权。

Result: 应用于Lotka - Volterra系统，1D时间动态相关度98.9%，2D系统捕获复杂螺旋波，验证守恒律误差在0.5%内，推理计算速度提升10 - 50倍。

Conclusion: USPIL是生态预测、保护规划等的变革性工具，确立物理信息深度学习范式。

Abstract: Ecological systems exhibit complex multi-scale dynamics that challenge
traditional modeling. New methods must capture temporal oscillations and
emergent spatiotemporal patterns while adhering to conservation principles. We
present the Unified Spatiotemporal Physics-Informed Learning (USPIL) framework,
a deep learning architecture integrating physics-informed neural networks
(PINNs) and conservation laws to model predator-prey dynamics across
dimensional scales. The framework provides a unified solution for both ordinary
(ODE) and partial (PDE) differential equation systems, describing temporal
cycles and reaction-diffusion patterns within a single neural network
architecture. Our methodology uses automatic differentiation to enforce physics
constraints and adaptive loss weighting to balance data fidelity with physical
consistency. Applied to the Lotka-Volterra system, USPIL achieves 98.9%
correlation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and captures
complex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94).
Validation confirms conservation law adherence within 0.5% and shows a 10-50x
computational speedup for inference compared to numerical solvers. USPIL also
enables mechanistic understanding through interpretable physics constraints,
facilitating parameter discovery and sensitivity analysis not possible with
purely data-driven methods. Its ability to transition between dimensional
formulations opens new avenues for multi-scale ecological modeling. These
capabilities make USPIL a transformative tool for ecological forecasting,
conservation planning, and understanding ecosystem resilience, establishing
physics-informed deep learning as a powerful and scientifically rigorous
paradigm.

</details>


### [50] [An Analysis of Optimizer Choice on Energy Efficiency and Performance in Neural Network Training](https://arxiv.org/abs/2509.13516)
*Tom Almog*

Main category: cs.LG

TL;DR: 文章对神经网络训练中优化器选择与能源效率的关系进行实证研究，发现不同优化器在训练速度、准确性和环境影响上存在权衡，为平衡性能与可持续性提供见解。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型复杂度和计算需求增加，理解训练决策对环境的影响对可持续AI发展至关重要。

Method: 在三个基准数据集上，使用八种流行优化器，每种设15个随机种子，进行360次对照实验，用CodeCarbon在Apple M1 Pro硬件上精确跟踪能源使用情况，测量训练时长、峰值内存使用、二氧化碳排放和最终模型性能。

Result: 不同数据集和模型复杂度下，训练速度、准确性和环境影响存在显著权衡；AdamW和NAdam始终高效，SGD在复杂数据集上性能优越但排放较高。

Conclusion: 研究结果为机器学习工作流中平衡性能和可持续性的从业者提供了可行的见解。

Abstract: As machine learning models grow increasingly complex and computationally
demanding, understanding the environmental impact of training decisions becomes
critical for sustainable AI development. This paper presents a comprehensive
empirical study investigating the relationship between optimizer choice and
energy efficiency in neural network training. We conducted 360 controlled
experiments across three benchmark datasets (MNIST, CIFAR-10, CIFAR-100) using
eight popular optimizers (SGD, Adam, AdamW, RMSprop, Adagrad, Adadelta, Adamax,
NAdam) with 15 random seeds each. Using CodeCarbon for precise energy tracking
on Apple M1 Pro hardware, we measured training duration, peak memory usage,
carbon dioxide emissions, and final model performance. Our findings reveal
substantial trade-offs between training speed, accuracy, and environmental
impact that vary across datasets and model complexity. We identify AdamW and
NAdam as consistently efficient choices, while SGD demonstrates superior
performance on complex datasets despite higher emissions. These results provide
actionable insights for practitioners seeking to balance performance and
sustainability in machine learning workflows.

</details>


### [51] [Learning Nonlinear Responses in PET Bottle Buckling with a Hybrid DeepONet-Transolver Framework](https://arxiv.org/abs/2509.13520)
*Varun Kumar,Jing Bi,Cyril Ngo Ngoc,Victor Oancea,George Em Karniadakis*

Main category: cs.LG

TL;DR: 提出混合DeepONet - Transolver框架用于PET瓶屈曲分析，评估其在不同瓶几何形状上的性能，展示了框架作为替代模型的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有解决偏微分方程问题的方法在不同非参数几何域上泛化能力有限，PET瓶屈曲分析传统方法计算成本高。

Method: 引入混合DeepONet - Transolver框架，用Abaqus进行非线性有限元分析生成训练数据，在两类瓶几何形状上评估。

Result: 四参数瓶族位移场平均相对$L^{2}$误差2.5 - 13%，时变反作用力约2.4%，点误差分析显示位移绝对误差在$10^{-4}-10^{-3}$，能准确捕捉关键物理现象。

Conclusion: 框架有潜力成为可扩展、计算高效的替代模型，适用于计算力学多任务预测和快速设计评估。

Abstract: Neural surrogates and operator networks for solving partial differential
equation (PDE) problems have attracted significant research interest in recent
years. However, most existing approaches are limited in their ability to
generalize solutions across varying non-parametric geometric domains. In this
work, we address this challenge in the context of Polyethylene Terephthalate
(PET) bottle buckling analysis, a representative packaging design problem
conventionally solved using computationally expensive finite element analysis
(FEA). We introduce a hybrid DeepONet-Transolver framework that simultaneously
predicts nodal displacement fields and the time evolution of reaction forces
during top load compression. Our methodology is evaluated on two families of
bottle geometries parameterized by two and four design variables. Training data
is generated using nonlinear FEA simulations in Abaqus for 254 unique designs
per family. The proposed framework achieves mean relative $L^{2}$ errors of
2.5-13% for displacement fields and approximately 2.4% for time-dependent
reaction forces for the four-parameter bottle family. Point-wise error analyses
further show absolute displacement errors on the order of $10^{-4}$-$10^{-3}$,
with the largest discrepancies confined to localized geometric regions.
Importantly, the model accurately captures key physical phenomena, such as
buckling behavior, across diverse bottle geometries. These results highlight
the potential of our framework as a scalable and computationally efficient
surrogate, particularly for multi-task predictions in computational mechanics
and applications requiring rapid design evaluation.

</details>


### [52] [AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions](https://arxiv.org/abs/2509.13523)
*Väinö Hatanpää,Eugene Ku,Jason Stock,Murali Emani,Sam Foreman,Chunyong Jung,Sandeep Madireddy,Tung Nguyen,Varuni Sastry,Ray A. O. Sinurat,Sam Wheeler,Huihuo Zheng,Troy Arcomano,Venkatram Vishwanath,Rao Kotamarthi*

Main category: cs.LG

TL;DR: 本文介绍AERIS模型和SWiPe技术，在气象预测中表现良好，凸显十亿参数扩散模型潜力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散方法在高分辨率下难以稳定扩展，需新方法解决。

Method: 引入AERIS模型和SWiPe技术。

Result: AERIS在Aurora上有高计算性能，有高缩放效率，表现优于IFS ENS且在90天季节尺度稳定。

Conclusion: 十亿参数扩散模型在气象和气候预测中有潜力。

Abstract: Generative machine learning offers new opportunities to better understand
complex Earth system dynamics. Recent diffusion-based methods address spectral
biases and improve ensemble calibration in weather forecasting compared to
deterministic methods, yet have so far proven difficult to scale stably at high
resolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin
diffusion transformer to address this gap, and SWiPe, a generalizable technique
that composes window parallelism with sequence and pipeline parallelism to
shard window-based transformers without added communication cost or increased
global batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS
(mixed precision) and a peak performance of 11.21 ExaFLOPS with $1 \times 1$
patch size on the 0.25{\deg} ERA5 dataset, achieving 95.5% weak scaling
efficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS
and remains stable on seasonal scales to 90 days, highlighting the potential of
billion-parameter diffusion models for weather and climate prediction.

</details>


### [53] [Towards a Physics Foundation Model](https://arxiv.org/abs/2509.13805)
*Florian Wiesner,Matthias Wessling,Stephen Baek*

Main category: cs.LG

TL;DR: 提出通用物理变压器（GPhyT），展示物理领域基础模型能力，有三项关键突破，为通用物理基础模型开辟道路。


<details>
  <summary>Details</summary>
Motivation: 当前物理感知机器学习方法局限于单一狭窄领域且需为新系统重新训练，期望获得物理基础模型，实现高保真模拟普及、加速科学发现和消除专用求解器开发需求。

Method: 基于1.8TB多样模拟数据训练通用物理变压器（GPhyT），让变换器从上下文学习推断控制动力学。

Result: GPhyT有三项突破：跨多物理领域性能优越，比专用架构高29倍；通过上下文学习零样本泛化到未见物理系统；50时间步长滚动实现稳定长期预测。

Conclusion: 单个模型可从数据学习通用物理原理，为通用物理基础模型变革计算科学和工程开辟道路。

Abstract: Foundation models have revolutionized natural language processing through a
``train once, deploy anywhere'' paradigm, where a single pre-trained model
adapts to countless downstream tasks without retraining. Access to a Physics
Foundation Model (PFM) would be transformative -- democratizing access to
high-fidelity simulations, accelerating scientific discovery, and eliminating
the need for specialized solver development. Yet current physics-aware machine
learning approaches remain fundamentally limited to single, narrow domains and
require retraining for each new system. We present the General Physics
Transformer (GPhyT), trained on 1.8 TB of diverse simulation data, that
demonstrates foundation model capabilities are achievable for physics. Our key
insight is that transformers can learn to infer governing dynamics from
context, enabling a single model to simulate fluid-solid interactions, shock
waves, thermal convection, and multi-phase dynamics without being told the
underlying equations. GPhyT achieves three critical breakthroughs: (1) superior
performance across multiple physics domains, outperforming specialized
architectures by up to 29x, (2) zero-shot generalization to entirely unseen
physical systems through in-context learning, and (3) stable long-term
predictions through 50-timestep rollouts. By establishing that a single model
can learn generalizable physical principles from data alone, this work opens
the path toward a universal PFM that could transform computational science and
engineering.

</details>


### [54] [Meta-Learning Linear Models for Molecular Property Prediction](https://arxiv.org/abs/2509.13527)
*Yulia Pimonova,Michael G. Taylor,Alice Allen,Ping Yang,Nicholas Lubbers*

Main category: cs.LG

TL;DR: 为应对化学结构 - 性质关系研究中数据挑战，引入LAMeL算法，在多属性预测上提升准确率并保持可解释性，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 化学领域高质量数据集有限，机器学习对数据需求增加，且需要可解释AI以平衡预测准确性和人类可理解性。

Method: 引入LAMeL线性元学习算法，利用元学习框架识别相关任务间共享模型参数，学习通用功能流形为新任务提供起点。

Result: LAMeL在不同数据集领域性能比标准岭回归提升1.1 - 25倍，在各任务中始终优于或匹配传统线性方法。

Conclusion: LAMeL是化学性质预测中兼顾准确性和可解释性的可靠工具。

Abstract: Chemists in search of structure-property relationships face great challenges
due to limited high quality, concordant datasets. Machine learning (ML) has
significantly advanced predictive capabilities in chemical sciences, but these
modern data-driven approaches have increased the demand for data. In response
to the growing demand for explainable AI (XAI) and to bridge the gap between
predictive accuracy and human comprehensibility, we introduce LAMeL - a Linear
Algorithm for Meta-Learning that preserves interpretability while improving the
prediction accuracy across multiple properties. While most approaches treat
each chemical prediction task in isolation, LAMeL leverages a meta-learning
framework to identify shared model parameters across related tasks, even if
those tasks do not share data, allowing it to learn a common functional
manifold that serves as a more informed starting point for new unseen tasks.
Our method delivers performance improvements ranging from 1.1- to 25-fold over
standard ridge regression, depending on the domain of the dataset. While the
degree of performance enhancement varies across tasks, LAMeL consistently
outperforms or matches traditional linear methods, making it a reliable tool
for chemical property prediction where both accuracy and interpretability are
critical.

</details>


### [55] [Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection](https://arxiv.org/abs/2509.13608)
*Niruthiha Selvanayagam,Ted Kurti*

Main category: cs.LG

TL;DR: 本文对OpenAI的GPT - 4o mini进行多模态仇恨言论检测分析，发现“单模态瓶颈”问题，凸显当前LMMs能力与安全的矛盾。


<details>
  <summary>Details</summary>
Motivation: 随着大跨模态模型融入日常生活，理解其安全架构对AI对齐至关重要，因此分析GPT - 4o mini的多模态仇恨言论检测能力。

Method: 使用Hateful Memes Challenge数据集，对500个样本进行多阶段调查。

Result: 发现“单模态瓶颈”架构缺陷，内容政策拒绝中有50%由单模态视觉内容触发，50%由文本内容触发，安全系统脆弱，会产生误判。

Conclusion: 当前最先进的LMMs在能力和安全之间存在根本矛盾，需要更集成、上下文感知的对齐策略。

Abstract: As Large Multimodal Models (LMMs) become integral to daily digital life,
understanding their safety architectures is a critical problem for AI
Alignment. This paper presents a systematic analysis of OpenAI's GPT-4o mini, a
globally deployed model, on the difficult task of multimodal hate speech
detection. Using the Hateful Memes Challenge dataset, we conduct a multi-phase
investigation on 500 samples to probe the model's reasoning and failure modes.
Our central finding is the experimental identification of a "Unimodal
Bottleneck," an architectural flaw where the model's advanced multimodal
reasoning is systematically preempted by context-blind safety filters. A
quantitative validation of 144 content policy refusals reveals that these
overrides are triggered in equal measure by unimodal visual 50% and textual 50%
content. We further demonstrate that this safety system is brittle, blocking
not only high-risk imagery but also benign, common meme formats, leading to
predictable false positives. These findings expose a fundamental tension
between capability and safety in state-of-the-art LMMs, highlighting the need
for more integrated, context-aware alignment strategies to ensure AI systems
can be deployed both safely and effectively.

</details>


### [56] [Sequential Data Augmentation for Generative Recommendation](https://arxiv.org/abs/2509.13648)
*Geon Lee,Bhuvesh Kumar,Clark Mingxuan Ju,Tong Zhao,Kijung Shin,Neil Shah,Liam Collins*

Main category: cs.LG

TL;DR: 文章分析生成式推荐模型训练中的数据增强问题，提出GenPAS框架，实验表明其效果优于现有策略。


<details>
  <summary>Details</summary>
Motivation: 现有工作对数据增强缺乏系统理解，不同增强策略性能差异大，需深入分析其对训练分布的影响。

Method: 提出GenPAS框架，将增强建模为输入 - 目标对的随机抽样过程，包含序列抽样、目标抽样和输入抽样三个步骤。

Result: 在基准和工业数据集上实验，GenPAS在准确性、数据效率和参数效率上优于现有策略。

Conclusion: GenPAS为生成式推荐中有原则的训练数据构建提供了实用指导。

Abstract: Generative recommendation plays a crucial role in personalized systems,
predicting users' future interactions from their historical behavior sequences.
A critical yet underexplored factor in training these models is data
augmentation, the process of constructing training data from user interaction
histories. By shaping the training distribution, data augmentation directly and
often substantially affects model generalization and performance. Nevertheless,
in much of the existing work, this process is simplified, applied
inconsistently, or treated as a minor design choice, without a systematic and
principled understanding of its effects.
  Motivated by our empirical finding that different augmentation strategies can
yield large performance disparities, we conduct an in-depth analysis of how
they reshape training distributions and influence alignment with future targets
and generalization to unseen inputs. To systematize this design space, we
propose GenPAS, a generalized and principled framework that models augmentation
as a stochastic sampling process over input-target pairs with three
bias-controlled steps: sequence sampling, target sampling, and input sampling.
This formulation unifies widely used strategies as special cases and enables
flexible control of the resulting training distribution. Our extensive
experiments on benchmark and industrial datasets demonstrate that GenPAS yields
superior accuracy, data efficiency, and parameter efficiency compared to
existing strategies, providing practical guidance for principled training data
construction in generative recommendation.

</details>


### [57] [Unsupervised Anomaly Detection in ALS EPICS Event Logs](https://arxiv.org/abs/2509.13621)
*Antonin Sulc,Thorsten Hellert,Steven Hunt*

Main category: cs.LG

TL;DR: 本文介绍用于ALS的自动故障分析框架，处理实时事件日志，用语义嵌入技术转换日志，用神经网络打分以识别故障前关键事件序列。


<details>
  <summary>Details</summary>
Motivation: 为先进光源（ALS）开发自动化的故障分析框架，帮助操作员快速识别复杂系统故障前的关键事件序列。

Method: 将日志条目视为自然语言，使用语义嵌入技术将其转换为上下文向量表示，用正常运行数据训练顺序感知神经网络，为每个事件分配实时异常分数。

Result: 能够标记出与基线行为的偏差。

Conclusion: 该方法可使操作员快速识别复杂系统故障前的关键事件序列。

Abstract: This paper introduces an automated fault analysis framework for the Advanced
Light Source (ALS) that processes real-time event logs from its EPICS control
system. By treating log entries as natural language, we transform them into
contextual vector representations using semantic embedding techniques. A
sequence-aware neural network, trained on normal operational data, assigns a
real-time anomaly score to each event. This method flags deviations from
baseline behavior, enabling operators to rapidly identify the critical event
sequences that precede complex system failures.

</details>


### [58] [Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics](https://arxiv.org/abs/2509.14225)
*Benjamin Sterling,Yousef El-Laham,Mónica F. Bugallo*

Main category: cs.LG

TL;DR: 本文聚焦防御扩散模型的成员推理攻击，提出用临界阻尼高阶朗之万动力学防御，进行理论研究并在数据集验证。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能应用发展引发新的数据安全担忧，扩散模型虽对成员推理攻击有一定抗性但仍易受攻击。

Method: 利用临界阻尼高阶朗之万动力学，引入辅助变量和联合扩散过程，用外部随机性破坏敏感输入数据。

Result: 在玩具数据集和语音数据集上，用AUROC曲线和FID指标验证该防御方法。

Conclusion: 未明确提及，但从验证情况推测所提防御方法有一定效果。

Abstract: Recent advances in generative artificial intelligence applications have
raised new data security concerns. This paper focuses on defending diffusion
models against membership inference attacks. This type of attack occurs when
the attacker can determine if a certain data point was used to train the model.
Although diffusion models are intrinsically more resistant to membership
inference attacks than other generative models, they are still susceptible. The
defense proposed here utilizes critically-damped higher-order Langevin
dynamics, which introduces several auxiliary variables and a joint diffusion
process along these variables. The idea is that the presence of auxiliary
variables mixes external randomness that helps to corrupt sensitive input data
earlier on in the diffusion process. This concept is theoretically investigated
and validated on a toy dataset and a speech dataset using the Area Under the
Receiver Operating Characteristic (AUROC) curves and the FID metric.

</details>


### [59] [Privacy-Aware In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.13625)
*Bishnu Bhusal,Manoj Acharya,Ramneet Kaur,Colin Samplawski,Anirban Roy,Adam D. Cobb,Rohit Chadha,Susmit Jha*

Main category: cs.LG

TL;DR: 本文提出用于生成高质量合成文本的隐私预测框架，在保护隐私同时提升效用，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在敏感信息泄露隐私问题，需解决信息泄露风险。

Method: 利用差分隐私框架，对私有记录进行推理并聚合每个token输出分布，还提出结合私有和公共推理的混合操作。

Result: 在上下文学习任务中，该方法优于之前的先进方法。

Conclusion: 此方法是在保持高效用的同时进行隐私保护文本生成的有前景方向。

Abstract: Large language models (LLMs) have significantly transformed natural language
understanding and generation, but they raise privacy concerns due to potential
exposure of sensitive information. Studies have highlighted the risk of
information leakage, where adversaries can extract sensitive information
embedded in the prompts. In this work, we introduce a novel private prediction
framework for generating high-quality synthetic text with strong privacy
guarantees. Our approach leverages the Differential Privacy (DP) framework to
ensure worst-case theoretical bounds on information leakage without requiring
any fine-tuning of the underlying models.The proposed method performs inference
on private records and aggregates the resulting per-token output distributions.
This enables the generation of longer and coherent synthetic text while
maintaining privacy guarantees. Additionally, we propose a simple blending
operation that combines private and public inference to further enhance
utility. Empirical evaluations demonstrate that our approach outperforms
previous state-of-the-art methods on in-context-learning (ICL) tasks, making it
a promising direction for privacy-preserving text generation while maintaining
high utility.

</details>


### [60] [ParaAegis: Parallel Protection for Flexible Privacy-preserved Federated Learning](https://arxiv.org/abs/2509.13739)
*Zihou Wu,Yuecheng Li,Tianchi Liao,Jian Lou,Chuan Chen*

Main category: cs.LG

TL;DR: 现有联邦学习保护机制缺乏灵活性，本文提出ParaAegis框架解决此问题，可灵活平衡隐私、效用和效率。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习保护机制（如DP和HE）在模型效用和计算效率间存在刚性权衡，缺乏灵活性，阻碍实际应用。

Method: 提出ParaAegis并行保护框架，采用战略模型分区方案，对模型低范数部分用轻量级DP，其余部分用HE保护，通过分布式投票机制达成分区共识。

Result: 理论分析证实可在相同隐私下调整效率和效用，实验表明调整超参数可灵活权衡模型准确性和训练时间。

Conclusion: ParaAegis框架能让从业者灵活控制隐私 - 效用 - 效率的平衡。

Abstract: Federated learning (FL) faces a critical dilemma: existing protection
mechanisms like differential privacy (DP) and homomorphic encryption (HE)
enforce a rigid trade-off, forcing a choice between model utility and
computational efficiency. This lack of flexibility hinders the practical
implementation. To address this, we introduce ParaAegis, a parallel protection
framework designed to give practitioners flexible control over the
privacy-utility-efficiency balance. Our core innovation is a strategic model
partitioning scheme. By applying lightweight DP to the less critical, low norm
portion of the model while protecting the remainder with HE, we create a
tunable system. A distributed voting mechanism ensures consensus on this
partitioning. Theoretical analysis confirms the adjustments between efficiency
and utility with the same privacy. Crucially, the experimental results
demonstrate that by adjusting the hyperparameters, our method enables flexible
prioritization between model accuracy and training time.

</details>


### [61] [DeepLogit: A sequentially constrained explainable deep learning modeling approach for transport policy analysis](https://arxiv.org/abs/2509.13633)
*Jeremy Oon,Rakhi Manohar Mepparambath,Ling Feng*

Main category: cs.LG

TL;DR: 本文开发DeepLogit模型用于交通政策分析，结合离散选择模型和AI模型优势，以新加坡公交卡数据验证，代码开源。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型因黑盒特性在规划和政策领域应用有挑战，需开发可解释且准确的模型用于交通政策分析。

Method: 采用顺序约束方法，先估计仅含线性项的CNN模型，再约束需解释参数值估计其他深度学习模型。

Result: 所提方法能保留所选参数可解释性，比离散选择模型有更高精度。

Conclusion: 理论离散选择模型和数据驱动AI模型可相互借鉴优势，该方法能在保持规划和政策领域适用性的同时提高模型准确性。

Abstract: Despite the significant progress of deep learning models in multitude of
applications, their adaption in planning and policy related areas remains
challenging due to the black-box nature of these models. In this work, we
develop a set of DeepLogit models that follow a novel sequentially constrained
approach in estimating deep learning models for transport policy analysis. In
the first step of the proposed approach, we estimate a convolutional neural
network (CNN) model with only linear terms, which is equivalent of a
linear-in-parameter multinomial logit model. We then estimate other deep
learning models by constraining the parameters that need interpretability at
the values obtained in the linear-in-parameter CNN model and including higher
order terms or by introducing advanced deep learning architectures like
Transformers. Our approach can retain the interpretability of the selected
parameters, yet provides significantly improved model accuracy than the
discrete choice model. We demonstrate our approach on a transit route choice
example using real-world transit smart card data from Singapore. This study
shows the potential for a unifying approach, where theory-based discrete choice
model (DCM) and data-driven AI models can leverage each other's strengths in
interpretability and predictive power. With the availability of larger datasets
and more complex constructions, such approach can lead to more accurate models
using discrete choice models while maintaining its applicability in planning
and policy-related areas. Our code is available on
https://github.com/jeremyoon/route-choice/ .

</details>


### [62] [Graph-Regularized Learning of Gaussian Mixture Models](https://arxiv.org/abs/2509.13855)
*Shamsiiat Abdurakhmanova,Alex Jung*

Main category: cs.LG

TL;DR: 提出分布式环境下基于图正则化的高斯混合模型学习方法，利用相似图引导参数共享，在异构小样本场景表现优。


<details>
  <summary>Details</summary>
Motivation: 解决分布式环境中异构且有限本地数据下高斯混合模型学习问题。

Method: 利用给定的相似图引导节点间参数共享，避免原始数据传输。

Result: 得到的模型能灵活聚合邻居参数，在异构小样本场景中表现优于集中式和本地训练的高斯混合模型。

Conclusion: 所提方法在分布式异构小样本数据下有效，可用于高斯混合模型学习。

Abstract: We present a graph-regularized learning of Gaussian Mixture Models (GMMs) in
distributed settings with heterogeneous and limited local data. The method
exploits a provided similarity graph to guide parameter sharing among nodes,
avoiding the transfer of raw data. The resulting model allows for flexible
aggregation of neighbors' parameters and outperforms both centralized and
locally trained GMMs in heterogeneous, low-sample regimes.

</details>


### [63] [Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs](https://arxiv.org/abs/2509.13634)
*Md Bokhtiar Al Zami,Md Raihan Uddin,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: 本文提出结合数字孪生（DT）技术和零知识联邦学习（zkFed）的框架，解决无人机辅助联邦学习系统的能耗、通信和安全问题，降低能耗，提升性能。


<details>
  <summary>Details</summary>
Motivation: 确保无人机辅助联邦学习系统可靠运行，解决能耗过高、通信低效和安全漏洞等问题。

Method: 提出结合DT技术和zkFed的框架，无人机作移动基站，利用DT技术实现实时监控和预测性维护，用零知识证明增强安全，引入动态分配策略，使用块坐标下降和凸优化技术。

Result: 相比传统联邦学习方法，系统能耗最多降低29.6%，模拟结果显示学习性能、安全性和可扩展性得到提升。

Conclusion: 该框架是下一代基于无人机的智能网络的有前景的解决方案。

Abstract: Federated learning (FL) has gained popularity as a privacy-preserving method
of training machine learning models on decentralized networks. However to
ensure reliable operation of UAV-assisted FL systems, issues like as excessive
energy consumption, communication inefficiencies, and security vulnerabilities
must be solved. This paper proposes an innovative framework that integrates
Digital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to
tackle these challenges. UAVs act as mobile base stations, allowing scattered
devices to train FL models locally and upload model updates for aggregation. By
incorporating DT technology, our approach enables real-time system monitoring
and predictive maintenance, improving UAV network efficiency. Additionally,
Zero-Knowledge Proofs (ZKPs) strengthen security by allowing model verification
without exposing sensitive data. To optimize energy efficiency and resource
management, we introduce a dynamic allocation strategy that adjusts UAV flight
paths, transmission power, and processing rates based on network conditions.
Using block coordinate descent and convex optimization techniques, our method
significantly reduces system energy consumption by up to 29.6% compared to
conventional FL approaches. Simulation results demonstrate improved learning
performance, security, and scalability, positioning this framework as a
promising solution for next-generation UAV-based intelligent networks.

</details>


### [64] [Adaptive Client Selection via Q-Learning-based Whittle Index in Wireless Federated Learning](https://arxiv.org/abs/2509.13933)
*Qiyue Li,Yingxin Liu,Hang Qi,Jieping Luo,Zhizhang Liu,Jingjin Wu*

Main category: cs.LG

TL;DR: 针对无线联邦学习中的客户端选择问题，提出WILF - Q方法，实验表明其在学习效率上显著优于现有基线策略。


<details>
  <summary>Details</summary>
Motivation: 解决服务器无法观测客户端动态状态，以减少达到一定学习精度所需的总时间。

Method: 将客户端选择问题建模为 restless多臂老虎机问题，提出WILF - Q方法，用Q - learning自适应学习和更新与每个客户端相关的近似Whittle指数，选择指数最高的客户端。

Result: 实验结果显示WILF - Q在学习效率上显著优于现有基线策略。

Conclusion: WILF - Q是无线联邦学习中客户端选择的强大而高效的方法，无需明确的客户端状态转移或数据分布知识，适合实际部署。

Abstract: We consider the client selection problem in wireless Federated Learning (FL),
with the objective of reducing the total required time to achieve a certain
level of learning accuracy. Since the server cannot observe the clients'
dynamic states that can change their computation and communication efficiency,
we formulate client selection as a restless multi-armed bandit problem. We
propose a scalable and efficient approach called the Whittle Index Learning in
Federated Q-learning (WILF-Q), which uses Q-learning to adaptively learn and
update an approximated Whittle index associated with each client, and then
selects the clients with the highest indices. Compared to existing approaches,
WILF-Q does not require explicit knowledge of client state transitions or data
distributions, making it well-suited for deployment in practical FL settings.
Experiment results demonstrate that WILF-Q significantly outperforms existing
baseline policies in terms of learning efficiency, providing a robust and
efficient approach to client selection in wireless FL.

</details>


### [65] [Multimodal signal fusion for stress detection using deep neural networks: a novel approach for converting 1D signals to unified 2D images](https://arxiv.org/abs/2509.13636)
*Yasin Hasanpoor,Bahram Tarvirdizadeh,Khalil Alipour,Mohammad Ghamari*

Main category: cs.LG

TL;DR: 提出将多模态生理信号转换为2D图像矩阵的新方法，提升压力检测效果，适用于多领域。


<details>
  <summary>Details</summary>
Motivation: 传统方法单独处理信号或依赖固定编码，不能有效捕捉时间和跨信号依赖，需改进压力检测方法。

Method: 将多模态生理信号转换为2D图像矩阵，用CNN处理；系统地将融合信号重组为多种格式，在多阶段训练管道中组合。

Result: 显著提升了分类性能。

Conclusion: 该方法适用于涉及多模态生理信号的任何领域，为可穿戴技术的健康监测提供支持。

Abstract: This study introduces a novel method that transforms multimodal physiological
signalsphotoplethysmography (PPG), galvanic skin response (GSR), and
acceleration (ACC) into 2D image matrices to enhance stress detection using
convolutional neural networks (CNNs). Unlike traditional approaches that
process these signals separately or rely on fixed encodings, our technique
fuses them into structured image representations that enable CNNs to capture
temporal and cross signal dependencies more effectively. This image based
transformation not only improves interpretability but also serves as a robust
form of data augmentation. To further enhance generalization and model
robustness, we systematically reorganize the fused signals into multiple
formats, combining them in a multi stage training pipeline. This approach
significantly boosts classification performance. While demonstrated here in the
context of stress detection, the proposed method is broadly applicable to any
domain involving multimodal physiological signals, paving the way for more
accurate, personalized, and real time health monitoring through wearable
technologies.

</details>


### [66] [LLM-I: LLMs are Naturally Interleaved Multimodal Creators](https://arxiv.org/abs/2509.13642)
*Zirun Guo,Feng Zhang,Kai Jia,Tao Jin*

Main category: cs.LG

TL;DR: 提出LLM - I框架，将交错图像文本生成视为工具使用问题，用RL训练，表现超现有方法，还有测试时缩放策略。


<details>
  <summary>Details</summary>
Motivation: 克服当前统一模型的“单工具”瓶颈，解决其在合成图像及需要事实依据或编程精度任务上的局限。

Method: 设计LLM - I框架，让中心LLM或MLLM代理智能编排多种视觉工具，通过结合规则逻辑和LLM、MLLM评估器判断的混合奖励系统的强化学习框架训练代理。

Result: 在四个基准测试中大幅超越现有方法，采用新测试时缩放策略进一步提升性能。

Conclusion: LLM - I框架在图像文本生成任务中表现出色，具有良好的应用前景。

Abstract: We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that
reframes interleaved image-text generation as a tool-use problem. LLM-I is
designed to overcome the "one-tool" bottleneck of current unified models, which
are limited to synthetic imagery and struggle with tasks requiring factual
grounding or programmatic precision. Our framework empowers a central LLM or
MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual
tools, including online image search, diffusion-based generation, code
execution, and image editing. The agent is trained to select and apply these
tools proficiently via a Reinforcement Learning (RL) framework that features a
hybrid reward system combining rule-based logic with judgments from LLM and
MLLM evaluators. Trained on a diverse new dataset using four different model
backbones, LLM-I demonstrates state-of-the-art performance, outperforming
existing methods by a large margin across four benchmarks. We also introduce a
novel test-time scaling strategy that provides further performance gains.
Project Page: https://github.com/ByteDance-BandAI/LLM-I.

</details>


### [67] [Controllable Pareto Trade-off between Fairness and Accuracy](https://arxiv.org/abs/2509.13651)
*Yongkang Du,Jieyu Zhao,Yijun Yang,Tianyi Zhou*

Main category: cs.LG

TL;DR: 本文针对NLP任务中公平性 - 准确性权衡问题，提出CPT方法，实验表明其能在帕累托前沿获得高质量解，且可控性更好。


<details>
  <summary>Details</summary>
Motivation: 当前工作在解决NLP任务中公平性与准确性权衡时，仅关注单一'最优'解，未考虑帕累托前沿的多样解，本文旨在根据用户偏好提供可控权衡。

Method: 应用多目标优化（MOO），提出可控帕累托权衡（CPT）方法，包括用随机梯度移动平均稳定公平性更新、修剪梯度保留关键参数梯度。

Result: 在仇恨言论检测和职业分类任务上评估，CPT在帕累托前沿获得的解质量高于基线方法，可控性更好，能精确遵循人为定义的参考向量。

Conclusion: CPT方法能有效解决NLP任务中公平性 - 准确性的可控权衡问题。

Abstract: The fairness-accuracy trade-off is a key challenge in NLP tasks. Current work
focuses on finding a single "optimal" solution to balance the two objectives,
which is limited considering the diverse solutions on the Pareto front. This
work intends to provide controllable trade-offs according to the user's
preference of the two objectives, which is defined as a reference vector. To
achieve this goal, we apply multi-objective optimization (MOO), which can find
solutions from various regions of the Pareto front. However, it is challenging
to precisely control the trade-off due to the stochasticity of the training
process and the high dimentional gradient vectors. Thus, we propose
Controllable Pareto Trade-off (CPT) that can effectively train models to
perform different trade-offs according to users' preferences. CPT 1) stabilizes
the fairness update with a moving average of stochastic gradients to determine
the update direction, and 2) prunes the gradients by only keeping the gradients
of the critical parameters. We evaluate CPT on hate speech detection and
occupation classification tasks. Experiments show that CPT can achieve a
higher-quality set of solutions on the Pareto front than the baseline methods.
It also exhibits better controllability and can precisely follow the
human-defined reference vectors.

</details>


### [68] [RF-LSCM: Pushing Radiance Fields to Multi-Domain Localized Statistical Channel Modeling for Cellular Network Optimization](https://arxiv.org/abs/2509.13686)
*Bingsheng Peng,Shutao Zhang,Xi Zheng,Ye Xue,Xinyu Qin,Tsung-Hui Chang*

Main category: cs.LG

TL;DR: 提出RF - LSCM框架用于无线信道建模，解决传统方法局限，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统LSCM方法在单小区、单网格、单载波频率分析及捕捉跨域交互上存在局限，需新方法解决。

Method: 提出RF - LSCM框架，引入多域LSCM公式、FDAM模型、点云辅助环境增强方法，利用低秩张量表示和HiTAM算法。

Result: 在真实多小区数据集上实验，RF - LSCM显著优于现有方法，覆盖预测MAE最多降30%，融合多频数据MAE提升22%。

Conclusion: RF - LSCM能有效解决传统方法局限，提升无线信道建模性能。

Abstract: Accurate localized wireless channel modeling is a cornerstone of cellular
network optimization, enabling reliable prediction of network performance
during parameter tuning. Localized statistical channel modeling (LSCM) is the
state-of-the-art channel modeling framework tailored for cellular network
optimization. However, traditional LSCM methods, which infer the channel's
Angular Power Spectrum (APS) from Reference Signal Received Power (RSRP)
measurements, suffer from critical limitations: they are typically confined to
single-cell, single-grid and single-carrier frequency analysis and fail to
capture complex cross-domain interactions. To overcome these challenges, we
propose RF-LSCM, a novel framework that models the channel APS by jointly
representing large-scale signal attenuation and multipath components within a
radiance field. RF-LSCM introduces a multi-domain LSCM formulation with a
physics-informed frequency-dependent Attenuation Model (FDAM) to facilitate the
cross frequency generalization as well as a point-cloud-aided environment
enhanced method to enable multi-cell and multi-grid channel modeling.
Furthermore, to address the computational inefficiency of typical neural
radiance fields, RF-LSCM leverages a low-rank tensor representation,
complemented by a novel Hierarchical Tensor Angular Modeling (HiTAM) algorithm.
This efficient design significantly reduces GPU memory requirements and
training time while preserving fine-grained accuracy. Extensive experiments on
real-world multi-cell datasets demonstrate that RF-LSCM significantly
outperforms state-of-the-art methods, achieving up to a 30% reduction in mean
absolute error (MAE) for coverage prediction and a 22% MAE improvement by
effectively fusing multi-frequency data.

</details>


### [69] [A Conformal Prediction Framework for Uncertainty Quantification in Physics-Informed Neural Networks](https://arxiv.org/abs/2509.13717)
*Yifan Yu,Cheuk Hin Ho,Yangshuai Wang*

Main category: cs.LG

TL;DR: 本文提出用于PINNs中UQ的无分布共形预测框架，经评估表现优于启发式UQ方法，增强了校准和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有PINNs的不确定性量化（UQ）方法缺乏严格统计保证，需改进。

Method: 引入无分布共形预测（CP）框架，通过在校准集上构建非一致性分数校准预测区间；引入局部共形分位数估计处理空间异方差性。

Result: 在典型PDEs上的系统评估和多不确定性指标测试表明，该框架实现可靠校准和局部自适应不确定性区间，优于启发式UQ方法。

Conclusion: 该工作将PINNs与无分布UQ结合，不仅增强校准和可靠性，还为复杂PDE系统的不确定性感知建模开辟新途径。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving PDEs, yet existing uncertainty quantification (UQ) approaches for
PINNs generally lack rigorous statistical guarantees. In this work, we bridge
this gap by introducing a distribution-free conformal prediction (CP) framework
for UQ in PINNs. This framework calibrates prediction intervals by constructing
nonconformity scores on a calibration set, thereby yielding distribution-free
uncertainty estimates with rigorous finite-sample coverage guarantees for
PINNs. To handle spatial heteroskedasticity, we further introduce local
conformal quantile estimation, enabling spatially adaptive uncertainty bands
while preserving theoretical guarantee. Through systematic evaluations on
typical PDEs (damped harmonic oscillator, Poisson, Allen-Cahn, and Helmholtz
equations) and comprehensive testing across multiple uncertainty metrics, our
results demonstrate that the proposed framework achieves reliable calibration
and locally adaptive uncertainty intervals, consistently outperforming
heuristic UQ approaches. By bridging PINNs with distribution-free UQ, this work
introduces a general framework that not only enhances calibration and
reliability, but also opens new avenues for uncertainty-aware modeling of
complex PDE systems.

</details>


### [70] [WatchAnxiety: A Transfer Learning Approach for State Anxiety Prediction from Smartwatch Data](https://arxiv.org/abs/2509.13725)
*Md Sabbir Ahmed,Noah French,Mark Rucker,Zhiyuan Wang,Taylor Myers-Brower,Kaitlyn Petz,Mehdi Boukhechba,Bethany A. Teachman,Laura E. Barnes*

Main category: cs.LG

TL;DR: 研究用智能手表系统对社交焦虑大学生进行研究，开发模型预测状态焦虑，在自有数据集和外部数据集上均取得不错准确率，优于先前工作。


<details>
  <summary>Details</summary>
Motivation: 社交焦虑在社交情境中会有状态焦虑波动，但此前少有研究测量或预测其全天波动，为设计实时个性化干预措施，需捕捉这些日内动态。

Method: 用定制智能手表系统对社交焦虑大学生进行研究，每日进行七次生态瞬时评估，开发基于外部心率数据的基础模型，迁移其表征并微调，结合特质水平测量在元学习器中进行预测。

Result: 在自有数据集上状态焦虑检测的平衡准确率达60.4%，在外部数据集上达59.1%，优于先前工作至少7%。

Conclusion: 所提出的方法能有效预测社交焦虑大学生的状态焦虑，具有一定的泛化能力。

Abstract: Social anxiety is a common mental health condition linked to significant
challenges in academic, social, and occupational functioning. A core feature is
elevated momentary (state) anxiety in social situations, yet little prior work
has measured or predicted fluctuations in this anxiety throughout the day.
Capturing these intra-day dynamics is critical for designing real-time,
personalized interventions such as Just-In-Time Adaptive Interventions
(JITAIs). To address this gap, we conducted a study with socially anxious
college students (N=91; 72 after exclusions) using our custom smartwatch-based
system over an average of 9.03 days (SD = 2.95). Participants received seven
ecological momentary assessments (EMAs) per day to report state anxiety. We
developed a base model on over 10,000 days of external heart rate data,
transferred its representations to our dataset, and fine-tuned it to generate
probabilistic predictions. These were combined with trait-level measures in a
meta-learner. Our pipeline achieved 60.4% balanced accuracy in state anxiety
detection in our dataset. To evaluate generalizability, we applied the training
approach to a separate hold-out set from the TILES-18 dataset-the same dataset
used for pretraining. On 10,095 once-daily EMAs, our method achieved 59.1%
balanced accuracy, outperforming prior work by at least 7%.

</details>


### [71] [State Space Models over Directed Graphs](https://arxiv.org/abs/2509.13735)
*Junzhi She,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 文章提出DirEgo2Token和DirGraphSSM，解决有向图学习问题，实验表现良好且训练速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有有向图GNN和图Transformer面临捕获长距离因果依赖和平衡精度与效率的挑战，现有图状态空间模型仅适用于无向图。

Method: 提出DirEgo2Token将有向图通过k跳自我图序列化，开发DirGraphSSM通过消息传递机制在有向图上实现状态空间模型。

Result: DirGraphSSM在三个有向图学习任务上达最优，在另外两个任务表现有竞争力，训练速度提升1.5 - 2倍。

Conclusion: 将状态空间模型系统扩展到有向图学习领域，所提方法有效且高效。

Abstract: Directed graphs are ubiquitous across numerous domains, where the
directionality of edges encodes critical causal dependencies. However, existing
GNNs and graph Transformers tailored for directed graphs face two major
challenges: (1) effectively capturing long-range causal dependencies derived
from directed edges; (2) balancing accuracy and training efficiency when
processing large-scale graph datasets. In recent years, state space models
(SSMs) have achieved substantial progress in causal sequence tasks, and their
variants designed for graphs have demonstrated state-of-the-art accuracy while
maintaining high efficiency across various graph learning benchmarks. However,
existing graph state space models are exclusively designed for undirected
graphs, which limits their performance in directed graph learning. To this end,
we propose an innovative approach DirEgo2Token which sequentializes directed
graphs via k-hop ego graphs. This marks the first systematic extension of state
space models to the field of directed graph learning. Building upon this, we
develop DirGraphSSM, a novel directed graph neural network architecture that
implements state space models on directed graphs via the message-passing
mechanism. Experimental results demonstrate that DirGraphSSM achieves
state-of-the-art performance on three representative directed graph learning
tasks while attaining competitive performance on two additional tasks with
1.5$\times $ to 2$\times $ training speed improvements compared to existing
state-of-the-art models.

</details>


### [72] [ST-LINK: Spatially-Aware Large Language Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.13753)
*Hyotaek Jeon,Hyunwook Lee,Juwon Kim,Sungahn Ko*

Main category: cs.LG

TL;DR: 提出ST - LINK框架增强大语言模型捕获时空依赖能力，实验表明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在捕获空间依赖方面存在挑战，建模空间关系有局限且与图结构空间数据不兼容。

Method: 引入ST - LINK框架，包含Spatially - Enhanced Attention（SE - Attention）和Memory Retrieval Feed - Forward Network（MRFFN）。SE - Attention扩展旋转位置嵌入，MRFFN动态检索利用历史模式。

Result: 在基准数据集上的综合实验显示，ST - LINK超越了传统深度学习和LLM方法，能有效捕获常规交通模式和突发变化。

Conclusion: ST - LINK框架能增强大语言模型捕获时空依赖的能力，可用于交通预测。

Abstract: Traffic forecasting represents a crucial problem within intelligent
transportation systems. In recent research, Large Language Models (LLMs) have
emerged as a promising method, but their intrinsic design, tailored primarily
for sequential token processing, introduces notable challenges in effectively
capturing spatial dependencies. Specifically, the inherent limitations of LLMs
in modeling spatial relationships and their architectural incompatibility with
graph-structured spatial data remain largely unaddressed. To overcome these
limitations, we introduce ST-LINK, a novel framework that enhances the
capability of Large Language Models to capture spatio-temporal dependencies.
Its key components are Spatially-Enhanced Attention (SE-Attention) and the
Memory Retrieval Feed-Forward Network (MRFFN). SE-Attention extends rotary
position embeddings to integrate spatial correlations as direct rotational
transformations within the attention mechanism. This approach maximizes spatial
learning while preserving the LLM's inherent sequential processing structure.
Meanwhile, MRFFN dynamically retrieves and utilizes key historical patterns to
capture complex temporal dependencies and improve the stability of long-term
forecasting. Comprehensive experiments on benchmark datasets demonstrate that
ST-LINK surpasses conventional deep learning and LLM approaches, and
effectively captures both regular traffic patterns and abrupt changes.

</details>


### [73] [Beyond Correlation: Causal Multi-View Unsupervised Feature Selection Learning](https://arxiv.org/abs/2509.13763)
*Zongxin Shen,Yanyong Huang,Bin Wang,Jinyuan Chang,Shiyu Liu,Tianrui Li*

Main category: cs.LG

TL;DR: 文章从因果视角分析多视图无监督特征选择（MUFS），提出CAUSA方法，实验表明其优于现有方法，是无监督下因果多视图特征选择的首次深入研究。


<details>
  <summary>Details</summary>
Motivation: 现有MUFS方法通过特征与聚类标签的相关性选择特征，需探究这种相关性是否足以指导特征选择。

Method: 引入结构因果模型分析问题，提出CAUSA方法，先使用广义无监督谱回归模型识别特征，再引入因果正则化模块分离混杂因素、学习样本权重，最后整合到统一框架。

Result: CAUSA在综合实验中优于多个现有方法。

Conclusion: CAUSA能有效选择因果信息特征，是无监督下因果多视图特征选择的首次深入研究。

Abstract: Multi-view unsupervised feature selection (MUFS) has recently received
increasing attention for its promising ability in dimensionality reduction on
multi-view unlabeled data. Existing MUFS methods typically select
discriminative features by capturing correlations between features and
clustering labels. However, an important yet underexplored question remains:
\textit{Are such correlations sufficiently reliable to guide feature
selection?} In this paper, we analyze MUFS from a causal perspective by
introducing a novel structural causal model, which reveals that existing
methods may select irrelevant features because they overlook spurious
correlations caused by confounders. Building on this causal perspective, we
propose a novel MUFS method called CAusal multi-view Unsupervised feature
Selection leArning (CAUSA). Specifically, we first employ a generalized
unsupervised spectral regression model that identifies informative features by
capturing dependencies between features and consensus clustering labels. We
then introduce a causal regularization module that can adaptively separate
confounders from multi-view data and simultaneously learn view-shared sample
weights to balance confounder distributions, thereby mitigating spurious
correlations. Thereafter, integrating both into a unified learning framework
enables CAUSA to select causally informative features. Comprehensive
experiments demonstrate that CAUSA outperforms several state-of-the-art
methods. To our knowledge, this is the first in-depth study of causal
multi-view feature selection in the unsupervised setting.

</details>


### [74] [Floating-Body Hydrodynamic Neural Networks](https://arxiv.org/abs/2509.13783)
*Tianshuo Zhang,Wenzhe Zhai,Rui Yann,Jia Gao,He Cao,Xianglei Xing*

Main category: cs.LG

TL;DR: 提出Floating - Body Hydrodynamic Neural Networks (FHNN)框架，能预测可解释的水动力参数，在合成涡旋数据集上比Neural ODEs误差低，比Hamiltonian和Lagrangian神经网络更有效处理耗散动力学。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒神经模型在对浮体运动的耗散动力学建模时存在可解释性有限和长时预测不稳定的问题。

Method: 提出FHNN框架，预测可解释的水动力参数并与解析运动方程耦合。

Result: 在合成涡旋数据集上，FHNN比Neural ODEs误差低一个数量级，能恢复物理上一致的流场；比Hamiltonian和Lagrangian神经网络更有效处理耗散动力学。

Conclusion: FHNN框架缩小了黑盒学习和透明系统识别之间的差距。

Abstract: Fluid-structure interaction is common in engineering and natural systems,
where floating-body motion is governed by added mass, drag, and background
flows. Modeling these dissipative dynamics is difficult: black-box neural
models regress state derivatives with limited interpretability and unstable
long-horizon predictions. We propose Floating-Body Hydrodynamic Neural Networks
(FHNN), a physics-structured framework that predicts interpretable hydrodynamic
parameters such as directional added masses, drag coefficients, and a
streamfunction-based flow, and couples them with analytic equations of motion.
This design constrains the hypothesis space, enhances interpretability, and
stabilizes integration. On synthetic vortex datasets, FHNN achieves up to an
order-of-magnitude lower error than Neural ODEs, recovers physically consistent
flow fields. Compared with Hamiltonian and Lagrangian neural networks, FHNN
more effectively handles dissipative dynamics while preserving
interpretability, which bridges the gap between black-box learning and
transparent system identification.

</details>


### [75] [Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment](https://arxiv.org/abs/2509.13818)
*Zheng-an Wang,Yanbo J. Wang,Jiachi Zhang,Qi Xu,Yilun Zhao,Jintao Li,Yipeng Zhang,Bo Yang,Xinkai Gao,Xiaofeng Cao,Kai Xu,Pengpeng Hao,Xuan Yang,Heng Fan*

Main category: cs.LG

TL;DR: 本文利用量子机器学习解决少样本信用风险评估问题，设计混合量子 - 经典工作流，在实际数据集上取得优于经典模型的结果，为量子计算在金融场景应用提供蓝图。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以解决少样本信用风险评估问题，数据稀缺和不平衡限制传统模型效果，需要新方法。

Method: 先使用经典机器学习模型进行特征工程和降维，再用参数转移规则训练的量子神经网络作为核心分类器，通过数值模拟评估并在量子云平台处理器上部署。

Result: 在 279 个样本的真实信用数据集上，模拟中 QNN 平均 AUC 为 0.852 +/- 0.027，硬件实验 AUC 为 0.88，超越经典基准模型，召回率表现出色。

Conclusion: 为 NISQ 时代量子计算在数据受限金融场景应用提供实用蓝图，证明其在普惠金融等高风险应用中的潜力。

Abstract: Quantum Machine Learning (QML) offers a new paradigm for addressing complex
financial problems intractable for classical methods. This work specifically
tackles the challenge of few-shot credit risk assessment, a critical issue in
inclusive finance where data scarcity and imbalance limit the effectiveness of
conventional models. To address this, we design and implement a novel hybrid
quantum-classical workflow. The methodology first employs an ensemble of
classical machine learning models (Logistic Regression, Random Forest, XGBoost)
for intelligent feature engineering and dimensionality reduction. Subsequently,
a Quantum Neural Network (QNN), trained via the parameter-shift rule, serves as
the core classifier. This framework was evaluated through numerical simulations
and deployed on the Quafu Quantum Cloud Platform's ScQ-P21 superconducting
processor. On a real-world credit dataset of 279 samples, our QNN achieved a
robust average AUC of 0.852 +/- 0.027 in simulations and yielded an impressive
AUC of 0.88 in the hardware experiment. This performance surpasses a suite of
classical benchmarks, with a particularly strong result on the recall metric.
This study provides a pragmatic blueprint for applying quantum computing to
data-constrained financial scenarios in the NISQ era and offers valuable
empirical evidence supporting its potential in high-stakes applications like
inclusive finance.

</details>


### [76] [An End-to-End Differentiable, Graph Neural Network-Embedded Pore Network Model for Permeability Prediction](https://arxiv.org/abs/2509.13841)
*Qingqi Zhao,Heng Xiao*

Main category: cs.LG

TL;DR: 提出将图神经网络嵌入孔隙网络模型的端到端可微混合框架预测多孔介质渗透率，精度高、泛化性好。


<details>
  <summary>Details</summary>
Motivation: 纯数据驱动模型缺乏跨尺度泛化能力且无明确物理约束，传统孔隙网络模型依赖理想化几何假设，精度受限。

Method: 构建端到端可微混合框架，用图神经网络预测替代孔隙网络模型中电导计算的解析公式，通过自动微分和离散伴随方法进行端到端训练。

Result: 模型精度高、跨尺度泛化性好，优于纯数据驱动和传统孔隙网络模型方法，梯度敏感性分析揭示物理上一致的特征影响。

Conclusion: 该方法为复杂多孔介质渗透率预测提供可扩展且有物理依据的框架，减少模型不确定性并提高精度。

Abstract: Accurate prediction of permeability in porous media is essential for modeling
subsurface flow. While pure data-driven models offer computational efficiency,
they often lack generalization across scales and do not incorporate explicit
physical constraints. Pore network models (PNMs), on the other hand, are
physics-based and efficient but rely on idealized geometric assumptions to
estimate pore-scale hydraulic conductance, limiting their accuracy in complex
structures. To overcome these limitations, we present an end-to-end
differentiable hybrid framework that embeds a graph neural network (GNN) into a
PNM. In this framework, the analytical formulas used for conductance
calculations are replaced by GNN-based predictions derived from pore and throat
features. The predicted conductances are then passed to the PNM solver for
permeability computation. In this way, the model avoids the idealized geometric
assumptions of PNM while preserving the physics-based flow calculations. The
GNN is trained without requiring labeled conductance data, which can number in
the thousands per pore network; instead, it learns conductance values by using
a single scalar permeability as the training target. This is made possible by
backpropagating gradients through both the GNN (via automatic differentiation)
and the PNM solver (via a discrete adjoint method), enabling fully coupled,
end-to-end training. The resulting model achieves high accuracy and generalizes
well across different scales, outperforming both pure data-driven and
traditional PNM approaches. Gradient-based sensitivity analysis further reveals
physically consistent feature influences, enhancing model interpretability.
This approach offers a scalable and physically informed framework for
permeability prediction in complex porous media, reducing model uncertainty and
improving accuracy.

</details>


### [77] [Joint data imputation and mechanistic modelling for simulating heart-brain interactions in incomplete datasets](https://arxiv.org/abs/2010.01052)
*Jaume Banus,Maxime Sermesant,Oscar Camara,Marco Lorenzi*

Main category: cs.LG

TL;DR: 提出概率框架解决临床研究中多模态患者数据不足问题，实现心脏数据插补和心血管机制模型个性化，实验表明模型可准确插补缺失心脏特征。


<details>
  <summary>Details</summary>
Motivation: 临床研究中使用机制模型受限于缺乏代表不同解剖和生理过程的多模态患者数据，如神经影像数据集无法为脑疾病心血管因素建模提供足够心脏特征。

Method: 引入概率框架，基于变分框架联合推断心脏信息插补模型，结合高斯过程模拟器重现个性化心血管动力学。

Result: 在UK Biobank实验中，模型能在仅含少量心脏信息的数据集中准确插补缺失心脏特征，同时估计集总模型的模拟参数。

Conclusion: 该模型可通过模拟对应不同脑解剖条件的现实心脏动力学，对心脑关系进行新探索。

Abstract: The use of mechanistic models in clinical studies is limited by the lack of
multi-modal patients data representing different anatomical and physiological
processes. For example, neuroimaging datasets do not provide a sufficient
representation of heart features for the modeling of cardiovascular factors in
brain disorders. To tackle this problem we introduce a probabilistic framework
for joint cardiac data imputation and personalisation of cardiovascular
mechanistic models, with application to brain studies with incomplete heart
data. Our approach is based on a variational framework for the joint inference
of an imputation model of cardiac information from the available features,
along with a Gaussian Process emulator that can faithfully reproduce
personalised cardiovascular dynamics. Experimental results on UK Biobank show
that our model allows accurate imputation of missing cardiac features in
datasets containing minimal heart information, e.g. systolic and diastolic
blood pressures only, while jointly estimating the emulated parameters of the
lumped model. This allows a novel exploration of the heart-brain joint
relationship through simulation of realistic cardiac dynamics corresponding to
different conditions of brain anatomy.

</details>


### [78] [Masked Diffusion Models as Energy Minimization](https://arxiv.org/abs/2509.13866)
*Sitong Chen,Shen Nie,Jiacheng Sun,Zijin Feng,Zhenguo Li,Ji-Rong Wen,Chongxuan Li*

Main category: cs.LG

TL;DR: 提出理论框架解释掩码扩散模型（MDMs），证明三种能量公式等价，提出高效调度方法并经实验验证效果好


<details>
  <summary>Details</summary>
Motivation: 阐明MDMs理论基础并推动采样实践改进

Method: 证明三种能量公式在MDMs结构下数学等价，用Beta分布参数化插值调度，将调度设计空间缩减为二维搜索

Result: 能量启发式调度在合成和真实基准测试中优于手工基准，在低步数采样中表现更佳

Conclusion: 该理论框架有效，能量启发式调度方法可行且高效

Abstract: We present a systematic theoretical framework that interprets masked
diffusion models (MDMs) as solutions to energy minimization problems in
discrete optimal transport. Specifically, we prove that three distinct energy
formulations--kinetic, conditional kinetic, and geodesic energy--are
mathematically equivalent under the structure of MDMs, and that MDMs minimize
all three when the mask schedule satisfies a closed-form optimality condition.
This unification not only clarifies the theoretical foundations of MDMs, but
also motivates practical improvements in sampling. By parameterizing
interpolation schedules via Beta distributions, we reduce the schedule design
space to a tractable 2D search, enabling efficient post-training tuning without
model modification. Experiments on synthetic and real-world benchmarks
demonstrate that our energy-inspired schedules outperform hand-crafted
baselines, particularly in low-step sampling settings.

</details>


### [79] [FedSSG: Expectation-Gated and History-Aware Drift Alignment for Federated Learning](https://arxiv.org/abs/2509.13895)
*Zhanting Zhou,Jinshan Lai,Fengchun Zhang,Zeqin Wu,Fengli Zhang*

Main category: cs.LG

TL;DR: 提出FedSSG方法解决联邦学习中因非IID数据和部分参与导致的问题，在多数据集上表现优于基线，加速收敛且资源开销小。


<details>
  <summary>Details</summary>
Motivation: 非IID数据和部分参与在联邦学习中会导致客户端漂移和局部最优不一致，造成收敛不稳定和精度损失。

Method: 提出随机采样引导、历史感知的漂移对齐方法FedSSG，维护客户端漂移内存，通过观察/期望参与率的平滑函数控制内存更新和局部对齐项。

Result: 在多个数据集和参与率下，FedSSG始终优于强漂移感知基线，加速收敛，提高测试精度，仅增加O(d)客户端内存和常数时间门控。

Conclusion: 采样统计可转化为有原则、历史感知的阶段控制，稳定并加速联邦训练。

Abstract: Non-IID data and partial participation induce client drift and inconsistent
local optima in federated learning, causing unstable convergence and accuracy
loss. We present FedSSG, a stochastic sampling-guided, history-aware drift
alignment method. FedSSG maintains a per-client drift memory that accumulates
local model differences as a lightweight sketch of historical gradients;
crucially, it gates both the memory update and the local alignment term by a
smooth function of the observed/expected participation ratio (a
phase-by-expectation signal derived from the server sampler). This
statistically grounded gate stays weak and smooth when sampling noise dominates
early, then strengthens once participation statistics stabilize, contracting
the local-global gap without extra communication. Across CIFAR-10/100 with
100/500 clients and 2-15 percent participation, FedSSG consistently outperforms
strong drift-aware baselines and accelerates convergence; on our benchmarks it
improves test accuracy by up to a few points (e.g., about +0.9 on CIFAR-10 and
about +2.7 on CIFAR-100 on average over the top-2 baseline) and yields about
4.5x faster target-accuracy convergence on average. The method adds only O(d)
client memory and a constant-time gate, and degrades gracefully to a mild
regularizer under near-IID or uniform sampling. FedSSG shows that sampling
statistics can be turned into a principled, history-aware phase control to
stabilize and speed up federated training.

</details>


### [80] [TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates](https://arxiv.org/abs/2509.13906)
*Afrin Dange,Sunita Sarawagi*

Main category: cs.LG

TL;DR: 提出TFMAdapter轻量级实例级适配器，无需微调增强TSFMs利用协变量信息能力，实验显示其性能优于基础模型和监督基线。


<details>
  <summary>Details</summary>
Motivation: 多数时间序列基础模型（TSFMs）因特定领域性质和缺乏相关归纳偏置，无法利用协变量进行准确预测。

Method: 提出TFMAdapter，采用两阶段方法：先用简单回归模型生成伪预测，再训练高斯过程回归器结合伪预测、TSFM预测和协变量改进预测。

Result: 在真实数据集上实验表明，TFMAdapter始终优于基础模型和监督基线，在最小数据和计算开销下比基础模型提升24 - 27%。

Conclusion: 轻量级适配器有潜力弥合通用基础模型和特定领域预测需求之间的差距。

Abstract: Time Series Foundation Models (TSFMs) have recently achieved state-of-the-art
performance in univariate forecasting on new time series simply by conditioned
on a brief history of past values. Their success demonstrates that large-scale
pretraining across diverse domains can acquire the inductive bias to generalize
from temporal patterns in a brief history. However, most TSFMs are unable to
leverage covariates -- future-available exogenous variables critical for
accurate forecasting in many applications -- due to their domain-specific
nature and the lack of associated inductive bias. We propose TFMAdapter, a
lightweight, instance-level adapter that augments TSFMs with covariate
information without fine-tuning. Instead of retraining, TFMAdapter operates on
the limited history provided during a single model call, learning a
non-parametric cascade that combines covariates with univariate TSFM forecasts.
However, such learning would require univariate forecasts at all steps in the
history, requiring too many calls to the TSFM. To enable training on the full
historical context while limiting TSFM invocations, TFMAdapter uses a two-stage
method: (1) generating pseudo-forecasts with a simple regression model, and (2)
training a Gaussian Process regressor to refine predictions using both pseudo-
and TSFM forecasts alongside covariates. Extensive experiments on real-world
datasets demonstrate that TFMAdapter consistently outperforms both foundation
models and supervised baselines, achieving a 24-27\% improvement over base
foundation models with minimal data and computational overhead. Our results
highlight the potential of lightweight adapters to bridge the gap between
generic foundation models and domain-specific forecasting needs.

</details>


### [81] [APFEx: Adaptive Pareto Front Explorer for Intersectional Fairness](https://arxiv.org/abs/2509.13908)
*Priyobrata Mondal,Faizanuddin Ansari,Swagatam Das*

Main category: cs.LG

TL;DR: 提出APFEx框架解决机器学习模型中交叉公平性问题，实验展示其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法捕捉交叉子群体面临的细微、多重偏差，需解决机器学习模型交叉公平性问题。

Method: 引入APFEx框架，结合自适应多目标优化器、可微交叉公平性指标和收敛到帕累托最优解的理论保证。

Result: 在四个真实数据集上的实验表明，APFEx在减少公平性违规的同时保持了有竞争力的准确性。

Conclusion: 工作填补了公平机器学习的关键空白，为交叉公平性提供了可扩展、模型无关的解决方案。

Abstract: Ensuring fairness in machine learning models is critical, especially when
biases compound across intersecting protected attributes like race, gender, and
age. While existing methods address fairness for single attributes, they fail
to capture the nuanced, multiplicative biases faced by intersectional
subgroups. We introduce Adaptive Pareto Front Explorer (APFEx), the first
framework to explicitly model intersectional fairness as a joint optimization
problem over the Cartesian product of sensitive attributes. APFEx combines
three key innovations- (1) an adaptive multi-objective optimizer that
dynamically switches between Pareto cone projection, gradient weighting, and
exploration strategies to navigate fairness-accuracy trade-offs, (2)
differentiable intersectional fairness metrics enabling gradient-based
optimization of non-smooth subgroup disparities, and (3) theoretical guarantees
of convergence to Pareto-optimal solutions. Experiments on four real-world
datasets demonstrate APFEx's superiority, reducing fairness violations while
maintaining competitive accuracy. Our work bridges a critical gap in fair ML,
providing a scalable, model-agnostic solution for intersectional fairness.

</details>


### [82] [Ensemble of Pre-Trained Models for Long-Tailed Trajectory Prediction](https://arxiv.org/abs/2509.13914)
*Divya Thuremella,Yi Yang,Simon Wanna,Lars Kunze,Daniele De Martini*

Main category: cs.LG

TL;DR: 本文探索集成建模在城市环境车辆轨迹预测多维回归问题中的应用，用简单置信加权平均法结合先进深度学习模型提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶先进预测模型不断涌现，需要解决如何在无需昂贵再训练的情况下结合这些大模型的优势。

Method: 采用简单的置信加权平均法，将先进深度学习模型直接组合，无需再训练或微调。

Result: 该方法比最佳预测模型性能提升10%，尤其在长尾指标上，且在NuScenes和Argoverse数据集上均有效，性能提升覆盖数据集分布。

Conclusion: 简单的置信加权平均法结合先进模型能有效提升车辆轨迹预测性能，代码开源。

Abstract: This work explores the application of ensemble modeling to the
multidimensional regression problem of trajectory prediction for vehicles in
urban environments. As newer and bigger state-of-the-art prediction models for
autonomous driving continue to emerge, an important open challenge is the
problem of how to combine the strengths of these big models without the need
for costly re-training. We show how, perhaps surprisingly, combining
state-of-the-art deep learning models out-of-the-box (without retraining or
fine-tuning) with a simple confidence-weighted average method can enhance the
overall prediction. Indeed, while combining trajectory prediction models is not
straightforward, this simple approach enhances performance by 10% over the best
prediction model, especially in the long-tailed metrics. We show that this
performance improvement holds on both the NuScenes and Argoverse datasets, and
that these improvements are made across the dataset distribution. The code for
our work is open source.

</details>


### [83] [eXtended Physics Informed Neural Network Method for Fracture Mechanics Problems](https://arxiv.org/abs/2509.13952)
*Amin Lotfalian,Mohammad Reza Banan,Pooyan Broumand*

Main category: cs.LG

TL;DR: 提出X - PINN框架解决含多裂纹的断裂力学问题，介绍方法并通过数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决含多裂纹的断裂力学问题。

Method: 提出基于能量的损失函数、定制积分方案和区域分解程序；借鉴XFEM用特殊函数丰富神经网络解空间；用不同神经网络分别建模标准和富集解分量。

Result: 通过数值实验验证了所提方法的有效性和鲁棒性。

Conclusion: X - PINN框架能有效且灵活地模拟1D和2D域中的复杂多裂纹问题，且便于扩展到3D问题。

Abstract: This paper presents eXtended Physics-Informed Neural Network (X-PINN), a
novel and robust framework for addressing fracture mechanics problems involving
multiple cracks in fractured media. To address this, an energy-based loss
function, customized integration schemes, and domain decomposition procedures
are proposed. Inspired by the Extended Finite Element Method (XFEM), the neural
network solution space is enriched with specialized functions that allow crack
body discontinuities and singularities at crack tips to be explicitly captured.
Furthermore, a structured framework is introduced in which standard and
enriched solution components are modeled using distinct neural networks,
enabling flexible and effective simulations of complex multiple-crack problems
in 1D and 2D domains, with convenient extensibility to 3D problems. Numerical
experiments are conducted to validate the effectiveness and robustness of the
proposed method.

</details>


### [84] [Personalization on a Budget: Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection](https://arxiv.org/abs/2509.13974)
*Amirhossein Shahbazinia,Jonathan Dan,Jose A. Miranda,Giovanni Ansaloni,David Atienza*

Main category: cs.LG

TL;DR: 本文提出EpiSMART框架用于癫痫发作自动检测，在CHB - MIT数据集验证效果良好，适合可穿戴系统实时部署。


<details>
  <summary>Details</summary>
Motivation: 癫痫诊断和护理需仔细，当前癫痫发作检测依赖专家分析脑电图，耗时且需专业知识，本文探索用深度学习进行自动检测，关注个性化持续学习模型。

Method: 提出EpiSMART持续学习框架，使用大小受限的重放缓冲区和明智的样本选择策略，选择性保留高熵和癫痫发作预测样本。

Result: 在CHB - MIT数据集上，EpiSMART的F1分数比未更新的基线提高21%，平均每天仅需6.46分钟标记数据和6.28次更新，适合可穿戴系统实时部署。

Conclusion: EpiSMART能在现实和资源受限条件下实现稳健且个性化的癫痫发作检测，有效整合新数据且不损害过往知识。

Abstract: Objective: Epilepsy, a prevalent neurological disease, demands careful
diagnosis and continuous care. Seizure detection remains challenging, as
current clinical practice relies on expert analysis of electroencephalography,
which is a time-consuming process and requires specialized knowledge.
Addressing this challenge, this paper explores automated epileptic seizure
detection using deep learning, focusing on personalized continual learning
models that adapt to each patient's unique electroencephalography signal
features, which evolve over time. Methods: In this context, our approach
addresses the challenge of integrating new data into existing models without
catastrophic forgetting, a common issue in static deep learning models. We
propose EpiSMART, a continual learning framework for seizure detection that
uses a size-constrained replay buffer and an informed sample selection strategy
to incrementally adapt to patient-specific electroencephalography signals. By
selectively retaining high-entropy and seizure-predicted samples, our method
preserves critical past information while maintaining high performance with
minimal memory and computational requirements. Results: Validation on the
CHB-MIT dataset, shows that EpiSMART achieves a 21% improvement in the F1 score
over a trained baseline without updates in all other patients. On average,
EpiSMART requires only 6.46 minutes of labeled data and 6.28 updates per day,
making it suitable for real-time deployment in wearable systems.
Conclusion:EpiSMART enables robust and personalized seizure detection under
realistic and resource-constrained conditions by effectively integrating new
data into existing models without degrading past knowledge. Significance: This
framework advances automated seizure detection by providing a continual
learning approach that supports patient-specific adaptation and practical
deployment in wearable healthcare systems.

</details>


### [85] [Deep Temporal Graph Networks for Real-Time Correction of GNSS Jamming-Induced Deviations](https://arxiv.org/abs/2509.14000)
*Ivana Kesić,Aljaž Blatnik,Carolina Fortuna,Blaž Bertalanič*

Main category: cs.LG

TL;DR: 本文将干扰缓解问题转化为动态图回归，引入以接收器为中心的深度时间图网络实时预测并校正接收器水平偏差，实验表明该模型在多方面优于强多元时间序列基线。


<details>
  <summary>Details</summary>
Motivation: 全球导航卫星系统（GNSS）受有意干扰影响，需解决干扰缓解问题以保证定位和授时功能正常运行。

Method: 将干扰缓解问题转化为动态图回归，引入接收器中心的深度时间图网络，用单层异质图卷积长短期记忆网络（HeteroGCLSTM）聚合空间和时间信息，输出二维偏差向量进行实时校正。

Result: 在不同接收器、干扰模式和功率水平下，该模型的平均绝对误差（MAE）低于强多元时间序列基线，在混合模式数据集上也表现出色，且数据效率高。

Conclusion: 提出的模型在解决GNSS干扰缓解问题上效果良好，优于传统基线模型，具有较高的数据效率。

Abstract: Global Navigation Satellite Systems (GNSS) are increasingly disrupted by
intentional jamming, degrading availability precisely when positioning and
timing must remain operational. We address this by reframing jamming mitigation
as dynamic graph regression and introducing a receiver-centric deep temporal
graph network that predicts, and thus corrects, the receivers horizontal
deviation in real time. At each 1 Hz epoch, the satellite receiver environment
is represented as a heterogeneous star graph (receiver center, tracked
satellites as leaves) with time varying attributes (e.g., SNR, azimuth,
elevation, latitude/longitude). A single layer Heterogeneous Graph ConvLSTM
(HeteroGCLSTM) aggregates one hop spatial context and temporal dynamics over a
short history to output the 2D deviation vector applied for on the fly
correction.
  We evaluate on datasets from two distinct receivers under three jammer
profiles, continuous wave (cw), triple tone (cw3), and wideband FM, each
exercised at six power levels between -45 and -70 dBm, with 50 repetitions per
scenario (prejam/jam/recovery). Against strong multivariate time series
baselines (MLP, uniform CNN, and Seq2Point CNN), our model consistently attains
the lowest mean absolute error (MAE). At -45 dBm, it achieves 3.64 cm
(GP01/cw), 7.74 cm (GP01/cw3), 4.41 cm (ublox/cw), 4.84 cm (ublox/cw3), and
4.82 cm (ublox/FM), improving to 1.65-2.08 cm by -60 to -70 dBm. On mixed mode
datasets pooling all powers, MAE is 3.78 cm (GP01) and 4.25 cm (ublox10),
outperforming Seq2Point, MLP, and CNN. A split study shows superior data
efficiency: with only 10\% training data our approach remains well ahead of
baselines (20 cm vs. 36-42 cm).

</details>


### [86] [Differentially private federated learning for localized control of infectious disease dynamics](https://arxiv.org/abs/2509.14024)
*Raouf Kerkouche,Henrik Zunker,Mario Fritz,Martin J. Kühn*

Main category: cs.LG

TL;DR: 本文提出基于联邦学习和差分隐私的隐私保护预测方法用于疫情病例预测，评估显示该方法能在保证隐私前提下提供有用的县级预测。


<details>
  <summary>Details</summary>
Motivation: 疫情期间本地化反应有优势，但本地训练机器学习模型因数据有限不可行，集中数据又受隐私限制，需保护隐私的预测方法。

Method: 提出基于德国县和社区的本地化策略，采用带客户端差分隐私的联邦学习框架，训练共享多层感知器预测病例数，客户端交换范数裁剪更新，服务器聚合带差分隐私噪声的更新。

Result: 在两个阶段的县级COVID - 19数据上评估，极强隐私下预测不稳定，适度强隐私下，DP模型接近非DP模型。

Conclusion: 客户端差分隐私联邦学习能在强隐私保证下提供有用的县级预测，可行的隐私预算取决于疫情阶段，可实现卫生部门间符合隐私规定的本地预测合作。

Abstract: In times of epidemics, swift reaction is necessary to mitigate epidemic
spreading. For this reaction, localized approaches have several advantages,
limiting necessary resources and reducing the impact of interventions on a
larger scale. However, training a separate machine learning (ML) model on a
local scale is often not feasible due to limited available data. Centralizing
the data is also challenging because of its high sensitivity and privacy
constraints. In this study, we consider a localized strategy based on the
German counties and communities managed by the related local health authorities
(LHA). For the preservation of privacy to not oppose the availability of
detailed situational data, we propose a privacy-preserving forecasting method
that can assist public health experts and decision makers. ML methods with
federated learning (FL) train a shared model without centralizing raw data.
Considering the counties, communities or LHAs as clients and finding a balance
between utility and privacy, we study a FL framework with client-level
differential privacy (DP). We train a shared multilayer perceptron on sliding
windows of recent case counts to forecast the number of cases, while clients
exchange only norm-clipped updates and the server aggregated updates with DP
noise. We evaluate the approach on COVID-19 data on county-level during two
phases. As expected, very strict privacy yields unstable, unusable forecasts.
At a moderately strong level, the DP model closely approaches the non-DP model:
$R^2= 0.94$ (vs. 0.95) and mean absolute percentage error (MAPE) of 26 % in
November 2020; $R^2= 0.88$ (vs. 0.93) and MAPE of 21 % in March 2022. Overall,
client-level DP-FL can deliver useful county-level predictions with strong
privacy guarantees, and viable privacy budgets depend on epidemic phase,
allowing privacy-compliant collaboration among health authorities for local
forecasting.

</details>


### [87] [Deep Learning-Driven Peptide Classification in Biological Nanopores](https://arxiv.org/abs/2509.14029)
*Samuel Tovey,Julian Hoßbach,Sandro Kuppel,Tobias Ensslen,Jan C. Behrends,Christian Holm*

Main category: cs.LG

TL;DR: 本文将纳米孔设备电流信号转换为尺度图图像进行蛋白质分类，在42种肽上实现约81%准确率，推动即时诊断。


<details>
  <summary>Details</summary>
Motivation: 现有纳米孔设备进行蛋白质实时分类时信号复杂，限制了分类准确性，需提高分类精度以实现临床廉价快速疾病诊断。

Method: 通过小波变换将电流信号转换为尺度图图像，利用适合机器学习算法的方式捕捉幅度、频率和时间信息；还展示了模型迁移技术。

Result: 在42种肽上测试，分类准确率约为81%，创造了该领域新的技术水平。

Conclusion: 该方法向即时护理中的肽/蛋白质诊断迈出一步，模型迁移技术为实时疾病诊断新方法铺平道路。

Abstract: A device capable of performing real time classification of proteins in a
clinical setting would allow for inexpensive and rapid disease diagnosis. One
such candidate for this technology are nanopore devices. These devices work by
measuring a current signal that arises when a protein or peptide enters a
nanometer-length-scale pore. Should this current be uniquely related to the
structure of the peptide and its interactions with the pore, the signals can be
used to perform identification. While such a method would allow for real time
identification of peptides and proteins in a clinical setting, to date, the
complexities of these signals limit their accuracy. In this work, we tackle the
issue of classification by converting the current signals into scaleogram
images via wavelet transforms, capturing amplitude, frequency, and time
information in a modality well-suited to machine learning algorithms. When
tested on 42 peptides, our method achieved a classification accuracy of
~$81\,\%$, setting a new state-of-the-art in the field and taking a step toward
practical peptide/protein diagnostics at the point of care. In addition, we
demonstrate model transfer techniques that will be critical when deploying
these models into real hardware, paving the way to a new method for real-time
disease diagnosis.

</details>


### [88] [Queen Detection in Beehives via Environmental Sensor Fusion for Low-Power Edge Computing](https://arxiv.org/abs/2509.14061)
*Chiara De Luca,Elisa Donati*

Main category: cs.LG

TL;DR: 提出基于环境传感器融合的轻量级多模态蜂王检测系统，在STM32微控制器实现低功耗边缘计算，检测准确率超99%，为养蜂监测提供可扩展可持续方案。


<details>
  <summary>Details</summary>
Motivation: 现有蜂王监测方法存在劳动密集、干扰大、功耗高、易受环境噪声影响等问题，需要新的监测方案。

Method: 基于环境传感器融合（温度、湿度、压力差），在商用STM32微控制器上采用量化决策树推理。

Result: 仅使用环境输入，系统蜂王检测准确率超99%，音频特征无显著性能提升。

Conclusion: 该工作为非侵入式蜂巢监测提供可扩展和可持续解决方案，为精准养蜂奠定基础。

Abstract: Queen bee presence is essential for the health and stability of honeybee
colonies, yet current monitoring methods rely on manual inspections that are
labor-intensive, disruptive, and impractical for large-scale beekeeping. While
recent audio-based approaches have shown promise, they often require high power
consumption, complex preprocessing, and are susceptible to ambient noise. To
overcome these limitations, we propose a lightweight, multimodal system for
queen detection based on environmental sensor fusion-specifically, temperature,
humidity, and pressure differentials between the inside and outside of the
hive. Our approach employs quantized decision tree inference on a commercial
STM32 microcontroller, enabling real-time, low-power edge computing without
compromising accuracy. We show that our system achieves over 99% queen
detection accuracy using only environmental inputs, with audio features
offering no significant performance gain. This work presents a scalable and
sustainable solution for non-invasive hive monitoring, paving the way for
autonomous, precision beekeeping using off-the-shelf, energy-efficient
hardware.

</details>


### [89] [Online Bayesian Risk-Averse Reinforcement Learning](https://arxiv.org/abs/2509.14077)
*Yuhao Wang,Enlu Zhou*

Main category: cs.LG

TL;DR: 研究强化学习中贝叶斯风险规避公式，推导渐近正态性，提出后验采样程序，建立次线性遗憾界并实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中因数据不足导致的认知不确定性问题。

Method: 采用贝叶斯风险马尔可夫决策过程处理参数不确定性，推导渐近正态性，利用自适应特性提出后验采样程序。

Result: 贝叶斯风险规避方法倾向于悲观低估原价值函数，建立次线性遗憾界。

Conclusion: 所提算法能有效处理认知不确定性，理论性质得到验证。

Abstract: In this paper, we study the Bayesian risk-averse formulation in reinforcement
learning (RL). To address the epistemic uncertainty due to a lack of data, we
adopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the
parameter uncertainty of the unknown underlying model. We derive the asymptotic
normality that characterizes the difference between the Bayesian risk value
function and the original value function under the true unknown distribution.
The results indicate that the Bayesian risk-averse approach tends to
pessimistically underestimate the original value function. This discrepancy
increases with stronger risk aversion and decreases as more data become
available. We then utilize this adaptive property in the setting of online RL
as well as online contextual multi-arm bandits (CMAB), a special case of online
RL. We provide two procedures using posterior sampling for both the general RL
problem and the CMAB problem. We establish a sub-linear regret bound, with the
regret defined as the conventional regret for both the RL and CMAB settings.
Additionally, we establish a sub-linear regret bound for the CMAB setting with
the regret defined as the Bayesian risk regret. Finally, we conduct numerical
experiments to demonstrate the effectiveness of the proposed algorithm in
addressing epistemic uncertainty and verifying the theoretical properties.

</details>


### [90] [Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques](https://arxiv.org/abs/2509.14078)
*Robiul Islam,Dmitry I. Ignatov,Karl Kaberg,Roman Nabatchikov*

Main category: cs.LG

TL;DR: 研究用不同优化器评估EEG频带分类器性能，比较三种神经网络架构，指出优化器、模型架构和频带分析对提升分类性能重要。


<details>
  <summary>Details</summary>
Motivation: 研究不同优化器下EEG频带的分类器性能，评估左右半球的有效类别预测。

Method: 实现三种神经网络架构（深度密集网络、浅层三层网络和CNN），用TensorFlow和PyTorch框架比较，使用SHAP图识别有效类别预测。

Result: Adagrad和RMSprop在不同频带表现好，Adadelta跨模型评估表现稳健；CNN精度第二，深度密集网络学习复杂模式有竞争力，浅层三层网络计算高效。

Conclusion: 优化器选择、模型架构和EEG频带分析对提升分类器性能和理解特征重要性很关键。

Abstract: This study investigates classifier performance across EEG frequency bands
using various optimizers and evaluates efficient class prediction for the left
and right hemispheres. Three neural network architectures - a deep dense
network, a shallow three-layer network, and a convolutional neural network
(CNN) - are implemented and compared using the TensorFlow and PyTorch
frameworks. Results indicate that the Adagrad and RMSprop optimizers
consistently perform well across different frequency bands, with Adadelta
exhibiting robust performance in cross-model evaluations. Specifically, Adagrad
excels in the beta band, while RMSprop achieves superior performance in the
gamma band. Conversely, SGD and FTRL exhibit inconsistent performance. Among
the models, the CNN demonstrates the second highest accuracy, particularly in
capturing spatial features of EEG data. The deep dense network shows
competitive performance in learning complex patterns, whereas the shallow
three-layer network, sometimes being less accurate, provides computational
efficiency. SHAP (Shapley Additive Explanations) plots are employed to identify
efficient class prediction, revealing nuanced contributions of EEG frequency
bands to model accuracy. Overall, the study highlights the importance of
optimizer selection, model architecture, and EEG frequency band analysis in
enhancing classifier performance and understanding feature importance in
neuroimaging-based classification tasks.

</details>


### [91] [From Distributional to Quantile Neural Basis Models: the case of Electricity Price Forecasting](https://arxiv.org/abs/2509.14113)
*Alessandro Brusaferri,Danial Ramin,Andrea Ballarino*

Main category: cs.LG

TL;DR: 引入分位数神经基模型解决多步概率预测中神经网络特征条件输出机制理解难题，在电价预测中验证效果。


<details>
  <summary>Details</summary>
Motivation: 神经网络在多步概率预测中虽精度高，但理解特征条件输出的潜在机制是重大挑战。

Method: 引入分位数神经基模型，将分位数广义可加模型的可解释性原则融入端到端神经网络训练框架，利用共享基分解和权重因子分解，避免参数分布假设。

Result: 在日前电价预测中实现了与分布和分位数回归神经网络相当的预测性能。

Conclusion: 该方法在实现可比预测性能的同时，能通过学习的非线性映射为模型行为提供有价值的见解。

Abstract: While neural networks are achieving high predictive accuracy in multi-horizon
probabilistic forecasting, understanding the underlying mechanisms that lead to
feature-conditioned outputs remains a significant challenge for forecasters. In
this work, we take a further step toward addressing this critical issue by
introducing the Quantile Neural Basis Model, which incorporates the
interpretability principles of Quantile Generalized Additive Models into an
end-to-end neural network training framework. To this end, we leverage shared
basis decomposition and weight factorization, complementing Neural Models for
Location, Scale, and Shape by avoiding any parametric distributional
assumptions. We validate our approach on day-ahead electricity price
forecasting, achieving predictive performance comparable to distributional and
quantile regression neural networks, while offering valuable insights into
model behavior through the learned nonlinear mappings from input features to
output predictions across the horizon.

</details>


### [92] [Breaking the Cycle of Incarceration With Targeted Mental Health Outreach: A Case Study in Machine Learning for Public Policy](https://arxiv.org/abs/2509.14129)
*Kit T. Rodolfa,Erika Salomon,Jin Yao,Steve Yoder,Robert Sullivan,Kevin McGuire,Allie Dickinson,Rob MacDougall,Brian Seidler,Christina Sung,Claire Herdeman,Rayid Ghani*

Main category: cs.LG

TL;DR: 文章介绍堪萨斯州约翰逊县与卡内基梅隆大学合作开展针对性心理健康外展活动以降低再入狱率，描述了数据、建模方法及结果，试验表明模型对新入狱情况预测性高，外展对高风险人群最有效。


<details>
  <summary>Details</summary>
Motivation: 现有刑事司法系统难以满足被监禁者需求，导致问题恶化和再入狱循环，为打破此循环，减少再入狱率。

Method: 采用预测建模方法，并进行实地试验来验证模型预测能力、评估外展影响及确定外展最有效风险水平。

Result: 模型对新入狱情况有高预测性，试验中高风险组超半数人员次年再次入狱，外展对高风险人群在心理健康利用、急救调度和刑事司法介入方面有影响。

Conclusion: 针对性心理健康外展活动对降低高风险人群再入狱率有积极作用。

Abstract: Many incarcerated individuals face significant and complex challenges,
including mental illness, substance dependence, and homelessness, yet jails and
prisons are often poorly equipped to address these needs. With little support
from the existing criminal justice system, these needs can remain untreated and
worsen, often leading to further offenses and a cycle of incarceration with
adverse outcomes both for the individual and for public safety, with
particularly large impacts on communities of color that continue to widen the
already extensive racial disparities in criminal justice outcomes. Responding
to these failures, a growing number of criminal justice stakeholders are
seeking to break this cycle through innovative approaches such as
community-driven and alternative approaches to policing, mentoring, community
building, restorative justice, pretrial diversion, holistic defense, and social
service connections. Here we report on a collaboration between Johnson County,
Kansas, and Carnegie Mellon University to perform targeted, proactive mental
health outreach in an effort to reduce reincarceration rates.
  This paper describes the data used, our predictive modeling approach and
results, as well as the design and analysis of a field trial conducted to
confirm our model's predictive power, evaluate the impact of this targeted
outreach, and understand at what level of reincarceration risk outreach might
be most effective. Through this trial, we find that our model is highly
predictive of new jail bookings, with more than half of individuals in the
trial's highest-risk group returning to jail in the following year. Outreach
was most effective among these highest-risk individuals, with impacts on mental
health utilization, EMS dispatches, and criminal justice involvement.

</details>


### [93] [A Compositional Kernel Model for Feature Learning](https://arxiv.org/abs/2509.14158)
*Feng Ruan,Keli Liu,Michael Jordan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study a compositional variant of kernel ridge regression in which the
predictor is applied to a coordinate-wise reweighting of the inputs. Formulated
as a variational problem, this model provides a simple testbed for feature
learning in compositional architectures. From the perspective of variable
selection, we show how relevant variables are recovered while noise variables
are eliminated. We establish guarantees showing that both global minimizers and
stationary points discard noise coordinates when the noise variables are
Gaussian distributed. A central finding is that $\ell_1$-type kernels, such as
the Laplace kernel, succeed in recovering features contributing to nonlinear
effects at stationary points, whereas Gaussian kernels recover only linear
ones.

</details>


### [94] [Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework](https://arxiv.org/abs/2509.14167)
*Md Rezwan Jaher,Abul Mukid Mohammad Mukaddes,A. B. M. Abdul Malek*

Main category: cs.LG

TL;DR: 提出端到端框架，从稀疏常规数据非侵入性估计不可测变量，解决医疗关键参数测量和计算难题，在青光眼研究中取得良好结果且具推广性。


<details>
  <summary>Details</summary>
Motivation: 解决医疗中关键潜在参数无法测量的问题，以及计算中因缺乏真实数据和高成本模拟导致的逆问题建模难题。

Method: 采用多阶段人工智能架构分离问题，使用PCDS数据生成策略减少计算时间，用贝叶斯引擎量化预测不确定性。

Result: 非侵入性估计的流出设施与最新眼压描记法高度一致，新导出的渗透率生物标志物在按疾病风险分层临床队列时准确性高。

Conclusion: 该框架为其他数据稀缺、计算密集领域的类似逆问题提供了可推广的解决方案。

Abstract: Many critical healthcare decisions are challenged by the inability to measure
key underlying parameters. Glaucoma, a leading cause of irreversible blindness
driven by elevated intraocular pressure (IOP), provides a stark example. The
primary determinant of IOP, a tissue property called trabecular meshwork
permeability, cannot be measured in vivo, forcing clinicians to depend on
indirect surrogates. This clinical challenge is compounded by a broader
computational one: developing predictive models for such ill-posed inverse
problems is hindered by a lack of ground-truth data and prohibitive cost of
large-scale, high-fidelity simulations. We address both challenges with an
end-to-end framework to noninvasively estimate unmeasurable variables from
sparse, routine data. Our approach combines a multi-stage artificial
intelligence architecture to functionally separate the problem; a novel data
generation strategy we term PCDS that obviates the need for hundreds of
thousands of costly simulations, reducing the effective computational time from
years to hours; and a Bayesian engine to quantify predictive uncertainty. Our
framework deconstructs a single IOP measurement into its fundamental components
from routine inputs only, yielding estimates for the unmeasurable tissue
permeability and a patient's outflow facility. Our noninvasively estimated
outflow facility achieved excellent agreement with state-of-the-art tonography
with precision comparable to direct physical instruments. Furthermore, the
newly derived permeability biomarker demonstrates high accuracy in stratifying
clinical cohorts by disease risk, highlighting its diagnostic potential. More
broadly, our framework establishes a generalizable blueprint for solving
similar inverse problems in other data-scarce, computationally-intensive
domains.

</details>


### [95] [TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing for AMS Circuits](https://arxiv.org/abs/2509.14169)
*Ziming Wei,Zichen Kong,Yuan Wang,David Z. Pan,Xiyuan Tang*

Main category: cs.LG

TL;DR: 提出TopoSizing端到端框架，从原始网表进行电路理解并优化电路设计。


<details>
  <summary>Details</summary>
Motivation: 模拟和混合信号电路设计因数据和嵌入领域知识难而具挑战性，传统方法有局限，现有大语言模型方法有不足。

Method: 先用图算法将电路组织成层次化表示，再用LLM代理执行迭代循环生成显式注释，将验证见解集成到贝叶斯优化中。

Result: 未提及具体结果。

Conclusion: TopoSizing框架能直接从原始网表进行电路理解并转化为优化收益，提高效率并保证可行性。

Abstract: Analog and mixed-signal circuit design remains challenging due to the
shortage of high-quality data and the difficulty of embedding domain knowledge
into automated flows. Traditional black-box optimization achieves sampling
efficiency but lacks circuit understanding, which often causes evaluations to
be wasted in low-value regions of the design space. In contrast, learning-based
methods embed structural knowledge but are case-specific and costly to retrain.
Recent attempts with large language models show potential, yet they often rely
on manual intervention, limiting generality and transparency. We propose
TopoSizing, an end-to-end framework that performs robust circuit understanding
directly from raw netlists and translates this knowledge into optimization
gains. Our approach first applies graph algorithms to organize circuits into a
hierarchical device-module-stage representation. LLM agents then execute an
iterative hypothesis-verification-refinement loop with built-in consistency
checks, producing explicit annotations. Verified insights are integrated into
Bayesian optimization through LLM-guided initial sampling and
stagnation-triggered trust-region updates, improving efficiency while
preserving feasibility.

</details>


### [96] [TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning](https://arxiv.org/abs/2509.14172)
*Ziyuan Chen,Zhenghui Zhao,Zhangye Han,Miancan Liu,Xianhang Ye,Yiqing Li,Hongbo Min,Jinkui Ren,Xiantao Zhang,Guitao Cao*

Main category: cs.LG

TL;DR: 提出TGPO离线强化学习框架解决训练Web Agents的问题，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 训练Web Agents面临信用分配错误、标注成本高和奖励稀疏等问题。

Method: 提出Tree - Guided Preference Optimization (TGPO)框架，用树结构轨迹表示消除标签冲突，结合Process Reward Model自动生成细粒度奖励，使用动态加权机制。

Result: 在Online - Mind2Web和C - WebShop数据集上实验，TGPO显著优于现有方法，成功率更高且冗余步骤更少。

Conclusion: TGPO能有效解决训练Web Agents的问题，性能表现良好。

Abstract: With the rapid advancement of large language models and vision-language
models, employing large models as Web Agents has become essential for automated
web interaction. However, training Web Agents with reinforcement learning faces
critical challenges including credit assignment misallocation, prohibitively
high annotation costs, and reward sparsity. To address these issues, we propose
Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning
framework that proposes a tree-structured trajectory representation merging
semantically identical states across trajectories to eliminate label conflicts.
Our framework incorporates a Process Reward Model that automatically generates
fine-grained rewards through subgoal progress, redundancy detection, and action
verification. Additionally, a dynamic weighting mechanism prioritizes
high-impact decision points during training. Experiments on Online-Mind2Web and
our self-constructed C-WebShop datasets demonstrate that TGPO significantly
outperforms existing methods, achieving higher success rates with fewer
redundant steps.

</details>


### [97] [Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting](https://arxiv.org/abs/2509.14181)
*Yifan Hu,Jie Yang,Tian Zhou,Peiyuan Liu,Yujin Tang,Rong Jin,Liang Sun*

Main category: cs.LG

TL;DR: 现有SOTA时间序列预测器很少用表征学习方法，本文提出TimeAlign框架，实验验证其性能优越，可作通用对齐模块。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA时间序列预测器很少采用表征学习方法，作者认为显式表征对齐能提供关键信息，缩小输入历史和未来目标之间的分布差距。

Method: 引入TimeAlign框架，通过简单的重建任务学习辅助特征并反馈给基础预测器。

Result: 在八个基准测试中验证了TimeAlign的优越性能，增益主要来自纠正历史输入和未来输出之间的频率不匹配。

Conclusion: TimeAlign架构无关且开销可忽略，可作为现代深度学习时间序列预测系统的通用对齐模块。

Abstract: Representation learning techniques like contrastive learning have long been
explored in time series forecasting, mirroring their success in computer vision
and natural language processing. Yet recent state-of-the-art (SOTA) forecasters
seldom adopt these representation approaches because they have shown little
performance advantage. We challenge this view and demonstrate that explicit
representation alignment can supply critical information that bridges the
distributional gap between input histories and future targets. To this end, we
introduce TimeAlign, a lightweight, plug-and-play framework that learns
auxiliary features via a simple reconstruction task and feeds them back to any
base forecaster. Extensive experiments across eight benchmarks verify its
superior performance. Further studies indicate that the gains arises primarily
from correcting frequency mismatches between historical inputs and future
outputs. We also provide a theoretical justification for the effectiveness of
TimeAlign in increasing the mutual information between learned representations
and predicted targets. As it is architecture-agnostic and incurs negligible
overhead, TimeAlign can serve as a general alignment module for modern deep
learning time-series forecasting systems. The code is available at
https://github.com/TROUBADOUR000/TimeAlign.

</details>


### [98] [A Variational Framework for Residual-Based Adaptivity in Neural PDE Solvers and Operator Learning](https://arxiv.org/abs/2509.14198)
*Juan Diego Toscano,Daniel T. Chen,Vivek Oommen,George Em Karniadakis*

Main category: cs.LG

TL;DR: 本文引入统一变分框架形式化基于残差的自适应策略，有三方面益处，扩展到算子学习表现良好，为策略提供理论依据。


<details>
  <summary>Details</summary>
Motivation: 基于残差的自适应策略在科学机器学习中广泛使用但大多是启发式的，缺乏理论形式化。

Method: 引入统一变分框架，通过整合残差的凸变换来形式化这些方法，不同变换对应不同目标泛函。

Result: 该方法有三方面益处，扩展到算子学习在优化器和架构上有显著性能提升。

Conclusion: 为基于残差的自适应策略提供理论依据，为原则性离散化和训练策略奠定基础。

Abstract: Residual-based adaptive strategies are widely used in scientific machine
learning but remain largely heuristic. We introduce a unifying variational
framework that formalizes these methods by integrating convex transformations
of the residual. Different transformations correspond to distinct objective
functionals: exponential weights target the minimization of uniform error,
while linear weights recover the minimization of quadratic error. Within this
perspective, adaptive weighting is equivalent to selecting sampling
distributions that optimize the primal objective, thereby linking
discretization choices directly to error metrics. This principled approach
yields three benefits: (1) it enables systematic design of adaptive schemes
across norms, (2) reduces discretization error through variance reduction of
the loss estimator, and (3) enhances learning dynamics by improving the
gradient signal-to-noise ratio. Extending the framework to operator learning,
we demonstrate substantial performance gains across optimizers and
architectures. Our results provide a theoretical justification of
residual-based adaptivity and establish a foundation for principled
discretization and training strategies.

</details>


### [99] [A Universal Banach--Bregman Framework for Stochastic Iterations: Unifying Stochastic Mirror Descent, Learning and LLM Training](https://arxiv.org/abs/2509.14216)
*Johnny R. Zhang,Xiaomei Mi,Gaoyuan Du,Qianyi Sun,Shiqi Wang,Jiaxuan Li,Wenhua Zhou*

Main category: cs.LG

TL;DR: 本文提出Banach - Bregman框架用于随机迭代，统一优化理论与实践，实证显示在多AI范式中有更好表现。


<details>
  <summary>Details</summary>
Motivation: 现有随机优化理论局限于希尔伯特空间，无法处理非欧几里得设置，需新框架。

Method: 引入Banach - Bregman框架，通过Bregman投影和Bregman - Fejer单调性提供统一模板，建立超松弛条件，给出收敛定理。

Result: 在合成和现实任务中验证，在多AI范式里比经典基线收敛快20%，方差降低，精度提高。

Conclusion: Banach - Bregman几何可作为统一核心AI范式优化理论与实践的基石。

Abstract: Stochastic optimization powers the scalability of modern artificial
intelligence, spanning machine learning, deep learning, reinforcement learning,
and large language model training. Yet, existing theory remains largely
confined to Hilbert spaces, relying on inner-product frameworks and
orthogonality. This paradigm fails to capture non-Euclidean settings, such as
mirror descent on simplices, Bregman proximal methods for sparse learning,
natural gradient descent in information geometry, or
Kullback--Leibler-regularized language model training. Unlike Euclidean-based
Hilbert-space methods, this approach embraces general Banach spaces. This work
introduces a pioneering Banach--Bregman framework for stochastic iterations,
establishing Bregman geometry as a foundation for next-generation optimization.
It (i) provides a unified template via Bregman projections and Bregman--Fejer
monotonicity, encompassing stochastic approximation, mirror descent, natural
gradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations
($\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and
elucidating their acceleration effect; and (iii) delivers convergence theorems
spanning almost-sure boundedness to geometric rates, validated on synthetic and
real-world tasks. Empirical studies across machine learning (UCI benchmarks),
deep learning (e.g., Transformer training), reinforcement learning
(actor--critic), and large language models (WikiText-2 with distilGPT-2) show
up to 20% faster convergence, reduced variance, and enhanced accuracy over
classical baselines. These results position Banach--Bregman geometry as a
cornerstone unifying optimization theory and practice across core AI paradigms.

</details>


### [100] [Data Denoising and Derivative Estimation for Data-Driven Modeling of Nonlinear Dynamical Systems](https://arxiv.org/abs/2509.14219)
*Jiaqi Yao,Lewis Mitchell,John Maclean,Hemanth Saratchandran*

Main category: cs.LG

TL;DR: 提出RKTV - INR去噪框架处理含噪数据以恢复动力系统控制方程，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的非线性动力系统建模常受测量噪声阻碍。

Method: 提出RKTV - INR框架，用隐式神经表示拟合含噪观测数据，施加龙格 - 库塔积分和总变差约束，将去噪后的状态和导数输入SINDy恢复控制方程。

Result: 实现有效噪声抑制、精确导数估计和可靠系统识别。

Conclusion: RKTV - INR框架能有效处理测量噪声，恢复动力系统控制方程。

Abstract: Data-driven modeling of nonlinear dynamical systems is often hampered by
measurement noise. We propose a denoising framework, called Runge-Kutta and
Total Variation Based Implicit Neural Representation (RKTV-INR), that
represents the state trajectory with an implicit neural representation (INR)
fitted directly to noisy observations. Runge-Kutta integration and total
variation are imposed as constraints to ensure that the reconstructed state is
a trajectory of a dynamical system that remains close to the original data. The
trained INR yields a clean, continuous trajectory and provides accurate
first-order derivatives via automatic differentiation. These denoised states
and derivatives are then supplied to Sparse Identification of Nonlinear
Dynamics (SINDy) to recover the governing equations. Experiments demonstrate
effective noise suppression, precise derivative estimation, and reliable system
identification.

</details>


### [101] [Language models' activations linearly encode training-order recency](https://arxiv.org/abs/2509.14223)
*Dmitrii Krasheninnikov,Richard E. Turner,David Krueger*

Main category: cs.LG

TL;DR: 研究表明语言模型激活值线性编码训练时信息学习的时间。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否能对训练时信息学习的时间进行编码。

Method: 对Llama - 3.2 - 1B在六个不相交但相似的命名实体数据集上顺序微调，创建已知训练顺序的模型。

Result: 测试样本平均激活值编码训练顺序，线性探针能准确区分‘早期’和‘晚期’实体，模型可微调以报告未见实体训练阶段，且时间信号并非源于激活幅度、损失或模型置信度差异。

Conclusion: 模型能够按信息获取时间区分信息，对处理冲突数据和知识修改有重要意义。

Abstract: We show that language models' activations linearly encode when information
was learned during training. Our setup involves creating a model with a known
training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but
otherwise similar datasets about named entities. We find that the average
activations of test samples for the six training datasets encode the training
order: when projected into a 2D subspace, these centroids are arranged exactly
in the order of training and lie on a straight line. Further, we show that
linear probes can accurately (~90%) distinguish "early" vs. "late" entities,
generalizing to entities unseen during the probes' own training. The model can
also be fine-tuned to explicitly report an unseen entity's training stage (~80%
accuracy). Interestingly, this temporal signal does not seem attributable to
simple differences in activation magnitudes, losses, or model confidence. Our
paper demonstrates that models are capable of differentiating information by
its acquisition time, and carries significant implications for how they might
manage conflicting data and respond to knowledge modifications.

</details>


### [102] [NIRVANA: Structured pruning reimagined for large language models compression](https://arxiv.org/abs/2509.14230)
*Mengting Ai,Tianxin Wei,Sirui Chen,Jingrui He*

Main category: cs.LG

TL;DR: 提出NIRVANA剪枝方法平衡大语言模型零样本准确率和微调能力，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型结构化剪枝方法会导致性能显著下降，需昂贵恢复技术，要解决这些问题。

Method: 利用Adam优化动力学下神经切线核的一阶显著性准则，结合跨层和模块的自适应稀疏分配机制，提出基于KL散度的校准数据选择策略。

Result: 在Llama3、Qwen和T5模型上实验，NIRVANA在相同稀疏约束下优于现有结构化剪枝方法。

Conclusion: NIRVANA是一种理论可靠且实用的大语言模型压缩方法。

Abstract: Structured pruning of large language models (LLMs) offers substantial
efficiency improvements by removing entire hidden units, yet current approaches
often suffer from significant performance degradation, particularly in
zero-shot settings, and necessitate costly recovery techniques such as
supervised fine-tuning (SFT) or adapter insertion. To address these critical
shortcomings, we introduce NIRVANA, a novel pruning method explicitly designed
to balance immediate zero-shot accuracy preservation with robust fine-tuning
capability. Leveraging a first-order saliency criterion derived from the Neural
Tangent Kernel under Adam optimization dynamics, NIRVANA provides a
theoretically grounded pruning strategy that respects essential model training
behaviors. To further address the unique challenges posed by structured
pruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across
layers and modules (attention vs. MLP), which adjusts pruning intensity between
modules in a globally balanced manner. Additionally, to mitigate the high
sensitivity of pruning decisions to calibration data quality, we propose a
simple yet effective KL divergence-based calibration data selection strategy,
ensuring more reliable and task-agnostic pruning outcomes. Comprehensive
experiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA
outperforms existing structured pruning methods under equivalent sparsity
constraints, providing a theoretically sound and practical approach to LLM
compression. The code is available at
https://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.

</details>


### [103] [Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision](https://arxiv.org/abs/2509.14234)
*Dulhan Jayalath,Shashwat Goel,Thomas Foster,Parag Jain,Suchin Gururangan,Cheng Zhang,Anirudh Goyal,Alan Schelten*

Main category: cs.LG

TL;DR: 提出Compute as Teacher (CaT)方法，将模型推理时的探索转化为无参考监督，在不同任务中生成奖励，提升模型性能，结合强化学习有进一步提升。


<details>
  <summary>Details</summary>
Motivation: 解决训练后无真实标签时学习信号来源的问题。

Method: 通过合成一组并行推演的单个参考，将模型推理时的探索转化为无参考监督；在可验证和不可验证任务中分别生成奖励。

Result: 作为测试时程序，CaT提升了Gemma 3 4B、Qwen 3 4B和Llama 3.1 8B的性能；结合强化学习有进一步提升。

Conclusion: CaT方法有效，能将探索转化为监督信号提升模型性能，结合强化学习效果更佳。

Abstract: Where do learning signals come from when there is no ground truth in
post-training? We propose turning exploration into supervision through Compute
as Teacher (CaT), which converts the model's own exploration at inference-time
into reference-free supervision by synthesizing a single reference from a group
of parallel rollouts and then optimizing toward it. Concretely, the current
policy produces a group of rollouts; a frozen anchor (the initial policy)
reconciles omissions and contradictions to estimate a reference, turning extra
inference-time compute into a teacher signal. We turn this into rewards in two
regimes: (i) verifiable tasks use programmatic equivalence on final answers;
(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria
scored by an independent LLM judge, with reward given by the fraction
satisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge
scores), synthesis may disagree with the majority and be correct even when all
rollouts are wrong; performance scales with the number of rollouts. As a
test-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up
to +27% on MATH-500; +12% on HealthBench). With reinforcement learning
(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained
policy surpassing the initial teacher signal.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [104] [Snail Homing and Mating Search Algorithm for Weight Optimization of Stepped-Transmission Shaft](https://arxiv.org/abs/2509.13721)
*Kaustav Saha,Ishaan R Kale,Vivek Patel,Anand J Kulkarni,Puskaraj D Sonawwanay*

Main category: cs.NE

TL;DR: 本文提出阶梯传动轴设计问题进行重量优化，用SHMS算法求解，建模、分析并对比结果。


<details>
  <summary>Details</summary>
Motivation: 对阶梯传动轴设计进行重量优化。

Method: 利用受蜗牛行为启发的SHMS算法，结合疲劳加载、组合载荷和修正古德曼准则建模，用MDSOLIDS软件获取力和弯矩图，用静态惩罚函数处理约束。

Result: SHMS算法以合理计算成本得到理想解，用统计结果生成CAD模型，在ANSYS Workbench分析并对比挠度。

Conclusion: 未明确提及结论，但通过算法求解和对比分析为阶梯传动轴设计优化提供了方法。

Abstract: In this paper, the steeped-transmission shaft design problem is proposed for
weight optimization. The bio-inspired search-based Snail Homing and Mating
Search (SHMS) algorithm is utilized to solve the problem. It is inspired by the
social behaviour of snails and their inherent nature of finding better homes,
and mate. The proposed steeped-transmission shaft design problem is modelled
considering the fatigue loading, combined bending, torsion loads, and the
principle of Modified Goodman criteria. The forces diagram and the bending
moment diagrams are obtained using the MDSOLIDS software. The forces and
bending moment are then used to mathematical model the objective function and
constraints. The SHMS algorithm has yielded the desired solution with
reasonable computational cost. The constraints are handled using a static
penalty function approach. The statistical results obtained using SHMS
algorithm are further used for generating CAD model. The analysis is carried
out in ANSYS Workbench. Further, the deflection obtained from SHMS algorithm
and ANSYS Workbench are compared and results are discussed in details.

</details>


### [105] [A neuromorphic continuous soil monitoring system for precision irrigation](https://arxiv.org/abs/2509.14066)
*Mirco Tincani,Khaled Kerouch,Umberto Garlando,Mattia Barezzi,Alessandro Sanginario,Giacomo Indiveri,Chiara De Luca*

Main category: cs.NE

TL;DR: 提出全节能神经形态灌溉控制系统，利用真实世界数据验证，结果显示本地推理可保持决策准确性，为大规模灌溉方案奠基。


<details>
  <summary>Details</summary>
Motivation: 边缘感官处理需超低功耗独立计算技术，现代农业和精准灌溉系统需持续监测环境，神经形态处理系统适用于资源受限的极端边缘计算应用。

Method: 利用生物现实的脉冲神经网络特性，在本地进行计算和决策。

Result: 使用苹果和猕猴桃果园的土壤湿度数据验证，生成的灌溉命令与传统方法得出的命令在不同土壤深度上紧密匹配。

Conclusion: 本地神经形态推理可保持决策准确性，为大规模自主、可持续灌溉解决方案铺平道路。

Abstract: Sensory processing at the edge requires ultra-low power stand-alone computing
technologies. This is particularly true for modern agriculture and precision
irrigation systems which aim to optimize water usage by monitoring key
environmental observables continuously using distributed efficient embedded
processing elements. Neuromorphic processing systems are emerging as a
promising technology for extreme edge-computing applications that need to run
on resource-constrained hardware. As such, they are a very good candidate for
implementing efficient water management systems based on data measured from
soil and plants, across large fields. In this work, we present a fully
energy-efficient neuromorphic irrigation control system that operates
autonomously without any need for data transmission or remote processing.
Leveraging the properties of a biologically realistic spiking neural network,
our system performs computation, and decision-making locally. We validate this
approach using real-world soil moisture data from apple and kiwi orchards
applied to a mixed-signal neuromorphic processor, and show that the generated
irrigation commands closely match those derived from conventional methods
across different soil depths. Our results show that local neuromorphic
inference can maintain decision accuracy, paving the way for autonomous,
sustainable irrigation solutions at scale.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [106] [Outperforming Dijkstra on Sparse Graphs: The Lightning Network Use Case](https://arxiv.org/abs/2509.13448)
*Danila Valko,Rohan Paranjpe,Jorge Marx Gómez*

Main category: cs.PF

TL;DR: 本文实现BMSSP算法并与Dijkstra算法对比，发现BMSSP实际未显著优于Dijkstra，为加速LN路由提供实证。


<details>
  <summary>Details</summary>
Motivation: 高效路由对支付通道网络至关重要，虽BMSSP理论更快，但需实际验证其性能。

Method: 在Rust中实现BMSSP算法，用真实LN拓扑数据与Dijkstra算法对比，进行多次随机试验和统计测试。

Result: 当前BMSSP实现未显著优于Dijkstra，加速效果小于理论预期，可能因实现和常数因子开销。

Conclusion: 为BMSSP加速LN路由提供首份实证，为PCN路径查找算法优化提供参考。

Abstract: Efficient routing is critical for payment channel networks (PCNs) such as the
Lightning Network (LN), where most clients currently rely on Dijkstra-based
algorithms for payment pathfinding. While Dijkstra's algorithm has long been
regarded as optimal on sparse graphs, recent theoretical work challenges this
view. The new Bounded Multi-Source Shortest Path (BMSSP) algorithm by Duan et
al. theoretically achieves $O(m~log^{2/3}~n)$ runtime, which is asymptotically
faster than Dijkstra's $O(m + n~log~n)$ on sparse directed graphs. In this
paper, we implement BMSSP on Rust and compare its performance against
Dijkstra's using real LN topology data. Our evaluation, based on multiple
randomized trials and statistical tests, shows that current implementations of
BMSSP do not significantly outperform Dijkstra's in practice, and speedups are
smaller than what theory predicts, possibly due to implementation and constant
factor overheads. These results provide the first empirical evidence of BMSSP's
potential to accelerate LN routing and inform future optimizations of PCN
pathfinding algorithms.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [107] [Is Research Software Science a Metascience?](https://arxiv.org/abs/2509.13436)
*Evan Eisinger,Michael A. Heroux*

Main category: cs.SE

TL;DR: 探讨研究软件科学（RSS）是否属于元科学，分析后认为RSS是独特跨学科领域，与元科学契合，认可其地位有益科研。


<details>
  <summary>Details</summary>
Motivation: 科研依赖计算方法，科研结果可靠性取决于研究软件质量等，需明确RSS分类以影响其认可、资助和融入科研改进。

Method: 定义元科学和RSS，比较原则与目标，分析重叠部分，列举支持和反对分类的论据。

Result: RSS推进元科学核心目标，尤其在计算可重复性方面，其分类取决于对元科学的宽窄定义。

Conclusion: RSS是独特跨学科领域，与元科学相符，认可其地位可强化其作用，严格对待研究软件可确保科研工具达标。

Abstract: As research increasingly relies on computational methods, the reliability of
scientific results depends on the quality, reproducibility, and transparency of
research software. Ensuring these qualities is critical for scientific
integrity and discovery. This paper asks whether Research Software Science
(RSS)--the empirical study of how research software is developed and
used--should be considered a form of metascience, the science of science.
Classification matters because it could affect recognition, funding, and
integration of RSS into research improvement. We define metascience and RSS,
compare their principles and objectives, and examine their overlaps. Arguments
for classification highlight shared commitments to reproducibility,
transparency, and empirical study of research processes. Arguments against
portraying RSS as a specialized domain focused on a tool rather than the
broader scientific enterprise. Our analysis finds RSS advances core goals of
metascience, especially in computational reproducibility, and bridges
technical, social, and cognitive aspects of research. Its classification
depends on whether one adopts a broad definition of metascience--any empirical
effort to improve science--or a narrow one focused on systemic and
epistemological structures. We argue RSS is best understood as a distinct
interdisciplinary domain that aligns with, and in some definitions fits within,
metascience. Recognizing it as such can strengthen its role in improving
reliability, justify funding, and elevate software development in research
institutions. Regardless of classification, applying scientific rigor to
research software ensures the tools of discovery meet the standards of the
discoveries themselves.

</details>


### [108] [An LLM Agentic Approach for Legal-Critical Software: A Case Study for Tax Prep Software](https://arxiv.org/abs/2509.13471)
*Sina Gogani-Khiabani,Ashutosh Trivedi,Diptikalyan Saha,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: 提出用代理方法开发法律关键软件，以美国联邦税务准备为例，结合高阶变形关系和LLM驱动框架，实验显示小模型框架表现更佳，支持代理LLM方法开发法律软件。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在将自然语言法规转化为可执行逻辑时，在法律关键场景的可靠性因歧义与幻觉问题面临挑战。

Method: 基于变形测试引入高阶变形关系，用LLM驱动的基于角色的框架自动化测试生成与代码合成，实现多智能体系统。

Result: 使用较小模型（GPT - 4o - mini）的框架在复杂税务代码任务上最坏情况通过率达45%，优于前沿模型（GPT - 4o和Claude 3.5，9 - 15%）。

Conclusion: 代理LLM方法是从自然语言规范开发健壮、可信法律关键软件的可行途径。

Abstract: Large language models (LLMs) show promise for translating natural-language
statutes into executable logic, but reliability in legally critical settings
remains challenging due to ambiguity and hallucinations. We present an agentic
approach for developing legal-critical software, using U.S. federal tax
preparation as a case study. The key challenge is test-case generation under
the oracle problem, where correct outputs require interpreting law. Building on
metamorphic testing, we introduce higher-order metamorphic relations that
compare system outputs across structured shifts among similar individuals.
Because authoring such relations is tedious and error-prone, we use an
LLM-driven, role-based framework to automate test generation and code
synthesis. We implement a multi-agent system that translates tax code into
executable software and incorporates a metamorphic-testing agent that searches
for counterexamples. In experiments, our framework using a smaller model
(GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier
models (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks. These results
support agentic LLM methodologies as a path to robust, trustworthy
legal-critical software from natural-language specifications.

</details>


### [109] [Prompt2DAG: A Modular Methodology for LLM-Based Data Enrichment Pipeline Generation](https://arxiv.org/abs/2509.13487)
*Abubakari Alidu,Michele Ciavotta,Flavio DePaoli*

Main category: cs.SE

TL;DR: 提出Prompt2DAG方法将自然语言描述转为可执行DAG，评估四种生成方法，混合方法最优，表明结构化混合方法对自动化工作流生成很重要。


<details>
  <summary>Details</summary>
Motivation: 开发可靠数据丰富管道需要大量工程专业知识，希望找到生产级自动化的最优策略。

Method: 提出Prompt2DAG方法，评估Direct、LLM - only、Hybrid和Template - based四种生成方法，用惩罚性评分框架衡量性能。

Result: 混合方法是最优生成方法，成功率78.5%，质量分数高，显著优于LLM - only和Direct方法，且成本效益更高。

Conclusion: 结构化混合方法对平衡自动化工作流生成的灵活性和可靠性至关重要，为数据管道开发民主化提供可行途径。

Abstract: Developing reliable data enrichment pipelines demands significant engineering
expertise. We present Prompt2DAG, a methodology that transforms natural
language descriptions into executable Apache Airflow DAGs. We evaluate four
generation approaches -- Direct, LLM-only, Hybrid, and Template-based -- across
260 experiments using thirteen LLMs and five case studies to identify optimal
strategies for production-grade automation. Performance is measured using a
penalized scoring framework that combines reliability with code quality (SAT),
structural integrity (DST), and executability (PCT). The Hybrid approach
emerges as the optimal generative method, achieving a 78.5% success rate with
robust quality scores (SAT: 6.79, DST: 7.67, PCT: 7.76). This significantly
outperforms the LLM-only (66.2% success) and Direct (29.2% success) methods.
Our findings show that reliability, not intrinsic code quality, is the primary
differentiator. Cost-effectiveness analysis reveals the Hybrid method is over
twice as efficient as Direct prompting per successful DAG. We conclude that a
structured, hybrid approach is essential for balancing flexibility and
reliability in automated workflow generation, offering a viable path to
democratize data pipeline development.

</details>


### [110] [Crash Report Enhancement with Large Language Models: An Empirical Study](https://arxiv.org/abs/2509.13535)
*S M Farah Al Fahim,Md Nakhla Rafi,Zeyang Ma,Dong Jae Kim,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: 研究大语言模型能否增强崩溃报告，对比两种策略，实验表明增强报告对调试更有用。


<details>
  <summary>Details</summary>
Motivation: 许多崩溃报告缺乏开发者高效调试所需的诊断细节，研究大语言模型能否增强崩溃报告。

Method: 研究两种增强策略，Direct - LLM单步方法和Agentic - LLM迭代方法，并在492个真实世界崩溃报告数据集上实验。

Result: 增强报告将Top - 1问题定位准确率从10.6%提高到40.2 - 43.1%，修复建议与开发者补丁相似，Agentic - LLM表现更好，用户研究表明增强报告使崩溃更易理解和解决。

Conclusion: 为大语言模型提供堆栈跟踪和仓库代码可生成对调试更有用的增强崩溃报告。

Abstract: Crash reports are central to software maintenance, yet many lack the
diagnostic detail developers need to debug efficiently. We examine whether
large language models can enhance crash reports by adding fault locations,
root-cause explanations, and repair suggestions. We study two enhancement
strategies: Direct-LLM, a single-shot approach that uses stack-trace context,
and Agentic-LLM, an iterative approach that explores the repository for
additional evidence. On a dataset of 492 real-world crash reports, LLM-enhanced
reports improve Top-1 problem-localization accuracy from 10.6% (original
reports) to 40.2-43.1%, and produce suggested fixes that closely resemble
developer patches (CodeBLEU around 56-57%). Both our manual evaluations and
LLM-as-a-judge assessment show that Agentic-LLM delivers stronger root-cause
explanations and more actionable repair guidance. A user study with 16
participants further confirms that enhanced reports make crashes easier to
understand and resolve, with the largest improvement in repair guidance. These
results indicate that supplying LLMs with stack traces and repository code
yields enhanced crash reports that are substantially more useful for debugging.

</details>


### [111] [GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You Commit?](https://arxiv.org/abs/2509.13650)
*Amena Amro,Manar H. Alalfi*

Main category: cs.SE

TL;DR: 研究评估GitHub Copilot代码审查功能检测安全漏洞的有效性，发现其常无法检测关键漏洞，凸显专用安全工具和人工审查的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着软件开发采用AI工具，确保其支持安全编码至关重要，评估GitHub Copilot代码审查功能检测安全漏洞的有效性。

Method: 使用来自不同开源项目的标记漏洞代码样本，系统评估Copilot识别常见安全漏洞并提供反馈的能力。

Result: Copilot代码审查常无法检测关键漏洞，如SQL注入、XSS和不安全反序列化，主要关注低严重性问题。

Conclusion: AI辅助代码审查实际效果与预期有差距，仍需专用安全工具和人工代码审计确保软件安全。

Abstract: As software development practices increasingly adopt AI-powered tools,
ensuring that such tools can support secure coding has become critical. This
study evaluates the effectiveness of GitHub Copilot's recently introduced code
review feature in detecting security vulnerabilities. Using a curated set of
labeled vulnerable code samples drawn from diverse open-source projects
spanning multiple programming languages and application domains, we
systematically assessed Copilot's ability to identify and provide feedback on
common security flaws. Contrary to expectations, our results reveal that
Copilot's code review frequently fails to detect critical vulnerabilities such
as SQL injection, cross-site scripting (XSS), and insecure deserialization.
Instead, its feedback primarily addresses low-severity issues, such as coding
style and typographical errors. These findings expose a significant gap between
the perceived capabilities of AI-assisted code review and its actual
effectiveness in supporting secure development practices. Our results highlight
the continued necessity of dedicated security tools and manual code audits to
ensure robust software security.

</details>


### [112] [A Regression Testing Framework with Automated Assertion Generation for Machine Learning Notebooks](https://arxiv.org/abs/2509.13656)
*Yingao Elaine Yao,Vedant Nimje,Varun Viswanath,Saikat Dutta*

Main category: cs.SE

TL;DR: 本文介绍NBTest回归测试框架，能在笔记本中写单元级断言，评估显示其有效且被采用，用户评价高。


<details>
  <summary>Details</summary>
Motivation: 现有笔记本对测试支持有限，开发中细微错误难发现，导致性能下降。

Method: 引入NBTest框架，提供断言API和JupyterLab插件，开发自动生成单元级断言方法，利用统计技术减少断言不稳定性。

Result: 在592个Kaggle笔记本上评估，生成21163个断言，突变得分0.57，能捕捉回归错误，被流行ML库CI采用，用户评价高。

Conclusion: NBTest能在不增加开发者负担的情况下，提高ML笔记本的可靠性和可维护性。

Abstract: Notebooks have become the de-facto choice for data scientists and machine
learning engineers for prototyping and experimenting with machine learning (ML)
pipelines. Notebooks provide an interactive interface for code, data, and
visualization. However, notebooks provide very limited support for testing.
Thus, during continuous development, many subtle bugs that do not lead to
crashes often go unnoticed and cause silent errors that manifest as performance
regressions.
  To address this, we introduce NBTest - the first regression testing framework
that allows developers to write cell-level assertions in notebooks and run such
notebooks in pytest or in continuous integration (CI) pipelines. NBTest offers
a library of assertion APIs, and a JupyterLab plugin that enables executing
assertions. We also develop the first automated approach for generating
cell-level assertions for key components in ML notebooks, such as data
processing, model building, and model evaluation. NBTest aims to improve the
reliability and maintainability of ML notebooks without adding developer
burden.
  We evaluate NBTest on 592 Kaggle notebooks. Overall, NBTest generates 21163
assertions (35.75 on average per notebook). The generated assertions obtain a
mutation score of 0.57 in killing ML-specific mutations. NBTest can catch
regression bugs in previous versions of the Kaggle notebooks using assertions
generated for the latest versions. Because ML pipelines involve non
deterministic computations, the assertions can be flaky. Hence, we also show
how NBTest leverages statistical techniques to minimize flakiness while
retaining high fault-detection effectiveness. NBTest has been adopted in the CI
of a popular ML library. Further, we perform a user study with 17 participants
that shows that notebook users find NBTest intuitive (Rating 4.3/5) and useful
in writing assertions and testing notebooks (Rating 4.24/5).

</details>


### [113] [Prompt Stability in Code LLMs: Measuring Sensitivity across Emotion- and Personality-Driven Variations](https://arxiv.org/abs/2509.13680)
*Wei Ma,Yixiao Yang,Jingquan Ge,Xiaofei Xie,Lingxiao Jiang*

Main category: cs.SE

TL;DR: 提出PromptSE框架评估代码生成模型对提示的敏感性，研究发现性能与稳定性解耦，支持模型筛选与分析。


<details>
  <summary>Details</summary>
Motivation: 代码生成模型对提示措辞的敏感性未充分研究，多数基准仅关注峰值性能。

Method: 创建带有情感和个性模板的语义等效提示变体，用概率感知连续评分或二元通过率评估稳定性，提出AUC - E指标进行跨模型比较。

Result: 研究显示性能和稳定性是解耦的优化目标，揭示了与架构和规模相关的模式。

Conclusion: PromptSE框架可量化性能稳定性权衡，将提示稳定性作为补充评估维度，有助于开发更可信的AI辅助软件开发工具。

Abstract: Code generation models are widely used in software development, yet their
sensitivity to prompt phrasing remains under-examined. Identical requirements
expressed with different emotions or communication styles can yield divergent
outputs, while most benchmarks emphasize only peak performance. We present
PromptSE (Prompt Sensitivity Evaluation), a framework that creates semantically
equivalent prompt variants with emotion and personality templates, and that
evaluates stability using probability aware continuous scoring or using binary
pass rates when logits are unavailable. The results are aggregated into a
proposed area under curve metric (AUC-E) for cross model comparison. Across 14
models from three families (Llama, Qwen, and DeepSeek), our study shows that
performance and stability behave as largely decoupled optimization objectives,
and it reveals architectural and scale related patterns that challenge common
assumptions about model robustness. The framework supports rapid screening for
closed-source models as well as detailed stability analysis in research
settings. PromptSE enables practitioners to quantify performance stability
trade offs for deployment and model selection, positioning prompt stability as
a complementary evaluation dimension alongside performance and fairness, and
contributing to more trustworthy AI-assisted software development tools.

</details>


### [114] [Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning](https://arxiv.org/abs/2509.13755)
*Zhaoyang Chu,Yao Wan,Zhikun Zhang,Di Wang,Zhou Yang,Hongyu Zhang,Pan Zhou,Xuanhua Shi,Hai Jin,David Lo*

Main category: cs.SE

TL;DR: 论文探讨CLMs敏感信息擦除问题，用机器学习遗忘方法，提出CodeEraser，实验验证其有效性和效率。


<details>
  <summary>Details</summary>
Motivation: CLMs存在敏感数据记忆隐私漏洞，现有方法需全模型重训成本高，要有效高效擦除敏感信息。

Method: 先量化CLM训练数据敏感记忆风险并整理目标数据集，研究两种梯度上升遗忘方法，提出CodeEraser。

Result: 在三类CLMs上实验，验证CodeEraser能擦除敏感记忆并保持模型效用。

Conclusion: CodeEraser可有效且高效地擦除CLMs中敏感记忆，同时维持模型实用性。

Abstract: While Code Language Models (CLMs) have demonstrated superior performance in
software engineering tasks such as code generation and summarization, recent
empirical studies reveal a critical privacy vulnerability: these models exhibit
unintended memorization of sensitive training data, enabling verbatim
reproduction of confidential information when specifically prompted. To address
this issue, several approaches, including training data de-duplication and
differential privacy augmentation, have been proposed. However, these methods
require full-model retraining for deployed CLMs, which incurs substantial
computational costs. In this paper, we aim to answer the following research
question: Can sensitive information memorized by CLMs be erased effectively and
efficiently?
  We conduct a pioneering investigation into erasing sensitive memorization in
CLMs through machine unlearning - a post-hoc modification method that removes
specific information from trained models without requiring full retraining.
Specifically, we first quantify the memorization risks of sensitive data within
CLM training datasets and curate a high-risk dataset of 50,000 sensitive
memorized samples as unlearning targets. We study two widely used gradient
ascent-based unlearning approaches: the vanilla and constraint-based methods,
and introduce CodeEraser, an advanced variant that selectively unlearns
sensitive memorized segments in code while preserving the structural integrity
and functional correctness of the surrounding code. Extensive experiments on
three families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder,
validate the effectiveness and efficiency of CodeEraser in erasing targeted
sensitive memorization while maintaining model utility.

</details>


### [115] [A Study on Thinking Patterns of Large Reasoning Models in Code Generation](https://arxiv.org/abs/2509.13758)
*Kevin Halim,Sin G. Teo,Ruitao Feng,Zhenpeng Chen,Yang Gu,Chong Wang,Yang Liu*

Main category: cs.SE

TL;DR: 文章对大推理模型（LRMs）在代码生成中的推理行为进行全面研究，得出推理行为分类，有关于推理模式、模型对比、推理与代码正确性关系等发现，评估轻量级提示策略以提升代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前对LRMs在代码生成中的推理模式及其对生成代码的影响缺乏系统分析，本文旨在填补这一空白。

Method: 用代码生成任务提示不同规模的LRMs，应用开放编码手动注释推理痕迹，得出推理行为分类。

Result: 发现常见推理模式，比较不同模型推理方式，分析推理与代码正确性关系，评估轻量级提示策略可提升代码质量。

Conclusion: 研究结果为推进自动代码生成提供了见解和实际意义。

Abstract: Currently, many large language models (LLMs) are utilized for software
engineering tasks such as code generation. The emergence of more advanced
models known as large reasoning models (LRMs), such as OpenAI's o3, DeepSeek
R1, and Qwen3. They have demonstrated the capability of performing multi-step
reasoning. Despite the advancement in LRMs, little attention has been paid to
systematically analyzing the reasoning patterns these models exhibit and how
such patterns influence the generated code. This paper presents a comprehensive
study aimed at investigating and uncovering the reasoning behavior of LRMs
during code generation. We prompted several state-of-the-art LRMs of varying
sizes with code generation tasks and applied open coding to manually annotate
the reasoning traces. From this analysis, we derive a taxonomy of LRM reasoning
behaviors, encompassing 15 reasoning actions across four phases.
  Our empirical study based on the taxonomy reveals a series of findings.
First, we identify common reasoning patterns, showing that LRMs generally
follow a human-like coding workflow, with more complex tasks eliciting
additional actions such as scaffolding, flaw detection, and style checks.
Second, we compare reasoning across models, finding that Qwen3 exhibits
iterative reasoning while DeepSeek-R1-7B follows a more linear, waterfall-like
approach. Third, we analyze the relationship between reasoning and code
correctness, showing that actions such as unit test creation and scaffold
generation strongly support functional outcomes, with LRMs adapting strategies
based on task context. Finally, we evaluate lightweight prompting strategies
informed by these findings, demonstrating the potential of context- and
reasoning-oriented prompts to improve LRM-generated code. Our results offer
insights and practical implications for advancing automatic code generation.

</details>


### [116] [Who is Introducing the Failure? Automatically Attributing Failures of Multi-Agent Systems via Spectrum Analysis](https://arxiv.org/abs/2509.13782)
*Yu Ge,Linna Xie,Zhong Li,Yu Pei,Tian Zhang*

Main category: cs.SE

TL;DR: 提出FAMAS用于大语言模型驱动的多智能体系统（MASs）故障归因，评估显示其性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: MASs虽有应用但故障归因研究不足且费力，影响调试和系统改进。

Method: 提出FAMAS，通过系统轨迹重放和抽象，再进行频谱分析，还有针对MASs的可疑性公式。

Result: 在Who and When基准上与12个基线对比评估，FAMAS表现优于所有对比方法。

Conclusion: FAMAS是有效的MASs故障归因方法。

Abstract: Large Language Model Powered Multi-Agent Systems (MASs) are increasingly
employed to automate complex real-world problems, such as programming and
scientific discovery. Despite their promising, MASs are not without their
flaws. However, failure attribution in MASs - pinpointing the specific agent
actions responsible for failures - remains underexplored and labor-intensive,
posing significant challenges for debugging and system improvement. To bridge
this gap, we propose FAMAS, the first spectrum-based failure attribution
approach for MASs, which operates through systematic trajectory replay and
abstraction, followed by spectrum analysis.The core idea of FAMAS is to
estimate, from variations across repeated MAS executions, the likelihood that
each agent action is responsible for the failure. In particular, we propose a
novel suspiciousness formula tailored to MASs, which integrates two key factor
groups, namely the agent behavior group and the action behavior group, to
account for the agent activation patterns and the action activation patterns
within the execution trajectories of MASs. Through expensive evaluations
against 12 baselines on the Who and When benchmark, FAMAS demonstrates superior
performance by outperforming all the methods in comparison.

</details>


### [117] [Trace Sampling 2.0: Code Knowledge Enhanced Span-level Sampling for Distributed Tracing](https://arxiv.org/abs/2509.13852)
*Yulun Wu,Guangba Yu,Zhihan Jiang,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: 提出Trace Sampling 2.0概念并实现Autoscope方法，在两开源微服务评估，能降迹大小81.2%、保98.1%故障跨度覆盖，根因分析平均提升8.3%。


<details>
  <summary>Details</summary>
Motivation: 传统跟踪采样方法常丢弃有价值信息，需新方法减少存储开销同时保留更多信息。

Method: 引入Trace Sampling 2.0，设计实现基于跨度级采样的Autoscope方法，利用静态分析提取执行逻辑。

Result: Autoscope减少跟踪大小81.2%，保持98.1%故障跨度覆盖，根因分析平均提升8.3%。

Conclusion: Autoscope能显著提高微服务可观测性和存储效率，为性能监控提供有力解决方案。

Abstract: Distributed tracing is an essential diagnostic tool in microservice systems,
but the sheer volume of traces places a significant burden on backend storage.
A common approach to mitigating this issue is trace sampling, which selectively
retains traces based on specific criteria, often preserving only anomalous
ones. However, this method frequently discards valuable information, including
normal traces that are essential for comparative analysis. To address this
limitation, we introduce Trace Sampling 2.0, which operates at the span level
while maintaining trace structure consistency. This approach allows for the
retention of all traces while significantly reducing storage overhead. Based on
this concept, we design and implement Autoscope, a span-level sampling method
that leverages static analysis to extract execution logic, ensuring that
critical spans are preserved without compromising structural integrity. We
evaluated Autoscope on two open-source microservices. Our results show that it
reduces trace size by 81.2% while maintaining 98.1% faulty span coverage,
outperforming existing trace-level sampling methods. Furthermore, we
demonstrate its effectiveness in root cause analysis, achieving an average
improvement of 8.3%. These findings indicate that Autoscope can significantly
enhance observability and storage efficiency in microservices, offering a
robust solution for performance monitoring.

</details>


### [118] [Are Prompts All You Need? Evaluating Prompt-Based Large Language Models (LLM)s for Software Requirements Classification](https://arxiv.org/abs/2509.13868)
*Manal Binkhonain,Reem Alfayaz*

Main category: cs.SE

TL;DR: 研究基于提示的大语言模型能否减少需求分类的数据需求，通过多任务基准测试，发现基于提示的LLM尤其是少样本提示可媲美或超越基线，加人设等可进一步提升效果，是实用且可扩展方案。


<details>
  <summary>Details</summary>
Motivation: 现有需求分类模型依赖监督学习，需大量标注数据，成本高、创建慢、依赖领域、泛化性差且常需为每个任务重新训练，因此测试基于提示的大语言模型能否减少数据需求。

Method: 在两个英语数据集PROMISE和SecReq上对多个任务，对几种模型和提示风格（零样本、少样本、人设、思维链）进行基准测试，比较模型提示配置并与强微调的Transformer基线对比。

Result: 基于提示的LLM尤其是少样本提示能匹配或超越基线，添加人设或人设加思维链可进一步提升效果。

Conclusion: 基于提示的LLM是实用且可扩展的选择，可减少对大量标注的依赖并提高跨任务的泛化性。

Abstract: Requirements classification assigns natural language requirements to
predefined classes, such as functional and non functional. Accurate
classification reduces risk and improves software quality. Most existing models
rely on supervised learning, which needs large labeled data that are costly,
slow to create, and domain dependent; they also generalize poorly and often
require retraining for each task. This study tests whether prompt based large
language models can reduce data needs. We benchmark several models and
prompting styles (zero shot, few shot, persona, and chain of thought) across
multiple tasks on two English datasets, PROMISE and SecReq. For each task we
compare model prompt configurations and then compare the best LLM setups with a
strong fine tuned transformer baseline. Results show that prompt based LLMs,
especially with few shot prompts, can match or exceed the baseline. Adding a
persona, or persona plus chain of thought, can yield further gains. We conclude
that prompt based LLMs are a practical and scalable option that reduces
dependence on large annotations and can improve generalizability across tasks.

</details>


### [119] [Mind the Ethics! The Overlooked Ethical Dimensions of GenAI in Software Modeling Education](https://arxiv.org/abs/2509.13896)
*Shalini Chakraborty,Lola Burgueño,Nathalie Moreno,Javier Troya,Paula Muñoz*

Main category: cs.SE

TL;DR: 文章指出GenAI在软件建模教育中应用增多但伦理问题研究不足，通过系统文献综述发现相关伦理研究稀缺，需构建伦理框架。


<details>
  <summary>Details</summary>
Motivation: GenAI在软件建模教育中应用广泛但缺乏伦理监督和教学指导，其伦理影响未充分探索，因此开展研究。

Method: 对计算机科学六大数字图书馆进行系统文献综述，识别讨论GenAI伦理方面的研究。

Result: 最初检索到1386篇论文，仅3篇明确提及伦理考量，凸显该领域伦理研究稀缺。

Conclusion: 软件建模教育中GenAI伦理讨论缺失，迫切需要构建结构化伦理框架，同时探讨了研究机会和挑战。

Abstract: Generative Artificial Intelligence (GenAI) is rapidly gaining momentum in
software modeling education, embraced by both students and educators. As GenAI
assists with interpreting requirements, formalizing models, and translating
students' mental models into structured notations, it increasingly shapes core
learning outcomes such as domain comprehension, diagrammatic thinking, and
modeling fluency without clear ethical oversight or pedagogical guidelines.
Yet, the ethical implications of this integration remain underexplored.
  In this paper, we conduct a systematic literature review across six major
digital libraries in computer science (ACM Digital Library, IEEE Xplore,
Scopus, ScienceDirect, SpringerLink, and Web of Science). Our aim is to
identify studies discussing the ethical aspects of GenAI in software modeling
education, including responsibility, fairness, transparency, diversity, and
inclusion among others.
  Out of 1,386 unique papers initially retrieved, only three explicitly
addressed ethical considerations. This scarcity highlights the critical absence
of ethical discourse surrounding GenAI in modeling education and raises urgent
questions about the responsible integration of AI in modeling curricula, as
well as it evinces the pressing need for structured ethical frameworks in this
emerging educational landscape. We examine these three studies and explore the
emerging research opportunities as well as the challenges that have arisen in
this field.

</details>


### [120] [An Empirical Study on Failures in Automated Issue Solving](https://arxiv.org/abs/2509.13941)
*Simiao Liu,Fang Liu,Liehao Li,Xin Tan,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: 分析SOTA工具在SWE - Bench自动问题解决任务中的表现，对失败实例手动分析建立分类法，发现不同架构失败特点，提出Expert - Executor框架解决部分难题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM - based工具在自动问题解决任务有失败情况，且当前评估主要是整体解决率，难以诊断模型弱点和指导改进。

Method: 分析三种SOTA工具在SWE - Bench - Verified自动问题解决任务的表现和效率；手动分析150个失败实例建立失败模式分类法；提出Expert - Executor框架。

Result: 发现不同架构有不同失败特点，多数代理失败源于推理缺陷和认知僵局；框架解决了领先单代理22.2%先前难解决的问题。

Conclusion: 通过诊断评估和协作设计为构建更强大的代理铺平道路。

Abstract: Automated issue solving seeks to autonomously identify and repair defective
code snippets across an entire codebase. SWE-Bench has emerged as the most
widely adopted benchmark for evaluating progress in this area. While LLM-based
agentic tools show great promise, they still fail on a substantial portion of
tasks. Moreover, current evaluations primarily report aggregate issue-solving
rates, which obscure the underlying causes of success and failure, making it
challenging to diagnose model weaknesses or guide targeted improvements. To
bridge this gap, we first analyze the performance and efficiency of three SOTA
tools, spanning both pipeline-based and agentic architectures, in automated
issue solving tasks of SWE-Bench-Verified under varying task characteristics.
Furthermore, to move from high-level performance metrics to underlying cause
analysis, we conducted a systematic manual analysis of 150 failed instances.
From this analysis, we developed a comprehensive taxonomy of failure modes
comprising 3 primary phases, 9 main categories, and 25 fine-grained
subcategories. Then we systematically analyze the distribution of the
identified failure modes, the results reveal distinct failure fingerprints
between the two architectural paradigms, with the majority of agentic failures
stemming from flawed reasoning and cognitive deadlocks. Motivated by these
insights, we propose a collaborative Expert-Executor framework. It introduces a
supervisory Expert agent tasked with providing strategic oversight and
course-correction for a primary Executor agent. This architecture is designed
to correct flawed reasoning and break the cognitive deadlocks that frequently
lead to failure. Experiments show that our framework solves 22.2% of previously
intractable issues for a leading single agent. These findings pave the way for
building more robust agents through diagnostic evaluation and collaborative
design.

</details>


### [121] [Evaluating Classical Software Process Models as Coordination Mechanisms for LLM-Based Software Generation](https://arxiv.org/abs/2509.13942)
*Duc Minh Ha,Phu Trac Kien,Tho Quan,Anh Nguyen-Duc*

Main category: cs.SE

TL;DR: 研究探索传统软件开发流程适配基于大语言模型的多智能体系统的方式及影响，通过实验得出不同流程各有优劣，选择应依项目目标而定。


<details>
  <summary>Details</summary>
Motivation: 探索传统软件开发流程如何作为基于大语言模型的多智能体系统的协调框架，并研究其对代码质量、成本和生产力的影响。

Method: 在三种流程模型和四种GPT变体下执行11个不同的软件项目，共132次运行，用标准化指标评估输出。

Result: 流程模型和大语言模型选择都显著影响系统性能，瀑布模型最有效率，V模型代码最冗长，敏捷模型代码质量最高但计算成本高。

Conclusion: 传统软件流程可有效应用于基于大语言模型的多智能体系统，但各有质量、成本和适应性的权衡，流程选择应反映项目目标。

Abstract: [Background] Large Language Model (LLM)-based multi-agent systems (MAS) are
transforming software development by enabling autonomous collaboration.
Classical software processes such asWaterfall, V-Model, and Agile offer
structured coordination patterns that can be repurposed to guide these agent
interactions. [Aims] This study explores how traditional software development
processes can be adapted as coordination scaffolds for LLM based MAS and
examines their impact on code quality, cost, and productivity. [Method] We
executed 11 diverse software projects under three process models and four GPT
variants, totaling 132 runs. Each output was evaluated using standardized
metrics for size (files, LOC), cost (execution time, token usage), and quality
(code smells, AI- and human detected bugs). [Results] Both process model and
LLM choice significantly affected system performance. Waterfall was most
efficient, V-Model produced the most verbose code, and Agile achieved the
highest code quality, albeit at higher computational cost. [Conclusions]
Classical software processes can be effectively instantiated in LLM-based MAS,
but each entails trade-offs across quality, cost, and adaptability. Process
selection should reflect project goals, whether prioritizing efficiency,
robustness, or structured validation.

</details>


### [122] [Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework](https://arxiv.org/abs/2509.14093)
*Kerui Huang,Shuhan Liu,Xing Hu,Tongtong Xu,Lingfeng Bao,Xin Xia*

Main category: cs.SE

TL;DR: 研究思维链推理在代码生成中的问题，提出SEER框架压缩思维链，实验证明其能提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 思维链推理虽提升大语言模型性能，但有高计算成本，在软件工程任务中问题突出，需权衡利弊。

Method: 基于代码生成基准进行实证研究，提出SEER框架，结合Best - of - N采样与任务感知自适应过滤动态调整阈值。

Result: SEER平均缩短思维链42.1%，减少截断提高准确率，消除多数无限循环。

Conclusion: SEER是使思维链增强的大语言模型在资源受限下更高效、更健壮的实用方法。

Abstract: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by
prompting intermediate steps, improving accuracy and robustness in arithmetic,
logic, and commonsense tasks. However, this benefit comes with high
computational costs: longer outputs increase latency, memory usage, and
KV-cache demands. These issues are especially critical in software engineering
tasks where concise and deterministic outputs are required. To investigate
these trade-offs, we conduct an empirical study based on code generation
benchmarks. The results reveal that longer CoT does not always help. Excessive
reasoning often causes truncation, accuracy drops, and latency up to five times
higher, with failed outputs consistently longer than successful ones. These
findings challenge the assumption that longer reasoning is inherently better
and highlight the need for adaptive CoT control. Motivated by this, we propose
SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that
compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with
task-aware adaptive filtering, dynamically adjusting thresholds based on
pre-inference outputs to reduce verbosity and computational overhead. We then
evaluate SEER on three software engineering tasks and one math task. On
average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,
and eliminates most infinite loops. These results demonstrate SEER as a
practical method to make CoT-enhanced LLMs more efficient and robust, even
under resource constraints.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [123] [Holdout cross-validation for large non-Gaussian covariance matrix estimation using Weingarten calculus](https://arxiv.org/abs/2509.13923)
*Lamia Lamrani,Benoît Collins,Jean-Philippe Bouchaud*

Main category: q-fin.ST

TL;DR: 本文推导了通用旋转不变乘性噪声模型下留一法的期望Frobenius误差，拓展了先前结果到非高斯数据分布，分析了最优训练 - 测试分割比例，通过模拟发现二次近似有改进。


<details>
  <summary>Details</summary>
Motivation: 虽然交叉验证在大协方差矩阵估计中实践效果稳健，但对其误差的理论行为了解甚少，本文旨在研究留一法误差的理论行为。

Method: 使用Weingarten微积分和Ledoit - Péché公式推导高维极限下的最优特征值，用线性和二次收缩近似期望留一误差，进行蒙特卡罗模拟。

Result: 线性近似下最优训练 - 测试分割比例与矩阵维度平方根成正比；二次近似有显著改进；噪声向量欧氏范数的四阶矩影响留一误差曲线和理想训练 - 测试比例。

Conclusion: 二次近似在留一法中有优势，噪声的四阶矩会影响留一法中训练 - 测试比例的选择。

Abstract: Cross-validation is one of the most widely used methods for model selection
and evaluation; its efficiency for large covariance matrix estimation appears
robust in practice, but little is known about the theoretical behavior of its
error. In this paper, we derive the expected Frobenius error of the holdout
method, a particular cross-validation procedure that involves a single train
and test split, for a generic rotationally invariant multiplicative noise
model, therefore extending previous results to non-Gaussian data distributions.
Our approach involves using the Weingarten calculus and the Ledoit-P\'ech\'e
formula to derive the oracle eigenvalues in the high-dimensional limit. When
the population covariance matrix follows an inverse Wishart distribution, we
approximate the expected holdout error, first with a linear shrinkage, then
with a quadratic shrinkage to approximate the oracle eigenvalues. Under the
linear approximation, we find that the optimal train-test split ratio is
proportional to the square root of the matrix dimension. Then we compute Monte
Carlo simulations of the holdout error for different distributions of the norm
of the noise, such as the Gaussian, Student, and Laplace distributions and
observe that the quadratic approximation yields a substantial improvement,
especially around the optimal train-test split ratio. We also observe that a
higher fourth-order moment of the Euclidean norm of the noise vector sharpens
the holdout error curve near the optimal split and lowers the ideal train-test
ratio, making the choice of the train-test ratio more important when performing
the holdout method.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [124] [On the Rate of Gaussian Approximation for Linear Regression Problems](https://arxiv.org/abs/2509.14039)
*Marat Khusainov,Marina Sheshukova,Alain Durmus,Sergey Samsonov*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we consider the problem of Gaussian approximation for the
online linear regression task. We derive the corresponding rates for the
setting of a constant learning rate and study the explicit dependence of the
convergence rate upon the problem dimension $d$ and quantities related to the
design matrix. When the number of iterations $n$ is known in advance, our
results yield the rate of normal approximation of order $\sqrt{\log{n}/n}$,
provided that the sample size $n$ is large enough.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [125] [Why all roads don't lead to Rome: Representation geometry varies across the human visual cortical hierarchy](https://arxiv.org/abs/2509.13459)
*Arna Ghosh,Zahraa Chorghay,Shahab Bakhtiari,Blake A. Richards*

Main category: q-bio.NC

TL;DR: 研究生物和人工智能系统在效率 - 鲁棒性权衡问题上的表现，发现系统表征几何并非普遍属性，而是取决于计算目标。


<details>
  <summary>Details</summary>
Motivation: 理解系统如何应对效率 - 鲁棒性权衡挑战。

Method: 使用群体几何框架分析人类视觉皮层和人工神经网络的表征。

Result: 人类腹侧视觉流多数区域有通用、无标度表征，部分高阶视觉区域无；自监督学习目标训练的ANNs有无标度几何，特定任务微调后则无。

Conclusion: 系统的表征几何并非普遍属性，取决于计算目标。

Abstract: Biological and artificial intelligence systems navigate the fundamental
efficiency-robustness tradeoff for optimal encoding, i.e., they must
efficiently encode numerous attributes of the input space while also being
robust to noise. This challenge is particularly evident in hierarchical
processing systems like the human brain. With a view towards understanding how
systems navigate the efficiency-robustness tradeoff, we turned to a population
geometry framework for analyzing representations in the human visual cortex
alongside artificial neural networks (ANNs). In the ventral visual stream, we
found general-purpose, scale-free representations characterized by a power
law-decaying eigenspectrum in most areas. However, in certain higher-order
visual areas did not have scale-free representations, indicating that
scale-free geometry is not a universal property of the brain. In parallel, ANNs
trained with a self-supervised learning objective also exhibited free-free
geometry, but not after fine-tune on a specific task. Based on these empirical
results and our analytical insights, we posit that a system's representation
geometry is not a universal property and instead depends upon the computational
objective.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [126] [Artificial neural networks ensemble methodology to predict significant wave height](https://arxiv.org/abs/2509.14020)
*Felipe Crivellaro Minuzzi,Leandro Farina*

Main category: physics.ao-ph

TL;DR: 本文提出用不同神经网络架构创建集成模型预测巴西海岸六个地点的有效波高，展示新数据集创建策略，结果高效且降低成本。


<details>
  <summary>Details</summary>
Motivation: 波浪变量预测对依赖海洋状态描述的应用很重要，传统方法有困难，机器学习算法有优势，因此研究用神经网络集成模型预测有效波高。

Method: 创建MLP、RNN、LSTM、CNN和混合CNN - LSTM等不同神经网络架构的集成，用NOAA数值再预测数据训练，目标是观测数据与数值模型输出的残差，展示新的训练和目标数据集创建策略。

Result: 框架能高效预测，平均准确率80%，最佳可达88%，比NOAA数值模型误差指标降低5%，且不断降低计算成本。

Conclusion: 所提出的神经网络集成模型预测有效波高的框架高效，能提高准确率并降低计算成本。

Abstract: The forecast of wave variables are important for several applications that
depend on a better description of the ocean state. Due to the chaotic behaviour
of the differential equations which model this problem, a well know strategy to
overcome the difficulties is basically to run several simulations, by for
instance, varying the initial condition, and averaging the result of each of
these, creating an ensemble. Moreover, in the last few years, considering the
amount of available data and the computational power increase, machine learning
algorithms have been applied as surrogate to traditional numerical models,
yielding comparative or better results. In this work, we present a methodology
to create an ensemble of different artificial neural networks architectures,
namely, MLP, RNN, LSTM, CNN and a hybrid CNN-LSTM, which aims to predict
significant wave height on six different locations in the Brazilian coast. The
networks are trained using NOAA's numerical reforecast data and target the
residual between observational data and the numerical model output. A new
strategy to create the training and target datasets is demonstrated. Results
show that our framework is capable of producing high efficient forecast, with
an average accuracy of $80\%$, that can achieve up to $88\%$ in the best case
scenario, which means $5\%$ reduction in error metrics if compared to NOAA's
numerical model, and a increasingly reduction of computational cost.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [127] [Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks](https://arxiv.org/abs/2509.13338)
*Hassan Gharoun,Mohammad Sadegh Khorshidi,Kasra Ranjbarigderi,Fang Chen,Amir H. Gandomi*

Main category: cs.CV

TL;DR: 提出基于证据检索机制的不确定性感知决策方法，实验表明比基于预测熵阈值方法更优。


<details>
  <summary>Details</summary>
Motivation: 为不确定性感知决策提供更可靠、可解释的方法。

Method: 用证据条件、实例自适应标准取代单一全局阈值，在嵌入空间检索近邻样本，用Dempster - Shafer理论融合预测分布，以融合后的置信度作为实例阈值机制。

Result: 在CIFAR - 10/100实验中，比基于预测熵阈值方法有更高或相当的不确定性感知性能，错误结果少且审查负担低，少量证据就能实现提升。

Conclusion: 证据条件标记为实际的不确定性感知决策提供了比固定预测熵阈值更可靠、可解释的替代方案。

Abstract: This work proposes an evidence-retrieval mechanism for uncertainty-aware
decision-making that replaces a single global cutoff with an
evidence-conditioned, instance-adaptive criterion. For each test instance,
proximal exemplars are retrieved in an embedding space; their predictive
distributions are fused via Dempster-Shafer theory. The resulting fused belief
acts as a per-instance thresholding mechanism. Because the supporting evidences
are explicit, decisions are transparent and auditable. Experiments on
CIFAR-10/100 with BiT and ViT backbones show higher or comparable
uncertainty-aware performance with materially fewer confidently incorrect
outcomes and a sustainable review load compared with applying threshold on
prediction entropy. Notably, only a few evidences are sufficient to realize
these gains; increasing the evidence set yields only modest changes. These
results indicate that evidence-conditioned tagging provides a more reliable and
interpretable alternative to fixed prediction entropy thresholds for
operational uncertainty-aware decision-making.

</details>


### [128] [Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection](https://arxiv.org/abs/2509.13586)
*Nathalie Neptune,Josiane Mothe*

Main category: cs.CV

TL;DR: 本文提出利用地球观测卫星图像对检测亚马逊雨林砍伐的方法，通过深度学习对比不同时期图像，还提出视觉语义模型标注变化，评估显示方法有效且可用于其他领域。


<details>
  <summary>Details</summary>
Motivation: 亚马逊雨林砍伐对全球碳排放和生物多样性有重大影响，需要有效方法检测。

Method: 利用深度学习技术对比不同日期同一区域卫星图像识别森林覆盖变化，提出视觉语义模型，从相关科学文档提取图像候选注释。

Result: 在亚马逊图像对数据集上评估，证明方法能有效检测砍伐并生成相关注释。

Conclusion: 该方法为监测和研究亚马逊雨林砍伐影响提供有用工具，且具有通用性可应用于其他领域。

Abstract: The Amazon rain forest is a vital ecosystem that plays a crucial role in
regulating the Earth's climate and providing habitat for countless species.
Deforestation in the Amazon is a major concern as it has a significant impact
on global carbon emissions and biodiversity. In this paper, we present a method
for detecting deforestation in the Amazon using image pairs from Earth
observation satellites. Our method leverages deep learning techniques to
compare the images of the same area at different dates and identify changes in
the forest cover. We also propose a visual semantic model that automatically
annotates the detected changes with relevant keywords. The candidate annotation
for images are extracted from scientific documents related to the Amazon
region. We evaluate our approach on a dataset of Amazon image pairs and
demonstrate its effectiveness in detecting deforestation and generating
relevant annotations. Our method provides a useful tool for monitoring and
studying the impact of deforestation in the Amazon. While we focus on
environment applications of our work by using images of deforestation in the
Amazon rain forest to demonstrate the effectiveness of our proposed approach,
it is generic enough to be applied to other domains.

</details>


### [129] [Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery](https://arxiv.org/abs/2509.13631)
*Yuvraj Dutta,Aaditya Sikder,Basabdatta Palit*

Main category: cs.CV

TL;DR: 本文提出用联邦学习（FL）进行分布式方法来识别和定位不同客户端的森林砍伐，利用FLOWER和RAY框架，测试多个模型，为卫星图像分割任务提供新视角。


<details>
  <summary>Details</summary>
Motivation: 为准确从卫星图像识别森林砍伐以了解区域地理情况，且解决集中训练方法存在的数据安全问题。

Method: 采用联邦学习，对应边缘卫星中心为客户端进行本地数据处理，利用FLOWER和RAY框架执行分布式学习工作负载，使用YOLOS - small、Faster R - CNN等模型在公开数据集上训练和测试。

Result: 文中未明确提及具体结果。

Conclusion: 该方法为基于图像分割的卫星图像任务提供了不同视角。

Abstract: Accurate identification of deforestation from satellite images is essential
in order to understand the geographical situation of an area. This paper
introduces a new distributed approach to identify as well as locate
deforestation across different clients using Federated Learning (FL). Federated
Learning enables distributed network clients to collaboratively train a model
while maintaining data privacy and security of the active users. In our
framework, a client corresponds to an edge satellite center responsible for
local data processing. Moreover, FL provides an advantage over centralized
training method which requires combining data, thereby compromising with data
security of the clients. Our framework leverages the FLOWER framework with RAY
framework to execute the distributed learning workload. Furthermore, efficient
client spawning is ensured by RAY as it can select definite amount of users to
create an emulation environment. Our FL framework uses YOLOS-small (a Vision
Transformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN
with a MobileNetV3 backbone models trained and tested on publicly available
datasets. Our approach provides us a different view for image
segmentation-based tasks on satellite imagery.

</details>


### [130] [Hybrid Quantum-Classical Model for Image Classification](https://arxiv.org/abs/2509.13353)
*Muhammad Adnan Shahzad*

Main category: cs.CV

TL;DR: 本文对混合量子 - 经典神经网络和纯经典模型在三个基准数据集上进行系统比较，发现混合模型在准确性、训练效率等方面有优势。


<details>
  <summary>Details</summary>
Motivation: 评估混合量子 - 经典神经网络和纯经典模型在性能、效率和鲁棒性方面的差异。

Method: 在MNIST、CIFAR100和STL10三个数据集上进行50个训练周期的实验，评估验证准确率、测试准确率、训练时间等指标。

Result: 混合模型在最终准确率上始终优于经典模型，训练速度快5 - 12倍，使用参数少6 - 32%，在简单数据集上对抗鲁棒性更强，且资源消耗更少。

Conclusion: 混合量子 - 经典架构在准确性、训练效率和参数可扩展性方面具有显著优势，尤其适用于复杂视觉任务。

Abstract: This study presents a systematic comparison between hybrid quantum-classical
neural networks and purely classical models across three benchmark datasets
(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and
robustness. The hybrid models integrate parameterized quantum circuits with
classical deep learning architectures, while the classical counterparts use
conventional convolutional neural networks (CNNs). Experiments were conducted
over 50 training epochs for each dataset, with evaluations on validation
accuracy, test accuracy, training time, computational resource usage, and
adversarial robustness (tested with $\epsilon=0.1$ perturbations).Key findings
demonstrate that hybrid models consistently outperform classical models in
final accuracy, achieving {99.38\% (MNIST), 41.69\% (CIFAR100), and 74.05\%
(STL10) validation accuracy, compared to classical benchmarks of 98.21\%,
32.25\%, and 63.76\%, respectively. Notably, the hybrid advantage scales with
dataset complexity, showing the most significant gains on CIFAR100 (+9.44\%)
and STL10 (+10.29\%). Hybrid models also train 5--12$\times$ faster (e.g.,
21.23s vs. 108.44s per epoch on MNIST) and use 6--32\% fewer parameters} while
maintaining superior generalization to unseen test data.Adversarial robustness
tests reveal that hybrid models are significantly more resilient on simpler
datasets (e.g., 45.27\% robust accuracy on MNIST vs. 10.80\% for classical) but
show comparable fragility on complex datasets like CIFAR100 ($\sim$1\%
robustness for both). Resource efficiency analyses indicate that hybrid models
consume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization
(9.5\% vs. 23.2\% on average).These results suggest that hybrid
quantum-classical architectures offer compelling advantages in accuracy,
training efficiency, and parameter scalability, particularly for complex vision
tasks.

</details>


### [131] [An Empirical Analysis of VLM-based OOD Detection: Mechanisms, Advantages, and Sensitivity](https://arxiv.org/abs/2509.13375)
*Yuxiao Lee,Xiaofeng Cao,Wei Ye,Jiangchao Yao,Jingkuan Song,Heng Tao Shen*

Main category: cs.CV

TL;DR: 本文对基于视觉语言模型（VLM）的零样本分布外（OOD）检测进行系统实证分析，揭示其机制、优势与敏感性。


<details>
  <summary>Details</summary>
Motivation: 当前研究对VLM在OOD检测中为何有效、与单模态方法相比的优势以及行为鲁棒性缺乏全面理解。

Method: 使用分布内（ID）和分布外（OOD）提示对基于VLM的OOD检测进行系统实证分析。

Result: 明确VLM嵌入空间促进零样本OOD检测的关键特性，量化其相对单模态方法的优势，发现其对图像噪声有一定抗性但对提示措辞敏感。

Conclusion: 研究为基于VLM的OOD检测的优势和关键弱点提供了更结构化的理解，为未来设计更鲁棒可靠的系统提供了基于实证的指导。

Abstract: Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable
zero-shot out-of-distribution (OOD) detection capabilities, vital for reliable
AI systems. Despite this promising capability, a comprehensive understanding of
(1) why they work so effectively, (2) what advantages do they have over
single-modal methods, and (3) how is their behavioral robustness -- remains
notably incomplete within the research community. This paper presents a
systematic empirical analysis of VLM-based OOD detection using in-distribution
(ID) and OOD prompts. (1) Mechanisms: We systematically characterize and
formalize key operational properties within the VLM embedding space that
facilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the
superiority of these models over established single-modal approaches,
attributing this distinct advantage to the VLM's capacity to leverage rich
semantic novelty. (3) Sensitivity: We uncovers a significant and previously
under-explored asymmetry in their robustness profile: while exhibiting
resilience to common image noise, these VLM-based methods are highly sensitive
to prompt phrasing. Our findings contribute a more structured understanding of
the strengths and critical vulnerabilities inherent in VLM-based OOD detection,
offering crucial, empirically-grounded guidance for developing more robust and
reliable future designs.

</details>


### [132] [Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji](https://arxiv.org/abs/2509.13388)
*Yadvendra Gurjar,Ruoni Wan,Ehsan Farahbakhsh,Rohitash Chandra*

Main category: cs.CV

TL;DR: 本文运用机器学习和遥感框架对比斐济楠迪2013 - 2024年土地利用与覆盖变化，提供技术支持并可视化变化。


<details>
  <summary>Details</summary>
Motivation: 斐济作为发展中国家面临快速城市化，该研究旨在为土地覆盖/土地利用建模和变化检测提供技术支持。

Method: 使用Landsat - 8卫星图像创建监督学习训练数据集；利用Google Earth Engine和k - means聚类进行无监督学习生成土地覆盖图；用卷积神经网络对选定区域土地覆盖类型分类。

Result: 呈现了变化检测的可视化结果，突出城市区域随时间的变化。

Conclusion: 该机器学习和遥感框架可用于监测土地利用和土地覆盖变化，为相关建模和检测提供了支持。

Abstract: As a developing country, Fiji is facing rapid urbanisation, which is visible
in the massive development projects that include housing, roads, and civil
works. In this study, we present machine learning and remote sensing frameworks
to compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The
ultimate goal of this study is to provide technical support in land cover/land
use modelling and change detection. We used Landsat-8 satellite image for the
study region and created our training dataset with labels for supervised
machine learning. We used Google Earth Engine and unsupervised machine learning
via k-means clustering to generate the land cover map. We used convolutional
neural networks to classify the selected regions' land cover types. We present
a visualisation of change detection, highlighting urban area changes over time
to monitor changes in the map.

</details>


### [133] [EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing](https://arxiv.org/abs/2509.13399)
*Tianyu Chen,Yasi Zhang,Zhi Zhang,Peiyu Yu,Shu Wang,Zhendong Wang,Kevin Lin,Xiaofei Wang,Zhengyuan Yang,Linjie Li,Chung-Ching Lin,Jianwen Xie,Oscar Leong,Lijuan Wang,Ying Nian Wu,Mingyuan Zhou*

Main category: cs.CV

TL;DR: 现有基于指令的图像编辑评估方法有局限，本文提出EdiVal - Agent框架及EdiVal - Bench基准，可识别现有模型失效模式，利于下一代模型开发。


<details>
  <summary>Details</summary>
Motivation: 当前基于指令的图像编辑评估方法存在覆盖范围有限、评估不精确等问题，需要可靠且可解释的评估方法。

Method: 引入EdiVal - Agent框架，从对象中心视角评估，分解图像成对象、合成指令；集成VLMs与对象检测器评估指令遵循度，用特征提取器评估内容一致性，用人类偏好模型评估视觉质量。构建EdiVal - Bench基准。

Result: 结合VLMs与对象检测器在指令遵循度评估上比单独使用VLMs和基于CLIP的指标更符合人类判断，框架模块化设计便于集成新工具。

Conclusion: EdiVal - Agent可用于识别现有模型失效模式，有助于下一代编辑模型的开发。

Abstract: Instruction-based image editing has advanced rapidly, yet reliable and
interpretable evaluation remains a bottleneck. Current protocols either (i)
depend on paired reference images -- resulting in limited coverage and
inheriting biases from prior generative models -- or (ii) rely solely on
zero-shot vision-language models (VLMs), whose prompt-based assessments of
instruction following, content consistency, and visual quality are often
imprecise.
  To address this, we introduce EdiVal-Agent, an automated, scalable, and
fine-grained evaluation framework for multi-turn instruction-based editing from
an object-centric perspective, supported by a suite of expert tools. Given an
image, EdiVal-Agent first decomposes it into semantically meaningful objects,
then synthesizes diverse, context-aware editing instructions. For evaluation,
it integrates VLMs with open-vocabulary object detectors to assess instruction
following, uses semantic-level feature extractors to evaluate content
consistency, and leverages human preference models to judge visual quality. We
show that combining VLMs with object detectors yields stronger agreement with
human judgments in instruction-following evaluation compared to using VLMs
alone and CLIP-based metrics. Furthermore, the pipeline's modular design allows
future tools to be seamlessly integrated, enhancing evaluation accuracy over
time.
  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing
benchmark covering 9 instruction types and 11 state-of-the-art editing models
spanning autoregressive (AR) (including Nano Banana, GPT-Image-1),
flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be
used to identify existing failure modes, thereby informing the development of
the next generation of editing models. Project page:
https://tianyucodings.github.io/EdiVAL-page/.

</details>


### [134] [MapAnything: Universal Feed-Forward Metric 3D Reconstruction](https://arxiv.org/abs/2509.13414)
*Nikhil Keetha,Norman Müller,Johannes Schönberger,Lorenzo Porzi,Yuchen Zhang,Tobias Fischer,Arno Knapitsch,Duncan Zauss,Ethan Weber,Nelson Antunes,Jonathon Luiten,Manuel Lopez-Antequera,Samuel Rota Bulò,Christian Richardt,Deva Ramanan,Sebastian Scherer,Peter Kontschieder*

Main category: cs.CV

TL;DR: 介绍基于Transformer的统一前馈模型MapAnything，可处理多种输入并回归3D场景几何与相机参数，能解决多种3D视觉任务，实验表现优。


<details>
  <summary>Details</summary>
Motivation: 构建能处理多种输入、解决多种3D视觉任务的统一模型。

Method: 引入MapAnything模型，利用多视图场景几何的分解表示，统一不同数据集的监督和训练，进行灵活输入增强。

Result: MapAnything在实验中表现优于或等同于专业前馈模型，且联合训练效率更高。

Conclusion: MapAnything为通用3D重建骨干网络奠定基础。

Abstract: We introduce MapAnything, a unified transformer-based feed-forward model that
ingests one or more images along with optional geometric inputs such as camera
intrinsics, poses, depth, or partial reconstructions, and then directly
regresses the metric 3D scene geometry and cameras. MapAnything leverages a
factored representation of multi-view scene geometry, i.e., a collection of
depth maps, local ray maps, camera poses, and a metric scale factor that
effectively upgrades local reconstructions into a globally consistent metric
frame. Standardizing the supervision and training across diverse datasets,
along with flexible input augmentation, enables MapAnything to address a broad
range of 3D vision tasks in a single feed-forward pass, including uncalibrated
structure-from-motion, calibrated multi-view stereo, monocular depth
estimation, camera localization, depth completion, and more. We provide
extensive experimental analyses and model ablations demonstrating that
MapAnything outperforms or matches specialist feed-forward models while
offering more efficient joint training behavior, thus paving the way toward a
universal 3D reconstruction backbone.

</details>


### [135] [ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors](https://arxiv.org/abs/2509.13525)
*Romain Hardy,Tyler Berzin,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: 提出基于扩散的深度估计模型ColonCrafter，可从单目结肠镜视频生成时间一致的深度图，在C3VD数据集上有零样本最优表现，并展示临床应用。


<details>
  <summary>Details</summary>
Motivation: 现有内窥镜深度估计模型在视频序列时间一致性上存在问题，限制3D重建应用，需自动化方法进行准确深度估计。

Method: 从合成结肠镜序列学习鲁棒几何先验生成深度图，引入风格迁移技术使真实临床视频适配合成训练域。

Result: ColonCrafter在C3VD数据集上达到零样本最优性能，优于通用和内窥镜特定方法。

Conclusion: 虽全轨迹3D重建仍有挑战，但ColonCrafter有临床相关应用，如3D点云生成和表面覆盖评估。

Abstract: Three-dimensional (3D) scene understanding in colonoscopy presents
significant challenges that necessitate automated methods for accurate depth
estimation. However, existing depth estimation models for endoscopy struggle
with temporal consistency across video sequences, limiting their applicability
for 3D reconstruction. We present ColonCrafter, a diffusion-based depth
estimation model that generates temporally consistent depth maps from monocular
colonoscopy videos. Our approach learns robust geometric priors from synthetic
colonoscopy sequences to generate temporally consistent depth maps. We also
introduce a style transfer technique that preserves geometric structure while
adapting real clinical videos to match our synthetic training domain.
ColonCrafter achieves state-of-the-art zero-shot performance on the C3VD
dataset, outperforming both general-purpose and endoscopy-specific approaches.
Although full trajectory 3D reconstruction remains a challenge, we demonstrate
clinically relevant applications of ColonCrafter, including 3D point cloud
generation and surface coverage assessment.

</details>


### [136] [Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation](https://arxiv.org/abs/2509.13590)
*Samer Al-Hamadani*

Main category: cs.CV

TL;DR: 本文提出基于VLMs的医疗图像分析智能多模态框架，结合多种技术实现肿瘤检测和报告生成，实验表现良好，有零样本学习能力，但需临床验证。


<details>
  <summary>Details</summary>
Motivation: 利用人工智能在医疗影像领域的发展，改进医疗诊断和临床决策过程。

Method: 集成Google Gemini 2.5 Flash，结合视觉特征提取和自然语言处理，采用坐标验证、高斯建模等技术，运用多层可视化，结果处理使用精确提示工程和文本分析。

Result: 在多模态异常检测中表现出高性能，定位测量平均偏差80像素，具备零样本学习能力，有用户友好界面。

Conclusion: 该框架是自动诊断支持和放射工作流程效率的重大进步，但需临床验证和多中心评估才能广泛应用。

Abstract: The rapid advancement of artificial intelligence (AI) in healthcare imaging
has revolutionized diagnostic medicine and clinical decision-making processes.
This work presents an intelligent multimodal framework for medical image
analysis that leverages Vision-Language Models (VLMs) in healthcare
diagnostics. The framework integrates Google Gemini 2.5 Flash for automated
tumor detection and clinical report generation across multiple imaging
modalities including CT, MRI, X-ray, and Ultrasound. The system combines visual
feature extraction with natural language processing to enable contextual image
interpretation, incorporating coordinate verification mechanisms and
probabilistic Gaussian modeling for anomaly distribution. Multi-layered
visualization techniques generate detailed medical illustrations, overlay
comparisons, and statistical representations to enhance clinical confidence,
with location measurement achieving 80 pixels average deviation. Result
processing utilizes precise prompt engineering and textual analysis to extract
structured clinical information while maintaining interpretability.
Experimental evaluations demonstrated high performance in anomaly detection
across multiple modalities. The system features a user-friendly Gradio
interface for clinical workflow integration and demonstrates zero-shot learning
capabilities to reduce dependence on large datasets. This framework represents
a significant advancement in automated diagnostic support and radiological
workflow efficiency, though clinical validation and multi-center evaluation are
necessary prior to widespread adoption.

</details>


### [137] [Deep Lookup Network](https://arxiv.org/abs/2509.13662)
*Yulan Guo,Longguang Wang,Wendong Mao,Xiaoyu Dong,Yingqian Wang,Li Liu,Wei An*

Main category: cs.CV

TL;DR: 本文引入通用高效的查找操作构建神经网络，替代乘法运算，开发查找网络用于多任务，实验表明该网络能耗低、推理快且性能优。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络中乘法运算复杂度高、能耗大、推理时间长，阻碍其在移动设备部署，而资源受限的边缘设备可用查找表降低计算成本。

Method: 引入通用高效查找操作替代乘法运算，以可微方式构建查找表并提出训练策略促进收敛，开发查找网络用于图像分类、超分辨率和点云分类任务。

Result: 查找网络在能耗和推理速度上更高效，同时在不同任务和数据类型上取得了与传统卷积网络相当的性能。

Conclusion: 查找网络在不同任务和数据类型上都能产生先进的性能。

Abstract: Convolutional neural networks are constructed with massive operations with
different types and are highly computationally intensive. Among these
operations, multiplication operation is higher in computational complexity and
usually requires {more} energy consumption with longer inference time than
other operations, which hinders the deployment of convolutional neural networks
on mobile devices. In many resource-limited edge devices, complicated
operations can be calculated via lookup tables to reduce computational cost.
Motivated by this, in this paper, we introduce a generic and efficient lookup
operation which can be used as a basic operation for the construction of neural
networks. Instead of calculating the multiplication of weights and activation
values, simple yet efficient lookup operations are adopted to compute their
responses. To enable end-to-end optimization of the lookup operation, we
construct the lookup tables in a differentiable manner and propose several
training strategies to promote their convergence. By replacing computationally
expensive multiplication operations with our lookup operations, we develop
lookup networks for the image classification, image super-resolution, and point
cloud classification tasks. It is demonstrated that our lookup networks can
benefit from the lookup operations to achieve higher efficiency in terms of
energy consumption and inference speed while maintaining competitive
performance to vanilla convolutional networks. Extensive experiments show that
our lookup networks produce state-of-the-art performance on different tasks
(both classification and regression tasks) and different data types (both
images and point clouds).

</details>


### [138] [Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation](https://arxiv.org/abs/2509.13676)
*Xiaobo Yang,Xiaojin Gong*

Main category: cs.CV

TL;DR: 提出语义视觉投影器，压缩视觉标记加速MLLM，在RIS上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有将MLLM适配到分割任务计算量大，传统投影器难平衡减少视觉标记和保留语义。

Method: 提出语义视觉投影器，利用SAM生成的语义超像素识别视觉词并压缩投影；提出语义超像素位置嵌入和聚合器减少信息损失。

Result: 减少93%视觉标记，加速MLLM训练和推理，在RIS上优于现有方法。

Conclusion: 所提方法有效减少视觉标记且不损失性能，能加速MLLM，在RIS任务表现更好。

Abstract: Recently, Referring Image Segmentation (RIS) frameworks that pair the
Multimodal Large Language Model (MLLM) with the Segment Anything Model (SAM)
have achieved impressive results. However, adapting MLLM to segmentation is
computationally intensive, primarily due to visual token redundancy. We observe
that traditional patch-wise visual projectors struggle to strike a balance
between reducing the number of visual tokens and preserving semantic clarity,
often retaining overly long token sequences to avoid performance drops.
Inspired by text tokenizers, we propose a novel semantic visual projector that
leverages semantic superpixels generated by SAM to identify "visual words" in
an image. By compressing and projecting semantic superpixels as visual tokens,
our approach adaptively shortens the token sequence according to scene
complexity while minimizing semantic loss in compression. To mitigate loss of
information, we propose a semantic superpixel positional embedding to
strengthen MLLM's awareness of superpixel geometry and position, alongside a
semantic superpixel aggregator to preserve both fine-grained details inside
superpixels and global context outside. Experiments show that our method cuts
visual tokens by 93% without compromising performance, notably speeding up MLLM
training and inference, and outperforming existing compressive visual
projectors on RIS.

</details>


### [139] [Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension](https://arxiv.org/abs/2509.13385)
*Charlotte Beylier,Parvaneh Joharinad,Jürgen Jost,Nahid Torbati*

Main category: cs.CV

TL;DR: 利用截面曲率概念构建离散度量空间的几何轮廓，引入评估数据表示有效性的方法，实验证明可估计数据集内在维度并评估降维技术。


<details>
  <summary>Details</summary>
Motivation: 引入基于曲率的方法来评估数据表示的有效性以及估计数据集的内在维度。

Method: 利用新开发的截面曲率抽象概念构建离散度量空间的曲率几何轮廓。

Result: 实验表明基于曲率的分析可用于估计数据集的内在维度，可探索经验网络的大规模几何结构并评估降维技术的有效性。

Conclusion: 基于曲率的几何轮廓构建方法能有效评估数据表示，可用于估计内在维度和评估降维技术。

Abstract: Utilizing recently developed abstract notions of sectional curvature, we
introduce a method for constructing a curvature-based geometric profile of
discrete metric spaces. The curvature concept that we use here captures the
metric relations between triples of points and other points. More
significantly, based on this curvature profile, we introduce a quantitative
measure to evaluate the effectiveness of data representations, such as those
produced by dimensionality reduction techniques. Furthermore, Our experiments
demonstrate that this curvature-based analysis can be employed to estimate the
intrinsic dimensionality of datasets. We use this to explore the large-scale
geometry of empirical networks and to evaluate the effectiveness of
dimensionality reduction techniques.

</details>


### [140] [Mitigating Query Selection Bias in Referring Video Object Segmentation](https://arxiv.org/abs/2509.13722)
*Dingwei Zhang,Dong Zhang,Jinhui Tang*

Main category: cs.CV

TL;DR: 提出Triple Query Former (TQF)解决RVOS中静态查询易受干扰问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的RVOS方法中静态查询易受干扰，产生查询选择偏差。

Method: 将查询分解为外观、帧内交互和帧间运动三个组件，动态构建查询；引入帧内交互聚合和帧间运动聚合两个模块增强对象表示。

Result: 在多个RVOS基准测试上的实验表明TQF具有优势，结构化查询设计和运动感知聚合模块有效。

Conclusion: TQF能有效解决静态查询易受干扰问题，提出的组件和模块有积极效果。

Abstract: Recently, query-based methods have achieved remarkable performance in
Referring Video Object Segmentation (RVOS) by using textual static object
queries to drive cross-modal alignment. However, these static queries are
easily misled by distractors with similar appearance or motion, resulting in
\emph{query selection bias}. To address this issue, we propose Triple Query
Former (TQF), which factorizes the referring query into three specialized
components: an appearance query for static attributes, an intra-frame
interaction query for spatial relations, and an inter-frame motion query for
temporal association. Instead of relying solely on textual embeddings, our
queries are dynamically constructed by integrating both linguistic cues and
visual guidance. Furthermore, we introduce two motion-aware aggregation modules
that enhance object token representations: Intra-frame Interaction Aggregation
incorporates position-aware interactions among objects within a single frame,
while Inter-frame Motion Aggregation leverages trajectory-guided alignment
across frames to ensure temporal coherence. Extensive experiments on multiple
RVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our
structured query design and motion-aware aggregation modules.

</details>


### [141] [BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching](https://arxiv.org/abs/2509.13789)
*Hanshuai Cui,Zhiqing Tang,Zhifei Xu,Zhi Yao,Wenyi Zeng,Weijia Jia*

Main category: cs.CV

TL;DR: 现有Diffusion Transformers（DiTs）视频生成方法有延迟问题，本文提出无训练的BWCache方法加速，实验显示可实现2.24倍加速且视觉质量相当。


<details>
  <summary>Details</summary>
Motivation: DiTs视频生成方法存在顺序去噪过程导致延迟，现有加速方法有质量下降或特征复用不佳的问题。

Method: 提出Block - Wise Caching（BWCache）方法，动态缓存和复用DiT块特征，引入相似度指标，在相邻时间步块特征差异低于阈值时触发特征复用。

Result: 在多个视频扩散模型上实验，BWCache实现了高达2.24倍的加速，视觉质量相当。

Conclusion: BWCache能有效加速DiT-based视频生成，减少冗余计算同时保持视觉保真度。

Abstract: Recent advancements in Diffusion Transformers (DiTs) have established them as
the state-of-the-art method for video generation. However, their inherently
sequential denoising process results in inevitable latency, limiting real-world
applicability. Existing acceleration methods either compromise visual quality
due to architectural modifications or fail to reuse intermediate features at
proper granularity. Our analysis reveals that DiT blocks are the primary
contributors to inference latency. Across diffusion timesteps, the feature
variations of DiT blocks exhibit a U-shaped pattern with high similarity during
intermediate timesteps, which suggests substantial computational redundancy. In
this paper, we propose Block-Wise Caching (BWCache), a training-free method to
accelerate DiT-based video generation. BWCache dynamically caches and reuses
features from DiT blocks across diffusion timesteps. Furthermore, we introduce
a similarity indicator that triggers feature reuse only when the differences
between block features at adjacent timesteps fall below a threshold, thereby
minimizing redundant computations while maintaining visual fidelity. Extensive
experiments on several video diffusion models demonstrate that BWCache achieves
up to 2.24$\times$ speedup with comparable visual quality.

</details>


### [142] [BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation](https://arxiv.org/abs/2509.13496)
*Rajatsubhra Chakraborty,Xujun Che,Depeng Xu,Cori Faklaris,Xi Niu,Shuhan Yuan*

Main category: cs.CV

TL;DR: 提出BiasMap框架，用于发现并缓解稳定扩散模型中潜在概念层面的表征偏差。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注输出层面的人口统计分布，不能保证缓解后概念表征解耦，需要深入挖掘概念层面的表征偏差。

Method: 利用交叉注意力归因图揭示人口统计与语义的结构纠缠，通过IoU量化概念纠缠，利用能量引导的扩散采样缓解偏差。

Result: 现有公平干预措施可能减少输出分布差距，但无法解耦概念层面的耦合，而提出的缓解方法可缓解图像生成中的概念纠缠。

Conclusion: BiasMap框架能有效发现并缓解稳定扩散模型中概念层面的表征偏差，补充了分布偏差缓解。

Abstract: Bias discovery is critical for black-box generative models, especiall
text-to-image (TTI) models. Existing works predominantly focus on output-level
demographic distributions, which do not necessarily guarantee concept
representations to be disentangled post-mitigation. We propose BiasMap, a
model-agnostic framework for uncovering latent concept-level representational
biases in stable diffusion models. BiasMap leverages cross-attention
attribution maps to reveal structural entanglements between demographics (e.g.,
gender, race) and semantics (e.g., professions), going deeper into
representational bias during the image generation. Using attribution maps of
these concepts, we quantify the spatial demographics-semantics concept
entanglement via Intersection over Union (IoU), offering a lens into bias that
remains hidden in existing fairness discovery approaches. In addition, we
further utilize BiasMap for bias mitigation through energy-guided diffusion
sampling that directly modifies latent noise space and minimizes the expected
SoftIoU during the denoising process. Our findings show that existing fairness
interventions may reduce the output distributional gap but often fail to
disentangle concept-level coupling, whereas our mitigation method can mitigate
concept entanglement in image generation while complementing distributional
bias mitigation.

</details>


### [143] [Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation](https://arxiv.org/abs/2509.13792)
*Inder Pal Singh,Nidhal Eddine Chenni,Abd El Rahman Shabayek,Arunkumar Rathinam,Djamila Aouada*

Main category: cs.CV

TL;DR: 提出针对航天器姿态估计（SPE）关键点回归的监督域适应（SDA）框架，在SPEED+基准测试中表现出色，轻量级且高效。


<details>
  <summary>Details</summary>
Motivation: 现有混合管道在真实图像上因合成到真实的域差距性能下降，无监督域适应方法在有少量标记目标样本时表现不佳，需新方法解决问题。

Method: 基于学习不变表示和风险（LIRR）范式，联合优化域不变表示和任务特定风险，使用标记的合成数据和有限的标记真实数据。

Result: 在SPEED+基准测试中，该方法始终优于仅源、微调、oracle基线，仅用5%标记目标数据就可媲美或超越用更多标记数据训练的oracle性能。

Conclusion: 该框架轻量级、与骨干网络无关且计算高效，为现实空间环境中实现鲁棒且可部署的航天器姿态估计提供了实用途径。

Abstract: Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous
space operations such as rendezvous, docking, and in-orbit servicing. Hybrid
pipelines that combine object detection, keypoint regression, and
Perspective-n-Point (PnP) solvers have recently achieved strong results on
synthetic datasets, yet their performance deteriorates sharply on real or
lab-generated imagery due to the persistent synthetic-to-real domain gap.
Existing unsupervised domain adaptation approaches aim to mitigate this issue
but often underperform when a modest number of labeled target samples are
available. In this work, we propose the first Supervised Domain Adaptation
(SDA) framework tailored for SPE keypoint regression. Building on the Learning
Invariant Representation and Risk (LIRR) paradigm, our method jointly optimizes
domain-invariant representations and task-specific risk using both labeled
synthetic and limited labeled real data, thereby reducing generalization error
under domain shift. Extensive experiments on the SPEED+ benchmark demonstrate
that our approach consistently outperforms source-only, fine-tuning, and oracle
baselines. Notably, with only 5% labeled target data, our method matches or
surpasses oracle performance trained on larger fractions of labeled data. The
framework is lightweight, backbone-agnostic, and computationally efficient,
offering a practical pathway toward robust and deployable spacecraft pose
estimation in real-world space environments.

</details>


### [144] [Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles](https://arxiv.org/abs/2509.13577)
*Tongfei Guo,Lili Su*

Main category: cs.CV

TL;DR: 提出新框架用于自动驾驶轨迹预测中的OOD检测，在多数据集实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶OOD检测研究多集中于计算机视觉任务，轨迹级OOD检测研究不足，且要应对复杂驾驶环境下的分布偏移问题。

Method: 提出新框架引入自适应机制，明确建模预测误差模式。

Result: 在多真实数据集上，新方法在检测延迟和误报率上有显著改善，综合实验中在准确性和计算效率上均优于现有方法。

Conclusion: 新框架为可靠的、驾驶感知的自动驾驶提供了可行途径。

Abstract: Trajectory prediction is central to the safe and seamless operation of
autonomous vehicles (AVs). In deployment, however, prediction models inevitably
face distribution shifts between training data and real-world conditions, where
rare or underrepresented traffic scenarios induce out-of-distribution (OOD)
cases. While most prior OOD detection research in AVs has concentrated on
computer vision tasks such as object detection and segmentation,
trajectory-level OOD detection remains largely underexplored. A recent study
formulated this problem as a quickest change detection (QCD) task, providing
formal guarantees on the trade-off between detection delay and false alarms
[1]. Building on this foundation, we propose a new framework that introduces
adaptive mechanisms to achieve robust detection in complex driving
environments. Empirical analysis across multiple real-world datasets reveals
that prediction errors -- even on in-distribution samples -- exhibit
mode-dependent distributions that evolve over time with dataset-specific
dynamics. By explicitly modeling these error modes, our method achieves
substantial improvements in both detection delay and false alarm rates.
Comprehensive experiments on established trajectory prediction benchmarks show
that our framework significantly outperforms prior UQ- and vision-based OOD
approaches in both accuracy and computational efficiency, offering a practical
path toward reliable, driving-aware autonomy.

</details>


### [145] [Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation](https://arxiv.org/abs/2509.13846)
*Puru Vaish,Felix Meister,Tobias Heimann,Christoph Brune,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 本文挑战表征学习中无相关视图足以学习有意义表征的假设，提出一致视图对齐的自监督学习方法，实验显示该方法提升下游任务性能，在竞赛中取得好成绩。


<details>
  <summary>Details</summary>
Motivation: 挑战表征学习中无相关视图足以学习有意义表征的假设，指出潜在空间的有意义结构需明确诱导。

Method: 提出一致视图对齐的自监督学习方法，对齐数据不同视图的表征以整合互补信息且避免误报。

Result: 该方法提升下游任务性能，使用Primus视觉变压器和ResEnc卷积神经网络时分别在MICCAI 2025 SSL3D挑战中获第一和第二名。

Conclusion: 结构化视图对齐在学习有效表征中起关键作用。

Abstract: Many recent approaches in representation learning implicitly assume that
uncorrelated views of a data point are sufficient to learn meaningful
representations for various downstream tasks. In this work, we challenge this
assumption and demonstrate that meaningful structure in the latent space does
not emerge naturally. Instead, it must be explicitly induced. We propose a
method that aligns representations from different views of the data to align
complementary information without inducing false positives. Our experiments
show that our proposed self-supervised learning method, Consistent View
Alignment, improves performance for downstream tasks, highlighting the critical
role of structured view alignment in learning effective representations. Our
method achieved first and second place in the MICCAI 2025 SSL3D challenge when
using a Primus vision transformer and ResEnc convolutional neural network,
respectively. The code and pretrained model weights are released at
https://github.com/Tenbatsu24/LatentCampus.

</details>


### [146] [SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation](https://arxiv.org/abs/2509.13848)
*Jiayi Pan,Jiaming Xu,Yongkang Zhou,Guohao Dai*

Main category: cs.CV

TL;DR: 本文从信息利用角度分析现有特征缓存方法，提出新范式并构建SpecDiff策略，实验表明其能在速度提升同时保证质量，突破速度与精度权衡瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有特征缓存方法仅依赖历史信息会导致精度和速度性能受限。

Method: 提出通过自推测引入未来信息的新范式，构建训练-free的SpecDiff策略，包括基于自推测信息的特征选择算法和基于特征重要性分数的多级特征分类算法。

Result: 在NVIDIA A800 - 80GB GPU上，与RFlow相比，SpecDiff在Stable Diffusion 3、3.5和FLUX中平均分别实现2.80×、2.74×和3.17×的加速，且质量损失可忽略不计。

Conclusion: 通过融合推测和历史信息，SpecDiff突破了速度与精度权衡的瓶颈，推动了高效扩散模型推理中速度和精度的帕累托前沿。

Abstract: Feature caching has recently emerged as a promising method for diffusion
model acceleration. It effectively alleviates the inefficiency problem caused
by high computational requirements by caching similar features in the inference
process of the diffusion model. In this paper, we analyze existing feature
caching methods from the perspective of information utilization, and point out
that relying solely on historical information will lead to constrained accuracy
and speed performance. And we propose a novel paradigm that introduces future
information via self-speculation based on the information similarity at the
same time step across different iteration times. Based on this paradigm, we
present \textit{SpecDiff}, a training-free multi-level feature caching strategy
including a cached feature selection algorithm and a multi-level feature
classification algorithm. (1) Feature selection algorithm based on
self-speculative information. \textit{SpecDiff} determines a dynamic importance
score for each token based on self-speculative information and historical
information, and performs cached feature selection through the importance
score. (2) Multi-level feature classification algorithm based on feature
importance scores. \textit{SpecDiff} classifies tokens by leveraging the
differences in feature importance scores and introduces a multi-level feature
calculation strategy. Extensive experiments show that \textit{SpecDiff}
achieves average 2.80 \times, 2.74 \times , and 3.17\times speedup with
negligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow
on NVIDIA A800-80GB GPU. By merging speculative and historical information,
\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing
the Pareto frontier of speedup and accuracy in the efficient diffusion model
inference.

</details>


### [147] [LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction](https://arxiv.org/abs/2509.13863)
*Chu Chen,Ander Biguri,Jean-Michel Morel,Raymond H. Chan,Carola-Bibiane Schönlieb,Jizhou Li*

Main category: cs.CV

TL;DR: 提出LamiGauss重建算法，结合高斯拼接辐射光栅化和探测器到世界的转换模型，能从稀疏投影中准确高效重建，实验证明优于现有技术。


<details>
  <summary>Details</summary>
Motivation: X射线计算层摄影（CL）在无损检测中重要，但从层摄影投影重建高质量体积，特别是在高稀疏视图采集条件下仍具挑战。

Method: 提出LamiGauss算法，结合高斯拼接辐射光栅化与含层摄影倾斜角的探测器到世界转换模型，利用初始化策略过滤常见层摄影伪影。

Result: 在合成和真实数据集上的实验表明，该方法有效且优于现有技术，仅用3%的全视图就比在完整数据集上优化的迭代方法性能更优。

Conclusion: LamiGauss算法能有效解决从稀疏层摄影投影重建高质量体积的问题，具有显著优势。

Abstract: X-ray Computed Laminography (CL) is essential for non-destructive inspection
of plate-like structures in applications such as microchips and composite
battery materials, where traditional computed tomography (CT) struggles due to
geometric constraints. However, reconstructing high-quality volumes from
laminographic projections remains challenging, particularly under highly
sparse-view acquisition conditions. In this paper, we propose a reconstruction
algorithm, namely LamiGauss, that combines Gaussian Splatting radiative
rasterization with a dedicated detector-to-world transformation model
incorporating the laminographic tilt angle. LamiGauss leverages an
initialization strategy that explicitly filters out common laminographic
artifacts from the preliminary reconstruction, preventing redundant Gaussians
from being allocated to false structures and thereby concentrating model
capacity on representing the genuine object. Our approach effectively optimizes
directly from sparse projections, enabling accurate and efficient
reconstruction with limited data. Extensive experiments on both synthetic and
real datasets demonstrate the effectiveness and superiority of the proposed
method over existing techniques. LamiGauss uses only 3$\%$ of full views to
achieve superior performance over the iterative method optimized on a full
dataset.

</details>


### [148] [MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment](https://arxiv.org/abs/2509.14001)
*Elena Camuffo,Francesco Barbato,Mete Ozay,Simone Milani,Umberto Michieli*

Main category: cs.CV

TL;DR: 介绍MOCHA知识蒸馏方法，将大视觉语言模型语义转移到轻量级目标检测器，在少样本检测基准上验证有效。


<details>
  <summary>Details</summary>
Motivation: 将大视觉语言模型的区域级多模态语义转移到轻量级视觉目标检测器，且避免修改教师模型和推理时文本输入。

Method: 引入MOCHA方法，用翻译模块将学生特征映射到联合空间，通过双目标损失指导学生和翻译器训练。

Result: 在四个少样本个性化检测基准上验证，平均得分比基线提高10.1，性能与更大的多模态模型相当。

Conclusion: MOCHA架构紧凑，适合实际部署。

Abstract: We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),
a knowledge distillation approach that transfers region-level multimodal
semantics from a large vision-language teacher (e.g., LLaVa) into a lightweight
vision-only object detector student (e.g., YOLO). A translation module maps
student features into a joint space, where the training of the student and
translator is guided by a dual-objective loss that enforces both local
alignment and global relational consistency. Unlike prior approaches focused on
dense or global alignment, MOCHA operates at the object level, enabling
efficient transfer of semantics without modifying the teacher or requiring
textual input at inference. We validate our method across four personalized
detection benchmarks under few-shot regimes. Results show consistent gains over
baselines, with a +10.1 average score improvement. Despite its compact
architecture, MOCHA reaches performance on par with larger multimodal models,
proving its suitability for real-world deployment.

</details>


### [149] [Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions](https://arxiv.org/abs/2509.14165)
*Michal Szczepanski,Martyna Poreba,Karim Haroun*

Main category: cs.CV

TL;DR: 提出STEP框架提升视觉Transformer语义分割效率，在保证精度下降低计算成本和提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer在语义分割中计算和内存成本高，需提升效率。

Method: 提出STEP混合标记减少框架，结合动态块合并和标记修剪，核心是dCTS策略网络，编码器块集成提前退出机制。

Result: dCTS单独应用时标记数量降低2.5倍、计算成本降低2.6倍、吞吐量提升3.4倍；完整STEP框架计算复杂度降低4倍、推理速度提升1.7倍，精度下降不超2.0%，最多40%标记可提前停止。

Conclusion: STEP框架能有效提升视觉Transformer语义分割效率。

Abstract: Vision Transformers (ViTs) achieve state-of-the-art performance in semantic
segmentation but are hindered by high computational and memory costs. To
address this, we propose STEP (SuperToken and Early-Pruning), a hybrid
token-reduction framework that combines dynamic patch merging and token pruning
to enhance efficiency without significantly compromising accuracy. At the core
of STEP is dCTS, a lightweight CNN-based policy network that enables flexible
merging into superpatches. Encoder blocks integrate also early-exits to remove
high-confident supertokens, lowering computational load. We evaluate our method
on high-resolution semantic segmentation benchmarks, including images up to
1024 x 1024, and show that when dCTS is applied alone, the token count can be
reduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching
scheme. This yields a 2.6x reduction in computational cost and a 3.4x increase
in throughput when using ViT-Large as the backbone. Applying the full STEP
framework further improves efficiency, reaching up to a 4x reduction in
computational complexity and a 1.7x gain in inference speed, with a maximum
accuracy drop of no more than 2.0%. With the proposed STEP configurations, up
to 40% of tokens can be confidently predicted and halted before reaching the
final encoder layer.

</details>


### [150] [Dense Video Understanding with Gated Residual Tokenization](https://arxiv.org/abs/2509.14199)
*Haichao Zhang,Wenhao Chai,Shwai He,Ang Li,Yun Fu*

Main category: cs.CV

TL;DR: 提出Dense Video Understanding (DVU)和DIVE基准，采用Gated Residual Tokenization (GRT)框架实现高效高帧率视频理解，实验表明GRT表现优且可扩展。


<details>
  <summary>Details</summary>
Motivation: 当前VLLMs和基准多采用低帧率采样，丢弃密集时间信息，现有基准QA对关注粗粒度内容变化，无法满足如讲座理解等任务需求。

Method: 引入DVU减少分词时间和令牌开销；提出DIVE基准用于密集时间推理；提出GRT两阶段框架，包括运动补偿门控分词和语义场景内令牌合并。

Result: 在DIVE上实验显示GRT优于更大的VLLM基线，且随FPS提升表现更好。

Conclusion: 强调了密集时间信息的重要性，证明GRT能实现高效、可扩展的高帧率视频理解。

Abstract: High temporal resolution is essential for capturing fine-grained details in
video understanding. However, current video large language models (VLLMs) and
benchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or
keyframe selection, discarding dense temporal information. This compromise
avoids the high cost of tokenizing every frame, which otherwise leads to
redundant computation and linear token growth as video length increases. While
this trade-off works for slowly changing content, it fails for tasks like
lecture comprehension, where information appears in nearly every frame and
requires precise temporal alignment. To address this gap, we introduce Dense
Video Understanding (DVU), which enables high-FPS video comprehension by
reducing both tokenization time and token overhead. Existing benchmarks are
also limited, as their QA pairs focus on coarse content changes. We therefore
propose DIVE (Dense Information Video Evaluation), the first benchmark designed
for dense temporal reasoning. To make DVU practical, we present Gated Residual
Tokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated
Tokenization uses pixel-level motion estimation to skip static regions during
tokenization, achieving sub-linear growth in token count and compute. (2)
Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions
within a scene, further reducing redundancy while preserving dynamic semantics.
Experiments on DIVE show that GRT outperforms larger VLLM baselines and scales
positively with FPS. These results highlight the importance of dense temporal
information and demonstrate that GRT enables efficient, scalable high-FPS video
understanding.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [151] [Valuation of Exotic Options and Counterparty Games Based on Conditional Diffusion](https://arxiv.org/abs/2509.13374)
*Helin Zhao,Junchi Shen*

Main category: q-fin.PR

TL;DR: 本文引入DDPM模型定价奇异期权和结构化产品，在静态验证和动态博弈中有较好表现，但对极端事件敏感产品定价有局限，扩散模型有提升定价准确性潜力。


<details>
  <summary>Details</summary>
Motivation: 传统模型无法捕捉现实市场现象，难以对奇异期权和结构化产品定价。

Method: 引入Diffusion - Conditional Probability Model (DDPM)生成价格路径，采用含金融特定特征的复合损失函数，提出P - Q动态博弈框架进行对抗回测。

Result: 静态验证中P模型能匹配市场均值和波动率；动态博弈中，对欧式和亚式期权的盈利能力显著高于传统蒙特卡罗模型；对雪球和累积期权等对极端事件敏感产品定价有局限，低估尾部风险。

Conclusion: 扩散模型有提升定价准确性的潜力，但需进一步研究以提高对极端市场风险的建模能力。

Abstract: This paper addresses the challenges of pricing exotic options and structured
products, which traditional models often fail to handle due to their inability
to capture real-world market phenomena like fat-tailed distributions and
volatility clustering. We introduce a Diffusion-Conditional Probability Model
(DDPM) to generate more realistic price paths. Our method incorporates a
composite loss function with financial-specific features, and we propose a P-Q
dynamic game framework for evaluating the model's economic value through
adversarial backtesting. Static validation shows our P-model effectively
matches market mean and volatility. In dynamic games, it demonstrates
significantly higher profitability than a traditional Monte Carlo-based model
for European and Asian options. However, the model shows limitations in pricing
products highly sensitive to extreme events, such as snowballs and
accumulators, because it tends to underestimate tail risks. The study concludes
that diffusion models hold significant potential for enhancing pricing
accuracy, though further research is needed to improve their ability to model
extreme market risks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [152] [Generative AI Pipeline for Interactive Prompt-driven 2D-to-3D Vascular Reconstruction for Fontan Geometries from Contrast-Enhanced X-Ray Fluoroscopy Imaging](https://arxiv.org/abs/2509.13372)
*Prahlad G Menon*

Main category: eess.IV

TL;DR: 本文开发多步AI管道处理心脏造影图像，生成适合CFD分析的几何结构及虚拟血流可视化，证明了临床可行性。


<details>
  <summary>Details</summary>
Motivation: 传统2D成像难以表征Fontan姑息治疗后复杂血流模式，现有评估方法提供的3D几何信息有限，无法满足CFD分析和手术规划需求。

Method: 开发多步AI管道，利用Google的Gemini 2.5 Flash处理血管造影图像，经多步骤处理后用Tencent的Hunyuan3D - 2mini生成立体光刻文件。

Result: 管道经16步处理从单视图血管造影成功生成几何优化的2D投影，能准确保留复杂几何结构和增强对比度，识别血流停滞区和模式，处理时间不到15分钟。

Conclusion: 该方法具有临床可行性，虽需迭代优化，但为利用现有成像数据进行高级几何和血流动力学分析奠定基础。

Abstract: Fontan palliation for univentricular congenital heart disease progresses to
hemodynamic failure with complex flow patterns poorly characterized by
conventional 2D imaging. Current assessment relies on fluoroscopic angiography,
providing limited 3D geometric information essential for computational fluid
dynamics (CFD) analysis and surgical planning.
  A multi-step AI pipeline was developed utilizing Google's Gemini 2.5 Flash
(2.5B parameters) for systematic, iterative processing of fluoroscopic
angiograms through transformer-based neural architecture. The pipeline
encompasses medical image preprocessing, vascular segmentation, contrast
enhancement, artifact removal, and virtual hemodynamic flow visualization
within 2D projections. Final views were processed through Tencent's
Hunyuan3D-2mini (384M parameters) for stereolithography file generation.
  The pipeline successfully generated geometrically optimized 2D projections
from single-view angiograms after 16 processing steps using a custom web
interface. Initial iterations contained hallucinated vascular features
requiring iterative refinement to achieve anatomically faithful
representations. Final projections demonstrated accurate preservation of
complex Fontan geometry with enhanced contrast suitable for 3D conversion.
AI-generated virtual flow visualization identified stagnation zones in central
connections and flow patterns in branch arteries. Complete processing required
under 15 minutes with second-level API response times.
  This approach demonstrates clinical feasibility of generating CFD-suitable
geometries from routine angiographic data, enabling 3D generation and rapid
virtual flow visualization for cursory insights prior to full CFD simulation.
While requiring refinement cycles for accuracy, this establishes foundation for
democratizing advanced geometric and hemodynamic analysis using readily
available imaging data.

</details>


### [153] [PREDICT-GBM: Platform for Robust Evaluation and Development of Individualized Computational Tumor Models in Glioblastoma](https://arxiv.org/abs/2509.13360)
*L. Zimmer,J. Weidner,M. Balcerak,F. Kofler,I. Ezhov,B. Menze,B. Wiestler*

Main category: eess.IV

TL;DR: 介绍PREDICT - GBM平台用于胶质母细胞瘤生长建模和评估，分析表明个性化放疗计划效果更好，为推进肿瘤生长建模和临床转化提供平台。


<details>
  <summary>Details</summary>
Motivation: 传统放疗未考虑患者特定因素，现有胶质母细胞瘤生长计算模型临床应用有限，需缩小转化差距。

Method: 引入PREDICT - GBM平台，用含255名受试者的临床数据集对先进肿瘤生长模型进行系统基准测试。

Result: 对于两个评估模型，基于肿瘤生长预测的个性化放疗计划在复发性覆盖方面优于传统统一边缘方法。

Conclusion: 建立了推进和系统评估前沿肿瘤生长建模方法的强大平台，有助于临床转化和改善患者预后。

Abstract: Glioblastoma is the most prevalent primary brain malignancy, distinguished by
its highly invasive behavior and exceptionally high rates of recurrence.
Conventional radiation therapy, which employs uniform treatment margins, fails
to account for patient-specific anatomical and biological factors that
critically influence tumor cell migration. To address this limitation, numerous
computational models of glioblastoma growth have been developed, enabling
generation of tumor cell distribution maps extending beyond radiographically
visible regions and thus informing more precise treatment strategies. However,
despite encouraging preliminary findings, the clinical adoption of these growth
models remains limited. To bridge this translational gap and accelerate both
model development and clinical validation, we introduce PREDICT-GBM, a
comprehensive integrated pipeline and dataset for modeling and evaluation. This
platform enables systematic benchmarking of state-of-the-art tumor growth
models using an expert-curated clinical dataset comprising 255 subjects with
complete tumor segmentations and tissue characterization maps. Our analysis
demonstrates that personalized radiation treatment plans derived from tumor
growth predictions achieved superior recurrence coverage compared to
conventional uniform margin approaches for two of the evaluated models. This
work establishes a robust platform for advancing and systematically evaluating
cutting-edge tumor growth modeling approaches, with the ultimate goal of
facilitating clinical translation and improving patient outcomes.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [154] [Secure, Scalable and Privacy Aware Data Strategy in Cloud](https://arxiv.org/abs/2509.13627)
*Vijay Kumar Butte,Sujata Butte*

Main category: cs.CR

TL;DR: 本文探讨企业在云环境下开发有效数据策略以应对数据处理和决策挑战。


<details>
  <summary>Details</summary>
Motivation: 企业面临安全、可扩展地处理和存储大量数据，以及支持决策者快速做出数据驱动决策的挑战。

Method: 讨论有效数据策略的各个组成部分，并提供解决安全、可扩展性和隐私方面的架构。

Result: 提出了有效企业数据策略的各组成部分和相关架构。

Conclusion: 开发出在云环境下的有效企业数据策略。

Abstract: The enterprises today are faced with the tough challenge of processing,
storing large amounts of data in a secure, scalable manner and enabling
decision makers to make quick, informed data driven decisions. This paper
addresses this challenge and develops an effective enterprise data strategy in
the cloud. Various components of an effective data strategy are discussed and
architectures addressing security, scalability and privacy aspects are
provided.

</details>


### [155] [Who Taught the Lie? Responsibility Attribution for Poisoned Knowledge in Retrieval-Augmented Generation](https://arxiv.org/abs/2509.13772)
*Baolei Zhang,Haoran Xin,Yuxi Chen,Zhuqing Liu,Biao Yi,Tong Li,Lihai Nie,Zheli Liu,Minghong Fang*

Main category: cs.CR

TL;DR: 提出RAGOrigin框架识别RAG系统知识数据库中导致错误生成的文本，评估效果好。


<details>
  <summary>Details</summary>
Motivation: RAG系统易受投毒攻击，现有防御方法常被绕过，需找出导致错误生成的文本。

Method: 构建针对每个错误生成事件的归因范围，通过评估检索排名、语义相关性和对生成响应的影响为候选文本分配责任分数，用无监督聚类方法隔离投毒文本。

Result: 在七个数据集和十五种投毒攻击上评估，RAGOrigin在识别投毒内容上优于现有基线，在动态和嘈杂条件下保持稳健。

Conclusion: RAGOrigin为追踪RAG系统中受污染知识的来源提供了实用有效的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge into large
language models to improve response quality. However, recent work has shown
that RAG systems are highly vulnerable to poisoning attacks, where malicious
texts are inserted into the knowledge database to influence model outputs.
While several defenses have been proposed, they are often circumvented by more
adaptive or sophisticated attacks.
  This paper presents RAGOrigin, a black-box responsibility attribution
framework designed to identify which texts in the knowledge database are
responsible for misleading or incorrect generations. Our method constructs a
focused attribution scope tailored to each misgeneration event and assigns a
responsibility score to each candidate text by evaluating its retrieval
ranking, semantic relevance, and influence on the generated response. The
system then isolates poisoned texts using an unsupervised clustering method. We
evaluate RAGOrigin across seven datasets and fifteen poisoning attacks,
including newly developed adaptive poisoning strategies and multi-attacker
scenarios. Our approach outperforms existing baselines in identifying poisoned
content and remains robust under dynamic and noisy conditions. These results
suggest that RAGOrigin provides a practical and effective solution for tracing
the origins of corrupted knowledge in RAG systems.

</details>


### [156] [Agentic JWT: A Secure Delegation Protocol for Autonomous AI Agents](https://arxiv.org/abs/2509.13597)
*Abhishek Goswami*

Main category: cs.CR

TL;DR: 本文介绍了Agentic JWT（A - JWT）以解决自主LLM代理在OAuth 2.0下的安全问题，实现安全代理身份和隔离，通过概念验证展示其功能并与OAuth代理讨论保持一致。


<details>
  <summary>Details</summary>
Motivation: 自主LLM代理可在无人监督下大量调用API，而OAuth 2.0在代理场景存在因随机推理、提示注入等导致权限无声扩展的问题。

Method: 引入A - JWT，定义新的授权机制，添加轻量级客户端垫片库，实现运行时代码自验证、生成意图令牌等。

Result: 实现Python概念验证，能在商品硬件上以亚毫秒级开销阻止违规请求、重放、模拟和提示注入等，设计与OAuth代理讨论一致。

Conclusion: 设计为代理应用提供了迈向零信任保障的途径，全面性能和安全评估结果将在后续期刊发表。

Abstract: Autonomous LLM agents can issue thousands of API calls per hour without human
oversight. OAuth 2.0 assumes deterministic clients, but in agentic settings
stochastic reasoning, prompt injection, or multi-agent orchestration can
silently expand privileges.
  We introduce Agentic JWT (A-JWT), a dual-faceted intent token that binds each
agent's action to verifiable user intent and, optionally, to a specific
workflow step. A-JWT carries an agent's identity as a one-way checksum hash
derived from its prompt, tools and configuration, and a chained delegation
assertion to prove which downstream agent may execute a given task, and
per-agent proof-of-possession keys to prevent replay and in-process
impersonation. We define a new authorization mechanism and add a lightweight
client shim library that self-verifies code at run time, mints intent tokens,
tracks workflow steps and derives keys, thus enabling secure agent identity and
separation even within a single process.
  We illustrate a comprehensive threat model for agentic applications,
implement a Python proof-of-concept and show functional blocking of
scope-violating requests, replay, impersonation, and prompt-injection pathways
with sub-millisecond overhead on commodity hardware. The design aligns with
ongoing OAuth agent discussions and offers a drop-in path toward zero-trust
guarantees for agentic applications. A comprehensive performance and security
evaluation with experimental results will appear in our forthcoming journal
publication

</details>


### [157] [Differential Privacy in Federated Learning: Mitigating Inference Attacks with Randomized Response](https://arxiv.org/abs/2509.13987)
*Ozer Ozturk,Busra Buyuktanir,Gozde Karatas Baydogmus,Kazim Yildiz*

Main category: cs.CR

TL;DR: 文章指出分布式机器学习模型存在安全隐私问题，提出联邦学习架构，但仍有安全漏洞。采用差分隐私和duCBA算法，研究发现隐私增强会使模型精度下降，需平衡安全与性能。


<details>
  <summary>Details</summary>
Motivation: 解决分布式机器学习模型将数据存储在中央服务器带来的安全和隐私问题，以及联邦学习架构存在的推理攻击导致数据泄露问题。

Method: 应用差分隐私，使用duCBA算法作为联邦聚合方法，用随机响应技术在数据上实现差分隐私，在不同epsilon值下研究安全与性能的权衡。

Result: 随着epsilon值降低，模型精度下降，出现类预测不平衡。

Conclusion: 更高的隐私级别不总能带来实际结果，必须仔细考虑安全和性能之间的平衡。

Abstract: Machine learning models used for distributed architectures consisting of
servers and clients require large amounts of data to achieve high accuracy.
Data obtained from clients are collected on a central server for model
training. However, storing data on a central server raises concerns about
security and privacy. To address this issue, a federated learning architecture
has been proposed. In federated learning, each client trains a local model
using its own data. The trained models are periodically transmitted to the
central server. The server then combines the received models using federated
aggregation algorithms to obtain a global model. This global model is
distributed back to the clients, and the process continues in a cyclical
manner. Although preventing data from leaving the clients enhances security,
certain concerns still remain. Attackers can perform inference attacks on the
obtained models to approximate the training dataset, potentially causing data
leakage. In this study, differential privacy was applied to address the
aforementioned security vulnerability, and a performance analysis was
conducted. The Data-Unaware Classification Based on Association (duCBA)
algorithm was used as the federated aggregation method. Differential privacy
was implemented on the data using the Randomized Response technique, and the
trade-off between security and performance was examined under different epsilon
values. As the epsilon value decreased, the model accuracy declined, and class
prediction imbalances were observed. This indicates that higher levels of
privacy do not always lead to practical outcomes and that the balance between
security and performance must be carefully considered.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [158] [Julia GraphBLAS with Nonblocking Execution](https://arxiv.org/abs/2509.14211)
*Pascal Costanza,Timothy G. Mattson,Raye Kimmerer,Benjamin Brock*

Main category: cs.MS

TL;DR: 本文介绍在Julia语言中实现支持积极非阻塞执行的GraphBLAS，展示该语言简化实现的特性，工作处于进行中，目前限于支持PageRank的方法。


<details>
  <summary>Details</summary>
Motivation: 现有GraphBLAS非阻塞执行实现范围有限，需实现支持积极非阻塞执行的GraphBLAS。

Method: 利用Julia编程语言的特性来实现非阻塞执行。

Result: 展示了Julia语言特性可极大简化非阻塞执行的实现。

Conclusion: 当前工作虽为进行中，但显示了非阻塞执行的潜力，目前局限于支持PageRank的GraphBLAS方法。

Abstract: From the beginning, the GraphBLAS were designed for ``nonblocking
execution''; i.e., calls to GraphBLAS methods return as soon as the arguments
to the methods are validated and define a directed acyclic graph (DAG) of
GraphBLAS operations. This lets GraphBLAS implementations fuse functions, elide
unneeded objects, exploit parallelism, plus any additional DAG-preserving
transformations. GraphBLAS implementations exist that utilize nonblocking
execution but with limited scope. In this paper, we describe our work to
implement GraphBLAS with support for aggressive nonblocking execution. We show
how features of the Julia programming language greatly simplify implementation
of nonblocking execution. This is \emph{work-in-progress} sufficient to show
the potential for nonblocking execution and is limited to GraphBLAS methods
required to support PageRank.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [159] [A reduced-order derivative-informed neural operator for subsurface fluid-flow](https://arxiv.org/abs/2509.13620)
*Jeongjin,Park,Grant Bruer,Huseyin Tuna Erdinc,Abhinav Prakash Gahlot,Felix J. Herrmann*

Main category: physics.comp-ph

TL;DR: 提出DeFINO框架解决现有神经算子求导计算成本高问题，在合成实验中验证其在降低成本同时提高梯度精度和保持正向预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在处理与系统参数相关的下游任务时，保真梯度计算成本高，需解决该限制。

Method: 提出DeFINO框架，将傅里叶神经算子与基于Fisher信息矩阵引导的训练策略结合，把雅可比矩阵投影到主导特征方向。

Result: 在地下多相流合成实验中，DeFINO提高了梯度精度，同时保持了对流体动力学的稳健正向预测。

Conclusion: DeFINO有潜力为复杂现实场景中的反演问题提供实用、可扩展且低成本的解决方案。

Abstract: Neural operators have emerged as cost-effective surrogates for expensive
fluid-flow simulators, particularly in computationally intensive tasks such as
permeability inversion from time-lapse seismic data, and uncertainty
quantification. In these applications, the fidelity of the surrogate's
gradients with respect to system parameters is crucial, as the accuracy of
downstream tasks, such as optimization and Bayesian inference, relies directly
on the quality of the derivative information. Recent advances in
physics-informed methods have leveraged derivative information to improve
surrogate accuracy. However, incorporating explicit Jacobians can become
computationally prohibitive, as the complexity typically scales quadratically
with the number of input parameters. To address this limitation, we propose
DeFINO (Derivative-based Fisher-score Informed Neural Operator), a
reduced-order, derivative-informed training framework. DeFINO integrates
Fourier neural operators (FNOs) with a novel derivative-based training strategy
guided by the Fisher Information Matrix (FIM). By projecting Jacobians onto
dominant eigen-directions identified by the FIM, DeFINO captures critical
sensitivity information directly informed by observational data, significantly
reducing computational expense. We validate DeFINO through synthetic
experiments in the context of subsurface multi-phase fluid-flow, demonstrating
improvements in gradient accuracy while maintaining robust forward predictions
of underlying fluid dynamics. These results highlight DeFINO's potential to
offer practical, scalable solutions for inversion problems in complex
real-world scenarios, all at substantially reduced computational cost.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [160] [How Fly Neural Perception Mechanisms Enhance Visuomotor Control of Micro Robots](https://arxiv.org/abs/2509.13827)
*Renyuan Liu,Haoting Zhou,Chuankai Fang,Qinbing Fu*

Main category: cs.RO

TL;DR: 本文提出受果蝇视觉神经元启发的注意力驱动视觉运动控制策略，用于移动机器人避障，经评估该策略有效且凸显果蝇神经模型潜力。


<details>
  <summary>Details</summary>
Motivation: 自主机器人在复杂环境实现类似果蝇的敏捷性受计算成本和性能权衡的限制，昆虫启发的智能提供低功耗、高效计算框架。

Method: 提出受果蝇LPLC2神经元及其逃逸行为启发的注意力驱动视觉运动控制策略，简化优化模型以适应微型机器人计算限制，引入多注意力机制，与蝗虫启发的碰撞检测模型对比评估。

Result: 果蝇启发的视觉运动模型在碰撞检测成功率达96.1%，具有相当的鲁棒性，能产生更自适应和优雅的规避动作。

Conclusion: 该工作不仅展示了有效的避障策略，还凸显了果蝇启发的神经模型在昆虫智能集体行为研究方面的潜力。

Abstract: Anyone who has tried to swat a fly has likely been frustrated by its
remarkable agility.This ability stems from its visual neural perception system,
particularly the collision-selective neurons within its small brain.For
autonomous robots operating in complex and unfamiliar environments, achieving
similar agility is highly desirable but often constrained by the trade-off
between computational cost and performance.In this context, insect-inspired
intelligence offers a parsimonious route to low-power, computationally
efficient frameworks.In this paper, we propose an attention-driven visuomotor
control strategy inspired by a specific class of fly visual projection
neurons-the lobula plate/lobula column type-2 (LPLC2)-and their associated
escape behaviors.To our knowledge, this represents the first embodiment of an
LPLC2 neural model in the embedded vision of a physical mobile robot, enabling
collision perception and reactive evasion.The model was simplified and
optimized at 70KB in memory to suit the computational constraints of a
vision-based micro robot, the Colias, while preserving key neural perception
mechanisms.We further incorporated multi-attention mechanisms to emulate the
distributed nature of LPLC2 responses, allowing the robot to detect and react
to approaching targets both rapidly and selectively.We systematically evaluated
the proposed method against a state-of-the-art locust-inspired collision
detection model.Results showed that the fly-inspired visuomotor model achieved
comparable robustness, at success rate of 96.1% in collision detection while
producing more adaptive and elegant evasive maneuvers.Beyond demonstrating an
effective collision-avoidance strategy, this work highlights the potential of
fly-inspired neural models for advancing research into collective behaviors in
insect intelligence.

</details>


### [161] [Real World Robotic Exploration using Deep Neural Networks Trained in Photorealistic Reconstructed Environments](https://arxiv.org/abs/2509.13342)
*Isaac Ronald Ward*

Main category: cs.RO

TL;DR: 改进现有深度神经网络方法，提升机器人定位性能，创建适用于室内场景的导航算法。


<details>
  <summary>Details</summary>
Motivation: 提高现有基于视觉信息确定机器人位姿的深度神经网络的定位性能，增强对感知混淆的鲁棒性。

Method: 扩展网络损失函数，结合位置和旋转误差；使用摄影测量数据生成带位姿标签的数据集训练模型。

Result: 室内场景定位精度提高，中位位置和旋转误差分别最多降低9.64%和2.99%，训练模型定位精度达0.11m和0.89度，并在TurtleBot上实时测试导航算法。

Conclusion: 引入了为任意真实室内场景创建鲁棒导航算法的完整流程，仅需短时间采集场景图像。

Abstract: In this work, an existing deep neural network approach for determining a
robot's pose from visual information (RGB images) is modified, improving its
localization performance without impacting its ease of training. Explicitly,
the network's loss function is extended in a manner which intuitively combines
the positional and rotational error in order to increase robustness to
perceptual aliasing. An improvement in the localization accuracy for indoor
scenes is observed: with decreases of up to 9.64% and 2.99% in the median
positional and rotational error respectively, when compared to the unmodified
network.
  Additionally, photogrammetry data is used to produce a pose-labelled dataset
which allows the above model to be trained on a local environment, resulting in
localization accuracies of 0.11m & 0.89 degrees. This trained model forms the
basis of a navigation algorithm, which is tested in real-time on a TurtleBot (a
wheeled robotic device). As such, this work introduces a full pipeline for
creating a robust navigational algorithm for any given real world indoor scene;
the only requirement being a collection of images from the scene, which can be
captured in as little as 330 seconds of

</details>


### [162] [Label-Efficient Grasp Joint Prediction with Point-JEPA](https://arxiv.org/abs/2509.13349)
*Jed Guzelkabaagac,Boris Petrović*

Main category: cs.RO

TL;DR: 研究3D自监督预训练Point - JEPA是否能实现高效的抓取关节角度预测，结果表明JEPA风格预训练对数据高效抓取学习有效。


<details>
  <summary>Details</summary>
Motivation: 探究3D自监督预训练Point - JEPA能否实现标签高效的抓取关节角度预测。

Method: 使用从网格中标记的点云以及ShapeNet预训练的Point - JEPA编码器，训练轻量级多假设头并采用胜者为王策略，通过最高对数选择进行评估。

Result: 在DLR - Hand II上，Point - JEPA在低标签情况下将RMSE降低了26%，并达到与全监督相当的效果。

Conclusion: JEPA风格的预训练是一种用于数据高效抓取学习的实用方法。

Abstract: We investigate whether 3D self-supervised pretraining with a Joint-Embedding
Predictive Architecture (Point-JEPA) enables label-efficient grasp joint-angle
prediction. Using point clouds tokenized from meshes and a ShapeNet-pretrained
Point-JEPA encoder, we train a lightweight multi-hypothesis head with
winner-takes-all and evaluate by top-logit selection. On DLR-Hand II with
object-level splits, Point-JEPA reduces RMSE by up to 26% in low-label regimes
and reaches parity with full supervision. These results suggest JEPA-style
pretraining is a practical approach for data-efficient grasp learning.

</details>


### [163] [ASTREA: Introducing Agentic Intelligence for Orbital Thermal Autonomy](https://arxiv.org/abs/2509.13380)
*Alejandro D. Mousist*

Main category: cs.RO

TL;DR: 本文介绍首个部署在飞行验证硬件上用于自主航天器操作的代理系统ASTREA，通过热控用例实验，展示其在地面和在轨表现，指出机遇与局限并给出设计指南。


<details>
  <summary>Details</summary>
Motivation: 开发适用于飞行验证硬件的自主航天器操作代理系统，探索语义推理与自适应控制在硬件约束下结合的可行性。

Method: 以热控为用例，将资源受限的大语言模型（LLM）代理与强化学习控制器集成在适用于航天平台的异步架构中。

Result: 地面实验显示LLM引导的监督提高热稳定性并减少违规；在轨验证中，因推理延迟与低地球轨道卫星快速热循环不匹配导致性能下降。

Conclusion: 凸显基于代理的LLM系统在实际飞行环境中的机遇和当前局限，为未来太空自主提供实用设计指南。

Abstract: This paper presents ASTREA, the first agentic system deployed on
flight-heritage hardware (TRL 9) for autonomous spacecraft operations. Using
thermal control as a representative use case, we integrate a
resource-constrained Large Language Model (LLM) agent with a reinforcement
learning controller in an asynchronous architecture tailored for
space-qualified platforms. Ground experiments show that LLM-guided supervision
improves thermal stability and reduces violations, confirming the feasibility
of combining semantic reasoning with adaptive control under hardware
constraints. However, on-orbit validation aboard the International Space
Station (ISS) reveals performance degradation caused by inference latency
mismatched with the rapid thermal cycles characteristic of Low Earth Orbit
(LEO) satellites. These results highlight both the opportunities and current
limitations of agentic LLM-based systems in real flight environments, providing
practical design guidelines for future space autonomy.

</details>


### [164] [Dense-Jump Flow Matching with Non-Uniform Time Scheduling for Robotic Policies: Mitigating Multi-Step Inference Degradation](https://arxiv.org/abs/2509.13574)
*Zidong Chen,Zihao Guo,Peng Wang,ThankGod Itua Egbe,Yan Lyu,Chenghao Qian*

Main category: cs.RO

TL;DR: 发现流匹配在机器人生成策略泛化问题，提出非均匀时间调度和密集跳跃积分策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决流匹配在机器人生成策略中泛化早饱和、增加欧拉积分步数反而降低性能的问题。

Method: 训练时采用非均匀时间调度（如U型），推理时采用密集跳跃积分策略。

Result: 在不同机器人任务中比现有基线性能提升达23.7%。

Conclusion: 所提策略是高效单步学习器，通过多步积分提升性能。

Abstract: Flow matching has emerged as a competitive framework for learning
high-quality generative policies in robotics; however, we find that
generalisation arises and saturates early along the flow trajectory, in
accordance with recent findings in the literature. We further observe that
increasing the number of Euler integration steps during inference
counter-intuitively and universally degrades policy performance. We attribute
this to (i) additional, uniformly spaced integration steps oversample the
late-time region, thereby constraining actions towards the training
trajectories and reducing generalisation; and (ii) the learned velocity field
becoming non-Lipschitz as integration time approaches 1, causing instability.
To address these issues, we propose a novel policy that utilises non-uniform
time scheduling (e.g., U-shaped) during training, which emphasises both early
and late temporal stages to regularise policy training, and a dense-jump
integration schedule at inference, which uses a single-step integration to
replace the multi-step integration beyond a jump point, to avoid unstable areas
around 1. Essentially, our policy is an efficient one-step learner that still
pushes forward performance through multi-step integration, yielding up to 23.7%
performance gains over state-of-the-art baselines across diverse robotic tasks.

</details>


### [165] [TreeIRL: Safe Urban Driving with Tree Search and Inverse Reinforcement Learning](https://arxiv.org/abs/2509.13579)
*Momchil S. Tomov,Sang Uk Lee,Hansford Hendrago,Jinwook Huh,Teawon Han,Forbes Howington,Rafael da Silva,Gianmarco Bernasconi,Marc Heim,Samuel Findler,Xiaonan Ji,Alexander Boule,Michael Napoli,Kuo Chen,Jesse Miller,Boaz Floor,Yunqing Hu*

Main category: cs.RO

TL;DR: 提出TreeIRL规划器，结合MCTS和IRL，在仿真和现实驾驶中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶规划瓶颈，实现更优规划性能。

Method: 结合蒙特卡罗树搜索（MCTS）和逆强化学习（IRL），用MCTS找候选轨迹，用IRL评分函数选最似人类的轨迹。

Result: 在大规模仿真和500多英里现实驾驶测试中，TreeIRL表现最佳，平衡了安全、进度、舒适和类人程度。

Conclusion: TreeIRL是首次基于MCTS的公共道路规划，强调多指标和现实环境评估规划器的重要性，且具有高扩展性。

Abstract: We present TreeIRL, a novel planner for autonomous driving that combines
Monte Carlo tree search (MCTS) and inverse reinforcement learning (IRL) to
achieve state-of-the-art performance in simulation and in real-world driving.
The core idea is to use MCTS to find a promising set of safe candidate
trajectories and a deep IRL scoring function to select the most human-like
among them. We evaluate TreeIRL against both classical and state-of-the-art
planners in large-scale simulations and on 500+ miles of real-world autonomous
driving in the Las Vegas metropolitan area. Test scenarios include dense urban
traffic, adaptive cruise control, cut-ins, and traffic lights. TreeIRL achieves
the best overall performance, striking a balance between safety, progress,
comfort, and human-likeness. To our knowledge, our work is the first
demonstration of MCTS-based planning on public roads and underscores the
importance of evaluating planners across a diverse set of metrics and in
real-world environments. TreeIRL is highly extensible and could be further
improved with reinforcement learning and imitation learning, providing a
framework for exploring different combinations of classical and learning-based
approaches to solve the planning bottleneck in autonomous driving.

</details>


### [166] [Maximizing UAV Cellular Connectivity with Reinforcement Learning for BVLoS Path Planning](https://arxiv.org/abs/2509.13336)
*Mehran Behjati,Rosdiadee Nordin,Nor Fadzilah Abdullah*

Main category: cs.RO

TL;DR: 提出基于强化学习的视距外蜂窝连接无人机路径规划方法，仿真验证有效，可集成到地面控制系统，有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 解决无人机蜂窝通信限制带来的挑战，在考虑现实空中覆盖约束下，最小化飞行距离并最大化蜂窝链路连接质量。

Method: 采用强化学习技术训练智能体，以无人机与基站通信链路质量为奖励函数。

Result: 仿真结果表明该方法能有效训练智能体并生成可行路径规划。

Conclusion: 强化学习算法能高效识别最优路径，确保最大连接性，可作为离线模块集成到地面控制系统，推动蜂窝连接无人机路径规划技术发展。

Abstract: This paper presents a reinforcement learning (RL) based approach for path
planning of cellular connected unmanned aerial vehicles (UAVs) operating beyond
visual line of sight (BVLoS). The objective is to minimize travel distance
while maximizing the quality of cellular link connectivity by considering real
world aerial coverage constraints and employing an empirical aerial channel
model. The proposed solution employs RL techniques to train an agent, using the
quality of communication links between the UAV and base stations (BSs) as the
reward function. Simulation results demonstrate the effectiveness of the
proposed method in training the agent and generating feasible UAV path plans.
The proposed approach addresses the challenges due to limitations in UAV
cellular communications, highlighting the need for investigations and
considerations in this area. The RL algorithm efficiently identifies optimal
paths, ensuring maximum connectivity with ground BSs to ensure safe and
reliable BVLoS flight operation. Moreover, the solution can be deployed as an
offline path planning module that can be integrated into future ground control
systems (GCS) for UAV operations, enhancing their capabilities and safety. The
method holds potential for complex long range UAV applications, advancing the
technology in the field of cellular connected UAV path planning.

</details>


### [167] [DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater Monitoring](https://arxiv.org/abs/2509.13666)
*Zhenqi Wu,Abhinav Modi,Angelos Mavrogiannis,Kaustubh Joshi,Nikhil Chopra,Yiannis Aloimonos,Nare Karapetyan,Ioannis Rekleitis,Xiaomin Lin*

Main category: cs.RO

TL;DR: 海洋变暖和酸化威胁贝类生存，需长期监测系统。本文提出DREAM框架用于水下长期探索和栖息地监测，结果显示其在目标探索任务中高效。


<details>
  <summary>Details</summary>
Motivation: 海洋变暖和酸化增加贝类大规模死亡风险，人类长期水下监测成本高且危险，需要开发长期水下监测系统及智能决策框架。

Method: 提出Vision Language Model (VLM) 引导的自主框架DREAM用于长期水下探索和栖息地监测。

Result: 框架在无先验位置信息下能高效寻找和探索目标对象。牡蛎监测任务中耗时比基线少31.5%；相比普通VLM，步数少23%且覆盖牡蛎多8.88%。沉船场景中，成功探索和绘制沉船图，步数比普通模型少27.5%，覆盖率达100%，普通模型为60.23%。

Conclusion: DREAM框架在水下目标探索和监测任务中表现高效，能有效完成长期水下探索和栖息地监测工作。

Abstract: The ocean is warming and acidifying, increasing the risk of mass mortality
events for temperature-sensitive shellfish such as oysters. This motivates the
development of long-term monitoring systems. However, human labor is costly and
long-duration underwater work is highly hazardous, thus favoring robotic
solutions as a safer and more efficient option. To enable underwater robots to
make real-time, environment-aware decisions without human intervention, we must
equip them with an intelligent "brain." This highlights the need for
persistent,wide-area, and low-cost benthic monitoring. To this end, we present
DREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term
underwater exploration and habitat monitoring. The results show that our
framework is highly efficient in finding and exploring target objects (e.g.,
oysters, shipwrecks) without prior location information. In the
oyster-monitoring task, our framework takes 31.5% less time than the previous
baseline with the same amount of oysters. Compared to the vanilla VLM, it uses
23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our
framework successfully explores and maps the wreck without collisions,
requiring 27.5% fewer steps than the vanilla model and achieving 100% coverage,
while the vanilla model achieves 60.23% average coverage in our shipwreck
environments.

</details>


### [168] [Cooperative Target Detection with AUVs: A Dual-Timescale Hierarchical MARDL Approach](https://arxiv.org/abs/2509.13381)
*Zhang Xueyao,Yang Bo,Yu Zhiwen,Cao Xuelin,George C. Alexandropoulos,Merouane Debbah,Chau Yuen*

Main category: cs.RO

TL;DR: 提出双时间尺度分层多智能体近端策略优化框架用于水下协作任务，模拟显示其性能优且能保障隐蔽操作。


<details>
  <summary>Details</summary>
Motivation: 协作式AUV通信有暴露风险，在对抗环境中实现高效协作与隐蔽操作是水下协作任务的关键挑战。

Method: 提出双时间尺度的Hierarchical Multi - Agent Proximal Policy Optimization (H - MAPPO)框架，高层基于中心AUV确定参与任务个体，低层参与AUV通过功率和轨迹控制降低暴露概率。

Result: 该框架实现快速收敛，性能优于基准算法。

Conclusion: 该框架能在确保隐蔽操作的同时最大化长期协作效率。

Abstract: Autonomous Underwater Vehicles (AUVs) have shown great potential for
cooperative detection and reconnaissance. However, collaborative AUV
communications introduce risks of exposure. In adversarial environments,
achieving efficient collaboration while ensuring covert operations becomes a
key challenge for underwater cooperative missions. In this paper, we propose a
novel dual time-scale Hierarchical Multi-Agent Proximal Policy Optimization
(H-MAPPO) framework. The high-level component determines the individuals
participating in the task based on a central AUV, while the low-level component
reduces exposure probabilities through power and trajectory control by the
participating AUVs. Simulation results show that the proposed framework
achieves rapid convergence, outperforms benchmark algorithms in terms of
performance, and maximizes long-term cooperative efficiency while ensuring
covert operations.

</details>


### [169] [VEGA: Electric Vehicle Navigation Agent via Physics-Informed Neural Operator and Proximal Policy Optimization](https://arxiv.org/abs/2509.13386)
*Hansol Lim,Minhyeok Im,Jonathan Boyack,Jee Won Lee,Jongseong Brad Choi*

Main category: cs.RO

TL;DR: 本文提出VEGA，一个充电感知的电动汽车导航代理，利用物理信息学习和强化学习实现生态路由，评估显示其在长路线上表现良好且有泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着软件定义汽车需求上升及电动汽车配备强大计算机，需要优化充电感知路径规划以适应车辆当前状况和环境。

Method: 提出VEGA，包含物理信息神经算子（PINO）模块和强化学习（RL）代理模块，前者基于车辆速度和电池功率日志学习车辆自定义动力学，后者在充电状态约束下优化路径。

Result: 在旧金山到纽约等长路线评估中，VEGA的停车、停留时间、充电状态管理和总旅行时间与特斯拉行程规划器相近且更保守，在法国和日本也能计算最优充电感知路径。

Conclusion: VEGA实现了物理信息学习和强化学习在电动汽车生态路由中的实际集成。

Abstract: Demands for software-defined vehicles (SDV) are rising and electric vehicles
(EVs) are increasingly being equipped with powerful computers. This enables
onboard AI systems to optimize charge-aware path optimization customized to
reflect vehicle's current condition and environment. We present VEGA, a
charge-aware EV navigation agent that plans over a charger-annotated road graph
using Proximal Policy Optimization (PPO) with budgeted A* teacher-student
guidance under state-of-charge (SoC) feasibility. VEGA consists of two modules.
First, a physics-informed neural operator (PINO), trained on real vehicle speed
and battery-power logs, uses recent vehicle speed logs to estimate aerodynamic
drag, rolling resistance, mass, motor and regenerative-braking efficiencies,
and auxiliary load by learning a vehicle-custom dynamics. Second, a
Reinforcement Learning (RL) agent uses these dynamics to optimize a path with
optimal charging stops and dwell times under SoC constraints. VEGA requires no
additional sensors and uses only vehicle speed signals. It may serve as a
virtual sensor for power and efficiency to potentially reduce EV cost. In
evaluation on long routes like San Francisco to New York, VEGA's stops, dwell
times, SoC management, and total travel time closely track Tesla Trip Planner
while being slightly more conservative, presumably due to real vehicle
conditions such as vehicle parameter drift due to deterioration. Although
trained only in U.S. regions, VEGA was able to compute optimal charge-aware
paths in France and Japan, demonstrating generalizability. It achieves
practical integration of physics-informed learning and RL for EV eco-routing.

</details>


### [170] [MAP: End-to-End Autonomous Driving with Map-Assisted Planning](https://arxiv.org/abs/2509.13926)
*Huilin Yin,Yiming Kan,Daniel Watzenig*

Main category: cs.RO

TL;DR: 本文提出MAP框架用于端到端自动驾驶轨迹规划，实验显示其效果良好，为系统结构设计提供新方向。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法未充分利用在线地图模块提升轨迹规划的潜力。

Method: 提出MAP框架，通过Plan - enhancing Online Mapping模块、Ego - status - guided Planning模块和基于当前自我状态的Weight Adapter，明确整合基于分割的地图特征和当前自我状态。

Result: 在DAIR - V2X - seq - SPD数据集上，L2位移误差降低16.6%，越野率降低56.2%，总体得分提高44.5%；在MEIS Workshop @CVPR2025挑战赛Track 2中排名第一，总体得分比第二好的模型高39.5%。

Conclusion: 明确利用语义地图特征进行规划是有效的，为端到端自动驾驶系统结构设计改进提供新方向。

Abstract: In recent years, end-to-end autonomous driving has attracted increasing
attention for its ability to jointly model perception, prediction, and planning
within a unified framework. However, most existing approaches underutilize the
online mapping module, leaving its potential to enhance trajectory planning
largely untapped. This paper proposes MAP (Map-Assisted Planning), a novel
map-assisted end-to-end trajectory planning framework. MAP explicitly
integrates segmentation-based map features and the current ego status through a
Plan-enhancing Online Mapping module, an Ego-status-guided Planning module, and
a Weight Adapter based on current ego status. Experiments conducted on the
DAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6%
reduction in L2 displacement error, a 56.2% reduction in off-road rate, and a
44.5% improvement in overall score compared to the UniV2X baseline, even
without post-processing. Furthermore, it achieves top ranking in Track 2 of the
End-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS
Workshop @CVPR2025, outperforming the second-best model by 39.5% in terms of
overall score. These results highlight the effectiveness of explicitly
leveraging semantic map features in planning and suggest new directions for
improving structure design in end-to-end autonomous driving systems. Our code
is available at https://gitee.com/kymkym/map.git

</details>


### [171] [Prompt2Auto: From Motion Prompt to Automated Control via Geometry-Invariant One-Shot Gaussian Process Learning](https://arxiv.org/abs/2509.14040)
*Zewen Yang,Xiaobing Dai,Dongfa Zhang,Yu Li,Ziyang Meng,Bingkun Huang,Hamid Sadeghian,Sami Haddadin*

Main category: cs.RO

TL;DR: 提出Prompt2Auto几何不变单样本高斯过程学习框架，让机器人从单运动提示中执行自动控制，经实验验证有效且能减轻演示负担。


<details>
  <summary>Details</summary>
Motivation: 传统从演示中学习的方法需大量数据集，且无法跨坐标变换泛化。

Method: 提出Prompt2Auto框架，引入基于坐标变换的数据集构建策略，实现平移、旋转和缩放不变性，支持多步预测。

Result: 通过数值模拟和两个真实世界机器人实验验证，方法有效、能跨任务泛化。

Conclusion: 所提方法有效，能跨任务泛化，显著减轻演示负担。

Abstract: Learning from demonstration allows robots to acquire complex skills from
human demonstrations, but conventional approaches often require large datasets
and fail to generalize across coordinate transformations. In this paper, we
propose Prompt2Auto, a geometry-invariant one-shot Gaussian process (GeoGP)
learning framework that enables robots to perform human-guided automated
control from a single motion prompt. A dataset-construction strategy based on
coordinate transformations is introduced that enforces invariance to
translation, rotation, and scaling, while supporting multi-step predictions.
Moreover, GeoGP is robust to variations in the user's motion prompt and
supports multi-skill autonomy. We validate the proposed approach through
numerical simulations with the designed user graphical interface and two
real-world robotic experiments, which demonstrate that the proposed method is
effective, generalizes across tasks, and significantly reduces the
demonstration burden. Project page is available at:
https://prompt2auto.github.io

</details>


### [172] [Multi-robot Multi-source Localization in Complex Flows with Physics-Preserving Environment Models](https://arxiv.org/abs/2509.14228)
*Benjamin Shaffer,Victoria Edwards,Brooks Kinch,Nathaniel Trask,M. Ani Hsieh*

Main category: cs.RO

TL;DR: 提出分布式移动传感框架用于复杂流中的源定位，比基线策略和方法更优。


<details>
  <summary>Details</summary>
Motivation: 复杂流中的源定位对多机器人团队是挑战，流动力学和环境几何使建模和预测困难，且机器人计算能力有限。

Method: 提出分布式移动传感框架，每个机器人携带机器学习的有限元模型，用近似互信息准则驱动信息寻优控制策略。

Result: 比基线传感策略更快减少误差，比基线机器学习方法实现更准确的源定位。

Conclusion: 所提出的分布式移动传感框架在复杂流源定位中表现良好。

Abstract: Source localization in a complex flow poses a significant challenge for
multi-robot teams tasked with localizing the source of chemical leaks or
tracking the dispersion of an oil spill. The flow dynamics can be time-varying
and chaotic, resulting in sporadic and intermittent sensor readings, and
complex environmental geometries further complicate a team's ability to model
and predict the dispersion. To accurately account for the physical processes
that drive the dispersion dynamics, robots must have access to computationally
intensive numerical models, which can be difficult when onboard computation is
limited. We present a distributed mobile sensing framework for source
localization in which each robot carries a machine-learned, finite element
model of its environment to guide information-based sampling. The models are
used to evaluate an approximate mutual information criterion to drive an
infotaxis control strategy, which selects sensing regions that are expected to
maximize informativeness for the source localization objective. Our approach
achieves faster error reduction compared to baseline sensing strategies and
results in more accurate source localization compared to baseline machine
learning approaches.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [173] [Complexity Bounds for Smooth Convex Multiobjective Optimization](https://arxiv.org/abs/2509.13550)
*Phillipe R. Sampaio*

Main category: math.OC

TL;DR: 研究光滑多目标优化中找ε - Pareto驻点的oracle复杂度，给出强凸、凸问题不同方法的收敛速率下界。


<details>
  <summary>Details</summary>
Motivation: 研究光滑多目标优化中找ε - Pareto驻点的oracle复杂度。

Method: 分析不同类型问题（强凸、凸）和不同方法（span一阶方法、oblivious一步方法、加速梯度下降、一般span方法）。

Result: 给出不同情况下的收敛速率下界，如强凸问题线性收敛速率，凸问题不同方法1/T或1/T²的速率。

Conclusion: 不同方法在多目标优化中有不同收敛速率，部分情况存在已知上界和最坏情况保证的差距。

Abstract: We study the oracle complexity of finding $\varepsilon$-Pareto stationary
points in smooth multiobjective optimization with $m$ objectives. The progress
metric is the Pareto stationarity gap $\mathcal{G}(x)$ (the norm of an optimal
convex combination of gradients). Our contributions are fourfold. (i) For
strongly convex objectives, any span first-order method (iterates lie in the
span of past gradients) exhibits linear convergence no faster than
$\exp(-\Theta(T/\sqrt{\kappa}))$ after $T$ oracle calls, where $\kappa$ is the
condition number, implying $\Theta(\sqrt{\kappa}\log(1/\varepsilon))$
iterations; this matches classical accelerated upper bounds. (ii) For convex
problems and oblivious one-step methods (a fixed scalarization with
pre-scheduled step sizes), we prove a lower bound of order $1/T$ on the best
gradient norm among the first $T$ iterates. (iii) Although accelerated gradient
descent is outside this restricted class, it is an oblivious span method and
attains the same $1/T$ upper rate on a fixed scalarization. (iv) For convex
problems and general span methods with adaptive scalarizations, we establish a
universal lower bound of order $1/T^{2}$ on the gradient norm of the final
iterate after $T$ steps, highlighting a gap between known upper bounds and
worst-case guarantees. All bounds hold on non-degenerate instances with
distinct objectives and non-singleton Pareto fronts; rates are stated up to
universal constants and natural problem scaling.

</details>


### [174] [Accelerated Gradient Methods with Biased Gradient Estimates: Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds](https://arxiv.org/abs/2509.13628)
*Mert Gürbüzbalaban,Yasa Syed,Necdet Serhat Aybat*

Main category: math.OC

TL;DR: 研究一阶方法中收敛速度与梯度误差鲁棒性的权衡，聚焦广义动量方法，给出二次目标下RSI的闭式表达式，证明大偏差原理，推导非渐近界，有数值实验。


<details>
  <summary>Details</summary>
Motivation: 研究一阶方法中收敛速度和对梯度误差鲁棒性之间的权衡，特别是广义动量方法（GMMs）在此方面的情况。

Method: 允许随机梯度误差，用风险敏感指数（RSI）量化鲁棒性，使用2x2 Riccati方程给出二次目标下RSI的闭式表达式，证明大偏差原理，推导非渐近界。

Result: 揭示了RSI和收敛速度在步长和动量选择上的帕累托前沿，证明大偏差原理，将RSI与$H_{\infty}$ - 范数联系起来，得到非渐近界，在平滑强凸函数上也观察到类似权衡。

Conclusion: 这是首次对有偏梯度的GMMs进行非渐近保证和风险敏感分析，数值实验验证了结果。

Abstract: We study trade-offs between convergence rate and robustness to gradient
errors in first-order methods. Our focus is on generalized momentum methods
(GMMs), a class that includes Nesterov's accelerated gradient, heavy-ball, and
gradient descent. We allow stochastic gradient errors that may be adversarial
and biased, and quantify robustness via the risk-sensitive index (RSI) from
robust control theory. For quadratic objectives with i.i.d. Gaussian noise, we
give closed-form expressions for RSI using 2x2 Riccati equations, revealing a
Pareto frontier between RSI and convergence rate over stepsize and momentum
choices. We prove a large-deviation principle for time-averaged suboptimality
and show that the rate function is, up to scaling, the convex conjugate of the
RSI. We further connect RSI to the $H_{\infty}$-norm, showing that stronger
worst-case robustness (smaller $H_{\infty}$ norm) yields sharper decay of tail
probabilities. Beyond quadratics, under biased sub-Gaussian gradient errors, we
derive non-asymptotic bounds on a finite-time analogue of the RSI, giving
finite-time high-probability guarantees and large-deviation bounds. We also
observe an analogous trade-off between RSI and convergence-rate bounds for
smooth strongly convex functions. To our knowledge, these are the first
non-asymptotic guarantees and risk-sensitive analysis of GMMs with biased
gradients. Numerical experiments on robust regression illustrate the results.

</details>


### [175] [Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain](https://arxiv.org/abs/2509.14203)
*Shengbo Wang,Nian Si*

Main category: math.OC

TL;DR: 本文分析恒定增益设置，为平均回报鲁棒马尔可夫决策过程构建通用框架，研究含信息不对称的控制问题，分析鲁棒贝尔曼方程，给出解的存在条件。


<details>
  <summary>Details</summary>
Motivation: 现有理论、算法和应用多关注有限时域或折扣模型，平均回报公式在运筹学和管理环境中虽自然但研究不足，动态规划基础技术挑战大且部分未被理解。

Method: 分析恒定增益设置，围绕恒定增益鲁棒贝尔曼方程展开研究，考察解的存在性及其与最优平均回报的关系。

Result: 确定了鲁棒贝尔曼方程的解何时能表征最优平均回报和稳态策略，并给出解存在的充分条件。

Conclusion: 扩展了平均回报鲁棒马尔可夫决策过程的动态规划理论，为运营环境中基于长期平均标准的鲁棒动态决策奠定基础。

Abstract: Learning and optimal control under robust Markov decision processes (MDPs)
have received increasing attention, yet most existing theory, algorithms, and
applications focus on finite-horizon or discounted models. The average-reward
formulation, while natural in many operations research and management contexts,
remains underexplored. This is primarily because the dynamic programming
foundations are technically challenging and only partially understood, with
several fundamental questions remaining open. This paper steps toward a general
framework for average-reward robust MDPs by analyzing the constant-gain
setting. We study the average-reward robust control problem with possible
information asymmetries between the controller and an S-rectangular adversary.
Our analysis centers on the constant-gain robust Bellman equation, examining
both the existence of solutions and their relationship to the optimal average
reward. Specifically, we identify when solutions to the robust Bellman equation
characterize the optimal average reward and stationary policies, and we provide
sufficient conditions ensuring solutions' existence. These findings expand the
dynamic programming theory for average-reward robust MDPs and lay a foundation
for robust dynamic decision making under long-run average criteria in
operational environments.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [176] [Unleashing the power of computational insights in revealing the complexity of biological systems in the new era of spatial multi-omics](https://arxiv.org/abs/2509.13376)
*Zhiwei Fan,Tiangang Wang,Kexin Huang,Binwu Ying,Xiaobo Zhou*

Main category: q-bio.QM

TL;DR: 本文综述空间组学技术进展，介绍机器学习与多组学整合建模应用，展望其在精准医学的未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着空间组学技术发展，需系统了解其在解析哺乳动物组织器官结构和机制的进展。

Method: 对空间多组学技术和计算算法的持续进展进行系统综述。

Result: 展示先进机器学习算法和多组学整合建模可解码复杂生物过程。

Conclusion: 给出空间组学在精准医学中技术创新和建模的未来方向。

Abstract: Recent advances in spatial omics technologies have revolutionized our ability
to study biological systems with unprecedented resolution. By preserving the
spatial context of molecular measurements, these methods enable comprehensive
mapping of cellular heterogeneity, tissue architecture, and dynamic biological
processes in developmental biology, neuroscience, oncology, and evolutionary
studies. This review highlights a systematic overview of the continuous
advancements in both technology and computational algorithms that are paving
the way for a deeper, more systematic comprehension of the structure and
mechanisms of mammalian tissues and organs by using spatial multi-omics. Our
viewpoint demonstrates how advanced machine learning algorithms and multi-omics
integrative modeling can decode complex biological processes, including the
spatial organization and topological relationships of cells during organ
development, as well as key molecular signatures and regulatory networks
underlying tumorigenesis and metastasis. Finally, we outline future directions
for technological innovation and modeling insights of spatial omics in
precision medicine.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [177] [Sample Size Calculations for the Development of Risk Prediction Models that Account for Performance Variability](https://arxiv.org/abs/2509.14028)
*Menelaos Pavlou,Rumana Z. Omar,Gareth Ambler*

Main category: stat.ME

TL;DR: 提出考虑性能变异性的样本量计算方法，通过模拟框架和解析计算实现，发现原方法性能变异性大，新方法需更大样本量，收缩法可改善性能。


<details>
  <summary>Details</summary>
Motivation: 现有临床预测模型样本量计算方法未考虑性能变异性，可能导致获得接近目标性能模型的概率低。

Method: 提出适应方法，以可接受性能概率为目标，通过R包在模拟框架实现，还提出解析计算方法。

Result: 原方法性能变异性随预测因子数量减少而增加，可接受性能概率低，新方法需更大样本量，收缩法可改善可接受性能概率。

Conclusion: 新的样本量计算方法考虑性能变异性，可提高获得接近目标性能模型的概率。

Abstract: Existing approaches to sample size calculations for developing clinical
prediction models have focused on ensuring that the expected value of a chosen
performance measure meets a pre-specified target. For example, to limit
model-overfitting, the sample size is commonly chosen such that the expected
calibration slope (CS) is 0.9, close to 1 for a perfectly calibrated model. In
practice, due to sampling variability, model performance can vary considerably
across different development samples of the recommended size. If this
variability is high, the probability of obtaining a model with performance
close to the target for a given measure may be unacceptably low. To address
this, we propose an adapted approach to sample size calculations that
explicitly incorporates performance variability by targeting the probability of
acceptable performance (PrAP). For example, in the context of calibration, we
may define a model as acceptably calibrated if CS falls in a pre-defined range,
e.g. between 0.85 and 1.15. Then we choose the required sample size to ensure
that PrAP(CS)=80%. For binary outcomes we implemented our approach for CS
within a simulation-based framework via the R package `samplesizedev'.
Additionally, for CS specifically, we have proposed an equivalent analytical
calculation which is computationally efficient. While we focused on CS, the
simulation-based framework is flexible and can be easily extended to
accommodate other performance measures and types of outcomes. When adhering to
existing recommendations, we found that performance variability increased
substantially as the number of predictors, p, decreased. Consequently, PrAP(CS)
was often low. For example, with 5 predictors, PrAP(CS) was around 50%. Our
adapted approach resulted in considerably larger sample sizes, especially for
p<10. Applying shrinkage tends to improve PrAP(CS).

</details>


### [178] [Imputation-Powered Inference](https://arxiv.org/abs/2509.13778)
*Sarah Zhao,Emmanuel Candès*

Main category: stat.ME

TL;DR: 提出IPI框架解决多模态多站点数据块缺失问题，结合黑盒插补灵活性与全观测数据偏差校正，模拟和临床应用显示其优势。


<details>
  <summary>Details</summary>
Motivation: 现代多模态多站点数据存在块缺失问题，现有方法有局限性，如完整案例分析丢弃数据有偏差、双稳健估计器缺乏闭式解且难扩展、黑盒插补指定错误时无推断保证。

Method: 提出插补驱动推断（IPI）框架，结合黑盒插补灵活性和全观测数据偏差校正，借鉴预测驱动推断和半参数推断思想。

Result: 模拟研究和临床应用表明，IPI在完全随机缺失下能进行有效和高效的M估计，在较弱假设下改善子总体推断，相对完整案例分析可大幅提高子总体效率，在双稳健估计器和简单插补失败时保持统计有效性。

Conclusion: IPI框架能有效解决多模态多站点数据块缺失问题，具有较好的统计性能和应用价值。

Abstract: Modern multi-modal and multi-site data frequently suffer from blockwise
missingness, where subsets of features are missing for groups of individuals,
creating complex patterns that challenge standard inference methods. Existing
approaches have critical limitations: complete-case analysis discards
informative data and is potentially biased; doubly robust estimators for
non-monotone missingness-where the missingness patterns are not nested subsets
of one another-can be theoretically efficient but lack closed-form solutions
and often fail to scale; and blackbox imputation can leverage partially
observed data to improve efficiency but provides no inferential guarantees when
misspecified. To address the limitations of these existing methods, we propose
imputation-powered inference (IPI), a model-lean framework that combines the
flexibility of blackbox imputation with bias correction using fully observed
data, drawing on ideas from prediction-powered inference and semiparametric
inference. IPI enables valid and efficient M-estimation under missing
completely at random (MCAR) blockwise missingness and improves subpopulation
inference under a weaker assumption we formalize as first-moment MCAR, for
which we also provide practical diagnostics. Simulation studies and a clinical
application demonstrate that IPI may substantially improve subpopulation
efficiency relative to complete-case analysis, while maintaining statistical
validity in settings where both doubly robust estimators and naive imputation
fail to achieve nominal coverage.

</details>


### [179] [Adaptive Off-Policy Inference for M-Estimators Under Model Misspecification](https://arxiv.org/abs/2509.14218)
*James Leiner,Robin Dunn,Aaditya Ramdas*

Main category: stat.ME

TL;DR: 提出一种方法，为使用自适应收集的老虎机数据的M - 估计量在工作模型可能错误设定时提供有效推断。


<details>
  <summary>Details</summary>
Motivation: 经典统计方法在自适应收集数据时难以实现渐近正态性，现有改进工作大多假设模型正确设定，需要解决模型错误设定时的有效推断问题。

Method: 使用灵活的机器学习方法来稳定自适应数据收集引起的方差。

Result: 在骨关节炎倡议构建的半合成数据集上的实证结果表明，该方法能控制一类错误，而现有自适应设置下的推断方法在模型错误设定时无法覆盖。

Conclusion: 所提方法能为使用自适应收集的老虎机数据的M - 估计量在模型可能错误设定时提供有效推断，在处理策略不稳定和不收敛的情况下也能构建有效置信集。

Abstract: When data are collected adaptively, such as in bandit algorithms, classical
statistical approaches such as ordinary least squares and $M$-estimation will
often fail to achieve asymptotic normality. Although recent lines of work have
modified the classical approaches to ensure valid inference on adaptively
collected data, most of these works assume that the model is correctly
specified. We propose a method that provides valid inference for M-estimators
that use adaptively collected bandit data with a (possibly) misspecified
working model. A key ingredient in our approach is the use of flexible machine
learning approaches to stabilize the variance induced by adaptive data
collection. A major novelty is that our procedure enables the construction of
valid confidence sets even in settings where treatment policies are unstable
and non-converging, such as when there is no unique optimal arm and standard
bandit algorithms are used. Empirical results on semi-synthetic datasets
constructed from the Osteoarthritis Initiative demonstrate that the method
maintains type I error control, while existing methods for inference in
adaptive settings do not cover in the misspecified case.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [180] [Combining Evidence and Reasoning for Biomedical Fact-Checking](https://arxiv.org/abs/2509.13879)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: 提出用于生物医学事实核查的CER框架，在数据集上表现优异并开源代码数据。


<details>
  <summary>Details</summary>
Motivation: 医疗领域错误信息危害大，生物医学声明验证因专业术语、领域知识和科学证据要求面临挑战。

Method: 引入CER框架，整合科学证据检索、大语言模型推理和监督真实性预测，结合大语言模型文本生成能力与先进检索技术。

Result: 在专家标注数据集上展现了最先进性能和良好的跨数据集泛化能力。

Conclusion: CER框架有效降低幻觉风险，确保输出基于可验证的证据源，具有较好效果。

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https: //github.com/PRAISELab-PicusLab/CER.

</details>


### [181] [Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification](https://arxiv.org/abs/2509.13888)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: 提出用于生物医学事实核查的CER框架，结合证据检索、大语言模型推理和监督真实性预测，在数据集上表现良好并开源。


<details>
  <summary>Details</summary>
Motivation: 医疗领域错误信息危害大，现有自动化事实核查方法在验证生物医学声明时面临挑战。

Method: 引入CER框架，整合科学证据检索、大语言模型推理和监督真实性预测，结合大语言模型文本生成能力与先进检索技术。

Result: 在专家标注数据集上展示了最先进的性能和有前景的跨数据集泛化能力。

Conclusion: CER框架能有效缓解幻觉风险，确保输出基于可验证的证据源，具有良好性能和泛化能力，代码和数据已开源。

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https://github.com/PRAISELab-PicusLab/CER

</details>


### [182] [Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs](https://arxiv.org/abs/2509.13664)
*Zhuoxuan Zhang,Jinhao Duan,Edward Kim,Kaidi Xu*

Main category: cs.CL

TL;DR: 研究发现大语言模型内部表征线性编码问题歧义，可在神经元层面检测和控制，操纵相关神经元能改变模型行为。


<details>
  <summary>Details</summary>
Motivation: 现实世界问题普遍存在歧义，但大语言模型常直接给出答案而不寻求澄清，需研究模型对歧义的处理。

Method: 在模型预填充阶段识别编码问题歧义信息的少量神经元（AENs），训练基于AENs的探测器，进行层分析。

Result: 基于AENs的探测器在歧义检测上表现出色，能跨数据集泛化，优于基线方法；AENs来自浅层，可通过操纵AENs改变模型行为。

Conclusion: 大语言模型对问题歧义形成紧凑内部表征，实现可解释和可控制的行为。

Abstract: Ambiguity is pervasive in real-world questions, yet large language models
(LLMs) often respond with confident answers rather than seeking clarification.
In this work, we show that question ambiguity is linearly encoded in the
internal representations of LLMs and can be both detected and controlled at the
neuron level. During the model's pre-filling stage, we identify that a small
number of neurons, as few as one, encode question ambiguity information. Probes
trained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance
on ambiguity detection and generalize across datasets, outperforming
prompting-based and representation-based baselines. Layerwise analysis reveals
that AENs emerge from shallow layers, suggesting early encoding of ambiguity
signals in the model's processing pipeline. Finally, we show that through
manipulating AENs, we can control LLM's behavior from direct answering to
abstention. Our findings reveal that LLMs form compact internal representations
of question ambiguity, enabling interpretable and controllable behavior.

</details>


### [183] [CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction](https://arxiv.org/abs/2509.13672)
*Shang Qin,Jingheng Ye,Yinghui Li,Hai-Tao Zheng,Qi Li,Jinxiao Shan,Zhixing Li,Hong-Gee Kim*

Main category: cs.CL

TL;DR: 本文引入首个中文文学语法纠错的持续学习基准CL$^2$GEC，评估跨多学科领域的自适应中文语法纠错系统，实验表明基于正则化的方法在缓解遗忘方面更有效。


<details>
  <summary>Details</summary>
Motivation: 现有中文语法纠错研究缺乏多学科写作的专用基准，忽略持续学习处理特定领域语言变体和防止灾难性遗忘的潜力。

Method: 引入CL$^2$GEC基准，包含10个学科的10000个人工标注句子，对大语言模型进行顺序调优、参数高效适配和4种代表性持续学习算法评估，使用标准GEC指标和适应任务级变化的持续学习指标。

Result: 基于正则化的方法比基于重放或朴素顺序的方法更能有效缓解遗忘。

Conclusion: 该基准为跨不同学术领域的自适应语法纠错未来研究提供了坚实基础。

Abstract: The growing demand for automated writing assistance in diverse academic
domains highlights the need for robust Chinese Grammatical Error Correction
(CGEC) systems that can adapt across disciplines. However, existing CGEC
research largely lacks dedicated benchmarks for multi-disciplinary academic
writing, overlooking continual learning (CL) as a promising solution to handle
domain-specific linguistic variation and prevent catastrophic forgetting. To
fill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning
benchmark for Chinese Literature Grammatical Error Correction, designed to
evaluate adaptive CGEC across multiple academic fields. Our benchmark includes
10,000 human-annotated sentences spanning 10 disciplines, each exhibiting
distinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating
grammatical error correction in a continual learning setting, simulating
sequential exposure to diverse academic disciplines to reflect real-world
editorial dynamics. We evaluate large language models under sequential tuning,
parameter-efficient adaptation, and four representative CL algorithms, using
both standard GEC metrics and continual learning metrics adapted to task-level
variation. Experimental results reveal that regularization-based methods
mitigate forgetting more effectively than replay-based or naive sequential
approaches. Our benchmark provides a rigorous foundation for future research in
adaptive grammatical error correction across diverse academic domains.

</details>


### [184] [AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation](https://arxiv.org/abs/2509.13677)
*Xinxu Zhou,Jiaqi Bai,Zhenqi Sun,Fanxiang Zeng,Yue Liu*

Main category: cs.CL

TL;DR: 本文提出AgentCTG框架解决可控文本生成（CTG）挑战，在多数据集达SOTA，还提出新任务验证其在实际应用有效性。


<details>
  <summary>Details</summary>
Motivation: NLP中CTG在实现细粒度条件控制及实际应用中面临诸多挑战。

Method: 引入AgentCTG框架，模拟多智能体工作流中的控制和调节机制，探索不同智能体协作方法，引入自动提示模块。

Result: 在多个公共数据集上取得了最先进的结果，在新提出的字符驱动重写任务和在线导航角色扮演应用中展现出有效性。

Conclusion: AgentCTG框架有效提升了文本生成的精确和复杂控制能力，优化相关文本生成，增强了在线社区交互体验和用户参与度。

Abstract: Although significant progress has been made in many tasks within the field of
Natural Language Processing (NLP), Controlled Text Generation (CTG) continues
to face numerous challenges, particularly in achieving fine-grained conditional
control over generation. Additionally, in real scenario and online
applications, cost considerations, scalability, domain knowledge learning and
more precise control are required, presenting more challenge for CTG. This
paper introduces a novel and scalable framework, AgentCTG, which aims to
enhance precise and complex control over the text generation by simulating the
control and regulation mechanisms in multi-agent workflows. We explore various
collaboration methods among different agents and introduce an auto-prompt
module to further enhance the generation effectiveness. AgentCTG achieves
state-of-the-art results on multiple public datasets. To validate its
effectiveness in practical applications, we propose a new challenging
Character-Driven Rewriting task, which aims to convert the original text into
new text that conform to specific character profiles and simultaneously
preserve the domain knowledge. When applied to online navigation with
role-playing, our approach significantly enhances the driving experience
through improved content delivery. By optimizing the generation of contextually
relevant text, we enable a more immersive interaction within online
communities, fostering greater personalization and user engagement.

</details>


### [185] [Improving Context Fidelity via Native Retrieval-Augmented Reasoning](https://arxiv.org/abs/2509.13683)
*Suyuchen Wang,Jinlin Wang,Xinyu Wang,Shiqi Li,Xiangru Tang,Sirui Hong,Xiao-Wen Chang,Chenglin Wu,Bang Liu*

Main category: cs.CL

TL;DR: 提出CARE框架解决大语言模型上下文保真度问题，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在上下文保真度方面存在问题，现有方法有局限性。

Method: 提出CARE框架，让大语言模型利用自身检索能力在推理过程中整合上下文证据。

Result: 在多个问答基准测试中，该方法显著优于监督微调、传统检索增强生成方法和外部检索解决方案。

Conclusion: 该工作在使大语言模型更准确、可靠和高效地处理知识密集型任务方面取得了根本性进展。

Abstract: Large language models (LLMs) often struggle with context fidelity, producing
inconsistent answers when responding to questions based on provided
information. Existing approaches either rely on expensive supervised
fine-tuning to generate evidence post-answer or train models to perform web
searches without necessarily improving utilization of the given context. We
propose CARE, a novel native retrieval-augmented reasoning framework that
teaches LLMs to explicitly integrate in-context evidence within their reasoning
process with the model's own retrieval capabilities. Our method requires
limited labeled evidence data while significantly enhancing both retrieval
accuracy and answer generation performance through strategically retrieved
in-context tokens in the reasoning chain. Extensive experiments on multiple
real-world and counterfactual QA benchmarks demonstrate that our approach
substantially outperforms supervised fine-tuning, traditional
retrieval-augmented generation methods, and external retrieval solutions. This
work represents a fundamental advancement in making LLMs more accurate,
reliable, and efficient for knowledge-intensive tasks.

</details>


### [186] [DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models](https://arxiv.org/abs/2509.13702)
*Xiao Zheng*

Main category: cs.CL

TL;DR: 提出DSCC - HS框架抑制大语言模型幻觉，实验表明该框架达SOTA性能，提升模型事实性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型幻觉是可靠部署的重大障碍，现有方法常是被动的。

Method: 引入DSCC - HS框架，受双过程认知理论启发，使用紧凑代理模型FAP和HDP，推理时在每个解码步骤注入实时引导向量。

Result: 在TruthfulQA和BioGEN上实验达SOTA性能，如TruthfulQA上FCR达99.2%，BioGEN上FActScore达46.50。

Conclusion: DSCC - HS是提升大语言模型事实性的有效方案。

Abstract: Large Language Model (LLM) hallucination is a significant barrier to their
reliable deployment. Current methods like Retrieval-Augmented Generation (RAG)
are often reactive. We introduce **Dynamic Self-reinforcing Calibration for
Hallucination Suppression (DSCC-HS)**, a novel, proactive framework that
intervenes during autoregressive decoding. Inspired by dual-process cognitive
theory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a
Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During
inference, these proxies dynamically steer a large target model by injecting a
real-time steering vector, which is the difference between FAP and HDP logits,
at each decoding step. This plug-and-play approach requires no modification to
the target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS
achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%
Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained
the highest FActScore of 46.50. These results validate DSCC-HS as a principled
and efficient solution for enhancing LLM factuality.

</details>


### [187] [Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models](https://arxiv.org/abs/2509.13706)
*Peter Beidler,Mark Nguyen,Kevin Lybarger,Ola Holmberg,Eric Ford,John Kang*

Main category: cs.CL

TL;DR: 本文提出用于检测放疗高严重度事件报告的NLP筛选工具，经不同数据集训练和评估模型，部分模型表现接近人类水平。


<details>
  <summary>Details</summary>
Motivation: 医疗事件报告手动审查耗时且需专业知识，故开发NLP筛选工具检测放疗高严重度事件报告。

Method: 使用两个文本数据集训练和评估SVM和BlueBERT两种模型，通过两种方式评估模型泛化性，还分析手动编辑子集。

Result: 在本机构测试集上SVM和BlueBERT的AUROC分别为0.82和0.81；无跨机构迁移学习时，SF测试集上SVM和BlueBERT的AUROC分别为0.42和0.56；BlueBERT_TRANSFER在SF测试集上AUROC达0.78；SVM和BlueBERT_TRANSFER在手动筛选报告上的表现与人类相近。

Conclusion: 成功开发跨机构NLP模型，在筛选数据集上检测高严重度报告能力与人类相近。

Abstract: PURPOSE: Incident reports are an important tool for safety and quality
improvement in healthcare, but manual review is time-consuming and requires
subject matter expertise. Here we present a natural language processing (NLP)
screening tool to detect high-severity incident reports in radiation oncology
across two institutions.
  METHODS AND MATERIALS: We used two text datasets to train and evaluate our
NLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA
SAFRON (SF), all of which had severity scores labeled by clinical content
experts. We trained and evaluated two types of models: baseline support vector
machines (SVM) and BlueBERT which is a large language model pretrained on
PubMed abstracts and hospitalized patient data. We assessed for
generalizability of our model in two ways. First, we evaluated models trained
using Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that
was first fine-tuned on Inst.-train then on SF-train before testing on SF-test
set. To further analyze model performance, we also examined a subset of 59
reports from our Inst. dataset, which were manually edited for clarity.
  RESULTS Classification performance on the Inst. test achieved AUROC 0.82
using SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,
performance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56
using BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,
improved the performance on SF test to AUROC 0.78. Performance of SVM, and
BlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and
0.74) was similar to human performance (AUROC 0.81).
  CONCLUSION: In summary, we successfully developed cross-institution NLP
models on incident report text from radiation oncology centers. These models
were able to detect high-severity reports similarly to humans on a curated
dataset.

</details>


### [188] [Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications](https://arxiv.org/abs/2509.13775)
*Vani Kanjirangat,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: 本文探索不同的数据高效和参数高效方法用于阿拉伯语方言识别，测试多种策略，发现大语言模型在少样本或零样本下表现不佳，软提示编码器变体表现较好，基于LoRA的微调模型最佳。


<details>
  <summary>Details</summary>
Motivation: 探索不同的数据高效和参数高效方法进行阿拉伯语方言识别。

Method: 研究多种软提示策略和LoRA重参数化；分析大语言模型在零样本和少样本推理下的硬提示；在多个主要数据集上使用阿拉伯语特定编码器模型进行实验；分析开源仅解码器模型、通用多语言模型和阿拉伯语特定模型的n样本推理。

Result: 大语言模型在少样本或零样本设置下难以区分方言细微差别；软提示编码器变体表现较好；基于LoRA的微调模型表现最佳，甚至超过全量微调。

Conclusion: 基于LoRA的微调模型在阿拉伯语方言识别中效果最好。

Abstract: This paper discusses our exploration of different data-efficient and
parameter-efficient approaches to Arabic Dialect Identification (ADI). In
particular, we investigate various soft-prompting strategies, including
prefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA
reparameterizations. For the data-efficient strategy, we analyze hard prompting
with zero-shot and few-shot inferences to analyze the dialect identification
capabilities of Large Language Models (LLMs). For the parameter-efficient PEFT
approaches, we conducted our experiments using Arabic-specific encoder models
on several major datasets. We also analyzed the n-shot inferences on
open-source decoder-only models, a general multilingual model (Phi-3.5), and an
Arabic-specific one(SILMA). We observed that the LLMs generally struggle to
differentiate the dialectal nuances in the few-shot or zero-shot setups. The
soft-prompted encoder variants perform better, while the LoRA-based fine-tuned
models perform best, even surpassing full fine-tuning.

</details>


### [189] [Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning](https://arxiv.org/abs/2509.13790)
*Yangning Li,Tingwei Lu,Yinghui Li,Yankai Chen,Wei-Chieh Huang,Wenhao Jiang,Hui Wang,Hai-Tao Zheng,Philip S. Yu*

Main category: cs.CL

TL;DR: 提出CAMPUS框架解决当前课程调优方法的课程刚性问题，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 当前课程调优方法依赖静态启发式难度指标，存在课程刚性问题，无法适应模型训练过程中不断变化的能力。

Method: 提出Competence - Aware Multi - Perspective cUrriculum inStruction tuning框架（CAMPUS），包括子课程动态选择、基于能力的课程进度调整和多难度调度。

Result: 广泛实验表明，与其他高效指令调优的先进基线相比，CAMPUS表现更优。

Conclusion: CAMPUS框架能有效解决当前课程调优方法的问题，提升高效指令调优的性能。

Abstract: Efficient instruction tuning aims to enhance the ultimate performance of
large language models (LLMs) trained on a given instruction dataset. Curriculum
learning as a typical data organization strategy has shown preliminary
effectiveness in instruction tuning. However, current curriculum tuning methods
suffer from the curriculum rigidity, since they rely solely on static heuristic
difficulty metrics. These methods fail to adapt to the evolving capabilities of
models during training, resulting in a fixed and potentially sub-optimal
learning trajectory. To address the issue, Competence-Aware Multi-Perspective
cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS
offers several advantages: (1) Dynamic selection for sub-curriculum. (2)
Competency-aware adjustment to the curriculum schedule. (3) Multiple
difficulty-based scheduling. Extensive experiments prove the superior
performance of CAMPUS, compared to other state-of-the-art baselines for
efficient instruction tuning.

</details>


### [190] [Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning](https://arxiv.org/abs/2509.13624)
*Shambhavi Krishna,Atharva Naik,Chaitali Agarwal,Sudharshan Govindan,Taesung Lee,Haw-Shiuan Chang*

Main category: cs.CL

TL;DR: 本文提出分析框架研究大语言模型跨任务迁移学习，发现隐藏统计因素和语言特征更影响性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用中难以获取所有任务高质量训练数据，需依赖迁移学习，因此要分析跨任务交互。

Method: 构建迁移学习矩阵和降维的分析框架，训练并分析10个模型。

Result: 性能提升难以用表面数据集相似性或源数据质量解释，源数据集隐藏统计因素和特定语言特征更具影响力。

Conclusion: 本研究为大语言模型更可预测和有效的适配提供思路。

Abstract: Large language models are increasingly deployed across diverse applications.
This often includes tasks LLMs have not encountered during training. This
implies that enumerating and obtaining the high-quality training data for all
tasks is infeasible. Thus, we often need to rely on transfer learning using
datasets with different characteristics, and anticipate out-of-distribution
requests. Motivated by this practical need, we propose an analysis framework,
building a transfer learning matrix and dimensionality reduction, to dissect
these cross-task interactions. We train and analyze 10 models to identify
latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)
and discover the side effects of the transfer learning. Our findings reveal
that performance improvements often defy explanations based on surface-level
dataset similarity or source data quality. Instead, hidden statistical factors
of the source dataset, such as class distribution and generation length
proclivities, alongside specific linguistic features, are actually more
influential. This work offers insights into the complex dynamics of transfer
learning, paving the way for more predictable and effective LLM adaptation.

</details>


### [191] [Do Large Language Models Understand Word Senses?](https://arxiv.org/abs/2509.13905)
*Domenico Meconi,Simone Stirpe,Federico Martelli,Leonardo Lavalle,Roberto Navigli*

Main category: cs.CL

TL;DR: 评估指令微调大语言模型的词义消歧能力及在三种生成场景下理解词义的能力，发现领先模型在词义消歧任务中表现与专业系统相当，生成任务准确率高。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型是否真正掌握词义的研究不足，需评估其词义理解能力。

Method: 评估指令微调大语言模型的词义消歧能力并与专业系统对比，评估两个顶尖开源和闭源大语言模型在三种生成场景下理解词义的能力。

Result: 在词义消歧任务中，GPT - 4o和DeepSeek - V3等领先模型表现与专业系统相当且更具鲁棒性；生成任务中，大语言模型解释词义的准确率达98%，自由形式解释任务表现最佳。

Conclusion: 大语言模型在词义消歧和词义理解生成任务中表现良好。

Abstract: Understanding the meaning of words in context is a fundamental capability for
Large Language Models (LLMs). Despite extensive evaluation efforts, the extent
to which LLMs show evidence that they truly grasp word senses remains
underexplored. In this paper, we address this gap by evaluating both i) the
Word Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,
comparing their performance to state-of-the-art systems specifically designed
for the task, and ii) the ability of two top-performing open- and closed-source
LLMs to understand word senses in three generative settings: definition
generation, free-form explanation, and example generation. Notably, we find
that, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve
performance on par with specialized WSD systems, while also demonstrating
greater robustness across domains and levels of difficulty. In the generation
tasks, results reveal that LLMs can explain the meaning of words in context up
to 98\% accuracy, with the highest performance observed in the free-form
explanation task, which best aligns with their generative capabilities.

</details>


### [192] [Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency](https://arxiv.org/abs/2509.13990)
*Colin Hong,Xu Guo,Anand Chaanan Singh,Esha Choukse,Dmitrii Ustiugov*

Main category: cs.CL

TL;DR: 文章分析Self - Consistency (SC)的低效性，提出Slim - SC策略，实验表明其可减少推理延迟和KVC使用，同时保持或提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有Self - Consistency (SC)技术虽能提升LLM推理性能，但计算开销大，先前加速方法缺乏实证支持，需改进。

Method: 理论和实证分析SC的低效性，提出Slim - SC，利用思想层面的链间相似度逐步修剪冗余推理链。

Result: 在三个STEM推理数据集和两个LLM架构上实验，Slim - SC结合R1 - Distill可将推理延迟和KVC使用分别最多降低45%和26%，并保持或提升准确率。

Conclusion: Slim - SC是一种简单高效的TTS替代方案，可解决SC的计算开销问题。

Abstract: Recently, Test-Time Scaling (TTS) has gained increasing attention for
improving LLM reasoning performance at test time without retraining the model.
A notable TTS technique is Self-Consistency (SC), which generates multiple
reasoning chains in parallel and selects the final answer via majority voting.
While effective, the order-of-magnitude computational overhead limits its broad
deployment. Prior attempts to accelerate SC mainly rely on model-based
confidence scores or heuristics with limited empirical support. For the first
time, we theoretically and empirically analyze the inefficiencies of SC and
reveal actionable opportunities for improvement. Building on these insights, we
propose Slim-SC, a step-wise pruning strategy that identifies and removes
redundant chains using inter-chain similarity at the thought level. Experiments
on three STEM reasoning datasets and two recent LLM architectures show that
Slim-SC reduces inference latency and KVC usage by up to 45% and 26%,
respectively, with R1-Distill, while maintaining or improving accuracy, thus
offering a simple yet efficient TTS alternative for SC.

</details>


### [193] [Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale](https://arxiv.org/abs/2509.14008)
*Hasan Abed Al Kader Hammoud,Mohammad Zbeeb,Bernard Ghanem*

Main category: cs.CL

TL;DR: 本文介绍了以阿拉伯语为中心的指令和翻译模型Hala，通过特定流程构建，在阿拉伯语基准测试中取得SOTA成果并开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 推动阿拉伯语自然语言处理研究，构建以阿拉伯语为中心的指令和翻译模型。

Method: 使用translate - and - tune管道，将强AR↔EN教师模型压缩到FP8创建双语监督，微调轻量级语言模型LFM2 - 1.2B翻译英语指令集，训练不同参数规模的Hala模型并应用slerp合并。

Result: Hala在阿拉伯语基准测试的“nano”（≤2B）和“small”（7 - 9B）类别中取得了SOTA成果，超越了其基础模型。

Conclusion: 发布模型、数据、评估和配方以加速阿拉伯语NLP研究。

Abstract: We present Hala, a family of Arabic-centric instruction and translation
models built with our translate-and-tune pipeline. We first compress a strong
AR$\leftrightarrow$EN teacher to FP8 (yielding $\sim$2$\times$ higher
throughput with no quality loss) and use it to create high-fidelity bilingual
supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this
data and used to translate high-quality English instruction sets into Arabic,
producing a million-scale corpus tailored to instruction following. We train
Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to
balance Arabic specialization with base-model strengths. On Arabic-centric
benchmarks, Hala achieves state-of-the-art results within both the "nano"
($\leq$2B) and "small" (7-9B) categories, outperforming their bases. We release
models, data, evaluation, and recipes to accelerate research in Arabic NLP.

</details>


### [194] [Long-context Reference-based MT Quality Estimation](https://arxiv.org/abs/2509.13980)
*Sami Ul Haq,Chinonso Cynthia Osuji,Sheila Castilho,Brian Davis*

Main category: cs.CL

TL;DR: 本文介绍WMT25自动翻译质量评估共享任务的提交系统，基于COMET框架，用增强长上下文数据训练预测ESA分数，实验表明长上下文信息提升与人工判断的相关性。


<details>
  <summary>Details</summary>
Motivation: 参与WMT25自动翻译质量评估共享任务，提高翻译质量评估效果。

Method: 基于COMET框架，用长上下文数据训练预测ESA分数，构建训练数据时拼接句并计算分数加权平均，整合多个人工判断数据集并归一化训练多语言回归模型。

Result: 包含长上下文信息的模型与人工判断的相关性比仅在短片段上训练的模型更高。

Conclusion: 使用长上下文信息能有效提升翻译质量评估模型与人工判断的相关性。

Abstract: In this paper, we present our submission to the Tenth Conference on Machine
Translation (WMT25) Shared Task on Automated Translation Quality Evaluation.
  Our systems are built upon the COMET framework and trained to predict
segment-level Error Span Annotation (ESA) scores using augmented long-context
data.
  To construct long-context training data, we concatenate in-domain,
human-annotated sentences and compute a weighted average of their scores.
  We integrate multiple human judgment datasets (MQM, SQM, and DA) by
normalising their scales and train multilingual regression models to predict
quality scores from the source, hypothesis, and reference translations.
  Experimental results show that incorporating long-context information
improves correlations with human judgments compared to models trained only on
short segments.

</details>


### [195] [You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models](https://arxiv.org/abs/2509.14031)
*Paweł Mąka,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 本文验证训练数据稀疏是上下文利用难的原因，揭示不同上下文现象改进难泛化，提出并评估两种训练策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决实现人类水平翻译时利用上下文难的问题，探究标准训练数据中上下文丰富示例稀疏是否为原因。

Method: 构建含不同比例上下文相关示例的训练数据集，验证数据稀疏与模型性能关联；提出并评估两种训练策略。

Result: 证实训练数据稀疏是关键瓶颈，不同上下文现象改进难泛化，跨语言转移不显著；两种策略分别在单语和多语环境下使ctxPro评估准确率提升6和8个百分点。

Conclusion: 训练数据稀疏是上下文利用难的关键因素，提出的两种训练策略能有效提升上下文利用和翻译准确性。

Abstract: Achieving human-level translations requires leveraging context to ensure
coherence and handle complex phenomena like pronoun disambiguation. Sparsity of
contextually rich examples in the standard training data has been hypothesized
as the reason for the difficulty of context utilization. In this work, we
systematically validate this claim in both single- and multilingual settings by
constructing training datasets with a controlled proportions of contextually
relevant examples. We demonstrate a strong association between training data
sparsity and model performance confirming sparsity as a key bottleneck.
Importantly, we reveal that improvements in one contextual phenomenon do no
generalize to others. While we observe some cross-lingual transfer, it is not
significantly higher between languages within the same sub-family. Finally, we
propose and empirically evaluate two training strategies designed to leverage
the available data. These strategies improve context utilization, resulting in
accuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in
single- and multilingual settings respectively.

</details>


### [196] [SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation](https://arxiv.org/abs/2509.14036)
*Zekang Liu,Wei Feng,Fanhua Shang,Lianyu Hu,Jichao Feng,Liqing Gao*

Main category: cs.CL

TL;DR: 提出基于问题的手语翻译（QB - SLT）新任务及跨模态自监督学习与Sigmoid自注意力加权（SSL - SSAW）融合方法，在新数据集上获SOTA性能，表明对话对提升翻译质量有效。


<details>
  <summary>Details</summary>
Motivation: 手语翻译中对话能提供关键上下文线索辅助翻译，现有研究缺乏对对话高效集成的探索，且对话比手语转录注释更易标注。

Method: 提出SSL - SSAW融合方法，用对比学习对齐多模态特征，引入SSAW模块自适应提取特征，通过自监督学习利用问题文本提升能力。

Result: 在CSL - Daily - QA和PHOENIX - 2014T - QA数据集上，SSL - SSAW取得SOTA性能，问题辅助效果可超手语转录注释辅助。

Conclusion: 对话对提升手语翻译质量有效，所提方法可行且效果好。

Abstract: Sign Language Translation (SLT) bridges the communication gap between deaf
people and hearing people, where dialogue provides crucial contextual cues to
aid in translation. Building on this foundational concept, this paper proposes
Question-based Sign Language Translation (QB-SLT), a novel task that explores
the efficient integration of dialogue. Unlike gloss (sign language
transcription) annotations, dialogue naturally occurs in communication and is
easier to annotate. The key challenge lies in aligning multimodality features
while leveraging the context of the question to improve translation. To address
this issue, we propose a cross-modality Self-supervised Learning with Sigmoid
Self-attention Weighting (SSL-SSAW) fusion method for sign language
translation. Specifically, we employ contrastive learning to align
multimodality features in QB-SLT, then introduce a Sigmoid Self-attention
Weighting (SSAW) module for adaptive feature extraction from question and sign
language sequences. Additionally, we leverage available question text through
self-supervised learning to enhance representation and translation
capabilities. We evaluated our approach on newly constructed CSL-Daily-QA and
PHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,
easily accessible question assistance can achieve or even surpass the
performance of gloss assistance. Furthermore, visualization results demonstrate
the effectiveness of incorporating dialogue in improving translation quality.

</details>


### [197] [Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs](https://arxiv.org/abs/2509.14180)
*Akhil Theerthala*

Main category: cs.CL

TL;DR: 本文提出新框架构建监督数据微调Qwen - 3 - 8B模型，该模型成本低且性能与大参数模型相当。


<details>
  <summary>Details</summary>
Motivation: 过往LLM工作集中在投资者和金融规划师支持系统，现有个人理财任务代理管道维护成本高、收益低，需要改进。

Method: 引入整合金融背景和行为金融研究的框架构建监督数据，创建19k样本推理数据集，在数据集上对Qwen - 3 - 8B模型进行全面微调。

Result: 通过保留测试和盲测，8B模型在事实准确性、流畅性和个性化指标上表现与14 - 32B参数的大模型相当，成本降低80%。

Conclusion: 通过精心的数据管理和行为整合，小参数模型可在低得多的成本下达到大模型性能。

Abstract: Personalized financial advice requires consideration of user goals,
constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on
support systems for investors and financial planners. Simultaneously, numerous
recent studies examine broader personal finance tasks, including budgeting,
debt management, retirement, and estate planning, through agentic pipelines
that incur high maintenance costs, yielding less than 25% of their expected
financial returns. In this study, we introduce a novel and reproducible
framework that integrates relevant financial context with behavioral finance
studies to construct supervision data for end-to-end advisors. Using this
framework, we create a 19k sample reasoning dataset and conduct a comprehensive
fine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test
split and a blind LLM-jury study, we demonstrate that through careful data
curation and behavioral integration, our 8B model achieves performance
comparable to significantly larger baselines (14-32B parameters) across factual
accuracy, fluency, and personalization metrics while incurring 80% lower costs
than the larger counterparts.

</details>


### [198] [Apertus: Democratizing Open and Compliant LLMs for Global Language Environments](https://arxiv.org/abs/2509.14233)
*Alejandro Hernández-Cano,Alexander Hägele,Allen Hao Huang,Angelika Romanou,Antoni-Joan Solergibert,Barna Pasztor,Bettina Messmer,Dhia Garbaya,Eduard Frank Ďurech,Ido Hakimi,Juan García Giraldo,Mete Ismayilzada,Negar Foroutan,Skander Moalla,Tiancheng Chen,Vinko Sabolčec,Yixuan Xu,Michael Aerni,Badr AlKhamissi,Ines Altemir Marinas,Mohammad Hossein Amani,Matin Ansaripour,Ilia Badanin,Harold Benoit,Emanuela Boros,Nicholas Browning,Fabian Bösch,Maximilian Böther,Niklas Canova,Camille Challier,Clement Charmillot,Jonathan Coles,Jan Deriu,Arnout Devos,Lukas Drescher,Daniil Dzenhaliou,Maud Ehrmann,Dongyang Fan,Simin Fan,Silin Gao,Miguel Gila,María Grandury,Diba Hashemi,Alexander Hoyle,Jiaming Jiang,Mark Klein,Andrei Kucharavy,Anastasiia Kucherenko,Frederike Lübeck,Roman Machacek,Theofilos Manitaras,Andreas Marfurt,Kyle Matoba,Simon Matrenok,Henrique Mendoncça,Fawzi Roberto Mohamed,Syrielle Montariol,Luca Mouchel,Sven Najem-Meyer,Jingwei Ni,Gennaro Oliva,Matteo Pagliardini,Elia Palme,Andrei Panferov,Léo Paoletti,Marco Passerini,Ivan Pavlov,Auguste Poiroux,Kaustubh Ponkshe,Nathan Ranchin,Javi Rando,Mathieu Sauser,Jakhongir Saydaliev,Muhammad Ali Sayfiddinov,Marian Schneider,Stefano Schuppli,Marco Scialanga,Andrei Semenov,Kumar Shridhar,Raghav Singhal,Anna Sotnikova,Alexander Sternfeld,Ayush Kumar Tarun,Paul Teiletche,Jannis Vamvas,Xiaozhe Yao,Hao Zhao Alexander Ilic,Ana Klimovic,Andreas Krause,Caglar Gulcehre,David Rosenthal,Elliott Ash,Florian Tramèr,Joost VandeVondele,Livio Veraldi,Martin Rajman,Thomas Schulthess,Torsten Hoefler,Antoine Bosselut,Martin Jaggi,Imanol Schlag*

Main category: cs.CL

TL;DR: 介绍了全开源大语言模型套件Apertus，解决数据合规和多语言表示问题，在多语言基准测试中接近最优，还开源开发周期的所有科学制品。


<details>
  <summary>Details</summary>
Motivation: 解决当前开源模型生态系统中数据合规和多语言表示的两个系统性缺陷。

Method: 仅在公开可用数据上预训练，追溯尊重robots.txt排除规则并过滤不良内容；预训练时采用Goldfish目标抑制数据逐字回忆；在1800多种语言的15T令牌上训练，约40%预训练数据为非英语内容。

Result: Apertus在多语言基准测试中接近全开源模型的最优结果，与或超越开源权重的同类模型。

Conclusion: 发布Apertus模型权重及开发周期的所有科学制品，便于透明审计和扩展。

Abstract: We present Apertus, a fully open suite of large language models (LLMs)
designed to address two systemic shortcomings in today's open model ecosystem:
data compliance and multilingual representation. Unlike many prior models that
release weights without reproducible data pipelines or regard for content-owner
rights, Apertus models are pretrained exclusively on openly available data,
retroactively respecting robots.txt exclusions and filtering for
non-permissive, toxic, and personally identifiable content. To mitigate risks
of memorization, we adopt the Goldfish objective during pretraining, strongly
suppressing verbatim recall of data while retaining downstream task
performance. The Apertus models also expand multilingual coverage, training on
15T tokens from over 1800 languages, with ~40% of pretraining data allocated to
non-English content. Released at 8B and 70B scales, Apertus approaches
state-of-the-art results among fully open models on multilingual benchmarks,
rivalling or surpassing open-weight counterparts. Beyond model weights, we
release all scientific artifacts from our development cycle with a permissive
license, including data preparation scripts, checkpoints, evaluation suites,
and training code, enabling transparent audit and extension.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [199] [Explainable AI-Enhanced Supervisory Control for High-Precision Spacecraft Formation](https://arxiv.org/abs/2509.13331)
*Reza Pirayeshshirazinezhad*

Main category: astro-ph.IM

TL;DR: 本文利用AI和监督自适应控制系统优化航天器精确编队任务，结果显示降低能耗、提高任务精度。


<details>
  <summary>Details</summary>
Motivation: 提升X射线观测虚拟望远镜（VTXO）太空任务中航天器精确编队的效率，满足高精度任务标准。

Method: 采用监督控制的时间自动机、蒙特卡罗模拟评估稳定性和鲁棒性，集成深度神经网络进行任务参数最优估计，并将其与受限非凸动态优化管道集成预测最优任务参数。

Result: 降低了能耗，提高了任务精度，系统能应对动态不确定性和干扰。

Conclusion: 该AI框架可预测能耗和任务误差，实现透明、合理和实时权衡，优于传统自适应控制器。

Abstract: We use artificial intelligence (AI) and supervisory adaptive control systems
to plan and optimize the mission of precise spacecraft formation. Machine
learning and robust control enhance the efficiency of spacecraft precision
formation of the Virtual Telescope for X-ray Observation (VTXO) space mission.
VTXO is a precise formation of two separate spacecraft making a virtual
telescope with a one-kilometer focal length. One spacecraft carries the lens
and the other spacecraft holds the camera to observe high-energy space objects
in the X-ray domain with 55 milli-arcsecond angular resolution accuracy. Timed
automata for supervisory control, Monte Carlo simulations for stability and
robustness evaluation, and integration of deep neural networks for optimal
estimation of mission parameters, satisfy the high precision mission criteria.
We integrate deep neural networks with a constrained, non-convex dynamic
optimization pipeline to predict optimal mission parameters, ensuring precision
mission criteria are met. AI framework provides explainability by predicting
the resulting energy consumption and mission error for a given set of mission
parameters. It allows for transparent, justifiable, and real-time trade-offs, a
capability not present in traditional adaptive controllers. The results show
reductions in energy consumption and improved mission accuracy, demonstrating
the capability of the system to address dynamic uncertainties and disturbances.

</details>


### [200] [Improving cosmological reach of a gravitational wave observatory using Deep Loop Shaping](https://arxiv.org/abs/2509.14016)
*Jonas Buchli,Brendan Tracey,Tomislav Andric,Christopher Wipf,Yu Him Justin Chiu,Matthias Lochbrunner,Craig Donner,Rana X. Adhikari,Jan Harms,Iain Barr,Roland Hafner,Andrea Huber,Abbas Abdolmaleki,Charlie Beattie,Joseph Betzwieser,Serkan Cabi,Jonas Degrave,Yuzhu Dong,Leslie Fritz,Anchal Gupta,Oliver Groth,Sandy Huang,Tamara Norman,Hannah Openshaw,Jameson Rollins,Greg Thornton,George Van Den Driessche,Markus Wulfmeier,Pushmeet Kohli,Martin Riedmiller,LIGO Instrument Team*

Main category: astro-ph.IM

TL;DR: 利用深度回路整形法消除引力波天文台镜稳定控制噪声，在LLO验证，大幅降低特定频段控制噪声，凸显该方法潜力。


<details>
  <summary>Details</summary>
Motivation: 现有镜稳定控制注入有害噪声阻碍引力波天文台低频灵敏度提升，需消除噪声以解锁更多研究及多信使观测预警。

Method: 采用基于频域奖励的强化学习方法Deep Loop Shaping消除噪声。

Result: 控制器使10 - 30Hz频段控制噪声降低超30倍，子频段最高降100倍，超量子极限设计目标。

Conclusion: Deep Loop Shaping有潜力改进当前及未来引力波天文台和仪器控制系统。

Abstract: Improved low-frequency sensitivity of gravitational wave observatories would
unlock study of intermediate-mass black hole mergers, binary black hole
eccentricity, and provide early warnings for multi-messenger observations of
binary neutron star mergers. Today's mirror stabilization control injects
harmful noise, constituting a major obstacle to sensitivity improvements. We
eliminated this noise through Deep Loop Shaping, a reinforcement learning
method using frequency domain rewards. We proved our methodology on the LIGO
Livingston Observatory (LLO). Our controller reduced control noise in the
10--30Hz band by over 30x, and up to 100x in sub-bands surpassing the design
goal motivated by the quantum limit. These results highlight the potential of
Deep Loop Shaping to improve current and future GW observatories, and more
broadly instrumentation and control systems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [201] [AI Behavioral Science](https://arxiv.org/abs/2509.13323)
*Matthew O. Jackson,Qiaozhu Me,Stephanie W. Wang,Yutong Xie,Walter Yuan,Seth Benzell,Erik Brynjolfsson,Colin F. Camerer,James Evans,Brian Jabarian,Jon Kleinberg,Juanjuan Meng,Sendhil Mullainathan,Asuman Ozdaglar,Thomas Pfeiffer,Moshe Tennenholtz,Robb Willer,Diyi Yang,Teng Ye*

Main category: cs.HC

TL;DR: 探讨新兴的“AI行为科学”的三个主要领域。


<details>
  <summary>Details</summary>
Motivation: 探索AI与行为科学相互促进及二者交互对世界的影响。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: We discuss the three main areas comprising the new and emerging field of "AI
Behavioral Science". This includes not only how AI can enhance research in the
behavioral sciences, but also how the behavioral sciences can be used to study
and better design AI and to understand how the world will change as AI and
humans interact in increasingly layered and complex ways.

</details>


### [202] [LLM Chatbot-Creation Approaches](https://arxiv.org/abs/2509.13326)
*Hemil Mehta,Tanvi Raut,Kohav Yadav,Edward F. Gehringer*

Main category: cs.HC

TL;DR: 本文对比低代码平台和自定义编码解决方案在教育场景开发课程聊天机器人的方法，给出选择开发策略的框架，未来关注混合解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型兴起，LLM 聊天机器人用于教学，但选择开发策略需平衡易用性、定制性、数据隐私和可扩展性。

Method: 对比低代码平台（如 AnythingLLM 和 Botpress）与使用 LangChain、FAISS 和 FastAPI 的自定义编码解决方案，用提示工程、检索增强生成和个性化评估聊天机器人原型。

Result: 低代码平台可快速原型开发，但定制和扩展有限；自定义编码系统控制更多，但需大量技术专长，两种方法都实现了关键研究原则。

Conclusion: 提供根据机构目标和资源选择合适开发策略的框架，未来关注结合低代码可访问性和模块化定制的混合解决方案及智能辅导系统的多模态输入。

Abstract: This full research-to-practice paper explores approaches for developing
course chatbots by comparing low-code platforms and custom-coded solutions in
educational contexts. With the rise of Large Language Models (LLMs) like GPT-4
and LLaMA, LLM-based chatbots are being integrated into teaching workflows to
automate tasks, provide assistance, and offer scalable support. However,
selecting the optimal development strategy requires balancing ease of use,
customization, data privacy, and scalability. This study compares two
development approaches: low-code platforms like AnythingLLM and Botpress, with
custom-coded solutions using LangChain, FAISS, and FastAPI. The research uses
Prompt engineering, Retrieval-augmented generation (RAG), and personalization
to evaluate chatbot prototypes across technical performance, scalability, and
user experience. Findings indicate that while low-code platforms enable rapid
prototyping, they face limitations in customization and scaling, while
custom-coded systems offer more control but require significant technical
expertise. Both approaches successfully implement key research principles such
as adaptive feedback loops and conversational continuity. The study provides a
framework for selecting the appropriate development strategy based on
institutional goals and resources. Future work will focus on hybrid solutions
that combine low-code accessibility with modular customization and incorporate
multimodal input for intelligent tutoring systems.

</details>


### [203] [Synthetic Data Generation for Screen Time and App Usage](https://arxiv.org/abs/2509.13892)
*Gustavo Kruger,Nikhil Sachdeva,Michael Sobolev*

Main category: cs.HC

TL;DR: 本文探讨用大语言模型生成合成智能手机使用数据，通过案例研究不同提示策略对数据质量的影响，发现详细提示下生成数据可行，但仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 真实世界收集智能手机使用日志存在成本高、隐私等问题，需探索替代方法。

Method: 开展案例研究，对比四种提示策略，结合提示详细程度和种子数据包含情况。

Result: 在一些用例中，使用大语言模型生成结构化且行为合理的智能手机使用数据集是可行的，尤其是使用详细提示时。

Conclusion: 虽有可行性，但在捕捉人类行为模式细微差别、平衡数据保真度和多样性方面有挑战，需特定用例评估指标和更多研究。

Abstract: Smartphone usage data can provide valuable insights for understanding
interaction with technology and human behavior. However, collecting
large-scale, in-the-wild smartphone usage logs is challenging due to high
costs, privacy concerns, under representative user samples and biases like
non-response that can skew results. These challenges call for exploring
alternative approaches to obtain smartphone usage datasets. In this context,
large language models (LLMs) such as Open AI's ChatGPT present a novel approach
for synthetic smartphone usage data generation, addressing limitations of
real-world data collection. We describe a case study on how four prompt
strategies influenced the quality of generated smartphone usage data. We
contribute with insights on prompt design and measures of data quality,
reporting a prompting strategy comparison combining two factors, prompt level
of detail (describing a user persona, describing the expected results
characteristics) and seed data inclusion (with versus without an initial real
usage example). Our findings suggest that using LLMs to generate structured and
behaviorally plausible smartphone use datasets is feasible for some use cases,
especially when using detailed prompts. Challenges remain in capturing diverse
nuances of human behavioral patterns in a single synthetic dataset, and
evaluating tradeoffs between data fidelity and diversity, suggesting the need
for use-case-specific evaluation metrics and future research with more diverse
seed data and different LLM models.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [204] [Hierarchical Importance Sampling for Estimating Occupation Time for SDE Solutions](https://arxiv.org/abs/2509.13950)
*Eya Ben Amar,Nadhir Ben Rached,Raul Tempone*

Main category: math.NA

TL;DR: 研究随机微分方程过程占用时间互补累积分布函数估计，提出单水平和多水平重要性抽样估计器，给出多水平优于单水平的条件，通过数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 在占用时间互补累积分布函数右尾估计中，底层事件罕见，需用方差缩减技术获高效估计。

Method: 基于重要性抽样与随机最优控制的关系，构建基于HJB - PDE解的最优单水平重要性抽样估计器，扩展到多水平，提出优化预处理和抽样权衡以最小化总成本。

Result: 建立多水平重要性抽样方法优于单水平的充要条件，引入满足条件的公式，经典多层蒙特卡罗复杂度理论可扩展，多水平总工作复杂度能优于二阶。

Conclusion: 数值实验证明了所提方法的优势并验证了理论结果。

Abstract: This study considers the estimation of the complementary cumulative
distribution function of the occupation time (i.e., the time spent below a
threshold) for a process governed by a stochastic differential equation. The
focus is on the right tail, where the underlying event becomes rare, and using
variance reduction techniques is essential to obtain computationally efficient
estimates. Building on recent developments that relate importance sampling (IS)
to stochastic optimal control, this work develops an optimal single level IS
(SLIS) estimator based on the solution of an auxiliary Hamilton Jacobi Bellman
(HJB) partial differential equation (PDE). The cost of solving the HJB-PDE is
incorporated into the total computational work, and an optimized trade off
between preprocessing and sampling is proposed to minimize the overall cost.
The SLIS approach is extended to the multilevel setting to enhance efficiency,
yielding a multilevel IS (MLIS) estimator. A necessary and sufficient condition
under which the MLIS method outperforms the SLIS method is established, and a
common likelihood MLIS formulation is introduced that satisfies this condition
under appropriate regularity assumptions. The classical multilevel Monte Carlo
complexity theory can be extended to accommodate settings where the
single-level variance varies with the discretization level. As a special case,
the variance-decay behavior observed in the IS framework stems from the zero
variance property of the optimal control. Notably, the total work complexity of
MLIS can be better than an order of two. Numerical experiments in the context
of fade duration estimation demonstrate the benefits of the proposed approach
and validate these theoretical results.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [205] [Catalpa: GC for a Low-Variance Software Stack](https://arxiv.org/abs/2509.13429)
*Anthony Arnold,Mark Marron*

Main category: cs.PL

TL;DR: 提出适用于Bosque语言和运行时的Catalpa垃圾收集器设计，利用语言特性实现低延迟、高吞吐量和小内存开销。


<details>
  <summary>Details</summary>
Motivation: 实际应用中性能多为二元函数，开发者关注尾延迟，需软件栈满足此需求。

Method: 利用Bosque语言的不可变性和无引用循环等特性，设计Catalpa收集器。

Result: 设计出的收集器有有界收集暂停、固定内存开销，无需与应用代码进行屏障或同步。

Conclusion: Catalpa收集器能最小化延迟和可变性，同时保持高吞吐量和小内存开销。

Abstract: The performance of an application/runtime is usually conceptualized as a
continuous function where, the lower the amount of memory/time used on a given
workload, then the better the compiler/runtime is. However, in practice, good
performance of an application is viewed as more of a binary function - either
the application responds in under, say 100 ms, and is fast enough for a user to
barely notice, or it takes a noticeable amount of time, leaving the user
waiting and potentially abandoning the task. Thus, performance really means how
often the application is fast enough to be usable, leading industrial
developers to focus on the 95th and 99th percentile tail-latencies as heavily,
or moreso, than average response time. Our vision is to create a software stack
that actively supports these needs via programming language and runtime system
design. In this paper we present a novel garbage-collector design, the Catalpa
collector, for the Bosque programming language and runtime. This allocator is
designed to minimize latency and variability while maintaining high-throughput
and incurring small memory overheads. To achieve these goals we leverage
various features of the Bosque language, including immutability and
reference-cycle freedom, to construct a collector that has bounded collection
pauses, incurs fixed-constant memory overheads, and does not require any
barriers or synchronization with application code.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [206] [Smaller Circuits for Bit Addition](https://arxiv.org/abs/2509.13966)
*Mikhail Goncharov,Alexander S. Kulikov,Georgie Levtsov*

Main category: cs.CC

TL;DR: 本文研究位加法在全二进制基上的电路规模，改进了上界，在某些场景有10%的提升，并给出开源实现。


<details>
  <summary>Details</summary>
Motivation: 已有很多关于位加法电路深度优化的研究，本文聚焦电路规模优化。

Method: 分析已知由半加器和全加器构建的电路，改进上界。

Result: 将上界从$5n - 3m$改进到$4.5n - 2m$，在$m$比$n$小的场景有10%提升。

Conclusion: 提出更优的位加法电路规模上界，并给出开源生成器用于生成和比较电路。

Abstract: Bit addition arises virtually everywhere in digital circuits: arithmetic
operations, increment/decrement operators, computing addresses and table
indices, and so on. Since bit addition is such a basic task in Boolean circuit
synthesis, a lot of research has been done on constructing efficient circuits
for various special cases of it. A vast majority of these results are devoted
to optimizing the circuit depth (also known as delay).
  In this paper, we investigate the circuit size (also known as area) over the
full binary basis of bit addition. Though most of the known circuits are built
from Half Adders and Full Adders, we show that, in many interesting scenarios,
these circuits have suboptimal size. Namely, we improve an upper bound $5n-3m$
to $4.5n-2m$, where $n$ is the number of input bits and $m$ is the number of
output bits. In the regimes where $m$ is small compared to $n$ (for example,
for computing the sum of $n$ bits or multiplying two $n$-bit integers), this
leads to $10\%$ improvement.
  We complement our theoretical result by an open-source implementation of
generators producing circuits for bit addition and multiplication. The
generators allow one to produce the corresponding circuits in two lines of code
and to compare them to existing designs.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [207] [Multi-Threaded Software Model Checking via Parallel Trace Abstraction Refinement](https://arxiv.org/abs/2509.13699)
*Max Barth,Marie-Christine Jakobs*

Main category: cs.LO

TL;DR: 本文提出并行化跟踪抽象的解决方案以加速软件模型检查，评估显示该并行化比顺序跟踪抽象和DSS方法更有效。


<details>
  <summary>Details</summary>
Motivation: 自动软件验证耗时长，阻碍实际应用，需利用多核CPU减少验证响应时间。

Method: 并行化跟踪抽象，并行分析可能违反安全属性的不同跟踪，在验证工具Ultimate Automizer中实现。

Result: 并行化比顺序跟踪抽象更有效，在许多耗时任务上能更快给出结果，也比DSS方法更有效。

Conclusion: 所提出的并行化跟踪抽象解决方案能有效加速软件模型检查。

Abstract: Automatic software verification is a valuable means for software quality
assurance. However, automatic verification and in particular software model
checking can be time-consuming, which hinders their practical applicability
e.g., the use in continuous integration. One solution to address the issue is
to reduce the response time of the verification procedure by leveraging today's
multi-core CPUs.
  In this paper, we propose a solution to parallelize trace abstraction, an
abstraction-based approach to software model checking. The underlying idea of
our approach is to parallelize the abstraction refinement. More concretely, our
approach analyzes different traces (syntactic program paths) that could violate
the safety property in parallel. We realize our parallelized version of trace
abstraction in the verification tool Ulti mate Automizer and perform a thorough
evaluation. Our evaluation shows that our parallelization is more effective
than sequential trace abstraction and can provide results significantly faster
on many time-consuming tasks. Also, our approach is more effective than DSS, a
recent parallel approach to abstraction-based software model checking.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [208] [To whom did my vote go?](https://arxiv.org/abs/2509.13370)
*Andrew Conway,Michelle Blom,Alexander Ek,Peter Stuckey,Vanessa Teague,Damjan Vukcevic*

Main category: cs.CY

TL;DR: 本文介绍了一个演示系统，可让选民了解其在澳大利亚STV选举中的选票转移和计票贡献。


<details>
  <summary>Details</summary>
Motivation: STV计票系统复杂，选民难以了解个人选票在各轮计票中对候选人票数的贡献。

Method: 开发一个演示系统，让选民输入过去澳大利亚STV选举的示例选票。

Result: 该系统可展示选票在候选人之间的转移情况，以及对相关候选人票数的贡献。

Conclusion: 该演示系统有助于选民理解STV计票过程中个人选票的作用。

Abstract: Single Transferable Vote (STV) counting, used in several jurisdictions in
Australia, is a system for choosing multiple election winners given voters'
preferences among candidates. The system is complex and it is not always
obvious how an individual's vote contributes to candidates' tallies across
rounds of tabulation. This short paper presents a demonstration system that
allows voters to enter an example vote in a past Australian STV election, and
see: (i)~how that vote would have been transferred between candidates; and
(ii)~how much that vote would have contributed to the tallies of relevant
candidates, across rounds of tabulation.

</details>


### [209] [The Economics of Information Pollution in the Age of AI: A General Equilibrium Approach to Welfare, Measurement, and Policy](https://arxiv.org/abs/2509.13729)
*Yukun Zhang,Tianyang Zhang*

Main category: cs.CY

TL;DR: 本文构建一般均衡框架分析大语言模型引发的信息污染挑战，证明“污染信息均衡”存在及低效性，推导信息污染指数，提出需组合政策工具并采用适应性治理框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型使低质量信息生产成本不对称下降，引发信息污染问题，需进行深入分析。

Method: 构建三阶段博弈模型，模拟垄断平台、生产者和消费者的战略互动，采用具有不同替代弹性的生产技术；推导信息污染指数。

Result: 证明存在唯一的“污染信息均衡”且该均衡无效率，由三重市场失灵导致；提出需组合政策工具应对。

Conclusion: 在AI时代，需采用适应性治理框架，根据实时信息污染指数动态调整政策工具来监管信息市场。

Abstract: The advent of Large Language Models (LLMs) represents a fundamental shock to
the economics of information production. By asymmetrically collapsing the
marginal cost of generating low-quality, synthetic content while leaving
high-quality production costly, AI systematically incentivizes information
pollution. This paper develops a general equilibrium framework to analyze this
challenge. We model the strategic interactions among a monopolistic platform,
profit-maximizing producers, and utility-maximizing consumers in a three-stage
game. The core of our model is a production technology with differential
elasticities of substitution ($\sigma_L > 1 > \sigma_H$), which formalizes the
insight that AI is a substitute for labor in low-quality production but a
complement in high-quality creation. We prove the existence of a unique
"Polluted Information Equilibrium" and demonstrate its inefficiency, which is
driven by a threefold market failure: a production externality, a platform
governance failure, and an information commons externality. Methodologically,
we derive a theoretically-grounded Information Pollution Index (IPI) with
endogenous welfare weights to measure ecosystem health. From a policy
perspective, we show that a first-best outcome requires a portfolio of
instruments targeting each failure. Finally, considering the challenges of deep
uncertainty, we advocate for an adaptive governance framework where policy
instruments are dynamically adjusted based on real-time IPI readings, offering
a robust blueprint for regulating information markets in the age of AI.

</details>


### [210] [Accuracy Paradox in Large Language Models: Regulating Hallucination Risks in Generative AI](https://arxiv.org/abs/2509.13345)
*Zihao Li,Weiwei Yi,Jiahong Chen*

Main category: cs.CY

TL;DR: 文章指出大语言模型幻觉问题严重，过度依赖准确性指标存在悖论，现行法规难以应对，呼吁转向多元、情境感知和抗操纵的AI可信治理方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型融入日常决策，其认知和社会风险需迫切审视，而过度依赖准确性指标来减轻幻觉危害存在问题。

Method: 结合跨学科文献，从输出、个体和社会三个维度分析准确性悖论，还通过研究欧盟AI法案、GDPR和DSA等法规。

Result: 发现准确性指标是可靠性的表面替代，无法检测非事实性错误但有误导性的危害，且法规过度强调准确性掩盖了幻觉的社会后果。

Conclusion: 现行法规在结构上无法应对这些危害，需向多元、情境感知和抗操纵的AI可信治理方法转变。

Abstract: As Large Language Models (LLMs) permeate everyday decision-making, their
epistemic and societal risks demand urgent scrutiny. Hallucinations, the
generation of fabricated, misleading, oversimplified or untrustworthy outputs,
has emerged as imperative challenges. While regulatory, academic, and technical
discourse position accuracy as the principal benchmark for mitigating such
harms, this article contends that overreliance on accuracy misdiagnoses the
problem and has counterproductive effect: the accuracy paradox. Drawing on
interdisciplinary literatures, this article develops a taxonomy of
hallucination types and shows the paradox along three intertwining dimensions:
outputs, individuals and society. First, accuracy functions as a superficial
proxy for reliability, incentivising the optimisation of rhetorical fluency and
surface-level correctness over epistemic trustworthiness. This encourages
passive user trust in outputs that appear accurate but epistemically untenable.
Second, accuracy as a singular metric fails to detect harms that are not
factually false but are nonetheless misleading, value-laden, or socially
distorting, including consensus illusions, sycophantic alignment, and subtle
manipulation. Third, regulatory overemphasis on accuracy obscures the wider
societal consequences of hallucination, including social sorting, privacy
violations, equity harms, epistemic convergence that marginalises dissent,
reduces pluralism, and causes social deskilling. By examining the EU AI Act,
GDPR, and DSA, the article argues that current regulations are not yet
structurally equipped to address these epistemic, relational, and systemic
harms and exacerbated by the overreliance on accuracy. By exposing such
conceptual and practical challenges, this article calls for a fundamental shift
towards pluralistic, context-aware, and manipulation-resilient approaches to AI
trustworthy governance.

</details>


### [211] [Synthetic Data and the Shifting Ground of Truth](https://arxiv.org/abs/2509.13355)
*Dietmar Offenhuber*

Main category: cs.CY

TL;DR: 合成数据使地面真值概念复杂化，缺乏现实参照仍有益模型，论文探讨在此情况下引导地面真值及数据概念转变影响。


<details>
  <summary>Details</summary>
Motivation: 合成数据出现使地面真值概念复杂，需研究在此矛盾情况下引导地面真值及数据概念转变影响。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: The emergence of synthetic data for privacy protection, training data
generation, or simply convenient access to quasi-realistic data in any shape or
volume complicates the concept of ground truth. Synthetic data mimic real-world
observations, but do not refer to external features. This lack of a
representational relationship, however, not prevent researchers from using
synthetic data as training data for AI models and ground truth repositories. It
is claimed that the lack of data realism is not merely an acceptable tradeoff,
but often leads to better model performance than realistic data: compensate for
known biases, prevent overfitting and support generalization, and make the
models more robust in dealing with unexpected outliers. Indeed, injecting noisy
and outright implausible data into training sets can be beneficial for the
model. This greatly complicates usual assumptions based on which
representational accuracy determines data fidelity (garbage in - garbage out).
Furthermore, ground truth becomes a self-referential affair, in which the
labels used as a ground truth repository are themselves synthetic products of a
generative model and as such not connected to real-world observations. My paper
examines how ML researchers and practitioners bootstrap ground truth under such
paradoxical circumstances without relying on the stable ground of
representation and real-world reference. It will also reflect on the broader
implications of a shift from a representational to what could be described as a
mimetic or iconic concept of data.

</details>


### [212] [Evaluating undergraduate mathematics examinations in the era of generative AI: a curriculum-level case study](https://arxiv.org/abs/2509.13359)
*Benjamin J. Walker,Beatriz Navarro Lameda,Ruth A. Reynolds*

Main category: cs.CY

TL;DR: 研究在无监考、可使用GenAI的开卷环境下，传统闭卷数学考试是否还有教学相关性，发现GenAI成绩达一等学位水平，表现比学生更稳定，需重新设计数学评估。


<details>
  <summary>Details</summary>
Motivation: GenAI改变教育格局，高校探索替代闭卷考试方式，引发学术诚信和教学一致性担忧，研究传统闭卷数学考试在新环境下的教学相关性。

Method: 采用实证方法，对罗素集团大学本科一年级八门数学考试的GenAI提交内容进行生成、转录和盲评，综合独立GenAI对单个问题的回答进行评估。

Result: GenAI成绩达一等学位水平，不同模块表现有差异，在整个课程中的表现比监考考试中学生的表现更稳定。

Conclusion: 有必要为无监督环境重新设计数学评估，当前标准在GenAI时代的教学价值可能降低。

Abstract: Generative artificial intelligence (GenAI) tools such as OpenAI's ChatGPT are
transforming the educational landscape, prompting reconsideration of
traditional assessment practices. In parallel, universities are exploring
alternatives to in-person, closed-book examinations, raising concerns about
academic integrity and pedagogical alignment in uninvigilated settings. This
study investigates whether traditional closed-book mathematics examinations
retain their pedagogical relevance when hypothetically administered in
uninvigilated, open-book settings with GenAI access. Adopting an empirical
approach, we generate, transcribe, and blind-mark GenAI submissions to eight
undergraduate mathematics examinations at a Russel Group university, spanning
the entirety of the first-year curriculum. By combining independent GenAI
responses to individual questions, we enable a meaningful evaluation of GenAI
performance, both at the level of modules and across the first-year curriculum.
We find that GenAI attainment is at the level of a first-class degree, though
current performance can vary between modules. Further, we find that GenAI
performance is remarkably consistent when viewed across the entire curriculum,
significantly more so than that of students in invigilated examinations. Our
findings evidence the need for redesigning assessments in mathematics for
unsupervised settings, and highlight the potential reduction in pedagogical
value of current standards in the era of generative artificial intelligence.

</details>


### [213] [The Provenance Problem: LLMs and the Breakdown of Citation Norms](https://arxiv.org/abs/2509.13365)
*Brian D. Earp,Haotian Yuan,Julian Koplin,Sebastian Porsdam Mann*

Main category: cs.CY

TL;DR: 生成式AI用于科研写作引发归属和学术信用问题，存在出处问题，当前框架无法解决，文章分析挑战并提出策略。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在科研写作中的使用日益增加，引发归属和学术信用的紧迫问题，当前伦理和专业框架无法应对新的归属伤害。

Method: 分析AI对既定作者规范的挑战，引入概念工具理解出处问题。

Result: 指出使用生成式AI写作存在不涉及欺骗意图但仍受益于他人未获认可智力贡献的出处问题。

Conclusion: 提出维护学术交流完整性和公平性的策略。

Abstract: The increasing use of generative AI in scientific writing raises urgent
questions about attribution and intellectual credit. When a researcher employs
ChatGPT to draft a manuscript, the resulting text may echo ideas from sources
the author has never encountered. If an AI system reproduces insights from, for
example, an obscure 1975 paper without citation, does this constitute
plagiarism? We argue that such cases exemplify the 'provenance problem': a
systematic breakdown in the chain of scholarly credit. Unlike conventional
plagiarism, this phenomenon does not involve intent to deceive (researchers may
disclose AI use and act in good faith) yet still benefit from the uncredited
intellectual contributions of others. This dynamic creates a novel category of
attributional harm that current ethical and professional frameworks fail to
address. As generative AI becomes embedded across disciplines, the risk that
significant ideas will circulate without recognition threatens both the
reputational economy of science and the demands of epistemic justice. This
Perspective analyzes how AI challenges established norms of authorship,
introduces conceptual tools for understanding the provenance problem, and
proposes strategies to preserve integrity and fairness in scholarly
communication.

</details>


### [214] [Uncovering AI Governance Themes in EU Policies using BERTopic and Thematic Analysis](https://arxiv.org/abs/2509.13387)
*Delaram Golpayegani,Marta Lasek-Markey,Arjumand Younus,Aphra Kerr,Dave Lewis*

Main category: cs.CY

TL;DR: 本文用定性和定量方法分析欧盟AI治理政策文件，呈现欧盟AI治理政策演变视角。


<details>
  <summary>Details</summary>
Motivation: 欧盟AI治理政策和指南零散，虽期望一致但存在差异，为全面了解欧盟AI治理。

Method: 运用定性主题分析方法研究关键文件，用BERTopic模型进行定量主题建模，增加2018年后文件样本。

Result: 呈现了对欧盟政策的新视角，追踪其解决AI治理方法的演变。

Conclusion: 通过定性和定量结合方法能有效分析欧盟AI治理政策演变。

Abstract: The upsurge of policies and guidelines that aim to ensure Artificial
Intelligence (AI) systems are safe and trustworthy has led to a fragmented
landscape of AI governance. The European Union (EU) is a key actor in the
development of such policies and guidelines. Its High-Level Expert Group (HLEG)
issued an influential set of guidelines for trustworthy AI, followed in 2024 by
the adoption of the EU AI Act. While the EU policies and guidelines are
expected to be aligned, they may differ in their scope, areas of emphasis,
degrees of normativity, and priorities in relation to AI. To gain a broad
understanding of AI governance from the EU perspective, we leverage qualitative
thematic analysis approaches to uncover prevalent themes in key EU documents,
including the AI Act and the HLEG Ethics Guidelines. We further employ
quantitative topic modelling approaches, specifically through the use of the
BERTopic model, to enhance the results and increase the document sample to
include EU AI policy documents published post-2018. We present a novel
perspective on EU policies, tracking the evolution of its approach to
addressing AI governance.

</details>


### [215] [The Intercepted Self: How Generative AI Challenges the Dynamics of the Relational Self](https://arxiv.org/abs/2509.13391)
*Sandrine R. Schiller,Camilo Miguel Signorelli,Filippos Stamatiou*

Main category: cs.CY

TL;DR: 探讨生成式AI对人机关系及自我认知的影响，分析其在不同领域对人类主动性的影响。


<details>
  <summary>Details</summary>
Motivation: 生成式AI发展促使人们重新思考人机关系及对自我的影响，有必要深入研究。

Method: 基于关系自我的概念，分析生成式AI在外部输出、情境和自我关联领域的可能影响。

Result: 未提及具体研究结果。

Conclusion: 通过概述生成式AI在不同领域对任务完成和人类主动性的影响，加深对AI革命的存在主义思考。

Abstract: Generative AI is changing our way of interacting with technology, others, and
ourselves. Systems such as Microsoft copilot, Gemini and the expected Apple
intelligence still awaits our prompt for action. Yet, it is likely that AI
assistant systems will only become better at predicting our behaviour and
acting on our behalf. Imagine new generations of generative and predictive AI
deciding what you might like best at a new restaurant, picking an outfit that
increases your chances on your date with a partner also chosen by the same or a
similar system. Far from a science fiction scenario, the goal of several
research programs is to build systems capable of assisting us in exactly this
manner. The prospect urges us to rethink human-technology relations, but it
also invites us to question how such systems might change the way we relate to
ourselves. Building on our conception of the relational self, we question the
possible effects of generative AI with respect to what we call the sphere of
externalised output, the contextual sphere and the sphere of self-relating. In
this paper, we attempt to deepen the existential considerations accompanying
the AI revolution by outlining how generative AI enables the fulfilment of
tasks and also increasingly anticipates, i.e. intercepts, our initiatives in
these different spheres.

</details>


### [216] [The threat of analytic flexibility in using large language models to simulate human data: A call to attention](https://arxiv.org/abs/2509.13397)
*Jamie Cummins*

Main category: cs.CY

TL;DR: 本文探讨用大语言模型创建“硅样本”用于人类主体研究，指出分析选择影响样本质量且无通用配置，呼吁关注分析灵活性威胁。


<details>
  <summary>Details</summary>
Motivation: 大语言模型创建硅样本用于人类主体研究时，众多分析选择对样本质量的影响尚不明确。

Method: 对创建硅样本的分析选择进行梳理，并展示少量决策如何显著改变硅样本与人类数据的对应关系，测试252种配置。

Result: 不同配置在估计参与者排名顺序、响应分布和量表间相关性的能力上差异大，且在质量上不一致。

Conclusion: 使用硅样本时没有“一刀切”的配置来优化样本准确性，需更多关注分析灵活性的威胁。

Abstract: Social scientists are now using large language models to create "silicon
samples" - synthetic datasets intended to stand in for human respondents, aimed
at revolutionising human subjects research. However, there are many analytic
choices which must be made to produce these samples. Though many of these
choices are defensible, their impact on sample quality is poorly understood. I
map out these analytic choices and demonstrate how a very small number of
decisions can dramatically change the correspondence between silicon samples
and human data. Configurations (N = 252) varied substantially in their capacity
to estimate (i) rank ordering of participants, (ii) response distributions, and
(iii) between-scale correlations. Most critically, configurations were not
consistent in quality: those that performed well on one dimension often
performed poorly on another, implying that there is no "one-size-fits-all"
configuration that optimises the accuracy of these samples. I call for greater
attention to the threat of analytic flexibility in using silicon samples.

</details>


### [217] [Justice in Judgment: Unveiling (Hidden) Bias in LLM-assisted Peer Reviews](https://arxiv.org/abs/2509.13400)
*Sai Suresh Marchala Vasu,Ivaxi Sheth,Hui-Po Wang,Ruta Binkyte,Mario Fritz*

Main category: cs.CY

TL;DR: 研究大语言模型生成同行评审中的偏见，发现存在机构排名和性别偏好等偏见。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用于同行评审带来机遇同时引发公平和可靠性担忧，需研究其生成评审中的偏见。

Method: 对作者所属机构和性别等敏感元数据进行受控实验。

Result: 分析显示存在偏向高排名机构的机构偏见，也发现有一定性别偏好，基于标记的软评分中隐式偏见更明显。

Conclusion: 大语言模型生成的同行评审存在机构排名和性别等方面的偏见问题。

Abstract: The adoption of large language models (LLMs) is transforming the peer review
process, from assisting reviewers in writing more detailed evaluations to
generating entire reviews automatically. While these capabilities offer
exciting opportunities, they also raise critical concerns about fairness and
reliability. In this paper, we investigate bias in LLM-generated peer reviews
by conducting controlled experiments on sensitive metadata, including author
affiliation and gender. Our analysis consistently shows affiliation bias
favoring institutions highly ranked on common academic rankings. Additionally,
we find some gender preferences, which, even though subtle in magnitude, have
the potential to compound over time. Notably, we uncover implicit biases that
become more evident with token-based soft ratings.

</details>


### [218] [Reproducible workflow for online AI in digital health](https://arxiv.org/abs/2509.13499)
*Susobhan Ghosh,Bhanu T. Gulapalli,Daiqi Gao,Asim Gazi,Anna Trella,Ziping Xu,Kelly Zhang,Susan A. Murphy*

Main category: cs.CY

TL;DR: 本文提出数字健康干预中在线AI决策算法的可复现科学工作流，应对可复现性挑战。


<details>
  <summary>Details</summary>
Motivation: 在线AI在数字健康干预中部署面临适应性与可复现性平衡的挑战，且该领域迭代部署特性凸显可复现性重要性。

Method: 基于多个实际部署的实践经验，提出一个科学工作流。

Result: 提出的工作流能应对在线AI算法开发生命周期各阶段可复现性的关键挑战。

Conclusion: 该可复现的科学工作流有助于数字健康干预中在线AI决策算法的开发、部署和分析。

Abstract: Online artificial intelligence (AI) algorithms are an important component of
digital health interventions. These online algorithms are designed to
continually learn and improve their performance as streaming data is collected
on individuals. Deploying online AI presents a key challenge: balancing
adaptability of online AI with reproducibility. Online AI in digital
interventions is a rapidly evolving area, driven by advances in algorithms,
sensors, software, and devices. Digital health intervention development and
deployment is a continuous process, where implementation - including the AI
decision-making algorithm - is interspersed with cycles of re-development and
optimization. Each deployment informs the next, making iterative deployment a
defining characteristic of this field. This iterative nature underscores the
importance of reproducibility: data collected across deployments must be
accurately stored to have scientific utility, algorithm behavior must be
auditable, and results must be comparable over time to facilitate scientific
discovery and trustworthy refinement. This paper proposes a reproducible
scientific workflow for developing, deploying, and analyzing online AI
decision-making algorithms in digital health interventions. Grounded in
practical experience from multiple real-world deployments, this workflow
addresses key challenges to reproducibility across all phases of the online AI
algorithm development life-cycle.

</details>


### [219] [Understanding the Process of Human-AI Value Alignment](https://arxiv.org/abs/2509.13854)
*Jack McKinlay,Marina De Vos,Janina A. Hoffmann,Andreas Theodorou*

Main category: cs.CY

TL;DR: 文章通过对172篇价值对齐研究文章进行主题分析，得出六个主题，并给出价值对齐的定义，指出研究挑战和机会。


<details>
  <summary>Details</summary>
Motivation: 当前价值对齐表述缺乏精确性，为增进对人工智能中价值对齐的理解，给出更精确的定义。

Method: 分析近年发表的172篇价值对齐研究文章，并用主题分析综合内容。

Result: 分析得出六个主题：价值对齐驱动与方法、价值对齐挑战、价值对齐中的价值、人类与AI认知过程、人机协作、设计开发价值对齐系统。

Conclusion: 将价值对齐定义为人类与自主代理间的持续过程，分析得出该领域未来研究的挑战和机会。

Abstract: Background: Value alignment in computer science research is often used to
refer to the process of aligning artificial intelligence with humans, but the
way the phrase is used often lacks precision. Objectives: In this paper, we
conduct a systematic literature review to advance the understanding of value
alignment in artificial intelligence by characterising the topic in the context
of its research literature. We use this to suggest a more precise definition of
the term. Methods: We analyse 172 value alignment research articles that have
been published in recent years and synthesise their content using thematic
analyses. Results: Our analysis leads to six themes: value alignment drivers &
approaches; challenges in value alignment; values in value alignment; cognitive
processes in humans and AI; human-agent teaming; and designing and developing
value-aligned systems. Conclusions: By analysing these themes in the context of
the literature we define value alignment as an ongoing process between humans
and autonomous agents that aims to express and implement abstract values in
diverse contexts, while managing the cognitive limits of both humans and AI
agents and also balancing the conflicting ethical and political demands
generated by the values in different groups. Our analysis gives rise to a set
of research challenges and opportunities in the field of value alignment for
future work.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [220] [CraftMesh: High-Fidelity Generative Mesh Manipulation via Poisson Seamless Fusion](https://arxiv.org/abs/2509.13688)
*James Jincheng,Youcheng Cai,Ligang Liu*

Main category: cs.GR

TL;DR: 提出CraftMesh框架用于高保真生成式网格操作，通过实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成式方法在处理复杂几何形状和生成详细结果方面存在困难，可控的高保真网格编辑仍是3D内容创作的重大挑战。

Method: 将网格编辑分解为利用2D和3D生成模型优势的管道，编辑2D参考图像，生成特定区域的3D网格并融合到原始模型中，引入Poisson Geometric Fusion和Poisson Texture Harmonization两种核心技术。

Result: CraftMesh在复杂编辑任务中优于现有方法，在全局一致性和局部细节上表现更优。

Conclusion: CraftMesh框架在高保真生成式网格操作方面是有效的，能解决现有方法的不足。

Abstract: Controllable, high-fidelity mesh editing remains a significant challenge in
3D content creation. Existing generative methods often struggle with complex
geometries and fail to produce detailed results. We propose CraftMesh, a novel
framework for high-fidelity generative mesh manipulation via Poisson Seamless
Fusion. Our key insight is to decompose mesh editing into a pipeline that
leverages the strengths of 2D and 3D generative models: we edit a 2D reference
image, then generate a region-specific 3D mesh, and seamlessly fuse it into the
original model. We introduce two core techniques: Poisson Geometric Fusion,
which utilizes a hybrid SDF/Mesh representation with normal blending to achieve
harmonious geometric integration, and Poisson Texture Harmonization for
visually consistent texture blending. Experimental results demonstrate that
CraftMesh outperforms state-of-the-art methods, delivering superior global
consistency and local detail in complex editing tasks.

</details>


<div id='cs.SY'></div>

# cs.SY [[Back]](#toc)

### [221] [Modeling skiers flows via Wardrope equilibrium in closed capacitated networks](https://arxiv.org/abs/2509.13392)
*Demyan Yarmoshik,Igor Ignashin,Ekaterina Sikacheva,Alexander Gasnikov*

Main category: cs.SY

TL;DR: 提出滑雪度假村均衡模型，用凸优化求等待时间，将均衡问题转化为变分不等式并用标准算法求解。


<details>
  <summary>Details</summary>
Motivation: 研究滑雪度假村用户在封闭网络中分配到循环的均衡情况，解决有限容量索道排队问题。

Method: 通过凸优化找到计算等待时间的有效方法，将均衡问题表述为变分不等式。

Result: 数值实验表明可以使用标准算法解决均衡问题。

Conclusion: 所提出的模型和方法能有效处理滑雪度假村的均衡问题。

Abstract: We propose an equilibrium model of ski resorts where users are assigned to
cycles in a closed network. As queues form on lifts with limited capacity, we
derive an efficient way to find waiting times via convex optimization. The
equilibrium problem is formulated as a variational inequality, and numerical
experiments show that it can be solved using standard algorithms.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [222] [Spacing Test for Fused Lasso](https://arxiv.org/abs/2509.14229)
*Rieko Tasaka,Tatsuya Kimura,Joe Suzuki*

Main category: math.ST

TL;DR: 本文将间距检验框架扩展到融合套索，推导选定变化点的精确条件p值，经实验证明该方法能控制一类错误且有高检测力，为结构化信号估计问题提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决融合套索中正则化参数选择的未决问题。

Method: 将Tibshirani等人提出的间距检验框架扩展到融合套索，用LARS型算法分析融合套索解路径推导精确条件p值，通过数值实验与AIC、BIC和交叉验证的顺序版本对比。

Result: 所提方法能适当控制一类错误，同时实现高检测力。

Conclusion: 为结构化信号估计问题中的参数选择和选择后推断提供了理论合理且计算可行的解决方案。

Abstract: This study addresses the unresolved problem of selecting the regularization
parameter in the fused lasso. In particular, we extend the framework of the
Spacing Test proposed by Tibshirani et al. to the fused lasso, providing a
theoretical foundation for post-selection inference by characterizing the
selection event as a polyhedral constraint. Based on the analysis of the
solution path of the fused lasso using a LARS-type algorithm, we derive exact
conditional $p$-values for the selected change-points. Our method broadens the
applicability of the Spacing Test from the standard lasso to fused penalty
structures. Furthermore, through numerical experiments comparing the proposed
method with sequential versions of AIC and BIC as well as cross-validation, we
demonstrate that the proposed approach properly controls the type I error while
achieving high detection power. This work offers a theoretically sound and
computationally practical solution for parameter selection and post-selection
inference in structured signal estimation problems. Keywords: Fused Lasso,
Regularization parameter selection, Spacing Test for Lasso, Selective
inference, Change-point detection

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [223] [Circuit realization and hardware linearization of monotone operator equilibrium networks](https://arxiv.org/abs/2509.13793)
*Thomas Chaffey*

Main category: eess.SY

TL;DR: 研究表明电阻 - 二极管网络端口行为对应ReLU单调算子平衡网络解，可在模拟硬件构建神经网络，提出硬件线性化计算梯度并训练，拓展至级联网络，引入新型二极管ReLU。


<details>
  <summary>Details</summary>
Motivation: 在模拟硬件中实现神经网络的简约构建并解决训练问题。

Method: 提出硬件线性化方法计算电路梯度，进行设备级电路仿真。

Result: 可在硬件中训练网络，拓展结果到级联网络，引入新型二极管ReLU。

Conclusion: 电阻 - 二极管网络能在模拟硬件构建神经网络，不同非线性元件产生不同激活函数。

Abstract: It is shown that the port behavior of a resistor-diode network corresponds to
the solution of a ReLU monotone operator equilibrium network (a neural network
in the limit of infinite depth), giving a parsimonious construction of a neural
network in analog hardware. We furthermore show that the gradient of such a
circuit can be computed directly in hardware, using a procedure we call
hardware linearization. This allows the network to be trained in hardware,
which we demonstrate with a device-level circuit simulation. We extend the
results to cascades of resistor-diode networks, which can be used to implement
feedforward and other asymmetric networks. We finally show that different
nonlinear elements give rise to different activation functions, and introduce
the novel diode ReLU which is induced by a non-ideal diode model.

</details>


### [224] [Zero-sum turn games using Q-learning: finite computation with security guarantees](https://arxiv.org/abs/2509.13585)
*Sean Anderson,Chris Darken,João Hespanha*

Main category: eess.SY

TL;DR: 本文研究零和回合制博弈，提出用Q - learning构建纯鞍点状态反馈策略，对折扣和非折扣成本给出收敛结果，还提出对手信息探索策略，并用多智能体游戏验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 解决零和回合制博弈中纯鞍点状态反馈策略的构建问题。

Method: 从动态规划不动点方程构建策略，使用Q - learning，对折扣成本用经典技术证明收敛，对非折扣成本给出有限时间确定性博弈收敛结果，提出对手信息探索策略。

Result: 对折扣成本证明Q - learning收敛，对非折扣成本给出有限时间确定性博弈收敛结果，对手信息探索策略能保证最终Q函数的安全水平。

Conclusion: 所提出的方法在多智能体游戏Atlatl中有效。

Abstract: This paper addresses zero-sum ``turn'' games, in which only one player can
make decisions at each state. We show that pure saddle-point state-feedback
policies for turn games can be constructed from dynamic programming fixed-point
equations for a single value function or Q-function. These fixed-points can be
constructed using a suitable form of Q-learning. For discounted costs,
convergence of this form of Q-learning can be established using classical
techniques. For undiscounted costs, we provide a convergence result that
applies to finite-time deterministic games, which we use to illustrate our
results. For complex games, the Q-learning iteration must be terminated before
exploring the full-state, which can lead to policies that cannot guarantee the
security levels implied by the final Q-function. To mitigate this, we propose
an ``opponent-informed'' exploration policy for selecting the Q-learning
samples. This form of exploration can guarantee that the final Q-function
provides security levels that hold, at least, against a given set of policies.
A numerical demonstration for a multi-agent game, Atlatl, indicates the
effectiveness of these methods.

</details>


### [225] [A novel approach of day-ahead cooling load prediction and optimal control for ice-based thermal energy storage (TES) system in commercial buildings](https://arxiv.org/abs/2509.13371)
*Xuyuan Kang,Xiao Wang,Jingjing An,Da Yan*

Main category: eess.SY

TL;DR: 本文提出一种用于商业建筑冰蓄冷系统的集成负荷预测与优化控制方法，应用于实际项目取得良好效果，提高了制冷系统效率和自动化程度。


<details>
  <summary>Details</summary>
Motivation: 现有冰蓄冷系统多按固定时间表运行，无法充分利用负荷转移能力，需要深入研究和优化。

Method: 开发冷却负荷预测模型并引入午间修正机制，基于预测结果根据分时电价提出基于规则的控制策略，同时引入午间控制调整机制。

Result: 在北京某商业综合体冰蓄冷系统应用中，平均绝对误差为389kW，平均绝对误差变异系数为12.5%，基于预测的集成控制策略实现9.9%的能源成本节约率。

Conclusion: 所提出的模型部署在实际建筑自动化系统中，显著提高了制冷系统的效率和自动化程度。

Abstract: Thermal energy storage (TES) is an effective method for load shifting and
demand response in buildings. Optimal TES control and management are essential
to improve the performance of the cooling system. Most existing TES systems
operate on a fixed schedule, which cannot take full advantage of its load
shifting capability, and requires extensive investigation and optimization.
This study proposed a novel integrated load prediction and optimized control
approach for ice-based TES in commercial buildings. A cooling load prediction
model was developed and a mid-day modification mechanism was introduced into
the prediction model to improve the accuracy. Based on the predictions, a
rule-based control strategy was proposed according to the time-of-use tariff;
the mid-day control adjustment mechanism was introduced in accordance with the
mid-day prediction modifications. The proposed approach was applied in the
ice-based TES system of a commercial complex in Beijing, and achieved a mean
absolute error (MAE) of 389 kW and coefficient of variance of MAE of 12.5%. The
integrated prediction-based control strategy achieved an energy cost saving
rate of 9.9%. The proposed model was deployed in the realistic building
automation system of the case building and significantly improved the
efficiency and automation of the cooling system.

</details>


### [226] [Large Language Model-Empowered Decision Transformer for UAV-Enabled Data Collection](https://arxiv.org/abs/2509.13934)
*Zhixion Chen,Jiangzhou Wang,and Hyundong Shin,Arumugam Nallanathan*

Main category: eess.SY

TL;DR: 本文提出LLM - CRDT框架解决无人机轨迹规划和资源分配问题，提升数据收集能效，模拟显示其优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 无人机续航和通信范围有限需智能轨迹规划，传统强化学习有成本和风险，离线强化学习训练不稳定且依赖优质数据集。

Method: 将资源分配子问题转化为线性规划求解，提出LLM - CRDT框架，整合批评网络正则化训练，用预训练大语言模型作骨干并采用LoRA微调策略。

Result: 广泛模拟表明，LLM - CRDT优于基准在线和离线强化学习方法，比现有DT方法能效最高提升36.7%。

Conclusion: LLM - CRDT能有效解决无人机轨迹规划和资源分配问题，提升数据收集能效。

Abstract: The deployment of unmanned aerial vehicles (UAVs) for reliable and
energy-efficient data collection from spatially distributed devices holds great
promise in supporting diverse Internet of Things (IoT) applications.
Nevertheless, the limited endurance and communication range of UAVs necessitate
intelligent trajectory planning. While reinforcement learning (RL) has been
extensively explored for UAV trajectory optimization, its interactive nature
entails high costs and risks in real-world environments. Offline RL mitigates
these issues but remains susceptible to unstable training and heavily rely on
expert-quality datasets. To address these challenges, we formulate a joint UAV
trajectory planning and resource allocation problem to maximize energy
efficiency of data collection. The resource allocation subproblem is first
transformed into an equivalent linear programming formulation and solved
optimally with polynomial-time complexity. Then, we propose a large language
model (LLM)-empowered critic-regularized decision transformer (DT) framework,
termed LLM-CRDT, to learn effective UAV control policies. In LLM-CRDT, we
incorporate critic networks to regularize the DT model training, thereby
integrating the sequence modeling capabilities of DT with critic-based value
guidance to enable learning effective policies from suboptimal datasets.
Furthermore, to mitigate the data-hungry nature of transformer models, we
employ a pre-trained LLM as the transformer backbone of the DT model and adopt
a parameter-efficient fine-tuning strategy, i.e., LoRA, enabling rapid
adaptation to UAV control tasks with small-scale dataset and low computational
overhead. Extensive simulations demonstrate that LLM-CRDT outperforms benchmark
online and offline RL methods, achieving up to 36.7\% higher energy efficiency
than the current state-of-the-art DT approaches.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [227] [In-between Transatlantic (Monetary) Disturbances](https://arxiv.org/abs/2509.13578)
*Santiago Camara,Jeanne Aublin*

Main category: econ.GN

TL;DR: 研究欧央行和美联储利率冲击对加拿大经济溢出效应，发现两者影响不同，加拿大通过金融和贸易市场受国外货币政策影响。


<details>
  <summary>Details</summary>
Motivation: 研究欧央行利率冲击对加拿大经济的溢出效应，并与美联储的冲击进行比较。

Method: 结合VAR模型和局部投影回归，采用识别策略消除政策公告周围的信息效应。

Result: 欧央行加息使加元贬值、经济活动收缩，主要通过国际贸易传导；美联储冲击显著收紧加拿大金融条件，对贸易流动影响有限。

Conclusion: 加拿大通过全球金融和贸易市场的整合，直接和间接受国外货币政策影响。

Abstract: This paper studies the spillovers of European Central Bank (ECB) interest
rate shocks into the Canadian economy and compares them with those of the U.S.
Federal Reserve (Fed). We combine a VAR model and local projection regressions
with identification strategies that explicitly purge information effects around
policy announcements. We find that an ECB rate hike leads to a depreciation of
the Canadian dollar and a sharp contraction in economic activity. The main
transmission channel is international trade: ECB shocks trigger a decline in
oil prices and exports, while leaving domestic financial conditions largely
unaffected. By contrast, Fed shocks tighten Canadian financial conditions
significantly, with more limited effects on trade flows. These findings show
that Canada is exposed to foreign monetary policy both directly and indirectly,
through its integration in global financial and trade markets.

</details>


### [228] [Deep Learning in the Sequence Space](https://arxiv.org/abs/2509.13623)
*Marlon Azinovic-Yang,Jan Žemlička*

Main category: econ.GN

TL;DR: 开发深度学习算法在序列空间近似动态随机经济的函数理性预期均衡，测试不同复杂度经济体并展示设计保证单调性的神经网络策略函数架构。


<details>
  <summary>Details</summary>
Motivation: 寻找在序列空间近似动态随机经济的函数理性预期均衡的方法。

Method: 使用深度神经网络将经济的均衡对象参数化为外生冲击截断历史的函数，并沿着经济模拟路径训练神经网络以满足所有均衡条件。

Result: 解决了三个复杂度递增的经济体，展示了设计保证单调性的神经网络策略函数架构。

Conclusion: 所开发的算法可用于近似动态随机经济的函数理性预期均衡，设计的架构有助于简化算法。

Abstract: We develop a deep learning algorithm for approximating functional rational
expectations equilibria of dynamic stochastic economies in the sequence space.
We use deep neural networks to parameterize equilibrium objects of the economy
as a function of truncated histories of exogenous shocks. We train the neural
networks to fulfill all equilibrium conditions along simulated paths of the
economy. To illustrate the performance of our method, we solve three economies
of increasing complexity: the stochastic growth model, a high-dimensional
overlapping generations economy with multiple sources of aggregate risk, and
finally an economy where households and firms face uninsurable idiosyncratic
risk, shocks to aggregate productivity, and shocks to idiosyncratic and
aggregate volatility. Furthermore, we show how to design practical neural
policy function architectures that guarantee monotonicity of the predicted
policies, facilitating the use of the endogenous grid method to simplify parts
of our algorithm.

</details>


### [229] [Can the decoy effect increase cooperation in networks? An experiment](https://arxiv.org/abs/2509.13887)
*Claudia Cerrone,Francesco Feri,Anita Gantner,Paolo Pin*

Main category: econ.GN

TL;DR: 研究诱饵效应（吸引效应）能否促进社交网络中的合作，实验表明引入劣势选项能增加目标选择，该效应在个体和网络中均存在。


<details>
  <summary>Details</summary>
Motivation: 探究诱饵效应（吸引效应）是否能促进社交网络中的合作。

Method: 进行实验室实验。

Result: 引入劣势选项增加了目标选择，该效应在个体设置中更强，在网络中虽有搭便车动机仍存在，且随决策者战略位置而变化。

Conclusion: 诱饵效应（吸引效应）在一定程度上能促进社交网络中的合作。

Abstract: This paper investigates whether the decoy effect - specifically the
attraction effect - can foster cooperation in social networks. In a lab
experiment, we show that introducing a dominated option increases the selection
of the target choice, especially in early decisions. The effect is stronger in
individual settings but persists in networks despite free-riding incentives,
with variation depending on the decision-maker's strategic position.

</details>


### [230] [Machines are more productive than humans until they aren't, and vice versa](https://arxiv.org/abs/2509.14057)
*Riccardo Zanardelli*

Main category: econ.GN

TL;DR: 本文构建计算机模拟框架分析人机技能经济影响，发现自动化适用于低中复杂度任务，人机结合在高泛化场景有效但需实现增强，决策者应致力于实现增强。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能技能发展，组织面临基于经济原则优化技能政策决策的挑战，需解决该问题的复杂性。

Method: 基于蒙特卡罗模拟开发计算机模拟框架，分析人机技能单独或联合执行不同复杂度任务的经济影响。

Result: 自动化在低中复杂度任务最具经济效益，在复杂场景人机结合可能最有效，但需实现增强，否则经济表现最差。

Conclusion: 简单分配人机技能不足，人机技能政策需组织致力于实现增强，提高机器技能成本效益不能替代对增强的关注。

Abstract: With the growth of artificial skills, organizations may increasingly confront
with the problem of optimizing skill policy decisions guided by economic
principles. This paper addresses the underlying complexity of this challenge by
developing an in-silico framework based on Monte Carlo simulations grounded in
empirical realism to analyze the economic impact of human and machine skills,
individually or jointly deployed, in the execution of tasks presenting varying
levels of complexity. Our results provide quantitative support for the
established notions that automation tends to be the most economically-effective
strategy for tasks characterized by low-to-medium generalization difficulty,
while automation struggles to match the economic utility of human skills in
more complex scenarios. Critically, our simulations highlight that combining
human and machine skills can be the most effective strategy when a high level
of generalization is required, but only if genuine augmentation is achieved. In
contrast, when failing to realize this synergy, the human-machine policy is
severely penalized by the inherent costs of its dual skill structure, causing
it to destroy value and becoming the worst choice from an economic perspective.
The takeaway for decision-makers is unambiguous: simply allocating human and
machine skills to a task is insufficient, and a human-machine skill policy is
neither a silver-bullet solution nor a low-risk compromise. Rather, it is a
critical opportunity to boost competitiveness that demands a strong
organizational commitment to enabling augmentation. Also, our findings show
that improving the cost-effectiveness of machine skills over time, while
useful, does not replace the fundamental need to focus on achieving
augmentation.

</details>


### [231] [Incentivizing High Quality Entrants When Creators Are Strategic](https://arxiv.org/abs/2509.14102)
*Felicia Nguyen*

Main category: econ.GN

TL;DR: 研究平台在创作者发布前策略性选择质量时如何设计早期曝光和奖励，得出三个主要结果并给出可操作框架解决创作者冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 探讨平台在创作者策略性选择质量的情况下，如何设计早期曝光和奖励机制。

Method: 以短测试窗口的通过/失败标准诱导通过概率，将其斜率作为激励的关键统计量。

Result: 一是得出封闭形式的“可实施赏金”能使创作者和平台目标一致；二是提前分配保证曝光量是增强激励的最有效方式；三是预算受限下最优政策遵循等边际价值规则。

Conclusion: 该框架易于操作，为平台解决创作者冷启动问题和培养高质量内容供应提供直接、可管理解释的解决方案。

Abstract: We study how a platform should design early exposure and rewards when
creators strategically choose quality before release. A short testing window
with a pass/fail bar induces a pass probability, the slope of which is the key
sufficient statistic for incentives. We derive three main results. First, a
closed-form ``implementability bounty'' can perfectly align creator and
platform objectives, correcting for incomplete revenue sharing. Second,
front-loading guaranteed impressions is the most effective way to strengthen
incentives for a given attention budget. Third, when impression and cash
budgets are constrained, the optimal policy follows an equal-marginal-value
rule based on the prize spread and certain exposure. We map realistic ranking
engines (e.g., Thompson sampling) into the model's parameters and provide
telemetry-based estimators. The framework is simple to operationalize and
offers a direct, managerially interpretable solution for platforms to solve the
creator cold-start problem and cultivate high-quality supply.

</details>


### [232] [Minimum pricing or volumetric taxation? Quantity, quality and competition effects of price regulations in alcohol markets](https://arxiv.org/abs/2509.14116)
*Celine Bonnet,Fabrice Etile,Sebastien Lecocq*

Main category: econ.GN

TL;DR: 评估法国不同酒精定价政策影响，发现最低单位价格（MUP）政策在减少乙醇购买和惠及中小企业方面表现更佳。


<details>
  <summary>Details</summary>
Motivation: 葡萄酒生产国改革酒精价格监管具有挑战性，当前政策未考虑公共健康，需评估不同定价政策影响。

Method: 开发微观基础局部均衡模型，基于家庭扫描数据校准，比较乙醇体积税和MUP政策影响。

Result: MUP政策在减少乙醇购买上优于税收改革，尤其对重度饮酒家庭；增加中小企业利润，减少大型制造商和零售商利润，维持税收稳定。

Conclusion: MUP政策可作为减少有害消费、惠及中小企业的目标策略，为葡萄酒生产国酒精定价政策提供事前证据。

Abstract: Reforming alcohol price regulations in wine-producing countries is
challenging, as current price regulations reflect the alignment of cultural
preferences with economic interests rather than public health concerns. We
evaluate and compare the impact of counterfactual alcohol pricing policies on
consumer behaviors, firms, and markets in France. We develop a micro-founded
partial equilibrium model that accounts for consumer preferences over purchase
volumes across alcohol categories and over product quality within categories,
and for firms' strategic price-setting. After calibration on household scanner
data, we compare the impacts of replacing current taxes by ethanol-based
volumetric taxes with a minimum unit price (MUP) policy of 0.50 Euro per
standard drink. The results show that the MUP in addition to the current tax
outperforms a tax reform in reducing ethanol purchases (-15% vs. -10% for
progressive taxation), especially among heavy drinking households (-17%). The
MUP increases the profits of small and medium wine firms (+39%) while
decreasing the profits of large manufacturers and retailers (-39%) and
maintaining tax revenues stable. The results support the MUP as a targeted
strategy to reduce harmful consumption while benefiting small and medium wine
producers. This study provides ex-ante evidence that is crucial for alcohol
pricing policies in wine-producing countries.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [233] [Field of View Enhanced Signal Dependent Binauralization with Mixture of Experts Framework for Continuous Source Motion](https://arxiv.org/abs/2509.13548)
*Manan Mittal,Thomas Deppisch,Joseph Forrer,Chris Le Sueur,Zamir Ben-Hur,David Lou Along,Daniel D. E. Wong*

Main category: cs.SD

TL;DR: 提出用于双耳信号匹配中视场增强的新型专家混合框架，能动态渲染空间音频，支持多应用且灵活。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在双耳信号匹配视场增强方面的不足，实现适应连续说话者运动的动态空间音频渲染。

Method: 采用隐式定位在线组合多个双耳滤波器的信号依赖框架，且与阵列几何无关。

Result: 可实时跟踪和增强移动声源，支持语音聚焦、降噪等应用。

Conclusion: 该方法为下一代消费音频设备的空间音频捕获和个性化播放提供了灵活解决方案。

Abstract: We propose a novel mixture of experts framework for field-of-view enhancement
in binaural signal matching. Our approach enables dynamic spatial audio
rendering that adapts to continuous talker motion, allowing users to emphasize
or suppress sounds from selected directions while preserving natural binaural
cues. Unlike traditional methods that rely on explicit direction-of-arrival
estimation or operate in the Ambisonics domain, our signal-dependent framework
combines multiple binaural filters in an online manner using implicit
localization. This allows for real-time tracking and enhancement of moving
sound sources, supporting applications such as speech focus, noise reduction,
and world-locked audio in augmented and virtual reality. The method is agnostic
to array geometry offering a flexible solution for spatial audio capture and
personalized playback in next-generation consumer audio devices.

</details>


### [234] [A Domain Knowledge Informed Approach for Anomaly Detection of Electric Vehicle Interior Sounds](https://arxiv.org/abs/2509.13390)
*Deepti Kunte,Bram Cornelis,Claudio Colangeli,Karl Janssens,Brecht Van Baelen,Konstantinos Gryllias*

Main category: cs.SD

TL;DR: 提出一种基于领域知识的模型选择方法，用健康频谱图的代理异常进行模型选择，在电动汽车舱内声音数据集上实验显示该方法优于传统策略。


<details>
  <summary>Details</summary>
Motivation: 汽车舱内声音异常检测常为无监督学习，缺乏标注故障样本和可靠指标，有效模型选择是挑战。

Method: 提出领域知识驱动的模型选择方法，通过对健康频谱图进行结构化扰动生成代理异常用于验证集来支持模型选择。

Result: 在包含五种故障类型的电动汽车数据集上实验，使用代理异常能选出最优模型，显著优于传统模型选择策略。

Conclusion: 基于领域知识利用代理异常进行模型选择的方法在汽车舱内声音异常检测中有效，优于传统策略。

Abstract: The detection of anomalies in automotive cabin sounds is critical for
ensuring vehicle quality and maintaining passenger comfort. In many real-world
settings, this task is more appropriately framed as an unsupervised learning
problem rather than the supervised case due to the scarcity or complete absence
of labeled faulty data. In such an unsupervised setting, the model is trained
exclusively on healthy samples and detects anomalies as deviations from normal
behavior. However, in the absence of labeled faulty samples for validation and
the limited reliability of commonly used metrics, such as validation
reconstruction error, effective model selection remains a significant
challenge. To overcome these limitations, a domain-knowledge-informed approach
for model selection is proposed, in which proxy-anomalies engineered through
structured perturbations of healthy spectrograms are used in the validation set
to support model selection. The proposed methodology is evaluated on a
high-fidelity electric vehicle dataset comprising healthy and faulty cabin
sounds across five representative fault types viz., Imbalance, Modulation,
Whine, Wind, and Pulse Width Modulation. This dataset, generated using advanced
sound synthesis techniques, and validated via expert jury assessments, has been
made publicly available to facilitate further research. Experimental
evaluations on the five fault cases demonstrate the selection of optimal models
using proxy-anomalies, significantly outperform conventional model selection
strategies.

</details>


### [235] [RFM-Editing: Rectified Flow Matching for Text-guided Audio Editing](https://arxiv.org/abs/2509.14003)
*Liting Gao,Yi Yuan,Yaru Chen,Yuelan Cheng,Zhenbo Li,Juan Wen,Shubin Zhang,Wenwu Wang*

Main category: cs.SD

TL;DR: 提出基于整流流匹配的扩散框架用于音频编辑，构建数据集，实验显示模型效果好。


<details>
  <summary>Details</summary>
Motivation: 文本引导的音频编辑处于早期阶段，现有方法在复杂编辑上有困难或缺乏实用性。

Method: 提出端到端高效的基于整流流匹配的扩散框架进行音频编辑，构建含重叠多事件音频的数据集支持训练和基准测试。

Result: 模型无需辅助字幕或掩码就能实现忠实的语义对齐，各项指标编辑质量有竞争力。

Conclusion: 所提方法在文本引导音频编辑任务中具有有效性和优势。

Abstract: Diffusion models have shown remarkable progress in text-to-audio generation.
However, text-guided audio editing remains in its early stages. This task
focuses on modifying the target content within an audio signal while preserving
the rest, thus demanding precise localization and faithful editing according to
the text prompt. Existing training-based and zero-shot methods that rely on
full-caption or costly optimization often struggle with complex editing or lack
practicality. In this work, we propose a novel end-to-end efficient rectified
flow matching-based diffusion framework for audio editing, and construct a
dataset featuring overlapping multi-event audio to support training and
benchmarking in complex scenarios. Experiments show that our model achieves
faithful semantic alignment without requiring auxiliary captions or masks,
while maintaining competitive editing quality across metrics.

</details>


### [236] [Comprehensive Evaluation of CNN-Based Audio Tagging Models on Resource-Constrained Devices](https://arxiv.org/abs/2509.14049)
*Jordi Grau-Haro,Ruben Ribes-Serrano,Javier Naranjo-Alcazar,Marta Garcia-Ballesteros,Pedro Zuccarello*

Main category: cs.SD

TL;DR: 本文对树莓派上用于音频标签的多种CNN架构进行评估，将模型转换为ONNX格式，实验表明合适选型和优化可有效管理推理延迟和热行为。


<details>
  <summary>Details</summary>
Motivation: CNN在音频标签任务表现出色，但在树莓派等资源受限设备上部署存在计算效率和热管理挑战。

Method: 对PANNs框架的1D和2D模型、基于ConvNeXt的音频分类模型、MobileNetV3架构，以及CNN9和CNN13进行评估，将模型转换为ONNX格式，进行24小时连续推理测试。

Result: 通过合适的模型选择和优化，可在长时间内保持一致的推理延迟并有效管理热行为。

Conclusion: 研究结果为在现实边缘计算场景中部署音频标签模型提供了有价值的见解。

Abstract: Convolutional Neural Networks (CNNs) have demonstrated exceptional
performance in audio tagging tasks. However, deploying these models on
resource-constrained devices like the Raspberry Pi poses challenges related to
computational efficiency and thermal management. In this paper, a comprehensive
evaluation of multiple convolutional neural network (CNN) architectures for
audio tagging on the Raspberry Pi is conducted, encompassing all 1D and 2D
models from the Pretrained Audio Neural Networks (PANNs) framework, a
ConvNeXt-based model adapted for audio classification, as well as MobileNetV3
architectures. In addition, two PANNs-derived networks, CNN9 and CNN13,
recently proposed, are also evaluated. To enhance deployment efficiency and
portability across diverse hardware platforms, all models are converted to the
Open Neural Network Exchange (ONNX) format. Unlike previous works that focus on
a single model, our analysis encompasses a broader range of architectures and
involves continuous 24-hour inference sessions to assess performance stability.
Our experiments reveal that, with appropriate model selection and optimization,
it is possible to maintain consistent inference latency and manage thermal
behavior effectively over extended periods. These findings provide valuable
insights for deploying audio tagging models in real-world edge computing
scenarios.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [237] [A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval Prediction For Instruction Caching](https://arxiv.org/abs/2509.14041)
*Henry Kao,Nikhil Sreekumar,Prabhdeep Singh Soni,Ali Sedaghati,Fang Su,Bryan Chan,Maziar Goudarzi,Reza Azimi*

Main category: cs.AR

TL;DR: 提出名为TRRIP的软硬件协同设计方法优化指令缓存替换策略，可降低L2 MPKI并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代移动CPU软件因复杂运行行为对传统指令缓存替换策略带来挑战，传统硬件方法管理指令缓存不足。

Method: 编译器基于代码“温度”分析、分类和转换代码，通过操作系统接口向硬件提供代码温度信息，硬件扩展利用温度属性优化指令缓存替换策略。

Result: TRRIP可使L2 MPKI降低26.5%，在已使用PGO优化的移动代码中实现3.9%的几何平均加速。

Conclusion: TRRIP实用且适用于有严格软硬件要求的真实移动系统，能有效优化指令缓存替换策略。

Abstract: Modern mobile CPU software pose challenges for conventional instruction cache
replacement policies due to their complex runtime behavior causing high reuse
distance between executions of the same instruction. Mobile code commonly
suffers from large amounts of stalls in the CPU frontend and thus starvation of
the rest of the CPU resources. Complexity of these applications and their code
footprint are projected to grow at a rate faster than available on-chip memory
due to power and area constraints, making conventional hardware-centric methods
for managing instruction caches to be inadequate. We present a novel
software-hardware co-design approach called TRRIP (Temperature-based
Re-Reference Interval Prediction) that enables the compiler to analyze,
classify, and transform code based on "temperature" (hot/cold), and to provide
the hardware with a summary of code temperature information through a
well-defined OS interface based on using code page attributes. TRRIP's
lightweight hardware extension employs code temperature attributes to optimize
the instruction cache replacement policy resulting in the eviction rate
reduction of hot code. TRRIP is designed to be practical and adoptable in real
mobile systems that have strict feature requirements on both the software and
hardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%
resulting in geomean speedup of 3.9%, on top of RRIP cache replacement running
mobile code already optimized using PGO.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [238] [A Closeness Centrality-based Circuit Partitioner for Quantum Simulations](https://arxiv.org/abs/2509.14098)
*Doru Thom Popovici,Harlin Lee,Mauro Del Ben,Naoki Yoshioka,Nobuyasu Ito,Katherine Klymko,Daan Camps,Anastasiia Butko*

Main category: quant-ph

TL;DR: 介绍了用于大规模量子电路模拟的端到端框架，可减少计算节点间数据移动。


<details>
  <summary>Details</summary>
Motivation: 当前量子电路模拟需大量资源，需大型集群和大内存，需高效解决方案。

Method: 将量子态和电路分布作为图问题，用紧密中心性评估门的重要性，设计分区方法，将分区编译为优化代码。

Result: 框架能在多种超级计算机上无缝运行。

Conclusion: 框架为量子算法模拟的性能和可扩展性提供关键见解。

Abstract: Simulating quantum circuits (QC) on high-performance computing (HPC) systems
has become an essential method to benchmark algorithms and probe the potential
of large-scale quantum computation despite the limitations of current quantum
hardware. However, these simulations often require large amounts of resources,
necessitating the use of large clusters with thousands of compute nodes and
large memory footprints. In this work, we introduce an end-to-end framework
that provides an efficient partitioning scheme for large-scale QCs alongside a
flexible code generator to offer a portable solution that minimizes data
movement between compute nodes. By formulating the distribution of quantum
states and circuits as a graph problem, we apply closeness centrality to assess
gate importance and design a fast, scalable partitioning method. The resulting
partitions are compiled into highly optimized codes that run seamlessly on a
wide range of supercomputers, providing critical insights into the performance
and scalability of quantum algorithm simulations.

</details>


### [239] [Learning quantum many-body data locally: A provably scalable framework](https://arxiv.org/abs/2509.13705)
*Koki Chinzei,Quoc Hoan Tran,Norifumi Matsumoto,Yasuhiro Endo,Hirotaka Oshima*

Main category: quant-ph

TL;DR: 提出可扩展机器学习框架GLQK学习量子多体实验数据，证明其在样本复杂度上的优势并数值验证可扩展性，为理解量子多体物理提供新途径。


<details>
  <summary>Details</summary>
Motivation: 机器学习处理量子多体数据有潜力，但解决大规模问题受近期限量子设备计算资源限制。

Method: 提出Geometrically Local Quantum Kernel (GLQK)框架，利用非临界系统中相关性指数衰减的特性，从相关长度尺度的局部量子信息构建特征空间。

Result: 在学习量子期望值的未知多项式任务中，GLQK相比现有影子核大幅提高了关于量子比特数n的多项式样本复杂度；对于平移对称数据，实现与n无关的恒定样本复杂度；在两个量子多体现象学习任务中数值验证了高可扩展性。

Conclusion: 研究结果为利用实验数据推进对量子多体物理的理解开辟了新途径。

Abstract: Machine learning (ML) holds great promise for extracting insights from
complex quantum many-body data obtained in quantum experiments. This approach
can efficiently solve certain quantum problems that are classically
intractable, suggesting potential advantages of harnessing quantum data.
However, addressing large-scale problems still requires significant amounts of
data beyond the limited computational resources of near-term quantum devices.
We propose a scalable ML framework called Geometrically Local Quantum Kernel
(GLQK), designed to efficiently learn quantum many-body experimental data by
leveraging the exponential decay of correlations, a phenomenon prevalent in
noncritical systems. In the task of learning an unknown polynomial of quantum
expectation values, we rigorously prove that GLQK substantially improves
polynomial sample complexity in the number of qubits $n$, compared to the
existing shadow kernel, by constructing a feature space from local quantum
information at the correlation length scale. This improvement is particularly
notable when each term of the target polynomial involves few local subsystems.
Remarkably, for translationally symmetric data, GLQK achieves constant sample
complexity, independent of $n$. We numerically demonstrate its high scalability
in two learning tasks on quantum many-body phenomena. These results establish
new avenues for utilizing experimental data to advance the understanding of
quantum many-body physics.

</details>


### [240] [Learning Minimal Representations of Many-Body Physics from Snapshots of a Quantum Simulator](https://arxiv.org/abs/2509.13821)
*Frederik Møller,Gabriel Fernández-Fernández,Thomas Schweigler,Paulin de Schoulepnikoff,Jörg Schmiedmayer,Gorka Muñoz-Gil*

Main category: quant-ph

TL;DR: 提出基于VAE的机器学习方法分析一维玻色气体干扰测量数据，能从噪声稀疏数据提取物理变量，补充场论技术。


<details>
  <summary>Details</summary>
Motivation: 传统方法在从模拟量子模拟器实验数据提取物理见解时受测量噪声、有限可观测量和模型知识不足的阻碍。

Method: 开发基于变分自编码器（VAE）的机器学习方法，无监督训练。

Result: VAE学习到与系统平衡控制参数强相关的潜在表示，揭示非平衡协议中孤子特征和反常淬火后动力学。

Conclusion: 生成模型可从噪声稀疏实验数据提取可物理解释变量，机器学习能补充场论技术，推动量子多体系统数据驱动发现。

Abstract: Analog quantum simulators provide access to many-body dynamics beyond the
reach of classical computation. However, extracting physical insights from
experimental data is often hindered by measurement noise, limited observables,
and incomplete knowledge of the underlying microscopic model. Here, we develop
a machine learning approach based on a variational autoencoder (VAE) to analyze
interference measurements of tunnel-coupled one-dimensional Bose gases, which
realize the sine-Gordon quantum field theory. Trained in an unsupervised
manner, the VAE learns a minimal latent representation that strongly correlates
with the equilibrium control parameter of the system. Applied to
non-equilibrium protocols, the latent space uncovers signatures of frozen-in
solitons following rapid cooling, and reveals anomalous post-quench dynamics
not captured by conventional correlation-based methods. These results
demonstrate that generative models can extract physically interpretable
variables directly from noisy and sparse experimental data, providing
complementary probes of equilibrium and non-equilibrium physics in quantum
simulators. More broadly, our work highlights how machine learning can
supplement established field-theoretical techniques, paving the way for
scalable, data-driven discovery in quantum many-body systems.

</details>


### [241] [Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks](https://arxiv.org/abs/2509.14026)
*Jiun-Cheng Jiang,Morris Yu-Chao Huang,Tianlong Chen,Hsi-Sheng Goan*

Main category: quant-ph

TL;DR: 提出量子变分激活函数QVAFs（通过DARUANs实现），嵌入KANs得到QKANs，引入新技术增强可扩展性，实验证明其效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 结合变分量子电路和Kolmogorov - Arnold网络中可学习激活函数的优势，推动量子机器学习发展。

Method: 引入QVAFs和DARUANs，将DARUAN嵌入KANs得到QKANs，还采用层扩展和混合QKANs技术。

Result: DARUAN频率谱随数据重复指数增长，可指数级减少参数大小；QKANs保持可解释性，提高参数效率、表达能力和泛化能力；实验证明其在多任务中的效率和可扩展性。

Conclusion: DARUANs和QKANs为在NISQ硬件和经典量子模拟器上推进量子机器学习提供了有前景的方向。

Abstract: Variational quantum circuits (VQCs) are central to quantum machine learning,
while recent progress in Kolmogorov-Arnold networks (KANs) highlights the power
of learnable activation functions. We unify these directions by introducing
quantum variational activation functions (QVAFs), realized through single-qubit
data re-uploading circuits called DatA Re-Uploading ActivatioNs (DARUANs). We
show that DARUAN with trainable weights in data pre-processing possesses an
exponentially growing frequency spectrum with data repetitions, enabling an
exponential reduction in parameter size compared with Fourier-based activations
without loss of expressivity. Embedding DARUAN into KANs yields
quantum-inspired KANs (QKANs), which retain the interpretability of KANs while
improving their parameter efficiency, expressivity, and generalization. We
further introduce two novel techniques to enhance scalability, feasibility and
computational efficiency, such as layer extension and hybrid QKANs (HQKANs) as
drop-in replacements of multi-layer perceptrons (MLPs) for feed-forward
networks in large-scale models. We provide theoretical analysis and extensive
experiments on function regression, image classification, and autoregressive
generative language modeling, demonstrating the efficiency and scalability of
QKANs. DARUANs and QKANs offer a promising direction for advancing quantum
machine learning on both noisy intermediate-scale quantum (NISQ) hardware and
classical quantum simulators.

</details>


### [242] [Quantum Reinforcement Learning-Guided Diffusion Model for Image Synthesis via Hybrid Quantum-Classical Generative Model Architectures](https://arxiv.org/abs/2509.14163)
*Chi-Sheng Chen,En-Jui Kuo*

Main category: quant-ph

TL;DR: 提出量子强化学习控制器动态调整分类器自由引导（CFG），在CIFAR - 10实验中提升感知质量并减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型使用的静态或启发式CFG调度难以适应不同时间步长和噪声条件。

Method: 采用混合量子 - 经典演员 - 评论家架构，用变分量子电路生成策略特征，多层感知机映射动作，使用PPO和GAE优化策略，由平衡多种因素的奖励引导。

Result: 在CIFAR - 10上实验显示提升了感知质量（LPIPS、PSNR、SSIM），减少了参数数量，消融实验揭示了准确性和效率的权衡，扩展评估证实长扩散调度下生成的鲁棒性。

Conclusion: 所提出的QRL策略有效，能动态调整CFG，在性能和效率上有优势。

Abstract: Diffusion models typically employ static or heuristic classifier-free
guidance (CFG) schedules, which often fail to adapt across timesteps and noise
conditions. In this work, we introduce a quantum reinforcement learning (QRL)
controller that dynamically adjusts CFG at each denoising step. The controller
adopts a hybrid quantum--classical actor--critic architecture: a shallow
variational quantum circuit (VQC) with ring entanglement generates policy
features, which are mapped by a compact multilayer perceptron (MLP) into
Gaussian actions over $\Delta$CFG, while a classical critic estimates value
functions. The policy is optimized using Proximal Policy Optimization (PPO)
with Generalized Advantage Estimation (GAE), guided by a reward that balances
classification confidence, perceptual improvement, and action regularization.
Experiments on CIFAR-10 demonstrate that our QRL policy improves perceptual
quality (LPIPS, PSNR, SSIM) while reducing parameter count compared to
classical RL actors and fixed schedules. Ablation studies on qubit number and
circuit depth reveal trade-offs between accuracy and efficiency, and extended
evaluations confirm robust generation under long diffusion schedules.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [243] [A Geometric Graph-Based Deep Learning Model for Drug-Target Affinity Prediction](https://arxiv.org/abs/2509.13476)
*Md Masud Rana,Farjana Tasnim Mukta,Duc D. Nguyen*

Main category: q-bio.BM

TL;DR: 本文介绍了用于基于结构的药物设计中预测结合亲和力的深度学习模型DeepGGL，该模型在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 在基于结构的药物设计中，准确估计候选配体与蛋白质受体之间的结合亲和力是核心挑战，现有方法有局限，需更优方法。

Method: 引入DeepGGL，在几何图学习框架中集成残差连接和注意力机制，利用多尺度加权彩色二分图子图捕获相互作用。

Result: 在CASF - 2013和CASF - 2016上达到了最先进的性能，在多个评估指标上有显著改进；在CSAR - NRC - HiQ数据集和PDBbind v2019保留集上保持高预测准确性。

Conclusion: DeepGGL具有适应性和可靠性，可用于基于结构的药物发现中结合亲和力预测。

Abstract: In structure-based drug design, accurately estimating the binding affinity
between a candidate ligand and its protein receptor is a central challenge.
Recent advances in artificial intelligence, particularly deep learning, have
demonstrated superior performance over traditional empirical and physics-based
methods for this task, enabled by the growing availability of structural and
experimental affinity data. In this work, we introduce DeepGGL, a deep
convolutional neural network that integrates residual connections and an
attention mechanism within a geometric graph learning framework. By leveraging
multiscale weighted colored bipartite subgraphs, DeepGGL effectively captures
fine-grained atom-level interactions in protein-ligand complexes across
multiple scales. We benchmarked DeepGGL against established models on CASF-2013
and CASF-2016, where it achieved state-of-the-art performance with significant
improvements across diverse evaluation metrics. To further assess robustness
and generalization, we tested the model on the CSAR-NRC-HiQ dataset and the
PDBbind v2019 holdout set. DeepGGL consistently maintained high predictive
accuracy, highlighting its adaptability and reliability for binding affinity
prediction in structure-based drug discovery.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [244] [Dual Actor DDPG for Airborne STAR-RIS Assisted Communications](https://arxiv.org/abs/2509.13328)
*Danish Rizvi,David Boyle*

Main category: eess.SP

TL;DR: 本文研究基于耦合TRC相移模型的Aerial - STAR多用户下行通信系统，提出DA - DDPG算法和HFI奖励函数，仿真显示其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 打破现有STAR - RIS研究中传输和反射系数独立的假设，提升通信效率并考虑无人机能量约束。

Method: 联合优化无人机轨迹、基站有源波束赋形向量和无源RIS的TRC；将TRC设计为离散和连续动作组合；提出DA - DDPG算法和HFI奖励函数；研究RIS尺寸对无人机空气动力学的影响。

Result: DA - DDPG算法在累积奖励上优于传统DDPG和DQN解决方案；三维无人机轨迹优化通信效率更高；HFI奖励函数降低QoS拒绝率；移动Aerial - STAR系统性能更佳。

Conclusion: Aerial - STAR系统有潜力，提出的DA - DDPG方法能有效优化其性能。

Abstract: This study departs from the prevailing assumption of independent Transmission
and Reflection Coefficients (TRC) in Airborne Simultaneous Transmit and Reflect
Reconfigurable Intelligent Surface (STAR-RIS) research. Instead, we explore a
novel multi-user downlink communication system that leverages a UAV-mounted
STAR-RIS (Aerial-STAR) incorporating a coupled TRC phase shift model. Our key
contributions include the joint optimization of UAV trajectory, active
beamforming vectors at the base station, and passive RIS TRCs to enhance
communication efficiency, while considering UAV energy constraints. We design
the TRC as a combination of discrete and continuous actions, and propose a
novel Dual Actor Deep Deterministic Policy Gradient (DA-DDPG) algorithm. The
algorithm relies on two separate actor networks for high-dimensional hybrid
action space. We also propose a novel harmonic mean index (HFI)-based reward
function to ensure communication fairness amongst users. For comprehensive
analysis, we study the impact of RIS size on UAV aerodynamics showing that it
increases drag and energy demand. Simulation results demonstrate that the
proposed DA-DDPG algorithm outperforms conventional DDPG and DQN-based
solutions by 24% and 97%, respectively, in accumulated reward.
Three-dimensional UAV trajectory optimization achieves 28% higher communication
efficiency compared to two-dimensional and altitude optimization. The HFI based
reward function provides 41% lower QoS denial rates as compared to other
benchmarks. The mobile Aerial-STAR system shows superior performance over fixed
deployed counterparts, with the coupled phase STAR-RIS outperforming dual
Transmit/Reflect RIS and conventional RIS setups. These findings highlight the
potential of Aerial-STAR systems and the effectiveness of our proposed DA-DDPG
approach in optimizing their performance.

</details>


### [245] [Self-Supervised and Topological Signal-Quality Assessment for Any PPG Device](https://arxiv.org/abs/2509.12510)
*Wei Shao,Ruoyu Zhang,Zequan Liang,Ehsan Kourkchi,Setareh Rafatirad,Houman Homayoun*

Main category: eess.SP

TL;DR: 提出首个用于手腕PPG的完全无监督信号质量评估（SQA）管道，结合自监督学习和拓扑数据分析，可作为PPG信号的质量门。


<details>
  <summary>Details</summary>
Motivation: 现有PPG信号质量评估方法要么依赖不可靠的启发式方法，要么依赖数据量大的监督模型，而PPG光学波形易受多种因素干扰。

Method: 第一阶段在276小时原始未标记数据上训练对比式1 - D ResNet - 18得到不变嵌入；第二阶段通过持久同调将嵌入转换为拓扑签名并聚类，以产生二进制信号质量指数。

Result: 在10000个窗口的分层样本上，信号质量指数的轮廓系数、戴维斯 - 布尔丁指数和卡林斯基 - 哈拉巴斯指数分别达到0.72、0.34和6173。

Conclusion: 所提出的混合自监督学习 - 拓扑数据分析框架可作为PPG信号的可扩展、跨设备质量门。

Abstract: Wearable photoplethysmography (PPG) is embedded in billions of devices, yet
its optical waveform is easily corrupted by motion, perfusion loss, and ambient
light, jeopardizing downstream cardiometric analytics. Existing signal-quality
assessment (SQA) methods rely either on brittle heuristics or on data-hungry
supervised models. We introduce the first fully unsupervised SQA pipeline for
wrist PPG. Stage 1 trains a contrastive 1-D ResNet-18 on 276 h of raw,
unlabeled data from heterogeneous sources (varying in device and sampling
frequency), yielding optical-emitter- and motion-invariant embeddings (i.e.,
the learned representation is stable across differences in LED wavelength,
drive intensity, and device optics, as well as wrist motion). Stage 2 converts
each 512-D encoder embedding into a 4-D topological signature via persistent
homology (PH) and clusters these signatures with HDBSCAN. To produce a binary
signal-quality index (SQI), the acceptable PPG signals are represented by the
densest cluster while the remaining clusters are assumed to mainly contain
poor-quality PPG signals. Without re-tuning, the SQI attains Silhouette,
Davies-Bouldin, and Calinski-Harabasz scores of 0.72, 0.34, and 6173,
respectively, on a stratified sample of 10,000 windows. In this study, we
propose a hybrid self-supervised-learning--topological-data-analysis (SSL--TDA)
framework that offers a drop-in, scalable, cross-device quality gate for PPG
signals.

</details>


### [246] [Classification Filtering](https://arxiv.org/abs/2509.13975)
*Ilker Bayram*

Main category: eess.SP

TL;DR: 考虑含潜在类别的流信号，多分类器输出融合问题，提出状态空间模型和实时滤波器，在活动分类应用验证效果。


<details>
  <summary>Details</summary>
Motivation: 融合多分类器输出并结合时间因素以提高分类准确性。

Method: 提出状态空间模型，开发适用于实时执行的滤波器。

Result: 在基于可穿戴设备IMU数据的活动分类应用中，证明了所提滤波器的有效性。

Conclusion: 所提出的状态空间模型和实时滤波器能够有效解决多分类器输出融合问题，提高分类准确性。

Abstract: We consider a streaming signal in which each sample is linked to a latent
class. We assume that multiple classifiers are available, each providing class
probabilities with varying degrees of accuracy. These classifiers are employed
following a straightforward and fixed policy. In this setting, we consider the
problem of fusing the output of the classifiers while incorporating the
temporal aspect to improve classification accuracy. We propose a state-space
model and develop a filter tailored for realtime execution. We demonstrate the
effectiveness of the proposed filter in an activity classification application
based on inertial measurement unit (IMU) data from a wearable device.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [247] [TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models](https://arxiv.org/abs/2509.13395)
*Haolong Zheng,Yekaterina Yegorova,Mark Hasegawa-Johnson*

Main category: eess.AS

TL;DR: 提出TICL方法提升大模型语音识别能力，在多任务中表现好。


<details>
  <summary>Details</summary>
Motivation: 语音上下文学习中有效示例选择方法待探索，需提升模型语音识别能力。

Method: 提出Text - Embedding KNN for SICL (TICL)方法，用语义上下文提升模型语音识别能力且无需微调。

Result: 在多种自动语音识别任务中，模型超越零样本性能，相对WER最多降低84.7%。

Conclusion: 通过消融实验证明方法具有鲁棒性和高效性。

Abstract: Speech foundation models have recently demonstrated the ability to perform
Speech In-Context Learning (SICL). Selecting effective in-context examples is
crucial for SICL performance, yet selection methodologies remain underexplored.
In this work, we propose Text-Embedding KNN for SICL (TICL), a simple pipeline
that uses semantic context to enhance off-the-shelf large multimodal models'
speech recognition ability without fine-tuning. Across challenging automatic
speech recognition tasks, including accented English, multilingual speech, and
children's speech, our method enables models to surpass zero-shot performance
with up to 84.7% relative WER reduction. We conduct ablation studies to show
the robustness and efficiency of our method.

</details>


### [248] [DSpAST: Disentangled Representations for Spatial Audio Reasoning with Large Language Models](https://arxiv.org/abs/2509.13927)
*Kevin Wilkinghoff,Zheng-Hua Tan*

Main category: eess.AS

TL;DR: 提出基于SpatialAST的新型音频编码器DSpAST，参数仅增加0.2%，在SpatialSoundQA实验中显著优于SpatialAST。


<details>
  <summary>Details</summary>
Motivation: 单音频编码器完成空间音频推理困难，性能不如特定任务编码器，需更好的编码器。

Method: 提出基于SpatialAST的新型音频编码器DSpAST学习空间音频解耦表示。

Result: 在SpatialSoundQA上用空间音频推理系统BAT实验，DSpAST显著优于SpatialAST。

Conclusion: DSpAST在空间音频推理任务中表现更好，是更优的音频编码器。

Abstract: Reasoning about spatial audio with large language models requires a spatial
audio encoder as an acoustic front-end to obtain audio embeddings for further
processing. Such an encoder needs to capture all information required to detect
the type of sound events, as well as the direction and distance of their
corresponding sources. Accomplishing this with a single audio encoder is
demanding as the information required for each of these tasks is mostly
independent of each other. As a result, the performance obtained with a single
encoder is often worse than when using task-specific audio encoders. In this
work, we present DSpAST, a novel audio encoder based on SpatialAST that learns
disentangled representations of spatial audio while having only 0.2% additional
parameters. Experiments on SpatialSoundQA with the spatial audio reasoning
system BAT demonstrate that DSpAST significantly outperforms SpatialAST.

</details>


### [249] [Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection](https://arxiv.org/abs/2509.13878)
*Janne Laakkonen,Ivan Kukanov,Ville Hautamäki*

Main category: eess.AS

TL;DR: 提出混合LoRA专家方法用于音频深度伪造检测，实验显示该方法优于标准微调，能有效实现可泛化检测。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在微调后难以泛化到训练集中未涵盖的新型深度伪造方法，需要提升音频深度伪造检测的泛化能力。

Method: 提出混合LoRA专家方法，将多个低秩适配器（LoRA）集成到模型的注意力层，并通过路由机制选择性激活专家。

Result: 该方法在域内和域外场景均优于标准微调，降低了相对基线模型的等错误率，最佳模型将平均域外EER从8.55%降至6.08%。

Conclusion: 所提方法能有效实现可泛化的音频深度伪造检测。

Abstract: Foundation models such as Wav2Vec2 excel at representation learning in speech
tasks, including audio deepfake detection. However, after being fine-tuned on a
fixed set of bonafide and spoofed audio clips, they often fail to generalize to
novel deepfake methods not represented in training. To address this, we propose
a mixture-of-LoRA-experts approach that integrates multiple low-rank adapters
(LoRA) into the model's attention layers. A routing mechanism selectively
activates specialized experts, enhancing adaptability to evolving deepfake
attacks. Experimental results show that our method outperforms standard
fine-tuning in both in-domain and out-of-domain scenarios, reducing equal error
rates relative to baseline models. Notably, our best MoE-LoRA model lowers the
average out-of-domain EER from 8.55\% to 6.08\%, demonstrating its
effectiveness in achieving generalizable audio deepfake detection.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [250] [Prognosis of COVID-19 using Artificial Intelligence: A Systematic Review and Meta-analysis](https://arxiv.org/abs/2408.00208)
*SaeedReza Motamedian,Sadra Mohaghegh,Elham Babadi Oregani,Mahrsa Amjadi,Parnian Shobeiri,Negin Cheraghi,Niusha Solouki,Nikoo Ahmadi,Hossein Mohammad-Rahimi,Yassine Bouchareb,Arman Rahmim*

Main category: physics.med-ph

TL;DR: 该研究对利用AI进行COVID - 19预后的已发表研究进行识别、评估和综合，发现使用CT或CXR图像的机器学习和深度学习方法可助临床管理与资源分配，结合多类数据能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 识别、评估和综合已发表的关于使用AI进行COVID - 19预后的研究。

Method: 通过多个数据库进行电子搜索，纳入用CT或胸部X光图像结合机器学习或深度学习方法判断COVID - 19预后的研究，并计算相关指标。

Result: 共纳入36篇文章，研究多种预后相关问题，采用多种AI模型和架构，模型对死亡率、严重程度评估和通气需求的敏感性分别为71%、88%和67%，特异性分别为69%、89%和89%。

Conclusion: 基于纳入文章，用CT或CXR图像的放射组学特征进行COVID - 19患者预后的机器学习和深度学习方法可帮助临床管理和资源分配，结合多种数据能提升模型性能。

Abstract: Purpose: Artificial intelligence (AI) techniques have been extensively
utilized for diagnosing and prognosis of several diseases in recent years. This
study identifies, appraises and synthesizes published studies on the use of AI
for the prognosis of COVID-19. Method: Electronic search was performed using
Medline, Google Scholar, Scopus, Embase, Cochrane and ProQuest. Studies that
examined machine learning or deep learning methods to determine the prognosis
of COVID-19 using CT or chest X-ray images were included. Polled sensitivity,
specificity area under the curve and diagnostic odds ratio were calculated.
Result: A total of 36 articles were included; various prognosis-related issues,
including disease severity, mechanical ventilation or admission to the
intensive care unit and mortality, were investigated. Several AI models and
architectures were employed, such as the Siamense model, support vector
machine, Random Forest , eXtreme Gradient Boosting, and convolutional neural
networks. The models achieved 71%, 88% and 67% sensitivity for mortality,
severity assessment and need for ventilation, respectively. The specificity of
69%, 89% and 89% were reported for the aforementioned variables. Conclusion:
Based on the included articles, machine learning and deep learning methods used
for the prognosis of COVID-19 patients using radiomic features from CT or CXR
images can help clinicians manage patients and allocate resources more
effectively. These studies also demonstrate that combining patient demographic,
clinical data, laboratory tests and radiomic features improves model
performances.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [251] [Benchmarking Dimensionality Reduction Techniques for Spatial Transcriptomics](https://arxiv.org/abs/2509.13344)
*Md Ishtyaq Mahmud,Veena Kochat,Suresh Satpati,Jagan Mohan Reddy Dwarampudi,Kunal Rai,Tania Banerjee*

Main category: q-bio.GN

TL;DR: 本文提出评估空间转录组降维技术的统一框架，对六种方法进行基准测试，展示不同性能，提供超参数选择和改进生物保真度方法，实现有原则的方法选择。


<details>
  <summary>Details</summary>
Motivation: 引入超越标准PCA的统一框架评估空间转录组降维技术。

Method: 在胆管癌Xenium数据集上对六种方法进行基准测试，系统改变潜在维度和聚类分辨率，用多种指标评估，使用Pareto最优分析选超参数，用MER引导重新分配。

Result: PCA提供快速基线，NMF使标记富集最大化，VAE平衡重建和可解释性，自编码器处于中间位置，MER引导重新分配平均使CMC分数提高12%。

Conclusion: 该框架能为特定空间转录组分析有原则地选择降维方法。

Abstract: We introduce a unified framework for evaluating dimensionality reduction
techniques in spatial transcriptomics beyond standard PCA approaches. We
benchmark six methods PCA, NMF, autoencoder, VAE, and two hybrid embeddings on
a cholangiocarcinoma Xenium dataset, systematically varying latent dimensions
($k$=5-40) and clustering resolutions ($\rho$=0.1-1.2). Each configuration is
evaluated using complementary metrics including reconstruction error, explained
variance, cluster cohesion, and two novel biologically-motivated measures:
Cluster Marker Coherence (CMC) and Marker Exclusion Rate (MER). Our results
demonstrate distinct performance profiles: PCA provides a fast baseline, NMF
maximizes marker enrichment, VAE balances reconstruction and interpretability,
while autoencoders occupy a middle ground. We provide systematic hyperparameter
selection using Pareto optimal analysis and demonstrate how MER-guided
reassignment improves biological fidelity across all methods, with CMC scores
improving by up to 12\% on average. This framework enables principled selection
of dimensionality reduction methods tailored to specific spatial
transcriptomics analyses.

</details>


### [252] [PhenoGnet: A Graph-Based Contrastive Learning Framework for Disease Similarity Prediction](https://arxiv.org/abs/2509.14037)
*Ranga Baminiwatte,Kazi Jewel Rana,Aaron J. Masino*

Main category: q-bio.GN

TL;DR: 提出PhenoGnet框架预测疾病相似度，效果超现有方法，有下游应用潜力。


<details>
  <summary>Details</summary>
Motivation: 理解疾病相似度对诊断、药物发现和个性化治疗策略至关重要。

Method: PhenoGnet整合基因功能交互网络和人类表型本体，包含内视图和跨视图模型，用已知基因表型关联作正样本，随机无关对作负样本训练。

Result: 在1100对相似和866对不相似疾病对基准上评估，基于基因的嵌入AUCPR达0.9012，AUROC达0.8764，超现有方法。

Conclusion: PhenoGnet能捕捉潜在生物关系，为疾病相似度预测提供可扩展和可解释方案，有下游应用潜力。

Abstract: Understanding disease similarity is critical for advancing diagnostics, drug
discovery, and personalized treatment strategies. We present PhenoGnet, a novel
graph-based contrastive learning framework designed to predict disease
similarity by integrating gene functional interaction networks with the Human
Phenotype Ontology (HPO). PhenoGnet comprises two key components: an intra-view
model that separately encodes gene and phenotype graphs using Graph
Convolutional Networks (GCNs) and Graph Attention Networks (GATs), and a cross
view model implemented as a shared weight multilayer perceptron (MLP) that
aligns gene and phenotype embeddings through contrastive learning. The model is
trained using known gene phenotype associations as positive pairs and randomly
sampled unrelated pairs as negatives. Diseases are represented by the mean
embeddings of their associated genes and/or phenotypes, and pairwise similarity
is computed via cosine similarity. Evaluation on a curated benchmark of 1,100
similar and 866 dissimilar disease pairs demonstrates strong performance, with
gene based embeddings achieving an AUCPR of 0.9012 and AUROC of 0.8764,
outperforming existing state of the art methods. Notably, PhenoGnet captures
latent biological relationships beyond direct overlap, offering a scalable and
interpretable solution for disease similarity prediction. These results
underscore its potential for enabling downstream applications in rare disease
research and precision medicine.

</details>
