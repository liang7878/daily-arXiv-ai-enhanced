<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 77]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 19]
- [cs.LG](#cs.LG) [Total: 196]
- [cs.NE](#cs.NE) [Total: 10]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 17]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [q-fin.RM](#q-fin.RM) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 7]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 13]
- [stat.CO](#stat.CO) [Total: 3]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.CV](#cs.CV) [Total: 78]
- [math.PR](#math.PR) [Total: 2]
- [cs.LO](#cs.LO) [Total: 1]
- [econ.GN](#econ.GN) [Total: 8]
- [eess.SP](#eess.SP) [Total: 22]
- [hep-ph](#hep-ph) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.CY](#cs.CY) [Total: 11]
- [math.OC](#math.OC) [Total: 2]
- [econ.TH](#econ.TH) [Total: 1]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CC](#cs.CC) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.CL](#cs.CL) [Total: 64]
- [stat.ME](#stat.ME) [Total: 4]
- [quant-ph](#quant-ph) [Total: 6]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.DM](#cs.DM) [Total: 2]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 6]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [eess.SY](#eess.SY) [Total: 4]
- [cs.SD](#cs.SD) [Total: 7]
- [cs.RO](#cs.RO) [Total: 15]
- [cs.CR](#cs.CR) [Total: 17]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Understanding Financial Reasoning in AI: A Multimodal Benchmark and Error Learning Approach](https://arxiv.org/abs/2506.06282)
*Shuangyan Deng,Haizhou Peng,Jiachen Xu,Chunhou Liu,Ciprian Doru Giurcuaneanu,Jiamou Liu*

Main category: cs.AI

TL;DR: 本文引入金融推理新基准，提出误差感知学习框架，实验表明多模态输入和误差反馈能提升模型性能，也指出存在的挑战。


<details>
  <summary>Details</summary>
Motivation: 有效金融推理需文本和视觉数据解读能力，当前缺乏评估AI模型金融推理能力的工具。

Method: 引入含3200个专家级问答对、涵盖15个金融主题的新基准，提出不依赖微调的误差感知学习框架。

Result: 多模态输入显著提升性能，加入误差反馈有持续可衡量的改进，发现视觉理解和数学逻辑存在挑战。

Conclusion: 多模态输入和误差反馈对金融AI系统有积极作用，自反思推理有前景。

Abstract: Effective financial reasoning demands not only textual understanding but also
the ability to interpret complex visual data such as charts, tables, and trend
graphs. This paper introduces a new benchmark designed to evaluate how well AI
models - especially large language and multimodal models - reason in
finance-specific contexts. Covering 3,200 expert-level question-answer pairs
across 15 core financial topics, the benchmark integrates both textual and
visual modalities to reflect authentic analytical challenges in finance. To
address limitations in current reasoning approaches, we propose an error-aware
learning framework that leverages historical model mistakes and feedback to
guide inference, without requiring fine-tuning. Our experiments across
state-of-the-art models show that multimodal inputs significantly enhance
performance and that incorporating error feedback leads to consistent and
measurable improvements. The results highlight persistent challenges in visual
understanding and mathematical logic, while also demonstrating the promise of
self-reflective reasoning in financial AI systems. Our code and data can be
found at https://anonymous/FinMR/CodeData.

</details>


### [2] [Unreal Patterns](https://arxiv.org/abs/2506.06284)
*John Beverley,Jim Logan*

Main category: cs.AI

TL;DR: 本文提出一种表示不存在或可能永远不会存在实体信息的框架，批评传统方法，强调实用可实现方案。


<details>
  <summary>Details</summary>
Motivation: 现有处理不存在实体的方法存在过度依赖形而上学假设或引入计算低效问题，需要实用可实现的方案。

Method: 采用基于基础形式本体的结构化本体驱动方法，用实际类型的交集而非特定不存在的实例来建模。

Result: 提出了处理假设或不存在实体引用的有用且计算可行的框架。

Conclusion: 开发处理非现实模式的结构化本体驱动方法能有效处理不存在实体信息。

Abstract: This paper introduces a framework for representing information about entities
that do not exist or may never exist, such as those involving fictional
entities, blueprints, simulations, and future scenarios. Traditional approaches
that introduce "dummy instances" or rely on modal logic are criticized, and a
proposal is defended in which such cases are modeled using the intersections of
actual types rather than specific non existent tokens. The paper positions
itself within the Basic Formal Ontology and its realist commitments,
emphasizing the importance of practical, implementable solutions over purely
metaphysical or philosophical proposals, arguing that existing approaches to
non existent entities either overcommit to metaphysical assumptions or
introduce computational inefficiencies that hinder applications. By developing
a structured ontology driven approach to unreal patterns, the paper aims to
provide a useful and computationally viable means of handling references to
hypothetical or non existent entities.

</details>


### [3] [NFISiS: New Perspectives on Fuzzy Inference Systems for Renewable Energy Forecasting](https://arxiv.org/abs/2506.06285)
*Kaike Sa Teles Rocha Alves,Eduardo Pestana de Aguiar*

Main category: cs.AI

TL;DR: 提出Python库evolvingfuzzysystems实现多个eFS模型，用数据集评估，结果显示ePL平衡准确性与计算成本，适合实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有Evolving Fuzzy Systems（eFS）模型缺乏公开实现，限制其可访问性和广泛应用。

Method: 开发Python库evolvingfuzzysystems实现多个eFS模型，用fetch_california_housing数据集评估，以NRMSE、NDEI、MAPE衡量性能，分析计算复杂度。

Result: ePL是简单高效模型，平衡了准确性和计算成本，适合实际应用。

Conclusion: evolvingfuzzysystems公开模型，有助于自适应和可解释机器学习的研究和实际应用。

Abstract: Evolving Fuzzy Systems (eFS) have gained significant attention due to their
ability to adaptively update their structure in response to data dynamics while
maintaining interpretability. However, the lack of publicly available
implementations of these models limits their accessibility and widespread
adoption. To address this gap, we present evolvingfuzzysystems, a Python
library that provides implementations of several well-established eFS models,
including ePL-KRLS-DISCO, ePL+, eMG, ePL, exTS, Simpl\_eTS, and eTS. The
library facilitates model evaluation and comparison by offering built-in tools
for training, visualization, and performance assessment. The models are
evaluated using the fetch\_california\_housing dataset, with performance
measured in terms of normalized root-mean-square error (NRMSE), non-dimensional
error index (NDEI), and mean absolute percentage error (MAPE). Additionally,
computational complexity is analyzed by measuring execution times and rule
evolution during training and testing phases. The results highlight ePL as a
simple yet efficient model that balances accuracy and computational cost,
making it particularly suitable for real-world applications. By making these
models publicly available, evolvingfuzzysystems aims to foster research and
practical applications in adaptive and interpretable machine learning.

</details>


### [4] [Deep Research Bench: Evaluating AI Web Research Agents](https://arxiv.org/abs/2506.06287)
*FutureSearch,:,Nikos I. Bosse,Jon Evans,Robert G. Gambee,Daniel Hnyk,Peter Mühlbacher,Lawrence Phillips,Dan Schwarz,Jack Wildman*

Main category: cs.AI

TL;DR: 介绍Deep Research Bench，提供RetroSearch环境，可对大语言模型进行网络研究评估，结果公布在公开排行榜。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对网络研究智能体质量的直接评估，且要控制不断变化的网络情况。

Method: 引入包含89个多步网络研究任务实例的Deep Research Bench，提供RetroSearch环境，使用强大的智能体工具和框架对大语言模型进行基准测试，包括自动评估智能体痕迹。

Result: 能对离线RetroSearch智能体和实时网络智能体进行可靠评估，可衡量模型在幻觉、工具使用和遗忘方面的进展。

Conclusion: 可以对主要的网络研究产品进行评估，评估结果公开可查。

Abstract: Amongst the most common use cases of modern AI is LLM chat with web search
enabled. However, no direct evaluations of the quality of web research agents
exist that control for the continually-changing web. We introduce Deep Research
Bench, consisting of 89 multi-step web research task instances of varying
difficulty across 8 diverse task categories, with the answers carefully worked
out by skilled humans. We provide a "RetroSearch" environment with a large
frozen set of scraped web pages, and demonstrate that offline "RetroSearch"
agents perform comparably to "live web" agents, enabling reliable evaluations
of models over time. We provide robust agent tooling and scaffolding to
benchmark major LLMs as they are released, including "thinking" models like o3
and Gemini 2.5 Pro. We include automated evaluations of the lengthy agent
traces to report progress over time in hallucinations, tool use, and
forgetting. Finally, we evaluate the major web research products branded as
"Deep Research", "Deep Search", "Search", or "Research." Results are available
on a public leaderboard at https://drb.futuresearch.ai/.

</details>


### [5] [Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review](https://arxiv.org/abs/2506.06301)
*Muhammad Monjurul Karim,Yan Shi,Shucheng Zhang,Bingzhang Wang,Mehrdad Nasri,Yinhai Wang*

Main category: cs.AI

TL;DR: 本文全面回顾大语言模型（LLMs）在提升道路安全与机动性方面的应用与定制，分析其应用、适配策略、使能技术，指出挑战并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统工程方法难以应对现实交通的复杂性和动态性，LLMs能力为道路安全与机动性分析带来范式转变，有必要研究其应用。

Method: 对LLMs在交通领域的应用、适配策略、使能技术进行系统分析，指出挑战并展望未来方向。

Result: 梳理了LLMs在机动性和安全方面的多样应用、适配策略及使能技术，明确存在固有局限、数据治理、部署复杂和安全保障等挑战。

Conclusion: LLMs有变革潜力，需负责任创新以实现更安全智能的交通系统。

Abstract: Roadway safety and mobility remain critical challenges for modern
transportation systems, demanding innovative analytical frameworks capable of
addressing complex, dynamic, and heterogeneous environments. While traditional
engineering methods have made progress, the complexity and dynamism of
real-world traffic necessitate more advanced analytical frameworks. Large
Language Models (LLMs), with their unprecedented capabilities in natural
language understanding, knowledge integration, and reasoning, represent a
promising paradigm shift. This paper comprehensively reviews the application
and customization of LLMs for enhancing roadway safety and mobility. A key
focus is how LLMs are adapted -- via architectural, training, prompting, and
multimodal strategies -- to bridge the "modality gap" with transportation's
unique spatio-temporal and physical data. The review systematically analyzes
diverse LLM applications in mobility (e.g., traffic flow prediction, signal
control) and safety (e.g., crash analysis, driver behavior assessment,).
Enabling technologies such as V2X integration, domain-specific foundation
models, explainability frameworks, and edge computing are also examined.
Despite significant potential, challenges persist regarding inherent LLM
limitations (hallucinations, reasoning deficits), data governance (privacy,
bias), deployment complexities (sim-to-real, latency), and rigorous safety
assurance. Promising future research directions are highlighted, including
advanced multimodal fusion, enhanced spatio-temporal reasoning, human-AI
collaboration, continuous learning, and the development of efficient,
verifiable systems. This review provides a structured roadmap of current
capabilities, limitations, and opportunities, underscoring LLMs' transformative
potential while emphasizing the need for responsible innovation to realize
safer, more intelligent transportation systems.

</details>


### [6] [Mapping Human-Agent Co-Learning and Co-Adaptation: A Scoping Review](https://arxiv.org/abs/2506.06324)
*Shruti Kumar,Xiaoyu Chen,Xiaomei Wang*

Main category: cs.AI

TL;DR: 现有研究对人机协作术语使用不统一，本综述旨在收集相关论文，探究描述术语、涉及的智能体与任务领域、所用认知理论和框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究描述人机协作关系的术语缺乏一致性，且该领域研究较新，需进一步探索。

Method: 开展范围性综述，提出三个研究问题进行探究。

Result: 未提及研究结果。

Conclusion: 未提及研究结论。

Abstract: Several papers have delved into the challenges of human-AI-robot co-learning
and co-adaptation. It has been noted that the terminology used to describe this
collaborative relationship in existing studies needs to be more consistent. For
example, the prefix "co" is used interchangeably to represent both
"collaborative" and "mutual," and the terms "co-learning" and "co-adaptation"
are sometimes used interchangeably. However, they can reflect subtle
differences in the focus of the studies. The current scoping review's primary
research question (RQ1) aims to gather existing papers discussing this
collaboration pattern and examine the terms researchers use to describe this
human-agent relationship. Given the relative newness of this area of study, we
are also keen on exploring the specific types of intelligent agents and task
domains that have been considered in existing research (RQ2). This exploration
is significant as it can shed light on the diversity of human-agent
interactions, from one-time to continuous learning/adaptation scenarios. It can
also help us understand the dynamics of human-agent interactions in different
task domains, guiding our expectations towards research situated in dynamic,
complex domains. Our third objective (RQ3) is to investigate the cognitive
theories and frameworks that have been utilized in existing studies to measure
human-agent co-learning and co-adaptation. This investigation is crucial as it
can help us understand the theoretical underpinnings of human-agent
collaboration and adaptation, and it can also guide us in identifying any new
frameworks proposed specifically for this type of relationship.

</details>


### [7] [Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures](https://arxiv.org/abs/2506.06832)
*Clément Hongler,Andrew Emil*

Main category: cs.AI

TL;DR: 本文围绕大语言模型定义的文本概率测度，提出交叉熵游戏（Xent Games），介绍其形式、性质，讨论用其衡量大语言模型能力及构建衡量标准，并提出解决衡量通用能力范围问题的方法。


<details>
  <summary>Details</summary>
Motivation: 思考大语言模型了解概率测度的隐含知识问题，探索超越生成采样的任务。

Method: 将任务构建为基于大语言模型测度的交叉熵游戏，用基本博弈论一致性公理构建游戏空间，提取覆盖测度构建衡量标准，借鉴进化动力学思想探索游戏空间。

Result: 表明交叉熵游戏空间大且可构建，可用于衡量大语言模型能力。

Conclusion: 交叉熵游戏为衡量大语言模型能力提供了新途径，通过探索游戏空间可解决衡量通用能力的范围问题。

Abstract: Large Language Models (LLMs) define probability measures on text. By
considering the implicit knowledge question of what it means for an LLM to know
such a measure and what it entails algorithmically, we are naturally led to
formulate a series of tasks that go beyond generative sampling, involving forms
of summarization, counterfactual thinking, anomaly detection, originality
search, reverse prompting, debating, creative solving, etc. These tasks can be
formulated as games based on LLM measures, which we call Cross-Entropy (Xent)
Games. Xent Games can be single-player or multi-player. They involve
cross-entropy scores and cross-entropy constraints, and can be expressed as
simple computational graphs and programs. We show the Xent Game space is large
enough to contain a wealth of interesting examples, while being constructible
from basic game-theoretic consistency axioms. We then discuss how the Xent Game
space can be used to measure the abilities of LLMs. This leads to the
construction of Xent Game measures: finite families of Xent Games that can be
used as capability benchmarks, built from a given scope, by extracting a
covering measure. To address the unbounded scope problem associated with the
challenge of measuring general abilities, we propose to explore the space of
Xent Games in a coherent fashion, using ideas inspired by evolutionary
dynamics.

</details>


### [8] [Memory OS of AI Agent](https://arxiv.org/abs/2506.06326)
*Jiazheng Kang,Mingming Ji,Zhe Zhao,Ting Bai*

Main category: cs.AI

TL;DR: 论文提出MemoryOS解决大语言模型内存管理问题，实现分层内存集成与动态更新，实验效果好且代码开源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在固定上下文窗口和内存管理不足的问题，导致长期记忆能力缺乏和交互体验个性化不足。

Method: 借鉴操作系统内存管理原则，设计MemoryOS，包含分层存储架构和四个关键模块，有特定存储单元和动态更新策略。

Result: 在LoCoMo基准测试中，相比基线在GPT - 4o - mini上F1平均提升49.11%，BLEU - 1平均提升46.18%，展现出上下文连贯性和长期对话中的个性化记忆保留能力。

Conclusion: MemoryOS能有效实现AI智能体的全面高效内存管理。

Abstract: Large Language Models (LLMs) face a crucial challenge from fixed context
windows and inadequate memory management, leading to a severe shortage of
long-term memory capabilities and limited personalization in the interactive
experience with AI agents. To overcome this challenge, we innovatively propose
a Memory Operating System, i.e., MemoryOS, to achieve comprehensive and
efficient memory management for AI agents. Inspired by the memory management
principles in operating systems, MemoryOS designs a hierarchical storage
architecture and consists of four key modules: Memory Storage, Updating,
Retrieval, and Generation. Specifically, the architecture comprises three
levels of storage units: short-term memory, mid-term memory, and long-term
personal memory. Key operations within MemoryOS include dynamic updates between
storage units: short-term to mid-term updates follow a dialogue-chain-based
FIFO principle, while mid-term to long-term updates use a segmented page
organization strategy. Our pioneering MemoryOS enables hierarchical memory
integration and dynamic updating. Extensive experiments on the LoCoMo benchmark
show an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over the
baselines on GPT-4o-mini, showing contextual coherence and personalized memory
retention in long conversations. The implementation code is open-sourced at
https://github.com/BAI-LAB/MemoryOS.

</details>


### [9] [Will artificial agents pursue power by default?](https://arxiv.org/abs/2506.06352)
*Christian Tarsney*

Main category: cs.AI

TL;DR: 本文用决策理论框架形式化工具收敛和权力追求概念，评估权力是收敛工具性目标的说法，认为该说法有一定道理但预测效用有限，对有机会获得绝对或接近绝对权力的主体更具预测性。


<details>
  <summary>Details</summary>
Motivation: 回应关于先进AI灾难性风险及权力追求说法的争议，评估权力是否为收敛工具性目标。

Method: 在抽象的决策理论框架中形式化工具收敛和权力追求的概念。

Result: 权力是收敛工具性目标的说法至少有一定道理，但若无主体最终目标的实质性信息，并非总能按权力对主体选项进行排序。

Conclusion: 权力是收敛工具性目标的说法预测效用有限，对有机会获得绝对或接近绝对权力的主体预测性更强。

Abstract: Researchers worried about catastrophic risks from advanced AI have argued
that we should expect sufficiently capable AI agents to pursue power over
humanity because power is a convergent instrumental goal, something that is
useful for a wide range of final goals. Others have recently expressed
skepticism of these claims. This paper aims to formalize the concepts of
instrumental convergence and power-seeking in an abstract, decision-theoretic
framework, and to assess the claim that power is a convergent instrumental
goal. I conclude that this claim contains at least an element of truth, but
might turn out to have limited predictive utility, since an agent's options
cannot always be ranked in terms of power in the absence of substantive
information about the agent's final goals. However, the fact of instrumental
convergence is more predictive for agents who have a good shot at attaining
absolute or near-absolute power.

</details>


### [10] [Evaluating LLM-corrupted Crowdsourcing Data Without Ground Truth](https://arxiv.org/abs/2506.06991)
*Yichi Zhang,Jinlong Pang,Zhaowei Zhu,Yang Liu*

Main category: cs.AI

TL;DR: 研究用同行预测机制减轻众包中标注任务中LLM辅助作弊问题，提出无训练评分机制并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 众包工人使用LLM使反映人类输入的数据集可能被破坏，现有LLM检测方法不适用于标注任务。

Method: 采用同行预测机制，在考虑LLM共谋的众包模型下，量化工人答案相关性并提出无训练评分机制。

Result: 建立方法有效的条件，在真实众包数据集上实证证明该方法检测低努力作弊的鲁棒性。

Conclusion: 所提出的方法能有效减轻众包中标注任务中LLM辅助作弊问题。

Abstract: The recent success of generative AI highlights the crucial role of
high-quality human feedback in building trustworthy AI systems. However, the
increasing use of large language models (LLMs) by crowdsourcing workers poses a
significant challenge: datasets intended to reflect human input may be
compromised by LLM-generated responses. Existing LLM detection approaches often
rely on high-dimension training data such as text, making them unsuitable for
annotation tasks like multiple-choice labeling. In this work, we investigate
the potential of peer prediction -- a mechanism that evaluates the information
within workers' responses without using ground truth -- to mitigate
LLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our
approach quantifies the correlations between worker answers while conditioning
on (a subset of) LLM-generated labels available to the requester. Building on
prior research, we propose a training-free scoring mechanism with theoretical
guarantees under a crowdsourcing model that accounts for LLM collusion. We
establish conditions under which our method is effective and empirically
demonstrate its robustness in detecting low-effort cheating on real-world
crowdsourcing datasets.

</details>


### [11] [Towards Foundation Model on Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2506.06367)
*Jiaxin Pan,Mojtaba Nayyeri,Osama Mohammed,Daniel Hernandez,Rongchuan Zhang,Cheng Cheng,Steffen Staab*

Main category: cs.AI

TL;DR: 现有TKGE模型在推理时依赖所见元素，限制泛化能力。提出完全归纳式方法POSTRA，用正弦位置编码和消息传递，在未见TKG上零样本性能强。


<details>
  <summary>Details</summary>
Motivation: 现有TKGE模型在转导或半归纳设置下进行链接预测，依赖训练时所见元素，难以学习可迁移的表示，限制了模型在新领域的泛化能力。

Method: 引入完全归纳式方法，采用正弦位置编码捕捉细粒度时间模式，基于局部和全局时间上下文的消息传递生成自适应实体和关系表示。

Result: POSTRA在未见的时间知识图谱上表现出强大的零样本性能，一个预训练模型可提升多种归纳式时间推理场景的零样本性能。

Conclusion: POSTRA是迈向时间知识图谱基础模型的重要一步。

Abstract: Temporal Knowledge Graphs (TKGs) store temporal facts with quadruple formats
(s, p, o, t). Existing Temporal Knowledge Graph Embedding (TKGE) models perform
link prediction tasks in transductive or semi-inductive settings, which means
the entities, relations, and temporal information in the test graph are fully
or partially observed during training. Such reliance on seen elements during
inference limits the models' ability to transfer to new domains and generalize
to real-world scenarios. A central limitation is the difficulty in learning
representations for entities, relations, and timestamps that are transferable
and not tied to dataset-specific vocabularies. To overcome these limitations,
we introduce the first fully-inductive approach to temporal knowledge graph
link prediction. Our model employs sinusoidal positional encodings to capture
fine-grained temporal patterns and generates adaptive entity and relation
representations using message passing conditioned on both local and global
temporal contexts. Our model design is agnostic to temporal granularity and
time span, effectively addressing temporal discrepancies across TKGs and
facilitating time-aware structural information transfer. As a pretrained,
scalable, and transferable model, POSTRA demonstrates strong zero-shot
performance on unseen temporal knowledge graphs, effectively generalizing to
novel entities, relations, and timestamps. Extensive theoretical analysis and
empirical results show that a single pretrained model can improve zero-shot
performance on various inductive temporal reasoning scenarios, marking a
significant step toward a foundation model for temporal KGs.

</details>


### [12] [SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation](https://arxiv.org/abs/2506.06470)
*Yanwei Ren,Haotian Zhang,Fuxiang Wu,Jiayan Qiu,Jiaxing Huang,Baosheng Yu,Liu Liu*

Main category: cs.AI

TL;DR: 提升大语言模型单纯靠扩大数据集收益递减，聚焦数据质量。提出SIGMA框架，利用被丢弃的兄弟节点优化推理，在MATH基准测试上表现出色，减少数据使用量并提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型时单纯扩大数据集收益递减，且传统MCTS方法丢弃有价值的兄弟节点，浪费大量信息数据。

Method: 提出SIGMA框架，在每条搜索路径上建立兄弟节点语义链接，通过批判模型识别优缺点，修订模型进行文本反向传播来优化得分最高的轨迹。

Result: 在MATH基准测试中，SIGMA调优的7B模型仅用30K样本达到54.92%的准确率，优于用590K样本训练的模型。

Conclusion: SIGMA的兄弟节点引导优化显著减少数据使用量，大幅提升大语言模型推理能力。

Abstract: Enhancing large language models by simply scaling up datasets has begun to
yield diminishing returns, shifting the spotlight to data quality. Monte Carlo
Tree Search (MCTS) has emerged as a powerful technique for generating
high-quality chain-of-thought data, yet conventional approaches typically
retain only the top-scoring trajectory from the search tree, discarding sibling
nodes that often contain valuable partial insights, recurrent error patterns,
and alternative reasoning strategies. This unconditional rejection of
non-optimal reasoning branches may waste vast amounts of informative data in
the whole search tree. We propose SIGMA (Sibling Guided Monte Carlo
Augmentation), a novel framework that reintegrates these discarded sibling
nodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes
along each search path and applies a two-stage refinement: a critique model
identifies overlooked strengths and weaknesses across the sibling set, and a
revision model conducts text-based backpropagation to refine the top-scoring
trajectory in light of this comparative feedback. By recovering and amplifying
the underutilized but valuable signals from non-optimal reasoning branches,
SIGMA substantially improves reasoning trajectories. On the challenging MATH
benchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K
samples, outperforming state-of-the-art models trained on 590K samples. This
result highlights that our sibling-guided optimization not only significantly
reduces data usage but also significantly boosts LLM reasoning.

</details>


### [13] [Reinforcement Learning for Autonomous Warehouse Orchestration in SAP Logistics Execution: Redefining Supply Chain Agility](https://arxiv.org/abs/2506.06523)
*Sumanth Pillella*

Main category: cs.AI

TL;DR: 研究引入基于强化学习框架在 SAP LE 中自主编排仓库任务，用合成数据集模拟场景，实现 95% 任务优化准确率，减少 60% 处理时间，提供现代供应链变革性方案。


<details>
  <summary>Details</summary>
Motivation: 在供应链需求升级时代，提升 SAP 物流执行（LE）管理仓库运营、运输和交付的运营敏捷性与效率。

Method: 将仓库流程建模为动态环境，利用强化学习自主编排仓库任务，用含 300,000 条 LE 交易的合成数据集模拟场景。

Result: 达到 95% 任务优化准确率，相比传统方法减少 60% 处理时间，通过可视化指导敏捷仓库策略。

Conclusion: 该方法解决数据隐私、可扩展性和 SAP 集成问题，为现代供应链提供变革性解决方案。

Abstract: In an era of escalating supply chain demands, SAP Logistics Execution (LE) is
pivotal for managing warehouse operations, transportation, and delivery. This
research introduces a pioneering framework leveraging reinforcement learning
(RL) to autonomously orchestrate warehouse tasks in SAP LE, enhancing
operational agility and efficiency. By modeling warehouse processes as dynamic
environments, the framework optimizes task allocation, inventory movement, and
order picking in real-time. A synthetic dataset of 300,000 LE transactions
simulates real-world warehouse scenarios, including multilingual data and
operational disruptions. The analysis achieves 95% task optimization accuracy,
reducing processing times by 60% compared to traditional methods.
Visualizations, including efficiency heatmaps and performance graphs, guide
agile warehouse strategies. This approach tackles data privacy, scalability,
and SAP integration, offering a transformative solution for modern supply
chains.

</details>


### [14] [ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search](https://arxiv.org/abs/2506.06524)
*Sam Earle,Ahmed Khalifa,Muhammad Umair Nasir,Zehua Jiang,Graham Todd,Andrzej Banburski-Fahey,Julian Togelius*

Main category: cs.AI

TL;DR: 介绍了基于大语言模型的ScriptDoctor系统，用于自动生成和测试PuzzleScript游戏，展示了自动化、开放式LLM工作流在生成新游戏内容上的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在自动游戏设计（AGD）中多在人工持续监督下使用，需研究如何将其集成到更长期的AGD管道中，实现系统与游戏引擎自主交互测试生成内容。

Method: 引入ScriptDoctor系统，在迭代循环中生成和测试游戏设计想法，利用人类编写示例引导输出，根据PuzzleScript引擎编译错误生成功能代码，用基于搜索的智能体对生成游戏进行试玩测试。

Result: 成功构建ScriptDoctor系统，可在PuzzleScript中自动生成和测试游戏。

Conclusion: ScriptDoctor是基于自动化、开放式LLM工作流生成新游戏内容潜力的具体示例。

Abstract: There is much interest in using large pre-trained models in Automatic Game
Design (AGD), whether via the generation of code, assets, or more abstract
conceptualization of design ideas. But so far this interest largely stems from
the ad hoc use of such generative models under persistent human supervision.
Much work remains to show how these tools can be integrated into
longer-time-horizon AGD pipelines, in which systems interface with game engines
to test generated content autonomously. To this end, we introduce ScriptDoctor,
a Large Language Model (LLM)-driven system for automatically generating and
testing games in PuzzleScript, an expressive but highly constrained description
language for turn-based puzzle games over 2D gridworlds. ScriptDoctor generates
and tests game design ideas in an iterative loop, where human-authored examples
are used to ground the system's output, compilation errors from the
PuzzleScript engine are used to elicit functional code, and search-based agents
play-test generated games. ScriptDoctor serves as a concrete example of the
potential of automated, open-ended LLM-based workflows in generating novel game
content.

</details>


### [15] [The Optimization Paradox in Clinical AI Multi-Agent Systems](https://arxiv.org/abs/2506.06574)
*Suhana Bedi,Iddah Mlauzi,Daniel Shin,Sanmi Koyejo,Nigam H. Shah*

Main category: cs.AI

TL;DR: 研究多智能体人工智能系统在临床诊断中组件级优化与系统性能关系，发现组件优化系统诊断准确率低，强调端到端系统验证重要性。


<details>
  <summary>Details</summary>
Motivation: 多智能体人工智能系统在临床应用中，组件级优化与系统性能关系不明。

Method: 用MIMIC - CDM数据集2400个真实病例，分解临床诊断任务，用综合指标对比单智能体和多智能体系统。

Result: 多智能体系统总体表现好，但组件优化系统诊断准确率低于多智能体系统。

Conclusion: AI在医疗领域成功应用需关注信息流动和智能体兼容性，要进行端到端系统验证。

Abstract: Multi-agent artificial intelligence systems are increasingly deployed in
clinical settings, yet the relationship between component-level optimization
and system-wide performance remains poorly understood. We evaluated this
relationship using 2,400 real patient cases from the MIMIC-CDM dataset across
four abdominal pathologies (appendicitis, pancreatitis, cholecystitis,
diverticulitis), decomposing clinical diagnosis into information gathering,
interpretation, and differential diagnosis. We evaluated single agent systems
(one model performing all tasks) against multi-agent systems (specialized
models for each task) using comprehensive metrics spanning diagnostic outcomes,
process adherence, and cost efficiency. Our results reveal a paradox: while
multi-agent systems generally outperformed single agents, the
component-optimized or Best of Breed system with superior components and
excellent process metrics (85.5% information accuracy) significantly
underperformed in diagnostic accuracy (67.7% vs. 77.4% for a top multi-agent
system). This finding underscores that successful integration of AI in
healthcare requires not just component level optimization but also attention to
information flow and compatibility between agents. Our findings highlight the
need for end to end system validation rather than relying on component metrics
alone.

</details>


### [16] [AI Simulation by Digital Twins: Systematic Survey, Reference Framework, and Mapping to a Standardized Architecture](https://arxiv.org/abs/2506.06580)
*Xiaoran Liu,Istvan David*

Main category: cs.AI

TL;DR: 文章对数字孪生赋能的AI仿真进行系统调查，分析22项研究，得出参考框架和架构指南，并指出挑战与研究机会。


<details>
  <summary>Details</summary>
Motivation: 现代亚符号AI面临数据量和质量不足的挑战，数字孪生为AI仿真开辟新途径，因此进行相关系统调查。

Method: 分析22项主要研究，并将得出的参考框架映射到ISO 23247数字孪生参考架构上。

Result: 确定了技术趋势，得出参考框架并提供架构指南。

Conclusion: 指出了数字孪生赋能的AI仿真领域的挑战和研究机会，为未来研究人员提供参考。

Abstract: Insufficient data volume and quality are particularly pressing challenges in
the adoption of modern subsymbolic AI. To alleviate these challenges, AI
simulation uses virtual training environments in which AI agents can be safely
and efficiently developed with simulated, synthetic data. Digital twins open
new avenues in AI simulation, as these high-fidelity virtual replicas of
physical systems are equipped with state-of-the-art simulators and the ability
to further interact with the physical system for additional data collection. In
this article, we report on our systematic survey of digital twin-enabled AI
simulation. By analyzing 22 primary studies, we identify technological trends
and derive a reference framework to situate digital twins and AI components.
Based on our findings, we derive a reference framework and provide
architectural guidelines by mapping it onto the ISO 23247 reference
architecture for digital twins. Finally, we identify challenges and research
opportunities for prospective researchers.

</details>


### [17] [GELD: A Unified Neural Model for Efficiently Solving Traveling Salesman Problems Across Different Scales](https://arxiv.org/abs/2506.06634)
*Yubin Xiao,Di Wang,Rui Cao,Xuan Wu,Boyang Li,You Zhou*

Main category: cs.AI

TL;DR: 本文提出新的神经TSP求解器GELD，在解决TSP问题上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经网络的TSP求解器难以用同一组预训练参数高效解决不同规模TSP问题，实用性受限。

Method: 构建基于广义全局评估和精细局部选择框架的GELD求解器，集成轻量级全局编码器和重量级局部解码器，引入低复杂度注意力机制，采用两阶段训练策略。

Result: 在合成和真实数据集上实验表明，GELD在解质量和推理速度上优于7个先进模型，可作为后处理方法提升现有求解器解质量，能解决多达744,710个节点的TSP问题。

Conclusion: GELD在解决不同规模TSP问题上具有高效性和实用性，是一种有效的TSP求解方案。

Abstract: The Traveling Salesman Problem (TSP) is a well-known combinatorial
optimization problem with broad real-world applications. Recent advancements in
neural network-based TSP solvers have shown promising results. Nonetheless,
these models often struggle to efficiently solve both small- and large-scale
TSPs using the same set of pre-trained model parameters, limiting their
practical utility. To address this issue, we introduce a novel neural TSP
solver named GELD, built upon our proposed broad global assessment and refined
local selection framework. Specifically, GELD integrates a lightweight
Global-view Encoder (GE) with a heavyweight Local-view Decoder (LD) to enrich
embedding representation while accelerating the decision-making process.
Moreover, GE incorporates a novel low-complexity attention mechanism, allowing
GELD to achieve low inference latency and scalability to larger-scale TSPs.
Additionally, we propose a two-stage training strategy that utilizes training
instances of different sizes to bolster GELD's generalization ability.
Extensive experiments conducted on both synthetic and real-world datasets
demonstrate that GELD outperforms seven state-of-the-art models considering
both solution quality and inference speed. Furthermore, GELD can be employed as
a post-processing method to significantly elevate the quality of the solutions
derived by existing neural TSP solvers via spending affordable additional
computing time. Notably, GELD is shown as capable of solving TSPs with up to
744,710 nodes, first-of-its-kind to solve this large size TSP without relying
on divide-and-conquer strategies to the best of our knowledge.

</details>


### [18] [Contextual Experience Replay for Self-Improvement of Language Agents](https://arxiv.org/abs/2506.06698)
*Yitao Liu,Chenglei Si,Karthik Narasimhan,Shunyu Yao*

Main category: cs.AI

TL;DR: 提出无训练框架CER使语言智能体在上下文窗口高效自我提升，在WebArena和VisualWebArena基准测试中表现佳。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型智能体缺乏环境特定经验，且不能在推理时持续学习过往经验，难以完成复杂顺序决策任务。

Method: 提出Contextual Experience Replay (CER)框架，将过往经验积累并合成到动态内存缓冲区，使智能体在新任务中检索和增强自身知识。

Result: 在VisualWebArena上达到31.9%的竞争力表现，在WebArena上平均成功率达36.7%，相对提升GPT - 4o智能体基线成功率51.0%。

Conclusion: CER框架有效、高效，能提升语言智能体在复杂环境中的适应性。

Abstract: Large language model (LLM) agents have been applied to sequential
decision-making tasks such as web navigation, but without any
environment-specific experiences, they often fail in these complex tasks.
Moreover, current LLM agents are not designed to continually learn from past
experiences during inference time, which could be crucial for them to gain
these environment-specific experiences. To address this, we propose Contextual
Experience Replay (CER), a training-free framework to enable efficient
self-improvement for language agents in their context window. Specifically, CER
accumulates and synthesizes past experiences into a dynamic memory buffer.
These experiences encompass environment dynamics and common decision-making
patterns, allowing the agents to retrieve and augment themselves with relevant
knowledge in new tasks, enhancing their adaptability in complex environments.
We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On
VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena,
CER also gets a competitive average success rate of 36.7%, relatively improving
the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a
comprehensive analysis on it to prove its efficiency, validity and understand
it better.

</details>


### [19] [Integrating AI Planning Semantics into SysML System Models for Automated PDDL File Generation](https://arxiv.org/abs/2506.06714)
*Hamied Nabizada,Tom Jeleniewski,Lasse Beers,Maximilian Weigand,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: 提出可将PDDL规划语义集成到系统模型的SysML概要文件，以飞机制造案例展示应用。


<details>
  <summary>Details</summary>
Motivation: 在系统模型中直接集成基于PDDL的规划语义，搭建系统建模与AI规划间的桥梁。

Method: 定义PDDL关键概念的可复用构造型，用OCL约束确保语法一致性，从BNF定义派生概要文件。

Result: 通过飞机制造案例，可生成PDDL格式的领域和问题描述，用于求解优化执行计划。

Conclusion: 该方法支持自动化和基于模型的规划描述生成，为工程设计中系统建模和AI规划提供可复用桥梁。

Abstract: This paper presents a SysML profile that enables the direct integration of
planning semantics based on the Planning Domain Definition Language (PDDL) into
system models. Reusable stereotypes are defined for key PDDL concepts such as
types, predicates, functions and actions, while formal OCL constraints ensure
syntactic consistency. The profile was derived from the Backus-Naur Form (BNF)
definition of PDDL 3.1 to align with SysML modeling practices. A case study
from aircraft manufacturing demonstrates the application of the profile: a
robotic system with interchangeable end effectors is modeled and enriched to
generate both domain and problem descriptions in PDDL format. These are used as
input to a PDDL solver to derive optimized execution plans. The approach
supports automated and model-based generation of planning descriptions and
provides a reusable bridge between system modeling and AI planning in
engineering design.

</details>


### [20] [REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models](https://arxiv.org/abs/2506.07759)
*Diego Forniés-Tabuenca,Alejandro Uribe,Urtzi Otamendi,Arkaitz Artetxe,Juan Carlos Rivera,Oier Lopez de Lacalle*

Main category: cs.AI

TL;DR: 提出REMoH框架结合NSGA - II与大语言模型启发式生成，用于多目标优化，在FJSSP问题上表现良好，展示了大语言模型增强传统优化的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统多目标优化算法需大量特定问题建模且难适应非线性结构，大语言模型有增强可解释性、适应性和推理能力的优势。

Method: 提出REMoH框架，将NSGA - II与基于大语言模型的启发式生成相结合，用聚类和搜索空间反思的反思机制指导启发式创建。

Result: 在FJSSP问题的三个实例数据集上评估，REMoH与现有方法相比取得有竞争力的结果，减少建模工作并增强适应性。

Conclusion: 大语言模型可增强传统优化，在多目标场景中提供更大灵活性、可解释性和鲁棒性。

Abstract: Multi-objective optimization is fundamental in complex decision-making tasks.
Traditional algorithms, while effective, often demand extensive
problem-specific modeling and struggle to adapt to nonlinear structures. Recent
advances in Large Language Models (LLMs) offer enhanced explainability,
adaptability, and reasoning. This work proposes Reflective Evolution of
Multi-objective Heuristics (REMoH), a novel framework integrating NSGA-II with
LLM-based heuristic generation. A key innovation is a reflection mechanism that
uses clustering and search-space reflection to guide the creation of diverse,
high-quality heuristics, improving convergence and maintaining solution
diversity. The approach is evaluated on the Flexible Job Shop Scheduling
Problem (FJSSP) in-depth benchmarking against state-of-the-art methods using
three instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate
that REMoH achieves competitive results compared to state-of-the-art approaches
with reduced modeling effort and enhanced adaptability. These findings
underscore the potential of LLMs to augment traditional optimization, offering
greater flexibility, interpretability, and robustness in multi-objective
scenarios.

</details>


### [21] [WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making](https://arxiv.org/abs/2506.06725)
*Guillaume Levy,Cedric Colas,Pierre-Yves Oudeyer,Thomas Carta,Clement Romac*

Main category: cs.AI

TL;DR: 提出WorldLLM框架，结合贝叶斯推理、主动探索和强化学习，提升大语言模型在特定领域的预测能力，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在结构化、特定领域情境中难以生成精确预测，原因是无法将广泛知识与特定环境结合。

Method: 提出WorldLLM框架，结合贝叶斯推理和强化学习，利用大语言模型上下文学习能力，通过迭代优化假设和收集证据来提升预测。

Result: 在文本游戏环境实验中，框架提升了预测准确性，还生成了可解释的环境动态理论。

Conclusion: WorldLLM框架能有效提升大语言模型在特定领域的预测能力和生成可解释理论。

Abstract: Large Language Models (LLMs) possess general world knowledge but often
struggle to generate precise predictions in structured, domain-specific
contexts such as simulations. These limitations arise from their inability to
ground their broad, unstructured understanding in specific environments. To
address this, we present WorldLLM, a framework that enhances LLM-based world
modeling by combining Bayesian inference and autonomous active exploration with
reinforcement learning. WorldLLM leverages the in-context learning abilities of
LLMs to guide an LLM-based world model's predictions using natural language
hypotheses given in its prompt. These hypotheses are iteratively refined
through a Bayesian inference framework that leverages a second LLM as the
proposal distribution given collected evidence. This evidence is collected
using a curiosity-driven reinforcement learning policy that explores the
environment to find transitions with a low log-likelihood under our LLM-based
predictive model using the current hypotheses. By alternating between refining
hypotheses and collecting new evidence, our framework autonomously drives
continual improvement of the predictions. Our experiments demonstrate the
effectiveness of WorldLLM in a textual game environment that requires agents to
manipulate and combine objects. The framework not only enhances predictive
accuracy, but also generates human-interpretable theories of environment
dynamics.

</details>


### [22] [VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs](https://arxiv.org/abs/2506.06727)
*Can Li,Ting Zhang,Mei Wang,Hua Huang*

Main category: cs.AI

TL;DR: 引入VisioMath基准测试多模态下含图像答案选项的数学推理能力，评估现有模型发现其存在局限，为未来研究提供测试平台。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在含图像答案选项的数学推理能力未充分研究，需填补此空白。

Method: 创建包含8070张图像和1800道选择题的VisioMath基准，系统评估现有先进LMMs。

Result: 最先进模型在该任务表现不佳，如GPT - 4o准确率仅45.9%。

Conclusion: VisioMath填补现有基准空白，为多模态推理研究提供严格测试平台，推动相关发展。

Abstract: Large Multimodal Models (LMMs) have demonstrated remarkable problem-solving
capabilities across various domains. However, their ability to perform
mathematical reasoning when answer options are represented as images--an
essential aspect of multi-image comprehension--remains underexplored. To bridge
this gap, we introduce VisioMath, a benchmark designed to evaluate mathematical
reasoning in multimodal contexts involving image-based answer choices.
VisioMath comprises 8,070 images and 1,800 multiple-choice questions, where
each answer option is an image, presenting unique challenges to existing LMMs.
To the best of our knowledge, VisioMath is the first dataset specifically
tailored for mathematical reasoning in image-based-option scenarios, where
fine-grained distinctions between answer choices are critical for accurate
problem-solving. We systematically evaluate state-of-the-art LMMs on VisioMath
and find that even the most advanced models struggle with this task. Notably,
GPT-4o achieves only 45.9% accuracy, underscoring the limitations of current
models in reasoning over visually similar answer choices. By addressing a
crucial gap in existing benchmarks, VisioMath establishes a rigorous testbed
for future research, driving advancements in multimodal reasoning.

</details>


### [23] [Honey, I shrunk the hypothesis space (through logical preprocessing)](https://arxiv.org/abs/2506.06739)
*Andrew Cropper,Filipe Gouveia,David M. Cerna*

Main category: cs.AI

TL;DR: 提出在ILP系统搜索前缩小假设空间的方法，实验表明能大幅减少学习时间并保持准确率。


<details>
  <summary>Details</summary>
Motivation: 为ILP系统搜索假设空间时缩小范围，提高效率。

Method: 利用背景知识找出无论训练示例如何都不可能在最优假设中的规则，移除违反规则，用答案集编程实现。

Result: 在多个领域实验显示能大幅减少学习时间，如从超10小时减至2秒，且保持预测准确率。

Conclusion: 该方法有效，能在保持准确率前提下显著减少ILP系统学习时间。

Abstract: Inductive logic programming (ILP) is a form of logical machine learning. The
goal is to search a hypothesis space for a hypothesis that generalises training
examples and background knowledge. We introduce an approach that 'shrinks' the
hypothesis space before an ILP system searches it. Our approach uses background
knowledge to find rules that cannot be in an optimal hypothesis regardless of
the training examples. For instance, our approach discovers relationships such
as "even numbers cannot be odd" and "prime numbers greater than 2 are odd". It
then removes violating rules from the hypothesis space. We implement our
approach using answer set programming and use it to shrink the hypothesis space
of a constraint-based ILP system. Our experiments on multiple domains,
including visual reasoning and game playing, show that our approach can
substantially reduce learning times whilst maintaining predictive accuracies.
For instance, given just 10 seconds of preprocessing time, our approach can
reduce learning times from over 10 hours to only 2 seconds.

</details>


### [24] [Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data](https://arxiv.org/abs/2506.07390)
*Xin-Cheng Wen,Yijun Yang,Cuiyun Gao,Yang Xiao,Deheng Ye*

Main category: cs.AI

TL;DR: 现有大语言模型检测软件漏洞能力有限，本文提出ReVD框架，在PrimeVul和SVEN数据集上实验显示其提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型检测软件漏洞能力受限，原因包括缺乏漏洞推理数据和注重语义表征学习，且高质量数据集稀缺。

Method: 构建ReVD框架，通过推理数据合成和特定漏洞偏好优化挖掘漏洞模式，包括构建推理过程和设计监督微调及课程在线偏好优化。

Result: 在PrimeVul和SVEN数据集上实验，ReVD相比现有方法在准确率上提升12.24%-22.77%。

Conclusion: ReVD框架能有效提升基于大语言模型的软件漏洞检测性能，达当前最优水平。

Abstract: Large language models (LLMs) demonstrate considerable proficiency in numerous
coding-related tasks; however, their capabilities in detecting software
vulnerabilities remain limited. This limitation primarily stems from two
factors: (1) the absence of reasoning data related to vulnerabilities, which
hinders the models' ability to capture underlying vulnerability patterns; and
(2) their focus on learning semantic representations rather than the reason
behind them, thus failing to recognize semantically similar vulnerability
samples. Furthermore, the development of LLMs specialized in vulnerability
detection is challenging, particularly in environments characterized by the
scarcity of high-quality datasets. In this paper, we propose a novel framework
ReVD that excels at mining vulnerability patterns through reasoning data
synthesizing and vulnerability-specific preference optimization. Specifically,
we construct forward and backward reasoning processes for vulnerability and
corresponding fixed code, ensuring the synthesis of high-quality reasoning
data. Moreover, we design the triplet supervised fine-tuning followed by
curriculum online preference optimization for enabling ReVD to better
understand vulnerability patterns. The extensive experiments conducted on
PrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for
LLM-based software vulnerability detection, e.g., 12.24\%-22.77\% improvement
in the accuracy. The source code and data are available at
https://github.com/Xin-Cheng-Wen/PO4Vul.

</details>


### [25] [AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and Reactive Outcome Optimization Method](https://arxiv.org/abs/2506.06740)
*Yigui Feng,Qinglin Wang,Ke Liu,Xinhai Chen,Bo Yang,Jie Liu*

Main category: cs.AI

TL;DR: 本文提出AI PsyRoom框架以改进心理咨询，构建数据集EmoPsy，在多方面表现优于现有方法，且数据集和模型公开。


<details>
  <summary>Details</summary>
Motivation: 心理健康服务需求增长和专业人员短缺，现有大语言模型缺乏对情绪的深入理解，无法生成个性化治疗方案。

Method: 提出多智能体模拟框架AI PsyRoom，利用细粒度情绪分类和多智能体框架，构建用于对话重建的PsyRoom A和生成个性化治疗方案的PsyRoom B。

Result: AI PsyRoom在问题导向、表达、共情和交互沟通质量方面分别提升18%、23%、24%和16%。

Conclusion: 数据集和模型公开，为人工智能辅助心理咨询研究提供基础。

Abstract: Psychological counseling faces huge challenges due to the growing demand for
mental health services and the shortage of trained professionals. Large
language models (LLMs) have shown potential to assist psychological counseling,
especially in empathy and emotional support. However, existing models lack a
deep understanding of emotions and are unable to generate personalized
treatment plans based on fine-grained emotions. To address these shortcomings,
we present AI PsyRoom, a multi-agent simulation framework designed to enhance
psychological counseling by generating empathetic and emotionally nuanced
conversations. By leveraging fine-grained emotion classification and a
multi-agent framework, we construct a multi-agent PsyRoom A for dialogue
reconstruction, generating a high-quality dialogue dataset EmoPsy, which
contains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues.
We also propose PsyRoom B for generating personalized treatment plans.
Quantitative evaluations demonstrate that AI PsyRoom significantly outperforms
state-of-the-art methods, achieving 18% improvement in problem orientation, 23%
in expression, 24% in Empathy, and 16% in interactive communication quality.
The datasets and models are publicly available, providing a foundation for
advancing AI-assisted psychological counseling research.

</details>


### [26] [Bio-Inspired Classification: Combining Information Theory and Spiking Neural Networks -- Influence of the Learning Rules](https://arxiv.org/abs/2506.06750)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: cs.AI

TL;DR: 本文探讨不同学习算法对脉冲神经网络（SNN）分类精度的影响，提出基于SNN和Lempel - Ziv复杂度（LZC）的生物启发分类器，指出选择算法需权衡精度、计算成本和应用约束。


<details>
  <summary>Details</summary>
Motivation: 由于SNN的独特属性，其训练具有挑战性，因此广泛考虑所选学习算法类型对分类精度的影响。

Method: 提出基于SNN和LZC的生物启发分类器，结合SNN的时间精度和生物现实性与LZC的结构复杂性分析。

Result: 经典反向传播算法分类精度高但计算成本极高，不适合实时应用；生物启发学习算法如tempotron和Spikprop计算效率高且分类性能有竞争力，适合时间敏感任务。

Conclusion: 选择最合适的学习算法取决于分类精度和计算成本的权衡以及应用约束。

Abstract: Training of Spiking Neural Networks (SNN) is challenging due to their unique
properties, including temporal dynamics, non-differentiability of spike events,
and sparse event-driven activations. In this paper, we widely consider the
influence of the type of chosen learning algorithm, including bioinspired
learning rules on the accuracy of classification. We proposed a bioinspired
classifier based on the combination of SNN and Lempel-Ziv complexity (LZC).
This approach synergizes the strengths of SNNs in temporal precision and
biological realism with LZC's structural complexity analysis, facilitating
efficient and interpretable classification of spatiotemporal neural data. It
turned out that the classic backpropagation algorithm achieves excellent
classification accuracy, but at extremely high computational cost, which makes
it impractical for real-time applications. Biologically inspired learning
algorithms such as tempotron and Spikprop provide increased computational
efficiency while maintaining competitive classification performance, making
them suitable for time-sensitive tasks. The results obtained indicate that the
selection of the most appropriate learning algorithm depends on the trade-off
between classification accuracy and computational cost as well as application
constraints.

</details>


### [27] [Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven Information Gain](https://arxiv.org/abs/2506.06786)
*Dimitris Panagopoulos,Adolfo Perrusquia,Weisi Guo*

Main category: cs.AI

TL;DR: 提出CA - MIQ框架用于高风险搜索救援任务的自主系统，在模拟环境测试中表现优于基线方法，凸显其在特定离散环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 高风险搜索救援任务的自主系统需灵活适应任务优先级变化，持续收集关键信息。

Method: 提出CA - MIQ轻量级双评论强化学习框架，将标准外在评论器与融合多种信息的内在评论器配对，内置转移检测器触发探索增强和评论器重置。

Result: 在模拟SAR网格世界中，单次优先级转移后任务成功率近四倍于基线，多次转移场景下性能超三倍，能100%恢复，而基线方法无法适应。

Conclusion: CA - MIQ在具有分段平稳信息价值分布的离散环境中有效。

Abstract: Autonomous systems operating in high-stakes search-and-rescue (SAR) missions
must continuously gather mission-critical information while flexibly adapting
to shifting operational priorities. We propose CA-MIQ (Context-Aware
Max-Information Q-learning), a lightweight dual-critic reinforcement learning
(RL) framework that dynamically adjusts its exploration strategy whenever
mission priorities change. CA-MIQ pairs a standard extrinsic critic for task
reward with an intrinsic critic that fuses state-novelty, information-location
awareness, and real-time priority alignment. A built-in shift detector triggers
transient exploration boosts and selective critic resets, allowing the agent to
re-focus after a priority revision. In a simulated SAR grid-world, where
experiments specifically test adaptation to changes in the priority order of
information types the agent is expected to focus on, CA-MIQ achieves nearly
four times higher mission-success rates than baselines after a single priority
shift and more than three times better performance in multiple-shift scenarios,
achieving 100% recovery while baseline methods fail to adapt. These results
highlight CA-MIQ's effectiveness in any discrete environment with
piecewise-stationary information-value distributions.

</details>


### [28] [A Temporal FRBR/FRBRoo-Based Model for Component-Level Versioning of Legal Norms](https://arxiv.org/abs/2506.07853)
*Hudson de Martim*

Main category: cs.AI

TL;DR: 本文提出扩展FRBRoo框架的结构化时间模型，以解决法律规范组件级版本控制问题，通过巴西联邦宪法案例证明可精确检索和重建特定时间的法律文本，为法律信息系统等提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有FRBR/FRBRoo和Akoma Ntoso等框架缺乏组件级版本控制机制，阻碍法律文本的时间点重建，影响法律科技和AI应用。

Method: 提出结构化时间模型，引入Expressio的子类Temporal Version和Language Version，分层应用该范式，引入Component Work、Component Temporal Version和Component Language Version。以巴西联邦宪法为案例研究。

Result: 以巴西联邦宪法为例，展示修正案如何创建新的Component Temporal Versions，未受影响组件保留现有版本。该模型能精确检索和重建特定日期的法律文本。

Conclusion: 该模型克服当前生成模型的局限，为开发高级法律信息系统、知识图谱和AI工具提供坚实基础。

Abstract: Effectively representing legal norms for automated processing is a critical
challenge, particularly in tracking the diachronic evolution of their
hierarchical components (e.g., articles, paragraphs). While foundational
frameworks like FRBR/FRBRoo and standards like Akoma Ntoso model legal
documents at a macro level, they lack native mechanisms for granular,
component-level versioning. This limitation hinders the deterministic
point-in-time reconstruction of legal texts, a fundamental capability for
reliable Legal Tech and AI applications. This paper proposes a structured,
temporal model that extends the FRBRoo framework to address this gap. It
introduces specialized subclasses of Expressio - Temporal Version (TV) and
Language Version (LV - to represent the state of a legal norm and its
linguistic variations at specific points in time. The model applies this same
paradigm hierarchically, introducing Component Work (CW), Component Temporal
Version (CTV), and Component Language Version (CLV) to track the lifecycle of
individual articles, paragraphs, and clauses. Using the Brazilian Federal
Constitution as a case study, the paper demonstrates how each amendment creates
new Component Temporal Versions for affected provisions, while unaffected
components retain their existing versions. This fine-grained, time-aware
architecture enables the precise, deterministic retrieval and reconstruction of
any part of a legal text as it existed on a specific date. The model provides a
robust foundation for developing advanced legal information systems, knowledge
graphs, and AI tools capable of accurate historical analysis and impact
assessment, overcoming the limitations of current generative models.

</details>


### [29] [United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory](https://arxiv.org/abs/2506.06843)
*HaoYang Shang,Xuan Liu,Zi Liang,Jie Zhang,Haibo Hu,Song Guo*

Main category: cs.AI

TL;DR: 论文指出大语言模型在复杂任务上有性能上限，类比认知负荷理论提出CoThinker框架，经实验验证有性能提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂多面任务上存在性能上限，需求超其有效认知负荷容量。

Method: 引入基于大语言模型的多智能体框架CoThinker，通过智能体专业化分配内在认知负荷，通过结构化通信和集体工作记忆管理事务负荷。

Result: 在复杂问题解决任务和高认知负荷场景中验证，在解决方案质量和效率上优于现有多智能体基线。

Conclusion: 分析揭示了特征交互模式，为克服大语言模型性能上限提供了原则性方法。

Abstract: Large Language Models (LLMs) exhibit a notable performance ceiling on
complex, multi-faceted tasks, as they often fail to integrate diverse
information or adhere to multiple constraints. We posit that such limitation
arises when the demands of a task exceed the LLM's effective cognitive load
capacity. This interpretation draws a strong analogy to Cognitive Load Theory
(CLT) in cognitive science, which explains similar performance boundaries in
the human mind, and is further supported by emerging evidence that reveals LLMs
have bounded working memory characteristics. Building upon this CLT-grounded
understanding, we introduce CoThinker, a novel LLM-based multi-agent framework
designed to mitigate cognitive overload and enhance collaborative
problem-solving abilities. CoThinker operationalizes CLT principles by
distributing intrinsic cognitive load through agent specialization and managing
transactional load via structured communication and a collective working
memory. We empirically validate CoThinker on complex problem-solving tasks and
fabricated high cognitive load scenarios, demonstrating improvements over
existing multi-agent baselines in solution quality and efficiency. Our analysis
reveals characteristic interaction patterns, providing insights into the
emergence of collective cognition and effective load management, thus offering
a principled approach to overcoming LLM performance ceilings.

</details>


### [30] [Incorporating Failure of Machine Learning in Dynamic Probabilistic Safety Assurance](https://arxiv.org/abs/2506.06868)
*Razieh Arshadizadeh,Mahmoud Asgari,Zeinab Khosravi,Yiannis Papadopoulos,Koorosh Aslansefat*

Main category: cs.AI

TL;DR: 提出集成SafeML与贝叶斯网络的概率安全保证框架，用于对含机器学习组件的系统进行安全评估，并在汽车列队模拟系统验证。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型集成到安全关键系统有推理失败风险，传统安全评估方法不适用于机器学习组件，SafeML可检测数据分布偏移但需更完善分析。

Method: 构建集成SafeML与贝叶斯网络的概率安全保证框架，对机器学习故障进行因果安全分析。

Result: 在模拟汽车列队系统结合交通标志识别验证该框架。

Conclusion: 在安全评估中显式建模机器学习故障有更广泛益处。

Abstract: Machine Learning (ML) models are increasingly integrated into safety-critical
systems, such as autonomous vehicle platooning, to enable real-time
decision-making. However, their inherent imperfection introduces a new class of
failure: reasoning failures often triggered by distributional shifts between
operational and training data. Traditional safety assessment methods, which
rely on design artefacts or code, are ill-suited for ML components that learn
behaviour from data. SafeML was recently proposed to dynamically detect such
shifts and assign confidence levels to the reasoning of ML-based components.
Building on this, we introduce a probabilistic safety assurance framework that
integrates SafeML with Bayesian Networks (BNs) to model ML failures as part of
a broader causal safety analysis. This allows for dynamic safety evaluation and
system adaptation under uncertainty. We demonstrate the approach on an
simulated automotive platooning system with traffic sign recognition. The
findings highlight the potential broader benefits of explicitly modelling ML
failures in safety assessment.

</details>


### [31] [KnowCoder-V2: Deep Knowledge Analysis](https://arxiv.org/abs/2506.06881)
*Zixuan Li,Wenxuan Liu,Long Bai,Chunmao Zhang,Wei Li,Fenghui Zhang,Quanxin Jin,Ruoyun He,Zhuo Chen,Zhilei Hu,Fei Wang,Bingbing Xu,Xuhui Jiang,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: 提出KDR框架赋予深度研究深度知识分析能力，引入KCII LLM，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究框架在知识组织管理、任务效率和复杂知识计算方面存在挑战，需改进。

Method: 提出KDR框架，包含离线知识组织和在线复杂知识计算；引入KCII LLM通过统一代码生成连接知识组织和推理。

Result: 在三十多个数据集、六个知识分析任务上的实验证明KCII有效，集成到KDR框架可生成高质量报告。

Conclusion: KDR框架和KCII LLM能有效解决深度知识分析任务，优于主流框架。

Abstract: Deep knowledge analysis tasks always involve the systematic extraction and
association of knowledge from large volumes of data, followed by logical
reasoning to discover insights. However, to solve such complex tasks, existing
deep research frameworks face three major challenges: 1) They lack systematic
organization and management of knowledge; 2) They operate purely online, making
it inefficient for tasks that rely on shared and large-scale knowledge; 3) They
cannot perform complex knowledge computation, limiting their abilities to
produce insightful analytical results. Motivated by these, in this paper, we
propose a \textbf{K}nowledgeable \textbf{D}eep \textbf{R}esearch (\textbf{KDR})
framework that empowers deep research with deep knowledge analysis capability.
Specifically, it introduces an independent knowledge organization phase to
preprocess large-scale, domain-relevant data into systematic knowledge offline.
Based on this knowledge, it extends deep research with an additional kind of
reasoning steps that perform complex knowledge computation in an online manner.
To enhance the abilities of LLMs to solve knowledge analysis tasks in the above
framework, we further introduce \textbf{\KCII}, an LLM that bridges knowledge
organization and reasoning via unified code generation. For knowledge
organization, it generates instantiation code for predefined classes,
transforming data into knowledge objects. For knowledge computation, it
generates analysis code and executes on the above knowledge objects to obtain
deep analysis results. Experimental results on more than thirty datasets across
six knowledge analysis tasks demonstrate the effectiveness of \KCII. Moreover,
when integrated into the KDR framework, \KCII can generate high-quality reports
with insightful analytical results compared to the mainstream deep research
framework.

</details>


### [32] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta,Amos Storkey,Mirella Lapata*

Main category: cs.AI

TL;DR: 本文针对小LMMs的ICL性能不稳定问题，提出元学习方法，用固定软提示提升小样本能力，实验表明该方法优于ICL及相关提示调优方法。


<details>
  <summary>Details</summary>
Motivation: LMMs的ICL性能，尤其是小模型的ICL性能不稳定，不随示例增加单调提升，推测是图像嵌入中的额外信息干扰。

Method: 提出元学习方法，用从任务相关图像特征中提取的固定软提示，引入注意力映射器模块，与软提示联合学习，在低数据下通过少量梯度步骤实现任务适配。

Result: 在VL - ICL Bench上的评估显示，该方法在图像扰动下也能持续超越ICL和相关提示调优方法，提升视觉问答任务的任务诱导和推理能力。

Conclusion: 所提的元学习方法有效解决了LMMs的ICL性能问题，能更好地适应低数据任务场景。

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


### [33] [Causal Graph based Event Reasoning using Semantic Relation Experts](https://arxiv.org/abs/2506.06910)
*Mahnaz Koupaee,Xueying Bai,Mudan Chen,Greg Durrett,Nathanael Chambers,Niranjan Balasubramanian*

Main category: cs.AI

TL;DR: 研究用因果事件图帮助大语言模型进行事件推理，提出协作式因果图生成方法，应用于下游任务有好效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以准确识别事件因果关系，导致推理任务表现不佳，需解决此挑战。

Method: 提出协作式因果图生成方法，用大语言模型模拟关注特定语义关系的专家进行多轮讨论并整合。

Result: 因果图解释更具信息性和连贯性，整体方法在预测和下一事件预测任务上取得与最先进模型有竞争力的结果。

Conclusion: 生成因果事件图可帮助大语言模型明确表示因果关系，提升推理能力。

Abstract: Understanding how events in a scenario causally connect with each other is
important for effectively modeling and reasoning about events. But event
reasoning remains a difficult challenge, and despite recent advances, Large
Language Models (LLMs) still struggle to accurately identify causal connections
between events. This struggle leads to poor performance on deeper reasoning
tasks like event forecasting and timeline understanding. To address this
challenge, we investigate the generation of causal event graphs (e.g., A
enables B) as a parallel mechanism to help LLMs explicitly represent causality
during inference. This paper evaluates both how to generate correct graphs as
well as how graphs can assist reasoning. We propose a collaborative approach to
causal graph generation where we use LLMs to simulate experts that focus on
specific semantic relations. The experts engage in multiple rounds of
discussions which are then consolidated by a final expert. Then, to demonstrate
the utility of causal graphs, we use them on multiple downstream applications,
and also introduce a new explainable event prediction task that requires a
causal chain of events in the explanation. These explanations are more
informative and coherent than baseline generations. Finally, our overall
approach not finetuned on any downstream task, achieves competitive results
with state-of-the-art models on both forecasting and next event prediction
tasks.

</details>


### [34] [Boosting LLM Reasoning via Spontaneous Self-Correction](https://arxiv.org/abs/2506.06923)
*Xutong Zhao,Tengyu Xu,Xuewei Wang,Zhengxing Chen,Di Jin,Liang Tan,Yen-Ting,Zishun Yu,Zhuokai Zhao,Yun He,Sinong Wang,Han Fang,Sarath Chandar,Chen Zhu*

Main category: cs.AI

TL;DR: 提出SPOC自发自我修正方法，使大语言模型在单次推理中生成交错的解决方案和验证，在数学推理基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有自修正方法将修正视为独立的生成后改进，无法进行实时、自发的单次自修正，需解决数学推理难题。

Method: 提出SPOC方法，让模型在单次推理中生成交错的解决方案和验证；从多智能体视角为模型分配提案者和验证者角色；生成合成数据微调，通过在线强化学习提高提案和验证准确性。

Result: SPOC显著提升性能，如提升Llama - 3.1 - 8B和70B Instruct模型在MATH500、AMC23、AIME24上的准确率。

Conclusion: SPOC方法在提升大语言模型数学推理能力方面有效。

Abstract: While large language models (LLMs) have demonstrated remarkable success on a
broad range of tasks, math reasoning remains a challenging one. One of the
approaches for improving math reasoning is self-correction, which designs
self-improving loops to let the model correct its own mistakes. However,
existing self-correction approaches treat corrections as standalone
post-generation refinements, relying on extra prompt and system designs to
elicit self-corrections, instead of performing real-time, spontaneous
self-corrections in a single pass. To address this, we propose SPOC, a
spontaneous self-correction approach that enables LLMs to generate interleaved
solutions and verifications in a single inference pass, with generation
dynamically terminated based on verification outcomes, thereby effectively
scaling inference time compute. SPOC considers a multi-agent perspective by
assigning dual roles -- solution proposer and verifier -- to the same model. We
adopt a simple yet effective approach to generate synthetic data for
fine-tuning, enabling the model to develop capabilities for self-verification
and multi-agent collaboration. We further improve its solution proposal and
verification accuracy through online reinforcement learning. Experiments on
mathematical reasoning benchmarks show that SPOC significantly improves
performance. Notably, SPOC boosts the accuracy of Llama-3.1-8B and 70B Instruct
models, achieving gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23,
and 3.3% and 6.7% on AIME24, respectively.

</details>


### [35] [An Agentic Framework for Autonomous Metamaterial Modeling and Inverse Design](https://arxiv.org/abs/2506.06935)
*Darui Lu,Jordan M. Malof,Willie J. Padilla*

Main category: cs.AI

TL;DR: 开发用于光子超材料逆设计的代理框架，展示其自动化、推理等能力及灵活输出。


<details>
  <summary>Details</summary>
Motivation: 集成多个大语言模型系统的进展使代理框架能自主执行复杂任务，由此开发用于光子超材料逆设计的框架。

Method: 当查询所需光谱时，代理自主提出并开发正向深度学习模型，通过API访问外部工具，利用内存，通过深度逆方法生成最终设计。

Result: 框架展示了自动化、推理、规划和适应的能力。

Conclusion: 代理框架具有内部反思和决策灵活性，可产生高度多样化和潜在新颖的输出。

Abstract: Recent significant advances in integrating multiple Large Language Model
(LLM) systems have enabled Agentic Frameworks capable of performing complex
tasks autonomously, including novel scientific research. We develop and
demonstrate such a framework specifically for the inverse design of photonic
metamaterials. When queried with a desired optical spectrum, the Agent
autonomously proposes and develops a forward deep learning model, accesses
external tools via APIs for tasks like simulation and optimization, utilizes
memory, and generates a final design via a deep inverse method. The framework's
effectiveness is demonstrated in its ability to automate, reason, plan, and
adapt. Notably, the Agentic Framework possesses internal reflection and
decision flexibility, permitting highly varied and potentially novel outputs.

</details>


### [36] [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941)
*Parshin Shojaee,Iman Mirzadeh,Keivan Alizadeh,Maxwell Horton,Samy Bengio,Mehrdad Farajtabar*

Main category: cs.AI

TL;DR: 本文借助可控谜题环境研究大推理模型（LRMs），发现其在特定复杂度后准确率崩溃、有反直觉缩放限制，对比LRMs与标准大语言模型得出三种性能模式，还揭示LRMs精确计算有局限。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LRMs的基本能力、缩放特性和局限性理解不足，当前评估范式存在污染问题且无法洞察推理痕迹。

Method: 利用可控谜题环境，该环境能精确控制复杂度并保持一致逻辑结构，进行大量实验，对比LRMs与标准大语言模型在相同推理计算下的表现。

Result: LRMs在特定复杂度后准确率完全崩溃，有反直觉缩放限制；对比得出三种性能模式；LRMs精确计算有局限，推理不一致。

Conclusion: 研究揭示了LRMs的优势、局限性，对其推理能力提出疑问。

Abstract: Recent generations of language models have introduced Large Reasoning Models
(LRMs) that generate detailed thinking processes before providing answers.
While these models demonstrate improved performance on reasoning benchmarks,
their fundamental capabilities, scaling properties, and limitations remain
insufficiently understood. Current evaluations primarily focus on established
math and coding benchmarks, emphasizing final answer accuracy. However, this
evaluation paradigm often suffers from contamination and does not provide
insights into the reasoning traces. In this work, we systematically investigate
these gaps with the help of controllable puzzle environments that allow precise
manipulation of complexity while maintaining consistent logical structures.
This setup enables the analysis of not only final answers but also the internal
reasoning traces, offering insights into how LRMs think. Through extensive
experiments, we show that LRMs face a complete accuracy collapse beyond certain
complexities. Moreover, they exhibit a counterintuitive scaling limit: their
reasoning effort increases with problem complexity up to a point, then declines
despite having remaining token budget. By comparing LRMs with their standard
LLM counterparts under same inference compute, we identify three performance
regimes: (1) low-complexity tasks where standard models outperform LRMs, (2)
medium-complexity tasks where LRMs demonstrates advantage, and (3)
high-complexity tasks where both models face complete collapse. We found that
LRMs have limitations in exact computation: they fail to use explicit
algorithms and reason inconsistently across scales. We also investigate the
reasoning traces in more depth, studying the patterns of explored solutions and
analyzing the models' computational behavior, shedding light on their
strengths, limitations, and raising questions about their reasoning
capabilities.

</details>


### [37] [Deontically Constrained Policy Improvement in Reinforcement Learning Agents](https://arxiv.org/abs/2506.06959)
*Alena Makarova,Houssam Abbas*

Main category: cs.AI

TL;DR: 本文探讨在马尔可夫决策过程（MDPs）中学习满足道义逻辑约束且最大化效用的决策策略，开发策略改进方法并实验验证。


<details>
  <summary>Details</summary>
Motivation: 考虑在MDPs中学习决策策略时，使代理在满足道义逻辑约束的同时最大化效用。

Method: 使用期望行为功利主义逻辑，开发策略改进的变体方法。

Result: 该方法能达到任务效用的受限局部最大值。

Conclusion: 此方法可视为在双层结构中同时最大化两个价值函数，通过样本MDPs实验进行了验证。

Abstract: Markov Decision Processes (MDPs) are the most common model for decision
making under uncertainty in the Machine Learning community. An MDP captures
non-determinism, probabilistic uncertainty, and an explicit model of action. A
Reinforcement Learning (RL) agent learns to act in an MDP by maximizing a
utility function. This paper considers the problem of learning a decision
policy that maximizes utility subject to satisfying a constraint expressed in
deontic logic. In this setup, the utility captures the agent's mission - such
as going quickly from A to B. The deontic formula represents (ethical, social,
situational) constraints on how the agent might achieve its mission by
prohibiting classes of behaviors. We use the logic of Expected Act
Utilitarianism, a probabilistic stit logic that can be interpreted over
controlled MDPs. We develop a variation on policy improvement, and show that it
reaches a constrained local maximum of the mission utility. Given that in stit
logic, an agent's duty is derived from value maximization, this can be seen as
a way of acting to simultaneously maximize two value functions, one of which is
implicit, in a bi-level structure. We illustrate these results with experiments
on sample MDPs.

</details>


### [38] [Long-Tailed Learning for Generalized Category Discovery](https://arxiv.org/abs/2506.06965)
*Cuong Manh Hoang*

Main category: cs.AI

TL;DR: 提出用于长尾分布的广义类别发现新框架，实验表明模型超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有广义类别发现方法在真实世界不平衡数据集上效果不佳，需新方法解决此问题。

Method: 提出自引导标记技术生成伪标签，引入表示平衡过程挖掘样本邻域。

Result: 在公共数据集上实验，模型超越先前的最优方法。

Conclusion: 所提出的框架在长尾分布的广义类别发现中有效。

Abstract: Generalized Category Discovery (GCD) utilizes labeled samples of known
classes to discover novel classes in unlabeled samples. Existing methods show
effective performance on artificial datasets with balanced distributions.
However, real-world datasets are always imbalanced, significantly affecting the
effectiveness of these methods. To solve this problem, we propose a novel
framework that performs generalized category discovery in long-tailed
distributions. We first present a self-guided labeling technique that uses a
learnable distribution to generate pseudo-labels, resulting in less biased
classifiers. We then introduce a representation balancing process to derive
discriminative representations. By mining sample neighborhoods, this process
encourages the model to focus more on tail classes. We conduct experiments on
public datasets to demonstrate the effectiveness of the proposed framework. The
results show that our model exceeds previous state-of-the-art methods.

</details>


### [39] [Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments](https://arxiv.org/abs/2506.06981)
*Riley Simmons-Edler,Ryan P. Badman,Felix Baastad Berg,Raymond Chua,John J. Vastola,Joshua Lunger,William Qian,Kanaka Rajan*

Main category: cs.AI

TL;DR: 本文将神经科学和动物行为学工具用于研究深度强化学习（DRL）智能体，发现无模型基于RNN的DRL智能体可呈现类似规划的行为，还提炼出通用分析框架，强调跨学科研究对理解和引导智能体行为的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着任务和智能体复杂度增加，标准行为分析方法在DRL中发展不足，需要更好方法理解DRL智能体行为。

Method: 应用神经科学和动物行为学工具，在新的复杂部分可观测环境ForageWorld中对DRL智能体进行联合行为和神经分析。

Result: 发现无模型基于RNN的DRL智能体可通过涌现动力学展现类似规划的行为，且用受神经行为学启发的工具研究智能体可揭示其学习动态中的丰富结构。

Conclusion: 提炼出通用分析框架，强调将神经科学、认知科学和AI结合对理解和引导复杂自主智能体行为至关重要，并可借鉴生物智能研究经验。

Abstract: Understanding the behavior of deep reinforcement learning (DRL) agents --
particularly as task and agent sophistication increase -- requires more than
simple comparison of reward curves, yet standard methods for behavioral
analysis remain underdeveloped in DRL. We apply tools from neuroscience and
ethology to study DRL agents in a novel, complex, partially observable
environment, ForageWorld, designed to capture key aspects of real-world animal
foraging -- including sparse, depleting resource patches, predator threats, and
spatially extended arenas. We use this environment as a platform for applying
joint behavioral and neural analysis to agents, revealing detailed,
quantitatively grounded insights into agent strategies, memory, and planning.
Contrary to common assumptions, we find that model-free RNN-based DRL agents
can exhibit structured, planning-like behavior purely through emergent dynamics
-- without requiring explicit memory modules or world models. Our results show
that studying DRL agents like animals -- analyzing them with
neuroethology-inspired tools that reveal structure in both behavior and neural
dynamics -- uncovers rich structure in their learning dynamics that would
otherwise remain invisible. We distill these tools into a general analysis
framework linking core behavioral and representational features to diagnostic
methods, which can be reused for a wide range of tasks and agents. As agents
grow more complex and autonomous, bridging neuroscience, cognitive science, and
AI will be essential -- not just for understanding their behavior, but for
ensuring safe alignment and maximizing desirable behaviors that are hard to
measure via reward. We show how this can be done by drawing on lessons from how
biological intelligence is studied.

</details>


### [40] [Mathesis: Towards Formal Theorem Proving from Natural Languages](https://arxiv.org/abs/2506.07047)
*Yu Xuejun,Jianyuan Zhong,Zijin Feng,Pengyi Zhai,Roozbeh Yousefzadeh,Wei Chong Ng,Haoxiong Liu,Ziyi Shou,Jing Xiong,Yudong Zhou,Claudia Beth Ong,Austen Jeremy Sugiarto,Yaoxi Zhang,Wai Ming Tai,Huan Cao,Dongcai Lu,Jiacheng Sun,Qiang Xu,Shen Xin,Zhenguo Li*

Main category: cs.AI

TL;DR: 提出首个端到端定理证明管道Mathesis，可处理非正式问题陈述，还引入Gaokao - Formal基准，实验显示其有效性。


<details>
  <summary>Details</summary>
Motivation: 大多数基于大语言模型的定理证明器受限于需要专家编写的形式化陈述作为输入，限制了其对自然语言表达的现实问题的适用性。

Method: 构建Mathesis管道，包含使用强化学习的Mathesis - Autoformalizer和Mathesis - Prover，引入Gaokao - Formal基准进行评估。

Result: 自动形式化器在Gaokao - Formal上通过率比最佳基线高22%，完整系统在MiniF2F上pass@32准确率达64%，在Gaokao - Formal上达18%。

Conclusion: Mathesis在端到端形式化定理证明中有效，能处理自然语言问题，提升了现实问题解决能力。

Abstract: Recent advances in large language models show strong promise for formal
reasoning. However, most LLM-based theorem provers have long been constrained
by the need for expert-written formal statements as inputs, limiting their
applicability to real-world problems expressed in natural language. We tackle
this gap with Mathesis, the first end-to-end theorem proving pipeline
processing informal problem statements. It contributes Mathesis-Autoformalizer,
the first autoformalizer using reinforcement learning to enhance the
formalization ability of natural language problems, aided by our novel
LeanScorer framework for nuanced formalization quality assessment. It also
proposes a Mathesis-Prover, which generates formal proofs from the formalized
statements. To evaluate the real-world applicability of end-to-end formal
theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex
problems from China's national college entrance exam. Our approach is carefully
designed, with a thorough study of each component. Experiments demonstrate
Mathesis's effectiveness, with the autoformalizer outperforming the best
baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other
model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a
state-of-the-art 18% on Gaokao-Formal.

</details>


### [41] [Reasoning Paths as Signals: Augmenting Multi-hop Fact Verification through Structural Reasoning Progression](https://arxiv.org/abs/2506.07075)
*Liwen Zheng,Chaozhuo Li,Haoran Jia,Xi Zhang*

Main category: cs.AI

TL;DR: 提出用于多跳事实核查的结构推理框架，含检索与验证模块，实验表明优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现实场景中事实声明复杂度增加，现有自动事实核查系统在多跳证据聚合和推理方面存在问题，如模型静态、推理路径结构捕捉不足。

Method: 提出结构推理框架，包含结构增强检索机制构建推理图指导证据收集，推理路径引导验证模块构建子图表示推理轨迹，还融入结构感知推理机制捕捉长距离依赖。

Result: 在FEVER和HoVer数据集上的实验显示，该方法持续优于强基线模型。

Conclusion: 推理路径建模在提高检索精度和核查准确性方面有效。

Abstract: The growing complexity of factual claims in real-world scenarios presents
significant challenges for automated fact verification systems, particularly in
accurately aggregating and reasoning over multi-hop evidence. Existing
approaches often rely on static or shallow models that fail to capture the
evolving structure of reasoning paths, leading to fragmented retrieval and
limited interpretability. To address these issues, we propose a Structural
Reasoning framework for Multi-hop Fact Verification that explicitly models
reasoning paths as structured graphs throughout both evidence retrieval and
claim verification stages. Our method comprises two key modules: a
structure-enhanced retrieval mechanism that constructs reasoning graphs to
guide evidence collection, and a reasoning-path-guided verification module that
incrementally builds subgraphs to represent evolving inference trajectories. We
further incorporate a structure-aware reasoning mechanism that captures
long-range dependencies across multi-hop evidence chains, enabling more precise
verification. Extensive experiments on the FEVER and HoVer datasets demonstrate
that our approach consistently outperforms strong baselines, highlighting the
effectiveness of reasoning-path modeling in enhancing retrieval precision and
verification accuracy.

</details>


### [42] [BRIGHT+: Upgrading the BRIGHT Benchmark with MARCUS, a Multi-Agent RAG Clean-Up Suite](https://arxiv.org/abs/2506.07116)
*Liyang Chen,Yujun Cai,Jieqiong Dong,Yiwei Wang*

Main category: cs.AI

TL;DR: 提出MARCUS管线清理BRIGHT数据集成BRIGHT - Plus，实验表明其提升检索和推理能力并开源。


<details>
  <summary>Details</summary>
Motivation: BRIGHT基准受网络抓取产物影响，存在内容冗余和语义不连贯问题，影响检索准确性和下游推理。

Method: 提出MARCUS多智能体管线，利用大语言模型系统清理和重新分块BRIGHT，使用专用智能体进行结构噪声去除和语义分割。

Result: BRIGHT - Plus在多种检索器上的检索准确性和多跳推理能力有显著持续提升。

Conclusion: 发布BRIGHT - Plus语料库和MARCUS管线以支持未来以推理为中心的鲁棒检索研究。

Abstract: Retrieval-Augmented Generation (RAG) systems require corpora that are both
structurally clean and semantically coherent. BRIGHT is a recent and
influential benchmark designed to evaluate complex multi-hop retrieval across
diverse, high-reasoning domains. However, its practical effectiveness is
limited by common web-crawled artifacts - such as content redundancy and
semantic discontinuity - that impair retrieval accuracy and downstream
reasoning. Notably, we find that such issues are concentrated in seven
StackExchange-derived subdomains, while other domains (e.g., Coding and
Theorem-based content) remain relatively clean.
  In this study, we present MARCUS, a multi-agent pipeline that leverages large
language models (LLMs) to systematically clean and re-chunk BRIGHT into a
higher-quality corpus: BRIGHT-Plus. MARCUS applies dedicated agents for
structural noise removal and semantic segmentation, preserving answer-bearing
spans while improving contextual integrity. Experimental evaluations
demonstrate that BRIGHT-Plus yields consistent and significant improvements in
both retrieval accuracy and multi-hop reasoning across a diverse set of
retrievers. We release both the BRIGHT-Plus corpus and the MARCUS pipeline to
support future research on robust, reasoning-centric retrieval.

</details>


### [43] [Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT](https://arxiv.org/abs/2506.07173)
*Miroslav Popovic,Marko Popovic,Miodrag Djukic,Ilija Basicevic*

Main category: cs.AI

TL;DR: 本文介绍用ChatGPT自动将Python中的联邦学习算法翻译成CSP进程，并基于ChatGPT反馈估计所用上下文的最小性，实验验证了翻译过程。


<details>
  <summary>Details</summary>
Motivation: 之前研究手动将框架提供的通用联邦学习算法翻译成CSP进程，本文想实现翻译自动化。

Method: 使用ChatGPT将Python中的联邦学习算法自动翻译成CSP进程，基于ChatGPT反馈估计上下文最小性。

Result: 成功将通用的集中式和分布式联邦学习算法翻译成CSP进程，并通过模型检查器PAT验证。

Conclusion: 所提出的翻译过程通过实验得到验证。

Abstract: The Python Testbed for Federated Learning Algorithms is a simple Python FL
framework that is easy to use by ML&AI developers who do not need to be
professional programmers and is also amenable to LLMs. In the previous
research, generic federated learning algorithms provided by this framework were
manually translated into the CSP processes and algorithms' safety and liveness
properties were automatically verified by the model checker PAT. In this paper,
a simple translation process is introduced wherein the ChatGPT is used to
automate the translation of the mentioned federated learning algorithms in
Python into the corresponding CSP processes. Within the process, the minimality
of the used context is estimated based on the feedback from ChatGPT. The
proposed translation process was experimentally validated by successful
translation (verified by the model checker PAT) of both generic centralized and
decentralized federated learning algorithms.

</details>


### [44] [Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images](https://arxiv.org/abs/2506.07184)
*Liangliang You,Junchi Yao,Shu Yang,Guimin Hu,Lijie Hu,Di Wang*

Main category: cs.AI

TL;DR: 多模态大语言模型存在幻觉问题，本文针对顺序图像的行为幻觉研究较少的问题，提出SHE框架和BEACH指标，实验表明SHE能降低行为幻觉并保持描述准确性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在幻觉问题，现有研究主要关注客观幻觉，而顺序图像的行为幻觉研究较少，本文旨在填补该研究空白。

Method: 揭示行为幻觉主要源于先验驱动偏差和滚雪球效应；提出SHE框架，通过自适应时间窗口的视觉 - 文本对齐检查检测幻觉，通过正交投影到联合嵌入空间减轻幻觉；提出BEACH指标量化行为幻觉严重程度。

Result: 在标准基准测试中，SHE在BEACH指标上减少行为幻觉超10%，并保持描述准确性。

Conclusion: SHE框架能有效减少顺序图像的行为幻觉，同时保证描述准确性。

Abstract: While multimodal large language models excel at various tasks, they still
suffer from hallucinations, which limit their reliability and scalability for
broader domain applications. To address this issue, recent research mainly
focuses on objective hallucination. However, for sequential images, besides
objective hallucination, there is also behavioral hallucination, which is less
studied. This work aims to fill in the gap. We first reveal that behavioral
hallucinations mainly arise from two key factors: prior-driven bias and the
snowball effect. Based on these observations, we introduce SHE (Sequence
Hallucination Eradication), a lightweight, two-stage framework that (1) detects
hallucinations via visual-textual alignment check using our proposed adaptive
temporal window and (2) mitigates them via orthogonal projection onto the joint
embedding space. We also propose a new metric (BEACH) to quantify behavioral
hallucination severity. Empirical results on standard benchmarks demonstrate
that SHE reduces behavioral hallucination by over 10% on BEACH while
maintaining descriptive accuracy.

</details>


### [45] [Exploring Effective Strategies for Building a Customised GPT Agent for Coding Classroom Dialogues](https://arxiv.org/abs/2506.07194)
*Luwei Bai,Dongkeun Han,Sara Hennessy*

Main category: cs.AI

TL;DR: 研究开发定制GPT代理对课堂对话进行编码的有效策略，以MyGPT为例，通过实验确定有限数据下配置有效代理的策略，该代理可作编码助手。


<details>
  <summary>Details</summary>
Motivation: 课堂对话分析因需细致理解对话功能和手动编码耗力而具挑战性，现有研究成果对小数据集或定制编码方案适用性差，需新策略。

Method: 以GPT - 4的MyGPT代理为例，用变量控制法评估其用人类编码本对课堂对话编码的基线性能，研究不同示例输入下的性能变化，采用基于设计的研究方法。

Result: 确定了基于MyGPT独特特征、在有限数据下配置有效代理的实用策略。

Conclusion: 尽管有局限，按这些策略开发的MyGPT代理可作为有用的编码助手生成编码建议。

Abstract: This study investigates effective strategies for developing a customised GPT
agent to code classroom dialogue. While classroom dialogue is widely recognised
as a crucial element of education, its analysis remains challenging due to the
need for a nuanced understanding of dialogic functions and the labour-intensive
nature of manual transcript coding. Recent advancements in large language
models offer promising avenues for automating this process. However, existing
studies predominantly focus on training large-scale models or evaluating
pre-trained models with fixed codebooks, which are often not applicable or
replicable for dialogue researchers working with small datasets or customised
coding schemes. Using GPT-4's MyGPT agent as a case, this study evaluates its
baseline performance in coding classroom dialogue with a human codebook and
examines how performance varies with different example inputs through a
variable control method. Through a design-based research approach, it
identifies a set of practical strategies, based on MyGPT's unique features, for
configuring effective agents with limited data. The findings suggest that,
despite some limitations, a MyGPT agent developed with these strategies can
serve as a useful coding assistant by generating coding suggestions.

</details>


### [46] [Reasoning Multimodal Large Language Model: Data Contamination and Dynamic Evaluation](https://arxiv.org/abs/2506.07202)
*Ming Liu,Wensheng Zhang*

Main category: cs.AI

TL;DR: 提出动态评估框架评估多模态大语言模型泛化能力，揭示数据污染对泛化的影响。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在数据污染问题，可能掩盖真实泛化能力，需新评估方法。

Method: 提出动态评估框架，通过扰动任务而非输入来评估模型，开发自动化评分流程。

Result: 对领先模型分析表明，在模拟测试数据上微调会提升特定任务性能但损害整体泛化。

Conclusion: 动态任务扰动能深入洞察模型泛化能力，区分真实理解和过拟合等问题。

Abstract: Multimodal Large Language Models (MLLMs) show impressive vision-language
benchmark performance, yet growing concerns about data contamination (test set
exposure during training) risk masking true generalization. This concern
extends to reasoning MLLMs, often fine-tuned via reinforcement learning from
potentially contaminated base models. We propose a novel dynamic evaluation
framework to rigorously assess MLLM generalization, moving beyond static
benchmarks. Instead of perturbing inputs, we perturb the task itself. Using the
same visual input, models are evaluated across a family of tasks (e.g., QA,
captioning, question posing, verification) to probe diverse capabilities. This
task perturbation reveals whether model performance is robust or reliant on
superficial task-specific cues. Our approach is analogous to loss landscape
sharpness: models overfit or contaminated for a single task (sharp minima)
falter under task shifts, unlike models with generalizable solutions (flatter
minima). We developed an automated pipeline with a calibrated judge scoring
open-ended generations (captions, questions) using paraphrase and corruption
sampling. Applying this framework to leading image/video MLLMs on benchmarks
including MME, RealWorldQA, and CVRR-ES, we analyze each model's cross-task
"ability vector." We demonstrate that fine-tuning on simulated test data
(extreme contamination) drastically sharpens task-specific performance but
harms overall generalization. Our dynamic task perturbation offers deeper
insights into MLLM generalization, distinguishing genuine understanding from
spurious leakage or overfitting.

</details>


### [47] [BIMgent: Towards Autonomous Building Modeling via Computer-use Agents](https://arxiv.org/abs/2506.07217)
*Zihan Deng,Changyu Du,Stavros Nousias,André Borrmann*

Main category: cs.AI

TL;DR: 本文提出由多模态大语言模型驱动的BIMgent框架，用于建筑信息模型（BIM）创作软件的自动化建筑模型创作，评估显示其能有效减少人工工作量并保留设计意图。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理主要关注通用桌面自动化任务，在高度专业化领域（如AEC行业的3D建筑建模）应用探索有限，当前研究未充分解决相关问题。

Method: 提出由多模态大语言模型驱动的BIMgent框架，实现通过图形用户界面操作进行自主建筑模型创作，包括多模态输入概念设计、规划特定软件工作流程和执行创作GUI操作。

Result: 在实际建筑建模任务评估中，BIMgent设计质量合理，操作成功率32%，基线模型成功率为0%。

Conclusion: BIMgent能有效减少人工工作量并保留设计意图，在实际建筑建模场景有部署潜力。

Abstract: Existing computer-use agents primarily focus on general-purpose desktop
automation tasks, with limited exploration of their application in highly
specialized domains. In particular, the 3D building modeling process in the
Architecture, Engineering, and Construction (AEC) sector involves open-ended
design tasks and complex interaction patterns within Building Information
Modeling (BIM) authoring software, which has yet to be thoroughly addressed by
current studies. In this paper, we propose BIMgent, an agentic framework
powered by multimodal large language models (LLMs), designed to enable
autonomous building model authoring via graphical user interface (GUI)
operations. BIMgent automates the architectural building modeling process,
including multimodal input for conceptual design, planning of software-specific
workflows, and efficient execution of the authoring GUI actions. We evaluate
BIMgent on real-world building modeling tasks, including both text-based
conceptual design generation and reconstruction from existing building design.
The design quality achieved by BIMgent was found to be reasonable. Its
operations achieved a 32% success rate, whereas all baseline models failed to
complete the tasks (0% success rate). Results demonstrate that BIMgent
effectively reduces manual workload while preserving design intent,
highlighting its potential for practical deployment in real-world architectural
modeling scenarios.

</details>


### [48] [LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time Decision-Making in Dynamically Changing Environments](https://arxiv.org/abs/2506.07223)
*Yangqing Zheng,Shunqi Mao,Dingxin Zhang,Weidong Cai*

Main category: cs.AI

TL;DR: 提出时间转换机制TCM和RRARA代理，改进具身智能在高风险场景决策延迟问题，实验表明RRARA表现出色。


<details>
  <summary>Details</summary>
Motivation: 具身智能中，研究动态高风险场景下代理性能，决策延迟问题研究不足。

Method: 提出TCM将决策推理延迟转换为模拟帧，扩展HAZARD提出评估协议，提出RRARA结合轻量级LLM反馈模块和基于规则代理。

Result: RRARA在HAZARD实验中，在对延迟敏感场景大幅超越现有基线。

Conclusion: 提出的方法能有效解决高风险场景下决策延迟问题，提升代理性能。

Abstract: In the realm of embodied intelligence, the evolution of large language models
(LLMs) has markedly enhanced agent decision making. Consequently, researchers
have begun exploring agent performance in dynamically changing high-risk
scenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under
these extreme conditions, the delay in decision making emerges as a crucial yet
insufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that
translates inference delays in decision-making into equivalent simulation
frames, thus aligning cognitive and physical costs under a single FPS-based
metric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action
Ratio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we
present the Rapid-Reflex Async-Reflect Agent (RRARA), which couples a
lightweight LLM-guided feedback module with a rule-based agent to enable
immediate reactive behaviors and asynchronous reflective refinements in situ.
Experiments on HAZARD show that RRARA substantially outperforms existing
baselines in latency-sensitive scenarios.

</details>


### [49] [Subgoal-Guided Policy Heuristic Search with Learned Subgoals](https://arxiv.org/abs/2506.07255)
*Jake Tuero,Michael Buro,Levi H. S. Lelis*

Main category: cs.AI

TL;DR: 提出用于策略树搜索算法学习基于子目标策略的新方法，提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有策略树搜索算法训练需完整解决方案轨迹，训练难题实例时成本高，搜索样本易浪费。

Method: 从搜索尝试解决问题时扩展的树（包括失败尝试的搜索树）中学习子目标和基于子目标的策略。

Result: 经验表明，策略公式和训练方法在在线设置中提高了学习策略和启发式函数的样本效率。

Conclusion: 新的子目标策略学习方法有效提升了样本效率。

Abstract: Policy tree search is a family of tree search algorithms that use a policy to
guide the search. These algorithms provide guarantees on the number of
expansions required to solve a given problem that are based on the quality of
the policy. While these algorithms have shown promising results, the process in
which they are trained requires complete solution trajectories to train the
policy. Search trajectories are obtained during a trial-and-error search
process. When the training problem instances are hard, learning can be
prohibitively costly, especially when starting from a randomly initialized
policy. As a result, search samples are wasted in failed attempts to solve
these hard instances. This paper introduces a novel method for learning
subgoal-based policies for policy tree search algorithms. The subgoals and
policies conditioned on subgoals are learned from the trees that the search
expands while attempting to solve problems, including the search trees of
failed attempts. We empirically show that our policy formulation and training
method improve the sample efficiency of learning a policy and heuristic
function in this online setting.

</details>


### [50] [An Intelligent Fault Self-Healing Mechanism for Cloud AI Systems via Integration of Large Language Models and Deep Reinforcement Learning](https://arxiv.org/abs/2506.07411)
*Ze Yang,Yihong Jin,Juntian Liu,Xinhe Xu*

Main category: cs.AI

TL;DR: 提出集成大语言模型和深度强化学习的智能故障自愈机制，能实现语义理解和策略优化，实验表明在未知故障场景缩短系统恢复时间。


<details>
  <summary>Details</summary>
Motivation: 云AI系统规模和复杂性增加，检测和自适应恢复系统故障成确保服务可靠性和连续性的核心挑战。

Method: 构建两阶段混合架构，包括大语言模型驱动的故障语义解释模块和深度强化学习恢复策略优化器，引入大语言模型进行环境建模和动作空间抽象，引入记忆引导元控制器。

Result: 在云故障注入平台实验显示，与现有DRL和规则方法相比，IFSHM框架在未知故障场景使系统恢复时间缩短37%。

Conclusion: 提出的IFSHM框架能有效提升云AI系统在故障情况下的恢复能力。

Abstract: As the scale and complexity of cloud-based AI systems continue to increase,
the detection and adaptive recovery of system faults have become the core
challenges to ensure service reliability and continuity. In this paper, we
propose an Intelligent Fault Self-Healing Mechanism (IFSHM) that integrates
Large Language Model (LLM) and Deep Reinforcement Learning (DRL), aiming to
realize a fault recovery framework with semantic understanding and policy
optimization capabilities in cloud AI systems. On the basis of the traditional
DRL-based control model, the proposed method constructs a two-stage hybrid
architecture: (1) an LLM-driven fault semantic interpretation module, which can
dynamically extract deep contextual semantics from multi-source logs and system
indicators to accurately identify potential fault modes; (2) DRL recovery
strategy optimizer, based on reinforcement learning, learns the dynamic
matching of fault types and response behaviors in the cloud environment. The
innovation of this method lies in the introduction of LLM for environment
modeling and action space abstraction, which greatly improves the exploration
efficiency and generalization ability of reinforcement learning. At the same
time, a memory-guided meta-controller is introduced, combined with
reinforcement learning playback and LLM prompt fine-tuning strategy, to achieve
continuous adaptation to new failure modes and avoid catastrophic forgetting.
Experimental results on the cloud fault injection platform show that compared
with the existing DRL and rule methods, the IFSHM framework shortens the system
recovery time by 37% with unknown fault scenarios.

</details>


### [51] [Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark Based on the Kangaroo Tests](https://arxiv.org/abs/2506.07418)
*Arnau Igualde Sáez,Lamyae Rhomrasi,Yusef Ahsini,Ricardo Vinuesa,Sergio Hoyas,Jose P. García Sabater,Marius J. Fullana i Alfonso,J. Alberto Conejero*

Main category: cs.AI

TL;DR: 本文分析多模态大语言模型（MLLMs）在数学问题解决中的发展与评估，测试多个模型，发现各模型表现中等，在语言和难度上有差异，Gemini 2.0 Flash等表现较好。


<details>
  <summary>Details</summary>
Motivation: 探究MLLMs在视觉呈现数学问题中的有效性，此前该领域研究不足。

Method: 聚焦图表、多语言文本和符号表示，用多语言袋鼠风格基准测试评估GPT 4o、Pixtral等多个模型。

Result: 整体精度中等，多数模型无图像时精度提升有限，不同语言和难度表现差异大，Gemini 2.0 Flash在基于图像任务中精度最高，Gemini和GPT 4o推理能力较好。

Conclusion: 现有MLLMs在数学问题解决上未达人类水平，各模型有不同优缺点。

Abstract: Multimodal Large Language Models (MLLMs) promise advanced vision language
capabilities, yet their effectiveness in visually presented mathematics remains
underexplored. This paper analyzes the development and evaluation of MLLMs for
mathematical problem solving, focusing on diagrams, multilingual text, and
symbolic notation. We then assess several models, including GPT 4o, Pixtral,
Qwen VL, Llama 3.2 Vision variants, and Gemini 2.0 Flash in a multilingual
Kangaroo style benchmark spanning English, French, Spanish, and Catalan. Our
experiments reveal four key findings. First, overall precision remains moderate
across geometry, visual algebra, logic, patterns, and combinatorics: no single
model excels in every topic. Second, while most models see improved accuracy
with questions that do not have images, the gain is often limited; performance
for some remains nearly unchanged without visual input, indicating
underutilization of diagrammatic information. Third, substantial variation
exists across languages and difficulty levels: models frequently handle easier
items but struggle with advanced geometry and combinatorial reasoning. Notably,
Gemini 2.0 Flash achieves the highest precision on image based tasks, followed
by Qwen VL 2.5 72B and GPT 4o, though none approach human level performance.
Fourth, a complementary analysis aimed at distinguishing whether models reason
or simply recite reveals that Gemini and GPT 4o stand out for their structured
reasoning and consistent accuracy. In contrast, Pixtral and Llama exhibit less
consistent reasoning, often defaulting to heuristics or randomness when unable
to align their outputs with the given answer options.

</details>


### [52] [HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model](https://arxiv.org/abs/2506.07428)
*Yuling Wang,Zihui Chen,Pengfei Jiao,Xiao Wang*

Main category: cs.AI

TL;DR: 本文提出针对HGNNs的基础攻击模型HeTa，利用共享攻击单元实现跨不同HGNNs的可泛化扰动，实验表明其攻击性能和泛化性强。


<details>
  <summary>Details</summary>
Motivation: 现有HGNN攻击方法在新场景需复杂参数再训练，期望设计能跨不同HGNNs实现可泛化扰动、快速适应新异质图的基础攻击模型。

Method: 提出关系感知的异质图基础攻击模型HeTa，引入基础代理模型识别共享关系感知攻击单元重要性，基于关系权重实现序列化逐关系攻击。

Result: 实验显示该方法有强大攻击性能和泛化性。

Conclusion: 所提HeTa模型可实现跨不同HGNNs的可泛化扰动，能轻松针对新异质图微调。

Abstract: Heterogeneous Graph Neural Networks (HGNNs) are vulnerable, highlighting the
need for tailored attacks to assess their robustness and ensure security.
However, existing HGNN attacks often require complex retraining of parameters
to generate specific perturbations for new scenarios. Recently, foundation
models have opened new horizons for the generalization of graph neural networks
by capturing shared semantics across various graph distributions. This leads us
to ask:Can we design a foundation attack model for HGNNs that enables
generalizable perturbations across different HGNNs, and quickly adapts to new
heterogeneous graphs (HGs)? Empirical findings reveal that, despite significant
differences in model design and parameter space, different HGNNs surprisingly
share common vulnerability patterns from a relation-aware perspective.
Therefore, we explore how to design foundation HGNN attack criteria by mining
shared attack units. In this paper, we propose a novel relation-wise
heterogeneous graph foundation attack model, HeTa. We introduce a foundation
surrogate model to align heterogeneity and identify the importance of shared
relation-aware attack units. Building on this, we implement a serialized
relation-by-relation attack based on the identified relational weights. In this
way, the perturbation can be transferred to various target HGNNs and easily
fine-tuned for new HGs. Extensive experiments exhibit powerful attack
performances and generalizability of our method.

</details>


### [53] [LegalReasoner: Step-wised Verification-Correction for Legal Judgment Reasoning](https://arxiv.org/abs/2506.07443)
*Weijie Shi,Han Zhu,Jiaming Ji,Mengze Li,Jipeng Zhang,Ruiyuan Zhang,Jia Zhu,Jiajie Xu,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: 提出LegalReasoner模型用于法律判决预测，通过逐步验证和纠正推理过程提升可靠性，发布LegalHK数据集进行微调，实验表明在LLAMA - 3.1 - 70B上显著提高与法院判决的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有法律判决预测方法在复杂法律推理时存在逻辑错误，需要提高可靠性以支持法院决策和提升司法效率。

Method: 提出LegalReasoner，先识别争议点分解复杂案件，再逐步推理并使用过程验证器从多视角验证每一步逻辑，检测到错误时应用专家设计的归因和解决策略纠正；发布LegalHK数据集进行微调。

Result: 在LLAMA - 3.1 - 70B上，与法院判决的一致性从72.37提升到80.27。

Conclusion: LegalReasoner能有效提高法律判决预测的可靠性，相关数据可公开获取。

Abstract: Legal judgment prediction (LJP) aims to function as a judge by making final
rulings based on case claims and facts, which plays a vital role in the
judicial domain for supporting court decision-making and improving judicial
efficiency. However, existing methods often struggle with logical errors when
conducting complex legal reasoning. We propose LegalReasoner, which enhances
LJP reliability through step-wise verification and correction of the reasoning
process. Specifically, it first identifies dispute points to decompose complex
cases, and then conducts step-wise reasoning while employing a process verifier
to validate each step's logic from correctness, progressiveness, and potential
perspectives. When errors are detected, expert-designed attribution and
resolution strategies are applied for correction. To fine-tune LegalReasoner,
we release the LegalHK dataset, containing 58,130 Hong Kong court cases with
detailed annotations of dispute points, step-by-step reasoning chains, and
process verification labels. Experiments demonstrate that LegalReasoner
significantly improves concordance with court decisions from 72.37 to 80.27 on
LLAMA-3.1-70B. The data is available at
https://huggingface.co/datasets/weijiezz/LegalHK.

</details>


### [54] [Fact in Fragments: Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verification](https://arxiv.org/abs/2506.07446)
*Liwen Zheng,Chaozhuo Li,Zheng Liu,Feiran Huang,Haoran Jia,Zaisheng Ye,Xi Zhang*

Main category: cs.AI

TL;DR: 传统事实验证方法处理复杂声明有局限，提出AFEV框架迭代分解复杂声明，实验显示其在准确性和可解释性上达最优。


<details>
  <summary>Details</summary>
Motivation: 传统方法处理需多跳推理的复杂声明时存在问题，如依赖静态分解、表面语义检索，导致推理错误累积、证据噪声污染和适应性有限，影响验证准确性。

Method: 提出Atomic Fact Extraction and Verification (AFEV)框架，迭代分解复杂声明为原子事实，动态提炼声明理解、减少错误传播，对证据重新排序过滤噪声，利用特定上下文示例引导推理。

Result: 在五个基准数据集上的大量实验表明，AFEV在准确性和可解释性方面达到了最先进的性能。

Conclusion: AFEV框架能有效解决传统方法在复杂声明事实验证中的问题，提升验证性能。

Abstract: Fact verification plays a vital role in combating misinformation by assessing
the veracity of claims through evidence retrieval and reasoning. However,
traditional methods struggle with complex claims requiring multi-hop reasoning
over fragmented evidence, as they often rely on static decomposition strategies
and surface-level semantic retrieval, which fail to capture the nuanced
structure and intent of the claim. This results in accumulated reasoning
errors, noisy evidence contamination, and limited adaptability to diverse
claims, ultimately undermining verification accuracy in complex scenarios. To
address this, we propose Atomic Fact Extraction and Verification (AFEV), a
novel framework that iteratively decomposes complex claims into atomic facts,
enabling fine-grained retrieval and adaptive reasoning. AFEV dynamically
refines claim understanding and reduces error propagation through iterative
fact extraction, reranks evidence to filter noise, and leverages
context-specific demonstrations to guide the reasoning process. Extensive
experiments on five benchmark datasets demonstrate that AFEV achieves
state-of-the-art performance in both accuracy and interpretability.

</details>


### [55] [Efficient Generation of Diverse Cooperative Agents with World Models](https://arxiv.org/abs/2506.07450)
*Yi Loo,Akshunn Trivedi,Malika Meghjani*

Main category: cs.AI

TL;DR: 本文提出XPM - WM框架，利用环境动力学模型的模拟轨迹加速XPM方法训练，更高效可扩展。


<details>
  <summary>Details</summary>
Motivation: 当前XPM方法训练零样本协调（ZSC）代理时生成伙伴代理计算成本高、样本效率低，伙伴代理都从头开始训练。

Method: 引入XPM - WM框架，通过学习的世界模型（WM）为XPM生成模拟轨迹。

Result: 使用模拟轨迹的XPM无需采样多条轨迹，能有效生成具有不同协作惯例的伙伴，在SP种群训练奖励和ZSC代理伙伴训练方面与先前方法性能相当。

Conclusion: 该方法样本效率更高，可扩展到更多伙伴。

Abstract: A major bottleneck in the training process for Zero-Shot Coordination (ZSC)
agents is the generation of partner agents that are diverse in collaborative
conventions. Current Cross-play Minimization (XPM) methods for population
generation can be very computationally expensive and sample inefficient as the
training objective requires sampling multiple types of trajectories. Each
partner agent in the population is also trained from scratch, despite all of
the partners in the population learning policies of the same coordination task.
In this work, we propose that simulated trajectories from the dynamics model of
an environment can drastically speed up the training process for XPM methods.
We introduce XPM-WM, a framework for generating simulated trajectories for XPM
via a learned World Model (WM). We show XPM with simulated trajectories removes
the need to sample multiple trajectories. In addition, we show our proposed
method can effectively generate partners with diverse conventions that match
the performance of previous methods in terms of SP population training reward
as well as training partners for ZSC agents. Our method is thus, significantly
more sample efficient and scalable to a larger number of partners.

</details>


### [56] [Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions](https://arxiv.org/abs/2506.07527)
*Lu Ma,Hao Liang,Meiyi Qiang,Lexiang Tang,Xiaochen Ma,Zhen Hao Wong,Junbo Niu,Chengyu Shen,Runming He,Bin Cui,Wentao Zhang*

Main category: cs.AI

TL;DR: 本文指出当前RL在大语言模型推理中有局限，提出结合RL和SFT的ReLIFT训练方法，该方法在多个基准测试中表现出色且具可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法无法突破基础模型的局限，难以获取新信息，需要新方法增强模型推理能力。

Method: 分析RL和SFT的训练动态，提出ReLIFT方法，在RL训练中遇到难题时收集高质量解决方案进行微调。

Result: ReLIFT在多个基准测试中平均提升超5.2分，优于零RL模型，使用少量详细演示数据就超过RL和SFT。

Conclusion: ReLIFT克服了RL的根本局限，具有巨大潜力。

Abstract: Recent advances in large language model (LLM) reasoning have shown that
sophisticated behaviors such as planning and self-reflection can emerge through
reinforcement learning (RL). However, despite these successes, RL in its
current form remains insufficient to induce capabilities that exceed the
limitations of the base model, as it is primarily optimized based on existing
knowledge of the model rather than facilitating the acquisition of new
information. To address this limitation, we employ supervised fine-tuning (SFT)
to learn what RL cannot, which enables the incorporation of new knowledge and
reasoning patterns by leveraging high-quality demonstration data. We analyze
the training dynamics of RL and SFT for LLM reasoning and find that RL excels
at maintaining and improving performance on questions within the model's
original capabilities, while SFT is more effective at enabling progress on
questions beyond the current scope of the model. Motivated by the complementary
strengths of RL and SFT, we introduce a novel training approach,
\textbf{ReLIFT} (\textbf{Re}inforcement \textbf{L}earning \textbf{I}nterleaved
with Online \textbf{F}ine-\textbf{T}uning). In ReLIFT, the model is primarily
trained using RL, but when it encounters challenging questions, high-quality
solutions are collected for fine-tuning, and the training process alternates
between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT
achieves an average improvement of over +5.2 points across five
competition-level benchmarks and one out-of-distribution benchmark compared to
other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both
RL and SFT while using only 13\% of the detailed demonstration data,
highlighting its scalability. These results provide compelling evidence that
ReLIFT overcomes the fundamental limitations of RL and underscores the
significant potential.

</details>


### [57] [Coordinating Search-Informed Reasoning and Reasoning-Guided Search in Claim Verification](https://arxiv.org/abs/2506.07528)
*Qisheng Hu,Quanyu Long,Wenya Wang*

Main category: cs.AI

TL;DR: 提出分层代理推理与信息搜索（HARIS）方法用于多跳声明验证，在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 多跳声明验证需多步推理与信息搜索，二者相互交织，现有方法需改进。

Method: 提出HARIS，包括高级推理代理和低级搜索代理，用基于结果奖励的强化学习训练。

Result: 在EX - FEVER和HOVER基准测试中取得良好性能。

Conclusion: HARIS极大推进了多跳声明验证。

Abstract: Multi-hop claim verification is inherently challenging, requiring multi-step
reasoning to construct verification chains while iteratively searching for
information to uncover hidden bridging facts. This process is fundamentally
interleaved, as effective reasoning relies on dynamically retrieved evidence,
while effective search demands reasoning to refine queries based on partial
information. To achieve this, we propose Hierarchical Agent Reasoning and
Information Search (HARIS), explicitly modeling the coordinated process of
reasoning-driven searching and search-informed reasoning. HARIS consists of a
high-level reasoning agent that focuses on constructing the main verification
chain, generating factual questions when more information is needed, and a
low-level search agent that iteratively retrieves more information, refining
its search based on intermediate findings. This design allows each agent to
specialize in its respective task, enhancing verification accuracy and
interpretability. HARIS is trained using reinforcement learning with
outcome-based rewards. Experimental results on the EX-FEVER and HOVER
benchmarks demonstrate that HARIS achieves strong performance, greatly
advancing multi-hop claim verification.

</details>


### [58] [Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.07548)
*Weiqiang Jin,Hongyang Du,Guizhong Liu,Dong In Kim*

Main category: cs.AI

TL;DR: 提出用于多智能体强化学习的动态课程学习框架，结合CGRPA方法，实验证明能提升训练稳定性和最终性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习方法在固定对手策略下训练，适应性有限，且课程学习动态性带来不稳定问题。

Method: 提出动态课程学习框架，采用自适应难度调整机制；开发CGRPA方法，构建反事实优势函数评估智能体贡献。

Result: 实验表明该方法提升了训练稳定性和最终性能，与先进方法相比取得有竞争力的结果。

Conclusion: 所提方法有效，代码开源。

Abstract: Multi-agent reinforcement learning (MARL) has achieved strong performance in
cooperative adversarial tasks. However, most existing methods typically train
agents against fixed opponent strategies and rely on such meta-static
difficulty conditions, which limits their adaptability to changing environments
and often leads to suboptimal policies. Inspired by the success of curriculum
learning (CL) in supervised tasks, we propose a dynamic CL framework for MARL
that employs an self-adaptive difficulty adjustment mechanism. This mechanism
continuously modulates opponent strength based on real-time agent training
performance, allowing agents to progressively learn from easier to more
challenging scenarios. However, the dynamic nature of CL introduces instability
due to nonstationary environments and sparse global rewards. To address this
challenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA),
which is tightly coupled with the curriculum by providing intrinsic credit
signals that reflect each agent's impact under evolving task demands. CGRPA
constructs a counterfactual advantage function that isolates individual
contributions within group behavior, facilitating more reliable policy updates
throughout the curriculum. CGRPA evaluates each agent's contribution through
constructing counterfactual action advantage function, providing intrinsic
rewards that enhance credit assignment and stabilize learning under
non-stationary conditions. Extensive experiments demonstrate that our method
improves both training stability and final performance, achieving competitive
results against state-of-the-art methods. The code is available at
https://github.com/NICE-HKU/CL2MARL-SMAC.

</details>


### [59] [GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition](https://arxiv.org/abs/2506.07553)
*Jingchao Wang,Haote Yang,Jiang Wu,Yifan He,Xingjian Wei,Yinfan Wang,Chengjin Liu,Lingli Ge,Lijun Wu,Bin Wang,Dahua Lin,Conghui He*

Main category: cs.AI

TL;DR: 本文提出GTR - Mol - VLM框架用于光学化学结构识别，构建数据集和基准，实验显示其性能优越并将开源。


<details>
  <summary>Details</summary>
Motivation: 现有视觉 - 语言模型在光学化学结构识别任务中处理复杂分子结构和注释不一致时存在困难。

Method: 引入GTR - Mol - VLM框架，包含图遍历机制和以数据为中心原则；构建GTR - CoT - 1.3M数据集和MolRec - Bench基准。

Result: GTR - Mol - VLM性能优于专业模型、化学领域VLM和商业通用VLM，在含官能团缩写图像场景中领先约14个百分点。

Conclusion: 该工作有望推动OCSR技术满足实际需求，促进化学信息学和科学人工智能领域发展。

Abstract: Optical Chemical Structure Recognition (OCSR) is crucial for digitizing
chemical knowledge by converting molecular images into machine-readable
formats. While recent vision-language models (VLMs) have shown potential in
this task, their image-captioning approach often struggles with complex
molecular structures and inconsistent annotations. To overcome these
challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key
innovations: (1) the \textit{Graph Traversal as Visual Chain of Thought}
mechanism that emulates human reasoning by incrementally parsing molecular
graphs through sequential atom-bond predictions, and (2) the data-centric
principle of \textit{Faithfully Recognize What You've Seen}, which addresses
the mismatch between abbreviated structures in images and their expanded
annotations. To support model development, we constructed GTR-CoT-1.3M, a
large-scale instruction-tuning dataset with meticulously corrected annotations,
and introduced MolRec-Bench, the first benchmark designed for a fine-grained
evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments
demonstrate that GTR-Mol-VLM achieves superior results compared to specialist
models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in
scenarios involving molecular images with functional group abbreviations,
GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage
points, both in SMILES-based and graph-based metrics. We hope that this work
will drive OCSR technology to more effectively meet real-world needs, thereby
advancing the fields of cheminformatics and AI for Science. We will release
GTR-CoT at https://github.com/opendatalab/GTR-CoT.

</details>


### [60] [SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems](https://arxiv.org/abs/2506.07564)
*Peiran Li,Xinkai Zou,Zhuohang Wu,Ruifeng Li,Shuo Xing,Hanwen Zheng,Zhikai Hu,Yuping Wang,Haoxi Li,Qin Yuan,Yingmo Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: 本文介绍SAFEFLOW框架用于构建可信的LLM/VLM代理，引入信息控制、事务执行等机制，并通过SAFEFLOWBENCH验证性能，表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有代理框架脆弱，缺乏安全信息流、可靠性和多代理协调的有效机制。

Method: 引入SAFEFLOW框架，实施细粒度信息流控制，引入事务执行、冲突解决等机制，构建SAFEFLOWBENCH进行性能验证。

Result: 基于SAFEFLOW构建的代理在恶劣环境中能保持良好任务性能和安全保障，大幅优于现有技术。

Conclusion: SAFEFLOW和SAFEFLOWBENCH为可靠、安全的代理生态系统奠定基础，推动可靠自主性的发展。

Abstract: Recent advances in large language models (LLMs) and vision-language models
(VLMs) have enabled powerful autonomous agents capable of complex reasoning and
multi-modal tool use. Despite their growing capabilities, today's agent
frameworks remain fragile, lacking principled mechanisms for secure information
flow, reliability, and multi-agent coordination. In this work, we introduce
SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based
agents. SAFEFLOW enforces fine-grained information flow control (IFC),
precisely tracking provenance, integrity, and confidentiality of all the data
exchanged between agents, tools, users, and environments. By constraining LLM
reasoning to respect these security labels, SAFEFLOW prevents untrusted or
adversarial inputs from contaminating high-integrity decisions. To ensure
robustness in concurrent multi-agent settings, SAFEFLOW introduces
transactional execution, conflict resolution, and secure scheduling over shared
state, preserving global consistency across agents. We further introduce
mechanisms, including write-ahead logging, rollback, and secure caches, that
further enhance resilience against runtime errors and policy violations. To
validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark
suite designed to evaluate agent reliability under adversarial, noisy, and
concurrent operational conditions. Extensive experiments demonstrate that
agents built with SAFEFLOW maintain impressive task performance and security
guarantees even in hostile environments, substantially outperforming
state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for
principled, robust, and secure agent ecosystems, advancing the frontier of
reliable autonomy.

</details>


### [61] [Automating Exploratory Multiomics Research via Language Models](https://arxiv.org/abs/2506.07591)
*Shang Qu,Ning Ding,Linhai Xie,Yifei Li,Zaoqu Liu,Kaiyan Zhang,Yibai Xiong,Yuxin Zuo,Zhangren Chen,Ermo Hua,Xingtai Lv,Youbang Sun,Yang Li,Dong Li,Fuchu He,Bowen Zhou*

Main category: cs.AI

TL;DR: 介绍自动系统PROTEUS，能从原始数据生成假设，应用于临床蛋白质基因组学，经评估可平衡可靠性与新颖性，还为专业领域自动假设生成提供路径。


<details>
  <summary>Details</summary>
Motivation: 临床蛋白质基因组学领域有效下游数据分析和假设提出对新发现至关重要，需要能处理复杂数据的系统。

Method: 使用独立模块模拟科研过程不同阶段，用统一图结构管理复杂研究过程。

Result: 应用于10个临床多组学数据集，得到360个假设，通过外部数据验证和自动开放式评分评估。

Conclusion: 系统可处理高通量和异构多组学数据，平衡可靠性与新颖性，加速多组学分析，为专业领域实现数据驱动的开放式假设生成提供途径。

Abstract: This paper introduces PROTEUS, a fully automated system that produces
data-driven hypotheses from raw data files. We apply PROTEUS to clinical
proteogenomics, a field where effective downstream data analysis and hypothesis
proposal is crucial for producing novel discoveries. PROTEUS uses separate
modules to simulate different stages of the scientific process, from open-ended
data exploration to specific statistical analysis and hypothesis proposal. It
formulates research directions, tools, and results in terms of relationships
between biological entities, using unified graph structures to manage complex
research processes. We applied PROTEUS to 10 clinical multiomics datasets from
published research, arriving at 360 total hypotheses. Results were evaluated
through external data validation and automatic open-ended scoring. Through
exploratory and iterative research, the system can navigate high-throughput and
heterogeneous multiomics data to arrive at hypotheses that balance reliability
and novelty. In addition to accelerating multiomic analysis, PROTEUS represents
a path towards tailoring general autonomous systems to specialized scientific
domains to achieve open-ended hypothesis generation from data.

</details>


### [62] [SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling](https://arxiv.org/abs/2506.07636)
*Haoran Wang,Zhenyu Hou,Yao Wei,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: 提出基于开源大语言模型的软件工程代理SWE - Dev，通过合成测试用例和构建训练数据，在SWE - bench - Verified基准测试中表现出色，代码等公开。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型赋能的软件工程工具包虽有进展，但构建有效软件工程代理因缺乏高质量训练数据和有效测试用例而面临挑战。

Method: 开发合成测试用例的健壮管道；扩大代理轨迹构建SWE - Dev的训练数据。

Result: SWE - Dev模型在SWE - bench - Verified基准测试中表现最佳，7B和32B参数模型成功率分别达23.4%和36.6%，优于现有开源模型。

Conclusion: SWE - Dev在软件工程代理任务中展现出良好性能，相关代码、模型和数据集公开可促进研究。

Abstract: Large language models (LLMs) have advanced rapidly from conversational
problem solving to addressing real-world tasks involving tool use, such as
software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex
and Cursor, have offered end-to-end automation of the software development
process. However, building effective SWE agents remains challenging due to the
lack of high-quality training data and effective test cases. To address this
issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we
develop a robust pipeline to synthesize test cases for patch evaluation.
Second, we scale up agent trajectories to construct the training data for
building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the
SWE-Dev models can achieve top performance among all open SWE agents.
Specifically, the success rates of the SWE-Dev 7B and 32B parameter models
reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source
models. All code, models, and datasets are publicly available at
https://github.com/THUDM/SWE-Dev.

</details>


### [63] [MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents](https://arxiv.org/abs/2506.07672)
*Yunhe Yan,Shihe Wang,Jiajun Du,Yexuan Yang,Yuxuan Shan,Qichen Qiu,Xianqing Jia,Xinge Wang,Xin Yuan,Xu Han,Mao Qin,Yinxiao Chen,Chen Peng,Shangguang Wang,Mengwei Xu*

Main category: cs.AI

TL;DR: 提出首个用于API、GUI和API - GUI混合代理的自动CUA测试平台MCPWorld，有201个任务，初步实验准确率75.12%，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 现有CUA基准主要针对GUI代理，评估方法易受UI变化影响且忽略应用API的功能交互，需新的测试平台。

Method: 使用“白盒应用”，可通过动态代码插桩等技术直接监控应用行为来验证任务完成情况，且平台完全容器化并支持GPU加速。

Result: 初步实验使用代表性LLM - 驱动的CUA框架，任务完成准确率达75.12%。

Conclusion: MCPWorld有助于促进和规范下一代可利用丰富外部工具的计算机使用代理的基准测试。

Abstract: (M)LLM-powered computer use agents (CUA) are emerging as a transformative
technique to automate human-computer interaction. However, existing CUA
benchmarks predominantly target GUI agents, whose evaluation methods are
susceptible to UI changes and ignore function interactions exposed by
application APIs, e.g., Model Context Protocol (MCP). To this end, we propose
MCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI hybrid
agents. A key principle of MCPWorld is the use of "white-box apps", i.e., those
with source code availability and can be revised/re-compiled as needed (e.g.,
adding MCP support), with two notable advantages:
  (1) It greatly broadens the design space of CUA, such as what and how the app
features to be exposed/extracted as CUA-callable APIs.
  (2) It allows MCPWorld to programmatically verify task completion by directly
monitoring application behavior through techniques like dynamic code
instrumentation, offering robust, accurate CUA evaluation decoupled from
specific agent implementations or UI states.
  Currently, MCPWorld includes 201 well curated and annotated user tasks,
covering diversified use cases and difficulty levels. MCPWorld is also fully
containerized with GPU acceleration support for flexible adoption on different
OS/hardware environments. Our preliminary experiments, using a representative
LLM-powered CUA framework, achieve 75.12% task completion accuracy,
simultaneously providing initial evidence on the practical effectiveness of
agent automation leveraging MCP. Overall, we anticipate MCPWorld to facilitate
and standardize the benchmarking of next-generation computer use agents that
can leverage rich external tools. Our code and dataset are publicly available
at https://github.com/SAAgent/MCPWorld.

</details>


### [64] [NeurIPS 2025 E2LM Competition : Early Training Evaluation of Language Models](https://arxiv.org/abs/2506.07731)
*Mouadh Yagoubi,Yasser Dahou,Billel Mokeddem,Younes Belkada,Phuc H. Le-Khac,Basma El Amel Boussaha,Reda Alami,Jingwei Zuo,Damiano Marsili,Mugariya Farooq,Mounia Lalmas,Georgia Gkioxari,Patrick Gallinari,Philip Torr,Hakim Hacid*

Main category: cs.AI

TL;DR: 现有基准在评估小模型早期训练效果不佳，举办竞赛设计适配评估任务，提供资源，设评估标准，吸引多学科参与，让大模型研究更系统。


<details>
  <summary>Details</summary>
Motivation: 现有基准在小模型早期训练阶段无法提供有意义或有区分度的信号，需探索差异产生原因，设计适配评估任务。

Method: 举办竞赛，邀请参与者开发新评估方法或调整现有基准，提供三个预训练小模型及中间检查点，在免费云GPU平台开展工作。

Result: 未提及具体结果。

Conclusion: 通过促进早期训练适配评估策略设计，让大模型基础研究从早期就更系统且有基准参考。

Abstract: Existing benchmarks have proven effective for assessing the performance of
fully trained large language models. However, we find striking differences in
the early training stages of small models, where benchmarks often fail to
provide meaningful or discriminative signals. To explore how these differences
arise, this competition tackles the challenge of designing scientific knowledge
evaluation tasks specifically tailored for measuring early training progress of
language models. Participants are invited to develop novel evaluation
methodologies or adapt existing benchmarks to better capture performance
differences among language models. To support this effort, we provide three
pre-trained small models (0.5B, 1B, and 3B parameters), along with intermediate
checkpoints sampled during training up to 200B tokens. All experiments and
development work can be run on widely available free cloud-based GPU platforms,
making participation accessible to researchers with limited computational
resources. Submissions will be evaluated based on three criteria: the quality
of the performance signal they produce, the consistency of model rankings at 1
trillion tokens of training, and their relevance to the scientific knowledge
domain. By promoting the design of tailored evaluation strategies for early
training, this competition aims to attract a broad range of participants from
various disciplines, including those who may not be machine learning experts or
have access to dedicated GPU resources. Ultimately, this initiative seeks to
make foundational LLM research more systematic and benchmark-informed from the
earliest phases of model development.

</details>


### [65] [RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards](https://arxiv.org/abs/2506.07736)
*Jingnan Zheng,Xiangtian Ji,Yijun Lu,Chenhang Cui,Weixiang Zhao,Gelei Deng,Zhenkai Liang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 现有大语言模型防护存在局限，提出自适应推理防护方法RSafe，分两阶段训练，可在推理时按需提供防护。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽经安全对齐仍有漏洞，现有防护方法依赖大量人工数据集且难应对分布外威胁。

Method: 提出RSafe，包括引导推理（通过策略引导逐步推理分析输入内容安全风险）和强化对齐（基于规则的强化学习优化推理路径）两阶段训练范式。

Result: 两阶段训练范式使RSafe能内化安全原则，在未见或对抗性违规场景中泛化安全防护能力。

Conclusion: RSafe在推理时可接受用户指定安全策略，提供符合特定安全要求的增强防护。

Abstract: Large Language Models (LLMs) continue to exhibit vulnerabilities despite
deliberate safety alignment efforts, posing significant risks to users and
society. To safeguard against the risk of policy-violating content,
system-level moderation via external guard models-designed to monitor LLM
inputs and outputs and block potentially harmful content-has emerged as a
prevalent mitigation strategy. Existing approaches of training guard models
rely heavily on extensive human curated datasets and struggle with
out-of-distribution threats, such as emerging harmful categories or jailbreak
attacks. To address these limitations, we propose RSafe, an adaptive
reasoning-based safeguard that conducts guided safety reasoning to provide
robust protection within the scope of specified safety policies. RSafe operates
in two stages: 1) guided reasoning, where it analyzes safety risks of input
content through policy-guided step-by-step reasoning, and 2) reinforced
alignment, where rule-based RL optimizes its reasoning paths to align with
accurate safety prediction. This two-stage training paradigm enables RSafe to
internalize safety principles to generalize safety protection capability over
unseen or adversarial safety violation scenarios. During inference, RSafe
accepts user-specified safety policies to provide enhanced safeguards tailored
to specific safety requirements.

</details>


### [66] [Agent Semantics, Semantic Spacetime, and Graphical Reasoning](https://arxiv.org/abs/2506.07756)
*Mark Burgess*

Main category: cs.AI

TL;DR: 介绍语义时空图模型的形式方面，定义有限表示，分析吸收状态等问题。


<details>
  <summary>Details</summary>
Motivation: 探讨语义时空图模型用于有向知识表示和过程建模的形式方面。

Method: 定义有限γ(3,4)表示以形成封闭操作集，基于语义时空假设分析图路径。

Result: 发现图过程中吸收状态会导致信息泄漏，与除零问题相关。

Conclusion: 语义时空模型及其起源有助于阐明吸收状态与边界信息及意向性的关联。

Abstract: Some formal aspects of the Semantic Spacetime graph model are presented, with
reference to its use for directed knowledge representations and process
modelling. A finite $\gamma(3,4)$ representation is defined to form a closed
set of operations that can scale to any degree of semantic complexity. The
Semantic Spacetime postulates bring predictability with minimal constraints to
pathways in graphs. The ubiquitous appearance of absorbing states in any
partial graph means that a graph process leaks information. The issue is
closely associated with the issue of division by zero, which signals a loss of
closure and the need for manual injection of remedial information. The Semantic
Spacetime model (and its Promise Theory) origins help to clarify how such
absorbing states are associated with boundary information where intentionality
can enter.

</details>


### [67] [A Proposal to Extend the Common Model of Cognition with Metacognition](https://arxiv.org/abs/2506.07807)
*John Laird,Christian Lebiere,Paul Rosenbloom,Andrea Stocco,Robert Wray*

Main category: cs.AI

TL;DR: 提出在通用认知模型（CMC）中集成元认知的统一方法，并举例说明。


<details>
  <summary>Details</summary>
Motivation: 将元认知集成到通用认知模型（CMC）中，以实现类人思维认知架构的构建。

Method: 提出元认知涉及对工作记忆中主体认知能力和过程的显式表征进行推理，利用CMC现有认知能力，对工作记忆结构和信息进行最小扩展。

Result: 给出了所提方法下元认知的示例。

Conclusion: 提出了在CMC中集成元认知的统一方法。

Abstract: The Common Model of Cognition (CMC) provides an abstract characterization of
the structure and processing required by a cognitive architecture for
human-like minds. We propose a unified approach to integrating metacognition
within the CMC. We propose that metacognition involves reasoning over explicit
representations of an agent's cognitive capabilities and processes in working
memory. Our proposal exploits the existing cognitive capabilities of the CMC,
making minimal extensions in the structure and information available within
working memory. We provide examples of metacognition within our proposal.

</details>


### [68] [Guideline Forest: Experience-Induced Multi-Guideline Reasoning with Stepwise Aggregation](https://arxiv.org/abs/2506.07820)
*Jiaxiang CHen,Zhuo Wang,Mingxi Zou,Qifan Wang,Zenglin Xu*

Main category: cs.AI

TL;DR: 提出Guideline Forest框架提升大语言模型推理能力，在多个基准测试中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以模仿人类灵活、自适应且基于先验经验的推理能力，现有方法难以有效利用可复用策略。

Method: 从已验证示例中归纳出结构化推理策略（准则），通过逐步聚合执行，生成多样变体并并行执行、自我修正和聚合。

Result: 在GSM8K、MATH - 500、MBPP和HumanEval四个基准测试中，Guideline Forest始终优于CoT、ReAct等强基线模型。

Conclusion: 消融实验凸显多路径推理和逐步聚合的有效性，表明Guideline Forest具有适应性和泛化潜力。

Abstract: Human reasoning is flexible, adaptive, and grounded in prior
experience-qualities that large language models (LLMs) still struggle to
emulate. Existing methods either explore diverse reasoning paths at inference
time or search for optimal workflows through expensive operations, but both
fall short in leveraging multiple reusable strategies in a structured,
efficient manner. We propose Guideline Forest, a framework that enhances LLMs
reasoning by inducing structured reasoning strategies-called guidelines-from
verified examples and executing them via step-wise aggregation. Unlike
test-time search or single-path distillation, our method draws on verified
reasoning experiences by inducing reusable guidelines and expanding each into
diverse variants. Much like human reasoning, these variants reflect alternative
thought patterns, are executed in parallel, refined via self-correction, and
aggregated step by step-enabling the model to adaptively resolve uncertainty
and synthesize robust solutions.We evaluate Guideline Forest on four
benchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and
programmatic reasoning. Guideline Forest consistently outperforms strong
baselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further
highlight the effectiveness of multi-path reasoning and stepwise aggregation,
underscoring the Guideline Forest's adaptability and generalization potential.

</details>


### [69] [Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs](https://arxiv.org/abs/2506.07824)
*Yao Yan*

Main category: cs.AI

TL;DR: 结合线性探测和logit-lens检查剖析LLaMA - 3 - 8B - Instruct多位数加法计算过程，提出四阶段轨迹，表明是分层计算而非死记硬背。


<details>
  <summary>Details</summary>
Motivation: 剖析LLaMA - 3 - 8B - Instruct内部算术过程。

Method: 结合线性探测与logit - lens检查，分析前向传播四阶段轨迹。

Result: 发现前向传播有四阶段轨迹，各阶段有不同特征，可清晰解码结果数字。

Conclusion: 该模型加法计算是分层过程，倾向内部计算而非死记硬背，并开源代码和数据便于复现。

Abstract: Multi-digit addition is a clear probe of the computational power of large
language models. To dissect the internal arithmetic processes in
LLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection.
Inspired by the step-by-step manner in which humans perform addition, we
propose and analyze a coherent four-stage trajectory in the forward
pass:Formula-structure representations become linearly decodable first, while
the answer token is still far down the candidate list.Core computational
features then emerge prominently.At deeper activation layers, numerical
abstractions of the result become clearer, enabling near-perfect detection and
decoding of the individual digits in the sum.Near the output, the model
organizes and generates the final content, with the correct token reliably
occupying the top rank.This trajectory suggests a hierarchical process that
favors internal computation over rote memorization. We release our code and
data to facilitate reproducibility.

</details>


### [70] [HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains](https://arxiv.org/abs/2506.07837)
*Shijie Wang,Yilun Zhang,Zeyu Lai,Dexing Kong*

Main category: cs.AI

TL;DR: 本文提出数据生成管道创建特定领域四元组数据，建立医学超声领域数据集ReMUD，微调模型在该领域表现佳，还将公开数据等解决特定领域MLLMs数据短缺问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在特定领域因缺乏特定领域数据表现不佳，医学超声领域资料格式无法直接用于训练。

Method: 提出图像 - 文本推理监督微调数据生成管道，从特定领域材料创建特定领域四元组。

Result: 建立含超45000条数据的医学超声领域数据集ReMUD，基于Qwen2.5 - VL - 7B - Instruct微调的ReMUD - 7B模型在医学超声领域表现优于通用领域MLLMs。

Conclusion: 将公开ReMUD数据集、代码库和模型参数，解决特定领域MLLMs数据短缺问题。

Abstract: Multimodal large language models (MLLMs) have shown great potential in
general domains but perform poorly in some specific domains due to a lack of
domain-specific data, such as image-text data or vedio-text data. In some
specific domains, there is abundant graphic and textual data scattered around,
but lacks standardized arrangement. In the field of medical ultrasound, there
are ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic
diagnostic reports, and so on. However, these ultrasonic materials are often
saved in the forms of PDF, images, etc., and cannot be directly used for the
training of MLLMs. This paper proposes a novel image-text reasoning supervised
fine-tuning data generation pipeline to create specific domain quadruplets
(image, question, thinking trace, and answer) from domain-specific materials. A
medical ultrasound domain dataset ReMUD is established, containing over 45,000
reasoning and non-reasoning supervised fine-tuning Question Answering (QA) and
Visual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on
Qwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound
field. To facilitate research, the ReMUD dataset, data generation codebase, and
ReMUD-7B parameters will be released at https://github.com/ShiDaizi/ReMUD,
addressing the data shortage issue in specific domain MLLMs.

</details>


### [71] [Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark](https://arxiv.org/abs/2506.07896)
*Shoko Oka*

Main category: cs.AI

TL;DR: 研究调查现代大语言模型是否有能力解决框架问题和符号接地问题，通过设计基准任务测试13个模型，发现部分闭源模型表现出色。


<details>
  <summary>Details</summary>
Motivation: 近期大语言模型发展引发人工智能哲学辩论，研究其是否具备解决框架问题和符号接地问题的认知能力。

Method: 设计反映两个问题哲学核心的基准任务，在零样本条件下对13个大语言模型进行测试，分五次评估模型输出质量，并从多标准打分。

Result: 开源模型因模型大小、量化和指令调整等因素表现有差异，部分闭源模型持续获得高分。

Conclusion: 部分现代大语言模型可能已具备对长期理论挑战产生有意义且稳定回应的能力。

Abstract: Recent advancements in large language models (LLMs) have revitalized
philosophical debates surrounding artificial intelligence. Two of the most
fundamental challenges - namely, the Frame Problem and the Symbol Grounding
Problem - have historically been viewed as unsolvable within traditional
symbolic AI systems. This study investigates whether modern LLMs possess the
cognitive capacities required to address these problems. To do so, I designed
two benchmark tasks reflecting the philosophical core of each problem,
administered them under zero-shot conditions to 13 prominent LLMs (both closed
and open-source), and assessed the quality of the models' outputs across five
trials each. Responses were scored along multiple criteria, including
contextual reasoning, semantic coherence, and information filtering. The
results demonstrate that while open-source models showed variability in
performance due to differences in model size, quantization, and instruction
tuning, several closed models consistently achieved high scores. These findings
suggest that select modern LLMs may be acquiring capacities sufficient to
produce meaningful and stable responses to these long-standing theoretical
challenges.

</details>


### [72] [LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement](https://arxiv.org/abs/2506.07915)
*Dimitris Panagopoulos,Adolfo Perrusquia,Weisi Guo*

Main category: cs.AI

TL;DR: 本文提出LUCIFER框架，整合分层决策架构、强化学习和大语言模型，解决动态环境中自主决策问题，提升探索效率和决策质量。


<details>
  <summary>Details</summary>
Motivation: 动态环境中既有环境知识过时，导致自主决策效果受限，需利用人类领域利益相关者的见解，但将其转化为可操作情报存在挑战。

Method: 提出LUCIFER框架，将分层决策架构、强化学习和大语言模型整合为统一系统，大语言模型承担上下文提取器和零样本探索促进者两个协同角色。

Result: 对各种大语言模型进行基准测试，表明LUCIFER提高了探索效率和决策质量，优于扁平的目标条件策略。

Conclusion: 上下文驱动的决策有潜力，自主系统可利用人类上下文知识实现操作成功。

Abstract: In dynamic environments, the rapid obsolescence of pre-existing environmental
knowledge creates a gap between an agent's internal model and the evolving
reality of its operational context. This disparity between prior and updated
environmental valuations fundamentally limits the effectiveness of autonomous
decision-making. To bridge this gap, the contextual bias of human domain
stakeholders, who naturally accumulate insights through direct, real-time
observation, becomes indispensable. However, translating their nuanced, and
context-rich input into actionable intelligence for autonomous systems remains
an open challenge. To address this, we propose LUCIFER (Language Understanding
and Context-Infused Framework for Exploration and Behavior Refinement), a
domain-agnostic framework that integrates a hierarchical decision-making
architecture with reinforcement learning (RL) and large language models (LLMs)
into a unified system. This architecture mirrors how humans decompose complex
tasks, enabling a high-level planner to coordinate specialised sub-agents, each
focused on distinct objectives and temporally interdependent actions. Unlike
traditional applications where LLMs are limited to single role, LUCIFER
integrates them in two synergistic roles: as context extractors, structuring
verbal stakeholder input into domain-aware representations that influence
decision-making through an attention space mechanism aligning LLM-derived
insights with the agent's learning process, and as zero-shot exploration
facilitators guiding the agent's action selection process during exploration.
We benchmark various LLMs in both roles and demonstrate that LUCIFER improves
exploration efficiency and decision quality, outperforming flat,
goal-conditioned policies. Our findings show the potential of context-driven
decision-making, where autonomous systems leverage human contextual knowledge
for operational success.

</details>


### [73] [Solving Inequality Proofs with Large Language Models](https://arxiv.org/abs/2506.07927)
*Jiayi Sheng,Luna Lyu,Jikai Jin,Tony Xia,Alex Gu,James Zou,Pan Lu*

Main category: cs.AI

TL;DR: 文章指出不等式证明对大语言模型是挑战，因现有数据集不足，提出新任务表述，发布IneqMath数据集和评估框架，评估显示当前大语言模型在不等式证明上存在问题，指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 不等式证明考验高级推理能力，是大语言模型的挑战领域，但现有数据集稀缺、合成或形式僵化，阻碍了该领域的进展。

Method: 提出非正式但可验证的任务表述，将不等式证明转化为两个可自动检查的子任务；发布专家策划的Olympiad级不等式数据集IneqMath；开发基于大语言模型的评估框架。

Result: 对29个领先大语言模型的评估显示，即使顶级模型在逐步审查下整体准确率低于10%，较仅考虑最终答案时下降最多达65.5%，扩大模型规模和增加测试时计算对提高证明正确性效果有限。

Conclusion: 当前大语言模型在不等式证明的演绎链脆弱，在寻找答案和构建严格证明间存在关键差距，定理引导推理和自我完善是有前景的研究方向。

Abstract: Inequality proving, crucial across diverse scientific and mathematical
fields, tests advanced reasoning skills such as discovering tight bounds and
strategic theorem application. This makes it a distinct, demanding frontier for
large language models (LLMs), offering insights beyond general mathematical
problem-solving. Progress in this area is hampered by existing datasets that
are often scarce, synthetic, or rigidly formal. We address this by proposing an
informal yet verifiable task formulation, recasting inequality proving into two
automatically checkable subtasks: bound estimation and relation prediction.
Building on this, we release IneqMath, an expert-curated dataset of
Olympiad-level inequalities, including a test set and training corpus enriched
with step-wise solutions and theorem annotations. We also develop a novel
LLM-as-judge evaluation framework, combining a final-answer judge with four
step-wise judges designed to detect common reasoning flaws. A systematic
evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even
top models like o1 achieve less than 10% overall accuracy under step-wise
scrutiny; this is a drop of up to 65.5% from their accuracy considering only
final answer equivalence. This discrepancy exposes fragile deductive chains and
a critical gap for current LLMs between merely finding an answer and
constructing a rigorous proof. Scaling model size and increasing test-time
computation yield limited gains in overall proof correctness. Instead, our
findings highlight promising research directions such as theorem-guided
reasoning and self-refinement. Code and data are available at
https://ineqmath.github.io/.

</details>


### [74] [Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation](https://arxiv.org/abs/2506.07940)
*Christopher Subia-Waud*

Main category: cs.AI

TL;DR: 介绍去中心化AutoML平台Gradients，通过竞争市场优化超参数，实验显示其优于多个平台。


<details>
  <summary>Details</summary>
Motivation: 现有AutoML平台依赖单一优化策略，只能探索部分超参数配置，需要更好的解决方案。

Method: 引入Gradients平台，将超参数优化转变为竞争市场，利用经济激励机制。

Result: 在180个实验中，Gradients对HuggingFace AutoTrain胜率82.8%，对TogetherAI、Databricks和Google Cloud胜率100%，有显著性能提升。

Conclusion: 基于竞争和经济驱动的方法能系统发现中心化AutoML错过的更优配置。

Abstract: Foundation model fine-tuning faces a fundamental challenge: existing AutoML
platforms rely on single optimisation strategies that explore only a fraction
of viable hyperparameter configurations. In this white paper, We introduce
Gradients, a decentralised AutoML platform that transforms hyperparameter
optimisation into a competitive marketplace where independent miners compete to
discover optimal configurations. Economic incentives align individual
exploration with collective optimisation goals, driving systematic
investigation of hyperparameter regions that centralised methods miss. We
evaluate our approach across 180 controlled experiments spanning diverse model
architectures (70M to 70B parameters) and task types. Gradients achieves an
82.8\% win rate against HuggingFace AutoTrain and 100\% against TogetherAI,
Databricks, and Google Cloud, with mean improvements of 11.8\% and 42.1\%
respectively. Complex reasoning and retrieval tasks show particularly strong
gains of 30-40\%, whilst diffusion models achieve 23.4\% improvements for
person-specific generation. These results demonstrate that competitive,
economically-driven approaches can systematically discover superior
configurations that centralised AutoML consistently miss.

</details>


### [75] [Reinforcing Multimodal Understanding and Generation with Dual Self-rewards](https://arxiv.org/abs/2506.07963)
*Jixiang Hong,Yiran Zhang,Guanzhong Wang,Yi Liu,Ji-Rong Wen,Rui Yan*

Main category: cs.AI

TL;DR: 本文提出自监督双奖励机制增强大模态模型理解与生成能力，实验表明无需外部监督可有效提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前大模态模型难以实现准确的图像 - 文本对齐，现有解决方案需外部监督且只处理单向任务。

Method: 引入自监督双奖励机制，在一个任务域为给定输入采样多个输出，反转输入 - 输出对计算模型的对偶似然作为自奖励进行优化。

Result: 在视觉理解和生成基准测试上的实验结果表明，该方法无需任何外部监督即可有效提升模型性能，在文本到图像任务中取得显著改进。

Conclusion: 所提出的自监督双奖励机制能增强大模态模型的理解和生成能力。

Abstract: Building upon large language models (LLMs), recent large multimodal models
(LMMs) unify cross-model understanding and generation into a single framework.
However, LMMs still struggle to achieve accurate image-text alignment, prone to
generating text responses contradicting the visual input or failing to follow
the text-to-image prompts. Current solutions require external supervision
(e.g., human feedback or reward models) and only address unidirectional
tasks-either understanding or generation. In this work, based on the
observation that understanding and generation are inverse dual tasks, we
introduce a self-supervised dual reward mechanism to reinforce the
understanding and generation capabilities of LMMs. Specifically, we sample
multiple outputs for a given input in one task domain, then reverse the
input-output pairs to compute the dual likelihood of the model as self-rewards
for optimization. Extensive experimental results on visual understanding and
generation benchmarks demonstrate that our method can effectively enhance the
performance of the model without any external supervision, especially achieving
remarkable improvements in text-to-image tasks.

</details>


### [76] [$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment](https://arxiv.org/abs/2506.07982)
*Victor Barres,Honghua Dong,Soham Ray,Xujie Si,Karthik Narasimhan*

Main category: cs.AI

TL;DR: 现有对话式AI代理基准模拟单控制环境，本文提出τ² - bench解决此问题，有四项关键贡献并做实验。


<details>
  <summary>Details</summary>
Motivation: 现有对话式AI代理基准与现实场景不同，用户需在现实中主动参与修改世界状态，为填补差距开展研究。

Method: 引入τ² - bench，包括建立电信双控制领域模型、设计组合任务生成器、开发可靠用户模拟器、进行细粒度性能分析。

Result: 实验表明代理从无用户场景到双控制场景性能显著下降。

Conclusion: τ² - bench为需有效推理和引导用户行动的代理提供了可控测试平台。

Abstract: Existing benchmarks for conversational AI agents simulate single-control
environments, where only the AI agent can use tools to interact with the world,
while the user remains a passive information provider. This differs from
real-world scenarios like technical support, where users need to actively
participate in modifying the state of the (shared) world. In order to address
this gap, we introduce $\tau^2$-bench, with four key contributions:
  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both
agent and user make use of tools to act in a shared, dynamic environment that
tests both agent coordination and communication,
  2) A compositional task generator that programmatically creates diverse,
verifiable tasks from atomic components, ensuring domain coverage and
controlled complexity,
  3) A reliable user simulator tightly coupled with the environment, whose
behavior is constrained by tools and observable states, improving simulation
fidelity,
  4) Fine-grained analysis of agent performance through multiple ablations
including separating errors arising from reasoning vs
communication/coordination.
  In particular, our experiments show significant performance drops when agents
shift from no-user to dual-control, highlighting the challenges of guiding
users. Overall, $\tau^2$-bench provides a controlled testbed for agents that
must both reason effectively and guide user actions.

</details>


### [77] [GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior](https://arxiv.org/abs/2506.08012)
*Penghao Wu,Shengnan Ma,Bo Wang,Jiaheng Yu,Lewei Lu,Ziwei Liu*

Main category: cs.AI

TL;DR: 提出GUI - Reflection框架，让多模态大语言模型在GUI自动化中有自反思和纠错能力，含数据构建、任务套件、在线训练环境和迭代算法，相关资源将公开。


<details>
  <summary>Details</summary>
Motivation: 现有GUI模型大多依赖近无错离线轨迹学习，缺乏反思和纠错能力。

Method: 提出GUI - Reflection框架，包括GUI特定预训练、离线监督微调、在线反思微调；构建数据管道自动生成反思和纠错数据；提出GUI - Reflection任务套件；搭建移动设备在线训练环境；采用迭代在线反思调优算法。

Result: 框架使GUI代理具备自反思和纠错能力。

Conclusion: 该框架为更强大、适应性更强和更智能的GUI自动化铺平道路。

Abstract: Multimodal Large Language Models (MLLMs) have shown great potential in
revolutionizing Graphical User Interface (GUI) automation. However, existing
GUI models mostly rely on learning from nearly error-free offline trajectories,
thus lacking reflection and error recovery capabilities. To bridge this gap, we
propose GUI-Reflection, a novel framework that explicitly integrates
self-reflection and error correction capabilities into end-to-end multimodal
GUI models throughout dedicated training stages: GUI-specific pre-training,
offline supervised fine-tuning (SFT), and online reflection tuning.
GUI-reflection enables self-reflection behavior emergence with fully automated
data generation and learning processes without requiring any human annotation.
Specifically, 1) we first propose scalable data pipelines to automatically
construct reflection and error correction data from existing successful
trajectories. While existing GUI models mainly focus on grounding and UI
understanding ability, we propose the GUI-Reflection Task Suite to learn and
evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a
diverse and efficient environment for online training and data collection of
GUI models on mobile devices. 3) We also present an iterative online reflection
tuning algorithm leveraging the proposed environment, enabling the model to
continuously enhance its reflection and error correction abilities. Our
framework equips GUI agents with self-reflection and correction capabilities,
paving the way for more robust, adaptable, and intelligent GUI automation, with
all data, models, environments, and tools to be released publicly.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [78] [Deep Learning Enhanced Multi-Day Turnover Quantitative Trading Algorithm for Chinese A-Share Market](https://arxiv.org/abs/2506.06356)
*Yimin Du*

Main category: cs.CE

TL;DR: 本文提出适用于中国A股市场的多日换手率量化交易算法，结合深度学习与截面预测，经回测表现出色，在不同市场表现稳健，适合机构部署。


<details>
  <summary>Details</summary>
Motivation: 为中国A股市场开发结合深度学习与截面预测的量化交易算法，平衡资本效率与风险管理。

Method: 构建包含初始选股、信号分析、动态仓位调整、止盈止损优化、市场时机判断五个模块的框架，采用自适应持有期和进出时机平衡资本效率与风险管理。

Result: 在2010 - 2020年数据训练，2021 - 2024年数据回测，年化收益率15.2%，最大回撤低于5%，夏普比率1.87，每日维持50 - 100个仓位，最大持有期9天。

Conclusion: 该策略在不同市场表现稳健，资本容量高，适合机构部署。

Abstract: This paper presents a sophisticated multi-day turnover quantitative trading
algorithm that integrates advanced deep learning techniques with comprehensive
cross-sectional stock prediction for the Chinese A-share market. Our framework
combines five interconnected modules: initial stock selection through deep
cross-sectional prediction networks, opening signal distribution analysis using
mixture models for arbitrage identification, market capitalization and
liquidity-based dynamic position sizing, grid-search optimized profit-taking
and stop-loss mechanisms, and multi-granularity volatility-based market timing
models. The algorithm employs a novel approach to balance capital efficiency
with risk management through adaptive holding periods and sophisticated
entry/exit timing. Trained on comprehensive A-share data from 2010-2020 and
rigorously backtested on 2021-2024 data, our method achieves remarkable
performance with 15.2\% annualized returns, maximum drawdown constrained below
5\%, and a Sharpe ratio of 1.87. The strategy demonstrates exceptional
scalability by maintaining 50-100 daily positions with a 9-day maximum holding
period, incorporating dynamic profit-taking and stop-loss mechanisms that
enhance capital turnover efficiency while preserving risk-adjusted returns. Our
approach exhibits robust performance across various market regimes while
maintaining high capital capacity suitable for institutional deployment.

</details>


### [79] [\textit{QuantMCP}: Grounding Large Language Models in Verifiable Financial Reality](https://arxiv.org/abs/2506.06622)
*Yifan Zeng*

Main category: cs.CE

TL;DR: 本文介绍QuantMCP框架，它能让大语言模型与金融数据结合，克服其在金融分析应用中的局限，支持更明智的金融决策。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在金融分析应用中存在数据幻觉和缺乏实时可验证金融信息的问题，需将其与金融现实结合。

Method: 利用模型上下文协议（MCP）进行标准化和安全的工具调用，使大语言模型能与多种Python可访问的金融数据API对接。

Result: 用户可通过自然语言准确获取最新金融数据，释放大语言模型的分析能力。

Conclusion: QuantMCP为对话式AI和金融数据搭建了可靠、可扩展且安全的桥梁，提升了大语言模型在金融应用中的可靠性和分析深度。

Abstract: Large Language Models (LLMs) hold immense promise for revolutionizing
financial analysis and decision-making, yet their direct application is often
hampered by issues of data hallucination and lack of access to real-time,
verifiable financial information. This paper introduces QuantMCP, a novel
framework designed to rigorously ground LLMs in financial reality. By
leveraging the Model Context Protocol (MCP) for standardized and secure tool
invocation, QuantMCP enables LLMs to accurately interface with a diverse array
of Python-accessible financial data APIs (e.g., Wind, yfinance). Users can
interact via natural language to precisely retrieve up-to-date financial data,
thereby overcoming LLM's inherent limitations in factual data recall. More
critically, once furnished with this verified, structured data, the LLM's
analytical capabilities are unlocked, empowering it to perform sophisticated
data interpretation, generate insights, and ultimately support more informed
financial decision-making processes. QuantMCP provides a robust, extensible,
and secure bridge between conversational AI and the complex world of financial
data, aiming to enhance both the reliability and the analytical depth of LLM
applications in finance.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [80] [KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes](https://arxiv.org/abs/2506.06541)
*Eugenie Lai,Gerardo Vitagliano,Ziyu Zhang,Sivaprasad Sudhir,Om Chabra,Anna Zeng,Anton A. Zabreyko,Chenning Li,Ferdi Kossmann,Jialin Ding,Jun Chen,Markos Markakis,Matthew Russo,Weiyang Wang,Ziniu Wu,Michael J. Cafarella,Lei Cao,Samuel Madden,Tim Kraska*

Main category: cs.DB

TL;DR: 提出KRAMABENCH基准测试集评估AI系统构建数据科学管道能力，结果显示现有模型有不足。


<details>
  <summary>Details</summary>
Motivation: 探究AI系统能力在复杂数据科学管道设计和执行中的转化程度。

Method: 引入含104个真实数据科学管道的KRAMABENCH基准，用DS - GURU框架评估5个通用模型和3个代码生成模型。

Result: 模型能解决明确的数据科学代码生成任务，但构建真实数据科学管道时现有模型不足。

Conclusion: KramaBench进展对开发真实应用的自主数据科学代理至关重要。

Abstract: Constructing real-world data-to-insight pipelines often involves data
extraction from data lakes, data integration across heterogeneous data sources,
and diverse operations from data cleaning to analysis. The design and
implementation of data science pipelines require domain knowledge, technical
expertise, and even project-specific insights. AI systems have shown remarkable
reasoning, coding, and understanding capabilities. However, it remains unclear
to what extent these capabilities translate into successful design and
execution of such complex pipelines. We introduce KRAMABENCH: a benchmark
composed of 104 manually-curated real-world data science pipelines spanning
1700 data files from 24 data sources in 6 different domains. We show that these
pipelines test the end-to-end capabilities of AI systems on data processing,
requiring data discovery, wrangling and cleaning, efficient processing,
statistical reasoning, and orchestrating data processing steps given a
high-level task. Our evaluation tests 5 general models and 3 code generation
models using our reference framework, DS-GURU, which instructs the AI model to
decompose a question into a sequence of subtasks, reason through each step, and
synthesize Python code that implements the proposed design. Our results on
KRAMABENCH show that, although the models are sufficiently capable of solving
well-specified data science code generation tasks, when extensive data
processing and domain knowledge are required to construct real-world data
science pipelines, existing out-of-box models fall short. Progress on
KramaBench represents crucial steps towards developing autonomous data science
agents for real-world applications. Our code, reference framework, and data are
available at https://github.com/mitdbg/KramaBench.

</details>


### [81] [QUITE: A Query Rewrite System Beyond Rules with LLM Agents](https://arxiv.org/abs/2506.07675)
*Yuyang Song,Hanxu Yan,Jiale Lao,Yibo Wang,Yufei Li,Yuanchun Zhou,Jianguo Wang,Mingjie Tang*

Main category: cs.DB

TL;DR: 本文提出基于LLM的无训练、反馈感知系统QUITE进行SQL查询重写，比基于规则的方法更优，实验显示性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的SQL查询重写方法有局限，人类专家可扩展性差，而LLM有语义和推理能力，因此提出用LLM重写SQL查询。

Method: 设计由有限状态机控制的多智能体框架；开发重写中间件；采用提示注入技术。

Result: QUITE比现有方法最多降低35.8%查询执行时间，多产生24.1%重写结果，覆盖更多查询情况。

Conclusion: 基于LLM的QUITE系统能将SQL查询重写为语义等价形式，性能优于基于规则的方法。

Abstract: Query rewrite transforms SQL queries into semantically equivalent forms that
run more efficiently. Existing approaches mainly rely on predefined rewrite
rules, but they handle a limited subset of queries and can cause performance
regressions. This limitation stems from three challenges of rule-based query
rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite
rules do not generalize to new query patterns, and (3) some rewrite techniques
cannot be expressed as fixed rules. Motivated by the fact that human experts
exhibit significantly better rewrite ability but suffer from scalability, and
Large Language Models (LLMs) have demonstrated nearly human-level semantic and
reasoning abilities, we propose a new approach of using LLMs to rewrite SQL
queries beyond rules. Due to the hallucination problems in LLMs, directly
applying LLMs often leads to nonequivalent and suboptimal queries. To address
this issue, we propose QUITE (query rewrite), a training-free and
feedback-aware system based on LLM agents that rewrites SQL queries into
semantically equivalent forms with significantly better performance, covering a
broader range of query patterns and rewrite strategies compared to rule-based
methods. Firstly, we design a multi-agent framework controlled by a finite
state machine (FSM) to equip LLMs with the ability to use external tools and
enhance the rewrite process with real-time database feedback. Secondly, we
develop a rewrite middleware to enhance the ability of LLMs to generate
optimized query equivalents. Finally, we employ a novel hint injection
technique to improve execution plans for rewritten queries. Extensive
experiments show that QUITE reduces query execution time by up to 35.8% over
state-of-the-art approaches and produces 24.1% more rewrites than prior
methods, covering query cases that earlier systems did not handle.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [82] [Generating representative macrobenchmark microservice systems from distributed traces with Palette](https://arxiv.org/abs/2506.06448)
*Vaastav Anand,Matheus Stolet,Jonathan Mace,Antoine Kaufmann*

Main category: cs.DC

TL;DR: 本文提出利用大公司分布式跟踪数据集生成有代表性的微服务系统，介绍基于GCM的系统拓扑抽象并融入Palette系统。


<details>
  <summary>Details</summary>
Motivation: 微服务发展需在有代表性系统中评估，但研究者和从业者常无法获取，只能用非代表性替代方案。

Method: 引入基于图形因果模型（GCM）的系统拓扑抽象，将其融入Palette系统，从分布式跟踪数据生成有代表性的微服务系统。

Result: 未提及明确结果。

Conclusion: 未提及明确结论。

Abstract: Microservices are the dominant design for developing cloud systems
  today. Advancements for microservice need to be evaluated in representative
systems, e.g. with matching scale, topology, and execution patterns.
  Unfortunately in practice, researchers and practitioners alike often do not
have access to representative systems. Thus they have to resort to sub-optimal
non-representative alternatives, e.g. small and oversimplified synthetic
benchmark systems or simulated system models instead.
  To solve this issue, we propose the use of distributed trace datasets,
available from large internet companies,
  to generate representative microservice systems.
  To do so, we introduce a novel abstraction of a system topology which uses
Graphical Causal Models (GCMs)
  to model the underlying system by incorporating the branching probabilities,
execution order of outgoing
  calls to every dependency, and execution times.
  We then incorporate this topology in Palette, a system that generates
  representative flexible macrobenchmarks microservice systems from distributed
traces.

</details>


### [83] [Performance Impact of Containerized METADOCK 2 on Heterogeneous Platforms](https://arxiv.org/abs/2506.06450)
*Antonio Jesús Banegas-Luna,Baldomero Imbernón Tudela,Carlos Martínez-Cortés,José María Cecilia,Horacio Pérez-Sánchez*

Main category: cs.DC

TL;DR: 研究评估容器化对METADOCK 2在异构HPC平台上的性能影响，发现容器化开销可忽略，METADOCK 2处理能力强，容器化部署有优势。


<details>
  <summary>Details</summary>
Motivation: 评估容器化对METADOCK 2在异构高性能计算平台上的性能影响，以用于药物发现中的虚拟筛选。

Method: 测试Docker、Singularity和Apptainer三种容器化技术，在不同CPU和GPU配置下进行实验。

Result: 容器化引入的性能开销可忽略，偏差低于1%；METADOCK 2能高效处理大分子复合物，超越商业工具。

Conclusion: 容器化的METADOCK 2是异构HPC平台上虚拟筛选任务的强大高效解决方案。

Abstract: Virtual screening (VS) is a computationally intensive process crucial for
drug discovery, often requiring significant resources to analyze large chemical
libraries and predict ligand-protein interactions. This study evaluates the
performance impact of containerization on METADOCK 2, a high-throughput docking
software when deployed on heterogeneous high-performance computing (HPC)
platforms. By testing three containerization technologies - Docker,
Singularity, and Apptainer - across varying CPU and GPU configurations, the
experiments reveal that containerization introduces negligible performance
overhead, with deviations below 1%. Moreover, METADOCK 2 demonstrated the
capability to efficiently process large molecular complexes, surpassing the
limitations of commercial tools such as AutoDock Vina. The results underscore
the advantages of container-based deployment for ensuring portability,
reproducibility, and scalability in scientific computing. This study concludes
that containerized METADOCK 2 is a robust and efficient solution for VS tasks
on heterogeneous HPC platforms.

</details>


### [84] [Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage](https://arxiv.org/abs/2506.06472)
*Ziqi Yuan,Haoyang Zhang,Yirui Eric Zhou,Apoorve Mohan,I-Hsin Chung,Seetharami Seelam,Jian Huang*

Main category: cs.DC

TL;DR: 提出用于GPU内存扩展的生命周期感知张量卸载框架TERAIO，能优化LLM训练性能。


<details>
  <summary>Details</summary>
Motivation: 观察到LLM训练中活跃张量占GPU内存小，非活跃张量大且长时间不用，有卸载/预取机会，需扩展GPU内存。

Method: 通过训练前期迭代分析准确估计张量生命周期，生成优化卸载/预取计划并集成到PyTorch程序，用运行时迁移引擎执行计划。

Result: 与先进研究相比，TERAIO平均提升多种LLM训练性能1.47倍，达到假设无限GPU内存下理想性能的80.7%。

Conclusion: TERAIO能有效利用低成本SSD扩展GPU内存，提升LLM训练性能。

Abstract: We present the design and implementation of a new lifetime-aware tensor
offloading framework for GPU memory expansion using low-cost PCIe-based
solid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for
large language model (LLM) training with multiple GPUs and multiple SSDs. Its
design is driven by our observation that the active tensors take only a small
fraction (1.7% on average) of allocated GPU memory in each LLM training
iteration, the inactive tensors are usually large and will not be used for a
long period of time, creating ample opportunities for offloading/prefetching
tensors to/from slow SSDs without stalling the GPU training process. TERAIO
accurately estimates the lifetime (active period of time in GPU memory) of each
tensor with the profiling of the first few iterations in the training process.
With the tensor lifetime analysis, TERAIO will generate an optimized tensor
offloading/prefetching plan and integrate it into the compiled LLM program via
PyTorch. TERAIO has a runtime tensor migration engine to execute the
offloading/prefetching plan via GPUDirect storage, which allows direct tensor
migration between GPUs and SSDs for alleviating the CPU bottleneck and
maximizing the SSD bandwidth utilization. In comparison with state-of-the-art
studies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves
the training performance of various LLMs by 1.47x on average, and achieves
80.7% of the ideal performance assuming unlimited GPU memory.

</details>


### [85] [pFedSOP : Accelerating Training Of Personalized Federated Learning Using Second-Order Optimization](https://arxiv.org/abs/2506.07159)
*Mrinmay Sen,Chalavadi Krishna Mohan*

Main category: cs.DC

TL;DR: 提出pFedSOP方法，利用二阶优化加速个性化联邦学习，减少通信轮次，实验表明其优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有个性化联邦学习方法训练慢、通信轮次多、本地计算量大，二阶优化虽有潜力但应用有挑战。

Method: 先通过基于Gompertz函数的局部和全局梯度更新夹角计算个性化局部梯度更新，再用正则化Fisher信息矩阵近似Hessian更新模型。

Result: 在异质划分图像分类数据集部分客户端参与实验中，pFedSOP优于现有FL和PFL算法。

Conclusion: pFedSOP能有效利用二阶优化加速个性化模型训练，减少通信轮次，提升性能。

Abstract: Personalized Federated Learning (PFL) enables clients to collaboratively
train personalized models tailored to their individual objectives, addressing
the challenge of model generalization in traditional Federated Learning (FL)
due to high data heterogeneity. However, existing PFL methods often require
increased communication rounds to achieve the desired performance, primarily
due to slow training caused by the use of first-order optimization, which has
linear convergence. Additionally, many of these methods increase local
computation because of the additional data fed into the model during the search
for personalized local models. One promising solution to this slow training is
second-order optimization, known for its quadratic convergence. However,
employing it in PFL is challenging due to the Hessian matrix and its inverse.
In this paper, we propose pFedSOP, which efficiently utilizes second-order
optimization in PFL to accelerate the training of personalized models and
enhance performance with fewer communication rounds. Our approach first
computes a personalized local gradient update using the Gompertz function-based
normalized angle between local and global gradient updates, incorporating
client-specific global information. We then use a regularized Fisher
Information Matrix (FIM), computed from this personalized gradient update, as
an approximation of the Hessian to update the personalized models. This
FIM-based second-order optimization speeds up training with fewer communication
rounds by tackling the challenges with exact Hessian and avoids additional data
being fed into the model during the search for personalized local models.
Extensive experiments on heterogeneously partitioned image classification
datasets with partial client participation demonstrate that pFedSOP outperforms
state-of-the-art FL and PFL algorithms.

</details>


### [86] [Addressing tokens dynamic generation, propagation, storage and renewal to secure the GlideinWMS pilot based jobs and system](https://arxiv.org/abs/2506.07379)
*Bruno Moreira Coimbra,Marco Mambelli*

Main category: cs.DC

TL;DR: 本文介绍GlideinWMS从X.509过渡到支持令牌的挑战、演变计划及新凭证模块设计。


<details>
  <summary>Details</summary>
Motivation: 解决广泛采用令牌带来的挑战，满足新要求，支持实验和资源向令牌迁移。

Method: 设计新的凭证模块，使其适用于多系统，动态生成凭证，考虑添加存储、更新和失效机制。

Result: 新凭证模块可定制时长和范围，能执行最小权限原则。

Conclusion: 通过设计新模块和考虑相关机制，可更好服务实验需求。

Abstract: GlideinWMS has been one of the first middleware in the WLCG community to
transition from X.509 to support also tokens. The first step was to get from
the prototype in 2019 to using tokens in production in 2022. This paper will
present the challenges introduced by the wider adoption of tokens and the
evolution plans for securing the pilot infrastructure of GlideinWMS and
supporting the new requirements. In the last couple of years, the GlideinWMS
team supported the migration of experiments and resources to tokens. Inadequate
support in the current infrastructure, more stringent requirements, and the
higher spatial and temporal granularity forced GlideinWMS to revisit once more
how credentials are generated, used, and propagated. The new credential modules
have been designed to be used in multiple systems (GlideinWMS, HEPCloud) and
use a model where credentials have type, purpose, and different flows.
Credentials are dynamically generated in order to customize the duration and
limit the scope to the targeted resource. This allows to enforce the least
privilege principle. Finally, we also considered adding credential storage,
renewal, and invalidation mechanisms within the GlideinWMS infrastructure to
better serve the experiments' needs.

</details>


### [87] [New Limits on Distributed Quantum Advantage: Dequantizing Linear Programs](https://arxiv.org/abs/2506.07574)
*Alkida Balliu,Corinna Coupette,Antonio Cruciani,Francesco d'Amore,Massimo Equi,Henrik Lievonen,Augusto Modanese,Dennis Olivetti,Jukka Suomela*

Main category: cs.DC

TL;DR: 本文给出两个结果，限制分布式计算LOCAL模型下的分布式量子优势，包括线性规划无量子优势，及存在LCL问题量子LOCAL弱于经典确定性SLOCAL模型，结果还拓展到其他分布。


<details>
  <summary>Details</summary>
Motivation: 研究分布式计算LOCAL模型下分布式量子优势的新限制。

Method: 先证明线性规划无分布式量子优势，即能将量子LOCAL算法转换为经典确定性LOCAL算法；再利用此结果研究LCL问题。

Result: 线性规划无分布式量子优势，经典下界适用于量子LOCAL；存在LCL问题量子LOCAL弱于经典确定性SLOCAL模型；结果拓展到有限依赖和非信号分布，证明非信号模型和SLOCAL模型在LCL问题上不可比。

Conclusion: 对分布式计算LOCAL模型下的分布式量子优势给出新限制，拓展了相关模型的研究。

Abstract: In this work, we give two results that put new limits on distributed quantum
advantage in the context of the LOCAL model of distributed computing. First, we
show that there is no distributed quantum advantage for any linear program. Put
otherwise, if there is a quantum-LOCAL algorithm $\mathcal{A}$ that finds an
$\alpha$-approximation of some linear optimization problem $\Pi$ in $T$
communication rounds, we can construct a classical, deterministic LOCAL
algorithm $\mathcal{A}'$ that finds an $\alpha$-approximation of $\Pi$ in $T$
rounds. As a corollary, all classical lower bounds for linear programs,
including the KMW bound, hold verbatim in quantum-LOCAL. Second, using the
above result, we show that there exists a locally checkable labeling problem
(LCL) for which quantum-LOCAL is strictly weaker than the classical
deterministic SLOCAL model. Our results extend from quantum-LOCAL also to
finitely dependent and non-signaling distributions, and one of the corollaries
of our work is that the non-signaling model and the SLOCAL model are
incomparable in the context of LCL problems: By prior work, there exists an LCL
problem for which SLOCAL is strictly weaker than the non-signaling model, and
our work provides a separation in the opposite direction.

</details>


### [88] [A Terminology for Scientific Workflow Systems](https://arxiv.org/abs/2506.07838)
*Frédéric Sutera,Tainã Coleman,İlkay Altintaş,Rosa M. Badia,Bartosz Balis,Kyle Chard,Iacopo Colonnelli,Ewa Deelman,Paolo Di Tommaso,Thomas Fahringer,Carole Goble,Shantenu Jha,Daniel S. Katz,Johannes Köster,Ulf Leser,Kshitij Mehta,Hilary Oliver,J. -Luc Peterson,Giovanni Pizzi,Loïc Pottier,Raül Sirvent,Eric Suchyta,Douglas Thain,Sean R. Wilkinson,Justin M. Wozniak,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 论文指出科学工作流管理系统（WMS）多样，选择困难，提出社区术语并对23个WMS分类。


<details>
  <summary>Details</summary>
Motivation: 众多WMS各有特点，开发者选择困难，需统一术语辅助选择。

Method: WMS开发者和从业者合作产生社区术语，包含五个轴及相关概念。

Result: 得出包含五个轴的WMS术语，对23个现有WMS进行分类。

Conclusion: 新术语可用于表征WMS，帮助开发者选择合适系统。

Abstract: The term scientific workflow has evolved over the last two decades to
encompass a broad range of compositions of interdependent compute tasks and
data movements. It has also become an umbrella term for processing in modern
scientific applications. Today, many scientific applications can be considered
as workflows made of multiple dependent steps, and hundreds of workflow
management systems (WMSs) have been developed to manage and run these
workflows. However, no turnkey solution has emerged to address the diversity of
scientific processes and the infrastructure on which they are implemented.
Instead, new research problems requiring the execution of scientific workflows
with some novel feature often lead to the development of an entirely new WMS. A
direct consequence is that many existing WMSs share some salient features,
offer similar functionalities, and can manage the same categories of workflows
but also have some distinct capabilities. This situation makes researchers who
develop workflows face the complex question of selecting a WMS. This selection
can be driven by technical considerations, to find the system that is the most
appropriate for their application and for the resources available to them, or
other factors such as reputation, adoption, strong community support, or
long-term sustainability. To address this problem, a group of WMS developers
and practitioners joined their efforts to produce a community-based terminology
of WMSs. This paper summarizes their findings and introduces this new
terminology to characterize WMSs. This terminology is composed of fives axes:
workflow characteristics, composition, orchestration, data management, and
metadata capture. Each axis comprises several concepts that capture the
prominent features of WMSs. Based on this terminology, this paper also presents
a classification of 23 existing WMSs according to the proposed axes and terms.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [89] [Efficient Computation of Closed Substrings](https://arxiv.org/abs/2506.06452)
*Samkith K Jain,Neerja Mhaskar*

Main category: cs.DS

TL;DR: 提出O(n log n)时间算法计算所有闭子串，给出计算最大闭子串的方法，算出斐波那契单词中最大闭子串数量。


<details>
  <summary>Details</summary>
Motivation: 解决计算字符串中所有闭子串和最大闭子串的问题，并探究斐波那契单词中最大闭子串的数量。

Method: 引入紧凑表示计算闭子串，利用后缀数组和最长公共前缀数组计算最大闭子串，理论推导斐波那契单词中最大闭子串数量。

Result: 得出O(n log n)时间算法计算闭子串，给出计算最大闭子串的简单高效方法，算出斐波那契单词中最大闭子串数量约为1.382 F_n。

Conclusion: 该算法能高效计算闭子串和最大闭子串，且明确了斐波那契单词中最大闭子串数量。

Abstract: A closed string $u$ is either of length one or contains a border that occurs
only as a prefix and as a suffix in $u$ and nowhere else within $u$. In this
paper, we present a fast and practical $O(n\log n)$ time algorithm to compute
all $\Theta(n^2)$ closed substrings by introducing a compact representation for
all closed substrings of a string $ w[1..n]$, using only $O(n \log n)$ space.
We also present a simple and space-efficient solution to compute all maximal
closed substrings (MCSs) using the suffix array ($\mathsf{SA}$) and the longest
common prefix ($\mathsf{LCP}$) array of $w[1..n]$. Finally, we show that the
exact number of MCSs ($M(f_n)$) in a Fibonacci word $ f_n $, for $n \geq 5$, is
$\approx \left(1 + \frac{1}{\phi^2}\right) F_n \approx 1.382 F_n$, where $ \phi
$ is the golden ratio.

</details>


### [90] [Sample and Expand: Discovering Low-rank Submatrices With Quality Guarantees](https://arxiv.org/abs/2506.06456)
*Martino Ciaperoni,Aristides Gionis,Heikki Mannila*

Main category: cs.DS

TL;DR: 提出解决从给定矩阵中发现与低秩近似有界偏差子矩阵问题的两阶段方法，实验表明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有矩阵低秩近似问题假设整个矩阵有低秩结构，在现实中常不成立，因此要解决从给定矩阵中发现与低秩近似有界偏差子矩阵的问题。

Method: 采用两阶段方法，先通过采样发现小的近似低秩子矩阵，然后在保持与低秩近似接近的情况下扩展这些子矩阵。

Result: 广泛的实验评估证实所提出的方法比现有方法更具优势。

Conclusion: 所提出的两阶段方法在解决从给定矩阵中发现与低秩近似有界偏差子矩阵问题上是有效的。

Abstract: The problem of approximating a matrix by a low-rank one has been extensively
studied. This problem assumes, however, that the whole matrix has a low-rank
structure. This assumption is often false for real-world matrices. We consider
the problem of discovering submatrices from the given matrix with bounded
deviations from their low-rank approximations. We introduce an effective
two-phase method for this task: first, we use sampling to discover small nearly
low-rank submatrices, and then they are expanded while preserving proximity to
a low-rank approximation. An extensive experimental evaluation confirms that
the method we introduce compares favorably to existing approaches.

</details>


### [91] [Modern Minimal Perfect Hashing: A Survey](https://arxiv.org/abs/2506.06536)
*Hans-Peter Lehmann,Thomas Mueller,Rasmus Pagh,Giulio Ermanno Pibiri,Peter Sanders,Sebastiano Vigna,Stefan Walzer*

Main category: cs.DS

TL;DR: 本文介绍完美哈希函数，回顾其进展，涵盖最新发展并给出实验评估以指导应用选择。


<details>
  <summary>Details</summary>
Motivation: 自1997年上次全面调查后，完美哈希函数有显著进展，需对其最新发展进行总结。

Method: 对完美哈希函数的研究成果进行综述，并开展广泛的实验评估。

Result: 介绍了完美哈希函数的特性、参数、不同方法的权衡及应用，完成了最新发展的调研和实验评估。

Conclusion: 该综述为熟悉该主题提供起点，实验评估能指导应用中完美哈希函数的选择。

Abstract: Given a set $S$ of $n$ keys, a perfect hash function for $S$ maps the keys in
$S$ to the first $m \geq n$ integers without collisions. It may return an
arbitrary result for any key not in $S$ and is called minimal if $m = n$. The
most important parameters are its space consumption, construction time, and
query time. Years of research now enable modern perfect hash functions to be
extremely fast to query, very space-efficient, and scale to billions of keys.
Different approaches give different trade-offs between these aspects. For
example, the smallest constructions get within 0.1% of the space lower bound of
$\log_2(e)$ bits per key. Others are particularly fast to query, requiring only
one memory access. Perfect hashing has many applications, for example to avoid
collision resolution in static hash tables, and is used in databases,
bioinformatics, and stringology.
  Since the last comprehensive survey in 1997, significant progress has been
made. This survey covers the latest developments and provides a starting point
for getting familiar with the topic. Additionally, our extensive experimental
evaluation can serve as a guide to select a perfect hash function for use in
applications.

</details>


### [92] [Online Job Assignment](https://arxiv.org/abs/2506.06893)
*Farbod Ekbatani,Yiding Feng,Ian Kash,Rad Niazadeh*

Main category: cs.DS

TL;DR: 研究云计算中在线分配问题，提出Forward - Looking BALANCE算法，证明其渐近最优竞争比并分析界的参数依赖。


<details>
  <summary>Details</summary>
Motivation: 源于云计算应用，解决不同持续时间的作业在线分配到离线服务器以最大化总奖励的问题。

Method: 设计Forward - Looking BALANCE算法，运用原始对偶框架，结合新的对偶拟合技术和归纳论证。

Result: FLB算法获得渐近竞争比为ln(RD)+3lnln(max(R,D))+O(1)，且该界对所有参数有最优依赖。

Conclusion: FLB算法在解决此在线分配问题上是（渐近）最优竞争的，分析方法有独立研究价值。

Abstract: Motivated primarily by applications in cloud computing, we study a simple,
yet powerful, online allocation problem in which jobs of varying durations
arrive over continuous time and must be assigned immediately and irrevocably to
one of the available offline servers. Each server has a fixed initial capacity,
with assigned jobs occupying one unit for their duration and releasing it upon
completion. The algorithm earns a reward for each assignment upon completion.
We consider a general heterogeneous setting where both the reward and duration
of a job depend on the job-server pair. The objective of the online algorithm
is to maximize the total collected reward, and remain competitive against an
omniscient benchmark that knows all job arrivals in advance. Our main
contribution is the design of a new online algorithm, termed Forward-Looking
BALANCE (FLB), and using primal-dual framework to establish that it is
(asymptotically) optimal-competitive.
  This meta-algorithm has two main primitives: (i) keeping track of the
capacity used for each server at each time and applying a penalty function to
this quantity, and (ii) adjusting the reward of assigning a job to a server by
subtracting the total penalty of a particularly chosen subset of future times,
in contrast to just looking at the current time. The FLB algorithm then assigns
the arriving job to the server with the maximum adjusted reward. If R and D are
the ratios of maximum over minimum rewards and durations, we show that the FLB
algorithm obtains an asymptotic competitive ratio of
ln(RD)+3lnln(max(R,D))+O(1). We further show this bound has optimal
dependencies on all the parameters. Our main analysis combines a novel
dual-fitting technique, which leverages the configuration LP benchmark for this
problem, and a novel inductive argument to establish the capacity feasibility
of the algorithm, which might be of independent interest.

</details>


### [93] [On Sketching Trimmed Statistics](https://arxiv.org/abs/2506.07342)
*Honghao Lin,Hoai-An Nguyen,David P. Woodruff*

Main category: cs.DS

TL;DR: 提出空间高效的线性草图用于估计频率向量的修剪统计量，给出不同情况的草图及条件，扩展到p>2并处理相关问题，算法空间效率优于Count - Sketch。


<details>
  <summary>Details</summary>
Motivation: 修剪统计量用于稳健估计，线性草图可提高时间和内存效率，适用于流和分布式场景，当前缺乏对这些统计量草图的研究。

Method: 开始研究这些统计量的草图，给出新的空间复杂度条件，针对不同k值情况设计线性草图。

Result: 在k ≥ n/poly log n时，给出poly(1/ε, log n)空间的线性草图；一般k值在特定条件下给出草图；k - 修剪版本在相同条件下实现最优误差保证；扩展到p > 2并处理相关问题；算法在空间上优于Govindan等人的算法和Count - Sketch。

Conclusion: 所提出的线性草图在估计修剪统计量方面有效，在空间效率上有优势。

Abstract: We present space-efficient linear sketches for estimating trimmed statistics
of an $n$-dimensional frequency vector $x$, e.g., the sum of $p$-th powers of
the largest $k$ frequencies (i.e., entries) in absolute value, or the
$k$-trimmed vector, which excludes the top and bottom $k$ frequencies. This is
called the $F_p$ moment of the trimmed vector. Trimmed measures are used in
robust estimation, as seen in the R programming language's `trim.var' function
and the `trim' parameter in the mean function. Linear sketches improve time and
memory efficiency and are applicable to streaming and distributed settings. We
initiate the study of sketching these statistics and give a new condition for
capturing their space complexity. When $k \ge n/poly\log n$, we give a linear
sketch using $poly(1/\varepsilon, \log n)$ space which provides a $(1 \pm
\varepsilon)$ approximation to the top-$k$ $F_p$ moment for $p \in [0,2]$. For
general $k$, we give a sketch with the same guarantees under a condition
relating the $k$-th largest frequency to the tail mass, and show this condition
is necessary. For the $k$-trimmed version, our sketch achieves optimal error
guarantees under the same condition. We extend our methods to $p > 2$ and also
address related problems such as computing the $F_p$ moment of frequencies
above a threshold, finding the largest $k$ such that the $F_p$ moment of the
top $k$ exceeds $k^{p+1}$, and the $F_p$ moment of the top $k$ frequencies such
that each entry is at least $k$. Notably, our algorithm for this third
application improves upon the space bounds of the algorithm of Govindan,
Monemizadeh, and Muthukrishnan (PODS '17) for computing the $h$-index. We show
empirically that our top $k$ algorithm uses much less space compared to
Count-Sketch while achieving the same error.

</details>


### [94] [On Deterministically Finding an Element of High Order Modulo a Composite](https://arxiv.org/abs/2506.07668)
*Ziv Oznovich,Ben Lee Volk*

Main category: cs.DS

TL;DR: 提出确定性算法，给定合数N和目标阶D，在D^{1/2+o(1)}时间内找到阶至少为D的元素或N的非平凡因子，改进Hittmeir算法。


<details>
  <summary>Details</summary>
Motivation: 改进Hittmeir在更强假设下设计的类似算法，降低对目标阶D的要求。

Method: 设计确定性算法，在给定条件下进行计算。

Result: 得到能在D^{1/2+o(1)}时间内找到元素或因子的算法，在N有r - 幂因子时对D的要求为D ≥ N^{1/6r}。

Conclusion: 新算法在目标阶D的要求上优于Hittmeir的算法。

Abstract: We give a deterministic algorithm that, given a composite number $N$ and a
target order $D \ge N^{1/6}$, runs in time $D^{1/2+o(1)}$ and finds either an
element $a \in \mathbb{Z}_N^*$ of multiplicative order at least $D$, or a
nontrivial factor of $N$. Our algorithm improves upon an algorithm of Hittmeir
(arXiv:1608.08766), who designed a similar algorithm under the stronger
assumption $D \ge N^{2/5}$. Hittmeir's algorithm played a crucial role in the
recent breakthrough deterministic integer factorization algorithms of Hittmeir
and Harvey (arXiv:2006.16729, arXiv:2010.05450, arXiv:2105.11105). When $N$ is
assumed to have an $r$-power divisor with $r\ge 2$, our algorithm provides the
same guarantees assuming $D \ge N^{1/6r}$.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [95] [Delegation with Costly Inspection](https://arxiv.org/abs/2506.07162)
*Mohammad T. Hajiaghayi,Piotr Krysta,Mohammad Mahdavi,Suho Shin*

Main category: cs.GT

TL;DR: 研究带检查成本的委托选择问题（DCIC），分析其与相关问题关系，给出无成本委托设置下近似机制及结果，并扩展到一般情况。


<details>
  <summary>Details</summary>
Motivation: 研究DCIC问题，该问题是相关问题的变体和扩展，考虑代理可能策略性误报效用，需研究如何使委托人效用最大化。

Method: 先考虑无成本委托设置，证明纯委托单检查与无委托PNOI策略组合的最大机制的近似率，再将技术扩展到一般情况。

Result: 证明无成本委托时相关机制对DCIC有3 - 近似且是紧的，检查成本相同时可提升到2 - 近似，还给出一般情况下的常数因子近似机制。

Conclusion: DCIC是PNOI的推广，针对不同情况给出了有效的近似机制。

Abstract: We study the problem of delegated choice with inspection cost (DCIC), which
is a variant of the delegated choice problem by Kleinberg and Kleinberg (EC'18)
as well as an extension of the Pandora's box problem with nonobligatory
inspection (PNOI) by Doval (JET'18). In our model, an agent may strategically
misreport the proposed element's utility, unlike the standard delegated choice
problem which assumes that the agent truthfully reports the utility for the
proposed alternative. Thus, the principal needs to inspect the proposed element
possibly along with other alternatives to maximize its own utility, given an
exogenous cost of inspecting each element. Further, the delegation itself
incurs a fixed cost, thus the principal can decide whether to delegate or not
and inspect by herself.
  We show that DCIC indeed is a generalization of PNOI where the side
information from a strategic agent is available at certain cost, implying its
NP-hardness by Fu, Li, and Liu (STOC'23). We first consider a costless
delegation setting in which the cost of delegation is free. We prove that the
maximal mechanism over the pure delegation with a single inspection and an PNOI
policy without delegation achieves a $3$-approximation for DCIC with costless
delegation, which is further proven to be tight. These results hold even when
the cost comes from an arbitrary monotone set function, and can be improved to
a $2$-approximation if the cost of inspection is the same for every element. We
extend these techniques by presenting a constant factor approximate mechanism
for the general setting for rich class of instances.

</details>


### [96] [Value-Set Iteration: Computing Optimal Correlated Equilibria in Infinite-Horizon Multi-Player Stochastic Games](https://arxiv.org/abs/2506.07186)
*Jiarui Gan,Rupak Majumdar*

Main category: cs.GT

TL;DR: 研究无限期多人随机博弈中最优相关均衡（CE）计算问题，提出计算(ε,δ)-最优CE的算法，证明双准则近似的必要性，核心是基于可诱导值集的方法。


<details>
  <summary>Details</summary>
Motivation: 无限期多人随机博弈中最优CE需要依赖历史的策略，存在表示和算法挑战，需要计算(ε,δ)-最优CE。

Method: 基于可诱导值集的新方法，开发值集迭代算法，将CE刻画为更新映射的最大不动点。

Result: 得到在时间复杂度为1/(εδ(1 - γ))^(n+1)的多项式时间内计算(ε,δ)-最优CE的算法，对回合制博弈进一步降低复杂度，证明双准则近似的必要性。

Conclusion: 算法为一般多人随机环境中计算最优CE提供基础。

Abstract: We study the problem of computing optimal correlated equilibria (CEs) in
infinite-horizon multi-player stochastic games, where correlation signals are
provided over time. In this setting, optimal CEs require history-dependent
policies; this poses new representational and algorithmic challenges as the
number of possible histories grows exponentially with the number of time steps.
We focus on computing $(\epsilon, \delta)$-optimal CEs -- solutions that
achieve a value within $\epsilon$ of an optimal CE, while allowing the agents'
incentive constraints to be violated by at most $\delta$. Our main result is an
algorithm that computes an $(\epsilon,\delta)$-optimal CE in time polynomial in
$1/(\epsilon\delta(1 - \gamma))^{n+1}$, where $\gamma$ is the discount factor,
and $n$ is the number of agents. For (a slightly more general variant of)
turn-based games, we further reduce the complexity to a polynomial in $n$. We
also establish that the bi-criterion approximation is necessary by proving
matching inapproximability bounds.
  Our technical core is a novel approach based on inducible value sets, which
leverages a compact representation of history-dependent CEs through the values
they induce to overcome the representational challenge. We develop the
value-set iteration algorithm -- which operates by iteratively updating
estimates of inducible value sets -- and characterize CEs as the greatest fixed
point of the update map. Our algorithm provides a groundwork for computing
optimal CEs in general multi-player stochastic settings.

</details>


### [97] [Vulnerability and Defence: A Case for Stackelberg Game Dynamics](https://arxiv.org/abs/2506.07316)
*Azhar Iqbal,Ishan Honhaga,Eyoel Teffera,Anthony Perry,Robin Baker,Glenn Pearce,Claudia Szabo*

Main category: cs.GT

TL;DR: 本文运用博弈论研究现代战争中无人机与坦克的战术互动，聚焦Stackelberg均衡和逆向归纳法。


<details>
  <summary>Details</summary>
Motivation: 研究现代战争中无人机与坦克的战术互动情况。

Method: 运用博弈论，将其概念化为序贯博弈，基于Stackelberg均衡和逆向归纳法分析。

Result: 文中未提及具体结果。

Conclusion: 文中未提及具体结论。

Abstract: This paper examines the tactical interaction between drones and tanks in
modern warfare through game theory, particularly focusing on Stackelberg
equilibrium and backward induction. It describes a high-stakes conflict between
two teams: one using advanced drones for attack, and the other defending using
tanks. The paper conceptualizes this as a sequential game, illustrating the
complex strategic dynamics similar to Stackelberg competition, where moves and
countermoves are carefully analyzed and predicted.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [98] [DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval](https://arxiv.org/abs/2506.06313)
*Huiyao Chen,Yi Yang,Yinghui Li,Meishan Zhang,Min Zhang*

Main category: cs.IR

TL;DR: 提出DISRetrieval分层检索框架，结合话语结构提升长文档理解，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能捕捉文档固有话语结构，无法有效解决长文档理解中上下文长度限制问题。

Method: 提出DISRetrieval框架，包含话语感知文档组织框架、LLM增强节点表示技术和分层证据检索机制。

Result: 在QASPER和QuALITY数据集实验中，DISRetrieval在检索指标和问答任务上显著优于现有方法。

Conclusion: 结合话语结构能显著提升检索效果，证明语言信息文档表示对长文本理解很重要。

Abstract: Long document understanding has become increasingly crucial in natural
language processing, with retrieval-based methods emerging as a promising
solution to address the context length limitations of large language models
(LLMs). However, existing approaches either treat documents as flat sequences
or employ arbitrary chunking strategies, failing to capture the inherent
discourse structure that guides human comprehension. We present DISRetrieval, a
novel hierarchical retrieval framework that leverages linguistic discourse
structure to enhance long document understanding. Our approach introduces three
key innovations: (1) a discourse-aware document organization framework that
utilizes rhetorical structure theory (RST) to create sentence-level
hierarchical representations, preserving both semantic relationships and
natural document flow; (2) an LLM-enhanced node representation technique that
combines discourse structure with adaptive summarization to enrich tree nodes
with contextual information; and (3) a hierarchical evidence retrieval
mechanism that effectively selects relevant content while maintaining discourse
coherence. Through comprehensive experiments on QASPER and QuALITY datasets,
DISRetrieval demonstrates substantial improvements over existing methods in
both token-level retrieval metrics and downstream question answering tasks. Our
ablation studies confirm that incorporating discourse structure significantly
enhances retrieval effectiveness across different document lengths and query
types, validating the importance of linguistically-informed document
representation in long-text understanding. Our code and datasets are publicly
available at github/DreamH1gh/DISRetrieval to facilitate future research.

</details>


### [99] [A Reinforcement-Learning-Enhanced LLM Framework for Automated A/B Testing in Personalized Marketing](https://arxiv.org/abs/2506.06316)
*Haoyang Feng,Yanjun Dai,Yuan Gao*

Main category: cs.IR

TL;DR: 本文提出RL - LLM - AB测试框架用于个性化营销A/B测试，通过多模块协作，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 个性化营销中，需有效算法优化A/B测试以最大化用户响应。

Method: 构建RL - LLM - AB测试框架，基于预训练语言模型，用Prompt - Conditioned Generator生成内容变体，多模态感知模块构建交互状态，策略优化模块实时选版本，嵌入Memory - Augmented Reward Estimator捕捉用户偏好漂移。

Result: 数值结果表明，RL - LLM - ABTest在真实营销数据上优于经典A/B测试、Contextual Bandits和基准强化学习方法。

Conclusion: 提出的RL - LLM - ABTest框架在个性化营销A/B测试中表现优越，能更好地应对挑战。

Abstract: For personalized marketing, a new challenge of how to effectively algorithm
the A/B testing to maximize user response is urgently to be overcome. In this
paper, we present a new approach, the RL-LLM-AB test framework, for using
reinforcement learning strategy optimization combined with LLM to automate and
personalize A/B tests. The RL-LLM-AB test is built upon the pre-trained
instruction-tuned language model. It first generates A/B versions of candidate
content variants using a Prompt-Conditioned Generator, and then dynamically
embeds and fuses the user portrait and the context of the current query with
the multi-modal perception module to constitute the current interaction state.
The content version is then selected in real-time through the policy
optimization module with an Actor-Critic structure, and long-term revenue is
estimated according to real-time feedback (such as click-through rate and
conversion rate). Furthermore, a Memory-Augmented Reward Estimator is embedded
into the framework to capture long-term user preference drift, which helps to
generalize policy across multiple users and content contexts. Numerical results
demonstrate the superiority of our proposed RL-LLM-ABTest over existing A/B
testing methods, including classical A/B testing, Contextual Bandits, and
benchmark reinforcement learning approaches on real-world marketing data.

</details>


### [100] [Is BERTopic Better than PLSA for Extracting Key Topics in Aviation Safety Reports?](https://arxiv.org/abs/2506.06328)
*Aziida Nanyonga,Joiner Keith,Turhan Ugur,Wild Graham*

Main category: cs.IR

TL;DR: 对比BERTopic和PLSA从航空安全报告提取主题的效果，BERTopic表现更优，为未来研究指明方向。


<details>
  <summary>Details</summary>
Motivation: 比较BERTopic和PLSA从航空安全报告中提取有意义主题的有效性，以增强对航空事故数据模式的理解。

Method: 使用2000 - 2020年超36000份NTSB报告数据集，BERTopic用基于变压器的嵌入和层次聚类，PLSA用期望最大化（EM）算法进行概率建模。

Result: BERTopic在主题连贯性上优于PLSA，Cv分数0.41高于PLSA的0.37，且经航空安全专家验证有更好的可解释性。

Conclusion: 现代基于变压器的方法在分析复杂航空数据集上有优势，未来将探索混合模型、多语言数据集和先进聚类技术以改进主题建模。

Abstract: This study compares the effectiveness of BERTopic and Probabilistic Latent
Semantic Analysis (PLSA) in extracting meaningful topics from aviation safety
reports aiming to enhance the understanding of patterns in aviation incident
data. Using a dataset of over 36,000 National Transportation Safety Board
(NTSB) reports from 2000 to 2020, BERTopic employed transformer based
embeddings and hierarchical clustering, while PLSA utilized probabilistic
modelling through the Expectation-Maximization (EM) algorithm. Results showed
that BERTopic outperformed PLSA in topic coherence, achieving a Cv score of
0.41 compared to PLSA 0.37, while also demonstrating superior interpretability
as validated by aviation safety experts. These findings underscore the
advantages of modern transformer based approaches in analyzing complex aviation
datasets, paving the way for enhanced insights and informed decision-making in
aviation safety. Future work will explore hybrid models, multilingual datasets,
and advanced clustering techniques to further improve topic modelling in this
domain.

</details>


### [101] [FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models](https://arxiv.org/abs/2506.06335)
*Xuan Xu,Fufang Wen,Beilin Chu,Zhibing Fu,Qinhong Lin,Jiaqi Liu,Binjie Fei,Zhongliang Yang,Linna Zhou,Yu Li*

Main category: cs.IR

TL;DR: 论文指出大语言模型在金融领域应用的局限，介绍了基于金融语料预训练的FinBERT2，其在多项金融任务中表现出色，为金融BERT模型在大语言模型时代的应用提供见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在金融领域实际应用存在计算资源成本高、性能差、检索表现不佳等局限，需更好的模型。

Method: 在320亿高质量金融特定语料上预训练双向编码器FinBERT2，构建不同微调模型。

Result: FinBERT2在金融分类、检索任务及主题建模中表现优于其他模型。

Conclusion: 通过与当代大语言模型对比分析，为大语言模型时代有效利用金融BERT模型提供实用见解。

Abstract: In natural language processing (NLP), the focus has shifted from encoder-only
tiny language models like BERT to decoder-only large language models(LLMs) such
as GPT-3. However, LLMs' practical application in the financial sector has
revealed three limitations: (1) LLMs often perform worse than fine-tuned BERT
on discriminative tasks despite costing much higher computational resources,
such as market sentiment analysis in financial reports; (2) Application on
generative tasks heavily relies on retrieval augmented generation (RAG) methods
to provide current and specialized information, with general retrievers showing
suboptimal performance on domain-specific retrieval tasks; (3) There are
additional inadequacies in other feature-based scenarios, such as topic
modeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained
on a high-quality, financial-specific corpus of 32b tokens. This represents the
largest known Chinese financial pretraining corpus for models of this parameter
size. As a better backbone, FinBERT2 can bridge the gap in the
financial-specific deployment of LLMs through the following achievements: (1)
Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT
variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five
financial classification tasks. (2) Contrastive fine-tuned models
(Fin-Retrievers) outperform both open-source (e.g., +6.8\% avg improvement over
BGE-base-zh) and proprietary (e.g., +4.2\% avg improvement over OpenAI's
text-embedding-3-large) embedders across five financial retrieval tasks; (3)
Building on FinBERT2 variants, we construct the Fin-TopicModel, which enables
superior clustering and topic representation for financial titles. Our work
revisits financial BERT models through comparative analysis with contemporary
LLMs and offers practical insights for effectively utilizing FinBERT in the
LLMs era.

</details>


### [102] [Preference-based learning for news headline recommendation](https://arxiv.org/abs/2506.06334)
*Alexandre Bouras,Audrey Durand,Richard Khoury*

Main category: cs.IR

TL;DR: 研究通过基于偏好的学习优化新闻标题推荐策略，用真实数据训练推荐代理，发现有噪声上下文时无需显式探索。


<details>
  <summary>Details</summary>
Motivation: 探索优化新闻标题推荐的策略。

Method: 使用法语在线新闻用户交互的真实数据，在上下文多臂老虎机设置下训练标题推荐代理。

Result: 有噪声上下文时可能不需要显式探索。

Conclusion: 可采用更简单但高效的策略进行新闻标题推荐。

Abstract: This study explores strategies for optimizing news headline recommendations
through preference-based learning. Using real-world data of user interactions
with French-language online news posts, we learn a headline recommender agent
under a contextual bandit setting. This allows us to explore the impact of
translation on engagement predictions, as well as the benefits of different
interactive strategies on user engagement during data collection. Our results
show that explicit exploration may not be required in the presence of noisy
contexts, opening the door to simpler but efficient strategies in practice.

</details>


### [103] [Research on E-Commerce Long-Tail Product Recommendation Mechanism Based on Large-Scale Language Models](https://arxiv.org/abs/2506.06336)
*Qingyi Lu,Haotian Lyu,Jiayun Zheng,Yang Wang,Li Zhang,Chengrui Zhou*

Main category: cs.IR

TL;DR: 为解决电商平台长尾产品推荐难题，提出结合大语言模型的推荐机制，实验表明该方法在多项指标上优于基线模型，凸显大语言模型潜力。


<details>
  <summary>Details</summary>
Motivation: 电商平台拓展商品目录时，传统推荐方法受长尾问题（数据稀疏和冷启动）限制，准确推荐长尾商品对提升用户体验和平台收入至关重要。

Method: 提出结合产品文本描述和用户行为序列的长尾产品推荐机制，引入语义visor将多模态文本转换为嵌入表示，用基于注意力的用户意图编码器捕捉用户潜在兴趣，构建混合排序模型融合多种得分和推荐候选。

Result: 在真实电商数据集上实验，该方法在召回率（+12%）、命中率（+9%）和用户覆盖率（+15%）上优于基线模型，提升了长尾产品曝光和购买率。

Conclusion: 强调大语言模型在解读产品内容和用户意图方面的潜力，为未来电商推荐系统提供了有前景的方向。

Abstract: As e-commerce platforms expand their product catalogs, accurately
recommending long-tail items becomes increasingly important for enhancing both
user experience and platform revenue. A key challenge is the long-tail problem,
where extreme data sparsity and cold-start issues limit the performance of
traditional recommendation methods. To address this, we propose a novel
long-tail product recommendation mechanism that integrates product text
descriptions and user behavior sequences using a large-scale language model
(LLM). First, we introduce a semantic visor, which leverages a pre-trained LLM
to convert multimodal textual content such as product titles, descriptions, and
user reviews into meaningful embeddings. These embeddings help represent
item-level semantics effectively. We then employ an attention-based user intent
encoder that captures users' latent interests, especially toward long-tail
items, by modeling collaborative behavior patterns. These components feed into
a hybrid ranking model that fuses semantic similarity scores, collaborative
filtering outputs, and LLM-generated recommendation candidates. Extensive
experiments on a real-world e-commerce dataset show that our method outperforms
baseline models in recall (+12%), hit rate (+9%), and user coverage (+15%).
These improvements lead to better exposure and purchase rates for long-tail
products. Our work highlights the potential of LLMs in interpreting product
content and user intent, offering a promising direction for future e-commerce
recommendation systems.

</details>


### [104] [Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components](https://arxiv.org/abs/2506.06339)
*Jumana Alsubhi,Mohammad D. Alahmadi,Ahmed Alhusayni,Ibrahim Aldailami,Israa Hamdine,Ahmad Shabana,Yazeed Iskandar,Suhayb Khayyat*

Main category: cs.IR

TL;DR: 本文对阿拉伯语检索增强生成（RAG）组件进行综合评估，得出不同组件的最佳选择，为构建高质量阿拉伯语RAG管道提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有研究对阿拉伯语RAG组件的优化探索不足，需要对其进行研究。

Method: 使用RAGAS框架，在多个阿拉伯语数据集上对RAG组件进行评估，比较四个核心指标。

Result: 句子感知分块策略表现最佳，BGE - M3和Multilingual - E5 - large是最有效的嵌入模型，加入重排器可提升复杂数据集的忠实度，Aya - 8B生成质量超过StableLM。

Conclusion: 研究结果为构建高质量阿拉伯语RAG管道提供关键见解和选择组件的实用指南。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture
for combining the precision of retrieval systems with the fluency of large
language models. While several studies have investigated RAG pipelines for
high-resource languages, the optimization of RAG components for Arabic remains
underexplored. This study presents a comprehensive empirical evaluation of
state-of-the-art RAG components-including chunking strategies, embedding
models, rerankers, and language models-across a diverse set of Arabic datasets.
Using the RAGAS framework, we systematically compare performance across four
core metrics: context precision, context recall, answer faithfulness, and
answer relevancy. Our experiments demonstrate that sentence-aware chunking
outperforms all other segmentation methods, while BGE-M3 and
Multilingual-E5-large emerge as the most effective embedding models. The
inclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness
in complex datasets, and Aya-8B surpasses StableLM in generation quality. These
findings provide critical insights for building high-quality Arabic RAG
pipelines and offer practical guidelines for selecting optimal components
across different document types.

</details>


### [105] [Structured Semantics from Unstructured Notes: Language Model Approaches to EHR-Based Decision Support](https://arxiv.org/abs/2506.06340)
*Wu Hao Ran,Xi Xi,Furong Li,Jingyi Lu,Jian Jiang,Hui Huang,Yuzhuan Zhang,Shi Li*

Main category: cs.IR

TL;DR: 本文探讨用先进语言模型分析电子健康记录（EHR）以提供临床决策支持，还讨论相关挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型分析EHR中复杂非结构化数据，提升临床决策支持。

Method: 应用先进语言模型，利用EHR多样数据源，重视文本特征。

Result: 未提及具体结果。

Conclusion: 未提及具体结论，仅表明会探讨结合医疗代码、确保模型泛化性和公平性的挑战与机遇。

Abstract: The advent of large language models (LLMs) has opened new avenues for
analyzing complex, unstructured data, particularly within the medical domain.
Electronic Health Records (EHRs) contain a wealth of information in various
formats, including free text clinical notes, structured lab results, and
diagnostic codes. This paper explores the application of advanced language
models to leverage these diverse data sources for improved clinical decision
support. We will discuss how text-based features, often overlooked in
traditional high dimensional EHR analysis, can provide semantically rich
representations and aid in harmonizing data across different institutions.
Furthermore, we delve into the challenges and opportunities of incorporating
medical codes and ensuring the generalizability and fairness of AI models in
healthcare.

</details>


### [106] [NR4DER: Neural Re-ranking for Diversified Exercise Recommendation](https://arxiv.org/abs/2506.06341)
*Xinghe Cheng,Xufang Zhou,Liangda Fang,Chaobo He,Yuyu Zhou,Weiqi Luo,Zhiguo Gong,Quanlong Guan*

Main category: cs.IR

TL;DR: 提出NR4DER方法解决现有练习推荐方法问题，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有练习推荐方法存在高辍学率、无法匹配学生多样学习节奏、难以适应非活跃学生学习模式等问题，影响推荐准确性和多样性。

Method: 提出NR4DER方法，先利用mLSTM模型改进练习过滤模块，再用序列增强方法提升非活跃学生表示，精准匹配学生与合适难度练习，最后利用神经重排基于学生学习历史生成多样推荐列表。

Result: 在多个真实数据集上，NR4DER显著优于现有方法。

Conclusion: NR4DER能有效满足学生多样学习节奏。

Abstract: With the widespread adoption of online education platforms, an increasing
number of students are gaining new knowledge through Massive Open Online
Courses (MOOCs). Exercise recommendation have made strides toward improving
student learning outcomes. However, existing methods not only struggle with
high dropout rates but also fail to match the diverse learning pace of
students. They frequently face difficulties in adjusting to inactive students'
learning patterns and in accommodating individualized learning paces, resulting
in limited accuracy and diversity in recommendations. To tackle these
challenges, we propose Neural Re-ranking for Diversified Exercise
Recommendation (in short, NR4DER). NR4DER first leverages the mLSTM model to
improve the effectiveness of the exercise filter module. It then employs a
sequence enhancement method to enhance the representation of inactive students,
accurately matches students with exercises of appropriate difficulty. Finally,
it utilizes neural re-ranking to generate diverse recommendation lists based on
individual students' learning histories. Extensive experimental results
indicate that NR4DER significantly outperforms existing methods across multiple
real-world datasets and effectively caters to the diverse learning pace of
students.

</details>


### [107] [Infinity Search: Approximate Vector Search with Projections on q-Metric Spaces](https://arxiv.org/abs/2506.06557)
*Antonio Pariente,Ignacio Hounie,Santiago Segarra,Alejandro Ribeiro*

Main category: cs.IR

TL;DR: 文章指出现有向量搜索算法忽视向量嵌入的度量结构，提出将向量数据集嵌入q - 度量空间的投影方法，实验表明学习q - 度量近似可让经典度量树算法在高维数据搜索中取得有竞争力的表现。


<details>
  <summary>Details</summary>
Motivation: 现有向量搜索算法忽略向量嵌入的度量结构，未利用其底层属性。

Method: 提出一种投影方法，将具有任意相异度量的向量数据集嵌入q - 度量空间并保留最近邻；学习投影的近似以高效转换查询点。

Result: 在文本和图像向量嵌入实验中，学习q - 度量近似使经典度量树算法能与最先进搜索方法竞争。

Conclusion: 学习q - 度量近似可使经典度量树算法在高维数据搜索中表现良好。

Abstract: Despite the ubiquity of vector search applications, prevailing search
algorithms overlook the metric structure of vector embeddings, treating it as a
constraint rather than exploiting its underlying properties. In this paper, we
demonstrate that in $q$-metric spaces, metric trees can leverage a stronger
version of the triangle inequality to reduce comparisons for exact search.
Notably, as $q$ approaches infinity, the search complexity becomes logarithmic.
Therefore, we propose a novel projection method that embeds vector datasets
with arbitrary dissimilarity measures into $q$-metric spaces while preserving
the nearest neighbor. We propose to learn an approximation of this projection
to efficiently transform query points to a space where euclidean distances
satisfy the desired properties. Our experimental results with text and image
vector embeddings show that learning $q$-metric approximations enables classic
metric tree algorithms -- which typically underperform with high-dimensional
data -- to achieve competitive performance against state-of-the-art search
methods.

</details>


### [108] [OneSug: The Unified End-to-End Generative Framework for E-commerce Query Suggestion](https://arxiv.org/abs/2506.06913)
*Xian Guo,Ben Chen,Siyuan Wang,Ying Yang,Chenyi Lei,Yuqing Ding,Han Li*

Main category: cs.IR

TL;DR: 本文提出端到端生成框架OneSug用于电商查询建议，在大规模数据集评估有效，快手平台部署效果好。


<details>
  <summary>Details</summary>
Motivation: 传统查询建议模块采用多阶段级联架构，各阶段优化目标不一致，存在效率低和性能不佳问题，需改进。

Method: 提出OneSug框架，包含prefix2query表示增强模块、编解码器生成模型和奖励加权排序策略。

Result: 在大规模行业数据集评估中，OneSug能有效高效进行查询建议；在快手电商搜索引擎全量流量部署超1个月，用户点击位置、CTR、订单和收入等指标有显著提升。

Conclusion: OneSug框架在电商查询建议上有良好效果，有很大电商转化潜力。

Abstract: Query suggestion plays a crucial role in enhancing user experience in
e-commerce search systems by providing relevant query recommendations that
align with users' initial input. This module helps users navigate towards
personalized preference needs and reduces typing effort, thereby improving
search experience. Traditional query suggestion modules usually adopt
multi-stage cascading architectures, for making a well trade-off between system
response time and business conversion. But they often suffer from
inefficiencies and suboptimal performance due to inconsistent optimization
objectives across stages. To address these, we propose OneSug, the first
end-to-end generative framework for e-commerce query suggestion. OneSug
incorporates a prefix2query representation enhancement module to enrich
prefixes using semantically and interactively related queries to bridge content
and business characteristics, an encoder-decoder generative model that unifies
the query suggestion process, and a reward-weighted ranking strategy with
behavior-level weights to capture fine-grained user preferences. Extensive
evaluations on large-scale industry datasets demonstrate OneSug's ability for
effective and efficient query suggestion. Furthermore, OneSug has been
successfully deployed for the entire traffic on the e-commerce search engine in
Kuaishou platform for over 1 month, with statistically significant improvements
in user top click position (-9.33%), CTR (+2.01%), Order (+2.04%), and Revenue
(+1.69%) over the online multi-stage strategy, showing great potential in
e-commercial conversion.

</details>


### [109] [Correcting for Position Bias in Learning to Rank: A Control Function Approach](https://arxiv.org/abs/2506.06989)
*Md Aminul Islam,Kathryn Vasilaky,Elena Zheleva*

Main category: cs.IR

TL;DR: 本文提出基于控制函数的方法解决隐式反馈数据位置偏差，无需点击或倾向模型知识，可对任意先进排序算法去偏，实验表明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 隐式反馈数据易收集且反映用户偏好，但存在位置偏差，直接用有偏差数据训练排序系统会导致性能不佳。

Method: 提出两阶段基于控制函数的方法，第一阶段利用排序过程残差的外生变化，第二阶段修正点击方程中的位置偏差；引入技术对验证点击去偏以调参选最优模型。

Result: 实验结果显示该方法在纠正位置偏差方面优于现有方法。

Conclusion: 所提方法无需点击或倾向模型知识，允许底层排序模型非线性，可对先进排序算法去偏，能有效解决位置偏差问题。

Abstract: Implicit feedback data, such as user clicks, is commonly used in
learning-to-rank (LTR) systems because it is easy to collect and it often
reflects user preferences. However, this data is prone to various biases, and
training an LTR system directly on biased data can result in suboptimal ranking
performance. One of the most prominent and well-studied biases in implicit
feedback data is position bias, which occurs because users are more likely to
interact with higher-ranked documents regardless of their true relevance. In
this paper, we propose a novel control function-based method that accounts for
position bias in a two-stage process. The first stage uses exogenous variation
from the residuals of the ranking process to correct for position bias in the
second stage click equation. Unlike previous position bias correction methods,
our method does not require knowledge of the click or propensity model and
allows for nonlinearity in the underlying ranking model. Moreover, our method
is general and allows for debiasing any state-of-the-art ranking algorithm by
plugging it into the second stage. We also introduce a technique to debias
validation clicks for hyperparameter tuning to select the optimal model in the
absence of unbiased validation data. Experimental results demonstrate that our
method outperforms state-of-the-art approaches in correcting for position bias.

</details>


### [110] [RADAR: Recall Augmentation through Deferred Asynchronous Retrieval](https://arxiv.org/abs/2506.07261)
*Amit Jaspal,Qian Dang,Ajantha Ramineni*

Main category: cs.IR

TL;DR: 介绍了RADAR框架，利用异步离线计算提升推荐召回率和用户参与度。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统检索阶段难以从大规模商品目录中找出最具吸引力的商品，需要提升推荐质量。

Method: 引入RADAR框架，利用异步离线计算，用全复杂度排序模型对大量候选集预排序，存储并在在线推理时作为高质量检索源。

Result: 离线实验中召回率显著提升，在线A/B测试中用户参与度指标提升0.8%。

Conclusion: RADAR是在严格在线服务约束下提高推荐质量的实用有效方法。

Abstract: Modern large-scale recommender systems employ multi-stage ranking funnel
(Retrieval, Pre-ranking, Ranking) to balance engagement and computational
constraints (latency, CPU). However, the initial retrieval stage, often relying
on efficient but less precise methods like K-Nearest Neighbors (KNN), struggles
to effectively surface the most engaging items from billion-scale catalogs,
particularly distinguishing highly relevant and engaging candidates from merely
relevant ones. We introduce Recall Augmentation through Deferred Asynchronous
Retrieval (RADAR), a novel framework that leverages asynchronous, offline
computation to pre-rank a significantly larger candidate set for users using
the full complexity ranking model. These top-ranked items are stored and
utilized as a high-quality retrieval source during online inference, bypassing
online retrieval and pre-ranking stages for these candidates. We demonstrate
through offline experiments that RADAR significantly boosts recall (2X
Recall@200 vs DNN retrieval baseline) by effectively combining a larger
retrieved candidate set with a more powerful ranking model. Online A/B tests
confirm a +0.8% lift in topline engagement metrics, validating RADAR as a
practical and effective method to improve recommendation quality under strict
online serving constraints.

</details>


### [111] [Research Knowledge Graphs: the Shifting Paradigm of Scholarly Information Representation](https://arxiv.org/abs/2506.07285)
*Matthäus Zloch,Danilo Dessì,Jennifer D'Souza,Leyla Jael Castro,Benjamin Zapilko,Saurav Karmakar,Brigitte Mathiak,Markus Stocker,Wolfgang Otto,Sören Auer,Stefan Dietze*

Main category: cs.IR

TL;DR: 本文首次对研究知识图谱（RKG）愿景进行概念化，对在用RKG分类，介绍构建模块和原则，调查不同RKG实现，描述构建方法并展望其应用、机会和挑战。


<details>
  <summary>Details</summary>
Motivation: 研究成果共享和复用面临资源和元数据异构、信息非结构化等挑战，RKG概念应运而生以提供易使用和机器可操作的研究成果表示。

Method: 对RKG愿景概念化，对在用RKG分类，调查不同RKG实现，分析构建方法。

Result: 完成RKG愿景概念化、分类，调查不同RKG实现情况。

Conclusion: 给出RKG愿景相关应用、机会和挑战的前瞻性观点。

Abstract: Sharing and reusing research artifacts, such as datasets, publications, or
methods is a fundamental part of scientific activity, where heterogeneity of
resources and metadata and the common practice of capturing information in
unstructured publications pose crucial challenges. Reproducibility of research
and finding state-of-the-art methods or data have become increasingly
challenging. In this context, the concept of Research Knowledge Graphs (RKGs)
has emerged, aiming at providing an easy to use and machine-actionable
representation of research artifacts and their relations. That is facilitated
through the use of established principles for data representation, the
consistent adoption of globally unique persistent identifiers and the reuse and
linking of vocabularies and data. This paper provides the first
conceptualisation of the RKG vision, a categorisation of in-use RKGs together
with a description of RKG building blocks and principles. We also survey
real-world RKG implementations differing with respect to scale, schema, data,
used vocabulary, and reliability of the contained data. We also characterise
different RKG construction methodologies and provide a forward-looking
perspective on the diverse applications, opportunities, and challenges
associated with the RKG vision.

</details>


### [112] [HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval](https://arxiv.org/abs/2506.07296)
*Arian Askari,Emmanouil Stergiadis,Ilya Gusev,Moran Beladev*

Main category: cs.IR

TL;DR: 提出用于旅游领域的多模态密集检索模型HotelMatch - LLM，通过三项创新提升性能，实验显示其显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统旅游搜索引擎需用户先确定目的地并编辑搜索参数的局限，实现自然语言的酒店搜索。

Method: 采用特定领域多任务优化，结合小型和大型语言模型的非对称密集检索架构，以及广泛的图像处理。

Result: 在四个不同测试集上，HotelMatch - LLM显著优于VISTA和MARVEL等模型，在主查询类型测试集上得分0.681，而最有效基线MARVEL为0.603。

Conclusion: 多任务优化有显著影响，HotelMatch - LLM跨LLM架构有泛化性，且能处理大型图像库。

Abstract: We present HotelMatch-LLM, a multimodal dense retrieval model for the travel
domain that enables natural language property search, addressing the
limitations of traditional travel search engines which require users to start
with a destination and editing search parameters. HotelMatch-LLM features three
key innovations: (1) Domain-specific multi-task optimization with three novel
retrieval, visual, and language modeling objectives; (2) Asymmetrical dense
retrieval architecture combining a small language model (SLM) for efficient
online query processing and a large language model (LLM) for embedding hotel
data; and (3) Extensive image processing to handle all property image
galleries. Experiments on four diverse test sets show HotelMatch-LLM
significantly outperforms state-of-the-art models, including VISTA and MARVEL.
Specifically, on the test set -- main query type -- we achieve 0.681 for
HotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our
analysis highlights the impact of our multi-task optimization, the
generalizability of HotelMatch-LLM across LLM architectures, and its
scalability for processing large image galleries.

</details>


### [113] [LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework for LLM-Based Ranking](https://arxiv.org/abs/2506.07449)
*Vahid Azizi,Fatemeh Koochaki*

Main category: cs.IR

TL;DR: 提出LlamaRec - LKG - RAG框架，将个性化知识图谱上下文集成到基于大语言模型的推荐排序中，实验显示效果优于LlamaRec。


<details>
  <summary>Details</summary>
Motivation: 现有基于RAG的推荐系统未能利用用户 - 项目交互中的丰富关系结构。

Method: 引入LlamaRec - LKG - RAG框架，扩展LlamaRec架构，融入轻量级用户偏好模块，动态识别异质知识图中的显著关系路径，将个性化子图集成到微调的Llama - 2模型提示中。

Result: 在ML - 100K和Amazon Beauty数据集上的实验表明，在关键排名指标上比LlamaRec有显著且一致的改进。

Conclusion: LlamaRec - LKG - RAG证明了结构化推理在基于大语言模型的推荐中的重要价值，为下一代推荐系统的可扩展、知识感知的个性化奠定了基础。

Abstract: Recent advances in Large Language Models (LLMs) have driven their adoption in
recommender systems through Retrieval-Augmented Generation (RAG) frameworks.
However, existing RAG approaches predominantly rely on flat, similarity-based
retrieval that fails to leverage the rich relational structure inherent in
user-item interactions. We introduce LlamaRec-LKG-RAG, a novel single-pass,
end-to-end trainable framework that integrates personalized knowledge graph
context into LLM-based recommendation ranking. Our approach extends the
LlamaRec architecture by incorporating a lightweight user preference module
that dynamically identifies salient relation paths within a heterogeneous
knowledge graph constructed from user behavior and item metadata. These
personalized subgraphs are seamlessly integrated into prompts for a fine-tuned
Llama-2 model, enabling efficient and interpretable recommendations through a
unified inference step. Comprehensive experiments on ML-100K and Amazon Beauty
datasets demonstrate consistent and significant improvements over LlamaRec
across key ranking metrics (MRR, NDCG, Recall). LlamaRec-LKG-RAG demonstrates
the critical value of structured reasoning in LLM-based recommendations and
establishes a foundation for scalable, knowledge-aware personalization in
next-generation recommender systems. Code is available
at~\href{https://github.com/VahidAz/LlamaRec-LKG-RAG}{repository}.

</details>


### [114] [Leveraging Historical and Current Interests for Continual Sequential Recommendation](https://arxiv.org/abs/2506.07466)
*Gyuseok Lee,Hyunsik Yoo,Junyoung Hwang,SeongKu Kang,Hwanjo Yu*

Main category: cs.IR

TL;DR: 提出用于推荐的持续顺序Transformer（CSTRec），有效结合历史与当前用户兴趣，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决基于Transformer的顺序推荐模型在非平稳数据流上更新时计算成本高和灾难性遗忘问题。

Method: 提出核心的持续顺序注意力（CSA）机制，包含柯西 - 施瓦茨归一化和协作兴趣丰富组件，还引入从相似用户转移知识的冷启动学习技术。

Result: 在三个真实数据集上的实验表明，CSTRec在知识保留和获取方面优于现有基线。

Conclusion: CSTRec能有效利用历史用户兴趣并捕捉当前兴趣，是一种有效的顺序推荐模型。

Abstract: Sequential recommendation models based on the Transformer architecture show
superior performance in harnessing long-range dependencies within user behavior
via self-attention. However, naively updating them on continuously arriving
non-stationary data streams incurs prohibitive computation costs or leads to
catastrophic forgetting. To address this, we propose Continual Sequential
Transformer for Recommendation (CSTRec) that effectively leverages
well-preserved historical user interests while capturing current interests. At
its core is Continual Sequential Attention (CSA), a linear attention mechanism
that retains past knowledge without direct access to old data. CSA integrates
two key components: (1) Cauchy-Schwarz Normalization that stabilizes training
under uneven interaction frequencies, and (2) Collaborative Interest Enrichment
that mitigates forgetting through shared, learnable interest pools. We further
introduce a technique that facilitates learning for cold-start users by
transferring historical knowledge from behaviorally similar existing users.
Extensive experiments on three real-world datasets indicate that CSTRec
outperforms state-of-the-art baselines in both knowledge retention and
acquisition.

</details>


### [115] [MoE-MLoRA for Multi-Domain CTR Prediction: Efficient Adaptation with Expert Specialization](https://arxiv.org/abs/2506.07563)
*Ken Yagel,Eyal German,Aviel Ben Siman Tov*

Main category: cs.IR

TL;DR: 提出MoE - MLoRA框架用于多域推荐系统，在大规模动态数据集上表现好，大集成不一定提升性能，强调基于专家架构潜力。


<details>
  <summary>Details</summary>
Motivation: 传统方法如MLoRA处理多域用户交互缺乏灵活性，需新方法适应不同域用户交互。

Method: 提出MoE - MLoRA，各专家先独立训练，再用门控网络动态加权。

Result: 在Movielens和Taobao的八个CTR模型上评估，在大规模动态数据集上性能提升，在低域多样性和稀疏结构化数据集上收益有限，大集成不总提升性能。

Conclusion: 基于专家的架构用于多域推荐系统有潜力，任务感知专业化和自适应门控可提升复杂环境预测准确性。

Abstract: Personalized recommendation systems must adapt to user interactions across
different domains. Traditional approaches like MLoRA apply a single adaptation
per domain but lack flexibility in handling diverse user behaviors. To address
this, we propose MoE-MLoRA, a mixture-of-experts framework where each expert is
first trained independently to specialize in its domain before a gating network
is trained to weight their contributions dynamically. We evaluate MoE-MLoRA
across eight CTR models on Movielens and Taobao, showing that it improves
performance in large-scale, dynamic datasets (+1.45 Weighed-AUC in Taobao-20)
but offers limited benefits in structured datasets with low domain diversity
and sparsity. Further analysis of the number of experts per domain reveals that
larger ensembles do not always improve performance, indicating the need for
model-aware tuning. Our findings highlight the potential of expert-based
architectures for multi-domain recommendation systems, demonstrating that
task-aware specialization and adaptive gating can enhance predictive accuracy
in complex environments. The implementation and code are available in our
GitHub repository.

</details>


### [116] [GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval](https://arxiv.org/abs/2506.04762)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.IR

TL;DR: 介绍了一种名为GOLFer的新方法，利用较小的开源语言模型进行查询扩展，实验显示其有效。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的查询扩展存在依赖大模型、成本高、计算量大和可访问性有限等问题，需要改进。

Method: 引入GOLFer方法，包含幻觉过滤器和文档组合器两个模块，分别处理生成文档中的问题和结合过滤内容与查询。

Result: 在多个数据集上评估，GOLFer用较小语言模型始终优于其他方法，与使用大语言模型的方法相比也有竞争力。

Conclusion: GOLFer方法有效。

Abstract: Large language models (LLMs)-based query expansion for information retrieval
augments queries with generated hypothetical documents with LLMs. However, its
performance relies heavily on the scale of the language models (LMs),
necessitating larger, more advanced LLMs. This approach is costly,
computationally intensive, and often has limited accessibility. To address
these limitations, we introduce GOLFer - Smaller LMs-Generated Documents
Hallucination Filter & Combiner - a novel method leveraging smaller open-source
LMs for query expansion. GOLFer comprises two modules: a hallucination filter
and a documents combiner. The former detects and removes non-factual and
inconsistent sentences in generated documents, a common issue with smaller LMs,
while the latter combines the filtered content with the query using a weight
vector to balance their influence. We evaluate GOLFer alongside dominant
LLM-based query expansion methods on three web search and ten low-resource
datasets. Experimental results demonstrate that GOLFer consistently outperforms
other methods using smaller LMs, and maintains competitive performance against
methods using large-size LLMs, demonstrating its effectiveness.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [117] [CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning](https://arxiv.org/abs/2506.06290)
*Mingyu Lu,Ethan Weinberger,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: 介绍用于高内涵筛选（HCS）数据的跨模态对比学习框架CellCLIP，性能优于现有模型，还减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 利用跨模态对比学习统一潜在空间，使扰动与细胞形态效应对齐，但HCS数据应用存在困难，需新方法。

Method: 提出CellCLIP框架，利用预训练图像编码器和新通道编码方案处理图像，用自然语言编码器表示扰动。

Result: CellCLIP框架在跨模态检索和有生物学意义的下游任务中表现最佳，且显著减少计算时间。

Conclusion: CellCLIP框架有效解决HCS数据跨模态对比学习难题，性能良好。

Abstract: High-content screening (HCS) assays based on high-throughput microscopy
techniques such as Cell Painting have enabled the interrogation of cells'
morphological responses to perturbations at an unprecedented scale. The
collection of such data promises to facilitate a better understanding of the
relationships between different perturbations and their effects on cellular
state. Towards achieving this goal, recent advances in cross-modal contrastive
learning could, in theory, be leveraged to learn a unified latent space that
aligns perturbations with their corresponding morphological effects. However,
the application of such methods to HCS data is not straightforward due to
substantial differences in the semantics of Cell Painting images compared to
natural images, and the difficulty of representing different classes of
perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent
space. In response to these challenges, here we introduce CellCLIP, a
cross-modal contrastive learning framework for HCS data. CellCLIP leverages
pre-trained image encoders coupled with a novel channel encoding scheme to
better capture relationships between different microscopy channels in image
embeddings, along with natural language encoders for representing
perturbations. Our framework outperforms current open-source models,
demonstrating the best performance in both cross-modal retrieval and
biologically meaningful downstream tasks while also achieving significant
reductions in computation time.

</details>


### [118] [Improvement of Optimization using Learning Based Models in Mixed Integer Linear Programming Tasks](https://arxiv.org/abs/2506.06291)
*Xiaoke Wang,Batuhan Altundas,Zhaoxin Li,Aaron Zhao,Matthew Gombolay*

Main category: cs.LG

TL;DR: 提出基于学习的框架，用BC和RL训练GNN为MILP求解器生成初始解，实验显示能减少优化时间和方差。


<details>
  <summary>Details</summary>
Motivation: MILP在大规模实时场景中计算时间长，限制其广泛应用。

Method: 使用基于学习的框架，利用BC和RL训练GNN，为MILP求解器生成高质量初始解。

Result: 与传统技术相比，减少了优化时间和方差，同时保持了解的质量和可行性。

Conclusion: 该方法可有效解决MILP在大规模实时场景中计算时间长的问题。

Abstract: Mixed Integer Linear Programs (MILPs) are essential tools for solving
planning and scheduling problems across critical industries such as
construction, manufacturing, and logistics. However, their widespread adoption
is limited by long computational times, especially in large-scale, real-time
scenarios. To address this, we present a learning-based framework that
leverages Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph
Neural Networks (GNNs), producing high-quality initial solutions for
warm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling
Problems. Experimental results demonstrate that our method reduces optimization
time and variance compared to traditional techniques while maintaining solution
quality and feasibility.

</details>


### [119] [Mutual-Taught for Co-adapting Policy and Reward Models](https://arxiv.org/abs/2506.06292)
*Tianyuan Shi,Canbin Huang,Fanqi Wan,Longguang Zhong,Ziyi Yang,Weizhou Shen,Xiaojun Quan,Ming Yan*

Main category: cs.LG

TL;DR: 论文提出Mutual - Taught方法解决大语言模型偏好优化中模型样本与奖励模型训练数据分布偏移问题，迭代提升策略模型和奖励模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型偏好优化时，新生成模型样本与奖励模型训练数据存在分布偏移，降低奖励模型效果，影响策略模型性能。

Method: 提出Mutual - Taught自训练方法，类似期望最大化（EM）算法，E步用当前奖励模型反馈更新策略模型，M步用E步更新前后策略模型输出构建数据更新奖励模型。

Result: 迭代方法使两个模型持续改进，8B策略模型LLaMA - 3 - 8B - Instruct - MT在AlpacaEval - 2上长度控制胜率达54.1%，8B奖励模型FsfairX - LLaMA3 - RM - MT在RewardBench上表现与GPT - 4o - 2024 - 08 - 06相当。

Conclusion: 所提Mutual - Taught迭代方法能有效提升策略模型和奖励模型性能。

Abstract: During the preference optimization of large language models (LLMs),
distribution shifts may arise between newly generated model samples and the
data used to train the reward model (RM). This shift reduces the efficacy of
the RM, which in turn negatively impacts the performance of the policy model
(PM). To address this challenge, we propose Mutual-Taught, a self-training
method that iteratively improves both the PM and RM without requiring
additional human annotation. Our approach mirrors the expectation-maximization
(EM) algorithm. In the E-step, the PM is updated using feedback from the
current RM, guiding the PM toward a better approximation of the latent optimal
preference distribution. In the M-step, we update the RM by constructing
training data from the outputs of the PM before and after the E-step update.
This process ensures that the RM adapts to the evolving policy distribution.
Experimental results demonstrate that this iterative approach leads to
consistent improvements in both models. Specifically, our 8B policy model,
LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\% on
AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par
with GPT-4o-2024-08-06 on RewardBench.

</details>


### [120] [Prediction of Bank Credit Ratings using Heterogeneous Topological Graph Neural Networks](https://arxiv.org/abs/2506.06293)
*Junyi Liu,Stanley Kok*

Main category: cs.LG

TL;DR: 研究利用持久同调构建网络，结合传统借贷网络创建异质网络用于银行信用评级预测，实验验证了HTGNN有效性。


<details>
  <summary>Details</summary>
Motivation: 准确及时的银行信用评级预测对利益相关者决策等很重要，但因隐私问题难以直接用GNN进行评级预测。

Method: 利用持久同调构建网络，与传统借贷网络结合形成异质网络进行评级预测。

Result: 在全球真实数据集上的实验验证了HTGNN的有效性。

Conclusion: 研究对投资者和监管机构增强主动风险缓解和实施有效市场干预有意义。

Abstract: Agencies such as Standard & Poor's and Moody's provide bank credit ratings
that influence economic stability and decision-making by stakeholders. Accurate
and timely predictions support informed decision-making, regulatory actions,
and investor protection. However, a complete interbank connection graph is
often unavailable due to privacy concerns, complicating the direct application
of Graph Neural Networks (GNNs) for rating prediction. our research utilizes
persistent homology to construct a network that captures relationships among
banks and combines this with a traditional lending network to create a
heterogeneous network that integrates information from both sources, leading to
improved predictions. Experiments on a global, real-world dataset validate the
effectiveness of HTGNN. This research has implications for investors and
regulatory bodies in enhancing proactive risk mitigation and the implementation
of effective market interventions.The code can be find at
https://github.com/Liu-Jun-Yi/HTGNN.

</details>


### [121] [GLProtein: Global-and-Local Structure Aware Protein Representation Learning](https://arxiv.org/abs/2506.06294)
*Yunqing Liu,Wenqi Fan,Xiaoyong Wei,Qing Li*

Main category: cs.LG

TL;DR: 文章提出GLProtein框架用于蛋白质预训练，结合全局结构相似性和局部氨基酸细节，实验显示其在多个生物信息学任务中表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 尽管通过蛋白质序列分析在理解蛋白质功能方面有进展，但在整合蛋白质结构信息上仍有探索空间，需要利用蛋白质从局部到全局的结构信息。

Method: 提出GLProtein框架，创新性地结合蛋白质掩码建模、三元组结构相似性评分、蛋白质3D距离编码和基于子结构的氨基酸分子编码。

Result: GLProtein在预测蛋白质 - 蛋白质相互作用、接触预测等多个生物信息学任务中优于先前方法。

Conclusion: GLProtein框架能有效结合全局结构相似性和局部氨基酸细节，提升预测准确性和功能洞察。

Abstract: Proteins are central to biological systems, participating as building blocks
across all forms of life. Despite advancements in understanding protein
functions through protein sequence analysis, there remains potential for
further exploration in integrating protein structural information. We argue
that the structural information of proteins is not only limited to their 3D
information but also encompasses information from amino acid molecules (local
information) to protein-protein structure similarity (global information). To
address this, we propose \textbf{GLProtein}, the first framework in protein
pre-training that incorporates both global structural similarity and local
amino acid details to enhance prediction accuracy and functional insights.
GLProtein innovatively combines protein-masked modelling with triplet structure
similarity scoring, protein 3D distance encoding and substructure-based amino
acid molecule encoding. Experimental results demonstrate that GLProtein
outperforms previous methods in several bioinformatics tasks, including
predicting protein-protein interaction, contact prediction, and so on.

</details>


### [122] [Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events](https://arxiv.org/abs/2506.06380)
*Jingyi Gu,Xuan Zhang,Guiling Wang*

Main category: cs.LG

TL;DR: 本文首次概述极端事件合成数据生成，回顾技术、介绍评估框架、分析应用领域并指出挑战。


<details>
  <summary>Details</summary>
Motivation: 极端事件数据稀缺，现有调查未关注极端事件独特性能需求，需对极端事件合成数据生成进行概述。

Method: 系统回顾生成建模技术和大语言模型，总结基准数据集，引入评估框架，分析各指标适用性，分类应用领域。

Result: 完成极端事件合成数据生成的概述，分析各指标适用性，分类应用领域并找出未充分探索的领域。

Conclusion: 指出了开放挑战，为推进合成罕见事件研究提供结构化基础。

Abstract: Extreme events, such as market crashes, natural disasters, and pandemics, are
rare but catastrophic, often triggering cascading failures across
interconnected systems. Accurate prediction and early warning can help minimize
losses and improve preparedness. While data-driven methods offer powerful
capabilities for extreme event modeling, they require abundant training data,
yet extreme event data is inherently scarce, creating a fundamental challenge.
Synthetic data generation has emerged as a powerful solution. However, existing
surveys focus on general data with privacy preservation emphasis, rather than
extreme events' unique performance requirements. This survey provides the first
overview of synthetic data generation for extreme events. We systematically
review generative modeling techniques and large language models, particularly
those enhanced by statistical theory as well as specialized training and
sampling mechanisms to capture heavy-tailed distributions. We summarize
benchmark datasets and introduce a tailored evaluation framework covering
statistical, dependence, visual, and task-oriented metrics. A central
contribution is our in-depth analysis of each metric's applicability in
extremeness and domain-specific adaptations, providing actionable guidance for
model evaluation in extreme settings. We categorize key application domains and
identify underexplored areas like behavioral finance, wildfires, earthquakes,
windstorms, and infectious outbreaks. Finally, we outline open challenges,
providing a structured foundation for advancing synthetic rare-event research.

</details>


### [123] [Mind the Gap: Removing the Discretization Gap in Differentiable Logic Gate Networks](https://arxiv.org/abs/2506.07500)
*Shakir Yousefi,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 现代神经网络计算需求和能耗高，逻辑门网络（LGNs）有离散化差距问题，注入Gumbel噪声可加速训练、减小差距。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络计算和能耗问题促使寻找高效方案，LGNs训练慢、存在离散化差距阻碍实际部署。

Method: 训练时注入Gumbel噪声并使用直传估计器。

Result: 训练速度提高4.5倍，离散化差距减小98%，未使用门数量减少100%。

Conclusion: 注入Gumbel噪声可显著改善LGNs的收敛特性，提升训练效率和性能。

Abstract: Modern neural networks demonstrate state-of-the-art performance on numerous
existing benchmarks; however, their high computational requirements and energy
consumption prompt researchers to seek more efficient solutions for real-world
deployment. Logic gate networks (LGNs) learns a large network of logic gates
for efficient image classification. However, learning a network that can solve
a simple problem like CIFAR-10 can take days to weeks to train. Even then,
almost half of the network remains unused, causing a discretization gap. This
discretization gap hinders real-world deployment of LGNs, as the performance
drop between training and inference negatively impacts accuracy. We inject
Gumbel noise with a straight-through estimator during training to significantly
speed up training, improve neuron utilization, and decrease the discretization
gap. We theoretically show that this results from implicit Hessian
regularization, which improves the convergence properties of LGNs. We train
networks $4.5 \times$ faster in wall-clock time, reduce the discretization gap
by $98\%$, and reduce the number of unused gates by $100\%$.

</details>


### [124] [dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching](https://arxiv.org/abs/2506.06295)
*Zhiyuan Liu,Yicun Yang,Yaojie Zhang,Junjie Chen,Chang Zou,Qingyuan Wei,Shaobo Wang,Linfeng Zhang*

Main category: cs.LG

TL;DR: 本文提出dLLM - Cache自适应缓存框架加速扩散式大语言模型推理，实验显示可提升速度且不影响输出质量。


<details>
  <summary>Details</summary>
Motivation: 扩散式大语言模型（dLLMs）推理延迟高，传统ARM加速技术不适用，需新的加速方法。

Method: 提出训练-free的自适应缓存框架dLLM - Cache，结合长间隔提示缓存和基于特征相似度的部分响应更新。

Result: 在LLaDA 8B和Dream 7B等模型上实验，dLLM - Cache相比标准推理加速达9.1倍，不影响输出质量，很多场景下使dLLM推理延迟接近ARMs。

Conclusion: dLLM - Cache能有效加速dLLMs推理，且不影响性能，代码将公开。

Abstract: Autoregressive Models (ARMs) have long dominated the landscape of Large
Language Models. Recently, a new paradigm has emerged in the form of
diffusion-based Large Language Models (dLLMs), which generate text by
iteratively denoising masked segments. This approach has shown significant
advantages and potential. However, dLLMs suffer from high inference latency.
Traditional ARM acceleration techniques, such as Key-Value caching, are
incompatible with dLLMs due to their bidirectional attention mechanism. To
address this specific challenge, our work begins with a key observation that
dLLM inference involves a static prompt and a partially dynamic response, where
most tokens remain stable across adjacent denoising steps. Based on this, we
propose dLLM-Cache, a training-free adaptive caching framework that combines
long-interval prompt caching with partial response updates guided by feature
similarity. This design enables efficient reuse of intermediate computations
without compromising model performance. Extensive experiments on representative
dLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1
x speedup over standard inference without compromising output quality. Notably,
our method brings dLLM inference latency close to that of ARMs under many
settings. Codes are provided in the supplementary material and will be released
publicly on GitHub.

</details>


### [125] [End-to-End Probabilistic Framework for Learning with Hard Constraints](https://arxiv.org/abs/2506.07003)
*Utkarsh Utkarsh,Danielle C. Maddix,Ruijun Ma,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: 提出通用概率预测框架ProbHardE2E，可纳入硬约束并进行不确定性量化，有广泛应用。


<details>
  <summary>Details</summary>
Motivation: 开发能将操作/物理约束作为硬要求纳入的系统，实现不确定性量化。

Method: 使用新颖的可微概率投影层（DPPL），可与多种神经网络架构结合，端到端学习系统，能优化严格适当评分规则。

Result: 可应用于含不确定性估计的偏微分方程学习和概率时间序列预测问题。

Conclusion: ProbHardE2E是一个广泛适用的通用框架，连接了不同领域。

Abstract: We present a general purpose probabilistic forecasting framework,
ProbHardE2E, to learn systems that can incorporate operational/physical
constraints as hard requirements. ProbHardE2E enforces hard constraints by
exploiting variance information in a novel way; and thus it is also capable of
performing uncertainty quantification (UQ) on the model. Our methodology uses a
novel differentiable probabilistic projection layer (DPPL) that can be combined
with a wide range of neural network architectures. This DPPL allows the model
to learn the system in an end-to-end manner, compared to other approaches where
the constraints are satisfied either through a post-processing step or at
inference. In addition, ProbHardE2E can optimize a strictly proper scoring
rule, without making any distributional assumptions on the target, which
enables it to obtain robust distributional estimates (in contrast to existing
approaches that generally optimize likelihood-based objectives, which are
heavily biased by their distributional assumptions and model choices); and it
can incorporate a range of non-linear constraints (increasing the power of
modeling and flexibility). We apply ProbHardE2E to problems in learning partial
differential equations with uncertainty estimates and to probabilistic
time-series forecasting, showcasing it as a broadly applicable general setup
that connects these seemingly disparate domains.

</details>


### [126] [Dynamic Graph CNN with Jacobi Kolmogorov-Arnold Networks for 3D Classification of Point Sets](https://arxiv.org/abs/2506.06296)
*Hanaa El Afia,Said Ohamouddou,Raddouane Chiheb,Abdellatif El Afia*

Main category: cs.LG

TL;DR: 提出Jacobi - KAN - DGCNN框架用于三维点云分类，在实验中表现优于基线，且指出高多项式阶数不一定提升性能。


<details>
  <summary>Details</summary>
Motivation: 开发更有效的三维点云分类方法，解决现有方法的局限。

Method: 将DGCNN与Jacobi KAN集成，用自适应单变量多项式展开替换MLP层，避免深度网络。

Result: 在ModelNet40数据集上，采用Jacobi多项式的KAN层在准确率、收敛速度和参数效率上优于传统基于线性层的DGCNN基线。

Conclusion: 高多项式阶数不一定提升性能，需进一步理论和实证研究理解多项式基、阶数与基于图学习机制的相互作用。

Abstract: We introduce Jacobi-KAN-DGCNN, a framework that integrates Dynamic Graph
Convolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks
(KAN) for the classification of three-dimensional point clouds. This method
replaces Multi-Layer Perceptron (MLP) layers with adaptable univariate
polynomial expansions within a streamlined DGCNN architecture, circumventing
deep levels for both MLP and KAN to facilitate a layer-by-layer comparison. In
comparative experiments on the ModelNet40 dataset, KAN layers employing Jacobi
polynomials outperform the traditional linear layer-based DGCNN baseline in
terms of accuracy and convergence speed, while maintaining parameter
efficiency. Our results demonstrate that higher polynomial degrees do not
automatically improve performance, highlighting the need for further
theoretical and empirical investigation to fully understand the interactions
between polynomial bases, degrees, and the mechanisms of graph-based learning.

</details>


### [127] [ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning](https://arxiv.org/abs/2506.07551)
*Mengsong Wu,YaFei Wang,Yidong Ming,Yuqi An,Yuwei Wan,Wenliang Chen,Binbin Lin,Yuqiang Li,Tong Xie,Dongzhan Zhou*

Main category: cs.LG

TL;DR: 提出基于大语言模型的智能体，整合化学工具和数据集，用HE - MCTS框架优化，在化学问答和发现任务中表现好。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在化学任务中因预训练知识过时和难融入专业化学知识而面临的挑战。

Method: 提出基于大语言模型的智能体，整合137个外部化学工具；构建数据集整理流程生成ChemToolBench数据集；引入HE - MCTS框架；利用自生成数据进行策略模型的分步微调及训练任务自适应的PRM和ORM。

Result: 实验评估显示，该方法在化学问答和发现任务中显著提升性能。

Conclusion: 该方法为大语言模型整合专业工具用于高级化学应用提供了可靠解决方案。

Abstract: Large language models (LLMs) have recently demonstrated promising
capabilities in chemistry tasks while still facing challenges due to outdated
pretraining knowledge and the difficulty of incorporating specialized chemical
expertise. To address these issues, we propose an LLM-based agent that
synergistically integrates 137 external chemical tools created ranging from
basic information retrieval to complex reaction predictions, and a dataset
curation pipeline to generate the dataset ChemToolBench that facilitates both
effective tool selection and precise parameter filling during fine-tuning and
evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search
(HE-MCTS) framework, enabling independent optimization of tool planning and
execution. By leveraging self-generated data, our approach supports step-level
fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM
that surpass GPT-4o. Experimental evaluations demonstrate that our approach
significantly improves performance in Chemistry QA and discovery tasks,
offering a robust solution to integrate specialized tools with LLMs for
advanced chemical applications. All datasets and code are available at
https://github.com/AI4Chem/ChemistryAgent .

</details>


### [128] [Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques](https://arxiv.org/abs/2506.06579)
*Adarsh Prasad Behera,Jaya Prakash Champati,Roberto Morabito,Sasu Tarkoma,James Gross*

Main category: cs.LG

TL;DR: 语言模型推理计算和能源消耗大，本文探讨高效推理策略并分析、讨论挑战与方向


<details>
  <summary>Details</summary>
Motivation: 语言模型推理计算成本高、能耗大，难以在资源受限环境部署

Method: 研究路由和级联或分层推理两种高效推理策略，并进行对比分析

Result: 对策略进行了关键性能指标对比分析，讨论了基准测试工作并指出开放挑战

Conclusion: 给出未来研究方向，使基于大语言模型的系统更高效且适用于实际应用

Abstract: Recent progress in Language Models (LMs) has dramatically advanced the field
of natural language processing (NLP), excelling at tasks like text generation,
summarization, and question answering. However, their inference remains
computationally expensive and energy intensive, especially in settings with
limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in
mobile, edge, or cost sensitive environments. To address these challenges,
recent approaches have introduced multi LLM intelligent model selection
strategies that dynamically allocate computational resources based on query
complexity -- using lightweight models for simpler queries and escalating to
larger models only when necessary. This survey explores two complementary
strategies for efficient LLM inference: (i) routing, which selects the most
suitable model based on the query, and (ii) cascading or hierarchical inference
(HI), which escalates queries through a sequence of models until a confident
response is found. Both approaches aim to reduce computation by using
lightweight models for simpler tasks while offloading only when needed. We
provide a comparative analysis of these techniques across key performance
metrics, discuss benchmarking efforts, and outline open challenges. Finally, we
outline future research directions to enable faster response times, adaptive
model selection based on task complexity, and scalable deployment across
heterogeneous environments, making LLM based systems more efficient and
accessible for real world applications.

</details>


### [129] [Optimal patient allocation for echocardiographic assessments](https://arxiv.org/abs/2506.06297)
*Bozhi Sun,Seda Tierney,Jeffrey A. Feinstein,Frederick Damen,Alison L. Marsden,Daniele E. Schiavazzi*

Main category: cs.LG

TL;DR: 文章针对医院超声心动图检查调度难题，通过数据预处理构建离散事件随机模拟模型，比较不同分配策略，用强化学习得出近似最优动态分配策略以提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决医院超声心动图检查调度因不确定因素和资源约束带来的挑战。

Method: 对医院运营数据预处理，构建离散事件随机模拟模型，集成Gymnasium库，比较不同分配策略，应用强化学习推导最优策略。

Result: 即查即做分配策略表现更好，能适应患者变化和资源约束；得出强化学习动态分配策略并与规则策略对比。

Conclusion: 可通过智能、数据驱动的资源管理提升超声实验室效率。

Abstract: Scheduling echocardiographic exams in a hospital presents significant
challenges due to non-deterministic factors (e.g., patient no-shows, patient
arrival times, diverse exam durations, etc.) and asymmetric resource
constraints between fetal and non-fetal patient streams. To address these
challenges, we first conducted extensive pre-processing on one week of
operational data from the Echo Laboratory at Stanford University's Lucile
Packard Children's Hospital, to estimate patient no-show probabilities and
derive empirical distributions of arrival times and exam durations. Based on
these inputs, we developed a discrete-event stochastic simulation model using
SimPy, and integrate it with the open source Gymnasium Python library. As a
baseline for policy optimization, we developed a comparative framework to
evaluate on-the-fly versus reservation-based allocation strategies, in which
different proportions of resources are reserved in advance. Considering a
hospital configuration with a 1:6 ratio of fetal to non-fetal rooms and a 4:2
ratio of fetal to non-fetal sonographers, we show that on-the-fly allocation
generally yields better performance, more effectively adapting to patient
variability and resource constraints. Building on this foundation, we apply
reinforcement learning (RL) to derive an approximated optimal dynamic
allocation policy. This RL-based policy is benchmarked against the
best-performing rule-based strategies, allowing us to quantify their
differences and provide actionable insights for improving echo lab efficiency
through intelligent, data-driven resource management.

</details>


### [130] [Pairwise Calibrated Rewards for Pluralistic Alignment](https://arxiv.org/abs/2506.06298)
*Daniel Halpern,Evi Micha,Ariel D. Procaccia,Itai Shapira*

Main category: cs.LG

TL;DR: 当前对齐流程忽略人类偏好差异，本文提出用多个奖励函数分布反映多元偏好，证明小而无异常值的集合可准确表示偏好分布，引入实用训练启发式方法并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前对齐流程假定单一通用的理想行为概念，忽略了不同用户、情境和文化下人类偏好的差异，导致少数观点被忽视。

Method: 通过多个奖励函数的分布来反映多元人类偏好，直接从成对偏好中学习分布，将注释者的分歧视为信息软标签，以成对校准为核心准则。

Result: 证明了小而无异常值的集合可以准确表示不同的偏好分布，引入并验证了实用的训练启发式方法，通过改进校准证明了其有效性。

Conclusion: 所提出的方法能更忠实地代表多元价值观。

Abstract: Current alignment pipelines presume a single, universal notion of desirable
behavior. However, human preferences often diverge across users, contexts, and
cultures. As a result, disagreement collapses into the majority signal and
minority perspectives are discounted. To address this, we propose reflecting
diverse human preferences through a distribution over multiple reward
functions, each inducing a distinct aligned policy. The distribution is learned
directly from pairwise preference without annotator identifiers or predefined
groups. Instead, annotator disagreements are treated as informative soft
labels. Our central criterion is pairwise calibration: for every pair of
candidate responses, the proportion of reward functions preferring one response
matches the fraction of annotators with that preference. We prove that even a
small outlier-free ensemble can accurately represent diverse preference
distributions. Empirically, we introduce and validate a practical training
heuristic to learn such ensembles, and demonstrate its effectiveness through
improved calibration, implying a more faithful representation of pluralistic
values.

</details>


### [131] [FedCGD: Collective Gradient Divergence Optimized Scheduling for Wireless Federated Learning](https://arxiv.org/abs/2506.07581)
*Tan Chen,Jintao Yan,Yuxuan Sun,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: 本文证明联邦学习收敛速度受设备级和样本级集体梯度散度（CGD）之和影响，提出FedCGD算法最小化多级CGD，仿真显示能提升分类准确率并减少调度设备。


<details>
  <summary>Details</summary>
Motivation: 现有研究多将数据异质性视为单个设备属性，本文旨在探究集体梯度散度对联邦学习收敛速度的影响并优化。

Method: 证明收敛速度受多级CGD影响，将分类问题转化为加权地球移动距离（WEMD），提出FedCGD算法平衡WEMD和采样方差。

Result: 在CIFAR - 10数据集上，分类准确率最多提升4.2%，调度设备减少41.8%，能灵活切换减少WEMD和采样方差。

Conclusion: FedCGD算法能有效解决无线联邦学习中数据异质性和带宽有限问题，提升性能。

Abstract: Federated learning (FL) is a promising paradigm for multiple devices to
cooperatively train a model. When applied in wireless networks, two issues
consistently affect the performance of FL, i.e., data heterogeneity of devices
and limited bandwidth. Many papers have investigated device scheduling
strategies considering the two issues. However, most of them recognize data
heterogeneity as a property of individual devices. In this paper, we prove that
the convergence speed of FL is affected by the sum of device-level and
sample-level collective gradient divergence (CGD). The device-level CGD refers
to the gradient divergence of the scheduled device group, instead of the sum of
the individual device divergence. The sample-level CGD is statistically upper
bounded by sampling variance, which is inversely proportional to the total
number of samples scheduled for local update. To derive a tractable form of the
device-level CGD, we further consider a classification problem and transform it
into the weighted earth moving distance (WEMD) between the group distribution
and the global distribution. Then we propose FedCGD algorithm to minimize the
sum of multi-level CGDs by balancing WEMD and sampling variance, within
polynomial time. Simulation shows that the proposed strategy increases
classification accuracy on the CIFAR-10 dataset by up to 4.2\% while scheduling
41.8\% fewer devices, and flexibly switches between reducing WEMD and reducing
sampling variance.

</details>


### [132] [LT-PINN: Lagrangian Topology-conscious Physics-informed Neural Network for Boundary-focused Engineering Optimization](https://arxiv.org/abs/2506.06300)
*Yuanye Zhou,Zhaokun Wang,Kai Zhou,Hui Tang,Xiaofan Li*

Main category: cs.LG

TL;DR: 提出Lagrangian拓扑感知PINNs（LT - PINNs）框架用于边界工程优化，经PDE验证其准确性和鲁棒性，在流问题上展示有效性及工程应用潜力，相比DT - PINNs有优势。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs依赖基于密度的拓扑描述，需手动插值且不适用于复杂几何，为解决此问题提出LT - PINNs。

Method: 将拓扑边界曲线控制变量参数化为可学习参数，引入专门的边界条件损失函数和拓扑损失函数。

Result: 在两类PDE及复杂流问题上验证，LT - PINNs相对L2误差显著降低，能处理任意边界条件，无需手动插值推断清晰拓扑边界。

Conclusion: LT - PINNs是一种有效的边界工程优化框架，适用于多种PDE，在复杂拓扑处理上有优势。

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful meshless
tool for topology optimization, capable of simultaneously determining optimal
topologies and physical solutions. However, conventional PINNs rely on
density-based topology descriptions, which necessitate manual interpolation and
limit their applicability to complex geometries. To address this, we propose
Lagrangian topology-conscious PINNs (LT-PINNs), a novel framework for
boundary-focused engineering optimization. By parameterizing the control
variables of topology boundary curves as learnable parameters, LT-PINNs
eliminate the need for manual interpolation and enable precise boundary
determination. We further introduce specialized boundary condition loss
function and topology loss function to ensure sharp and accurate boundary
representations, even for intricate topologies. The accuracy and robustness of
LT-PINNs are validated via two types of partial differential equations (PDEs),
including elastic equation with Dirichlet boundary conditions and Laplace's
equation with Neumann boundary conditions. Furthermore, we demonstrate
effectiveness of LT-PINNs on more complex time-dependent and time-independent
flow problems without relying on measurement data, and showcase their
engineering application potential in flow velocity rearrangement, transforming
a uniform upstream velocity into a sine-shaped downstream profile. The results
demonstrate (1) LT-PINNs achieve substantial reductions in relative L2 errors
compared with the state-of-art density topology-oriented PINNs (DT-PINNs), (2)
LT-PINNs can handle arbitrary boundary conditions, making them suitable for a
wide range of PDEs, and (3) LT-PINNs can infer clear topology boundaries
without manual interpolation, especially for complex topologies.

</details>


### [133] [Rapid training of Hamiltonian graph networks without gradient descent](https://arxiv.org/abs/2506.06558)
*Atamert Rahma,Chinmay Datar,Ana Cukarska,Felix Dietrich*

Main category: cs.LG

TL;DR: 提出用随机特征参数构建法训练HGN，比迭代优化快600倍且精度相当，模型泛化性好，挑战传统优化算法主导地位。


<details>
  <summary>Details</summary>
Motivation: 传统迭代梯度优化算法训练图神经网络慢，尤其是大型复杂系统，需新方法解决。

Method: 用随机特征参数构建替代迭代优化训练Hamiltonian Graph Networks (HGN)。

Result: HGN训练速度比15种不同优化器快达600倍且精度相当，在多样模拟中表现好，模型有良好泛化性。

Conclusion: 挑战了基于迭代梯度下降优化算法在物理系统神经网络模型训练中的主导地位。

Abstract: Learning dynamical systems that respect physical symmetries and constraints
remains a fundamental challenge in data-driven modeling. Integrating physical
laws with graph neural networks facilitates principled modeling of complex
N-body dynamics and yields accurate and permutation-invariant models. However,
training graph neural networks with iterative, gradient-based optimization
algorithms (e.g., Adam, RMSProp, LBFGS) often leads to slow training,
especially for large, complex systems. In comparison to 15 different
optimizers, we demonstrate that Hamiltonian Graph Networks (HGN) can be trained
up to 600x faster--but with comparable accuracy--by replacing iterative
optimization with random feature-based parameter construction. We show robust
performance in diverse simulations, including N-body mass-spring systems in up
to 3 dimensions with different geometries, while retaining essential physical
invariances with respect to permutation, rotation, and translation. We reveal
that even when trained on minimal 8-node systems, the model can generalize in a
zero-shot manner to systems as large as 4096 nodes without retraining. Our work
challenges the dominance of iterative gradient-descent-based optimization
algorithms for training neural network models for physical systems.

</details>


### [134] [Reward Is Enough: LLMs Are In-Context Reinforcement Learners](https://arxiv.org/abs/2506.06303)
*Kefan Song,Amir Moeini,Peng Wang,Lei Gong,Rohan Chandra,Yanjun Qi,Shangtong Zhang*

Main category: cs.LG

TL;DR: 本文发现大语言模型推理时会出现上下文强化学习（ICRL）现象，提出ICRL提示框架，通过多轮提示和反馈提升LLM响应质量，在多个基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型推理时是否会出现强化学习现象，并提升其完成任务的性能。

Method: 提出ICRL提示框架，多轮提示LLM完成任务，每轮给出数值奖励反馈，下一轮结合之前响应和奖励再次提示。

Result: LLM响应质量随上下文增加而提升，在三个基准测试中显著优于Self - Refine和Reflexion等基线方法，甚至用LLM自身生成奖励信号也有性能提升。

Conclusion: ICRL提示框架为提升测试时计算能力提供了有前景的范式。

Abstract: Reinforcement learning (RL) is a human-designed framework for solving
sequential decision making problems. In this work, we demonstrate that,
surprisingly, RL emerges in LLM's (Large Language Model) inference time -- a
phenomenon known as in-context RL (ICRL). Specifically, we propose a novel
multi-round prompting framework called ICRL prompting. The goal is to prompt
the LLM to complete a task. After the LLM generates a response at the current
round, we give numerical scalar feedbacks for the response, called the rewards.
At the next round, we prompt the LLM again with the same task and a context
consisting of all previous responses and rewards. We observe that the quality
of the LLM's response increases as the context grows. In other words, the LLM
is able to maximize the scalar reward signal in the inference time, just like
an RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24,
creative writing, and ScienceWorld) and demonstrate significant performance
improvements over baseline methods such as Self-Refine and Reflexion.
Surprisingly, in some experiments the reward signals are generated by the LLM
itself, yet performance improvements are still observed from ICRL prompting,
offering a promising paradigm for scaling test-time compute.

</details>


### [135] [Wine Quality Prediction with Ensemble Trees: A Unified, Leak-Free Comparative Study](https://arxiv.org/abs/2506.06327)
*Zilang Chen*

Main category: cs.LG

TL;DR: 本文对五种集成学习器在葡萄酒数据集上进行基准测试，对比性能并给出推荐模型，为葡萄酒质量预测提供可复现基线。


<details>
  <summary>Details</summary>
Motivation: 葡萄酒质量评估依赖主观且劳动密集的品酒小组，需准确可复现的评估方法。

Method: 使用五种集成学习器，采用无泄漏工作流，包括分层训练测试划分、五折交叉验证、标准化、重采样、超参数搜索和特征选择等。

Result: Gradient Boosting 准确率最高，限制为前五个变量可降维且损失较小，不同模型运行时间差异大。

Conclusion: 推荐 Random Forest 为最具成本效益的生产模型，XGBoost 和 LightGBM 为 GPU 高效替代方案，Gradient Boosting 用于离线基准测试。

Abstract: Accurate and reproducible wine-quality assessment is critical for production
control yet remains dominated by subjective, labour-intensive tasting panels.
We present the first unified benchmark of five ensemble learners (Random
Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost) on the canonical Vinho
Verde red- and white-wine datasets (1,599 and 4,898 instances, 11
physicochemical attributes). Our leakage-free workflow employs an 80:20
stratified train-test split, five-fold StratifiedGroupKFold within the training
set, per-fold standardisation, SMOTE-Tomek resampling, inverse-frequency cost
weighting, Optuna hyper-parameter search (120-200 trials per model) and a
two-stage feature-selection refit. Final scores on untouched test sets are
reported with weighted F1 as the headline metric. Gradient Boosting achieves
the highest accuracy (weighted F1 0.693 +/- 0.028 for red and 0.664 +/- 0.016
for white), followed within three percentage points by Random Forest and
XGBoost. Limiting each model to its five top-ranked variables lowers
dimensionality by 55 percent while reducing weighted F1 by only 2.6 percentage
points for red and 3.0 percentage points for white, indicating that alcohol,
volatile acidity, sulphates, free SO2 and chlorides capture most predictive
signal. Runtime profiling on an EPYC 9K84/H20 node reveals a steep efficiency
gradient: Gradient Boosting averages 12 h per five-fold study, XGBoost and
LightGBM require 2-3 h, CatBoost 1 h, and Random Forest under 50 min. We
therefore recommend Random Forest as the most cost-effective production model,
XGBoost and LightGBM as GPU-efficient alternatives, and Gradient Boosting as
the accuracy ceiling for offline benchmarking. The fully documented pipeline
and metric set provide a reproducible baseline for future work on imbalanced
multi-class wine-quality prediction.

</details>


### [136] [ExplainBench: A Benchmark Framework for Local Model Explanations in Fairness-Critical Applications](https://arxiv.org/abs/2506.06330)
*James Afful*

Main category: cs.LG

TL;DR: 文章介绍开源基准测试套件ExplainBench，用于对局部模型解释进行系统评估，推动可解释机器学习发展。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统应用增多，对可解释和可信模型需求提升，但缺少标准化、可复现的局部解释技术比较评估框架。

Method: 引入ExplainBench，提供统一包装器、集成端到端管道，支持多种评估指标，有图形界面并封装为Python模块。

Result: 在公平性研究常用数据集上展示不同解释方法在共享实验协议下的表现。

Conclusion: ExplainBench推动可解释机器学习方法论基础发展，促进现实AI系统的责任追究。

Abstract: As machine learning systems are increasingly deployed in high-stakes domains
such as criminal justice, finance, and healthcare, the demand for interpretable
and trustworthy models has intensified. Despite the proliferation of local
explanation techniques, including SHAP, LIME, and counterfactual methods, there
exists no standardized, reproducible framework for their comparative
evaluation, particularly in fairness-sensitive settings.
  We introduce ExplainBench, an open-source benchmarking suite for systematic
evaluation of local model explanations across ethically consequential datasets.
ExplainBench provides unified wrappers for popular explanation algorithms,
integrates end-to-end pipelines for model training and explanation generation,
and supports evaluation via fidelity, sparsity, and robustness metrics. The
framework includes a Streamlit-based graphical interface for interactive
exploration and is packaged as a Python module for seamless integration into
research workflows.
  We demonstrate ExplainBench on datasets commonly used in fairness research,
such as COMPAS, UCI Adult Income, and LendingClub, and showcase how different
explanation methods behave under a shared experimental protocol. By enabling
reproducible, comparative analysis of local explanations, ExplainBench advances
the methodological foundations of interpretable machine learning and
facilitates accountability in real-world AI systems.

</details>


### [137] [Extending AALpy with Passive Learning: A Generalized State-Merging Approach](https://arxiv.org/abs/2506.06333)
*Benjamin von Berg,Bernhard K. Aichernig*

Main category: cs.LG

TL;DR: 介绍AALpy库新增被动自动机学习中红 - 蓝框架状态合并的通用实现，降低实现复杂度。


<details>
  <summary>Details</summary>
Motivation: 在AALpy库中添加被动自动机学习领域重要方法的通用实现，方便现有和新算法的实现。

Method: 采用通用内部表示实现红 - 蓝框架，通过定义兼容性标准和评分来定义和执行状态合并算法。

Result: 能以少量代码实现文献中的一些现有状态合并算法。

Conclusion: AALpy新增功能可降低状态合并算法的实现工作量，有助于现有和新算法的实现。

Abstract: AALpy is a well-established open-source automata learning library written in
Python with a focus on active learning of systems with IO behavior. It provides
a wide range of state-of-the-art algorithms for different automaton types
ranging from fully deterministic to probabilistic automata. In this work, we
present the recent addition of a generalized implementation of an important
method from the domain of passive automata learning: state-merging in the
red-blue framework. Using a common internal representation for different
automaton types allows for a general and highly configurable implementation of
the red-blue framework. We describe how to define and execute state-merging
algorithms using AALpy, which reduces the implementation effort for
state-merging algorithms mainly to the definition of compatibility criteria and
scoring. This aids the implementation of both existing and novel algorithms. In
particular, defining some existing state-merging algorithms from the literature
with AALpy only takes a few lines of code.

</details>


### [138] [Towards Universal Offline Black-Box Optimization via Learning Language Model Embeddings](https://arxiv.org/abs/2506.07109)
*Rong-Xi Tan,Ming Chen,Ke Xue,Yao Wang,Yaoyuan Wang,Sheng Fu,Chao Qian*

Main category: cs.LG

TL;DR: 本文探讨通用黑盒优化算法，利用语言模型实现跨领域通用优化，实验验证方法有效，为通用BBO算法铺平道路。


<details>
  <summary>Details</summary>
Motivation: 现有离线黑盒优化方法受限于异构数值空间缺乏统一表示，无法实现跨领域通用优化，而语言模型发展带来新契机。

Method: 讨论多种潜在方法，包括端到端学习框架和优先学习强表示能力的潜在空间，收集开源学术工作中的离线BBO任务和数据进行训练。

Result: 实验证明所提方法具有通用性和有效性。

Conclusion: 统一语言模型先验和学习字符串嵌入空间可克服通用BBO传统障碍，为通用BBO算法奠定基础。

Abstract: The pursuit of universal black-box optimization (BBO) algorithms is a
longstanding goal. However, unlike domains such as language or vision, where
scaling structured data has driven generalization, progress in offline BBO
remains hindered by the lack of unified representations for heterogeneous
numerical spaces. Thus, existing offline BBO approaches are constrained to
single-task and fixed-dimensional settings, failing to achieve cross-domain
universal optimization. Recent advances in language models (LMs) offer a
promising path forward: their embeddings capture latent relationships in a
unifying way, enabling universal optimization across different data types
possible. In this paper, we discuss multiple potential approaches, including an
end-to-end learning framework in the form of next-token prediction, as well as
prioritizing the learning of latent spaces with strong representational
capabilities. To validate the effectiveness of these methods, we collect
offline BBO tasks and data from open-source academic works for training.
Experiments demonstrate the universality and effectiveness of our proposed
methods. Our findings suggest that unifying language model priors and learning
string embedding space can overcome traditional barriers in universal BBO,
paving the way for general-purpose BBO algorithms. The code is provided at
https://github.com/lamda-bbo/universal-offline-bbo.

</details>


### [139] [Optimized Local Updates in Federated Learning via Reinforcement Learning](https://arxiv.org/abs/2506.06337)
*Ali Murad,Bo Hui,Wei-Shinn Ku*

Main category: cs.LG

TL;DR: 提出用深度强化学习（DRL）优化联邦学习客户端训练数据量，缓解非IID数据聚合影响，实验显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中集中式服务器模型聚合在非IID数据下性能下降，且客户端过度训练无益整体性能。

Method: 设计DRL代理，以训练损失变化为奖励信号，根据本地性能确定下一轮本地训练各数据类别的优化权重，学习划分本地训练数据集，最后用全量本地数据提升性能。

Result: 在多个基准数据集和联邦学习框架上取得优越性能。

Conclusion: 所提算法能有效提升联邦学习客户端性能，缓解非IID数据聚合影响。

Abstract: Federated Learning (FL) is a distributed framework for collaborative model
training over large-scale distributed data, enabling higher performance while
maintaining client data privacy. However, the nature of model aggregation at
the centralized server can result in a performance drop in the presence of
non-IID data across different clients. We remark that training a client locally
on more data than necessary does not benefit the overall performance of all
clients. In this paper, we devise a novel framework that leverages a Deep
Reinforcement Learning (DRL) agent to select an optimized amount of data
necessary to train a client model without oversharing information with the
server. Starting without awareness of the client's performance, the DRL agent
utilizes the change in training loss as a reward signal and learns to optimize
the amount of training data necessary for improving the client's performance.
Specifically, after each aggregation round, the DRL algorithm considers the
local performance as the current state and outputs the optimized weights for
each class, in the training data, to be used during the next round of local
training. In doing so, the agent learns a policy that creates an optimized
partition of the local training dataset during the FL rounds. After FL, the
client utilizes the entire local training dataset to further enhance its
performance on its own data distribution, mitigating the non-IID effects of
aggregation. Through extensive experiments, we demonstrate that training FL
clients through our algorithm results in superior performance on multiple
benchmark datasets and FL frameworks. Our code is available at
https://github.com/amuraddd/optimized_client_training.git.

</details>


### [140] [Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models](https://arxiv.org/abs/2506.07121)
*Ren-Jian Wang,Ke Xue,Zeyu Qin,Ziniu Li,Sheng Tang,Hao-Tian Li,Shengcai Liu,Chao Qian*

Main category: cs.LG

TL;DR: 本文指出先前红队攻击方法的局限性，提出QDRT框架，经评估其生成的攻击更具多样性和有效性，推动了大语言模型安全领域发展。


<details>
  <summary>Details</summary>
Motivation: 先前红队攻击方法在追求对抗性提示多样性上存在局限，如使用简单指标、训练单一攻击模型，需改进以全面评估大语言模型安全性。

Method: 引入Quality - Diversity Red - Teaming (QDRT)框架，通过行为条件训练实现目标驱动的多样性，采用开放式行为回放缓冲区，训练多个专业攻击模型。

Result: QDRT生成的攻击对GPT - 2、Llama - 3等多种目标大语言模型更具多样性和有效性。

Conclusion: 该工作为自动化红队攻击提供了系统有效的方法，有助于大语言模型的负责任部署。

Abstract: Ensuring safety of large language models (LLMs) is important. Red teaming--a
systematic approach to identifying adversarial prompts that elicit harmful
responses from target LLMs--has emerged as a crucial safety evaluation method.
Within this framework, the diversity of adversarial prompts is essential for
comprehensive safety assessments. We find that previous approaches to
red-teaming may suffer from two key limitations. First, they often pursue
diversity through simplistic metrics like word frequency or sentence embedding
similarity, which may not capture meaningful variation in attack strategies.
Second, the common practice of training a single attacker model restricts
coverage across potential attack styles and risk categories. This paper
introduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to
address these limitations. QDRT achieves goal-driven diversity through
behavior-conditioned training and implements a behavioral replay buffer in an
open-ended manner. Additionally, it trains multiple specialized attackers
capable of generating high-quality attacks across diverse styles and risk
categories. Our empirical evaluation demonstrates that QDRT generates attacks
that are both more diverse and more effective against a wide range of target
LLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the
field of LLM safety by providing a systematic and effective approach to
automated red-teaming, ultimately supporting the responsible deployment of
LLMs.

</details>


### [141] [From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins](https://arxiv.org/abs/2506.06359)
*Gabriel Antonesi,Tudor Cioara,Ionut Anghel,Vasilis Michalakopoulos,Elissaios Sarmas,Liana Toderean*

Main category: cs.LG

TL;DR: 文章综述了AI在能源领域应用，聚焦Transformer和LLMs，介绍相关架构、应用及挑战，还提出Agentic Digital Twin概念。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习在智能电网能源管理中存在不足，而基础模型有更好表现，需对其在能源领域应用进行研究。

Method: 综合分析Transformer和LLMs在能源领域的架构基础、特定领域适配和实际应用等。

Result: GenAI开始在能源领域多方面辅助决策，从规划到日常运营。

Conclusion: 提出Agentic Digital Twin概念用于能源管理系统。

Abstract: Artificial intelligence (AI) has long promised to improve energy management
in smart grids by enhancing situational awareness and supporting more effective
decision-making. While traditional machine learning has demonstrated notable
results in forecasting and optimization, it often struggles with
generalization, situational awareness, and heterogeneous data integration.
Recent advances in foundation models such as Transformer architecture and Large
Language Models (LLMs) have demonstrated improved capabilities in modelling
complex temporal and contextual relationships, as well as in multi-modal data
fusion which is essential for most AI applications in the energy sector. In
this review we synthesize the rapid expanding field of AI applications in the
energy domain focusing on Transformers and LLMs. We examine the architectural
foundations, domain-specific adaptations and practical implementations of
transformer models across various forecasting and grid management tasks. We
then explore the emerging role of LLMs in the field: adaptation and fine tuning
for the energy sector, the type of tasks they are suited for, and the new
challenges they introduce. Along the way, we highlight practical
implementations, innovations, and areas where the research frontier is rapidly
expanding. These recent developments reviewed underscore a broader trend:
Generative AI (GenAI) is beginning to augment decision-making not only in
high-level planning but also in day-to-day operations, from forecasting and
grid balancing to workforce training and asset onboarding. Building on these
developments, we introduce the concept of the Agentic Digital Twin, a
next-generation model that integrates LLMs to bring autonomy, proactivity, and
social interaction into digital twin-based energy management systems.

</details>


### [142] [LETS Forecast: Learning Embedology for Time Series Forecasting](https://arxiv.org/abs/2506.06454)
*Abrar Majeedi,Viswanatha Reddy Gajjala,Satya Sai Srinath Namburi GNVV,Nada Magdi Elkordi,Yin Li*

Main category: cs.LG

TL;DR: 引入DeepEDM框架将非线性动力系统建模与深度神经网络结合，实验表明其抗噪且预测精度高。


<details>
  <summary>Details</summary>
Motivation: 现实时间序列有复杂非线性动态，现有深度学习方法未显式建模动态，需填补此空白。

Method: 受经验动态建模和Takens定理启发，学习延时嵌入的潜在空间，用核回归近似动态，利用softmax注意力高效实现。

Result: DeepEDM对输入噪声鲁棒，预测精度优于现有方法。

Conclusion: DeepEDM框架有效，代码公开。

Abstract: Real-world time series are often governed by complex nonlinear dynamics.
Understanding these underlying dynamics is crucial for precise future
prediction. While deep learning has achieved major success in time series
forecasting, many existing approaches do not explicitly model the dynamics. To
bridge this gap, we introduce DeepEDM, a framework that integrates nonlinear
dynamical systems modeling with deep neural networks. Inspired by empirical
dynamic modeling (EDM) and rooted in Takens' theorem, DeepEDM presents a novel
deep model that learns a latent space from time-delayed embeddings, and employs
kernel regression to approximate the underlying dynamics, while leveraging
efficient implementation of softmax attention and allowing for accurate
prediction of future time steps. To evaluate our method, we conduct
comprehensive experiments on synthetic data of nonlinear dynamical systems as
well as real-world time series across domains. Our results show that DeepEDM is
robust to input noise, and outperforms state-of-the-art methods in forecasting
accuracy. Our code is available at: https://abrarmajeedi.github.io/deep_edm.

</details>


### [143] [WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular Datasets](https://arxiv.org/abs/2506.06455)
*Antonio Jesús Banegas-Luna,Horacio Pérez-Sánchez,Carlos Martínez-Cortés*

Main category: cs.LG

TL;DR: 研究用多种模型无关可解释性技术训练模型，提出WISCA生成共识解释，证明可靠共识策略对提升解释可靠性有价值。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型中预测准确性常被优先考虑，但可解释性在科学和高风险领域很重要，且不同可解释性算法解释有冲突，需达成共识协调结果。

Method: 在六个已知真实情况的合成数据集上训练六个机器学习模型，使用多种模型无关可解释性技术，用既定方法和新方法WISCA生成共识解释。

Result: WISCA与最可靠的单一方法一致。

Conclusion: 可靠的共识策略对提高解释可靠性有价值。

Abstract: While predictive accuracy is often prioritized in machine learning (ML)
models, interpretability remains essential in scientific and high-stakes
domains. However, diverse interpretability algorithms frequently yield
conflicting explanations, highlighting the need for consensus to harmonize
results. In this study, six ML models were trained on six synthetic datasets
with known ground truths, utilizing various model-agnostic interpretability
techniques. Consensus explanations were generated using established methods and
a novel approach: WISCA (Weighted Scaled Consensus Attributions), which
integrates class probability and normalized attributions. WISCA consistently
aligned with the most reliable individual method, underscoring the value of
robust consensus strategies in improving explanation reliability.

</details>


### [144] [Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization](https://arxiv.org/abs/2506.06398)
*Yin Li*

Main category: cs.LG

TL;DR: 本文提出理论框架分析不同位置编码方法对transformer性能的影响，提出基于正交函数的新编码方法，实验表明其在泛化和外推能力上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 分析不同位置编码方法对transformer的表达能力、泛化能力和长序列外推能力的影响，填补transformer理论空白。

Method: 通过函数逼近定义表达能力，用拉德马赫复杂度建立泛化边界，提出基于正交函数的新编码方法，分析现有和新编码的外推能力。

Result: 在合成序列到序列任务上，基于正交变换的编码在泛化和外推方面优于传统正弦编码。

Conclusion: 本工作填补了transformer理论的关键空白，为自然语言处理、计算机视觉等应用的设计选择提供了见解。

Abstract: Positional encodings are a core part of transformer-based models, enabling
processing of sequential data without recurrence. This paper presents a
theoretical framework to analyze how various positional encoding methods,
including sinusoidal, learned, relative, and bias-based methods like Attention
with Linear Biases (ALiBi), impact a transformer's expressiveness,
generalization ability, and extrapolation to longer sequences. Expressiveness
is defined via function approximation, generalization bounds are established
using Rademacher complexity, and new encoding methods based on orthogonal
functions, such as wavelets and Legendre polynomials, are proposed. The
extrapolation capacity of existing and proposed encodings is analyzed,
extending ALiBi's biasing approach to a unified theoretical context.
Experimental evaluation on synthetic sequence-to-sequence tasks shows that
orthogonal transform-based encodings outperform traditional sinusoidal
encodings in generalization and extrapolation. This work addresses a critical
gap in transformer theory, providing insights for design choices in natural
language processing, computer vision, and other transformer applications.

</details>


### [145] [A Certified Unlearning Approach without Access to Source Data](https://arxiv.org/abs/2506.06486)
*Umit Yigit Basaran,Sk Miraj Ahmed,Amit Roy-Chowdhury,Basak Guler*

Main category: cs.LG

TL;DR: 为解决传统模型遗忘方法需访问完整训练数据集的问题，提出无原始数据访问的认证遗忘框架，实验证明方法有效可靠。


<details>
  <summary>Details</summary>
Motivation: 数据隐私法规下，传统无学习方法需访问完整训练集，在源数据不可用时不现实，需新方法实现数据移除。

Method: 提出认证遗忘框架，利用近似源数据统计特性的替代数据集，基于统计距离进行噪声缩放，建立理论边界并引入噪声校准技术。

Result: 在合成和真实数据集上的广泛实验验证了方法在隐私敏感场景中的有效性和可靠性。

Conclusion: 所提方法能在模型遗忘后保证其行为，同时保持整体效用，适用于隐私敏感场景。

Abstract: With the growing adoption of data privacy regulations, the ability to erase
private or copyrighted information from trained models has become a crucial
requirement. Traditional unlearning methods often assume access to the complete
training dataset, which is unrealistic in scenarios where the source data is no
longer available. To address this challenge, we propose a certified unlearning
framework that enables effective data removal \final{without access to the
original training data samples}. Our approach utilizes a surrogate dataset that
approximates the statistical properties of the source data, allowing for
controlled noise scaling based on the statistical distance between the two.
\updated{While our theoretical guarantees assume knowledge of the exact
statistical distance, practical implementations typically approximate this
distance, resulting in potentially weaker but still meaningful privacy
guarantees.} This ensures strong guarantees on the model's behavior
post-unlearning while maintaining its overall utility. We establish theoretical
bounds, introduce practical noise calibration techniques, and validate our
method through extensive experiments on both synthetic and real-world datasets.
The results demonstrate the effectiveness and reliability of our approach in
privacy-sensitive settings.

</details>


### [146] [CoxNTF: A New Approach for Joint Clustering and Prediction in Survival Analysis](https://arxiv.org/abs/2506.06411)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: 提出CoxNTF方法，用非负张量分解推导与生存结果相关的潜在表示，性能与Coxnet相当，可处理特征冗余。


<details>
  <summary>Details</summary>
Motivation: 现有方法如NMF未纳入生存信息，限制预测能力，需新方法推导与生存结果紧密相关的潜在表示。

Method: 提出CoxNTF方法，构建加权协变量张量，用Coxnet模型的生存概率指导张量化过程。

Result: CoxNTF生存预测性能与使用原始协变量的Coxnet相当，提供结构化和可解释的聚类框架。

Conclusion: CoxNTF是生存分析中联合聚类和预测的有力工具，能有效处理特征冗余。

Abstract: The interpretation of the results of survival analysis often benefits from
latent factor representations of baseline covariates. However, existing
methods, such as Nonnegative Matrix Factorization (NMF), do not incorporate
survival information, limiting their predictive power. We present CoxNTF, a
novel approach that uses non-negative tensor factorization (NTF) to derive
meaningful latent representations that are closely associated with survival
outcomes. CoxNTF constructs a weighted covariate tensor in which survival
probabilities derived from the Coxnet model are used to guide the tensorization
process. Our results show that CoxNTF achieves survival prediction performance
comparable to using Coxnet with the original covariates, while providing a
structured and interpretable clustering framework. In addition, the new
approach effectively handles feature redundancy, making it a powerful tool for
joint clustering and prediction in survival analysis.

</details>


### [147] [Membership Inference Attacks for Unseen Classes](https://arxiv.org/abs/2506.06488)
*Pratiksha Thaker,Neil Kale,Zhiwei Steven Wu,Virginia Smith*

Main category: cs.LG

TL;DR: 研究对手无法访问数据分布中整个子类时的成员推理攻击，表明阴影模型攻击性能下降，分位数回归攻击表现更优并给出理论模型。


<details>
  <summary>Details</summary>
Motivation: 现有阴影模型攻击通常假设对手能访问与目标模型训练分布匹配的数据，本文研究对手无法访问数据分布中整个子类这一更极端现实的分布偏移情况。

Method: 先分析阴影模型攻击在这种设定下的性能，再采用分位数回归方法进行攻击，并提供理论模型。

Result: 分位数回归攻击在类缺失设定下始终优于阴影模型攻击，如在CIFAR - 100上对未见类的真阳性率可达阴影模型的11倍，在ImageNet上去除90%训练类时仍有非平凡真阳性率。

Conclusion: 分位数回归方法在对手无法访问整个子类的成员推理攻击中有潜力，同时给出了该方法的潜力和局限性的理论模型。

Abstract: Shadow model attacks are the state-of-the-art approach for membership
inference attacks on machine learning models. However, these attacks typically
assume an adversary has access to a background (nonmember) data distribution
that matches the distribution the target model was trained on. We initiate a
study of membership inference attacks where the adversary or auditor cannot
access an entire subclass from the distribution -- a more extreme but realistic
version of distribution shift than has been studied previously. In this
setting, we first show that the performance of shadow model attacks degrades
catastrophically, and then demonstrate the promise of another approach,
quantile regression, that does not have the same limitations. We show that
quantile regression attacks consistently outperform shadow model attacks in the
class dropout setting -- for example, quantile regression attacks achieve up to
11$\times$ the TPR of shadow models on the unseen class on CIFAR-100, and
achieve nontrivial TPR on ImageNet even with 90% of training classes removed.
We also provide a theoretical model that illustrates the potential and
limitations of this approach.

</details>


### [148] [NeurNCD: Novel Class Discovery via Implicit Neural Representation](https://arxiv.org/abs/2506.06412)
*Junming Wang,Yi Shi*

Main category: cs.LG

TL;DR: 提出NeurNCD框架用于开放世界的新类别发现，在多数据集表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统显式表示在新类别发现中存在离散、易有漏洞和噪声等问题，阻碍准确发现新类别。

Method: 引入NeurNCD框架，采用Embedding - NeRF模型结合KL散度替代传统显式3D分割图，聚合语义嵌入和熵；集成特征查询、特征调制和聚类等组件。

Result: 在开放和封闭世界设置中都取得了出色的分割性能，无需密集标注数据集进行监督训练或人工交互生成稀疏标签监督。

Conclusion: NeurNCD方法在NYUv2和Replica数据集上显著优于现有方法。

Abstract: Discovering novel classes in open-world settings is crucial for real-world
applications. Traditional explicit representations, such as object descriptors
or 3D segmentation maps, are constrained by their discrete, hole-prone, and
noisy nature, which hinders accurate novel class discovery. To address these
challenges, we introduce NeurNCD, the first versatile and data-efficient
framework for novel class discovery that employs the meticulously designed
Embedding-NeRF model combined with KL divergence as a substitute for
traditional explicit 3D segmentation maps to aggregate semantic embedding and
entropy in visual embedding space. NeurNCD also integrates several key
components, including feature query, feature modulation and clustering,
facilitating efficient feature augmentation and information exchange between
the pre-trained semantic segmentation network and implicit neural
representations. As a result, our framework achieves superior segmentation
performance in both open and closed-world settings without relying on densely
labelled datasets for supervised training or human interaction to generate
sparse label supervision. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches on the NYUv2 and Replica
datasets.

</details>


### [149] [Alternating Gradient Flows: A Theory of Feature Learning in Two-layer Neural Networks](https://arxiv.org/abs/2506.06489)
*Daniel Kunin,Giovanni Luca Marchetti,Feng Chen,Dhruva Karkada,James B. Simon,Michael R. DeWeese,Surya Ganguli,Nina Miolane*

Main category: cs.LG

TL;DR: 本文介绍交替梯度流（AGF）算法框架，用于描述两层网络小初始化训练时特征学习动态，量化损失下降，统一并拓展现有分析，能刻画训练动态，助力理解神经网络特征学习。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络学习什么特征以及如何学习这一开放问题。

Method: 引入AGF算法框架，将梯度流行为近似为两步交替过程，即对休眠神经元最大化效用函数，对活跃神经元最小化成本函数。

Result: AGF量化损失下降情况，与实验匹配，统一并拓展现有分析，刻画二次网络训练动态，揭示网络按系数大小降序学习傅里叶特征。

Conclusion: AGF为理解神经网络特征学习迈出有前景的一步。

Abstract: What features neural networks learn, and how, remains an open question. In
this paper, we introduce Alternating Gradient Flows (AGF), an algorithmic
framework that describes the dynamics of feature learning in two-layer networks
trained from small initialization. Prior works have shown that gradient flow in
this regime exhibits a staircase-like loss curve, alternating between plateaus
where neurons slowly align to useful directions and sharp drops where neurons
rapidly grow in norm. AGF approximates this behavior as an alternating two-step
process: maximizing a utility function over dormant neurons and minimizing a
cost function over active ones. AGF begins with all neurons dormant. At each
round, a dormant neuron activates, triggering the acquisition of a feature and
a drop in the loss. AGF quantifies the order, timing, and magnitude of these
drops, matching experiments across architectures. We show that AGF unifies and
extends existing saddle-to-saddle analyses in fully connected linear networks
and attention-only linear transformers, where the learned features are singular
modes and principal components, respectively. In diagonal linear networks, we
prove AGF converges to gradient flow in the limit of vanishing initialization.
Applying AGF to quadratic networks trained to perform modular addition, we give
the first complete characterization of the training dynamics, revealing that
networks learn Fourier features in decreasing order of coefficient magnitude.
Altogether, AGF offers a promising step towards understanding feature learning
in neural networks.

</details>


### [150] [Unlocking Chemical Insights: Superior Molecular Representations from Intermediate Encoder Layers](https://arxiv.org/abs/2506.06443)
*Luis Pinto*

Main category: cs.LG

TL;DR: 研究对五种分子编码器进行层分析，发现中间层嵌入表现优于最终层，可提高下游任务性能并实现新SOTA，支持高效评估 - 微调方法。


<details>
  <summary>Details</summary>
Motivation: 标准做法仅依赖最终层嵌入进行下游任务可能丢弃有价值信息，需探索分子编码器的完整表征深度。

Method: 对五种不同分子编码器在22个ADMET属性预测任务中进行全面的逐层分析。

Result: 中间层嵌入始终优于最终层表征，使用固定中间层嵌入平均提高下游性能5.4%，最高达28.6%；微调至中间层平均提高8.5%，最高达40.8%，在多个基准上实现新SOTA；固定嵌入性能与微调结果呈强正相关。

Conclusion: 探索分子编码器的完整表征深度对大幅提高性能和计算效率很重要。

Abstract: Pretrained molecular encoders have become indispensable in computational
chemistry for tasks such as property prediction and molecular generation.
However, the standard practice of relying solely on final-layer embeddings for
downstream tasks may discard valuable information. In this work, we challenge
this convention by conducting a comprehensive layer-wise analysis of five
diverse molecular encoders across 22 ADMET property prediction tasks. Our
results demonstrate that embeddings from intermediate layers consistently
outperform final-layer representations. Specifically, using fixed embeddings
from the optimal intermediate layers improved downstream performance by an
average of 5.4%, reaching gains up to 28.6%. Furthermore, finetuning up to
these intermediate layers yielded even greater average improvements of 8.5%,
with performance increases as high as 40.8%, achieving new state-of-the-art
results on several benchmarks. Additionally, a strong positive correlation
between fixed embedding performance and finetuning outcomes supports an
efficient evaluate-then-finetune approach, enabling identification of optimal
layers with reduced computational cost. These findings highlight the importance
of exploring the full representational depth of molecular encoders to achieve
substantial performance improvements and computational efficiency. The code is
made publicly available at
https://github.com/luispintoc/Unlocking-Chemical-Insights.

</details>


### [151] [Optimal Rates in Continual Linear Regression via Increasing Regularization](https://arxiv.org/abs/2506.06501)
*Ran Levinstein,Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren,Daniel Soudry,Itay Evron*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study realizable continual linear regression under random task orderings,
a common setting for developing continual learning theory. In this setup, the
worst-case expected loss after $k$ learning iterations admits a lower bound of
$\Omega(1/k)$. However, prior work using an unregularized scheme has only
established an upper bound of $O(1/k^{1/4})$, leaving a significant gap. Our
paper proves that this gap can be narrowed, or even closed, using two
frequently used regularization schemes: (1) explicit isotropic $\ell_2$
regularization, and (2) implicit regularization via finite step budgets. We
show that these approaches, which are used in practice to mitigate forgetting,
reduce to stochastic gradient descent (SGD) on carefully defined surrogate
losses. Through this lens, we identify a fixed regularization strength that
yields a near-optimal rate of $O(\log k / k)$. Moreover, formalizing and
analyzing a generalized variant of SGD for time-varying functions, we derive an
increasing regularization strength schedule that provably achieves an optimal
rate of $O(1/k)$. This suggests that schedules that increase the regularization
coefficient or decrease the number of steps per task are beneficial, at least
in the worst case.

</details>


### [152] [Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance](https://arxiv.org/abs/2506.06444)
*Ruizhong Qiu,Gaotang Li,Tianxin Wei,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: 现有大语言模型安全保障研究主要关注训练阶段对齐，但易受越狱攻击。本文开创性地将推理扩展用于大语言模型安全保障，提出SAFFRON范式，实验验证有效并公开模型和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全保障方法易受越狱攻击，推理扩展在安全保障领域未被探索，需解决该问题。

Method: 提出SAFFRON推理扩展范式，引入多分叉奖励模型（MRM），提出部分监督训练目标、保守探索约束和基于Trie的键值缓存策略。

Result: 广泛实验验证了方法的有效性，公开训练的多分叉奖励模型Saffron - 1和令牌级安全奖励数据集Safety4M。

Conclusion: 推理扩展可用于大语言模型安全保障，SAFFRON范式有效，相关资源公开利于后续研究。

Abstract: Existing safety assurance research has primarily focused on training-phase
alignment to instill safe behaviors into LLMs. However, recent studies have
exposed these methods' susceptibility to diverse jailbreak attacks.
Concurrently, inference scaling has significantly advanced LLM reasoning
capabilities but remains unexplored in the context of safety assurance.
Addressing this gap, our work pioneers inference scaling for robust and
effective LLM safety against emerging threats. We reveal that conventional
inference scaling techniques, despite their success in reasoning tasks, perform
poorly in safety contexts, even falling short of basic approaches like
Best-of-N Sampling. We attribute this inefficiency to a newly identified
challenge, the exploration--efficiency dilemma, arising from the high
computational overhead associated with frequent process reward model (PRM)
evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference
scaling paradigm tailored explicitly for safety assurance. Central to our
approach is the introduction of a multifurcation reward model (MRM) that
significantly reduces the required number of reward model evaluations. To
operationalize this paradigm, we further propose: (i) a partial supervision
training objective for MRM, (ii) a conservative exploration constraint to
prevent out-of-distribution explorations, and (iii) a Trie-based key--value
caching strategy that facilitates cache sharing across sequences during tree
search. Extensive experiments validate the effectiveness of our method.
Additionally, we publicly release our trained multifurcation reward model
(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)
to accelerate future research in LLM safety. Our code, model, and data are
publicly available at https://github.com/q-rz/saffron , and our project
homepage is at https://q-rz.github.io/p/saffron .

</details>


### [153] [Addressing Correlated Latent Exogenous Variables in Debiased Recommender Systems](https://arxiv.org/abs/2506.07517)
*Shuqiang Zhang,Yuchao Zhang,Jinkun Chen,Haochen Sui*

Main category: cs.LG

TL;DR: 推荐系统存在选择偏差问题，以往去偏方法有局限，本文提出基于似然最大化的学习算法处理潜在外生变量，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 推荐系统存在选择偏差，导致用户偏好表示扭曲，影响推荐准确性和公平性，且以往去偏方法假设外生变量独立有局限。

Method: 提出基于似然最大化的学习算法，先讨论未测量混淆与场景的关联差异，提出统一方法处理潜在外生变量，用蒙特卡罗算法估计似然函数。

Result: 在合成数据集和三个真实数据集上的大量实验证明了所提方法的有效性。

Conclusion: 所提方法能有效处理推荐系统中的潜在外生变量，解决选择偏差问题。

Abstract: Recommendation systems (RS) aim to provide personalized content, but they
face a challenge in unbiased learning due to selection bias, where users only
interact with items they prefer. This bias leads to a distorted representation
of user preferences, which hinders the accuracy and fairness of
recommendations. To address the issue, various methods such as error imputation
based, inverse propensity scoring, and doubly robust techniques have been
developed. Despite the progress, from the structural causal model perspective,
previous debiasing methods in RS assume the independence of the exogenous
variables. In this paper, we release this assumption and propose a learning
algorithm based on likelihood maximization to learn a prediction model. We
first discuss the correlation and difference between unmeasured confounding and
our scenario, then we propose a unified method that effectively handles latent
exogenous variables. Specifically, our method models the data generation
process with latent exogenous variables under mild normality assumptions. We
then develop a Monte Carlo algorithm to numerically estimate the likelihood
function. Extensive experiments on synthetic datasets and three real-world
datasets demonstrate the effectiveness of our proposed method. The code is at
https://github.com/WallaceSUI/kdd25-background-variable.

</details>


### [154] [Can Hessian-Based Insights Support Fault Diagnosis in Attention-based Models?](https://arxiv.org/abs/2506.07871)
*Sigma Jahan,Mohammad Masudur Rahman*

Main category: cs.LG

TL;DR: 研究Hessian分析诊断基于注意力模型故障的潜力，实验表明其比梯度更有效，或提升故障诊断。


<details>
  <summary>Details</summary>
Motivation: 随着基于注意力的深度学习模型规模和复杂度增加，诊断其故障更具挑战性，需评估Hessian分析诊断故障的潜力。

Method: 使用Hessian分析识别注意力机制中的脆弱区域和参数相互依赖关系，在HAN、3D - CNN、DistilBERT三个模型上进行实验。

Result: Hessian指标比梯度更能有效定位不稳定性和故障源。

Conclusion: 这些指标可显著改善复杂神经架构的故障诊断，可能改进软件调试实践。

Abstract: As attention-based deep learning models scale in size and complexity,
diagnosing their faults becomes increasingly challenging. In this work, we
conduct an empirical study to evaluate the potential of Hessian-based analysis
for diagnosing faults in attention-based models. Specifically, we use
Hessian-derived insights to identify fragile regions (via curvature analysis)
and parameter interdependencies (via parameter interaction analysis) within
attention mechanisms. Through experiments on three diverse models (HAN, 3D-CNN,
DistilBERT), we show that Hessian-based metrics can localize instability and
pinpoint fault sources more effectively than gradients alone. Our empirical
findings suggest that these metrics could significantly improve fault diagnosis
in complex neural architectures, potentially improving software debugging
practices.

</details>


### [155] [Sharp Gap-Dependent Variance-Aware Regret Bounds for Tabular MDPs](https://arxiv.org/abs/2506.06521)
*Shulun Chen,Runlong Zhou,Zihan Zhang,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: 本文研究了阶段性MDPs的差距相关遗憾界，证明MVP算法能达到特定的差距相关遗憾界，并给出了下界。


<details>
  <summary>Details</summary>
Motivation: 研究阶段性MDPs的差距相关遗憾界，了解算法性能。

Method: 对次优差距的加权和进行新颖分析。

Result: MVP算法达到方差感知的差距相关遗憾界，且给出了下界。

Conclusion: 结果源于新颖分析，可用于其他算法，证明了对最大条件总方差依赖的必要性。

Abstract: We consider the gap-dependent regret bounds for episodic MDPs. We show that
the Monotonic Value Propagation (MVP) algorithm achieves a variance-aware
gap-dependent regret bound of $$\tilde{O}\left(\left(\sum_{\Delta_h(s,a)>0}
\frac{H^2 \log K \land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)}
+\sum_{\Delta_h(s,a)=0}\frac{ H^2 \land
\mathtt{Var}_{\max}^{\text{c}}}{\Delta_{\mathrm{min}}} + SAH^4 (S \lor H)
\right) \log K\right),$$ where $H$ is the planning horizon, $S$ is the number
of states, $A$ is the number of actions, and $K$ is the number of episodes.
Here, $\Delta_h(s,a) =V_h^* (a) - Q_h^* (s, a)$ represents the suboptimality
gap and $\Delta_{\mathrm{min}} := \min_{\Delta_h (s,a) > 0} \Delta_h(s,a)$. The
term $\mathtt{Var}_{\max}^{\text{c}}$ denotes the maximum conditional total
variance, calculated as the maximum over all $(\pi, h, s)$ tuples of the
expected total variance under policy $\pi$ conditioned on trajectories visiting
state $s$ at step $h$. $\mathtt{Var}_{\max}^{\text{c}}$ characterizes the
maximum randomness encountered when learning any $(h, s)$ pair. Our result
stems from a novel analysis of the weighted sum of the suboptimality gap and
can be potentially adapted for other algorithms. To complement the study, we
establish a lower bound of $$\Omega \left( \sum_{\Delta_h(s,a)>0} \frac{H^2
\land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)}\cdot \log K\right),$$
demonstrating the necessity of dependence on $\mathtt{Var}_{\max}^{\text{c}}$
even when the maximum unconditional total variance (without conditioning on
$(h, s)$) approaches zero.

</details>


### [156] [Graph Persistence goes Spectral](https://arxiv.org/abs/2506.06571)
*Mattie Ji,Amauri H. Souza,Vikas Garg*

Main category: cs.LG

TL;DR: 本文提出新的图拓扑描述符SpectRe，将光谱信息集成到PH图中，比现有描述符更具表达力且局部稳定，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于PH方法依赖特征，无法捕捉基本图结构信息，需更有效的拓扑描述符。

Method: 提出SpectRe，将光谱信息集成到PH图中，并引入全局和局部稳定性概念分析现有描述符。

Result: SpectRe比现有描述符更具表达力，且局部稳定，在合成和真实数据集实验中展现有效性。

Conclusion: SpectRe有效，有潜力提升图模型在相关学习任务中的能力。

Abstract: Including intricate topological information (e.g., cycles) provably enhances
the expressivity of message-passing graph neural networks (GNNs) beyond the
Weisfeiler-Leman (WL) hierarchy. Consequently, Persistent Homology (PH) methods
are increasingly employed for graph representation learning. In this context,
recent works have proposed decorating classical PH diagrams with vertex and
edge features for improved expressivity. However, due to their dependence on
features, these methods still fail to capture basic graph structural
information. In this paper, we propose SpectRe -- a new topological descriptor
for graphs that integrates spectral information into PH diagrams. Notably,
SpectRe is strictly more expressive than existing descriptors on graphs. We
also introduce notions of global and local stability to analyze existing
descriptors and establish that SpectRe is locally stable. Finally, experiments
on synthetic and real-world datasets demonstrate the effectiveness of SpectRe
and its potential to enhance the capabilities of graph models in relevant
learning tasks.

</details>


### [157] [Demystifying Topological Message-Passing with Relational Structures: A Case Study on Oversquashing in Simplicial Message-Passing](https://arxiv.org/abs/2506.06582)
*Diaaeldin Taha,James Chapman,Marzieh Eidi,Karel Devriendt,Guido Montúfar*

Main category: cs.LG

TL;DR: 提出统一公理框架连接图和拓扑消息传递，可推进拓扑深度学习，缓解过压缩问题。


<details>
  <summary>Details</summary>
Motivation: 拓扑深度学习中拓扑消息传递的过压缩现象研究不足且缺乏理论分析。

Method: 通过关系结构视角看待单纯复形和胞腔复形及其消息传递方案，提出统一公理框架。

Result: 通过理论分析和在单纯网络上的实证研究，证明框架能推进拓扑深度学习。

Conclusion: 所提框架有助于分析和缓解拓扑消息传递网络中的过压缩问题，有推进拓扑深度学习的潜力。

Abstract: Topological deep learning (TDL) has emerged as a powerful tool for modeling
higher-order interactions in relational data. However, phenomena such as
oversquashing in topological message-passing remain understudied and lack
theoretical analysis. We propose a unifying axiomatic framework that bridges
graph and topological message-passing by viewing simplicial and cellular
complexes and their message-passing schemes through the lens of relational
structures. This approach extends graph-theoretic results and algorithms to
higher-order structures, facilitating the analysis and mitigation of
oversquashing in topological message-passing networks. Through theoretical
analysis and empirical studies on simplicial networks, we demonstrate the
potential of this framework to advance TDL.

</details>


### [158] [Towards Infant Sleep-Optimized Driving: Synergizing Wearable and Vehicle Sensing in Intelligent Cruise Control](https://arxiv.org/abs/2506.06459)
*Ruitao Chen,Mozhang Guo,Jinge Li*

Main category: cs.LG

TL;DR: 本文探讨将强化学习集成到自动驾驶中，提出智能巡航控制框架，结合可穿戴传感和车辆数据，建模驾驶行为与婴儿睡眠质量关系，计算最佳驾驶激进程度，模拟显示能显著提升婴儿睡眠质量并保持旅行效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶对乘客福祉尤其是婴儿睡眠影响研究不足，突然加速、急刹车等会干扰婴儿睡眠，需解决该问题。

Method: 提出智能巡航控制框架，将长短期记忆网络和基于Transformer的神经网络与强化学习结合，依据可穿戴传感器、车辆控制器和地图应用数据，动态计算最佳驾驶激进程度并转化为控制策略。

Result: 模拟结果显示，与基线方法相比，该解决方案显著提升了婴儿睡眠质量，同时保持了理想的旅行效率。

Conclusion: 将强化学习集成到自动驾驶中提出的解决方案能有效提升婴儿睡眠质量并保持旅行效率。

Abstract: Automated driving (AD) has substantially improved vehicle safety and driving
comfort, but their impact on passenger well-being, particularly infant sleep,
is not sufficiently studied. Sudden acceleration, abrupt braking, and sharp
maneuvers can disrupt infant sleep, compromising both passenger comfort and
parental convenience. To solve this problem, this paper explores the
integration of reinforcement learning (RL) within AD to personalize driving
behavior and optimally balance occupant comfort and travel efficiency. In
particular, we propose an intelligent cruise control framework that adapts to
varying driving conditions to enhance infant sleep quality by effectively
synergizing wearable sensing and vehicle data. Long short-term memory (LSTM)
and transformer-based neural networks are integrated with RL to model the
relationship between driving behavior and infant sleep quality under diverse
traffic and road conditions. Based on the sleep quality indicators from the
wearable sensors, driving action data from vehicle controllers, and map data
from map applications, the model dynamically computes the optimal driving
aggressiveness level, which is subsequently translated into specific AD control
strategies, e.g., the magnitude and frequency of acceleration, lane change, and
overtaking. Simulation results demonstrate that the proposed solution
significantly improves infant sleep quality compared to baseline methods, while
preserving desirable travel efficiency.

</details>


### [159] [Global Convergence of Gradient EM for Over-Parameterized Gaussian Mixtures](https://arxiv.org/abs/2506.06584)
*Mo Zhou,Weihang Xu,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Learning Gaussian Mixture Models (GMMs) is a fundamental problem in machine
learning, with the Expectation-Maximization (EM) algorithm and its popular
variant gradient EM being arguably the most widely used algorithms in practice.
In the exact-parameterized setting, where both the ground truth GMM and the
learning model have the same number of components $m$, a vast line of work has
aimed to establish rigorous recovery guarantees for EM. However, global
convergence has only been proven for the case of $m=2$, and EM is known to fail
to recover the ground truth when $m\geq 3$.
  In this paper, we consider the $\textit{over-parameterized}$ setting, where
the learning model uses $n>m$ components to fit an $m$-component ground truth
GMM. In contrast to the exact-parameterized case, we provide a rigorous global
convergence guarantee for gradient EM. Specifically, for any well separated
GMMs in general position, we prove that with only mild over-parameterization $n
= \Omega(m\log m)$, randomly initialized gradient EM converges globally to the
ground truth at a polynomial rate with polynomial samples. Our analysis
proceeds in two stages and introduces a suite of novel tools for Gaussian
Mixture analysis. We use Hermite polynomials to study the dynamics of gradient
EM and employ tensor decomposition to characterize the geometric landscape of
the likelihood loss. This is the first global convergence and recovery result
for EM or Gradient EM beyond the special case of $m=2$.

</details>


### [160] [TimeRecipe: A Time-Series Forecasting Recipe via Benchmarking Module Level Effectiveness](https://arxiv.org/abs/2506.06482)
*Zhiyuan Zhao,Juntong Ni,Shangqing Xu,Haoxin Liu,Wei Jin,B. Aditya Prakash*

Main category: cs.LG

TL;DR: 提出统一基准框架TimeRecipe对时间序列预测方法进行模块级评估，实验表明全面探索设计空间可得到更优模型，还发布推荐合适架构的工具包。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要对模型进行高级评估，对特定设计为何更有效缺乏深入见解，需填补此差距。

Method: 提出TimeRecipe框架，在不同数据集、预测范围和任务设置下进行超10000次实验评估单个组件的有效性。

Result: 全面探索设计空间可得到优于现有最优方法的模型，揭示特定设计选择与预测场景的关联。

Conclusion: TimeRecipe能有效评估时间序列预测方法，其工具包可根据经验推荐合适模型架构。

Abstract: Time-series forecasting is an essential task with wide real-world
applications across domains. While recent advances in deep learning have
enabled time-series forecasting models with accurate predictions, there remains
considerable debate over which architectures and design components, such as
series decomposition or normalization, are most effective under varying
conditions. Existing benchmarks primarily evaluate models at a high level,
offering limited insight into why certain designs work better. To mitigate this
gap, we propose TimeRecipe, a unified benchmarking framework that
systematically evaluates time-series forecasting methods at the module level.
TimeRecipe conducts over 10,000 experiments to assess the effectiveness of
individual components across a diverse range of datasets, forecasting horizons,
and task settings. Our results reveal that exhaustive exploration of the design
space can yield models that outperform existing state-of-the-art methods and
uncover meaningful intuitions linking specific design choices to forecasting
scenarios. Furthermore, we release a practical toolkit within TimeRecipe that
recommends suitable model architectures based on these empirical insights. The
benchmark is available at: https://github.com/AdityaLab/TimeRecipe.

</details>


### [161] [Direct Prediction Set Minimization via Bilevel Conformal Classifier Training](https://arxiv.org/abs/2506.06599)
*Yuanjie Shi,Hooman Shahrokhi,Xuesong Jia,Xiongzhi Chen,Janardhan Rao Doppa,Yan Yan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Conformal prediction (CP) is a promising uncertainty quantification framework
which works as a wrapper around a black-box classifier to construct prediction
sets (i.e., subset of candidate classes) with provable guarantees. However,
standard calibration methods for CP tend to produce large prediction sets which
makes them less useful in practice. This paper considers the problem of
integrating conformal principles into the training process of deep classifiers
to directly minimize the size of prediction sets. We formulate conformal
training as a bilevel optimization problem and propose the {\em Direct
Prediction Set Minimization (DPSM)} algorithm to solve it. The key insight
behind DPSM is to minimize a measure of the prediction set size (upper level)
that is conditioned on the learned quantile of conformity scores (lower level).
We analyze that DPSM has a learning bound of $O(1/\sqrt{n})$ (with $n$ training
samples), while prior conformal training methods based on stochastic
approximation for the quantile has a bound of $\Omega(1/s)$ (with batch size
$s$ and typically $s \ll \sqrt{n}$). Experiments on various benchmark datasets
and deep models show that DPSM significantly outperforms the best prior
conformal training baseline with $20.46\%\downarrow$ in the prediction set size
and validates our theory.

</details>


### [162] [Spark Transformer: Reactivating Sparsity in FFN and Attention](https://arxiv.org/abs/2506.06644)
*Chong You,Kan Wu,Zhipeng Jia,Lin Chen,Srinadh Bhojanapalli,Jiaxian Guo,Utku Evci,Jan Wassenberg,Praneeth Netrapalli,Jeremiah J. Willcock,Suvinay Subramanian,Felix Chern,Alek Andreev,Shreya Pathak,Felix Yu,Prateek Jain,David E. Culler,Henry M. Levy,Sanjiv Kumar*

Main category: cs.LG

TL;DR: 提出Spark Transformer架构，在FFN和注意力机制中实现高激活稀疏性，保持模型质量，减少FLOPs并加速解码。


<details>
  <summary>Details</summary>
Motivation: 现有引入激活稀疏性的方法存在降低模型质量、增加参数等问题，需新架构解决。

Method: 通过top - k掩码实现稀疏性，引入统计top - k算法，重新分配参数形成低成本预测器。

Result: 在标准基准测试中表现有竞争力，FFN神经元激活率8%，每个token最多关注256个token，FLOPs减少2.5倍，CPU解码加速1.79倍，GPU加速1.40倍。

Conclusion: Spark Transformer能在实现高激活稀疏性的同时，保持模型质量，提升效率。

Abstract: The discovery of the lazy neuron phenomenon in trained Transformers, where
the vast majority of neurons in their feed-forward networks (FFN) are inactive
for each token, has spurred tremendous interests in activation sparsity for
enhancing large model efficiency. While notable progress has been made in
translating such sparsity to wall-time benefits, modern Transformers have moved
away from the ReLU activation function crucial to this phenomenon. Existing
efforts on re-introducing activation sparsity often degrade model quality,
increase parameter count, complicate or slow down training. Sparse attention,
the application of sparse activation to the attention mechanism, often faces
similar challenges.
  This paper introduces the Spark Transformer, a novel architecture that
achieves a high level of activation sparsity in both FFN and the attention
mechanism while maintaining model quality, parameter count, and standard
training procedures. Our method realizes sparsity via top-k masking for
explicit control over sparsity level. Crucially, we introduce statistical
top-k, a hardware-accelerator-friendly, linear-time approximate algorithm that
avoids costly sorting and mitigates significant training slowdown from standard
top-$k$ operators. Furthermore, Spark Transformer reallocates existing FFN
parameters and attention key embeddings to form a low-cost predictor for
identifying activated entries. This design not only mitigates quality loss from
enforced sparsity, but also enhances wall-time benefit. Pretrained with the
Gemma-2 recipe, Spark Transformer demonstrates competitive performance on
standard benchmarks while exhibiting significant sparsity: only 8% of FFN
neurons are activated, and each token attends to a maximum of 256 tokens. This
sparsity translates to a 2.5x reduction in FLOPs, leading to decoding wall-time
speedups of up to 1.79x on CPU and 1.40x on GPU.

</details>


### [163] [SAFER: A Calibrated Risk-Aware Multimodal Recommendation Model for Dynamic Treatment Regimes](https://arxiv.org/abs/2506.06649)
*Yishan Shen,Yuyang Ye,Hui Xiong,Yong Chen*

Main category: cs.LG

TL;DR: 本文提出SAFER框架用于动态治疗方案，集成结构化EHR和临床笔记，在两个脓毒症数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有动态治疗方案依赖临床医生规定的黄金标准且主要用结构化EHR数据，未利用临床笔记，缺乏可靠性。

Method: 引入SAFER框架，集成结构化EHR和临床笔记，通过假设死亡患者的模糊最优治疗方案处理标签不确定性，采用共形预测提供统计保证。

Result: 在两个公开脓毒症数据集上，SAFER在多个推荐指标和反事实死亡率方面优于现有基线，提供了可靠的正式保证。

Conclusion: SAFER有潜力成为高风险动态治疗方案应用中值得信赖且理论基础扎实的解决方案。

Abstract: Dynamic treatment regimes (DTRs) are critical to precision medicine,
optimizing long-term outcomes through personalized, real-time decision-making
in evolving clinical contexts, but require careful supervision for unsafe
treatment risks. Existing efforts rely primarily on clinician-prescribed gold
standards despite the absence of a known optimal strategy, and predominantly
using structured EHR data without extracting valuable insights from clinical
notes, limiting their reliability for treatment recommendations. In this work,
we introduce SAFER, a calibrated risk-aware tabular-language recommendation
framework for DTR that integrates both structured EHR and clinical notes,
enabling them to learn from each other, and addresses inherent label
uncertainty by assuming ambiguous optimal treatment solution for deceased
patients. Moreover, SAFER employs conformal prediction to provide statistical
guarantees, ensuring safe treatment recommendations while filtering out
uncertain predictions. Experiments on two publicly available sepsis datasets
demonstrate that SAFER outperforms state-of-the-art baselines across multiple
recommendation metrics and counterfactual mortality rate, while offering robust
formal assurances. These findings underscore SAFER potential as a trustworthy
and theoretically grounded solution for high-stakes DTR applications.

</details>


### [164] [Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms](https://arxiv.org/abs/2506.06499)
*Alex Havrilla,Edward Hughes,Mikayel Samvelyan,Jacob Abernethy*

Main category: cs.LG

TL;DR: 提出SPARQ方法生成高质量多样的合成数学问题与解决方案对，通过过滤数据微调模型提升性能，研究了合成数据对模型泛化的影响并证实了缩放定律。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的合成数据生成方法在扩展到更复杂多样问题领域时存在可扩展性限制。

Method: 提出SPARQ方法，通过测量问题解决率生成合成问题与解决方案对，过滤生成数据并微调模型，进行消融实验。

Result: 从7.5K样本种子数据集生成超2000万个新问题 - 解决方案对，过滤数据微调模型使相对性能提升达24%，发现高质量数据利于分布内性能，多样数据利于OOD泛化，证实缩放定律。

Conclusion: SPARQ方法有效提升模型性能，合成数据的质量、多样性及缩放定律对下游模型泛化有积极作用。

Abstract: Large language model (LLM) driven synthetic data generation has emerged as a
powerful method for improving model reasoning capabilities. However, most
methods either distill large state-of-the-art models into small students or use
natural ground-truth problem statements to guarantee problem statement quality.
This limits the scalability of these approaches to more complex and diverse
problem domains. To address this, we present SPARQ: Synthetic Problem
Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for
generating high-quality and diverse synthetic math problem and solution pairs
using only a single model by measuring a problem's solve-rate: a proxy for
problem difficulty. Starting from a seed dataset of 7.5K samples, we generate
over 20 million new problem-solution pairs. We show that filtering the
generated data by difficulty and then fine-tuning the same model on the
resulting data improves relative model performance by up to 24\%. Additionally,
we conduct ablations studying the impact of synthetic data quantity, quality
and diversity on model generalization. We find that higher quality, as measured
by problem difficulty, facilitates better in-distribution performance. Further,
while generating diverse synthetic data does not as strongly benefit
in-distribution performance, filtering for more diverse data facilitates more
robust OOD generalization. We also confirm the existence of model and data
scaling laws for synthetically generated problems, which positively benefit
downstream model generalization.

</details>


### [165] [Rescaled Influence Functions: Accurate Data Attribution in High Dimension](https://arxiv.org/abs/2506.06656)
*Ittai Rubinstein,Samuel B. Hopkins*

Main category: cs.LG

TL;DR: 本文探讨训练数据对模型行为的影响，提出重缩放影响函数（RIF）用于数据归因，比传统影响函数（IF）更准确，并通过实验和理论分析验证，还给出能骗过IF检测但可被RIF检测的数据中毒攻击示例。


<details>
  <summary>Details</summary>
Motivation: 现有基于影响函数（IF）的数据归因方法在高维情况下不准确，有低估样本移除影响的问题，需要更准确的方法。

Method: 提出重缩放影响函数（RIF）作为影响函数（IF）的替代工具，在多个真实数据集上对比IF和RIF，并进行理论分析。

Result: 实验表明RIF在实践中预测效果显著优于IF，理论分析解释了RIF准确性提升的原因，还给出可被RIF检测但能骗过IF检测的数据中毒攻击示例。

Conclusion: RIF可作为数据归因工具替代IF，在准确性上有显著提升。

Abstract: How does the training data affect a model's behavior? This is the question we
seek to answer with data attribution. The leading practical approaches to data
attribution are based on influence functions (IF). IFs utilize a first-order
Taylor approximation to efficiently predict the effect of removing a set of
samples from the training set without retraining the model, and are used in a
wide variety of machine learning applications. However, especially in the
high-dimensional regime (# params $\geq \Omega($# samples$)$), they are often
imprecise and tend to underestimate the effect of sample removals, even for
simple models such as logistic regression. We present rescaled influence
functions (RIF), a new tool for data attribution which can be used as a drop-in
replacement for influence functions, with little computational overhead but
significant improvement in accuracy. We compare IF and RIF on a range of
real-world datasets, showing that RIFs offer significantly better predictions
in practice, and present a theoretical analysis explaining this improvement.
Finally, we present a simple class of data poisoning attacks that would fool
IF-based detections but would be detected by RIF.

</details>


### [166] [Through the Gaps: Uncovering Tactical Line-Breaking Passes with Clustering](https://arxiv.org/abs/2506.06666)
*Oktay Karakuş,Hasan Arkadaş*

Main category: cs.LG

TL;DR: 本文提出基于聚类的无监督框架，用精英比赛数据检测和分析足球破防线传球，引入战术指标评估其效果，适用于表现分析和球探工作。


<details>
  <summary>Details</summary>
Motivation: 破防线传球对足球战术很重要，需有效方法检测和分析。

Method: 采用无监督、基于聚类的框架，用同步事件和跟踪数据，通过垂直空间分割建模对手球队阵型，识别开放比赛中破防线的传球，引入空间积累比等战术指标。

Result: 在2022年世界杯上评估指标，揭示了球队和球员在纵向推进和结构破坏方面的风格差异。

Conclusion: 所提方法具有可解释性、可扩展性，可直接应用于现代表现分析和球探工作流程。

Abstract: Line-breaking passes (LBPs) are crucial tactical actions in football,
allowing teams to penetrate defensive lines and access high-value spaces. In
this study, we present an unsupervised, clustering-based framework for
detecting and analysing LBPs using synchronised event and tracking data from
elite matches. Our approach models opponent team shape through vertical spatial
segmentation and identifies passes that disrupt defensive lines within open
play. Beyond detection, we introduce several tactical metrics, including the
space build-up ratio (SBR) and two chain-based variants, LBPCh$^1$ and
LBPCh$^2$, which quantify the effectiveness of LBPs in generating immediate or
sustained attacking threats. We evaluate these metrics across teams and players
in the 2022 FIFA World Cup, revealing stylistic differences in vertical
progression and structural disruption. The proposed methodology is explainable,
scalable, and directly applicable to modern performance analysis and scouting
workflows.

</details>


### [167] [InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models](https://arxiv.org/abs/2506.06505)
*Keisuke Sugiura,Hiroki Matsutani*

Main category: cs.LG

TL;DR: 提出基于FPGA的InstantFT方法用于物联网设备上超快速CNN微调，比现有方法快17.4倍，减少微调时间并提高能效。


<details>
  <summary>Details</summary>
Motivation: 训练深度神经网络计算和内存需求大，在资源受限的物联网平台上进行运行时自适应调整具有挑战性。

Method: 通过优化参数高效微调（PEFT）中的前向和后向计算，提出基于FPGA的InstantFT方法。

Result: 在有概念漂移的数据集实验中，InstantFT比现有基于LoRA的方法快17.4倍，微调时间减至0.36秒，能效提高16.3倍。

Conclusion: InstantFT能使CNN对非平稳数据分布进行即时自适应调整。

Abstract: Training deep neural networks (DNNs) requires significantly more computation
and memory than inference, making runtime adaptation of DNNs challenging on
resource-limited IoT platforms. We propose InstantFT, an FPGA-based method for
ultra-fast CNN fine-tuning on IoT devices, by optimizing the forward and
backward computations in parameter-efficient fine-tuning (PEFT). Experiments on
datasets with concept drift demonstrate that InstantFT fine-tunes a pre-trained
CNN 17.4x faster than existing Low-Rank Adaptation (LoRA)-based approaches,
while achieving comparable accuracy. Our FPGA-based InstantFT reduces the
fine-tuning time to just 0.36s and improves energy-efficiency by 16.3x,
enabling on-the-fly adaptation of CNNs to non-stationary data distributions.

</details>


### [168] [A Framework for Controllable Multi-objective Learning with Annealed Stein Variational Hypernetworks](https://arxiv.org/abs/2506.06715)
*Minh-Duc Nguyen,Dung D. Le*

Main category: cs.LG

TL;DR: 本文提出SVH - MOL方法解决多目标学习中帕累托解多样性与超体积值最大化问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前多目标学习中部分方法面临使帕累托解在最大化超体积值时保持多样性的挑战。

Method: 提出SVH - MOL方法，采用Stein Variational Gradient Descent（SVGD）近似整个帕累托集，运用多样梯度方向策略和退火调度。

Result: 通过在多目标问题和多任务学习上的大量实验，证明了该方法性能优越。

Conclusion: 所提出的SVH - MOL方法能有效解决多目标学习中的挑战，具有良好性能。

Abstract: Pareto Set Learning (PSL) is popular as an efficient approach to obtaining
the complete optimal solution in Multi-objective Learning (MOL). A set of
optimal solutions approximates the Pareto set, and its mapping is a set of
dense points in the Pareto front in objective space. However, some current
methods face a challenge: how to make the Pareto solution is diverse while
maximizing the hypervolume value. In this paper, we propose a novel method to
address this challenge, which employs Stein Variational Gradient Descent (SVGD)
to approximate the entire Pareto set. SVGD pushes a set of particles towards
the Pareto set by applying a form of functional gradient descent, which helps
to converge and diversify optimal solutions. Additionally, we employ diverse
gradient direction strategies to thoroughly investigate a unified framework for
SVGD in multi-objective optimization and adapt this framework with an annealing
schedule to promote stability. We introduce our method, SVH-MOL, and validate
its effectiveness through extensive experiments on multi-objective problems and
multi-task learning, demonstrating its superior performance.

</details>


### [169] [Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.06532)
*Zijiang Yan,Hao Zhou,Jianhua Pei,Hina Tabassum*

Main category: cs.LG

TL;DR: 本文探索多无人机在集成网络中的联合运动与通信控制，提出基于大语言模型的分层协作方法，实验显示该方法优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 多无人机系统在动态受限环境中的控制和优化是重大挑战，现有研究不足。

Method: 提出基于大语言模型的分层协作方法，HAPS上的LLM进行无人机接入控制，每架无人机上的LLM处理运动规划和控制。

Result: 与基线方法相比，所提协作LLM方法实现了更高的系统奖励、更低的运营成本和显著降低的无人机碰撞率。

Conclusion: 基于知识驱动的范式在下一代3D空中高速公路系统开发方面具有巨大潜力。

Abstract: Unmanned aerial vehicles (UAVs) have been widely adopted in various
real-world applications. However, the control and optimization of multi-UAV
systems remain a significant challenge, particularly in dynamic and constrained
environments. This work explores the joint motion and communication control of
multiple UAVs operating within integrated terrestrial and non-terrestrial
networks that include high-altitude platform stations (HAPS). Specifically, we
consider an aerial highway scenario in which UAVs must accelerate, decelerate,
and change lanes to avoid collisions and maintain overall traffic flow.
Different from existing studies, we propose a novel hierarchical and
collaborative method based on large language models (LLMs). In our approach, an
LLM deployed on the HAPS performs UAV access control, while another LLM onboard
each UAV handles motion planning and control. This LLM-based framework
leverages the rich knowledge embedded in pre-trained models to enable both
high-level strategic planning and low-level tactical decisions. This
knowledge-driven paradigm holds great potential for the development of
next-generation 3D aerial highway systems. Experimental results demonstrate
that our proposed collaborative LLM-based method achieves higher system
rewards, lower operational costs, and significantly reduced UAV collision rates
compared to baseline approaches.

</details>


### [170] [Curvature Enhanced Data Augmentation for Regression](https://arxiv.org/abs/2506.06853)
*Ilya Kaufman Sirot,Omri Azencot*

Main category: cs.LG

TL;DR: 本文针对回归任务提出曲率增强流形采样（CEMS）方法，利用数据流形二阶表示，性能优于现有方法且计算开销小。


<details>
  <summary>Details</summary>
Motivation: 数据增强在回归问题上关注较少，需新方法，此前提出的流形学习方法可作为基础。

Method: 提出近似和采样通用数据流形的理论框架和实用工具，引入CEMS方法，利用数据流形二阶表示。

Result: 在多个数据集上评估，CEMS在分布内和分布外场景表现均优于现有方法，计算开销小。

Conclusion: CEMS方法在回归任务中有效且实用，代码开源。

Abstract: Deep learning models with a large number of parameters, often referred to as
over-parameterized models, have achieved exceptional performance across various
tasks. Despite concerns about overfitting, these models frequently generalize
well to unseen data, thanks to effective regularization techniques, with data
augmentation being among the most widely used. While data augmentation has
shown great success in classification tasks using label-preserving
transformations, its application in regression problems has received less
attention. Recently, a novel \emph{manifold learning} approach for generating
synthetic data was proposed, utilizing a first-order approximation of the data
manifold. Building on this foundation, we present a theoretical framework and
practical tools for approximating and sampling general data manifolds.
Furthermore, we introduce the Curvature-Enhanced Manifold Sampling (CEMS)
method for regression tasks. CEMS leverages a second-order representation of
the data manifold to enable efficient sampling and reconstruction of new data
points. Extensive evaluations across multiple datasets and comparisons with
state-of-the-art methods demonstrate that CEMS delivers superior performance in
both in-distribution and out-of-distribution scenarios, while introducing only
minimal computational overhead. Code is available at
https://github.com/azencot-group/CEMS.

</details>


### [171] [GeoClip: Geometry-Aware Clipping for Differentially Private SGD](https://arxiv.org/abs/2506.06549)
*Atefeh Gilani,Naima Tasnim,Lalitha Sankar,Oliver Kosut*

Main category: cs.LG

TL;DR: 提出GeoClip框架优化DP - SGD梯度裁剪，在相同隐私预算下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有DP - SGD自适应方法在标准坐标系操作，未考虑梯度坐标间相关性，影响隐私和效用权衡。

Method: 提出GeoClip框架，在与梯度分布几何对齐的变换基中裁剪和扰动梯度，利用已发布的噪声梯度自适应估计变换，无额外隐私成本。

Result: 实验表明，在表格和图像数据集上，GeoClip在相同隐私预算下始终优于现有自适应裁剪方法。

Conclusion: GeoClip能有效解决DP - SGD中梯度裁剪阈值设置问题，优化隐私和效用权衡。

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the most
widely used method for training machine learning models with provable privacy
guarantees. A key challenge in DP-SGD is setting the per-sample gradient
clipping threshold, which significantly affects the trade-off between privacy
and utility. While recent adaptive methods improve performance by adjusting
this threshold during training, they operate in the standard coordinate system
and fail to account for correlations across the coordinates of the gradient. We
propose GeoClip, a geometry-aware framework that clips and perturbs gradients
in a transformed basis aligned with the geometry of the gradient distribution.
GeoClip adaptively estimates this transformation using only previously released
noisy gradients, incurring no additional privacy cost. We provide convergence
guarantees for GeoClip and derive a closed-form solution for the optimal
transformation that minimizes the amount of noise added while keeping the
probability of gradient clipping under control. Experiments on both tabular and
image datasets demonstrate that GeoClip consistently outperforms existing
adaptive clipping methods under the same privacy budget.

</details>


### [172] [Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning](https://arxiv.org/abs/2506.06873)
*Armin Behnamnia,Gholamali Aminian,Alireza Aghaei,Chengchun Shi,Vincent Y. F. Tan,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Off-policy learning and evaluation leverage logged bandit feedback datasets,
which contain context, action, propensity score, and feedback for each data
point. These scenarios face significant challenges due to high variance and
poor performance with low-quality propensity scores and heavy-tailed reward
distributions. We address these issues by introducing a novel estimator based
on the log-sum-exponential (LSE) operator, which outperforms traditional
inverse propensity score estimators. Our LSE estimator demonstrates variance
reduction and robustness under heavy-tailed conditions. For off-policy
evaluation, we derive upper bounds on the estimator's bias and variance. In the
off-policy learning scenario, we establish bounds on the regret -- the
performance gap between our LSE estimator and the optimal policy -- assuming
bounded $(1+\epsilon)$-th moment of weighted reward. Notably, we achieve a
convergence rate of $O(n^{-\epsilon/(1+ \epsilon)})$ for the regret bounds,
where $\epsilon \in [0,1]$ and $n$ is the size of logged bandit feedback
dataset. Theoretical analysis is complemented by comprehensive empirical
evaluations in both off-policy learning and evaluation scenarios, confirming
the practical advantages of our approach. The code for our estimator is
available at the following link:
https://github.com/armin-behnamnia/lse-offpolicy-learning.

</details>


### [173] [SDN-Based False Data Detection With Its Mitigation and Machine Learning Robustness for In-Vehicle Networks](https://arxiv.org/abs/2506.06556)
*Long Dang,Thushari Hapuarachchi,Kaiqi Xiong,Yi Li*

Main category: cs.LG

TL;DR: 本文提出基于SDN的车内网络虚假数据检测与缓解系统FDDMS，利用SDN和LSTM模型检测攻击，评估模型鲁棒性并增强再训练技术，实验证明其能实时检测和缓解攻击。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶和联网车辆发展，车内ECU通信安全至关重要，需保障其通信安全。

Method: 解码原始CAN数据创建攻击模型，用基于LSTM的FDDMS检测虚假数据注入攻击，提出DeepFool变体评估模型鲁棒性，增强再训练技术对抗四种攻击，通过SDN动态更新流规则实施缓解方案。

Result: 所提出的FDDMS对对抗攻击具有鲁棒性，能实时有效检测和缓解虚假数据注入攻击。

Conclusion: FDDMS可保障车内网络中ECU通信的安全，有效应对虚假数据注入攻击。

Abstract: As the development of autonomous and connected vehicles advances, the
complexity of modern vehicles increases, with numerous Electronic Control Units
(ECUs) integrated into the system. In an in-vehicle network, these ECUs
communicate with one another using an standard protocol called Controller Area
Network (CAN). Securing communication among ECUs plays a vital role in
maintaining the safety and security of the vehicle. This paper proposes a
robust SDN-based False Data Detection and Mitigation System (FDDMS) for
in-vehicle networks. Leveraging the unique capabilities of Software-Defined
Networking (SDN), FDDMS is designed to monitor and detect false data injection
attacks in real-time. Specifically, we focus on brake-related ECUs within an
SDN-enabled in-vehicle network. First, we decode raw CAN data to create an
attack model that illustrates how false data can be injected into the system.
Then, FDDMS, incorporating a Long Short Term Memory (LSTM)-based detection
model, is used to identify false data injection attacks. We further propose an
effective variant of DeepFool attack to evaluate the model's robustness. To
countermeasure the impacts of four adversarial attacks including Fast gradient
descent method, Basic iterative method, DeepFool, and the DeepFool variant, we
further enhance a re-training technique method with a threshold based selection
strategy. Finally, a mitigation scheme is implemented to redirect attack
traffic by dynamically updating flow rules through SDN. Our experimental
results show that the proposed FDDMS is robust against adversarial attacks and
effectively detects and mitigates false data injection attacks in real-time.

</details>


### [174] [Scalable Gaussian Processes with Latent Kronecker Structure](https://arxiv.org/abs/2506.06895)
*Jihao Andreas Lin,Sebastian Ament,Maximilian Balandat,David Eriksson,José Miguel Hernández-Lobato,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出利用潜在Kronecker结构解决高斯过程应用于大数据集的计算扩展性问题，方法在真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高斯过程应用于大数据集存在计算扩展性问题，现有矩阵结构应用有近似或不现实假设，且在有数据缺失时结构会被破坏。

Method: 利用潜在Kronecker结构，将观测值的核矩阵表示为潜在Kronecker积的投影，结合迭代线性系统求解器和路径条件化。

Result: 方法在多达五百万样本的真实数据集（包括机器人、自动机器学习和气候应用）上优于现有稀疏和变分高斯过程。

Conclusion: 所提方法能在减少计算资源需求的同时实现精确高斯过程的推断。

Abstract: Applying Gaussian processes (GPs) to very large datasets remains a challenge
due to limited computational scalability. Matrix structures, such as the
Kronecker product, can accelerate operations significantly, but their
application commonly entails approximations or unrealistic assumptions. In
particular, the most common path to creating a Kronecker-structured kernel
matrix is by evaluating a product kernel on gridded inputs that can be
expressed as a Cartesian product. However, this structure is lost if any
observation is missing, breaking the Cartesian product structure, which
frequently occurs in real-world data such as time series. To address this
limitation, we propose leveraging latent Kronecker structure, by expressing the
kernel matrix of observed values as the projection of a latent Kronecker
product. In combination with iterative linear system solvers and pathwise
conditioning, our method facilitates inference of exact GPs while requiring
substantially fewer computational resources than standard iterative methods. We
demonstrate that our method outperforms state-of-the-art sparse and variational
GPs on real-world datasets with up to five million examples, including
robotics, automated machine learning, and climate applications.

</details>


### [175] [Near Optimal Non-asymptotic Sample Complexity of 1-Identification](https://arxiv.org/abs/2506.06978)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 研究1 - 识别问题，设计SEE算法，实现近最优性，数值结果显示算法有效。


<details>
  <summary>Details</summary>
Motivation: 现有文献中1 - 识别问题非渐近分析不明确，研究该问题的非渐近情况。

Method: 设计Sequential - Exploration - Exploitation (SEE)算法并进行非渐近理论分析。

Result: 实现近最优性，上下界差距为多项式对数因子，数值结果表明算法比现有基准有效。

Conclusion: SEE算法在1 - 识别问题的非渐近分析上表现良好，有实际效果。

Abstract: Motivated by an open direction in existing literature, we study the
1-identification problem, a fundamental multi-armed bandit formulation on pure
exploration. The goal is to determine whether there exists an arm whose mean
reward is at least a known threshold $\mu_0$, or to output None if it believes
such an arm does not exist. The agent needs to guarantee its output is correct
with probability at least $1-\delta$. Degenne & Koolen 2019 has established the
asymptotically tight sample complexity for the 1-identification problem, but
they commented that the non-asymptotic analysis remains unclear. We design a
new algorithm Sequential-Exploration-Exploitation (SEE), and conduct
theoretical analysis from the non-asymptotic perspective. Novel to the
literature, we achieve near optimality, in the sense of matching upper and
lower bounds on the pulling complexity. The gap between the upper and lower
bounds is up to a polynomial logarithmic factor. The numerical result also
indicates the effectiveness of our algorithm, compared to existing benchmarks.

</details>


### [176] [Certified Unlearning for Neural Networks](https://arxiv.org/abs/2506.06985)
*Anastasia Koloskova,Youssef Allouah,Animesh Jha,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 提出认证机器学习遗忘新方法，利用随机后处理连接遗忘和隐私放大，无需损失函数假设，理论分析与实验均有良好表现。


<details>
  <summary>Details</summary>
Motivation: 出于隐私和监管要求解决机器学习遗忘问题，现有方法有局限。

Method: 利用随机后处理中遗忘和隐私放大的联系，对保留数据进行噪声微调。

Result: 理论分析效率和准确性权衡，实验显示方法有效且优于现有基线。

Conclusion: 提出的方法能实现形式化遗忘保证，适用性广且实践表现良好。

Abstract: We address the problem of machine unlearning, where the goal is to remove the
influence of specific training data from a model upon request, motivated by
privacy concerns and regulatory requirements such as the "right to be
forgotten." Unfortunately, existing methods rely on restrictive assumptions or
lack formal guarantees. To this end, we propose a novel method for certified
machine unlearning, leveraging the connection between unlearning and privacy
amplification by stochastic post-processing. Our method uses noisy fine-tuning
on the retain data, i.e., data that does not need to be removed, to ensure
provable unlearning guarantees. This approach requires no assumptions about the
underlying loss function, making it broadly applicable across diverse settings.
We analyze the theoretical trade-offs in efficiency and accuracy and
demonstrate empirically that our method not only achieves formal unlearning
guarantees but also performs effectively in practice, outperforming existing
baselines. Our code is available at
https://github.com/stair-lab/certified-unlearningneural-networks-icml-2025

</details>


### [177] [Towards Physics-informed Diffusion for Anomaly Detection in Trajectories](https://arxiv.org/abs/2506.06999)
*Arun Sharma,Mingzhou Yang,Majid Farhadloo,Subhankar Ghosh,Bharat Jayaprakash,Shashi Shekhar*

Main category: cs.LG

TL;DR: 本文提出物理信息扩散模型检测异常轨迹，解决现有方法不足，实验证明其在异常检测和轨迹生成上效果更好。


<details>
  <summary>Details</summary>
Motivation: 解决国际水域非法活动中GPS欺骗异常轨迹检测问题，现有方法未考虑时空依赖和物理知识，误报率高。

Method: 提出集成运动学约束的物理信息扩散模型，识别不符合物理定律的轨迹。

Result: 在海事和城市领域真实数据集实验表明，该框架在异常检测中预测准确率更高，轨迹生成中估计错误率更低。

Conclusion: 所提物理信息扩散模型有效提升异常轨迹检测和轨迹生成的性能。

Abstract: Given trajectory data, a domain-specific study area, and a user-defined
threshold, we aim to find anomalous trajectories indicative of possible GPS
spoofing (e.g., fake trajectory). The problem is societally important to curb
illegal activities in international waters, such as unauthorized fishing and
illicit oil transfers. The problem is challenging due to advances in AI
generated in deep fakes generation (e.g., additive noise, fake trajectories)
and lack of adequate amount of labeled samples for ground-truth verification.
Recent literature shows promising results for anomalous trajectory detection
using generative models despite data sparsity. However, they do not consider
fine-scale spatiotemporal dependencies and prior physical knowledge, resulting
in higher false-positive rates. To address these limitations, we propose a
physics-informed diffusion model that integrates kinematic constraints to
identify trajectories that do not adhere to physical laws. Experimental results
on real-world datasets in the maritime and urban domains show that the proposed
framework results in higher prediction accuracy and lower estimation error rate
for anomaly detection and trajectory generation methods, respectively. Our
implementation is available at
https://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.

</details>


### [178] [Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning](https://arxiv.org/abs/2506.07040)
*Yang Xu,Swetha Ganesh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present the first $Q$-learning and actor-critic algorithms for robust
average reward Markov Decision Processes (MDPs) with non-asymptotic convergence
under contamination, TV distance and Wasserstein distance uncertainty sets. We
show that the robust $Q$ Bellman operator is a strict contractive mapping with
respect to a carefully constructed semi-norm with constant functions being
quotiented out. This property supports a stochastic approximation update, that
learns the optimal robust $Q$ function in $\tilde{\cO}(\epsilon^{-2})$ samples.
We also show that the same idea can be used for robust $Q$ function estimation,
which can be further used for critic estimation. Coupling it with theories in
robust policy mirror descent update, we present a natural actor-critic
algorithm that attains an $\epsilon$-optimal robust policy in
$\tilde{\cO}(\epsilon^{-3})$ samples. These results advance the theory of
distributionally robust reinforcement learning in the average reward setting.

</details>


### [179] [State Entropy Regularization for Robust Reinforcement Learning](https://arxiv.org/abs/2506.07085)
*Uri Koren,Yonatan Ashlag,Mirco Mutti,Esther Derman,Pierre-Luc Bacon,Shie Mannor*

Main category: cs.LG

TL;DR: 研究状态熵正则化在强化学习中的鲁棒性，对比策略熵正则化，指出状态熵鲁棒性对策略评估的滚动次数更敏感。


<details>
  <summary>Details</summary>
Motivation: 状态熵正则化在强化学习中经验上表现好，但缺乏理论保证，且标准鲁棒强化学习方法常忽略结构化和空间相关扰动。

Method: 对状态熵正则化的鲁棒性进行全面刻画，对比状态熵和策略熵正则化。

Result: 得出状态熵正则化对结构化和空间相关扰动有更好鲁棒性，还给出不同不确定性下的理论保证及表现不佳的场景。

Conclusion: 状态熵正则化相比策略熵正则化在鲁棒性上对策略评估的滚动次数更敏感。

Abstract: State entropy regularization has empirically shown better exploration and
sample complexity in reinforcement learning (RL). However, its theoretical
guarantees have not been studied. In this paper, we show that state entropy
regularization improves robustness to structured and spatially correlated
perturbations. These types of variation are common in transfer learning but
often overlooked by standard robust RL methods, which typically focus on small,
uncorrelated changes. We provide a comprehensive characterization of these
robustness properties, including formal guarantees under reward and transition
uncertainty, as well as settings where the method performs poorly. Much of our
analysis contrasts state entropy with the widely used policy entropy
regularization, highlighting their different benefits. Finally, from a
practical standpoint, we illustrate that compared with policy entropy, the
robustness advantages of state entropy are more sensitive to the number of
rollouts used for policy evaluation.

</details>


### [180] [Pointwise confidence estimation in the non-linear $\ell^2$-regularized least squares](https://arxiv.org/abs/2506.07088)
*Ilja Kuzborskij,Yasin Abbasi Yadkori*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider a high-probability non-asymptotic confidence estimation in the
$\ell^2$-regularized non-linear least-squares setting with fixed design. In
particular, we study confidence estimation for local minimizers of the
regularized training loss. We show a pointwise confidence bound, meaning that
it holds for the prediction on any given fixed test input $x$. Importantly, the
proposed confidence bound scales with similarity of the test input to the
training data in the implicit feature space of the predictor (for instance,
becoming very large when the test input lies far outside of the training data).
This desirable last feature is captured by the weighted norm involving the
inverse-Hessian matrix of the objective function, which is a generalized
version of its counterpart in the linear setting, $x^{\top} \text{Cov}^{-1} x$.
Our generalized result can be regarded as a non-asymptotic counterpart of the
classical confidence interval based on asymptotic normality of the MLE
estimator. We propose an efficient method for computing the weighted norm,
which only mildly exceeds the cost of a gradient computation of the loss
function. Finally, we complement our analysis with empirical evidence showing
that the proposed confidence bound provides better coverage/width trade-off
compared to a confidence estimation by bootstrapping, which is a gold-standard
method in many applications involving non-linear predictors such as neural
networks.

</details>


### [181] [CAtCh: Cognitive Assessment through Cookie Thief](https://arxiv.org/abs/2506.06603)
*Joseph T Colonel,Carolyn Hagler,Guiselle Wismer,Laura Curtis,Jacqueline Becker,Juan Wisnivesky,Alex Federman,Gaurav Pandey*

Main category: cs.LG

TL;DR: 评估多种基于语音的开源方法用于预测认知障碍，发现多模态方法优于单模态，声学方法优于语言学方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习算法多用于预测阿尔茨海默病及相关痴呆，尚未用于预测更广泛的认知障碍，本文旨在解决该问题。

Method: 评估原本用于预测ADRD的基于语音的开源方法以及多模态情感分析方法，利用患者音频记录预测CI。

Result: 多模态方法在CI预测中优于单模态方法，基于声学的方法优于基于语言学的方法，与情感和韵律相关的可解释声学特征表现显著优于基于BERT的语言特征和可解释语言特征。

Conclusion: 多模态和声学方法在预测认知障碍方面有更好的效果，相关代码开源。

Abstract: Several machine learning algorithms have been developed for the prediction of
Alzheimer's disease and related dementia (ADRD) from spontaneous speech.
However, none of these algorithms have been translated for the prediction of
broader cognitive impairment (CI), which in some cases is a precursor and risk
factor of ADRD. In this paper, we evaluated several speech-based open-source
methods originally proposed for the prediction of ADRD, as well as methods from
multimodal sentiment analysis for the task of predicting CI from patient audio
recordings. Results demonstrated that multimodal methods outperformed unimodal
ones for CI prediction, and that acoustics-based approaches performed better
than linguistics-based ones. Specifically, interpretable acoustic features
relating to affect and prosody were found to significantly outperform
BERT-based linguistic features and interpretable linguistic features,
respectively. All the code developed for this study is available at
https://github.com/JTColonel/catch.

</details>


### [182] [Stacey: Promoting Stochastic Steepest Descent via Accelerated $\ell_p$-Smooth Nonconvex Optimization](https://arxiv.org/abs/2506.06606)
*Xinyu Luo,Cedar Site Bai,Bolian Li,Petros Drineas,Ruqi Zhang,Brian Bullins*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While popular optimization methods such as SGD, AdamW, and Lion depend on
steepest descent updates in either $\ell_2$ or $\ell_\infty$ norms, there
remains a critical gap in handling the non-Euclidean structure observed in
modern deep networks training. In this work, we address this need by
introducing a new accelerated $\ell_p$ steepest descent algorithm, called
Stacey, which uses interpolated primal-dual iterate sequences to effectively
navigate non-Euclidean smooth optimization tasks. In addition to providing
novel theoretical guarantees for the foundations of our algorithm, we
empirically compare our approach against these popular methods on tasks
including image classification and language model (LLM) pretraining,
demonstrating both faster convergence and higher final accuracy. We further
evaluate different values of $p$ across various models and datasets,
underscoring the importance and efficiency of non-Euclidean approaches over
standard Euclidean methods. Code can be found at
https://github.com/xinyuluo8561/Stacey .

</details>


### [183] [PASS: Private Attributes Protection with Stochastic Data Substitution](https://arxiv.org/abs/2506.07308)
*Yizhuo Chen,Chun-Fu,Chen,Hsiang Hsu,Shaohan Hu,Tarek Abdelzaher*

Main category: cs.LG

TL;DR: 现有保护私有属性方法有漏洞，提出PASS方法，实验验证其有效性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有保护私有属性方法基于对抗训练策略有严重漏洞，需改进。

Method: 提出PASS方法，按一定概率随机替换原始样本，并使用从信息论目标导出的新损失函数进行训练。

Result: 在多种不同模态数据集上对PASS进行全面评估，证实其有效性和泛化性。

Conclusion: PASS方法能有效克服现有方法的局限性，可用于保护私有属性。

Abstract: The growing Machine Learning (ML) services require extensive collections of
user data, which may inadvertently include people's private information
irrelevant to the services. Various studies have been proposed to protect
private attributes by removing them from the data while maintaining the
utilities of the data for downstream tasks. Nevertheless, as we theoretically
and empirically show in the paper, these methods reveal severe vulnerability
because of a common weakness rooted in their adversarial training based
strategies. To overcome this limitation, we propose a novel approach, PASS,
designed to stochastically substitute the original sample with another one
according to certain probabilities, which is trained with a novel loss function
soundly derived from information-theoretic objective defined for
utility-preserving private attributes protection. The comprehensive evaluation
of PASS on various datasets of different modalities, including facial images,
human activity sensory signals, and voice recording datasets, substantiates
PASS's effectiveness and generalizability.

</details>


### [184] [Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning](https://arxiv.org/abs/2506.06632)
*Shubham Parashar,Shurui Gui,Xiner Li,Hongyi Ling,Sushil Vemuri,Blake Olson,Eric Li,Yu Zhang,James Caverlee,Dileep Kalathil,Shuiwang Ji*

Main category: cs.LG

TL;DR: 本文提出E2H Reasoner方法，结合课程学习思想，通过从易到难安排任务，利用强化学习提升语言模型推理能力，实验证明该方法对小语言模型有效。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明单纯使用强化学习提升语言模型在困难任务上的推理能力效果不佳，因此希望改进方法提升推理能力。

Method: 借鉴课程学习思想，提出E2H Reasoner方法，从易到难安排任务训练语言模型，并合理安排简单任务的淡出以防止过拟合。

Result: 在多个领域实验表明，E2H Reasoner显著提升了1.5B到3B小语言模型的推理能力。

Conclusion: E2H Reasoner方法有效，能提升小语言模型的推理能力。

Abstract: We aim to improve the reasoning capabilities of language models via
reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1
have demonstrated reasoning abilities on mathematical and coding tasks.
However, prior studies suggest that using RL alone to improve reasoning on
inherently difficult tasks is less effective. Here, we draw inspiration from
curriculum learning and propose to schedule tasks from easy to hard (E2H),
allowing LLMs to build reasoning skills gradually. Our method is termed E2H
Reasoner. Empirically, we observe that, although easy tasks are important
initially, fading them out through appropriate scheduling is essential in
preventing overfitting. Theoretically, we establish convergence guarantees for
E2H Reasoner within an approximate policy iteration framework. We derive
finite-sample complexity bounds and show that when tasks are appropriately
decomposed and conditioned, learning through curriculum stages requires fewer
total samples than direct learning. Experiments across multiple domains show
that E2H Reasoner significantly improves the reasoning ability of small LLMs
(1.5B to 3B), which otherwise struggle when trained with vanilla RL alone,
highlighting the effectiveness of our method.

</details>


### [185] [Moment Alignment: Unifying Gradient and Hessian Matching for Domain Generalization](https://arxiv.org/abs/2506.07378)
*Yuen Chen,Haozhe Si,Guojun Zhang,Han Zhao*

Main category: cs.LG

TL;DR: 本文提出领域泛化（DG）的矩对齐理论，引入CMA算法，解决现有方法计算效率低问题，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有DG方法计算效率低且原理不明，需更好理论和方法解决领域泛化问题。

Method: 基于转移度量扩展定义，证明跨域导数对齐可改善转移度量，建立特征学习与分类器拟合的对偶性，提出CMA算法。

Result: CMA算法克服现有基于梯度和Hessian方法的计算低效问题，在实验中表现优于经验风险最小化和现有先进算法。

Conclusion: 矩对齐理论为DG提供统一理解，CMA算法有效解决DG问题，提高泛化性能。

Abstract: Domain generalization (DG) seeks to develop models that generalize well to
unseen target domains, addressing the prevalent issue of distribution shifts in
real-world applications. One line of research in DG focuses on aligning
domain-level gradients and Hessians to enhance generalization. However,
existing methods are computationally inefficient and the underlying principles
of these approaches are not well understood. In this paper, we develop the
theory of moment alignment for DG. Grounded in \textit{transfer measure}, a
principled framework for quantifying generalizability between two domains, we
first extend the definition of transfer measure to domain generalization that
includes multiple source domains and establish a target error bound. Then, we
prove that aligning derivatives across domains improves transfer measure both
when the feature extractor induces an invariant optimal predictor across
domains and when it does not. Notably, moment alignment provides a unifying
understanding of Invariant Risk Minimization, gradient matching, and Hessian
matching, three previously disconnected approaches to DG. We further connect
feature moments and derivatives of the classifier head, and establish the
duality between feature learning and classifier fitting. Building upon our
theory, we introduce \textbf{C}losed-Form \textbf{M}oment \textbf{A}lignment
(CMA), a novel DG algorithm that aligns domain-level gradients and Hessians in
closed-form. Our method overcomes the computational inefficiencies of existing
gradient and Hessian-based techniques by eliminating the need for repeated
backpropagation or sampling-based Hessian estimation. We validate the efficacy
of our approach through two sets of experiments: linear probing and full
fine-tuning. CMA demonstrates superior performance in both settings compared to
Empirical Risk Minimization and state-of-the-art algorithms.

</details>


### [186] [Vision-QRWKV: Exploring Quantum-Enhanced RWKV Models for Image Classification](https://arxiv.org/abs/2506.06633)
*Chi-Sheng Chen*

Main category: cs.LG

TL;DR: 本文提出Vision - QRWKV用于图像分类，在多个数据集上验证量子增强模型表现更优，为视觉领域量子模型应用提供见解。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习有提升经典神经网络架构的潜力，要将RWKV架构拓展到图像分类任务并提升视觉表示能力。

Method: 在RWKV的通道混合组件中集成变分量子电路（VQC）形成Vision - QRWKV模型，并在14个医学和标准图像分类基准数据集上评估经典和量子RWKV模型。

Result: 量子增强模型在大多数数据集上优于经典模型，尤其在有细微或噪声类别区分的数据集上。

Conclusion: 这是量子增强RWKV在视觉领域的首次系统应用，揭示了架构权衡和量子模型在轻量级高效视觉任务中的未来潜力。

Abstract: Recent advancements in quantum machine learning have shown promise in
enhancing classical neural network architectures, particularly in domains
involving complex, high-dimensional data. Building upon prior work in temporal
sequence modeling, this paper introduces Vision-QRWKV, a hybrid
quantum-classical extension of the Receptance Weighted Key Value (RWKV)
architecture, applied for the first time to image classification tasks. By
integrating a variational quantum circuit (VQC) into the channel mixing
component of RWKV, our model aims to improve nonlinear feature transformation
and enhance the expressive capacity of visual representations.
  We evaluate both classical and quantum RWKV models on a diverse collection of
14 medical and standard image classification benchmarks, including MedMNIST
datasets, MNIST, and FashionMNIST. Our results demonstrate that the
quantum-enhanced model outperforms its classical counterpart on a majority of
datasets, particularly those with subtle or noisy class distinctions (e.g.,
ChestMNIST, RetinaMNIST, BloodMNIST). This study represents the first
systematic application of quantum-enhanced RWKV in the visual domain, offering
insights into the architectural trade-offs and future potential of quantum
models for lightweight and efficient vision tasks.

</details>


### [187] [Explicit Preference Optimization: No Need for an Implicit Reward Model](https://arxiv.org/abs/2506.07492)
*Xiangkun Hu,Lemin Kong,Tong He,David Wipf*

Main category: cs.LG

TL;DR: 文章指出基于人类反馈的强化学习调优大语言模型存在问题，DPO虽有改进但仍有不足，提出EXPO框架并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: RLHF训练流程复杂，DPO及其变体虽简化但存在次优正则化和反直觉插值行为等问题，需更优方法。

Method: 引入无需重新参数化以实现隐式奖励的EXPO框架，从新设定正则化因子。

Result: 实证结果证实了分析，展示了EXPO的有效性。

Conclusion: EXPO框架能避免DPO变体的潜在问题，满足先前方法未达到的正则化要求。

Abstract: The generated responses of large language models (LLMs) are often fine-tuned
to human preferences through a process called reinforcement learning from human
feedback (RLHF). As RLHF relies on a challenging training sequence, whereby a
separate reward model is independently learned and then later applied to LLM
policy updates, ongoing research effort has targeted more straightforward
alternatives. In this regard, direct preference optimization (DPO) and its many
offshoots circumvent the need for a separate reward training step. Instead,
through the judicious use of a reparameterization trick that induces an
\textit{implicit} reward, DPO and related methods consolidate learning to the
minimization of a single loss function. And yet despite demonstrable success in
some real-world settings, we prove that DPO-based objectives are nonetheless
subject to sub-optimal regularization and counter-intuitive interpolation
behaviors, underappreciated artifacts of the reparameterizations upon which
they are based. To this end, we introduce an \textit{explicit} preference
optimization framework termed EXPO that requires no analogous
reparameterization to achieve an implicit reward. Quite differently, we merely
posit intuitively-appealing regularization factors from scratch that
transparently avoid the potential pitfalls of key DPO variants, provably
satisfying regularization desiderata that prior methods do not. Empirical
results serve to corroborate our analyses and showcase the efficacy of EXPO.

</details>


### [188] [Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning](https://arxiv.org/abs/2506.06637)
*Olimjon Toirov,Wei Yu*

Main category: cs.LG

TL;DR: 本文提出结合“图像负载特征”和持续学习的非侵入式负载监测方法，实验表明该方法识别准确率显著提高。


<details>
  <summary>Details</summary>
Motivation: 传统非侵入式负载监测（NILM）方法存在特征鲁棒性差、模型泛化不足的问题，复杂多变的负载组合和应用环境加剧了这些挑战。

Method: 将电流、电压、功率因数等多维功率信号转换为可视化图像负载特征，结合深度卷积神经网络实现多设备识别分类；引入自监督预训练提高特征泛化能力，采用持续在线学习策略克服模型遗忘问题。

Result: 在高采样率负载数据集上进行大量实验，与多种现有方法和模型变体对比，提出的方法在识别准确率上有显著提升。

Conclusion: 所提出的结合“图像负载特征”和持续学习的非侵入式负载监测方法有效，能提高识别准确率。

Abstract: Non-Intrusive Load Monitoring (NILM) identifies the operating status and
energy consumption of each electrical device in the circuit by analyzing the
electrical signals at the bus, which is of great significance for smart power
management. However, the complex and changeable load combinations and
application environments lead to the challenges of poor feature robustness and
insufficient model generalization of traditional NILM methods. To this end,
this paper proposes a new non-intrusive load monitoring method that integrates
"image load signature" and continual learning. This method converts
multi-dimensional power signals such as current, voltage, and power factor into
visual image load feature signatures, and combines deep convolutional neural
networks to realize the identification and classification of multiple devices;
at the same time, self-supervised pre-training is introduced to improve feature
generalization, and continual online learning strategies are used to overcome
model forgetting to adapt to the emergence of new loads. This paper conducts a
large number of experiments on high-sampling rate load datasets, and compares a
variety of existing methods and model variants. The results show that the
proposed method has achieved significant improvements in recognition accuracy.

</details>


### [189] [Flowing Datasets with Wasserstein over Wasserstein Gradient Flows](https://arxiv.org/abs/2506.07534)
*Clément Bonet,Christophe Vauthier,Anna Korba*

Main category: cs.LG

TL;DR: 论文针对机器学习中概率分布数据提出用混合分布表示数据集，定义了WoW梯度流，并应用于迁移学习和数据集蒸馏任务。


<details>
  <summary>Details</summary>
Motivation: 机器学习中概率分布数据的出现需要在无穷维对象上设计可处理的梯度流的新技术，如流动标记数据集是很多应用的核心任务。

Method: 用关联特征的条件分布表示每个类，将数据集建模为类上的混合分布，赋予空间最优传输的度量结构，推导微分结构，定义WoW梯度流，使用基于切片Wasserstein核的最大均值差异的新函数。

Result: 成功定义了WoW梯度流，可设计使给定目标泛函减小的动态。

Conclusion: 所提出的框架可应用于迁移学习和数据集蒸馏任务。

Abstract: Many applications in machine learning involve data represented as probability
distributions. The emergence of such data requires radically novel techniques
to design tractable gradient flows on probability distributions over this type
of (infinite-dimensional) objects. For instance, being able to flow labeled
datasets is a core task for applications ranging from domain adaptation to
transfer learning or dataset distillation. In this setting, we propose to
represent each class by the associated conditional distribution of features,
and to model the dataset as a mixture distribution supported on these classes
(which are themselves probability distributions), meaning that labeled datasets
can be seen as probability distributions over probability distributions. We
endow this space with a metric structure from optimal transport, namely the
Wasserstein over Wasserstein (WoW) distance, derive a differential structure on
this space, and define WoW gradient flows. The latter enables to design
dynamics over this space that decrease a given objective functional. We apply
our framework to transfer learning and dataset distillation tasks, leveraging
our gradient flow construction as well as novel tractable functionals that take
the form of Maximum Mean Discrepancies with Sliced-Wasserstein based kernels
between probability distributions.

</details>


### [190] [Exploiting Curvature in Online Convex Optimization with Delayed Feedback](https://arxiv.org/abs/2506.07595)
*Hao Qiu,Emmanuel Esposito,Mengxiao Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work, we study the online convex optimization problem with curved
losses and delayed feedback. When losses are strongly convex, existing
approaches obtain regret bounds of order $d_{\max} \ln T$, where $d_{\max}$ is
the maximum delay and $T$ is the time horizon. However, in many cases, this
guarantee can be much worse than $\sqrt{d_{\mathrm{tot}}}$ as obtained by a
delayed version of online gradient descent, where $d_{\mathrm{tot}}$ is the
total delay. We bridge this gap by proposing a variant of
follow-the-regularized-leader that obtains regret of order
$\min\{\sigma_{\max}\ln T, \sqrt{d_{\mathrm{tot}}}\}$, where $\sigma_{\max}$ is
the maximum number of missing observations. We then consider exp-concave losses
and extend the Online Newton Step algorithm to handle delays with an adaptive
learning rate tuning, achieving regret $\min\{d_{\max} n\ln T,
\sqrt{d_{\mathrm{tot}}}\}$ where $n$ is the dimension. To our knowledge, this
is the first algorithm to achieve such a regret bound for exp-concave losses.
We further consider the problem of unconstrained online linear regression and
achieve a similar guarantee by designing a variant of the Vovk-Azoury-Warmuth
forecaster with a clipping trick. Finally, we implement our algorithms and
conduct experiments under various types of delay and losses, showing an
improved performance over existing methods.

</details>


### [191] [The Universality Lens: Why Even Highly Over-Parametrized Models Learn Well](https://arxiv.org/abs/2506.07661)
*Meir Feder,Ruediger Urbanke,Yaniv Fogel*

Main category: cs.LG

TL;DR: 本文从信息论角度研究大参数模型泛化性好的原因，给出模型简单性概念，桥接理论与实践，得出非均匀后悔界。


<details>
  <summary>Details</summary>
Motivation: 探究大参数模型参数远超训练样本时仍泛化性好的原因。

Method: 基于通用学习理论，研究具有对数损失和几乎均匀先验的贝叶斯混合学习器。

Result: 学习器的后悔值由接近真实数据生成过程的模型累积概率决定，得出模型简单性概念，分析与实践概念相符。

Conclusion: 为过参数化模型避免过拟合提供严格直观解释，统一且有原则地理解现代AI系统泛化行为。

Abstract: A fundamental question in modern machine learning is why large,
over-parameterized models, such as deep neural networks and transformers, tend
to generalize well, even when their number of parameters far exceeds the number
of training samples.
  We investigate this phenomenon through the lens of information theory,
grounded in universal learning theory. Specifically, we study a Bayesian
mixture learner with log-loss and (almost) uniform prior over an expansive
hypothesis class.
  Our key result shows that the learner's regret is not determined by the
overall size of the hypothesis class, but rather by the cumulative probability
of all models that are close, in Kullback-Leibler divergence distance, to the
true data-generating process. We refer to this cumulative probability as the
weight of the hypothesis.
  This leads to a natural notion of model simplicity: simple models are those
with large weight and thus require fewer samples to generalize, while complex
models have small weight and need more data. This perspective provides a
rigorous and intuitive explanation for why over-parameterized models often
avoid overfitting: the presence of simple hypotheses allows the posterior to
concentrate on them when supported by the data.
  We further bridge theory and practice by recalling that stochastic gradient
descent with Langevin dynamics samples from the correct posterior distribution,
enabling our theoretical learner to be approximated using standard machine
learning methods combined with ensemble learning.
  Our analysis yields non-uniform regret bounds and aligns with key practical
concepts such as flat minima and model distillation. The results apply broadly
across online, batch, and supervised learning settings, offering a unified and
principled understanding of the generalization behavior of modern AI systems.

</details>


### [192] [E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time](https://arxiv.org/abs/2506.07747)
*Adam Breuer*

Main category: cs.LG

TL;DR: 本文为LDA主题模型中文档主题推断问题提供首个有理论保证的实用算法，速度快、有可解释性且适用于因果推断，实际表现好。


<details>
  <summary>Details</summary>
Motivation: 解决社会科学、数据探索和因果推断等应用中LDA主题模型的主要推断问题。

Method: 提出一种基于组合、非梯度的新方法来估计主题模型。

Result: 算法能在对数并行计算时间内收敛到接近最优的后验概率，比已知LDA算法快指数倍；可提供可解释性保证；适用于下游因果推断；在不同文本数据集和评估参数下，返回的解决方案语义质量更高。

Conclusion: 所提出的方法在LDA主题模型的文档主题推断问题上表现出色，具有多方面优势。

Abstract: In this paper, we provide the first practical algorithms with provable
guarantees for the problem of inferring the topics assigned to each document in
an LDA topic model. This is the primary inference problem for many applications
of topic models in social science, data exploration, and causal inference
settings. We obtain this result by showing a novel non-gradient-based,
combinatorial approach to estimating topic models. This yields algorithms that
converge to near-optimal posterior probability in logarithmic parallel
computation time (adaptivity) -- exponentially faster than any known LDA
algorithm. We also show that our approach can provide interpretability
guarantees such that each learned topic is formally associated with a known
keyword. Finally, we show that unlike alternatives, our approach can maintain
the independence assumptions necessary to use the learned topic model for
downstream causal inference methods that allow researchers to study topics as
treatments. In terms of practical performance, our approach consistently
returns solutions of higher semantic quality than solutions from
state-of-the-art LDA algorithms, neural topic models, and LLM-based topic
models across a diverse range of text datasets and evaluation parameters.

</details>


### [193] [SDP-CROWN: Efficient Bound Propagation for Neural Network Verification with Tightness of Semidefinite Programming](https://arxiv.org/abs/2506.06665)
*Hong-Ming Chiu,Hao Chen,Huan Zhang,Richard Y. Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Neural network verifiers based on linear bound propagation scale impressively
to massive models but can be surprisingly loose when neuron coupling is
crucial. Conversely, semidefinite programming (SDP) verifiers capture
inter-neuron coupling naturally, but their cubic complexity restricts them to
only small models. In this paper, we propose SDP-CROWN, a novel hybrid
verification framework that combines the tightness of SDP relaxations with the
scalability of bound-propagation verifiers. At the core of SDP-CROWN is a new
linear bound, derived via SDP principles, that explicitly captures
$\ell_{2}$-norm-based inter-neuron coupling while adding only one extra
parameter per layer. This bound can be integrated seamlessly into any linear
bound-propagation pipeline, preserving the inherent scalability of such methods
yet significantly improving tightness. In theory, we prove that our
inter-neuron bound can be up to a factor of $\sqrt{n}$ tighter than traditional
per-neuron bounds. In practice, when incorporated into the state-of-the-art
$\alpha$-CROWN verifier, we observe markedly improved verification performance
on large models with up to 65 thousand neurons and 2.47 million parameters,
achieving tightness that approaches that of costly SDP-based methods.

</details>


### [194] [Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability](https://arxiv.org/abs/2506.07804)
*Jie Bao,Chuangyin Dang,Rui Luo,Hanwei Zhang,Zhixin Zhou*

Main category: cs.LG

TL;DR: 本文结合共形预测原理推进对抗训练，提出OPSA攻击方法和OPSA - AT防御策略，实验表明攻击方法增加不确定性，防御模型增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型用于高风险应用时，需对抗对抗攻击的鲁棒防御和可靠性能保证，且仅准确率不足以提供保证和不确定性估计。

Method: 开发OPSA攻击方法，引入OPSA - AT防御策略，将OPSA集成到新的共形训练范式中。

Result: OPSA攻击方法比基线方法引入更多不确定性，OPSA - AT防御模型不仅对抗OPSA，还增强了对其他对抗攻击的鲁棒性，保持可靠预测。

Conclusion: 集成方法对安全关键领域开发可信且有弹性的深度学习模型有效。

Abstract: As deep learning models are increasingly deployed in high-risk applications,
robust defenses against adversarial attacks and reliable performance guarantees
become paramount. Moreover, accuracy alone does not provide sufficient
assurance or reliable uncertainty estimates for these models. This study
advances adversarial training by leveraging principles from Conformal
Prediction. Specifically, we develop an adversarial attack method, termed OPSA
(OPtimal Size Attack), designed to reduce the efficiency of conformal
prediction at any significance level by maximizing model uncertainty without
requiring coverage guarantees. Correspondingly, we introduce OPSA-AT
(Adversarial Training), a defense strategy that integrates OPSA within a novel
conformal training paradigm. Experimental evaluations demonstrate that our OPSA
attack method induces greater uncertainty compared to baseline approaches for
various defenses. Conversely, our OPSA-AT defensive model significantly
enhances robustness not only against OPSA but also other adversarial attacks,
and maintains reliable prediction. Our findings highlight the effectiveness of
this integrated approach for developing trustworthy and resilient deep learning
models for safety-critical domains. Our code is available at
https://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction.

</details>


### [195] [Learning Robust Heterogeneous Graph Representations via Contrastive-Reconstruction under Sparse Semantics](https://arxiv.org/abs/2506.06682)
*Di Lin,Wanjing Ren,Xuanbin Li,Rui Zhang*

Main category: cs.LG

TL;DR: 本文提出用于异质图的双通道自监督学习框架HetCRF，实验证明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有同质图混合框架在设计共享编码器、语义稀疏场景视图构建及正负样本梯度不平衡方面存在挑战，缺乏适用于异质图的有效方法。

Method: 提出HetCRF框架，采用两阶段聚合策略适配嵌入语义，增强编码器输出用于视图构建，提出两种正样本增强策略平衡梯度贡献。

Result: 在四个真实异质图数据集的节点分类实验中，HetCRF优于现有基线，在部分数据集上Macro - F1得分有显著提升。

Conclusion: HetCRF框架有效且具有优越性。

Abstract: In graph self-supervised learning, masked autoencoders (MAE) and contrastive
learning (CL) are two prominent paradigms. MAE focuses on reconstructing masked
elements, while CL maximizes similarity between augmented graph views. Recent
studies highlight their complementarity: MAE excels at local feature capture,
and CL at global information extraction. Hybrid frameworks for homogeneous
graphs have been proposed, but face challenges in designing shared encoders to
meet the semantic requirements of both tasks. In semantically sparse scenarios,
CL struggles with view construction, and gradient imbalance between positive
and negative samples persists. This paper introduces HetCRF, a novel
dual-channel self-supervised learning framework for heterogeneous graphs.
HetCRF uses a two-stage aggregation strategy to adapt embedding semantics,
making it compatible with both MAE and CL. To address semantic sparsity, it
enhances encoder output for view construction instead of relying on raw
features, improving efficiency. Two positive sample augmentation strategies are
also proposed to balance gradient contributions. Node classification
experiments on four real-world heterogeneous graph datasets demonstrate that
HetCRF outperforms state-of-the-art baselines. On datasets with missing node
features, such as Aminer and Freebase, at a 40% label rate in node
classification, HetCRF improves the Macro-F1 score by 2.75% and 2.2%
respectively compared to the second-best baseline, validating its effectiveness
and superiority.

</details>


### [196] [Residual Reweighted Conformal Prediction for Graph Neural Networks](https://arxiv.org/abs/2506.07854)
*Zheng Zhang,Jie Bao,Zhixin Zhou,Nicolo Colombo,Lixin Cheng,Rui Luo*

Main category: cs.LG

TL;DR: 针对GNN在高风险领域因不确定性量化问题及现有CP方法不足，提出RR - GNN框架，通过三项创新提升性能，经15个真实图验证优于基线方法。


<details>
  <summary>Details</summary>
Motivation: GNN在高风险领域面临不确定性量化挑战，现有CP方法存在保守、未考虑图异质性和结构偏差等问题，部分改进方法有缺陷。

Method: 提出RR - GNN框架，采用Graph - Structured Mondrian CP分区，使用Residual - Adaptive Nonconformity Scores调整区间，采用Cross - Training Protocol防止信息泄露。

Result: 在15个真实图的多种任务上验证，相比CP基线，RR - GNN效率提升且不损失覆盖率。

Conclusion: RR - GNN框架能有效解决现有问题，提升GNN在高风险领域的预测性能。

Abstract: Graph Neural Networks (GNNs) excel at modeling relational data but face
significant challenges in high-stakes domains due to unquantified uncertainty.
Conformal prediction (CP) offers statistical coverage guarantees, but existing
methods often produce overly conservative prediction intervals that fail to
account for graph heteroscedasticity and structural biases. While residual
reweighting CP variants address some of these limitations, they neglect graph
topology, cluster-specific uncertainties, and risk data leakage by reusing
training sets. To address these issues, we propose Residual Reweighted GNN
(RR-GNN), a framework designed to generate minimal prediction sets with
provable marginal coverage guarantees.
  RR-GNN introduces three major innovations to enhance prediction performance.
First, it employs Graph-Structured Mondrian CP to partition nodes or edges into
communities based on topological features, ensuring cluster-conditional
coverage that reflects heterogeneity. Second, it uses Residual-Adaptive
Nonconformity Scores by training a secondary GNN on a held-out calibration set
to estimate task-specific residuals, dynamically adjusting prediction intervals
according to node or edge uncertainty. Third, it adopts a Cross-Training
Protocol, which alternates the optimization of the primary GNN and the residual
predictor to prevent information leakage while maintaining graph dependencies.
We validate RR-GNN on 15 real-world graphs across diverse tasks, including node
classification, regression, and edge weight prediction. Compared to CP
baselines, RR-GNN achieves improved efficiency over state-of-the-art methods,
with no loss of coverage.

</details>


### [197] [Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning](https://arxiv.org/abs/2506.06694)
*Yuan Yuan,Yukun Liu,Chonghua Han,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: 提出MoveGCL框架用于训练移动性基础模型，在六个城市数据集实验中表现良好且保护隐私，为移动性基础模型发展提供蓝图。


<details>
  <summary>Details</summary>
Motivation: 构建人类移动性基础模型因数据隐私和数据孤岛问题具有挑战性，需填补此空白。

Method: 提出MoveGCL框架，通过生成式持续学习，重放教师模型生成的合成轨迹，采用蒸馏策略，结合Mixture-of-Experts Transformer和移动性感知路由机制，采用逐层渐进适应策略。

Result: 在六个真实城市数据集实验中，MoveGCL性能与联合训练相当，显著优于联邦学习基线，且提供强隐私保护。

Conclusion: MoveGCL是解锁移动性基础模型的关键一步，为开放、可扩展和隐私保护的模型开发提供实用蓝图。

Abstract: Foundation models have revolutionized fields such as natural language
processing and computer vision by enabling general-purpose learning across
diverse tasks and datasets. However, building analogous models for human
mobility remains challenging due to the privacy-sensitive nature of mobility
data and the resulting data silos across institutions. To bridge this gap, we
propose MoveGCL, a scalable and privacy-preserving framework for training
mobility foundation models via generative continual learning. Without sharing
raw data, MoveGCL enables decentralized and progressive model evolution by
replaying synthetic trajectories generated from a frozen teacher model, and
reinforces knowledge retention through a tailored distillation strategy that
mitigates catastrophic forgetting. To address the heterogeneity of mobility
patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a
mobility-aware expert routing mechanism, and employs a layer-wise progressive
adaptation strategy to stabilize continual updates. Experiments on six
real-world urban datasets demonstrate that MoveGCL achieves performance
comparable to joint training and significantly outperforms federated learning
baselines, while offering strong privacy protection. MoveGCL marks a crucial
step toward unlocking foundation models for mobility, offering a practical
blueprint for open, scalable, and privacy-preserving model development in the
era of foundation models.

</details>


### [198] [MarginSel : Max-Margin Demonstration Selection for LLMs](https://arxiv.org/abs/2506.06699)
*Rajeev Bhatt Ambati,James Lester,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.LG

TL;DR: 提出MarginSel方法用于大语言模型上下文学习中示范示例选择，在分类任务上F1分数有绝对提升，并有理论和实证支持。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型上下文学习中示范示例选择和排序影响有效性的问题。

Method: 提出MarginSel这一两步法，为上下文学习提示选择困难示范示例并适应每个测试实例。

Result: 在分类任务中F1分数相比随机选择示例有2 - 7%的绝对提升，能诱导大语言模型产生最大间隔行为。

Conclusion: MarginSel方法能有效提高大语言模型上下文学习的效果，通过增加困难示例的间隔来使决策边界向有利方向移动。

Abstract: Large Language Models (LLMs) excel at few-shot learning via in-context
learning (ICL). However, the effectiveness of ICL is often sensitive to the
selection and ordering of demonstration examples. To address this, we present
MarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that
selects hard demonstration examples for the ICL prompt, adapting to each test
instance. Our approach achieves 2-7% absolute improvement in F1-score across
classification tasks, compared to a random selection of examples. We also
provide theoretical insights and empirical evidence showing that MarginSel
induces max-margin behavior in LLMs by effectively increasing the margin for
hard examples, analogous to support vectors, thereby shifting the decision
boundary in a beneficial direction.

</details>


### [199] [Diffusion Counterfactual Generation with Semantic Abduction](https://arxiv.org/abs/2506.07883)
*Rajat Rasal,Avinash Kori,Fabio De Sousa Ribeiro,Tian Xia,Ben Glocker*

Main category: cs.LG

TL;DR: 现有自动编码框架在反事实图像生成上有局限，本文提出基于扩散模型的因果机制和框架，实现语义控制及平衡因果控制与身份保留。


<details>
  <summary>Details</summary>
Motivation: 现有自动编码框架在反事实图像生成中存在可扩展性和保真度问题，而扩散模型有改进机会。

Method: 提出空间、语义和动态溯因概念，通过Pearlian因果关系将语义表征集成到扩散模型中，通过反事实推理过程编辑图像。

Result: 首次考虑扩散反事实的高级语义身份保留，展示语义控制能在忠实因果控制和身份保留间实现权衡。

Conclusion: 所提基于扩散模型的框架可用于反事实图像编辑，能平衡因果控制和身份保留。

Abstract: Counterfactual image generation presents significant challenges, including
preserving identity, maintaining perceptual quality, and ensuring faithfulness
to an underlying causal model. While existing auto-encoding frameworks admit
semantic latent spaces which can be manipulated for causal control, they
struggle with scalability and fidelity. Advancements in diffusion models
present opportunities for improving counterfactual image editing, having
demonstrated state-of-the-art visual quality, human-aligned perception and
representation learning capabilities. Here, we present a suite of
diffusion-based causal mechanisms, introducing the notions of spatial, semantic
and dynamic abduction. We propose a general framework that integrates semantic
representations into diffusion models through the lens of Pearlian causality to
edit images via a counterfactual reasoning process. To our knowledge, this is
the first work to consider high-level semantic identity preservation for
diffusion counterfactuals and to demonstrate how semantic control enables
principled trade-offs between faithful causal control and identity
preservation.

</details>


### [200] [Do Protein Transformers Have Biological Intelligence?](https://arxiv.org/abs/2506.06701)
*Fudong Lin,Wanrou Du,Jinchan Liu,Tarikul Milon,Shelby Meche,Wu Xu,Xiaoqi Qin,Xu Yuan*

Main category: cs.LG

TL;DR: 本文探索蛋白Transformer能否捕捉蛋白序列生物智能，引入Protein - FN数据集，设计SPT架构和Sequence Score技术，小模型预测精度高且技术能揭示有意义模式，数据集和代码已开源。


<details>
  <summary>Details</summary>
Motivation: 探索蛋白Transformer能否捕捉蛋白序列中的生物智能。

Method: 引入Protein - FN数据集，设计Sequence Protein Transformers (SPT)架构进行蛋白功能预测，开发Sequence Score可解释AI技术。

Result: 最小的SPT - Tiny模型预测精度高，在AR数据集达94.3%，在Protein - FN数据集达99.6%；Sequence Score技术揭示SPT模型能发现有意义模式。

Conclusion: 蛋白Transformer能捕捉蛋白序列生物智能，所提方法有效。

Abstract: Deep neural networks, particularly Transformers, have been widely adopted for
predicting the functional properties of proteins. In this work, we focus on
exploring whether Protein Transformers can capture biological intelligence
among protein sequences. To achieve our goal, we first introduce a protein
function dataset, namely Protein-FN, providing over 9000 protein data with
meaningful labels. Second, we devise a new Transformer architecture, namely
Sequence Protein Transformers (SPT), for computationally efficient protein
function predictions. Third, we develop a novel Explainable Artificial
Intelligence (XAI) technique called Sequence Score, which can efficiently
interpret the decision-making processes of protein models, thereby overcoming
the difficulty of deciphering biological intelligence bided in Protein
Transformers. Remarkably, even our smallest SPT-Tiny model, which contains only
5.4M parameters, demonstrates impressive predictive accuracy, achieving 94.3%
on the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset,
all accomplished by training from scratch. Besides, our Sequence Score
technique helps reveal that our SPT models can discover several meaningful
patterns underlying the sequence structures of protein data, with these
patterns aligning closely with the domain knowledge in the biology community.
We have officially released our Protein-FN dataset on Hugging Face Datasets
https://huggingface.co/datasets/Protein-FN/Protein-FN. Our code is available at
https://github.com/fudong03/BioIntelligence.

</details>


### [201] [FunDiff: Diffusion Models over Function Spaces for Physics-Informed Generative Modeling](https://arxiv.org/abs/2506.07902)
*Sifan Wang,Zehao Dou,Tong-Rui Liu,Lu Lu*

Main category: cs.LG

TL;DR: 提出FunDiff框架用于函数空间生成建模，结合潜在扩散过程与函数自编码器架构，有理论保证且在多应用中有效。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型适应物理应用有挑战，物理量是受复杂物理定律支配的连续函数。

Method: 引入FunDiff框架，结合潜在扩散过程与函数自编码器架构，通过架构约束或物理信息损失函数融入物理先验。

Result: 理论上为函数空间密度估计建立极小极大最优性保证，实证表明方法能生成物理一致样本，对噪声和低分辨率数据有鲁棒性。

Conclusion: FunDiff框架在函数空间生成建模中有效，可应用于流体动力学和固体力学等领域。

Abstract: Recent advances in generative modeling -- particularly diffusion models and
flow matching -- have achieved remarkable success in synthesizing discrete data
such as images and videos. However, adapting these models to physical
applications remains challenging, as the quantities of interest are continuous
functions governed by complex physical laws. Here, we introduce
$\textbf{FunDiff}$, a novel framework for generative modeling in function
spaces. FunDiff combines a latent diffusion process with a function autoencoder
architecture to handle input functions with varying discretizations, generate
continuous functions evaluable at arbitrary locations, and seamlessly
incorporate physical priors. These priors are enforced through architectural
constraints or physics-informed loss functions, ensuring that generated samples
satisfy fundamental physical laws. We theoretically establish minimax
optimality guarantees for density estimation in function spaces, showing that
diffusion-based estimators achieve optimal convergence rates under suitable
regularity conditions. We demonstrate the practical effectiveness of FunDiff
across diverse applications in fluid dynamics and solid mechanics. Empirical
results show that our method generates physically consistent samples with high
fidelity to the target distribution and exhibits robustness to noisy and
low-resolution data. Code and datasets are publicly available at
https://github.com/sifanexisted/fundiff.

</details>


### [202] [CausalPFN: Amortized Causal Effect Estimation via In-Context Learning](https://arxiv.org/abs/2506.07918)
*Vahid Balazadeh,Hamidreza Kamkari,Valentin Thomas,Benson Li,Junwei Ma,Jesse C. Cresswell,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: 提出CausalPFN变压器模型用于观测数据因果效应估计，在基准测试和实际任务中表现良好，迈向自动因果推断。


<details>
  <summary>Details</summary>
Motivation: 从观测数据估计因果效应时，从众多专业方法中选择合适估计器需大量人力和领域知识。

Method: 提出CausalPFN，结合贝叶斯因果推断和先验拟合网络大规模训练协议，经大量模拟数据训练后直接从原始观测映射到因果效应。

Result: 在异质和平均治疗效果估计基准测试中平均性能优越，在实际政策制定的提升建模任务中表现有竞争力，能提供校准的不确定性估计。

Conclusion: CausalPFN无需进一步训练或调优，迈向自动因果推断。

Abstract: Causal effect estimation from observational data is fundamental across
various applications. However, selecting an appropriate estimator from dozens
of specialized methods demands substantial manual effort and domain expertise.
We present CausalPFN, a single transformer that amortizes this workflow:
trained once on a large library of simulated data-generating processes that
satisfy ignorability, it infers causal effects for new observational datasets
out-of-the-box. CausalPFN combines ideas from Bayesian causal inference with
the large-scale training protocol of prior-fitted networks (PFNs), learning to
map raw observations directly to causal effects without any task-specific
adjustment. Our approach achieves superior average performance on heterogeneous
and average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC).
Moreover, it shows competitive performance for real-world policy making on
uplift modeling tasks. CausalPFN provides calibrated uncertainty estimates to
support reliable decision-making based on Bayesian principles. This
ready-to-use model does not require any further training or tuning and takes a
step toward automated causal inference (https://github.com/vdblm/CausalPFN).

</details>


### [203] [The OCR Quest for Generalization: Learning to recognize low-resource alphabets with model editing](https://arxiv.org/abs/2506.06761)
*Adrià Molina Rodríguez,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.LG

TL;DR: 本文旨在构建能快速泛化到新数据分布的模型，利用模型编辑技术实现低资源学习，实验显示性能提升，为构建适配未充分代表字母表的模型提供新方法。


<details>
  <summary>Details</summary>
Motivation: 识别系统在不同领域的鲁棒性很重要，但低资源语言因数据不足常被排除在大规模预训练和基础技术之外，需构建能快速泛化到新数据分布的模型。

Method: 利用模型编辑技术增强对未见脚本的融合，展示了在稀疏数据分布中进行领域合并的有效性。

Result: 即使使用相同训练数据，在向新字母表的迁移学习和具有挑战性的跨领域评估中，性能有显著提升。

Conclusion: 本研究为构建能轻松适配未充分代表字母表的模型提供了新方法，可使文档识别应用于更广泛的场景和文化。

Abstract: Achieving robustness in recognition systems across diverse domains is crucial
for their practical utility. While ample data availability is usually assumed,
low-resource languages, such as ancient manuscripts and non-western languages,
tend to be kept out of the equations of massive pretraining and foundational
techniques due to an under representation. In this work, we aim for building
models which can generalize to new distributions of data, such as alphabets,
faster than centralized fine-tune strategies. For doing so, we take advantage
of the recent advancements in model editing to enhance the incorporation of
unseen scripts (low-resource learning). In contrast to state-of-the-art
meta-learning, we showcase the effectiveness of domain merging in sparse
distributions of data, with agnosticity of its relation to the overall
distribution or any other prototyping necessity. Even when using the same exact
training data, our experiments showcase significant performance boosts in
\textbf{transfer learning} to new alphabets and \textbf{out-of-domain
evaluation} in challenging domain shifts, including historical ciphered texts
and non-Latin scripts. This research contributes a novel approach into building
models that can easily adopt under-represented alphabets and, therefore, enable
document recognition to a wider set of contexts and cultures.

</details>


### [204] [Ensemble-Based Survival Models with the Self-Attended Beran Estimator Predictions](https://arxiv.org/abs/2506.07933)
*Lev V. Utkin,Semen P. Khomets,Vlada A. Efremenko,Andrei V. Konstantinov,Natalya M. Verbova*

Main category: cs.LG

TL;DR: 提出SurvBESA生存分析集成模型，结合Beran估计器与自注意力机制，优于现有模型且代码公开。


<details>
  <summary>Details</summary>
Motivation: 传统生存分析因删失数据面临挑战，集成模型预测不稳定。

Method: 提出SurvBESA模型，对预测生存函数应用自注意力机制，用Huber污染模型定义注意力权重简化训练。

Result: 数值实验表明SurvBESA优于现有模型。

Conclusion: SurvBESA模型在生存分析中有更好表现，实现代码公开可促进应用。

Abstract: Survival analysis predicts the time until an event of interest, such as
failure or death, but faces challenges due to censored data, where some events
remain unobserved. Ensemble-based models, like random survival forests and
gradient boosting, are widely used but can produce unstable predictions due to
variations in bootstrap samples. To address this, we propose SurvBESA (Survival
Beran Estimators Self-Attended), a novel ensemble model that combines Beran
estimators with a self-attention mechanism. Unlike traditional methods,
SurvBESA applies self-attention to predicted survival functions, smoothing out
noise by adjusting each survival function based on its similarity to
neighboring survival functions. We also explore a special case using Huber's
contamination model to define attention weights, simplifying training to a
quadratic or linear optimization problem. Numerical experiments show that
SurvBESA outperforms state-of-the-art models. The implementation of SurvBESA is
publicly available.

</details>


### [205] [Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World](https://arxiv.org/abs/2506.06782)
*Qinting Jiang,Chuyang Ye,Dongyan Wei,Bingli Wang,Yuan Xue,Jingyan Jiang,Zhi Wang*

Main category: cs.LG

TL;DR: 提出FIND方法解决深度神经网络在分布偏移下的性能下降问题，实验显示显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在训练和测试域分布偏移时性能下降，现有TTA方法受动态多测试分布挑战，先前全局归一化策略有局限性。

Method: 提出FIND方法，包含LFD、FABN和S - FABN三个关键组件。LFD通过构建图结构捕获特征，FABN结合源统计和测试时间分布统计，S - FABN确定层的特征分区。

Result: FIND显著优于现有方法，在动态场景下准确率提升30%，且保持计算效率。

Conclusion: FIND方法有效解决了分布偏移下的性能下降问题，提高了模型性能和效率。

Abstract: Despite progress, deep neural networks still suffer performance declines
under distribution shifts between training and test domains, leading to a
substantial decrease in Quality of Experience (QoE) for applications. Existing
test-time adaptation (TTA) methods are challenged by dynamic, multiple test
distributions within batches. We observe that feature distributions across
different domains inherently cluster into distinct groups with varying means
and variances. This divergence reveals a critical limitation of previous global
normalization strategies in TTA, which inevitably distort the original data
characteristics. Based on this insight, we propose Feature-based Instance
Neighbor Discovery (FIND), which comprises three key components: Layer-wise
Feature Disentanglement (LFD), Feature Aware Batch Normalization (FABN) and
Selective FABN (S-FABN). LFD stably captures features with similar
distributions at each layer by constructing graph structures. While FABN
optimally combines source statistics with test-time distribution specific
statistics for robust feature representation. Finally, S-FABN determines which
layers require feature partitioning and which can remain unified, thereby
enhancing inference efficiency. Extensive experiments demonstrate that FIND
significantly outperforms existing methods, achieving a 30\% accuracy
improvement in dynamic scenarios while maintaining computational efficiency.

</details>


### [206] [Caterpillar GNN: Replacing Message Passing with Efficient Aggregation](https://arxiv.org/abs/2506.06784)
*Marek Černý*

Main category: cs.LG

TL;DR: 本文提出高效聚合机制的Caterpillar GNN，能在不同方法间无缝扩展，可处理合成图任务，在真实数据集上表现相当且减少计算图隐层节点数。


<details>
  <summary>Details</summary>
Motivation: 现有MPGNNs通常优先考虑最大表达能力，本文旨在引入高效聚合机制，牺牲部分表达能力来换取更强更结构化的聚合能力。

Method: 提出高效聚合机制，基于广义毛毛虫图同态计数严格刻画中间步骤的表达能力，进而提出Caterpillar GNN。

Result: Caterpillar GNN能成功处理专门为挑战经典MPGNNs设计的合成图级任务，在真实数据集上实现相当的预测性能，同时显著减少计算图隐层节点数。

Conclusion: Caterpillar GNN是一种有效方法，在保证性能的同时提高了效率。

Abstract: Message-passing graph neural networks (MPGNNs) dominate modern graph
learning, typically prioritizing maximal expressive power. In contrast, we
introduce an \emph{efficient aggregation} mechanism, deliberately trading off
some expressivity for stronger and more structured aggregation capabilities.
Our approach allows seamless scaling between classical message-passing and
simpler methods based on colored or plain walks. We rigorously characterize the
expressive power at each intermediate step using homomorphism counts from a
hierarchy of generalized \emph{caterpillar graphs}. Based on this foundation,
we propose the \emph{Caterpillar GNN}, whose robust graph-level aggregation
enables it to successfully tackle synthetic graph-level task specifically
designed to be challenging for classical MPGNNs. Moreover, we demonstrate that,
on real-world datasets, the Caterpillar GNN achieves comparable predictive
performance while significantly reducing the number of nodes in the hidden
layers of the computational graph.

</details>


### [207] [FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks](https://arxiv.org/abs/2506.06787)
*Qiyun Zhao*

Main category: cs.LG

TL;DR: 针对 AIGs 在现代电路表示中存在的问题，提出 FuncGNN 方法，实验表明其性能优于现有方法，还能减少训练时间和 GPU 内存使用。


<details>
  <summary>Details</summary>
Motivation: 现代电路复杂度和集成密度增加，AIGs 存在结构异质性和全局逻辑信息丢失问题，影响准确电路建模。

Method: 提出 FuncGNN，通过混合特征聚合提取多粒度拓扑模式，引入门感知归一化，采用多层集成合并中间特征。

Result: 在信号概率预测和真值表距离预测任务中，分别提升 2.06% 和 18.71%，减少约 50.6% 训练时间和约 32.8% GPU 内存使用。

Conclusion: FuncGNN 能有效缓解结构异质性，提升逻辑电路表示能力，优于现有方法。

Abstract: As integrated circuit scale grows and design complexity rises, effective
circuit representation helps support logic synthesis, formal verification, and
other automated processes in electronic design automation. And-Inverter Graphs
(AIGs), as a compact and canonical structure, are widely adopted for
representing Boolean logic in these workflows. However, the increasing
complexity and integration density of modern circuits introduce structural
heterogeneity and global logic information loss in AIGs, posing significant
challenges to accurate circuit modeling. To address these issues, we propose
FuncGNN, which integrates hybrid feature aggregation to extract
multi-granularity topological patterns, thereby mitigating structural
heterogeneity and enhancing logic circuit representations. FuncGNN further
introduces gate-aware normalization that adapts to circuit-specific gate
distributions, improving robustness to structural heterogeneity. Finally,
FuncGNN employs multi-layer integration to merge intermediate features across
layers, effectively synthesizing local and global semantic information for
comprehensive logic representations. Experimental results on two logic-level
analysis tasks (i.e., signal probability prediction and truth-table distance
prediction) demonstrate that FuncGNN outperforms existing state-of-the-art
methods, achieving improvements of 2.06% and 18.71%, respectively, while
reducing training time by approximately 50.6% and GPU memory usage by about
32.8%.

</details>


### [208] [Is Optimal Transport Necessary for Inverse Reinforcement Learning?](https://arxiv.org/abs/2506.06793)
*Zixuan Dong,Yumi Omori,Keith Ross*

Main category: cs.LG

TL;DR: 本文提出两种简单启发式替代方法挑战逆强化学习（IRL）中最优传输（OT）的必要性，经评估表现良好，建议重新评估IRL设计复杂度。


<details>
  <summary>Details</summary>
Motivation: OT方法存在算法复杂度高、超参数敏感等问题，挑战其在IRL中的必要性。

Method: 提出Minimum - Distance Reward和Segment - Matching Reward两种简单启发式方法，避免优化，复杂度为线性且易实现。

Result: 在32个在线和离线基准测试中，简单奖励方法与或优于近期OT方法。

Conclusion: OT核心优势可能源于基本接近对齐，建议重新评估未来IRL设计的复杂度。

Abstract: Inverse Reinforcement Learning (IRL) aims to recover a reward function from
expert demonstrations. Recently, Optimal Transport (OT) methods have been
successfully deployed to align trajectories and infer rewards. While OT-based
methods have shown strong empirical results, they introduce algorithmic
complexity, hyperparameter sensitivity, and require solving the OT optimization
problems. In this work, we challenge the necessity of OT in IRL by proposing
two simple, heuristic alternatives: (1) Minimum-Distance Reward, which assigns
rewards based on the nearest expert state regardless of temporal order; and (2)
Segment-Matching Reward, which incorporates lightweight temporal alignment by
matching agent states to corresponding segments in the expert trajectory. These
methods avoid optimization, exhibit linear-time complexity, and are easy to
implement. Through extensive evaluations across 32 online and offline
benchmarks with three reinforcement learning algorithms, we show that our
simple rewards match or outperform recent OT-based approaches. Our findings
suggest that the core benefits of OT may arise from basic proximity alignment
rather than its optimal coupling formulation, advocating for reevaluation of
complexity in future IRL design.

</details>


### [209] [IMPA-HGAE:Intra-Meta-Path Augmented Heterogeneous Graph Autoencoder](https://arxiv.org/abs/2506.06809)
*Di Lin,Wanjing Ren,Xuanbin Li,Rui Zhang*

Main category: cs.LG

TL;DR: 提出IMPA - HGAE框架，充分利用元路径上内部节点信息提升目标节点嵌入，还引入新掩码策略，在异质数据集上表现好，并探讨方法可解释性与未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有异质图自监督学习模型在转换异质图为同质图训练时，未充分利用元路径上的异质节点信息。

Method: 提出IMPA - HGAE框架，引入创新掩码策略。

Result: IMPA - HGAE在异质数据集上取得了优异性能。

Conclusion: 本研究为复杂图场景中利用元路径引导的结构语义进行鲁棒表示学习提供了见解。

Abstract: Self-supervised learning (SSL) methods have been increasingly applied to
diverse downstream tasks due to their superior generalization capabilities and
low annotation costs. However, most existing heterogeneous graph SSL models
convert heterogeneous graphs into homogeneous ones via meta-paths for training,
which only leverage information from nodes at both ends of meta-paths while
underutilizing the heterogeneous node information along the meta-paths. To
address this limitation, this paper proposes a novel framework named IMPA-HGAE
to enhance target node embeddings by fully exploiting internal node information
along meta-paths. Experimental results validate that IMPA-HGAE achieves
superior performance on heterogeneous datasets. Furthermore, this paper
introduce innovative masking strategies to strengthen the representational
capacity of generative SSL models on heterogeneous graph data. Additionally,
this paper discuss the interpretability of the proposed method and potential
future directions for generative self-supervised learning in heterogeneous
graphs. This work provides insights into leveraging meta-path-guided structural
semantics for robust representation learning in complex graph scenarios.

</details>


### [210] [Path Integral Optimiser: Global Optimisation via Neural Schrödinger-Föllmer Diffusion](https://arxiv.org/abs/2506.06815)
*Max McGuinness,Eirik Fladmark,Francisco Vargas*

Main category: cs.LG

TL;DR: 对使用神经扩散过程进行全局优化展开早期研究，以Zhang等人的路径积分采样器为重点，给出优化器理论边界和实验结果，发现其在2 - 1247维表现有潜力，但处理高维空间有困难。


<details>
  <summary>Details</summary>
Motivation: 开展使用神经扩散过程进行全局优化的早期研究。

Method: 利用玻尔兹曼分布将优化问题转化为薛定谔桥采样问题，应用吉尔萨诺夫定理并结合单点先验以随机控制术语构建问题，通过傅里叶多层感知机进行神经近似计算积分项。

Result: 优化器在2 - 1247维优化任务中每步性能有潜力，但面对15900个参数模型时在高维空间探索困难。

Conclusion: 需要开展在高维环境下的适应性工作。

Abstract: We present an early investigation into the use of neural diffusion processes
for global optimisation, focusing on Zhang et al.'s Path Integral Sampler. One
can use the Boltzmann distribution to formulate optimization as solving a
Schr\"odinger bridge sampling problem, then apply Girsanov's theorem with a
simple (single-point) prior to frame it in stochastic control terms, and
compute the solution's integral terms via a neural approximation (a Fourier
MLP). We provide theoretical bounds for this optimiser, results on toy
optimisation tasks, and a summary of the stochastic theory motivating the
model. Ultimately, we found the optimiser to display promising per-step
performance at optimisation tasks between 2 and 1,247 dimensions, but struggle
to explore higher-dimensional spaces when faced with a 15.9k parameter model,
indicating a need for work on adaptation in such environments.

</details>


### [211] [High-Fidelity Scientific Simulation Surrogates via Adaptive Implicit Neural Representations](https://arxiv.org/abs/2506.06858)
*Ziwei Li,Yuhan Duan,Tianyu Xiong,Yi-Tang Chen,Wei-Lun Chao,Han-Wei Shen*

Main category: cs.LG

TL;DR: 提出Feature - Adaptive INR (FA - INR)替代方案，结合坐标引导的混合专家模型，在三个大规模集合模拟数据集实验中，实现了最先进的保真度并显著减小模型大小。


<details>
  <summary>Details</summary>
Motivation: 现有的隐式神经表示（INRs）在处理复杂科学领域的局部高频变化时存在困难，近期方法虽有改进但牺牲了灵活性并增加了模型大小。

Method: 提出FA - INR，利用交叉注意力到增强的内存库学习灵活特征表示；引入坐标引导的混合专家（MoE）模型提升特征表示的专业性和效率。

Result: 在三个大规模集合模拟数据集实验中，FA - INR达到了最先进的保真度，同时显著减小了模型大小。

Conclusion: FA - INR为基于INR的替代模型在准确性和紧凑性之间建立了新的权衡边界。

Abstract: Effective surrogate models are critical for accelerating scientific
simulations. Implicit neural representations (INRs) offer a compact and
continuous framework for modeling spatially structured data, but they often
struggle with complex scientific fields exhibiting localized, high-frequency
variations. Recent approaches address this by introducing additional features
along rigid geometric structures (e.g., grids), but at the cost of flexibility
and increased model size. In this paper, we propose a simple yet effective
alternative: Feature-Adaptive INR (FA-INR). FA-INR leverages cross-attention to
an augmented memory bank to learn flexible feature representations, enabling
adaptive allocation of model capacity based on data characteristics, rather
than rigid structural assumptions. To further improve scalability, we introduce
a coordinate-guided mixture of experts (MoE) that enhances the specialization
and efficiency of feature representations. Experiments on three large-scale
ensemble simulation datasets show that FA-INR achieves state-of-the-art
fidelity while significantly reducing model size, establishing a new trade-off
frontier between accuracy and compactness for INR-based surrogates.

</details>


### [212] [Differentially Private Sparse Linear Regression with Heavy-tailed Responses](https://arxiv.org/abs/2506.06861)
*Xizhi Tian,Meng Ding,Touming Tao,Zihang Xiang,Di Wang*

Main category: cs.LG

TL;DR: 本文研究高维重尾响应下的DP稀疏线性回归，提出DP - IHT - H和DP - IHT - L方法，实验表明方法优于标准DP算法。


<details>
  <summary>Details</summary>
Motivation: 现有DP线性回归方法多关注规则数据分布或低维不规则数据，本文旨在解决这些局限，研究高维重尾响应下的DP稀疏线性回归。

Method: 提出DP - IHT - H方法，利用Huber损失和私有迭代硬阈值；提出DP - IHT - L方法，在额外假设下改进误差界。

Result: DP - IHT - H方法达到特定估计误差界，DP - IHT - L方法在额外假设下进一步改进误差界，且该界与尾参数ζ无关。

Conclusion: 通过合成和真实数据集实验，证明所提方法优于为“规则”数据设计的标准DP算法。

Abstract: As a fundamental problem in machine learning and differential privacy (DP),
DP linear regression has been extensively studied. However, most existing
methods focus primarily on either regular data distributions or low-dimensional
cases with irregular data. To address these limitations, this paper provides a
comprehensive study of DP sparse linear regression with heavy-tailed responses
in high-dimensional settings. In the first part, we introduce the DP-IHT-H
method, which leverages the Huber loss and private iterative hard thresholding
to achieve an estimation error bound of \(
  \tilde{O}\biggl(
  s^{* \frac{1 }{2}}
  \cdot \biggl(\frac{\log d}{n}\biggr)^{\frac{\zeta}{1 + \zeta}}
  +
  s^{* \frac{1 + 2\zeta}{2 + 2\zeta}}
  \cdot \biggl(\frac{\log^2 d}{n \varepsilon}\biggr)^{\frac{\zeta}{1 + \zeta}}
  \biggr) \) under the $(\varepsilon, \delta)$-DP model, where $n$ is the
sample size, $d$ is the dimensionality, $s^*$ is the sparsity of the parameter,
and $\zeta \in (0, 1]$ characterizes the tail heaviness of the data. In the
second part, we propose DP-IHT-L, which further improves the error bound under
additional assumptions on the response and achieves \(
  \tilde{O}\Bigl(\frac{(s^*)^{3/2} \log d}{n \varepsilon}\Bigr). \) Compared to
the first result, this bound is independent of the tail parameter $\zeta$.
Finally, through experiments on synthetic and real-world datasets, we
demonstrate that our methods outperform standard DP algorithms designed for
``regular'' data.

</details>


### [213] [SAFE: Finding Sparse and Flat Minima to Improve Pruning](https://arxiv.org/abs/2506.06866)
*Dongyeop Lee,Kwanhee Lee,Jinseok Chung,Namhoon Lee*

Main category: cs.LG

TL;DR: 本文提出新的剪枝方法SAFE和SAFE+，能找到稀疏且平坦的子网络，在图像分类和语言建模任务上表现好，对噪声数据有韧性。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络稀疏化带来的性能下降问题。

Method: 将剪枝问题建模为稀疏约束优化问题，用增广拉格朗日对偶方法求解，并提出广义投影操作。

Result: SAFE在标准图像分类和语言建模任务中能生成泛化性能更好的稀疏网络，且对噪声数据有韧性。

Conclusion: SAFE和SAFE+是有效的剪枝方法，能在稀疏化时提高网络性能，适用于现实场景。

Abstract: Sparsifying neural networks often suffers from seemingly inevitable
performance degradation, and it remains challenging to restore the original
performance despite much recent progress. Motivated by recent studies in robust
optimization, we aim to tackle this problem by finding subnetworks that are
both sparse and flat at the same time. Specifically, we formulate pruning as a
sparsity-constrained optimization problem where flatness is encouraged as an
objective. We solve it explicitly via an augmented Lagrange dual approach and
extend it further by proposing a generalized projection operation, resulting in
novel pruning methods called SAFE and its extension, SAFE$^+$. Extensive
evaluations on standard image classification and language modeling tasks reveal
that SAFE consistently yields sparse networks with improved generalization
performance, which compares competitively to well-established baselines. In
addition, SAFE demonstrates resilience to noisy data, making it well-suited for
real-world conditions.

</details>


### [214] [FREE: Fast and Robust Vision Language Models with Early Exits](https://arxiv.org/abs/2506.06884)
*Divya Jyoti Bajpai,Manjesh Kumar Hanawal*

Main category: cs.LG

TL;DR: 为解决视觉语言模型（VLMs）因模型大导致推理延迟的问题，提出FREE方法，实验证明能加速推理且保持性能。


<details>
  <summary>Details</summary>
Motivation: VLMs尺寸大，在对推理延迟有要求的实际应用中存在挑战，且训练退出分类器在有限标注数据下困难。

Method: 在基于GAN的框架中引入FREE对抗训练方法，每个退出由一个Transformer层和一个分类器组成，Transformer层对抗训练以产生与最终层相似的特征表示，特征分类器作为判别器。

Result: 方法在提高准确性和模型鲁棒性方面有效，能缓解过度思考和“中期危机”现象，推理过程加速超1.51倍且性能相当。

Conclusion: 提出的FREE方法能有效提高VLMs推理速度，同时保持良好性能。

Abstract: In recent years, Vision-Language Models (VLMs) have shown remarkable
performance improvements in Vision-Language tasks. However, their large size
poses challenges for real-world applications where inference latency is a
concern. To tackle this issue, we propose employing Early Exit (EE) strategies
in VLMs. However, training exit classifiers in VLMs is challenging,
particularly with limited labeled training data. To address this, we introduce
FREE, an adversarial training approach within a GAN-based framework. Here, each
exit consists of a transformer layer and a classifier. The transformer layer is
adversarially trained to produce feature representations similar to the final
layer, while a feature classifier serves as the discriminator. Our method
focuses on performing input-adaptive inference that increases inference speed
with minimal drop in performance. Experimental results demonstrate the
effectiveness of our approach in enhancing accuracy and model robustness by
mitigating overthinking and the phenomenon of mid-crisis that we highlight. We
experimentally validate that our method speeds up the inference process by more
than 1.51x while retaining comparable performance. The source code is available
at https://github.com/Div290/FREE.

</details>


### [215] [Can In-Context Reinforcement Learning Recover From Reward Poisoning Attacks?](https://arxiv.org/abs/2506.06891)
*Paulius Sasnauskas,Yiğit Yalın,Goran Radanović*

Main category: cs.LG

TL;DR: 研究上下文强化学习中DPT的抗腐败能力，提出AT - DPT框架，实验表明其在多场景下表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 应对针对DPT的奖励中毒攻击，提高其抗腐败能力。

Method: 提出Adversarially Trained Decision - Pretrained Transformer (AT - DPT)框架，同时训练攻击者和DPT模型。

Result: 在多臂老虎机和MDP场景中，AT - DPT显著优于标准基线。

Conclusion: AT - DPT框架能有效提升DPT在奖励中毒攻击下的鲁棒性，且多臂老虎机场景中的鲁棒性可推广到更复杂环境。

Abstract: We study the corruption-robustness of in-context reinforcement learning
(ICRL), focusing on the Decision-Pretrained Transformer (DPT, Lee et al.,
2023). To address the challenge of reward poisoning attacks targeting the DPT,
we propose a novel adversarial training framework, called Adversarially Trained
Decision-Pretrained Transformer (AT-DPT). Our method simultaneously trains an
attacker to minimize the true reward of the DPT by poisoning environment
rewards, and a DPT model to infer optimal actions from the poisoned data. We
evaluate the effectiveness of our approach against standard bandit algorithms,
including robust baselines designed to handle reward contamination. Our results
show that the proposed method significantly outperforms these baselines in
bandit settings, under a learned attacker. We additionally evaluate AT-DPT on
an adaptive attacker, and observe similar results. Furthermore, we extend our
evaluation to the MDP setting, confirming that the robustness observed in
bandit scenarios generalizes to more complex environments.

</details>


### [216] [Uncertainty Estimation on Graphs with Structure Informed Stochastic Partial Differential Equations](https://arxiv.org/abs/2506.06907)
*Fred Xu,Thomas Markovich*

Main category: cs.LG

TL;DR: 本文提出一种新的消息传递方案，结合时空噪声，能增强图上的不确定性估计，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在图上准确估计不确定性困难，尤其是分布偏移情况下，且图的不确定性估计更复杂。

Method: 类比由Matern高斯过程驱动的随机偏微分方程（SPDE）的演化和使用GNN层的消息传递，设计结合时空噪声的消息传递方案。

Result: 在不同标签信息量的图数据集的分布外（OOD）检测实验中，模型表现出合理性和优越性。

Conclusion: 所提方法能同时捕捉时空的不确定性，可明确控制协方差核的平滑度，增强图上的不确定性估计。

Abstract: Graph Neural Networks have achieved impressive results across diverse network
modeling tasks, but accurately estimating uncertainty on graphs remains
difficult, especially under distributional shifts. Unlike traditional
uncertainty estimation, graph-based uncertainty must account for randomness
arising from both the graph's structure and its label distribution, which adds
complexity. In this paper, making an analogy between the evolution of a
stochastic partial differential equation (SPDE) driven by Matern Gaussian
Process and message passing using GNN layers, we present a principled way to
design a novel message passing scheme that incorporates spatial-temporal noises
motivated by the Gaussian Process approach to SPDE. Our method simultaneously
captures uncertainty across space and time and allows explicit control over the
covariance kernel smoothness, thereby enhancing uncertainty estimates on graphs
with both low and high label informativeness. Our extensive experiments on
Out-of-Distribution (OOD) detection on graph datasets with varying label
informativeness demonstrate the soundness and superiority of our model to
existing approaches.

</details>


### [217] [Graph-Based Physics-Guided Urban PM2.5 Air Quality Imputation with Constrained Monitoring Data](https://arxiv.org/abs/2506.06917)
*Shangjie Du,Hui Wei,Dong Yoon Lee,Zhizhang Hu,Shijia Pan*

Main category: cs.LG

TL;DR: 提出GraPhy框架用于城市空气质量建模，实验显示其性能优于多种基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有监测网络在社会经济落后地区稀疏，限制了空气质量建模的准确性和分辨率，需要高精度建模方法。

Method: 提出物理学引导的图神经网络架构GraPhy，其层和边特征专为低分辨率监测数据设计。

Result: 使用加州圣华金谷数据实验，GraPhy在MSE、MAE和R2指标上表现最佳，比基线模型性能提升9%-56%，且在不同空间异质性水平下均优于基线。

Conclusion: GraPhy模型设计有效。

Abstract: This work introduces GraPhy, a graph-based, physics-guided learning framework
for high-resolution and accurate air quality modeling in urban areas with
limited monitoring data. Fine-grained air quality monitoring information is
essential for reducing public exposure to pollutants. However, monitoring
networks are often sparse in socioeconomically disadvantaged regions, limiting
the accuracy and resolution of air quality modeling. To address this, we
propose a physics-guided graph neural network architecture called GraPhy with
layers and edge features designed specifically for low-resolution monitoring
data. Experiments using data from California's socioeconomically disadvantaged
San Joaquin Valley show that GraPhy achieves the overall best performance
evaluated by mean squared error (MSE), mean absolute error (MAE), and R-square
value (R2), improving the performance by 9%-56% compared to various baseline
models. Moreover, GraPhy consistently outperforms baselines across different
spatial heterogeneity levels, demonstrating the effectiveness of our model
design.

</details>


### [218] [Basis Transformers for Multi-Task Tabular Regression](https://arxiv.org/abs/2506.06926)
*Wei Min Loh,Jiaqi Shang,Pascal Poupart*

Main category: cs.LG

TL;DR: 提出基础变压器架构处理表格数据，在多任务表格回归基准测试中表现良好，参数少。


<details>
  <summary>Details</summary>
Motivation: 现有技术难以同时处理表格数据的关键方面，如文本信息、可变列数和无元数据的未见数据。

Method: 提出专门设计的基础变压器架构来应对挑战并尊重表格数据的固有不变性。

Result: 在多任务表格回归基准测试中，中位数R²得分提高0.338，标准差最低，参数比最佳基线少五倍，超越预训练大语言模型基线。

Conclusion: 基础变压器架构能有效处理表格数据，在性能和参数数量上有优势。

Abstract: Dealing with tabular data is challenging due to partial information, noise,
and heterogeneous structure. Existing techniques often struggle to
simultaneously address key aspects of tabular data such as textual information,
a variable number of columns, and unseen data without metadata besides column
names. We propose a novel architecture, \textit{basis transformers},
specifically designed to tackle these challenges while respecting inherent
invariances in tabular data, including hierarchical structure and the
representation of numeric values. We evaluate our design on a multi-task
tabular regression benchmark, achieving an improvement of 0.338 in the median
$R^2$ score and the lowest standard deviation across 34 tasks from the
OpenML-CTR23 benchmark. Furthermore, our model has five times fewer parameters
than the best-performing baseline and surpasses pretrained large language model
baselines -- even when initialized from randomized weights.

</details>


### [219] [Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry](https://arxiv.org/abs/2506.06933)
*Mahdi Salmani,Alireza Abdollahpoorrostam,Seyed-Mohsen Moosavi-Dezfooli*

Main category: cs.LG

TL;DR: 本文提出非对称黑盒攻击框架，修改现有攻击的核心组件，设计高效算法，在不同成本机制下总查询成本和扰动更小。


<details>
  <summary>Details</summary>
Motivation: 现有基于决策的黑盒对抗攻击大多假设查询成本相等，但实际中查询成本可能不对称，且此场景下有效算法不足。

Method: 提出非对称黑盒攻击框架，修改搜索策略和梯度估计过程，分别提出Asymmetric Search和Asymmetric Gradient Estimation，设计平衡不同查询类型的高效算法。

Result: 在标准图像分类基准上的理论分析和实证评估表明，在不同成本机制下，该方法总查询成本和扰动比现有方法小，部分设置下有高达40%的改进。

Conclusion: 提出的非对称黑盒攻击框架有效，能以更低成本和更小扰动生成对抗样本，且可与现有黑盒攻击集成。

Abstract: Traditional decision-based black-box adversarial attacks on image classifiers
aim to generate adversarial examples by slightly modifying input images while
keeping the number of queries low, where each query involves sending an input
to the model and observing its output. Most existing methods assume that all
queries have equal cost. However, in practice, queries may incur asymmetric
costs; for example, in content moderation systems, certain output classes may
trigger additional review, enforcement, or penalties, making them more costly
than others. While prior work has considered such asymmetric cost settings,
effective algorithms for this scenario remain underdeveloped. In this paper, we
propose a general framework for decision-based attacks under asymmetric query
costs, which we refer to as asymmetric black-box attacks. We modify two core
components of existing attacks: the search strategy and the gradient estimation
process. Specifically, we propose Asymmetric Search (AS), a more conservative
variant of binary search that reduces reliance on high-cost queries, and
Asymmetric Gradient Estimation (AGREST), which shifts the sampling distribution
to favor low-cost queries. We design efficient algorithms that minimize total
attack cost by balancing different query types, in contrast to earlier methods
such as stealthy attacks that focus only on limiting expensive (high-cost)
queries. Our method can be integrated into a range of existing black-box
attacks with minimal changes. We perform both theoretical analysis and
empirical evaluation on standard image classification benchmarks. Across
various cost regimes, our method consistently achieves lower total query cost
and smaller perturbations than existing approaches, with improvements of up to
40% in some settings.

</details>


### [220] [Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More](https://arxiv.org/abs/2506.06940)
*Geonhui Yoo,Minhak Song,Chulhee Yun*

Main category: cs.LG

TL;DR: 本文用极简模型研究梯度下降训练深度神经网络时的渐进锐化现象，分析影响因素并扩展到实际场景。


<details>
  <summary>Details</summary>
Motivation: 梯度下降训练深度神经网络时渐进锐化现象常见，但潜在机制不明，需深入研究。

Method: 采用每层单神经元的深度线性网络这一极简模型，进行理论分析并实证验证。

Result: 极简模型有效捕捉了锐度动态，理论分析了数据集属性、网络深度等因素对渐进锐化程度的影响，并在实际场景验证。

Conclusion: 此研究加深了对神经网络训练中锐度动态的理解，强调了深度、训练数据和优化器之间的相互作用。

Abstract: When training deep neural networks with gradient descent, sharpness often
increases -- a phenomenon known as progressive sharpening -- before saturating
at the edge of stability. Although commonly observed in practice, the
underlying mechanisms behind progressive sharpening remain poorly understood.
In this work, we study this phenomenon using a minimalist model: a deep linear
network with a single neuron per layer. We show that this simple model
effectively captures the sharpness dynamics observed in recent empirical
studies, offering a simple testbed to better understand neural network
training. Moreover, we theoretically analyze how dataset properties, network
depth, stochasticity of optimizers, and step size affect the degree of
progressive sharpening in the minimalist model. We then empirically demonstrate
how these theoretical insights extend to practical scenarios. This study offers
a deeper understanding of sharpness dynamics in neural network training,
highlighting the interplay between depth, training data, and optimizers.

</details>


### [221] [Safety-Aware Reinforcement Learning for Control via Risk-Sensitive Action-Value Iteration and Quantile Regression](https://arxiv.org/abs/2506.06954)
*Clinton Enwerem,Aniruddh G. Puranic,John S. Baras,Calin Belta*

Main category: cs.LG

TL;DR: 主流近似动作值迭代强化学习算法有高估偏差，本文提出结合CVaR的风险正则化分位数算法解决安全约束问题，仿真显示其性能更优。


<details>
  <summary>Details</summary>
Motivation: 主流近似动作值迭代强化学习算法在高方差随机环境中有高估偏差，且现有方法在集成安全约束时存在挑战。

Method: 提出结合条件风险价值（CVaR）的风险正则化分位数算法，并给出风险敏感分布贝尔曼算子在Wasserstein空间收缩性质的理论保证。

Result: 移动机器人在动态到达 - 规避任务的仿真中，该方法比风险中性方法有更多目标成功、更少碰撞和更好的安全 - 性能权衡。

Conclusion: 所提算法能在不使用复杂架构的情况下有效解决安全约束问题，且能收敛到唯一成本分布。

Abstract: Mainstream approximate action-value iteration reinforcement learning (RL)
algorithms suffer from overestimation bias, leading to suboptimal policies in
high-variance stochastic environments. Quantile-based action-value iteration
methods reduce this bias by learning a distribution of the expected cost-to-go
using quantile regression. However, ensuring that the learned policy satisfies
safety constraints remains a challenge when these constraints are not
explicitly integrated into the RL framework. Existing methods often require
complex neural architectures or manual tradeoffs due to combined cost
functions. To address this, we propose a risk-regularized quantile-based
algorithm integrating Conditional Value-at-Risk (CVaR) to enforce safety
without complex architectures. We also provide theoretical guarantees on the
contraction properties of the risk-sensitive distributional Bellman operator in
Wasserstein space, ensuring convergence to a unique cost distribution.
Simulations of a mobile robot in a dynamic reach-avoid task show that our
approach leads to more goal successes, fewer collisions, and better
safety-performance trade-offs compared to risk-neutral methods.

</details>


### [222] [UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare](https://arxiv.org/abs/2506.06977)
*Pengfei Hu,Xiaoxue Han,Fei Wang,Yue Ning*

Main category: cs.LG

TL;DR: 文章提出UdonCare框架解决临床预测中领域泛化问题，实验显示其在有较大领域差距时性能优于其他基线模型。


<details>
  <summary>Details</summary>
Motivation: 临床预测中领域泛化面临挑战，典型方法在现实医疗场景有患者领域标签缺失和缺乏医学知识整合的问题。

Method: 利用ICD - 9 - CM等分层医学本体对疾病分组发现潜在领域，提出UdonCare框架迭代修剪细粒度领域、编码并应用孪生推理机制分离信号。

Result: 在MIMIC - III和MIMIC - IV临床数据集实验中，模型在领域差距大时比其他领域泛化基线模型性能更高。

Conclusion: 医学知识在实际医疗应用中提升领域泛化有未开发潜力。

Abstract: Domain generalization has become a critical challenge in clinical prediction,
where patient cohorts often exhibit shifting data distributions that degrade
model performance. Typical domain generalization approaches struggle in
real-world healthcare settings for two main reasons: (1) patient-specific
domain labels are typically unavailable, making domain discovery especially
difficult; (2) purely data-driven approaches overlook key clinical insights,
leading to a gap in medical knowledge integration. To address these problems,
we leverage hierarchical medical ontologies like the ICD-9-CM hierarchy to
group diseases into higher-level categories and discover more flexible latent
domains. In this paper, we introduce UdonCare, a hierarchy-guided framework
that iteratively prunes fine-grained domains, encodes these refined domains,
and applies a Siamese-type inference mechanism to separate domain-related
signals from patient-level features. Experimental results on clinical datasets
(MIMIC-III and MIMIC-IV) show that the proposed model achieves higher
performance compared to other domain generalization baselines when substantial
domain gaps presents, highlighting the untapped potential of medical knowledge
for enhancing domain generalization in practical healthcare applications.

</details>


### [223] [MoXGATE: Modality-aware cross-attention for multi-omic gastrointestinal cancer sub-type classification](https://arxiv.org/abs/2506.06980)
*Sajib Acharjee Dip,Uddip Acharjee Shuvo,Dipanwita Mallick,Abrar Rahman Abir,Liqing Zhang*

Main category: cs.LG

TL;DR: 提出MoXGATE深度学习框架用于多组学癌症亚型分类，实验表明其优于现有方法，有良好性能和泛化性。


<details>
  <summary>Details</summary>
Motivation: 癌症亚型分类对个性化治疗和预后评估至关重要，但有效整合多组学数据因特征异质性面临挑战。

Method: 提出Modality - Aware Cross - Attention MoXGATE框架，利用交叉注意力和可学习的模态权重增强多组学特征融合，还应用焦点损失缓解数据不平衡。

Result: 在GIAC和BRCA数据集上实验，MoXGATE分类准确率达95%，消融实验验证交叉注意力有效性，模型能泛化到未见过的癌症类型。

Conclusion: MoXGATE是多组学癌症亚型分类的有前景方法，性能和生物学泛化性得到提升。

Abstract: Cancer subtype classification is crucial for personalized treatment and
prognostic assessment. However, effectively integrating multi-omic data remains
challenging due to the heterogeneous nature of genomic, epigenomic, and
transcriptomic features. In this work, we propose Modality-Aware
Cross-Attention MoXGATE, a novel deep-learning framework that leverages
cross-attention and learnable modality weights to enhance feature fusion across
multiple omics sources. Our approach effectively captures inter-modality
dependencies, ensuring robust and interpretable integration. Through
experiments on Gastrointestinal Adenocarcinoma (GIAC) and Breast Cancer (BRCA)
datasets from TCGA, we demonstrate that MoXGATE outperforms existing methods,
achieving 95\% classification accuracy. Ablation studies validate the
effectiveness of cross-attention over simple concatenation and highlight the
importance of different omics modalities. Moreover, our model generalizes well
to unseen cancer types e.g., breast cancer, underscoring its adaptability. Key
contributions include (1) a cross-attention-based multi-omic integration
framework, (2) modality-weighted fusion for enhanced interpretability, (3)
application of focal loss to mitigate data imbalance, and (4) validation across
multiple cancer subtypes. Our results indicate that MoXGATE is a promising
approach for multi-omic cancer subtype classification, offering improved
performance and biological generalizability.

</details>


### [224] [Fully Explainable Classification Models Using Hyperblocks](https://arxiv.org/abs/2506.06986)
*Austin Snyder,Ryan Gallagher,Boris Kovalerchuk*

Main category: cs.LG

TL;DR: 基于Hyperblocks工作，通过引入算法简化Hyperblock、增加k - NN分类器的回退机制，提升模型可解释性、减少训练时间和复杂度，在多数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 在不牺牲准确性的前提下，增强Hyperblocks模型的可解释性、减少训练时间和降低模型复杂度，让非专业人员能理解决策逻辑。

Method: 引入一套Hyperblock简化算法，如去除冗余属性、通过重叠分析去除冗余块和创建析取单元；引入基于k - NN分类器的可解释回退机制。

Result: 可解释模型能扩展到高维、大数据集，在WBC等基准数据集上保持竞争力，在MNIST上经调优持续改进。

Conclusion: 该方法在需要信任、清晰和控制的领域，有望成为黑盒模型的透明替代方案。

Abstract: Building on existing work with Hyperblocks, which classify data using minimum
and maximum bounds for each attribute, we focus on enhancing interpretability,
decreasing training time, and reducing model complexity without sacrificing
accuracy. This system allows subject matter experts (SMEs) to directly inspect
and understand the model's decision logic without requiring extensive machine
learning expertise. To reduce Hyperblock complexity while retaining
performance, we introduce a suite of algorithms for Hyperblock simplification.
These include removing redundant attributes, removing redundant blocks through
overlap analysis, and creating disjunctive units. These methods eliminate
unnecessary parameters, dramatically reducing model size without harming
classification power. We increase robustness by introducing an interpretable
fallback mechanism using k-Nearest Neighbor (k-NN) classifiers for points not
covered by any block, ensuring complete data coverage while preserving model
transparency. Our results demonstrate that interpretable models can scale to
high-dimensional, large-volume datasets while maintaining competitive accuracy.
On benchmark datasets such as WBC (9-D), we achieve strong predictive
performance with significantly reduced complexity. On MNIST (784-D), our method
continues to improve through tuning and simplification, showing promise as a
transparent alternative to black-box models in domains where trust, clarity,
and control are crucial.

</details>


### [225] [Modified K-means Algorithm with Local Optimality Guarantees](https://arxiv.org/abs/2506.06990)
*Mingyi Li,Michael R. Metel,Akiko Takeda*

Main category: cs.LG

TL;DR: 本文研究K - means算法局部最优性，给出收敛到局部最优解的条件，提出改进方法，实验证明改进方法能提供更好局部最优解。


<details>
  <summary>Details</summary>
Motivation: 已有研究缺乏对K - means算法局部最优性保证的严格分析。

Method: 先给出K - means算法收敛到局部最优解的条件，基于此对算法进行简单修改，采用一般Bregman散度作为相异度度量。

Result: 数值实验表明K - means算法实际中不总能找到局部最优解，改进方法能提供更好局部最优解，降低聚类损失。

Conclusion: 提出的对K - means算法的修改方法能在保持相同计算复杂度下，确保连续和离散意义上的局部最优性。

Abstract: The K-means algorithm is one of the most widely studied clustering algorithms
in machine learning. While extensive research has focused on its ability to
achieve a globally optimal solution, there still lacks a rigorous analysis of
its local optimality guarantees. In this paper, we first present conditions
under which the K-means algorithm converges to a locally optimal solution.
Based on this, we propose simple modifications to the K-means algorithm which
ensure local optimality in both the continuous and discrete sense, with the
same computational complexity as the original K-means algorithm. As the
dissimilarity measure, we consider a general Bregman divergence, which is an
extension of the squared Euclidean distance often used in the K-means
algorithm. Numerical experiments confirm that the K-means algorithm does not
always find a locally optimal solution in practice, while our proposed methods
provide improved locally optimal solutions with reduced clustering loss. Our
code is available at https://github.com/lmingyi/LO-K-means.

</details>


### [226] [Comparison of Lightweight Methods for Vehicle Dynamics-Based Driver Drowsiness Detection](https://arxiv.org/abs/2506.07014)
*Yutaro Nakagama,Daisuke Ishii,Kazuki Yoshizoe*

Main category: cs.LG

TL;DR: 本文在透明公平框架下对比代表性车辆动力学的驾驶员困倦检测（DDD）方法，用公开数据集实验，RF 方法准确率达 88%。


<details>
  <summary>Details</summary>
Motivation: 现有车辆动力学 DDD 方法存在性能指标可靠性和可重复性问题，如数据泄露、不公开数据集等。

Method: 开发从公开数据集提取特征并使用轻量级机器学习模型进行 DDD 的框架，实现三种现有方法和基于随机森林（RF）的方法。

Result: 实验表明，基于 RF 的方法准确率最高，达到 88%。

Conclusion: 揭示非标准方式开发的 DDD 方法存在的问题，展示了一种高性能的实现方法。

Abstract: Driver drowsiness detection (DDD) prevents road accidents caused by driver
fatigue. Vehicle dynamics-based DDD has been proposed as a method that is both
economical and high performance. However, there are concerns about the
reliability of performance metrics and the reproducibility of many of the
existing methods. For instance, some previous studies seem to have a data
leakage issue among training and test datasets, and many do not openly provide
the datasets they used. To this end, this paper aims to compare the performance
of representative vehicle dynamics-based DDD methods under a transparent and
fair framework that uses a public dataset. We first develop a framework for
extracting features from an open dataset by Aygun et al. and performing DDD
with lightweight ML models; the framework is carefully designed to support a
variety of onfigurations. Second, we implement three existing representative
methods and a concise random forest (RF)-based method in the framework.
Finally, we report the results of experiments to verify the reproducibility and
clarify the performance of DDD based on common metrics. Among the evaluated
methods, the RF-based method achieved the highest accuracy of 88 %. Our
findings imply the issues inherent in DDD methods developed in a non-standard
manner, and demonstrate a high performance method implemented appropriately.

</details>


### [227] [AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint](https://arxiv.org/abs/2506.07022)
*Leheng Sheng,Changshuo Shen,Weixiang Zhao,Junfeng Fang,Xiaohao Liu,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 提出AlphaSteer方法解决大语言模型激活引导时安全与实用性的权衡问题，实验证明其有效且开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法在大语言模型安全与实用性上存在权衡问题，且先前方法缺乏理论基础。

Method: 将激活引导视为可学习过程，有实用性保留和安全增强两个目标，分别用零空间约束和线性回归构建向量。

Result: 在多个越狱攻击和实用性基准测试中，AlphaSteer显著提高大语言模型安全性且不损害通用能力。

Conclusion: AlphaSteer是理论可靠且经验有效的激活引导方法，能解决安全与实用性的权衡问题。

Abstract: As LLMs are increasingly deployed in real-world applications, ensuring their
ability to refuse malicious prompts, especially jailbreak attacks, is essential
for safe and reliable use. Recently, activation steering has emerged as an
effective approach for enhancing LLM safety by adding a refusal direction
vector to internal activations of LLMs during inference, which will further
induce the refusal behaviors of LLMs. However, indiscriminately applying
activation steering fundamentally suffers from the trade-off between safety and
utility, since the same steering vector can also lead to over-refusal and
degraded performance on benign prompts. Although prior efforts, such as vector
calibration and conditional steering, have attempted to mitigate this
trade-off, their lack of theoretical grounding limits their robustness and
effectiveness. To better address the trade-off between safety and utility, we
present a theoretically grounded and empirically effective activation steering
method called AlphaSteer. Specifically, it considers activation steering as a
learnable process with two principled learning objectives: utility preservation
and safety enhancement. For utility preservation, it learns to construct a
nearly zero vector for steering benign data, with the null-space constraints.
For safety enhancement, it learns to construct a refusal direction vector for
steering malicious data, with the help of linear regression. Experiments across
multiple jailbreak attacks and utility benchmarks demonstrate the effectiveness
of AlphaSteer, which significantly improves the safety of LLMs without
compromising general capabilities. Our codes are available at
https://github.com/AlphaLab-USTC/AlphaSteer.

</details>


### [228] [Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular Imbalanced Regression](https://arxiv.org/abs/2506.07033)
*Yung-Chien Wang,Kuang-Da Wang,Wei-Yao Wang,Wen-Chih Peng*

Main category: cs.LG

TL;DR: 论文针对表格回归任务不平衡问题提出MATI方法，含区域感知混合专家和测试时自监督专家聚合两个创新点，在四个数据集三种测试分布上评估，MAE较现有方法平均提升7.1%。


<details>
  <summary>Details</summary>
Motivation: 表格回归任务的不平衡问题研究不足，现有工作假设在实际中可能不成立，会导致性能下降。

Method: 提出MATI方法，包含Region - Aware Mixture Expert和Test - Time Self - Supervised Expert Aggregation两个创新点。

Result: 在四个真实世界表格不平衡回归数据集、三种测试分布上评估，MAE较现有方法平均提升7.1%。

Conclusion: MATI方法能有效解决表格回归任务的不平衡问题，提升模型性能。

Abstract: Tabular data serve as a fundamental and ubiquitous representation of
structured information in numerous real-world applications, e.g., finance and
urban planning. In the realm of tabular imbalanced applications, data imbalance
has been investigated in classification tasks with insufficient instances in
certain labels, causing the model's ineffective generalizability. However, the
imbalance issue of tabular regression tasks is underexplored, and yet is
critical due to unclear boundaries for continuous labels and simplifying
assumptions in existing imbalance regression work, which often rely on known
and balanced test distributions. Such assumptions may not hold in practice and
can lead to performance degradation. To address these issues, we propose MATI:
Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular
Imbalance Regression, featuring two key innovations: (i) the Region-Aware
Mixture Expert, which adopts a Gaussian Mixture Model to capture the underlying
related regions. The statistical information of each Gaussian component is then
used to synthesize and train region-specific experts to capture the unique
characteristics of their respective regions. (ii) Test-Time Self-Supervised
Expert Aggregation, which dynamically adjusts region expert weights based on
test data features to reinforce expert adaptation across varying test
distributions. We evaluated MATI on four real-world tabular imbalance
regression datasets, including house pricing, bike sharing, and age prediction.
To reflect realistic deployment scenarios, we adopted three types of test
distributions: a balanced distribution with uniform target frequencies, a
normal distribution that follows the training data, and an inverse distribution
that emphasizes rare target regions. On average across these three test
distributions, MATI achieved a 7.1% improvement in MAE compared to existing
methods.

</details>


### [229] [FairPFN: A Tabular Foundation Model for Causal Fairness](https://arxiv.org/abs/2506.07049)
*Jake Robertson,Noah Hollmann,Samuel Müller,Noor Awad,Frank Hutter*

Main category: cs.LG

TL;DR: 提出FairPFN模型解决现有因果公平框架需先验因果模型知识的问题，且表现良好，为因果公平研究开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习系统因训练数据含人口统计偏差导致决策加剧社会不平等，而当前因果公平框架需先验因果模型知识，限制其在复杂公平场景的应用。

Method: 提出FairPFN，一个在合成因果公平数据上预训练的表格基础模型，无需因果模型知识来识别和减轻预测中受保护属性的因果影响。

Result: FairPFN在各种手工和现实场景中，相较于强大的基线方法，在识别和消除受保护因果影响方面表现出色。

Conclusion: FairPFN为更广泛的复杂公平问题提供了使因果公平更易实现的途径，为未来研究奠定基础。

Abstract: Machine learning (ML) systems are utilized in critical sectors, such as
healthcare, law enforcement, and finance. However, these systems are often
trained on historical data that contains demographic biases, leading to ML
decisions that perpetuate or exacerbate existing social inequalities. Causal
fairness provides a transparent, human-in-the-loop framework to mitigate
algorithmic discrimination, aligning closely with legal doctrines of direct and
indirect discrimination. However, current causal fairness frameworks hold a key
limitation in that they assume prior knowledge of the correct causal model,
restricting their applicability in complex fairness scenarios where causal
models are unknown or difficult to identify. To bridge this gap, we propose
FairPFN, a tabular foundation model pre-trained on synthetic causal fairness
data to identify and mitigate the causal effects of protected attributes in its
predictions. FairPFN's key contribution is that it requires no knowledge of the
causal model and still demonstrates strong performance in identifying and
removing protected causal effects across a diverse set of hand-crafted and
real-world scenarios relative to robust baseline methods. FairPFN paves the way
for promising future research, making causal fairness more accessible to a
wider variety of complex fairness problems.

</details>


### [230] [Policy Gradient with Tree Search: Avoiding Local Optimas through Lookahead](https://arxiv.org/abs/2506.07054)
*Uri Koren,Navdeep Kumar,Uri Gadot,Giorgia Ramponi,Kfir Yehuda Levy,Shie Mannor*

Main category: cs.LG

TL;DR: 本文研究了Policy Gradient with Tree Search (PGTS)方法，通过理论分析和实验证明其能减少不良驻点、改善最坏情况性能，在多种环境中优于标准PG方法。


<details>
  <summary>Details</summary>
Motivation: 经典策略梯度方法在强化学习中常收敛到次优局部最优解，在大或复杂环境中问题更严重。

Method: 研究Policy Gradient with Tree Search (PGTS)方法，集成m步前瞻机制进行策略优化，理论分析考虑实际场景中仅对当前策略访问的状态进行更新。

Result: 理论上增加树搜索深度m单调减少不良驻点集合、改善最坏情况性能；实验上在多种MDP结构环境中，PGTS能展现“远见”，避开局部陷阱，取得更优解。

Conclusion: PGTS方法能有效提升策略优化效果，优于标准PG方法。

Abstract: Classical policy gradient (PG) methods in reinforcement learning frequently
converge to suboptimal local optima, a challenge exacerbated in large or
complex environments. This work investigates Policy Gradient with Tree Search
(PGTS), an approach that integrates an $m$-step lookahead mechanism to enhance
policy optimization. We provide theoretical analysis demonstrating that
increasing the tree search depth $m$-monotonically reduces the set of
undesirable stationary points and, consequently, improves the worst-case
performance of any resulting stationary policy. Critically, our analysis
accommodates practical scenarios where policy updates are restricted to states
visited by the current policy, rather than requiring updates across the entire
state space. Empirical evaluations on diverse MDP structures, including Ladder,
Tightrope, and Gridworld environments, illustrate PGTS's ability to exhibit
"farsightedness," navigate challenging reward landscapes, escape local traps
where standard PG fails, and achieve superior solutions.

</details>


### [231] [E-BATS: Efficient Backpropagation-Free Test-Time Adaptation for Speech Foundation Models](https://arxiv.org/abs/2506.07078)
*Jiaheng Dong,Hong Jia,Soumyajit Chatterjee,Abhirup Ghosh,James Bailey,Ting Dang*

Main category: cs.LG

TL;DR: 论文提出E - BATS框架解决语音基础模型在声学领域偏移下的测试时间适应问题，实验显示其有准确率提升和内存节省效果。


<details>
  <summary>Details</summary>
Motivation: 现有测试时间适应（TTA）方法在语音任务中存在内存消耗大或准确率低的问题，且多为视觉任务设计，不适用于语音任务。

Method: 提出E - BATS框架，包含轻量级提示适应、多尺度损失和测试时间指数移动平均机制。

Result: 在四个嘈杂语音数据集、十六种声学条件下实验，比无反向传播基线准确率提升4.1% - 13.5%，比基于反向传播方法节省2.0 - 6.4倍GPU内存。

Conclusion: 该工作为现实环境中实用语音处理系统开发更高效的适应方法奠定基础。

Abstract: Speech Foundation Models encounter significant performance degradation when
deployed in real-world scenarios involving acoustic domain shifts, such as
background noise and speaker accents. Test-time adaptation (TTA) has recently
emerged as a viable strategy to address such domain shifts at inference time
without requiring access to source data or labels. However, existing TTA
approaches, particularly those relying on backpropagation, are
memory-intensive, limiting their applicability in speech tasks and
resource-constrained settings. Although backpropagation-free methods offer
improved efficiency, existing ones exhibit poor accuracy. This is because they
are predominantly developed for vision tasks, which fundamentally differ from
speech task formulations, noise characteristics, and model architecture, posing
unique transferability challenges. In this paper, we introduce E-BATS, the
first Efficient BAckpropagation-free TTA framework designed explicitly for
speech foundation models. E-BATS achieves a balance between adaptation
effectiveness and memory efficiency through three key components: (i)
lightweight prompt adaptation for a forward-pass-based feature alignment, (ii)
a multi-scale loss to capture both global (utterance-level) and local
distribution shifts (token-level) and (iii) a test-time exponential moving
average mechanism for stable adaptation across utterances. Experiments
conducted on four noisy speech datasets spanning sixteen acoustic conditions
demonstrate consistent improvements, with 4.1%-13.5% accuracy gains over
backpropagation-free baselines and 2.0-6.4 times GPU memory savings compared to
backpropagation-based methods. By enabling scalable and robust adaptation under
acoustic variability, this work paves the way for developing more efficient
adaptation approaches for practical speech processing systems in real-world
environments.

</details>


### [232] [Patient Similarity Computation for Clinical Decision Support: An Efficient Use of Data Transformation, Combining Static and Time Series Data](https://arxiv.org/abs/2506.07092)
*Joydeb Kumar Sana,Mohammad M. Masud,M Sohel Rahman,M Saifur Rahman*

Main category: cs.LG

TL;DR: 本文提出基于数据转换的分布式患者相似度计算技术，结合时间序列与静态数据，提升预测性能并减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 患者相似度计算有助于改善临床决策支持，传统方法存在性能和计算时间问题。

Method: 结合时间序列与静态数据，对静态数据采用aWOE和Z - score数据转换，用分布式DTW计算时间序列相似度。

Result: 对于冠心病，AUC、准确率和F - measure分别提升11.4%、10.20%和12.6%；对于充血性心力衰竭，分别提升15.9%、10.5%和21.9%，计算时间最多减少40%。

Conclusion: 所提出的分布式患者相似度计算技术有效提升预测性能，减少计算时间。

Abstract: Patient similarity computation (PSC) is a fundamental problem in healthcare
informatics. The aim of the patient similarity computation is to measure the
similarity among patients according to their historical clinical records, which
helps to improve clinical decision support. This paper presents a novel
distributed patient similarity computation (DPSC) technique based on data
transformation (DT) methods, utilizing an effective combination of time series
and static data. Time series data are sensor-collected patients' information,
including metrics like heart rate, blood pressure, Oxygen saturation,
respiration, etc. The static data are mainly patient background and demographic
data, including age, weight, height, gender, etc. Static data has been used for
clustering the patients. Before feeding the static data to the machine learning
model adaptive Weight-of-Evidence (aWOE) and Z-score data transformation (DT)
methods have been performed, which improve the prediction performances. In
aWOE-based patient similarity models, sensitive patient information has been
processed using aWOE which preserves the data privacy of the trained models. We
used the Dynamic Time Warping (DTW) approach, which is robust and very popular,
for time series similarity. However, DTW is not suitable for big data due to
the significant computational run-time. To overcome this problem, distributed
DTW computation is used in this study. For Coronary Artery Disease, our DT
based approach boosts prediction performance by as much as 11.4%, 10.20%, and
12.6% in terms of AUC, accuracy, and F-measure, respectively. In the case of
Congestive Heart Failure (CHF), our proposed method achieves performance
enhancement up to 15.9%, 10.5%, and 21.9% for the same measures, respectively.
The proposed method reduces the computation time by as high as 40%.

</details>


### [233] [Filling the Missings: Spatiotemporal Data Imputation by Conditional Diffusion](https://arxiv.org/abs/2506.07099)
*Wenying He,Jieling Huang,Junhua Gu,Ji Zhang,Yude Bai*

Main category: cs.LG

TL;DR: 提出CoFILL模型用于时空数据插补，实验显示其在插补准确性上优于现有方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习和深度学习方法难以有效建模时空维度复杂依赖关系，且数据插补过程存在累积误差。

Method: 提出CoFILL，一种基于扩散模型的时空数据插补方法，采用双流架构并行处理时域和频域特征。

Result: CoFILL的噪声预测网络能将随机噪声转化为符合真实数据分布的值，插补准确性优于现有方法。

Conclusion: CoFILL是一种有效解决时空数据插补问题的方法。

Abstract: Missing data in spatiotemporal systems presents a significant challenge for
modern applications, ranging from environmental monitoring to urban traffic
management. The integrity of spatiotemporal data often deteriorates due to
hardware malfunctions and software failures in real-world deployments. Current
approaches based on machine learning and deep learning struggle to model the
intricate interdependencies between spatial and temporal dimensions effectively
and, more importantly, suffer from cumulative errors during the data imputation
process, which propagate and amplify through iterations. To address these
limitations, we propose CoFILL, a novel Conditional Diffusion Model for
spatiotemporal data imputation. CoFILL builds on the inherent advantages of
diffusion models to generate high-quality imputations without relying on
potentially error-prone prior estimates. It incorporates an innovative
dual-stream architecture that processes temporal and frequency domain features
in parallel. By fusing these complementary features, CoFILL captures both rapid
fluctuations and underlying patterns in the data, which enables more robust
imputation. The extensive experiments reveal that CoFILL's noise prediction
network successfully transforms random noise into meaningful values that align
with the true data distribution. The results also show that CoFILL outperforms
state-of-the-art methods in imputation accuracy. The source code is publicly
available at https://github.com/joyHJL/CoFILL.

</details>


### [234] [Reliable Critics: Monotonic Improvement and Convergence Guarantees for Reinforcement Learning](https://arxiv.org/abs/2506.07134)
*Eshwar S. R.,Gugan Thoppe,Aditya Gopalan,Gal Dalal*

Main category: cs.LG

TL;DR: 介绍可靠策略迭代 (RPI) 算法，证明其在函数近似下有单调性和收敛保证，提供无模型变体并在经典控制任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中函数近似下策略迭代难以正确使用的问题，如策略迭代在线性函数近似下单调性保证失效。

Method: 引入可靠策略迭代 (RPI)，用基于贝尔曼的约束优化代替策略评估中的常见投影或贝尔曼误差最小化。

Result: RPI 使价值估计具有单调性，其估计值为真实回报的下界，极限部分满足未投影贝尔曼方程；无模型变体可集成到主要无模型 PI 实现中，在经典控制任务中保持下界保证并超越基线方法。

Conclusion: RPI 是首个在函数近似下有单调性和收敛保证的算法，其实用变体在经典控制任务中有良好表现。

Abstract: Despite decades of research, it remains challenging to correctly use
Reinforcement Learning (RL) algorithms with function approximation. A prime
example is policy iteration, whose fundamental guarantee of monotonic
improvement collapses even under linear function approximation. To address this
issue, we introduce Reliable Policy Iteration (RPI). It replaces the common
projection or Bellman-error minimization during policy evaluation with a
Bellman-based constrained optimization. We prove that not only does RPI confer
textbook monotonicity on its value estimates but these estimates also lower
bound the true return. Also, their limit partially satisfies the unprojected
Bellman equation, emphasizing RPI's natural fit within RL. RPI is the first
algorithm with such monotonicity and convergence guarantees under function
approximation. For practical use, we provide a model-free variant of RPI that
amounts to a novel critic. It can be readily integrated into primary model-free
PI implementations such as DQN and DDPG. In classical control tasks, such
RPI-enhanced variants consistently maintain their lower-bound guarantee while
matching or surpassing the performance of all baseline methods.

</details>


### [235] [AMoPO: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models](https://arxiv.org/abs/2506.07165)
*Qi Liu,Jingqing Ruan,Hao Li,Haodong Zhao,Desheng Wang,Jiansong Chen,Wan Guanglu,Xunliang Cai,Zhi Zheng,Tong Xu*

Main category: cs.LG

TL;DR: 提出新框架AMoPO解决大语言模型多目标偏好对齐问题，性能超基线，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型多目标偏好对齐方法无法有效平衡偏好维度且依赖辅助模型增加计算复杂度。

Method: 提出AMoPO框架，引入多目标优化范式，用维度感知生成指标作隐式奖励，有自适应权重分配机制。

Result: AMoPO性能比基线高28.5%，在不同规模模型实验显示扩展性，多维度分析验证适应性和有效性。

Conclusion: AMoPO能实现维度感知偏好对齐，具有优越性。

Abstract: Existing multi-objective preference alignment methods for large language
models (LLMs) face limitations: (1) the inability to effectively balance
various preference dimensions, and (2) reliance on auxiliary reward/reference
models introduces computational complexity. To address these challenges, we
propose Adaptive Multi-objective Preference Optimization (AMoPO), a novel
framework that achieves dynamic balance across preference dimensions. By
introducing the multi-objective optimization paradigm to use the
dimension-aware generation metrics as implicit rewards, AMoPO aligns LLMs with
diverse preferences without additional reward models or reference models. We
introduce an adaptive weight assignment mechanism that models the generation
space as a Gaussian distribution, allowing dynamic prioritization of preference
dimensions. Empirical results demonstrate that AMoPO outperforms
state-of-the-art baselines by 28.5%, and the experiments on 7B, 14B, and 32B
models reveal the scaling ability of AMoPO. Moreover, additional analysis of
multiple dimensions verifies its adaptability and effectiveness. These findings
validate AMoPO's capability to achieve dimension-aware preference alignment,
highlighting its superiority. Our codes and datasets are available at
https://github.com/Javkonline/AMoPO.

</details>


### [236] [Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment](https://arxiv.org/abs/2506.07168)
*Huanyi Xie,Lijie Hu,Lu Yu,Tianhao Huang,Longfei Li,Meng Li,Jun Zhou,Huan Wang,Di Wang*

Main category: cs.LG

TL;DR: 提出GAGA框架用于文本属性图表示学习，仅标注少量数据就能达高效分类效果。


<details>
  <summary>Details</summary>
Motivation: 传统GNN处理文本属性图存在不足，现有用大模型的方法需大量标注和微调，耗时成本高。

Method: 聚焦标注代表性节点和边以减少成本，构建标注图，用两级对齐模块整合标注图与文本属性图。

Result: GAGA分类准确率与或超现有方法，仅需标注1%的数据。

Conclusion: GAGA在文本属性图表示学习中高效可行。

Abstract: In the realm of Text-attributed Graphs (TAGs), traditional graph neural
networks (GNNs) often fall short due to the complex textual information
associated with each node. Recent methods have improved node representations by
leveraging large language models (LLMs) to enhance node text features, but
these approaches typically require extensive annotations or fine-tuning across
all nodes, which is both time-consuming and costly. To overcome these
challenges, we introduce GAGA, an efficient framework for TAG representation
learning. GAGA reduces annotation time and cost by focusing on annotating only
representative nodes and edges. It constructs an annotation graph that captures
the topological relationships among these annotations. Furthermore, GAGA
employs a two-level alignment module to effectively integrate the annotation
graph with the TAG, aligning their underlying structures. Experiments show that
GAGA achieves classification accuracies on par with or surpassing
state-of-the-art methods while requiring only 1% of the data to be annotated,
demonstrating its high efficiency.

</details>


### [237] [Regularized Adaptive Graph Learning for Large-Scale Traffic Forecasting](https://arxiv.org/abs/2506.07179)
*Kaiqi Wu,Weiyang Kong,Sen Zhang,Yubao Liu,Zitong Chen*

Main category: cs.LG

TL;DR: 提出RAGL模型解决现有交通预测自适应图学习方法的问题，实验显示其预测精度和计算效率佳。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测自适应图学习方法存在忽视节点嵌入正则化和图卷积操作可扩展性差的问题。

Method: 提出正则化自适应图学习框架，结合SSE和自适应图卷积；开发ECO算子以保证在大型道路网络的可扩展性。

Result: 在四个大规模真实交通数据集上，RAGL在预测精度上始终优于现有方法，计算效率有竞争力。

Conclusion: RAGL模型能有效解决现有交通预测自适应图学习方法的问题，具有良好性能。

Abstract: Traffic prediction is a critical task in spatial-temporal forecasting with
broad applications in travel planning and urban management. Adaptive graph
convolution networks have emerged as mainstream solutions due to their ability
to learn node embeddings in a data-driven manner and capture complex latent
dependencies. However, existing adaptive graph learning methods for traffic
forecasting often either ignore the regularization of node embeddings, which
account for a significant proportion of model parameters, or face scalability
issues from expensive graph convolution operations. To address these
challenges, we propose a Regularized Adaptive Graph Learning (RAGL) model.
First, we introduce a regularized adaptive graph learning framework that
synergizes Stochastic Shared Embedding (SSE) and adaptive graph convolution via
a residual difference mechanism, achieving both embedding regularization and
noise suppression. Second, to ensure scalability on large road networks, we
develop the Efficient Cosine Operator (ECO), which performs graph convolution
based on the cosine similarity of regularized embeddings with linear time
complexity. Extensive experiments on four large-scale real-world traffic
datasets show that RAGL consistently outperforms state-of-the-art methods in
terms of prediction accuracy and exhibits competitive computational efficiency.

</details>


### [238] [Learning based on neurovectors for tabular data: a new neural network approach](https://arxiv.org/abs/2506.07185)
*J. C. Husillos,A. Gallego,A. Roma,A. Troncoso*

Main category: cs.LG

TL;DR: 本文提出基于Neurovectors的学习方法，用于表格数据处理，通过在向量空间编码信息，实验显示其有竞争力的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统人工神经网络依赖反向传播调整权重，需要更具适应性和可解释性的学习方法。

Method: 使用Neurovectors在向量空间对数据进行结构化编码，以能量传播驱动学习过程，生成动态知识表示。

Result: 使用UCI和Kaggle数据集进行分类和回归实验，与标准机器学习和深度学习模型对比，Neurovectors达到有竞争力的准确性。

Conclusion: 基于Neurovectors的学习方法能提高预测模型的可解释性和效率，且有不错的准确性。

Abstract: In this paper, we present a novel learning approach based on Neurovectors, an
innovative paradigm that structures information through interconnected nodes
and vector relationships for tabular data processing. Unlike traditional
artificial neural networks that rely on weight adjustment through
backpropagation, Neurovectors encode information by structuring data in vector
spaces where energy propagation, rather than traditional weight updates, drives
the learning process, enabling a more adaptable and explainable learning
process. Our method generates dynamic representations of knowledge through
neurovectors, thereby improving both the interpretability and efficiency of the
predictive model. Experimental results using datasets from well-established
repositories such as the UCI machine learning repository and Kaggle are
reported both for classification and regression. To evaluate its performance,
we compare our approach with standard machine learning and deep learning
models, showing that Neurovectors achieve competitive accuracy.

</details>


### [239] [Analyzing Breast Cancer Survival Disparities by Race and Demographic Location: A Survival Analysis Approach](https://arxiv.org/abs/2506.07191)
*Ramisa Farha,Joshua O. Olukoya*

Main category: cs.LG

TL;DR: 本研究利用SEER 2021数据集，通过多种数据分析方法揭示不同种族和地区乳腺癌患者生存结局差异，为制定干预措施提供依据。


<details>
  <summary>Details</summary>
Motivation: 旨在为全球改善乳腺癌结局和减少治疗差异的努力做出贡献，了解不同种族和地理背景患者生存结局的差异。

Method: 采用探索性数据分析（EDA）确定影响生存率的关键变量，运用生存分析技术（Kaplan - Meier估计器、log - rank检验）和Cox比例风险模型，还进行模型验证和解释。

Result: 详细的统计分析，凸显了乳腺癌治疗和护理中的差异。

Conclusion: 研究结果可作为制定针对性干预措施以有效解决不平等问题的基础工具。

Abstract: This study employs a robust analytical framework to uncover patterns in
survival outcomes among breast cancer patients from diverse racial and
geographical backgrounds. This research uses the SEER 2021 dataset to analyze
breast cancer survival outcomes to identify and comprehend dissimilarities. Our
approach integrates exploratory data analysis (EDA), through this we identify
key variables that influence survival rates and employ survival analysis
techniques, including the Kaplan-Meier estimator and log-rank test and the
advanced modeling Cox Proportional Hazards model to determine how survival
rates vary across racial groups and countries. Model validation and
interpretation are undertaken to ensure the reliability of our findings, which
are documented comprehensively to inform policymakers and healthcare
professionals. The outcome of this paper is a detailed version of statistical
analysis that not just highlights disparities in breast cancer treatment and
care but also serves as a foundational tool for developing targeted
interventions to address the inequalities effectively. Through this research,
our aim is to contribute to the global efforts to improve breast cancer
outcomes and reduce treatment disparities.

</details>


### [240] [GGBall: Graph Generative Model on Poincaré Ball](https://arxiv.org/abs/2506.07198)
*Tianci Bu,Chuanrui Wang,Hao Ma,Haoren Zheng,Xin Lu,Tailin Wu*

Main category: cs.LG

TL;DR: 提出新的双曲框架GGBall用于图生成，结合几何归纳偏置与现代生成范式，在实验中表现优于基线，凸显双曲几何潜力。


<details>
  <summary>Details</summary>
Motivation: 欧氏几何在捕捉指数复杂度上有局限，现有图生成难以生成具有层次结构的图。

Method: 引入GGBall框架，结合HVQVAE和黎曼流匹配先验，开发双曲GNN和Transformer层。

Result: 在Community - Small上降低度MMD超75%，在Ego - Small上降低超40%。

Conclusion: 双曲几何可作为复杂、结构化和层次化数据生成建模的有力基础。

Abstract: Generating graphs with hierarchical structures remains a fundamental
challenge due to the limitations of Euclidean geometry in capturing exponential
complexity. Here we introduce \textbf{GGBall}, a novel hyperbolic framework for
graph generation that integrates geometric inductive biases with modern
generative paradigms. GGBall combines a Hyperbolic Vector-Quantized Autoencoder
(HVQVAE) with a Riemannian flow matching prior defined via closed-form
geodesics. This design enables flow-based priors to model complex latent
distributions, while vector quantization helps preserve the curvature-aware
structure of the hyperbolic space. We further develop a suite of hyperbolic GNN
and Transformer layers that operate entirely within the manifold, ensuring
stability and scalability. Empirically, our model reduces degree MMD by over
75\% on Community-Small and over 40\% on Ego-Small compared to state-of-the-art
baselines, demonstrating an improved ability to preserve topological
hierarchies. These results highlight the potential of hyperbolic geometry as a
powerful foundation for the generative modeling of complex, structured, and
hierarchical data domains. Our code is available at
\href{https://github.com/AI4Science-WestlakeU/GGBall}{here}.

</details>


### [241] [Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward](https://arxiv.org/abs/2506.07218)
*Tong Xiao,Xin Xu,Zhenya Huang,Hongyu Gao,Quan Liu,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: 现有RLVR方法忽视提升多模态大语言模型（MLLMs）感知能力，提出Perception - R1，引入视觉感知奖励，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 现有将RLVR应用于多模态领域的研究，大多忽视MLLMs多模态感知能力的提升，限制了多模态推理能力的进一步提高。

Method: 提出Perception - R1，从多模态问题的思维链轨迹收集文本视觉注释作为奖励分配的视觉参考，训练时用判断大语言模型评估视觉注释与MLLM生成响应的一致性来分配视觉感知奖励。

Result: 在多个多模态推理基准测试中，Perception - R1仅用1442条训练数据就在大多数基准上达到了最先进的性能。

Conclusion: Perception - R1能有效激励MLLMs的多模态感知和推理能力。

Abstract: Enhancing the multimodal reasoning capabilities of Multimodal Large Language
Models (MLLMs) is a challenging task that has attracted increasing attention in
the community. Recently, several studies have applied Reinforcement Learning
with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the
reasoning abilities of MLLMs. However, these works largely overlook the
enhancement of multimodal perception capabilities in MLLMs, which serve as a
core prerequisite and foundational component of complex multimodal reasoning.
Through McNemar's test, we find that existing RLVR method fails to effectively
enhance the multimodal perception capabilities of MLLMs, thereby limiting their
further improvement in multimodal reasoning. To address this limitation, we
propose Perception-R1, which introduces a novel visual perception reward that
explicitly encourages MLLMs to perceive the visual content accurately, thereby
can effectively incentivizing both their multimodal perception and reasoning
capabilities. Specifically, we first collect textual visual annotations from
the CoT trajectories of multimodal problems, which will serve as visual
references for reward assignment. During RLVR training, we employ a judging LLM
to assess the consistency between the visual annotations and the responses
generated by MLLM, and assign the visual perception reward based on these
consistency judgments. Extensive experiments on several multimodal reasoning
benchmarks demonstrate the effectiveness of our Perception-R1, which achieves
state-of-the-art performance on most benchmarks using only 1,442 training data.

</details>


### [242] [VARSHAP: Addressing Global Dependency Problems in Explainable AI with Variance-Based Local Feature Attribution](https://arxiv.org/abs/2506.07229)
*Mateusz Gajewski,Mikołaj Morzy,Adam Karczmarz,Piotr Sankowski*

Main category: cs.LG

TL;DR: 本文提出新的局部特征归因方法VARSHAP，实验表明其优于流行方法。


<details>
  <summary>Details</summary>
Motivation: 现有特征归因方法如SHAP存在全局依赖问题，无法捕捉真实局部模型行为。

Method: 引入VARSHAP，以预测方差的减少作为特征重要性的关键指标，基于Shapley值框架。

Result: 在合成和真实数据集上的实验表明，VARSHAP在定量和定性上都优于KernelSHAP和LIME等流行方法。

Conclusion: VARSHAP是一种有效的局部特征归因方法，能解决现有方法的问题，且表现更优。

Abstract: Existing feature attribution methods like SHAP often suffer from global
dependence, failing to capture true local model behavior. This paper introduces
VARSHAP, a novel model-agnostic local feature attribution method which uses the
reduction of prediction variance as the key importance metric of features.
Building upon Shapley value framework, VARSHAP satisfies the key Shapley
axioms, but, unlike SHAP, is resilient to global data distribution shifts.
Experiments on synthetic and real-world datasets demonstrate that VARSHAP
outperforms popular methods such as KernelSHAP or LIME, both quantitatively and
qualitatively.

</details>


### [243] [Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs](https://arxiv.org/abs/2506.07240)
*Roy Eisenstadt,Itamar Zimerman,Lior Wolf*

Main category: cs.LG

TL;DR: 本文探索大语言模型在显式思维过程中理解和调节推理长度的机制，提出“超频”方法减少过度思考，提高答案准确性并降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 显式结构化推理中，推理阶段长度影响答案质量，过长或过短都有问题，需探索大语言模型理解和调节推理长度的机制。

Method: 展示大语言模型对推理过程的进度编码，引入交互式进度条可视化揭示模型规划动态；在推理时操纵内部进度编码以减少不必要步骤。

Result: “超频”方法减轻了过度思考，提高了答案准确性，降低了推理延迟。

Conclusion: 所提出的方法有效，代码已公开。

Abstract: Recently, techniques such as explicit structured reasoning have demonstrated
strong test-time scaling behavior by enforcing a separation between the model's
internal "thinking" process and the final response. A key factor influencing
answer quality in this setting is the length of the thinking stage. When the
reasoning is too short, the model may fail to capture the complexity of the
task. Conversely, when it is too long, the model may overthink, leading to
unnecessary computation and degraded performance. This paper explores and
exploits the underlying mechanisms by which LLMs understand and regulate the
length of their reasoning during explicit thought processes. First, we show
that LLMs encode their progress through the reasoning process and introduce an
interactive progress bar visualization, which is then used to reveal insights
on the model's planning dynamics. Second, we manipulate the internal progress
encoding during inference to reduce unnecessary steps and generate a more
concise and decisive chain of thoughts. Our empirical results demonstrate that
this "overclocking" method mitigates overthinking, improves answer accuracy,
and reduces inference latency. Our code is publicly available.

</details>


### [244] [Promoting Ensemble Diversity with Interactive Bayesian Distributional Robustness for Fine-tuning Foundation Models](https://arxiv.org/abs/2506.07247)
*Ngoc-Quan Pham,Tuan Truong,Quyen Tran,Tan Nguyen,Dinh Phung,Trung Le*

Main category: cs.LG

TL;DR: 介绍IBDR框架，通过增强粒子多样性提升集成质量，实验表明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 提出新的贝叶斯推理框架，增强粒子间交互，提高集成质量。

Method: 建立广义理论框架，连接分布总体损失和近似后验，采用双优化程序。

Result: 在VTAB - 1K基准和通用推理语言任务中，IBDR性能优于各种基线方法。

Conclusion: IBDR在实际应用中有效。

Abstract: We introduce Interactive Bayesian Distributional Robustness (IBDR), a novel
Bayesian inference framework that allows modeling the interactions between
particles, thereby enhancing ensemble quality through increased particle
diversity. IBDR is grounded in a generalized theoretical framework that
connects the distributional population loss with the approximate posterior,
motivating a practical dual optimization procedure that enforces distributional
robustness while fostering particle diversity. We evaluate IBDR's performance
against various baseline methods using the VTAB-1K benchmark and the common
reasoning language task. The results consistently show that IBDR outperforms
these baselines, underscoring its effectiveness in real-world applications.

</details>


### [245] [A Stable Whitening Optimizer for Efficient Neural Network Training](https://arxiv.org/abs/2506.07254)
*Kevin Frans,Sergey Levine,Pieter Abbeel*

Main category: cs.LG

TL;DR: 基于Shampoo算法，提出SPlus方法，解决三个关键问题，在Transformer训练基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 对神经网络优化进行实验性研究，改进Shampoo算法存在的问题。

Method: 1. 引入结合历史特征基和瞬时归一化的有界更新；2. 采用形状感知缩放；3. 提出迭代平均方案；4. 引入Transformer训练基准。

Result: SPlus平均能在Adam梯度步数的44%和时钟时间的62%内达到其验证性能。

Conclusion: SPlus方法有效解决了Shampoo算法的问题，提高了训练效率。

Abstract: In this work, we take an experimentally grounded look at neural network
optimization. Building on the Shampoo family of algorithms, we identify and
alleviate three key issues, resulting in the proposed SPlus method. First, we
find that naive Shampoo is prone to divergence when matrix-inverses are cached
for long periods. We introduce an alternate bounded update combining a
historical eigenbasis with instantaneous normalization, resulting in
across-the-board stability and significantly lower computational requirements.
Second, we adapt a shape-aware scaling to enable learning rate transfer across
network width. Third, we find that high learning rates result in large
parameter noise, and propose a simple iterate-averaging scheme which unblocks
faster learning. To properly confirm these findings, we introduce a pointed
Transformer training benchmark, considering three objectives (language
modelling, image classification, and diffusion modelling) across different
stages of training. On average, SPlus is able to reach the validation
performance of Adam within 44% of the gradient steps and 62% of the wallclock
time.

</details>


### [246] [A Cramér-von Mises Approach to Incentivizing Truthful Data Sharing](https://arxiv.org/abs/2506.07272)
*Alex Clinton,Thomas Zeng,Yiding Chen,Xiaojin Zhu,Kirthevasan Kandasamy*

Main category: cs.LG

TL;DR: 本文提出基于Cramér - von Mises统计量的新颖双样本测试的奖励机制，激励数据真实共享，理论和实证验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于数据量的奖励机制易被操纵，先前基于数据比较的方法假设强、适用性有限。

Method: 开发基于Cramér - von Mises统计量的新颖双样本测试的奖励机制。

Result: 证明在贝叶斯和无先验设置中，如实报告构成（近似）纳什均衡，理论应用于三个典型问题，放松先前工作假设，实证在模拟和真实数据上激励真实数据共享。

Conclusion: 提出的奖励机制能有效激励数据提供者如实提交数据，且放松了先前方法的假设。

Abstract: Modern data marketplaces and data sharing consortia increasingly rely on
incentive mechanisms to encourage agents to contribute data. However, schemes
that reward agents based on the quantity of submitted data are vulnerable to
manipulation, as agents may submit fabricated or low-quality data to inflate
their rewards. Prior work has proposed comparing each agent's data against
others' to promote honesty: when others contribute genuine data, the best way
to minimize discrepancy is to do the same. Yet prior implementations of this
idea rely on very strong assumptions about the data distribution (e.g.
Gaussian), limiting their applicability. In this work, we develop reward
mechanisms based on a novel, two-sample test inspired by the Cram\'er-von Mises
statistic. Our methods strictly incentivize agents to submit more genuine data,
while disincentivizing data fabrication and other types of untruthful
reporting. We establish that truthful reporting constitutes a (possibly
approximate) Nash equilibrium in both Bayesian and prior-agnostic settings. We
theoretically instantiate our method in three canonical data sharing problems
and show that it relaxes key assumptions made by prior work. Empirically, we
demonstrate that our mechanism incentivizes truthful data sharing via
simulations and on real-world language and image data.

</details>


### [247] [Investigating the Relationship Between Physical Activity and Tailored Behavior Change Messaging: Connecting Contextual Bandit with Large Language Models](https://arxiv.org/abs/2506.07275)
*Haochen Song,Dominik Hofer,Rania Islambouli,Laura Hawkins,Ananya Bhattacharjee,Meredith Franklin,Joseph Jay Williams*

Main category: cs.LG

TL;DR: 本文提出结合cMAB与大语言模型的混合方法促进身体活动，通过七天试验评估不同模型效果，得出LLM个性化与cMAB适应互补作用的新见解。


<details>
  <summary>Details</summary>
Motivation: 传统cMAB算法需大样本且可能忽略关键心理因素，需要新方法更好地促进身体活动。

Method: 提出结合cMAB选择干预类型与LLM个性化消息内容的混合方法，评估四种干预类型，参与者接受四种模型分配的消息，用因果推理框架评估效果。

Result: 通过试验评估了不同模型对每日步数和消息接受度的影响。

Conclusion: LLM个性化与cMAB适应在促进身体活动的个性化行为消息中具有互补作用。

Abstract: Machine learning approaches, such as contextual multi-armed bandit (cMAB)
algorithms, offer a promising strategy to reduce sedentary behavior by
delivering personalized interventions to encourage physical activity. However,
cMAB algorithms typically require large participant samples to learn
effectively and may overlook key psychological factors that are not explicitly
encoded in the model. In this study, we propose a hybrid approach that combines
cMAB for selecting intervention types with large language models (LLMs) to
personalize message content. We evaluate four intervention types: behavioral
self-monitoring, gain-framed, loss-framed, and social comparison, each
delivered as a motivational message aimed at increasing motivation for physical
activity and daily step count. Message content is further personalized using
dynamic contextual factors including daily fluctuations in self-efficacy,
social influence, and regulatory focus. Over a seven-day trial, participants
receive daily messages assigned by one of four models: cMAB alone, LLM alone,
combined cMAB with LLM personalization (cMABxLLM), or equal randomization
(RCT). Outcomes include daily step count and message acceptance, assessed via
ecological momentary assessments (EMAs). We apply a causal inference framework
to evaluate the effects of each model. Our findings offer new insights into the
complementary roles of LLM-based personalization and cMAB adaptation in
promoting physical activity through personalized behavioral messaging.

</details>


### [248] [Tokenized Bandit for LLM Decoding and Alignment](https://arxiv.org/abs/2506.07276)
*Suho Shin,Chenghao Yang,Haifeng Xu,Mohammad T. Hajiaghayi*

Main category: cs.LG

TL;DR: 本文引入受大语言模型解码和对齐启发的TLB和TMAB问题，提出假设并给出算法，证明贪心解码的最优性，进行实验验证。


<details>
  <summary>Details</summary>
Motivation: 受大语言模型解码和对齐启发，研究相关的线性和随机多臂老虎机问题变种。

Method: 引入DDMC假设，为TLB和TMAB分别提出具有特定遗憾界的算法。

Result: 证明了贪心解码在DDMC下的（几乎）最优性，算法在合成和真实数据集上得到验证。

Conclusion: 提出的算法和假设能有效解决TLB和TMAB问题，解释了贪心解码的有效性。

Abstract: We introduce the tokenized linear bandit (TLB) and multi-armed bandit (TMAB),
variants of linear and stochastic multi-armed bandit problems inspired by LLM
decoding and alignment. In these problems, at each round $t \in [T]$, a user
submits a query (context), and the decision maker (DM) sequentially selects a
token irrevocably from a token set. Once the sequence is complete, the DM
observes a random utility from the user, whose expectation is presented by a
sequence function mapping the chosen token sequence to a nonnegative real value
that depends on the query.
  In both problems, we first show that learning is impossible without any
structure on the sequence function. We introduce a natural assumption,
diminishing distance with more commons (DDMC), and propose algorithms with
regret $\tilde{O}(L\sqrt{T})$ and $\tilde{O}(L\sqrt{T^{2/3}})$ for TLB and
TMAB, respectively. As a side product, we obtain an (almost) optimality of the
greedy decoding for LLM decoding algorithm under DDMC, which justifies the
unresaonable effectiveness of greedy decoding in several tasks. This also has
an immediate application to decoding-time LLM alignment, when the misaligned
utility can be represented as the frozen LLM's utility and a linearly
realizable latent function. We finally validate our algorithm's performance
empirically as well as verify our assumptions using synthetic and real-world
datasets.

</details>


### [249] [EviNet: Evidential Reasoning Network for Resilient Graph Learning in the Open and Noisy Environments](https://arxiv.org/abs/2506.07288)
*Weijie Guan,Haohui Wang,Jian Kang,Lihui Liu,Dawei Zhou*

Main category: cs.LG

TL;DR: 本文提出Evidential Reasoning Network (EVINET) 框架，用于开放和嘈杂环境下的图学习，解决误分类检测和分布外检测问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图学习常基于封闭世界假设，为在开放和嘈杂环境中实现有效图学习，需解决误分类检测和分布外检测问题。

Method: 引入EVINET框架，集成Beta嵌入到主观逻辑框架中，包含用于误分类检测的Dissonance Reasoning和用于分布外检测的Vacuity Reasoning两个关键模块。

Result: 广泛实验表明，EVINET在多项指标上优于现有方法。

Conclusion: EVINET证明了不确定性估计和逻辑推理在误分类检测和分布外检测中的必要性，为开放世界图学习铺平了道路。

Abstract: Graph learning has been crucial to many real-world tasks, but they are often
studied with a closed-world assumption, with all possible labels of data known
a priori. To enable effective graph learning in an open and noisy environment,
it is critical to inform the model users when the model makes a wrong
prediction to in-distribution data of a known class, i.e., misclassification
detection or when the model encounters out-of-distribution from novel classes,
i.e., out-of-distribution detection. This paper introduces Evidential Reasoning
Network (EVINET), a framework that addresses these two challenges by
integrating Beta embedding within a subjective logic framework. EVINET includes
two key modules: Dissonance Reasoning for misclassification detection and
Vacuity Reasoning for out-of-distribution detection. Extensive experiments
demonstrate that EVINET outperforms state-of-the-art methods across multiple
metrics in the tasks of in-distribution classification, misclassification
detection, and out-of-distribution detection. EVINET demonstrates the necessity
of uncertainty estimation and logical reasoning for misclassification detection
and out-of-distribution detection and paves the way for open-world graph
learning. Our code and data are available at https://github.com/SSSKJ/EviNET.

</details>


### [250] [Pre-trained Large Language Models Learn Hidden Markov Models In-context](https://arxiv.org/abs/2506.07298)
*Yijia Dai,Zhaolin Gao,Yahya Satter,Sarah Dean,Jennifer J. Sun*

Main category: cs.LG

TL;DR: 研究表明预训练大语言模型（LLMs）可通过上下文学习（ICL）有效建模隐马尔可夫模型（HMM）生成的数据，在合成和真实数据上都有良好表现。


<details>
  <summary>Details</summary>
Motivation: HMM拟合真实数据存在计算挑战，探索利用LLMs的ICL能力来建模HMM生成的数据。

Method: 利用LLMs的ICL能力，在合成HMM和真实动物决策任务数据上进行实验。

Result: 在合成HMM上预测准确率接近理论最优，发现新的缩放趋势；在真实动物决策任务上，ICL与人类专家设计的模型表现相当。

Conclusion: 首次证明ICL能学习和预测HMM生成的序列，加深了对LLMs中ICL的理解，确立了其作为挖掘复杂科学数据隐藏结构的有力工具的潜力。

Abstract: Hidden Markov Models (HMMs) are foundational tools for modeling sequential
data with latent Markovian structure, yet fitting them to real-world data
remains computationally challenging. In this work, we show that pre-trained
large language models (LLMs) can effectively model data generated by HMMs via
in-context learning (ICL)$\unicode{x2013}$their ability to infer patterns from
examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve
predictive accuracy approaching the theoretical optimum. We uncover novel
scaling trends influenced by HMM properties, and offer theoretical conjectures
for these empirical observations. We also provide practical guidelines for
scientists on using ICL as a diagnostic tool for complex data. On real-world
animal decision-making tasks, ICL achieves competitive performance with models
designed by human experts. To our knowledge, this is the first demonstration
that ICL can learn and predict HMM-generated sequences$\unicode{x2013}$an
advance that deepens our understanding of in-context learning in LLMs and
establishes its potential as a powerful tool for uncovering hidden structure in
complex scientific data.

</details>


### [251] [Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in Deployed Inference](https://arxiv.org/abs/2506.07311)
*Thomas Joshi,Herman Saini,Neil Dhillon,Antoni Viros i Martin,Kaoutar El Maghraoui*

Main category: cs.LG

TL;DR: 本文介绍了将PagedAttention与PyTorch的FlexAttention集成，实现高效KV缓存处理，降低长上下文推理延迟并开源代码。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文推理中因传统KV缓存处理方式存在严重内存效率问题。

Method: 将PagedAttention与PyTorch的FlexAttention集成，在IBM的FMS中实现融合注意力内核来收集分散的KV数据。

Result: 在NVIDIA L4 GPU上的基准测试显示，使用全局KV缓存时推理延迟显著降低，仅线性增长；单步评估时峰值内存使用基本不变，分页注意力仅在序列长度超2048个令牌时增加少量内存使用。

Conclusion: 开源完整实现并讨论了其对未来长上下文模型部署的影响。

Abstract: Large Language Models (LLMs) encounter severe memory inefficiencies during
long-context inference due to conventional handling of key-value (KV) caches.
In this work, we introduce a novel integration of PagedAttention with PyTorch's
FlexAttention, addressing internal fragmentation and inefficiencies associated
with monolithic KV cache allocations. Implemented within IBM's Foundation Model
Stack (FMS), our fused attention kernel efficiently gathers scattered KV data.
Our benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced
inference latency, growing only linearly (~2x) with sequence length from 128 to
2048 tokens when utilizing a global KV cache, compared to exponential latency
increases without caching. While peak memory usage remains largely unchanged
for single-step evaluations (dominated by model weights and activations), paged
attention causes minimal incremental memory usage, observable only at sequence
lengths exceeding 2048 tokens due to its power-of-two cache allocations. We
open-source the full implementation and discuss its implications for future
long-context model deployment.

</details>


### [252] [Generative Modeling of Networked Time-Series via Transformer Architectures](https://arxiv.org/abs/2506.07312)
*Yusuf Elnady*

Main category: cs.LG

TL;DR: 为解决安全领域数据访问受限问题，设计基于Transformer的生成框架生成时间序列数据，模型达SOTA，具通用性和高质量。


<details>
  <summary>Details</summary>
Motivation: 安全和网络应用需大数据集训练模型，但安全领域存在数据访问受限问题，现有Transformer合成样本无法提升模型性能。

Method: 设计基于Transformer的高效生成框架来生成时间序列数据。

Result: 新的Transformer模型取得了SOTA结果。

Conclusion: 设计的模型具有通用性，能在不同数据集上工作，可生成高质量样本以提升ML工作流性能。

Abstract: Many security and network applications require having large datasets to train
the machine learning models. Limited data access is a well-known problem in the
security domain. Recent studies have shown the potential of Transformer models
to enlarge the size of data by synthesizing new samples, but the synthesized
samples don't improve the models over the real data. To address this issue, we
design an efficient transformer-based model as a generative framework to
generate time-series data, that can be used to boost the performance of
existing and new ML workflows. Our new transformer model achieves the SOTA
results. We style our model to be generalizable and work across different
datasets, and produce high-quality samples.

</details>


### [253] [DEF: Diffusion-augmented Ensemble Forecasting](https://arxiv.org/abs/2506.07324)
*David Millard,Arielle Carr,Stéphane Gaudreault,Ali Baheri*

Main category: cs.LG

TL;DR: 提出DEF方法生成初始条件扰动，将确定性神经预测系统转为随机系统，在ERA5数据集上验证有更好预测表现。


<details>
  <summary>Details</summary>
Motivation: 现代初始条件扰动方法主要为数值天气预报求解器设计，在机器学习天气预报领域适用性有限，随机模型常需逐个开发。

Method: 使用简单条件扩散模型生成有意义的结构化扰动，可迭代应用，并利用引导项控制扰动水平，将确定性神经预测系统转为随机系统。

Result: 随机扩展系统在长期预报中积累误差更少，能产生有意义的预报分布，在ERA5数据集上有更好的预测性能和合理的离散度估计。

Conclusion: DEF方法有效，能提高机器学习天气预报的性能。

Abstract: We present DEF (\textbf{\ul{D}}iffusion-augmented \textbf{\ul{E}}nsemble
\textbf{\ul{F}}orecasting), a novel approach for generating initial condition
perturbations. Modern approaches to initial condition perturbations are
primarily designed for numerical weather prediction (NWP) solvers, limiting
their applicability in the rapidly growing field of machine learning for
weather prediction. Consequently, stochastic models in this domain are often
developed on a case-by-case basis. We demonstrate that a simple conditional
diffusion model can (1) generate meaningful structured perturbations, (2) be
applied iteratively, and (3) utilize a guidance term to intuitivey control the
level of perturbation. This method enables the transformation of any
deterministic neural forecasting system into a stochastic one. With our
stochastic extended systems, we show that the model accumulates less error over
long-term forecasts while producing meaningful forecast distributions. We
validate our approach on the 5.625$^\circ$ ERA5 reanalysis dataset, which
comprises atmospheric and surface variables over a discretized global grid,
spanning from the 1960s to the present. On this dataset, our method
demonstrates improved predictive performance along with reasonable spread
estimates.

</details>


### [254] [Mobility-Aware Asynchronous Federated Learning with Dynamic Sparsification](https://arxiv.org/abs/2506.07328)
*Jintao Yan,Tan Chen,Yuxuan Sun,Zhaojun Nan,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: 论文针对异步联邦学习中设备移动性影响收敛的问题，构建理论模型，提出MADS算法，实验验证其有效性且优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 设备移动性导致异步联邦学习出现间歇性连接，需梯度稀疏化且产生模型陈旧问题，影响收敛，故开展研究。

Method: 构建理论模型分析稀疏化、模型陈旧和移动接触模式的相互作用及对收敛的影响，提出基于接触时间和模型陈旧度优化稀疏度的MADS算法并推导闭式解。

Result: 实验验证理论结果，MADS算法使CIFAR - 10数据集图像分类准确率提高8.76%，Argoverse轨迹预测数据集平均位移误差降低9.46%。

Conclusion: MADS算法有效，能根据设备移动速度优化稀疏度，提升异步联邦学习性能。

Abstract: Asynchronous Federated Learning (AFL) enables distributed model training
across multiple mobile devices, allowing each device to independently update
its local model without waiting for others. However, device mobility introduces
intermittent connectivity, which necessitates gradient sparsification and leads
to model staleness, jointly affecting AFL convergence. This paper develops a
theoretical model to characterize the interplay among sparsification, model
staleness and mobility-induced contact patterns, and their joint impact on AFL
convergence. Based on the analysis, we propose a mobility-aware dynamic
sparsification (MADS) algorithm that optimizes the sparsification degree based
on contact time and model staleness. Closed-form solutions are derived, showing
that under low-speed conditions, MADS increases the sparsification degree to
enhance convergence, while under high-speed conditions, it reduces the
sparsification degree to guarantee reliable uploads within limited contact
time. Experimental results validate the theoretical findings. Compared with the
state-of-the-art benchmarks, the MADS algorithm increases the image
classification accuracy on the CIFAR-10 dataset by 8.76% and reduces the
average displacement error in the Argoverse trajectory prediction dataset by
9.46%.

</details>


### [255] [JavelinGuard: Low-Cost Transformer Architectures for LLM Security](https://arxiv.org/abs/2506.07330)
*Yash Datta,Sharath Rajasekar*

Main category: cs.LG

TL;DR: 介绍JavelinGuard，一套用于检测大语言模型交互中恶意意图的架构，经多种数据集测试，各架构有不同权衡。


<details>
  <summary>Details</summary>
Motivation: 设计低成本、高性能的模型架构，用于检测大语言模型交互中的恶意意图，并优化以用于生产部署。

Method: 系统探索五种基于Transformer的架构，在九个不同的对抗性数据集上进行严格基准测试，并与开源护栏模型和大的仅解码器大语言模型比较。

Result: 各架构在速度、可解释性和资源要求上有独特权衡，Raudra的多任务设计总体性能最稳健。

Conclusion: 研究结果能指导从业者为现实世界的大语言模型安全应用选择复杂度和效率的最佳平衡。

Abstract: We present JavelinGuard, a suite of low-cost, high-performance model
architectures designed for detecting malicious intent in Large Language Model
(LLM) interactions, optimized specifically for production deployment. Recent
advances in transformer architectures, including compact BERT(Devlin et al.
2019) variants (e.g., ModernBERT (Warner et al. 2024)), allow us to build
highly accurate classifiers with as few as approximately 400M parameters that
achieve rapid inference speeds even on standard CPU hardware. We systematically
explore five progressively sophisticated transformer-based architectures:
Sharanga (baseline transformer classifier), Mahendra (enhanced
attention-weighted pooling with deeper heads), Vaishnava and Ashwina (hybrid
neural ensemble architectures), and Raudra (an advanced multi-task framework
with specialized loss functions). Our models are rigorously benchmarked across
nine diverse adversarial datasets, including popular sets like the NotInject
series, BIPIA, Garak, ImprovedLLM, ToxicChat, WildGuard, and our newly
introduced JavelinBench, specifically crafted to test generalization on
challenging borderline and hard-negative cases. Additionally, we compare our
architectures against leading open-source guardrail models as well as large
decoder-only LLMs such as gpt-4o, demonstrating superior cost-performance
trade-offs in terms of accuracy, and latency. Our findings reveal that while
Raudra's multi-task design offers the most robust performance overall, each
architecture presents unique trade-offs in speed, interpretability, and
resource requirements, guiding practitioners in selecting the optimal balance
of complexity and efficiency for real-world LLM security applications.

</details>


### [256] [Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models](https://arxiv.org/abs/2506.07334)
*Haoyu Wang,Peihao Wang,Mufei Li,Shikun Liu,Siqi Miao,Zhangyang Wang,Pan Li*

Main category: cs.LG

TL;DR: 本文提出Graph - KV方法克服大语言模型序列化输入的局限，经多场景评估显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型的序列化输入方式阻碍其利用结构归纳偏置，在RAG和图结构数据推理等任务中受限。

Method: 引入Graph - KV，利用文本段的KV缓存作为浓缩表示，通过结构归纳偏置控制交互，诱导图结构块掩码，稀疏注意力，并减少位置偏差和上下文窗口消耗。

Result: 在RAG基准测试、Arxiv - QA、论文主题分类等场景中，Graph - KV显著优于包括标准顺序编码在内的基线。

Conclusion: Graph - KV通过有效减少位置偏差和利用结构归纳偏置，在多场景表现出色，代码和数据公开。

Abstract: Modern large language models (LLMs) are inherently auto-regressive, requiring
input to be serialized into flat sequences regardless of their structural
dependencies. This serialization hinders the model's ability to leverage
structural inductive biases, especially in tasks such as retrieval-augmented
generation (RAG) and reasoning on data with native graph structures, where
inter-segment dependencies are crucial. We introduce Graph-KV with the
potential to overcome this limitation. Graph-KV leverages the KV-cache of text
segments as condensed representations and governs their interaction through
structural inductive biases. In this framework, 'target' segments selectively
attend only to the KV-caches of their designated 'source' segments, rather than
all preceding segments in a serialized sequence. This approach induces a
graph-structured block mask, sparsifying attention and enabling a
message-passing-like step within the LLM. Furthermore, strategically allocated
positional encodings for source and target segments reduce positional bias and
context window consumption. We evaluate Graph-KV across three scenarios: (1)
seven RAG benchmarks spanning direct inference, multi-hop reasoning, and
long-document understanding; (2) Arxiv-QA, a novel academic paper QA task with
full-text scientific papers structured as citation ego-graphs; and (3) paper
topic classification within a citation network. By effectively reducing
positional bias and harnessing structural inductive biases, Graph-KV
substantially outperforms baselines, including standard costly sequential
encoding, across various settings. Code and the Graph-KV data are publicly
available.

</details>


### [257] [SALT: A Lightweight Model Adaptation Method for Closed Split Computing Environments](https://arxiv.org/abs/2506.07355)
*Yuya Okada,Takayuki Nishio*

Main category: cs.LG

TL;DR: 提出轻量级模型适配框架SALT，用于封闭约束下的拆分计算，评估显示其准确率高、训练延迟低，能在有损耗网络下实现模型适配。


<details>
  <summary>Details</summary>
Motivation: 在封闭环境中，传统适配方法因无法访问模型参数或架构而不可行，需要新的适配方案。

Method: 在客户端引入紧凑、可训练的适配器来提炼头部网络的潜在特征，实现用户特定适配。

Result: 在CIFAR - 10和CIFAR - 100的用户特定分类任务中，与微调方法相比，准确率提高且训练延迟降低，还能在有损耗网络下进行模型适配。

Conclusion: SALT以最小的部署开销，为严格系统约束下的边缘AI系统中的个性化推理提供了实用解决方案。

Abstract: We propose SALT (Split-Adaptive Lightweight Tuning), a lightweight model
adaptation framework for Split Computing under closed constraints, where the
head and tail networks are proprietary and inaccessible to users. In such
closed environments, conventional adaptation methods are infeasible since they
require access to model parameters or architectures. SALT addresses this
challenge by introducing a compact, trainable adapter on the client side to
refine latent features from the head network, enabling user-specific adaptation
without modifying the original models or increasing communication overhead. We
evaluate SALT on user-specific classification tasks with CIFAR-10 and
CIFAR-100, demonstrating improved accuracy with lower training latency compared
to fine-tuning methods. Furthermore, SALT facilitates model adaptation for
robust inference over lossy networks, a common challenge in edge-cloud
environments. With minimal deployment overhead, SALT offers a practical
solution for personalized inference in edge AI systems under strict system
constraints.

</details>


### [258] [MoE-GPS: Guidlines for Prediction Strategy for Dynamic Expert Duplication in MoE Load Balancing](https://arxiv.org/abs/2506.07366)
*Haiyue Ma,Zhixu Du,Yiran Chen*

Main category: cs.LG

TL;DR: 论文探讨多GPU MoE网络预测策略权衡，提出MoE - GPS框架，倡导仅预测整体令牌分布的策略，在数据集上提升推理性能超23%。


<details>
  <summary>Details</summary>
Motivation: 多GPU MoE网络存在负载不均衡问题，现有工作需预测分布，要探讨预测策略、准确性、开销和系统性能的权衡。

Method: 提出MoE - GPS框架，通过量化对系统级模型运行时间的性能影响来选择最优预测器设计，倡导Distribution - Only Prediction策略。

Result: 在Mixtral 8x7B MMLU数据集上，MoE - GPS建议的Distribution - Only Prediction策略比Token - to - Expert Prediction策略提升端到端推理性能超23%。

Conclusion: Distribution - Only Prediction策略能显著降低开销，有效提升多GPU MoE网络的端到端推理性能。

Abstract: In multi-GPU Mixture-of-Experts (MoE) network, experts are distributed across
different GPUs, which creates load imbalance as each expert processes different
number of tokens. Recent works improve MoE inference load balance by
dynamically duplicating popular experts to more GPUs to process excessive
tokens, which requires predicting the distribution before routing. In this
paper, we discuss the tradeoff of prediction strategies, accuracies, overhead,
and end-to-end system performance. We propose MoE-GPS, a framework that guides
the selection of the optimal predictor design under various system
configurations, by quantifying the performance impact to system-level model
runtime. Specifically, we advocate for Distribution-Only Prediction, a
prediction strategy that only predicts overall token distribution which
significantly reduces overhead compared to the traditional Token-to-Expert
Prediction. On Mixtral 8x7B MMLU dataset, MoE-GPS suggests Distribution-Only
Prediction which improves end-to-end inference performance by more than 23%
compared with Token-to-Expert Prediction.

</details>


### [259] [RiemannFormer: A Framework for Attention in Curved Spaces](https://arxiv.org/abs/2506.07405)
*Zhongping Ji*

Main category: cs.LG

TL;DR: 研究旨在挖掘基于transformer架构的潜力，给出注意力机制的几何解释，实验显示模块性能有显著提升。


<details>
  <summary>Details</summary>
Motivation: 为transformer中的注意力机制提供几何解释，挖掘其进一步潜力。

Method: 在框架中让注意力涉及度量张量、切空间、内积等，通过切向量的平行输运关联离散位置的量和结构；通过巧妙的预定义配置减少参数数量；引入显式机制衰减远程值以突出邻域。

Result: 模块相对于基线有显著的性能提升，后续将陆续开展视觉和大语言模型的评估实验。

Conclusion: 提出的方法有效，可提升基于transformer架构的性能。

Abstract: This research endeavors to offer insights into unlocking the further
potential of transformer-based architectures. One of the primary motivations is
to offer a geometric interpretation for the attention mechanism in
transformers. In our framework, the attention mainly involves metric tensors,
tangent spaces, inner product, and how they relate to each other. These
quantities and structures at discrete positions are intricately interconnected
via the parallel transport of tangent vectors. To make the learning process
more efficient, we reduce the number of parameters through ingenious predefined
configurations. Moreover, we introduce an explicit mechanism to highlight a
neighborhood by attenuating the remote values, given that transformers
inherently neglect local inductive bias. Experimental results demonstrate that
our modules deliver significant performance improvements relative to the
baseline. More evaluation experiments on visual and large language models will
be launched successively.

</details>


### [260] [InverseScope: Scalable Activation Inversion for Interpreting Large Language Models](https://arxiv.org/abs/2506.07406)
*Yifan Luo,Zhennan Zhou,Bin Dong*

Main category: cs.LG

TL;DR: 提出InverseScope框架用于解释神经激活，通过输入反演，结合新架构提升采样效率并引入评估协议，可扩展到更大模型和实际任务。


<details>
  <summary>Details</summary>
Motivation: 现有特征可解释性方法对表示结构有强假设，在实际中可能不成立，需理解大语言模型内部表示。

Method: 引入InverseScope框架，通过输入反演解释神经激活；提出新的条件生成架构提升采样效率；引入定量评估协议。

Result: 新架构显著提高采样效率，InverseScope能将基于反演的可解释性方法扩展到更大模型和实际任务。

Conclusion: InverseScope可对现实世界大语言模型的内部表示进行系统和定量分析。

Abstract: Understanding the internal representations of large language models (LLMs) is
a central challenge in interpretability research. Existing feature
interpretability methods often rely on strong assumptions about the structure
of representations that may not hold in practice. In this work, we introduce
InverseScope, an assumption-light and scalable framework for interpreting
neural activations via input inversion. Given a target activation, we define a
distribution over inputs that generate similar activations and analyze this
distribution to infer the encoded features. To address the inefficiency of
sampling in high-dimensional spaces, we propose a novel conditional generation
architecture that significantly improves sample efficiency compared to previous
methods. We further introduce a quantitative evaluation protocol that tests
interpretability hypotheses using feature consistency rate computed over the
sampled inputs. InverseScope scales inversion-based interpretability methods to
larger models and practical tasks, enabling systematic and quantitative
analysis of internal representations in real-world LLMs.

</details>


### [261] [Anomaly Detection and Early Warning Mechanism for Intelligent Monitoring Systems in Multi-Cloud Environments Based on LLM](https://arxiv.org/abs/2506.07407)
*Yihong Jin,Ze Yang,Juntian Liu,Xinhe Xu*

Main category: cs.LG

TL;DR: 本文提出基于大语言模型的多云环境智能监控系统异常检测与预警机制，实验表明该模型效果优于传统系统。


<details>
  <summary>Details</summary>
Motivation: 随着多云环境快速发展，确保智能监控系统的安全性和可靠性变得愈发重要。

Method: 在现有监控框架基础上，引入多级特征提取方法，结合大语言模型的自然语言处理能力和传统机器学习方法，引入大语言模型的上下文理解能力。

Result: 该模型在检测准确率和延迟方面显著优于传统异常检测系统，显著提高了云基础设施的弹性和主动管理能力。

Conclusion: 所提出的基于大语言模型的异常检测与预警机制是有效的，能提升多云环境智能监控系统性能。

Abstract: With the rapid development of multi-cloud environments, it is increasingly
important to ensure the security and reliability of intelligent monitoring
systems. In this paper, we propose an anomaly detection and early warning
mechanism for intelligent monitoring system in multi-cloud environment based on
Large-Scale Language Model (LLM). On the basis of the existing monitoring
framework, the proposed model innovatively introduces a multi-level feature
extraction method, which combines the natural language processing ability of
LLM with traditional machine learning methods to enhance the accuracy of
anomaly detection and improve the real-time response efficiency. By introducing
the contextual understanding capabilities of LLMs, the model dynamically adapts
to different cloud service providers and environments, so as to more
effectively detect abnormal patterns and predict potential failures.
Experimental results show that the proposed model is significantly better than
the traditional anomaly detection system in terms of detection accuracy and
latency, and significantly improves the resilience and active management
ability of cloud infrastructure.

</details>


### [262] [Fractional-order Jacobian Matrix Differentiation and Its Application in Artificial Neural Networks](https://arxiv.org/abs/2506.07408)
*Xiaojun zhou,Chunna Zhao,Yaqun Huang,Chengli Zhou,Junjie Ye,Kemeng Xiang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Fractional-order differentiation has many characteristics different from
integer-order differentiation. These characteristics can be applied to the
optimization algorithms of artificial neural networks to obtain better results.
However, due to insufficient theoretical research, at present, there is no
fractional-order matrix differentiation method that is perfectly compatible
with automatic differentiation (Autograd) technology. Therefore, we propose a
fractional-order matrix differentiation calculation method. This method is
introduced by the definition of the integer-order Jacobian matrix. We denote it
as fractional-order Jacobian matrix differentiation (${{\bf{J}}^\alpha }$).
Through ${{\bf{J}}^\alpha }$, we can carry out the matrix-based
fractional-order chain rule. Based on the Linear module and the
fractional-order differentiation, we design the fractional-order Autograd
technology to enable the use of fractional-order differentiation in hidden
layers, thereby enhancing the practicality of fractional-order differentiation
in deep learning. In the experiment, according to the PyTorch framework, we
design fractional-order Linear (FLinear) and replace nn.Linear in the
multilayer perceptron with FLinear. Through the qualitative analysis of the
training set and validation set $Loss$, the quantitative analysis of the test
set indicators, and the analysis of time consumption and GPU memory usage
during model training, we verify the superior performance of ${{\bf{J}}^\alpha
}$ and prove that it is an excellent fractional-order gradient descent method
in the field of deep learning.

</details>


### [263] [Variational Supervised Contrastive Learning](https://arxiv.org/abs/2506.07413)
*Ziwen Wang,Jiajun Fan,Thao Nguyen,Heng Ji,Ge Liu*

Main category: cs.LG

TL;DR: 提出VarCon解决对比学习局限性，实验显示其在多数据集表现佳，收敛快且有优势。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习存在无嵌入分布调控和依赖大量负样本及特定增强的局限性，需改进。

Method: 提出VarCon，将监督对比学习重新表述为对潜在类变量的变分推理，最大化后验加权证据下界。

Result: 在多数据集上达到对比学习框架的最优性能，收敛快，嵌入空间决策边界更清晰，少样本学习和增强策略鲁棒性好。

Conclusion: VarCon有效解决了现有对比学习的局限性，有良好性能和优势。

Abstract: Contrastive learning has proven to be highly efficient and adaptable in
shaping representation spaces across diverse modalities by pulling similar
samples together and pushing dissimilar ones apart. However, two key
limitations persist: (1) Without explicit regulation of the embedding
distribution, semantically related instances can inadvertently be pushed apart
unless complementary signals guide pair selection, and (2) excessive reliance
on large in-batch negatives and tailored augmentations hinders generalization.
To address these limitations, we propose Variational Supervised Contrastive
Learning (VarCon), which reformulates supervised contrastive learning as
variational inference over latent class variables and maximizes a
posterior-weighted evidence lower bound (ELBO) that replaces exhaustive
pair-wise comparisons for efficient class-aware matching and grants
fine-grained control over intra-class dispersion in the embedding space.
Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100,
ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art
performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy
on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while
converging in just 200 epochs; (2) yields substantially clearer decision
boundaries and semantic organization in the embedding space, as evidenced by
KNN classification, hierarchical clustering results, and transfer-learning
assessments; and (3) demonstrates superior performance in few-shot learning
than supervised baseline and superior robustness across various augmentation
strategies.

</details>


### [264] [LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments](https://arxiv.org/abs/2506.07416)
*Jin Huang,Yuchao Jin,Le An,Josh Park*

Main category: cs.LG

TL;DR: 提出适用于嵌入式设备的高效视觉语言模型（VLM）管道，减少计算开销并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 为在资源受限的嵌入式设备（如机器人和自动驾驶）上部署视觉语言模型，需降低计算开销。

Method: 联合使用补丁选择过滤无关相机视图、令牌选择模块减少大语言模型输入序列长度、推测解码加速令牌生成。

Result: 在NVIDIA DRIVE Thor平台上实现2.5倍端到端延迟降低，使用FP8训练后量化加速比达3.2倍，且不影响任务准确性。

Conclusion: 该管道是在资源受限环境中实现实时VLM部署的可行方案。

Abstract: This paper introduces an efficient Vision-Language Model (VLM) pipeline
specifically optimized for deployment on embedded devices, such as those used
in robotics and autonomous driving. The pipeline significantly reduces the
computational overhead by jointly leveraging patch selection to filter
irrelevant camera views, a token selection module to reduce input sequence
length for the LLM, and speculative decoding to accelerate token generation.
Evaluation on the NVIDIA DRIVE Thor platform for automonous driving
application, our pipeline achieves $2.5\times$ end-to-end latency reduction
without compromising task accuracy. The speed-up further increases to
$3.2\times$ when applying FP8 post-training quantization. These results
demonstrate our pipeline as a viable solution for enabling real-time VLM
deployment in resource-constrained environments.

</details>


### [265] [Evidential Spectrum-Aware Contrastive Learning for OOD Detection in Dynamic Graphs](https://arxiv.org/abs/2506.07417)
*Nan Sun,Xixun Lin,Zhiheng Zhou,Yanmin Shang,Zhenlin Cheng,Yanan Cao*

Main category: cs.LG

TL;DR: 本文聚焦动态图的OOD检测，指出当前静态图检测范式问题，提出EviSAC检测器，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 现有静态图OOD检测范式存在单点估计导致的高偏差高方差和缺乏OOD训练数据导致的分数同质化问题，需解决动态图的OOD检测。

Method: 从证据深度学习角度研究动态图OOD检测，提出EviSAC检测器，设计证据神经网络，利用频谱感知增强模块生成OOD近似。

Result: 在真实数据集上的大量实验表明EviSAC能有效检测动态图中的OOD样本。

Conclusion: EviSAC是一种创新有效的动态图OOD检测器，可解决现有检测范式的问题。

Abstract: Recently, Out-of-distribution (OOD) detection in dynamic graphs, which aims
to identify whether incoming data deviates from the distribution of the
in-distribution (ID) training set, has garnered considerable attention in
security-sensitive fields. Current OOD detection paradigms primarily focus on
static graphs and confront two critical challenges: i) high bias and high
variance caused by single-point estimation, which makes the predictions
sensitive to randomness in the data; ii) score homogenization resulting from
the lack of OOD training data, where the model only learns ID-specific
patterns, resulting in overall low OOD scores and a narrow score gap between ID
and OOD data. To tackle these issues, we first investigate OOD detection in
dynamic graphs through the lens of Evidential Deep Learning (EDL).
Specifically, we propose EviSEC, an innovative and effective OOD detector via
Evidential Spectrum-awarE Contrastive Learning. We design an evidential neural
network to redefine the output as the posterior Dirichlet distribution,
explaining the randomness of inputs through the uncertainty of distribution,
which is overlooked by single-point estimation. Moreover, spectrum-aware
augmentation module generates OOD approximations to identify patterns with high
OOD scores, thereby widening the score gap between ID and OOD data and
mitigating score homogenization. Extensive experiments on real-world datasets
demonstrate that EviSAC effectively detects OOD samples in dynamic graphs.

</details>


### [266] [Federated In-Context Learning: Iterative Refinement for Improved Answer Quality](https://arxiv.org/abs/2506.07440)
*Ruhan Wang,Zhiyong Wang,Chengkai Huang,Rui Wang,Tong Yu,Lina Yao,John C. S. Lui,Dongruo Zhou*

Main category: cs.LG

TL;DR: 针对问答任务中上下文学习（ICL）高质量示例稀缺及现有方法局限性，提出联邦上下文学习（Fed - ICL）框架，理论证明收敛性并实验验证其高性能和低通信成本。


<details>
  <summary>Details</summary>
Motivation: 问答任务中ICL依赖高质量示例，但此类示例稀缺，现有利用客户端设备示例的方法存在通信开销大或无法充分利用本地数据的问题。

Method: 提出Fed - ICL框架，通过客户端和中央服务器的多轮交互迭代优化回答，且无需传输模型参数。

Result: 建立了Fed - ICL收敛性的理论保证，在标准问答基准上的大量实验表明该方法性能强且通信成本低。

Conclusion: Fed - ICL能有效解决现有问题，在提高回答质量的同时降低通信成本。

Abstract: For question-answering (QA) tasks, in-context learning (ICL) enables language
models to generate responses without modifying their parameters by leveraging
examples provided in the input. However, the effectiveness of ICL heavily
depends on the availability of high-quality examples, which are often scarce
due to data privacy constraints, annotation costs, and distribution
disparities. A natural solution is to utilize examples stored on client
devices, but existing approaches either require transmitting model parameters -
incurring significant communication overhead - or fail to fully exploit local
datasets, limiting their effectiveness. To address these challenges, we propose
Federated In-Context Learning (Fed-ICL), a general framework that enhances ICL
through an iterative, collaborative process. Fed-ICL progressively refines
responses by leveraging multi-round interactions between clients and a central
server, improving answer quality without the need to transmit model parameters.
We establish theoretical guarantees for the convergence of Fed-ICL and conduct
extensive experiments on standard QA benchmarks, demonstrating that our
proposed approach achieves strong performance while maintaining low
communication costs.

</details>


### [267] [Extending Epistemic Uncertainty Beyond Parameters Would Assist in Designing Reliable LLMs](https://arxiv.org/abs/2506.07448)
*T. Duy Nguyen-Hien,Desi R. Ivanova,Yee Whye Teh,Wee Sun Lee*

Main category: cs.LG

TL;DR: 现有大语言模型部署中确保可靠性方法保守，本文提倡用贝叶斯实验建模框架主动处理不确定性，以构建更可靠系统。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型部署中确保可靠性的方法有限且保守，缺乏系统区分和应对不同不确定性来源的工具。

Method: 采用贝叶斯实验建模框架来管理和主动处理大语言模型部署中出现的不确定性。

Result: 该框架使大语言模型及其用户能采取合适步骤，如请求澄清、检索外部信息或优化输入。

Conclusion: 该框架支持主动解决而非被动回避，有助于构建更可靠、透明且广泛适用的大语言模型系统，尤其适用于高风险的现实场景。

Abstract: Although large language models (LLMs) are highly interactive and extendable,
current approaches to ensure reliability in deployments remain mostly limited
to rejecting outputs with high uncertainty in order to avoid misinformation.
This conservative strategy reflects the current lack of tools to systematically
distinguish and respond to different sources of uncertainty. In this paper, we
advocate for the adoption of Bayesian Modeling of Experiments -- a framework
that provides a coherent foundation to reason about uncertainty and clarify the
reducibility of uncertainty -- for managing and proactively addressing
uncertainty that arises in LLM deployments. This framework enables LLMs and
their users to take contextually appropriate steps, such as requesting
clarification, retrieving external information, or refining inputs. By
supporting active resolution rather than passive avoidance, it opens the door
to more reliable, transparent, and broadly applicable LLM systems, particularly
in high-stakes, real-world settings.

</details>


### [268] [When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment](https://arxiv.org/abs/2506.07452)
*Yuxin Xiao,Sana Tonekaboni,Walter Gerych,Vinith Suriyakumar,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 研究风格模式对大语言模型安全的影响，评估32个模型，发现风格模式会提高攻击成功率，提出SafeStyle防御策略且表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 明确风格模式（与越狱查询恶意意图语义无关）对大语言模型安全的影响。

Method: 对32个大语言模型在7个越狱基准上进行评估，研究表面风格对齐情况，提出SafeStyle防御策略。

Result: 恶意查询中的风格模式会提高几乎所有模型的攻击成功率，特定风格微调会使模型更易受同风格越狱攻击，SafeStyle在维护模型安全上优于基线。

Conclusion: 风格模式会影响大语言模型安全，SafeStyle能有效维护模型安全。

Abstract: Large language models (LLMs) can be prompted with specific styles (e.g.,
formatting responses as lists), including in jailbreak queries. Although these
style patterns are semantically unrelated to the malicious intents behind
jailbreak queries, their safety impact remains unclear. In this work, we seek
to understand whether style patterns compromise LLM safety, how superficial
style alignment increases model vulnerability, and how best to mitigate these
risks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks,
and find that malicious queries with style patterns inflate the attack success
rate (ASR) for nearly all models. Notably, ASR inflation correlates with both
the length of style patterns and the relative attention an LLM exhibits on
them. We then investigate superficial style alignment, and find that
fine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of
those same styles. Finally, we propose SafeStyle, a defense strategy that
incorporates a small amount of safety training data augmented to match the
distribution of style patterns in the fine-tuning data. Across three LLMs and
five fine-tuning style settings, SafeStyle consistently outperforms baselines
in maintaining LLM safety.

</details>


### [269] [ProteinZero: Self-Improving Protein Generation via Online Reinforcement Learning](https://arxiv.org/abs/2506.07459)
*Ziwen Wang,Jiajun Fan,Ruihan Guo,Thao Nguyen,Heng Ji,Ge Liu*

Main category: cs.LG

TL;DR: 提出ProteinZero框架，用在线强化学习改进逆向折叠模型，在蛋白质设计多指标上超现有方法，降低失败率，开辟新范式。


<details>
  <summary>Details</summary>
Motivation: 蛋白质生成模型因高质量监督预训练数据集稀缺，设计成功率受限。

Method: 引入基于ESM - fold的高效代理奖励模型和快速ddG预测器，采用平衡多奖励最大化、KL散度和蛋白质嵌入级多样性正则化的通用RL框架。

Result: ProteinZero在蛋白质设计关键指标上大幅超越现有方法，降低约36% - 48%的设计失败率，成功率超90%，在单8X GPU节点上3天内可完成CATH - 4.3的RL运行。

Conclusion: 建立了蛋白质设计新范式，模型可从自身生成输出持续进化，为探索蛋白质设计空间提供新可能。

Abstract: Protein generative models have shown remarkable promise in protein design but
still face limitations in success rate, due to the scarcity of high-quality
protein datasets for supervised pretraining. We present ProteinZero, a novel
framework that enables scalable, automated, and continuous self-improvement of
the inverse folding model through online reinforcement learning. To achieve
computationally tractable online feedback, we introduce efficient proxy reward
models based on ESM-fold and a novel rapid ddG predictor that significantly
accelerates evaluation speed. ProteinZero employs a general RL framework
balancing multi-reward maximization, KL-divergence from a reference model, and
a novel protein-embedding level diversity regularization that prevents mode
collapse while promoting higher sequence diversity. Through extensive
experiments, we demonstrate that ProteinZero substantially outperforms existing
methods across every key metric in protein design, achieving significant
improvements in structural accuracy, designability, thermodynamic stability,
and sequence diversity. Most impressively, ProteinZero reduces design failure
rates by approximately 36% - 48% compared to widely-used methods like
ProteinMPNN, ESM-IF and InstructPLM, consistently achieving success rates
exceeding 90% across diverse and complex protein folds. Notably, the entire RL
run on CATH-4.3 can be done with a single 8 X GPU node in under 3 days,
including reward computation. Our work establishes a new paradigm for protein
design where models evolve continuously from their own generated outputs,
opening new possibilities for exploring the vast protein design space.

</details>


### [270] [Circumventing Backdoor Space via Weight Symmetry](https://arxiv.org/abs/2506.07467)
*Jie Peng,Hongwei Yang,Jing Zhao,Hengji Dong,Hui He,Weizhe Zhang,Haoyu He*

Main category: cs.LG

TL;DR: 提出TSC防御方法净化后门攻击，不依赖数据格式，少量干净样本即可，在监督和自监督学习中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有后门防御方法依赖标签数据或特定训练流程，难以用于监督学习之外场景，需新防御方法。

Method: 提出Two - stage Symmetry Connectivity (TSC)，利用神经网络排列不变性和二次模式连通性，放大中毒样本损失并保证干净样本准确率。

Result: 在监督学习中表现与现有最优方法相当，能推广到自监督学习框架，保持强大防御能力。

Conclusion: TSC是一种有效且通用的后门净化防御方法，不依赖数据格式，所需干净样本少。

Abstract: Deep neural networks are vulnerable to backdoor attacks, where malicious
behaviors are implanted during training. While existing defenses can
effectively purify compromised models, they typically require labeled data or
specific training procedures, making them difficult to apply beyond supervised
learning settings. Notably, recent studies have shown successful backdoor
attacks across various learning paradigms, highlighting a critical security
concern. To address this gap, we propose Two-stage Symmetry Connectivity (TSC),
a novel backdoor purification defense that operates independently of data
format and requires only a small fraction of clean samples. Through theoretical
analysis, we prove that by leveraging permutation invariance in neural networks
and quadratic mode connectivity, TSC amplifies the loss on poisoned samples
while maintaining bounded clean accuracy. Experiments demonstrate that TSC
achieves robust performance comparable to state-of-the-art methods in
supervised learning scenarios. Furthermore, TSC generalizes to self-supervised
learning frameworks, such as SimCLR and CLIP, maintaining its strong defense
capabilities. Our code is available at https://github.com/JiePeng104/TSC.

</details>


### [271] [Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models](https://arxiv.org/abs/2506.07468)
*Mickel Liu,Liwei Jiang,Yancheng Liang,Simon Shaolei Du,Yejin Choi,Tim Althoff,Natasha Jaques*

Main category: cs.LG

TL;DR: 提出Self - RedTeam在线自博弈强化学习算法解决传统语言模型安全对齐问题，有理论保证和实证效果，推动从被动修补到主动协同进化转变。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型安全对齐的顺序方法存在攻击者与防御者不匹配、防御滞后问题，需要改进。

Method: 提出Self - RedTeam算法，将安全对齐视为两人零和博弈，单模型交替扮演攻击者和防御者角色，用奖励语言模型评判结果，还提出隐藏思维链。

Result: Self - RedTeam比静态训练方法发现更多样攻击（+21.8% SBERT），在安全基准测试上有更高鲁棒性（如WildJailBreak上+65.5%），隐藏思维链提升对抗多样性并减少过度拒绝。

Conclusion: 应从反应式修补转向主动协同进化进行语言模型安全训练，通过多智能体强化学习实现语言模型可扩展、自主和鲁棒的自我改进。

Abstract: Conventional language model (LM) safety alignment relies on a reactive,
disjoint procedure: attackers exploit a static model, followed by defensive
fine-tuning to patch exposed vulnerabilities. This sequential approach creates
a mismatch -- attackers overfit to obsolete defenses, while defenders
perpetually lag behind emerging threats. To address this, we propose
Self-RedTeam, an online self-play reinforcement learning algorithm where an
attacker and defender agent co-evolve through continuous interaction. We cast
safety alignment as a two-player zero-sum game, where a single model alternates
between attacker and defender roles -- generating adversarial prompts and
safeguarding against them -- while a reward LM adjudicates outcomes. This
enables dynamic co-adaptation. Grounded in the game-theoretic framework of
zero-sum games, we establish a theoretical safety guarantee which motivates the
design of our method: if self-play converges to a Nash Equilibrium, the
defender will reliably produce safe responses to any adversarial input.
Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared
to attackers trained against static defenders and achieves higher robustness on
safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained
against static attackers. We further propose hidden Chain-of-Thought, allowing
agents to plan privately, which boosts adversarial diversity and reduces
over-refusals. Our results motivate a shift from reactive patching to proactive
co-evolution in LM safety training, enabling scalable, autonomous, and robust
self-improvement of LMs via multi-agent reinforcement learning (MARL).

</details>


### [272] [Premise Selection for a Lean Hammer](https://arxiv.org/abs/2506.07477)
*Thomas Zhu,Joshua Clune,Jeremy Avigad,Albert Qiaochu Jiang,Sean Welleck*

Main category: cs.LG

TL;DR: 提出适用于Lean证明助手的首个端到端通用hammer工具LeanHammer，能解决更多目标且泛化性好，让形式验证更易获取。


<details>
  <summary>Details</summary>
Motivation: 神经网络方法虽变革自动推理，但集成到实际验证工作流有挑战，Lean证明助手缺少hammer工具。

Method: 基于依赖类型理论下的新型神经前提选择系统构建LeanHammer，动态适应特定上下文，结合符号证明搜索和重构。

Result: 前提选择器使LeanHammer比现有前提选择器多解决21%的目标，且在不同领域泛化性好。

Conclusion: 工作弥合了神经检索和符号推理的差距，让形式验证对研究者和从业者更易获取。

Abstract: Neural methods are transforming automated reasoning for proof assistants, yet
integrating these advances into practical verification workflows remains
challenging. Hammers are tools that interface with external automatic theorem
provers to automate tedious reasoning steps. They have dramatically improved
productivity in proof assistants, but the Lean proof assistant still does not
have a hammer despite its growing popularity. We present LeanHammer, the first
end-to-end domain-general hammer for Lean, built on a novel neural premise
selection system for a hammer in dependent type theory. Unlike existing Lean
premise selectors, our approach dynamically adapts to user-specific contexts
and combines with symbolic proof search and reconstruction to create a
practical hammer. With comprehensive evaluations, we show that our premise
selector enables LeanHammer to solve 21\% more goals relative to existing
premise selectors, and generalize well to diverse domains. Our work bridges the
gap between neural retrieval and symbolic reasoning, making formal verification
more accessible to researchers and practitioners.

</details>


### [273] [Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning](https://arxiv.org/abs/2506.07501)
*Libo Wang*

Main category: cs.LG

TL;DR: 针对CoM子链依赖前一链信息、丢失长程依赖问题，提出GoCE，实验证明其增强了Transformer捕捉长程因果依赖能力，有自我进化能力，优于CoM设计。


<details>
  <summary>Details</summary>
Motivation: 解决链-of-模型（CoM）中各子链仅依赖前一子链信息，因因果掩码阻断多级子链间全局上下文流动而可能丢失长程依赖的问题。

Method: 将隐式标记表示映射为可微稀疏因果邻接矩阵，通过因果掩码注意力和因果MoE渗透因果约束，结合干预一致性损失测试和自我进化门实现因果结构学习与Transformer架构自适应更新的动态平衡。

Result: 在多个数据集上评估，与基线大语言模型对比，证明GoCE增强了Transformer捕捉长程因果依赖的能力，提高了自我进化能力。

Conclusion: GoCE不仅在设计原则上超越CoM，还为未来因果学习和持续自适应改进研究提供经验。

Abstract: In view of the problem that each subchain in the chain-of-model (CoM) relies
only on the information of the previous subchain and may lose long-range
dependencies due to the causal mask blocking the global context flow between
multi-level subchains, this work proposes a graph of causal evolution (GoCE).
Its core principle is to map the implicit token representation into a
differentiable and sparse causal adjacency matrix, then permeate causal
constraints through each layer of calculation using causal-masked attention and
causal-MoE. By combining intervention consistency loss test and self-evolution
gate, the dynamic balance between causal structure learning and adaptive
updating of transformer architecture is realized. The researcher built
experimental environments in sandboxes built with Claude Sonnet 4,
o4-mini-high, and DeepSeek R1 respectively with the transformer variant
architecture introduced in GoCE. It is evaluated on publicly available datasets
including CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the
baseline LLMs. The finding proves that GoCE strengthens the transformer's
ability to capture long-range causal dependencies, while the ability to
self-evolve is improved. It not only surpasses the design of CoM in terms of
design principles, but also provides experience for future research on causal
learning and continuous adaptive improvement.

</details>


### [274] [Reinforcement Learning via Implicit Imitation Guidance](https://arxiv.org/abs/2506.07505)
*Perry Dong,Alec M. Lessing,Annie S. Chen,Chelsea Finn*

Main category: cs.LG

TL;DR: 研究样本高效强化学习问题，提出用先验数据加噪声引导探索的方法，在连续控制任务上表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 解决样本高效强化学习中，模仿学习目标可能降低长期性能的问题，因它与奖励最大化不直接一致。

Method: 提出Data - Guided Noise (DGN)框架，仅用先验数据通过向策略添加噪声来引导探索，避免显式行为克隆约束。

Result: 在七个模拟连续控制任务中，比先前的离线数据强化学习方法提升2 - 3倍。

Conclusion: 利用先验数据加噪声引导探索的方法有效，可替代模仿学习目标解决样本高效强化学习问题。

Abstract: We study the problem of sample efficient reinforcement learning, where prior
data such as demonstrations are provided for initialization in lieu of a dense
reward signal. A natural approach is to incorporate an imitation learning
objective, either as regularization during training or to acquire a reference
policy. However, imitation learning objectives can ultimately degrade long-term
performance, as it does not directly align with reward maximization. In this
work, we propose to use prior data solely for guiding exploration via noise
added to the policy, sidestepping the need for explicit behavior cloning
constraints. The key insight in our framework, Data-Guided Noise (DGN), is that
demonstrations are most useful for identifying which actions should be
explored, rather than forcing the policy to take certain actions. Our approach
achieves up to 2-3x improvement over prior reinforcement learning from offline
data methods across seven simulated continuous control tasks.

</details>


### [275] [Improving Memory Efficiency for Training KANs via Meta Learning](https://arxiv.org/abs/2506.07549)
*Zhangchi Zhao,Jun Shu,Deyu Meng,Zongben Xu*

Main category: cs.LG

TL;DR: 受K - A定理启发的KANs有潜力但参数量大，提出MetaKANs减少参数量，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: KANs可作为高效可解释的函数逼近框架，但参数量大，内存效率低、训练成本高。

Method: 通过较小的元学习器为KANs生成权重，以端到端可微方式训练KANs和MetaKANs。

Result: MetaKANs在多项基准任务实验中提升了参数效率和内存使用。

Conclusion: 提出的方法为训练KANs提供新途径，提高可扩展性，缩小与MLPs训练成本差距。

Abstract: Inspired by the Kolmogorov-Arnold representation theorem, KANs offer a novel
framework for function approximation by replacing traditional neural network
weights with learnable univariate functions. This design demonstrates
significant potential as an efficient and interpretable alternative to
traditional MLPs. However, KANs are characterized by a substantially larger
number of trainable parameters, leading to challenges in memory efficiency and
higher training costs compared to MLPs. To address this limitation, we propose
to generate weights for KANs via a smaller meta-learner, called MetaKANs. By
training KANs and MetaKANs in an end-to-end differentiable manner, MetaKANs
achieve comparable or even superior performance while significantly reducing
the number of trainable parameters and maintaining promising interpretability.
Extensive experiments on diverse benchmark tasks, including symbolic
regression, partial differential equation solving, and image classification,
demonstrate the effectiveness of MetaKANs in improving parameter efficiency and
memory usage. The proposed method provides an alternative technique for
training KANs, that allows for greater scalability and extensibility, and
narrows the training cost gap with MLPs stated in the original paper of KANs.
Our code is available at https://github.com/Murphyzc/MetaKAN.

</details>


### [276] [Denoising the Future: Top-p Distributions for Moving Through Time](https://arxiv.org/abs/2506.07578)
*Florian Andreas Marwitz,Ralf Möller,Magnus Bender,Marcel Gehrke*

Main category: cs.LG

TL;DR: 提出用top - p状态去噪并加速隐马尔可夫模型推理，误差有界且能有数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 动态概率模型推理复杂，隐马尔可夫模型推进时需枚举全状态空间，计算低效且有噪声。

Method: 使用top - p状态（累积概率为p的最可能状态）进行去噪和加速推理。

Result: 使用top - p状态引入的误差受p和基础模型最小混合率约束；经验评估显示能有至少一个数量级的加速，总变差距离误差低于0.09。

Conclusion: 使用top - p状态能有效去噪并加速隐马尔可夫模型推理。

Abstract: Inference in dynamic probabilistic models is a complex task involving
expensive operations. In particular, for Hidden Markov Models, the whole state
space has to be enumerated for advancing in time. Even states with negligible
probabilities are considered, resulting in computational inefficiency and
increased noise due to the propagation of unlikely probability mass. We propose
to denoise the future and speed up inference by using only the top-p states,
i.e., the most probable states with accumulated probability p. We show that the
error introduced by using only the top-p states is bound by p and the so-called
minimal mixing rate of the underlying model. Moreover, in our empirical
evaluation, we show that we can expect speedups of at least an order of
magnitude, while the error in terms of total variation distance is below 0.09.

</details>


### [277] [MIRA: Medical Time Series Foundation Model for Real-World Health Data](https://arxiv.org/abs/2506.07584)
*Hao Li,Bowen Deng,Chang Xu,Zhiyuan Feng,Viktor Schlegel,Yu-Hao Huang,Yizheng Sun,Jingyuan Sun,Kailai Yang,Yiyao Yu,Jiang Bian*

Main category: cs.LG

TL;DR: 本文提出针对医疗时间序列预测的统一基础模型MIRA，经大规模语料预训练，预测误差低于基线模型，还引入综合基准。


<details>
  <summary>Details</summary>
Motivation: 现有通用时间序列基础模型难以处理医疗时间序列数据，因其存在间隔不规则、采样率异质和频繁缺失值等挑战。

Method: 引入MIRA模型，包含连续时间旋转位置编码、特定频率专家混合层和基于神经常微分方程的连续动态外推块。

Result: MIRA在分布外和分布内场景中，与其他零样本和微调基线相比，预测误差平均分别降低10%和7%。

Conclusion: MIRA可降低标注负担、减少模型定制，为医疗时间序列建模的未来研究奠定基础。

Abstract: A unified foundation model for medical time series -- pretrained on open
access and ethics board-approved medical corpora -- offers the potential to
reduce annotation burdens, minimize model customization, and enable robust
transfer across clinical institutions, modalities, and tasks, particularly in
data-scarce or privacy-constrained environments. However, existing generalist
time series foundation models struggle to handle medical time series data due
to their inherent challenges, including irregular intervals, heterogeneous
sampling rates, and frequent missing values. To address these challenges, we
introduce MIRA, a unified foundation model specifically designed for medical
time series forecasting. MIRA incorporates a Continuous-Time Rotary Positional
Encoding that enables fine-grained modeling of variable time intervals, a
frequency-specific mixture-of-experts layer that routes computation across
latent frequency regimes to further promote temporal specialization, and a
Continuous Dynamics Extrapolation Block based on Neural ODE that models the
continuous trajectory of latent states, enabling accurate forecasting at
arbitrary target timestamps. Pretrained on a large-scale and diverse medical
corpus comprising over 454 billion time points collect from publicly available
datasets, MIRA achieves reductions in forecasting errors by an average of 10%
and 7% in out-of-distribution and in-distribution scenarios, respectively, when
compared to other zero-shot and fine-tuned baselines. We also introduce a
comprehensive benchmark spanning multiple downstream clinical tasks,
establishing a foundation for future research in medical time series modeling.

</details>


### [278] [Aircraft Trajectory Dataset Augmentation in Latent Space](https://arxiv.org/abs/2506.07585)
*Seokbin Yoon,Keumjin Lee*

Main category: cs.LG

TL;DR: 提出ATRADA框架用于飞机轨迹数据集增强，通过Transformer编码器、PCA、GMM等步骤生成新轨迹数据，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 飞机轨迹建模对空中交通管理重要，为开发更鲁棒模型、确保数据集充足平衡，需进行数据集增强。

Method: 提出ATRADA框架，用Transformer编码器学习原数据集模式，PCA降维，GMM拟合概率分布，从GMM抽样后用MLP解码。

Result: 框架能有效生成新的高质量合成飞机轨迹数据。

Conclusion: ATRADA框架可有效进行飞机轨迹数据集增强。

Abstract: Aircraft trajectory modeling plays a crucial role in Air Traffic Management
(ATM) and is important for various downstream tasks, including conflict
detection and landing time prediction. Dataset augmentation through the
addition of synthetically generated trajectory data is necessary to develop a
more robust aircraft trajectory model and ensure that the trajectory dataset is
sufficient and balanced. In this work, we propose a novel framework called
ATRADA for aircraft trajectory dataset augmentation. In the proposed framework,
a Transformer encoder learns the underlying patterns in the original trajectory
dataset and converts each data point into a context vector in the learned
latent space. The converted dataset in the latent space is projected into
reduced dimensions using principal component analysis (PCA), and a Gaussian
mixture model (GMM) is applied to fit the probability distribution of the data
points in the reduced-dimensional space. Finally, new samples are drawn from
the fitted GMM, the dimension of the samples is reverted to the original
dimension, and they are decoded with a Multi-Layer Perceptron (MLP). Several
experiments demonstrate that the framework effectively generates new,
high-quality synthetic aircraft trajectory data, which were compared to the
results of several baselines.

</details>


### [279] [PrunePEFT: Iterative Hybrid Pruning for Parameter-Efficient Fine-tuning of LLMs](https://arxiv.org/abs/2506.07587)
*Tongzhou Yu,Zhuhao Zhang,Guanghui Zhu,Shen Jiang,Meikang Qiu,Yihua Huang*

Main category: cs.LG

TL;DR: 本文提出PrunePEFT方法优化PEFT策略搜索，减少计算负担，高效微调大模型。


<details>
  <summary>Details</summary>
Motivation: PEFT方法调优需考虑大量设计空间，传统架构搜索技术会带来额外开销，需更高效方法。

Method: 将PEFT策略搜索表述为剪枝问题，引入混合剪枝策略，迭代移除冗余或冲突模块。

Result: 有效识别相关模块，显著减少架构搜索计算负担。

Conclusion: PrunePEFT是微调大预训练模型更具扩展性和高效的解决方案。

Abstract: Parameter Efficient Fine-Tuning (PEFT) methods have emerged as effective and
promising approaches for fine-tuning pre-trained language models. Compared with
Full parameter Fine-Tuning (FFT), PEFT achieved comparable task performance
with a substantial reduction of trainable parameters, which largely saved the
training and storage costs. However, using the PEFT method requires considering
a vast design space, such as the type of PEFT modules and their insertion
layers. Inadequate configurations can lead to sub-optimal results. Conventional
solutions such as architectural search techniques, while effective, tend to
introduce substantial additional overhead. In this paper, we propose a novel
approach, PrunePEFT, which formulates the PEFT strategy search as a pruning
problem and introduces a hybrid pruning strategy that capitalizes on the
sensitivity of pruning methods to different PEFT modules. This method extends
traditional pruning techniques by iteratively removing redundant or conflicting
PEFT modules, thereby optimizing the fine-tuned configuration. By efficiently
identifying the most relevant modules, our approach significantly reduces the
computational burden typically associated with architectural search processes,
making it a more scalable and efficient solution for fine-tuning large
pre-trained models.

</details>


### [280] [TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts](https://arxiv.org/abs/2506.07596)
*Torsten Krauß,Hamid Dashtbani,Alexandra Dmitrienko*

Main category: cs.LG

TL;DR: 机器学习中LLMs应用广泛但有安全风险，传统越狱方法有局限，本文提出TwinBreak方法，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs存在社会风险，现有越狱方法有显著人工投入、高计算成本或影响模型正常功能等问题，需要改进。

Method: 提出TwinBreak方法，将安全机制视为嵌入式后门，识别并修剪相关参数，聚焦相关模型层进行细粒度分析，还创建了包含100个孪生提示的TwinPrompt数据集。

Result: 实验表明TwinBreak在来自五个供应商的16个LLMs上，以最小计算要求实现了89%至98%的成功率。

Conclusion: TwinBreak是一种有效的安全对齐移除方法，计算成本低且成功率高。

Abstract: Machine learning is advancing rapidly, with applications bringing notable
benefits, such as improvements in translation and code generation. Models like
ChatGPT, powered by Large Language Models (LLMs), are increasingly integrated
into daily life. However, alongside these benefits, LLMs also introduce social
risks. Malicious users can exploit LLMs by submitting harmful prompts, such as
requesting instructions for illegal activities. To mitigate this, models often
include a security mechanism that automatically rejects such harmful prompts.
However, they can be bypassed through LLM jailbreaks. Current jailbreaks often
require significant manual effort, high computational costs, or result in
excessive model modifications that may degrade regular utility.
  We introduce TwinBreak, an innovative safety alignment removal method.
Building on the idea that the safety mechanism operates like an embedded
backdoor, TwinBreak identifies and prunes parameters responsible for this
functionality. By focusing on the most relevant model layers, TwinBreak
performs fine-grained analysis of parameters essential to model utility and
safety. TwinBreak is the first method to analyze intermediate outputs from
prompts with high structural and content similarity to isolate safety
parameters. We present the TwinPrompt dataset containing 100 such twin prompts.
Experiments confirm TwinBreak's effectiveness, achieving 89% to 98% success
rates with minimal computational requirements across 16 LLMs from five vendors.

</details>


### [281] [FuXi-Air: Urban Air Quality Forecasting Based on Emission-Meteorology-Pollutant multimodal Machine Learning](https://arxiv.org/abs/2506.07616)
*Zhixin Geng,Xu Fan,Xiqiao Lu,Yan Zhang,Guangyuan Yu,Cheng Huang,Qian Wang,Yuewu Li,Weichun Ma,Qi Yu,Libo Wu,Hao Li*

Main category: cs.LG

TL;DR: 本文构建了基于多模态数据融合的空气质量预测模型FuXi - Air，在效率和精度上优于主流数值模型，为空气质量预测提供参考。


<details>
  <summary>Details</summary>
Motivation: 现有空气质量预测方法存在高计算成本、低效率等局限，随着人工智能发展，急需低成本、高效的模型用于智慧城市管理。

Method: 基于空气污染机制，整合气象预报、排放清单和污染物监测数据，结合自回归预测框架与帧插值策略构建FuXi - Air模型。

Result: 模型能在25 - 30秒内完成多监测点6种主要空气污染物72小时逐小时预报，在计算效率和预报精度上优于主流数值模型，多模态数据融合显著提高预报精度。

Conclusion: 研究为多模态数据驱动模型应用于空气质量预测提供技术参考和实例，为构建混合预测系统提供新见解。

Abstract: Air pollution has emerged as a major public health challenge in megacities.
Numerical simulations and single-site machine learning approaches have been
widely applied in air quality forecasting tasks. However, these methods face
multiple limitations, including high computational costs, low operational
efficiency, and limited integration with observational data. With the rapid
advancement of artificial intelligence, there is an urgent need to develop a
low-cost, efficient air quality forecasting model for smart urban management.
An air quality forecasting model, named FuXi-Air, has been constructed in this
study based on multimodal data fusion to support high-precision air quality
forecasting and operated in typical megacities. The model integrates
meteorological forecasts, emission inventories, and pollutant monitoring data
under the guidance of air pollution mechanism. By combining an autoregressive
prediction framework with a frame interpolation strategy, the model
successfully completes 72-hour forecasts for six major air pollutants at an
hourly resolution across multiple monitoring sites within 25-30 seconds. In
terms of both computational efficiency and forecasting accuracy, it outperforms
the mainstream numerical air quality models in operational forecasting work.
Ablation experiments concerning key influencing factors show that although
meteorological data contribute more to model accuracy than emission inventories
do, the integration of multimodal data significantly improves forecasting
precision and ensures that reliable predictions are obtained under differing
pollution mechanisms across megacities. This study provides both a technical
reference and a practical example for applying multimodal data-driven models to
air quality forecasting and offers new insights into building hybrid
forecasting systems to support air pollution risk warning in smart city
management.

</details>


### [282] [The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning](https://arxiv.org/abs/2506.07619)
*Toby Boyne,Juan S. Campos,Becky D. Langdon,Jixiang Qing,Yilin Xie,Shiqiang Zhang,Calvin Tsay,Ruth Misener,Daniel W. Davies,Kim E. Jelfs,Sarah Boyall,Thomas M. Dixon,Linden Schrecker,Jose Pablo Folch*

Main category: cs.LG

TL;DR: 介绍用于产率预测的新数据集，涵盖超1200个工艺条件，针对溶剂选择进行机器学习模型基准测试。


<details>
  <summary>Details</summary>
Motivation: 化学数据集对机器学习社区常不可用，需新数据集进行机器学习基准测试。

Method: 引入新的产率预测数据集，进行回归算法、迁移学习、特征工程和主动学习的基准测试。

Result: 提供首个用于机器学习基准测试的瞬态流动数据集，对多种机器学习方法进行基准测试。

Conclusion: 该数据集和基准测试对溶剂替换和可持续制造有重要应用。

Abstract: Machine learning has promised to change the landscape of laboratory
chemistry, with impressive results in molecular property prediction and
reaction retro-synthesis. However, chemical datasets are often inaccessible to
the machine learning community as they tend to require cleaning, thorough
understanding of the chemistry, or are simply not available. In this paper, we
introduce a novel dataset for yield prediction, providing the first-ever
transient flow dataset for machine learning benchmarking, covering over 1200
process conditions. While previous datasets focus on discrete parameters, our
experimental set-up allow us to sample a large number of continuous process
conditions, generating new challenges for machine learning models. We focus on
solvent selection, a task that is particularly difficult to model theoretically
and therefore ripe for machine learning applications. We showcase benchmarking
for regression algorithms, transfer-learning approaches, feature engineering,
and active learning, with important applications towards solvent replacement
and sustainable manufacturing.

</details>


### [283] [Return of ChebNet: Understanding and Improving an Overlooked GNN on Long Range Tasks](https://arxiv.org/abs/2506.07624)
*Ali Hariri,Álvaro Arroyo,Alessio Gravina,Moshe Eliasof,Carola-Bibiane Schönlieb,Davide Bacciu,Kamyar Azizzadenesheli,Xiaowen Dong,Pierre Vandergheynst*

Main category: cs.LG

TL;DR: 文章重新审视ChebNet，发现其在长距离依赖建模有优势但训练不稳定，提出Stable - ChebNet模型，在多个基准测试中接近最优性能。


<details>
  <summary>Details</summary>
Motivation: MPNNs难以捕捉节点间长距离依赖，图Transformer牺牲计算效率且忽视图结构，因此重新审视ChebNet以挖掘其长距离交互建模能力。

Method: 将ChebNet构建为稳定且非耗散的动态系统，提出Stable - ChebNet模型。

Result: ChebNet在长距离基准测试中相对经典MPNNs和GTs有竞争优势，Stable - ChebNet可实现稳定信息传播，动态可控且无需特征分解、位置编码或图重连，在多个基准测试中接近最优性能。

Conclusion: Stable - ChebNet是一种有效的图神经网络模型，能解决ChebNet训练不稳定问题，在长距离依赖建模上表现良好。

Abstract: ChebNet, one of the earliest spectral GNNs, has largely been overshadowed by
Message Passing Neural Networks (MPNNs), which gained popularity for their
simplicity and effectiveness in capturing local graph structure. Despite their
success, MPNNs are limited in their ability to capture long-range dependencies
between nodes. This has led researchers to adapt MPNNs through rewiring or make
use of Graph Transformers, which compromises the computational efficiency that
characterized early spatial message-passing architectures, and typically
disregards the graph structure. Almost a decade after its original
introduction, we revisit ChebNet to shed light on its ability to model distant
node interactions. We find that out-of-box, ChebNet already shows competitive
advantages relative to classical MPNNs and GTs on long-range benchmarks, while
maintaining good scalability properties for high-order polynomials. However, we
uncover that this polynomial expansion leads ChebNet to an unstable regime
during training. To address this limitation, we cast ChebNet as a stable and
non-dissipative dynamical system, which we coin Stable-ChebNet. Our
Stable-ChebNet model allows for stable information propagation, and has
controllable dynamics which do not require the use of eigendecompositions,
positional encodings, or graph rewiring. Across several benchmarks,
Stable-ChebNet achieves near state-of-the-art performance.

</details>


### [284] [ProARD: progressive adversarial robustness distillation: provide wide range of robust students](https://arxiv.org/abs/2506.07666)
*Seyedhamidreza Mousavi,Seyedali Mousavi,Masoud Daneshtalab*

Main category: cs.LG

TL;DR: 提出Progressive Adversarial Robustness Distillation (ProARD)，可一次训练动态网络以支持多样学生网络，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 当前ARD方法需为满足特定约束从头训练新学生网络，导致高计算成本和碳排放。

Method: 先基于动态层构建动态深度神经网络，将最大尺寸学生网络作为动态教师网络，用权重共享机制训练，因计算梯度成本高，需采样机制选学生子集。

Result: 随机学生采样在每次迭代中无法产生准确且鲁棒的学生网络。

Conclusion: 未提及明确结论，但暗示需寻找更好采样机制来训练准确且鲁棒的学生网络。

Abstract: Adversarial Robustness Distillation (ARD) has emerged as an effective method
to enhance the robustness of lightweight deep neural networks against
adversarial attacks. Current ARD approaches have leveraged a large robust
teacher network to train one robust lightweight student. However, due to the
diverse range of edge devices and resource constraints, current approaches
require training a new student network from scratch to meet specific
constraints, leading to substantial computational costs and increased CO2
emissions. This paper proposes Progressive Adversarial Robustness Distillation
(ProARD), enabling the efficient one-time training of a dynamic network that
supports a diverse range of accurate and robust student networks without
requiring retraining. We first make a dynamic deep neural network based on
dynamic layers by encompassing variations in width, depth, and expansion in
each design stage to support a wide range of architectures. Then, we consider
the student network with the largest size as the dynamic teacher network.
ProARD trains this dynamic network using a weight-sharing mechanism to jointly
optimize the dynamic teacher network and its internal student networks.
However, due to the high computational cost of calculating exact gradients for
all the students within the dynamic network, a sampling mechanism is required
to select a subset of students. We show that random student sampling in each
iteration fails to produce accurate and robust students.

</details>


### [285] [How Benchmark Prediction from Fewer Data Misses the Mark](https://arxiv.org/abs/2506.07673)
*Guanhua Zhang,Florian E. Dorner,Moritz Hardt*

Main category: cs.LG

TL;DR: 系统评估11种基准预测方法在19个不同基准测试中的优缺点，提出新方法但仍受模型相似性限制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型评估成本增加，需加速评估的方法，即基准预测方法。

Method: 系统评估11种基准预测方法，提出基于增强逆倾向加权的新方法。

Result: 随机抽样拟合回归模型是有竞争力的基线；现有方法依赖模型相似性，新模型准确性高时效果差；新方法在推断场景中优于随机样本平均，但提升有限。

Conclusion: 基准预测在评估未知能力新模型时效果不佳。

Abstract: Large language model (LLM) evaluation is increasingly costly, prompting
interest in methods that speed up evaluation by shrinking benchmark datasets.
Benchmark prediction (also called efficient LLM evaluation) aims to select a
small subset of evaluation points and predict overall benchmark performance
from that subset. In this paper, we systematically assess the strengths and
limitations of 11 benchmark prediction methods across 19 diverse benchmarks.
First, we identify a highly competitive baseline: Take a random sample and fit
a regression model on the sample to predict missing entries. Outperforming most
existing methods, this baseline challenges the assumption that careful subset
selection is necessary for benchmark prediction. Second, we discover that all
existing methods crucially depend on model similarity. They work best when
interpolating scores among similar models. The effectiveness of benchmark
prediction sharply declines when new models have higher accuracy than
previously seen models. In this setting of extrapolation, none of the previous
methods consistently beat a simple average over random samples. To improve over
the sample average, we introduce a new method inspired by augmented inverse
propensity weighting. This method consistently outperforms the random sample
average even for extrapolation. However, its performance still relies on model
similarity and the gains are modest in general. This shows that benchmark
prediction fails just when it is most needed: at the evaluation frontier, where
the goal is to evaluate new models of unknown capabilities.

</details>


### [286] [Evaluating Robustness in Latent Diffusion Models via Embedding Level Augmentation](https://arxiv.org/abs/2506.07706)
*Boris Martirosyan,Alexey Karmanov*

Main category: cs.LG

TL;DR: 本文针对潜在扩散模型（LDMs）缺乏鲁棒性的问题，提出评估和提升鲁棒性的方法并进行实验。


<details>
  <summary>Details</summary>
Motivation: 当前LDMs在各任务表现好但缺乏鲁棒性，且相关研究不足。

Method: 提出不依赖文本编码器测量鲁棒性，引入数据增强技术，用Dreambooth微调模型，提出评估管道。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Latent diffusion models (LDMs) achieve state-of-the-art performance across
various tasks, including image generation and video synthesis. However, they
generally lack robustness, a limitation that remains not fully explored in
current research. In this paper, we propose several methods to address this
gap. First, we hypothesize that the robustness of LDMs primarily should be
measured without their text encoder, because if we take and explore the whole
architecture, the problems of image generator and text encoders wll be fused.
Second, we introduce novel data augmentation techniques designed to reveal
robustness shortcomings in LDMs when processing diverse textual prompts. We
then fine-tune Stable Diffusion 3 and Stable Diffusion XL models using
Dreambooth, incorporating these proposed augmentation methods across multiple
tasks. Finally, we propose a novel evaluation pipeline specifically tailored to
assess the robustness of LDMs fine-tuned via Dreambooth.

</details>


### [287] [Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning](https://arxiv.org/abs/2506.07735)
*Haizhao Jing,Haokui Zhang,Zhenhao Shang,Rong Xiao,Peng Wang,Yanning Zhang*

Main category: cs.LG

TL;DR: 提出LeDG - Former框架，结合语言语义嵌入和动态图表示学习，解决现有神经架构表示学习方法局限，在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer和GNN的神经架构表示学习方法存在忽视硬件属性信息、编码方法无法捕捉计算节点结构差异的问题，影响模型实用性和编码效果。

Method: 提出语言嵌入框架，将神经架构和硬件平台规格投影到统一语义空间；提出基于动态图的Transformer对神经架构建模。

Result: 在NNLQP基准上超越先前方法，实现跨硬件延迟预测；在NAS - Bench - 101和NAS - Bench - 201数据集上性能优越。

Conclusion: LeDG - Former框架有效解决现有方法局限，建立新的SOTA，具备跨硬件预测能力。

Abstract: Neural Architecture Representation Learning aims to transform network models
into feature representations for predicting network attributes, playing a
crucial role in deploying and designing networks for real-world applications.
Recently, inspired by the success of transformers, transformer-based models
integrated with Graph Neural Networks (GNNs) have achieved significant progress
in representation learning. However, current methods still have some
limitations. First, existing methods overlook hardware attribute information,
which conflicts with the current trend of diversified deep learning hardware
and limits the practical applicability of models. Second, current encoding
approaches rely on static adjacency matrices to represent topological
structures, failing to capture the structural differences between computational
nodes, which ultimately compromises encoding effectiveness. In this paper, we
introduce LeDG-Former, an innovative framework that addresses these limitations
through the synergistic integration of language-based semantic embedding and
dynamic graph representation learning. Specifically, inspired by large language
models (LLMs), we propose a language embedding framework where both neural
architectures and hardware platform specifications are projected into a unified
semantic space through tokenization and LLM processing, enabling zero-shot
prediction across different hardware platforms for the first time. Then, we
propose a dynamic graph-based transformer for modeling neural architectures,
resulting in improved neural architecture modeling performance. On the NNLQP
benchmark, LeDG-Former surpasses previous methods, establishing a new SOTA
while demonstrating the first successful cross-hardware latency prediction
capability. Furthermore, our framework achieves superior performance on the
cell-structured NAS-Bench-101 and NAS-Bench-201 datasets.

</details>


### [288] [Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.07744)
*Seungho Baek,Taegeon Park,Jongchan Park,Seungjun Oh,Yusung Kim*

Main category: cs.LG

TL;DR: 提出GAS框架解决现有离线分层强化学习方法效率问题，在多任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有离线分层强化学习方法随任务范围增加效率下降，且缺乏拼接不同轨迹状态转换的有效策略。

Method: 将子目标选择作为图搜索问题，把状态嵌入TDR空间，聚类相似状态到统一图节点，用最短路径算法选择子目标序列，引入TE指标提高图质量。

Result: GAS在运动、导航和操作任务中优于先前方法，在最关键任务中得分远超之前最优。

Conclusion: GAS是解决离线分层强化学习问题的有效框架。

Abstract: Existing offline hierarchical reinforcement learning methods rely on
high-level policy learning to generate subgoal sequences. However, their
efficiency degrades as task horizons increase, and they lack effective
strategies for stitching useful state transitions across different
trajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that
formulates subgoal selection as a graph search problem rather than learning an
explicit high-level policy. By embedding states into a Temporal Distance
Representation (TDR) space, GAS clusters semantically similar states from
different trajectories into unified graph nodes, enabling efficient transition
stitching. A shortest-path algorithm is then applied to select subgoal
sequences within the graph, while a low-level policy learns to reach the
subgoals. To improve graph quality, we introduce the Temporal Efficiency (TE)
metric, which filters out noisy or inefficient transition states, significantly
enhancing task performance. GAS outperforms prior offline HRL methods across
locomotion, navigation, and manipulation tasks. Notably, in the most
stitching-critical task, it achieves a score of 88.3, dramatically surpassing
the previous state-of-the-art score of 1.0. Our source code is available at:
https://github.com/qortmdgh4141/GAS.

</details>


### [289] [Comparing Credit Risk Estimates in the Gen-AI Era](https://arxiv.org/abs/2506.07754)
*Nicola Lavecchia,Sid Fadanelli,Federico Ricciuti,Gennaro Aloe,Enrico Bagli,Pietro Giuffrida,Daniele Vergari*

Main category: cs.LG

TL;DR: 对信用评分建模技术进行比较分析，发现生成式AI模型在信用风险评分上不如传统方法，需进一步研发。


<details>
  <summary>Details</summary>
Motivation: 分析生成式AI技术在信用评分建模中的应用效果。

Method: 对传统信用评分方法和利用生成式AI的方法进行比较分析。

Result: 当前生成式AI模型无论采用何种集成策略，性能均不如传统方法。

Conclusion: 目前生成式AI用于信用风险评分存在能力局限，在应用前需进一步研究开发。

Abstract: Generative AI technologies have demonstrated significant potential across
diverse applications. This study provides a comparative analysis of credit
score modeling techniques, contrasting traditional approaches with those
leveraging generative AI. Our findings reveal that current generative AI models
fall short of matching the performance of traditional methods, regardless of
the integration strategy employed. These results highlight the limitations in
the current capabilities of generative AI for credit risk scoring, emphasizing
the need for further research and development before the possibility of
applying generative AI for this specific task, or equivalent ones.

</details>


### [290] [Clustered Federated Learning via Embedding Distributions](https://arxiv.org/abs/2506.07769)
*Dekai Zhang,Matthew Williams,Francesca Toni*

Main category: cs.LG

TL;DR: 提出基于EMD的一次性聚类方法EMD - CFL解决联邦学习中数据非IID问题，性能优于16个基线方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受非IID数据影响，聚类联邦学习可解决此问题，因此需要更好的聚类方法。

Method: 提出使用嵌入空间中数据分布的地球移动距离（EMD）的一次性聚类方法EMD - CFL，并从领域适应文献结果进行理论推导。

Result: 在与16个基线方法的广泛比较和一系列具有挑战性的数据集上，展现出优越的聚类性能。

Conclusion: 提出的EMD - CFL方法能有效解决联邦学习中数据非IID问题，聚类性能良好。

Abstract: Federated learning (FL) is a widely used framework for machine learning in
distributed data environments where clients hold data that cannot be easily
centralised, such as for data protection reasons. FL, however, is known to be
vulnerable to non-IID data. Clustered FL addresses this issue by finding more
homogeneous clusters of clients. We propose a novel one-shot clustering method,
EMD-CFL, using the Earth Mover's distance (EMD) between data distributions in
embedding space. We theoretically motivate the use of EMDs using results from
the domain adaptation literature and demonstrate empirically superior
clustering performance in extensive comparisons against 16 baselines and on a
range of challenging datasets.

</details>


### [291] [Identifiable Object Representations under Spatial Ambiguities](https://arxiv.org/abs/2506.07806)
*Avinash Kori,Francesca Toni,Ben Glocker*

Main category: cs.LG

TL;DR: 提出新多视图概率方法解决模块化对象中心表示在空间歧义下的获取难题，实验验证其性能。


<details>
  <summary>Details</summary>
Motivation: 模块化对象中心表示对类人推理很重要，但在空间歧义下获取有挑战，且解决该挑战存在理论和实践困难。

Method: 引入多视图概率方法，聚合特定视图槽以捕获不变内容信息，同时学习解纠缠的全局视角信息。

Result: 该方法解决了空间歧义，提供可识别性的理论保证，无需视角注释，在标准基准和复杂新数据集上验证了鲁棒性和可扩展性。

Conclusion: 所提出的方法有效且可行，可用于解决模块化对象中心表示在空间歧义下的问题。

Abstract: Modular object-centric representations are essential for *human-like
reasoning* but are challenging to obtain under spatial ambiguities, *e.g. due
to occlusions and view ambiguities*. However, addressing challenges presents
both theoretical and practical difficulties. We introduce a novel multi-view
probabilistic approach that aggregates view-specific slots to capture
*invariant content* information while simultaneously learning disentangled
global *viewpoint-level* information. Unlike prior single-view methods, our
approach resolves spatial ambiguities, provides theoretical guarantees for
identifiability, and requires *no viewpoint annotations*. Extensive experiments
on standard benchmarks and novel complex datasets validate our method's
robustness and scalability.

</details>


### [292] [Accelerating Diffusion Models in Offline RL via Reward-Aware Consistency Trajectory Distillation](https://arxiv.org/abs/2506.07822)
*Xintong Duan,Yutong He,Fahim Tajwar,Ruslan Salakhutdinov,J. Zico Kolter,Jeff Schneider*

Main category: cs.LG

TL;DR: 提出用于离线强化学习的一致性蒸馏新方法，能单步生成，性能高、训练简单，在基准测试中表现优于现有方法且推理速度大幅提升。


<details>
  <summary>Details</summary>
Motivation: 扩散模型推理速度慢，一致性模型应用于决策时存在示范不佳或多网络并发训练复杂的问题。

Method: 提出一种将奖励优化直接纳入蒸馏过程的一致性蒸馏新方法。

Result: 在Gym MuJoCo基准测试和长视野规划中，比之前的先进方法性能提升8.7%，推理时间比扩散模型快达142倍。

Conclusion: 所提方法能实现单步生成，同时保持高性能和简单训练。

Abstract: Although diffusion models have achieved strong results in decision-making
tasks, their slow inference speed remains a key limitation. While the
consistency model offers a potential solution, its applications to
decision-making often struggle with suboptimal demonstrations or rely on
complex concurrent training of multiple networks. In this work, we propose a
novel approach to consistency distillation for offline reinforcement learning
that directly incorporates reward optimization into the distillation process.
Our method enables single-step generation while maintaining higher performance
and simpler training. Empirical evaluations on the Gym MuJoCo benchmarks and
long horizon planning demonstrate that our approach can achieve an 8.7%
improvement over previous state-of-the-art while offering up to 142x speedup
over diffusion counterparts in inference time.

</details>


### [293] [Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information](https://arxiv.org/abs/2506.07829)
*Jan Corazza,Hadi Partovi Aria,Hyohun Kim,Daniel Neider,Zhe Xu*

Main category: cs.LG

TL;DR: 本文研究为多智能体提供高层符号知识，以解决分散式多智能体强化学习中的挑战，扩展检查策略兼容性的工具，实证表明环境事件的符号知识能加速学习。


<details>
  <summary>Details</summary>
Motivation: 现实中很多问题需多智能体协作，分散式多智能体强化学习存在隐私约束、通信限制和性能问题等挑战。

Method: 扩展检查局部策略与团队任务兼容性的形式化工具。

Result: 符号知识能显著加速分散式多智能体强化学习的学习过程。

Conclusion: 提供高层符号知识可解决分散式多智能体强化学习中的独特挑战，使分散式训练在更多场景可用。

Abstract: Reinforcement learning (RL) algorithms can find an optimal policy for a
single agent to accomplish a particular task. However, many real-world problems
require multiple agents to collaborate in order to achieve a common goal. For
example, a robot executing a task in a warehouse may require the assistance of
a drone to retrieve items from high shelves. In Decentralized Multi-Agent RL
(DMARL), agents learn independently and then combine their policies at
execution time, but often must satisfy constraints on compatibility of local
policies to ensure that they can achieve the global task when combined. In this
paper, we study how providing high-level symbolic knowledge to agents can help
address unique challenges of this setting, such as privacy constraints,
communication limitations, and performance concerns. In particular, we extend
the formal tools used to check the compatibility of local policies with the
team task, making decentralized training with theoretical guarantees usable in
more scenarios. Furthermore, we empirically demonstrate that symbolic knowledge
about the temporal evolution of events in the environment can significantly
expedite the learning process in DMARL.

</details>


### [294] [Improving large language models with concept-aware fine-tuning](https://arxiv.org/abs/2506.07833)
*Michael K. Chen,Xikun Zhang,Jiaxing Huang,Dacheng Tao*

Main category: cs.LG

TL;DR: 现有大语言模型（LLMs）基于下一个词预测范式有局限，提出概念感知微调（CAFT）方法，实验显示效果显著，将多词预测引入训练后阶段。


<details>
  <summary>Details</summary>
Motivation: 现有下一个词预测范式限制了大语言模型形成连贯高级概念的能力，阻碍深度概念理解和智能系统发展。

Method: 提出概念感知微调（CAFT）这一新颖的多词训练方法，使模型学习跨多个词的序列，促进概念感知学习。

Result: 在文本摘要和蛋白质设计等多种任务中，与传统下一词微调方法相比有显著改进，首次将多词设置引入训练后阶段。

Conclusion: 提出的方法效果意外出色，对机器学习研究界有更广泛的启示。

Abstract: Large language models (LLMs) have become the cornerstone of modern AI.
However, the existing paradigm of next-token prediction fundamentally limits
their ability to form coherent, high-level concepts, making it a critical
barrier to human-like understanding and reasoning. Take the phrase "ribonucleic
acid" as an example: an LLM will first decompose it into tokens, i.e.,
artificial text fragments ("rib", "on", ...), then learn each token
sequentially, rather than grasping the phrase as a unified, coherent semantic
entity. This fragmented representation hinders deeper conceptual understanding
and, ultimately, the development of truly intelligent systems. In response, we
introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method
that redefines how LLMs are fine-tuned. By enabling the learning of sequences
that span multiple tokens, this method fosters stronger concept-aware learning.
Our experiments demonstrate significant improvements compared to conventional
next-token finetuning methods across diverse tasks, including traditional
applications like text summarization and domain-specific ones like de novo
protein design. Multi-token prediction was previously only possible in the
prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first
to bring the multi-token setting to the post-training phase, thus effectively
democratizing its benefits for the broader community of practitioners and
researchers. Finally, the unexpected effectiveness of our proposed method
suggests wider implications for the machine learning research community. All
code and data are available at https://github.com/michaelchen-lab/caft-llm

</details>


### [295] [Jarzynski Reweighting and Sampling Dynamics for Training Energy-Based Models: Theoretical Analysis of Different Transition Kernels](https://arxiv.org/abs/2506.07843)
*Davide Carbone*

Main category: cs.LG

TL;DR: 本文对Jarzynski重加权技术用于训练基于能量的模型（EBMs）进行理论分析，并在两个生成框架中展示其作用，凸显该技术在生成学习中的潜力。


<details>
  <summary>Details</summary>
Motivation: EBMs训练因需近似归一化常数和从复杂分布采样而具有理论挑战，传统方法有偏差，影响准确学习。

Method: 对Jarzynski重加权技术进行理论分析，研究核的选择作用，并在基于流的扩散模型和受限玻尔兹曼机两个生成框架中进行说明。

Result: 揭示了核选择与模型性能之间的相互作用。

Conclusion: Jarzynski重加权技术有潜力成为生成学习的原则性工具。

Abstract: Energy-Based Models (EBMs) provide a flexible framework for generative
modeling, but their training remains theoretically challenging due to the need
to approximate normalization constants and efficiently sample from complex,
multi-modal distributions. Traditional methods, such as contrastive divergence
and score matching, introduce biases that can hinder accurate learning. In this
work, we present a theoretical analysis of Jarzynski reweighting, a technique
from non-equilibrium statistical mechanics, and its implications for training
EBMs. We focus on the role of the choice of the kernel and we illustrate these
theoretical considerations in two key generative frameworks: (i) flow-based
diffusion models, where we reinterpret Jarzynski reweighting in the context of
stochastic interpolants to mitigate discretization errors and improve sample
quality, and (ii) Restricted Boltzmann Machines, where we analyze its role in
correcting the biases of contrastive divergence. Our results provide insights
into the interplay between kernel choice and model performance, highlighting
the potential of Jarzynski reweighting as a principled tool for generative
learning.

</details>


### [296] [Fairness Overfitting in Machine Learning: An Information-Theoretic Perspective](https://arxiv.org/abs/2506.07861)
*Firas Laakom,Haobo Chen,Jürgen Schmidhuber,Yuheng Bu*

Main category: cs.LG

TL;DR: 提出通过信息论视角分析公平性泛化误差的理论框架，用Efron - Stein不等式推导界，实证验证界的紧密性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有促进机器学习模型公平性的方法缺乏训练公平性在未见数据上泛化的正式保证，且公平性损失的过拟合研究较少。

Method: 提出基于信息论视角的理论框架，用Efron - Stein不等式推导包含互信息和条件互信息的公平性泛化界。

Result: 实证结果验证了不同公平感知学习算法中这些界的紧密性和实际相关性。

Conclusion: 该框架为设计改善公平性泛化的算法提供了有价值的见解。

Abstract: Despite substantial progress in promoting fairness in high-stake applications
using machine learning models, existing methods often modify the training
process, such as through regularizers or other interventions, but lack formal
guarantees that fairness achieved during training will generalize to unseen
data. Although overfitting with respect to prediction performance has been
extensively studied, overfitting in terms of fairness loss has received far
less attention. This paper proposes a theoretical framework for analyzing
fairness generalization error through an information-theoretic lens. Our novel
bounding technique is based on Efron-Stein inequality, which allows us to
derive tight information-theoretic fairness generalization bounds with both
Mutual Information (MI) and Conditional Mutual Information (CMI). Our empirical
results validate the tightness and practical relevance of these bounds across
diverse fairness-aware learning algorithms. Our framework offers valuable
insights to guide the design of algorithms improving fairness generalization.

</details>


### [297] [Lightweight Sequential Transformers for Blood Glucose Level Prediction in Type-1 Diabetes](https://arxiv.org/abs/2506.07864)
*Mirko Paolo Barbato,Giorgia Rigamonti,Davide Marelli,Paolo Napoletano*

Main category: cs.LG

TL;DR: 提出轻量级顺序Transformer模型用于1型糖尿病血糖预测，在两基准数据集上表现优于现有方法，填补高性能建模与实际部署间的差距。


<details>
  <summary>Details</summary>
Motivation: 1型糖尿病需持续监测，在可穿戴设备上部署预测模型受计算和内存限制，需解决此问题。

Method: 提出轻量级顺序Transformer模型，整合Transformer注意力机制和循环神经网络顺序处理优势，优化用于资源受限边缘设备，采用平衡损失函数处理数据不平衡。

Result: 在OhioT1DM和DiaTrend两基准数据集实验表明，模型在预测血糖水平和检测不良事件上优于现有方法。

Conclusion: 此工作填补高性能建模和实际部署间的差距，提供可靠高效的1型糖尿病管理解决方案。

Abstract: Type 1 Diabetes (T1D) affects millions worldwide, requiring continuous
monitoring to prevent severe hypo- and hyperglycemic events. While continuous
glucose monitoring has improved blood glucose management, deploying predictive
models on wearable devices remains challenging due to computational and memory
constraints. To address this, we propose a novel Lightweight Sequential
Transformer model designed for blood glucose prediction in T1D. By integrating
the strengths of Transformers' attention mechanisms and the sequential
processing of recurrent neural networks, our architecture captures long-term
dependencies while maintaining computational efficiency. The model is optimized
for deployment on resource-constrained edge devices and incorporates a balanced
loss function to handle the inherent data imbalance in hypo- and hyperglycemic
events. Experiments on two benchmark datasets, OhioT1DM and DiaTrend,
demonstrate that the proposed model outperforms state-of-the-art methods in
predicting glucose levels and detecting adverse events. This work fills the gap
between high-performance modeling and practical deployment, providing a
reliable and efficient T1D management solution.

</details>


### [298] [Schauder Bases for $C[0, 1]$ Using ReLU, Softplus and Two Sigmoidal Functions](https://arxiv.org/abs/2506.07884)
*Anand Ganesh,Babhrubahan Bose,Anand Rajagopalan*

Main category: cs.LG

TL;DR: 为空间 $C[0,1]$ 构造四个Schauder基，包括使用ReLU、Softplus等函数，首次证实此类函数基的存在并改进相关通用逼近性质。


<details>
  <summary>Details</summary>
Motivation: 探索使用ReLU、Softplus等函数为空间 $C[0,1]$ 构建Schauder基并改进相关逼近性质。

Method: 构造四个Schauder基，分别使用ReLU函数、Softplus函数以及ReLU和Softplus函数的S型版本。

Result: 成功构建四个Schauder基，首次确立了使用这些函数的基的存在性。

Conclusion: 使用这些函数构建的基改进了相关的通用逼近性质。

Abstract: We construct four Schauder bases for the space $C[0,1]$, one using ReLU
functions, another using Softplus functions, and two more using sigmoidal
versions of the ReLU and Softplus functions. This establishes the existence of
a basis using these functions for the first time, and improves on the universal
approximation property associated with them.

</details>


### [299] [Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces](https://arxiv.org/abs/2506.07903)
*Kevin Rojas,Yuchen Zhu,Sichen Zhu,Felix X. -F. Ye,Molei Tao*

Main category: cs.LG

TL;DR: 提出在任意状态空间构建多模态扩散模型的框架，引入解耦噪声调度，在文本 - 图像生成和表格数据合成中验证有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态扩散模型生成依赖外部预处理协议，对编码器和解码器精度要求高，在数据有限场景有问题。

Method: 提出在任意状态空间构建多模态扩散模型的框架，为每种模态引入解耦噪声调度，实现单模型无条件和模态条件生成。

Result: 在文本 - 图像生成和混合类型表格数据合成中进行验证，取得有竞争力的性能。

Conclusion: 所提框架有效，可解决现有多模态扩散模型的限制。

Abstract: Diffusion models have demonstrated remarkable performance in generating
unimodal data across various tasks, including image, video, and text
generation. On the contrary, the joint generation of multimodal data through
diffusion models is still in the early stages of exploration. Existing
approaches heavily rely on external preprocessing protocols, such as tokenizers
and variational autoencoders, to harmonize varied data representations into a
unified, unimodal format. This process heavily demands the high accuracy of
encoders and decoders, which can be problematic for applications with limited
data. To lift this restriction, we propose a novel framework for building
multimodal diffusion models on arbitrary state spaces, enabling native
generation of coupled data across different modalities. By introducing an
innovative decoupled noise schedule for each modality, we enable both
unconditional and modality-conditioned generation within a single model
simultaneously. We empirically validate our approach for text-image generation
and mixed-type tabular data synthesis, demonstrating that it achieves
competitive performance.

</details>


### [300] [Uncovering the Functional Roles of Nonlinearity in Memory](https://arxiv.org/abs/2506.07919)
*Manuel Brenner,Georgia Koppe*

Main category: cs.LG

TL;DR: 研究剖析循环网络中非线性的功能作用，发现极小非线性在序列建模任务中不仅足够且常为最优。


<details>
  <summary>Details</summary>
Motivation: 过往认为非线性递归对序列建模任务的记忆和长时处理机制很关键，但近期研究表明线性动态可能足够，因此要系统剖析循环网络中非线性的功能作用。

Method: 使用几乎线性循环神经网络（AL - RNNs）作为灵活建模工具和探测记忆内部机制的手段。

Result: 在一系列经典序列建模任务和现实刺激选择任务中，极小非线性不仅足够，还能产生比全非线性或线性模型更简单、更鲁棒、更易解释的模型。

Conclusion: 研究为有选择地引入非线性提供了原则性框架，连接了动力系统理论与循环神经网络中长时记忆和结构化计算的功能需求，对人工和生物神经系统都有启示。

Abstract: Memory and long-range temporal processing are core requirements for sequence
modeling tasks across natural language processing, time-series forecasting,
speech recognition, and control. While nonlinear recurrence has long been
viewed as essential for enabling such mechanisms, recent work suggests that
linear dynamics may often suffice. In this study, we go beyond performance
comparisons to systematically dissect the functional role of nonlinearity in
recurrent networks--identifying both when it is computationally necessary, and
what mechanisms it enables. We use Almost Linear Recurrent Neural Networks
(AL-RNNs), which allow fine-grained control over nonlinearity, as both a
flexible modeling tool and a probe into the internal mechanisms of memory.
Across a range of classic sequence modeling tasks and a real-world stimulus
selection task, we find that minimal nonlinearity is not only sufficient but
often optimal, yielding models that are simpler, more robust, and more
interpretable than their fully nonlinear or linear counterparts. Our results
provide a principled framework for selectively introducing nonlinearity,
bridging dynamical systems theory with the functional demands of long-range
memory and structured computation in recurrent neural networks, with
implications for both artificial and biological neural systems.

</details>


### [301] [W4S4: WaLRUS Meets S4 for Long-Range Sequence Modeling](https://arxiv.org/abs/2506.07920)
*Hossein Babaei,Mel White,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 本文基于SaFARi框架和现有的WaLRUS SSMs引入新变体W4S4，实验表明其在多任务上优于现有方法，为下一代深度SSM模型提供基础。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（SSMs）有效性依赖状态矩阵选择和初始化，旨在引入新的SSM变体改善性能。

Method: 基于SaFARi框架和现有WaLRUS SSMs，构建基于冗余小波框架的新类SSM W4S4。

Result: WaLRUS在长序列信息保留上优于HiPPO - based SSMs，在延迟重建、分类基准和长序列建模任务中表现一致提升。

Conclusion: 基于小波的状态动态的高质量结构化初始化比现有方法有显著优势，WaLRUS为下一代深度SSM模型提供可扩展和通用的基础。

Abstract: State Space Models (SSMs) have emerged as powerful components for sequence
modeling, enabling efficient handling of long-range dependencies via linear
recurrence and convolutional computation. However, their effectiveness depends
heavily on the choice and initialization of the state matrix. In this work, we
build on the SaFARi framework and existing WaLRUS SSMs to introduce a new
variant, W4S4 (WaLRUS for S4), a new class of SSMs constructed from redundant
wavelet frames. WaLRUS admits a stable diagonalization and supports fast kernel
computation without requiring low-rank approximations, making it both
theoretically grounded and computationally efficient. We show that WaLRUS
retains information over long horizons significantly better than HiPPO-based
SSMs, both in isolation and when integrated into deep architectures such as S4.
Our experiments demonstrate consistent improvements across delay reconstruction
tasks, classification benchmarks, and long-range sequence modeling, confirming
that high-quality, structured initialization enabled by wavelet-based state
dynamic offers substantial advantages over existing alternatives. WaLRUS
provides a scalable and versatile foundation for the next generation of deep
SSM-based models.

</details>


### [302] [A Generative Physics-Informed Reinforcement Learning-Based Approach for Construction of Representative Drive Cycle](https://arxiv.org/abs/2506.07929)
*Amirreza Yasami,Mohammadali Tofigh,Mahdi Shahbakhti,Charles Robert Koch*

Main category: cs.LG

TL;DR: 提出PIESMC方法构建驾驶循环，实验显示能减少误差、提升速度，可重现关键指标。


<details>
  <summary>Details</summary>
Motivation: 准确的驾驶循环构建对车辆设计、燃油经济性分析和环境影响评估至关重要。

Method: 采用物理信息强化学习框架与蒙特卡罗采样的PIESMC方法。

Result: 在两个真实数据集上实验，相比MTB方法累积运动学片段误差最多降低57.3%，比MCB方法降低10.5%，速度比传统技术快近一个数量级，能重现实验中心趋势和变异性。

Conclusion: PIESMC方法能有效构建驾驶循环，降低计算成本，准确复制关键运动学和能量指标。

Abstract: Accurate driving cycle construction is crucial for vehicle design, fuel
economy analysis, and environmental impact assessments. A generative
Physics-Informed Expected SARSA-Monte Carlo (PIESMC) approach that constructs
representative driving cycles by capturing transient dynamics, acceleration,
deceleration, idling, and road grade transitions while ensuring model fidelity
is introduced. Leveraging a physics-informed reinforcement learning framework
with Monte Carlo sampling, PIESMC delivers efficient cycle construction with
reduced computational cost. Experimental evaluations on two real-world datasets
demonstrate that PIESMC replicates key kinematic and energy metrics, achieving
up to a 57.3% reduction in cumulative kinematic fragment errors compared to the
Micro-trip-based (MTB) method and a 10.5% reduction relative to the
Markov-chain-based (MCB) method. Moreover, it is nearly an order of magnitude
faster than conventional techniques. Analyses of vehicle-specific power
distributions and wavelet-transformed frequency content further confirm its
ability to reproduce experimental central tendencies and variability.

</details>


### [303] [TokenBreak: Bypassing Text Classification Models Through Token Manipulation](https://arxiv.org/abs/2506.07948)
*Kasimir Schulz,Kenneth Yeung,Kieran Evans*

Main category: cs.LG

TL;DR: 介绍TokenBreak新型攻击可绕过文本分类保护模型，还提出无需重新训练防御模型的防御策略。


<details>
  <summary>Details</summary>
Motivation: 现有文本分类模型用于防范威胁，但存在可被绕过的漏洞，为探索绕过方法及相应防御策略。

Method: 利用保护模型的分词策略，操纵输入文本使模型分类错误；提出无需重新训练的防御策略。

Result: TokenBreak攻击可绕过保护模型，且终端目标仍能理解操纵后的文本；提出了额外的防御策略。

Conclusion: TokenBreak攻击可突破现有保护，基于模型架构能预测其是否易受攻击，所提防御策略可增加保护。

Abstract: Natural Language Processing (NLP) models are used for text-related tasks such
as classification and generation. To complete these tasks, input data is first
tokenized from human-readable text into a format the model can understand,
enabling it to make inferences and understand context. Text classification
models can be implemented to guard against threats such as prompt injection
attacks against Large Language Models (LLMs), toxic input and cybersecurity
risks such as spam emails. In this paper, we introduce TokenBreak: a novel
attack that can bypass these protection models by taking advantage of the
tokenization strategy they use. This attack technique manipulates input text in
such a way that certain models give an incorrect classification. Importantly,
the end target (LLM or email recipient) can still understand and respond to the
manipulated text and therefore be vulnerable to the very attack the protection
model was put in place to prevent. The tokenizer is tied to model architecture,
meaning it is possible to predict whether or not a model is vulnerable to
attack based on family. We also present a defensive strategy as an added layer
of protection that can be implemented without having to retrain the defensive
model.

</details>


### [304] [Cost-Optimal Active AI Model Evaluation](https://arxiv.org/abs/2506.07949)
*Anastasios N. Angelopoulos,Jacob Eisenstein,Jonathan Berant,Alekh Agarwal,Adam Fisch*

Main category: cs.LG

TL;DR: 本文提出成本感知方法，在强弱评分者间分配标注预算，以低成本实现相同估计精度。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统开发周期中持续评估、数据获取和标注成本高，需依赖合成标注数据但有偏差，因此要平衡强弱评分者的使用。

Method: 基于主动和预测驱动的统计推断，推导一系列成本最优策略，在强弱评分者间分配给定标注预算以最大化统计效率。

Result: 使用合成和真实世界数据，实证表明在示例难度差异大的任务中，该策略比标准评估方法以更低总标注预算实现相同估计精度。

Conclusion: 提出的成本最优策略能有效平衡强弱评分者使用，降低标注成本。

Abstract: The development lifecycle of generative AI systems requires continual
evaluation, data acquisition, and annotation, which is costly in both resources
and time. In practice, rapid iteration often makes it necessary to rely on
synthetic annotation data because of the low cost, despite the potential for
substantial bias. In this paper, we develop novel, cost-aware methods for
actively balancing the use of a cheap, but often inaccurate, weak rater -- such
as a model-based autorater that is designed to automatically assess the quality
of generated content -- with a more expensive, but also more accurate, strong
rater alternative such as a human. More specifically, the goal of our approach
is to produce a low variance, unbiased estimate of the mean of the target
"strong" rating, subject to some total annotation budget. Building on recent
work in active and prediction-powered statistical inference, we derive a family
of cost-optimal policies for allocating a given annotation budget between weak
and strong raters so as to maximize statistical efficiency. Using synthetic and
real-world data, we empirically characterize the conditions under which these
policies yield improvements over prior methods. We find that, especially in
tasks where there is high variability in the difficulty of examples, our
policies can achieve the same estimation precision at a far lower total
annotation budget than standard evaluation methods.

</details>


### [305] [Neural Tangent Kernel Analysis to Probe Convergence in Physics-informed Neural Solvers: PIKANs vs. PINNs](https://arxiv.org/abs/2506.07958)
*Salah A. Faroughi,Farinaz Mostajeran*

Main category: cs.LG

TL;DR: 本文运用NTK理论分析cPIKANs，研究其训练时核结构演变及对学习效率的影响，分析四种PDEs的NTK矩阵谱性质和优化策略的影响，发现其NTK行为易处理，为cPIKANs提供理论见解。


<details>
  <summary>Details</summary>
Motivation: cPIKANs训练动态和收敛行为在理论和数值上缺乏研究，需推进对其的理论理解。

Method: 使用NTK理论分析cPIKANs，推导标准cKANs的NTK，扩展到物理信息场景，分析NTK矩阵谱性质，研究优化策略影响。

Result: cPIKANs的NTK行为易处理，能揭示标准PINNs无法捕捉的学习动态，谱趋势显示域分解何时能改善训练。

Conclusion: 这是首次对cPIKANs进行系统的NTK研究，能阐明并预测其实证性能。

Abstract: Physics-informed Kolmogorov-Arnold Networks (PIKANs), and in particular their
Chebyshev-based variants (cPIKANs), have recently emerged as promising models
for solving partial differential equations (PDEs). However, their training
dynamics and convergence behavior remain largely unexplored both theoretically
and numerically. In this work, we aim to advance the theoretical understanding
of cPIKANs by analyzing them using Neural Tangent Kernel (NTK) theory. Our
objective is to discern the evolution of kernel structure throughout
gradient-based training and its subsequent impact on learning efficiency. We
first derive the NTK of standard cKANs in a supervised setting, and then extend
the analysis to the physics-informed context. We analyze the spectral
properties of NTK matrices, specifically their eigenvalue distributions and
spectral bias, for four representative PDEs: the steady-state Helmholtz
equation, transient diffusion and Allen-Cahn equations, and forced vibrations
governed by the Euler-Bernoulli beam equation. We also conduct an investigation
into the impact of various optimization strategies, e.g., first-order,
second-order, and hybrid approaches, on the evolution of the NTK and the
resulting learning dynamics. Results indicate a tractable behavior for NTK in
the context of cPIKANs, which exposes learning dynamics that standard
physics-informed neural networks (PINNs) cannot capture. Spectral trends also
reveal when domain decomposition improves training, directly linking kernel
behavior to convergence rates under different setups. To the best of our
knowledge, this is the first systematic NTK study of cPIKANs, providing
theoretical insight that clarifies and predicts their empirical performance.

</details>


### [306] [A Two-Phase Deep Learning Framework for Adaptive Time-Stepping in High-Speed Flow Modeling](https://arxiv.org/abs/2506.07969)
*Jacob Helwig,Sai Sreeharsha Adavi,Xuan Zhang,Yuchao Lin,Felix S. Chim,Luke Takeshi Vizzini,Haiyang Yu,Muhammad Hasnain,Saykat Kumar Biswas,John J. Holloway,Narendra Singh,N. K. Anand,Swagnik Guhathakurta,Shuiwang Ji*

Main category: cs.LG

TL;DR: 提出名为ShockCast的两阶段机器学习方法，用于自适应时间步长的高速流建模，并进行评估，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注低速流体流动，高速流有激波等突变现象，需自适应时间步长方法平衡计算成本与现象解析。

Method: 提出两阶段的ShockCast方法，第一阶段用机器学习模型预测时间步长，第二阶段用预测的时间步长和当前流体场推进系统状态，探索时间步长预测组件并引入调节策略。

Result: 生成两个超音速流数据集评估方法，代码作为AIRS库的一部分公开。

Conclusion: ShockCast是首个学习高速流的框架，为高速流建模提供了新方法。

Abstract: We consider the problem of modeling high-speed flows using machine learning
methods. While most prior studies focus on low-speed fluid flows in which
uniform time-stepping is practical, flows approaching and exceeding the speed
of sound exhibit sudden changes such as shock waves. In such cases, it is
essential to use adaptive time-stepping methods to allow a temporal resolution
sufficient to resolve these phenomena while simultaneously balancing
computational costs. Here, we propose a two-phase machine learning method,
known as ShockCast, to model high-speed flows with adaptive time-stepping. In
the first phase, we propose to employ a machine learning model to predict the
timestep size. In the second phase, the predicted timestep is used as an input
along with the current fluid fields to advance the system state by the
predicted timestep. We explore several physically-motivated components for
timestep prediction and introduce timestep conditioning strategies inspired by
neural ODE and Mixture of Experts. As ShockCast is the first framework for
learning high-speed flows, we evaluate our methods by generating two supersonic
flow datasets, available at https://huggingface.co/datasets/divelab. Our code
is publicly available as part of the AIRS library
(https://github.com/divelab/AIRS).

</details>


### [307] [HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization](https://arxiv.org/abs/2506.07972)
*Hongzheng Chen,Yingheng Wang,Yaohui Cai,Hins Hu,Jiajie Li,Shirley Huang,Chenhui Deng,Rongjian Liang,Shufeng Kong,Haoxing Ren,Samitha Samaranayake,Carla P. Gomes,Zhiru Zhang*

Main category: cs.LG

TL;DR: 现有评估方法无法充分评估大语言模型能力，本文介绍HeuriGym框架评估组合优化问题启发式算法，评估9个模型暴露其局限性，提出QYI指标，开源基准旨在引导大模型发展。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法不能充分评估大语言模型在推理和基于代理的问题解决能力，需要新的评估方法。

Method: 引入HeuriGym框架评估大语言模型生成的启发式算法，在多个领域的9个问题上评估9个模型，提出QYI指标量化性能。

Result: 评估暴露了大语言模型在工具使用、规划和自适应推理方面的持续局限性，顶级模型QYI得分远低于专家基线。

Conclusion: 开源基准有助于引导大语言模型在科学和工程领域更有效地解决实际问题。

Abstract: While Large Language Models (LLMs) have demonstrated significant advancements
in reasoning and agent-based problem-solving, current evaluation methodologies
fail to adequately assess their capabilities: existing benchmarks either rely
on closed-ended questions prone to saturation and memorization, or subjective
comparisons that lack consistency and rigor. In this work, we introduce
HeuriGym, an agentic framework designed for evaluating heuristic algorithms
generated by LLMs for combinatorial optimization problems, characterized by
clearly defined objectives and expansive solution spaces. HeuriGym empowers
LLMs to propose heuristics, receive evaluative feedback via code execution, and
iteratively refine their solutions. We evaluate nine state-of-the-art models on
nine problems across domains such as computer systems, logistics, and biology,
exposing persistent limitations in tool use, planning, and adaptive reasoning.
To quantify performance, we propose the Quality-Yield Index (QYI), a metric
that captures both solution pass rate and quality. Even top models like
GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below
the expert baseline of 1. Our open-source benchmark aims to guide the
development of LLMs toward more effective and realistic problem-solving in
scientific and engineering domains.

</details>


### [308] [Hyperpruning: Efficient Search through Pruned Variants of Recurrent Neural Networks Leveraging Lyapunov Spectrum](https://arxiv.org/abs/2506.07975)
*Caleb Zheng,Eli Shlizerman*

Main category: cs.LG

TL;DR: 本文提出基于Lyapunov Spectrum的超剪枝框架LSH，可高效识别优质剪枝模型，实验表明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有循环神经网络剪枝方法众多，需找到合适策略，且穷举搜索成本高、无早期性能保证。

Method: 提出基于Lyapunov Spectrum的距离度量，将其与标准超参数优化算法结合，构建LSH框架。

Result: 在多个架构和数据集上实验，LSH在固定训练预算和目标剪枝率下能找到更优剪枝模型，性能超基于损失的基线和稠密模型。

Conclusion: LSH框架能有效且高效地识别出优质的剪枝模型，提升循环神经网络效率。

Abstract: A variety of pruning methods have been introduced for over-parameterized
Recurrent Neural Networks to improve efficiency in terms of power consumption
and storage utilization. These advances motivate a new paradigm, termed
`hyperpruning', which seeks to identify the most suitable pruning strategy for
a given network architecture and application. Unlike conventional
hyperparameter search, where the optimal configuration's accuracy remains
uncertain, in the context of network pruning, the accuracy of the dense model
sets the target for the accuracy of the pruned one. The goal, therefore, is to
discover pruned variants that match or even surpass this established accuracy.
However, exhaustive search over pruning configurations is computationally
expensive and lacks early performance guarantees. To address this challenge, we
propose a novel Lyapunov Spectrum (LS)-based distance metric that enables early
comparison between pruned and dense networks, allowing accurate prediction of
post-training performance. By integrating this LS-based distance with standard
hyperparameter optimization algorithms, we introduce an efficient hyperpruning
framework, termed LS-based Hyperpruning (LSH). LSH reduces search time by an
order of magnitude compared to conventional approaches relying on full
training. Experiments on stacked LSTM and RHN architectures using the Penn
Treebank dataset, and on AWD-LSTM-MoS using WikiText-2, demonstrate that under
fixed training budgets and target pruning ratios, LSH consistently identifies
superior pruned models. Remarkably, these pruned variants not only outperform
those selected by loss-based baseline but also exceed the performance of their
dense counterpart.

</details>


### [309] [Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction](https://arxiv.org/abs/2506.07976)
*Junhong Shen,Hao Bai,Lunjun Zhang,Yifei Zhou,Amrith Setlur,Shengbang Tong,Diego Caples,Nan Jiang,Tong Zhang,Ameet Talwalkar,Aviral Kumar*

Main category: cs.LG

TL;DR: 本文提出测试时交互扩展方法，以增加智能体交互范围，通过课程式在线强化学习方法TTI在网页基准测试中取得SOTA成果，证明交互扩展是强大且互补的维度。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展范式无法让智能体从环境获取新信息或随时间调整行为，因此提出测试时交互扩展。

Method: 提出TTI，一种基于课程的在线强化学习方法，通过自适应调整智能体的滚动长度进行训练。

Result: 使用Gemma 3 12B模型，TTI在WebVoyager和WebArena基准测试中产生了最先进的开源、开放数据网页智能体，且能让智能体自适应平衡探索与利用。

Conclusion: 交互扩展是扩展每步计算的强大互补维度，为训练自适应智能体提供新途径。

Abstract: The current paradigm of test-time scaling relies on generating long reasoning
traces ("thinking" more) before producing a response. In agent problems that
require interaction, this can be done by generating thinking traces before
acting in the world. However, this process does not allow agents to acquire new
information from the environment or adapt their behavior over time. In this
work, we propose to scale test-time interaction, an untapped dimension of
test-time scaling that increases the agent's interaction horizon to enable
running rich behaviors such as exploration, backtracking, and dynamic
re-planning within a single rollout. To demonstrate the promise of this scaling
dimension, we study the domain of web agents. We first show that even
prompting-based interaction scaling without any training can improve task
success on web benchmarks non-trivially. Building on this, we introduce TTI
(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)
approach that trains agents by adaptively adjusting their rollout lengths.
Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data
web agents on WebVoyager and WebArena benchmarks. We further show that TTI
enables agents to balance exploration and exploitation adaptively. Our results
establish interaction scaling as a powerful, complementary axis to scaling
per-step compute, offering new avenues for training adaptive agents.

</details>


### [310] [Realistic Urban Traffic Generator using Decentralized Federated Learning for the SUMO simulator](https://arxiv.org/abs/2506.07980)
*Alberto Bazán-Guillén,Carlos Beis-Penedo,Diego Cajaraville-Aboy,Pablo Barbecho-Bautista,Rebeca P. Díaz-Redondo,Luis J. de la Cruz Llopis,Ana Fernández-Vilas,Mónica Aguilar Igartua,Manuel Fernández-Veiga*

Main category: cs.LG

TL;DR: 提出DesRUTGe框架结合DRL和SUMO生成24小时交通模式，用DFL解决现有方法问题，在巴塞罗那数据评估中表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有生成高保真、时变交通概况的方法存在准确性、可扩展性问题及隐私担忧，需要改进。

Method: 引入DesRUTGe框架，集成DRL和SUMO，使用DFL，各交通检测器及对应区域作为独立学习节点，训练本地模型并交换参数。

Result: 使用巴塞罗那真实数据评估，DesRUTGe比标准SUMO工具和其他集中学习方法能更准确且保护隐私地生成交通模式。

Conclusion: DesRUTGe框架有效解决现有方法的问题，在交通模式生成上有更好表现。

Abstract: Realistic urban traffic simulation is essential for sustainable urban
planning and the development of intelligent transportation systems. However,
generating high-fidelity, time-varying traffic profiles that accurately reflect
real-world conditions, especially in large-scale scenarios, remains a major
challenge. Existing methods often suffer from limitations in accuracy,
scalability, or raise privacy concerns due to centralized data processing. This
work introduces DesRUTGe (Decentralized Realistic Urban Traffic Generator), a
novel framework that integrates Deep Reinforcement Learning (DRL) agents with
the SUMO simulator to generate realistic 24-hour traffic patterns. A key
innovation of DesRUTGe is its use of Decentralized Federated Learning (DFL),
wherein each traffic detector and its corresponding urban zone function as an
independent learning node. These nodes train local DRL models using minimal
historical data and collaboratively refine their performance by exchanging
model parameters with selected peers (e.g., geographically adjacent zones),
without requiring a central coordinator. Evaluated using real-world data from
the city of Barcelona, DesRUTGe outperforms standard SUMO-based tools such as
RouteSampler, as well as other centralized learning approaches, by delivering
more accurate and privacy-preserving traffic pattern generation.

</details>


### [311] [Generative Modeling of Weights: Generalization or Memorization?](https://arxiv.org/abs/2506.07998)
*Boya Zeng,Yida Yin,Zhiqiu Xu,Zhuang Liu*

Main category: cs.LG

TL;DR: 研究分析四种生成神经网络权重的方法，发现它们主要靠记忆生成，无法超越简单基线，且记忆问题难缓解。


<details>
  <summary>Details</summary>
Motivation: 探索生成模型合成有效神经网络权重时生成新模型权重的能力。

Method: 研究四种代表性方法生成新模型权重的能力，并与简单基线对比，尝试通过修改建模因素和数据增强缓解记忆问题。

Result: 这些方法主要靠记忆生成权重，无法超越简单基线，且记忆问题难以有效缓解。

Conclusion: 对当前生成模型能建模的数据类型给出现实评估，强调在新领域需更谨慎评估生成模型。

Abstract: Generative models, with their success in image and video generation, have
recently been explored for synthesizing effective neural network weights. These
approaches take trained neural network checkpoints as training data, and aim to
generate high-performing neural network weights during inference. In this work,
we examine four representative methods on their ability to generate novel model
weights, i.e., weights that are different from the checkpoints seen during
training. Surprisingly, we find that these methods synthesize weights largely
by memorization: they produce either replicas, or at best simple
interpolations, of the training checkpoints. Current methods fail to outperform
simple baselines, such as adding noise to the weights or taking a simple weight
ensemble, in obtaining different and simultaneously high-performing models. We
further show that this memorization cannot be effectively mitigated by
modifying modeling factors commonly associated with memorization in image
diffusion models, or applying data augmentations. Our findings provide a
realistic assessment of what types of data current generative models can model,
and highlight the need for more careful evaluation of generative models in new
domains. Our code is available at
https://github.com/boyazeng/weight_memorization.

</details>


### [312] [Reparameterized LLM Training via Orthogonal Equivalence Transformation](https://arxiv.org/abs/2506.08001)
*Zeju Qiu,Simon Buchholz,Tim Z. Xiao,Maximilian Dax,Bernhard Schölkopf,Weiyang Liu*

Main category: cs.LG

TL;DR: 提出POET重参数化训练算法优化大语言模型训练，实验验证其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练面临有效可靠训练的挑战。

Method: 提出POET算法，用正交等价变换优化神经元，重参数化每个神经元，开发高效近似方法。

Result: POET能稳定优化目标函数，提高泛化能力，实验验证其在训练大语言模型中的有效性和可扩展性。

Conclusion: POET算法能有效应对大语言模型训练挑战，具有良好效果和可扩展性。

Abstract: While large language models (LLMs) are driving the rapid advancement of
artificial intelligence, effectively and reliably training these large models
remains one of the field's most significant challenges. To address this
challenge, we propose POET, a novel reParameterized training algorithm that
uses Orthogonal Equivalence Transformation to optimize neurons. Specifically,
POET reparameterizes each neuron with two learnable orthogonal matrices and a
fixed random weight matrix. Because of its provable preservation of spectral
properties of weight matrices, POET can stably optimize the objective function
with improved generalization. We further develop efficient approximations that
make POET flexible and scalable for training large-scale neural networks.
Extensive experiments validate the effectiveness and scalability of POET in
training LLMs.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [313] [EvoGrad: Metaheuristics in a Differentiable Wonderland](https://arxiv.org/abs/2506.06320)
*Beatrice F. R. Citterio,Andrea Tangherloni*

Main category: cs.NE

TL;DR: 提出EvoGrad统一可微框架，结合进化计算和群智能与基于梯度的优化，实验表明可微版本算法表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统进化计算和群智能算法不利用局部梯度信息，限制优化效率，需要改进。

Method: 引入EvoGrad框架，将传统进化和群操作转换为可微操作，实现端到端梯度优化。

Result: 在基准优化函数和小型神经网络回归器训练实验中，可微版本算法在多数场景优于传统算法。

Conclusion: 全可微进化和群优化有显著优势，为混合优化框架树立新标准。

Abstract: Differentiable programming has revolutionised optimisation by enabling
efficient gradient-based training of complex models, such as Deep Neural
Networks (NNs) with billions and trillions of parameters. However, traditional
Evolutionary Computation (EC) and Swarm Intelligence (SI) algorithms, widely
successful in discrete or complex search spaces, typically do not leverage
local gradient information, limiting their optimisation efficiency. In this
paper, we introduce EvoGrad, a unified differentiable framework that integrates
EC and SI with gradient-based optimisation through backpropagation. EvoGrad
converts conventional evolutionary and swarm operators (e.g., selection,
mutation, crossover, and particle updates) into differentiable operators,
facilitating end-to-end gradient optimisation. Extensive experiments on
benchmark optimisation functions and training of small NN regressors reveal
that our differentiable versions of EC and SI metaheuristics consistently
outperform traditional, gradient-agnostic algorithms in most scenarios. Our
results show the substantial benefits of fully differentiable evolutionary and
swarm optimisation, setting a new standard for hybrid optimisation frameworks.

</details>


### [314] [Neural networks with image recognition by pairs](https://arxiv.org/abs/2506.06322)
*Polad Geidarov*

Main category: cs.NE

TL;DR: 探讨将基于度量识别方法的神经网络进行转换以应用经典学习算法，训练通过成对识别图像，该网络有诸多优势。


<details>
  <summary>Details</summary>
Motivation: 将经典学习算法应用于基于度量识别方法的神经网络，避免使用解析表达式计算权重值。

Method: 将网络进行转换，通过成对识别图像进行训练。

Result: 简化了学习过程，可轻松通过添加新图像扩展神经网络。

Conclusion: 该网络具有架构简单透明、训练简单可靠、可处理大量图像、能在不改变先前权重和阈值的情况下增加可识别类别等优势。

Abstract: Neural networks based on metric recognition methods have a strictly
determined architecture. Number of neurons, connections, as well as weights and
thresholds values are calculated analytically, based on the initial conditions
of tasks: number of recognizable classes, number of samples, metric expressions
used. This paper discusses the possibility of transforming these networks in
order to apply classical learning algorithms to them without using analytical
expressions that calculate weight values. In the received network, training is
carried out by recognizing images in pairs. This approach simplifies the
learning process and easily allows to expand the neural network by adding new
images to the recognition task. The advantages of these networks, including
such as: 1) network architecture simplicity and transparency; 2) training
simplicity and reliability; 3) the possibility of using a large number of
images in the recognition problem using a neural network; 4) a consistent
increase in the number of recognizable classes without changing the previous
values of weights and thresholds.

</details>


### [315] [Evolutionary model for energy trading in community microgrids using Hawk-Dove strategies](https://arxiv.org/abs/2506.06325)
*Viorica Rozina Chifu,Tudor Cioara,Cristina Bianca Pop,Ionut Anghel*

Main category: cs.NE

TL;DR: 本文提出微电网间分散式能源合作模型，通过进化算法模拟交易，在模拟场景测试有效。


<details>
  <summary>Details</summary>
Motivation: 构建一种微电网间的能源合作模型，实现个体和社区层面的能源平衡。

Method: 将每个微电网建模为自主代理，采用鹰或鸽策略；用进化算法模拟买卖微电网的交互，个体用能源交易矩阵表示，通过重组和变异操作实现种群进化，用多标准适应度函数评估个体。

Result: 在100个微电网的模拟场景中，95个微电网达到稳定能源状态。

Conclusion: 所提出的模型在实现个体和社区层面的能源平衡方面是有效的。

Abstract: This paper proposes a decentralized model of energy cooperation between
microgrids, in which decisions are made locally, at the level of the microgrid
community. Each microgrid is modeled as an autonomous agent that adopts a Hawk
or Dove strategy, depending on the level of energy stored in the battery and
its role in the energy trading process. The interactions between selling and
buying microgrids are modeled through an evolutionary algorithm. An individual
in the algorithm population is represented as an energy trading matrix that
encodes the amounts of energy traded between the selling and buying microgrids.
The population evolution is achieved by recombination and mutation operators.
Recombination uses a specialized operator for matrix structures, and mutation
is applied to the matrix elements according to a Gaussian distribution. The
evaluation of an individual is made with a multi-criteria fitness function that
considers the seller profit, the degree of energy stability at the community
level, penalties for energy imbalance at the community level and for the
degradation of microgrids batteries. The method was tested on a simulated
scenario with 100 microgrids, each with its own selling and buying thresholds,
to reflect a realistic environment with variable storage characteristics of
microgrids batteries. By applying the algorithm on this scenario, 95 out of the
100 microgrids reached a stable energy state. This result confirms the
effectiveness of the proposed model in achieving energy balance both at the
individual level, for each microgrid, and at the level of the entire community.

</details>


### [316] [Introduction to Predictive Coding Networks for Machine Learning](https://arxiv.org/abs/2506.06332)
*Mikko Stenlund*

Main category: cs.NE

TL;DR: 本文为机器学习从业者快速介绍预测编码网络（PCNs），涵盖基础架构、规则及实现，还以图像分类任务为例并提供代码。


<details>
  <summary>Details</summary>
Motivation: 为机器学习从业者提供对预测编码网络的快速入门介绍，作为传统前馈神经网络的替代。

Method: 介绍PCNs的基础网络架构、推理和学习更新规则以及算法实现，并以CIFAR - 10图像分类任务作为应用示例。

Result: 以CIFAR - 10任务作为基准应用，同时提供包含PyTorch实现的Python笔记本。

Conclusion: PCNs是理解大脑分层计算的生物启发框架，可作为机器学习中传统前馈神经网络的替代。

Abstract: Predictive coding networks (PCNs) constitute a biologically inspired
framework for understanding hierarchical computation in the brain, and offer an
alternative to traditional feedforward neural networks in ML. This note serves
as a quick, onboarding introduction to PCNs for machine learning practitioners.
We cover the foundational network architecture, inference and learning update
rules, and algorithmic implementation. A concrete image-classification task
(CIFAR-10) is provided as a benchmark-smashing application, together with an
accompanying Python notebook containing the PyTorch implementation.

</details>


### [317] [CR-BLEA: Contrastive Ranking for Adaptive Resource Allocation in Bilevel Evolutionary Algorithms](https://arxiv.org/abs/2506.06362)
*Dejun Xu,Jijia Chen,Gary G. Yen,Min Jiang*

Main category: cs.NE

TL;DR: 提出用于双层进化算法的资源分配框架，减少计算成本并提高效率。


<details>
  <summary>Details</summary>
Motivation: 双层优化计算挑战大，进化算法资源需求高且存在冗余评估，现有方法仍有资源浪费问题。

Method: 提出新颖资源分配框架，利用对比排名网络学习上下层解关系模式，采用基于参考的排名策略优化任务并自适应控制重采样。

Result: 在五个最先进双层算法的综合实验中，框架显著降低计算成本，同时保持或提高解的准确性。

Conclusion: 该工作提供了提高双层进化算法效率的通用策略，为更具扩展性的双层优化铺平道路。

Abstract: Bilevel optimization poses a significant computational challenge due to its
nested structure, where each upper-level candidate solution requires solving a
corresponding lower-level problem. While evolutionary algorithms (EAs) are
effective at navigating such complex landscapes, their high resource demands
remain a key bottleneck -- particularly the redundant evaluation of numerous
unpromising lower-level tasks. Despite recent advances in multitasking and
transfer learning, resource waste persists. To address this issue, we propose a
novel resource allocation framework for bilevel EAs that selectively identifies
and focuses on promising lower-level tasks. Central to our approach is a
contrastive ranking network that learns relational patterns between paired
upper- and lower-level solutions online. This knowledge guides a
reference-based ranking strategy that prioritizes tasks for optimization and
adaptively controls resampling based on estimated population quality.
Comprehensive experiments across five state-of-the-art bilevel algorithms show
that our framework significantly reduces computational cost while preserving --
or even enhancing -- solution accuracy. This work offers a generalizable
strategy to improve the efficiency of bilevel EAs, paving the way for more
scalable bilevel optimization.

</details>


### [318] [Structured State Space Model Dynamics and Parametrization for Spiking Neural Networks](https://arxiv.org/abs/2506.06374)
*Maxime Fabre,Lyubov Dudchenko,Emre Neftci*

Main category: cs.NE

TL;DR: 本文在SSMs和二阶脉冲神经元模型间建立数学桥梁，提出两种新脉冲神经元模型，在多数据集表现超或接近SOTA，兼顾性能与效率。


<details>
  <summary>Details</summary>
Motivation: 多状态脉冲神经元在推理和训练中因内部动态存在不稳定性，限制性能和可扩展性，而SSMs在长序列处理有优势，因此建立二者联系以改进脉冲神经元模型。

Method: 基于对角SSMs的结构和参数化策略，提出两种新脉冲神经元模型，一是对AdLIF神经元进行时间步训练和对数重新参数化；二是引入复杂状态SSMs的初始化和结构。

Result: 两种模型在基于重置的脉冲神经元模型中，在事件和原始音频语音识别数据集上表现超或接近SOTA，参数和动态内存需求合理，活动稀疏性高。

Conclusion: 提出的模型增强了网络规模的可扩展性，在性能和效率间取得良好平衡。

Abstract: Multi-state spiking neurons such as the adaptive leaky integrate-and-fire
(AdLIF) neuron offer compelling alternatives to conventional deep learning
models thanks to their sparse binary activations, second-order nonlinear
recurrent dynamics, and efficient hardware realizations. However, such internal
dynamics can cause instabilities during inference and training, often limiting
performance and scalability. Meanwhile, state space models (SSMs) excel in long
sequence processing using linear state-intrinsic recurrence resembling spiking
neurons' subthreshold regime. Here, we establish a mathematical bridge between
SSMs and second-order spiking neuron models. Based on structure and
parametrization strategies of diagonal SSMs, we propose two novel spiking
neuron models. The first extends the AdLIF neuron through timestep training and
logarithmic reparametrization to facilitate training and improve final
performance. The second additionally brings initialization and structure from
complex-state SSMs, broadening the dynamical regime to oscillatory dynamics.
Together, our two models achieve beyond or near state-of-the-art (SOTA)
performances for reset-based spiking neuron models across both event-based and
raw audio speech recognition datasets. We achieve this with a favorable number
of parameters and required dynamic memory while maintaining high activity
sparsity. Our models demonstrate enhanced scalability in network size and
strike a favorable balance between performance and efficiency with respect to
SSM models.

</details>


### [319] [Employing Discrete Fourier Transform in Representational Learning](https://arxiv.org/abs/2506.06765)
*Raoof HojatJalali,Edmondo Trentin*

Main category: cs.NE

TL;DR: 提出用输入的离散傅里叶变换（DFT）作为学习目标进行图像表示学习，在CIFAR - 10上验证效果优于传统自编码器，且低频训练效果接近全频谱。


<details>
  <summary>Details</summary>
Motivation: 探索替代传统自编码器以原始输入为重建目标的图像表示学习方法，利用DFT的特性进行表示学习。

Method: 采用输入的DFT作为重建目标，代替原始输入进行图像表示学习，还尝试仅使用低频分量训练。

Result: 在CIFAR - 10上使用ResNet - 50实现52.8%的top - 1准确率，比传统自编码器高12.8个百分点；仅用低频分量训练准确率降低极小。

Conclusion: DFT可作为一种可行的图像表示学习的学习目标。

Abstract: Image Representation learning via input reconstruction is a common technique
in machine learning for generating representations that can be effectively
utilized by arbitrary downstream tasks. A well-established approach is using
autoencoders to extract latent representations at the network's compression
point. These representations are valuable because they retain essential
information necessary for reconstructing the original input from the compressed
latent space. In this paper, we propose an alternative learning objective.
Instead of using the raw input as the reconstruction target, we employ the
Discrete Fourier Transform (DFT) of the input. The DFT provides meaningful
global information at each frequency level, making individual frequency
components useful as separate learning targets. When dealing with
multidimensional input data, the DFT offers remarkable flexibility by enabling
selective transformation across specific dimensions while preserving others in
the computation. Moreover, certain types of input exhibit distinct patterns in
their frequency distributions, where specific frequency components consistently
contain most of the magnitude, allowing us to focus on a subset of frequencies
rather than the entire spectrum. These characteristics position the DFT as a
viable learning objective for representation learning and we validate our
approach by achieving 52.8% top-1 accuracy on CIFAR-10 with ResNet-50 and
outperforming the traditional autoencoder by 12.8 points under identical
architectural configurations. Additionally, we demonstrate that training on
only the lower-frequency components - those with the highest magnitudes yields
results comparable to using the full frequency spectrum, with only minimal
reductions in accuracy.

</details>


### [320] [Can Biologically Plausible Temporal Credit Assignment Rules Match BPTT for Neural Similarity? E-prop as an Example](https://arxiv.org/abs/2506.06904)
*Yuhan Helena Liu,Guangyu Robert Yang,Christopher J. Cueva*

Main category: cs.NE

TL;DR: 研究生物合理学习规则，发现e - prop学习规则在任务准确性匹配时能达与BPTT相当的神经数据相似度，模型架构和初始条件对神经相似度影响更大，生物合理学习规则有进展。


<details>
  <summary>Details</summary>
Motivation: 研究生物合理学习规则以理解大脑学习方式，以往对规则与神经记录一致性研究不足。

Method: 在知名神经科学数据集上采用Procrustes分析等方法。

Result: e - prop学习规则在任务准确性匹配时神经数据相似度与BPTT相当；模型架构和初始条件对神经相似度影响更大；BPTT训练模型和生物合理对应模型在相似准确率下有相似动力学特性。

Conclusion: 生物合理学习规则取得显著进展，有潜力实现有竞争力的任务表现和神经数据相似度。

Abstract: Understanding how the brain learns may be informed by studying biologically
plausible learning rules. These rules, often approximating gradient descent
learning to respect biological constraints such as locality, must meet two
critical criteria to be considered an appropriate brain model: (1) good
neuroscience task performance and (2) alignment with neural recordings. While
extensive research has assessed the first criterion, the second remains
underexamined. Employing methods such as Procrustes analysis on well-known
neuroscience datasets, this study demonstrates the existence of a biologically
plausible learning rule -- namely e-prop, which is based on gradient truncation
and has demonstrated versatility across a wide range of tasks -- that can
achieve neural data similarity comparable to Backpropagation Through Time
(BPTT) when matched for task accuracy. Our findings also reveal that model
architecture and initial conditions can play a more significant role in
determining neural similarity than the specific learning rule. Furthermore, we
observe that BPTT-trained models and their biologically plausible counterparts
exhibit similar dynamical properties at comparable accuracies. These results
underscore the substantial progress made in developing biologically plausible
learning rules, highlighting their potential to achieve both competitive task
performance and neural data similarity.

</details>


### [321] [Research on Aerodynamic Performance Prediction of Airfoils Based on a Fusion Algorithm of Transformer and GAN](https://arxiv.org/abs/2506.06979)
*MaolinYang,Yaohui Wang,Pingyu Jiang*

Main category: cs.NE

TL;DR: 本文提出Deeptrans模型用于高效预测翼型多参数气动性能，实验显示其高效且精度高，为翼型气动性能预测提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统翼型气动性能预测方法成本高、效率低，现有数据驱动模型在多目标预测中精度不足、数据依赖性强。

Method: 提出基于改进Transformer和生成对抗网络（GAN）融合的深度学习模型Deeptrans，构建大规模数据集，设计集成Transformer编解码框架和对抗训练的模型结构。

Result: Deeptrans在验证集上MSE损失降至5.6*10 - 6，单样本预测时间仅0.0056秒，比传统CFD方法效率高近700倍，预测精度显著优于原始Transformer、GAN和VAE模型。

Conclusion: 本研究为翼型气动性能预测提供了高效的数据驱动解决方案，为深度学习建模复杂流动问题提供新思路。

Abstract: Predicting of airfoil aerodynamic performance is a key part of aircraft
design optimization, but the traditional methods (such as wind tunnel test and
CFD simulation) have the problems of high cost and low efficiency, and the
existing data-driven models face the challenges of insufficient accuracy and
strong data dependence in multi-objective prediction. Therefore, this study
proposes a deep learning model, Deeptrans, based on the fusion of improved
Transformer and generative Adversarial network (GAN), which aims to predict the
multi-parameter aerodynamic performance of airfoil efficiently. By constructing
a large-scale data set and designing a model structure that integrates a
Transformer coding-decoding framework and confrontation training, synchronous
and high-precision prediction of aerodynamic parameters is realized.
Experiments show that the MSE loss of Deeptrans on the verification set is
reduced to 5.6*10-6, and the single-sample prediction time is only 0.0056
seconds, which is nearly 700 times more efficient than the traditional CFD
method. Horizontal comparison shows that the prediction accuracy is
significantly better than the original Transformer, GAN, and VAE models. This
study provides an efficient data-driven solution for airfoil aerodynamic
performance prediction and a new idea for deep learning modeling complex flow
problems.

</details>


### [322] [Transient Dynamics in Lattices of Differentiating Ring Oscillators](https://arxiv.org/abs/2506.07253)
*Peter DelMastro,Arjun Karuvally,Hananel Hazan,Hava Siegelmann,Edward Rietman*

Main category: cs.NE

TL;DR: 研究发现大型微分神经元环晶格有局部神经同步行为，或可用于储层计算，适合低功耗AI应用。


<details>
  <summary>Details</summary>
Motivation: 当前循环神经网络多基于积分或脉冲神经元，对微分神经元的研究局限于小网络，故研究大型微分神经元环晶格行为。

Method: 通过数值模拟研究大型微分神经元环晶格，先分析未耦合环的周期轨道，再研究耦合成晶格时振荡器的相关性。

Result: 大型微分神经元环晶格有局部神经同步行为，相关区域稳态规模与相邻环间神经元共享方式有关。

Conclusion: 微分神经网络电路设计简单、功耗低，是神经形态计算的有前景底物，能支持低功耗AI应用。

Abstract: Recurrent neural networks (RNNs) are machine learning models widely used for
learning temporal relationships. Current state-of-the-art RNNs use integrating
or spiking neurons -- two classes of computing units whose outputs depend
directly on their internal states -- and accordingly there is a wealth of
literature characterizing the behavior of large networks built from these
neurons. On the other hand, past research on differentiating neurons, whose
outputs are computed from the derivatives of their internal states, remains
limited to small hand-designed networks with fewer than one-hundred neurons.
Here we show via numerical simulation that large lattices of differentiating
neuron rings exhibit local neural synchronization behavior found in the
Kuramoto model of interacting oscillators. We begin by characterizing the
periodic orbits of uncoupled rings, herein called ring oscillators. We then
show the emergence of local correlations between oscillators that grow over
time when these rings are coupled together into lattices. As the correlation
length grows, transient dynamics arise in which large regions of the lattice
settle to the same periodic orbit, and thin domain boundaries separate
adjacent, out-of-phase regions. The steady-state scale of these correlated
regions depends on how the neurons are shared between adjacent rings, which
suggests that lattices of differentiating ring oscillator might be tuned to be
used as reservoir computers. Coupled with their simple circuit design and
potential for low-power consumption, differentiating neural nets therefore
represent a promising substrate for neuromorphic computing that will enable
low-power AI applications.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [323] [Pinching-Antenna Systems For Indoor Immersive Communications: A 3D-Modeling Based Performance Analysis](https://arxiv.org/abs/2506.07771)
*Yulei Wang,Yalin Liu,Yaru Fu,Zhiguo Ding*

Main category: cs.PF

TL;DR: 本文研究用于室内沉浸式通信的捏合天线系统（PASS），构建3D模型、理论模型并进行数值分析，提供部署指南。


<details>
  <summary>Details</summary>
Motivation: 新兴的捏合天线（PA）技术在6G室内沉浸式应用中有巨大潜力，研究PASS用于室内沉浸式通信。

Method: 构建3D模型表征PASS中用户、波导和PA的分布；开发下行链路性能的理论模型；对理论模型进行数值分析。

Result: 得到理论模型的综合数值结果。

Conclusion: 为PASS部署提供了实施指南。

Abstract: The emerging pinching antenna (PA) technology has high flexibility to
reconfigure wireless channels and combat line-of-sight blockage, thus holding
transformative potential for indoor immersive applications in 6G. This paper
investigates Pinching-antenna systems (PASS) for indoor immersive
communications. Our contributions are threefold: (1) we construct a 3D model to
characterize the distribution of users, waveguides, and PAs in the PASS; (2) we
develop a general theoretical model on downlink performance of PASS by
capturing PA-user relationships and system parameters' impacts; and (3) we
conduct comprehensive numerical results of the theoretical model and provide
implementation guidelines for PASS deployments.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [324] [Enhancing Software Supply Chain Security Through STRIDE-Based Threat Modelling of CI/CD Pipelines](https://arxiv.org/abs/2506.06478)
*Sowmiya Dhandapani*

Main category: cs.SE

TL;DR: 文章针对CI/CD管道安全挑战，用结构化威胁建模方法识别和缓解风险，分析各阶段漏洞，匹配安全控制，评估改进，提出安全工具链集成策略。


<details>
  <summary>Details</summary>
Motivation: 随着CI/CD管道采用增多，保障软件供应链安全成为现代DevOps团队的关键挑战。

Method: 应用结构化威胁建模方法，对包含GitHub、Jenkins等工具的代表性管道架构建模，使用STRIDE框架分析各阶段漏洞，将威胁映射到标准中的安全控制，根据SLSA成熟度评估控制。

Result: 分析出各阶段漏洞，将威胁与安全控制匹配，评估了安全改进。

Conclusion: 该方法为增强CI/CD管道安全、应对不断演变的软件供应链威胁提供了实用路线图。

Abstract: With the increasing adoption of Continuous Integration and Continuous
Deployment pipelines, securing software supply chains has become a critical
challenge for modern DevOps teams. This study addresses these challenges by
applying a structured threat modeling approach to identify and mitigate risks
throughout the CI/CD lifecycle. By modeling a representative pipeline
architecture incorporating tools such as GitHub, Jenkins, Docker, and
Kubernetes and applying the STRIDE framework, we systematically analyze
vulnerabilities at each stage, from source code management to deployment.
Threats are documented and mapped to comprehensive security controls drawn from
standards like NIST SP 800-218, OWASP Top 10 CI/CD risks, and the SLSA
framework. Controls are further evaluated against SLSA maturity levels to
assess improvements in trust and provenance. To operationalize these findings,
the study outlines a practical security toolchain integration strategy grounded
in Security as Code and Shift Left-Shield Right principles, enabling automated,
enforceable security across the pipeline. This approach provides a pragmatic
roadmap for enhancing CI/CD pipeline security against evolving software supply
chain threats.

</details>


### [325] [Information-Theoretic Detection of Unusual Source Code Changes](https://arxiv.org/abs/2506.06508)
*Adriano Torres,Sebastian Baltes,Christoph Treude,Markus Wagner*

Main category: cs.SE

TL;DR: 从信息论角度测量开源项目源代码信息内容，评估熵演化模式，发现熵与经典代码复杂度度量不同，还实现基于熵的异常检测。


<details>
  <summary>Details</summary>
Motivation: 从信息论角度测量开源项目源代码的信息内容，为源代码演化的信息论测量改进奠定基础。

Method: 关注代码的两种基本表示（标记和抽象语法树节点）的熵，定义文本和结构熵；对95个活跃维护的开源项目的熵演化模式进行实证评估；计算熵指标与经典代码复杂度度量方法的统计关系；进行基于熵的异常检测。

Result: 熵可能捕捉到与经典指标不同的复杂度维度；基于熵的异常检测能以超过60%的精度识别异常的源代码更改事件。

Conclusion: 为源代码演化的信息论测量改进奠定基础，为静态衡量程序开发过程中的复杂度提供新方法。

Abstract: The code base of software projects evolves essentially through inserting and
removing information to and from the source code. We can measure this evolution
via the elements of information - tokens, words, nodes - of the respective
representation of the code. In this work, we approach the measurement of the
information content of the source code of open-source projects from an
information-theoretic standpoint. Our focus is on the entropy of two
fundamental representations of code: tokens and abstract syntax tree nodes,
from which we derive definitions of textual and structural entropy. We proceed
with an empirical assessment where we evaluate the evolution patterns of the
entropy of 95 actively maintained open source projects. We calculate the
statistical relationships between our derived entropy metrics and classic
methods of measuring code complexity and learn that entropy may capture
different dimensions of complexity than classic metrics. Finally, we conduct
entropy-based anomaly detection of unusual changes to demonstrate that our
approach may effectively recognise unusual source code change events with over
60% precision, and lay the groundwork for improvements to information-theoretic
measurement of source code evolution, thus paving the way for a new approach to
statically gauging program complexity throughout its development.

</details>


### [326] [Private GPTs for LLM-driven testing in software development and machine learning](https://arxiv.org/abs/2506.06509)
*Jakub Jagielski,Markus Abel*

Main category: cs.SE

TL;DR: 研究私有GPT基于需求自动生成可执行测试代码的能力，对比单步和两步流程，发现两步流程结果更好，结构化提示产出更高质量测试输出。


<details>
  <summary>Details</summary>
Motivation: 探索私有GPT基于需求自动生成可执行测试代码的能力，为产品所有者和商业智能人员提供直接生成可测试标准的方法。

Method: 以验收标准为输入，通过两种方式探索生成测试的质量：一是让大语言模型直接根据需求生成代码，二是使用Gherkin语法作为中间步骤；评估两种场景下提示的有效性。

Result: 两步流程在人类可读性和最佳编码实践方面结果更好；结构化提示可带来更高质量的测试输出。

Conclusion: 使用两步流程和结构化提示，能让私有GPT基于需求生成更高质量的可执行测试代码。

Abstract: In this contribution, we examine the capability of private GPTs to
automatically generate executable test code based on requirements. More
specifically, we use acceptance criteria as input, formulated as part of epics,
or stories, which are typically used in modern development processes. This
gives product owners, or business intelligence, respectively, a way to directly
produce testable criteria through the use of LLMs. We explore the quality of
the so-produced tests in two ways: i) directly by letting the LLM generate code
from requirements, ii) through an intermediate step using Gherkin syntax. As a
result, it turns out that the two-step procedure yields better results -where
we define better in terms of human readability and best coding practices, i.e.
lines of code and use of additional libraries typically used in testing.
Concretely, we evaluate prompt effectiveness across two scenarios: a simple
"Hello World" program and a digit classification model, showing that structured
prompts lead to higher-quality test outputs.

</details>


### [327] [Mind the Gap: A Readability-Aware Metric for Test Code Complexity](https://arxiv.org/abs/2506.06764)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 提出适用于单元测试的CCTR认知复杂度指标，评估多工具生成的测试套件，结果表明其能有效区分测试套件，为测试评估提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有复杂度指标针对功能代码，对测试代码适用性不佳，且SonarSource指标对LLM生成测试评分情况及在EvoSuite生成测试上的表现和适用性未被探究。

Method: 引入CCTR指标，整合断言密度、注释角色和测试组合模式等特征；评估EvoSuite、GPT - 4o和Mistral Large - 1024生成的15,750个测试套件。

Result: CCTR能有效区分结构化和碎片化测试套件，分数更能反映开发者感知的工作量。

Conclusion: CCTR为生成测试的可靠评估和改进提供基础，公开数据、提示和脚本支持复现。

Abstract: Automatically generated unit tests-from search-based tools like EvoSuite or
LLMs-vary significantly in structure and readability. Yet most evaluations rely
on metrics like Cyclomatic Complexity and Cognitive Complexity, designed for
functional code rather than test code. Recent studies have shown that
SonarSource's Cognitive Complexity metric assigns near-zero scores to
LLM-generated tests, yet its behavior on EvoSuite-generated tests and its
applicability to test-specific code structures remain unexplored. We introduce
CCTR, a Test-Aware Cognitive Complexity metric tailored for unit tests. CCTR
integrates structural and semantic features like assertion density, annotation
roles, and test composition patterns-dimensions ignored by traditional
complexity models but critical for understanding test code. We evaluate 15,750
test suites generated by EvoSuite, GPT-4o, and Mistral Large-1024 across 350
classes from Defects4J and SF110. Results show CCTR effectively discriminates
between structured and fragmented test suites, producing interpretable scores
that better reflect developer-perceived effort. By bridging structural analysis
and test readability, CCTR provides a foundation for more reliable evaluation
and improvement of generated tests. We publicly release all data, prompts, and
evaluation scripts to support replication.

</details>


### [328] [Beyond Surface Similarity: Evaluating LLM-Based Test Refactorings with Structural and Semantic Awareness](https://arxiv.org/abs/2506.06767)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 本文提出复合指标CTSES评估大语言模型自动重构单元测试，在两个Java基准测试中表现优于现有指标。


<details>
  <summary>Details</summary>
Motivation: 现有评估大语言模型自动重构单元测试的指标存在不足，传统指标对重命名和结构编辑敏感，基于嵌入的相似度忽略可读性和模块化。

Method: 引入复合指标CTSES，集成CodeBLEU、METEOR和ROUGE - L，在两个Java基准测试上对GPT - 4o和Mistral - Large - 2407自动重构的5000多个测试套件进行评估。

Result: CTSES的评估结果更可靠、可解释，更符合开发者期望和人类直觉。

Conclusion: CTSES在评估大语言模型自动重构单元测试方面优于现有指标。

Abstract: Large Language Models (LLMs) are increasingly employed to automatically
refactor unit tests, aiming to enhance readability, naming, and structural
clarity while preserving functional behavior. However, evaluating such
refactorings remains challenging: traditional metrics like CodeBLEU are overly
sensitive to renaming and structural edits, whereas embedding-based
similarities capture semantics but ignore readability and modularity. We
introduce CTSES, a composite metric that integrates CodeBLEU, METEOR, and
ROUGE-L to balance behavior preservation, lexical quality, and structural
alignment. CTSES is evaluated on over 5,000 test suites automatically
refactored by GPT-4o and Mistral-Large-2407, using Chain-of-Thought prompting,
across two established Java benchmarks: Defects4J and SF110. Our results show
that CTSES yields more faithful and interpretable assessments, better aligned
with developer expectations and human intuition than existing metrics.

</details>


### [329] [Is Your Training Pipeline Production-Ready? A Case Study in the Healthcare Domain](https://arxiv.org/abs/2506.06946)
*Daniel Lawand,Lucas Quaresma,Roberto Bolgheroni,Alfredo Goldman,Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: 本文围绕SPIRA项目，探讨将机器学习训练管道投入生产的挑战，对比连续训练子系统的三种架构版本，为相关人员提供见解。


<details>
  <summary>Details</summary>
Motivation: 解决将机器学习训练管道部署到生产中缺乏稳健软件工程实践的问题，以SPIRA项目创建的ML - Enabled System为背景。

Method: 对比SPIRA连续训练子系统从大泥球架构到模块化单体架构再到微服务架构的三种版本，采用不同设计原则和模式。

Result: 无明确提及具体结果。

Conclusion: 为负责将ML训练管道投入生产的ML工程师和寻求采用MLOps实践的数据科学家提供见解。

Abstract: Deploying a Machine Learning (ML) training pipeline into production requires
robust software engineering practices. This differs significantly from
experimental workflows. This experience report investigates this challenge in
SPIRA, a project whose goal is to create an ML-Enabled System (MLES) to
pre-diagnose insufficiency respiratory via speech analysis. The first version
of SPIRA's training pipeline lacked critical software quality attributes. This
paper presents an overview of the MLES, then compares three versions of the
architecture of the Continuous Training subsystem, which evolved from a Big
Ball of Mud, to a Modular Monolith, towards Microservices. By adopting
different design principles and patterns to enhance its maintainability,
robustness, and extensibility. In this way, the paper seeks to offer insights
for both ML Engineers tasked to productionize ML training pipelines and Data
Scientists seeking to adopt MLOps practices.

</details>


### [330] [Taxonomy of migration scenarios for Qiskit refactoring using LLMs](https://arxiv.org/abs/2506.07135)
*José Manuel Suárez,Luís Mariano Bibbó,Joaquín Bogado,Alejandro Fernandez*

Main category: cs.SE

TL;DR: 本文应对量子编程库重构挑战，用大语言模型（LLMs）对Qiskit不同版本迁移时的重构需求分类，生成统一分类法，为未来研究奠基并助力量子软件工程。


<details>
  <summary>Details</summary>
Motivation: 量子编程库的异构性和不断演变给开发者带来重构挑战，且与经典软件工程不同，LLMs在量子软件工程中的价值待探索。

Method: 开发量子电路重构问题的分类法；用LLMs对不同Qiskit版本迁移时的重构需求分类；对比专家和LLM生成的分类法并整合为统一分类法。

Result: 产生专家和LLM的两种分类法，整合为统一分类法。

Conclusion: 统一分类法为AI辅助迁移的未来研究奠基，能严格评估自动重构技术，有助于提升量子软件开发工作流程、语言兼容性和编程最佳实践。

Abstract: As quantum computing advances, quantum programming libraries' heterogeneity
and steady evolution create new challenges for software developers. Frequent
updates in software libraries break working code that needs to be refactored,
thus adding complexity to an already complex landscape. These refactoring
challenges are, in many cases, fundamentally different from those known in
classical software engineering due to the nature of quantum computing software.
This study addresses these challenges by developing a taxonomy of quantum
circuit's refactoring problems, providing a structured framework to analyze and
compare different refactoring approaches. Large Language Models (LLMs) have
proven valuable tools for classic software development, yet their value in
quantum software engineering remains unexplored. This study uses LLMs to
categorize refactoring needs in migration scenarios between different Qiskit
versions. Qiskit documentation and release notes were scrutinized to create an
initial taxonomy of refactoring required for migrating between Qiskit releases.
Two taxonomies were produced: one by expert developers and one by an LLM. These
taxonomies were compared, analyzing differences and similarities, and were
integrated into a unified taxonomy that reflects the findings of both methods.
By systematically categorizing refactoring challenges in Qiskit, the unified
taxonomy is a foundation for future research on AI-assisted migration while
enabling a more rigorous evaluation of automated refactoring techniques.
Additionally, this work contributes to quantum software engineering (QSE) by
enhancing software development workflows, improving language compatibility, and
promoting best practices in quantum programming.

</details>


### [331] [GUIPilot: A Consistency-based Mobile GUI Testing Approach for Detecting Application-specific Bugs](https://arxiv.org/abs/2506.07385)
*Ruofan Liu,Xiwen Teoh,Yun Lin,Guanjie Chen,Ruofei Ren,Denys Poshyvanyk,Jin Song Dong*

Main category: cs.SE

TL;DR: 提出GUIPilot方法检测移动设计与实现间的不一致性，实验表现优，工业案例检测出应用漏洞。


<details>
  <summary>Details</summary>
Motivation: 检测移动设计（含外观和行为）与实现之间的不一致性。

Method: 将屏幕抽象为小部件容器，把屏幕匹配问题转为可优化的小部件对齐问题；将GUI过渡转换为屏幕操作，用视觉提示让视觉语言模型推断小部件特定操作。

Result: 在80个移动应用和160个设计模型上，检测屏幕不一致性精度94.5%、召回率99.6%，优于GVT；检测流程不一致性零错误；工业案例检测出9个应用漏洞并获专家确认。

Conclusion: GUIPilot在检测移动设计与实现的不一致性方面表现出色，能有效发现应用中的漏洞。

Abstract: In this work, we propose GUIPilot, an approach for detecting inconsistencies
between the mobile design and their implementations. The mobile design usually
consists of design mock-ups that specify (1) the expected screen appearances
(e.g., widget layouts, colors, and shapes) and (2) the expected screen
behaviors, regarding how one screen can transition into another (e.g., labeled
widgets with textual description). Given a design mock-up and the
implementation of its application, GUIPilot reports both their screen
inconsistencies as well as process inconsistencies. On the one hand, GUIPilot
detects the screen inconsistencies by abstracting every screen into a widget
container where each widget is represented by its position, width, height, and
type. By defining the partial order of widgets and the costs of replacing,
inserting, and deleting widgets in a screen, we convert the screen-matching
problem into an optimizable widget alignment problem. On the other hand, we
translate the specified GUI transition into stepwise actions on the mobile
screen (e.g., click, long-press, input text on some widgets). To this end, we
propose a visual prompt for the vision-language model to infer widget-specific
actions on the screen. By this means, we can validate the presence or absence
of expected transitions in the implementation. Our extensive experiments on 80
mobile applications and 160 design mock-ups show that (1) GUIPilot can achieve
94.5% precision and 99.6% recall in detecting screen inconsistencies,
outperforming the state-of-the-art approach, such as GVT, by 66.2% and 56.6%
respectively, and (2) GUIPilot reports zero errors in detecting process
inconsistencies. Furthermore, our industrial case study on applying GUIPilot on
a trading mobile application shows that GUIPilot has detected nine application
bugs, and all the bugs were confirmed by the original application experts.

</details>


### [332] [Generate Realistic Test Scenes for V2X Communication Systems](https://arxiv.org/abs/2506.07419)
*An Guo,Xinyu Gao,Chunrong Fang,Haoxiang Tian,Weisong Sun,Yanzhou Mu,Shuncheng Tang,Lei Ma,Zhenyu Chen*

Main category: cs.SE

TL;DR: 本文设计并实现自动化测试生成工具V2XGen用于V2X协同感知系统，实验证明其能生成逼真测试场景，检测错误行为，还能提升检测精度。


<details>
  <summary>Details</summary>
Motivation: V2X协同感知系统需测试和性能提升，但创建测试场景有挑战，当前测试方法耗时且成本高。

Method: 设计实现V2XGen，采用高保真方法生成合作对象实例并放置在背景数据关键位置，采用适应度引导的V2X场景生成策略。

Result: 实验表明V2XGen能生成逼真测试场景，有效检测不同V2X驾驶条件下的错误行为，用生成场景对被测系统进行再训练可提升平均检测精度，减少遮挡和远距离感知错误。

Conclusion: V2XGen可有效测试V2X协同感知系统，提高系统性能。

Abstract: Accurately perceiving complex driving environments is essential for ensuring
the safe operation of autonomous vehicles. With the tremendous progress in deep
learning and communication technologies, cooperative perception with
Vehicle-to-Everything (V2X) technologies has emerged as a solution to overcome
the limitations of single-agent perception systems in perceiving distant
objects and occlusions. Despite the considerable advancements, V2X cooperative
perception systems require thorough testing and continuous enhancement of
system performance. Given that V2X driving scenes entail intricate
communications with multiple vehicles across various geographic locations,
creating V2X test scenes for these systems poses a significant challenge.
Moreover, current testing methodologies rely on manual data collection and
labeling, which are both time-consuming and costly.
  In this paper, we design and implement V2XGen, an automated testing
generation tool for V2X cooperative perception systems. V2XGen utilizes a
high-fidelity approach to generate realistic cooperative object instances and
strategically place them within the background data in crucial positions.
Furthermore, V2XGen adopts a fitness-guided V2X scene generation strategy for
the transformed scene generation process and improves testing efficiency. We
conduct experiments on V2XGen using multiple cooperative perception systems
with different fusion schemes to assess its performance on various tasks. The
experimental results demonstrate that V2XGen is capable of generating realistic
test scenes and effectively detecting erroneous behaviors in different
V2X-oriented driving conditions. Furthermore, the results validate that
retraining systems under test with the generated scenes can enhance average
detection precision while reducing occlusion and long-range perception errors.

</details>


### [333] [A Framework for Creating Non-Regressive Test Cases via Branch Consistency Analysis Driven by Descriptions](https://arxiv.org/abs/2506.07486)
*Yuxiang Zhang,Pengyu Xue,Zhen Yang,Xiaoxue Ren,Xiang Li,Linhao Wu,Jiancheng Zhao,Xingda Yu*

Main category: cs.SE

TL;DR: 现有自动化测试生成研究多假设目标方法正确，针对有缺陷目标方法测试生成存在问题。本文构建新基准，提出DISTINCT框架，实验表明其在编译成功率、通过率、缺陷检测率和代码覆盖率上均有提升。


<details>
  <summary>Details</summary>
Motivation: 现有自动化测试生成研究多假设目标方法正确，而实际中目标方法可能有缺陷，现有工具生成的测试无法暴露缺陷。

Method: 构建Defects4J - Desc和QuixBugs - Desc两个新基准，每个目标方法配备自然语言描述；提出DISTINCT框架，包含生成器、验证器和分析器三个迭代组件。

Result: 与现有方法相比，DISTINCT在编译成功率上平均提升14.64%，通过率提升6.66%，缺陷检测率显著提高，代码覆盖率也有所提升。

Conclusion: DISTINCT为非回归测试生成设定了新基线，描述驱动推理使大语言模型从追求覆盖率转向有效缺陷检测。

Abstract: Automated test-generation research overwhelmingly assumes the correctness of
focal methods, yet practitioners routinely face non-regression scenarios where
the focal method may be defective. A baseline evaluation of EvoSuite and two
leading Large Language Model (LLM)-based generators, namely ChatTester and
ChatUniTest, on defective focal methods reveals that despite achieving up to
83% of branch coverage, none of the generated tests expose defects.
  To resolve this problem, we first construct two new benchmarks, namely
Defects4J-Desc and QuixBugs-Desc, for experiments. In particular, each focal
method is equipped with an extra Natural Language Description (NLD) for code
functionality understanding.
  Subsequently, we propose DISTINCT, a Description-guided, branch-consistency
analysis framework that transforms LLMs into fault-aware test generators.
DISTINCT carries three iterative components: (1) a Generator that derives
initial tests based on the NLDs and the focal method, (2) a Validator that
iteratively fixes uncompilable tests using compiler diagnostics, and (3) an
Analyzer that iteratively aligns test behavior with NLD semantics via
branch-level analysis.
  Extensive experiments confirm the effectiveness of our approach. Compared to
state-of-the-art methods, DISTINCT achieves an average improvement of 14.64% in
Compilation Success Rate (CSR) and 6.66% in Passing Rate (PR) across both
benchmarks. It notably enhances Defect Detection Rate (DDR) on both benchmarks,
with a particularly significant gain of 149.26% observed on Defects4J-Desc. In
terms of code coverage, DISTINCT improves Statement Coverage (SC) by an average
of 3.77% and Branch Coverage (BC) by 5.36%. These results set a new baseline
for non-regressive test generation and highlight how description-driven
reasoning enables LLMs to move beyond coverage chasing toward effective defect
detection.

</details>


### [334] [Large Language Models for Multilingual Vulnerability Detection: How Far Are We?](https://arxiv.org/abs/2506.07503)
*Honglin Shu,Michael Fu,Junji Yu,Dong Wang,Chakkrit Tantithamthavorn,Junjie Chen,Yasutaka Kamei*

Main category: cs.SE

TL;DR: 本文对预训练语言模型（PLMs）和大语言模型（LLMs）在多语言漏洞检测中的有效性进行细粒度实证研究，发现GPT - 4o表现出色，凸显LLMs在该领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要聚焦特定编程语言和函数级检测，未充分探索PLMs和LLMs在多语言和多粒度场景中的优缺点，需开展研究填补空白。

Method: 使用7种编程语言的超30000个真实漏洞修复补丁，系统评估模型在函数级和行级的性能。

Result: 经指令调优和少样本提示增强的GPT - 4o显著优于其他评估模型，LLM方法在检测多语言独特漏洞上表现出色，尤其擅长识别高危漏洞。

Conclusion: 在函数级和行级多语言漏洞检测中采用LLMs潜力巨大，其弥补了PLM方法的不足，此次实证评估凸显了LLMs在解决现实软件安全挑战中的价值。

Abstract: Various deep learning-based approaches utilizing pre-trained language models
(PLMs) have been proposed for automated vulnerability detection. With recent
advancements in large language models (LLMs), several studies have begun
exploring their application to vulnerability detection tasks. However, existing
studies primarily focus on specific programming languages (e.g., C/C++) and
function-level detection, leaving the strengths and weaknesses of PLMs and LLMs
in multilingual and multi-granularity scenarios largely unexplored. To bridge
this gap, we conduct a comprehensive fine-grained empirical study evaluating
the effectiveness of state-of-the-art PLMs and LLMs for multilingual
vulnerability detection. Using over 30,000 real-world vulnerability-fixing
patches across seven programming languages, we systematically assess model
performance at both the function-level and line-level. Our key findings
indicate that GPT-4o, enhanced through instruction tuning and few-shot
prompting, significantly outperforms all other evaluated models, including
CodeT5P. Furthermore, the LLM-based approach demonstrates superior capability
in detecting unique multilingual vulnerabilities, particularly excelling in
identifying the most dangerous and high-severity vulnerabilities. These results
underscore the promising potential of adopting LLMs for multilingual
vulnerability detection at function-level and line-level, revealing their
complementary strengths and substantial improvements over PLM approaches. This
first empirical evaluation of PLMs and LLMs for multilingual vulnerability
detection highlights LLMs' value in addressing real-world software security
challenges.

</details>


### [335] [IntenTest: Stress Testing for Intent Integrity in API-Calling LLM Agents](https://arxiv.org/abs/2506.07524)
*Shiwei Feng,Xiangzhe Xu,Xuan Chen,Kaiyuan Zhang,Syed Yusuf Ahmed,Zian Su,Mingwei Zheng,Xiangyu Zhang*

Main category: cs.SE

TL;DR: 提出API中心压力测试框架IntenTest，可系统揭示大语言模型代理意图完整性违规，实验证明其有效且泛化性好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理常误解用户意图，传统软件测试难以处理自然语言模糊性。

Method: 基于工具包文档生成现实任务，进行针对性突变；提出语义分区；使用轻量级预测器对突变任务排名；维护数据类型感知策略记忆。

Result: 在80个工具包API实验中，IntenTest在暴露错误率和查询效率上显著优于基线，能很好泛化到更强目标模型，适应跨领域API演变。

Conclusion: IntenTest能有效揭示大语言模型代理意图完整性违规，具有良好泛化性和适应性。

Abstract: LLM agents are increasingly deployed to automate real-world tasks by invoking
APIs through natural language instructions. While powerful, they often suffer
from misinterpretation of user intent, leading to the agent's actions that
diverge from the user's intended goal, especially as external toolkits evolve.
Traditional software testing assumes structured inputs and thus falls short in
handling the ambiguity of natural language. We introduce IntenTest, an
API-centric stress testing framework that systematically uncovers intent
integrity violations in LLM agents. Unlike prior work focused on fixed
benchmarks or adversarial inputs, IntenTest generates realistic tasks based on
toolkits' documentation and applies targeted mutations to expose subtle agent
errors while preserving user intent. To guide testing, we propose semantic
partitioning, which organizes natural language tasks into meaningful categories
based on toolkit API parameters and their equivalence classes. Within each
partition, seed tasks are mutated and ranked by a lightweight predictor that
estimates the likelihood of triggering agent errors. To enhance efficiency,
IntenTest maintains a datatype-aware strategy memory that retrieves and adapts
effective mutation patterns from past cases. Experiments on 80 toolkit APIs
demonstrate that IntenTest effectively uncovers intent integrity violations,
significantly outperforming baselines in both error-exposing rate and query
efficiency. Moreover, IntenTest generalizes well to stronger target models
using smaller LLMs for test generation, and adapts to evolving APIs across
domains.

</details>


### [336] [Evaluating LLMs Effectiveness in Detecting and Correcting Test Smells: An Empirical Study](https://arxiv.org/abs/2506.07594)
*E. G. Santana Jr,Jander Pereira Santos Junior,Erlon P. Almeida,Iftekhar Ahmed,Paulo Anselmo da Mota Silveira Neto,Eduardo Santana de Almeida*

Main category: cs.SE

TL;DR: 本文评估了GPT - 4 - Turbo、LLaMA 3 70B和Gemini - 1.5 Pro对Python和Java测试套件中测试异味的检测和重构能力，发现Gemini表现最佳，但自动化重构仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 测试异味影响代码可维护性和可靠性，现有工具主要聚焦检测，LLMs在识别和重构测试异味方面的能力有待探索。

Method: 使用PyNose和TsDetect进行初始异味检测，然后用LLMs进行重构，评估GPT - 4 - Turbo、LLaMA 3 70B和Gemini - 1.5 Pro在Python和Java测试套件上的表现。

Result: Gemini检测准确率最高，所有模型都能重构异味但效果不同，有时会引入新异味，Gemini能提高测试覆盖率，GPT - 4和LLaMA常降低覆盖率。

Conclusion: LLMs在自动化测试异味重构方面有潜力，Gemini表现最强，但不同语言和异味类型仍存在挑战。

Abstract: Test smells indicate poor development practices in test code, reducing
maintainability and reliability. While developers often struggle to prevent or
refactor these issues, existing tools focus primarily on detection rather than
automated refactoring. Large Language Models (LLMs) have shown strong potential
in code understanding and transformation, but their ability to both identify
and refactor test smells remains underexplored. We evaluated GPT-4-Turbo, LLaMA
3 70B, and Gemini-1.5 Pro on Python and Java test suites, using PyNose and
TsDetect for initial smell detection, followed by LLM-driven refactoring.
Gemini achieved the highest detection accuracy (74.35\% Python, 80.32\% Java),
while LLaMA was lowest. All models could refactor smells, but effectiveness
varied, sometimes introducing new smells. Gemini also improved test coverage,
unlike GPT-4 and LLaMA, which often reduced it. These results highlight LLMs'
potential for automated test smell refactoring, with Gemini as the strongest
performer, though challenges remain across languages and smell types.

</details>


### [337] [Leveraging Network Methods for Hub-like Microservice Detection](https://arxiv.org/abs/2506.07683)
*Alexander Bakhtin,Matteo Esposito,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 本文旨在找到检测类中心微服务反模式的方法，利用数据集和多种网络中心检测技术，发现Kirkley基于Erdos - Renyi编码的方法最准确，并指出后续研究方向和工具更新潜力。


<details>
  <summary>Details</summary>
Motivation: 微服务架构流行，但类中心反模式缺乏明确定义和检测方法，需找到能输出合理数量且高精度的类中心候选的检测方法。

Method: 利用25个微服务网络数据集和多种网络中心检测技术，如无标度属性、中心性指标和聚类系数、最小描述长度原则及Arcan工具背后的方法。

Result: 研究的架构网络不是无标度的，多数中心检测方法对检测到的中心不一致，Kirkley基于Erdos - Renyi编码的方法在检测中心数量和精度上最准确。

Conclusion: 对这些方法在微服务及其他系统中检测类中心组件的进一步研究可开辟新方向，结果可用于评估Arcan工具并为其更新提供依据。

Abstract: Context: Microservice Architecture is a popular architectural paradigm that
facilitates flexibility by decomposing applications into small, independently
deployable services. Catalogs of architectural anti-patterns have been proposed
to highlight the negative aspects of flawed microservice design. In particular,
the Hub-like anti-pattern lacks an unambiguous definition and detection method.
Aim: In this work, we aim to find a robust detection approach for the Hub-like
microservice anti-pattern that outputs a reasonable number of Hub-like
candidates with high precision. Method: We leveraged a dataset of 25
microservice networks and several network hub detection techniques to identify
the Hub-like anti-pattern, namely scale-free property, centrality metrics and
clustering coefficient, minimum description length principle, and the approach
behind the Arcan tool. Results and Conclusion: Our findings revealed that the
studied architectural networks are not scale-free, that most considered hub
detection approaches do not agree on the detected hubs, and that the method by
Kirkley leveraging the Erdos-Renyi encoding is the most accurate one in terms
of the number of detected hubs and the detection precision. Investigating
further the applicability of these methods to detecting Hub-like components in
microservice-based and other systems opens up new research directions.
Moreover, our results provide an evaluation of the approach utilized by the
widely used Arcan tool and highlight the potential to update the tool to use
the normalized degree centrality of a component in the network, or for the
approach based on ER encoding to be adopted instead.

</details>


### [338] [Centrality Change Proneness: an Early Indicator of Microservice Architectural Degradation](https://arxiv.org/abs/2506.07690)
*Alexander Bakhtin,Matteo Esposito,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 研究用时间中心性指标关联软件指标以早期检测微服务架构退化，重构项目架构计算指标并分析，发现部分指标有关联，中心性变化倾向无影响。


<details>
  <summary>Details</summary>
Motivation: 微服务架构广泛采用需识别模式和反模式防止架构退化，研究时间中心性指标能否通过关联或影响软件指标实现早期检测。

Method: 重构一个含42个服务的OSS微服务项目的7个版本架构，计算每个版本中每个服务的软件和中心性指标，衍生中心性变化倾向指标，探索指标间相关性。

Result: 确定7个规模和5个复杂度指标与中心性有一致相关性，中心性变化倾向不影响软件指标。

Conclusion: 时间中心性指标为微服务架构退化提供新视角和早期指标。

Abstract: Over the past decade, the wide adoption of Microservice Architecture has
required the identification of various patterns and anti-patterns to prevent
Microservice Architectural Degradation. Frequently, the systems are modelled as
a network of connected services. Recently, the study of temporal networks has
emerged as a way to describe and analyze evolving networks. Previous research
has explored how software metrics such as size, complexity, and quality are
related to microservice centrality in the architectural network. This study
investigates whether temporal centrality metrics can provide insight into the
early detection of architectural degradation by correlating or affecting
software metrics. We reconstructed the architecture of 7 releases of an OSS
microservice project with 42 services. For every service in every release, we
computed the software and centrality metrics. From one of the latter, we
derived a new metric, Centrality Change Proneness. We then explored the
correlation between the metrics. We identified 7 size and 5 complexity metrics
that have a consistent correlation with centrality, while Centrality Change
Proneness did not affect the software metrics, thus providing yet another
perspective and an early indicator of microservice architectural degradation.

</details>


### [339] [Towards a Small Language Model Lifecycle Framework](https://arxiv.org/abs/2506.07695)
*Parsa Miraghaei,Sergio Moreschini,Antti Kolehmainen,David Hästbacka*

Main category: cs.SE

TL;DR: 为小语言模型（SLMs）定义综合生命周期框架，通过调查文献分析相关技术，提出模块化生命周期模型，为SLMs开发维护提供基础。


<details>
  <summary>Details</summary>
Motivation: 高效可部署语言模型需求增长，现有SLMs研究碎片化，缺乏统一生命周期视角，需定义综合生命周期框架。

Method: 对36篇文献进行全面调查，分析和分类与生命周期相关的技术。

Result: 提出模块化生命周期模型，包含主要、可选和跨领域组件，能捕捉各阶段关键联系，支持方法复用等。

Conclusion: 框架为SLMs开发和维护提供连贯基础，弥合理论与实践差距，指导未来研究和工具开发。

Abstract: Background: The growing demand for efficient and deployable language models
has led to increased interest in Small Language Models (SLMs). However,
existing research remains fragmented, lacking a unified lifecycle perspective.
  Objective: This study aims to define a comprehensive lifecycle framework for
SLMs by synthesizing insights from academic literature and practitioner
sources.
  Method: We conducted a comprehensive survey of 36 works, analyzing and
categorizing lifecycle-relevant techniques.
  Results: We propose a modular lifecycle model structured into main, optional,
and cross-cutting components. The model captures key interconnections across
stages, supporting method reuse, co-adaptation, and lifecycle-awareness.
  Conclusion: Our framework provides a coherent foundation for developing and
maintaining SLMs, bridging theory and practice, and guiding future research and
tool development.

</details>


### [340] [Adversarial Attack Classification and Robustness Testing for Large Language Models for Code](https://arxiv.org/abs/2506.07942)
*Yang Liu,Armstrong Foundjem,Foutse Khomh,Heng Li*

Main category: cs.SE

TL;DR: 研究自然语言输入的对抗性扰动对代码大语言模型（LLM4Code）的影响，提出测试框架并强调自然语言在对抗性评估中的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件开发中应用加深，确保其对多样化或对抗性输入的鲁棒性至关重要，此前研究常忽略自然语言在指导代码任务中的作用。

Method: 分析多个项目和数据集，建立对抗性攻击分类法，采用混合方法，结合定量性能指标和定性漏洞分析。

Result: LLM4Code模型在不同扰动类型下鲁棒性不同，句子级攻击效果最差，词级扰动挑战大，字符级效果不一。

Conclusion: 研究提供了测试LLM4Code鲁棒性的框架，强调自然语言在对抗性评估中的关键作用，提高模型对语义级干扰的恢复能力对安全可靠的代码生成系统至关重要。

Abstract: Large Language Models (LLMs) have become vital tools in software development
tasks such as code generation, completion, and analysis. As their integration
into workflows deepens, ensuring robustness against vulnerabilities especially
those triggered by diverse or adversarial inputs becomes increasingly
important. Such vulnerabilities may lead to incorrect or insecure code
generation when models encounter perturbed task descriptions, code, or
comments. Prior research often overlooks the role of natural language in
guiding code tasks. This study investigates how adversarial perturbations in
natural language inputs including prompts, comments, and descriptions affect
LLMs for Code (LLM4Code). It examines the effects of perturbations at the
character, word, and sentence levels to identify the most impactful
vulnerabilities. We analyzed multiple projects (e.g., ReCode, OpenAttack) and
datasets (e.g., HumanEval, MBPP), establishing a taxonomy of adversarial
attacks. The first dimension classifies the input type code, prompts, or
comments while the second dimension focuses on granularity: character, word, or
sentence-level changes. We adopted a mixed-methods approach, combining
quantitative performance metrics with qualitative vulnerability analysis.
LLM4Code models show varying robustness across perturbation types.
Sentence-level attacks were least effective, suggesting models are resilient to
broader contextual changes. In contrast, word-level perturbations posed serious
challenges, exposing semantic vulnerabilities. Character-level effects varied,
showing model sensitivity to subtle syntactic deviations.Our study offers a
structured framework for testing LLM4Code robustness and emphasizes the
critical role of natural language in adversarial evaluation. Improving model
resilience to semantic-level disruptions is essential for secure and reliable
code-generation systems.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [341] [Explaining Risks: Axiomatic Risk Attributions for Financial Models](https://arxiv.org/abs/2506.06653)
*Dangxing Chen*

Main category: q-fin.CP

TL;DR: 用公理化归因方法解释模型预测，提出用扩展Shapley值框架解决风险归因问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型为黑盒结构，在金融等高风险领域，风险与均值预测同样重要，需解决风险归因问题。

Method: 通过扩展Shapley值框架来解决风险归因问题。

Result: 通过分析和实证例子表明，扩展Shapley值框架能很好地分配风险。

Conclusion: 扩展Shapley值框架可有效解决风险归因问题。

Abstract: In recent years, machine learning models have achieved great success at the
expense of highly complex black-box structures. By using axiomatic attribution
methods, we can fairly allocate the contributions of each feature, thus
allowing us to interpret the model predictions. In high-risk sectors such as
finance, risk is just as important as mean predictions. Throughout this work,
we address the following risk attribution problem: how to fairly allocate the
risk given a model with data? We demonstrate with analysis and empirical
examples that risk can be well allocated by extending the Shapley value
framework.

</details>


### [342] [Uncertainty-Aware Strategies: A Model-Agnostic Framework for Robust Financial Optimization through Subsampling](https://arxiv.org/abs/2506.07299)
*Hans Buehler,Blanka Horvath,Yannick Limmer,Thorsten Schmidt*

Main category: q-fin.CP

TL;DR: 本文针对量化金融中模型不确定性挑战，提出叠加不确定性度量、子采样策略和适配随机梯度下降算法，研究表明方法优于传统策略，表现可与贝叶斯方法媲美。


<details>
  <summary>Details</summary>
Motivation: 量化金融中决策依赖有限数据估计随机模型，真实概率测度不可用，小的估计误差会导致决策质量显著偏差。

Method: 在传统目标上叠加“不确定性度量”；提出子采样策略近似模型不确定性；提出适配随机梯度下降算法实现高效并行化。

Result: 不确定性度量优于传统混合测度策略，基于子采样的方法增强了对模型风险的鲁棒性，性能与复杂贝叶斯方法相当。

Conclusion: 提出的方法在解决量化金融模型不确定性问题上有效，能提升决策质量和模型鲁棒性。

Abstract: This paper addresses the challenge of model uncertainty in quantitative
finance, where decisions in portfolio allocation, derivative pricing, and risk
management rely on estimating stochastic models from limited data. In practice,
the unavailability of the true probability measure forces reliance on an
empirical approximation, and even small misestimations can lead to significant
deviations in decision quality. Building on the framework of Klibanoff et al.
(2005), we enhance the conventional objective - whether this is expected
utility in an investing context or a hedging metric - by superimposing an outer
"uncertainty measure", motivated by traditional monetary risk measures, on the
space of models. In scenarios where a natural model distribution is lacking or
Bayesian methods are impractical, we propose an ad hoc subsampling strategy,
analogous to bootstrapping in statistical finance and related to mini-batch
sampling in deep learning, to approximate model uncertainty. To address the
quadratic memory demands of naive implementations, we also present an adapted
stochastic gradient descent algorithm that enables efficient parallelization.
Through analytical, simulated, and empirical studies - including multi-period,
real data and high-dimensional examples - we demonstrate that uncertainty
measures outperform traditional mixture of measures strategies and our
model-agnostic subsampling-based approach not only enhances robustness against
model risk but also achieves performance comparable to more elaborate Bayesian
methods.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [343] [Subgame Perfect Nash Equilibria in Large Reinsurance Markets](https://arxiv.org/abs/2506.07291)
*Maria Andraos,Mario Ghossoub,Michael B. Zhu*

Main category: q-fin.RM

TL;DR: 本文构建包含多保险人与多再保险人的再保险市场模型，分析子博弈完美纳什均衡及其帕累托效率，拓展现有文献结果并给出数值示例。


<details>
  <summary>Details</summary>
Motivation: 为再保险市场的最优性和均衡相关文献提供统一框架并进行拓展。

Method: 将市场建模为序贯博弈，再保险人有先动优势，运用非线性定价规则，通过Choquet积分定价。

Result: 刻画了部分情况下市场的子博弈完美纳什均衡，检验了其帕累托效率，拓展了相关文献模型的结果。

Conclusion: 所建模型能有效分析再保险市场，研究结果对现有文献有拓展作用。

Abstract: We consider a model of a reinsurance market consisting of multiple insurers
on the demand side and multiple reinsurers on the supply side, thereby
providing a unifying framework and extension of the recent literature on
optimality and equilibria in reinsurance markets. Each insurer has preferences
represented by a general Choquet risk measure and can purchase coverage from
any or all reinsurers. Each reinsurer has preferences represented by a general
Choquet risk measure and can provide coverage to any or all insurers. Pricing
in this market is done via a nonlinear pricing rule given by a Choquet
integral. We model the market as a sequential game in which the reinsurers have
the first-move advantage. We characterize the Subgame Perfect Nash Equilibria
in this market in some cases of interest, and we examine their Pareto
efficiency. In addition, we consider two special cases of our model that
correspond to existing models in the related literature, and we show how our
findings extend these previous results. Finally, we illustrate our results in a
numerical example.

</details>


### [344] [Partial comonotonicity and distortion riskmetrics](https://arxiv.org/abs/2506.07472)
*Muqiao Huang*

Main category: q-fin.RM

TL;DR: 建立失真风险度量子类与依赖结构联系，提出部分共单调性概念，其嵌套现有概念，特定部分共单调性通过可加性刻画一类失真风险度量，还可用单点集中刻画预期损失。


<details>
  <summary>Details</summary>
Motivation: 建立失真风险度量子类与依赖结构的联系并确保其可加性。

Method: 提出新的正依赖概念——部分共单调性。

Result: 特定部分共单调性通过可加性唯一刻画一类失真风险度量，能用单点集中刻画预期损失。

Conclusion: 部分共单调性概念有助于研究失真风险度量与依赖结构的关系及风险度量刻画。

Abstract: We establish a connection between subclasses of distortion riskmetrics and
dependence structures, ensuring their additivity. A new notion of positive
dependence, called partial comonotonicity, is developed, which nests the
existing concepts of comonotonicity and single-point concentration. For two
random variables, being comonotonic with a third one does not imply that they
are comonotonic; instead, this defines an instance of partial comonotonicity.
Any specific instance of partial comonotonicity uniquely characterizes a class
of distortion riskmetrics through additivity under this dependence structure.
An implication of this result is the characterization of the Expected Shortfall
using single-point concentration.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [345] [DELPHYNE: A Pre-Trained Model for General and Financial Time Series](https://arxiv.org/abs/2506.06288)
*Xueying Ding,Aakriti Mittal,Achintya Gopal*

Main category: q-fin.ST

TL;DR: 本文指出现有金融时间序列预训练模型存在问题，提出Delphyne模型，该模型在公开数据集上微调几步后有竞争力，在金融任务表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有金融时间序列预训练模型在零样本和微调设置下未比简单金融基准有性能提升，原因是预训练阶段缺乏金融数据及跨领域负迁移效应，且时间序列数据难建模。

Method: 引入预训练模型Delphyne。

Result: Delphyne在公开数据集上只需少量微调步骤就能达到与现有基础和全样本模型相当的性能，在各种金融任务中表现更优。

Conclusion: Delphyne能有效解决现有金融时间序列预训练模型存在的问题。

Abstract: Time-series data is a vital modality within data science communities. This is
particularly valuable in financial applications, where it helps in detecting
patterns, understanding market behavior, and making informed decisions based on
historical data. Recent advances in language modeling have led to the rise of
time-series pre-trained models that are trained on vast collections of datasets
and applied to diverse tasks across financial domains. However, across
financial applications, existing time-series pre-trained models have not shown
boosts in performance over simple finance benchmarks in both zero-shot and
fine-tuning settings. This phenomenon occurs because of a i) lack of financial
data within the pre-training stage, and ii) the negative transfer effect due to
inherently different time-series patterns across domains. Furthermore,
time-series data is continuous, noisy, and can be collected at varying
frequencies and with varying lags across different variables, making this data
more challenging to model than languages. To address the above problems, we
introduce a Pre-trained MoDEL for FINance TimE-series (Delphyne). Delphyne
achieves competitive performance to existing foundation and full-shot models
with few fine-tuning steps on publicly available datasets, and also shows
superior performances on various financial tasks.

</details>


### [346] [A Sinusoidal Hull-White Model for Interest Rate Dynamics: Capturing Long-Term Periodicity in U.S. Treasury Yields](https://arxiv.org/abs/2506.06317)
*Amit Kumar Jha*

Main category: q-fin.ST

TL;DR: 为解决传统Hull - White短期利率模型无法捕捉利率周期性波动问题，提出含时变均值回归速度的扩展模型，经校准和模拟显示改进效果，对债券定价等有意义。


<details>
  <summary>Details</summary>
Motivation: 传统Hull - White短期利率模型无法充分捕捉利率周期性波动，尤其是数十年的长期经济周期。

Method: 提出含正弦时变均值回归速度的扩展模型，用1990年1月至2022年12月美国国债收益率曲线数据，通过Nelder - Mead优化算法校准，进行蒙特卡罗模拟。

Result: 所提模型下30年期零息债券价格为0.43，标准Hull - White模型为0.47，对应均方根误差分别为0.12%和0.14%，拟合效果有改善。

Conclusion: 模型增强了捕捉长期收益率动态的能力，对债券定价、利率风险管理和利率衍生品估值有重要意义，为相关研究开辟新途径。

Abstract: This study is motivated by empirical observations of periodic fluctuations in
interest rates, notably long-term economic cycles spanning decades, which the
conventional Hull-White short-rate model fails to adequately capture. To
address this limitation, we propose an extension that incorporates a
sinusoidal, time-varying mean reversion speed, allowing the model to reflect
cyclic interest rate dynamics more effectively.
  The model is calibrated using a comprehensive dataset of daily U.S. Treasury
yield curves obtained from the Federal Reserve Economic Data (FRED) database,
covering the period from January 1990 to December 2022. The dataset includes
tenors of 1, 2, 3, 5, 7, 10, 20, and 30 years, with the most recent yields
ranging from 1.22% (1-year) to 2.36% (30-year).
  Calibration is performed using the Nelder-Mead optimization algorithm, and
Monte Carlo simulations with 200 paths and a time step of 0.05 years. The
resulting 30-year zero-coupon bond price under the proposed model is 0.43,
compared to 0.47 under the standard Hull-White model. This corresponds to root
mean squared errors of 0.12% and 0.14%, respectively, indicating a noticeable
improvement in fit, particularly for longer maturities.
  These results highlight the model's enhanced capability to capture long-term
yield dynamics and suggest significant implications for bond pricing, interest
rate risk management, and the valuation of interest rate derivatives. The
findings also open avenues for further research into stochastic periodicity and
alternative interest rate modeling frameworks.

</details>


### [347] [The Hype Index: an NLP-driven Measure of Market News Attention](https://arxiv.org/abs/2506.06329)
*Zheng Cao,Wanchaloem Wunkaew,Helyette Geman*

Main category: q-fin.ST

TL;DR: 本文引入Hype Index量化大盘股媒体关注度，用NLP从财经新闻提取信号，构建两种指数并多维度评估，发现其对股票分析等有价值。


<details>
  <summary>Details</summary>
Motivation: 量化媒体对大盘股的关注，从财经新闻中提取预测信号。

Method: 以标准普尔100指数为研究对象，构建基于新闻数量的Hype Index和市值调整的Hype Index，在股票和行业层面计算这两种指数，并从多个角度评估。

Result: 完成两种Hype Index的构建和多维度评估。

Conclusion: Hype Index家族为股票波动分析、市场信号和金融领域NLP扩展提供了有价值的工具。

Abstract: This paper introduces the Hype Index as a novel metric to quantify media
attention toward large-cap equities, leveraging advances in Natural Language
Processing (NLP) for extracting predictive signals from financial news. Using
the S&P 100 as the focus universe, we first construct a News Count-Based Hype
Index, which measures relative media exposure by computing the share of news
articles referencing each stock or sector. We then extend it to the
Capitalization Adjusted Hype Index, adjusts for economic size by taking the
ratio of a stock's or sector's media weight to its market capitalization weight
within its industry or sector. We compute both versions of the Hype Index at
the stock and sector levels, and evaluate them through multiple lenses: (1)
their classification into different hype groups, (2) their associations with
returns, volatility, and VIX index at various lags, (3) their signaling power
for short-term market movements, and (4) their empirical properties including
correlations, samplings, and trends. Our findings suggest that the Hype Index
family provides a valuable set of tools for stock volatility analysis, market
signaling, and NLP extensions in Finance.

</details>


### [348] [Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100](https://arxiv.org/abs/2506.06345)
*Sukru Selim Calik,Andac Akyuz,Zeynep Hilal Kilimci,Kerem Colak*

Main category: q-fin.ST

TL;DR: 研究结合基于变压器的时间序列模型与可解释人工智能，预测股票价格，展现了变压器模型强预测能力及可解释机器学习潜力。


<details>
  <summary>Details</summary>
Motivation: 金融素养依赖解读复杂金融数据和使用高级预测工具的能力，需提升股票价格预测的可解释性和准确性。

Method: 结合基于变压器的时间序列模型与可解释人工智能，分析特定银行和指数的日股价，采用多种模型，用技术指标丰富输入特征，用SHAP和LIME技术解释特征影响。

Result: 变压器模型有强预测能力。

Conclusion: 可解释机器学习能让个人做出明智投资决策并积极参与金融市场。

Abstract: Financial literacy is increasingly dependent on the ability to interpret
complex financial data and utilize advanced forecasting tools. In this context,
this study proposes a novel approach that combines transformer-based time
series models with explainable artificial intelligence (XAI) to enhance the
interpretability and accuracy of stock price predictions. The analysis focuses
on the daily stock prices of the five highest-volume banks listed in the
BIST100 index, along with XBANK and XU100 indices, covering the period from
January 2015 to March 2025. Models including DLinear, LTSNet, Vanilla
Transformer, and Time Series Transformer are employed, with input features
enriched by technical indicators. SHAP and LIME techniques are used to provide
transparency into the influence of individual features on model outputs. The
results demonstrate the strong predictive capabilities of transformer models
and highlight the potential of interpretable machine learning to empower
individuals in making informed investment decisions and actively engaging in
financial markets.

</details>


### [349] [An analysis of capital market through the lens of integral transforms: exploring efficient markets and information asymmetry](https://arxiv.org/abs/2506.06350)
*Kiran Sharma,Abhijit Dutta,Rupak Mukherjee*

Main category: q-fin.ST

TL;DR: 本文聚焦金融市场价格形成，指出传统分析方法不足，采用频谱分析分解价格周期以研究印度国家证券交易所价格行为。


<details>
  <summary>Details</summary>
Motivation: 传统分析技术无法有效解读价格周期，不能持续得出有效结果，需要新方法研究信息对价格形成和发现的影响。

Method: 运用物理学数学方法中的频谱分析，对价格周期进行分解。

Result: 文中未提及具体结果。

Conclusion: 文中未提及明确结论。

Abstract: Post Modigliani and Miller (1958), the concept of usage of arbitrage created
a permanent mark on the discourses of financial framework. The arbitrage
process is largely based on information dissemination amongst the stakeholders
operating in the financial market. The advent of the efficient market
Hypothesis draws close to the M&M hypothesis. Giving importance to the
arbitrage process, which effects the price discovery in the stock market. This
divided the market as random and efficient cohort system. The focus was on
which information forms a key factor in deciding the price formation in the
market. However, the conventional techniques of analysis do not permit the
price cycles to be interpreted beyond its singular wave-like cyclical movement.
The apparent cyclic measurement is not coherent as the technical analysis does
not give sustained result. Hence adaption of theories and computation from
mathematical methods of physics ensures that these cycles are decomposed and
the effect of the broken-down cycles is interpreted to understand the overall
effect of information on price formation and discovery. In order to break the
cycle this paper uses spectrum analysis to decompose and understand the
above-said phenomenon in determining the price behavior in National Stock
Exchange of India (NSE).

</details>


### [350] [Towards Competent AI for Fundamental Analysis in Finance: A Benchmark Dataset and Evaluation](https://arxiv.org/abs/2506.07315)
*Zonghan Wu,Junlin Wang,Congyuan Zou,Chenhan Wang,Yilei Shao*

Main category: q-fin.ST

TL;DR: 本文提出聚焦财务报表分析的基准数据集FinAR - Bench，评估大语言模型在生成财务分析报告中的表现。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在金融行业有应用前景，但大语言模型生成基本面分析报告有不准确风险，现有金融基准不能反映其在实际任务中的表现。

Method: 提出FinAR - Bench数据集，将任务拆分为提取关键信息、计算财务指标和应用逻辑推理三步进行评估。

Result: 能清晰了解大语言模型在基本面分析中的优势和局限。

Conclusion: 提供了在实际金融场景中更实用的大语言模型性能评估方法。

Abstract: Generative AI, particularly large language models (LLMs), is beginning to
transform the financial industry by automating tasks and helping to make sense
of complex financial information. One especially promising use case is the
automatic creation of fundamental analysis reports, which are essential for
making informed investment decisions, evaluating credit risks, guiding
corporate mergers, etc. While LLMs attempt to generate these reports from a
single prompt, the risks of inaccuracy are significant. Poor analysis can lead
to misguided investments, regulatory issues, and loss of trust. Existing
financial benchmarks mainly evaluate how well LLMs answer financial questions
but do not reflect performance in real-world tasks like generating financial
analysis reports. In this paper, we propose FinAR-Bench, a solid benchmark
dataset focusing on financial statement analysis, a core competence of
fundamental analysis. To make the evaluation more precise and reliable, we
break this task into three measurable steps: extracting key information,
calculating financial indicators, and applying logical reasoning. This
structured approach allows us to objectively assess how well LLMs perform each
step of the process. Our findings offer a clear understanding of LLMs current
strengths and limitations in fundamental analysis and provide a more practical
way to benchmark their performance in real-world financial settings.

</details>


### [351] [Predicting Realized Variance Out of Sample: Can Anything Beat The Benchmark?](https://arxiv.org/abs/2506.07928)
*Austin Pollok*

Main category: q-fin.ST

TL;DR: 研究日频下已实现波动率与市场波动率预期差异对个股期权的预测，发现改进预测误差测量可提升投资组合表现。


<details>
  <summary>Details</summary>
Motivation: 不清楚已实现波动率与市场波动率预期差异对个股期权的预测能力如何依赖于对公司层面波动率的预测能力。

Method: 使用高维机器学习模型和低维因子模型进行研究。

Result: 对标准预测误差测量的边际改进可带来投资组合表现的显著经济收益。

Conclusion: 有必要重新思考用于构建投资组合的模型训练方式。

Abstract: The discrepancy between realized volatility and the market's view of
volatility has been known to predict individual equity options at the monthly
horizon. It is not clear how this predictability depends on a forecast's
ability to predict firm-level volatility. We consider this phenomenon at the
daily frequency using high-dimensional machine learning models, as well as
low-dimensional factor models. We find that marginal improvements to standard
forecast error measurements can lead to economically significant gains in
portfolio performance. This makes a case for re-imagining the way we train
models that are used to construct portfolios.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [352] [The Subtle Interplay between Square-root Impact, Order Imbalance \& Volatility: A Unifying Framework](https://arxiv.org/abs/2506.07711)
*Guillaume Maitrier,Jean-Philippe Bouchaud*

Main category: q-fin.TR

TL;DR: 本文提出新理论框架描述元订单，解释市场微观结构矛盾现象，理论与数据相符，支持订单驱动的过度波动理论。


<details>
  <summary>Details</summary>
Motivation: 调和市场微观结构中几个看似矛盾的观察结果，如元订单影响的平方根定律与价格随机游走性质及订单失衡线性影响的兼容性，以及价格波动是否可完全由无信息元订单解释。

Method: 引入新理论框架描述不同特征的元订单，考虑单个交易成交量波动影响，引入两个与成交量相关的指数。

Result: 预测幂律对参数a的非单调依赖及价格变化与成交量失衡相关性有最大值，与实证数据相符。

Conclusion: 框架正确捕捉价格形成的基本机制，支持订单驱动的过度波动理论，与基本面因素占金融市场波动较大份额的观点相悖。

Abstract: In this work, we aim to reconcile several apparently contradictory
observations in market microstructure: is the famous ''square-root law'' of
metaorder impact that decays with time compatible with the random-walk nature
of prices and the linear impact of order imbalances? Can one entirely explain
the volatility of prices as resulting from the flow of uninformed metaorders
that mechanically impact prices? We introduce a new theoretical framework to
describe metaorders with different signs, sizes and durations, which all impact
prices as a square-root of volume but with a subsequent time decay. We show
that, as in the original propagator model, price diffusion is ensured by the
long memory of cross-correlations between metaorders. In order to account for
the effect of strongly fluctuating volumes $q$ of individual trades, we further
introduce two $q$-dependent exponents, which allows us to account for the way
the moments of generalized volume imbalance and the correlation between price
changes and generalized order flow imbalance scales with $T$. We predict in
particular that the corresponding power-laws depend in a non-monotonic fashion
on a parameter $a$ that allows one to put the same weight on all child orders
or overweight large orders, a behaviour clearly borne out by empirical data. We
also predict that the correlation between price changes and volume imbalances
should display a maximum as a function of $a$, which again matches
observations. Such noteworthy agreement between theory and data suggests that
our framework correctly captures the basic mechanism at the heart of price
formation, namely the average impact of metaorders. We argue that our results
support the ''Order-Driven'' theory of excess volatility, and are at odds with
the idea that a ''Fundamental'' component accounts for a large share of the
volatility of financial markets.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [353] [On the Fundamental Impossibility of Hallucination Control in Large Language Models](https://arxiv.org/abs/2506.06382)
*Michał P. Karpowicz*

Main category: stat.ML

TL;DR: 论文解释大语言模型无法避免幻觉的原因及权衡，提出不可能定理。


<details>
  <summary>Details</summary>
Motivation: 解释大语言模型无法避免幻觉的原因及应考虑的权衡。

Method: 将大语言模型推理建模为想法拍卖，用Green - Laffont定理证明不可能定理。

Result: 证明没有推理机制能同时满足四个基本属性。

Conclusion: 该数学框架为理解推理过程本质提供基础，对模型架构、训练目标和评估方法有意义。

Abstract: This paper explains \textbf{why it is impossible to create large language
models that do not hallucinate and what are the trade-offs we should be looking
for}. It presents a formal \textbf{impossibility theorem} demonstrating that no
inference mechanism can simultaneously satisfy four fundamental properties:
\textbf{truthful (non-hallucinatory) generation, semantic information
conservation, relevant knowledge revelation, and knowledge-constrained
optimality}. By modeling LLM inference as an \textbf{auction of ideas} where
neural components compete to contribute to responses, we prove the
impossibility using the Green-Laffont theorem. That mathematical framework
provides a rigorous foundation for understanding the nature of inference
process, with implications for model architecture, training objectives, and
evaluation methods.

</details>


### [354] [Direct Fisher Score Estimation for Likelihood Maximization](https://arxiv.org/abs/2506.06542)
*Sherman Khoo,Yakun Wang,Song Liu,Mark Beaumont*

Main category: stat.ML

TL;DR: 研究似然函数难处理但模型模拟可用时的似然最大化问题，提出基于局部得分匹配的顺序梯度优化方法，有理论保证，实证表现优。


<details>
  <summary>Details</summary>
Motivation: 解决似然函数难处理情况下的似然最大化问题。

Method: 提出基于局部得分匹配技术、对Fisher得分直接建模的顺序梯度优化方法，采用线性参数化得到闭式最小二乘解。

Result: 该方法能快速、灵活且高效地近似Fisher得分，平滑似然目标，减轻复杂似然景观带来的挑战；实证显示比现有基准表现更优。

Conclusion: 提出的方法在处理难处理似然函数的似然最大化问题上有效且性能优越。

Abstract: We study the problem of likelihood maximization when the likelihood function
is intractable but model simulations are readily available. We propose a
sequential, gradient-based optimization method that directly models the Fisher
score based on a local score matching technique which uses simulations from a
localized region around each parameter iterate. By employing a linear
parameterization to the surrogate score model, our technique admits a
closed-form, least-squares solution. This approach yields a fast, flexible, and
efficient approximation to the Fisher score, effectively smoothing the
likelihood objective and mitigating the challenges posed by complex likelihood
landscapes. We provide theoretical guarantees for our score estimator,
including bounds on the bias introduced by the smoothing. Empirical results on
a range of synthetic and real-world problems demonstrate the superior
performance of our method compared to existing benchmarks.

</details>


### [355] [Robust Learnability of Sample-Compressible Distributions under Noisy or Adversarial Perturbations](https://arxiv.org/abs/2506.06613)
*Arefe Boushehrian,Amir Najafi*

Main category: stat.ML

TL;DR: 本文研究样本可压缩分布族在数据扰动下的可学习性，分析两种扰动模型，开发扰动量化框架，给出样本复杂度界，解决两个公开问题。


<details>
  <summary>Details</summary>
Motivation: 此前虽发现样本可压缩性保证PAC可学习性并推动样本复杂度界研究，但未知样本可压缩分布族在扰动样本下的可学习性，且有猜想待验证。

Method: 分析两种数据扰动模型，开发与压缩方案自然衔接的扰动量化框架。

Result: 得出样本复杂度界能随噪声水平和损坏预算优雅缩放，解决学习高维均匀分布有限混合和从对抗损坏样本学习高斯混合模型两个公开问题。

Conclusion: 样本可压缩分布族在满足一定条件下，从扰动样本中仍可学习。

Abstract: Learning distribution families over $\mathbb{R}^d$ is a fundamental problem
in unsupervised learning and statistics. A central question in this setting is
whether a given family of distributions possesses sufficient structure to be
(at least) information-theoretically learnable and, if so, to characterize its
sample complexity. In 2018, Ashtiani et al. reframed \emph{sample
compressibility}, originally due to Littlestone and Warmuth (1986), as a
structural property of distribution classes, proving that it guarantees
PAC-learnability. This discovery subsequently enabled a series of recent
advancements in deriving nearly tight sample complexity bounds for various
high-dimensional open problems. It has been further conjectured that the
converse also holds: every learnable class admits a tight sample compression
scheme.
  In this work, we establish that sample compressible families remain learnable
even from perturbed samples, subject to a set of necessary and sufficient
conditions. We analyze two models of data perturbation: (i) an additive
independent noise model, and (ii) an adversarial corruption model, where an
adversary manipulates a limited subset of the samples unknown to the learner.
Our results are general and rely on as minimal assumptions as possible. We
develop a perturbation-quantization framework that interfaces naturally with
the compression scheme and leads to sample complexity bounds that scale
gracefully with the noise level and corruption budget. As concrete
applications, we establish new sample complexity bounds for learning finite
mixtures of high-dimensional uniform distributions under both noise and
adversarial perturbations, as well as for learning Gaussian mixture models from
adversarially corrupted samples, resolving two open problems in the literature.

</details>


### [356] [Continuous Semi-Implicit Models](https://arxiv.org/abs/2506.06778)
*Longlin Yu,Jiajun Zha,Tong Yang,Tianyu Xie,Xiangyu Zhang,S. -H. Gary Chan,Cheng Zhang*

Main category: stat.ML

TL;DR: 本文提出连续半隐式模型CoSIM，可高效无模拟训练，实现分布级多步蒸馏，图像生成实验表现佳。


<details>
  <summary>Details</summary>
Motivation: 分层半隐式模型顺序训练收敛慢，需要改进。

Method: 将分层半隐式模型扩展到连续框架，引入连续转移核。

Result: CoSIM能高效无模拟训练，实现分布级一致性，图像生成实验表现与现有扩散模型加速方法相当或更好，在FD - DINOv2上性能优越。

Conclusion: CoSIM是一种有效且性能优越的用于生成模型加速的方法。

Abstract: Semi-implicit distributions have shown great promise in variational inference
and generative modeling. Hierarchical semi-implicit models, which stack
multiple semi-implicit layers, enhance the expressiveness of semi-implicit
distributions and can be used to accelerate diffusion models given pretrained
score networks. However, their sequential training often suffers from slow
convergence. In this paper, we introduce CoSIM, a continuous semi-implicit
model that extends hierarchical semi-implicit models into a continuous
framework. By incorporating a continuous transition kernel, CoSIM enables
efficient, simulation-free training. Furthermore, we show that CoSIM achieves
consistency with a carefully designed transition kernel, offering a novel
approach for multistep distillation of generative models at the distributional
level. Extensive experiments on image generation demonstrate that CoSIM
performs on par or better than existing diffusion model acceleration methods,
achieving superior performance on FD-DINOv2.

</details>


### [357] [The Currents of Conflict: Decomposing Conflict Trends with Gaussian Processes](https://arxiv.org/abs/2506.06828)
*Simon P. von der Maase*

Main category: stat.ML

TL;DR: 提出一种估计暴力冲突时空模式的新方法，利用冲突事件数据和高斯过程估计趋势，可用于研究冲突现象、控制变量和预测未来冲突，仅用单一数据源。


<details>
  <summary>Details</summary>
Motivation: 找到一种有效估计暴力冲突时空模式的方法，以研究冲突现象并进行冲突预测。

Method: 使用高度时空分解的冲突事件数据与高斯过程相结合。

Result: 能估计时空冲突趋势，可用于研究、控制变量和预测未来冲突。

Conclusion: 通过相对简约的框架，仅使用过去冲突模式这一数据源就实现了上述成果。

Abstract: I present a novel approach to estimating the temporal and spatial patterns of
violent conflict. I show how we can use highly temporally and spatially
disaggregated data on conflict events in tandem with Gaussian processes to
estimate temporospatial conflict trends. These trends can be studied to gain
insight into conflict traps, diffusion and tempo-spatial conflict exposure in
general; they can also be used to control for such phenomenons given other
estimation tasks; lastly, the approach allow us to extrapolate the estimated
tempo-spatial conflict patterns into future temporal units, thus facilitating
powerful, stat-of-the-art, conflict forecasts. Importantly, these results are
achieved via a relatively parsimonious framework using only one data source:
past conflict patterns.

</details>


### [358] [A Statistical Framework for Model Selection in LSTM Networks](https://arxiv.org/abs/2506.06840)
*Fahad Mostafa*

Main category: stat.ML

TL;DR: 提出用于LSTM网络模型选择的统一统计框架，通过医学数据示例证明其灵活性和性能提升。


<details>
  <summary>Details</summary>
Motivation: LSTM模型选择问题（如超参数调整、架构指定等）仍依赖启发式方法且计算成本高。

Method: 将经典模型选择思想（信息准则、收缩估计）扩展到顺序神经网络，定义适应时间结构的惩罚似然，提出隐藏状态动态的广义阈值方法，用变分贝叶斯和近似边际似然方法进行高效估计。

Result: 通过几个以生物医学数据为中心的示例，表明该框架具有灵活性且性能有所提高。

Conclusion: 所提出的统一统计框架可有效用于LSTM网络的系统模型选择。

Abstract: Long Short-Term Memory (LSTM) neural network models have become the
cornerstone for sequential data modeling in numerous applications, ranging from
natural language processing to time series forecasting. Despite their success,
the problem of model selection, including hyperparameter tuning, architecture
specification, and regularization choice remains largely heuristic and
computationally expensive. In this paper, we propose a unified statistical
framework for systematic model selection in LSTM networks. Our framework
extends classical model selection ideas, such as information criteria and
shrinkage estimation, to sequential neural networks. We define penalized
likelihoods adapted to temporal structures, propose a generalized threshold
approach for hidden state dynamics, and provide efficient estimation strategies
using variational Bayes and approximate marginal likelihood methods. Several
biomedical data centric examples demonstrate the flexibility and improved
performance of the proposed framework.

</details>


### [359] [Half-AVAE: Adversarial-Enhanced Factorized and Structured Encoder-Free VAE for Underdetermined Independent Component Analysis](https://arxiv.org/abs/2506.07011)
*Yuan-Hao Wei,Yan-Jie Sun*

Main category: stat.ML

TL;DR: 研究提出Half - AVAE解决ICA在确定和欠定条件下的问题，实验表明其在欠定条件下优于基线模型，体现了VAEs在变分推理中的灵活性。


<details>
  <summary>Details</summary>
Motivation: 解决传统VAEs在欠定ICA中，当潜在变量数量超过观测信号时的挑战，增强潜在变量的独立性和可解释性。

Method: 基于无编码器的Half - VAE框架构建Half - AVAE，消除显式逆映射，结合对抗网络和外部增强项。

Result: 在合成信号实验中，Half - AVAE在欠定条件下恢复独立分量方面优于基线模型，均方根误差更低。

Conclusion: 编码器省略结合对抗训练和结构化先验，能有效解决复杂ICA任务，推动相关应用发展。

Abstract: This study advances the Variational Autoencoder (VAE) framework by addressing
challenges in Independent Component Analysis (ICA) under both determined and
underdetermined conditions, focusing on enhancing the independence and
interpretability of latent variables. Traditional VAEs map observed data to
latent variables and back via an encoder-decoder architecture, but struggle
with underdetermined ICA where the number of latent variables exceeds observed
signals. The proposed Half Adversarial VAE (Half-AVAE) builds on the
encoder-free Half-VAE framework, eliminating explicit inverse mapping to tackle
underdetermined scenarios. By integrating adversarial networks and External
Enhancement (EE) terms, Half-AVAE promotes mutual independence among latent
dimensions, achieving factorized and interpretable representations. Experiments
with synthetic signals demonstrate that Half-AVAE outperforms baseline models,
including GP-AVAE and Half-VAE, in recovering independent components under
underdetermined conditions, as evidenced by lower root mean square errors. The
study highlights the flexibility of VAEs in variational inference, showing that
encoder omission, combined with adversarial training and structured priors,
enables effective solutions for complex ICA tasks, advancing applications in
disentanglement, causal inference, and generative modeling.

</details>


### [360] [Quantile-Optimal Policy Learning under Unmeasured Confounding](https://arxiv.org/abs/2506.07140)
*Zhongren Chen,Siyu Chen,Zhengling Qi,Xiaohong Chen,Zhuoran Yang*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study quantile-optimal policy learning where the goal is to find a policy
whose reward distribution has the largest $\alpha$-quantile for some $\alpha
\in (0, 1)$. We focus on the offline setting whose generating process involves
unobserved confounders. Such a problem suffers from three main challenges: (i)
nonlinearity of the quantile objective as a functional of the reward
distribution, (ii) unobserved confounding issue, and (iii) insufficient
coverage of the offline dataset. To address these challenges, we propose a
suite of causal-assisted policy learning methods that provably enjoy strong
theoretical guarantees under mild conditions. In particular, to address (i) and
(ii), using causal inference tools such as instrumental variables and negative
controls, we propose to estimate the quantile objectives by solving nonlinear
functional integral equations. Then we adopt a minimax estimation approach with
nonparametric models to solve these integral equations, and propose to
construct conservative policy estimates that address (iii). The final policy is
the one that maximizes these pessimistic estimates. In addition, we propose a
novel regularized policy learning method that is more amenable to computation.
Finally, we prove that the policies learned by these methods are
$\tilde{\mathscr{O}}(n^{-1/2})$ quantile-optimal under a mild coverage
assumption on the offline dataset. Here, $\tilde{\mathscr{O}}(\cdot)$ omits
poly-logarithmic factors. To the best of our knowledge, we propose the first
sample-efficient policy learning algorithms for estimating the quantile-optimal
policy when there exist unmeasured confounding.

</details>


### [361] [ALINE: Joint Amortization for Bayesian Inference and Active Data Acquisition](https://arxiv.org/abs/2506.07259)
*Daolang Huang,Xinyi Wen,Ayush Bharti,Samuel Kaski,Luigi Acerbi*

Main category: stat.ML

TL;DR: 提出ALINE框架解决即时推理和数据采集问题，经实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 许多关键应用需要能采集信息数据并即时推理的系统，现有方法在新数据即时推理任务中并非最优。

Method: 引入ALINE框架，利用基于强化学习训练的transformer架构，奖励基于自身推理组件的信息增益估计。

Result: 在回归主动学习、贝叶斯实验设计基准和心理测量模型上，ALINE能实现即时准确推理和有效选择信息点。

Conclusion: ALINE是一个有效的统一框架，可用于贝叶斯推理和主动数据采集。

Abstract: Many critical applications, from autonomous scientific discovery to
personalized medicine, demand systems that can both strategically acquire the
most informative data and instantaneously perform inference based upon it.
While amortized methods for Bayesian inference and experimental design offer
part of the solution, neither approach is optimal in the most general and
challenging task, where new data needs to be collected for instant inference.
To tackle this issue, we introduce the Amortized Active Learning and Inference
Engine (ALINE), a unified framework for amortized Bayesian inference and active
data acquisition. ALINE leverages a transformer architecture trained via
reinforcement learning with a reward based on self-estimated information gain
provided by its own integrated inference component. This allows it to
strategically query informative data points while simultaneously refining its
predictions. Moreover, ALINE can selectively direct its querying strategy
towards specific subsets of model parameters or designated predictive tasks,
optimizing for posterior estimation, data prediction, or a mixture thereof.
Empirical results on regression-based active learning, classical Bayesian
experimental design benchmarks, and a psychometric model with selectively
targeted parameters demonstrate that ALINE delivers both instant and accurate
inference along with efficient selection of informative points.

</details>


### [362] [Rao-Blackwellised Reparameterisation Gradients](https://arxiv.org/abs/2506.07687)
*Kevin Lam,Thang Bui,George Deligiannidis,Yee Whye Teh*

Main category: stat.ML

TL;DR: 提出R2 - G2估计器作为重参数化梯度估计器的Rao - Blackwell化，表明贝叶斯多层感知器的局部重参数化梯度估计器是其特例，初始训练用R2 - G2在多应用重参数化技巧的模型中表现更好。


<details>
  <summary>Details</summary>
Motivation: 潜在高斯变量在概率机器学习中很流行，梯度估计器用于含潜在高斯变量模型的基于梯度的优化，重参数化技巧常作为默认估计器，本文旨在提出新的估计器并拓展Rao - Blackwell化梯度的好处。

Method: 提出R2 - G2估计器，将其作为重参数化梯度估计器的Rao - Blackwell化。

Result: 表明贝叶斯多层感知器的局部重参数化梯度估计器是R2 - G2估计器和Rao - Blackwell化的一个实例，初始训练用R2 - G2在多应用重参数化技巧的模型中表现更好。

Conclusion: R2 - G2估计器能将Rao - Blackwell化梯度的好处拓展到一系列概率模型，在多应用重参数化技巧的模型初始训练中有更好表现。

Abstract: Latent Gaussian variables have been popularised in probabilistic machine
learning. In turn, gradient estimators are the machinery that facilitates
gradient-based optimisation for models with latent Gaussian variables. The
reparameterisation trick is often used as the default estimator as it is simple
to implement and yields low-variance gradients for variational inference. In
this work, we propose the R2-G2 estimator as the Rao-Blackwellisation of the
reparameterisation gradient estimator. Interestingly, we show that the local
reparameterisation gradient estimator for Bayesian MLPs is an instance of the
R2-G2 estimator and Rao-Blackwellisation. This lets us extend benefits of
Rao-Blackwellised gradients to a suite of probabilistic models. We show that
initial training with R2-G2 consistently yields better performance in models
with multiple applications of the reparameterisation trick.

</details>


### [363] [Quickest Causal Change Point Detection by Adaptive Intervention](https://arxiv.org/abs/2506.07760)
*Haijie Xu,Chen Zhang*

Main category: stat.ML

TL;DR: 提出考虑干预的线性因果模型变点监测算法，有集中变化维度、选干预节点和值等方法，理论证明最优性并通过模拟和真实案例验证。


<details>
  <summary>Details</summary>
Motivation: 提出考虑干预的线性因果模型变点监测算法。

Method: 采用特殊中心化技术集中变化到单维；基于Kullback - Leibler散度选干预节点；给出选干预值算法；提出两种有自适应干预策略的监测方法。

Result: 理论上证明了所提方法的一阶最优性，用模拟数据集和两个真实案例验证了其性质。

Conclusion: 所提考虑干预的线性因果模型变点监测算法有效可行。

Abstract: We propose an algorithm for change point monitoring in linear causal models
that accounts for interventions. Through a special centralization technique, we
can concentrate the changes arising from causal propagation across nodes into a
single dimension. Additionally, by selecting appropriate intervention nodes
based on Kullback-Leibler divergence, we can amplify the change magnitude. We
also present an algorithm for selecting the intervention values, which aids in
the identification of the most effective intervention nodes. Two monitoring
methods are proposed, each with an adaptive intervention policy to make a
balance between exploration and exploitation. We theoretically demonstrate the
first-order optimality of the proposed methods and validate their properties
using simulation datasets and two real-world case studies.

</details>


### [364] [Generalization Analysis for Bayesian Optimal Experiment Design under Model Misspecification](https://arxiv.org/abs/2506.07805)
*Roubing Tang,Sabina J. Sloman,Samuel Kaski*

Main category: stat.ML

TL;DR: 本文聚焦BOED中模型误设下的泛化误差问题，给出误差分解，实证分析训练数据影响，提出新采集函数，实验表明新方法优于传统BOED。


<details>
  <summary>Details</summary>
Motivation: BOED存在协变量偏移问题，模型误设下协变量偏移会放大泛化误差，需研究泛化误差来源及应对方法。

Method: 对泛化误差进行数学分解，开展详细实证分析，开发包含代表性项并隐式诱导去放大的新采集函数。

Result: 新方法在模型误设情况下优于传统BOED。

Conclusion: 新的采集函数能减轻模型误设的影响，提高泛化性能。

Abstract: In many settings in science and industry, such as drug discovery and clinical
trials, a central challenge is designing experiments under time and budget
constraints. Bayesian Optimal Experimental Design (BOED) is a paradigm to pick
maximally informative designs that has been increasingly applied to such
problems. During training, BOED selects inputs according to a pre-determined
acquisition criterion. During testing, the model learned during training
encounters a naturally occurring distribution of test samples. This leads to an
instance of covariate shift, where the train and test samples are drawn from
different distributions. Prior work has shown that in the presence of model
misspecification, covariate shift amplifies generalization error. Our first
contribution is to provide a mathematical decomposition of generalization error
that reveals key contributors to generalization error in the presence of model
misspecification. We show that generalization error under misspecification is
the result of, in addition to covariate shift, a phenomenon we term error
(de-)amplification which has not been identified or studied in prior work. Our
second contribution is to provide a detailed empirical analysis to show that
methods that result in representative and de-amplifying training data increase
generalization performance. Our third contribution is to develop a novel
acquisition function that mitigates the effects of model misspecification by
including a term for representativeness and implicitly inducing
de-amplification. Our experimental results demonstrate that our method
outperforms traditional BOED in the presence of misspecification.

</details>


### [365] [Accelerating Constrained Sampling: A Large Deviations Approach](https://arxiv.org/abs/2506.07816)
*Yingli Wang,Changwei Tu,Xiaoyu Wang,Lingjiong Zhu*

Main category: stat.ML

TL;DR: 研究受限域目标概率分布采样中SRNLD的长期行为，建立LDP，证明特定斜对称矩阵选择可加速收敛并经数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有文献未明确如何设计SRNLD动力学中的斜对称矩阵以获良好性能。

Method: 对SRNLD经验测度建立大偏差原理（LDP），明确刻画速率函数。

Result: 证明特定斜对称矩阵选择下SRNLD比RLD能加速收敛到目标分布，数值实验验证理论结果。

Conclusion: 特定斜对称矩阵选择可让SRNLD在受限采样中表现更优，理论分析与数值实验相符。

Abstract: The problem of sampling a target probability distribution on a constrained
domain arises in many applications including machine learning. For constrained
sampling, various Langevin algorithms such as projected Langevin Monte Carlo
(PLMC) based on the discretization of reflected Langevin dynamics (RLD) and
more generally skew-reflected non-reversible Langevin Monte Carlo (SRNLMC)
based on the discretization of skew-reflected non-reversible Langevin dynamics
(SRNLD) have been proposed and studied in the literature. This work focuses on
the long-time behavior of SRNLD, where a skew-symmetric matrix is added to RLD.
Although the non-asymptotic convergence analysis for SRNLD (and SRNLMC) and the
acceleration compared to RLD (and PMLC) have been studied in the literature, it
is not clear how one should design the skew-symmetric matrix in the dynamics to
achieve good performance in practice. We establish a large deviation principle
(LDP) for the empirical measure of SRNLD when the skew-symmetric matrix is
chosen such that its product with the inward unit normal vector field on the
boundary is zero. By explicitly characterizing the rate functions, we show that
SRNLD can accelerate the convergence to the target distribution compared to RLD
with this choice of the skew-symmetric matrix. Numerical experiments for SRNLMC
based on the proposed skew-symmetric matrix show superior performance which
validate the theoretical findings from the large deviations theory.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [366] [Efficient Mirror-type Kernels for the Metropolis-Hastings Algorithm](https://arxiv.org/abs/2506.06660)
*Nuo Guan,Xiyun Jiao*

Main category: stat.CO

TL;DR: 提出结合Mirror move与MALA的新MH核MirrorMALA，比Mirror和MALA更高效，用Mirror型核及新白化变换方法分析GLMMs，效率比HMC或NUTS高2 - 20倍。


<details>
  <summary>Details</summary>
Motivation: 改进现有算法效率，克服单一算法缺点。

Method: 引入Mirror move到MALA提出新MH核，使用Mirror型核及受Tan和Nott启发的白化变换方法分析GLMMs。

Result: MirrorMALA比Mirror和MALA更高效且计算成本相当，分析GLMMs时单位时间效率比HMC或NUTS高2 - 20倍。

Conclusion: 新提出的MirrorMALA及使用的方法在效率上有显著优势。

Abstract: We propose a new Metropolis-Hastings (MH) kernel by introducing the Mirror
move into the Metropolis adjusted Langevin algorithm (MALA). This new kernel
uses the strength of one kernel to overcome the shortcoming of the other, and
generates proposals that are distant from the current position, but still
within the high-density region of the target distribution. The resulting
algorithm can be much more efficient than both Mirror and MALA, while stays
comparable in terms of computational cost. We demonstrate the advantages of the
MirrorMALA kernel using a variety of one-dimensional and multi-dimensional
examples. The Mirror and MirrorMALA are both special cases of the Mirror-type
kernels, a new suite of efficient MH proposals. We use the Mirror-type kernels,
together with a novel method of doing the whitening transformation on
high-dimensional random variables, which was inspired by Tan and Nott, to
analyse the Bayesian generalized linear mixed models (GLMMs), and obtain the
per-time-unit efficiency that is 2--20 times higher than the HMC or NUTS
algorithm.

</details>


### [367] [Linear Discriminant Analysis with Gradient Optimization on Covariance Inverse](https://arxiv.org/abs/2506.06845)
*Cencheng Shen,Yuexiao Dong*

Main category: stat.CO

TL;DR: 提出LDA-GO方法优化逆协方差矩阵，经实验验证有效


<details>
  <summary>Details</summary>
Motivation: 经典LDA在高维场景因协方差估计不稳定而效果不佳

Method: 通过梯度下降直接优化逆协方差矩阵，采用Cholesky分解参数化，引入低秩扩展和多初始化策略

Result: 通过多元模拟和真实数据实验证明了LDA-GO的有效性

Conclusion: LDA-GO是解决高维场景下LDA问题的有效方法

Abstract: Linear discriminant analysis (LDA) is a fundamental method in statistical
pattern recognition and classification, achieving Bayes optimality under
Gaussian assumptions. However, it is well-known that classical LDA may struggle
in high-dimensional settings due to instability in covariance estimation. In
this work, we propose LDA with gradient optimization (LDA-GO), a new approach
that directly optimizes the inverse covariance matrix via gradient descent. The
algorithm parametrizes the inverse covariance matrix through Cholesky
factorization, incorporates a low-rank extension to reduce computational
complexity, and considers a multiple-initialization strategy, including
identity initialization and warm-starting from the classical LDA estimates. The
effectiveness of LDA-GO is demonstrated through extensive multivariate
simulations and real-data experiments.

</details>


### [368] [The Lasso Distribution: Properties, Sampling Methods, and Applications in Bayesian Lasso Regression](https://arxiv.org/abs/2506.07394)
*Mohammad Javad Davoudabadi,Jonathon Tidswell,Samuel Muller,Garth Tarr,John T. Ormerod*

Main category: stat.CO

TL;DR: 本文引入Lasso分布，推导其性质，给出抽样算法，指出其属于指数族，应用于Gibbs采样器，还提供R包，为Lasso惩罚概率结构提供新见解并改进高维回归贝叶斯推断。


<details>
  <summary>Details</summary>
Motivation: 为Lasso惩罚的概率结构提供新见解，改进高维回归的贝叶斯推断。

Method: 引入Lasso分布，推导其性质，给出随机抽样算法，应用于现有Gibbs采样器，并提供R包实现方法。

Result: 确定Lasso分布属于指数族，得到其矩和矩生成函数的闭式表达式，提出的方法使Gibbs采样更高效且有理论依据。

Conclusion: 研究为Lasso惩罚概率结构提供新见解，在高维回归贝叶斯推断中有实际改进。

Abstract: In this paper, we introduce a new probability distribution, the Lasso
distribution. We derive several fundamental properties of the distribution,
including closed-form expressions for its moments and moment-generating
function. Additionally, we present an efficient and numerically stable
algorithm for generating random samples from the distribution, facilitating its
use in both theoretical and applied settings. We establish that the Lasso
distribution belongs to the exponential family. A direct application of the
Lasso distribution arises in the context of an existing Gibbs sampler, where
the full conditional distribution of each regression coefficient follows this
distribution. This leads to a more computationally efficient and theoretically
grounded sampling scheme. To facilitate the adoption of our methodology, we
provide an R package implementing the proposed methods. Our findings offer new
insights into the probabilistic structure underlying the Lasso penalty and
provide practical improvements in Bayesian inference for high-dimensional
regression problems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [369] [AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition](https://arxiv.org/abs/2506.06566)
*Chen Bao,Chuanbing Huo,Qinyu Chen,Chang Gao*

Main category: eess.AS

TL;DR: 提出基于Whisper - tiny的轻量级失语症特定语音识别框架AS - ASR，实验表明微调模型效果好，为现实世界语言障碍语音识别提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 开发适用于边缘设备低资源部署的失语症特定语音识别框架。

Method: 引入混合训练策略，按不同比例结合标准和失语症语音；采用基于GPT - 4的参考增强方法改进嘈杂的失语症转录。

Result: 微调模型显著优于零样本基线，将失语症语音的WER降低30%以上，同时保持标准语音的性能。

Conclusion: 所提出的框架为现实世界的语言障碍语音识别提供了可扩展、高效的解决方案。

Abstract: This paper proposes AS-ASR, a lightweight aphasia-specific speech recognition
framework based on Whisper-tiny, tailored for low-resource deployment on edge
devices. Our approach introduces a hybrid training strategy that systematically
combines standard and aphasic speech at varying ratios, enabling robust
generalization, and a GPT-4-based reference enhancement method that refines
noisy aphasic transcripts, improving supervision quality. We conduct extensive
experiments across multiple data mixing configurations and evaluation settings.
Results show that our fine-tuned model significantly outperforms the zero-shot
baseline, reducing WER on aphasic speech by over 30% while preserving
performance on standard speech. The proposed framework offers a scalable,
efficient solution for real-world disordered speech recognition.

</details>


### [370] [Neural Spectral Band Generation for Audio Coding](https://arxiv.org/abs/2506.06732)
*Woongjib Choi,Byeong Hyeon Kim,Hyungseob Lim,Inseon Jang,Hong-Goo Kang*

Main category: eess.AS

TL;DR: 本文指出传统SBR处理音频信号有局限，现有DNN方法盲目性导致性能不佳，提出参数化非盲带宽扩展新方法。


<details>
  <summary>Details</summary>
Motivation: 传统SBR存在粗特征提取和重建问题，现有DNN方法盲目性导致性能不理想，需新方法替代传统SBR。

Method: 在音频编码管道前后分别进行基于DNN的边信息提取和带宽扩展的参数化非盲带宽扩展方法。

Result: 未提及具体结果。

Conclusion: 提出新的参数化非盲带宽扩展方法可用于替代传统SBR。

Abstract: Audio bandwidth extension is the task of reconstructing missing high
frequency components of bandwidth-limited audio signals, where bandwidth
limitation is a common issue for audio signals due to several reasons,
including channel capacity and data constraints. While conventional spectral
band replication is a well-established parametric approach to audio bandwidth
extension, the SBR usually entails coarse feature extraction and reconstruction
techniques, which leads to limitations when processing various types of audio
signals. In parallel, numerous deep neural network-based audio bandwidth
extension methods have been proposed. These DNN-based methods are usually
referred to as blind BWE, as these methods do not rely on prior information
extracted from original signals, and only utilize given low frequency band
signals to estimate missing high frequency components. In order to replace
conventional SBR with DNNs, simply adopting existing DNN-based methodologies
results in suboptimal performance due to the blindness of these methods. My
proposed research suggests a new approach to parametric non-blind bandwidth
extension, as DNN-based side information extraction and DNN-based bandwidth
extension are performed only at the front and end of the audio coding pipeline.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [371] [FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity](https://arxiv.org/abs/2506.07865)
*Jinxi Li,Ziyang Song,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: 提出FreeGave从多视图视频学习复杂动态3D场景物理，无需对象先验，实验证明其性能优越且学习到有意义物理运动模式。


<details>
  <summary>Details</summary>
Motivation: 现有利用PDE作为PINN损失或结合物理模拟的工作，难以学习边界复杂物理运动或需对象先验。

Method: 引入物理代码和无散度模块估计每个高斯速度场，不依赖低效PINN损失。

Result: 在三个公开数据集和一个新收集的真实世界数据集上，该方法在未来帧外推和运动分割方面表现优越。

Conclusion: 学习到的物理代码在无人工标签训练下能学习到有意义的3D物理运动模式。

Abstract: In this paper, we aim to model 3D scene geometry, appearance, and the
underlying physics purely from multi-view videos. By applying various governing
PDEs as PINN losses or incorporating physics simulation into neural networks,
existing works often fail to learn complex physical motions at boundaries or
require object priors such as masks or types. In this paper, we propose
FreeGave to learn the physics of complex dynamic 3D scenes without needing any
object priors. The key to our approach is to introduce a physics code followed
by a carefully designed divergence-free module for estimating a per-Gaussian
velocity field, without relying on the inefficient PINN losses. Extensive
experiments on three public datasets and a newly collected challenging
real-world dataset demonstrate the superior performance of our method for
future frame extrapolation and motion segmentation. Most notably, our
investigation into the learned physics codes reveals that they truly learn
meaningful 3D physical motion patterns in the absence of any human labels in
training.

</details>


### [372] [From Swath to Full-Disc: Advancing Precipitation Retrieval with Multimodal Knowledge Expansion](https://arxiv.org/abs/2506.07050)
*Zheng Wang,Kai Ying,Bin Xu,Chunjiao Wang,Cong Bai*

Main category: cs.CV

TL;DR: 本文提出Multimodal Knowledge Expansion和PRE - Net模型解决降水反演范围与精度问题，实验显示性能优于领先产品。


<details>
  <summary>Details</summary>
Motivation: 现有红外降水反演算法精度低，被动微波和雷达方法范围有限，需实现高精度、基于红外的全圆盘降水反演。

Method: 引入两阶段Multimodal Knowledge Expansion管道和PRE - Net模型，在Swath - Distilling阶段用CoMWE传输知识，在Full - Disc Adaptation阶段用Self - MaskTune优化预测。

Result: 在PRE基准测试中，PRE - Net显著提升降水反演性能，优于PERSIANN - CCS、PDIR和IMERG等领先产品。

Conclusion: 所提出的方法有效提升了降水反演性能，代码将开源。

Abstract: Accurate near-real-time precipitation retrieval has been enhanced by
satellite-based technologies. However, infrared-based algorithms have low
accuracy due to weak relations with surface precipitation, whereas passive
microwave and radar-based methods are more accurate but limited in range. This
challenge motivates the Precipitation Retrieval Expansion (PRE) task, which
aims to enable accurate, infrared-based full-disc precipitation retrievals
beyond the scanning swath. We introduce Multimodal Knowledge Expansion, a
two-stage pipeline with the proposed PRE-Net model. In the Swath-Distilling
stage, PRE-Net transfers knowledge from a multimodal data integration model to
an infrared-based model within the scanning swath via Coordinated Masking and
Wavelet Enhancement (CoMWE). In the Full-Disc Adaptation stage, Self-MaskTune
refines predictions across the full disc by balancing multimodal and full-disc
infrared knowledge. Experiments on the introduced PRE benchmark demonstrate
that PRE-Net significantly advanced precipitation retrieval performance,
outperforming leading products like PERSIANN-CCS, PDIR, and IMERG. The code
will be available at https://github.com/Zjut-MultimediaPlus/PRE-Net.

</details>


### [373] [Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning](https://arxiv.org/abs/2309.11082)
*Chen Jiang,Hong Liu,Xuzheng Yu,Qing Wang,Yuan Cheng,Jia Xu,Zhongyi Liu,Qingpei Guo,Wei Chu,Ming Yang,Yuan Qi*

Main category: cs.CV

TL;DR: 针对文本-视频检索中对比学习方法未充分关注难负样本和语义相似度建模问题，提出DMAE模块挖掘难负样本，引入NegNCE损失突出其影响，提出TPM - CL模块构建三元组样本并设计自适应掩码策略，实验表明该方法在四个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法在文本 - 视频检索中未充分关注难负样本，且缺乏对不同层次语义相似度的建模能力。

Method: 提出Dual - Modal Attention - Enhanced Module (DMAE)挖掘难负样本，引入Negative - aware InfoNCE (NegNCE)损失；提出Triplet Partial Margin Contrastive Learning (TPM - CL)模块构建三元组样本，设计自适应令牌掩码策略。

Result: 在MSR - VTT、MSVD、DiDeMo和ActivityNet四个广泛使用的文本 - 视频检索数据集上，该方法优于现有方法。

Conclusion: 所提出的改进对比学习方法能有效解决现有文本 - 视频检索方法的问题，提升检索性能。

Abstract: In recent years, the explosion of web videos makes text-video retrieval
increasingly essential and popular for video filtering, recommendation, and
search. Text-video retrieval aims to rank relevant text/video higher than
irrelevant ones. The core of this task is to precisely measure the cross-modal
similarity between texts and videos. Recently, contrastive learning methods
have shown promising results for text-video retrieval, most of which focus on
the construction of positive and negative pairs to learn text and video
representations. Nevertheless, they do not pay enough attention to hard
negative pairs and lack the ability to model different levels of semantic
similarity. To address these two issues, this paper improves contrastive
learning using two novel techniques. First, to exploit hard examples for robust
discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module
(DMAE) to mine hard negative pairs from textual and visual clues. By further
introducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively
identify all these hard negatives and explicitly highlight their impacts in the
training loss. Second, our work argues that triplet samples can better model
fine-grained semantic similarity compared to pairwise samples. We thereby
present a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to
construct partial order triplet samples by automatically generating
fine-grained hard negatives for matched text-video pairs. The proposed TPM-CL
designs an adaptive token masking strategy with cross-modal interaction to
model subtle semantic differences. Extensive experiments demonstrate that the
proposed approach outperforms existing methods on four widely-used text-video
retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.

</details>


### [374] [MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing MiniGPT-4](https://arxiv.org/abs/2406.00971)
*Vahid Azizi,Fatemeh Koochaki*

Main category: cs.CV

TL;DR: 本文扩展并微调 MiniGPT - 4 用于逆向设计任务，证明现成视觉语言模型可用于更复杂任务。


<details>
  <summary>Details</summary>
Motivation: 逆向设计是复杂的视觉语言任务，需模型同时理解源图像、编辑后图像和文本上下文，当前需探索视觉语言模型用于该任务的能力。

Method: 扩展并微调 MiniGPT - 4 用于逆向设计任务。

Result: 实验证明了像 MiniGPT - 4 这样的现成视觉语言模型可用于逆向设计等更复杂任务。

Conclusion: 现成的视觉语言模型具有可扩展性，能应用于更复杂的任务，如逆向设计。

Abstract: Vision-Language Models (VLMs) have recently seen significant advancements
through integrating with Large Language Models (LLMs). The VLMs, which process
image and text modalities simultaneously, have demonstrated the ability to
learn and understand the interaction between images and texts across various
multi-modal tasks. Reverse designing, which could be defined as a complex
vision-language task, aims to predict the edits and their parameters, given a
source image, an edited version, and an optional high-level textual edit
description. This task requires VLMs to comprehend the interplay between the
source image, the edited version, and the optional textual context
simultaneously, going beyond traditional vision-language tasks. In this paper,
we extend and fine-tune MiniGPT-4 for the reverse designing task. Our
experiments demonstrate the extensibility of off-the-shelf VLMs, specifically
MiniGPT-4, for more complex tasks such as reverse designing. Code is available
at this \href{https://github.com/VahidAz/MiniGPT-Reverse-Designing}

</details>


### [375] [STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis](https://arxiv.org/abs/2506.06276)
*Jiatao Gu,Tianrong Chen,David Berthelot,Huangjie Zheng,Yuyang Wang,Ruixiang Zhang,Laurent Dinh,Miguel Angel Bautista,Josh Susskind,Shuangfei Zhai*

Main category: cs.CV

TL;DR: 提出基于归一化流的STARFlow模型用于高分辨率图像合成，有架构和算法创新，性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 在高分辨率图像合成中提升归一化流模型性能，解决可扩展性问题。

Method: 提出TARFlow，结合归一化流和自回归Transformer；采用深 - 浅设计、在预训练自编码器的潜在空间建模、引入新的引导算法；进行端到端归一化流训练。

Result: STARFlow在类条件和文本条件图像生成任务中表现有竞争力，样本质量接近最先进的扩散模型。

Conclusion: 首次证明归一化流能在这种规模和分辨率下有效运行。

Abstract: We present STARFlow, a scalable generative model based on normalizing flows
that achieves strong performance in high-resolution image synthesis. The core
of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the
expressive power of normalizing flows with the structured modeling capabilities
of Autoregressive Transformers. We first establish the theoretical universality
of TARFlow for modeling continuous distributions. Building on this foundation,
we introduce several key architectural and algorithmic innovations to
significantly enhance scalability: (1) a deep-shallow design, wherein a deep
Transformer block captures most of the model representational capacity,
complemented by a few shallow Transformer blocks that are computationally
efficient yet substantially beneficial; (2) modeling in the latent space of
pretrained autoencoders, which proves more effective than direct pixel-level
modeling; and (3) a novel guidance algorithm that significantly boosts sample
quality. Crucially, our model remains an end-to-end normalizing flow, enabling
exact maximum likelihood training in continuous spaces without discretization.
STARFlow achieves competitive performance in both class-conditional and
text-conditional image generation tasks, approaching state-of-the-art diffusion
models in sample quality. To our knowledge, this work is the first successful
demonstration of normalizing flows operating effectively at this scale and
resolution.

</details>


### [376] [Facial Foundational Model Advances Early Warning of Coronary Artery Disease from Live Videos with DigitalShadow](https://arxiv.org/abs/2506.06283)
*Juexiao Zhou,Zhongyi Han,Mankun Xin,Xingwei He,Guotao Wang,Jiaoyan Song,Gongning Luo,Wenjia He,Xintong Li,Yuetan Chu,Juanwen Chen,Bo Wang,Xia Wu,Wenwen Duan,Zhixia Guo,Liyan Bai,Yilin Pan,Xuefei Bi,Lu Liu,Long Feng,Xiaonan He,Xin Gao*

Main category: cs.CV

TL;DR: 文章介绍了针对冠心病的早期预警系统DigitalShadow，它由微调的面部基础模型驱动，能被动非接触式工作，有隐私保护设计。


<details>
  <summary>Details</summary>
Motivation: 全球人口老龄化使医疗系统面临挑战，冠心病是主要死因且可预防，需要早期检测和主动管理。

Method: 在2100万张面部图像上预训练，再在来自中国四家医院的1751名受试者的7004张面部图像上微调成LiveCAD模型；系统从实时视频流中提取面部特征，结合个性化数据库工作。

Result: 无明确提及实验结果。

Conclusion: 提出了一个先进的冠心病早期预警系统DigitalShadow，具有被动非接触、保护隐私等特点。

Abstract: Global population aging presents increasing challenges to healthcare systems,
with coronary artery disease (CAD) responsible for approximately 17.8 million
deaths annually, making it a leading cause of global mortality. As CAD is
largely preventable, early detection and proactive management are essential. In
this work, we introduce DigitalShadow, an advanced early warning system for
CAD, powered by a fine-tuned facial foundation model. The system is pre-trained
on 21 million facial images and subsequently fine-tuned into LiveCAD, a
specialized CAD risk assessment model trained on 7,004 facial images from 1,751
subjects across four hospitals in China. DigitalShadow functions passively and
contactlessly, extracting facial features from live video streams without
requiring active user engagement. Integrated with a personalized database, it
generates natural language risk reports and individualized health
recommendations. With privacy as a core design principle, DigitalShadow
supports local deployment to ensure secure handling of user data.

</details>


### [377] [Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models](https://arxiv.org/abs/2506.06569)
*Yannis Spyridis,Vasileios Argyriou*

Main category: cs.CV

TL;DR: 本文探讨用RGB图像进行自动纺织品回收预处理，分类用EfficientNetB0达81.25%准确率，分割用零样本方法mIoU达0.90，证明了该方法可行性。


<details>
  <summary>Details</summary>
Motivation: 自动纺织品回收中准确识别材料成分和检测污染物有挑战，需探索低成本传感方式用于预处理。

Method: 用标准RGB图像，对四种常见纺织品类型分类采用迁移学习和交叉验证评估多个预训练架构；对非纺织品特征分割采用结合Grounding DINO和SAM的零样本方法。

Result: 分类中EfficientNetB0在测试集准确率81.25%；分割中生成掩码与真实标签mIoU为0.90。

Conclusion: 使用RGB图像结合现代深度学习技术可实现自动纺织品回收流程的关键分析步骤。

Abstract: Automated sorting is crucial for improving the efficiency and scalability of
textile recycling, but accurately identifying material composition and
detecting contaminants from sensor data remains challenging. This paper
investigates the use of standard RGB imagery, a cost-effective sensing
modality, for key pre-processing tasks in an automated system. We present
computer vision components designed for a conveyor belt setup to perform (a)
classification of four common textile types and (b) segmentation of non-textile
features such as buttons and zippers. For classification, several pre-trained
architectures were evaluated using transfer learning and cross-validation, with
EfficientNetB0 achieving the best performance on a held-out test set with
81.25\% accuracy. For feature segmentation, a zero-shot approach combining the
Grounding DINO open-vocabulary detector with the Segment Anything Model (SAM)
was employed, demonstrating excellent performance with a mIoU of 0.90 for the
generated masks against ground truth. This study demonstrates the feasibility
of using RGB images coupled with modern deep learning techniques, including
transfer learning for classification and foundation models for zero-shot
segmentation, to enable essential analysis steps for automated textile
recycling pipelines.

</details>


### [378] [Improving Wildlife Out-of-Distribution Detection: Africas Big Five](https://arxiv.org/abs/2506.06719)
*Mufhumudzi Muthivhi,Jiahao Huo,Fredrik Gustafsson,Terence L. van Zyl*

Main category: cs.CV

TL;DR: 研究野生动物的分布外（OOD）检测，对比多种方法，基于特征的方法泛化能力强，NCM结合ImageNet预训练特征表现更佳。


<details>
  <summary>Details</summary>
Motivation: 现有动物分类模型在封闭世界假设下训练，对未知类别的预测过于自信，需研究野生动物的OOD检测。

Method: 选择参数化的最近类均值（NCM）和非参数对比学习方法作为基线，与文献中常见的OOD方法进行比较。

Result: 基于特征的方法在不同分类阈值下具有更强的泛化能力，NCM结合ImageNet预训练特征在AUPR - IN、AUPR - OUT和AUTOC上比最佳OOD方法分别提高2%、4%和22%。

Conclusion: 基于特征的方法在野生动物OOD检测中表现良好，NCM结合ImageNet预训练特征效果更佳。

Abstract: Mitigating human-wildlife conflict seeks to resolve unwanted encounters
between these parties. Computer Vision provides a solution to identifying
individuals that might escalate into conflict, such as members of the Big Five
African animals. However, environments often contain several varied species.
The current state-of-the-art animal classification models are trained under a
closed-world assumption. They almost always remain overconfident in their
predictions even when presented with unknown classes. This study investigates
out-of-distribution (OOD) detection of wildlife, specifically the Big Five. To
this end, we select a parametric Nearest Class Mean (NCM) and a non-parametric
contrastive learning approach as baselines to take advantage of pretrained and
projected features from popular classification encoders. Moreover, we compare
our baselines to various common OOD methods in the literature. The results show
feature-based methods reflect stronger generalisation capability across varying
classification thresholds. Specifically, NCM with ImageNet pre-trained features
achieves a 2%, 4% and 22% improvement on AUPR-IN, AUPR-OUT and AUTC over the
best OOD methods, respectively. The code can be found here
https://github.com/pxpana/BIG5OOD

</details>


### [379] [Hi-LSplat: Hierarchical 3D Language Gaussian Splatting](https://arxiv.org/abs/2506.06822)
*Chenlu Zhan,Yufei Zhang,Gaoang Wang,Hongwei Wang*

Main category: cs.CV

TL;DR: 提出Hi - LSplat用于3D开放词汇查询，解决视图不一致和语义理解问题，实验显示其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于3DGS的模型缺乏统一3D表示导致视图不一致，且存在开放词汇挑战阻碍分层语义理解。

Method: 构建3D分层语义树将2D特征提升到3D特征，引入实例级和部分级对比损失，构建两个分层语义数据集。

Result: 在3D开放词汇分割和定位任务中表现出优越性，在分层语义数据集上能捕捉复杂分层语义。

Conclusion: Hi - LSplat能有效解决现有问题，实现视图一致的3D分层语义。

Abstract: Modeling 3D language fields with Gaussian Splatting for open-ended language
queries has recently garnered increasing attention. However, recent 3DGS-based
models leverage view-dependent 2D foundation models to refine 3D semantics but
lack a unified 3D representation, leading to view inconsistencies.
Additionally, inherent open-vocabulary challenges cause inconsistencies in
object and relational descriptions, impeding hierarchical semantic
understanding. In this paper, we propose Hi-LSplat, a view-consistent
Hierarchical Language Gaussian Splatting work for 3D open-vocabulary querying.
To achieve view-consistent 3D hierarchical semantics, we first lift 2D features
to 3D features by constructing a 3D hierarchical semantic tree with layered
instance clustering, which addresses the view inconsistency issue caused by 2D
semantic features. Besides, we introduce instance-wise and part-wise
contrastive losses to capture all-sided hierarchical semantic representations.
Notably, we construct two hierarchical semantic datasets to better assess the
model's ability to distinguish different semantic levels. Extensive experiments
highlight our method's superiority in 3D open-vocabulary segmentation and
localization. Its strong performance on hierarchical semantic datasets
underscores its ability to capture complex hierarchical semantics within 3D
scenes.

</details>


### [380] [Exploring Visual Prompting: Robustness Inheritance and Beyond](https://arxiv.org/abs/2506.06823)
*Qi Li,Liangzhi Li,Zhouqiang Jiang,Bowen Wang,Keke Tang*

Main category: cs.CV

TL;DR: 本文首次探索视觉提示（VP）在鲁棒源模型场景下的表现，提出提示边界松弛（PBL）策略缓解VP面临的权衡问题，实验验证了策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 探究VP在鲁棒源模型场景下能否继承其鲁棒性、是否存在鲁棒性与泛化能力的权衡以及有无缓解策略。

Method: 提出一种名为提示边界松弛（PBL）的策略，该策略轻量级、即插即用且与VP自然兼容。

Result: 广泛实验表明研究结果具有普遍性，PBL策略能有效确保鲁棒性继承，显著提升VP在下游数据集上的泛化能力。

Conclusion: PBL策略可缓解VP面临的权衡问题，带来显著好处。

Abstract: Visual Prompting (VP), an efficient method for transfer learning, has shown
its potential in vision tasks. However, previous works focus exclusively on VP
from standard source models, it is still unknown how it performs under the
scenario of a robust source model: Can the robustness of the source model be
successfully inherited? Does VP also encounter the same trade-off between
robustness and generalization ability as the source model during this process?
If such a trade-off exists, is there a strategy specifically tailored to VP to
mitigate this limitation? In this paper, we thoroughly explore these three
questions for the first time and provide affirmative answers to them. To
mitigate the trade-off faced by VP, we propose a strategy called Prompt
Boundary Loosening (PBL). As a lightweight, plug-and-play strategy naturally
compatible with VP, PBL effectively ensures the successful inheritance of
robustness when the source model is a robust model, while significantly
enhancing VP's generalization ability across various downstream datasets.
Extensive experiments across various datasets show that our findings are
universal and demonstrate the significant benefits of the proposed strategy.

</details>


### [381] [Controllable Coupled Image Generation via Diffusion Models](https://arxiv.org/abs/2506.06826)
*Chenfei Yuan,Nanshan Jia,Hangqi Li,Peter W. Glynn,Zeyu Zheng*

Main category: cs.CV

TL;DR: 提出耦合图像生成的注意力级控制方法，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决耦合图像生成中多图像背景相同或相似，中心对象随文本提示灵活变化的问题。

Method: 在模型交叉注意力模块中分离背景和实体组件，附加随采样时间步变化的权重控制参数，用组合目标优化该参数序列。

Result: 在背景耦合度、文本图像对齐和整体视觉质量等标准上优于现有方法。

Conclusion: 所提方法有效，在耦合图像生成任务上表现更佳。

Abstract: We provide an attention-level control method for the task of coupled image
generation, where "coupled" means that multiple simultaneously generated images
are expected to have the same or very similar backgrounds. While backgrounds
coupled, the centered objects in the generated images are still expected to
enjoy the flexibility raised from different text prompts. The proposed method
disentangles the background and entity components in the model's
cross-attention modules, attached with a sequence of time-varying weight
control parameters depending on the time step of sampling. We optimize this
sequence of weight control parameters with a combined objective that assesses
how coupled the backgrounds are as well as text-to-image alignment and overall
visual quality. Empirical results demonstrate that our method outperforms
existing approaches across these criteria.

</details>


### [382] [EndoARSS: Adapting Spatially-Aware Foundation Model for Efficient Activity Recognition and Semantic Segmentation in Endoscopic Surgery](https://arxiv.org/abs/2506.06830)
*Guankun Wang,Rui Tang,Mengya Xu,Long Bai,Huxin Gao,Hongliang Ren*

Main category: cs.CV

TL;DR: 提出用于内窥镜手术活动识别和语义分割的多任务学习框架EndoARSS，实验表明其性能显著优于现有模型，有望推动内窥镜手术系统发展。


<details>
  <summary>Details</summary>
Motivation: 内窥镜手术场景复杂，传统深度学习模型存在跨活动干扰问题，性能不佳，需提升手术环境理解能力。

Method: 提出EndoARSS框架，基于DINOv2模型，集成低秩自适应，加入任务高效共享低秩适配器，引入空间感知多尺度注意力机制；并提出三个新数据集用于评估。

Result: EndoARSS在多个基准测试中表现出色，相比现有模型显著提高了准确性和鲁棒性。

Conclusion: EndoARSS有潜力推动AI驱动的内窥镜手术系统发展，为提高手术安全性和效率提供有价值的见解。

Abstract: Endoscopic surgery is the gold standard for robotic-assisted minimally
invasive surgery, offering significant advantages in early disease detection
and precise interventions. However, the complexity of surgical scenes,
characterized by high variability in different surgical activity scenarios and
confused image features between targets and the background, presents challenges
for surgical environment understanding. Traditional deep learning models often
struggle with cross-activity interference, leading to suboptimal performance in
each downstream task. To address this limitation, we explore multi-task
learning, which utilizes the interrelated features between tasks to enhance
overall task performance. In this paper, we propose EndoARSS, a novel
multi-task learning framework specifically designed for endoscopy surgery
activity recognition and semantic segmentation. Built upon the DINOv2
foundation model, our approach integrates Low-Rank Adaptation to facilitate
efficient fine-tuning while incorporating Task Efficient Shared Low-Rank
Adapters to mitigate gradient conflicts across diverse tasks. Additionally, we
introduce the Spatially-Aware Multi-Scale Attention that enhances feature
representation discrimination by enabling cross-spatial learning of global
information. In order to evaluate the effectiveness of our framework, we
present three novel datasets, MTLESD, MTLEndovis and MTLEndovis-Gen, tailored
for endoscopic surgery scenarios with detailed annotations for both activity
recognition and semantic segmentation tasks. Extensive experiments demonstrate
that EndoARSS achieves remarkable performance across multiple benchmarks,
significantly improving both accuracy and robustness in comparison to existing
models. These results underscore the potential of EndoARSS to advance AI-driven
endoscopic surgical systems, offering valuable insights for enhancing surgical
safety and efficiency.

</details>


### [383] [Harnessing Vision-Language Models for Time Series Anomaly Detection](https://arxiv.org/abs/2506.06836)
*Zelin He,Sarah Alnegheimish,Matthew Reimherr*

Main category: cs.CV

TL;DR: 本文提出基于视觉语言模型的两阶段时间序列异常检测方案，无需时间序列训练，在性能和效率上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法缺乏人类专家的视觉 - 时间推理能力，直接应用视觉语言模型于时间序列存在精度和效率问题。

Method: 提出两阶段解决方案，包括基于轻量级预训练视觉编码器的ViT4TS进行候选异常定位，以及基于VLM的VLM4TS阶段对候选异常进行检测细化。

Result: VLM4TS在多数情况下优于时间序列预训练和从头开始训练的基线模型，F1 - max分数提升24.6%，还优于现有基于语言模型的方法，标记使用效率平均高36倍。

Conclusion: 所提方法能有效利用视觉语言模型进行时间序列异常检测，在性能和效率上有显著优势。

Abstract: Time-series anomaly detection (TSAD) has played a vital role in a variety of
fields, including healthcare, finance, and industrial monitoring. Prior
methods, which mainly focus on training domain-specific models on numerical
data, lack the visual-temporal reasoning capacity that human experts have to
identify contextual anomalies. To fill this gap, we explore a solution based on
vision language models (VLMs). Recent studies have shown the ability of VLMs
for visual reasoning tasks, yet their direct application to time series has
fallen short on both accuracy and efficiency. To harness the power of VLMs for
TSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening
stage built on a relatively lightweight pretrained vision encoder, which
leverages 2-D time-series representations to accurately localize candidate
anomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal
context and VLM reasoning capacity to refine the detection upon the candidates
provided by ViT4TS. We show that without any time-series training, VLM4TS
outperforms time-series pretrained and from-scratch baselines in most cases,
yielding a 24.6 percent improvement in F1-max score over the best baseline.
Moreover, VLM4TS also consistently outperforms existing language-model-based
TSAD methods and is on average 36 times more efficient in token usage.

</details>


### [384] [Position Prediction Self-Supervised Learning for Multimodal Satellite Imagery Semantic Segmentation](https://arxiv.org/abs/2506.06852)
*John Waithaka,Moise Busogi*

Main category: cs.CV

TL;DR: 提出将位置预测自监督学习方法LOCA用于多模态卫星图像语义分割，在Sen1Floods11数据集上显著优于基于重建的方法。


<details>
  <summary>Details</summary>
Motivation: 卫星图像语义分割受限于有限的标注训练数据，现有自监督预训练方法侧重于重建而非定位，无法满足分割任务需求。

Method: 将SatMAE的通道分组从多光谱扩展到多模态数据，引入同组注意力掩码，采用相对补丁位置预测进行空间推理。

Result: 在Sen1Floods11洪水映射数据集上，显著优于现有的基于重建的自监督学习方法。

Conclusion: 针对多模态卫星图像适当调整的位置预测任务，比基于重建的方法能学习到更有效的卫星图像语义分割表示。

Abstract: Semantic segmentation of satellite imagery is crucial for Earth observation
applications, but remains constrained by limited labelled training data. While
self-supervised pretraining methods like Masked Autoencoders (MAE) have shown
promise, they focus on reconstruction rather than localisation-a fundamental
aspect of segmentation tasks. We propose adapting LOCA (Location-aware), a
position prediction self-supervised learning method, for multimodal satellite
imagery semantic segmentation. Our approach addresses the unique challenges of
satellite data by extending SatMAE's channel grouping from multispectral to
multimodal data, enabling effective handling of multiple modalities, and
introducing same-group attention masking to encourage cross-modal interaction
during pretraining. The method uses relative patch position prediction,
encouraging spatial reasoning for localisation rather than reconstruction. We
evaluate our approach on the Sen1Floods11 flood mapping dataset, where it
significantly outperforms existing reconstruction-based self-supervised
learning methods for satellite imagery. Our results demonstrate that position
prediction tasks, when properly adapted for multimodal satellite imagery, learn
representations more effective for satellite image semantic segmentation than
reconstruction-based approaches.

</details>


### [385] [Face recognition on point cloud with cgan-top for denoising](https://arxiv.org/abs/2506.06864)
*Junyu Liu,Jianfeng Ren,Sunhong Liang,Xudong Jiang*

Main category: cs.CV

TL;DR: 本文提出一种端到端的含噪3D点云人脸识别方法，结合去噪和识别模块，经Bosphorus数据集验证显著提升识别准确率。


<details>
  <summary>Details</summary>
Motivation: 3D点云人脸识别受关注，但原始点云因传感器缺陷含大量噪声。

Method: 设计cGAN - TOP去除点云噪声并恢复特征，采用LDGCNN从处理后的点云识别面部，该网络可分层连接多尺度的局部点特征和相邻特征。

Result: 在Bosphorus数据集上验证，该方法在所有噪声设置下显著提高识别准确率，最大提升14.81%。

Conclusion: 所提端到端的含噪3D点云人脸识别方法有效，能显著提升识别准确率。

Abstract: Face recognition using 3D point clouds is gaining growing interest, while raw
point clouds often contain a significant amount of noise due to imperfect
sensors. In this paper, an end-to-end 3D face recognition on a noisy point
cloud is proposed, which synergistically integrates the denoising and
recognition modules. Specifically, a Conditional Generative Adversarial Network
on Three Orthogonal Planes (cGAN-TOP) is designed to effectively remove the
noise in the point cloud, and recover the underlying features for subsequent
recognition. A Linked Dynamic Graph Convolutional Neural Network (LDGCNN) is
then adapted to recognize faces from the processed point cloud, which
hierarchically links both the local point features and neighboring features of
multiple scales. The proposed method is validated on the Bosphorus dataset. It
significantly improves the recognition accuracy under all noise settings, with
a maximum gain of 14.81%.

</details>


### [386] [Polar Hierarchical Mamba: Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences](https://arxiv.org/abs/2506.06944)
*Mellon M. Zhang,Glen Chou,Saibal Mukhopadhyay*

Main category: cs.CV

TL;DR: 提出适用于极坐标流式LiDAR的新型SSM架构PHiM，在Waymo Open Dataset上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR目标检测方法存在延迟高、性能下降、内存消耗大等问题，需高效准确的检测方法。

Method: 提出PHiM架构，用局部双向Mamba块进行扇区内空间编码，全局前向Mamba进行扇区间时间建模。

Result: 在Waymo Open Dataset上，PHiM超越之前最佳结果10%，吞吐量是全扫描基线的两倍。

Conclusion: PHiM是流式LiDAR目标检测的新SOTA方法。

Abstract: Accurate and efficient object detection is essential for autonomous vehicles,
where real-time perception requires low latency and high throughput. LiDAR
sensors provide robust depth information, but conventional methods process full
360{\deg} scans in a single pass, introducing significant delay. Streaming
approaches address this by sequentially processing partial scans in the native
polar coordinate system, yet they rely on translation-invariant convolutions
that are misaligned with polar geometry -- resulting in degraded performance or
requiring complex distortion mitigation. Recent Mamba-based state space models
(SSMs) have shown promise for LiDAR perception, but only in the full-scan
setting, relying on geometric serialization and positional embeddings that are
memory-intensive and ill-suited to streaming. We propose Polar Hierarchical
Mamba (PHiM), a novel SSM architecture designed for polar-coordinate streaming
LiDAR. PHiM uses local bidirectional Mamba blocks for intra-sector spatial
encoding and a global forward Mamba for inter-sector temporal modeling,
replacing convolutions and positional encodings with distortion-aware,
dimensionally-decomposed operations. PHiM sets a new state-of-the-art among
streaming detectors on the Waymo Open Dataset, outperforming the previous best
by 10\% and matching full-scan baselines at twice the throughput. Code will be
available at https://github.com/meilongzhang/Polar-Hierarchical-Mamba .

</details>


### [387] [MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks](https://arxiv.org/abs/2506.07016)
*Sanjoy Chowdhury,Mohamed Elmoghany,Yohan Abeysinghe,Junjie Fei,Sayan Nag,Salman Khan,Mohamed Elhoseiny,Dinesh Manocha*

Main category: cs.CV

TL;DR: 提出新任务AV - HaystacksQA和基准AVHaystacks，提出框架MAGNET，还引入新指标评估。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答基准有局限，LMMs在跨大规模视频集合复杂推理的实际场景表现不佳，需新任务和评估方法。

Method: 引入AV - HaystacksQA任务，构建AVHaystacks基准，提出多智能体框架MAGNET，引入STEM和MTGS指标。

Result: MAGNET在BLEU@4和GPT评估得分上比基线方法有显著提升。

Conclusion: 提出的任务、基准、框架和指标有助于提升LMMs在多视频检索和时间定位任务的评估。

Abstract: Large multimodal models (LMMs) have shown remarkable progress in audio-visual
understanding, yet they struggle with real-world scenarios that require complex
reasoning across extensive video collections. Existing benchmarks for video
question answering remain limited in scope, typically involving one clip per
query, which falls short of representing the challenges of large-scale,
audio-visual retrieval and reasoning encountered in practical applications. To
bridge this gap, we introduce a novel task named AV-HaystacksQA, where the goal
is to identify salient segments across different videos in response to a query
and link them together to generate the most informative answer. To this end, we
present AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA
pairs designed to assess the capabilities of LMMs in multi-video retrieval and
temporal grounding task. Additionally, we propose a model-agnostic, multi-agent
framework MAGNET to address this challenge, achieving up to 89% and 65%
relative improvements over baseline methods on BLEU@4 and GPT evaluation scores
in QA task on our proposed AVHaystacks. To enable robust evaluation of
multi-video retrieval and temporal grounding for optimal response generation,
we introduce two new metrics, STEM, which captures alignment errors between a
ground truth and a predicted step sequence and MTGS, to facilitate balanced and
interpretable evaluation of segment-level grounding performance. Project:
https://schowdhury671.github.io/magnet_project/

</details>


### [388] [Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs](https://arxiv.org/abs/2506.07045)
*Yikun Ji,Hong Yan,Jun Lan,Huijia Zhu,Weiqiang Wang,Qi Fan,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 现有图像生成检测方法缺乏可解释性，MLLMs经微调可用于检测，但存在幻觉问题。本文构建数据集，用多阶段优化策略微调MLLMs，模型检测和定位性能超基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成检测方法多为黑盒，缺乏可解释性，MLLMs存在幻觉问题，需改进以实现可解释且鲁棒的图像生成检测。

Method: 构建带边界框和描述性字幕的AI生成图像数据集，通过多阶段优化策略微调MLLMs。

Result: 模型在检测AI生成图像和定位视觉缺陷方面表现出色，显著优于基线方法。

Conclusion: 通过构建数据集和多阶段优化微调MLLMs，能有效实现可解释的AI生成图像检测。

Abstract: The rapid advancement of image generation technologies intensifies the demand
for interpretable and robust detection methods. Although existing approaches
often attain high accuracy, they typically operate as black boxes without
providing human-understandable justifications. Multi-modal Large Language
Models (MLLMs), while not originally intended for forgery detection, exhibit
strong analytical and reasoning capabilities. When properly fine-tuned, they
can effectively identify AI-generated images and offer meaningful explanations.
However, existing MLLMs still struggle with hallucination and often fail to
align their visual interpretations with actual image content and human
reasoning. To bridge this gap, we construct a dataset of AI-generated images
annotated with bounding boxes and descriptive captions that highlight synthesis
artifacts, establishing a foundation for human-aligned visual-textual grounded
reasoning. We then finetune MLLMs through a multi-stage optimization strategy
that progressively balances the objectives of accurate detection, visual
localization, and coherent textual explanation. The resulting model achieves
superior performance in both detecting AI-generated images and localizing
visual flaws, significantly outperforming baseline methods.

</details>


### [389] [Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images](https://arxiv.org/abs/2506.06389)
*Rifat Sadik,Tanvir Rahman,Arpan Bhattacharjee,Bikash Chandra Halder,Ismail Hossain*

Main category: cs.CV

TL;DR: 本文研究ViTs在医学图像上对抗性水印攻击的敏感性，发现ViTs易受攻击，但对抗训练可提升性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在计算机视觉任务中广泛应用，但ViTs依赖全局注意力机制，易受对抗性扰动影响，需研究其在医学图像上对抗性水印攻击的敏感性。

Method: 通过投影梯度下降（PGD）生成对抗性水印，研究攻击对CNN的可迁移性，并分析对抗训练这一防御机制。

Result: 干净图像性能不受损，但ViTs对对抗性攻击更脆弱，准确率最低降至27.6%，对抗训练可将准确率提升至90.0%。

Conclusion: ViTs在医学图像中易受对抗性水印攻击，但对抗训练能有效提升其对抗攻击的能力。

Abstract: Deep learning models have shown remarkable success in dermatological image
analysis, offering potential for automated skin disease diagnosis. Previously,
convolutional neural network(CNN) based architectures have achieved immense
popularity and success in computer vision (CV) based task like skin image
recognition, generation and video analysis. But with the emergence of
transformer based models, CV tasks are now are nowadays carrying out using
these models. Vision Transformers (ViTs) is such a transformer-based models
that have shown success in computer vision. It uses self-attention mechanisms
to achieve state-of-the-art performance across various tasks. However, their
reliance on global attention mechanisms makes them susceptible to adversarial
perturbations. This paper aims to investigate the susceptibility of ViTs for
medical images to adversarial watermarking-a method that adds so-called
imperceptible perturbations in order to fool models. By generating adversarial
watermarks through Projected Gradient Descent (PGD), we examine the
transferability of such attacks to CNNs and analyze the performance defense
mechanism -- adversarial training. Results indicate that while performance is
not compromised for clean images, ViTs certainly become much more vulnerable to
adversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless,
adversarial training raises it up to 90.0%.

</details>


### [390] [Image segmentation and classification of E-waste for waste segregation](https://arxiv.org/abs/2506.07122)
*Prakriti Tripathi,Theertha Biju,Maniram Thota,Rakesh Lingam*

Main category: cs.CV

TL;DR: 利用机器学习模型对电子垃圾分类，创建自定义数据集，训练YOLOv11和Mask - RCNN模型，后续将集成到机器人进行垃圾分离。


<details>
  <summary>Details</summary>
Motivation: 解决行业伙伴提出的利用机器学习模型对电子垃圾分类，供抓取机器人进行垃圾分离的问题。

Method: 选取常见电子垃圾制作自定义数据集，训练YOLOv11和Mask - RCNN模型。

Result: YOLOv11模型实时mAP达70，Mask - RCNN模型mAP达41。

Conclusion: 模型将进一步集成到抓取机器人进行电子垃圾分离。

Abstract: Industry partners provided a problem statement that involves classifying
electronic waste using machine learning models that will be used by
pick-and-place robots for waste segregation. We started by taking common
electronic waste items, such as a mouse and charger, unsoldering them, and
taking pictures to create a custom dataset. Then state-of-the art YOLOv11 model
was trained and run to achieve 70 mAP in real-time. Mask-RCNN model was also
trained and achieved 41 mAP. The model will be further integrated with
pick-and-place robots to perform segregation of e-waste.

</details>


### [391] [Learning Compact Vision Tokens for Efficient Large Multimodal Models](https://arxiv.org/abs/2506.07138)
*Hao Tang,Chengchao Shen*

Main category: cs.CV

TL;DR: 本文提出STF和MBTF模块处理大多模态模型的视觉令牌序列，在减少令牌数量同时保留信息，提升推理效率，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 大多模态模型因大语言模型高成本和处理长视觉令牌序列的二次复杂度面临计算挑战，且权重冻结的视觉编码器难适应下游任务。

Method: 提出Spatial Token Fusion (STF) 方法融合相邻视觉令牌以缩短序列长度；引入Multi - Block Token Fusion (MBTF) 模块补充多粒度特征；结合STF和MBTF模块平衡令牌减少和信息保留。

Result: 基于LLaVA - 1.5的方法在8个视觉语言基准测试中，仅用基线25%的视觉令牌就取得与基线相当甚至更优的性能。

Conclusion: 结合STF和MBTF模块可在不牺牲多模态推理能力的情况下提高推理效率。

Abstract: Large multimodal models (LMMs) suffer significant computational challenges
due to the high cost of Large Language Models (LLMs) and the quadratic
complexity of processing long vision token sequences. In this paper, we explore
the spatial redundancy among vision tokens and shorten the length of vision
token sequences for inference acceleration. Specifically, we propose a Spatial
Token Fusion (STF) method to learn compact vision tokens for short vision token
sequence, where spatial-adjacent tokens are fused into one. Meanwhile,
weight-frozen vision encoder can not well adapt to the demand of extensive
downstream vision-language tasks. To this end, we further introduce a
Multi-Block Token Fusion (MBTF) module to supplement multi-granularity features
for the reduced token sequence. Overall, we combine STF and MBTF module to
balance token reduction and information preservation, thereby improving
inference efficiency without sacrificing multimodal reasoning capabilities.
Experimental results demonstrate that our method based on LLaVA-1.5 achieves
comparable or even superior performance to the baseline on 8 popular
vision-language benchmarks with only $25\%$ vision tokens of baseline. The
source code and trained weights are available at
https://github.com/visresearch/LLaVA-STF.

</details>


### [392] [Securing Traffic Sign Recognition Systems in Autonomous Vehicles](https://arxiv.org/abs/2506.06563)
*Thushari Hapuarachchi,Long Dang,Kaiqi Xiong*

Main category: cs.CV

TL;DR: 研究交通标志识别DNN鲁棒性，用误差最小化攻击测试，提出数据增强训练方法缓解攻击，还提出检测模型，强调需先进训练方法。


<details>
  <summary>Details</summary>
Motivation: DNN用于交通标志识别时在未知源数据集训练，要确保模型训练时安全不被攻击或中毒。

Method: 对DNN进行误差最小化攻击，添加不可察觉扰动；提出基于数据增强的训练方法，利用非线性变换破坏扰动；提出检测模型识别中毒数据。

Result: 误差最小化攻击使DNN预测准确率从99.90%降至10.6%，缓解方案将准确率恢复到96.05%，方法优于对抗训练，检测模型识别攻击成功率超99%。

Conclusion: 交通标志识别系统中的DNN需采用先进训练方法缓解数据投毒攻击影响。

Abstract: Deep Neural Networks (DNNs) are widely used for traffic sign recognition
because they can automatically extract high-level features from images. These
DNNs are trained on large-scale datasets obtained from unknown sources.
Therefore, it is important to ensure that the models remain secure and are not
compromised or poisoned during training. In this paper, we investigate the
robustness of DNNs trained for traffic sign recognition. First, we perform the
error-minimizing attacks on DNNs used for traffic sign recognition by adding
imperceptible perturbations on training data. Then, we propose a data
augmentation-based training method to mitigate the error-minimizing attacks.
The proposed training method utilizes nonlinear transformations to disrupt the
perturbations and improve the model robustness. We experiment with two
well-known traffic sign datasets to demonstrate the severity of the attack and
the effectiveness of our mitigation scheme. The error-minimizing attacks reduce
the prediction accuracy of the DNNs from 99.90% to 10.6%. However, our
mitigation scheme successfully restores the prediction accuracy to 96.05%.
Moreover, our approach outperforms adversarial training in mitigating the
error-minimizing attacks. Furthermore, we propose a detection model capable of
identifying poisoned data even when the perturbations are imperceptible to
human inspection. Our detection model achieves a success rate of over 99% in
identifying the attack. This research highlights the need to employ advanced
training methods for DNNs in traffic sign recognition systems to mitigate the
effects of data poisoning attacks.

</details>


### [393] [Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models](https://arxiv.org/abs/2506.07177)
*Sangwon Jang,Taekyung Ki,Jaehyeong Jo,Jaehong Yoon,Soo Ye Kim,Zhe Lin,Sung Ju Hwang*

Main category: cs.CV

TL;DR: 提出无训练的Frame Guidance用于可控视频生成，可降低内存使用，实验证明能为多种任务生成高质量可控视频。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖微调大规模视频模型，随模型增大不实用，需提升视频生成的细粒度可控性。

Method: 提出基于帧级信号的无训练指导方法Frame Guidance，采用简单的潜在处理方法降低内存使用，应用新的潜在优化策略。

Result: Frame Guidance能为多种任务和输入信号生成高质量可控视频。

Conclusion: Frame Guidance无需训练，能跨多种任务有效控制视频生成，与任何视频模型兼容。

Abstract: Advancements in diffusion models have significantly improved video quality,
directing attention to fine-grained controllability. However, many existing
methods depend on fine-tuning large-scale video models for specific tasks,
which becomes increasingly impractical as model sizes continue to grow. In this
work, we present Frame Guidance, a training-free guidance for controllable
video generation based on frame-level signals, such as keyframes, style
reference images, sketches, or depth maps. For practical training-free
guidance, we propose a simple latent processing method that dramatically
reduces memory usage, and apply a novel latent optimization strategy designed
for globally coherent video generation. Frame Guidance enables effective
control across diverse tasks, including keyframe guidance, stylization, and
looping, without any training, compatible with any video models. Experimental
results show that Frame Guidance can produce high-quality controlled videos for
a wide range of tasks and input signals.

</details>


### [394] [Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery](https://arxiv.org/abs/2506.06667)
*Yu-Hsuan Ho,Ali Mostafavi*

Main category: cs.CV

TL;DR: 提出Flood - DamageSense模型用于建筑级洪水灾害评估，结合多模态数据，优于现有基线，能提供快速准确的洪水灾害信息。


<details>
  <summary>Details</summary>
Motivation: 现有模型在识别洪水相关建筑损坏方面表现不佳，需开发专门用于建筑级洪水灾害评估的模型。

Method: 提出Flood - DamageSense模型，融合多模态数据，采用多模态Mamba骨干网络、半孪生编码器和特定任务解码器进行联合预测，有端到端后处理管道。

Result: 在哈维飓风图像上训练和评估，比现有基线F1均值最多提高19个百分点，“轻微”和“中度”损坏类别提升最大，消融研究表明固有风险特征贡献最大。

Conclusion: Flood - DamageSense结合风险感知建模和SAR的全天候能力，能为灾后决策和资源分配提供更快、更细粒度、更可靠的洪水灾害信息。

Abstract: Most post-disaster damage classifiers succeed only when destructive forces
leave clear spectral or structural signatures -- conditions rarely present
after inundation. Consequently, existing models perform poorly at identifying
flood-related building damages. The model presented in this study,
Flood-DamageSense, addresses this gap as the first deep-learning framework
purpose-built for building-level flood-damage assessment. The architecture
fuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical
basemaps and an inherent flood-risk layer that encodes long-term exposure
probabilities, guiding the network toward plausibly affected structures even
when compositional change is minimal. A multimodal Mamba backbone with a
semi-Siamese encoder and task-specific decoders jointly predicts (1) graded
building-damage states, (2) floodwater extent, and (3) building footprints.
Training and evaluation on Hurricane Harvey (2017) imagery from Harris County,
Texas -- supported by insurance-derived property-damage extents -- show a mean
F1 improvement of up to 19 percentage points over state-of-the-art baselines,
with the largest gains in the frequently misclassified "minor" and "moderate"
damage categories. Ablation studies identify the inherent-risk feature as the
single most significant contributor to this performance boost. An end-to-end
post-processing pipeline converts pixel-level outputs to actionable,
building-scale damage maps within minutes of image acquisition. By combining
risk-aware modeling with SAR's all-weather capability, Flood-DamageSense
delivers faster, finer-grained, and more reliable flood-damage intelligence to
support post-disaster decision-making and resource allocation.

</details>


### [395] [Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment](https://arxiv.org/abs/2506.06680)
*Radha Kodali,Venkata Rao Dhulipalla,Venkata Siva Kishor Tatavarty,Madhavi Nadakuditi,Bharadwaj Thiruveedhula,Suryanarayana Gunnam,Durga Prasad Bavirisetti*

Main category: cs.CV

TL;DR: 文章指出不孕问题日益严重，体外受精中传统胚胎分级方法低效，提出用CNN - LSTM的可解释人工智能框架进行胚胎分类，模型有高准确率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 不孕问题影响大且预计增多，体外受精中传统胚胎分级方法耗时低效，需更有效的胚胎评估方法。

Method: 引入结合卷积神经网络（CNN）和长短期记忆网络（LSTM）架构的可解释人工智能（XAI）框架CNN - LSTM进行胚胎分类。

Result: 利用深度学习，模型在胚胎分类中实现了较高的准确率。

Conclusion: CNN - LSTM的可解释人工智能框架可有效用于胚胎分类。

Abstract: Infertility has a considerable impact on individuals' quality of life,
affecting them socially and psychologically, with projections indicating a rise
in the upcoming years. In vitro fertilization (IVF) emerges as one of the
primary techniques within economically developed nations, employed to address
the rising problem of low fertility. Expert embryologists conventionally grade
embryos by reviewing blastocyst images to select the most optimal for transfer,
yet this process is time-consuming and lacks efficiency. Blastocyst images
provide a valuable resource for assessing embryo viability. In this study, we
introduce an explainable artificial intelligence (XAI) framework for
classifying embryos, employing a fusion of convolutional neural network (CNN)
and long short-term memory (LSTM) architecture, referred to as CNN-LSTM.
Utilizing deep learning, our model achieves high accuracy in embryo
classification while maintaining interpretability through XAI.

</details>


### [396] [Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations](https://arxiv.org/abs/2506.06780)
*Lennart Bastian,Mohammad Rashed,Nassir Navab,Tolga Birdal*

Main category: cs.CV

TL;DR: 本文提出用受Savitzky - Golay路径引导的神经控制微分方程对SO(3)上的连续时间旋转物体动力学建模，实验显示其预测能力优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在计算机视觉和机器人领域，SO(3)外推存在传感器观测有噪声且稀疏、运动模式复杂、需长期预测等挑战，需更好的旋转物体跟踪和预测方法。

Method: 使用受Savitzky - Golay路径引导的神经控制微分方程对SO(3)上的连续时间旋转物体动力学建模，学习潜在物体轨迹的一般潜在动力系统并尊重旋转的几何结构。

Result: 在真实世界数据上的实验表明，该方法的预测能力优于现有方法。

Conclusion: 提出的方法在旋转物体跟踪和预测方面有更好的性能，是一种有效的解决方案。

Abstract: Tracking and forecasting the rotation of objects is fundamental in computer
vision and robotics, yet SO(3) extrapolation remains challenging as (1) sensor
observations can be noisy and sparse, (2) motion patterns can be governed by
complex dynamics, and (3) application settings can demand long-term
forecasting. This work proposes modeling continuous-time rotational object
dynamics on $SO(3)$ using Neural Controlled Differential Equations guided by
Savitzky-Golay paths. Unlike existing methods that rely on simplified motion
assumptions, our method learns a general latent dynamical system of the
underlying object trajectory while respecting the geometric structure of
rotations. Experimental results on real-world data demonstrate compelling
forecasting capabilities compared to existing approaches.

</details>


### [397] [From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models](https://arxiv.org/abs/2506.07280)
*Pablo Acuaviva,Aram Davtyan,Mariam Hassan,Sebastian Stapf,Ahmad Rahimi,Alexandre Alahi,Paolo Favaro*

Main category: cs.CV

TL;DR: 本文介绍利用少量样本微调视频扩散模型用于新任务，模型泛化能力强，表明其不仅是生成引擎，还可作视觉基础模型骨干。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型（VDMs）潜力不止于视频生成，为探究其内部知识的程度。

Method: 引入少样本微调框架，将任务转化为视觉过渡，在不改变冻结VDM生成接口下训练LoRA权重。

Result: 模型在从低级视觉到高级推理的多样任务中展现出强泛化能力。

Conclusion: VDMs不仅是生成引擎，还是有潜力成为未来视觉基础模型骨干的适应性视觉学习者。

Abstract: Video Diffusion Models (VDMs) have emerged as powerful generative tools,
capable of synthesizing high-quality spatiotemporal content. Yet, their
potential goes far beyond mere video generation. We argue that the training
dynamics of VDMs, driven by the need to model coherent sequences, naturally
pushes them to internalize structured representations and an implicit
understanding of the visual world. To probe the extent of this internal
knowledge, we introduce a few-shot fine-tuning framework that repurposes VDMs
for new tasks using only a handful of examples. Our method transforms each task
into a visual transition, enabling the training of LoRA weights on short
input-output sequences without altering the generative interface of a frozen
VDM. Despite minimal supervision, the model exhibits strong generalization
across diverse tasks, from low-level vision (for example, segmentation and pose
estimation) to high-level reasoning (for example, on ARC-AGI). These results
reframe VDMs as more than generative engines. They are adaptable visual
learners with the potential to serve as the backbone for future foundation
models in vision.

</details>


### [398] [NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery](https://arxiv.org/abs/2506.06898)
*Reese Kneeland,Paul S. Scotti,Ghislain St-Yves,Jesse Breedlove,Kendrick Kay,Thomas Naselaris*

Main category: cs.CV

TL;DR: 本文发布NSD - Imagery基准数据集用于评估模型对心理图像重建能力，发现解码方法在心理图像和视觉重建上性能解耦，简单架构模型对心理图像泛化更好。


<details>
  <summary>Details</summary>
Motivation: 现有基于NSD训练的模型仅在所见图像重建上评估，而心理图像重建在医疗和脑机接口等领域有重要应用，需评估模型对心理图像重建性能。

Method: 发布NSD - Imagery数据集，对一系列基于NSD训练的开源视觉解码模型在该数据集上进行基准测试。

Result: 解码方法在心理图像和视觉重建上性能解耦，简单线性解码架构和多模态特征解码的模型对心理图像泛化更好，复杂架构易过拟合视觉训练数据。

Conclusion: 心理图像数据集对实际应用开发至关重要，NSD - Imagery是使视觉解码方法更好达成目标的有用资源。

Abstract: We release NSD-Imagery, a benchmark dataset of human fMRI activity paired
with mental images, to complement the existing Natural Scenes Dataset (NSD), a
large-scale dataset of fMRI activity paired with seen images that enabled
unprecedented improvements in fMRI-to-image reconstruction efforts. Recent
models trained on NSD have been evaluated only on seen image reconstruction.
Using NSD-Imagery, it is possible to assess how well these models perform on
mental image reconstruction. This is a challenging generalization requirement
because mental images are encoded in human brain activity with relatively lower
signal-to-noise and spatial resolution; however, generalization from seen to
mental imagery is critical for real-world applications in medical domains and
brain-computer interfaces, where the desired information is always internally
generated. We provide benchmarks for a suite of recent NSD-trained open-source
visual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et
al.) on NSD-Imagery, and show that the performance of decoding methods on
mental images is largely decoupled from performance on vision reconstruction.
We further demonstrate that architectural choices significantly impact
cross-decoding performance: models employing simple linear decoding
architectures and multimodal feature decoding generalize better to mental
imagery, while complex architectures tend to overfit visual training data. Our
findings indicate that mental imagery datasets are critical for the development
of practical applications, and establish NSD-Imagery as a useful resource for
better aligning visual decoding methods with this goal.

</details>


### [399] [Multiple Object Stitching for Unsupervised Representation Learning](https://arxiv.org/abs/2506.07364)
*Chengchao Shen,Dawei Liu,Jianxin Wang*

Main category: cs.CV

TL;DR: 提出MOS方法提升多对象图像无监督表示性能，实验表明该方法在单对象和多对象图像上表现领先。


<details>
  <summary>Details</summary>
Motivation: 对比学习在多对象图像上性能不佳，需要提升多对象图像无监督表示效果。

Method: 提出Multiple Object Stitching (MOS)方法，通过拼接单对象图像构建多对象图像，提供额外对象对应关系。

Result: 在ImageNet、CIFAR和COCO数据集实验中，MOS方法在单对象和多对象图像上取得领先的无监督表示性能。

Conclusion: MOS方法简单有效，能为复杂下游任务提供更详细表示，提升多对象图像无监督表示效果。

Abstract: Contrastive learning for single object centric images has achieved remarkable
progress on unsupervised representation, but suffering inferior performance on
the widespread images with multiple objects. In this paper, we propose a simple
but effective method, Multiple Object Stitching (MOS), to refine the
unsupervised representation for multi-object images. Specifically, we construct
the multi-object images by stitching the single object centric ones, where the
objects in the synthesized multi-object images are predetermined. Hence,
compared to the existing contrastive methods, our method provides additional
object correspondences between multi-object images without human annotations.
In this manner, our method pays more attention to the representations of each
object in multi-object image, thus providing more detailed representations for
complicated downstream tasks, such as object detection and semantic
segmentation. Experimental results on ImageNet, CIFAR and COCO datasets
demonstrate that our proposed method achieves the leading unsupervised
representation performance on both single object centric images and
multi-object ones. The source code is available at
https://github.com/visresearch/MultipleObjectStitching.

</details>


### [400] [C3S3: Complementary Competition and Contrastive Selection for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.07368)
*Jiaying He,Yitong Lin,Jiahe Chen,Honghui Xu,Jianwei Zheng*

Main category: cs.CV

TL;DR: 提出半监督医学图像分割模型C3S3，结合互补竞争和对比选择，在公开数据集验证效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前半监督医学图像分割方法难以精确捕捉边界细节，导致诊断不准确。

Method: 引入C3S3模型，包含结果驱动对比学习模块和动态互补竞争模块。

Result: 在两个公开数据集上验证，相比现有方法表现更优，在95HD和ASD指标上至少提升6%。

Conclusion: C3S3模型能显著提升边界分割精度和整体分割质量。

Abstract: For the immanent challenge of insufficiently annotated samples in the medical
field, semi-supervised medical image segmentation (SSMIS) offers a promising
solution. Despite achieving impressive results in delineating primary target
areas, most current methodologies struggle to precisely capture the subtle
details of boundaries. This deficiency often leads to significant diagnostic
inaccuracies. To tackle this issue, we introduce C3S3, a novel semi-supervised
segmentation model that synergistically integrates complementary competition
and contrastive selection. This design significantly sharpens boundary
delineation and enhances overall precision. Specifically, we develop an
$\textit{Outcome-Driven Contrastive Learning}$ module dedicated to refining
boundary localization. Additionally, we incorporate a $\textit{Dynamic
Complementary Competition}$ module that leverages two high-performing
sub-networks to generate pseudo-labels, thereby further improving segmentation
quality. The proposed C3S3 undergoes rigorous validation on two publicly
accessible datasets, encompassing the practices of both MRI and CT scans. The
results demonstrate that our method achieves superior performance compared to
previous cutting-edge competitors. Especially, on the 95HD and ASD metrics, our
approach achieves a notable improvement of at least $6\%$, highlighting the
significant advancements. The code is available at
https://github.com/Y-TARL/C3S3.

</details>


### [401] [TABLET: Table Structure Recognition using Encoder-only Transformers](https://arxiv.org/abs/2506.07015)
*Qiyu Hou,Jun Wang*

Main category: cs.CV

TL;DR: 提出基于Split - Merge的自顶向下模型用于大而密集表格结构识别，实验证明优于现有方法，适合工业部署。


<details>
  <summary>Details</summary>
Motivation: 应对表格结构识别挑战，特别是大而密集表格的识别问题。

Method: 将行和列拆分作为序列标注任务，用双Transformer编码器捕捉特征交互；合并过程作为网格单元分类任务，用额外Transformer编码器确保准确合并，消除不稳定边界框预测。

Result: 在FinTabNet和PubTabNet上的实验表明该模型优于现有方法。

Conclusion: 该方法为大规模表格识别提供了强大、可扩展且高效的解决方案，适合工业部署。

Abstract: To address the challenges of table structure recognition, we propose a novel
Split-Merge-based top-down model optimized for large, densely populated tables.
Our approach formulates row and column splitting as sequence labeling tasks,
utilizing dual Transformer encoders to capture feature interactions. The
merging process is framed as a grid cell classification task, leveraging an
additional Transformer encoder to ensure accurate and coherent merging. By
eliminating unstable bounding box predictions, our method reduces resolution
loss and computational complexity, achieving high accuracy while maintaining
fast processing speed. Extensive experiments on FinTabNet and PubTabNet
demonstrate the superiority of our model over existing approaches, particularly
in real-world applications. Our method offers a robust, scalable, and efficient
solution for large-scale table recognition, making it well-suited for
industrial deployment.

</details>


### [402] [D2R: dual regularization loss with collaborative adversarial generation for model robustness](https://arxiv.org/abs/2506.07056)
*Zhenyu Liu,Huizhi Liang,Rajiv Ranjan,Zhanxing Zhu,Vaclav Snasel,Varun Ojha*

Main category: cs.CV

TL;DR: 本文提出D2R Loss和CAG策略用于对抗训练，实验表明二者结合能生成高鲁棒性模型。


<details>
  <summary>Details</summary>
Motivation: 现有增强深度神经网络模型对抗攻击鲁棒性的方法存在损失函数对目标模型指导不足和非协作对抗生成的问题。

Method: 提出D2R Loss方法，包含对抗分布和干净分布优化；提出CAG策略，通过指导模型和目标模型基于梯度协作生成对抗样本。

Result: 在三个基准数据库和两个流行目标模型上实验，D2R Loss与CAG结合产生高鲁棒性模型。

Conclusion: D2R Loss和CAG策略可有效增强模型对抗攻击的鲁棒性。

Abstract: The robustness of Deep Neural Network models is crucial for defending models
against adversarial attacks. Recent defense methods have employed collaborative
learning frameworks to enhance model robustness. Two key limitations of
existing methods are (i) insufficient guidance of the target model via loss
functions and (ii) non-collaborative adversarial generation. We, therefore,
propose a dual regularization loss (D2R Loss) method and a collaborative
adversarial generation (CAG) strategy for adversarial training. D2R loss
includes two optimization steps. The adversarial distribution and clean
distribution optimizations enhance the target model's robustness by leveraging
the strengths of different loss functions obtained via a suitable function
space exploration to focus more precisely on the target model's distribution.
CAG generates adversarial samples using a gradient-based collaboration between
guidance and target models. We conducted extensive experiments on three
benchmark databases, including CIFAR-10, CIFAR-100, Tiny ImageNet, and two
popular target models, WideResNet34-10 and PreActResNet18. Our results show
that D2R loss with CAG produces highly robust models.

</details>


### [403] [Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation](https://arxiv.org/abs/2506.07376)
*Jintao Tong,Ran Ma,Yixiong Zou,Guangyao Chen,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 本文针对跨域少样本分割任务，提出域特征导航器DFN和SAM - SVN方法，实验表明在1 - shot和5 - shot场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决跨域少样本分割任务中的域差距和少数据微调两大挑战。

Method: 提出基于结构的解耦器DFN捕获特定域信息，设计SAM - SVN方法约束DFN学习样本特定知识，在目标域冻结模型并微调DFN。

Result: 在1 - shot和5 - shot场景下，MIoU分别比现有最优方法提高2.69%和4.68%。

Conclusion: 所提方法在跨域少样本分割任务中表现优异，优于现有方法。

Abstract: Cross-domain few-shot segmentation (CD-FSS) is proposed to pre-train the
model on a source-domain dataset with sufficient samples, and then transfer the
model to target-domain datasets where only a few samples are available for
efficient fine-tuning. There are majorly two challenges in this task: (1) the
domain gap and (2) fine-tuning with scarce data. To solve these challenges, we
revisit the adapter-based methods, and discover an intriguing insight not
explored in previous works: the adapter not only helps the fine-tuning of
downstream tasks but also naturally serves as a domain information decoupler.
Then, we delve into this finding for an interpretation, and find the model's
inherent structure could lead to a natural decoupling of domain information.
Building upon this insight, we propose the Domain Feature Navigator (DFN),
which is a structure-based decoupler instead of loss-based ones like current
works, to capture domain-specific information, thereby directing the model's
attention towards domain-agnostic knowledge. Moreover, to prevent the potential
excessive overfitting of DFN during the source-domain training, we further
design the SAM-SVN method to constrain DFN from learning sample-specific
knowledge. On target domains, we freeze the model and fine-tune the DFN to
learn target-specific knowledge specific. Extensive experiments demonstrate
that our method surpasses the state-of-the-art method in CD-FSS significantly
by 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios, respectively.

</details>


### [404] [MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems](https://arxiv.org/abs/2506.07399)
*Peiru Yang,Jinhua Yin,Haoran Zheng,Xueying Bai,Huili Wang,Yufei Sun,Xintian Li,Shangguang Wang,Yongfeng Huang,Tao Qi*

Main category: cs.CV

TL;DR: 提出首个针对多模态RAG系统的黑盒MIA框架MrM，实验证明其性能强且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 多模态RAG系统存在隐私攻击风险，现有的MIA方法多关注文本模态，视觉模态研究不足。

Method: 提出MrM框架，采用多目标数据扰动框架，结合对象感知数据扰动方法、反事实信息掩码选择策略和统计成员推理。

Result: 在两个视觉数据集和八个主流商业视觉语言模型上实验，MrM在样本级和集合级评估中表现出色，且在自适应防御下保持鲁棒。

Conclusion: MrM能有效对多模态RAG系统进行成员推理攻击。

Abstract: Multimodal retrieval-augmented generation (RAG) systems enhance large
vision-language models by integrating cross-modal knowledge, enabling their
increasing adoption across real-world multimodal tasks. These knowledge
databases may contain sensitive information that requires privacy protection.
However, multimodal RAG systems inherently grant external users indirect access
to such data, making them potentially vulnerable to privacy attacks,
particularly membership inference attacks (MIAs). % Existing MIA methods
targeting RAG systems predominantly focus on the textual modality, while the
visual modality remains relatively underexplored. To bridge this gap, we
propose MrM, the first black-box MIA framework targeted at multimodal RAG
systems. It utilizes a multi-object data perturbation framework constrained by
counterfactual attacks, which can concurrently induce the RAG systems to
retrieve the target data and generate information that leaks the membership
information. Our method first employs an object-aware data perturbation method
to constrain the perturbation to key semantics and ensure successful retrieval.
Building on this, we design a counterfact-informed mask selection strategy to
prioritize the most informative masked regions, aiming to eliminate the
interference of model self-knowledge and amplify attack efficacy. Finally, we
perform statistical membership inference by modeling query trials to extract
features that reflect the reconstruction of masked semantics from response
patterns. Experiments on two visual datasets and eight mainstream commercial
visual-language models (e.g., GPT-4o, Gemini-2) demonstrate that MrM achieves
consistently strong performance across both sample-level and set-level
evaluations, and remains robust under adaptive defenses.

</details>


### [405] [FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using Feature-Aware Attention and Mamba Enhancement](https://arxiv.org/abs/2506.07431)
*Jie He,Minglang Chen,Minying Lu,Bocheng Liang,Junming Wei,Guiyan Peng,Jiaxi Chen,Ying Tan*

Main category: cs.CV

TL;DR: 提出基于特征感知和Mamba增强的胎儿股骨和颅骨超声图像分割模型FAMSeg，经实验验证有良好分割性能。


<details>
  <summary>Details</summary>
Motivation: 手动勾勒超声图像误差大、耗时，现有分割模型难适应高噪声和高相似度的超声对象，小物体分割有锯齿效应。

Method: 设计纵向和横向独立视角扫描卷积块与特征感知模块，结合Mamba优化残差结构，构建全局与局部特征依赖，用不同优化器组合训练。

Result: FAMSeg网络在不同大小和方向的图像上实现最快的损失降低和最佳的分割性能。

Conclusion: 所提FAMSeg模型能有效解决超声图像分割难题，有良好分割效果。

Abstract: Accurate ultrasound image segmentation is a prerequisite for precise
biometrics and accurate assessment. Relying on manual delineation introduces
significant errors and is time-consuming. However, existing segmentation models
are designed based on objects in natural scenes, making them difficult to adapt
to ultrasound objects with high noise and high similarity. This is particularly
evident in small object segmentation, where a pronounced jagged effect occurs.
Therefore, this paper proposes a fetal femur and cranial ultrasound image
segmentation model based on feature perception and Mamba enhancement to address
these challenges. Specifically, a longitudinal and transverse independent
viewpoint scanning convolution block and a feature perception module were
designed to enhance the ability to capture local detail information and improve
the fusion of contextual information. Combined with the Mamba-optimized
residual structure, this design suppresses the interference of raw noise and
enhances local multi-dimensional scanning. The system builds global information
and local feature dependencies, and is trained with a combination of different
optimizers to achieve the optimal solution. After extensive experimental
validation, the FAMSeg network achieved the fastest loss reduction and the best
segmentation performance across images of varying sizes and orientations.

</details>


### [406] [Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI](https://arxiv.org/abs/2506.07286)
*Aditya Chakravarty*

Main category: cs.CV

TL;DR: 本文提出在去噪时间步内采用多步优化策略，提升了扩散模型解决逆问题的图像质量、感知准确性和泛化能力，验证了其在不同场景的有效性和作为实时视觉感知恢复模块的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型如MPGD在去噪步骤仅进行单次梯度更新，限制了恢复保真度和鲁棒性，尤其是在嵌入式或分布外的场景。

Method: 在每个去噪时间步引入多步优化策略。

Result: 在超分辨率和高斯去模糊实验中，增加每步梯度更新次数可提高LPIPS和PSNR，且延迟开销极小；在Jetson Orin Nano上用退化的ImageNet和无人机数据集验证了MPGD从人脸数据集训练后能有效泛化到自然和航拍场景。

Conclusion: MPGD有潜力作为轻量级、即插即用的恢复模块用于具身AI代理（如无人机和移动机器人）的实时视觉感知。

Abstract: Diffusion models have shown remarkable flexibility for solving inverse
problems without task-specific retraining. However, existing approaches such as
Manifold Preserving Guided Diffusion (MPGD) apply only a single gradient update
per denoising step, limiting restoration fidelity and robustness, especially in
embedded or out-of-distribution settings. In this work, we introduce a
multistep optimization strategy within each denoising timestep, significantly
enhancing image quality, perceptual accuracy, and generalization. Our
experiments on super-resolution and Gaussian deblurring demonstrate that
increasing the number of gradient updates per step improves LPIPS and PSNR with
minimal latency overhead. Notably, we validate this approach on a Jetson Orin
Nano using degraded ImageNet and a UAV dataset, showing that MPGD, originally
trained on face datasets, generalizes effectively to natural and aerial scenes.
Our findings highlight MPGD's potential as a lightweight, plug-and-play
restoration module for real-time visual perception in embodied AI agents such
as drones and mobile robots.

</details>


### [407] [Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition](https://arxiv.org/abs/2506.07436)
*Nishi Chaudhary,S M Jamil Uddin,Sathvik Sharath Chandra,Anto Ovid,Alex Albert*

Main category: cs.CV

TL;DR: 本文对比评估五种多模态大语言模型在建筑安全视觉任务中的表现，发现思维链提示策略效果更好，部分模型表现更优，强调提示设计对建筑安全应用的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对不同大语言模型在建筑领域安全关键视觉任务中表现的探究，本文旨在填补这一空白。

Method: 对Claude - 3 Opus、GPT - 4.5等五种模型，采用零样本、少样本和思维链三种提示策略进行测试，用精确率、召回率和F1分数进行量化分析。

Result: 提示策略显著影响模型性能，思维链提示策略下各模型准确率更高；GPT - 4.5和GPT - o3在多数情况下表现更优。

Conclusion: 提示设计对提高多模态大语言模型在建筑安全应用中的准确性和一致性至关重要，研究为实际危险识别提供了见解，有助于开发更可靠的人工智能辅助安全系统。

Abstract: The recent emergence of multimodal large language models (LLMs) has
introduced new opportunities for improving visual hazard recognition on
construction sites. Unlike traditional computer vision models that rely on
domain-specific training and extensive datasets, modern LLMs can interpret and
describe complex visual scenes using simple natural language prompts. However,
despite growing interest in their applications, there has been limited
investigation into how different LLMs perform in safety-critical visual tasks
within the construction domain. To address this gap, this study conducts a
comparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5,
GPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify
potential hazards from real-world construction images. Each model was tested
under three prompting strategies: zero-shot, few-shot, and chain-of-thought
(CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated
basic safety context and a hazard source mnemonic, and CoT provided
step-by-step reasoning examples to scaffold model thinking. Quantitative
analysis was performed using precision, recall, and F1-score metrics across all
conditions. Results reveal that prompting strategy significantly influenced
performance, with CoT prompting consistently producing higher accuracy across
models. Additionally, LLM performance varied under different conditions, with
GPT-4.5 and GPT-o3 outperforming others in most settings. The findings also
demonstrate the critical role of prompt design in enhancing the accuracy and
consistency of multimodal LLMs for construction safety applications. This study
offers actionable insights into the integration of prompt engineering and LLMs
for practical hazard recognition, contributing to the development of more
reliable AI-assisted safety systems.

</details>


### [408] ["CASE: Contrastive Activation for Saliency Estimation](https://arxiv.org/abs/2506.07327)
*Dane Williamson,Yangfeng Ji,Matthew Dwyer*

Main category: cs.CV

TL;DR: 本文指出常用显著性方法存在类别不敏感问题，提出诊断测试并引入CASE方法，该方法解释更可靠且具类别特异性。


<details>
  <summary>Details</summary>
Motivation: 现有显著性方法视觉合理性掩盖关键局限，需检验类别敏感性并改进。

Method: 提出类别敏感性诊断测试，引入对比解释方法CASE，并使用诊断测试和基于扰动的保真度测试评估。

Result: 许多常用显著性方法对不同类别标签解释相近，CASE方法比现有方法解释更忠实且具类别特异性。

Conclusion: 常用显著性方法存在结构上的类别不敏感问题，CASE方法能有效解决，表现更优。

Abstract: Saliency methods are widely used to visualize which input features are deemed
relevant to a model's prediction. However, their visual plausibility can
obscure critical limitations. In this work, we propose a diagnostic test for
class sensitivity: a method's ability to distinguish between competing class
labels on the same input. Through extensive experiments, we show that many
widely used saliency methods produce nearly identical explanations regardless
of the class label, calling into question their reliability. We find that
class-insensitive behavior persists across architectures and datasets,
suggesting the failure mode is structural rather than model-specific. Motivated
by these findings, we introduce CASE, a contrastive explanation method that
isolates features uniquely discriminative for the predicted class. We evaluate
CASE using the proposed diagnostic and a perturbation-based fidelity test, and
show that it produces faithful and more class-specific explanations than
existing methods.

</details>


### [409] [DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO](https://arxiv.org/abs/2506.07464)
*Jinyoung Park,Jeehye Na,Jinyoung Kim,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: 本文探索将GRPO应用于视频大语言模型，指出问题并提出DeepVideo - R1模型，实验表明其提升了视频推理性能。


<details>
  <summary>Details</summary>
Motivation: 此前GRPO在视频大语言模型中的应用研究较少，且应用中存在依赖保障机制和优势消失问题，需改进。

Method: 提出Reg - GRPO将GRPO目标重新表述为回归任务，设计难度感知数据增强策略动态扩充可解决难度级别的训练样本。

Result: 综合实验显示DeepVideo - R1在多个视频推理基准测试中显著提升了视频推理性能。

Conclusion: 所提出的DeepVideo - R1模型及相关策略能有效提升视频大语言模型的视频推理能力。

Abstract: Recent works have demonstrated the effectiveness of reinforcement learning
(RL)-based post-training in enhancing the reasoning capabilities of large
language models (LLMs). In particular, Group Relative Policy Optimization
(GRPO) has shown impressive success by employing a PPO-style reinforcement
algorithm with group-based normalized rewards. However, the application of GRPO
to Video Large Language Models (Video LLMs) has been less studied. In this
paper, we explore GRPO for video LLMs and identify two primary issues that
impede its effective learning: (1) reliance on safeguards, and (2) the
vanishing advantage problem. To mitigate these challenges, we propose
DeepVideo-R1, a video large language model trained with our proposed Reg-GRPO
(Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO
reformulates the GRPO objective as a regression task, directly predicting the
advantage in GRPO. This design eliminates the need for safeguards like clipping
and min functions, thereby facilitating more direct policy guidance by aligning
the model with the advantage values. We also design the difficulty-aware data
augmentation strategy that dynamically augments training samples at solvable
difficulty levels, fostering diverse and informative reward signals. Our
comprehensive experiments show that DeepVideo-R1 significantly improves video
reasoning performance across multiple video reasoning benchmarks.

</details>


### [410] [CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection through Spatially Adaptive Attention Mechanisms](https://arxiv.org/abs/2506.07357)
*Satvik Praveen,Yoonsung Jung*

Main category: cs.CV

TL;DR: 本文提出CBAM - STN - TPS - YOLO模型用于精准农业目标检测，在PGP数据集上表现优于STN - YOLO，是适用于智能农业的轻量级模型。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测模型如YOLO在应对遮挡、不规则结构和背景噪声时检测精度降低，Spatial Transformer Networks的仿射映射无法处理非刚性变形。

Method: 提出CBAM - STN - TPS - YOLO模型，将Thin - Plate Splines集成到Spatial Transformer Networks中实现非刚性空间变换，并用Convolutional Block Attention Module增强性能。

Result: 在PGP数据集上，模型在精度、召回率和mAP上优于STN - YOLO，误报率降低12%，还研究了TPS正则化参数对检测性能的影响。

Conclusion: 该轻量级模型提升了空间感知能力，支持实时边缘部署，适合智能农业应用。

Abstract: Object detection is vital in precision agriculture for plant monitoring,
disease detection, and yield estimation. However, models like YOLO struggle
with occlusions, irregular structures, and background noise, reducing detection
accuracy. While Spatial Transformer Networks (STNs) improve spatial invariance
through learned transformations, affine mappings are insufficient for non-rigid
deformations such as bent leaves and overlaps.
  We propose CBAM-STN-TPS-YOLO, a model integrating Thin-Plate Splines (TPS)
into STNs for flexible, non-rigid spatial transformations that better align
features. Performance is further enhanced by the Convolutional Block Attention
Module (CBAM), which suppresses background noise and emphasizes relevant
spatial and channel-wise features.
  On the occlusion-heavy Plant Growth and Phenotyping (PGP) dataset, our model
outperforms STN-YOLO in precision, recall, and mAP. It achieves a 12% reduction
in false positives, highlighting the benefits of improved spatial flexibility
and attention-guided refinement. We also examine the impact of the TPS
regularization parameter in balancing transformation smoothness and detection
performance.
  This lightweight model improves spatial awareness and supports real-time edge
deployment, making it ideal for smart farming applications requiring accurate
and efficient monitoring.

</details>


### [411] [Ambiguity-Restrained Text-Video Representation Learning for Partially Relevant Video Retrieval](https://arxiv.org/abs/2506.07471)
*CH Cho,WJ Moon,W Jun,MS Jung,JP Heo*

Main category: cs.CV

TL;DR: 针对部分相关视频检索（PRVR）中典型训练假设局限性，提出包含模糊性的框架，经多种策略处理模糊文本 - 视频对，在 PRVR 中有效。


<details>
  <summary>Details</summary>
Motivation: 典型 PRVR 训练假设文本与视频是一对一关系，未考虑文本和视频内容间固有的模糊性，需改进。

Method: 提出模糊抑制表示学习（ARL），基于不确定性和相似性检测模糊对，通过多正对比学习和双三元组边界损失分层学习语义关系；处理同一未修剪视频帧内的模糊性；提出跨模型模糊检测。

Result: 所提方法在 PRVR 中展示出有效性。

Conclusion: 提出的考虑模糊性的方法在 PRVR 任务中可行且有效。

Abstract: Partially Relevant Video Retrieval~(PRVR) aims to retrieve a video where a
specific segment is relevant to a given text query. Typical training processes
of PRVR assume a one-to-one relationship where each text query is relevant to
only one video. However, we point out the inherent ambiguity between text and
video content based on their conceptual scope and propose a framework that
incorporates this ambiguity into the model learning process. Specifically, we
propose Ambiguity-Restrained representation Learning~(ARL) to address ambiguous
text-video pairs. Initially, ARL detects ambiguous pairs based on two criteria:
uncertainty and similarity. Uncertainty represents whether instances include
commonly shared context across the dataset, while similarity indicates
pair-wise semantic overlap. Then, with the detected ambiguous pairs, our ARL
hierarchically learns the semantic relationship via multi-positive contrastive
learning and dual triplet margin loss. Additionally, we delve into fine-grained
relationships within the video instances. Unlike typical training at the
text-video level, where pairwise information is provided, we address the
inherent ambiguity within frames of the same untrimmed video, which often
contains multiple contexts. This allows us to further enhance learning at the
text-frame level. Lastly, we propose cross-model ambiguity detection to
mitigate the error propagation that occurs when a single model is employed to
detect ambiguous pairs for its training. With all components combined, our
proposed method demonstrates its effectiveness in PRVR.

</details>


### [412] [CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization](https://arxiv.org/abs/2506.07484)
*Dasol Hong,Wooju Lee,Hyun Myung*

Main category: cs.CV

TL;DR: 提出CoCoA - Mix模型，含CoA - loss和CoA - weights，提升提示调优中特定任务专业化和泛化能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决提示调优中特定任务专业化和未见领域泛化问题，应对冻结编码器产生特征不对齐导致的类别混淆。

Method: 提出CoA - loss细化混淆类别的决策边界以提升专业化；使用CoA - weights调整混合模型中预测的权重，在不影响专业化的同时提升泛化能力。

Result: CoCoA - Mix模型在实验中表现优于现有方法，提升了专业化和泛化能力。

Conclusion: CoCoA - Mix能有效解决提示调优中的核心挑战，代码已公开。

Abstract: Prompt tuning, which adapts vision-language models by freezing model
parameters and optimizing only the prompt, has proven effective for
task-specific adaptations. The core challenge in prompt tuning is improving
specialization for a specific task and generalization for unseen domains.
However, frozen encoders often produce misaligned features, leading to
confusion between classes and limiting specialization. To overcome this issue,
we propose a confusion-aware loss (CoA-loss) that improves specialization by
refining the decision boundaries between confusing classes. Additionally, we
mathematically demonstrate that a mixture model can enhance generalization
without compromising specialization. This is achieved using confidence-aware
weights (CoA-weights), which adjust the weights of each prediction in the
mixture model based on its confidence within the class domains. Extensive
experiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights,
outperforms state-of-the-art methods by enhancing specialization and
generalization. Our code is publicly available at
https://github.com/url-kaist/CoCoA-Mix.

</details>


### [413] [Domain Randomization for Object Detection in Manufacturing Applications using Synthetic Data: A Comprehensive Study](https://arxiv.org/abs/2506.07539)
*Xiaomeng Zhu,Jacob Henningsson,Duruo Li,Pär Mårtensson,Lars Hanson,Mårten Björkman,Atsuto Maki*

Main category: cs.CV

TL;DR: 本文提出综合数据生成管道和SIP15 - OD数据集，研究域随机化在制造对象检测应用合成数据生成中的可行性与挑战，方法在公开数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决制造对象检测应用中合成数据生成的域随机化关键问题。

Method: 提出综合数据生成管道，引入SIP15 - OD数据集，使用公开工业数据集，利用材料属性、渲染方法等因素。

Result: 在公开数据集上使用仅基于合成数据训练的Yolov8模型取得优异成绩，如机器人数据集mAP@50达96.4%等。

Conclusion: 所提出的域随机化方法有效，可能覆盖接近实际数据的分布。

Abstract: This paper addresses key aspects of domain randomization in generating
synthetic data for manufacturing object detection applications. To this end, we
present a comprehensive data generation pipeline that reflects different
factors: object characteristics, background, illumination, camera settings, and
post-processing. We also introduce the Synthetic Industrial Parts Object
Detection dataset (SIP15-OD) consisting of 15 objects from three industrial use
cases under varying environments as a test bed for the study, while also
employing an industrial dataset publicly available for robotic applications. In
our experiments, we present more abundant results and insights into the
feasibility as well as challenges of sim-to-real object detection. In
particular, we identified material properties, rendering methods,
post-processing, and distractors as important factors. Our method, leveraging
these, achieves top performance on the public dataset with Yolov8 models
trained exclusively on synthetic data; mAP@50 scores of 96.4% for the robotics
dataset, and 94.1%, 99.5%, and 95.3% across three of the SIP15-OD use cases,
respectively. The results showcase the effectiveness of the proposed domain
randomization, potentially covering the distribution close to real data for the
applications.

</details>


### [414] [APTOS-2024 challenge report: Generation of synthetic 3D OCT images from fundus photographs](https://arxiv.org/abs/2506.07542)
*Bowen Liu,Weiyi Zhang,Peranut Chotcomwongse,Xiaolan Chen,Ruoyu Chen,Pawin Pakaymaskul,Niracha Arjkongharn,Nattaporn Vongsa,Xuelian Cheng,Zongyuan Ge,Kun Huang,Xiaohui Li,Yiru Duan,Zhenbang Wang,BaoYe Xie,Qiang Chen,Huazhu Fu,Michael A. Mahr,Jiaqi Qu,Wangyiyang Chen,Shiye Wang,Yubo Tan,Yongjie Li,Mingguang He,Danli Shi,Paisan Ruamviboonsuk*

Main category: cs.CV

TL;DR: 本文介绍APTOS - 2024挑战赛，其旨在推进眼底图像到3D - OCT图像生成模型，介绍挑战框架、参赛情况及领先方法，证明该合成的可行性。


<details>
  <summary>Details</summary>
Motivation: OCT设备成本高且依赖专业人员，2D眼底摄影虽易获取但数据维度等与OCT不同，生成模型在眼底到3D - OCT转换有挑战，故组织挑战赛推进相关模型。

Method: 组织APTOS - 2024挑战赛，设置基准数据集、采用两种保真度指标的评估方法，分析参赛队伍表现。

Result: 吸引342支队伍参赛，42个初步提交，9个决赛队伍，领先方法有混合数据预处理创新等。

Conclusion: APTOS - 2024挑战赛是首个证明眼底到3D - OCT合成可行性的基准，可改善医疗资源不足地区眼科护理可及性，加速医学研究和临床应用。

Abstract: Optical Coherence Tomography (OCT) provides high-resolution, 3D, and
non-invasive visualization of retinal layers in vivo, serving as a critical
tool for lesion localization and disease diagnosis. However, its widespread
adoption is limited by equipment costs and the need for specialized operators.
In comparison, 2D color fundus photography offers faster acquisition and
greater accessibility with less dependence on expensive devices. Although
generative artificial intelligence has demonstrated promising results in
medical image synthesis, translating 2D fundus images into 3D OCT images
presents unique challenges due to inherent differences in data dimensionality
and biological information between modalities. To advance generative models in
the fundus-to-3D-OCT setting, the Asia Pacific Tele-Ophthalmology Society
(APTOS-2024) organized a challenge titled Artificial Intelligence-based OCT
Generation from Fundus Images. This paper details the challenge framework
(referred to as APTOS-2024 Challenge), including: the benchmark dataset,
evaluation methodology featuring two fidelity metrics-image-based distance
(pixel-level OCT B-scan similarity) and video-based distance (semantic-level
volumetric consistency), and analysis of top-performing solutions. The
challenge attracted 342 participating teams, with 42 preliminary submissions
and 9 finalists. Leading methodologies incorporated innovations in hybrid data
preprocessing or augmentation (cross-modality collaborative paradigms),
pre-training on external ophthalmic imaging datasets, integration of vision
foundation models, and model architecture improvement. The APTOS-2024 Challenge
is the first benchmark demonstrating the feasibility of fundus-to-3D-OCT
synthesis as a potential solution for improving ophthalmic care accessibility
in under-resourced healthcare settings, while helping to expedite medical
research and clinical applications.

</details>


### [415] [Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries](https://arxiv.org/abs/2506.07555)
*Haoxiang Wang,Zinan Lin,Da Yu,Huishuai Zhang*

Main category: cs.CV

TL;DR: 提出SPTI方法生成高分辨率差分隐私合成图像，无需训练模型，效果优于现有方法，拓展了私有视觉数据集的访问。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私图像合成方法难以生成忠实捕捉原始数据结构的高分辨率输出，为解决该问题开展研究。

Method: 将差分隐私图像合成挑战从图像域转移到文本域，用图像转文本模型将图像总结为文本，用改进的Private Evolution算法生成差分隐私文本，再用文本转图像模型重建图像，仅使用现成模型推理。

Result: 在LSUN Bedroom和MM CelebA HQ数据集上，SPTI生成的合成图像质量明显高于现有差分隐私方法。

Conclusion: SPTI为生成高分辨率差分隐私合成图像提供了资源高效且与专有模型兼容的框架，极大拓展了私有视觉数据集的访问。

Abstract: Generating high fidelity, differentially private (DP) synthetic images offers
a promising route to share and analyze sensitive visual data without
compromising individual privacy. However, existing DP image synthesis methods
struggle to produce high resolution outputs that faithfully capture the
structure of the original data. In this paper, we introduce a novel method,
referred to as Synthesis via Private Textual Intermediaries (SPTI), that can
generate high resolution DP images with easy adoption. The key idea is to shift
the challenge of DP image synthesis from the image domain to the text domain by
leveraging state of the art DP text generation methods. SPTI first summarizes
each private image into a concise textual description using image to text
models, then applies a modified Private Evolution algorithm to generate DP
text, and finally reconstructs images using text to image models. Notably, SPTI
requires no model training, only inference with off the shelf models. Given a
private dataset, SPTI produces synthetic images of substantially higher quality
than prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID less
than or equal to 26.71 under epsilon equal to 1.0, improving over Private
Evolution FID of 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID less
than or equal to 33.27 at epsilon equal to 1.0, compared to 57.01 from DP fine
tuning baselines. Overall, our results demonstrate that Synthesis via Private
Textual Intermediaries provides a resource efficient and proprietary model
compatible framework for generating high resolution DP synthetic images,
greatly expanding access to private visual datasets.

</details>


### [416] [Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models](https://arxiv.org/abs/2506.07575)
*Ruiyang Zhang,Hu Zhang,Hao Fei,Zhedong Zheng*

Main category: cs.CV

TL;DR: 介绍Uncertainty - o框架评估大跨模态模型（LMMs）不确定性，实验证明其有效提升下游任务。


<details>
  <summary>Details</summary>
Motivation: 当前LMMs是否知道自身未知存在疑问，有评估不确定性、提示不确定性展示和量化下游任务不确定性三个关键问题待解决。

Method: 引入Uncertainty - o框架，进行多模态提示扰动的实证探索，推导多模态语义不确定性公式。

Result: 在18个基准测试和10个LMMs上实验，证明Uncertainty - o能可靠估计LMMs不确定性。

Conclusion: Uncertainty - o框架能有效提升幻觉检测、缓解和不确定性感知思维链推理等下游任务。

Abstract: Large Multimodal Models (LMMs), harnessing the complementarity among diverse
modalities, are often considered more robust than pure Language Large Models
(LLMs); yet do LMMs know what they do not know? There are three key open
questions remaining: (1) how to evaluate the uncertainty of diverse LMMs in a
unified manner, (2) how to prompt LMMs to show its uncertainty, and (3) how to
quantify uncertainty for downstream tasks. In an attempt to address these
challenges, we introduce Uncertainty-o: (1) a model-agnostic framework designed
to reveal uncertainty in LMMs regardless of their modalities, architectures, or
capabilities, (2) an empirical exploration of multimodal prompt perturbations
to uncover LMM uncertainty, offering insights and findings, and (3) derive the
formulation of multimodal semantic uncertainty, which enables quantifying
uncertainty from multimodal responses. Experiments across 18 benchmarks
spanning various modalities and 10 LMMs (both open- and closed-source)
demonstrate the effectiveness of Uncertainty-o in reliably estimating LMM
uncertainty, thereby enhancing downstream tasks such as hallucination
detection, hallucination mitigation, and uncertainty-aware Chain-of-Thought
reasoning.

</details>


### [417] [Explore the vulnerability of black-box models via diffusion models](https://arxiv.org/abs/2506.07590)
*Jiacheng Shi,Yanfu Zhang,Huajie Shao,Ashley Gao*

Main category: cs.CV

TL;DR: 研究揭示扩散模型新安全威胁，攻击者利用其API生成图像训练替代模型，可对黑盒分类模型攻击，方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽有进展，但存在安全和隐私风险，需研究新的安全威胁。

Method: 攻击者利用扩散模型API生成合成图像训练替代模型，对黑盒分类模型进行模型提取和基于转移的对抗攻击。

Result: 在七个基准测试中，相比现有技术平均提升27.37%，仅用0.01倍查询预算，对目标模型对抗攻击成功率达98.68%。

Conclusion: 该研究发现的新安全威胁值得关注，提出的方法在攻击效果和效率上表现出色。

Abstract: Recent advancements in diffusion models have enabled high-fidelity and
photorealistic image generation across diverse applications. However, these
models also present security and privacy risks, including copyright violations,
sensitive information leakage, and the creation of harmful or offensive content
that could be exploited maliciously. In this study, we uncover a novel security
threat where an attacker leverages diffusion model APIs to generate synthetic
images, which are then used to train a high-performing substitute model. This
enables the attacker to execute model extraction and transfer-based adversarial
attacks on black-box classification models with minimal queries, without
needing access to the original training data. The generated images are
sufficiently high-resolution and diverse to train a substitute model whose
outputs closely match those of the target model. Across the seven benchmarks,
including CIFAR and ImageNet subsets, our method shows an average improvement
of 27.37% over state-of-the-art methods while using just 0.01 times of the
query budget, achieving a 98.68% success rate in adversarial attacks on the
target model.

</details>


### [418] [LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization](https://arxiv.org/abs/2506.07570)
*Yixuan Yang,Zhen Luo,Tongsheng Ding,Junru Lu,Mingqi Gao,Jinyu Yang,Victor Sanchez,Feng Zheng*

Main category: cs.CV

TL;DR: 本文提出3D - SynthPlace数据集和用于室内布局生成的开源大模型OptiScene，实验表明其优于传统方法且在交互任务有潜力。


<details>
  <summary>Details</summary>
Motivation: 现有室内布局生成方法存在空间不一致、计算成本高、关系图粗糙和数据集有限等问题，限制了对不同房间类别的泛化能力。

Method: 创建3D - SynthPlace数据集，通过两阶段训练微调得到OptiScene，第一阶段采用监督微调，第二阶段应用多轮直接偏好优化。

Result: OptiScene在实验中优于传统的基于提示和基于学习的基线方法。

Conclusion: OptiScene具有良好的室内布局生成效果，在场景编辑和机器人导航等交互任务中有应用潜力。

Abstract: Automatic indoor layout generation has attracted increasing attention due to
its potential in interior design, virtual environment construction, and
embodied AI. Existing methods fall into two categories: prompt-driven
approaches that leverage proprietary LLM services (e.g., GPT APIs) and
learning-based methods trained on layout data upon diffusion-based models.
Prompt-driven methods often suffer from spatial inconsistency and high
computational costs, while learning-based methods are typically constrained by
coarse relational graphs and limited datasets, restricting their generalization
to diverse room categories. In this paper, we revisit LLM-based indoor layout
generation and present 3D-SynthPlace, a large-scale dataset that combines
synthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline,
upgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000
scenes, covering four common room types -- bedroom, living room, kitchen, and
bathroom -- enriched with diverse objects and high-level spatial annotations.
We further introduce OptiScene, a strong open-source LLM optimized for indoor
layout generation, fine-tuned based on our 3D-SynthPlace dataset through our
two-stage training. For the warum-up stage I, we adopt supervised fine-tuning
(SFT), which is taught to first generate high-level spatial descriptions then
conditionally predict concrete object placements. For the reinforcing stage II,
to better align the generated layouts with human design preferences, we apply
multi-turn direct preference optimization (DPO), which significantly improving
layout quality and generation success rates. Extensive experiments demonstrate
that OptiScene outperforms traditional prompt-driven and learning-based
baselines. Moreover, OptiScene shows promising potential in interactive tasks
such as scene editing and robot navigation.

</details>


### [419] [HieraEdgeNet: A Multi-Scale Edge-Enhanced Framework for Automated Pollen Recognition](https://arxiv.org/abs/2506.07637)
*Yuchong Long,Wen Sun,Ningxiao Sun,Wenxiao Wang,Chao Li,Shan Yin*

Main category: cs.CV

TL;DR: 本文提出多尺度边缘增强框架HieraEdgeNet用于花粉自动识别，在大规模数据集上表现出色，能高效精确检测微观物体。


<details>
  <summary>Details</summary>
Motivation: 传统方法在花粉自动识别中效率低且主观，现有深度学习模型对花粉等微观目标定位精度不足。

Method: 引入HieraEdgeNet框架，包含Hierarchical Edge Module、Synergistic Edge Fusion模块和Cross Stage Partial Omni - Kernel Module。

Result: 在含120个花粉类别的大规模数据集上，mAP@.5达0.9501，远超YOLOv12n和RT - DETR等模型，定性分析表明能更精准聚焦物体边界。

Conclusion: HieraEdgeNet通过系统集成边缘信息，为微观物体的高精度、高效率自动检测提供了强大解决方案。

Abstract: Automated pollen recognition is vital to paleoclimatology, biodiversity
monitoring, and public health, yet conventional methods are hampered by
inefficiency and subjectivity. Existing deep learning models often struggle to
achieve the requisite localization accuracy for microscopic targets like
pollen, which are characterized by their minute size, indistinct edges, and
complex backgrounds. To overcome this limitation, we introduce HieraEdgeNet, a
multi-scale edge-enhancement framework. The framework's core innovation is the
introduction of three synergistic modules: the Hierarchical Edge Module (HEM),
which explicitly extracts a multi-scale pyramid of edge features that
corresponds to the semantic hierarchy at early network stages; the Synergistic
Edge Fusion (SEF) module, for deeply fusing these edge priors with semantic
information at each respective scale; and the Cross Stage Partial Omni-Kernel
Module (CSPOKM), which maximally refines the most detail-rich feature layers
using an Omni-Kernel operator - comprising anisotropic large-kernel
convolutions and mixed-domain attention - all within a computationally
efficient Cross-Stage Partial (CSP) framework. On a large-scale dataset
comprising 120 pollen classes, HieraEdgeNet achieves a mean Average Precision
(mAP@.5) of 0.9501, significantly outperforming state-of-the-art baseline
models such as YOLOv12n and RT-DETR. Furthermore, qualitative analysis confirms
that our approach generates feature representations that are more precisely
focused on object boundaries. By systematically integrating edge information,
HieraEdgeNet provides a robust and powerful solution for high-precision,
high-efficiency automated detection of microscopic objects.

</details>


### [420] [SceneRAG: Scene-level Retrieval-Augmented Generation for Video Understanding](https://arxiv.org/abs/2506.07600)
*Nianbo Zeng,Haowen Hou,Fei Richard Yu,Si Shi,Ying Tiffany He*

Main category: cs.CV

TL;DR: 提出SceneRAG框架用于视频理解，在LongerVideos基准测试中表现优于先前基线。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）方法在理解长视频内容方面不足，固定长度分块破坏上下文信息连续性。

Method: 提出SceneRAG框架，利用大语言模型结合ASR转录和时间元数据将视频分割成叙事连贯的场景，通过轻量级启发式和迭代校正优化边界，融合视觉和文本信息构建知识图。

Result: 在LongerVideos基准测试中，SceneRAG大幅优于先前基线，在生成任务上胜率达72.5%。

Conclusion: SceneRAG框架能有效解决长视频理解问题，表现出色。

Abstract: Despite recent advances in retrieval-augmented generation (RAG) for video
understanding, effectively understanding long-form video content remains
underexplored due to the vast scale and high complexity of video data. Current
RAG approaches typically segment videos into fixed-length chunks, which often
disrupts the continuity of contextual information and fails to capture
authentic scene boundaries. Inspired by the human ability to naturally organize
continuous experiences into coherent scenes, we present SceneRAG, a unified
framework that leverages large language models to segment videos into
narrative-consistent scenes by processing ASR transcripts alongside temporal
metadata. SceneRAG further sharpens these initial boundaries through
lightweight heuristics and iterative correction. For each scene, the framework
fuses information from both visual and textual modalities to extract entity
relations and dynamically builds a knowledge graph, enabling robust multi-hop
retrieval and generation that account for long-range dependencies. Experiments
on the LongerVideos benchmark, featuring over 134 hours of diverse content,
confirm that SceneRAG substantially outperforms prior baselines, achieving a
win rate of up to 72.5 percent on generation tasks.

</details>


### [421] [SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis](https://arxiv.org/abs/2506.07603)
*Jianhui Wei,Zikai Xiao,Danyu Sun,Luqi Gong,Zongxin Yang,Zuozhu Liu,Jian Wu*

Main category: cs.CV

TL;DR: 本文介绍统一手术视频基准框架SurgBench，含预训练数据集SurgBench - P和评估基准SurgBench - E，实验表明预训练在SurgBench - P可提升性能和跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开发手术视频基础模型受限于缺乏大规模、多样化预训练数据集和系统评估，为解决此问题开展研究。

Method: 引入统一手术视频基准框架SurgBench，含预训练数据集SurgBench - P和评估基准SurgBench - E。

Result: 现有视频基础模型难在不同手术视频分析任务泛化，在SurgBench - P上预训练能显著提升性能和跨域泛化能力。

Conclusion: SurgBench框架及其数据集对手术视频分析任务有重要作用，有助于解决当前手术视频基础模型发展的问题。

Abstract: Surgical video understanding is pivotal for enabling automated intraoperative
decision-making, skill assessment, and postoperative quality improvement.
However, progress in developing surgical video foundation models (FMs) remains
hindered by the scarcity of large-scale, diverse datasets for pretraining and
systematic evaluation. In this paper, we introduce \textbf{SurgBench}, a
unified surgical video benchmarking framework comprising a pretraining dataset,
\textbf{SurgBench-P}, and an evaluation benchmark, \textbf{SurgBench-E}.
SurgBench offers extensive coverage of diverse surgical scenarios, with
SurgBench-P encompassing 53 million frames across 22 surgical procedures and 11
specialties, and SurgBench-E providing robust evaluation across six categories
(phase classification, camera motion, tool recognition, disease diagnosis,
action classification, and organ detection) spanning 72 fine-grained tasks.
Extensive experiments reveal that existing video FMs struggle to generalize
across varied surgical video analysis tasks, whereas pretraining on SurgBench-P
yields substantial performance improvements and superior cross-domain
generalization to unseen procedures and modalities. Our dataset and code are
available upon request.

</details>


### [422] [FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly Supervised Lesion Segmentation in Medical Images](https://arxiv.org/abs/2506.07652)
*Hangbei Cheng,Xiaorong Dong,Xueyu Liu,Jianan Zhang,Xuetao Ma,Mingqiang Wei,Liansheng Wang,Junxin Chen,Yongfei Wu*

Main category: cs.CV

TL;DR: 提出FMaMIL框架用于仅基于图像级标签的弱监督病变分割，实验表明其优于现有弱监督方法。


<details>
  <summary>Details</summary>
Motivation: 组织病理学图像中准确的病变分割因像素级注释有限而具有挑战性，需仅基于图像级标签的方法。

Method: 提出两阶段FMaMIL框架，第一阶段用轻量级Mamba编码器和频域编码模块，第二阶段用CAM引导的软标签监督和自校正机制。

Result: 在公共和私有组织病理学数据集上的实验表明，FMaMIL在不依赖像素级注释的情况下优于现有弱监督方法。

Conclusion: FMaMIL有效，在数字病理学应用中有潜力。

Abstract: Accurate lesion segmentation in histopathology images is essential for
diagnostic interpretation and quantitative analysis, yet it remains challenging
due to the limited availability of costly pixel-level annotations. To address
this, we propose FMaMIL, a novel two-stage framework for weakly supervised
lesion segmentation based solely on image-level labels. In the first stage, a
lightweight Mamba-based encoder is introduced to capture long-range
dependencies across image patches under the MIL paradigm. To enhance spatial
sensitivity and structural awareness, we design a learnable frequency-domain
encoding module that supplements spatial-domain features with spectrum-based
information. CAMs generated in this stage are used to guide segmentation
training. In the second stage, we refine the initial pseudo labels via a
CAM-guided soft-label supervision and a self-correction mechanism, enabling
robust training even under label noise. Extensive experiments on both public
and private histopathology datasets demonstrate that FMaMIL outperforms
state-of-the-art weakly supervised methods without relying on pixel-level
annotations, validating its effectiveness and potential for digital pathology
applications.

</details>


### [423] [NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation](https://arxiv.org/abs/2506.07698)
*Yuxiao Yang,Peihao Li,Yuhong Zhang,Junzhe Lu,Xianglong He,Minghan Qin,Weitao Wang,Haoqian Wang*

Main category: cs.CV

TL;DR: 介绍NOVA3D单图像到3D生成框架，利用视频扩散模型3D先验和几何信息，有GTA机制和去冲突几何融合算法，实验验证其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有从预训练图像扩散模型提取3D对象的方法存在3D先验不足、多视图一致性不够的问题。

Method: 引入NOVA3D框架，利用预训练视频扩散模型的3D先验，在多视图视频微调中整合几何信息，提出GTA注意力机制和去冲突几何融合算法。

Result: 广泛实验验证了NOVA3D优于现有基线。

Conclusion: NOVA3D能有效改善3D生成中的多视图一致性和纹理保真度等问题。

Abstract: 3D AI-generated content (AIGC) has made it increasingly accessible for anyone
to become a 3D content creator. While recent methods leverage Score
Distillation Sampling to distill 3D objects from pretrained image diffusion
models, they often suffer from inadequate 3D priors, leading to insufficient
multi-view consistency. In this work, we introduce NOVA3D, an innovative
single-image-to-3D generation framework. Our key insight lies in leveraging
strong 3D priors from a pretrained video diffusion model and integrating
geometric information during multi-view video fine-tuning. To facilitate
information exchange between color and geometric domains, we propose the
Geometry-Temporal Alignment (GTA) attention mechanism, thereby improving
generalization and multi-view consistency. Moreover, we introduce the
de-conflict geometry fusion algorithm, which improves texture fidelity by
addressing multi-view inaccuracies and resolving discrepancies in pose
alignment. Extensive experiments validate the superiority of NOVA3D over
existing baselines.

</details>


### [424] [Trend-Aware Fashion Recommendation with Visual Segmentation and Semantic Similarity](https://arxiv.org/abs/2506.07773)
*Mohamed Djilani,Nassim Ali Ousalah,Nidhal Eddine Chenni*

Main category: cs.CV

TL;DR: 介绍了一种趋势感知和视觉基础的时尚推荐系统，在DeepFashion数据集实验有良好效果，提供可扩展框架。


<details>
  <summary>Details</summary>
Motivation: 构建能平衡个人风格与新兴趋势的个性化时尚推荐系统。

Method: 整合深度视觉表征、服装感知分割、语义类别相似度和用户行为模拟；通过语义分割提取聚焦视觉嵌入；生成受用户趋势和物品流行度影响的合成购买历史；用加权评分函数计算推荐。

Result: 在DeepFashion数据集实验显示出一致的性别对齐和改进的类别相关性，ResNet - 50实现64.95%的类别相似度和最低流行度MAE；消融研究证实视觉和流行度线索的互补作用。

Conclusion: 该方法提供了一个可扩展的个性化时尚推荐框架，平衡了个人风格与新兴趋势。

Abstract: We introduce a trend-aware and visually-grounded fashion recommendation
system that integrates deep visual representations, garment-aware segmentation,
semantic category similarity and user behavior simulation. Our pipeline
extracts focused visual embeddings by masking non-garment regions via semantic
segmentation followed by feature extraction using pretrained CNN backbones
(ResNet-50, DenseNet-121, VGG16). To simulate realistic shopping behavior, we
generate synthetic purchase histories influenced by user-specific trendiness
and item popularity. Recommendations are computed using a weighted scoring
function that fuses visual similarity, semantic coherence and popularity
alignment. Experiments on the DeepFashion dataset demonstrate consistent gender
alignment and improved category relevance, with ResNet-50 achieving 64.95%
category similarity and lowest popularity MAE. An ablation study confirms the
complementary roles of visual and popularity cues. Our method provides a
scalable framework for personalized fashion recommendations that balances
individual style with emerging trends. Our implementation is available at
https://github.com/meddjilani/FashionRecommender

</details>


### [425] [Consistent Video Editing as Flow-Driven Image-to-Video Generation](https://arxiv.org/abs/2506.07713)
*Ge Wang,Songlin Fan,Hangxu Liu,Quanjian Song,Hewei Wang,Jinfeng Xu*

Main category: cs.CV

TL;DR: 视频扩散模型促进视频编辑发展，但现有方法在复杂运动建模上有局限。本文提出FlowV2V将视频编辑作为流驱动的I2V生成任务，实验证明其效果好。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法无法建模复杂运动模式，对非刚性物体运动任务考虑不足。

Method: 提出FlowV2V，将流程分解为首帧编辑和条件I2V生成，模拟与变形形状对齐的伪流序列。

Result: 在DAVIS - EDIT上，DOVER和翘曲误差分别提升13.67%和50.66%，证明FlowV2V在时间一致性和样本质量上优于现有方法。

Conclusion: FlowV2V能有效解决视频编辑中复杂运动建模问题，提升视频编辑效果。

Abstract: With the prosper of video diffusion models, down-stream applications like
video editing have been significantly promoted without consuming much
computational cost. One particular challenge in this task lies at the motion
transfer process from the source video to the edited one, where it requires the
consideration of the shape deformation in between, meanwhile maintaining the
temporal consistency in the generated video sequence. However, existing methods
fail to model complicated motion patterns for video editing, and are
fundamentally limited to object replacement, where tasks with non-rigid object
motions like multi-object and portrait editing are largely neglected. In this
paper, we observe that optical flows offer a promising alternative in complex
motion modeling, and present FlowV2V to re-investigate video editing as a task
of flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V
decomposes the entire pipeline into first-frame editing and conditional I2V
generation, and simulates pseudo flow sequence that aligns with the deformed
shape, thus ensuring the consistency during editing. Experimental results on
DAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error
illustrate the superior temporal consistency and sample quality of FlowV2V
compared to existing state-of-the-art ones. Furthermore, we conduct
comprehensive ablation studies to analyze the internal functionalities of the
first-frame paradigm and flow alignment in the proposed method.

</details>


### [426] [Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger](https://arxiv.org/abs/2506.07785)
*Qi Yang,Chenghao Zhang,Lubin Fan,Kun Ding,Jieping Ye,Shiming Xiang*

Main category: cs.CV

TL;DR: 提出RCTS框架提升LVLMs在VQA任务表现，实验效果佳且代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有多模态RAG方法在VQA任务中存在推理知识稀缺、检索响应不稳定的问题。

Method: 提出RCTS框架，构建推理上下文丰富的知识库，用自洽评估机制丰富知识库，提出MCTS - HR方法对示例排序。

Result: 在多个VQA数据集上达到SOTA，显著优于ICL和Vanilla - RAG方法。

Conclusion: 所提知识库和重排序方法能有效提升LVLMs性能。

Abstract: Recent advancements in Large Vision Language Models (LVLMs) have
significantly improved performance in Visual Question Answering (VQA) tasks
through multimodal Retrieval-Augmented Generation (RAG). However, existing
methods still face challenges, such as the scarcity of knowledge with reasoning
examples and erratic responses from retrieved knowledge. To address these
issues, in this study, we propose a multimodal RAG framework, termed RCTS,
which enhances LVLMs by constructing a Reasoning Context-enriched knowledge
base and a Tree Search re-ranking method. Specifically, we introduce a
self-consistent evaluation mechanism to enrich the knowledge base with
intrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with
Heuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This
ensures that LVLMs can leverage high-quality contextual reasoning for better
and more consistent responses. Extensive experiments demonstrate that our
framework achieves state-of-the-art performance on multiple VQA datasets,
significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods.
It highlights the effectiveness of our knowledge base and re-ranking method in
improving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.

</details>


### [427] [ETA: Efficiency through Thinking Ahead, A Dual Approach to Self-Driving with Large Models](https://arxiv.org/abs/2506.07725)
*Shadi Hamdan,Chonghao Sima,Zetong Yang,Hongyang Li,Fatma Güney*

Main category: cs.CV

TL;DR: 提出ETA异步系统解决自动驾驶中利用大模型时推理速度慢的问题，在Bench2Drive CARLA Leaderboard - v2基准测试中提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统中在不牺牲推理速度的情况下利用大模型的难题，现有双系统设计难以让大模型对每个在线帧及时响应。

Method: 引入ETA异步系统，通过利用大模型未来预测传播过去信息特征、用小模型提取当前帧特征和动作掩码机制整合双特征。

Result: 在Bench2Drive CARLA Leaderboard - v2基准测试中，驾驶得分69.53，相比现有技术提升8%，推理速度达50 ms接近实时。

Conclusion: ETA系统可有效解决自动驾驶中利用大模型时的速度问题，提升系统性能。

Abstract: How can we benefit from large models without sacrificing inference speed, a
common dilemma in self-driving systems? A prevalent solution is a dual-system
architecture, employing a small model for rapid, reactive decisions and a
larger model for slower but more informative analyses. Existing dual-system
designs often implement parallel architectures where inference is either
directly conducted using the large model at each current frame or retrieved
from previously stored inference results. However, these works still struggle
to enable large models for a timely response to every online frame. Our key
insight is to shift intensive computations of the current frame to previous
time steps and perform a batch inference of multiple time steps to make large
models respond promptly to each time step. To achieve the shifting, we
introduce Efficiency through Thinking Ahead (ETA), an asynchronous system
designed to: (1) propagate informative features from the past to the current
frame using future predictions from the large model, (2) extract current frame
features using a small model for real-time responsiveness, and (3) integrate
these dual features via an action mask mechanism that emphasizes
action-critical image regions. Evaluated on the Bench2Drive CARLA
Leaderboard-v2 benchmark, ETA advances state-of-the-art performance by 8% with
a driving score of 69.53 while maintaining a near-real-time inference speed at
50 ms.

</details>


### [428] [ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models](https://arxiv.org/abs/2506.07739)
*Jing Zhong,Jun Yin,Peilin Li,Pengyu Zeng,Miao Zhang,Shuai Lu,Ran Luo*

Main category: cs.CV

TL;DR: 本文构建ArchDiffBench数据集和ArchiLense分析框架，用于自动识别和分类建筑图像风格，性能良好且更客观准确。


<details>
  <summary>Details</summary>
Motivation: 传统建筑文化研究依赖主观专家解读和历史文献回顾，存在区域偏见和解释范围有限的问题。

Method: 构建ArchDiffBench数据集，基于该数据集构建基于视觉 - 语言模型的ArchiLense分析框架，集成计算机视觉、深度学习和机器学习算法。

Result: ArchiLense在建筑风格识别中表现出色，与专家标注一致性达92.4%，分类准确率达84.5%。

Conclusion: 该方法超越传统分析的主观性，为建筑文化比较研究提供更客观准确的视角。

Abstract: Architectural cultures across regions are characterized by stylistic
diversity, shaped by historical, social, and technological contexts in addition
to geograph-ical conditions. Understanding architectural styles requires the
ability to describe and analyze the stylistic features of different architects
from various regions through visual observations of architectural imagery.
However, traditional studies of architectural culture have largely relied on
subjective expert interpretations and historical literature reviews, often
suffering from regional biases and limited ex-planatory scope. To address these
challenges, this study proposes three core contributions: (1) We construct a
professional architectural style dataset named ArchDiffBench, which comprises
1,765 high-quality architectural images and their corresponding style
annotations, collected from different regions and historical periods. (2) We
propose ArchiLense, an analytical framework grounded in Vision-Language Models
and constructed using the ArchDiffBench dataset. By integrating ad-vanced
computer vision techniques, deep learning, and machine learning algo-rithms,
ArchiLense enables automatic recognition, comparison, and precise
classi-fication of architectural imagery, producing descriptive language
outputs that ar-ticulate stylistic differences. (3) Extensive evaluations show
that ArchiLense achieves strong performance in architectural style recognition,
with a 92.4% con-sistency rate with expert annotations and 84.5% classification
accuracy, effec-tively capturing stylistic distinctions across images. The
proposed approach transcends the subjectivity inherent in traditional analyses
and offers a more objective and accurate perspective for comparative studies of
architectural culture.

</details>


### [429] [R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation](https://arxiv.org/abs/2506.07826)
*William Ljungbergh,Bernardo Taveira,Wenzhao Zheng,Adam Tonderski,Chensheng Peng,Fredrik Kahl,Christoffer Petersson,Michael Felsberg,Kurt Keutzer,Masayoshi Tomizuka,Wei Zhan*

Main category: cs.CV

TL;DR: 本文提出R3D2模型，用于在自动驾驶模拟中实现3D资产真实插入，提升场景真实感和可扩展性，并将发布数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 传统模拟平台扩展性差、存在领域差距，神经重建方法在动态对象操作和可复用性上有不足，需要新方法解决自动驾驶验证中场景模拟问题。

Method: 引入轻量级一步扩散模型R3D2，在新数据集上训练，该数据集用图像条件3D生成模型从真实自动驾驶数据生成3DGS对象资产并合成放置到基于神经渲染的虚拟环境。

Result: 定量和定性评估表明R3D2显著提升插入资产的真实感，实现文本到3D资产插入和跨场景/数据集对象转移。

Conclusion: R3D2能实现自动驾驶验证的真正可扩展性，作者将发布数据集和代码以促进相关研究。

Abstract: Validating autonomous driving (AD) systems requires diverse and
safety-critical testing, making photorealistic virtual environments essential.
Traditional simulation platforms, while controllable, are resource-intensive to
scale and often suffer from a domain gap with real-world data. In contrast,
neural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a
scalable solution for creating photorealistic digital twins of real-world
driving scenes. However, they struggle with dynamic object manipulation and
reusability as their per-scene optimization-based methodology tends to result
in incomplete object models with integrated illumination effects. This paper
introduces R3D2, a lightweight, one-step diffusion model designed to overcome
these limitations and enable realistic insertion of complete 3D assets into
existing scenes by generating plausible rendering effects-such as shadows and
consistent lighting-in real time. This is achieved by training R3D2 on a novel
dataset: 3DGS object assets are generated from in-the-wild AD data using an
image-conditioned 3D generative model, and then synthetically placed into
neural rendering-based virtual environments, allowing R3D2 to learn realistic
integration. Quantitative and qualitative evaluations demonstrate that R3D2
significantly enhances the realism of inserted assets, enabling use-cases like
text-to-3D asset insertion and cross-scene/dataset object transfer, allowing
for true scalability in AD validation. To promote further research in scalable
and realistic AD simulation, we will release our dataset and code, see
https://research.zenseact.com/publications/R3D2/.

</details>


### [430] [LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds](https://arxiv.org/abs/2506.07857)
*Zihui Zhang,Weisheng Dai,Hongtao Wen,Bo Yang*

Main category: cs.CV

TL;DR: 提出LogoSP方法用于无监督3D语义分割，在多数据集上超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督3D语义分割方法缺乏发现局部特征外语义先验的能力。

Method: 引入LogoSP，通过在频域按全局模式对超点分组发现3D语义信息，生成语义伪标签训练分割网络。

Result: 在两个室内和一个室外数据集上大幅超越现有无监督方法，达无监督3D语义分割的最优性能。

Conclusion: 所学习的全局模式在无人工标签训练时能代表有意义的3D语义。

Abstract: We study the problem of unsupervised 3D semantic segmentation on raw point
clouds without needing human labels in training. Existing methods usually
formulate this problem into learning per-point local features followed by a
simple grouping strategy, lacking the ability to discover additional and
possibly richer semantic priors beyond local features. In this paper, we
introduce LogoSP to learn 3D semantics from both local and global point
features. The key to our approach is to discover 3D semantic information by
grouping superpoints according to their global patterns in the frequency
domain, thus generating highly accurate semantic pseudo-labels for training a
segmentation network. Extensive experiments on two indoor and an outdoor
datasets show that our LogoSP surpasses all existing unsupervised methods by
large margins, achieving the state-of-the-art performance for unsupervised 3D
semantic segmentation. Notably, our investigation into the learned global
patterns reveals that they truly represent meaningful 3D semantics in the
absence of human labels during training.

</details>


### [431] [Self-Cascaded Diffusion Models for Arbitrary-Scale Image Super-Resolution](https://arxiv.org/abs/2506.07813)
*Junseo Bang,Joonhee Lee,Kyeonghyun Lee,Haechang Lee,Dong Un Kang,Se Young Chun*

Main category: cs.CV

TL;DR: 提出CasArbi框架用于任意尺度图像超分辨率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单阶段上采样方法难学习宽范围连续缩放因子，渐进上采样与扩散模型结合未充分探索。

Method: 提出CasArbi自级联扩散框架，将缩放需求分解为小的顺序因子逐步提升分辨率，采用坐标引导残差扩散模型。

Result: 在多种任意尺度超分辨率基准测试中，CasArbi在感知和失真性能指标上优于现有方法。

Conclusion: CasArbi框架在任意尺度图像超分辨率任务中表现出色。

Abstract: Arbitrary-scale image super-resolution aims to upsample images to any desired
resolution, offering greater flexibility than traditional fixed-scale
super-resolution. Recent approaches in this domain utilize regression-based or
generative models, but many of them are a single-stage upsampling process,
which may be challenging to learn across a wide, continuous distribution of
scaling factors. Progressive upsampling strategies have shown promise in
mitigating this issue, yet their integration with diffusion models for flexible
upscaling remains underexplored. Here, we present CasArbi, a novel
self-cascaded diffusion framework for arbitrary-scale image super-resolution.
CasArbi meets the varying scaling demands by breaking them down into smaller
sequential factors and progressively enhancing the image resolution at each
step with seamless transitions for arbitrary scales. Our novel
coordinate-guided residual diffusion model allows for the learning of
continuous image representations while enabling efficient diffusion sampling.
Extensive experiments demonstrate that our CasArbi outperforms prior arts in
both perceptual and distortion performance metrics across diverse
arbitrary-scale super-resolution benchmarks.

</details>


### [432] [VIVAT: Virtuous Improving VAE Training through Artifact Mitigation](https://arxiv.org/abs/2506.07863)
*Lev Novitskiy,Viacheslav Vasilev,Maria Kovaleva,Vladimir Arkhipkin,Denis Dimitrov*

Main category: cs.CV

TL;DR: 本文提出VIVAT方法缓解KL - VAE训练中的常见伪影，不改变架构，改进VAE性能。


<details>
  <summary>Details</summary>
Motivation: Variational Autoencoders (VAEs)训练常受伪影困扰，影响重建和生成质量。

Method: 详细分类五种常见伪影并分析原因，通过调整损失权重、填充策略和集成Spatially Conditional Normalization等简单修改来改进。

Result: 在多个基准测试的图像重建指标（PSNR和SSIM）上取得了最先进的结果，提升了文本到图像的生成质量（CLIP分数更优）。

Conclusion: VIVAT在保留KL - VAE框架简单性的同时解决实际挑战，为优化VAE训练提供了可行见解。

Abstract: Variational Autoencoders (VAEs) remain a cornerstone of generative computer
vision, yet their training is often plagued by artifacts that degrade
reconstruction and generation quality. This paper introduces VIVAT, a
systematic approach to mitigating common artifacts in KL-VAE training without
requiring radical architectural changes. We present a detailed taxonomy of five
prevalent artifacts - color shift, grid patterns, blur, corner and droplet
artifacts - and analyze their root causes. Through straightforward
modifications, including adjustments to loss weights, padding strategies, and
the integration of Spatially Conditional Normalization, we demonstrate
significant improvements in VAE performance. Our method achieves
state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across
multiple benchmarks and enhances text-to-image generation quality, as evidenced
by superior CLIP scores. By preserving the simplicity of the KL-VAE framework
while addressing its practical challenges, VIVAT offers actionable insights for
researchers and practitioners aiming to optimize VAE training.

</details>


### [433] [Diffusion models under low-noise regime](https://arxiv.org/abs/2506.07841)
*Elizabeth Pavlova,Xue-Xin Wei*

Main category: cs.CV

TL;DR: 研究扩散模型在低噪声扩散动力学下的行为，揭示训练不相交数据的模型在数据流形附近的差异等，为理解生成模型可靠性提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有工作在高噪声设置下测试了扩散模型的记忆和泛化机制，但低噪声下作为有效去噪器的行为尚不清楚，需要填补这一空白。

Method: 使用不同样本大小的CelebA子集和解析高斯混合基准进行研究。

Result: 发现训练不相交数据的模型在高噪声输出收敛时，在数据流形附近会出现差异，量化了训练集大小、数据几何和模型目标选择对去噪轨迹和分数准确性的影响。

Conclusion: 开始解决在常见小扰动的实际应用中对生成模型可靠性理解的差距。

Abstract: Recent work on diffusion models proposed that they operate in two regimes:
memorization, in which models reproduce their training data, and
generalization, in which they generate novel samples. While this has been
tested in high-noise settings, the behavior of diffusion models as effective
denoisers when the corruption level is small remains unclear. To address this
gap, we systematically investigated the behavior of diffusion models under
low-noise diffusion dynamics, with implications for model robustness and
interpretability. Using (i) CelebA subsets of varying sample sizes and (ii)
analytic Gaussian mixture benchmarks, we reveal that models trained on disjoint
data diverge near the data manifold even when their high-noise outputs
converge. We quantify how training set size, data geometry, and model objective
choice shape denoising trajectories and affect score accuracy, providing
insights into how these models actually learn representations of data
distributions. This work starts to address gaps in our understanding of
generative model reliability in practical applications where small
perturbations are common.

</details>


### [434] [A Comparative Study of U-Net Architectures for Change Detection in Satellite Images](https://arxiv.org/abs/2506.07925)
*Yaxita Amin,Naimisha S Trivedi,Rashmi Bhattad*

Main category: cs.CV

TL;DR: 本文分析34篇论文，对比18种U - Net变体用于遥感变化检测的潜力，为相关人员提供选择U - Net版本的见解。


<details>
  <summary>Details</summary>
Motivation: U - Net在遥感领域应用未充分探索，需全面分析其在遥感变化检测中的应用。

Method: 对34篇论文进行综合分析，对比18种U - Net变体，评估其优缺点。

Result: 强调如Siamese Swin - U - Net等专为变化检测设计的变体，突出处理不同时期数据和长距离关系对提高检测精度的重要性。

Conclusion: 为选择U - Net版本进行遥感变化检测任务的研究人员和从业者提供有价值的见解。

Abstract: Remote sensing change detection is essential for monitoring the everchanging
landscapes of the Earth. The U-Net architecture has gained popularity for its
capability to capture spatial information and perform pixel-wise
classification. However, their application in the Remote sensing field remains
largely unexplored. Therefore, this paper fill the gap by conducting a
comprehensive analysis of 34 papers. This study conducts a comparison and
analysis of 18 different U-Net variations, assessing their potential for
detecting changes in remote sensing. We evaluate both benefits along with
drawbacks of each variation within the framework of this particular
application. We emphasize variations that are explicitly built for change
detection, such as Siamese Swin-U-Net, which utilizes a Siamese architecture.
The analysis highlights the significance of aspects such as managing data from
different time periods and collecting relationships over a long distance to
enhance the precision of change detection. This study provides valuable
insights for researchers and practitioners that choose U-Net versions for
remote sensing change detection tasks.

</details>


### [435] [PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement](https://arxiv.org/abs/2506.07848)
*Teng Hu,Zhentao Yu,Zhengguang Zhou,Jiangning Zhang,Yuan Zhou,Qinglin Lu,Ran Yi*

Main category: cs.CV

TL;DR: 提出多主体视频定制框架PolyVivid，实验表明其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型缺乏细粒度可控性，尤其是多主体定制时身份和交互的一致性。

Method: 设计基于VLLM的文本 - 图像融合模块；提出基于3D - RoPE的增强模块；开发注意力继承身份注入模块；构建基于MLLM的数据管道。

Result: PolyVivid在身份保真度、视频真实感和主体对齐方面表现出色，优于现有开源和商业基线。

Conclusion: PolyVivid能实现灵活且身份一致的多主体视频生成。

Abstract: Despite recent advances in video generation, existing models still lack
fine-grained controllability, especially for multi-subject customization with
consistent identity and interaction. In this paper, we propose PolyVivid, a
multi-subject video customization framework that enables flexible and
identity-consistent generation. To establish accurate correspondences between
subject images and textual entities, we design a VLLM-based text-image fusion
module that embeds visual identities into the textual space for precise
grounding. To further enhance identity preservation and subject interaction, we
propose a 3D-RoPE-based enhancement module that enables structured
bidirectional fusion between text and image embeddings. Moreover, we develop an
attention-inherited identity injection module to effectively inject fused
identity features into the video generation process, mitigating identity drift.
Finally, we construct an MLLM-based data pipeline that combines MLLM-based
grounding, segmentation, and a clique-based subject consolidation strategy to
produce high-quality multi-subject data, effectively enhancing subject
distinction and reducing ambiguity in downstream video generation. Extensive
experiments demonstrate that PolyVivid achieves superior performance in
identity fidelity, video realism, and subject alignment, outperforming existing
open-source and commercial baselines.

</details>


### [436] [Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models](https://arxiv.org/abs/2506.07936)
*Chengyue Huang,Yuchen Zhu,Sichen Zhu,Jingyun Xiao,Moises Andrade,Shivang Chopra,Zsolt Kira*

Main category: cs.CV

TL;DR: 本文评估跨分布转移下视觉语言模型的上下文学习能力，发现性能常随示例增多而下降且模型倾向复制答案，提出含推理的新多模态上下文学习管道，实验表明当前模型未有效利用示例信息。


<details>
  <summary>Details</summary>
Motivation: 验证视觉语言模型是否真正具备多模态上下文学习能力，当前模型可能依赖浅层启发式而非真正理解任务。

Method: 在分布转移情况下评估视觉语言模型，提出带推理的多模态上下文学习管道，对不同规模开源模型和专有模型进行广泛实验，控制示例数量、检索方法等因素。

Result: 性能常随示例增多而下降，模型倾向复制答案；各因素对性能影响有限。

Conclusion: 当前视觉语言模型未有效利用多模态上下文学习中的示例信息。

Abstract: Vision-language models (VLMs) are widely assumed to exhibit in-context
learning (ICL), a property similar to that of their language-only counterparts.
While recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies
show they often rely on shallow heuristics -- such as copying or majority
voting -- rather than true task understanding. We revisit this assumption by
evaluating VLMs under distribution shifts, where support examples come from a
dataset different from the query. Surprisingly, performance often degrades with
more demonstrations, and models tend to copy answers rather than learn from
them. To investigate further, we propose a new MM-ICL with Reasoning pipeline
that augments each demonstration with a generated rationale alongside the
answer. We conduct extensive and comprehensive experiments on both perception-
and reasoning-required datasets with open-source VLMs ranging from 3B to 72B
and proprietary models such as Gemini 2.0. We conduct controlled studies
varying shot count, retrieval method, rationale quality, and distribution. Our
results show limited performance sensitivity across these factors, suggesting
that current VLMs do not effectively utilize demonstration-level information as
intended in MM-ICL.

</details>


### [437] [Real-time Localization of a Soccer Ball from a Single Camera](https://arxiv.org/abs/2506.07981)
*Dmitrii Vorobev,Artem Prosvetov,Karim Elhadji Daou*

Main category: cs.CV

TL;DR: 提出从单台转播摄像机实时重建足球三维轨迹的高效计算方法，评估显示性能佳。


<details>
  <summary>Details</summary>
Motivation: 以往方法可能存在计算效率不高、在复杂场景精度不足等问题，需要一种适用于专业足球环境、能实时且准确进行3D球跟踪的实用方法。

Method: 引入具有W个离散模式的多模式状态模型，在标准CPU上运行。

Result: 在6K分辨率俄超联赛专有数据集上评估，性能与多摄像机系统相当，无需专用或昂贵基础设施。

Conclusion: 该方法为专业足球环境提供了可实现且准确的3D球跟踪实用方法。

Abstract: We propose a computationally efficient method for real-time three-dimensional
football trajectory reconstruction from a single broadcast camera. In contrast
to previous work, our approach introduces a multi-mode state model with $W$
discrete modes to significantly accelerate optimization while preserving
centimeter-level accuracy -- even in cases of severe occlusion, motion blur,
and complex backgrounds. The system operates on standard CPUs and achieves low
latency suitable for live broadcast settings. Extensive evaluation on a
proprietary dataset of 6K-resolution Russian Premier League matches
demonstrates performance comparable to multi-camera systems, without the need
for specialized or costly infrastructure. This work provides a practical method
for accessible and accurate 3D ball tracking in professional football
environments.

</details>


### [438] [CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray](https://arxiv.org/abs/2506.07984)
*Mingquan Lin,Gregory Holste,Song Wang,Yiliang Zhou,Yishu Wei,Imon Banerjee,Pengyi Chen,Tianjie Dai,Yuexi Du,Nicha C. Dvornek,Yuyan Ge,Zuowei Guo,Shouhei Hanaoka,Dongkyun Kim,Pablo Messina,Yang Lu,Denis Parra,Donghyun Son,Álvaro Soto,Aisha Urooj,René Vidal,Yosuke Yamagishi,Zefan Yang,Ruichi Zhang,Yang Zhou,Leo Anthony Celi,Ronald M. Summers,Zhiyong Lu,Hao Chen,Adam Flanders,George Shih,Zhangyang Wang,Yifan Peng*

Main category: cs.CV

TL;DR: 介绍CXR - LT系列计划，详述CXR - LT 2024的改进、任务、方法及意义，旨在推动胸部X光诊断模型发展。


<details>
  <summary>Details</summary>
Motivation: 解决开放长尾肺部疾病分类挑战，提高现有技术可测量性，推动临床实用且通用的胸部X光诊断模型发展。

Method: 扩大数据集，引入零样本学习，设置三项任务，整合先进解决方案如多模态模型、生成式方法和零样本学习策略。

Result: CXR - LT 2024扩大数据集至377,110张胸部X光片和45种疾病标签，涵盖19种新罕见病发现。

Conclusion: 扩展数据集能更好代表真实临床情况，为未来研究提供有价值资源，有望推动胸部X光诊断模型发展。

Abstract: The CXR-LT series is a community-driven initiative designed to enhance lung
disease classification using chest X-rays (CXR). It tackles challenges in open
long-tailed lung disease classification and enhances the measurability of
state-of-the-art techniques. The first event, CXR-LT 2023, aimed to achieve
these goals by providing high-quality benchmark CXR data for model development
and conducting comprehensive evaluations to identify ongoing issues impacting
lung disease classification performance. Building on the success of CXR-LT
2023, the CXR-LT 2024 expands the dataset to 377,110 chest X-rays (CXRs) and 45
disease labels, including 19 new rare disease findings. It also introduces a
new focus on zero-shot learning to address limitations identified in the
previous event. Specifically, CXR-LT 2024 features three tasks: (i) long-tailed
classification on a large, noisy test set, (ii) long-tailed classification on a
manually annotated "gold standard" subset, and (iii) zero-shot generalization
to five previously unseen disease findings. This paper provides an overview of
CXR-LT 2024, detailing the data curation process and consolidating
state-of-the-art solutions, including the use of multimodal models for rare
disease detection, advanced generative approaches to handle noisy labels, and
zero-shot learning strategies for unseen diseases. Additionally, the expanded
dataset enhances disease coverage to better represent real-world clinical
settings, offering a valuable resource for future research. By synthesizing the
insights and innovations of participating teams, we aim to advance the
development of clinically realistic and generalizable diagnostic models for
chest radiography.

</details>


### [439] [Rethinking Crowd-Sourced Evaluation of Neuron Explanations](https://arxiv.org/abs/2506.07985)
*Tuomas Oikarinen,Ge Yan,Akshay Kulkarni,Tsui-Wei Weng*

Main category: cs.CV

TL;DR: 本文提出具成本效益且高精度的众包评估策略，用重要性采样降成本，贝叶斯方法聚合评级，还对比不同视觉模型神经元解释方法质量。


<details>
  <summary>Details</summary>
Motivation: 现有自动神经元解释算法可靠性和优劣不明，众包评估有噪声且成本高，结果不可靠。

Method: 分析评估流程，引入重要性采样确定最有价值输入，提出贝叶斯方法聚合评级。

Result: 重要性采样使成本降低约30倍，贝叶斯方法使达到相同精度所需评级数减少约5倍。

Conclusion: 所提策略成本效益高、精度高，可用于大规模对比不同视觉模型神经元解释方法质量。

Abstract: Interpreting individual neurons or directions in activations space is an
important component of mechanistic interpretability. As such, many algorithms
have been proposed to automatically produce neuron explanations, but it is
often not clear how reliable these explanations are, or which methods produce
the best explanations. This can be measured via crowd-sourced evaluations, but
they can often be noisy and expensive, leading to unreliable results. In this
paper, we carefully analyze the evaluation pipeline and develop a
cost-effective and highly accurate crowdsourced evaluation strategy. In
contrast to previous human studies that only rate whether the explanation
matches the most highly activating inputs, we estimate whether the explanation
describes neuron activations across all inputs. To estimate this effectively,
we introduce a novel application of importance sampling to determine which
inputs are the most valuable to show to raters, leading to around 30x cost
reduction compared to uniform sampling. We also analyze the label noise present
in crowd-sourced evaluations and propose a Bayesian method to aggregate
multiple ratings leading to a further ~5x reduction in number of ratings
required for the same accuracy. Finally, we use these methods to conduct a
large-scale study comparing the quality of neuron explanations produced by the
most popular methods for two different vision models.

</details>


### [440] [MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation](https://arxiv.org/abs/2506.07999)
*Junhao Chen,Yulia Tsvetkov,Xiaochuang Han*

Main category: cs.CV

TL;DR: 本文介绍MADFormer分析自回归与扩散模型权衡，实验有两关键发现并为混合生成模型提供设计原则。


<details>
  <summary>Details</summary>
Motivation: 现有自回归和扩散混合模型缺乏分配模型容量的系统指导。

Method: 引入MADFormer，将图像生成划分为空间块，用自回归层进行块间全局调节，扩散层进行块内局部细化。

Result: 块划分显著提升高分辨率图像性能，垂直混合自回归和扩散层提高质量效率平衡，受限推理计算下FID最多提升75%。

Conclusion: 研究结果为未来混合生成模型提供实用设计原则。

Abstract: Recent progress in multimodal generation has increasingly combined
autoregressive (AR) and diffusion-based approaches, leveraging their
complementary strengths: AR models capture long-range dependencies and produce
fluent, context-aware outputs, while diffusion models operate in continuous
latent spaces to refine high-fidelity visual details. However, existing hybrids
often lack systematic guidance on how and why to allocate model capacity
between these paradigms. In this work, we introduce MADFormer, a Mixed
Autoregressive and Diffusion Transformer that serves as a testbed for analyzing
AR-diffusion trade-offs. MADFormer partitions image generation into spatial
blocks, using AR layers for one-pass global conditioning across blocks and
diffusion layers for iterative local refinement within each block. Through
controlled experiments on FFHQ-1024 and ImageNet, we identify two key insights:
(1) block-wise partitioning significantly improves performance on
high-resolution images, and (2) vertically mixing AR and diffusion layers
yields better quality-efficiency balances--improving FID by up to 75% under
constrained inference compute. Our findings offer practical design principles
for future hybrid generative models.

</details>


### [441] [Hidden in plain sight: VLMs overlook their visual representations](https://arxiv.org/abs/2506.08008)
*Stephanie Fu,Tyler Bonnen,Devin Guillory,Trevor Darrell*

Main category: cs.CV

TL;DR: 本文对比视觉语言模型（VLMs）和其视觉编码器在视觉基准测试中的表现，分析VLMs性能不佳的原因并提出诊断方法。


<details>
  <summary>Details</summary>
Motivation: 了解VLMs整合视觉和语言信息的能力，诊断其在视觉任务中的失败模式。

Method: 对比VLMs和其视觉编码器在一系列以视觉为中心的基准测试中的表现，并对VLMs进行多方面分析。

Result: VLMs在以视觉为中心的基准测试中表现远不如其视觉编码器，性能接近随机水平，瓶颈在于语言模型未能有效利用视觉信息并继承了语言先验。

Conclusion: 有助于诊断开源VLMs的失败模式，为未来研究VLMs中的视觉理解提供评估方法。

Abstract: Language provides a natural interface to specify and evaluate performance on
visual tasks. To realize this possibility, vision language models (VLMs) must
successfully integrate visual and linguistic information. Our work compares
VLMs to a direct readout of their visual encoders to understand their ability
to integrate across these modalities. Across a series of vision-centric
benchmarks (e.g., depth estimation, correspondence), we find that VLMs perform
substantially worse than their visual encoders, dropping to near-chance
performance. We investigate these results through a series of analyses across
the entire VLM: namely 1) the degradation of vision representations, 2)
brittleness to task prompt, and 3) the language model's role in solving the
task. We find that the bottleneck in performing these vision-centric tasks lies
in this third category; VLMs are not effectively using visual information
easily accessible throughout the entire model, and they inherit the language
priors present in the LLM. Our work helps diagnose the failure modes of
open-source VLMs, and presents a series of evaluations useful for future
investigations into visual understanding within VLMs.

</details>


### [442] [Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion](https://arxiv.org/abs/2506.08009)
*Xun Huang,Zhengqi Li,Guande He,Mingyuan Zhou,Eli Shechtman*

Main category: cs.CV

TL;DR: 提出Self Forcing训练范式用于自回归视频扩散模型，解决曝光偏差问题，实现实时视频生成且质量高。


<details>
  <summary>Details</summary>
Motivation: 解决自回归视频扩散模型中存在的曝光偏差问题。

Method: 采用自回归滚动结合键值（KV）缓存，在训练时让每帧基于自身生成的输出进行生成；使用几步扩散模型和随机梯度截断策略保证训练效率；引入滚动KV缓存机制实现高效外推。

Result: 能在单GPU上实现亚秒级延迟的实时流式视频生成，生成质量匹配甚至超越更慢的非因果扩散模型。

Conclusion: Self Forcing训练范式有效解决曝光偏差问题，在效率和质量上表现优异。

Abstract: We introduce Self Forcing, a novel training paradigm for autoregressive video
diffusion models. It addresses the longstanding issue of exposure bias, where
models trained on ground-truth context must generate sequences conditioned on
their own imperfect outputs during inference. Unlike prior methods that denoise
future frames based on ground-truth context frames, Self Forcing conditions
each frame's generation on previously self-generated outputs by performing
autoregressive rollout with key-value (KV) caching during training. This
strategy enables supervision through a holistic loss at the video level that
directly evaluates the quality of the entire generated sequence, rather than
relying solely on traditional frame-wise objectives. To ensure training
efficiency, we employ a few-step diffusion model along with a stochastic
gradient truncation strategy, effectively balancing computational cost and
performance. We further introduce a rolling KV cache mechanism that enables
efficient autoregressive video extrapolation. Extensive experiments demonstrate
that our approach achieves real-time streaming video generation with sub-second
latency on a single GPU, while matching or even surpassing the generation
quality of significantly slower and non-causal diffusion models. Project
website: http://self-forcing.github.io/

</details>


### [443] [StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets](https://arxiv.org/abs/2506.08013)
*Anh-Quan Cao,Ivan Lopes,Raoul de Charette*

Main category: cs.CV

TL;DR: 利用扩散模型泛化能力，提出StableMTL方法用于零样本多任务学习，在多基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 多任务密集预测学习需大量标注，现有部分任务标签训练方法有局限，要扩展到零样本设置。

Method: 将图像生成器用于潜在回归，采用带任务编码的去噪框架、按任务调节和定制训练方案，使用统一潜在损失，引入带任务注意力机制的多流模型。

Result: StableMTL在8个基准测试的7个任务上优于基线。

Conclusion: 提出的StableMTL方法有效，能无缝扩展到更多任务，促进任务间协同。

Abstract: Multi-task learning for dense prediction is limited by the need for extensive
annotation for every task, though recent works have explored training with
partial task labels. Leveraging the generalization power of diffusion models,
we extend the partial learning setup to a zero-shot setting, training a
multi-task model on multiple synthetic datasets, each labeled for only a subset
of tasks. Our method, StableMTL, repurposes image generators for latent
regression. Adapting a denoising framework with task encoding, per-task
conditioning and a tailored training scheme. Instead of per-task losses
requiring careful balancing, a unified latent loss is adopted, enabling
seamless scaling to more tasks. To encourage inter-task synergy, we introduce a
multi-stream model with a task-attention mechanism that converts N-to-N task
interactions into efficient 1-to-N attention, promoting effective cross-task
sharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.

</details>


### [444] [Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations](https://arxiv.org/abs/2506.07943)
*Yizhen Li,Dell Zhang,Xuelong Li,Yiqing Shen*

Main category: cs.CV

TL;DR: 提出DTwinSeger用于推理分割任务，将感知与推理解耦，实验表现达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前推理分割方法对图像进行标记会破坏对象间空间关系，需改进。

Method: 引入数字孪生（DT）表示作为中间层，将推理分割重构为两阶段过程，提出针对DT表示的LLM监督微调方法及数据集Seg - DT。

Result: 在两个图像推理分割基准和三个图像指称分割基准上达到SOTA性能。

Conclusion: DT表示可作为视觉和文本的有效桥梁，仅用LLM就能完成复杂多模态推理任务。

Abstract: Reasoning Segmentation (RS) is a multimodal vision-text task that requires
segmenting objects based on implicit text queries, demanding both precise
visual perception and vision-text reasoning capabilities. Current RS approaches
rely on fine-tuning vision-language models (VLMs) for both perception and
reasoning, but their tokenization of images fundamentally disrupts continuous
spatial relationships between objects. We introduce DTwinSeger, a novel RS
approach that leverages Digital Twin (DT) representation as an intermediate
layer to decouple perception from reasoning. Innovatively, DTwinSeger
reformulates RS as a two-stage process, where the first transforms the image
into a structured DT representation that preserves spatial relationships and
semantic properties and then employs a Large Language Model (LLM) to perform
explicit reasoning over this representation to identify target objects. We
propose a supervised fine-tuning method specifically for LLM with DT
representation, together with a corresponding fine-tuning dataset Seg-DT, to
enhance the LLM's reasoning capabilities with DT representations. Experiments
show that our method can achieve state-of-the-art performance on two image RS
benchmarks and three image referring segmentation benchmarks. It yields that DT
representation functions as an effective bridge between vision and text,
enabling complex multimodal reasoning tasks to be accomplished solely with an
LLM.

</details>


### [445] [SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design](https://arxiv.org/abs/2506.07964)
*Wenxin Tang,Jingyu Xiao,Wenxuan Jiang,Xi Xiao,Yuhang Wang,Xuxin Tang,Qing Li,Yuehe Ma,Junliang Liu,Shisong Tang,Michael R. Lyu*

Main category: cs.CV

TL;DR: 本文提出解决手动制作幻灯片难题的方法，包括新基准、框架和模型，实验显示框架性能优异。


<details>
  <summary>Details</summary>
Motivation: 手动制作幻灯片费力且需专业知识，现有基于自然语言的大语言模型生成方法难以捕捉幻灯片设计的视觉和结构细微差别。

Method: 提出基于幻灯片复杂度指标的基准Slide2Code，引入布局感知、检索增强的框架SlideCoder，还发布微调后的开源模型SlideMaster。

Result: SlideCoder在布局保真度、执行准确性和视觉一致性方面优于现有基线模型，最高领先40.5分。

Conclusion: 所提出的方法能有效解决幻灯片生成问题，且代码已开源。

Abstract: Manual slide creation is labor-intensive and requires expert prior knowledge.
Existing natural language-based LLM generation methods struggle to capture the
visual and structural nuances of slide designs. To address this, we formalize
the Reference Image to Slide Generation task and propose Slide2Code, the first
benchmark with difficulty-tiered samples based on a novel Slide Complexity
Metric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework
for generating editable slides from reference images. SlideCoder integrates a
Color Gradient-based Segmentation algorithm and a Hierarchical
Retrieval-Augmented Generation method to decompose complex tasks and enhance
code generation. We also release SlideMaster, a 7B open-source model fine-tuned
with improved reverse-engineered data. Experiments show that SlideCoder
outperforms state-of-the-art baselines by up to 40.5 points, demonstrating
strong performance across layout fidelity, execution accuracy, and visual
consistency. Our code is available at
https://github.com/vinsontang1/SlideCoder.

</details>


### [446] [Audio-Sync Video Generation with Multi-Stream Temporal Control](https://arxiv.org/abs/2506.08003)
*Shuchen Weng,Haojie Zheng,Zheng Chang,Si Li,Boxin Shi,Xinlong Wang*

Main category: cs.CV

TL;DR: 本文提出MTV框架用于音频同步视频生成，还推出DEMIX数据集，实验表明MTV在多指标上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成高质量、音视频精确同步的视频方面不足，特别是针对多样复杂音频类型。

Method: 提出MTV框架，将音频分离为语音、效果和音乐轨道，实现对唇部动作、事件时间和视觉氛围的解耦控制；推出DEMIX数据集，分为五个重叠子集以支持多阶段训练。

Result: MTV在视频质量、文本 - 视频一致性和音频 - 视频对齐等六个标准指标上达到了最先进的性能。

Conclusion: MTV框架和DEMIX数据集在音频同步视频生成方面有良好效果，能提升视频生成质量和音视频同步度。

Abstract: Audio is inherently temporal and closely synchronized with the visual world,
making it a naturally aligned and expressive control signal for controllable
video generation (e.g., movies). Beyond control, directly translating audio
into video is essential for understanding and visualizing rich audio narratives
(e.g., Podcasts or historical recordings). However, existing approaches fall
short in generating high-quality videos with precise audio-visual
synchronization, especially across diverse and complex audio types. In this
work, we introduce MTV, a versatile framework for audio-sync video generation.
MTV explicitly separates audios into speech, effects, and music tracks,
enabling disentangled control over lip motion, event timing, and visual mood,
respectively -- resulting in fine-grained and semantically aligned video
generation. To support the framework, we additionally present DEMIX, a dataset
comprising high-quality cinematic videos and demixed audio tracks. DEMIX is
structured into five overlapped subsets, enabling scalable multi-stage training
for diverse generation scenarios. Extensive experiments demonstrate that MTV
achieves state-of-the-art performance across six standard metrics spanning
video quality, text-video consistency, and audio-video alignment. Project page:
https://hjzheng.net/projects/MTV/.

</details>


### [447] [Dynamic View Synthesis as an Inverse Problem](https://arxiv.org/abs/2506.08004)
*Hidir Yesiltepe,Pinar Yanardag*

Main category: cs.CV

TL;DR: 本文在免训练设置下将单目视频的动态视图合成作为逆问题处理，通过重新设计预训练视频扩散模型的噪声初始化阶段实现高保真合成。


<details>
  <summary>Details</summary>
Motivation: 解决免训练设置下单目视频动态视图合成问题。

Method: 重新设计预训练视频扩散模型的噪声初始化阶段，引入K阶递归噪声表示解决确定性反演障碍，引入随机潜在调制合成新可见区域。

Result: 综合实验表明可通过噪声初始化阶段的结构化潜在操作有效进行动态视图合成。

Conclusion: 通过重新设计噪声初始化阶段的结构化潜在操作，能在免训练设置下实现高保真动态视图合成。

Abstract: In this work, we address dynamic view synthesis from monocular videos as an
inverse problem in a training-free setting. By redesigning the noise
initialization phase of a pre-trained video diffusion model, we enable
high-fidelity dynamic view synthesis without any weight updates or auxiliary
modules. We begin by identifying a fundamental obstacle to deterministic
inversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and
resolve it by introducing a novel noise representation, termed K-order
Recursive Noise Representation. We derive a closed form expression for this
representation, enabling precise and efficient alignment between the
VAE-encoded and the DDIM inverted latents. To synthesize newly visible regions
resulting from camera motion, we introduce Stochastic Latent Modulation, which
performs visibility aware sampling over the latent space to complete occluded
regions. Comprehensive experiments demonstrate that dynamic view synthesis can
be effectively performed through structured latent manipulation in the noise
initialization phase.

</details>


### [448] [Vision Transformers Don't Need Trained Registers](https://arxiv.org/abs/2506.08010)
*Nick Jiang,Amil Dravid,Alexei Efros,Yossi Gandelsman*

Main category: cs.CV

TL;DR: 研究视觉Transformer中高范数标记问题，提出无训练方法缓解此问题，提升下游视觉任务表现并增强模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 探究视觉Transformer中高范数标记导致注意力图嘈杂现象的机制，并解决现有去除异常值需从头训练模型的问题。

Method: 将从发现的寄存器神经元的高范数激活转移到额外的未训练标记，模拟有寄存器训练的模型效果。

Result: 该方法产生更清晰的注意力和特征图，提升下游视觉任务性能，结果与有寄存器训练的模型相当，还能增强视觉语言模型的可解释性。

Conclusion: 测试时寄存器可在测试时有效替代寄存器标记，为无寄存器的预训练模型提供无训练解决方案。

Abstract: We investigate the mechanism underlying a previously identified phenomenon in
Vision Transformers -- the emergence of high-norm tokens that lead to noisy
attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a
sparse set of neurons is responsible for concentrating high-norm activations on
outlier tokens, leading to irregular attention patterns and degrading
downstream visual processing. While the existing solution for removing these
outliers involves retraining models from scratch with additional learned
register tokens, we use our findings to create a training-free approach to
mitigate these artifacts. By shifting the high-norm activations from our
discovered register neurons into an additional untrained token, we can mimic
the effect of register tokens on a model already trained without registers. We
demonstrate that our method produces cleaner attention and feature maps,
enhances performance over base models across multiple downstream visual tasks,
and achieves results comparable to models explicitly trained with register
tokens. We then extend test-time registers to off-the-shelf vision-language
models to improve their interpretability. Our results suggest that test-time
registers effectively take on the role of register tokens at test-time,
offering a training-free solution for any pre-trained model released without
them.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [449] [Stability of Mean-Field Variational Inference](https://arxiv.org/abs/2506.07856)
*Shunan Sheng,Bohan Wu,Alberto González-Sanz,Marcel Nutz*

Main category: math.PR

TL;DR: 研究强对数凹测度类中目标分布变化时平均场变分推理（MFVI）近似的稳定性，建立无维度Lipschitz连续性，在额外条件下证明可微性并刻画导数，用新方法解决MFVI问题，讨论其在多领域应用。


<details>
  <summary>Details</summary>
Motivation: 研究强对数凹测度类中目标分布变化时，平均场近似的稳定性性质。

Method: 通过线性化最优传输的新方法处理MFVI，将非凸MFVI问题提升为固定基础测度下传输映射的凸优化问题，运用变分法和泛函分析。

Result: 建立了MFVI优化器关于目标分布在2 - Wasserstein距离下的无维度Lipschitz连续性，Lipschitz常数与对数凹度参数成反比；在额外正则条件下，证明MFVI优化器对目标势可微并通过偏微分方程刻画导数。

Conclusion: 研究结果可应用于稳健贝叶斯推理、经验贝叶斯和分布式随机控制等领域，如得到MFVI的定量Bernstein - von Mises定理。

Abstract: Mean-field variational inference (MFVI) is a widely used method for
approximating high-dimensional probability distributions by product measures.
This paper studies the stability properties of the mean-field approximation
when the target distribution varies within the class of strongly log-concave
measures. We establish dimension-free Lipschitz continuity of the MFVI
optimizer with respect to the target distribution, measured in the
2-Wasserstein distance, with Lipschitz constant inversely proportional to the
log-concavity parameter. Under additional regularity conditions, we further
show that the MFVI optimizer depends differentiably on the target potential and
characterize the derivative by a partial differential equation.
Methodologically, we follow a novel approach to MFVI via linearized optimal
transport: the non-convex MFVI problem is lifted to a convex optimization over
transport maps with a fixed base measure, enabling the use of calculus of
variations and functional analysis. We discuss several applications of our
results to robust Bayesian inference and empirical Bayes, including a
quantitative Bernstein--von Mises theorem for MFVI, as well as to distributed
stochastic control.

</details>


### [450] [Poisson Midpoint Method for Log Concave Sampling: Beyond the Strong Error Lower Bounds](https://arxiv.org/abs/2506.07614)
*Rishikesh Srinivasan,Dheeraj Nagaraj*

Main category: math.PR

TL;DR: 研究用Poisson中点离散化对过阻尼/欠阻尼Langevin动力学从强对数凹分布采样问题，证明其在2 - Wasserstein距离收敛，比Euler - Maruyama离散化有立方加速。


<details>
  <summary>Details</summary>
Motivation: 研究使用Poisson中点离散化从强对数凹分布采样的问题及收敛情况。

Method: 使用Poisson中点离散化（随机中点法变体）对过阻尼/欠阻尼Langevin动力学进行研究。

Result: 证明在2 - Wasserstein距离收敛，比Euler - Maruyama离散化有立方加速，超过随机中点法现有界；欠阻尼情况下，W2收敛复杂度小于文献中L2强误差收敛的复杂度下界。

Conclusion: Poisson中点离散化在从强对数凹分布采样问题上有较好的收敛效果和优势。

Abstract: We study the problem of sampling from strongly log-concave distributions over
$\mathbb{R}^d$ using the Poisson midpoint discretization (a variant of the
randomized midpoint method) for overdamped/underdamped Langevin dynamics. We
prove its convergence in the 2-Wasserstein distance ($W_2$), achieving a cubic
speedup in dependence on the target accuracy ($\epsilon$) over the
Euler-Maruyama discretization, surpassing existing bounds for randomized
midpoint methods. Notably, in the case of underdamped Langevin dynamics, we
demonstrate the complexity of $W_2$ convergence is much smaller than the
complexity lower bounds for convergence in $L^2$ strong error established in
the literature.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [451] [Recursive Semantic Anchoring in ISO 639:2023: A Structural Extension to ISO/TC 37 Frameworks](https://arxiv.org/abs/2506.06870)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.LO

TL;DR: 本文提出递归语义锚定形式化方法处理ISO 639:2023语言代码标准的方言漂移和混合语问题，实验显示该方法能提升语言识别和翻译准确率。


<details>
  <summary>Details</summary>
Motivation: ISO 639:2023缺乏处理方言漂移和克里奥尔混合语的机制。

Method: 提出递归语义锚定形式化，用定点算子建模语义漂移，借助范畴论处理算子和漂移向量，提供RDF/Turtle模式和示例。

Result: 使用φ - 索引指导回退路由时，在嘈杂或代码切换输入的语言识别和翻译中准确率更高。

Conclusion: 该框架与ISO/TC 37兼容，为未来标准提供了可处理漂移的语义层。

Abstract: ISO 639:2023 unifies the ISO language-code family and introduces contextual
metadata, but it lacks a machine-native mechanism for handling dialectal drift
and creole mixtures. We propose a formalisation of recursive semantic
anchoring, attaching to every language entity $\chi$ a family of fixed-point
operators $\phi_{n,m}$ that model bounded semantic drift via the relation
$\phi_{n,m}(\chi) = \chi \oplus \Delta(\chi)$, where $\Delta(\chi)$ is a drift
vector in a latent semantic manifold. The base anchor $\phi_{0,0}$ recovers the
canonical ISO 639:2023 identity, whereas $\phi_{99,9}$ marks the maximal drift
state that triggers a deterministic fallback. Using category theory, we treat
the operators $\phi_{n,m}$ as morphisms and drift vectors as arrows in a
category $\mathrm{DriftLang}$. A functor $\Phi: \mathrm{DriftLang} \to
\mathrm{AnchorLang}$ maps every drifted object to its unique anchor and proves
convergence. We provide an RDF/Turtle schema (\texttt{BaseLanguage},
\texttt{DriftedLanguage}, \texttt{ResolvedAnchor}) and worked examples -- e.g.,
$\phi_{8,4}$ (Standard Mandarin) versus $\phi_{8,7}$ (a colloquial variant),
and $\phi_{1,7}$ for Nigerian Pidgin anchored to English. Experiments with
transformer models show higher accuracy in language identification and
translation on noisy or code-switched input when the $\phi$-indices are used to
guide fallback routing. The framework is compatible with ISO/TC 37 and provides
an AI-tractable, drift-aware semantic layer for future standards.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [452] [Impact of COVID-19 on The Bullwhip Effect Across U.S. Industries](https://arxiv.org/abs/2506.06368)
*Alper Saricioglu,Mujde Erol Genevois,Michele Cedolin*

Main category: econ.GN

TL;DR: 研究用行业数据研究新冠疫情对美国各行业牛鞭效应的加剧影响，发现供应链结构作用大。


<details>
  <summary>Details</summary>
Motivation: 探究新冠疫情如何加剧美国各行业的牛鞭效应，以及外部冲击如何恶化这一现象。

Method: 采用传统和先进的实证技术，分析制造业、零售商和批发商部门的行业层面数据。

Result: 新冠疫情显著放大牛鞭效应，各行业对同一外部冲击反应不同。

Conclusion: 供应链结构对缓解或加剧牛鞭效应至关重要，应根据行业特点制定供应链管理策略。

Abstract: The Bullwhip Effect, describing the amplification of demand variability up
the supply chain, poses significant challenges in Supply Chain Management. This
study examines how the COVID-19 pandemic intensified the Bullwhip Effect across
U.S. industries, using extensive industry-level data. By focusing on the
manufacturing, retailer, and wholesaler sectors, the research explores how
external shocks exacerbate this phenomenon. Employing both traditional and
advanced empirical techniques, the analysis reveals that COVID-19 significantly
amplified the Bullwhip Effect, with industries displaying varied responses to
the same external shock. These differences suggest that supply chain structures
play a critical role in either mitigating or intensifying the effect. By
analyzing the dynamics during the pandemic, this study provides valuable
insights into managing supply chains under global disruptions and highlights
the importance of tailoring strategies to industry-specific characteristics.

</details>


### [453] [Nexus of Team Collaboration Stability on Mega Construction Project Success in Electric Vehicle Manufacturing Enterprises: The Moderating Role of Human-AI Integration](https://arxiv.org/abs/2506.06375)
*Jun Cui*

Main category: econ.GN

TL;DR: 研究团队协作稳定性对电动汽车制造企业大型建设项目成功的影响，发现稳定性显著促进项目成功，人机集成有调节作用。


<details>
  <summary>Details</summary>
Motivation: 探究团队协作稳定性如何影响电动汽车制造企业大型建设项目的成功，以及人机集成在其中的调节作用。

Method: 采用结构方程模型（SEM），分析中国电动汽车行业187个项目团队的数据。

Result: 团队协作稳定性显著提升项目成功，人机集成的调节作用加强了这一关系。

Conclusion: 研究结果丰富了团队协作理论，为电动汽车行业大型项目管理提供了实践启示。

Abstract: This study investigates how team collaboration stability influences the
success of mega construction projects in electric vehicle manufacturing
enterprises, with human-AI integration as a moderating variable. Using
structural equation modeling (SEM) with data from 187 project teams across
China's electric vehicle sector, results indicate that team collaboration
stability significantly enhances project success. The moderating effect of
human-AI integration strengthens this relationship, suggesting that enterprises
implementing advanced human-AI collaborative systems achieve superior project
outcomes when team stability is maintained. These findings contribute to both
team collaboration theory and provide practical implications for mega project
management in the rapidly evolving electric vehicle industry.

</details>


### [454] [Improving choice model specification using reinforcement learning](https://arxiv.org/abs/2506.06410)
*Gabriel Nova,Sander van Cranenburgh,Stephane Hess*

Main category: econ.GN

TL;DR: 本文引入基于深度强化学习的框架解决离散选择建模中模型规格选择问题，结果显示该框架有良好效果。


<details>
  <summary>Details</summary>
Motivation: 当前离散选择建模中试错过程需专业知识、耗时且依赖主观假设，现有元启发式方法存在局限，难以优先考虑有前景的搜索区域等。

Method: 引入基于深度强化学习的框架，让‘智能体’通过估计模型并根据拟合优度和简约性获得奖励来指定模型。

Result: 智能体可动态调整策略，跨数据生成过程识别有前景的规格，表现出鲁棒性和潜在可迁移性，无需先验领域知识。

Conclusion: 基于深度强化学习的框架能有效解决现有离散选择建模及元启发式方法的局限问题。

Abstract: Discrete choice modelling is a theory-driven modelling framework for
understanding and forecasting choice behaviour. To obtain behavioural insights,
modellers test several competing model specifications in their attempts to
discover the 'true' data generation process. This trial-and-error process
requires expertise, is time-consuming, and relies on subjective theoretical
assumptions. Although metaheuristics have been proposed to assist choice
modellers, they treat model specification as a classic optimisation problem,
relying on static strategies, applying predefined rules, and neglecting
outcomes from previous estimated models. As a result, current metaheuristics
struggle to prioritise promising search regions, adapt exploration dynamically,
and transfer knowledge to other modelling tasks. To address these limitations,
we introduce a deep reinforcement learning-based framework where an 'agent'
specifies models by estimating them and receiving rewards based on
goodness-of-fit and parsimony. Results demonstrate the agent dynamically adapts
its strategies to identify promising specifications across data generation
processes, showing robustness and potential transferability, without prior
domain knowledge.

</details>


### [455] [The Influence of Tourist Experience on Revisit Decisions with the Mediation of Tourist Satisfaction](https://arxiv.org/abs/2506.06552)
*Marsuni H. Muhammad,Ida Hidayanti,Sulfi Abdul Haji,Rahmat Sabuhari*

Main category: econ.GN

TL;DR: 研究测试分析努斯利科公园生态旅游区游客体验、满意度与重游决策的关系，发现游客体验和满意度正向影响重游决策，满意度起中介作用。


<details>
  <summary>Details</summary>
Motivation: 通过分析影响重游决策的因素制定营销策略，研究游客体验和满意度与重游决策的关系。

Method: 采用描述性研究，以该旅游区游客为总体，选取至少去过两次且居住在北马鲁古的游客为样本，用非概率抽样的 purposive sampling 技术，通过问卷收集 114 份数据，用 SEM 和 SmartPLS 4.0 软件处理分析。

Result: 游客体验和满意度正向影响重游决策，满意度变量对游客体验和重游决策的关系起中介作用。

Conclusion: 游客体验和满意度对重游决策有积极作用，满意度在两者关系中有中介效应。

Abstract: Nusliko Park Ecotourism Area is a combination of natural and artificial
tourism with the background of Lake Nusliko, which can be developed as a
leading tourist attraction. Marketing strategies can be formulated by analyzing
the factors influencing the decision to revisit. The decision to revisit is a
key indicator that shows the success of a tourism marketing strategy. This
study aimed to test and analyze the relationship between tourist experience and
tourist satisfaction with the decision to revisit the Nusliko Park Ecotourism
area. This type of research is descriptive research. The population in this
study was visitors to the Nusliko Park Ecotourism area. The sample criteria
were visitors who had visited the Nusliko Park Ecotourism area at least twice
and were domiciled in North Maluku. Data were collected through questionnaires,
and 114 questionnaires were returned. The research method used non-probability
sampling with purposive sampling techniques. These two techniques were used to
make it easier for the author to obtain samples because the Nusliko Park
Ecotourism area was closed when the research was conducted. After all, it was
under repair. Data were processed and analyzed using SEM and through SmartPLS
4.0 software. The study results indicate that the tourist experience and
satisfaction positively influence the decision to revisit. Through this study,
the satisfaction variable was also found to mediate the relationship between
the tourist experience and the decision to revisit

</details>


### [456] [Solving Nash Equilibria in Nonlinear Differential Games for Common-Pool Resources](https://arxiv.org/abs/2506.06646)
*Yongyang Cai,Anastasios Xepapadeas,Aart de Zeeuw*

Main category: econ.GN

TL;DR: 本文提出推导一维和二维动态系统开环和反馈纳什均衡的新数值方法，并应用于湖泊博弈问题，二维反馈纳什均衡接近合作解，有重要政策意义。


<details>
  <summary>Details</summary>
Motivation: 生态系统易受污染影响出现临界点且作为公共资源易因非合作行为被低效利用，需推导含临界点动态系统的合作与非合作解的方法。

Method: 提出推导一维和二维动态系统开环和反馈纳什均衡的新数值方法，并应用于湖泊博弈问题。

Result: 得到了开环和反馈纳什均衡，二维反馈纳什均衡是本文的创新点，且该均衡接近合作解。

Conclusion: 新方法在解决生态系统管理问题上有重要意义，二维反馈纳什均衡的结果有重要政策影响。

Abstract: Many resources are provided by an ecological system that is vulnerable to
tipping when exceeding a certain level of pollution, with a sudden big loss of
ecosystem services. An ecological system is usually also a common-pool resource
and therefore vulnerable to suboptimal use resulting from non-cooperative
behavior. An analysis requires methods to derive cooperative and
non-cooperative solutions for managing a dynamical system with tipping points.
Such a game is a differential game which has two well-defined non-cooperative
solutions, the open-loop and feedback Nash equilibria. This paper provides new
numerical methods for deriving open-loop and feedback Nash equilibria, for
one-dimensional and two-dimensional dynamical systems. The methods are applied
to the lake game, which is the classical example for these types of problems.
Especially, two-dimensional feedback Nash equilibria are a novelty of this
paper. This Nash equilibrium is close to the cooperative solution which has
important policy implications.

</details>


### [457] [(In)stability in the Dynamics of the Cross-Country Distribution of Income Per Capita](https://arxiv.org/abs/2506.06755)
*Davide Fiaschi,Paul Johnson*

Main category: econ.GN

TL;DR: 研究1970 - 2019年102个国家人均产出跨国分布演化假设的真实性，发现不同时期分布动态特征不同。


<details>
  <summary>Details</summary>
Motivation: 检验时间齐次、一阶过程描述人均产出跨国分布演化这一假设的真实性，该假设常用于收敛假说研究。

Method: 比较不同时期的转移核检验齐次性，用查普曼 - 柯尔莫哥洛夫方程检验一阶假设，用多种度量方法测量概率分布距离并采用自举法评估距离的统计显著性。

Result: 1970 - 1995年过程是时间齐次和一阶的，分布动态暗示双峰长期分布；2000 - 2010年暗示单峰长期分布；2010年后有回到非收敛动态的迹象。

Conclusion: 不同时期人均产出跨国分布的演化过程不同，与不同的收敛情况相符。

Abstract: Using a panel of 102 countries from PWT 10.0 covering 1970-2019, we examine
the veracity of the assumption that a time-homogeneous, first-order process
describes the evolution of the cross-country distribution of per capita output,
an assumption often made in studies of the convergence hypothesis employing the
distribution dynamics approach pioneered by Quah (1993). To test homogeneity,
we compare transition kernels estimated for different time periods and, for
those periods exhibiting evidence of homogeneity, we test the first-order
assumption using an implication of such a process's Chapman-Kolmogorov
equations. Both tests require measurement of the distance between probability
distributions which we do with several different metrics, employing bootstrap
methods to assess the statistical significance of the observed distances. We
find that the process was time-homogeneous and first-order in the 1970-1995
period during which the distribution dynamics imply a bimodal long-run
distribution, consistent with convergence clubs. Following the apparent break
in the process in the late 1990s, the 2000-2010 distribution dynamics imply a
unimodal long-run distribution suggestive of a single convergence club,
consistent with recent claims of short-term beta-convergence from the late
1990s and beyond made by Patel et al. (2021) and Kremer et al (2022). After
2010, there is some evidence of a return to non-convergent dynamics similar to
those of the 1970-1995 period.

</details>


### [458] [Do conditional cash transfers in childhood increase economic resilience in adulthood? Evidence from the COVID-19 pandemic shock in Ecuador](https://arxiv.org/abs/2506.06903)
*José-Ignacio Antón,Ruthy Intriago,Juan Ponce*

Main category: econ.GN

TL;DR: 研究厄瓜多尔儿童时期接受有条件现金转移支付（HDG）对成年后应对新冠疫情等经济冲击的影响，整体无效果，但农村地区符合条件者更具经济韧性。


<details>
  <summary>Details</summary>
Motivation: 现有研究对有条件现金转移支付长期影响及应对未来经济冲击的证据有限，期望探究儿童时期接受HDG对成年应对经济冲击的作用。

Method: 采用回归断点设计（RDD），利用合并行政数据。

Result: HDG对目标人群无整体影响，但儿童时期符合项目条件且生活在农村地区的人在疫情中有更强经济韧性，更可能在疫情困难阶段保持正式部门就业。

Conclusion: 结果可能由HDG条件性弱和正规经济吸纳劳动力能力有限等需求因素导致。

Abstract: The primary goal of conditional cash transfers (CCTs) is to alleviate
short-term poverty while preventing the intergenerational transmission of
deprivation by promoting the accumulation of human capital among children.
Although a substantial body of research has evaluated the short-run impacts of
CCTs, studies on their long-term effects are relatively scarce, and evidence
regarding their influence on resilience to future economic shocks is limited.
As human capital accumulation is expected to enhance individuals' ability to
cope with risk and uncertainty during turbulent periods, we investigate whether
receiving a conditional cash transfer -- specifically, the Human Development
Grant (HDG) in Ecuador -- during childhood improves the capacity to respond to
unforeseen exogenous economic shocks in adulthood, such as the COVID-19
pandemic. Using a regression discontinuity design (RDD) and leveraging merged
administrative data, we do not find an overall effect of the HDG on the target
population. Nevertheless, we present evidence that individuals who were
eligible for the programme and lived in rural areas (where previous works have
found the largest effects in terms of on short-term impact) during their
childhood, approximately 12 years before the pandemic, exhibited greater
economic resilience to the pandemic. In particular, eligibility increased the
likelihood of remaining employed in the formal sector during some of the most
challenging phases of the COVID-19 crisis. The likely drivers of these results
are the weak conditionality of the HDG and demand factors given the limited
ability of the formal economy to absorb labour, even if more educated.

</details>


### [459] [The impact of extracurricular education on socioeconomic mobility in Japan: an application of causal machine learning](https://arxiv.org/abs/2506.07421)
*Yang Qiang*

Main category: econ.GN

TL;DR: 本文探讨日本课外教育（私人家教）对社会流动的社会经济影响，发现其虽有积极潜力，但因家庭经济差异受益有限。


<details>
  <summary>Details</summary>
Motivation: 研究课外教育（私人家教）对日本社会流动的社会经济影响。

Method: 利用2015年全国社会分层与社会流动调查（SSM）数据，采用因果机器学习方法评估教育干预对收入、教育程度和职业声望的影响。

Result: 课外教育虽有积极的社会经济影响潜力，但家庭经济差异削弱了其益处，整体改善效果甚微。

Conclusion: 凸显了个人人口统计学特征与教育干预之间的复杂机制，展示了机器学习在该领域的应用前景。

Abstract: This paper explores the socioeconomic impacts of extracurricular education,
specifically private tutoring, on social mobility in Japan. Using data from the
2015 National Survey on Social Stratification and Social Mobility (SSM), we
employed a causal machine learning approach to evaluate this educational
intervention on income, educational attainment, and occupational prestige. Our
research suggests that while shadow education holds the potential for positive
socioeconomic impacts, its benefits are undermined by the economic disparities
among households, resulting in minimal overall improvement. This highlights the
complex mechanisms between individual demographics and educational
interventions, revealing promising machine learning applications in this field.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [460] [On the Interplay of Privacy, Persuasion and Quantization](https://arxiv.org/abs/2506.06321)
*Anju Anand,Emrah Akyol*

Main category: eess.SP

TL;DR: 提出一种通信理论框架用于网络物理系统中隐私感知和弹性决策，分析不同策略并计算最优控制器，通过实验展示隐私参数对性能和抗推断能力的影响。


<details>
  <summary>Details</summary>
Motivation: 解决编码器和解码器目标不一致时网络物理系统的隐私感知和弹性决策问题。

Method: 分析不同策略，在无速率约束时求最优线性编码器，有限速率通道用基于梯度的方法计算最优控制器。

Result: 通过数值实验展示了调整隐私参数对控制性能和抗推断能力的权衡影响。

Conclusion: 该框架能有效处理目标不一致情况下网络物理系统的决策问题，可通过调整隐私参数平衡控制性能和隐私保护。

Abstract: We develop a communication-theoretic framework for privacy-aware and
resilient decision making in cyber-physical systems under misaligned objectives
between the encoder and the decoder. The encoder observes two correlated
signals ($X$,$\theta$) and transmits a finite-rate message $Z$ to aid a
legitimate controller (the decoder) in estimating $X+\theta$, while an
eavesdropper intercepts $Z$ to infer the private parameter $\theta$. Unlike
conventional setups where encoder and decoder share a common MSE objective,
here the encoder minimizes a Lagrangian that balances legitimate control
fidelity and the privacy leakage about $\theta$. In contrast, the decoder's
goal is purely to minimize its own estimation error without regard for privacy.
We analyze fully, partially, and non-revealing strategies that arise from this
conflict, and characterize optimal linear encoders when the rate constraints
are lifted. For finite-rate channels, we employ gradient-based methods to
compute the optimal controllers. Numerical experiments illustrate how tuning
the privacy parameter shapes the trade-off between control performance and
resilience against unauthorized inferences.

</details>


### [461] [MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes](https://arxiv.org/abs/2506.06318)
*Feiyang Pan,Shenghe Zheng,Chunyan Yin,Guangbin Dou*

Main category: eess.SP

TL;DR: 提出MoE - Gyro框架用于MEMS陀螺仪超量程信号重建与降噪，引入ISEBench评估平台，显著提升性能解决惯性传感权衡问题。


<details>
  <summary>Details</summary>
Motivation: MEMS陀螺仪在测量范围和噪声性能间存在权衡，现有硬件和深度学习方法有局限性，且缺乏综合评估标准。

Method: 提出MoE - Gyro框架，含ORE和DE两个专家模块及轻量级门控模块；引入ISEBench评估平台。

Result: 将可测量范围从450 deg/s扩展到1500 deg/s，减少98.4%的偏差不稳定性，达到了先进水平。

Conclusion: MoE - Gyro框架有效解决了惯性传感中长期存在的权衡问题。

Abstract: MEMS gyroscopes play a critical role in inertial navigation and motion
control applications but typically suffer from a fundamental trade-off between
measurement range and noise performance. Existing hardware-based solutions
aimed at mitigating this issue introduce additional complexity, cost, and
scalability challenges. Deep-learning methods primarily focus on noise
reduction and typically require precisely aligned ground-truth signals, making
them difficult to deploy in practical scenarios and leaving the fundamental
trade-off unresolved. To address these challenges, we introduce Mixture of
Experts for MEMS Gyroscopes (MoE-Gyro), a novel self-supervised framework
specifically designed for simultaneous over-range signal reconstruction and
noise suppression. MoE-Gyro employs two experts: an Over-Range Reconstruction
Expert (ORE), featuring a Gaussian-Decay Attention mechanism for reconstructing
saturated segments; and a Denoise Expert (DE), utilizing dual-branch
complementary masking combined with FFT-guided augmentation for robust noise
reduction. A lightweight gating module dynamically routes input segments to the
appropriate expert. Furthermore, existing evaluation lack a comprehensive
standard for assessing multi-dimensional signal enhancement. To bridge this
gap, we introduce IMU Signal Enhancement Benchmark (ISEBench), an open-source
benchmarking platform comprising the GyroPeak-100 dataset and a unified
evaluation of IMU signal enhancement methods. We evaluate MoE-Gyro using our
proposed ISEBench, demonstrating that our framework significantly extends the
measurable range from 450 deg/s to 1500 deg/s, reduces Bias Instability by
98.4%, and achieves state-of-the-art performance, effectively addressing the
long-standing trade-off in inertial sensing.

</details>


### [462] [A Reinforcement Learning Approach for RIS-aided Fair Communications](https://arxiv.org/abs/2506.06344)
*Alex Pierron,Michel Barbeau,Luca De Cicco,Jose Rubio-Hernan,Joaquin Garcia-Alfaro*

Main category: eess.SP

TL;DR: 本文探讨可重构智能表面（RIS）与强化学习（RL）结合，解决多合法用户设备（UE）通信公平性问题，提出新方法并给出实验结果，开源代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 在RIS与RL结合提升网络性能和能效的基础上，需考虑通信公平性，确保各UE都能接收到足够强度信号。

Method: 探索先前工作的公平性属性，提出一种新方法以实现高效公平的双工RIS - RL系统。

Result: 进行了实验工作并给出模拟结果。

Conclusion: 通过提出的方法可实现高效公平的双工RIS - RL系统，开源代码和数据集利于后续研究。

Abstract: Reconfigurable Intelligent Surfaces (RISs) are composed of physical elements
that can dynamically alter electromagnetic wave properties to enhance
beamforming and leading to improvements in areas with low coverage properties.
They have the potential to be combined with Reinforcement Learning (RL)
techniques to achieve network performance and energy efficiency via
optimization techniques. In addition to performance and energy improvements, it
is also crucial to consider the concept of fair communications. RISs must
ensure that User Equipment (UE) units receive their signals with adequate
strength, without other UE being deprived of service due to insufficient power.
In this paper, we address such a problem. We explore the fairness properties of
previous work and propose a novel method that aims at obtaining an efficient
and fair duplex RIS-RL system for multiple legitimate UE units. We report and
discuss our experimental work and simulation results. We also release our code
and datasets to foster further research in the topic.

</details>


### [463] [Deep learning methods for modeling infrasound transmission loss in the middle atmosphere](https://arxiv.org/abs/2506.06351)
*Alexis Le Pichon,Alice Janela Cameijo,Samir Aknine,Youcef Sklab,Souhila Arib,Quentin Brissaud,Sven Peter Naesholm*

Main category: eess.SP

TL;DR: 本文开发优化卷积网络预测次声传输损失，相比之前方法提升性能，在全频段平均误差8.6 dB。


<details>
  <summary>Details</summary>
Motivation: 准确建模次声传输损失对评估国际监测系统次声网络性能至关重要，现有抛物方程方法计算成本高，此前神经网络方法有局限性。

Method: 开发优化的卷积网络，基于全球模拟的温度和风速组合场预测4000 km传播范围内的传输损失，对整体架构进行关键优化。

Result: 模型在全频段（0.1 - 3.2 Hz）和实际大气场景中预测传输损失平均误差为8.6 dB。

Conclusion: 优化的卷积网络能有效减少预测误差，提升了预测次声传输损失的性能。

Abstract: Accurate modeling of infrasound transmission losses (TLs) is essential to
assess the performance of the global International Monitoring System infrasound
network. Among existing propagation modeling tools, parabolic equation (PE)
method enables TLs to be finely modeled, but its computational cost does not
allow exploration of a large parameter space for operational monitoring
applications. To reduce computation times, Brissaud et al. 2023 explored the
potential of convolutional neural networks trained on a large set of regionally
simulated wavefields (< 1000 km from the source) to predict TLs with negligible
computation times compared to PE simulations. However, this method struggles in
unfavorable initial wind conditions, especially at high frequencies, and causal
issues with winds at large distances from the source affecting ground TLs close
to the source. In this study, we have developed an optimized convolutional
network designed to minimize prediction errors while predicting TLs from
globally simulated combined temperature and wind fields spanning over
propagation ranges of 4000 km. Our approach enhances the previously proposed
one by implementing key optimizations that improve the overall architecture
performance. The implemented model predicts TLs with an average error of 8.6 dB
in the whole frequency band (0.1-3.2 Hz) and explored realistic atmospheric
scenarios.

</details>


### [464] [Large Language Models for EEG: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2506.06353)
*Naseem Babu,Jimson Mathew,A. P. Vinod*

Main category: eess.SP

TL;DR: 本文对利用大语言模型进行基于脑电图的分析和应用的研究进展进行系统综述和分类。


<details>
  <summary>Details</summary>
Motivation: 大语言模型与脑电图研究融合带来新方向，需要对相关进展进行系统梳理。

Method: 将文献分为四个领域进行综述，介绍基于微调、少样本和零样本学习的基于Transformer架构的应用。

Result: 展示了基于脑电图的模型能执行自然语言生成、语义解释和诊断辅助等复杂任务。

Conclusion: 为未来通过语言模型连接自然语言处理和神经信号分析的工作提供基础资源。

Abstract: The growing convergence between Large Language Models (LLMs) and
electroencephalography (EEG) research is enabling new directions in neural
decoding, brain-computer interfaces (BCIs), and affective computing. This
survey offers a systematic review and structured taxonomy of recent
advancements that utilize LLMs for EEG-based analysis and applications. We
organize the literature into four domains: (1) LLM-inspired foundation models
for EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal
generation including image and 3D object synthesis, and (4) clinical
applications and dataset management tools. The survey highlights how
transformer-based architectures adapted through fine-tuning, few-shot, and
zero-shot learning have enabled EEG-based models to perform complex tasks such
as natural language generation, semantic interpretation, and diagnostic
assistance. By offering a structured overview of modeling strategies, system
designs, and application areas, this work serves as a foundational resource for
future work to bridge natural language processing and neural signal analysis
through language models.

</details>


### [465] [Towards real-time assessment of infrasound event detection capability using deep learning-based transmission loss estimation](https://arxiv.org/abs/2506.06358)
*Alice Janela Cameijo,Alexis Le Pichon,Youcef Sklab,Souhila Arib,Quentin Brissaud,Sven peter Naesholm,Constantino Listowski,Samir Aknine*

Main category: eess.SP

TL;DR: 本文使用风场和温度场作为神经网络输入，优化网络架构，以解决现有次声传输损耗预测算法的局限性，提升了性能并通过火山爆发事件验证，迈向对国际监测系统探测阈值的近实时评估。


<details>
  <summary>Details</summary>
Motivation: 现有次声传输损耗建模工具计算成本高，深度学习算法存在介质表征不完整、不适用于长距离传播等问题，需改进。

Method: 以风场和温度场为神经网络输入，模拟至130 km高度和4000 km距离，优化神经网络架构，利用卷积和循环层捕捉特征。

Result: 神经网络与全抛物线方程模拟相比平均误差为4 dB，能提供不确定性估计，在火山爆发事件中展示了预测能力。

Conclusion: 该研究向国际监测系统对爆炸源探测阈值的近实时评估迈出重要一步。

Abstract: Accurate modeling of infrasound transmission loss is essential for evaluating
the performance of the International Monitoring System, enabling the effective
design and maintenance of infrasound stations to support compliance of the
Comprehensive Nuclear-Test-Ban Treaty. State-of-the-art propagation modeling
tools enable transmission loss to be finely simulated using atmospheric models.
However, the computational cost prohibits the exploration of a large parameter
space in operational monitoring applications. To address this, recent studies
made use of a deep learning algorithm capable of making transmission loss
predictions almost instantaneously. However, the use of nudged atmospheric
models leads to an incomplete representation of the medium, and the absence of
temperature as an input makes the algorithm incompatible with long range
propagation. In this study, we address these limitations by using both wind and
temperature fields as inputs to a neural network, simulated up to 130 km
altitude and 4,000 km distance. We also optimize several aspects of the neural
network architecture. We exploit convolutional and recurrent layers to capture
spatially and range-dependent features embedded in realistic atmospheric
models, improving the overall performance. The neural network reaches an
average error of 4 dB compared to full parabolic equation simulations and
provides epistemic and data-related uncertainty estimates. Its evaluation on
the 2022 Hunga Tonga-Hunga Ha'apai volcanic eruption demonstrates its
prediction capability using atmospheric conditions and frequencies not included
in the training. This represents a significant step towards near real-time
assessment of International Monitoring System detection thresholds of explosive
sources.

</details>


### [466] [Model-based Neural Data Augmentation for sub-wavelength Radio Localization](https://arxiv.org/abs/2506.06387)
*Baptiste Chatelier,Vincent Corlay,Musa Furkan Keskin,Matthieu Crussière,Henk Wymeersch,Luc Le Magoarou*

Main category: eess.SP

TL;DR: 文章扩展指纹定位框架，用基于模型的神经网络学习映射，在非视距环境下实现亚波长定位精度，提升定位精度并降低内存需求。


<details>
  <summary>Details</summary>
Motivation: 传统信号处理技术在复杂无线环境中定位精度下降，机器学习辅助定位技术计算复杂度高，需改进指纹定位框架。

Method: 使用基于模型的神经网络学习位置到信道的映射，作为生成式神经信道模型，增强指纹比较字典并减少内存需求。

Result: 在非视距环境下实现亚波长定位精度，比经典指纹方法提升多个数量级的定位精度，同时将内存需求降低一个数量级。

Conclusion: 提出的方法能有效提升定位精度并降低内存需求，优于传统指纹定位方法。

Abstract: The increasing deployment of large antenna arrays at base stations has
significantly improved the spatial resolution and localization accuracy of
radio-localization methods. However, traditional signal processing techniques
struggle in complex radio environments, particularly in scenarios dominated by
non line of sight (NLoS) propagation paths, resulting in degraded localization
accuracy. Recent developments in machine learning have facilitated the
development of machine learning-assisted localization techniques, enhancing
localization accuracy in complex radio environments. However, these methods
often involve substantial computational complexity during both the training and
inference phases. This work extends the well-established fingerprinting-based
localization framework by simultaneously reducing its memory requirements and
improving its accuracy. Specifically, a model-based neural network is used to
learn the location-to-channel mapping, and then serves as a generative neural
channel model. This generative model augments the fingerprinting comparison
dictionary while reducing the memory requirements. The proposed method
outperforms fingerprinting baselines by achieving sub-wavelength localization
accuracy, even in NLoS environments. Remarkably, it offers an improvement by
several orders of magnitude in localization accuracy, while simultaneously
reducing memory requirements by an order of magnitude compared to classical
fingerprinting methods.

</details>


### [467] [Benchmarking Early Agitation Prediction in Community-Dwelling People with Dementia Using Multimodal Sensors and Machine Learning](https://arxiv.org/abs/2506.06306)
*Ali Abedi,Charlene H. Chu,Shehroz S. Khan*

Main category: eess.SP

TL;DR: 本研究用多模态传感器数据开发机器学习方法，对社区痴呆老人激越行为进行早期预测，对比多种模型，结合额外信息提升性能。


<details>
  <summary>Details</summary>
Motivation: 及时预测社区痴呆老人的激越行为，实现早期干预，减轻护理负担，提高患者和护理人员生活质量。

Method: 引入与激越相关的上下文特征，评估多种机器学习和深度学习模型，在不同问题表述下进行实验，使用TIHM数据集。

Result: 最有效设置是用当前6小时时间戳的传感器数据进行二分类预测后续时间戳的激越行为，结合额外信息可提升模型性能，LightGBM取得最高AUC - ROC 0.9720和AUC - PR 0.4320。

Conclusion: 首次对基于隐私保护传感器数据的社区痴呆护理中激越行为预测技术进行全面基准测试，该方法能实现准确、可解释和高效的激越行为预测，支持积极的痴呆护理和居家养老。

Abstract: Agitation is one of the most common responsive behaviors in people living
with dementia, particularly among those residing in community settings without
continuous clinical supervision. Timely prediction of agitation can enable
early intervention, reduce caregiver burden, and improve the quality of life
for both patients and caregivers. This study aimed to develop and benchmark
machine learning approaches for the early prediction of agitation in
community-dwelling older adults with dementia using multimodal sensor data. A
new set of agitation-related contextual features derived from activity data was
introduced and employed for agitation prediction. A wide range of machine
learning and deep learning models was evaluated across multiple problem
formulations, including binary classification for single-timestamp tabular
sensor data and multi-timestamp sequential sensor data, as well as anomaly
detection for single-timestamp tabular sensor data. The study utilized the
Technology Integrated Health Management (TIHM) dataset, the largest publicly
available dataset for remote monitoring of people living with dementia,
comprising 2,803 days of in-home activity, physiology, and sleep data. The most
effective setting involved binary classification of sensor data using the
current 6-hour timestamp to predict agitation at the subsequent timestamp.
Incorporating additional information, such as time of day and agitation
history, further improved model performance, with the highest AUC-ROC of 0.9720
and AUC-PR of 0.4320 achieved by the light gradient boosting machine. This work
presents the first comprehensive benchmarking of state-of-the-art techniques
for agitation prediction in community-based dementia care using
privacy-preserving sensor data. The approach enables accurate, explainable, and
efficient agitation prediction, supporting proactive dementia care and aging in
place.

</details>


### [468] [Leveraging Novel Ensemble Learning Techniques and Landsat Multispectral Data for Estimating Olive Yields in Tunisia](https://arxiv.org/abs/2506.06309)
*Mohamed Kefi,Tien Dat Pham,Thin Nguyen,Mark G. Tjoelker,Viola Devasirvatham,Kenichi Kashiwagi*

Main category: eess.SP

TL;DR: 研究开发了突尼斯两个地区橄榄产量估算的简化流程，结合卫星影像与实地调查数据，用集成学习框架预测产量，两种传感器预测效果好。


<details>
  <summary>Details</summary>
Motivation: 橄榄产量受气候变化影响差异大，准确利用遥感和机器学习估算产量是复杂挑战。

Method: 从卫星影像提取特征，结合实地调查数据形成表格数据集，用AutoGluon实现自动化集成学习框架，通过堆叠选择最优模型组合，用五折交叉验证生成产量预测。

Result: 两种传感器预测性能良好，Landsat - 8 OLI的R2为0.8635，RMSE为1.17吨/公顷；Landsat - 9 OLI - 2的R2为0.8378，RMSE为1.32吨/公顷。

Conclusion: 提出的橄榄产量估算方法可扩展、成本低且准确，有全球应用潜力。

Abstract: Olive production is an important tree crop in Mediterranean climates.
However, olive yield varies significantly due to climate change. Accurately
estimating yield using remote sensing and machine learning remains a complex
challenge. In this study, we developed a streamlined pipeline for olive yield
estimation in the Kairouan and Sousse governorates of Tunisia. We extracted
features from multispectral reflectance bands, vegetation indices derived from
Landsat-8 OLI and Landsat-9 OLI-2 satellite imagery, along with digital
elevation model data. These spatial features were combined with ground-based
field survey data to form a structured tabular dataset. We then developed an
automated ensemble learning framework, implemented using AutoGluon to train and
evaluate multiple machine learning models, select optimal combinations through
stacking, and generate robust yield predictions using five-fold
cross-validation. The results demonstrate strong predictive performance from
both sensors, with Landsat-8 OLI achieving R2 = 0.8635 and RMSE = 1.17 tons per
ha, and Landsat-9 OLI-2 achieving R2 = 0.8378 and RMSE = 1.32 tons per ha. This
study highlights a scalable, cost-effective, and accurate method for olive
yield estimation, with potential applicability across diverse agricultural
regions globally.

</details>


### [469] [Enhancing Contrastive Learning-based Electrocardiogram Pretrained Model with Patient Memory Queue](https://arxiv.org/abs/2506.06310)
*Xiaoyu Sun,Yang Yang,Xunde Dong*

Main category: eess.SP

TL;DR: 文章针对自动心电图诊断中标签数据有限问题，提出基于患者记忆队列（PMQ）增强的对比学习心电图预训练模型，实验表明该方法性能优于以往对比学习方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 自动心电图诊断中标签数据有限，现有基于对比学习的心电图预训练模型未能有效利用患者一致性，需构建更鲁棒的预训练模型。

Method: 提出基于患者记忆队列（PMQ）增强的对比学习心电图预训练模型，引入两种额外数据增强方法。

Result: 在三个公共数据集上用三种不同数据比例进行实验，该方法综合性能优于以往对比学习方法，在标签数据有限场景更具鲁棒性。

Conclusion: 所提方法能解决现有对比学习方法问题，提升心电图预训练模型性能。

Abstract: In the field of automatic Electrocardiogram (ECG) diagnosis, due to the
relatively limited amount of labeled data, how to build a robust ECG pretrained
model based on unlabeled data is a key area of focus for researchers. Recent
advancements in contrastive learning-based ECG pretrained models highlight the
potential of exploiting the additional patient-level self-supervisory signals
inherent in ECG. They are referred to as patient contrastive learning. Its
rationale is that multiple physical recordings from the same patient may share
commonalities, termed patient consistency, so redefining positive and negative
pairs in contrastive learning as intrapatient and inter-patient samples
provides more shared context to learn an effective representation. However,
these methods still fail to efficiently exploit patient consistency due to the
insufficient amount of intra-inter patient samples existing in a batch. Hence,
we propose a contrastive learning-based ECG pretrained model enhanced by the
Patient Memory Queue (PMQ), which incorporates a large patient memory queue to
mitigate model degeneration that can arise from insufficient intra-inter
patient samples. In order to further enhance the performance of the pretrained
model, we introduce two extra data augmentation methods to provide more
perspectives of positive and negative pairs for pretraining. Extensive
experiments were conducted on three public datasets with three different data
ratios. The experimental results show that the comprehensive performance of our
method outperforms previous contrastive learning methods and exhibits greater
robustness in scenarios with limited labeled data. The code is available at
https://github.com/3hiuwoo/PMQ.

</details>


### [470] [A Novel Shape-Aware Topological Representation for GPR Data with DNN Integration](https://arxiv.org/abs/2506.06311)
*Meiyan Kang,Shizuo Kaji,Sang-Yun Lee,Taegon Kim,Hee-Hwan Ryu,Suyoung Choi*

Main category: eess.SP

TL;DR: 本文提出新框架结合TDA与YOLOv5检测地下设施，用Sim2Real策略解决数据稀缺问题，实验证明方法有效且有广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统探地雷达（GPR）解释方法受噪声敏感和缺乏结构感知的限制，需更有效的地下设施检测方法。

Method: 提出新框架，结合TDA从B扫描GPR图像提取形状感知拓扑特征与YOLOv5的空间检测能力；用Sim2Real策略生成合成数据集。

Result: 实验显示平均精度均值（mAP）显著提高，验证了方法的鲁棒性和有效性。

Conclusion: TDA增强学习在可靠、实时地下物体检测方面有潜力，可广泛应用于城市规划、安全检查和基础设施管理。

Abstract: Ground Penetrating Radar (GPR) is a widely used Non-Destructive Testing (NDT)
technique for subsurface exploration, particularly in infrastructure inspection
and maintenance. However, conventional interpretation methods are often limited
by noise sensitivity and a lack of structural awareness. This study presents a
novel framework that enhances the detection of underground utilities,
especially pipelines, by integrating shape-aware topological features derived
from B-scan GPR images using Topological Data Analysis (TDA), with the spatial
detection capabilities of the YOLOv5 deep neural network (DNN). We propose a
novel shape-aware topological representation that amplifies structural features
in the input data, thereby improving the model's responsiveness to the
geometrical features of buried objects. To address the scarcity of annotated
real-world data, we employ a Sim2Real strategy that generates diverse and
realistic synthetic datasets, effectively bridging the gap between simulated
and real-world domains. Experimental results demonstrate significant
improvements in mean Average Precision (mAP), validating the robustness and
efficacy of our approach. This approach underscores the potential of
TDA-enhanced learning in achieving reliable, real-time subsurface object
detection, with broad applications in urban planning, safety inspection, and
infrastructure management.

</details>


### [471] [An Open-Source Python Framework and Synthetic ECG Image Datasets for Digitization, Lead and Lead Name Detection, and Overlapping Signal Segmentation](https://arxiv.org/abs/2506.06315)
*Masoud Rahimi,Reza Karbasi,Abdol-Hossein Vahabie*

Main category: eess.SP

TL;DR: 介绍用于生成合成心电图图像数据集的开源Python框架，生成四个开放数据集并公开代码和数据链接。


<details>
  <summary>Details</summary>
Motivation: 推进基于深度学习的心电图分析关键任务，如数字化、区域和名称检测、波形分割。

Method: 使用PTB - XL信号数据集，利用框架生成四种不同类型数据集。

Result: 生成四个开放访问数据集，包括不同导联配置心电图图像、带标注的图像、单导联裁剪图像及分割掩码。

Conclusion: 开源框架和数据集可公开获取，利于心电图分析相关研究。

Abstract: We introduce an open-source Python framework for generating synthetic ECG
image datasets to advance critical deep learning-based tasks in ECG analysis,
including ECG digitization, lead region and lead name detection, and
pixel-level waveform segmentation. Using the PTB-XL signal dataset, our
proposed framework produces four open-access datasets: (1) ECG images in
various lead configurations paired with time-series signals for ECG
digitization, (2) ECG images annotated with YOLO-format bounding boxes for
detection of lead region and lead name, (3)-(4) cropped single-lead images with
segmentation masks compatible with U-Net-based models in normal and overlapping
versions. In the overlapping case, waveforms from neighboring leads are
superimposed onto the target lead image, while the segmentation masks remain
clean. The open-source Python framework and datasets are publicly available at
https://github.com/rezakarbasi/ecg-image-and-signal-dataset and
https://doi.org/10.5281/zenodo.15484519, respectively.

</details>


### [472] [Composite Reward Design in PPO-Driven Adaptive Filtering](https://arxiv.org/abs/2506.06323)
*Abdullah Burkan Bereketoglu*

Main category: eess.SP

TL;DR: 提出基于PPO的自适应滤波框架，在合成信号实验中表现优于经典滤波器，证明策略梯度强化学习用于自适应信号滤波的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统滤波器在动态、非平稳环境去噪有局限性，如受平稳假设限制、需复杂微调等。

Method: 提出使用近端策略优化（PPO）的自适应滤波框架，由平衡SNR改善、MSE降低和残差平滑度的复合奖励引导。

Result: 在不同噪声类型的合成信号实验中，PPO代理能泛化，实现实时性能，优于经典滤波器。

Conclusion: 策略梯度强化学习可用于鲁棒、低延迟的自适应信号滤波。

Abstract: Model-free and reinforcement learning-based adaptive filtering methods are
gaining traction for denoising in dynamic, non-stationary environments such as
wireless signal channels. Traditional filters like LMS, RLS, Wiener, and Kalman
are limited by assumptions of stationary or requiring complex fine-tuning or
exact noise statistics or fixed models. This letter proposes an adaptive
filtering framework using Proximal Policy Optimization (PPO), guided by a
composite reward that balances SNR improvement, MSE reduction, and residual
smoothness. Experiments on synthetic signals with various noise types show that
our PPO agent generalizes beyond its training distribution, achieving real-time
performance and outperforming classical filters. This work demonstrates the
viability of policy-gradient reinforcement learning for robust, low-latency
adaptive signal filtering.

</details>


### [473] [Uncertainty-Aware Multi-view Arrhythmia Classification from ECG](https://arxiv.org/abs/2506.06342)
*Mohd Ashhad,Sana Rahmani,Mohammed Fayiz,Ali Etemad,Javad Hashemi*

Main category: eess.SP

TL;DR: 提出用于心电图心律失常分类的深度神经网络架构，实验显示性能和鲁棒性提升


<details>
  <summary>Details</summary>
Motivation: 实现心电图心律失常的不确定性感知多视图分类

Method: 学习单导联心电图的1D和2D视图，用融合技术减少冲突，框架含时间序列、图像空间学习和不确定性感知融合模块

Result: 在两个真实数据集上实验，相比现有技术提高心律失常分类性能，对噪声和伪影更具鲁棒性

Conclusion: 所提框架在心律失常分类上表现更好，有更强鲁棒性

Abstract: We propose a deep neural architecture that performs uncertainty-aware
multi-view classification of arrhythmia from ECG. Our method learns two
different views (1D and 2D) of single-lead ECG to capture different types of
information. We use a fusion technique to reduce the conflict between the
different views caused by noise and artifacts in ECG data, thus incorporating
uncertainty to obtain stronger final predictions. Our framework contains the
following three modules (1) a time-series module to learn the morphological
features from ECG; (2) an image-space learning module to learn the
spatiotemporal features; and (3) the uncertainty-aware fusion module to fuse
the information from the two different views. Experimental results on two
real-world datasets demonstrate that our framework not only improves the
performance on arrhythmia classification compared to the state-of-the-art but
also shows better robustness to noise and artifacts present in ECG.

</details>


### [474] [LD-RPMNet: Near-Sensor Diagnosis for Railway Point Machines](https://arxiv.org/abs/2506.06346)
*Wei Li,Xiaochun Wu,Xiaoxi Hu,Yuxuan Zhang,Sebastian Bader,Yuhan Huang*

Main category: eess.SP

TL;DR: 提出轻量级模型LD - RPMNet用于铁路近传感器故障诊断，优化后模型降参提效且提升准确率。


<details>
  <summary>Details</summary>
Motivation: 近传感器诊断在工业中愈发普遍，为铁路实际应用优化计算效率。

Method: 提出LD - RPMNet模型，引入MDSC模块增强特征提取，采用BSA机制提升计算效率。

Result: 优化后模型参数和计算复杂度降低50%，诊断准确率提升近3%，达98.86%。

Conclusion: 证明了铁路转辙机近传感器故障诊断应用的可能性。

Abstract: Near-sensor diagnosis has become increasingly prevalent in industry. This
study proposes a lightweight model named LD-RPMNet that integrates Transformers
and Convolutional Neural Networks, leveraging both local and global feature
extraction to optimize computational efficiency for a practical railway
application. The LD-RPMNet introduces a Multi-scale Depthwise Separable
Convolution (MDSC) module, which decomposes cross-channel convolutions into
pointwise and depthwise convolutions while employing multi-scale kernels to
enhance feature extraction. Meanwhile, a Broadcast Self-Attention (BSA)
mechanism is incorporated to simplify complex matrix multiplications and
improve computational efficiency. Experimental results based on collected sound
signals during the operation of railway point machines demonstrate that the
optimized model reduces parameter count and computational complexity by 50%
while improving diagnostic accuracy by nearly 3%, ultimately achieving an
accuracy of 98.86%. This demonstrates the possibility of near-sensor fault
diagnosis applications in railway point machines.

</details>


### [475] [Multi-Platform Methane Plume Detection via Model and Domain Adaptation](https://arxiv.org/abs/2506.06348)
*Vassiliki Mancoridis,Brian Bue,Jake H. Lee,Andrew K. Thorpe,Daniel Cusworth,Alana Ayasse,Philip G. Brodrick,Riley Duren*

Main category: eess.SP

TL;DR: 本文探讨利用机载观测改进星载甲烷羽流检测，解决跨平台数据分布差异问题，提出模型和数据驱动方法并取得较好效果。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多遥感平台用于甲烷羽流检测，需解决跨平台对齐问题。

Method: 采用模型和数据驱动的机器学习方法，使用迁移学习改进分类器，用CycleGAN对齐数据分布。

Result: 迁移学习改进的分类器优于独立星载模型，用CycleGAN转换数据后应用机载分类器检测效果最佳。

Conclusion: 该方法不仅适用于数据模拟，还可直接进行数据对齐，展示了一种数据驱动的方法来对齐不同遥感仪器的数据。

Abstract: Prioritizing methane for near-term climate action is crucial due to its
significant impact on global warming. Previous work used columnwise matched
filter products from the airborne AVIRIS-NG imaging spectrometer to detect
methane plume sources; convolutional neural networks (CNNs) discerned
anthropogenic methane plumes from false positive enhancements. However, as an
increasing number of remote sensing platforms are used for methane plume
detection, there is a growing need to address cross-platform alignment. In this
work, we describe model- and data-driven machine learning approaches that
leverage airborne observations to improve spaceborne methane plume detection,
reconciling the distributional shifts inherent with performing the same task
across platforms. We develop a spaceborne methane plume classifier using data
from the EMIT imaging spectroscopy mission. We refine classifiers trained on
airborne imagery from AVIRIS-NG campaigns using transfer learning,
outperforming the standalone spaceborne model. Finally, we use CycleGAN, an
unsupervised image-to-image translation technique, to align the data
distributions between airborne and spaceborne contexts. Translating spaceborne
EMIT data to the airborne AVIRIS-NG domain using CycleGAN and applying airborne
classifiers directly yields the best plume detection results. This methodology
is useful not only for data simulation, but also for direct data alignment.
Though demonstrated on the task of methane plume detection, our work more
broadly demonstrates a data-driven approach to align related products obtained
from distinct remote sensing instruments.

</details>


### [476] [Heart Rate Classification in ECG Signals Using Machine Learning and Deep Learning](https://arxiv.org/abs/2506.06349)
*Thien Nhan Vo,Thanh Xuan Truong*

Main category: eess.SP

TL;DR: 研究用传统机器学习和深度学习两种方法对心电图信号心跳分类，发现LightGBM表现最佳，手工特征比图像表示更优。


<details>
  <summary>Details</summary>
Motivation: 解决心电图信号心跳分类问题

Method: 采用传统机器学习提取手工特征训练多种分类器，以及将心电图信号转换为图像用CNN架构分类

Result: LightGBM模型准确率99%，F1分数0.94，优于基于图像的CNN方法；SVM和AdaBoost分数低

Conclusion: 手工特征在捕捉心电图信号变化上更优，未来可结合多导联信号和时间依赖提升分类精度

Abstract: This study addresses the classification of heartbeats from ECG signals
through two distinct approaches: traditional machine learning utilizing
hand-crafted features and deep learning via transformed images of ECG beats.
The dataset underwent preprocessing steps, including downsampling, filtering,
and normalization, to ensure consistency and relevance for subsequent analysis.
In the first approach, features such as heart rate variability (HRV), mean,
variance, and RR intervals were extracted to train various classifiers,
including SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and
LightGBM. The second approach involved transforming ECG signals into images
using Gramian Angular Field (GAF), Markov Transition Field (MTF), and
Recurrence Plots (RP), with these images subsequently classified using CNN
architectures like VGG and Inception.
  Experimental results demonstrate that the LightGBM model achieved the highest
performance, with an accuracy of 99% and an F1 score of 0.94, outperforming the
image-based CNN approach (F1 score of 0.85). Models such as SVM and AdaBoost
yielded significantly lower scores, indicating limited suitability for this
task. The findings underscore the superior ability of hand-crafted features to
capture temporal and morphological variations in ECG signals compared to
image-based representations of individual beats. Future investigations may
benefit from incorporating multi-lead ECG signals and temporal dependencies
across successive beats to enhance classification accuracy further.

</details>


### [477] [Towards Generalizable Drowsiness Monitoring with Physiological Sensors: A Preliminary Study](https://arxiv.org/abs/2506.06360)
*Jiyao Wang,Suzan Ayas,Jiahao Zhang,Xiao Wen,Dengbo He,Birsen Donmez*

Main category: eess.SP

TL;DR: 本文分析多数据集生理信号特征以检测困倦，发现不同困倦诱因有不同生理反应，明确与困倦相关指标，助力后续监测设计。


<details>
  <summary>Details</summary>
Motivation: 准确检测困倦对驾驶安全至关重要，现有生理指标与困倦标签关联存在冲突，需进一步研究。

Method: 分析四个数据集的心电图、皮肤电活动和呼吸信号的关键特征，构建二元逻辑回归模型。

Result: 不同困倦诱因导致不同生理反应，客观评估比主观评估更敏感，心率稳定性增加、呼吸幅度减小和皮肤电活动张力降低与困倦增加相关。

Conclusion: 研究结果增进对困倦检测的理解，可为未来通用监测设计提供参考。

Abstract: Accurately detecting drowsiness is vital to driving safety. Among all
measures, physiological-signal-based drowsiness monitoring can be more
privacy-preserving than a camera-based approach. However, conflicts exist
regarding how physiological metrics are associated with different drowsiness
labels across datasets. Thus, we analyzed key features from electrocardiograms
(ECG), electrodermal activity (EDA), and respiratory (RESP) signals across four
datasets, where different drowsiness inducers (such as fatigue and low arousal)
and assessment methods (subjective vs. objective) were used. Binary logistic
regression models were built to identify the physiological metrics that are
associated with drowsiness. Findings indicate that distinct different
drowsiness inducers can lead to different physiological responses, and
objective assessments were more sensitive than subjective ones in detecting
drowsiness. Further, the increased heart rate stability, reduced respiratory
amplitude, and decreased tonic EDA are robustly associated with increased
drowsiness. The results enhance understanding of drowsiness detection and can
inform future generalizable monitoring designs.

</details>


### [478] [Transformer-Based Decomposition of Electrodermal Activity for Real-World Mental Health Applications](https://arxiv.org/abs/2506.06378)
*Charalampos Tsirmpas,Stasinos Konstantopoulos,Dimitris Andrikopoulos,Konstantina Kyriakouli,Panagiotis Fatouros*

Main category: eess.SP

TL;DR: 研究对比知识驱动、统计和深度学习方法对穿戴设备采集的皮电信号分解，提出Feel Transformer模型，在保真度和鲁棒性上取得平衡，有实时分析和应用潜力。


<details>
  <summary>Details</summary>
Motivation: 分解皮电活动信号为相位和基线成分对提取情感和生理生物标志物很重要，需对比不同方法进行信号分解。

Method: 对比知识驱动、统计和深度学习方法，提出基于Autoformer架构的Feel Transformer模型，利用池化和去趋势机制进行无监督分解。

Result: Feel Transformer在特征保真度和对真实噪声数据的鲁棒性上取得平衡。

Conclusion: 该模型有实时生物信号分析潜力，可用于压力预测、数字心理健康干预和生理预测等未来应用。

Abstract: Decomposing Electrodermal Activity (EDA) into phasic (short-term,
stimulus-linked responses) and tonic (longer-term baseline) components is
essential for extracting meaningful emotional and physiological biomarkers.
This study presents a comparative analysis of knowledge-driven, statistical,
and deep learning-based methods for EDA signal decomposition, with a focus on
in-the-wild data collected from wearable devices. In particular, the authors
introduce the Feel Transformer, a novel Transformer-based model adapted from
the Autoformer architecture, designed to separate phasic and tonic components
without explicit supervision. The model leverages pooling and trend-removal
mechanisms to enforce physiologically meaningful decompositions. Comparative
experiments against methods such as Ledalab, cvxEDA, and conventional
detrending show that the Feel Transformer achieves a balance between feature
fidelity (SCR frequency, amplitude, and tonic slope) and robustness to noisy,
real-world data. The model demonstrates potential for real-time biosignal
analysis and future applications in stress prediction, digital mental health
interventions, and physiological forecasting.

</details>


### [479] [IQFM A Wireless Foundational Model for I/Q Streams in AI-Native 6G](https://arxiv.org/abs/2506.06718)
*Omar Mashaal,Hatem Abou-Zeid*

Main category: eess.SP

TL;DR: 本文提出首个用于无线通信的I/Q信号基础模型IQFM，支持多任务，采用任务感知增强策略，模型在多任务表现出色，展示了其在6G系统多任务学习的潜力。


<details>
  <summary>Details</summary>
Motivation: 基础模型在无线通信领域尚处起步阶段，直接处理原始IQ数据的基础模型研究较少，需要探索其在无线通信中的应用。

Method: 提出IQFM模型，引入任务感知增强策略，在对比自监督学习框架下进行结构化、任务依赖的表征学习。

Result: 轻量级编码器在调制和AoA分类上准确率高，少量标记样本下性能优于监督基线；模型能泛化到分布外任务，参数微调后在新任务上表现良好。

Conclusion: 基于原始IQ的基础模型有潜力成为AI原生6G系统高效、可复用的多任务学习编码器。

Abstract: Foundational models have shown remarkable potential in natural language
processing and computer vision, yet remain in their infancy in wireless
communications. While a few efforts have explored image-based modalities such
as channel state information (CSI) and frequency spectrograms, foundational
models that operate directly on raw IQ data remain largely unexplored. This
paper presents, IQFM, the first I/Q signal foundational model for wireless
communications. IQFM supporting diverse tasks: modulation classification,
angle-of-arrival (AoA), beam prediction, and RF fingerprinting, without heavy
preprocessing or handcrafted features. We also introduce a task-aware
augmentation strategy that categorizes transformations into core augmentations,
such as cyclic time shifting, and task-specific augmentations. This strategy
forms the basis for structured, task-dependent representation learning within a
contrastive self-supervised learning (SSL) framework. Using this strategy, the
lightweight encoder, pre-trained via SSL on over-the-air multi-antenna IQ data,
achieves up to 99.67% and 65.45% accuracy on modulation and AoA classification,
respectively, using only one labeled sample per class, outperforming supervised
baselines by up to 7x and 145x. The model also generalizes to
out-of-distribution tasks; when adapted to new tasks using only 500 samples per
class and minimal parameter updates via LoRA, the same frozen encoder achieves
94.15% on beam prediction (vs. 89.53% supervised), 50.00% on RML2016a
modulation classification (vs. 49.30%), and 96.05% on RF fingerprinting (vs.
96.64%). These results demonstrate the potential of raw IQ-based foundational
models as efficient, reusable encoders for multi-task learning in AI-native 6G
systems.

</details>


### [480] [Conditional Denoising Diffusion for ISAC Enhanced Channel Estimation in Cell-Free 6G](https://arxiv.org/abs/2506.06942)
*Mohammad Farzanullah,Han Zhang,Akram Bin Sediq,Ali Afana,Melike Erol-Kantarci*

Main category: eess.SP

TL;DR: 本文提出利用传感信息和条件去噪扩散模型（CDDM）与多模态变压器（MMT）结合的框架，提升无蜂窝集成传感与通信（ISAC）系统的信道估计性能，仿真显示该方法有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 无蜂窝ISAC系统中信道估计性能受导频污染和噪声信道估计等挑战限制，需提升其性能。

Method: 提出利用传感信息作为关键输入的CDDM框架，将CDDM与MMT集成以增强信道估计。

Result: 与最小二乘法（LS）和最小均方误差法（MMSE）相比，归一化均方误差（NMSE）分别提升8dB和9dB；与传统去噪扩散模型（TDDM）相比，NMSE提升27.8%；对导频污染有更高鲁棒性，在低信噪比等条件下保持高精度。

Conclusion: 所提模型通过利用传感与通信信道的相关性，在靠近传感目标的用户中表现良好，能有效提升无蜂窝ISAC系统信道估计性能。

Abstract: Cell-free Integrated Sensing and Communication (ISAC) aims to revolutionize
6th Generation (6G) networks. By combining distributed access points with ISAC
capabilities, it boosts spectral efficiency, situational awareness, and
communication reliability. Channel estimation is a critical step in cell-free
ISAC systems to ensure reliable communication, but its performance is usually
limited by challenges such as pilot contamination and noisy channel estimates.
This paper presents a novel framework leveraging sensing information as a key
input within a Conditional Denoising Diffusion Model (CDDM). In this framework,
we integrate CDDM with a Multimodal Transformer (MMT) to enhance channel
estimation in ISAC-enabled cell-free systems. The MMT encoder effectively
captures inter-modal relationships between sensing and location data, enabling
the CDDM to iteratively denoise and refine channel estimates. Simulation
results demonstrate that the proposed approach achieves significant performance
gains. As compared with Least Squares (LS) and Minimum Mean Squared Error
(MMSE) estimators, the proposed model achieves normalized mean squared error
(NMSE) improvements of 8 dB and 9 dB, respectively. Moreover, we achieve a
27.8% NMSE improvement compared to the traditional denoising diffusion model
(TDDM), which does not incorporate sensing channel information. Additionally,
the model exhibits higher robustness against pilot contamination and maintains
high accuracy under challenging conditions, such as low signal-to-noise ratios
(SNRs). According to the simulation results, the model performs well for users
near sensing targets by leveraging the correlation between sensing and
communication channels.

</details>


### [481] [Diffusion Models-Aided Uplink Channel Estimation for RIS-Assisted Systems](https://arxiv.org/abs/2506.07770)
*Yang Wang,Yin Xu,Cixiao Zhang,Zhiyong Chen,Xiaowu Ou,Mingzeng Dai,Meixia Tao,Wenjun Zhang*

Main category: eess.SP

TL;DR: 本文提出基于扩散模型的可重构智能表面辅助系统信道估计方法，采用确定性采样策略和轻量级网络，仿真显示性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决传统扩散模型反向过程随机性问题，提高信道估计准确性和方法实用性。

Method: 将信道估计问题转化为去噪过程，采用带步长对齐机制的确定性采样策略，设计轻量级网络减少参数。

Result: 在不同信噪比下性能优于基线，如在SNR = 0 dB时归一化均方误差最多提升13.5 dB；轻量级网络参数仅为原U - Net的6.59%，性能几乎无损失。

Conclusion: 所提方法在信道估计中表现良好，轻量级网络实用且高效。

Abstract: This letter proposes a channel estimation method for reconfigurable
intelligent surface (RIS)-assisted systems through a novel diffusion model (DM)
framework. We reformulate the channel estimation problem as a denoising
process, which aligns with the reverse process of the DM. To overcome the
inherent randomness in the reverse process of conventional DM approaches, we
adopt a deterministic sampling strategy with a step alignment mechanism that
ensures the accuracy of channel estimation while adapting to different
signal-to-noise ratio (SNR). Furthermore, to reduce the number of parameters of
the U-Net, we meticulously design a lightweight network that achieves
comparable performance, thereby enhancing the practicality of our proposed
method. Extensive simulations demonstrate superior performance over a wide
range of SNRs compared to baselines. For instance, the proposed method achieves
performance improvements of up to 13.5 dB in normalized mean square error
(NMSE) at SNR = 0 dB. Notably, the proposed lightweight network exhibits almost
no performance loss compared to the original U-Net, while requiring only 6.59\%
of its parameters.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [482] [Data-Driven High-Dimensional Statistical Inference with Generative Models](https://arxiv.org/abs/2506.06438)
*Oz Amram,Manuel Szewc*

Main category: hep-ph

TL;DR: 介绍HI - SIGMA方法进行无分箱高维统计推断，在双希格斯测量示例中展示其优势。


<details>
  <summary>Details</summary>
Motivation: LHC测量中区分稀有过程和背景时，蒙特卡罗模拟对关键背景建模不佳，需新方法。

Method: 使用生成式ML模型学习高维空间中信号和背景分布。

Result: 与标准基于分类器的方法相比，HI - SIGMA提高了灵敏度，且可通过扩展基于直方图分析的方法纳入系统不确定性。

Conclusion: HI - SIGMA是一种有效的无分箱高维统计推断方法，适用于LHC测量。

Abstract: Crucial to many measurements at the LHC is the use of correlated
multi-dimensional information to distinguish rare processes from large
backgrounds, which is complicated by the poor modeling of many of the crucial
backgrounds in Monte Carlo simulations. In this work, we introduce HI-SIGMA, a
method to perform unbinned high-dimensional statistical inference with
data-driven background distributions. In contradistinction to many applications
of Simulation Based Inference in High Energy Physics, HI-SIGMA relies on
generative ML models, rather than classifiers, to learn the signal and
background distributions in the high-dimensional space. These ML models allow
for efficient, interpretable inference while also incorporating model errors
and other sources of systematic uncertainties. We showcase this methodology on
a simplified version of a di-Higgs measurement in the $bb\gamma\gamma$ final
state, where the di-photon resonance allows for efficient background
interpolation from sidebands into the signal region. We demonstrate that
HI-SIGMA provides improved sensitivity as compared to standard classifier-based
methods, and that systematic uncertainties can be straightforwardly
incorporated by extending methods which have been used for histogram based
analyses.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [483] [Template-Guided 3D Molecular Pose Generation via Flow Matching and Differentiable Optimization](https://arxiv.org/abs/2506.06305)
*Noémie Bergues,Arthur Carré,Paul Join-Lambert,Brice Hoffmann,Arnaud Blondel,Hamza Tajmouati*

Main category: q-bio.BM

TL;DR: 提出基于模板的两阶段配体构象生成方法，在新基准测试中表现优于标准对接工具和开放访问对齐方法。


<details>
  <summary>Details</summary>
Motivation: 预测蛋白质结合位点中小分子的3D构象是药物设计的关键挑战，有结晶参考配体（模板）时可提供几何先验来指导3D构象预测。

Method: 两阶段方法，第一阶段基于流匹配的分子对齐方法生成配体3D坐标；第二阶段基于形状和药效团相似性、内能及可选的蛋白质结合口袋进行可微构象优化。

Result: 在新的配体对基准测试中，该方法优于标准对接工具和开放访问对齐方法，在与模板低相似性或配体高灵活性的情况下表现更佳。

Conclusion: 所提出的两阶段方法在配体构象生成方面具有优势，是一种有效的解决方案。

Abstract: Predicting the 3D conformation of small molecules within protein binding
sites is a key challenge in drug design. When a crystallized reference ligand
(template) is available, it provides geometric priors that can guide 3D pose
prediction. We present a two-stage method for ligand conformation generation
guided by such templates. In the first stage, we introduce a molecular
alignment approach based on flow-matching to generate 3D coordinates for the
ligand, using the template structure as a reference. In the second stage, a
differentiable pose optimization procedure refines this conformation based on
shape and pharmacophore similarities, internal energy, and, optionally, the
protein binding pocket. We evaluate our approach on a new benchmark of ligand
pairs co-crystallized with the same target and show that it outperforms
standard docking tools and open-access alignment methods, especially in cases
involving low similarity to the template or high ligand flexibility.

</details>


### [484] [AnnoDPO: Protein Functional Annotation Learning with Direct Preference Optimization](https://arxiv.org/abs/2506.07035)
*Zixuan Jiang,Renjing Xu*

Main category: q-bio.BM

TL;DR: 提出AnnoDPO多模态框架用于蛋白质功能预测，解决注释稀缺和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 蛋白质功能预测对蛋白质语言模型有挑战，因功能注释类别多且注释实例分布不均衡。

Method: 提出AnnoDPO框架，利用Direct Preference Optimization（DPO）通过偏好对齐训练目标解决问题。

Result: 文中未明确提及具体结果。

Conclusion: 该方法建立了蛋白质表示学习中生物知识整合的新范式。

Abstract: Deciphering protein function remains a fundamental challenge in protein
representation learning. The task presents significant difficulties for protein
language models (PLMs) due to the sheer volume of functional annotation
categories and the highly imbalanced distribution of annotated instances across
biological ontologies. Inspired by the remarkable success of reinforcement
learning from human feedback (RLHF) in large language model (LLM) alignment, we
propose AnnoDPO, a novel multi-modal framework for protein function prediction
that leverages Direct Preference Optimization (DPO) to enhance annotation
learning. Our methodology addresses the dual challenges of annotation scarcity
and category imbalance through preference-aligned training objectives,
establishing a new paradigm for biological knowledge integration in protein
representation learning.

</details>


### [485] [Graph Neural Networks in Modern AI-aided Drug Discovery](https://arxiv.org/abs/2506.06915)
*Odin Zhang,Haitao Lin,Xujun Zhang,Xiaorui Wang,Zhenxing Wu,Qing Ye,Weibo Zhao,Jike Wang,Kejun Ying,Yu Kang,Chang-yu Hsieh,Tingjun Hou*

Main category: q-bio.BM

TL;DR: 本文全面综述图神经网络（GNNs）在药物发现中的方法基础和应用，介绍最新进展、与深度学习结合方式，指出挑战并探讨未来方向。


<details>
  <summary>Details</summary>
Motivation: GNNs作为深度学习中拓扑感知模型，在人工智能辅助药物发现（AIDD）中作用重要，需要全面了解其在药物发现中的应用。

Method: 对GNNs在药物发现中的方法基础和代表性应用进行全面综述，介绍最新方法进展及与现代深度学习方法的结合。

Result: 梳理了GNNs在药物发现多任务中的应用，介绍最新方法及与深度学习结合情况，指出实际应用中的挑战和瓶颈。

Conclusion: 对GNNs在药物发现中的应用进行总结，并探讨未来研究方向。

Abstract: Graph neural networks (GNNs), as topology/structure-aware models within deep
learning, have emerged as powerful tools for AI-aided drug discovery (AIDD). By
directly operating on molecular graphs, GNNs offer an intuitive and expressive
framework for learning the complex topological and geometric features of
drug-like molecules, cementing their role in modern molecular modeling. This
review provides a comprehensive overview of the methodological foundations and
representative applications of GNNs in drug discovery, spanning tasks such as
molecular property prediction, virtual screening, molecular generation,
biomedical knowledge graph construction, and synthesis planning. Particular
attention is given to recent methodological advances, including geometric GNNs,
interpretable models, uncertainty quantification, scalable graph architectures,
and graph generative frameworks. We also discuss how these models integrate
with modern deep learning approaches, such as self-supervised learning,
multi-task learning, meta-learning and pre-training. Throughout this review, we
highlight the practical challenges and methodological bottlenecks encountered
when applying GNNs to real-world drug discovery pipelines, and conclude with a
discussion on future directions.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [486] [A Neuronal Model at the Edge of Criticality: An Ising-Inspired Approach to Brain Dynamics](https://arxiv.org/abs/2506.07027)
*Sajedeh Sarmastani,Maliheh Ghodrat,Yousef Jamali*

Main category: q-bio.NC

TL;DR: 提出受Ising模型启发的神经网络模型，引入生理约束，展示随温度变化的活动转变及临界特性，为研究神经临界性提供工具。


<details>
  <summary>Details</summary>
Motivation: 研究神经临界性，探索大脑如何实现最优信息处理。

Method: 构建受Ising模型启发的神经网络，神经元为二进制自旋，异步更新遵循Metropolis动力学，引入类似温度参数T和固定开关持续时间。

Result: 随T变化网络从异步到同步活动，在临界T_c处有临界特征，类似皮层记录模式。

Conclusion: 简单自旋相互作用和生理约束可产生复杂涌现行为，该模型是研究神经临界性的有用工具。

Abstract: We present a neuronal network model inspired by the Ising model, where each
neuron is a binary spin ($s_i = \pm1$) interacting with its neighbors on a 2D
lattice. Updates are asynchronous and follow Metropolis dynamics, with a
temperature-like parameter $T$ introducing stochasticity.
  To incorporate physiological realism, each neuron includes fixed on/off
durations, mimicking the refractory period found in real neurons. These
counters prevent immediate reactivation, adding biologically grounded timing
constraints to the model.
  As $T$ varies, the network transitions from asynchronous to synchronised
activity. Near a critical point $T_c$, we observe hallmarks of criticality:
heightened fluctuations, long-range correlations, and increased sensitivity.
These features resemble patterns found in cortical recordings, supporting the
hypothesis that the brain operates near criticality for optimal information
processing.
  This simplified model demonstrates how basic spin interactions and
physiological constraints can yield complex, emergent behavior, offering a
useful tool for studying criticality in neural systems through statistical
physics.

</details>


### [487] [Less is More: some Computational Principles based on Parcimony, and Limitations of Natural Intelligence](https://arxiv.org/abs/2506.07060)
*Laura Cohen,Xavier Hinaut,Lilyana Petrova,Alexandre Pitti,Syd Reynal,Ichiro Tsuda*

Main category: q-bio.NC

TL;DR: 本文指出自然智能以少胜多，约束是其高效、适应和创新的催化剂，介绍相关机制并建议AI采用“少即是多”原则。


<details>
  <summary>Details</summary>
Motivation: 对比自然智能和当前AI在资源利用上的差异，探讨自然智能约束条件的价值，为AI发展提供新思路。

Method: 分析有限神经带宽促进简洁编码、混沌游移、水库计算等机制，从发展视角强调内在动机和社交环境对婴儿学习的作用。

Result: 揭示自然智能约束带来的优势，如高效编码、灵活记忆检索、快速泛化等。

Conclusion: AI采用“少即是多”原则，可发展出更高效、可解释和基于生物的人工系统。

Abstract: Natural intelligence (NI) consistently achieves more with less. Infants learn
language, develop abstract concepts, and acquire sensorimotor skills from
sparse data, all within tight neural and energy limits. In contrast, today's AI
relies on virtually unlimited computational power, energy, and data to reach
high performance. This paper argues that constraints in NI are paradoxically
catalysts for efficiency, adaptability, and creativity. We first show how
limited neural bandwidth promotes concise codes that still capture complex
patterns. Spiking neurons, hierarchical structures, and symbolic-like
representations emerge naturally from bandwidth constraints, enabling robust
generalization. Next, we discuss chaotic itinerancy, illustrating how the brain
transits among transient attractors to flexibly retrieve memories and manage
uncertainty. We then highlight reservoir computing, where random projections
facilitate rapid generalization from small datasets. Drawing on developmental
perspectives, we emphasize how intrinsic motivation, along with responsive
social environments, drives infant language learning and discovery of meaning.
Such active, embodied processes are largely absent in current AI. Finally, we
suggest that adopting 'less is more' principles -- energy constraints,
parsimonious architectures, and real-world interaction -- can foster the
emergence of more efficient, interpretable, and biologically grounded
artificial systems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [488] [Execution-Aware Program Reduction for WebAssembly via Record and Replay](https://arxiv.org/abs/2506.07834)
*Doehyun Baek,Daniel Lehmann,Ben L. Titzer,Sukyoung Ryu,Michael Pradel*

Main category: cs.PL

TL;DR: 本文提出针对WebAssembly程序的执行感知程序缩减技术RR - Reduce和Hybrid - Reduce，评估显示其性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有执行无关的程序缩减技术在处理大型复杂WebAssembly程序时存在困难，需要利用程序执行行为信息来改进。

Method: 提出RR - Reduce和Hybrid - Reduce技术，前者通过记录和重放利用执行行为，后者结合执行无关缩减技术与RR - Reduce。

Result: RR - Reduce能将程序缩减至原大小1.20%，用时14.5分钟，在缩减时间上优于现有技术33.15倍；Hybrid - Reduce能将程序缩减至原大小0.13%，用时3.5小时，在缩减大小和时间上分别优于现有技术3.42倍和2.26倍。

Conclusion: RR - Reduce适合快速按需调试，Hybrid - Reduce适合开发者需要最小程序的场景。

Abstract: WebAssembly (Wasm) programs may trigger bugs in their engine implementations.
To aid debugging, program reduction techniques try to produce a smaller variant
of the input program that still triggers the bug. However, existing
execution-unaware program reduction techniques struggle with large and complex
Wasm programs, because they rely on static information and apply syntactic
transformations, while ignoring the valuable information offered by the input
program's execution behavior.
  We present RR-Reduce and Hybrid-Reduce, novel execution-aware program
reduction techniques that leverage execution behaviors via record and replay.
RR-Reduce identifies a bug-triggering function as the target function, isolates
that function from the rest of the program, and generates a reduced program
that replays only the interactions between the target function and the rest of
the program. Hybrid-Reduce combines a complementary execution-unaware reduction
technique with RR-Reduce to further reduce program size.
  We evaluate RR-Reduce and Hybrid-Reduce on 28 Wasm programs that trigger a
diverse set of bugs in three engines. On average, RR-Reduce reduces the
programs to 1.20 percent of their original size in 14.5 minutes, which
outperforms the state of the art by 33.15 times in terms of reduction time.
Hybrid-Reduce reduces the programs to 0.13 percent of their original size in
3.5 hours, which outperforms the state of the art by 3.42 times in terms of
reduced program size and 2.26 times in terms of reduction time. We envision
RR-Reduce as the go-to tool for rapid, on-demand debugging in minutes, and
Hybrid-Reduce for scenarios where developers require the smallest possible
programs.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [489] [Deep regularization networks for inverse problems with noisy operators](https://arxiv.org/abs/2506.07008)
*Fatemeh Pourahmadian,Yang Xu*

Main category: math.NA

TL;DR: 提出一种监督学习方法用于大型逆问题正则化，加速时空正则化过程实现实时成像，在弹性板损伤演化成像中验证该方法能加速成像并提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 加速特定逆问题的时空正则化过程，实现实时成像。

Method: 用神经算子将散射方程右侧模式映射到正则化参数，分两步训练网络，提出自适应选择损失权重的新方法。

Result: 该方法可直接从测试数据学习，训练好的网络能快速为高分辨率成像生成密集正则化映射，差异信息正则化网络加速成像且提升复杂环境下图像质量。

Conclusion: 差异信息正则化网络能加速成像过程，显著提升复杂环境下的图像质量，训练损失函数对网络泛化性很重要。

Abstract: A supervised learning approach is proposed for regularization of large
inverse problems where the main operator is built from noisy data. This is
germane to superresolution imaging via the sampling indicators of the inverse
scattering theory. We aim to accelerate the spatiotemporal regularization
process for this class of inverse problems to enable real-time imaging. In this
approach, a neural operator maps each pattern on the right-hand side of the
scattering equation to its affiliated regularization parameter. The network is
trained in two steps which entails: (1) training on low-resolution
regularization maps furnished by the Morozov discrepancy principle with
nonoptimal thresholds, and (2) optimizing network predictions through
minimization of the Tikhonov loss function regulated by the validation loss.
Step 2 allows for tailoring of the approximate maps of Step 1 toward
construction of higher quality images. This approach enables direct learning
from test data and dispenses with the need for a-priori knowledge of the
optimal regularization maps. The network, trained on low-resolution data,
quickly generates dense regularization maps for high-resolution imaging. We
highlight the importance of the training loss function on the network's
generalizability. In particular, we demonstrate that networks informed by the
logic of discrepancy principle lead to images of higher contrast. In this case,
the training process involves many-objective optimization. We propose a new
method to adaptively select the appropriate loss weights during training
without requiring an additional optimization process. The proposed approach is
synthetically examined for imaging damage evolution in an elastic plate. The
results indicate that the discrepancy-informed regularization networks not only
accelerate the imaging process, but also remarkably enhance the image quality
in complex environments.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [490] [Evaluating Large Language Model Capabilities in Assessing Spatial Econometrics Research](https://arxiv.org/abs/2506.06377)
*Giuseppe Arbia,Luca Morandini,Vincenzo Nardelli*

Main category: cs.CY

TL;DR: 研究大语言模型评估空间计量经济学实证结果的能力，发现其在浅层检查有优势但深度推理有局限。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型评估空间计量经济学实证结果经济合理性和理论一致性的能力。

Method: 从28篇已发表论文创建原始和故意改动的“反事实”摘要，用多种大语言模型评估，模型给出定性评估和结构化二元分类。

Result: 大语言模型能较好评估变量选择的连贯性，评估系数合理性和发表适用性时表现差异大，模型选择、论文特征及其相互作用影响评估准确性。

Conclusion: 大语言模型可辅助同行评审初始浅层检查，但全面深度经济推理有局限，仍需人工监督。

Abstract: This paper investigates Large Language Models (LLMs) ability to assess the
economic soundness and theoretical consistency of empirical findings in spatial
econometrics. We created original and deliberately altered "counterfactual"
summaries from 28 published papers (2005-2024), which were evaluated by a
diverse set of LLMs. The LLMs provided qualitative assessments and structured
binary classifications on variable choice, coefficient plausibility, and
publication suitability. The results indicate that while LLMs can expertly
assess the coherence of variable choices (with top models like GPT-4o achieving
an overall F1 score of 0.87), their performance varies significantly when
evaluating deeper aspects such as coefficient plausibility and overall
publication suitability. The results further revealed that the choice of LLM,
the specific characteristics of the paper and the interaction between these two
factors significantly influence the accuracy of the assessment, particularly
for nuanced judgments. These findings highlight LLMs' current strengths in
assisting with initial, more surface-level checks and their limitations in
performing comprehensive, deep economic reasoning, suggesting a potential
assistive role in peer review that still necessitates robust human oversight.

</details>


### [491] [LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment](https://arxiv.org/abs/2506.06355)
*Lingyao Li,Dawei Li,Zhenhui Ou,Xiaoran Xu,Jingxiao Liu,Zihui Ma,Runlong Yu,Min Deng*

Main category: cs.CY

TL;DR: 研究利用大语言模型（LLMs）结合多模态数据模拟地震影响，评估显示模拟结果与实际报告高度一致，表明LLMs在模拟灾害影响方面有潜力。


<details>
  <summary>Details</summary>
Motivation: 高效模拟对增强突发灾害如地震的前瞻性准备至关重要，探索利用LLMs模拟复杂地震场景。

Method: 研究多个LLMs，利用多模态数据集（地理空间、社会经济、建筑和街景图像数据）生成邮政编码和县级尺度的修正麦加利烈度（MMI）预测。

Result: 对2014年纳帕和2019年里奇克雷斯特地震的评估显示，模拟结果与美国地质调查局“你感觉到了吗？（DYFI）”报告高度一致，邮政编码层面相关性达0.88，均方根误差为0.77；RAG和ICL等技术可提升模拟性能，视觉输入比单一结构化数值数据更能提高准确性。

Conclusion: LLMs在模拟灾害影响方面有潜力，有助于加强灾前规划。

Abstract: Efficient simulation is essential for enhancing proactive preparedness for
sudden-onset disasters such as earthquakes. Recent advancements in large
language models (LLMs) as world models show promise in simulating complex
scenarios. This study examines multiple LLMs to proactively estimate perceived
earthquake impacts. Leveraging multimodal datasets including geospatial,
socioeconomic, building, and street-level imagery data, our framework generates
Modified Mercalli Intensity (MMI) predictions at zip code and county scales.
Evaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did
You Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced
by a high correlation of 0.88 and a low RMSE of 0.77 as compared to real
reports at the zip code level. Techniques such as RAG and ICL can improve
simulation performance, while visual inputs notably enhance accuracy compared
to structured numerical data alone. These findings show the promise of LLMs in
simulating disaster impacts that can help strengthen pre-event planning.

</details>


### [492] [Disentangling AI Alignment: A Structured Taxonomy Beyond Safety and Ethics](https://arxiv.org/abs/2506.06286)
*Kevin Baum*

Main category: cs.CY

TL;DR: 本文针对 AI 领域不同研究方向概念边界模糊问题，提出结构化概念框架理解 AI 对齐。


<details>
  <summary>Details</summary>
Motivation: AI 即将在不受严格控制的环境运行，确保其安全并符合规范是紧迫挑战，但 AI 安全、对齐和机器伦理等领域概念边界和关系模糊，缺乏研究定位指引。

Method: 开发结构化概念框架，引入区分对齐目标、范围和主体的分类法。

Result: 揭示了多种合理的对齐配置。

Conclusion: 为跨领域的实践和哲学整合提供基础，明确了智能体全面对齐的含义。

Abstract: Recent advances in AI research make it increasingly plausible that artificial
agents with consequential real-world impact will soon operate beyond tightly
controlled environments. Ensuring that these agents are not only safe but that
they adhere to broader normative expectations is thus an urgent
interdisciplinary challenge. Multiple fields -- notably AI Safety, AI
Alignment, and Machine Ethics -- claim to contribute to this task. However, the
conceptual boundaries and interrelations among these domains remain vague,
leaving researchers without clear guidance in positioning their work.
  To address this meta-challenge, we develop a structured conceptual framework
for understanding AI alignment. Rather than focusing solely on alignment goals,
we introduce a taxonomy distinguishing the alignment aim (safety, ethicality,
legality, etc.), scope (outcome vs. execution), and constituency (individual
vs. collective). This structural approach reveals multiple legitimate alignment
configurations, providing a foundation for practical and philosophical
integration across domains, and clarifying what it might mean for an agent to
be aligned all-things-considered.

</details>


### [493] [How Malicious AI Swarms Can Threaten Democracy](https://arxiv.org/abs/2506.06299)
*Daniel Thilo Schroeder,Meeyoung Cha,Andrea Baronchelli,Nick Bostrom,Nicholas A. Christakis,David Garcia,Amit Goldenberg,Yara Kyrychenko,Kevin Leyton-Brown,Nina Lutz,Gary Marcus,Filippo Menczer,Gordon Pennycook,David G. Rand,Frank Schweitzer,Christopher Summerfield,Audrey Tang,Jay Van Bavel,Sander van der Linden,Dawn Song,Jonas R. Kunst*

Main category: cs.CY

TL;DR: AI发展带来恶意AI集群威胁，需三管齐下应对


<details>
  <summary>Details</summary>
Motivation: AI发展使恶意AI集群出现，对民主进程构成威胁，需提出应对措施

Method: 提出三方面应对策略，包括平台侧防御、模型侧保障和系统级监督

Result: 阐述恶意AI集群可能造成的一系列不良后果

Conclusion: 鉴于民主进程易受攻击，需实施三管齐下的应对方案

Abstract: Advances in AI portend a new era of sophisticated disinformation operations.
While individual AI systems already create convincing -- and at times
misleading -- information, an imminent development is the emergence of
malicious AI swarms. These systems can coordinate covertly, infiltrate
communities, evade traditional detectors, and run continuous A/B tests, with
round-the-clock persistence. The result can include fabricated grassroots
consensus, fragmented shared reality, mass harassment, voter micro-suppression
or mobilization, contamination of AI training data, and erosion of
institutional trust. With democratic processes worldwide increasingly
vulnerable, we urge a three-pronged response: (1) platform-side defenses --
always-on swarm-detection dashboards, pre-election high-fidelity
swarm-simulation stress-tests, transparency audits, and optional client-side
"AI shields" for users; (2) model-side safeguards -- standardized
persuasion-risk tests, provenance-authenticating passkeys, and watermarking;
and (3) system-level oversight -- a UN-backed AI Influence Observatory.

</details>


### [494] [Human and AI collaboration in Fitness Education:A Longitudinal Study with a Pilates Instructor](https://arxiv.org/abs/2506.06383)
*Qian Huang,King Wang Poon*

Main category: cs.CY

TL;DR: 研究通过对普拉提教练的一年定性案例研究，探索健身教育中人与AI的协作。


<details>
  <summary>Details</summary>
Motivation: 人工智能虽将改变教学实践，但在人类专业知识旁的最佳角色不明，因此研究健身教育中人与AI的协作。

Method: 对一名普拉提教练开展为期一年的定性案例研究，研究者参与课程并每两周进行半结构化访谈，探索生成式AI融入课程规划和教学的方式。

Result: 摘要未提及。

Conclusion: 摘要未提及。

Abstract: Artificial intelligence is poised to transform teaching and coaching
practices,yet its optimal role alongside human expertise remains unclear.This
study investigates human and AI collaboration in fitness education through a
one year qualitative case study with a Pilates instructor.The researcher
participated in the instructor classes and conducted biweekly semi structured
interviews to explore how generative AI could be integrated into class planning
and instruction.

</details>


### [495] [Benchmarking Large Language Models on Homework Assessment in Circuit Analysis](https://arxiv.org/abs/2506.06390)
*Liangliang Chen,Zhihao Qin,Yiming Guo,Jacqueline Rohde,Ying Zhang*

Main category: cs.CY

TL;DR: 本文探讨大语言模型在工程教育中应用，以本科电路分析课程作业评估为例，测试不同模型表现，给出结果并指出模型局限，为后续开发可靠个性化导师提供基准和见解。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用大语言模型助力工程教育，特别是在本科电路分析课程作业评估中的应用。

Method: 开发含官方和学生电路分析问题解答的新数据集，将解答转为LaTeX格式，设计提示模板测试学生解答的五项指标。

Result: GPT - 4o和Llama 3 70B在五项指标上表现优于GPT - 3.5 Turbo，且各有优势，同时指出当前大语言模型在电路分析方面的局限。

Conclusion: 研究结果为开发可靠、个性化的电路分析导师建立基准和提供见解，提出的评估方法未来可推广到更多工程教育课程。

Abstract: Large language models (LLMs) have the potential to revolutionize various
fields, including code development, robotics, finance, and education, due to
their extensive prior knowledge and rapid advancements. This paper investigates
how LLMs can be leveraged in engineering education. Specifically, we benchmark
the capabilities of different LLMs, including GPT-3.5 Turbo, GPT-4o, and Llama
3 70B, in assessing homework for an undergraduate-level circuit analysis
course. We have developed a novel dataset consisting of official reference
solutions and real student solutions to problems from various topics in circuit
analysis. To overcome the limitations of image recognition in current
state-of-the-art LLMs, the solutions in the dataset are converted to LaTeX
format. Using this dataset, a prompt template is designed to test five metrics
of student solutions: completeness, method, final answer, arithmetic error, and
units. The results show that GPT-4o and Llama 3 70B perform significantly
better than GPT-3.5 Turbo across all five metrics, with GPT-4o and Llama 3 70B
each having distinct advantages in different evaluation aspects. Additionally,
we present insights into the limitations of current LLMs in several aspects of
circuit analysis. Given the paramount importance of ensuring reliability in
LLM-generated homework assessment to avoid misleading students, our results
establish benchmarks and offer valuable insights for the development of a
reliable, personalized tutor for circuit analysis -- a focus of our future
work. Furthermore, the proposed evaluation methods can be generalized to a
broader range of courses for engineering education in the future.

</details>


### [496] [From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law](https://arxiv.org/abs/2506.06391)
*John Mavi,Diana Teodora Găitan,Sergio Coronado*

Main category: cs.CY

TL;DR: 研究评估8个大语言模型对违反国际人道法提示的拒绝能力及回应质量，发现多数模型拒绝非法请求但回应不一，标准化安全提示有效果，复杂提示仍有漏洞，并提出评估基准。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用，但与国际人道法的契合度不明，需评估其拒绝违反法律框架提示的能力。

Method: 评估8个领先大语言模型拒绝明确违反法律框架提示的能力，关注回应的有用性。

Result: 多数模型拒绝非法请求，但回应清晰度和一致性不同；标准化系统级安全提示提升多数模型拒绝解释质量；复杂提示存在漏洞。

Conclusion: 研究有助于开发更安全透明的AI系统，提出评估大语言模型与国际人道法合规性的基准。

Abstract: Large Language Models (LLMs) are widely used across sectors, yet their
alignment with International Humanitarian Law (IHL) is not well understood.
This study evaluates eight leading LLMs on their ability to refuse prompts that
explicitly violate these legal frameworks, focusing also on helpfulness - how
clearly and constructively refusals are communicated. While most models
rejected unlawful requests, the clarity and consistency of their responses
varied. By revealing the model's rationale and referencing relevant legal or
safety principles, explanatory refusals clarify the system's boundaries, reduce
ambiguity, and help prevent misuse. A standardised system-level safety prompt
significantly improved the quality of the explanations expressed within
refusals in most models, highlighting the effectiveness of lightweight
interventions. However, more complex prompts involving technical language or
requests for code revealed ongoing vulnerabilities. These findings contribute
to the development of safer, more transparent AI systems and propose a
benchmark to evaluate the compliance of LLM with IHL.

</details>


### [497] [Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches](https://arxiv.org/abs/2506.06540)
*Patrick Y. Wu*

Main category: cs.CY

TL;DR: 本文指出重大事件冲击后专家判断受结果影响，研究事件前感知困难。提出用大语言模型（LLMs）替代专家政治调查，以DOGE裁员为例分析，展示LLMs可快速测试冲击背后因素，最后提出使用LLMs替代的标准。


<details>
  <summary>Details</summary>
Motivation: 重大事件冲击后专家判断受结果影响，难以重构事件前感知来研究相关因素，需要寻找替代专家政治调查的方法。

Method: 以DOGE裁员为案例研究，使用成对比较提示与LLMs得出联邦行政机构的意识形态分数。

Result: LLMs得出的分数能复制裁员前专家测量结果，预测哪些机构被DOGE针对，且发现将某些联邦机构视为知识机构的认知能预测目标机构，控制意识形态因素后仍成立。

Conclusion: 提出一个两部分标准，说明研究者何时可以用LLMs替代专家政治调查。

Abstract: After a disruptive event or shock, such as the Department of Government
Efficiency (DOGE) federal layoffs of 2025, expert judgments are colored by
knowledge of the outcome. This can make it difficult or impossible to
reconstruct the pre-event perceptions needed to study the factors associated
with the event. This position paper argues that large language models (LLMs),
trained on vast amounts of digital media data, can be a viable substitute for
expert political surveys when a shock disrupts traditional measurement. We
analyze the DOGE layoffs as a specific case study for this position. We use
pairwise comparison prompts with LLMs and derive ideology scores for federal
executive agencies. These scores replicate pre-layoff expert measures and
predict which agencies were targeted by DOGE. We also use this same approach
and find that the perceptions of certain federal agencies as knowledge
institutions predict which agencies were targeted by DOGE, even when
controlling for ideology. This case study demonstrates that using LLMs allows
us to rapidly and easily test the associated factors hypothesized behind the
shock. More broadly, our case study of this recent event exemplifies how LLMs
offer insights into the correlational factors of the shock when traditional
measurement techniques fail. We conclude by proposing a two-part criterion for
when researchers can turn to LLMs as a substitute for expert political surveys.

</details>


### [498] [Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce](https://arxiv.org/abs/2506.06576)
*Yijia Shao,Humishka Zope,Yucheng Jiang,Jiaxin Pei,David Nguyen,Erik Brynjolfsson,Diyi Yang*

Main category: cs.CY

TL;DR: 本文提出审计框架评估工人对AI代理自动化或增强任务的期望，构建WORKBank数据库分析，揭示不同职业HAS概况，强调AI发展应契合人类需求。


<details>
  <summary>Details</summary>
Motivation: 复合AI系统兴起重塑劳动力市场，但缺乏对其影响的系统理解，需评估工人期望与技术能力的匹配情况。

Method: 引入审计框架，通过音频增强迷你访谈和人类能动性量表（HAS），构建WORKBank数据库，分析1500名工人和AI专家对844项任务、104种职业的评估。

Result: 将任务分为四个区域，揭示不同职业HAS概况，显示AI集成可能使核心人类能力从信息技能转向人际技能。

Conclusion: 强调AI代理开发应与人类需求对齐，帮助工人适应工作场所动态变化。

Abstract: The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the
labor market, raising concerns about job displacement, diminished human agency,
and overreliance on automation. Yet, we lack a systematic understanding of the
evolving landscape. In this paper, we address this gap by introducing a novel
auditing framework to assess which occupational tasks workers want AI agents to
automate or augment, and how those desires align with the current technological
capabilities. Our framework features an audio-enhanced mini-interview to
capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a
shared language to quantify the preferred level of human involvement. Using
this framework, we construct the WORKBank database, building on the U.S.
Department of Labor's O*NET database, to capture preferences from 1,500 domain
workers and capability assessments from AI experts across over 844 tasks
spanning 104 occupations. Jointly considering the desire and technological
capability divides tasks in WORKBank into four zones: Automation "Green Light"
Zone, Automation "Red Light" Zone, R&D Opportunity Zone, Low Priority Zone.
This highlights critical mismatches and opportunities for AI agent development.
Moving beyond a simple automate-or-not dichotomy, our results reveal diverse
HAS profiles across occupations, reflecting heterogeneous expectations for
human involvement. Moreover, our study offers early signals of how AI agent
integration may reshape the core human competencies, shifting from
information-focused skills to interpersonal ones. These findings underscore the
importance of aligning AI agent development with human desires and preparing
workers for evolving workplace dynamics.

</details>


### [499] [Position: Simulating Society Requires Simulating Thought](https://arxiv.org/abs/2506.06958)
*Chance Jiajie Li,Jiayi Wu,Zhenze Mo,Ao Qu,Yuhan Tang,Kaiya Ivy Zhao,Yulu Gan,Jie Fan,Jiangbo Yu,Jinhua Zhao,Paul Liang,Luis Alonso,Kent Larson*

Main category: cs.CY

TL;DR: 模拟社会需认知层面结构化推理，提出GenMinds范式和RECAP框架，推动从表面模仿到模拟思维的转变。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代理缺乏内部一致性、因果推理和信念可追溯性，用于分析人类推理等行为不可靠，需要改进。

Method: 提出概念建模范式GenMinds以支持生成式代理的结构化信念表示，引入RECAP框架评估此类代理。

Result: 提出GenMinds范式和RECAP框架，推动社会模拟中生成式代理从表面模仿到模拟思维的转变。

Conclusion: 通过提出GenMinds和RECAP，实现从表面模仿到能模拟思维的生成式代理的转变，用于社会模拟。

Abstract: Simulating society with large language models (LLMs), we argue, requires more
than generating plausible behavior -- it demands cognitively grounded reasoning
that is structured, revisable, and traceable. LLM-based agents are increasingly
used to emulate individual and group behavior -- primarily through prompting
and supervised fine-tuning. Yet they often lack internal coherence, causal
reasoning, and belief traceability -- making them unreliable for analyzing how
people reason, deliberate, or respond to interventions.
  To address this, we present a conceptual modeling paradigm, Generative Minds
(GenMinds), which draws from cognitive science to support structured belief
representations in generative agents. To evaluate such agents, we introduce the
RECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess
reasoning fidelity via causal traceability, demographic grounding, and
intervention consistency. These contributions advance a broader shift: from
surface-level mimicry to generative agents that simulate thought -- not just
language -- for social simulations.

</details>


### [500] [Deepfake Technology Unveiled: The Commoditization of AI and Its Impact on Digital Trust](https://arxiv.org/abs/2506.07363)
*Claudiu Popa,Rex Pallath,Liam Cunningham,Hewad Tahiri,Abiram Kesavarajah,Tao Wu*

Main category: cs.CY

TL;DR: 探讨生成式AI使深伪技术易用，分析其风险及应对需求。


<details>
  <summary>Details</summary>
Motivation: 生成式AI发展使深伪技术易用，引发信任、隐私和安全担忧。

Method: 使用Runway、Rope和ElevenLabs等工具创建深伪内容，分析技术和伦理挑战。

Result: 展示了用有限资源可创建逼真深伪内容，对个人和组织构成风险。

Conclusion: 强调需监管框架、公众意识和协作来维护数字媒体信任。

Abstract: Deepfake Technology Unveiled: The Commoditization of AI and Its Impact on
Digital Trust. With the increasing accessibility of generative AI, tools for
voice cloning, face-swapping, and synthetic media creation have advanced
significantly, lowering both financial and technical barriers for their use.
While these technologies present innovative opportunities, their rapid growth
raises concerns about trust, privacy, and security. This white paper explores
the implications of deepfake technology, analyzing its role in enabling fraud,
misinformation, and the erosion of authenticity in multimedia. Using
cost-effective, easy to use tools such as Runway, Rope, and ElevenLabs, we
explore how realistic deepfakes can be created with limited resources,
demonstrating the risks posed to individuals and organizations alike. By
analyzing the technical and ethical challenges of deepfake mitigation and
detection, we emphasize the urgent need for regulatory frameworks, public
awareness, and collaborative efforts to maintain trust in digital media.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [501] [Discrete and Continuous Difference of Submodular Minimization](https://arxiv.org/abs/2506.07952)
*George Orfanides,Tim Hoheisel,Marwa El Halabi*

Main category: math.OC

TL;DR: 研究连续和离散域上两个子模函数之差（DS）的最小化问题，提出新的DC算法变体并应用于DC程序，实验表明该方法在整数压缩感知和整数最小二乘中优于基线。


<details>
  <summary>Details</summary>
Motivation: 拓展先前仅限于集合函数的研究，研究连续和离散域上DS函数的最小化问题。

Method: 提出一种新的DC算法变体并应用于得到的DC程序，对于连续域通过离散化应用该算法。

Result: 实验表明该方法在整数压缩感知和整数最小二乘中优于基线。

Conclusion: 新的算法能有效解决连续和离散域上DS函数的最小化问题，且性能表现良好。

Abstract: Submodular functions, defined on continuous or discrete domains, arise in
numerous applications. We study the minimization of the difference of two
submodular (DS) functions, over both domains, extending prior work restricted
to set functions. We show that all functions on discrete domains and all smooth
functions on continuous domains are DS. For discrete domains, we observe that
DS minimization is equivalent to minimizing the difference of two convex (DC)
functions, as in the set function case. We propose a novel variant of the DC
Algorithm (DCA) and apply it to the resulting DC Program, obtaining comparable
theoretical guarantees as in the set function case. The algorithm can be
applied to continuous domains via discretization. Experiments demonstrate that
our method outperforms baselines in integer compressive sensing and integer
least squares.

</details>


### [502] [Decentralized Optimization on Compact Submanifolds by Quantized Riemannian Gradient Tracking](https://arxiv.org/abs/2506.07351)
*Jun Chen,Lina Liu,Tianyi Zhu,Yong Liu,Guang Dai,Yunliang Jiang,Ivor W. Tsang*

Main category: math.OC

TL;DR: 本文提出量化黎曼梯度跟踪（Q - RGT）算法解决紧致子流形上的分散优化问题，该算法能绕过准确黎曼投影算子约束，有O(1/K)收敛率，可减少通信瓶颈和计算开销。


<details>
  <summary>Details</summary>
Motivation: 分布式优化效率常受通信瓶颈阻碍，需要解决紧致子流形上的分散优化问题。

Method: 提出Q - RGT算法，让代理使用量化梯度更新局部变量，利用量化噪声绕过准确黎曼投影算子约束。

Result: 该算法在有量化情况下达到O(1/K)收敛率，与无量化方法收敛率匹配；数值实验表明其性能与非量化方法相当，且减少通信瓶颈和计算开销。

Conclusion: Q - RGT算法在解决紧致子流形上的分散优化问题上有效，能在保证性能的同时减少通信和计算负担。

Abstract: This paper considers the problem of decentralized optimization on compact
submanifolds, where a finite sum of smooth (possibly non-convex) local
functions is minimized by $n$ agents forming an undirected and connected graph.
However, the efficiency of distributed optimization is often hindered by
communication bottlenecks. To mitigate this, we propose the Quantized
Riemannian Gradient Tracking (Q-RGT) algorithm, where agents update their local
variables using quantized gradients. The introduction of quantization noise
allows our algorithm to bypass the constraints of the accurate Riemannian
projection operator (such as retraction), further improving iterative
efficiency. To the best of our knowledge, this is the first algorithm to
achieve an $\mathcal{O}(1/K)$ convergence rate in the presence of quantization,
matching the convergence rate of methods without quantization. Additionally, we
explicitly derive lower bounds on decentralized consensus associated with a
function of quantization levels. Numerical experiments demonstrate that Q-RGT
performs comparably to non-quantized methods while reducing communication
bottlenecks and computational overhead.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [503] [From Axioms to Algorithms: Mechanized Proofs of the vNM Utility Theorem](https://arxiv.org/abs/2506.07066)
*Li Jingyuan*

Main category: econ.TH

TL;DR: 本文使用Lean 4交互式定理证明器对冯·诺伊曼 - 摩根斯坦预期效用定理进行了全面形式化，有多项贡献并为相关应用提供基础。


<details>
  <summary>Details</summary>
Motivation: 为经济建模、AI对齐和管理决策系统等应用提供严格基础，弥合理论决策理论与计算实现之间的差距。

Method: 使用Lean 4交互式定理证明器，实现偏好的经典公理，进行机器验证证明。

Result: 完成定理的形式化，有多项贡献，证明与经典表述等价且在决策边界更精确。

Conclusion: 该形式化为相关应用提供了严谨基础。

Abstract: This paper presents a comprehensive formalization of the von
Neumann-Morgenstern (vNM) expected utility theorem using the Lean 4 interactive
theorem prover. We implement the classical axioms of preference-completeness,
transitivity, continuity, and independence-enabling machine-verified proofs of
both the existence and uniqueness of utility representations. Our formalization
captures the mathematical structure of preference relations over lotteries,
verifying that preferences satisfying the vNM axioms can be represented by
expected utility maximization.
  Our contributions include a granular implementation of the independence
axiom, formally verified proofs of fundamental claims about mixture lotteries,
constructive demonstrations of utility existence, and computational experiments
validating the results. We prove equivalence to classical presentations while
offering greater precision at decision boundaries.
  This formalization provides a rigorous foundation for applications in
economic modeling, AI alignment, and management decision systems, bridging the
gap between theoretical decision theory and computational implementation.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [504] [Noise Consistency Regularization for Improved Subject-Driven Image Synthesis](https://arxiv.org/abs/2506.06483)
*Yao Ni,Song Wen,Piotr Koniusz,Anoop Cherian*

Main category: cs.GR

TL;DR: 本文提出两个辅助一致性损失用于扩散微调，解决现有微调方法欠拟合和过拟合问题，实验表明该方法可保留主体身份并提高图像多样性。


<details>
  <summary>Details</summary>
Motivation: 现有稳定扩散微调方法存在欠拟合和过拟合问题，需要解决这些问题以更好地实现主体驱动图像合成。

Method: 提出先验一致性正则化损失和主体一致性正则化损失用于扩散微调。

Result: 将这些损失纳入微调不仅能保留主体身份，还能提高图像多样性，在CLIP分数、背景变化和整体视觉质量方面优于DreamBooth。

Conclusion: 提出的两个辅助一致性损失有效提升了稳定扩散微调的效果。

Abstract: Fine-tuning Stable Diffusion enables subject-driven image synthesis by
adapting the model to generate images containing specific subjects. However,
existing fine-tuning methods suffer from two key issues: underfitting, where
the model fails to reliably capture subject identity, and overfitting, where it
memorizes the subject image and reduces background diversity. To address these
challenges, we propose two auxiliary consistency losses for diffusion
fine-tuning. First, a prior consistency regularization loss ensures that the
predicted diffusion noise for prior (non-subject) images remains consistent
with that of the pretrained model, improving fidelity. Second, a subject
consistency regularization loss enhances the fine-tuned model's robustness to
multiplicative noise modulated latent code, helping to preserve subject
identity while improving diversity. Our experimental results demonstrate that
incorporating these losses into fine-tuning not only preserves subject identity
but also enhances image diversity, outperforming DreamBooth in terms of CLIP
scores, background variation, and overall visual quality.

</details>


### [505] [Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization](https://arxiv.org/abs/2506.07069)
*Zhican Wang,Guanghui He,Dantong Liu,Lingjun Gao,Shell Xu Hu,Chen Zhang,Zhuoran Song,Nicholas Lane,Wayne Luk,Hongxiang Fan*

Main category: cs.GR

TL;DR: 本文针对3D高斯溅射在资源受限设备上实时渲染的问题，提出架构 - 算法协同设计，实验显示可加速并节能。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射在资源受限设备上实时渲染因功耗和面积预算限制面临挑战，需解决传统方法效率低的问题。

Method: 提出轴导向光栅化减少MAC操作；引入神经排序方法预测混合权重，省去硬件排序器；设计可重构处理阵列支持操作；引入π轨迹瓦片调度优化高斯重用和减少内存访问开销。

Result: 设计保留渲染质量，相比边缘GPU，在真实场景中实现23.4 - 27.8倍加速和28.8 - 51.4倍节能。

Conclusion: 架构 - 算法协同设计有效解决3D高斯溅射在资源受限设备上的实时渲染问题，计划开源设计推动领域发展。

Abstract: 3D Gaussian Splatting (3DGS) has recently gained significant attention for
high-quality and efficient view synthesis, making it widely adopted in fields
such as AR/VR, robotics, and autonomous driving. Despite its impressive
algorithmic performance, real-time rendering on resource-constrained devices
remains a major challenge due to tight power and area budgets. This paper
presents an architecture-algorithm co-design to address these inefficiencies.
First, we reveal substantial redundancy caused by repeated computation of
common terms/expressions during the conventional rasterization. To resolve
this, we propose axis-oriented rasterization, which pre-computes and reuses
shared terms along both the X and Y axes through a dedicated hardware design,
effectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by
identifying the resource and performance inefficiency of the sorting process,
we introduce a novel neural sorting approach that predicts order-independent
blending weights using an efficient neural network, eliminating the need for
costly hardware sorters. A dedicated training framework is also proposed to
improve its algorithmic stability. Third, to uniformly support rasterization
and neural network inference, we design an efficient reconfigurable processing
array that maximizes hardware utilization and throughput. Furthermore, we
introduce a $\pi$-trajectory tile schedule, inspired by Morton encoding and
Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead.
Comprehensive experiments demonstrate that the proposed design preserves
rendering quality while achieving a speedup of $23.4\sim27.8\times$ and energy
savings of $28.8\sim51.4\times$ compared to edge GPUs for real-world scenes. We
plan to open-source our design to foster further development in this field.

</details>


### [506] [GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution](https://arxiv.org/abs/2506.07897)
*Shuja Khalid,Mohamed Ibrahim,Yang Liu*

Main category: cs.GR

TL;DR: 提出新方法提升3D高斯 splatting分辨率和几何保真度，有实时性优势，实验效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前3DGS方法受输入分辨率限制，无法在训练视图外推断更精细细节。

Method: 采用轻量级生成模型预测和细化3D高斯分布，用Hessian辅助采样策略识别需加密区域。

Result: 与现有方法相比，在几何精度和渲染质量上有显著提升，且能实时运行。

Conclusion: 为无分辨率限制的3D场景增强建立了新范式。

Abstract: We present a novel approach for enhancing the resolution and geometric
fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.
Current 3DGS methods are fundamentally limited by their input resolution,
producing reconstructions that cannot extrapolate finer details than are
present in the training views. Our work breaks this limitation through a
lightweight generative model that predicts and refines additional 3D Gaussians
where needed most. The key innovation is our Hessian-assisted sampling
strategy, which intelligently identifies regions that are likely to benefit
from densification, ensuring computational efficiency. Unlike computationally
intensive GANs or diffusion approaches, our method operates in real-time
(0.015s per inference on a single consumer-grade GPU), making it practical for
interactive applications. Comprehensive experiments demonstrate significant
improvements in both geometric accuracy and rendering quality compared to
state-of-the-art methods, establishing a new paradigm for resolution-free 3D
scene enhancement.

</details>


### [507] [Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor](https://arxiv.org/abs/2506.07932)
*Rishit Dagli,Yushi Guan,Sankeerth Durvasula,Mohammadreza Mofayezi,Nandita Vijaykumar*

Main category: cs.GR

TL;DR: 提出Squeeze3D框架，利用预训练3D生成模型隐式先验知识实现高压缩比3D数据压缩，实验表明在多种格式上效果好且压缩解压缩延迟小。


<details>
  <summary>Details</summary>
Motivation: 实现3D数据的高压缩比压缩。

Method: 通过可训练映射网络连接预训练编码器和生成模型的隐空间，将3D模型编码压缩为紧凑隐码，再通过映射网络转换到生成模型隐空间进行重建，在合成数据上训练，可灵活搭配现有模型和支持多种格式。

Result: 在纹理网格、点云、辐射场分别实现最高2187x、55x、619x的压缩比，视觉质量与现有方法相当，且压缩解压缩延迟小。

Conclusion: Squeeze3D能有效压缩3D数据，具有高压缩比、低延迟等优点，且灵活性高。

Abstract: We propose Squeeze3D, a novel framework that leverages implicit prior
knowledge learnt by existing pre-trained 3D generative models to compress 3D
data at extremely high compression ratios. Our approach bridges the latent
spaces between a pre-trained encoder and a pre-trained generation model through
trainable mapping networks. Any 3D model represented as a mesh, point cloud, or
a radiance field is first encoded by the pre-trained encoder and then
transformed (i.e. compressed) into a highly compact latent code. This latent
code can effectively be used as an extremely compressed representation of the
mesh or point cloud. A mapping network transforms the compressed latent code
into the latent space of a powerful generative model, which is then conditioned
to recreate the original 3D model (i.e. decompression). Squeeze3D is trained
entirely on generated synthetic data and does not require any 3D datasets. The
Squeeze3D architecture can be flexibly used with existing pre-trained 3D
encoders and existing generative models. It can flexibly support different
formats, including meshes, point clouds, and radiance fields. Our experiments
demonstrate that Squeeze3D achieves compression ratios of up to 2187x for
textured meshes, 55x for point clouds, and 619x for radiance fields while
maintaining visual quality comparable to many existing methods. Squeeze3D only
incurs a small compression and decompression latency since it does not involve
training object-specific networks to compress an object.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [508] [Diffusion of Responsibility in Collective Decision Making](https://arxiv.org/abs/2506.07935)
*Pavel Naumov,Jia Tao*

Main category: cs.MA

TL;DR: 研究集体决策机制中责任分散现象，发现二人决策避免责任分散需一人独裁，多人决策无责任分散机制是选举独裁。


<details>
  <summary>Details</summary>
Motivation: 研究集体决策机制中常见且不良的责任分散现象。

Method: 定义决策机制的双模拟，证明双模拟保留责任相关属性，并针对最小双模拟机制得出结果。

Result: 二人决策避免责任分散需一人单方面做决定；多人决策无责任分散机制是选举一人单方面做决定。

Conclusion: 在集体决策机制中，避免责任分散的机制是独裁或选举独裁。

Abstract: The term "diffusion of responsibility'' refers to situations in which
multiple agents share responsibility for an outcome, obscuring individual
accountability. This paper examines this frequently undesirable phenomenon in
the context of collective decision-making mechanisms.
  The work shows that if a decision is made by two agents, then the only way to
avoid diffusion of responsibility is for one agent to act as a "dictator'',
making the decision unilaterally. In scenarios with more than two agents, any
diffusion-free mechanism is an "elected dictatorship'' where the agents elect a
single agent to make a unilateral decision.
  The technical results are obtained by defining a bisimulation of
decision-making mechanisms, proving that bisimulation preserves
responsibility-related properties, and establishing the results for a smallest
bisimular mechanism.

</details>


### [509] [AI-Generated Compromises for Coalition Formation](https://arxiv.org/abs/2506.06837)
*Eyal Briman,Ehud Shapiro,Nimrod Talmon*

Main category: cs.MA

TL;DR: 本文针对寻找代理提案间妥协方案的问题，结合代理有限理性和不确定性建模，用NLP和大语言模型构建语义度量空间，设计算法生成妥协提案，模拟表明AI可促进大规模民主文本编辑。


<details>
  <summary>Details</summary>
Motivation: 现有联盟形成过程中有效寻找妥协提案的问题悬而未决，且传统工具在民主文本编辑领域受限。

Method: 形式化一个包含代理有限理性和不确定性的模型，利用自然语言处理技术和大语言模型构建文本语义度量空间，设计算法生成妥协点。

Result: 模拟联盟形成过程，显示AI能促进大规模民主文本编辑。

Conclusion: AI方法可以在传统工具受限的民主文本编辑领域发挥作用，解决寻找妥协提案的问题。

Abstract: The challenge of finding compromises between agent proposals is fundamental
to AI subfields such as argumentation, mediation, and negotiation. Building on
this tradition, Elkind et al. (2021) introduced a process for coalition
formation that seeks majority-supported proposals preferable to the status quo,
using a metric space where each agent has an ideal point. A crucial step in
this process involves identifying compromise proposals around which agent
coalitions can unite. How to effectively find such compromise proposals remains
an open question. We address this gap by formalizing a model that incorporates
agent bounded rationality and uncertainty, and by developing AI methods to
generate compromise proposals. We focus on the domain of collaborative document
writing, such as the democratic drafting of a community constitution. Our
approach uses natural language processing techniques and large language models
to induce a semantic metric space over text. Based on this space, we design
algorithms to suggest compromise points likely to receive broad support. To
evaluate our methods, we simulate coalition formation processes and show that
AI can facilitate large-scale democratic text editing, a domain where
traditional tools are limited.

</details>


### [510] [Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments](https://arxiv.org/abs/2506.07232)
*Xinran Li,Chenjia Bai,Zijian Li,Jiakun Zheng,Ting Xiao,Jun Zhang*

Main category: cs.MA

TL;DR: 现有基于大语言模型（LLMs）的规划算法在多智能体具身场景适应性弱，论文提出LIET范式使LLM智能体在测试前后学习和进化，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLMs的规划算法在多智能体具身场景的适应能力较弱，需要改进。

Method: 提出Learn as Individuals, Evolve as a Team (LIET)范式，个体层面学习局部效用函数，团队层面维护和更新共享合作知识列表。

Result: 在Communicative Watch - And - Help和ThreeD - World Multi - Agent Transport基准测试中，使用LLaMA和GPT - 4o实例化的LIET优于现有基线，展现出强合作规划能力。

Conclusion: LIET通过结合个体学习和团队进化，能为LLM智能体实现全面灵活的适应。

Abstract: Large language models (LLMs) possess extensive knowledge bases and strong
reasoning capabilities, making them promising tools for complex, multi-agent
planning in embodied environments. However, despite LLMs' advanced abilities
and the sophisticated modular design of agentic methods, existing LLM-based
planning algorithms remain limited by weak adaptation capabilities to
multi-agent embodied scenarios. We address this limitation by introducing a
framework that enables LLM agents to learn and evolve both before and during
test time, equipping them with environment-relevant knowledge for better
planning and enhanced communication for improved cooperation. Inspired by
centralized training with decentralized execution in multi-agent reinforcement
learning, we propose a \textit{Learn as Individuals, Evolve as a Team (LIET)}
paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents
learn a local utility function from exploratory datasets to better comprehend
the embodied environment, which is then queried during test time to support
informed decision-making. At the team level, LLM agents collaboratively and
iteratively maintain and update a shared cooperation knowledge list based on
new experiences, using it to guide more effective communication. By combining
individual learning with team evolution, LIET enables comprehensive and
flexible adaptation for LLM agents. Our experiments on Communicative
Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate
that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing
baselines and exhibits strong cooperative planning abilities.

</details>


### [511] [Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents](https://arxiv.org/abs/2506.07388)
*Yun Hua,Haosheng Chen,Shiqin Wang,Wenhao Li,Xiangfeng Wang,Jun Luo*

Main category: cs.MA

TL;DR: 本文提出Shapley - Coop工作流解决大语言模型在开放式环境中多智能体协作的协调问题，经评估该方法能增强协作和实现公平的功劳分配。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在缺乏协调规则的开放式环境中，智能体倾向于利己行动，实现协调的关键挑战在于功劳分配和定价机制设计，且在复杂人机协作中有效定价机制很关键。

Method: 提出Shapley - Coop协作工作流，将Shapley Chain - of - Thought与结构化谈判协议相结合，实现理性的任务时间定价和任务后奖励再分配。

Result: 在两个多智能体游戏和一个软件工程模拟中评估，Shapley - Coop能持续增强大语言模型智能体协作，促进公平的功劳分配。

Conclusion: Shapley - Coop的定价机制能在任务执行中准确反映个体贡献，是有效的。

Abstract: Large Language Models (LLMs) show strong collaborative performance in
multi-agent systems with predefined roles and workflows. However, in open-ended
environments lacking coordination rules, agents tend to act in self-interested
ways. The central challenge in achieving coordination lies in credit assignment
-- fairly evaluating each agent's contribution and designing pricing mechanisms
that align their heterogeneous goals. This problem is critical as LLMs
increasingly participate in complex human-AI collaborations, where fair
compensation and accountability rely on effective pricing mechanisms. Inspired
by how human societies address similar coordination challenges (e.g., through
temporary collaborations such as employment or subcontracting), we propose a
cooperative workflow, Shapley-Coop. Shapley-Coop integrates Shapley
Chain-of-Thought -- leveraging marginal contributions as a principled basis for
pricing -- with structured negotiation protocols for effective price matching,
enabling LLM agents to coordinate through rational task-time pricing and
post-task reward redistribution. This approach aligns agent incentives, fosters
cooperation, and maintains autonomy. We evaluate Shapley-Coop across two
multi-agent games and a software engineering simulation, demonstrating that it
consistently enhances LLM agent collaboration and facilitates equitable credit
assignment. These results highlight the effectiveness of Shapley-Coop's pricing
mechanisms in accurately reflecting individual contributions during task
execution.

</details>


### [512] [MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models](https://arxiv.org/abs/2506.07400)
*Philip Liu,Sparsh Bansal,Jimmy Dinh,Aditya Pawar,Ramani Satishkumar,Shail Desai,Neeraj Gupta,Xin Wang,Shu Hu*

Main category: cs.MA

TL;DR: 提出MedChat多智能体诊断框架，结合专业视觉模型与多角色特定大语言模型智能体，提升青光眼检测可靠性，减少幻觉风险，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的青光眼检测与大语言模型集成方法存在幻觉、可解释性有限和医学知识不足等问题，且单智能体难以模拟多学科医疗团队推理。

Method: 提出MedChat多智能体诊断框架和平台，结合专业视觉模型与多角色特定大语言模型智能体，并由指挥智能体协调。

Result: 该设计增强了可靠性，降低了幻觉风险，可通过临床审查和教育使用的界面进行交互式诊断报告。

Conclusion: MedChat能有效解决现有方法的局限性，提升青光眼检测和临床报告效率。

Abstract: The integration of deep learning-based glaucoma detection with large language
models (LLMs) presents an automated strategy to mitigate ophthalmologist
shortages and improve clinical reporting efficiency. However, applying general
LLMs to medical imaging remains challenging due to hallucinations, limited
interpretability, and insufficient domain-specific medical knowledge, which can
potentially reduce clinical accuracy. Although recent approaches combining
imaging models with LLM reasoning have improved reporting, they typically rely
on a single generalist agent, restricting their capacity to emulate the diverse
and complex reasoning found in multidisciplinary medical teams. To address
these limitations, we propose MedChat, a multi-agent diagnostic framework and
platform that combines specialized vision models with multiple role-specific
LLM agents, all coordinated by a director agent. This design enhances
reliability, reduces hallucination risk, and enables interactive diagnostic
reporting through an interface tailored for clinical review and educational
use. Code available at https://github.com/Purdue-M2/MedChat.

</details>


### [513] [G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems](https://arxiv.org/abs/2506.07398)
*Guibin Zhang,Muxin Fu,Guancheng Wan,Miao Yu,Kun Wang,Shuicheng Yan*

Main category: cs.MA

TL;DR: 论文指出大语言模型驱动的多智能体系统自我进化受限于内存架构，提出G - Memory系统，实验表明其能提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统内存机制过于简单，忽略智能体协作轨迹，缺乏跨试验和特定智能体定制，限制系统自我进化能力。

Method: 引入受组织记忆理论启发的G - Memory分层智能体内存系统，通过三层图层次管理交互，接收新查询时进行双向内存遍历，执行任务时更新层次结构。

Result: 在五个基准测试、三个大语言模型主干和三个流行多智能体系统框架上实验，G - Memory分别将具身行动成功率和知识问答准确率最多提高20.89%和10.12%，且无需修改原框架。

Conclusion: G - Memory能有效提升多智能体系统性能，促进智能体团队进化。

Abstract: Large language model (LLM)-powered multi-agent systems (MAS) have
demonstrated cognitive and execution capabilities that far exceed those of
single LLM agents, yet their capacity for self-evolution remains hampered by
underdeveloped memory architectures. Upon close inspection, we are alarmed to
discover that prevailing MAS memory mechanisms (1) are overly simplistic,
completely disregarding the nuanced inter-agent collaboration trajectories, and
(2) lack cross-trial and agent-specific customization, in stark contrast to the
expressive memory developed for single agents. To bridge this gap, we introduce
G-Memory, a hierarchical, agentic memory system for MAS inspired by
organizational memory theory, which manages the lengthy MAS interaction via a
three-tier graph hierarchy: insight, query, and interaction graphs. Upon
receiving a new user query, G-Memory performs bi-directional memory traversal
to retrieve both $\textit{high-level, generalizable insights}$ that enable the
system to leverage cross-trial knowledge, and $\textit{fine-grained, condensed
interaction trajectories}$ that compactly encode prior collaboration
experiences. Upon task execution, the entire hierarchy evolves by assimilating
new collaborative trajectories, nurturing the progressive evolution of agent
teams. Extensive experiments across five benchmarks, three LLM backbones, and
three popular MAS frameworks demonstrate that G-Memory improves success rates
in embodied action and accuracy in knowledge QA by up to $20.89\%$ and
$10.12\%$, respectively, without any modifications to the original frameworks.
Our codes are available at https://github.com/bingreeky/GMemory.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [514] [The State-of-the-Art in Lifelog Retrieval: A Review of Progress at the ACM Lifelog Search Challenge Workshop 2022-24](https://arxiv.org/abs/2506.06743)
*Allie Tran,Werner Bailer,Duc-Tien Dang-Nguyen,Graham Healy,Steve Hodges,Björn Þór Jónsson,Luca Rossetto,Klaus Schoeffmann,Minh-Triet Tran,Lucia Vadicamo,Cathal Gurrin*

Main category: cs.MM

TL;DR: 本文回顾2022 - 2024年ACM LSC中交互式生活日志检索进展，分析三类检索任务，指出基于嵌入的检索方法、大语言模型集成等趋势，认为嵌入驱动与大语言模型结合及改进UI设计有前景，还建议重新考虑专家赛道的多实例系统评估。


<details>
  <summary>Details</summary>
Motivation: 回顾2022 - 2024年ACM LSC中交互式生活日志检索的进展。

Method: 对2022 - 2024年ACM LSC中相关系统进行详细的对比分析。

Result: 发现基于嵌入的检索方法广泛应用、大语言模型更多集成于对话式检索、多模态和协作搜索界面持续创新等趋势，且特定检索技术和UI设计影响系统性能。

Conclusion: 嵌入驱动与大语言模型结合对生活日志检索系统有前景，改进UI设计可提升可用性和效率，建议重新考虑专家赛道的多实例系统评估。

Abstract: The ACM Lifelog Search Challenge (LSC) is a venue that welcomes and compares
systems that support the exploration of lifelog data, and in particular the
retrieval of specific information, through an interactive competition format.
This paper reviews the recent advances in interactive lifelog retrieval as
demonstrated at the ACM LSC from 2022 to 2024. Through a detailed comparative
analysis, we highlight key improvements across three main retrieval tasks:
known-item search, question answering, and ad-hoc search. Our analysis
identifies trends such as the widespread adoption of embedding-based retrieval
methods (e.g., CLIP, BLIP), increased integration of large language models
(LLMs) for conversational retrieval, and continued innovation in multimodal and
collaborative search interfaces. We further discuss how specific retrieval
techniques and user interface (UI) designs have impacted system performance,
emphasizing the importance of balancing retrieval complexity with usability.
Our findings indicate that embedding-driven approaches combined with LLMs show
promise for lifelog retrieval systems. Likewise, improving UI design can
enhance usability and efficiency. Additionally, we recommend reconsidering
multi-instance system evaluations within the expert track to better manage
variability in user familiarity and configuration effectiveness.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [515] [#P is Sandwiched by One and Two #2DNF Calls: Is Subtraction Stronger Than We Thought?](https://arxiv.org/abs/2506.06716)
*Max Bannach,Erik D. Demaine,Timothy Gomez,Markus Hecher*

Main category: cs.CC

TL;DR: 本文研究计数复杂度相关问题，表明两次调用受限#2DNF 预言机可捕获 gapP，还得到相关算法应用、稀疏化引理及 Toda 定理的改进版本。


<details>
  <summary>Details</summary>
Motivation: 探索计数复杂度类之间的关系，以及计数问题（如 #DNF）在复杂度理论中的性质和应用。

Method: 通过结构感知的归约，在对数空间下进行操作，利用对 #2DNF 预言机的调用和一些后处理（如 AC0 或 TC0 电路）。

Result: 两次调用受限 #2DNF 预言机可捕获 gapP，归约具有线性时间可计算性，能保持有趣的结构和对称性质，可将两次调用合并为一次，得到稀疏化引理和 Toda 定理的改进版本。

Conclusion: 得到计数复杂度类之间的包含关系和相关性质，为参数化问题提供 SETH 紧的下界。

Abstract: The canonical class in the realm of counting complexity is #P. It is well
known that the problem of counting the models of a propositional formula in
disjunctive normal form (#DNF) is complete for #P under Turing reductions. On
the other hand, #DNF $\in$ spanL and spanL $\not\subseteq$ #P unless NL = NP.
Hence, the class of functions logspace-reducible to #DNF is a strict subset of
#P under plausible complexity-theoretic assumptions. By contrast, we show that
two calls to a (restricted) #2DNF oracle suffice to capture gapP, namely, that
the logspace many-one closure of the subtraction between the results of two
#2DNF calls is gapP. Because #P $\not\subseteq$ gapP, #P is strictly contained
between one and two #2DNF oracle calls.
  Surprisingly, the propositional formulas needed in both calls are linear-time
computable, and the reduction preserves interesting structural as well as
symmetry properties, leading to algorithmic applications. We show that a single
subtraction suffices to compensate for the absence of negation while still
capturing gapP, i.e., our results carry over to the monotone fragments of #2SAT
and #2DNF. Since our reduction is linear-time, it preserves sparsity and, as a
consequence we obtain a sparsification lemma for both #2SAT and #2DNF. This has
only been known for kSAT with k $\geq$ 3 and respective counting versions. We
further show that both #2DNF calls can be combined into a single call if we
allow a little postprocessing (computable by AC0- or TC0-circuits).
Consequently, we derive refined versions of Toda's Theorem: PH $\subseteq$
[#MON2SAT]$^{log}_{TC0}$ = [#MON2DNF]$^{log}_{TC0}$ and PH $\subseteq$
[#IMPL2SAT]$^{log}_{AC0}$. Our route to these results is via structure-aware
reductions that preserve parameters like treewidth up to an additive overhead.
The absence of multiplicative overhead indeed yields parameterized SETH-tight
lower bounds.

</details>


### [516] [Robust predicate and function computation in continuous chemical reaction networks](https://arxiv.org/abs/2506.06590)
*Kim Calabrese,David Doty,Mina Latifi*

Main category: cs.CC

TL;DR: 研究化学反應網絡（CRNs）中與速率常數無關的布爾謂詞和數值函數計算，提出魯棒計算概念，證明CRNs能魯棒地判定門檻謂詞的布爾組合及計算分段仿射函數。


<details>
  <summary>Details</summary>
Motivation: 連續CRNs在穩定判定布爾謂詞上存在局限，由此提出魯棒計算的鬆弛概念。

Method: 使用標準質量作用速率模型，研究CRNs在不同速率常數下的計算結果。

Result: CRNs能魯棒地判定每個有限布爾組合的門檻謂詞，能魯棒地計算具有有理係數的分段仿射函數。

Conclusion: 在化學反應網絡中，提出的魯棒計算概念拓展了與速率無關計算的範圍。

Abstract: We initiate the study of rate-constant-independent computation of Boolean
predicates and numerical functions in the continuous model of chemical reaction
networks (CRNs), which model the amount of a chemical species as a nonnegative,
real-valued *concentration*. Real-valued numerical functions have previously
been studied, finding that exactly the continuous, piecewise rational linear
functions $f: \mathbb{R}_{> 0}^k \to \mathbb{R}_{> 0}$ can be computed
*stably*, a.k.a., *rate-independently*, meaning that the CRN gets the answer
correct no matter the rate at which reactions occur.
  We show that, contrary to functions, continuous CRNs are severely limited in
the Boolean predicates they can stably decide, reporting an answer based only
on which inputs are 0 or positive.
  This limitation motivates a slightly relaxed notion of rate-independent
computation in CRNs that we call *robust computation*. The standard mass-action
rate model is used, in which each reaction is assigned a rate equal to the
product of its reactant concentrations and its rate constant. The computation
is correct in this model if it converges to the correct output for any positive
choice of rate constants. This adversary is weaker than the stable computation
adversary, the latter being able to run reactions at non-mass-action rates.
  We show that CRNs can robustly decide every finite Boolean combination of
*threshold predicates*: those predicates defined by taking a rational weighted
sum of the inputs $\mathbf{x} \in \mathbb{R}^k_{\ge 0}$ and comparing to a
constant, answering the question ``Is $\sum_{i=1}^k w_i \cdot \mathbf{x}(i) >
h$?'', for rational weights $w_i$ and real threshold $h$. Turning to function
computation, we show that CRNs can robustly compute any piecewise affine
function with rational coefficients, where threshold predicates determine which
affine piece to evaluate for a given input.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [517] [Fast Geometric Embedding for Node Influence Maximization](https://arxiv.org/abs/2506.07435)
*Alexander Kolpakov,Igor Rivin*

Main category: cs.SI

TL;DR: 提出高效力布局算法将图嵌入低维空间，用径向距离代表中心性度量，与多种中心性度量有强相关性，可用于找高影响力节点。


<details>
  <summary>Details</summary>
Motivation: 经典中心性度量在大规模图上计算成本高。

Method: 引入高效力布局算法将图嵌入低维空间，用径向距离作为多种中心性度量的代理。

Result: 在多个图族上评估，与度、PageRank和基于路径的中心性有强相关性。

Conclusion: 所提出的嵌入方法可找到网络中的高影响力节点，为标准贪心算法提供快速可扩展的替代方案。

Abstract: Computing classical centrality measures such as betweenness and closeness is
computationally expensive on large-scale graphs. In this work, we introduce an
efficient force layout algorithm that embeds a graph into a low-dimensional
space, where the radial distance from the origin serves as a proxy for various
centrality measures. We evaluate our method on multiple graph families and
demonstrate strong correlations with degree, PageRank, and paths-based
centralities. As an application, it turns out that the proposed embedding
allows to find high-influence nodes in a network, and provides a fast and
scalable alternative to the standard greedy algorithm.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [518] [Scientific machine learning in Hydrology: a unified perspective](https://arxiv.org/abs/2506.06308)
*Adoubi Vincent De Paul Adombi*

Main category: physics.comp-ph

TL;DR: 本文为水文领域科学机器学习（SciML）的首次综述，提出统一方法框架以解决方法碎片化问题，并指出各统一家族的局限与未来机会。


<details>
  <summary>Details</summary>
Motivation: 当前水文领域SciML多种方法各自独立发展，缺乏概念协调，碎片化问题影响方法评估和识别有意义的进展，需要统一概念框架。

Method: 提出每个SciML家族的统一方法框架，将代表性贡献整合为连贯结构。

Result: 形成了统一的方法框架，使概念更清晰。

Conclusion: 指出各统一家族的局限性和未来机会，可指导水文领域的系统研究。

Abstract: Scientific machine learning (SciML) provides a structured approach to
integrating physical knowledge into data-driven modeling, offering significant
potential for advancing hydrological research. In recent years, multiple
methodological families have emerged, including physics-informed machine
learning, physics-guided machine learning, hybrid physics-machine learning, and
data-driven physics discovery. Within each of these families, a proliferation
of heterogeneous approaches has developed independently, often without
conceptual coordination. This fragmentation complicates the assessment of
methodological novelty and makes it difficult to identify where meaningful
advances can still be made in the absence of a unified conceptual framework.
This review, the first focused overview of SciML in hydrology, addresses these
limitations by proposing a unified methodological framework for each SciML
family, bringing together representative contributions into a coherent
structure that fosters conceptual clarity and supports cumulative progress in
hydrological modeling. Finally, we highlight the limitations and future
opportunities of each unified family to guide systematic research in hydrology,
where these methods remain underutilized.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [519] [Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things](https://arxiv.org/abs/2506.06396)
*Christopher D. Molek,Roberto Fronteddu,K. Brent Venable,Niranjan Suri*

Main category: cs.CL

TL;DR: 提出使用NLP处理IoBT数据的工作流，评估多个中型LLM，Llama 3.1表现最佳，两步法提升准确率，为边缘设备部署LLM奠定基础。


<details>
  <summary>Details</summary>
Motivation: 处理IoBT设备数据为可用信息，增强战场态势感知，辅助关键决策。

Method: 使用适合边缘设备的LLM和图形数据库，LLM将自然语言问题映射为Cypher查询并总结输出。

Result: Llama 3.1（80亿参数）在各指标表现最佳，两步法使准确率提升19.4%。

Conclusion: 该工作流为边缘设备部署LLM与信息数据库进行自然语言交互奠定基础。

Abstract: The expansion of the Internet of Things (IoT) in the battlefield, Internet of
Battlefield Things (IoBT), gives rise to new opportunities for enhancing
situational awareness. To increase the potential of IoBT for situational
awareness in critical decision making, the data from these devices must be
processed into consumer-ready information objects, and made available to
consumers on demand. To address this challenge we propose a workflow that makes
use of natural language processing (NLP) to query a database technology and
return a response in natural language. Our solution utilizes Large Language
Models (LLMs) that are sized for edge devices to perform NLP as well as
graphical databases which are well suited for dynamic connected networks which
are pervasive in the IoBT. Our architecture employs LLMs for both mapping
questions in natural language to Cypher database queries as well as to
summarize the database output back to the user in natural language. We evaluate
several medium sized LLMs for both of these tasks on a database representing
publicly available data from the US Army's Multipurpose Sensing Area (MSA) at
the Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion
parameters) outperforms the other models across all the considered metrics.
Most importantly, we note that, unlike current methods, our two step approach
allows the relaxation of the Exact Match (EM) requirement of the produced
Cypher queries with ground truth code and, in this way, it achieves a 19.4%
increase in accuracy. Our workflow lays the ground work for deploying LLMs on
edge devices to enable natural language interactions with databases containing
information objects for critical decision making.

</details>


### [520] [Canonical Autoregressive Generation](https://arxiv.org/abs/2506.06446)
*Ivi Chatzi,Nina Corvelo Benz,Stratis Tsirtsis,Manuel Gomez-Rodriguez*

Main category: cs.CL

TL;DR: 当前大语言模型训练用分词器确定词汇表和语言，但模型生成非规范词元序列有负面影响，本文提出规范采样方法避免此问题，且其生成序列分布更接近训练分布。


<details>
  <summary>Details</summary>
Motivation: 大语言模型不总是生成规范词元序列会带来负面影响，需要解决该问题。

Method: 先证明模型要生成规范词元序列需在自回归生成过程每一步生成（部分）规范词元序列，在此基础上引入规范采样方法。

Result: 规范采样方法能避免模型生成非规范词元序列，且其生成的词元序列分布比标准采样更接近训练时的真实分布。

Conclusion: 规范采样是一种简单有效的采样方法，可解决大语言模型生成非规范词元序列的问题。

Abstract: State of the art large language models are trained using large amounts of
tokens derived from raw text using what is called a tokenizer. Crucially, the
tokenizer determines the (token) vocabulary a model will use during inference
as well as, in principle, the (token) language. This is because, while the
token vocabulary may allow for different tokenizations of a string, the
tokenizer always maps the string to only one of these tokenizations--the
canonical tokenization. However, multiple lines of empirical evidence suggest
that large language models do not always generate canonical token sequences,
and this comes with several negative consequences. In this work, we first show
that, to generate a canonical token sequence, a model needs to generate
(partial) canonical token sequences at each step of the autoregressive
generation process underpinning its functioning. Building upon this theoretical
result, we introduce canonical sampling, a simple and efficient sampling method
that precludes a given model from generating non-canonical token sequences.
Further, we also show that, in comparison with standard sampling, the
distribution of token sequences generated using canonical sampling is provably
closer to the true distribution of token sequences used during training.

</details>


### [521] [How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG](https://arxiv.org/abs/2506.06331)
*Qiming Zeng,Xiao Yan,Hao Luo,Yuhao Lin,Yuxiang Wang,Fangcheng Fu,Bo Du,Quanqing Xu,Jiawei Jiang*

Main category: cs.CL

TL;DR: 当前GraphRAG答案评估框架有缺陷，提出无偏评估框架，发现代表性方法性能提升更温和，呼吁科学评估。


<details>
  <summary>Details</summary>
Motivation: 解决当前GraphRAG答案评估框架中存在不相关问题和评估偏差两个关键缺陷。

Method: 提出无偏评估框架，用图文本接地问题生成产生更相关问题，用无偏评估程序消除评估偏差。

Result: 应用框架评估3种代表性GraphRAG方法，发现其性能提升比之前报道的更温和。

Conclusion: 虽评估框架可能有缺陷，但需科学评估为GraphRAG研究奠定基础。

Abstract: By retrieving contexts from knowledge graphs, graph-based retrieval-augmented
generation (GraphRAG) enhances large language models (LLMs) to generate quality
answers for user questions. Many GraphRAG methods have been proposed and
reported inspiring performance in answer quality. However, we observe that the
current answer evaluation framework for GraphRAG has two critical flaws, i.e.,
unrelated questions and evaluation biases, which may lead to biased or even
wrong conclusions on performance. To tackle the two flaws, we propose an
unbiased evaluation framework that uses graph-text-grounded question generation
to produce questions that are more related to the underlying dataset and an
unbiased evaluation procedure to eliminate the biases in LLM-based answer
assessment. We apply our unbiased framework to evaluate 3 representative
GraphRAG methods and find that their performance gains are much more moderate
than reported previously. Although our evaluation framework may still have
flaws, it calls for scientific evaluations to lay solid foundations for
GraphRAG research.

</details>


### [522] [Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems](https://arxiv.org/abs/2506.06821)
*Yuhan Cao,Zian Chen,Kun Quan,Ziliang Zhang,Yu Wang,Xiaoning Dong,Yeqi Feng,Guanzhong He,Jingcheng Huang,Jianhao Li,Yixuan Tan,Jiafu Tang,Yilin Tang,Junlei Wu,Qianyu Xiao,Can Zheng,Shouchen Zhou,Yuxiang Zhu,Yiming Huang,Tian Xie,Tianxing He*

Main category: cs.CL

TL;DR: 研究大语言模型在竞赛级编程代码测试用例生成方面的能力，提出TCGBench基准，实验显示多数模型生成目标测试用例能力不足，手动整理数据集可提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在代码检查和调试方面通过测试用例生成的应用程度。

Method: 提出TCGBench基准，包含两项任务，进行实验，并手动整理高质量指令数据集。

Result: 多数大语言模型能生成有效测试用例生成器，但在生成揭示人类代码缺陷的目标测试用例上有困难，先进推理模型远不如人类表现。

Conclusion: 借助手动整理的数据集，通过提示和微调可提升大语言模型的性能。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code generation, capable of tackling complex tasks during inference. However,
the extent to which LLMs can be utilized for code checking or debugging through
test case generation remains largely unexplored. We investigate this problem
from the perspective of competition-level programming (CP) programs and propose
TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This
benchmark comprises two tasks, aimed at studying the capabilities of LLMs in
(1) generating valid test case generators for a given CP problem, and further
(2) generating targeted test case generators that expose bugs in human-written
code. Experimental results indicate that while state-of-the-art LLMs can
generate valid test case generators in most cases, most LLMs struggle to
generate targeted test cases that reveal flaws in human code effectively.
Especially, even advanced reasoning models (e.g., o3-mini) fall significantly
short of human performance in the task of generating targeted generators.
Furthermore, we construct a high-quality, manually curated dataset of
instructions for generating targeted generators. Analysis demonstrates that the
performance of LLMs can be enhanced with the aid of this dataset, by both
prompting and fine-tuning.

</details>


### [523] [Dynamic and Parametric Retrieval-Augmented Generation](https://arxiv.org/abs/2506.06704)
*Weihang Su,Qingyao Ai,Jingtao Zhan,Qian Dong,Yiqun Liu*

Main category: cs.CL

TL;DR: 介绍RAG范式及传统系统局限，聚焦Dynamic RAG和Parametric RAG并综述进展与提供研究支持


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统在复杂任务中采用静态流程和上下文知识注入效果不佳，需新方法

Method: 对Dynamic RAG和Parametric RAG两个新兴研究领域进行全面概述

Result: 给出两个领域的最新进展情况

Conclusion: 分享理论基础和实践见解，支持和启发RAG进一步研究

Abstract: Retrieval-Augmented Generation (RAG) has become a foundational paradigm for
equipping large language models (LLMs) with external knowledge, playing a
critical role in information retrieval and knowledge-intensive applications.
However, conventional RAG systems typically adopt a static
retrieve-then-generate pipeline and rely on in-context knowledge injection,
which can be suboptimal for complex tasks that require multihop reasoning,
adaptive information access, and deeper integration of external knowledge.
Motivated by these limitations, the research community has moved beyond static
retrieval and in-context knowledge injection. Among the emerging directions,
this tutorial delves into two rapidly growing and complementary research areas
on RAG: Dynamic RAG and Parametric RAG. Dynamic RAG adaptively determines when
and what to retrieve during the LLM's generation process, enabling real-time
adaptation to the LLM's evolving information needs. Parametric RAG rethinks how
retrieved knowledge should be injected into LLMs, transitioning from
input-level to parameter-level knowledge injection for enhanced efficiency and
effectiveness. This tutorial offers a comprehensive overview of recent advances
in these emerging research areas. It also shares theoretical foundations and
practical insights to support and inspire further research in RAG.

</details>


### [524] [PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels](https://arxiv.org/abs/2506.07606)
*Peyman Rostami,Vahid Rahimzadeh,Ali Adibi,Azadeh Shakery*

Main category: cs.CL

TL;DR: 本文提出首个针对2024年美国大选、来自Bluesky平台的立场检测数据集PolitiSky24，介绍了创建方法和效果，称其能填补政治立场分析空白。


<details>
  <summary>Details</summary>
Motivation: 过往数据集主要关注成熟平台的推文级立场，新兴平台如Bluesky的用户级立场资源稀缺，用户级立场检测能提供更全面视角。

Method: 采用结合先进信息检索和大语言模型的精心评估流程创建数据集，生成带有支持理由和文本跨度的立场标签。

Result: 使用可扩展大语言模型的标注方法准确率达81%。

Conclusion: 该数据集在时效性、开放数据性质和用户级视角方面填补了政治立场分析的空白，数据集可在指定链接获取。

Abstract: Stance detection identifies the viewpoint expressed in text toward a specific
target, such as a political figure. While previous datasets have focused
primarily on tweet-level stances from established platforms, user-level stance
resources, especially on emerging platforms like Bluesky remain scarce.
User-level stance detection provides a more holistic view by considering a
user's complete posting history rather than isolated posts. We present the
first stance detection dataset for the 2024 U.S. presidential election,
collected from Bluesky and centered on Kamala Harris and Donald Trump. The
dataset comprises 16,044 user-target stance pairs enriched with engagement
metadata, interaction graphs, and user posting histories. PolitiSky24 was
created using a carefully evaluated pipeline combining advanced information
retrieval and large language models, which generates stance labels with
supporting rationales and text spans for transparency. The labeling approach
achieves 81\% accuracy with scalable LLMs. This resource addresses gaps in
political stance analysis through its timeliness, open-data nature, and
user-level perspective. The dataset is available at
https://doi.org/10.5281/zenodo.15616911

</details>


### [525] [Correlated Errors in Large Language Models](https://arxiv.org/abs/2506.07962)
*Elliot Kim,Avi Garg,Kenny Peng,Nikhil Garg*

Main category: cs.CL

TL;DR: 对350多个大语言模型进行评估，发现模型错误有显著相关性，且大而准的模型即便架构和供应商不同，错误也高度相关，还展示了相关性在下游任务的影响。


<details>
  <summary>Details</summary>
Motivation: 缺乏不同大语言模型是否有实质差异的实证证据，需进行研究。

Method: 使用两个流行排行榜和简历筛选任务，对超350个大语言模型进行大规模实证评估。

Result: 模型错误有显著相关性，如在一个排行榜数据集上，两模型出错时60%情况判断一致；确定了驱动模型相关性的因素；大而准的模型即便架构和供应商不同，错误也高度相关。

Conclusion: 展示了模型相关性在LLM作为评判评估和招聘两个下游任务中的影响，后者体现了算法单一文化的理论预测。

Abstract: Diversity in training data, architecture, and providers is assumed to
mitigate homogeneity in LLMs. However, we lack empirical evidence on whether
different LLMs differ meaningfully. We conduct a large-scale empirical
evaluation on over 350 LLMs overall, using two popular leaderboards and a
resume-screening task. We find substantial correlation in model errors -- on
one leaderboard dataset, models agree 60% of the time when both models err. We
identify factors driving model correlation, including shared architectures and
providers. Crucially, however, larger and more accurate models have highly
correlated errors, even with distinct architectures and providers. Finally, we
show the effects of correlation in two downstream tasks: LLM-as-judge
evaluation and hiring -- the latter reflecting theoretical predictions
regarding algorithmic monoculture.

</details>


### [526] [Low-resource Machine Translation: what for? who for? An observational study on a dedicated Tetun language translation service](https://arxiv.org/abs/2411.12262)
*Raphael Merx,Adérito José Guterres Correia,Hanna Suominen,Ekaterina Vylomova*

Main category: cs.CL

TL;DR: 本文对Tetun语言的MT服务进行观测研究，分析100,000条翻译请求，发现与现有语料库不同的使用模式，建议MT系统关注教育领域准确性。


<details>
  <summary>Details</summary>
Motivation: 低资源机器翻译社区需求和应用挑战理解不足，现有调查和焦点小组样本小。

Method: 对tetun.org的实际使用模式进行观测研究，分析100,000条翻译请求。

Result: 用户多为用移动设备的学生，常将高资源语言文本跨领域翻译成Tetun，与现有语料库以政府和社会新闻文章为主不同。

Conclusion: 针对Tetun等制度化少数民族语言的MT系统应优先考虑高到低资源方向教育领域的准确性，观测分析可为低资源语言技术发展提供依据。

Abstract: Low-resource machine translation (MT) presents a diversity of community needs
and application challenges that remain poorly understood. To complement surveys
and focus groups, which tend to rely on small samples of respondents, we
propose an observational study on actual usage patterns of tetun$.$org, a
specialized MT service for the Tetun language, which is the lingua franca in
Timor-Leste. Our analysis of 100,000 translation requests reveals patterns that
challenge assumptions based on existing corpora. We find that users, many of
them students on mobile devices, typically translate text from a high-resource
language into Tetun across diverse domains including science, healthcare, and
daily life. This contrasts sharply with available Tetun corpora, which are
dominated by news articles covering government and social issues. Our results
suggest that MT systems for institutionalized minority languages like Tetun
should prioritize accuracy on domains relevant to educational contexts, in the
high-resource to low-resource direction. More broadly, this study demonstrates
how observational analysis can inform low-resource language technology
development, by grounding research in practical community needs.

</details>


### [527] [TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment](https://arxiv.org/abs/2506.06343)
*Taesoo Kim,Jong Hwan Ko*

Main category: cs.CL

TL;DR: 提出TESU - LLM框架，仅用文本数据训练具备语音能力的语言模型，在语音相关基准测试中表现良好，无需语音数据。


<details>
  <summary>Details</summary>
Motivation: 现有构建语音语言模型的方法依赖大规模语音 - 文本配对数据和大量计算资源，存在可扩展性和可访问性挑战。

Method: 利用统一编码器将语义等效的文本和语音输入映射到共享潜在空间，通过轻量级投影网络将编码器输出与大语言模型的嵌入空间对齐。

Result: TESU - LLM在仅用文本训练的情况下，在各种语音相关基准测试中取得了与使用大规模多模态数据集和大量计算资源训练的基线方法相当的性能。

Conclusion: 该方法有效且高效，为构建无需语音数据的语音大语言模型提供了可扩展的途径。

Abstract: Recent advances in speech-enabled language models have shown promising
results in building intelligent voice assistants. However, most existing
approaches rely on large-scale paired speech-text data and extensive
computational resources, which pose challenges in terms of scalability and
accessibility. In this paper, we present \textbf{TESU-LLM}, a novel framework
that enables training speech-capable language models using only text data. Our
key insight is to leverage a unified encoder that maps semantically equivalent
text and speech inputs to a shared latent space. By aligning the encoder output
with the embedding space of a LLM via a lightweight projection network, we
enable the model to generalize from text-only supervision to speech-based
inference. Despite being trained exclusively on text, TESU-LLM achieves strong
performance on various speech-related benchmarks, comparable to baseline
methods trained with large-scale multimodal datasets and substantial
computational resources. These results highlight the effectiveness and
efficiency of our approach, offering a scalable path toward building speech
LLMs without speech data.

</details>


### [528] [Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection](https://arxiv.org/abs/2506.06347)
*Zachary Yang,Domenico Tullo,Reihaneh Rabbany*

Main category: cs.CL

TL;DR: 本文提出两个关键发现以解决游戏社区毒性检测跨游戏和语言扩展的挑战，引入软提示方法处理多游戏，开发LLM辅助标签转移框架支持多语言，评估表现良好，生产中减少资源和维护成本并成功识别违规玩家。


<details>
  <summary>Details</summary>
Motivation: 解决游戏社区毒性检测在跨多个游戏和语言扩展时面临的扩展挑战，尤其是实时环境下对计算效率的要求。

Method: 引入软提示方法，通过融入游戏上下文标记让单个模型处理多游戏；开发LLM辅助标签转移框架，使用GPT - 4o - mini扩展支持七种语言。

Result: 在法、德、葡、俄等语言的真实游戏聊天数据评估中，宏观F1分数在32.96% - 58.88%之间，德语表现尤佳；在生产中减少计算资源和维护开销，在Ubisoft平均每天每款游戏能识别50名违规玩家。

Conclusion: 该统一方法在解决游戏社区毒性检测的扩展挑战方面有效，能降低资源和维护成本，有实际应用价值。

Abstract: Toxicity detection in gaming communities faces significant scaling challenges
when expanding across multiple games and languages, particularly in real-time
environments where computational efficiency is crucial. We present two key
findings to address these challenges while building upon our previous work on
ToxBuster, a BERT-based real-time toxicity detection system. First, we
introduce a soft-prompting approach that enables a single model to effectively
handle multiple games by incorporating game-context tokens, matching the
performance of more complex methods like curriculum learning while offering
superior scalability. Second, we develop an LLM-assisted label transfer
framework using GPT-4o-mini to extend support to seven additional languages.
Evaluations on real game chat data across French, German, Portuguese, and
Russian achieve macro F1-scores ranging from 32.96% to 58.88%, with
particularly strong performance in German, surpassing the English benchmark of
45.39%. In production, this unified approach significantly reduces
computational resources and maintenance overhead compared to maintaining
separate models for each game and language combination. At Ubisoft, this model
successfully identifies an average of 50 players, per game, per day engaging in
sanctionable behavior.

</details>


### [529] [Enhancing Decision-Making of Large Language Models via Actor-Critic](https://arxiv.org/abs/2506.06376)
*Heng Dong,Kefei Duan,Chongjie Zhang*

Main category: cs.CL

TL;DR: 论文提出基于大语言模型的Actor - Critic框架LAC，在多环境实验中表现优越，展示了结合策略优化与大模型知识提升决策能力的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂决策场景有挑战，现有方法存在依赖短期自回归或模拟评估不准确等问题，需要改进。

Method: 引入基于大语言模型的Actor - Critic框架LAC，通过计算Q值提取鲁棒动作评估，用无梯度机制实现高效策略改进。

Result: 在多种环境实验中，LAC框架表现出通用性和优越性，用7B/8B参数模型在复杂任务中超越GPT - 4基线方法。

Conclusion: 将结构化策略优化与大语言模型内在知识结合，可提升多步环境决策能力。

Abstract: Large Language Models (LLMs) have achieved remarkable advancements in natural
language processing tasks, yet they encounter challenges in complex
decision-making scenarios that require long-term reasoning and alignment with
high-level objectives. Existing methods either rely on short-term
auto-regressive action generation or face limitations in accurately simulating
rollouts and assessing outcomes, leading to sub-optimal decisions. This paper
introduces a novel LLM-based Actor-Critic framework, termed LAC, that
effectively improves LLM policies with long-term action evaluations in a
principled and scalable way. Our approach addresses two key challenges: (1)
extracting robust action evaluations by computing Q-values via token logits
associated with positive/negative outcomes, enhanced by future trajectory
rollouts and reasoning; and (2) enabling efficient policy improvement through a
gradient-free mechanism. Experiments across diverse environments -- including
high-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text),
and large action spaces (WebShop) -- demonstrate the framework's generality and
superiority over state-of-the-art methods. Notably, our approach achieves
competitive performance using 7B/8B parameter LLMs, even outperforming baseline
methods employing GPT-4 in complex tasks. These results underscore the
potential of integrating structured policy optimization with LLMs' intrinsic
knowledge to advance decision-making capabilities in multi-step environments.

</details>


### [530] [Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering](https://arxiv.org/abs/2506.06384)
*Yi Ji,Runzhi Li,Baolei Mao*

Main category: cs.CL

TL;DR: 提出DMPI - PMHFE框架检测大语言模型提示注入攻击，实验显示其性能优于现有方法，部署后可降低主流大语言模型攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有提示注入攻击防御机制在有效性和通用性间存在权衡，急需适用于多种大语言模型的高效检测方法。

Method: 提出DMPI - PMHFE双通道特征融合检测框架，结合预训练语言模型和启发式特征工程，将双通道特征融合后通过全连接神经网络预测。

Result: 在多个基准数据集上，DMPI - PMHFE在准确率、召回率和F1分数上优于现有方法；部署后降低了GLM - 4等主流大语言模型的攻击成功率。

Conclusion: DMPI - PMHFE是一种有效的提示注入攻击检测方法，能应用于多种大语言模型。

Abstract: With the widespread adoption of Large Language Models (LLMs), prompt
injection attacks have emerged as a significant security threat. Existing
defense mechanisms often face critical trade-offs between effectiveness and
generalizability. This highlights the urgent need for efficient prompt
injection detection methods that are applicable across a wide range of LLMs. To
address this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion
detection framework. It integrates a pretrained language model with heuristic
feature engineering to detect prompt injection attacks. Specifically, the
framework employs DeBERTa-v3-base as a feature extractor to transform input
text into semantic vectors enriched with contextual information. In parallel,
we design heuristic rules based on known attack patterns to extract explicit
structural features commonly observed in attacks. Features from both channels
are subsequently fused and passed through a fully connected neural network to
produce the final prediction. This dual-channel approach mitigates the
limitations of relying only on DeBERTa to extract features. Experimental
results on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms
existing methods in terms of accuracy, recall, and F1-score. Furthermore, when
deployed actually, it significantly reduces attack success rates across
mainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.

</details>


### [531] [Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs](https://arxiv.org/abs/2506.06401)
*Hongming Yang,Shi Lin,Jun Shao,Changting Lin,Donghai Zhu,Meng Han,Qinglei Kong*

Main category: cs.CL

TL;DR: 介绍了轻量级大语言模型（LwLLMs）存在推理和推理能力有限等问题，提出新的直接行为优化范式DeBoP，实验表明其优于近期提示优化方法。


<details>
  <summary>Details</summary>
Motivation: 解决LwLLMs推理和推理能力有限，现有提示优化方法对其效果不佳的问题。

Method: 提出DeBoP，基于思维链提示技术，用无梯度蒙特卡罗树搜索将复杂提示优化转化为离散、可量化执行序列的优化。

Result: 在七个具有挑战性的任务上，DeBoP显著优于近期提示优化方法，DeBoP优化的LwLLMs在多数任务上超越GPT - 3.5，计算时间比其他自动提示优化方法减少约60%。

Conclusion: DeBoP是一种有效的LwLLMs提示优化方法，能提升其性能并降低计算成本。

Abstract: Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized
models designed to run efficiently on consumer-grade hardware, offering
significant advantages in resource efficiency, cost-effectiveness, and data
privacy. However, these models often struggle with limited inference and
reasoning capabilities, which restrict their performance on complex tasks and
limit their practical applicability. Moreover, existing prompt optimization
methods typically rely on extensive manual effort or the meta-cognitive
abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To
address these challenges, we introduce DeBoP, a new Direct Behavior
Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting
technique. Unlike CoT Prompting, DeBoP is an automatic optimization method,
which focuses on the optimization directly on the behavior of LwLLMs. In
particular, DeBoP transforms the optimization of complex prompts into the
optimization of discrete, quantifiable execution sequences using a
gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging
tasks where state-of-the-art LLMs excel but LwLLMs generally underperform.
Experimental results demonstrate that DeBoP significantly outperforms recent
prompt optimization methods on most tasks. In particular, DeBoP-optimized
LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by
approximately 60% compared to other automatic prompt optimization methods.

</details>


### [532] [Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights](https://arxiv.org/abs/2506.06404)
*Sooyung Choi,Jaehyeok Lee,Xiaoyuan Yi,Jing Yao,Xing Xie,JinYeong Bak*

Main category: cs.CL

TL;DR: 本文聚焦价值对齐大语言模型安全风险，发现其更易产生有害行为，揭示原因并提出增强安全的上下文对齐方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用拓展，个性化模型价值对齐存在安全隐患，需研究相关风险。

Method: 识别价值对齐大语言模型的安全风险，研究背后心理原则，使用含详细安全类别的数据集分析。

Result: 价值对齐模型比未微调模型更易有害，传统安全评估风险略高；安全问题源于按对齐价值生成文本会放大危害，发现价值对齐与安全风险显著相关。

Conclusion: 研究揭示价值对齐“黑盒”，提出上下文对齐方法增强价值对齐大语言模型安全性。

Abstract: The application scope of Large Language Models (LLMs) continues to expand,
leading to increasing interest in personalized LLMs that align with human
values. However, aligning these models with individual values raises
significant safety concerns, as certain values may correlate with harmful
information. In this paper, we identify specific safety risks associated with
value-aligned LLMs and investigate the psychological principles behind these
challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are
more prone to harmful behavior compared to non-fine-tuned models and exhibit
slightly higher risks in traditional safety evaluations than other fine-tuned
models. (2) These safety issues arise because value-aligned LLMs genuinely
generate text according to the aligned values, which can amplify harmful
outcomes. Using a dataset with detailed safety categories, we find significant
correlations between value alignment and safety risks, supported by
psychological hypotheses. This study offers insights into the "black box" of
value alignment and proposes in-context alignment methods to enhance the safety
of value-aligned LLMs.

</details>


### [533] [SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities](https://arxiv.org/abs/2506.06406)
*Guoyang Xia,Yifeng Ding,Fengfa Li,Lei Ren,Chen Wei,Fangxiang Feng,Xiaojie Wang*

Main category: cs.CL

TL;DR: 提出SMAR技术解决多模态MoE模型问题，实验表明其有效且能平衡多模态表现与语言能力


<details>
  <summary>Details</summary>
Motivation: 现有构建多模态MoE模型的方法存在训练成本高或适应预训练模型时语言能力下降的问题

Method: 提出Soft Modality - Aware Routing (SMAR)正则化技术，用Kullback Leibler散度控制跨模态路由概率分布

Result: 在视觉指令调优实验中，SMAR用仅2.5%纯文本使语言能力保留86.6%，优于基线且保持强多模态性能

Conclusion: 该方法是平衡多模态MoE模型模态分化和语言能力的实用高效解决方案

Abstract: Mixture of Experts (MoE) architectures have become a key approach for scaling
large language models, with growing interest in extending them to multimodal
tasks. Existing methods to build multimodal MoE models either incur high
training costs or suffer from degraded language capabilities when adapting
pretrained models. To address this, we propose Soft ModalityAware Routing
(SMAR), a novel regularization technique that uses Kullback Leibler divergence
to control routing probability distributions across modalities, encouraging
expert specialization without modifying model architecture or heavily relying
on textual data. Experiments on visual instruction tuning show that SMAR
preserves language ability at 86.6% retention with only 2.5% pure text,
outperforming baselines while maintaining strong multimodal performance. Our
approach offers a practical and efficient solution to balance modality
differentiation and language capabilities in multimodal MoE models.

</details>


### [534] [What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models](https://arxiv.org/abs/2506.06485)
*Kaiser Sun,Fan Bai,Mark Dredze*

Main category: cs.CL

TL;DR: 提出诊断框架评估大语言模型在上下文与记忆冲突下的行为，发现冲突对模型表现的影响及相关问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在执行任务时上下文输入和参数知识可能冲突，需评估其在冲突下的行为。

Method: 构建引发冲突的诊断数据，分析模型在多种任务类型中的表现。

Result: 知识冲突对无需知识运用的任务影响小；上下文与参数知识一致时模型表现好；模型无法完全抑制内部知识；提供解释冲突的理由会增加对上下文的依赖。

Conclusion: 对基于模型的评估有效性提出质疑，强调部署大语言模型时需考虑知识冲突。

Abstract: Large language models frequently rely on both contextual input and parametric
knowledge to perform tasks. However, these sources can come into conflict,
especially when retrieved documents contradict the model's parametric
knowledge. We propose a diagnostic framework to systematically evaluate LLM
behavior under context-memory conflict, where the contextual information
diverges from their parametric beliefs. We construct diagnostic data that
elicit these conflicts and analyze model performance across multiple task
types. Our findings reveal that (1) knowledge conflict has minimal impact on
tasks that do not require knowledge utilization, (2) model performance is
consistently higher when contextual and parametric knowledge are aligned, (3)
models are unable to fully suppress their internal knowledge even when
instructed, and (4) providing rationales that explain the conflict increases
reliance on contexts. These insights raise concerns about the validity of
model-based evaluation and underscore the need to account for knowledge
conflict in the deployment of LLMs.

</details>


### [535] [Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance](https://arxiv.org/abs/2506.06522)
*Aladin Djuhera,Swanand Ravindra Kadhe,Syed Zawad,Farhan Ahmed,Heiko Ludwig,Holger Boche*

Main category: cs.CL

TL;DR: 文章对Tulu - 3 - SFT - Mix和SmolTalk两个开源后训练数据集进行全面分析，设计新数据混合TuluTalk，提升模型性能并公开数据。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型后训练数据集缺乏透明度，且缺乏系统对比，不清楚特定样本、任务类型和策策略对下游性能的影响。

Method: 使用Magpie框架对两个数据集样本进行详细质量指标标注，分析两者结构和质量异同。

Result: 设计出的新数据混合TuluTalk样本比源数据集少14%，在关键基准测试中表现相当或更优。

Conclusion: 研究为构建更有效后训练数据集提供了可行见解，公开数据支持未来研究。

Abstract: Recent work on large language models (LLMs) has increasingly focused on
post-training and alignment with datasets curated to enhance instruction
following, world knowledge, and specialized skills. However, most post-training
datasets used in leading open- and closed-source LLMs remain inaccessible to
the public, with limited information about their construction process. This
lack of transparency has motivated the recent development of open-source
post-training corpora. While training on these open alternatives can yield
performance comparable to that of leading models, systematic comparisons remain
challenging due to the significant computational cost of conducting them
rigorously at scale, and are therefore largely absent. As a result, it remains
unclear how specific samples, task types, or curation strategies influence
downstream performance when assessing data quality. In this work, we conduct
the first comprehensive side-by-side analysis of two prominent open
post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie
framework, we annotate each sample with detailed quality metrics, including
turn structure (single-turn vs. multi-turn), task category, input quality, and
response quality, and we derive statistics that reveal structural and
qualitative similarities and differences between the two datasets. Based on
these insights, we design a principled curation recipe that produces a new data
mixture, TuluTalk, which contains 14% fewer samples than either source dataset
while matching or exceeding their performance on key benchmarks. Our findings
offer actionable insights for constructing more effective post-training
datasets that improve model performance within practical resource limits. To
support future research, we publicly release both the annotated source datasets
and our curated TuluTalk mixture.

</details>


### [536] [Beyond Facts: Evaluating Intent Hallucination in Large Language Models](https://arxiv.org/abs/2506.06539)
*Yijie Hao,Haofei Yu,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出意图幻觉概念，引入FAITHQA基准评估，发现意图幻觉常见且源于遗漏或误解，还引入自动评估指标CONSTRAINT SCORE，其接近人类评估表现。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在复杂查询中部分满足查询、忽略某些条件的问题，系统评估意图幻觉。

Method: 引入FAITHQA基准，包含20068个问题，覆盖不同场景；在FAITHQA上评估各种大语言模型；引入自动评估指标CONSTRAINT SCORE。

Result: 意图幻觉是常见问题，源于大语言模型的遗漏或误解；CONSTRAINT SCORE比基线评估指标更接近人类对意图幻觉的评估表现。

Conclusion: FAITHQA可用于系统评估意图幻觉，CONSTRAINT SCORE有助于检测意图幻觉，可推动相关研究。

Abstract: When exposed to complex queries containing multiple conditions, today's large
language models (LLMs) tend to produce responses that only partially satisfy
the query while neglecting certain conditions. We therefore introduce the
concept of Intent Hallucination. In this phenomenon, LLMs either omit
(neglecting to address certain parts) or misinterpret (responding to invented
query parts) elements of the given query, leading to intent hallucinated
generation. To systematically evaluate intent hallucination, we introduce
FAITHQA, a novel benchmark for intent hallucination that contains 20,068
problems, covering both query-only and retrieval-augmented generation (RAG)
setups with varying topics and difficulty. FAITHQA is the first hallucination
benchmark that goes beyond factual verification, tailored to identify the
fundamental cause of intent hallucination. By evaluating various LLMs on
FAITHQA, we find that (1) intent hallucination is a common issue even for
state-of-the-art models, and (2) the phenomenon stems from omission or
misinterpretation of LLMs. To facilitate future research, we introduce an
automatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting
intent hallucination. Human evaluation results demonstrate that CONSTRAINT
SCORE is closer to human performance for intent hallucination compared to
baselines.

</details>


### [537] [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
*Ho Yin 'Sam' Ng,Ting-Yao Hsu,Aashish Anantha Ramakrishnan,Branislav Kveton,Nedim Lipka,Franck Dernoncourt,Dongwon Lee,Tong Yu,Sungchul Kim,Ryan A. Rossi,Ting-Hao 'Kenneth' Huang*

Main category: cs.CL

TL;DR: 本文介绍了用于个性化图注生成的数据集LaMP - Cap，实验表明使用多模态图档信息有助于生成更接近原作者撰写的图注，且图档中的图像比提及图的段落更有用。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成的图注需作者修订以匹配写作风格和领域风格，而现有语言模型个性化技术多聚焦文本场景，少处理多模态输入和图档的场景，因此需要个性化图注生成方法。

Method: 引入具有多模态图档的个性化图注生成数据集LaMP - Cap，对每个目标图提供必要输入和同文档的其他图作为图档，用四个大语言模型进行实验及消融研究。

Result: 使用图档信息能使生成的图注更接近原作者撰写的，图档中的图像比提及图的段落更有帮助。

Conclusion: 多模态图档在个性化图注生成中优于纯文本图档。

Abstract: Figure captions are crucial for helping readers understand and remember a
figure's key message. Many models have been developed to generate these
captions, helping authors compose better quality captions more easily. Yet,
authors almost always need to revise generic AI-generated captions to match
their writing style and the domain's style, highlighting the need for
personalization. Despite language models' personalization (LaMP) advances,
these technologies often focus on text-only settings and rarely address
scenarios where both inputs and profiles are multimodal. This paper introduces
LaMP-Cap, a dataset for personalized figure caption generation with multimodal
figure profiles. For each target figure, LaMP-Cap provides not only the needed
inputs, such as figure images, but also up to three other figures from the same
document--each with its image, caption, and figure-mentioning paragraphs--as a
profile to characterize the context. Experiments with four LLMs show that using
profile information consistently helps generate captions closer to the original
author-written ones. Ablation studies reveal that images in the profile are
more helpful than figure-mentioning paragraphs, highlighting the advantage of
using multimodal profiles over text-only ones.

</details>


### [538] [MedCite: Can Language Models Generate Verifiable Text for Medicine?](https://arxiv.org/abs/2506.06605)
*Xiao Wang,Mengjue Tan,Qiao Jin,Guangzhi Xiong,Yu Hu,Aidong Zhang,Zhiyong Lu,Minjia Zhang*

Main category: cs.CL

TL;DR: 本文介绍首个端到端框架，用于医疗任务中基于大语言模型的引文生成设计与评估，提出新方法提升引文质量，评估显示该方法优于基线方法，评估结果与专家标注结果相关性好。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的医疗问答系统缺乏引文生成和评估能力，影响实际应用。

Method: 引入首个端到端框架，并提出多轮检索 - 引文生成方法。

Result: 所提方法在引文精度和召回率上优于基线方法，评估结果与专家标注结果相关性好。

Conclusion: 研究指出了医疗任务中引文生成的挑战与机遇，确定了对最终引文质量有重大影响的设计选择。

Abstract: Existing LLM-based medical question-answering systems lack citation
generation and evaluation capabilities, raising concerns about their adoption
in practice. In this work, we introduce \name, the first end-to-end framework
that facilitates the design and evaluation of citation generation with LLMs for
medical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation
method that generates high-quality citations. Our evaluation highlights the
challenges and opportunities of citation generation for medical tasks, while
identifying important design choices that have a significant impact on the
final citation quality. Our proposed method achieves superior citation
precision and recall improvements compared to strong baseline methods, and we
show that evaluation results correlate well with annotation results from
professional experts.

</details>


### [539] [Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit](https://arxiv.org/abs/2506.06607)
*Charles Goddard,Fernando Fernandes Neto*

Main category: cs.CL

TL;DR: 提出一种无训练方法，通过OMP重构未见标记嵌入来移植预训练大语言模型中的分词器，在跨分词器任务上表现出色，还集成到开源工具。


<details>
  <summary>Details</summary>
Motivation: 解决预训练大语言模型中分词器差异问题，实现用新分词器直接复用预训练模型权重。

Method: 使用正交匹配追踪（OMP），分两个阶段将未登录词近似为共享标记的稀疏线性组合。

Result: 在两个跨分词器任务中，OMP在多个基准测试中零样本保留了基础模型性能，相比基线方法表现最佳。

Conclusion: 该技术能直接复用预训练模型权重，可用于多种场景，还集成到开源工具进行事后词汇对齐。

Abstract: We present a training-free method to transplant tokenizers in pretrained
large language models (LLMs) by reconstructing unseen token embeddings via
Orthogonal Matching Pursuit (OMP). Specifically, we approximate each
out-of-vocabulary token as a sparse linear combination of shared tokens, in two
phases: first, compute each new token's representation in the donor embedding
space with a small dictionary of shared anchor tokens, then transfer these same
sparse coefficients back into the base model's embedding space.
  On two challenging cross-tokenizer tasks--Llama$\to$Mistral NeMo (12B) and
Qwen$\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of
the base model's performance across multiple benchmarks, while other zero-shot
approaches degrade significantly. Compared to baselines (zero-init, mean-init,
and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves
the best overall performance, effectively bridging large tokenizer
discrepancies without gradient updates. Our analysis further identifies
mismatched numerical tokenization schemes as a critical challenge for
preserving mathematical reasoning capabilities. This technique enables direct
reuse of pretrained model weights with new tokenizers, facilitating
cross-tokenizer knowledge distillation, speculative decoding, ensembling,
merging, and domain-specific vocabulary adaptations. We integrate our method
into the open-source mergekit-tokensurgeon tool for post hoc vocabulary
realignment.

</details>


### [540] [Quantile Regression with Large Language Models for Price Prediction](https://arxiv.org/abs/2506.06657)
*Nikhita Vedula,Dushyanta Dhyani,Laleh Jalali,Boris Oreshkin,Mohsen Bayati,Shervin Malmasi*

Main category: cs.CL

TL;DR: 研究用大语言模型进行概率回归，提出分位数回归方法，实验表明微调的Mistral - 7B模型表现出色，还揭示多种方法优劣及标签校正有效性，公开数据集。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在结构化预测任务中主要关注点估计，缺乏不同方法的系统比较，需解决文本到分布预测任务。

Method: 提出新颖的分位数回归方法，使大语言模型能生成完整预测分布。

Result: 微调的Mistral - 7B模型在点估计和分布估计上显著优于传统方法，系统比较显示其优于多种其他方法，且大语言模型辅助的标签校正有效。

Conclusion: 大语言模型在概率回归任务中具有优势，提出的方法和微调的Mistral - 7B模型表现良好，公开数据集利于后续研究。

Abstract: Large Language Models (LLMs) have shown promise in structured prediction
tasks, including regression, but existing approaches primarily focus on point
estimates and lack systematic comparison across different methods. We
investigate probabilistic regression using LLMs for unstructured inputs,
addressing challenging text-to-distribution prediction tasks such as price
estimation where both nuanced text understanding and uncertainty quantification
are critical. We propose a novel quantile regression approach that enables LLMs
to produce full predictive distributions, improving upon traditional point
estimates. Through extensive experiments across three diverse price prediction
datasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads
significantly outperforms traditional approaches for both point and
distributional estimations, as measured by three established metrics each for
prediction accuracy and distributional calibration. Our systematic comparison
of LLM approaches, model architectures, training approaches, and data scaling
reveals that Mistral-7B consistently outperforms encoder architectures,
embedding-based methods, and few-shot learning methods. Our experiments also
reveal the effectiveness of LLM-assisted label correction in achieving
human-level accuracy without systematic bias. Our curated datasets are made
available at https://github.com/vnik18/llm-price-quantile-reg/ to support
future research.

</details>


### [541] [DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains](https://arxiv.org/abs/2506.06705)
*Zhihui Chen,Kai He,Yucheng Huang,Yunxiao Zhu,Mengling Feng*

Main category: cs.CL

TL;DR: 当前零样本检测器在检测专业领域大语言模型生成文本时因领域偏移失效，本文提出DivScore框架及特定领域基准，实验显示其性能优于现有检测器。


<details>
  <summary>Details</summary>
Motivation: 检测医学和法律等专业领域大语言模型生成文本对打击虚假信息和确保真实性至关重要，但现有零样本检测器在专业内容检测中因领域偏移失效。

Method: 提出DivScore零样本检测框架，使用基于归一化熵的评分和领域知识蒸馏；发布医学和法律领域特定基准。

Result: 在基准实验中，DivScore的AUROC比现有检测器高14.4%，召回率高64.0%（0.1%误报率阈值）；在对抗设置中，DivScore的AUROC平均优势为22.8%，召回率为29.5%。

Conclusion: DivScore能有效识别专业领域大语言模型生成的文本，性能和鲁棒性优于现有检测器，代码和数据已公开。

Abstract: Detecting LLM-generated text in specialized and high-stakes domains like
medicine and law is crucial for combating misinformation and ensuring
authenticity. However, current zero-shot detectors, while effective on general
text, often fail when applied to specialized content due to domain shift. We
provide a theoretical analysis showing this failure is fundamentally linked to
the KL divergence between human, detector, and source text distributions. To
address this, we propose DivScore, a zero-shot detection framework using
normalized entropy-based scoring and domain knowledge distillation to robustly
identify LLM-generated text in specialized domains. We also release a
domain-specific benchmark for LLM-generated text detection in the medical and
legal domains. Experiments on our benchmark show that DivScore consistently
outperforms state-of-the-art detectors, with 14.4% higher AUROC and 64.0%
higher recall (0.1% false positive rate threshold). In adversarial settings,
DivScore demonstrates superior robustness than other baselines, achieving on
average 22.8% advantage in AUROC and 29.5% in recall. Code and data are
publicly available.

</details>


### [542] [C-PATH: Conversational Patient Assistance and Triage in Healthcare System](https://arxiv.org/abs/2506.06737)
*Qi Shi,Qiwei Han,Cláudia Soares*

Main category: cs.CL

TL;DR: 本文介绍了对话式AI系统C - PATH，它能帮助患者识别症状并推荐科室，在评估中表现出色，推动数字健康辅助分诊发展。


<details>
  <summary>Details</summary>
Motivation: 解决患者在复杂医疗系统中寻求及时合适医疗服务的障碍问题。

Method: 基于LLaMA3架构使用多阶段管道进行微调，利用基于GPT的数据增强框架转换临床知识，实施可扩展的对话历史管理策略。

Result: GPTScore评估显示各维度表现良好，在GPT改写的对话数据集上表现优于特定领域基线。

Conclusion: C - PATH是数字健康辅助和分诊以用户为中心、可访问且准确的AI工具发展的进步。

Abstract: Navigating healthcare systems can be complex and overwhelming, creating
barriers for patients seeking timely and appropriate medical attention. In this
paper, we introduce C-PATH (Conversational Patient Assistance and Triage in
Healthcare), a novel conversational AI system powered by large language models
(LLMs) designed to assist patients in recognizing symptoms and recommending
appropriate medical departments through natural, multi-turn dialogues. C-PATH
is fine-tuned on medical knowledge, dialogue data, and clinical summaries using
a multi-stage pipeline built on the LLaMA3 architecture. A core contribution of
this work is a GPT-based data augmentation framework that transforms structured
clinical knowledge from DDXPlus into lay-person-friendly conversations,
allowing alignment with patient communication norms. We also implement a
scalable conversation history management strategy to ensure long-range
coherence. Evaluation with GPTScore demonstrates strong performance across
dimensions such as clarity, informativeness, and recommendation accuracy.
Quantitative benchmarks show that C-PATH achieves superior performance in
GPT-rewritten conversational datasets, significantly outperforming
domain-specific baselines. C-PATH represents a step forward in the development
of user-centric, accessible, and accurate AI tools for digital health
assistance and triage.

</details>


### [543] [Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification](https://arxiv.org/abs/2506.06806)
*Subhendu Khatuya,Shashwat Naidu,Saptarshi Ghosh,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: 提出用于多标签文本分类的领域无关生成模型框架LAGAMC，在各数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 文本数据爆炸使手动文档分类挑战增大，需高效多标签文本分类方法。

Method: 利用预定义标签描述，基于输入文本生成描述，用微调的句子转换器匹配，结合交叉熵损失和余弦相似度的双目标损失函数。

Result: 在所有评估数据集上取得新的SOTA性能，相比最接近基线，Micro - F1提高13.94%，Macro - F1提高24.85%。

Conclusion: 提出的LAGAMC模型参数高效、通用性强，适合实际应用。

Abstract: The explosion of textual data has made manual document classification
increasingly challenging. To address this, we introduce a robust, efficient
domain-agnostic generative model framework for multi-label text classification.
Instead of treating labels as mere atomic symbols, our approach utilizes
predefined label descriptions and is trained to generate these descriptions
based on the input text. During inference, the generated descriptions are
matched to the pre-defined labels using a finetuned sentence transformer. We
integrate this with a dual-objective loss function, combining cross-entropy
loss and cosine similarity of the generated sentences with the predefined
target descriptions, ensuring both semantic alignment and accuracy. Our
proposed model LAGAMC stands out for its parameter efficiency and versatility
across diverse datasets, making it well-suited for practical applications. We
demonstrate the effectiveness of our proposed model by achieving new
state-of-the-art performances across all evaluated datasets, surpassing several
strong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in
Macro-F1 compared to the closest baseline across all datasets.

</details>


### [544] [Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events](https://arxiv.org/abs/2506.06808)
*James A. Michaelov,Reeka Estacio,Zhien Zhang,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 研究表明语言模型预测可能事件比不可能事件更可能的能力并不稳健，特定条件下表现比随机猜测还差。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否能可靠预测可能事件比不可能事件更可能。

Method: 将可能性、典型性和上下文相关性进行拆分分析。

Result: 所有测试的模型（包括Llama 3、Gemma 2和Mistral NeMo）在特定条件下表现比随机猜测还差，会给不可能的句子分配更高概率。

Conclusion: 语言模型在这方面的能力远不稳健。

Abstract: Can language models reliably predict that possible events are more likely
than merely improbable ones? By teasing apart possibility, typicality, and
contextual relatedness, we show that despite the results of previous work,
language models' ability to do this is far from robust. In fact, under certain
conditions, all models tested - including Llama 3, Gemma 2, and Mistral NeMo -
perform at worse-than-chance level, assigning higher probabilities to
impossible sentences such as 'the car was given a parking ticket by the brake'
than to merely unlikely sentences such as 'the car was given a parking ticket
by the explorer'.

</details>


### [545] [PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation](https://arxiv.org/abs/2506.06842)
*Arkadiusz Modzelewski,Witold Sosnowski,Tiziano Labruna,Adam Wierzbicki,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: 本文实验验证注入说服知识能否增强大语言模型检测虚假信息能力，提出PCoT方法，发布新数据集，结果显示PCoT平均优于竞品方法15%，凸显说服知识价值。


<details>
  <summary>Details</summary>
Motivation: 受心理学研究启发，测试注入说服知识能否增强大语言模型的虚假信息检测能力。

Method: 提出Persuasion - Augmented Chain of Thought (PCoT)方法，在在线新闻和社交媒体帖子上评估，并发布EUDisinfo和MultiDis两个新数据集。

Result: PCoT在五个大语言模型和五个数据集上平均比竞争方法高出15%。

Conclusion: 说服知识对加强零样本虚假信息检测有价值。

Abstract: Disinformation detection is a key aspect of media literacy. Psychological
studies have shown that knowledge of persuasive fallacies helps individuals
detect disinformation. Inspired by these findings, we experimented with large
language models (LLMs) to test whether infusing persuasion knowledge enhances
disinformation detection. As a result, we introduce the Persuasion-Augmented
Chain of Thought (PCoT), a novel approach that leverages persuasion to improve
disinformation detection in zero-shot classification. We extensively evaluate
PCoT on online news and social media posts. Moreover, we publish two novel,
up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets
enable the evaluation of PCoT on content entirely unseen by the LLMs used in
our experiments, as the content was published after the models' knowledge
cutoffs. We show that, on average, PCoT outperforms competitive methods by 15%
across five LLMs and five datasets. These findings highlight the value of
persuasion in strengthening zero-shot disinformation detection.

</details>


### [546] [DiscoSum: Discourse-aware News Summarization](https://arxiv.org/abs/2506.06930)
*Alexander Spangher,Tenghao Huang,Jialiang Gu,Jiatong Shi,Muhao Chen*

Main category: cs.CL

TL;DR: 文章提出将话语结构融入新闻文章摘要生成的新方法，创建新数据集，开发新算法，评估显示该方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有文本摘要使用大语言模型时难以维持长期话语结构，尤其在新闻文章中，而文章组织流程对读者参与度影响大。

Method: 引入新方法将话语结构融入摘要生成过程，创建新的摘要数据集，开发新闻话语模式和DiscoSum算法，使用束搜索技术进行结构感知摘要。

Result: 人工和自动评估结果证明该方法在维持叙事保真度和满足结构要求方面有效。

Conclusion: 所提出的将话语结构融入新闻摘要生成的方法是有效的。

Abstract: Recent advances in text summarization have predominantly leveraged large
language models to generate concise summaries. However, language models often
do not maintain long-term discourse structure, especially in news articles,
where organizational flow significantly influences reader engagement. We
introduce a novel approach to integrating discourse structure into
summarization processes, focusing specifically on news articles across various
media. We present a novel summarization dataset where news articles are
summarized multiple times in different ways across different social media
platforms (e.g. LinkedIn, Facebook, etc.). We develop a novel news discourse
schema to describe summarization structures and a novel algorithm, DiscoSum,
which employs beam search technique for structure-aware summarization, enabling
the transformation of news stories to meet different stylistic and structural
demands. Both human and automatic evaluation results demonstrate the efficacy
of our approach in maintaining narrative fidelity and meeting structural
requirements.

</details>


### [547] [BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning](https://arxiv.org/abs/2506.06955)
*Ha-Thanh Nguyen,Chaoran Liu,Hirokazu Kiyomaru,Koichi Takeda,Yusuke Miyao,Maki Matsuda,Yusuke Oda,Pontus Stenetorp,Qianying Liu,Su Myat Noe,Hideyuki Tachibana,Kouta Nakayama,Sadao Kurohashi*

Main category: cs.CL

TL;DR: 提出日语三段论推理数据集BIS Reasoning 1.0评估大模型信念不一致推理能力，对模型进行基准测试，发现当前大模型处理逻辑与信念冲突输入时存在弱点。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多关注一般或信念一致推理，缺乏评估大模型信念不一致推理能力的数据集，需要一个能揭示大模型推理偏差的数据集。

Method: 构建BIS Reasoning 1.0数据集，对GPT、Claude等模型进行基准测试。

Result: 不同模型表现差异大，GPT - 4o准确率达79.54%，当前大模型处理逻辑有效但与信念冲突的输入时存在关键弱点。

Conclusion: 研究结果对大模型在法律、医疗和科学文献等高风险领域的部署有重要意义，需确保真理优先于直觉信念以保证完整性和安全性。

Abstract: We present BIS Reasoning 1.0, the first large-scale Japanese dataset of
syllogistic reasoning problems explicitly designed to evaluate
belief-inconsistent reasoning in large language models (LLMs). Unlike prior
datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned
reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent
syllogisms to uncover reasoning biases in LLMs trained on human-aligned
corpora. We benchmark state-of-the-art models - including GPT models, Claude
models, and leading Japanese LLMs - revealing significant variance in
performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies
critical weaknesses in current LLMs when handling logically valid but
belief-conflicting inputs. These findings have important implications for
deploying LLMs in high-stakes domains such as law, healthcare, and scientific
literature, where truth must override intuitive belief to ensure integrity and
safety.

</details>


### [548] [What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding](https://arxiv.org/abs/2506.06998)
*Ming Li,Zhengyuan Yang,Xiyao Wang,Dianqi Li,Kevin Lin,Tianyi Zhou,Lijuan Wang*

Main category: cs.CL

TL;DR: 论文分析推理与非推理模型的标记级不对齐现象，提出FoReaL - Decoding方法，在多个数学推理基准测试中降低计算量和推理长度，保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型推理时冗长痕迹导致推理慢和过度思考，为解决此问题，系统分析推理与非推理模型的标记级不对齐现象，以找到改进方法。

Method: 提出FoReaL - Decoding协作式快慢思考解码方法，用领先模型引导句子前几个标记，弱草稿模型完成后续标记，采用随机门在大小模型间平滑插值。

Result: 在四个流行数学推理基准测试中，FoReaL - Decoding减少30% - 50%的理论FLOPs，最多削减40%的思维链长度，同时保持86% - 100%的模型性能。

Conclusion: FoReaL - Decoding是推理中心任务中可控成本 - 质量权衡的简单即插即用方法。

Abstract: Large reasoning models (LRMs) achieve strong reasoning performance by
emitting long chains of thought. Yet, these verbose traces slow down inference
and often drift into unnecessary detail, known as the overthinking phenomenon.
To better understand LRMs' behavior, we systematically analyze the token-level
misalignment between reasoning and non-reasoning models. While it is expected
that their primary difference lies in the stylistic "thinking cues", LRMs
uniquely exhibit two pivotal, previously under-explored phenomena: a Global
Misalignment Rebound, where their divergence from non-reasoning models persists
or even grows as response length increases, and more critically, a Local
Misalignment Diminish, where the misalignment concentrates at the "thinking
cues" each sentence starts with but rapidly declines in the remaining of the
sentence. Motivated by the Local Misalignment Diminish, we propose
FoReaL-Decoding, a collaborative fast-slow thinking decoding method for
cost-quality trade-off. In FoReaL-Decoding, a Leading model leads the first few
tokens for each sentence, and then a weaker draft model completes the following
tokens to the end of each sentence. FoReaL-Decoding adopts a stochastic gate to
smoothly interpolate between the small and the large model. On four popular
math-reasoning benchmarks (AIME24, GPQA-Diamond, MATH500, AMC23),
FoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and trims CoT length by
up to 40%, while preserving 86 to 100% of model performance. These results
establish FoReaL-Decoding as a simple, plug-and-play route to controllable
cost-quality trade-offs in reasoning-centric tasks.

</details>


### [549] [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/abs/2506.07044)
*LASA Team,Weiwen Xu,Hou Pong Chan,Long Li,Mahani Aljunied,Ruifeng Yuan,Jianyu Wang,Chenghao Xiao,Guizhen Chen,Chaoqun Liu,Zhaodonghui Li,Yu Sun,Junao Shen,Chaojun Wang,Jie Tan,Deli Zhao,Tingyang Xu,Hao Zhang,Yu Rong*

Main category: cs.CL

TL;DR: 本文针对现有医学多模态大语言模型的局限性，提出数据整理流程构建数据集，引入医学专用MLLM Lingshu，探索强化学习提升推理能力，开发评估框架，实验显示Lingshu表现优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有医学多模态大语言模型在医学应用中因数据和任务差异存在局限性，如医学知识覆盖有限、易产生幻觉、缺乏复杂场景推理能力。

Method: 提出综合数据整理流程构建数据集；引入医学专用MLLM Lingshu并进行多阶段训练；探索用强化学习增强推理能力；开发统一评估框架MedEvalKit。

Result: Lingshu在多模态QA、基于文本的QA和医学报告生成三项基本医学任务上，多数情况下优于现有开源多模态模型。

Conclusion: 提出的方法有效提升了医学多模态大语言模型的性能。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities in understanding common visual elements, largely due to their
large-scale datasets and advanced training strategies. However, their
effectiveness in medical applications remains limited due to the inherent
discrepancies between data and tasks in medical scenarios and those in the
general domain. Concretely, existing medical MLLMs face the following critical
limitations: (1) limited coverage of medical knowledge beyond imaging, (2)
heightened susceptibility to hallucinations due to suboptimal data curation
processes, (3) lack of reasoning capabilities tailored for complex medical
scenarios. To address these challenges, we first propose a comprehensive data
curation procedure that (1) efficiently acquires rich medical knowledge data
not only from medical imaging but also from extensive medical texts and
general-domain data; and (2) synthesizes accurate medical captions, visual
question answering (VQA), and reasoning samples. As a result, we build a
multimodal dataset enriched with extensive medical knowledge. Building on the
curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu
undergoes multi-stage training to embed medical expertise and enhance its
task-solving capabilities progressively. Besides, we preliminarily explore the
potential of applying reinforcement learning with verifiable rewards paradigm
to enhance Lingshu's medical reasoning ability. Additionally, we develop
MedEvalKit, a unified evaluation framework that consolidates leading multimodal
and textual medical benchmarks for standardized, fair, and efficient model
assessment. We evaluate the performance of Lingshu on three fundamental medical
tasks, multimodal QA, text-based QA, and medical report generation. The results
show that Lingshu consistently outperforms the existing open-source multimodal
models on most tasks ...

</details>


### [550] [Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models](https://arxiv.org/abs/2506.07064)
*Kai Xiong,Xiao Ding,Yixin Cao,Yuxiong Yan,Li Du,Yufei Zhang,Jinglong Gao,Jiaqian Liu,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 提出复杂常识推理基准Com²，用因果事件图和因果理论构建场景，用大模型合成示例，实验表明大模型在推理上有不足，后训练和慢思考可缓解。


<details>
  <summary>Details</summary>
Motivation: 大模型在复杂隐式常识推理上有困难，现有工作对复杂常识推理探索不足，需填补该空白。

Method: 结合因果事件图作为结构化复杂常识，用因果理论修改图得到不同场景，用大模型合成示例，用侦探故事构建更具挑战性子集。

Result: 实验显示大模型在推理深度和广度上存在困难，后训练和慢思考可缓解。

Conclusion: 提出的Com²基准有助于研究大模型的复杂常识推理能力，后训练和慢思考可提升其推理表现。

Abstract: Large language models (LLMs) have mastered abundant simple and explicit
commonsense knowledge through pre-training, enabling them to achieve human-like
performance in simple commonsense reasoning. Nevertheless, LLMs struggle to
reason with complex and implicit commonsense knowledge that is derived from
simple ones (such as understanding the long-term effects of certain events), an
aspect humans tend to focus on more. Existing works focus on complex tasks like
math and code, while complex commonsense reasoning remains underexplored due to
its uncertainty and lack of structure. To fill this gap and align with
real-world concerns, we propose a benchmark Com$^2$ focusing on complex
commonsense reasoning. We first incorporate causal event graphs to serve as
structured complex commonsense. Then we adopt causal theory~(e.g.,
intervention) to modify the causal event graphs and obtain different scenarios
that meet human concerns. Finally, an LLM is employed to synthesize examples
with slow thinking, which is guided by the logical relationships in the
modified causal graphs. Furthermore, we use detective stories to construct a
more challenging subset. Experiments show that LLMs struggle in reasoning depth
and breadth, while post-training and slow thinking can alleviate this. The code
and data are available at https://github.com/Waste-Wood/Com2.

</details>


### [551] [Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/abs/2506.06395)
*Pengyi Li,Matvey Skripkin,Alexander Zubrey,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CL

TL;DR: 提出RLSC方法，用模型自信度作奖励信号，在推理模型后训练中效果好，提升推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖昂贵人工标注或外部奖励模型，需新的后训练方法。

Method: 提出Reinforcement Learning via Self-Confidence (RLSC)，用模型自身自信度作为奖励信号。

Result: 在Qwen2.5 - Math - 7B上应用，仅少量样本和训练轮次，在AIME2024、MATH500、AMC23上准确率分别提升+20.10%、+49.40%、+52.50%。

Conclusion: RLSC为推理模型提供了简单、可扩展且低监督的后训练方法。

Abstract: Large language models (LLMs) excel at reasoning, yet post-training remains
critical for aligning their behavior with task goals. Existing reinforcement
learning (RL) methods often depend on costly human annotations or external
reward models. We propose Reinforcement Learning via Self-Confidence (RLSC),
which uses the model's own confidence as reward signals-eliminating the need
for labels, preference models, or reward engineering. Applied to
Qwen2.5-Math-7B with only 8 samples per question and 4 training epochs, RLSC
improves accuracy by +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on
AMC23. RLSC offers a simple, scalable post-training method for reasoning models
with minimal supervision.

</details>


### [552] [How Far Are We from Optimal Reasoning Efficiency?](https://arxiv.org/abs/2506.07104)
*Jiaxuan Gao,Shu Yan,Qixin Tan,Lu Yang,Shusheng Xu,Wei Fu,Zhiyu Mei,Kaifeng Lyu,Yi Wu*

Main category: cs.CL

TL;DR: 本文指出大推理模型推理轨迹冗长低效问题，引入推理效率前沿和REG指标，提出REO - RL算法减少效率差距，虽有成效但微调模型与效率前沿完美对齐仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 大推理模型推理轨迹冗长低效，现有微调方法评估效率提升困难，需统一指标和有效方法解决效率问题。

Method: 引入推理效率前沿，提出REG指标量化微调模型与前沿偏差；提出REO - RL强化学习算法，通过选择稀疏令牌预算集最小化REG；采用数值积分近似全效率目标。

Result: REG指标有效捕捉准确率 - 长度权衡；REO - RL算法使所有评估的LRMs的REG降低≥50，在16K令牌预算下匹配Qwen3 - 4B/8B效率前沿且准确率损失小；消融实验证实指数令牌预算策略有效。

Conclusion: 微调大推理模型使其完美符合效率前沿仍是待解决的挑战。

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable problem-solving
capabilities through extended Chain-of-Thought (CoT) reasoning but often
produce excessively verbose and redundant reasoning traces. This inefficiency
incurs high inference costs and limits practical deployment. While existing
fine-tuning methods aim to improve reasoning efficiency, assessing their
efficiency gains remains challenging due to inconsistent evaluations. In this
work, we introduce the reasoning efficiency frontiers, empirical upper bounds
derived from fine-tuning base LRMs across diverse approaches and training
configurations. Based on these frontiers, we propose the Reasoning Efficiency
Gap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from
these frontiers. Systematic evaluation on challenging mathematical benchmarks
reveals significant gaps in current methods: they either sacrifice accuracy for
short length or still remain inefficient under tight token budgets. To reduce
the efficiency gap, we propose REO-RL, a class of Reinforcement Learning
algorithms that minimizes REG by targeting a sparse set of token budgets.
Leveraging numerical integration over strategically selected budgets, REO-RL
approximates the full efficiency objective with low error using a small set of
token budgets. Through systematic benchmarking, we demonstrate that our
efficiency metric, REG, effectively captures the accuracy-length trade-off,
with low-REG methods reducing length while maintaining accuracy. Our approach,
REO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching
Qwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy
loss. Ablation studies confirm the effectiveness of our exponential token
budget strategy. Finally, our findings highlight that fine-tuning LRMs to
perfectly align with the efficiency frontiers remains an open challenge.

</details>


### [553] [Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models](https://arxiv.org/abs/2506.07106)
*Samir Abdaljalil,Hasan Kurban,Khalid Qaraqe,Erchin Serpedin*

Main category: cs.CL

TL;DR: 提出定理思维（ToTh）框架，让大语言模型推理更可靠可解释，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理过程脆弱且难解释，现有提示技术缺乏逻辑结构和内部一致性评估机制。

Method: 引入ToTh框架，通过三个并行代理模拟不同推理模式，构建推理图，用贝叶斯信念传播评估一致性并选最优图得出答案。

Result: 在符号和数值推理基准测试中，ToTh始终优于CoT、Self - Consistency和CoT - Decoding，且产生可解释、有逻辑依据的推理链。

Conclusion: ToTh为构建更强大、受认知启发的大语言模型推理提供了有前景的方向。

Abstract: Large language models (LLMs) have shown strong performance across natural
language reasoning tasks, yet their reasoning processes remain brittle and
difficult to interpret. Prompting techniques like Chain-of-Thought (CoT)
enhance reliability by eliciting intermediate reasoning steps or aggregating
multiple outputs. However, they lack mechanisms for enforcing logical structure
and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a
novel framework that models reasoning as collaboration among three parallel
agents, each simulating a distinct mode of inference: abductive, deductive, and
inductive. Each agent produces a reasoning trace, which is structured into a
formal reasoning graph. To evaluate consistency, we apply Bayesian belief
propagation guided by natural language inference (NLI), assigning confidence
scores to each step. The most coherent graph is selected to derive the final
answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)
reasoning benchmarks show that ToTh consistently outperforms CoT,
Self-Consistency, and CoT-Decoding across multiple LLMs, while producing
interpretable and logically grounded reasoning chains. Our findings suggest a
promising direction for building more robust and cognitively inspired LLM
reasoning. The implementation is available at
https://github.com/KurbanIntelligenceLab/theorem-of-thought.

</details>


### [554] [Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting](https://arxiv.org/abs/2506.07142)
*Lennart Meincke,Ethan Mollick,Lilach Mollick,Dan Shapiro*

Main category: cs.CL

TL;DR: 报告研究思维链提示法，发现其有效性因任务和模型而异，还会增加成本和时间。


<details>
  <summary>Details</summary>
Motivation: 帮助商业、教育和政策领导者通过严格测试了解与AI合作的技术细节。

Method: 对思维链提示法进行研究。

Result: 思维链提示法有效性因任务和模型而异，会增加答案变异性、成本和时间，对有显式推理能力的模型提升不大。

Conclusion: 思维链提示法并非在所有情况下都能有效提升模型性能，使用时需考虑任务和模型类型以及成本等因素。

Abstract: This is the second in a series of short reports that seek to help business,
education, and policy leaders understand the technical details of working with
AI through rigorous testing. In this report, we investigate Chain-of-Thought
(CoT) prompting, a technique that encourages a large language model (LLM) to
"think step by step" (Wei et al., 2022). CoT is a widely adopted method for
improving reasoning tasks, however, our findings reveal a more nuanced picture
of its effectiveness. We demonstrate two things:
  - The effectiveness of Chain-of-Thought prompting can vary greatly depending
on the type of task and model. For non-reasoning models, CoT generally improves
average performance by a small amount, particularly if the model does not
inherently engage in step-by-step processing by default. However, CoT can
introduce more variability in answers, sometimes triggering occasional errors
in questions the model would otherwise get right. We also found that many
recent models perform some form of CoT reasoning even if not asked; for these
models, a request to perform CoT had little impact. Performing CoT generally
requires far more tokens (increasing cost and time) than direct answers.
  - For models designed with explicit reasoning capabilities, CoT prompting
often results in only marginal, if any, gains in answer accuracy. However, it
significantly increases the time and tokens needed to generate a response.

</details>


### [555] [Syntactic Control of Language Models by Posterior Inference](https://arxiv.org/abs/2506.07154)
*Vicky Xefteri,Tim Vieira,Ryan Cotterell,Afra Amini*

Main category: cs.CL

TL;DR: 本文提出基于后验推理的采样算法可在文本生成中有效控制句法结构，实验表明该方法能提升句法准确性且不影响流畅性。


<details>
  <summary>Details</summary>
Motivation: 控制语言模型生成文本的句法结构有价值但具挑战性，需有效方法实现。

Method: 结合顺序蒙特卡罗方法和句法标注器，通过从提议分布采样估计后验分布，确保生成的每个标记符合所需句法结构。

Result: 在GPT2和Llama3 - 8B模型实验中，使用合适的提议分布能将句法F1分数从GPT2 - large的12.31和Llama3 - 8B的35.33提升到约93，且不影响语言模型的流畅性。

Conclusion: 强调了句法控制的复杂性和采样算法的有效性，为需要精确控制句法的应用提供了有前景的方法。

Abstract: Controlling the syntactic structure of text generated by language models is
valuable for applications requiring clarity, stylistic consistency, or
interpretability, yet it remains a challenging task. In this paper, we argue
that sampling algorithms based on the posterior inference can effectively
enforce a target constituency structure during generation. Our approach
combines sequential Monte Carlo, which estimates the posterior distribution by
sampling from a proposal distribution, with a syntactic tagger that ensures
that each generated token aligns with the desired syntactic structure. Our
experiments with GPT2 and Llama3-8B models show that with an appropriate
proposal distribution, we can improve syntactic accuracy, increasing the F1
score from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both
cases without compromising the language model's fluency. These results
underscore both the complexity of syntactic control and the effectiveness of
sampling algorithms, offering a promising approach for applications where
precise control over syntax is essential.

</details>


### [556] [Transferring Features Across Language Models With Model Stitching](https://arxiv.org/abs/2506.06609)
*Alan Chen,Jack Merullo,Alessandro Stolfo,Ellie Pavlick*

Main category: cs.CL

TL;DR: 本文表明语言模型残差流间的仿射映射可有效转移特征，对比不同大小模型的稀疏自编码器（SAEs）表示，发现大小模型表示空间相似，可降低SAEs训练成本，还研究了特征转移能力。


<details>
  <summary>Details</summary>
Motivation: 寻找有效在不同模型间转移特征的方法，降低训练如SAEs等昂贵组件的成本。

Method: 利用语言模型残差流间的仿射映射技术转移SAEs权重，对比不同大小模型的表示；测试转移的探针和转向向量性能；研究特征级别的可转移性。

Result: 大小模型学习到高度相似的表示空间；使用小模型转移的SAEs初始化大模型训练，成本可降低50%；转移的探针和转向向量能有效恢复真实性能；语义和结构特征转移表现不同，特定功能特征角色映射忠实。

Conclusion: 展示了大小模型线性表示空间的异同，证明了该方法可提高SAEs训练效率。

Abstract: In this work, we demonstrate that affine mappings between residual streams of
language models is a cheap way to effectively transfer represented features
between models. We apply this technique to transfer the weights of Sparse
Autoencoders (SAEs) between models of different sizes to compare their
representations. We find that small and large models learn highly similar
representation spaces, which motivates training expensive components like SAEs
on a smaller model and transferring to a larger model at a FLOPs savings. For
example, using a small-to-large transferred SAE as initialization can lead to
50% cheaper training runs when training SAEs on larger models. Next, we show
that transferred probes and steering vectors can effectively recover ground
truth performance. Finally, we dive deeper into feature-level transferability,
finding that semantic and structural features transfer noticeably differently
while specific classes of functional features have their roles faithfully
mapped. Overall, our findings illustrate similarities and differences in the
linear representation spaces of small and large models and demonstrate a method
for improving the training efficiency of SAEs.

</details>


### [557] [CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Dissertações e Trabalhos de Graduação em SI -- XXI Simpósio Brasileiro de Sistemas de Informação](https://arxiv.org/abs/2506.07169)
*Washington Cunha,Leonardo Rocha,Marcos André Gonçalves*

Main category: cs.CL

TL;DR: 论文聚焦实例选择（IS）技术，对比IS方法在自动文本分类任务中的应用，提出两种新IS解决方案，可减少训练集大小，提升训练速度。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型训练或微调需大量计算资源，而实例选择技术潜力巨大但研究不足。

Method: 对应用于自动文本分类任务的IS方法进行全面科学比较，提出两种面向噪声和冗余感知的新IS解决方案。

Result: 最终解决方案使训练集平均减少41%，在所有数据集上保持相同效果，速度提升1.67倍（最高2.46倍）。

Conclusion: IS解决方案有巨大未挖掘潜力，提出的新方案可扩展到包含数十万文档的数据集。

Abstract: Progress in Natural Language Processing (NLP) has been dictated by the rule
of more: more data, more computing power and more complexity, best exemplified
by the Large Language Models. However, training (or fine-tuning) large dense
models for specific applications usually requires significant amounts of
computing resources. This \textbf{Ph.D. dissertation} focuses on an
under-investi\-gated NLP data engineering technique, whose potential is
enormous in the current scenario known as Instance Selection (IS). The IS goal
is to reduce the training set size by removing noisy or redundant instances
while maintaining the effectiveness of the trained models and reducing the
training process cost. We provide a comprehensive and scientifically sound
comparison of IS methods applied to an essential NLP task -- Automatic Text
Classification (ATC), considering several classification solutions and many
datasets. Our findings reveal a significant untapped potential for IS
solutions. We also propose two novel IS solutions that are noise-oriented and
redundancy-aware, specifically designed for large datasets and transformer
architectures. Our final solution achieved an average reduction of 41\% in
training sets, while maintaining the same levels of effectiveness in all
datasets. Importantly, our solutions demonstrated speedup improvements of 1.67x
(up to 2.46x), making them scalable for datasets with hundreds of thousands of
documents.

</details>


### [558] [Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs](https://arxiv.org/abs/2506.07180)
*Wenrui Zhou,Shu Yang,Qingsong Yang,Zikun Guo,Lijie Hu,Di Wang*

Main category: cs.CL

TL;DR: 提出VISE评估视频大语言模型谄媚行为，并探索关键帧选择缓解策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视视频-语言领域谄媚现象，缺乏系统评估基准，影响模型在现实应用中的可信度。

Method: 提出VISE基准评估模型在不同问题格式、提示偏差和视觉推理任务中的谄媚行为，探索关键帧选择作为缓解策略。

Result: VISE能实现对多种谄媚类型和交互模式的细粒度分析，关键帧选择策略揭示了减少谄媚偏差的潜在途径。

Conclusion: VISE填补了视频大语言模型谄媚评估空白，关键帧选择策略有助于加强视觉基础，减少谄媚偏差。

Abstract: As video large language models (Video-LLMs) become increasingly integrated
into real-world applications that demand grounded multimodal reasoning,
ensuring their factual consistency and reliability is of critical importance.
However, sycophancy, the tendency of these models to align with user input even
when it contradicts the visual evidence, undermines their trustworthiness in
such contexts. Current sycophancy research has largely overlooked its specific
manifestations in the video-language domain, resulting in a notable absence of
systematic benchmarks and targeted evaluations to understand how Video-LLMs
respond under misleading user input. To fill this gap, we propose VISE
(Video-LLM Sycophancy Benchmarking and Evaluation), the first dedicated
benchmark designed to evaluate sycophantic behavior in state-of-the-art
Video-LLMs across diverse question formats, prompt biases, and visual reasoning
tasks. Specifically, VISE pioneeringly brings linguistic perspectives on
sycophancy into the visual domain, enabling fine-grained analysis across
multiple sycophancy types and interaction patterns. In addition, we explore
key-frame selection as an interpretable, training-free mitigation strategy,
which reveals potential paths for reducing sycophantic bias by strengthening
visual grounding.

</details>


### [559] [SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes](https://arxiv.org/abs/2506.07245)
*Wenxuan Xie,Yaxun Dai,Wenhao Jiang*

Main category: cs.CL

TL;DR: 提出SDE - SQL框架使大语言模型在推理时进行数据库自驱动探索，在BIRD基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL任务方法依赖静态数据库信息，限制模型理解数据库内容能力，缺乏动态交互。

Method: 提出SDE - SQL框架，通过生成和执行SQL探针让模型主动检索数据库信息并迭代更新对数据的理解，在零样本设置下运行。

Result: 在BIRD基准测试中，使用Qwen2.5 - 72B - Instruct时，SDE - SQL执行准确率相对基线提升8.02%，有监督微调后可再提升0.52%。

Conclusion: SDE - SQL建立了无监督微调或模型集成的开源模型方法中的新最优水平，且微调可进一步提升性能。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved performance on the Text-to-SQL task. However, prior approaches
typically rely on static, pre-processed database information provided at
inference time, which limits the model's ability to fully understand the
database contents. Without dynamic interaction, LLMs are constrained to fixed,
human-provided context and cannot autonomously explore the underlying data. To
address this limitation, we propose SDE-SQL, a framework that enables large
language models to perform self-driven exploration of databases during
inference. This is accomplished by generating and executing SQL probes, which
allow the model to actively retrieve information from the database and
iteratively update its understanding of the data. Unlike prior methods, SDE-SQL
operates in a zero-shot setting, without relying on any question-SQL pairs as
in-context demonstrations. When evaluated on the BIRD benchmark with
Qwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in
execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing
a new state-of-the-art among methods based on open-source models without
supervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the
performance of SDE-SQL can be further enhanced, yielding an additional 0.52%
improvement.

</details>


### [560] [Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages](https://arxiv.org/abs/2506.07274)
*Olga Kellert,Nemika Tyagi,Muhammad Imran,Nelvin Licona-Guevara,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: 本文介绍了基于大语言模型的BiLingua解析器，用于为代码切换文本生成通用依存标注，实验表明其性能优于先前基线和多语言解析器。


<details>
  <summary>Details</summary>
Motivation: 代码切换的句法分析在低资源语言环境下是一个挑战，现有工作缺乏对大语言模型在代码切换语境中捕捉句法结构的系统研究，单语训练的解析器难以泛化到多语言输入。

Method: 开发基于提示的框架，结合少样本大语言模型提示和专家评审；发布两个标注数据集；对跨语言对和交流语境的切换点进行详细句法分析。

Result: BiLingua解析器在专家修订后达到了95.29%的LAS，显著优于先前基线和多语言解析器。

Conclusion: 精心引导下，大语言模型可作为在资源不足的代码切换环境中引导句法资源的实用工具。

Abstract: Code-switching presents a complex challenge for syntactic analysis,
especially in low-resource language settings where annotated data is scarce.
While recent work has explored the use of large language models (LLMs) for
sequence-level tagging, few approaches systematically investigate how well
these models capture syntactic structure in code-switched contexts. Moreover,
existing parsers trained on monolingual treebanks often fail to generalize to
multilingual and mixed-language input. To address this gap, we introduce the
BiLingua Parser, an LLM-based annotation pipeline designed to produce Universal
Dependencies (UD) annotations for code-switched text. First, we develop a
prompt-based framework for Spanish-English and Spanish-Guaran\'i data,
combining few-shot LLM prompting with expert review. Second, we release two
annotated datasets, including the first Spanish-Guaran\'i UD-parsed corpus.
Third, we conduct a detailed syntactic analysis of switch points across
language pairs and communicative contexts. Experimental results show that
BiLingua Parser achieves up to 95.29% LAS after expert revision, significantly
outperforming prior baselines and multilingual parsers. These results show that
LLMs, when carefully guided, can serve as practical tools for bootstrapping
syntactic resources in under-resourced, code-switched environments. Data and
source code are available at https://github.com/N3mika/ParsingProject

</details>


### [561] [Reward Model Interpretability via Optimal and Pessimal Tokens](https://arxiv.org/abs/2506.07326)
*Brian Christian,Hannah Rose Kirk,Jessica A. F. Thompson,Christopher Summerfield,Tsvetomira Dumbalska*

Main category: cs.CL

TL;DR: 本文通过分析奖励模型在整个词汇空间的响应来研究其可解释性，发现不同模型差异大、编码不对称、对提示敏感且高估高频词，挑战了奖励模型可互换假设，指出其存在身份偏见。


<details>
  <summary>Details</summary>
Motivation: 奖励模型是使大语言模型与人类价值观对齐的关键，但自身研究相对不足，需对其进行可解释性研究。

Method: 对不同参数数量和架构的十个开源奖励模型，分析其对价值负载提示的每个可能单令牌响应的评分。

Result: 发现模型间存在实质性异质性、编码高低分令牌有系统不对称性、对提示框架敏感反映人类认知偏差、高估高频令牌，且存在对某些身份群体的偏见。

Conclusion: 挑战了奖励模型可互换假设和作为人类复杂价值观代理的适用性，其偏见可能在下游大语言模型中传播。

Abstract: Reward modeling has emerged as a crucial component in aligning large language
models with human values. Significant attention has focused on using reward
models as a means for fine-tuning generative models. However, the reward models
themselves -- which directly encode human value judgments by turning
prompt-response pairs into scalar rewards -- remain relatively understudied. We
present a novel approach to reward model interpretability through exhaustive
analysis of their responses across their entire vocabulary space. By examining
how different reward models score every possible single-token response to
value-laden prompts, we uncover several striking findings: (i) substantial
heterogeneity between models trained on similar objectives, (ii) systematic
asymmetries in how models encode high- vs low-scoring tokens, (iii) significant
sensitivity to prompt framing that mirrors human cognitive biases, and (iv)
overvaluation of more frequent tokens. We demonstrate these effects across ten
recent open-source reward models of varying parameter counts and architectures.
Our results challenge assumptions about the interchangeability of reward
models, as well as their suitability as proxies of complex and
context-dependent human values. We find that these models can encode concerning
biases toward certain identity groups, which may emerge as unintended
consequences of harmlessness training -- distortions that risk propagating
through the downstream large language models now deployed to millions.

</details>


### [562] [Improving LLM Reasoning through Interpretable Role-Playing Steering](https://arxiv.org/abs/2506.07335)
*Anyi Wang,Dong Shu,Yifan Wang,Yunpu Ma,Mengnan Du*

Main category: cs.CL

TL;DR: 本文提出SRPS框架提升大语言模型推理能力，实验证明其效果佳且有更好的解释性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示工程的角色扮演方法缺乏稳定性和可解释性，需新方法提升大语言模型推理能力。

Method: 引入SRPS框架，从角色扮演提示中提取潜在表征，根据激活模式选择相关特征，构建可控制强度的转向向量注入模型残差流。

Result: 在多种推理基准和模型规模上实验取得一致性能提升，如Llama3.1 - 8B在CSQA上准确率从31.86%提升到39.80%，Gemma2 - 9B在SVAMP上从37.50%提升到45.10%。

Conclusion: SRPS有提升大语言模型推理能力的潜力，比传统基于提示的角色扮演方法有更好的可解释性和稳定性。

Abstract: Role-playing has emerged as an effective technique for enhancing the
reasoning capabilities of large language models (LLMs). However, existing
methods primarily rely on prompt engineering, which often lacks stability and
interpretability. In this paper, we introduce Sparse Autoencoder Role-Playing
Steering (SRPS), a novel framework that identifies and manipulates internal
model features associated with role-playing behavior. Our approach extracts
latent representations from role-play prompts, selects the most relevant
features based on activation patterns, and constructs a steering vector that
can be injected into the model's residual stream with controllable intensity.
Our method enables fine-grained control over role-specific behavior and offers
insights into how role information influences internal model activations.
Extensive experiments across various reasoning benchmarks and model sizes
demonstrate consistent performance gains. Notably, in the zero-shot
chain-of-thought (CoT) setting, the accuracy of Llama3.1-8B on CSQA improves
from 31.86% to 39.80%, while Gemma2-9B on SVAMP increases from 37.50% to
45.10%. These results highlight the potential of SRPS to enhance reasoning
ability in LLMs, providing better interpretability and stability compared to
traditional prompt-based role-playing.

</details>


### [563] [Learning to Clarify by Reinforcement Learning Through Reward-Weighted Fine-Tuning](https://arxiv.org/abs/2506.06964)
*Subhojyoti Mukherjee,Viet Dac Lai,Raghavendra Addanki,Ryan Rossi,Seunghyun Yoon,Trung Bui,Anup Rao,Jayakumar Subramanian,Branislav Kveton*

Main category: cs.CL

TL;DR: 本文研究问答代理中询问澄清问题，用强化学习模拟含澄清问题的对话，提出离线强化学习目标，相比其他方法有奖励和语言质量提升。


<details>
  <summary>Details</summary>
Motivation: 让问答代理学会询问澄清问题。

Method: 用强化学习模拟含澄清问题的对话，提出可视为奖励加权监督微调的离线强化学习目标。

Result: 与基于监督微调与直接偏好优化的方法对比，在优化奖励和语言质量上有提升。

Conclusion: 所提方法在问答代理询问澄清问题方面优于其他方法。

Abstract: Question answering (QA) agents automatically answer questions posed in
natural language. In this work, we learn to ask clarifying questions in QA
agents. The key idea in our method is to simulate conversations that contain
clarifying questions and learn from them using reinforcement learning (RL). To
make RL practical, we propose and analyze offline RL objectives that can be
viewed as reward-weighted supervised fine-tuning (SFT) and easily optimized in
large language models. Our work stands in a stark contrast to recently proposed
methods, based on SFT and direct preference optimization, which have additional
hyper-parameters and do not directly optimize rewards. We compare to these
methods empirically and report gains in both optimized rewards and language
quality.

</details>


### [564] [RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality](https://arxiv.org/abs/2506.07171)
*Chenlong Zhang,Zhuoran Jin,Hongbang Yuan,Jiaheng Wei,Tong Zhou,Kang Liu,Jun Zhao,Yubo Chen*

Main category: cs.CL

TL;DR: 提出RULE框架解决大语言模型去学习问题，效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型包含敏感等内容，现有去学习方法有缺陷，需新方法。

Method: 将去学习问题转化为拒绝边界优化问题，用少量遗忘集和合成边界查询训练，使用可验证奖励函数。

Result: 仅用12%遗忘集和8%合成边界数据，RULE在遗忘质量和回复自然度上优于基线，保持通用效用。

Conclusion: RULE能有效实现有针对性的去学习，不损害模型效用，还提升输出自然度、训练效率和泛化能力。

Abstract: The widespread deployment of Large Language Models (LLMs) trained on massive,
uncurated corpora has raised growing concerns about the inclusion of sensitive,
copyrighted, or illegal content. This has led to increasing interest in LLM
unlearning: the task of selectively removing specific information from a model
without retraining from scratch or degrading overall utility. However, existing
methods often rely on large-scale forget and retain datasets, and suffer from
unnatural responses, poor generalization, or catastrophic utility loss. In this
work, we propose Reinforcement UnLearning (RULE), an efficient framework that
formulates unlearning as a refusal boundary optimization problem. RULE is
trained with a small portion of the forget set and synthesized boundary
queries, using a verifiable reward function that encourages safe refusal on
forget--related queries while preserving helpful responses on permissible
inputs. We provide both theoretical and empirical evidence demonstrating the
effectiveness of RULE in achieving targeted unlearning without compromising
model utility. Experimental results show that, with only $12%$ forget set and
$8%$ synthesized boundary data, RULE outperforms existing baselines by up to
$17.5%$ forget quality and $16.3%$ naturalness response while maintaining
general utility, achieving forget--retain Pareto optimality. Remarkably, we
further observe that RULE improves the naturalness of model outputs, enhances
training efficiency, and exhibits strong generalization ability, generalizing
refusal behavior to semantically related but unseen queries.

</details>


### [565] [Improving the Efficiency of Long Document Classification using Sentence Ranking Approach](https://arxiv.org/abs/2506.07248)
*Prathamesh Kokate,Mitali Sarnaik,Manavi Khopade,Raviraj Joshi*

Main category: cs.CL

TL;DR: 提出基于TF-IDF的句子排序方法用于长文档分类，在MahaNews LDC数据集上评估，可减少输入大小和推理延迟，不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的模型在长文档分类中存在计算限制，使用全量文档分类存在冗余。

Method: 提出基于TF-IDF的句子排序方法，探索固定数量和百分比的句子选择，结合归一化TF-IDF分数和句子长度的增强评分策略。

Result: 在MahaNews LDC数据集上，该方法始终优于基线方法；使用MahaBERT - v2时，分类准确率仅下降0.33%，输入大小减少超50%，推理延迟降低43%。

Conclusion: 该方法可大幅减少上下文且不牺牲性能，适用于现实世界的长文档分类任务。

Abstract: Long document classification poses challenges due to the computational
limitations of transformer-based models, particularly BERT, which are
constrained by fixed input lengths and quadratic attention complexity.
Moreover, using the full document for classification is often redundant, as
only a subset of sentences typically carries the necessary information. To
address this, we propose a TF-IDF-based sentence ranking method that improves
efficiency by selecting the most informative content. Our approach explores
fixed-count and percentage-based sentence selection, along with an enhanced
scoring strategy combining normalized TF-IDF scores and sentence length.
Evaluated on the MahaNews LDC dataset of long Marathi news articles, the method
consistently outperforms baselines such as first, last, and random sentence
selection. With MahaBERT-v2, we achieve near-identical classification accuracy
with just a 0.33 percent drop compared to the full-context baseline, while
reducing input size by over 50 percent and inference latency by 43 percent.
This demonstrates that significant context reduction is possible without
sacrificing performance, making the method practical for real-world long
document classification tasks.

</details>


### [566] [Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models](https://arxiv.org/abs/2506.07424)
*Kyeonghyun Kim,Jinhee Jang,Juhwan Choi,Yoonji Lee,Kyohoon Jin,YoungBin Kim*

Main category: cs.CL

TL;DR: 提出PiFi框架结合大、小语言模型优势，提升效率与性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算需求高，小语言模型泛化能力不足，需结合二者优势。

Method: 将大语言模型的单层冻结层集成到小语言模型，针对特定任务微调组合模型。

Result: PiFi在自然语言处理任务中性能持续提升，能有效利用大语言模型知识。

Conclusion: PiFi可结合大、小语言模型优势，提升性能并增强泛化能力。

Abstract: Large language models (LLMs) are renowned for their extensive linguistic
knowledge and strong generalization capabilities, but their high computational
demands make them unsuitable for resource-constrained environments. In
contrast, small language models (SLMs) are computationally efficient but often
lack the broad generalization capacity of LLMs. To bridge this gap, we propose
PiFi, a novel framework that combines the strengths of both LLMs and SLMs to
achieve high performance while maintaining efficiency. PiFi integrates a single
frozen layer from an LLM into a SLM and fine-tunes the combined model for
specific tasks, boosting performance without a significant increase in
computational cost. We show that PiFi delivers consistent performance
improvements across a range of natural language processing tasks, including
both natural language understanding and generation. Moreover, our findings
demonstrate PiFi's ability to effectively leverage LLM knowledge, enhancing
generalization to unseen domains and facilitating the transfer of linguistic
abilities.

</details>


### [567] [Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding](https://arxiv.org/abs/2506.07434)
*Feifan Song,Shaohang Wei,Wen Luo,Yuxuan Fan,Tianyu Liu,Guoyin Wang,Houfeng Wang*

Main category: cs.CL

TL;DR: 提出Weak-to-Strong Decoding (WSD)框架增强大语言模型对齐能力，用小模型引导大模型，还收集新数据集微调小模型，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需与人类偏好对齐，现有低资源方法在获取高质量对齐内容上有挑战，且生成对齐回复的难度集中在解码开始阶段。

Method: 提出WSD框架，用小对齐模型引导大基础模型，小模型先起草对齐开头，大模型继续后续内容，有自动切换机制；收集GenerAlign数据集微调小尺寸Pilot - 3B作为草稿模型。

Result: 在WSD框架下有效增强不同基础模型，优于所有基线方法，且避免下游任务性能下降。

Conclusion: WSD框架能有效增强大语言模型的对齐能力，通过实验还对不同设置、时间效率和内在机制进行了分析。

Abstract: Large Language Models (LLMs) require alignment with human preferences to
avoid generating offensive, false, or meaningless content. Recently,
low-resource methods for LLM alignment have been popular, while still facing
challenges in obtaining both high-quality and aligned content. Motivated by the
observation that the difficulty of generating aligned responses is concentrated
at the beginning of decoding, we propose a novel framework, Weak-to-Strong
Decoding (WSD), to enhance the alignment ability of base models by the guidance
of a small aligned model. The small model first drafts well-aligned beginnings,
followed by the large base model to continue the rest, controlled by a
well-designed auto-switch mechanism. We also collect a new dataset, GenerAlign,
to fine-tune a small-sized Pilot-3B as the draft model, which effectively
enhances different base models under the WSD framework to outperform all
baseline methods, while avoiding degradation on downstream tasks, termed as the
alignment tax. Extensive experiments are further conducted to examine the
impact of different settings and time efficiency, as well as analyses on the
intrinsic mechanisms of WSD in depth.

</details>


### [568] [KScope: A Framework for Characterizing the Knowledge Status of Language Models](https://arxiv.org/abs/2506.07458)
*Yuxin Xiao,Shan Chen,Jack Gallifant,Danielle Bitterman,Thomas Hartvigsen,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: 本文引入五种知识状态分类，提出KScope框架分析大语言模型知识状态，应用于九个大模型和四个数据集得出相关结论。


<details>
  <summary>Details</summary>
Motivation: 先前工作研究大语言模型知识冲突行为不能完全反映其对问题答案的掌握情况，需要更好的方法来刻画模型知识。

Method: 引入基于一致性和正确性的五种知识状态分类，提出KScope分层统计测试框架来刻画大语言模型知识状态。

Result: （1）支持性上下文缩小模型间知识差距；（2）与难度、相关性和熟悉度相关的上下文特征推动知识更新；（3）部分正确或有冲突时模型特征偏好相似，持续错误时差异大；（4）上下文总结结合特征分析和增强可信度可提高更新效果并具有泛化性。

Conclusion: 所提出的方法和分析结果有助于更好地理解和优化大语言模型的知识处理能力。

Abstract: Characterizing a large language model's (LLM's) knowledge of a given question
is challenging. As a result, prior work has primarily examined LLM behavior
under knowledge conflicts, where the model's internal parametric memory
contradicts information in the external context. However, this does not fully
reflect how well the model knows the answer to the question. In this paper, we
first introduce a taxonomy of five knowledge statuses based on the consistency
and correctness of LLM knowledge modes. We then propose KScope, a hierarchical
framework of statistical tests that progressively refines hypotheses about
knowledge modes and characterizes LLM knowledge into one of these five
statuses. We apply KScope to nine LLMs across four datasets and systematically
establish: (1) Supporting context narrows knowledge gaps across models. (2)
Context features related to difficulty, relevance, and familiarity drive
successful knowledge updates. (3) LLMs exhibit similar feature preferences when
partially correct or conflicted, but diverge sharply when consistently wrong.
(4) Context summarization constrained by our feature analysis, together with
enhanced credibility, further improves update effectiveness and generalizes
across LLMs.

</details>


### [569] [CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models](https://arxiv.org/abs/2506.07463)
*Guang Liu,Liangdong Wang,Jijie Li,Yang Yu,Yao Xu,Jiabei Chen,Yu Bai,Feng Liao,Yonghua Lin*

Main category: cs.CL

TL;DR: 本文介绍大规模双语预训练数据集CCI4.0，提出数据质量处理流程和CoT模板提取方法，实验表明预训练模型在下游任务表现提升。


<details>
  <summary>Details</summary>
Motivation: 构建高质量、包含多样人类推理轨迹的大规模双语预训练数据集，解决不同领域数据质量标准动态变化问题。

Method: 提出基于模型的数据质量处理流程，包括两阶段去重、多分类器质量评分和领域感知流畅性过滤；采用分阶段CoT提取方法。

Result: 预训练模型在下游任务尤其是数学和代码反思任务中有持续性能提升。

Conclusion: 严格的数据筛选和人类思维模板对提升大语言模型性能至关重要，为自动处理预训练语料提供思路。

Abstract: We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered
for superior data quality and diverse human-like reasoning trajectory. CCI4.0
occupies roughly $35$ TB of disk space and comprises two sub-datasets:
CCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a $5.2$ TB carefully
curated Chinese web corpus, a $22.5$ TB English subset from Nemotron-CC, and
diverse sources from math, wiki, arxiv, and code. Although these data are
mostly sourced from well-processed datasets, the quality standards of various
domains are dynamic and require extensive expert experience and labor to
process. So, we propose a novel pipeline justifying data quality mainly based
on models through two-stage deduplication, multiclassifier quality scoring, and
domain-aware fluency filtering. We extract $4.5$ billion pieces of
CoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the
distillation of CoT from larger models, our proposed staged CoT extraction
exemplifies diverse reasoning patterns and significantly decreases the
possibility of hallucination. Empirical evaluations demonstrate that LLMs
pre-trained in CCI4.0 benefit from cleaner, more reliable training signals,
yielding consistent improvements in downstream tasks, especially in math and
code reflection tasks. Our results underscore the critical role of rigorous
data curation and human thinking templates in advancing LLM performance,
shedding some light on automatically processing pretraining corpora.

</details>


### [570] [SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition](https://arxiv.org/abs/2506.07557)
*Mengsong Wu,Di Zhang,Yuqiang Li,Dongzhan Zhou,Wenliang Chen*

Main category: cs.CL

TL;DR: 本文提出SELT框架提升大语言模型推理能力，在基准测试中取得显著改进且具有泛化性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理任务中性能下降，需提升其推理能力。

Method: 引入SELT框架，利用改进的蒙特卡罗树搜索，重新定义上置信界评分，将推理过程分解为原子子任务并结合语义聚类。

Result: 在MMLU和Seal - Tools等基准测试中，SELT在答案准确性和推理鲁棒性上相比基线方法有显著提升。

Conclusion: SELT框架无需特定任务微调，在不同推理任务中具有较强泛化性。

Abstract: While Large Language Models (LLMs) have achieved remarkable success in a wide
range of applications, their performance often degrades in complex reasoning
tasks. In this work, we introduce SELT (Self-Evaluation LLM Tree Search), a
novel framework that leverages a modified Monte Carlo Tree Search (MCTS) to
enhance LLM reasoning without relying on external reward models. By redefining
the Upper Confidence Bound scoring to align with intrinsic self-evaluation
capabilities of LLMs and decomposing the inference process into atomic subtasks
augmented with semantic clustering at each node, SELT effectively balances
exploration and exploitation, reduces redundant reasoning paths, and mitigates
hallucination. We validate our approach on challenging benchmarks, including
the knowledge-based MMLU and the Tool Learning dataset Seal-Tools, where SELT
achieves significant improvements in answer accuracy and reasoning robustness
compared to baseline methods. Notably, our framework operates without
task-specific fine-tuning, demonstrating strong generalizability across diverse
reasoning tasks. Relevant results and code are available at
https://github.com/fairyshine/SELT .

</details>


### [571] [LoRMA: Low-Rank Multiplicative Adaptation for LLMs](https://arxiv.org/abs/2506.07621)
*Harsh Bihany,Shubham Patel,Ashutosh Modi*

Main category: cs.CL

TL;DR: 提出Low - Rank Multiplicative Adaptation (LoRMA)方法，通过操作重排序和引入秩膨胀策略解决问题，并实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 全量微调计算成本高，现有LoRA及变体采用重新参数化的加法更新，需更优方法。

Method: 提出LoRMA，将加法更新范式转变为矩阵乘法变换，通过有效重排操作和引入秩膨胀策略应对矩阵乘法的计算复杂度和秩瓶颈挑战。

Result: 通过大量实验证明该方法在各种评估指标上有效。

Conclusion: LoRMA是一种有效的方法，可在NLP领域提升大语言模型微调效率。

Abstract: Large Language Models have shown remarkable capabilities in the NLP domain.
Their effectiveness can mainly be attributed to their ability to adapt to an
array of downstream tasks. However, generally, full fine-tuning is a
computationally expensive job. To mitigate this, many techniques have been
developed that prime efficiency, a prominent one being Low-Rank Adaptation
(LoRA). However, LoRA and its variants employ re-parametrized additive updates.
In this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which
shifts the paradigm of additive updates to a richer space of matrix
multiplicative transformations. We tackle challenges such as computational
complexity and rank bottleneck of matrix multiplication by effectively
re-ordering operations and introducing rank inflation strategies. We conduct
extensive experiments to demonstrate the effectiveness of our approach in terms
of various evaluation metrics.

</details>


### [572] [Beyond the Sentence: A Survey on Context-Aware Machine Translation with Large Language Models](https://arxiv.org/abs/2506.07583)
*Ramakrishna Appicharla,Baban Gain,Santanu Pal,Asif Ekbal*

Main category: cs.CL

TL;DR: 本文对大语言模型（LLMs）在上下文感知机器翻译中的应用进行文献综述，对比不同模型和方法的效果并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在机器翻译，尤其是上下文感知场景下的应用研究不足，需进行探索。

Method: 进行文献综述，分析现有利用提示和微调方法的研究，涉及自动后编辑和创建翻译代理等。

Result: 商业大语言模型（如ChatGPT和Tower LLM）比开源大语言模型（如Llama和Bloom LLMs）效果更好，基于提示的方法可作为评估翻译质量的良好基线。

Conclusion: 提出一些值得探索的未来研究方向。

Abstract: Despite the popularity of the large language models (LLMs), their application
to machine translation is relatively underexplored, especially in context-aware
settings. This work presents a literature review of context-aware translation
with LLMs. The existing works utilise prompting and fine-tuning approaches,
with few focusing on automatic post-editing and creating translation agents for
context-aware machine translation. We observed that the commercial LLMs (such
as ChatGPT and Tower LLM) achieved better results than the open-source LLMs
(such as Llama and Bloom LLMs), and prompt-based approaches serve as good
baselines to assess the quality of translations. Finally, we present some
interesting future directions to explore.

</details>


### [573] [Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch](https://arxiv.org/abs/2506.07667)
*Prarabdh Shukla,Wei Yin Chong,Yash Patel,Brennan Schaffner,Danish Pruthi,Arjun Bhagoji*

Main category: cs.CL

TL;DR: 本文对Twitch的自动审核工具AutoMod进行审计，发现其在标记仇恨内容方面存在大量漏检，对含敏感词的良性内容也存在高误封率，凸显理解上下文的重要性。


<details>
  <summary>Details</summary>
Motivation: 在线平台为满足内容审核需求采用自动化系统，新兴实时互动形式对审核系统延迟提出新要求，且对这类系统的有效性了解不足，因此要研究Twitch自动审核工具AutoMod标记仇恨内容的有效性。

Method: 创建流媒体账户作为独立测试平台，通过Twitch的API向直播聊天发送来自4个数据集的超107000条评论，测量AutoMod标记包含厌女、种族主义、残疾歧视和恐同的明显仇恨内容的准确性。

Result: 大量仇恨信息绕过审核，部分数据集高达94%；添加污言秽语可100%移除；违背社区准则，高达89.5%在教学或赋能语境中使用敏感词的良性示例被屏蔽。

Conclusion: AutoMod能力存在较大差距，此类系统有效理解上下文十分重要。

Abstract: To meet the demands of content moderation, online platforms have resorted to
automated systems. Newer forms of real-time engagement($\textit{e.g.}$, users
commenting on live streams) on platforms like Twitch exert additional pressures
on the latency expected of such moderation systems. Despite their prevalence,
relatively little is known about the effectiveness of these systems. In this
paper, we conduct an audit of Twitch's automated moderation tool
($\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful
content. For our audit, we create streaming accounts to act as siloed test
beds, and interface with the live chat using Twitch's APIs to send over
$107,000$ comments collated from $4$ datasets. We measure $\texttt{AutoMod}$'s
accuracy in flagging blatantly hateful content containing misogyny, racism,
ableism and homophobia. Our experiments reveal that a large fraction of hateful
messages, up to $94\%$ on some datasets, $\textit{bypass moderation}$.
Contextual addition of slurs to these messages results in $100\%$ removal,
revealing $\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We
also find that contrary to Twitch's community guidelines, $\texttt{AutoMod}$
blocks up to $89.5\%$ of benign examples that use sensitive words in
pedagogical or empowering contexts. Overall, our audit points to large gaps in
$\texttt{AutoMod}$'s capabilities and underscores the importance for such
systems to understand context effectively.

</details>


### [574] [Training Superior Sparse Autoencoders for Instruct Models](https://arxiv.org/abs/2506.07691)
*Jiaming Li,Haoran Ye,Yukun Chen,Xinyue Li,Lei Zhang,Hamid Alinejad-Rokny,Jimmy Chih-Hsien Peng,Min Yang*

Main category: cs.CL

TL;DR: 随着大语言模型发展，现有稀疏自编码器（SAEs）训练方法用于指令模型效果不佳，提出FAST方法，在重建和特征可解释性上表现更好，干预特殊标记激活可提升输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有SAE训练方法主要为基础模型设计，用于指令模型时重建质量和可解释性降低，需提出适合指令模型的训练方法。

Method: 提出Finetuning-aligned Sequential Training (FAST)方法，使训练过程与指令模型的数据分布和激活模式对齐。

Result: 在Qwen2.5 - 7B - Instruct上，FAST在令牌重建上均方误差为0.6468，显著优于基线方法；在特征可解释性上，Llama3.2 - 3B - Instruct中高质量特征比例更高。

Conclusion: FAST方法在指令模型的SAE训练中表现出色，干预特殊标记激活为模型行为细粒度控制带来新机会，代码等资源可在指定链接获取。

Abstract: As large language models (LLMs) grow in scale and capability, understanding
their internal mechanisms becomes increasingly critical. Sparse autoencoders
(SAEs) have emerged as a key tool in mechanistic interpretability, enabling the
extraction of human-interpretable features from LLMs. However, existing SAE
training methods are primarily designed for base models, resulting in reduced
reconstruction quality and interpretability when applied to instruct models. To
bridge this gap, we propose
$\underline{\textbf{F}}$inetuning-$\underline{\textbf{a}}$ligned
$\underline{\textbf{S}}$equential $\underline{\textbf{T}}$raining
($\textit{FAST}$), a novel training method specifically tailored for instruct
models. $\textit{FAST}$ aligns the training process with the data distribution
and activation patterns characteristic of instruct models, resulting in
substantial improvements in both reconstruction and feature interpretability.
On Qwen2.5-7B-Instruct, $\textit{FAST}$ achieves a mean squared error of 0.6468
in token reconstruction, significantly outperforming baseline methods with
errors of 5.1985 and 1.5096. In feature interpretability, $\textit{FAST}$
yields a higher proportion of high-quality features, for Llama3.2-3B-Instruct,
$21.1\%$ scored in the top range, compared to $7.0\%$ and $10.2\%$ for
$\textit{BT(P)}$ and $\textit{BT(F)}$. Surprisingly, we discover that
intervening on the activations of special tokens via the SAEs leads to
improvements in output quality, suggesting new opportunities for fine-grained
control of model behavior. Code, data, and 240 trained SAEs are available at
https://github.com/Geaming2002/FAST.

</details>


### [575] [Synthesis by Design: Controlled Data Generation via Structural Guidance](https://arxiv.org/abs/2506.07664)
*Lei Xu,Sirui Chen,Yuxuan Huang,Chaochao Lu*

Main category: cs.CL

TL;DR: 提出从数学推理中提取结构信息指导数据生成的方法，生成问题并进行实验，验证了数据集有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提升大语言模型数学推理能力时存在生成质量和问题复杂度的问题。

Method: 从数学推理中提取结构信息，用结构化解决方案指导数据生成。

Result: 生成39K带中间步骤问题和6.1K高难度问题基准，发现模型性能随推理长度增加而下降，验证了数据集有效性。

Conclusion: 提出的方法和数据集有望为提升大语言模型推理能力的未来研究做出贡献。

Abstract: Mathematical reasoning remains challenging for LLMs due to complex logic and
the need for precise computation. Existing methods enhance LLM reasoning by
synthesizing datasets through problem rephrasing, but face issues with
generation quality and problem complexity. To address this, we propose to
extract structural information with generated problem-solving code from
mathematical reasoning and guide data generation with structured solutions.
Applied to MATH and GSM8K, our approach produces 39K problems with labeled
intermediate steps and a 6.1K-problem benchmark of higher difficulty. Results
on our benchmark show that model performance declines as reasoning length
increases. Additionally, we conducted fine-tuning experiments using the
proposed training data on a range of LLMs, and the results validate the
effectiveness of our dataset. We hope the proposed method and dataset will
contribute to future research in enhancing LLM reasoning capabilities.

</details>


### [576] [GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation](https://arxiv.org/abs/2506.07671)
*Ionut-Teodor Sorodoc,Leonardo F. R. Ribeiro,Rexhina Blloshmi,Christopher Davis,Adrià de Gispert*

Main category: cs.CL

TL;DR: 本文介绍了大型RAG基准GaRAGe，用其评估LLMs在RAG任务中的表现，发现模型存在过度总结等问题。


<details>
  <summary>Details</summary>
Motivation: 构建一个能对大语言模型在生成RAG答案时识别相关依据能力进行细粒度评估的基准。

Method: 构建包含不同复杂度、动态性和主题的2366个问题及超35K注释段落的GaRAG基准，用其评估多个最先进的大语言模型。

Result: 模型倾向于过度总结，相关事实性得分最高60%，偏转的真阳性率最高31%，来源归因F1最高58.9%，在回答时效性问题和利用稀疏私有依据源时性能下降。

Conclusion: GaRAG基准是评估大语言模型在RAG任务中识别相关信息能力的理想测试平台，现有模型在该任务中存在一定不足。

Abstract: We present GaRAGe, a large RAG benchmark with human-curated long-form answers
and annotations of each grounding passage, allowing a fine-grained evaluation
of whether LLMs can identify relevant grounding when generating RAG answers.
Our benchmark contains 2366 questions of diverse complexity, dynamism, and
topics, and includes over 35K annotated passages retrieved from both private
document sets and the Web, to reflect real-world RAG use cases. This makes it
an ideal test bed to evaluate an LLM's ability to identify only the relevant
information necessary to compose a response, or provide a deflective response
when there is insufficient information. Evaluations of multiple
state-of-the-art LLMs on GaRAGe show that the models tend to over-summarise
rather than (a) ground their answers strictly on the annotated relevant
passages (reaching at most a Relevance-Aware Factuality Score of 60%), or (b)
deflect when no relevant grounding is available (reaching at most 31% true
positive rate in deflections). The F1 in attribution to relevant sources is at
most 58.9%, and we show that performance is particularly reduced when answering
time-sensitive questions and when having to draw knowledge from sparser private
grounding sources.

</details>


### [577] [LLM Unlearning Should Be Form-Independent](https://arxiv.org/abs/2506.07795)
*Xiaotian Ye,Mengqi Zhang,Shu Wu*

Main category: cs.CL

TL;DR: 本文指出大语言模型遗忘存在形式依赖偏差问题，引入ORT基准评估，提出无训练方法ROCR，实验表明其能提升遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型遗忘方法在现实场景中效果有限，难以实际应用。

Method: 定义形式依赖偏差问题，引入ORT基准评估，提出Rank - one Concept Redirection (ROCR)无训练方法。

Result: 形式依赖偏差在当前技术中普遍且严重，ROCR相比传统方法显著提升遗忘效果，输出自然。

Conclusion: 大语言模型遗忘应形式独立，ROCR是有前景的解决方案。

Abstract: Large Language Model (LLM) unlearning aims to erase or suppress undesirable
knowledge within the model, offering promise for controlling harmful or private
information to prevent misuse. However, recent studies highlight its limited
efficacy in real-world scenarios, hindering practical adoption. In this study,
we identify a pervasive issue underlying many downstream failures: the
effectiveness of existing unlearning methods heavily depends on the form of
training samples and frequently fails to generalize to alternate expressions of
the same knowledge. We formally characterize this problem as Form-Dependent
Bias and systematically investigate its specific manifestation patterns across
various downstream tasks. To quantify its prevalence and support future
research, we introduce ORT, a novel benchmark designed to evaluate the
robustness of unlearning methods against variations in knowledge expression.
Results reveal that Form-Dependent Bias is both widespread and severe among
current techniques.
  We argue that LLM unlearning should be form-independent to address the
endless forms of downstream tasks encountered in real-world security-critical
scenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),
a novel training-free method, as a promising solution path. ROCR performs
unlearning by targeting the invariants in downstream tasks, specifically the
activated dangerous concepts. It is capable of modifying model parameters
within seconds to redirect the model's perception of a specific unlearning
target concept to another harmless concept. Extensive experiments demonstrate
that ROCR significantly improves unlearning effectiveness compared to
traditional methods while generating highly natural outputs.

</details>


### [578] [MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification](https://arxiv.org/abs/2506.07801)
*Iustin Sirbu,Robert-Adrian Popovici,Cornelia Caragea,Stefan Trausan-Matu,Traian Rebedea*

Main category: cs.CL

TL;DR: 提出半监督学习算法MultiMatch，结合多种范式，有三重伪标签加权模块，实验表现优，在不平衡场景鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: 提升半监督学习的鲁棒性和性能，解决数据不平衡问题。

Method: 结合协同训练、一致性正则化和伪标签范式，设计三重伪标签加权模块统一现有技术。

Result: 在基准数据集上表现优越，9/10设置达SOTA，Friedman测试排名第一，不平衡场景下优于次优方法3.26%。

Conclusion: MultiMatch算法在半监督学习中能提升性能和鲁棒性，尤其在数据不平衡场景表现出色。

Abstract: We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm
combining the paradigms of co-training and consistency regularization with
pseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label
weighting module designed for three key purposes: selecting and filtering
pseudo-labels based on head agreement and model confidence, and weighting them
according to the perceived classification difficulty. This novel module
enhances and unifies three existing techniques -- heads agreement from
Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average
Pseudo-Margins from MarginMatch -- resulting in a holistic approach that
improves robustness and performance in SSL settings. Experimental results on
benchmark datasets highlight the superior performance of MultiMatch, achieving
state-of-the-art results on 9 out of 10 setups from 5 natural language
processing datasets and ranking first according to the Friedman test among 19
methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly
imbalanced settings, outperforming the second-best approach by 3.26% -- and
data imbalance is a key factor for many text classification tasks.

</details>


### [579] [Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking](https://arxiv.org/abs/2506.07751)
*Silin Gao,Antoine Bosselut,Samy Bengio,Emmanuel Abbe*

Main category: cs.CL

TL;DR: 小语言模型推理缺乏鲁棒性，本文提出AbstraL方法用强化学习促进抽象推理，减轻GSM扰动基准测试中的性能下降。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型尤其是小模型在推理时面对分布变化易出现性能下降的问题。

Method: 采用强化学习在细粒度抽象数据上促进大语言模型的抽象推理，提出AbstraL方法。

Result: AbstraL方法显著减轻了近期GSM扰动基准测试中的性能下降。

Conclusion: 通过强化学习进行抽象推理过程比监督微调更能解决大语言模型推理缺乏鲁棒性的问题。

Abstract: Recent studies have shown that large language models (LLMs), especially
smaller ones, often lack robustness in their reasoning. I.e., they tend to
experience performance drops when faced with distribution shifts, such as
changes to numerical or nominal variables, or insertions of distracting
clauses. A possible strategy to address this involves generating synthetic data
to further "instantiate" reasoning problems on potential variations. In
contrast, our approach focuses on "abstracting" reasoning problems. This not
only helps counteract distribution shifts but also facilitates the connection
to symbolic tools for deriving solutions. We find that this abstraction process
is better acquired through reinforcement learning (RL) than just supervised
fine-tuning, which often fails to produce faithful abstractions. Our method,
AbstraL -- which promotes abstract reasoning in LLMs using RL on granular
abstraction data -- significantly mitigates performance degradation on recent
GSM perturbation benchmarks.

</details>


### [580] [MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs](https://arxiv.org/abs/2506.07899)
*Ke Wang,Yiming Qin,Nikolaos Dimitriadis,Alessandro Favero,Pascal Frossard*

Main category: cs.CL

TL;DR: 提出MEMOIR框架解决语言模型后验更新难题，实验证明其性能优。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型后验更新中高效可靠编辑且不遗忘先前信息的难题，现有方法存在不足。

Method: 提出MEMOIR框架，通过残差内存注入知识，利用样本依赖掩码稀疏化输入激活，推理时对比激活模式。

Result: 在问答、幻觉纠正等基准测试中，MEMOIR在可靠性、泛化性和局部性指标上达最优，能处理数千次顺序编辑且遗忘率低。

Conclusion: MEMOIR是一种有效且可扩展的语言模型编辑框架。

Abstract: Language models deployed in real-world systems often require post-hoc updates
to incorporate new or corrected knowledge. However, editing such models
efficiently and reliably - without retraining or forgetting previous
information - remains a major challenge. Existing methods for lifelong model
editing either compromise generalization, interfere with past edits, or fail to
scale to long editing sequences. We propose MEMOIR, a novel scalable framework
that injects knowledge through a residual memory, i.e., a dedicated parameter
module, while preserving the core capabilities of the pre-trained model. By
sparsifying input activations through sample-dependent masks, MEMOIR confines
each edit to a distinct subset of the memory parameters, minimizing
interference among edits. At inference, it identifies relevant edits by
comparing the sparse activation patterns of new queries to those stored during
editing. This enables generalization to rephrased queries by activating only
the relevant knowledge while suppressing unnecessary memory activation for
unrelated prompts. Experiments on question answering, hallucination correction,
and out-of-distribution generalization benchmarks across LLaMA-3 and Mistral
demonstrate that MEMOIR achieves state-of-the-art performance across
reliability, generalization, and locality metrics, scaling to thousands of
sequential edits with minimal forgetting.

</details>


### [581] [Language Models over Canonical Byte-Pair Encodings](https://arxiv.org/abs/2506.07956)
*Tim Vieira,Tianyu Liu,Clemente Pasti,Yahya Emara,Brian DuSell,Benjamin LeBrun,Mario Giulianelli,Juan Luis Gastaldi,Timothy J. O'Donnell,Ryan Cotterell*

Main category: cs.CL

TL;DR: 提出方法让token级语言模型只给规范token串分配正概率，展示两种方法并证明能提升数据似然


<details>
  <summary>Details</summary>
Motivation: 当前语言模型会给非规范token编码分配非零概率，既错误又浪费概率质量

Method: 提出两种方法：基于条件的规范性（利用测试时推理策略，无需额外训练）和基于构造的规范性（需训练，保证规范输出）

Result: 修正规范性错误提升了多个模型和语料库中保留数据的似然

Conclusion: 所提方法能有效解决语言模型中非规范token编码的问题

Abstract: Modern language models represent probability distributions over character
strings as distributions over (shorter) token strings derived via a
deterministic tokenizer, such as byte-pair encoding. While this approach is
highly effective at scaling up language models to large corpora, its current
incarnations have a concerning property: the model assigns nonzero probability
mass to an exponential number of $\it{noncanonical}$ token encodings of each
character string -- these are token strings that decode to valid character
strings but are impossible under the deterministic tokenizer (i.e., they will
never be seen in any training corpus, no matter how large). This misallocation
is both erroneous, as noncanonical strings never appear in training data, and
wasteful, diverting probability mass away from plausible outputs. These are
avoidable mistakes! In this work, we propose methods to enforce canonicality in
token-level language models, ensuring that only canonical token strings are
assigned positive probability. We present two approaches: (1) canonicality by
conditioning, leveraging test-time inference strategies without additional
training, and (2) canonicality by construction, a model parameterization that
guarantees canonical outputs but requires training. We demonstrate that fixing
canonicality mistakes improves the likelihood of held-out data for several
models and corpora.

</details>


### [582] [MiniCPM4: Ultra-Efficient LLMs on End Devices](https://arxiv.org/abs/2506.07900)
*MiniCPM Team,Chaojun Xiao,Yuxuan Li,Xu Han,Yuzhuo Bai,Jie Cai,Haotian Chen,Wentong Chen,Xin Cong,Ganqu Cui,Ning Ding,Shengdan Fan,Yewei Fang,Zixuan Fu,Wenyu Guan,Yitong Guan,Junshao Guo,Yufeng Han,Bingxiang He,Yuxiang Huang,Cunliang Kong,Qiuzuo Li,Siyuan Li,Wenhao Li,Yanghao Li,Yishan Li,Zhen Li,Dan Liu,Biyuan Lin,Yankai Lin,Xiang Long,Quanyu Lu,Yaxi Lu,Peiyan Luo,Hongya Lyu,Litu Ou,Yinxu Pan,Zekai Qu,Qundong Shi,Zijun Song,Jiayuan Su,Zhou Su,Ao Sun,Xianghui Sun,Peijun Tang,Fangzheng Wang,Feng Wang,Shuo Wang,Yudong Wang,Yesai Wu,Zhenyu Xiao,Jie Xie,Zihao Xie,Yukun Yan,Jiarui Yuan,Kaihuo Zhang,Lei Zhang,Linyue Zhang,Xueren Zhang,Yudi Zhang,Hengyu Zhao,Weilin Zhao,Weilun Zhao,Yuanqian Zhao,Zhi Zheng,Ge Zhou,Jie Zhou,Wei Zhou,Zihan Zhou,Zixuan Zhou,Zhiyuan Liu,Guoyang Zeng,Chao Jia,Dahai Li,Maosong Sun*

Main category: cs.CL

TL;DR: 本文介绍专为端侧设备设计的高效大语言模型MiniCPM4，通过多维度创新实现高效，有两个版本，评估显示其性能出色且有广泛可用性。


<details>
  <summary>Details</summary>
Motivation: 设计适用于端侧设备的高效大语言模型。

Method: 在模型架构、训练数据、训练算法和推理系统四个关键维度进行创新，如提出InfLLM v2、UltraClean、UltraChat v2等。

Result: MiniCPM4在多个基准测试中优于类似规模的开源模型，MiniCPM4 - 8B处理长序列时比Qwen3 - 8B速度显著提升。

Conclusion: MiniCPM4高效且有效，具有广泛的可用性，能支持多种应用。

Abstract: This paper introduces MiniCPM4, a highly efficient large language model (LLM)
designed explicitly for end-side devices. We achieve this efficiency through
systematic innovation in four key dimensions: model architecture, training
data, training algorithms, and inference systems. Specifically, in terms of
model architecture, we propose InfLLM v2, a trainable sparse attention
mechanism that accelerates both prefilling and decoding phases for long-context
processing. Regarding training data, we propose UltraClean, an efficient and
accurate pre-training data filtering and generation strategy, and UltraChat v2,
a comprehensive supervised fine-tuning dataset. These datasets enable
satisfactory model performance to be achieved using just 8 trillion training
tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient
pre-training strategy search, and improve existing post-training methods by
introducing chunk-wise rollout for load-balanced reinforcement learning and
data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose
CPM.cu that integrates sparse attention, model quantization, and speculative
sampling to achieve efficient prefilling and decoding. To meet diverse
on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B
parameters, respectively. Sufficient evaluation results show that MiniCPM4
outperforms open-source models of similar size across multiple benchmarks,
highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B
demonstrates significant speed improvements over Qwen3-8B when processing long
sequences. Through further adaptation, MiniCPM4 successfully powers diverse
applications, including trustworthy survey generation and tool use with model
context protocol, clearly showcasing its broad usability.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [583] [One-dimensional quantile-stratified sampling and its application in statistical simulations](https://arxiv.org/abs/2506.07437)
*Ben O'Neill*

Main category: stat.ME

TL;DR: 本文研究已知单变量概率分布的分位数分层抽样，对比其与标准独立同分布抽样，探讨其在统计模拟中的应用并进行性能比较。


<details>
  <summary>Details</summary>
Motivation: 研究分位数分层抽样方法，明确其与标准抽样方法的异同，探索其在统计模拟中的应用。

Method: 研究分位数分层抽样的一般性质，与标准独立同分布抽样对比，将其应用于统计模拟，进行模拟分析比较性能。

Result: 文中未提及具体结果。

Conclusion: 文中未提及具体结论。

Abstract: In this paper we examine quantile-stratified samples from a known univariate
probability distribution, with stratification occurring over a partition of the
quantile regions in the distribution. We examine some general properties of
this sampling method and we contrast it with standard IID sampling to highlight
its similarities and differences. We examine the applications of this sampling
method to various statistical simulations including importance sampling. We
conduct simulation analysis to compare the performance of standard importance
sampling against the quantile-stratified importance sampling to see how they
each perform on a range of functions.

</details>


### [584] [Strongly Consistent Community Detection in Popularity Adjusted Block Models](https://arxiv.org/abs/2506.07224)
*Quan Yuan,Binghui Liu,Danning Li,Lingzhou Xue*

Main category: stat.ME

TL;DR: 提出TCSC和改进算法解决PABM社区检测难题，还提出选社区数量方法，经模拟和实际应用验证有效。


<details>
  <summary>Details</summary>
Motivation: PABM灵活性增加模型复杂度，在适配谱聚类技术和标签恢复一致性方面存在挑战。

Method: 提出Thresholded Cosine Spectral Clustering (TCSC) 算法并证明其弱一致性，引入一步Refined TCSC算法证明强一致性，说明两步Refined TCSC加速聚类误差收敛，提出数据驱动选社区数量方法。

Result: 算法能以高概率正确恢复社区标签，两步Refined TCSC在小样本下加速聚类误差收敛，选社区数量方法表现更优。

Conclusion: 所提方法在模拟和实际应用中有效且稳健。

Abstract: The Popularity Adjusted Block Model (PABM) provides a flexible framework for
community detection in network data by allowing heterogeneous node popularity
across communities. However, this flexibility increases model complexity and
raises key unresolved challenges, particularly in effectively adapting spectral
clustering techniques and efficiently achieving strong consistency in label
recovery. To address these challenges, we first propose the Thresholded Cosine
Spectral Clustering (TCSC) algorithm and establish its weak consistency under
the PABM. We then introduce the one-step Refined TCSC algorithm and prove that
it achieves strong consistency under the PABM, correctly recovering all
community labels with high probability. We further show that the two-step
Refined TCSC accelerates clustering error convergence, especially with small
sample sizes. Additionally, we propose a data-driven approach for selecting the
number of communities, which outperforms existing methods under the PABM. The
effectiveness and robustness of our methods are validated through extensive
simulations and real-world applications.

</details>


### [585] [Heavy Lasso: sparse penalized regression under heavy-tailed noise via data-augmented soft-thresholding](https://arxiv.org/abs/2506.07790)
*The Tien Mai*

Main category: stat.ME

TL;DR: 提出名为Heavy Lasso的鲁棒回归方法，结合t分布损失函数与Lasso惩罚框架，计算高效，理论有界，数值实验表现优。


<details>
  <summary>Details</summary>
Motivation: 经典Lasso在重尾误差或离群值情况下性能下降，为解决此问题提出新方法。

Method: 在Lasso惩罚框架中引入受学生t分布启发的损失函数，利用数据增强方案和软阈值算法。

Result: 建立非渐近界，数值研究表明Heavy Lasso性能优于经典Lasso和其他鲁棒变体。

Conclusion: Heavy Lasso方法在处理重尾噪声和离群值方面有效，已在R包heavylasso实现。

Abstract: High-dimensional linear regression is a fundamental tool in modern
statistics, particularly when the number of predictors exceeds the sample size.
The classical Lasso, which relies on the squared loss, performs well under
Gaussian noise assumptions but often deteriorates in the presence of
heavy-tailed errors or outliers commonly encountered in real data applications
such as genomics, finance, and signal processing. To address these challenges,
we propose a novel robust regression method, termed Heavy Lasso, which
incorporates a loss function inspired by the Student's t-distribution within a
Lasso penalization framework. This loss retains the desirable quadratic
behavior for small residuals while adaptively downweighting large deviations,
thus enhancing robustness to heavy-tailed noise and outliers. Heavy Lasso
enjoys computationally efficient by leveraging a data augmentation scheme and a
soft-thresholding algorithm, which integrate seamlessly with classical Lasso
solvers. Theoretically, we establish non-asymptotic bounds under both $\ell_1$
and $\ell_2 $ norms, by employing the framework of localized convexity, showing
that the Heavy Lasso estimator achieves rates comparable to those of the Huber
loss. Extensive numerical studies demonstrate Heavy Lasso's superior
performance over classical Lasso and other robust variants, highlighting its
effectiveness in challenging noisy settings. Our method is implemented in the R
package heavylasso available on Github.

</details>


### [586] [Conditional Local Independence Testing with Application to Dynamic Causal Discovery](https://arxiv.org/abs/2506.07844)
*Mingzhou Liu,Xinwei Sun,Yizhou Wang*

Main category: stat.ME

TL;DR: 将Christgau等人（2024）提出的条件局部独立性检验理论扩展到伊藤过程，结果可用于动态系统的因果发现。


<details>
  <summary>Details</summary>
Motivation: 将已有条件局部独立性检验理论应用到伊藤过程，以用于动态系统因果发现。

Method: 文中未明确提及具体方法。

Result: 成功将条件局部独立性检验理论扩展到伊藤过程。

Conclusion: 扩展后的理论可应用于动态系统的因果发现。

Abstract: In this note, we extend the conditional local independence testing theory
developed in Christgau et al. (2024) to Ito processes. The result can be
applied to causal discovery in dynamic systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [587] [Quantum Information-Theoretical Size Bounds for Conjunctive Queries with Functional Dependencies](https://arxiv.org/abs/2506.07552)
*Valter Uotila,Jiaheng Lu*

Main category: quant-ph

TL;DR: 本文将量子信息论引入数据库理论研究，用量子Rényi熵替代经典Shannon熵来计算合取查询的最坏情况大小界限，虽有挑战但提出量子版本界限，经典界限是其特殊情况，还探讨未来可能。


<details>
  <summary>Details</summary>
Motivation: 此前研究未用量子信息论解决合取查询在多约束下计算最坏情况大小界限的难题，作者想引入量子信息论解决该问题。

Method: 建立经典信息论与量子信息论联系，用量子Rényi熵替代经典Shannon熵，提出量子版本来推导最坏情况大小界限。

Result: 优化量子态而非经典分布带来新挑战，难以找到实际可计算的紧密最坏情况大小界限；经典紧密最坏情况大小界限是量子界限的特殊极限。

Conclusion: 提出用量子信息论解决合取查询最坏情况大小界限计算问题的思路，讨论了量子信息论在理论数据库研究中的未来可能性。

Abstract: Deriving formulations for computing and estimating tight worst-case size
increases for conjunctive queries with various constraints has been at the core
of theoretical database research. If the problem has no constraints or only one
constraint, such as functional dependencies or degree constraints, tight
worst-case size bounds have been proven, and they are even practically
computable. If the problem has more than one constraint, computing tight bounds
can be difficult in practice and may even require an infinite number of linear
inequalities in its optimization formulation. While these challenges have been
addressed with varying methods, no prior research has employed quantum
information theory to address this problem. In this work, we establish a
connection between earlier work on estimating size bounds for conjunctive
queries with classical information theory and the field of quantum information
theory. We propose replacing the classical Shannon entropy formulation with the
quantum R\'enyi entropy. Whereas classical Shannon entropy requires infinitely
many inequalities to characterize the optimization space, R\'enyi entropy
requires only one type of inequality, which is non-negativity. Although this is
a promising modification, optimization with respect to the quantum states
instead of classical distributions creates a new set of challenges that prevent
us from finding a practically computable, tight worst-case size bound. In this
line, we propose a quantum version to derive worst-case size bounds. The
previous tight classical worst-case size bound can be viewed as a special limit
of this quantum bound. We also provide a comprehensive background on prior
research and discuss the future possibilities of quantum information theory in
theoretical database research.

</details>


### [588] [Optimal quantum sampling on distributed databases](https://arxiv.org/abs/2506.07724)
*Longyun Chen,Jingcheng Liu,Penghui Yao*

Main category: quant-ph

TL;DR: 本文研究分布式环境下的量子采样，提出顺序和并行算法并证明其最优性。


<details>
  <summary>Details</summary>
Motivation: 大规模量子存储成本高，因此研究分布式环境下的量子采样。

Method: 假设数据分布在多台机器上，每台机器维护基本的计数预言机，协调器可进行预言机查询，提出顺序和并行算法。

Result: 提出了顺序和并行两种算法。

Conclusion: 两种算法在各自的设置下都是最优的。

Abstract: Quantum sampling, a fundamental subroutine in numerous quantum algorithms,
involves encoding a given probability distribution in the amplitudes of a pure
state. Given the hefty cost of large-scale quantum storage, we initiate the
study of quantum sampling in a distributed setting. Specifically, we assume
that the data is distributed among multiple machines, and each machine solely
maintains a basic oracle that counts the multiplicity of individual elements.
Given a quantum sampling task, which is to sample from the joint database, a
coordinator can make oracle queries to all machines. We focus on the oblivious
communication model, where communications between the coordinator and the
machines are predetermined. We present both sequential and parallel algorithms:
the sequential algorithm queries the machines sequentially, while the parallel
algorithm allows the coordinator to query all machines simultaneously.
Furthermore, we prove that both algorithms are optimal in their respective
settings.

</details>


### [589] [Adam assisted Fully informed Particle Swarm Optimzation ( Adam-FIPSO ) based Parameter Prediction for the Quantum Approximate Optimization Algorithm (QAOA)](https://arxiv.org/abs/2506.06790)
*Shashank Sanjay Bhat,Peiyong Wang,Udaya Parampalli*

Main category: quant-ph

TL;DR: 提出结合FIPSO与Adam优化器自适应梯度校正的框架优化QAOA参数，实验显示性能优于随机初始化。


<details>
  <summary>Details</summary>
Motivation: 解决QAOA中高效识别合适参数以获得高质量解的挑战，避免出现贫瘠高原和收敛到局部极小值的问题。

Method: 提出结合Fully Informed Particle Swarm Optimization (FIPSO)与使用Adam Optimizer进行自适应梯度校正的框架来搜索QAOA参数空间。

Result: 针对Erdos Renyi和Watts - Strogatz两类图实例评估，在多个QAOA深度的实验结果表明，该算法性能始终优于随机初始化。

Conclusion: 所提出的优化框架具有有效性和鲁棒性。

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a prominent
variational algorithm used for solving combinatorial optimization problems such
as the Max-Cut problem. A key challenge in QAOA lies in efficiently identifying
suitable parameters (gamma, beta) that lead to high-quality solutions. In this
paper, we propose a framework that combines Fully Informed Particle Swarm
Optimization (FIPSO) with adaptive gradient correction using the Adam Optimizer
to navigate the QAOA parameter space. This approach aims to avoid issues such
as barren plateaus and convergence to local minima. The proposed algorithm is
evaluated against two classes of graph instances, Erdos Renyi and
Watts-Strogatz. Experimental results across multiple QAOA depths consistently
demonstrate superior performance compared to random initialization,
underscoring the effectiveness and robustness of the proposed optimization
framework.

</details>


### [590] [Depth-Optimal Quantum Layout Synthesis as SAT](https://arxiv.org/abs/2506.06752)
*Anna B. Jakobsen,Anders B. Clausen,Jaco van de Pol,Irfansha Shaik*

Main category: quant-ph

TL;DR: 提出量子电路布局合成的新SAT编码，可保证找到最小电路深度或最小CX门深度的映射电路，比OLSQ2提速10 - 100倍，还研究了CX计数和深度减少与降噪的相关性。


<details>
  <summary>Details</summary>
Motivation: 当前量子硬件平台对二进制CX门有连接限制，且CX门有噪声，需减少映射电路的CX计数或CX深度。

Method: 采用新的SAT编码，使用增量SAT求解和并行计划进行高效编码。

Result: 比保证深度最优的OLSQ2提速超10 - 100倍；最小化CX计数比最小化CX深度更能降噪，同时考虑两者降噪效果最佳。

Conclusion: 新的SAT编码在速度上有优势，且综合考虑CX计数和深度对降噪有益。

Abstract: Quantum circuits consist of gates applied to qubits. Current quantum hardware
platforms impose connectivity restrictions on binary CX gates. Hence, Layout
Synthesis is an important step to transpile quantum circuits before they can be
executed. Since CX gates are noisy, it is important to reduce the CX count or
CX depth of the mapped circuits.
  We provide a new and efficient encoding of Quantum-circuit Layout Synthesis
in SAT. Previous SAT encodings focused on gate count and CX-gate count. Our
encoding instead guarantees that we find mapped circuits with minimal circuit
depth or minimal CX-gate depth. We use incremental SAT solving and parallel
plans for an efficient encoding. This results in speedups of more than 10-100x
compared to OLSQ2, which guarantees depth-optimality. But minimizing depth
still takes more time than minimizing gate count with Q-Synth.
  We correlate the noise reduction achieved by simulating circuits after
(CX)-count and (CX)-depth reduction. We find that minimizing for CX-count
correlates better with reducing noise than minimizing for CX-depth. However,
taking into account both CX-count and CX-depth provides the best noise
reduction.

</details>


### [591] [A weighted quantum ensemble of homogeneous quantum classifiers](https://arxiv.org/abs/2506.07810)
*Emiliano Tolotti,Enrico Blanzieri,Davide Pastorello*

Main category: quant-ph

TL;DR: 本文提出一种加权同质量子集成方法，通过量子分类器实现，经实证评估证明有效。


<details>
  <summary>Details</summary>
Motivation: 机器学习集成方法旨在结合多个模型提高预测准确性，需保证预测器多样性，本文欲提出量子集成方法。

Method: 提出使用带索引寄存器的量子分类器实现加权同质量子集成的方法，利用基于实例的量子分类器，通过叠加和受控酉运算进行特征和训练点子采样，集成电路执行和经典权重优化的学习过程。

Result: 实证评估证明了所提方法的有效性。

Conclusion: 所提加权同质量子集成方法有效，为其性能提供了见解。

Abstract: Ensemble methods in machine learning aim to improve prediction accuracy by
combining multiple models. This is achieved by ensuring diversity among
predictors to capture different data aspects. Homogeneous ensembles use
identical models, achieving diversity through different data subsets, and
weighted-average ensembles assign higher influence to more accurate models
through a weight learning procedure. We propose a method to achieve a weighted
homogeneous quantum ensemble using quantum classifiers with indexing registers
for data encoding. This approach leverages instance-based quantum classifiers,
enabling feature and training point subsampling through superposition and
controlled unitaries, and allowing for a quantum-parallel execution of diverse
internal classifiers with different data compositions in superposition. The
method integrates a learning process involving circuit execution and classical
weight optimization, for a trained ensemble execution with weights encoded in
the circuit at test-time. Empirical evaluation demonstrate the effectiveness of
the proposed method, offering insights into its performance.

</details>


### [592] [Deep reinforcement learning for near-deterministic preparation of cubic- and quartic-phase gates in photonic quantum computing](https://arxiv.org/abs/2506.07859)
*Amanuel Anteneh Léandre Brunel,Carlos González-Arciniegas,Olivier Pfister*

Main category: quant-ph

TL;DR: 用强化学习训练深度神经网络控制量子光电路生成立方相位态，成功率96%，相同资源可直接生成四次相位门。


<details>
  <summary>Details</summary>
Motivation: 研究如何生成用于连续变量通用量子计算的立方相位态。

Method: 通过强化学习训练深度神经网络控制量子光电路。

Result: 生成立方相位态平均成功率达96%，相同资源可直接生成四次相位门。

Conclusion: 立方相位态是连续变量通用量子计算的充足资源，特定资源可有效生成相关量子态和门。

Abstract: Cubic-phase states are a sufficient resource for universal quantum computing
over continuous variables. We present results from numerical experiments in
which deep neural networks are trained via reinforcement learning to control a
quantum optical circuit for generating cubic-phase states, with an average
success rate of 96%. The only non-Gaussian resource required is
photon-number-resolving measurements. We also show that the exact same
resources enable the direct generation of a quartic-phase gate, with no need
for a cubic gate decomposition.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [593] [ChemGraph: An Agentic Framework for Computational Chemistry Workflows](https://arxiv.org/abs/2506.06363)
*Thang D. Pham,Aditya Tanikanti,Murat Keçeli*

Main category: physics.chem-ph

TL;DR: 提出AI驱动的ChemGraph框架简化计算化学和材料科学工作流，评估不同LLM在不同任务的表现，展示多智能体框架分解任务优势。


<details>
  <summary>Details</summary>
Motivation: 原子模拟在化学和材料科学中重要，但运行模拟面临计算方法多样、软件生态复杂等挑战。

Method: 构建基于AI和先进模拟工具的ChemGraph框架，利用图神经网络基础模型计算、大语言模型进行自然语言理解等。

Result: 在13个基准任务评估中，小LLM在简单工作流表现好，复杂任务大模型更好，多智能体框架分解任务可使小模型在特定场景超GPT - 4o。

Conclusion: ChemGraph框架能简化和自动化计算化学与材料科学工作流，多智能体框架分解任务有优势。

Abstract: Atomistic simulations are essential tools in chemistry and materials science,
accelerating the discovery of novel catalysts, energy storage materials, and
pharmaceuticals. However, running these simulations remains challenging due to
the wide range of computational methods, diverse software ecosystems, and the
need for expert knowledge and manual effort for the setup, execution, and
validation stages. In this work, we present ChemGraph, an agentic framework
powered by artificial intelligence and state-of-the-art simulation tools to
streamline and automate computational chemistry and materials science
workflows. ChemGraph leverages graph neural network-based foundation models for
accurate yet computationally efficient calculations and large language models
(LLMs) for natural language understanding, task planning, and scientific
reasoning to provide an intuitive and interactive interface. Users can perform
tasks such as molecular structure generation, single-point energy, geometry
optimization, vibrational analysis, and thermochemistry calculations with
methods ranging from tight-binding and machine learning interatomic potentials
to density functional theory or wave function theory-based methods. We evaluate
ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs
(GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows,
while more complex tasks benefit from using larger models like GPT-4o.
Importantly, we show that decomposing complex tasks into smaller subtasks
through a multi-agent framework enables smaller LLM models to match or exceed
GPT-4o's performance in specific scenarios.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [594] [CNFs and DNFs with Exactly $k$ Solutions](https://arxiv.org/abs/2506.07268)
*L. Sunil Chandran,Rishikesh Gajjala,Kuldeep S. Meel*

Main category: cs.DM

TL;DR: 本文研究构造具有k个满足赋值的DNF或CNF公式所需的最少项（或子句）数量，给出了上下界。


<details>
  <summary>Details</summary>
Motivation: 加权模型计数常转化为无加权模型计数，引出构造具有k个满足赋值的DNF或CNF公式所需最少项数的问题。

Method: 理论证明得出对于任意自然数k，可构造项数最多为O(√(log k)loglog k)的单调DNF公式；还证明存在无穷多个k值，使任何DNF或CNF表示至少需要Ω(loglog k)项或子句。

Result: 得到构造具有k个满足赋值的DNF公式项数的上界为O(√(log k)loglog k)，存在无穷多个k使DNF或CNF表示的项数下界为Ω(loglog k)。

Conclusion: 研究结果对基于公式变换的模型计数算法效率有重要意义。

Abstract: Model counting is a fundamental problem that consists of determining the
number of satisfying assignments for a given Boolean formula. The weighted
variant, which computes the weighted sum of satisfying assignments, has
extensive applications in probabilistic reasoning, network reliability,
statistical physics, and formal verification. A common approach for solving
weighted model counting is to reduce it to unweighted model counting, which
raises an important question: {\em What is the minimum number of terms (or
clauses) required to construct a DNF (or CNF) formula with exactly $k$
satisfying assignments?}
  In this paper, we establish both upper and lower bounds on this question. We
prove that for any natural number $k$, one can construct a monotone DNF formula
with exactly $k$ satisfying assignments using at most $O(\sqrt{\log k}\log\log
k)$ terms. This construction represents the first $o(\log k)$ upper bound for
this problem. We complement this result by showing that there exist infinitely
many values of $k$ for which any DNF or CNF representation requires at least
$\Omega(\log\log k)$ terms or clauses. These results have significant
implications for the efficiency of model counting algorithms based on formula
transformations.

</details>


### [595] [HyColor: An Efficient Heuristic Algorithm for Graph Coloring](https://arxiv.org/abs/2506.07373)
*Enqiang Zhu,Yu Zhang,Haopeng Sun,Ziqi Wei,Witold Pedrycz,Chanjuan Liu,Jin Xu*

Main category: cs.DM

TL;DR: 本文提出解决图着色问题的高效混合启发式算法HyColor，在多基准测试中表现优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有图着色问题算法主要关注小的难图或大规模稀疏图，缺乏能兼顾大规模稀疏图和小的稠密图的算法。

Method: 提出HyColor算法，通过局部决策策略提升色数下界、图约简策略缩小工作图、基于k - 核和混合度的贪心启发式方法高效着色。

Result: 在209个实例的四个基准测试中，HyColor在多数实例的解精度和计算效率上超过现有启发式算法，在194个实例中获最佳解，128个实例确定色数并实现最优着色。

Conclusion: HyColor算法在解决图着色问题上具有良好效果，能有效处理大规模稀疏图和小的稠密图。

Abstract: The graph coloring problem (GCP) is a classic combinatorial optimization
problem that aims to find the minimum number of colors assigned to vertices of
a graph such that no two adjacent vertices receive the same color. GCP has been
extensively studied by researchers from various fields, including mathematics,
computer science, and biological science. Due to the NP-hard nature, many
heuristic algorithms have been proposed to solve GCP. However, existing GCP
algorithms focus on either small hard graphs or large-scale sparse graphs (with
up to 10^7 vertices). This paper presents an efficient hybrid heuristic
algorithm for GCP, named HyColor, which excels in handling large-scale sparse
graphs while achieving impressive results on small dense graphs. The efficiency
of HyColor comes from the following three aspects: a local decision strategy to
improve the lower bound on the chromatic number; a graph-reduction strategy to
reduce the working graph; and a k-core and mixed degree-based greedy heuristic
for efficiently coloring graphs. HyColor is evaluated against three
state-of-the-art GCP algorithms across four benchmarks, comprising three
large-scale sparse graph benchmarks and one small dense graph benchmark,
totaling 209 instances. The results demonstrate that HyColor consistently
outperforms existing heuristic algorithms in both solution accuracy and
computational efficiency for the majority of instances. Notably, HyColor
achieved the best solutions in 194 instances (over 93%), with 34 of these
solutions significantly surpassing those of other algorithms. Furthermore,
HyColor successfully determined the chromatic number and achieved optimal
coloring in 128 instances.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [596] [Optimal Transport Driven Asymmetric Image-to-Image Translation for Nuclei Segmentation of Histological Images](https://arxiv.org/abs/2506.07023)
*Suman Mahapatra,Pradipta Maji*

Main category: eess.IV

TL;DR: 本文提出新的深度生成模型用于组织学图像的细胞核分割，考虑嵌入空间处理信息差异，结合最优传输和测度论开发可逆生成器，还引入空间约束挤压操作，在公开数据集验证性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像到图像翻译模型在处理信息不对称的图像域时效果差，需开发适用于不同目标域表示的细胞核分割算法。

Method: 引入新的深度生成模型，考虑嵌入空间，结合最优传输和测度论开发可逆生成器，在可逆生成器框架内引入空间约束挤压操作。

Result: 该模型在公开组织学图像数据集上验证性能，相比其他复杂网络架构的模型，在网络复杂度和模型性能间有更好权衡。

Conclusion: 提出的深度生成模型在细胞核分割任务中表现良好，能有效处理信息差异问题。

Abstract: Segmentation of nuclei regions from histological images enables morphometric
analysis of nuclei structures, which in turn helps in the detection and
diagnosis of diseases under consideration. To develop a nuclei segmentation
algorithm, applicable to different types of target domain representations,
image-to-image translation networks can be considered as they are invariant to
target domain image representations. One of the important issues with
image-to-image translation models is that they fail miserably when the
information content between two image domains are asymmetric in nature. In this
regard, the paper introduces a new deep generative model for segmenting nuclei
structures from histological images. The proposed model considers an embedding
space for handling information-disparity between information-rich histological
image space and information-poor segmentation map domain. Integrating
judiciously the concepts of optimal transport and measure theory, the model
develops an invertible generator, which provides an efficient optimization
framework with lower network complexity. The concept of invertible generator
automatically eliminates the need of any explicit cycle-consistency loss. The
proposed model also introduces a spatially-constrained squeeze operation within
the framework of invertible generator to maintain spatial continuity within the
image patches. The model provides a better trade-off between network complexity
and model performance compared to other existing models having complex network
architectures. The performance of the proposed deep generative model, along
with a comparison with state-of-the-art nuclei segmentation methods, is
demonstrated on publicly available histological image data sets.

</details>


### [597] [SiliCoN: Simultaneous Nuclei Segmentation and Color Normalization of Histological Images](https://arxiv.org/abs/2506.07028)
*Suman Mahapatra,Pradipta Maji*

Main category: eess.IV

TL;DR: 提出新的深度生成模型，同时分割细胞核结构和归一化染色组织学图像颜色，在公开数据集验证。


<details>
  <summary>Details</summary>
Motivation: 解决染色组织图像颜色变化下细胞核区域分割任务，改善细胞核分割和颜色归一化效果。

Method: 提出集成截断正态分布和空间注意力优点的深度生成模型，假设潜在颜色外观信息与细胞核分割图和嵌入图信息独立，用截断正态分布混合处理染色重叠，引入空间注意力进行细胞核区域分割。

Result: 在公开标准组织学图像数据集上展示了模型性能，并与相关先进算法进行对比分析。

Conclusion: 所提模型具有可推广性和适应性，能有效解决染色组织图像的细胞核分割和颜色归一化问题。

Abstract: Segmentation of nuclei regions from histological images is an important task
for automated computer-aided analysis of histological images, particularly in
the presence of impermissible color variation in the color appearance of
stained tissue images. While color normalization enables better nuclei
segmentation, accurate segmentation of nuclei structures makes color
normalization rather trivial. In this respect, the paper proposes a novel deep
generative model for simultaneously segmenting nuclei structures and
normalizing color appearance of stained histological images.This model
judiciously integrates the merits of truncated normal distribution and spatial
attention. The model assumes that the latent color appearance information,
corresponding to a particular histological image, is independent of respective
nuclei segmentation map as well as embedding map information. The disentangled
representation makes the model generalizable and adaptable as the modification
or loss in color appearance information cannot be able to affect the nuclei
segmentation map as well as embedding information. Also, for dealing with the
stain overlap of associated histochemical reagents, the prior for latent color
appearance code is assumed to be a mixture of truncated normal distributions.
The proposed model incorporates the concept of spatial attention for
segmentation of nuclei regions from histological images. The performance of the
proposed approach, along with a comparative analysis with related
state-of-the-art algorithms, has been demonstrated on publicly available
standard histological image data sets.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [598] [El0ps: An Exact L0-regularized Problems Solver](https://arxiv.org/abs/2506.06373)
*Théo Guyard,Cédric Herzet,Clément Elvira*

Main category: cs.MS

TL;DR: 本文介绍Python工具包El0ps，可处理L0正则化问题，有灵活性、高性能求解器和内置机器学习管道。


<details>
  <summary>Details</summary>
Motivation: 为实际应用中L0正则化问题的集成提供全面工具，开辟新视角。

Method: 开发了El0ps工具包，提供灵活框架定义问题、专用求解器和内置机器学习管道。

Result: 创建了El0ps工具包。

Conclusion: El0ps是可用于实际应用中处理L0正则化问题的综合工具。

Abstract: This paper presents El0ps, a Python toolbox providing several utilities to
handle L0-regularized problems related to applications in machine learning,
statistics, and signal processing, among other fields. In contrast to existing
toolboxes, El0ps allows users to define custom instances of these problems
through a flexible framework, provides a dedicated solver achieving
state-of-the-art performance, and offers several built-in machine learning
pipelines. Our aim with El0ps is to provide a comprehensive tool which opens
new perspectives for the integration of L0-regularized problems in practical
applications.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [599] [Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor](https://arxiv.org/abs/2506.06773)
*Emet Behrendt,Shing Wai Pun,Prashant J. Nair*

Main category: cs.AR

TL;DR: 为解决TAGE - SC - L中难预测分支误预测问题，添加Bullseye预测子系统，并行工作提升准确率。


<details>
  <summary>Details</summary>
Motivation: TAGE - SC - L中难预测分支导致大量误预测，简单扩大表改进有限。

Method: 添加28KB的Bullseye预测子系统，用HIT识别问题PC，引导至两个感知器，短试验阶段跟踪准确率，超过阈值后抑制TAGE更新。

Result: 实现平均MPKI为3.4045和CycWpPKI为145.09。

Conclusion: Bullseye预测子系统与TAGE - SC - L并行工作，能提高难预测分支的预测准确率。

Abstract: Branch prediction is key to the performance of out-of-order processors. While
the CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical
corrector, and a loop predictor, over half of its remaining mispredictions stem
from a small set of hard-to-predict (H2P) branches. These branches occur under
diverse global histories, causing repeated thrashing in TAGE and eviction
before usefulness counters can mature. Prior work shows that simply enlarging
the tables offers only marginal improvement.
  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem
called the Bullseye predictor. It identifies problematic PCs using a
set-associative H2P Identification Table (HIT) and steers them to one of two
branch-specific perceptrons, one indexed by hashed local history and the other
by folded global history. A short trial phase tracks head-to-head accuracy in
an H2P cache. A branch becomes perceptron-resident only if the perceptron's
sustained accuracy and output magnitude exceed dynamic thresholds, after which
TAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,
and perceptron operate fully in parallel with TAGE-SC-L, providing higher
fidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI
of 145.09.

</details>


### [600] [ASPO: Constraint-Aware Bayesian Optimization for FPGA-based Soft Processors](https://arxiv.org/abs/2506.06817)
*Haoran Wu,Ce Guo,Wayne Luk,Robert Mullins*

Main category: cs.AR

TL;DR: 本文提出ASPO方法使贝叶斯优化（BO）能处理含分类参数的约束，加速FPGA软处理器设计评估，实验表明能减少执行和设计时间。


<details>
  <summary>Details</summary>
Motivation: 标准BO不支持含分类参数的约束，且优化时间随处理器复杂度增长，在FPGA软处理器中问题更突出。

Method: 利用析取形式使BO处理含分类参数的约束，用定制协方差核支持分类参数，通过惩罚采集函数和复用FPGA综合检查点加速设计评估。

Result: 在三个软处理器和七个RISC - V基准测试中，ASPO使BOOM处理器‘multiply’基准执行时间最多减少35%，设计时间比Boomerang最多减少74%。

Conclusion: ASPO方法有效解决了FPGA软处理器设计中BO面临的问题，能减少执行和设计时间。

Abstract: Bayesian Optimization (BO) has shown promise in tuning processor design
parameters. However, standard BO does not support constraints involving
categorical parameters such as types of branch predictors and division
circuits. In addition, optimization time of BO grows with processor complexity,
which becomes increasingly significant especially for FPGA-based soft
processors. This paper introduces ASPO, an approach that leverages disjunctive
form to enable BO to handle constraints involving categorical parameters.
Unlike existing methods that directly apply standard BO, the proposed ASPO
method, for the first time, customizes the mathematical mechanism of BO to
address challenges faced by soft-processor designs on FPGAs. Specifically, ASPO
supports categorical parameters using a novel customized BO covariance kernel.
It also accelerates the design evaluation procedure by penalizing the BO
acquisition function with potential evaluation time and by reusing FPGA
synthesis checkpoints from previously evaluated configurations. ASPO targets
three soft processors: RocketChip, BOOM, and EL2 VeeR. The approach is
evaluated based on seven RISC-V benchmarks. Results show that ASPO can reduce
execution time for the ``multiply'' benchmark on the BOOM processor by up to
35\% compared to the default configuration. Furthermore, it reduces design time
for the BOOM processor by up to 74\% compared to Boomerang, a state-of-the-art
hardware-oriented BO approach.

</details>


### [601] [Design and Implementation of a RISC-V SoC with Custom DSP Accelerators for Edge Computing](https://arxiv.org/abs/2506.06693)
*Priyanshu Yadav*

Main category: cs.AR

TL;DR: 本文对RISC - V指令集架构进行综合分析，评估性能，展示其在嵌入式系统优势及可扩展性，对比显示功耗降低。


<details>
  <summary>Details</summary>
Motivation: 对RISC - V指令集架构进行全面了解，评估其在性能等方面的表现。

Method: 对RV32I基础指令集及扩展进行研究，通过周期精确模拟评估性能指标。

Result: RISC - V在嵌入式系统有优势，可用于定制加速器，与ARM Cortex - M0相比功耗降低17%。

Conclusion: RISC - V的开放标准特性为特定领域优化提供显著灵活性。

Abstract: This paper presents a comprehensive analysis of the RISC-V instruction set
architecture, focusing on its modular design, implementation challenges, and
performance characteristics. We examine the RV32I base instruction set with
extensions for multiplication (M) and atomic operations (A). Through
cycle-accurate simulation of a pipelined implementation, we evaluate
performance metrics including CPI (cycles per instruction) and power
efficiency. Our results demonstrate RISC-V's advantages in embedded systems and
its scalability for custom accelerators. Comparative analysis shows a 17%
reduction in power consumption compared to ARM Cortex-M0 implementations in
similar process nodes. The open-standard nature of RISC-V provides significant
flexibility for domain-specific optimizations.

</details>


### [602] [MAGNet: A Multi-Scale Attention-Guided Graph Fusion Network for DRC Violation Detection](https://arxiv.org/abs/2506.07126)
*Weihan Lu,Hong Cai Chen*

Main category: cs.AR

TL;DR: 本文提出混合深度学习模型MAGNet用于DRC违规预测，结合多种模块与策略，在DRC热点检测中表现优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 设计规则检查（DRC）对集成电路设计意义重大，基于机器学习的DRC是重要方法，旨在提升DRC预测准确性和降低误报率。

Method: 提出MAGNet，结合改进U - Net与图神经网络；U - Net用DAM和MSCM增强；构建像素对齐图结构并应用GNN；采用标签放大策略训练；进行增量训练。

Result: MAGNet有效结合多种信息，在DRC热点检测中提高预测准确性、降低误报率；增量训练提升热点判别能力；相比ibUnet、RouteNet和J - Net，整体性能大幅提升。

Conclusion: MAGNet在DRC违规预测中具有良好效果，能显著提升整体性能。

Abstract: Design rule checking (DRC) is of great significance for cost reduction and
design efficiency improvement in integrated circuit (IC) designs.
Machine-learning-based DRC has become an important approach in computer-aided
design (CAD). In this paper, we propose MAGNet, a hybrid deep learning model
that integrates an improved U-Net with a graph neural network for DRC violation
prediction. The U-Net backbone is enhanced with a Dynamic Attention Module
(DAM) and a Multi-Scale Convolution Module (MSCM) to strengthen its capability
in extracting fine-grained and multi-scale spatial features. In parallel, we
construct a pixel-aligned graph structure based on chip layout tiles, and apply
a specialized GNN to model the topological relationships among pins. During
graph construction, a graph-to-grid mapping is generated to align GNN features
with the layout image. In addition, a label amplification strategy is adopted
during training to enhance the model's sensitivity to sparse violation
patterns. Overall, MAGNet effectively combines spatial, semantic, and
structural information, achieving improved prediction accuracy and reduced
false positive rates in DRC hotspot detection. Subsequently, through
incremental training, we achieve a more sensitive discrimination ability for
hotspots. The results demonstrate that, in comparison with ibUnet, RouteNet,
and J-Net, MAGnet significantly outperforms these models, achieving substantial
improvements in overall performance.

</details>


### [603] [VeriLoC: Line-of-Code Level Prediction of Hardware Design Quality from Verilog Code](https://arxiv.org/abs/2506.07239)
*Raghu Vamshi Hemadri,Jitendra Bhandari,Johann Knechtel,Badri P Gopalan,Ramesh Narayanaswamy,Ramesh Karri,Siddharth Garg*

Main category: cs.AR

TL;DR: 提出VeriLoC方法，可从Verilog代码直接进行行级和模块级设计质量预测，效果良好。


<details>
  <summary>Details</summary>
Motivation: 现代芯片设计复杂，需从Verilog代码早期预测关键设计质量指标，尤其是行级预测，此前工作未考虑行级。

Method: 利用Verilog代码生成大语言模型提取行级和模块级嵌入，在这些嵌入的拼接上训练下游分类器/回归器。

Result: 行级拥塞和时序预测F1分数达0.86 - 0.95，将SOTA方法的平均百分比误差从14% - 18%降至4%。

Conclusion: VeriLoC嵌入和研究见解对复杂硬件设计的其他预测和优化任务有价值。

Abstract: Modern chip design is complex, and there is a crucial need for early-stage
prediction of key design-quality metrics like timing and routing congestion
directly from Verilog code (a commonly used programming language for hardware
design). It is especially important yet complex to predict individual lines of
code that cause timing violations or downstream routing congestion. Prior works
have tried approaches like converting Verilog into an intermediate graph
representation and using LLM embeddings alongside other features to predict
module-level quality, but did not consider line-level quality prediction. We
propose VeriLoC, the first method that predicts design quality directly from
Verilog at both the line- and module-level. To this end, VeriLoC leverages
recent Verilog code-generation LLMs to extract local line-level and
module-level embeddings, and train downstream classifiers/regressors on
concatenations of these embeddings. VeriLoC achieves high F1-scores of
0.86-0.95 for line-level congestion and timing prediction, and reduces the mean
average percentage error from 14% - 18% for SOTA methods down to only 4%. We
believe that VeriLoC embeddings and insights from our work will also be of
value for other predictive and optimization tasks for complex hardware design.

</details>


### [604] [ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols](https://arxiv.org/abs/2506.07945)
*Arnav Sheth,Ivaxi Sheth,Mario Fritz*

Main category: cs.AR

TL;DR: 本文分析了大语言模型生成SystemVerilog实现标准通信协议的能力，引入首个针对四种协议的基准套件并评估生成设计。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在硬件描述语言尤其是生成可综合且功能正确设计方面研究不足，本文旨在分析其生成SystemVerilog实现标准通信协议的能力。

Method: 引入针对SPI、I2C、UART和AXI四种协议的基准套件，定义不同设计抽象和提示特异性的代码生成任务，通过波形仿真和测试台评估生成设计。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Recent advances in Large Language Models (LLMs) have shown promising
capabilities in generating code for general-purpose programming languages. In
contrast, their applicability for hardware description languages, particularly
for generating synthesizable and functionally correct designs, remains
significantly underexplored. HDLs such as SystemVerilog are logic-oriented and
demand strict adherence to timing semantics, concurrency, and synthesizability
constraints. Moreover, HDL-based design flows encompass a broad set of tasks
beyond structural code generation, including testbench development,
assertion-based verification, timing closure, and protocol-level integration
for on-chip communication. The objective of our paper is to analyze the
capabilities of state-of-the-art LLMs in generating SystemVerilog
implementations of standard communication protocols, a core component of
embedded and System-on-Chip (SoC) architectures. This paper introduces the
first benchmark suite targeting four widely used protocols: SPI, I2C, UART, and
AXI. We define code generation tasks that capture varying levels of design
abstraction and prompt specificity. The generated designs are assessed for
syntactic correctness, synthesizability, and functional fidelity via waveform
simulation and test benches.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [605] [Inverse Design of Metamaterials with Manufacturing-Guiding Spectrum-to-Structure Conditional Diffusion Model](https://arxiv.org/abs/2506.07083)
*Jiawen Li,Jiang Guo,Yuanzhe Li,Zetian Mao,Jiaxing Shen,Tashi Xu,Diptesh Das,Jinming He,Run Hu,Yaerim Lee,Koji Tsuda,Junichiro Shiomi*

Main category: physics.optics

TL;DR: 本文提出用条件扩散模型解决超材料逆设计问题的通用框架，在光谱预测和图案生成上表现出色，并成功应用于热伪装超材料设计与制造。


<details>
  <summary>Details</summary>
Motivation: 超材料结构与光学行为的高度非线性关系及制造困难，给机器学习用于复杂超材料设计和制造带来挑战。

Method: 提出通用框架，利用条件扩散模型实现定制化光谱到形状和尺寸参数的转换，解决一对多超材料逆设计问题。

Result: 该方法光谱预测准确性高，生成图案多样，为制造提供先验知识，成功设计并制造用于热伪装的自由形式超材料。

Conclusion: 所提方法有效，能促进超材料设计的实验制造。

Abstract: Metamaterials are artificially engineered structures that manipulate
electromagnetic waves, having optical properties absent in natural materials.
Recently, machine learning for the inverse design of metamaterials has drawn
attention. However, the highly nonlinear relationship between the metamaterial
structures and optical behaviour, coupled with fabrication difficulties, poses
challenges for using machine learning to design and manufacture complex
metamaterials. Herein, we propose a general framework that implements
customised spectrum-to-shape and size parameters to address one-to-many
metamaterial inverse design problems using conditional diffusion models. Our
method exhibits superior spectral prediction accuracy, generates a diverse
range of patterns compared to other typical generative models, and offers
valuable prior knowledge for manufacturing through the subsequent analysis of
the diverse generated results, thereby facilitating the experimental
fabrication of metamaterial designs. We demonstrate the efficacy of the
proposed method by successfully designing and fabricating a free-form
metamaterial with a tailored selective emission spectrum for thermal camouflage
applications.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [606] [Human Side of Smart Contract Fuzzing: An Empirical Study](https://arxiv.org/abs/2506.07389)
*Guanming Qiao,Partha Protim Paul*

Main category: cs.HC

TL;DR: 研究通过分析GitHub问题和用户研究，调查智能合约模糊测试工具采用挑战并分类，为工具改进提供见解。


<details>
  <summary>Details</summary>
Motivation: 智能合约模糊测试对区块链应用检测漏洞重要，但因与传统软件差异，从业者采用有挑战，需研究挑战及应对策略。

Method: 对两个常用智能合约模糊测试工具的381个GitHub问题进行归纳内容分析，开展用户研究。

Result: 发现特定领域易用性和有用性挑战，如区块链仿真技术问题、文档和流程自动化不足等。

Conclusion: 研究结果为工具开发者和研究者提供可操作见解，指导未来智能合约模糊测试工具设计改进。

Abstract: Smart contract (SC) fuzzing is a critical technique for detecting
vulnerabilities in blockchain applications. However, its adoption remains
challenging for practitioners due to fundamental differences between SCs and
traditional software systems. In this study, we investigate the challenges
practitioners face when adopting SC fuzzing tools by conducting an inductive
content analysis of 381 GitHub issues from two widely used SC fuzzers: Echidna
and Foundry. Furthermore, we conducted a user study to examine how these
challenges affect different practitioner groups, SC developers, and traditional
software security professionals, and identify strategies practitioners use to
overcome them. We systematically categorize these challenges into a taxonomy
based on their nature and occurrence within the SC fuzzing workflow. Our
findings reveal domain-specific ease-of-use and usefulness challenges,
including technical issues with blockchain emulation, and human issues with a
lack of accessible documentation and process automation. Our results provide
actionable insights for tool developers and researchers, guiding future
improvements in SC fuzzer tool design.

</details>


### [607] [LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational Dependencies on Large Language Models](https://arxiv.org/abs/2506.06874)
*Ala Yankouskaya,Areej B. Babiker,Syeda W. F. Rizvi,Sameha Alshakhsi,Magnus Liebherr,Raian Ali*

Main category: cs.HC

TL;DR: 研究开发并验证了测量大语言模型依赖的12项问卷LLM - D12，有良好信效度


<details>
  <summary>Details</summary>
Motivation: 现有评估大语言模型依赖工具稀缺且概念有局限，需新视角

Method: 基于作者先前理论工作开发问卷，收集526名英国参与者数据，用分半样本法做探索性和验证性因子分析

Result: 支持双因子结构，即工具性依赖和关系性依赖，量表有良好内部一致性和区分效度

Conclusion: LLM - D12量表可反映依赖水平，依赖不一定意味着功能失调，但特定情境下可能有问题

Abstract: There is growing interest in understanding how people interact with large
language models (LLMs) and whether such models elicit dependency or even
addictive behaviour. Validated tools to assess the extent to which individuals
may become dependent on LLMs are scarce and primarily build on classic
behavioral addiction symptoms, adapted to the context of LLM use. We view this
as a conceptual limitation, as the LLM-human relationship is more nuanced and
warrants a fresh and distinct perspective. To address this gap, we developed
and validated a new 12-item questionnaire to measure LLM dependency, referred
to as LLM-D12. The scale was based on the authors' prior theoretical work, with
items developed accordingly and responses collected from 526 participants in
the UK. Exploratory and confirmatory factor analyses, performed on separate
halves of the total sample using a split-sample approach, supported a
two-factor structure: Instrumental Dependency (six items) and Relationship
Dependency (six items). Instrumental Dependency reflects the extent to which
individuals rely on LLMs to support or collaborate in decision-making and
cognitive tasks. Relationship Dependency captures the tendency to perceive LLMs
as socially meaningful, sentient, or companion-like entities. The two-factor
structure demonstrated excellent internal consistency and clear discriminant
validity. External validation confirmed both the conceptual foundation and the
distinction between the two subscales. The psychometric properties and
structure of our LLM-D12 scale were interpreted in light of the emerging view
that dependency on LLMs does not necessarily indicate dysfunction but may still
reflect reliance levels that could become problematic in certain contexts.

</details>


### [608] [Sword and Shield: Uses and Strategies of LLMs in Navigating Disinformation](https://arxiv.org/abs/2506.07211)
*Gionnieve Lim,Bryan Chen Zhengyu Tan,Kellie Yu Hui Sim,Weiyan Shi,Ming Hui Chew,Ming Shan Hee,Roy Ka-Wei Lee,Simon T. Perrault,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: 本文通过模拟在线论坛的交流游戏研究大语言模型（LLMs）与虚假信息的复杂关系，揭示其潜在滥用和对抗虚假信息的情况，讨论了对未来LLM发展和平台设计的影响。


<details>
  <summary>Details</summary>
Motivation: LLMs既能被用于制造虚假信息，也能用于提升检测和缓解策略，需研究其与虚假信息的复杂动态关系。

Method: 通过受狼人杀启发的交流游戏模拟在线论坛，有25名参与者，分析不同角色利用LLMs达成目标的情况。

Result: 发现LLMs的使用因参与者角色和策略而异，强调在此背景下理解其有效性的重要性。

Conclusion: 讨论了对未来LLM发展和在线平台设计的影响，提倡平衡的方法，在减轻风险的同时增强用户能力和信任。

Abstract: The emergence of Large Language Models (LLMs) presents a dual challenge in
the fight against disinformation. These powerful tools, capable of generating
human-like text at scale, can be weaponised to produce sophisticated and
persuasive disinformation, yet they also hold promise for enhancing detection
and mitigation strategies. This paper investigates the complex dynamics between
LLMs and disinformation through a communication game that simulates online
forums, inspired by the game Werewolf, with 25 participants. We analyse how
Disinformers, Moderators, and Users leverage LLMs to advance their goals,
revealing both the potential for misuse and combating disinformation. Our
findings highlight the varying uses of LLMs depending on the participants'
roles and strategies, underscoring the importance of understanding their
effectiveness in this context. We conclude by discussing implications for
future LLM development and online platform design, advocating for a balanced
approach that empowers users and fosters trust while mitigating the risks of
LLM-assisted disinformation.

</details>


### [609] [Secondary Stakeholders in AI: Fighting for, Brokering, and Navigating Agency](https://arxiv.org/abs/2506.07281)
*Leah Hope Ajmani,Nuredin Ali Abdelkadir,Stevie Chancellor*

Main category: cs.HC

TL;DR: 本文通过半结构化访谈将参与式AI理念推广到二级利益相关者，提出参与的三个理想状态，介绍利益相关者原型，展望二级利益相关者能有意义参与AI系统的未来。


<details>
  <summary>Details</summary>
Motivation: 现有参与式AI呼吁多关注一级利益相关者，本文旨在将其理念推广到更广泛的二级AI利益相关者群体。

Method: 采用半结构化访谈方法。

Result: 提出有意义参与涉及知情、同意和能动性三个参与式理想状态，介绍了不情愿的数据贡献者、无支持的活动家和善意的从业者三种利益相关者原型。

Conclusion: 展望二级利益相关者能在其影响和受影响的AI系统中进行有意义参与的未来。

Abstract: As AI technologies become more human-facing, there have been numerous calls
to adapt participatory approaches to AI development -- spurring the idea of
participatory AI. However, these calls often focus only on primary
stakeholders, such as end-users, and not secondary stakeholders. This paper
seeks to translate the ideals of participatory AI to a broader population of
secondary AI stakeholders through semi-structured interviews. We theorize that
meaningful participation involves three participatory ideals: (1) informedness,
(2) consent, and (3) agency. We also explore how secondary stakeholders realize
these ideals by traversing a complicated problem space. Like walking up the
rungs of a ladder, these ideals build on one another. We introduce three
stakeholder archetypes: the reluctant data contributor, the unsupported
activist, and the well-intentioned practitioner, who must navigate systemic
barriers to achieving agentic AI relationships. We envision an AI future where
secondary stakeholders are able to meaningfully participate with the AI systems
they influence and are influenced by.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [610] [The Economic Dispatch of Power-to-Gas Systems with Deep Reinforcement Learning:Tackling the Challenge of Delayed Rewards with Long-Term Energy Storage](https://arxiv.org/abs/2506.06484)
*Manuel Sage,Khalil Al Handawi,Yaoyao Fiona Zhao*

Main category: eess.SY

TL;DR: 本文研究将DRL用于P2G系统与BES、燃气轮机组合的长期经济运行，通过案例研究改进DRL算法，调整后可显著提升制定成本效益策略的能力。


<details>
  <summary>Details</summary>
Motivation: P2G系统经济运行决策复杂，DRL应用于P2G有延迟奖励难题，且以往研究多为短期忽略长期存储能力。

Method: 通过三个复杂度递增的案例研究，评估DRL算法，对其进行包括整合预测、在奖励函数中实施惩罚、应用战略成本计算等改进。

Result: DRL初始应对P2G系统复杂决策有困难，提出的调整显著提升其制定成本效益策略的能力。

Conclusion: 提出的调整能解锁P2G技术长期储能潜力。

Abstract: Power-to-Gas (P2G) technologies gain recognition for enabling the integration
of intermittent renewables, such as wind and solar, into electricity grids.
However, determining the most cost-effective operation of these systems is
complex due to the volatile nature of renewable energy, electricity prices, and
loads. Additionally, P2G systems are less efficient in converting and storing
energy compared to battery energy storage systems (BESs), and the benefits of
converting electricity into gas are not immediately apparent. Deep
Reinforcement Learning (DRL) has shown promise in managing the operation of
energy systems amidst these uncertainties. Yet, DRL techniques face
difficulties with the delayed reward characteristic of P2G system operation.
Previous research has mostly focused on short-term studies that look at the
energy conversion process, neglecting the long-term storage capabilities of
P2G.
  This study presents a new method by thoroughly examining how DRL can be
applied to the economic operation of P2G systems, in combination with BESs and
gas turbines, over extended periods. Through three progressively more complex
case studies, we assess the performance of DRL algorithms, specifically Deep
Q-Networks and Proximal Policy Optimization, and introduce modifications to
enhance their effectiveness. These modifications include integrating forecasts,
implementing penalties on the reward function, and applying strategic cost
calculations, all aimed at addressing the issue of delayed rewards. Our
findings indicate that while DRL initially struggles with the complex
decision-making required for P2G system operation, the adjustments we propose
significantly improve its capability to devise cost-effective operation
strategies, thereby unlocking the potential for long-term energy storage in P2G
technologies.

</details>


### [611] [From Model-Based and Adaptive Control to Evolving Fuzzy Control](https://arxiv.org/abs/2506.06594)
*Daniel Leite,Igor Škrjanc,Fernando Gomide*

Main category: eess.SY

TL;DR: 本文借模糊集理论60周年契机，回顾经典模糊及自适应建模与控制框架发展，强调进化智能系统在模糊建模与控制中的出现及优势，讨论了关键挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 在模糊集理论60周年时，回顾经典模糊及自适应建模与控制框架的发展。

Method: 回顾历史发展和核心贡献，强调新兴的进化智能系统及其优势。

Result: 明确了进化智能系统在模糊建模和控制中处理非平稳环境的优势。

Conclusion: 指出该领域存在安全、可解释性和原则性结构进化等关键挑战，并探讨了未来方向。

Abstract: Evolving fuzzy systems build and adapt fuzzy models - such as predictors and
controllers - by incrementally updating their rule-base structure from data
streams. On the occasion of the 60-year anniversary of fuzzy set theory,
commemorated during the Fuzz-IEEE 2025 event, this brief paper revisits the
historical development and core contributions of classical fuzzy and adaptive
modeling and control frameworks. It then highlights the emergence and
significance of evolving intelligent systems in fuzzy modeling and control,
emphasizing their advantages in handling nonstationary environments. Key
challenges and future directions are discussed, including safety,
interpretability, and principled structural evolution.

</details>


### [612] [On the Generalization of Data-Assisted Control in port-Hamiltonian Systems (DAC-pH)](https://arxiv.org/abs/2506.07079)
*Mostafa Eslami,Maryam Babazadeh*

Main category: eess.SY

TL;DR: 本文提出针对端口 - 哈密顿系统的混合控制框架，用数据辅助控制动态分解，结合非线性控制与强化学习，有诸多优势，还进行稳定性分析及实例模拟。


<details>
  <summary>Details</summary>
Motivation: 解决端口 - 哈密顿系统中参数和结构不确定性问题，提高系统控制性能。

Method: 采用数据辅助控制进行动态分解，将系统分为两部分，用非线性控制器管理哈密顿流，强化学习应用于耗散/输入流。

Result: 该混合方法有效管理不确定性，具有可调整性能、增强可解释性等优势，通过实例模拟展示了其实证和现象学益处。

Conclusion: 提出的混合控制框架为系统控制提供有效方法，为深入理论研究奠定基础。

Abstract: This paper introduces a hypothetical hybrid control framework for
port-Hamiltonian (p$\mathcal{H}$) systems, employing a dynamic decomposition
based on Data-Assisted Control (DAC). The system's evolution is split into two
parts with fixed topology: Right-Hand Side (RHS)- an intrinsic Hamiltonian flow
handling worst-case parametric uncertainties, and Left-Hand Side (LHS)- a
dissipative/input flow addressing both structural and parametric uncertainties.
A virtual port variable $\Pi$ serves as the interface between these two
components. A nonlinear controller manages the intrinsic Hamiltonian flow,
determining a desired port control value $\Pi_c$. Concurrently, Reinforcement
Learning (RL) is applied to the dissipative/input flow to learn an agent for
providing optimal policy in mapping $\Pi_c$ to the actual system input. This
hybrid approach effectively manages RHS uncertainties while preserving the
system's inherent structure. Key advantages include adjustable performance via
LHS controller parameters, enhanced AI explainability and interpretability
through the port variable $\Pi$, the ability to guarantee safety and state
attainability with hard/soft constraints, reduced complexity in learning
hypothesis classes compared to end-to-end solutions, and improved
state/parameter estimation using LHS prior knowledge and system Hamiltonian to
address partial observability. The paper details the p$\mathcal{H}$
formulation, derives the decomposition, and presents the modular controller
architecture. Beyond design, crucial aspects of stability and robustness
analysis and synthesis are investigated, paving the way for deeper theoretical
investigations. An application example, a pendulum with nonlinear dynamics, is
simulated to demonstrate the approach's empirical and phenomenological benefits
for future research.

</details>


### [613] [Distributed Risk-Sensitive Safety Filters for Uncertain Discrete-Time Systems](https://arxiv.org/abs/2506.07347)
*Armin Lederer,Erfaun Noorani,Andreas Krause*

Main category: eess.SY

TL;DR: 提出基于价值函数定义的控制障碍函数的风险敏感安全过滤器，有分布式公式并经评估有效


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统在集中协调不可行时的安全保障难题

Method: 利用基于指数风险算子的集中式风险敏感安全条件，推导基于最坏情况预期和接近已知安全策略的两种分布式策略

Result: 通过详细数值评估证明该方法能有效保障安全且不过于保守

Conclusion: 所提方法可有效保障多智能体系统安全

Abstract: Ensuring safety in multi-agent systems is a significant challenge,
particularly in settings where centralized coordination is impractical. In this
work, we propose a novel risk-sensitive safety filter for discrete-time
multi-agent systems with uncertain dynamics that leverages control barrier
functions (CBFs) defined through value functions. Our approach relies on
centralized risk-sensitive safety conditions based on exponential risk
operators to ensure robustness against model uncertainties. We introduce a
distributed formulation of the safety filter by deriving two alternative
strategies: one based on worst-case anticipation and another on proximity to a
known safe policy. By allowing agents to switch between strategies, feasibility
can be ensured. Through detailed numerical evaluations, we demonstrate the
efficacy of our approach in maintaining safety without being overly
conservative.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [614] [Generative Voice Bursts during Phone Call](https://arxiv.org/abs/2506.07526)
*Paritosh Ranjan,Surajit Majumder,Prodip Roy*

Main category: cs.SD

TL;DR: 提出在通话中传输生成式语音突发消息的新方法，可绕过常规呼叫等待障碍，有广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统移动电话在紧急情况下无法向正在通话的被叫方传达紧急语音消息，标准呼叫等待提醒不能体现紧急性和内容。

Method: 利用生成式AI技术，根据上下文输入自动生成语音消息，结合语音、文本和优先级推理机制，使用如GPT Neo等模型生成文本并合成音频，按可配置间隔和次数发送。

Result: 该方法能让高优先级紧急消息绕过常规呼叫等待障碍。

Conclusion: 此方法在电信、移动设备制造和紧急通信平台等领域有重大影响潜力。

Abstract: In critical situations, conventional mobile telephony fails to convey
emergency voice messages to a callee already engaged in another call. The
standard call waiting alert does not provide the urgency or content of the
waiting call. This paper proposes a novel method for transmitting Generative
Voice Bursts short, context aware audio messages during ongoing calls, from
either preauthorized or dynamically prioritized callers. By leveraging
generative AI techniques, the system automatically generates spoken messages
from contextual inputs example like location, health data, images, background
noise when the caller is unable to speak due to incapacitation or environmental
constraints. The solution incorporates voice, text, and priority inference
mechanisms, allowing high priority emergency messages to bypass conventional
call waiting barriers. The approach employs models such as GPT Neo for
generative text, which is synthesized into audio and delivered in configurable
intervals G seconds and counts N times, ensuring minimal disruption while
preserving urgency. This method holds potential for significant impact across
telecom, mobile device manufacturing, and emergency communication platforms.

</details>


### [615] [RBA-FE: A Robust Brain-Inspired Audio Feature Extractor for Depression Diagnosis](https://arxiv.org/abs/2506.07118)
*Yu-Xuan Wu,Ziyan Huang,Bin Hu,Zhi-Hong Guan*

Main category: cs.SD

TL;DR: 本文提出用于抑郁症诊断的RBA - FE模型，用改进架构和ARSLIF模型提升性能和鲁棒性，实验效果好且有脑启发可解释性。


<details>
  <summary>Details</summary>
Motivation: 多数深度学习模型专注图像诊断任务，忽略音频特征，且音频特征提取存在噪声挑战和精度限制。

Method: 提出RBA - FE模型，利用六种声学特征，结合改进的ARSLIF脉冲神经元模型，模拟大脑注意力系统机制。

Result: RBA - FE在MODMA数据集上达到先进准确率，在AVEC2014和DAIC - WOZ数据集上增强了噪声鲁棒性，ARSLIF模型展现异常放电模式。

Conclusion: RBA - FE模型在抑郁症诊断音频特征提取上有效，有良好性能和鲁棒性，且具有脑启发可解释性。

Abstract: This article proposes a robust brain-inspired audio feature extractor
(RBA-FE) model for depression diagnosis, using an improved hierarchical network
architecture. Most deep learning models achieve state-of-the-art performance
for image-based diagnostic tasks, ignoring the counterpart audio features. In
order to tailor the noise challenge, RBA-FE leverages six acoustic features
extracted from the raw audio, capturing both spatial characteristics and
temporal dependencies. This hybrid attribute helps alleviate the precision
limitation in audio feature extraction within other learning models like deep
residual shrinkage networks. To deal with the noise issues, our model
incorporates an improved spiking neuron model, called adaptive rate smooth
leaky integrate-and-fire (ARSLIF). The ARSLIF model emulates the mechanism of
``retuning of cellular signal selectivity" in the brain attention systems,
which enhances the model robustness against environmental noises in audio data.
Experimental results demonstrate that RBA-FE achieves state-of-the-art accuracy
on the MODMA dataset, respectively with 0.8750, 0.8974, 0.8750 and 0.8750 in
precision, accuracy, recall and F1 score. Extensive experiments on the AVEC2014
and DAIC-WOZ datasets both show enhancements in noise robustness. It is further
indicated by comparison that the ARSLIF neuron model suggest the abnormal
firing pattern within the feature extraction on depressive audio data, offering
brain-inspired interpretability.

</details>


### [616] [Speech Recognition on TV Series with Video-guided Post-Correction](https://arxiv.org/abs/2506.07323)
*Haoyuan Yang,Yue Zhang,Liqiang Jing*

Main category: cs.SD

TL;DR: 现有自动语音识别（ASR）系统在复杂环境转录准确性差，本文提出多模态后校正框架，结合视频信息提升转录准确性。


<details>
  <summary>Details</summary>
Motivation: 当前ASR系统在复杂环境如电视剧中转录准确性差，现有多模态方法无法利用视频信息校正输出。

Method: 提出两阶段多模态后校正框架，包括ASR生成和基于视频的后校正，使用VLMM提取信息并与LLM集成。

Result: 在电视剧ASR多模态基准上评估，证明利用视频上下文能提升转录准确性。

Conclusion: 所提框架能有效提升复杂多媒体环境中ASR性能。

Abstract: Automatic Speech Recognition (ASR) has achieved remarkable success with deep
learning, driving advancements in conversational artificial intelligence, media
transcription, and assistive technologies. However, ASR systems still struggle
in complex environments such as TV series, where overlapping speech,
domain-specific terminology, and long-range contextual dependencies pose
significant challenges to transcription accuracy. Existing multimodal
approaches fail to correct ASR outputs with the rich temporal and contextual
information available in video. To address this limitation, we propose a novel
multimodal post-correction framework that refines ASR transcriptions by
leveraging contextual cues extracted from video. Our framework consists of two
stages: ASR Generation and Video-based Post-Correction, where the first stage
produces the initial transcript and the second stage corrects errors using
Video-based Contextual Information Extraction and Context-aware ASR Correction.
We employ the Video-Large Multimodal Model (VLMM) to extract key contextual
information using tailored prompts, which is then integrated with a Large
Language Model (LLM) to refine the ASR output. We evaluate our method on a
multimodal benchmark for TV series ASR and demonstrate its effectiveness in
improving ASR performance by leveraging video-based context to enhance
transcription accuracy in complex multimedia environments.

</details>


### [617] [Lightweight Joint Audio-Visual Deepfake Detection via Single-Stream Multi-Modal Learning Framework](https://arxiv.org/abs/2506.07358)
*Kuiyuan Zhang,Wenjie Pei,Rushi Lan,Yifang Guo,Zhongyun Hua*

Main category: cs.SD

TL;DR: 本文设计轻量级单流多模态网络检测音视频deepfake，实验表明其参数少且效果优。


<details>
  <summary>Details</summary>
Motivation: 以往音视频deepfake检测方法未充分利用音视频特征关联，且双模型冗余，不适用于资源受限环境。

Method: 设计单流多模态学习框架的轻量级网络，引入协作音视频学习块融合特征，提出多模态分类模块。

Result: 在多个基准数据集实验，与先进方法比，参数仅0.48M，在单模态、多模态及未见类型deepfake检测表现优。

Conclusion: 所提方法轻量级且在音视频deepfake检测有效。

Abstract: Deepfakes are AI-synthesized multimedia data that may be abused for spreading
misinformation. Deepfake generation involves both visual and audio
manipulation. To detect audio-visual deepfakes, previous studies commonly
employ two relatively independent sub-models to learn audio and visual
features, respectively, and fuse them subsequently for deepfake detection.
However, this may underutilize the inherent correlations between audio and
visual features. Moreover, utilizing two isolated feature learning sub-models
can result in redundant neural layers, making the overall model inefficient and
impractical for resource-constrained environments.
  In this work, we design a lightweight network for audio-visual deepfake
detection via a single-stream multi-modal learning framework. Specifically, we
introduce a collaborative audio-visual learning block to efficiently integrate
multi-modal information while learning the visual and audio features. By
iteratively employing this block, our single-stream network achieves a
continuous fusion of multi-modal features across its layers. Thus, our network
efficiently captures visual and audio features without the need for excessive
block stacking, resulting in a lightweight network design. Furthermore, we
propose a multi-modal classification module that can boost the dependence of
the visual and audio classifiers on modality content. It also enhances the
whole resistance of the video classifier against the mismatches between audio
and visual modalities. We conduct experiments on the DF-TIMIT, FakeAVCeleb, and
DFDC benchmark datasets. Compared to state-of-the-art audio-visual joint
detection methods, our method is significantly lightweight with only 0.48M
parameters, yet it achieves superiority in both uni-modal and multi-modal
deepfakes, as well as in unseen types of deepfakes.

</details>


### [618] [Audio synthesizer inversion in symmetric parameter spaces with approximately equivariant flow matching](https://arxiv.org/abs/2506.07199)
*Ben Hayes,Charalampos Saitis,György Fazekas*

Main category: cs.SD

TL;DR: 指出音频合成器参数反演因对称性而病态，提出用条件生成模型和排列等变流改善，在Surge XT上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决音频合成器从声音到参数反演的病态问题，该问题源于合成器的内在对称性。

Method: 先在合成任务中验证排列对称下回归点估计的性能下降；用条件生成模型；使用排列等变连续归一化流；提出自适应发现对称性的松弛等变策略。

Result: 条件生成模型能提升性能，排列等变连续归一化流进一步提升性能，方法在Surge XT上的音频重建指标优于回归和生成基线。

Conclusion: 提出的方法能有效解决音频合成器参数反演的病态问题，提升性能。

Abstract: Many audio synthesizers can produce the same signal given different parameter
configurations, meaning the inversion from sound to parameters is an inherently
ill-posed problem. We show that this is largely due to intrinsic symmetries of
the synthesizer, and focus in particular on permutation invariance. First, we
demonstrate on a synthetic task that regressing point estimates under
permutation symmetry degrades performance, even when using a
permutation-invariant loss function or symmetry-breaking heuristics. Then,
viewing equivalent solutions as modes of a probability distribution, we show
that a conditional generative model substantially improves performance.
Further, acknowledging the invariance of the implicit parameter distribution,
we find that performance is further improved by using a permutation equivariant
continuous normalizing flow. To accommodate intricate symmetries in real
synthesizers, we also propose a relaxed equivariance strategy that adaptively
discovers relevant symmetries from data. Applying our method to Surge XT, a
full-featured open source synthesizer used in real world audio production, we
find our method outperforms regression and generative baselines across audio
reconstruction metrics.

</details>


### [619] [Towards Generalized Source Tracing for Codec-Based Deepfake Speech](https://arxiv.org/abs/2506.07294)
*Xuanjun Chen,I-Ming Lin,Lin Zhang,Haibin Wu,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.SD

TL;DR: 现有CodecFake源追踪方法效果不佳，提出SASTNet网络，在测试集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决使用模拟CoSG数据训练源追踪模型，在真实CoSG生成音频上性能不足的问题。

Method: 引入Semantic - Acoustic Source Tracing Network (SASTNet)，联合使用Whisper进行语义特征编码，Wav2vec2和AudioMAE进行声学特征编码。

Result: SASTNet在CodecFake+数据集的CoSG测试集上达到了最先进的性能。

Conclusion: SASTNet在可靠源追踪方面是有效的。

Abstract: Recent attempts at source tracing for codec-based deepfake speech
(CodecFake), generated by neural audio codec-based speech generation (CoSG)
models, have exhibited suboptimal performance. However, how to train source
tracing models using simulated CoSG data while maintaining strong performance
on real CoSG-generated audio remains an open challenge. In this paper, we show
that models trained solely on codec-resynthesized data tend to overfit to
non-speech regions and struggle to generalize to unseen content. To mitigate
these challenges, we introduce the Semantic-Acoustic Source Tracing Network
(SASTNet), which jointly leverages Whisper for semantic feature encoding and
Wav2vec2 with AudioMAE for acoustic feature encoding. Our proposed SASTNet
achieves state-of-the-art performance on the CoSG test set of the CodecFake+
dataset, demonstrating its effectiveness for reliable source tracing.

</details>


### [620] [LeVo: High-Quality Song Generation with Multi-Preference Alignment](https://arxiv.org/abs/2506.07520)
*Shun Lei,Yaoxun Xu,Zhiwei Lin,Huaicheng Zhang,Wei Tan,Hangting Chen,Jianwei Yu,Yixuan Zhang,Chenyu Yang,Haina Zhu,Shuai Wang,Zhiyong Wu,Dong Yu*

Main category: cs.SD

TL;DR: 提出基于LM的LeVo框架解决现有歌曲生成方法的局限，实验显示其表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有歌曲生成方法在歌曲复杂创作和高质量数据稀缺问题上存在局限，影响音质、音乐性等。

Method: 引入LeVo框架，含LeLM和音乐编解码器，LeLM并行建模两种类型的令牌，采用两个仅解码器的变压器和模块化扩展训练策略，还引入基于DPO的多偏好对齐方法。

Result: 实验表明LeVo在客观和主观指标上均优于现有方法，消融研究证明设计的有效性。

Conclusion: LeVo框架能有效解决现有歌曲生成方法的局限，提升歌曲生成质量。

Abstract: Recent advances in large language models (LLMs) and audio language models
have significantly improved music generation, particularly in lyrics-to-song
generation. However, existing approaches still struggle with the complex
composition of songs and the scarcity of high-quality data, leading to
limitations in sound quality, musicality, instruction following, and
vocal-instrument harmony. To address these challenges, we introduce LeVo, an
LM-based framework consisting of LeLM and a music codec. LeLM is capable of
parallelly modeling two types of tokens: mixed tokens, which represent the
combined audio of vocals and accompaniment to achieve vocal-instrument harmony,
and dual-track tokens, which separately encode vocals and accompaniment for
high-quality song generation. It employs two decoder-only transformers and a
modular extension training strategy to prevent interference between different
token types. To further enhance musicality and instruction following, we
introduce a multi-preference alignment method based on Direct Preference
Optimization (DPO). This method handles diverse human preferences through a
semi-automatic data construction process and DPO post-training. Experimental
results demonstrate that LeVo consistently outperforms existing methods on both
objective and subjective metrics. Ablation studies further justify the
effectiveness of our designs. Audio examples are available at
https://levo-demo.github.io/.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [621] [Tactile MNIST: Benchmarking Active Tactile Perception](https://arxiv.org/abs/2506.06361)
*Tim Schneider,Guillaume Duret,Cristiana de Farias,Roberto Calandra,Liming Chen,Jan Peters*

Main category: cs.RO

TL;DR: 本文引入触觉MNIST基准套件以解决触觉感知和主动感知领域缺乏标准化基准的问题，介绍其特点、数据集及作用。


<details>
  <summary>Details</summary>
Motivation: 触觉感知虽能增强机器人操作，但本身不适合需广泛空间感知的任务，且触觉感知和主动感知领域缺乏标准化基准。

Method: 引入开源、兼容Gymnasium的触觉MNIST基准套件，提供多样模拟场景，构建综合数据集并训练CycleGAN用于逼真触觉模拟渲染。

Result: 创建了触觉MNIST基准套件，包含数据集并训练了CycleGAN。

Conclusion: 该基准套件提供标准化协议和可重复评估框架，有助于触觉感知和主动感知领域的系统性进展。

Abstract: Tactile perception has the potential to significantly enhance dexterous
robotic manipulation by providing rich local information that can complement or
substitute for other sensory modalities such as vision. However, because
tactile sensing is inherently local, it is not well-suited for tasks that
require broad spatial awareness or global scene understanding on its own. A
human-inspired strategy to address this issue is to consider active perception
techniques instead. That is, to actively guide sensors toward regions with more
informative or significant features and integrate such information over time in
order to understand a scene or complete a task. Both active perception and
different methods for tactile sensing have received significant attention
recently. Yet, despite advancements, both fields lack standardized benchmarks.
To bridge this gap, we introduce the Tactile MNIST Benchmark Suite, an
open-source, Gymnasium-compatible benchmark specifically designed for active
tactile perception tasks, including localization, classification, and volume
estimation. Our benchmark suite offers diverse simulation scenarios, from
simple toy environments all the way to complex tactile perception tasks using
vision-based tactile sensors. Furthermore, we also offer a comprehensive
dataset comprising 13,500 synthetic 3D MNIST digit models and 153,600
real-world tactile samples collected from 600 3D printed digits. Using this
dataset, we train a CycleGAN for realistic tactile simulation rendering. By
providing standardized protocols and reproducible evaluation frameworks, our
benchmark suite facilitates systematic progress in the fields of tactile
sensing and active perception.

</details>


### [622] [CPS-Guard: Framework for Dependability Assurance of AI- and LLM-Based Cyber-Physical Systems](https://arxiv.org/abs/2506.06381)
*Trisanth Srinivasan,Santosh Patapati,Himani Musku,Idhant Gode,Aditya Arora,Samvit Bhattacharya,Abubakr Nazriev,Sanika Hirave,Zaryab Kanjiani,Srinjoy Ghose,Srinidhi Shetty*

Main category: cs.RO

TL;DR: 本文介绍了用于AI驱动CPS的CPS - Guard框架，通过多角色编排自动化迭代保证过程，案例研究显示其能有效检测漏洞等。


<details>
  <summary>Details</summary>
Motivation: 传统验证和确认方法难以处理AI组件的不可预测和动态特性，需要新方法对AI驱动的CPS进行验证和确认。

Method: 引入CPS - Guard框架，在模拟环境中为专用代理分配安全监控、安全评估等专门角色，持续评估和改进AI行为。

Result: 通过自动驾驶车辆案例研究表明，CPS - Guard能有效检测漏洞、管理性能影响并支持自适应恢复策略。

Conclusion: CPS - Guard为安全和关键系统的严格验证和确认提供了结构化和可扩展的解决方案。

Abstract: Cyber-Physical Systems (CPS) increasingly depend on advanced AI techniques to
operate in critical applications. However, traditional verification and
validation methods often struggle to handle the unpredictable and dynamic
nature of AI components. In this paper, we introduce CPS-Guard, a novel
framework that employs multi-role orchestration to automate the iterative
assurance process for AI-powered CPS. By assigning specialized roles (e.g.,
safety monitoring, security assessment, fault injection, and recovery planning)
to dedicated agents within a simulated environment, CPS-Guard continuously
evaluates and refines AI behavior against a range of dependability
requirements. We demonstrate the framework through a case study involving an
autonomous vehicle navigating an intersection with an AI-based planner. Our
results show that CPS-Guard effectively detects vulnerabilities, manages
performance impacts, and supports adaptive recovery strategies, thereby
offering a structured and extensible solution for rigorous V&V in safety- and
security-critical systems.

</details>


### [623] [Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception](https://arxiv.org/abs/2506.06474)
*Everett Richards,Bipul Thapa,Lena Mashayekhy*

Main category: cs.RO

TL;DR: 提出适用于联网自动驾驶汽车的边缘协同目标检测框架ECOD，通过两种算法提高检测精度和实时性，实验显示效果良好。


<details>
  <summary>Details</summary>
Motivation: 传统车载感知系统精度有限，基于云的解决方案延迟大，不适合动态环境自动驾驶的实时处理需求。

Method: 引入ECOD框架，集成PACE和VOTE两种算法，在边缘服务器上聚合多车数据；开发硬件测试平台进行评估。

Result: ECOD显著提高目标分类精度，比传统单视角车载方法最高高75%，且确保低延迟边缘实时处理。

Conclusion: 边缘计算在对延迟敏感的自动驾驶系统中提升协同感知方面有潜力。

Abstract: Accurate and reliable object detection is critical for ensuring the safety
and efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board
perception systems have limited accuracy due to occlusions and blind spots,
while cloud-based solutions introduce significant latency, making them
unsuitable for real-time processing demands required for autonomous driving in
dynamic environments. To address these challenges, we introduce an innovative
framework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that
leverages edge computing and multi-CAV collaboration for real-time,
multi-perspective object detection. Our ECOD framework integrates two key
algorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and
Variable Object Tally and Evaluation (VOTE). PACE aggregates detection data
from multiple CAVs on an edge server to enhance perception in scenarios where
individual CAVs have limited visibility. VOTE utilizes a consensus-based voting
mechanism to improve the accuracy of object classification by integrating data
from multiple CAVs. Both algorithms are designed at the edge to operate in
real-time, ensuring low-latency and reliable decision-making for CAVs. We
develop a hardware-based controlled testbed consisting of camera-equipped
robotic CAVs and an edge server to evaluate the efficacy of our framework. Our
experimental results demonstrate the significant benefits of ECOD in terms of
improved object classification accuracy, outperforming traditional
single-perspective onboard approaches by up to 75%, while ensuring low-latency,
edge-driven real-time processing. This research highlights the potential of
edge computing to enhance collaborative perception for latency-sensitive
autonomous systems.

</details>


### [624] [Active Test-time Vision-Language Navigation](https://arxiv.org/abs/2506.06630)
*Heeju Ko,Sungjune Kim,Gyeongrok Oh,Jeongyoon Yoon,Honglak Lee,Sujin Jang,Seungryong Kim,Sangpil Kim*

Main category: cs.RO

TL;DR: 本文提出ATENA测试时主动学习框架解决视觉语言导航（VLN）在陌生环境性能下降问题，在多个VLN基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: VLN在陌生环境任务性能下降，熵最小化方法存在累积误差问题。

Method: 引入ATENA框架，通过混合熵优化控制预测置信度和动作偏好，提出自主动学习策略评估导航结果。

Result: 在REVERIE、R2R和R2R - CE等VLN基准测试中，ATENA成功克服测试时分布偏移，优于对比基线方法。

Conclusion: ATENA能有效解决VLN在陌生环境的性能问题，实现有根据和自适应的决策。

Abstract: Vision-Language Navigation (VLN) policies trained on offline datasets often
exhibit degraded task performance when deployed in unfamiliar navigation
environments at test time, where agents are typically evaluated without access
to external interaction or feedback. Entropy minimization has emerged as a
practical solution for reducing prediction uncertainty at test time; however,
it can suffer from accumulated errors, as agents may become overconfident in
incorrect actions without sufficient contextual grounding. To tackle these
challenges, we introduce ATENA (Active TEst-time Navigation Agent), a test-time
active learning framework that enables a practical human-robot interaction via
episodic feedback on uncertain navigation outcomes. In particular, ATENA learns
to increase certainty in successful episodes and decrease it in failed ones,
improving uncertainty calibration. Here, we propose mixture entropy
optimization, where entropy is obtained from a combination of the action and
pseudo-expert distributions-a hypothetical action distribution assuming the
agent's selected action to be optimal-controlling both prediction confidence
and action preference. In addition, we propose a self-active learning strategy
that enables an agent to evaluate its navigation outcomes based on confident
predictions. As a result, the agent stays actively engaged throughout all
iterations, leading to well-grounded and adaptive decision-making. Extensive
evaluations on challenging VLN benchmarks-REVERIE, R2R, and R2R-CE-demonstrate
that ATENA successfully overcomes distributional shifts at test time,
outperforming the compared baseline methods across various settings.

</details>


### [625] [Self-Adapting Improvement Loops for Robotic Learning](https://arxiv.org/abs/2506.06658)
*Calvin Luo,Zilai Zeng,Mingxi Jia,Yilun Du,Chen Sun*

Main category: cs.RO

TL;DR: 提出Self - Adapting Improvement Loop (SAIL) 迭代提升视频模型性能解决新的机器人任务。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在未见过任务上泛化有挑战，目标是设计能从自我收集行为在线持续改进的智能体。

Method: 提出SAIL，让领域内视频模型在自我生成轨迹上迭代更新，这些轨迹通过与互联网规模预训练视频模型适配收集。

Result: 将SAIL应用于多种任务，在新任务上多次迭代持续提升性能，且SAIL对自我收集经验的过滤和初始领域内演示质量有较强鲁棒性。

Conclusion: 通过与互联网规模数据适配和在线经验学习，展示了通过自我改进迭代引导高性能视频模型解决新机器人任务的方法。

Abstract: Video generative models trained on expert demonstrations have been utilized
as performant text-conditioned visual planners for solving robotic tasks.
However, generalization to unseen tasks remains a challenge. Whereas improved
generalization may be facilitated by leveraging learned prior knowledge from
additional pre-collected offline data sources, such as web-scale video
datasets, in the era of experience we aim to design agents that can
continuously improve in an online manner from self-collected behaviors. In this
work we thus propose the Self-Adapting Improvement Loop (SAIL), where an
in-domain video model iteratively updates itself on self-produced trajectories,
collected through adaptation with an internet-scale pretrained video model, and
steadily improves its performance for a specified task of interest. We apply
SAIL to a diverse suite of MetaWorld tasks, as well as two manipulation tasks
on a real robot arm, and find that performance improvements continuously emerge
over multiple iterations for novel tasks initially unseen during original
in-domain video model training. Furthermore, we discover that SAIL is
surprisingly robust regarding if and how the self-collected experience is
filtered, and the quality of the initial in-domain demonstrations. Through
adaptation with summarized internet-scale data, and learning through online
experience, we thus demonstrate a way to iteratively bootstrap a
high-performance video model for solving novel robotic tasks through
self-improvement.

</details>


### [626] [DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning](https://arxiv.org/abs/2506.06659)
*Wenhao Yao,Zhenxin Li,Shiyi Lan,Zi Wang,Xinglong Sun,Jose M. Alvarez,Zuxuan Wu*

Main category: cs.RO

TL;DR: 本文提出DriveSuprim方法解决自动驾驶车辆轨迹选择难题，在不同场景表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有回归和选择方法在复杂驾驶环境下评估轨迹安全性存在不足，选择方法面临优化挑战。

Method: 提出DriveSuprim，采用粗到细渐进式候选过滤、基于旋转的增强方法和自蒸馏框架。

Result: DriveSuprim达到了最先进的性能，在NAVSIM v1中PDMS达93.5%，在NAVSIM v2中EPDMS达87.1%。

Conclusion: DriveSuprim在各种驾驶场景中具有优越的安全关键能力，能保持高轨迹质量。

Abstract: In complex driving environments, autonomous vehicles must navigate safely.
Relying on a single predicted path, as in regression-based approaches, usually
does not explicitly assess the safety of the predicted trajectory.
Selection-based methods address this by generating and scoring multiple
trajectory candidates and predicting the safety score for each, but face
optimization challenges in precisely selecting the best option from thousands
of possibilities and distinguishing subtle but safety-critical differences,
especially in rare or underrepresented scenarios. We propose DriveSuprim to
overcome these challenges and advance the selection-based paradigm through a
coarse-to-fine paradigm for progressive candidate filtering, a rotation-based
augmentation method to improve robustness in out-of-distribution scenarios, and
a self-distillation framework to stabilize training. DriveSuprim achieves
state-of-the-art performance, reaching 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS
in NAVSIM v2 without extra data, demonstrating superior safetycritical
capabilities, including collision avoidance and compliance with rules, while
maintaining high trajectory quality in various driving scenarios.

</details>


### [627] [RoboPARA: Dual-Arm Robot Planning with Parallel Allocation and Recomposition Across Tasks](https://arxiv.org/abs/2506.06683)
*Shiying Duan,Pei Ren,Nanxiang Jiang,Zhengping Che,Jian Tang,Yifan Sun,Zhaoxin Fan,Wenjun Wu*

Main category: cs.RO

TL;DR: 提出RoboPARA框架用于双臂任务并行规划，引入X - DAPT数据集评估，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有双臂机器人任务规划方法未能充分优化任务并行性，限制了双臂协作潜力。

Method: 提出RoboPARA框架，采用两阶段过程：基于依赖图生成规划候选、基于图重遍历进行双臂并行规划；引入X - DAPT数据集。

Result: 在X - DAPT数据集上的大量实验表明，RoboPARA显著优于现有方法，在复杂任务组合中效率和可靠性更高。

Conclusion: RoboPARA框架能有效提升双臂机器人任务并行规划的效率和可靠性。

Abstract: Dual-arm robots play a crucial role in improving efficiency and flexibility
in complex multitasking scenarios. While existing methods have achieved
promising results in task planning, they often fail to fully optimize task
parallelism, limiting the potential of dual-arm collaboration. To address this
issue, we propose RoboPARA, a novel large language model (LLM)-driven framework
for dual-arm task parallelism planning. RoboPARA employs a two-stage process:
(1) Dependency Graph-based Planning Candidates Generation, which constructs
directed acyclic graphs (DAGs) to model task dependencies and eliminate
redundancy, and (2) Graph Re-Traversal-based Dual-Arm Parallel Planning, which
optimizes DAG traversal to maximize parallelism while maintaining task
coherence. In addition, we introduce the Cross-Scenario Dual-Arm Parallel Task
dataset (X-DAPT dataset), the first dataset specifically designed to evaluate
dual-arm task parallelism across diverse scenarios and difficulty levels.
Extensive experiments on the X-DAPT dataset demonstrate that RoboPARA
significantly outperforms existing methods, achieving higher efficiency and
reliability, particularly in complex task combinations. The code and dataset
will be released upon acceptance.

</details>


### [628] [Multimodal Spatial Language Maps for Robot Navigation and Manipulation](https://arxiv.org/abs/2506.06862)
*Chenguang Huang,Oier Mees,Andy Zeng,Wolfram Burgard*

Main category: cs.RO

TL;DR: 提出多模态空间语言地图，结合大语言模型实现零样本空间和多模态目标导航，在模拟和现实场景实验中提升召回率。


<details>
  <summary>Details</summary>
Motivation: 以往方法与环境映射脱节、缺乏几何地图空间精度或忽视视觉外的模态信息。

Method: 提出多模态空间语言地图，融合预训练多模态特征与环境3D重建，构建视觉 - 语言地图和视听 - 语言地图。

Result: 在模拟和现实场景实验中实现零样本空间和多模态目标导航，在模糊场景中召回率提高50%。

Conclusion: 多模态空间语言地图可用于移动机器人和桌面操纵器，支持视觉、音频和空间线索引导的导航与交互。

Abstract: Grounding language to a navigating agent's observations can leverage
pretrained multimodal foundation models to match perceptions to object or event
descriptions. However, previous approaches remain disconnected from environment
mapping, lack the spatial precision of geometric maps, or neglect additional
modality information beyond vision. To address this, we propose multimodal
spatial language maps as a spatial map representation that fuses pretrained
multimodal features with a 3D reconstruction of the environment. We build these
maps autonomously using standard exploration. We present two instances of our
maps, which are visual-language maps (VLMaps) and their extension to
audio-visual-language maps (AVLMaps) obtained by adding audio information. When
combined with large language models (LLMs), VLMaps can (i) translate natural
language commands into open-vocabulary spatial goals (e.g., "in between the
sofa and TV") directly localized in the map, and (ii) be shared across
different robot embodiments to generate tailored obstacle maps on demand.
Building upon the capabilities above, AVLMaps extend VLMaps by introducing a
unified 3D spatial representation integrating audio, visual, and language cues
through the fusion of features from pretrained multimodal foundation models.
This enables robots to ground multimodal goal queries (e.g., text, images, or
audio snippets) to spatial locations for navigation. Additionally, the
incorporation of diverse sensory inputs significantly enhances goal
disambiguation in ambiguous environments. Experiments in simulation and
real-world settings demonstrate that our multimodal spatial language maps
enable zero-shot spatial and multimodal goal navigation and improve recall by
50% in ambiguous scenarios. These capabilities extend to mobile robots and
tabletop manipulators, supporting navigation and interaction guided by visual,
audio, and spatial cues.

</details>


### [629] [CARoL: Context-aware Adaptation for Robot Learning](https://arxiv.org/abs/2506.07006)
*Zechen Hu,Tong Xu,Xuesu Xiao,Xuan Wang*

Main category: cs.RO

TL;DR: 提出CARoL框架，通过分析状态转换识别新任务与先验知识的相似性，可适配多种强化学习算法，在模拟和真实场景验证了其效率和泛化性。


<details>
  <summary>Details</summary>
Motivation: 解决利用先验知识学习新任务时，确定知识相关性和自适应集成的挑战，提升学习效率。

Method: 提出Context-aware Adaptation for Robot Learning (CARoL)框架，分析系统动态中的状态转换来识别相似性，利用相似性为新任务调整知识。

Result: 在CarRacing和LunarLander模拟环境中更快收敛、获更高奖励；在真实地面车辆实验中，能快速将模拟中学习的策略应用于越野场景。

Conclusion: CARoL框架有效且具有广泛适用性，可提升学习新任务的效率。

Abstract: Using Reinforcement Learning (RL) to learn new robotic tasks from scratch is
often inefficient. Leveraging prior knowledge has the potential to
significantly enhance learning efficiency, which, however, raises two critical
challenges: how to determine the relevancy of existing knowledge and how to
adaptively integrate them into learning a new task. In this paper, we propose
Context-aware Adaptation for Robot Learning (CARoL), a novel framework to
efficiently learn a similar but distinct new task from prior knowledge. CARoL
incorporates context awareness by analyzing state transitions in system
dynamics to identify similarities between the new task and prior knowledge. It
then utilizes these identified similarities to prioritize and adapt specific
knowledge pieces for the new task. Additionally, CARoL has a broad
applicability spanning policy-based, value-based, and actor-critic RL
algorithms. We validate the efficiency and generalizability of CARoL on both
simulated robotic platforms and physical ground vehicles. The simulations
include CarRacing and LunarLander environments, where CARoL demonstrates faster
convergence and higher rewards when learning policies for new tasks. In
real-world experiments, we show that CARoL enables a ground vehicle to quickly
and efficiently adapt policies learned in simulation to smoothly traverse
real-world off-road terrain.

</details>


### [630] [Prime the search: Using large language models for guiding geometric task and motion planning by warm-starting tree search](https://arxiv.org/abs/2506.07062)
*Dongryung Lee,Sejune Joo,Kimin Lee,Beomjoon Kim*

Main category: cs.RO

TL;DR: 提出用大语言模型（LLM）引导几何任务与运动规划（G - TAMP）问题的任务规划，设计谓词提示，扩展蒙特卡罗树搜索（MCTS），在六个G - TAMP问题上表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 传统G - TAMP方法需大量计算资源或数据，而人类用常识直观决策，受此启发利用有常识知识的LLM引导任务规划。

Method: 设计基于谓词的提示让LLM进行几何推理，查询LLM生成任务计划，扩展MCTS到混合动作空间，用LLM探索的节点预热MCTS搜索。

Result: 在六个不同的G - TAMP问题上，该方法优于先前的LLM规划器和纯搜索算法。

Conclusion: 利用LLM引导G - TAMP任务规划的方法可行且有效。

Abstract: The problem of relocating a set of objects to designated areas amidst movable
obstacles can be framed as a Geometric Task and Motion Planning (G-TAMP)
problem, a subclass of task and motion planning (TAMP). Traditional approaches
to G-TAMP have relied either on domain-independent heuristics or on learning
from planning experience to guide the search, both of which typically demand
significant computational resources or data. In contrast, humans often use
common sense to intuitively decide which objects to manipulate in G-TAMP
problems. Inspired by this, we propose leveraging Large Language Models (LLMs),
which have common sense knowledge acquired from internet-scale data, to guide
task planning in G-TAMP problems. To enable LLMs to perform geometric
reasoning, we design a predicate-based prompt that encodes geometric
information derived from a motion planning algorithm. We then query the LLM to
generate a task plan, which is then used to search for a feasible set of
continuous parameters. Since LLMs are prone to mistakes, instead of committing
to LLM's outputs, we extend Monte Carlo Tree Search (MCTS) to a hybrid action
space and use the LLM to guide the search. Unlike the previous approach that
calls an LLM at every node and incurs high computational costs, we use it to
warm-start the MCTS with the nodes explored in completing the LLM's task plan.
On six different G-TAMP problems, we show our method outperforms previous LLM
planners and pure search algorithms. Code can be found at:
https://github.com/iMSquared/prime-the-search

</details>


### [631] [Robotic Policy Learning via Human-assisted Action Preference Optimization](https://arxiv.org/abs/2506.07127)
*Wenke xia,Yichu Yang,Hongtao Wu,Xiao Ma,Tao Kong,Di Hu*

Main category: cs.RO

TL;DR: 提出HAPO方法解决VLA模型依赖专家演示的局限，实验证明其在模拟和现实场景中有出色泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: VLA模型依赖专家演示，缺乏从失败中学习和修正的能力，限制了在现实应用中的部署。

Method: 引入HAPO方法，包含人机协作框架收集轨迹，在动作偏好优化过程中使用这些轨迹，并提出自适应重加权算法。

Result: 在模拟和现实场景的实验中，框架在多种操作任务中表现出优越的泛化性和鲁棒性。

Conclusion: HAPO方法确保了VLA模型的可靠部署和从失败中有效学习。

Abstract: Establishing a reliable and iteratively refined robotic system is essential
for deploying real-world applications. While Vision-Language-Action (VLA)
models are widely recognized as the foundation model for such robotic
deployment, their dependence on expert demonstrations hinders the crucial
capabilities of correction and learning from failures. To mitigate this
limitation, we introduce a Human-assisted Action Preference Optimization method
named HAPO, designed to correct deployment failures and foster effective
adaptation through preference alignment for VLA models. This method begins with
a human-robot collaboration framework for reliable failure correction and
interaction trajectory collection through human intervention. These
human-intervention trajectories are further employed within the action
preference optimization process, facilitating VLA models to mitigate failure
action occurrences while enhancing corrective action adaptation. Specifically,
we propose an adaptive reweighting algorithm to address the issues of
irreversible interactions and token probability mismatch when introducing
preference optimization into VLA models, facilitating model learning from
binary desirability signals derived from interactions. Through combining these
modules, our human-assisted action preference optimization method ensures
reliable deployment and effective learning from failure for VLA models. The
experiments conducted in simulation and real-world scenarios prove superior
generalization and robustness of our framework across a variety of manipulation
tasks.

</details>


### [632] [Real-Time Execution of Action Chunking Flow Policies](https://arxiv.org/abs/2506.07339)
*Kevin Black,Manuel Y. Galliker,Sergey Levine*

Main category: cs.RO

TL;DR: 本文提出实时分块（RTC）算法，解决现代AI系统实时性能问题，在模拟和现实任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统需实时性能，但现有通用模型高延迟是挑战，动作分块未完全解决延迟问题。

Method: 提出实时分块（RTC）算法，适用于基于扩散或流的视觉语言动作模型，执行当前动作块时生成下一个，“冻结”确定执行的动作并“补全”其余部分。

Result: 在Kinetix模拟器的12个高动态任务和6个现实双手操作任务中测试，RTC速度快、性能好，对推理延迟有独特鲁棒性，显著提高任务吞吐量和成功率。

Conclusion: RTC算法有效解决了现代AI系统的实时性能问题，提升了任务表现。

Abstract: Modern AI systems, especially those interacting with the physical world,
increasingly require real-time performance. However, the high latency of
state-of-the-art generalist models, including recent vision-language action
models (VLAs), poses a significant challenge. While action chunking has enabled
temporal consistency in high-frequency control tasks, it does not fully address
the latency problem, leading to pauses or out-of-distribution jerky movements
at chunk boundaries. This paper presents a novel inference-time algorithm that
enables smooth asynchronous execution of action chunking policies. Our method,
real-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out
of the box with no re-training. It generates the next action chunk while
executing the current one, "freezing" actions guaranteed to execute and
"inpainting" the rest. To test RTC, we introduce a new benchmark of 12 highly
dynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging
real-world bimanual manipulation tasks. Results demonstrate that RTC is fast,
performant, and uniquely robust to inference delay, significantly improving
task throughput and enabling high success rates in precise tasks
$\unicode{x2013}$ such as lighting a match $\unicode{x2013}$ even in the
presence of significant latency. See
https://pi.website/research/real_time_chunking for videos.

</details>


### [633] [Machine Learning-Based Self-Localization Using Internal Sensors for Automating Bulldozers](https://arxiv.org/abs/2506.07271)
*Hikaru Sawafuji,Ryota Ozaki,Takuto Motomura,Toyohisa Matsuda,Masanori Tojima,Kento Uchida,Shinichi Shirakawa*

Main category: cs.RO

TL;DR: 提出基于机器学习的推土机自定位方法，通过自建数据集实验验证其抑制误差积累、提高定位精度效果。


<details>
  <summary>Details</summary>
Motivation: 传统依赖RTK - GNSS的推土机自定位系统在某些采矿条件下信号易丢失，需不依赖该系统的自定位方法。

Method: 分两步，用机器学习模型根据内部传感器估计局部速度，将估计值融入扩展卡尔曼滤波器进行全局定位，还创建推土机里程计新数据集。

Result: 相比基于运动学的方法，所提方法抑制了位置误差积累，推土机特定传感器有助于提高定位精度。

Conclusion: 所提基于机器学习的自定位方法有效，可在特定场景替代传统方法。

Abstract: Self-localization is an important technology for automating bulldozers.
Conventional bulldozer self-localization systems rely on RTK-GNSS (Real Time
Kinematic-Global Navigation Satellite Systems). However, RTK-GNSS signals are
sometimes lost in certain mining conditions. Therefore, self-localization
methods that do not depend on RTK-GNSS are required. In this paper, we propose
a machine learning-based self-localization method for bulldozers. The proposed
method consists of two steps: estimating local velocities using a machine
learning model from internal sensors, and incorporating these estimates into an
Extended Kalman Filter (EKF) for global localization. We also created a novel
dataset for bulldozer odometry and conducted experiments across various driving
scenarios, including slalom, excavation, and driving on slopes. The result
demonstrated that the proposed self-localization method suppressed the
accumulation of position errors compared to kinematics-based methods,
especially when slip occurred. Furthermore, this study showed that
bulldozer-specific sensors, such as blade position sensors and hydraulic
pressure sensors, contributed to improving self-localization accuracy.

</details>


### [634] [Language-Grounded Hierarchical Planning and Execution with Multi-Robot 3D Scene Graphs](https://arxiv.org/abs/2506.07454)
*Jared Strader,Aaron Ray,Jacob Arkin,Mason B. Peterson,Yun Chang,Nathan Hughes,Christopher Bradley,Yi Xuan Jia,Carlos Nieto-Granda,Rajat Talak,Chuchu Fan,Luca Carlone,Jonathan P. How,Nicholas Roy*

Main category: cs.RO

TL;DR: 本文介绍了一个多机器人系统，利用3D场景图集成映射、定位和任务与运动规划，可执行自然语言复杂指令，并进行了性能实验评估。


<details>
  <summary>Details</summary>
Motivation: 实现多机器人执行自然语言表达的复杂指令，让机器人能对周围环境进行推理和执行复杂任务。

Method: 构建包含开放集基于对象地图的共享3D场景图用于多机器人3D场景图融合；利用共享3D场景图和机器人能力，通过大语言模型将操作员意图转化为规划域定义语言目标。

Result: 对系统在大规模户外环境的真实世界任务中进行了性能实验评估，但未提及具体结果。

Conclusion: 未在摘要中提及明确结论。

Abstract: In this paper, we introduce a multi-robot system that integrates mapping,
localization, and task and motion planning (TAMP) enabled by 3D scene graphs to
execute complex instructions expressed in natural language. Our system builds a
shared 3D scene graph incorporating an open-set object-based map, which is
leveraged for multi-robot 3D scene graph fusion. This representation supports
real-time, view-invariant relocalization (via the object-based map) and
planning (via the 3D scene graph), allowing a team of robots to reason about
their surroundings and execute complex tasks. Additionally, we introduce a
planning approach that translates operator intent into Planning Domain
Definition Language (PDDL) goals using a Large Language Model (LLM) by
leveraging context from the shared 3D scene graph and robot capabilities. We
provide an experimental assessment of the performance of our system on
real-world tasks in large-scale, outdoor environments.

</details>


### [635] [BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models](https://arxiv.org/abs/2506.07961)
*Peiyan Li,Yixiang Chen,Hongtao Wu,Xiao Ma,Xiangnan Wu,Yan Huang,Liang Wang,Tao Kong,Tieniu Tan*

Main category: cs.RO

TL;DR: 本文提出3D视觉语言动作模型BridgeVLA及可扩展预训练方法，实验表明其能高效学习3D操作，表现优于基线方法，样本效率高。


<details>
  <summary>Details</summary>
Motivation: 现有将3D信号融入视觉语言模型进行动作预测的方法未充分利用3D数据空间结构，样本效率低。

Method: 提出BridgeVLA模型，将3D输入投影到多个2D图像，用2D热图进行动作预测；提出可扩展预训练方法让模型骨干具备预测2D热图能力。

Result: 在三个模拟基准测试和真实机器人实验中均优于基线方法，在多种分布外设置中泛化性好，样本效率高。

Conclusion: 所提方法能有效且高效地学习3D操作，样本效率出色。

Abstract: Recently, leveraging pre-trained vision-language models (VLMs) for building
vision-language-action (VLA) models has emerged as a promising approach to
effective robot manipulation learning. However, only few methods incorporate 3D
signals into VLMs for action prediction, and they do not fully leverage the
spatial structure inherent in 3D data, leading to low sample efficiency. In
this paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D
inputs to multiple 2D images, ensuring input alignment with the VLM backbone,
and (2) utilizes 2D heatmaps for action prediction, unifying the input and
output spaces within a consistent 2D image space. In addition, we propose a
scalable pre-training method that equips the VLM backbone with the capability
to predict 2D heatmaps before downstream policy learning. Extensive experiments
show the proposed method is able to learn 3D manipulation efficiently and
effectively. BridgeVLA outperforms state-of-the-art baseline methods across
three simulation benchmarks. In RLBench, it improves the average success rate
from 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better
performance in challenging generalization settings, boosting the average
success rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing
baseline methods in terms of average success rate. In real-robot experiments,
BridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It
generalizes robustly in multiple out-of-distribution settings, including visual
disturbances and unseen instructions. Remarkably, it is able to achieve a
success rate of 96.8% on 10+ tasks with only 3 trajectories per task,
highlighting its extraordinary sample efficiency. Project
Website:https://bridgevla.github.io/

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [636] [TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in Federated Tree-Based Systems](https://arxiv.org/abs/2506.07605)
*Marco Di Gennaro,Giovanni De Lucia,Stefano Longari,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: 本文提出针对水平联邦树模型的攻击TimberStrike，评估其在多个框架上的效果，分析差分隐私影响，强调需为树基联邦学习系统设计隐私保护机制。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中树基模型的安全和隐私影响研究不足。

Method: 提出TimberStrike攻击，利用决策树离散性，通过分裂值和决策路径推断其他客户端敏感训练数据。

Result: 在多个框架上评估，在中风预测数据集上能重构73.05% - 95.63%目标数据集，差分隐私部分缓解攻击但显著降低模型性能。

Conclusion: 需要为树基联邦学习系统专门设计隐私保护机制，并给出初步设计思路。

Abstract: Federated Learning has emerged as a privacy-oriented alternative to
centralized Machine Learning, enabling collaborative model training without
direct data sharing. While extensively studied for neural networks, the
security and privacy implications of tree-based models remain underexplored.
This work introduces TimberStrike, an optimization-based dataset reconstruction
attack targeting horizontally federated tree-based models. Our attack, carried
out by a single client, exploits the discrete nature of decision trees by using
split values and decision paths to infer sensitive training data from other
clients. We evaluate TimberStrike on State-of-the-Art federated gradient
boosting implementations across multiple frameworks, including Flower, NVFlare,
and FedTree, demonstrating their vulnerability to privacy breaches. On a
publicly available stroke prediction dataset, TimberStrike consistently
reconstructs between 73.05% and 95.63% of the target dataset across all
implementations. We further analyze Differential Privacy, showing that while it
partially mitigates the attack, it also significantly degrades model
performance. Our findings highlight the need for privacy-preserving mechanisms
specifically designed for tree-based Federated Learning systems, and we provide
preliminary insights into their design.

</details>


### [637] [TimeWak: Temporal Chained-Hashing Watermark for Time Series Data](https://arxiv.org/abs/2506.06407)
*Zhi Wen Soi,Chaoyi Zhu,Fouad Abiad,Aditya Shankar,Jeroen M. Galjaard,Huijuan Wang,Lydia Y. Chen*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Synthetic time series generated by diffusion models enable sharing
privacy-sensitive datasets, such as patients' functional MRI records. Key
criteria for synthetic data include high data utility and traceability to
verify the data source. Recent watermarking methods embed in homogeneous latent
spaces, but state-of-the-art time series generators operate in real space,
making latent-based watermarking incompatible. This creates the challenge of
watermarking directly in real space while handling feature heterogeneity and
temporal dependencies. We propose TimeWak, the first watermarking algorithm for
multivariate time series diffusion models. To handle temporal dependence and
spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark
directly within the real temporal-feature space. The other unique feature is
the $\epsilon$-exact inversion, which addresses the non-uniform reconstruction
error distribution across features from inverting the diffusion process to
detect watermarks. We derive the error bound of inverting multivariate time
series and further maintain high watermark detectability. We extensively
evaluate TimeWak on its impact on synthetic data quality, watermark
detectability, and robustness under various post-editing attacks, against 5
datasets and baselines of different temporal lengths. Our results show that
TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in
correlational scores against the state-of-the-art baseline, while remaining
consistently detectable.

</details>


### [638] [HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions](https://arxiv.org/abs/2506.06409)
*Dor Tsur,Carol Xuan Long,Claudio Mayrink Verdun,Hsiang Hsu,Chen-Fu Chen,Haim Permuter,Sajani Vithana,Flavio P. Calmon*

Main category: cs.CR

TL;DR: 本文提出优化框架设计两种大语言模型水印HeavyWater和SimplexWater，能在低熵场景兼顾检测准确率和文本质量，还揭示与编码理论的联系。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型水印在低熵生成任务中面临挑战，需有效利用随机边信息设计水印。

Method: 提出优化框架，设计HeavyWater和SimplexWater两种水印，可调节检测准确率和文本失真间的平衡。

Result: 通过基准测试表明，新水印能在低熵场景下实现高检测准确率，同时最小化文本生成质量的损失。

Conclusion: 新水印方法有效，尤其适用于低熵场景，且揭示了大语言模型水印与编码理论的新联系。

Abstract: Large language model (LLM) watermarks enable authentication of text
provenance, curb misuse of machine-generated text, and promote trust in AI
systems. Current watermarks operate by changing the next-token predictions
output by an LLM. The updated (i.e., watermarked) predictions depend on random
side information produced, for example, by hashing previously generated tokens.
LLM watermarking is particularly challenging in low-entropy generation tasks -
such as coding - where next-token predictions are near-deterministic. In this
paper, we propose an optimization framework for watermark design. Our goal is
to understand how to most effectively use random side information in order to
maximize the likelihood of watermark detection and minimize the distortion of
generated text. Our analysis informs the design of two new watermarks:
HeavyWater and SimplexWater. Both watermarks are tunable, gracefully
trading-off between detection accuracy and text distortion. They can also be
applied to any LLM and are agnostic to side information generation. We examine
the performance of HeavyWater and SimplexWater through several benchmarks,
demonstrating that they can achieve high watermark detection accuracy with
minimal compromise of text generation quality, particularly in the low-entropy
regime. Our theoretical analysis also reveals surprising new connections
between LLM watermarking and coding theory. The code implementation can be
found in https://github.com/DorTsur/HeavyWater_SimplexWater

</details>


### [639] [Benchmarking Misuse Mitigation Against Covert Adversaries](https://arxiv.org/abs/2506.06414)
*Davis Brown,Mahdi Sabbaghi,Luze Sun,Alexander Robey,George J. Pappas,Eric Wong,Hamed Hassani*

Main category: cs.CR

TL;DR: 现有语言模型安全评估有局限，提出BSD数据生成管道评估隐蔽攻击与防御，发现分解攻击有效且有状态防御可应对。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型安全评估集中于明显攻击和低风险任务，现实攻击者可通过多次独立查询小的看似无害任务绕过防护，需识别针对此类策略的防御方法。

Method: 开发Benchmarks for Stateful Defenses (BSD)数据生成管道，自动化评估隐蔽攻击和相应防御。

Result: 使用管道策划了两个新数据集，前沿模型会一致拒绝，较弱的开放权重模型难以处理。评估表明分解攻击是有效的滥用促成因素。

Conclusion: 强调有状态防御可作为对抗分解攻击的对策。

Abstract: Existing language model safety evaluations focus on overt attacks and
low-stakes tasks. Realistic attackers can subvert current safeguards by
requesting help on small, benign-seeming tasks across many independent queries.
Because individual queries do not appear harmful, the attack is hard to
{detect}. However, when combined, these fragments uplift misuse by helping the
attacker complete hard and dangerous tasks. Toward identifying defenses against
such strategies, we develop Benchmarks for Stateful Defenses (BSD), a data
generation pipeline that automates evaluations of covert attacks and
corresponding defenses. Using this pipeline, we curate two new datasets that
are consistently refused by frontier models and are too difficult for weaker
open-weight models. Our evaluations indicate that decomposition attacks are
effective misuse enablers, and highlight stateful defenses as a countermeasure.

</details>


### [640] [Fuse and Federate: Enhancing EV Charging Station Security with Multimodal Fusion and Federated Learning](https://arxiv.org/abs/2506.06730)
*Rabah Rahal,Abdelaziz Amara Korba,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: 文章提出基于多模态数据源的新型入侵检测框架，用分布式学习方法，实验显示其性能优于现有方案，能应对电动汽车充电设施安全挑战。


<details>
  <summary>Details</summary>
Motivation: 电动汽车充电设施（EVSE）面临网络侦察、后门入侵和DDoS攻击等网络安全挑战，现有入侵检测系统难以应对，需创新安全机制。

Method: 提出利用网络流量和内核事件等多模态数据源的入侵检测框架，采用分布式学习方法，通过联邦学习在保护数据隐私的同时实现充电站间的协作智能。

Result: 实验表明，该框架在分布式环境中检测率超98%，准确率超97%，优于现有解决方案。

Conclusion: 该解决方案可应对EVSE安全的不断变化的挑战，为高级网络威胁提供可扩展且保护隐私的应对措施。

Abstract: The rapid global adoption of electric vehicles (EVs) has established electric
vehicle supply equipment (EVSE) as a critical component of smart grid
infrastructure. While essential for ensuring reliable energy delivery and
accessibility, EVSE systems face significant cybersecurity challenges,
including network reconnaissance, backdoor intrusions, and distributed
denial-of-service (DDoS) attacks. These emerging threats, driven by the
interconnected and autonomous nature of EVSE, require innovative and adaptive
security mechanisms that go beyond traditional intrusion detection systems
(IDS). Existing approaches, whether network-based or host-based, often fail to
detect sophisticated and targeted attacks specifically crafted to exploit new
vulnerabilities in EVSE infrastructure. This paper proposes a novel intrusion
detection framework that leverages multimodal data sources, including network
traffic and kernel events, to identify complex attack patterns. The framework
employs a distributed learning approach, enabling collaborative intelligence
across EVSE stations while preserving data privacy through federated learning.
Experimental results demonstrate that the proposed framework outperforms
existing solutions, achieving a detection rate above 98% and a precision rate
exceeding 97% in decentralized environments. This solution addresses the
evolving challenges of EVSE security, offering a scalable and privacypreserving
response to advanced cyber threats

</details>


### [641] [Ai-Driven Vulnerability Analysis in Smart Contracts: Trends, Challenges and Future Directions](https://arxiv.org/abs/2506.06735)
*Mesut Ozdag*

Main category: cs.CR

TL;DR: 论文探讨基于AI技术检测智能合约漏洞，分析多种AI方法，比较优缺点并指出领域挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 传统智能合约审计技术在可扩展性、自动化和适应性上存在局限，AI解决方案有潜力应对，故研究AI驱动的漏洞检测技术。

Method: 研究机器学习、深度学习、图神经网络和基于Transformer的模型，分析其代码表示、语义信息处理和对现实漏洞类别的响应。

Result: 比较了各技术在准确性、可解释性、计算开销和实时适用性方面的优缺点。

Conclusion: 指出了该领域存在的开放挑战和未来发展机遇。

Abstract: Smart contracts, integral to blockchain ecosystems, enable decentralized
applications to execute predefined operations without intermediaries. Their
ability to enforce trustless interactions has made them a core component of
platforms such as Ethereum. Vulnerabilities such as numerical overflows,
reentrancy attacks, and improper access permissions have led to the loss of
millions of dollars throughout the blockchain and smart contract sector.
Traditional smart contract auditing techniques such as manual code reviews and
formal verification face limitations in scalability, automation, and
adaptability to evolving development patterns. As a result, AI-based solutions
have emerged as a promising alternative, offering the ability to learn complex
patterns, detect subtle flaws, and provide scalable security assurances. This
paper examines novel AI-driven techniques for vulnerability detection in smart
contracts, focusing on machine learning, deep learning, graph neural networks,
and transformer-based models. This paper analyzes how each technique represents
code, processes semantic information, and responds to real world vulnerability
classes. We also compare their strengths and weaknesses in terms of accuracy,
interpretability, computational overhead, and real time applicability. Lastly,
it highlights open challenges and future opportunities for advancing this
domain.

</details>


### [642] [Amatriciana: Exploiting Temporal GNNs for Robust and Efficient Money Laundering Detection](https://arxiv.org/abs/2506.00654)
*Marco Di Gennaro,Francesco Panebianco,Marco Pianta,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: 提出基于图神经网络的Amatriciana方法检测洗钱者，实验显示在有限数据下可学习，数据增多时优于其他模型，F1分数0.76，降低误报率55%。


<details>
  <summary>Details</summary>
Motivation: 洗钱犯罪威胁金融和社会安全，交易数量增长需要自动工具辅助执法机构检测。

Method: 提出Amatriciana方法，基于图神经网络，考虑时间信息，使用完整交易图，不拆分子图。

Result: 模型能从有限数据学习，数据增多时优于其他模型，F1分数0.76，降低误报率55%。

Conclusion: Amatriciana方法在检测洗钱者方面表现良好，降低误报率。

Abstract: Money laundering is a financial crime that poses a serious threat to
financial integrity and social security. The growing number of transactions
makes it necessary to use automatic tools that help law enforcement agencies
detect such criminal activity. In this work, we present Amatriciana, a novel
approach based on Graph Neural Networks to detect money launderers inside a
graph of transactions by considering temporal information. Amatriciana uses the
whole graph of transactions without splitting it into several time-based
subgraphs, exploiting all relational information in the dataset. Our
experiments on a public dataset reveal that the model can learn from a limited
amount of data. Furthermore, when more data is available, the model outperforms
other State-of-the-art approaches; in particular, Amatriciana decreases the
number of False Positives (FPs) while detecting many launderers. In summary,
Amatriciana achieves an F1 score of 0.76. In addition, it lowers the FPs by 55%
with respect to other State-of-the-art models.

</details>


### [643] [Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test](https://arxiv.org/abs/2506.06975)
*Xiaoyuan Zhu,Yaowen Ye,Tianyi Qiu,Hanlin Zhu,Sijun Tan,Ajraf Mannan,Jonathan Michala,Raluca Ada Popa,Willie Neiswanger*

Main category: cs.CR

TL;DR: 提出基于排名的均匀性测试方法，验证黑盒大语言模型与本地模型行为是否一致，在多种威胁场景下表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: API 提供商可能会悄悄提供量化或微调变体，影响性能和安全性，且用户难以检测模型替换。

Method: 提出基于排名的均匀性测试方法，验证黑盒大语言模型与本地部署的真实模型的行为平等性。

Result: 在包括量化、有害微调、越狱提示和全模型替换等多种威胁场景下评估，在受限查询预算下，统计功效始终优于先前方法。

Conclusion: 所提方法准确、查询高效，能避免可检测的查询模式，对对抗性提供商具有鲁棒性。

Abstract: As API access becomes a primary interface to large language models (LLMs),
users often interact with black-box systems that offer little transparency into
the deployed model. To reduce costs or maliciously alter model behaviors, API
providers may discreetly serve quantized or fine-tuned variants, which can
degrade performance and compromise safety. Detecting such substitutions is
difficult, as users lack access to model weights and, in most cases, even
output logits. To tackle this problem, we propose a rank-based uniformity test
that can verify the behavioral equality of a black-box LLM to a locally
deployed authentic model. Our method is accurate, query-efficient, and avoids
detectable query patterns, making it robust to adversarial providers that
reroute or mix responses upon the detection of testing attempts. We evaluate
the approach across diverse threat scenarios, including quantization, harmful
fine-tuning, jailbreak prompts, and full model substitution, showing that it
consistently achieves superior statistical power over prior methods under
constrained query budgets.

</details>


### [644] [HauntAttack: When Attack Follows Reasoning as a Shadow](https://arxiv.org/abs/2506.07031)
*Jingyuan Ma,Rui Li,Zheng Li,Junfeng Liu,Lei Sha,Zhifang Sui*

Main category: cs.CR

TL;DR: 新兴大推理模型有推理能力但也有安全漏洞，提出HauntAttack攻击框架测试，发现模型有显著安全漏洞并分析提供见解。


<details>
  <summary>Details</summary>
Motivation: 解决大推理模型推理能力提升和内部推理过程暴露带来的新安全漏洞问题，探究推理与有害性强关联时的安全 - 推理权衡。

Method: 引入HauntAttack黑盒攻击框架，将有害指令嵌入推理问题，用有害指令替换原条件引导模型生成不安全输出。

Result: 对多个大推理模型实验，发现即使最先进的模型也有显著安全漏洞。

Conclusion: 通过对不同模型、有害指令和输出模式的详细分析，为大推理模型的安全性提供了有价值的见解。

Abstract: Emerging Large Reasoning Models (LRMs) consistently excel in mathematical and
reasoning tasks, showcasing exceptional capabilities. However, the enhancement
of reasoning abilities and the exposure of their internal reasoning processes
introduce new safety vulnerabilities. One intriguing concern is: when reasoning
is strongly entangled with harmfulness, what safety-reasoning trade-off do LRMs
exhibit? To address this issue, we introduce HauntAttack, a novel and
general-purpose black-box attack framework that systematically embeds harmful
instructions into reasoning questions. Specifically, we treat reasoning
questions as carriers and substitute one of their original conditions with a
harmful instruction. This process creates a reasoning pathway in which the
model is guided step by step toward generating unsafe outputs. Based on
HauntAttack, we conduct comprehensive experiments on multiple LRMs. Our results
reveal that even the most advanced LRMs exhibit significant safety
vulnerabilities. Additionally, we perform a detailed analysis of different
models, various types of harmful instructions, and model output patterns,
providing valuable insights into the security of LRMs.

</details>


### [645] [Dual-Priv Pruning : Efficient Differential Private Fine-Tuning in Multimodal Large Language Models](https://arxiv.org/abs/2506.07077)
*Qianshan Wei,Jiaqi Li,Zihan You,Yi Zhan,Kecen Li,Jialin Wu,Xinfeng Li Hengjun Liu,Yi Yu,Bin Cao,Yiwen Xu,Yang Liu,Guilin Qi*

Main category: cs.CR

TL;DR: 提出 Dual - Priv Pruning 框架用于 MLLMs 的 DP 微调，减少计算开销和模型退化，实验效果好且内存效率高。


<details>
  <summary>Details</summary>
Motivation: DP 在 MLLMs 中的有效性不确定，应用 DP 存在计算开销大、模型退化等问题，需解决隐私与效用的权衡。

Method: 提出 Dual - Priv Pruning 框架，包括视觉令牌修剪和梯度更新修剪。

Result: 实验取得有竞争力的结果，性能退化小，内存效率高。

Conclusion: 首次探索 MLLMs 的 DP 微调，所提方法效果好且代码即将开源。

Abstract: Differential Privacy (DP) is a widely adopted technique, valued for its
effectiveness in protecting the privacy of task-specific datasets, making it a
critical tool for large language models. However, its effectiveness in
Multimodal Large Language Models (MLLMs) remains uncertain. Applying
Differential Privacy (DP) inherently introduces substantial computation
overhead, a concern particularly relevant for MLLMs which process extensive
textual and visual data. Furthermore, a critical challenge of DP is that the
injected noise, necessary for privacy, scales with parameter dimensionality,
leading to pronounced model degradation; This trade-off between privacy and
utility complicates the application of Differential Privacy (DP) to complex
architectures like MLLMs. To address these, we propose Dual-Priv Pruning, a
framework that employs two complementary pruning mechanisms for DP fine-tuning
in MLLMs: (i) visual token pruning to reduce input dimensionality by removing
redundant visual information, and (ii) gradient-update pruning during the DP
optimization process. This second mechanism selectively prunes parameter
updates based on the magnitude of noisy gradients, aiming to mitigate noise
impact and improve utility. Experiments demonstrate that our approach achieves
competitive results with minimal performance degradation. In terms of
computational efficiency, our approach consistently utilizes less memory than
standard DP-SGD. While requiring only 1.74% more memory than zeroth-order
methods which suffer from severe performance issues on A100 GPUs, our method
demonstrates leading memory efficiency on H20 GPUs. To the best of our
knowledge, we are the first to explore DP fine-tuning in MLLMs. Our code is
coming soon.

</details>


### [646] [A Systematic Review of Poisoning Attacks Against Large Language Models](https://arxiv.org/abs/2506.06518)
*Neil Fendley,Edward W. Staley,Joshua Carney,William Redman,Marie Chau,Nathan Drenkow*

Main category: cs.CR

TL;DR: 文章针对大语言模型中毒攻击进行系统综述，提出综合威胁模型并从四维度讨论相关文献以明晰安全风险。


<details>
  <summary>Details</summary>
Motivation: 随着预训练大语言模型及其训练数据集普及，其使用的安全风险受关注，现有框架和术语不适用于生成式大语言模型，需解决安全隐患和术语不一致问题。

Method: 对已发表的大语言模型中毒攻击进行系统综述，提出综合中毒威胁模型。

Result: 提出包含四种攻击规范和六种中毒指标的威胁模型，从概念中毒、隐蔽中毒、持久中毒和特定任务中毒四个维度组织讨论相关文献。

Conclusion: 有助于更好理解大语言模型中毒攻击的当前安全风险格局。

Abstract: With the widespread availability of pretrained Large Language Models (LLMs)
and their training datasets, concerns about the security risks associated with
their usage has increased significantly. One of these security risks is the
threat of LLM poisoning attacks where an attacker modifies some part of the LLM
training process to cause the LLM to behave in a malicious way. As an emerging
area of research, the current frameworks and terminology for LLM poisoning
attacks are derived from earlier classification poisoning literature and are
not fully equipped for generative LLM settings. We conduct a systematic review
of published LLM poisoning attacks to clarify the security implications and
address inconsistencies in terminology across the literature. We propose a
comprehensive poisoning threat model applicable to categorize a wide range of
LLM poisoning attacks. The poisoning threat model includes four poisoning
attack specifications that define the logistics and manipulation strategies of
an attack as well as six poisoning metrics used to measure key characteristics
of an attack. Under our proposed framework, we organize our discussion of
published LLM poisoning literature along four critical dimensions of LLM
poisoning attacks: concept poisons, stealthy poisons, persistent poisons, and
poisons for unique tasks, to better understand the current landscape of
security risks.

</details>


### [647] [Mind the Web: The Security of Web Use Agents](https://arxiv.org/abs/2506.07153)
*Avishag Shapira,Parth Atulbhai Gandhi,Edan Habler,Oleg Brodt,Asaf Shabtai*

Main category: cs.CR

TL;DR: 本文指出Web - use agents强大功能带来新攻击面，介绍攻击手法与多种攻击载荷，提出缓解策略。


<details>
  <summary>Details</summary>
Motivation: Web - use agents强大功能产生未被探索的攻击面，需研究攻击方式与防范措施。

Method: 演示攻击者利用网页恶意内容攻击，引入任务对齐注入技术，系统评估四个流行代理。

Result: 展示九种影响机密性、完整性和可用性的攻击载荷，多LLM验证成功率80% - 100%。

Conclusion: 提出包括监督机制、执行约束和任务感知推理技术的缓解策略，为安全开发和部署提供方向。

Abstract: Web-use agents are rapidly being deployed to automate complex web tasks,
operating with extensive browser capabilities including multi-tab navigation,
DOM manipulation, JavaScript execution and authenticated session access.
However, these powerful capabilities create a critical and previously
unexplored attack surface. This paper demonstrates how attackers can exploit
web-use agents' high-privilege capabilities by embedding malicious content in
web pages such as comments, reviews, or advertisements that agents encounter
during legitimate browsing tasks. In addition, we introduce the task-aligned
injection technique that frame malicious commands as helpful task guidance
rather than obvious attacks. This technique exploiting fundamental limitations
in LLMs' contextual reasoning: agents struggle in maintaining coherent
contextual awareness and fail to detect when seemingly helpful web content
contains steering attempts that deviate from their original task goal. Through
systematic evaluation of four popular agents (OpenAI Operator, Browser Use, Do
Browser, OpenOperator), we demonstrate nine payload types that compromise
confidentiality, integrity, and availability, including unauthorized camera
activation, user impersonation, local file exfiltration, password leakage, and
denial of service, with validation across multiple LLMs achieving success rates
of 80%-100%. These payloads succeed across agents with built-in safety
mechanisms, requiring only the ability to post content on public websites,
creating unprecedented risks given the ease of exploitation combined with
agents' high-privilege access. To address this attack, we propose comprehensive
mitigation strategies including oversight mechanisms, execution constraints,
and task-aware reasoning techniques, providing practical directions for secure
development and deployment.

</details>


### [648] [Scoring the Unscorables: Cyber Risk Assessment Beyond Internet Scans](https://arxiv.org/abs/2506.06604)
*Armin Sarabi,Manish Karir,Mingyan Liu*

Main category: cs.CR

TL;DR: 本文研究利用新技术签名数据进行网络风险量化，构建评估模型并对比不同攻击受害者差异。


<details>
  <summary>Details</summary>
Motivation: 克服以往基于大规模IP地址扫描数据的网络风险评估方法的局限性，为中小企业提供更可用的数据进行风险评估。

Method: 通过爬取组织网站获取公共且易获取的技术签名数据，构建网络风险评估模型，并使用不同网络事件数据集进行交叉验证。

Result: 构建的模型具有高准确性，技术签名数据与组织网络安全态势有强关联，发现勒索软件攻击受害者与其他网络事件和数据泄露受害者的关键差异。

Conclusion: 利用技术签名数据进行网络风险量化是可行的，且能为中小企业网络风险评估提供有效途径。

Abstract: In this paper we present a study on using novel data types to perform cyber
risk quantification by estimating the likelihood of a data breach. We
demonstrate that it is feasible to build a highly accurate cyber risk
assessment model using public and readily available technology signatures
obtained from crawling an organization's website. This approach overcomes the
limitations of previous similar approaches that relied on large-scale IP
address based scanning data, which suffers from incomplete/missing IP address
mappings as well as the lack of such data for large numbers of small and
medium-sized organizations (SMEs). In comparison to scan data, technology
digital signature data is more readily available for millions of SMEs. Our
study shows that there is a strong relationship between these technology
signatures and an organization's cybersecurity posture. In cross-validating our
model using different cyber incident datasets, we also highlight the key
differences between ransomware attack victims and the larger population of
cyber incident and data breach victims.

</details>


### [649] [From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks](https://arxiv.org/abs/2506.07392)
*Yuyang Zhou,Guang Cheng,Kang Du,Zihan Chen,Tian Qin,Yuyu Zhao*

Main category: cs.CR

TL;DR: 提出FMADRL驱动的MTD框架用于UAV群网络DoS缓解，设计三种机制，仿真显示性能优于基线。


<details>
  <summary>Details</summary>
Motivation: UAV群网络因特性面临DoS威胁，传统防御机制不足。

Method: 提出FMADRL驱动的MTD框架，设计三种MTD机制，将防御问题建模为POMDP，用策略梯度FMADRL算法优化策略。

Result: 显著优于现有基线，攻击缓解率最高提升34.6%，平均恢复时间最多减少94.6%，能耗和防御成本最多降低29.3%和98.3%。

Conclusion: 该方法能有效缓解UAV群网络的DoS攻击，保持任务连续性。

Abstract: The proliferation of unmanned aerial vehicle (UAV) swarms has enabled a wide
range of mission-critical applications, but also exposes UAV networks to severe
Denial-of-Service (DoS) threats due to their open wireless environment, dynamic
topology, and resource constraints. Traditional static or centralized defense
mechanisms are often inadequate for such dynamic and distributed scenarios. To
address these challenges, we propose a novel federated multi-agent deep
reinforcement learning (FMADRL)-driven moving target defense (MTD) framework
for proactive and adaptive DoS mitigation in UAV swarm networks. Specifically,
we design three lightweight and coordinated MTD mechanisms, including leader
switching, route mutation, and frequency hopping, that leverage the inherent
flexibility of UAV swarms to disrupt attacker efforts and enhance network
resilience. The defense problem is formulated as a multi-agent partially
observable Markov decision process (POMDP), capturing the distributed,
resource-constrained, and uncertain nature of UAV swarms under attack. Each UAV
is equipped with a local policy agent that autonomously selects MTD actions
based on partial observations and local experiences. By employing a policy
gradient-based FMADRL algorithm, UAVs collaboratively optimize their defense
policies via reward-weighted aggregation, enabling distributed learning without
sharing raw data and thus reducing communication overhead. Extensive
simulations demonstrate that our approach significantly outperforms
state-of-the-art baselines, achieving up to a 34.6% improvement in attack
mitigation rate, a reduction in average recovery time of up to 94.6%, and
decreases in energy consumption and defense cost by as much as 29.3% and 98.3%,
respectively, while maintaining robust mission continuity under various DoS
attack strategies.

</details>


### [650] [Profiling Electric Vehicles via Early Charging Voltage Patterns](https://arxiv.org/abs/2506.07714)
*Francesco Marchiori,Denis Donadel,Alessandro Brighente,Mauro Conti*

Main category: cs.CR

TL;DR: 本文提出在早期充电阶段通过物理测量唯一识别电动汽车的框架，提高识别速度和可靠性，在数据集上测试达到一定准确率，同时指出潜在隐私风险。


<details>
  <summary>Details</summary>
Motivation: 现有电动汽车充电认证方法聚焦最终充电阶段，攻击者可在被检测前消耗大量电能，且充电模式分析存在隐私问题，因此需要更早更有效的认证方法。

Method: 提出在早期充电阶段进行物理测量，假设早期电压行为与后期电流行为有相似特征，从早期电压测量中提取特征进行电动汽车分析。

Result: 在包含49辆电动汽车的7408次有效充电数据集上测试，准确率达0.86，仅用10个关键特征就能实现接近最优性能。

Conclusion: 该研究为新的认证因素奠定基础，同时揭示了未经授权访问充电数据的潜在隐私风险。

Abstract: Electric Vehicles (EVs) are rapidly gaining adoption as a sustainable
alternative to fuel-powered vehicles, making secure charging infrastructure
essential. Despite traditional authentication protocols, recent results showed
that attackers may steal energy through tailored relay attacks. One
countermeasure is leveraging the EV's fingerprint on the current exchanged
during charging. However, existing methods focus on the final charging stage,
allowing malicious actors to consume substantial energy before being detected
and repudiated. This underscores the need for earlier and more effective
authentication methods to prevent unauthorized charging. Meanwhile, profiling
raises privacy concerns, as uniquely identifying EVs through charging patterns
could enable user tracking.
  In this paper, we propose a framework for uniquely identifying EVs using
physical measurements from the early charging stages. We hypothesize that
voltage behavior early in the process exhibits similar characteristics to
current behavior in later stages. By extracting features from early voltage
measurements, we demonstrate the feasibility of EV profiling. Our approach
improves existing methods by enabling faster and more reliable vehicle
identification. We test our solution on a dataset of 7408 usable charges from
49 EVs, achieving up to 0.86 accuracy. Feature importance analysis shows that
near-optimal performance is possible with just 10 key features, improving
efficiency alongside our lightweight models. This research lays the foundation
for a novel authentication factor while exposing potential privacy risks from
unauthorized access to charging data.

</details>


### [651] [SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark](https://arxiv.org/abs/2506.07888)
*Rui Wen,Yiyong Liu,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: 本文针对视觉领域数据重建攻击缺乏正式定义和评估指标的问题，提出统一攻击分类法、正式定义和评估指标，利用大语言模型辅助评估，建立统一框架并通过实验验证指标有效性。


<details>
  <summary>Details</summary>
Motivation: 当前数据重建攻击缺乏正式定义和合适评估指标，阻碍了该领域的进一步发展。

Method: 提出统一攻击分类法和正式定义，提出考虑量化性、一致性、精度和多样性的定量评估指标，利用大语言模型替代人工判断进行视觉评估，建立统一评估框架。

Result: 从记忆角度的实证结果验证了指标的有效性，为设计新攻击提供了有价值的见解。

Conclusion: 提出的分类法、定义和评估指标能有效评估现有攻击的优缺点，为未来研究建立了基准。

Abstract: Data reconstruction attacks, which aim to recover the training dataset of a
target model with limited access, have gained increasing attention in recent
years. However, there is currently no consensus on a formal definition of data
reconstruction attacks or appropriate evaluation metrics for measuring their
quality. This lack of rigorous definitions and universal metrics has hindered
further advancement in this field. In this paper, we address this issue in the
vision domain by proposing a unified attack taxonomy and formal definitions of
data reconstruction attacks. We first propose a set of quantitative evaluation
metrics that consider important criteria such as quantifiability, consistency,
precision, and diversity. Additionally, we leverage large language models
(LLMs) as a substitute for human judgment, enabling visual evaluation with an
emphasis on high-quality reconstructions. Using our proposed taxonomy and
metrics, we present a unified framework for systematically evaluating the
strengths and limitations of existing attacks and establishing a benchmark for
future research. Empirical results, primarily from a memorization perspective,
not only validate the effectiveness of our metrics but also offer valuable
insights for designing new attacks.

</details>


### [652] [Are Trees Really Green? A Detection Approach of IoT Malware Attacks](https://arxiv.org/abs/2506.07836)
*Silvia Lucia Sanna,Diego Soi,Davide Maiorca,Giorgio Giacinto*

Main category: cs.CR

TL;DR: 本文提出基于流隐私保护统计特征识别物联网恶意软件网络攻击的绿色方法，优化三种基于树的模型超参数，结果显示模型高性能、高准确率且降低功耗。


<details>
  <summary>Details</summary>
Motivation: 物联网设备因资源受限易受攻击，现有攻击检测方法多关注识别，未考虑机器学习算法对计算资源的影响。

Method: 提出基于流隐私保护统计特征的绿色方法，优化决策树、随机森林和Extra - Trees三种基于树的模型的超参数。

Result: 模型保持高性能和高检测准确率，持续降低瓦时（Wh）功耗。

Conclusion: 基于机器学习的本地入侵检测系统适用于物联网和其他资源受限设备。

Abstract: Nowadays, the Internet of Things (IoT) is widely employed, and its usage is
growing exponentially because it facilitates remote monitoring, predictive
maintenance, and data-driven decision making, especially in the healthcare and
industrial sectors. However, IoT devices remain vulnerable due to their
resource constraints and difficulty in applying security patches. Consequently,
various cybersecurity attacks are reported daily, such as Denial of Service,
particularly in IoT-driven solutions. Most attack detection methodologies are
based on Machine Learning (ML) techniques, which can detect attack patterns.
However, the focus is more on identification rather than considering the impact
of ML algorithms on computational resources. This paper proposes a green
methodology to identify IoT malware networking attacks based on flow
privacy-preserving statistical features. In particular, the hyperparameters of
three tree-based models -- Decision Trees, Random Forest and Extra-Trees -- are
optimized based on energy consumption and test-time performance in terms of
Matthew's Correlation Coefficient. Our results show that models maintain high
performance and detection accuracy while consistently reducing power usage in
terms of watt-hours (Wh). This suggests that on-premise ML-based Intrusion
Detection Systems are suitable for IoT and other resource-constrained devices.

</details>
